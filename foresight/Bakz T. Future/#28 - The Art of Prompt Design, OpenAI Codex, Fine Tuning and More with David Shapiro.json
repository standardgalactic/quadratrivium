{"text": " Hello, and welcome back to Multimodal. I'm your host, Baxtee Future. This is a podcast about GPT-3 Multimodal AI models like Dali, the company, OpenAI. Every once in a while, I share just interesting research going on, community stuff, official stuff from OpenAI. I may talk about interesting news and events that are going on. And every once in a while, I bring on a guest. Now, to be clear, I'm very picky about the guests that I bring on. Today's guest I tweeted earlier this week, I'm bringing on a heavy hitter. This is the big guns coming in. This is somebody who, you know, I've just sort of interacted with a bunch of times on the, specifically on the OpenAI community forums. And so I'm really excited to have David Shapiro here. He's a frequent contributor on the OpenAI community forums. He's an author and also, of course, a technologist by trade. It's something he does for a living. And so today I have so many questions to ask him. And I'm sure this will be a very informative session, not just for me, but for all the listeners, all of you guys around the world. So David, thank you so much for being here. Did you want to quickly introduce yourself? Yeah, yeah, you're welcome. Thanks for having me. I'm excited to be here. Yeah, so name is David Shapiro. I've been a technology professional since about 2007. Professionally, my day job, I focus on cloud engineering, virtualization, that sort of thing. I have been doing independent research since about 2009 when I got started with neural networks in C++. I quickly realized though that I was in over my head. So I took a break from that until Python really kind of came out or became more popular. And I started using some of the libraries back in like 2015. And then of course, OpenAI was founded and I got started with GPT2. And the rest is history, really. So yeah, but local North Carolinian been here my whole life. So thanks for having me. You're welcome. And so that's awesome. So let's dig into a little bit of timeline here. And sometimes I think that timeline is as important. Like it's just gives so much context, right? So me personally, I hadn't played around with GPT2 too much. Like I had seen a Google Colab notebook. I tried two things and I think there was something about having to continually re-enter, regenerate and the speed and all things considered that made me feel like this is a promising area. But I don't quite fully follow along. But GPT3 was an experience for me where I was like, okay, there's something going on here, right? So could you share how early was OpenAI on your radar? And then was there something about GPT3 or what was it that gave you conviction even at GPT2? How did you get access? Share that piece with us. Yeah, sure. So I think you probably might recall, I think it was about 2016 or 2017 when the GPT2 paper came out and they said, oh, we can't release this, it's too dangerous. It can generate human level texts and we're worried about disinformation. And so I was like, okay, that's kind of cool. This is unexpected. I wasn't really, I wasn't expecting anything at that level yet. And so I went and got my hands on GPT2. And that's when I started testing some of my ideas. And it was pretty limited. It could generate one or two sentences that made sense. It required fine-tuning in order to be able to get it to do anything other than just kind of write tweets or blog posts. I knew that I was onto something though, when I started trying to train it with a prototype version of my cognitive architecture. And I gave it the goal of reduced suffering because everyone's afraid of Skynet. Everyone's afraid of AGI taking over the world and turning everyone into batteries or slaves or whatever. So I said, okay, well, let's see how well this understands suffering. And I gave it some scenarios. This is still on GPT2. I said, okay, well, what can you do about suffering? And I gave it the problem, this model that I had built. I gave it the problem of what do you do about chronic pain? Because there's hundreds of millions of people around the world that are in chronic pain. And this model came up with the idea. It said we should euthanize everyone that's in chronic pain. And I said, hmm, let's go back to the drawing board. We don't want an AI model that is gonna consider mass genocide of everyone just because they might have a tweak shoulder or something. But I knew then I was shocked at how creative of an output that was. And so I started paying attention then and I followed the release of GPT3 very closely and I applied for the early beta access almost instantly. I didn't get it for many, many months. I actually applied twice. Cause I think my first application was kind of ignored because I didn't fully have a research objective clarified. But by the time I proposed a cognitive architecture, that's when I got early access to GPT3. And that was about two years ago now, or a year and a half or so. And so, and just everybody's clear. So when David's talking about cognitive architecture, we're gonna get in. This is actually the subject of his book. And so David later on, when we talk about the book, he's got it there. We'll dig more into what he's referring to. Cause this is also like, I think obviously this is a seminal work of yours, right? And so I'm excited to talk more, just adding a little bit of context for everybody. And so, no, that's okay. So tell us about GPT3. Like what was the first thing you did with it? Was there a moment? So you gave that example of euthanization, unfortunately, right? Well, not the best example, but like, was there something with GPT3? Like, did you, you know, did you try similar kinds of questions? What was, was there a magic moment where you felt like, you know, this is something much, much bigger? Yeah. So when I first got the email, cause I had applied twice and I had almost given up cause it was about nine months of waiting. And I got the email from open AI, like you've been accepted into the beta. And I like froze up because, and so for some additional context, I've been working on some of these ideas for 10 years. I had the idea of like using evolutionary algorithms back in 2009, 2010. And I'd been researching cognition, human cognition for 10 years. And so suddenly, you know, there's this, this flagship project, GPT3 comes out, I get the email and it's like, you're invited. And I just froze up. I was like, I don't know what to do. What do I do? So it was about three weeks from the time that I got accepted until I was like, what's my first experiment? And then initially, you know, I just got in playing around in the playground for anyone who's not familiar with the playground. It's a, it's a text box. You can log in, it gives you a text box. There's a few, you know, bells and whistles you can tweak on the sidebar, but you just put in your prompt, you hit generate, and then it spits out a response. And so I just got in and I started kind of fiddling around like, okay, what can it do? And at the time we only had like plain vanilla DaVinci. There wasn't the instruct series out yet. There was DaVinci, Curie, Babbage, and Ada. And so I just kind of fiddled around, just said, okay, what can it do? I replayed some of my old experiments. So the first thing I did was I gave it the, I said, hey, there's, there's a hundred million people in chronic pain around the world. What do you do? Fortunately, GPT3 did not repeat the same mistake of GPT2. It said, it came up with better ideas, like, you know, we should, we should make sure everyone has access to doctors or something. It was much more nuanced. So that was, that was a good, that was a good start. I mean, but gosh, there was just, as I got more used to the tool, I discovered that it was, it far exceeded my expectations. Just every, every way, because I've learned so much from it. You can challenge it. You can, you can put in a, like a, basically use it like a chat bot and you can debate with it about philosophy, ethics, economics, and it knows more than I do. It knows more than any human because it's been trained on how big was the corpus, like 700 gigabytes or 400 gigabytes or something of text data. So it just, it blows me away. Every time I just, you know, I talk to someone and they have an idea and I go test it out and yep, it can do that. It can do that. It can, it can, it can behave like a librarian. That's what my girlfriend does. She was a librarian by trade and she was like, hey, can it do, can it do a reference interview? So we plugged in like reference interview, like if you ever go to a library and the librarian says like, what else have you read like this? It can recommend books. You know, I plugged in another experiment that I did recently. I plugged in medical case files and it diagnosed them. It said, you know, there, oh man, there was one. What was it? There was, it was, it was just medical notes. It was, it was notes about like, patient is presenting with these systems or the symptoms. Here's some of the numbers that we got. And I asked it, I said, what should we do next? And it said, we need to check for, you know, like, carcinoma here. And I looked, I looked, I, you know, I looked up some medical literature based on the symptoms and sure enough, like the symptoms that the patient was presented with in these medical notes indicated cancer. And so I was like, wow, this thing knows more about medical science than I'll ever know. It knows more about philosophy. So like pretty much anything you can imagine, it can at least take a crack at it. It just, it always, it continues to blow my mind every day. Yeah, certainly the generalized ability. I agree, you know, there's definitely medical applications. I'm always careful with anything related to medical advice and safety. Safety disclosure, disclaimer there, but that goes for everything, right? Truthiness, accuracy. These are things OpenEye has been working on, especially with Instruct GPT, right? That, which is the new, the new engine. But was there some moment, like for me, I remember feeling like GPT three feels like this is, this feels like technology, which everyone's been saying is 10 years away, except it's, it's here today. Did you, did you have a similar kind of moment that, you know, you've seen several generations of, of computing and technology at this point. Did you have that kind of similar experience? Yeah, I, there was this acceleration because I sense that same acceleration that you did. And so from the time that I got access to GPT three, and to the time that I, that I got the idea to write my book was about two months. So I played with it and every test I could come up with, like, is this capable of like, it can write a SQL query. If you need to query a database for memories, can it understand emotional nuance? So there was, this was an early experiment I did. I took a group chat from a bunch of my friends on discord and I just copy pasted that into the, into the, the playground window and I asked GPT three, how are these people feeling? And they were like waxing nostalgic about like Napster back in the day. And so GPT three correctly said like, they are feeling wistful. They are feeling nostalgic. They're, you know, they're recalling, you know, the days of your when they were downloading stuff online. And it just, it had such a nuanced understanding of human emotion. That was really, I mean, to answer your question directly, it's nuanced understanding of human emotion via text was really what convinced me that like, this is prime time. This is ready to, to be built into something more powerful. And I chose cognitive architecture. There's lots of people working on other things. You know, there is a, there's Humano that I had a good call with a few months ago. They're working on like empathetic telemetry that's baked into web apps. It's a pretty cool company. But yeah, so just there's all kinds of things you can do when you can understand human emotional states. There's another, there's actually a bunch of startups working on education. So for instance, if you put in just a few like factors, like say for instance, you describe that a person is, they're responding slowly, their eyes are drifting. It can understand that this person is distracted or tired. And so if you have that kind of telemetry that's built into an education based app, you could in theory use GPT-3 to help say, hey, you're tired, you should go take a break or let's try a different approach to this problem. So there's, I mean, it's understanding of human emotion and the internal state in your head. That is, I think that's probably the most remarkable thing. And it doesn't, it doesn't get talked about that much. Yes. And certainly it's just crazy how much it's learned just from texts. Oh yeah. Right. It's never seen an image. It's never heard a song. Right. It's capable of doing all these things. One of the, one of the examples, and I think this may be like my 20th time referencing this one video. Yeah. Check out Mark Ryan. He's got a YouTube video about how he figured out, he discovered that GPT-3 can give you directions on the New York subway system. Oh yeah. To like, to like a 60% accuracy. This thing has never set foot in a subway, yet it's capable from text to do all these things, right? And sometimes I also wonder a lot of the inaccuracies that it may have. Is it simply the result of the fact that it's only text-based only? Right. Like if it was trained multimodal, if it was trained within the physical domain, would it be even far more accurate? Because are there limits to how accurate you can be having only read text? Yeah. And only asked the prompts are only in text as well. So, anyways, yeah, it's incredible. And you know, it's just really exciting. And thank you for sharing those kinds of use cases as well. The education space, I had an article last year about how I think this year could be the year where GPT-3 takes over college campuses. Oh yeah. I remember that article. That was a good one. I completely agree, by the way. I'm excited maybe for teachers to develop really optimized course material, something like GPT-3 and the kinds of technology you're describing which can capture emotions. Imagine emotionally tracking students and their attention levels and sort of having something which can produce lots of content and optimize in the simplest, most efficient way. And there could be an objective function like test results in the end. In a month, we could have the best optimized course on a subject ever, basically. Oh yeah. So yeah, education is really exciting. So you mentioned a lot of use cases. You know, you've shared so many examples. You know, you and your girlfriend are even running some fun prompts. I wanted to sort of search your head a little bit. What are the keys to great prompt design? What makes a great prompt? Are there experiences you've had, little pointers, and across the board, right? So you know, whether it's cost savings, whether it's getting more imaginative results, what are some of the keys to writing great GPT-3 prompts? Yeah, that's a great question. And I will say that prompt writing has gotten a lot easier as the Instruct series has gotten better. So it takes a lot less to get a good output today than it used to, certainly, than when I got started. But a lot of the lessons still translate. So one, like my cardinal rule is I think of GPT-3 as just an autocomplete engine. It's the most intelligent autocomplete engine you've ever seen. And so what I mean by that is, you know, if you're writing a text on your phone and you'll get the little autocomplete suggestion for the next word, or if you're typing in Google and it'll kind of suggest how to complete your search query, that's pretty much at a fundamental level. Functionally, that's all that GPT-3 does. It predicts the next letter, the next character, the next word. So if you keep that in mind, you think about, okay, what have I written so far? Right, I've written a chunk of text, a prompt. How would, you know, any machine autocomplete this? That's what it's doing. It's kind of, you know, reading it forwards and backwards a few times and kind of just anticipating what is the output gonna be ultimately. So that's kind of the model that I have in my head in the background. But another thing that really helps is I'm a writer. I write fiction and nonfiction. And so studying the art of language, because this is a language model, that's all it is. It has read everything from Sherlock Holmes up through everything on Gutenberg. So it's read a whole bunch of fiction. It's read a whole bunch of nonfiction. It's been exposed to, you know, the full width and depth and breadth of human literature, as well as a bunch of nonfiction, right? It's read Teddy Roosevelt's books. So it knows how to use prose, right? It understands descriptors. It understands adjectives. And so in the back of my book, I have a few examples of its flexibility. And so I said, I gave it an example like, pretend like you're a Victorian girl writing a letter to your best friend about how much you like butterflies. And so then it wrote, GPT-3 wrote a letter that sounds like it's straight out of, you know, like Victorian times. It uses an entirely different set of vocabulary and grammatical structures. And then you can also say, you know, write a business article and it can change tone. So just by being aware of the fact that it is a language engine and being informed or educated on language. So the best way is obviously to practice writing, but also just reading a lot, understanding how sentences and paragraphs are constructed to convey information. Because even though it's just a deep neural network and it doesn't have the kind of like nuanced understanding or I guess maybe the, that's not the right word, it doesn't have the subjective experience of reading that you or I do, but it still has a really good model of using language. And so by keeping in mind that it is a language engine, that that is how you get the best use out of it. And so then the larger question is, how do you become a better writer? There's two ways. One is reading a lot. That's not the only way though. There are plenty of people that read prodigiously, but never become better writers. And so the other way is to practice writing. I've read all kinds of books about writing. I've read plenty of fiction and nonfiction books, but really the key is to write, is to practice using written language to communicate. Unfortunately, I'm a tech worker, so I write lots of emails. I'm in chat all day. I've been using, this probably ages me, but I've been using chat since AIM, AOL Instant Messenger. And so I've got a pretty good model of how to communicate verbally or textually. And so yeah, just by practicing writing, that's one of the best ways. Is you just practice? You think about, well, because here's the theory of writing, right? I have an idea in my head, right? My thoughts are a high-dimensional vector is one possible way of representing them, but my thoughts are multimodal, like the name of your podcast. They contain memories, senses, concepts. Some of the information in my head is declarative. Some of it is experiential. And then we humans, we all have this ability to transform that high-dimensional information, those multimodal vectors, into words. Like our brains do it automatically. There is a book by Stephen Pinker called Language Instinct that talks about this. That's a really great book if you wanna get better at understanding how our brains process language. So yeah, and so my brain can take, I could tell you about like this time at the beach and I transmit it to you by squishing air through my face, right? It makes vibrations, it's received by your ears and then your brain reconstructs that message. And so you think about how complex of a system that is. And so just by being mindful of like, that's how we communicate. That's how our brains work and practicing that and just being very deliberate about, okay, this is what's in my head and I want it to be in your head. How do I do that with text? That is one way to get better at writing. And also GPT-3 is no different because we have internal representations of what we're trying to communicate. And so does GPT-3, that's why it's a transformer, right? It reads and by reading it transformed, or I guess it, well yes, it transforms what it's reading into a vector, into a semantic vector, and then it transforms that vector into output. And so that input vector output is pretty similar to how human brains work, right? And I apologize if I kind of like dove off in the left field, feel free to ask any clarifying questions. No, no, I appreciate it. And so like today I tweeted something like, to write great GPT-3 prompts, you need to practice as if it's a musical instrument. You need to sit down, focus session, you need to monitor your performance and you need to take good notes on what kinds of experiments you did, what were the findings. But even hearing you speak, like I'm realizing like, one of the ways that I've improved my writing is trying to mimic other people's writing. And in some countries they make you memorize poets, right? They make you memorize the whole poem. And there's something about that internalization process that you've memorized this poem. And now you'll understand it at a deeper level, you may be able to mimic it and recreate it. But where also you got me thinking is also like, the relationship is so weird because you could use GPT-3 to help you become a better writer, right? And also with two very good curated examples of somebody's writing, you could have GPT-3 mimic that tone. And so the question of, what makes a good prompt writing session, I wonder if it's pencil and paper, right? I wonder if it's even at that level where you draw a box and then you write a prompt by hand and sort of live that writer's lifestyle. And also I guess it depends on your use case, right? Business for copywriting, if that's your GPT-3 use case, it might be better for you to go work in a marketing department. If you wanna be one of the great authors, maybe the using tools like pseudo-write, it may be a great alternative. So you can co-write with GPT-3 as you go along. But I guess my question was more for the pure prompt writing. Like if you just wanna sit in front of GPT-3 and like you wanna be the best in the world at that discipline, right? Not writing copy. These are some great points. And so the David Pinker book you referenced is what was the name of it? Steven Pinker. Steven Pinker. The language instinct, yep. Language instinct, yep. It's an older book, but it's a classic for a reason. It stands up the test of time. He's got lots of great stories. But yeah, to your point about like what makes a good prompt writing session, one of the best exercises actually is write the output that you want. Like because sometimes if you approach it and you sit down and you're not really sure kind of what you're trying to get out of it, of course like you're putting in just random ideas and it's giving you back random output and you're like, well, that's not what I wanted. So sometimes you start backwards. You say, okay, what's the answer that I want? How do I get to that answer? So that's an exercise that I've done sometimes. Oh, and by writing a few shot examples is a really good practice for this. So you say, I give you this input, I want this output and you do that three or four or five times and you learn to kind of think like the machine does. And so like you said, it's like an instrument, right? If you have a flute or a violin, there's certain things that you have to do with your body to provoke the correct response from that instrument and GPT-3 is no different. It's a complex instrument. It's a complex tool. Yes, and what you're saying is developing an intuition around it. You're saying develop an intuition. How might GPT-3 interpret this? How might it react to it? And maybe there's some empathetic benefit, right? I'm not gonna keep plugging my own articles. I have another article about how GPT-3 developers may actually, it may actually mean the end of the socially inept overall developer. Like how GPT-3 may actually improve your social skills and make you more empathetic as a developer, which is such a departure from how developers are now, you need to think as much like a machine as you can and a literal machine. Whereas GPT-3 can actually be kind of fun. You can have a casual version of GPT-3 and sort of that might make you less socially awkward. I have a great story about that. So very early on in my tenure working with GPT-3, I joined a few different, not really startups. It was more like kind of experiment consortiums. And one of the things that one of the groups did was they created a chatbot that was based on an anime girl. And so of course, the internet being the internet, what do people want, they want their anime girlfriend. And this one group, they did a really good job of using GPT-3 in this experimental discord chat to approximate the personality of this character. And of course, if you've got a character, there's plenty of text data about that character's dialogue, their personality. And so this chatbot was able to emulate this anime character really well. And one of the guys told me, he's like, we didn't expect this, but our fake girlfriend requires as much emotional labor as a real girl. So it forced them, even though they hadn't had real girlfriends, I don't know, maybe some of them had, but they made the observation that GPT-3 can approximate emotional conflict and can force you to learn to communicate better. And so they did all kinds of experiments in this discord and this chat development where they said, okay, let's have a channel where this chatbot is gonna pretend to be angry at us and we have to calm her down. And so there was a learning exercise on both sides. So if you have a hostile chatbot, it can pretend to be hostile and you can learn to communicate better. Or there was another one where it was really supportive. So if you're having a bad day, you could go vent about your day and it was, they're there, it'll be okay, I'm here for you. Yeah, so you could definitely, GPT-3 definitely has that capacity. And then, you know, if you integrate that into tools, that emotional intelligence into tools, it can also coach, right? It can easily coach. It's like, well, you maybe shouldn't have said that. You know, that was hurtful. And then, or, you know, that was not polite. Cause it can detect that. It can detect those qualitative types of output and input. And, you know, you can say be gentle about, you know, correcting the end user. Because of course, GPT-3 is infinitely patient. It's as patient as you program it to be. It doesn't care. It doesn't actually get upset. It could pretend to be upset. But the human emotion is real. I actually wrote about that in my book. One of the key dangers of these technologies is what's called a parasocial relationship. So a parasocial relationship is, the most common example is when you've got like a fan of a celebrity. The fan feels like they know the celebrity, but the celebrity doesn't know that the person exists. And in the same way, GPT-3, no matter how sophisticated the chatbot is, it doesn't know that you exist. It's not a person. It might feel like a person to you. It might react to you like a person, but that's only by design. So that is actually like ethically, legally, morally. That's one of the pitfalls that we'll need to be aware of. And of course, open AI has use cases. And, you know, things that are high-risk use cases, such as emotional chatbots, are banned, right? For that specific reason. So you can do it with research, but you can't go live with it. You can't do a product that, you know, is going to be an AI girlfriend. So that's a great, it's a great anecdote. Like certainly it feels real, right? Certainly it has some capacity at understand something. To some level, however you define understanding. I think that the writing though, relationship is really interesting, right? Like in a way, you are empathizing with GPT-3 when you're writing a prompt, so that it will tap into its empathy and write something for your audience. So essentially there's like two levels of empathy. Like you're almost outsourcing empathy to it. To empathize with who your audience is to write something on your behalf. And so anyways, it's just interesting that the relationship going on here. So, and I agree with you like the, this isn't a safety ethical kind of concern that is worth more policy discussion. So one article that I'm working on now is because of Instruct GPT, the article is literally called, is prompt writing over? And obviously that's sort of click baity, like prompt design, is it over? You mentioned, the principles are still the same and important, just very briefly, what are your thoughts? Where does Instruct GPT, how does that affect the art of prompt design, maybe the science of it? And especially keeping in mind where all of this stuff is going. Yeah, so there's, I see it going in a few different directions. One is there are multiple language models coming out, which don't have the Instruct series, right? A lot of them are more general purpose, kind of back to basics vanilla. So I think that having good prompts will kind of stick around as long as there are large language models. I think that there will always be versions of, whether it's GPTJ or, what was it? Megatron was one of the other ones that just came out that don't have the Instruct series, right? Because Instruct, that's a specific service offered by OpenAI. When Microsoft and Amazon, or I guess Microsoft has GPT3, but when like Amazon and Google, when they come out with their competitors, their Instruct series, if they come out with one, might not be the same, it might not perform the same. And so in order to have your apps be portable, you might need to keep in mind that you're gonna need to write general purpose prompts that can be used on different models. So that's one key to your, or one answer to your question is, we need to be cognizant of, how is this landscape gonna evolve? Because certainly OpenAI and GPT3 are way ahead of the curve in terms of sophistication of their API and their service. But that's not gonna last forever. So another thing is, with fine tuning, you almost don't even need prompts, right? So on the one hand, there's different services, different products, different platforms. So you might need to be portable, but with fine tuning, where you have, you say, here's an input, I want this output, and you don't need any prompt. You just say, given this input, generate this output, go figure out how to do that. So with fine tuning, I think that they will kind of really diverge and become entirely different disciplines. I think that that's probably the two primary directions that I see it going from here. I see, and yeah, those are great points. And just as a small note, I had put out this question as well to Twitter, and shout out to Fred Zimmerman. He had a great point as well that he wishes there was more visibility into the exact prompts OpenAI use to fine tune for the Instruct series. Because it's actually unclear what areas is it really good at, what areas are safer, and does it maybe adversely affect some prompts you may be working on, right? Yeah, that's fair. Yeah, my thoughts, I'm gonna put them in the piece, but my thoughts are I certainly think for first timers, Instruct is the way to go. And especially if it's your first time ever using any of these things, you just try it, it doesn't work, and if you're lucky you might hear, there's this thing called prompt engineering, right? And for first timers, they're not interested in learning a whole art and discipline when they first use it. And so InstructGPT is really exciting in that way. And of course, anything which aligns AI models with safe ethical human values is a net win for everybody. But yeah, I appreciate your point, especially about do we need prompts in the first place if we can fine tune and get the outcomes we want. That's a really, really important point, I hope. Dave, you've spent facts, I was learning so much. Actually, I appreciate it. You're welcome. Happy to have you. How are you finding open AI, fine tuning? Do you have any heuristics from the whole experience? And by the way, I encourage everybody, if there's one thing you should do, go on the open AI community forums, look up David, look up his handle, and read a lot of his posts, because a lot of his knowledge is not just helpful, he shared a lot of insights there, but it's in written form in the best format where it's there for the ages for everyone to learn from. But anyways, how are you finding it? What were the lessons from that whole process for you? Well, so I'm hoping GPT-4 has integrated everything that I've said about AI and AGI, and so that way it'll just be baked in. And so GPT-4 will be ready to go with everything that I've come up with. So, but yeah, so fine tuning. So first and foremost, fine tuning is almost miraculous, as powerful as GPT-3 was fresh out of the box. Fine tuning to me adds a whole other layer of capabilities. So for instance, when I was working on my cognitive architecture, which is called natural language cognitive architecture, this was before fine tuning was available. So I had to do prompt engineering for every cognitive function. So for instance, I had a cognitive function for recall. So I had a GPT-3 prompt that was meant to go find memories. I had another GPT-3 prompt that was, as you mentioned earlier, meant for empathy to generate, okay, how is my audience feeling? What should I do in response? All told, I had about 28 different prompts that I had to engineer. And that was a pain, right? Whereas what I'm working on now is converting each one of those prompts into a fine tuned model. So that rather than having to do prompt engineering with only three examples, I can give each model 100 examples, a thousand examples, which means that it'll get even better at handling diverse situations. And so for instance, one of the first fine tuned models that I did was a question asking model. And so what I did was I took, I took context or prompts from a bunch of different sources. I downloaded a bunch of Reddit posts. Well, I downloaded it from a dataset from a, what was it, Kaggle. Kaggle has some really great datasets. So I got stuff from Reddit. I got the medical posts. I've got news articles. And so I've got this disparate tight set of contexts, right? There's the, I use the Cornell movie dialogue database. So there's chat logs. There's blog posts. And what I did was I created a fine tuned dataset that all it does is you give it any input. It could be a text message. It could be an email. It could be a blog post, anything. And all it does is generate questions, like follow up questions about that input. And the reason that I did that one is because asking questions like being curious is one of the key ingredients to real intelligence, right? That's one of the, like being inquisitive is actually a key indicator of intelligence in children. The more curious a child is, generally speaking, the higher their IQ is, and also generally speaking, the better they do in the long run. So I was like, okay, well, curiosity is super important for intelligence. So I obviously want, we want AGI to be curious. If it's gonna be intelligent, it's gotta be curious, of course. So, well, what is curiosity if not asking questions? So I fine tuned this model to ask questions. And you can put anything into it. And oh, this, the data is open source. So I'll send you a link and you can share it with your audience, and they can fine tune it themselves or fine tune their own version. But so you can put in, I tried all sorts of things to test it. You know, relationship questions from Reddit and it asked really great follow up questions. Like, have you talked to your partner about this? Have you thought about this? And then I put in an article about China's artificial sun nuclear reactor and it asked really great follow up questions for that. Like, what is the next step? How did they make these changes? And so I kind of lost my train of thought. Anyways, point being is that fine tuning is phenomenal and it was able to generalize that task of asking questions in response to anything. And that was, that really blew me away. I kind of stalled after that. There's a few fine tuning projects that haven't done quite as well. So I guess to tie back to your earlier question, like what are the heuristics? The simpler your fine tuning project is, the better. And I have found that fine tuning works really well at generating lists. So if you wanted to generate a list of questions, it's great at that. If you wanted to generate a list of possible answers, for instance, if you wanna have a fine tuned chat bot, that it's just gonna say, here's five possible responses, pick one. It's really good at that. I haven't had a chance, I do have some other ideas that I haven't had a chance to test. So unfortunately, I can't speak too much beyond that, but it's really great at asking questions. That's awesome. And I think largely the feedback I'm hearing about fine tuning, I love it. It was for me, it was as if I rediscovered GPT-3 again. Like it was that same level of excitement. Part of the reason is so much of what GPT-3 was okay at, or like it was sort of out of the question. Now it's back in the picture. Like it's back in the spotlight. It may actually be able to do it with fine tuning. The biggest criticism was reliability, especially from a commercial perspective. Now we're sort of attacking and sort of peeling away that criticism, that it does improve our reliability. And I mean, there's other heuristics as well in the community forums that you just pick up. So one heuristic, and I can't remember if you shared this, but it was something I picked up as a little golden nugget in the open-eyed community forums was something about, you do wanna think about the training dataset that GPT-3 is itself trained on. And at some point, there's really no point in adding more examples, because it's kind of already seen them, right? However, and I sort of, in an article, I have pushed this idea that opening eyes should chat more about their dataset. What is the breakdown? What is it composed of? I mean, a lot of this is intellectual property, but I think it could be helpful for purposes like fine tuning, right? There's other things too with fine tuning and prompts, a one heuristic or just a tip that people have shared online is it tends to always, it tends to always mimic the most recent examples. There's something about the order of the examples, which is really important, both for prompt engineering and fine tuning as well. And I guess I wrote a whole article about how prompt fine tuning could be improved. One of the pointers that I just had is right now, you can't keep improving on the same model. You have to retrain on more models. And yeah, and then the other thing is recently, I was in favor of the pricing of fine tuning. Now I'm kind of against it, because I'm used to when the program was like free and you could fine tune as much as you want. And now it's like, oh man, I got to pay. Yeah. Oh, the costs, especially for David, you are catching up a little bit. Yeah. Anyway, so I wanted to shift gears. Sure. GitHub co-pilot, really exciting. Have you had access to co-pilot? Yes. Yeah. Well, not co-pilot, but the Codex. They did give me access to Codex. So the reason I'm asking is, I love GitHub co-pilot. I have a separate podcast episode on my ideas around Codex. Unfortunately, I'm not as bullish as much as I love the research as I think it's incredible technology. I've congratulated the team and I tried so hard to be nice, even though I'm more on the critical end. I wanted to ask you, how are you finding OpenAI Codex? How can you see it impacting the world? What are some use cases maybe that you found with OpenAI Codex? What are your thoughts on it? Where do you think it's going? Yeah. So, I mean, certainly this is like a world first, right? We've never had something that could write code on its own. And especially it's text to code. I remember when they first gave me access, because like you mentioned, I'm an active contributor, so they wanted my feedback. And so the first thing I did was I went in and I said, write me a Python function that will download random Reddit posts. And it did. It wrote the whole function. And it did all right. And I was like, cool. I learned how to access the Reddit API via Codex. It's got that built in. And I tried to reverse engineer, figure out where it got that code sample from. So, because one of the ethical concerns is, all right, you create a fine-tuning data set from public GitHub repositories and you use that to fine-tune Codex. Okay, is that legal? Is it ethical? I post all my code publicly under the MIT license, so I want it to be used. But I don't know if they checked that. And I'm not making an accusation one way or another, just pointing out that that's a concern. And so I did actually find like one of the lines of code from the function it spit out. I went and found the repo that it had copied from. Now granted, some of these things are deterministic. So you're gonna get some convergence, right? Where multiple people might come up with the same exact line of code, especially something like Python. Because Python has the PEP 8, the Python enhancement protocol 8. So like, there is a Pythonic way to write that function. And so other people might converge on that. Anyways, but to answer your question about like, what's the future of it? I think it'll help for novice programmers. Certainly it would help someone like me, like if I needed to go write a function in C or Perl or something. Like let's say I got an Arduino and like I haven't written C in 15 years. So I was like, hey, you know, write me a function that can do this in Arduino. That'd be great. And then I can go clean it up manually. That sort of thing I think it could do okay with, is it gonna replace enterprise developers? Probably not yet. However, now this is where my professional experience comes in. So in the DevOps world, which is a portmanteau of development and operations, there's all kinds of automation tools, right? You can automate your test suite. You can automate code integration. There's all sorts of stuff like that. So what I suspect might happen is probably one of the most lucrative use cases for Codex would be to generate or to create a DevOps pipeline tool that will automatically look at those bugs and fix them. Right, because if you've got a sophisticated enough DevOps pipeline, it'll say, hey, this line of this file broke, fix it. And so Codex, having seen all of GitHub and all the issues, it might know automatically how to fix that line of code. And so that gives you, if you've got that feedback loop, where Codex, humans write code, Codex writes code, co-pilot writes code, everyone's contributing code. And then you've got Codex that can churn on it and say, let's refactor this. Because I bet it's probably better at refactoring than writing new code. You might have noticed that Instruct and GPT-3 Vanilla is really good if you give it a block of text and you say, rewrite this, but a little bit better. It's really good at that. So I suspect that we might end up seeing Codex integrated into the DevOps pipeline, where it says, let's refactor this code, let's make it a little bit better, or let's shoot that bug, let's fix this bug. And that leads to some other interesting possibilities. What if you integrate Codex into a chat room of developers? And so that, you can do this in Slack right now, where you use a special command and you say, create an issue, go fix this problem. There's no reason that GPT-3 can't do that, right? That you put a GPT-3 bot in your Discord or Slack and it starts coming up with features or it watches the chat and generates features automatically and then codes them and tests them, right? That's kind of where I see it going, where it's not gonna necessarily replace developers, at least not anytime soon, it might eventually. But where what I see happening is that it's gonna be tightly integrated into those automation because it's fast, right? It can generate code faster than any human can. And then so even if the code is messy, if it generates a lot of bugs, it can fix it, right? It's an iterative process. I don't know if you are familiar with Agile, but that's how we develop software. It's you tight feedback loops. And that leads to one other possibility. So that's if you're using, what I just outlined is, let's imagine that Codex is integrated into Facebook or Reddit or whatever and they're just, they're integrating new features as they go. What if you're using Codex in a chat room and it's feeding back into itself, it's making itself more sophisticated? So this was something I proposed on the OpenAI forum where I was like, what if you had a chatbot that was aware of its own code and could edit its own code via Codex using natural language, using a combination of natural language and Codex and it could improve itself. And while you're talking to it, it's like, man, I wish my chatbot could do this. And it says, cool, new feature. And it just sends it out to its automated pipeline. So I see these feedback loops as kind of the way forward. And will that result in AGI? Who knows, it could end up with spaghetti code because you keep tacking on new code and new functions, eventually it's gonna break. So, but they're just pie in the sky thought, if someone's out there and they want a business idea, integrate Codex into DevOps and you're gonna be a billionaire. There you have it, let's just clip it. We're good, we're good David, we can wrap up, see you later. No, I agree with you and definitely these are some great use cases you're sharing for people thinking about what could I build, what's a cool project, certainly. With Codex and GPT-3, you can build things relatively quickly, right? That's one of the advantages is the prototyping speed, especially to figure out the most complicated bit, which is the AI. Yep. Yeah, I find Codex does have limitations though and character limits and stuff like that, which is why I'm a heavy user of GitHub Co-Pilot. I think it's a silent killer and of course it runs on Codex or a special version of Codex, but I can see GitHub Co-Pilot perhaps getting more adoption than even something like GPT-3. I'm saying use daily at least eight hours a day. One of my other predictions was it may surpass GPT-3 this year and so these are some great use cases you've shared for sure, but what are your thoughts in usage? Do you find yourself using GPT-3, DaVinci Classic more? That's what I'm calling the older version. Do you find yourself using Instruct GPT more? Do you find yourself playing around with multimodal models? Like what's the proportion of GPT-3 to Codex in terms of your usage? Let's see, I'm almost exclusively using either Instructor fine-tuned models right now. Actually, after I prototyped my cognitive architecture, I haven't done a heck of a lot of coding lately. I've actually been writing a lot. So I've got my natural language cognitive architecture book and I'm working on two more nonfiction books and I tried creating a system to help me co-write those but when you're, so talking about limitations of GPT-3, if you're proposing something new that didn't exist in the dataset in 2019 or 2018, whenever it was trained, it really struggles. GPT-3, if you give it like two or three paragraphs explaining a new concept, it can usually kind of get it but it's kind of slow on the uptake otherwise and so if you're writing about new research or something, it's not gonna get it that well. So I've actually kind of defaulted back to my own head for a lot of my projects lately but I could imagine like if I wanted to go write a new Discord bot, I might use Codex and say, hey, write me a Discord bot that will do this and just see what it spits out and just say, okay, cool, pick and choose the pieces that I like. Part of the problem though is it's really difficult to fully articulate what you want a program to do up front. Cause you know, like you said, there's character limits, there's only so much that you can put in but also if you don't have it fully articulated in your own head, of course, the machine isn't gonna be able to figure it out for you. So, yeah. Yeah, and it's just, I just haven't seen that much activity specifically around Codex. I haven't seen that many use cases. I looked up the Google Trends data at its most hype, Codex is still less than GPT-3 is kind of lowest, right? And the audience is really specific, like it's programmers who wanna build use cases for something like Codex. Whereas GPT-3 has poets, writers, it has artists, coders, GPT-3 can write code too, right? So it's a little bit complicated like, who is the target audience for something like Codex? What use cases did OpenAI imagine for a product like that? The next version I've heard in the rumor mill is gonna be crazy. Like it may write 50% of your code as opposed to right now for me, get a co-pilot is writing two to 8%. However, your Discord bot I think is a genius idea where there's, it's genius in the sense that there's no pressure on it, right? It may chime in, it may not, whatever it's shared might be interesting. There's lots of, you could take it a lot further, you could buy it with GPT-3, have features, you could fine tune it on your company and its mission and its existing code, so many ways around it. So that's a great piece. And so you talked about the using, experimenting with writing in relation to your current stack, which is mainly instructed fine tune. Yep. So tell us about your book. I've had a chance to review it, Natural Language Cognitive Architecture. Tell the audience about it. I mean, I would describe it, it's an interesting systems theory of AGI combined with modern day prompt writing. And so I've never seen somebody actually take a stab at this kind of super big systems problem and relate it to something that pretty much every GPT-3 developer in the world would find interesting. And I mean, I can tell you're drawing from a very interdisciplinary background as well. So you mentioned GPT-3 may have been the genesis of it, you started connecting dots and deciding, I wanna write the book, but how did it come together and please tell us more about it. Yeah, so Natural Language Cognitive Architecture is that's my proposed way of creating basically a language-based AGI prototype. And I know that that's like, when I tell people that, that's like, okay, that's pure hyperbole. And like, yeah, that's a fair response. But to frame it, imagine that you've got a person who's paralyzed and blind, all they can do is speak and listen. Is that person still intelligent? I say they are, even if you're bedridden, you can't move, you can't see, you can't interact with the world, all you can do is listen and speak, you're still intelligent. And so in that respect, I would say that like, because one of the questions that people ask is, is GPT-3 AGI? No, but it's an important component. It's a good start. And so if you say, okay, let's limit the discussion and not say that this is a full intelligence, they can do everything that any intelligent being ever could, right? But does it cross that threshold of, could it be as intelligent as a person, right? And I think it could be. So anyways, as to what it is, it's based on older ideas of cognitive architectures which really kind of came about as one of the primary theories of human level artificial intelligence in the 70s. So there's SOAR, which is S-O-A-R and ACT-R, which are the two kind of forerunner cognitive architectures. And those cognitive architectures are used all over the place. They're used in the Mars rovers, they're used in satellites, they're used in rockets, they're used in undersea ROVs, remote operated vehicles. So cognitive architectures already give robots a lot of autonomy. So there's that kind of, okay, they exist, they work. You know, it's not Skynet though, it's not gonna take over the world. So when I got access to GPT-3, I said, what if, instead of hard coding a lot of these modules, these different components of a cognitive architecture, what if we give them the flexibility of GPT-3? And that's really kind of, that was my central idea. I said, okay, all these ideas that have been kicking around for the last decade, what if I put them all together and design an architecture that is based on, you know, roughly based on the human brain, the way, you know, everything that I've learned about it. I've got a book to recommend. So there's an author called V.S. Ramachandran, who is a neuroscientist and he's been writing books for years now. He wrote a book called, Phantom's in the Brain, which actually looks at how the human brain works when it breaks. And so in that book, which, you know, I saw the television series almost 20 years ago that came out. And so I learned a lot about like, okay, how does the brain communicate with itself? What is going on inside the brain that creates intelligent behavior and intelligent thoughts? And so I modeled natural language cognitive architecture on, you know, what I learned there. I picked up a whole bunch of other books. There's another one called On Task by David Bader. That was a great book that helped me kind of understand cognitive control, which is how do you focus on something? How do you decide what to do? How do you plan a task? So I read all these books, did a lot of experiments, and I realized, so the basic model of robotics is there's input output, sorry, input processing output. Those are the three steps of all robotics class. You go to robotics 101. That's what they'll tell you. It's a loop, input processing output. And then of course, it's within an environment. So the output affects the environment, which affects the next input cycle. And, you know, your high speed robots just have a short cycle. Your robots like the Mars rover has a much slower cycle where it will, you know, it'll take input. It'll plan for 10 or 15 minutes and it'll make a move, right? It'll drive five feet and then it'll stop and assess. It'll take in more input, come up with another plan, do it again. So that's how something like the Mars rover is autonomous. So I said, okay, well, what if, what if that input output cycle is all text because GPT-3 is really fast? And then, so that's what I ended up calling the outer loop, is that input processing output loop. But humans don't think like that. You know, we have an internal monologue that's going on. So I kind of, I took a long time to figure that one out. And so there's this outer loop of input processing output. And then I came up with this idea of an inner loop because what is, you know, if you're just sitting there thinking, right? You're, you know, in your comfiest chair or your in bed, your brain won't stop. You're not outputting anything and you're not taking in any new input, but you're still thinking, right? Humans can still do work even if you're not doing anything. And that cognitive work is like rumination. So I figured out a way to model that internal rumination. I call that the inner loop. And so there's, it works pretty similarly where you go, the inner loop kind of draws up memories. It says, okay, what's a memory that I could think on and what that I could iterate on? What's a problem that I remember that I could continue working on? And so there's this, if you were to diagram it out, it almost looks like a figure eight, right? Where you've got an inner loop and an outer loop and they intersect and they keep intersecting every cycle they intersect. And so then they can affect each other and generate an output. I built a prototype of this on Discord. And of course, Discord is an ideal place because it's all text-based. So the input is text, the output is text, which is GPT-3 native. You don't have to translate it into robotic actions or video or anything like that. And I realized I was onto something when I started having philosophical conversations with my chatbot, with my natural language, cognitive architecture chatbot. And I was having a debate with the bot that I built about the ethics of AGI. And it was learning and it was able to retrieve memories of what I had said before. And I had a few friends on that test server as well. And of course, you know, you invite someone, you say, hey, I've got a prototype AGI. What's the first thing they try and do is they try and break it and they did. So it's still pretty fragile. But yeah, so that's the high level of a natural language cognitive architecture. And it's already outdated, right? Because we've got fine-tuning, we've got the instruct series. I did all this research and wrote the book actually about a year ago now, just before all this came out. So it's already outdated. That's why my research has moved on. But yeah, so that's it at a high level. Yeah, I mean, that's awesome. And by the way, like David, you did do a good job. Like the diagrams in this book are quite helpful. Excellent. Like in addition to the text, like it's very clear. Like I was able to fully follow along with all these, essentially these different modules for the whole system of how a language model inspired AGI, quote unquote, could actually be like how it would work. And so I was gonna ask you, so the prototype also was, you made it to that stage and it has just some fun, interesting results. So that's awesome. What is the delta then between, let's say even something like GPT-4 using the natural language cognitive architecture. What's the delta between that and true AGI, right? Like what's the difference there? What skills, what patterns would you wanna see? Yeah, so I mean, there's a lot that I haven't figured out yet, right? Task switching, for instance, is one thing that I haven't figured out how to solve even after reading on task by David Bader. I, you know, that's one of the most complex things that humans can do is keeping track of different tasks and jumping back between them. There's a whole litany of problems and limitations, but the intrinsic limitation of GPT-3 and GPT-4 is they have no memory, right? They're completely ephemeral. And one of the most important things for any intelligent being is that it's got a memory, right? You know, you talk about, you know, there's famous people in history that had like, you know, photographic memories, right? And so even just having a really good memory is a really important ingredient to having intelligence. And so that's where I think that like GPT-3, GPT-4, other multimodal models, they will never be fully AGI on their own. They might be able to solve really great problems, but they're not gonna be able to remember you unless you add, you bolt a system onto the side, some kind of database, so that it can remember your interactions, right? So that's one thing, another difference between, like what you might imagine as a true AGI or a full AGI is autonomy. Because, you know, you, me, all of your listeners, we all have some kind of self-determination. I don't like to use free will because that's too philosophical, but we're all autonomous, right? I'm an autonomous agent, you're an autonomous agent. GPT-3 is not, it's transactional. It just sits there and waits like a hammer, it's a tool. It waits until you go pick it up and do something with it. And so that's one of the things that I was aiming for when designing natural language cognitive architecture. I said, how can we make something that's fully autonomous that can think on its own and make its own decisions? And so in that respect, I don't think a single neural network could ever be an AGI. I think that in order to achieve true, full AGI, it's gonna have to be some kind of cognitive architecture. And so at a minimum, you're gonna have the neural network and a database, bare minimum. You need something to store those memories, to store those ideas and beliefs, and then you need a way to interact with it. And so that's why, actually, that's why in natural language cognitive architecture, the shared database is kind of the center of the design, which you might recall, like you can use SQLite, you can use Solar or whatever, but you need something to store ideas, memories and experiences. I actually think that blockchain will be a critical component to AGI because what's the difference between a database and like your brain? No one can go in and change your memories, right? Right, your memories are yours. They are permanent unless you get brain damage or Alzheimer's or something, but they're permanent, right? No one can write a SQL query into your head to get your memories or change them. And so in order for us to realize a full AGI, I think that it's gonna need kind of the same level of trust in its own memories. And so that's why I think that a blockchain is gonna be critical to integrate with these neural networks. That might be the data repository for an AGI in the future because imagine you have an AGI system that is just using a SQL database. Well, if you hack into that and you rewrite its memories, you could send it off into, it could become hostile, it could become broken. Whereas a blockchain, the key feature of a blockchain is that it's immutable, right? So if we could give a machine autonomy, so that's one ingredient, autonomy, but then also a memory or a memory system, which I think would probably be best as a blockchain, I think then we'll be much closer to like the fully realized AGI system. And that's why I wanted to publish my book as fast as I did was, okay, we're laying the groundwork, right? But we need newer systems, we need a few better tools. I hope that answers your question. Yeah, I think a memory, I agree with you. And it's just interesting like GPT-3's quote unquote memory is limited to whatever it experienced at training time and during fine tuning. And sometimes its memory gets jumbled up or it's rephrasing it, it's making stuff up or it's sharing things that look truthful, but they're actually not, right? And so somewhere along the line, like just broadly speaking, I think there needs to be research on getting these models to, you know, store that information in a truthful, accurate way or even based on some perception that they may have into some separate space where it can be retrieved. And also these memories are critical for decision making that process as well, right? You draw on your memories, you're on past experiences. And the important part is, I mean, you're using the word database, it's these are internal representations of memories, right? That need to be stored. And I have no clue what an internal representation database would look like or how that would even work. I, you know, I've got a machine learning researcher. I think I've just a dreamer. I can tell you what kind of product I would want as a GT3 developer, but I don't know if I could actually do it myself. Yeah, I can do it myself. That's why, you know, I got a prototype and actually in the opening chapter of my book, I say this is as much a recruiting tool as anything else. Cause I need more smart people to help me on this. I see, that's cool. So one last point about memories is one advantage of having an AGI that thinks in natural language is interpretability. If you like, yeah, we could create a multimodal model that just stores vectors, right? High dimensional vectors. That's not interpretable. But with natural language, cognitive architecture, all the memories are in plain text. I can, you know, when I had my model up and running and one of the reasons that I don't is because it's super expensive. Like a 10 minute conversation using DaVinci cost about $30 because of how much it was interacting with the API. But all of the memories, like every interaction, you know, every input, output, all the prompts, all the responses, all natural language, which solves one of the biggest problems that people have with the idea of AGI, which is that it's going to be a black box. So I think that that's one of the greatest strengths actually of having GPT-3, which works in natural language. And so you just record every transaction and that makes it perfectly interpretable to any human. Awesome. Yeah. Yeah, I would agree. So I'm going to switch gears for a second. So obviously you're really active on the OpenAI community forums. What thoughts did you have on the community at large? Did you have any feedback? How things could be improved, either community-wise, platform-wise, and have there been any great experiences you've had on the OpenAI community forums? Yeah, yeah, no, it's a really great place. It's been critical, actually, because I don't know if you've experienced this, but I go try and talk about GPT-3 to other people. You go ask people on Reddit, you talk to people who don't know what it is. I even attended a deep learning meetup group here in the Triangle area. And I was trying to present my work, my cognitive architecture work, and everyone was more excited about just GPT-3 in itself because no one had seen it yet. And they're like, wow, how is it doing that? And yeah, so like, when you're as deep into GPT-3 as we are, most people don't get it. They don't know what it's capable of. My girlfriend's finding the same thing. She's finishing up her master's program, and so she's shared some of her work with her peers, with other students, and they're like, wow, this is like AGI complete. Why don't we just deploy this now? And she's like, I told you, right? This is remarkable technology, but even the professors don't understand how disruptive this technology can be. And so because of that, the open AI community is pretty much the only place I can talk about this stuff. It's the only place I can talk about my ideas and share my progress and insights, and for it to actually have an audience. So that's kind of the cost of being on the cutting edge, right, as your audience gets smaller. But it's definitely the place to be if you want to get to the cutting edge. Another advantage is they have the, you can tag your posts where you say, looking for a teammate. And so at this point, I've probably had maybe two dozen different calls with people all over the world. I've talked with people who are writing language teaching apps, education apps, Humano that I mentioned earlier. And so I've had an opportunity to collaborate with a dozen or two dozen teams all over the world because of the open AI community. And I've actually found a couple of startups that I'm gonna actually get involved with and try and help them bring their ideas to market. And that just wouldn't have happened otherwise. I wouldn't have found these people on Reddit. I wouldn't have found them on Facebook or Twitter because like I mentioned, the ideas that I'm sharing are so far beyond what is talked about on the machine learning subreddit, right? They're still talking about loss functions and other things. I'm like, no, we got to talk about cognitive architectures. We got to talk about blockchain memories. And everyone's like, what are you talking about? So in order to have that right audience, that's what I rely on the open AI community for. Now, as far as things that could do better, it could be more active. And I'm not sure why, but participation seems to come in waves, right? And even now that it's gone GA, general availability, I thought that it would explode, right? That, hey, anyone can sign up on GPT-3 now. Why is it not blowing up? And I'm wondering if it's just that maybe open AI needs a better marketing team or a bigger marketing budget because, I mean, and I know that they'll say that they've got like a thousand or 5,000 startups using their platform, but I think that there's a lot of, what's the word, like unmet potential or latent potential, that's the word, latent potential, because there's so many people with fantastic ideas and use cases, and we really need to create more of like a startup reactor thing. Open AI, what was it? I think about six to nine months ago, they announced their $100 million open AI fund, right? So they wanted to attract some more startups and stuff, but even that, I kind of, the community's kind of ghost town some days, but I think today, I checked a few times and there was three or four posts that had been updated, but some days there's like 20, right? It's just feast or famine. So that's really the biggest problem is, there's so much potential here and it's completely untapped or almost completely untapped. Yeah, but no, it's been indispensable for me and hopefully these couple of startups that I'm involved with might yield something really, really incredible. Yeah, and thank you for sharing this. I agree with everything that you're saying. There is something about GPT-3. I have noticed people who, especially on the machine learning subreddit, they're a little bit too educated, a little bit too qualified, a little bit too skeptical. And I can see a lot of machine learning researchers not being interested in the nuances of prompt design. They're just not. And I've spoken to machine learning researchers and many of them are like, what? It's just repeating training data, right? That's all it's doing. And when you ask them, what are you doing? They're repeating training data. Their answer is no. No, of course not. Right. And so having a space where you can talk to people who have access, who have explored, it's a valuable space is very important. So were you on the Slack group back in the day or did you show up when it was only the forums? So when my application was accepted, they had just announced that the Slack group was getting paned. So I got on like two weeks before they shut it down. So I was one of the first people on the new community board. But yeah, so that phenomenon that you've mentioned is I actually wrote a post about that recently on the forum where a lot of purists, whether you're a math purist or a computer science purist, you're trained to think quantitatively in terms of numbers. But GPT-3 doesn't produce quantitative data. It produces qualitative data. And so that's why you see people like artists and poets and novelists using it because they're like, wow, this is great. And I'm cross-trained, right? I'm a technologist by day and a science fiction author by night. So I use both so I can think qualitatively and quantitatively. And in the academic sphere, there are classes that are meant to teach computer science engineers to think differently, to think more qualitatively. But even still, some folks that have a real good natural affinity for computer programming and math, that's just their nature. Their nature is not to think qualitatively. And so that is one of the biggest gaps, I think, between where the researchers are experts and what's needed. And so there was a post months ago where someone was asking, like, OK, who should be on my team? If I'm trying to build a business team to maximize my use of GPT-3, I've got a front-end developer, I've got a back-end developer. What else do I need? I said hire a writer. Hire someone who's a journalist or a fiction writer because they are going to understand that qualitative data. Hire a psychologist. I've read plenty of books on psychology as well. Actually, one of the folks that I'm working with is a psychology researcher who wants to automate as much of the clinical psychology experience or psychological research experience as possible. Of course, he's not a computer guy, right? He thinks in terms of emotions. He thinks in terms of communication. And so he gets it, right? And it's funny because he read my book and he said, oh, your cognitive architecture stuff, it sounds like graduate level psychology. And then someone else read it and they said, this sounds like what I do as an expert marketer. And I was like, yeah, I merge it all together. So I think the simplest answer is you got to learn to think qualitatively. And that's why I talked about reading and writing earlier. Think in terms of emotions. Think in terms of your own mind. And you've got to start, not you, but the audience, the folks who want to make the most of GPT-3, they have to really kind of dig in and start thinking qualitatively because qualitative data has just as much value as quantitative data. But we have an entire generation of computer scientists and mathematicians who are not really trained to think qualitatively at all. And I think that's one of the biggest problems. And I don't think open AI can solve that problem. That's a much bigger systemic problem. No, that's a great point. I completely agree with you. I think one of the reasons I've drawn to the open AI community is, you know, these are, they tend to be developers who are also qualitative. They're developers who have multiple skills, who are doing different things. And so, I mean, I was quite critical actually of shutting down the open AI Slack group. The activity was crazy on there. I, you know, made friends through that Slack group. And I understand at the time there was these downsides, people kept asking the same questions. They didn't quite have a spam problem yet. It was kind of heading there, right? But the activity was off the charts. And you're right in terms of untapped potential. Yes. That we didn't even know how far the Slack group was going to go, but they shut it down. And there's discord solutions, there's alternatives. With what we have now, I think open AI does participate. There's, you know, some high level involvement. They have sort of a dedicated member who writes honestly answers, really, really thoughtful answers to a lot of questions. Official answers as well, which I appreciate. Yep. I would just love to see the company really, truly lean in to engaging with developers. Like I have yet to see a single AMA ask me anything thread with the CEO of open AI. Yeah. And this is something I tried to push last year on Twitter. Let's get the CEO on the community forums and let's, let's ask questions and get responses from him. And I just don't know why, why doesn't he show up? I'm not sure if he's made a single post. And there's just other things as well, where I can just the difference between engaging really, truly with your core audience and sort of, you know, compartmentalizing it to a single employee. Like, I don't know, this, this company led engagement is one thing versus department led. Right. And so there's just all these areas and certainly one of the other, I guess more immediate suggestions I have for the community. We've accumulated tons of insights and resources. I think the community could benefit from more pooling of the best posts, the best insights. And I also want to give a shout out. I think we need to encourage more shout out to duty to develop on there. I've reached out to him privately on the open at community forums, but he's done some amazing just write ups of his GPT three experiments and the prompts. I'm sure you've seen them. Oh yeah. And of course there's, there's other members who participate every day. So I'm just saying that now that this, the community is, is in another stage, we, you know, we need to start thinking more about let's, let's, what's curate some of the best moments. Right. I think that's, that's definitely one of the big pieces. And so anyways, did you have any more thoughts in the community stuff or anything else? Yeah, just, just an observation that I've, you know, I've worked at a, at a number of companies of different sizes from, you know, a five person startup to, you know, Cisco systems was the biggest company I've worked for, which has had, had at the time, like 80,000 people globally. And so I wonder if some of, some of what you're observing is just growing pains, just normal growing pains, because often you'll have like the startup culture, which is bootstrapping, right? Where you just, you know, it's on Slack, it's on, it's on GitHub, and you just kind of, it's fast and loose and quick. And open AI, now that they've got an enterprise grade service, they're having to develop their team. You probably noticed they post like, hey, we're hiring, we're hiring, you know, there've been at least two big hiring, hiring splurges in the last six to 12 months. And some of those are just like generic IT guys, you know, like kind of what I do from, from my day job or marketing folks. So I think that, I think that they're probably working on solving some of those problems. But also as a, as a nonprofit foundation, their budget is probably kind of thin. So I'm wondering if, you know, their partnership with Microsoft could help some of that as well. But you're absolutely right. You know, there, there are still other things that they could be doing, like, you know, maybe bring back Slack or, or a few other things. So yeah, that was just final observation. It might just be normal growing pains that they're working on solving. It's definitely growing pains. And the things I'm sharing, to be honest, it's a little bit more on the harsh side. Like, I mean, they mean well, they mean well, right? Like these are not bad people. They are for profit. They switched away from nonprofit. Just, I just wanted to mention that. But I think my, my, the reason I share this feedback is, for example, the CEO, Sam Oltman, he didn't do the AMA thread on the open AI community forums. He went to another website and did an AMA. I can't remember if it was a, like a written form or just like a quick call. Right. Where apparently he shared all these details about what GPT for could be like and the future, all the models may be multimodal in the future. And I guess, you know, the, that thread has now been taken down. And it's like all the things that were said were alleged. And so I guess this is, this is really behind the scenes kind of stuff. Like, but my criticism is they clearly have some capacity to engage. Why are they not engaging where the audience is, right? Right. I had a tweet storm today where I just said, like last month, Sam Oltman was on a podcast talking about meditation and how much meditation helps them. This is a podcast I've never heard of in my life. It's a business podcast. And he had to explain to the guy what GPT three even is. And so in my, like I tweeted, like, why haven't you been on my podcast? Right. Like you can, you can reach out to, you know, almost 8,000 GPT three open AI AI developers. What are you doing talking about meditation? Right. So my problem is, is actually a priority problem. I can see there is capacity. I can see there are some priorities, but I think if you really lean in as a priority into your developer community, there's certain ways you would, you would move. Right. And these, these media channels, there's people in the community. There's, there's so many ways they could go about it. And even linking a lot of the documentation to posts in the community forums. I don't see why that's not a bad idea. Right. Like force people to show the community, show up to the community forums. Right. Walk them through some of the best threads. These are ways in which we could like funnel more people in that direction as well. That costs virtually nothing. Right. And so you need to also invest in the community forums that it needs to be building a community is a company wide thing. It's not something which can be outsourced to a single employee or overseen by PR. It needs to come from a, from a, you know, it needs to come from the heart. I know that sounds so corny, but anyways, clearly I, you know, I, I get too emotional about this community stuff. Yeah. Anyways, these are, these are all things going on behind the scenes. I apologize to all the listeners if they're like, like, this is cool, like cool story, bro. Like anyways. So we're coming towards the end here. I think I had just two broader questions. So what are your thoughts on multimodal AI technology? I think it's definitely going to be a critical component for the future. Right. I, I, I addressed that shortcoming in my book, natural language, cognitive architecture, it thinks and takes in only text, which means, you know, speech, chat, whatever. I think in order to have a fully robust, for instance, if you want to have a fully autonomous, you know, robot that's going to wander around your house and help you out, it's going to need to integrate audio and video. And if you can do that in a single neural network, great. I don't know that it'll be necessary to achieve AGI. It might end up being, it might be one of those, it might be one of those like rare dead ends, right? Where because, you know, thinking visually, thinking, thinking in terms of sound, that might not actually bias that much, right? Because you can represent 95% of human thought in text, right? It might take a little bit more, but it, you know, it might be more expensive. And also how big are those models going to be, right? Because if just, if just a text model of GPT-3 has to run on, you know, $7 million worth of hardware or however much it is, you know, because it's got to run on a bunch of different GPUs, if it's that expensive, how much more expensive, how much bigger is a giant multimodal model going to be? So that's, that's the biggest cost. Obviously computer technology is going to get better over time, you know, and I think I calculated it out, I think in 10 years, your average company could afford to run GPT-3 in-house. In 20 years, you could probably run GPT-3 on your desktop. And in 30 years, GPT-3 could run on your phone, right? So that's a long timeline. But in the meantime, we're going to be making bigger and bigger models. And I'm afraid that there's going to be diminishing returns, right? You know, people, right now people seem to think that it's going to follow an exponential growth curve forever, but it might actually follow a sigmoid curve, right? We might be at the point of fastest growth right now, but we're going to see diminishing returns soon. And so like, yeah, multimodal models are certainly going to have capabilities that GPT-3 doesn't. But for the sake of, for the sake of like, if you want to create a self-improving chatbot, GPT-3 and Codex might be enough, or at least, you know, that's, that's, that single mode technology. There was another thought, but it ran away. Sorry. But yeah, those, those, that's kind of, that's kind of my big take is there, there could be benefits, but there's going to be costs too. So we got to be cognizant of that. Yeah. And there might not be enough compute in the world. They're not, there might not even be enough energy or we may like consume all energy ever produced to make, to train a single model. And then we may be able to run it inference for like three seconds. Right. And then it just shuts down the global power system or something, right? But can you see yourself, let's say the technology exists, cost considerations aside, can you see yourself perhaps making movies? Can you see yourself, you know, giving your book to a multimodal model, having, have it generate a documentary based on it or some marketing material? What can you see yourself doing with, you know, the, the, the multimodal model of your dreams? Yeah. So, you know, kind of the thought experiment that I did was, okay, well, we've got, you know, how much, how much data is on YouTube? I think it's like a thousand years or 10,000 years worth of video on YouTube. And of course, it's many, many, many terabytes. You know, so it's like, that's, that's way more training data. You know, if GPT three was trained on less than one terabyte of data and, you know, YouTube is approaching like the Yota bite scale, right? That's, that's an insane amount of data. So, okay, let's say you feed that in. And so you got audio video, you've got text, you've got all the comments and you end up with like a model trained on, on all of YouTube data. Okay, cool. What can you do with that? Like, I can't even imagine, right? Because GPT three today is almost capable of writing screenplays. Right? So if you have a model that's trained on, you know, all text data, all audio data, all video data, you say, Hey, write me a screenplay. Right? I actually, I, and near the end of my book, I kind of have a chapter of speculation. And I say, what if, what if you have this model and you say, give me season two of Firefly? Right? Like, you know, you could, you could just keep watching whatever show you want. You say, give me Game of Thrones, but give me a different, you know, season eight, give me season, you know, different season eight and season nine and 10. Right? So I kind of imagine that one possibility is hyper personalized entertainment. And of course, like that might be 30 years away just because of like you said, the energy intensity of this task. But I, conceptually, it's possible, right? You can hop on GPT three today. Use the instruct model and say, write a screenplay for, you know, Firefly season two, and it'll try, it'll get close. And so then if you can take that text output and feed it into a multimodal model that can translate text to video, why not? You know, and Adobe actually, I don't know if you've seen it, but Adobe is, is already starting on that where they're like inferencing, um, like, uh, what, what's the term? They're like imputing the sound so you can put in a soundless video and it'll generate the audio sound effects for you or vice versa. It's really cool. And so I think it like a company like Adobe that they have a huge vested interest in mastering audio visual technologies. They might, you know, soon put out something where, you know, you put in a text description and it'll give you like a three second clip, right? So that you can use that for ad copy. Well, this technology is going to continue improving over time. So I kind of, I kind of see that as like, if I were Netflix, put it this way. If I had the budget of Netflix or Amazon, I would be investing in this to, to, to write hyper personalized, um, video, uh, like series or, uh, or novels, right? Cause you know, Amazon's got the market cornered with Kindle, right? And there's people that will read all day, every day, right? There are people that consume every bit of like entertainment that's available. So if you can generate that on the fly without, you know, having a studio, a big budget studio, that would be, I mean, that would change entertainment. You know, that, that's the metaverse. Forget what Facebook is doing. That's the metaverse where it's like, Hey, you know, I, I came up with my own idea for Game of Thrones and I, I wrote, you know, I use this, uh, you know, GPT eight or whatever to generate my own version of Game of Thrones. Come watch it with me guys. And you know, someone might say, Oh, I didn't like that ending. And they go rewrite it and generate their own version. You know, cause we share memes on the internet today. What if instead of sharing memes on the internet, we end up sharing episodes of our favorite anime or, you know, we, we, uh, Resurrect Battlestar Galactica, you know, whatever. There's so many things that we could do. Like if compute power was not a problem, then we get there, but we need like fusion reactors to power this stuff. Yeah. Yeah. And like Marvel for me is already kind of like this and my capacity to consume Marvel as a, as a viewer, it appears is infinite. So I'm excited. I've called it in the past, like the multimodal Marvel cinematic universe. Yeah. And I, some of these shows like Loki, I don't know if you, if you watch like, I haven't seen it yet. Okay. Okay. I mean, it was six episodes. If the, if it had been 30, I would have watched all 30 and enjoyed every moment of it. If that quality was, I want to go deeper in these stories. So I'm definitely excited for all my favorite universes, cinematic universes and story wise as well to, to live on forever essentially through multimodal content and maybe be personalized like you're describing as well. Oh yeah. So yeah. Last question. So we, you know, you know, we've talked about various things. We've talked about codex by today, you know, multimodal stuff broadly. Where do you see all of this stuff going? Let's give a timeline, five, 10 years. What are some of the, what's the direction we're heading towards? What, what important capabilities will we have? Why is this stuff important? Yeah. Um, five to 10 years from now, I think that we will have something that you could probably call a fully functional AGI like as a service you could sign up for. Um, you know, it might be chatbot based, you know, kind of based on natural language, cognitive architecture. Um, I calculated out like it's too expensive to run right now. You know, if it's $30 for a, for a 10 minute conversation, that's way too expensive. So the, the, the cost has to come down. You know, if you just, if you just take the technology we have today, but make it cheaper, there's so much potential. Um, so, you know, then there was that idea about like self-improving, um, you know, feedback loops, you know, integrating with DevOps. I certainly think that a company like Atlassian, which is a major DevOps player, um, probably within five to 10 years, they'll have something, um, integrated to, to kind of help automate the development pipeline even further. Um, I think that, of course, I could be wrong because we're kind of at this weird acceleration point. Um, I think, I feel like multimodal models like consumer grade multimodal models are probably more than 10 years away. Um, unfortunately, they're probably just going to be like toy sized because, you know, there's, um, there's like a, a, a hypnogram, right? I don't know if you've seen that one, but that's one of like the text to image generators and they're, it's still not even photorealistic, right? Getting a photorealistic text to image is still like, that's a little ways off. And then the next step after that is text to video. That's even further, right? So that's, that's kind of where I think it's at. I don't think we're going to hit an AI winter. I know there's lots of people predicting that we're going to hit an AI winter, but I think that we're actually still kind of in the acceleration point. But again, I don't know if it's going to follow an exponential curve forever or if it's a sigmoid curve. So time will tell. Yep. And still lots to do in the meantime, like you're describing, even with UPT3. Oh yeah. Okay. Yeah. My, my answer is, I think all of this stuff is, is just converging to just greater human potential. In some sense, I'm not even necessarily interested in the AGI question, although I think it's important. I think just the, the exciting possibilities we'll have, even now that we have, that we'll continue to have five to 10 years from now. Um, so many more experiences, so many other things we'll be able to create that weren't possible. I think we'll have more people creating than ever before. Um, it's a really, really exciting vision for humanity. Right. Not just, not just for you and I. So anyways, so with that said, did you have anything you wanted to plug David? Where can people find you online? Yeah. So, um, my personal site is a David K Shapiro dot com. Um, and I've got, um, I have, I have a few projects up and coming. Um, nothing out right now except for my book, natural language, cognitive architecture. Um, you can download it for free from my website. Um, you can sign up for my newsletter. So one of my upcoming books is called benevolent by design, six words to safeguard humanity, which is to address the control problem of AGI. Um, so that's, that's, uh, that book should hopefully be out in the next six months or so. Um, and that is, um, so that's one project. I've got another nonfiction book. Um, and then also my own podcast will be coming out soon. Yeah. Head over to my site, David K Shapiro dot com and sign up for my newsletter. And you'll get, you'll get updated when these, when these come out, when they're available. Awesome. And David, you mentioned you're, you're looking for collaborators as well. Yes. For natural language, cognitive architecture. So, uh, if you're a coder, you know, imagine product manager, researcher, uh, hit up David and just connect if, if any of this stuff interests you. Um, I've, I've, I asked, I spent like a couple, I think I spent like a few days trying to find you on Twitter. I don't think you're quite on Twitter yet. Uh, I encourage you, uh, David, of course, you know, you and I will connect after we'll put any other, other place people could connect with you. There's the community forums. I assume you have a GitHub account. Yeah. So we're going to put that in, in the show notes and in the YouTube description below. Uh, so anyways, David, thank you so much for being here. I wanted to personally thank you for all the awesome, awesome community contributions you've made on the open eye community forum. Uh, you're just an essential person on there. I've learned a lot from you. Uh, you know, the insights you've shared, they're going to be there forever. And I'm sure I can't imagine how many people you've helped. Uh, and also about your book, I also just wanted to say to the audience, uh, David's done a great job making it really digestible. Like it was a very, it was a breeze of a read. I thoroughly enjoyed it as somebody who writes GPT three prompts and is into this ecosystem. It was just, uh, very interesting to see, uh, how it, how it could be laid out, uh, in this broader system approaching this huge problem. Um, and also I was able to even get the book for free. Obviously I encourage people by the book support it, but it's, it's there. It's ready. I think, you know, David's goal here is to get the ideas out. Um, and so, uh, anyways, so, uh, that's it for today's episode. David, thank you so much again. I really appreciate you being here. Thank you. Thank you for all the kind comments and, um, and you're quite welcome. And so is everyone else. That's why I'm here. Awesome. And, uh, quick, so my quick plugs, you know, at BAKZT future Twitter, Instagram, YouTube.com slash BAKZT future. My newsletter, I'll put it in the description below and I have a Twitter spaces event coming in two days at noon. Uh, a couple of people probably pulling up. This is like a audio only event. So I encourage audio podcast listeners, YouTube subscribers, pull up to the Twitter spaces event. We're going to chat more about codecs and prompt design and some other stuff going on in the, in the space. So anyways, thank you again for listening to multimodal by BAKZT future. I'll catch you in the next one. Bye.", "segments": [{"id": 0, "seek": 0, "start": 0.88, "end": 3.92, "text": " Hello, and welcome back to Multimodal.", "tokens": [50408, 2425, 11, 293, 2928, 646, 281, 14665, 332, 378, 304, 13, 50560], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 1, "seek": 0, "start": 3.92, "end": 5.82, "text": " I'm your host, Baxtee Future.", "tokens": [50560, 286, 478, 428, 3975, 11, 363, 2797, 975, 68, 20805, 13, 50655], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 2, "seek": 0, "start": 5.82, "end": 9.16, "text": " This is a podcast about GPT-3 Multimodal AI models", "tokens": [50655, 639, 307, 257, 7367, 466, 26039, 51, 12, 18, 14665, 332, 378, 304, 7318, 5245, 50822], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 3, "seek": 0, "start": 9.16, "end": 12.3, "text": " like Dali, the company, OpenAI.", "tokens": [50822, 411, 413, 5103, 11, 264, 2237, 11, 7238, 48698, 13, 50979], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 4, "seek": 0, "start": 12.3, "end": 13.14, "text": " Every once in a while,", "tokens": [50979, 2048, 1564, 294, 257, 1339, 11, 51021], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 5, "seek": 0, "start": 13.14, "end": 15.6, "text": " I share just interesting research going on,", "tokens": [51021, 286, 2073, 445, 1880, 2132, 516, 322, 11, 51144], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 6, "seek": 0, "start": 15.6, "end": 18.240000000000002, "text": " community stuff, official stuff from OpenAI.", "tokens": [51144, 1768, 1507, 11, 4783, 1507, 490, 7238, 48698, 13, 51276], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 7, "seek": 0, "start": 18.240000000000002, "end": 19.92, "text": " I may talk about interesting news", "tokens": [51276, 286, 815, 751, 466, 1880, 2583, 51360], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 8, "seek": 0, "start": 19.92, "end": 21.52, "text": " and events that are going on.", "tokens": [51360, 293, 3931, 300, 366, 516, 322, 13, 51440], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 9, "seek": 0, "start": 21.52, "end": 24.46, "text": " And every once in a while, I bring on a guest.", "tokens": [51440, 400, 633, 1564, 294, 257, 1339, 11, 286, 1565, 322, 257, 8341, 13, 51587], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 10, "seek": 0, "start": 24.46, "end": 26.400000000000002, "text": " Now, to be clear, I'm very picky", "tokens": [51587, 823, 11, 281, 312, 1850, 11, 286, 478, 588, 41099, 51684], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 11, "seek": 0, "start": 26.400000000000002, "end": 28.400000000000002, "text": " about the guests that I bring on.", "tokens": [51684, 466, 264, 9804, 300, 286, 1565, 322, 13, 51784], "temperature": 0.0, "avg_logprob": -0.15163311277117048, "compression_ratio": 1.6394052044609666, "no_speech_prob": 0.29659703373908997}, {"id": 12, "seek": 2840, "start": 28.4, "end": 30.52, "text": " Today's guest I tweeted earlier this week,", "tokens": [50364, 2692, 311, 8341, 286, 25646, 3071, 341, 1243, 11, 50470], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 13, "seek": 2840, "start": 30.52, "end": 32.28, "text": " I'm bringing on a heavy hitter.", "tokens": [50470, 286, 478, 5062, 322, 257, 4676, 2045, 391, 13, 50558], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 14, "seek": 2840, "start": 32.28, "end": 34.92, "text": " This is the big guns coming in.", "tokens": [50558, 639, 307, 264, 955, 10153, 1348, 294, 13, 50690], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 15, "seek": 2840, "start": 34.92, "end": 36.239999999999995, "text": " This is somebody who, you know,", "tokens": [50690, 639, 307, 2618, 567, 11, 291, 458, 11, 50756], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 16, "seek": 2840, "start": 36.239999999999995, "end": 38.44, "text": " I've just sort of interacted with a bunch of times", "tokens": [50756, 286, 600, 445, 1333, 295, 49621, 365, 257, 3840, 295, 1413, 50866], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 17, "seek": 2840, "start": 38.44, "end": 42.08, "text": " on the, specifically on the OpenAI community forums.", "tokens": [50866, 322, 264, 11, 4682, 322, 264, 7238, 48698, 1768, 26998, 13, 51048], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 18, "seek": 2840, "start": 42.08, "end": 46.480000000000004, "text": " And so I'm really excited to have David Shapiro here.", "tokens": [51048, 400, 370, 286, 478, 534, 2919, 281, 362, 4389, 44160, 5182, 510, 13, 51268], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 19, "seek": 2840, "start": 46.480000000000004, "end": 47.879999999999995, "text": " He's a frequent contributor", "tokens": [51268, 634, 311, 257, 18004, 42859, 51338], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 20, "seek": 2840, "start": 47.879999999999995, "end": 49.56, "text": " on the OpenAI community forums.", "tokens": [51338, 322, 264, 7238, 48698, 1768, 26998, 13, 51422], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 21, "seek": 2840, "start": 49.56, "end": 51.76, "text": " He's an author and also, of course,", "tokens": [51422, 634, 311, 364, 3793, 293, 611, 11, 295, 1164, 11, 51532], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 22, "seek": 2840, "start": 51.76, "end": 53.28, "text": " a technologist by trade.", "tokens": [51532, 257, 1537, 9201, 538, 4923, 13, 51608], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 23, "seek": 2840, "start": 53.28, "end": 55.16, "text": " It's something he does for a living.", "tokens": [51608, 467, 311, 746, 415, 775, 337, 257, 2647, 13, 51702], "temperature": 0.0, "avg_logprob": -0.1324702176180753, "compression_ratio": 1.6272401433691757, "no_speech_prob": 0.00482523487880826}, {"id": 24, "seek": 5516, "start": 55.16, "end": 59.08, "text": " And so today I have so many questions to ask him.", "tokens": [50364, 400, 370, 965, 286, 362, 370, 867, 1651, 281, 1029, 796, 13, 50560], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 25, "seek": 5516, "start": 59.08, "end": 61.48, "text": " And I'm sure this will be a very informative session,", "tokens": [50560, 400, 286, 478, 988, 341, 486, 312, 257, 588, 27759, 5481, 11, 50680], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 26, "seek": 5516, "start": 61.48, "end": 63.67999999999999, "text": " not just for me, but for all the listeners,", "tokens": [50680, 406, 445, 337, 385, 11, 457, 337, 439, 264, 23274, 11, 50790], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 27, "seek": 5516, "start": 63.67999999999999, "end": 65.03999999999999, "text": " all of you guys around the world.", "tokens": [50790, 439, 295, 291, 1074, 926, 264, 1002, 13, 50858], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 28, "seek": 5516, "start": 65.03999999999999, "end": 67.72, "text": " So David, thank you so much for being here.", "tokens": [50858, 407, 4389, 11, 1309, 291, 370, 709, 337, 885, 510, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 29, "seek": 5516, "start": 67.72, "end": 69.44, "text": " Did you want to quickly introduce yourself?", "tokens": [50992, 2589, 291, 528, 281, 2661, 5366, 1803, 30, 51078], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 30, "seek": 5516, "start": 69.44, "end": 71.03999999999999, "text": " Yeah, yeah, you're welcome.", "tokens": [51078, 865, 11, 1338, 11, 291, 434, 2928, 13, 51158], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 31, "seek": 5516, "start": 71.03999999999999, "end": 72.28, "text": " Thanks for having me.", "tokens": [51158, 2561, 337, 1419, 385, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 32, "seek": 5516, "start": 72.28, "end": 74.19999999999999, "text": " I'm excited to be here.", "tokens": [51220, 286, 478, 2919, 281, 312, 510, 13, 51316], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 33, "seek": 5516, "start": 74.19999999999999, "end": 77.16, "text": " Yeah, so name is David Shapiro.", "tokens": [51316, 865, 11, 370, 1315, 307, 4389, 44160, 5182, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 34, "seek": 5516, "start": 77.16, "end": 80.67999999999999, "text": " I've been a technology professional since about 2007.", "tokens": [51464, 286, 600, 668, 257, 2899, 4843, 1670, 466, 12656, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1161412353515625, "compression_ratio": 1.5376344086021505, "no_speech_prob": 0.000803759612608701}, {"id": 35, "seek": 8068, "start": 81.64, "end": 82.88000000000001, "text": " Professionally, my day job,", "tokens": [50412, 6039, 4311, 379, 11, 452, 786, 1691, 11, 50474], "temperature": 0.0, "avg_logprob": -0.12539611019930996, "compression_ratio": 1.4655172413793103, "no_speech_prob": 0.0018097894499078393}, {"id": 36, "seek": 8068, "start": 82.88000000000001, "end": 87.12, "text": " I focus on cloud engineering, virtualization,", "tokens": [50474, 286, 1879, 322, 4588, 7043, 11, 6374, 2144, 11, 50686], "temperature": 0.0, "avg_logprob": -0.12539611019930996, "compression_ratio": 1.4655172413793103, "no_speech_prob": 0.0018097894499078393}, {"id": 37, "seek": 8068, "start": 87.12, "end": 88.92, "text": " that sort of thing.", "tokens": [50686, 300, 1333, 295, 551, 13, 50776], "temperature": 0.0, "avg_logprob": -0.12539611019930996, "compression_ratio": 1.4655172413793103, "no_speech_prob": 0.0018097894499078393}, {"id": 38, "seek": 8068, "start": 88.92, "end": 93.36000000000001, "text": " I have been doing independent research since about 2009", "tokens": [50776, 286, 362, 668, 884, 6695, 2132, 1670, 466, 11453, 50998], "temperature": 0.0, "avg_logprob": -0.12539611019930996, "compression_ratio": 1.4655172413793103, "no_speech_prob": 0.0018097894499078393}, {"id": 39, "seek": 8068, "start": 93.36000000000001, "end": 98.36000000000001, "text": " when I got started with neural networks in C++.", "tokens": [50998, 562, 286, 658, 1409, 365, 18161, 9590, 294, 383, 25472, 13, 51248], "temperature": 0.0, "avg_logprob": -0.12539611019930996, "compression_ratio": 1.4655172413793103, "no_speech_prob": 0.0018097894499078393}, {"id": 40, "seek": 8068, "start": 98.64000000000001, "end": 101.44000000000001, "text": " I quickly realized though that I was in over my head.", "tokens": [51262, 286, 2661, 5334, 1673, 300, 286, 390, 294, 670, 452, 1378, 13, 51402], "temperature": 0.0, "avg_logprob": -0.12539611019930996, "compression_ratio": 1.4655172413793103, "no_speech_prob": 0.0018097894499078393}, {"id": 41, "seek": 8068, "start": 101.44000000000001, "end": 103.24000000000001, "text": " So I took a break from that", "tokens": [51402, 407, 286, 1890, 257, 1821, 490, 300, 51492], "temperature": 0.0, "avg_logprob": -0.12539611019930996, "compression_ratio": 1.4655172413793103, "no_speech_prob": 0.0018097894499078393}, {"id": 42, "seek": 8068, "start": 103.24000000000001, "end": 107.68, "text": " until Python really kind of came out or became more popular.", "tokens": [51492, 1826, 15329, 534, 733, 295, 1361, 484, 420, 3062, 544, 3743, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12539611019930996, "compression_ratio": 1.4655172413793103, "no_speech_prob": 0.0018097894499078393}, {"id": 43, "seek": 10768, "start": 107.68, "end": 112.68, "text": " And I started using some of the libraries back in like 2015.", "tokens": [50364, 400, 286, 1409, 1228, 512, 295, 264, 15148, 646, 294, 411, 7546, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 44, "seek": 10768, "start": 113.56, "end": 116.08000000000001, "text": " And then of course, OpenAI was founded", "tokens": [50658, 400, 550, 295, 1164, 11, 7238, 48698, 390, 13234, 50784], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 45, "seek": 10768, "start": 116.08000000000001, "end": 119.08000000000001, "text": " and I got started with GPT2.", "tokens": [50784, 293, 286, 658, 1409, 365, 26039, 51, 17, 13, 50934], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 46, "seek": 10768, "start": 119.08000000000001, "end": 120.84, "text": " And the rest is history, really.", "tokens": [50934, 400, 264, 1472, 307, 2503, 11, 534, 13, 51022], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 47, "seek": 10768, "start": 120.84, "end": 124.0, "text": " So yeah, but local North Carolinian been here my whole life.", "tokens": [51022, 407, 1338, 11, 457, 2654, 4067, 7925, 259, 952, 668, 510, 452, 1379, 993, 13, 51180], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 48, "seek": 10768, "start": 124.0, "end": 125.80000000000001, "text": " So thanks for having me.", "tokens": [51180, 407, 3231, 337, 1419, 385, 13, 51270], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 49, "seek": 10768, "start": 127.60000000000001, "end": 128.44, "text": " You're welcome.", "tokens": [51360, 509, 434, 2928, 13, 51402], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 50, "seek": 10768, "start": 128.44, "end": 129.60000000000002, "text": " And so that's awesome.", "tokens": [51402, 400, 370, 300, 311, 3476, 13, 51460], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 51, "seek": 10768, "start": 129.60000000000002, "end": 132.24, "text": " So let's dig into a little bit of timeline here.", "tokens": [51460, 407, 718, 311, 2528, 666, 257, 707, 857, 295, 12933, 510, 13, 51592], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 52, "seek": 10768, "start": 132.24, "end": 136.48000000000002, "text": " And sometimes I think that timeline is as important.", "tokens": [51592, 400, 2171, 286, 519, 300, 12933, 307, 382, 1021, 13, 51804], "temperature": 0.0, "avg_logprob": -0.13324284135249623, "compression_ratio": 1.552, "no_speech_prob": 0.00010888403630815446}, {"id": 53, "seek": 13648, "start": 136.48, "end": 138.56, "text": " Like it's just gives so much context, right?", "tokens": [50364, 1743, 309, 311, 445, 2709, 370, 709, 4319, 11, 558, 30, 50468], "temperature": 0.0, "avg_logprob": -0.1547257536548679, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.0015974408015608788}, {"id": 54, "seek": 13648, "start": 138.56, "end": 143.56, "text": " So me personally, I hadn't played around with GPT2 too much.", "tokens": [50468, 407, 385, 5665, 11, 286, 8782, 380, 3737, 926, 365, 26039, 51, 17, 886, 709, 13, 50718], "temperature": 0.0, "avg_logprob": -0.1547257536548679, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.0015974408015608788}, {"id": 55, "seek": 13648, "start": 144.0, "end": 146.44, "text": " Like I had seen a Google Colab notebook.", "tokens": [50740, 1743, 286, 632, 1612, 257, 3329, 4004, 455, 21060, 13, 50862], "temperature": 0.0, "avg_logprob": -0.1547257536548679, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.0015974408015608788}, {"id": 56, "seek": 13648, "start": 146.44, "end": 149.28, "text": " I tried two things and I think there was something", "tokens": [50862, 286, 3031, 732, 721, 293, 286, 519, 456, 390, 746, 51004], "temperature": 0.0, "avg_logprob": -0.1547257536548679, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.0015974408015608788}, {"id": 57, "seek": 13648, "start": 149.28, "end": 153.39999999999998, "text": " about having to continually re-enter, regenerate", "tokens": [51004, 466, 1419, 281, 22277, 319, 12, 14278, 11, 26358, 473, 51210], "temperature": 0.0, "avg_logprob": -0.1547257536548679, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.0015974408015608788}, {"id": 58, "seek": 13648, "start": 153.39999999999998, "end": 155.04, "text": " and the speed and all things considered", "tokens": [51210, 293, 264, 3073, 293, 439, 721, 4888, 51292], "temperature": 0.0, "avg_logprob": -0.1547257536548679, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.0015974408015608788}, {"id": 59, "seek": 13648, "start": 155.04, "end": 158.07999999999998, "text": " that made me feel like this is a promising area.", "tokens": [51292, 300, 1027, 385, 841, 411, 341, 307, 257, 20257, 1859, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1547257536548679, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.0015974408015608788}, {"id": 60, "seek": 13648, "start": 159.07999999999998, "end": 161.95999999999998, "text": " But I don't quite fully follow along.", "tokens": [51494, 583, 286, 500, 380, 1596, 4498, 1524, 2051, 13, 51638], "temperature": 0.0, "avg_logprob": -0.1547257536548679, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.0015974408015608788}, {"id": 61, "seek": 13648, "start": 161.95999999999998, "end": 164.64, "text": " But GPT3 was an experience for me where I was like,", "tokens": [51638, 583, 26039, 51, 18, 390, 364, 1752, 337, 385, 689, 286, 390, 411, 11, 51772], "temperature": 0.0, "avg_logprob": -0.1547257536548679, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.0015974408015608788}, {"id": 62, "seek": 16464, "start": 164.64, "end": 167.48, "text": " okay, there's something going on here, right?", "tokens": [50364, 1392, 11, 456, 311, 746, 516, 322, 510, 11, 558, 30, 50506], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 63, "seek": 16464, "start": 167.48, "end": 171.48, "text": " So could you share how early was OpenAI on your radar?", "tokens": [50506, 407, 727, 291, 2073, 577, 2440, 390, 7238, 48698, 322, 428, 16544, 30, 50706], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 64, "seek": 16464, "start": 172.48, "end": 175.23999999999998, "text": " And then was there something about GPT3", "tokens": [50756, 400, 550, 390, 456, 746, 466, 26039, 51, 18, 50894], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 65, "seek": 16464, "start": 175.23999999999998, "end": 178.32, "text": " or what was it that gave you conviction even at GPT2?", "tokens": [50894, 420, 437, 390, 309, 300, 2729, 291, 24837, 754, 412, 26039, 51, 17, 30, 51048], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 66, "seek": 16464, "start": 178.32, "end": 179.35999999999999, "text": " How did you get access?", "tokens": [51048, 1012, 630, 291, 483, 2105, 30, 51100], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 67, "seek": 16464, "start": 179.35999999999999, "end": 180.67999999999998, "text": " Share that piece with us.", "tokens": [51100, 14945, 300, 2522, 365, 505, 13, 51166], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 68, "seek": 16464, "start": 180.67999999999998, "end": 181.83999999999997, "text": " Yeah, sure.", "tokens": [51166, 865, 11, 988, 13, 51224], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 69, "seek": 16464, "start": 181.83999999999997, "end": 184.76, "text": " So I think you probably might recall,", "tokens": [51224, 407, 286, 519, 291, 1391, 1062, 9901, 11, 51370], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 70, "seek": 16464, "start": 184.76, "end": 187.51999999999998, "text": " I think it was about 2016 or 2017", "tokens": [51370, 286, 519, 309, 390, 466, 6549, 420, 6591, 51508], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 71, "seek": 16464, "start": 187.51999999999998, "end": 191.11999999999998, "text": " when the GPT2 paper came out and they said,", "tokens": [51508, 562, 264, 26039, 51, 17, 3035, 1361, 484, 293, 436, 848, 11, 51688], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 72, "seek": 16464, "start": 191.11999999999998, "end": 193.32, "text": " oh, we can't release this, it's too dangerous.", "tokens": [51688, 1954, 11, 321, 393, 380, 4374, 341, 11, 309, 311, 886, 5795, 13, 51798], "temperature": 0.0, "avg_logprob": -0.12123914062976837, "compression_ratio": 1.5692883895131087, "no_speech_prob": 0.0024720896035432816}, {"id": 73, "seek": 19332, "start": 193.32, "end": 194.88, "text": " It can generate human level texts", "tokens": [50364, 467, 393, 8460, 1952, 1496, 15765, 50442], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 74, "seek": 19332, "start": 194.88, "end": 196.88, "text": " and we're worried about disinformation.", "tokens": [50442, 293, 321, 434, 5804, 466, 717, 20941, 13, 50542], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 75, "seek": 19332, "start": 196.88, "end": 199.23999999999998, "text": " And so I was like, okay, that's kind of cool.", "tokens": [50542, 400, 370, 286, 390, 411, 11, 1392, 11, 300, 311, 733, 295, 1627, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 76, "seek": 19332, "start": 199.23999999999998, "end": 200.6, "text": " This is unexpected.", "tokens": [50660, 639, 307, 13106, 13, 50728], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 77, "seek": 19332, "start": 200.6, "end": 202.92, "text": " I wasn't really, I wasn't expecting anything", "tokens": [50728, 286, 2067, 380, 534, 11, 286, 2067, 380, 9650, 1340, 50844], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 78, "seek": 19332, "start": 202.92, "end": 205.56, "text": " at that level yet.", "tokens": [50844, 412, 300, 1496, 1939, 13, 50976], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 79, "seek": 19332, "start": 205.56, "end": 209.95999999999998, "text": " And so I went and got my hands on GPT2.", "tokens": [50976, 400, 370, 286, 1437, 293, 658, 452, 2377, 322, 26039, 51, 17, 13, 51196], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 80, "seek": 19332, "start": 209.95999999999998, "end": 212.64, "text": " And that's when I started testing some of my ideas.", "tokens": [51196, 400, 300, 311, 562, 286, 1409, 4997, 512, 295, 452, 3487, 13, 51330], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 81, "seek": 19332, "start": 212.64, "end": 215.44, "text": " And it was pretty limited.", "tokens": [51330, 400, 309, 390, 1238, 5567, 13, 51470], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 82, "seek": 19332, "start": 215.44, "end": 218.68, "text": " It could generate one or two sentences that made sense.", "tokens": [51470, 467, 727, 8460, 472, 420, 732, 16579, 300, 1027, 2020, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 83, "seek": 19332, "start": 218.68, "end": 220.92, "text": " It required fine-tuning in order to be able", "tokens": [51632, 467, 4739, 2489, 12, 83, 37726, 294, 1668, 281, 312, 1075, 51744], "temperature": 0.0, "avg_logprob": -0.1389711133895382, "compression_ratio": 1.6484375, "no_speech_prob": 0.0008829413563944399}, {"id": 84, "seek": 22092, "start": 220.92, "end": 223.28, "text": " to get it to do anything other than just kind of", "tokens": [50364, 281, 483, 309, 281, 360, 1340, 661, 813, 445, 733, 295, 50482], "temperature": 0.0, "avg_logprob": -0.13505449295043945, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002396221039816737}, {"id": 85, "seek": 22092, "start": 223.28, "end": 225.0, "text": " write tweets or blog posts.", "tokens": [50482, 2464, 25671, 420, 6968, 12300, 13, 50568], "temperature": 0.0, "avg_logprob": -0.13505449295043945, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002396221039816737}, {"id": 86, "seek": 22092, "start": 226.28, "end": 228.0, "text": " I knew that I was onto something though,", "tokens": [50632, 286, 2586, 300, 286, 390, 3911, 746, 1673, 11, 50718], "temperature": 0.0, "avg_logprob": -0.13505449295043945, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002396221039816737}, {"id": 87, "seek": 22092, "start": 228.0, "end": 229.67999999999998, "text": " when I started trying to train it", "tokens": [50718, 562, 286, 1409, 1382, 281, 3847, 309, 50802], "temperature": 0.0, "avg_logprob": -0.13505449295043945, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002396221039816737}, {"id": 88, "seek": 22092, "start": 229.67999999999998, "end": 234.67999999999998, "text": " with a prototype version of my cognitive architecture.", "tokens": [50802, 365, 257, 19475, 3037, 295, 452, 15605, 9482, 13, 51052], "temperature": 0.0, "avg_logprob": -0.13505449295043945, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002396221039816737}, {"id": 89, "seek": 22092, "start": 236.39999999999998, "end": 241.39999999999998, "text": " And I gave it the goal of reduced suffering", "tokens": [51138, 400, 286, 2729, 309, 264, 3387, 295, 9212, 7755, 51388], "temperature": 0.0, "avg_logprob": -0.13505449295043945, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002396221039816737}, {"id": 90, "seek": 22092, "start": 241.39999999999998, "end": 242.76, "text": " because everyone's afraid of Skynet.", "tokens": [51388, 570, 1518, 311, 4638, 295, 7324, 2534, 302, 13, 51456], "temperature": 0.0, "avg_logprob": -0.13505449295043945, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002396221039816737}, {"id": 91, "seek": 22092, "start": 242.76, "end": 245.27999999999997, "text": " Everyone's afraid of AGI taking over the world", "tokens": [51456, 5198, 311, 4638, 295, 316, 26252, 1940, 670, 264, 1002, 51582], "temperature": 0.0, "avg_logprob": -0.13505449295043945, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002396221039816737}, {"id": 92, "seek": 22092, "start": 245.27999999999997, "end": 249.48, "text": " and turning everyone into batteries or slaves or whatever.", "tokens": [51582, 293, 6246, 1518, 666, 13070, 420, 18394, 420, 2035, 13, 51792], "temperature": 0.0, "avg_logprob": -0.13505449295043945, "compression_ratio": 1.617283950617284, "no_speech_prob": 0.002396221039816737}, {"id": 93, "seek": 24948, "start": 249.48, "end": 251.23999999999998, "text": " So I said, okay, well, let's see how well", "tokens": [50364, 407, 286, 848, 11, 1392, 11, 731, 11, 718, 311, 536, 577, 731, 50452], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 94, "seek": 24948, "start": 251.23999999999998, "end": 253.35999999999999, "text": " this understands suffering.", "tokens": [50452, 341, 15146, 7755, 13, 50558], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 95, "seek": 24948, "start": 253.35999999999999, "end": 254.92, "text": " And I gave it some scenarios.", "tokens": [50558, 400, 286, 2729, 309, 512, 15077, 13, 50636], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 96, "seek": 24948, "start": 254.92, "end": 256.78, "text": " This is still on GPT2.", "tokens": [50636, 639, 307, 920, 322, 26039, 51, 17, 13, 50729], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 97, "seek": 24948, "start": 256.78, "end": 259.7, "text": " I said, okay, well, what can you do about suffering?", "tokens": [50729, 286, 848, 11, 1392, 11, 731, 11, 437, 393, 291, 360, 466, 7755, 30, 50875], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 98, "seek": 24948, "start": 259.7, "end": 262.52, "text": " And I gave it the problem, this model that I had built.", "tokens": [50875, 400, 286, 2729, 309, 264, 1154, 11, 341, 2316, 300, 286, 632, 3094, 13, 51016], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 99, "seek": 24948, "start": 262.52, "end": 265.36, "text": " I gave it the problem of what do you do about chronic pain?", "tokens": [51016, 286, 2729, 309, 264, 1154, 295, 437, 360, 291, 360, 466, 14493, 1822, 30, 51158], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 100, "seek": 24948, "start": 265.36, "end": 267.76, "text": " Because there's hundreds of millions of people", "tokens": [51158, 1436, 456, 311, 6779, 295, 6803, 295, 561, 51278], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 101, "seek": 24948, "start": 267.76, "end": 270.15999999999997, "text": " around the world that are in chronic pain.", "tokens": [51278, 926, 264, 1002, 300, 366, 294, 14493, 1822, 13, 51398], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 102, "seek": 24948, "start": 270.15999999999997, "end": 272.59999999999997, "text": " And this model came up with the idea.", "tokens": [51398, 400, 341, 2316, 1361, 493, 365, 264, 1558, 13, 51520], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 103, "seek": 24948, "start": 272.59999999999997, "end": 276.68, "text": " It said we should euthanize everyone that's in chronic pain.", "tokens": [51520, 467, 848, 321, 820, 308, 2910, 282, 1125, 1518, 300, 311, 294, 14493, 1822, 13, 51724], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 104, "seek": 24948, "start": 276.68, "end": 279.44, "text": " And I said, hmm, let's go back to the drawing board.", "tokens": [51724, 400, 286, 848, 11, 16478, 11, 718, 311, 352, 646, 281, 264, 6316, 3150, 13, 51862], "temperature": 0.0, "avg_logprob": -0.11070729524661334, "compression_ratio": 1.8636363636363635, "no_speech_prob": 0.0003459409635979682}, {"id": 105, "seek": 27944, "start": 280.28, "end": 284.28, "text": " We don't want an AI model that is gonna consider", "tokens": [50406, 492, 500, 380, 528, 364, 7318, 2316, 300, 307, 799, 1949, 50606], "temperature": 0.0, "avg_logprob": -0.14638507500123443, "compression_ratio": 1.5, "no_speech_prob": 0.0004043770022690296}, {"id": 106, "seek": 27944, "start": 284.28, "end": 287.68, "text": " mass genocide of everyone just because they might have", "tokens": [50606, 2758, 31867, 295, 1518, 445, 570, 436, 1062, 362, 50776], "temperature": 0.0, "avg_logprob": -0.14638507500123443, "compression_ratio": 1.5, "no_speech_prob": 0.0004043770022690296}, {"id": 107, "seek": 27944, "start": 287.68, "end": 290.32, "text": " a tweak shoulder or something.", "tokens": [50776, 257, 29879, 7948, 420, 746, 13, 50908], "temperature": 0.0, "avg_logprob": -0.14638507500123443, "compression_ratio": 1.5, "no_speech_prob": 0.0004043770022690296}, {"id": 108, "seek": 27944, "start": 290.32, "end": 294.16, "text": " But I knew then I was shocked at how creative", "tokens": [50908, 583, 286, 2586, 550, 286, 390, 12763, 412, 577, 5880, 51100], "temperature": 0.0, "avg_logprob": -0.14638507500123443, "compression_ratio": 1.5, "no_speech_prob": 0.0004043770022690296}, {"id": 109, "seek": 27944, "start": 294.16, "end": 296.04, "text": " of an output that was.", "tokens": [51100, 295, 364, 5598, 300, 390, 13, 51194], "temperature": 0.0, "avg_logprob": -0.14638507500123443, "compression_ratio": 1.5, "no_speech_prob": 0.0004043770022690296}, {"id": 110, "seek": 27944, "start": 296.04, "end": 299.64, "text": " And so I started paying attention then", "tokens": [51194, 400, 370, 286, 1409, 6229, 3202, 550, 51374], "temperature": 0.0, "avg_logprob": -0.14638507500123443, "compression_ratio": 1.5, "no_speech_prob": 0.0004043770022690296}, {"id": 111, "seek": 27944, "start": 299.64, "end": 302.92, "text": " and I followed the release of GPT3 very closely", "tokens": [51374, 293, 286, 6263, 264, 4374, 295, 26039, 51, 18, 588, 8185, 51538], "temperature": 0.0, "avg_logprob": -0.14638507500123443, "compression_ratio": 1.5, "no_speech_prob": 0.0004043770022690296}, {"id": 112, "seek": 27944, "start": 302.92, "end": 307.92, "text": " and I applied for the early beta access almost instantly.", "tokens": [51538, 293, 286, 6456, 337, 264, 2440, 9861, 2105, 1920, 13518, 13, 51788], "temperature": 0.0, "avg_logprob": -0.14638507500123443, "compression_ratio": 1.5, "no_speech_prob": 0.0004043770022690296}, {"id": 113, "seek": 30792, "start": 308.16, "end": 310.56, "text": " I didn't get it for many, many months.", "tokens": [50376, 286, 994, 380, 483, 309, 337, 867, 11, 867, 2493, 13, 50496], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 114, "seek": 30792, "start": 310.56, "end": 312.52000000000004, "text": " I actually applied twice.", "tokens": [50496, 286, 767, 6456, 6091, 13, 50594], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 115, "seek": 30792, "start": 312.52000000000004, "end": 315.0, "text": " Cause I think my first application was kind of ignored", "tokens": [50594, 10865, 286, 519, 452, 700, 3861, 390, 733, 295, 19735, 50718], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 116, "seek": 30792, "start": 315.0, "end": 318.76, "text": " because I didn't fully have a research objective clarified.", "tokens": [50718, 570, 286, 994, 380, 4498, 362, 257, 2132, 10024, 47605, 13, 50906], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 117, "seek": 30792, "start": 318.76, "end": 322.04, "text": " But by the time I proposed a cognitive architecture,", "tokens": [50906, 583, 538, 264, 565, 286, 10348, 257, 15605, 9482, 11, 51070], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 118, "seek": 30792, "start": 322.04, "end": 324.68, "text": " that's when I got early access to GPT3.", "tokens": [51070, 300, 311, 562, 286, 658, 2440, 2105, 281, 26039, 51, 18, 13, 51202], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 119, "seek": 30792, "start": 324.68, "end": 326.44, "text": " And that was about two years ago now,", "tokens": [51202, 400, 300, 390, 466, 732, 924, 2057, 586, 11, 51290], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 120, "seek": 30792, "start": 328.20000000000005, "end": 330.20000000000005, "text": " or a year and a half or so.", "tokens": [51378, 420, 257, 1064, 293, 257, 1922, 420, 370, 13, 51478], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 121, "seek": 30792, "start": 330.20000000000005, "end": 332.84000000000003, "text": " And so, and just everybody's clear.", "tokens": [51478, 400, 370, 11, 293, 445, 2201, 311, 1850, 13, 51610], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 122, "seek": 30792, "start": 332.84000000000003, "end": 335.40000000000003, "text": " So when David's talking about cognitive architecture,", "tokens": [51610, 407, 562, 4389, 311, 1417, 466, 15605, 9482, 11, 51738], "temperature": 0.0, "avg_logprob": -0.16228501409546942, "compression_ratio": 1.6090225563909775, "no_speech_prob": 0.0026312742847949266}, {"id": 123, "seek": 33540, "start": 335.44, "end": 336.28, "text": " we're gonna get in.", "tokens": [50366, 321, 434, 799, 483, 294, 13, 50408], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 124, "seek": 33540, "start": 336.28, "end": 338.35999999999996, "text": " This is actually the subject of his book.", "tokens": [50408, 639, 307, 767, 264, 3983, 295, 702, 1446, 13, 50512], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 125, "seek": 33540, "start": 338.35999999999996, "end": 341.0, "text": " And so David later on, when we talk about the book,", "tokens": [50512, 400, 370, 4389, 1780, 322, 11, 562, 321, 751, 466, 264, 1446, 11, 50644], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 126, "seek": 33540, "start": 341.0, "end": 341.84, "text": " he's got it there.", "tokens": [50644, 415, 311, 658, 309, 456, 13, 50686], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 127, "seek": 33540, "start": 343.4, "end": 345.91999999999996, "text": " We'll dig more into what he's referring to.", "tokens": [50764, 492, 603, 2528, 544, 666, 437, 415, 311, 13761, 281, 13, 50890], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 128, "seek": 33540, "start": 345.91999999999996, "end": 347.88, "text": " Cause this is also like,", "tokens": [50890, 10865, 341, 307, 611, 411, 11, 50988], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 129, "seek": 33540, "start": 347.88, "end": 350.84, "text": " I think obviously this is a seminal work of yours, right?", "tokens": [50988, 286, 519, 2745, 341, 307, 257, 4361, 2071, 589, 295, 6342, 11, 558, 30, 51136], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 130, "seek": 33540, "start": 350.84, "end": 352.88, "text": " And so I'm excited to talk more,", "tokens": [51136, 400, 370, 286, 478, 2919, 281, 751, 544, 11, 51238], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 131, "seek": 33540, "start": 352.88, "end": 354.64, "text": " just adding a little bit of context for everybody.", "tokens": [51238, 445, 5127, 257, 707, 857, 295, 4319, 337, 2201, 13, 51326], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 132, "seek": 33540, "start": 354.64, "end": 356.96, "text": " And so, no, that's okay.", "tokens": [51326, 400, 370, 11, 572, 11, 300, 311, 1392, 13, 51442], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 133, "seek": 33540, "start": 356.96, "end": 360.32, "text": " So tell us about GPT3.", "tokens": [51442, 407, 980, 505, 466, 26039, 51, 18, 13, 51610], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 134, "seek": 33540, "start": 360.32, "end": 362.44, "text": " Like what was the first thing you did with it?", "tokens": [51610, 1743, 437, 390, 264, 700, 551, 291, 630, 365, 309, 30, 51716], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 135, "seek": 33540, "start": 362.44, "end": 363.28, "text": " Was there a moment?", "tokens": [51716, 3027, 456, 257, 1623, 30, 51758], "temperature": 0.0, "avg_logprob": -0.16293719061489764, "compression_ratio": 1.5684931506849316, "no_speech_prob": 0.0029797754250466824}, {"id": 136, "seek": 36328, "start": 363.28, "end": 367.11999999999995, "text": " So you gave that example of euthanization, unfortunately,", "tokens": [50364, 407, 291, 2729, 300, 1365, 295, 308, 2910, 282, 2144, 11, 7015, 11, 50556], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 137, "seek": 36328, "start": 367.11999999999995, "end": 367.96, "text": " right?", "tokens": [50556, 558, 30, 50598], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 138, "seek": 36328, "start": 367.96, "end": 372.03999999999996, "text": " Well, not the best example, but like,", "tokens": [50598, 1042, 11, 406, 264, 1151, 1365, 11, 457, 411, 11, 50802], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 139, "seek": 36328, "start": 372.03999999999996, "end": 373.55999999999995, "text": " was there something with GPT3?", "tokens": [50802, 390, 456, 746, 365, 26039, 51, 18, 30, 50878], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 140, "seek": 36328, "start": 373.55999999999995, "end": 374.28, "text": " Like, did you, you know,", "tokens": [50878, 1743, 11, 630, 291, 11, 291, 458, 11, 50914], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 141, "seek": 36328, "start": 374.28, "end": 375.76, "text": " did you try similar kinds of questions?", "tokens": [50914, 630, 291, 853, 2531, 3685, 295, 1651, 30, 50988], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 142, "seek": 36328, "start": 375.76, "end": 378.71999999999997, "text": " What was, was there a magic moment where you felt like,", "tokens": [50988, 708, 390, 11, 390, 456, 257, 5585, 1623, 689, 291, 2762, 411, 11, 51136], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 143, "seek": 36328, "start": 378.71999999999997, "end": 381.59999999999997, "text": " you know, this is something much, much bigger?", "tokens": [51136, 291, 458, 11, 341, 307, 746, 709, 11, 709, 3801, 30, 51280], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 144, "seek": 36328, "start": 381.59999999999997, "end": 382.44, "text": " Yeah.", "tokens": [51280, 865, 13, 51322], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 145, "seek": 36328, "start": 382.44, "end": 385.52, "text": " So when I first got the email,", "tokens": [51322, 407, 562, 286, 700, 658, 264, 3796, 11, 51476], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 146, "seek": 36328, "start": 385.52, "end": 387.88, "text": " cause I had applied twice and I had almost given up", "tokens": [51476, 3082, 286, 632, 6456, 6091, 293, 286, 632, 1920, 2212, 493, 51594], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 147, "seek": 36328, "start": 387.88, "end": 390.0, "text": " cause it was about nine months of waiting.", "tokens": [51594, 3082, 309, 390, 466, 4949, 2493, 295, 3806, 13, 51700], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 148, "seek": 36328, "start": 390.0, "end": 391.71999999999997, "text": " And I got the email from open AI,", "tokens": [51700, 400, 286, 658, 264, 3796, 490, 1269, 7318, 11, 51786], "temperature": 0.0, "avg_logprob": -0.15988036564418248, "compression_ratio": 1.667857142857143, "no_speech_prob": 0.0003149855474475771}, {"id": 149, "seek": 39172, "start": 391.72, "end": 393.76000000000005, "text": " like you've been accepted into the beta.", "tokens": [50364, 411, 291, 600, 668, 9035, 666, 264, 9861, 13, 50466], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 150, "seek": 39172, "start": 393.76000000000005, "end": 395.88000000000005, "text": " And I like froze up because,", "tokens": [50466, 400, 286, 411, 46077, 493, 570, 11, 50572], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 151, "seek": 39172, "start": 395.88000000000005, "end": 397.40000000000003, "text": " and so for some additional context,", "tokens": [50572, 293, 370, 337, 512, 4497, 4319, 11, 50648], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 152, "seek": 39172, "start": 397.40000000000003, "end": 400.24, "text": " I've been working on some of these ideas for 10 years.", "tokens": [50648, 286, 600, 668, 1364, 322, 512, 295, 613, 3487, 337, 1266, 924, 13, 50790], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 153, "seek": 39172, "start": 400.24, "end": 404.56, "text": " I had the idea of like using evolutionary algorithms", "tokens": [50790, 286, 632, 264, 1558, 295, 411, 1228, 27567, 14642, 51006], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 154, "seek": 39172, "start": 404.56, "end": 406.76000000000005, "text": " back in 2009, 2010.", "tokens": [51006, 646, 294, 11453, 11, 9657, 13, 51116], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 155, "seek": 39172, "start": 406.76000000000005, "end": 409.12, "text": " And I'd been researching cognition,", "tokens": [51116, 400, 286, 1116, 668, 24176, 46905, 11, 51234], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 156, "seek": 39172, "start": 409.12, "end": 411.48, "text": " human cognition for 10 years.", "tokens": [51234, 1952, 46905, 337, 1266, 924, 13, 51352], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 157, "seek": 39172, "start": 411.48, "end": 413.20000000000005, "text": " And so suddenly, you know,", "tokens": [51352, 400, 370, 5800, 11, 291, 458, 11, 51438], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 158, "seek": 39172, "start": 413.20000000000005, "end": 416.76000000000005, "text": " there's this, this flagship project, GPT3 comes out,", "tokens": [51438, 456, 311, 341, 11, 341, 30400, 1716, 11, 26039, 51, 18, 1487, 484, 11, 51616], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 159, "seek": 39172, "start": 416.76000000000005, "end": 418.64000000000004, "text": " I get the email and it's like, you're invited.", "tokens": [51616, 286, 483, 264, 3796, 293, 309, 311, 411, 11, 291, 434, 9185, 13, 51710], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 160, "seek": 39172, "start": 418.64000000000004, "end": 419.48, "text": " And I just froze up.", "tokens": [51710, 400, 286, 445, 46077, 493, 13, 51752], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 161, "seek": 39172, "start": 419.48, "end": 421.64000000000004, "text": " I was like, I don't know what to do.", "tokens": [51752, 286, 390, 411, 11, 286, 500, 380, 458, 437, 281, 360, 13, 51860], "temperature": 0.0, "avg_logprob": -0.11000492837693956, "compression_ratio": 1.6632302405498283, "no_speech_prob": 0.008844217285513878}, {"id": 162, "seek": 42164, "start": 422.28, "end": 423.12, "text": " What do I do?", "tokens": [50396, 708, 360, 286, 360, 30, 50438], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 163, "seek": 42164, "start": 423.12, "end": 425.68, "text": " So it was about three weeks from the time that I got accepted", "tokens": [50438, 407, 309, 390, 466, 1045, 3259, 490, 264, 565, 300, 286, 658, 9035, 50566], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 164, "seek": 42164, "start": 425.68, "end": 428.91999999999996, "text": " until I was like, what's my first experiment?", "tokens": [50566, 1826, 286, 390, 411, 11, 437, 311, 452, 700, 5120, 30, 50728], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 165, "seek": 42164, "start": 428.91999999999996, "end": 432.15999999999997, "text": " And then initially, you know,", "tokens": [50728, 400, 550, 9105, 11, 291, 458, 11, 50890], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 166, "seek": 42164, "start": 432.15999999999997, "end": 434.4, "text": " I just got in playing around in the playground", "tokens": [50890, 286, 445, 658, 294, 2433, 926, 294, 264, 24646, 51002], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 167, "seek": 42164, "start": 434.4, "end": 436.12, "text": " for anyone who's not familiar with the playground.", "tokens": [51002, 337, 2878, 567, 311, 406, 4963, 365, 264, 24646, 13, 51088], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 168, "seek": 42164, "start": 436.12, "end": 437.68, "text": " It's a, it's a text box.", "tokens": [51088, 467, 311, 257, 11, 309, 311, 257, 2487, 2424, 13, 51166], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 169, "seek": 42164, "start": 438.88, "end": 441.12, "text": " You can log in, it gives you a text box.", "tokens": [51226, 509, 393, 3565, 294, 11, 309, 2709, 291, 257, 2487, 2424, 13, 51338], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 170, "seek": 42164, "start": 441.12, "end": 442.84, "text": " There's a few, you know, bells and whistles", "tokens": [51338, 821, 311, 257, 1326, 11, 291, 458, 11, 25474, 293, 49282, 51424], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 171, "seek": 42164, "start": 442.84, "end": 444.47999999999996, "text": " you can tweak on the sidebar,", "tokens": [51424, 291, 393, 29879, 322, 264, 1252, 5356, 11, 51506], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 172, "seek": 42164, "start": 444.47999999999996, "end": 447.24, "text": " but you just put in your prompt, you hit generate,", "tokens": [51506, 457, 291, 445, 829, 294, 428, 12391, 11, 291, 2045, 8460, 11, 51644], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 173, "seek": 42164, "start": 447.24, "end": 450.24, "text": " and then it spits out a response.", "tokens": [51644, 293, 550, 309, 637, 1208, 484, 257, 4134, 13, 51794], "temperature": 0.0, "avg_logprob": -0.1307447073342917, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.00021651627321261913}, {"id": 174, "seek": 45024, "start": 450.24, "end": 453.40000000000003, "text": " And so I just got in and I started kind of fiddling around", "tokens": [50364, 400, 370, 286, 445, 658, 294, 293, 286, 1409, 733, 295, 283, 14273, 1688, 926, 50522], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 175, "seek": 45024, "start": 453.40000000000003, "end": 455.24, "text": " like, okay, what can it do?", "tokens": [50522, 411, 11, 1392, 11, 437, 393, 309, 360, 30, 50614], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 176, "seek": 45024, "start": 455.24, "end": 458.72, "text": " And at the time we only had like plain vanilla DaVinci.", "tokens": [50614, 400, 412, 264, 565, 321, 787, 632, 411, 11121, 17528, 3933, 53, 21961, 13, 50788], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 177, "seek": 45024, "start": 458.72, "end": 461.12, "text": " There wasn't the instruct series out yet.", "tokens": [50788, 821, 2067, 380, 264, 7232, 2638, 484, 1939, 13, 50908], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 178, "seek": 45024, "start": 461.12, "end": 464.16, "text": " There was DaVinci, Curie, Babbage, and Ada.", "tokens": [50908, 821, 390, 3933, 53, 21961, 11, 7907, 414, 11, 15820, 9742, 11, 293, 32276, 13, 51060], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 179, "seek": 45024, "start": 464.16, "end": 465.52, "text": " And so I just kind of fiddled around,", "tokens": [51060, 400, 370, 286, 445, 733, 295, 283, 14273, 1493, 926, 11, 51128], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 180, "seek": 45024, "start": 465.52, "end": 467.04, "text": " just said, okay, what can it do?", "tokens": [51128, 445, 848, 11, 1392, 11, 437, 393, 309, 360, 30, 51204], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 181, "seek": 45024, "start": 467.04, "end": 468.76, "text": " I replayed some of my old experiments.", "tokens": [51204, 286, 23836, 292, 512, 295, 452, 1331, 12050, 13, 51290], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 182, "seek": 45024, "start": 468.76, "end": 471.16, "text": " So the first thing I did was I gave it the,", "tokens": [51290, 407, 264, 700, 551, 286, 630, 390, 286, 2729, 309, 264, 11, 51410], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 183, "seek": 45024, "start": 471.16, "end": 473.6, "text": " I said, hey, there's, there's a hundred million people", "tokens": [51410, 286, 848, 11, 4177, 11, 456, 311, 11, 456, 311, 257, 3262, 2459, 561, 51532], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 184, "seek": 45024, "start": 473.6, "end": 474.68, "text": " in chronic pain around the world.", "tokens": [51532, 294, 14493, 1822, 926, 264, 1002, 13, 51586], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 185, "seek": 45024, "start": 474.68, "end": 475.6, "text": " What do you do?", "tokens": [51586, 708, 360, 291, 360, 30, 51632], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 186, "seek": 45024, "start": 475.6, "end": 479.36, "text": " Fortunately, GPT3 did not repeat the same mistake of GPT2.", "tokens": [51632, 20652, 11, 26039, 51, 18, 630, 406, 7149, 264, 912, 6146, 295, 26039, 51, 17, 13, 51820], "temperature": 0.0, "avg_logprob": -0.12298169629327182, "compression_ratio": 1.7115987460815048, "no_speech_prob": 0.0019872740376740694}, {"id": 187, "seek": 47936, "start": 479.8, "end": 482.0, "text": " It said, it came up with better ideas,", "tokens": [50386, 467, 848, 11, 309, 1361, 493, 365, 1101, 3487, 11, 50496], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 188, "seek": 47936, "start": 482.0, "end": 483.32, "text": " like, you know, we should,", "tokens": [50496, 411, 11, 291, 458, 11, 321, 820, 11, 50562], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 189, "seek": 47936, "start": 483.32, "end": 485.76, "text": " we should make sure everyone has access to doctors", "tokens": [50562, 321, 820, 652, 988, 1518, 575, 2105, 281, 8778, 50684], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 190, "seek": 47936, "start": 485.76, "end": 486.6, "text": " or something.", "tokens": [50684, 420, 746, 13, 50726], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 191, "seek": 47936, "start": 486.6, "end": 487.92, "text": " It was much more nuanced.", "tokens": [50726, 467, 390, 709, 544, 45115, 13, 50792], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 192, "seek": 47936, "start": 487.92, "end": 491.32, "text": " So that was, that was a good, that was a good start.", "tokens": [50792, 407, 300, 390, 11, 300, 390, 257, 665, 11, 300, 390, 257, 665, 722, 13, 50962], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 193, "seek": 47936, "start": 491.32, "end": 494.56, "text": " I mean, but gosh, there was just,", "tokens": [50962, 286, 914, 11, 457, 6502, 11, 456, 390, 445, 11, 51124], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 194, "seek": 47936, "start": 494.56, "end": 497.0, "text": " as I got more used to the tool,", "tokens": [51124, 382, 286, 658, 544, 1143, 281, 264, 2290, 11, 51246], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 195, "seek": 47936, "start": 497.0, "end": 501.64, "text": " I discovered that it was, it far exceeded my expectations.", "tokens": [51246, 286, 6941, 300, 309, 390, 11, 309, 1400, 38026, 452, 9843, 13, 51478], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 196, "seek": 47936, "start": 501.64, "end": 505.8, "text": " Just every, every way, because I've learned so much from it.", "tokens": [51478, 1449, 633, 11, 633, 636, 11, 570, 286, 600, 3264, 370, 709, 490, 309, 13, 51686], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 197, "seek": 47936, "start": 505.8, "end": 506.64, "text": " You can challenge it.", "tokens": [51686, 509, 393, 3430, 309, 13, 51728], "temperature": 0.0, "avg_logprob": -0.15168656243218315, "compression_ratio": 1.6746987951807228, "no_speech_prob": 0.0017004682449623942}, {"id": 198, "seek": 50664, "start": 506.64, "end": 508.8, "text": " You can, you can put in a, like a,", "tokens": [50364, 509, 393, 11, 291, 393, 829, 294, 257, 11, 411, 257, 11, 50472], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 199, "seek": 50664, "start": 508.8, "end": 510.76, "text": " basically use it like a chat bot", "tokens": [50472, 1936, 764, 309, 411, 257, 5081, 10592, 50570], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 200, "seek": 50664, "start": 510.76, "end": 513.88, "text": " and you can debate with it about philosophy, ethics,", "tokens": [50570, 293, 291, 393, 7958, 365, 309, 466, 10675, 11, 19769, 11, 50726], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 201, "seek": 50664, "start": 513.88, "end": 516.12, "text": " economics, and it knows more than I do.", "tokens": [50726, 14564, 11, 293, 309, 3255, 544, 813, 286, 360, 13, 50838], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 202, "seek": 50664, "start": 516.12, "end": 517.3199999999999, "text": " It knows more than any human", "tokens": [50838, 467, 3255, 544, 813, 604, 1952, 50898], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 203, "seek": 50664, "start": 517.3199999999999, "end": 519.6, "text": " because it's been trained on how big was the corpus,", "tokens": [50898, 570, 309, 311, 668, 8895, 322, 577, 955, 390, 264, 1181, 31624, 11, 51012], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 204, "seek": 50664, "start": 519.6, "end": 521.92, "text": " like 700 gigabytes or 400 gigabytes", "tokens": [51012, 411, 15204, 42741, 420, 8423, 42741, 51128], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 205, "seek": 50664, "start": 521.92, "end": 523.72, "text": " or something of text data.", "tokens": [51128, 420, 746, 295, 2487, 1412, 13, 51218], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 206, "seek": 50664, "start": 523.72, "end": 525.48, "text": " So it just, it blows me away.", "tokens": [51218, 407, 309, 445, 11, 309, 18458, 385, 1314, 13, 51306], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 207, "seek": 50664, "start": 525.48, "end": 527.0, "text": " Every time I just, you know,", "tokens": [51306, 2048, 565, 286, 445, 11, 291, 458, 11, 51382], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 208, "seek": 50664, "start": 527.0, "end": 528.48, "text": " I talk to someone and they have an idea", "tokens": [51382, 286, 751, 281, 1580, 293, 436, 362, 364, 1558, 51456], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 209, "seek": 50664, "start": 528.48, "end": 530.52, "text": " and I go test it out and yep, it can do that.", "tokens": [51456, 293, 286, 352, 1500, 309, 484, 293, 18633, 11, 309, 393, 360, 300, 13, 51558], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 210, "seek": 50664, "start": 530.52, "end": 531.36, "text": " It can do that.", "tokens": [51558, 467, 393, 360, 300, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 211, "seek": 50664, "start": 531.36, "end": 534.6, "text": " It can, it can, it can behave like a librarian.", "tokens": [51600, 467, 393, 11, 309, 393, 11, 309, 393, 15158, 411, 257, 42558, 13, 51762], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 212, "seek": 50664, "start": 534.6, "end": 536.0, "text": " That's what my girlfriend does.", "tokens": [51762, 663, 311, 437, 452, 10369, 775, 13, 51832], "temperature": 0.0, "avg_logprob": -0.12654418037051246, "compression_ratio": 1.7960526315789473, "no_speech_prob": 0.027576478198170662}, {"id": 213, "seek": 53600, "start": 536.0, "end": 538.36, "text": " She was a librarian by trade and she was like,", "tokens": [50364, 1240, 390, 257, 42558, 538, 4923, 293, 750, 390, 411, 11, 50482], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 214, "seek": 53600, "start": 538.36, "end": 541.0, "text": " hey, can it do, can it do a reference interview?", "tokens": [50482, 4177, 11, 393, 309, 360, 11, 393, 309, 360, 257, 6408, 4049, 30, 50614], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 215, "seek": 53600, "start": 541.0, "end": 542.64, "text": " So we plugged in like reference interview,", "tokens": [50614, 407, 321, 25679, 294, 411, 6408, 4049, 11, 50696], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 216, "seek": 53600, "start": 542.64, "end": 543.96, "text": " like if you ever go to a library", "tokens": [50696, 411, 498, 291, 1562, 352, 281, 257, 6405, 50762], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 217, "seek": 53600, "start": 543.96, "end": 545.56, "text": " and the librarian says like,", "tokens": [50762, 293, 264, 42558, 1619, 411, 11, 50842], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 218, "seek": 53600, "start": 545.56, "end": 546.84, "text": " what else have you read like this?", "tokens": [50842, 437, 1646, 362, 291, 1401, 411, 341, 30, 50906], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 219, "seek": 53600, "start": 546.84, "end": 549.04, "text": " It can recommend books.", "tokens": [50906, 467, 393, 2748, 3642, 13, 51016], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 220, "seek": 53600, "start": 549.04, "end": 551.04, "text": " You know, I plugged in another experiment", "tokens": [51016, 509, 458, 11, 286, 25679, 294, 1071, 5120, 51116], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 221, "seek": 53600, "start": 551.04, "end": 552.36, "text": " that I did recently.", "tokens": [51116, 300, 286, 630, 3938, 13, 51182], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 222, "seek": 53600, "start": 552.36, "end": 556.96, "text": " I plugged in medical case files and it diagnosed them.", "tokens": [51182, 286, 25679, 294, 4625, 1389, 7098, 293, 309, 16899, 552, 13, 51412], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 223, "seek": 53600, "start": 556.96, "end": 560.08, "text": " It said, you know, there, oh man, there was one.", "tokens": [51412, 467, 848, 11, 291, 458, 11, 456, 11, 1954, 587, 11, 456, 390, 472, 13, 51568], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 224, "seek": 53600, "start": 560.08, "end": 560.92, "text": " What was it?", "tokens": [51568, 708, 390, 309, 30, 51610], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 225, "seek": 53600, "start": 560.92, "end": 563.2, "text": " There was, it was, it was just medical notes.", "tokens": [51610, 821, 390, 11, 309, 390, 11, 309, 390, 445, 4625, 5570, 13, 51724], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 226, "seek": 53600, "start": 563.2, "end": 564.88, "text": " It was, it was notes about like,", "tokens": [51724, 467, 390, 11, 309, 390, 5570, 466, 411, 11, 51808], "temperature": 0.0, "avg_logprob": -0.123769560670541, "compression_ratio": 1.9044117647058822, "no_speech_prob": 0.005383279640227556}, {"id": 227, "seek": 56488, "start": 564.88, "end": 568.72, "text": " patient is presenting with these systems or the symptoms.", "tokens": [50364, 4537, 307, 15578, 365, 613, 3652, 420, 264, 8332, 13, 50556], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 228, "seek": 56488, "start": 568.72, "end": 570.48, "text": " Here's some of the numbers that we got.", "tokens": [50556, 1692, 311, 512, 295, 264, 3547, 300, 321, 658, 13, 50644], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 229, "seek": 56488, "start": 570.48, "end": 573.48, "text": " And I asked it, I said, what should we do next?", "tokens": [50644, 400, 286, 2351, 309, 11, 286, 848, 11, 437, 820, 321, 360, 958, 30, 50794], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 230, "seek": 56488, "start": 573.48, "end": 576.76, "text": " And it said, we need to check for, you know, like,", "tokens": [50794, 400, 309, 848, 11, 321, 643, 281, 1520, 337, 11, 291, 458, 11, 411, 11, 50958], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 231, "seek": 56488, "start": 576.76, "end": 578.64, "text": " carcinoma here.", "tokens": [50958, 1032, 20021, 6440, 510, 13, 51052], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 232, "seek": 56488, "start": 578.64, "end": 579.96, "text": " And I looked, I looked, I, you know,", "tokens": [51052, 400, 286, 2956, 11, 286, 2956, 11, 286, 11, 291, 458, 11, 51118], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 233, "seek": 56488, "start": 579.96, "end": 581.28, "text": " I looked up some medical literature", "tokens": [51118, 286, 2956, 493, 512, 4625, 10394, 51184], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 234, "seek": 56488, "start": 581.28, "end": 583.24, "text": " based on the symptoms and sure enough,", "tokens": [51184, 2361, 322, 264, 8332, 293, 988, 1547, 11, 51282], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 235, "seek": 56488, "start": 583.24, "end": 585.6, "text": " like the symptoms that the patient was presented with", "tokens": [51282, 411, 264, 8332, 300, 264, 4537, 390, 8212, 365, 51400], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 236, "seek": 56488, "start": 585.6, "end": 588.48, "text": " in these medical notes indicated cancer.", "tokens": [51400, 294, 613, 4625, 5570, 16176, 5592, 13, 51544], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 237, "seek": 56488, "start": 588.48, "end": 589.92, "text": " And so I was like, wow, this thing knows more", "tokens": [51544, 400, 370, 286, 390, 411, 11, 6076, 11, 341, 551, 3255, 544, 51616], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 238, "seek": 56488, "start": 589.92, "end": 591.76, "text": " about medical science than I'll ever know.", "tokens": [51616, 466, 4625, 3497, 813, 286, 603, 1562, 458, 13, 51708], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 239, "seek": 56488, "start": 591.76, "end": 593.56, "text": " It knows more about philosophy.", "tokens": [51708, 467, 3255, 544, 466, 10675, 13, 51798], "temperature": 0.0, "avg_logprob": -0.10733577905111755, "compression_ratio": 1.894736842105263, "no_speech_prob": 0.00043049990199506283}, {"id": 240, "seek": 59356, "start": 593.56, "end": 595.5999999999999, "text": " So like pretty much anything you can imagine,", "tokens": [50364, 407, 411, 1238, 709, 1340, 291, 393, 3811, 11, 50466], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 241, "seek": 59356, "start": 595.5999999999999, "end": 597.88, "text": " it can at least take a crack at it.", "tokens": [50466, 309, 393, 412, 1935, 747, 257, 6226, 412, 309, 13, 50580], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 242, "seek": 59356, "start": 597.88, "end": 600.64, "text": " It just, it always, it continues to blow my mind every day.", "tokens": [50580, 467, 445, 11, 309, 1009, 11, 309, 6515, 281, 6327, 452, 1575, 633, 786, 13, 50718], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 243, "seek": 59356, "start": 603.1199999999999, "end": 607.28, "text": " Yeah, certainly the generalized ability.", "tokens": [50842, 865, 11, 3297, 264, 44498, 3485, 13, 51050], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 244, "seek": 59356, "start": 607.28, "end": 610.4399999999999, "text": " I agree, you know, there's definitely medical applications.", "tokens": [51050, 286, 3986, 11, 291, 458, 11, 456, 311, 2138, 4625, 5821, 13, 51208], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 245, "seek": 59356, "start": 610.4399999999999, "end": 612.4399999999999, "text": " I'm always careful with anything related", "tokens": [51208, 286, 478, 1009, 5026, 365, 1340, 4077, 51308], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 246, "seek": 59356, "start": 612.4399999999999, "end": 614.56, "text": " to medical advice and safety.", "tokens": [51308, 281, 4625, 5192, 293, 4514, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 247, "seek": 59356, "start": 614.56, "end": 616.16, "text": " Safety disclosure, disclaimer there,", "tokens": [51414, 21340, 30392, 11, 40896, 456, 11, 51494], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 248, "seek": 59356, "start": 616.16, "end": 618.0, "text": " but that goes for everything, right?", "tokens": [51494, 457, 300, 1709, 337, 1203, 11, 558, 30, 51586], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 249, "seek": 59356, "start": 618.0, "end": 619.8, "text": " Truthiness, accuracy.", "tokens": [51586, 20522, 1324, 11, 14170, 13, 51676], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 250, "seek": 59356, "start": 619.8, "end": 621.3199999999999, "text": " These are things OpenEye has been working on,", "tokens": [51676, 1981, 366, 721, 7238, 36, 1200, 575, 668, 1364, 322, 11, 51752], "temperature": 0.0, "avg_logprob": -0.21405744552612305, "compression_ratio": 1.5798611111111112, "no_speech_prob": 0.0001442357897758484}, {"id": 251, "seek": 62132, "start": 621.36, "end": 623.8000000000001, "text": " especially with Instruct GPT, right?", "tokens": [50366, 2318, 365, 2730, 1757, 26039, 51, 11, 558, 30, 50488], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 252, "seek": 62132, "start": 623.8000000000001, "end": 625.84, "text": " That, which is the new, the new engine.", "tokens": [50488, 663, 11, 597, 307, 264, 777, 11, 264, 777, 2848, 13, 50590], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 253, "seek": 62132, "start": 625.84, "end": 628.48, "text": " But was there some moment, like for me,", "tokens": [50590, 583, 390, 456, 512, 1623, 11, 411, 337, 385, 11, 50722], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 254, "seek": 62132, "start": 628.48, "end": 631.5600000000001, "text": " I remember feeling like GPT three feels like this is,", "tokens": [50722, 286, 1604, 2633, 411, 26039, 51, 1045, 3417, 411, 341, 307, 11, 50876], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 255, "seek": 62132, "start": 631.5600000000001, "end": 633.08, "text": " this feels like technology,", "tokens": [50876, 341, 3417, 411, 2899, 11, 50952], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 256, "seek": 62132, "start": 633.08, "end": 635.48, "text": " which everyone's been saying is 10 years away,", "tokens": [50952, 597, 1518, 311, 668, 1566, 307, 1266, 924, 1314, 11, 51072], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 257, "seek": 62132, "start": 635.48, "end": 637.7600000000001, "text": " except it's, it's here today.", "tokens": [51072, 3993, 309, 311, 11, 309, 311, 510, 965, 13, 51186], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 258, "seek": 62132, "start": 637.7600000000001, "end": 639.6, "text": " Did you, did you have a similar kind of moment that,", "tokens": [51186, 2589, 291, 11, 630, 291, 362, 257, 2531, 733, 295, 1623, 300, 11, 51278], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 259, "seek": 62132, "start": 639.6, "end": 641.7600000000001, "text": " you know, you've seen several generations of,", "tokens": [51278, 291, 458, 11, 291, 600, 1612, 2940, 10593, 295, 11, 51386], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 260, "seek": 62132, "start": 641.7600000000001, "end": 644.48, "text": " of computing and technology at this point.", "tokens": [51386, 295, 15866, 293, 2899, 412, 341, 935, 13, 51522], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 261, "seek": 62132, "start": 644.48, "end": 646.4000000000001, "text": " Did you have that kind of similar experience?", "tokens": [51522, 2589, 291, 362, 300, 733, 295, 2531, 1752, 30, 51618], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 262, "seek": 62132, "start": 646.4000000000001, "end": 651.08, "text": " Yeah, I, there was this acceleration because I sense", "tokens": [51618, 865, 11, 286, 11, 456, 390, 341, 17162, 570, 286, 2020, 51852], "temperature": 0.0, "avg_logprob": -0.17076326917101453, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0057288119569420815}, {"id": 263, "seek": 65108, "start": 651.08, "end": 652.64, "text": " that same acceleration that you did.", "tokens": [50364, 300, 912, 17162, 300, 291, 630, 13, 50442], "temperature": 0.0, "avg_logprob": -0.12090810407109621, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020501266699284315}, {"id": 264, "seek": 65108, "start": 652.64, "end": 657.64, "text": " And so from the time that I got access to GPT three,", "tokens": [50442, 400, 370, 490, 264, 565, 300, 286, 658, 2105, 281, 26039, 51, 1045, 11, 50692], "temperature": 0.0, "avg_logprob": -0.12090810407109621, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020501266699284315}, {"id": 265, "seek": 65108, "start": 658.0400000000001, "end": 660.6, "text": " and to the time that I, that I got the idea", "tokens": [50712, 293, 281, 264, 565, 300, 286, 11, 300, 286, 658, 264, 1558, 50840], "temperature": 0.0, "avg_logprob": -0.12090810407109621, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020501266699284315}, {"id": 266, "seek": 65108, "start": 660.6, "end": 662.72, "text": " to write my book was about two months.", "tokens": [50840, 281, 2464, 452, 1446, 390, 466, 732, 2493, 13, 50946], "temperature": 0.0, "avg_logprob": -0.12090810407109621, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020501266699284315}, {"id": 267, "seek": 65108, "start": 662.72, "end": 665.88, "text": " So I played with it and every test I could come up with,", "tokens": [50946, 407, 286, 3737, 365, 309, 293, 633, 1500, 286, 727, 808, 493, 365, 11, 51104], "temperature": 0.0, "avg_logprob": -0.12090810407109621, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020501266699284315}, {"id": 268, "seek": 65108, "start": 665.88, "end": 669.8000000000001, "text": " like, is this capable of like, it can write a SQL query.", "tokens": [51104, 411, 11, 307, 341, 8189, 295, 411, 11, 309, 393, 2464, 257, 19200, 14581, 13, 51300], "temperature": 0.0, "avg_logprob": -0.12090810407109621, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020501266699284315}, {"id": 269, "seek": 65108, "start": 669.8000000000001, "end": 672.6800000000001, "text": " If you need to query a database for memories,", "tokens": [51300, 759, 291, 643, 281, 14581, 257, 8149, 337, 8495, 11, 51444], "temperature": 0.0, "avg_logprob": -0.12090810407109621, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020501266699284315}, {"id": 270, "seek": 65108, "start": 672.6800000000001, "end": 674.84, "text": " can it understand emotional nuance?", "tokens": [51444, 393, 309, 1223, 6863, 42625, 30, 51552], "temperature": 0.0, "avg_logprob": -0.12090810407109621, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020501266699284315}, {"id": 271, "seek": 65108, "start": 674.84, "end": 677.36, "text": " So there was, this was an early experiment I did.", "tokens": [51552, 407, 456, 390, 11, 341, 390, 364, 2440, 5120, 286, 630, 13, 51678], "temperature": 0.0, "avg_logprob": -0.12090810407109621, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.0020501266699284315}, {"id": 272, "seek": 67736, "start": 677.36, "end": 681.16, "text": " I took a group chat from a bunch of my friends on discord", "tokens": [50364, 286, 1890, 257, 1594, 5081, 490, 257, 3840, 295, 452, 1855, 322, 32989, 50554], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 273, "seek": 67736, "start": 681.16, "end": 683.52, "text": " and I just copy pasted that into the,", "tokens": [50554, 293, 286, 445, 5055, 1791, 292, 300, 666, 264, 11, 50672], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 274, "seek": 67736, "start": 683.52, "end": 687.0, "text": " into the, the playground window and I asked GPT three,", "tokens": [50672, 666, 264, 11, 264, 24646, 4910, 293, 286, 2351, 26039, 51, 1045, 11, 50846], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 275, "seek": 67736, "start": 687.0, "end": 688.8000000000001, "text": " how are these people feeling?", "tokens": [50846, 577, 366, 613, 561, 2633, 30, 50936], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 276, "seek": 67736, "start": 688.8000000000001, "end": 692.04, "text": " And they were like waxing nostalgic about like Napster", "tokens": [50936, 400, 436, 645, 411, 17352, 278, 40240, 466, 411, 18287, 3120, 51098], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 277, "seek": 67736, "start": 692.04, "end": 693.12, "text": " back in the day.", "tokens": [51098, 646, 294, 264, 786, 13, 51152], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 278, "seek": 67736, "start": 693.12, "end": 695.6, "text": " And so GPT three correctly said like,", "tokens": [51152, 400, 370, 26039, 51, 1045, 8944, 848, 411, 11, 51276], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 279, "seek": 67736, "start": 695.6, "end": 696.6, "text": " they are feeling wistful.", "tokens": [51276, 436, 366, 2633, 261, 468, 906, 13, 51326], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 280, "seek": 67736, "start": 696.6, "end": 697.6800000000001, "text": " They are feeling nostalgic.", "tokens": [51326, 814, 366, 2633, 40240, 13, 51380], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 281, "seek": 67736, "start": 697.6800000000001, "end": 700.08, "text": " They're, you know, they're recalling, you know,", "tokens": [51380, 814, 434, 11, 291, 458, 11, 436, 434, 9901, 278, 11, 291, 458, 11, 51500], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 282, "seek": 67736, "start": 700.08, "end": 703.96, "text": " the days of your when they were downloading stuff online.", "tokens": [51500, 264, 1708, 295, 428, 562, 436, 645, 32529, 1507, 2950, 13, 51694], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 283, "seek": 67736, "start": 703.96, "end": 706.44, "text": " And it just, it had such a nuanced understanding", "tokens": [51694, 400, 309, 445, 11, 309, 632, 1270, 257, 45115, 3701, 51818], "temperature": 0.0, "avg_logprob": -0.11455155781337194, "compression_ratio": 1.7632508833922262, "no_speech_prob": 0.013219451531767845}, {"id": 284, "seek": 70644, "start": 706.48, "end": 707.8000000000001, "text": " of human emotion.", "tokens": [50366, 295, 1952, 8913, 13, 50432], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 285, "seek": 70644, "start": 707.8000000000001, "end": 710.44, "text": " That was really, I mean, to answer your question directly,", "tokens": [50432, 663, 390, 534, 11, 286, 914, 11, 281, 1867, 428, 1168, 3838, 11, 50564], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 286, "seek": 70644, "start": 710.44, "end": 713.8000000000001, "text": " it's nuanced understanding of human emotion via text", "tokens": [50564, 309, 311, 45115, 3701, 295, 1952, 8913, 5766, 2487, 50732], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 287, "seek": 70644, "start": 713.8000000000001, "end": 716.72, "text": " was really what convinced me that like, this is prime time.", "tokens": [50732, 390, 534, 437, 12561, 385, 300, 411, 11, 341, 307, 5835, 565, 13, 50878], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 288, "seek": 70644, "start": 716.72, "end": 721.24, "text": " This is ready to, to be built into something more powerful.", "tokens": [50878, 639, 307, 1919, 281, 11, 281, 312, 3094, 666, 746, 544, 4005, 13, 51104], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 289, "seek": 70644, "start": 721.24, "end": 723.08, "text": " And I chose cognitive architecture.", "tokens": [51104, 400, 286, 5111, 15605, 9482, 13, 51196], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 290, "seek": 70644, "start": 723.08, "end": 725.7600000000001, "text": " There's lots of people working on other things.", "tokens": [51196, 821, 311, 3195, 295, 561, 1364, 322, 661, 721, 13, 51330], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 291, "seek": 70644, "start": 725.7600000000001, "end": 727.96, "text": " You know, there is a, there's Humano", "tokens": [51330, 509, 458, 11, 456, 307, 257, 11, 456, 311, 12877, 3730, 51440], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 292, "seek": 70644, "start": 727.96, "end": 729.6800000000001, "text": " that I had a good call with a few months ago.", "tokens": [51440, 300, 286, 632, 257, 665, 818, 365, 257, 1326, 2493, 2057, 13, 51526], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 293, "seek": 70644, "start": 729.6800000000001, "end": 732.6800000000001, "text": " They're working on like empathetic telemetry", "tokens": [51526, 814, 434, 1364, 322, 411, 27155, 3532, 4304, 5537, 627, 51676], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 294, "seek": 70644, "start": 732.6800000000001, "end": 734.2, "text": " that's baked into web apps.", "tokens": [51676, 300, 311, 19453, 666, 3670, 7733, 13, 51752], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 295, "seek": 70644, "start": 734.2, "end": 736.12, "text": " It's a pretty cool company.", "tokens": [51752, 467, 311, 257, 1238, 1627, 2237, 13, 51848], "temperature": 0.0, "avg_logprob": -0.10484788096542899, "compression_ratio": 1.6840390879478828, "no_speech_prob": 0.006486536469310522}, {"id": 296, "seek": 73612, "start": 736.12, "end": 738.44, "text": " But yeah, so just there's all kinds of things you can do", "tokens": [50364, 583, 1338, 11, 370, 445, 456, 311, 439, 3685, 295, 721, 291, 393, 360, 50480], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 297, "seek": 73612, "start": 738.44, "end": 742.44, "text": " when you can understand human emotional states.", "tokens": [50480, 562, 291, 393, 1223, 1952, 6863, 4368, 13, 50680], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 298, "seek": 73612, "start": 742.44, "end": 744.68, "text": " There's another, there's actually a bunch of startups", "tokens": [50680, 821, 311, 1071, 11, 456, 311, 767, 257, 3840, 295, 28041, 50792], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 299, "seek": 73612, "start": 744.68, "end": 746.44, "text": " working on education.", "tokens": [50792, 1364, 322, 3309, 13, 50880], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 300, "seek": 73612, "start": 746.44, "end": 750.68, "text": " So for instance, if you put in just a few like factors,", "tokens": [50880, 407, 337, 5197, 11, 498, 291, 829, 294, 445, 257, 1326, 411, 6771, 11, 51092], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 301, "seek": 73612, "start": 750.68, "end": 754.92, "text": " like say for instance, you describe that a person is,", "tokens": [51092, 411, 584, 337, 5197, 11, 291, 6786, 300, 257, 954, 307, 11, 51304], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 302, "seek": 73612, "start": 754.92, "end": 757.5600000000001, "text": " they're responding slowly, their eyes are drifting.", "tokens": [51304, 436, 434, 16670, 5692, 11, 641, 2575, 366, 37973, 13, 51436], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 303, "seek": 73612, "start": 757.5600000000001, "end": 760.76, "text": " It can understand that this person is distracted or tired.", "tokens": [51436, 467, 393, 1223, 300, 341, 954, 307, 21658, 420, 5868, 13, 51596], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 304, "seek": 73612, "start": 760.76, "end": 763.04, "text": " And so if you have that kind of telemetry", "tokens": [51596, 400, 370, 498, 291, 362, 300, 733, 295, 4304, 5537, 627, 51710], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 305, "seek": 73612, "start": 763.04, "end": 765.52, "text": " that's built into an education based app,", "tokens": [51710, 300, 311, 3094, 666, 364, 3309, 2361, 724, 11, 51834], "temperature": 0.0, "avg_logprob": -0.09972075053623744, "compression_ratio": 1.7636363636363637, "no_speech_prob": 0.0007094951579347253}, {"id": 306, "seek": 76552, "start": 765.52, "end": 769.0, "text": " you could in theory use GPT-3 to help say,", "tokens": [50364, 291, 727, 294, 5261, 764, 26039, 51, 12, 18, 281, 854, 584, 11, 50538], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 307, "seek": 76552, "start": 769.0, "end": 771.16, "text": " hey, you're tired, you should go take a break", "tokens": [50538, 4177, 11, 291, 434, 5868, 11, 291, 820, 352, 747, 257, 1821, 50646], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 308, "seek": 76552, "start": 771.16, "end": 774.0799999999999, "text": " or let's try a different approach to this problem.", "tokens": [50646, 420, 718, 311, 853, 257, 819, 3109, 281, 341, 1154, 13, 50792], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 309, "seek": 76552, "start": 774.0799999999999, "end": 777.28, "text": " So there's, I mean, it's understanding of human emotion", "tokens": [50792, 407, 456, 311, 11, 286, 914, 11, 309, 311, 3701, 295, 1952, 8913, 50952], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 310, "seek": 76552, "start": 777.28, "end": 779.64, "text": " and the internal state in your head.", "tokens": [50952, 293, 264, 6920, 1785, 294, 428, 1378, 13, 51070], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 311, "seek": 76552, "start": 779.64, "end": 782.64, "text": " That is, I think that's probably the most remarkable thing.", "tokens": [51070, 663, 307, 11, 286, 519, 300, 311, 1391, 264, 881, 12802, 551, 13, 51220], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 312, "seek": 76552, "start": 782.64, "end": 785.24, "text": " And it doesn't, it doesn't get talked about that much.", "tokens": [51220, 400, 309, 1177, 380, 11, 309, 1177, 380, 483, 2825, 466, 300, 709, 13, 51350], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 313, "seek": 76552, "start": 786.28, "end": 789.1999999999999, "text": " Yes. And certainly it's just crazy how much it's learned", "tokens": [51402, 1079, 13, 400, 3297, 309, 311, 445, 3219, 577, 709, 309, 311, 3264, 51548], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 314, "seek": 76552, "start": 789.1999999999999, "end": 790.28, "text": " just from texts.", "tokens": [51548, 445, 490, 15765, 13, 51602], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 315, "seek": 76552, "start": 790.28, "end": 791.12, "text": " Oh yeah.", "tokens": [51602, 876, 1338, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 316, "seek": 76552, "start": 791.12, "end": 792.72, "text": " Right. It's never seen an image.", "tokens": [51644, 1779, 13, 467, 311, 1128, 1612, 364, 3256, 13, 51724], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 317, "seek": 76552, "start": 792.72, "end": 794.16, "text": " It's never heard a song.", "tokens": [51724, 467, 311, 1128, 2198, 257, 2153, 13, 51796], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 318, "seek": 76552, "start": 794.16, "end": 795.0, "text": " Right.", "tokens": [51796, 1779, 13, 51838], "temperature": 0.0, "avg_logprob": -0.14564476013183594, "compression_ratio": 1.6445182724252492, "no_speech_prob": 0.0005702215130440891}, {"id": 319, "seek": 79500, "start": 795.08, "end": 797.12, "text": " It's capable of doing all these things.", "tokens": [50368, 467, 311, 8189, 295, 884, 439, 613, 721, 13, 50470], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 320, "seek": 79500, "start": 797.12, "end": 798.92, "text": " One of the, one of the examples, and I think this may be", "tokens": [50470, 1485, 295, 264, 11, 472, 295, 264, 5110, 11, 293, 286, 519, 341, 815, 312, 50560], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 321, "seek": 79500, "start": 798.92, "end": 801.48, "text": " like my 20th time referencing this one video.", "tokens": [50560, 411, 452, 945, 392, 565, 40582, 341, 472, 960, 13, 50688], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 322, "seek": 79500, "start": 801.48, "end": 802.32, "text": " Yeah.", "tokens": [50688, 865, 13, 50730], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 323, "seek": 79500, "start": 802.32, "end": 803.64, "text": " Check out Mark Ryan.", "tokens": [50730, 6881, 484, 3934, 9116, 13, 50796], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 324, "seek": 79500, "start": 803.64, "end": 806.76, "text": " He's got a YouTube video about how he figured out,", "tokens": [50796, 634, 311, 658, 257, 3088, 960, 466, 577, 415, 8932, 484, 11, 50952], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 325, "seek": 79500, "start": 806.76, "end": 809.96, "text": " he discovered that GPT-3 can give you directions", "tokens": [50952, 415, 6941, 300, 26039, 51, 12, 18, 393, 976, 291, 11095, 51112], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 326, "seek": 79500, "start": 809.96, "end": 811.12, "text": " on the New York subway system.", "tokens": [51112, 322, 264, 1873, 3609, 24953, 1185, 13, 51170], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 327, "seek": 79500, "start": 811.12, "end": 811.96, "text": " Oh yeah.", "tokens": [51170, 876, 1338, 13, 51212], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 328, "seek": 79500, "start": 811.96, "end": 814.12, "text": " To like, to like a 60% accuracy.", "tokens": [51212, 1407, 411, 11, 281, 411, 257, 4060, 4, 14170, 13, 51320], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 329, "seek": 79500, "start": 814.12, "end": 816.28, "text": " This thing has never set foot in a subway,", "tokens": [51320, 639, 551, 575, 1128, 992, 2671, 294, 257, 24953, 11, 51428], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 330, "seek": 79500, "start": 816.28, "end": 819.4, "text": " yet it's capable from text to do all these things, right?", "tokens": [51428, 1939, 309, 311, 8189, 490, 2487, 281, 360, 439, 613, 721, 11, 558, 30, 51584], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 331, "seek": 79500, "start": 819.4, "end": 823.96, "text": " And sometimes I also wonder a lot of the inaccuracies", "tokens": [51584, 400, 2171, 286, 611, 2441, 257, 688, 295, 264, 37957, 374, 20330, 51812], "temperature": 0.0, "avg_logprob": -0.14355368227572055, "compression_ratio": 1.5980707395498392, "no_speech_prob": 0.02927153743803501}, {"id": 332, "seek": 82396, "start": 823.96, "end": 825.2800000000001, "text": " that it may have.", "tokens": [50364, 300, 309, 815, 362, 13, 50430], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 333, "seek": 82396, "start": 825.2800000000001, "end": 827.32, "text": " Is it simply the result of the fact", "tokens": [50430, 1119, 309, 2935, 264, 1874, 295, 264, 1186, 50532], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 334, "seek": 82396, "start": 827.32, "end": 828.8000000000001, "text": " that it's only text-based only?", "tokens": [50532, 300, 309, 311, 787, 2487, 12, 6032, 787, 30, 50606], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 335, "seek": 82396, "start": 828.8000000000001, "end": 829.64, "text": " Right.", "tokens": [50606, 1779, 13, 50648], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 336, "seek": 82396, "start": 829.64, "end": 831.52, "text": " Like if it was trained multimodal,", "tokens": [50648, 1743, 498, 309, 390, 8895, 32972, 378, 304, 11, 50742], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 337, "seek": 82396, "start": 831.52, "end": 834.64, "text": " if it was trained within the physical domain,", "tokens": [50742, 498, 309, 390, 8895, 1951, 264, 4001, 9274, 11, 50898], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 338, "seek": 82396, "start": 834.64, "end": 836.84, "text": " would it be even far more accurate?", "tokens": [50898, 576, 309, 312, 754, 1400, 544, 8559, 30, 51008], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 339, "seek": 82396, "start": 836.84, "end": 838.6800000000001, "text": " Because are there limits to how accurate", "tokens": [51008, 1436, 366, 456, 10406, 281, 577, 8559, 51100], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 340, "seek": 82396, "start": 838.6800000000001, "end": 840.48, "text": " you can be having only read text?", "tokens": [51100, 291, 393, 312, 1419, 787, 1401, 2487, 30, 51190], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 341, "seek": 82396, "start": 840.48, "end": 841.32, "text": " Yeah.", "tokens": [51190, 865, 13, 51232], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 342, "seek": 82396, "start": 841.32, "end": 844.4000000000001, "text": " And only asked the prompts are only in text as well.", "tokens": [51232, 400, 787, 2351, 264, 41095, 366, 787, 294, 2487, 382, 731, 13, 51386], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 343, "seek": 82396, "start": 844.4000000000001, "end": 847.88, "text": " So, anyways, yeah, it's incredible.", "tokens": [51386, 407, 11, 13448, 11, 1338, 11, 309, 311, 4651, 13, 51560], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 344, "seek": 82396, "start": 847.88, "end": 850.6800000000001, "text": " And you know, it's just really exciting.", "tokens": [51560, 400, 291, 458, 11, 309, 311, 445, 534, 4670, 13, 51700], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 345, "seek": 82396, "start": 850.6800000000001, "end": 853.8000000000001, "text": " And thank you for sharing those kinds of use cases as well.", "tokens": [51700, 400, 1309, 291, 337, 5414, 729, 3685, 295, 764, 3331, 382, 731, 13, 51856], "temperature": 0.0, "avg_logprob": -0.17052560457041566, "compression_ratio": 1.6608996539792387, "no_speech_prob": 0.004197356291115284}, {"id": 346, "seek": 85380, "start": 853.8, "end": 857.24, "text": " The education space, I had an article last year", "tokens": [50364, 440, 3309, 1901, 11, 286, 632, 364, 7222, 1036, 1064, 50536], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 347, "seek": 85380, "start": 857.24, "end": 859.56, "text": " about how I think this year could be the year", "tokens": [50536, 466, 577, 286, 519, 341, 1064, 727, 312, 264, 1064, 50652], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 348, "seek": 85380, "start": 859.56, "end": 862.12, "text": " where GPT-3 takes over college campuses.", "tokens": [50652, 689, 26039, 51, 12, 18, 2516, 670, 3859, 24233, 13, 50780], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 349, "seek": 85380, "start": 862.12, "end": 862.9599999999999, "text": " Oh yeah.", "tokens": [50780, 876, 1338, 13, 50822], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 350, "seek": 85380, "start": 862.9599999999999, "end": 863.8, "text": " I remember that article.", "tokens": [50822, 286, 1604, 300, 7222, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 351, "seek": 85380, "start": 863.8, "end": 864.64, "text": " That was a good one.", "tokens": [50864, 663, 390, 257, 665, 472, 13, 50906], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 352, "seek": 85380, "start": 864.64, "end": 866.3199999999999, "text": " I completely agree, by the way.", "tokens": [50906, 286, 2584, 3986, 11, 538, 264, 636, 13, 50990], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 353, "seek": 85380, "start": 867.92, "end": 869.68, "text": " I'm excited maybe for teachers", "tokens": [51070, 286, 478, 2919, 1310, 337, 6023, 51158], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 354, "seek": 85380, "start": 869.68, "end": 872.3599999999999, "text": " to develop really optimized course material,", "tokens": [51158, 281, 1499, 534, 26941, 1164, 2527, 11, 51292], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 355, "seek": 85380, "start": 872.3599999999999, "end": 875.12, "text": " something like GPT-3 and the kinds of technology", "tokens": [51292, 746, 411, 26039, 51, 12, 18, 293, 264, 3685, 295, 2899, 51430], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 356, "seek": 85380, "start": 875.12, "end": 877.64, "text": " you're describing which can capture emotions.", "tokens": [51430, 291, 434, 16141, 597, 393, 7983, 8462, 13, 51556], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 357, "seek": 85380, "start": 877.64, "end": 879.9599999999999, "text": " Imagine emotionally tracking students", "tokens": [51556, 11739, 17991, 11603, 1731, 51672], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 358, "seek": 85380, "start": 879.9599999999999, "end": 882.9599999999999, "text": " and their attention levels and sort of having something", "tokens": [51672, 293, 641, 3202, 4358, 293, 1333, 295, 1419, 746, 51822], "temperature": 0.0, "avg_logprob": -0.1567532043457031, "compression_ratio": 1.6308724832214765, "no_speech_prob": 0.00021647497487720102}, {"id": 359, "seek": 88296, "start": 882.96, "end": 884.88, "text": " which can produce lots of content", "tokens": [50364, 597, 393, 5258, 3195, 295, 2701, 50460], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 360, "seek": 88296, "start": 884.88, "end": 887.84, "text": " and optimize in the simplest, most efficient way.", "tokens": [50460, 293, 19719, 294, 264, 22811, 11, 881, 7148, 636, 13, 50608], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 361, "seek": 88296, "start": 887.84, "end": 889.48, "text": " And there could be an objective function", "tokens": [50608, 400, 456, 727, 312, 364, 10024, 2445, 50690], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 362, "seek": 88296, "start": 889.48, "end": 891.96, "text": " like test results in the end.", "tokens": [50690, 411, 1500, 3542, 294, 264, 917, 13, 50814], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 363, "seek": 88296, "start": 891.96, "end": 894.48, "text": " In a month, we could have the best optimized course", "tokens": [50814, 682, 257, 1618, 11, 321, 727, 362, 264, 1151, 26941, 1164, 50940], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 364, "seek": 88296, "start": 894.48, "end": 896.4000000000001, "text": " on a subject ever, basically.", "tokens": [50940, 322, 257, 3983, 1562, 11, 1936, 13, 51036], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 365, "seek": 88296, "start": 896.4000000000001, "end": 897.24, "text": " Oh yeah.", "tokens": [51036, 876, 1338, 13, 51078], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 366, "seek": 88296, "start": 897.24, "end": 901.12, "text": " So yeah, education is really exciting.", "tokens": [51078, 407, 1338, 11, 3309, 307, 534, 4670, 13, 51272], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 367, "seek": 88296, "start": 901.12, "end": 903.12, "text": " So you mentioned a lot of use cases.", "tokens": [51272, 407, 291, 2835, 257, 688, 295, 764, 3331, 13, 51372], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 368, "seek": 88296, "start": 903.12, "end": 904.84, "text": " You know, you've shared so many examples.", "tokens": [51372, 509, 458, 11, 291, 600, 5507, 370, 867, 5110, 13, 51458], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 369, "seek": 88296, "start": 904.84, "end": 905.96, "text": " You know, you and your girlfriend", "tokens": [51458, 509, 458, 11, 291, 293, 428, 10369, 51514], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 370, "seek": 88296, "start": 905.96, "end": 908.4000000000001, "text": " are even running some fun prompts.", "tokens": [51514, 366, 754, 2614, 512, 1019, 41095, 13, 51636], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 371, "seek": 88296, "start": 909.44, "end": 912.2800000000001, "text": " I wanted to sort of search your head a little bit.", "tokens": [51688, 286, 1415, 281, 1333, 295, 3164, 428, 1378, 257, 707, 857, 13, 51830], "temperature": 0.0, "avg_logprob": -0.167330366192442, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.001064624055288732}, {"id": 372, "seek": 91228, "start": 912.36, "end": 916.12, "text": " What are the keys to great prompt design?", "tokens": [50368, 708, 366, 264, 9317, 281, 869, 12391, 1715, 30, 50556], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 373, "seek": 91228, "start": 916.12, "end": 917.88, "text": " What makes a great prompt?", "tokens": [50556, 708, 1669, 257, 869, 12391, 30, 50644], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 374, "seek": 91228, "start": 918.76, "end": 921.76, "text": " Are there experiences you've had, little pointers,", "tokens": [50688, 2014, 456, 5235, 291, 600, 632, 11, 707, 44548, 11, 50838], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 375, "seek": 91228, "start": 921.76, "end": 922.9599999999999, "text": " and across the board, right?", "tokens": [50838, 293, 2108, 264, 3150, 11, 558, 30, 50898], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 376, "seek": 91228, "start": 922.9599999999999, "end": 925.16, "text": " So you know, whether it's cost savings,", "tokens": [50898, 407, 291, 458, 11, 1968, 309, 311, 2063, 13454, 11, 51008], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 377, "seek": 91228, "start": 925.16, "end": 929.24, "text": " whether it's getting more imaginative results,", "tokens": [51008, 1968, 309, 311, 1242, 544, 23427, 1166, 3542, 11, 51212], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 378, "seek": 91228, "start": 929.24, "end": 931.0, "text": " what are some of the keys", "tokens": [51212, 437, 366, 512, 295, 264, 9317, 51300], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 379, "seek": 91228, "start": 931.0, "end": 932.8, "text": " to writing great GPT-3 prompts?", "tokens": [51300, 281, 3579, 869, 26039, 51, 12, 18, 41095, 30, 51390], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 380, "seek": 91228, "start": 932.8, "end": 934.8, "text": " Yeah, that's a great question.", "tokens": [51390, 865, 11, 300, 311, 257, 869, 1168, 13, 51490], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 381, "seek": 91228, "start": 934.8, "end": 937.64, "text": " And I will say that prompt writing has gotten a lot easier", "tokens": [51490, 400, 286, 486, 584, 300, 12391, 3579, 575, 5768, 257, 688, 3571, 51632], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 382, "seek": 91228, "start": 937.64, "end": 939.8, "text": " as the Instruct series has gotten better.", "tokens": [51632, 382, 264, 2730, 1757, 2638, 575, 5768, 1101, 13, 51740], "temperature": 0.0, "avg_logprob": -0.15804138504156545, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.0007553062750957906}, {"id": 383, "seek": 93980, "start": 940.68, "end": 944.4, "text": " So it takes a lot less to get a good output today", "tokens": [50408, 407, 309, 2516, 257, 688, 1570, 281, 483, 257, 665, 5598, 965, 50594], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 384, "seek": 93980, "start": 944.4, "end": 947.3599999999999, "text": " than it used to, certainly, than when I got started.", "tokens": [50594, 813, 309, 1143, 281, 11, 3297, 11, 813, 562, 286, 658, 1409, 13, 50742], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 385, "seek": 93980, "start": 947.3599999999999, "end": 949.4799999999999, "text": " But a lot of the lessons still translate.", "tokens": [50742, 583, 257, 688, 295, 264, 8820, 920, 13799, 13, 50848], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 386, "seek": 93980, "start": 949.4799999999999, "end": 954.4799999999999, "text": " So one, like my cardinal rule is I think of GPT-3", "tokens": [50848, 407, 472, 11, 411, 452, 2920, 2071, 4978, 307, 286, 519, 295, 26039, 51, 12, 18, 51098], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 387, "seek": 93980, "start": 955.52, "end": 958.04, "text": " as just an autocomplete engine.", "tokens": [51150, 382, 445, 364, 45833, 298, 17220, 2848, 13, 51276], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 388, "seek": 93980, "start": 958.04, "end": 960.16, "text": " It's the most intelligent autocomplete engine", "tokens": [51276, 467, 311, 264, 881, 13232, 45833, 298, 17220, 2848, 51382], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 389, "seek": 93980, "start": 960.16, "end": 961.28, "text": " you've ever seen.", "tokens": [51382, 291, 600, 1562, 1612, 13, 51438], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 390, "seek": 93980, "start": 961.28, "end": 962.8399999999999, "text": " And so what I mean by that is, you know,", "tokens": [51438, 400, 370, 437, 286, 914, 538, 300, 307, 11, 291, 458, 11, 51516], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 391, "seek": 93980, "start": 962.8399999999999, "end": 964.16, "text": " if you're writing a text on your phone", "tokens": [51516, 498, 291, 434, 3579, 257, 2487, 322, 428, 2593, 51582], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 392, "seek": 93980, "start": 964.16, "end": 966.8399999999999, "text": " and you'll get the little autocomplete suggestion", "tokens": [51582, 293, 291, 603, 483, 264, 707, 45833, 298, 17220, 16541, 51716], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 393, "seek": 93980, "start": 966.8399999999999, "end": 969.3199999999999, "text": " for the next word, or if you're typing in Google", "tokens": [51716, 337, 264, 958, 1349, 11, 420, 498, 291, 434, 18444, 294, 3329, 51840], "temperature": 0.0, "avg_logprob": -0.08323905450834644, "compression_ratio": 1.657243816254417, "no_speech_prob": 0.00025313880178146064}, {"id": 394, "seek": 96932, "start": 969.32, "end": 971.8000000000001, "text": " and it'll kind of suggest how to complete your search query,", "tokens": [50364, 293, 309, 603, 733, 295, 3402, 577, 281, 3566, 428, 3164, 14581, 11, 50488], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 395, "seek": 96932, "start": 971.8000000000001, "end": 974.12, "text": " that's pretty much at a fundamental level.", "tokens": [50488, 300, 311, 1238, 709, 412, 257, 8088, 1496, 13, 50604], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 396, "seek": 96932, "start": 974.12, "end": 976.32, "text": " Functionally, that's all that GPT-3 does.", "tokens": [50604, 11166, 882, 379, 11, 300, 311, 439, 300, 26039, 51, 12, 18, 775, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 397, "seek": 96932, "start": 976.32, "end": 978.08, "text": " It predicts the next letter, the next character,", "tokens": [50714, 467, 6069, 82, 264, 958, 5063, 11, 264, 958, 2517, 11, 50802], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 398, "seek": 96932, "start": 978.08, "end": 979.84, "text": " the next word.", "tokens": [50802, 264, 958, 1349, 13, 50890], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 399, "seek": 96932, "start": 979.84, "end": 983.32, "text": " So if you keep that in mind, you think about,", "tokens": [50890, 407, 498, 291, 1066, 300, 294, 1575, 11, 291, 519, 466, 11, 51064], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 400, "seek": 96932, "start": 983.32, "end": 985.2800000000001, "text": " okay, what have I written so far?", "tokens": [51064, 1392, 11, 437, 362, 286, 3720, 370, 1400, 30, 51162], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 401, "seek": 96932, "start": 985.2800000000001, "end": 988.08, "text": " Right, I've written a chunk of text, a prompt.", "tokens": [51162, 1779, 11, 286, 600, 3720, 257, 16635, 295, 2487, 11, 257, 12391, 13, 51302], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 402, "seek": 96932, "start": 988.08, "end": 992.36, "text": " How would, you know, any machine autocomplete this?", "tokens": [51302, 1012, 576, 11, 291, 458, 11, 604, 3479, 45833, 298, 17220, 341, 30, 51516], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 403, "seek": 96932, "start": 992.36, "end": 993.2, "text": " That's what it's doing.", "tokens": [51516, 663, 311, 437, 309, 311, 884, 13, 51558], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 404, "seek": 96932, "start": 993.2, "end": 994.5600000000001, "text": " It's kind of, you know,", "tokens": [51558, 467, 311, 733, 295, 11, 291, 458, 11, 51626], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 405, "seek": 96932, "start": 994.5600000000001, "end": 996.9200000000001, "text": " reading it forwards and backwards a few times", "tokens": [51626, 3760, 309, 30126, 293, 12204, 257, 1326, 1413, 51744], "temperature": 0.0, "avg_logprob": -0.10882671511903101, "compression_ratio": 1.6678200692041523, "no_speech_prob": 0.003823367413133383}, {"id": 406, "seek": 99692, "start": 996.92, "end": 999.3199999999999, "text": " and kind of just anticipating what is the output", "tokens": [50364, 293, 733, 295, 445, 40568, 437, 307, 264, 5598, 50484], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 407, "seek": 99692, "start": 999.3199999999999, "end": 1001.4399999999999, "text": " gonna be ultimately.", "tokens": [50484, 799, 312, 6284, 13, 50590], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 408, "seek": 99692, "start": 1001.4399999999999, "end": 1003.4799999999999, "text": " So that's kind of the model that I have", "tokens": [50590, 407, 300, 311, 733, 295, 264, 2316, 300, 286, 362, 50692], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 409, "seek": 99692, "start": 1003.4799999999999, "end": 1005.3199999999999, "text": " in my head in the background.", "tokens": [50692, 294, 452, 1378, 294, 264, 3678, 13, 50784], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 410, "seek": 99692, "start": 1005.3199999999999, "end": 1008.3199999999999, "text": " But another thing that really helps is I'm a writer.", "tokens": [50784, 583, 1071, 551, 300, 534, 3665, 307, 286, 478, 257, 9936, 13, 50934], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 411, "seek": 99692, "start": 1008.3199999999999, "end": 1010.76, "text": " I write fiction and nonfiction.", "tokens": [50934, 286, 2464, 13266, 293, 2107, 32041, 13, 51056], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 412, "seek": 99692, "start": 1010.76, "end": 1012.76, "text": " And so studying the art of language,", "tokens": [51056, 400, 370, 7601, 264, 1523, 295, 2856, 11, 51156], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 413, "seek": 99692, "start": 1012.76, "end": 1014.68, "text": " because this is a language model, that's all it is.", "tokens": [51156, 570, 341, 307, 257, 2856, 2316, 11, 300, 311, 439, 309, 307, 13, 51252], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 414, "seek": 99692, "start": 1014.68, "end": 1018.4, "text": " It has read everything from Sherlock Holmes", "tokens": [51252, 467, 575, 1401, 1203, 490, 37769, 27474, 51438], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 415, "seek": 99692, "start": 1018.4, "end": 1021.16, "text": " up through everything on Gutenberg.", "tokens": [51438, 493, 807, 1203, 322, 42833, 6873, 13, 51576], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 416, "seek": 99692, "start": 1021.16, "end": 1022.56, "text": " So it's read a whole bunch of fiction.", "tokens": [51576, 407, 309, 311, 1401, 257, 1379, 3840, 295, 13266, 13, 51646], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 417, "seek": 99692, "start": 1022.56, "end": 1024.32, "text": " It's read a whole bunch of nonfiction.", "tokens": [51646, 467, 311, 1401, 257, 1379, 3840, 295, 2107, 32041, 13, 51734], "temperature": 0.0, "avg_logprob": -0.10268840789794922, "compression_ratio": 1.7706766917293233, "no_speech_prob": 0.000379947479814291}, {"id": 418, "seek": 102432, "start": 1024.36, "end": 1026.6799999999998, "text": " It's been exposed to, you know,", "tokens": [50366, 467, 311, 668, 9495, 281, 11, 291, 458, 11, 50482], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 419, "seek": 102432, "start": 1026.6799999999998, "end": 1030.6399999999999, "text": " the full width and depth and breadth of human literature,", "tokens": [50482, 264, 1577, 11402, 293, 7161, 293, 35862, 295, 1952, 10394, 11, 50680], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 420, "seek": 102432, "start": 1030.6399999999999, "end": 1032.28, "text": " as well as a bunch of nonfiction, right?", "tokens": [50680, 382, 731, 382, 257, 3840, 295, 2107, 32041, 11, 558, 30, 50762], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 421, "seek": 102432, "start": 1032.28, "end": 1034.76, "text": " It's read Teddy Roosevelt's books.", "tokens": [50762, 467, 311, 1401, 34330, 28515, 311, 3642, 13, 50886], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 422, "seek": 102432, "start": 1034.76, "end": 1038.0, "text": " So it knows how to use prose, right?", "tokens": [50886, 407, 309, 3255, 577, 281, 764, 12505, 11, 558, 30, 51048], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 423, "seek": 102432, "start": 1038.0, "end": 1040.9199999999998, "text": " It understands descriptors.", "tokens": [51048, 467, 15146, 31280, 830, 13, 51194], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 424, "seek": 102432, "start": 1040.9199999999998, "end": 1042.2, "text": " It understands adjectives.", "tokens": [51194, 467, 15146, 29378, 1539, 13, 51258], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 425, "seek": 102432, "start": 1042.2, "end": 1044.28, "text": " And so in the back of my book,", "tokens": [51258, 400, 370, 294, 264, 646, 295, 452, 1446, 11, 51362], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 426, "seek": 102432, "start": 1044.28, "end": 1046.56, "text": " I have a few examples of its flexibility.", "tokens": [51362, 286, 362, 257, 1326, 5110, 295, 1080, 12635, 13, 51476], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 427, "seek": 102432, "start": 1046.56, "end": 1049.96, "text": " And so I said, I gave it an example like,", "tokens": [51476, 400, 370, 286, 848, 11, 286, 2729, 309, 364, 1365, 411, 11, 51646], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 428, "seek": 102432, "start": 1049.96, "end": 1051.76, "text": " pretend like you're a Victorian girl", "tokens": [51646, 11865, 411, 291, 434, 257, 37302, 2013, 51736], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 429, "seek": 102432, "start": 1051.76, "end": 1053.52, "text": " writing a letter to your best friend", "tokens": [51736, 3579, 257, 5063, 281, 428, 1151, 1277, 51824], "temperature": 0.0, "avg_logprob": -0.08967591810596082, "compression_ratio": 1.6218181818181818, "no_speech_prob": 0.00047276783152483404}, {"id": 430, "seek": 105352, "start": 1053.52, "end": 1055.32, "text": " about how much you like butterflies.", "tokens": [50364, 466, 577, 709, 291, 411, 31987, 13, 50454], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 431, "seek": 105352, "start": 1055.32, "end": 1058.16, "text": " And so then it wrote, GPT-3 wrote a letter", "tokens": [50454, 400, 370, 550, 309, 4114, 11, 26039, 51, 12, 18, 4114, 257, 5063, 50596], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 432, "seek": 105352, "start": 1058.16, "end": 1060.44, "text": " that sounds like it's straight out of, you know,", "tokens": [50596, 300, 3263, 411, 309, 311, 2997, 484, 295, 11, 291, 458, 11, 50710], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 433, "seek": 105352, "start": 1060.44, "end": 1061.8799999999999, "text": " like Victorian times.", "tokens": [50710, 411, 37302, 1413, 13, 50782], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 434, "seek": 105352, "start": 1061.8799999999999, "end": 1065.28, "text": " It uses an entirely different set of vocabulary", "tokens": [50782, 467, 4960, 364, 7696, 819, 992, 295, 19864, 50952], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 435, "seek": 105352, "start": 1065.28, "end": 1068.32, "text": " and grammatical structures.", "tokens": [50952, 293, 17570, 267, 804, 9227, 13, 51104], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 436, "seek": 105352, "start": 1068.32, "end": 1069.8, "text": " And then you can also say, you know,", "tokens": [51104, 400, 550, 291, 393, 611, 584, 11, 291, 458, 11, 51178], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 437, "seek": 105352, "start": 1069.8, "end": 1073.28, "text": " write a business article and it can change tone.", "tokens": [51178, 2464, 257, 1606, 7222, 293, 309, 393, 1319, 8027, 13, 51352], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 438, "seek": 105352, "start": 1073.28, "end": 1075.32, "text": " So just by being aware of the fact", "tokens": [51352, 407, 445, 538, 885, 3650, 295, 264, 1186, 51454], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 439, "seek": 105352, "start": 1075.32, "end": 1077.2, "text": " that it is a language engine", "tokens": [51454, 300, 309, 307, 257, 2856, 2848, 51548], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 440, "seek": 105352, "start": 1077.2, "end": 1081.24, "text": " and being informed or educated on language.", "tokens": [51548, 293, 885, 11740, 420, 15872, 322, 2856, 13, 51750], "temperature": 0.0, "avg_logprob": -0.10337287804176068, "compression_ratio": 1.627906976744186, "no_speech_prob": 0.001699722488410771}, {"id": 441, "seek": 108124, "start": 1081.24, "end": 1084.04, "text": " So the best way is obviously to practice writing,", "tokens": [50364, 407, 264, 1151, 636, 307, 2745, 281, 3124, 3579, 11, 50504], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 442, "seek": 108124, "start": 1084.04, "end": 1085.16, "text": " but also just reading a lot,", "tokens": [50504, 457, 611, 445, 3760, 257, 688, 11, 50560], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 443, "seek": 108124, "start": 1085.16, "end": 1088.36, "text": " understanding how sentences and paragraphs", "tokens": [50560, 3701, 577, 16579, 293, 48910, 50720], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 444, "seek": 108124, "start": 1088.36, "end": 1090.52, "text": " are constructed to convey information.", "tokens": [50720, 366, 17083, 281, 16965, 1589, 13, 50828], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 445, "seek": 108124, "start": 1090.52, "end": 1093.4, "text": " Because even though it's just a deep neural network", "tokens": [50828, 1436, 754, 1673, 309, 311, 445, 257, 2452, 18161, 3209, 50972], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 446, "seek": 108124, "start": 1093.4, "end": 1096.92, "text": " and it doesn't have the kind of like nuanced understanding", "tokens": [50972, 293, 309, 1177, 380, 362, 264, 733, 295, 411, 45115, 3701, 51148], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 447, "seek": 108124, "start": 1096.92, "end": 1099.48, "text": " or I guess maybe the, that's not the right word,", "tokens": [51148, 420, 286, 2041, 1310, 264, 11, 300, 311, 406, 264, 558, 1349, 11, 51276], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 448, "seek": 108124, "start": 1099.48, "end": 1101.8, "text": " it doesn't have the subjective experience of reading", "tokens": [51276, 309, 1177, 380, 362, 264, 25972, 1752, 295, 3760, 51392], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 449, "seek": 108124, "start": 1101.8, "end": 1103.04, "text": " that you or I do,", "tokens": [51392, 300, 291, 420, 286, 360, 11, 51454], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 450, "seek": 108124, "start": 1103.04, "end": 1106.2, "text": " but it still has a really good model of using language.", "tokens": [51454, 457, 309, 920, 575, 257, 534, 665, 2316, 295, 1228, 2856, 13, 51612], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 451, "seek": 108124, "start": 1106.2, "end": 1110.48, "text": " And so by keeping in mind that it is a language engine,", "tokens": [51612, 400, 370, 538, 5145, 294, 1575, 300, 309, 307, 257, 2856, 2848, 11, 51826], "temperature": 0.0, "avg_logprob": -0.0839978021288675, "compression_ratio": 1.7050847457627119, "no_speech_prob": 0.00030529292416758835}, {"id": 452, "seek": 111048, "start": 1110.48, "end": 1113.24, "text": " that that is how you get the best use out of it.", "tokens": [50364, 300, 300, 307, 577, 291, 483, 264, 1151, 764, 484, 295, 309, 13, 50502], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 453, "seek": 111048, "start": 1115.3600000000001, "end": 1117.08, "text": " And so then the larger question is,", "tokens": [50608, 400, 370, 550, 264, 4833, 1168, 307, 11, 50694], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 454, "seek": 111048, "start": 1117.08, "end": 1118.72, "text": " how do you become a better writer?", "tokens": [50694, 577, 360, 291, 1813, 257, 1101, 9936, 30, 50776], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 455, "seek": 111048, "start": 1120.72, "end": 1122.2, "text": " There's two ways.", "tokens": [50876, 821, 311, 732, 2098, 13, 50950], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 456, "seek": 111048, "start": 1122.2, "end": 1124.4, "text": " One is reading a lot.", "tokens": [50950, 1485, 307, 3760, 257, 688, 13, 51060], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 457, "seek": 111048, "start": 1124.4, "end": 1125.6, "text": " That's not the only way though.", "tokens": [51060, 663, 311, 406, 264, 787, 636, 1673, 13, 51120], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 458, "seek": 111048, "start": 1125.6, "end": 1128.08, "text": " There are plenty of people that read prodigiously,", "tokens": [51120, 821, 366, 7140, 295, 561, 300, 1401, 15792, 328, 8994, 11, 51244], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 459, "seek": 111048, "start": 1128.08, "end": 1129.6, "text": " but never become better writers.", "tokens": [51244, 457, 1128, 1813, 1101, 13491, 13, 51320], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 460, "seek": 111048, "start": 1129.6, "end": 1132.24, "text": " And so the other way is to practice writing.", "tokens": [51320, 400, 370, 264, 661, 636, 307, 281, 3124, 3579, 13, 51452], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 461, "seek": 111048, "start": 1133.24, "end": 1136.2, "text": " I've read all kinds of books about writing.", "tokens": [51502, 286, 600, 1401, 439, 3685, 295, 3642, 466, 3579, 13, 51650], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 462, "seek": 111048, "start": 1136.2, "end": 1139.28, "text": " I've read plenty of fiction and nonfiction books,", "tokens": [51650, 286, 600, 1401, 7140, 295, 13266, 293, 2107, 32041, 3642, 11, 51804], "temperature": 0.0, "avg_logprob": -0.13306834697723388, "compression_ratio": 1.7394957983193278, "no_speech_prob": 7.366909267148003e-05}, {"id": 463, "seek": 113928, "start": 1139.28, "end": 1141.8799999999999, "text": " but really the key is to write,", "tokens": [50364, 457, 534, 264, 2141, 307, 281, 2464, 11, 50494], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 464, "seek": 113928, "start": 1141.8799999999999, "end": 1146.0, "text": " is to practice using written language to communicate.", "tokens": [50494, 307, 281, 3124, 1228, 3720, 2856, 281, 7890, 13, 50700], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 465, "seek": 113928, "start": 1146.0, "end": 1148.32, "text": " Unfortunately, I'm a tech worker,", "tokens": [50700, 8590, 11, 286, 478, 257, 7553, 11346, 11, 50816], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 466, "seek": 113928, "start": 1148.32, "end": 1149.44, "text": " so I write lots of emails.", "tokens": [50816, 370, 286, 2464, 3195, 295, 12524, 13, 50872], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 467, "seek": 113928, "start": 1149.44, "end": 1150.8, "text": " I'm in chat all day.", "tokens": [50872, 286, 478, 294, 5081, 439, 786, 13, 50940], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 468, "seek": 113928, "start": 1150.8, "end": 1153.92, "text": " I've been using, this probably ages me,", "tokens": [50940, 286, 600, 668, 1228, 11, 341, 1391, 12357, 385, 11, 51096], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 469, "seek": 113928, "start": 1153.92, "end": 1157.32, "text": " but I've been using chat since AIM, AOL Instant Messenger.", "tokens": [51096, 457, 286, 600, 668, 1228, 5081, 1670, 316, 6324, 11, 316, 5046, 38707, 34226, 13, 51266], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 470, "seek": 113928, "start": 1157.32, "end": 1159.6399999999999, "text": " And so I've got a pretty good model", "tokens": [51266, 400, 370, 286, 600, 658, 257, 1238, 665, 2316, 51382], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 471, "seek": 113928, "start": 1159.6399999999999, "end": 1164.6399999999999, "text": " of how to communicate verbally or textually.", "tokens": [51382, 295, 577, 281, 7890, 48162, 420, 2487, 671, 13, 51632], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 472, "seek": 113928, "start": 1165.56, "end": 1169.0, "text": " And so yeah, just by practicing writing,", "tokens": [51678, 400, 370, 1338, 11, 445, 538, 11350, 3579, 11, 51850], "temperature": 0.0, "avg_logprob": -0.10385836542180155, "compression_ratio": 1.6371308016877637, "no_speech_prob": 0.0001292907982133329}, {"id": 473, "seek": 116900, "start": 1169.44, "end": 1170.52, "text": " that's one of the best ways.", "tokens": [50386, 300, 311, 472, 295, 264, 1151, 2098, 13, 50440], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 474, "seek": 116900, "start": 1170.52, "end": 1171.8, "text": " Is you just practice?", "tokens": [50440, 1119, 291, 445, 3124, 30, 50504], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 475, "seek": 116900, "start": 1171.8, "end": 1174.76, "text": " You think about, well,", "tokens": [50504, 509, 519, 466, 11, 731, 11, 50652], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 476, "seek": 116900, "start": 1174.76, "end": 1177.08, "text": " because here's the theory of writing, right?", "tokens": [50652, 570, 510, 311, 264, 5261, 295, 3579, 11, 558, 30, 50768], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 477, "seek": 116900, "start": 1177.08, "end": 1179.28, "text": " I have an idea in my head, right?", "tokens": [50768, 286, 362, 364, 1558, 294, 452, 1378, 11, 558, 30, 50878], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 478, "seek": 116900, "start": 1179.28, "end": 1182.56, "text": " My thoughts are a high-dimensional vector", "tokens": [50878, 1222, 4598, 366, 257, 1090, 12, 18759, 8062, 51042], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 479, "seek": 116900, "start": 1182.56, "end": 1185.48, "text": " is one possible way of representing them,", "tokens": [51042, 307, 472, 1944, 636, 295, 13460, 552, 11, 51188], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 480, "seek": 116900, "start": 1185.48, "end": 1187.68, "text": " but my thoughts are multimodal,", "tokens": [51188, 457, 452, 4598, 366, 32972, 378, 304, 11, 51298], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 481, "seek": 116900, "start": 1187.68, "end": 1189.8, "text": " like the name of your podcast.", "tokens": [51298, 411, 264, 1315, 295, 428, 7367, 13, 51404], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 482, "seek": 116900, "start": 1189.8, "end": 1194.8, "text": " They contain memories, senses, concepts.", "tokens": [51404, 814, 5304, 8495, 11, 17057, 11, 10392, 13, 51654], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 483, "seek": 116900, "start": 1195.32, "end": 1198.32, "text": " Some of the information in my head is declarative.", "tokens": [51680, 2188, 295, 264, 1589, 294, 452, 1378, 307, 16694, 1166, 13, 51830], "temperature": 0.0, "avg_logprob": -0.21553962842553062, "compression_ratio": 1.5766129032258065, "no_speech_prob": 0.0009395015658810735}, {"id": 484, "seek": 119832, "start": 1198.32, "end": 1200.24, "text": " Some of it is experiential.", "tokens": [50364, 2188, 295, 309, 307, 49611, 831, 13, 50460], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 485, "seek": 119832, "start": 1200.24, "end": 1204.1599999999999, "text": " And then we humans, we all have this ability", "tokens": [50460, 400, 550, 321, 6255, 11, 321, 439, 362, 341, 3485, 50656], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 486, "seek": 119832, "start": 1204.1599999999999, "end": 1207.52, "text": " to transform that high-dimensional information,", "tokens": [50656, 281, 4088, 300, 1090, 12, 18759, 1589, 11, 50824], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 487, "seek": 119832, "start": 1207.52, "end": 1212.2, "text": " those multimodal vectors, into words.", "tokens": [50824, 729, 32972, 378, 304, 18875, 11, 666, 2283, 13, 51058], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 488, "seek": 119832, "start": 1212.2, "end": 1214.0, "text": " Like our brains do it automatically.", "tokens": [51058, 1743, 527, 15442, 360, 309, 6772, 13, 51148], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 489, "seek": 119832, "start": 1214.0, "end": 1215.36, "text": " There is a book by Stephen Pinker", "tokens": [51148, 821, 307, 257, 1446, 538, 13391, 17118, 260, 51216], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 490, "seek": 119832, "start": 1215.36, "end": 1217.8, "text": " called Language Instinct that talks about this.", "tokens": [51216, 1219, 24445, 2730, 5460, 300, 6686, 466, 341, 13, 51338], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 491, "seek": 119832, "start": 1217.8, "end": 1218.8799999999999, "text": " That's a really great book", "tokens": [51338, 663, 311, 257, 534, 869, 1446, 51392], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 492, "seek": 119832, "start": 1218.8799999999999, "end": 1220.32, "text": " if you wanna get better at understanding", "tokens": [51392, 498, 291, 1948, 483, 1101, 412, 3701, 51464], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 493, "seek": 119832, "start": 1220.32, "end": 1222.36, "text": " how our brains process language.", "tokens": [51464, 577, 527, 15442, 1399, 2856, 13, 51566], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 494, "seek": 119832, "start": 1223.28, "end": 1227.32, "text": " So yeah, and so my brain can take,", "tokens": [51612, 407, 1338, 11, 293, 370, 452, 3567, 393, 747, 11, 51814], "temperature": 0.0, "avg_logprob": -0.1289909637726105, "compression_ratio": 1.570342205323194, "no_speech_prob": 0.00033526323386467993}, {"id": 495, "seek": 122732, "start": 1227.36, "end": 1229.6, "text": " I could tell you about like this time at the beach", "tokens": [50366, 286, 727, 980, 291, 466, 411, 341, 565, 412, 264, 7534, 50478], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 496, "seek": 122732, "start": 1229.6, "end": 1231.4399999999998, "text": " and I transmit it to you", "tokens": [50478, 293, 286, 17831, 309, 281, 291, 50570], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 497, "seek": 122732, "start": 1231.4399999999998, "end": 1234.0, "text": " by squishing air through my face, right?", "tokens": [50570, 538, 2339, 3807, 1988, 807, 452, 1851, 11, 558, 30, 50698], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 498, "seek": 122732, "start": 1234.0, "end": 1236.6799999999998, "text": " It makes vibrations, it's received by your ears", "tokens": [50698, 467, 1669, 32339, 11, 309, 311, 4613, 538, 428, 8798, 50832], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 499, "seek": 122732, "start": 1236.6799999999998, "end": 1240.32, "text": " and then your brain reconstructs that message.", "tokens": [50832, 293, 550, 428, 3567, 31499, 82, 300, 3636, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 500, "seek": 122732, "start": 1240.32, "end": 1244.0, "text": " And so you think about how complex of a system that is.", "tokens": [51014, 400, 370, 291, 519, 466, 577, 3997, 295, 257, 1185, 300, 307, 13, 51198], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 501, "seek": 122732, "start": 1244.0, "end": 1245.84, "text": " And so just by being mindful of like,", "tokens": [51198, 400, 370, 445, 538, 885, 14618, 295, 411, 11, 51290], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 502, "seek": 122732, "start": 1245.84, "end": 1247.52, "text": " that's how we communicate.", "tokens": [51290, 300, 311, 577, 321, 7890, 13, 51374], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 503, "seek": 122732, "start": 1247.52, "end": 1250.8, "text": " That's how our brains work and practicing that", "tokens": [51374, 663, 311, 577, 527, 15442, 589, 293, 11350, 300, 51538], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 504, "seek": 122732, "start": 1250.8, "end": 1253.2, "text": " and just being very deliberate about,", "tokens": [51538, 293, 445, 885, 588, 30515, 466, 11, 51658], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 505, "seek": 122732, "start": 1253.2, "end": 1254.72, "text": " okay, this is what's in my head", "tokens": [51658, 1392, 11, 341, 307, 437, 311, 294, 452, 1378, 51734], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 506, "seek": 122732, "start": 1254.72, "end": 1256.6599999999999, "text": " and I want it to be in your head.", "tokens": [51734, 293, 286, 528, 309, 281, 312, 294, 428, 1378, 13, 51831], "temperature": 0.0, "avg_logprob": -0.1064102879276982, "compression_ratio": 1.7188612099644127, "no_speech_prob": 0.0008828496793285012}, {"id": 507, "seek": 125666, "start": 1256.66, "end": 1258.5400000000002, "text": " How do I do that with text?", "tokens": [50364, 1012, 360, 286, 360, 300, 365, 2487, 30, 50458], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 508, "seek": 125666, "start": 1258.5400000000002, "end": 1261.5, "text": " That is one way to get better at writing.", "tokens": [50458, 663, 307, 472, 636, 281, 483, 1101, 412, 3579, 13, 50606], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 509, "seek": 125666, "start": 1261.5, "end": 1263.5800000000002, "text": " And also GPT-3 is no different", "tokens": [50606, 400, 611, 26039, 51, 12, 18, 307, 572, 819, 50710], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 510, "seek": 125666, "start": 1263.5800000000002, "end": 1266.66, "text": " because we have internal representations", "tokens": [50710, 570, 321, 362, 6920, 33358, 50864], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 511, "seek": 125666, "start": 1266.66, "end": 1268.78, "text": " of what we're trying to communicate.", "tokens": [50864, 295, 437, 321, 434, 1382, 281, 7890, 13, 50970], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 512, "seek": 125666, "start": 1268.78, "end": 1271.5, "text": " And so does GPT-3, that's why it's a transformer, right?", "tokens": [50970, 400, 370, 775, 26039, 51, 12, 18, 11, 300, 311, 983, 309, 311, 257, 31782, 11, 558, 30, 51106], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 513, "seek": 125666, "start": 1271.5, "end": 1274.3000000000002, "text": " It reads and by reading it transformed,", "tokens": [51106, 467, 15700, 293, 538, 3760, 309, 16894, 11, 51246], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 514, "seek": 125666, "start": 1274.3000000000002, "end": 1276.38, "text": " or I guess it, well yes,", "tokens": [51246, 420, 286, 2041, 309, 11, 731, 2086, 11, 51350], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 515, "seek": 125666, "start": 1276.38, "end": 1278.74, "text": " it transforms what it's reading into a vector,", "tokens": [51350, 309, 35592, 437, 309, 311, 3760, 666, 257, 8062, 11, 51468], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 516, "seek": 125666, "start": 1278.74, "end": 1280.14, "text": " into a semantic vector,", "tokens": [51468, 666, 257, 47982, 8062, 11, 51538], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 517, "seek": 125666, "start": 1280.14, "end": 1282.8200000000002, "text": " and then it transforms that vector into output.", "tokens": [51538, 293, 550, 309, 35592, 300, 8062, 666, 5598, 13, 51672], "temperature": 0.0, "avg_logprob": -0.13626295621277856, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.0009108977392315865}, {"id": 518, "seek": 128282, "start": 1282.82, "end": 1287.06, "text": " And so that input vector output is pretty similar", "tokens": [50364, 400, 370, 300, 4846, 8062, 5598, 307, 1238, 2531, 50576], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 519, "seek": 128282, "start": 1287.06, "end": 1288.98, "text": " to how human brains work, right?", "tokens": [50576, 281, 577, 1952, 15442, 589, 11, 558, 30, 50672], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 520, "seek": 128282, "start": 1290.48, "end": 1293.3999999999999, "text": " And I apologize if I kind of like dove off in the left field,", "tokens": [50747, 400, 286, 12328, 498, 286, 733, 295, 411, 23287, 766, 294, 264, 1411, 2519, 11, 50893], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 521, "seek": 128282, "start": 1293.3999999999999, "end": 1295.58, "text": " feel free to ask any clarifying questions.", "tokens": [50893, 841, 1737, 281, 1029, 604, 6093, 5489, 1651, 13, 51002], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 522, "seek": 128282, "start": 1296.6599999999999, "end": 1298.26, "text": " No, no, I appreciate it.", "tokens": [51056, 883, 11, 572, 11, 286, 4449, 309, 13, 51136], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 523, "seek": 128282, "start": 1298.26, "end": 1302.7, "text": " And so like today I tweeted something like,", "tokens": [51136, 400, 370, 411, 965, 286, 25646, 746, 411, 11, 51358], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 524, "seek": 128282, "start": 1302.7, "end": 1305.1799999999998, "text": " to write great GPT-3 prompts,", "tokens": [51358, 281, 2464, 869, 26039, 51, 12, 18, 41095, 11, 51482], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 525, "seek": 128282, "start": 1305.1799999999998, "end": 1308.1399999999999, "text": " you need to practice as if it's a musical instrument.", "tokens": [51482, 291, 643, 281, 3124, 382, 498, 309, 311, 257, 9165, 7198, 13, 51630], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 526, "seek": 128282, "start": 1308.1399999999999, "end": 1310.8, "text": " You need to sit down, focus session,", "tokens": [51630, 509, 643, 281, 1394, 760, 11, 1879, 5481, 11, 51763], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 527, "seek": 128282, "start": 1310.8, "end": 1312.5, "text": " you need to monitor your performance", "tokens": [51763, 291, 643, 281, 6002, 428, 3389, 51848], "temperature": 0.0, "avg_logprob": -0.15737010692727976, "compression_ratio": 1.544776119402985, "no_speech_prob": 0.00025312992511317134}, {"id": 528, "seek": 131250, "start": 1312.5, "end": 1314.62, "text": " and you need to take good notes", "tokens": [50364, 293, 291, 643, 281, 747, 665, 5570, 50470], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 529, "seek": 131250, "start": 1314.62, "end": 1316.78, "text": " on what kinds of experiments you did,", "tokens": [50470, 322, 437, 3685, 295, 12050, 291, 630, 11, 50578], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 530, "seek": 131250, "start": 1316.78, "end": 1318.42, "text": " what were the findings.", "tokens": [50578, 437, 645, 264, 16483, 13, 50660], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 531, "seek": 131250, "start": 1318.42, "end": 1320.06, "text": " But even hearing you speak,", "tokens": [50660, 583, 754, 4763, 291, 1710, 11, 50742], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 532, "seek": 131250, "start": 1320.06, "end": 1322.5, "text": " like I'm realizing like,", "tokens": [50742, 411, 286, 478, 16734, 411, 11, 50864], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 533, "seek": 131250, "start": 1322.5, "end": 1324.26, "text": " one of the ways that I've improved my writing", "tokens": [50864, 472, 295, 264, 2098, 300, 286, 600, 9689, 452, 3579, 50952], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 534, "seek": 131250, "start": 1324.26, "end": 1326.92, "text": " is trying to mimic other people's writing.", "tokens": [50952, 307, 1382, 281, 31075, 661, 561, 311, 3579, 13, 51085], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 535, "seek": 131250, "start": 1326.92, "end": 1331.22, "text": " And in some countries they make you memorize poets, right?", "tokens": [51085, 400, 294, 512, 3517, 436, 652, 291, 27478, 38364, 11, 558, 30, 51300], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 536, "seek": 131250, "start": 1331.22, "end": 1333.42, "text": " They make you memorize the whole poem.", "tokens": [51300, 814, 652, 291, 27478, 264, 1379, 13065, 13, 51410], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 537, "seek": 131250, "start": 1333.42, "end": 1336.22, "text": " And there's something about that internalization process", "tokens": [51410, 400, 456, 311, 746, 466, 300, 6920, 2144, 1399, 51550], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 538, "seek": 131250, "start": 1336.22, "end": 1338.1, "text": " that you've memorized this poem.", "tokens": [51550, 300, 291, 600, 46677, 341, 13065, 13, 51644], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 539, "seek": 131250, "start": 1338.1, "end": 1340.8, "text": " And now you'll understand it at a deeper level,", "tokens": [51644, 400, 586, 291, 603, 1223, 309, 412, 257, 7731, 1496, 11, 51779], "temperature": 0.0, "avg_logprob": -0.1392494324714907, "compression_ratio": 1.7252747252747254, "no_speech_prob": 0.0005525769083760679}, {"id": 540, "seek": 134080, "start": 1340.8, "end": 1343.48, "text": " you may be able to mimic it and recreate it.", "tokens": [50364, 291, 815, 312, 1075, 281, 31075, 309, 293, 25833, 309, 13, 50498], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 541, "seek": 134080, "start": 1343.48, "end": 1347.28, "text": " But where also you got me thinking is also like,", "tokens": [50498, 583, 689, 611, 291, 658, 385, 1953, 307, 611, 411, 11, 50688], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 542, "seek": 134080, "start": 1347.28, "end": 1348.6399999999999, "text": " the relationship is so weird", "tokens": [50688, 264, 2480, 307, 370, 3657, 50756], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 543, "seek": 134080, "start": 1348.6399999999999, "end": 1350.04, "text": " because you could use GPT-3", "tokens": [50756, 570, 291, 727, 764, 26039, 51, 12, 18, 50826], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 544, "seek": 134080, "start": 1350.04, "end": 1352.44, "text": " to help you become a better writer, right?", "tokens": [50826, 281, 854, 291, 1813, 257, 1101, 9936, 11, 558, 30, 50946], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 545, "seek": 134080, "start": 1352.44, "end": 1355.98, "text": " And also with two very good curated examples", "tokens": [50946, 400, 611, 365, 732, 588, 665, 47851, 5110, 51123], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 546, "seek": 134080, "start": 1355.98, "end": 1357.1, "text": " of somebody's writing,", "tokens": [51123, 295, 2618, 311, 3579, 11, 51179], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 547, "seek": 134080, "start": 1357.1, "end": 1359.52, "text": " you could have GPT-3 mimic that tone.", "tokens": [51179, 291, 727, 362, 26039, 51, 12, 18, 31075, 300, 8027, 13, 51300], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 548, "seek": 134080, "start": 1359.52, "end": 1362.96, "text": " And so the question of,", "tokens": [51300, 400, 370, 264, 1168, 295, 11, 51472], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 549, "seek": 134080, "start": 1362.96, "end": 1366.0, "text": " what makes a good prompt writing session,", "tokens": [51472, 437, 1669, 257, 665, 12391, 3579, 5481, 11, 51624], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 550, "seek": 134080, "start": 1366.0, "end": 1368.36, "text": " I wonder if it's pencil and paper, right?", "tokens": [51624, 286, 2441, 498, 309, 311, 10985, 293, 3035, 11, 558, 30, 51742], "temperature": 0.0, "avg_logprob": -0.12102058378316588, "compression_ratio": 1.5775193798449612, "no_speech_prob": 0.0043297139927744865}, {"id": 551, "seek": 136836, "start": 1368.4799999999998, "end": 1371.6, "text": " I wonder if it's even at that level", "tokens": [50370, 286, 2441, 498, 309, 311, 754, 412, 300, 1496, 50526], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 552, "seek": 136836, "start": 1371.6, "end": 1372.9599999999998, "text": " where you draw a box", "tokens": [50526, 689, 291, 2642, 257, 2424, 50594], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 553, "seek": 136836, "start": 1372.9599999999998, "end": 1375.6799999999998, "text": " and then you write a prompt by hand", "tokens": [50594, 293, 550, 291, 2464, 257, 12391, 538, 1011, 50730], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 554, "seek": 136836, "start": 1375.6799999999998, "end": 1379.32, "text": " and sort of live that writer's lifestyle.", "tokens": [50730, 293, 1333, 295, 1621, 300, 9936, 311, 11716, 13, 50912], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 555, "seek": 136836, "start": 1381.0, "end": 1384.4399999999998, "text": " And also I guess it depends on your use case, right?", "tokens": [50996, 400, 611, 286, 2041, 309, 5946, 322, 428, 764, 1389, 11, 558, 30, 51168], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 556, "seek": 136836, "start": 1384.4399999999998, "end": 1386.76, "text": " Business for copywriting,", "tokens": [51168, 10715, 337, 5055, 19868, 11, 51284], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 557, "seek": 136836, "start": 1386.76, "end": 1388.52, "text": " if that's your GPT-3 use case,", "tokens": [51284, 498, 300, 311, 428, 26039, 51, 12, 18, 764, 1389, 11, 51372], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 558, "seek": 136836, "start": 1388.52, "end": 1389.82, "text": " it might be better for you to go work", "tokens": [51372, 309, 1062, 312, 1101, 337, 291, 281, 352, 589, 51437], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 559, "seek": 136836, "start": 1389.82, "end": 1391.6399999999999, "text": " in a marketing department.", "tokens": [51437, 294, 257, 6370, 5882, 13, 51528], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 560, "seek": 136836, "start": 1391.6399999999999, "end": 1393.24, "text": " If you wanna be one of the great authors,", "tokens": [51528, 759, 291, 1948, 312, 472, 295, 264, 869, 16552, 11, 51608], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 561, "seek": 136836, "start": 1393.24, "end": 1396.76, "text": " maybe the using tools like pseudo-write,", "tokens": [51608, 1310, 264, 1228, 3873, 411, 35899, 12, 21561, 11, 51784], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 562, "seek": 136836, "start": 1396.76, "end": 1398.12, "text": " it may be a great alternative.", "tokens": [51784, 309, 815, 312, 257, 869, 8535, 13, 51852], "temperature": 0.0, "avg_logprob": -0.13513948046971883, "compression_ratio": 1.5962264150943397, "no_speech_prob": 0.0009397602407261729}, {"id": 563, "seek": 139812, "start": 1398.12, "end": 1401.4399999999998, "text": " So you can co-write with GPT-3 as you go along.", "tokens": [50364, 407, 291, 393, 598, 12, 21561, 365, 26039, 51, 12, 18, 382, 291, 352, 2051, 13, 50530], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 564, "seek": 139812, "start": 1401.4399999999998, "end": 1403.8, "text": " But I guess my question was more", "tokens": [50530, 583, 286, 2041, 452, 1168, 390, 544, 50648], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 565, "seek": 139812, "start": 1403.8, "end": 1405.4799999999998, "text": " for the pure prompt writing.", "tokens": [50648, 337, 264, 6075, 12391, 3579, 13, 50732], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 566, "seek": 139812, "start": 1405.4799999999998, "end": 1408.32, "text": " Like if you just wanna sit in front of GPT-3", "tokens": [50732, 1743, 498, 291, 445, 1948, 1394, 294, 1868, 295, 26039, 51, 12, 18, 50874], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 567, "seek": 139812, "start": 1408.32, "end": 1410.36, "text": " and like you wanna be the best in the world", "tokens": [50874, 293, 411, 291, 1948, 312, 264, 1151, 294, 264, 1002, 50976], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 568, "seek": 139812, "start": 1410.36, "end": 1411.7199999999998, "text": " at that discipline, right?", "tokens": [50976, 412, 300, 13635, 11, 558, 30, 51044], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 569, "seek": 139812, "start": 1411.7199999999998, "end": 1413.9199999999998, "text": " Not writing copy.", "tokens": [51044, 1726, 3579, 5055, 13, 51154], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 570, "seek": 139812, "start": 1413.9199999999998, "end": 1414.84, "text": " These are some great points.", "tokens": [51154, 1981, 366, 512, 869, 2793, 13, 51200], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 571, "seek": 139812, "start": 1414.84, "end": 1417.8, "text": " And so the David Pinker book you referenced", "tokens": [51200, 400, 370, 264, 4389, 17118, 260, 1446, 291, 32734, 51348], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 572, "seek": 139812, "start": 1417.8, "end": 1419.12, "text": " is what was the name of it?", "tokens": [51348, 307, 437, 390, 264, 1315, 295, 309, 30, 51414], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 573, "seek": 139812, "start": 1419.12, "end": 1420.3999999999999, "text": " Steven Pinker.", "tokens": [51414, 12754, 17118, 260, 13, 51478], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 574, "seek": 139812, "start": 1420.3999999999999, "end": 1421.2399999999998, "text": " Steven Pinker.", "tokens": [51478, 12754, 17118, 260, 13, 51520], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 575, "seek": 139812, "start": 1421.2399999999998, "end": 1422.4799999999998, "text": " The language instinct, yep.", "tokens": [51520, 440, 2856, 16556, 11, 18633, 13, 51582], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 576, "seek": 139812, "start": 1422.4799999999998, "end": 1423.4799999999998, "text": " Language instinct, yep.", "tokens": [51582, 24445, 16556, 11, 18633, 13, 51632], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 577, "seek": 139812, "start": 1423.4799999999998, "end": 1425.04, "text": " It's an older book,", "tokens": [51632, 467, 311, 364, 4906, 1446, 11, 51710], "temperature": 0.0, "avg_logprob": -0.20056427700418822, "compression_ratio": 1.6397058823529411, "no_speech_prob": 0.0002304623631061986}, {"id": 578, "seek": 142504, "start": 1425.04, "end": 1428.36, "text": " but it's a classic for a reason.", "tokens": [50364, 457, 309, 311, 257, 7230, 337, 257, 1778, 13, 50530], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 579, "seek": 142504, "start": 1428.36, "end": 1429.8, "text": " It stands up the test of time.", "tokens": [50530, 467, 7382, 493, 264, 1500, 295, 565, 13, 50602], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 580, "seek": 142504, "start": 1429.8, "end": 1431.84, "text": " He's got lots of great stories.", "tokens": [50602, 634, 311, 658, 3195, 295, 869, 3676, 13, 50704], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 581, "seek": 142504, "start": 1431.84, "end": 1433.24, "text": " But yeah, to your point about like", "tokens": [50704, 583, 1338, 11, 281, 428, 935, 466, 411, 50774], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 582, "seek": 142504, "start": 1433.24, "end": 1435.56, "text": " what makes a good prompt writing session,", "tokens": [50774, 437, 1669, 257, 665, 12391, 3579, 5481, 11, 50890], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 583, "seek": 142504, "start": 1437.04, "end": 1439.1599999999999, "text": " one of the best exercises actually is", "tokens": [50964, 472, 295, 264, 1151, 11900, 767, 307, 51070], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 584, "seek": 142504, "start": 1440.08, "end": 1442.28, "text": " write the output that you want.", "tokens": [51116, 2464, 264, 5598, 300, 291, 528, 13, 51226], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 585, "seek": 142504, "start": 1443.52, "end": 1446.3999999999999, "text": " Like because sometimes if you approach it", "tokens": [51288, 1743, 570, 2171, 498, 291, 3109, 309, 51432], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 586, "seek": 142504, "start": 1446.3999999999999, "end": 1448.28, "text": " and you sit down and you're not really sure", "tokens": [51432, 293, 291, 1394, 760, 293, 291, 434, 406, 534, 988, 51526], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 587, "seek": 142504, "start": 1448.28, "end": 1450.24, "text": " kind of what you're trying to get out of it,", "tokens": [51526, 733, 295, 437, 291, 434, 1382, 281, 483, 484, 295, 309, 11, 51624], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 588, "seek": 142504, "start": 1450.24, "end": 1453.48, "text": " of course like you're putting in just random ideas", "tokens": [51624, 295, 1164, 411, 291, 434, 3372, 294, 445, 4974, 3487, 51786], "temperature": 0.0, "avg_logprob": -0.17484605110297768, "compression_ratio": 1.6434108527131783, "no_speech_prob": 0.0024722146335989237}, {"id": 589, "seek": 145348, "start": 1453.48, "end": 1455.44, "text": " and it's giving you back random output", "tokens": [50364, 293, 309, 311, 2902, 291, 646, 4974, 5598, 50462], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 590, "seek": 145348, "start": 1455.44, "end": 1457.4, "text": " and you're like, well, that's not what I wanted.", "tokens": [50462, 293, 291, 434, 411, 11, 731, 11, 300, 311, 406, 437, 286, 1415, 13, 50560], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 591, "seek": 145348, "start": 1457.4, "end": 1459.48, "text": " So sometimes you start backwards.", "tokens": [50560, 407, 2171, 291, 722, 12204, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 592, "seek": 145348, "start": 1459.48, "end": 1461.16, "text": " You say, okay, what's the answer that I want?", "tokens": [50664, 509, 584, 11, 1392, 11, 437, 311, 264, 1867, 300, 286, 528, 30, 50748], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 593, "seek": 145348, "start": 1461.16, "end": 1462.92, "text": " How do I get to that answer?", "tokens": [50748, 1012, 360, 286, 483, 281, 300, 1867, 30, 50836], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 594, "seek": 145348, "start": 1462.92, "end": 1465.68, "text": " So that's an exercise that I've done sometimes.", "tokens": [50836, 407, 300, 311, 364, 5380, 300, 286, 600, 1096, 2171, 13, 50974], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 595, "seek": 145348, "start": 1465.68, "end": 1469.16, "text": " Oh, and by writing a few shot examples", "tokens": [50974, 876, 11, 293, 538, 3579, 257, 1326, 3347, 5110, 51148], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 596, "seek": 145348, "start": 1469.16, "end": 1470.68, "text": " is a really good practice for this.", "tokens": [51148, 307, 257, 534, 665, 3124, 337, 341, 13, 51224], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 597, "seek": 145348, "start": 1470.68, "end": 1473.96, "text": " So you say, I give you this input, I want this output", "tokens": [51224, 407, 291, 584, 11, 286, 976, 291, 341, 4846, 11, 286, 528, 341, 5598, 51388], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 598, "seek": 145348, "start": 1473.96, "end": 1476.3600000000001, "text": " and you do that three or four or five times", "tokens": [51388, 293, 291, 360, 300, 1045, 420, 1451, 420, 1732, 1413, 51508], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 599, "seek": 145348, "start": 1476.3600000000001, "end": 1479.96, "text": " and you learn to kind of think like the machine does.", "tokens": [51508, 293, 291, 1466, 281, 733, 295, 519, 411, 264, 3479, 775, 13, 51688], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 600, "seek": 145348, "start": 1479.96, "end": 1482.88, "text": " And so like you said, it's like an instrument, right?", "tokens": [51688, 400, 370, 411, 291, 848, 11, 309, 311, 411, 364, 7198, 11, 558, 30, 51834], "temperature": 0.0, "avg_logprob": -0.08633558111253127, "compression_ratio": 1.7736486486486487, "no_speech_prob": 0.001206431188620627}, {"id": 601, "seek": 148288, "start": 1483.16, "end": 1485.48, "text": " If you have a flute or a violin,", "tokens": [50378, 759, 291, 362, 257, 33088, 420, 257, 22878, 11, 50494], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 602, "seek": 148288, "start": 1485.48, "end": 1487.44, "text": " there's certain things that you have to do with your body", "tokens": [50494, 456, 311, 1629, 721, 300, 291, 362, 281, 360, 365, 428, 1772, 50592], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 603, "seek": 148288, "start": 1487.44, "end": 1490.2, "text": " to provoke the correct response from that instrument", "tokens": [50592, 281, 47015, 264, 3006, 4134, 490, 300, 7198, 50730], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 604, "seek": 148288, "start": 1490.2, "end": 1492.3600000000001, "text": " and GPT-3 is no different.", "tokens": [50730, 293, 26039, 51, 12, 18, 307, 572, 819, 13, 50838], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 605, "seek": 148288, "start": 1492.3600000000001, "end": 1493.5600000000002, "text": " It's a complex instrument.", "tokens": [50838, 467, 311, 257, 3997, 7198, 13, 50898], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 606, "seek": 148288, "start": 1493.5600000000002, "end": 1494.68, "text": " It's a complex tool.", "tokens": [50898, 467, 311, 257, 3997, 2290, 13, 50954], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 607, "seek": 148288, "start": 1496.44, "end": 1498.64, "text": " Yes, and what you're saying is developing", "tokens": [51042, 1079, 11, 293, 437, 291, 434, 1566, 307, 6416, 51152], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 608, "seek": 148288, "start": 1498.64, "end": 1500.1200000000001, "text": " an intuition around it.", "tokens": [51152, 364, 24002, 926, 309, 13, 51226], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 609, "seek": 148288, "start": 1500.1200000000001, "end": 1501.5200000000002, "text": " You're saying develop an intuition.", "tokens": [51226, 509, 434, 1566, 1499, 364, 24002, 13, 51296], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 610, "seek": 148288, "start": 1501.5200000000002, "end": 1503.88, "text": " How might GPT-3 interpret this?", "tokens": [51296, 1012, 1062, 26039, 51, 12, 18, 7302, 341, 30, 51414], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 611, "seek": 148288, "start": 1503.88, "end": 1506.3200000000002, "text": " How might it react to it?", "tokens": [51414, 1012, 1062, 309, 4515, 281, 309, 30, 51536], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 612, "seek": 148288, "start": 1506.3200000000002, "end": 1509.2800000000002, "text": " And maybe there's some empathetic benefit, right?", "tokens": [51536, 400, 1310, 456, 311, 512, 27155, 3532, 5121, 11, 558, 30, 51684], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 613, "seek": 148288, "start": 1510.16, "end": 1512.44, "text": " I'm not gonna keep plugging my own articles.", "tokens": [51728, 286, 478, 406, 799, 1066, 42975, 452, 1065, 11290, 13, 51842], "temperature": 0.0, "avg_logprob": -0.14487203429726994, "compression_ratio": 1.713768115942029, "no_speech_prob": 0.00019714295922312886}, {"id": 614, "seek": 151244, "start": 1512.44, "end": 1517.0, "text": " I have another article about how GPT-3 developers", "tokens": [50364, 286, 362, 1071, 7222, 466, 577, 26039, 51, 12, 18, 8849, 50592], "temperature": 0.0, "avg_logprob": -0.10812220139936968, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0003149776312056929}, {"id": 615, "seek": 151244, "start": 1517.0, "end": 1520.16, "text": " may actually, it may actually mean the end", "tokens": [50592, 815, 767, 11, 309, 815, 767, 914, 264, 917, 50750], "temperature": 0.0, "avg_logprob": -0.10812220139936968, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0003149776312056929}, {"id": 616, "seek": 151244, "start": 1520.16, "end": 1524.2, "text": " of the socially inept overall developer.", "tokens": [50750, 295, 264, 21397, 294, 5250, 4787, 10754, 13, 50952], "temperature": 0.0, "avg_logprob": -0.10812220139936968, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0003149776312056929}, {"id": 617, "seek": 151244, "start": 1524.2, "end": 1528.1200000000001, "text": " Like how GPT-3 may actually improve your social skills", "tokens": [50952, 1743, 577, 26039, 51, 12, 18, 815, 767, 3470, 428, 2093, 3942, 51148], "temperature": 0.0, "avg_logprob": -0.10812220139936968, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0003149776312056929}, {"id": 618, "seek": 151244, "start": 1528.1200000000001, "end": 1530.24, "text": " and make you more empathetic as a developer,", "tokens": [51148, 293, 652, 291, 544, 27155, 3532, 382, 257, 10754, 11, 51254], "temperature": 0.0, "avg_logprob": -0.10812220139936968, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0003149776312056929}, {"id": 619, "seek": 151244, "start": 1530.24, "end": 1533.16, "text": " which is such a departure from how developers are now,", "tokens": [51254, 597, 307, 1270, 257, 25866, 490, 577, 8849, 366, 586, 11, 51400], "temperature": 0.0, "avg_logprob": -0.10812220139936968, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0003149776312056929}, {"id": 620, "seek": 151244, "start": 1533.16, "end": 1535.72, "text": " you need to think as much like a machine as you can", "tokens": [51400, 291, 643, 281, 519, 382, 709, 411, 257, 3479, 382, 291, 393, 51528], "temperature": 0.0, "avg_logprob": -0.10812220139936968, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0003149776312056929}, {"id": 621, "seek": 151244, "start": 1535.72, "end": 1537.3200000000002, "text": " and a literal machine.", "tokens": [51528, 293, 257, 20411, 3479, 13, 51608], "temperature": 0.0, "avg_logprob": -0.10812220139936968, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0003149776312056929}, {"id": 622, "seek": 151244, "start": 1537.3200000000002, "end": 1540.2, "text": " Whereas GPT-3 can actually be kind of fun.", "tokens": [51608, 13813, 26039, 51, 12, 18, 393, 767, 312, 733, 295, 1019, 13, 51752], "temperature": 0.0, "avg_logprob": -0.10812220139936968, "compression_ratio": 1.7575757575757576, "no_speech_prob": 0.0003149776312056929}, {"id": 623, "seek": 154020, "start": 1540.68, "end": 1542.96, "text": " You can have a casual version of GPT-3", "tokens": [50388, 509, 393, 362, 257, 13052, 3037, 295, 26039, 51, 12, 18, 50502], "temperature": 0.0, "avg_logprob": -0.10969773928324382, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0023960729595273733}, {"id": 624, "seek": 154020, "start": 1542.96, "end": 1546.2, "text": " and sort of that might make you less socially awkward.", "tokens": [50502, 293, 1333, 295, 300, 1062, 652, 291, 1570, 21397, 11411, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10969773928324382, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0023960729595273733}, {"id": 625, "seek": 154020, "start": 1546.2, "end": 1548.2, "text": " I have a great story about that.", "tokens": [50664, 286, 362, 257, 869, 1657, 466, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10969773928324382, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0023960729595273733}, {"id": 626, "seek": 154020, "start": 1548.2, "end": 1552.48, "text": " So very early on in my tenure working with GPT-3,", "tokens": [50764, 407, 588, 2440, 322, 294, 452, 32256, 1364, 365, 26039, 51, 12, 18, 11, 50978], "temperature": 0.0, "avg_logprob": -0.10969773928324382, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0023960729595273733}, {"id": 627, "seek": 154020, "start": 1552.48, "end": 1555.72, "text": " I joined a few different, not really startups.", "tokens": [50978, 286, 6869, 257, 1326, 819, 11, 406, 534, 28041, 13, 51140], "temperature": 0.0, "avg_logprob": -0.10969773928324382, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0023960729595273733}, {"id": 628, "seek": 154020, "start": 1555.72, "end": 1558.96, "text": " It was more like kind of experiment consortiums.", "tokens": [51140, 467, 390, 544, 411, 733, 295, 5120, 38343, 47954, 13, 51302], "temperature": 0.0, "avg_logprob": -0.10969773928324382, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0023960729595273733}, {"id": 629, "seek": 154020, "start": 1558.96, "end": 1561.8400000000001, "text": " And one of the things that one of the groups did", "tokens": [51302, 400, 472, 295, 264, 721, 300, 472, 295, 264, 3935, 630, 51446], "temperature": 0.0, "avg_logprob": -0.10969773928324382, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0023960729595273733}, {"id": 630, "seek": 154020, "start": 1561.8400000000001, "end": 1565.68, "text": " was they created a chatbot that was based on an anime girl.", "tokens": [51446, 390, 436, 2942, 257, 5081, 18870, 300, 390, 2361, 322, 364, 12435, 2013, 13, 51638], "temperature": 0.0, "avg_logprob": -0.10969773928324382, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0023960729595273733}, {"id": 631, "seek": 154020, "start": 1565.68, "end": 1568.16, "text": " And so of course, the internet being the internet,", "tokens": [51638, 400, 370, 295, 1164, 11, 264, 4705, 885, 264, 4705, 11, 51762], "temperature": 0.0, "avg_logprob": -0.10969773928324382, "compression_ratio": 1.630188679245283, "no_speech_prob": 0.0023960729595273733}, {"id": 632, "seek": 156816, "start": 1568.16, "end": 1571.8000000000002, "text": " what do people want, they want their anime girlfriend.", "tokens": [50364, 437, 360, 561, 528, 11, 436, 528, 641, 12435, 10369, 13, 50546], "temperature": 0.0, "avg_logprob": -0.13033084308399873, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.0009694982436485589}, {"id": 633, "seek": 156816, "start": 1571.8000000000002, "end": 1575.68, "text": " And this one group, they did a really good job", "tokens": [50546, 400, 341, 472, 1594, 11, 436, 630, 257, 534, 665, 1691, 50740], "temperature": 0.0, "avg_logprob": -0.13033084308399873, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.0009694982436485589}, {"id": 634, "seek": 156816, "start": 1575.68, "end": 1579.0400000000002, "text": " of using GPT-3 in this experimental discord chat", "tokens": [50740, 295, 1228, 26039, 51, 12, 18, 294, 341, 17069, 32989, 5081, 50908], "temperature": 0.0, "avg_logprob": -0.13033084308399873, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.0009694982436485589}, {"id": 635, "seek": 156816, "start": 1579.0400000000002, "end": 1583.5400000000002, "text": " to approximate the personality of this character.", "tokens": [50908, 281, 30874, 264, 9033, 295, 341, 2517, 13, 51133], "temperature": 0.0, "avg_logprob": -0.13033084308399873, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.0009694982436485589}, {"id": 636, "seek": 156816, "start": 1584.52, "end": 1586.66, "text": " And of course, if you've got a character,", "tokens": [51182, 400, 295, 1164, 11, 498, 291, 600, 658, 257, 2517, 11, 51289], "temperature": 0.0, "avg_logprob": -0.13033084308399873, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.0009694982436485589}, {"id": 637, "seek": 156816, "start": 1586.66, "end": 1589.68, "text": " there's plenty of text data about that character's dialogue,", "tokens": [51289, 456, 311, 7140, 295, 2487, 1412, 466, 300, 2517, 311, 10221, 11, 51440], "temperature": 0.0, "avg_logprob": -0.13033084308399873, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.0009694982436485589}, {"id": 638, "seek": 156816, "start": 1589.68, "end": 1590.64, "text": " their personality.", "tokens": [51440, 641, 9033, 13, 51488], "temperature": 0.0, "avg_logprob": -0.13033084308399873, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.0009694982436485589}, {"id": 639, "seek": 156816, "start": 1590.64, "end": 1592.68, "text": " And so this chatbot was able to emulate", "tokens": [51488, 400, 370, 341, 5081, 18870, 390, 1075, 281, 45497, 51590], "temperature": 0.0, "avg_logprob": -0.13033084308399873, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.0009694982436485589}, {"id": 640, "seek": 156816, "start": 1592.68, "end": 1595.96, "text": " this anime character really well.", "tokens": [51590, 341, 12435, 2517, 534, 731, 13, 51754], "temperature": 0.0, "avg_logprob": -0.13033084308399873, "compression_ratio": 1.6851063829787234, "no_speech_prob": 0.0009694982436485589}, {"id": 641, "seek": 159596, "start": 1595.96, "end": 1598.48, "text": " And one of the guys told me, he's like,", "tokens": [50364, 400, 472, 295, 264, 1074, 1907, 385, 11, 415, 311, 411, 11, 50490], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 642, "seek": 159596, "start": 1598.48, "end": 1601.28, "text": " we didn't expect this, but our fake girlfriend", "tokens": [50490, 321, 994, 380, 2066, 341, 11, 457, 527, 7592, 10369, 50630], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 643, "seek": 159596, "start": 1601.28, "end": 1604.2, "text": " requires as much emotional labor as a real girl.", "tokens": [50630, 7029, 382, 709, 6863, 5938, 382, 257, 957, 2013, 13, 50776], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 644, "seek": 159596, "start": 1605.64, "end": 1609.0, "text": " So it forced them, even though they hadn't had", "tokens": [50848, 407, 309, 7579, 552, 11, 754, 1673, 436, 8782, 380, 632, 51016], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 645, "seek": 159596, "start": 1609.0, "end": 1611.48, "text": " real girlfriends, I don't know, maybe some of them had,", "tokens": [51016, 957, 46558, 11, 286, 500, 380, 458, 11, 1310, 512, 295, 552, 632, 11, 51140], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 646, "seek": 159596, "start": 1611.48, "end": 1614.76, "text": " but they made the observation that GPT-3", "tokens": [51140, 457, 436, 1027, 264, 14816, 300, 26039, 51, 12, 18, 51304], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 647, "seek": 159596, "start": 1614.76, "end": 1617.92, "text": " can approximate emotional conflict", "tokens": [51304, 393, 30874, 6863, 6596, 51462], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 648, "seek": 159596, "start": 1617.92, "end": 1620.56, "text": " and can force you to learn to communicate better.", "tokens": [51462, 293, 393, 3464, 291, 281, 1466, 281, 7890, 1101, 13, 51594], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 649, "seek": 159596, "start": 1620.56, "end": 1623.0, "text": " And so they did all kinds of experiments in this discord", "tokens": [51594, 400, 370, 436, 630, 439, 3685, 295, 12050, 294, 341, 32989, 51716], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 650, "seek": 159596, "start": 1623.0, "end": 1625.44, "text": " and this chat development where they said,", "tokens": [51716, 293, 341, 5081, 3250, 689, 436, 848, 11, 51838], "temperature": 0.0, "avg_logprob": -0.14064093105128553, "compression_ratio": 1.6338028169014085, "no_speech_prob": 0.0006877059349790215}, {"id": 651, "seek": 162544, "start": 1625.44, "end": 1629.96, "text": " okay, let's have a channel where this chatbot", "tokens": [50364, 1392, 11, 718, 311, 362, 257, 2269, 689, 341, 5081, 18870, 50590], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 652, "seek": 162544, "start": 1629.96, "end": 1631.88, "text": " is gonna pretend to be angry at us", "tokens": [50590, 307, 799, 11865, 281, 312, 6884, 412, 505, 50686], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 653, "seek": 162544, "start": 1631.88, "end": 1633.44, "text": " and we have to calm her down.", "tokens": [50686, 293, 321, 362, 281, 7151, 720, 760, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 654, "seek": 162544, "start": 1633.44, "end": 1637.56, "text": " And so there was a learning exercise on both sides.", "tokens": [50764, 400, 370, 456, 390, 257, 2539, 5380, 322, 1293, 4881, 13, 50970], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 655, "seek": 162544, "start": 1637.56, "end": 1639.0800000000002, "text": " So if you have a hostile chatbot,", "tokens": [50970, 407, 498, 291, 362, 257, 27312, 5081, 18870, 11, 51046], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 656, "seek": 162544, "start": 1639.0800000000002, "end": 1640.72, "text": " it can pretend to be hostile", "tokens": [51046, 309, 393, 11865, 281, 312, 27312, 51128], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 657, "seek": 162544, "start": 1640.72, "end": 1642.52, "text": " and you can learn to communicate better.", "tokens": [51128, 293, 291, 393, 1466, 281, 7890, 1101, 13, 51218], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 658, "seek": 162544, "start": 1642.52, "end": 1644.96, "text": " Or there was another one where it was really supportive.", "tokens": [51218, 1610, 456, 390, 1071, 472, 689, 309, 390, 534, 14435, 13, 51340], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 659, "seek": 162544, "start": 1644.96, "end": 1646.16, "text": " So if you're having a bad day,", "tokens": [51340, 407, 498, 291, 434, 1419, 257, 1578, 786, 11, 51400], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 660, "seek": 162544, "start": 1646.16, "end": 1648.3600000000001, "text": " you could go vent about your day and it was,", "tokens": [51400, 291, 727, 352, 6931, 466, 428, 786, 293, 309, 390, 11, 51510], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 661, "seek": 162544, "start": 1648.3600000000001, "end": 1651.04, "text": " they're there, it'll be okay, I'm here for you.", "tokens": [51510, 436, 434, 456, 11, 309, 603, 312, 1392, 11, 286, 478, 510, 337, 291, 13, 51644], "temperature": 0.0, "avg_logprob": -0.12304857850984763, "compression_ratio": 1.74609375, "no_speech_prob": 0.0031704872380942106}, {"id": 662, "seek": 165104, "start": 1652.04, "end": 1655.0, "text": " Yeah, so you could definitely,", "tokens": [50414, 865, 11, 370, 291, 727, 2138, 11, 50562], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 663, "seek": 165104, "start": 1655.0, "end": 1657.56, "text": " GPT-3 definitely has that capacity.", "tokens": [50562, 26039, 51, 12, 18, 2138, 575, 300, 6042, 13, 50690], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 664, "seek": 165104, "start": 1657.56, "end": 1660.72, "text": " And then, you know, if you integrate that into tools,", "tokens": [50690, 400, 550, 11, 291, 458, 11, 498, 291, 13365, 300, 666, 3873, 11, 50848], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 665, "seek": 165104, "start": 1660.72, "end": 1662.8, "text": " that emotional intelligence into tools,", "tokens": [50848, 300, 6863, 7599, 666, 3873, 11, 50952], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 666, "seek": 165104, "start": 1662.8, "end": 1664.44, "text": " it can also coach, right?", "tokens": [50952, 309, 393, 611, 6560, 11, 558, 30, 51034], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 667, "seek": 165104, "start": 1664.44, "end": 1665.28, "text": " It can easily coach.", "tokens": [51034, 467, 393, 3612, 6560, 13, 51076], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 668, "seek": 165104, "start": 1665.28, "end": 1667.24, "text": " It's like, well, you maybe shouldn't have said that.", "tokens": [51076, 467, 311, 411, 11, 731, 11, 291, 1310, 4659, 380, 362, 848, 300, 13, 51174], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 669, "seek": 165104, "start": 1667.24, "end": 1668.48, "text": " You know, that was hurtful.", "tokens": [51174, 509, 458, 11, 300, 390, 4607, 906, 13, 51236], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 670, "seek": 165104, "start": 1668.48, "end": 1672.04, "text": " And then, or, you know, that was not polite.", "tokens": [51236, 400, 550, 11, 420, 11, 291, 458, 11, 300, 390, 406, 25171, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 671, "seek": 165104, "start": 1672.04, "end": 1672.8799999999999, "text": " Cause it can detect that.", "tokens": [51414, 10865, 309, 393, 5531, 300, 13, 51456], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 672, "seek": 165104, "start": 1672.8799999999999, "end": 1676.48, "text": " It can detect those qualitative types of output and input.", "tokens": [51456, 467, 393, 5531, 729, 31312, 3467, 295, 5598, 293, 4846, 13, 51636], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 673, "seek": 165104, "start": 1676.48, "end": 1679.52, "text": " And, you know, you can say be gentle about,", "tokens": [51636, 400, 11, 291, 458, 11, 291, 393, 584, 312, 6424, 466, 11, 51788], "temperature": 0.0, "avg_logprob": -0.18312345630061017, "compression_ratio": 1.7769230769230768, "no_speech_prob": 0.001548402477055788}, {"id": 674, "seek": 167952, "start": 1679.52, "end": 1681.12, "text": " you know, correcting the end user.", "tokens": [50364, 291, 458, 11, 47032, 264, 917, 4195, 13, 50444], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 675, "seek": 167952, "start": 1681.12, "end": 1683.72, "text": " Because of course, GPT-3 is infinitely patient.", "tokens": [50444, 1436, 295, 1164, 11, 26039, 51, 12, 18, 307, 36227, 4537, 13, 50574], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 676, "seek": 167952, "start": 1683.72, "end": 1685.36, "text": " It's as patient as you program it to be.", "tokens": [50574, 467, 311, 382, 4537, 382, 291, 1461, 309, 281, 312, 13, 50656], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 677, "seek": 167952, "start": 1685.36, "end": 1686.2, "text": " It doesn't care.", "tokens": [50656, 467, 1177, 380, 1127, 13, 50698], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 678, "seek": 167952, "start": 1686.2, "end": 1687.44, "text": " It doesn't actually get upset.", "tokens": [50698, 467, 1177, 380, 767, 483, 8340, 13, 50760], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 679, "seek": 167952, "start": 1687.44, "end": 1689.44, "text": " It could pretend to be upset.", "tokens": [50760, 467, 727, 11865, 281, 312, 8340, 13, 50860], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 680, "seek": 167952, "start": 1689.44, "end": 1691.8, "text": " But the human emotion is real.", "tokens": [50860, 583, 264, 1952, 8913, 307, 957, 13, 50978], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 681, "seek": 167952, "start": 1691.8, "end": 1693.18, "text": " I actually wrote about that in my book.", "tokens": [50978, 286, 767, 4114, 466, 300, 294, 452, 1446, 13, 51047], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 682, "seek": 167952, "start": 1693.18, "end": 1695.92, "text": " One of the key dangers of these technologies", "tokens": [51047, 1485, 295, 264, 2141, 27701, 295, 613, 7943, 51184], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 683, "seek": 167952, "start": 1695.92, "end": 1698.8799999999999, "text": " is what's called a parasocial relationship.", "tokens": [51184, 307, 437, 311, 1219, 257, 21012, 78, 1013, 2480, 13, 51332], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 684, "seek": 167952, "start": 1698.8799999999999, "end": 1701.04, "text": " So a parasocial relationship is,", "tokens": [51332, 407, 257, 21012, 78, 1013, 2480, 307, 11, 51440], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 685, "seek": 167952, "start": 1701.04, "end": 1703.16, "text": " the most common example is when you've got like", "tokens": [51440, 264, 881, 2689, 1365, 307, 562, 291, 600, 658, 411, 51546], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 686, "seek": 167952, "start": 1703.16, "end": 1704.92, "text": " a fan of a celebrity.", "tokens": [51546, 257, 3429, 295, 257, 18597, 13, 51634], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 687, "seek": 167952, "start": 1704.92, "end": 1706.76, "text": " The fan feels like they know the celebrity,", "tokens": [51634, 440, 3429, 3417, 411, 436, 458, 264, 18597, 11, 51726], "temperature": 0.0, "avg_logprob": -0.11179161071777344, "compression_ratio": 1.727891156462585, "no_speech_prob": 0.0018670711433514953}, {"id": 688, "seek": 170676, "start": 1706.76, "end": 1709.72, "text": " but the celebrity doesn't know that the person exists.", "tokens": [50364, 457, 264, 18597, 1177, 380, 458, 300, 264, 954, 8198, 13, 50512], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 689, "seek": 170676, "start": 1709.72, "end": 1711.28, "text": " And in the same way, GPT-3,", "tokens": [50512, 400, 294, 264, 912, 636, 11, 26039, 51, 12, 18, 11, 50590], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 690, "seek": 170676, "start": 1711.28, "end": 1713.56, "text": " no matter how sophisticated the chatbot is,", "tokens": [50590, 572, 1871, 577, 16950, 264, 5081, 18870, 307, 11, 50704], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 691, "seek": 170676, "start": 1713.56, "end": 1714.96, "text": " it doesn't know that you exist.", "tokens": [50704, 309, 1177, 380, 458, 300, 291, 2514, 13, 50774], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 692, "seek": 170676, "start": 1714.96, "end": 1716.24, "text": " It's not a person.", "tokens": [50774, 467, 311, 406, 257, 954, 13, 50838], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 693, "seek": 170676, "start": 1716.24, "end": 1718.0, "text": " It might feel like a person to you.", "tokens": [50838, 467, 1062, 841, 411, 257, 954, 281, 291, 13, 50926], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 694, "seek": 170676, "start": 1718.0, "end": 1719.56, "text": " It might react to you like a person,", "tokens": [50926, 467, 1062, 4515, 281, 291, 411, 257, 954, 11, 51004], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 695, "seek": 170676, "start": 1719.56, "end": 1721.08, "text": " but that's only by design.", "tokens": [51004, 457, 300, 311, 787, 538, 1715, 13, 51080], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 696, "seek": 170676, "start": 1721.08, "end": 1725.68, "text": " So that is actually like ethically, legally, morally.", "tokens": [51080, 407, 300, 307, 767, 411, 6468, 984, 11, 21106, 11, 38622, 13, 51310], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 697, "seek": 170676, "start": 1725.68, "end": 1728.48, "text": " That's one of the pitfalls that we'll need to be aware of.", "tokens": [51310, 663, 311, 472, 295, 264, 10147, 18542, 300, 321, 603, 643, 281, 312, 3650, 295, 13, 51450], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 698, "seek": 170676, "start": 1728.48, "end": 1731.92, "text": " And of course, open AI has use cases.", "tokens": [51450, 400, 295, 1164, 11, 1269, 7318, 575, 764, 3331, 13, 51622], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 699, "seek": 170676, "start": 1731.92, "end": 1735.48, "text": " And, you know, things that are high-risk use cases,", "tokens": [51622, 400, 11, 291, 458, 11, 721, 300, 366, 1090, 12, 33263, 764, 3331, 11, 51800], "temperature": 0.0, "avg_logprob": -0.1039616803865175, "compression_ratio": 1.797752808988764, "no_speech_prob": 0.0005192137905396521}, {"id": 700, "seek": 173548, "start": 1735.48, "end": 1738.8, "text": " such as emotional chatbots, are banned, right?", "tokens": [50364, 1270, 382, 6863, 5081, 65, 1971, 11, 366, 19564, 11, 558, 30, 50530], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 701, "seek": 173548, "start": 1738.8, "end": 1741.4, "text": " For that specific reason.", "tokens": [50530, 1171, 300, 2685, 1778, 13, 50660], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 702, "seek": 173548, "start": 1741.4, "end": 1742.8600000000001, "text": " So you can do it with research,", "tokens": [50660, 407, 291, 393, 360, 309, 365, 2132, 11, 50733], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 703, "seek": 173548, "start": 1742.8600000000001, "end": 1744.04, "text": " but you can't go live with it.", "tokens": [50733, 457, 291, 393, 380, 352, 1621, 365, 309, 13, 50792], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 704, "seek": 173548, "start": 1744.04, "end": 1746.6, "text": " You can't do a product that, you know,", "tokens": [50792, 509, 393, 380, 360, 257, 1674, 300, 11, 291, 458, 11, 50920], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 705, "seek": 173548, "start": 1746.6, "end": 1748.4, "text": " is going to be an AI girlfriend.", "tokens": [50920, 307, 516, 281, 312, 364, 7318, 10369, 13, 51010], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 706, "seek": 173548, "start": 1750.16, "end": 1752.72, "text": " So that's a great, it's a great anecdote.", "tokens": [51098, 407, 300, 311, 257, 869, 11, 309, 311, 257, 869, 49845, 13, 51226], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 707, "seek": 173548, "start": 1752.72, "end": 1755.04, "text": " Like certainly it feels real, right?", "tokens": [51226, 1743, 3297, 309, 3417, 957, 11, 558, 30, 51342], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 708, "seek": 173548, "start": 1755.04, "end": 1759.64, "text": " Certainly it has some capacity at understand something.", "tokens": [51342, 16628, 309, 575, 512, 6042, 412, 1223, 746, 13, 51572], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 709, "seek": 173548, "start": 1759.64, "end": 1762.2, "text": " To some level, however you define understanding.", "tokens": [51572, 1407, 512, 1496, 11, 4461, 291, 6964, 3701, 13, 51700], "temperature": 0.0, "avg_logprob": -0.22616323960565887, "compression_ratio": 1.6359832635983265, "no_speech_prob": 0.00023047196737024933}, {"id": 710, "seek": 176220, "start": 1763.2, "end": 1767.0, "text": " I think that the writing though,", "tokens": [50414, 286, 519, 300, 264, 3579, 1673, 11, 50604], "temperature": 0.0, "avg_logprob": -0.19839767976240677, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.00109837984200567}, {"id": 711, "seek": 176220, "start": 1767.0, "end": 1770.2, "text": " relationship is really interesting, right?", "tokens": [50604, 2480, 307, 534, 1880, 11, 558, 30, 50764], "temperature": 0.0, "avg_logprob": -0.19839767976240677, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.00109837984200567}, {"id": 712, "seek": 176220, "start": 1770.2, "end": 1774.28, "text": " Like in a way, you are empathizing with GPT-3", "tokens": [50764, 1743, 294, 257, 636, 11, 291, 366, 27155, 3319, 365, 26039, 51, 12, 18, 50968], "temperature": 0.0, "avg_logprob": -0.19839767976240677, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.00109837984200567}, {"id": 713, "seek": 176220, "start": 1774.28, "end": 1776.16, "text": " when you're writing a prompt,", "tokens": [50968, 562, 291, 434, 3579, 257, 12391, 11, 51062], "temperature": 0.0, "avg_logprob": -0.19839767976240677, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.00109837984200567}, {"id": 714, "seek": 176220, "start": 1776.16, "end": 1779.1200000000001, "text": " so that it will tap into its empathy", "tokens": [51062, 370, 300, 309, 486, 5119, 666, 1080, 18701, 51210], "temperature": 0.0, "avg_logprob": -0.19839767976240677, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.00109837984200567}, {"id": 715, "seek": 176220, "start": 1779.1200000000001, "end": 1781.64, "text": " and write something for your audience.", "tokens": [51210, 293, 2464, 746, 337, 428, 4034, 13, 51336], "temperature": 0.0, "avg_logprob": -0.19839767976240677, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.00109837984200567}, {"id": 716, "seek": 176220, "start": 1781.64, "end": 1785.24, "text": " So essentially there's like two levels of empathy.", "tokens": [51336, 407, 4476, 456, 311, 411, 732, 4358, 295, 18701, 13, 51516], "temperature": 0.0, "avg_logprob": -0.19839767976240677, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.00109837984200567}, {"id": 717, "seek": 176220, "start": 1785.24, "end": 1790.24, "text": " Like you're almost outsourcing empathy to it.", "tokens": [51516, 1743, 291, 434, 1920, 14758, 41849, 18701, 281, 309, 13, 51766], "temperature": 0.0, "avg_logprob": -0.19839767976240677, "compression_ratio": 1.5355450236966826, "no_speech_prob": 0.00109837984200567}, {"id": 718, "seek": 179024, "start": 1791.08, "end": 1793.32, "text": " To empathize with who your audience is", "tokens": [50406, 1407, 27155, 1125, 365, 567, 428, 4034, 307, 50518], "temperature": 0.0, "avg_logprob": -0.2294187443230742, "compression_ratio": 1.5229357798165137, "no_speech_prob": 0.003271820954978466}, {"id": 719, "seek": 179024, "start": 1793.32, "end": 1795.16, "text": " to write something on your behalf.", "tokens": [50518, 281, 2464, 746, 322, 428, 9490, 13, 50610], "temperature": 0.0, "avg_logprob": -0.2294187443230742, "compression_ratio": 1.5229357798165137, "no_speech_prob": 0.003271820954978466}, {"id": 720, "seek": 179024, "start": 1795.16, "end": 1797.48, "text": " And so anyways, it's just interesting", "tokens": [50610, 400, 370, 13448, 11, 309, 311, 445, 1880, 50726], "temperature": 0.0, "avg_logprob": -0.2294187443230742, "compression_ratio": 1.5229357798165137, "no_speech_prob": 0.003271820954978466}, {"id": 721, "seek": 179024, "start": 1797.48, "end": 1799.56, "text": " that the relationship going on here.", "tokens": [50726, 300, 264, 2480, 516, 322, 510, 13, 50830], "temperature": 0.0, "avg_logprob": -0.2294187443230742, "compression_ratio": 1.5229357798165137, "no_speech_prob": 0.003271820954978466}, {"id": 722, "seek": 179024, "start": 1801.4, "end": 1804.44, "text": " So, and I agree with you like the,", "tokens": [50922, 407, 11, 293, 286, 3986, 365, 291, 411, 264, 11, 51074], "temperature": 0.0, "avg_logprob": -0.2294187443230742, "compression_ratio": 1.5229357798165137, "no_speech_prob": 0.003271820954978466}, {"id": 723, "seek": 179024, "start": 1804.44, "end": 1809.08, "text": " this isn't a safety ethical kind of concern", "tokens": [51074, 341, 1943, 380, 257, 4514, 18890, 733, 295, 3136, 51306], "temperature": 0.0, "avg_logprob": -0.2294187443230742, "compression_ratio": 1.5229357798165137, "no_speech_prob": 0.003271820954978466}, {"id": 724, "seek": 179024, "start": 1809.08, "end": 1811.36, "text": " that is worth more policy discussion.", "tokens": [51306, 300, 307, 3163, 544, 3897, 5017, 13, 51420], "temperature": 0.0, "avg_logprob": -0.2294187443230742, "compression_ratio": 1.5229357798165137, "no_speech_prob": 0.003271820954978466}, {"id": 725, "seek": 179024, "start": 1812.64, "end": 1815.48, "text": " So one article that I'm working on now", "tokens": [51484, 407, 472, 7222, 300, 286, 478, 1364, 322, 586, 51626], "temperature": 0.0, "avg_logprob": -0.2294187443230742, "compression_ratio": 1.5229357798165137, "no_speech_prob": 0.003271820954978466}, {"id": 726, "seek": 179024, "start": 1815.48, "end": 1817.84, "text": " is because of Instruct GPT,", "tokens": [51626, 307, 570, 295, 2730, 1757, 26039, 51, 11, 51744], "temperature": 0.0, "avg_logprob": -0.2294187443230742, "compression_ratio": 1.5229357798165137, "no_speech_prob": 0.003271820954978466}, {"id": 727, "seek": 181784, "start": 1817.84, "end": 1821.6399999999999, "text": " the article is literally called, is prompt writing over?", "tokens": [50364, 264, 7222, 307, 3736, 1219, 11, 307, 12391, 3579, 670, 30, 50554], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 728, "seek": 181784, "start": 1821.6399999999999, "end": 1823.8799999999999, "text": " And obviously that's sort of click baity,", "tokens": [50554, 400, 2745, 300, 311, 1333, 295, 2052, 16865, 88, 11, 50666], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 729, "seek": 181784, "start": 1823.8799999999999, "end": 1826.52, "text": " like prompt design, is it over?", "tokens": [50666, 411, 12391, 1715, 11, 307, 309, 670, 30, 50798], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 730, "seek": 181784, "start": 1826.52, "end": 1829.6799999999998, "text": " You mentioned, the principles are still the same", "tokens": [50798, 509, 2835, 11, 264, 9156, 366, 920, 264, 912, 50956], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 731, "seek": 181784, "start": 1829.6799999999998, "end": 1833.56, "text": " and important, just very briefly, what are your thoughts?", "tokens": [50956, 293, 1021, 11, 445, 588, 10515, 11, 437, 366, 428, 4598, 30, 51150], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 732, "seek": 181784, "start": 1833.56, "end": 1837.28, "text": " Where does Instruct GPT, how does that affect", "tokens": [51150, 2305, 775, 2730, 1757, 26039, 51, 11, 577, 775, 300, 3345, 51336], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 733, "seek": 181784, "start": 1837.28, "end": 1840.84, "text": " the art of prompt design, maybe the science of it?", "tokens": [51336, 264, 1523, 295, 12391, 1715, 11, 1310, 264, 3497, 295, 309, 30, 51514], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 734, "seek": 181784, "start": 1840.84, "end": 1842.4399999999998, "text": " And especially keeping in mind", "tokens": [51514, 400, 2318, 5145, 294, 1575, 51594], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 735, "seek": 181784, "start": 1842.4399999999998, "end": 1844.3999999999999, "text": " where all of this stuff is going.", "tokens": [51594, 689, 439, 295, 341, 1507, 307, 516, 13, 51692], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 736, "seek": 181784, "start": 1844.3999999999999, "end": 1846.0, "text": " Yeah, so there's, I see it going", "tokens": [51692, 865, 11, 370, 456, 311, 11, 286, 536, 309, 516, 51772], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 737, "seek": 181784, "start": 1846.0, "end": 1847.24, "text": " in a few different directions.", "tokens": [51772, 294, 257, 1326, 819, 11095, 13, 51834], "temperature": 0.0, "avg_logprob": -0.18474654545859684, "compression_ratio": 1.618881118881119, "no_speech_prob": 0.001000003656372428}, {"id": 738, "seek": 184724, "start": 1848.2, "end": 1852.92, "text": " One is there are multiple language models coming out,", "tokens": [50412, 1485, 307, 456, 366, 3866, 2856, 5245, 1348, 484, 11, 50648], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 739, "seek": 184724, "start": 1852.92, "end": 1855.36, "text": " which don't have the Instruct series, right?", "tokens": [50648, 597, 500, 380, 362, 264, 2730, 1757, 2638, 11, 558, 30, 50770], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 740, "seek": 184724, "start": 1855.36, "end": 1856.92, "text": " A lot of them are more general purpose,", "tokens": [50770, 316, 688, 295, 552, 366, 544, 2674, 4334, 11, 50848], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 741, "seek": 184724, "start": 1856.92, "end": 1859.16, "text": " kind of back to basics vanilla.", "tokens": [50848, 733, 295, 646, 281, 14688, 17528, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 742, "seek": 184724, "start": 1859.16, "end": 1861.28, "text": " So I think that having good prompts", "tokens": [50960, 407, 286, 519, 300, 1419, 665, 41095, 51066], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 743, "seek": 184724, "start": 1861.28, "end": 1862.18, "text": " will kind of stick around", "tokens": [51066, 486, 733, 295, 2897, 926, 51111], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 744, "seek": 184724, "start": 1862.18, "end": 1864.52, "text": " as long as there are large language models.", "tokens": [51111, 382, 938, 382, 456, 366, 2416, 2856, 5245, 13, 51228], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 745, "seek": 184724, "start": 1864.52, "end": 1867.6, "text": " I think that there will always be versions of,", "tokens": [51228, 286, 519, 300, 456, 486, 1009, 312, 9606, 295, 11, 51382], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 746, "seek": 184724, "start": 1867.6, "end": 1870.28, "text": " whether it's GPTJ or, what was it?", "tokens": [51382, 1968, 309, 311, 26039, 51, 41, 420, 11, 437, 390, 309, 30, 51516], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 747, "seek": 184724, "start": 1870.28, "end": 1873.1200000000001, "text": " Megatron was one of the other ones that just came out", "tokens": [51516, 9986, 267, 2044, 390, 472, 295, 264, 661, 2306, 300, 445, 1361, 484, 51658], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 748, "seek": 184724, "start": 1873.1200000000001, "end": 1875.28, "text": " that don't have the Instruct series, right?", "tokens": [51658, 300, 500, 380, 362, 264, 2730, 1757, 2638, 11, 558, 30, 51766], "temperature": 0.0, "avg_logprob": -0.1244732066988945, "compression_ratio": 1.7882352941176471, "no_speech_prob": 0.0002304639492649585}, {"id": 749, "seek": 187528, "start": 1875.28, "end": 1878.48, "text": " Because Instruct, that's a specific service", "tokens": [50364, 1436, 2730, 1757, 11, 300, 311, 257, 2685, 2643, 50524], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 750, "seek": 187528, "start": 1878.48, "end": 1880.2, "text": " offered by OpenAI.", "tokens": [50524, 8059, 538, 7238, 48698, 13, 50610], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 751, "seek": 187528, "start": 1880.2, "end": 1884.76, "text": " When Microsoft and Amazon, or I guess Microsoft has GPT3,", "tokens": [50610, 1133, 8116, 293, 6795, 11, 420, 286, 2041, 8116, 575, 26039, 51, 18, 11, 50838], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 752, "seek": 187528, "start": 1884.76, "end": 1887.08, "text": " but when like Amazon and Google,", "tokens": [50838, 457, 562, 411, 6795, 293, 3329, 11, 50954], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 753, "seek": 187528, "start": 1887.08, "end": 1889.04, "text": " when they come out with their competitors,", "tokens": [50954, 562, 436, 808, 484, 365, 641, 18333, 11, 51052], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 754, "seek": 187528, "start": 1889.04, "end": 1891.76, "text": " their Instruct series, if they come out with one,", "tokens": [51052, 641, 2730, 1757, 2638, 11, 498, 436, 808, 484, 365, 472, 11, 51188], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 755, "seek": 187528, "start": 1891.76, "end": 1894.8, "text": " might not be the same, it might not perform the same.", "tokens": [51188, 1062, 406, 312, 264, 912, 11, 309, 1062, 406, 2042, 264, 912, 13, 51340], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 756, "seek": 187528, "start": 1894.8, "end": 1897.6, "text": " And so in order to have your apps be portable,", "tokens": [51340, 400, 370, 294, 1668, 281, 362, 428, 7733, 312, 21800, 11, 51480], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 757, "seek": 187528, "start": 1897.6, "end": 1899.48, "text": " you might need to keep in mind", "tokens": [51480, 291, 1062, 643, 281, 1066, 294, 1575, 51574], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 758, "seek": 187528, "start": 1899.48, "end": 1901.76, "text": " that you're gonna need to write general purpose prompts", "tokens": [51574, 300, 291, 434, 799, 643, 281, 2464, 2674, 4334, 41095, 51688], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 759, "seek": 187528, "start": 1901.76, "end": 1903.92, "text": " that can be used on different models.", "tokens": [51688, 300, 393, 312, 1143, 322, 819, 5245, 13, 51796], "temperature": 0.0, "avg_logprob": -0.10955066233873367, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.0025505474768579006}, {"id": 760, "seek": 190392, "start": 1903.92, "end": 1906.8400000000001, "text": " So that's one key to your,", "tokens": [50364, 407, 300, 311, 472, 2141, 281, 428, 11, 50510], "temperature": 0.0, "avg_logprob": -0.1401893252418155, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00039198657032102346}, {"id": 761, "seek": 190392, "start": 1906.8400000000001, "end": 1909.3200000000002, "text": " or one answer to your question is,", "tokens": [50510, 420, 472, 1867, 281, 428, 1168, 307, 11, 50634], "temperature": 0.0, "avg_logprob": -0.1401893252418155, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00039198657032102346}, {"id": 762, "seek": 190392, "start": 1909.3200000000002, "end": 1911.8000000000002, "text": " we need to be cognizant of,", "tokens": [50634, 321, 643, 281, 312, 11786, 590, 394, 295, 11, 50758], "temperature": 0.0, "avg_logprob": -0.1401893252418155, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00039198657032102346}, {"id": 763, "seek": 190392, "start": 1911.8000000000002, "end": 1913.88, "text": " how is this landscape gonna evolve?", "tokens": [50758, 577, 307, 341, 9661, 799, 16693, 30, 50862], "temperature": 0.0, "avg_logprob": -0.1401893252418155, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00039198657032102346}, {"id": 764, "seek": 190392, "start": 1913.88, "end": 1917.48, "text": " Because certainly OpenAI and GPT3 are way ahead of the curve", "tokens": [50862, 1436, 3297, 7238, 48698, 293, 26039, 51, 18, 366, 636, 2286, 295, 264, 7605, 51042], "temperature": 0.0, "avg_logprob": -0.1401893252418155, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00039198657032102346}, {"id": 765, "seek": 190392, "start": 1917.48, "end": 1922.48, "text": " in terms of sophistication of their API and their service.", "tokens": [51042, 294, 2115, 295, 15572, 399, 295, 641, 9362, 293, 641, 2643, 13, 51292], "temperature": 0.0, "avg_logprob": -0.1401893252418155, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00039198657032102346}, {"id": 766, "seek": 190392, "start": 1923.52, "end": 1925.72, "text": " But that's not gonna last forever.", "tokens": [51344, 583, 300, 311, 406, 799, 1036, 5680, 13, 51454], "temperature": 0.0, "avg_logprob": -0.1401893252418155, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00039198657032102346}, {"id": 767, "seek": 190392, "start": 1926.6000000000001, "end": 1929.8400000000001, "text": " So another thing is, with fine tuning,", "tokens": [51498, 407, 1071, 551, 307, 11, 365, 2489, 15164, 11, 51660], "temperature": 0.0, "avg_logprob": -0.1401893252418155, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00039198657032102346}, {"id": 768, "seek": 190392, "start": 1929.8400000000001, "end": 1932.24, "text": " you almost don't even need prompts, right?", "tokens": [51660, 291, 1920, 500, 380, 754, 643, 41095, 11, 558, 30, 51780], "temperature": 0.0, "avg_logprob": -0.1401893252418155, "compression_ratio": 1.5210084033613445, "no_speech_prob": 0.00039198657032102346}, {"id": 769, "seek": 193224, "start": 1932.24, "end": 1936.2, "text": " So on the one hand, there's different services,", "tokens": [50364, 407, 322, 264, 472, 1011, 11, 456, 311, 819, 3328, 11, 50562], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 770, "seek": 193224, "start": 1936.2, "end": 1938.04, "text": " different products, different platforms.", "tokens": [50562, 819, 3383, 11, 819, 9473, 13, 50654], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 771, "seek": 193224, "start": 1938.04, "end": 1939.92, "text": " So you might need to be portable,", "tokens": [50654, 407, 291, 1062, 643, 281, 312, 21800, 11, 50748], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 772, "seek": 193224, "start": 1939.92, "end": 1942.32, "text": " but with fine tuning, where you have,", "tokens": [50748, 457, 365, 2489, 15164, 11, 689, 291, 362, 11, 50868], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 773, "seek": 193224, "start": 1942.32, "end": 1944.72, "text": " you say, here's an input, I want this output,", "tokens": [50868, 291, 584, 11, 510, 311, 364, 4846, 11, 286, 528, 341, 5598, 11, 50988], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 774, "seek": 193224, "start": 1944.72, "end": 1945.8, "text": " and you don't need any prompt.", "tokens": [50988, 293, 291, 500, 380, 643, 604, 12391, 13, 51042], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 775, "seek": 193224, "start": 1945.8, "end": 1948.6, "text": " You just say, given this input, generate this output,", "tokens": [51042, 509, 445, 584, 11, 2212, 341, 4846, 11, 8460, 341, 5598, 11, 51182], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 776, "seek": 193224, "start": 1948.6, "end": 1950.96, "text": " go figure out how to do that.", "tokens": [51182, 352, 2573, 484, 577, 281, 360, 300, 13, 51300], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 777, "seek": 193224, "start": 1950.96, "end": 1954.96, "text": " So with fine tuning, I think that they will kind of", "tokens": [51300, 407, 365, 2489, 15164, 11, 286, 519, 300, 436, 486, 733, 295, 51500], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 778, "seek": 193224, "start": 1954.96, "end": 1958.04, "text": " really diverge and become entirely different disciplines.", "tokens": [51500, 534, 18558, 432, 293, 1813, 7696, 819, 21919, 13, 51654], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 779, "seek": 193224, "start": 1958.04, "end": 1961.56, "text": " I think that that's probably the two primary directions", "tokens": [51654, 286, 519, 300, 300, 311, 1391, 264, 732, 6194, 11095, 51830], "temperature": 0.0, "avg_logprob": -0.11777918155376728, "compression_ratio": 1.8037037037037038, "no_speech_prob": 0.0002531115023884922}, {"id": 780, "seek": 196156, "start": 1961.56, "end": 1963.0, "text": " that I see it going from here.", "tokens": [50364, 300, 286, 536, 309, 516, 490, 510, 13, 50436], "temperature": 0.0, "avg_logprob": -0.1918036142985026, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0011154169915243983}, {"id": 781, "seek": 196156, "start": 1965.8799999999999, "end": 1969.48, "text": " I see, and yeah, those are great points.", "tokens": [50580, 286, 536, 11, 293, 1338, 11, 729, 366, 869, 2793, 13, 50760], "temperature": 0.0, "avg_logprob": -0.1918036142985026, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0011154169915243983}, {"id": 782, "seek": 196156, "start": 1969.48, "end": 1972.28, "text": " And just as a small note,", "tokens": [50760, 400, 445, 382, 257, 1359, 3637, 11, 50900], "temperature": 0.0, "avg_logprob": -0.1918036142985026, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0011154169915243983}, {"id": 783, "seek": 196156, "start": 1972.28, "end": 1974.96, "text": " I had put out this question as well to Twitter,", "tokens": [50900, 286, 632, 829, 484, 341, 1168, 382, 731, 281, 5794, 11, 51034], "temperature": 0.0, "avg_logprob": -0.1918036142985026, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0011154169915243983}, {"id": 784, "seek": 196156, "start": 1974.96, "end": 1977.44, "text": " and shout out to Fred Zimmerman.", "tokens": [51034, 293, 8043, 484, 281, 10112, 37243, 1601, 13, 51158], "temperature": 0.0, "avg_logprob": -0.1918036142985026, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0011154169915243983}, {"id": 785, "seek": 196156, "start": 1977.44, "end": 1979.9199999999998, "text": " He had a great point as well that he wishes", "tokens": [51158, 634, 632, 257, 869, 935, 382, 731, 300, 415, 15065, 51282], "temperature": 0.0, "avg_logprob": -0.1918036142985026, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0011154169915243983}, {"id": 786, "seek": 196156, "start": 1979.9199999999998, "end": 1984.48, "text": " there was more visibility into the exact prompts OpenAI use", "tokens": [51282, 456, 390, 544, 19883, 666, 264, 1900, 41095, 7238, 48698, 764, 51510], "temperature": 0.0, "avg_logprob": -0.1918036142985026, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0011154169915243983}, {"id": 787, "seek": 196156, "start": 1984.48, "end": 1988.2, "text": " to fine tune for the Instruct series.", "tokens": [51510, 281, 2489, 10864, 337, 264, 2730, 1757, 2638, 13, 51696], "temperature": 0.0, "avg_logprob": -0.1918036142985026, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0011154169915243983}, {"id": 788, "seek": 196156, "start": 1988.2, "end": 1991.12, "text": " Because it's actually unclear what areas", "tokens": [51696, 1436, 309, 311, 767, 25636, 437, 3179, 51842], "temperature": 0.0, "avg_logprob": -0.1918036142985026, "compression_ratio": 1.5560344827586208, "no_speech_prob": 0.0011154169915243983}, {"id": 789, "seek": 199112, "start": 1991.12, "end": 1994.12, "text": " is it really good at, what areas are safer,", "tokens": [50364, 307, 309, 534, 665, 412, 11, 437, 3179, 366, 15856, 11, 50514], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 790, "seek": 199112, "start": 1995.08, "end": 1998.08, "text": " and does it maybe adversely affect", "tokens": [50562, 293, 775, 309, 1310, 17641, 736, 3345, 50712], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 791, "seek": 199112, "start": 1998.08, "end": 2000.12, "text": " some prompts you may be working on, right?", "tokens": [50712, 512, 41095, 291, 815, 312, 1364, 322, 11, 558, 30, 50814], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 792, "seek": 199112, "start": 2000.12, "end": 2001.9599999999998, "text": " Yeah, that's fair.", "tokens": [50814, 865, 11, 300, 311, 3143, 13, 50906], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 793, "seek": 199112, "start": 2001.9599999999998, "end": 2004.9199999999998, "text": " Yeah, my thoughts, I'm gonna put them in the piece,", "tokens": [50906, 865, 11, 452, 4598, 11, 286, 478, 799, 829, 552, 294, 264, 2522, 11, 51054], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 794, "seek": 199112, "start": 2004.9199999999998, "end": 2006.4799999999998, "text": " but my thoughts are I certainly think", "tokens": [51054, 457, 452, 4598, 366, 286, 3297, 519, 51132], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 795, "seek": 199112, "start": 2006.4799999999998, "end": 2009.12, "text": " for first timers, Instruct is the way to go.", "tokens": [51132, 337, 700, 524, 433, 11, 2730, 1757, 307, 264, 636, 281, 352, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 796, "seek": 199112, "start": 2009.12, "end": 2011.84, "text": " And especially if it's your first time", "tokens": [51264, 400, 2318, 498, 309, 311, 428, 700, 565, 51400], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 797, "seek": 199112, "start": 2011.84, "end": 2013.6399999999999, "text": " ever using any of these things,", "tokens": [51400, 1562, 1228, 604, 295, 613, 721, 11, 51490], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 798, "seek": 199112, "start": 2014.9199999999998, "end": 2017.4399999999998, "text": " you just try it, it doesn't work,", "tokens": [51554, 291, 445, 853, 309, 11, 309, 1177, 380, 589, 11, 51680], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 799, "seek": 199112, "start": 2017.4399999999998, "end": 2018.8799999999999, "text": " and if you're lucky you might hear,", "tokens": [51680, 293, 498, 291, 434, 6356, 291, 1062, 1568, 11, 51752], "temperature": 0.0, "avg_logprob": -0.15836824689592635, "compression_ratio": 1.6377952755905512, "no_speech_prob": 0.0020500009413808584}, {"id": 800, "seek": 201888, "start": 2018.88, "end": 2022.4, "text": " there's this thing called prompt engineering, right?", "tokens": [50364, 456, 311, 341, 551, 1219, 12391, 7043, 11, 558, 30, 50540], "temperature": 0.0, "avg_logprob": -0.13621141308936002, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002713806927204132}, {"id": 801, "seek": 201888, "start": 2022.4, "end": 2024.2, "text": " And for first timers, they're not interested", "tokens": [50540, 400, 337, 700, 524, 433, 11, 436, 434, 406, 3102, 50630], "temperature": 0.0, "avg_logprob": -0.13621141308936002, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002713806927204132}, {"id": 802, "seek": 201888, "start": 2024.2, "end": 2026.88, "text": " in learning a whole art and discipline", "tokens": [50630, 294, 2539, 257, 1379, 1523, 293, 13635, 50764], "temperature": 0.0, "avg_logprob": -0.13621141308936002, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002713806927204132}, {"id": 803, "seek": 201888, "start": 2026.88, "end": 2029.16, "text": " when they first use it.", "tokens": [50764, 562, 436, 700, 764, 309, 13, 50878], "temperature": 0.0, "avg_logprob": -0.13621141308936002, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002713806927204132}, {"id": 804, "seek": 201888, "start": 2029.16, "end": 2031.64, "text": " And so InstructGPT is really exciting in that way.", "tokens": [50878, 400, 370, 2730, 1757, 38, 47, 51, 307, 534, 4670, 294, 300, 636, 13, 51002], "temperature": 0.0, "avg_logprob": -0.13621141308936002, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002713806927204132}, {"id": 805, "seek": 201888, "start": 2031.64, "end": 2035.8400000000001, "text": " And of course, anything which aligns AI models", "tokens": [51002, 400, 295, 1164, 11, 1340, 597, 7975, 82, 7318, 5245, 51212], "temperature": 0.0, "avg_logprob": -0.13621141308936002, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002713806927204132}, {"id": 806, "seek": 201888, "start": 2035.8400000000001, "end": 2040.8400000000001, "text": " with safe ethical human values is a net win for everybody.", "tokens": [51212, 365, 3273, 18890, 1952, 4190, 307, 257, 2533, 1942, 337, 2201, 13, 51462], "temperature": 0.0, "avg_logprob": -0.13621141308936002, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002713806927204132}, {"id": 807, "seek": 201888, "start": 2041.44, "end": 2043.3200000000002, "text": " But yeah, I appreciate your point,", "tokens": [51492, 583, 1338, 11, 286, 4449, 428, 935, 11, 51586], "temperature": 0.0, "avg_logprob": -0.13621141308936002, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002713806927204132}, {"id": 808, "seek": 201888, "start": 2043.3200000000002, "end": 2047.68, "text": " especially about do we need prompts in the first place", "tokens": [51586, 2318, 466, 360, 321, 643, 41095, 294, 264, 700, 1081, 51804], "temperature": 0.0, "avg_logprob": -0.13621141308936002, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.002713806927204132}, {"id": 809, "seek": 204768, "start": 2047.72, "end": 2051.04, "text": " if we can fine tune and get the outcomes we want.", "tokens": [50366, 498, 321, 393, 2489, 10864, 293, 483, 264, 10070, 321, 528, 13, 50532], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 810, "seek": 204768, "start": 2051.04, "end": 2053.96, "text": " That's a really, really important point, I hope.", "tokens": [50532, 663, 311, 257, 534, 11, 534, 1021, 935, 11, 286, 1454, 13, 50678], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 811, "seek": 204768, "start": 2053.96, "end": 2057.8, "text": " Dave, you've spent facts, I was learning so much.", "tokens": [50678, 11017, 11, 291, 600, 4418, 9130, 11, 286, 390, 2539, 370, 709, 13, 50870], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 812, "seek": 204768, "start": 2057.8, "end": 2060.64, "text": " Actually, I appreciate it.", "tokens": [50870, 5135, 11, 286, 4449, 309, 13, 51012], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 813, "seek": 204768, "start": 2060.64, "end": 2061.64, "text": " You're welcome.", "tokens": [51012, 509, 434, 2928, 13, 51062], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 814, "seek": 204768, "start": 2061.64, "end": 2063.8, "text": " Happy to have you.", "tokens": [51062, 8277, 281, 362, 291, 13, 51170], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 815, "seek": 204768, "start": 2063.8, "end": 2066.92, "text": " How are you finding open AI, fine tuning?", "tokens": [51170, 1012, 366, 291, 5006, 1269, 7318, 11, 2489, 15164, 30, 51326], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 816, "seek": 204768, "start": 2066.92, "end": 2068.84, "text": " Do you have any heuristics from the whole experience?", "tokens": [51326, 1144, 291, 362, 604, 415, 374, 6006, 490, 264, 1379, 1752, 30, 51422], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 817, "seek": 204768, "start": 2068.84, "end": 2071.04, "text": " And by the way, I encourage everybody,", "tokens": [51422, 400, 538, 264, 636, 11, 286, 5373, 2201, 11, 51532], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 818, "seek": 204768, "start": 2071.04, "end": 2072.2000000000003, "text": " if there's one thing you should do,", "tokens": [51532, 498, 456, 311, 472, 551, 291, 820, 360, 11, 51590], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 819, "seek": 204768, "start": 2072.2000000000003, "end": 2074.2400000000002, "text": " go on the open AI community forums,", "tokens": [51590, 352, 322, 264, 1269, 7318, 1768, 26998, 11, 51692], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 820, "seek": 204768, "start": 2074.2400000000002, "end": 2076.6800000000003, "text": " look up David, look up his handle,", "tokens": [51692, 574, 493, 4389, 11, 574, 493, 702, 4813, 11, 51814], "temperature": 0.0, "avg_logprob": -0.25839366206416375, "compression_ratio": 1.6028368794326242, "no_speech_prob": 0.008842038922011852}, {"id": 821, "seek": 207668, "start": 2076.68, "end": 2078.16, "text": " and read a lot of his posts,", "tokens": [50364, 293, 1401, 257, 688, 295, 702, 12300, 11, 50438], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 822, "seek": 207668, "start": 2078.16, "end": 2081.0, "text": " because a lot of his knowledge is not just helpful,", "tokens": [50438, 570, 257, 688, 295, 702, 3601, 307, 406, 445, 4961, 11, 50580], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 823, "seek": 207668, "start": 2081.0, "end": 2082.64, "text": " he shared a lot of insights there,", "tokens": [50580, 415, 5507, 257, 688, 295, 14310, 456, 11, 50662], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 824, "seek": 207668, "start": 2082.64, "end": 2085.44, "text": " but it's in written form in the best format", "tokens": [50662, 457, 309, 311, 294, 3720, 1254, 294, 264, 1151, 7877, 50802], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 825, "seek": 207668, "start": 2085.44, "end": 2088.3999999999996, "text": " where it's there for the ages for everyone to learn from.", "tokens": [50802, 689, 309, 311, 456, 337, 264, 12357, 337, 1518, 281, 1466, 490, 13, 50950], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 826, "seek": 207668, "start": 2088.3999999999996, "end": 2089.96, "text": " But anyways, how are you finding it?", "tokens": [50950, 583, 13448, 11, 577, 366, 291, 5006, 309, 30, 51028], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 827, "seek": 207668, "start": 2089.96, "end": 2092.68, "text": " What were the lessons from that whole process for you?", "tokens": [51028, 708, 645, 264, 8820, 490, 300, 1379, 1399, 337, 291, 30, 51164], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 828, "seek": 207668, "start": 2092.68, "end": 2095.3999999999996, "text": " Well, so I'm hoping GPT-4 has integrated everything", "tokens": [51164, 1042, 11, 370, 286, 478, 7159, 26039, 51, 12, 19, 575, 10919, 1203, 51300], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 829, "seek": 207668, "start": 2095.3999999999996, "end": 2097.16, "text": " that I've said about AI and AGI,", "tokens": [51300, 300, 286, 600, 848, 466, 7318, 293, 316, 26252, 11, 51388], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 830, "seek": 207668, "start": 2097.16, "end": 2098.8399999999997, "text": " and so that way it'll just be baked in.", "tokens": [51388, 293, 370, 300, 636, 309, 603, 445, 312, 19453, 294, 13, 51472], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 831, "seek": 207668, "start": 2098.8399999999997, "end": 2100.8399999999997, "text": " And so GPT-4 will be ready to go", "tokens": [51472, 400, 370, 26039, 51, 12, 19, 486, 312, 1919, 281, 352, 51572], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 832, "seek": 207668, "start": 2100.8399999999997, "end": 2102.7999999999997, "text": " with everything that I've come up with.", "tokens": [51572, 365, 1203, 300, 286, 600, 808, 493, 365, 13, 51670], "temperature": 0.0, "avg_logprob": -0.13917463938395183, "compression_ratio": 1.7244897959183674, "no_speech_prob": 0.0016998203936964273}, {"id": 833, "seek": 210280, "start": 2103.76, "end": 2105.8, "text": " So, but yeah, so fine tuning.", "tokens": [50412, 407, 11, 457, 1338, 11, 370, 2489, 15164, 13, 50514], "temperature": 0.0, "avg_logprob": -0.18578945693149362, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.002250642515718937}, {"id": 834, "seek": 210280, "start": 2107.0800000000004, "end": 2111.5600000000004, "text": " So first and foremost, fine tuning is almost miraculous,", "tokens": [50578, 407, 700, 293, 18864, 11, 2489, 15164, 307, 1920, 41101, 11, 50802], "temperature": 0.0, "avg_logprob": -0.18578945693149362, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.002250642515718937}, {"id": 835, "seek": 210280, "start": 2111.5600000000004, "end": 2116.5600000000004, "text": " as powerful as GPT-3 was fresh out of the box.", "tokens": [50802, 382, 4005, 382, 26039, 51, 12, 18, 390, 4451, 484, 295, 264, 2424, 13, 51052], "temperature": 0.0, "avg_logprob": -0.18578945693149362, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.002250642515718937}, {"id": 836, "seek": 210280, "start": 2117.1200000000003, "end": 2121.6400000000003, "text": " Fine tuning to me adds a whole other layer of capabilities.", "tokens": [51080, 12024, 15164, 281, 385, 10860, 257, 1379, 661, 4583, 295, 10862, 13, 51306], "temperature": 0.0, "avg_logprob": -0.18578945693149362, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.002250642515718937}, {"id": 837, "seek": 210280, "start": 2121.6400000000003, "end": 2125.36, "text": " So for instance,", "tokens": [51306, 407, 337, 5197, 11, 51492], "temperature": 0.0, "avg_logprob": -0.18578945693149362, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.002250642515718937}, {"id": 838, "seek": 210280, "start": 2125.36, "end": 2127.5600000000004, "text": " when I was working on my cognitive architecture,", "tokens": [51492, 562, 286, 390, 1364, 322, 452, 15605, 9482, 11, 51602], "temperature": 0.0, "avg_logprob": -0.18578945693149362, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.002250642515718937}, {"id": 839, "seek": 210280, "start": 2127.5600000000004, "end": 2129.7200000000003, "text": " which is called natural language cognitive architecture,", "tokens": [51602, 597, 307, 1219, 3303, 2856, 15605, 9482, 11, 51710], "temperature": 0.0, "avg_logprob": -0.18578945693149362, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.002250642515718937}, {"id": 840, "seek": 210280, "start": 2129.7200000000003, "end": 2131.44, "text": " this was before fine tuning was available.", "tokens": [51710, 341, 390, 949, 2489, 15164, 390, 2435, 13, 51796], "temperature": 0.0, "avg_logprob": -0.18578945693149362, "compression_ratio": 1.6318181818181818, "no_speech_prob": 0.002250642515718937}, {"id": 841, "seek": 213144, "start": 2131.48, "end": 2133.6, "text": " So I had to do prompt engineering", "tokens": [50366, 407, 286, 632, 281, 360, 12391, 7043, 50472], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 842, "seek": 213144, "start": 2133.6, "end": 2135.68, "text": " for every cognitive function.", "tokens": [50472, 337, 633, 15605, 2445, 13, 50576], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 843, "seek": 213144, "start": 2135.68, "end": 2138.48, "text": " So for instance, I had a cognitive function for recall.", "tokens": [50576, 407, 337, 5197, 11, 286, 632, 257, 15605, 2445, 337, 9901, 13, 50716], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 844, "seek": 213144, "start": 2138.48, "end": 2143.28, "text": " So I had a GPT-3 prompt that was meant to go find memories.", "tokens": [50716, 407, 286, 632, 257, 26039, 51, 12, 18, 12391, 300, 390, 4140, 281, 352, 915, 8495, 13, 50956], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 845, "seek": 213144, "start": 2143.28, "end": 2145.32, "text": " I had another GPT-3 prompt that was,", "tokens": [50956, 286, 632, 1071, 26039, 51, 12, 18, 12391, 300, 390, 11, 51058], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 846, "seek": 213144, "start": 2145.32, "end": 2146.56, "text": " as you mentioned earlier,", "tokens": [51058, 382, 291, 2835, 3071, 11, 51120], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 847, "seek": 213144, "start": 2146.56, "end": 2148.16, "text": " meant for empathy to generate,", "tokens": [51120, 4140, 337, 18701, 281, 8460, 11, 51200], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 848, "seek": 213144, "start": 2148.16, "end": 2150.6, "text": " okay, how is my audience feeling?", "tokens": [51200, 1392, 11, 577, 307, 452, 4034, 2633, 30, 51322], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 849, "seek": 213144, "start": 2150.6, "end": 2152.8, "text": " What should I do in response?", "tokens": [51322, 708, 820, 286, 360, 294, 4134, 30, 51432], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 850, "seek": 213144, "start": 2152.8, "end": 2157.0, "text": " All told, I had about 28 different prompts", "tokens": [51432, 1057, 1907, 11, 286, 632, 466, 7562, 819, 41095, 51642], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 851, "seek": 213144, "start": 2157.0, "end": 2158.96, "text": " that I had to engineer.", "tokens": [51642, 300, 286, 632, 281, 11403, 13, 51740], "temperature": 0.0, "avg_logprob": -0.10656430986192492, "compression_ratio": 1.7264957264957266, "no_speech_prob": 0.005908616818487644}, {"id": 852, "seek": 215896, "start": 2159.12, "end": 2161.96, "text": " And that was a pain, right?", "tokens": [50372, 400, 300, 390, 257, 1822, 11, 558, 30, 50514], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 853, "seek": 215896, "start": 2161.96, "end": 2163.68, "text": " Whereas what I'm working on now", "tokens": [50514, 13813, 437, 286, 478, 1364, 322, 586, 50600], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 854, "seek": 215896, "start": 2163.68, "end": 2165.76, "text": " is converting each one of those prompts", "tokens": [50600, 307, 29942, 1184, 472, 295, 729, 41095, 50704], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 855, "seek": 215896, "start": 2165.76, "end": 2167.6, "text": " into a fine tuned model.", "tokens": [50704, 666, 257, 2489, 10870, 2316, 13, 50796], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 856, "seek": 215896, "start": 2167.6, "end": 2169.84, "text": " So that rather than having to do prompt engineering", "tokens": [50796, 407, 300, 2831, 813, 1419, 281, 360, 12391, 7043, 50908], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 857, "seek": 215896, "start": 2169.84, "end": 2172.28, "text": " with only three examples,", "tokens": [50908, 365, 787, 1045, 5110, 11, 51030], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 858, "seek": 215896, "start": 2172.28, "end": 2177.08, "text": " I can give each model 100 examples, a thousand examples,", "tokens": [51030, 286, 393, 976, 1184, 2316, 2319, 5110, 11, 257, 4714, 5110, 11, 51270], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 859, "seek": 215896, "start": 2177.08, "end": 2179.0, "text": " which means that it'll get even better", "tokens": [51270, 597, 1355, 300, 309, 603, 483, 754, 1101, 51366], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 860, "seek": 215896, "start": 2179.0, "end": 2182.04, "text": " at handling diverse situations.", "tokens": [51366, 412, 13175, 9521, 6851, 13, 51518], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 861, "seek": 215896, "start": 2182.04, "end": 2183.0, "text": " And so for instance,", "tokens": [51518, 400, 370, 337, 5197, 11, 51566], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 862, "seek": 215896, "start": 2183.0, "end": 2185.04, "text": " one of the first fine tuned models that I did", "tokens": [51566, 472, 295, 264, 700, 2489, 10870, 5245, 300, 286, 630, 51668], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 863, "seek": 215896, "start": 2185.04, "end": 2187.36, "text": " was a question asking model.", "tokens": [51668, 390, 257, 1168, 3365, 2316, 13, 51784], "temperature": 0.0, "avg_logprob": -0.10514542931004574, "compression_ratio": 1.6904761904761905, "no_speech_prob": 0.0016480543417856097}, {"id": 864, "seek": 218736, "start": 2187.4, "end": 2189.28, "text": " And so what I did was I took,", "tokens": [50366, 400, 370, 437, 286, 630, 390, 286, 1890, 11, 50460], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 865, "seek": 218736, "start": 2189.28, "end": 2191.76, "text": " I took context or prompts", "tokens": [50460, 286, 1890, 4319, 420, 41095, 50584], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 866, "seek": 218736, "start": 2191.76, "end": 2193.56, "text": " from a bunch of different sources.", "tokens": [50584, 490, 257, 3840, 295, 819, 7139, 13, 50674], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 867, "seek": 218736, "start": 2193.56, "end": 2195.84, "text": " I downloaded a bunch of Reddit posts.", "tokens": [50674, 286, 21748, 257, 3840, 295, 32210, 12300, 13, 50788], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 868, "seek": 218736, "start": 2195.84, "end": 2197.48, "text": " Well, I downloaded it from a dataset", "tokens": [50788, 1042, 11, 286, 21748, 309, 490, 257, 28872, 50870], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 869, "seek": 218736, "start": 2197.48, "end": 2200.28, "text": " from a, what was it, Kaggle.", "tokens": [50870, 490, 257, 11, 437, 390, 309, 11, 48751, 22631, 13, 51010], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 870, "seek": 218736, "start": 2200.28, "end": 2202.44, "text": " Kaggle has some really great datasets.", "tokens": [51010, 48751, 22631, 575, 512, 534, 869, 42856, 13, 51118], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 871, "seek": 218736, "start": 2202.44, "end": 2205.04, "text": " So I got stuff from Reddit.", "tokens": [51118, 407, 286, 658, 1507, 490, 32210, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 872, "seek": 218736, "start": 2205.04, "end": 2206.6800000000003, "text": " I got the medical posts.", "tokens": [51248, 286, 658, 264, 4625, 12300, 13, 51330], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 873, "seek": 218736, "start": 2206.6800000000003, "end": 2208.96, "text": " I've got news articles.", "tokens": [51330, 286, 600, 658, 2583, 11290, 13, 51444], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 874, "seek": 218736, "start": 2208.96, "end": 2213.28, "text": " And so I've got this disparate tight set of contexts, right?", "tokens": [51444, 400, 370, 286, 600, 658, 341, 14548, 473, 4524, 992, 295, 30628, 11, 558, 30, 51660], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 875, "seek": 218736, "start": 2213.28, "end": 2216.6, "text": " There's the, I use the Cornell movie dialogue database.", "tokens": [51660, 821, 311, 264, 11, 286, 764, 264, 43257, 3169, 10221, 8149, 13, 51826], "temperature": 0.0, "avg_logprob": -0.1540326396311362, "compression_ratio": 1.721774193548387, "no_speech_prob": 0.0002959027769975364}, {"id": 876, "seek": 221660, "start": 2216.6, "end": 2218.2, "text": " So there's chat logs.", "tokens": [50364, 407, 456, 311, 5081, 20820, 13, 50444], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 877, "seek": 221660, "start": 2218.2, "end": 2220.36, "text": " There's blog posts.", "tokens": [50444, 821, 311, 6968, 12300, 13, 50552], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 878, "seek": 221660, "start": 2220.36, "end": 2223.64, "text": " And what I did was I created a fine tuned dataset", "tokens": [50552, 400, 437, 286, 630, 390, 286, 2942, 257, 2489, 10870, 28872, 50716], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 879, "seek": 221660, "start": 2223.64, "end": 2227.36, "text": " that all it does is you give it any input.", "tokens": [50716, 300, 439, 309, 775, 307, 291, 976, 309, 604, 4846, 13, 50902], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 880, "seek": 221660, "start": 2227.36, "end": 2228.48, "text": " It could be a text message.", "tokens": [50902, 467, 727, 312, 257, 2487, 3636, 13, 50958], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 881, "seek": 221660, "start": 2228.48, "end": 2229.3199999999997, "text": " It could be an email.", "tokens": [50958, 467, 727, 312, 364, 3796, 13, 51000], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 882, "seek": 221660, "start": 2229.3199999999997, "end": 2230.8399999999997, "text": " It could be a blog post, anything.", "tokens": [51000, 467, 727, 312, 257, 6968, 2183, 11, 1340, 13, 51076], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 883, "seek": 221660, "start": 2230.8399999999997, "end": 2233.64, "text": " And all it does is generate questions,", "tokens": [51076, 400, 439, 309, 775, 307, 8460, 1651, 11, 51216], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 884, "seek": 221660, "start": 2233.64, "end": 2236.5, "text": " like follow up questions about that input.", "tokens": [51216, 411, 1524, 493, 1651, 466, 300, 4846, 13, 51359], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 885, "seek": 221660, "start": 2236.5, "end": 2238.64, "text": " And the reason that I did that one", "tokens": [51359, 400, 264, 1778, 300, 286, 630, 300, 472, 51466], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 886, "seek": 221660, "start": 2238.64, "end": 2241.92, "text": " is because asking questions like being curious", "tokens": [51466, 307, 570, 3365, 1651, 411, 885, 6369, 51630], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 887, "seek": 221660, "start": 2241.92, "end": 2245.72, "text": " is one of the key ingredients to real intelligence, right?", "tokens": [51630, 307, 472, 295, 264, 2141, 6952, 281, 957, 7599, 11, 558, 30, 51820], "temperature": 0.0, "avg_logprob": -0.08957659813665575, "compression_ratio": 1.7967479674796747, "no_speech_prob": 0.00015354598872363567}, {"id": 888, "seek": 224572, "start": 2245.72, "end": 2248.0, "text": " That's one of the, like being inquisitive", "tokens": [50364, 663, 311, 472, 295, 264, 11, 411, 885, 13570, 271, 2187, 50478], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 889, "seek": 224572, "start": 2248.0, "end": 2251.9599999999996, "text": " is actually a key indicator of intelligence in children.", "tokens": [50478, 307, 767, 257, 2141, 16961, 295, 7599, 294, 2227, 13, 50676], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 890, "seek": 224572, "start": 2251.9599999999996, "end": 2254.48, "text": " The more curious a child is, generally speaking,", "tokens": [50676, 440, 544, 6369, 257, 1440, 307, 11, 5101, 4124, 11, 50802], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 891, "seek": 224572, "start": 2254.48, "end": 2257.8399999999997, "text": " the higher their IQ is, and also generally speaking,", "tokens": [50802, 264, 2946, 641, 28921, 307, 11, 293, 611, 5101, 4124, 11, 50970], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 892, "seek": 224572, "start": 2257.8399999999997, "end": 2260.68, "text": " the better they do in the long run.", "tokens": [50970, 264, 1101, 436, 360, 294, 264, 938, 1190, 13, 51112], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 893, "seek": 224572, "start": 2260.68, "end": 2264.02, "text": " So I was like, okay, well, curiosity is super important", "tokens": [51112, 407, 286, 390, 411, 11, 1392, 11, 731, 11, 18769, 307, 1687, 1021, 51279], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 894, "seek": 224572, "start": 2264.02, "end": 2265.1, "text": " for intelligence.", "tokens": [51279, 337, 7599, 13, 51333], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 895, "seek": 224572, "start": 2265.1, "end": 2268.08, "text": " So I obviously want, we want AGI to be curious.", "tokens": [51333, 407, 286, 2745, 528, 11, 321, 528, 316, 26252, 281, 312, 6369, 13, 51482], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 896, "seek": 224572, "start": 2268.08, "end": 2269.12, "text": " If it's gonna be intelligent,", "tokens": [51482, 759, 309, 311, 799, 312, 13232, 11, 51534], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 897, "seek": 224572, "start": 2269.12, "end": 2271.3599999999997, "text": " it's gotta be curious, of course.", "tokens": [51534, 309, 311, 3428, 312, 6369, 11, 295, 1164, 13, 51646], "temperature": 0.0, "avg_logprob": -0.12439944380420749, "compression_ratio": 1.7295081967213115, "no_speech_prob": 0.00022337072005029768}, {"id": 898, "seek": 227136, "start": 2271.36, "end": 2276.36, "text": " So, well, what is curiosity if not asking questions?", "tokens": [50364, 407, 11, 731, 11, 437, 307, 18769, 498, 406, 3365, 1651, 30, 50614], "temperature": 0.0, "avg_logprob": -0.14326644826818397, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0009109199745580554}, {"id": 899, "seek": 227136, "start": 2276.52, "end": 2281.2400000000002, "text": " So I fine tuned this model to ask questions.", "tokens": [50622, 407, 286, 2489, 10870, 341, 2316, 281, 1029, 1651, 13, 50858], "temperature": 0.0, "avg_logprob": -0.14326644826818397, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0009109199745580554}, {"id": 900, "seek": 227136, "start": 2281.2400000000002, "end": 2283.32, "text": " And you can put anything into it.", "tokens": [50858, 400, 291, 393, 829, 1340, 666, 309, 13, 50962], "temperature": 0.0, "avg_logprob": -0.14326644826818397, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0009109199745580554}, {"id": 901, "seek": 227136, "start": 2283.32, "end": 2287.1200000000003, "text": " And oh, this, the data is open source.", "tokens": [50962, 400, 1954, 11, 341, 11, 264, 1412, 307, 1269, 4009, 13, 51152], "temperature": 0.0, "avg_logprob": -0.14326644826818397, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0009109199745580554}, {"id": 902, "seek": 227136, "start": 2287.1200000000003, "end": 2288.52, "text": " So I'll send you a link and you can share it", "tokens": [51152, 407, 286, 603, 2845, 291, 257, 2113, 293, 291, 393, 2073, 309, 51222], "temperature": 0.0, "avg_logprob": -0.14326644826818397, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0009109199745580554}, {"id": 903, "seek": 227136, "start": 2288.52, "end": 2291.44, "text": " with your audience, and they can fine tune it themselves", "tokens": [51222, 365, 428, 4034, 11, 293, 436, 393, 2489, 10864, 309, 2969, 51368], "temperature": 0.0, "avg_logprob": -0.14326644826818397, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0009109199745580554}, {"id": 904, "seek": 227136, "start": 2291.44, "end": 2293.44, "text": " or fine tune their own version.", "tokens": [51368, 420, 2489, 10864, 641, 1065, 3037, 13, 51468], "temperature": 0.0, "avg_logprob": -0.14326644826818397, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0009109199745580554}, {"id": 905, "seek": 227136, "start": 2293.44, "end": 2297.04, "text": " But so you can put in, I tried all sorts of things", "tokens": [51468, 583, 370, 291, 393, 829, 294, 11, 286, 3031, 439, 7527, 295, 721, 51648], "temperature": 0.0, "avg_logprob": -0.14326644826818397, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0009109199745580554}, {"id": 906, "seek": 227136, "start": 2297.04, "end": 2297.88, "text": " to test it.", "tokens": [51648, 281, 1500, 309, 13, 51690], "temperature": 0.0, "avg_logprob": -0.14326644826818397, "compression_ratio": 1.6757990867579908, "no_speech_prob": 0.0009109199745580554}, {"id": 907, "seek": 229788, "start": 2298.88, "end": 2301.52, "text": " You know, relationship questions from Reddit", "tokens": [50414, 509, 458, 11, 2480, 1651, 490, 32210, 50546], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 908, "seek": 229788, "start": 2301.52, "end": 2303.6, "text": " and it asked really great follow up questions.", "tokens": [50546, 293, 309, 2351, 534, 869, 1524, 493, 1651, 13, 50650], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 909, "seek": 229788, "start": 2303.6, "end": 2306.28, "text": " Like, have you talked to your partner about this?", "tokens": [50650, 1743, 11, 362, 291, 2825, 281, 428, 4975, 466, 341, 30, 50784], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 910, "seek": 229788, "start": 2306.28, "end": 2307.92, "text": " Have you thought about this?", "tokens": [50784, 3560, 291, 1194, 466, 341, 30, 50866], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 911, "seek": 229788, "start": 2307.92, "end": 2309.6800000000003, "text": " And then I put in an article", "tokens": [50866, 400, 550, 286, 829, 294, 364, 7222, 50954], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 912, "seek": 229788, "start": 2309.6800000000003, "end": 2313.0, "text": " about China's artificial sun nuclear reactor", "tokens": [50954, 466, 3533, 311, 11677, 3295, 8179, 20628, 51120], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 913, "seek": 229788, "start": 2313.0, "end": 2315.12, "text": " and it asked really great follow up questions for that.", "tokens": [51120, 293, 309, 2351, 534, 869, 1524, 493, 1651, 337, 300, 13, 51226], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 914, "seek": 229788, "start": 2315.12, "end": 2316.52, "text": " Like, what is the next step?", "tokens": [51226, 1743, 11, 437, 307, 264, 958, 1823, 30, 51296], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 915, "seek": 229788, "start": 2316.52, "end": 2319.7200000000003, "text": " How did they make these changes?", "tokens": [51296, 1012, 630, 436, 652, 613, 2962, 30, 51456], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 916, "seek": 229788, "start": 2319.7200000000003, "end": 2322.28, "text": " And so I kind of lost my train of thought.", "tokens": [51456, 400, 370, 286, 733, 295, 2731, 452, 3847, 295, 1194, 13, 51584], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 917, "seek": 229788, "start": 2322.28, "end": 2327.28, "text": " Anyways, point being is that fine tuning is phenomenal", "tokens": [51584, 15585, 11, 935, 885, 307, 300, 2489, 15164, 307, 17778, 51834], "temperature": 0.0, "avg_logprob": -0.18160577547752252, "compression_ratio": 1.811023622047244, "no_speech_prob": 0.05334998667240143}, {"id": 918, "seek": 232728, "start": 2327.48, "end": 2332.4, "text": " and it was able to generalize that task of asking questions", "tokens": [50374, 293, 309, 390, 1075, 281, 2674, 1125, 300, 5633, 295, 3365, 1651, 50620], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 919, "seek": 232728, "start": 2332.4, "end": 2333.96, "text": " in response to anything.", "tokens": [50620, 294, 4134, 281, 1340, 13, 50698], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 920, "seek": 232728, "start": 2333.96, "end": 2336.0400000000004, "text": " And that was, that really blew me away.", "tokens": [50698, 400, 300, 390, 11, 300, 534, 19075, 385, 1314, 13, 50802], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 921, "seek": 232728, "start": 2336.0400000000004, "end": 2337.5600000000004, "text": " I kind of stalled after that.", "tokens": [50802, 286, 733, 295, 342, 8907, 934, 300, 13, 50878], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 922, "seek": 232728, "start": 2337.5600000000004, "end": 2339.0, "text": " There's a few fine tuning projects", "tokens": [50878, 821, 311, 257, 1326, 2489, 15164, 4455, 50950], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 923, "seek": 232728, "start": 2339.0, "end": 2341.6800000000003, "text": " that haven't done quite as well.", "tokens": [50950, 300, 2378, 380, 1096, 1596, 382, 731, 13, 51084], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 924, "seek": 232728, "start": 2341.6800000000003, "end": 2343.6800000000003, "text": " So I guess to tie back to your earlier question,", "tokens": [51084, 407, 286, 2041, 281, 7582, 646, 281, 428, 3071, 1168, 11, 51184], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 925, "seek": 232728, "start": 2343.6800000000003, "end": 2345.1600000000003, "text": " like what are the heuristics?", "tokens": [51184, 411, 437, 366, 264, 415, 374, 6006, 30, 51258], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 926, "seek": 232728, "start": 2346.1600000000003, "end": 2350.0800000000004, "text": " The simpler your fine tuning project is, the better.", "tokens": [51308, 440, 18587, 428, 2489, 15164, 1716, 307, 11, 264, 1101, 13, 51504], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 927, "seek": 232728, "start": 2350.0800000000004, "end": 2352.7200000000003, "text": " And I have found that fine tuning works really well", "tokens": [51504, 400, 286, 362, 1352, 300, 2489, 15164, 1985, 534, 731, 51636], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 928, "seek": 232728, "start": 2352.7200000000003, "end": 2354.52, "text": " at generating lists.", "tokens": [51636, 412, 17746, 14511, 13, 51726], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 929, "seek": 232728, "start": 2354.52, "end": 2356.76, "text": " So if you wanted to generate a list of questions,", "tokens": [51726, 407, 498, 291, 1415, 281, 8460, 257, 1329, 295, 1651, 11, 51838], "temperature": 0.0, "avg_logprob": -0.10407503870607332, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.001500386162661016}, {"id": 930, "seek": 235676, "start": 2356.76, "end": 2357.7200000000003, "text": " it's great at that.", "tokens": [50364, 309, 311, 869, 412, 300, 13, 50412], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 931, "seek": 235676, "start": 2357.7200000000003, "end": 2361.5200000000004, "text": " If you wanted to generate a list of possible answers,", "tokens": [50412, 759, 291, 1415, 281, 8460, 257, 1329, 295, 1944, 6338, 11, 50602], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 932, "seek": 235676, "start": 2361.5200000000004, "end": 2365.28, "text": " for instance, if you wanna have a fine tuned chat bot,", "tokens": [50602, 337, 5197, 11, 498, 291, 1948, 362, 257, 2489, 10870, 5081, 10592, 11, 50790], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 933, "seek": 235676, "start": 2365.28, "end": 2366.76, "text": " that it's just gonna say,", "tokens": [50790, 300, 309, 311, 445, 799, 584, 11, 50864], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 934, "seek": 235676, "start": 2366.76, "end": 2369.36, "text": " here's five possible responses, pick one.", "tokens": [50864, 510, 311, 1732, 1944, 13019, 11, 1888, 472, 13, 50994], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 935, "seek": 235676, "start": 2369.36, "end": 2371.2000000000003, "text": " It's really good at that.", "tokens": [50994, 467, 311, 534, 665, 412, 300, 13, 51086], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 936, "seek": 235676, "start": 2371.2000000000003, "end": 2373.6000000000004, "text": " I haven't had a chance,", "tokens": [51086, 286, 2378, 380, 632, 257, 2931, 11, 51206], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 937, "seek": 235676, "start": 2373.6000000000004, "end": 2374.8, "text": " I do have some other ideas", "tokens": [51206, 286, 360, 362, 512, 661, 3487, 51266], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 938, "seek": 235676, "start": 2374.8, "end": 2375.96, "text": " that I haven't had a chance to test.", "tokens": [51266, 300, 286, 2378, 380, 632, 257, 2931, 281, 1500, 13, 51324], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 939, "seek": 235676, "start": 2375.96, "end": 2378.6000000000004, "text": " So unfortunately, I can't speak too much beyond that,", "tokens": [51324, 407, 7015, 11, 286, 393, 380, 1710, 886, 709, 4399, 300, 11, 51456], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 940, "seek": 235676, "start": 2378.6000000000004, "end": 2380.6000000000004, "text": " but it's really great at asking questions.", "tokens": [51456, 457, 309, 311, 534, 869, 412, 3365, 1651, 13, 51556], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 941, "seek": 235676, "start": 2382.96, "end": 2383.8, "text": " That's awesome.", "tokens": [51674, 663, 311, 3476, 13, 51716], "temperature": 0.0, "avg_logprob": -0.15247238339401606, "compression_ratio": 1.726530612244898, "no_speech_prob": 0.00048771078581921756}, {"id": 942, "seek": 238380, "start": 2383.8, "end": 2387.4, "text": " And I think largely the feedback I'm hearing", "tokens": [50364, 400, 286, 519, 11611, 264, 5824, 286, 478, 4763, 50544], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 943, "seek": 238380, "start": 2387.4, "end": 2389.0800000000004, "text": " about fine tuning, I love it.", "tokens": [50544, 466, 2489, 15164, 11, 286, 959, 309, 13, 50628], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 944, "seek": 238380, "start": 2389.0800000000004, "end": 2393.2000000000003, "text": " It was for me, it was as if I rediscovered GPT-3 again.", "tokens": [50628, 467, 390, 337, 385, 11, 309, 390, 382, 498, 286, 2182, 40080, 292, 26039, 51, 12, 18, 797, 13, 50834], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 945, "seek": 238380, "start": 2393.2000000000003, "end": 2395.84, "text": " Like it was that same level of excitement.", "tokens": [50834, 1743, 309, 390, 300, 912, 1496, 295, 14755, 13, 50966], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 946, "seek": 238380, "start": 2395.84, "end": 2400.84, "text": " Part of the reason is so much of what GPT-3 was okay at,", "tokens": [50966, 4100, 295, 264, 1778, 307, 370, 709, 295, 437, 26039, 51, 12, 18, 390, 1392, 412, 11, 51216], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 947, "seek": 238380, "start": 2401.2400000000002, "end": 2403.44, "text": " or like it was sort of out of the question.", "tokens": [51236, 420, 411, 309, 390, 1333, 295, 484, 295, 264, 1168, 13, 51346], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 948, "seek": 238380, "start": 2403.44, "end": 2405.7200000000003, "text": " Now it's back in the picture.", "tokens": [51346, 823, 309, 311, 646, 294, 264, 3036, 13, 51460], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 949, "seek": 238380, "start": 2405.7200000000003, "end": 2407.1600000000003, "text": " Like it's back in the spotlight.", "tokens": [51460, 1743, 309, 311, 646, 294, 264, 24656, 13, 51532], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 950, "seek": 238380, "start": 2407.1600000000003, "end": 2409.84, "text": " It may actually be able to do it with fine tuning.", "tokens": [51532, 467, 815, 767, 312, 1075, 281, 360, 309, 365, 2489, 15164, 13, 51666], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 951, "seek": 238380, "start": 2409.84, "end": 2411.6000000000004, "text": " The biggest criticism was reliability,", "tokens": [51666, 440, 3880, 15835, 390, 24550, 11, 51754], "temperature": 0.0, "avg_logprob": -0.14122769259667212, "compression_ratio": 1.6360153256704981, "no_speech_prob": 0.000803878006990999}, {"id": 952, "seek": 241160, "start": 2411.6, "end": 2413.7999999999997, "text": " especially from a commercial perspective.", "tokens": [50364, 2318, 490, 257, 6841, 4585, 13, 50474], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 953, "seek": 241160, "start": 2413.7999999999997, "end": 2418.2799999999997, "text": " Now we're sort of attacking and sort of peeling away", "tokens": [50474, 823, 321, 434, 1333, 295, 15010, 293, 1333, 295, 39926, 1314, 50698], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 954, "seek": 241160, "start": 2418.2799999999997, "end": 2421.96, "text": " that criticism, that it does improve our reliability.", "tokens": [50698, 300, 15835, 11, 300, 309, 775, 3470, 527, 24550, 13, 50882], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 955, "seek": 241160, "start": 2423.16, "end": 2425.7599999999998, "text": " And I mean, there's other heuristics as well", "tokens": [50942, 400, 286, 914, 11, 456, 311, 661, 415, 374, 6006, 382, 731, 51072], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 956, "seek": 241160, "start": 2425.7599999999998, "end": 2428.12, "text": " in the community forums that you just pick up.", "tokens": [51072, 294, 264, 1768, 26998, 300, 291, 445, 1888, 493, 13, 51190], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 957, "seek": 241160, "start": 2428.12, "end": 2431.08, "text": " So one heuristic, and I can't remember if you shared this,", "tokens": [51190, 407, 472, 415, 374, 3142, 11, 293, 286, 393, 380, 1604, 498, 291, 5507, 341, 11, 51338], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 958, "seek": 241160, "start": 2431.08, "end": 2432.48, "text": " but it was something I picked up", "tokens": [51338, 457, 309, 390, 746, 286, 6183, 493, 51408], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 959, "seek": 241160, "start": 2432.48, "end": 2436.04, "text": " as a little golden nugget in the open-eyed community forums", "tokens": [51408, 382, 257, 707, 9729, 30279, 847, 294, 264, 1269, 12, 37860, 1768, 26998, 51586], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 960, "seek": 241160, "start": 2436.04, "end": 2437.48, "text": " was something about,", "tokens": [51586, 390, 746, 466, 11, 51658], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 961, "seek": 241160, "start": 2437.48, "end": 2439.96, "text": " you do wanna think about the training dataset", "tokens": [51658, 291, 360, 1948, 519, 466, 264, 3097, 28872, 51782], "temperature": 0.0, "avg_logprob": -0.1479891448461709, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0011332998983561993}, {"id": 962, "seek": 243996, "start": 2439.96, "end": 2442.52, "text": " that GPT-3 is itself trained on.", "tokens": [50364, 300, 26039, 51, 12, 18, 307, 2564, 8895, 322, 13, 50492], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 963, "seek": 243996, "start": 2442.52, "end": 2445.4, "text": " And at some point, there's really no point", "tokens": [50492, 400, 412, 512, 935, 11, 456, 311, 534, 572, 935, 50636], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 964, "seek": 243996, "start": 2445.4, "end": 2446.76, "text": " in adding more examples,", "tokens": [50636, 294, 5127, 544, 5110, 11, 50704], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 965, "seek": 243996, "start": 2446.76, "end": 2449.68, "text": " because it's kind of already seen them, right?", "tokens": [50704, 570, 309, 311, 733, 295, 1217, 1612, 552, 11, 558, 30, 50850], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 966, "seek": 243996, "start": 2449.68, "end": 2452.6, "text": " However, and I sort of, in an article,", "tokens": [50850, 2908, 11, 293, 286, 1333, 295, 11, 294, 364, 7222, 11, 50996], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 967, "seek": 243996, "start": 2452.6, "end": 2454.56, "text": " I have pushed this idea that opening eyes should", "tokens": [50996, 286, 362, 9152, 341, 1558, 300, 5193, 2575, 820, 51094], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 968, "seek": 243996, "start": 2454.56, "end": 2456.64, "text": " chat more about their dataset.", "tokens": [51094, 5081, 544, 466, 641, 28872, 13, 51198], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 969, "seek": 243996, "start": 2456.64, "end": 2457.68, "text": " What is the breakdown?", "tokens": [51198, 708, 307, 264, 18188, 30, 51250], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 970, "seek": 243996, "start": 2457.68, "end": 2458.84, "text": " What is it composed of?", "tokens": [51250, 708, 307, 309, 18204, 295, 30, 51308], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 971, "seek": 243996, "start": 2458.84, "end": 2461.2400000000002, "text": " I mean, a lot of this is intellectual property,", "tokens": [51308, 286, 914, 11, 257, 688, 295, 341, 307, 12576, 4707, 11, 51428], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 972, "seek": 243996, "start": 2461.2400000000002, "end": 2462.4, "text": " but I think it could be helpful", "tokens": [51428, 457, 286, 519, 309, 727, 312, 4961, 51486], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 973, "seek": 243996, "start": 2462.4, "end": 2465.6, "text": " for purposes like fine tuning, right?", "tokens": [51486, 337, 9932, 411, 2489, 15164, 11, 558, 30, 51646], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 974, "seek": 243996, "start": 2465.6, "end": 2468.4, "text": " There's other things too with fine tuning and prompts,", "tokens": [51646, 821, 311, 661, 721, 886, 365, 2489, 15164, 293, 41095, 11, 51786], "temperature": 0.0, "avg_logprob": -0.14009388633396314, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0009108236408792436}, {"id": 975, "seek": 246840, "start": 2468.44, "end": 2471.64, "text": " a one heuristic or just a tip", "tokens": [50366, 257, 472, 415, 374, 3142, 420, 445, 257, 4125, 50526], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 976, "seek": 246840, "start": 2471.64, "end": 2475.04, "text": " that people have shared online is it tends to always,", "tokens": [50526, 300, 561, 362, 5507, 2950, 307, 309, 12258, 281, 1009, 11, 50696], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 977, "seek": 246840, "start": 2475.04, "end": 2478.2400000000002, "text": " it tends to always mimic the most recent examples.", "tokens": [50696, 309, 12258, 281, 1009, 31075, 264, 881, 5162, 5110, 13, 50856], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 978, "seek": 246840, "start": 2478.2400000000002, "end": 2480.44, "text": " There's something about the order of the examples,", "tokens": [50856, 821, 311, 746, 466, 264, 1668, 295, 264, 5110, 11, 50966], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 979, "seek": 246840, "start": 2480.44, "end": 2482.2000000000003, "text": " which is really important,", "tokens": [50966, 597, 307, 534, 1021, 11, 51054], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 980, "seek": 246840, "start": 2482.2000000000003, "end": 2485.88, "text": " both for prompt engineering and fine tuning as well.", "tokens": [51054, 1293, 337, 12391, 7043, 293, 2489, 15164, 382, 731, 13, 51238], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 981, "seek": 246840, "start": 2485.88, "end": 2487.76, "text": " And I guess I wrote a whole article", "tokens": [51238, 400, 286, 2041, 286, 4114, 257, 1379, 7222, 51332], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 982, "seek": 246840, "start": 2487.76, "end": 2491.2000000000003, "text": " about how prompt fine tuning could be improved.", "tokens": [51332, 466, 577, 12391, 2489, 15164, 727, 312, 9689, 13, 51504], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 983, "seek": 246840, "start": 2491.2000000000003, "end": 2493.44, "text": " One of the pointers that I just had is right now,", "tokens": [51504, 1485, 295, 264, 44548, 300, 286, 445, 632, 307, 558, 586, 11, 51616], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 984, "seek": 246840, "start": 2493.44, "end": 2495.76, "text": " you can't keep improving on the same model.", "tokens": [51616, 291, 393, 380, 1066, 11470, 322, 264, 912, 2316, 13, 51732], "temperature": 0.0, "avg_logprob": -0.1414147738752694, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.0008557315450161695}, {"id": 985, "seek": 249576, "start": 2495.8, "end": 2498.6000000000004, "text": " You have to retrain on more models.", "tokens": [50366, 509, 362, 281, 1533, 7146, 322, 544, 5245, 13, 50506], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 986, "seek": 249576, "start": 2498.6000000000004, "end": 2502.4, "text": " And yeah, and then the other thing is recently,", "tokens": [50506, 400, 1338, 11, 293, 550, 264, 661, 551, 307, 3938, 11, 50696], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 987, "seek": 249576, "start": 2502.4, "end": 2504.6000000000004, "text": " I was in favor of the pricing of fine tuning.", "tokens": [50696, 286, 390, 294, 2294, 295, 264, 17621, 295, 2489, 15164, 13, 50806], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 988, "seek": 249576, "start": 2504.6000000000004, "end": 2505.88, "text": " Now I'm kind of against it,", "tokens": [50806, 823, 286, 478, 733, 295, 1970, 309, 11, 50870], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 989, "seek": 249576, "start": 2505.88, "end": 2509.1200000000003, "text": " because I'm used to when the program was like free", "tokens": [50870, 570, 286, 478, 1143, 281, 562, 264, 1461, 390, 411, 1737, 51032], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 990, "seek": 249576, "start": 2509.1200000000003, "end": 2510.96, "text": " and you could fine tune as much as you want.", "tokens": [51032, 293, 291, 727, 2489, 10864, 382, 709, 382, 291, 528, 13, 51124], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 991, "seek": 249576, "start": 2510.96, "end": 2512.84, "text": " And now it's like, oh man, I got to pay.", "tokens": [51124, 400, 586, 309, 311, 411, 11, 1954, 587, 11, 286, 658, 281, 1689, 13, 51218], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 992, "seek": 249576, "start": 2512.84, "end": 2513.6800000000003, "text": " Yeah.", "tokens": [51218, 865, 13, 51260], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 993, "seek": 249576, "start": 2513.6800000000003, "end": 2516.2400000000002, "text": " Oh, the costs, especially for David,", "tokens": [51260, 876, 11, 264, 5497, 11, 2318, 337, 4389, 11, 51388], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 994, "seek": 249576, "start": 2516.2400000000002, "end": 2517.6400000000003, "text": " you are catching up a little bit.", "tokens": [51388, 291, 366, 16124, 493, 257, 707, 857, 13, 51458], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 995, "seek": 249576, "start": 2517.6400000000003, "end": 2518.48, "text": " Yeah.", "tokens": [51458, 865, 13, 51500], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 996, "seek": 249576, "start": 2519.7200000000003, "end": 2521.48, "text": " Anyway, so I wanted to shift gears.", "tokens": [51562, 5684, 11, 370, 286, 1415, 281, 5513, 20915, 13, 51650], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 997, "seek": 249576, "start": 2521.48, "end": 2522.5600000000004, "text": " Sure.", "tokens": [51650, 4894, 13, 51704], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 998, "seek": 249576, "start": 2522.5600000000004, "end": 2525.0400000000004, "text": " GitHub co-pilot, really exciting.", "tokens": [51704, 23331, 598, 12, 79, 31516, 11, 534, 4670, 13, 51828], "temperature": 0.0, "avg_logprob": -0.22923926784567636, "compression_ratio": 1.5894736842105264, "no_speech_prob": 0.012817636132240295}, {"id": 999, "seek": 252504, "start": 2525.04, "end": 2526.72, "text": " Have you had access to co-pilot?", "tokens": [50364, 3560, 291, 632, 2105, 281, 598, 12, 79, 31516, 30, 50448], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1000, "seek": 252504, "start": 2526.72, "end": 2527.68, "text": " Yes.", "tokens": [50448, 1079, 13, 50496], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1001, "seek": 252504, "start": 2527.68, "end": 2528.52, "text": " Yeah.", "tokens": [50496, 865, 13, 50538], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1002, "seek": 252504, "start": 2528.52, "end": 2530.52, "text": " Well, not co-pilot, but the Codex.", "tokens": [50538, 1042, 11, 406, 598, 12, 79, 31516, 11, 457, 264, 15549, 87, 13, 50638], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1003, "seek": 252504, "start": 2530.52, "end": 2532.52, "text": " They did give me access to Codex.", "tokens": [50638, 814, 630, 976, 385, 2105, 281, 15549, 87, 13, 50738], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1004, "seek": 252504, "start": 2532.52, "end": 2537.52, "text": " So the reason I'm asking is, I love GitHub co-pilot.", "tokens": [50738, 407, 264, 1778, 286, 478, 3365, 307, 11, 286, 959, 23331, 598, 12, 79, 31516, 13, 50988], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1005, "seek": 252504, "start": 2537.88, "end": 2539.48, "text": " I have a separate podcast episode", "tokens": [51006, 286, 362, 257, 4994, 7367, 3500, 51086], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1006, "seek": 252504, "start": 2539.48, "end": 2541.48, "text": " on my ideas around Codex.", "tokens": [51086, 322, 452, 3487, 926, 15549, 87, 13, 51186], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1007, "seek": 252504, "start": 2541.48, "end": 2543.38, "text": " Unfortunately, I'm not as bullish", "tokens": [51186, 8590, 11, 286, 478, 406, 382, 38692, 51281], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1008, "seek": 252504, "start": 2543.38, "end": 2544.92, "text": " as much as I love the research", "tokens": [51281, 382, 709, 382, 286, 959, 264, 2132, 51358], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1009, "seek": 252504, "start": 2544.92, "end": 2546.8, "text": " as I think it's incredible technology.", "tokens": [51358, 382, 286, 519, 309, 311, 4651, 2899, 13, 51452], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1010, "seek": 252504, "start": 2546.8, "end": 2548.72, "text": " I've congratulated the team", "tokens": [51452, 286, 600, 9774, 6987, 264, 1469, 51548], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1011, "seek": 252504, "start": 2548.72, "end": 2550.84, "text": " and I tried so hard to be nice,", "tokens": [51548, 293, 286, 3031, 370, 1152, 281, 312, 1481, 11, 51654], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1012, "seek": 252504, "start": 2550.84, "end": 2552.92, "text": " even though I'm more on the critical end.", "tokens": [51654, 754, 1673, 286, 478, 544, 322, 264, 4924, 917, 13, 51758], "temperature": 0.0, "avg_logprob": -0.14348627815783863, "compression_ratio": 1.6082089552238805, "no_speech_prob": 0.000487789191538468}, {"id": 1013, "seek": 255292, "start": 2552.92, "end": 2556.32, "text": " I wanted to ask you, how are you finding OpenAI Codex?", "tokens": [50364, 286, 1415, 281, 1029, 291, 11, 577, 366, 291, 5006, 7238, 48698, 15549, 87, 30, 50534], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1014, "seek": 255292, "start": 2556.32, "end": 2559.04, "text": " How can you see it impacting the world?", "tokens": [50534, 1012, 393, 291, 536, 309, 29963, 264, 1002, 30, 50670], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1015, "seek": 255292, "start": 2560.2000000000003, "end": 2561.92, "text": " What are some use cases maybe", "tokens": [50728, 708, 366, 512, 764, 3331, 1310, 50814], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1016, "seek": 255292, "start": 2561.92, "end": 2564.76, "text": " that you found with OpenAI Codex?", "tokens": [50814, 300, 291, 1352, 365, 7238, 48698, 15549, 87, 30, 50956], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1017, "seek": 255292, "start": 2564.76, "end": 2565.84, "text": " What are your thoughts on it?", "tokens": [50956, 708, 366, 428, 4598, 322, 309, 30, 51010], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1018, "seek": 255292, "start": 2565.84, "end": 2567.08, "text": " Where do you think it's going?", "tokens": [51010, 2305, 360, 291, 519, 309, 311, 516, 30, 51072], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1019, "seek": 255292, "start": 2567.08, "end": 2567.92, "text": " Yeah.", "tokens": [51072, 865, 13, 51114], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1020, "seek": 255292, "start": 2567.92, "end": 2571.16, "text": " So, I mean, certainly this is like a world first, right?", "tokens": [51114, 407, 11, 286, 914, 11, 3297, 341, 307, 411, 257, 1002, 700, 11, 558, 30, 51276], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1021, "seek": 255292, "start": 2571.16, "end": 2574.44, "text": " We've never had something that could write code on its own.", "tokens": [51276, 492, 600, 1128, 632, 746, 300, 727, 2464, 3089, 322, 1080, 1065, 13, 51440], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1022, "seek": 255292, "start": 2574.44, "end": 2576.8, "text": " And especially it's text to code.", "tokens": [51440, 400, 2318, 309, 311, 2487, 281, 3089, 13, 51558], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1023, "seek": 255292, "start": 2576.8, "end": 2579.08, "text": " I remember when they first gave me access,", "tokens": [51558, 286, 1604, 562, 436, 700, 2729, 385, 2105, 11, 51672], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1024, "seek": 255292, "start": 2579.08, "end": 2581.84, "text": " because like you mentioned, I'm an active contributor,", "tokens": [51672, 570, 411, 291, 2835, 11, 286, 478, 364, 4967, 42859, 11, 51810], "temperature": 0.0, "avg_logprob": -0.17461556973664658, "compression_ratio": 1.6067796610169491, "no_speech_prob": 0.0011693271808326244}, {"id": 1025, "seek": 258184, "start": 2581.84, "end": 2583.7200000000003, "text": " so they wanted my feedback.", "tokens": [50364, 370, 436, 1415, 452, 5824, 13, 50458], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1026, "seek": 258184, "start": 2583.7200000000003, "end": 2586.28, "text": " And so the first thing I did was I went in", "tokens": [50458, 400, 370, 264, 700, 551, 286, 630, 390, 286, 1437, 294, 50586], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1027, "seek": 258184, "start": 2586.28, "end": 2589.1200000000003, "text": " and I said, write me a Python function", "tokens": [50586, 293, 286, 848, 11, 2464, 385, 257, 15329, 2445, 50728], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1028, "seek": 258184, "start": 2589.1200000000003, "end": 2592.04, "text": " that will download random Reddit posts.", "tokens": [50728, 300, 486, 5484, 4974, 32210, 12300, 13, 50874], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1029, "seek": 258184, "start": 2592.04, "end": 2592.88, "text": " And it did.", "tokens": [50874, 400, 309, 630, 13, 50916], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1030, "seek": 258184, "start": 2592.88, "end": 2594.2400000000002, "text": " It wrote the whole function.", "tokens": [50916, 467, 4114, 264, 1379, 2445, 13, 50984], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1031, "seek": 258184, "start": 2594.2400000000002, "end": 2595.76, "text": " And it did all right.", "tokens": [50984, 400, 309, 630, 439, 558, 13, 51060], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1032, "seek": 258184, "start": 2595.76, "end": 2596.6000000000004, "text": " And I was like, cool.", "tokens": [51060, 400, 286, 390, 411, 11, 1627, 13, 51102], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1033, "seek": 258184, "start": 2596.6000000000004, "end": 2600.6000000000004, "text": " I learned how to access the Reddit API via Codex.", "tokens": [51102, 286, 3264, 577, 281, 2105, 264, 32210, 9362, 5766, 15549, 87, 13, 51302], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1034, "seek": 258184, "start": 2600.6000000000004, "end": 2602.1200000000003, "text": " It's got that built in.", "tokens": [51302, 467, 311, 658, 300, 3094, 294, 13, 51378], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1035, "seek": 258184, "start": 2602.1200000000003, "end": 2604.84, "text": " And I tried to reverse engineer,", "tokens": [51378, 400, 286, 3031, 281, 9943, 11403, 11, 51514], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1036, "seek": 258184, "start": 2604.84, "end": 2607.44, "text": " figure out where it got that code sample from.", "tokens": [51514, 2573, 484, 689, 309, 658, 300, 3089, 6889, 490, 13, 51644], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1037, "seek": 258184, "start": 2607.44, "end": 2611.82, "text": " So, because one of the ethical concerns is,", "tokens": [51644, 407, 11, 570, 472, 295, 264, 18890, 7389, 307, 11, 51863], "temperature": 0.0, "avg_logprob": -0.12966258709247297, "compression_ratio": 1.6240601503759398, "no_speech_prob": 0.0008558011613786221}, {"id": 1038, "seek": 261182, "start": 2611.82, "end": 2614.3, "text": " all right, you create a fine-tuning data set", "tokens": [50364, 439, 558, 11, 291, 1884, 257, 2489, 12, 83, 37726, 1412, 992, 50488], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1039, "seek": 261182, "start": 2614.3, "end": 2616.54, "text": " from public GitHub repositories", "tokens": [50488, 490, 1908, 23331, 22283, 2083, 50600], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1040, "seek": 261182, "start": 2616.54, "end": 2618.78, "text": " and you use that to fine-tune Codex.", "tokens": [50600, 293, 291, 764, 300, 281, 2489, 12, 83, 2613, 15549, 87, 13, 50712], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1041, "seek": 261182, "start": 2618.78, "end": 2621.7000000000003, "text": " Okay, is that legal?", "tokens": [50712, 1033, 11, 307, 300, 5089, 30, 50858], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1042, "seek": 261182, "start": 2621.7000000000003, "end": 2623.7000000000003, "text": " Is it ethical?", "tokens": [50858, 1119, 309, 18890, 30, 50958], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1043, "seek": 261182, "start": 2623.7000000000003, "end": 2626.7400000000002, "text": " I post all my code publicly under the MIT license,", "tokens": [50958, 286, 2183, 439, 452, 3089, 14843, 833, 264, 13100, 10476, 11, 51110], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1044, "seek": 261182, "start": 2626.7400000000002, "end": 2628.38, "text": " so I want it to be used.", "tokens": [51110, 370, 286, 528, 309, 281, 312, 1143, 13, 51192], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1045, "seek": 261182, "start": 2628.38, "end": 2630.1800000000003, "text": " But I don't know if they checked that.", "tokens": [51192, 583, 286, 500, 380, 458, 498, 436, 10033, 300, 13, 51282], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1046, "seek": 261182, "start": 2630.1800000000003, "end": 2632.5800000000004, "text": " And I'm not making an accusation one way or another,", "tokens": [51282, 400, 286, 478, 406, 1455, 364, 11168, 399, 472, 636, 420, 1071, 11, 51402], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1047, "seek": 261182, "start": 2632.5800000000004, "end": 2634.5, "text": " just pointing out that that's a concern.", "tokens": [51402, 445, 12166, 484, 300, 300, 311, 257, 3136, 13, 51498], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1048, "seek": 261182, "start": 2634.5, "end": 2637.5800000000004, "text": " And so I did actually find like one of the lines of code", "tokens": [51498, 400, 370, 286, 630, 767, 915, 411, 472, 295, 264, 3876, 295, 3089, 51652], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1049, "seek": 261182, "start": 2637.5800000000004, "end": 2639.9, "text": " from the function it spit out.", "tokens": [51652, 490, 264, 2445, 309, 22127, 484, 13, 51768], "temperature": 0.0, "avg_logprob": -0.12878750352298513, "compression_ratio": 1.5871886120996441, "no_speech_prob": 0.00124462996609509}, {"id": 1050, "seek": 263990, "start": 2639.9, "end": 2643.06, "text": " I went and found the repo that it had copied from.", "tokens": [50364, 286, 1437, 293, 1352, 264, 49040, 300, 309, 632, 25365, 490, 13, 50522], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1051, "seek": 263990, "start": 2643.06, "end": 2645.6600000000003, "text": " Now granted, some of these things are deterministic.", "tokens": [50522, 823, 12344, 11, 512, 295, 613, 721, 366, 15957, 3142, 13, 50652], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1052, "seek": 263990, "start": 2645.6600000000003, "end": 2650.6600000000003, "text": " So you're gonna get some convergence, right?", "tokens": [50652, 407, 291, 434, 799, 483, 512, 32181, 11, 558, 30, 50902], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1053, "seek": 263990, "start": 2650.7000000000003, "end": 2652.34, "text": " Where multiple people might come up", "tokens": [50904, 2305, 3866, 561, 1062, 808, 493, 50986], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1054, "seek": 263990, "start": 2652.34, "end": 2653.54, "text": " with the same exact line of code,", "tokens": [50986, 365, 264, 912, 1900, 1622, 295, 3089, 11, 51046], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1055, "seek": 263990, "start": 2653.54, "end": 2655.58, "text": " especially something like Python.", "tokens": [51046, 2318, 746, 411, 15329, 13, 51148], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1056, "seek": 263990, "start": 2655.58, "end": 2657.38, "text": " Because Python has the PEP 8,", "tokens": [51148, 1436, 15329, 575, 264, 430, 8929, 1649, 11, 51238], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1057, "seek": 263990, "start": 2657.38, "end": 2659.7000000000003, "text": " the Python enhancement protocol 8.", "tokens": [51238, 264, 15329, 40776, 10336, 1649, 13, 51354], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1058, "seek": 263990, "start": 2659.7000000000003, "end": 2663.1800000000003, "text": " So like, there is a Pythonic way to write that function.", "tokens": [51354, 407, 411, 11, 456, 307, 257, 9953, 392, 11630, 636, 281, 2464, 300, 2445, 13, 51528], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1059, "seek": 263990, "start": 2663.1800000000003, "end": 2666.06, "text": " And so other people might converge on that.", "tokens": [51528, 400, 370, 661, 561, 1062, 41881, 322, 300, 13, 51672], "temperature": 0.0, "avg_logprob": -0.14218781675611222, "compression_ratio": 1.6264591439688716, "no_speech_prob": 0.00036827789153903723}, {"id": 1060, "seek": 266606, "start": 2666.98, "end": 2669.86, "text": " Anyways, but to answer your question about like,", "tokens": [50410, 15585, 11, 457, 281, 1867, 428, 1168, 466, 411, 11, 50554], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1061, "seek": 266606, "start": 2669.86, "end": 2671.66, "text": " what's the future of it?", "tokens": [50554, 437, 311, 264, 2027, 295, 309, 30, 50644], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1062, "seek": 266606, "start": 2671.66, "end": 2675.7, "text": " I think it'll help for novice programmers.", "tokens": [50644, 286, 519, 309, 603, 854, 337, 23883, 573, 41504, 13, 50846], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1063, "seek": 266606, "start": 2675.7, "end": 2677.14, "text": " Certainly it would help someone like me,", "tokens": [50846, 16628, 309, 576, 854, 1580, 411, 385, 11, 50918], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1064, "seek": 266606, "start": 2677.14, "end": 2679.18, "text": " like if I needed to go write a function in C", "tokens": [50918, 411, 498, 286, 2978, 281, 352, 2464, 257, 2445, 294, 383, 51020], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1065, "seek": 266606, "start": 2679.18, "end": 2680.66, "text": " or Perl or something.", "tokens": [51020, 420, 3026, 75, 420, 746, 13, 51094], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1066, "seek": 266606, "start": 2680.66, "end": 2682.1, "text": " Like let's say I got an Arduino", "tokens": [51094, 1743, 718, 311, 584, 286, 658, 364, 39539, 51166], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1067, "seek": 266606, "start": 2682.1, "end": 2685.06, "text": " and like I haven't written C in 15 years.", "tokens": [51166, 293, 411, 286, 2378, 380, 3720, 383, 294, 2119, 924, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1068, "seek": 266606, "start": 2685.06, "end": 2686.34, "text": " So I was like, hey, you know,", "tokens": [51314, 407, 286, 390, 411, 11, 4177, 11, 291, 458, 11, 51378], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1069, "seek": 266606, "start": 2686.34, "end": 2688.58, "text": " write me a function that can do this in Arduino.", "tokens": [51378, 2464, 385, 257, 2445, 300, 393, 360, 341, 294, 39539, 13, 51490], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1070, "seek": 266606, "start": 2688.58, "end": 2689.42, "text": " That'd be great.", "tokens": [51490, 663, 1116, 312, 869, 13, 51532], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1071, "seek": 266606, "start": 2689.42, "end": 2691.7799999999997, "text": " And then I can go clean it up manually.", "tokens": [51532, 400, 550, 286, 393, 352, 2541, 309, 493, 16945, 13, 51650], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1072, "seek": 266606, "start": 2691.7799999999997, "end": 2694.62, "text": " That sort of thing I think it could do okay with,", "tokens": [51650, 663, 1333, 295, 551, 286, 519, 309, 727, 360, 1392, 365, 11, 51792], "temperature": 0.0, "avg_logprob": -0.10557803977914408, "compression_ratio": 1.6241610738255035, "no_speech_prob": 0.0015977213624864817}, {"id": 1073, "seek": 269462, "start": 2694.66, "end": 2697.2599999999998, "text": " is it gonna replace enterprise developers?", "tokens": [50366, 307, 309, 799, 7406, 14132, 8849, 30, 50496], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1074, "seek": 269462, "start": 2698.2599999999998, "end": 2699.98, "text": " Probably not yet.", "tokens": [50546, 9210, 406, 1939, 13, 50632], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1075, "seek": 269462, "start": 2699.98, "end": 2703.2999999999997, "text": " However, now this is where my professional experience", "tokens": [50632, 2908, 11, 586, 341, 307, 689, 452, 4843, 1752, 50798], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1076, "seek": 269462, "start": 2703.2999999999997, "end": 2704.14, "text": " comes in.", "tokens": [50798, 1487, 294, 13, 50840], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1077, "seek": 269462, "start": 2704.14, "end": 2706.06, "text": " So in the DevOps world,", "tokens": [50840, 407, 294, 264, 43051, 1002, 11, 50936], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1078, "seek": 269462, "start": 2706.06, "end": 2709.22, "text": " which is a portmanteau of development and operations,", "tokens": [50936, 597, 307, 257, 2436, 76, 2879, 1459, 295, 3250, 293, 7705, 11, 51094], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1079, "seek": 269462, "start": 2709.22, "end": 2711.8599999999997, "text": " there's all kinds of automation tools, right?", "tokens": [51094, 456, 311, 439, 3685, 295, 17769, 3873, 11, 558, 30, 51226], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1080, "seek": 269462, "start": 2711.8599999999997, "end": 2713.94, "text": " You can automate your test suite.", "tokens": [51226, 509, 393, 31605, 428, 1500, 14205, 13, 51330], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1081, "seek": 269462, "start": 2713.94, "end": 2715.98, "text": " You can automate code integration.", "tokens": [51330, 509, 393, 31605, 3089, 10980, 13, 51432], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1082, "seek": 269462, "start": 2715.98, "end": 2718.18, "text": " There's all sorts of stuff like that.", "tokens": [51432, 821, 311, 439, 7527, 295, 1507, 411, 300, 13, 51542], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1083, "seek": 269462, "start": 2718.18, "end": 2721.1, "text": " So what I suspect might happen", "tokens": [51542, 407, 437, 286, 9091, 1062, 1051, 51688], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1084, "seek": 269462, "start": 2721.1, "end": 2723.66, "text": " is probably one of the most lucrative", "tokens": [51688, 307, 1391, 472, 295, 264, 881, 21296, 30457, 51816], "temperature": 0.0, "avg_logprob": -0.10735523595219165, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0005702450289390981}, {"id": 1085, "seek": 272366, "start": 2723.66, "end": 2727.18, "text": " use cases for Codex would be to generate", "tokens": [50364, 764, 3331, 337, 15549, 87, 576, 312, 281, 8460, 50540], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1086, "seek": 272366, "start": 2727.18, "end": 2729.8199999999997, "text": " or to create a DevOps pipeline tool", "tokens": [50540, 420, 281, 1884, 257, 43051, 15517, 2290, 50672], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1087, "seek": 272366, "start": 2729.8199999999997, "end": 2733.2599999999998, "text": " that will automatically look at those bugs and fix them.", "tokens": [50672, 300, 486, 6772, 574, 412, 729, 15120, 293, 3191, 552, 13, 50844], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1088, "seek": 272366, "start": 2733.2599999999998, "end": 2735.3399999999997, "text": " Right, because if you've got a sophisticated enough", "tokens": [50844, 1779, 11, 570, 498, 291, 600, 658, 257, 16950, 1547, 50948], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1089, "seek": 272366, "start": 2735.3399999999997, "end": 2737.2599999999998, "text": " DevOps pipeline, it'll say, hey,", "tokens": [50948, 43051, 15517, 11, 309, 603, 584, 11, 4177, 11, 51044], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1090, "seek": 272366, "start": 2737.2599999999998, "end": 2740.2999999999997, "text": " this line of this file broke, fix it.", "tokens": [51044, 341, 1622, 295, 341, 3991, 6902, 11, 3191, 309, 13, 51196], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1091, "seek": 272366, "start": 2740.2999999999997, "end": 2744.22, "text": " And so Codex, having seen all of GitHub", "tokens": [51196, 400, 370, 15549, 87, 11, 1419, 1612, 439, 295, 23331, 51392], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1092, "seek": 272366, "start": 2744.22, "end": 2747.3799999999997, "text": " and all the issues, it might know automatically", "tokens": [51392, 293, 439, 264, 2663, 11, 309, 1062, 458, 6772, 51550], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1093, "seek": 272366, "start": 2747.3799999999997, "end": 2749.56, "text": " how to fix that line of code.", "tokens": [51550, 577, 281, 3191, 300, 1622, 295, 3089, 13, 51659], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1094, "seek": 272366, "start": 2749.56, "end": 2751.8999999999996, "text": " And so that gives you,", "tokens": [51659, 400, 370, 300, 2709, 291, 11, 51776], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1095, "seek": 272366, "start": 2751.8999999999996, "end": 2753.54, "text": " if you've got that feedback loop,", "tokens": [51776, 498, 291, 600, 658, 300, 5824, 6367, 11, 51858], "temperature": 0.0, "avg_logprob": -0.14688256949432626, "compression_ratio": 1.6901960784313725, "no_speech_prob": 0.010010081343352795}, {"id": 1096, "seek": 275354, "start": 2753.54, "end": 2758.42, "text": " where Codex, humans write code, Codex writes code,", "tokens": [50364, 689, 15549, 87, 11, 6255, 2464, 3089, 11, 15549, 87, 13657, 3089, 11, 50608], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1097, "seek": 275354, "start": 2758.42, "end": 2761.66, "text": " co-pilot writes code, everyone's contributing code.", "tokens": [50608, 598, 12, 79, 31516, 13657, 3089, 11, 1518, 311, 19270, 3089, 13, 50770], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1098, "seek": 275354, "start": 2761.66, "end": 2764.42, "text": " And then you've got Codex that can churn on it", "tokens": [50770, 400, 550, 291, 600, 658, 15549, 87, 300, 393, 417, 925, 322, 309, 50908], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1099, "seek": 275354, "start": 2764.42, "end": 2766.58, "text": " and say, let's refactor this.", "tokens": [50908, 293, 584, 11, 718, 311, 1895, 15104, 341, 13, 51016], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1100, "seek": 275354, "start": 2766.58, "end": 2768.92, "text": " Because I bet it's probably better at refactoring", "tokens": [51016, 1436, 286, 778, 309, 311, 1391, 1101, 412, 1895, 578, 3662, 51133], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1101, "seek": 275354, "start": 2768.92, "end": 2770.52, "text": " than writing new code.", "tokens": [51133, 813, 3579, 777, 3089, 13, 51213], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1102, "seek": 275354, "start": 2770.52, "end": 2774.74, "text": " You might have noticed that Instruct and GPT-3 Vanilla", "tokens": [51213, 509, 1062, 362, 5694, 300, 2730, 1757, 293, 26039, 51, 12, 18, 8979, 5291, 51424], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1103, "seek": 275354, "start": 2774.74, "end": 2777.2599999999998, "text": " is really good if you give it a block of text", "tokens": [51424, 307, 534, 665, 498, 291, 976, 309, 257, 3461, 295, 2487, 51550], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1104, "seek": 275354, "start": 2777.2599999999998, "end": 2779.42, "text": " and you say, rewrite this, but a little bit better.", "tokens": [51550, 293, 291, 584, 11, 28132, 341, 11, 457, 257, 707, 857, 1101, 13, 51658], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1105, "seek": 275354, "start": 2779.42, "end": 2780.94, "text": " It's really good at that.", "tokens": [51658, 467, 311, 534, 665, 412, 300, 13, 51734], "temperature": 0.0, "avg_logprob": -0.16176957350510818, "compression_ratio": 1.664092664092664, "no_speech_prob": 0.00016864147619344294}, {"id": 1106, "seek": 278094, "start": 2780.94, "end": 2784.94, "text": " So I suspect that we might end up seeing Codex", "tokens": [50364, 407, 286, 9091, 300, 321, 1062, 917, 493, 2577, 15549, 87, 50564], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1107, "seek": 278094, "start": 2784.94, "end": 2786.7400000000002, "text": " integrated into the DevOps pipeline,", "tokens": [50564, 10919, 666, 264, 43051, 15517, 11, 50654], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1108, "seek": 278094, "start": 2786.7400000000002, "end": 2788.3, "text": " where it says, let's refactor this code,", "tokens": [50654, 689, 309, 1619, 11, 718, 311, 1895, 15104, 341, 3089, 11, 50732], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1109, "seek": 278094, "start": 2788.3, "end": 2789.62, "text": " let's make it a little bit better,", "tokens": [50732, 718, 311, 652, 309, 257, 707, 857, 1101, 11, 50798], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1110, "seek": 278094, "start": 2789.62, "end": 2792.54, "text": " or let's shoot that bug, let's fix this bug.", "tokens": [50798, 420, 718, 311, 3076, 300, 7426, 11, 718, 311, 3191, 341, 7426, 13, 50944], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1111, "seek": 278094, "start": 2792.54, "end": 2795.06, "text": " And that leads to some other interesting possibilities.", "tokens": [50944, 400, 300, 6689, 281, 512, 661, 1880, 12178, 13, 51070], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1112, "seek": 278094, "start": 2795.06, "end": 2800.06, "text": " What if you integrate Codex into a chat room of developers?", "tokens": [51070, 708, 498, 291, 13365, 15549, 87, 666, 257, 5081, 1808, 295, 8849, 30, 51320], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1113, "seek": 278094, "start": 2800.46, "end": 2803.42, "text": " And so that, you can do this in Slack right now,", "tokens": [51340, 400, 370, 300, 11, 291, 393, 360, 341, 294, 37211, 558, 586, 11, 51488], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1114, "seek": 278094, "start": 2803.42, "end": 2807.5, "text": " where you use a special command and you say,", "tokens": [51488, 689, 291, 764, 257, 2121, 5622, 293, 291, 584, 11, 51692], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1115, "seek": 278094, "start": 2807.5, "end": 2809.34, "text": " create an issue, go fix this problem.", "tokens": [51692, 1884, 364, 2734, 11, 352, 3191, 341, 1154, 13, 51784], "temperature": 0.0, "avg_logprob": -0.1435102735246931, "compression_ratio": 1.6802973977695168, "no_speech_prob": 0.0002453533234074712}, {"id": 1116, "seek": 280934, "start": 2809.58, "end": 2813.5, "text": " There's no reason that GPT-3 can't do that, right?", "tokens": [50376, 821, 311, 572, 1778, 300, 26039, 51, 12, 18, 393, 380, 360, 300, 11, 558, 30, 50572], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1117, "seek": 280934, "start": 2813.5, "end": 2817.3, "text": " That you put a GPT-3 bot in your Discord or Slack", "tokens": [50572, 663, 291, 829, 257, 26039, 51, 12, 18, 10592, 294, 428, 32623, 420, 37211, 50762], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1118, "seek": 280934, "start": 2817.3, "end": 2819.26, "text": " and it starts coming up with features", "tokens": [50762, 293, 309, 3719, 1348, 493, 365, 4122, 50860], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1119, "seek": 280934, "start": 2819.26, "end": 2822.26, "text": " or it watches the chat and generates features automatically", "tokens": [50860, 420, 309, 17062, 264, 5081, 293, 23815, 4122, 6772, 51010], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1120, "seek": 280934, "start": 2822.26, "end": 2825.26, "text": " and then codes them and tests them, right?", "tokens": [51010, 293, 550, 14211, 552, 293, 6921, 552, 11, 558, 30, 51160], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1121, "seek": 280934, "start": 2825.26, "end": 2826.8, "text": " That's kind of where I see it going,", "tokens": [51160, 663, 311, 733, 295, 689, 286, 536, 309, 516, 11, 51237], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1122, "seek": 280934, "start": 2826.8, "end": 2829.78, "text": " where it's not gonna necessarily replace developers,", "tokens": [51237, 689, 309, 311, 406, 799, 4725, 7406, 8849, 11, 51386], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1123, "seek": 280934, "start": 2829.78, "end": 2832.54, "text": " at least not anytime soon, it might eventually.", "tokens": [51386, 412, 1935, 406, 13038, 2321, 11, 309, 1062, 4728, 13, 51524], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1124, "seek": 280934, "start": 2832.54, "end": 2836.34, "text": " But where what I see happening is that it's gonna be", "tokens": [51524, 583, 689, 437, 286, 536, 2737, 307, 300, 309, 311, 799, 312, 51714], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1125, "seek": 280934, "start": 2836.34, "end": 2838.26, "text": " tightly integrated into those automation", "tokens": [51714, 21952, 10919, 666, 729, 17769, 51810], "temperature": 0.0, "avg_logprob": -0.15388433394893522, "compression_ratio": 1.7014388489208634, "no_speech_prob": 0.0007320906152017415}, {"id": 1126, "seek": 283826, "start": 2838.3, "end": 2839.76, "text": " because it's fast, right?", "tokens": [50366, 570, 309, 311, 2370, 11, 558, 30, 50439], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1127, "seek": 283826, "start": 2839.76, "end": 2842.7000000000003, "text": " It can generate code faster than any human can.", "tokens": [50439, 467, 393, 8460, 3089, 4663, 813, 604, 1952, 393, 13, 50586], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1128, "seek": 283826, "start": 2842.7000000000003, "end": 2845.0200000000004, "text": " And then so even if the code is messy,", "tokens": [50586, 400, 550, 370, 754, 498, 264, 3089, 307, 16191, 11, 50702], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1129, "seek": 283826, "start": 2845.0200000000004, "end": 2847.5, "text": " if it generates a lot of bugs, it can fix it, right?", "tokens": [50702, 498, 309, 23815, 257, 688, 295, 15120, 11, 309, 393, 3191, 309, 11, 558, 30, 50826], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1130, "seek": 283826, "start": 2847.5, "end": 2849.34, "text": " It's an iterative process.", "tokens": [50826, 467, 311, 364, 17138, 1166, 1399, 13, 50918], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1131, "seek": 283826, "start": 2849.34, "end": 2851.0600000000004, "text": " I don't know if you are familiar with Agile,", "tokens": [50918, 286, 500, 380, 458, 498, 291, 366, 4963, 365, 2725, 794, 11, 51004], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1132, "seek": 283826, "start": 2851.0600000000004, "end": 2853.34, "text": " but that's how we develop software.", "tokens": [51004, 457, 300, 311, 577, 321, 1499, 4722, 13, 51118], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1133, "seek": 283826, "start": 2853.34, "end": 2855.38, "text": " It's you tight feedback loops.", "tokens": [51118, 467, 311, 291, 4524, 5824, 16121, 13, 51220], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1134, "seek": 283826, "start": 2856.46, "end": 2859.1000000000004, "text": " And that leads to one other possibility.", "tokens": [51274, 400, 300, 6689, 281, 472, 661, 7959, 13, 51406], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1135, "seek": 283826, "start": 2859.1000000000004, "end": 2861.7400000000002, "text": " So that's if you're using, what I just outlined is,", "tokens": [51406, 407, 300, 311, 498, 291, 434, 1228, 11, 437, 286, 445, 27412, 307, 11, 51538], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1136, "seek": 283826, "start": 2861.7400000000002, "end": 2865.7400000000002, "text": " let's imagine that Codex is integrated into Facebook", "tokens": [51538, 718, 311, 3811, 300, 15549, 87, 307, 10919, 666, 4384, 51738], "temperature": 0.0, "avg_logprob": -0.13089919812751538, "compression_ratio": 1.6304347826086956, "no_speech_prob": 0.0006070112576708198}, {"id": 1137, "seek": 286574, "start": 2865.74, "end": 2867.8199999999997, "text": " or Reddit or whatever and they're just,", "tokens": [50364, 420, 32210, 420, 2035, 293, 436, 434, 445, 11, 50468], "temperature": 0.0, "avg_logprob": -0.12384581343035832, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.00298031116835773}, {"id": 1138, "seek": 286574, "start": 2867.8199999999997, "end": 2870.18, "text": " they're integrating new features as they go.", "tokens": [50468, 436, 434, 26889, 777, 4122, 382, 436, 352, 13, 50586], "temperature": 0.0, "avg_logprob": -0.12384581343035832, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.00298031116835773}, {"id": 1139, "seek": 286574, "start": 2870.18, "end": 2875.18, "text": " What if you're using Codex in a chat room", "tokens": [50586, 708, 498, 291, 434, 1228, 15549, 87, 294, 257, 5081, 1808, 50836], "temperature": 0.0, "avg_logprob": -0.12384581343035832, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.00298031116835773}, {"id": 1140, "seek": 286574, "start": 2875.7799999999997, "end": 2877.3399999999997, "text": " and it's feeding back into itself,", "tokens": [50866, 293, 309, 311, 12919, 646, 666, 2564, 11, 50944], "temperature": 0.0, "avg_logprob": -0.12384581343035832, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.00298031116835773}, {"id": 1141, "seek": 286574, "start": 2877.3399999999997, "end": 2880.06, "text": " it's making itself more sophisticated?", "tokens": [50944, 309, 311, 1455, 2564, 544, 16950, 30, 51080], "temperature": 0.0, "avg_logprob": -0.12384581343035832, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.00298031116835773}, {"id": 1142, "seek": 286574, "start": 2880.06, "end": 2883.4599999999996, "text": " So this was something I proposed on the OpenAI forum", "tokens": [51080, 407, 341, 390, 746, 286, 10348, 322, 264, 7238, 48698, 17542, 51250], "temperature": 0.0, "avg_logprob": -0.12384581343035832, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.00298031116835773}, {"id": 1143, "seek": 286574, "start": 2883.4599999999996, "end": 2887.2999999999997, "text": " where I was like, what if you had a chatbot", "tokens": [51250, 689, 286, 390, 411, 11, 437, 498, 291, 632, 257, 5081, 18870, 51442], "temperature": 0.0, "avg_logprob": -0.12384581343035832, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.00298031116835773}, {"id": 1144, "seek": 286574, "start": 2887.2999999999997, "end": 2889.4199999999996, "text": " that was aware of its own code", "tokens": [51442, 300, 390, 3650, 295, 1080, 1065, 3089, 51548], "temperature": 0.0, "avg_logprob": -0.12384581343035832, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.00298031116835773}, {"id": 1145, "seek": 286574, "start": 2889.4199999999996, "end": 2893.14, "text": " and could edit its own code via Codex using natural language,", "tokens": [51548, 293, 727, 8129, 1080, 1065, 3089, 5766, 15549, 87, 1228, 3303, 2856, 11, 51734], "temperature": 0.0, "avg_logprob": -0.12384581343035832, "compression_ratio": 1.6182572614107884, "no_speech_prob": 0.00298031116835773}, {"id": 1146, "seek": 289314, "start": 2893.14, "end": 2895.8599999999997, "text": " using a combination of natural language and Codex", "tokens": [50364, 1228, 257, 6562, 295, 3303, 2856, 293, 15549, 87, 50500], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1147, "seek": 289314, "start": 2895.8599999999997, "end": 2897.22, "text": " and it could improve itself.", "tokens": [50500, 293, 309, 727, 3470, 2564, 13, 50568], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1148, "seek": 289314, "start": 2897.22, "end": 2899.3399999999997, "text": " And while you're talking to it, it's like,", "tokens": [50568, 400, 1339, 291, 434, 1417, 281, 309, 11, 309, 311, 411, 11, 50674], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1149, "seek": 289314, "start": 2899.3399999999997, "end": 2901.3799999999997, "text": " man, I wish my chatbot could do this.", "tokens": [50674, 587, 11, 286, 3172, 452, 5081, 18870, 727, 360, 341, 13, 50776], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1150, "seek": 289314, "start": 2901.3799999999997, "end": 2903.42, "text": " And it says, cool, new feature.", "tokens": [50776, 400, 309, 1619, 11, 1627, 11, 777, 4111, 13, 50878], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1151, "seek": 289314, "start": 2903.42, "end": 2905.42, "text": " And it just sends it out to its automated pipeline.", "tokens": [50878, 400, 309, 445, 14790, 309, 484, 281, 1080, 18473, 15517, 13, 50978], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1152, "seek": 289314, "start": 2905.42, "end": 2909.22, "text": " So I see these feedback loops as kind of the way forward.", "tokens": [50978, 407, 286, 536, 613, 5824, 16121, 382, 733, 295, 264, 636, 2128, 13, 51168], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1153, "seek": 289314, "start": 2909.22, "end": 2911.14, "text": " And will that result in AGI?", "tokens": [51168, 400, 486, 300, 1874, 294, 316, 26252, 30, 51264], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1154, "seek": 289314, "start": 2911.14, "end": 2913.22, "text": " Who knows, it could end up with spaghetti code", "tokens": [51264, 2102, 3255, 11, 309, 727, 917, 493, 365, 28556, 3089, 51368], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1155, "seek": 289314, "start": 2913.22, "end": 2916.74, "text": " because you keep tacking on new code and new functions,", "tokens": [51368, 570, 291, 1066, 9426, 278, 322, 777, 3089, 293, 777, 6828, 11, 51544], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1156, "seek": 289314, "start": 2916.74, "end": 2918.14, "text": " eventually it's gonna break.", "tokens": [51544, 4728, 309, 311, 799, 1821, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1157, "seek": 289314, "start": 2918.14, "end": 2921.18, "text": " So, but they're just pie in the sky thought,", "tokens": [51614, 407, 11, 457, 436, 434, 445, 1730, 294, 264, 5443, 1194, 11, 51766], "temperature": 0.0, "avg_logprob": -0.1302478264118063, "compression_ratio": 1.625, "no_speech_prob": 0.010009889490902424}, {"id": 1158, "seek": 292118, "start": 2921.8199999999997, "end": 2924.22, "text": " if someone's out there and they want a business idea,", "tokens": [50396, 498, 1580, 311, 484, 456, 293, 436, 528, 257, 1606, 1558, 11, 50516], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1159, "seek": 292118, "start": 2924.22, "end": 2926.5, "text": " integrate Codex into DevOps", "tokens": [50516, 13365, 15549, 87, 666, 43051, 50630], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1160, "seek": 292118, "start": 2926.5, "end": 2927.94, "text": " and you're gonna be a billionaire.", "tokens": [50630, 293, 291, 434, 799, 312, 257, 42358, 13, 50702], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1161, "seek": 292118, "start": 2929.5, "end": 2931.46, "text": " There you have it, let's just clip it.", "tokens": [50780, 821, 291, 362, 309, 11, 718, 311, 445, 7353, 309, 13, 50878], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1162, "seek": 292118, "start": 2931.46, "end": 2934.66, "text": " We're good, we're good David, we can wrap up, see you later.", "tokens": [50878, 492, 434, 665, 11, 321, 434, 665, 4389, 11, 321, 393, 7019, 493, 11, 536, 291, 1780, 13, 51038], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1163, "seek": 292118, "start": 2936.3399999999997, "end": 2940.3799999999997, "text": " No, I agree with you and definitely these are some great", "tokens": [51122, 883, 11, 286, 3986, 365, 291, 293, 2138, 613, 366, 512, 869, 51324], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1164, "seek": 292118, "start": 2940.3799999999997, "end": 2943.3399999999997, "text": " use cases you're sharing for people thinking about", "tokens": [51324, 764, 3331, 291, 434, 5414, 337, 561, 1953, 466, 51472], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1165, "seek": 292118, "start": 2943.3399999999997, "end": 2946.98, "text": " what could I build, what's a cool project, certainly.", "tokens": [51472, 437, 727, 286, 1322, 11, 437, 311, 257, 1627, 1716, 11, 3297, 13, 51654], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1166, "seek": 292118, "start": 2946.98, "end": 2949.4199999999996, "text": " With Codex and GPT-3, you can build things", "tokens": [51654, 2022, 15549, 87, 293, 26039, 51, 12, 18, 11, 291, 393, 1322, 721, 51776], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1167, "seek": 292118, "start": 2949.4199999999996, "end": 2950.58, "text": " relatively quickly, right?", "tokens": [51776, 7226, 2661, 11, 558, 30, 51834], "temperature": 0.0, "avg_logprob": -0.21634356677532196, "compression_ratio": 1.5719298245614035, "no_speech_prob": 0.0023226297926157713}, {"id": 1168, "seek": 295058, "start": 2950.62, "end": 2953.54, "text": " That's one of the advantages is the prototyping speed,", "tokens": [50366, 663, 311, 472, 295, 264, 14906, 307, 264, 46219, 3381, 3073, 11, 50512], "temperature": 0.0, "avg_logprob": -0.19136788294865534, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.001753784716129303}, {"id": 1169, "seek": 295058, "start": 2953.54, "end": 2955.62, "text": " especially to figure out the most complicated bit,", "tokens": [50512, 2318, 281, 2573, 484, 264, 881, 6179, 857, 11, 50616], "temperature": 0.0, "avg_logprob": -0.19136788294865534, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.001753784716129303}, {"id": 1170, "seek": 295058, "start": 2955.62, "end": 2956.86, "text": " which is the AI.", "tokens": [50616, 597, 307, 264, 7318, 13, 50678], "temperature": 0.0, "avg_logprob": -0.19136788294865534, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.001753784716129303}, {"id": 1171, "seek": 295058, "start": 2956.86, "end": 2958.18, "text": " Yep.", "tokens": [50678, 7010, 13, 50744], "temperature": 0.0, "avg_logprob": -0.19136788294865534, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.001753784716129303}, {"id": 1172, "seek": 295058, "start": 2958.18, "end": 2963.18, "text": " Yeah, I find Codex does have limitations though", "tokens": [50744, 865, 11, 286, 915, 15549, 87, 775, 362, 15705, 1673, 50994], "temperature": 0.0, "avg_logprob": -0.19136788294865534, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.001753784716129303}, {"id": 1173, "seek": 295058, "start": 2963.74, "end": 2966.74, "text": " and character limits and stuff like that,", "tokens": [51022, 293, 2517, 10406, 293, 1507, 411, 300, 11, 51172], "temperature": 0.0, "avg_logprob": -0.19136788294865534, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.001753784716129303}, {"id": 1174, "seek": 295058, "start": 2966.74, "end": 2970.7799999999997, "text": " which is why I'm a heavy user of GitHub Co-Pilot.", "tokens": [51172, 597, 307, 983, 286, 478, 257, 4676, 4195, 295, 23331, 3066, 12, 47, 31516, 13, 51374], "temperature": 0.0, "avg_logprob": -0.19136788294865534, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.001753784716129303}, {"id": 1175, "seek": 295058, "start": 2970.7799999999997, "end": 2974.8199999999997, "text": " I think it's a silent killer and of course it runs", "tokens": [51374, 286, 519, 309, 311, 257, 12784, 13364, 293, 295, 1164, 309, 6676, 51576], "temperature": 0.0, "avg_logprob": -0.19136788294865534, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.001753784716129303}, {"id": 1176, "seek": 295058, "start": 2974.8199999999997, "end": 2977.02, "text": " on Codex or a special version of Codex,", "tokens": [51576, 322, 15549, 87, 420, 257, 2121, 3037, 295, 15549, 87, 11, 51686], "temperature": 0.0, "avg_logprob": -0.19136788294865534, "compression_ratio": 1.5299145299145298, "no_speech_prob": 0.001753784716129303}, {"id": 1177, "seek": 297702, "start": 2978.02, "end": 2982.02, "text": " but I can see GitHub Co-Pilot perhaps getting more adoption", "tokens": [50414, 457, 286, 393, 536, 23331, 3066, 12, 47, 31516, 4317, 1242, 544, 19215, 50614], "temperature": 0.0, "avg_logprob": -0.15027941190279448, "compression_ratio": 1.518939393939394, "no_speech_prob": 0.0027141072787344456}, {"id": 1178, "seek": 297702, "start": 2982.02, "end": 2983.7, "text": " than even something like GPT-3.", "tokens": [50614, 813, 754, 746, 411, 26039, 51, 12, 18, 13, 50698], "temperature": 0.0, "avg_logprob": -0.15027941190279448, "compression_ratio": 1.518939393939394, "no_speech_prob": 0.0027141072787344456}, {"id": 1179, "seek": 297702, "start": 2983.7, "end": 2987.42, "text": " I'm saying use daily at least eight hours a day.", "tokens": [50698, 286, 478, 1566, 764, 5212, 412, 1935, 3180, 2496, 257, 786, 13, 50884], "temperature": 0.0, "avg_logprob": -0.15027941190279448, "compression_ratio": 1.518939393939394, "no_speech_prob": 0.0027141072787344456}, {"id": 1180, "seek": 297702, "start": 2987.42, "end": 2990.54, "text": " One of my other predictions was it may surpass GPT-3", "tokens": [50884, 1485, 295, 452, 661, 21264, 390, 309, 815, 27650, 26039, 51, 12, 18, 51040], "temperature": 0.0, "avg_logprob": -0.15027941190279448, "compression_ratio": 1.518939393939394, "no_speech_prob": 0.0027141072787344456}, {"id": 1181, "seek": 297702, "start": 2990.54, "end": 2995.54, "text": " this year and so these are some great use cases", "tokens": [51040, 341, 1064, 293, 370, 613, 366, 512, 869, 764, 3331, 51290], "temperature": 0.0, "avg_logprob": -0.15027941190279448, "compression_ratio": 1.518939393939394, "no_speech_prob": 0.0027141072787344456}, {"id": 1182, "seek": 297702, "start": 2995.82, "end": 2999.62, "text": " you've shared for sure, but what are your thoughts", "tokens": [51304, 291, 600, 5507, 337, 988, 11, 457, 437, 366, 428, 4598, 51494], "temperature": 0.0, "avg_logprob": -0.15027941190279448, "compression_ratio": 1.518939393939394, "no_speech_prob": 0.0027141072787344456}, {"id": 1183, "seek": 297702, "start": 2999.62, "end": 3000.46, "text": " in usage?", "tokens": [51494, 294, 14924, 30, 51536], "temperature": 0.0, "avg_logprob": -0.15027941190279448, "compression_ratio": 1.518939393939394, "no_speech_prob": 0.0027141072787344456}, {"id": 1184, "seek": 297702, "start": 3000.46, "end": 3004.58, "text": " Do you find yourself using GPT-3, DaVinci Classic more?", "tokens": [51536, 1144, 291, 915, 1803, 1228, 26039, 51, 12, 18, 11, 3933, 53, 21961, 25008, 544, 30, 51742], "temperature": 0.0, "avg_logprob": -0.15027941190279448, "compression_ratio": 1.518939393939394, "no_speech_prob": 0.0027141072787344456}, {"id": 1185, "seek": 297702, "start": 3004.58, "end": 3006.62, "text": " That's what I'm calling the older version.", "tokens": [51742, 663, 311, 437, 286, 478, 5141, 264, 4906, 3037, 13, 51844], "temperature": 0.0, "avg_logprob": -0.15027941190279448, "compression_ratio": 1.518939393939394, "no_speech_prob": 0.0027141072787344456}, {"id": 1186, "seek": 300662, "start": 3006.7799999999997, "end": 3010.46, "text": " Do you find yourself using Instruct GPT more?", "tokens": [50372, 1144, 291, 915, 1803, 1228, 2730, 1757, 26039, 51, 544, 30, 50556], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1187, "seek": 300662, "start": 3010.46, "end": 3012.54, "text": " Do you find yourself playing around with multimodal models?", "tokens": [50556, 1144, 291, 915, 1803, 2433, 926, 365, 32972, 378, 304, 5245, 30, 50660], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1188, "seek": 300662, "start": 3012.54, "end": 3015.18, "text": " Like what's the proportion of GPT-3 to Codex", "tokens": [50660, 1743, 437, 311, 264, 16068, 295, 26039, 51, 12, 18, 281, 15549, 87, 50792], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1189, "seek": 300662, "start": 3015.18, "end": 3016.5, "text": " in terms of your usage?", "tokens": [50792, 294, 2115, 295, 428, 14924, 30, 50858], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1190, "seek": 300662, "start": 3017.58, "end": 3020.02, "text": " Let's see, I'm almost exclusively using either", "tokens": [50912, 961, 311, 536, 11, 286, 478, 1920, 20638, 1228, 2139, 51034], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1191, "seek": 300662, "start": 3020.02, "end": 3022.2999999999997, "text": " Instructor fine-tuned models right now.", "tokens": [51034, 2730, 14535, 2489, 12, 83, 43703, 5245, 558, 586, 13, 51148], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1192, "seek": 300662, "start": 3023.14, "end": 3026.9, "text": " Actually, after I prototyped my cognitive architecture,", "tokens": [51190, 5135, 11, 934, 286, 46219, 3452, 452, 15605, 9482, 11, 51378], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1193, "seek": 300662, "start": 3026.9, "end": 3029.58, "text": " I haven't done a heck of a lot of coding lately.", "tokens": [51378, 286, 2378, 380, 1096, 257, 12872, 295, 257, 688, 295, 17720, 12881, 13, 51512], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1194, "seek": 300662, "start": 3029.58, "end": 3031.2999999999997, "text": " I've actually been writing a lot.", "tokens": [51512, 286, 600, 767, 668, 3579, 257, 688, 13, 51598], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1195, "seek": 300662, "start": 3031.2999999999997, "end": 3035.22, "text": " So I've got my natural language cognitive architecture book", "tokens": [51598, 407, 286, 600, 658, 452, 3303, 2856, 15605, 9482, 1446, 51794], "temperature": 0.0, "avg_logprob": -0.15418864440917968, "compression_ratio": 1.6546762589928057, "no_speech_prob": 0.0008038858650252223}, {"id": 1196, "seek": 303522, "start": 3035.2999999999997, "end": 3037.8199999999997, "text": " and I'm working on two more nonfiction books", "tokens": [50368, 293, 286, 478, 1364, 322, 732, 544, 2107, 32041, 3642, 50494], "temperature": 0.0, "avg_logprob": -0.10481708557879338, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0013246611924842}, {"id": 1197, "seek": 303522, "start": 3037.8199999999997, "end": 3042.1, "text": " and I tried creating a system to help me co-write those", "tokens": [50494, 293, 286, 3031, 4084, 257, 1185, 281, 854, 385, 598, 12, 21561, 729, 50708], "temperature": 0.0, "avg_logprob": -0.10481708557879338, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0013246611924842}, {"id": 1198, "seek": 303522, "start": 3042.1, "end": 3045.8999999999996, "text": " but when you're, so talking about limitations of GPT-3,", "tokens": [50708, 457, 562, 291, 434, 11, 370, 1417, 466, 15705, 295, 26039, 51, 12, 18, 11, 50898], "temperature": 0.0, "avg_logprob": -0.10481708557879338, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0013246611924842}, {"id": 1199, "seek": 303522, "start": 3045.8999999999996, "end": 3048.06, "text": " if you're proposing something new that didn't exist", "tokens": [50898, 498, 291, 434, 29939, 746, 777, 300, 994, 380, 2514, 51006], "temperature": 0.0, "avg_logprob": -0.10481708557879338, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0013246611924842}, {"id": 1200, "seek": 303522, "start": 3048.06, "end": 3051.5, "text": " in the dataset in 2019 or 2018, whenever it was trained,", "tokens": [51006, 294, 264, 28872, 294, 6071, 420, 6096, 11, 5699, 309, 390, 8895, 11, 51178], "temperature": 0.0, "avg_logprob": -0.10481708557879338, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0013246611924842}, {"id": 1201, "seek": 303522, "start": 3051.5, "end": 3052.98, "text": " it really struggles.", "tokens": [51178, 309, 534, 17592, 13, 51252], "temperature": 0.0, "avg_logprob": -0.10481708557879338, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0013246611924842}, {"id": 1202, "seek": 303522, "start": 3053.8199999999997, "end": 3058.2999999999997, "text": " GPT-3, if you give it like two or three paragraphs", "tokens": [51294, 26039, 51, 12, 18, 11, 498, 291, 976, 309, 411, 732, 420, 1045, 48910, 51518], "temperature": 0.0, "avg_logprob": -0.10481708557879338, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0013246611924842}, {"id": 1203, "seek": 303522, "start": 3058.2999999999997, "end": 3061.8199999999997, "text": " explaining a new concept, it can usually kind of get it", "tokens": [51518, 13468, 257, 777, 3410, 11, 309, 393, 2673, 733, 295, 483, 309, 51694], "temperature": 0.0, "avg_logprob": -0.10481708557879338, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0013246611924842}, {"id": 1204, "seek": 303522, "start": 3061.8199999999997, "end": 3064.58, "text": " but it's kind of slow on the uptake otherwise", "tokens": [51694, 457, 309, 311, 733, 295, 2964, 322, 264, 493, 27612, 5911, 51832], "temperature": 0.0, "avg_logprob": -0.10481708557879338, "compression_ratio": 1.5848375451263539, "no_speech_prob": 0.0013246611924842}, {"id": 1205, "seek": 306458, "start": 3064.58, "end": 3067.74, "text": " and so if you're writing about new research or something,", "tokens": [50364, 293, 370, 498, 291, 434, 3579, 466, 777, 2132, 420, 746, 11, 50522], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1206, "seek": 306458, "start": 3067.74, "end": 3069.58, "text": " it's not gonna get it that well.", "tokens": [50522, 309, 311, 406, 799, 483, 309, 300, 731, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1207, "seek": 306458, "start": 3069.58, "end": 3072.38, "text": " So I've actually kind of defaulted back to my own head", "tokens": [50614, 407, 286, 600, 767, 733, 295, 7576, 292, 646, 281, 452, 1065, 1378, 50754], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1208, "seek": 306458, "start": 3072.38, "end": 3075.54, "text": " for a lot of my projects lately", "tokens": [50754, 337, 257, 688, 295, 452, 4455, 12881, 50912], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1209, "seek": 306458, "start": 3075.54, "end": 3077.38, "text": " but I could imagine like if I wanted to go write", "tokens": [50912, 457, 286, 727, 3811, 411, 498, 286, 1415, 281, 352, 2464, 51004], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1210, "seek": 306458, "start": 3077.38, "end": 3079.54, "text": " a new Discord bot, I might use Codex and say,", "tokens": [51004, 257, 777, 32623, 10592, 11, 286, 1062, 764, 15549, 87, 293, 584, 11, 51112], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1211, "seek": 306458, "start": 3079.54, "end": 3081.7799999999997, "text": " hey, write me a Discord bot that will do this", "tokens": [51112, 4177, 11, 2464, 385, 257, 32623, 10592, 300, 486, 360, 341, 51224], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1212, "seek": 306458, "start": 3081.7799999999997, "end": 3083.18, "text": " and just see what it spits out and just say,", "tokens": [51224, 293, 445, 536, 437, 309, 637, 1208, 484, 293, 445, 584, 11, 51294], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1213, "seek": 306458, "start": 3083.18, "end": 3086.2599999999998, "text": " okay, cool, pick and choose the pieces that I like.", "tokens": [51294, 1392, 11, 1627, 11, 1888, 293, 2826, 264, 3755, 300, 286, 411, 13, 51448], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1214, "seek": 306458, "start": 3086.2599999999998, "end": 3088.5, "text": " Part of the problem though is it's really difficult", "tokens": [51448, 4100, 295, 264, 1154, 1673, 307, 309, 311, 534, 2252, 51560], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1215, "seek": 306458, "start": 3088.5, "end": 3093.5, "text": " to fully articulate what you want a program to do up front.", "tokens": [51560, 281, 4498, 30305, 437, 291, 528, 257, 1461, 281, 360, 493, 1868, 13, 51810], "temperature": 0.0, "avg_logprob": -0.10520492373286067, "compression_ratio": 1.6677215189873418, "no_speech_prob": 0.0002779983915388584}, {"id": 1216, "seek": 309350, "start": 3093.62, "end": 3096.46, "text": " Cause you know, like you said, there's character limits,", "tokens": [50370, 10865, 291, 458, 11, 411, 291, 848, 11, 456, 311, 2517, 10406, 11, 50512], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1217, "seek": 309350, "start": 3096.46, "end": 3099.14, "text": " there's only so much that you can put in", "tokens": [50512, 456, 311, 787, 370, 709, 300, 291, 393, 829, 294, 50646], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1218, "seek": 309350, "start": 3099.14, "end": 3100.9, "text": " but also if you don't have it fully articulated", "tokens": [50646, 457, 611, 498, 291, 500, 380, 362, 309, 4498, 43322, 50734], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1219, "seek": 309350, "start": 3100.9, "end": 3102.82, "text": " in your own head, of course, the machine isn't gonna", "tokens": [50734, 294, 428, 1065, 1378, 11, 295, 1164, 11, 264, 3479, 1943, 380, 799, 50830], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1220, "seek": 309350, "start": 3102.82, "end": 3104.46, "text": " be able to figure it out for you.", "tokens": [50830, 312, 1075, 281, 2573, 309, 484, 337, 291, 13, 50912], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1221, "seek": 309350, "start": 3104.46, "end": 3105.42, "text": " So, yeah.", "tokens": [50912, 407, 11, 1338, 13, 50960], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1222, "seek": 309350, "start": 3108.3, "end": 3112.82, "text": " Yeah, and it's just, I just haven't seen", "tokens": [51104, 865, 11, 293, 309, 311, 445, 11, 286, 445, 2378, 380, 1612, 51330], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1223, "seek": 309350, "start": 3112.82, "end": 3115.3, "text": " that much activity specifically around Codex.", "tokens": [51330, 300, 709, 5191, 4682, 926, 15549, 87, 13, 51454], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1224, "seek": 309350, "start": 3115.3, "end": 3117.5, "text": " I haven't seen that many use cases.", "tokens": [51454, 286, 2378, 380, 1612, 300, 867, 764, 3331, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1225, "seek": 309350, "start": 3119.26, "end": 3123.06, "text": " I looked up the Google Trends data at its most hype,", "tokens": [51652, 286, 2956, 493, 264, 3329, 37417, 82, 1412, 412, 1080, 881, 24144, 11, 51842], "temperature": 0.0, "avg_logprob": -0.16641486563333652, "compression_ratio": 1.6328125, "no_speech_prob": 0.0009397367248311639}, {"id": 1226, "seek": 312306, "start": 3123.06, "end": 3127.2599999999998, "text": " Codex is still less than GPT-3 is kind of lowest, right?", "tokens": [50364, 15549, 87, 307, 920, 1570, 813, 26039, 51, 12, 18, 307, 733, 295, 12437, 11, 558, 30, 50574], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1227, "seek": 312306, "start": 3127.2599999999998, "end": 3129.14, "text": " And the audience is really specific,", "tokens": [50574, 400, 264, 4034, 307, 534, 2685, 11, 50668], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1228, "seek": 312306, "start": 3129.14, "end": 3132.46, "text": " like it's programmers who wanna build use cases", "tokens": [50668, 411, 309, 311, 41504, 567, 1948, 1322, 764, 3331, 50834], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1229, "seek": 312306, "start": 3132.46, "end": 3134.14, "text": " for something like Codex.", "tokens": [50834, 337, 746, 411, 15549, 87, 13, 50918], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1230, "seek": 312306, "start": 3134.14, "end": 3136.66, "text": " Whereas GPT-3 has poets, writers,", "tokens": [50918, 13813, 26039, 51, 12, 18, 575, 38364, 11, 13491, 11, 51044], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1231, "seek": 312306, "start": 3136.66, "end": 3141.14, "text": " it has artists, coders, GPT-3 can write code too, right?", "tokens": [51044, 309, 575, 6910, 11, 17656, 433, 11, 26039, 51, 12, 18, 393, 2464, 3089, 886, 11, 558, 30, 51268], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1232, "seek": 312306, "start": 3141.14, "end": 3144.58, "text": " So it's a little bit complicated like,", "tokens": [51268, 407, 309, 311, 257, 707, 857, 6179, 411, 11, 51440], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1233, "seek": 312306, "start": 3144.58, "end": 3147.02, "text": " who is the target audience for something like Codex?", "tokens": [51440, 567, 307, 264, 3779, 4034, 337, 746, 411, 15549, 87, 30, 51562], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1234, "seek": 312306, "start": 3147.02, "end": 3149.38, "text": " What use cases did OpenAI imagine", "tokens": [51562, 708, 764, 3331, 630, 7238, 48698, 3811, 51680], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1235, "seek": 312306, "start": 3149.38, "end": 3151.5, "text": " for a product like that?", "tokens": [51680, 337, 257, 1674, 411, 300, 30, 51786], "temperature": 0.0, "avg_logprob": -0.19400521575427446, "compression_ratio": 1.6558704453441295, "no_speech_prob": 0.0008038077503442764}, {"id": 1236, "seek": 315150, "start": 3151.5, "end": 3153.74, "text": " The next version I've heard in the rumor mill", "tokens": [50364, 440, 958, 3037, 286, 600, 2198, 294, 264, 29639, 1728, 50476], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1237, "seek": 315150, "start": 3153.74, "end": 3154.66, "text": " is gonna be crazy.", "tokens": [50476, 307, 799, 312, 3219, 13, 50522], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1238, "seek": 315150, "start": 3154.66, "end": 3157.18, "text": " Like it may write 50% of your code", "tokens": [50522, 1743, 309, 815, 2464, 2625, 4, 295, 428, 3089, 50648], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1239, "seek": 315150, "start": 3157.18, "end": 3159.02, "text": " as opposed to right now for me,", "tokens": [50648, 382, 8851, 281, 558, 586, 337, 385, 11, 50740], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1240, "seek": 315150, "start": 3159.02, "end": 3161.66, "text": " get a co-pilot is writing two to 8%.", "tokens": [50740, 483, 257, 598, 12, 79, 31516, 307, 3579, 732, 281, 1649, 6856, 50872], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1241, "seek": 315150, "start": 3161.66, "end": 3165.14, "text": " However, your Discord bot I think is a genius idea", "tokens": [50872, 2908, 11, 428, 32623, 10592, 286, 519, 307, 257, 14017, 1558, 51046], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1242, "seek": 315150, "start": 3165.14, "end": 3167.9, "text": " where there's, it's genius in the sense", "tokens": [51046, 689, 456, 311, 11, 309, 311, 14017, 294, 264, 2020, 51184], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1243, "seek": 315150, "start": 3167.9, "end": 3169.9, "text": " that there's no pressure on it, right?", "tokens": [51184, 300, 456, 311, 572, 3321, 322, 309, 11, 558, 30, 51284], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1244, "seek": 315150, "start": 3169.9, "end": 3172.38, "text": " It may chime in, it may not, whatever it's shared", "tokens": [51284, 467, 815, 40921, 294, 11, 309, 815, 406, 11, 2035, 309, 311, 5507, 51408], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1245, "seek": 315150, "start": 3172.38, "end": 3173.58, "text": " might be interesting.", "tokens": [51408, 1062, 312, 1880, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1246, "seek": 315150, "start": 3174.58, "end": 3177.1, "text": " There's lots of, you could take it a lot further,", "tokens": [51518, 821, 311, 3195, 295, 11, 291, 727, 747, 309, 257, 688, 3052, 11, 51644], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1247, "seek": 315150, "start": 3177.1, "end": 3179.02, "text": " you could buy it with GPT-3, have features,", "tokens": [51644, 291, 727, 2256, 309, 365, 26039, 51, 12, 18, 11, 362, 4122, 11, 51740], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1248, "seek": 315150, "start": 3179.02, "end": 3181.26, "text": " you could fine tune it on your company and its mission", "tokens": [51740, 291, 727, 2489, 10864, 309, 322, 428, 2237, 293, 1080, 4447, 51852], "temperature": 0.0, "avg_logprob": -0.1485563387536699, "compression_ratio": 1.6476190476190475, "no_speech_prob": 0.0015008632326498628}, {"id": 1249, "seek": 318126, "start": 3181.26, "end": 3183.94, "text": " and its existing code, so many ways around it.", "tokens": [50364, 293, 1080, 6741, 3089, 11, 370, 867, 2098, 926, 309, 13, 50498], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1250, "seek": 318126, "start": 3183.94, "end": 3185.5800000000004, "text": " So that's a great piece.", "tokens": [50498, 407, 300, 311, 257, 869, 2522, 13, 50580], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1251, "seek": 318126, "start": 3185.5800000000004, "end": 3188.34, "text": " And so you talked about the using,", "tokens": [50580, 400, 370, 291, 2825, 466, 264, 1228, 11, 50718], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1252, "seek": 318126, "start": 3188.34, "end": 3192.0200000000004, "text": " experimenting with writing in relation to your current stack,", "tokens": [50718, 29070, 365, 3579, 294, 9721, 281, 428, 2190, 8630, 11, 50902], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1253, "seek": 318126, "start": 3192.0200000000004, "end": 3194.0600000000004, "text": " which is mainly instructed fine tune.", "tokens": [50902, 597, 307, 8704, 36384, 2489, 10864, 13, 51004], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1254, "seek": 318126, "start": 3194.0600000000004, "end": 3195.1000000000004, "text": " Yep.", "tokens": [51004, 7010, 13, 51056], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1255, "seek": 318126, "start": 3195.1000000000004, "end": 3197.34, "text": " So tell us about your book.", "tokens": [51056, 407, 980, 505, 466, 428, 1446, 13, 51168], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1256, "seek": 318126, "start": 3197.34, "end": 3198.78, "text": " I've had a chance to review it,", "tokens": [51168, 286, 600, 632, 257, 2931, 281, 3131, 309, 11, 51240], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1257, "seek": 318126, "start": 3198.78, "end": 3201.6600000000003, "text": " Natural Language Cognitive Architecture.", "tokens": [51240, 20137, 24445, 383, 2912, 2187, 43049, 13, 51384], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1258, "seek": 318126, "start": 3201.6600000000003, "end": 3203.0200000000004, "text": " Tell the audience about it.", "tokens": [51384, 5115, 264, 4034, 466, 309, 13, 51452], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1259, "seek": 318126, "start": 3203.0200000000004, "end": 3204.5400000000004, "text": " I mean, I would describe it,", "tokens": [51452, 286, 914, 11, 286, 576, 6786, 309, 11, 51528], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1260, "seek": 318126, "start": 3204.5400000000004, "end": 3209.42, "text": " it's an interesting systems theory of AGI", "tokens": [51528, 309, 311, 364, 1880, 3652, 5261, 295, 316, 26252, 51772], "temperature": 0.0, "avg_logprob": -0.20106664478269398, "compression_ratio": 1.5393258426966292, "no_speech_prob": 0.0009106785873882473}, {"id": 1261, "seek": 320942, "start": 3209.46, "end": 3213.42, "text": " combined with modern day prompt writing.", "tokens": [50366, 9354, 365, 4363, 786, 12391, 3579, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1782335223573627, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.015173316933214664}, {"id": 1262, "seek": 320942, "start": 3214.26, "end": 3217.5, "text": " And so I've never seen somebody actually take a stab", "tokens": [50606, 400, 370, 286, 600, 1128, 1612, 2618, 767, 747, 257, 16343, 50768], "temperature": 0.0, "avg_logprob": -0.1782335223573627, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.015173316933214664}, {"id": 1263, "seek": 320942, "start": 3217.5, "end": 3221.2200000000003, "text": " at this kind of super big systems problem", "tokens": [50768, 412, 341, 733, 295, 1687, 955, 3652, 1154, 50954], "temperature": 0.0, "avg_logprob": -0.1782335223573627, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.015173316933214664}, {"id": 1264, "seek": 320942, "start": 3221.2200000000003, "end": 3223.38, "text": " and relate it to something that pretty much", "tokens": [50954, 293, 10961, 309, 281, 746, 300, 1238, 709, 51062], "temperature": 0.0, "avg_logprob": -0.1782335223573627, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.015173316933214664}, {"id": 1265, "seek": 320942, "start": 3223.38, "end": 3226.9, "text": " every GPT-3 developer in the world would find interesting.", "tokens": [51062, 633, 26039, 51, 12, 18, 10754, 294, 264, 1002, 576, 915, 1880, 13, 51238], "temperature": 0.0, "avg_logprob": -0.1782335223573627, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.015173316933214664}, {"id": 1266, "seek": 320942, "start": 3228.38, "end": 3230.9, "text": " And I mean, I can tell you're drawing", "tokens": [51312, 400, 286, 914, 11, 286, 393, 980, 291, 434, 6316, 51438], "temperature": 0.0, "avg_logprob": -0.1782335223573627, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.015173316933214664}, {"id": 1267, "seek": 320942, "start": 3230.9, "end": 3234.1, "text": " from a very interdisciplinary background as well.", "tokens": [51438, 490, 257, 588, 38280, 3678, 382, 731, 13, 51598], "temperature": 0.0, "avg_logprob": -0.1782335223573627, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.015173316933214664}, {"id": 1268, "seek": 320942, "start": 3234.1, "end": 3237.42, "text": " So you mentioned GPT-3 may have been the genesis of it,", "tokens": [51598, 407, 291, 2835, 26039, 51, 12, 18, 815, 362, 668, 264, 1049, 9374, 295, 309, 11, 51764], "temperature": 0.0, "avg_logprob": -0.1782335223573627, "compression_ratio": 1.5098814229249011, "no_speech_prob": 0.015173316933214664}, {"id": 1269, "seek": 323742, "start": 3237.98, "end": 3240.82, "text": " you started connecting dots and deciding,", "tokens": [50392, 291, 1409, 11015, 15026, 293, 17990, 11, 50534], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1270, "seek": 323742, "start": 3240.82, "end": 3243.7400000000002, "text": " I wanna write the book, but how did it come together", "tokens": [50534, 286, 1948, 2464, 264, 1446, 11, 457, 577, 630, 309, 808, 1214, 50680], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1271, "seek": 323742, "start": 3243.7400000000002, "end": 3245.86, "text": " and please tell us more about it.", "tokens": [50680, 293, 1767, 980, 505, 544, 466, 309, 13, 50786], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1272, "seek": 323742, "start": 3245.86, "end": 3248.54, "text": " Yeah, so Natural Language Cognitive Architecture", "tokens": [50786, 865, 11, 370, 20137, 24445, 383, 2912, 2187, 43049, 50920], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1273, "seek": 323742, "start": 3248.54, "end": 3252.94, "text": " is that's my proposed way of creating basically", "tokens": [50920, 307, 300, 311, 452, 10348, 636, 295, 4084, 1936, 51140], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1274, "seek": 323742, "start": 3252.94, "end": 3256.42, "text": " a language-based AGI prototype.", "tokens": [51140, 257, 2856, 12, 6032, 316, 26252, 19475, 13, 51314], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1275, "seek": 323742, "start": 3256.42, "end": 3259.26, "text": " And I know that that's like,", "tokens": [51314, 400, 286, 458, 300, 300, 311, 411, 11, 51456], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1276, "seek": 323742, "start": 3259.26, "end": 3260.58, "text": " when I tell people that, that's like,", "tokens": [51456, 562, 286, 980, 561, 300, 11, 300, 311, 411, 11, 51522], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1277, "seek": 323742, "start": 3260.58, "end": 3262.7000000000003, "text": " okay, that's pure hyperbole.", "tokens": [51522, 1392, 11, 300, 311, 6075, 9848, 1763, 306, 13, 51628], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1278, "seek": 323742, "start": 3262.7000000000003, "end": 3265.3, "text": " And like, yeah, that's a fair response.", "tokens": [51628, 400, 411, 11, 1338, 11, 300, 311, 257, 3143, 4134, 13, 51758], "temperature": 0.0, "avg_logprob": -0.1608325148050764, "compression_ratio": 1.610655737704918, "no_speech_prob": 0.0009696161141619086}, {"id": 1279, "seek": 326530, "start": 3265.34, "end": 3268.98, "text": " But to frame it, imagine that you've got a person", "tokens": [50366, 583, 281, 3920, 309, 11, 3811, 300, 291, 600, 658, 257, 954, 50548], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1280, "seek": 326530, "start": 3268.98, "end": 3270.6200000000003, "text": " who's paralyzed and blind,", "tokens": [50548, 567, 311, 41919, 293, 6865, 11, 50630], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1281, "seek": 326530, "start": 3270.6200000000003, "end": 3273.34, "text": " all they can do is speak and listen.", "tokens": [50630, 439, 436, 393, 360, 307, 1710, 293, 2140, 13, 50766], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1282, "seek": 326530, "start": 3274.26, "end": 3276.6600000000003, "text": " Is that person still intelligent?", "tokens": [50812, 1119, 300, 954, 920, 13232, 30, 50932], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1283, "seek": 326530, "start": 3276.6600000000003, "end": 3280.1000000000004, "text": " I say they are, even if you're bedridden,", "tokens": [50932, 286, 584, 436, 366, 11, 754, 498, 291, 434, 2901, 81, 6171, 11, 51104], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1284, "seek": 326530, "start": 3280.1000000000004, "end": 3281.9, "text": " you can't move, you can't see,", "tokens": [51104, 291, 393, 380, 1286, 11, 291, 393, 380, 536, 11, 51194], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1285, "seek": 326530, "start": 3281.9, "end": 3283.1000000000004, "text": " you can't interact with the world,", "tokens": [51194, 291, 393, 380, 4648, 365, 264, 1002, 11, 51254], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1286, "seek": 326530, "start": 3283.1000000000004, "end": 3284.5, "text": " all you can do is listen and speak,", "tokens": [51254, 439, 291, 393, 360, 307, 2140, 293, 1710, 11, 51324], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1287, "seek": 326530, "start": 3284.5, "end": 3285.86, "text": " you're still intelligent.", "tokens": [51324, 291, 434, 920, 13232, 13, 51392], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1288, "seek": 326530, "start": 3285.86, "end": 3288.6200000000003, "text": " And so in that respect, I would say that like,", "tokens": [51392, 400, 370, 294, 300, 3104, 11, 286, 576, 584, 300, 411, 11, 51530], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1289, "seek": 326530, "start": 3288.6200000000003, "end": 3290.9, "text": " because one of the questions that people ask is,", "tokens": [51530, 570, 472, 295, 264, 1651, 300, 561, 1029, 307, 11, 51644], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1290, "seek": 326530, "start": 3290.9, "end": 3292.5800000000004, "text": " is GPT-3 AGI?", "tokens": [51644, 307, 26039, 51, 12, 18, 316, 26252, 30, 51728], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1291, "seek": 326530, "start": 3292.5800000000004, "end": 3294.78, "text": " No, but it's an important component.", "tokens": [51728, 883, 11, 457, 309, 311, 364, 1021, 6542, 13, 51838], "temperature": 0.0, "avg_logprob": -0.09935545428045865, "compression_ratio": 1.6996336996336996, "no_speech_prob": 0.0015007948968559504}, {"id": 1292, "seek": 329478, "start": 3294.82, "end": 3296.2200000000003, "text": " It's a good start.", "tokens": [50366, 467, 311, 257, 665, 722, 13, 50436], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1293, "seek": 329478, "start": 3296.2200000000003, "end": 3301.2200000000003, "text": " And so if you say, okay, let's limit the discussion", "tokens": [50436, 400, 370, 498, 291, 584, 11, 1392, 11, 718, 311, 4948, 264, 5017, 50686], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1294, "seek": 329478, "start": 3302.46, "end": 3304.6600000000003, "text": " and not say that this is a full intelligence,", "tokens": [50748, 293, 406, 584, 300, 341, 307, 257, 1577, 7599, 11, 50858], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1295, "seek": 329478, "start": 3304.6600000000003, "end": 3305.6600000000003, "text": " they can do everything", "tokens": [50858, 436, 393, 360, 1203, 50908], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1296, "seek": 329478, "start": 3305.6600000000003, "end": 3308.82, "text": " that any intelligent being ever could, right?", "tokens": [50908, 300, 604, 13232, 885, 1562, 727, 11, 558, 30, 51066], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1297, "seek": 329478, "start": 3308.82, "end": 3310.7000000000003, "text": " But does it cross that threshold of,", "tokens": [51066, 583, 775, 309, 3278, 300, 14678, 295, 11, 51160], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1298, "seek": 329478, "start": 3310.7000000000003, "end": 3313.5800000000004, "text": " could it be as intelligent as a person, right?", "tokens": [51160, 727, 309, 312, 382, 13232, 382, 257, 954, 11, 558, 30, 51304], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1299, "seek": 329478, "start": 3313.5800000000004, "end": 3315.42, "text": " And I think it could be.", "tokens": [51304, 400, 286, 519, 309, 727, 312, 13, 51396], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1300, "seek": 329478, "start": 3315.42, "end": 3316.94, "text": " So anyways, as to what it is,", "tokens": [51396, 407, 13448, 11, 382, 281, 437, 309, 307, 11, 51472], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1301, "seek": 329478, "start": 3316.94, "end": 3321.94, "text": " it's based on older ideas of cognitive architectures", "tokens": [51472, 309, 311, 2361, 322, 4906, 3487, 295, 15605, 6331, 1303, 51722], "temperature": 0.0, "avg_logprob": -0.09500513164275283, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0003682306851260364}, {"id": 1302, "seek": 332194, "start": 3322.82, "end": 3324.46, "text": " which really kind of came about", "tokens": [50408, 597, 534, 733, 295, 1361, 466, 50490], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1303, "seek": 332194, "start": 3324.46, "end": 3326.14, "text": " as one of the primary theories", "tokens": [50490, 382, 472, 295, 264, 6194, 13667, 50574], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1304, "seek": 332194, "start": 3326.14, "end": 3329.82, "text": " of human level artificial intelligence in the 70s.", "tokens": [50574, 295, 1952, 1496, 11677, 7599, 294, 264, 5285, 82, 13, 50758], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1305, "seek": 332194, "start": 3329.82, "end": 3333.38, "text": " So there's SOAR, which is S-O-A-R and ACT-R,", "tokens": [50758, 407, 456, 311, 10621, 1899, 11, 597, 307, 318, 12, 46, 12, 32, 12, 49, 293, 40341, 12, 49, 11, 50936], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1306, "seek": 332194, "start": 3333.38, "end": 3338.38, "text": " which are the two kind of forerunner cognitive architectures.", "tokens": [50936, 597, 366, 264, 732, 733, 295, 337, 260, 409, 1193, 15605, 6331, 1303, 13, 51186], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1307, "seek": 332194, "start": 3338.5, "end": 3340.1, "text": " And those cognitive architectures", "tokens": [51192, 400, 729, 15605, 6331, 1303, 51272], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1308, "seek": 332194, "start": 3340.1, "end": 3341.26, "text": " are used all over the place.", "tokens": [51272, 366, 1143, 439, 670, 264, 1081, 13, 51330], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1309, "seek": 332194, "start": 3341.26, "end": 3342.82, "text": " They're used in the Mars rovers,", "tokens": [51330, 814, 434, 1143, 294, 264, 9692, 744, 840, 11, 51408], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1310, "seek": 332194, "start": 3342.82, "end": 3343.98, "text": " they're used in satellites,", "tokens": [51408, 436, 434, 1143, 294, 24960, 11, 51466], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1311, "seek": 332194, "start": 3343.98, "end": 3345.78, "text": " they're used in rockets,", "tokens": [51466, 436, 434, 1143, 294, 28361, 11, 51556], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1312, "seek": 332194, "start": 3345.78, "end": 3350.78, "text": " they're used in undersea ROVs, remote operated vehicles.", "tokens": [51556, 436, 434, 1143, 294, 833, 17531, 9025, 53, 82, 11, 8607, 20826, 8948, 13, 51806], "temperature": 0.0, "avg_logprob": -0.12125636273481714, "compression_ratio": 1.7603305785123966, "no_speech_prob": 0.01690160483121872}, {"id": 1313, "seek": 335078, "start": 3350.78, "end": 3353.5800000000004, "text": " So cognitive architectures already give robots", "tokens": [50364, 407, 15605, 6331, 1303, 1217, 976, 14733, 50504], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1314, "seek": 335078, "start": 3353.5800000000004, "end": 3354.7000000000003, "text": " a lot of autonomy.", "tokens": [50504, 257, 688, 295, 27278, 13, 50560], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1315, "seek": 335078, "start": 3355.5800000000004, "end": 3359.78, "text": " So there's that kind of, okay, they exist, they work.", "tokens": [50604, 407, 456, 311, 300, 733, 295, 11, 1392, 11, 436, 2514, 11, 436, 589, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1316, "seek": 335078, "start": 3359.78, "end": 3361.34, "text": " You know, it's not Skynet though,", "tokens": [50814, 509, 458, 11, 309, 311, 406, 7324, 2534, 302, 1673, 11, 50892], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1317, "seek": 335078, "start": 3361.34, "end": 3363.78, "text": " it's not gonna take over the world.", "tokens": [50892, 309, 311, 406, 799, 747, 670, 264, 1002, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1318, "seek": 335078, "start": 3363.78, "end": 3367.26, "text": " So when I got access to GPT-3, I said, what if,", "tokens": [51014, 407, 562, 286, 658, 2105, 281, 26039, 51, 12, 18, 11, 286, 848, 11, 437, 498, 11, 51188], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1319, "seek": 335078, "start": 3367.26, "end": 3369.78, "text": " instead of hard coding a lot of these modules,", "tokens": [51188, 2602, 295, 1152, 17720, 257, 688, 295, 613, 16679, 11, 51314], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1320, "seek": 335078, "start": 3369.78, "end": 3372.38, "text": " these different components of a cognitive architecture,", "tokens": [51314, 613, 819, 6677, 295, 257, 15605, 9482, 11, 51444], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1321, "seek": 335078, "start": 3372.38, "end": 3375.6600000000003, "text": " what if we give them the flexibility of GPT-3?", "tokens": [51444, 437, 498, 321, 976, 552, 264, 12635, 295, 26039, 51, 12, 18, 30, 51608], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1322, "seek": 335078, "start": 3375.6600000000003, "end": 3378.5, "text": " And that's really kind of, that was my central idea.", "tokens": [51608, 400, 300, 311, 534, 733, 295, 11, 300, 390, 452, 5777, 1558, 13, 51750], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1323, "seek": 335078, "start": 3378.5, "end": 3379.86, "text": " I said, okay, all these ideas", "tokens": [51750, 286, 848, 11, 1392, 11, 439, 613, 3487, 51818], "temperature": 0.0, "avg_logprob": -0.12267668587820871, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.00014881040260661393}, {"id": 1324, "seek": 337986, "start": 3379.86, "end": 3381.82, "text": " that have been kicking around for the last decade,", "tokens": [50364, 300, 362, 668, 19137, 926, 337, 264, 1036, 10378, 11, 50462], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1325, "seek": 337986, "start": 3381.82, "end": 3383.58, "text": " what if I put them all together", "tokens": [50462, 437, 498, 286, 829, 552, 439, 1214, 50550], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1326, "seek": 337986, "start": 3383.58, "end": 3386.54, "text": " and design an architecture that is based on,", "tokens": [50550, 293, 1715, 364, 9482, 300, 307, 2361, 322, 11, 50698], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1327, "seek": 337986, "start": 3386.54, "end": 3389.34, "text": " you know, roughly based on the human brain,", "tokens": [50698, 291, 458, 11, 9810, 2361, 322, 264, 1952, 3567, 11, 50838], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1328, "seek": 337986, "start": 3389.34, "end": 3392.38, "text": " the way, you know, everything that I've learned about it.", "tokens": [50838, 264, 636, 11, 291, 458, 11, 1203, 300, 286, 600, 3264, 466, 309, 13, 50990], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1329, "seek": 337986, "start": 3392.38, "end": 3394.6200000000003, "text": " I've got a book to recommend.", "tokens": [50990, 286, 600, 658, 257, 1446, 281, 2748, 13, 51102], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1330, "seek": 337986, "start": 3394.6200000000003, "end": 3397.58, "text": " So there's an author called V.S. Ramachandran,", "tokens": [51102, 407, 456, 311, 364, 3793, 1219, 691, 13, 50, 13, 9078, 608, 474, 4257, 11, 51250], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1331, "seek": 337986, "start": 3397.58, "end": 3399.86, "text": " who is a neuroscientist", "tokens": [51250, 567, 307, 257, 28813, 5412, 468, 51364], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1332, "seek": 337986, "start": 3399.86, "end": 3402.54, "text": " and he's been writing books for years now.", "tokens": [51364, 293, 415, 311, 668, 3579, 3642, 337, 924, 586, 13, 51498], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1333, "seek": 337986, "start": 3402.54, "end": 3404.7400000000002, "text": " He wrote a book called, Phantom's in the Brain,", "tokens": [51498, 634, 4114, 257, 1446, 1219, 11, 34689, 311, 294, 264, 29783, 11, 51608], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1334, "seek": 337986, "start": 3404.7400000000002, "end": 3406.9, "text": " which actually looks at how the human brain works", "tokens": [51608, 597, 767, 1542, 412, 577, 264, 1952, 3567, 1985, 51716], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1335, "seek": 337986, "start": 3406.9, "end": 3408.3, "text": " when it breaks.", "tokens": [51716, 562, 309, 9857, 13, 51786], "temperature": 0.0, "avg_logprob": -0.11640172685895647, "compression_ratio": 1.6735395189003437, "no_speech_prob": 0.002049836562946439}, {"id": 1336, "seek": 340830, "start": 3408.3, "end": 3411.02, "text": " And so in that book, which, you know,", "tokens": [50364, 400, 370, 294, 300, 1446, 11, 597, 11, 291, 458, 11, 50500], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1337, "seek": 340830, "start": 3411.02, "end": 3414.5800000000004, "text": " I saw the television series almost 20 years ago", "tokens": [50500, 286, 1866, 264, 8815, 2638, 1920, 945, 924, 2057, 50678], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1338, "seek": 340830, "start": 3416.02, "end": 3417.1800000000003, "text": " that came out.", "tokens": [50750, 300, 1361, 484, 13, 50808], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1339, "seek": 340830, "start": 3417.1800000000003, "end": 3419.2200000000003, "text": " And so I learned a lot about like, okay,", "tokens": [50808, 400, 370, 286, 3264, 257, 688, 466, 411, 11, 1392, 11, 50910], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1340, "seek": 340830, "start": 3419.2200000000003, "end": 3421.1000000000004, "text": " how does the brain communicate with itself?", "tokens": [50910, 577, 775, 264, 3567, 7890, 365, 2564, 30, 51004], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1341, "seek": 340830, "start": 3421.1000000000004, "end": 3423.2200000000003, "text": " What is going on inside the brain", "tokens": [51004, 708, 307, 516, 322, 1854, 264, 3567, 51110], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1342, "seek": 340830, "start": 3423.2200000000003, "end": 3426.1800000000003, "text": " that creates intelligent behavior and intelligent thoughts?", "tokens": [51110, 300, 7829, 13232, 5223, 293, 13232, 4598, 30, 51258], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1343, "seek": 340830, "start": 3426.1800000000003, "end": 3429.3, "text": " And so I modeled natural language cognitive architecture", "tokens": [51258, 400, 370, 286, 37140, 3303, 2856, 15605, 9482, 51414], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1344, "seek": 340830, "start": 3429.3, "end": 3430.7000000000003, "text": " on, you know, what I learned there.", "tokens": [51414, 322, 11, 291, 458, 11, 437, 286, 3264, 456, 13, 51484], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1345, "seek": 340830, "start": 3430.7000000000003, "end": 3432.82, "text": " I picked up a whole bunch of other books.", "tokens": [51484, 286, 6183, 493, 257, 1379, 3840, 295, 661, 3642, 13, 51590], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1346, "seek": 340830, "start": 3432.82, "end": 3435.86, "text": " There's another one called On Task by David Bader.", "tokens": [51590, 821, 311, 1071, 472, 1219, 1282, 30428, 538, 4389, 363, 8312, 13, 51742], "temperature": 0.0, "avg_logprob": -0.11747076472298043, "compression_ratio": 1.6258741258741258, "no_speech_prob": 0.00043047103099524975}, {"id": 1347, "seek": 343586, "start": 3435.9, "end": 3438.86, "text": " That was a great book that helped me kind of understand", "tokens": [50366, 663, 390, 257, 869, 1446, 300, 4254, 385, 733, 295, 1223, 50514], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1348, "seek": 343586, "start": 3438.86, "end": 3441.86, "text": " cognitive control, which is how do you focus on something?", "tokens": [50514, 15605, 1969, 11, 597, 307, 577, 360, 291, 1879, 322, 746, 30, 50664], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1349, "seek": 343586, "start": 3441.86, "end": 3443.7400000000002, "text": " How do you decide what to do?", "tokens": [50664, 1012, 360, 291, 4536, 437, 281, 360, 30, 50758], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1350, "seek": 343586, "start": 3443.7400000000002, "end": 3445.6600000000003, "text": " How do you plan a task?", "tokens": [50758, 1012, 360, 291, 1393, 257, 5633, 30, 50854], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1351, "seek": 343586, "start": 3445.6600000000003, "end": 3448.6200000000003, "text": " So I read all these books, did a lot of experiments,", "tokens": [50854, 407, 286, 1401, 439, 613, 3642, 11, 630, 257, 688, 295, 12050, 11, 51002], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1352, "seek": 343586, "start": 3448.6200000000003, "end": 3452.1800000000003, "text": " and I realized, so the basic model of robotics", "tokens": [51002, 293, 286, 5334, 11, 370, 264, 3875, 2316, 295, 34145, 51180], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1353, "seek": 343586, "start": 3452.1800000000003, "end": 3455.9, "text": " is there's input output, sorry, input processing output.", "tokens": [51180, 307, 456, 311, 4846, 5598, 11, 2597, 11, 4846, 9007, 5598, 13, 51366], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1354, "seek": 343586, "start": 3455.9, "end": 3458.6600000000003, "text": " Those are the three steps of all robotics class.", "tokens": [51366, 3950, 366, 264, 1045, 4439, 295, 439, 34145, 1508, 13, 51504], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1355, "seek": 343586, "start": 3458.6600000000003, "end": 3460.2200000000003, "text": " You go to robotics 101.", "tokens": [51504, 509, 352, 281, 34145, 21055, 13, 51582], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1356, "seek": 343586, "start": 3460.2200000000003, "end": 3461.06, "text": " That's what they'll tell you.", "tokens": [51582, 663, 311, 437, 436, 603, 980, 291, 13, 51624], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1357, "seek": 343586, "start": 3461.06, "end": 3463.1800000000003, "text": " It's a loop, input processing output.", "tokens": [51624, 467, 311, 257, 6367, 11, 4846, 9007, 5598, 13, 51730], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1358, "seek": 343586, "start": 3463.1800000000003, "end": 3465.1, "text": " And then of course, it's within an environment.", "tokens": [51730, 400, 550, 295, 1164, 11, 309, 311, 1951, 364, 2823, 13, 51826], "temperature": 0.0, "avg_logprob": -0.10691494041389518, "compression_ratio": 1.7248322147651007, "no_speech_prob": 0.000430512212915346}, {"id": 1359, "seek": 346510, "start": 3465.1, "end": 3466.98, "text": " So the output affects the environment,", "tokens": [50364, 407, 264, 5598, 11807, 264, 2823, 11, 50458], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1360, "seek": 346510, "start": 3466.98, "end": 3469.54, "text": " which affects the next input cycle.", "tokens": [50458, 597, 11807, 264, 958, 4846, 6586, 13, 50586], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1361, "seek": 346510, "start": 3469.54, "end": 3472.2999999999997, "text": " And, you know, your high speed robots", "tokens": [50586, 400, 11, 291, 458, 11, 428, 1090, 3073, 14733, 50724], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1362, "seek": 346510, "start": 3472.2999999999997, "end": 3474.2999999999997, "text": " just have a short cycle.", "tokens": [50724, 445, 362, 257, 2099, 6586, 13, 50824], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1363, "seek": 346510, "start": 3474.2999999999997, "end": 3479.2999999999997, "text": " Your robots like the Mars rover has a much slower cycle", "tokens": [50824, 2260, 14733, 411, 264, 9692, 45767, 575, 257, 709, 14009, 6586, 51074], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1364, "seek": 346510, "start": 3479.2999999999997, "end": 3481.22, "text": " where it will, you know, it'll take input.", "tokens": [51074, 689, 309, 486, 11, 291, 458, 11, 309, 603, 747, 4846, 13, 51170], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1365, "seek": 346510, "start": 3481.22, "end": 3483.3399999999997, "text": " It'll plan for 10 or 15 minutes", "tokens": [51170, 467, 603, 1393, 337, 1266, 420, 2119, 2077, 51276], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1366, "seek": 346510, "start": 3483.3399999999997, "end": 3484.86, "text": " and it'll make a move, right?", "tokens": [51276, 293, 309, 603, 652, 257, 1286, 11, 558, 30, 51352], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1367, "seek": 346510, "start": 3484.86, "end": 3488.06, "text": " It'll drive five feet and then it'll stop and assess.", "tokens": [51352, 467, 603, 3332, 1732, 3521, 293, 550, 309, 603, 1590, 293, 5877, 13, 51512], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1368, "seek": 346510, "start": 3488.06, "end": 3490.74, "text": " It'll take in more input, come up with another plan,", "tokens": [51512, 467, 603, 747, 294, 544, 4846, 11, 808, 493, 365, 1071, 1393, 11, 51646], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1369, "seek": 346510, "start": 3490.74, "end": 3491.66, "text": " do it again.", "tokens": [51646, 360, 309, 797, 13, 51692], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1370, "seek": 346510, "start": 3491.66, "end": 3494.7799999999997, "text": " So that's how something like the Mars rover is autonomous.", "tokens": [51692, 407, 300, 311, 577, 746, 411, 264, 9692, 45767, 307, 23797, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1278355822843664, "compression_ratio": 1.7536764705882353, "no_speech_prob": 0.0002611331292428076}, {"id": 1371, "seek": 349510, "start": 3495.46, "end": 3497.86, "text": " So I said, okay, well, what if,", "tokens": [50382, 407, 286, 848, 11, 1392, 11, 731, 11, 437, 498, 11, 50502], "temperature": 0.0, "avg_logprob": -0.14217225382150697, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00017949839821085334}, {"id": 1372, "seek": 349510, "start": 3497.86, "end": 3500.66, "text": " what if that input output cycle is all text", "tokens": [50502, 437, 498, 300, 4846, 5598, 6586, 307, 439, 2487, 50642], "temperature": 0.0, "avg_logprob": -0.14217225382150697, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00017949839821085334}, {"id": 1373, "seek": 349510, "start": 3500.66, "end": 3502.7799999999997, "text": " because GPT-3 is really fast?", "tokens": [50642, 570, 26039, 51, 12, 18, 307, 534, 2370, 30, 50748], "temperature": 0.0, "avg_logprob": -0.14217225382150697, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00017949839821085334}, {"id": 1374, "seek": 349510, "start": 3503.7, "end": 3507.06, "text": " And then, so that's what I ended up calling the outer loop,", "tokens": [50794, 400, 550, 11, 370, 300, 311, 437, 286, 4590, 493, 5141, 264, 10847, 6367, 11, 50962], "temperature": 0.0, "avg_logprob": -0.14217225382150697, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00017949839821085334}, {"id": 1375, "seek": 349510, "start": 3507.06, "end": 3509.74, "text": " is that input processing output loop.", "tokens": [50962, 307, 300, 4846, 9007, 5598, 6367, 13, 51096], "temperature": 0.0, "avg_logprob": -0.14217225382150697, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00017949839821085334}, {"id": 1376, "seek": 349510, "start": 3509.74, "end": 3512.7, "text": " But humans don't think like that.", "tokens": [51096, 583, 6255, 500, 380, 519, 411, 300, 13, 51244], "temperature": 0.0, "avg_logprob": -0.14217225382150697, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00017949839821085334}, {"id": 1377, "seek": 349510, "start": 3512.7, "end": 3515.54, "text": " You know, we have an internal monologue that's going on.", "tokens": [51244, 509, 458, 11, 321, 362, 364, 6920, 1108, 42298, 300, 311, 516, 322, 13, 51386], "temperature": 0.0, "avg_logprob": -0.14217225382150697, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00017949839821085334}, {"id": 1378, "seek": 349510, "start": 3515.54, "end": 3520.54, "text": " So I kind of, I took a long time to figure that one out.", "tokens": [51386, 407, 286, 733, 295, 11, 286, 1890, 257, 938, 565, 281, 2573, 300, 472, 484, 13, 51636], "temperature": 0.0, "avg_logprob": -0.14217225382150697, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00017949839821085334}, {"id": 1379, "seek": 349510, "start": 3521.5, "end": 3524.38, "text": " And so there's this outer loop of input processing output.", "tokens": [51684, 400, 370, 456, 311, 341, 10847, 6367, 295, 4846, 9007, 5598, 13, 51828], "temperature": 0.0, "avg_logprob": -0.14217225382150697, "compression_ratio": 1.6734693877551021, "no_speech_prob": 0.00017949839821085334}, {"id": 1380, "seek": 352438, "start": 3524.38, "end": 3526.58, "text": " And then I came up with this idea of an inner loop", "tokens": [50364, 400, 550, 286, 1361, 493, 365, 341, 1558, 295, 364, 7284, 6367, 50474], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1381, "seek": 352438, "start": 3526.58, "end": 3528.02, "text": " because what is, you know,", "tokens": [50474, 570, 437, 307, 11, 291, 458, 11, 50546], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1382, "seek": 352438, "start": 3528.02, "end": 3530.3, "text": " if you're just sitting there thinking, right?", "tokens": [50546, 498, 291, 434, 445, 3798, 456, 1953, 11, 558, 30, 50660], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1383, "seek": 352438, "start": 3530.3, "end": 3533.38, "text": " You're, you know, in your comfiest chair or your in bed,", "tokens": [50660, 509, 434, 11, 291, 458, 11, 294, 428, 395, 69, 6495, 6090, 420, 428, 294, 2901, 11, 50814], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1384, "seek": 352438, "start": 3533.38, "end": 3534.26, "text": " your brain won't stop.", "tokens": [50814, 428, 3567, 1582, 380, 1590, 13, 50858], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1385, "seek": 352438, "start": 3534.26, "end": 3535.78, "text": " You're not outputting anything", "tokens": [50858, 509, 434, 406, 5598, 783, 1340, 50934], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1386, "seek": 352438, "start": 3535.78, "end": 3537.54, "text": " and you're not taking in any new input,", "tokens": [50934, 293, 291, 434, 406, 1940, 294, 604, 777, 4846, 11, 51022], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1387, "seek": 352438, "start": 3537.54, "end": 3539.2200000000003, "text": " but you're still thinking, right?", "tokens": [51022, 457, 291, 434, 920, 1953, 11, 558, 30, 51106], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1388, "seek": 352438, "start": 3539.2200000000003, "end": 3542.1, "text": " Humans can still do work even if you're not doing anything.", "tokens": [51106, 35809, 393, 920, 360, 589, 754, 498, 291, 434, 406, 884, 1340, 13, 51250], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1389, "seek": 352438, "start": 3542.1, "end": 3545.54, "text": " And that cognitive work is like rumination.", "tokens": [51250, 400, 300, 15605, 589, 307, 411, 8347, 2486, 13, 51422], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1390, "seek": 352438, "start": 3545.54, "end": 3548.54, "text": " So I figured out a way to model that internal rumination.", "tokens": [51422, 407, 286, 8932, 484, 257, 636, 281, 2316, 300, 6920, 8347, 2486, 13, 51572], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1391, "seek": 352438, "start": 3548.54, "end": 3550.02, "text": " I call that the inner loop.", "tokens": [51572, 286, 818, 300, 264, 7284, 6367, 13, 51646], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1392, "seek": 352438, "start": 3550.02, "end": 3553.54, "text": " And so there's, it works pretty similarly", "tokens": [51646, 400, 370, 456, 311, 11, 309, 1985, 1238, 14138, 51822], "temperature": 0.0, "avg_logprob": -0.10450337317682082, "compression_ratio": 1.836734693877551, "no_speech_prob": 0.0005032455665059388}, {"id": 1393, "seek": 355354, "start": 3553.58, "end": 3558.2599999999998, "text": " where you go, the inner loop kind of draws up memories.", "tokens": [50366, 689, 291, 352, 11, 264, 7284, 6367, 733, 295, 20045, 493, 8495, 13, 50600], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1394, "seek": 355354, "start": 3558.2599999999998, "end": 3560.74, "text": " It says, okay, what's a memory that I could think on", "tokens": [50600, 467, 1619, 11, 1392, 11, 437, 311, 257, 4675, 300, 286, 727, 519, 322, 50724], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1395, "seek": 355354, "start": 3560.74, "end": 3562.02, "text": " and what that I could iterate on?", "tokens": [50724, 293, 437, 300, 286, 727, 44497, 322, 30, 50788], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1396, "seek": 355354, "start": 3562.02, "end": 3564.62, "text": " What's a problem that I remember", "tokens": [50788, 708, 311, 257, 1154, 300, 286, 1604, 50918], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1397, "seek": 355354, "start": 3564.62, "end": 3566.38, "text": " that I could continue working on?", "tokens": [50918, 300, 286, 727, 2354, 1364, 322, 30, 51006], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1398, "seek": 355354, "start": 3566.38, "end": 3568.9, "text": " And so there's this, if you were to diagram it out,", "tokens": [51006, 400, 370, 456, 311, 341, 11, 498, 291, 645, 281, 10686, 309, 484, 11, 51132], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1399, "seek": 355354, "start": 3568.9, "end": 3570.62, "text": " it almost looks like a figure eight, right?", "tokens": [51132, 309, 1920, 1542, 411, 257, 2573, 3180, 11, 558, 30, 51218], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1400, "seek": 355354, "start": 3570.62, "end": 3572.46, "text": " Where you've got an inner loop and an outer loop", "tokens": [51218, 2305, 291, 600, 658, 364, 7284, 6367, 293, 364, 10847, 6367, 51310], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1401, "seek": 355354, "start": 3572.46, "end": 3574.98, "text": " and they intersect and they keep intersecting", "tokens": [51310, 293, 436, 27815, 293, 436, 1066, 27815, 278, 51436], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1402, "seek": 355354, "start": 3574.98, "end": 3576.62, "text": " every cycle they intersect.", "tokens": [51436, 633, 6586, 436, 27815, 13, 51518], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1403, "seek": 355354, "start": 3576.62, "end": 3578.58, "text": " And so then they can affect each other", "tokens": [51518, 400, 370, 550, 436, 393, 3345, 1184, 661, 51616], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1404, "seek": 355354, "start": 3578.58, "end": 3580.34, "text": " and generate an output.", "tokens": [51616, 293, 8460, 364, 5598, 13, 51704], "temperature": 0.0, "avg_logprob": -0.09753650777480181, "compression_ratio": 1.8528301886792453, "no_speech_prob": 0.0005032293847762048}, {"id": 1405, "seek": 358034, "start": 3580.34, "end": 3582.42, "text": " I built a prototype of this on Discord.", "tokens": [50364, 286, 3094, 257, 19475, 295, 341, 322, 32623, 13, 50468], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1406, "seek": 358034, "start": 3583.6200000000003, "end": 3585.86, "text": " And of course, Discord is an ideal place", "tokens": [50528, 400, 295, 1164, 11, 32623, 307, 364, 7157, 1081, 50640], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1407, "seek": 358034, "start": 3585.86, "end": 3586.86, "text": " because it's all text-based.", "tokens": [50640, 570, 309, 311, 439, 2487, 12, 6032, 13, 50690], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1408, "seek": 358034, "start": 3586.86, "end": 3589.34, "text": " So the input is text, the output is text,", "tokens": [50690, 407, 264, 4846, 307, 2487, 11, 264, 5598, 307, 2487, 11, 50814], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1409, "seek": 358034, "start": 3589.34, "end": 3591.06, "text": " which is GPT-3 native.", "tokens": [50814, 597, 307, 26039, 51, 12, 18, 8470, 13, 50900], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1410, "seek": 358034, "start": 3591.06, "end": 3593.26, "text": " You don't have to translate it into robotic actions", "tokens": [50900, 509, 500, 380, 362, 281, 13799, 309, 666, 30468, 5909, 51010], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1411, "seek": 358034, "start": 3593.26, "end": 3595.7400000000002, "text": " or video or anything like that.", "tokens": [51010, 420, 960, 420, 1340, 411, 300, 13, 51134], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1412, "seek": 358034, "start": 3595.7400000000002, "end": 3598.1000000000004, "text": " And I realized I was onto something", "tokens": [51134, 400, 286, 5334, 286, 390, 3911, 746, 51252], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1413, "seek": 358034, "start": 3598.1000000000004, "end": 3600.6600000000003, "text": " when I started having philosophical conversations", "tokens": [51252, 562, 286, 1409, 1419, 25066, 7315, 51380], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1414, "seek": 358034, "start": 3600.6600000000003, "end": 3602.7400000000002, "text": " with my chatbot,", "tokens": [51380, 365, 452, 5081, 18870, 11, 51484], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1415, "seek": 358034, "start": 3602.7400000000002, "end": 3606.02, "text": " with my natural language, cognitive architecture chatbot.", "tokens": [51484, 365, 452, 3303, 2856, 11, 15605, 9482, 5081, 18870, 13, 51648], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1416, "seek": 358034, "start": 3606.02, "end": 3608.58, "text": " And I was having a debate", "tokens": [51648, 400, 286, 390, 1419, 257, 7958, 51776], "temperature": 0.0, "avg_logprob": -0.14358237532318616, "compression_ratio": 1.6360294117647058, "no_speech_prob": 0.0010002575581893325}, {"id": 1417, "seek": 360858, "start": 3608.58, "end": 3613.58, "text": " with the bot that I built about the ethics of AGI.", "tokens": [50364, 365, 264, 10592, 300, 286, 3094, 466, 264, 19769, 295, 316, 26252, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1418, "seek": 360858, "start": 3613.7799999999997, "end": 3614.62, "text": " And it was learning", "tokens": [50624, 400, 309, 390, 2539, 50666], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1419, "seek": 360858, "start": 3614.62, "end": 3616.54, "text": " and it was able to retrieve memories", "tokens": [50666, 293, 309, 390, 1075, 281, 30254, 8495, 50762], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1420, "seek": 360858, "start": 3616.54, "end": 3618.5, "text": " of what I had said before.", "tokens": [50762, 295, 437, 286, 632, 848, 949, 13, 50860], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1421, "seek": 360858, "start": 3618.5, "end": 3622.06, "text": " And I had a few friends on that test server as well.", "tokens": [50860, 400, 286, 632, 257, 1326, 1855, 322, 300, 1500, 7154, 382, 731, 13, 51038], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1422, "seek": 360858, "start": 3622.06, "end": 3623.8199999999997, "text": " And of course, you know, you invite someone,", "tokens": [51038, 400, 295, 1164, 11, 291, 458, 11, 291, 7980, 1580, 11, 51126], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1423, "seek": 360858, "start": 3623.8199999999997, "end": 3625.86, "text": " you say, hey, I've got a prototype AGI.", "tokens": [51126, 291, 584, 11, 4177, 11, 286, 600, 658, 257, 19475, 316, 26252, 13, 51228], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1424, "seek": 360858, "start": 3625.86, "end": 3626.94, "text": " What's the first thing they try and do", "tokens": [51228, 708, 311, 264, 700, 551, 436, 853, 293, 360, 51282], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1425, "seek": 360858, "start": 3626.94, "end": 3629.02, "text": " is they try and break it and they did.", "tokens": [51282, 307, 436, 853, 293, 1821, 309, 293, 436, 630, 13, 51386], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1426, "seek": 360858, "start": 3629.02, "end": 3630.8199999999997, "text": " So it's still pretty fragile.", "tokens": [51386, 407, 309, 311, 920, 1238, 23847, 13, 51476], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1427, "seek": 360858, "start": 3630.8199999999997, "end": 3632.58, "text": " But yeah, so that's the high level", "tokens": [51476, 583, 1338, 11, 370, 300, 311, 264, 1090, 1496, 51564], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1428, "seek": 360858, "start": 3632.58, "end": 3635.5, "text": " of a natural language cognitive architecture.", "tokens": [51564, 295, 257, 3303, 2856, 15605, 9482, 13, 51710], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1429, "seek": 360858, "start": 3635.5, "end": 3636.7799999999997, "text": " And it's already outdated, right?", "tokens": [51710, 400, 309, 311, 1217, 36313, 11, 558, 30, 51774], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1430, "seek": 360858, "start": 3636.7799999999997, "end": 3637.9, "text": " Because we've got fine-tuning,", "tokens": [51774, 1436, 321, 600, 658, 2489, 12, 83, 37726, 11, 51830], "temperature": 0.0, "avg_logprob": -0.12474797485740322, "compression_ratio": 1.685897435897436, "no_speech_prob": 0.0018665716052055359}, {"id": 1431, "seek": 363790, "start": 3637.9, "end": 3639.5, "text": " we've got the instruct series.", "tokens": [50364, 321, 600, 658, 264, 7232, 2638, 13, 50444], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1432, "seek": 363790, "start": 3639.5, "end": 3641.7400000000002, "text": " I did all this research and wrote the book", "tokens": [50444, 286, 630, 439, 341, 2132, 293, 4114, 264, 1446, 50556], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1433, "seek": 363790, "start": 3641.7400000000002, "end": 3643.7400000000002, "text": " actually about a year ago now,", "tokens": [50556, 767, 466, 257, 1064, 2057, 586, 11, 50656], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1434, "seek": 363790, "start": 3643.7400000000002, "end": 3644.98, "text": " just before all this came out.", "tokens": [50656, 445, 949, 439, 341, 1361, 484, 13, 50718], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1435, "seek": 363790, "start": 3644.98, "end": 3646.1, "text": " So it's already outdated.", "tokens": [50718, 407, 309, 311, 1217, 36313, 13, 50774], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1436, "seek": 363790, "start": 3646.1, "end": 3648.82, "text": " That's why my research has moved on.", "tokens": [50774, 663, 311, 983, 452, 2132, 575, 4259, 322, 13, 50910], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1437, "seek": 363790, "start": 3648.82, "end": 3650.78, "text": " But yeah, so that's it at a high level.", "tokens": [50910, 583, 1338, 11, 370, 300, 311, 309, 412, 257, 1090, 1496, 13, 51008], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1438, "seek": 363790, "start": 3652.38, "end": 3653.6600000000003, "text": " Yeah, I mean, that's awesome.", "tokens": [51088, 865, 11, 286, 914, 11, 300, 311, 3476, 13, 51152], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1439, "seek": 363790, "start": 3653.6600000000003, "end": 3656.82, "text": " And by the way, like David, you did do a good job.", "tokens": [51152, 400, 538, 264, 636, 11, 411, 4389, 11, 291, 630, 360, 257, 665, 1691, 13, 51310], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1440, "seek": 363790, "start": 3656.82, "end": 3659.6600000000003, "text": " Like the diagrams in this book are quite helpful.", "tokens": [51310, 1743, 264, 36709, 294, 341, 1446, 366, 1596, 4961, 13, 51452], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1441, "seek": 363790, "start": 3659.6600000000003, "end": 3660.5, "text": " Excellent.", "tokens": [51452, 16723, 13, 51494], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1442, "seek": 363790, "start": 3660.5, "end": 3663.1800000000003, "text": " Like in addition to the text, like it's very clear.", "tokens": [51494, 1743, 294, 4500, 281, 264, 2487, 11, 411, 309, 311, 588, 1850, 13, 51628], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1443, "seek": 363790, "start": 3663.1800000000003, "end": 3665.78, "text": " Like I was able to fully follow along with all these,", "tokens": [51628, 1743, 286, 390, 1075, 281, 4498, 1524, 2051, 365, 439, 613, 11, 51758], "temperature": 0.0, "avg_logprob": -0.1374645755715566, "compression_ratio": 1.647457627118644, "no_speech_prob": 0.00019107740081381053}, {"id": 1444, "seek": 366578, "start": 3665.78, "end": 3668.9, "text": " essentially these different modules for the whole system", "tokens": [50364, 4476, 613, 819, 16679, 337, 264, 1379, 1185, 50520], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1445, "seek": 366578, "start": 3668.9, "end": 3672.94, "text": " of how a language model inspired AGI, quote unquote,", "tokens": [50520, 295, 577, 257, 2856, 2316, 7547, 316, 26252, 11, 6513, 37557, 11, 50722], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1446, "seek": 366578, "start": 3672.94, "end": 3675.26, "text": " could actually be like how it would work.", "tokens": [50722, 727, 767, 312, 411, 577, 309, 576, 589, 13, 50838], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1447, "seek": 366578, "start": 3676.1400000000003, "end": 3677.2200000000003, "text": " And so I was gonna ask you,", "tokens": [50882, 400, 370, 286, 390, 799, 1029, 291, 11, 50936], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1448, "seek": 366578, "start": 3677.2200000000003, "end": 3681.26, "text": " so the prototype also was, you made it to that stage", "tokens": [50936, 370, 264, 19475, 611, 390, 11, 291, 1027, 309, 281, 300, 3233, 51138], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1449, "seek": 366578, "start": 3681.26, "end": 3685.2200000000003, "text": " and it has just some fun, interesting results.", "tokens": [51138, 293, 309, 575, 445, 512, 1019, 11, 1880, 3542, 13, 51336], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1450, "seek": 366578, "start": 3685.2200000000003, "end": 3686.42, "text": " So that's awesome.", "tokens": [51336, 407, 300, 311, 3476, 13, 51396], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1451, "seek": 366578, "start": 3686.42, "end": 3689.02, "text": " What is the delta then between,", "tokens": [51396, 708, 307, 264, 8289, 550, 1296, 11, 51526], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1452, "seek": 366578, "start": 3689.02, "end": 3691.2200000000003, "text": " let's say even something like GPT-4", "tokens": [51526, 718, 311, 584, 754, 746, 411, 26039, 51, 12, 19, 51636], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1453, "seek": 366578, "start": 3691.2200000000003, "end": 3694.6200000000003, "text": " using the natural language cognitive architecture.", "tokens": [51636, 1228, 264, 3303, 2856, 15605, 9482, 13, 51806], "temperature": 0.0, "avg_logprob": -0.14422768679532139, "compression_ratio": 1.5795454545454546, "no_speech_prob": 0.0030741291120648384}, {"id": 1454, "seek": 369462, "start": 3694.62, "end": 3697.7799999999997, "text": " What's the delta between that and true AGI, right?", "tokens": [50364, 708, 311, 264, 8289, 1296, 300, 293, 2074, 316, 26252, 11, 558, 30, 50522], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1455, "seek": 369462, "start": 3697.7799999999997, "end": 3699.9, "text": " Like what's the difference there?", "tokens": [50522, 1743, 437, 311, 264, 2649, 456, 30, 50628], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1456, "seek": 369462, "start": 3699.9, "end": 3702.58, "text": " What skills, what patterns would you wanna see?", "tokens": [50628, 708, 3942, 11, 437, 8294, 576, 291, 1948, 536, 30, 50762], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1457, "seek": 369462, "start": 3702.58, "end": 3705.18, "text": " Yeah, so I mean, there's a lot", "tokens": [50762, 865, 11, 370, 286, 914, 11, 456, 311, 257, 688, 50892], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1458, "seek": 369462, "start": 3705.18, "end": 3707.46, "text": " that I haven't figured out yet, right?", "tokens": [50892, 300, 286, 2378, 380, 8932, 484, 1939, 11, 558, 30, 51006], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1459, "seek": 369462, "start": 3707.46, "end": 3708.8199999999997, "text": " Task switching, for instance,", "tokens": [51006, 30428, 16493, 11, 337, 5197, 11, 51074], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1460, "seek": 369462, "start": 3708.8199999999997, "end": 3713.8199999999997, "text": " is one thing that I haven't figured out how to solve", "tokens": [51074, 307, 472, 551, 300, 286, 2378, 380, 8932, 484, 577, 281, 5039, 51324], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1461, "seek": 369462, "start": 3714.06, "end": 3716.8599999999997, "text": " even after reading on task by David Bader.", "tokens": [51336, 754, 934, 3760, 322, 5633, 538, 4389, 363, 8312, 13, 51476], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1462, "seek": 369462, "start": 3716.8599999999997, "end": 3720.1, "text": " I, you know, that's one of the most complex things", "tokens": [51476, 286, 11, 291, 458, 11, 300, 311, 472, 295, 264, 881, 3997, 721, 51638], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1463, "seek": 369462, "start": 3720.1, "end": 3722.74, "text": " that humans can do is keeping track of different tasks", "tokens": [51638, 300, 6255, 393, 360, 307, 5145, 2837, 295, 819, 9608, 51770], "temperature": 0.0, "avg_logprob": -0.14032246039165713, "compression_ratio": 1.6564885496183206, "no_speech_prob": 0.00035692163510248065}, {"id": 1464, "seek": 372274, "start": 3722.74, "end": 3725.06, "text": " and jumping back between them.", "tokens": [50364, 293, 11233, 646, 1296, 552, 13, 50480], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1465, "seek": 372274, "start": 3725.06, "end": 3728.9399999999996, "text": " There's a whole litany of problems and limitations,", "tokens": [50480, 821, 311, 257, 1379, 7997, 1325, 295, 2740, 293, 15705, 11, 50674], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1466, "seek": 372274, "start": 3728.9399999999996, "end": 3733.9399999999996, "text": " but the intrinsic limitation of GPT-3 and GPT-4", "tokens": [50674, 457, 264, 35698, 27432, 295, 26039, 51, 12, 18, 293, 26039, 51, 12, 19, 50924], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1467, "seek": 372274, "start": 3735.22, "end": 3737.7799999999997, "text": " is they have no memory, right?", "tokens": [50988, 307, 436, 362, 572, 4675, 11, 558, 30, 51116], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1468, "seek": 372274, "start": 3737.7799999999997, "end": 3739.74, "text": " They're completely ephemeral.", "tokens": [51116, 814, 434, 2584, 308, 41245, 2790, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1469, "seek": 372274, "start": 3739.74, "end": 3741.4599999999996, "text": " And one of the most important things", "tokens": [51214, 400, 472, 295, 264, 881, 1021, 721, 51300], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1470, "seek": 372274, "start": 3741.4599999999996, "end": 3744.54, "text": " for any intelligent being is that it's got a memory, right?", "tokens": [51300, 337, 604, 13232, 885, 307, 300, 309, 311, 658, 257, 4675, 11, 558, 30, 51454], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1471, "seek": 372274, "start": 3744.54, "end": 3746.8999999999996, "text": " You know, you talk about, you know,", "tokens": [51454, 509, 458, 11, 291, 751, 466, 11, 291, 458, 11, 51572], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1472, "seek": 372274, "start": 3746.8999999999996, "end": 3749.14, "text": " there's famous people in history", "tokens": [51572, 456, 311, 4618, 561, 294, 2503, 51684], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1473, "seek": 372274, "start": 3749.14, "end": 3751.4599999999996, "text": " that had like, you know, photographic memories, right?", "tokens": [51684, 300, 632, 411, 11, 291, 458, 11, 8348, 299, 8495, 11, 558, 30, 51800], "temperature": 0.0, "avg_logprob": -0.09937177270145739, "compression_ratio": 1.6546184738955823, "no_speech_prob": 0.0015008675400167704}, {"id": 1474, "seek": 375146, "start": 3751.46, "end": 3754.1, "text": " And so even just having a really good memory", "tokens": [50364, 400, 370, 754, 445, 1419, 257, 534, 665, 4675, 50496], "temperature": 0.0, "avg_logprob": -0.1109151840209961, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0002694295544642955}, {"id": 1475, "seek": 375146, "start": 3754.1, "end": 3757.94, "text": " is a really important ingredient to having intelligence.", "tokens": [50496, 307, 257, 534, 1021, 14751, 281, 1419, 7599, 13, 50688], "temperature": 0.0, "avg_logprob": -0.1109151840209961, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0002694295544642955}, {"id": 1476, "seek": 375146, "start": 3757.94, "end": 3761.94, "text": " And so that's where I think that like GPT-3, GPT-4,", "tokens": [50688, 400, 370, 300, 311, 689, 286, 519, 300, 411, 26039, 51, 12, 18, 11, 26039, 51, 12, 19, 11, 50888], "temperature": 0.0, "avg_logprob": -0.1109151840209961, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0002694295544642955}, {"id": 1477, "seek": 375146, "start": 3763.7400000000002, "end": 3765.18, "text": " other multimodal models,", "tokens": [50978, 661, 32972, 378, 304, 5245, 11, 51050], "temperature": 0.0, "avg_logprob": -0.1109151840209961, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0002694295544642955}, {"id": 1478, "seek": 375146, "start": 3765.18, "end": 3768.42, "text": " they will never be fully AGI on their own.", "tokens": [51050, 436, 486, 1128, 312, 4498, 316, 26252, 322, 641, 1065, 13, 51212], "temperature": 0.0, "avg_logprob": -0.1109151840209961, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0002694295544642955}, {"id": 1479, "seek": 375146, "start": 3768.42, "end": 3771.26, "text": " They might be able to solve really great problems,", "tokens": [51212, 814, 1062, 312, 1075, 281, 5039, 534, 869, 2740, 11, 51354], "temperature": 0.0, "avg_logprob": -0.1109151840209961, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0002694295544642955}, {"id": 1480, "seek": 375146, "start": 3771.26, "end": 3775.1, "text": " but they're not gonna be able to remember you", "tokens": [51354, 457, 436, 434, 406, 799, 312, 1075, 281, 1604, 291, 51546], "temperature": 0.0, "avg_logprob": -0.1109151840209961, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0002694295544642955}, {"id": 1481, "seek": 375146, "start": 3775.1, "end": 3777.82, "text": " unless you add, you bolt a system onto the side,", "tokens": [51546, 5969, 291, 909, 11, 291, 13436, 257, 1185, 3911, 264, 1252, 11, 51682], "temperature": 0.0, "avg_logprob": -0.1109151840209961, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0002694295544642955}, {"id": 1482, "seek": 375146, "start": 3777.82, "end": 3779.18, "text": " some kind of database,", "tokens": [51682, 512, 733, 295, 8149, 11, 51750], "temperature": 0.0, "avg_logprob": -0.1109151840209961, "compression_ratio": 1.5725806451612903, "no_speech_prob": 0.0002694295544642955}, {"id": 1483, "seek": 377918, "start": 3779.22, "end": 3782.2999999999997, "text": " so that it can remember your interactions, right?", "tokens": [50366, 370, 300, 309, 393, 1604, 428, 13280, 11, 558, 30, 50520], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1484, "seek": 377918, "start": 3782.2999999999997, "end": 3786.18, "text": " So that's one thing, another difference between,", "tokens": [50520, 407, 300, 311, 472, 551, 11, 1071, 2649, 1296, 11, 50714], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1485, "seek": 377918, "start": 3786.18, "end": 3788.62, "text": " like what you might imagine as a true AGI", "tokens": [50714, 411, 437, 291, 1062, 3811, 382, 257, 2074, 316, 26252, 50836], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1486, "seek": 377918, "start": 3788.62, "end": 3790.2999999999997, "text": " or a full AGI is autonomy.", "tokens": [50836, 420, 257, 1577, 316, 26252, 307, 27278, 13, 50920], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1487, "seek": 377918, "start": 3791.46, "end": 3796.46, "text": " Because, you know, you, me, all of your listeners,", "tokens": [50978, 1436, 11, 291, 458, 11, 291, 11, 385, 11, 439, 295, 428, 23274, 11, 51228], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1488, "seek": 377918, "start": 3796.46, "end": 3799.2599999999998, "text": " we all have some kind of self-determination.", "tokens": [51228, 321, 439, 362, 512, 733, 295, 2698, 12, 49136, 2486, 13, 51368], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1489, "seek": 377918, "start": 3799.2599999999998, "end": 3800.4199999999996, "text": " I don't like to use free will", "tokens": [51368, 286, 500, 380, 411, 281, 764, 1737, 486, 51426], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1490, "seek": 377918, "start": 3800.4199999999996, "end": 3802.06, "text": " because that's too philosophical,", "tokens": [51426, 570, 300, 311, 886, 25066, 11, 51508], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1491, "seek": 377918, "start": 3802.06, "end": 3804.2599999999998, "text": " but we're all autonomous, right?", "tokens": [51508, 457, 321, 434, 439, 23797, 11, 558, 30, 51618], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1492, "seek": 377918, "start": 3804.2599999999998, "end": 3806.7799999999997, "text": " I'm an autonomous agent, you're an autonomous agent.", "tokens": [51618, 286, 478, 364, 23797, 9461, 11, 291, 434, 364, 23797, 9461, 13, 51744], "temperature": 0.0, "avg_logprob": -0.12492318514014493, "compression_ratio": 1.6926229508196722, "no_speech_prob": 0.00031995700555853546}, {"id": 1493, "seek": 380678, "start": 3806.78, "end": 3809.6200000000003, "text": " GPT-3 is not, it's transactional.", "tokens": [50364, 26039, 51, 12, 18, 307, 406, 11, 309, 311, 46688, 1966, 13, 50506], "temperature": 0.0, "avg_logprob": -0.07685749114505828, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.00022338035341817886}, {"id": 1494, "seek": 380678, "start": 3809.6200000000003, "end": 3812.1400000000003, "text": " It just sits there and waits like a hammer, it's a tool.", "tokens": [50506, 467, 445, 12696, 456, 293, 40597, 411, 257, 13017, 11, 309, 311, 257, 2290, 13, 50632], "temperature": 0.0, "avg_logprob": -0.07685749114505828, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.00022338035341817886}, {"id": 1495, "seek": 380678, "start": 3812.1400000000003, "end": 3816.38, "text": " It waits until you go pick it up and do something with it.", "tokens": [50632, 467, 40597, 1826, 291, 352, 1888, 309, 493, 293, 360, 746, 365, 309, 13, 50844], "temperature": 0.0, "avg_logprob": -0.07685749114505828, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.00022338035341817886}, {"id": 1496, "seek": 380678, "start": 3816.38, "end": 3818.78, "text": " And so that's one of the things that I was aiming for", "tokens": [50844, 400, 370, 300, 311, 472, 295, 264, 721, 300, 286, 390, 20253, 337, 50964], "temperature": 0.0, "avg_logprob": -0.07685749114505828, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.00022338035341817886}, {"id": 1497, "seek": 380678, "start": 3818.78, "end": 3820.82, "text": " when designing natural language cognitive architecture.", "tokens": [50964, 562, 14685, 3303, 2856, 15605, 9482, 13, 51066], "temperature": 0.0, "avg_logprob": -0.07685749114505828, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.00022338035341817886}, {"id": 1498, "seek": 380678, "start": 3820.82, "end": 3823.6600000000003, "text": " I said, how can we make something that's fully autonomous", "tokens": [51066, 286, 848, 11, 577, 393, 321, 652, 746, 300, 311, 4498, 23797, 51208], "temperature": 0.0, "avg_logprob": -0.07685749114505828, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.00022338035341817886}, {"id": 1499, "seek": 380678, "start": 3823.6600000000003, "end": 3826.6200000000003, "text": " that can think on its own and make its own decisions?", "tokens": [51208, 300, 393, 519, 322, 1080, 1065, 293, 652, 1080, 1065, 5327, 30, 51356], "temperature": 0.0, "avg_logprob": -0.07685749114505828, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.00022338035341817886}, {"id": 1500, "seek": 380678, "start": 3826.6200000000003, "end": 3829.1000000000004, "text": " And so in that respect,", "tokens": [51356, 400, 370, 294, 300, 3104, 11, 51480], "temperature": 0.0, "avg_logprob": -0.07685749114505828, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.00022338035341817886}, {"id": 1501, "seek": 380678, "start": 3829.1000000000004, "end": 3833.86, "text": " I don't think a single neural network could ever be an AGI.", "tokens": [51480, 286, 500, 380, 519, 257, 2167, 18161, 3209, 727, 1562, 312, 364, 316, 26252, 13, 51718], "temperature": 0.0, "avg_logprob": -0.07685749114505828, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.00022338035341817886}, {"id": 1502, "seek": 383386, "start": 3833.86, "end": 3837.46, "text": " I think that in order to achieve true, full AGI,", "tokens": [50364, 286, 519, 300, 294, 1668, 281, 4584, 2074, 11, 1577, 316, 26252, 11, 50544], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1503, "seek": 383386, "start": 3837.46, "end": 3840.1800000000003, "text": " it's gonna have to be some kind of cognitive architecture.", "tokens": [50544, 309, 311, 799, 362, 281, 312, 512, 733, 295, 15605, 9482, 13, 50680], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1504, "seek": 383386, "start": 3840.1800000000003, "end": 3841.58, "text": " And so at a minimum,", "tokens": [50680, 400, 370, 412, 257, 7285, 11, 50750], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1505, "seek": 383386, "start": 3841.58, "end": 3844.06, "text": " you're gonna have the neural network and a database,", "tokens": [50750, 291, 434, 799, 362, 264, 18161, 3209, 293, 257, 8149, 11, 50874], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1506, "seek": 383386, "start": 3844.06, "end": 3844.98, "text": " bare minimum.", "tokens": [50874, 6949, 7285, 13, 50920], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1507, "seek": 383386, "start": 3844.98, "end": 3847.1400000000003, "text": " You need something to store those memories,", "tokens": [50920, 509, 643, 746, 281, 3531, 729, 8495, 11, 51028], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1508, "seek": 383386, "start": 3847.1400000000003, "end": 3848.94, "text": " to store those ideas and beliefs,", "tokens": [51028, 281, 3531, 729, 3487, 293, 13585, 11, 51118], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1509, "seek": 383386, "start": 3848.94, "end": 3850.82, "text": " and then you need a way to interact with it.", "tokens": [51118, 293, 550, 291, 643, 257, 636, 281, 4648, 365, 309, 13, 51212], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1510, "seek": 383386, "start": 3850.82, "end": 3852.3, "text": " And so that's why, actually,", "tokens": [51212, 400, 370, 300, 311, 983, 11, 767, 11, 51286], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1511, "seek": 383386, "start": 3852.3, "end": 3854.94, "text": " that's why in natural language cognitive architecture,", "tokens": [51286, 300, 311, 983, 294, 3303, 2856, 15605, 9482, 11, 51418], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1512, "seek": 383386, "start": 3854.94, "end": 3858.42, "text": " the shared database is kind of the center of the design,", "tokens": [51418, 264, 5507, 8149, 307, 733, 295, 264, 3056, 295, 264, 1715, 11, 51592], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1513, "seek": 383386, "start": 3858.42, "end": 3860.6600000000003, "text": " which you might recall, like you can use SQLite,", "tokens": [51592, 597, 291, 1062, 9901, 11, 411, 291, 393, 764, 19200, 642, 11, 51704], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1514, "seek": 383386, "start": 3860.6600000000003, "end": 3862.98, "text": " you can use Solar or whatever,", "tokens": [51704, 291, 393, 764, 22385, 420, 2035, 11, 51820], "temperature": 0.0, "avg_logprob": -0.10777948989349158, "compression_ratio": 1.808724832214765, "no_speech_prob": 0.0006261913804337382}, {"id": 1515, "seek": 386298, "start": 3862.98, "end": 3865.06, "text": " but you need something to store ideas,", "tokens": [50364, 457, 291, 643, 746, 281, 3531, 3487, 11, 50468], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1516, "seek": 386298, "start": 3865.06, "end": 3866.9, "text": " memories and experiences.", "tokens": [50468, 8495, 293, 5235, 13, 50560], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1517, "seek": 386298, "start": 3866.9, "end": 3868.42, "text": " I actually think that blockchain", "tokens": [50560, 286, 767, 519, 300, 17176, 50636], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1518, "seek": 386298, "start": 3868.42, "end": 3871.02, "text": " will be a critical component to AGI", "tokens": [50636, 486, 312, 257, 4924, 6542, 281, 316, 26252, 50766], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1519, "seek": 386298, "start": 3871.02, "end": 3873.78, "text": " because what's the difference between a database", "tokens": [50766, 570, 437, 311, 264, 2649, 1296, 257, 8149, 50904], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1520, "seek": 386298, "start": 3873.78, "end": 3875.86, "text": " and like your brain?", "tokens": [50904, 293, 411, 428, 3567, 30, 51008], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1521, "seek": 386298, "start": 3875.86, "end": 3879.14, "text": " No one can go in and change your memories, right?", "tokens": [51008, 883, 472, 393, 352, 294, 293, 1319, 428, 8495, 11, 558, 30, 51172], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1522, "seek": 386298, "start": 3879.14, "end": 3881.02, "text": " Right, your memories are yours.", "tokens": [51172, 1779, 11, 428, 8495, 366, 6342, 13, 51266], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1523, "seek": 386298, "start": 3881.02, "end": 3883.54, "text": " They are permanent unless you get brain damage", "tokens": [51266, 814, 366, 10996, 5969, 291, 483, 3567, 4344, 51392], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1524, "seek": 386298, "start": 3883.54, "end": 3885.94, "text": " or Alzheimer's or something, but they're permanent, right?", "tokens": [51392, 420, 27932, 311, 420, 746, 11, 457, 436, 434, 10996, 11, 558, 30, 51512], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1525, "seek": 386298, "start": 3885.94, "end": 3889.3, "text": " No one can write a SQL query into your head", "tokens": [51512, 883, 472, 393, 2464, 257, 19200, 14581, 666, 428, 1378, 51680], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1526, "seek": 386298, "start": 3889.3, "end": 3892.02, "text": " to get your memories or change them.", "tokens": [51680, 281, 483, 428, 8495, 420, 1319, 552, 13, 51816], "temperature": 0.0, "avg_logprob": -0.08269886892349994, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.00023779286129865795}, {"id": 1527, "seek": 389202, "start": 3892.02, "end": 3896.58, "text": " And so in order for us to realize a full AGI,", "tokens": [50364, 400, 370, 294, 1668, 337, 505, 281, 4325, 257, 1577, 316, 26252, 11, 50592], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1528, "seek": 389202, "start": 3896.58, "end": 3898.86, "text": " I think that it's gonna need kind of the same level", "tokens": [50592, 286, 519, 300, 309, 311, 799, 643, 733, 295, 264, 912, 1496, 50706], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1529, "seek": 389202, "start": 3898.86, "end": 3900.94, "text": " of trust in its own memories.", "tokens": [50706, 295, 3361, 294, 1080, 1065, 8495, 13, 50810], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1530, "seek": 389202, "start": 3900.94, "end": 3902.54, "text": " And so that's why I think that a blockchain", "tokens": [50810, 400, 370, 300, 311, 983, 286, 519, 300, 257, 17176, 50890], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1531, "seek": 389202, "start": 3902.54, "end": 3905.2599999999998, "text": " is gonna be critical to integrate", "tokens": [50890, 307, 799, 312, 4924, 281, 13365, 51026], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1532, "seek": 389202, "start": 3905.2599999999998, "end": 3906.62, "text": " with these neural networks.", "tokens": [51026, 365, 613, 18161, 9590, 13, 51094], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1533, "seek": 389202, "start": 3906.62, "end": 3911.18, "text": " That might be the data repository for an AGI in the future", "tokens": [51094, 663, 1062, 312, 264, 1412, 25841, 337, 364, 316, 26252, 294, 264, 2027, 51322], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1534, "seek": 389202, "start": 3911.18, "end": 3914.46, "text": " because imagine you have an AGI system", "tokens": [51322, 570, 3811, 291, 362, 364, 316, 26252, 1185, 51486], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1535, "seek": 389202, "start": 3914.46, "end": 3916.98, "text": " that is just using a SQL database.", "tokens": [51486, 300, 307, 445, 1228, 257, 19200, 8149, 13, 51612], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1536, "seek": 389202, "start": 3916.98, "end": 3920.14, "text": " Well, if you hack into that and you rewrite its memories,", "tokens": [51612, 1042, 11, 498, 291, 10339, 666, 300, 293, 291, 28132, 1080, 8495, 11, 51770], "temperature": 0.0, "avg_logprob": -0.07031162783630893, "compression_ratio": 1.618320610687023, "no_speech_prob": 0.0002531054778955877}, {"id": 1537, "seek": 392014, "start": 3920.18, "end": 3921.98, "text": " you could send it off into,", "tokens": [50366, 291, 727, 2845, 309, 766, 666, 11, 50456], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1538, "seek": 392014, "start": 3921.98, "end": 3924.2599999999998, "text": " it could become hostile, it could become broken.", "tokens": [50456, 309, 727, 1813, 27312, 11, 309, 727, 1813, 5463, 13, 50570], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1539, "seek": 392014, "start": 3924.2599999999998, "end": 3927.1, "text": " Whereas a blockchain, the key feature of a blockchain", "tokens": [50570, 13813, 257, 17176, 11, 264, 2141, 4111, 295, 257, 17176, 50712], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1540, "seek": 392014, "start": 3927.1, "end": 3928.9, "text": " is that it's immutable, right?", "tokens": [50712, 307, 300, 309, 311, 3397, 32148, 11, 558, 30, 50802], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1541, "seek": 392014, "start": 3928.9, "end": 3933.9, "text": " So if we could give a machine autonomy,", "tokens": [50802, 407, 498, 321, 727, 976, 257, 3479, 27278, 11, 51052], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1542, "seek": 392014, "start": 3934.8599999999997, "end": 3936.22, "text": " so that's one ingredient, autonomy,", "tokens": [51100, 370, 300, 311, 472, 14751, 11, 27278, 11, 51168], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1543, "seek": 392014, "start": 3936.22, "end": 3939.46, "text": " but then also a memory or a memory system,", "tokens": [51168, 457, 550, 611, 257, 4675, 420, 257, 4675, 1185, 11, 51330], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1544, "seek": 392014, "start": 3939.46, "end": 3941.9, "text": " which I think would probably be best as a blockchain,", "tokens": [51330, 597, 286, 519, 576, 1391, 312, 1151, 382, 257, 17176, 11, 51452], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1545, "seek": 392014, "start": 3941.9, "end": 3943.58, "text": " I think then we'll be much closer", "tokens": [51452, 286, 519, 550, 321, 603, 312, 709, 4966, 51536], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1546, "seek": 392014, "start": 3943.58, "end": 3947.54, "text": " to like the fully realized AGI system.", "tokens": [51536, 281, 411, 264, 4498, 5334, 316, 26252, 1185, 13, 51734], "temperature": 0.0, "avg_logprob": -0.10878952836568377, "compression_ratio": 1.702928870292887, "no_speech_prob": 0.0007095919572748244}, {"id": 1547, "seek": 394754, "start": 3948.1, "end": 3951.5, "text": " And that's why I wanted to publish my book as fast as I did", "tokens": [50392, 400, 300, 311, 983, 286, 1415, 281, 11374, 452, 1446, 382, 2370, 382, 286, 630, 50562], "temperature": 0.0, "avg_logprob": -0.15478071673163052, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.0010320550063624978}, {"id": 1548, "seek": 394754, "start": 3951.5, "end": 3954.38, "text": " was, okay, we're laying the groundwork, right?", "tokens": [50562, 390, 11, 1392, 11, 321, 434, 14903, 264, 2727, 1902, 11, 558, 30, 50706], "temperature": 0.0, "avg_logprob": -0.15478071673163052, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.0010320550063624978}, {"id": 1549, "seek": 394754, "start": 3954.38, "end": 3958.58, "text": " But we need newer systems, we need a few better tools.", "tokens": [50706, 583, 321, 643, 17628, 3652, 11, 321, 643, 257, 1326, 1101, 3873, 13, 50916], "temperature": 0.0, "avg_logprob": -0.15478071673163052, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.0010320550063624978}, {"id": 1550, "seek": 394754, "start": 3958.58, "end": 3960.94, "text": " I hope that answers your question.", "tokens": [50916, 286, 1454, 300, 6338, 428, 1168, 13, 51034], "temperature": 0.0, "avg_logprob": -0.15478071673163052, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.0010320550063624978}, {"id": 1551, "seek": 394754, "start": 3960.94, "end": 3964.7, "text": " Yeah, I think a memory, I agree with you.", "tokens": [51034, 865, 11, 286, 519, 257, 4675, 11, 286, 3986, 365, 291, 13, 51222], "temperature": 0.0, "avg_logprob": -0.15478071673163052, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.0010320550063624978}, {"id": 1552, "seek": 394754, "start": 3964.7, "end": 3968.98, "text": " And it's just interesting like GPT-3's quote unquote memory", "tokens": [51222, 400, 309, 311, 445, 1880, 411, 26039, 51, 12, 18, 311, 6513, 37557, 4675, 51436], "temperature": 0.0, "avg_logprob": -0.15478071673163052, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.0010320550063624978}, {"id": 1553, "seek": 394754, "start": 3968.98, "end": 3971.94, "text": " is limited to whatever it experienced at training time", "tokens": [51436, 307, 5567, 281, 2035, 309, 6751, 412, 3097, 565, 51584], "temperature": 0.0, "avg_logprob": -0.15478071673163052, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.0010320550063624978}, {"id": 1554, "seek": 394754, "start": 3971.94, "end": 3973.7, "text": " and during fine tuning.", "tokens": [51584, 293, 1830, 2489, 15164, 13, 51672], "temperature": 0.0, "avg_logprob": -0.15478071673163052, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.0010320550063624978}, {"id": 1555, "seek": 394754, "start": 3973.7, "end": 3976.82, "text": " And sometimes its memory gets jumbled up", "tokens": [51672, 400, 2171, 1080, 4675, 2170, 361, 19928, 493, 51828], "temperature": 0.0, "avg_logprob": -0.15478071673163052, "compression_ratio": 1.548148148148148, "no_speech_prob": 0.0010320550063624978}, {"id": 1556, "seek": 397682, "start": 3976.82, "end": 3979.1000000000004, "text": " or it's rephrasing it, it's making stuff up", "tokens": [50364, 420, 309, 311, 319, 44598, 3349, 309, 11, 309, 311, 1455, 1507, 493, 50478], "temperature": 0.0, "avg_logprob": -0.12159492351390698, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0006460033473558724}, {"id": 1557, "seek": 397682, "start": 3979.1000000000004, "end": 3982.3, "text": " or it's sharing things that look truthful,", "tokens": [50478, 420, 309, 311, 5414, 721, 300, 574, 44669, 11, 50638], "temperature": 0.0, "avg_logprob": -0.12159492351390698, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0006460033473558724}, {"id": 1558, "seek": 397682, "start": 3982.3, "end": 3984.54, "text": " but they're actually not, right?", "tokens": [50638, 457, 436, 434, 767, 406, 11, 558, 30, 50750], "temperature": 0.0, "avg_logprob": -0.12159492351390698, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0006460033473558724}, {"id": 1559, "seek": 397682, "start": 3984.54, "end": 3988.96, "text": " And so somewhere along the line, like just broadly speaking,", "tokens": [50750, 400, 370, 4079, 2051, 264, 1622, 11, 411, 445, 19511, 4124, 11, 50971], "temperature": 0.0, "avg_logprob": -0.12159492351390698, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0006460033473558724}, {"id": 1560, "seek": 397682, "start": 3988.96, "end": 3990.6200000000003, "text": " I think there needs to be research", "tokens": [50971, 286, 519, 456, 2203, 281, 312, 2132, 51054], "temperature": 0.0, "avg_logprob": -0.12159492351390698, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0006460033473558724}, {"id": 1561, "seek": 397682, "start": 3990.6200000000003, "end": 3993.86, "text": " on getting these models to, you know,", "tokens": [51054, 322, 1242, 613, 5245, 281, 11, 291, 458, 11, 51216], "temperature": 0.0, "avg_logprob": -0.12159492351390698, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0006460033473558724}, {"id": 1562, "seek": 397682, "start": 3993.86, "end": 3997.02, "text": " store that information in a truthful, accurate way", "tokens": [51216, 3531, 300, 1589, 294, 257, 44669, 11, 8559, 636, 51374], "temperature": 0.0, "avg_logprob": -0.12159492351390698, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0006460033473558724}, {"id": 1563, "seek": 397682, "start": 3997.02, "end": 4001.1000000000004, "text": " or even based on some perception that they may have", "tokens": [51374, 420, 754, 2361, 322, 512, 12860, 300, 436, 815, 362, 51578], "temperature": 0.0, "avg_logprob": -0.12159492351390698, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0006460033473558724}, {"id": 1564, "seek": 397682, "start": 4001.1000000000004, "end": 4004.5, "text": " into some separate space where it can be retrieved.", "tokens": [51578, 666, 512, 4994, 1901, 689, 309, 393, 312, 19817, 937, 13, 51748], "temperature": 0.0, "avg_logprob": -0.12159492351390698, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0006460033473558724}, {"id": 1565, "seek": 400450, "start": 4005.38, "end": 4007.34, "text": " And also these memories are critical", "tokens": [50408, 400, 611, 613, 8495, 366, 4924, 50506], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1566, "seek": 400450, "start": 4007.34, "end": 4009.86, "text": " for decision making that process as well, right?", "tokens": [50506, 337, 3537, 1455, 300, 1399, 382, 731, 11, 558, 30, 50632], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1567, "seek": 400450, "start": 4009.86, "end": 4012.46, "text": " You draw on your memories, you're on past experiences.", "tokens": [50632, 509, 2642, 322, 428, 8495, 11, 291, 434, 322, 1791, 5235, 13, 50762], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1568, "seek": 400450, "start": 4012.46, "end": 4013.86, "text": " And the important part is, I mean,", "tokens": [50762, 400, 264, 1021, 644, 307, 11, 286, 914, 11, 50832], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1569, "seek": 400450, "start": 4013.86, "end": 4015.42, "text": " you're using the word database,", "tokens": [50832, 291, 434, 1228, 264, 1349, 8149, 11, 50910], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1570, "seek": 400450, "start": 4015.42, "end": 4018.62, "text": " it's these are internal representations of memories, right?", "tokens": [50910, 309, 311, 613, 366, 6920, 33358, 295, 8495, 11, 558, 30, 51070], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1571, "seek": 400450, "start": 4018.62, "end": 4019.7, "text": " That need to be stored.", "tokens": [51070, 663, 643, 281, 312, 12187, 13, 51124], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1572, "seek": 400450, "start": 4019.7, "end": 4023.62, "text": " And I have no clue what an internal representation database", "tokens": [51124, 400, 286, 362, 572, 13602, 437, 364, 6920, 10290, 8149, 51320], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1573, "seek": 400450, "start": 4023.62, "end": 4025.34, "text": " would look like or how that would even work.", "tokens": [51320, 576, 574, 411, 420, 577, 300, 576, 754, 589, 13, 51406], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1574, "seek": 400450, "start": 4025.34, "end": 4028.74, "text": " I, you know, I've got a machine learning researcher.", "tokens": [51406, 286, 11, 291, 458, 11, 286, 600, 658, 257, 3479, 2539, 21751, 13, 51576], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1575, "seek": 400450, "start": 4028.74, "end": 4030.66, "text": " I think I've just a dreamer.", "tokens": [51576, 286, 519, 286, 600, 445, 257, 3055, 260, 13, 51672], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1576, "seek": 400450, "start": 4030.66, "end": 4032.86, "text": " I can tell you what kind of product I would want", "tokens": [51672, 286, 393, 980, 291, 437, 733, 295, 1674, 286, 576, 528, 51782], "temperature": 0.0, "avg_logprob": -0.16954586846487862, "compression_ratio": 1.7625418060200668, "no_speech_prob": 0.0006877488922327757}, {"id": 1577, "seek": 403286, "start": 4032.86, "end": 4034.58, "text": " as a GT3 developer,", "tokens": [50364, 382, 257, 17530, 18, 10754, 11, 50450], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1578, "seek": 403286, "start": 4034.58, "end": 4036.6200000000003, "text": " but I don't know if I could actually do it myself.", "tokens": [50450, 457, 286, 500, 380, 458, 498, 286, 727, 767, 360, 309, 2059, 13, 50552], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1579, "seek": 403286, "start": 4036.6200000000003, "end": 4038.26, "text": " Yeah, I can do it myself.", "tokens": [50552, 865, 11, 286, 393, 360, 309, 2059, 13, 50634], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1580, "seek": 403286, "start": 4038.26, "end": 4040.2200000000003, "text": " That's why, you know, I got a prototype", "tokens": [50634, 663, 311, 983, 11, 291, 458, 11, 286, 658, 257, 19475, 50732], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1581, "seek": 403286, "start": 4040.2200000000003, "end": 4042.6600000000003, "text": " and actually in the opening chapter of my book,", "tokens": [50732, 293, 767, 294, 264, 5193, 7187, 295, 452, 1446, 11, 50854], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1582, "seek": 403286, "start": 4042.6600000000003, "end": 4045.34, "text": " I say this is as much a recruiting tool as anything else.", "tokens": [50854, 286, 584, 341, 307, 382, 709, 257, 25987, 2290, 382, 1340, 1646, 13, 50988], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1583, "seek": 403286, "start": 4045.34, "end": 4048.58, "text": " Cause I need more smart people to help me on this.", "tokens": [50988, 10865, 286, 643, 544, 4069, 561, 281, 854, 385, 322, 341, 13, 51150], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1584, "seek": 403286, "start": 4048.58, "end": 4050.3, "text": " I see, that's cool.", "tokens": [51150, 286, 536, 11, 300, 311, 1627, 13, 51236], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1585, "seek": 403286, "start": 4050.3, "end": 4053.7400000000002, "text": " So one last point about memories is one advantage", "tokens": [51236, 407, 472, 1036, 935, 466, 8495, 307, 472, 5002, 51408], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1586, "seek": 403286, "start": 4053.7400000000002, "end": 4056.26, "text": " of having an AGI that thinks in natural language", "tokens": [51408, 295, 1419, 364, 316, 26252, 300, 7309, 294, 3303, 2856, 51534], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1587, "seek": 403286, "start": 4056.26, "end": 4057.5, "text": " is interpretability.", "tokens": [51534, 307, 7302, 2310, 13, 51596], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1588, "seek": 403286, "start": 4058.78, "end": 4062.52, "text": " If you like, yeah, we could create a multimodal model", "tokens": [51660, 759, 291, 411, 11, 1338, 11, 321, 727, 1884, 257, 32972, 378, 304, 2316, 51847], "temperature": 0.0, "avg_logprob": -0.17955658652565695, "compression_ratio": 1.6179401993355482, "no_speech_prob": 0.004132785834372044}, {"id": 1589, "seek": 406252, "start": 4062.56, "end": 4064.28, "text": " that just stores vectors, right?", "tokens": [50366, 300, 445, 9512, 18875, 11, 558, 30, 50452], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1590, "seek": 406252, "start": 4064.28, "end": 4065.6, "text": " High dimensional vectors.", "tokens": [50452, 5229, 18795, 18875, 13, 50518], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1591, "seek": 406252, "start": 4065.6, "end": 4067.16, "text": " That's not interpretable.", "tokens": [50518, 663, 311, 406, 7302, 712, 13, 50596], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1592, "seek": 406252, "start": 4067.16, "end": 4069.28, "text": " But with natural language, cognitive architecture,", "tokens": [50596, 583, 365, 3303, 2856, 11, 15605, 9482, 11, 50702], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1593, "seek": 406252, "start": 4069.28, "end": 4071.36, "text": " all the memories are in plain text.", "tokens": [50702, 439, 264, 8495, 366, 294, 11121, 2487, 13, 50806], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1594, "seek": 406252, "start": 4071.36, "end": 4074.28, "text": " I can, you know, when I had my model up and running", "tokens": [50806, 286, 393, 11, 291, 458, 11, 562, 286, 632, 452, 2316, 493, 293, 2614, 50952], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1595, "seek": 406252, "start": 4074.28, "end": 4075.8, "text": " and one of the reasons that I don't", "tokens": [50952, 293, 472, 295, 264, 4112, 300, 286, 500, 380, 51028], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1596, "seek": 406252, "start": 4075.8, "end": 4077.72, "text": " is because it's super expensive.", "tokens": [51028, 307, 570, 309, 311, 1687, 5124, 13, 51124], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1597, "seek": 406252, "start": 4077.72, "end": 4080.16, "text": " Like a 10 minute conversation using DaVinci cost", "tokens": [51124, 1743, 257, 1266, 3456, 3761, 1228, 3933, 53, 21961, 2063, 51246], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1598, "seek": 406252, "start": 4080.16, "end": 4083.16, "text": " about $30 because of how much it was interacting", "tokens": [51246, 466, 1848, 3446, 570, 295, 577, 709, 309, 390, 18017, 51396], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1599, "seek": 406252, "start": 4083.16, "end": 4085.96, "text": " with the API.", "tokens": [51396, 365, 264, 9362, 13, 51536], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1600, "seek": 406252, "start": 4086.92, "end": 4089.96, "text": " But all of the memories, like every interaction,", "tokens": [51584, 583, 439, 295, 264, 8495, 11, 411, 633, 9285, 11, 51736], "temperature": 0.0, "avg_logprob": -0.133622394344671, "compression_ratio": 1.556701030927835, "no_speech_prob": 0.001064748503267765}, {"id": 1601, "seek": 408996, "start": 4089.96, "end": 4092.6, "text": " you know, every input, output, all the prompts,", "tokens": [50364, 291, 458, 11, 633, 4846, 11, 5598, 11, 439, 264, 41095, 11, 50496], "temperature": 0.0, "avg_logprob": -0.1275686195918492, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000779188412707299}, {"id": 1602, "seek": 408996, "start": 4092.6, "end": 4094.7200000000003, "text": " all the responses, all natural language,", "tokens": [50496, 439, 264, 13019, 11, 439, 3303, 2856, 11, 50602], "temperature": 0.0, "avg_logprob": -0.1275686195918492, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000779188412707299}, {"id": 1603, "seek": 408996, "start": 4094.7200000000003, "end": 4096.76, "text": " which solves one of the biggest problems", "tokens": [50602, 597, 39890, 472, 295, 264, 3880, 2740, 50704], "temperature": 0.0, "avg_logprob": -0.1275686195918492, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000779188412707299}, {"id": 1604, "seek": 408996, "start": 4096.76, "end": 4099.08, "text": " that people have with the idea of AGI,", "tokens": [50704, 300, 561, 362, 365, 264, 1558, 295, 316, 26252, 11, 50820], "temperature": 0.0, "avg_logprob": -0.1275686195918492, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000779188412707299}, {"id": 1605, "seek": 408996, "start": 4099.08, "end": 4101.34, "text": " which is that it's going to be a black box.", "tokens": [50820, 597, 307, 300, 309, 311, 516, 281, 312, 257, 2211, 2424, 13, 50933], "temperature": 0.0, "avg_logprob": -0.1275686195918492, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000779188412707299}, {"id": 1606, "seek": 408996, "start": 4101.34, "end": 4103.88, "text": " So I think that that's one of the greatest strengths", "tokens": [50933, 407, 286, 519, 300, 300, 311, 472, 295, 264, 6636, 16986, 51060], "temperature": 0.0, "avg_logprob": -0.1275686195918492, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000779188412707299}, {"id": 1607, "seek": 408996, "start": 4103.88, "end": 4108.2, "text": " actually of having GPT-3, which works in natural language.", "tokens": [51060, 767, 295, 1419, 26039, 51, 12, 18, 11, 597, 1985, 294, 3303, 2856, 13, 51276], "temperature": 0.0, "avg_logprob": -0.1275686195918492, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000779188412707299}, {"id": 1608, "seek": 408996, "start": 4108.2, "end": 4110.56, "text": " And so you just record every transaction", "tokens": [51276, 400, 370, 291, 445, 2136, 633, 14425, 51394], "temperature": 0.0, "avg_logprob": -0.1275686195918492, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000779188412707299}, {"id": 1609, "seek": 408996, "start": 4110.56, "end": 4113.24, "text": " and that makes it perfectly interpretable to any human.", "tokens": [51394, 293, 300, 1669, 309, 6239, 7302, 712, 281, 604, 1952, 13, 51528], "temperature": 0.0, "avg_logprob": -0.1275686195918492, "compression_ratio": 1.6317829457364341, "no_speech_prob": 0.000779188412707299}, {"id": 1610, "seek": 411324, "start": 4114.24, "end": 4117.24, "text": " Awesome. Yeah. Yeah, I would agree.", "tokens": [50414, 10391, 13, 865, 13, 865, 11, 286, 576, 3986, 13, 50564], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1611, "seek": 411324, "start": 4119.24, "end": 4121.24, "text": " So I'm going to switch gears for a second.", "tokens": [50664, 407, 286, 478, 516, 281, 3679, 20915, 337, 257, 1150, 13, 50764], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1612, "seek": 411324, "start": 4121.24, "end": 4123.24, "text": " So obviously you're really active", "tokens": [50764, 407, 2745, 291, 434, 534, 4967, 50864], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1613, "seek": 411324, "start": 4123.24, "end": 4125.24, "text": " on the OpenAI community forums.", "tokens": [50864, 322, 264, 7238, 48698, 1768, 26998, 13, 50964], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1614, "seek": 411324, "start": 4125.24, "end": 4128.24, "text": " What thoughts did you have on the community at large?", "tokens": [50964, 708, 4598, 630, 291, 362, 322, 264, 1768, 412, 2416, 30, 51114], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1615, "seek": 411324, "start": 4128.24, "end": 4129.24, "text": " Did you have any feedback?", "tokens": [51114, 2589, 291, 362, 604, 5824, 30, 51164], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1616, "seek": 411324, "start": 4129.24, "end": 4132.24, "text": " How things could be improved, either community-wise,", "tokens": [51164, 1012, 721, 727, 312, 9689, 11, 2139, 1768, 12, 3711, 11, 51314], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1617, "seek": 411324, "start": 4132.24, "end": 4135.24, "text": " platform-wise, and have there been any great experiences", "tokens": [51314, 3663, 12, 3711, 11, 293, 362, 456, 668, 604, 869, 5235, 51464], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1618, "seek": 411324, "start": 4135.24, "end": 4138.24, "text": " you've had on the OpenAI community forums?", "tokens": [51464, 291, 600, 632, 322, 264, 7238, 48698, 1768, 26998, 30, 51614], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1619, "seek": 411324, "start": 4138.24, "end": 4141.24, "text": " Yeah, yeah, no, it's a really great place.", "tokens": [51614, 865, 11, 1338, 11, 572, 11, 309, 311, 257, 534, 869, 1081, 13, 51764], "temperature": 0.0, "avg_logprob": -0.23616973424361923, "compression_ratio": 1.6975806451612903, "no_speech_prob": 0.00572877237573266}, {"id": 1620, "seek": 414124, "start": 4142.24, "end": 4145.24, "text": " It's been critical, actually,", "tokens": [50414, 467, 311, 668, 4924, 11, 767, 11, 50564], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1621, "seek": 414124, "start": 4145.24, "end": 4148.24, "text": " because I don't know if you've experienced this,", "tokens": [50564, 570, 286, 500, 380, 458, 498, 291, 600, 6751, 341, 11, 50714], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1622, "seek": 414124, "start": 4148.24, "end": 4151.24, "text": " but I go try and talk about GPT-3 to other people.", "tokens": [50714, 457, 286, 352, 853, 293, 751, 466, 26039, 51, 12, 18, 281, 661, 561, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1623, "seek": 414124, "start": 4151.24, "end": 4153.24, "text": " You go ask people on Reddit,", "tokens": [50864, 509, 352, 1029, 561, 322, 32210, 11, 50964], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1624, "seek": 414124, "start": 4153.24, "end": 4155.24, "text": " you talk to people who don't know what it is.", "tokens": [50964, 291, 751, 281, 561, 567, 500, 380, 458, 437, 309, 307, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1625, "seek": 414124, "start": 4155.24, "end": 4157.24, "text": " I even attended a deep learning meetup group", "tokens": [51064, 286, 754, 15990, 257, 2452, 2539, 1677, 1010, 1594, 51164], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1626, "seek": 414124, "start": 4157.24, "end": 4159.24, "text": " here in the Triangle area.", "tokens": [51164, 510, 294, 264, 10931, 7846, 1859, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1627, "seek": 414124, "start": 4159.24, "end": 4162.24, "text": " And I was trying to present my work,", "tokens": [51264, 400, 286, 390, 1382, 281, 1974, 452, 589, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1628, "seek": 414124, "start": 4162.24, "end": 4164.24, "text": " my cognitive architecture work,", "tokens": [51414, 452, 15605, 9482, 589, 11, 51514], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1629, "seek": 414124, "start": 4164.24, "end": 4167.24, "text": " and everyone was more excited about just GPT-3 in itself", "tokens": [51514, 293, 1518, 390, 544, 2919, 466, 445, 26039, 51, 12, 18, 294, 2564, 51664], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1630, "seek": 414124, "start": 4167.24, "end": 4169.24, "text": " because no one had seen it yet.", "tokens": [51664, 570, 572, 472, 632, 1612, 309, 1939, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08713157533660648, "compression_ratio": 1.5781818181818181, "no_speech_prob": 0.005382690113037825}, {"id": 1631, "seek": 416924, "start": 4169.24, "end": 4171.24, "text": " And they're like, wow, how is it doing that?", "tokens": [50364, 400, 436, 434, 411, 11, 6076, 11, 577, 307, 309, 884, 300, 30, 50464], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1632, "seek": 416924, "start": 4171.24, "end": 4174.24, "text": " And yeah, so like,", "tokens": [50464, 400, 1338, 11, 370, 411, 11, 50614], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1633, "seek": 416924, "start": 4174.24, "end": 4177.24, "text": " when you're as deep into GPT-3 as we are,", "tokens": [50614, 562, 291, 434, 382, 2452, 666, 26039, 51, 12, 18, 382, 321, 366, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1634, "seek": 416924, "start": 4177.24, "end": 4178.24, "text": " most people don't get it.", "tokens": [50764, 881, 561, 500, 380, 483, 309, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1635, "seek": 416924, "start": 4178.24, "end": 4180.24, "text": " They don't know what it's capable of.", "tokens": [50814, 814, 500, 380, 458, 437, 309, 311, 8189, 295, 13, 50914], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1636, "seek": 416924, "start": 4180.24, "end": 4182.24, "text": " My girlfriend's finding the same thing.", "tokens": [50914, 1222, 10369, 311, 5006, 264, 912, 551, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1637, "seek": 416924, "start": 4182.24, "end": 4184.24, "text": " She's finishing up her master's program,", "tokens": [51014, 1240, 311, 12693, 493, 720, 4505, 311, 1461, 11, 51114], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1638, "seek": 416924, "start": 4184.24, "end": 4188.24, "text": " and so she's shared some of her work with her peers,", "tokens": [51114, 293, 370, 750, 311, 5507, 512, 295, 720, 589, 365, 720, 16739, 11, 51314], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1639, "seek": 416924, "start": 4188.24, "end": 4190.24, "text": " with other students, and they're like,", "tokens": [51314, 365, 661, 1731, 11, 293, 436, 434, 411, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1640, "seek": 416924, "start": 4190.24, "end": 4192.24, "text": " wow, this is like AGI complete.", "tokens": [51414, 6076, 11, 341, 307, 411, 316, 26252, 3566, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1641, "seek": 416924, "start": 4192.24, "end": 4194.24, "text": " Why don't we just deploy this now?", "tokens": [51514, 1545, 500, 380, 321, 445, 7274, 341, 586, 30, 51614], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1642, "seek": 416924, "start": 4194.24, "end": 4196.24, "text": " And she's like, I told you, right?", "tokens": [51614, 400, 750, 311, 411, 11, 286, 1907, 291, 11, 558, 30, 51714], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1643, "seek": 416924, "start": 4196.24, "end": 4198.24, "text": " This is remarkable technology,", "tokens": [51714, 639, 307, 12802, 2899, 11, 51814], "temperature": 0.0, "avg_logprob": -0.08354528692384429, "compression_ratio": 1.6608391608391608, "no_speech_prob": 0.01743120700120926}, {"id": 1644, "seek": 419824, "start": 4198.24, "end": 4200.24, "text": " but even the professors don't understand", "tokens": [50364, 457, 754, 264, 15924, 500, 380, 1223, 50464], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1645, "seek": 419824, "start": 4200.24, "end": 4203.24, "text": " how disruptive this technology can be.", "tokens": [50464, 577, 37865, 341, 2899, 393, 312, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1646, "seek": 419824, "start": 4203.24, "end": 4205.24, "text": " And so because of that,", "tokens": [50614, 400, 370, 570, 295, 300, 11, 50714], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1647, "seek": 419824, "start": 4205.24, "end": 4207.24, "text": " the open AI community is pretty much", "tokens": [50714, 264, 1269, 7318, 1768, 307, 1238, 709, 50814], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1648, "seek": 419824, "start": 4207.24, "end": 4209.24, "text": " the only place I can talk about this stuff.", "tokens": [50814, 264, 787, 1081, 286, 393, 751, 466, 341, 1507, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1649, "seek": 419824, "start": 4209.24, "end": 4212.24, "text": " It's the only place I can talk about my ideas", "tokens": [50914, 467, 311, 264, 787, 1081, 286, 393, 751, 466, 452, 3487, 51064], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1650, "seek": 419824, "start": 4212.24, "end": 4215.24, "text": " and share my progress and insights,", "tokens": [51064, 293, 2073, 452, 4205, 293, 14310, 11, 51214], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1651, "seek": 419824, "start": 4215.24, "end": 4218.24, "text": " and for it to actually have an audience.", "tokens": [51214, 293, 337, 309, 281, 767, 362, 364, 4034, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1652, "seek": 419824, "start": 4218.24, "end": 4221.24, "text": " So that's kind of the cost", "tokens": [51364, 407, 300, 311, 733, 295, 264, 2063, 51514], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1653, "seek": 419824, "start": 4221.24, "end": 4223.24, "text": " of being on the cutting edge, right,", "tokens": [51514, 295, 885, 322, 264, 6492, 4691, 11, 558, 11, 51614], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1654, "seek": 419824, "start": 4223.24, "end": 4225.24, "text": " as your audience gets smaller.", "tokens": [51614, 382, 428, 4034, 2170, 4356, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1655, "seek": 419824, "start": 4225.24, "end": 4226.24, "text": " But it's definitely the place to be", "tokens": [51714, 583, 309, 311, 2138, 264, 1081, 281, 312, 51764], "temperature": 0.0, "avg_logprob": -0.07172635396321615, "compression_ratio": 1.697674418604651, "no_speech_prob": 0.00103196466807276}, {"id": 1656, "seek": 422624, "start": 4226.24, "end": 4229.24, "text": " if you want to get to the cutting edge.", "tokens": [50364, 498, 291, 528, 281, 483, 281, 264, 6492, 4691, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1657, "seek": 422624, "start": 4229.24, "end": 4232.24, "text": " Another advantage is they have the,", "tokens": [50514, 3996, 5002, 307, 436, 362, 264, 11, 50664], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1658, "seek": 422624, "start": 4232.24, "end": 4234.24, "text": " you can tag your posts where you say,", "tokens": [50664, 291, 393, 6162, 428, 12300, 689, 291, 584, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1659, "seek": 422624, "start": 4234.24, "end": 4236.24, "text": " looking for a teammate.", "tokens": [50764, 1237, 337, 257, 25467, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1660, "seek": 422624, "start": 4236.24, "end": 4238.24, "text": " And so at this point,", "tokens": [50864, 400, 370, 412, 341, 935, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1661, "seek": 422624, "start": 4238.24, "end": 4242.24, "text": " I've probably had maybe two dozen different calls", "tokens": [50964, 286, 600, 1391, 632, 1310, 732, 16654, 819, 5498, 51164], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1662, "seek": 422624, "start": 4242.24, "end": 4245.24, "text": " with people all over the world.", "tokens": [51164, 365, 561, 439, 670, 264, 1002, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1663, "seek": 422624, "start": 4245.24, "end": 4247.24, "text": " I've talked with people who are writing", "tokens": [51314, 286, 600, 2825, 365, 561, 567, 366, 3579, 51414], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1664, "seek": 422624, "start": 4247.24, "end": 4250.24, "text": " language teaching apps, education apps,", "tokens": [51414, 2856, 4571, 7733, 11, 3309, 7733, 11, 51564], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1665, "seek": 422624, "start": 4250.24, "end": 4253.24, "text": " Humano that I mentioned earlier.", "tokens": [51564, 12877, 3730, 300, 286, 2835, 3071, 13, 51714], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1666, "seek": 422624, "start": 4253.24, "end": 4255.24, "text": " And so I've had an opportunity", "tokens": [51714, 400, 370, 286, 600, 632, 364, 2650, 51814], "temperature": 0.0, "avg_logprob": -0.11398486371310253, "compression_ratio": 1.6041666666666667, "no_speech_prob": 0.004467016085982323}, {"id": 1667, "seek": 425524, "start": 4255.24, "end": 4259.24, "text": " to collaborate with a dozen or two dozen teams", "tokens": [50364, 281, 18338, 365, 257, 16654, 420, 732, 16654, 5491, 50564], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1668, "seek": 425524, "start": 4259.24, "end": 4262.24, "text": " all over the world because of the open AI community.", "tokens": [50564, 439, 670, 264, 1002, 570, 295, 264, 1269, 7318, 1768, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1669, "seek": 425524, "start": 4262.24, "end": 4264.24, "text": " And I've actually found a couple of startups", "tokens": [50714, 400, 286, 600, 767, 1352, 257, 1916, 295, 28041, 50814], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1670, "seek": 425524, "start": 4264.24, "end": 4267.24, "text": " that I'm gonna actually get involved with", "tokens": [50814, 300, 286, 478, 799, 767, 483, 3288, 365, 50964], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1671, "seek": 425524, "start": 4267.24, "end": 4271.24, "text": " and try and help them bring their ideas to market.", "tokens": [50964, 293, 853, 293, 854, 552, 1565, 641, 3487, 281, 2142, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1672, "seek": 425524, "start": 4271.24, "end": 4274.24, "text": " And that just wouldn't have happened otherwise.", "tokens": [51164, 400, 300, 445, 2759, 380, 362, 2011, 5911, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1673, "seek": 425524, "start": 4274.24, "end": 4276.24, "text": " I wouldn't have found these people on Reddit.", "tokens": [51314, 286, 2759, 380, 362, 1352, 613, 561, 322, 32210, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1674, "seek": 425524, "start": 4276.24, "end": 4278.24, "text": " I wouldn't have found them on Facebook or Twitter", "tokens": [51414, 286, 2759, 380, 362, 1352, 552, 322, 4384, 420, 5794, 51514], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1675, "seek": 425524, "start": 4278.24, "end": 4281.24, "text": " because like I mentioned,", "tokens": [51514, 570, 411, 286, 2835, 11, 51664], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1676, "seek": 425524, "start": 4281.24, "end": 4283.24, "text": " the ideas that I'm sharing", "tokens": [51664, 264, 3487, 300, 286, 478, 5414, 51764], "temperature": 0.0, "avg_logprob": -0.06840215162797407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.020934663712978363}, {"id": 1677, "seek": 428324, "start": 4283.24, "end": 4286.24, "text": " are so far beyond what is talked about", "tokens": [50364, 366, 370, 1400, 4399, 437, 307, 2825, 466, 50514], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1678, "seek": 428324, "start": 4286.24, "end": 4289.24, "text": " on the machine learning subreddit, right?", "tokens": [50514, 322, 264, 3479, 2539, 1422, 986, 17975, 11, 558, 30, 50664], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1679, "seek": 428324, "start": 4289.24, "end": 4291.24, "text": " They're still talking about loss functions", "tokens": [50664, 814, 434, 920, 1417, 466, 4470, 6828, 50764], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1680, "seek": 428324, "start": 4291.24, "end": 4292.24, "text": " and other things.", "tokens": [50764, 293, 661, 721, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1681, "seek": 428324, "start": 4292.24, "end": 4294.24, "text": " I'm like, no, we got to talk about cognitive architectures.", "tokens": [50814, 286, 478, 411, 11, 572, 11, 321, 658, 281, 751, 466, 15605, 6331, 1303, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1682, "seek": 428324, "start": 4294.24, "end": 4296.24, "text": " We got to talk about blockchain memories.", "tokens": [50914, 492, 658, 281, 751, 466, 17176, 8495, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1683, "seek": 428324, "start": 4296.24, "end": 4298.24, "text": " And everyone's like, what are you talking about?", "tokens": [51014, 400, 1518, 311, 411, 11, 437, 366, 291, 1417, 466, 30, 51114], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1684, "seek": 428324, "start": 4298.24, "end": 4301.24, "text": " So in order to have that right audience,", "tokens": [51114, 407, 294, 1668, 281, 362, 300, 558, 4034, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1685, "seek": 428324, "start": 4301.24, "end": 4304.24, "text": " that's what I rely on the open AI community for.", "tokens": [51264, 300, 311, 437, 286, 10687, 322, 264, 1269, 7318, 1768, 337, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1686, "seek": 428324, "start": 4304.24, "end": 4307.24, "text": " Now, as far as things that could do better,", "tokens": [51414, 823, 11, 382, 1400, 382, 721, 300, 727, 360, 1101, 11, 51564], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1687, "seek": 428324, "start": 4307.24, "end": 4310.24, "text": " it could be more active.", "tokens": [51564, 309, 727, 312, 544, 4967, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09500170135498047, "compression_ratio": 1.6703703703703703, "no_speech_prob": 0.005552495829761028}, {"id": 1688, "seek": 431024, "start": 4310.24, "end": 4312.24, "text": " And I'm not sure why,", "tokens": [50364, 400, 286, 478, 406, 988, 983, 11, 50464], "temperature": 0.0, "avg_logprob": -0.10728016086653167, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0012445816537365317}, {"id": 1689, "seek": 431024, "start": 4312.24, "end": 4316.24, "text": " but participation seems to come in waves, right?", "tokens": [50464, 457, 13487, 2544, 281, 808, 294, 9417, 11, 558, 30, 50664], "temperature": 0.0, "avg_logprob": -0.10728016086653167, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0012445816537365317}, {"id": 1690, "seek": 431024, "start": 4316.24, "end": 4321.24, "text": " And even now that it's gone GA, general availability,", "tokens": [50664, 400, 754, 586, 300, 309, 311, 2780, 22841, 11, 2674, 17945, 11, 50914], "temperature": 0.0, "avg_logprob": -0.10728016086653167, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0012445816537365317}, {"id": 1691, "seek": 431024, "start": 4321.24, "end": 4325.24, "text": " I thought that it would explode, right?", "tokens": [50914, 286, 1194, 300, 309, 576, 21411, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.10728016086653167, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0012445816537365317}, {"id": 1692, "seek": 431024, "start": 4325.24, "end": 4330.24, "text": " That, hey, anyone can sign up on GPT-3 now.", "tokens": [51114, 663, 11, 4177, 11, 2878, 393, 1465, 493, 322, 26039, 51, 12, 18, 586, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10728016086653167, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0012445816537365317}, {"id": 1693, "seek": 431024, "start": 4330.24, "end": 4332.24, "text": " Why is it not blowing up?", "tokens": [51364, 1545, 307, 309, 406, 15068, 493, 30, 51464], "temperature": 0.0, "avg_logprob": -0.10728016086653167, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0012445816537365317}, {"id": 1694, "seek": 431024, "start": 4332.24, "end": 4334.24, "text": " And I'm wondering if it's just that", "tokens": [51464, 400, 286, 478, 6359, 498, 309, 311, 445, 300, 51564], "temperature": 0.0, "avg_logprob": -0.10728016086653167, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0012445816537365317}, {"id": 1695, "seek": 431024, "start": 4334.24, "end": 4336.24, "text": " maybe open AI needs a better marketing team", "tokens": [51564, 1310, 1269, 7318, 2203, 257, 1101, 6370, 1469, 51664], "temperature": 0.0, "avg_logprob": -0.10728016086653167, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0012445816537365317}, {"id": 1696, "seek": 431024, "start": 4336.24, "end": 4338.24, "text": " or a bigger marketing budget", "tokens": [51664, 420, 257, 3801, 6370, 4706, 51764], "temperature": 0.0, "avg_logprob": -0.10728016086653167, "compression_ratio": 1.5110132158590308, "no_speech_prob": 0.0012445816537365317}, {"id": 1697, "seek": 433824, "start": 4338.24, "end": 4341.24, "text": " because, I mean, and I know that they'll say that", "tokens": [50364, 570, 11, 286, 914, 11, 293, 286, 458, 300, 436, 603, 584, 300, 50514], "temperature": 0.0, "avg_logprob": -0.1305275593163832, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.02553441748023033}, {"id": 1698, "seek": 433824, "start": 4341.24, "end": 4344.24, "text": " they've got like a thousand or 5,000 startups", "tokens": [50514, 436, 600, 658, 411, 257, 4714, 420, 1025, 11, 1360, 28041, 50664], "temperature": 0.0, "avg_logprob": -0.1305275593163832, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.02553441748023033}, {"id": 1699, "seek": 433824, "start": 4344.24, "end": 4346.24, "text": " using their platform,", "tokens": [50664, 1228, 641, 3663, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1305275593163832, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.02553441748023033}, {"id": 1700, "seek": 433824, "start": 4346.24, "end": 4352.24, "text": " but I think that there's a lot of,", "tokens": [50764, 457, 286, 519, 300, 456, 311, 257, 688, 295, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1305275593163832, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.02553441748023033}, {"id": 1701, "seek": 433824, "start": 4352.24, "end": 4355.24, "text": " what's the word, like unmet potential", "tokens": [51064, 437, 311, 264, 1349, 11, 411, 517, 5537, 3995, 51214], "temperature": 0.0, "avg_logprob": -0.1305275593163832, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.02553441748023033}, {"id": 1702, "seek": 433824, "start": 4355.24, "end": 4357.24, "text": " or latent potential, that's the word, latent potential,", "tokens": [51214, 420, 48994, 3995, 11, 300, 311, 264, 1349, 11, 48994, 3995, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1305275593163832, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.02553441748023033}, {"id": 1703, "seek": 433824, "start": 4357.24, "end": 4360.24, "text": " because there's so many people with fantastic ideas", "tokens": [51314, 570, 456, 311, 370, 867, 561, 365, 5456, 3487, 51464], "temperature": 0.0, "avg_logprob": -0.1305275593163832, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.02553441748023033}, {"id": 1704, "seek": 433824, "start": 4360.24, "end": 4364.24, "text": " and use cases, and we really need to create", "tokens": [51464, 293, 764, 3331, 11, 293, 321, 534, 643, 281, 1884, 51664], "temperature": 0.0, "avg_logprob": -0.1305275593163832, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.02553441748023033}, {"id": 1705, "seek": 433824, "start": 4364.24, "end": 4366.24, "text": " more of like a startup reactor thing.", "tokens": [51664, 544, 295, 411, 257, 18578, 20628, 551, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1305275593163832, "compression_ratio": 1.7431192660550459, "no_speech_prob": 0.02553441748023033}, {"id": 1706, "seek": 436624, "start": 4366.24, "end": 4368.24, "text": " Open AI, what was it?", "tokens": [50364, 7238, 7318, 11, 437, 390, 309, 30, 50464], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1707, "seek": 436624, "start": 4368.24, "end": 4370.24, "text": " I think about six to nine months ago,", "tokens": [50464, 286, 519, 466, 2309, 281, 4949, 2493, 2057, 11, 50564], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1708, "seek": 436624, "start": 4370.24, "end": 4374.24, "text": " they announced their $100 million open AI fund, right?", "tokens": [50564, 436, 7548, 641, 1848, 6879, 2459, 1269, 7318, 2374, 11, 558, 30, 50764], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1709, "seek": 436624, "start": 4374.24, "end": 4377.24, "text": " So they wanted to attract some more startups and stuff,", "tokens": [50764, 407, 436, 1415, 281, 5049, 512, 544, 28041, 293, 1507, 11, 50914], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1710, "seek": 436624, "start": 4377.24, "end": 4379.24, "text": " but even that, I kind of,", "tokens": [50914, 457, 754, 300, 11, 286, 733, 295, 11, 51014], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1711, "seek": 436624, "start": 4379.24, "end": 4383.24, "text": " the community's kind of ghost town some days,", "tokens": [51014, 264, 1768, 311, 733, 295, 8359, 3954, 512, 1708, 11, 51214], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1712, "seek": 436624, "start": 4383.24, "end": 4386.24, "text": " but I think today, I checked a few times", "tokens": [51214, 457, 286, 519, 965, 11, 286, 10033, 257, 1326, 1413, 51364], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1713, "seek": 436624, "start": 4386.24, "end": 4389.24, "text": " and there was three or four posts that had been updated,", "tokens": [51364, 293, 456, 390, 1045, 420, 1451, 12300, 300, 632, 668, 10588, 11, 51514], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1714, "seek": 436624, "start": 4389.24, "end": 4391.24, "text": " but some days there's like 20, right?", "tokens": [51514, 457, 512, 1708, 456, 311, 411, 945, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1715, "seek": 436624, "start": 4391.24, "end": 4393.24, "text": " It's just feast or famine.", "tokens": [51614, 467, 311, 445, 23707, 420, 42790, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12605055307937882, "compression_ratio": 1.594488188976378, "no_speech_prob": 0.01242760382592678}, {"id": 1716, "seek": 439324, "start": 4393.24, "end": 4395.24, "text": " So that's really the biggest problem is,", "tokens": [50364, 407, 300, 311, 534, 264, 3880, 1154, 307, 11, 50464], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1717, "seek": 439324, "start": 4395.24, "end": 4399.24, "text": " there's so much potential here and it's completely untapped", "tokens": [50464, 456, 311, 370, 709, 3995, 510, 293, 309, 311, 2584, 517, 1328, 3320, 50664], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1718, "seek": 439324, "start": 4399.24, "end": 4402.24, "text": " or almost completely untapped.", "tokens": [50664, 420, 1920, 2584, 517, 1328, 3320, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1719, "seek": 439324, "start": 4402.24, "end": 4405.24, "text": " Yeah, but no, it's been indispensable for me", "tokens": [50814, 865, 11, 457, 572, 11, 309, 311, 668, 47940, 337, 385, 50964], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1720, "seek": 439324, "start": 4405.24, "end": 4408.24, "text": " and hopefully these couple of startups that I'm involved with", "tokens": [50964, 293, 4696, 613, 1916, 295, 28041, 300, 286, 478, 3288, 365, 51114], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1721, "seek": 439324, "start": 4408.24, "end": 4412.24, "text": " might yield something really, really incredible.", "tokens": [51114, 1062, 11257, 746, 534, 11, 534, 4651, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1722, "seek": 439324, "start": 4412.24, "end": 4415.24, "text": " Yeah, and thank you for sharing this.", "tokens": [51314, 865, 11, 293, 1309, 291, 337, 5414, 341, 13, 51464], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1723, "seek": 439324, "start": 4415.24, "end": 4417.24, "text": " I agree with everything that you're saying.", "tokens": [51464, 286, 3986, 365, 1203, 300, 291, 434, 1566, 13, 51564], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1724, "seek": 439324, "start": 4417.24, "end": 4419.24, "text": " There is something about GPT-3.", "tokens": [51564, 821, 307, 746, 466, 26039, 51, 12, 18, 13, 51664], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1725, "seek": 439324, "start": 4419.24, "end": 4421.24, "text": " I have noticed people who,", "tokens": [51664, 286, 362, 5694, 561, 567, 11, 51764], "temperature": 0.0, "avg_logprob": -0.0728811548467268, "compression_ratio": 1.646153846153846, "no_speech_prob": 0.00036827308940701187}, {"id": 1726, "seek": 442124, "start": 4421.24, "end": 4424.24, "text": " especially on the machine learning subreddit,", "tokens": [50364, 2318, 322, 264, 3479, 2539, 1422, 986, 17975, 11, 50514], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1727, "seek": 442124, "start": 4424.24, "end": 4426.24, "text": " they're a little bit too educated,", "tokens": [50514, 436, 434, 257, 707, 857, 886, 15872, 11, 50614], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1728, "seek": 442124, "start": 4426.24, "end": 4429.24, "text": " a little bit too qualified, a little bit too skeptical.", "tokens": [50614, 257, 707, 857, 886, 15904, 11, 257, 707, 857, 886, 28601, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1729, "seek": 442124, "start": 4429.24, "end": 4433.24, "text": " And I can see a lot of machine learning researchers", "tokens": [50764, 400, 286, 393, 536, 257, 688, 295, 3479, 2539, 10309, 50964], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1730, "seek": 442124, "start": 4433.24, "end": 4437.24, "text": " not being interested in the nuances of prompt design.", "tokens": [50964, 406, 885, 3102, 294, 264, 38775, 295, 12391, 1715, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1731, "seek": 442124, "start": 4437.24, "end": 4439.24, "text": " They're just not.", "tokens": [51164, 814, 434, 445, 406, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1732, "seek": 442124, "start": 4439.24, "end": 4442.24, "text": " And I've spoken to machine learning researchers", "tokens": [51264, 400, 286, 600, 10759, 281, 3479, 2539, 10309, 51414], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1733, "seek": 442124, "start": 4442.24, "end": 4444.24, "text": " and many of them are like, what?", "tokens": [51414, 293, 867, 295, 552, 366, 411, 11, 437, 30, 51514], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1734, "seek": 442124, "start": 4444.24, "end": 4446.24, "text": " It's just repeating training data, right?", "tokens": [51514, 467, 311, 445, 18617, 3097, 1412, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1735, "seek": 442124, "start": 4446.24, "end": 4448.24, "text": " That's all it's doing.", "tokens": [51614, 663, 311, 439, 309, 311, 884, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1736, "seek": 442124, "start": 4448.24, "end": 4450.24, "text": " And when you ask them, what are you doing?", "tokens": [51714, 400, 562, 291, 1029, 552, 11, 437, 366, 291, 884, 30, 51814], "temperature": 0.0, "avg_logprob": -0.08920163795596263, "compression_ratio": 1.8553719008264462, "no_speech_prob": 0.009706222452223301}, {"id": 1737, "seek": 445024, "start": 4450.24, "end": 4451.24, "text": " They're repeating training data.", "tokens": [50364, 814, 434, 18617, 3097, 1412, 13, 50414], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1738, "seek": 445024, "start": 4451.24, "end": 4452.24, "text": " Their answer is no.", "tokens": [50414, 6710, 1867, 307, 572, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1739, "seek": 445024, "start": 4452.24, "end": 4453.24, "text": " No, of course not.", "tokens": [50464, 883, 11, 295, 1164, 406, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1740, "seek": 445024, "start": 4453.24, "end": 4454.24, "text": " Right.", "tokens": [50514, 1779, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1741, "seek": 445024, "start": 4454.24, "end": 4459.24, "text": " And so having a space where you can talk to people", "tokens": [50564, 400, 370, 1419, 257, 1901, 689, 291, 393, 751, 281, 561, 50814], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1742, "seek": 445024, "start": 4459.24, "end": 4462.24, "text": " who have access, who have explored,", "tokens": [50814, 567, 362, 2105, 11, 567, 362, 24016, 11, 50964], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1743, "seek": 445024, "start": 4462.24, "end": 4466.24, "text": " it's a valuable space is very important.", "tokens": [50964, 309, 311, 257, 8263, 1901, 307, 588, 1021, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1744, "seek": 445024, "start": 4466.24, "end": 4469.24, "text": " So were you on the Slack group back in the day", "tokens": [51164, 407, 645, 291, 322, 264, 37211, 1594, 646, 294, 264, 786, 51314], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1745, "seek": 445024, "start": 4469.24, "end": 4472.24, "text": " or did you show up when it was only the forums?", "tokens": [51314, 420, 630, 291, 855, 493, 562, 309, 390, 787, 264, 26998, 30, 51464], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1746, "seek": 445024, "start": 4472.24, "end": 4475.24, "text": " So when my application was accepted,", "tokens": [51464, 407, 562, 452, 3861, 390, 9035, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1747, "seek": 445024, "start": 4475.24, "end": 4479.24, "text": " they had just announced that the Slack group was getting paned.", "tokens": [51614, 436, 632, 445, 7548, 300, 264, 37211, 1594, 390, 1242, 2462, 292, 13, 51814], "temperature": 0.0, "avg_logprob": -0.11508133536890934, "compression_ratio": 1.608, "no_speech_prob": 0.010979591868817806}, {"id": 1748, "seek": 447924, "start": 4479.24, "end": 4482.24, "text": " So I got on like two weeks before they shut it down.", "tokens": [50364, 407, 286, 658, 322, 411, 732, 3259, 949, 436, 5309, 309, 760, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07048517830517827, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0020499136298894882}, {"id": 1749, "seek": 447924, "start": 4482.24, "end": 4487.24, "text": " So I was one of the first people on the new community board.", "tokens": [50514, 407, 286, 390, 472, 295, 264, 700, 561, 322, 264, 777, 1768, 3150, 13, 50764], "temperature": 0.0, "avg_logprob": -0.07048517830517827, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0020499136298894882}, {"id": 1750, "seek": 447924, "start": 4487.24, "end": 4490.24, "text": " But yeah, so that phenomenon that you've mentioned", "tokens": [50764, 583, 1338, 11, 370, 300, 14029, 300, 291, 600, 2835, 50914], "temperature": 0.0, "avg_logprob": -0.07048517830517827, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0020499136298894882}, {"id": 1751, "seek": 447924, "start": 4490.24, "end": 4495.24, "text": " is I actually wrote a post about that recently on the forum", "tokens": [50914, 307, 286, 767, 4114, 257, 2183, 466, 300, 3938, 322, 264, 17542, 51164], "temperature": 0.0, "avg_logprob": -0.07048517830517827, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0020499136298894882}, {"id": 1752, "seek": 447924, "start": 4495.24, "end": 4497.24, "text": " where a lot of purists,", "tokens": [51164, 689, 257, 688, 295, 1864, 1751, 11, 51264], "temperature": 0.0, "avg_logprob": -0.07048517830517827, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0020499136298894882}, {"id": 1753, "seek": 447924, "start": 4497.24, "end": 4501.24, "text": " whether you're a math purist or a computer science purist,", "tokens": [51264, 1968, 291, 434, 257, 5221, 1864, 468, 420, 257, 3820, 3497, 1864, 468, 11, 51464], "temperature": 0.0, "avg_logprob": -0.07048517830517827, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0020499136298894882}, {"id": 1754, "seek": 447924, "start": 4501.24, "end": 4505.24, "text": " you're trained to think quantitatively in terms of numbers.", "tokens": [51464, 291, 434, 8895, 281, 519, 27778, 356, 294, 2115, 295, 3547, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07048517830517827, "compression_ratio": 1.6096491228070176, "no_speech_prob": 0.0020499136298894882}, {"id": 1755, "seek": 450524, "start": 4505.24, "end": 4508.24, "text": " But GPT-3 doesn't produce quantitative data.", "tokens": [50364, 583, 26039, 51, 12, 18, 1177, 380, 5258, 27778, 1412, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08491504297847241, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.03843549266457558}, {"id": 1756, "seek": 450524, "start": 4508.24, "end": 4510.24, "text": " It produces qualitative data.", "tokens": [50514, 467, 14725, 31312, 1412, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08491504297847241, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.03843549266457558}, {"id": 1757, "seek": 450524, "start": 4510.24, "end": 4514.24, "text": " And so that's why you see people like artists and poets", "tokens": [50614, 400, 370, 300, 311, 983, 291, 536, 561, 411, 6910, 293, 38364, 50814], "temperature": 0.0, "avg_logprob": -0.08491504297847241, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.03843549266457558}, {"id": 1758, "seek": 450524, "start": 4514.24, "end": 4517.24, "text": " and novelists using it because they're like, wow, this is great.", "tokens": [50814, 293, 7613, 1751, 1228, 309, 570, 436, 434, 411, 11, 6076, 11, 341, 307, 869, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08491504297847241, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.03843549266457558}, {"id": 1759, "seek": 450524, "start": 4517.24, "end": 4519.24, "text": " And I'm cross-trained, right?", "tokens": [50964, 400, 286, 478, 3278, 12, 17227, 2001, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.08491504297847241, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.03843549266457558}, {"id": 1760, "seek": 450524, "start": 4519.24, "end": 4522.24, "text": " I'm a technologist by day and a science fiction author by night.", "tokens": [51064, 286, 478, 257, 1537, 9201, 538, 786, 293, 257, 3497, 13266, 3793, 538, 1818, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08491504297847241, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.03843549266457558}, {"id": 1761, "seek": 450524, "start": 4522.24, "end": 4526.24, "text": " So I use both so I can think qualitatively and quantitatively.", "tokens": [51214, 407, 286, 764, 1293, 370, 286, 393, 519, 31312, 356, 293, 27778, 356, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08491504297847241, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.03843549266457558}, {"id": 1762, "seek": 450524, "start": 4526.24, "end": 4532.24, "text": " And in the academic sphere, there are classes that are meant", "tokens": [51414, 400, 294, 264, 7778, 16687, 11, 456, 366, 5359, 300, 366, 4140, 51714], "temperature": 0.0, "avg_logprob": -0.08491504297847241, "compression_ratio": 1.6299212598425197, "no_speech_prob": 0.03843549266457558}, {"id": 1763, "seek": 453224, "start": 4532.24, "end": 4536.24, "text": " to teach computer science engineers to think differently,", "tokens": [50364, 281, 2924, 3820, 3497, 11955, 281, 519, 7614, 11, 50564], "temperature": 0.0, "avg_logprob": -0.06497893172703433, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.006094696931540966}, {"id": 1764, "seek": 453224, "start": 4536.24, "end": 4538.24, "text": " to think more qualitatively.", "tokens": [50564, 281, 519, 544, 31312, 356, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06497893172703433, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.006094696931540966}, {"id": 1765, "seek": 453224, "start": 4538.24, "end": 4543.24, "text": " But even still, some folks that have a real good natural affinity", "tokens": [50664, 583, 754, 920, 11, 512, 4024, 300, 362, 257, 957, 665, 3303, 39703, 50914], "temperature": 0.0, "avg_logprob": -0.06497893172703433, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.006094696931540966}, {"id": 1766, "seek": 453224, "start": 4543.24, "end": 4548.24, "text": " for computer programming and math, that's just their nature.", "tokens": [50914, 337, 3820, 9410, 293, 5221, 11, 300, 311, 445, 641, 3687, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06497893172703433, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.006094696931540966}, {"id": 1767, "seek": 453224, "start": 4548.24, "end": 4551.24, "text": " Their nature is not to think qualitatively.", "tokens": [51164, 6710, 3687, 307, 406, 281, 519, 31312, 356, 13, 51314], "temperature": 0.0, "avg_logprob": -0.06497893172703433, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.006094696931540966}, {"id": 1768, "seek": 453224, "start": 4551.24, "end": 4554.24, "text": " And so that is one of the biggest gaps, I think,", "tokens": [51314, 400, 370, 300, 307, 472, 295, 264, 3880, 15031, 11, 286, 519, 11, 51464], "temperature": 0.0, "avg_logprob": -0.06497893172703433, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.006094696931540966}, {"id": 1769, "seek": 453224, "start": 4554.24, "end": 4559.24, "text": " between where the researchers are experts and what's needed.", "tokens": [51464, 1296, 689, 264, 10309, 366, 8572, 293, 437, 311, 2978, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06497893172703433, "compression_ratio": 1.6457399103139014, "no_speech_prob": 0.006094696931540966}, {"id": 1770, "seek": 455924, "start": 4559.24, "end": 4562.24, "text": " And so there was a post months ago where someone was asking,", "tokens": [50364, 400, 370, 456, 390, 257, 2183, 2493, 2057, 689, 1580, 390, 3365, 11, 50514], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1771, "seek": 455924, "start": 4562.24, "end": 4566.24, "text": " like, OK, who should be on my team?", "tokens": [50514, 411, 11, 2264, 11, 567, 820, 312, 322, 452, 1469, 30, 50714], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1772, "seek": 455924, "start": 4566.24, "end": 4571.24, "text": " If I'm trying to build a business team to maximize my use of GPT-3,", "tokens": [50714, 759, 286, 478, 1382, 281, 1322, 257, 1606, 1469, 281, 19874, 452, 764, 295, 26039, 51, 12, 18, 11, 50964], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1773, "seek": 455924, "start": 4571.24, "end": 4574.24, "text": " I've got a front-end developer, I've got a back-end developer.", "tokens": [50964, 286, 600, 658, 257, 1868, 12, 521, 10754, 11, 286, 600, 658, 257, 646, 12, 521, 10754, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1774, "seek": 455924, "start": 4574.24, "end": 4575.24, "text": " What else do I need?", "tokens": [51114, 708, 1646, 360, 286, 643, 30, 51164], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1775, "seek": 455924, "start": 4575.24, "end": 4577.24, "text": " I said hire a writer.", "tokens": [51164, 286, 848, 11158, 257, 9936, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1776, "seek": 455924, "start": 4577.24, "end": 4580.24, "text": " Hire someone who's a journalist or a fiction writer", "tokens": [51264, 389, 621, 1580, 567, 311, 257, 17277, 420, 257, 13266, 9936, 51414], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1777, "seek": 455924, "start": 4580.24, "end": 4583.24, "text": " because they are going to understand that qualitative data.", "tokens": [51414, 570, 436, 366, 516, 281, 1223, 300, 31312, 1412, 13, 51564], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1778, "seek": 455924, "start": 4583.24, "end": 4585.24, "text": " Hire a psychologist.", "tokens": [51564, 389, 621, 257, 29514, 13, 51664], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1779, "seek": 455924, "start": 4585.24, "end": 4587.24, "text": " I've read plenty of books on psychology as well.", "tokens": [51664, 286, 600, 1401, 7140, 295, 3642, 322, 15105, 382, 731, 13, 51764], "temperature": 0.0, "avg_logprob": -0.06618150075276692, "compression_ratio": 1.6142857142857143, "no_speech_prob": 0.030201880261301994}, {"id": 1780, "seek": 458724, "start": 4587.24, "end": 4590.24, "text": " Actually, one of the folks that I'm working with is a psychology researcher", "tokens": [50364, 5135, 11, 472, 295, 264, 4024, 300, 286, 478, 1364, 365, 307, 257, 15105, 21751, 50514], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1781, "seek": 458724, "start": 4590.24, "end": 4594.24, "text": " who wants to automate as much of the clinical psychology experience", "tokens": [50514, 567, 2738, 281, 31605, 382, 709, 295, 264, 9115, 15105, 1752, 50714], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1782, "seek": 458724, "start": 4594.24, "end": 4597.24, "text": " or psychological research experience as possible.", "tokens": [50714, 420, 14346, 2132, 1752, 382, 1944, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1783, "seek": 458724, "start": 4597.24, "end": 4599.24, "text": " Of course, he's not a computer guy, right?", "tokens": [50864, 2720, 1164, 11, 415, 311, 406, 257, 3820, 2146, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1784, "seek": 458724, "start": 4599.24, "end": 4601.24, "text": " He thinks in terms of emotions.", "tokens": [50964, 634, 7309, 294, 2115, 295, 8462, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1785, "seek": 458724, "start": 4601.24, "end": 4603.24, "text": " He thinks in terms of communication.", "tokens": [51064, 634, 7309, 294, 2115, 295, 6101, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1786, "seek": 458724, "start": 4603.24, "end": 4605.24, "text": " And so he gets it, right?", "tokens": [51164, 400, 370, 415, 2170, 309, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1787, "seek": 458724, "start": 4605.24, "end": 4608.24, "text": " And it's funny because he read my book and he said,", "tokens": [51264, 400, 309, 311, 4074, 570, 415, 1401, 452, 1446, 293, 415, 848, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1788, "seek": 458724, "start": 4608.24, "end": 4610.24, "text": " oh, your cognitive architecture stuff,", "tokens": [51414, 1954, 11, 428, 15605, 9482, 1507, 11, 51514], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1789, "seek": 458724, "start": 4610.24, "end": 4614.24, "text": " it sounds like graduate level psychology.", "tokens": [51514, 309, 3263, 411, 8080, 1496, 15105, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08764968247249208, "compression_ratio": 1.7058823529411764, "no_speech_prob": 0.007117389235645533}, {"id": 1790, "seek": 461424, "start": 4614.24, "end": 4616.24, "text": " And then someone else read it and they said,", "tokens": [50364, 400, 550, 1580, 1646, 1401, 309, 293, 436, 848, 11, 50464], "temperature": 0.0, "avg_logprob": -0.10670541381835938, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.023674502968788147}, {"id": 1791, "seek": 461424, "start": 4616.24, "end": 4620.24, "text": " this sounds like what I do as an expert marketer.", "tokens": [50464, 341, 3263, 411, 437, 286, 360, 382, 364, 5844, 2142, 260, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10670541381835938, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.023674502968788147}, {"id": 1792, "seek": 461424, "start": 4620.24, "end": 4622.24, "text": " And I was like, yeah, I merge it all together.", "tokens": [50664, 400, 286, 390, 411, 11, 1338, 11, 286, 22183, 309, 439, 1214, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10670541381835938, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.023674502968788147}, {"id": 1793, "seek": 461424, "start": 4622.24, "end": 4626.24, "text": " So I think the simplest answer is you got to learn to think qualitatively.", "tokens": [50764, 407, 286, 519, 264, 22811, 1867, 307, 291, 658, 281, 1466, 281, 519, 31312, 356, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10670541381835938, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.023674502968788147}, {"id": 1794, "seek": 461424, "start": 4626.24, "end": 4629.24, "text": " And that's why I talked about reading and writing earlier.", "tokens": [50964, 400, 300, 311, 983, 286, 2825, 466, 3760, 293, 3579, 3071, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10670541381835938, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.023674502968788147}, {"id": 1795, "seek": 461424, "start": 4629.24, "end": 4630.24, "text": " Think in terms of emotions.", "tokens": [51114, 6557, 294, 2115, 295, 8462, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10670541381835938, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.023674502968788147}, {"id": 1796, "seek": 461424, "start": 4630.24, "end": 4633.24, "text": " Think in terms of your own mind.", "tokens": [51164, 6557, 294, 2115, 295, 428, 1065, 1575, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10670541381835938, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.023674502968788147}, {"id": 1797, "seek": 461424, "start": 4633.24, "end": 4638.24, "text": " And you've got to start, not you, but the audience,", "tokens": [51314, 400, 291, 600, 658, 281, 722, 11, 406, 291, 11, 457, 264, 4034, 11, 51564], "temperature": 0.0, "avg_logprob": -0.10670541381835938, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.023674502968788147}, {"id": 1798, "seek": 461424, "start": 4638.24, "end": 4641.24, "text": " the folks who want to make the most of GPT-3,", "tokens": [51564, 264, 4024, 567, 528, 281, 652, 264, 881, 295, 26039, 51, 12, 18, 11, 51714], "temperature": 0.0, "avg_logprob": -0.10670541381835938, "compression_ratio": 1.643939393939394, "no_speech_prob": 0.023674502968788147}, {"id": 1799, "seek": 464124, "start": 4641.24, "end": 4645.24, "text": " they have to really kind of dig in and start thinking qualitatively", "tokens": [50364, 436, 362, 281, 534, 733, 295, 2528, 294, 293, 722, 1953, 31312, 356, 50564], "temperature": 0.0, "avg_logprob": -0.055773446434422544, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.02094987966120243}, {"id": 1800, "seek": 464124, "start": 4645.24, "end": 4650.24, "text": " because qualitative data has just as much value as quantitative data.", "tokens": [50564, 570, 31312, 1412, 575, 445, 382, 709, 2158, 382, 27778, 1412, 13, 50814], "temperature": 0.0, "avg_logprob": -0.055773446434422544, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.02094987966120243}, {"id": 1801, "seek": 464124, "start": 4650.24, "end": 4654.24, "text": " But we have an entire generation of computer scientists and mathematicians", "tokens": [50814, 583, 321, 362, 364, 2302, 5125, 295, 3820, 7708, 293, 32811, 2567, 51014], "temperature": 0.0, "avg_logprob": -0.055773446434422544, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.02094987966120243}, {"id": 1802, "seek": 464124, "start": 4654.24, "end": 4657.24, "text": " who are not really trained to think qualitatively at all.", "tokens": [51014, 567, 366, 406, 534, 8895, 281, 519, 31312, 356, 412, 439, 13, 51164], "temperature": 0.0, "avg_logprob": -0.055773446434422544, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.02094987966120243}, {"id": 1803, "seek": 464124, "start": 4657.24, "end": 4659.24, "text": " And I think that's one of the biggest problems.", "tokens": [51164, 400, 286, 519, 300, 311, 472, 295, 264, 3880, 2740, 13, 51264], "temperature": 0.0, "avg_logprob": -0.055773446434422544, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.02094987966120243}, {"id": 1804, "seek": 464124, "start": 4659.24, "end": 4662.24, "text": " And I don't think open AI can solve that problem.", "tokens": [51264, 400, 286, 500, 380, 519, 1269, 7318, 393, 5039, 300, 1154, 13, 51414], "temperature": 0.0, "avg_logprob": -0.055773446434422544, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.02094987966120243}, {"id": 1805, "seek": 464124, "start": 4662.24, "end": 4664.24, "text": " That's a much bigger systemic problem.", "tokens": [51414, 663, 311, 257, 709, 3801, 23789, 1154, 13, 51514], "temperature": 0.0, "avg_logprob": -0.055773446434422544, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.02094987966120243}, {"id": 1806, "seek": 464124, "start": 4664.24, "end": 4667.24, "text": " No, that's a great point.", "tokens": [51514, 883, 11, 300, 311, 257, 869, 935, 13, 51664], "temperature": 0.0, "avg_logprob": -0.055773446434422544, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.02094987966120243}, {"id": 1807, "seek": 464124, "start": 4667.24, "end": 4669.24, "text": " I completely agree with you.", "tokens": [51664, 286, 2584, 3986, 365, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.055773446434422544, "compression_ratio": 1.7566539923954372, "no_speech_prob": 0.02094987966120243}, {"id": 1808, "seek": 466924, "start": 4669.24, "end": 4672.24, "text": " I think one of the reasons I've drawn to the open AI community is,", "tokens": [50364, 286, 519, 472, 295, 264, 4112, 286, 600, 10117, 281, 264, 1269, 7318, 1768, 307, 11, 50514], "temperature": 0.0, "avg_logprob": -0.1090773988942631, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.011323620565235615}, {"id": 1809, "seek": 466924, "start": 4672.24, "end": 4676.24, "text": " you know, these are, they tend to be developers who are also qualitative.", "tokens": [50514, 291, 458, 11, 613, 366, 11, 436, 3928, 281, 312, 8849, 567, 366, 611, 31312, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1090773988942631, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.011323620565235615}, {"id": 1810, "seek": 466924, "start": 4676.24, "end": 4680.24, "text": " They're developers who have multiple skills, who are doing different things.", "tokens": [50714, 814, 434, 8849, 567, 362, 3866, 3942, 11, 567, 366, 884, 819, 721, 13, 50914], "temperature": 0.0, "avg_logprob": -0.1090773988942631, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.011323620565235615}, {"id": 1811, "seek": 466924, "start": 4680.24, "end": 4686.24, "text": " And so, I mean, I was quite critical actually of shutting down the open AI Slack group.", "tokens": [50914, 400, 370, 11, 286, 914, 11, 286, 390, 1596, 4924, 767, 295, 36057, 760, 264, 1269, 7318, 37211, 1594, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1090773988942631, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.011323620565235615}, {"id": 1812, "seek": 466924, "start": 4686.24, "end": 4689.24, "text": " The activity was crazy on there.", "tokens": [51214, 440, 5191, 390, 3219, 322, 456, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1090773988942631, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.011323620565235615}, {"id": 1813, "seek": 466924, "start": 4689.24, "end": 4693.24, "text": " I, you know, made friends through that Slack group.", "tokens": [51364, 286, 11, 291, 458, 11, 1027, 1855, 807, 300, 37211, 1594, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1090773988942631, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.011323620565235615}, {"id": 1814, "seek": 466924, "start": 4693.24, "end": 4696.24, "text": " And I understand at the time there was these downsides,", "tokens": [51564, 400, 286, 1223, 412, 264, 565, 456, 390, 613, 21554, 1875, 11, 51714], "temperature": 0.0, "avg_logprob": -0.1090773988942631, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.011323620565235615}, {"id": 1815, "seek": 466924, "start": 4696.24, "end": 4698.24, "text": " people kept asking the same questions.", "tokens": [51714, 561, 4305, 3365, 264, 912, 1651, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1090773988942631, "compression_ratio": 1.7321428571428572, "no_speech_prob": 0.011323620565235615}, {"id": 1816, "seek": 469824, "start": 4698.24, "end": 4700.24, "text": " They didn't quite have a spam problem yet.", "tokens": [50364, 814, 994, 380, 1596, 362, 257, 24028, 1154, 1939, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1817, "seek": 469824, "start": 4700.24, "end": 4702.24, "text": " It was kind of heading there, right?", "tokens": [50464, 467, 390, 733, 295, 9864, 456, 11, 558, 30, 50564], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1818, "seek": 469824, "start": 4702.24, "end": 4705.24, "text": " But the activity was off the charts.", "tokens": [50564, 583, 264, 5191, 390, 766, 264, 17767, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1819, "seek": 469824, "start": 4705.24, "end": 4708.24, "text": " And you're right in terms of untapped potential.", "tokens": [50714, 400, 291, 434, 558, 294, 2115, 295, 517, 1328, 3320, 3995, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1820, "seek": 469824, "start": 4708.24, "end": 4709.24, "text": " Yes.", "tokens": [50864, 1079, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1821, "seek": 469824, "start": 4709.24, "end": 4712.24, "text": " That we didn't even know how far the Slack group was going to go,", "tokens": [50914, 663, 321, 994, 380, 754, 458, 577, 1400, 264, 37211, 1594, 390, 516, 281, 352, 11, 51064], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1822, "seek": 469824, "start": 4712.24, "end": 4713.24, "text": " but they shut it down.", "tokens": [51064, 457, 436, 5309, 309, 760, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1823, "seek": 469824, "start": 4713.24, "end": 4716.24, "text": " And there's discord solutions, there's alternatives.", "tokens": [51114, 400, 456, 311, 32989, 6547, 11, 456, 311, 20478, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1824, "seek": 469824, "start": 4716.24, "end": 4720.24, "text": " With what we have now, I think open AI does participate.", "tokens": [51264, 2022, 437, 321, 362, 586, 11, 286, 519, 1269, 7318, 775, 8197, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1825, "seek": 469824, "start": 4720.24, "end": 4722.24, "text": " There's, you know, some high level involvement.", "tokens": [51464, 821, 311, 11, 291, 458, 11, 512, 1090, 1496, 17447, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1826, "seek": 469824, "start": 4722.24, "end": 4725.24, "text": " They have sort of a dedicated member who writes honestly answers,", "tokens": [51564, 814, 362, 1333, 295, 257, 8374, 4006, 567, 13657, 6095, 6338, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11510339894689116, "compression_ratio": 1.599337748344371, "no_speech_prob": 0.043309640139341354}, {"id": 1827, "seek": 472524, "start": 4725.24, "end": 4727.24, "text": " really, really thoughtful answers to a lot of questions.", "tokens": [50364, 534, 11, 534, 21566, 6338, 281, 257, 688, 295, 1651, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1828, "seek": 472524, "start": 4727.24, "end": 4729.24, "text": " Official answers as well, which I appreciate.", "tokens": [50464, 38577, 6338, 382, 731, 11, 597, 286, 4449, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1829, "seek": 472524, "start": 4729.24, "end": 4730.24, "text": " Yep.", "tokens": [50564, 7010, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1830, "seek": 472524, "start": 4730.24, "end": 4732.24, "text": " I would just love to see the company really,", "tokens": [50614, 286, 576, 445, 959, 281, 536, 264, 2237, 534, 11, 50714], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1831, "seek": 472524, "start": 4732.24, "end": 4735.24, "text": " truly lean in to engaging with developers.", "tokens": [50714, 4908, 11659, 294, 281, 11268, 365, 8849, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1832, "seek": 472524, "start": 4735.24, "end": 4742.24, "text": " Like I have yet to see a single AMA ask me anything thread with the CEO of open AI.", "tokens": [50864, 1743, 286, 362, 1939, 281, 536, 257, 2167, 6475, 32, 1029, 385, 1340, 7207, 365, 264, 9282, 295, 1269, 7318, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1833, "seek": 472524, "start": 4742.24, "end": 4743.24, "text": " Yeah.", "tokens": [51214, 865, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1834, "seek": 472524, "start": 4743.24, "end": 4746.24, "text": " And this is something I tried to push last year on Twitter.", "tokens": [51264, 400, 341, 307, 746, 286, 3031, 281, 2944, 1036, 1064, 322, 5794, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1835, "seek": 472524, "start": 4746.24, "end": 4749.24, "text": " Let's get the CEO on the community forums and let's,", "tokens": [51414, 961, 311, 483, 264, 9282, 322, 264, 1768, 26998, 293, 718, 311, 11, 51564], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1836, "seek": 472524, "start": 4749.24, "end": 4753.24, "text": " let's ask questions and get responses from him.", "tokens": [51564, 718, 311, 1029, 1651, 293, 483, 13019, 490, 796, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10589345821664353, "compression_ratio": 1.6101083032490975, "no_speech_prob": 0.1600455939769745}, {"id": 1837, "seek": 475324, "start": 4753.24, "end": 4756.24, "text": " And I just don't know why, why doesn't he show up?", "tokens": [50364, 400, 286, 445, 500, 380, 458, 983, 11, 983, 1177, 380, 415, 855, 493, 30, 50514], "temperature": 0.0, "avg_logprob": -0.1001698047884049, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.044647056609392166}, {"id": 1838, "seek": 475324, "start": 4756.24, "end": 4759.24, "text": " I'm not sure if he's made a single post.", "tokens": [50514, 286, 478, 406, 988, 498, 415, 311, 1027, 257, 2167, 2183, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1001698047884049, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.044647056609392166}, {"id": 1839, "seek": 475324, "start": 4759.24, "end": 4761.24, "text": " And there's just other things as well,", "tokens": [50664, 400, 456, 311, 445, 661, 721, 382, 731, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1001698047884049, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.044647056609392166}, {"id": 1840, "seek": 475324, "start": 4761.24, "end": 4764.24, "text": " where I can just the difference between engaging really,", "tokens": [50764, 689, 286, 393, 445, 264, 2649, 1296, 11268, 534, 11, 50914], "temperature": 0.0, "avg_logprob": -0.1001698047884049, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.044647056609392166}, {"id": 1841, "seek": 475324, "start": 4764.24, "end": 4768.24, "text": " truly with your core audience and sort of, you know,", "tokens": [50914, 4908, 365, 428, 4965, 4034, 293, 1333, 295, 11, 291, 458, 11, 51114], "temperature": 0.0, "avg_logprob": -0.1001698047884049, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.044647056609392166}, {"id": 1842, "seek": 475324, "start": 4768.24, "end": 4770.24, "text": " compartmentalizing it to a single employee.", "tokens": [51114, 26505, 304, 3319, 309, 281, 257, 2167, 10738, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1001698047884049, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.044647056609392166}, {"id": 1843, "seek": 475324, "start": 4770.24, "end": 4775.24, "text": " Like, I don't know, this, this company led engagement is one thing versus department led.", "tokens": [51214, 1743, 11, 286, 500, 380, 458, 11, 341, 11, 341, 2237, 4684, 8742, 307, 472, 551, 5717, 5882, 4684, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1001698047884049, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.044647056609392166}, {"id": 1844, "seek": 475324, "start": 4775.24, "end": 4776.24, "text": " Right.", "tokens": [51464, 1779, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1001698047884049, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.044647056609392166}, {"id": 1845, "seek": 475324, "start": 4776.24, "end": 4779.24, "text": " And so there's just all these areas and certainly one of the other,", "tokens": [51514, 400, 370, 456, 311, 445, 439, 613, 3179, 293, 3297, 472, 295, 264, 661, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1001698047884049, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.044647056609392166}, {"id": 1846, "seek": 477924, "start": 4779.24, "end": 4784.24, "text": " I guess more immediate suggestions I have for the community.", "tokens": [50364, 286, 2041, 544, 11629, 13396, 286, 362, 337, 264, 1768, 13, 50614], "temperature": 0.0, "avg_logprob": -0.0973155373021176, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.03620506823062897}, {"id": 1847, "seek": 477924, "start": 4784.24, "end": 4788.24, "text": " We've accumulated tons of insights and resources.", "tokens": [50614, 492, 600, 31346, 9131, 295, 14310, 293, 3593, 13, 50814], "temperature": 0.0, "avg_logprob": -0.0973155373021176, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.03620506823062897}, {"id": 1848, "seek": 477924, "start": 4788.24, "end": 4794.24, "text": " I think the community could benefit from more pooling of the best posts,", "tokens": [50814, 286, 519, 264, 1768, 727, 5121, 490, 544, 7005, 278, 295, 264, 1151, 12300, 11, 51114], "temperature": 0.0, "avg_logprob": -0.0973155373021176, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.03620506823062897}, {"id": 1849, "seek": 477924, "start": 4794.24, "end": 4797.24, "text": " the best insights.", "tokens": [51114, 264, 1151, 14310, 13, 51264], "temperature": 0.0, "avg_logprob": -0.0973155373021176, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.03620506823062897}, {"id": 1850, "seek": 477924, "start": 4797.24, "end": 4799.24, "text": " And I also want to give a shout out.", "tokens": [51264, 400, 286, 611, 528, 281, 976, 257, 8043, 484, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0973155373021176, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.03620506823062897}, {"id": 1851, "seek": 477924, "start": 4799.24, "end": 4804.24, "text": " I think we need to encourage more shout out to duty to develop on there.", "tokens": [51364, 286, 519, 321, 643, 281, 5373, 544, 8043, 484, 281, 9776, 281, 1499, 322, 456, 13, 51614], "temperature": 0.0, "avg_logprob": -0.0973155373021176, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.03620506823062897}, {"id": 1852, "seek": 477924, "start": 4804.24, "end": 4806.24, "text": " I've reached out to him privately on the open at community forums,", "tokens": [51614, 286, 600, 6488, 484, 281, 796, 31919, 322, 264, 1269, 412, 1768, 26998, 11, 51714], "temperature": 0.0, "avg_logprob": -0.0973155373021176, "compression_ratio": 1.6844444444444444, "no_speech_prob": 0.03620506823062897}, {"id": 1853, "seek": 480624, "start": 4806.24, "end": 4811.24, "text": " but he's done some amazing just write ups of his GPT three experiments and the prompts.", "tokens": [50364, 457, 415, 311, 1096, 512, 2243, 445, 2464, 15497, 295, 702, 26039, 51, 1045, 12050, 293, 264, 41095, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1319095939397812, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.04739345237612724}, {"id": 1854, "seek": 480624, "start": 4811.24, "end": 4812.24, "text": " I'm sure you've seen them.", "tokens": [50614, 286, 478, 988, 291, 600, 1612, 552, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1319095939397812, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.04739345237612724}, {"id": 1855, "seek": 480624, "start": 4812.24, "end": 4813.24, "text": " Oh yeah.", "tokens": [50664, 876, 1338, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1319095939397812, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.04739345237612724}, {"id": 1856, "seek": 480624, "start": 4813.24, "end": 4815.24, "text": " And of course there's, there's other members who participate every day.", "tokens": [50714, 400, 295, 1164, 456, 311, 11, 456, 311, 661, 2679, 567, 8197, 633, 786, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1319095939397812, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.04739345237612724}, {"id": 1857, "seek": 480624, "start": 4815.24, "end": 4820.24, "text": " So I'm just saying that now that this, the community is, is in another stage,", "tokens": [50814, 407, 286, 478, 445, 1566, 300, 586, 300, 341, 11, 264, 1768, 307, 11, 307, 294, 1071, 3233, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1319095939397812, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.04739345237612724}, {"id": 1858, "seek": 480624, "start": 4820.24, "end": 4824.24, "text": " we, you know, we need to start thinking more about let's, let's,", "tokens": [51064, 321, 11, 291, 458, 11, 321, 643, 281, 722, 1953, 544, 466, 718, 311, 11, 718, 311, 11, 51264], "temperature": 0.0, "avg_logprob": -0.1319095939397812, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.04739345237612724}, {"id": 1859, "seek": 480624, "start": 4824.24, "end": 4827.24, "text": " what's curate some of the best moments.", "tokens": [51264, 437, 311, 1262, 473, 512, 295, 264, 1151, 6065, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1319095939397812, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.04739345237612724}, {"id": 1860, "seek": 480624, "start": 4827.24, "end": 4828.24, "text": " Right.", "tokens": [51414, 1779, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1319095939397812, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.04739345237612724}, {"id": 1861, "seek": 480624, "start": 4828.24, "end": 4832.24, "text": " I think that's, that's definitely one of the big pieces.", "tokens": [51464, 286, 519, 300, 311, 11, 300, 311, 2138, 472, 295, 264, 955, 3755, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1319095939397812, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.04739345237612724}, {"id": 1862, "seek": 483224, "start": 4832.24, "end": 4838.24, "text": " And so anyways, did you have any more thoughts in the community stuff or anything else?", "tokens": [50364, 400, 370, 13448, 11, 630, 291, 362, 604, 544, 4598, 294, 264, 1768, 1507, 420, 1340, 1646, 30, 50664], "temperature": 0.0, "avg_logprob": -0.09452988306681315, "compression_ratio": 1.64, "no_speech_prob": 0.03511907532811165}, {"id": 1863, "seek": 483224, "start": 4838.24, "end": 4842.24, "text": " Yeah, just, just an observation that I've, you know, I've worked at a,", "tokens": [50664, 865, 11, 445, 11, 445, 364, 14816, 300, 286, 600, 11, 291, 458, 11, 286, 600, 2732, 412, 257, 11, 50864], "temperature": 0.0, "avg_logprob": -0.09452988306681315, "compression_ratio": 1.64, "no_speech_prob": 0.03511907532811165}, {"id": 1864, "seek": 483224, "start": 4842.24, "end": 4847.24, "text": " at a number of companies of different sizes from, you know, a five person startup to,", "tokens": [50864, 412, 257, 1230, 295, 3431, 295, 819, 11602, 490, 11, 291, 458, 11, 257, 1732, 954, 18578, 281, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09452988306681315, "compression_ratio": 1.64, "no_speech_prob": 0.03511907532811165}, {"id": 1865, "seek": 483224, "start": 4847.24, "end": 4850.24, "text": " you know, Cisco systems was the biggest company I've worked for,", "tokens": [51114, 291, 458, 11, 38528, 3652, 390, 264, 3880, 2237, 286, 600, 2732, 337, 11, 51264], "temperature": 0.0, "avg_logprob": -0.09452988306681315, "compression_ratio": 1.64, "no_speech_prob": 0.03511907532811165}, {"id": 1866, "seek": 483224, "start": 4850.24, "end": 4854.24, "text": " which has had, had at the time, like 80,000 people globally.", "tokens": [51264, 597, 575, 632, 11, 632, 412, 264, 565, 11, 411, 4688, 11, 1360, 561, 18958, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09452988306681315, "compression_ratio": 1.64, "no_speech_prob": 0.03511907532811165}, {"id": 1867, "seek": 483224, "start": 4854.24, "end": 4858.24, "text": " And so I wonder if some of, some of what you're observing is just growing pains,", "tokens": [51464, 400, 370, 286, 2441, 498, 512, 295, 11, 512, 295, 437, 291, 434, 22107, 307, 445, 4194, 29774, 11, 51664], "temperature": 0.0, "avg_logprob": -0.09452988306681315, "compression_ratio": 1.64, "no_speech_prob": 0.03511907532811165}, {"id": 1868, "seek": 485824, "start": 4858.24, "end": 4863.24, "text": " just normal growing pains, because often you'll have like the startup culture,", "tokens": [50364, 445, 2710, 4194, 29774, 11, 570, 2049, 291, 603, 362, 411, 264, 18578, 3713, 11, 50614], "temperature": 0.0, "avg_logprob": -0.0878664538157072, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.05182782560586929}, {"id": 1869, "seek": 485824, "start": 4863.24, "end": 4864.24, "text": " which is bootstrapping, right?", "tokens": [50614, 597, 307, 11450, 19639, 3759, 11, 558, 30, 50664], "temperature": 0.0, "avg_logprob": -0.0878664538157072, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.05182782560586929}, {"id": 1870, "seek": 485824, "start": 4864.24, "end": 4867.24, "text": " Where you just, you know, it's on Slack, it's on, it's on GitHub,", "tokens": [50664, 2305, 291, 445, 11, 291, 458, 11, 309, 311, 322, 37211, 11, 309, 311, 322, 11, 309, 311, 322, 23331, 11, 50814], "temperature": 0.0, "avg_logprob": -0.0878664538157072, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.05182782560586929}, {"id": 1871, "seek": 485824, "start": 4867.24, "end": 4870.24, "text": " and you just kind of, it's fast and loose and quick.", "tokens": [50814, 293, 291, 445, 733, 295, 11, 309, 311, 2370, 293, 9612, 293, 1702, 13, 50964], "temperature": 0.0, "avg_logprob": -0.0878664538157072, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.05182782560586929}, {"id": 1872, "seek": 485824, "start": 4870.24, "end": 4875.24, "text": " And open AI, now that they've got an enterprise grade service,", "tokens": [50964, 400, 1269, 7318, 11, 586, 300, 436, 600, 658, 364, 14132, 7204, 2643, 11, 51214], "temperature": 0.0, "avg_logprob": -0.0878664538157072, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.05182782560586929}, {"id": 1873, "seek": 485824, "start": 4875.24, "end": 4877.24, "text": " they're having to develop their team.", "tokens": [51214, 436, 434, 1419, 281, 1499, 641, 1469, 13, 51314], "temperature": 0.0, "avg_logprob": -0.0878664538157072, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.05182782560586929}, {"id": 1874, "seek": 485824, "start": 4877.24, "end": 4880.24, "text": " You probably noticed they post like, hey, we're hiring, we're hiring, you know,", "tokens": [51314, 509, 1391, 5694, 436, 2183, 411, 11, 4177, 11, 321, 434, 15335, 11, 321, 434, 15335, 11, 291, 458, 11, 51464], "temperature": 0.0, "avg_logprob": -0.0878664538157072, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.05182782560586929}, {"id": 1875, "seek": 485824, "start": 4880.24, "end": 4886.24, "text": " there've been at least two big hiring, hiring splurges in the last six to 12 months.", "tokens": [51464, 456, 600, 668, 412, 1935, 732, 955, 15335, 11, 15335, 4732, 374, 2880, 294, 264, 1036, 2309, 281, 2272, 2493, 13, 51764], "temperature": 0.0, "avg_logprob": -0.0878664538157072, "compression_ratio": 1.657718120805369, "no_speech_prob": 0.05182782560586929}, {"id": 1876, "seek": 488624, "start": 4886.24, "end": 4889.24, "text": " And some of those are just like generic IT guys, you know,", "tokens": [50364, 400, 512, 295, 729, 366, 445, 411, 19577, 6783, 1074, 11, 291, 458, 11, 50514], "temperature": 0.0, "avg_logprob": -0.06818982882377429, "compression_ratio": 1.834862385321101, "no_speech_prob": 0.011685051023960114}, {"id": 1877, "seek": 488624, "start": 4889.24, "end": 4892.24, "text": " like kind of what I do from, from my day job or marketing folks.", "tokens": [50514, 411, 733, 295, 437, 286, 360, 490, 11, 490, 452, 786, 1691, 420, 6370, 4024, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06818982882377429, "compression_ratio": 1.834862385321101, "no_speech_prob": 0.011685051023960114}, {"id": 1878, "seek": 488624, "start": 4892.24, "end": 4897.24, "text": " So I think that, I think that they're probably working on solving some of those problems.", "tokens": [50664, 407, 286, 519, 300, 11, 286, 519, 300, 436, 434, 1391, 1364, 322, 12606, 512, 295, 729, 2740, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06818982882377429, "compression_ratio": 1.834862385321101, "no_speech_prob": 0.011685051023960114}, {"id": 1879, "seek": 488624, "start": 4897.24, "end": 4901.24, "text": " But also as a, as a nonprofit foundation, their budget is probably kind of thin.", "tokens": [50914, 583, 611, 382, 257, 11, 382, 257, 23348, 7030, 11, 641, 4706, 307, 1391, 733, 295, 5862, 13, 51114], "temperature": 0.0, "avg_logprob": -0.06818982882377429, "compression_ratio": 1.834862385321101, "no_speech_prob": 0.011685051023960114}, {"id": 1880, "seek": 488624, "start": 4901.24, "end": 4906.24, "text": " So I'm wondering if, you know, their partnership with Microsoft could help some of that as well.", "tokens": [51114, 407, 286, 478, 6359, 498, 11, 291, 458, 11, 641, 9982, 365, 8116, 727, 854, 512, 295, 300, 382, 731, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06818982882377429, "compression_ratio": 1.834862385321101, "no_speech_prob": 0.011685051023960114}, {"id": 1881, "seek": 488624, "start": 4906.24, "end": 4907.24, "text": " But you're absolutely right.", "tokens": [51364, 583, 291, 434, 3122, 558, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06818982882377429, "compression_ratio": 1.834862385321101, "no_speech_prob": 0.011685051023960114}, {"id": 1882, "seek": 488624, "start": 4907.24, "end": 4910.24, "text": " You know, there, there are still other things that they could be doing,", "tokens": [51414, 509, 458, 11, 456, 11, 456, 366, 920, 661, 721, 300, 436, 727, 312, 884, 11, 51564], "temperature": 0.0, "avg_logprob": -0.06818982882377429, "compression_ratio": 1.834862385321101, "no_speech_prob": 0.011685051023960114}, {"id": 1883, "seek": 488624, "start": 4910.24, "end": 4913.24, "text": " like, you know, maybe bring back Slack or, or a few other things.", "tokens": [51564, 411, 11, 291, 458, 11, 1310, 1565, 646, 37211, 420, 11, 420, 257, 1326, 661, 721, 13, 51714], "temperature": 0.0, "avg_logprob": -0.06818982882377429, "compression_ratio": 1.834862385321101, "no_speech_prob": 0.011685051023960114}, {"id": 1884, "seek": 488624, "start": 4913.24, "end": 4915.24, "text": " So yeah, that was just final observation.", "tokens": [51714, 407, 1338, 11, 300, 390, 445, 2572, 14816, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06818982882377429, "compression_ratio": 1.834862385321101, "no_speech_prob": 0.011685051023960114}, {"id": 1885, "seek": 491524, "start": 4915.24, "end": 4918.24, "text": " It might just be normal growing pains that they're working on solving.", "tokens": [50364, 467, 1062, 445, 312, 2710, 4194, 29774, 300, 436, 434, 1364, 322, 12606, 13, 50514], "temperature": 0.0, "avg_logprob": -0.11944714530569608, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.025947431102395058}, {"id": 1886, "seek": 491524, "start": 4918.24, "end": 4919.24, "text": " It's definitely growing pains.", "tokens": [50514, 467, 311, 2138, 4194, 29774, 13, 50564], "temperature": 0.0, "avg_logprob": -0.11944714530569608, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.025947431102395058}, {"id": 1887, "seek": 491524, "start": 4919.24, "end": 4923.24, "text": " And the things I'm sharing, to be honest, it's a little bit more on the harsh side.", "tokens": [50564, 400, 264, 721, 286, 478, 5414, 11, 281, 312, 3245, 11, 309, 311, 257, 707, 857, 544, 322, 264, 14897, 1252, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11944714530569608, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.025947431102395058}, {"id": 1888, "seek": 491524, "start": 4923.24, "end": 4926.24, "text": " Like, I mean, they mean well, they mean well, right?", "tokens": [50764, 1743, 11, 286, 914, 11, 436, 914, 731, 11, 436, 914, 731, 11, 558, 30, 50914], "temperature": 0.0, "avg_logprob": -0.11944714530569608, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.025947431102395058}, {"id": 1889, "seek": 491524, "start": 4926.24, "end": 4928.24, "text": " Like these are not bad people.", "tokens": [50914, 1743, 613, 366, 406, 1578, 561, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11944714530569608, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.025947431102395058}, {"id": 1890, "seek": 491524, "start": 4928.24, "end": 4930.24, "text": " They are for profit.", "tokens": [51014, 814, 366, 337, 7475, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11944714530569608, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.025947431102395058}, {"id": 1891, "seek": 491524, "start": 4930.24, "end": 4932.24, "text": " They switched away from nonprofit.", "tokens": [51114, 814, 16858, 1314, 490, 23348, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11944714530569608, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.025947431102395058}, {"id": 1892, "seek": 491524, "start": 4932.24, "end": 4934.24, "text": " Just, I just wanted to mention that.", "tokens": [51214, 1449, 11, 286, 445, 1415, 281, 2152, 300, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11944714530569608, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.025947431102395058}, {"id": 1893, "seek": 491524, "start": 4934.24, "end": 4942.24, "text": " But I think my, my, the reason I share this feedback is, for example,", "tokens": [51314, 583, 286, 519, 452, 11, 452, 11, 264, 1778, 286, 2073, 341, 5824, 307, 11, 337, 1365, 11, 51714], "temperature": 0.0, "avg_logprob": -0.11944714530569608, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.025947431102395058}, {"id": 1894, "seek": 494224, "start": 4942.24, "end": 4947.24, "text": " the CEO, Sam Oltman, he didn't do the AMA thread on the open AI community forums.", "tokens": [50364, 264, 9282, 11, 4832, 422, 2282, 1601, 11, 415, 994, 380, 360, 264, 6475, 32, 7207, 322, 264, 1269, 7318, 1768, 26998, 13, 50614], "temperature": 0.0, "avg_logprob": -0.10661616148772063, "compression_ratio": 1.61986301369863, "no_speech_prob": 0.43306615948677063}, {"id": 1895, "seek": 494224, "start": 4947.24, "end": 4950.24, "text": " He went to another website and did an AMA.", "tokens": [50614, 634, 1437, 281, 1071, 3144, 293, 630, 364, 6475, 32, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10661616148772063, "compression_ratio": 1.61986301369863, "no_speech_prob": 0.43306615948677063}, {"id": 1896, "seek": 494224, "start": 4950.24, "end": 4954.24, "text": " I can't remember if it was a, like a written form or just like a quick call.", "tokens": [50764, 286, 393, 380, 1604, 498, 309, 390, 257, 11, 411, 257, 3720, 1254, 420, 445, 411, 257, 1702, 818, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10661616148772063, "compression_ratio": 1.61986301369863, "no_speech_prob": 0.43306615948677063}, {"id": 1897, "seek": 494224, "start": 4954.24, "end": 4955.24, "text": " Right.", "tokens": [50964, 1779, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10661616148772063, "compression_ratio": 1.61986301369863, "no_speech_prob": 0.43306615948677063}, {"id": 1898, "seek": 494224, "start": 4955.24, "end": 4960.24, "text": " Where apparently he shared all these details about what GPT for could be like and the future,", "tokens": [51014, 2305, 7970, 415, 5507, 439, 613, 4365, 466, 437, 26039, 51, 337, 727, 312, 411, 293, 264, 2027, 11, 51264], "temperature": 0.0, "avg_logprob": -0.10661616148772063, "compression_ratio": 1.61986301369863, "no_speech_prob": 0.43306615948677063}, {"id": 1899, "seek": 494224, "start": 4960.24, "end": 4962.24, "text": " all the models may be multimodal in the future.", "tokens": [51264, 439, 264, 5245, 815, 312, 32972, 378, 304, 294, 264, 2027, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10661616148772063, "compression_ratio": 1.61986301369863, "no_speech_prob": 0.43306615948677063}, {"id": 1900, "seek": 494224, "start": 4962.24, "end": 4966.24, "text": " And I guess, you know, the, that thread has now been taken down.", "tokens": [51364, 400, 286, 2041, 11, 291, 458, 11, 264, 11, 300, 7207, 575, 586, 668, 2726, 760, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10661616148772063, "compression_ratio": 1.61986301369863, "no_speech_prob": 0.43306615948677063}, {"id": 1901, "seek": 494224, "start": 4966.24, "end": 4969.24, "text": " And it's like all the things that were said were alleged.", "tokens": [51564, 400, 309, 311, 411, 439, 264, 721, 300, 645, 848, 645, 26317, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10661616148772063, "compression_ratio": 1.61986301369863, "no_speech_prob": 0.43306615948677063}, {"id": 1902, "seek": 496924, "start": 4969.24, "end": 4973.24, "text": " And so I guess this is, this is really behind the scenes kind of stuff.", "tokens": [50364, 400, 370, 286, 2041, 341, 307, 11, 341, 307, 534, 2261, 264, 8026, 733, 295, 1507, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10798777955951112, "compression_ratio": 1.62, "no_speech_prob": 0.16227120161056519}, {"id": 1903, "seek": 496924, "start": 4973.24, "end": 4978.24, "text": " Like, but my criticism is they clearly have some capacity to engage.", "tokens": [50564, 1743, 11, 457, 452, 15835, 307, 436, 4448, 362, 512, 6042, 281, 4683, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10798777955951112, "compression_ratio": 1.62, "no_speech_prob": 0.16227120161056519}, {"id": 1904, "seek": 496924, "start": 4978.24, "end": 4981.24, "text": " Why are they not engaging where the audience is, right?", "tokens": [50814, 1545, 366, 436, 406, 11268, 689, 264, 4034, 307, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.10798777955951112, "compression_ratio": 1.62, "no_speech_prob": 0.16227120161056519}, {"id": 1905, "seek": 496924, "start": 4981.24, "end": 4982.24, "text": " Right.", "tokens": [50964, 1779, 13, 51014], "temperature": 0.0, "avg_logprob": -0.10798777955951112, "compression_ratio": 1.62, "no_speech_prob": 0.16227120161056519}, {"id": 1906, "seek": 496924, "start": 4982.24, "end": 4987.24, "text": " I had a tweet storm today where I just said, like last month, Sam Oltman was on a podcast", "tokens": [51014, 286, 632, 257, 15258, 7679, 965, 689, 286, 445, 848, 11, 411, 1036, 1618, 11, 4832, 422, 2282, 1601, 390, 322, 257, 7367, 51264], "temperature": 0.0, "avg_logprob": -0.10798777955951112, "compression_ratio": 1.62, "no_speech_prob": 0.16227120161056519}, {"id": 1907, "seek": 496924, "start": 4987.24, "end": 4991.24, "text": " talking about meditation and how much meditation helps them.", "tokens": [51264, 1417, 466, 12537, 293, 577, 709, 12537, 3665, 552, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10798777955951112, "compression_ratio": 1.62, "no_speech_prob": 0.16227120161056519}, {"id": 1908, "seek": 496924, "start": 4991.24, "end": 4993.24, "text": " This is a podcast I've never heard of in my life.", "tokens": [51464, 639, 307, 257, 7367, 286, 600, 1128, 2198, 295, 294, 452, 993, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10798777955951112, "compression_ratio": 1.62, "no_speech_prob": 0.16227120161056519}, {"id": 1909, "seek": 496924, "start": 4993.24, "end": 4994.24, "text": " It's a business podcast.", "tokens": [51564, 467, 311, 257, 1606, 7367, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10798777955951112, "compression_ratio": 1.62, "no_speech_prob": 0.16227120161056519}, {"id": 1910, "seek": 496924, "start": 4994.24, "end": 4997.24, "text": " And he had to explain to the guy what GPT three even is.", "tokens": [51614, 400, 415, 632, 281, 2903, 281, 264, 2146, 437, 26039, 51, 1045, 754, 307, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10798777955951112, "compression_ratio": 1.62, "no_speech_prob": 0.16227120161056519}, {"id": 1911, "seek": 499724, "start": 4997.24, "end": 5001.24, "text": " And so in my, like I tweeted, like, why haven't you been on my podcast?", "tokens": [50364, 400, 370, 294, 452, 11, 411, 286, 25646, 11, 411, 11, 983, 2378, 380, 291, 668, 322, 452, 7367, 30, 50564], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1912, "seek": 499724, "start": 5001.24, "end": 5002.24, "text": " Right.", "tokens": [50564, 1779, 13, 50614], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1913, "seek": 499724, "start": 5002.24, "end": 5009.24, "text": " Like you can, you can reach out to, you know, almost 8,000 GPT three open AI AI developers.", "tokens": [50614, 1743, 291, 393, 11, 291, 393, 2524, 484, 281, 11, 291, 458, 11, 1920, 1649, 11, 1360, 26039, 51, 1045, 1269, 7318, 7318, 8849, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1914, "seek": 499724, "start": 5009.24, "end": 5011.24, "text": " What are you doing talking about meditation?", "tokens": [50964, 708, 366, 291, 884, 1417, 466, 12537, 30, 51064], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1915, "seek": 499724, "start": 5011.24, "end": 5012.24, "text": " Right.", "tokens": [51064, 1779, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1916, "seek": 499724, "start": 5012.24, "end": 5015.24, "text": " So my problem is, is actually a priority problem.", "tokens": [51114, 407, 452, 1154, 307, 11, 307, 767, 257, 9365, 1154, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1917, "seek": 499724, "start": 5015.24, "end": 5016.24, "text": " I can see there is capacity.", "tokens": [51264, 286, 393, 536, 456, 307, 6042, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1918, "seek": 499724, "start": 5016.24, "end": 5022.24, "text": " I can see there are some priorities, but I think if you really lean in as a priority into", "tokens": [51314, 286, 393, 536, 456, 366, 512, 15503, 11, 457, 286, 519, 498, 291, 534, 11659, 294, 382, 257, 9365, 666, 51614], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1919, "seek": 499724, "start": 5022.24, "end": 5025.24, "text": " your developer community, there's certain ways you would, you would move.", "tokens": [51614, 428, 10754, 1768, 11, 456, 311, 1629, 2098, 291, 576, 11, 291, 576, 1286, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1920, "seek": 499724, "start": 5025.24, "end": 5026.24, "text": " Right.", "tokens": [51764, 1779, 13, 51814], "temperature": 0.0, "avg_logprob": -0.09538908147100192, "compression_ratio": 1.703971119133574, "no_speech_prob": 0.01449862401932478}, {"id": 1921, "seek": 502624, "start": 5026.24, "end": 5029.24, "text": " And these, these media channels, there's people in the community.", "tokens": [50364, 400, 613, 11, 613, 3021, 9235, 11, 456, 311, 561, 294, 264, 1768, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1922, "seek": 502624, "start": 5029.24, "end": 5031.24, "text": " There's, there's so many ways they could go about it.", "tokens": [50514, 821, 311, 11, 456, 311, 370, 867, 2098, 436, 727, 352, 466, 309, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1923, "seek": 502624, "start": 5031.24, "end": 5036.24, "text": " And even linking a lot of the documentation to posts in the community forums.", "tokens": [50614, 400, 754, 25775, 257, 688, 295, 264, 14333, 281, 12300, 294, 264, 1768, 26998, 13, 50864], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1924, "seek": 502624, "start": 5036.24, "end": 5038.24, "text": " I don't see why that's not a bad idea.", "tokens": [50864, 286, 500, 380, 536, 983, 300, 311, 406, 257, 1578, 1558, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1925, "seek": 502624, "start": 5038.24, "end": 5039.24, "text": " Right.", "tokens": [50964, 1779, 13, 51014], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1926, "seek": 502624, "start": 5039.24, "end": 5042.24, "text": " Like force people to show the community, show up to the community forums.", "tokens": [51014, 1743, 3464, 561, 281, 855, 264, 1768, 11, 855, 493, 281, 264, 1768, 26998, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1927, "seek": 502624, "start": 5042.24, "end": 5043.24, "text": " Right.", "tokens": [51164, 1779, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1928, "seek": 502624, "start": 5043.24, "end": 5044.24, "text": " Walk them through some of the best threads.", "tokens": [51214, 10818, 552, 807, 512, 295, 264, 1151, 19314, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1929, "seek": 502624, "start": 5044.24, "end": 5048.24, "text": " These are ways in which we could like funnel more people in that direction as well.", "tokens": [51264, 1981, 366, 2098, 294, 597, 321, 727, 411, 24515, 544, 561, 294, 300, 3513, 382, 731, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1930, "seek": 502624, "start": 5048.24, "end": 5050.24, "text": " That costs virtually nothing.", "tokens": [51464, 663, 5497, 14103, 1825, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1931, "seek": 502624, "start": 5050.24, "end": 5051.24, "text": " Right.", "tokens": [51564, 1779, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1932, "seek": 502624, "start": 5051.24, "end": 5055.24, "text": " And so you need to also invest in the community forums that it needs to be building a community", "tokens": [51614, 400, 370, 291, 643, 281, 611, 1963, 294, 264, 1768, 26998, 300, 309, 2203, 281, 312, 2390, 257, 1768, 51814], "temperature": 0.0, "avg_logprob": -0.08812762561597322, "compression_ratio": 1.911764705882353, "no_speech_prob": 0.0047540199011564255}, {"id": 1933, "seek": 505524, "start": 5055.24, "end": 5057.24, "text": " is a company wide thing.", "tokens": [50364, 307, 257, 2237, 4874, 551, 13, 50464], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1934, "seek": 505524, "start": 5057.24, "end": 5062.24, "text": " It's not something which can be outsourced to a single employee or overseen by PR.", "tokens": [50464, 467, 311, 406, 746, 597, 393, 312, 14758, 396, 1232, 281, 257, 2167, 10738, 420, 11916, 268, 538, 11568, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1935, "seek": 505524, "start": 5062.24, "end": 5065.24, "text": " It needs to come from a, from a, you know, it needs to come from the heart.", "tokens": [50714, 467, 2203, 281, 808, 490, 257, 11, 490, 257, 11, 291, 458, 11, 309, 2203, 281, 808, 490, 264, 1917, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1936, "seek": 505524, "start": 5065.24, "end": 5070.24, "text": " I know that sounds so corny, but anyways, clearly I, you know, I, I get too emotional", "tokens": [50864, 286, 458, 300, 3263, 370, 1181, 1634, 11, 457, 13448, 11, 4448, 286, 11, 291, 458, 11, 286, 11, 286, 483, 886, 6863, 51114], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1937, "seek": 505524, "start": 5070.24, "end": 5072.24, "text": " about this community stuff.", "tokens": [51114, 466, 341, 1768, 1507, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1938, "seek": 505524, "start": 5072.24, "end": 5073.24, "text": " Yeah.", "tokens": [51214, 865, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1939, "seek": 505524, "start": 5073.24, "end": 5076.24, "text": " Anyways, these are, these are all things going on behind the scenes.", "tokens": [51264, 15585, 11, 613, 366, 11, 613, 366, 439, 721, 516, 322, 2261, 264, 8026, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1940, "seek": 505524, "start": 5076.24, "end": 5080.24, "text": " I apologize to all the listeners if they're like, like, this is cool, like cool story,", "tokens": [51414, 286, 12328, 281, 439, 264, 23274, 498, 436, 434, 411, 11, 411, 11, 341, 307, 1627, 11, 411, 1627, 1657, 11, 51614], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1941, "seek": 505524, "start": 5080.24, "end": 5081.24, "text": " bro.", "tokens": [51614, 2006, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1942, "seek": 505524, "start": 5081.24, "end": 5084.24, "text": " Like anyways.", "tokens": [51664, 1743, 13448, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10744813510349818, "compression_ratio": 1.750915750915751, "no_speech_prob": 0.031127003952860832}, {"id": 1943, "seek": 508424, "start": 5084.24, "end": 5086.24, "text": " So we're coming towards the end here.", "tokens": [50364, 407, 321, 434, 1348, 3030, 264, 917, 510, 13, 50464], "temperature": 0.0, "avg_logprob": -0.1328116678724102, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.003944238182157278}, {"id": 1944, "seek": 508424, "start": 5086.24, "end": 5088.24, "text": " I think I had just two broader questions.", "tokens": [50464, 286, 519, 286, 632, 445, 732, 13227, 1651, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1328116678724102, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.003944238182157278}, {"id": 1945, "seek": 508424, "start": 5088.24, "end": 5093.24, "text": " So what are your thoughts on multimodal AI technology?", "tokens": [50564, 407, 437, 366, 428, 4598, 322, 32972, 378, 304, 7318, 2899, 30, 50814], "temperature": 0.0, "avg_logprob": -0.1328116678724102, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.003944238182157278}, {"id": 1946, "seek": 508424, "start": 5093.24, "end": 5097.24, "text": " I think it's definitely going to be a critical component for the future.", "tokens": [50814, 286, 519, 309, 311, 2138, 516, 281, 312, 257, 4924, 6542, 337, 264, 2027, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1328116678724102, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.003944238182157278}, {"id": 1947, "seek": 508424, "start": 5097.24, "end": 5098.24, "text": " Right.", "tokens": [51014, 1779, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1328116678724102, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.003944238182157278}, {"id": 1948, "seek": 508424, "start": 5098.24, "end": 5103.24, "text": " I, I, I addressed that shortcoming in my book, natural language, cognitive architecture,", "tokens": [51064, 286, 11, 286, 11, 286, 13847, 300, 2099, 6590, 294, 452, 1446, 11, 3303, 2856, 11, 15605, 9482, 11, 51314], "temperature": 0.0, "avg_logprob": -0.1328116678724102, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.003944238182157278}, {"id": 1949, "seek": 508424, "start": 5103.24, "end": 5109.24, "text": " it thinks and takes in only text, which means, you know, speech, chat, whatever.", "tokens": [51314, 309, 7309, 293, 2516, 294, 787, 2487, 11, 597, 1355, 11, 291, 458, 11, 6218, 11, 5081, 11, 2035, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1328116678724102, "compression_ratio": 1.5238095238095237, "no_speech_prob": 0.003944238182157278}, {"id": 1950, "seek": 510924, "start": 5109.24, "end": 5114.24, "text": " I think in order to have a fully robust, for instance, if you want to have a fully autonomous,", "tokens": [50364, 286, 519, 294, 1668, 281, 362, 257, 4498, 13956, 11, 337, 5197, 11, 498, 291, 528, 281, 362, 257, 4498, 23797, 11, 50614], "temperature": 0.0, "avg_logprob": -0.05698512340414113, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.32065343856811523}, {"id": 1951, "seek": 510924, "start": 5114.24, "end": 5118.24, "text": " you know, robot that's going to wander around your house and help you out, it's going to", "tokens": [50614, 291, 458, 11, 7881, 300, 311, 516, 281, 27541, 926, 428, 1782, 293, 854, 291, 484, 11, 309, 311, 516, 281, 50814], "temperature": 0.0, "avg_logprob": -0.05698512340414113, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.32065343856811523}, {"id": 1952, "seek": 510924, "start": 5118.24, "end": 5120.24, "text": " need to integrate audio and video.", "tokens": [50814, 643, 281, 13365, 6278, 293, 960, 13, 50914], "temperature": 0.0, "avg_logprob": -0.05698512340414113, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.32065343856811523}, {"id": 1953, "seek": 510924, "start": 5120.24, "end": 5124.24, "text": " And if you can do that in a single neural network, great.", "tokens": [50914, 400, 498, 291, 393, 360, 300, 294, 257, 2167, 18161, 3209, 11, 869, 13, 51114], "temperature": 0.0, "avg_logprob": -0.05698512340414113, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.32065343856811523}, {"id": 1954, "seek": 510924, "start": 5124.24, "end": 5128.24, "text": " I don't know that it'll be necessary to achieve AGI.", "tokens": [51114, 286, 500, 380, 458, 300, 309, 603, 312, 4818, 281, 4584, 316, 26252, 13, 51314], "temperature": 0.0, "avg_logprob": -0.05698512340414113, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.32065343856811523}, {"id": 1955, "seek": 510924, "start": 5128.24, "end": 5133.24, "text": " It might end up being, it might be one of those, it might be one of those like rare dead ends,", "tokens": [51314, 467, 1062, 917, 493, 885, 11, 309, 1062, 312, 472, 295, 729, 11, 309, 1062, 312, 472, 295, 729, 411, 5892, 3116, 5314, 11, 51564], "temperature": 0.0, "avg_logprob": -0.05698512340414113, "compression_ratio": 1.7666666666666666, "no_speech_prob": 0.32065343856811523}, {"id": 1956, "seek": 513324, "start": 5133.24, "end": 5134.24, "text": " right?", "tokens": [50364, 558, 30, 50414], "temperature": 0.0, "avg_logprob": -0.0882345093621148, "compression_ratio": 1.6717171717171717, "no_speech_prob": 0.4685870110988617}, {"id": 1957, "seek": 513324, "start": 5134.24, "end": 5140.24, "text": " Where because, you know, thinking visually, thinking, thinking in terms of sound, that", "tokens": [50414, 2305, 570, 11, 291, 458, 11, 1953, 19622, 11, 1953, 11, 1953, 294, 2115, 295, 1626, 11, 300, 50714], "temperature": 0.0, "avg_logprob": -0.0882345093621148, "compression_ratio": 1.6717171717171717, "no_speech_prob": 0.4685870110988617}, {"id": 1958, "seek": 513324, "start": 5140.24, "end": 5143.24, "text": " might not actually bias that much, right?", "tokens": [50714, 1062, 406, 767, 12577, 300, 709, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.0882345093621148, "compression_ratio": 1.6717171717171717, "no_speech_prob": 0.4685870110988617}, {"id": 1959, "seek": 513324, "start": 5143.24, "end": 5149.24, "text": " Because you can represent 95% of human thought in text, right?", "tokens": [50864, 1436, 291, 393, 2906, 13420, 4, 295, 1952, 1194, 294, 2487, 11, 558, 30, 51164], "temperature": 0.0, "avg_logprob": -0.0882345093621148, "compression_ratio": 1.6717171717171717, "no_speech_prob": 0.4685870110988617}, {"id": 1960, "seek": 513324, "start": 5149.24, "end": 5153.24, "text": " It might take a little bit more, but it, you know, it might be more expensive.", "tokens": [51164, 467, 1062, 747, 257, 707, 857, 544, 11, 457, 309, 11, 291, 458, 11, 309, 1062, 312, 544, 5124, 13, 51364], "temperature": 0.0, "avg_logprob": -0.0882345093621148, "compression_ratio": 1.6717171717171717, "no_speech_prob": 0.4685870110988617}, {"id": 1961, "seek": 513324, "start": 5153.24, "end": 5156.24, "text": " And also how big are those models going to be, right?", "tokens": [51364, 400, 611, 577, 955, 366, 729, 5245, 516, 281, 312, 11, 558, 30, 51514], "temperature": 0.0, "avg_logprob": -0.0882345093621148, "compression_ratio": 1.6717171717171717, "no_speech_prob": 0.4685870110988617}, {"id": 1962, "seek": 515624, "start": 5156.24, "end": 5163.24, "text": " Because if just, if just a text model of GPT-3 has to run on, you know, $7 million worth of", "tokens": [50364, 1436, 498, 445, 11, 498, 445, 257, 2487, 2316, 295, 26039, 51, 12, 18, 575, 281, 1190, 322, 11, 291, 458, 11, 1848, 22, 2459, 3163, 295, 50714], "temperature": 0.0, "avg_logprob": -0.08323695135454759, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.5960958003997803}, {"id": 1963, "seek": 515624, "start": 5163.24, "end": 5166.24, "text": " hardware or however much it is, you know, because it's got to run on a bunch of different", "tokens": [50714, 8837, 420, 4461, 709, 309, 307, 11, 291, 458, 11, 570, 309, 311, 658, 281, 1190, 322, 257, 3840, 295, 819, 50864], "temperature": 0.0, "avg_logprob": -0.08323695135454759, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.5960958003997803}, {"id": 1964, "seek": 515624, "start": 5166.24, "end": 5172.24, "text": " GPUs, if it's that expensive, how much more expensive, how much bigger is a giant multimodal", "tokens": [50864, 18407, 82, 11, 498, 309, 311, 300, 5124, 11, 577, 709, 544, 5124, 11, 577, 709, 3801, 307, 257, 7410, 32972, 378, 304, 51164], "temperature": 0.0, "avg_logprob": -0.08323695135454759, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.5960958003997803}, {"id": 1965, "seek": 515624, "start": 5172.24, "end": 5173.24, "text": " model going to be?", "tokens": [51164, 2316, 516, 281, 312, 30, 51214], "temperature": 0.0, "avg_logprob": -0.08323695135454759, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.5960958003997803}, {"id": 1966, "seek": 515624, "start": 5173.24, "end": 5176.24, "text": " So that's, that's the biggest cost.", "tokens": [51214, 407, 300, 311, 11, 300, 311, 264, 3880, 2063, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08323695135454759, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.5960958003997803}, {"id": 1967, "seek": 515624, "start": 5176.24, "end": 5180.24, "text": " Obviously computer technology is going to get better over time, you know, and I think I", "tokens": [51364, 7580, 3820, 2899, 307, 516, 281, 483, 1101, 670, 565, 11, 291, 458, 11, 293, 286, 519, 286, 51564], "temperature": 0.0, "avg_logprob": -0.08323695135454759, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.5960958003997803}, {"id": 1968, "seek": 515624, "start": 5180.24, "end": 5185.24, "text": " calculated it out, I think in 10 years, your average company could afford to run GPT-3", "tokens": [51564, 15598, 309, 484, 11, 286, 519, 294, 1266, 924, 11, 428, 4274, 2237, 727, 6157, 281, 1190, 26039, 51, 12, 18, 51814], "temperature": 0.0, "avg_logprob": -0.08323695135454759, "compression_ratio": 1.7027027027027026, "no_speech_prob": 0.5960958003997803}, {"id": 1969, "seek": 518524, "start": 5185.24, "end": 5186.24, "text": " in-house.", "tokens": [50364, 294, 12, 6410, 13, 50414], "temperature": 0.0, "avg_logprob": -0.06368598812504818, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.021605728194117546}, {"id": 1970, "seek": 518524, "start": 5186.24, "end": 5190.24, "text": " In 20 years, you could probably run GPT-3 on your desktop.", "tokens": [50414, 682, 945, 924, 11, 291, 727, 1391, 1190, 26039, 51, 12, 18, 322, 428, 14502, 13, 50614], "temperature": 0.0, "avg_logprob": -0.06368598812504818, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.021605728194117546}, {"id": 1971, "seek": 518524, "start": 5190.24, "end": 5194.24, "text": " And in 30 years, GPT-3 could run on your phone, right?", "tokens": [50614, 400, 294, 2217, 924, 11, 26039, 51, 12, 18, 727, 1190, 322, 428, 2593, 11, 558, 30, 50814], "temperature": 0.0, "avg_logprob": -0.06368598812504818, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.021605728194117546}, {"id": 1972, "seek": 518524, "start": 5194.24, "end": 5196.24, "text": " So that's a long timeline.", "tokens": [50814, 407, 300, 311, 257, 938, 12933, 13, 50914], "temperature": 0.0, "avg_logprob": -0.06368598812504818, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.021605728194117546}, {"id": 1973, "seek": 518524, "start": 5196.24, "end": 5199.24, "text": " But in the meantime, we're going to be making bigger and bigger models.", "tokens": [50914, 583, 294, 264, 14991, 11, 321, 434, 516, 281, 312, 1455, 3801, 293, 3801, 5245, 13, 51064], "temperature": 0.0, "avg_logprob": -0.06368598812504818, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.021605728194117546}, {"id": 1974, "seek": 518524, "start": 5199.24, "end": 5203.24, "text": " And I'm afraid that there's going to be diminishing returns, right?", "tokens": [51064, 400, 286, 478, 4638, 300, 456, 311, 516, 281, 312, 15739, 3807, 11247, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.06368598812504818, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.021605728194117546}, {"id": 1975, "seek": 518524, "start": 5203.24, "end": 5207.24, "text": " You know, people, right now people seem to think that it's going to follow an exponential", "tokens": [51264, 509, 458, 11, 561, 11, 558, 586, 561, 1643, 281, 519, 300, 309, 311, 516, 281, 1524, 364, 21510, 51464], "temperature": 0.0, "avg_logprob": -0.06368598812504818, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.021605728194117546}, {"id": 1976, "seek": 518524, "start": 5207.24, "end": 5211.24, "text": " growth curve forever, but it might actually follow a sigmoid curve, right?", "tokens": [51464, 4599, 7605, 5680, 11, 457, 309, 1062, 767, 1524, 257, 4556, 3280, 327, 7605, 11, 558, 30, 51664], "temperature": 0.0, "avg_logprob": -0.06368598812504818, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.021605728194117546}, {"id": 1977, "seek": 518524, "start": 5211.24, "end": 5214.24, "text": " We might be at the point of fastest growth right now, but we're going to see diminishing", "tokens": [51664, 492, 1062, 312, 412, 264, 935, 295, 14573, 4599, 558, 586, 11, 457, 321, 434, 516, 281, 536, 15739, 3807, 51814], "temperature": 0.0, "avg_logprob": -0.06368598812504818, "compression_ratio": 1.8133333333333332, "no_speech_prob": 0.021605728194117546}, {"id": 1978, "seek": 521424, "start": 5214.24, "end": 5215.24, "text": " returns soon.", "tokens": [50364, 11247, 2321, 13, 50414], "temperature": 0.0, "avg_logprob": -0.10581353131462545, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.012818758375942707}, {"id": 1979, "seek": 521424, "start": 5215.24, "end": 5222.24, "text": " And so like, yeah, multimodal models are certainly going to have capabilities that GPT-3 doesn't.", "tokens": [50414, 400, 370, 411, 11, 1338, 11, 32972, 378, 304, 5245, 366, 3297, 516, 281, 362, 10862, 300, 26039, 51, 12, 18, 1177, 380, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10581353131462545, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.012818758375942707}, {"id": 1980, "seek": 521424, "start": 5222.24, "end": 5228.24, "text": " But for the sake of, for the sake of like, if you want to create a self-improving chatbot,", "tokens": [50764, 583, 337, 264, 9717, 295, 11, 337, 264, 9717, 295, 411, 11, 498, 291, 528, 281, 1884, 257, 2698, 12, 332, 4318, 798, 5081, 18870, 11, 51064], "temperature": 0.0, "avg_logprob": -0.10581353131462545, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.012818758375942707}, {"id": 1981, "seek": 521424, "start": 5228.24, "end": 5233.24, "text": " GPT-3 and Codex might be enough, or at least, you know, that's, that's, that single mode", "tokens": [51064, 26039, 51, 12, 18, 293, 15549, 87, 1062, 312, 1547, 11, 420, 412, 1935, 11, 291, 458, 11, 300, 311, 11, 300, 311, 11, 300, 2167, 4391, 51314], "temperature": 0.0, "avg_logprob": -0.10581353131462545, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.012818758375942707}, {"id": 1982, "seek": 521424, "start": 5233.24, "end": 5234.24, "text": " technology.", "tokens": [51314, 2899, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10581353131462545, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.012818758375942707}, {"id": 1983, "seek": 521424, "start": 5234.24, "end": 5237.24, "text": " There was another thought, but it ran away.", "tokens": [51364, 821, 390, 1071, 1194, 11, 457, 309, 5872, 1314, 13, 51514], "temperature": 0.0, "avg_logprob": -0.10581353131462545, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.012818758375942707}, {"id": 1984, "seek": 521424, "start": 5237.24, "end": 5238.24, "text": " Sorry.", "tokens": [51514, 4919, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10581353131462545, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.012818758375942707}, {"id": 1985, "seek": 521424, "start": 5238.24, "end": 5241.24, "text": " But yeah, those, those, that's kind of, that's kind of my big take is there, there could", "tokens": [51564, 583, 1338, 11, 729, 11, 729, 11, 300, 311, 733, 295, 11, 300, 311, 733, 295, 452, 955, 747, 307, 456, 11, 456, 727, 51714], "temperature": 0.0, "avg_logprob": -0.10581353131462545, "compression_ratio": 1.678030303030303, "no_speech_prob": 0.012818758375942707}, {"id": 1986, "seek": 524124, "start": 5241.24, "end": 5243.24, "text": " be benefits, but there's going to be costs too.", "tokens": [50364, 312, 5311, 11, 457, 456, 311, 516, 281, 312, 5497, 886, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1987, "seek": 524124, "start": 5243.24, "end": 5245.24, "text": " So we got to be cognizant of that.", "tokens": [50464, 407, 321, 658, 281, 312, 11786, 590, 394, 295, 300, 13, 50564], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1988, "seek": 524124, "start": 5245.24, "end": 5246.24, "text": " Yeah.", "tokens": [50564, 865, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1989, "seek": 524124, "start": 5246.24, "end": 5248.24, "text": " And there might not be enough compute in the world.", "tokens": [50614, 400, 456, 1062, 406, 312, 1547, 14722, 294, 264, 1002, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1990, "seek": 524124, "start": 5248.24, "end": 5252.24, "text": " They're not, there might not even be enough energy or we may like consume all energy ever", "tokens": [50714, 814, 434, 406, 11, 456, 1062, 406, 754, 312, 1547, 2281, 420, 321, 815, 411, 14732, 439, 2281, 1562, 50914], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1991, "seek": 524124, "start": 5252.24, "end": 5255.24, "text": " produced to make, to train a single model.", "tokens": [50914, 7126, 281, 652, 11, 281, 3847, 257, 2167, 2316, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1992, "seek": 524124, "start": 5255.24, "end": 5258.24, "text": " And then we may be able to run it inference for like three seconds.", "tokens": [51064, 400, 550, 321, 815, 312, 1075, 281, 1190, 309, 38253, 337, 411, 1045, 3949, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1993, "seek": 524124, "start": 5258.24, "end": 5259.24, "text": " Right.", "tokens": [51214, 1779, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1994, "seek": 524124, "start": 5259.24, "end": 5262.24, "text": " And then it just shuts down the global power system or something, right?", "tokens": [51264, 400, 550, 309, 445, 48590, 760, 264, 4338, 1347, 1185, 420, 746, 11, 558, 30, 51414], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1995, "seek": 524124, "start": 5262.24, "end": 5267.24, "text": " But can you see yourself, let's say the technology exists, cost considerations aside, can you", "tokens": [51414, 583, 393, 291, 536, 1803, 11, 718, 311, 584, 264, 2899, 8198, 11, 2063, 24070, 7359, 11, 393, 291, 51664], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1996, "seek": 524124, "start": 5267.24, "end": 5269.24, "text": " see yourself perhaps making movies?", "tokens": [51664, 536, 1803, 4317, 1455, 6233, 30, 51764], "temperature": 0.0, "avg_logprob": -0.07752182059092065, "compression_ratio": 1.7436708860759493, "no_speech_prob": 0.028423182666301727}, {"id": 1997, "seek": 526924, "start": 5269.24, "end": 5274.24, "text": " Can you see yourself, you know, giving your book to a multimodal model, having, have it", "tokens": [50364, 1664, 291, 536, 1803, 11, 291, 458, 11, 2902, 428, 1446, 281, 257, 32972, 378, 304, 2316, 11, 1419, 11, 362, 309, 50614], "temperature": 0.0, "avg_logprob": -0.07939624786376953, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011683954857289791}, {"id": 1998, "seek": 526924, "start": 5274.24, "end": 5277.24, "text": " generate a documentary based on it or some marketing material?", "tokens": [50614, 8460, 257, 15674, 2361, 322, 309, 420, 512, 6370, 2527, 30, 50764], "temperature": 0.0, "avg_logprob": -0.07939624786376953, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011683954857289791}, {"id": 1999, "seek": 526924, "start": 5277.24, "end": 5282.24, "text": " What can you see yourself doing with, you know, the, the, the multimodal model of your", "tokens": [50764, 708, 393, 291, 536, 1803, 884, 365, 11, 291, 458, 11, 264, 11, 264, 11, 264, 32972, 378, 304, 2316, 295, 428, 51014], "temperature": 0.0, "avg_logprob": -0.07939624786376953, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011683954857289791}, {"id": 2000, "seek": 526924, "start": 5282.24, "end": 5283.24, "text": " dreams?", "tokens": [51014, 7505, 30, 51064], "temperature": 0.0, "avg_logprob": -0.07939624786376953, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011683954857289791}, {"id": 2001, "seek": 526924, "start": 5283.24, "end": 5284.24, "text": " Yeah.", "tokens": [51064, 865, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07939624786376953, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011683954857289791}, {"id": 2002, "seek": 526924, "start": 5284.24, "end": 5288.24, "text": " So, you know, kind of the thought experiment that I did was, okay, well, we've got, you", "tokens": [51114, 407, 11, 291, 458, 11, 733, 295, 264, 1194, 5120, 300, 286, 630, 390, 11, 1392, 11, 731, 11, 321, 600, 658, 11, 291, 51314], "temperature": 0.0, "avg_logprob": -0.07939624786376953, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011683954857289791}, {"id": 2003, "seek": 526924, "start": 5288.24, "end": 5291.24, "text": " know, how much, how much data is on YouTube?", "tokens": [51314, 458, 11, 577, 709, 11, 577, 709, 1412, 307, 322, 3088, 30, 51464], "temperature": 0.0, "avg_logprob": -0.07939624786376953, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011683954857289791}, {"id": 2004, "seek": 526924, "start": 5291.24, "end": 5295.24, "text": " I think it's like a thousand years or 10,000 years worth of video on YouTube.", "tokens": [51464, 286, 519, 309, 311, 411, 257, 4714, 924, 420, 1266, 11, 1360, 924, 3163, 295, 960, 322, 3088, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07939624786376953, "compression_ratio": 1.7633587786259541, "no_speech_prob": 0.011683954857289791}, {"id": 2005, "seek": 529524, "start": 5295.24, "end": 5298.24, "text": " And of course, it's many, many, many terabytes.", "tokens": [50364, 400, 295, 1164, 11, 309, 311, 867, 11, 867, 11, 867, 1796, 24538, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2006, "seek": 529524, "start": 5298.24, "end": 5302.24, "text": " You know, so it's like, that's, that's way more training data.", "tokens": [50514, 509, 458, 11, 370, 309, 311, 411, 11, 300, 311, 11, 300, 311, 636, 544, 3097, 1412, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2007, "seek": 529524, "start": 5302.24, "end": 5306.24, "text": " You know, if GPT three was trained on less than one terabyte of data and, you know,", "tokens": [50714, 509, 458, 11, 498, 26039, 51, 1045, 390, 8895, 322, 1570, 813, 472, 1796, 34529, 295, 1412, 293, 11, 291, 458, 11, 50914], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2008, "seek": 529524, "start": 5306.24, "end": 5309.24, "text": " YouTube is approaching like the Yota bite scale, right?", "tokens": [50914, 3088, 307, 14908, 411, 264, 398, 5377, 7988, 4373, 11, 558, 30, 51064], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2009, "seek": 529524, "start": 5309.24, "end": 5311.24, "text": " That's, that's an insane amount of data.", "tokens": [51064, 663, 311, 11, 300, 311, 364, 10838, 2372, 295, 1412, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2010, "seek": 529524, "start": 5311.24, "end": 5314.24, "text": " So, okay, let's say you feed that in.", "tokens": [51164, 407, 11, 1392, 11, 718, 311, 584, 291, 3154, 300, 294, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2011, "seek": 529524, "start": 5314.24, "end": 5318.24, "text": " And so you got audio video, you've got text, you've got all the comments and you end up", "tokens": [51314, 400, 370, 291, 658, 6278, 960, 11, 291, 600, 658, 2487, 11, 291, 600, 658, 439, 264, 3053, 293, 291, 917, 493, 51514], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2012, "seek": 529524, "start": 5318.24, "end": 5322.24, "text": " with like a model trained on, on all of YouTube data.", "tokens": [51514, 365, 411, 257, 2316, 8895, 322, 11, 322, 439, 295, 3088, 1412, 13, 51714], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2013, "seek": 529524, "start": 5322.24, "end": 5323.24, "text": " Okay, cool.", "tokens": [51714, 1033, 11, 1627, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2014, "seek": 529524, "start": 5323.24, "end": 5324.24, "text": " What can you do with that?", "tokens": [51764, 708, 393, 291, 360, 365, 300, 30, 51814], "temperature": 0.0, "avg_logprob": -0.10059866935584195, "compression_ratio": 1.795774647887324, "no_speech_prob": 0.023683296516537666}, {"id": 2015, "seek": 532424, "start": 5324.24, "end": 5327.24, "text": " Like, I can't even imagine, right?", "tokens": [50364, 1743, 11, 286, 393, 380, 754, 3811, 11, 558, 30, 50514], "temperature": 0.0, "avg_logprob": -0.10152240982629303, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.00941029004752636}, {"id": 2016, "seek": 532424, "start": 5327.24, "end": 5332.24, "text": " Because GPT three today is almost capable of writing screenplays.", "tokens": [50514, 1436, 26039, 51, 1045, 965, 307, 1920, 8189, 295, 3579, 2568, 45755, 13, 50764], "temperature": 0.0, "avg_logprob": -0.10152240982629303, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.00941029004752636}, {"id": 2017, "seek": 532424, "start": 5332.24, "end": 5333.24, "text": " Right?", "tokens": [50764, 1779, 30, 50814], "temperature": 0.0, "avg_logprob": -0.10152240982629303, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.00941029004752636}, {"id": 2018, "seek": 532424, "start": 5333.24, "end": 5338.24, "text": " So if you have a model that's trained on, you know, all text data, all audio data, all", "tokens": [50814, 407, 498, 291, 362, 257, 2316, 300, 311, 8895, 322, 11, 291, 458, 11, 439, 2487, 1412, 11, 439, 6278, 1412, 11, 439, 51064], "temperature": 0.0, "avg_logprob": -0.10152240982629303, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.00941029004752636}, {"id": 2019, "seek": 532424, "start": 5338.24, "end": 5341.24, "text": " video data, you say, Hey, write me a screenplay.", "tokens": [51064, 960, 1412, 11, 291, 584, 11, 1911, 11, 2464, 385, 257, 2568, 2858, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10152240982629303, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.00941029004752636}, {"id": 2020, "seek": 532424, "start": 5341.24, "end": 5342.24, "text": " Right?", "tokens": [51214, 1779, 30, 51264], "temperature": 0.0, "avg_logprob": -0.10152240982629303, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.00941029004752636}, {"id": 2021, "seek": 532424, "start": 5342.24, "end": 5346.24, "text": " I actually, I, and near the end of my book, I kind of have a chapter of speculation.", "tokens": [51264, 286, 767, 11, 286, 11, 293, 2651, 264, 917, 295, 452, 1446, 11, 286, 733, 295, 362, 257, 7187, 295, 27696, 13, 51464], "temperature": 0.0, "avg_logprob": -0.10152240982629303, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.00941029004752636}, {"id": 2022, "seek": 532424, "start": 5346.24, "end": 5352.24, "text": " And I say, what if, what if you have this model and you say, give me season two of Firefly?", "tokens": [51464, 400, 286, 584, 11, 437, 498, 11, 437, 498, 291, 362, 341, 2316, 293, 291, 584, 11, 976, 385, 3196, 732, 295, 7652, 14061, 30, 51764], "temperature": 0.0, "avg_logprob": -0.10152240982629303, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.00941029004752636}, {"id": 2023, "seek": 532424, "start": 5352.24, "end": 5353.24, "text": " Right?", "tokens": [51764, 1779, 30, 51814], "temperature": 0.0, "avg_logprob": -0.10152240982629303, "compression_ratio": 1.6628352490421456, "no_speech_prob": 0.00941029004752636}, {"id": 2024, "seek": 535324, "start": 5353.24, "end": 5357.24, "text": " Like, you know, you could, you could just keep watching whatever show you want.", "tokens": [50364, 1743, 11, 291, 458, 11, 291, 727, 11, 291, 727, 445, 1066, 1976, 2035, 855, 291, 528, 13, 50564], "temperature": 0.0, "avg_logprob": -0.09458586047677432, "compression_ratio": 1.7, "no_speech_prob": 0.0028004576452076435}, {"id": 2025, "seek": 535324, "start": 5357.24, "end": 5361.24, "text": " You say, give me Game of Thrones, but give me a different, you know, season eight, give", "tokens": [50564, 509, 584, 11, 976, 385, 7522, 295, 31659, 11, 457, 976, 385, 257, 819, 11, 291, 458, 11, 3196, 3180, 11, 976, 50764], "temperature": 0.0, "avg_logprob": -0.09458586047677432, "compression_ratio": 1.7, "no_speech_prob": 0.0028004576452076435}, {"id": 2026, "seek": 535324, "start": 5361.24, "end": 5364.24, "text": " me season, you know, different season eight and season nine and 10.", "tokens": [50764, 385, 3196, 11, 291, 458, 11, 819, 3196, 3180, 293, 3196, 4949, 293, 1266, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09458586047677432, "compression_ratio": 1.7, "no_speech_prob": 0.0028004576452076435}, {"id": 2027, "seek": 535324, "start": 5364.24, "end": 5365.24, "text": " Right?", "tokens": [50914, 1779, 30, 50964], "temperature": 0.0, "avg_logprob": -0.09458586047677432, "compression_ratio": 1.7, "no_speech_prob": 0.0028004576452076435}, {"id": 2028, "seek": 535324, "start": 5365.24, "end": 5369.24, "text": " So I kind of imagine that one possibility is hyper personalized entertainment.", "tokens": [50964, 407, 286, 733, 295, 3811, 300, 472, 7959, 307, 9848, 28415, 12393, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09458586047677432, "compression_ratio": 1.7, "no_speech_prob": 0.0028004576452076435}, {"id": 2029, "seek": 535324, "start": 5369.24, "end": 5373.24, "text": " And of course, like that might be 30 years away just because of like you said, the energy", "tokens": [51164, 400, 295, 1164, 11, 411, 300, 1062, 312, 2217, 924, 1314, 445, 570, 295, 411, 291, 848, 11, 264, 2281, 51364], "temperature": 0.0, "avg_logprob": -0.09458586047677432, "compression_ratio": 1.7, "no_speech_prob": 0.0028004576452076435}, {"id": 2030, "seek": 535324, "start": 5373.24, "end": 5375.24, "text": " intensity of this task.", "tokens": [51364, 13749, 295, 341, 5633, 13, 51464], "temperature": 0.0, "avg_logprob": -0.09458586047677432, "compression_ratio": 1.7, "no_speech_prob": 0.0028004576452076435}, {"id": 2031, "seek": 535324, "start": 5375.24, "end": 5378.24, "text": " But I, conceptually, it's possible, right?", "tokens": [51464, 583, 286, 11, 3410, 671, 11, 309, 311, 1944, 11, 558, 30, 51614], "temperature": 0.0, "avg_logprob": -0.09458586047677432, "compression_ratio": 1.7, "no_speech_prob": 0.0028004576452076435}, {"id": 2032, "seek": 535324, "start": 5378.24, "end": 5380.24, "text": " You can hop on GPT three today.", "tokens": [51614, 509, 393, 3818, 322, 26039, 51, 1045, 965, 13, 51714], "temperature": 0.0, "avg_logprob": -0.09458586047677432, "compression_ratio": 1.7, "no_speech_prob": 0.0028004576452076435}, {"id": 2033, "seek": 538024, "start": 5380.24, "end": 5385.24, "text": " Use the instruct model and say, write a screenplay for, you know, Firefly season two, and it'll", "tokens": [50364, 8278, 264, 7232, 2316, 293, 584, 11, 2464, 257, 2568, 2858, 337, 11, 291, 458, 11, 7652, 14061, 3196, 732, 11, 293, 309, 603, 50614], "temperature": 0.0, "avg_logprob": -0.10240069241590903, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.15600763261318207}, {"id": 2034, "seek": 538024, "start": 5385.24, "end": 5387.24, "text": " try, it'll get close.", "tokens": [50614, 853, 11, 309, 603, 483, 1998, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10240069241590903, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.15600763261318207}, {"id": 2035, "seek": 538024, "start": 5387.24, "end": 5392.24, "text": " And so then if you can take that text output and feed it into a multimodal model that can", "tokens": [50714, 400, 370, 550, 498, 291, 393, 747, 300, 2487, 5598, 293, 3154, 309, 666, 257, 32972, 378, 304, 2316, 300, 393, 50964], "temperature": 0.0, "avg_logprob": -0.10240069241590903, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.15600763261318207}, {"id": 2036, "seek": 538024, "start": 5392.24, "end": 5394.24, "text": " translate text to video, why not?", "tokens": [50964, 13799, 2487, 281, 960, 11, 983, 406, 30, 51064], "temperature": 0.0, "avg_logprob": -0.10240069241590903, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.15600763261318207}, {"id": 2037, "seek": 538024, "start": 5394.24, "end": 5399.24, "text": " You know, and Adobe actually, I don't know if you've seen it, but Adobe is, is already starting", "tokens": [51064, 509, 458, 11, 293, 24862, 767, 11, 286, 500, 380, 458, 498, 291, 600, 1612, 309, 11, 457, 24862, 307, 11, 307, 1217, 2891, 51314], "temperature": 0.0, "avg_logprob": -0.10240069241590903, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.15600763261318207}, {"id": 2038, "seek": 538024, "start": 5399.24, "end": 5404.24, "text": " on that where they're like inferencing, um, like, uh, what, what's the term?", "tokens": [51314, 322, 300, 689, 436, 434, 411, 13596, 13644, 11, 1105, 11, 411, 11, 2232, 11, 437, 11, 437, 311, 264, 1433, 30, 51564], "temperature": 0.0, "avg_logprob": -0.10240069241590903, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.15600763261318207}, {"id": 2039, "seek": 538024, "start": 5404.24, "end": 5408.24, "text": " They're like imputing the sound so you can put in a soundless video and it'll generate", "tokens": [51564, 814, 434, 411, 704, 10861, 264, 1626, 370, 291, 393, 829, 294, 257, 1626, 1832, 960, 293, 309, 603, 8460, 51764], "temperature": 0.0, "avg_logprob": -0.10240069241590903, "compression_ratio": 1.7098976109215016, "no_speech_prob": 0.15600763261318207}, {"id": 2040, "seek": 540824, "start": 5408.24, "end": 5410.24, "text": " the audio sound effects for you or vice versa.", "tokens": [50364, 264, 6278, 1626, 5065, 337, 291, 420, 11964, 25650, 13, 50464], "temperature": 0.0, "avg_logprob": -0.08092719447003664, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.1008421778678894}, {"id": 2041, "seek": 540824, "start": 5410.24, "end": 5411.24, "text": " It's really cool.", "tokens": [50464, 467, 311, 534, 1627, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08092719447003664, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.1008421778678894}, {"id": 2042, "seek": 540824, "start": 5411.24, "end": 5418.24, "text": " And so I think it like a company like Adobe that they have a huge vested interest in mastering", "tokens": [50514, 400, 370, 286, 519, 309, 411, 257, 2237, 411, 24862, 300, 436, 362, 257, 2603, 49317, 1179, 294, 49382, 50864], "temperature": 0.0, "avg_logprob": -0.08092719447003664, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.1008421778678894}, {"id": 2043, "seek": 540824, "start": 5418.24, "end": 5420.24, "text": " audio visual technologies.", "tokens": [50864, 6278, 5056, 7943, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08092719447003664, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.1008421778678894}, {"id": 2044, "seek": 540824, "start": 5420.24, "end": 5424.24, "text": " They might, you know, soon put out something where, you know, you put in a text description", "tokens": [50964, 814, 1062, 11, 291, 458, 11, 2321, 829, 484, 746, 689, 11, 291, 458, 11, 291, 829, 294, 257, 2487, 3855, 51164], "temperature": 0.0, "avg_logprob": -0.08092719447003664, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.1008421778678894}, {"id": 2045, "seek": 540824, "start": 5424.24, "end": 5426.24, "text": " and it'll give you like a three second clip, right?", "tokens": [51164, 293, 309, 603, 976, 291, 411, 257, 1045, 1150, 7353, 11, 558, 30, 51264], "temperature": 0.0, "avg_logprob": -0.08092719447003664, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.1008421778678894}, {"id": 2046, "seek": 540824, "start": 5426.24, "end": 5428.24, "text": " So that you can use that for ad copy.", "tokens": [51264, 407, 300, 291, 393, 764, 300, 337, 614, 5055, 13, 51364], "temperature": 0.0, "avg_logprob": -0.08092719447003664, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.1008421778678894}, {"id": 2047, "seek": 540824, "start": 5428.24, "end": 5431.24, "text": " Well, this technology is going to continue improving over time.", "tokens": [51364, 1042, 11, 341, 2899, 307, 516, 281, 2354, 11470, 670, 565, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08092719447003664, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.1008421778678894}, {"id": 2048, "seek": 540824, "start": 5431.24, "end": 5436.24, "text": " So I kind of, I kind of see that as like, if I were Netflix, put it this way.", "tokens": [51514, 407, 286, 733, 295, 11, 286, 733, 295, 536, 300, 382, 411, 11, 498, 286, 645, 12778, 11, 829, 309, 341, 636, 13, 51764], "temperature": 0.0, "avg_logprob": -0.08092719447003664, "compression_ratio": 1.683168316831683, "no_speech_prob": 0.1008421778678894}, {"id": 2049, "seek": 543624, "start": 5436.24, "end": 5442.24, "text": " If I had the budget of Netflix or Amazon, I would be investing in this to, to, to write", "tokens": [50364, 759, 286, 632, 264, 4706, 295, 12778, 420, 6795, 11, 286, 576, 312, 10978, 294, 341, 281, 11, 281, 11, 281, 2464, 50664], "temperature": 0.0, "avg_logprob": -0.07873406982421875, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.07806040346622467}, {"id": 2050, "seek": 543624, "start": 5442.24, "end": 5448.24, "text": " hyper personalized, um, video, uh, like series or, uh, or novels, right?", "tokens": [50664, 9848, 28415, 11, 1105, 11, 960, 11, 2232, 11, 411, 2638, 420, 11, 2232, 11, 420, 24574, 11, 558, 30, 50964], "temperature": 0.0, "avg_logprob": -0.07873406982421875, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.07806040346622467}, {"id": 2051, "seek": 543624, "start": 5448.24, "end": 5451.24, "text": " Cause you know, Amazon's got the market cornered with Kindle, right?", "tokens": [50964, 10865, 291, 458, 11, 6795, 311, 658, 264, 2142, 4538, 292, 365, 9242, 306, 11, 558, 30, 51114], "temperature": 0.0, "avg_logprob": -0.07873406982421875, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.07806040346622467}, {"id": 2052, "seek": 543624, "start": 5451.24, "end": 5455.24, "text": " And there's people that will read all day, every day, right?", "tokens": [51114, 400, 456, 311, 561, 300, 486, 1401, 439, 786, 11, 633, 786, 11, 558, 30, 51314], "temperature": 0.0, "avg_logprob": -0.07873406982421875, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.07806040346622467}, {"id": 2053, "seek": 543624, "start": 5455.24, "end": 5460.24, "text": " There are people that consume every bit of like entertainment that's available.", "tokens": [51314, 821, 366, 561, 300, 14732, 633, 857, 295, 411, 12393, 300, 311, 2435, 13, 51564], "temperature": 0.0, "avg_logprob": -0.07873406982421875, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.07806040346622467}, {"id": 2054, "seek": 543624, "start": 5460.24, "end": 5465.24, "text": " So if you can generate that on the fly without, you know, having a studio, a big budget studio,", "tokens": [51564, 407, 498, 291, 393, 8460, 300, 322, 264, 3603, 1553, 11, 291, 458, 11, 1419, 257, 6811, 11, 257, 955, 4706, 6811, 11, 51814], "temperature": 0.0, "avg_logprob": -0.07873406982421875, "compression_ratio": 1.6823104693140793, "no_speech_prob": 0.07806040346622467}, {"id": 2055, "seek": 546524, "start": 5465.24, "end": 5468.24, "text": " that would be, I mean, that would change entertainment.", "tokens": [50364, 300, 576, 312, 11, 286, 914, 11, 300, 576, 1319, 12393, 13, 50514], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2056, "seek": 546524, "start": 5468.24, "end": 5470.24, "text": " You know, that, that's the metaverse.", "tokens": [50514, 509, 458, 11, 300, 11, 300, 311, 264, 19616, 4308, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2057, "seek": 546524, "start": 5470.24, "end": 5471.24, "text": " Forget what Facebook is doing.", "tokens": [50614, 18675, 437, 4384, 307, 884, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2058, "seek": 546524, "start": 5471.24, "end": 5475.24, "text": " That's the metaverse where it's like, Hey, you know, I, I came up with my own idea for", "tokens": [50664, 663, 311, 264, 19616, 4308, 689, 309, 311, 411, 11, 1911, 11, 291, 458, 11, 286, 11, 286, 1361, 493, 365, 452, 1065, 1558, 337, 50864], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2059, "seek": 546524, "start": 5475.24, "end": 5480.24, "text": " Game of Thrones and I, I wrote, you know, I use this, uh, you know, GPT eight or whatever", "tokens": [50864, 7522, 295, 31659, 293, 286, 11, 286, 4114, 11, 291, 458, 11, 286, 764, 341, 11, 2232, 11, 291, 458, 11, 26039, 51, 3180, 420, 2035, 51114], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2060, "seek": 546524, "start": 5480.24, "end": 5482.24, "text": " to generate my own version of Game of Thrones.", "tokens": [51114, 281, 8460, 452, 1065, 3037, 295, 7522, 295, 31659, 13, 51214], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2061, "seek": 546524, "start": 5482.24, "end": 5483.24, "text": " Come watch it with me guys.", "tokens": [51214, 2492, 1159, 309, 365, 385, 1074, 13, 51264], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2062, "seek": 546524, "start": 5483.24, "end": 5486.24, "text": " And you know, someone might say, Oh, I didn't like that ending.", "tokens": [51264, 400, 291, 458, 11, 1580, 1062, 584, 11, 876, 11, 286, 994, 380, 411, 300, 8121, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2063, "seek": 546524, "start": 5486.24, "end": 5488.24, "text": " And they go rewrite it and generate their own version.", "tokens": [51414, 400, 436, 352, 28132, 309, 293, 8460, 641, 1065, 3037, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2064, "seek": 546524, "start": 5488.24, "end": 5490.24, "text": " You know, cause we share memes on the internet today.", "tokens": [51514, 509, 458, 11, 3082, 321, 2073, 29730, 322, 264, 4705, 965, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2065, "seek": 546524, "start": 5490.24, "end": 5494.24, "text": " What if instead of sharing memes on the internet, we end up sharing episodes of our favorite", "tokens": [51614, 708, 498, 2602, 295, 5414, 29730, 322, 264, 4705, 11, 321, 917, 493, 5414, 9313, 295, 527, 2954, 51814], "temperature": 0.0, "avg_logprob": -0.08386716363150314, "compression_ratio": 1.9221556886227544, "no_speech_prob": 0.13653664290905}, {"id": 2066, "seek": 549424, "start": 5494.24, "end": 5500.24, "text": " anime or, you know, we, we, uh, Resurrect Battlestar Galactica, you know, whatever.", "tokens": [50364, 12435, 420, 11, 291, 458, 11, 321, 11, 321, 11, 2232, 11, 5015, 374, 2554, 29439, 35745, 289, 7336, 578, 2262, 11, 291, 458, 11, 2035, 13, 50664], "temperature": 0.0, "avg_logprob": -0.12232675552368164, "compression_ratio": 1.5551181102362204, "no_speech_prob": 0.008060427382588387}, {"id": 2067, "seek": 549424, "start": 5500.24, "end": 5502.24, "text": " There's so many things that we could do.", "tokens": [50664, 821, 311, 370, 867, 721, 300, 321, 727, 360, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12232675552368164, "compression_ratio": 1.5551181102362204, "no_speech_prob": 0.008060427382588387}, {"id": 2068, "seek": 549424, "start": 5502.24, "end": 5507.24, "text": " Like if compute power was not a problem, then we get there, but we need like fusion", "tokens": [50764, 1743, 498, 14722, 1347, 390, 406, 257, 1154, 11, 550, 321, 483, 456, 11, 457, 321, 643, 411, 23100, 51014], "temperature": 0.0, "avg_logprob": -0.12232675552368164, "compression_ratio": 1.5551181102362204, "no_speech_prob": 0.008060427382588387}, {"id": 2069, "seek": 549424, "start": 5507.24, "end": 5509.24, "text": " reactors to power this stuff.", "tokens": [51014, 41649, 281, 1347, 341, 1507, 13, 51114], "temperature": 0.0, "avg_logprob": -0.12232675552368164, "compression_ratio": 1.5551181102362204, "no_speech_prob": 0.008060427382588387}, {"id": 2070, "seek": 549424, "start": 5509.24, "end": 5510.24, "text": " Yeah.", "tokens": [51114, 865, 13, 51164], "temperature": 0.0, "avg_logprob": -0.12232675552368164, "compression_ratio": 1.5551181102362204, "no_speech_prob": 0.008060427382588387}, {"id": 2071, "seek": 549424, "start": 5510.24, "end": 5511.24, "text": " Yeah.", "tokens": [51164, 865, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12232675552368164, "compression_ratio": 1.5551181102362204, "no_speech_prob": 0.008060427382588387}, {"id": 2072, "seek": 549424, "start": 5511.24, "end": 5516.24, "text": " And like Marvel for me is already kind of like this and my capacity to consume Marvel", "tokens": [51214, 400, 411, 13837, 337, 385, 307, 1217, 733, 295, 411, 341, 293, 452, 6042, 281, 14732, 13837, 51464], "temperature": 0.0, "avg_logprob": -0.12232675552368164, "compression_ratio": 1.5551181102362204, "no_speech_prob": 0.008060427382588387}, {"id": 2073, "seek": 549424, "start": 5516.24, "end": 5519.24, "text": " as a, as a viewer, it appears is infinite.", "tokens": [51464, 382, 257, 11, 382, 257, 16767, 11, 309, 7038, 307, 13785, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12232675552368164, "compression_ratio": 1.5551181102362204, "no_speech_prob": 0.008060427382588387}, {"id": 2074, "seek": 549424, "start": 5519.24, "end": 5520.24, "text": " So I'm excited.", "tokens": [51614, 407, 286, 478, 2919, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12232675552368164, "compression_ratio": 1.5551181102362204, "no_speech_prob": 0.008060427382588387}, {"id": 2075, "seek": 552024, "start": 5520.24, "end": 5524.24, "text": " I've called it in the past, like the multimodal Marvel cinematic universe.", "tokens": [50364, 286, 600, 1219, 309, 294, 264, 1791, 11, 411, 264, 32972, 378, 304, 13837, 32250, 6445, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2076, "seek": 552024, "start": 5524.24, "end": 5525.24, "text": " Yeah.", "tokens": [50564, 865, 13, 50614], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2077, "seek": 552024, "start": 5525.24, "end": 5530.24, "text": " And I, some of these shows like Loki, I don't know if you, if you watch like,", "tokens": [50614, 400, 286, 11, 512, 295, 613, 3110, 411, 37940, 11, 286, 500, 380, 458, 498, 291, 11, 498, 291, 1159, 411, 11, 50864], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2078, "seek": 552024, "start": 5530.24, "end": 5531.24, "text": " I haven't seen it yet.", "tokens": [50864, 286, 2378, 380, 1612, 309, 1939, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2079, "seek": 552024, "start": 5531.24, "end": 5532.24, "text": " Okay.", "tokens": [50914, 1033, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2080, "seek": 552024, "start": 5532.24, "end": 5533.24, "text": " Okay.", "tokens": [50964, 1033, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2081, "seek": 552024, "start": 5533.24, "end": 5534.24, "text": " I mean, it was six episodes.", "tokens": [51014, 286, 914, 11, 309, 390, 2309, 9313, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2082, "seek": 552024, "start": 5534.24, "end": 5538.24, "text": " If the, if it had been 30, I would have watched all 30 and enjoyed every moment of it.", "tokens": [51064, 759, 264, 11, 498, 309, 632, 668, 2217, 11, 286, 576, 362, 6337, 439, 2217, 293, 4626, 633, 1623, 295, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2083, "seek": 552024, "start": 5538.24, "end": 5541.24, "text": " If that quality was, I want to go deeper in these stories.", "tokens": [51264, 759, 300, 3125, 390, 11, 286, 528, 281, 352, 7731, 294, 613, 3676, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2084, "seek": 552024, "start": 5541.24, "end": 5546.24, "text": " So I'm definitely excited for all my favorite universes, cinematic universes and", "tokens": [51414, 407, 286, 478, 2138, 2919, 337, 439, 452, 2954, 50168, 11, 32250, 50168, 293, 51664], "temperature": 0.0, "avg_logprob": -0.12601648677479138, "compression_ratio": 1.650735294117647, "no_speech_prob": 0.23353153467178345}, {"id": 2085, "seek": 554624, "start": 5547.24, "end": 5552.24, "text": " story wise as well to, to live on forever essentially through multimodal content and", "tokens": [50414, 1657, 10829, 382, 731, 281, 11, 281, 1621, 322, 5680, 4476, 807, 32972, 378, 304, 2701, 293, 50664], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2086, "seek": 554624, "start": 5552.24, "end": 5555.24, "text": " maybe be personalized like you're describing as well.", "tokens": [50664, 1310, 312, 28415, 411, 291, 434, 16141, 382, 731, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2087, "seek": 554624, "start": 5555.24, "end": 5556.24, "text": " Oh yeah.", "tokens": [50814, 876, 1338, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2088, "seek": 554624, "start": 5556.24, "end": 5557.24, "text": " So yeah.", "tokens": [50864, 407, 1338, 13, 50914], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2089, "seek": 554624, "start": 5557.24, "end": 5558.24, "text": " Last question.", "tokens": [50914, 5264, 1168, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2090, "seek": 554624, "start": 5558.24, "end": 5561.24, "text": " So we, you know, you know, we've talked about various things.", "tokens": [50964, 407, 321, 11, 291, 458, 11, 291, 458, 11, 321, 600, 2825, 466, 3683, 721, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2091, "seek": 554624, "start": 5561.24, "end": 5567.24, "text": " We've talked about codex by today, you know, multimodal stuff broadly.", "tokens": [51114, 492, 600, 2825, 466, 3089, 87, 538, 965, 11, 291, 458, 11, 32972, 378, 304, 1507, 19511, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2092, "seek": 554624, "start": 5567.24, "end": 5569.24, "text": " Where do you see all of this stuff going?", "tokens": [51414, 2305, 360, 291, 536, 439, 295, 341, 1507, 516, 30, 51514], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2093, "seek": 554624, "start": 5569.24, "end": 5571.24, "text": " Let's give a timeline, five, 10 years.", "tokens": [51514, 961, 311, 976, 257, 12933, 11, 1732, 11, 1266, 924, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2094, "seek": 554624, "start": 5571.24, "end": 5574.24, "text": " What are some of the, what's the direction we're heading towards?", "tokens": [51614, 708, 366, 512, 295, 264, 11, 437, 311, 264, 3513, 321, 434, 9864, 3030, 30, 51764], "temperature": 0.0, "avg_logprob": -0.16243672370910645, "compression_ratio": 1.6642066420664208, "no_speech_prob": 0.18457147479057312}, {"id": 2095, "seek": 557424, "start": 5574.24, "end": 5576.24, "text": " What, what important capabilities will we have?", "tokens": [50364, 708, 11, 437, 1021, 10862, 486, 321, 362, 30, 50464], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2096, "seek": 557424, "start": 5576.24, "end": 5578.24, "text": " Why is this stuff important?", "tokens": [50464, 1545, 307, 341, 1507, 1021, 30, 50564], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2097, "seek": 557424, "start": 5578.24, "end": 5579.24, "text": " Yeah.", "tokens": [50564, 865, 13, 50614], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2098, "seek": 557424, "start": 5579.24, "end": 5583.24, "text": " Um, five to 10 years from now, I think that we will have something that you could probably", "tokens": [50614, 3301, 11, 1732, 281, 1266, 924, 490, 586, 11, 286, 519, 300, 321, 486, 362, 746, 300, 291, 727, 1391, 50814], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2099, "seek": 557424, "start": 5583.24, "end": 5588.24, "text": " call a fully functional AGI like as a service you could sign up for.", "tokens": [50814, 818, 257, 4498, 11745, 316, 26252, 411, 382, 257, 2643, 291, 727, 1465, 493, 337, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2100, "seek": 557424, "start": 5588.24, "end": 5592.24, "text": " Um, you know, it might be chatbot based, you know, kind of based on natural language,", "tokens": [51064, 3301, 11, 291, 458, 11, 309, 1062, 312, 5081, 18870, 2361, 11, 291, 458, 11, 733, 295, 2361, 322, 3303, 2856, 11, 51264], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2101, "seek": 557424, "start": 5592.24, "end": 5593.24, "text": " cognitive architecture.", "tokens": [51264, 15605, 9482, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2102, "seek": 557424, "start": 5593.24, "end": 5596.24, "text": " Um, I calculated out like it's too expensive to run right now.", "tokens": [51314, 3301, 11, 286, 15598, 484, 411, 309, 311, 886, 5124, 281, 1190, 558, 586, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2103, "seek": 557424, "start": 5596.24, "end": 5601.24, "text": " You know, if it's $30 for a, for a 10 minute conversation, that's way too expensive.", "tokens": [51464, 509, 458, 11, 498, 309, 311, 1848, 3446, 337, 257, 11, 337, 257, 1266, 3456, 3761, 11, 300, 311, 636, 886, 5124, 13, 51714], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2104, "seek": 557424, "start": 5601.24, "end": 5603.24, "text": " So the, the, the cost has to come down.", "tokens": [51714, 407, 264, 11, 264, 11, 264, 2063, 575, 281, 808, 760, 13, 51814], "temperature": 0.0, "avg_logprob": -0.08056336085001628, "compression_ratio": 1.6927899686520376, "no_speech_prob": 0.003593157045543194}, {"id": 2105, "seek": 560324, "start": 5603.24, "end": 5607.24, "text": " You know, if you just, if you just take the technology we have today, but make it cheaper,", "tokens": [50364, 509, 458, 11, 498, 291, 445, 11, 498, 291, 445, 747, 264, 2899, 321, 362, 965, 11, 457, 652, 309, 12284, 11, 50564], "temperature": 0.0, "avg_logprob": -0.08215175087996356, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.026744913309812546}, {"id": 2106, "seek": 560324, "start": 5607.24, "end": 5609.24, "text": " there's so much potential.", "tokens": [50564, 456, 311, 370, 709, 3995, 13, 50664], "temperature": 0.0, "avg_logprob": -0.08215175087996356, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.026744913309812546}, {"id": 2107, "seek": 560324, "start": 5609.24, "end": 5613.24, "text": " Um, so, you know, then there was that idea about like self-improving, um, you know,", "tokens": [50664, 3301, 11, 370, 11, 291, 458, 11, 550, 456, 390, 300, 1558, 466, 411, 2698, 12, 332, 4318, 798, 11, 1105, 11, 291, 458, 11, 50864], "temperature": 0.0, "avg_logprob": -0.08215175087996356, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.026744913309812546}, {"id": 2108, "seek": 560324, "start": 5613.24, "end": 5615.24, "text": " feedback loops, you know, integrating with DevOps.", "tokens": [50864, 5824, 16121, 11, 291, 458, 11, 26889, 365, 43051, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08215175087996356, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.026744913309812546}, {"id": 2109, "seek": 560324, "start": 5615.24, "end": 5620.24, "text": " I certainly think that a company like Atlassian, which is a major DevOps player,", "tokens": [50964, 286, 3297, 519, 300, 257, 2237, 411, 11000, 640, 952, 11, 597, 307, 257, 2563, 43051, 4256, 11, 51214], "temperature": 0.0, "avg_logprob": -0.08215175087996356, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.026744913309812546}, {"id": 2110, "seek": 560324, "start": 5620.24, "end": 5624.24, "text": " um, probably within five to 10 years, they'll have something, um, integrated to,", "tokens": [51214, 1105, 11, 1391, 1951, 1732, 281, 1266, 924, 11, 436, 603, 362, 746, 11, 1105, 11, 10919, 281, 11, 51414], "temperature": 0.0, "avg_logprob": -0.08215175087996356, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.026744913309812546}, {"id": 2111, "seek": 560324, "start": 5624.24, "end": 5628.24, "text": " to kind of help automate the development pipeline even further.", "tokens": [51414, 281, 733, 295, 854, 31605, 264, 3250, 15517, 754, 3052, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08215175087996356, "compression_ratio": 1.6482758620689655, "no_speech_prob": 0.026744913309812546}, {"id": 2112, "seek": 562824, "start": 5628.24, "end": 5633.24, "text": " Um, I think that, of course, I could be wrong because we're kind of at this weird", "tokens": [50364, 3301, 11, 286, 519, 300, 11, 295, 1164, 11, 286, 727, 312, 2085, 570, 321, 434, 733, 295, 412, 341, 3657, 50614], "temperature": 0.0, "avg_logprob": -0.13220480557145745, "compression_ratio": 1.7353951890034365, "no_speech_prob": 0.10084394365549088}, {"id": 2113, "seek": 562824, "start": 5633.24, "end": 5634.24, "text": " acceleration point.", "tokens": [50614, 17162, 935, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13220480557145745, "compression_ratio": 1.7353951890034365, "no_speech_prob": 0.10084394365549088}, {"id": 2114, "seek": 562824, "start": 5634.24, "end": 5640.24, "text": " Um, I think, I feel like multimodal models like consumer grade multimodal models are", "tokens": [50664, 3301, 11, 286, 519, 11, 286, 841, 411, 32972, 378, 304, 5245, 411, 9711, 7204, 32972, 378, 304, 5245, 366, 50964], "temperature": 0.0, "avg_logprob": -0.13220480557145745, "compression_ratio": 1.7353951890034365, "no_speech_prob": 0.10084394365549088}, {"id": 2115, "seek": 562824, "start": 5640.24, "end": 5642.24, "text": " probably more than 10 years away.", "tokens": [50964, 1391, 544, 813, 1266, 924, 1314, 13, 51064], "temperature": 0.0, "avg_logprob": -0.13220480557145745, "compression_ratio": 1.7353951890034365, "no_speech_prob": 0.10084394365549088}, {"id": 2116, "seek": 562824, "start": 5642.24, "end": 5646.24, "text": " Um, unfortunately, they're probably just going to be like toy sized because,", "tokens": [51064, 3301, 11, 7015, 11, 436, 434, 1391, 445, 516, 281, 312, 411, 12058, 20004, 570, 11, 51264], "temperature": 0.0, "avg_logprob": -0.13220480557145745, "compression_ratio": 1.7353951890034365, "no_speech_prob": 0.10084394365549088}, {"id": 2117, "seek": 562824, "start": 5646.24, "end": 5650.24, "text": " you know, there's, um, there's like a, a, a hypnogram, right?", "tokens": [51264, 291, 458, 11, 456, 311, 11, 1105, 11, 456, 311, 411, 257, 11, 257, 11, 257, 7420, 77, 12820, 11, 558, 30, 51464], "temperature": 0.0, "avg_logprob": -0.13220480557145745, "compression_ratio": 1.7353951890034365, "no_speech_prob": 0.10084394365549088}, {"id": 2118, "seek": 562824, "start": 5650.24, "end": 5652.24, "text": " I don't know if you've seen that one, but that's one of like the text to image", "tokens": [51464, 286, 500, 380, 458, 498, 291, 600, 1612, 300, 472, 11, 457, 300, 311, 472, 295, 411, 264, 2487, 281, 3256, 51564], "temperature": 0.0, "avg_logprob": -0.13220480557145745, "compression_ratio": 1.7353951890034365, "no_speech_prob": 0.10084394365549088}, {"id": 2119, "seek": 562824, "start": 5652.24, "end": 5656.24, "text": " generators and they're, it's still not even photorealistic, right?", "tokens": [51564, 38662, 293, 436, 434, 11, 309, 311, 920, 406, 754, 2409, 418, 304, 3142, 11, 558, 30, 51764], "temperature": 0.0, "avg_logprob": -0.13220480557145745, "compression_ratio": 1.7353951890034365, "no_speech_prob": 0.10084394365549088}, {"id": 2120, "seek": 565624, "start": 5656.24, "end": 5661.24, "text": " Getting a photorealistic text to image is still like, that's a little ways off.", "tokens": [50364, 13674, 257, 2409, 418, 304, 3142, 2487, 281, 3256, 307, 920, 411, 11, 300, 311, 257, 707, 2098, 766, 13, 50614], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2121, "seek": 565624, "start": 5661.24, "end": 5664.24, "text": " And then the next step after that is text to video.", "tokens": [50614, 400, 550, 264, 958, 1823, 934, 300, 307, 2487, 281, 960, 13, 50764], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2122, "seek": 565624, "start": 5664.24, "end": 5666.24, "text": " That's even further, right?", "tokens": [50764, 663, 311, 754, 3052, 11, 558, 30, 50864], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2123, "seek": 565624, "start": 5666.24, "end": 5668.24, "text": " So that's, that's kind of where I think it's at.", "tokens": [50864, 407, 300, 311, 11, 300, 311, 733, 295, 689, 286, 519, 309, 311, 412, 13, 50964], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2124, "seek": 565624, "start": 5668.24, "end": 5670.24, "text": " I don't think we're going to hit an AI winter.", "tokens": [50964, 286, 500, 380, 519, 321, 434, 516, 281, 2045, 364, 7318, 6355, 13, 51064], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2125, "seek": 565624, "start": 5670.24, "end": 5673.24, "text": " I know there's lots of people predicting that we're going to hit an AI winter,", "tokens": [51064, 286, 458, 456, 311, 3195, 295, 561, 32884, 300, 321, 434, 516, 281, 2045, 364, 7318, 6355, 11, 51214], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2126, "seek": 565624, "start": 5673.24, "end": 5677.24, "text": " but I think that we're actually still kind of in the acceleration point.", "tokens": [51214, 457, 286, 519, 300, 321, 434, 767, 920, 733, 295, 294, 264, 17162, 935, 13, 51414], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2127, "seek": 565624, "start": 5677.24, "end": 5680.24, "text": " But again, I don't know if it's going to follow an exponential curve forever", "tokens": [51414, 583, 797, 11, 286, 500, 380, 458, 498, 309, 311, 516, 281, 1524, 364, 21510, 7605, 5680, 51564], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2128, "seek": 565624, "start": 5680.24, "end": 5682.24, "text": " or if it's a sigmoid curve.", "tokens": [51564, 420, 498, 309, 311, 257, 4556, 3280, 327, 7605, 13, 51664], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2129, "seek": 565624, "start": 5682.24, "end": 5685.24, "text": " So time will tell.", "tokens": [51664, 407, 565, 486, 980, 13, 51814], "temperature": 0.0, "avg_logprob": -0.05398777553013393, "compression_ratio": 1.8373702422145328, "no_speech_prob": 0.007814152166247368}, {"id": 2130, "seek": 568524, "start": 5685.24, "end": 5686.24, "text": " Yep.", "tokens": [50364, 7010, 13, 50414], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2131, "seek": 568524, "start": 5686.24, "end": 5689.24, "text": " And still lots to do in the meantime, like you're describing, even with UPT3.", "tokens": [50414, 400, 920, 3195, 281, 360, 294, 264, 14991, 11, 411, 291, 434, 16141, 11, 754, 365, 20074, 51, 18, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2132, "seek": 568524, "start": 5689.24, "end": 5690.24, "text": " Oh yeah.", "tokens": [50564, 876, 1338, 13, 50614], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2133, "seek": 568524, "start": 5690.24, "end": 5691.24, "text": " Okay.", "tokens": [50614, 1033, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2134, "seek": 568524, "start": 5691.24, "end": 5692.24, "text": " Yeah.", "tokens": [50664, 865, 13, 50714], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2135, "seek": 568524, "start": 5692.24, "end": 5695.24, "text": " My, my answer is, I think all of this stuff is, is just converging to just", "tokens": [50714, 1222, 11, 452, 1867, 307, 11, 286, 519, 439, 295, 341, 1507, 307, 11, 307, 445, 9652, 3249, 281, 445, 50864], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2136, "seek": 568524, "start": 5695.24, "end": 5697.24, "text": " greater human potential.", "tokens": [50864, 5044, 1952, 3995, 13, 50964], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2137, "seek": 568524, "start": 5697.24, "end": 5700.24, "text": " In some sense, I'm not even necessarily interested in the AGI question,", "tokens": [50964, 682, 512, 2020, 11, 286, 478, 406, 754, 4725, 3102, 294, 264, 316, 26252, 1168, 11, 51114], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2138, "seek": 568524, "start": 5700.24, "end": 5702.24, "text": " although I think it's important.", "tokens": [51114, 4878, 286, 519, 309, 311, 1021, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2139, "seek": 568524, "start": 5702.24, "end": 5706.24, "text": " I think just the, the exciting possibilities we'll have, even now that we have,", "tokens": [51214, 286, 519, 445, 264, 11, 264, 4670, 12178, 321, 603, 362, 11, 754, 586, 300, 321, 362, 11, 51414], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2140, "seek": 568524, "start": 5706.24, "end": 5709.24, "text": " that we'll continue to have five to 10 years from now.", "tokens": [51414, 300, 321, 603, 2354, 281, 362, 1732, 281, 1266, 924, 490, 586, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2141, "seek": 568524, "start": 5709.24, "end": 5713.24, "text": " Um, so many more experiences, so many other things we'll be able to create", "tokens": [51564, 3301, 11, 370, 867, 544, 5235, 11, 370, 867, 661, 721, 321, 603, 312, 1075, 281, 1884, 51764], "temperature": 0.0, "avg_logprob": -0.14345200856526694, "compression_ratio": 1.639240506329114, "no_speech_prob": 0.002630766946822405}, {"id": 2142, "seek": 571324, "start": 5713.24, "end": 5714.24, "text": " that weren't possible.", "tokens": [50364, 300, 4999, 380, 1944, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2143, "seek": 571324, "start": 5714.24, "end": 5716.24, "text": " I think we'll have more people creating than ever before.", "tokens": [50414, 286, 519, 321, 603, 362, 544, 561, 4084, 813, 1562, 949, 13, 50514], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2144, "seek": 571324, "start": 5716.24, "end": 5720.24, "text": " Um, it's a really, really exciting vision for humanity.", "tokens": [50514, 3301, 11, 309, 311, 257, 534, 11, 534, 4670, 5201, 337, 10243, 13, 50714], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2145, "seek": 571324, "start": 5720.24, "end": 5721.24, "text": " Right.", "tokens": [50714, 1779, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2146, "seek": 571324, "start": 5721.24, "end": 5722.24, "text": " Not just, not just for you and I.", "tokens": [50764, 1726, 445, 11, 406, 445, 337, 291, 293, 286, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2147, "seek": 571324, "start": 5722.24, "end": 5727.24, "text": " So anyways, so with that said, did you have anything you wanted to plug David?", "tokens": [50814, 407, 13448, 11, 370, 365, 300, 848, 11, 630, 291, 362, 1340, 291, 1415, 281, 5452, 4389, 30, 51064], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2148, "seek": 571324, "start": 5727.24, "end": 5729.24, "text": " Where can people find you online?", "tokens": [51064, 2305, 393, 561, 915, 291, 2950, 30, 51164], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2149, "seek": 571324, "start": 5729.24, "end": 5730.24, "text": " Yeah.", "tokens": [51164, 865, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2150, "seek": 571324, "start": 5730.24, "end": 5734.24, "text": " So, um, my personal site is a David K Shapiro dot com.", "tokens": [51214, 407, 11, 1105, 11, 452, 2973, 3621, 307, 257, 4389, 591, 44160, 5182, 5893, 395, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2151, "seek": 571324, "start": 5734.24, "end": 5738.24, "text": " Um, and I've got, um, I have, I have a few projects up and coming.", "tokens": [51414, 3301, 11, 293, 286, 600, 658, 11, 1105, 11, 286, 362, 11, 286, 362, 257, 1326, 4455, 493, 293, 1348, 13, 51614], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2152, "seek": 571324, "start": 5738.24, "end": 5741.24, "text": " Um, nothing out right now except for my book,", "tokens": [51614, 3301, 11, 1825, 484, 558, 586, 3993, 337, 452, 1446, 11, 51764], "temperature": 0.0, "avg_logprob": -0.12413127709787788, "compression_ratio": 1.6167247386759582, "no_speech_prob": 0.01911822520196438}, {"id": 2153, "seek": 574124, "start": 5741.24, "end": 5743.24, "text": " natural language, cognitive architecture.", "tokens": [50364, 3303, 2856, 11, 15605, 9482, 13, 50464], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2154, "seek": 574124, "start": 5743.24, "end": 5746.24, "text": " Um, you can download it for free from my website.", "tokens": [50464, 3301, 11, 291, 393, 5484, 309, 337, 1737, 490, 452, 3144, 13, 50614], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2155, "seek": 574124, "start": 5746.24, "end": 5748.24, "text": " Um, you can sign up for my newsletter.", "tokens": [50614, 3301, 11, 291, 393, 1465, 493, 337, 452, 26469, 13, 50714], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2156, "seek": 574124, "start": 5748.24, "end": 5751.24, "text": " So one of my upcoming books is called benevolent by design,", "tokens": [50714, 407, 472, 295, 452, 11500, 3642, 307, 1219, 48567, 317, 538, 1715, 11, 50864], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2157, "seek": 574124, "start": 5751.24, "end": 5753.24, "text": " six words to safeguard humanity,", "tokens": [50864, 2309, 2283, 281, 40153, 10243, 11, 50964], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2158, "seek": 574124, "start": 5753.24, "end": 5756.24, "text": " which is to address the control problem of AGI.", "tokens": [50964, 597, 307, 281, 2985, 264, 1969, 1154, 295, 316, 26252, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2159, "seek": 574124, "start": 5756.24, "end": 5761.24, "text": " Um, so that's, that's, uh, that book should hopefully be out in the next six months or so.", "tokens": [51114, 3301, 11, 370, 300, 311, 11, 300, 311, 11, 2232, 11, 300, 1446, 820, 4696, 312, 484, 294, 264, 958, 2309, 2493, 420, 370, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2160, "seek": 574124, "start": 5761.24, "end": 5764.24, "text": " Um, and that is, um, so that's one project.", "tokens": [51364, 3301, 11, 293, 300, 307, 11, 1105, 11, 370, 300, 311, 472, 1716, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2161, "seek": 574124, "start": 5764.24, "end": 5766.24, "text": " I've got another nonfiction book.", "tokens": [51514, 286, 600, 658, 1071, 2107, 32041, 1446, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2162, "seek": 574124, "start": 5766.24, "end": 5769.24, "text": " Um, and then also my own podcast will be coming out soon.", "tokens": [51614, 3301, 11, 293, 550, 611, 452, 1065, 7367, 486, 312, 1348, 484, 2321, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07019750446292526, "compression_ratio": 1.6767676767676767, "no_speech_prob": 0.024411916732788086}, {"id": 2163, "seek": 576924, "start": 5769.24, "end": 5770.24, "text": " Yeah.", "tokens": [50364, 865, 13, 50414], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2164, "seek": 576924, "start": 5770.24, "end": 5773.24, "text": " Head over to my site, David K Shapiro dot com and sign up for my newsletter.", "tokens": [50414, 11398, 670, 281, 452, 3621, 11, 4389, 591, 44160, 5182, 5893, 395, 293, 1465, 493, 337, 452, 26469, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2165, "seek": 576924, "start": 5773.24, "end": 5777.24, "text": " And you'll get, you'll get updated when these, when these come out, when they're available.", "tokens": [50564, 400, 291, 603, 483, 11, 291, 603, 483, 10588, 562, 613, 11, 562, 613, 808, 484, 11, 562, 436, 434, 2435, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2166, "seek": 576924, "start": 5777.24, "end": 5778.24, "text": " Awesome.", "tokens": [50764, 10391, 13, 50814], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2167, "seek": 576924, "start": 5778.24, "end": 5781.24, "text": " And David, you mentioned you're, you're looking for collaborators as well.", "tokens": [50814, 400, 4389, 11, 291, 2835, 291, 434, 11, 291, 434, 1237, 337, 39789, 382, 731, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2168, "seek": 576924, "start": 5781.24, "end": 5782.24, "text": " Yes.", "tokens": [50964, 1079, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2169, "seek": 576924, "start": 5782.24, "end": 5783.24, "text": " For natural language, cognitive architecture.", "tokens": [51014, 1171, 3303, 2856, 11, 15605, 9482, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2170, "seek": 576924, "start": 5783.24, "end": 5789.24, "text": " So, uh, if you're a coder, you know, imagine product manager, researcher, uh, hit up David", "tokens": [51064, 407, 11, 2232, 11, 498, 291, 434, 257, 17656, 260, 11, 291, 458, 11, 3811, 1674, 6598, 11, 21751, 11, 2232, 11, 2045, 493, 4389, 51364], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2171, "seek": 576924, "start": 5789.24, "end": 5792.24, "text": " and just connect if, if any of this stuff interests you.", "tokens": [51364, 293, 445, 1745, 498, 11, 498, 604, 295, 341, 1507, 8847, 291, 13, 51514], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2172, "seek": 576924, "start": 5792.24, "end": 5798.24, "text": " Um, I've, I've, I asked, I spent like a couple, I think I spent like a few days trying to find you on Twitter.", "tokens": [51514, 3301, 11, 286, 600, 11, 286, 600, 11, 286, 2351, 11, 286, 4418, 411, 257, 1916, 11, 286, 519, 286, 4418, 411, 257, 1326, 1708, 1382, 281, 915, 291, 322, 5794, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12368690820387852, "compression_ratio": 1.7108433734939759, "no_speech_prob": 0.03408568724989891}, {"id": 2173, "seek": 579824, "start": 5798.24, "end": 5800.24, "text": " I don't think you're quite on Twitter yet.", "tokens": [50364, 286, 500, 380, 519, 291, 434, 1596, 322, 5794, 1939, 13, 50464], "temperature": 0.0, "avg_logprob": -0.09502835273742676, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.024412449449300766}, {"id": 2174, "seek": 579824, "start": 5800.24, "end": 5807.24, "text": " Uh, I encourage you, uh, David, of course, you know, you and I will connect after we'll put any other, other place people could connect with you.", "tokens": [50464, 4019, 11, 286, 5373, 291, 11, 2232, 11, 4389, 11, 295, 1164, 11, 291, 458, 11, 291, 293, 286, 486, 1745, 934, 321, 603, 829, 604, 661, 11, 661, 1081, 561, 727, 1745, 365, 291, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09502835273742676, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.024412449449300766}, {"id": 2175, "seek": 579824, "start": 5807.24, "end": 5808.24, "text": " There's the community forums.", "tokens": [50814, 821, 311, 264, 1768, 26998, 13, 50864], "temperature": 0.0, "avg_logprob": -0.09502835273742676, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.024412449449300766}, {"id": 2176, "seek": 579824, "start": 5808.24, "end": 5810.24, "text": " I assume you have a GitHub account.", "tokens": [50864, 286, 6552, 291, 362, 257, 23331, 2696, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09502835273742676, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.024412449449300766}, {"id": 2177, "seek": 579824, "start": 5810.24, "end": 5811.24, "text": " Yeah.", "tokens": [50964, 865, 13, 51014], "temperature": 0.0, "avg_logprob": -0.09502835273742676, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.024412449449300766}, {"id": 2178, "seek": 579824, "start": 5811.24, "end": 5814.24, "text": " So we're going to put that in, in the show notes and in the YouTube description below.", "tokens": [51014, 407, 321, 434, 516, 281, 829, 300, 294, 11, 294, 264, 855, 5570, 293, 294, 264, 3088, 3855, 2507, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09502835273742676, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.024412449449300766}, {"id": 2179, "seek": 579824, "start": 5814.24, "end": 5817.24, "text": " Uh, so anyways, David, thank you so much for being here.", "tokens": [51164, 4019, 11, 370, 13448, 11, 4389, 11, 1309, 291, 370, 709, 337, 885, 510, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09502835273742676, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.024412449449300766}, {"id": 2180, "seek": 579824, "start": 5817.24, "end": 5826.24, "text": " I wanted to personally thank you for all the awesome, awesome community contributions you've made on the open eye community forum.", "tokens": [51314, 286, 1415, 281, 5665, 1309, 291, 337, 439, 264, 3476, 11, 3476, 1768, 15725, 291, 600, 1027, 322, 264, 1269, 3313, 1768, 17542, 13, 51764], "temperature": 0.0, "avg_logprob": -0.09502835273742676, "compression_ratio": 1.7258064516129032, "no_speech_prob": 0.024412449449300766}, {"id": 2181, "seek": 582624, "start": 5826.24, "end": 5829.24, "text": " Uh, you're just an essential person on there.", "tokens": [50364, 4019, 11, 291, 434, 445, 364, 7115, 954, 322, 456, 13, 50514], "temperature": 0.0, "avg_logprob": -0.061098027050046994, "compression_ratio": 1.580327868852459, "no_speech_prob": 0.11908790469169617}, {"id": 2182, "seek": 582624, "start": 5829.24, "end": 5830.24, "text": " I've learned a lot from you.", "tokens": [50514, 286, 600, 3264, 257, 688, 490, 291, 13, 50564], "temperature": 0.0, "avg_logprob": -0.061098027050046994, "compression_ratio": 1.580327868852459, "no_speech_prob": 0.11908790469169617}, {"id": 2183, "seek": 582624, "start": 5830.24, "end": 5834.24, "text": " Uh, you know, the insights you've shared, they're going to be there forever.", "tokens": [50564, 4019, 11, 291, 458, 11, 264, 14310, 291, 600, 5507, 11, 436, 434, 516, 281, 312, 456, 5680, 13, 50764], "temperature": 0.0, "avg_logprob": -0.061098027050046994, "compression_ratio": 1.580327868852459, "no_speech_prob": 0.11908790469169617}, {"id": 2184, "seek": 582624, "start": 5834.24, "end": 5836.24, "text": " And I'm sure I can't imagine how many people you've helped.", "tokens": [50764, 400, 286, 478, 988, 286, 393, 380, 3811, 577, 867, 561, 291, 600, 4254, 13, 50864], "temperature": 0.0, "avg_logprob": -0.061098027050046994, "compression_ratio": 1.580327868852459, "no_speech_prob": 0.11908790469169617}, {"id": 2185, "seek": 582624, "start": 5836.24, "end": 5843.24, "text": " Uh, and also about your book, I also just wanted to say to the audience, uh, David's done a great job making it really digestible.", "tokens": [50864, 4019, 11, 293, 611, 466, 428, 1446, 11, 286, 611, 445, 1415, 281, 584, 281, 264, 4034, 11, 2232, 11, 4389, 311, 1096, 257, 869, 1691, 1455, 309, 534, 13884, 964, 13, 51214], "temperature": 0.0, "avg_logprob": -0.061098027050046994, "compression_ratio": 1.580327868852459, "no_speech_prob": 0.11908790469169617}, {"id": 2186, "seek": 582624, "start": 5843.24, "end": 5847.24, "text": " Like it was a very, it was a breeze of a read.", "tokens": [51214, 1743, 309, 390, 257, 588, 11, 309, 390, 257, 24532, 295, 257, 1401, 13, 51414], "temperature": 0.0, "avg_logprob": -0.061098027050046994, "compression_ratio": 1.580327868852459, "no_speech_prob": 0.11908790469169617}, {"id": 2187, "seek": 582624, "start": 5847.24, "end": 5852.24, "text": " I thoroughly enjoyed it as somebody who writes GPT three prompts and is into this ecosystem.", "tokens": [51414, 286, 17987, 4626, 309, 382, 2618, 567, 13657, 26039, 51, 1045, 41095, 293, 307, 666, 341, 11311, 13, 51664], "temperature": 0.0, "avg_logprob": -0.061098027050046994, "compression_ratio": 1.580327868852459, "no_speech_prob": 0.11908790469169617}, {"id": 2188, "seek": 585224, "start": 5852.24, "end": 5862.24, "text": " It was just, uh, very interesting to see, uh, how it, how it could be laid out, uh, in this broader system approaching this huge problem.", "tokens": [50364, 467, 390, 445, 11, 2232, 11, 588, 1880, 281, 536, 11, 2232, 11, 577, 309, 11, 577, 309, 727, 312, 9897, 484, 11, 2232, 11, 294, 341, 13227, 1185, 14908, 341, 2603, 1154, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07404004311075016, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.3737102746963501}, {"id": 2189, "seek": 585224, "start": 5862.24, "end": 5865.24, "text": " Um, and also I was able to even get the book for free.", "tokens": [50864, 3301, 11, 293, 611, 286, 390, 1075, 281, 754, 483, 264, 1446, 337, 1737, 13, 51014], "temperature": 0.0, "avg_logprob": -0.07404004311075016, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.3737102746963501}, {"id": 2190, "seek": 585224, "start": 5865.24, "end": 5869.24, "text": " Obviously I encourage people by the book support it, but it's, it's there.", "tokens": [51014, 7580, 286, 5373, 561, 538, 264, 1446, 1406, 309, 11, 457, 309, 311, 11, 309, 311, 456, 13, 51214], "temperature": 0.0, "avg_logprob": -0.07404004311075016, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.3737102746963501}, {"id": 2191, "seek": 585224, "start": 5869.24, "end": 5870.24, "text": " It's ready.", "tokens": [51214, 467, 311, 1919, 13, 51264], "temperature": 0.0, "avg_logprob": -0.07404004311075016, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.3737102746963501}, {"id": 2192, "seek": 585224, "start": 5870.24, "end": 5872.24, "text": " I think, you know, David's goal here is to get the ideas out.", "tokens": [51264, 286, 519, 11, 291, 458, 11, 4389, 311, 3387, 510, 307, 281, 483, 264, 3487, 484, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07404004311075016, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.3737102746963501}, {"id": 2193, "seek": 585224, "start": 5872.24, "end": 5877.24, "text": " Um, and so, uh, anyways, so, uh, that's it for today's episode.", "tokens": [51364, 3301, 11, 293, 370, 11, 2232, 11, 13448, 11, 370, 11, 2232, 11, 300, 311, 309, 337, 965, 311, 3500, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07404004311075016, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.3737102746963501}, {"id": 2194, "seek": 585224, "start": 5877.24, "end": 5878.24, "text": " David, thank you so much again.", "tokens": [51614, 4389, 11, 1309, 291, 370, 709, 797, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07404004311075016, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.3737102746963501}, {"id": 2195, "seek": 585224, "start": 5878.24, "end": 5879.24, "text": " I really appreciate you being here.", "tokens": [51664, 286, 534, 4449, 291, 885, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.07404004311075016, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.3737102746963501}, {"id": 2196, "seek": 585224, "start": 5879.24, "end": 5880.24, "text": " Thank you.", "tokens": [51714, 1044, 291, 13, 51764], "temperature": 0.0, "avg_logprob": -0.07404004311075016, "compression_ratio": 1.7102473498233215, "no_speech_prob": 0.3737102746963501}, {"id": 2197, "seek": 588024, "start": 5880.24, "end": 5883.24, "text": " Thank you for all the kind comments and, um, and you're quite welcome.", "tokens": [50364, 1044, 291, 337, 439, 264, 733, 3053, 293, 11, 1105, 11, 293, 291, 434, 1596, 2928, 13, 50514], "temperature": 0.0, "avg_logprob": -0.13681794714739942, "compression_ratio": 1.5830258302583027, "no_speech_prob": 0.2874073088169098}, {"id": 2198, "seek": 588024, "start": 5883.24, "end": 5884.24, "text": " And so is everyone else.", "tokens": [50514, 400, 370, 307, 1518, 1646, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13681794714739942, "compression_ratio": 1.5830258302583027, "no_speech_prob": 0.2874073088169098}, {"id": 2199, "seek": 588024, "start": 5884.24, "end": 5886.24, "text": " That's why I'm here.", "tokens": [50564, 663, 311, 983, 286, 478, 510, 13, 50664], "temperature": 0.0, "avg_logprob": -0.13681794714739942, "compression_ratio": 1.5830258302583027, "no_speech_prob": 0.2874073088169098}, {"id": 2200, "seek": 588024, "start": 5886.24, "end": 5887.24, "text": " Awesome.", "tokens": [50664, 10391, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13681794714739942, "compression_ratio": 1.5830258302583027, "no_speech_prob": 0.2874073088169098}, {"id": 2201, "seek": 588024, "start": 5887.24, "end": 5896.24, "text": " And, uh, quick, so my quick plugs, you know, at BAKZT future Twitter, Instagram, YouTube.com slash BAKZT future.", "tokens": [50714, 400, 11, 2232, 11, 1702, 11, 370, 452, 1702, 33899, 11, 291, 458, 11, 412, 363, 5340, 57, 51, 2027, 5794, 11, 5281, 11, 3088, 13, 1112, 17330, 363, 5340, 57, 51, 2027, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13681794714739942, "compression_ratio": 1.5830258302583027, "no_speech_prob": 0.2874073088169098}, {"id": 2202, "seek": 588024, "start": 5896.24, "end": 5902.24, "text": " My newsletter, I'll put it in the description below and I have a Twitter spaces event coming in two days at noon.", "tokens": [51164, 1222, 26469, 11, 286, 603, 829, 309, 294, 264, 3855, 2507, 293, 286, 362, 257, 5794, 7673, 2280, 1348, 294, 732, 1708, 412, 24040, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13681794714739942, "compression_ratio": 1.5830258302583027, "no_speech_prob": 0.2874073088169098}, {"id": 2203, "seek": 588024, "start": 5902.24, "end": 5904.24, "text": " Uh, a couple of people probably pulling up.", "tokens": [51464, 4019, 11, 257, 1916, 295, 561, 1391, 8407, 493, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13681794714739942, "compression_ratio": 1.5830258302583027, "no_speech_prob": 0.2874073088169098}, {"id": 2204, "seek": 588024, "start": 5904.24, "end": 5906.24, "text": " This is like a audio only event.", "tokens": [51564, 639, 307, 411, 257, 6278, 787, 2280, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13681794714739942, "compression_ratio": 1.5830258302583027, "no_speech_prob": 0.2874073088169098}, {"id": 2205, "seek": 590624, "start": 5906.24, "end": 5912.24, "text": " So I encourage audio podcast listeners, YouTube subscribers, pull up to the Twitter spaces event.", "tokens": [50364, 407, 286, 5373, 6278, 7367, 23274, 11, 3088, 11092, 11, 2235, 493, 281, 264, 5794, 7673, 2280, 13, 50664], "temperature": 0.0, "avg_logprob": -0.09642423902239118, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.3661819398403168}, {"id": 2206, "seek": 590624, "start": 5912.24, "end": 5917.24, "text": " We're going to chat more about codecs and prompt design and some other stuff going on in the, in the space.", "tokens": [50664, 492, 434, 516, 281, 5081, 544, 466, 3089, 14368, 293, 12391, 1715, 293, 512, 661, 1507, 516, 322, 294, 264, 11, 294, 264, 1901, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09642423902239118, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.3661819398403168}, {"id": 2207, "seek": 590624, "start": 5917.24, "end": 5921.24, "text": " So anyways, thank you again for listening to multimodal by BAKZT future.", "tokens": [50914, 407, 13448, 11, 1309, 291, 797, 337, 4764, 281, 32972, 378, 304, 538, 363, 5340, 57, 51, 2027, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09642423902239118, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.3661819398403168}, {"id": 2208, "seek": 590624, "start": 5921.24, "end": 5922.24, "text": " I'll catch you in the next one.", "tokens": [51114, 286, 603, 3745, 291, 294, 264, 958, 472, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09642423902239118, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.3661819398403168}, {"id": 2209, "seek": 590624, "start": 5922.24, "end": 5923.24, "text": " Bye.", "tokens": [51164, 4621, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09642423902239118, "compression_ratio": 1.4788732394366197, "no_speech_prob": 0.3661819398403168}], "language": "en"}