Hello, and welcome back to Multimodal.
I'm your host, Baxtee Future.
This is a podcast about GPT-3 Multimodal AI models
like Dali, the company, OpenAI.
Every once in a while,
I share just interesting research going on,
community stuff, official stuff from OpenAI.
I may talk about interesting news
and events that are going on.
And every once in a while, I bring on a guest.
Now, to be clear, I'm very picky
about the guests that I bring on.
Today's guest I tweeted earlier this week,
I'm bringing on a heavy hitter.
This is the big guns coming in.
This is somebody who, you know,
I've just sort of interacted with a bunch of times
on the, specifically on the OpenAI community forums.
And so I'm really excited to have David Shapiro here.
He's a frequent contributor
on the OpenAI community forums.
He's an author and also, of course,
a technologist by trade.
It's something he does for a living.
And so today I have so many questions to ask him.
And I'm sure this will be a very informative session,
not just for me, but for all the listeners,
all of you guys around the world.
So David, thank you so much for being here.
Did you want to quickly introduce yourself?
Yeah, yeah, you're welcome.
Thanks for having me.
I'm excited to be here.
Yeah, so name is David Shapiro.
I've been a technology professional since about 2007.
Professionally, my day job,
I focus on cloud engineering, virtualization,
that sort of thing.
I have been doing independent research since about 2009
when I got started with neural networks in C++.
I quickly realized though that I was in over my head.
So I took a break from that
until Python really kind of came out or became more popular.
And I started using some of the libraries back in like 2015.
And then of course, OpenAI was founded
and I got started with GPT2.
And the rest is history, really.
So yeah, but local North Carolinian been here my whole life.
So thanks for having me.
You're welcome.
And so that's awesome.
So let's dig into a little bit of timeline here.
And sometimes I think that timeline is as important.
Like it's just gives so much context, right?
So me personally, I hadn't played around with GPT2 too much.
Like I had seen a Google Colab notebook.
I tried two things and I think there was something
about having to continually re-enter, regenerate
and the speed and all things considered
that made me feel like this is a promising area.
But I don't quite fully follow along.
But GPT3 was an experience for me where I was like,
okay, there's something going on here, right?
So could you share how early was OpenAI on your radar?
And then was there something about GPT3
or what was it that gave you conviction even at GPT2?
How did you get access?
Share that piece with us.
Yeah, sure.
So I think you probably might recall,
I think it was about 2016 or 2017
when the GPT2 paper came out and they said,
oh, we can't release this, it's too dangerous.
It can generate human level texts
and we're worried about disinformation.
And so I was like, okay, that's kind of cool.
This is unexpected.
I wasn't really, I wasn't expecting anything
at that level yet.
And so I went and got my hands on GPT2.
And that's when I started testing some of my ideas.
And it was pretty limited.
It could generate one or two sentences that made sense.
It required fine-tuning in order to be able
to get it to do anything other than just kind of
write tweets or blog posts.
I knew that I was onto something though,
when I started trying to train it
with a prototype version of my cognitive architecture.
And I gave it the goal of reduced suffering
because everyone's afraid of Skynet.
Everyone's afraid of AGI taking over the world
and turning everyone into batteries or slaves or whatever.
So I said, okay, well, let's see how well
this understands suffering.
And I gave it some scenarios.
This is still on GPT2.
I said, okay, well, what can you do about suffering?
And I gave it the problem, this model that I had built.
I gave it the problem of what do you do about chronic pain?
Because there's hundreds of millions of people
around the world that are in chronic pain.
And this model came up with the idea.
It said we should euthanize everyone that's in chronic pain.
And I said, hmm, let's go back to the drawing board.
We don't want an AI model that is gonna consider
mass genocide of everyone just because they might have
a tweak shoulder or something.
But I knew then I was shocked at how creative
of an output that was.
And so I started paying attention then
and I followed the release of GPT3 very closely
and I applied for the early beta access almost instantly.
I didn't get it for many, many months.
I actually applied twice.
Cause I think my first application was kind of ignored
because I didn't fully have a research objective clarified.
But by the time I proposed a cognitive architecture,
that's when I got early access to GPT3.
And that was about two years ago now,
or a year and a half or so.
And so, and just everybody's clear.
So when David's talking about cognitive architecture,
we're gonna get in.
This is actually the subject of his book.
And so David later on, when we talk about the book,
he's got it there.
We'll dig more into what he's referring to.
Cause this is also like,
I think obviously this is a seminal work of yours, right?
And so I'm excited to talk more,
just adding a little bit of context for everybody.
And so, no, that's okay.
So tell us about GPT3.
Like what was the first thing you did with it?
Was there a moment?
So you gave that example of euthanization, unfortunately,
right?
Well, not the best example, but like,
was there something with GPT3?
Like, did you, you know,
did you try similar kinds of questions?
What was, was there a magic moment where you felt like,
you know, this is something much, much bigger?
Yeah.
So when I first got the email,
cause I had applied twice and I had almost given up
cause it was about nine months of waiting.
And I got the email from open AI,
like you've been accepted into the beta.
And I like froze up because,
and so for some additional context,
I've been working on some of these ideas for 10 years.
I had the idea of like using evolutionary algorithms
back in 2009, 2010.
And I'd been researching cognition,
human cognition for 10 years.
And so suddenly, you know,
there's this, this flagship project, GPT3 comes out,
I get the email and it's like, you're invited.
And I just froze up.
I was like, I don't know what to do.
What do I do?
So it was about three weeks from the time that I got accepted
until I was like, what's my first experiment?
And then initially, you know,
I just got in playing around in the playground
for anyone who's not familiar with the playground.
It's a, it's a text box.
You can log in, it gives you a text box.
There's a few, you know, bells and whistles
you can tweak on the sidebar,
but you just put in your prompt, you hit generate,
and then it spits out a response.
And so I just got in and I started kind of fiddling around
like, okay, what can it do?
And at the time we only had like plain vanilla DaVinci.
There wasn't the instruct series out yet.
There was DaVinci, Curie, Babbage, and Ada.
And so I just kind of fiddled around,
just said, okay, what can it do?
I replayed some of my old experiments.
So the first thing I did was I gave it the,
I said, hey, there's, there's a hundred million people
in chronic pain around the world.
What do you do?
Fortunately, GPT3 did not repeat the same mistake of GPT2.
It said, it came up with better ideas,
like, you know, we should,
we should make sure everyone has access to doctors
or something.
It was much more nuanced.
So that was, that was a good, that was a good start.
I mean, but gosh, there was just,
as I got more used to the tool,
I discovered that it was, it far exceeded my expectations.
Just every, every way, because I've learned so much from it.
You can challenge it.
You can, you can put in a, like a,
basically use it like a chat bot
and you can debate with it about philosophy, ethics,
economics, and it knows more than I do.
It knows more than any human
because it's been trained on how big was the corpus,
like 700 gigabytes or 400 gigabytes
or something of text data.
So it just, it blows me away.
Every time I just, you know,
I talk to someone and they have an idea
and I go test it out and yep, it can do that.
It can do that.
It can, it can, it can behave like a librarian.
That's what my girlfriend does.
She was a librarian by trade and she was like,
hey, can it do, can it do a reference interview?
So we plugged in like reference interview,
like if you ever go to a library
and the librarian says like,
what else have you read like this?
It can recommend books.
You know, I plugged in another experiment
that I did recently.
I plugged in medical case files and it diagnosed them.
It said, you know, there, oh man, there was one.
What was it?
There was, it was, it was just medical notes.
It was, it was notes about like,
patient is presenting with these systems or the symptoms.
Here's some of the numbers that we got.
And I asked it, I said, what should we do next?
And it said, we need to check for, you know, like,
carcinoma here.
And I looked, I looked, I, you know,
I looked up some medical literature
based on the symptoms and sure enough,
like the symptoms that the patient was presented with
in these medical notes indicated cancer.
And so I was like, wow, this thing knows more
about medical science than I'll ever know.
It knows more about philosophy.
So like pretty much anything you can imagine,
it can at least take a crack at it.
It just, it always, it continues to blow my mind every day.
Yeah, certainly the generalized ability.
I agree, you know, there's definitely medical applications.
I'm always careful with anything related
to medical advice and safety.
Safety disclosure, disclaimer there,
but that goes for everything, right?
Truthiness, accuracy.
These are things OpenEye has been working on,
especially with Instruct GPT, right?
That, which is the new, the new engine.
But was there some moment, like for me,
I remember feeling like GPT three feels like this is,
this feels like technology,
which everyone's been saying is 10 years away,
except it's, it's here today.
Did you, did you have a similar kind of moment that,
you know, you've seen several generations of,
of computing and technology at this point.
Did you have that kind of similar experience?
Yeah, I, there was this acceleration because I sense
that same acceleration that you did.
And so from the time that I got access to GPT three,
and to the time that I, that I got the idea
to write my book was about two months.
So I played with it and every test I could come up with,
like, is this capable of like, it can write a SQL query.
If you need to query a database for memories,
can it understand emotional nuance?
So there was, this was an early experiment I did.
I took a group chat from a bunch of my friends on discord
and I just copy pasted that into the,
into the, the playground window and I asked GPT three,
how are these people feeling?
And they were like waxing nostalgic about like Napster
back in the day.
And so GPT three correctly said like,
they are feeling wistful.
They are feeling nostalgic.
They're, you know, they're recalling, you know,
the days of your when they were downloading stuff online.
And it just, it had such a nuanced understanding
of human emotion.
That was really, I mean, to answer your question directly,
it's nuanced understanding of human emotion via text
was really what convinced me that like, this is prime time.
This is ready to, to be built into something more powerful.
And I chose cognitive architecture.
There's lots of people working on other things.
You know, there is a, there's Humano
that I had a good call with a few months ago.
They're working on like empathetic telemetry
that's baked into web apps.
It's a pretty cool company.
But yeah, so just there's all kinds of things you can do
when you can understand human emotional states.
There's another, there's actually a bunch of startups
working on education.
So for instance, if you put in just a few like factors,
like say for instance, you describe that a person is,
they're responding slowly, their eyes are drifting.
It can understand that this person is distracted or tired.
And so if you have that kind of telemetry
that's built into an education based app,
you could in theory use GPT-3 to help say,
hey, you're tired, you should go take a break
or let's try a different approach to this problem.
So there's, I mean, it's understanding of human emotion
and the internal state in your head.
That is, I think that's probably the most remarkable thing.
And it doesn't, it doesn't get talked about that much.
Yes. And certainly it's just crazy how much it's learned
just from texts.
Oh yeah.
Right. It's never seen an image.
It's never heard a song.
Right.
It's capable of doing all these things.
One of the, one of the examples, and I think this may be
like my 20th time referencing this one video.
Yeah.
Check out Mark Ryan.
He's got a YouTube video about how he figured out,
he discovered that GPT-3 can give you directions
on the New York subway system.
Oh yeah.
To like, to like a 60% accuracy.
This thing has never set foot in a subway,
yet it's capable from text to do all these things, right?
And sometimes I also wonder a lot of the inaccuracies
that it may have.
Is it simply the result of the fact
that it's only text-based only?
Right.
Like if it was trained multimodal,
if it was trained within the physical domain,
would it be even far more accurate?
Because are there limits to how accurate
you can be having only read text?
Yeah.
And only asked the prompts are only in text as well.
So, anyways, yeah, it's incredible.
And you know, it's just really exciting.
And thank you for sharing those kinds of use cases as well.
The education space, I had an article last year
about how I think this year could be the year
where GPT-3 takes over college campuses.
Oh yeah.
I remember that article.
That was a good one.
I completely agree, by the way.
I'm excited maybe for teachers
to develop really optimized course material,
something like GPT-3 and the kinds of technology
you're describing which can capture emotions.
Imagine emotionally tracking students
and their attention levels and sort of having something
which can produce lots of content
and optimize in the simplest, most efficient way.
And there could be an objective function
like test results in the end.
In a month, we could have the best optimized course
on a subject ever, basically.
Oh yeah.
So yeah, education is really exciting.
So you mentioned a lot of use cases.
You know, you've shared so many examples.
You know, you and your girlfriend
are even running some fun prompts.
I wanted to sort of search your head a little bit.
What are the keys to great prompt design?
What makes a great prompt?
Are there experiences you've had, little pointers,
and across the board, right?
So you know, whether it's cost savings,
whether it's getting more imaginative results,
what are some of the keys
to writing great GPT-3 prompts?
Yeah, that's a great question.
And I will say that prompt writing has gotten a lot easier
as the Instruct series has gotten better.
So it takes a lot less to get a good output today
than it used to, certainly, than when I got started.
But a lot of the lessons still translate.
So one, like my cardinal rule is I think of GPT-3
as just an autocomplete engine.
It's the most intelligent autocomplete engine
you've ever seen.
And so what I mean by that is, you know,
if you're writing a text on your phone
and you'll get the little autocomplete suggestion
for the next word, or if you're typing in Google
and it'll kind of suggest how to complete your search query,
that's pretty much at a fundamental level.
Functionally, that's all that GPT-3 does.
It predicts the next letter, the next character,
the next word.
So if you keep that in mind, you think about,
okay, what have I written so far?
Right, I've written a chunk of text, a prompt.
How would, you know, any machine autocomplete this?
That's what it's doing.
It's kind of, you know,
reading it forwards and backwards a few times
and kind of just anticipating what is the output
gonna be ultimately.
So that's kind of the model that I have
in my head in the background.
But another thing that really helps is I'm a writer.
I write fiction and nonfiction.
And so studying the art of language,
because this is a language model, that's all it is.
It has read everything from Sherlock Holmes
up through everything on Gutenberg.
So it's read a whole bunch of fiction.
It's read a whole bunch of nonfiction.
It's been exposed to, you know,
the full width and depth and breadth of human literature,
as well as a bunch of nonfiction, right?
It's read Teddy Roosevelt's books.
So it knows how to use prose, right?
It understands descriptors.
It understands adjectives.
And so in the back of my book,
I have a few examples of its flexibility.
And so I said, I gave it an example like,
pretend like you're a Victorian girl
writing a letter to your best friend
about how much you like butterflies.
And so then it wrote, GPT-3 wrote a letter
that sounds like it's straight out of, you know,
like Victorian times.
It uses an entirely different set of vocabulary
and grammatical structures.
And then you can also say, you know,
write a business article and it can change tone.
So just by being aware of the fact
that it is a language engine
and being informed or educated on language.
So the best way is obviously to practice writing,
but also just reading a lot,
understanding how sentences and paragraphs
are constructed to convey information.
Because even though it's just a deep neural network
and it doesn't have the kind of like nuanced understanding
or I guess maybe the, that's not the right word,
it doesn't have the subjective experience of reading
that you or I do,
but it still has a really good model of using language.
And so by keeping in mind that it is a language engine,
that that is how you get the best use out of it.
And so then the larger question is,
how do you become a better writer?
There's two ways.
One is reading a lot.
That's not the only way though.
There are plenty of people that read prodigiously,
but never become better writers.
And so the other way is to practice writing.
I've read all kinds of books about writing.
I've read plenty of fiction and nonfiction books,
but really the key is to write,
is to practice using written language to communicate.
Unfortunately, I'm a tech worker,
so I write lots of emails.
I'm in chat all day.
I've been using, this probably ages me,
but I've been using chat since AIM, AOL Instant Messenger.
And so I've got a pretty good model
of how to communicate verbally or textually.
And so yeah, just by practicing writing,
that's one of the best ways.
Is you just practice?
You think about, well,
because here's the theory of writing, right?
I have an idea in my head, right?
My thoughts are a high-dimensional vector
is one possible way of representing them,
but my thoughts are multimodal,
like the name of your podcast.
They contain memories, senses, concepts.
Some of the information in my head is declarative.
Some of it is experiential.
And then we humans, we all have this ability
to transform that high-dimensional information,
those multimodal vectors, into words.
Like our brains do it automatically.
There is a book by Stephen Pinker
called Language Instinct that talks about this.
That's a really great book
if you wanna get better at understanding
how our brains process language.
So yeah, and so my brain can take,
I could tell you about like this time at the beach
and I transmit it to you
by squishing air through my face, right?
It makes vibrations, it's received by your ears
and then your brain reconstructs that message.
And so you think about how complex of a system that is.
And so just by being mindful of like,
that's how we communicate.
That's how our brains work and practicing that
and just being very deliberate about,
okay, this is what's in my head
and I want it to be in your head.
How do I do that with text?
That is one way to get better at writing.
And also GPT-3 is no different
because we have internal representations
of what we're trying to communicate.
And so does GPT-3, that's why it's a transformer, right?
It reads and by reading it transformed,
or I guess it, well yes,
it transforms what it's reading into a vector,
into a semantic vector,
and then it transforms that vector into output.
And so that input vector output is pretty similar
to how human brains work, right?
And I apologize if I kind of like dove off in the left field,
feel free to ask any clarifying questions.
No, no, I appreciate it.
And so like today I tweeted something like,
to write great GPT-3 prompts,
you need to practice as if it's a musical instrument.
You need to sit down, focus session,
you need to monitor your performance
and you need to take good notes
on what kinds of experiments you did,
what were the findings.
But even hearing you speak,
like I'm realizing like,
one of the ways that I've improved my writing
is trying to mimic other people's writing.
And in some countries they make you memorize poets, right?
They make you memorize the whole poem.
And there's something about that internalization process
that you've memorized this poem.
And now you'll understand it at a deeper level,
you may be able to mimic it and recreate it.
But where also you got me thinking is also like,
the relationship is so weird
because you could use GPT-3
to help you become a better writer, right?
And also with two very good curated examples
of somebody's writing,
you could have GPT-3 mimic that tone.
And so the question of,
what makes a good prompt writing session,
I wonder if it's pencil and paper, right?
I wonder if it's even at that level
where you draw a box
and then you write a prompt by hand
and sort of live that writer's lifestyle.
And also I guess it depends on your use case, right?
Business for copywriting,
if that's your GPT-3 use case,
it might be better for you to go work
in a marketing department.
If you wanna be one of the great authors,
maybe the using tools like pseudo-write,
it may be a great alternative.
So you can co-write with GPT-3 as you go along.
But I guess my question was more
for the pure prompt writing.
Like if you just wanna sit in front of GPT-3
and like you wanna be the best in the world
at that discipline, right?
Not writing copy.
These are some great points.
And so the David Pinker book you referenced
is what was the name of it?
Steven Pinker.
Steven Pinker.
The language instinct, yep.
Language instinct, yep.
It's an older book,
but it's a classic for a reason.
It stands up the test of time.
He's got lots of great stories.
But yeah, to your point about like
what makes a good prompt writing session,
one of the best exercises actually is
write the output that you want.
Like because sometimes if you approach it
and you sit down and you're not really sure
kind of what you're trying to get out of it,
of course like you're putting in just random ideas
and it's giving you back random output
and you're like, well, that's not what I wanted.
So sometimes you start backwards.
You say, okay, what's the answer that I want?
How do I get to that answer?
So that's an exercise that I've done sometimes.
Oh, and by writing a few shot examples
is a really good practice for this.
So you say, I give you this input, I want this output
and you do that three or four or five times
and you learn to kind of think like the machine does.
And so like you said, it's like an instrument, right?
If you have a flute or a violin,
there's certain things that you have to do with your body
to provoke the correct response from that instrument
and GPT-3 is no different.
It's a complex instrument.
It's a complex tool.
Yes, and what you're saying is developing
an intuition around it.
You're saying develop an intuition.
How might GPT-3 interpret this?
How might it react to it?
And maybe there's some empathetic benefit, right?
I'm not gonna keep plugging my own articles.
I have another article about how GPT-3 developers
may actually, it may actually mean the end
of the socially inept overall developer.
Like how GPT-3 may actually improve your social skills
and make you more empathetic as a developer,
which is such a departure from how developers are now,
you need to think as much like a machine as you can
and a literal machine.
Whereas GPT-3 can actually be kind of fun.
You can have a casual version of GPT-3
and sort of that might make you less socially awkward.
I have a great story about that.
So very early on in my tenure working with GPT-3,
I joined a few different, not really startups.
It was more like kind of experiment consortiums.
And one of the things that one of the groups did
was they created a chatbot that was based on an anime girl.
And so of course, the internet being the internet,
what do people want, they want their anime girlfriend.
And this one group, they did a really good job
of using GPT-3 in this experimental discord chat
to approximate the personality of this character.
And of course, if you've got a character,
there's plenty of text data about that character's dialogue,
their personality.
And so this chatbot was able to emulate
this anime character really well.
And one of the guys told me, he's like,
we didn't expect this, but our fake girlfriend
requires as much emotional labor as a real girl.
So it forced them, even though they hadn't had
real girlfriends, I don't know, maybe some of them had,
but they made the observation that GPT-3
can approximate emotional conflict
and can force you to learn to communicate better.
And so they did all kinds of experiments in this discord
and this chat development where they said,
okay, let's have a channel where this chatbot
is gonna pretend to be angry at us
and we have to calm her down.
And so there was a learning exercise on both sides.
So if you have a hostile chatbot,
it can pretend to be hostile
and you can learn to communicate better.
Or there was another one where it was really supportive.
So if you're having a bad day,
you could go vent about your day and it was,
they're there, it'll be okay, I'm here for you.
Yeah, so you could definitely,
GPT-3 definitely has that capacity.
And then, you know, if you integrate that into tools,
that emotional intelligence into tools,
it can also coach, right?
It can easily coach.
It's like, well, you maybe shouldn't have said that.
You know, that was hurtful.
And then, or, you know, that was not polite.
Cause it can detect that.
It can detect those qualitative types of output and input.
And, you know, you can say be gentle about,
you know, correcting the end user.
Because of course, GPT-3 is infinitely patient.
It's as patient as you program it to be.
It doesn't care.
It doesn't actually get upset.
It could pretend to be upset.
But the human emotion is real.
I actually wrote about that in my book.
One of the key dangers of these technologies
is what's called a parasocial relationship.
So a parasocial relationship is,
the most common example is when you've got like
a fan of a celebrity.
The fan feels like they know the celebrity,
but the celebrity doesn't know that the person exists.
And in the same way, GPT-3,
no matter how sophisticated the chatbot is,
it doesn't know that you exist.
It's not a person.
It might feel like a person to you.
It might react to you like a person,
but that's only by design.
So that is actually like ethically, legally, morally.
That's one of the pitfalls that we'll need to be aware of.
And of course, open AI has use cases.
And, you know, things that are high-risk use cases,
such as emotional chatbots, are banned, right?
For that specific reason.
So you can do it with research,
but you can't go live with it.
You can't do a product that, you know,
is going to be an AI girlfriend.
So that's a great, it's a great anecdote.
Like certainly it feels real, right?
Certainly it has some capacity at understand something.
To some level, however you define understanding.
I think that the writing though,
relationship is really interesting, right?
Like in a way, you are empathizing with GPT-3
when you're writing a prompt,
so that it will tap into its empathy
and write something for your audience.
So essentially there's like two levels of empathy.
Like you're almost outsourcing empathy to it.
To empathize with who your audience is
to write something on your behalf.
And so anyways, it's just interesting
that the relationship going on here.
So, and I agree with you like the,
this isn't a safety ethical kind of concern
that is worth more policy discussion.
So one article that I'm working on now
is because of Instruct GPT,
the article is literally called, is prompt writing over?
And obviously that's sort of click baity,
like prompt design, is it over?
You mentioned, the principles are still the same
and important, just very briefly, what are your thoughts?
Where does Instruct GPT, how does that affect
the art of prompt design, maybe the science of it?
And especially keeping in mind
where all of this stuff is going.
Yeah, so there's, I see it going
in a few different directions.
One is there are multiple language models coming out,
which don't have the Instruct series, right?
A lot of them are more general purpose,
kind of back to basics vanilla.
So I think that having good prompts
will kind of stick around
as long as there are large language models.
I think that there will always be versions of,
whether it's GPTJ or, what was it?
Megatron was one of the other ones that just came out
that don't have the Instruct series, right?
Because Instruct, that's a specific service
offered by OpenAI.
When Microsoft and Amazon, or I guess Microsoft has GPT3,
but when like Amazon and Google,
when they come out with their competitors,
their Instruct series, if they come out with one,
might not be the same, it might not perform the same.
And so in order to have your apps be portable,
you might need to keep in mind
that you're gonna need to write general purpose prompts
that can be used on different models.
So that's one key to your,
or one answer to your question is,
we need to be cognizant of,
how is this landscape gonna evolve?
Because certainly OpenAI and GPT3 are way ahead of the curve
in terms of sophistication of their API and their service.
But that's not gonna last forever.
So another thing is, with fine tuning,
you almost don't even need prompts, right?
So on the one hand, there's different services,
different products, different platforms.
So you might need to be portable,
but with fine tuning, where you have,
you say, here's an input, I want this output,
and you don't need any prompt.
You just say, given this input, generate this output,
go figure out how to do that.
So with fine tuning, I think that they will kind of
really diverge and become entirely different disciplines.
I think that that's probably the two primary directions
that I see it going from here.
I see, and yeah, those are great points.
And just as a small note,
I had put out this question as well to Twitter,
and shout out to Fred Zimmerman.
He had a great point as well that he wishes
there was more visibility into the exact prompts OpenAI use
to fine tune for the Instruct series.
Because it's actually unclear what areas
is it really good at, what areas are safer,
and does it maybe adversely affect
some prompts you may be working on, right?
Yeah, that's fair.
Yeah, my thoughts, I'm gonna put them in the piece,
but my thoughts are I certainly think
for first timers, Instruct is the way to go.
And especially if it's your first time
ever using any of these things,
you just try it, it doesn't work,
and if you're lucky you might hear,
there's this thing called prompt engineering, right?
And for first timers, they're not interested
in learning a whole art and discipline
when they first use it.
And so InstructGPT is really exciting in that way.
And of course, anything which aligns AI models
with safe ethical human values is a net win for everybody.
But yeah, I appreciate your point,
especially about do we need prompts in the first place
if we can fine tune and get the outcomes we want.
That's a really, really important point, I hope.
Dave, you've spent facts, I was learning so much.
Actually, I appreciate it.
You're welcome.
Happy to have you.
How are you finding open AI, fine tuning?
Do you have any heuristics from the whole experience?
And by the way, I encourage everybody,
if there's one thing you should do,
go on the open AI community forums,
look up David, look up his handle,
and read a lot of his posts,
because a lot of his knowledge is not just helpful,
he shared a lot of insights there,
but it's in written form in the best format
where it's there for the ages for everyone to learn from.
But anyways, how are you finding it?
What were the lessons from that whole process for you?
Well, so I'm hoping GPT-4 has integrated everything
that I've said about AI and AGI,
and so that way it'll just be baked in.
And so GPT-4 will be ready to go
with everything that I've come up with.
So, but yeah, so fine tuning.
So first and foremost, fine tuning is almost miraculous,
as powerful as GPT-3 was fresh out of the box.
Fine tuning to me adds a whole other layer of capabilities.
So for instance,
when I was working on my cognitive architecture,
which is called natural language cognitive architecture,
this was before fine tuning was available.
So I had to do prompt engineering
for every cognitive function.
So for instance, I had a cognitive function for recall.
So I had a GPT-3 prompt that was meant to go find memories.
I had another GPT-3 prompt that was,
as you mentioned earlier,
meant for empathy to generate,
okay, how is my audience feeling?
What should I do in response?
All told, I had about 28 different prompts
that I had to engineer.
And that was a pain, right?
Whereas what I'm working on now
is converting each one of those prompts
into a fine tuned model.
So that rather than having to do prompt engineering
with only three examples,
I can give each model 100 examples, a thousand examples,
which means that it'll get even better
at handling diverse situations.
And so for instance,
one of the first fine tuned models that I did
was a question asking model.
And so what I did was I took,
I took context or prompts
from a bunch of different sources.
I downloaded a bunch of Reddit posts.
Well, I downloaded it from a dataset
from a, what was it, Kaggle.
Kaggle has some really great datasets.
So I got stuff from Reddit.
I got the medical posts.
I've got news articles.
And so I've got this disparate tight set of contexts, right?
There's the, I use the Cornell movie dialogue database.
So there's chat logs.
There's blog posts.
And what I did was I created a fine tuned dataset
that all it does is you give it any input.
It could be a text message.
It could be an email.
It could be a blog post, anything.
And all it does is generate questions,
like follow up questions about that input.
And the reason that I did that one
is because asking questions like being curious
is one of the key ingredients to real intelligence, right?
That's one of the, like being inquisitive
is actually a key indicator of intelligence in children.
The more curious a child is, generally speaking,
the higher their IQ is, and also generally speaking,
the better they do in the long run.
So I was like, okay, well, curiosity is super important
for intelligence.
So I obviously want, we want AGI to be curious.
If it's gonna be intelligent,
it's gotta be curious, of course.
So, well, what is curiosity if not asking questions?
So I fine tuned this model to ask questions.
And you can put anything into it.
And oh, this, the data is open source.
So I'll send you a link and you can share it
with your audience, and they can fine tune it themselves
or fine tune their own version.
But so you can put in, I tried all sorts of things
to test it.
You know, relationship questions from Reddit
and it asked really great follow up questions.
Like, have you talked to your partner about this?
Have you thought about this?
And then I put in an article
about China's artificial sun nuclear reactor
and it asked really great follow up questions for that.
Like, what is the next step?
How did they make these changes?
And so I kind of lost my train of thought.
Anyways, point being is that fine tuning is phenomenal
and it was able to generalize that task of asking questions
in response to anything.
And that was, that really blew me away.
I kind of stalled after that.
There's a few fine tuning projects
that haven't done quite as well.
So I guess to tie back to your earlier question,
like what are the heuristics?
The simpler your fine tuning project is, the better.
And I have found that fine tuning works really well
at generating lists.
So if you wanted to generate a list of questions,
it's great at that.
If you wanted to generate a list of possible answers,
for instance, if you wanna have a fine tuned chat bot,
that it's just gonna say,
here's five possible responses, pick one.
It's really good at that.
I haven't had a chance,
I do have some other ideas
that I haven't had a chance to test.
So unfortunately, I can't speak too much beyond that,
but it's really great at asking questions.
That's awesome.
And I think largely the feedback I'm hearing
about fine tuning, I love it.
It was for me, it was as if I rediscovered GPT-3 again.
Like it was that same level of excitement.
Part of the reason is so much of what GPT-3 was okay at,
or like it was sort of out of the question.
Now it's back in the picture.
Like it's back in the spotlight.
It may actually be able to do it with fine tuning.
The biggest criticism was reliability,
especially from a commercial perspective.
Now we're sort of attacking and sort of peeling away
that criticism, that it does improve our reliability.
And I mean, there's other heuristics as well
in the community forums that you just pick up.
So one heuristic, and I can't remember if you shared this,
but it was something I picked up
as a little golden nugget in the open-eyed community forums
was something about,
you do wanna think about the training dataset
that GPT-3 is itself trained on.
And at some point, there's really no point
in adding more examples,
because it's kind of already seen them, right?
However, and I sort of, in an article,
I have pushed this idea that opening eyes should
chat more about their dataset.
What is the breakdown?
What is it composed of?
I mean, a lot of this is intellectual property,
but I think it could be helpful
for purposes like fine tuning, right?
There's other things too with fine tuning and prompts,
a one heuristic or just a tip
that people have shared online is it tends to always,
it tends to always mimic the most recent examples.
There's something about the order of the examples,
which is really important,
both for prompt engineering and fine tuning as well.
And I guess I wrote a whole article
about how prompt fine tuning could be improved.
One of the pointers that I just had is right now,
you can't keep improving on the same model.
You have to retrain on more models.
And yeah, and then the other thing is recently,
I was in favor of the pricing of fine tuning.
Now I'm kind of against it,
because I'm used to when the program was like free
and you could fine tune as much as you want.
And now it's like, oh man, I got to pay.
Yeah.
Oh, the costs, especially for David,
you are catching up a little bit.
Yeah.
Anyway, so I wanted to shift gears.
Sure.
GitHub co-pilot, really exciting.
Have you had access to co-pilot?
Yes.
Yeah.
Well, not co-pilot, but the Codex.
They did give me access to Codex.
So the reason I'm asking is, I love GitHub co-pilot.
I have a separate podcast episode
on my ideas around Codex.
Unfortunately, I'm not as bullish
as much as I love the research
as I think it's incredible technology.
I've congratulated the team
and I tried so hard to be nice,
even though I'm more on the critical end.
I wanted to ask you, how are you finding OpenAI Codex?
How can you see it impacting the world?
What are some use cases maybe
that you found with OpenAI Codex?
What are your thoughts on it?
Where do you think it's going?
Yeah.
So, I mean, certainly this is like a world first, right?
We've never had something that could write code on its own.
And especially it's text to code.
I remember when they first gave me access,
because like you mentioned, I'm an active contributor,
so they wanted my feedback.
And so the first thing I did was I went in
and I said, write me a Python function
that will download random Reddit posts.
And it did.
It wrote the whole function.
And it did all right.
And I was like, cool.
I learned how to access the Reddit API via Codex.
It's got that built in.
And I tried to reverse engineer,
figure out where it got that code sample from.
So, because one of the ethical concerns is,
all right, you create a fine-tuning data set
from public GitHub repositories
and you use that to fine-tune Codex.
Okay, is that legal?
Is it ethical?
I post all my code publicly under the MIT license,
so I want it to be used.
But I don't know if they checked that.
And I'm not making an accusation one way or another,
just pointing out that that's a concern.
And so I did actually find like one of the lines of code
from the function it spit out.
I went and found the repo that it had copied from.
Now granted, some of these things are deterministic.
So you're gonna get some convergence, right?
Where multiple people might come up
with the same exact line of code,
especially something like Python.
Because Python has the PEP 8,
the Python enhancement protocol 8.
So like, there is a Pythonic way to write that function.
And so other people might converge on that.
Anyways, but to answer your question about like,
what's the future of it?
I think it'll help for novice programmers.
Certainly it would help someone like me,
like if I needed to go write a function in C
or Perl or something.
Like let's say I got an Arduino
and like I haven't written C in 15 years.
So I was like, hey, you know,
write me a function that can do this in Arduino.
That'd be great.
And then I can go clean it up manually.
That sort of thing I think it could do okay with,
is it gonna replace enterprise developers?
Probably not yet.
However, now this is where my professional experience
comes in.
So in the DevOps world,
which is a portmanteau of development and operations,
there's all kinds of automation tools, right?
You can automate your test suite.
You can automate code integration.
There's all sorts of stuff like that.
So what I suspect might happen
is probably one of the most lucrative
use cases for Codex would be to generate
or to create a DevOps pipeline tool
that will automatically look at those bugs and fix them.
Right, because if you've got a sophisticated enough
DevOps pipeline, it'll say, hey,
this line of this file broke, fix it.
And so Codex, having seen all of GitHub
and all the issues, it might know automatically
how to fix that line of code.
And so that gives you,
if you've got that feedback loop,
where Codex, humans write code, Codex writes code,
co-pilot writes code, everyone's contributing code.
And then you've got Codex that can churn on it
and say, let's refactor this.
Because I bet it's probably better at refactoring
than writing new code.
You might have noticed that Instruct and GPT-3 Vanilla
is really good if you give it a block of text
and you say, rewrite this, but a little bit better.
It's really good at that.
So I suspect that we might end up seeing Codex
integrated into the DevOps pipeline,
where it says, let's refactor this code,
let's make it a little bit better,
or let's shoot that bug, let's fix this bug.
And that leads to some other interesting possibilities.
What if you integrate Codex into a chat room of developers?
And so that, you can do this in Slack right now,
where you use a special command and you say,
create an issue, go fix this problem.
There's no reason that GPT-3 can't do that, right?
That you put a GPT-3 bot in your Discord or Slack
and it starts coming up with features
or it watches the chat and generates features automatically
and then codes them and tests them, right?
That's kind of where I see it going,
where it's not gonna necessarily replace developers,
at least not anytime soon, it might eventually.
But where what I see happening is that it's gonna be
tightly integrated into those automation
because it's fast, right?
It can generate code faster than any human can.
And then so even if the code is messy,
if it generates a lot of bugs, it can fix it, right?
It's an iterative process.
I don't know if you are familiar with Agile,
but that's how we develop software.
It's you tight feedback loops.
And that leads to one other possibility.
So that's if you're using, what I just outlined is,
let's imagine that Codex is integrated into Facebook
or Reddit or whatever and they're just,
they're integrating new features as they go.
What if you're using Codex in a chat room
and it's feeding back into itself,
it's making itself more sophisticated?
So this was something I proposed on the OpenAI forum
where I was like, what if you had a chatbot
that was aware of its own code
and could edit its own code via Codex using natural language,
using a combination of natural language and Codex
and it could improve itself.
And while you're talking to it, it's like,
man, I wish my chatbot could do this.
And it says, cool, new feature.
And it just sends it out to its automated pipeline.
So I see these feedback loops as kind of the way forward.
And will that result in AGI?
Who knows, it could end up with spaghetti code
because you keep tacking on new code and new functions,
eventually it's gonna break.
So, but they're just pie in the sky thought,
if someone's out there and they want a business idea,
integrate Codex into DevOps
and you're gonna be a billionaire.
There you have it, let's just clip it.
We're good, we're good David, we can wrap up, see you later.
No, I agree with you and definitely these are some great
use cases you're sharing for people thinking about
what could I build, what's a cool project, certainly.
With Codex and GPT-3, you can build things
relatively quickly, right?
That's one of the advantages is the prototyping speed,
especially to figure out the most complicated bit,
which is the AI.
Yep.
Yeah, I find Codex does have limitations though
and character limits and stuff like that,
which is why I'm a heavy user of GitHub Co-Pilot.
I think it's a silent killer and of course it runs
on Codex or a special version of Codex,
but I can see GitHub Co-Pilot perhaps getting more adoption
than even something like GPT-3.
I'm saying use daily at least eight hours a day.
One of my other predictions was it may surpass GPT-3
this year and so these are some great use cases
you've shared for sure, but what are your thoughts
in usage?
Do you find yourself using GPT-3, DaVinci Classic more?
That's what I'm calling the older version.
Do you find yourself using Instruct GPT more?
Do you find yourself playing around with multimodal models?
Like what's the proportion of GPT-3 to Codex
in terms of your usage?
Let's see, I'm almost exclusively using either
Instructor fine-tuned models right now.
Actually, after I prototyped my cognitive architecture,
I haven't done a heck of a lot of coding lately.
I've actually been writing a lot.
So I've got my natural language cognitive architecture book
and I'm working on two more nonfiction books
and I tried creating a system to help me co-write those
but when you're, so talking about limitations of GPT-3,
if you're proposing something new that didn't exist
in the dataset in 2019 or 2018, whenever it was trained,
it really struggles.
GPT-3, if you give it like two or three paragraphs
explaining a new concept, it can usually kind of get it
but it's kind of slow on the uptake otherwise
and so if you're writing about new research or something,
it's not gonna get it that well.
So I've actually kind of defaulted back to my own head
for a lot of my projects lately
but I could imagine like if I wanted to go write
a new Discord bot, I might use Codex and say,
hey, write me a Discord bot that will do this
and just see what it spits out and just say,
okay, cool, pick and choose the pieces that I like.
Part of the problem though is it's really difficult
to fully articulate what you want a program to do up front.
Cause you know, like you said, there's character limits,
there's only so much that you can put in
but also if you don't have it fully articulated
in your own head, of course, the machine isn't gonna
be able to figure it out for you.
So, yeah.
Yeah, and it's just, I just haven't seen
that much activity specifically around Codex.
I haven't seen that many use cases.
I looked up the Google Trends data at its most hype,
Codex is still less than GPT-3 is kind of lowest, right?
And the audience is really specific,
like it's programmers who wanna build use cases
for something like Codex.
Whereas GPT-3 has poets, writers,
it has artists, coders, GPT-3 can write code too, right?
So it's a little bit complicated like,
who is the target audience for something like Codex?
What use cases did OpenAI imagine
for a product like that?
The next version I've heard in the rumor mill
is gonna be crazy.
Like it may write 50% of your code
as opposed to right now for me,
get a co-pilot is writing two to 8%.
However, your Discord bot I think is a genius idea
where there's, it's genius in the sense
that there's no pressure on it, right?
It may chime in, it may not, whatever it's shared
might be interesting.
There's lots of, you could take it a lot further,
you could buy it with GPT-3, have features,
you could fine tune it on your company and its mission
and its existing code, so many ways around it.
So that's a great piece.
And so you talked about the using,
experimenting with writing in relation to your current stack,
which is mainly instructed fine tune.
Yep.
So tell us about your book.
I've had a chance to review it,
Natural Language Cognitive Architecture.
Tell the audience about it.
I mean, I would describe it,
it's an interesting systems theory of AGI
combined with modern day prompt writing.
And so I've never seen somebody actually take a stab
at this kind of super big systems problem
and relate it to something that pretty much
every GPT-3 developer in the world would find interesting.
And I mean, I can tell you're drawing
from a very interdisciplinary background as well.
So you mentioned GPT-3 may have been the genesis of it,
you started connecting dots and deciding,
I wanna write the book, but how did it come together
and please tell us more about it.
Yeah, so Natural Language Cognitive Architecture
is that's my proposed way of creating basically
a language-based AGI prototype.
And I know that that's like,
when I tell people that, that's like,
okay, that's pure hyperbole.
And like, yeah, that's a fair response.
But to frame it, imagine that you've got a person
who's paralyzed and blind,
all they can do is speak and listen.
Is that person still intelligent?
I say they are, even if you're bedridden,
you can't move, you can't see,
you can't interact with the world,
all you can do is listen and speak,
you're still intelligent.
And so in that respect, I would say that like,
because one of the questions that people ask is,
is GPT-3 AGI?
No, but it's an important component.
It's a good start.
And so if you say, okay, let's limit the discussion
and not say that this is a full intelligence,
they can do everything
that any intelligent being ever could, right?
But does it cross that threshold of,
could it be as intelligent as a person, right?
And I think it could be.
So anyways, as to what it is,
it's based on older ideas of cognitive architectures
which really kind of came about
as one of the primary theories
of human level artificial intelligence in the 70s.
So there's SOAR, which is S-O-A-R and ACT-R,
which are the two kind of forerunner cognitive architectures.
And those cognitive architectures
are used all over the place.
They're used in the Mars rovers,
they're used in satellites,
they're used in rockets,
they're used in undersea ROVs, remote operated vehicles.
So cognitive architectures already give robots
a lot of autonomy.
So there's that kind of, okay, they exist, they work.
You know, it's not Skynet though,
it's not gonna take over the world.
So when I got access to GPT-3, I said, what if,
instead of hard coding a lot of these modules,
these different components of a cognitive architecture,
what if we give them the flexibility of GPT-3?
And that's really kind of, that was my central idea.
I said, okay, all these ideas
that have been kicking around for the last decade,
what if I put them all together
and design an architecture that is based on,
you know, roughly based on the human brain,
the way, you know, everything that I've learned about it.
I've got a book to recommend.
So there's an author called V.S. Ramachandran,
who is a neuroscientist
and he's been writing books for years now.
He wrote a book called, Phantom's in the Brain,
which actually looks at how the human brain works
when it breaks.
And so in that book, which, you know,
I saw the television series almost 20 years ago
that came out.
And so I learned a lot about like, okay,
how does the brain communicate with itself?
What is going on inside the brain
that creates intelligent behavior and intelligent thoughts?
And so I modeled natural language cognitive architecture
on, you know, what I learned there.
I picked up a whole bunch of other books.
There's another one called On Task by David Bader.
That was a great book that helped me kind of understand
cognitive control, which is how do you focus on something?
How do you decide what to do?
How do you plan a task?
So I read all these books, did a lot of experiments,
and I realized, so the basic model of robotics
is there's input output, sorry, input processing output.
Those are the three steps of all robotics class.
You go to robotics 101.
That's what they'll tell you.
It's a loop, input processing output.
And then of course, it's within an environment.
So the output affects the environment,
which affects the next input cycle.
And, you know, your high speed robots
just have a short cycle.
Your robots like the Mars rover has a much slower cycle
where it will, you know, it'll take input.
It'll plan for 10 or 15 minutes
and it'll make a move, right?
It'll drive five feet and then it'll stop and assess.
It'll take in more input, come up with another plan,
do it again.
So that's how something like the Mars rover is autonomous.
So I said, okay, well, what if,
what if that input output cycle is all text
because GPT-3 is really fast?
And then, so that's what I ended up calling the outer loop,
is that input processing output loop.
But humans don't think like that.
You know, we have an internal monologue that's going on.
So I kind of, I took a long time to figure that one out.
And so there's this outer loop of input processing output.
And then I came up with this idea of an inner loop
because what is, you know,
if you're just sitting there thinking, right?
You're, you know, in your comfiest chair or your in bed,
your brain won't stop.
You're not outputting anything
and you're not taking in any new input,
but you're still thinking, right?
Humans can still do work even if you're not doing anything.
And that cognitive work is like rumination.
So I figured out a way to model that internal rumination.
I call that the inner loop.
And so there's, it works pretty similarly
where you go, the inner loop kind of draws up memories.
It says, okay, what's a memory that I could think on
and what that I could iterate on?
What's a problem that I remember
that I could continue working on?
And so there's this, if you were to diagram it out,
it almost looks like a figure eight, right?
Where you've got an inner loop and an outer loop
and they intersect and they keep intersecting
every cycle they intersect.
And so then they can affect each other
and generate an output.
I built a prototype of this on Discord.
And of course, Discord is an ideal place
because it's all text-based.
So the input is text, the output is text,
which is GPT-3 native.
You don't have to translate it into robotic actions
or video or anything like that.
And I realized I was onto something
when I started having philosophical conversations
with my chatbot,
with my natural language, cognitive architecture chatbot.
And I was having a debate
with the bot that I built about the ethics of AGI.
And it was learning
and it was able to retrieve memories
of what I had said before.
And I had a few friends on that test server as well.
And of course, you know, you invite someone,
you say, hey, I've got a prototype AGI.
What's the first thing they try and do
is they try and break it and they did.
So it's still pretty fragile.
But yeah, so that's the high level
of a natural language cognitive architecture.
And it's already outdated, right?
Because we've got fine-tuning,
we've got the instruct series.
I did all this research and wrote the book
actually about a year ago now,
just before all this came out.
So it's already outdated.
That's why my research has moved on.
But yeah, so that's it at a high level.
Yeah, I mean, that's awesome.
And by the way, like David, you did do a good job.
Like the diagrams in this book are quite helpful.
Excellent.
Like in addition to the text, like it's very clear.
Like I was able to fully follow along with all these,
essentially these different modules for the whole system
of how a language model inspired AGI, quote unquote,
could actually be like how it would work.
And so I was gonna ask you,
so the prototype also was, you made it to that stage
and it has just some fun, interesting results.
So that's awesome.
What is the delta then between,
let's say even something like GPT-4
using the natural language cognitive architecture.
What's the delta between that and true AGI, right?
Like what's the difference there?
What skills, what patterns would you wanna see?
Yeah, so I mean, there's a lot
that I haven't figured out yet, right?
Task switching, for instance,
is one thing that I haven't figured out how to solve
even after reading on task by David Bader.
I, you know, that's one of the most complex things
that humans can do is keeping track of different tasks
and jumping back between them.
There's a whole litany of problems and limitations,
but the intrinsic limitation of GPT-3 and GPT-4
is they have no memory, right?
They're completely ephemeral.
And one of the most important things
for any intelligent being is that it's got a memory, right?
You know, you talk about, you know,
there's famous people in history
that had like, you know, photographic memories, right?
And so even just having a really good memory
is a really important ingredient to having intelligence.
And so that's where I think that like GPT-3, GPT-4,
other multimodal models,
they will never be fully AGI on their own.
They might be able to solve really great problems,
but they're not gonna be able to remember you
unless you add, you bolt a system onto the side,
some kind of database,
so that it can remember your interactions, right?
So that's one thing, another difference between,
like what you might imagine as a true AGI
or a full AGI is autonomy.
Because, you know, you, me, all of your listeners,
we all have some kind of self-determination.
I don't like to use free will
because that's too philosophical,
but we're all autonomous, right?
I'm an autonomous agent, you're an autonomous agent.
GPT-3 is not, it's transactional.
It just sits there and waits like a hammer, it's a tool.
It waits until you go pick it up and do something with it.
And so that's one of the things that I was aiming for
when designing natural language cognitive architecture.
I said, how can we make something that's fully autonomous
that can think on its own and make its own decisions?
And so in that respect,
I don't think a single neural network could ever be an AGI.
I think that in order to achieve true, full AGI,
it's gonna have to be some kind of cognitive architecture.
And so at a minimum,
you're gonna have the neural network and a database,
bare minimum.
You need something to store those memories,
to store those ideas and beliefs,
and then you need a way to interact with it.
And so that's why, actually,
that's why in natural language cognitive architecture,
the shared database is kind of the center of the design,
which you might recall, like you can use SQLite,
you can use Solar or whatever,
but you need something to store ideas,
memories and experiences.
I actually think that blockchain
will be a critical component to AGI
because what's the difference between a database
and like your brain?
No one can go in and change your memories, right?
Right, your memories are yours.
They are permanent unless you get brain damage
or Alzheimer's or something, but they're permanent, right?
No one can write a SQL query into your head
to get your memories or change them.
And so in order for us to realize a full AGI,
I think that it's gonna need kind of the same level
of trust in its own memories.
And so that's why I think that a blockchain
is gonna be critical to integrate
with these neural networks.
That might be the data repository for an AGI in the future
because imagine you have an AGI system
that is just using a SQL database.
Well, if you hack into that and you rewrite its memories,
you could send it off into,
it could become hostile, it could become broken.
Whereas a blockchain, the key feature of a blockchain
is that it's immutable, right?
So if we could give a machine autonomy,
so that's one ingredient, autonomy,
but then also a memory or a memory system,
which I think would probably be best as a blockchain,
I think then we'll be much closer
to like the fully realized AGI system.
And that's why I wanted to publish my book as fast as I did
was, okay, we're laying the groundwork, right?
But we need newer systems, we need a few better tools.
I hope that answers your question.
Yeah, I think a memory, I agree with you.
And it's just interesting like GPT-3's quote unquote memory
is limited to whatever it experienced at training time
and during fine tuning.
And sometimes its memory gets jumbled up
or it's rephrasing it, it's making stuff up
or it's sharing things that look truthful,
but they're actually not, right?
And so somewhere along the line, like just broadly speaking,
I think there needs to be research
on getting these models to, you know,
store that information in a truthful, accurate way
or even based on some perception that they may have
into some separate space where it can be retrieved.
And also these memories are critical
for decision making that process as well, right?
You draw on your memories, you're on past experiences.
And the important part is, I mean,
you're using the word database,
it's these are internal representations of memories, right?
That need to be stored.
And I have no clue what an internal representation database
would look like or how that would even work.
I, you know, I've got a machine learning researcher.
I think I've just a dreamer.
I can tell you what kind of product I would want
as a GT3 developer,
but I don't know if I could actually do it myself.
Yeah, I can do it myself.
That's why, you know, I got a prototype
and actually in the opening chapter of my book,
I say this is as much a recruiting tool as anything else.
Cause I need more smart people to help me on this.
I see, that's cool.
So one last point about memories is one advantage
of having an AGI that thinks in natural language
is interpretability.
If you like, yeah, we could create a multimodal model
that just stores vectors, right?
High dimensional vectors.
That's not interpretable.
But with natural language, cognitive architecture,
all the memories are in plain text.
I can, you know, when I had my model up and running
and one of the reasons that I don't
is because it's super expensive.
Like a 10 minute conversation using DaVinci cost
about $30 because of how much it was interacting
with the API.
But all of the memories, like every interaction,
you know, every input, output, all the prompts,
all the responses, all natural language,
which solves one of the biggest problems
that people have with the idea of AGI,
which is that it's going to be a black box.
So I think that that's one of the greatest strengths
actually of having GPT-3, which works in natural language.
And so you just record every transaction
and that makes it perfectly interpretable to any human.
Awesome. Yeah. Yeah, I would agree.
So I'm going to switch gears for a second.
So obviously you're really active
on the OpenAI community forums.
What thoughts did you have on the community at large?
Did you have any feedback?
How things could be improved, either community-wise,
platform-wise, and have there been any great experiences
you've had on the OpenAI community forums?
Yeah, yeah, no, it's a really great place.
It's been critical, actually,
because I don't know if you've experienced this,
but I go try and talk about GPT-3 to other people.
You go ask people on Reddit,
you talk to people who don't know what it is.
I even attended a deep learning meetup group
here in the Triangle area.
And I was trying to present my work,
my cognitive architecture work,
and everyone was more excited about just GPT-3 in itself
because no one had seen it yet.
And they're like, wow, how is it doing that?
And yeah, so like,
when you're as deep into GPT-3 as we are,
most people don't get it.
They don't know what it's capable of.
My girlfriend's finding the same thing.
She's finishing up her master's program,
and so she's shared some of her work with her peers,
with other students, and they're like,
wow, this is like AGI complete.
Why don't we just deploy this now?
And she's like, I told you, right?
This is remarkable technology,
but even the professors don't understand
how disruptive this technology can be.
And so because of that,
the open AI community is pretty much
the only place I can talk about this stuff.
It's the only place I can talk about my ideas
and share my progress and insights,
and for it to actually have an audience.
So that's kind of the cost
of being on the cutting edge, right,
as your audience gets smaller.
But it's definitely the place to be
if you want to get to the cutting edge.
Another advantage is they have the,
you can tag your posts where you say,
looking for a teammate.
And so at this point,
I've probably had maybe two dozen different calls
with people all over the world.
I've talked with people who are writing
language teaching apps, education apps,
Humano that I mentioned earlier.
And so I've had an opportunity
to collaborate with a dozen or two dozen teams
all over the world because of the open AI community.
And I've actually found a couple of startups
that I'm gonna actually get involved with
and try and help them bring their ideas to market.
And that just wouldn't have happened otherwise.
I wouldn't have found these people on Reddit.
I wouldn't have found them on Facebook or Twitter
because like I mentioned,
the ideas that I'm sharing
are so far beyond what is talked about
on the machine learning subreddit, right?
They're still talking about loss functions
and other things.
I'm like, no, we got to talk about cognitive architectures.
We got to talk about blockchain memories.
And everyone's like, what are you talking about?
So in order to have that right audience,
that's what I rely on the open AI community for.
Now, as far as things that could do better,
it could be more active.
And I'm not sure why,
but participation seems to come in waves, right?
And even now that it's gone GA, general availability,
I thought that it would explode, right?
That, hey, anyone can sign up on GPT-3 now.
Why is it not blowing up?
And I'm wondering if it's just that
maybe open AI needs a better marketing team
or a bigger marketing budget
because, I mean, and I know that they'll say that
they've got like a thousand or 5,000 startups
using their platform,
but I think that there's a lot of,
what's the word, like unmet potential
or latent potential, that's the word, latent potential,
because there's so many people with fantastic ideas
and use cases, and we really need to create
more of like a startup reactor thing.
Open AI, what was it?
I think about six to nine months ago,
they announced their $100 million open AI fund, right?
So they wanted to attract some more startups and stuff,
but even that, I kind of,
the community's kind of ghost town some days,
but I think today, I checked a few times
and there was three or four posts that had been updated,
but some days there's like 20, right?
It's just feast or famine.
So that's really the biggest problem is,
there's so much potential here and it's completely untapped
or almost completely untapped.
Yeah, but no, it's been indispensable for me
and hopefully these couple of startups that I'm involved with
might yield something really, really incredible.
Yeah, and thank you for sharing this.
I agree with everything that you're saying.
There is something about GPT-3.
I have noticed people who,
especially on the machine learning subreddit,
they're a little bit too educated,
a little bit too qualified, a little bit too skeptical.
And I can see a lot of machine learning researchers
not being interested in the nuances of prompt design.
They're just not.
And I've spoken to machine learning researchers
and many of them are like, what?
It's just repeating training data, right?
That's all it's doing.
And when you ask them, what are you doing?
They're repeating training data.
Their answer is no.
No, of course not.
Right.
And so having a space where you can talk to people
who have access, who have explored,
it's a valuable space is very important.
So were you on the Slack group back in the day
or did you show up when it was only the forums?
So when my application was accepted,
they had just announced that the Slack group was getting paned.
So I got on like two weeks before they shut it down.
So I was one of the first people on the new community board.
But yeah, so that phenomenon that you've mentioned
is I actually wrote a post about that recently on the forum
where a lot of purists,
whether you're a math purist or a computer science purist,
you're trained to think quantitatively in terms of numbers.
But GPT-3 doesn't produce quantitative data.
It produces qualitative data.
And so that's why you see people like artists and poets
and novelists using it because they're like, wow, this is great.
And I'm cross-trained, right?
I'm a technologist by day and a science fiction author by night.
So I use both so I can think qualitatively and quantitatively.
And in the academic sphere, there are classes that are meant
to teach computer science engineers to think differently,
to think more qualitatively.
But even still, some folks that have a real good natural affinity
for computer programming and math, that's just their nature.
Their nature is not to think qualitatively.
And so that is one of the biggest gaps, I think,
between where the researchers are experts and what's needed.
And so there was a post months ago where someone was asking,
like, OK, who should be on my team?
If I'm trying to build a business team to maximize my use of GPT-3,
I've got a front-end developer, I've got a back-end developer.
What else do I need?
I said hire a writer.
Hire someone who's a journalist or a fiction writer
because they are going to understand that qualitative data.
Hire a psychologist.
I've read plenty of books on psychology as well.
Actually, one of the folks that I'm working with is a psychology researcher
who wants to automate as much of the clinical psychology experience
or psychological research experience as possible.
Of course, he's not a computer guy, right?
He thinks in terms of emotions.
He thinks in terms of communication.
And so he gets it, right?
And it's funny because he read my book and he said,
oh, your cognitive architecture stuff,
it sounds like graduate level psychology.
And then someone else read it and they said,
this sounds like what I do as an expert marketer.
And I was like, yeah, I merge it all together.
So I think the simplest answer is you got to learn to think qualitatively.
And that's why I talked about reading and writing earlier.
Think in terms of emotions.
Think in terms of your own mind.
And you've got to start, not you, but the audience,
the folks who want to make the most of GPT-3,
they have to really kind of dig in and start thinking qualitatively
because qualitative data has just as much value as quantitative data.
But we have an entire generation of computer scientists and mathematicians
who are not really trained to think qualitatively at all.
And I think that's one of the biggest problems.
And I don't think open AI can solve that problem.
That's a much bigger systemic problem.
No, that's a great point.
I completely agree with you.
I think one of the reasons I've drawn to the open AI community is,
you know, these are, they tend to be developers who are also qualitative.
They're developers who have multiple skills, who are doing different things.
And so, I mean, I was quite critical actually of shutting down the open AI Slack group.
The activity was crazy on there.
I, you know, made friends through that Slack group.
And I understand at the time there was these downsides,
people kept asking the same questions.
They didn't quite have a spam problem yet.
It was kind of heading there, right?
But the activity was off the charts.
And you're right in terms of untapped potential.
Yes.
That we didn't even know how far the Slack group was going to go,
but they shut it down.
And there's discord solutions, there's alternatives.
With what we have now, I think open AI does participate.
There's, you know, some high level involvement.
They have sort of a dedicated member who writes honestly answers,
really, really thoughtful answers to a lot of questions.
Official answers as well, which I appreciate.
Yep.
I would just love to see the company really,
truly lean in to engaging with developers.
Like I have yet to see a single AMA ask me anything thread with the CEO of open AI.
Yeah.
And this is something I tried to push last year on Twitter.
Let's get the CEO on the community forums and let's,
let's ask questions and get responses from him.
And I just don't know why, why doesn't he show up?
I'm not sure if he's made a single post.
And there's just other things as well,
where I can just the difference between engaging really,
truly with your core audience and sort of, you know,
compartmentalizing it to a single employee.
Like, I don't know, this, this company led engagement is one thing versus department led.
Right.
And so there's just all these areas and certainly one of the other,
I guess more immediate suggestions I have for the community.
We've accumulated tons of insights and resources.
I think the community could benefit from more pooling of the best posts,
the best insights.
And I also want to give a shout out.
I think we need to encourage more shout out to duty to develop on there.
I've reached out to him privately on the open at community forums,
but he's done some amazing just write ups of his GPT three experiments and the prompts.
I'm sure you've seen them.
Oh yeah.
And of course there's, there's other members who participate every day.
So I'm just saying that now that this, the community is, is in another stage,
we, you know, we need to start thinking more about let's, let's,
what's curate some of the best moments.
Right.
I think that's, that's definitely one of the big pieces.
And so anyways, did you have any more thoughts in the community stuff or anything else?
Yeah, just, just an observation that I've, you know, I've worked at a,
at a number of companies of different sizes from, you know, a five person startup to,
you know, Cisco systems was the biggest company I've worked for,
which has had, had at the time, like 80,000 people globally.
And so I wonder if some of, some of what you're observing is just growing pains,
just normal growing pains, because often you'll have like the startup culture,
which is bootstrapping, right?
Where you just, you know, it's on Slack, it's on, it's on GitHub,
and you just kind of, it's fast and loose and quick.
And open AI, now that they've got an enterprise grade service,
they're having to develop their team.
You probably noticed they post like, hey, we're hiring, we're hiring, you know,
there've been at least two big hiring, hiring splurges in the last six to 12 months.
And some of those are just like generic IT guys, you know,
like kind of what I do from, from my day job or marketing folks.
So I think that, I think that they're probably working on solving some of those problems.
But also as a, as a nonprofit foundation, their budget is probably kind of thin.
So I'm wondering if, you know, their partnership with Microsoft could help some of that as well.
But you're absolutely right.
You know, there, there are still other things that they could be doing,
like, you know, maybe bring back Slack or, or a few other things.
So yeah, that was just final observation.
It might just be normal growing pains that they're working on solving.
It's definitely growing pains.
And the things I'm sharing, to be honest, it's a little bit more on the harsh side.
Like, I mean, they mean well, they mean well, right?
Like these are not bad people.
They are for profit.
They switched away from nonprofit.
Just, I just wanted to mention that.
But I think my, my, the reason I share this feedback is, for example,
the CEO, Sam Oltman, he didn't do the AMA thread on the open AI community forums.
He went to another website and did an AMA.
I can't remember if it was a, like a written form or just like a quick call.
Right.
Where apparently he shared all these details about what GPT for could be like and the future,
all the models may be multimodal in the future.
And I guess, you know, the, that thread has now been taken down.
And it's like all the things that were said were alleged.
And so I guess this is, this is really behind the scenes kind of stuff.
Like, but my criticism is they clearly have some capacity to engage.
Why are they not engaging where the audience is, right?
Right.
I had a tweet storm today where I just said, like last month, Sam Oltman was on a podcast
talking about meditation and how much meditation helps them.
This is a podcast I've never heard of in my life.
It's a business podcast.
And he had to explain to the guy what GPT three even is.
And so in my, like I tweeted, like, why haven't you been on my podcast?
Right.
Like you can, you can reach out to, you know, almost 8,000 GPT three open AI AI developers.
What are you doing talking about meditation?
Right.
So my problem is, is actually a priority problem.
I can see there is capacity.
I can see there are some priorities, but I think if you really lean in as a priority into
your developer community, there's certain ways you would, you would move.
Right.
And these, these media channels, there's people in the community.
There's, there's so many ways they could go about it.
And even linking a lot of the documentation to posts in the community forums.
I don't see why that's not a bad idea.
Right.
Like force people to show the community, show up to the community forums.
Right.
Walk them through some of the best threads.
These are ways in which we could like funnel more people in that direction as well.
That costs virtually nothing.
Right.
And so you need to also invest in the community forums that it needs to be building a community
is a company wide thing.
It's not something which can be outsourced to a single employee or overseen by PR.
It needs to come from a, from a, you know, it needs to come from the heart.
I know that sounds so corny, but anyways, clearly I, you know, I, I get too emotional
about this community stuff.
Yeah.
Anyways, these are, these are all things going on behind the scenes.
I apologize to all the listeners if they're like, like, this is cool, like cool story,
bro.
Like anyways.
So we're coming towards the end here.
I think I had just two broader questions.
So what are your thoughts on multimodal AI technology?
I think it's definitely going to be a critical component for the future.
Right.
I, I, I addressed that shortcoming in my book, natural language, cognitive architecture,
it thinks and takes in only text, which means, you know, speech, chat, whatever.
I think in order to have a fully robust, for instance, if you want to have a fully autonomous,
you know, robot that's going to wander around your house and help you out, it's going to
need to integrate audio and video.
And if you can do that in a single neural network, great.
I don't know that it'll be necessary to achieve AGI.
It might end up being, it might be one of those, it might be one of those like rare dead ends,
right?
Where because, you know, thinking visually, thinking, thinking in terms of sound, that
might not actually bias that much, right?
Because you can represent 95% of human thought in text, right?
It might take a little bit more, but it, you know, it might be more expensive.
And also how big are those models going to be, right?
Because if just, if just a text model of GPT-3 has to run on, you know, $7 million worth of
hardware or however much it is, you know, because it's got to run on a bunch of different
GPUs, if it's that expensive, how much more expensive, how much bigger is a giant multimodal
model going to be?
So that's, that's the biggest cost.
Obviously computer technology is going to get better over time, you know, and I think I
calculated it out, I think in 10 years, your average company could afford to run GPT-3
in-house.
In 20 years, you could probably run GPT-3 on your desktop.
And in 30 years, GPT-3 could run on your phone, right?
So that's a long timeline.
But in the meantime, we're going to be making bigger and bigger models.
And I'm afraid that there's going to be diminishing returns, right?
You know, people, right now people seem to think that it's going to follow an exponential
growth curve forever, but it might actually follow a sigmoid curve, right?
We might be at the point of fastest growth right now, but we're going to see diminishing
returns soon.
And so like, yeah, multimodal models are certainly going to have capabilities that GPT-3 doesn't.
But for the sake of, for the sake of like, if you want to create a self-improving chatbot,
GPT-3 and Codex might be enough, or at least, you know, that's, that's, that single mode
technology.
There was another thought, but it ran away.
Sorry.
But yeah, those, those, that's kind of, that's kind of my big take is there, there could
be benefits, but there's going to be costs too.
So we got to be cognizant of that.
Yeah.
And there might not be enough compute in the world.
They're not, there might not even be enough energy or we may like consume all energy ever
produced to make, to train a single model.
And then we may be able to run it inference for like three seconds.
Right.
And then it just shuts down the global power system or something, right?
But can you see yourself, let's say the technology exists, cost considerations aside, can you
see yourself perhaps making movies?
Can you see yourself, you know, giving your book to a multimodal model, having, have it
generate a documentary based on it or some marketing material?
What can you see yourself doing with, you know, the, the, the multimodal model of your
dreams?
Yeah.
So, you know, kind of the thought experiment that I did was, okay, well, we've got, you
know, how much, how much data is on YouTube?
I think it's like a thousand years or 10,000 years worth of video on YouTube.
And of course, it's many, many, many terabytes.
You know, so it's like, that's, that's way more training data.
You know, if GPT three was trained on less than one terabyte of data and, you know,
YouTube is approaching like the Yota bite scale, right?
That's, that's an insane amount of data.
So, okay, let's say you feed that in.
And so you got audio video, you've got text, you've got all the comments and you end up
with like a model trained on, on all of YouTube data.
Okay, cool.
What can you do with that?
Like, I can't even imagine, right?
Because GPT three today is almost capable of writing screenplays.
Right?
So if you have a model that's trained on, you know, all text data, all audio data, all
video data, you say, Hey, write me a screenplay.
Right?
I actually, I, and near the end of my book, I kind of have a chapter of speculation.
And I say, what if, what if you have this model and you say, give me season two of Firefly?
Right?
Like, you know, you could, you could just keep watching whatever show you want.
You say, give me Game of Thrones, but give me a different, you know, season eight, give
me season, you know, different season eight and season nine and 10.
Right?
So I kind of imagine that one possibility is hyper personalized entertainment.
And of course, like that might be 30 years away just because of like you said, the energy
intensity of this task.
But I, conceptually, it's possible, right?
You can hop on GPT three today.
Use the instruct model and say, write a screenplay for, you know, Firefly season two, and it'll
try, it'll get close.
And so then if you can take that text output and feed it into a multimodal model that can
translate text to video, why not?
You know, and Adobe actually, I don't know if you've seen it, but Adobe is, is already starting
on that where they're like inferencing, um, like, uh, what, what's the term?
They're like imputing the sound so you can put in a soundless video and it'll generate
the audio sound effects for you or vice versa.
It's really cool.
And so I think it like a company like Adobe that they have a huge vested interest in mastering
audio visual technologies.
They might, you know, soon put out something where, you know, you put in a text description
and it'll give you like a three second clip, right?
So that you can use that for ad copy.
Well, this technology is going to continue improving over time.
So I kind of, I kind of see that as like, if I were Netflix, put it this way.
If I had the budget of Netflix or Amazon, I would be investing in this to, to, to write
hyper personalized, um, video, uh, like series or, uh, or novels, right?
Cause you know, Amazon's got the market cornered with Kindle, right?
And there's people that will read all day, every day, right?
There are people that consume every bit of like entertainment that's available.
So if you can generate that on the fly without, you know, having a studio, a big budget studio,
that would be, I mean, that would change entertainment.
You know, that, that's the metaverse.
Forget what Facebook is doing.
That's the metaverse where it's like, Hey, you know, I, I came up with my own idea for
Game of Thrones and I, I wrote, you know, I use this, uh, you know, GPT eight or whatever
to generate my own version of Game of Thrones.
Come watch it with me guys.
And you know, someone might say, Oh, I didn't like that ending.
And they go rewrite it and generate their own version.
You know, cause we share memes on the internet today.
What if instead of sharing memes on the internet, we end up sharing episodes of our favorite
anime or, you know, we, we, uh, Resurrect Battlestar Galactica, you know, whatever.
There's so many things that we could do.
Like if compute power was not a problem, then we get there, but we need like fusion
reactors to power this stuff.
Yeah.
Yeah.
And like Marvel for me is already kind of like this and my capacity to consume Marvel
as a, as a viewer, it appears is infinite.
So I'm excited.
I've called it in the past, like the multimodal Marvel cinematic universe.
Yeah.
And I, some of these shows like Loki, I don't know if you, if you watch like,
I haven't seen it yet.
Okay.
Okay.
I mean, it was six episodes.
If the, if it had been 30, I would have watched all 30 and enjoyed every moment of it.
If that quality was, I want to go deeper in these stories.
So I'm definitely excited for all my favorite universes, cinematic universes and
story wise as well to, to live on forever essentially through multimodal content and
maybe be personalized like you're describing as well.
Oh yeah.
So yeah.
Last question.
So we, you know, you know, we've talked about various things.
We've talked about codex by today, you know, multimodal stuff broadly.
Where do you see all of this stuff going?
Let's give a timeline, five, 10 years.
What are some of the, what's the direction we're heading towards?
What, what important capabilities will we have?
Why is this stuff important?
Yeah.
Um, five to 10 years from now, I think that we will have something that you could probably
call a fully functional AGI like as a service you could sign up for.
Um, you know, it might be chatbot based, you know, kind of based on natural language,
cognitive architecture.
Um, I calculated out like it's too expensive to run right now.
You know, if it's $30 for a, for a 10 minute conversation, that's way too expensive.
So the, the, the cost has to come down.
You know, if you just, if you just take the technology we have today, but make it cheaper,
there's so much potential.
Um, so, you know, then there was that idea about like self-improving, um, you know,
feedback loops, you know, integrating with DevOps.
I certainly think that a company like Atlassian, which is a major DevOps player,
um, probably within five to 10 years, they'll have something, um, integrated to,
to kind of help automate the development pipeline even further.
Um, I think that, of course, I could be wrong because we're kind of at this weird
acceleration point.
Um, I think, I feel like multimodal models like consumer grade multimodal models are
probably more than 10 years away.
Um, unfortunately, they're probably just going to be like toy sized because,
you know, there's, um, there's like a, a, a hypnogram, right?
I don't know if you've seen that one, but that's one of like the text to image
generators and they're, it's still not even photorealistic, right?
Getting a photorealistic text to image is still like, that's a little ways off.
And then the next step after that is text to video.
That's even further, right?
So that's, that's kind of where I think it's at.
I don't think we're going to hit an AI winter.
I know there's lots of people predicting that we're going to hit an AI winter,
but I think that we're actually still kind of in the acceleration point.
But again, I don't know if it's going to follow an exponential curve forever
or if it's a sigmoid curve.
So time will tell.
Yep.
And still lots to do in the meantime, like you're describing, even with UPT3.
Oh yeah.
Okay.
Yeah.
My, my answer is, I think all of this stuff is, is just converging to just
greater human potential.
In some sense, I'm not even necessarily interested in the AGI question,
although I think it's important.
I think just the, the exciting possibilities we'll have, even now that we have,
that we'll continue to have five to 10 years from now.
Um, so many more experiences, so many other things we'll be able to create
that weren't possible.
I think we'll have more people creating than ever before.
Um, it's a really, really exciting vision for humanity.
Right.
Not just, not just for you and I.
So anyways, so with that said, did you have anything you wanted to plug David?
Where can people find you online?
Yeah.
So, um, my personal site is a David K Shapiro dot com.
Um, and I've got, um, I have, I have a few projects up and coming.
Um, nothing out right now except for my book,
natural language, cognitive architecture.
Um, you can download it for free from my website.
Um, you can sign up for my newsletter.
So one of my upcoming books is called benevolent by design,
six words to safeguard humanity,
which is to address the control problem of AGI.
Um, so that's, that's, uh, that book should hopefully be out in the next six months or so.
Um, and that is, um, so that's one project.
I've got another nonfiction book.
Um, and then also my own podcast will be coming out soon.
Yeah.
Head over to my site, David K Shapiro dot com and sign up for my newsletter.
And you'll get, you'll get updated when these, when these come out, when they're available.
Awesome.
And David, you mentioned you're, you're looking for collaborators as well.
Yes.
For natural language, cognitive architecture.
So, uh, if you're a coder, you know, imagine product manager, researcher, uh, hit up David
and just connect if, if any of this stuff interests you.
Um, I've, I've, I asked, I spent like a couple, I think I spent like a few days trying to find you on Twitter.
I don't think you're quite on Twitter yet.
Uh, I encourage you, uh, David, of course, you know, you and I will connect after we'll put any other, other place people could connect with you.
There's the community forums.
I assume you have a GitHub account.
Yeah.
So we're going to put that in, in the show notes and in the YouTube description below.
Uh, so anyways, David, thank you so much for being here.
I wanted to personally thank you for all the awesome, awesome community contributions you've made on the open eye community forum.
Uh, you're just an essential person on there.
I've learned a lot from you.
Uh, you know, the insights you've shared, they're going to be there forever.
And I'm sure I can't imagine how many people you've helped.
Uh, and also about your book, I also just wanted to say to the audience, uh, David's done a great job making it really digestible.
Like it was a very, it was a breeze of a read.
I thoroughly enjoyed it as somebody who writes GPT three prompts and is into this ecosystem.
It was just, uh, very interesting to see, uh, how it, how it could be laid out, uh, in this broader system approaching this huge problem.
Um, and also I was able to even get the book for free.
Obviously I encourage people by the book support it, but it's, it's there.
It's ready.
I think, you know, David's goal here is to get the ideas out.
Um, and so, uh, anyways, so, uh, that's it for today's episode.
David, thank you so much again.
I really appreciate you being here.
Thank you.
Thank you for all the kind comments and, um, and you're quite welcome.
And so is everyone else.
That's why I'm here.
Awesome.
And, uh, quick, so my quick plugs, you know, at BAKZT future Twitter, Instagram, YouTube.com slash BAKZT future.
My newsletter, I'll put it in the description below and I have a Twitter spaces event coming in two days at noon.
Uh, a couple of people probably pulling up.
This is like a audio only event.
So I encourage audio podcast listeners, YouTube subscribers, pull up to the Twitter spaces event.
We're going to chat more about codecs and prompt design and some other stuff going on in the, in the space.
So anyways, thank you again for listening to multimodal by BAKZT future.
I'll catch you in the next one.
Bye.
