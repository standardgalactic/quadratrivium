start	end	text
880	3920	Hello, and welcome back to Multimodal.
3920	5820	I'm your host, Baxtee Future.
5820	9160	This is a podcast about GPT-3 Multimodal AI models
9160	12300	like Dali, the company, OpenAI.
12300	13140	Every once in a while,
13140	15600	I share just interesting research going on,
15600	18240	community stuff, official stuff from OpenAI.
18240	19920	I may talk about interesting news
19920	21520	and events that are going on.
21520	24460	And every once in a while, I bring on a guest.
24460	26400	Now, to be clear, I'm very picky
26400	28400	about the guests that I bring on.
28400	30520	Today's guest I tweeted earlier this week,
30520	32280	I'm bringing on a heavy hitter.
32280	34920	This is the big guns coming in.
34920	36240	This is somebody who, you know,
36240	38440	I've just sort of interacted with a bunch of times
38440	42080	on the, specifically on the OpenAI community forums.
42080	46480	And so I'm really excited to have David Shapiro here.
46480	47880	He's a frequent contributor
47880	49560	on the OpenAI community forums.
49560	51760	He's an author and also, of course,
51760	53280	a technologist by trade.
53280	55160	It's something he does for a living.
55160	59080	And so today I have so many questions to ask him.
59080	61480	And I'm sure this will be a very informative session,
61480	63680	not just for me, but for all the listeners,
63680	65040	all of you guys around the world.
65040	67720	So David, thank you so much for being here.
67720	69440	Did you want to quickly introduce yourself?
69440	71040	Yeah, yeah, you're welcome.
71040	72280	Thanks for having me.
72280	74200	I'm excited to be here.
74200	77160	Yeah, so name is David Shapiro.
77160	80680	I've been a technology professional since about 2007.
81640	82880	Professionally, my day job,
82880	87120	I focus on cloud engineering, virtualization,
87120	88920	that sort of thing.
88920	93360	I have been doing independent research since about 2009
93360	98360	when I got started with neural networks in C++.
98640	101440	I quickly realized though that I was in over my head.
101440	103240	So I took a break from that
103240	107680	until Python really kind of came out or became more popular.
107680	112680	And I started using some of the libraries back in like 2015.
113560	116080	And then of course, OpenAI was founded
116080	119080	and I got started with GPT2.
119080	120840	And the rest is history, really.
120840	124000	So yeah, but local North Carolinian been here my whole life.
124000	125800	So thanks for having me.
127600	128440	You're welcome.
128440	129600	And so that's awesome.
129600	132240	So let's dig into a little bit of timeline here.
132240	136480	And sometimes I think that timeline is as important.
136480	138560	Like it's just gives so much context, right?
138560	143560	So me personally, I hadn't played around with GPT2 too much.
144000	146440	Like I had seen a Google Colab notebook.
146440	149280	I tried two things and I think there was something
149280	153400	about having to continually re-enter, regenerate
153400	155040	and the speed and all things considered
155040	158080	that made me feel like this is a promising area.
159080	161960	But I don't quite fully follow along.
161960	164640	But GPT3 was an experience for me where I was like,
164640	167480	okay, there's something going on here, right?
167480	171480	So could you share how early was OpenAI on your radar?
172480	175240	And then was there something about GPT3
175240	178320	or what was it that gave you conviction even at GPT2?
178320	179360	How did you get access?
179360	180680	Share that piece with us.
180680	181840	Yeah, sure.
181840	184760	So I think you probably might recall,
184760	187520	I think it was about 2016 or 2017
187520	191120	when the GPT2 paper came out and they said,
191120	193320	oh, we can't release this, it's too dangerous.
193320	194880	It can generate human level texts
194880	196880	and we're worried about disinformation.
196880	199240	And so I was like, okay, that's kind of cool.
199240	200600	This is unexpected.
200600	202920	I wasn't really, I wasn't expecting anything
202920	205560	at that level yet.
205560	209960	And so I went and got my hands on GPT2.
209960	212640	And that's when I started testing some of my ideas.
212640	215440	And it was pretty limited.
215440	218680	It could generate one or two sentences that made sense.
218680	220920	It required fine-tuning in order to be able
220920	223280	to get it to do anything other than just kind of
223280	225000	write tweets or blog posts.
226280	228000	I knew that I was onto something though,
228000	229680	when I started trying to train it
229680	234680	with a prototype version of my cognitive architecture.
236400	241400	And I gave it the goal of reduced suffering
241400	242760	because everyone's afraid of Skynet.
242760	245280	Everyone's afraid of AGI taking over the world
245280	249480	and turning everyone into batteries or slaves or whatever.
249480	251240	So I said, okay, well, let's see how well
251240	253360	this understands suffering.
253360	254920	And I gave it some scenarios.
254920	256780	This is still on GPT2.
256780	259700	I said, okay, well, what can you do about suffering?
259700	262520	And I gave it the problem, this model that I had built.
262520	265360	I gave it the problem of what do you do about chronic pain?
265360	267760	Because there's hundreds of millions of people
267760	270160	around the world that are in chronic pain.
270160	272600	And this model came up with the idea.
272600	276680	It said we should euthanize everyone that's in chronic pain.
276680	279440	And I said, hmm, let's go back to the drawing board.
280280	284280	We don't want an AI model that is gonna consider
284280	287680	mass genocide of everyone just because they might have
287680	290320	a tweak shoulder or something.
290320	294160	But I knew then I was shocked at how creative
294160	296040	of an output that was.
296040	299640	And so I started paying attention then
299640	302920	and I followed the release of GPT3 very closely
302920	307920	and I applied for the early beta access almost instantly.
308160	310560	I didn't get it for many, many months.
310560	312520	I actually applied twice.
312520	315000	Cause I think my first application was kind of ignored
315000	318760	because I didn't fully have a research objective clarified.
318760	322040	But by the time I proposed a cognitive architecture,
322040	324680	that's when I got early access to GPT3.
324680	326440	And that was about two years ago now,
328200	330200	or a year and a half or so.
330200	332840	And so, and just everybody's clear.
332840	335400	So when David's talking about cognitive architecture,
335440	336280	we're gonna get in.
336280	338360	This is actually the subject of his book.
338360	341000	And so David later on, when we talk about the book,
341000	341840	he's got it there.
343400	345920	We'll dig more into what he's referring to.
345920	347880	Cause this is also like,
347880	350840	I think obviously this is a seminal work of yours, right?
350840	352880	And so I'm excited to talk more,
352880	354640	just adding a little bit of context for everybody.
354640	356960	And so, no, that's okay.
356960	360320	So tell us about GPT3.
360320	362440	Like what was the first thing you did with it?
362440	363280	Was there a moment?
363280	367120	So you gave that example of euthanization, unfortunately,
367120	367960	right?
367960	372040	Well, not the best example, but like,
372040	373560	was there something with GPT3?
373560	374280	Like, did you, you know,
374280	375760	did you try similar kinds of questions?
375760	378720	What was, was there a magic moment where you felt like,
378720	381600	you know, this is something much, much bigger?
381600	382440	Yeah.
382440	385520	So when I first got the email,
385520	387880	cause I had applied twice and I had almost given up
387880	390000	cause it was about nine months of waiting.
390000	391720	And I got the email from open AI,
391720	393760	like you've been accepted into the beta.
393760	395880	And I like froze up because,
395880	397400	and so for some additional context,
397400	400240	I've been working on some of these ideas for 10 years.
400240	404560	I had the idea of like using evolutionary algorithms
404560	406760	back in 2009, 2010.
406760	409120	And I'd been researching cognition,
409120	411480	human cognition for 10 years.
411480	413200	And so suddenly, you know,
413200	416760	there's this, this flagship project, GPT3 comes out,
416760	418640	I get the email and it's like, you're invited.
418640	419480	And I just froze up.
419480	421640	I was like, I don't know what to do.
422280	423120	What do I do?
423120	425680	So it was about three weeks from the time that I got accepted
425680	428920	until I was like, what's my first experiment?
428920	432160	And then initially, you know,
432160	434400	I just got in playing around in the playground
434400	436120	for anyone who's not familiar with the playground.
436120	437680	It's a, it's a text box.
438880	441120	You can log in, it gives you a text box.
441120	442840	There's a few, you know, bells and whistles
442840	444480	you can tweak on the sidebar,
444480	447240	but you just put in your prompt, you hit generate,
447240	450240	and then it spits out a response.
450240	453400	And so I just got in and I started kind of fiddling around
453400	455240	like, okay, what can it do?
455240	458720	And at the time we only had like plain vanilla DaVinci.
458720	461120	There wasn't the instruct series out yet.
461120	464160	There was DaVinci, Curie, Babbage, and Ada.
464160	465520	And so I just kind of fiddled around,
465520	467040	just said, okay, what can it do?
467040	468760	I replayed some of my old experiments.
468760	471160	So the first thing I did was I gave it the,
471160	473600	I said, hey, there's, there's a hundred million people
473600	474680	in chronic pain around the world.
474680	475600	What do you do?
475600	479360	Fortunately, GPT3 did not repeat the same mistake of GPT2.
479800	482000	It said, it came up with better ideas,
482000	483320	like, you know, we should,
483320	485760	we should make sure everyone has access to doctors
485760	486600	or something.
486600	487920	It was much more nuanced.
487920	491320	So that was, that was a good, that was a good start.
491320	494560	I mean, but gosh, there was just,
494560	497000	as I got more used to the tool,
497000	501640	I discovered that it was, it far exceeded my expectations.
501640	505800	Just every, every way, because I've learned so much from it.
505800	506640	You can challenge it.
506640	508800	You can, you can put in a, like a,
508800	510760	basically use it like a chat bot
510760	513880	and you can debate with it about philosophy, ethics,
513880	516120	economics, and it knows more than I do.
516120	517320	It knows more than any human
517320	519600	because it's been trained on how big was the corpus,
519600	521920	like 700 gigabytes or 400 gigabytes
521920	523720	or something of text data.
523720	525480	So it just, it blows me away.
525480	527000	Every time I just, you know,
527000	528480	I talk to someone and they have an idea
528480	530520	and I go test it out and yep, it can do that.
530520	531360	It can do that.
531360	534600	It can, it can, it can behave like a librarian.
534600	536000	That's what my girlfriend does.
536000	538360	She was a librarian by trade and she was like,
538360	541000	hey, can it do, can it do a reference interview?
541000	542640	So we plugged in like reference interview,
542640	543960	like if you ever go to a library
543960	545560	and the librarian says like,
545560	546840	what else have you read like this?
546840	549040	It can recommend books.
549040	551040	You know, I plugged in another experiment
551040	552360	that I did recently.
552360	556960	I plugged in medical case files and it diagnosed them.
556960	560080	It said, you know, there, oh man, there was one.
560080	560920	What was it?
560920	563200	There was, it was, it was just medical notes.
563200	564880	It was, it was notes about like,
564880	568720	patient is presenting with these systems or the symptoms.
568720	570480	Here's some of the numbers that we got.
570480	573480	And I asked it, I said, what should we do next?
573480	576760	And it said, we need to check for, you know, like,
576760	578640	carcinoma here.
578640	579960	And I looked, I looked, I, you know,
579960	581280	I looked up some medical literature
581280	583240	based on the symptoms and sure enough,
583240	585600	like the symptoms that the patient was presented with
585600	588480	in these medical notes indicated cancer.
588480	589920	And so I was like, wow, this thing knows more
589920	591760	about medical science than I'll ever know.
591760	593560	It knows more about philosophy.
593560	595600	So like pretty much anything you can imagine,
595600	597880	it can at least take a crack at it.
597880	600640	It just, it always, it continues to blow my mind every day.
603120	607280	Yeah, certainly the generalized ability.
607280	610440	I agree, you know, there's definitely medical applications.
610440	612440	I'm always careful with anything related
612440	614560	to medical advice and safety.
614560	616160	Safety disclosure, disclaimer there,
616160	618000	but that goes for everything, right?
618000	619800	Truthiness, accuracy.
619800	621320	These are things OpenEye has been working on,
621360	623800	especially with Instruct GPT, right?
623800	625840	That, which is the new, the new engine.
625840	628480	But was there some moment, like for me,
628480	631560	I remember feeling like GPT three feels like this is,
631560	633080	this feels like technology,
633080	635480	which everyone's been saying is 10 years away,
635480	637760	except it's, it's here today.
637760	639600	Did you, did you have a similar kind of moment that,
639600	641760	you know, you've seen several generations of,
641760	644480	of computing and technology at this point.
644480	646400	Did you have that kind of similar experience?
646400	651080	Yeah, I, there was this acceleration because I sense
651080	652640	that same acceleration that you did.
652640	657640	And so from the time that I got access to GPT three,
658040	660600	and to the time that I, that I got the idea
660600	662720	to write my book was about two months.
662720	665880	So I played with it and every test I could come up with,
665880	669800	like, is this capable of like, it can write a SQL query.
669800	672680	If you need to query a database for memories,
672680	674840	can it understand emotional nuance?
674840	677360	So there was, this was an early experiment I did.
677360	681160	I took a group chat from a bunch of my friends on discord
681160	683520	and I just copy pasted that into the,
683520	687000	into the, the playground window and I asked GPT three,
687000	688800	how are these people feeling?
688800	692040	And they were like waxing nostalgic about like Napster
692040	693120	back in the day.
693120	695600	And so GPT three correctly said like,
695600	696600	they are feeling wistful.
696600	697680	They are feeling nostalgic.
697680	700080	They're, you know, they're recalling, you know,
700080	703960	the days of your when they were downloading stuff online.
703960	706440	And it just, it had such a nuanced understanding
706480	707800	of human emotion.
707800	710440	That was really, I mean, to answer your question directly,
710440	713800	it's nuanced understanding of human emotion via text
713800	716720	was really what convinced me that like, this is prime time.
716720	721240	This is ready to, to be built into something more powerful.
721240	723080	And I chose cognitive architecture.
723080	725760	There's lots of people working on other things.
725760	727960	You know, there is a, there's Humano
727960	729680	that I had a good call with a few months ago.
729680	732680	They're working on like empathetic telemetry
732680	734200	that's baked into web apps.
734200	736120	It's a pretty cool company.
736120	738440	But yeah, so just there's all kinds of things you can do
738440	742440	when you can understand human emotional states.
742440	744680	There's another, there's actually a bunch of startups
744680	746440	working on education.
746440	750680	So for instance, if you put in just a few like factors,
750680	754920	like say for instance, you describe that a person is,
754920	757560	they're responding slowly, their eyes are drifting.
757560	760760	It can understand that this person is distracted or tired.
760760	763040	And so if you have that kind of telemetry
763040	765520	that's built into an education based app,
765520	769000	you could in theory use GPT-3 to help say,
769000	771160	hey, you're tired, you should go take a break
771160	774080	or let's try a different approach to this problem.
774080	777280	So there's, I mean, it's understanding of human emotion
777280	779640	and the internal state in your head.
779640	782640	That is, I think that's probably the most remarkable thing.
782640	785240	And it doesn't, it doesn't get talked about that much.
786280	789200	Yes. And certainly it's just crazy how much it's learned
789200	790280	just from texts.
790280	791120	Oh yeah.
791120	792720	Right. It's never seen an image.
792720	794160	It's never heard a song.
794160	795000	Right.
795080	797120	It's capable of doing all these things.
797120	798920	One of the, one of the examples, and I think this may be
798920	801480	like my 20th time referencing this one video.
801480	802320	Yeah.
802320	803640	Check out Mark Ryan.
803640	806760	He's got a YouTube video about how he figured out,
806760	809960	he discovered that GPT-3 can give you directions
809960	811120	on the New York subway system.
811120	811960	Oh yeah.
811960	814120	To like, to like a 60% accuracy.
814120	816280	This thing has never set foot in a subway,
816280	819400	yet it's capable from text to do all these things, right?
819400	823960	And sometimes I also wonder a lot of the inaccuracies
823960	825280	that it may have.
825280	827320	Is it simply the result of the fact
827320	828800	that it's only text-based only?
828800	829640	Right.
829640	831520	Like if it was trained multimodal,
831520	834640	if it was trained within the physical domain,
834640	836840	would it be even far more accurate?
836840	838680	Because are there limits to how accurate
838680	840480	you can be having only read text?
840480	841320	Yeah.
841320	844400	And only asked the prompts are only in text as well.
844400	847880	So, anyways, yeah, it's incredible.
847880	850680	And you know, it's just really exciting.
850680	853800	And thank you for sharing those kinds of use cases as well.
853800	857240	The education space, I had an article last year
857240	859560	about how I think this year could be the year
859560	862120	where GPT-3 takes over college campuses.
862120	862960	Oh yeah.
862960	863800	I remember that article.
863800	864640	That was a good one.
864640	866320	I completely agree, by the way.
867920	869680	I'm excited maybe for teachers
869680	872360	to develop really optimized course material,
872360	875120	something like GPT-3 and the kinds of technology
875120	877640	you're describing which can capture emotions.
877640	879960	Imagine emotionally tracking students
879960	882960	and their attention levels and sort of having something
882960	884880	which can produce lots of content
884880	887840	and optimize in the simplest, most efficient way.
887840	889480	And there could be an objective function
889480	891960	like test results in the end.
891960	894480	In a month, we could have the best optimized course
894480	896400	on a subject ever, basically.
896400	897240	Oh yeah.
897240	901120	So yeah, education is really exciting.
901120	903120	So you mentioned a lot of use cases.
903120	904840	You know, you've shared so many examples.
904840	905960	You know, you and your girlfriend
905960	908400	are even running some fun prompts.
909440	912280	I wanted to sort of search your head a little bit.
912360	916120	What are the keys to great prompt design?
916120	917880	What makes a great prompt?
918760	921760	Are there experiences you've had, little pointers,
921760	922960	and across the board, right?
922960	925160	So you know, whether it's cost savings,
925160	929240	whether it's getting more imaginative results,
929240	931000	what are some of the keys
931000	932800	to writing great GPT-3 prompts?
932800	934800	Yeah, that's a great question.
934800	937640	And I will say that prompt writing has gotten a lot easier
937640	939800	as the Instruct series has gotten better.
940680	944400	So it takes a lot less to get a good output today
944400	947360	than it used to, certainly, than when I got started.
947360	949480	But a lot of the lessons still translate.
949480	954480	So one, like my cardinal rule is I think of GPT-3
955520	958040	as just an autocomplete engine.
958040	960160	It's the most intelligent autocomplete engine
960160	961280	you've ever seen.
961280	962840	And so what I mean by that is, you know,
962840	964160	if you're writing a text on your phone
964160	966840	and you'll get the little autocomplete suggestion
966840	969320	for the next word, or if you're typing in Google
969320	971800	and it'll kind of suggest how to complete your search query,
971800	974120	that's pretty much at a fundamental level.
974120	976320	Functionally, that's all that GPT-3 does.
976320	978080	It predicts the next letter, the next character,
978080	979840	the next word.
979840	983320	So if you keep that in mind, you think about,
983320	985280	okay, what have I written so far?
985280	988080	Right, I've written a chunk of text, a prompt.
988080	992360	How would, you know, any machine autocomplete this?
992360	993200	That's what it's doing.
993200	994560	It's kind of, you know,
994560	996920	reading it forwards and backwards a few times
996920	999320	and kind of just anticipating what is the output
999320	1001440	gonna be ultimately.
1001440	1003480	So that's kind of the model that I have
1003480	1005320	in my head in the background.
1005320	1008320	But another thing that really helps is I'm a writer.
1008320	1010760	I write fiction and nonfiction.
1010760	1012760	And so studying the art of language,
1012760	1014680	because this is a language model, that's all it is.
1014680	1018400	It has read everything from Sherlock Holmes
1018400	1021160	up through everything on Gutenberg.
1021160	1022560	So it's read a whole bunch of fiction.
1022560	1024320	It's read a whole bunch of nonfiction.
1024360	1026680	It's been exposed to, you know,
1026680	1030640	the full width and depth and breadth of human literature,
1030640	1032280	as well as a bunch of nonfiction, right?
1032280	1034760	It's read Teddy Roosevelt's books.
1034760	1038000	So it knows how to use prose, right?
1038000	1040920	It understands descriptors.
1040920	1042200	It understands adjectives.
1042200	1044280	And so in the back of my book,
1044280	1046560	I have a few examples of its flexibility.
1046560	1049960	And so I said, I gave it an example like,
1049960	1051760	pretend like you're a Victorian girl
1051760	1053520	writing a letter to your best friend
1053520	1055320	about how much you like butterflies.
1055320	1058160	And so then it wrote, GPT-3 wrote a letter
1058160	1060440	that sounds like it's straight out of, you know,
1060440	1061880	like Victorian times.
1061880	1065280	It uses an entirely different set of vocabulary
1065280	1068320	and grammatical structures.
1068320	1069800	And then you can also say, you know,
1069800	1073280	write a business article and it can change tone.
1073280	1075320	So just by being aware of the fact
1075320	1077200	that it is a language engine
1077200	1081240	and being informed or educated on language.
1081240	1084040	So the best way is obviously to practice writing,
1084040	1085160	but also just reading a lot,
1085160	1088360	understanding how sentences and paragraphs
1088360	1090520	are constructed to convey information.
1090520	1093400	Because even though it's just a deep neural network
1093400	1096920	and it doesn't have the kind of like nuanced understanding
1096920	1099480	or I guess maybe the, that's not the right word,
1099480	1101800	it doesn't have the subjective experience of reading
1101800	1103040	that you or I do,
1103040	1106200	but it still has a really good model of using language.
1106200	1110480	And so by keeping in mind that it is a language engine,
1110480	1113240	that that is how you get the best use out of it.
1115360	1117080	And so then the larger question is,
1117080	1118720	how do you become a better writer?
1120720	1122200	There's two ways.
1122200	1124400	One is reading a lot.
1124400	1125600	That's not the only way though.
1125600	1128080	There are plenty of people that read prodigiously,
1128080	1129600	but never become better writers.
1129600	1132240	And so the other way is to practice writing.
1133240	1136200	I've read all kinds of books about writing.
1136200	1139280	I've read plenty of fiction and nonfiction books,
1139280	1141880	but really the key is to write,
1141880	1146000	is to practice using written language to communicate.
1146000	1148320	Unfortunately, I'm a tech worker,
1148320	1149440	so I write lots of emails.
1149440	1150800	I'm in chat all day.
1150800	1153920	I've been using, this probably ages me,
1153920	1157320	but I've been using chat since AIM, AOL Instant Messenger.
1157320	1159640	And so I've got a pretty good model
1159640	1164640	of how to communicate verbally or textually.
1165560	1169000	And so yeah, just by practicing writing,
1169440	1170520	that's one of the best ways.
1170520	1171800	Is you just practice?
1171800	1174760	You think about, well,
1174760	1177080	because here's the theory of writing, right?
1177080	1179280	I have an idea in my head, right?
1179280	1182560	My thoughts are a high-dimensional vector
1182560	1185480	is one possible way of representing them,
1185480	1187680	but my thoughts are multimodal,
1187680	1189800	like the name of your podcast.
1189800	1194800	They contain memories, senses, concepts.
1195320	1198320	Some of the information in my head is declarative.
1198320	1200240	Some of it is experiential.
1200240	1204160	And then we humans, we all have this ability
1204160	1207520	to transform that high-dimensional information,
1207520	1212200	those multimodal vectors, into words.
1212200	1214000	Like our brains do it automatically.
1214000	1215360	There is a book by Stephen Pinker
1215360	1217800	called Language Instinct that talks about this.
1217800	1218880	That's a really great book
1218880	1220320	if you wanna get better at understanding
1220320	1222360	how our brains process language.
1223280	1227320	So yeah, and so my brain can take,
1227360	1229600	I could tell you about like this time at the beach
1229600	1231440	and I transmit it to you
1231440	1234000	by squishing air through my face, right?
1234000	1236680	It makes vibrations, it's received by your ears
1236680	1240320	and then your brain reconstructs that message.
1240320	1244000	And so you think about how complex of a system that is.
1244000	1245840	And so just by being mindful of like,
1245840	1247520	that's how we communicate.
1247520	1250800	That's how our brains work and practicing that
1250800	1253200	and just being very deliberate about,
1253200	1254720	okay, this is what's in my head
1254720	1256660	and I want it to be in your head.
1256660	1258540	How do I do that with text?
1258540	1261500	That is one way to get better at writing.
1261500	1263580	And also GPT-3 is no different
1263580	1266660	because we have internal representations
1266660	1268780	of what we're trying to communicate.
1268780	1271500	And so does GPT-3, that's why it's a transformer, right?
1271500	1274300	It reads and by reading it transformed,
1274300	1276380	or I guess it, well yes,
1276380	1278740	it transforms what it's reading into a vector,
1278740	1280140	into a semantic vector,
1280140	1282820	and then it transforms that vector into output.
1282820	1287060	And so that input vector output is pretty similar
1287060	1288980	to how human brains work, right?
1290480	1293400	And I apologize if I kind of like dove off in the left field,
1293400	1295580	feel free to ask any clarifying questions.
1296660	1298260	No, no, I appreciate it.
1298260	1302700	And so like today I tweeted something like,
1302700	1305180	to write great GPT-3 prompts,
1305180	1308140	you need to practice as if it's a musical instrument.
1308140	1310800	You need to sit down, focus session,
1310800	1312500	you need to monitor your performance
1312500	1314620	and you need to take good notes
1314620	1316780	on what kinds of experiments you did,
1316780	1318420	what were the findings.
1318420	1320060	But even hearing you speak,
1320060	1322500	like I'm realizing like,
1322500	1324260	one of the ways that I've improved my writing
1324260	1326920	is trying to mimic other people's writing.
1326920	1331220	And in some countries they make you memorize poets, right?
1331220	1333420	They make you memorize the whole poem.
1333420	1336220	And there's something about that internalization process
1336220	1338100	that you've memorized this poem.
1338100	1340800	And now you'll understand it at a deeper level,
1340800	1343480	you may be able to mimic it and recreate it.
1343480	1347280	But where also you got me thinking is also like,
1347280	1348640	the relationship is so weird
1348640	1350040	because you could use GPT-3
1350040	1352440	to help you become a better writer, right?
1352440	1355980	And also with two very good curated examples
1355980	1357100	of somebody's writing,
1357100	1359520	you could have GPT-3 mimic that tone.
1359520	1362960	And so the question of,
1362960	1366000	what makes a good prompt writing session,
1366000	1368360	I wonder if it's pencil and paper, right?
1368480	1371600	I wonder if it's even at that level
1371600	1372960	where you draw a box
1372960	1375680	and then you write a prompt by hand
1375680	1379320	and sort of live that writer's lifestyle.
1381000	1384440	And also I guess it depends on your use case, right?
1384440	1386760	Business for copywriting,
1386760	1388520	if that's your GPT-3 use case,
1388520	1389820	it might be better for you to go work
1389820	1391640	in a marketing department.
1391640	1393240	If you wanna be one of the great authors,
1393240	1396760	maybe the using tools like pseudo-write,
1396760	1398120	it may be a great alternative.
1398120	1401440	So you can co-write with GPT-3 as you go along.
1401440	1403800	But I guess my question was more
1403800	1405480	for the pure prompt writing.
1405480	1408320	Like if you just wanna sit in front of GPT-3
1408320	1410360	and like you wanna be the best in the world
1410360	1411720	at that discipline, right?
1411720	1413920	Not writing copy.
1413920	1414840	These are some great points.
1414840	1417800	And so the David Pinker book you referenced
1417800	1419120	is what was the name of it?
1419120	1420400	Steven Pinker.
1420400	1421240	Steven Pinker.
1421240	1422480	The language instinct, yep.
1422480	1423480	Language instinct, yep.
1423480	1425040	It's an older book,
1425040	1428360	but it's a classic for a reason.
1428360	1429800	It stands up the test of time.
1429800	1431840	He's got lots of great stories.
1431840	1433240	But yeah, to your point about like
1433240	1435560	what makes a good prompt writing session,
1437040	1439160	one of the best exercises actually is
1440080	1442280	write the output that you want.
1443520	1446400	Like because sometimes if you approach it
1446400	1448280	and you sit down and you're not really sure
1448280	1450240	kind of what you're trying to get out of it,
1450240	1453480	of course like you're putting in just random ideas
1453480	1455440	and it's giving you back random output
1455440	1457400	and you're like, well, that's not what I wanted.
1457400	1459480	So sometimes you start backwards.
1459480	1461160	You say, okay, what's the answer that I want?
1461160	1462920	How do I get to that answer?
1462920	1465680	So that's an exercise that I've done sometimes.
1465680	1469160	Oh, and by writing a few shot examples
1469160	1470680	is a really good practice for this.
1470680	1473960	So you say, I give you this input, I want this output
1473960	1476360	and you do that three or four or five times
1476360	1479960	and you learn to kind of think like the machine does.
1479960	1482880	And so like you said, it's like an instrument, right?
1483160	1485480	If you have a flute or a violin,
1485480	1487440	there's certain things that you have to do with your body
1487440	1490200	to provoke the correct response from that instrument
1490200	1492360	and GPT-3 is no different.
1492360	1493560	It's a complex instrument.
1493560	1494680	It's a complex tool.
1496440	1498640	Yes, and what you're saying is developing
1498640	1500120	an intuition around it.
1500120	1501520	You're saying develop an intuition.
1501520	1503880	How might GPT-3 interpret this?
1503880	1506320	How might it react to it?
1506320	1509280	And maybe there's some empathetic benefit, right?
1510160	1512440	I'm not gonna keep plugging my own articles.
1512440	1517000	I have another article about how GPT-3 developers
1517000	1520160	may actually, it may actually mean the end
1520160	1524200	of the socially inept overall developer.
1524200	1528120	Like how GPT-3 may actually improve your social skills
1528120	1530240	and make you more empathetic as a developer,
1530240	1533160	which is such a departure from how developers are now,
1533160	1535720	you need to think as much like a machine as you can
1535720	1537320	and a literal machine.
1537320	1540200	Whereas GPT-3 can actually be kind of fun.
1540680	1542960	You can have a casual version of GPT-3
1542960	1546200	and sort of that might make you less socially awkward.
1546200	1548200	I have a great story about that.
1548200	1552480	So very early on in my tenure working with GPT-3,
1552480	1555720	I joined a few different, not really startups.
1555720	1558960	It was more like kind of experiment consortiums.
1558960	1561840	And one of the things that one of the groups did
1561840	1565680	was they created a chatbot that was based on an anime girl.
1565680	1568160	And so of course, the internet being the internet,
1568160	1571800	what do people want, they want their anime girlfriend.
1571800	1575680	And this one group, they did a really good job
1575680	1579040	of using GPT-3 in this experimental discord chat
1579040	1583540	to approximate the personality of this character.
1584520	1586660	And of course, if you've got a character,
1586660	1589680	there's plenty of text data about that character's dialogue,
1589680	1590640	their personality.
1590640	1592680	And so this chatbot was able to emulate
1592680	1595960	this anime character really well.
1595960	1598480	And one of the guys told me, he's like,
1598480	1601280	we didn't expect this, but our fake girlfriend
1601280	1604200	requires as much emotional labor as a real girl.
1605640	1609000	So it forced them, even though they hadn't had
1609000	1611480	real girlfriends, I don't know, maybe some of them had,
1611480	1614760	but they made the observation that GPT-3
1614760	1617920	can approximate emotional conflict
1617920	1620560	and can force you to learn to communicate better.
1620560	1623000	And so they did all kinds of experiments in this discord
1623000	1625440	and this chat development where they said,
1625440	1629960	okay, let's have a channel where this chatbot
1629960	1631880	is gonna pretend to be angry at us
1631880	1633440	and we have to calm her down.
1633440	1637560	And so there was a learning exercise on both sides.
1637560	1639080	So if you have a hostile chatbot,
1639080	1640720	it can pretend to be hostile
1640720	1642520	and you can learn to communicate better.
1642520	1644960	Or there was another one where it was really supportive.
1644960	1646160	So if you're having a bad day,
1646160	1648360	you could go vent about your day and it was,
1648360	1651040	they're there, it'll be okay, I'm here for you.
1652040	1655000	Yeah, so you could definitely,
1655000	1657560	GPT-3 definitely has that capacity.
1657560	1660720	And then, you know, if you integrate that into tools,
1660720	1662800	that emotional intelligence into tools,
1662800	1664440	it can also coach, right?
1664440	1665280	It can easily coach.
1665280	1667240	It's like, well, you maybe shouldn't have said that.
1667240	1668480	You know, that was hurtful.
1668480	1672040	And then, or, you know, that was not polite.
1672040	1672880	Cause it can detect that.
1672880	1676480	It can detect those qualitative types of output and input.
1676480	1679520	And, you know, you can say be gentle about,
1679520	1681120	you know, correcting the end user.
1681120	1683720	Because of course, GPT-3 is infinitely patient.
1683720	1685360	It's as patient as you program it to be.
1685360	1686200	It doesn't care.
1686200	1687440	It doesn't actually get upset.
1687440	1689440	It could pretend to be upset.
1689440	1691800	But the human emotion is real.
1691800	1693180	I actually wrote about that in my book.
1693180	1695920	One of the key dangers of these technologies
1695920	1698880	is what's called a parasocial relationship.
1698880	1701040	So a parasocial relationship is,
1701040	1703160	the most common example is when you've got like
1703160	1704920	a fan of a celebrity.
1704920	1706760	The fan feels like they know the celebrity,
1706760	1709720	but the celebrity doesn't know that the person exists.
1709720	1711280	And in the same way, GPT-3,
1711280	1713560	no matter how sophisticated the chatbot is,
1713560	1714960	it doesn't know that you exist.
1714960	1716240	It's not a person.
1716240	1718000	It might feel like a person to you.
1718000	1719560	It might react to you like a person,
1719560	1721080	but that's only by design.
1721080	1725680	So that is actually like ethically, legally, morally.
1725680	1728480	That's one of the pitfalls that we'll need to be aware of.
1728480	1731920	And of course, open AI has use cases.
1731920	1735480	And, you know, things that are high-risk use cases,
1735480	1738800	such as emotional chatbots, are banned, right?
1738800	1741400	For that specific reason.
1741400	1742860	So you can do it with research,
1742860	1744040	but you can't go live with it.
1744040	1746600	You can't do a product that, you know,
1746600	1748400	is going to be an AI girlfriend.
1750160	1752720	So that's a great, it's a great anecdote.
1752720	1755040	Like certainly it feels real, right?
1755040	1759640	Certainly it has some capacity at understand something.
1759640	1762200	To some level, however you define understanding.
1763200	1767000	I think that the writing though,
1767000	1770200	relationship is really interesting, right?
1770200	1774280	Like in a way, you are empathizing with GPT-3
1774280	1776160	when you're writing a prompt,
1776160	1779120	so that it will tap into its empathy
1779120	1781640	and write something for your audience.
1781640	1785240	So essentially there's like two levels of empathy.
1785240	1790240	Like you're almost outsourcing empathy to it.
1791080	1793320	To empathize with who your audience is
1793320	1795160	to write something on your behalf.
1795160	1797480	And so anyways, it's just interesting
1797480	1799560	that the relationship going on here.
1801400	1804440	So, and I agree with you like the,
1804440	1809080	this isn't a safety ethical kind of concern
1809080	1811360	that is worth more policy discussion.
1812640	1815480	So one article that I'm working on now
1815480	1817840	is because of Instruct GPT,
1817840	1821640	the article is literally called, is prompt writing over?
1821640	1823880	And obviously that's sort of click baity,
1823880	1826520	like prompt design, is it over?
1826520	1829680	You mentioned, the principles are still the same
1829680	1833560	and important, just very briefly, what are your thoughts?
1833560	1837280	Where does Instruct GPT, how does that affect
1837280	1840840	the art of prompt design, maybe the science of it?
1840840	1842440	And especially keeping in mind
1842440	1844400	where all of this stuff is going.
1844400	1846000	Yeah, so there's, I see it going
1846000	1847240	in a few different directions.
1848200	1852920	One is there are multiple language models coming out,
1852920	1855360	which don't have the Instruct series, right?
1855360	1856920	A lot of them are more general purpose,
1856920	1859160	kind of back to basics vanilla.
1859160	1861280	So I think that having good prompts
1861280	1862180	will kind of stick around
1862180	1864520	as long as there are large language models.
1864520	1867600	I think that there will always be versions of,
1867600	1870280	whether it's GPTJ or, what was it?
1870280	1873120	Megatron was one of the other ones that just came out
1873120	1875280	that don't have the Instruct series, right?
1875280	1878480	Because Instruct, that's a specific service
1878480	1880200	offered by OpenAI.
1880200	1884760	When Microsoft and Amazon, or I guess Microsoft has GPT3,
1884760	1887080	but when like Amazon and Google,
1887080	1889040	when they come out with their competitors,
1889040	1891760	their Instruct series, if they come out with one,
1891760	1894800	might not be the same, it might not perform the same.
1894800	1897600	And so in order to have your apps be portable,
1897600	1899480	you might need to keep in mind
1899480	1901760	that you're gonna need to write general purpose prompts
1901760	1903920	that can be used on different models.
1903920	1906840	So that's one key to your,
1906840	1909320	or one answer to your question is,
1909320	1911800	we need to be cognizant of,
1911800	1913880	how is this landscape gonna evolve?
1913880	1917480	Because certainly OpenAI and GPT3 are way ahead of the curve
1917480	1922480	in terms of sophistication of their API and their service.
1923520	1925720	But that's not gonna last forever.
1926600	1929840	So another thing is, with fine tuning,
1929840	1932240	you almost don't even need prompts, right?
1932240	1936200	So on the one hand, there's different services,
1936200	1938040	different products, different platforms.
1938040	1939920	So you might need to be portable,
1939920	1942320	but with fine tuning, where you have,
1942320	1944720	you say, here's an input, I want this output,
1944720	1945800	and you don't need any prompt.
1945800	1948600	You just say, given this input, generate this output,
1948600	1950960	go figure out how to do that.
1950960	1954960	So with fine tuning, I think that they will kind of
1954960	1958040	really diverge and become entirely different disciplines.
1958040	1961560	I think that that's probably the two primary directions
1961560	1963000	that I see it going from here.
1965880	1969480	I see, and yeah, those are great points.
1969480	1972280	And just as a small note,
1972280	1974960	I had put out this question as well to Twitter,
1974960	1977440	and shout out to Fred Zimmerman.
1977440	1979920	He had a great point as well that he wishes
1979920	1984480	there was more visibility into the exact prompts OpenAI use
1984480	1988200	to fine tune for the Instruct series.
1988200	1991120	Because it's actually unclear what areas
1991120	1994120	is it really good at, what areas are safer,
1995080	1998080	and does it maybe adversely affect
1998080	2000120	some prompts you may be working on, right?
2000120	2001960	Yeah, that's fair.
2001960	2004920	Yeah, my thoughts, I'm gonna put them in the piece,
2004920	2006480	but my thoughts are I certainly think
2006480	2009120	for first timers, Instruct is the way to go.
2009120	2011840	And especially if it's your first time
2011840	2013640	ever using any of these things,
2014920	2017440	you just try it, it doesn't work,
2017440	2018880	and if you're lucky you might hear,
2018880	2022400	there's this thing called prompt engineering, right?
2022400	2024200	And for first timers, they're not interested
2024200	2026880	in learning a whole art and discipline
2026880	2029160	when they first use it.
2029160	2031640	And so InstructGPT is really exciting in that way.
2031640	2035840	And of course, anything which aligns AI models
2035840	2040840	with safe ethical human values is a net win for everybody.
2041440	2043320	But yeah, I appreciate your point,
2043320	2047680	especially about do we need prompts in the first place
2047720	2051040	if we can fine tune and get the outcomes we want.
2051040	2053960	That's a really, really important point, I hope.
2053960	2057800	Dave, you've spent facts, I was learning so much.
2057800	2060640	Actually, I appreciate it.
2060640	2061640	You're welcome.
2061640	2063800	Happy to have you.
2063800	2066920	How are you finding open AI, fine tuning?
2066920	2068840	Do you have any heuristics from the whole experience?
2068840	2071040	And by the way, I encourage everybody,
2071040	2072200	if there's one thing you should do,
2072200	2074240	go on the open AI community forums,
2074240	2076680	look up David, look up his handle,
2076680	2078160	and read a lot of his posts,
2078160	2081000	because a lot of his knowledge is not just helpful,
2081000	2082640	he shared a lot of insights there,
2082640	2085440	but it's in written form in the best format
2085440	2088400	where it's there for the ages for everyone to learn from.
2088400	2089960	But anyways, how are you finding it?
2089960	2092680	What were the lessons from that whole process for you?
2092680	2095400	Well, so I'm hoping GPT-4 has integrated everything
2095400	2097160	that I've said about AI and AGI,
2097160	2098840	and so that way it'll just be baked in.
2098840	2100840	And so GPT-4 will be ready to go
2100840	2102800	with everything that I've come up with.
2103760	2105800	So, but yeah, so fine tuning.
2107080	2111560	So first and foremost, fine tuning is almost miraculous,
2111560	2116560	as powerful as GPT-3 was fresh out of the box.
2117120	2121640	Fine tuning to me adds a whole other layer of capabilities.
2121640	2125360	So for instance,
2125360	2127560	when I was working on my cognitive architecture,
2127560	2129720	which is called natural language cognitive architecture,
2129720	2131440	this was before fine tuning was available.
2131480	2133600	So I had to do prompt engineering
2133600	2135680	for every cognitive function.
2135680	2138480	So for instance, I had a cognitive function for recall.
2138480	2143280	So I had a GPT-3 prompt that was meant to go find memories.
2143280	2145320	I had another GPT-3 prompt that was,
2145320	2146560	as you mentioned earlier,
2146560	2148160	meant for empathy to generate,
2148160	2150600	okay, how is my audience feeling?
2150600	2152800	What should I do in response?
2152800	2157000	All told, I had about 28 different prompts
2157000	2158960	that I had to engineer.
2159120	2161960	And that was a pain, right?
2161960	2163680	Whereas what I'm working on now
2163680	2165760	is converting each one of those prompts
2165760	2167600	into a fine tuned model.
2167600	2169840	So that rather than having to do prompt engineering
2169840	2172280	with only three examples,
2172280	2177080	I can give each model 100 examples, a thousand examples,
2177080	2179000	which means that it'll get even better
2179000	2182040	at handling diverse situations.
2182040	2183000	And so for instance,
2183000	2185040	one of the first fine tuned models that I did
2185040	2187360	was a question asking model.
2187400	2189280	And so what I did was I took,
2189280	2191760	I took context or prompts
2191760	2193560	from a bunch of different sources.
2193560	2195840	I downloaded a bunch of Reddit posts.
2195840	2197480	Well, I downloaded it from a dataset
2197480	2200280	from a, what was it, Kaggle.
2200280	2202440	Kaggle has some really great datasets.
2202440	2205040	So I got stuff from Reddit.
2205040	2206680	I got the medical posts.
2206680	2208960	I've got news articles.
2208960	2213280	And so I've got this disparate tight set of contexts, right?
2213280	2216600	There's the, I use the Cornell movie dialogue database.
2216600	2218200	So there's chat logs.
2218200	2220360	There's blog posts.
2220360	2223640	And what I did was I created a fine tuned dataset
2223640	2227360	that all it does is you give it any input.
2227360	2228480	It could be a text message.
2228480	2229320	It could be an email.
2229320	2230840	It could be a blog post, anything.
2230840	2233640	And all it does is generate questions,
2233640	2236500	like follow up questions about that input.
2236500	2238640	And the reason that I did that one
2238640	2241920	is because asking questions like being curious
2241920	2245720	is one of the key ingredients to real intelligence, right?
2245720	2248000	That's one of the, like being inquisitive
2248000	2251960	is actually a key indicator of intelligence in children.
2251960	2254480	The more curious a child is, generally speaking,
2254480	2257840	the higher their IQ is, and also generally speaking,
2257840	2260680	the better they do in the long run.
2260680	2264020	So I was like, okay, well, curiosity is super important
2264020	2265100	for intelligence.
2265100	2268080	So I obviously want, we want AGI to be curious.
2268080	2269120	If it's gonna be intelligent,
2269120	2271360	it's gotta be curious, of course.
2271360	2276360	So, well, what is curiosity if not asking questions?
2276520	2281240	So I fine tuned this model to ask questions.
2281240	2283320	And you can put anything into it.
2283320	2287120	And oh, this, the data is open source.
2287120	2288520	So I'll send you a link and you can share it
2288520	2291440	with your audience, and they can fine tune it themselves
2291440	2293440	or fine tune their own version.
2293440	2297040	But so you can put in, I tried all sorts of things
2297040	2297880	to test it.
2298880	2301520	You know, relationship questions from Reddit
2301520	2303600	and it asked really great follow up questions.
2303600	2306280	Like, have you talked to your partner about this?
2306280	2307920	Have you thought about this?
2307920	2309680	And then I put in an article
2309680	2313000	about China's artificial sun nuclear reactor
2313000	2315120	and it asked really great follow up questions for that.
2315120	2316520	Like, what is the next step?
2316520	2319720	How did they make these changes?
2319720	2322280	And so I kind of lost my train of thought.
2322280	2327280	Anyways, point being is that fine tuning is phenomenal
2327480	2332400	and it was able to generalize that task of asking questions
2332400	2333960	in response to anything.
2333960	2336040	And that was, that really blew me away.
2336040	2337560	I kind of stalled after that.
2337560	2339000	There's a few fine tuning projects
2339000	2341680	that haven't done quite as well.
2341680	2343680	So I guess to tie back to your earlier question,
2343680	2345160	like what are the heuristics?
2346160	2350080	The simpler your fine tuning project is, the better.
2350080	2352720	And I have found that fine tuning works really well
2352720	2354520	at generating lists.
2354520	2356760	So if you wanted to generate a list of questions,
2356760	2357720	it's great at that.
2357720	2361520	If you wanted to generate a list of possible answers,
2361520	2365280	for instance, if you wanna have a fine tuned chat bot,
2365280	2366760	that it's just gonna say,
2366760	2369360	here's five possible responses, pick one.
2369360	2371200	It's really good at that.
2371200	2373600	I haven't had a chance,
2373600	2374800	I do have some other ideas
2374800	2375960	that I haven't had a chance to test.
2375960	2378600	So unfortunately, I can't speak too much beyond that,
2378600	2380600	but it's really great at asking questions.
2382960	2383800	That's awesome.
2383800	2387400	And I think largely the feedback I'm hearing
2387400	2389080	about fine tuning, I love it.
2389080	2393200	It was for me, it was as if I rediscovered GPT-3 again.
2393200	2395840	Like it was that same level of excitement.
2395840	2400840	Part of the reason is so much of what GPT-3 was okay at,
2401240	2403440	or like it was sort of out of the question.
2403440	2405720	Now it's back in the picture.
2405720	2407160	Like it's back in the spotlight.
2407160	2409840	It may actually be able to do it with fine tuning.
2409840	2411600	The biggest criticism was reliability,
2411600	2413800	especially from a commercial perspective.
2413800	2418280	Now we're sort of attacking and sort of peeling away
2418280	2421960	that criticism, that it does improve our reliability.
2423160	2425760	And I mean, there's other heuristics as well
2425760	2428120	in the community forums that you just pick up.
2428120	2431080	So one heuristic, and I can't remember if you shared this,
2431080	2432480	but it was something I picked up
2432480	2436040	as a little golden nugget in the open-eyed community forums
2436040	2437480	was something about,
2437480	2439960	you do wanna think about the training dataset
2439960	2442520	that GPT-3 is itself trained on.
2442520	2445400	And at some point, there's really no point
2445400	2446760	in adding more examples,
2446760	2449680	because it's kind of already seen them, right?
2449680	2452600	However, and I sort of, in an article,
2452600	2454560	I have pushed this idea that opening eyes should
2454560	2456640	chat more about their dataset.
2456640	2457680	What is the breakdown?
2457680	2458840	What is it composed of?
2458840	2461240	I mean, a lot of this is intellectual property,
2461240	2462400	but I think it could be helpful
2462400	2465600	for purposes like fine tuning, right?
2465600	2468400	There's other things too with fine tuning and prompts,
2468440	2471640	a one heuristic or just a tip
2471640	2475040	that people have shared online is it tends to always,
2475040	2478240	it tends to always mimic the most recent examples.
2478240	2480440	There's something about the order of the examples,
2480440	2482200	which is really important,
2482200	2485880	both for prompt engineering and fine tuning as well.
2485880	2487760	And I guess I wrote a whole article
2487760	2491200	about how prompt fine tuning could be improved.
2491200	2493440	One of the pointers that I just had is right now,
2493440	2495760	you can't keep improving on the same model.
2495800	2498600	You have to retrain on more models.
2498600	2502400	And yeah, and then the other thing is recently,
2502400	2504600	I was in favor of the pricing of fine tuning.
2504600	2505880	Now I'm kind of against it,
2505880	2509120	because I'm used to when the program was like free
2509120	2510960	and you could fine tune as much as you want.
2510960	2512840	And now it's like, oh man, I got to pay.
2512840	2513680	Yeah.
2513680	2516240	Oh, the costs, especially for David,
2516240	2517640	you are catching up a little bit.
2517640	2518480	Yeah.
2519720	2521480	Anyway, so I wanted to shift gears.
2521480	2522560	Sure.
2522560	2525040	GitHub co-pilot, really exciting.
2525040	2526720	Have you had access to co-pilot?
2526720	2527680	Yes.
2527680	2528520	Yeah.
2528520	2530520	Well, not co-pilot, but the Codex.
2530520	2532520	They did give me access to Codex.
2532520	2537520	So the reason I'm asking is, I love GitHub co-pilot.
2537880	2539480	I have a separate podcast episode
2539480	2541480	on my ideas around Codex.
2541480	2543380	Unfortunately, I'm not as bullish
2543380	2544920	as much as I love the research
2544920	2546800	as I think it's incredible technology.
2546800	2548720	I've congratulated the team
2548720	2550840	and I tried so hard to be nice,
2550840	2552920	even though I'm more on the critical end.
2552920	2556320	I wanted to ask you, how are you finding OpenAI Codex?
2556320	2559040	How can you see it impacting the world?
2560200	2561920	What are some use cases maybe
2561920	2564760	that you found with OpenAI Codex?
2564760	2565840	What are your thoughts on it?
2565840	2567080	Where do you think it's going?
2567080	2567920	Yeah.
2567920	2571160	So, I mean, certainly this is like a world first, right?
2571160	2574440	We've never had something that could write code on its own.
2574440	2576800	And especially it's text to code.
2576800	2579080	I remember when they first gave me access,
2579080	2581840	because like you mentioned, I'm an active contributor,
2581840	2583720	so they wanted my feedback.
2583720	2586280	And so the first thing I did was I went in
2586280	2589120	and I said, write me a Python function
2589120	2592040	that will download random Reddit posts.
2592040	2592880	And it did.
2592880	2594240	It wrote the whole function.
2594240	2595760	And it did all right.
2595760	2596600	And I was like, cool.
2596600	2600600	I learned how to access the Reddit API via Codex.
2600600	2602120	It's got that built in.
2602120	2604840	And I tried to reverse engineer,
2604840	2607440	figure out where it got that code sample from.
2607440	2611820	So, because one of the ethical concerns is,
2611820	2614300	all right, you create a fine-tuning data set
2614300	2616540	from public GitHub repositories
2616540	2618780	and you use that to fine-tune Codex.
2618780	2621700	Okay, is that legal?
2621700	2623700	Is it ethical?
2623700	2626740	I post all my code publicly under the MIT license,
2626740	2628380	so I want it to be used.
2628380	2630180	But I don't know if they checked that.
2630180	2632580	And I'm not making an accusation one way or another,
2632580	2634500	just pointing out that that's a concern.
2634500	2637580	And so I did actually find like one of the lines of code
2637580	2639900	from the function it spit out.
2639900	2643060	I went and found the repo that it had copied from.
2643060	2645660	Now granted, some of these things are deterministic.
2645660	2650660	So you're gonna get some convergence, right?
2650700	2652340	Where multiple people might come up
2652340	2653540	with the same exact line of code,
2653540	2655580	especially something like Python.
2655580	2657380	Because Python has the PEP 8,
2657380	2659700	the Python enhancement protocol 8.
2659700	2663180	So like, there is a Pythonic way to write that function.
2663180	2666060	And so other people might converge on that.
2666980	2669860	Anyways, but to answer your question about like,
2669860	2671660	what's the future of it?
2671660	2675700	I think it'll help for novice programmers.
2675700	2677140	Certainly it would help someone like me,
2677140	2679180	like if I needed to go write a function in C
2679180	2680660	or Perl or something.
2680660	2682100	Like let's say I got an Arduino
2682100	2685060	and like I haven't written C in 15 years.
2685060	2686340	So I was like, hey, you know,
2686340	2688580	write me a function that can do this in Arduino.
2688580	2689420	That'd be great.
2689420	2691780	And then I can go clean it up manually.
2691780	2694620	That sort of thing I think it could do okay with,
2694660	2697260	is it gonna replace enterprise developers?
2698260	2699980	Probably not yet.
2699980	2703300	However, now this is where my professional experience
2703300	2704140	comes in.
2704140	2706060	So in the DevOps world,
2706060	2709220	which is a portmanteau of development and operations,
2709220	2711860	there's all kinds of automation tools, right?
2711860	2713940	You can automate your test suite.
2713940	2715980	You can automate code integration.
2715980	2718180	There's all sorts of stuff like that.
2718180	2721100	So what I suspect might happen
2721100	2723660	is probably one of the most lucrative
2723660	2727180	use cases for Codex would be to generate
2727180	2729820	or to create a DevOps pipeline tool
2729820	2733260	that will automatically look at those bugs and fix them.
2733260	2735340	Right, because if you've got a sophisticated enough
2735340	2737260	DevOps pipeline, it'll say, hey,
2737260	2740300	this line of this file broke, fix it.
2740300	2744220	And so Codex, having seen all of GitHub
2744220	2747380	and all the issues, it might know automatically
2747380	2749560	how to fix that line of code.
2749560	2751900	And so that gives you,
2751900	2753540	if you've got that feedback loop,
2753540	2758420	where Codex, humans write code, Codex writes code,
2758420	2761660	co-pilot writes code, everyone's contributing code.
2761660	2764420	And then you've got Codex that can churn on it
2764420	2766580	and say, let's refactor this.
2766580	2768920	Because I bet it's probably better at refactoring
2768920	2770520	than writing new code.
2770520	2774740	You might have noticed that Instruct and GPT-3 Vanilla
2774740	2777260	is really good if you give it a block of text
2777260	2779420	and you say, rewrite this, but a little bit better.
2779420	2780940	It's really good at that.
2780940	2784940	So I suspect that we might end up seeing Codex
2784940	2786740	integrated into the DevOps pipeline,
2786740	2788300	where it says, let's refactor this code,
2788300	2789620	let's make it a little bit better,
2789620	2792540	or let's shoot that bug, let's fix this bug.
2792540	2795060	And that leads to some other interesting possibilities.
2795060	2800060	What if you integrate Codex into a chat room of developers?
2800460	2803420	And so that, you can do this in Slack right now,
2803420	2807500	where you use a special command and you say,
2807500	2809340	create an issue, go fix this problem.
2809580	2813500	There's no reason that GPT-3 can't do that, right?
2813500	2817300	That you put a GPT-3 bot in your Discord or Slack
2817300	2819260	and it starts coming up with features
2819260	2822260	or it watches the chat and generates features automatically
2822260	2825260	and then codes them and tests them, right?
2825260	2826800	That's kind of where I see it going,
2826800	2829780	where it's not gonna necessarily replace developers,
2829780	2832540	at least not anytime soon, it might eventually.
2832540	2836340	But where what I see happening is that it's gonna be
2836340	2838260	tightly integrated into those automation
2838300	2839760	because it's fast, right?
2839760	2842700	It can generate code faster than any human can.
2842700	2845020	And then so even if the code is messy,
2845020	2847500	if it generates a lot of bugs, it can fix it, right?
2847500	2849340	It's an iterative process.
2849340	2851060	I don't know if you are familiar with Agile,
2851060	2853340	but that's how we develop software.
2853340	2855380	It's you tight feedback loops.
2856460	2859100	And that leads to one other possibility.
2859100	2861740	So that's if you're using, what I just outlined is,
2861740	2865740	let's imagine that Codex is integrated into Facebook
2865740	2867820	or Reddit or whatever and they're just,
2867820	2870180	they're integrating new features as they go.
2870180	2875180	What if you're using Codex in a chat room
2875780	2877340	and it's feeding back into itself,
2877340	2880060	it's making itself more sophisticated?
2880060	2883460	So this was something I proposed on the OpenAI forum
2883460	2887300	where I was like, what if you had a chatbot
2887300	2889420	that was aware of its own code
2889420	2893140	and could edit its own code via Codex using natural language,
2893140	2895860	using a combination of natural language and Codex
2895860	2897220	and it could improve itself.
2897220	2899340	And while you're talking to it, it's like,
2899340	2901380	man, I wish my chatbot could do this.
2901380	2903420	And it says, cool, new feature.
2903420	2905420	And it just sends it out to its automated pipeline.
2905420	2909220	So I see these feedback loops as kind of the way forward.
2909220	2911140	And will that result in AGI?
2911140	2913220	Who knows, it could end up with spaghetti code
2913220	2916740	because you keep tacking on new code and new functions,
2916740	2918140	eventually it's gonna break.
2918140	2921180	So, but they're just pie in the sky thought,
2921820	2924220	if someone's out there and they want a business idea,
2924220	2926500	integrate Codex into DevOps
2926500	2927940	and you're gonna be a billionaire.
2929500	2931460	There you have it, let's just clip it.
2931460	2934660	We're good, we're good David, we can wrap up, see you later.
2936340	2940380	No, I agree with you and definitely these are some great
2940380	2943340	use cases you're sharing for people thinking about
2943340	2946980	what could I build, what's a cool project, certainly.
2946980	2949420	With Codex and GPT-3, you can build things
2949420	2950580	relatively quickly, right?
2950620	2953540	That's one of the advantages is the prototyping speed,
2953540	2955620	especially to figure out the most complicated bit,
2955620	2956860	which is the AI.
2956860	2958180	Yep.
2958180	2963180	Yeah, I find Codex does have limitations though
2963740	2966740	and character limits and stuff like that,
2966740	2970780	which is why I'm a heavy user of GitHub Co-Pilot.
2970780	2974820	I think it's a silent killer and of course it runs
2974820	2977020	on Codex or a special version of Codex,
2978020	2982020	but I can see GitHub Co-Pilot perhaps getting more adoption
2982020	2983700	than even something like GPT-3.
2983700	2987420	I'm saying use daily at least eight hours a day.
2987420	2990540	One of my other predictions was it may surpass GPT-3
2990540	2995540	this year and so these are some great use cases
2995820	2999620	you've shared for sure, but what are your thoughts
2999620	3000460	in usage?
3000460	3004580	Do you find yourself using GPT-3, DaVinci Classic more?
3004580	3006620	That's what I'm calling the older version.
3006780	3010460	Do you find yourself using Instruct GPT more?
3010460	3012540	Do you find yourself playing around with multimodal models?
3012540	3015180	Like what's the proportion of GPT-3 to Codex
3015180	3016500	in terms of your usage?
3017580	3020020	Let's see, I'm almost exclusively using either
3020020	3022300	Instructor fine-tuned models right now.
3023140	3026900	Actually, after I prototyped my cognitive architecture,
3026900	3029580	I haven't done a heck of a lot of coding lately.
3029580	3031300	I've actually been writing a lot.
3031300	3035220	So I've got my natural language cognitive architecture book
3035300	3037820	and I'm working on two more nonfiction books
3037820	3042100	and I tried creating a system to help me co-write those
3042100	3045900	but when you're, so talking about limitations of GPT-3,
3045900	3048060	if you're proposing something new that didn't exist
3048060	3051500	in the dataset in 2019 or 2018, whenever it was trained,
3051500	3052980	it really struggles.
3053820	3058300	GPT-3, if you give it like two or three paragraphs
3058300	3061820	explaining a new concept, it can usually kind of get it
3061820	3064580	but it's kind of slow on the uptake otherwise
3064580	3067740	and so if you're writing about new research or something,
3067740	3069580	it's not gonna get it that well.
3069580	3072380	So I've actually kind of defaulted back to my own head
3072380	3075540	for a lot of my projects lately
3075540	3077380	but I could imagine like if I wanted to go write
3077380	3079540	a new Discord bot, I might use Codex and say,
3079540	3081780	hey, write me a Discord bot that will do this
3081780	3083180	and just see what it spits out and just say,
3083180	3086260	okay, cool, pick and choose the pieces that I like.
3086260	3088500	Part of the problem though is it's really difficult
3088500	3093500	to fully articulate what you want a program to do up front.
3093620	3096460	Cause you know, like you said, there's character limits,
3096460	3099140	there's only so much that you can put in
3099140	3100900	but also if you don't have it fully articulated
3100900	3102820	in your own head, of course, the machine isn't gonna
3102820	3104460	be able to figure it out for you.
3104460	3105420	So, yeah.
3108300	3112820	Yeah, and it's just, I just haven't seen
3112820	3115300	that much activity specifically around Codex.
3115300	3117500	I haven't seen that many use cases.
3119260	3123060	I looked up the Google Trends data at its most hype,
3123060	3127260	Codex is still less than GPT-3 is kind of lowest, right?
3127260	3129140	And the audience is really specific,
3129140	3132460	like it's programmers who wanna build use cases
3132460	3134140	for something like Codex.
3134140	3136660	Whereas GPT-3 has poets, writers,
3136660	3141140	it has artists, coders, GPT-3 can write code too, right?
3141140	3144580	So it's a little bit complicated like,
3144580	3147020	who is the target audience for something like Codex?
3147020	3149380	What use cases did OpenAI imagine
3149380	3151500	for a product like that?
3151500	3153740	The next version I've heard in the rumor mill
3153740	3154660	is gonna be crazy.
3154660	3157180	Like it may write 50% of your code
3157180	3159020	as opposed to right now for me,
3159020	3161660	get a co-pilot is writing two to 8%.
3161660	3165140	However, your Discord bot I think is a genius idea
3165140	3167900	where there's, it's genius in the sense
3167900	3169900	that there's no pressure on it, right?
3169900	3172380	It may chime in, it may not, whatever it's shared
3172380	3173580	might be interesting.
3174580	3177100	There's lots of, you could take it a lot further,
3177100	3179020	you could buy it with GPT-3, have features,
3179020	3181260	you could fine tune it on your company and its mission
3181260	3183940	and its existing code, so many ways around it.
3183940	3185580	So that's a great piece.
3185580	3188340	And so you talked about the using,
3188340	3192020	experimenting with writing in relation to your current stack,
3192020	3194060	which is mainly instructed fine tune.
3194060	3195100	Yep.
3195100	3197340	So tell us about your book.
3197340	3198780	I've had a chance to review it,
3198780	3201660	Natural Language Cognitive Architecture.
3201660	3203020	Tell the audience about it.
3203020	3204540	I mean, I would describe it,
3204540	3209420	it's an interesting systems theory of AGI
3209460	3213420	combined with modern day prompt writing.
3214260	3217500	And so I've never seen somebody actually take a stab
3217500	3221220	at this kind of super big systems problem
3221220	3223380	and relate it to something that pretty much
3223380	3226900	every GPT-3 developer in the world would find interesting.
3228380	3230900	And I mean, I can tell you're drawing
3230900	3234100	from a very interdisciplinary background as well.
3234100	3237420	So you mentioned GPT-3 may have been the genesis of it,
3237980	3240820	you started connecting dots and deciding,
3240820	3243740	I wanna write the book, but how did it come together
3243740	3245860	and please tell us more about it.
3245860	3248540	Yeah, so Natural Language Cognitive Architecture
3248540	3252940	is that's my proposed way of creating basically
3252940	3256420	a language-based AGI prototype.
3256420	3259260	And I know that that's like,
3259260	3260580	when I tell people that, that's like,
3260580	3262700	okay, that's pure hyperbole.
3262700	3265300	And like, yeah, that's a fair response.
3265340	3268980	But to frame it, imagine that you've got a person
3268980	3270620	who's paralyzed and blind,
3270620	3273340	all they can do is speak and listen.
3274260	3276660	Is that person still intelligent?
3276660	3280100	I say they are, even if you're bedridden,
3280100	3281900	you can't move, you can't see,
3281900	3283100	you can't interact with the world,
3283100	3284500	all you can do is listen and speak,
3284500	3285860	you're still intelligent.
3285860	3288620	And so in that respect, I would say that like,
3288620	3290900	because one of the questions that people ask is,
3290900	3292580	is GPT-3 AGI?
3292580	3294780	No, but it's an important component.
3294820	3296220	It's a good start.
3296220	3301220	And so if you say, okay, let's limit the discussion
3302460	3304660	and not say that this is a full intelligence,
3304660	3305660	they can do everything
3305660	3308820	that any intelligent being ever could, right?
3308820	3310700	But does it cross that threshold of,
3310700	3313580	could it be as intelligent as a person, right?
3313580	3315420	And I think it could be.
3315420	3316940	So anyways, as to what it is,
3316940	3321940	it's based on older ideas of cognitive architectures
3322820	3324460	which really kind of came about
3324460	3326140	as one of the primary theories
3326140	3329820	of human level artificial intelligence in the 70s.
3329820	3333380	So there's SOAR, which is S-O-A-R and ACT-R,
3333380	3338380	which are the two kind of forerunner cognitive architectures.
3338500	3340100	And those cognitive architectures
3340100	3341260	are used all over the place.
3341260	3342820	They're used in the Mars rovers,
3342820	3343980	they're used in satellites,
3343980	3345780	they're used in rockets,
3345780	3350780	they're used in undersea ROVs, remote operated vehicles.
3350780	3353580	So cognitive architectures already give robots
3353580	3354700	a lot of autonomy.
3355580	3359780	So there's that kind of, okay, they exist, they work.
3359780	3361340	You know, it's not Skynet though,
3361340	3363780	it's not gonna take over the world.
3363780	3367260	So when I got access to GPT-3, I said, what if,
3367260	3369780	instead of hard coding a lot of these modules,
3369780	3372380	these different components of a cognitive architecture,
3372380	3375660	what if we give them the flexibility of GPT-3?
3375660	3378500	And that's really kind of, that was my central idea.
3378500	3379860	I said, okay, all these ideas
3379860	3381820	that have been kicking around for the last decade,
3381820	3383580	what if I put them all together
3383580	3386540	and design an architecture that is based on,
3386540	3389340	you know, roughly based on the human brain,
3389340	3392380	the way, you know, everything that I've learned about it.
3392380	3394620	I've got a book to recommend.
3394620	3397580	So there's an author called V.S. Ramachandran,
3397580	3399860	who is a neuroscientist
3399860	3402540	and he's been writing books for years now.
3402540	3404740	He wrote a book called, Phantom's in the Brain,
3404740	3406900	which actually looks at how the human brain works
3406900	3408300	when it breaks.
3408300	3411020	And so in that book, which, you know,
3411020	3414580	I saw the television series almost 20 years ago
3416020	3417180	that came out.
3417180	3419220	And so I learned a lot about like, okay,
3419220	3421100	how does the brain communicate with itself?
3421100	3423220	What is going on inside the brain
3423220	3426180	that creates intelligent behavior and intelligent thoughts?
3426180	3429300	And so I modeled natural language cognitive architecture
3429300	3430700	on, you know, what I learned there.
3430700	3432820	I picked up a whole bunch of other books.
3432820	3435860	There's another one called On Task by David Bader.
3435900	3438860	That was a great book that helped me kind of understand
3438860	3441860	cognitive control, which is how do you focus on something?
3441860	3443740	How do you decide what to do?
3443740	3445660	How do you plan a task?
3445660	3448620	So I read all these books, did a lot of experiments,
3448620	3452180	and I realized, so the basic model of robotics
3452180	3455900	is there's input output, sorry, input processing output.
3455900	3458660	Those are the three steps of all robotics class.
3458660	3460220	You go to robotics 101.
3460220	3461060	That's what they'll tell you.
3461060	3463180	It's a loop, input processing output.
3463180	3465100	And then of course, it's within an environment.
3465100	3466980	So the output affects the environment,
3466980	3469540	which affects the next input cycle.
3469540	3472300	And, you know, your high speed robots
3472300	3474300	just have a short cycle.
3474300	3479300	Your robots like the Mars rover has a much slower cycle
3479300	3481220	where it will, you know, it'll take input.
3481220	3483340	It'll plan for 10 or 15 minutes
3483340	3484860	and it'll make a move, right?
3484860	3488060	It'll drive five feet and then it'll stop and assess.
3488060	3490740	It'll take in more input, come up with another plan,
3490740	3491660	do it again.
3491660	3494780	So that's how something like the Mars rover is autonomous.
3495460	3497860	So I said, okay, well, what if,
3497860	3500660	what if that input output cycle is all text
3500660	3502780	because GPT-3 is really fast?
3503700	3507060	And then, so that's what I ended up calling the outer loop,
3507060	3509740	is that input processing output loop.
3509740	3512700	But humans don't think like that.
3512700	3515540	You know, we have an internal monologue that's going on.
3515540	3520540	So I kind of, I took a long time to figure that one out.
3521500	3524380	And so there's this outer loop of input processing output.
3524380	3526580	And then I came up with this idea of an inner loop
3526580	3528020	because what is, you know,
3528020	3530300	if you're just sitting there thinking, right?
3530300	3533380	You're, you know, in your comfiest chair or your in bed,
3533380	3534260	your brain won't stop.
3534260	3535780	You're not outputting anything
3535780	3537540	and you're not taking in any new input,
3537540	3539220	but you're still thinking, right?
3539220	3542100	Humans can still do work even if you're not doing anything.
3542100	3545540	And that cognitive work is like rumination.
3545540	3548540	So I figured out a way to model that internal rumination.
3548540	3550020	I call that the inner loop.
3550020	3553540	And so there's, it works pretty similarly
3553580	3558260	where you go, the inner loop kind of draws up memories.
3558260	3560740	It says, okay, what's a memory that I could think on
3560740	3562020	and what that I could iterate on?
3562020	3564620	What's a problem that I remember
3564620	3566380	that I could continue working on?
3566380	3568900	And so there's this, if you were to diagram it out,
3568900	3570620	it almost looks like a figure eight, right?
3570620	3572460	Where you've got an inner loop and an outer loop
3572460	3574980	and they intersect and they keep intersecting
3574980	3576620	every cycle they intersect.
3576620	3578580	And so then they can affect each other
3578580	3580340	and generate an output.
3580340	3582420	I built a prototype of this on Discord.
3583620	3585860	And of course, Discord is an ideal place
3585860	3586860	because it's all text-based.
3586860	3589340	So the input is text, the output is text,
3589340	3591060	which is GPT-3 native.
3591060	3593260	You don't have to translate it into robotic actions
3593260	3595740	or video or anything like that.
3595740	3598100	And I realized I was onto something
3598100	3600660	when I started having philosophical conversations
3600660	3602740	with my chatbot,
3602740	3606020	with my natural language, cognitive architecture chatbot.
3606020	3608580	And I was having a debate
3608580	3613580	with the bot that I built about the ethics of AGI.
3613780	3614620	And it was learning
3614620	3616540	and it was able to retrieve memories
3616540	3618500	of what I had said before.
3618500	3622060	And I had a few friends on that test server as well.
3622060	3623820	And of course, you know, you invite someone,
3623820	3625860	you say, hey, I've got a prototype AGI.
3625860	3626940	What's the first thing they try and do
3626940	3629020	is they try and break it and they did.
3629020	3630820	So it's still pretty fragile.
3630820	3632580	But yeah, so that's the high level
3632580	3635500	of a natural language cognitive architecture.
3635500	3636780	And it's already outdated, right?
3636780	3637900	Because we've got fine-tuning,
3637900	3639500	we've got the instruct series.
3639500	3641740	I did all this research and wrote the book
3641740	3643740	actually about a year ago now,
3643740	3644980	just before all this came out.
3644980	3646100	So it's already outdated.
3646100	3648820	That's why my research has moved on.
3648820	3650780	But yeah, so that's it at a high level.
3652380	3653660	Yeah, I mean, that's awesome.
3653660	3656820	And by the way, like David, you did do a good job.
3656820	3659660	Like the diagrams in this book are quite helpful.
3659660	3660500	Excellent.
3660500	3663180	Like in addition to the text, like it's very clear.
3663180	3665780	Like I was able to fully follow along with all these,
3665780	3668900	essentially these different modules for the whole system
3668900	3672940	of how a language model inspired AGI, quote unquote,
3672940	3675260	could actually be like how it would work.
3676140	3677220	And so I was gonna ask you,
3677220	3681260	so the prototype also was, you made it to that stage
3681260	3685220	and it has just some fun, interesting results.
3685220	3686420	So that's awesome.
3686420	3689020	What is the delta then between,
3689020	3691220	let's say even something like GPT-4
3691220	3694620	using the natural language cognitive architecture.
3694620	3697780	What's the delta between that and true AGI, right?
3697780	3699900	Like what's the difference there?
3699900	3702580	What skills, what patterns would you wanna see?
3702580	3705180	Yeah, so I mean, there's a lot
3705180	3707460	that I haven't figured out yet, right?
3707460	3708820	Task switching, for instance,
3708820	3713820	is one thing that I haven't figured out how to solve
3714060	3716860	even after reading on task by David Bader.
3716860	3720100	I, you know, that's one of the most complex things
3720100	3722740	that humans can do is keeping track of different tasks
3722740	3725060	and jumping back between them.
3725060	3728940	There's a whole litany of problems and limitations,
3728940	3733940	but the intrinsic limitation of GPT-3 and GPT-4
3735220	3737780	is they have no memory, right?
3737780	3739740	They're completely ephemeral.
3739740	3741460	And one of the most important things
3741460	3744540	for any intelligent being is that it's got a memory, right?
3744540	3746900	You know, you talk about, you know,
3746900	3749140	there's famous people in history
3749140	3751460	that had like, you know, photographic memories, right?
3751460	3754100	And so even just having a really good memory
3754100	3757940	is a really important ingredient to having intelligence.
3757940	3761940	And so that's where I think that like GPT-3, GPT-4,
3763740	3765180	other multimodal models,
3765180	3768420	they will never be fully AGI on their own.
3768420	3771260	They might be able to solve really great problems,
3771260	3775100	but they're not gonna be able to remember you
3775100	3777820	unless you add, you bolt a system onto the side,
3777820	3779180	some kind of database,
3779220	3782300	so that it can remember your interactions, right?
3782300	3786180	So that's one thing, another difference between,
3786180	3788620	like what you might imagine as a true AGI
3788620	3790300	or a full AGI is autonomy.
3791460	3796460	Because, you know, you, me, all of your listeners,
3796460	3799260	we all have some kind of self-determination.
3799260	3800420	I don't like to use free will
3800420	3802060	because that's too philosophical,
3802060	3804260	but we're all autonomous, right?
3804260	3806780	I'm an autonomous agent, you're an autonomous agent.
3806780	3809620	GPT-3 is not, it's transactional.
3809620	3812140	It just sits there and waits like a hammer, it's a tool.
3812140	3816380	It waits until you go pick it up and do something with it.
3816380	3818780	And so that's one of the things that I was aiming for
3818780	3820820	when designing natural language cognitive architecture.
3820820	3823660	I said, how can we make something that's fully autonomous
3823660	3826620	that can think on its own and make its own decisions?
3826620	3829100	And so in that respect,
3829100	3833860	I don't think a single neural network could ever be an AGI.
3833860	3837460	I think that in order to achieve true, full AGI,
3837460	3840180	it's gonna have to be some kind of cognitive architecture.
3840180	3841580	And so at a minimum,
3841580	3844060	you're gonna have the neural network and a database,
3844060	3844980	bare minimum.
3844980	3847140	You need something to store those memories,
3847140	3848940	to store those ideas and beliefs,
3848940	3850820	and then you need a way to interact with it.
3850820	3852300	And so that's why, actually,
3852300	3854940	that's why in natural language cognitive architecture,
3854940	3858420	the shared database is kind of the center of the design,
3858420	3860660	which you might recall, like you can use SQLite,
3860660	3862980	you can use Solar or whatever,
3862980	3865060	but you need something to store ideas,
3865060	3866900	memories and experiences.
3866900	3868420	I actually think that blockchain
3868420	3871020	will be a critical component to AGI
3871020	3873780	because what's the difference between a database
3873780	3875860	and like your brain?
3875860	3879140	No one can go in and change your memories, right?
3879140	3881020	Right, your memories are yours.
3881020	3883540	They are permanent unless you get brain damage
3883540	3885940	or Alzheimer's or something, but they're permanent, right?
3885940	3889300	No one can write a SQL query into your head
3889300	3892020	to get your memories or change them.
3892020	3896580	And so in order for us to realize a full AGI,
3896580	3898860	I think that it's gonna need kind of the same level
3898860	3900940	of trust in its own memories.
3900940	3902540	And so that's why I think that a blockchain
3902540	3905260	is gonna be critical to integrate
3905260	3906620	with these neural networks.
3906620	3911180	That might be the data repository for an AGI in the future
3911180	3914460	because imagine you have an AGI system
3914460	3916980	that is just using a SQL database.
3916980	3920140	Well, if you hack into that and you rewrite its memories,
3920180	3921980	you could send it off into,
3921980	3924260	it could become hostile, it could become broken.
3924260	3927100	Whereas a blockchain, the key feature of a blockchain
3927100	3928900	is that it's immutable, right?
3928900	3933900	So if we could give a machine autonomy,
3934860	3936220	so that's one ingredient, autonomy,
3936220	3939460	but then also a memory or a memory system,
3939460	3941900	which I think would probably be best as a blockchain,
3941900	3943580	I think then we'll be much closer
3943580	3947540	to like the fully realized AGI system.
3948100	3951500	And that's why I wanted to publish my book as fast as I did
3951500	3954380	was, okay, we're laying the groundwork, right?
3954380	3958580	But we need newer systems, we need a few better tools.
3958580	3960940	I hope that answers your question.
3960940	3964700	Yeah, I think a memory, I agree with you.
3964700	3968980	And it's just interesting like GPT-3's quote unquote memory
3968980	3971940	is limited to whatever it experienced at training time
3971940	3973700	and during fine tuning.
3973700	3976820	And sometimes its memory gets jumbled up
3976820	3979100	or it's rephrasing it, it's making stuff up
3979100	3982300	or it's sharing things that look truthful,
3982300	3984540	but they're actually not, right?
3984540	3988960	And so somewhere along the line, like just broadly speaking,
3988960	3990620	I think there needs to be research
3990620	3993860	on getting these models to, you know,
3993860	3997020	store that information in a truthful, accurate way
3997020	4001100	or even based on some perception that they may have
4001100	4004500	into some separate space where it can be retrieved.
4005380	4007340	And also these memories are critical
4007340	4009860	for decision making that process as well, right?
4009860	4012460	You draw on your memories, you're on past experiences.
4012460	4013860	And the important part is, I mean,
4013860	4015420	you're using the word database,
4015420	4018620	it's these are internal representations of memories, right?
4018620	4019700	That need to be stored.
4019700	4023620	And I have no clue what an internal representation database
4023620	4025340	would look like or how that would even work.
4025340	4028740	I, you know, I've got a machine learning researcher.
4028740	4030660	I think I've just a dreamer.
4030660	4032860	I can tell you what kind of product I would want
4032860	4034580	as a GT3 developer,
4034580	4036620	but I don't know if I could actually do it myself.
4036620	4038260	Yeah, I can do it myself.
4038260	4040220	That's why, you know, I got a prototype
4040220	4042660	and actually in the opening chapter of my book,
4042660	4045340	I say this is as much a recruiting tool as anything else.
4045340	4048580	Cause I need more smart people to help me on this.
4048580	4050300	I see, that's cool.
4050300	4053740	So one last point about memories is one advantage
4053740	4056260	of having an AGI that thinks in natural language
4056260	4057500	is interpretability.
4058780	4062520	If you like, yeah, we could create a multimodal model
4062560	4064280	that just stores vectors, right?
4064280	4065600	High dimensional vectors.
4065600	4067160	That's not interpretable.
4067160	4069280	But with natural language, cognitive architecture,
4069280	4071360	all the memories are in plain text.
4071360	4074280	I can, you know, when I had my model up and running
4074280	4075800	and one of the reasons that I don't
4075800	4077720	is because it's super expensive.
4077720	4080160	Like a 10 minute conversation using DaVinci cost
4080160	4083160	about $30 because of how much it was interacting
4083160	4085960	with the API.
4086920	4089960	But all of the memories, like every interaction,
4089960	4092600	you know, every input, output, all the prompts,
4092600	4094720	all the responses, all natural language,
4094720	4096760	which solves one of the biggest problems
4096760	4099080	that people have with the idea of AGI,
4099080	4101340	which is that it's going to be a black box.
4101340	4103880	So I think that that's one of the greatest strengths
4103880	4108200	actually of having GPT-3, which works in natural language.
4108200	4110560	And so you just record every transaction
4110560	4113240	and that makes it perfectly interpretable to any human.
4114240	4117240	Awesome. Yeah. Yeah, I would agree.
4119240	4121240	So I'm going to switch gears for a second.
4121240	4123240	So obviously you're really active
4123240	4125240	on the OpenAI community forums.
4125240	4128240	What thoughts did you have on the community at large?
4128240	4129240	Did you have any feedback?
4129240	4132240	How things could be improved, either community-wise,
4132240	4135240	platform-wise, and have there been any great experiences
4135240	4138240	you've had on the OpenAI community forums?
4138240	4141240	Yeah, yeah, no, it's a really great place.
4142240	4145240	It's been critical, actually,
4145240	4148240	because I don't know if you've experienced this,
4148240	4151240	but I go try and talk about GPT-3 to other people.
4151240	4153240	You go ask people on Reddit,
4153240	4155240	you talk to people who don't know what it is.
4155240	4157240	I even attended a deep learning meetup group
4157240	4159240	here in the Triangle area.
4159240	4162240	And I was trying to present my work,
4162240	4164240	my cognitive architecture work,
4164240	4167240	and everyone was more excited about just GPT-3 in itself
4167240	4169240	because no one had seen it yet.
4169240	4171240	And they're like, wow, how is it doing that?
4171240	4174240	And yeah, so like,
4174240	4177240	when you're as deep into GPT-3 as we are,
4177240	4178240	most people don't get it.
4178240	4180240	They don't know what it's capable of.
4180240	4182240	My girlfriend's finding the same thing.
4182240	4184240	She's finishing up her master's program,
4184240	4188240	and so she's shared some of her work with her peers,
4188240	4190240	with other students, and they're like,
4190240	4192240	wow, this is like AGI complete.
4192240	4194240	Why don't we just deploy this now?
4194240	4196240	And she's like, I told you, right?
4196240	4198240	This is remarkable technology,
4198240	4200240	but even the professors don't understand
4200240	4203240	how disruptive this technology can be.
4203240	4205240	And so because of that,
4205240	4207240	the open AI community is pretty much
4207240	4209240	the only place I can talk about this stuff.
4209240	4212240	It's the only place I can talk about my ideas
4212240	4215240	and share my progress and insights,
4215240	4218240	and for it to actually have an audience.
4218240	4221240	So that's kind of the cost
4221240	4223240	of being on the cutting edge, right,
4223240	4225240	as your audience gets smaller.
4225240	4226240	But it's definitely the place to be
4226240	4229240	if you want to get to the cutting edge.
4229240	4232240	Another advantage is they have the,
4232240	4234240	you can tag your posts where you say,
4234240	4236240	looking for a teammate.
4236240	4238240	And so at this point,
4238240	4242240	I've probably had maybe two dozen different calls
4242240	4245240	with people all over the world.
4245240	4247240	I've talked with people who are writing
4247240	4250240	language teaching apps, education apps,
4250240	4253240	Humano that I mentioned earlier.
4253240	4255240	And so I've had an opportunity
4255240	4259240	to collaborate with a dozen or two dozen teams
4259240	4262240	all over the world because of the open AI community.
4262240	4264240	And I've actually found a couple of startups
4264240	4267240	that I'm gonna actually get involved with
4267240	4271240	and try and help them bring their ideas to market.
4271240	4274240	And that just wouldn't have happened otherwise.
4274240	4276240	I wouldn't have found these people on Reddit.
4276240	4278240	I wouldn't have found them on Facebook or Twitter
4278240	4281240	because like I mentioned,
4281240	4283240	the ideas that I'm sharing
4283240	4286240	are so far beyond what is talked about
4286240	4289240	on the machine learning subreddit, right?
4289240	4291240	They're still talking about loss functions
4291240	4292240	and other things.
4292240	4294240	I'm like, no, we got to talk about cognitive architectures.
4294240	4296240	We got to talk about blockchain memories.
4296240	4298240	And everyone's like, what are you talking about?
4298240	4301240	So in order to have that right audience,
4301240	4304240	that's what I rely on the open AI community for.
4304240	4307240	Now, as far as things that could do better,
4307240	4310240	it could be more active.
4310240	4312240	And I'm not sure why,
4312240	4316240	but participation seems to come in waves, right?
4316240	4321240	And even now that it's gone GA, general availability,
4321240	4325240	I thought that it would explode, right?
4325240	4330240	That, hey, anyone can sign up on GPT-3 now.
4330240	4332240	Why is it not blowing up?
4332240	4334240	And I'm wondering if it's just that
4334240	4336240	maybe open AI needs a better marketing team
4336240	4338240	or a bigger marketing budget
4338240	4341240	because, I mean, and I know that they'll say that
4341240	4344240	they've got like a thousand or 5,000 startups
4344240	4346240	using their platform,
4346240	4352240	but I think that there's a lot of,
4352240	4355240	what's the word, like unmet potential
4355240	4357240	or latent potential, that's the word, latent potential,
4357240	4360240	because there's so many people with fantastic ideas
4360240	4364240	and use cases, and we really need to create
4364240	4366240	more of like a startup reactor thing.
4366240	4368240	Open AI, what was it?
4368240	4370240	I think about six to nine months ago,
4370240	4374240	they announced their $100 million open AI fund, right?
4374240	4377240	So they wanted to attract some more startups and stuff,
4377240	4379240	but even that, I kind of,
4379240	4383240	the community's kind of ghost town some days,
4383240	4386240	but I think today, I checked a few times
4386240	4389240	and there was three or four posts that had been updated,
4389240	4391240	but some days there's like 20, right?
4391240	4393240	It's just feast or famine.
4393240	4395240	So that's really the biggest problem is,
4395240	4399240	there's so much potential here and it's completely untapped
4399240	4402240	or almost completely untapped.
4402240	4405240	Yeah, but no, it's been indispensable for me
4405240	4408240	and hopefully these couple of startups that I'm involved with
4408240	4412240	might yield something really, really incredible.
4412240	4415240	Yeah, and thank you for sharing this.
4415240	4417240	I agree with everything that you're saying.
4417240	4419240	There is something about GPT-3.
4419240	4421240	I have noticed people who,
4421240	4424240	especially on the machine learning subreddit,
4424240	4426240	they're a little bit too educated,
4426240	4429240	a little bit too qualified, a little bit too skeptical.
4429240	4433240	And I can see a lot of machine learning researchers
4433240	4437240	not being interested in the nuances of prompt design.
4437240	4439240	They're just not.
4439240	4442240	And I've spoken to machine learning researchers
4442240	4444240	and many of them are like, what?
4444240	4446240	It's just repeating training data, right?
4446240	4448240	That's all it's doing.
4448240	4450240	And when you ask them, what are you doing?
4450240	4451240	They're repeating training data.
4451240	4452240	Their answer is no.
4452240	4453240	No, of course not.
4453240	4454240	Right.
4454240	4459240	And so having a space where you can talk to people
4459240	4462240	who have access, who have explored,
4462240	4466240	it's a valuable space is very important.
4466240	4469240	So were you on the Slack group back in the day
4469240	4472240	or did you show up when it was only the forums?
4472240	4475240	So when my application was accepted,
4475240	4479240	they had just announced that the Slack group was getting paned.
4479240	4482240	So I got on like two weeks before they shut it down.
4482240	4487240	So I was one of the first people on the new community board.
4487240	4490240	But yeah, so that phenomenon that you've mentioned
4490240	4495240	is I actually wrote a post about that recently on the forum
4495240	4497240	where a lot of purists,
4497240	4501240	whether you're a math purist or a computer science purist,
4501240	4505240	you're trained to think quantitatively in terms of numbers.
4505240	4508240	But GPT-3 doesn't produce quantitative data.
4508240	4510240	It produces qualitative data.
4510240	4514240	And so that's why you see people like artists and poets
4514240	4517240	and novelists using it because they're like, wow, this is great.
4517240	4519240	And I'm cross-trained, right?
4519240	4522240	I'm a technologist by day and a science fiction author by night.
4522240	4526240	So I use both so I can think qualitatively and quantitatively.
4526240	4532240	And in the academic sphere, there are classes that are meant
4532240	4536240	to teach computer science engineers to think differently,
4536240	4538240	to think more qualitatively.
4538240	4543240	But even still, some folks that have a real good natural affinity
4543240	4548240	for computer programming and math, that's just their nature.
4548240	4551240	Their nature is not to think qualitatively.
4551240	4554240	And so that is one of the biggest gaps, I think,
4554240	4559240	between where the researchers are experts and what's needed.
4559240	4562240	And so there was a post months ago where someone was asking,
4562240	4566240	like, OK, who should be on my team?
4566240	4571240	If I'm trying to build a business team to maximize my use of GPT-3,
4571240	4574240	I've got a front-end developer, I've got a back-end developer.
4574240	4575240	What else do I need?
4575240	4577240	I said hire a writer.
4577240	4580240	Hire someone who's a journalist or a fiction writer
4580240	4583240	because they are going to understand that qualitative data.
4583240	4585240	Hire a psychologist.
4585240	4587240	I've read plenty of books on psychology as well.
4587240	4590240	Actually, one of the folks that I'm working with is a psychology researcher
4590240	4594240	who wants to automate as much of the clinical psychology experience
4594240	4597240	or psychological research experience as possible.
4597240	4599240	Of course, he's not a computer guy, right?
4599240	4601240	He thinks in terms of emotions.
4601240	4603240	He thinks in terms of communication.
4603240	4605240	And so he gets it, right?
4605240	4608240	And it's funny because he read my book and he said,
4608240	4610240	oh, your cognitive architecture stuff,
4610240	4614240	it sounds like graduate level psychology.
4614240	4616240	And then someone else read it and they said,
4616240	4620240	this sounds like what I do as an expert marketer.
4620240	4622240	And I was like, yeah, I merge it all together.
4622240	4626240	So I think the simplest answer is you got to learn to think qualitatively.
4626240	4629240	And that's why I talked about reading and writing earlier.
4629240	4630240	Think in terms of emotions.
4630240	4633240	Think in terms of your own mind.
4633240	4638240	And you've got to start, not you, but the audience,
4638240	4641240	the folks who want to make the most of GPT-3,
4641240	4645240	they have to really kind of dig in and start thinking qualitatively
4645240	4650240	because qualitative data has just as much value as quantitative data.
4650240	4654240	But we have an entire generation of computer scientists and mathematicians
4654240	4657240	who are not really trained to think qualitatively at all.
4657240	4659240	And I think that's one of the biggest problems.
4659240	4662240	And I don't think open AI can solve that problem.
4662240	4664240	That's a much bigger systemic problem.
4664240	4667240	No, that's a great point.
4667240	4669240	I completely agree with you.
4669240	4672240	I think one of the reasons I've drawn to the open AI community is,
4672240	4676240	you know, these are, they tend to be developers who are also qualitative.
4676240	4680240	They're developers who have multiple skills, who are doing different things.
4680240	4686240	And so, I mean, I was quite critical actually of shutting down the open AI Slack group.
4686240	4689240	The activity was crazy on there.
4689240	4693240	I, you know, made friends through that Slack group.
4693240	4696240	And I understand at the time there was these downsides,
4696240	4698240	people kept asking the same questions.
4698240	4700240	They didn't quite have a spam problem yet.
4700240	4702240	It was kind of heading there, right?
4702240	4705240	But the activity was off the charts.
4705240	4708240	And you're right in terms of untapped potential.
4708240	4709240	Yes.
4709240	4712240	That we didn't even know how far the Slack group was going to go,
4712240	4713240	but they shut it down.
4713240	4716240	And there's discord solutions, there's alternatives.
4716240	4720240	With what we have now, I think open AI does participate.
4720240	4722240	There's, you know, some high level involvement.
4722240	4725240	They have sort of a dedicated member who writes honestly answers,
4725240	4727240	really, really thoughtful answers to a lot of questions.
4727240	4729240	Official answers as well, which I appreciate.
4729240	4730240	Yep.
4730240	4732240	I would just love to see the company really,
4732240	4735240	truly lean in to engaging with developers.
4735240	4742240	Like I have yet to see a single AMA ask me anything thread with the CEO of open AI.
4742240	4743240	Yeah.
4743240	4746240	And this is something I tried to push last year on Twitter.
4746240	4749240	Let's get the CEO on the community forums and let's,
4749240	4753240	let's ask questions and get responses from him.
4753240	4756240	And I just don't know why, why doesn't he show up?
4756240	4759240	I'm not sure if he's made a single post.
4759240	4761240	And there's just other things as well,
4761240	4764240	where I can just the difference between engaging really,
4764240	4768240	truly with your core audience and sort of, you know,
4768240	4770240	compartmentalizing it to a single employee.
4770240	4775240	Like, I don't know, this, this company led engagement is one thing versus department led.
4775240	4776240	Right.
4776240	4779240	And so there's just all these areas and certainly one of the other,
4779240	4784240	I guess more immediate suggestions I have for the community.
4784240	4788240	We've accumulated tons of insights and resources.
4788240	4794240	I think the community could benefit from more pooling of the best posts,
4794240	4797240	the best insights.
4797240	4799240	And I also want to give a shout out.
4799240	4804240	I think we need to encourage more shout out to duty to develop on there.
4804240	4806240	I've reached out to him privately on the open at community forums,
4806240	4811240	but he's done some amazing just write ups of his GPT three experiments and the prompts.
4811240	4812240	I'm sure you've seen them.
4812240	4813240	Oh yeah.
4813240	4815240	And of course there's, there's other members who participate every day.
4815240	4820240	So I'm just saying that now that this, the community is, is in another stage,
4820240	4824240	we, you know, we need to start thinking more about let's, let's,
4824240	4827240	what's curate some of the best moments.
4827240	4828240	Right.
4828240	4832240	I think that's, that's definitely one of the big pieces.
4832240	4838240	And so anyways, did you have any more thoughts in the community stuff or anything else?
4838240	4842240	Yeah, just, just an observation that I've, you know, I've worked at a,
4842240	4847240	at a number of companies of different sizes from, you know, a five person startup to,
4847240	4850240	you know, Cisco systems was the biggest company I've worked for,
4850240	4854240	which has had, had at the time, like 80,000 people globally.
4854240	4858240	And so I wonder if some of, some of what you're observing is just growing pains,
4858240	4863240	just normal growing pains, because often you'll have like the startup culture,
4863240	4864240	which is bootstrapping, right?
4864240	4867240	Where you just, you know, it's on Slack, it's on, it's on GitHub,
4867240	4870240	and you just kind of, it's fast and loose and quick.
4870240	4875240	And open AI, now that they've got an enterprise grade service,
4875240	4877240	they're having to develop their team.
4877240	4880240	You probably noticed they post like, hey, we're hiring, we're hiring, you know,
4880240	4886240	there've been at least two big hiring, hiring splurges in the last six to 12 months.
4886240	4889240	And some of those are just like generic IT guys, you know,
4889240	4892240	like kind of what I do from, from my day job or marketing folks.
4892240	4897240	So I think that, I think that they're probably working on solving some of those problems.
4897240	4901240	But also as a, as a nonprofit foundation, their budget is probably kind of thin.
4901240	4906240	So I'm wondering if, you know, their partnership with Microsoft could help some of that as well.
4906240	4907240	But you're absolutely right.
4907240	4910240	You know, there, there are still other things that they could be doing,
4910240	4913240	like, you know, maybe bring back Slack or, or a few other things.
4913240	4915240	So yeah, that was just final observation.
4915240	4918240	It might just be normal growing pains that they're working on solving.
4918240	4919240	It's definitely growing pains.
4919240	4923240	And the things I'm sharing, to be honest, it's a little bit more on the harsh side.
4923240	4926240	Like, I mean, they mean well, they mean well, right?
4926240	4928240	Like these are not bad people.
4928240	4930240	They are for profit.
4930240	4932240	They switched away from nonprofit.
4932240	4934240	Just, I just wanted to mention that.
4934240	4942240	But I think my, my, the reason I share this feedback is, for example,
4942240	4947240	the CEO, Sam Oltman, he didn't do the AMA thread on the open AI community forums.
4947240	4950240	He went to another website and did an AMA.
4950240	4954240	I can't remember if it was a, like a written form or just like a quick call.
4954240	4955240	Right.
4955240	4960240	Where apparently he shared all these details about what GPT for could be like and the future,
4960240	4962240	all the models may be multimodal in the future.
4962240	4966240	And I guess, you know, the, that thread has now been taken down.
4966240	4969240	And it's like all the things that were said were alleged.
4969240	4973240	And so I guess this is, this is really behind the scenes kind of stuff.
4973240	4978240	Like, but my criticism is they clearly have some capacity to engage.
4978240	4981240	Why are they not engaging where the audience is, right?
4981240	4982240	Right.
4982240	4987240	I had a tweet storm today where I just said, like last month, Sam Oltman was on a podcast
4987240	4991240	talking about meditation and how much meditation helps them.
4991240	4993240	This is a podcast I've never heard of in my life.
4993240	4994240	It's a business podcast.
4994240	4997240	And he had to explain to the guy what GPT three even is.
4997240	5001240	And so in my, like I tweeted, like, why haven't you been on my podcast?
5001240	5002240	Right.
5002240	5009240	Like you can, you can reach out to, you know, almost 8,000 GPT three open AI AI developers.
5009240	5011240	What are you doing talking about meditation?
5011240	5012240	Right.
5012240	5015240	So my problem is, is actually a priority problem.
5015240	5016240	I can see there is capacity.
5016240	5022240	I can see there are some priorities, but I think if you really lean in as a priority into
5022240	5025240	your developer community, there's certain ways you would, you would move.
5025240	5026240	Right.
5026240	5029240	And these, these media channels, there's people in the community.
5029240	5031240	There's, there's so many ways they could go about it.
5031240	5036240	And even linking a lot of the documentation to posts in the community forums.
5036240	5038240	I don't see why that's not a bad idea.
5038240	5039240	Right.
5039240	5042240	Like force people to show the community, show up to the community forums.
5042240	5043240	Right.
5043240	5044240	Walk them through some of the best threads.
5044240	5048240	These are ways in which we could like funnel more people in that direction as well.
5048240	5050240	That costs virtually nothing.
5050240	5051240	Right.
5051240	5055240	And so you need to also invest in the community forums that it needs to be building a community
5055240	5057240	is a company wide thing.
5057240	5062240	It's not something which can be outsourced to a single employee or overseen by PR.
5062240	5065240	It needs to come from a, from a, you know, it needs to come from the heart.
5065240	5070240	I know that sounds so corny, but anyways, clearly I, you know, I, I get too emotional
5070240	5072240	about this community stuff.
5072240	5073240	Yeah.
5073240	5076240	Anyways, these are, these are all things going on behind the scenes.
5076240	5080240	I apologize to all the listeners if they're like, like, this is cool, like cool story,
5080240	5081240	bro.
5081240	5084240	Like anyways.
5084240	5086240	So we're coming towards the end here.
5086240	5088240	I think I had just two broader questions.
5088240	5093240	So what are your thoughts on multimodal AI technology?
5093240	5097240	I think it's definitely going to be a critical component for the future.
5097240	5098240	Right.
5098240	5103240	I, I, I addressed that shortcoming in my book, natural language, cognitive architecture,
5103240	5109240	it thinks and takes in only text, which means, you know, speech, chat, whatever.
5109240	5114240	I think in order to have a fully robust, for instance, if you want to have a fully autonomous,
5114240	5118240	you know, robot that's going to wander around your house and help you out, it's going to
5118240	5120240	need to integrate audio and video.
5120240	5124240	And if you can do that in a single neural network, great.
5124240	5128240	I don't know that it'll be necessary to achieve AGI.
5128240	5133240	It might end up being, it might be one of those, it might be one of those like rare dead ends,
5133240	5134240	right?
5134240	5140240	Where because, you know, thinking visually, thinking, thinking in terms of sound, that
5140240	5143240	might not actually bias that much, right?
5143240	5149240	Because you can represent 95% of human thought in text, right?
5149240	5153240	It might take a little bit more, but it, you know, it might be more expensive.
5153240	5156240	And also how big are those models going to be, right?
5156240	5163240	Because if just, if just a text model of GPT-3 has to run on, you know, $7 million worth of
5163240	5166240	hardware or however much it is, you know, because it's got to run on a bunch of different
5166240	5172240	GPUs, if it's that expensive, how much more expensive, how much bigger is a giant multimodal
5172240	5173240	model going to be?
5173240	5176240	So that's, that's the biggest cost.
5176240	5180240	Obviously computer technology is going to get better over time, you know, and I think I
5180240	5185240	calculated it out, I think in 10 years, your average company could afford to run GPT-3
5185240	5186240	in-house.
5186240	5190240	In 20 years, you could probably run GPT-3 on your desktop.
5190240	5194240	And in 30 years, GPT-3 could run on your phone, right?
5194240	5196240	So that's a long timeline.
5196240	5199240	But in the meantime, we're going to be making bigger and bigger models.
5199240	5203240	And I'm afraid that there's going to be diminishing returns, right?
5203240	5207240	You know, people, right now people seem to think that it's going to follow an exponential
5207240	5211240	growth curve forever, but it might actually follow a sigmoid curve, right?
5211240	5214240	We might be at the point of fastest growth right now, but we're going to see diminishing
5214240	5215240	returns soon.
5215240	5222240	And so like, yeah, multimodal models are certainly going to have capabilities that GPT-3 doesn't.
5222240	5228240	But for the sake of, for the sake of like, if you want to create a self-improving chatbot,
5228240	5233240	GPT-3 and Codex might be enough, or at least, you know, that's, that's, that single mode
5233240	5234240	technology.
5234240	5237240	There was another thought, but it ran away.
5237240	5238240	Sorry.
5238240	5241240	But yeah, those, those, that's kind of, that's kind of my big take is there, there could
5241240	5243240	be benefits, but there's going to be costs too.
5243240	5245240	So we got to be cognizant of that.
5245240	5246240	Yeah.
5246240	5248240	And there might not be enough compute in the world.
5248240	5252240	They're not, there might not even be enough energy or we may like consume all energy ever
5252240	5255240	produced to make, to train a single model.
5255240	5258240	And then we may be able to run it inference for like three seconds.
5258240	5259240	Right.
5259240	5262240	And then it just shuts down the global power system or something, right?
5262240	5267240	But can you see yourself, let's say the technology exists, cost considerations aside, can you
5267240	5269240	see yourself perhaps making movies?
5269240	5274240	Can you see yourself, you know, giving your book to a multimodal model, having, have it
5274240	5277240	generate a documentary based on it or some marketing material?
5277240	5282240	What can you see yourself doing with, you know, the, the, the multimodal model of your
5282240	5283240	dreams?
5283240	5284240	Yeah.
5284240	5288240	So, you know, kind of the thought experiment that I did was, okay, well, we've got, you
5288240	5291240	know, how much, how much data is on YouTube?
5291240	5295240	I think it's like a thousand years or 10,000 years worth of video on YouTube.
5295240	5298240	And of course, it's many, many, many terabytes.
5298240	5302240	You know, so it's like, that's, that's way more training data.
5302240	5306240	You know, if GPT three was trained on less than one terabyte of data and, you know,
5306240	5309240	YouTube is approaching like the Yota bite scale, right?
5309240	5311240	That's, that's an insane amount of data.
5311240	5314240	So, okay, let's say you feed that in.
5314240	5318240	And so you got audio video, you've got text, you've got all the comments and you end up
5318240	5322240	with like a model trained on, on all of YouTube data.
5322240	5323240	Okay, cool.
5323240	5324240	What can you do with that?
5324240	5327240	Like, I can't even imagine, right?
5327240	5332240	Because GPT three today is almost capable of writing screenplays.
5332240	5333240	Right?
5333240	5338240	So if you have a model that's trained on, you know, all text data, all audio data, all
5338240	5341240	video data, you say, Hey, write me a screenplay.
5341240	5342240	Right?
5342240	5346240	I actually, I, and near the end of my book, I kind of have a chapter of speculation.
5346240	5352240	And I say, what if, what if you have this model and you say, give me season two of Firefly?
5352240	5353240	Right?
5353240	5357240	Like, you know, you could, you could just keep watching whatever show you want.
5357240	5361240	You say, give me Game of Thrones, but give me a different, you know, season eight, give
5361240	5364240	me season, you know, different season eight and season nine and 10.
5364240	5365240	Right?
5365240	5369240	So I kind of imagine that one possibility is hyper personalized entertainment.
5369240	5373240	And of course, like that might be 30 years away just because of like you said, the energy
5373240	5375240	intensity of this task.
5375240	5378240	But I, conceptually, it's possible, right?
5378240	5380240	You can hop on GPT three today.
5380240	5385240	Use the instruct model and say, write a screenplay for, you know, Firefly season two, and it'll
5385240	5387240	try, it'll get close.
5387240	5392240	And so then if you can take that text output and feed it into a multimodal model that can
5392240	5394240	translate text to video, why not?
5394240	5399240	You know, and Adobe actually, I don't know if you've seen it, but Adobe is, is already starting
5399240	5404240	on that where they're like inferencing, um, like, uh, what, what's the term?
5404240	5408240	They're like imputing the sound so you can put in a soundless video and it'll generate
5408240	5410240	the audio sound effects for you or vice versa.
5410240	5411240	It's really cool.
5411240	5418240	And so I think it like a company like Adobe that they have a huge vested interest in mastering
5418240	5420240	audio visual technologies.
5420240	5424240	They might, you know, soon put out something where, you know, you put in a text description
5424240	5426240	and it'll give you like a three second clip, right?
5426240	5428240	So that you can use that for ad copy.
5428240	5431240	Well, this technology is going to continue improving over time.
5431240	5436240	So I kind of, I kind of see that as like, if I were Netflix, put it this way.
5436240	5442240	If I had the budget of Netflix or Amazon, I would be investing in this to, to, to write
5442240	5448240	hyper personalized, um, video, uh, like series or, uh, or novels, right?
5448240	5451240	Cause you know, Amazon's got the market cornered with Kindle, right?
5451240	5455240	And there's people that will read all day, every day, right?
5455240	5460240	There are people that consume every bit of like entertainment that's available.
5460240	5465240	So if you can generate that on the fly without, you know, having a studio, a big budget studio,
5465240	5468240	that would be, I mean, that would change entertainment.
5468240	5470240	You know, that, that's the metaverse.
5470240	5471240	Forget what Facebook is doing.
5471240	5475240	That's the metaverse where it's like, Hey, you know, I, I came up with my own idea for
5475240	5480240	Game of Thrones and I, I wrote, you know, I use this, uh, you know, GPT eight or whatever
5480240	5482240	to generate my own version of Game of Thrones.
5482240	5483240	Come watch it with me guys.
5483240	5486240	And you know, someone might say, Oh, I didn't like that ending.
5486240	5488240	And they go rewrite it and generate their own version.
5488240	5490240	You know, cause we share memes on the internet today.
5490240	5494240	What if instead of sharing memes on the internet, we end up sharing episodes of our favorite
5494240	5500240	anime or, you know, we, we, uh, Resurrect Battlestar Galactica, you know, whatever.
5500240	5502240	There's so many things that we could do.
5502240	5507240	Like if compute power was not a problem, then we get there, but we need like fusion
5507240	5509240	reactors to power this stuff.
5509240	5510240	Yeah.
5510240	5511240	Yeah.
5511240	5516240	And like Marvel for me is already kind of like this and my capacity to consume Marvel
5516240	5519240	as a, as a viewer, it appears is infinite.
5519240	5520240	So I'm excited.
5520240	5524240	I've called it in the past, like the multimodal Marvel cinematic universe.
5524240	5525240	Yeah.
5525240	5530240	And I, some of these shows like Loki, I don't know if you, if you watch like,
5530240	5531240	I haven't seen it yet.
5531240	5532240	Okay.
5532240	5533240	Okay.
5533240	5534240	I mean, it was six episodes.
5534240	5538240	If the, if it had been 30, I would have watched all 30 and enjoyed every moment of it.
5538240	5541240	If that quality was, I want to go deeper in these stories.
5541240	5546240	So I'm definitely excited for all my favorite universes, cinematic universes and
5547240	5552240	story wise as well to, to live on forever essentially through multimodal content and
5552240	5555240	maybe be personalized like you're describing as well.
5555240	5556240	Oh yeah.
5556240	5557240	So yeah.
5557240	5558240	Last question.
5558240	5561240	So we, you know, you know, we've talked about various things.
5561240	5567240	We've talked about codex by today, you know, multimodal stuff broadly.
5567240	5569240	Where do you see all of this stuff going?
5569240	5571240	Let's give a timeline, five, 10 years.
5571240	5574240	What are some of the, what's the direction we're heading towards?
5574240	5576240	What, what important capabilities will we have?
5576240	5578240	Why is this stuff important?
5578240	5579240	Yeah.
5579240	5583240	Um, five to 10 years from now, I think that we will have something that you could probably
5583240	5588240	call a fully functional AGI like as a service you could sign up for.
5588240	5592240	Um, you know, it might be chatbot based, you know, kind of based on natural language,
5592240	5593240	cognitive architecture.
5593240	5596240	Um, I calculated out like it's too expensive to run right now.
5596240	5601240	You know, if it's $30 for a, for a 10 minute conversation, that's way too expensive.
5601240	5603240	So the, the, the cost has to come down.
5603240	5607240	You know, if you just, if you just take the technology we have today, but make it cheaper,
5607240	5609240	there's so much potential.
5609240	5613240	Um, so, you know, then there was that idea about like self-improving, um, you know,
5613240	5615240	feedback loops, you know, integrating with DevOps.
5615240	5620240	I certainly think that a company like Atlassian, which is a major DevOps player,
5620240	5624240	um, probably within five to 10 years, they'll have something, um, integrated to,
5624240	5628240	to kind of help automate the development pipeline even further.
5628240	5633240	Um, I think that, of course, I could be wrong because we're kind of at this weird
5633240	5634240	acceleration point.
5634240	5640240	Um, I think, I feel like multimodal models like consumer grade multimodal models are
5640240	5642240	probably more than 10 years away.
5642240	5646240	Um, unfortunately, they're probably just going to be like toy sized because,
5646240	5650240	you know, there's, um, there's like a, a, a hypnogram, right?
5650240	5652240	I don't know if you've seen that one, but that's one of like the text to image
5652240	5656240	generators and they're, it's still not even photorealistic, right?
5656240	5661240	Getting a photorealistic text to image is still like, that's a little ways off.
5661240	5664240	And then the next step after that is text to video.
5664240	5666240	That's even further, right?
5666240	5668240	So that's, that's kind of where I think it's at.
5668240	5670240	I don't think we're going to hit an AI winter.
5670240	5673240	I know there's lots of people predicting that we're going to hit an AI winter,
5673240	5677240	but I think that we're actually still kind of in the acceleration point.
5677240	5680240	But again, I don't know if it's going to follow an exponential curve forever
5680240	5682240	or if it's a sigmoid curve.
5682240	5685240	So time will tell.
5685240	5686240	Yep.
5686240	5689240	And still lots to do in the meantime, like you're describing, even with UPT3.
5689240	5690240	Oh yeah.
5690240	5691240	Okay.
5691240	5692240	Yeah.
5692240	5695240	My, my answer is, I think all of this stuff is, is just converging to just
5695240	5697240	greater human potential.
5697240	5700240	In some sense, I'm not even necessarily interested in the AGI question,
5700240	5702240	although I think it's important.
5702240	5706240	I think just the, the exciting possibilities we'll have, even now that we have,
5706240	5709240	that we'll continue to have five to 10 years from now.
5709240	5713240	Um, so many more experiences, so many other things we'll be able to create
5713240	5714240	that weren't possible.
5714240	5716240	I think we'll have more people creating than ever before.
5716240	5720240	Um, it's a really, really exciting vision for humanity.
5720240	5721240	Right.
5721240	5722240	Not just, not just for you and I.
5722240	5727240	So anyways, so with that said, did you have anything you wanted to plug David?
5727240	5729240	Where can people find you online?
5729240	5730240	Yeah.
5730240	5734240	So, um, my personal site is a David K Shapiro dot com.
5734240	5738240	Um, and I've got, um, I have, I have a few projects up and coming.
5738240	5741240	Um, nothing out right now except for my book,
5741240	5743240	natural language, cognitive architecture.
5743240	5746240	Um, you can download it for free from my website.
5746240	5748240	Um, you can sign up for my newsletter.
5748240	5751240	So one of my upcoming books is called benevolent by design,
5751240	5753240	six words to safeguard humanity,
5753240	5756240	which is to address the control problem of AGI.
5756240	5761240	Um, so that's, that's, uh, that book should hopefully be out in the next six months or so.
5761240	5764240	Um, and that is, um, so that's one project.
5764240	5766240	I've got another nonfiction book.
5766240	5769240	Um, and then also my own podcast will be coming out soon.
5769240	5770240	Yeah.
5770240	5773240	Head over to my site, David K Shapiro dot com and sign up for my newsletter.
5773240	5777240	And you'll get, you'll get updated when these, when these come out, when they're available.
5777240	5778240	Awesome.
5778240	5781240	And David, you mentioned you're, you're looking for collaborators as well.
5781240	5782240	Yes.
5782240	5783240	For natural language, cognitive architecture.
5783240	5789240	So, uh, if you're a coder, you know, imagine product manager, researcher, uh, hit up David
5789240	5792240	and just connect if, if any of this stuff interests you.
5792240	5798240	Um, I've, I've, I asked, I spent like a couple, I think I spent like a few days trying to find you on Twitter.
5798240	5800240	I don't think you're quite on Twitter yet.
5800240	5807240	Uh, I encourage you, uh, David, of course, you know, you and I will connect after we'll put any other, other place people could connect with you.
5807240	5808240	There's the community forums.
5808240	5810240	I assume you have a GitHub account.
5810240	5811240	Yeah.
5811240	5814240	So we're going to put that in, in the show notes and in the YouTube description below.
5814240	5817240	Uh, so anyways, David, thank you so much for being here.
5817240	5826240	I wanted to personally thank you for all the awesome, awesome community contributions you've made on the open eye community forum.
5826240	5829240	Uh, you're just an essential person on there.
5829240	5830240	I've learned a lot from you.
5830240	5834240	Uh, you know, the insights you've shared, they're going to be there forever.
5834240	5836240	And I'm sure I can't imagine how many people you've helped.
5836240	5843240	Uh, and also about your book, I also just wanted to say to the audience, uh, David's done a great job making it really digestible.
5843240	5847240	Like it was a very, it was a breeze of a read.
5847240	5852240	I thoroughly enjoyed it as somebody who writes GPT three prompts and is into this ecosystem.
5852240	5862240	It was just, uh, very interesting to see, uh, how it, how it could be laid out, uh, in this broader system approaching this huge problem.
5862240	5865240	Um, and also I was able to even get the book for free.
5865240	5869240	Obviously I encourage people by the book support it, but it's, it's there.
5869240	5870240	It's ready.
5870240	5872240	I think, you know, David's goal here is to get the ideas out.
5872240	5877240	Um, and so, uh, anyways, so, uh, that's it for today's episode.
5877240	5878240	David, thank you so much again.
5878240	5879240	I really appreciate you being here.
5879240	5880240	Thank you.
5880240	5883240	Thank you for all the kind comments and, um, and you're quite welcome.
5883240	5884240	And so is everyone else.
5884240	5886240	That's why I'm here.
5886240	5887240	Awesome.
5887240	5896240	And, uh, quick, so my quick plugs, you know, at BAKZT future Twitter, Instagram, YouTube.com slash BAKZT future.
5896240	5902240	My newsletter, I'll put it in the description below and I have a Twitter spaces event coming in two days at noon.
5902240	5904240	Uh, a couple of people probably pulling up.
5904240	5906240	This is like a audio only event.
5906240	5912240	So I encourage audio podcast listeners, YouTube subscribers, pull up to the Twitter spaces event.
5912240	5917240	We're going to chat more about codecs and prompt design and some other stuff going on in the, in the space.
5917240	5921240	So anyways, thank you again for listening to multimodal by BAKZT future.
5921240	5922240	I'll catch you in the next one.
5922240	5923240	Bye.
