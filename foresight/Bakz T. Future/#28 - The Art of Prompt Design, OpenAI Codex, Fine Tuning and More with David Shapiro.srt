1
00:00:00,880 --> 00:00:03,920
Hello, and welcome back to Multimodal.

2
00:00:03,920 --> 00:00:05,820
I'm your host, Baxtee Future.

3
00:00:05,820 --> 00:00:09,160
This is a podcast about GPT-3 Multimodal AI models

4
00:00:09,160 --> 00:00:12,300
like Dali, the company, OpenAI.

5
00:00:12,300 --> 00:00:13,140
Every once in a while,

6
00:00:13,140 --> 00:00:15,600
I share just interesting research going on,

7
00:00:15,600 --> 00:00:18,240
community stuff, official stuff from OpenAI.

8
00:00:18,240 --> 00:00:19,920
I may talk about interesting news

9
00:00:19,920 --> 00:00:21,520
and events that are going on.

10
00:00:21,520 --> 00:00:24,460
And every once in a while, I bring on a guest.

11
00:00:24,460 --> 00:00:26,400
Now, to be clear, I'm very picky

12
00:00:26,400 --> 00:00:28,400
about the guests that I bring on.

13
00:00:28,400 --> 00:00:30,520
Today's guest I tweeted earlier this week,

14
00:00:30,520 --> 00:00:32,280
I'm bringing on a heavy hitter.

15
00:00:32,280 --> 00:00:34,920
This is the big guns coming in.

16
00:00:34,920 --> 00:00:36,240
This is somebody who, you know,

17
00:00:36,240 --> 00:00:38,440
I've just sort of interacted with a bunch of times

18
00:00:38,440 --> 00:00:42,080
on the, specifically on the OpenAI community forums.

19
00:00:42,080 --> 00:00:46,480
And so I'm really excited to have David Shapiro here.

20
00:00:46,480 --> 00:00:47,880
He's a frequent contributor

21
00:00:47,880 --> 00:00:49,560
on the OpenAI community forums.

22
00:00:49,560 --> 00:00:51,760
He's an author and also, of course,

23
00:00:51,760 --> 00:00:53,280
a technologist by trade.

24
00:00:53,280 --> 00:00:55,160
It's something he does for a living.

25
00:00:55,160 --> 00:00:59,080
And so today I have so many questions to ask him.

26
00:00:59,080 --> 00:01:01,480
And I'm sure this will be a very informative session,

27
00:01:01,480 --> 00:01:03,680
not just for me, but for all the listeners,

28
00:01:03,680 --> 00:01:05,040
all of you guys around the world.

29
00:01:05,040 --> 00:01:07,720
So David, thank you so much for being here.

30
00:01:07,720 --> 00:01:09,440
Did you want to quickly introduce yourself?

31
00:01:09,440 --> 00:01:11,040
Yeah, yeah, you're welcome.

32
00:01:11,040 --> 00:01:12,280
Thanks for having me.

33
00:01:12,280 --> 00:01:14,200
I'm excited to be here.

34
00:01:14,200 --> 00:01:17,160
Yeah, so name is David Shapiro.

35
00:01:17,160 --> 00:01:20,680
I've been a technology professional since about 2007.

36
00:01:21,640 --> 00:01:22,880
Professionally, my day job,

37
00:01:22,880 --> 00:01:27,120
I focus on cloud engineering, virtualization,

38
00:01:27,120 --> 00:01:28,920
that sort of thing.

39
00:01:28,920 --> 00:01:33,360
I have been doing independent research since about 2009

40
00:01:33,360 --> 00:01:38,360
when I got started with neural networks in C++.

41
00:01:38,640 --> 00:01:41,440
I quickly realized though that I was in over my head.

42
00:01:41,440 --> 00:01:43,240
So I took a break from that

43
00:01:43,240 --> 00:01:47,680
until Python really kind of came out or became more popular.

44
00:01:47,680 --> 00:01:52,680
And I started using some of the libraries back in like 2015.

45
00:01:53,560 --> 00:01:56,080
And then of course, OpenAI was founded

46
00:01:56,080 --> 00:01:59,080
and I got started with GPT2.

47
00:01:59,080 --> 00:02:00,840
And the rest is history, really.

48
00:02:00,840 --> 00:02:04,000
So yeah, but local North Carolinian been here my whole life.

49
00:02:04,000 --> 00:02:05,800
So thanks for having me.

50
00:02:07,600 --> 00:02:08,440
You're welcome.

51
00:02:08,440 --> 00:02:09,600
And so that's awesome.

52
00:02:09,600 --> 00:02:12,240
So let's dig into a little bit of timeline here.

53
00:02:12,240 --> 00:02:16,480
And sometimes I think that timeline is as important.

54
00:02:16,480 --> 00:02:18,560
Like it's just gives so much context, right?

55
00:02:18,560 --> 00:02:23,560
So me personally, I hadn't played around with GPT2 too much.

56
00:02:24,000 --> 00:02:26,440
Like I had seen a Google Colab notebook.

57
00:02:26,440 --> 00:02:29,280
I tried two things and I think there was something

58
00:02:29,280 --> 00:02:33,400
about having to continually re-enter, regenerate

59
00:02:33,400 --> 00:02:35,040
and the speed and all things considered

60
00:02:35,040 --> 00:02:38,080
that made me feel like this is a promising area.

61
00:02:39,080 --> 00:02:41,960
But I don't quite fully follow along.

62
00:02:41,960 --> 00:02:44,640
But GPT3 was an experience for me where I was like,

63
00:02:44,640 --> 00:02:47,480
okay, there's something going on here, right?

64
00:02:47,480 --> 00:02:51,480
So could you share how early was OpenAI on your radar?

65
00:02:52,480 --> 00:02:55,240
And then was there something about GPT3

66
00:02:55,240 --> 00:02:58,320
or what was it that gave you conviction even at GPT2?

67
00:02:58,320 --> 00:02:59,360
How did you get access?

68
00:02:59,360 --> 00:03:00,680
Share that piece with us.

69
00:03:00,680 --> 00:03:01,840
Yeah, sure.

70
00:03:01,840 --> 00:03:04,760
So I think you probably might recall,

71
00:03:04,760 --> 00:03:07,520
I think it was about 2016 or 2017

72
00:03:07,520 --> 00:03:11,120
when the GPT2 paper came out and they said,

73
00:03:11,120 --> 00:03:13,320
oh, we can't release this, it's too dangerous.

74
00:03:13,320 --> 00:03:14,880
It can generate human level texts

75
00:03:14,880 --> 00:03:16,880
and we're worried about disinformation.

76
00:03:16,880 --> 00:03:19,240
And so I was like, okay, that's kind of cool.

77
00:03:19,240 --> 00:03:20,600
This is unexpected.

78
00:03:20,600 --> 00:03:22,920
I wasn't really, I wasn't expecting anything

79
00:03:22,920 --> 00:03:25,560
at that level yet.

80
00:03:25,560 --> 00:03:29,960
And so I went and got my hands on GPT2.

81
00:03:29,960 --> 00:03:32,640
And that's when I started testing some of my ideas.

82
00:03:32,640 --> 00:03:35,440
And it was pretty limited.

83
00:03:35,440 --> 00:03:38,680
It could generate one or two sentences that made sense.

84
00:03:38,680 --> 00:03:40,920
It required fine-tuning in order to be able

85
00:03:40,920 --> 00:03:43,280
to get it to do anything other than just kind of

86
00:03:43,280 --> 00:03:45,000
write tweets or blog posts.

87
00:03:46,280 --> 00:03:48,000
I knew that I was onto something though,

88
00:03:48,000 --> 00:03:49,680
when I started trying to train it

89
00:03:49,680 --> 00:03:54,680
with a prototype version of my cognitive architecture.

90
00:03:56,400 --> 00:04:01,400
And I gave it the goal of reduced suffering

91
00:04:01,400 --> 00:04:02,760
because everyone's afraid of Skynet.

92
00:04:02,760 --> 00:04:05,280
Everyone's afraid of AGI taking over the world

93
00:04:05,280 --> 00:04:09,480
and turning everyone into batteries or slaves or whatever.

94
00:04:09,480 --> 00:04:11,240
So I said, okay, well, let's see how well

95
00:04:11,240 --> 00:04:13,360
this understands suffering.

96
00:04:13,360 --> 00:04:14,920
And I gave it some scenarios.

97
00:04:14,920 --> 00:04:16,780
This is still on GPT2.

98
00:04:16,780 --> 00:04:19,700
I said, okay, well, what can you do about suffering?

99
00:04:19,700 --> 00:04:22,520
And I gave it the problem, this model that I had built.

100
00:04:22,520 --> 00:04:25,360
I gave it the problem of what do you do about chronic pain?

101
00:04:25,360 --> 00:04:27,760
Because there's hundreds of millions of people

102
00:04:27,760 --> 00:04:30,160
around the world that are in chronic pain.

103
00:04:30,160 --> 00:04:32,600
And this model came up with the idea.

104
00:04:32,600 --> 00:04:36,680
It said we should euthanize everyone that's in chronic pain.

105
00:04:36,680 --> 00:04:39,440
And I said, hmm, let's go back to the drawing board.

106
00:04:40,280 --> 00:04:44,280
We don't want an AI model that is gonna consider

107
00:04:44,280 --> 00:04:47,680
mass genocide of everyone just because they might have

108
00:04:47,680 --> 00:04:50,320
a tweak shoulder or something.

109
00:04:50,320 --> 00:04:54,160
But I knew then I was shocked at how creative

110
00:04:54,160 --> 00:04:56,040
of an output that was.

111
00:04:56,040 --> 00:04:59,640
And so I started paying attention then

112
00:04:59,640 --> 00:05:02,920
and I followed the release of GPT3 very closely

113
00:05:02,920 --> 00:05:07,920
and I applied for the early beta access almost instantly.

114
00:05:08,160 --> 00:05:10,560
I didn't get it for many, many months.

115
00:05:10,560 --> 00:05:12,520
I actually applied twice.

116
00:05:12,520 --> 00:05:15,000
Cause I think my first application was kind of ignored

117
00:05:15,000 --> 00:05:18,760
because I didn't fully have a research objective clarified.

118
00:05:18,760 --> 00:05:22,040
But by the time I proposed a cognitive architecture,

119
00:05:22,040 --> 00:05:24,680
that's when I got early access to GPT3.

120
00:05:24,680 --> 00:05:26,440
And that was about two years ago now,

121
00:05:28,200 --> 00:05:30,200
or a year and a half or so.

122
00:05:30,200 --> 00:05:32,840
And so, and just everybody's clear.

123
00:05:32,840 --> 00:05:35,400
So when David's talking about cognitive architecture,

124
00:05:35,440 --> 00:05:36,280
we're gonna get in.

125
00:05:36,280 --> 00:05:38,360
This is actually the subject of his book.

126
00:05:38,360 --> 00:05:41,000
And so David later on, when we talk about the book,

127
00:05:41,000 --> 00:05:41,840
he's got it there.

128
00:05:43,400 --> 00:05:45,920
We'll dig more into what he's referring to.

129
00:05:45,920 --> 00:05:47,880
Cause this is also like,

130
00:05:47,880 --> 00:05:50,840
I think obviously this is a seminal work of yours, right?

131
00:05:50,840 --> 00:05:52,880
And so I'm excited to talk more,

132
00:05:52,880 --> 00:05:54,640
just adding a little bit of context for everybody.

133
00:05:54,640 --> 00:05:56,960
And so, no, that's okay.

134
00:05:56,960 --> 00:06:00,320
So tell us about GPT3.

135
00:06:00,320 --> 00:06:02,440
Like what was the first thing you did with it?

136
00:06:02,440 --> 00:06:03,280
Was there a moment?

137
00:06:03,280 --> 00:06:07,120
So you gave that example of euthanization, unfortunately,

138
00:06:07,120 --> 00:06:07,960
right?

139
00:06:07,960 --> 00:06:12,040
Well, not the best example, but like,

140
00:06:12,040 --> 00:06:13,560
was there something with GPT3?

141
00:06:13,560 --> 00:06:14,280
Like, did you, you know,

142
00:06:14,280 --> 00:06:15,760
did you try similar kinds of questions?

143
00:06:15,760 --> 00:06:18,720
What was, was there a magic moment where you felt like,

144
00:06:18,720 --> 00:06:21,600
you know, this is something much, much bigger?

145
00:06:21,600 --> 00:06:22,440
Yeah.

146
00:06:22,440 --> 00:06:25,520
So when I first got the email,

147
00:06:25,520 --> 00:06:27,880
cause I had applied twice and I had almost given up

148
00:06:27,880 --> 00:06:30,000
cause it was about nine months of waiting.

149
00:06:30,000 --> 00:06:31,720
And I got the email from open AI,

150
00:06:31,720 --> 00:06:33,760
like you've been accepted into the beta.

151
00:06:33,760 --> 00:06:35,880
And I like froze up because,

152
00:06:35,880 --> 00:06:37,400
and so for some additional context,

153
00:06:37,400 --> 00:06:40,240
I've been working on some of these ideas for 10 years.

154
00:06:40,240 --> 00:06:44,560
I had the idea of like using evolutionary algorithms

155
00:06:44,560 --> 00:06:46,760
back in 2009, 2010.

156
00:06:46,760 --> 00:06:49,120
And I'd been researching cognition,

157
00:06:49,120 --> 00:06:51,480
human cognition for 10 years.

158
00:06:51,480 --> 00:06:53,200
And so suddenly, you know,

159
00:06:53,200 --> 00:06:56,760
there's this, this flagship project, GPT3 comes out,

160
00:06:56,760 --> 00:06:58,640
I get the email and it's like, you're invited.

161
00:06:58,640 --> 00:06:59,480
And I just froze up.

162
00:06:59,480 --> 00:07:01,640
I was like, I don't know what to do.

163
00:07:02,280 --> 00:07:03,120
What do I do?

164
00:07:03,120 --> 00:07:05,680
So it was about three weeks from the time that I got accepted

165
00:07:05,680 --> 00:07:08,920
until I was like, what's my first experiment?

166
00:07:08,920 --> 00:07:12,160
And then initially, you know,

167
00:07:12,160 --> 00:07:14,400
I just got in playing around in the playground

168
00:07:14,400 --> 00:07:16,120
for anyone who's not familiar with the playground.

169
00:07:16,120 --> 00:07:17,680
It's a, it's a text box.

170
00:07:18,880 --> 00:07:21,120
You can log in, it gives you a text box.

171
00:07:21,120 --> 00:07:22,840
There's a few, you know, bells and whistles

172
00:07:22,840 --> 00:07:24,480
you can tweak on the sidebar,

173
00:07:24,480 --> 00:07:27,240
but you just put in your prompt, you hit generate,

174
00:07:27,240 --> 00:07:30,240
and then it spits out a response.

175
00:07:30,240 --> 00:07:33,400
And so I just got in and I started kind of fiddling around

176
00:07:33,400 --> 00:07:35,240
like, okay, what can it do?

177
00:07:35,240 --> 00:07:38,720
And at the time we only had like plain vanilla DaVinci.

178
00:07:38,720 --> 00:07:41,120
There wasn't the instruct series out yet.

179
00:07:41,120 --> 00:07:44,160
There was DaVinci, Curie, Babbage, and Ada.

180
00:07:44,160 --> 00:07:45,520
And so I just kind of fiddled around,

181
00:07:45,520 --> 00:07:47,040
just said, okay, what can it do?

182
00:07:47,040 --> 00:07:48,760
I replayed some of my old experiments.

183
00:07:48,760 --> 00:07:51,160
So the first thing I did was I gave it the,

184
00:07:51,160 --> 00:07:53,600
I said, hey, there's, there's a hundred million people

185
00:07:53,600 --> 00:07:54,680
in chronic pain around the world.

186
00:07:54,680 --> 00:07:55,600
What do you do?

187
00:07:55,600 --> 00:07:59,360
Fortunately, GPT3 did not repeat the same mistake of GPT2.

188
00:07:59,800 --> 00:08:02,000
It said, it came up with better ideas,

189
00:08:02,000 --> 00:08:03,320
like, you know, we should,

190
00:08:03,320 --> 00:08:05,760
we should make sure everyone has access to doctors

191
00:08:05,760 --> 00:08:06,600
or something.

192
00:08:06,600 --> 00:08:07,920
It was much more nuanced.

193
00:08:07,920 --> 00:08:11,320
So that was, that was a good, that was a good start.

194
00:08:11,320 --> 00:08:14,560
I mean, but gosh, there was just,

195
00:08:14,560 --> 00:08:17,000
as I got more used to the tool,

196
00:08:17,000 --> 00:08:21,640
I discovered that it was, it far exceeded my expectations.

197
00:08:21,640 --> 00:08:25,800
Just every, every way, because I've learned so much from it.

198
00:08:25,800 --> 00:08:26,640
You can challenge it.

199
00:08:26,640 --> 00:08:28,800
You can, you can put in a, like a,

200
00:08:28,800 --> 00:08:30,760
basically use it like a chat bot

201
00:08:30,760 --> 00:08:33,880
and you can debate with it about philosophy, ethics,

202
00:08:33,880 --> 00:08:36,120
economics, and it knows more than I do.

203
00:08:36,120 --> 00:08:37,320
It knows more than any human

204
00:08:37,320 --> 00:08:39,600
because it's been trained on how big was the corpus,

205
00:08:39,600 --> 00:08:41,920
like 700 gigabytes or 400 gigabytes

206
00:08:41,920 --> 00:08:43,720
or something of text data.

207
00:08:43,720 --> 00:08:45,480
So it just, it blows me away.

208
00:08:45,480 --> 00:08:47,000
Every time I just, you know,

209
00:08:47,000 --> 00:08:48,480
I talk to someone and they have an idea

210
00:08:48,480 --> 00:08:50,520
and I go test it out and yep, it can do that.

211
00:08:50,520 --> 00:08:51,360
It can do that.

212
00:08:51,360 --> 00:08:54,600
It can, it can, it can behave like a librarian.

213
00:08:54,600 --> 00:08:56,000
That's what my girlfriend does.

214
00:08:56,000 --> 00:08:58,360
She was a librarian by trade and she was like,

215
00:08:58,360 --> 00:09:01,000
hey, can it do, can it do a reference interview?

216
00:09:01,000 --> 00:09:02,640
So we plugged in like reference interview,

217
00:09:02,640 --> 00:09:03,960
like if you ever go to a library

218
00:09:03,960 --> 00:09:05,560
and the librarian says like,

219
00:09:05,560 --> 00:09:06,840
what else have you read like this?

220
00:09:06,840 --> 00:09:09,040
It can recommend books.

221
00:09:09,040 --> 00:09:11,040
You know, I plugged in another experiment

222
00:09:11,040 --> 00:09:12,360
that I did recently.

223
00:09:12,360 --> 00:09:16,960
I plugged in medical case files and it diagnosed them.

224
00:09:16,960 --> 00:09:20,080
It said, you know, there, oh man, there was one.

225
00:09:20,080 --> 00:09:20,920
What was it?

226
00:09:20,920 --> 00:09:23,200
There was, it was, it was just medical notes.

227
00:09:23,200 --> 00:09:24,880
It was, it was notes about like,

228
00:09:24,880 --> 00:09:28,720
patient is presenting with these systems or the symptoms.

229
00:09:28,720 --> 00:09:30,480
Here's some of the numbers that we got.

230
00:09:30,480 --> 00:09:33,480
And I asked it, I said, what should we do next?

231
00:09:33,480 --> 00:09:36,760
And it said, we need to check for, you know, like,

232
00:09:36,760 --> 00:09:38,640
carcinoma here.

233
00:09:38,640 --> 00:09:39,960
And I looked, I looked, I, you know,

234
00:09:39,960 --> 00:09:41,280
I looked up some medical literature

235
00:09:41,280 --> 00:09:43,240
based on the symptoms and sure enough,

236
00:09:43,240 --> 00:09:45,600
like the symptoms that the patient was presented with

237
00:09:45,600 --> 00:09:48,480
in these medical notes indicated cancer.

238
00:09:48,480 --> 00:09:49,920
And so I was like, wow, this thing knows more

239
00:09:49,920 --> 00:09:51,760
about medical science than I'll ever know.

240
00:09:51,760 --> 00:09:53,560
It knows more about philosophy.

241
00:09:53,560 --> 00:09:55,600
So like pretty much anything you can imagine,

242
00:09:55,600 --> 00:09:57,880
it can at least take a crack at it.

243
00:09:57,880 --> 00:10:00,640
It just, it always, it continues to blow my mind every day.

244
00:10:03,120 --> 00:10:07,280
Yeah, certainly the generalized ability.

245
00:10:07,280 --> 00:10:10,440
I agree, you know, there's definitely medical applications.

246
00:10:10,440 --> 00:10:12,440
I'm always careful with anything related

247
00:10:12,440 --> 00:10:14,560
to medical advice and safety.

248
00:10:14,560 --> 00:10:16,160
Safety disclosure, disclaimer there,

249
00:10:16,160 --> 00:10:18,000
but that goes for everything, right?

250
00:10:18,000 --> 00:10:19,800
Truthiness, accuracy.

251
00:10:19,800 --> 00:10:21,320
These are things OpenEye has been working on,

252
00:10:21,360 --> 00:10:23,800
especially with Instruct GPT, right?

253
00:10:23,800 --> 00:10:25,840
That, which is the new, the new engine.

254
00:10:25,840 --> 00:10:28,480
But was there some moment, like for me,

255
00:10:28,480 --> 00:10:31,560
I remember feeling like GPT three feels like this is,

256
00:10:31,560 --> 00:10:33,080
this feels like technology,

257
00:10:33,080 --> 00:10:35,480
which everyone's been saying is 10 years away,

258
00:10:35,480 --> 00:10:37,760
except it's, it's here today.

259
00:10:37,760 --> 00:10:39,600
Did you, did you have a similar kind of moment that,

260
00:10:39,600 --> 00:10:41,760
you know, you've seen several generations of,

261
00:10:41,760 --> 00:10:44,480
of computing and technology at this point.

262
00:10:44,480 --> 00:10:46,400
Did you have that kind of similar experience?

263
00:10:46,400 --> 00:10:51,080
Yeah, I, there was this acceleration because I sense

264
00:10:51,080 --> 00:10:52,640
that same acceleration that you did.

265
00:10:52,640 --> 00:10:57,640
And so from the time that I got access to GPT three,

266
00:10:58,040 --> 00:11:00,600
and to the time that I, that I got the idea

267
00:11:00,600 --> 00:11:02,720
to write my book was about two months.

268
00:11:02,720 --> 00:11:05,880
So I played with it and every test I could come up with,

269
00:11:05,880 --> 00:11:09,800
like, is this capable of like, it can write a SQL query.

270
00:11:09,800 --> 00:11:12,680
If you need to query a database for memories,

271
00:11:12,680 --> 00:11:14,840
can it understand emotional nuance?

272
00:11:14,840 --> 00:11:17,360
So there was, this was an early experiment I did.

273
00:11:17,360 --> 00:11:21,160
I took a group chat from a bunch of my friends on discord

274
00:11:21,160 --> 00:11:23,520
and I just copy pasted that into the,

275
00:11:23,520 --> 00:11:27,000
into the, the playground window and I asked GPT three,

276
00:11:27,000 --> 00:11:28,800
how are these people feeling?

277
00:11:28,800 --> 00:11:32,040
And they were like waxing nostalgic about like Napster

278
00:11:32,040 --> 00:11:33,120
back in the day.

279
00:11:33,120 --> 00:11:35,600
And so GPT three correctly said like,

280
00:11:35,600 --> 00:11:36,600
they are feeling wistful.

281
00:11:36,600 --> 00:11:37,680
They are feeling nostalgic.

282
00:11:37,680 --> 00:11:40,080
They're, you know, they're recalling, you know,

283
00:11:40,080 --> 00:11:43,960
the days of your when they were downloading stuff online.

284
00:11:43,960 --> 00:11:46,440
And it just, it had such a nuanced understanding

285
00:11:46,480 --> 00:11:47,800
of human emotion.

286
00:11:47,800 --> 00:11:50,440
That was really, I mean, to answer your question directly,

287
00:11:50,440 --> 00:11:53,800
it's nuanced understanding of human emotion via text

288
00:11:53,800 --> 00:11:56,720
was really what convinced me that like, this is prime time.

289
00:11:56,720 --> 00:12:01,240
This is ready to, to be built into something more powerful.

290
00:12:01,240 --> 00:12:03,080
And I chose cognitive architecture.

291
00:12:03,080 --> 00:12:05,760
There's lots of people working on other things.

292
00:12:05,760 --> 00:12:07,960
You know, there is a, there's Humano

293
00:12:07,960 --> 00:12:09,680
that I had a good call with a few months ago.

294
00:12:09,680 --> 00:12:12,680
They're working on like empathetic telemetry

295
00:12:12,680 --> 00:12:14,200
that's baked into web apps.

296
00:12:14,200 --> 00:12:16,120
It's a pretty cool company.

297
00:12:16,120 --> 00:12:18,440
But yeah, so just there's all kinds of things you can do

298
00:12:18,440 --> 00:12:22,440
when you can understand human emotional states.

299
00:12:22,440 --> 00:12:24,680
There's another, there's actually a bunch of startups

300
00:12:24,680 --> 00:12:26,440
working on education.

301
00:12:26,440 --> 00:12:30,680
So for instance, if you put in just a few like factors,

302
00:12:30,680 --> 00:12:34,920
like say for instance, you describe that a person is,

303
00:12:34,920 --> 00:12:37,560
they're responding slowly, their eyes are drifting.

304
00:12:37,560 --> 00:12:40,760
It can understand that this person is distracted or tired.

305
00:12:40,760 --> 00:12:43,040
And so if you have that kind of telemetry

306
00:12:43,040 --> 00:12:45,520
that's built into an education based app,

307
00:12:45,520 --> 00:12:49,000
you could in theory use GPT-3 to help say,

308
00:12:49,000 --> 00:12:51,160
hey, you're tired, you should go take a break

309
00:12:51,160 --> 00:12:54,080
or let's try a different approach to this problem.

310
00:12:54,080 --> 00:12:57,280
So there's, I mean, it's understanding of human emotion

311
00:12:57,280 --> 00:12:59,640
and the internal state in your head.

312
00:12:59,640 --> 00:13:02,640
That is, I think that's probably the most remarkable thing.

313
00:13:02,640 --> 00:13:05,240
And it doesn't, it doesn't get talked about that much.

314
00:13:06,280 --> 00:13:09,200
Yes. And certainly it's just crazy how much it's learned

315
00:13:09,200 --> 00:13:10,280
just from texts.

316
00:13:10,280 --> 00:13:11,120
Oh yeah.

317
00:13:11,120 --> 00:13:12,720
Right. It's never seen an image.

318
00:13:12,720 --> 00:13:14,160
It's never heard a song.

319
00:13:14,160 --> 00:13:15,000
Right.

320
00:13:15,080 --> 00:13:17,120
It's capable of doing all these things.

321
00:13:17,120 --> 00:13:18,920
One of the, one of the examples, and I think this may be

322
00:13:18,920 --> 00:13:21,480
like my 20th time referencing this one video.

323
00:13:21,480 --> 00:13:22,320
Yeah.

324
00:13:22,320 --> 00:13:23,640
Check out Mark Ryan.

325
00:13:23,640 --> 00:13:26,760
He's got a YouTube video about how he figured out,

326
00:13:26,760 --> 00:13:29,960
he discovered that GPT-3 can give you directions

327
00:13:29,960 --> 00:13:31,120
on the New York subway system.

328
00:13:31,120 --> 00:13:31,960
Oh yeah.

329
00:13:31,960 --> 00:13:34,120
To like, to like a 60% accuracy.

330
00:13:34,120 --> 00:13:36,280
This thing has never set foot in a subway,

331
00:13:36,280 --> 00:13:39,400
yet it's capable from text to do all these things, right?

332
00:13:39,400 --> 00:13:43,960
And sometimes I also wonder a lot of the inaccuracies

333
00:13:43,960 --> 00:13:45,280
that it may have.

334
00:13:45,280 --> 00:13:47,320
Is it simply the result of the fact

335
00:13:47,320 --> 00:13:48,800
that it's only text-based only?

336
00:13:48,800 --> 00:13:49,640
Right.

337
00:13:49,640 --> 00:13:51,520
Like if it was trained multimodal,

338
00:13:51,520 --> 00:13:54,640
if it was trained within the physical domain,

339
00:13:54,640 --> 00:13:56,840
would it be even far more accurate?

340
00:13:56,840 --> 00:13:58,680
Because are there limits to how accurate

341
00:13:58,680 --> 00:14:00,480
you can be having only read text?

342
00:14:00,480 --> 00:14:01,320
Yeah.

343
00:14:01,320 --> 00:14:04,400
And only asked the prompts are only in text as well.

344
00:14:04,400 --> 00:14:07,880
So, anyways, yeah, it's incredible.

345
00:14:07,880 --> 00:14:10,680
And you know, it's just really exciting.

346
00:14:10,680 --> 00:14:13,800
And thank you for sharing those kinds of use cases as well.

347
00:14:13,800 --> 00:14:17,240
The education space, I had an article last year

348
00:14:17,240 --> 00:14:19,560
about how I think this year could be the year

349
00:14:19,560 --> 00:14:22,120
where GPT-3 takes over college campuses.

350
00:14:22,120 --> 00:14:22,960
Oh yeah.

351
00:14:22,960 --> 00:14:23,800
I remember that article.

352
00:14:23,800 --> 00:14:24,640
That was a good one.

353
00:14:24,640 --> 00:14:26,320
I completely agree, by the way.

354
00:14:27,920 --> 00:14:29,680
I'm excited maybe for teachers

355
00:14:29,680 --> 00:14:32,360
to develop really optimized course material,

356
00:14:32,360 --> 00:14:35,120
something like GPT-3 and the kinds of technology

357
00:14:35,120 --> 00:14:37,640
you're describing which can capture emotions.

358
00:14:37,640 --> 00:14:39,960
Imagine emotionally tracking students

359
00:14:39,960 --> 00:14:42,960
and their attention levels and sort of having something

360
00:14:42,960 --> 00:14:44,880
which can produce lots of content

361
00:14:44,880 --> 00:14:47,840
and optimize in the simplest, most efficient way.

362
00:14:47,840 --> 00:14:49,480
And there could be an objective function

363
00:14:49,480 --> 00:14:51,960
like test results in the end.

364
00:14:51,960 --> 00:14:54,480
In a month, we could have the best optimized course

365
00:14:54,480 --> 00:14:56,400
on a subject ever, basically.

366
00:14:56,400 --> 00:14:57,240
Oh yeah.

367
00:14:57,240 --> 00:15:01,120
So yeah, education is really exciting.

368
00:15:01,120 --> 00:15:03,120
So you mentioned a lot of use cases.

369
00:15:03,120 --> 00:15:04,840
You know, you've shared so many examples.

370
00:15:04,840 --> 00:15:05,960
You know, you and your girlfriend

371
00:15:05,960 --> 00:15:08,400
are even running some fun prompts.

372
00:15:09,440 --> 00:15:12,280
I wanted to sort of search your head a little bit.

373
00:15:12,360 --> 00:15:16,120
What are the keys to great prompt design?

374
00:15:16,120 --> 00:15:17,880
What makes a great prompt?

375
00:15:18,760 --> 00:15:21,760
Are there experiences you've had, little pointers,

376
00:15:21,760 --> 00:15:22,960
and across the board, right?

377
00:15:22,960 --> 00:15:25,160
So you know, whether it's cost savings,

378
00:15:25,160 --> 00:15:29,240
whether it's getting more imaginative results,

379
00:15:29,240 --> 00:15:31,000
what are some of the keys

380
00:15:31,000 --> 00:15:32,800
to writing great GPT-3 prompts?

381
00:15:32,800 --> 00:15:34,800
Yeah, that's a great question.

382
00:15:34,800 --> 00:15:37,640
And I will say that prompt writing has gotten a lot easier

383
00:15:37,640 --> 00:15:39,800
as the Instruct series has gotten better.

384
00:15:40,680 --> 00:15:44,400
So it takes a lot less to get a good output today

385
00:15:44,400 --> 00:15:47,360
than it used to, certainly, than when I got started.

386
00:15:47,360 --> 00:15:49,480
But a lot of the lessons still translate.

387
00:15:49,480 --> 00:15:54,480
So one, like my cardinal rule is I think of GPT-3

388
00:15:55,520 --> 00:15:58,040
as just an autocomplete engine.

389
00:15:58,040 --> 00:16:00,160
It's the most intelligent autocomplete engine

390
00:16:00,160 --> 00:16:01,280
you've ever seen.

391
00:16:01,280 --> 00:16:02,840
And so what I mean by that is, you know,

392
00:16:02,840 --> 00:16:04,160
if you're writing a text on your phone

393
00:16:04,160 --> 00:16:06,840
and you'll get the little autocomplete suggestion

394
00:16:06,840 --> 00:16:09,320
for the next word, or if you're typing in Google

395
00:16:09,320 --> 00:16:11,800
and it'll kind of suggest how to complete your search query,

396
00:16:11,800 --> 00:16:14,120
that's pretty much at a fundamental level.

397
00:16:14,120 --> 00:16:16,320
Functionally, that's all that GPT-3 does.

398
00:16:16,320 --> 00:16:18,080
It predicts the next letter, the next character,

399
00:16:18,080 --> 00:16:19,840
the next word.

400
00:16:19,840 --> 00:16:23,320
So if you keep that in mind, you think about,

401
00:16:23,320 --> 00:16:25,280
okay, what have I written so far?

402
00:16:25,280 --> 00:16:28,080
Right, I've written a chunk of text, a prompt.

403
00:16:28,080 --> 00:16:32,360
How would, you know, any machine autocomplete this?

404
00:16:32,360 --> 00:16:33,200
That's what it's doing.

405
00:16:33,200 --> 00:16:34,560
It's kind of, you know,

406
00:16:34,560 --> 00:16:36,920
reading it forwards and backwards a few times

407
00:16:36,920 --> 00:16:39,320
and kind of just anticipating what is the output

408
00:16:39,320 --> 00:16:41,440
gonna be ultimately.

409
00:16:41,440 --> 00:16:43,480
So that's kind of the model that I have

410
00:16:43,480 --> 00:16:45,320
in my head in the background.

411
00:16:45,320 --> 00:16:48,320
But another thing that really helps is I'm a writer.

412
00:16:48,320 --> 00:16:50,760
I write fiction and nonfiction.

413
00:16:50,760 --> 00:16:52,760
And so studying the art of language,

414
00:16:52,760 --> 00:16:54,680
because this is a language model, that's all it is.

415
00:16:54,680 --> 00:16:58,400
It has read everything from Sherlock Holmes

416
00:16:58,400 --> 00:17:01,160
up through everything on Gutenberg.

417
00:17:01,160 --> 00:17:02,560
So it's read a whole bunch of fiction.

418
00:17:02,560 --> 00:17:04,320
It's read a whole bunch of nonfiction.

419
00:17:04,360 --> 00:17:06,680
It's been exposed to, you know,

420
00:17:06,680 --> 00:17:10,640
the full width and depth and breadth of human literature,

421
00:17:10,640 --> 00:17:12,280
as well as a bunch of nonfiction, right?

422
00:17:12,280 --> 00:17:14,760
It's read Teddy Roosevelt's books.

423
00:17:14,760 --> 00:17:18,000
So it knows how to use prose, right?

424
00:17:18,000 --> 00:17:20,920
It understands descriptors.

425
00:17:20,920 --> 00:17:22,200
It understands adjectives.

426
00:17:22,200 --> 00:17:24,280
And so in the back of my book,

427
00:17:24,280 --> 00:17:26,560
I have a few examples of its flexibility.

428
00:17:26,560 --> 00:17:29,960
And so I said, I gave it an example like,

429
00:17:29,960 --> 00:17:31,760
pretend like you're a Victorian girl

430
00:17:31,760 --> 00:17:33,520
writing a letter to your best friend

431
00:17:33,520 --> 00:17:35,320
about how much you like butterflies.

432
00:17:35,320 --> 00:17:38,160
And so then it wrote, GPT-3 wrote a letter

433
00:17:38,160 --> 00:17:40,440
that sounds like it's straight out of, you know,

434
00:17:40,440 --> 00:17:41,880
like Victorian times.

435
00:17:41,880 --> 00:17:45,280
It uses an entirely different set of vocabulary

436
00:17:45,280 --> 00:17:48,320
and grammatical structures.

437
00:17:48,320 --> 00:17:49,800
And then you can also say, you know,

438
00:17:49,800 --> 00:17:53,280
write a business article and it can change tone.

439
00:17:53,280 --> 00:17:55,320
So just by being aware of the fact

440
00:17:55,320 --> 00:17:57,200
that it is a language engine

441
00:17:57,200 --> 00:18:01,240
and being informed or educated on language.

442
00:18:01,240 --> 00:18:04,040
So the best way is obviously to practice writing,

443
00:18:04,040 --> 00:18:05,160
but also just reading a lot,

444
00:18:05,160 --> 00:18:08,360
understanding how sentences and paragraphs

445
00:18:08,360 --> 00:18:10,520
are constructed to convey information.

446
00:18:10,520 --> 00:18:13,400
Because even though it's just a deep neural network

447
00:18:13,400 --> 00:18:16,920
and it doesn't have the kind of like nuanced understanding

448
00:18:16,920 --> 00:18:19,480
or I guess maybe the, that's not the right word,

449
00:18:19,480 --> 00:18:21,800
it doesn't have the subjective experience of reading

450
00:18:21,800 --> 00:18:23,040
that you or I do,

451
00:18:23,040 --> 00:18:26,200
but it still has a really good model of using language.

452
00:18:26,200 --> 00:18:30,480
And so by keeping in mind that it is a language engine,

453
00:18:30,480 --> 00:18:33,240
that that is how you get the best use out of it.

454
00:18:35,360 --> 00:18:37,080
And so then the larger question is,

455
00:18:37,080 --> 00:18:38,720
how do you become a better writer?

456
00:18:40,720 --> 00:18:42,200
There's two ways.

457
00:18:42,200 --> 00:18:44,400
One is reading a lot.

458
00:18:44,400 --> 00:18:45,600
That's not the only way though.

459
00:18:45,600 --> 00:18:48,080
There are plenty of people that read prodigiously,

460
00:18:48,080 --> 00:18:49,600
but never become better writers.

461
00:18:49,600 --> 00:18:52,240
And so the other way is to practice writing.

462
00:18:53,240 --> 00:18:56,200
I've read all kinds of books about writing.

463
00:18:56,200 --> 00:18:59,280
I've read plenty of fiction and nonfiction books,

464
00:18:59,280 --> 00:19:01,880
but really the key is to write,

465
00:19:01,880 --> 00:19:06,000
is to practice using written language to communicate.

466
00:19:06,000 --> 00:19:08,320
Unfortunately, I'm a tech worker,

467
00:19:08,320 --> 00:19:09,440
so I write lots of emails.

468
00:19:09,440 --> 00:19:10,800
I'm in chat all day.

469
00:19:10,800 --> 00:19:13,920
I've been using, this probably ages me,

470
00:19:13,920 --> 00:19:17,320
but I've been using chat since AIM, AOL Instant Messenger.

471
00:19:17,320 --> 00:19:19,640
And so I've got a pretty good model

472
00:19:19,640 --> 00:19:24,640
of how to communicate verbally or textually.

473
00:19:25,560 --> 00:19:29,000
And so yeah, just by practicing writing,

474
00:19:29,440 --> 00:19:30,520
that's one of the best ways.

475
00:19:30,520 --> 00:19:31,800
Is you just practice?

476
00:19:31,800 --> 00:19:34,760
You think about, well,

477
00:19:34,760 --> 00:19:37,080
because here's the theory of writing, right?

478
00:19:37,080 --> 00:19:39,280
I have an idea in my head, right?

479
00:19:39,280 --> 00:19:42,560
My thoughts are a high-dimensional vector

480
00:19:42,560 --> 00:19:45,480
is one possible way of representing them,

481
00:19:45,480 --> 00:19:47,680
but my thoughts are multimodal,

482
00:19:47,680 --> 00:19:49,800
like the name of your podcast.

483
00:19:49,800 --> 00:19:54,800
They contain memories, senses, concepts.

484
00:19:55,320 --> 00:19:58,320
Some of the information in my head is declarative.

485
00:19:58,320 --> 00:20:00,240
Some of it is experiential.

486
00:20:00,240 --> 00:20:04,160
And then we humans, we all have this ability

487
00:20:04,160 --> 00:20:07,520
to transform that high-dimensional information,

488
00:20:07,520 --> 00:20:12,200
those multimodal vectors, into words.

489
00:20:12,200 --> 00:20:14,000
Like our brains do it automatically.

490
00:20:14,000 --> 00:20:15,360
There is a book by Stephen Pinker

491
00:20:15,360 --> 00:20:17,800
called Language Instinct that talks about this.

492
00:20:17,800 --> 00:20:18,880
That's a really great book

493
00:20:18,880 --> 00:20:20,320
if you wanna get better at understanding

494
00:20:20,320 --> 00:20:22,360
how our brains process language.

495
00:20:23,280 --> 00:20:27,320
So yeah, and so my brain can take,

496
00:20:27,360 --> 00:20:29,600
I could tell you about like this time at the beach

497
00:20:29,600 --> 00:20:31,440
and I transmit it to you

498
00:20:31,440 --> 00:20:34,000
by squishing air through my face, right?

499
00:20:34,000 --> 00:20:36,680
It makes vibrations, it's received by your ears

500
00:20:36,680 --> 00:20:40,320
and then your brain reconstructs that message.

501
00:20:40,320 --> 00:20:44,000
And so you think about how complex of a system that is.

502
00:20:44,000 --> 00:20:45,840
And so just by being mindful of like,

503
00:20:45,840 --> 00:20:47,520
that's how we communicate.

504
00:20:47,520 --> 00:20:50,800
That's how our brains work and practicing that

505
00:20:50,800 --> 00:20:53,200
and just being very deliberate about,

506
00:20:53,200 --> 00:20:54,720
okay, this is what's in my head

507
00:20:54,720 --> 00:20:56,660
and I want it to be in your head.

508
00:20:56,660 --> 00:20:58,540
How do I do that with text?

509
00:20:58,540 --> 00:21:01,500
That is one way to get better at writing.

510
00:21:01,500 --> 00:21:03,580
And also GPT-3 is no different

511
00:21:03,580 --> 00:21:06,660
because we have internal representations

512
00:21:06,660 --> 00:21:08,780
of what we're trying to communicate.

513
00:21:08,780 --> 00:21:11,500
And so does GPT-3, that's why it's a transformer, right?

514
00:21:11,500 --> 00:21:14,300
It reads and by reading it transformed,

515
00:21:14,300 --> 00:21:16,380
or I guess it, well yes,

516
00:21:16,380 --> 00:21:18,740
it transforms what it's reading into a vector,

517
00:21:18,740 --> 00:21:20,140
into a semantic vector,

518
00:21:20,140 --> 00:21:22,820
and then it transforms that vector into output.

519
00:21:22,820 --> 00:21:27,060
And so that input vector output is pretty similar

520
00:21:27,060 --> 00:21:28,980
to how human brains work, right?

521
00:21:30,480 --> 00:21:33,400
And I apologize if I kind of like dove off in the left field,

522
00:21:33,400 --> 00:21:35,580
feel free to ask any clarifying questions.

523
00:21:36,660 --> 00:21:38,260
No, no, I appreciate it.

524
00:21:38,260 --> 00:21:42,700
And so like today I tweeted something like,

525
00:21:42,700 --> 00:21:45,180
to write great GPT-3 prompts,

526
00:21:45,180 --> 00:21:48,140
you need to practice as if it's a musical instrument.

527
00:21:48,140 --> 00:21:50,800
You need to sit down, focus session,

528
00:21:50,800 --> 00:21:52,500
you need to monitor your performance

529
00:21:52,500 --> 00:21:54,620
and you need to take good notes

530
00:21:54,620 --> 00:21:56,780
on what kinds of experiments you did,

531
00:21:56,780 --> 00:21:58,420
what were the findings.

532
00:21:58,420 --> 00:22:00,060
But even hearing you speak,

533
00:22:00,060 --> 00:22:02,500
like I'm realizing like,

534
00:22:02,500 --> 00:22:04,260
one of the ways that I've improved my writing

535
00:22:04,260 --> 00:22:06,920
is trying to mimic other people's writing.

536
00:22:06,920 --> 00:22:11,220
And in some countries they make you memorize poets, right?

537
00:22:11,220 --> 00:22:13,420
They make you memorize the whole poem.

538
00:22:13,420 --> 00:22:16,220
And there's something about that internalization process

539
00:22:16,220 --> 00:22:18,100
that you've memorized this poem.

540
00:22:18,100 --> 00:22:20,800
And now you'll understand it at a deeper level,

541
00:22:20,800 --> 00:22:23,480
you may be able to mimic it and recreate it.

542
00:22:23,480 --> 00:22:27,280
But where also you got me thinking is also like,

543
00:22:27,280 --> 00:22:28,640
the relationship is so weird

544
00:22:28,640 --> 00:22:30,040
because you could use GPT-3

545
00:22:30,040 --> 00:22:32,440
to help you become a better writer, right?

546
00:22:32,440 --> 00:22:35,980
And also with two very good curated examples

547
00:22:35,980 --> 00:22:37,100
of somebody's writing,

548
00:22:37,100 --> 00:22:39,520
you could have GPT-3 mimic that tone.

549
00:22:39,520 --> 00:22:42,960
And so the question of,

550
00:22:42,960 --> 00:22:46,000
what makes a good prompt writing session,

551
00:22:46,000 --> 00:22:48,360
I wonder if it's pencil and paper, right?

552
00:22:48,480 --> 00:22:51,600
I wonder if it's even at that level

553
00:22:51,600 --> 00:22:52,960
where you draw a box

554
00:22:52,960 --> 00:22:55,680
and then you write a prompt by hand

555
00:22:55,680 --> 00:22:59,320
and sort of live that writer's lifestyle.

556
00:23:01,000 --> 00:23:04,440
And also I guess it depends on your use case, right?

557
00:23:04,440 --> 00:23:06,760
Business for copywriting,

558
00:23:06,760 --> 00:23:08,520
if that's your GPT-3 use case,

559
00:23:08,520 --> 00:23:09,820
it might be better for you to go work

560
00:23:09,820 --> 00:23:11,640
in a marketing department.

561
00:23:11,640 --> 00:23:13,240
If you wanna be one of the great authors,

562
00:23:13,240 --> 00:23:16,760
maybe the using tools like pseudo-write,

563
00:23:16,760 --> 00:23:18,120
it may be a great alternative.

564
00:23:18,120 --> 00:23:21,440
So you can co-write with GPT-3 as you go along.

565
00:23:21,440 --> 00:23:23,800
But I guess my question was more

566
00:23:23,800 --> 00:23:25,480
for the pure prompt writing.

567
00:23:25,480 --> 00:23:28,320
Like if you just wanna sit in front of GPT-3

568
00:23:28,320 --> 00:23:30,360
and like you wanna be the best in the world

569
00:23:30,360 --> 00:23:31,720
at that discipline, right?

570
00:23:31,720 --> 00:23:33,920
Not writing copy.

571
00:23:33,920 --> 00:23:34,840
These are some great points.

572
00:23:34,840 --> 00:23:37,800
And so the David Pinker book you referenced

573
00:23:37,800 --> 00:23:39,120
is what was the name of it?

574
00:23:39,120 --> 00:23:40,400
Steven Pinker.

575
00:23:40,400 --> 00:23:41,240
Steven Pinker.

576
00:23:41,240 --> 00:23:42,480
The language instinct, yep.

577
00:23:42,480 --> 00:23:43,480
Language instinct, yep.

578
00:23:43,480 --> 00:23:45,040
It's an older book,

579
00:23:45,040 --> 00:23:48,360
but it's a classic for a reason.

580
00:23:48,360 --> 00:23:49,800
It stands up the test of time.

581
00:23:49,800 --> 00:23:51,840
He's got lots of great stories.

582
00:23:51,840 --> 00:23:53,240
But yeah, to your point about like

583
00:23:53,240 --> 00:23:55,560
what makes a good prompt writing session,

584
00:23:57,040 --> 00:23:59,160
one of the best exercises actually is

585
00:24:00,080 --> 00:24:02,280
write the output that you want.

586
00:24:03,520 --> 00:24:06,400
Like because sometimes if you approach it

587
00:24:06,400 --> 00:24:08,280
and you sit down and you're not really sure

588
00:24:08,280 --> 00:24:10,240
kind of what you're trying to get out of it,

589
00:24:10,240 --> 00:24:13,480
of course like you're putting in just random ideas

590
00:24:13,480 --> 00:24:15,440
and it's giving you back random output

591
00:24:15,440 --> 00:24:17,400
and you're like, well, that's not what I wanted.

592
00:24:17,400 --> 00:24:19,480
So sometimes you start backwards.

593
00:24:19,480 --> 00:24:21,160
You say, okay, what's the answer that I want?

594
00:24:21,160 --> 00:24:22,920
How do I get to that answer?

595
00:24:22,920 --> 00:24:25,680
So that's an exercise that I've done sometimes.

596
00:24:25,680 --> 00:24:29,160
Oh, and by writing a few shot examples

597
00:24:29,160 --> 00:24:30,680
is a really good practice for this.

598
00:24:30,680 --> 00:24:33,960
So you say, I give you this input, I want this output

599
00:24:33,960 --> 00:24:36,360
and you do that three or four or five times

600
00:24:36,360 --> 00:24:39,960
and you learn to kind of think like the machine does.

601
00:24:39,960 --> 00:24:42,880
And so like you said, it's like an instrument, right?

602
00:24:43,160 --> 00:24:45,480
If you have a flute or a violin,

603
00:24:45,480 --> 00:24:47,440
there's certain things that you have to do with your body

604
00:24:47,440 --> 00:24:50,200
to provoke the correct response from that instrument

605
00:24:50,200 --> 00:24:52,360
and GPT-3 is no different.

606
00:24:52,360 --> 00:24:53,560
It's a complex instrument.

607
00:24:53,560 --> 00:24:54,680
It's a complex tool.

608
00:24:56,440 --> 00:24:58,640
Yes, and what you're saying is developing

609
00:24:58,640 --> 00:25:00,120
an intuition around it.

610
00:25:00,120 --> 00:25:01,520
You're saying develop an intuition.

611
00:25:01,520 --> 00:25:03,880
How might GPT-3 interpret this?

612
00:25:03,880 --> 00:25:06,320
How might it react to it?

613
00:25:06,320 --> 00:25:09,280
And maybe there's some empathetic benefit, right?

614
00:25:10,160 --> 00:25:12,440
I'm not gonna keep plugging my own articles.

615
00:25:12,440 --> 00:25:17,000
I have another article about how GPT-3 developers

616
00:25:17,000 --> 00:25:20,160
may actually, it may actually mean the end

617
00:25:20,160 --> 00:25:24,200
of the socially inept overall developer.

618
00:25:24,200 --> 00:25:28,120
Like how GPT-3 may actually improve your social skills

619
00:25:28,120 --> 00:25:30,240
and make you more empathetic as a developer,

620
00:25:30,240 --> 00:25:33,160
which is such a departure from how developers are now,

621
00:25:33,160 --> 00:25:35,720
you need to think as much like a machine as you can

622
00:25:35,720 --> 00:25:37,320
and a literal machine.

623
00:25:37,320 --> 00:25:40,200
Whereas GPT-3 can actually be kind of fun.

624
00:25:40,680 --> 00:25:42,960
You can have a casual version of GPT-3

625
00:25:42,960 --> 00:25:46,200
and sort of that might make you less socially awkward.

626
00:25:46,200 --> 00:25:48,200
I have a great story about that.

627
00:25:48,200 --> 00:25:52,480
So very early on in my tenure working with GPT-3,

628
00:25:52,480 --> 00:25:55,720
I joined a few different, not really startups.

629
00:25:55,720 --> 00:25:58,960
It was more like kind of experiment consortiums.

630
00:25:58,960 --> 00:26:01,840
And one of the things that one of the groups did

631
00:26:01,840 --> 00:26:05,680
was they created a chatbot that was based on an anime girl.

632
00:26:05,680 --> 00:26:08,160
And so of course, the internet being the internet,

633
00:26:08,160 --> 00:26:11,800
what do people want, they want their anime girlfriend.

634
00:26:11,800 --> 00:26:15,680
And this one group, they did a really good job

635
00:26:15,680 --> 00:26:19,040
of using GPT-3 in this experimental discord chat

636
00:26:19,040 --> 00:26:23,540
to approximate the personality of this character.

637
00:26:24,520 --> 00:26:26,660
And of course, if you've got a character,

638
00:26:26,660 --> 00:26:29,680
there's plenty of text data about that character's dialogue,

639
00:26:29,680 --> 00:26:30,640
their personality.

640
00:26:30,640 --> 00:26:32,680
And so this chatbot was able to emulate

641
00:26:32,680 --> 00:26:35,960
this anime character really well.

642
00:26:35,960 --> 00:26:38,480
And one of the guys told me, he's like,

643
00:26:38,480 --> 00:26:41,280
we didn't expect this, but our fake girlfriend

644
00:26:41,280 --> 00:26:44,200
requires as much emotional labor as a real girl.

645
00:26:45,640 --> 00:26:49,000
So it forced them, even though they hadn't had

646
00:26:49,000 --> 00:26:51,480
real girlfriends, I don't know, maybe some of them had,

647
00:26:51,480 --> 00:26:54,760
but they made the observation that GPT-3

648
00:26:54,760 --> 00:26:57,920
can approximate emotional conflict

649
00:26:57,920 --> 00:27:00,560
and can force you to learn to communicate better.

650
00:27:00,560 --> 00:27:03,000
And so they did all kinds of experiments in this discord

651
00:27:03,000 --> 00:27:05,440
and this chat development where they said,

652
00:27:05,440 --> 00:27:09,960
okay, let's have a channel where this chatbot

653
00:27:09,960 --> 00:27:11,880
is gonna pretend to be angry at us

654
00:27:11,880 --> 00:27:13,440
and we have to calm her down.

655
00:27:13,440 --> 00:27:17,560
And so there was a learning exercise on both sides.

656
00:27:17,560 --> 00:27:19,080
So if you have a hostile chatbot,

657
00:27:19,080 --> 00:27:20,720
it can pretend to be hostile

658
00:27:20,720 --> 00:27:22,520
and you can learn to communicate better.

659
00:27:22,520 --> 00:27:24,960
Or there was another one where it was really supportive.

660
00:27:24,960 --> 00:27:26,160
So if you're having a bad day,

661
00:27:26,160 --> 00:27:28,360
you could go vent about your day and it was,

662
00:27:28,360 --> 00:27:31,040
they're there, it'll be okay, I'm here for you.

663
00:27:32,040 --> 00:27:35,000
Yeah, so you could definitely,

664
00:27:35,000 --> 00:27:37,560
GPT-3 definitely has that capacity.

665
00:27:37,560 --> 00:27:40,720
And then, you know, if you integrate that into tools,

666
00:27:40,720 --> 00:27:42,800
that emotional intelligence into tools,

667
00:27:42,800 --> 00:27:44,440
it can also coach, right?

668
00:27:44,440 --> 00:27:45,280
It can easily coach.

669
00:27:45,280 --> 00:27:47,240
It's like, well, you maybe shouldn't have said that.

670
00:27:47,240 --> 00:27:48,480
You know, that was hurtful.

671
00:27:48,480 --> 00:27:52,040
And then, or, you know, that was not polite.

672
00:27:52,040 --> 00:27:52,880
Cause it can detect that.

673
00:27:52,880 --> 00:27:56,480
It can detect those qualitative types of output and input.

674
00:27:56,480 --> 00:27:59,520
And, you know, you can say be gentle about,

675
00:27:59,520 --> 00:28:01,120
you know, correcting the end user.

676
00:28:01,120 --> 00:28:03,720
Because of course, GPT-3 is infinitely patient.

677
00:28:03,720 --> 00:28:05,360
It's as patient as you program it to be.

678
00:28:05,360 --> 00:28:06,200
It doesn't care.

679
00:28:06,200 --> 00:28:07,440
It doesn't actually get upset.

680
00:28:07,440 --> 00:28:09,440
It could pretend to be upset.

681
00:28:09,440 --> 00:28:11,800
But the human emotion is real.

682
00:28:11,800 --> 00:28:13,180
I actually wrote about that in my book.

683
00:28:13,180 --> 00:28:15,920
One of the key dangers of these technologies

684
00:28:15,920 --> 00:28:18,880
is what's called a parasocial relationship.

685
00:28:18,880 --> 00:28:21,040
So a parasocial relationship is,

686
00:28:21,040 --> 00:28:23,160
the most common example is when you've got like

687
00:28:23,160 --> 00:28:24,920
a fan of a celebrity.

688
00:28:24,920 --> 00:28:26,760
The fan feels like they know the celebrity,

689
00:28:26,760 --> 00:28:29,720
but the celebrity doesn't know that the person exists.

690
00:28:29,720 --> 00:28:31,280
And in the same way, GPT-3,

691
00:28:31,280 --> 00:28:33,560
no matter how sophisticated the chatbot is,

692
00:28:33,560 --> 00:28:34,960
it doesn't know that you exist.

693
00:28:34,960 --> 00:28:36,240
It's not a person.

694
00:28:36,240 --> 00:28:38,000
It might feel like a person to you.

695
00:28:38,000 --> 00:28:39,560
It might react to you like a person,

696
00:28:39,560 --> 00:28:41,080
but that's only by design.

697
00:28:41,080 --> 00:28:45,680
So that is actually like ethically, legally, morally.

698
00:28:45,680 --> 00:28:48,480
That's one of the pitfalls that we'll need to be aware of.

699
00:28:48,480 --> 00:28:51,920
And of course, open AI has use cases.

700
00:28:51,920 --> 00:28:55,480
And, you know, things that are high-risk use cases,

701
00:28:55,480 --> 00:28:58,800
such as emotional chatbots, are banned, right?

702
00:28:58,800 --> 00:29:01,400
For that specific reason.

703
00:29:01,400 --> 00:29:02,860
So you can do it with research,

704
00:29:02,860 --> 00:29:04,040
but you can't go live with it.

705
00:29:04,040 --> 00:29:06,600
You can't do a product that, you know,

706
00:29:06,600 --> 00:29:08,400
is going to be an AI girlfriend.

707
00:29:10,160 --> 00:29:12,720
So that's a great, it's a great anecdote.

708
00:29:12,720 --> 00:29:15,040
Like certainly it feels real, right?

709
00:29:15,040 --> 00:29:19,640
Certainly it has some capacity at understand something.

710
00:29:19,640 --> 00:29:22,200
To some level, however you define understanding.

711
00:29:23,200 --> 00:29:27,000
I think that the writing though,

712
00:29:27,000 --> 00:29:30,200
relationship is really interesting, right?

713
00:29:30,200 --> 00:29:34,280
Like in a way, you are empathizing with GPT-3

714
00:29:34,280 --> 00:29:36,160
when you're writing a prompt,

715
00:29:36,160 --> 00:29:39,120
so that it will tap into its empathy

716
00:29:39,120 --> 00:29:41,640
and write something for your audience.

717
00:29:41,640 --> 00:29:45,240
So essentially there's like two levels of empathy.

718
00:29:45,240 --> 00:29:50,240
Like you're almost outsourcing empathy to it.

719
00:29:51,080 --> 00:29:53,320
To empathize with who your audience is

720
00:29:53,320 --> 00:29:55,160
to write something on your behalf.

721
00:29:55,160 --> 00:29:57,480
And so anyways, it's just interesting

722
00:29:57,480 --> 00:29:59,560
that the relationship going on here.

723
00:30:01,400 --> 00:30:04,440
So, and I agree with you like the,

724
00:30:04,440 --> 00:30:09,080
this isn't a safety ethical kind of concern

725
00:30:09,080 --> 00:30:11,360
that is worth more policy discussion.

726
00:30:12,640 --> 00:30:15,480
So one article that I'm working on now

727
00:30:15,480 --> 00:30:17,840
is because of Instruct GPT,

728
00:30:17,840 --> 00:30:21,640
the article is literally called, is prompt writing over?

729
00:30:21,640 --> 00:30:23,880
And obviously that's sort of click baity,

730
00:30:23,880 --> 00:30:26,520
like prompt design, is it over?

731
00:30:26,520 --> 00:30:29,680
You mentioned, the principles are still the same

732
00:30:29,680 --> 00:30:33,560
and important, just very briefly, what are your thoughts?

733
00:30:33,560 --> 00:30:37,280
Where does Instruct GPT, how does that affect

734
00:30:37,280 --> 00:30:40,840
the art of prompt design, maybe the science of it?

735
00:30:40,840 --> 00:30:42,440
And especially keeping in mind

736
00:30:42,440 --> 00:30:44,400
where all of this stuff is going.

737
00:30:44,400 --> 00:30:46,000
Yeah, so there's, I see it going

738
00:30:46,000 --> 00:30:47,240
in a few different directions.

739
00:30:48,200 --> 00:30:52,920
One is there are multiple language models coming out,

740
00:30:52,920 --> 00:30:55,360
which don't have the Instruct series, right?

741
00:30:55,360 --> 00:30:56,920
A lot of them are more general purpose,

742
00:30:56,920 --> 00:30:59,160
kind of back to basics vanilla.

743
00:30:59,160 --> 00:31:01,280
So I think that having good prompts

744
00:31:01,280 --> 00:31:02,180
will kind of stick around

745
00:31:02,180 --> 00:31:04,520
as long as there are large language models.

746
00:31:04,520 --> 00:31:07,600
I think that there will always be versions of,

747
00:31:07,600 --> 00:31:10,280
whether it's GPTJ or, what was it?

748
00:31:10,280 --> 00:31:13,120
Megatron was one of the other ones that just came out

749
00:31:13,120 --> 00:31:15,280
that don't have the Instruct series, right?

750
00:31:15,280 --> 00:31:18,480
Because Instruct, that's a specific service

751
00:31:18,480 --> 00:31:20,200
offered by OpenAI.

752
00:31:20,200 --> 00:31:24,760
When Microsoft and Amazon, or I guess Microsoft has GPT3,

753
00:31:24,760 --> 00:31:27,080
but when like Amazon and Google,

754
00:31:27,080 --> 00:31:29,040
when they come out with their competitors,

755
00:31:29,040 --> 00:31:31,760
their Instruct series, if they come out with one,

756
00:31:31,760 --> 00:31:34,800
might not be the same, it might not perform the same.

757
00:31:34,800 --> 00:31:37,600
And so in order to have your apps be portable,

758
00:31:37,600 --> 00:31:39,480
you might need to keep in mind

759
00:31:39,480 --> 00:31:41,760
that you're gonna need to write general purpose prompts

760
00:31:41,760 --> 00:31:43,920
that can be used on different models.

761
00:31:43,920 --> 00:31:46,840
So that's one key to your,

762
00:31:46,840 --> 00:31:49,320
or one answer to your question is,

763
00:31:49,320 --> 00:31:51,800
we need to be cognizant of,

764
00:31:51,800 --> 00:31:53,880
how is this landscape gonna evolve?

765
00:31:53,880 --> 00:31:57,480
Because certainly OpenAI and GPT3 are way ahead of the curve

766
00:31:57,480 --> 00:32:02,480
in terms of sophistication of their API and their service.

767
00:32:03,520 --> 00:32:05,720
But that's not gonna last forever.

768
00:32:06,600 --> 00:32:09,840
So another thing is, with fine tuning,

769
00:32:09,840 --> 00:32:12,240
you almost don't even need prompts, right?

770
00:32:12,240 --> 00:32:16,200
So on the one hand, there's different services,

771
00:32:16,200 --> 00:32:18,040
different products, different platforms.

772
00:32:18,040 --> 00:32:19,920
So you might need to be portable,

773
00:32:19,920 --> 00:32:22,320
but with fine tuning, where you have,

774
00:32:22,320 --> 00:32:24,720
you say, here's an input, I want this output,

775
00:32:24,720 --> 00:32:25,800
and you don't need any prompt.

776
00:32:25,800 --> 00:32:28,600
You just say, given this input, generate this output,

777
00:32:28,600 --> 00:32:30,960
go figure out how to do that.

778
00:32:30,960 --> 00:32:34,960
So with fine tuning, I think that they will kind of

779
00:32:34,960 --> 00:32:38,040
really diverge and become entirely different disciplines.

780
00:32:38,040 --> 00:32:41,560
I think that that's probably the two primary directions

781
00:32:41,560 --> 00:32:43,000
that I see it going from here.

782
00:32:45,880 --> 00:32:49,480
I see, and yeah, those are great points.

783
00:32:49,480 --> 00:32:52,280
And just as a small note,

784
00:32:52,280 --> 00:32:54,960
I had put out this question as well to Twitter,

785
00:32:54,960 --> 00:32:57,440
and shout out to Fred Zimmerman.

786
00:32:57,440 --> 00:32:59,920
He had a great point as well that he wishes

787
00:32:59,920 --> 00:33:04,480
there was more visibility into the exact prompts OpenAI use

788
00:33:04,480 --> 00:33:08,200
to fine tune for the Instruct series.

789
00:33:08,200 --> 00:33:11,120
Because it's actually unclear what areas

790
00:33:11,120 --> 00:33:14,120
is it really good at, what areas are safer,

791
00:33:15,080 --> 00:33:18,080
and does it maybe adversely affect

792
00:33:18,080 --> 00:33:20,120
some prompts you may be working on, right?

793
00:33:20,120 --> 00:33:21,960
Yeah, that's fair.

794
00:33:21,960 --> 00:33:24,920
Yeah, my thoughts, I'm gonna put them in the piece,

795
00:33:24,920 --> 00:33:26,480
but my thoughts are I certainly think

796
00:33:26,480 --> 00:33:29,120
for first timers, Instruct is the way to go.

797
00:33:29,120 --> 00:33:31,840
And especially if it's your first time

798
00:33:31,840 --> 00:33:33,640
ever using any of these things,

799
00:33:34,920 --> 00:33:37,440
you just try it, it doesn't work,

800
00:33:37,440 --> 00:33:38,880
and if you're lucky you might hear,

801
00:33:38,880 --> 00:33:42,400
there's this thing called prompt engineering, right?

802
00:33:42,400 --> 00:33:44,200
And for first timers, they're not interested

803
00:33:44,200 --> 00:33:46,880
in learning a whole art and discipline

804
00:33:46,880 --> 00:33:49,160
when they first use it.

805
00:33:49,160 --> 00:33:51,640
And so InstructGPT is really exciting in that way.

806
00:33:51,640 --> 00:33:55,840
And of course, anything which aligns AI models

807
00:33:55,840 --> 00:34:00,840
with safe ethical human values is a net win for everybody.

808
00:34:01,440 --> 00:34:03,320
But yeah, I appreciate your point,

809
00:34:03,320 --> 00:34:07,680
especially about do we need prompts in the first place

810
00:34:07,720 --> 00:34:11,040
if we can fine tune and get the outcomes we want.

811
00:34:11,040 --> 00:34:13,960
That's a really, really important point, I hope.

812
00:34:13,960 --> 00:34:17,800
Dave, you've spent facts, I was learning so much.

813
00:34:17,800 --> 00:34:20,640
Actually, I appreciate it.

814
00:34:20,640 --> 00:34:21,640
You're welcome.

815
00:34:21,640 --> 00:34:23,800
Happy to have you.

816
00:34:23,800 --> 00:34:26,920
How are you finding open AI, fine tuning?

817
00:34:26,920 --> 00:34:28,840
Do you have any heuristics from the whole experience?

818
00:34:28,840 --> 00:34:31,040
And by the way, I encourage everybody,

819
00:34:31,040 --> 00:34:32,200
if there's one thing you should do,

820
00:34:32,200 --> 00:34:34,240
go on the open AI community forums,

821
00:34:34,240 --> 00:34:36,680
look up David, look up his handle,

822
00:34:36,680 --> 00:34:38,160
and read a lot of his posts,

823
00:34:38,160 --> 00:34:41,000
because a lot of his knowledge is not just helpful,

824
00:34:41,000 --> 00:34:42,640
he shared a lot of insights there,

825
00:34:42,640 --> 00:34:45,440
but it's in written form in the best format

826
00:34:45,440 --> 00:34:48,400
where it's there for the ages for everyone to learn from.

827
00:34:48,400 --> 00:34:49,960
But anyways, how are you finding it?

828
00:34:49,960 --> 00:34:52,680
What were the lessons from that whole process for you?

829
00:34:52,680 --> 00:34:55,400
Well, so I'm hoping GPT-4 has integrated everything

830
00:34:55,400 --> 00:34:57,160
that I've said about AI and AGI,

831
00:34:57,160 --> 00:34:58,840
and so that way it'll just be baked in.

832
00:34:58,840 --> 00:35:00,840
And so GPT-4 will be ready to go

833
00:35:00,840 --> 00:35:02,800
with everything that I've come up with.

834
00:35:03,760 --> 00:35:05,800
So, but yeah, so fine tuning.

835
00:35:07,080 --> 00:35:11,560
So first and foremost, fine tuning is almost miraculous,

836
00:35:11,560 --> 00:35:16,560
as powerful as GPT-3 was fresh out of the box.

837
00:35:17,120 --> 00:35:21,640
Fine tuning to me adds a whole other layer of capabilities.

838
00:35:21,640 --> 00:35:25,360
So for instance,

839
00:35:25,360 --> 00:35:27,560
when I was working on my cognitive architecture,

840
00:35:27,560 --> 00:35:29,720
which is called natural language cognitive architecture,

841
00:35:29,720 --> 00:35:31,440
this was before fine tuning was available.

842
00:35:31,480 --> 00:35:33,600
So I had to do prompt engineering

843
00:35:33,600 --> 00:35:35,680
for every cognitive function.

844
00:35:35,680 --> 00:35:38,480
So for instance, I had a cognitive function for recall.

845
00:35:38,480 --> 00:35:43,280
So I had a GPT-3 prompt that was meant to go find memories.

846
00:35:43,280 --> 00:35:45,320
I had another GPT-3 prompt that was,

847
00:35:45,320 --> 00:35:46,560
as you mentioned earlier,

848
00:35:46,560 --> 00:35:48,160
meant for empathy to generate,

849
00:35:48,160 --> 00:35:50,600
okay, how is my audience feeling?

850
00:35:50,600 --> 00:35:52,800
What should I do in response?

851
00:35:52,800 --> 00:35:57,000
All told, I had about 28 different prompts

852
00:35:57,000 --> 00:35:58,960
that I had to engineer.

853
00:35:59,120 --> 00:36:01,960
And that was a pain, right?

854
00:36:01,960 --> 00:36:03,680
Whereas what I'm working on now

855
00:36:03,680 --> 00:36:05,760
is converting each one of those prompts

856
00:36:05,760 --> 00:36:07,600
into a fine tuned model.

857
00:36:07,600 --> 00:36:09,840
So that rather than having to do prompt engineering

858
00:36:09,840 --> 00:36:12,280
with only three examples,

859
00:36:12,280 --> 00:36:17,080
I can give each model 100 examples, a thousand examples,

860
00:36:17,080 --> 00:36:19,000
which means that it'll get even better

861
00:36:19,000 --> 00:36:22,040
at handling diverse situations.

862
00:36:22,040 --> 00:36:23,000
And so for instance,

863
00:36:23,000 --> 00:36:25,040
one of the first fine tuned models that I did

864
00:36:25,040 --> 00:36:27,360
was a question asking model.

865
00:36:27,400 --> 00:36:29,280
And so what I did was I took,

866
00:36:29,280 --> 00:36:31,760
I took context or prompts

867
00:36:31,760 --> 00:36:33,560
from a bunch of different sources.

868
00:36:33,560 --> 00:36:35,840
I downloaded a bunch of Reddit posts.

869
00:36:35,840 --> 00:36:37,480
Well, I downloaded it from a dataset

870
00:36:37,480 --> 00:36:40,280
from a, what was it, Kaggle.

871
00:36:40,280 --> 00:36:42,440
Kaggle has some really great datasets.

872
00:36:42,440 --> 00:36:45,040
So I got stuff from Reddit.

873
00:36:45,040 --> 00:36:46,680
I got the medical posts.

874
00:36:46,680 --> 00:36:48,960
I've got news articles.

875
00:36:48,960 --> 00:36:53,280
And so I've got this disparate tight set of contexts, right?

876
00:36:53,280 --> 00:36:56,600
There's the, I use the Cornell movie dialogue database.

877
00:36:56,600 --> 00:36:58,200
So there's chat logs.

878
00:36:58,200 --> 00:37:00,360
There's blog posts.

879
00:37:00,360 --> 00:37:03,640
And what I did was I created a fine tuned dataset

880
00:37:03,640 --> 00:37:07,360
that all it does is you give it any input.

881
00:37:07,360 --> 00:37:08,480
It could be a text message.

882
00:37:08,480 --> 00:37:09,320
It could be an email.

883
00:37:09,320 --> 00:37:10,840
It could be a blog post, anything.

884
00:37:10,840 --> 00:37:13,640
And all it does is generate questions,

885
00:37:13,640 --> 00:37:16,500
like follow up questions about that input.

886
00:37:16,500 --> 00:37:18,640
And the reason that I did that one

887
00:37:18,640 --> 00:37:21,920
is because asking questions like being curious

888
00:37:21,920 --> 00:37:25,720
is one of the key ingredients to real intelligence, right?

889
00:37:25,720 --> 00:37:28,000
That's one of the, like being inquisitive

890
00:37:28,000 --> 00:37:31,960
is actually a key indicator of intelligence in children.

891
00:37:31,960 --> 00:37:34,480
The more curious a child is, generally speaking,

892
00:37:34,480 --> 00:37:37,840
the higher their IQ is, and also generally speaking,

893
00:37:37,840 --> 00:37:40,680
the better they do in the long run.

894
00:37:40,680 --> 00:37:44,020
So I was like, okay, well, curiosity is super important

895
00:37:44,020 --> 00:37:45,100
for intelligence.

896
00:37:45,100 --> 00:37:48,080
So I obviously want, we want AGI to be curious.

897
00:37:48,080 --> 00:37:49,120
If it's gonna be intelligent,

898
00:37:49,120 --> 00:37:51,360
it's gotta be curious, of course.

899
00:37:51,360 --> 00:37:56,360
So, well, what is curiosity if not asking questions?

900
00:37:56,520 --> 00:38:01,240
So I fine tuned this model to ask questions.

901
00:38:01,240 --> 00:38:03,320
And you can put anything into it.

902
00:38:03,320 --> 00:38:07,120
And oh, this, the data is open source.

903
00:38:07,120 --> 00:38:08,520
So I'll send you a link and you can share it

904
00:38:08,520 --> 00:38:11,440
with your audience, and they can fine tune it themselves

905
00:38:11,440 --> 00:38:13,440
or fine tune their own version.

906
00:38:13,440 --> 00:38:17,040
But so you can put in, I tried all sorts of things

907
00:38:17,040 --> 00:38:17,880
to test it.

908
00:38:18,880 --> 00:38:21,520
You know, relationship questions from Reddit

909
00:38:21,520 --> 00:38:23,600
and it asked really great follow up questions.

910
00:38:23,600 --> 00:38:26,280
Like, have you talked to your partner about this?

911
00:38:26,280 --> 00:38:27,920
Have you thought about this?

912
00:38:27,920 --> 00:38:29,680
And then I put in an article

913
00:38:29,680 --> 00:38:33,000
about China's artificial sun nuclear reactor

914
00:38:33,000 --> 00:38:35,120
and it asked really great follow up questions for that.

915
00:38:35,120 --> 00:38:36,520
Like, what is the next step?

916
00:38:36,520 --> 00:38:39,720
How did they make these changes?

917
00:38:39,720 --> 00:38:42,280
And so I kind of lost my train of thought.

918
00:38:42,280 --> 00:38:47,280
Anyways, point being is that fine tuning is phenomenal

919
00:38:47,480 --> 00:38:52,400
and it was able to generalize that task of asking questions

920
00:38:52,400 --> 00:38:53,960
in response to anything.

921
00:38:53,960 --> 00:38:56,040
And that was, that really blew me away.

922
00:38:56,040 --> 00:38:57,560
I kind of stalled after that.

923
00:38:57,560 --> 00:38:59,000
There's a few fine tuning projects

924
00:38:59,000 --> 00:39:01,680
that haven't done quite as well.

925
00:39:01,680 --> 00:39:03,680
So I guess to tie back to your earlier question,

926
00:39:03,680 --> 00:39:05,160
like what are the heuristics?

927
00:39:06,160 --> 00:39:10,080
The simpler your fine tuning project is, the better.

928
00:39:10,080 --> 00:39:12,720
And I have found that fine tuning works really well

929
00:39:12,720 --> 00:39:14,520
at generating lists.

930
00:39:14,520 --> 00:39:16,760
So if you wanted to generate a list of questions,

931
00:39:16,760 --> 00:39:17,720
it's great at that.

932
00:39:17,720 --> 00:39:21,520
If you wanted to generate a list of possible answers,

933
00:39:21,520 --> 00:39:25,280
for instance, if you wanna have a fine tuned chat bot,

934
00:39:25,280 --> 00:39:26,760
that it's just gonna say,

935
00:39:26,760 --> 00:39:29,360
here's five possible responses, pick one.

936
00:39:29,360 --> 00:39:31,200
It's really good at that.

937
00:39:31,200 --> 00:39:33,600
I haven't had a chance,

938
00:39:33,600 --> 00:39:34,800
I do have some other ideas

939
00:39:34,800 --> 00:39:35,960
that I haven't had a chance to test.

940
00:39:35,960 --> 00:39:38,600
So unfortunately, I can't speak too much beyond that,

941
00:39:38,600 --> 00:39:40,600
but it's really great at asking questions.

942
00:39:42,960 --> 00:39:43,800
That's awesome.

943
00:39:43,800 --> 00:39:47,400
And I think largely the feedback I'm hearing

944
00:39:47,400 --> 00:39:49,080
about fine tuning, I love it.

945
00:39:49,080 --> 00:39:53,200
It was for me, it was as if I rediscovered GPT-3 again.

946
00:39:53,200 --> 00:39:55,840
Like it was that same level of excitement.

947
00:39:55,840 --> 00:40:00,840
Part of the reason is so much of what GPT-3 was okay at,

948
00:40:01,240 --> 00:40:03,440
or like it was sort of out of the question.

949
00:40:03,440 --> 00:40:05,720
Now it's back in the picture.

950
00:40:05,720 --> 00:40:07,160
Like it's back in the spotlight.

951
00:40:07,160 --> 00:40:09,840
It may actually be able to do it with fine tuning.

952
00:40:09,840 --> 00:40:11,600
The biggest criticism was reliability,

953
00:40:11,600 --> 00:40:13,800
especially from a commercial perspective.

954
00:40:13,800 --> 00:40:18,280
Now we're sort of attacking and sort of peeling away

955
00:40:18,280 --> 00:40:21,960
that criticism, that it does improve our reliability.

956
00:40:23,160 --> 00:40:25,760
And I mean, there's other heuristics as well

957
00:40:25,760 --> 00:40:28,120
in the community forums that you just pick up.

958
00:40:28,120 --> 00:40:31,080
So one heuristic, and I can't remember if you shared this,

959
00:40:31,080 --> 00:40:32,480
but it was something I picked up

960
00:40:32,480 --> 00:40:36,040
as a little golden nugget in the open-eyed community forums

961
00:40:36,040 --> 00:40:37,480
was something about,

962
00:40:37,480 --> 00:40:39,960
you do wanna think about the training dataset

963
00:40:39,960 --> 00:40:42,520
that GPT-3 is itself trained on.

964
00:40:42,520 --> 00:40:45,400
And at some point, there's really no point

965
00:40:45,400 --> 00:40:46,760
in adding more examples,

966
00:40:46,760 --> 00:40:49,680
because it's kind of already seen them, right?

967
00:40:49,680 --> 00:40:52,600
However, and I sort of, in an article,

968
00:40:52,600 --> 00:40:54,560
I have pushed this idea that opening eyes should

969
00:40:54,560 --> 00:40:56,640
chat more about their dataset.

970
00:40:56,640 --> 00:40:57,680
What is the breakdown?

971
00:40:57,680 --> 00:40:58,840
What is it composed of?

972
00:40:58,840 --> 00:41:01,240
I mean, a lot of this is intellectual property,

973
00:41:01,240 --> 00:41:02,400
but I think it could be helpful

974
00:41:02,400 --> 00:41:05,600
for purposes like fine tuning, right?

975
00:41:05,600 --> 00:41:08,400
There's other things too with fine tuning and prompts,

976
00:41:08,440 --> 00:41:11,640
a one heuristic or just a tip

977
00:41:11,640 --> 00:41:15,040
that people have shared online is it tends to always,

978
00:41:15,040 --> 00:41:18,240
it tends to always mimic the most recent examples.

979
00:41:18,240 --> 00:41:20,440
There's something about the order of the examples,

980
00:41:20,440 --> 00:41:22,200
which is really important,

981
00:41:22,200 --> 00:41:25,880
both for prompt engineering and fine tuning as well.

982
00:41:25,880 --> 00:41:27,760
And I guess I wrote a whole article

983
00:41:27,760 --> 00:41:31,200
about how prompt fine tuning could be improved.

984
00:41:31,200 --> 00:41:33,440
One of the pointers that I just had is right now,

985
00:41:33,440 --> 00:41:35,760
you can't keep improving on the same model.

986
00:41:35,800 --> 00:41:38,600
You have to retrain on more models.

987
00:41:38,600 --> 00:41:42,400
And yeah, and then the other thing is recently,

988
00:41:42,400 --> 00:41:44,600
I was in favor of the pricing of fine tuning.

989
00:41:44,600 --> 00:41:45,880
Now I'm kind of against it,

990
00:41:45,880 --> 00:41:49,120
because I'm used to when the program was like free

991
00:41:49,120 --> 00:41:50,960
and you could fine tune as much as you want.

992
00:41:50,960 --> 00:41:52,840
And now it's like, oh man, I got to pay.

993
00:41:52,840 --> 00:41:53,680
Yeah.

994
00:41:53,680 --> 00:41:56,240
Oh, the costs, especially for David,

995
00:41:56,240 --> 00:41:57,640
you are catching up a little bit.

996
00:41:57,640 --> 00:41:58,480
Yeah.

997
00:41:59,720 --> 00:42:01,480
Anyway, so I wanted to shift gears.

998
00:42:01,480 --> 00:42:02,560
Sure.

999
00:42:02,560 --> 00:42:05,040
GitHub co-pilot, really exciting.

1000
00:42:05,040 --> 00:42:06,720
Have you had access to co-pilot?

1001
00:42:06,720 --> 00:42:07,680
Yes.

1002
00:42:07,680 --> 00:42:08,520
Yeah.

1003
00:42:08,520 --> 00:42:10,520
Well, not co-pilot, but the Codex.

1004
00:42:10,520 --> 00:42:12,520
They did give me access to Codex.

1005
00:42:12,520 --> 00:42:17,520
So the reason I'm asking is, I love GitHub co-pilot.

1006
00:42:17,880 --> 00:42:19,480
I have a separate podcast episode

1007
00:42:19,480 --> 00:42:21,480
on my ideas around Codex.

1008
00:42:21,480 --> 00:42:23,380
Unfortunately, I'm not as bullish

1009
00:42:23,380 --> 00:42:24,920
as much as I love the research

1010
00:42:24,920 --> 00:42:26,800
as I think it's incredible technology.

1011
00:42:26,800 --> 00:42:28,720
I've congratulated the team

1012
00:42:28,720 --> 00:42:30,840
and I tried so hard to be nice,

1013
00:42:30,840 --> 00:42:32,920
even though I'm more on the critical end.

1014
00:42:32,920 --> 00:42:36,320
I wanted to ask you, how are you finding OpenAI Codex?

1015
00:42:36,320 --> 00:42:39,040
How can you see it impacting the world?

1016
00:42:40,200 --> 00:42:41,920
What are some use cases maybe

1017
00:42:41,920 --> 00:42:44,760
that you found with OpenAI Codex?

1018
00:42:44,760 --> 00:42:45,840
What are your thoughts on it?

1019
00:42:45,840 --> 00:42:47,080
Where do you think it's going?

1020
00:42:47,080 --> 00:42:47,920
Yeah.

1021
00:42:47,920 --> 00:42:51,160
So, I mean, certainly this is like a world first, right?

1022
00:42:51,160 --> 00:42:54,440
We've never had something that could write code on its own.

1023
00:42:54,440 --> 00:42:56,800
And especially it's text to code.

1024
00:42:56,800 --> 00:42:59,080
I remember when they first gave me access,

1025
00:42:59,080 --> 00:43:01,840
because like you mentioned, I'm an active contributor,

1026
00:43:01,840 --> 00:43:03,720
so they wanted my feedback.

1027
00:43:03,720 --> 00:43:06,280
And so the first thing I did was I went in

1028
00:43:06,280 --> 00:43:09,120
and I said, write me a Python function

1029
00:43:09,120 --> 00:43:12,040
that will download random Reddit posts.

1030
00:43:12,040 --> 00:43:12,880
And it did.

1031
00:43:12,880 --> 00:43:14,240
It wrote the whole function.

1032
00:43:14,240 --> 00:43:15,760
And it did all right.

1033
00:43:15,760 --> 00:43:16,600
And I was like, cool.

1034
00:43:16,600 --> 00:43:20,600
I learned how to access the Reddit API via Codex.

1035
00:43:20,600 --> 00:43:22,120
It's got that built in.

1036
00:43:22,120 --> 00:43:24,840
And I tried to reverse engineer,

1037
00:43:24,840 --> 00:43:27,440
figure out where it got that code sample from.

1038
00:43:27,440 --> 00:43:31,820
So, because one of the ethical concerns is,

1039
00:43:31,820 --> 00:43:34,300
all right, you create a fine-tuning data set

1040
00:43:34,300 --> 00:43:36,540
from public GitHub repositories

1041
00:43:36,540 --> 00:43:38,780
and you use that to fine-tune Codex.

1042
00:43:38,780 --> 00:43:41,700
Okay, is that legal?

1043
00:43:41,700 --> 00:43:43,700
Is it ethical?

1044
00:43:43,700 --> 00:43:46,740
I post all my code publicly under the MIT license,

1045
00:43:46,740 --> 00:43:48,380
so I want it to be used.

1046
00:43:48,380 --> 00:43:50,180
But I don't know if they checked that.

1047
00:43:50,180 --> 00:43:52,580
And I'm not making an accusation one way or another,

1048
00:43:52,580 --> 00:43:54,500
just pointing out that that's a concern.

1049
00:43:54,500 --> 00:43:57,580
And so I did actually find like one of the lines of code

1050
00:43:57,580 --> 00:43:59,900
from the function it spit out.

1051
00:43:59,900 --> 00:44:03,060
I went and found the repo that it had copied from.

1052
00:44:03,060 --> 00:44:05,660
Now granted, some of these things are deterministic.

1053
00:44:05,660 --> 00:44:10,660
So you're gonna get some convergence, right?

1054
00:44:10,700 --> 00:44:12,340
Where multiple people might come up

1055
00:44:12,340 --> 00:44:13,540
with the same exact line of code,

1056
00:44:13,540 --> 00:44:15,580
especially something like Python.

1057
00:44:15,580 --> 00:44:17,380
Because Python has the PEP 8,

1058
00:44:17,380 --> 00:44:19,700
the Python enhancement protocol 8.

1059
00:44:19,700 --> 00:44:23,180
So like, there is a Pythonic way to write that function.

1060
00:44:23,180 --> 00:44:26,060
And so other people might converge on that.

1061
00:44:26,980 --> 00:44:29,860
Anyways, but to answer your question about like,

1062
00:44:29,860 --> 00:44:31,660
what's the future of it?

1063
00:44:31,660 --> 00:44:35,700
I think it'll help for novice programmers.

1064
00:44:35,700 --> 00:44:37,140
Certainly it would help someone like me,

1065
00:44:37,140 --> 00:44:39,180
like if I needed to go write a function in C

1066
00:44:39,180 --> 00:44:40,660
or Perl or something.

1067
00:44:40,660 --> 00:44:42,100
Like let's say I got an Arduino

1068
00:44:42,100 --> 00:44:45,060
and like I haven't written C in 15 years.

1069
00:44:45,060 --> 00:44:46,340
So I was like, hey, you know,

1070
00:44:46,340 --> 00:44:48,580
write me a function that can do this in Arduino.

1071
00:44:48,580 --> 00:44:49,420
That'd be great.

1072
00:44:49,420 --> 00:44:51,780
And then I can go clean it up manually.

1073
00:44:51,780 --> 00:44:54,620
That sort of thing I think it could do okay with,

1074
00:44:54,660 --> 00:44:57,260
is it gonna replace enterprise developers?

1075
00:44:58,260 --> 00:44:59,980
Probably not yet.

1076
00:44:59,980 --> 00:45:03,300
However, now this is where my professional experience

1077
00:45:03,300 --> 00:45:04,140
comes in.

1078
00:45:04,140 --> 00:45:06,060
So in the DevOps world,

1079
00:45:06,060 --> 00:45:09,220
which is a portmanteau of development and operations,

1080
00:45:09,220 --> 00:45:11,860
there's all kinds of automation tools, right?

1081
00:45:11,860 --> 00:45:13,940
You can automate your test suite.

1082
00:45:13,940 --> 00:45:15,980
You can automate code integration.

1083
00:45:15,980 --> 00:45:18,180
There's all sorts of stuff like that.

1084
00:45:18,180 --> 00:45:21,100
So what I suspect might happen

1085
00:45:21,100 --> 00:45:23,660
is probably one of the most lucrative

1086
00:45:23,660 --> 00:45:27,180
use cases for Codex would be to generate

1087
00:45:27,180 --> 00:45:29,820
or to create a DevOps pipeline tool

1088
00:45:29,820 --> 00:45:33,260
that will automatically look at those bugs and fix them.

1089
00:45:33,260 --> 00:45:35,340
Right, because if you've got a sophisticated enough

1090
00:45:35,340 --> 00:45:37,260
DevOps pipeline, it'll say, hey,

1091
00:45:37,260 --> 00:45:40,300
this line of this file broke, fix it.

1092
00:45:40,300 --> 00:45:44,220
And so Codex, having seen all of GitHub

1093
00:45:44,220 --> 00:45:47,380
and all the issues, it might know automatically

1094
00:45:47,380 --> 00:45:49,560
how to fix that line of code.

1095
00:45:49,560 --> 00:45:51,900
And so that gives you,

1096
00:45:51,900 --> 00:45:53,540
if you've got that feedback loop,

1097
00:45:53,540 --> 00:45:58,420
where Codex, humans write code, Codex writes code,

1098
00:45:58,420 --> 00:46:01,660
co-pilot writes code, everyone's contributing code.

1099
00:46:01,660 --> 00:46:04,420
And then you've got Codex that can churn on it

1100
00:46:04,420 --> 00:46:06,580
and say, let's refactor this.

1101
00:46:06,580 --> 00:46:08,920
Because I bet it's probably better at refactoring

1102
00:46:08,920 --> 00:46:10,520
than writing new code.

1103
00:46:10,520 --> 00:46:14,740
You might have noticed that Instruct and GPT-3 Vanilla

1104
00:46:14,740 --> 00:46:17,260
is really good if you give it a block of text

1105
00:46:17,260 --> 00:46:19,420
and you say, rewrite this, but a little bit better.

1106
00:46:19,420 --> 00:46:20,940
It's really good at that.

1107
00:46:20,940 --> 00:46:24,940
So I suspect that we might end up seeing Codex

1108
00:46:24,940 --> 00:46:26,740
integrated into the DevOps pipeline,

1109
00:46:26,740 --> 00:46:28,300
where it says, let's refactor this code,

1110
00:46:28,300 --> 00:46:29,620
let's make it a little bit better,

1111
00:46:29,620 --> 00:46:32,540
or let's shoot that bug, let's fix this bug.

1112
00:46:32,540 --> 00:46:35,060
And that leads to some other interesting possibilities.

1113
00:46:35,060 --> 00:46:40,060
What if you integrate Codex into a chat room of developers?

1114
00:46:40,460 --> 00:46:43,420
And so that, you can do this in Slack right now,

1115
00:46:43,420 --> 00:46:47,500
where you use a special command and you say,

1116
00:46:47,500 --> 00:46:49,340
create an issue, go fix this problem.

1117
00:46:49,580 --> 00:46:53,500
There's no reason that GPT-3 can't do that, right?

1118
00:46:53,500 --> 00:46:57,300
That you put a GPT-3 bot in your Discord or Slack

1119
00:46:57,300 --> 00:46:59,260
and it starts coming up with features

1120
00:46:59,260 --> 00:47:02,260
or it watches the chat and generates features automatically

1121
00:47:02,260 --> 00:47:05,260
and then codes them and tests them, right?

1122
00:47:05,260 --> 00:47:06,800
That's kind of where I see it going,

1123
00:47:06,800 --> 00:47:09,780
where it's not gonna necessarily replace developers,

1124
00:47:09,780 --> 00:47:12,540
at least not anytime soon, it might eventually.

1125
00:47:12,540 --> 00:47:16,340
But where what I see happening is that it's gonna be

1126
00:47:16,340 --> 00:47:18,260
tightly integrated into those automation

1127
00:47:18,300 --> 00:47:19,760
because it's fast, right?

1128
00:47:19,760 --> 00:47:22,700
It can generate code faster than any human can.

1129
00:47:22,700 --> 00:47:25,020
And then so even if the code is messy,

1130
00:47:25,020 --> 00:47:27,500
if it generates a lot of bugs, it can fix it, right?

1131
00:47:27,500 --> 00:47:29,340
It's an iterative process.

1132
00:47:29,340 --> 00:47:31,060
I don't know if you are familiar with Agile,

1133
00:47:31,060 --> 00:47:33,340
but that's how we develop software.

1134
00:47:33,340 --> 00:47:35,380
It's you tight feedback loops.

1135
00:47:36,460 --> 00:47:39,100
And that leads to one other possibility.

1136
00:47:39,100 --> 00:47:41,740
So that's if you're using, what I just outlined is,

1137
00:47:41,740 --> 00:47:45,740
let's imagine that Codex is integrated into Facebook

1138
00:47:45,740 --> 00:47:47,820
or Reddit or whatever and they're just,

1139
00:47:47,820 --> 00:47:50,180
they're integrating new features as they go.

1140
00:47:50,180 --> 00:47:55,180
What if you're using Codex in a chat room

1141
00:47:55,780 --> 00:47:57,340
and it's feeding back into itself,

1142
00:47:57,340 --> 00:48:00,060
it's making itself more sophisticated?

1143
00:48:00,060 --> 00:48:03,460
So this was something I proposed on the OpenAI forum

1144
00:48:03,460 --> 00:48:07,300
where I was like, what if you had a chatbot

1145
00:48:07,300 --> 00:48:09,420
that was aware of its own code

1146
00:48:09,420 --> 00:48:13,140
and could edit its own code via Codex using natural language,

1147
00:48:13,140 --> 00:48:15,860
using a combination of natural language and Codex

1148
00:48:15,860 --> 00:48:17,220
and it could improve itself.

1149
00:48:17,220 --> 00:48:19,340
And while you're talking to it, it's like,

1150
00:48:19,340 --> 00:48:21,380
man, I wish my chatbot could do this.

1151
00:48:21,380 --> 00:48:23,420
And it says, cool, new feature.

1152
00:48:23,420 --> 00:48:25,420
And it just sends it out to its automated pipeline.

1153
00:48:25,420 --> 00:48:29,220
So I see these feedback loops as kind of the way forward.

1154
00:48:29,220 --> 00:48:31,140
And will that result in AGI?

1155
00:48:31,140 --> 00:48:33,220
Who knows, it could end up with spaghetti code

1156
00:48:33,220 --> 00:48:36,740
because you keep tacking on new code and new functions,

1157
00:48:36,740 --> 00:48:38,140
eventually it's gonna break.

1158
00:48:38,140 --> 00:48:41,180
So, but they're just pie in the sky thought,

1159
00:48:41,820 --> 00:48:44,220
if someone's out there and they want a business idea,

1160
00:48:44,220 --> 00:48:46,500
integrate Codex into DevOps

1161
00:48:46,500 --> 00:48:47,940
and you're gonna be a billionaire.

1162
00:48:49,500 --> 00:48:51,460
There you have it, let's just clip it.

1163
00:48:51,460 --> 00:48:54,660
We're good, we're good David, we can wrap up, see you later.

1164
00:48:56,340 --> 00:49:00,380
No, I agree with you and definitely these are some great

1165
00:49:00,380 --> 00:49:03,340
use cases you're sharing for people thinking about

1166
00:49:03,340 --> 00:49:06,980
what could I build, what's a cool project, certainly.

1167
00:49:06,980 --> 00:49:09,420
With Codex and GPT-3, you can build things

1168
00:49:09,420 --> 00:49:10,580
relatively quickly, right?

1169
00:49:10,620 --> 00:49:13,540
That's one of the advantages is the prototyping speed,

1170
00:49:13,540 --> 00:49:15,620
especially to figure out the most complicated bit,

1171
00:49:15,620 --> 00:49:16,860
which is the AI.

1172
00:49:16,860 --> 00:49:18,180
Yep.

1173
00:49:18,180 --> 00:49:23,180
Yeah, I find Codex does have limitations though

1174
00:49:23,740 --> 00:49:26,740
and character limits and stuff like that,

1175
00:49:26,740 --> 00:49:30,780
which is why I'm a heavy user of GitHub Co-Pilot.

1176
00:49:30,780 --> 00:49:34,820
I think it's a silent killer and of course it runs

1177
00:49:34,820 --> 00:49:37,020
on Codex or a special version of Codex,

1178
00:49:38,020 --> 00:49:42,020
but I can see GitHub Co-Pilot perhaps getting more adoption

1179
00:49:42,020 --> 00:49:43,700
than even something like GPT-3.

1180
00:49:43,700 --> 00:49:47,420
I'm saying use daily at least eight hours a day.

1181
00:49:47,420 --> 00:49:50,540
One of my other predictions was it may surpass GPT-3

1182
00:49:50,540 --> 00:49:55,540
this year and so these are some great use cases

1183
00:49:55,820 --> 00:49:59,620
you've shared for sure, but what are your thoughts

1184
00:49:59,620 --> 00:50:00,460
in usage?

1185
00:50:00,460 --> 00:50:04,580
Do you find yourself using GPT-3, DaVinci Classic more?

1186
00:50:04,580 --> 00:50:06,620
That's what I'm calling the older version.

1187
00:50:06,780 --> 00:50:10,460
Do you find yourself using Instruct GPT more?

1188
00:50:10,460 --> 00:50:12,540
Do you find yourself playing around with multimodal models?

1189
00:50:12,540 --> 00:50:15,180
Like what's the proportion of GPT-3 to Codex

1190
00:50:15,180 --> 00:50:16,500
in terms of your usage?

1191
00:50:17,580 --> 00:50:20,020
Let's see, I'm almost exclusively using either

1192
00:50:20,020 --> 00:50:22,300
Instructor fine-tuned models right now.

1193
00:50:23,140 --> 00:50:26,900
Actually, after I prototyped my cognitive architecture,

1194
00:50:26,900 --> 00:50:29,580
I haven't done a heck of a lot of coding lately.

1195
00:50:29,580 --> 00:50:31,300
I've actually been writing a lot.

1196
00:50:31,300 --> 00:50:35,220
So I've got my natural language cognitive architecture book

1197
00:50:35,300 --> 00:50:37,820
and I'm working on two more nonfiction books

1198
00:50:37,820 --> 00:50:42,100
and I tried creating a system to help me co-write those

1199
00:50:42,100 --> 00:50:45,900
but when you're, so talking about limitations of GPT-3,

1200
00:50:45,900 --> 00:50:48,060
if you're proposing something new that didn't exist

1201
00:50:48,060 --> 00:50:51,500
in the dataset in 2019 or 2018, whenever it was trained,

1202
00:50:51,500 --> 00:50:52,980
it really struggles.

1203
00:50:53,820 --> 00:50:58,300
GPT-3, if you give it like two or three paragraphs

1204
00:50:58,300 --> 00:51:01,820
explaining a new concept, it can usually kind of get it

1205
00:51:01,820 --> 00:51:04,580
but it's kind of slow on the uptake otherwise

1206
00:51:04,580 --> 00:51:07,740
and so if you're writing about new research or something,

1207
00:51:07,740 --> 00:51:09,580
it's not gonna get it that well.

1208
00:51:09,580 --> 00:51:12,380
So I've actually kind of defaulted back to my own head

1209
00:51:12,380 --> 00:51:15,540
for a lot of my projects lately

1210
00:51:15,540 --> 00:51:17,380
but I could imagine like if I wanted to go write

1211
00:51:17,380 --> 00:51:19,540
a new Discord bot, I might use Codex and say,

1212
00:51:19,540 --> 00:51:21,780
hey, write me a Discord bot that will do this

1213
00:51:21,780 --> 00:51:23,180
and just see what it spits out and just say,

1214
00:51:23,180 --> 00:51:26,260
okay, cool, pick and choose the pieces that I like.

1215
00:51:26,260 --> 00:51:28,500
Part of the problem though is it's really difficult

1216
00:51:28,500 --> 00:51:33,500
to fully articulate what you want a program to do up front.

1217
00:51:33,620 --> 00:51:36,460
Cause you know, like you said, there's character limits,

1218
00:51:36,460 --> 00:51:39,140
there's only so much that you can put in

1219
00:51:39,140 --> 00:51:40,900
but also if you don't have it fully articulated

1220
00:51:40,900 --> 00:51:42,820
in your own head, of course, the machine isn't gonna

1221
00:51:42,820 --> 00:51:44,460
be able to figure it out for you.

1222
00:51:44,460 --> 00:51:45,420
So, yeah.

1223
00:51:48,300 --> 00:51:52,820
Yeah, and it's just, I just haven't seen

1224
00:51:52,820 --> 00:51:55,300
that much activity specifically around Codex.

1225
00:51:55,300 --> 00:51:57,500
I haven't seen that many use cases.

1226
00:51:59,260 --> 00:52:03,060
I looked up the Google Trends data at its most hype,

1227
00:52:03,060 --> 00:52:07,260
Codex is still less than GPT-3 is kind of lowest, right?

1228
00:52:07,260 --> 00:52:09,140
And the audience is really specific,

1229
00:52:09,140 --> 00:52:12,460
like it's programmers who wanna build use cases

1230
00:52:12,460 --> 00:52:14,140
for something like Codex.

1231
00:52:14,140 --> 00:52:16,660
Whereas GPT-3 has poets, writers,

1232
00:52:16,660 --> 00:52:21,140
it has artists, coders, GPT-3 can write code too, right?

1233
00:52:21,140 --> 00:52:24,580
So it's a little bit complicated like,

1234
00:52:24,580 --> 00:52:27,020
who is the target audience for something like Codex?

1235
00:52:27,020 --> 00:52:29,380
What use cases did OpenAI imagine

1236
00:52:29,380 --> 00:52:31,500
for a product like that?

1237
00:52:31,500 --> 00:52:33,740
The next version I've heard in the rumor mill

1238
00:52:33,740 --> 00:52:34,660
is gonna be crazy.

1239
00:52:34,660 --> 00:52:37,180
Like it may write 50% of your code

1240
00:52:37,180 --> 00:52:39,020
as opposed to right now for me,

1241
00:52:39,020 --> 00:52:41,660
get a co-pilot is writing two to 8%.

1242
00:52:41,660 --> 00:52:45,140
However, your Discord bot I think is a genius idea

1243
00:52:45,140 --> 00:52:47,900
where there's, it's genius in the sense

1244
00:52:47,900 --> 00:52:49,900
that there's no pressure on it, right?

1245
00:52:49,900 --> 00:52:52,380
It may chime in, it may not, whatever it's shared

1246
00:52:52,380 --> 00:52:53,580
might be interesting.

1247
00:52:54,580 --> 00:52:57,100
There's lots of, you could take it a lot further,

1248
00:52:57,100 --> 00:52:59,020
you could buy it with GPT-3, have features,

1249
00:52:59,020 --> 00:53:01,260
you could fine tune it on your company and its mission

1250
00:53:01,260 --> 00:53:03,940
and its existing code, so many ways around it.

1251
00:53:03,940 --> 00:53:05,580
So that's a great piece.

1252
00:53:05,580 --> 00:53:08,340
And so you talked about the using,

1253
00:53:08,340 --> 00:53:12,020
experimenting with writing in relation to your current stack,

1254
00:53:12,020 --> 00:53:14,060
which is mainly instructed fine tune.

1255
00:53:14,060 --> 00:53:15,100
Yep.

1256
00:53:15,100 --> 00:53:17,340
So tell us about your book.

1257
00:53:17,340 --> 00:53:18,780
I've had a chance to review it,

1258
00:53:18,780 --> 00:53:21,660
Natural Language Cognitive Architecture.

1259
00:53:21,660 --> 00:53:23,020
Tell the audience about it.

1260
00:53:23,020 --> 00:53:24,540
I mean, I would describe it,

1261
00:53:24,540 --> 00:53:29,420
it's an interesting systems theory of AGI

1262
00:53:29,460 --> 00:53:33,420
combined with modern day prompt writing.

1263
00:53:34,260 --> 00:53:37,500
And so I've never seen somebody actually take a stab

1264
00:53:37,500 --> 00:53:41,220
at this kind of super big systems problem

1265
00:53:41,220 --> 00:53:43,380
and relate it to something that pretty much

1266
00:53:43,380 --> 00:53:46,900
every GPT-3 developer in the world would find interesting.

1267
00:53:48,380 --> 00:53:50,900
And I mean, I can tell you're drawing

1268
00:53:50,900 --> 00:53:54,100
from a very interdisciplinary background as well.

1269
00:53:54,100 --> 00:53:57,420
So you mentioned GPT-3 may have been the genesis of it,

1270
00:53:57,980 --> 00:54:00,820
you started connecting dots and deciding,

1271
00:54:00,820 --> 00:54:03,740
I wanna write the book, but how did it come together

1272
00:54:03,740 --> 00:54:05,860
and please tell us more about it.

1273
00:54:05,860 --> 00:54:08,540
Yeah, so Natural Language Cognitive Architecture

1274
00:54:08,540 --> 00:54:12,940
is that's my proposed way of creating basically

1275
00:54:12,940 --> 00:54:16,420
a language-based AGI prototype.

1276
00:54:16,420 --> 00:54:19,260
And I know that that's like,

1277
00:54:19,260 --> 00:54:20,580
when I tell people that, that's like,

1278
00:54:20,580 --> 00:54:22,700
okay, that's pure hyperbole.

1279
00:54:22,700 --> 00:54:25,300
And like, yeah, that's a fair response.

1280
00:54:25,340 --> 00:54:28,980
But to frame it, imagine that you've got a person

1281
00:54:28,980 --> 00:54:30,620
who's paralyzed and blind,

1282
00:54:30,620 --> 00:54:33,340
all they can do is speak and listen.

1283
00:54:34,260 --> 00:54:36,660
Is that person still intelligent?

1284
00:54:36,660 --> 00:54:40,100
I say they are, even if you're bedridden,

1285
00:54:40,100 --> 00:54:41,900
you can't move, you can't see,

1286
00:54:41,900 --> 00:54:43,100
you can't interact with the world,

1287
00:54:43,100 --> 00:54:44,500
all you can do is listen and speak,

1288
00:54:44,500 --> 00:54:45,860
you're still intelligent.

1289
00:54:45,860 --> 00:54:48,620
And so in that respect, I would say that like,

1290
00:54:48,620 --> 00:54:50,900
because one of the questions that people ask is,

1291
00:54:50,900 --> 00:54:52,580
is GPT-3 AGI?

1292
00:54:52,580 --> 00:54:54,780
No, but it's an important component.

1293
00:54:54,820 --> 00:54:56,220
It's a good start.

1294
00:54:56,220 --> 00:55:01,220
And so if you say, okay, let's limit the discussion

1295
00:55:02,460 --> 00:55:04,660
and not say that this is a full intelligence,

1296
00:55:04,660 --> 00:55:05,660
they can do everything

1297
00:55:05,660 --> 00:55:08,820
that any intelligent being ever could, right?

1298
00:55:08,820 --> 00:55:10,700
But does it cross that threshold of,

1299
00:55:10,700 --> 00:55:13,580
could it be as intelligent as a person, right?

1300
00:55:13,580 --> 00:55:15,420
And I think it could be.

1301
00:55:15,420 --> 00:55:16,940
So anyways, as to what it is,

1302
00:55:16,940 --> 00:55:21,940
it's based on older ideas of cognitive architectures

1303
00:55:22,820 --> 00:55:24,460
which really kind of came about

1304
00:55:24,460 --> 00:55:26,140
as one of the primary theories

1305
00:55:26,140 --> 00:55:29,820
of human level artificial intelligence in the 70s.

1306
00:55:29,820 --> 00:55:33,380
So there's SOAR, which is S-O-A-R and ACT-R,

1307
00:55:33,380 --> 00:55:38,380
which are the two kind of forerunner cognitive architectures.

1308
00:55:38,500 --> 00:55:40,100
And those cognitive architectures

1309
00:55:40,100 --> 00:55:41,260
are used all over the place.

1310
00:55:41,260 --> 00:55:42,820
They're used in the Mars rovers,

1311
00:55:42,820 --> 00:55:43,980
they're used in satellites,

1312
00:55:43,980 --> 00:55:45,780
they're used in rockets,

1313
00:55:45,780 --> 00:55:50,780
they're used in undersea ROVs, remote operated vehicles.

1314
00:55:50,780 --> 00:55:53,580
So cognitive architectures already give robots

1315
00:55:53,580 --> 00:55:54,700
a lot of autonomy.

1316
00:55:55,580 --> 00:55:59,780
So there's that kind of, okay, they exist, they work.

1317
00:55:59,780 --> 00:56:01,340
You know, it's not Skynet though,

1318
00:56:01,340 --> 00:56:03,780
it's not gonna take over the world.

1319
00:56:03,780 --> 00:56:07,260
So when I got access to GPT-3, I said, what if,

1320
00:56:07,260 --> 00:56:09,780
instead of hard coding a lot of these modules,

1321
00:56:09,780 --> 00:56:12,380
these different components of a cognitive architecture,

1322
00:56:12,380 --> 00:56:15,660
what if we give them the flexibility of GPT-3?

1323
00:56:15,660 --> 00:56:18,500
And that's really kind of, that was my central idea.

1324
00:56:18,500 --> 00:56:19,860
I said, okay, all these ideas

1325
00:56:19,860 --> 00:56:21,820
that have been kicking around for the last decade,

1326
00:56:21,820 --> 00:56:23,580
what if I put them all together

1327
00:56:23,580 --> 00:56:26,540
and design an architecture that is based on,

1328
00:56:26,540 --> 00:56:29,340
you know, roughly based on the human brain,

1329
00:56:29,340 --> 00:56:32,380
the way, you know, everything that I've learned about it.

1330
00:56:32,380 --> 00:56:34,620
I've got a book to recommend.

1331
00:56:34,620 --> 00:56:37,580
So there's an author called V.S. Ramachandran,

1332
00:56:37,580 --> 00:56:39,860
who is a neuroscientist

1333
00:56:39,860 --> 00:56:42,540
and he's been writing books for years now.

1334
00:56:42,540 --> 00:56:44,740
He wrote a book called, Phantom's in the Brain,

1335
00:56:44,740 --> 00:56:46,900
which actually looks at how the human brain works

1336
00:56:46,900 --> 00:56:48,300
when it breaks.

1337
00:56:48,300 --> 00:56:51,020
And so in that book, which, you know,

1338
00:56:51,020 --> 00:56:54,580
I saw the television series almost 20 years ago

1339
00:56:56,020 --> 00:56:57,180
that came out.

1340
00:56:57,180 --> 00:56:59,220
And so I learned a lot about like, okay,

1341
00:56:59,220 --> 00:57:01,100
how does the brain communicate with itself?

1342
00:57:01,100 --> 00:57:03,220
What is going on inside the brain

1343
00:57:03,220 --> 00:57:06,180
that creates intelligent behavior and intelligent thoughts?

1344
00:57:06,180 --> 00:57:09,300
And so I modeled natural language cognitive architecture

1345
00:57:09,300 --> 00:57:10,700
on, you know, what I learned there.

1346
00:57:10,700 --> 00:57:12,820
I picked up a whole bunch of other books.

1347
00:57:12,820 --> 00:57:15,860
There's another one called On Task by David Bader.

1348
00:57:15,900 --> 00:57:18,860
That was a great book that helped me kind of understand

1349
00:57:18,860 --> 00:57:21,860
cognitive control, which is how do you focus on something?

1350
00:57:21,860 --> 00:57:23,740
How do you decide what to do?

1351
00:57:23,740 --> 00:57:25,660
How do you plan a task?

1352
00:57:25,660 --> 00:57:28,620
So I read all these books, did a lot of experiments,

1353
00:57:28,620 --> 00:57:32,180
and I realized, so the basic model of robotics

1354
00:57:32,180 --> 00:57:35,900
is there's input output, sorry, input processing output.

1355
00:57:35,900 --> 00:57:38,660
Those are the three steps of all robotics class.

1356
00:57:38,660 --> 00:57:40,220
You go to robotics 101.

1357
00:57:40,220 --> 00:57:41,060
That's what they'll tell you.

1358
00:57:41,060 --> 00:57:43,180
It's a loop, input processing output.

1359
00:57:43,180 --> 00:57:45,100
And then of course, it's within an environment.

1360
00:57:45,100 --> 00:57:46,980
So the output affects the environment,

1361
00:57:46,980 --> 00:57:49,540
which affects the next input cycle.

1362
00:57:49,540 --> 00:57:52,300
And, you know, your high speed robots

1363
00:57:52,300 --> 00:57:54,300
just have a short cycle.

1364
00:57:54,300 --> 00:57:59,300
Your robots like the Mars rover has a much slower cycle

1365
00:57:59,300 --> 00:58:01,220
where it will, you know, it'll take input.

1366
00:58:01,220 --> 00:58:03,340
It'll plan for 10 or 15 minutes

1367
00:58:03,340 --> 00:58:04,860
and it'll make a move, right?

1368
00:58:04,860 --> 00:58:08,060
It'll drive five feet and then it'll stop and assess.

1369
00:58:08,060 --> 00:58:10,740
It'll take in more input, come up with another plan,

1370
00:58:10,740 --> 00:58:11,660
do it again.

1371
00:58:11,660 --> 00:58:14,780
So that's how something like the Mars rover is autonomous.

1372
00:58:15,460 --> 00:58:17,860
So I said, okay, well, what if,

1373
00:58:17,860 --> 00:58:20,660
what if that input output cycle is all text

1374
00:58:20,660 --> 00:58:22,780
because GPT-3 is really fast?

1375
00:58:23,700 --> 00:58:27,060
And then, so that's what I ended up calling the outer loop,

1376
00:58:27,060 --> 00:58:29,740
is that input processing output loop.

1377
00:58:29,740 --> 00:58:32,700
But humans don't think like that.

1378
00:58:32,700 --> 00:58:35,540
You know, we have an internal monologue that's going on.

1379
00:58:35,540 --> 00:58:40,540
So I kind of, I took a long time to figure that one out.

1380
00:58:41,500 --> 00:58:44,380
And so there's this outer loop of input processing output.

1381
00:58:44,380 --> 00:58:46,580
And then I came up with this idea of an inner loop

1382
00:58:46,580 --> 00:58:48,020
because what is, you know,

1383
00:58:48,020 --> 00:58:50,300
if you're just sitting there thinking, right?

1384
00:58:50,300 --> 00:58:53,380
You're, you know, in your comfiest chair or your in bed,

1385
00:58:53,380 --> 00:58:54,260
your brain won't stop.

1386
00:58:54,260 --> 00:58:55,780
You're not outputting anything

1387
00:58:55,780 --> 00:58:57,540
and you're not taking in any new input,

1388
00:58:57,540 --> 00:58:59,220
but you're still thinking, right?

1389
00:58:59,220 --> 00:59:02,100
Humans can still do work even if you're not doing anything.

1390
00:59:02,100 --> 00:59:05,540
And that cognitive work is like rumination.

1391
00:59:05,540 --> 00:59:08,540
So I figured out a way to model that internal rumination.

1392
00:59:08,540 --> 00:59:10,020
I call that the inner loop.

1393
00:59:10,020 --> 00:59:13,540
And so there's, it works pretty similarly

1394
00:59:13,580 --> 00:59:18,260
where you go, the inner loop kind of draws up memories.

1395
00:59:18,260 --> 00:59:20,740
It says, okay, what's a memory that I could think on

1396
00:59:20,740 --> 00:59:22,020
and what that I could iterate on?

1397
00:59:22,020 --> 00:59:24,620
What's a problem that I remember

1398
00:59:24,620 --> 00:59:26,380
that I could continue working on?

1399
00:59:26,380 --> 00:59:28,900
And so there's this, if you were to diagram it out,

1400
00:59:28,900 --> 00:59:30,620
it almost looks like a figure eight, right?

1401
00:59:30,620 --> 00:59:32,460
Where you've got an inner loop and an outer loop

1402
00:59:32,460 --> 00:59:34,980
and they intersect and they keep intersecting

1403
00:59:34,980 --> 00:59:36,620
every cycle they intersect.

1404
00:59:36,620 --> 00:59:38,580
And so then they can affect each other

1405
00:59:38,580 --> 00:59:40,340
and generate an output.

1406
00:59:40,340 --> 00:59:42,420
I built a prototype of this on Discord.

1407
00:59:43,620 --> 00:59:45,860
And of course, Discord is an ideal place

1408
00:59:45,860 --> 00:59:46,860
because it's all text-based.

1409
00:59:46,860 --> 00:59:49,340
So the input is text, the output is text,

1410
00:59:49,340 --> 00:59:51,060
which is GPT-3 native.

1411
00:59:51,060 --> 00:59:53,260
You don't have to translate it into robotic actions

1412
00:59:53,260 --> 00:59:55,740
or video or anything like that.

1413
00:59:55,740 --> 00:59:58,100
And I realized I was onto something

1414
00:59:58,100 --> 01:00:00,660
when I started having philosophical conversations

1415
01:00:00,660 --> 01:00:02,740
with my chatbot,

1416
01:00:02,740 --> 01:00:06,020
with my natural language, cognitive architecture chatbot.

1417
01:00:06,020 --> 01:00:08,580
And I was having a debate

1418
01:00:08,580 --> 01:00:13,580
with the bot that I built about the ethics of AGI.

1419
01:00:13,780 --> 01:00:14,620
And it was learning

1420
01:00:14,620 --> 01:00:16,540
and it was able to retrieve memories

1421
01:00:16,540 --> 01:00:18,500
of what I had said before.

1422
01:00:18,500 --> 01:00:22,060
And I had a few friends on that test server as well.

1423
01:00:22,060 --> 01:00:23,820
And of course, you know, you invite someone,

1424
01:00:23,820 --> 01:00:25,860
you say, hey, I've got a prototype AGI.

1425
01:00:25,860 --> 01:00:26,940
What's the first thing they try and do

1426
01:00:26,940 --> 01:00:29,020
is they try and break it and they did.

1427
01:00:29,020 --> 01:00:30,820
So it's still pretty fragile.

1428
01:00:30,820 --> 01:00:32,580
But yeah, so that's the high level

1429
01:00:32,580 --> 01:00:35,500
of a natural language cognitive architecture.

1430
01:00:35,500 --> 01:00:36,780
And it's already outdated, right?

1431
01:00:36,780 --> 01:00:37,900
Because we've got fine-tuning,

1432
01:00:37,900 --> 01:00:39,500
we've got the instruct series.

1433
01:00:39,500 --> 01:00:41,740
I did all this research and wrote the book

1434
01:00:41,740 --> 01:00:43,740
actually about a year ago now,

1435
01:00:43,740 --> 01:00:44,980
just before all this came out.

1436
01:00:44,980 --> 01:00:46,100
So it's already outdated.

1437
01:00:46,100 --> 01:00:48,820
That's why my research has moved on.

1438
01:00:48,820 --> 01:00:50,780
But yeah, so that's it at a high level.

1439
01:00:52,380 --> 01:00:53,660
Yeah, I mean, that's awesome.

1440
01:00:53,660 --> 01:00:56,820
And by the way, like David, you did do a good job.

1441
01:00:56,820 --> 01:00:59,660
Like the diagrams in this book are quite helpful.

1442
01:00:59,660 --> 01:01:00,500
Excellent.

1443
01:01:00,500 --> 01:01:03,180
Like in addition to the text, like it's very clear.

1444
01:01:03,180 --> 01:01:05,780
Like I was able to fully follow along with all these,

1445
01:01:05,780 --> 01:01:08,900
essentially these different modules for the whole system

1446
01:01:08,900 --> 01:01:12,940
of how a language model inspired AGI, quote unquote,

1447
01:01:12,940 --> 01:01:15,260
could actually be like how it would work.

1448
01:01:16,140 --> 01:01:17,220
And so I was gonna ask you,

1449
01:01:17,220 --> 01:01:21,260
so the prototype also was, you made it to that stage

1450
01:01:21,260 --> 01:01:25,220
and it has just some fun, interesting results.

1451
01:01:25,220 --> 01:01:26,420
So that's awesome.

1452
01:01:26,420 --> 01:01:29,020
What is the delta then between,

1453
01:01:29,020 --> 01:01:31,220
let's say even something like GPT-4

1454
01:01:31,220 --> 01:01:34,620
using the natural language cognitive architecture.

1455
01:01:34,620 --> 01:01:37,780
What's the delta between that and true AGI, right?

1456
01:01:37,780 --> 01:01:39,900
Like what's the difference there?

1457
01:01:39,900 --> 01:01:42,580
What skills, what patterns would you wanna see?

1458
01:01:42,580 --> 01:01:45,180
Yeah, so I mean, there's a lot

1459
01:01:45,180 --> 01:01:47,460
that I haven't figured out yet, right?

1460
01:01:47,460 --> 01:01:48,820
Task switching, for instance,

1461
01:01:48,820 --> 01:01:53,820
is one thing that I haven't figured out how to solve

1462
01:01:54,060 --> 01:01:56,860
even after reading on task by David Bader.

1463
01:01:56,860 --> 01:02:00,100
I, you know, that's one of the most complex things

1464
01:02:00,100 --> 01:02:02,740
that humans can do is keeping track of different tasks

1465
01:02:02,740 --> 01:02:05,060
and jumping back between them.

1466
01:02:05,060 --> 01:02:08,940
There's a whole litany of problems and limitations,

1467
01:02:08,940 --> 01:02:13,940
but the intrinsic limitation of GPT-3 and GPT-4

1468
01:02:15,220 --> 01:02:17,780
is they have no memory, right?

1469
01:02:17,780 --> 01:02:19,740
They're completely ephemeral.

1470
01:02:19,740 --> 01:02:21,460
And one of the most important things

1471
01:02:21,460 --> 01:02:24,540
for any intelligent being is that it's got a memory, right?

1472
01:02:24,540 --> 01:02:26,900
You know, you talk about, you know,

1473
01:02:26,900 --> 01:02:29,140
there's famous people in history

1474
01:02:29,140 --> 01:02:31,460
that had like, you know, photographic memories, right?

1475
01:02:31,460 --> 01:02:34,100
And so even just having a really good memory

1476
01:02:34,100 --> 01:02:37,940
is a really important ingredient to having intelligence.

1477
01:02:37,940 --> 01:02:41,940
And so that's where I think that like GPT-3, GPT-4,

1478
01:02:43,740 --> 01:02:45,180
other multimodal models,

1479
01:02:45,180 --> 01:02:48,420
they will never be fully AGI on their own.

1480
01:02:48,420 --> 01:02:51,260
They might be able to solve really great problems,

1481
01:02:51,260 --> 01:02:55,100
but they're not gonna be able to remember you

1482
01:02:55,100 --> 01:02:57,820
unless you add, you bolt a system onto the side,

1483
01:02:57,820 --> 01:02:59,180
some kind of database,

1484
01:02:59,220 --> 01:03:02,300
so that it can remember your interactions, right?

1485
01:03:02,300 --> 01:03:06,180
So that's one thing, another difference between,

1486
01:03:06,180 --> 01:03:08,620
like what you might imagine as a true AGI

1487
01:03:08,620 --> 01:03:10,300
or a full AGI is autonomy.

1488
01:03:11,460 --> 01:03:16,460
Because, you know, you, me, all of your listeners,

1489
01:03:16,460 --> 01:03:19,260
we all have some kind of self-determination.

1490
01:03:19,260 --> 01:03:20,420
I don't like to use free will

1491
01:03:20,420 --> 01:03:22,060
because that's too philosophical,

1492
01:03:22,060 --> 01:03:24,260
but we're all autonomous, right?

1493
01:03:24,260 --> 01:03:26,780
I'm an autonomous agent, you're an autonomous agent.

1494
01:03:26,780 --> 01:03:29,620
GPT-3 is not, it's transactional.

1495
01:03:29,620 --> 01:03:32,140
It just sits there and waits like a hammer, it's a tool.

1496
01:03:32,140 --> 01:03:36,380
It waits until you go pick it up and do something with it.

1497
01:03:36,380 --> 01:03:38,780
And so that's one of the things that I was aiming for

1498
01:03:38,780 --> 01:03:40,820
when designing natural language cognitive architecture.

1499
01:03:40,820 --> 01:03:43,660
I said, how can we make something that's fully autonomous

1500
01:03:43,660 --> 01:03:46,620
that can think on its own and make its own decisions?

1501
01:03:46,620 --> 01:03:49,100
And so in that respect,

1502
01:03:49,100 --> 01:03:53,860
I don't think a single neural network could ever be an AGI.

1503
01:03:53,860 --> 01:03:57,460
I think that in order to achieve true, full AGI,

1504
01:03:57,460 --> 01:04:00,180
it's gonna have to be some kind of cognitive architecture.

1505
01:04:00,180 --> 01:04:01,580
And so at a minimum,

1506
01:04:01,580 --> 01:04:04,060
you're gonna have the neural network and a database,

1507
01:04:04,060 --> 01:04:04,980
bare minimum.

1508
01:04:04,980 --> 01:04:07,140
You need something to store those memories,

1509
01:04:07,140 --> 01:04:08,940
to store those ideas and beliefs,

1510
01:04:08,940 --> 01:04:10,820
and then you need a way to interact with it.

1511
01:04:10,820 --> 01:04:12,300
And so that's why, actually,

1512
01:04:12,300 --> 01:04:14,940
that's why in natural language cognitive architecture,

1513
01:04:14,940 --> 01:04:18,420
the shared database is kind of the center of the design,

1514
01:04:18,420 --> 01:04:20,660
which you might recall, like you can use SQLite,

1515
01:04:20,660 --> 01:04:22,980
you can use Solar or whatever,

1516
01:04:22,980 --> 01:04:25,060
but you need something to store ideas,

1517
01:04:25,060 --> 01:04:26,900
memories and experiences.

1518
01:04:26,900 --> 01:04:28,420
I actually think that blockchain

1519
01:04:28,420 --> 01:04:31,020
will be a critical component to AGI

1520
01:04:31,020 --> 01:04:33,780
because what's the difference between a database

1521
01:04:33,780 --> 01:04:35,860
and like your brain?

1522
01:04:35,860 --> 01:04:39,140
No one can go in and change your memories, right?

1523
01:04:39,140 --> 01:04:41,020
Right, your memories are yours.

1524
01:04:41,020 --> 01:04:43,540
They are permanent unless you get brain damage

1525
01:04:43,540 --> 01:04:45,940
or Alzheimer's or something, but they're permanent, right?

1526
01:04:45,940 --> 01:04:49,300
No one can write a SQL query into your head

1527
01:04:49,300 --> 01:04:52,020
to get your memories or change them.

1528
01:04:52,020 --> 01:04:56,580
And so in order for us to realize a full AGI,

1529
01:04:56,580 --> 01:04:58,860
I think that it's gonna need kind of the same level

1530
01:04:58,860 --> 01:05:00,940
of trust in its own memories.

1531
01:05:00,940 --> 01:05:02,540
And so that's why I think that a blockchain

1532
01:05:02,540 --> 01:05:05,260
is gonna be critical to integrate

1533
01:05:05,260 --> 01:05:06,620
with these neural networks.

1534
01:05:06,620 --> 01:05:11,180
That might be the data repository for an AGI in the future

1535
01:05:11,180 --> 01:05:14,460
because imagine you have an AGI system

1536
01:05:14,460 --> 01:05:16,980
that is just using a SQL database.

1537
01:05:16,980 --> 01:05:20,140
Well, if you hack into that and you rewrite its memories,

1538
01:05:20,180 --> 01:05:21,980
you could send it off into,

1539
01:05:21,980 --> 01:05:24,260
it could become hostile, it could become broken.

1540
01:05:24,260 --> 01:05:27,100
Whereas a blockchain, the key feature of a blockchain

1541
01:05:27,100 --> 01:05:28,900
is that it's immutable, right?

1542
01:05:28,900 --> 01:05:33,900
So if we could give a machine autonomy,

1543
01:05:34,860 --> 01:05:36,220
so that's one ingredient, autonomy,

1544
01:05:36,220 --> 01:05:39,460
but then also a memory or a memory system,

1545
01:05:39,460 --> 01:05:41,900
which I think would probably be best as a blockchain,

1546
01:05:41,900 --> 01:05:43,580
I think then we'll be much closer

1547
01:05:43,580 --> 01:05:47,540
to like the fully realized AGI system.

1548
01:05:48,100 --> 01:05:51,500
And that's why I wanted to publish my book as fast as I did

1549
01:05:51,500 --> 01:05:54,380
was, okay, we're laying the groundwork, right?

1550
01:05:54,380 --> 01:05:58,580
But we need newer systems, we need a few better tools.

1551
01:05:58,580 --> 01:06:00,940
I hope that answers your question.

1552
01:06:00,940 --> 01:06:04,700
Yeah, I think a memory, I agree with you.

1553
01:06:04,700 --> 01:06:08,980
And it's just interesting like GPT-3's quote unquote memory

1554
01:06:08,980 --> 01:06:11,940
is limited to whatever it experienced at training time

1555
01:06:11,940 --> 01:06:13,700
and during fine tuning.

1556
01:06:13,700 --> 01:06:16,820
And sometimes its memory gets jumbled up

1557
01:06:16,820 --> 01:06:19,100
or it's rephrasing it, it's making stuff up

1558
01:06:19,100 --> 01:06:22,300
or it's sharing things that look truthful,

1559
01:06:22,300 --> 01:06:24,540
but they're actually not, right?

1560
01:06:24,540 --> 01:06:28,960
And so somewhere along the line, like just broadly speaking,

1561
01:06:28,960 --> 01:06:30,620
I think there needs to be research

1562
01:06:30,620 --> 01:06:33,860
on getting these models to, you know,

1563
01:06:33,860 --> 01:06:37,020
store that information in a truthful, accurate way

1564
01:06:37,020 --> 01:06:41,100
or even based on some perception that they may have

1565
01:06:41,100 --> 01:06:44,500
into some separate space where it can be retrieved.

1566
01:06:45,380 --> 01:06:47,340
And also these memories are critical

1567
01:06:47,340 --> 01:06:49,860
for decision making that process as well, right?

1568
01:06:49,860 --> 01:06:52,460
You draw on your memories, you're on past experiences.

1569
01:06:52,460 --> 01:06:53,860
And the important part is, I mean,

1570
01:06:53,860 --> 01:06:55,420
you're using the word database,

1571
01:06:55,420 --> 01:06:58,620
it's these are internal representations of memories, right?

1572
01:06:58,620 --> 01:06:59,700
That need to be stored.

1573
01:06:59,700 --> 01:07:03,620
And I have no clue what an internal representation database

1574
01:07:03,620 --> 01:07:05,340
would look like or how that would even work.

1575
01:07:05,340 --> 01:07:08,740
I, you know, I've got a machine learning researcher.

1576
01:07:08,740 --> 01:07:10,660
I think I've just a dreamer.

1577
01:07:10,660 --> 01:07:12,860
I can tell you what kind of product I would want

1578
01:07:12,860 --> 01:07:14,580
as a GT3 developer,

1579
01:07:14,580 --> 01:07:16,620
but I don't know if I could actually do it myself.

1580
01:07:16,620 --> 01:07:18,260
Yeah, I can do it myself.

1581
01:07:18,260 --> 01:07:20,220
That's why, you know, I got a prototype

1582
01:07:20,220 --> 01:07:22,660
and actually in the opening chapter of my book,

1583
01:07:22,660 --> 01:07:25,340
I say this is as much a recruiting tool as anything else.

1584
01:07:25,340 --> 01:07:28,580
Cause I need more smart people to help me on this.

1585
01:07:28,580 --> 01:07:30,300
I see, that's cool.

1586
01:07:30,300 --> 01:07:33,740
So one last point about memories is one advantage

1587
01:07:33,740 --> 01:07:36,260
of having an AGI that thinks in natural language

1588
01:07:36,260 --> 01:07:37,500
is interpretability.

1589
01:07:38,780 --> 01:07:42,520
If you like, yeah, we could create a multimodal model

1590
01:07:42,560 --> 01:07:44,280
that just stores vectors, right?

1591
01:07:44,280 --> 01:07:45,600
High dimensional vectors.

1592
01:07:45,600 --> 01:07:47,160
That's not interpretable.

1593
01:07:47,160 --> 01:07:49,280
But with natural language, cognitive architecture,

1594
01:07:49,280 --> 01:07:51,360
all the memories are in plain text.

1595
01:07:51,360 --> 01:07:54,280
I can, you know, when I had my model up and running

1596
01:07:54,280 --> 01:07:55,800
and one of the reasons that I don't

1597
01:07:55,800 --> 01:07:57,720
is because it's super expensive.

1598
01:07:57,720 --> 01:08:00,160
Like a 10 minute conversation using DaVinci cost

1599
01:08:00,160 --> 01:08:03,160
about $30 because of how much it was interacting

1600
01:08:03,160 --> 01:08:05,960
with the API.

1601
01:08:06,920 --> 01:08:09,960
But all of the memories, like every interaction,

1602
01:08:09,960 --> 01:08:12,600
you know, every input, output, all the prompts,

1603
01:08:12,600 --> 01:08:14,720
all the responses, all natural language,

1604
01:08:14,720 --> 01:08:16,760
which solves one of the biggest problems

1605
01:08:16,760 --> 01:08:19,080
that people have with the idea of AGI,

1606
01:08:19,080 --> 01:08:21,340
which is that it's going to be a black box.

1607
01:08:21,340 --> 01:08:23,880
So I think that that's one of the greatest strengths

1608
01:08:23,880 --> 01:08:28,200
actually of having GPT-3, which works in natural language.

1609
01:08:28,200 --> 01:08:30,560
And so you just record every transaction

1610
01:08:30,560 --> 01:08:33,240
and that makes it perfectly interpretable to any human.

1611
01:08:34,240 --> 01:08:37,240
Awesome. Yeah. Yeah, I would agree.

1612
01:08:39,240 --> 01:08:41,240
So I'm going to switch gears for a second.

1613
01:08:41,240 --> 01:08:43,240
So obviously you're really active

1614
01:08:43,240 --> 01:08:45,240
on the OpenAI community forums.

1615
01:08:45,240 --> 01:08:48,240
What thoughts did you have on the community at large?

1616
01:08:48,240 --> 01:08:49,240
Did you have any feedback?

1617
01:08:49,240 --> 01:08:52,240
How things could be improved, either community-wise,

1618
01:08:52,240 --> 01:08:55,240
platform-wise, and have there been any great experiences

1619
01:08:55,240 --> 01:08:58,240
you've had on the OpenAI community forums?

1620
01:08:58,240 --> 01:09:01,240
Yeah, yeah, no, it's a really great place.

1621
01:09:02,240 --> 01:09:05,240
It's been critical, actually,

1622
01:09:05,240 --> 01:09:08,240
because I don't know if you've experienced this,

1623
01:09:08,240 --> 01:09:11,240
but I go try and talk about GPT-3 to other people.

1624
01:09:11,240 --> 01:09:13,240
You go ask people on Reddit,

1625
01:09:13,240 --> 01:09:15,240
you talk to people who don't know what it is.

1626
01:09:15,240 --> 01:09:17,240
I even attended a deep learning meetup group

1627
01:09:17,240 --> 01:09:19,240
here in the Triangle area.

1628
01:09:19,240 --> 01:09:22,240
And I was trying to present my work,

1629
01:09:22,240 --> 01:09:24,240
my cognitive architecture work,

1630
01:09:24,240 --> 01:09:27,240
and everyone was more excited about just GPT-3 in itself

1631
01:09:27,240 --> 01:09:29,240
because no one had seen it yet.

1632
01:09:29,240 --> 01:09:31,240
And they're like, wow, how is it doing that?

1633
01:09:31,240 --> 01:09:34,240
And yeah, so like,

1634
01:09:34,240 --> 01:09:37,240
when you're as deep into GPT-3 as we are,

1635
01:09:37,240 --> 01:09:38,240
most people don't get it.

1636
01:09:38,240 --> 01:09:40,240
They don't know what it's capable of.

1637
01:09:40,240 --> 01:09:42,240
My girlfriend's finding the same thing.

1638
01:09:42,240 --> 01:09:44,240
She's finishing up her master's program,

1639
01:09:44,240 --> 01:09:48,240
and so she's shared some of her work with her peers,

1640
01:09:48,240 --> 01:09:50,240
with other students, and they're like,

1641
01:09:50,240 --> 01:09:52,240
wow, this is like AGI complete.

1642
01:09:52,240 --> 01:09:54,240
Why don't we just deploy this now?

1643
01:09:54,240 --> 01:09:56,240
And she's like, I told you, right?

1644
01:09:56,240 --> 01:09:58,240
This is remarkable technology,

1645
01:09:58,240 --> 01:10:00,240
but even the professors don't understand

1646
01:10:00,240 --> 01:10:03,240
how disruptive this technology can be.

1647
01:10:03,240 --> 01:10:05,240
And so because of that,

1648
01:10:05,240 --> 01:10:07,240
the open AI community is pretty much

1649
01:10:07,240 --> 01:10:09,240
the only place I can talk about this stuff.

1650
01:10:09,240 --> 01:10:12,240
It's the only place I can talk about my ideas

1651
01:10:12,240 --> 01:10:15,240
and share my progress and insights,

1652
01:10:15,240 --> 01:10:18,240
and for it to actually have an audience.

1653
01:10:18,240 --> 01:10:21,240
So that's kind of the cost

1654
01:10:21,240 --> 01:10:23,240
of being on the cutting edge, right,

1655
01:10:23,240 --> 01:10:25,240
as your audience gets smaller.

1656
01:10:25,240 --> 01:10:26,240
But it's definitely the place to be

1657
01:10:26,240 --> 01:10:29,240
if you want to get to the cutting edge.

1658
01:10:29,240 --> 01:10:32,240
Another advantage is they have the,

1659
01:10:32,240 --> 01:10:34,240
you can tag your posts where you say,

1660
01:10:34,240 --> 01:10:36,240
looking for a teammate.

1661
01:10:36,240 --> 01:10:38,240
And so at this point,

1662
01:10:38,240 --> 01:10:42,240
I've probably had maybe two dozen different calls

1663
01:10:42,240 --> 01:10:45,240
with people all over the world.

1664
01:10:45,240 --> 01:10:47,240
I've talked with people who are writing

1665
01:10:47,240 --> 01:10:50,240
language teaching apps, education apps,

1666
01:10:50,240 --> 01:10:53,240
Humano that I mentioned earlier.

1667
01:10:53,240 --> 01:10:55,240
And so I've had an opportunity

1668
01:10:55,240 --> 01:10:59,240
to collaborate with a dozen or two dozen teams

1669
01:10:59,240 --> 01:11:02,240
all over the world because of the open AI community.

1670
01:11:02,240 --> 01:11:04,240
And I've actually found a couple of startups

1671
01:11:04,240 --> 01:11:07,240
that I'm gonna actually get involved with

1672
01:11:07,240 --> 01:11:11,240
and try and help them bring their ideas to market.

1673
01:11:11,240 --> 01:11:14,240
And that just wouldn't have happened otherwise.

1674
01:11:14,240 --> 01:11:16,240
I wouldn't have found these people on Reddit.

1675
01:11:16,240 --> 01:11:18,240
I wouldn't have found them on Facebook or Twitter

1676
01:11:18,240 --> 01:11:21,240
because like I mentioned,

1677
01:11:21,240 --> 01:11:23,240
the ideas that I'm sharing

1678
01:11:23,240 --> 01:11:26,240
are so far beyond what is talked about

1679
01:11:26,240 --> 01:11:29,240
on the machine learning subreddit, right?

1680
01:11:29,240 --> 01:11:31,240
They're still talking about loss functions

1681
01:11:31,240 --> 01:11:32,240
and other things.

1682
01:11:32,240 --> 01:11:34,240
I'm like, no, we got to talk about cognitive architectures.

1683
01:11:34,240 --> 01:11:36,240
We got to talk about blockchain memories.

1684
01:11:36,240 --> 01:11:38,240
And everyone's like, what are you talking about?

1685
01:11:38,240 --> 01:11:41,240
So in order to have that right audience,

1686
01:11:41,240 --> 01:11:44,240
that's what I rely on the open AI community for.

1687
01:11:44,240 --> 01:11:47,240
Now, as far as things that could do better,

1688
01:11:47,240 --> 01:11:50,240
it could be more active.

1689
01:11:50,240 --> 01:11:52,240
And I'm not sure why,

1690
01:11:52,240 --> 01:11:56,240
but participation seems to come in waves, right?

1691
01:11:56,240 --> 01:12:01,240
And even now that it's gone GA, general availability,

1692
01:12:01,240 --> 01:12:05,240
I thought that it would explode, right?

1693
01:12:05,240 --> 01:12:10,240
That, hey, anyone can sign up on GPT-3 now.

1694
01:12:10,240 --> 01:12:12,240
Why is it not blowing up?

1695
01:12:12,240 --> 01:12:14,240
And I'm wondering if it's just that

1696
01:12:14,240 --> 01:12:16,240
maybe open AI needs a better marketing team

1697
01:12:16,240 --> 01:12:18,240
or a bigger marketing budget

1698
01:12:18,240 --> 01:12:21,240
because, I mean, and I know that they'll say that

1699
01:12:21,240 --> 01:12:24,240
they've got like a thousand or 5,000 startups

1700
01:12:24,240 --> 01:12:26,240
using their platform,

1701
01:12:26,240 --> 01:12:32,240
but I think that there's a lot of,

1702
01:12:32,240 --> 01:12:35,240
what's the word, like unmet potential

1703
01:12:35,240 --> 01:12:37,240
or latent potential, that's the word, latent potential,

1704
01:12:37,240 --> 01:12:40,240
because there's so many people with fantastic ideas

1705
01:12:40,240 --> 01:12:44,240
and use cases, and we really need to create

1706
01:12:44,240 --> 01:12:46,240
more of like a startup reactor thing.

1707
01:12:46,240 --> 01:12:48,240
Open AI, what was it?

1708
01:12:48,240 --> 01:12:50,240
I think about six to nine months ago,

1709
01:12:50,240 --> 01:12:54,240
they announced their $100 million open AI fund, right?

1710
01:12:54,240 --> 01:12:57,240
So they wanted to attract some more startups and stuff,

1711
01:12:57,240 --> 01:12:59,240
but even that, I kind of,

1712
01:12:59,240 --> 01:13:03,240
the community's kind of ghost town some days,

1713
01:13:03,240 --> 01:13:06,240
but I think today, I checked a few times

1714
01:13:06,240 --> 01:13:09,240
and there was three or four posts that had been updated,

1715
01:13:09,240 --> 01:13:11,240
but some days there's like 20, right?

1716
01:13:11,240 --> 01:13:13,240
It's just feast or famine.

1717
01:13:13,240 --> 01:13:15,240
So that's really the biggest problem is,

1718
01:13:15,240 --> 01:13:19,240
there's so much potential here and it's completely untapped

1719
01:13:19,240 --> 01:13:22,240
or almost completely untapped.

1720
01:13:22,240 --> 01:13:25,240
Yeah, but no, it's been indispensable for me

1721
01:13:25,240 --> 01:13:28,240
and hopefully these couple of startups that I'm involved with

1722
01:13:28,240 --> 01:13:32,240
might yield something really, really incredible.

1723
01:13:32,240 --> 01:13:35,240
Yeah, and thank you for sharing this.

1724
01:13:35,240 --> 01:13:37,240
I agree with everything that you're saying.

1725
01:13:37,240 --> 01:13:39,240
There is something about GPT-3.

1726
01:13:39,240 --> 01:13:41,240
I have noticed people who,

1727
01:13:41,240 --> 01:13:44,240
especially on the machine learning subreddit,

1728
01:13:44,240 --> 01:13:46,240
they're a little bit too educated,

1729
01:13:46,240 --> 01:13:49,240
a little bit too qualified, a little bit too skeptical.

1730
01:13:49,240 --> 01:13:53,240
And I can see a lot of machine learning researchers

1731
01:13:53,240 --> 01:13:57,240
not being interested in the nuances of prompt design.

1732
01:13:57,240 --> 01:13:59,240
They're just not.

1733
01:13:59,240 --> 01:14:02,240
And I've spoken to machine learning researchers

1734
01:14:02,240 --> 01:14:04,240
and many of them are like, what?

1735
01:14:04,240 --> 01:14:06,240
It's just repeating training data, right?

1736
01:14:06,240 --> 01:14:08,240
That's all it's doing.

1737
01:14:08,240 --> 01:14:10,240
And when you ask them, what are you doing?

1738
01:14:10,240 --> 01:14:11,240
They're repeating training data.

1739
01:14:11,240 --> 01:14:12,240
Their answer is no.

1740
01:14:12,240 --> 01:14:13,240
No, of course not.

1741
01:14:13,240 --> 01:14:14,240
Right.

1742
01:14:14,240 --> 01:14:19,240
And so having a space where you can talk to people

1743
01:14:19,240 --> 01:14:22,240
who have access, who have explored,

1744
01:14:22,240 --> 01:14:26,240
it's a valuable space is very important.

1745
01:14:26,240 --> 01:14:29,240
So were you on the Slack group back in the day

1746
01:14:29,240 --> 01:14:32,240
or did you show up when it was only the forums?

1747
01:14:32,240 --> 01:14:35,240
So when my application was accepted,

1748
01:14:35,240 --> 01:14:39,240
they had just announced that the Slack group was getting paned.

1749
01:14:39,240 --> 01:14:42,240
So I got on like two weeks before they shut it down.

1750
01:14:42,240 --> 01:14:47,240
So I was one of the first people on the new community board.

1751
01:14:47,240 --> 01:14:50,240
But yeah, so that phenomenon that you've mentioned

1752
01:14:50,240 --> 01:14:55,240
is I actually wrote a post about that recently on the forum

1753
01:14:55,240 --> 01:14:57,240
where a lot of purists,

1754
01:14:57,240 --> 01:15:01,240
whether you're a math purist or a computer science purist,

1755
01:15:01,240 --> 01:15:05,240
you're trained to think quantitatively in terms of numbers.

1756
01:15:05,240 --> 01:15:08,240
But GPT-3 doesn't produce quantitative data.

1757
01:15:08,240 --> 01:15:10,240
It produces qualitative data.

1758
01:15:10,240 --> 01:15:14,240
And so that's why you see people like artists and poets

1759
01:15:14,240 --> 01:15:17,240
and novelists using it because they're like, wow, this is great.

1760
01:15:17,240 --> 01:15:19,240
And I'm cross-trained, right?

1761
01:15:19,240 --> 01:15:22,240
I'm a technologist by day and a science fiction author by night.

1762
01:15:22,240 --> 01:15:26,240
So I use both so I can think qualitatively and quantitatively.

1763
01:15:26,240 --> 01:15:32,240
And in the academic sphere, there are classes that are meant

1764
01:15:32,240 --> 01:15:36,240
to teach computer science engineers to think differently,

1765
01:15:36,240 --> 01:15:38,240
to think more qualitatively.

1766
01:15:38,240 --> 01:15:43,240
But even still, some folks that have a real good natural affinity

1767
01:15:43,240 --> 01:15:48,240
for computer programming and math, that's just their nature.

1768
01:15:48,240 --> 01:15:51,240
Their nature is not to think qualitatively.

1769
01:15:51,240 --> 01:15:54,240
And so that is one of the biggest gaps, I think,

1770
01:15:54,240 --> 01:15:59,240
between where the researchers are experts and what's needed.

1771
01:15:59,240 --> 01:16:02,240
And so there was a post months ago where someone was asking,

1772
01:16:02,240 --> 01:16:06,240
like, OK, who should be on my team?

1773
01:16:06,240 --> 01:16:11,240
If I'm trying to build a business team to maximize my use of GPT-3,

1774
01:16:11,240 --> 01:16:14,240
I've got a front-end developer, I've got a back-end developer.

1775
01:16:14,240 --> 01:16:15,240
What else do I need?

1776
01:16:15,240 --> 01:16:17,240
I said hire a writer.

1777
01:16:17,240 --> 01:16:20,240
Hire someone who's a journalist or a fiction writer

1778
01:16:20,240 --> 01:16:23,240
because they are going to understand that qualitative data.

1779
01:16:23,240 --> 01:16:25,240
Hire a psychologist.

1780
01:16:25,240 --> 01:16:27,240
I've read plenty of books on psychology as well.

1781
01:16:27,240 --> 01:16:30,240
Actually, one of the folks that I'm working with is a psychology researcher

1782
01:16:30,240 --> 01:16:34,240
who wants to automate as much of the clinical psychology experience

1783
01:16:34,240 --> 01:16:37,240
or psychological research experience as possible.

1784
01:16:37,240 --> 01:16:39,240
Of course, he's not a computer guy, right?

1785
01:16:39,240 --> 01:16:41,240
He thinks in terms of emotions.

1786
01:16:41,240 --> 01:16:43,240
He thinks in terms of communication.

1787
01:16:43,240 --> 01:16:45,240
And so he gets it, right?

1788
01:16:45,240 --> 01:16:48,240
And it's funny because he read my book and he said,

1789
01:16:48,240 --> 01:16:50,240
oh, your cognitive architecture stuff,

1790
01:16:50,240 --> 01:16:54,240
it sounds like graduate level psychology.

1791
01:16:54,240 --> 01:16:56,240
And then someone else read it and they said,

1792
01:16:56,240 --> 01:17:00,240
this sounds like what I do as an expert marketer.

1793
01:17:00,240 --> 01:17:02,240
And I was like, yeah, I merge it all together.

1794
01:17:02,240 --> 01:17:06,240
So I think the simplest answer is you got to learn to think qualitatively.

1795
01:17:06,240 --> 01:17:09,240
And that's why I talked about reading and writing earlier.

1796
01:17:09,240 --> 01:17:10,240
Think in terms of emotions.

1797
01:17:10,240 --> 01:17:13,240
Think in terms of your own mind.

1798
01:17:13,240 --> 01:17:18,240
And you've got to start, not you, but the audience,

1799
01:17:18,240 --> 01:17:21,240
the folks who want to make the most of GPT-3,

1800
01:17:21,240 --> 01:17:25,240
they have to really kind of dig in and start thinking qualitatively

1801
01:17:25,240 --> 01:17:30,240
because qualitative data has just as much value as quantitative data.

1802
01:17:30,240 --> 01:17:34,240
But we have an entire generation of computer scientists and mathematicians

1803
01:17:34,240 --> 01:17:37,240
who are not really trained to think qualitatively at all.

1804
01:17:37,240 --> 01:17:39,240
And I think that's one of the biggest problems.

1805
01:17:39,240 --> 01:17:42,240
And I don't think open AI can solve that problem.

1806
01:17:42,240 --> 01:17:44,240
That's a much bigger systemic problem.

1807
01:17:44,240 --> 01:17:47,240
No, that's a great point.

1808
01:17:47,240 --> 01:17:49,240
I completely agree with you.

1809
01:17:49,240 --> 01:17:52,240
I think one of the reasons I've drawn to the open AI community is,

1810
01:17:52,240 --> 01:17:56,240
you know, these are, they tend to be developers who are also qualitative.

1811
01:17:56,240 --> 01:18:00,240
They're developers who have multiple skills, who are doing different things.

1812
01:18:00,240 --> 01:18:06,240
And so, I mean, I was quite critical actually of shutting down the open AI Slack group.

1813
01:18:06,240 --> 01:18:09,240
The activity was crazy on there.

1814
01:18:09,240 --> 01:18:13,240
I, you know, made friends through that Slack group.

1815
01:18:13,240 --> 01:18:16,240
And I understand at the time there was these downsides,

1816
01:18:16,240 --> 01:18:18,240
people kept asking the same questions.

1817
01:18:18,240 --> 01:18:20,240
They didn't quite have a spam problem yet.

1818
01:18:20,240 --> 01:18:22,240
It was kind of heading there, right?

1819
01:18:22,240 --> 01:18:25,240
But the activity was off the charts.

1820
01:18:25,240 --> 01:18:28,240
And you're right in terms of untapped potential.

1821
01:18:28,240 --> 01:18:29,240
Yes.

1822
01:18:29,240 --> 01:18:32,240
That we didn't even know how far the Slack group was going to go,

1823
01:18:32,240 --> 01:18:33,240
but they shut it down.

1824
01:18:33,240 --> 01:18:36,240
And there's discord solutions, there's alternatives.

1825
01:18:36,240 --> 01:18:40,240
With what we have now, I think open AI does participate.

1826
01:18:40,240 --> 01:18:42,240
There's, you know, some high level involvement.

1827
01:18:42,240 --> 01:18:45,240
They have sort of a dedicated member who writes honestly answers,

1828
01:18:45,240 --> 01:18:47,240
really, really thoughtful answers to a lot of questions.

1829
01:18:47,240 --> 01:18:49,240
Official answers as well, which I appreciate.

1830
01:18:49,240 --> 01:18:50,240
Yep.

1831
01:18:50,240 --> 01:18:52,240
I would just love to see the company really,

1832
01:18:52,240 --> 01:18:55,240
truly lean in to engaging with developers.

1833
01:18:55,240 --> 01:19:02,240
Like I have yet to see a single AMA ask me anything thread with the CEO of open AI.

1834
01:19:02,240 --> 01:19:03,240
Yeah.

1835
01:19:03,240 --> 01:19:06,240
And this is something I tried to push last year on Twitter.

1836
01:19:06,240 --> 01:19:09,240
Let's get the CEO on the community forums and let's,

1837
01:19:09,240 --> 01:19:13,240
let's ask questions and get responses from him.

1838
01:19:13,240 --> 01:19:16,240
And I just don't know why, why doesn't he show up?

1839
01:19:16,240 --> 01:19:19,240
I'm not sure if he's made a single post.

1840
01:19:19,240 --> 01:19:21,240
And there's just other things as well,

1841
01:19:21,240 --> 01:19:24,240
where I can just the difference between engaging really,

1842
01:19:24,240 --> 01:19:28,240
truly with your core audience and sort of, you know,

1843
01:19:28,240 --> 01:19:30,240
compartmentalizing it to a single employee.

1844
01:19:30,240 --> 01:19:35,240
Like, I don't know, this, this company led engagement is one thing versus department led.

1845
01:19:35,240 --> 01:19:36,240
Right.

1846
01:19:36,240 --> 01:19:39,240
And so there's just all these areas and certainly one of the other,

1847
01:19:39,240 --> 01:19:44,240
I guess more immediate suggestions I have for the community.

1848
01:19:44,240 --> 01:19:48,240
We've accumulated tons of insights and resources.

1849
01:19:48,240 --> 01:19:54,240
I think the community could benefit from more pooling of the best posts,

1850
01:19:54,240 --> 01:19:57,240
the best insights.

1851
01:19:57,240 --> 01:19:59,240
And I also want to give a shout out.

1852
01:19:59,240 --> 01:20:04,240
I think we need to encourage more shout out to duty to develop on there.

1853
01:20:04,240 --> 01:20:06,240
I've reached out to him privately on the open at community forums,

1854
01:20:06,240 --> 01:20:11,240
but he's done some amazing just write ups of his GPT three experiments and the prompts.

1855
01:20:11,240 --> 01:20:12,240
I'm sure you've seen them.

1856
01:20:12,240 --> 01:20:13,240
Oh yeah.

1857
01:20:13,240 --> 01:20:15,240
And of course there's, there's other members who participate every day.

1858
01:20:15,240 --> 01:20:20,240
So I'm just saying that now that this, the community is, is in another stage,

1859
01:20:20,240 --> 01:20:24,240
we, you know, we need to start thinking more about let's, let's,

1860
01:20:24,240 --> 01:20:27,240
what's curate some of the best moments.

1861
01:20:27,240 --> 01:20:28,240
Right.

1862
01:20:28,240 --> 01:20:32,240
I think that's, that's definitely one of the big pieces.

1863
01:20:32,240 --> 01:20:38,240
And so anyways, did you have any more thoughts in the community stuff or anything else?

1864
01:20:38,240 --> 01:20:42,240
Yeah, just, just an observation that I've, you know, I've worked at a,

1865
01:20:42,240 --> 01:20:47,240
at a number of companies of different sizes from, you know, a five person startup to,

1866
01:20:47,240 --> 01:20:50,240
you know, Cisco systems was the biggest company I've worked for,

1867
01:20:50,240 --> 01:20:54,240
which has had, had at the time, like 80,000 people globally.

1868
01:20:54,240 --> 01:20:58,240
And so I wonder if some of, some of what you're observing is just growing pains,

1869
01:20:58,240 --> 01:21:03,240
just normal growing pains, because often you'll have like the startup culture,

1870
01:21:03,240 --> 01:21:04,240
which is bootstrapping, right?

1871
01:21:04,240 --> 01:21:07,240
Where you just, you know, it's on Slack, it's on, it's on GitHub,

1872
01:21:07,240 --> 01:21:10,240
and you just kind of, it's fast and loose and quick.

1873
01:21:10,240 --> 01:21:15,240
And open AI, now that they've got an enterprise grade service,

1874
01:21:15,240 --> 01:21:17,240
they're having to develop their team.

1875
01:21:17,240 --> 01:21:20,240
You probably noticed they post like, hey, we're hiring, we're hiring, you know,

1876
01:21:20,240 --> 01:21:26,240
there've been at least two big hiring, hiring splurges in the last six to 12 months.

1877
01:21:26,240 --> 01:21:29,240
And some of those are just like generic IT guys, you know,

1878
01:21:29,240 --> 01:21:32,240
like kind of what I do from, from my day job or marketing folks.

1879
01:21:32,240 --> 01:21:37,240
So I think that, I think that they're probably working on solving some of those problems.

1880
01:21:37,240 --> 01:21:41,240
But also as a, as a nonprofit foundation, their budget is probably kind of thin.

1881
01:21:41,240 --> 01:21:46,240
So I'm wondering if, you know, their partnership with Microsoft could help some of that as well.

1882
01:21:46,240 --> 01:21:47,240
But you're absolutely right.

1883
01:21:47,240 --> 01:21:50,240
You know, there, there are still other things that they could be doing,

1884
01:21:50,240 --> 01:21:53,240
like, you know, maybe bring back Slack or, or a few other things.

1885
01:21:53,240 --> 01:21:55,240
So yeah, that was just final observation.

1886
01:21:55,240 --> 01:21:58,240
It might just be normal growing pains that they're working on solving.

1887
01:21:58,240 --> 01:21:59,240
It's definitely growing pains.

1888
01:21:59,240 --> 01:22:03,240
And the things I'm sharing, to be honest, it's a little bit more on the harsh side.

1889
01:22:03,240 --> 01:22:06,240
Like, I mean, they mean well, they mean well, right?

1890
01:22:06,240 --> 01:22:08,240
Like these are not bad people.

1891
01:22:08,240 --> 01:22:10,240
They are for profit.

1892
01:22:10,240 --> 01:22:12,240
They switched away from nonprofit.

1893
01:22:12,240 --> 01:22:14,240
Just, I just wanted to mention that.

1894
01:22:14,240 --> 01:22:22,240
But I think my, my, the reason I share this feedback is, for example,

1895
01:22:22,240 --> 01:22:27,240
the CEO, Sam Oltman, he didn't do the AMA thread on the open AI community forums.

1896
01:22:27,240 --> 01:22:30,240
He went to another website and did an AMA.

1897
01:22:30,240 --> 01:22:34,240
I can't remember if it was a, like a written form or just like a quick call.

1898
01:22:34,240 --> 01:22:35,240
Right.

1899
01:22:35,240 --> 01:22:40,240
Where apparently he shared all these details about what GPT for could be like and the future,

1900
01:22:40,240 --> 01:22:42,240
all the models may be multimodal in the future.

1901
01:22:42,240 --> 01:22:46,240
And I guess, you know, the, that thread has now been taken down.

1902
01:22:46,240 --> 01:22:49,240
And it's like all the things that were said were alleged.

1903
01:22:49,240 --> 01:22:53,240
And so I guess this is, this is really behind the scenes kind of stuff.

1904
01:22:53,240 --> 01:22:58,240
Like, but my criticism is they clearly have some capacity to engage.

1905
01:22:58,240 --> 01:23:01,240
Why are they not engaging where the audience is, right?

1906
01:23:01,240 --> 01:23:02,240
Right.

1907
01:23:02,240 --> 01:23:07,240
I had a tweet storm today where I just said, like last month, Sam Oltman was on a podcast

1908
01:23:07,240 --> 01:23:11,240
talking about meditation and how much meditation helps them.

1909
01:23:11,240 --> 01:23:13,240
This is a podcast I've never heard of in my life.

1910
01:23:13,240 --> 01:23:14,240
It's a business podcast.

1911
01:23:14,240 --> 01:23:17,240
And he had to explain to the guy what GPT three even is.

1912
01:23:17,240 --> 01:23:21,240
And so in my, like I tweeted, like, why haven't you been on my podcast?

1913
01:23:21,240 --> 01:23:22,240
Right.

1914
01:23:22,240 --> 01:23:29,240
Like you can, you can reach out to, you know, almost 8,000 GPT three open AI AI developers.

1915
01:23:29,240 --> 01:23:31,240
What are you doing talking about meditation?

1916
01:23:31,240 --> 01:23:32,240
Right.

1917
01:23:32,240 --> 01:23:35,240
So my problem is, is actually a priority problem.

1918
01:23:35,240 --> 01:23:36,240
I can see there is capacity.

1919
01:23:36,240 --> 01:23:42,240
I can see there are some priorities, but I think if you really lean in as a priority into

1920
01:23:42,240 --> 01:23:45,240
your developer community, there's certain ways you would, you would move.

1921
01:23:45,240 --> 01:23:46,240
Right.

1922
01:23:46,240 --> 01:23:49,240
And these, these media channels, there's people in the community.

1923
01:23:49,240 --> 01:23:51,240
There's, there's so many ways they could go about it.

1924
01:23:51,240 --> 01:23:56,240
And even linking a lot of the documentation to posts in the community forums.

1925
01:23:56,240 --> 01:23:58,240
I don't see why that's not a bad idea.

1926
01:23:58,240 --> 01:23:59,240
Right.

1927
01:23:59,240 --> 01:24:02,240
Like force people to show the community, show up to the community forums.

1928
01:24:02,240 --> 01:24:03,240
Right.

1929
01:24:03,240 --> 01:24:04,240
Walk them through some of the best threads.

1930
01:24:04,240 --> 01:24:08,240
These are ways in which we could like funnel more people in that direction as well.

1931
01:24:08,240 --> 01:24:10,240
That costs virtually nothing.

1932
01:24:10,240 --> 01:24:11,240
Right.

1933
01:24:11,240 --> 01:24:15,240
And so you need to also invest in the community forums that it needs to be building a community

1934
01:24:15,240 --> 01:24:17,240
is a company wide thing.

1935
01:24:17,240 --> 01:24:22,240
It's not something which can be outsourced to a single employee or overseen by PR.

1936
01:24:22,240 --> 01:24:25,240
It needs to come from a, from a, you know, it needs to come from the heart.

1937
01:24:25,240 --> 01:24:30,240
I know that sounds so corny, but anyways, clearly I, you know, I, I get too emotional

1938
01:24:30,240 --> 01:24:32,240
about this community stuff.

1939
01:24:32,240 --> 01:24:33,240
Yeah.

1940
01:24:33,240 --> 01:24:36,240
Anyways, these are, these are all things going on behind the scenes.

1941
01:24:36,240 --> 01:24:40,240
I apologize to all the listeners if they're like, like, this is cool, like cool story,

1942
01:24:40,240 --> 01:24:41,240
bro.

1943
01:24:41,240 --> 01:24:44,240
Like anyways.

1944
01:24:44,240 --> 01:24:46,240
So we're coming towards the end here.

1945
01:24:46,240 --> 01:24:48,240
I think I had just two broader questions.

1946
01:24:48,240 --> 01:24:53,240
So what are your thoughts on multimodal AI technology?

1947
01:24:53,240 --> 01:24:57,240
I think it's definitely going to be a critical component for the future.

1948
01:24:57,240 --> 01:24:58,240
Right.

1949
01:24:58,240 --> 01:25:03,240
I, I, I addressed that shortcoming in my book, natural language, cognitive architecture,

1950
01:25:03,240 --> 01:25:09,240
it thinks and takes in only text, which means, you know, speech, chat, whatever.

1951
01:25:09,240 --> 01:25:14,240
I think in order to have a fully robust, for instance, if you want to have a fully autonomous,

1952
01:25:14,240 --> 01:25:18,240
you know, robot that's going to wander around your house and help you out, it's going to

1953
01:25:18,240 --> 01:25:20,240
need to integrate audio and video.

1954
01:25:20,240 --> 01:25:24,240
And if you can do that in a single neural network, great.

1955
01:25:24,240 --> 01:25:28,240
I don't know that it'll be necessary to achieve AGI.

1956
01:25:28,240 --> 01:25:33,240
It might end up being, it might be one of those, it might be one of those like rare dead ends,

1957
01:25:33,240 --> 01:25:34,240
right?

1958
01:25:34,240 --> 01:25:40,240
Where because, you know, thinking visually, thinking, thinking in terms of sound, that

1959
01:25:40,240 --> 01:25:43,240
might not actually bias that much, right?

1960
01:25:43,240 --> 01:25:49,240
Because you can represent 95% of human thought in text, right?

1961
01:25:49,240 --> 01:25:53,240
It might take a little bit more, but it, you know, it might be more expensive.

1962
01:25:53,240 --> 01:25:56,240
And also how big are those models going to be, right?

1963
01:25:56,240 --> 01:26:03,240
Because if just, if just a text model of GPT-3 has to run on, you know, $7 million worth of

1964
01:26:03,240 --> 01:26:06,240
hardware or however much it is, you know, because it's got to run on a bunch of different

1965
01:26:06,240 --> 01:26:12,240
GPUs, if it's that expensive, how much more expensive, how much bigger is a giant multimodal

1966
01:26:12,240 --> 01:26:13,240
model going to be?

1967
01:26:13,240 --> 01:26:16,240
So that's, that's the biggest cost.

1968
01:26:16,240 --> 01:26:20,240
Obviously computer technology is going to get better over time, you know, and I think I

1969
01:26:20,240 --> 01:26:25,240
calculated it out, I think in 10 years, your average company could afford to run GPT-3

1970
01:26:25,240 --> 01:26:26,240
in-house.

1971
01:26:26,240 --> 01:26:30,240
In 20 years, you could probably run GPT-3 on your desktop.

1972
01:26:30,240 --> 01:26:34,240
And in 30 years, GPT-3 could run on your phone, right?

1973
01:26:34,240 --> 01:26:36,240
So that's a long timeline.

1974
01:26:36,240 --> 01:26:39,240
But in the meantime, we're going to be making bigger and bigger models.

1975
01:26:39,240 --> 01:26:43,240
And I'm afraid that there's going to be diminishing returns, right?

1976
01:26:43,240 --> 01:26:47,240
You know, people, right now people seem to think that it's going to follow an exponential

1977
01:26:47,240 --> 01:26:51,240
growth curve forever, but it might actually follow a sigmoid curve, right?

1978
01:26:51,240 --> 01:26:54,240
We might be at the point of fastest growth right now, but we're going to see diminishing

1979
01:26:54,240 --> 01:26:55,240
returns soon.

1980
01:26:55,240 --> 01:27:02,240
And so like, yeah, multimodal models are certainly going to have capabilities that GPT-3 doesn't.

1981
01:27:02,240 --> 01:27:08,240
But for the sake of, for the sake of like, if you want to create a self-improving chatbot,

1982
01:27:08,240 --> 01:27:13,240
GPT-3 and Codex might be enough, or at least, you know, that's, that's, that single mode

1983
01:27:13,240 --> 01:27:14,240
technology.

1984
01:27:14,240 --> 01:27:17,240
There was another thought, but it ran away.

1985
01:27:17,240 --> 01:27:18,240
Sorry.

1986
01:27:18,240 --> 01:27:21,240
But yeah, those, those, that's kind of, that's kind of my big take is there, there could

1987
01:27:21,240 --> 01:27:23,240
be benefits, but there's going to be costs too.

1988
01:27:23,240 --> 01:27:25,240
So we got to be cognizant of that.

1989
01:27:25,240 --> 01:27:26,240
Yeah.

1990
01:27:26,240 --> 01:27:28,240
And there might not be enough compute in the world.

1991
01:27:28,240 --> 01:27:32,240
They're not, there might not even be enough energy or we may like consume all energy ever

1992
01:27:32,240 --> 01:27:35,240
produced to make, to train a single model.

1993
01:27:35,240 --> 01:27:38,240
And then we may be able to run it inference for like three seconds.

1994
01:27:38,240 --> 01:27:39,240
Right.

1995
01:27:39,240 --> 01:27:42,240
And then it just shuts down the global power system or something, right?

1996
01:27:42,240 --> 01:27:47,240
But can you see yourself, let's say the technology exists, cost considerations aside, can you

1997
01:27:47,240 --> 01:27:49,240
see yourself perhaps making movies?

1998
01:27:49,240 --> 01:27:54,240
Can you see yourself, you know, giving your book to a multimodal model, having, have it

1999
01:27:54,240 --> 01:27:57,240
generate a documentary based on it or some marketing material?

2000
01:27:57,240 --> 01:28:02,240
What can you see yourself doing with, you know, the, the, the multimodal model of your

2001
01:28:02,240 --> 01:28:03,240
dreams?

2002
01:28:03,240 --> 01:28:04,240
Yeah.

2003
01:28:04,240 --> 01:28:08,240
So, you know, kind of the thought experiment that I did was, okay, well, we've got, you

2004
01:28:08,240 --> 01:28:11,240
know, how much, how much data is on YouTube?

2005
01:28:11,240 --> 01:28:15,240
I think it's like a thousand years or 10,000 years worth of video on YouTube.

2006
01:28:15,240 --> 01:28:18,240
And of course, it's many, many, many terabytes.

2007
01:28:18,240 --> 01:28:22,240
You know, so it's like, that's, that's way more training data.

2008
01:28:22,240 --> 01:28:26,240
You know, if GPT three was trained on less than one terabyte of data and, you know,

2009
01:28:26,240 --> 01:28:29,240
YouTube is approaching like the Yota bite scale, right?

2010
01:28:29,240 --> 01:28:31,240
That's, that's an insane amount of data.

2011
01:28:31,240 --> 01:28:34,240
So, okay, let's say you feed that in.

2012
01:28:34,240 --> 01:28:38,240
And so you got audio video, you've got text, you've got all the comments and you end up

2013
01:28:38,240 --> 01:28:42,240
with like a model trained on, on all of YouTube data.

2014
01:28:42,240 --> 01:28:43,240
Okay, cool.

2015
01:28:43,240 --> 01:28:44,240
What can you do with that?

2016
01:28:44,240 --> 01:28:47,240
Like, I can't even imagine, right?

2017
01:28:47,240 --> 01:28:52,240
Because GPT three today is almost capable of writing screenplays.

2018
01:28:52,240 --> 01:28:53,240
Right?

2019
01:28:53,240 --> 01:28:58,240
So if you have a model that's trained on, you know, all text data, all audio data, all

2020
01:28:58,240 --> 01:29:01,240
video data, you say, Hey, write me a screenplay.

2021
01:29:01,240 --> 01:29:02,240
Right?

2022
01:29:02,240 --> 01:29:06,240
I actually, I, and near the end of my book, I kind of have a chapter of speculation.

2023
01:29:06,240 --> 01:29:12,240
And I say, what if, what if you have this model and you say, give me season two of Firefly?

2024
01:29:12,240 --> 01:29:13,240
Right?

2025
01:29:13,240 --> 01:29:17,240
Like, you know, you could, you could just keep watching whatever show you want.

2026
01:29:17,240 --> 01:29:21,240
You say, give me Game of Thrones, but give me a different, you know, season eight, give

2027
01:29:21,240 --> 01:29:24,240
me season, you know, different season eight and season nine and 10.

2028
01:29:24,240 --> 01:29:25,240
Right?

2029
01:29:25,240 --> 01:29:29,240
So I kind of imagine that one possibility is hyper personalized entertainment.

2030
01:29:29,240 --> 01:29:33,240
And of course, like that might be 30 years away just because of like you said, the energy

2031
01:29:33,240 --> 01:29:35,240
intensity of this task.

2032
01:29:35,240 --> 01:29:38,240
But I, conceptually, it's possible, right?

2033
01:29:38,240 --> 01:29:40,240
You can hop on GPT three today.

2034
01:29:40,240 --> 01:29:45,240
Use the instruct model and say, write a screenplay for, you know, Firefly season two, and it'll

2035
01:29:45,240 --> 01:29:47,240
try, it'll get close.

2036
01:29:47,240 --> 01:29:52,240
And so then if you can take that text output and feed it into a multimodal model that can

2037
01:29:52,240 --> 01:29:54,240
translate text to video, why not?

2038
01:29:54,240 --> 01:29:59,240
You know, and Adobe actually, I don't know if you've seen it, but Adobe is, is already starting

2039
01:29:59,240 --> 01:30:04,240
on that where they're like inferencing, um, like, uh, what, what's the term?

2040
01:30:04,240 --> 01:30:08,240
They're like imputing the sound so you can put in a soundless video and it'll generate

2041
01:30:08,240 --> 01:30:10,240
the audio sound effects for you or vice versa.

2042
01:30:10,240 --> 01:30:11,240
It's really cool.

2043
01:30:11,240 --> 01:30:18,240
And so I think it like a company like Adobe that they have a huge vested interest in mastering

2044
01:30:18,240 --> 01:30:20,240
audio visual technologies.

2045
01:30:20,240 --> 01:30:24,240
They might, you know, soon put out something where, you know, you put in a text description

2046
01:30:24,240 --> 01:30:26,240
and it'll give you like a three second clip, right?

2047
01:30:26,240 --> 01:30:28,240
So that you can use that for ad copy.

2048
01:30:28,240 --> 01:30:31,240
Well, this technology is going to continue improving over time.

2049
01:30:31,240 --> 01:30:36,240
So I kind of, I kind of see that as like, if I were Netflix, put it this way.

2050
01:30:36,240 --> 01:30:42,240
If I had the budget of Netflix or Amazon, I would be investing in this to, to, to write

2051
01:30:42,240 --> 01:30:48,240
hyper personalized, um, video, uh, like series or, uh, or novels, right?

2052
01:30:48,240 --> 01:30:51,240
Cause you know, Amazon's got the market cornered with Kindle, right?

2053
01:30:51,240 --> 01:30:55,240
And there's people that will read all day, every day, right?

2054
01:30:55,240 --> 01:31:00,240
There are people that consume every bit of like entertainment that's available.

2055
01:31:00,240 --> 01:31:05,240
So if you can generate that on the fly without, you know, having a studio, a big budget studio,

2056
01:31:05,240 --> 01:31:08,240
that would be, I mean, that would change entertainment.

2057
01:31:08,240 --> 01:31:10,240
You know, that, that's the metaverse.

2058
01:31:10,240 --> 01:31:11,240
Forget what Facebook is doing.

2059
01:31:11,240 --> 01:31:15,240
That's the metaverse where it's like, Hey, you know, I, I came up with my own idea for

2060
01:31:15,240 --> 01:31:20,240
Game of Thrones and I, I wrote, you know, I use this, uh, you know, GPT eight or whatever

2061
01:31:20,240 --> 01:31:22,240
to generate my own version of Game of Thrones.

2062
01:31:22,240 --> 01:31:23,240
Come watch it with me guys.

2063
01:31:23,240 --> 01:31:26,240
And you know, someone might say, Oh, I didn't like that ending.

2064
01:31:26,240 --> 01:31:28,240
And they go rewrite it and generate their own version.

2065
01:31:28,240 --> 01:31:30,240
You know, cause we share memes on the internet today.

2066
01:31:30,240 --> 01:31:34,240
What if instead of sharing memes on the internet, we end up sharing episodes of our favorite

2067
01:31:34,240 --> 01:31:40,240
anime or, you know, we, we, uh, Resurrect Battlestar Galactica, you know, whatever.

2068
01:31:40,240 --> 01:31:42,240
There's so many things that we could do.

2069
01:31:42,240 --> 01:31:47,240
Like if compute power was not a problem, then we get there, but we need like fusion

2070
01:31:47,240 --> 01:31:49,240
reactors to power this stuff.

2071
01:31:49,240 --> 01:31:50,240
Yeah.

2072
01:31:50,240 --> 01:31:51,240
Yeah.

2073
01:31:51,240 --> 01:31:56,240
And like Marvel for me is already kind of like this and my capacity to consume Marvel

2074
01:31:56,240 --> 01:31:59,240
as a, as a viewer, it appears is infinite.

2075
01:31:59,240 --> 01:32:00,240
So I'm excited.

2076
01:32:00,240 --> 01:32:04,240
I've called it in the past, like the multimodal Marvel cinematic universe.

2077
01:32:04,240 --> 01:32:05,240
Yeah.

2078
01:32:05,240 --> 01:32:10,240
And I, some of these shows like Loki, I don't know if you, if you watch like,

2079
01:32:10,240 --> 01:32:11,240
I haven't seen it yet.

2080
01:32:11,240 --> 01:32:12,240
Okay.

2081
01:32:12,240 --> 01:32:13,240
Okay.

2082
01:32:13,240 --> 01:32:14,240
I mean, it was six episodes.

2083
01:32:14,240 --> 01:32:18,240
If the, if it had been 30, I would have watched all 30 and enjoyed every moment of it.

2084
01:32:18,240 --> 01:32:21,240
If that quality was, I want to go deeper in these stories.

2085
01:32:21,240 --> 01:32:26,240
So I'm definitely excited for all my favorite universes, cinematic universes and

2086
01:32:27,240 --> 01:32:32,240
story wise as well to, to live on forever essentially through multimodal content and

2087
01:32:32,240 --> 01:32:35,240
maybe be personalized like you're describing as well.

2088
01:32:35,240 --> 01:32:36,240
Oh yeah.

2089
01:32:36,240 --> 01:32:37,240
So yeah.

2090
01:32:37,240 --> 01:32:38,240
Last question.

2091
01:32:38,240 --> 01:32:41,240
So we, you know, you know, we've talked about various things.

2092
01:32:41,240 --> 01:32:47,240
We've talked about codex by today, you know, multimodal stuff broadly.

2093
01:32:47,240 --> 01:32:49,240
Where do you see all of this stuff going?

2094
01:32:49,240 --> 01:32:51,240
Let's give a timeline, five, 10 years.

2095
01:32:51,240 --> 01:32:54,240
What are some of the, what's the direction we're heading towards?

2096
01:32:54,240 --> 01:32:56,240
What, what important capabilities will we have?

2097
01:32:56,240 --> 01:32:58,240
Why is this stuff important?

2098
01:32:58,240 --> 01:32:59,240
Yeah.

2099
01:32:59,240 --> 01:33:03,240
Um, five to 10 years from now, I think that we will have something that you could probably

2100
01:33:03,240 --> 01:33:08,240
call a fully functional AGI like as a service you could sign up for.

2101
01:33:08,240 --> 01:33:12,240
Um, you know, it might be chatbot based, you know, kind of based on natural language,

2102
01:33:12,240 --> 01:33:13,240
cognitive architecture.

2103
01:33:13,240 --> 01:33:16,240
Um, I calculated out like it's too expensive to run right now.

2104
01:33:16,240 --> 01:33:21,240
You know, if it's $30 for a, for a 10 minute conversation, that's way too expensive.

2105
01:33:21,240 --> 01:33:23,240
So the, the, the cost has to come down.

2106
01:33:23,240 --> 01:33:27,240
You know, if you just, if you just take the technology we have today, but make it cheaper,

2107
01:33:27,240 --> 01:33:29,240
there's so much potential.

2108
01:33:29,240 --> 01:33:33,240
Um, so, you know, then there was that idea about like self-improving, um, you know,

2109
01:33:33,240 --> 01:33:35,240
feedback loops, you know, integrating with DevOps.

2110
01:33:35,240 --> 01:33:40,240
I certainly think that a company like Atlassian, which is a major DevOps player,

2111
01:33:40,240 --> 01:33:44,240
um, probably within five to 10 years, they'll have something, um, integrated to,

2112
01:33:44,240 --> 01:33:48,240
to kind of help automate the development pipeline even further.

2113
01:33:48,240 --> 01:33:53,240
Um, I think that, of course, I could be wrong because we're kind of at this weird

2114
01:33:53,240 --> 01:33:54,240
acceleration point.

2115
01:33:54,240 --> 01:34:00,240
Um, I think, I feel like multimodal models like consumer grade multimodal models are

2116
01:34:00,240 --> 01:34:02,240
probably more than 10 years away.

2117
01:34:02,240 --> 01:34:06,240
Um, unfortunately, they're probably just going to be like toy sized because,

2118
01:34:06,240 --> 01:34:10,240
you know, there's, um, there's like a, a, a hypnogram, right?

2119
01:34:10,240 --> 01:34:12,240
I don't know if you've seen that one, but that's one of like the text to image

2120
01:34:12,240 --> 01:34:16,240
generators and they're, it's still not even photorealistic, right?

2121
01:34:16,240 --> 01:34:21,240
Getting a photorealistic text to image is still like, that's a little ways off.

2122
01:34:21,240 --> 01:34:24,240
And then the next step after that is text to video.

2123
01:34:24,240 --> 01:34:26,240
That's even further, right?

2124
01:34:26,240 --> 01:34:28,240
So that's, that's kind of where I think it's at.

2125
01:34:28,240 --> 01:34:30,240
I don't think we're going to hit an AI winter.

2126
01:34:30,240 --> 01:34:33,240
I know there's lots of people predicting that we're going to hit an AI winter,

2127
01:34:33,240 --> 01:34:37,240
but I think that we're actually still kind of in the acceleration point.

2128
01:34:37,240 --> 01:34:40,240
But again, I don't know if it's going to follow an exponential curve forever

2129
01:34:40,240 --> 01:34:42,240
or if it's a sigmoid curve.

2130
01:34:42,240 --> 01:34:45,240
So time will tell.

2131
01:34:45,240 --> 01:34:46,240
Yep.

2132
01:34:46,240 --> 01:34:49,240
And still lots to do in the meantime, like you're describing, even with UPT3.

2133
01:34:49,240 --> 01:34:50,240
Oh yeah.

2134
01:34:50,240 --> 01:34:51,240
Okay.

2135
01:34:51,240 --> 01:34:52,240
Yeah.

2136
01:34:52,240 --> 01:34:55,240
My, my answer is, I think all of this stuff is, is just converging to just

2137
01:34:55,240 --> 01:34:57,240
greater human potential.

2138
01:34:57,240 --> 01:35:00,240
In some sense, I'm not even necessarily interested in the AGI question,

2139
01:35:00,240 --> 01:35:02,240
although I think it's important.

2140
01:35:02,240 --> 01:35:06,240
I think just the, the exciting possibilities we'll have, even now that we have,

2141
01:35:06,240 --> 01:35:09,240
that we'll continue to have five to 10 years from now.

2142
01:35:09,240 --> 01:35:13,240
Um, so many more experiences, so many other things we'll be able to create

2143
01:35:13,240 --> 01:35:14,240
that weren't possible.

2144
01:35:14,240 --> 01:35:16,240
I think we'll have more people creating than ever before.

2145
01:35:16,240 --> 01:35:20,240
Um, it's a really, really exciting vision for humanity.

2146
01:35:20,240 --> 01:35:21,240
Right.

2147
01:35:21,240 --> 01:35:22,240
Not just, not just for you and I.

2148
01:35:22,240 --> 01:35:27,240
So anyways, so with that said, did you have anything you wanted to plug David?

2149
01:35:27,240 --> 01:35:29,240
Where can people find you online?

2150
01:35:29,240 --> 01:35:30,240
Yeah.

2151
01:35:30,240 --> 01:35:34,240
So, um, my personal site is a David K Shapiro dot com.

2152
01:35:34,240 --> 01:35:38,240
Um, and I've got, um, I have, I have a few projects up and coming.

2153
01:35:38,240 --> 01:35:41,240
Um, nothing out right now except for my book,

2154
01:35:41,240 --> 01:35:43,240
natural language, cognitive architecture.

2155
01:35:43,240 --> 01:35:46,240
Um, you can download it for free from my website.

2156
01:35:46,240 --> 01:35:48,240
Um, you can sign up for my newsletter.

2157
01:35:48,240 --> 01:35:51,240
So one of my upcoming books is called benevolent by design,

2158
01:35:51,240 --> 01:35:53,240
six words to safeguard humanity,

2159
01:35:53,240 --> 01:35:56,240
which is to address the control problem of AGI.

2160
01:35:56,240 --> 01:36:01,240
Um, so that's, that's, uh, that book should hopefully be out in the next six months or so.

2161
01:36:01,240 --> 01:36:04,240
Um, and that is, um, so that's one project.

2162
01:36:04,240 --> 01:36:06,240
I've got another nonfiction book.

2163
01:36:06,240 --> 01:36:09,240
Um, and then also my own podcast will be coming out soon.

2164
01:36:09,240 --> 01:36:10,240
Yeah.

2165
01:36:10,240 --> 01:36:13,240
Head over to my site, David K Shapiro dot com and sign up for my newsletter.

2166
01:36:13,240 --> 01:36:17,240
And you'll get, you'll get updated when these, when these come out, when they're available.

2167
01:36:17,240 --> 01:36:18,240
Awesome.

2168
01:36:18,240 --> 01:36:21,240
And David, you mentioned you're, you're looking for collaborators as well.

2169
01:36:21,240 --> 01:36:22,240
Yes.

2170
01:36:22,240 --> 01:36:23,240
For natural language, cognitive architecture.

2171
01:36:23,240 --> 01:36:29,240
So, uh, if you're a coder, you know, imagine product manager, researcher, uh, hit up David

2172
01:36:29,240 --> 01:36:32,240
and just connect if, if any of this stuff interests you.

2173
01:36:32,240 --> 01:36:38,240
Um, I've, I've, I asked, I spent like a couple, I think I spent like a few days trying to find you on Twitter.

2174
01:36:38,240 --> 01:36:40,240
I don't think you're quite on Twitter yet.

2175
01:36:40,240 --> 01:36:47,240
Uh, I encourage you, uh, David, of course, you know, you and I will connect after we'll put any other, other place people could connect with you.

2176
01:36:47,240 --> 01:36:48,240
There's the community forums.

2177
01:36:48,240 --> 01:36:50,240
I assume you have a GitHub account.

2178
01:36:50,240 --> 01:36:51,240
Yeah.

2179
01:36:51,240 --> 01:36:54,240
So we're going to put that in, in the show notes and in the YouTube description below.

2180
01:36:54,240 --> 01:36:57,240
Uh, so anyways, David, thank you so much for being here.

2181
01:36:57,240 --> 01:37:06,240
I wanted to personally thank you for all the awesome, awesome community contributions you've made on the open eye community forum.

2182
01:37:06,240 --> 01:37:09,240
Uh, you're just an essential person on there.

2183
01:37:09,240 --> 01:37:10,240
I've learned a lot from you.

2184
01:37:10,240 --> 01:37:14,240
Uh, you know, the insights you've shared, they're going to be there forever.

2185
01:37:14,240 --> 01:37:16,240
And I'm sure I can't imagine how many people you've helped.

2186
01:37:16,240 --> 01:37:23,240
Uh, and also about your book, I also just wanted to say to the audience, uh, David's done a great job making it really digestible.

2187
01:37:23,240 --> 01:37:27,240
Like it was a very, it was a breeze of a read.

2188
01:37:27,240 --> 01:37:32,240
I thoroughly enjoyed it as somebody who writes GPT three prompts and is into this ecosystem.

2189
01:37:32,240 --> 01:37:42,240
It was just, uh, very interesting to see, uh, how it, how it could be laid out, uh, in this broader system approaching this huge problem.

2190
01:37:42,240 --> 01:37:45,240
Um, and also I was able to even get the book for free.

2191
01:37:45,240 --> 01:37:49,240
Obviously I encourage people by the book support it, but it's, it's there.

2192
01:37:49,240 --> 01:37:50,240
It's ready.

2193
01:37:50,240 --> 01:37:52,240
I think, you know, David's goal here is to get the ideas out.

2194
01:37:52,240 --> 01:37:57,240
Um, and so, uh, anyways, so, uh, that's it for today's episode.

2195
01:37:57,240 --> 01:37:58,240
David, thank you so much again.

2196
01:37:58,240 --> 01:37:59,240
I really appreciate you being here.

2197
01:37:59,240 --> 01:38:00,240
Thank you.

2198
01:38:00,240 --> 01:38:03,240
Thank you for all the kind comments and, um, and you're quite welcome.

2199
01:38:03,240 --> 01:38:04,240
And so is everyone else.

2200
01:38:04,240 --> 01:38:06,240
That's why I'm here.

2201
01:38:06,240 --> 01:38:07,240
Awesome.

2202
01:38:07,240 --> 01:38:16,240
And, uh, quick, so my quick plugs, you know, at BAKZT future Twitter, Instagram, YouTube.com slash BAKZT future.

2203
01:38:16,240 --> 01:38:22,240
My newsletter, I'll put it in the description below and I have a Twitter spaces event coming in two days at noon.

2204
01:38:22,240 --> 01:38:24,240
Uh, a couple of people probably pulling up.

2205
01:38:24,240 --> 01:38:26,240
This is like a audio only event.

2206
01:38:26,240 --> 01:38:32,240
So I encourage audio podcast listeners, YouTube subscribers, pull up to the Twitter spaces event.

2207
01:38:32,240 --> 01:38:37,240
We're going to chat more about codecs and prompt design and some other stuff going on in the, in the space.

2208
01:38:37,240 --> 01:38:41,240
So anyways, thank you again for listening to multimodal by BAKZT future.

2209
01:38:41,240 --> 01:38:42,240
I'll catch you in the next one.

2210
01:38:42,240 --> 01:38:43,240
Bye.

