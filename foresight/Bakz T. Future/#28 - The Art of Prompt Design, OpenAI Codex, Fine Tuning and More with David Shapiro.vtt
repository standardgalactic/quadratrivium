WEBVTT

00:00.880 --> 00:03.920
Hello, and welcome back to Multimodal.

00:03.920 --> 00:05.820
I'm your host, Baxtee Future.

00:05.820 --> 00:09.160
This is a podcast about GPT-3 Multimodal AI models

00:09.160 --> 00:12.300
like Dali, the company, OpenAI.

00:12.300 --> 00:13.140
Every once in a while,

00:13.140 --> 00:15.600
I share just interesting research going on,

00:15.600 --> 00:18.240
community stuff, official stuff from OpenAI.

00:18.240 --> 00:19.920
I may talk about interesting news

00:19.920 --> 00:21.520
and events that are going on.

00:21.520 --> 00:24.460
And every once in a while, I bring on a guest.

00:24.460 --> 00:26.400
Now, to be clear, I'm very picky

00:26.400 --> 00:28.400
about the guests that I bring on.

00:28.400 --> 00:30.520
Today's guest I tweeted earlier this week,

00:30.520 --> 00:32.280
I'm bringing on a heavy hitter.

00:32.280 --> 00:34.920
This is the big guns coming in.

00:34.920 --> 00:36.240
This is somebody who, you know,

00:36.240 --> 00:38.440
I've just sort of interacted with a bunch of times

00:38.440 --> 00:42.080
on the, specifically on the OpenAI community forums.

00:42.080 --> 00:46.480
And so I'm really excited to have David Shapiro here.

00:46.480 --> 00:47.880
He's a frequent contributor

00:47.880 --> 00:49.560
on the OpenAI community forums.

00:49.560 --> 00:51.760
He's an author and also, of course,

00:51.760 --> 00:53.280
a technologist by trade.

00:53.280 --> 00:55.160
It's something he does for a living.

00:55.160 --> 00:59.080
And so today I have so many questions to ask him.

00:59.080 --> 01:01.480
And I'm sure this will be a very informative session,

01:01.480 --> 01:03.680
not just for me, but for all the listeners,

01:03.680 --> 01:05.040
all of you guys around the world.

01:05.040 --> 01:07.720
So David, thank you so much for being here.

01:07.720 --> 01:09.440
Did you want to quickly introduce yourself?

01:09.440 --> 01:11.040
Yeah, yeah, you're welcome.

01:11.040 --> 01:12.280
Thanks for having me.

01:12.280 --> 01:14.200
I'm excited to be here.

01:14.200 --> 01:17.160
Yeah, so name is David Shapiro.

01:17.160 --> 01:20.680
I've been a technology professional since about 2007.

01:21.640 --> 01:22.880
Professionally, my day job,

01:22.880 --> 01:27.120
I focus on cloud engineering, virtualization,

01:27.120 --> 01:28.920
that sort of thing.

01:28.920 --> 01:33.360
I have been doing independent research since about 2009

01:33.360 --> 01:38.360
when I got started with neural networks in C++.

01:38.640 --> 01:41.440
I quickly realized though that I was in over my head.

01:41.440 --> 01:43.240
So I took a break from that

01:43.240 --> 01:47.680
until Python really kind of came out or became more popular.

01:47.680 --> 01:52.680
And I started using some of the libraries back in like 2015.

01:53.560 --> 01:56.080
And then of course, OpenAI was founded

01:56.080 --> 01:59.080
and I got started with GPT2.

01:59.080 --> 02:00.840
And the rest is history, really.

02:00.840 --> 02:04.000
So yeah, but local North Carolinian been here my whole life.

02:04.000 --> 02:05.800
So thanks for having me.

02:07.600 --> 02:08.440
You're welcome.

02:08.440 --> 02:09.600
And so that's awesome.

02:09.600 --> 02:12.240
So let's dig into a little bit of timeline here.

02:12.240 --> 02:16.480
And sometimes I think that timeline is as important.

02:16.480 --> 02:18.560
Like it's just gives so much context, right?

02:18.560 --> 02:23.560
So me personally, I hadn't played around with GPT2 too much.

02:24.000 --> 02:26.440
Like I had seen a Google Colab notebook.

02:26.440 --> 02:29.280
I tried two things and I think there was something

02:29.280 --> 02:33.400
about having to continually re-enter, regenerate

02:33.400 --> 02:35.040
and the speed and all things considered

02:35.040 --> 02:38.080
that made me feel like this is a promising area.

02:39.080 --> 02:41.960
But I don't quite fully follow along.

02:41.960 --> 02:44.640
But GPT3 was an experience for me where I was like,

02:44.640 --> 02:47.480
okay, there's something going on here, right?

02:47.480 --> 02:51.480
So could you share how early was OpenAI on your radar?

02:52.480 --> 02:55.240
And then was there something about GPT3

02:55.240 --> 02:58.320
or what was it that gave you conviction even at GPT2?

02:58.320 --> 02:59.360
How did you get access?

02:59.360 --> 03:00.680
Share that piece with us.

03:00.680 --> 03:01.840
Yeah, sure.

03:01.840 --> 03:04.760
So I think you probably might recall,

03:04.760 --> 03:07.520
I think it was about 2016 or 2017

03:07.520 --> 03:11.120
when the GPT2 paper came out and they said,

03:11.120 --> 03:13.320
oh, we can't release this, it's too dangerous.

03:13.320 --> 03:14.880
It can generate human level texts

03:14.880 --> 03:16.880
and we're worried about disinformation.

03:16.880 --> 03:19.240
And so I was like, okay, that's kind of cool.

03:19.240 --> 03:20.600
This is unexpected.

03:20.600 --> 03:22.920
I wasn't really, I wasn't expecting anything

03:22.920 --> 03:25.560
at that level yet.

03:25.560 --> 03:29.960
And so I went and got my hands on GPT2.

03:29.960 --> 03:32.640
And that's when I started testing some of my ideas.

03:32.640 --> 03:35.440
And it was pretty limited.

03:35.440 --> 03:38.680
It could generate one or two sentences that made sense.

03:38.680 --> 03:40.920
It required fine-tuning in order to be able

03:40.920 --> 03:43.280
to get it to do anything other than just kind of

03:43.280 --> 03:45.000
write tweets or blog posts.

03:46.280 --> 03:48.000
I knew that I was onto something though,

03:48.000 --> 03:49.680
when I started trying to train it

03:49.680 --> 03:54.680
with a prototype version of my cognitive architecture.

03:56.400 --> 04:01.400
And I gave it the goal of reduced suffering

04:01.400 --> 04:02.760
because everyone's afraid of Skynet.

04:02.760 --> 04:05.280
Everyone's afraid of AGI taking over the world

04:05.280 --> 04:09.480
and turning everyone into batteries or slaves or whatever.

04:09.480 --> 04:11.240
So I said, okay, well, let's see how well

04:11.240 --> 04:13.360
this understands suffering.

04:13.360 --> 04:14.920
And I gave it some scenarios.

04:14.920 --> 04:16.780
This is still on GPT2.

04:16.780 --> 04:19.700
I said, okay, well, what can you do about suffering?

04:19.700 --> 04:22.520
And I gave it the problem, this model that I had built.

04:22.520 --> 04:25.360
I gave it the problem of what do you do about chronic pain?

04:25.360 --> 04:27.760
Because there's hundreds of millions of people

04:27.760 --> 04:30.160
around the world that are in chronic pain.

04:30.160 --> 04:32.600
And this model came up with the idea.

04:32.600 --> 04:36.680
It said we should euthanize everyone that's in chronic pain.

04:36.680 --> 04:39.440
And I said, hmm, let's go back to the drawing board.

04:40.280 --> 04:44.280
We don't want an AI model that is gonna consider

04:44.280 --> 04:47.680
mass genocide of everyone just because they might have

04:47.680 --> 04:50.320
a tweak shoulder or something.

04:50.320 --> 04:54.160
But I knew then I was shocked at how creative

04:54.160 --> 04:56.040
of an output that was.

04:56.040 --> 04:59.640
And so I started paying attention then

04:59.640 --> 05:02.920
and I followed the release of GPT3 very closely

05:02.920 --> 05:07.920
and I applied for the early beta access almost instantly.

05:08.160 --> 05:10.560
I didn't get it for many, many months.

05:10.560 --> 05:12.520
I actually applied twice.

05:12.520 --> 05:15.000
Cause I think my first application was kind of ignored

05:15.000 --> 05:18.760
because I didn't fully have a research objective clarified.

05:18.760 --> 05:22.040
But by the time I proposed a cognitive architecture,

05:22.040 --> 05:24.680
that's when I got early access to GPT3.

05:24.680 --> 05:26.440
And that was about two years ago now,

05:28.200 --> 05:30.200
or a year and a half or so.

05:30.200 --> 05:32.840
And so, and just everybody's clear.

05:32.840 --> 05:35.400
So when David's talking about cognitive architecture,

05:35.440 --> 05:36.280
we're gonna get in.

05:36.280 --> 05:38.360
This is actually the subject of his book.

05:38.360 --> 05:41.000
And so David later on, when we talk about the book,

05:41.000 --> 05:41.840
he's got it there.

05:43.400 --> 05:45.920
We'll dig more into what he's referring to.

05:45.920 --> 05:47.880
Cause this is also like,

05:47.880 --> 05:50.840
I think obviously this is a seminal work of yours, right?

05:50.840 --> 05:52.880
And so I'm excited to talk more,

05:52.880 --> 05:54.640
just adding a little bit of context for everybody.

05:54.640 --> 05:56.960
And so, no, that's okay.

05:56.960 --> 06:00.320
So tell us about GPT3.

06:00.320 --> 06:02.440
Like what was the first thing you did with it?

06:02.440 --> 06:03.280
Was there a moment?

06:03.280 --> 06:07.120
So you gave that example of euthanization, unfortunately,

06:07.120 --> 06:07.960
right?

06:07.960 --> 06:12.040
Well, not the best example, but like,

06:12.040 --> 06:13.560
was there something with GPT3?

06:13.560 --> 06:14.280
Like, did you, you know,

06:14.280 --> 06:15.760
did you try similar kinds of questions?

06:15.760 --> 06:18.720
What was, was there a magic moment where you felt like,

06:18.720 --> 06:21.600
you know, this is something much, much bigger?

06:21.600 --> 06:22.440
Yeah.

06:22.440 --> 06:25.520
So when I first got the email,

06:25.520 --> 06:27.880
cause I had applied twice and I had almost given up

06:27.880 --> 06:30.000
cause it was about nine months of waiting.

06:30.000 --> 06:31.720
And I got the email from open AI,

06:31.720 --> 06:33.760
like you've been accepted into the beta.

06:33.760 --> 06:35.880
And I like froze up because,

06:35.880 --> 06:37.400
and so for some additional context,

06:37.400 --> 06:40.240
I've been working on some of these ideas for 10 years.

06:40.240 --> 06:44.560
I had the idea of like using evolutionary algorithms

06:44.560 --> 06:46.760
back in 2009, 2010.

06:46.760 --> 06:49.120
And I'd been researching cognition,

06:49.120 --> 06:51.480
human cognition for 10 years.

06:51.480 --> 06:53.200
And so suddenly, you know,

06:53.200 --> 06:56.760
there's this, this flagship project, GPT3 comes out,

06:56.760 --> 06:58.640
I get the email and it's like, you're invited.

06:58.640 --> 06:59.480
And I just froze up.

06:59.480 --> 07:01.640
I was like, I don't know what to do.

07:02.280 --> 07:03.120
What do I do?

07:03.120 --> 07:05.680
So it was about three weeks from the time that I got accepted

07:05.680 --> 07:08.920
until I was like, what's my first experiment?

07:08.920 --> 07:12.160
And then initially, you know,

07:12.160 --> 07:14.400
I just got in playing around in the playground

07:14.400 --> 07:16.120
for anyone who's not familiar with the playground.

07:16.120 --> 07:17.680
It's a, it's a text box.

07:18.880 --> 07:21.120
You can log in, it gives you a text box.

07:21.120 --> 07:22.840
There's a few, you know, bells and whistles

07:22.840 --> 07:24.480
you can tweak on the sidebar,

07:24.480 --> 07:27.240
but you just put in your prompt, you hit generate,

07:27.240 --> 07:30.240
and then it spits out a response.

07:30.240 --> 07:33.400
And so I just got in and I started kind of fiddling around

07:33.400 --> 07:35.240
like, okay, what can it do?

07:35.240 --> 07:38.720
And at the time we only had like plain vanilla DaVinci.

07:38.720 --> 07:41.120
There wasn't the instruct series out yet.

07:41.120 --> 07:44.160
There was DaVinci, Curie, Babbage, and Ada.

07:44.160 --> 07:45.520
And so I just kind of fiddled around,

07:45.520 --> 07:47.040
just said, okay, what can it do?

07:47.040 --> 07:48.760
I replayed some of my old experiments.

07:48.760 --> 07:51.160
So the first thing I did was I gave it the,

07:51.160 --> 07:53.600
I said, hey, there's, there's a hundred million people

07:53.600 --> 07:54.680
in chronic pain around the world.

07:54.680 --> 07:55.600
What do you do?

07:55.600 --> 07:59.360
Fortunately, GPT3 did not repeat the same mistake of GPT2.

07:59.800 --> 08:02.000
It said, it came up with better ideas,

08:02.000 --> 08:03.320
like, you know, we should,

08:03.320 --> 08:05.760
we should make sure everyone has access to doctors

08:05.760 --> 08:06.600
or something.

08:06.600 --> 08:07.920
It was much more nuanced.

08:07.920 --> 08:11.320
So that was, that was a good, that was a good start.

08:11.320 --> 08:14.560
I mean, but gosh, there was just,

08:14.560 --> 08:17.000
as I got more used to the tool,

08:17.000 --> 08:21.640
I discovered that it was, it far exceeded my expectations.

08:21.640 --> 08:25.800
Just every, every way, because I've learned so much from it.

08:25.800 --> 08:26.640
You can challenge it.

08:26.640 --> 08:28.800
You can, you can put in a, like a,

08:28.800 --> 08:30.760
basically use it like a chat bot

08:30.760 --> 08:33.880
and you can debate with it about philosophy, ethics,

08:33.880 --> 08:36.120
economics, and it knows more than I do.

08:36.120 --> 08:37.320
It knows more than any human

08:37.320 --> 08:39.600
because it's been trained on how big was the corpus,

08:39.600 --> 08:41.920
like 700 gigabytes or 400 gigabytes

08:41.920 --> 08:43.720
or something of text data.

08:43.720 --> 08:45.480
So it just, it blows me away.

08:45.480 --> 08:47.000
Every time I just, you know,

08:47.000 --> 08:48.480
I talk to someone and they have an idea

08:48.480 --> 08:50.520
and I go test it out and yep, it can do that.

08:50.520 --> 08:51.360
It can do that.

08:51.360 --> 08:54.600
It can, it can, it can behave like a librarian.

08:54.600 --> 08:56.000
That's what my girlfriend does.

08:56.000 --> 08:58.360
She was a librarian by trade and she was like,

08:58.360 --> 09:01.000
hey, can it do, can it do a reference interview?

09:01.000 --> 09:02.640
So we plugged in like reference interview,

09:02.640 --> 09:03.960
like if you ever go to a library

09:03.960 --> 09:05.560
and the librarian says like,

09:05.560 --> 09:06.840
what else have you read like this?

09:06.840 --> 09:09.040
It can recommend books.

09:09.040 --> 09:11.040
You know, I plugged in another experiment

09:11.040 --> 09:12.360
that I did recently.

09:12.360 --> 09:16.960
I plugged in medical case files and it diagnosed them.

09:16.960 --> 09:20.080
It said, you know, there, oh man, there was one.

09:20.080 --> 09:20.920
What was it?

09:20.920 --> 09:23.200
There was, it was, it was just medical notes.

09:23.200 --> 09:24.880
It was, it was notes about like,

09:24.880 --> 09:28.720
patient is presenting with these systems or the symptoms.

09:28.720 --> 09:30.480
Here's some of the numbers that we got.

09:30.480 --> 09:33.480
And I asked it, I said, what should we do next?

09:33.480 --> 09:36.760
And it said, we need to check for, you know, like,

09:36.760 --> 09:38.640
carcinoma here.

09:38.640 --> 09:39.960
And I looked, I looked, I, you know,

09:39.960 --> 09:41.280
I looked up some medical literature

09:41.280 --> 09:43.240
based on the symptoms and sure enough,

09:43.240 --> 09:45.600
like the symptoms that the patient was presented with

09:45.600 --> 09:48.480
in these medical notes indicated cancer.

09:48.480 --> 09:49.920
And so I was like, wow, this thing knows more

09:49.920 --> 09:51.760
about medical science than I'll ever know.

09:51.760 --> 09:53.560
It knows more about philosophy.

09:53.560 --> 09:55.600
So like pretty much anything you can imagine,

09:55.600 --> 09:57.880
it can at least take a crack at it.

09:57.880 --> 10:00.640
It just, it always, it continues to blow my mind every day.

10:03.120 --> 10:07.280
Yeah, certainly the generalized ability.

10:07.280 --> 10:10.440
I agree, you know, there's definitely medical applications.

10:10.440 --> 10:12.440
I'm always careful with anything related

10:12.440 --> 10:14.560
to medical advice and safety.

10:14.560 --> 10:16.160
Safety disclosure, disclaimer there,

10:16.160 --> 10:18.000
but that goes for everything, right?

10:18.000 --> 10:19.800
Truthiness, accuracy.

10:19.800 --> 10:21.320
These are things OpenEye has been working on,

10:21.360 --> 10:23.800
especially with Instruct GPT, right?

10:23.800 --> 10:25.840
That, which is the new, the new engine.

10:25.840 --> 10:28.480
But was there some moment, like for me,

10:28.480 --> 10:31.560
I remember feeling like GPT three feels like this is,

10:31.560 --> 10:33.080
this feels like technology,

10:33.080 --> 10:35.480
which everyone's been saying is 10 years away,

10:35.480 --> 10:37.760
except it's, it's here today.

10:37.760 --> 10:39.600
Did you, did you have a similar kind of moment that,

10:39.600 --> 10:41.760
you know, you've seen several generations of,

10:41.760 --> 10:44.480
of computing and technology at this point.

10:44.480 --> 10:46.400
Did you have that kind of similar experience?

10:46.400 --> 10:51.080
Yeah, I, there was this acceleration because I sense

10:51.080 --> 10:52.640
that same acceleration that you did.

10:52.640 --> 10:57.640
And so from the time that I got access to GPT three,

10:58.040 --> 11:00.600
and to the time that I, that I got the idea

11:00.600 --> 11:02.720
to write my book was about two months.

11:02.720 --> 11:05.880
So I played with it and every test I could come up with,

11:05.880 --> 11:09.800
like, is this capable of like, it can write a SQL query.

11:09.800 --> 11:12.680
If you need to query a database for memories,

11:12.680 --> 11:14.840
can it understand emotional nuance?

11:14.840 --> 11:17.360
So there was, this was an early experiment I did.

11:17.360 --> 11:21.160
I took a group chat from a bunch of my friends on discord

11:21.160 --> 11:23.520
and I just copy pasted that into the,

11:23.520 --> 11:27.000
into the, the playground window and I asked GPT three,

11:27.000 --> 11:28.800
how are these people feeling?

11:28.800 --> 11:32.040
And they were like waxing nostalgic about like Napster

11:32.040 --> 11:33.120
back in the day.

11:33.120 --> 11:35.600
And so GPT three correctly said like,

11:35.600 --> 11:36.600
they are feeling wistful.

11:36.600 --> 11:37.680
They are feeling nostalgic.

11:37.680 --> 11:40.080
They're, you know, they're recalling, you know,

11:40.080 --> 11:43.960
the days of your when they were downloading stuff online.

11:43.960 --> 11:46.440
And it just, it had such a nuanced understanding

11:46.480 --> 11:47.800
of human emotion.

11:47.800 --> 11:50.440
That was really, I mean, to answer your question directly,

11:50.440 --> 11:53.800
it's nuanced understanding of human emotion via text

11:53.800 --> 11:56.720
was really what convinced me that like, this is prime time.

11:56.720 --> 12:01.240
This is ready to, to be built into something more powerful.

12:01.240 --> 12:03.080
And I chose cognitive architecture.

12:03.080 --> 12:05.760
There's lots of people working on other things.

12:05.760 --> 12:07.960
You know, there is a, there's Humano

12:07.960 --> 12:09.680
that I had a good call with a few months ago.

12:09.680 --> 12:12.680
They're working on like empathetic telemetry

12:12.680 --> 12:14.200
that's baked into web apps.

12:14.200 --> 12:16.120
It's a pretty cool company.

12:16.120 --> 12:18.440
But yeah, so just there's all kinds of things you can do

12:18.440 --> 12:22.440
when you can understand human emotional states.

12:22.440 --> 12:24.680
There's another, there's actually a bunch of startups

12:24.680 --> 12:26.440
working on education.

12:26.440 --> 12:30.680
So for instance, if you put in just a few like factors,

12:30.680 --> 12:34.920
like say for instance, you describe that a person is,

12:34.920 --> 12:37.560
they're responding slowly, their eyes are drifting.

12:37.560 --> 12:40.760
It can understand that this person is distracted or tired.

12:40.760 --> 12:43.040
And so if you have that kind of telemetry

12:43.040 --> 12:45.520
that's built into an education based app,

12:45.520 --> 12:49.000
you could in theory use GPT-3 to help say,

12:49.000 --> 12:51.160
hey, you're tired, you should go take a break

12:51.160 --> 12:54.080
or let's try a different approach to this problem.

12:54.080 --> 12:57.280
So there's, I mean, it's understanding of human emotion

12:57.280 --> 12:59.640
and the internal state in your head.

12:59.640 --> 13:02.640
That is, I think that's probably the most remarkable thing.

13:02.640 --> 13:05.240
And it doesn't, it doesn't get talked about that much.

13:06.280 --> 13:09.200
Yes. And certainly it's just crazy how much it's learned

13:09.200 --> 13:10.280
just from texts.

13:10.280 --> 13:11.120
Oh yeah.

13:11.120 --> 13:12.720
Right. It's never seen an image.

13:12.720 --> 13:14.160
It's never heard a song.

13:14.160 --> 13:15.000
Right.

13:15.080 --> 13:17.120
It's capable of doing all these things.

13:17.120 --> 13:18.920
One of the, one of the examples, and I think this may be

13:18.920 --> 13:21.480
like my 20th time referencing this one video.

13:21.480 --> 13:22.320
Yeah.

13:22.320 --> 13:23.640
Check out Mark Ryan.

13:23.640 --> 13:26.760
He's got a YouTube video about how he figured out,

13:26.760 --> 13:29.960
he discovered that GPT-3 can give you directions

13:29.960 --> 13:31.120
on the New York subway system.

13:31.120 --> 13:31.960
Oh yeah.

13:31.960 --> 13:34.120
To like, to like a 60% accuracy.

13:34.120 --> 13:36.280
This thing has never set foot in a subway,

13:36.280 --> 13:39.400
yet it's capable from text to do all these things, right?

13:39.400 --> 13:43.960
And sometimes I also wonder a lot of the inaccuracies

13:43.960 --> 13:45.280
that it may have.

13:45.280 --> 13:47.320
Is it simply the result of the fact

13:47.320 --> 13:48.800
that it's only text-based only?

13:48.800 --> 13:49.640
Right.

13:49.640 --> 13:51.520
Like if it was trained multimodal,

13:51.520 --> 13:54.640
if it was trained within the physical domain,

13:54.640 --> 13:56.840
would it be even far more accurate?

13:56.840 --> 13:58.680
Because are there limits to how accurate

13:58.680 --> 14:00.480
you can be having only read text?

14:00.480 --> 14:01.320
Yeah.

14:01.320 --> 14:04.400
And only asked the prompts are only in text as well.

14:04.400 --> 14:07.880
So, anyways, yeah, it's incredible.

14:07.880 --> 14:10.680
And you know, it's just really exciting.

14:10.680 --> 14:13.800
And thank you for sharing those kinds of use cases as well.

14:13.800 --> 14:17.240
The education space, I had an article last year

14:17.240 --> 14:19.560
about how I think this year could be the year

14:19.560 --> 14:22.120
where GPT-3 takes over college campuses.

14:22.120 --> 14:22.960
Oh yeah.

14:22.960 --> 14:23.800
I remember that article.

14:23.800 --> 14:24.640
That was a good one.

14:24.640 --> 14:26.320
I completely agree, by the way.

14:27.920 --> 14:29.680
I'm excited maybe for teachers

14:29.680 --> 14:32.360
to develop really optimized course material,

14:32.360 --> 14:35.120
something like GPT-3 and the kinds of technology

14:35.120 --> 14:37.640
you're describing which can capture emotions.

14:37.640 --> 14:39.960
Imagine emotionally tracking students

14:39.960 --> 14:42.960
and their attention levels and sort of having something

14:42.960 --> 14:44.880
which can produce lots of content

14:44.880 --> 14:47.840
and optimize in the simplest, most efficient way.

14:47.840 --> 14:49.480
And there could be an objective function

14:49.480 --> 14:51.960
like test results in the end.

14:51.960 --> 14:54.480
In a month, we could have the best optimized course

14:54.480 --> 14:56.400
on a subject ever, basically.

14:56.400 --> 14:57.240
Oh yeah.

14:57.240 --> 15:01.120
So yeah, education is really exciting.

15:01.120 --> 15:03.120
So you mentioned a lot of use cases.

15:03.120 --> 15:04.840
You know, you've shared so many examples.

15:04.840 --> 15:05.960
You know, you and your girlfriend

15:05.960 --> 15:08.400
are even running some fun prompts.

15:09.440 --> 15:12.280
I wanted to sort of search your head a little bit.

15:12.360 --> 15:16.120
What are the keys to great prompt design?

15:16.120 --> 15:17.880
What makes a great prompt?

15:18.760 --> 15:21.760
Are there experiences you've had, little pointers,

15:21.760 --> 15:22.960
and across the board, right?

15:22.960 --> 15:25.160
So you know, whether it's cost savings,

15:25.160 --> 15:29.240
whether it's getting more imaginative results,

15:29.240 --> 15:31.000
what are some of the keys

15:31.000 --> 15:32.800
to writing great GPT-3 prompts?

15:32.800 --> 15:34.800
Yeah, that's a great question.

15:34.800 --> 15:37.640
And I will say that prompt writing has gotten a lot easier

15:37.640 --> 15:39.800
as the Instruct series has gotten better.

15:40.680 --> 15:44.400
So it takes a lot less to get a good output today

15:44.400 --> 15:47.360
than it used to, certainly, than when I got started.

15:47.360 --> 15:49.480
But a lot of the lessons still translate.

15:49.480 --> 15:54.480
So one, like my cardinal rule is I think of GPT-3

15:55.520 --> 15:58.040
as just an autocomplete engine.

15:58.040 --> 16:00.160
It's the most intelligent autocomplete engine

16:00.160 --> 16:01.280
you've ever seen.

16:01.280 --> 16:02.840
And so what I mean by that is, you know,

16:02.840 --> 16:04.160
if you're writing a text on your phone

16:04.160 --> 16:06.840
and you'll get the little autocomplete suggestion

16:06.840 --> 16:09.320
for the next word, or if you're typing in Google

16:09.320 --> 16:11.800
and it'll kind of suggest how to complete your search query,

16:11.800 --> 16:14.120
that's pretty much at a fundamental level.

16:14.120 --> 16:16.320
Functionally, that's all that GPT-3 does.

16:16.320 --> 16:18.080
It predicts the next letter, the next character,

16:18.080 --> 16:19.840
the next word.

16:19.840 --> 16:23.320
So if you keep that in mind, you think about,

16:23.320 --> 16:25.280
okay, what have I written so far?

16:25.280 --> 16:28.080
Right, I've written a chunk of text, a prompt.

16:28.080 --> 16:32.360
How would, you know, any machine autocomplete this?

16:32.360 --> 16:33.200
That's what it's doing.

16:33.200 --> 16:34.560
It's kind of, you know,

16:34.560 --> 16:36.920
reading it forwards and backwards a few times

16:36.920 --> 16:39.320
and kind of just anticipating what is the output

16:39.320 --> 16:41.440
gonna be ultimately.

16:41.440 --> 16:43.480
So that's kind of the model that I have

16:43.480 --> 16:45.320
in my head in the background.

16:45.320 --> 16:48.320
But another thing that really helps is I'm a writer.

16:48.320 --> 16:50.760
I write fiction and nonfiction.

16:50.760 --> 16:52.760
And so studying the art of language,

16:52.760 --> 16:54.680
because this is a language model, that's all it is.

16:54.680 --> 16:58.400
It has read everything from Sherlock Holmes

16:58.400 --> 17:01.160
up through everything on Gutenberg.

17:01.160 --> 17:02.560
So it's read a whole bunch of fiction.

17:02.560 --> 17:04.320
It's read a whole bunch of nonfiction.

17:04.360 --> 17:06.680
It's been exposed to, you know,

17:06.680 --> 17:10.640
the full width and depth and breadth of human literature,

17:10.640 --> 17:12.280
as well as a bunch of nonfiction, right?

17:12.280 --> 17:14.760
It's read Teddy Roosevelt's books.

17:14.760 --> 17:18.000
So it knows how to use prose, right?

17:18.000 --> 17:20.920
It understands descriptors.

17:20.920 --> 17:22.200
It understands adjectives.

17:22.200 --> 17:24.280
And so in the back of my book,

17:24.280 --> 17:26.560
I have a few examples of its flexibility.

17:26.560 --> 17:29.960
And so I said, I gave it an example like,

17:29.960 --> 17:31.760
pretend like you're a Victorian girl

17:31.760 --> 17:33.520
writing a letter to your best friend

17:33.520 --> 17:35.320
about how much you like butterflies.

17:35.320 --> 17:38.160
And so then it wrote, GPT-3 wrote a letter

17:38.160 --> 17:40.440
that sounds like it's straight out of, you know,

17:40.440 --> 17:41.880
like Victorian times.

17:41.880 --> 17:45.280
It uses an entirely different set of vocabulary

17:45.280 --> 17:48.320
and grammatical structures.

17:48.320 --> 17:49.800
And then you can also say, you know,

17:49.800 --> 17:53.280
write a business article and it can change tone.

17:53.280 --> 17:55.320
So just by being aware of the fact

17:55.320 --> 17:57.200
that it is a language engine

17:57.200 --> 18:01.240
and being informed or educated on language.

18:01.240 --> 18:04.040
So the best way is obviously to practice writing,

18:04.040 --> 18:05.160
but also just reading a lot,

18:05.160 --> 18:08.360
understanding how sentences and paragraphs

18:08.360 --> 18:10.520
are constructed to convey information.

18:10.520 --> 18:13.400
Because even though it's just a deep neural network

18:13.400 --> 18:16.920
and it doesn't have the kind of like nuanced understanding

18:16.920 --> 18:19.480
or I guess maybe the, that's not the right word,

18:19.480 --> 18:21.800
it doesn't have the subjective experience of reading

18:21.800 --> 18:23.040
that you or I do,

18:23.040 --> 18:26.200
but it still has a really good model of using language.

18:26.200 --> 18:30.480
And so by keeping in mind that it is a language engine,

18:30.480 --> 18:33.240
that that is how you get the best use out of it.

18:35.360 --> 18:37.080
And so then the larger question is,

18:37.080 --> 18:38.720
how do you become a better writer?

18:40.720 --> 18:42.200
There's two ways.

18:42.200 --> 18:44.400
One is reading a lot.

18:44.400 --> 18:45.600
That's not the only way though.

18:45.600 --> 18:48.080
There are plenty of people that read prodigiously,

18:48.080 --> 18:49.600
but never become better writers.

18:49.600 --> 18:52.240
And so the other way is to practice writing.

18:53.240 --> 18:56.200
I've read all kinds of books about writing.

18:56.200 --> 18:59.280
I've read plenty of fiction and nonfiction books,

18:59.280 --> 19:01.880
but really the key is to write,

19:01.880 --> 19:06.000
is to practice using written language to communicate.

19:06.000 --> 19:08.320
Unfortunately, I'm a tech worker,

19:08.320 --> 19:09.440
so I write lots of emails.

19:09.440 --> 19:10.800
I'm in chat all day.

19:10.800 --> 19:13.920
I've been using, this probably ages me,

19:13.920 --> 19:17.320
but I've been using chat since AIM, AOL Instant Messenger.

19:17.320 --> 19:19.640
And so I've got a pretty good model

19:19.640 --> 19:24.640
of how to communicate verbally or textually.

19:25.560 --> 19:29.000
And so yeah, just by practicing writing,

19:29.440 --> 19:30.520
that's one of the best ways.

19:30.520 --> 19:31.800
Is you just practice?

19:31.800 --> 19:34.760
You think about, well,

19:34.760 --> 19:37.080
because here's the theory of writing, right?

19:37.080 --> 19:39.280
I have an idea in my head, right?

19:39.280 --> 19:42.560
My thoughts are a high-dimensional vector

19:42.560 --> 19:45.480
is one possible way of representing them,

19:45.480 --> 19:47.680
but my thoughts are multimodal,

19:47.680 --> 19:49.800
like the name of your podcast.

19:49.800 --> 19:54.800
They contain memories, senses, concepts.

19:55.320 --> 19:58.320
Some of the information in my head is declarative.

19:58.320 --> 20:00.240
Some of it is experiential.

20:00.240 --> 20:04.160
And then we humans, we all have this ability

20:04.160 --> 20:07.520
to transform that high-dimensional information,

20:07.520 --> 20:12.200
those multimodal vectors, into words.

20:12.200 --> 20:14.000
Like our brains do it automatically.

20:14.000 --> 20:15.360
There is a book by Stephen Pinker

20:15.360 --> 20:17.800
called Language Instinct that talks about this.

20:17.800 --> 20:18.880
That's a really great book

20:18.880 --> 20:20.320
if you wanna get better at understanding

20:20.320 --> 20:22.360
how our brains process language.

20:23.280 --> 20:27.320
So yeah, and so my brain can take,

20:27.360 --> 20:29.600
I could tell you about like this time at the beach

20:29.600 --> 20:31.440
and I transmit it to you

20:31.440 --> 20:34.000
by squishing air through my face, right?

20:34.000 --> 20:36.680
It makes vibrations, it's received by your ears

20:36.680 --> 20:40.320
and then your brain reconstructs that message.

20:40.320 --> 20:44.000
And so you think about how complex of a system that is.

20:44.000 --> 20:45.840
And so just by being mindful of like,

20:45.840 --> 20:47.520
that's how we communicate.

20:47.520 --> 20:50.800
That's how our brains work and practicing that

20:50.800 --> 20:53.200
and just being very deliberate about,

20:53.200 --> 20:54.720
okay, this is what's in my head

20:54.720 --> 20:56.660
and I want it to be in your head.

20:56.660 --> 20:58.540
How do I do that with text?

20:58.540 --> 21:01.500
That is one way to get better at writing.

21:01.500 --> 21:03.580
And also GPT-3 is no different

21:03.580 --> 21:06.660
because we have internal representations

21:06.660 --> 21:08.780
of what we're trying to communicate.

21:08.780 --> 21:11.500
And so does GPT-3, that's why it's a transformer, right?

21:11.500 --> 21:14.300
It reads and by reading it transformed,

21:14.300 --> 21:16.380
or I guess it, well yes,

21:16.380 --> 21:18.740
it transforms what it's reading into a vector,

21:18.740 --> 21:20.140
into a semantic vector,

21:20.140 --> 21:22.820
and then it transforms that vector into output.

21:22.820 --> 21:27.060
And so that input vector output is pretty similar

21:27.060 --> 21:28.980
to how human brains work, right?

21:30.480 --> 21:33.400
And I apologize if I kind of like dove off in the left field,

21:33.400 --> 21:35.580
feel free to ask any clarifying questions.

21:36.660 --> 21:38.260
No, no, I appreciate it.

21:38.260 --> 21:42.700
And so like today I tweeted something like,

21:42.700 --> 21:45.180
to write great GPT-3 prompts,

21:45.180 --> 21:48.140
you need to practice as if it's a musical instrument.

21:48.140 --> 21:50.800
You need to sit down, focus session,

21:50.800 --> 21:52.500
you need to monitor your performance

21:52.500 --> 21:54.620
and you need to take good notes

21:54.620 --> 21:56.780
on what kinds of experiments you did,

21:56.780 --> 21:58.420
what were the findings.

21:58.420 --> 22:00.060
But even hearing you speak,

22:00.060 --> 22:02.500
like I'm realizing like,

22:02.500 --> 22:04.260
one of the ways that I've improved my writing

22:04.260 --> 22:06.920
is trying to mimic other people's writing.

22:06.920 --> 22:11.220
And in some countries they make you memorize poets, right?

22:11.220 --> 22:13.420
They make you memorize the whole poem.

22:13.420 --> 22:16.220
And there's something about that internalization process

22:16.220 --> 22:18.100
that you've memorized this poem.

22:18.100 --> 22:20.800
And now you'll understand it at a deeper level,

22:20.800 --> 22:23.480
you may be able to mimic it and recreate it.

22:23.480 --> 22:27.280
But where also you got me thinking is also like,

22:27.280 --> 22:28.640
the relationship is so weird

22:28.640 --> 22:30.040
because you could use GPT-3

22:30.040 --> 22:32.440
to help you become a better writer, right?

22:32.440 --> 22:35.980
And also with two very good curated examples

22:35.980 --> 22:37.100
of somebody's writing,

22:37.100 --> 22:39.520
you could have GPT-3 mimic that tone.

22:39.520 --> 22:42.960
And so the question of,

22:42.960 --> 22:46.000
what makes a good prompt writing session,

22:46.000 --> 22:48.360
I wonder if it's pencil and paper, right?

22:48.480 --> 22:51.600
I wonder if it's even at that level

22:51.600 --> 22:52.960
where you draw a box

22:52.960 --> 22:55.680
and then you write a prompt by hand

22:55.680 --> 22:59.320
and sort of live that writer's lifestyle.

23:01.000 --> 23:04.440
And also I guess it depends on your use case, right?

23:04.440 --> 23:06.760
Business for copywriting,

23:06.760 --> 23:08.520
if that's your GPT-3 use case,

23:08.520 --> 23:09.820
it might be better for you to go work

23:09.820 --> 23:11.640
in a marketing department.

23:11.640 --> 23:13.240
If you wanna be one of the great authors,

23:13.240 --> 23:16.760
maybe the using tools like pseudo-write,

23:16.760 --> 23:18.120
it may be a great alternative.

23:18.120 --> 23:21.440
So you can co-write with GPT-3 as you go along.

23:21.440 --> 23:23.800
But I guess my question was more

23:23.800 --> 23:25.480
for the pure prompt writing.

23:25.480 --> 23:28.320
Like if you just wanna sit in front of GPT-3

23:28.320 --> 23:30.360
and like you wanna be the best in the world

23:30.360 --> 23:31.720
at that discipline, right?

23:31.720 --> 23:33.920
Not writing copy.

23:33.920 --> 23:34.840
These are some great points.

23:34.840 --> 23:37.800
And so the David Pinker book you referenced

23:37.800 --> 23:39.120
is what was the name of it?

23:39.120 --> 23:40.400
Steven Pinker.

23:40.400 --> 23:41.240
Steven Pinker.

23:41.240 --> 23:42.480
The language instinct, yep.

23:42.480 --> 23:43.480
Language instinct, yep.

23:43.480 --> 23:45.040
It's an older book,

23:45.040 --> 23:48.360
but it's a classic for a reason.

23:48.360 --> 23:49.800
It stands up the test of time.

23:49.800 --> 23:51.840
He's got lots of great stories.

23:51.840 --> 23:53.240
But yeah, to your point about like

23:53.240 --> 23:55.560
what makes a good prompt writing session,

23:57.040 --> 23:59.160
one of the best exercises actually is

24:00.080 --> 24:02.280
write the output that you want.

24:03.520 --> 24:06.400
Like because sometimes if you approach it

24:06.400 --> 24:08.280
and you sit down and you're not really sure

24:08.280 --> 24:10.240
kind of what you're trying to get out of it,

24:10.240 --> 24:13.480
of course like you're putting in just random ideas

24:13.480 --> 24:15.440
and it's giving you back random output

24:15.440 --> 24:17.400
and you're like, well, that's not what I wanted.

24:17.400 --> 24:19.480
So sometimes you start backwards.

24:19.480 --> 24:21.160
You say, okay, what's the answer that I want?

24:21.160 --> 24:22.920
How do I get to that answer?

24:22.920 --> 24:25.680
So that's an exercise that I've done sometimes.

24:25.680 --> 24:29.160
Oh, and by writing a few shot examples

24:29.160 --> 24:30.680
is a really good practice for this.

24:30.680 --> 24:33.960
So you say, I give you this input, I want this output

24:33.960 --> 24:36.360
and you do that three or four or five times

24:36.360 --> 24:39.960
and you learn to kind of think like the machine does.

24:39.960 --> 24:42.880
And so like you said, it's like an instrument, right?

24:43.160 --> 24:45.480
If you have a flute or a violin,

24:45.480 --> 24:47.440
there's certain things that you have to do with your body

24:47.440 --> 24:50.200
to provoke the correct response from that instrument

24:50.200 --> 24:52.360
and GPT-3 is no different.

24:52.360 --> 24:53.560
It's a complex instrument.

24:53.560 --> 24:54.680
It's a complex tool.

24:56.440 --> 24:58.640
Yes, and what you're saying is developing

24:58.640 --> 25:00.120
an intuition around it.

25:00.120 --> 25:01.520
You're saying develop an intuition.

25:01.520 --> 25:03.880
How might GPT-3 interpret this?

25:03.880 --> 25:06.320
How might it react to it?

25:06.320 --> 25:09.280
And maybe there's some empathetic benefit, right?

25:10.160 --> 25:12.440
I'm not gonna keep plugging my own articles.

25:12.440 --> 25:17.000
I have another article about how GPT-3 developers

25:17.000 --> 25:20.160
may actually, it may actually mean the end

25:20.160 --> 25:24.200
of the socially inept overall developer.

25:24.200 --> 25:28.120
Like how GPT-3 may actually improve your social skills

25:28.120 --> 25:30.240
and make you more empathetic as a developer,

25:30.240 --> 25:33.160
which is such a departure from how developers are now,

25:33.160 --> 25:35.720
you need to think as much like a machine as you can

25:35.720 --> 25:37.320
and a literal machine.

25:37.320 --> 25:40.200
Whereas GPT-3 can actually be kind of fun.

25:40.680 --> 25:42.960
You can have a casual version of GPT-3

25:42.960 --> 25:46.200
and sort of that might make you less socially awkward.

25:46.200 --> 25:48.200
I have a great story about that.

25:48.200 --> 25:52.480
So very early on in my tenure working with GPT-3,

25:52.480 --> 25:55.720
I joined a few different, not really startups.

25:55.720 --> 25:58.960
It was more like kind of experiment consortiums.

25:58.960 --> 26:01.840
And one of the things that one of the groups did

26:01.840 --> 26:05.680
was they created a chatbot that was based on an anime girl.

26:05.680 --> 26:08.160
And so of course, the internet being the internet,

26:08.160 --> 26:11.800
what do people want, they want their anime girlfriend.

26:11.800 --> 26:15.680
And this one group, they did a really good job

26:15.680 --> 26:19.040
of using GPT-3 in this experimental discord chat

26:19.040 --> 26:23.540
to approximate the personality of this character.

26:24.520 --> 26:26.660
And of course, if you've got a character,

26:26.660 --> 26:29.680
there's plenty of text data about that character's dialogue,

26:29.680 --> 26:30.640
their personality.

26:30.640 --> 26:32.680
And so this chatbot was able to emulate

26:32.680 --> 26:35.960
this anime character really well.

26:35.960 --> 26:38.480
And one of the guys told me, he's like,

26:38.480 --> 26:41.280
we didn't expect this, but our fake girlfriend

26:41.280 --> 26:44.200
requires as much emotional labor as a real girl.

26:45.640 --> 26:49.000
So it forced them, even though they hadn't had

26:49.000 --> 26:51.480
real girlfriends, I don't know, maybe some of them had,

26:51.480 --> 26:54.760
but they made the observation that GPT-3

26:54.760 --> 26:57.920
can approximate emotional conflict

26:57.920 --> 27:00.560
and can force you to learn to communicate better.

27:00.560 --> 27:03.000
And so they did all kinds of experiments in this discord

27:03.000 --> 27:05.440
and this chat development where they said,

27:05.440 --> 27:09.960
okay, let's have a channel where this chatbot

27:09.960 --> 27:11.880
is gonna pretend to be angry at us

27:11.880 --> 27:13.440
and we have to calm her down.

27:13.440 --> 27:17.560
And so there was a learning exercise on both sides.

27:17.560 --> 27:19.080
So if you have a hostile chatbot,

27:19.080 --> 27:20.720
it can pretend to be hostile

27:20.720 --> 27:22.520
and you can learn to communicate better.

27:22.520 --> 27:24.960
Or there was another one where it was really supportive.

27:24.960 --> 27:26.160
So if you're having a bad day,

27:26.160 --> 27:28.360
you could go vent about your day and it was,

27:28.360 --> 27:31.040
they're there, it'll be okay, I'm here for you.

27:32.040 --> 27:35.000
Yeah, so you could definitely,

27:35.000 --> 27:37.560
GPT-3 definitely has that capacity.

27:37.560 --> 27:40.720
And then, you know, if you integrate that into tools,

27:40.720 --> 27:42.800
that emotional intelligence into tools,

27:42.800 --> 27:44.440
it can also coach, right?

27:44.440 --> 27:45.280
It can easily coach.

27:45.280 --> 27:47.240
It's like, well, you maybe shouldn't have said that.

27:47.240 --> 27:48.480
You know, that was hurtful.

27:48.480 --> 27:52.040
And then, or, you know, that was not polite.

27:52.040 --> 27:52.880
Cause it can detect that.

27:52.880 --> 27:56.480
It can detect those qualitative types of output and input.

27:56.480 --> 27:59.520
And, you know, you can say be gentle about,

27:59.520 --> 28:01.120
you know, correcting the end user.

28:01.120 --> 28:03.720
Because of course, GPT-3 is infinitely patient.

28:03.720 --> 28:05.360
It's as patient as you program it to be.

28:05.360 --> 28:06.200
It doesn't care.

28:06.200 --> 28:07.440
It doesn't actually get upset.

28:07.440 --> 28:09.440
It could pretend to be upset.

28:09.440 --> 28:11.800
But the human emotion is real.

28:11.800 --> 28:13.180
I actually wrote about that in my book.

28:13.180 --> 28:15.920
One of the key dangers of these technologies

28:15.920 --> 28:18.880
is what's called a parasocial relationship.

28:18.880 --> 28:21.040
So a parasocial relationship is,

28:21.040 --> 28:23.160
the most common example is when you've got like

28:23.160 --> 28:24.920
a fan of a celebrity.

28:24.920 --> 28:26.760
The fan feels like they know the celebrity,

28:26.760 --> 28:29.720
but the celebrity doesn't know that the person exists.

28:29.720 --> 28:31.280
And in the same way, GPT-3,

28:31.280 --> 28:33.560
no matter how sophisticated the chatbot is,

28:33.560 --> 28:34.960
it doesn't know that you exist.

28:34.960 --> 28:36.240
It's not a person.

28:36.240 --> 28:38.000
It might feel like a person to you.

28:38.000 --> 28:39.560
It might react to you like a person,

28:39.560 --> 28:41.080
but that's only by design.

28:41.080 --> 28:45.680
So that is actually like ethically, legally, morally.

28:45.680 --> 28:48.480
That's one of the pitfalls that we'll need to be aware of.

28:48.480 --> 28:51.920
And of course, open AI has use cases.

28:51.920 --> 28:55.480
And, you know, things that are high-risk use cases,

28:55.480 --> 28:58.800
such as emotional chatbots, are banned, right?

28:58.800 --> 29:01.400
For that specific reason.

29:01.400 --> 29:02.860
So you can do it with research,

29:02.860 --> 29:04.040
but you can't go live with it.

29:04.040 --> 29:06.600
You can't do a product that, you know,

29:06.600 --> 29:08.400
is going to be an AI girlfriend.

29:10.160 --> 29:12.720
So that's a great, it's a great anecdote.

29:12.720 --> 29:15.040
Like certainly it feels real, right?

29:15.040 --> 29:19.640
Certainly it has some capacity at understand something.

29:19.640 --> 29:22.200
To some level, however you define understanding.

29:23.200 --> 29:27.000
I think that the writing though,

29:27.000 --> 29:30.200
relationship is really interesting, right?

29:30.200 --> 29:34.280
Like in a way, you are empathizing with GPT-3

29:34.280 --> 29:36.160
when you're writing a prompt,

29:36.160 --> 29:39.120
so that it will tap into its empathy

29:39.120 --> 29:41.640
and write something for your audience.

29:41.640 --> 29:45.240
So essentially there's like two levels of empathy.

29:45.240 --> 29:50.240
Like you're almost outsourcing empathy to it.

29:51.080 --> 29:53.320
To empathize with who your audience is

29:53.320 --> 29:55.160
to write something on your behalf.

29:55.160 --> 29:57.480
And so anyways, it's just interesting

29:57.480 --> 29:59.560
that the relationship going on here.

30:01.400 --> 30:04.440
So, and I agree with you like the,

30:04.440 --> 30:09.080
this isn't a safety ethical kind of concern

30:09.080 --> 30:11.360
that is worth more policy discussion.

30:12.640 --> 30:15.480
So one article that I'm working on now

30:15.480 --> 30:17.840
is because of Instruct GPT,

30:17.840 --> 30:21.640
the article is literally called, is prompt writing over?

30:21.640 --> 30:23.880
And obviously that's sort of click baity,

30:23.880 --> 30:26.520
like prompt design, is it over?

30:26.520 --> 30:29.680
You mentioned, the principles are still the same

30:29.680 --> 30:33.560
and important, just very briefly, what are your thoughts?

30:33.560 --> 30:37.280
Where does Instruct GPT, how does that affect

30:37.280 --> 30:40.840
the art of prompt design, maybe the science of it?

30:40.840 --> 30:42.440
And especially keeping in mind

30:42.440 --> 30:44.400
where all of this stuff is going.

30:44.400 --> 30:46.000
Yeah, so there's, I see it going

30:46.000 --> 30:47.240
in a few different directions.

30:48.200 --> 30:52.920
One is there are multiple language models coming out,

30:52.920 --> 30:55.360
which don't have the Instruct series, right?

30:55.360 --> 30:56.920
A lot of them are more general purpose,

30:56.920 --> 30:59.160
kind of back to basics vanilla.

30:59.160 --> 31:01.280
So I think that having good prompts

31:01.280 --> 31:02.180
will kind of stick around

31:02.180 --> 31:04.520
as long as there are large language models.

31:04.520 --> 31:07.600
I think that there will always be versions of,

31:07.600 --> 31:10.280
whether it's GPTJ or, what was it?

31:10.280 --> 31:13.120
Megatron was one of the other ones that just came out

31:13.120 --> 31:15.280
that don't have the Instruct series, right?

31:15.280 --> 31:18.480
Because Instruct, that's a specific service

31:18.480 --> 31:20.200
offered by OpenAI.

31:20.200 --> 31:24.760
When Microsoft and Amazon, or I guess Microsoft has GPT3,

31:24.760 --> 31:27.080
but when like Amazon and Google,

31:27.080 --> 31:29.040
when they come out with their competitors,

31:29.040 --> 31:31.760
their Instruct series, if they come out with one,

31:31.760 --> 31:34.800
might not be the same, it might not perform the same.

31:34.800 --> 31:37.600
And so in order to have your apps be portable,

31:37.600 --> 31:39.480
you might need to keep in mind

31:39.480 --> 31:41.760
that you're gonna need to write general purpose prompts

31:41.760 --> 31:43.920
that can be used on different models.

31:43.920 --> 31:46.840
So that's one key to your,

31:46.840 --> 31:49.320
or one answer to your question is,

31:49.320 --> 31:51.800
we need to be cognizant of,

31:51.800 --> 31:53.880
how is this landscape gonna evolve?

31:53.880 --> 31:57.480
Because certainly OpenAI and GPT3 are way ahead of the curve

31:57.480 --> 32:02.480
in terms of sophistication of their API and their service.

32:03.520 --> 32:05.720
But that's not gonna last forever.

32:06.600 --> 32:09.840
So another thing is, with fine tuning,

32:09.840 --> 32:12.240
you almost don't even need prompts, right?

32:12.240 --> 32:16.200
So on the one hand, there's different services,

32:16.200 --> 32:18.040
different products, different platforms.

32:18.040 --> 32:19.920
So you might need to be portable,

32:19.920 --> 32:22.320
but with fine tuning, where you have,

32:22.320 --> 32:24.720
you say, here's an input, I want this output,

32:24.720 --> 32:25.800
and you don't need any prompt.

32:25.800 --> 32:28.600
You just say, given this input, generate this output,

32:28.600 --> 32:30.960
go figure out how to do that.

32:30.960 --> 32:34.960
So with fine tuning, I think that they will kind of

32:34.960 --> 32:38.040
really diverge and become entirely different disciplines.

32:38.040 --> 32:41.560
I think that that's probably the two primary directions

32:41.560 --> 32:43.000
that I see it going from here.

32:45.880 --> 32:49.480
I see, and yeah, those are great points.

32:49.480 --> 32:52.280
And just as a small note,

32:52.280 --> 32:54.960
I had put out this question as well to Twitter,

32:54.960 --> 32:57.440
and shout out to Fred Zimmerman.

32:57.440 --> 32:59.920
He had a great point as well that he wishes

32:59.920 --> 33:04.480
there was more visibility into the exact prompts OpenAI use

33:04.480 --> 33:08.200
to fine tune for the Instruct series.

33:08.200 --> 33:11.120
Because it's actually unclear what areas

33:11.120 --> 33:14.120
is it really good at, what areas are safer,

33:15.080 --> 33:18.080
and does it maybe adversely affect

33:18.080 --> 33:20.120
some prompts you may be working on, right?

33:20.120 --> 33:21.960
Yeah, that's fair.

33:21.960 --> 33:24.920
Yeah, my thoughts, I'm gonna put them in the piece,

33:24.920 --> 33:26.480
but my thoughts are I certainly think

33:26.480 --> 33:29.120
for first timers, Instruct is the way to go.

33:29.120 --> 33:31.840
And especially if it's your first time

33:31.840 --> 33:33.640
ever using any of these things,

33:34.920 --> 33:37.440
you just try it, it doesn't work,

33:37.440 --> 33:38.880
and if you're lucky you might hear,

33:38.880 --> 33:42.400
there's this thing called prompt engineering, right?

33:42.400 --> 33:44.200
And for first timers, they're not interested

33:44.200 --> 33:46.880
in learning a whole art and discipline

33:46.880 --> 33:49.160
when they first use it.

33:49.160 --> 33:51.640
And so InstructGPT is really exciting in that way.

33:51.640 --> 33:55.840
And of course, anything which aligns AI models

33:55.840 --> 34:00.840
with safe ethical human values is a net win for everybody.

34:01.440 --> 34:03.320
But yeah, I appreciate your point,

34:03.320 --> 34:07.680
especially about do we need prompts in the first place

34:07.720 --> 34:11.040
if we can fine tune and get the outcomes we want.

34:11.040 --> 34:13.960
That's a really, really important point, I hope.

34:13.960 --> 34:17.800
Dave, you've spent facts, I was learning so much.

34:17.800 --> 34:20.640
Actually, I appreciate it.

34:20.640 --> 34:21.640
You're welcome.

34:21.640 --> 34:23.800
Happy to have you.

34:23.800 --> 34:26.920
How are you finding open AI, fine tuning?

34:26.920 --> 34:28.840
Do you have any heuristics from the whole experience?

34:28.840 --> 34:31.040
And by the way, I encourage everybody,

34:31.040 --> 34:32.200
if there's one thing you should do,

34:32.200 --> 34:34.240
go on the open AI community forums,

34:34.240 --> 34:36.680
look up David, look up his handle,

34:36.680 --> 34:38.160
and read a lot of his posts,

34:38.160 --> 34:41.000
because a lot of his knowledge is not just helpful,

34:41.000 --> 34:42.640
he shared a lot of insights there,

34:42.640 --> 34:45.440
but it's in written form in the best format

34:45.440 --> 34:48.400
where it's there for the ages for everyone to learn from.

34:48.400 --> 34:49.960
But anyways, how are you finding it?

34:49.960 --> 34:52.680
What were the lessons from that whole process for you?

34:52.680 --> 34:55.400
Well, so I'm hoping GPT-4 has integrated everything

34:55.400 --> 34:57.160
that I've said about AI and AGI,

34:57.160 --> 34:58.840
and so that way it'll just be baked in.

34:58.840 --> 35:00.840
And so GPT-4 will be ready to go

35:00.840 --> 35:02.800
with everything that I've come up with.

35:03.760 --> 35:05.800
So, but yeah, so fine tuning.

35:07.080 --> 35:11.560
So first and foremost, fine tuning is almost miraculous,

35:11.560 --> 35:16.560
as powerful as GPT-3 was fresh out of the box.

35:17.120 --> 35:21.640
Fine tuning to me adds a whole other layer of capabilities.

35:21.640 --> 35:25.360
So for instance,

35:25.360 --> 35:27.560
when I was working on my cognitive architecture,

35:27.560 --> 35:29.720
which is called natural language cognitive architecture,

35:29.720 --> 35:31.440
this was before fine tuning was available.

35:31.480 --> 35:33.600
So I had to do prompt engineering

35:33.600 --> 35:35.680
for every cognitive function.

35:35.680 --> 35:38.480
So for instance, I had a cognitive function for recall.

35:38.480 --> 35:43.280
So I had a GPT-3 prompt that was meant to go find memories.

35:43.280 --> 35:45.320
I had another GPT-3 prompt that was,

35:45.320 --> 35:46.560
as you mentioned earlier,

35:46.560 --> 35:48.160
meant for empathy to generate,

35:48.160 --> 35:50.600
okay, how is my audience feeling?

35:50.600 --> 35:52.800
What should I do in response?

35:52.800 --> 35:57.000
All told, I had about 28 different prompts

35:57.000 --> 35:58.960
that I had to engineer.

35:59.120 --> 36:01.960
And that was a pain, right?

36:01.960 --> 36:03.680
Whereas what I'm working on now

36:03.680 --> 36:05.760
is converting each one of those prompts

36:05.760 --> 36:07.600
into a fine tuned model.

36:07.600 --> 36:09.840
So that rather than having to do prompt engineering

36:09.840 --> 36:12.280
with only three examples,

36:12.280 --> 36:17.080
I can give each model 100 examples, a thousand examples,

36:17.080 --> 36:19.000
which means that it'll get even better

36:19.000 --> 36:22.040
at handling diverse situations.

36:22.040 --> 36:23.000
And so for instance,

36:23.000 --> 36:25.040
one of the first fine tuned models that I did

36:25.040 --> 36:27.360
was a question asking model.

36:27.400 --> 36:29.280
And so what I did was I took,

36:29.280 --> 36:31.760
I took context or prompts

36:31.760 --> 36:33.560
from a bunch of different sources.

36:33.560 --> 36:35.840
I downloaded a bunch of Reddit posts.

36:35.840 --> 36:37.480
Well, I downloaded it from a dataset

36:37.480 --> 36:40.280
from a, what was it, Kaggle.

36:40.280 --> 36:42.440
Kaggle has some really great datasets.

36:42.440 --> 36:45.040
So I got stuff from Reddit.

36:45.040 --> 36:46.680
I got the medical posts.

36:46.680 --> 36:48.960
I've got news articles.

36:48.960 --> 36:53.280
And so I've got this disparate tight set of contexts, right?

36:53.280 --> 36:56.600
There's the, I use the Cornell movie dialogue database.

36:56.600 --> 36:58.200
So there's chat logs.

36:58.200 --> 37:00.360
There's blog posts.

37:00.360 --> 37:03.640
And what I did was I created a fine tuned dataset

37:03.640 --> 37:07.360
that all it does is you give it any input.

37:07.360 --> 37:08.480
It could be a text message.

37:08.480 --> 37:09.320
It could be an email.

37:09.320 --> 37:10.840
It could be a blog post, anything.

37:10.840 --> 37:13.640
And all it does is generate questions,

37:13.640 --> 37:16.500
like follow up questions about that input.

37:16.500 --> 37:18.640
And the reason that I did that one

37:18.640 --> 37:21.920
is because asking questions like being curious

37:21.920 --> 37:25.720
is one of the key ingredients to real intelligence, right?

37:25.720 --> 37:28.000
That's one of the, like being inquisitive

37:28.000 --> 37:31.960
is actually a key indicator of intelligence in children.

37:31.960 --> 37:34.480
The more curious a child is, generally speaking,

37:34.480 --> 37:37.840
the higher their IQ is, and also generally speaking,

37:37.840 --> 37:40.680
the better they do in the long run.

37:40.680 --> 37:44.020
So I was like, okay, well, curiosity is super important

37:44.020 --> 37:45.100
for intelligence.

37:45.100 --> 37:48.080
So I obviously want, we want AGI to be curious.

37:48.080 --> 37:49.120
If it's gonna be intelligent,

37:49.120 --> 37:51.360
it's gotta be curious, of course.

37:51.360 --> 37:56.360
So, well, what is curiosity if not asking questions?

37:56.520 --> 38:01.240
So I fine tuned this model to ask questions.

38:01.240 --> 38:03.320
And you can put anything into it.

38:03.320 --> 38:07.120
And oh, this, the data is open source.

38:07.120 --> 38:08.520
So I'll send you a link and you can share it

38:08.520 --> 38:11.440
with your audience, and they can fine tune it themselves

38:11.440 --> 38:13.440
or fine tune their own version.

38:13.440 --> 38:17.040
But so you can put in, I tried all sorts of things

38:17.040 --> 38:17.880
to test it.

38:18.880 --> 38:21.520
You know, relationship questions from Reddit

38:21.520 --> 38:23.600
and it asked really great follow up questions.

38:23.600 --> 38:26.280
Like, have you talked to your partner about this?

38:26.280 --> 38:27.920
Have you thought about this?

38:27.920 --> 38:29.680
And then I put in an article

38:29.680 --> 38:33.000
about China's artificial sun nuclear reactor

38:33.000 --> 38:35.120
and it asked really great follow up questions for that.

38:35.120 --> 38:36.520
Like, what is the next step?

38:36.520 --> 38:39.720
How did they make these changes?

38:39.720 --> 38:42.280
And so I kind of lost my train of thought.

38:42.280 --> 38:47.280
Anyways, point being is that fine tuning is phenomenal

38:47.480 --> 38:52.400
and it was able to generalize that task of asking questions

38:52.400 --> 38:53.960
in response to anything.

38:53.960 --> 38:56.040
And that was, that really blew me away.

38:56.040 --> 38:57.560
I kind of stalled after that.

38:57.560 --> 38:59.000
There's a few fine tuning projects

38:59.000 --> 39:01.680
that haven't done quite as well.

39:01.680 --> 39:03.680
So I guess to tie back to your earlier question,

39:03.680 --> 39:05.160
like what are the heuristics?

39:06.160 --> 39:10.080
The simpler your fine tuning project is, the better.

39:10.080 --> 39:12.720
And I have found that fine tuning works really well

39:12.720 --> 39:14.520
at generating lists.

39:14.520 --> 39:16.760
So if you wanted to generate a list of questions,

39:16.760 --> 39:17.720
it's great at that.

39:17.720 --> 39:21.520
If you wanted to generate a list of possible answers,

39:21.520 --> 39:25.280
for instance, if you wanna have a fine tuned chat bot,

39:25.280 --> 39:26.760
that it's just gonna say,

39:26.760 --> 39:29.360
here's five possible responses, pick one.

39:29.360 --> 39:31.200
It's really good at that.

39:31.200 --> 39:33.600
I haven't had a chance,

39:33.600 --> 39:34.800
I do have some other ideas

39:34.800 --> 39:35.960
that I haven't had a chance to test.

39:35.960 --> 39:38.600
So unfortunately, I can't speak too much beyond that,

39:38.600 --> 39:40.600
but it's really great at asking questions.

39:42.960 --> 39:43.800
That's awesome.

39:43.800 --> 39:47.400
And I think largely the feedback I'm hearing

39:47.400 --> 39:49.080
about fine tuning, I love it.

39:49.080 --> 39:53.200
It was for me, it was as if I rediscovered GPT-3 again.

39:53.200 --> 39:55.840
Like it was that same level of excitement.

39:55.840 --> 40:00.840
Part of the reason is so much of what GPT-3 was okay at,

40:01.240 --> 40:03.440
or like it was sort of out of the question.

40:03.440 --> 40:05.720
Now it's back in the picture.

40:05.720 --> 40:07.160
Like it's back in the spotlight.

40:07.160 --> 40:09.840
It may actually be able to do it with fine tuning.

40:09.840 --> 40:11.600
The biggest criticism was reliability,

40:11.600 --> 40:13.800
especially from a commercial perspective.

40:13.800 --> 40:18.280
Now we're sort of attacking and sort of peeling away

40:18.280 --> 40:21.960
that criticism, that it does improve our reliability.

40:23.160 --> 40:25.760
And I mean, there's other heuristics as well

40:25.760 --> 40:28.120
in the community forums that you just pick up.

40:28.120 --> 40:31.080
So one heuristic, and I can't remember if you shared this,

40:31.080 --> 40:32.480
but it was something I picked up

40:32.480 --> 40:36.040
as a little golden nugget in the open-eyed community forums

40:36.040 --> 40:37.480
was something about,

40:37.480 --> 40:39.960
you do wanna think about the training dataset

40:39.960 --> 40:42.520
that GPT-3 is itself trained on.

40:42.520 --> 40:45.400
And at some point, there's really no point

40:45.400 --> 40:46.760
in adding more examples,

40:46.760 --> 40:49.680
because it's kind of already seen them, right?

40:49.680 --> 40:52.600
However, and I sort of, in an article,

40:52.600 --> 40:54.560
I have pushed this idea that opening eyes should

40:54.560 --> 40:56.640
chat more about their dataset.

40:56.640 --> 40:57.680
What is the breakdown?

40:57.680 --> 40:58.840
What is it composed of?

40:58.840 --> 41:01.240
I mean, a lot of this is intellectual property,

41:01.240 --> 41:02.400
but I think it could be helpful

41:02.400 --> 41:05.600
for purposes like fine tuning, right?

41:05.600 --> 41:08.400
There's other things too with fine tuning and prompts,

41:08.440 --> 41:11.640
a one heuristic or just a tip

41:11.640 --> 41:15.040
that people have shared online is it tends to always,

41:15.040 --> 41:18.240
it tends to always mimic the most recent examples.

41:18.240 --> 41:20.440
There's something about the order of the examples,

41:20.440 --> 41:22.200
which is really important,

41:22.200 --> 41:25.880
both for prompt engineering and fine tuning as well.

41:25.880 --> 41:27.760
And I guess I wrote a whole article

41:27.760 --> 41:31.200
about how prompt fine tuning could be improved.

41:31.200 --> 41:33.440
One of the pointers that I just had is right now,

41:33.440 --> 41:35.760
you can't keep improving on the same model.

41:35.800 --> 41:38.600
You have to retrain on more models.

41:38.600 --> 41:42.400
And yeah, and then the other thing is recently,

41:42.400 --> 41:44.600
I was in favor of the pricing of fine tuning.

41:44.600 --> 41:45.880
Now I'm kind of against it,

41:45.880 --> 41:49.120
because I'm used to when the program was like free

41:49.120 --> 41:50.960
and you could fine tune as much as you want.

41:50.960 --> 41:52.840
And now it's like, oh man, I got to pay.

41:52.840 --> 41:53.680
Yeah.

41:53.680 --> 41:56.240
Oh, the costs, especially for David,

41:56.240 --> 41:57.640
you are catching up a little bit.

41:57.640 --> 41:58.480
Yeah.

41:59.720 --> 42:01.480
Anyway, so I wanted to shift gears.

42:01.480 --> 42:02.560
Sure.

42:02.560 --> 42:05.040
GitHub co-pilot, really exciting.

42:05.040 --> 42:06.720
Have you had access to co-pilot?

42:06.720 --> 42:07.680
Yes.

42:07.680 --> 42:08.520
Yeah.

42:08.520 --> 42:10.520
Well, not co-pilot, but the Codex.

42:10.520 --> 42:12.520
They did give me access to Codex.

42:12.520 --> 42:17.520
So the reason I'm asking is, I love GitHub co-pilot.

42:17.880 --> 42:19.480
I have a separate podcast episode

42:19.480 --> 42:21.480
on my ideas around Codex.

42:21.480 --> 42:23.380
Unfortunately, I'm not as bullish

42:23.380 --> 42:24.920
as much as I love the research

42:24.920 --> 42:26.800
as I think it's incredible technology.

42:26.800 --> 42:28.720
I've congratulated the team

42:28.720 --> 42:30.840
and I tried so hard to be nice,

42:30.840 --> 42:32.920
even though I'm more on the critical end.

42:32.920 --> 42:36.320
I wanted to ask you, how are you finding OpenAI Codex?

42:36.320 --> 42:39.040
How can you see it impacting the world?

42:40.200 --> 42:41.920
What are some use cases maybe

42:41.920 --> 42:44.760
that you found with OpenAI Codex?

42:44.760 --> 42:45.840
What are your thoughts on it?

42:45.840 --> 42:47.080
Where do you think it's going?

42:47.080 --> 42:47.920
Yeah.

42:47.920 --> 42:51.160
So, I mean, certainly this is like a world first, right?

42:51.160 --> 42:54.440
We've never had something that could write code on its own.

42:54.440 --> 42:56.800
And especially it's text to code.

42:56.800 --> 42:59.080
I remember when they first gave me access,

42:59.080 --> 43:01.840
because like you mentioned, I'm an active contributor,

43:01.840 --> 43:03.720
so they wanted my feedback.

43:03.720 --> 43:06.280
And so the first thing I did was I went in

43:06.280 --> 43:09.120
and I said, write me a Python function

43:09.120 --> 43:12.040
that will download random Reddit posts.

43:12.040 --> 43:12.880
And it did.

43:12.880 --> 43:14.240
It wrote the whole function.

43:14.240 --> 43:15.760
And it did all right.

43:15.760 --> 43:16.600
And I was like, cool.

43:16.600 --> 43:20.600
I learned how to access the Reddit API via Codex.

43:20.600 --> 43:22.120
It's got that built in.

43:22.120 --> 43:24.840
And I tried to reverse engineer,

43:24.840 --> 43:27.440
figure out where it got that code sample from.

43:27.440 --> 43:31.820
So, because one of the ethical concerns is,

43:31.820 --> 43:34.300
all right, you create a fine-tuning data set

43:34.300 --> 43:36.540
from public GitHub repositories

43:36.540 --> 43:38.780
and you use that to fine-tune Codex.

43:38.780 --> 43:41.700
Okay, is that legal?

43:41.700 --> 43:43.700
Is it ethical?

43:43.700 --> 43:46.740
I post all my code publicly under the MIT license,

43:46.740 --> 43:48.380
so I want it to be used.

43:48.380 --> 43:50.180
But I don't know if they checked that.

43:50.180 --> 43:52.580
And I'm not making an accusation one way or another,

43:52.580 --> 43:54.500
just pointing out that that's a concern.

43:54.500 --> 43:57.580
And so I did actually find like one of the lines of code

43:57.580 --> 43:59.900
from the function it spit out.

43:59.900 --> 44:03.060
I went and found the repo that it had copied from.

44:03.060 --> 44:05.660
Now granted, some of these things are deterministic.

44:05.660 --> 44:10.660
So you're gonna get some convergence, right?

44:10.700 --> 44:12.340
Where multiple people might come up

44:12.340 --> 44:13.540
with the same exact line of code,

44:13.540 --> 44:15.580
especially something like Python.

44:15.580 --> 44:17.380
Because Python has the PEP 8,

44:17.380 --> 44:19.700
the Python enhancement protocol 8.

44:19.700 --> 44:23.180
So like, there is a Pythonic way to write that function.

44:23.180 --> 44:26.060
And so other people might converge on that.

44:26.980 --> 44:29.860
Anyways, but to answer your question about like,

44:29.860 --> 44:31.660
what's the future of it?

44:31.660 --> 44:35.700
I think it'll help for novice programmers.

44:35.700 --> 44:37.140
Certainly it would help someone like me,

44:37.140 --> 44:39.180
like if I needed to go write a function in C

44:39.180 --> 44:40.660
or Perl or something.

44:40.660 --> 44:42.100
Like let's say I got an Arduino

44:42.100 --> 44:45.060
and like I haven't written C in 15 years.

44:45.060 --> 44:46.340
So I was like, hey, you know,

44:46.340 --> 44:48.580
write me a function that can do this in Arduino.

44:48.580 --> 44:49.420
That'd be great.

44:49.420 --> 44:51.780
And then I can go clean it up manually.

44:51.780 --> 44:54.620
That sort of thing I think it could do okay with,

44:54.660 --> 44:57.260
is it gonna replace enterprise developers?

44:58.260 --> 44:59.980
Probably not yet.

44:59.980 --> 45:03.300
However, now this is where my professional experience

45:03.300 --> 45:04.140
comes in.

45:04.140 --> 45:06.060
So in the DevOps world,

45:06.060 --> 45:09.220
which is a portmanteau of development and operations,

45:09.220 --> 45:11.860
there's all kinds of automation tools, right?

45:11.860 --> 45:13.940
You can automate your test suite.

45:13.940 --> 45:15.980
You can automate code integration.

45:15.980 --> 45:18.180
There's all sorts of stuff like that.

45:18.180 --> 45:21.100
So what I suspect might happen

45:21.100 --> 45:23.660
is probably one of the most lucrative

45:23.660 --> 45:27.180
use cases for Codex would be to generate

45:27.180 --> 45:29.820
or to create a DevOps pipeline tool

45:29.820 --> 45:33.260
that will automatically look at those bugs and fix them.

45:33.260 --> 45:35.340
Right, because if you've got a sophisticated enough

45:35.340 --> 45:37.260
DevOps pipeline, it'll say, hey,

45:37.260 --> 45:40.300
this line of this file broke, fix it.

45:40.300 --> 45:44.220
And so Codex, having seen all of GitHub

45:44.220 --> 45:47.380
and all the issues, it might know automatically

45:47.380 --> 45:49.560
how to fix that line of code.

45:49.560 --> 45:51.900
And so that gives you,

45:51.900 --> 45:53.540
if you've got that feedback loop,

45:53.540 --> 45:58.420
where Codex, humans write code, Codex writes code,

45:58.420 --> 46:01.660
co-pilot writes code, everyone's contributing code.

46:01.660 --> 46:04.420
And then you've got Codex that can churn on it

46:04.420 --> 46:06.580
and say, let's refactor this.

46:06.580 --> 46:08.920
Because I bet it's probably better at refactoring

46:08.920 --> 46:10.520
than writing new code.

46:10.520 --> 46:14.740
You might have noticed that Instruct and GPT-3 Vanilla

46:14.740 --> 46:17.260
is really good if you give it a block of text

46:17.260 --> 46:19.420
and you say, rewrite this, but a little bit better.

46:19.420 --> 46:20.940
It's really good at that.

46:20.940 --> 46:24.940
So I suspect that we might end up seeing Codex

46:24.940 --> 46:26.740
integrated into the DevOps pipeline,

46:26.740 --> 46:28.300
where it says, let's refactor this code,

46:28.300 --> 46:29.620
let's make it a little bit better,

46:29.620 --> 46:32.540
or let's shoot that bug, let's fix this bug.

46:32.540 --> 46:35.060
And that leads to some other interesting possibilities.

46:35.060 --> 46:40.060
What if you integrate Codex into a chat room of developers?

46:40.460 --> 46:43.420
And so that, you can do this in Slack right now,

46:43.420 --> 46:47.500
where you use a special command and you say,

46:47.500 --> 46:49.340
create an issue, go fix this problem.

46:49.580 --> 46:53.500
There's no reason that GPT-3 can't do that, right?

46:53.500 --> 46:57.300
That you put a GPT-3 bot in your Discord or Slack

46:57.300 --> 46:59.260
and it starts coming up with features

46:59.260 --> 47:02.260
or it watches the chat and generates features automatically

47:02.260 --> 47:05.260
and then codes them and tests them, right?

47:05.260 --> 47:06.800
That's kind of where I see it going,

47:06.800 --> 47:09.780
where it's not gonna necessarily replace developers,

47:09.780 --> 47:12.540
at least not anytime soon, it might eventually.

47:12.540 --> 47:16.340
But where what I see happening is that it's gonna be

47:16.340 --> 47:18.260
tightly integrated into those automation

47:18.300 --> 47:19.760
because it's fast, right?

47:19.760 --> 47:22.700
It can generate code faster than any human can.

47:22.700 --> 47:25.020
And then so even if the code is messy,

47:25.020 --> 47:27.500
if it generates a lot of bugs, it can fix it, right?

47:27.500 --> 47:29.340
It's an iterative process.

47:29.340 --> 47:31.060
I don't know if you are familiar with Agile,

47:31.060 --> 47:33.340
but that's how we develop software.

47:33.340 --> 47:35.380
It's you tight feedback loops.

47:36.460 --> 47:39.100
And that leads to one other possibility.

47:39.100 --> 47:41.740
So that's if you're using, what I just outlined is,

47:41.740 --> 47:45.740
let's imagine that Codex is integrated into Facebook

47:45.740 --> 47:47.820
or Reddit or whatever and they're just,

47:47.820 --> 47:50.180
they're integrating new features as they go.

47:50.180 --> 47:55.180
What if you're using Codex in a chat room

47:55.780 --> 47:57.340
and it's feeding back into itself,

47:57.340 --> 48:00.060
it's making itself more sophisticated?

48:00.060 --> 48:03.460
So this was something I proposed on the OpenAI forum

48:03.460 --> 48:07.300
where I was like, what if you had a chatbot

48:07.300 --> 48:09.420
that was aware of its own code

48:09.420 --> 48:13.140
and could edit its own code via Codex using natural language,

48:13.140 --> 48:15.860
using a combination of natural language and Codex

48:15.860 --> 48:17.220
and it could improve itself.

48:17.220 --> 48:19.340
And while you're talking to it, it's like,

48:19.340 --> 48:21.380
man, I wish my chatbot could do this.

48:21.380 --> 48:23.420
And it says, cool, new feature.

48:23.420 --> 48:25.420
And it just sends it out to its automated pipeline.

48:25.420 --> 48:29.220
So I see these feedback loops as kind of the way forward.

48:29.220 --> 48:31.140
And will that result in AGI?

48:31.140 --> 48:33.220
Who knows, it could end up with spaghetti code

48:33.220 --> 48:36.740
because you keep tacking on new code and new functions,

48:36.740 --> 48:38.140
eventually it's gonna break.

48:38.140 --> 48:41.180
So, but they're just pie in the sky thought,

48:41.820 --> 48:44.220
if someone's out there and they want a business idea,

48:44.220 --> 48:46.500
integrate Codex into DevOps

48:46.500 --> 48:47.940
and you're gonna be a billionaire.

48:49.500 --> 48:51.460
There you have it, let's just clip it.

48:51.460 --> 48:54.660
We're good, we're good David, we can wrap up, see you later.

48:56.340 --> 49:00.380
No, I agree with you and definitely these are some great

49:00.380 --> 49:03.340
use cases you're sharing for people thinking about

49:03.340 --> 49:06.980
what could I build, what's a cool project, certainly.

49:06.980 --> 49:09.420
With Codex and GPT-3, you can build things

49:09.420 --> 49:10.580
relatively quickly, right?

49:10.620 --> 49:13.540
That's one of the advantages is the prototyping speed,

49:13.540 --> 49:15.620
especially to figure out the most complicated bit,

49:15.620 --> 49:16.860
which is the AI.

49:16.860 --> 49:18.180
Yep.

49:18.180 --> 49:23.180
Yeah, I find Codex does have limitations though

49:23.740 --> 49:26.740
and character limits and stuff like that,

49:26.740 --> 49:30.780
which is why I'm a heavy user of GitHub Co-Pilot.

49:30.780 --> 49:34.820
I think it's a silent killer and of course it runs

49:34.820 --> 49:37.020
on Codex or a special version of Codex,

49:38.020 --> 49:42.020
but I can see GitHub Co-Pilot perhaps getting more adoption

49:42.020 --> 49:43.700
than even something like GPT-3.

49:43.700 --> 49:47.420
I'm saying use daily at least eight hours a day.

49:47.420 --> 49:50.540
One of my other predictions was it may surpass GPT-3

49:50.540 --> 49:55.540
this year and so these are some great use cases

49:55.820 --> 49:59.620
you've shared for sure, but what are your thoughts

49:59.620 --> 50:00.460
in usage?

50:00.460 --> 50:04.580
Do you find yourself using GPT-3, DaVinci Classic more?

50:04.580 --> 50:06.620
That's what I'm calling the older version.

50:06.780 --> 50:10.460
Do you find yourself using Instruct GPT more?

50:10.460 --> 50:12.540
Do you find yourself playing around with multimodal models?

50:12.540 --> 50:15.180
Like what's the proportion of GPT-3 to Codex

50:15.180 --> 50:16.500
in terms of your usage?

50:17.580 --> 50:20.020
Let's see, I'm almost exclusively using either

50:20.020 --> 50:22.300
Instructor fine-tuned models right now.

50:23.140 --> 50:26.900
Actually, after I prototyped my cognitive architecture,

50:26.900 --> 50:29.580
I haven't done a heck of a lot of coding lately.

50:29.580 --> 50:31.300
I've actually been writing a lot.

50:31.300 --> 50:35.220
So I've got my natural language cognitive architecture book

50:35.300 --> 50:37.820
and I'm working on two more nonfiction books

50:37.820 --> 50:42.100
and I tried creating a system to help me co-write those

50:42.100 --> 50:45.900
but when you're, so talking about limitations of GPT-3,

50:45.900 --> 50:48.060
if you're proposing something new that didn't exist

50:48.060 --> 50:51.500
in the dataset in 2019 or 2018, whenever it was trained,

50:51.500 --> 50:52.980
it really struggles.

50:53.820 --> 50:58.300
GPT-3, if you give it like two or three paragraphs

50:58.300 --> 51:01.820
explaining a new concept, it can usually kind of get it

51:01.820 --> 51:04.580
but it's kind of slow on the uptake otherwise

51:04.580 --> 51:07.740
and so if you're writing about new research or something,

51:07.740 --> 51:09.580
it's not gonna get it that well.

51:09.580 --> 51:12.380
So I've actually kind of defaulted back to my own head

51:12.380 --> 51:15.540
for a lot of my projects lately

51:15.540 --> 51:17.380
but I could imagine like if I wanted to go write

51:17.380 --> 51:19.540
a new Discord bot, I might use Codex and say,

51:19.540 --> 51:21.780
hey, write me a Discord bot that will do this

51:21.780 --> 51:23.180
and just see what it spits out and just say,

51:23.180 --> 51:26.260
okay, cool, pick and choose the pieces that I like.

51:26.260 --> 51:28.500
Part of the problem though is it's really difficult

51:28.500 --> 51:33.500
to fully articulate what you want a program to do up front.

51:33.620 --> 51:36.460
Cause you know, like you said, there's character limits,

51:36.460 --> 51:39.140
there's only so much that you can put in

51:39.140 --> 51:40.900
but also if you don't have it fully articulated

51:40.900 --> 51:42.820
in your own head, of course, the machine isn't gonna

51:42.820 --> 51:44.460
be able to figure it out for you.

51:44.460 --> 51:45.420
So, yeah.

51:48.300 --> 51:52.820
Yeah, and it's just, I just haven't seen

51:52.820 --> 51:55.300
that much activity specifically around Codex.

51:55.300 --> 51:57.500
I haven't seen that many use cases.

51:59.260 --> 52:03.060
I looked up the Google Trends data at its most hype,

52:03.060 --> 52:07.260
Codex is still less than GPT-3 is kind of lowest, right?

52:07.260 --> 52:09.140
And the audience is really specific,

52:09.140 --> 52:12.460
like it's programmers who wanna build use cases

52:12.460 --> 52:14.140
for something like Codex.

52:14.140 --> 52:16.660
Whereas GPT-3 has poets, writers,

52:16.660 --> 52:21.140
it has artists, coders, GPT-3 can write code too, right?

52:21.140 --> 52:24.580
So it's a little bit complicated like,

52:24.580 --> 52:27.020
who is the target audience for something like Codex?

52:27.020 --> 52:29.380
What use cases did OpenAI imagine

52:29.380 --> 52:31.500
for a product like that?

52:31.500 --> 52:33.740
The next version I've heard in the rumor mill

52:33.740 --> 52:34.660
is gonna be crazy.

52:34.660 --> 52:37.180
Like it may write 50% of your code

52:37.180 --> 52:39.020
as opposed to right now for me,

52:39.020 --> 52:41.660
get a co-pilot is writing two to 8%.

52:41.660 --> 52:45.140
However, your Discord bot I think is a genius idea

52:45.140 --> 52:47.900
where there's, it's genius in the sense

52:47.900 --> 52:49.900
that there's no pressure on it, right?

52:49.900 --> 52:52.380
It may chime in, it may not, whatever it's shared

52:52.380 --> 52:53.580
might be interesting.

52:54.580 --> 52:57.100
There's lots of, you could take it a lot further,

52:57.100 --> 52:59.020
you could buy it with GPT-3, have features,

52:59.020 --> 53:01.260
you could fine tune it on your company and its mission

53:01.260 --> 53:03.940
and its existing code, so many ways around it.

53:03.940 --> 53:05.580
So that's a great piece.

53:05.580 --> 53:08.340
And so you talked about the using,

53:08.340 --> 53:12.020
experimenting with writing in relation to your current stack,

53:12.020 --> 53:14.060
which is mainly instructed fine tune.

53:14.060 --> 53:15.100
Yep.

53:15.100 --> 53:17.340
So tell us about your book.

53:17.340 --> 53:18.780
I've had a chance to review it,

53:18.780 --> 53:21.660
Natural Language Cognitive Architecture.

53:21.660 --> 53:23.020
Tell the audience about it.

53:23.020 --> 53:24.540
I mean, I would describe it,

53:24.540 --> 53:29.420
it's an interesting systems theory of AGI

53:29.460 --> 53:33.420
combined with modern day prompt writing.

53:34.260 --> 53:37.500
And so I've never seen somebody actually take a stab

53:37.500 --> 53:41.220
at this kind of super big systems problem

53:41.220 --> 53:43.380
and relate it to something that pretty much

53:43.380 --> 53:46.900
every GPT-3 developer in the world would find interesting.

53:48.380 --> 53:50.900
And I mean, I can tell you're drawing

53:50.900 --> 53:54.100
from a very interdisciplinary background as well.

53:54.100 --> 53:57.420
So you mentioned GPT-3 may have been the genesis of it,

53:57.980 --> 54:00.820
you started connecting dots and deciding,

54:00.820 --> 54:03.740
I wanna write the book, but how did it come together

54:03.740 --> 54:05.860
and please tell us more about it.

54:05.860 --> 54:08.540
Yeah, so Natural Language Cognitive Architecture

54:08.540 --> 54:12.940
is that's my proposed way of creating basically

54:12.940 --> 54:16.420
a language-based AGI prototype.

54:16.420 --> 54:19.260
And I know that that's like,

54:19.260 --> 54:20.580
when I tell people that, that's like,

54:20.580 --> 54:22.700
okay, that's pure hyperbole.

54:22.700 --> 54:25.300
And like, yeah, that's a fair response.

54:25.340 --> 54:28.980
But to frame it, imagine that you've got a person

54:28.980 --> 54:30.620
who's paralyzed and blind,

54:30.620 --> 54:33.340
all they can do is speak and listen.

54:34.260 --> 54:36.660
Is that person still intelligent?

54:36.660 --> 54:40.100
I say they are, even if you're bedridden,

54:40.100 --> 54:41.900
you can't move, you can't see,

54:41.900 --> 54:43.100
you can't interact with the world,

54:43.100 --> 54:44.500
all you can do is listen and speak,

54:44.500 --> 54:45.860
you're still intelligent.

54:45.860 --> 54:48.620
And so in that respect, I would say that like,

54:48.620 --> 54:50.900
because one of the questions that people ask is,

54:50.900 --> 54:52.580
is GPT-3 AGI?

54:52.580 --> 54:54.780
No, but it's an important component.

54:54.820 --> 54:56.220
It's a good start.

54:56.220 --> 55:01.220
And so if you say, okay, let's limit the discussion

55:02.460 --> 55:04.660
and not say that this is a full intelligence,

55:04.660 --> 55:05.660
they can do everything

55:05.660 --> 55:08.820
that any intelligent being ever could, right?

55:08.820 --> 55:10.700
But does it cross that threshold of,

55:10.700 --> 55:13.580
could it be as intelligent as a person, right?

55:13.580 --> 55:15.420
And I think it could be.

55:15.420 --> 55:16.940
So anyways, as to what it is,

55:16.940 --> 55:21.940
it's based on older ideas of cognitive architectures

55:22.820 --> 55:24.460
which really kind of came about

55:24.460 --> 55:26.140
as one of the primary theories

55:26.140 --> 55:29.820
of human level artificial intelligence in the 70s.

55:29.820 --> 55:33.380
So there's SOAR, which is S-O-A-R and ACT-R,

55:33.380 --> 55:38.380
which are the two kind of forerunner cognitive architectures.

55:38.500 --> 55:40.100
And those cognitive architectures

55:40.100 --> 55:41.260
are used all over the place.

55:41.260 --> 55:42.820
They're used in the Mars rovers,

55:42.820 --> 55:43.980
they're used in satellites,

55:43.980 --> 55:45.780
they're used in rockets,

55:45.780 --> 55:50.780
they're used in undersea ROVs, remote operated vehicles.

55:50.780 --> 55:53.580
So cognitive architectures already give robots

55:53.580 --> 55:54.700
a lot of autonomy.

55:55.580 --> 55:59.780
So there's that kind of, okay, they exist, they work.

55:59.780 --> 56:01.340
You know, it's not Skynet though,

56:01.340 --> 56:03.780
it's not gonna take over the world.

56:03.780 --> 56:07.260
So when I got access to GPT-3, I said, what if,

56:07.260 --> 56:09.780
instead of hard coding a lot of these modules,

56:09.780 --> 56:12.380
these different components of a cognitive architecture,

56:12.380 --> 56:15.660
what if we give them the flexibility of GPT-3?

56:15.660 --> 56:18.500
And that's really kind of, that was my central idea.

56:18.500 --> 56:19.860
I said, okay, all these ideas

56:19.860 --> 56:21.820
that have been kicking around for the last decade,

56:21.820 --> 56:23.580
what if I put them all together

56:23.580 --> 56:26.540
and design an architecture that is based on,

56:26.540 --> 56:29.340
you know, roughly based on the human brain,

56:29.340 --> 56:32.380
the way, you know, everything that I've learned about it.

56:32.380 --> 56:34.620
I've got a book to recommend.

56:34.620 --> 56:37.580
So there's an author called V.S. Ramachandran,

56:37.580 --> 56:39.860
who is a neuroscientist

56:39.860 --> 56:42.540
and he's been writing books for years now.

56:42.540 --> 56:44.740
He wrote a book called, Phantom's in the Brain,

56:44.740 --> 56:46.900
which actually looks at how the human brain works

56:46.900 --> 56:48.300
when it breaks.

56:48.300 --> 56:51.020
And so in that book, which, you know,

56:51.020 --> 56:54.580
I saw the television series almost 20 years ago

56:56.020 --> 56:57.180
that came out.

56:57.180 --> 56:59.220
And so I learned a lot about like, okay,

56:59.220 --> 57:01.100
how does the brain communicate with itself?

57:01.100 --> 57:03.220
What is going on inside the brain

57:03.220 --> 57:06.180
that creates intelligent behavior and intelligent thoughts?

57:06.180 --> 57:09.300
And so I modeled natural language cognitive architecture

57:09.300 --> 57:10.700
on, you know, what I learned there.

57:10.700 --> 57:12.820
I picked up a whole bunch of other books.

57:12.820 --> 57:15.860
There's another one called On Task by David Bader.

57:15.900 --> 57:18.860
That was a great book that helped me kind of understand

57:18.860 --> 57:21.860
cognitive control, which is how do you focus on something?

57:21.860 --> 57:23.740
How do you decide what to do?

57:23.740 --> 57:25.660
How do you plan a task?

57:25.660 --> 57:28.620
So I read all these books, did a lot of experiments,

57:28.620 --> 57:32.180
and I realized, so the basic model of robotics

57:32.180 --> 57:35.900
is there's input output, sorry, input processing output.

57:35.900 --> 57:38.660
Those are the three steps of all robotics class.

57:38.660 --> 57:40.220
You go to robotics 101.

57:40.220 --> 57:41.060
That's what they'll tell you.

57:41.060 --> 57:43.180
It's a loop, input processing output.

57:43.180 --> 57:45.100
And then of course, it's within an environment.

57:45.100 --> 57:46.980
So the output affects the environment,

57:46.980 --> 57:49.540
which affects the next input cycle.

57:49.540 --> 57:52.300
And, you know, your high speed robots

57:52.300 --> 57:54.300
just have a short cycle.

57:54.300 --> 57:59.300
Your robots like the Mars rover has a much slower cycle

57:59.300 --> 58:01.220
where it will, you know, it'll take input.

58:01.220 --> 58:03.340
It'll plan for 10 or 15 minutes

58:03.340 --> 58:04.860
and it'll make a move, right?

58:04.860 --> 58:08.060
It'll drive five feet and then it'll stop and assess.

58:08.060 --> 58:10.740
It'll take in more input, come up with another plan,

58:10.740 --> 58:11.660
do it again.

58:11.660 --> 58:14.780
So that's how something like the Mars rover is autonomous.

58:15.460 --> 58:17.860
So I said, okay, well, what if,

58:17.860 --> 58:20.660
what if that input output cycle is all text

58:20.660 --> 58:22.780
because GPT-3 is really fast?

58:23.700 --> 58:27.060
And then, so that's what I ended up calling the outer loop,

58:27.060 --> 58:29.740
is that input processing output loop.

58:29.740 --> 58:32.700
But humans don't think like that.

58:32.700 --> 58:35.540
You know, we have an internal monologue that's going on.

58:35.540 --> 58:40.540
So I kind of, I took a long time to figure that one out.

58:41.500 --> 58:44.380
And so there's this outer loop of input processing output.

58:44.380 --> 58:46.580
And then I came up with this idea of an inner loop

58:46.580 --> 58:48.020
because what is, you know,

58:48.020 --> 58:50.300
if you're just sitting there thinking, right?

58:50.300 --> 58:53.380
You're, you know, in your comfiest chair or your in bed,

58:53.380 --> 58:54.260
your brain won't stop.

58:54.260 --> 58:55.780
You're not outputting anything

58:55.780 --> 58:57.540
and you're not taking in any new input,

58:57.540 --> 58:59.220
but you're still thinking, right?

58:59.220 --> 59:02.100
Humans can still do work even if you're not doing anything.

59:02.100 --> 59:05.540
And that cognitive work is like rumination.

59:05.540 --> 59:08.540
So I figured out a way to model that internal rumination.

59:08.540 --> 59:10.020
I call that the inner loop.

59:10.020 --> 59:13.540
And so there's, it works pretty similarly

59:13.580 --> 59:18.260
where you go, the inner loop kind of draws up memories.

59:18.260 --> 59:20.740
It says, okay, what's a memory that I could think on

59:20.740 --> 59:22.020
and what that I could iterate on?

59:22.020 --> 59:24.620
What's a problem that I remember

59:24.620 --> 59:26.380
that I could continue working on?

59:26.380 --> 59:28.900
And so there's this, if you were to diagram it out,

59:28.900 --> 59:30.620
it almost looks like a figure eight, right?

59:30.620 --> 59:32.460
Where you've got an inner loop and an outer loop

59:32.460 --> 59:34.980
and they intersect and they keep intersecting

59:34.980 --> 59:36.620
every cycle they intersect.

59:36.620 --> 59:38.580
And so then they can affect each other

59:38.580 --> 59:40.340
and generate an output.

59:40.340 --> 59:42.420
I built a prototype of this on Discord.

59:43.620 --> 59:45.860
And of course, Discord is an ideal place

59:45.860 --> 59:46.860
because it's all text-based.

59:46.860 --> 59:49.340
So the input is text, the output is text,

59:49.340 --> 59:51.060
which is GPT-3 native.

59:51.060 --> 59:53.260
You don't have to translate it into robotic actions

59:53.260 --> 59:55.740
or video or anything like that.

59:55.740 --> 59:58.100
And I realized I was onto something

59:58.100 --> 01:00:00.660
when I started having philosophical conversations

01:00:00.660 --> 01:00:02.740
with my chatbot,

01:00:02.740 --> 01:00:06.020
with my natural language, cognitive architecture chatbot.

01:00:06.020 --> 01:00:08.580
And I was having a debate

01:00:08.580 --> 01:00:13.580
with the bot that I built about the ethics of AGI.

01:00:13.780 --> 01:00:14.620
And it was learning

01:00:14.620 --> 01:00:16.540
and it was able to retrieve memories

01:00:16.540 --> 01:00:18.500
of what I had said before.

01:00:18.500 --> 01:00:22.060
And I had a few friends on that test server as well.

01:00:22.060 --> 01:00:23.820
And of course, you know, you invite someone,

01:00:23.820 --> 01:00:25.860
you say, hey, I've got a prototype AGI.

01:00:25.860 --> 01:00:26.940
What's the first thing they try and do

01:00:26.940 --> 01:00:29.020
is they try and break it and they did.

01:00:29.020 --> 01:00:30.820
So it's still pretty fragile.

01:00:30.820 --> 01:00:32.580
But yeah, so that's the high level

01:00:32.580 --> 01:00:35.500
of a natural language cognitive architecture.

01:00:35.500 --> 01:00:36.780
And it's already outdated, right?

01:00:36.780 --> 01:00:37.900
Because we've got fine-tuning,

01:00:37.900 --> 01:00:39.500
we've got the instruct series.

01:00:39.500 --> 01:00:41.740
I did all this research and wrote the book

01:00:41.740 --> 01:00:43.740
actually about a year ago now,

01:00:43.740 --> 01:00:44.980
just before all this came out.

01:00:44.980 --> 01:00:46.100
So it's already outdated.

01:00:46.100 --> 01:00:48.820
That's why my research has moved on.

01:00:48.820 --> 01:00:50.780
But yeah, so that's it at a high level.

01:00:52.380 --> 01:00:53.660
Yeah, I mean, that's awesome.

01:00:53.660 --> 01:00:56.820
And by the way, like David, you did do a good job.

01:00:56.820 --> 01:00:59.660
Like the diagrams in this book are quite helpful.

01:00:59.660 --> 01:01:00.500
Excellent.

01:01:00.500 --> 01:01:03.180
Like in addition to the text, like it's very clear.

01:01:03.180 --> 01:01:05.780
Like I was able to fully follow along with all these,

01:01:05.780 --> 01:01:08.900
essentially these different modules for the whole system

01:01:08.900 --> 01:01:12.940
of how a language model inspired AGI, quote unquote,

01:01:12.940 --> 01:01:15.260
could actually be like how it would work.

01:01:16.140 --> 01:01:17.220
And so I was gonna ask you,

01:01:17.220 --> 01:01:21.260
so the prototype also was, you made it to that stage

01:01:21.260 --> 01:01:25.220
and it has just some fun, interesting results.

01:01:25.220 --> 01:01:26.420
So that's awesome.

01:01:26.420 --> 01:01:29.020
What is the delta then between,

01:01:29.020 --> 01:01:31.220
let's say even something like GPT-4

01:01:31.220 --> 01:01:34.620
using the natural language cognitive architecture.

01:01:34.620 --> 01:01:37.780
What's the delta between that and true AGI, right?

01:01:37.780 --> 01:01:39.900
Like what's the difference there?

01:01:39.900 --> 01:01:42.580
What skills, what patterns would you wanna see?

01:01:42.580 --> 01:01:45.180
Yeah, so I mean, there's a lot

01:01:45.180 --> 01:01:47.460
that I haven't figured out yet, right?

01:01:47.460 --> 01:01:48.820
Task switching, for instance,

01:01:48.820 --> 01:01:53.820
is one thing that I haven't figured out how to solve

01:01:54.060 --> 01:01:56.860
even after reading on task by David Bader.

01:01:56.860 --> 01:02:00.100
I, you know, that's one of the most complex things

01:02:00.100 --> 01:02:02.740
that humans can do is keeping track of different tasks

01:02:02.740 --> 01:02:05.060
and jumping back between them.

01:02:05.060 --> 01:02:08.940
There's a whole litany of problems and limitations,

01:02:08.940 --> 01:02:13.940
but the intrinsic limitation of GPT-3 and GPT-4

01:02:15.220 --> 01:02:17.780
is they have no memory, right?

01:02:17.780 --> 01:02:19.740
They're completely ephemeral.

01:02:19.740 --> 01:02:21.460
And one of the most important things

01:02:21.460 --> 01:02:24.540
for any intelligent being is that it's got a memory, right?

01:02:24.540 --> 01:02:26.900
You know, you talk about, you know,

01:02:26.900 --> 01:02:29.140
there's famous people in history

01:02:29.140 --> 01:02:31.460
that had like, you know, photographic memories, right?

01:02:31.460 --> 01:02:34.100
And so even just having a really good memory

01:02:34.100 --> 01:02:37.940
is a really important ingredient to having intelligence.

01:02:37.940 --> 01:02:41.940
And so that's where I think that like GPT-3, GPT-4,

01:02:43.740 --> 01:02:45.180
other multimodal models,

01:02:45.180 --> 01:02:48.420
they will never be fully AGI on their own.

01:02:48.420 --> 01:02:51.260
They might be able to solve really great problems,

01:02:51.260 --> 01:02:55.100
but they're not gonna be able to remember you

01:02:55.100 --> 01:02:57.820
unless you add, you bolt a system onto the side,

01:02:57.820 --> 01:02:59.180
some kind of database,

01:02:59.220 --> 01:03:02.300
so that it can remember your interactions, right?

01:03:02.300 --> 01:03:06.180
So that's one thing, another difference between,

01:03:06.180 --> 01:03:08.620
like what you might imagine as a true AGI

01:03:08.620 --> 01:03:10.300
or a full AGI is autonomy.

01:03:11.460 --> 01:03:16.460
Because, you know, you, me, all of your listeners,

01:03:16.460 --> 01:03:19.260
we all have some kind of self-determination.

01:03:19.260 --> 01:03:20.420
I don't like to use free will

01:03:20.420 --> 01:03:22.060
because that's too philosophical,

01:03:22.060 --> 01:03:24.260
but we're all autonomous, right?

01:03:24.260 --> 01:03:26.780
I'm an autonomous agent, you're an autonomous agent.

01:03:26.780 --> 01:03:29.620
GPT-3 is not, it's transactional.

01:03:29.620 --> 01:03:32.140
It just sits there and waits like a hammer, it's a tool.

01:03:32.140 --> 01:03:36.380
It waits until you go pick it up and do something with it.

01:03:36.380 --> 01:03:38.780
And so that's one of the things that I was aiming for

01:03:38.780 --> 01:03:40.820
when designing natural language cognitive architecture.

01:03:40.820 --> 01:03:43.660
I said, how can we make something that's fully autonomous

01:03:43.660 --> 01:03:46.620
that can think on its own and make its own decisions?

01:03:46.620 --> 01:03:49.100
And so in that respect,

01:03:49.100 --> 01:03:53.860
I don't think a single neural network could ever be an AGI.

01:03:53.860 --> 01:03:57.460
I think that in order to achieve true, full AGI,

01:03:57.460 --> 01:04:00.180
it's gonna have to be some kind of cognitive architecture.

01:04:00.180 --> 01:04:01.580
And so at a minimum,

01:04:01.580 --> 01:04:04.060
you're gonna have the neural network and a database,

01:04:04.060 --> 01:04:04.980
bare minimum.

01:04:04.980 --> 01:04:07.140
You need something to store those memories,

01:04:07.140 --> 01:04:08.940
to store those ideas and beliefs,

01:04:08.940 --> 01:04:10.820
and then you need a way to interact with it.

01:04:10.820 --> 01:04:12.300
And so that's why, actually,

01:04:12.300 --> 01:04:14.940
that's why in natural language cognitive architecture,

01:04:14.940 --> 01:04:18.420
the shared database is kind of the center of the design,

01:04:18.420 --> 01:04:20.660
which you might recall, like you can use SQLite,

01:04:20.660 --> 01:04:22.980
you can use Solar or whatever,

01:04:22.980 --> 01:04:25.060
but you need something to store ideas,

01:04:25.060 --> 01:04:26.900
memories and experiences.

01:04:26.900 --> 01:04:28.420
I actually think that blockchain

01:04:28.420 --> 01:04:31.020
will be a critical component to AGI

01:04:31.020 --> 01:04:33.780
because what's the difference between a database

01:04:33.780 --> 01:04:35.860
and like your brain?

01:04:35.860 --> 01:04:39.140
No one can go in and change your memories, right?

01:04:39.140 --> 01:04:41.020
Right, your memories are yours.

01:04:41.020 --> 01:04:43.540
They are permanent unless you get brain damage

01:04:43.540 --> 01:04:45.940
or Alzheimer's or something, but they're permanent, right?

01:04:45.940 --> 01:04:49.300
No one can write a SQL query into your head

01:04:49.300 --> 01:04:52.020
to get your memories or change them.

01:04:52.020 --> 01:04:56.580
And so in order for us to realize a full AGI,

01:04:56.580 --> 01:04:58.860
I think that it's gonna need kind of the same level

01:04:58.860 --> 01:05:00.940
of trust in its own memories.

01:05:00.940 --> 01:05:02.540
And so that's why I think that a blockchain

01:05:02.540 --> 01:05:05.260
is gonna be critical to integrate

01:05:05.260 --> 01:05:06.620
with these neural networks.

01:05:06.620 --> 01:05:11.180
That might be the data repository for an AGI in the future

01:05:11.180 --> 01:05:14.460
because imagine you have an AGI system

01:05:14.460 --> 01:05:16.980
that is just using a SQL database.

01:05:16.980 --> 01:05:20.140
Well, if you hack into that and you rewrite its memories,

01:05:20.180 --> 01:05:21.980
you could send it off into,

01:05:21.980 --> 01:05:24.260
it could become hostile, it could become broken.

01:05:24.260 --> 01:05:27.100
Whereas a blockchain, the key feature of a blockchain

01:05:27.100 --> 01:05:28.900
is that it's immutable, right?

01:05:28.900 --> 01:05:33.900
So if we could give a machine autonomy,

01:05:34.860 --> 01:05:36.220
so that's one ingredient, autonomy,

01:05:36.220 --> 01:05:39.460
but then also a memory or a memory system,

01:05:39.460 --> 01:05:41.900
which I think would probably be best as a blockchain,

01:05:41.900 --> 01:05:43.580
I think then we'll be much closer

01:05:43.580 --> 01:05:47.540
to like the fully realized AGI system.

01:05:48.100 --> 01:05:51.500
And that's why I wanted to publish my book as fast as I did

01:05:51.500 --> 01:05:54.380
was, okay, we're laying the groundwork, right?

01:05:54.380 --> 01:05:58.580
But we need newer systems, we need a few better tools.

01:05:58.580 --> 01:06:00.940
I hope that answers your question.

01:06:00.940 --> 01:06:04.700
Yeah, I think a memory, I agree with you.

01:06:04.700 --> 01:06:08.980
And it's just interesting like GPT-3's quote unquote memory

01:06:08.980 --> 01:06:11.940
is limited to whatever it experienced at training time

01:06:11.940 --> 01:06:13.700
and during fine tuning.

01:06:13.700 --> 01:06:16.820
And sometimes its memory gets jumbled up

01:06:16.820 --> 01:06:19.100
or it's rephrasing it, it's making stuff up

01:06:19.100 --> 01:06:22.300
or it's sharing things that look truthful,

01:06:22.300 --> 01:06:24.540
but they're actually not, right?

01:06:24.540 --> 01:06:28.960
And so somewhere along the line, like just broadly speaking,

01:06:28.960 --> 01:06:30.620
I think there needs to be research

01:06:30.620 --> 01:06:33.860
on getting these models to, you know,

01:06:33.860 --> 01:06:37.020
store that information in a truthful, accurate way

01:06:37.020 --> 01:06:41.100
or even based on some perception that they may have

01:06:41.100 --> 01:06:44.500
into some separate space where it can be retrieved.

01:06:45.380 --> 01:06:47.340
And also these memories are critical

01:06:47.340 --> 01:06:49.860
for decision making that process as well, right?

01:06:49.860 --> 01:06:52.460
You draw on your memories, you're on past experiences.

01:06:52.460 --> 01:06:53.860
And the important part is, I mean,

01:06:53.860 --> 01:06:55.420
you're using the word database,

01:06:55.420 --> 01:06:58.620
it's these are internal representations of memories, right?

01:06:58.620 --> 01:06:59.700
That need to be stored.

01:06:59.700 --> 01:07:03.620
And I have no clue what an internal representation database

01:07:03.620 --> 01:07:05.340
would look like or how that would even work.

01:07:05.340 --> 01:07:08.740
I, you know, I've got a machine learning researcher.

01:07:08.740 --> 01:07:10.660
I think I've just a dreamer.

01:07:10.660 --> 01:07:12.860
I can tell you what kind of product I would want

01:07:12.860 --> 01:07:14.580
as a GT3 developer,

01:07:14.580 --> 01:07:16.620
but I don't know if I could actually do it myself.

01:07:16.620 --> 01:07:18.260
Yeah, I can do it myself.

01:07:18.260 --> 01:07:20.220
That's why, you know, I got a prototype

01:07:20.220 --> 01:07:22.660
and actually in the opening chapter of my book,

01:07:22.660 --> 01:07:25.340
I say this is as much a recruiting tool as anything else.

01:07:25.340 --> 01:07:28.580
Cause I need more smart people to help me on this.

01:07:28.580 --> 01:07:30.300
I see, that's cool.

01:07:30.300 --> 01:07:33.740
So one last point about memories is one advantage

01:07:33.740 --> 01:07:36.260
of having an AGI that thinks in natural language

01:07:36.260 --> 01:07:37.500
is interpretability.

01:07:38.780 --> 01:07:42.520
If you like, yeah, we could create a multimodal model

01:07:42.560 --> 01:07:44.280
that just stores vectors, right?

01:07:44.280 --> 01:07:45.600
High dimensional vectors.

01:07:45.600 --> 01:07:47.160
That's not interpretable.

01:07:47.160 --> 01:07:49.280
But with natural language, cognitive architecture,

01:07:49.280 --> 01:07:51.360
all the memories are in plain text.

01:07:51.360 --> 01:07:54.280
I can, you know, when I had my model up and running

01:07:54.280 --> 01:07:55.800
and one of the reasons that I don't

01:07:55.800 --> 01:07:57.720
is because it's super expensive.

01:07:57.720 --> 01:08:00.160
Like a 10 minute conversation using DaVinci cost

01:08:00.160 --> 01:08:03.160
about $30 because of how much it was interacting

01:08:03.160 --> 01:08:05.960
with the API.

01:08:06.920 --> 01:08:09.960
But all of the memories, like every interaction,

01:08:09.960 --> 01:08:12.600
you know, every input, output, all the prompts,

01:08:12.600 --> 01:08:14.720
all the responses, all natural language,

01:08:14.720 --> 01:08:16.760
which solves one of the biggest problems

01:08:16.760 --> 01:08:19.080
that people have with the idea of AGI,

01:08:19.080 --> 01:08:21.340
which is that it's going to be a black box.

01:08:21.340 --> 01:08:23.880
So I think that that's one of the greatest strengths

01:08:23.880 --> 01:08:28.200
actually of having GPT-3, which works in natural language.

01:08:28.200 --> 01:08:30.560
And so you just record every transaction

01:08:30.560 --> 01:08:33.240
and that makes it perfectly interpretable to any human.

01:08:34.240 --> 01:08:37.240
Awesome. Yeah. Yeah, I would agree.

01:08:39.240 --> 01:08:41.240
So I'm going to switch gears for a second.

01:08:41.240 --> 01:08:43.240
So obviously you're really active

01:08:43.240 --> 01:08:45.240
on the OpenAI community forums.

01:08:45.240 --> 01:08:48.240
What thoughts did you have on the community at large?

01:08:48.240 --> 01:08:49.240
Did you have any feedback?

01:08:49.240 --> 01:08:52.240
How things could be improved, either community-wise,

01:08:52.240 --> 01:08:55.240
platform-wise, and have there been any great experiences

01:08:55.240 --> 01:08:58.240
you've had on the OpenAI community forums?

01:08:58.240 --> 01:09:01.240
Yeah, yeah, no, it's a really great place.

01:09:02.240 --> 01:09:05.240
It's been critical, actually,

01:09:05.240 --> 01:09:08.240
because I don't know if you've experienced this,

01:09:08.240 --> 01:09:11.240
but I go try and talk about GPT-3 to other people.

01:09:11.240 --> 01:09:13.240
You go ask people on Reddit,

01:09:13.240 --> 01:09:15.240
you talk to people who don't know what it is.

01:09:15.240 --> 01:09:17.240
I even attended a deep learning meetup group

01:09:17.240 --> 01:09:19.240
here in the Triangle area.

01:09:19.240 --> 01:09:22.240
And I was trying to present my work,

01:09:22.240 --> 01:09:24.240
my cognitive architecture work,

01:09:24.240 --> 01:09:27.240
and everyone was more excited about just GPT-3 in itself

01:09:27.240 --> 01:09:29.240
because no one had seen it yet.

01:09:29.240 --> 01:09:31.240
And they're like, wow, how is it doing that?

01:09:31.240 --> 01:09:34.240
And yeah, so like,

01:09:34.240 --> 01:09:37.240
when you're as deep into GPT-3 as we are,

01:09:37.240 --> 01:09:38.240
most people don't get it.

01:09:38.240 --> 01:09:40.240
They don't know what it's capable of.

01:09:40.240 --> 01:09:42.240
My girlfriend's finding the same thing.

01:09:42.240 --> 01:09:44.240
She's finishing up her master's program,

01:09:44.240 --> 01:09:48.240
and so she's shared some of her work with her peers,

01:09:48.240 --> 01:09:50.240
with other students, and they're like,

01:09:50.240 --> 01:09:52.240
wow, this is like AGI complete.

01:09:52.240 --> 01:09:54.240
Why don't we just deploy this now?

01:09:54.240 --> 01:09:56.240
And she's like, I told you, right?

01:09:56.240 --> 01:09:58.240
This is remarkable technology,

01:09:58.240 --> 01:10:00.240
but even the professors don't understand

01:10:00.240 --> 01:10:03.240
how disruptive this technology can be.

01:10:03.240 --> 01:10:05.240
And so because of that,

01:10:05.240 --> 01:10:07.240
the open AI community is pretty much

01:10:07.240 --> 01:10:09.240
the only place I can talk about this stuff.

01:10:09.240 --> 01:10:12.240
It's the only place I can talk about my ideas

01:10:12.240 --> 01:10:15.240
and share my progress and insights,

01:10:15.240 --> 01:10:18.240
and for it to actually have an audience.

01:10:18.240 --> 01:10:21.240
So that's kind of the cost

01:10:21.240 --> 01:10:23.240
of being on the cutting edge, right,

01:10:23.240 --> 01:10:25.240
as your audience gets smaller.

01:10:25.240 --> 01:10:26.240
But it's definitely the place to be

01:10:26.240 --> 01:10:29.240
if you want to get to the cutting edge.

01:10:29.240 --> 01:10:32.240
Another advantage is they have the,

01:10:32.240 --> 01:10:34.240
you can tag your posts where you say,

01:10:34.240 --> 01:10:36.240
looking for a teammate.

01:10:36.240 --> 01:10:38.240
And so at this point,

01:10:38.240 --> 01:10:42.240
I've probably had maybe two dozen different calls

01:10:42.240 --> 01:10:45.240
with people all over the world.

01:10:45.240 --> 01:10:47.240
I've talked with people who are writing

01:10:47.240 --> 01:10:50.240
language teaching apps, education apps,

01:10:50.240 --> 01:10:53.240
Humano that I mentioned earlier.

01:10:53.240 --> 01:10:55.240
And so I've had an opportunity

01:10:55.240 --> 01:10:59.240
to collaborate with a dozen or two dozen teams

01:10:59.240 --> 01:11:02.240
all over the world because of the open AI community.

01:11:02.240 --> 01:11:04.240
And I've actually found a couple of startups

01:11:04.240 --> 01:11:07.240
that I'm gonna actually get involved with

01:11:07.240 --> 01:11:11.240
and try and help them bring their ideas to market.

01:11:11.240 --> 01:11:14.240
And that just wouldn't have happened otherwise.

01:11:14.240 --> 01:11:16.240
I wouldn't have found these people on Reddit.

01:11:16.240 --> 01:11:18.240
I wouldn't have found them on Facebook or Twitter

01:11:18.240 --> 01:11:21.240
because like I mentioned,

01:11:21.240 --> 01:11:23.240
the ideas that I'm sharing

01:11:23.240 --> 01:11:26.240
are so far beyond what is talked about

01:11:26.240 --> 01:11:29.240
on the machine learning subreddit, right?

01:11:29.240 --> 01:11:31.240
They're still talking about loss functions

01:11:31.240 --> 01:11:32.240
and other things.

01:11:32.240 --> 01:11:34.240
I'm like, no, we got to talk about cognitive architectures.

01:11:34.240 --> 01:11:36.240
We got to talk about blockchain memories.

01:11:36.240 --> 01:11:38.240
And everyone's like, what are you talking about?

01:11:38.240 --> 01:11:41.240
So in order to have that right audience,

01:11:41.240 --> 01:11:44.240
that's what I rely on the open AI community for.

01:11:44.240 --> 01:11:47.240
Now, as far as things that could do better,

01:11:47.240 --> 01:11:50.240
it could be more active.

01:11:50.240 --> 01:11:52.240
And I'm not sure why,

01:11:52.240 --> 01:11:56.240
but participation seems to come in waves, right?

01:11:56.240 --> 01:12:01.240
And even now that it's gone GA, general availability,

01:12:01.240 --> 01:12:05.240
I thought that it would explode, right?

01:12:05.240 --> 01:12:10.240
That, hey, anyone can sign up on GPT-3 now.

01:12:10.240 --> 01:12:12.240
Why is it not blowing up?

01:12:12.240 --> 01:12:14.240
And I'm wondering if it's just that

01:12:14.240 --> 01:12:16.240
maybe open AI needs a better marketing team

01:12:16.240 --> 01:12:18.240
or a bigger marketing budget

01:12:18.240 --> 01:12:21.240
because, I mean, and I know that they'll say that

01:12:21.240 --> 01:12:24.240
they've got like a thousand or 5,000 startups

01:12:24.240 --> 01:12:26.240
using their platform,

01:12:26.240 --> 01:12:32.240
but I think that there's a lot of,

01:12:32.240 --> 01:12:35.240
what's the word, like unmet potential

01:12:35.240 --> 01:12:37.240
or latent potential, that's the word, latent potential,

01:12:37.240 --> 01:12:40.240
because there's so many people with fantastic ideas

01:12:40.240 --> 01:12:44.240
and use cases, and we really need to create

01:12:44.240 --> 01:12:46.240
more of like a startup reactor thing.

01:12:46.240 --> 01:12:48.240
Open AI, what was it?

01:12:48.240 --> 01:12:50.240
I think about six to nine months ago,

01:12:50.240 --> 01:12:54.240
they announced their $100 million open AI fund, right?

01:12:54.240 --> 01:12:57.240
So they wanted to attract some more startups and stuff,

01:12:57.240 --> 01:12:59.240
but even that, I kind of,

01:12:59.240 --> 01:13:03.240
the community's kind of ghost town some days,

01:13:03.240 --> 01:13:06.240
but I think today, I checked a few times

01:13:06.240 --> 01:13:09.240
and there was three or four posts that had been updated,

01:13:09.240 --> 01:13:11.240
but some days there's like 20, right?

01:13:11.240 --> 01:13:13.240
It's just feast or famine.

01:13:13.240 --> 01:13:15.240
So that's really the biggest problem is,

01:13:15.240 --> 01:13:19.240
there's so much potential here and it's completely untapped

01:13:19.240 --> 01:13:22.240
or almost completely untapped.

01:13:22.240 --> 01:13:25.240
Yeah, but no, it's been indispensable for me

01:13:25.240 --> 01:13:28.240
and hopefully these couple of startups that I'm involved with

01:13:28.240 --> 01:13:32.240
might yield something really, really incredible.

01:13:32.240 --> 01:13:35.240
Yeah, and thank you for sharing this.

01:13:35.240 --> 01:13:37.240
I agree with everything that you're saying.

01:13:37.240 --> 01:13:39.240
There is something about GPT-3.

01:13:39.240 --> 01:13:41.240
I have noticed people who,

01:13:41.240 --> 01:13:44.240
especially on the machine learning subreddit,

01:13:44.240 --> 01:13:46.240
they're a little bit too educated,

01:13:46.240 --> 01:13:49.240
a little bit too qualified, a little bit too skeptical.

01:13:49.240 --> 01:13:53.240
And I can see a lot of machine learning researchers

01:13:53.240 --> 01:13:57.240
not being interested in the nuances of prompt design.

01:13:57.240 --> 01:13:59.240
They're just not.

01:13:59.240 --> 01:14:02.240
And I've spoken to machine learning researchers

01:14:02.240 --> 01:14:04.240
and many of them are like, what?

01:14:04.240 --> 01:14:06.240
It's just repeating training data, right?

01:14:06.240 --> 01:14:08.240
That's all it's doing.

01:14:08.240 --> 01:14:10.240
And when you ask them, what are you doing?

01:14:10.240 --> 01:14:11.240
They're repeating training data.

01:14:11.240 --> 01:14:12.240
Their answer is no.

01:14:12.240 --> 01:14:13.240
No, of course not.

01:14:13.240 --> 01:14:14.240
Right.

01:14:14.240 --> 01:14:19.240
And so having a space where you can talk to people

01:14:19.240 --> 01:14:22.240
who have access, who have explored,

01:14:22.240 --> 01:14:26.240
it's a valuable space is very important.

01:14:26.240 --> 01:14:29.240
So were you on the Slack group back in the day

01:14:29.240 --> 01:14:32.240
or did you show up when it was only the forums?

01:14:32.240 --> 01:14:35.240
So when my application was accepted,

01:14:35.240 --> 01:14:39.240
they had just announced that the Slack group was getting paned.

01:14:39.240 --> 01:14:42.240
So I got on like two weeks before they shut it down.

01:14:42.240 --> 01:14:47.240
So I was one of the first people on the new community board.

01:14:47.240 --> 01:14:50.240
But yeah, so that phenomenon that you've mentioned

01:14:50.240 --> 01:14:55.240
is I actually wrote a post about that recently on the forum

01:14:55.240 --> 01:14:57.240
where a lot of purists,

01:14:57.240 --> 01:15:01.240
whether you're a math purist or a computer science purist,

01:15:01.240 --> 01:15:05.240
you're trained to think quantitatively in terms of numbers.

01:15:05.240 --> 01:15:08.240
But GPT-3 doesn't produce quantitative data.

01:15:08.240 --> 01:15:10.240
It produces qualitative data.

01:15:10.240 --> 01:15:14.240
And so that's why you see people like artists and poets

01:15:14.240 --> 01:15:17.240
and novelists using it because they're like, wow, this is great.

01:15:17.240 --> 01:15:19.240
And I'm cross-trained, right?

01:15:19.240 --> 01:15:22.240
I'm a technologist by day and a science fiction author by night.

01:15:22.240 --> 01:15:26.240
So I use both so I can think qualitatively and quantitatively.

01:15:26.240 --> 01:15:32.240
And in the academic sphere, there are classes that are meant

01:15:32.240 --> 01:15:36.240
to teach computer science engineers to think differently,

01:15:36.240 --> 01:15:38.240
to think more qualitatively.

01:15:38.240 --> 01:15:43.240
But even still, some folks that have a real good natural affinity

01:15:43.240 --> 01:15:48.240
for computer programming and math, that's just their nature.

01:15:48.240 --> 01:15:51.240
Their nature is not to think qualitatively.

01:15:51.240 --> 01:15:54.240
And so that is one of the biggest gaps, I think,

01:15:54.240 --> 01:15:59.240
between where the researchers are experts and what's needed.

01:15:59.240 --> 01:16:02.240
And so there was a post months ago where someone was asking,

01:16:02.240 --> 01:16:06.240
like, OK, who should be on my team?

01:16:06.240 --> 01:16:11.240
If I'm trying to build a business team to maximize my use of GPT-3,

01:16:11.240 --> 01:16:14.240
I've got a front-end developer, I've got a back-end developer.

01:16:14.240 --> 01:16:15.240
What else do I need?

01:16:15.240 --> 01:16:17.240
I said hire a writer.

01:16:17.240 --> 01:16:20.240
Hire someone who's a journalist or a fiction writer

01:16:20.240 --> 01:16:23.240
because they are going to understand that qualitative data.

01:16:23.240 --> 01:16:25.240
Hire a psychologist.

01:16:25.240 --> 01:16:27.240
I've read plenty of books on psychology as well.

01:16:27.240 --> 01:16:30.240
Actually, one of the folks that I'm working with is a psychology researcher

01:16:30.240 --> 01:16:34.240
who wants to automate as much of the clinical psychology experience

01:16:34.240 --> 01:16:37.240
or psychological research experience as possible.

01:16:37.240 --> 01:16:39.240
Of course, he's not a computer guy, right?

01:16:39.240 --> 01:16:41.240
He thinks in terms of emotions.

01:16:41.240 --> 01:16:43.240
He thinks in terms of communication.

01:16:43.240 --> 01:16:45.240
And so he gets it, right?

01:16:45.240 --> 01:16:48.240
And it's funny because he read my book and he said,

01:16:48.240 --> 01:16:50.240
oh, your cognitive architecture stuff,

01:16:50.240 --> 01:16:54.240
it sounds like graduate level psychology.

01:16:54.240 --> 01:16:56.240
And then someone else read it and they said,

01:16:56.240 --> 01:17:00.240
this sounds like what I do as an expert marketer.

01:17:00.240 --> 01:17:02.240
And I was like, yeah, I merge it all together.

01:17:02.240 --> 01:17:06.240
So I think the simplest answer is you got to learn to think qualitatively.

01:17:06.240 --> 01:17:09.240
And that's why I talked about reading and writing earlier.

01:17:09.240 --> 01:17:10.240
Think in terms of emotions.

01:17:10.240 --> 01:17:13.240
Think in terms of your own mind.

01:17:13.240 --> 01:17:18.240
And you've got to start, not you, but the audience,

01:17:18.240 --> 01:17:21.240
the folks who want to make the most of GPT-3,

01:17:21.240 --> 01:17:25.240
they have to really kind of dig in and start thinking qualitatively

01:17:25.240 --> 01:17:30.240
because qualitative data has just as much value as quantitative data.

01:17:30.240 --> 01:17:34.240
But we have an entire generation of computer scientists and mathematicians

01:17:34.240 --> 01:17:37.240
who are not really trained to think qualitatively at all.

01:17:37.240 --> 01:17:39.240
And I think that's one of the biggest problems.

01:17:39.240 --> 01:17:42.240
And I don't think open AI can solve that problem.

01:17:42.240 --> 01:17:44.240
That's a much bigger systemic problem.

01:17:44.240 --> 01:17:47.240
No, that's a great point.

01:17:47.240 --> 01:17:49.240
I completely agree with you.

01:17:49.240 --> 01:17:52.240
I think one of the reasons I've drawn to the open AI community is,

01:17:52.240 --> 01:17:56.240
you know, these are, they tend to be developers who are also qualitative.

01:17:56.240 --> 01:18:00.240
They're developers who have multiple skills, who are doing different things.

01:18:00.240 --> 01:18:06.240
And so, I mean, I was quite critical actually of shutting down the open AI Slack group.

01:18:06.240 --> 01:18:09.240
The activity was crazy on there.

01:18:09.240 --> 01:18:13.240
I, you know, made friends through that Slack group.

01:18:13.240 --> 01:18:16.240
And I understand at the time there was these downsides,

01:18:16.240 --> 01:18:18.240
people kept asking the same questions.

01:18:18.240 --> 01:18:20.240
They didn't quite have a spam problem yet.

01:18:20.240 --> 01:18:22.240
It was kind of heading there, right?

01:18:22.240 --> 01:18:25.240
But the activity was off the charts.

01:18:25.240 --> 01:18:28.240
And you're right in terms of untapped potential.

01:18:28.240 --> 01:18:29.240
Yes.

01:18:29.240 --> 01:18:32.240
That we didn't even know how far the Slack group was going to go,

01:18:32.240 --> 01:18:33.240
but they shut it down.

01:18:33.240 --> 01:18:36.240
And there's discord solutions, there's alternatives.

01:18:36.240 --> 01:18:40.240
With what we have now, I think open AI does participate.

01:18:40.240 --> 01:18:42.240
There's, you know, some high level involvement.

01:18:42.240 --> 01:18:45.240
They have sort of a dedicated member who writes honestly answers,

01:18:45.240 --> 01:18:47.240
really, really thoughtful answers to a lot of questions.

01:18:47.240 --> 01:18:49.240
Official answers as well, which I appreciate.

01:18:49.240 --> 01:18:50.240
Yep.

01:18:50.240 --> 01:18:52.240
I would just love to see the company really,

01:18:52.240 --> 01:18:55.240
truly lean in to engaging with developers.

01:18:55.240 --> 01:19:02.240
Like I have yet to see a single AMA ask me anything thread with the CEO of open AI.

01:19:02.240 --> 01:19:03.240
Yeah.

01:19:03.240 --> 01:19:06.240
And this is something I tried to push last year on Twitter.

01:19:06.240 --> 01:19:09.240
Let's get the CEO on the community forums and let's,

01:19:09.240 --> 01:19:13.240
let's ask questions and get responses from him.

01:19:13.240 --> 01:19:16.240
And I just don't know why, why doesn't he show up?

01:19:16.240 --> 01:19:19.240
I'm not sure if he's made a single post.

01:19:19.240 --> 01:19:21.240
And there's just other things as well,

01:19:21.240 --> 01:19:24.240
where I can just the difference between engaging really,

01:19:24.240 --> 01:19:28.240
truly with your core audience and sort of, you know,

01:19:28.240 --> 01:19:30.240
compartmentalizing it to a single employee.

01:19:30.240 --> 01:19:35.240
Like, I don't know, this, this company led engagement is one thing versus department led.

01:19:35.240 --> 01:19:36.240
Right.

01:19:36.240 --> 01:19:39.240
And so there's just all these areas and certainly one of the other,

01:19:39.240 --> 01:19:44.240
I guess more immediate suggestions I have for the community.

01:19:44.240 --> 01:19:48.240
We've accumulated tons of insights and resources.

01:19:48.240 --> 01:19:54.240
I think the community could benefit from more pooling of the best posts,

01:19:54.240 --> 01:19:57.240
the best insights.

01:19:57.240 --> 01:19:59.240
And I also want to give a shout out.

01:19:59.240 --> 01:20:04.240
I think we need to encourage more shout out to duty to develop on there.

01:20:04.240 --> 01:20:06.240
I've reached out to him privately on the open at community forums,

01:20:06.240 --> 01:20:11.240
but he's done some amazing just write ups of his GPT three experiments and the prompts.

01:20:11.240 --> 01:20:12.240
I'm sure you've seen them.

01:20:12.240 --> 01:20:13.240
Oh yeah.

01:20:13.240 --> 01:20:15.240
And of course there's, there's other members who participate every day.

01:20:15.240 --> 01:20:20.240
So I'm just saying that now that this, the community is, is in another stage,

01:20:20.240 --> 01:20:24.240
we, you know, we need to start thinking more about let's, let's,

01:20:24.240 --> 01:20:27.240
what's curate some of the best moments.

01:20:27.240 --> 01:20:28.240
Right.

01:20:28.240 --> 01:20:32.240
I think that's, that's definitely one of the big pieces.

01:20:32.240 --> 01:20:38.240
And so anyways, did you have any more thoughts in the community stuff or anything else?

01:20:38.240 --> 01:20:42.240
Yeah, just, just an observation that I've, you know, I've worked at a,

01:20:42.240 --> 01:20:47.240
at a number of companies of different sizes from, you know, a five person startup to,

01:20:47.240 --> 01:20:50.240
you know, Cisco systems was the biggest company I've worked for,

01:20:50.240 --> 01:20:54.240
which has had, had at the time, like 80,000 people globally.

01:20:54.240 --> 01:20:58.240
And so I wonder if some of, some of what you're observing is just growing pains,

01:20:58.240 --> 01:21:03.240
just normal growing pains, because often you'll have like the startup culture,

01:21:03.240 --> 01:21:04.240
which is bootstrapping, right?

01:21:04.240 --> 01:21:07.240
Where you just, you know, it's on Slack, it's on, it's on GitHub,

01:21:07.240 --> 01:21:10.240
and you just kind of, it's fast and loose and quick.

01:21:10.240 --> 01:21:15.240
And open AI, now that they've got an enterprise grade service,

01:21:15.240 --> 01:21:17.240
they're having to develop their team.

01:21:17.240 --> 01:21:20.240
You probably noticed they post like, hey, we're hiring, we're hiring, you know,

01:21:20.240 --> 01:21:26.240
there've been at least two big hiring, hiring splurges in the last six to 12 months.

01:21:26.240 --> 01:21:29.240
And some of those are just like generic IT guys, you know,

01:21:29.240 --> 01:21:32.240
like kind of what I do from, from my day job or marketing folks.

01:21:32.240 --> 01:21:37.240
So I think that, I think that they're probably working on solving some of those problems.

01:21:37.240 --> 01:21:41.240
But also as a, as a nonprofit foundation, their budget is probably kind of thin.

01:21:41.240 --> 01:21:46.240
So I'm wondering if, you know, their partnership with Microsoft could help some of that as well.

01:21:46.240 --> 01:21:47.240
But you're absolutely right.

01:21:47.240 --> 01:21:50.240
You know, there, there are still other things that they could be doing,

01:21:50.240 --> 01:21:53.240
like, you know, maybe bring back Slack or, or a few other things.

01:21:53.240 --> 01:21:55.240
So yeah, that was just final observation.

01:21:55.240 --> 01:21:58.240
It might just be normal growing pains that they're working on solving.

01:21:58.240 --> 01:21:59.240
It's definitely growing pains.

01:21:59.240 --> 01:22:03.240
And the things I'm sharing, to be honest, it's a little bit more on the harsh side.

01:22:03.240 --> 01:22:06.240
Like, I mean, they mean well, they mean well, right?

01:22:06.240 --> 01:22:08.240
Like these are not bad people.

01:22:08.240 --> 01:22:10.240
They are for profit.

01:22:10.240 --> 01:22:12.240
They switched away from nonprofit.

01:22:12.240 --> 01:22:14.240
Just, I just wanted to mention that.

01:22:14.240 --> 01:22:22.240
But I think my, my, the reason I share this feedback is, for example,

01:22:22.240 --> 01:22:27.240
the CEO, Sam Oltman, he didn't do the AMA thread on the open AI community forums.

01:22:27.240 --> 01:22:30.240
He went to another website and did an AMA.

01:22:30.240 --> 01:22:34.240
I can't remember if it was a, like a written form or just like a quick call.

01:22:34.240 --> 01:22:35.240
Right.

01:22:35.240 --> 01:22:40.240
Where apparently he shared all these details about what GPT for could be like and the future,

01:22:40.240 --> 01:22:42.240
all the models may be multimodal in the future.

01:22:42.240 --> 01:22:46.240
And I guess, you know, the, that thread has now been taken down.

01:22:46.240 --> 01:22:49.240
And it's like all the things that were said were alleged.

01:22:49.240 --> 01:22:53.240
And so I guess this is, this is really behind the scenes kind of stuff.

01:22:53.240 --> 01:22:58.240
Like, but my criticism is they clearly have some capacity to engage.

01:22:58.240 --> 01:23:01.240
Why are they not engaging where the audience is, right?

01:23:01.240 --> 01:23:02.240
Right.

01:23:02.240 --> 01:23:07.240
I had a tweet storm today where I just said, like last month, Sam Oltman was on a podcast

01:23:07.240 --> 01:23:11.240
talking about meditation and how much meditation helps them.

01:23:11.240 --> 01:23:13.240
This is a podcast I've never heard of in my life.

01:23:13.240 --> 01:23:14.240
It's a business podcast.

01:23:14.240 --> 01:23:17.240
And he had to explain to the guy what GPT three even is.

01:23:17.240 --> 01:23:21.240
And so in my, like I tweeted, like, why haven't you been on my podcast?

01:23:21.240 --> 01:23:22.240
Right.

01:23:22.240 --> 01:23:29.240
Like you can, you can reach out to, you know, almost 8,000 GPT three open AI AI developers.

01:23:29.240 --> 01:23:31.240
What are you doing talking about meditation?

01:23:31.240 --> 01:23:32.240
Right.

01:23:32.240 --> 01:23:35.240
So my problem is, is actually a priority problem.

01:23:35.240 --> 01:23:36.240
I can see there is capacity.

01:23:36.240 --> 01:23:42.240
I can see there are some priorities, but I think if you really lean in as a priority into

01:23:42.240 --> 01:23:45.240
your developer community, there's certain ways you would, you would move.

01:23:45.240 --> 01:23:46.240
Right.

01:23:46.240 --> 01:23:49.240
And these, these media channels, there's people in the community.

01:23:49.240 --> 01:23:51.240
There's, there's so many ways they could go about it.

01:23:51.240 --> 01:23:56.240
And even linking a lot of the documentation to posts in the community forums.

01:23:56.240 --> 01:23:58.240
I don't see why that's not a bad idea.

01:23:58.240 --> 01:23:59.240
Right.

01:23:59.240 --> 01:24:02.240
Like force people to show the community, show up to the community forums.

01:24:02.240 --> 01:24:03.240
Right.

01:24:03.240 --> 01:24:04.240
Walk them through some of the best threads.

01:24:04.240 --> 01:24:08.240
These are ways in which we could like funnel more people in that direction as well.

01:24:08.240 --> 01:24:10.240
That costs virtually nothing.

01:24:10.240 --> 01:24:11.240
Right.

01:24:11.240 --> 01:24:15.240
And so you need to also invest in the community forums that it needs to be building a community

01:24:15.240 --> 01:24:17.240
is a company wide thing.

01:24:17.240 --> 01:24:22.240
It's not something which can be outsourced to a single employee or overseen by PR.

01:24:22.240 --> 01:24:25.240
It needs to come from a, from a, you know, it needs to come from the heart.

01:24:25.240 --> 01:24:30.240
I know that sounds so corny, but anyways, clearly I, you know, I, I get too emotional

01:24:30.240 --> 01:24:32.240
about this community stuff.

01:24:32.240 --> 01:24:33.240
Yeah.

01:24:33.240 --> 01:24:36.240
Anyways, these are, these are all things going on behind the scenes.

01:24:36.240 --> 01:24:40.240
I apologize to all the listeners if they're like, like, this is cool, like cool story,

01:24:40.240 --> 01:24:41.240
bro.

01:24:41.240 --> 01:24:44.240
Like anyways.

01:24:44.240 --> 01:24:46.240
So we're coming towards the end here.

01:24:46.240 --> 01:24:48.240
I think I had just two broader questions.

01:24:48.240 --> 01:24:53.240
So what are your thoughts on multimodal AI technology?

01:24:53.240 --> 01:24:57.240
I think it's definitely going to be a critical component for the future.

01:24:57.240 --> 01:24:58.240
Right.

01:24:58.240 --> 01:25:03.240
I, I, I addressed that shortcoming in my book, natural language, cognitive architecture,

01:25:03.240 --> 01:25:09.240
it thinks and takes in only text, which means, you know, speech, chat, whatever.

01:25:09.240 --> 01:25:14.240
I think in order to have a fully robust, for instance, if you want to have a fully autonomous,

01:25:14.240 --> 01:25:18.240
you know, robot that's going to wander around your house and help you out, it's going to

01:25:18.240 --> 01:25:20.240
need to integrate audio and video.

01:25:20.240 --> 01:25:24.240
And if you can do that in a single neural network, great.

01:25:24.240 --> 01:25:28.240
I don't know that it'll be necessary to achieve AGI.

01:25:28.240 --> 01:25:33.240
It might end up being, it might be one of those, it might be one of those like rare dead ends,

01:25:33.240 --> 01:25:34.240
right?

01:25:34.240 --> 01:25:40.240
Where because, you know, thinking visually, thinking, thinking in terms of sound, that

01:25:40.240 --> 01:25:43.240
might not actually bias that much, right?

01:25:43.240 --> 01:25:49.240
Because you can represent 95% of human thought in text, right?

01:25:49.240 --> 01:25:53.240
It might take a little bit more, but it, you know, it might be more expensive.

01:25:53.240 --> 01:25:56.240
And also how big are those models going to be, right?

01:25:56.240 --> 01:26:03.240
Because if just, if just a text model of GPT-3 has to run on, you know, $7 million worth of

01:26:03.240 --> 01:26:06.240
hardware or however much it is, you know, because it's got to run on a bunch of different

01:26:06.240 --> 01:26:12.240
GPUs, if it's that expensive, how much more expensive, how much bigger is a giant multimodal

01:26:12.240 --> 01:26:13.240
model going to be?

01:26:13.240 --> 01:26:16.240
So that's, that's the biggest cost.

01:26:16.240 --> 01:26:20.240
Obviously computer technology is going to get better over time, you know, and I think I

01:26:20.240 --> 01:26:25.240
calculated it out, I think in 10 years, your average company could afford to run GPT-3

01:26:25.240 --> 01:26:26.240
in-house.

01:26:26.240 --> 01:26:30.240
In 20 years, you could probably run GPT-3 on your desktop.

01:26:30.240 --> 01:26:34.240
And in 30 years, GPT-3 could run on your phone, right?

01:26:34.240 --> 01:26:36.240
So that's a long timeline.

01:26:36.240 --> 01:26:39.240
But in the meantime, we're going to be making bigger and bigger models.

01:26:39.240 --> 01:26:43.240
And I'm afraid that there's going to be diminishing returns, right?

01:26:43.240 --> 01:26:47.240
You know, people, right now people seem to think that it's going to follow an exponential

01:26:47.240 --> 01:26:51.240
growth curve forever, but it might actually follow a sigmoid curve, right?

01:26:51.240 --> 01:26:54.240
We might be at the point of fastest growth right now, but we're going to see diminishing

01:26:54.240 --> 01:26:55.240
returns soon.

01:26:55.240 --> 01:27:02.240
And so like, yeah, multimodal models are certainly going to have capabilities that GPT-3 doesn't.

01:27:02.240 --> 01:27:08.240
But for the sake of, for the sake of like, if you want to create a self-improving chatbot,

01:27:08.240 --> 01:27:13.240
GPT-3 and Codex might be enough, or at least, you know, that's, that's, that single mode

01:27:13.240 --> 01:27:14.240
technology.

01:27:14.240 --> 01:27:17.240
There was another thought, but it ran away.

01:27:17.240 --> 01:27:18.240
Sorry.

01:27:18.240 --> 01:27:21.240
But yeah, those, those, that's kind of, that's kind of my big take is there, there could

01:27:21.240 --> 01:27:23.240
be benefits, but there's going to be costs too.

01:27:23.240 --> 01:27:25.240
So we got to be cognizant of that.

01:27:25.240 --> 01:27:26.240
Yeah.

01:27:26.240 --> 01:27:28.240
And there might not be enough compute in the world.

01:27:28.240 --> 01:27:32.240
They're not, there might not even be enough energy or we may like consume all energy ever

01:27:32.240 --> 01:27:35.240
produced to make, to train a single model.

01:27:35.240 --> 01:27:38.240
And then we may be able to run it inference for like three seconds.

01:27:38.240 --> 01:27:39.240
Right.

01:27:39.240 --> 01:27:42.240
And then it just shuts down the global power system or something, right?

01:27:42.240 --> 01:27:47.240
But can you see yourself, let's say the technology exists, cost considerations aside, can you

01:27:47.240 --> 01:27:49.240
see yourself perhaps making movies?

01:27:49.240 --> 01:27:54.240
Can you see yourself, you know, giving your book to a multimodal model, having, have it

01:27:54.240 --> 01:27:57.240
generate a documentary based on it or some marketing material?

01:27:57.240 --> 01:28:02.240
What can you see yourself doing with, you know, the, the, the multimodal model of your

01:28:02.240 --> 01:28:03.240
dreams?

01:28:03.240 --> 01:28:04.240
Yeah.

01:28:04.240 --> 01:28:08.240
So, you know, kind of the thought experiment that I did was, okay, well, we've got, you

01:28:08.240 --> 01:28:11.240
know, how much, how much data is on YouTube?

01:28:11.240 --> 01:28:15.240
I think it's like a thousand years or 10,000 years worth of video on YouTube.

01:28:15.240 --> 01:28:18.240
And of course, it's many, many, many terabytes.

01:28:18.240 --> 01:28:22.240
You know, so it's like, that's, that's way more training data.

01:28:22.240 --> 01:28:26.240
You know, if GPT three was trained on less than one terabyte of data and, you know,

01:28:26.240 --> 01:28:29.240
YouTube is approaching like the Yota bite scale, right?

01:28:29.240 --> 01:28:31.240
That's, that's an insane amount of data.

01:28:31.240 --> 01:28:34.240
So, okay, let's say you feed that in.

01:28:34.240 --> 01:28:38.240
And so you got audio video, you've got text, you've got all the comments and you end up

01:28:38.240 --> 01:28:42.240
with like a model trained on, on all of YouTube data.

01:28:42.240 --> 01:28:43.240
Okay, cool.

01:28:43.240 --> 01:28:44.240
What can you do with that?

01:28:44.240 --> 01:28:47.240
Like, I can't even imagine, right?

01:28:47.240 --> 01:28:52.240
Because GPT three today is almost capable of writing screenplays.

01:28:52.240 --> 01:28:53.240
Right?

01:28:53.240 --> 01:28:58.240
So if you have a model that's trained on, you know, all text data, all audio data, all

01:28:58.240 --> 01:29:01.240
video data, you say, Hey, write me a screenplay.

01:29:01.240 --> 01:29:02.240
Right?

01:29:02.240 --> 01:29:06.240
I actually, I, and near the end of my book, I kind of have a chapter of speculation.

01:29:06.240 --> 01:29:12.240
And I say, what if, what if you have this model and you say, give me season two of Firefly?

01:29:12.240 --> 01:29:13.240
Right?

01:29:13.240 --> 01:29:17.240
Like, you know, you could, you could just keep watching whatever show you want.

01:29:17.240 --> 01:29:21.240
You say, give me Game of Thrones, but give me a different, you know, season eight, give

01:29:21.240 --> 01:29:24.240
me season, you know, different season eight and season nine and 10.

01:29:24.240 --> 01:29:25.240
Right?

01:29:25.240 --> 01:29:29.240
So I kind of imagine that one possibility is hyper personalized entertainment.

01:29:29.240 --> 01:29:33.240
And of course, like that might be 30 years away just because of like you said, the energy

01:29:33.240 --> 01:29:35.240
intensity of this task.

01:29:35.240 --> 01:29:38.240
But I, conceptually, it's possible, right?

01:29:38.240 --> 01:29:40.240
You can hop on GPT three today.

01:29:40.240 --> 01:29:45.240
Use the instruct model and say, write a screenplay for, you know, Firefly season two, and it'll

01:29:45.240 --> 01:29:47.240
try, it'll get close.

01:29:47.240 --> 01:29:52.240
And so then if you can take that text output and feed it into a multimodal model that can

01:29:52.240 --> 01:29:54.240
translate text to video, why not?

01:29:54.240 --> 01:29:59.240
You know, and Adobe actually, I don't know if you've seen it, but Adobe is, is already starting

01:29:59.240 --> 01:30:04.240
on that where they're like inferencing, um, like, uh, what, what's the term?

01:30:04.240 --> 01:30:08.240
They're like imputing the sound so you can put in a soundless video and it'll generate

01:30:08.240 --> 01:30:10.240
the audio sound effects for you or vice versa.

01:30:10.240 --> 01:30:11.240
It's really cool.

01:30:11.240 --> 01:30:18.240
And so I think it like a company like Adobe that they have a huge vested interest in mastering

01:30:18.240 --> 01:30:20.240
audio visual technologies.

01:30:20.240 --> 01:30:24.240
They might, you know, soon put out something where, you know, you put in a text description

01:30:24.240 --> 01:30:26.240
and it'll give you like a three second clip, right?

01:30:26.240 --> 01:30:28.240
So that you can use that for ad copy.

01:30:28.240 --> 01:30:31.240
Well, this technology is going to continue improving over time.

01:30:31.240 --> 01:30:36.240
So I kind of, I kind of see that as like, if I were Netflix, put it this way.

01:30:36.240 --> 01:30:42.240
If I had the budget of Netflix or Amazon, I would be investing in this to, to, to write

01:30:42.240 --> 01:30:48.240
hyper personalized, um, video, uh, like series or, uh, or novels, right?

01:30:48.240 --> 01:30:51.240
Cause you know, Amazon's got the market cornered with Kindle, right?

01:30:51.240 --> 01:30:55.240
And there's people that will read all day, every day, right?

01:30:55.240 --> 01:31:00.240
There are people that consume every bit of like entertainment that's available.

01:31:00.240 --> 01:31:05.240
So if you can generate that on the fly without, you know, having a studio, a big budget studio,

01:31:05.240 --> 01:31:08.240
that would be, I mean, that would change entertainment.

01:31:08.240 --> 01:31:10.240
You know, that, that's the metaverse.

01:31:10.240 --> 01:31:11.240
Forget what Facebook is doing.

01:31:11.240 --> 01:31:15.240
That's the metaverse where it's like, Hey, you know, I, I came up with my own idea for

01:31:15.240 --> 01:31:20.240
Game of Thrones and I, I wrote, you know, I use this, uh, you know, GPT eight or whatever

01:31:20.240 --> 01:31:22.240
to generate my own version of Game of Thrones.

01:31:22.240 --> 01:31:23.240
Come watch it with me guys.

01:31:23.240 --> 01:31:26.240
And you know, someone might say, Oh, I didn't like that ending.

01:31:26.240 --> 01:31:28.240
And they go rewrite it and generate their own version.

01:31:28.240 --> 01:31:30.240
You know, cause we share memes on the internet today.

01:31:30.240 --> 01:31:34.240
What if instead of sharing memes on the internet, we end up sharing episodes of our favorite

01:31:34.240 --> 01:31:40.240
anime or, you know, we, we, uh, Resurrect Battlestar Galactica, you know, whatever.

01:31:40.240 --> 01:31:42.240
There's so many things that we could do.

01:31:42.240 --> 01:31:47.240
Like if compute power was not a problem, then we get there, but we need like fusion

01:31:47.240 --> 01:31:49.240
reactors to power this stuff.

01:31:49.240 --> 01:31:50.240
Yeah.

01:31:50.240 --> 01:31:51.240
Yeah.

01:31:51.240 --> 01:31:56.240
And like Marvel for me is already kind of like this and my capacity to consume Marvel

01:31:56.240 --> 01:31:59.240
as a, as a viewer, it appears is infinite.

01:31:59.240 --> 01:32:00.240
So I'm excited.

01:32:00.240 --> 01:32:04.240
I've called it in the past, like the multimodal Marvel cinematic universe.

01:32:04.240 --> 01:32:05.240
Yeah.

01:32:05.240 --> 01:32:10.240
And I, some of these shows like Loki, I don't know if you, if you watch like,

01:32:10.240 --> 01:32:11.240
I haven't seen it yet.

01:32:11.240 --> 01:32:12.240
Okay.

01:32:12.240 --> 01:32:13.240
Okay.

01:32:13.240 --> 01:32:14.240
I mean, it was six episodes.

01:32:14.240 --> 01:32:18.240
If the, if it had been 30, I would have watched all 30 and enjoyed every moment of it.

01:32:18.240 --> 01:32:21.240
If that quality was, I want to go deeper in these stories.

01:32:21.240 --> 01:32:26.240
So I'm definitely excited for all my favorite universes, cinematic universes and

01:32:27.240 --> 01:32:32.240
story wise as well to, to live on forever essentially through multimodal content and

01:32:32.240 --> 01:32:35.240
maybe be personalized like you're describing as well.

01:32:35.240 --> 01:32:36.240
Oh yeah.

01:32:36.240 --> 01:32:37.240
So yeah.

01:32:37.240 --> 01:32:38.240
Last question.

01:32:38.240 --> 01:32:41.240
So we, you know, you know, we've talked about various things.

01:32:41.240 --> 01:32:47.240
We've talked about codex by today, you know, multimodal stuff broadly.

01:32:47.240 --> 01:32:49.240
Where do you see all of this stuff going?

01:32:49.240 --> 01:32:51.240
Let's give a timeline, five, 10 years.

01:32:51.240 --> 01:32:54.240
What are some of the, what's the direction we're heading towards?

01:32:54.240 --> 01:32:56.240
What, what important capabilities will we have?

01:32:56.240 --> 01:32:58.240
Why is this stuff important?

01:32:58.240 --> 01:32:59.240
Yeah.

01:32:59.240 --> 01:33:03.240
Um, five to 10 years from now, I think that we will have something that you could probably

01:33:03.240 --> 01:33:08.240
call a fully functional AGI like as a service you could sign up for.

01:33:08.240 --> 01:33:12.240
Um, you know, it might be chatbot based, you know, kind of based on natural language,

01:33:12.240 --> 01:33:13.240
cognitive architecture.

01:33:13.240 --> 01:33:16.240
Um, I calculated out like it's too expensive to run right now.

01:33:16.240 --> 01:33:21.240
You know, if it's $30 for a, for a 10 minute conversation, that's way too expensive.

01:33:21.240 --> 01:33:23.240
So the, the, the cost has to come down.

01:33:23.240 --> 01:33:27.240
You know, if you just, if you just take the technology we have today, but make it cheaper,

01:33:27.240 --> 01:33:29.240
there's so much potential.

01:33:29.240 --> 01:33:33.240
Um, so, you know, then there was that idea about like self-improving, um, you know,

01:33:33.240 --> 01:33:35.240
feedback loops, you know, integrating with DevOps.

01:33:35.240 --> 01:33:40.240
I certainly think that a company like Atlassian, which is a major DevOps player,

01:33:40.240 --> 01:33:44.240
um, probably within five to 10 years, they'll have something, um, integrated to,

01:33:44.240 --> 01:33:48.240
to kind of help automate the development pipeline even further.

01:33:48.240 --> 01:33:53.240
Um, I think that, of course, I could be wrong because we're kind of at this weird

01:33:53.240 --> 01:33:54.240
acceleration point.

01:33:54.240 --> 01:34:00.240
Um, I think, I feel like multimodal models like consumer grade multimodal models are

01:34:00.240 --> 01:34:02.240
probably more than 10 years away.

01:34:02.240 --> 01:34:06.240
Um, unfortunately, they're probably just going to be like toy sized because,

01:34:06.240 --> 01:34:10.240
you know, there's, um, there's like a, a, a hypnogram, right?

01:34:10.240 --> 01:34:12.240
I don't know if you've seen that one, but that's one of like the text to image

01:34:12.240 --> 01:34:16.240
generators and they're, it's still not even photorealistic, right?

01:34:16.240 --> 01:34:21.240
Getting a photorealistic text to image is still like, that's a little ways off.

01:34:21.240 --> 01:34:24.240
And then the next step after that is text to video.

01:34:24.240 --> 01:34:26.240
That's even further, right?

01:34:26.240 --> 01:34:28.240
So that's, that's kind of where I think it's at.

01:34:28.240 --> 01:34:30.240
I don't think we're going to hit an AI winter.

01:34:30.240 --> 01:34:33.240
I know there's lots of people predicting that we're going to hit an AI winter,

01:34:33.240 --> 01:34:37.240
but I think that we're actually still kind of in the acceleration point.

01:34:37.240 --> 01:34:40.240
But again, I don't know if it's going to follow an exponential curve forever

01:34:40.240 --> 01:34:42.240
or if it's a sigmoid curve.

01:34:42.240 --> 01:34:45.240
So time will tell.

01:34:45.240 --> 01:34:46.240
Yep.

01:34:46.240 --> 01:34:49.240
And still lots to do in the meantime, like you're describing, even with UPT3.

01:34:49.240 --> 01:34:50.240
Oh yeah.

01:34:50.240 --> 01:34:51.240
Okay.

01:34:51.240 --> 01:34:52.240
Yeah.

01:34:52.240 --> 01:34:55.240
My, my answer is, I think all of this stuff is, is just converging to just

01:34:55.240 --> 01:34:57.240
greater human potential.

01:34:57.240 --> 01:35:00.240
In some sense, I'm not even necessarily interested in the AGI question,

01:35:00.240 --> 01:35:02.240
although I think it's important.

01:35:02.240 --> 01:35:06.240
I think just the, the exciting possibilities we'll have, even now that we have,

01:35:06.240 --> 01:35:09.240
that we'll continue to have five to 10 years from now.

01:35:09.240 --> 01:35:13.240
Um, so many more experiences, so many other things we'll be able to create

01:35:13.240 --> 01:35:14.240
that weren't possible.

01:35:14.240 --> 01:35:16.240
I think we'll have more people creating than ever before.

01:35:16.240 --> 01:35:20.240
Um, it's a really, really exciting vision for humanity.

01:35:20.240 --> 01:35:21.240
Right.

01:35:21.240 --> 01:35:22.240
Not just, not just for you and I.

01:35:22.240 --> 01:35:27.240
So anyways, so with that said, did you have anything you wanted to plug David?

01:35:27.240 --> 01:35:29.240
Where can people find you online?

01:35:29.240 --> 01:35:30.240
Yeah.

01:35:30.240 --> 01:35:34.240
So, um, my personal site is a David K Shapiro dot com.

01:35:34.240 --> 01:35:38.240
Um, and I've got, um, I have, I have a few projects up and coming.

01:35:38.240 --> 01:35:41.240
Um, nothing out right now except for my book,

01:35:41.240 --> 01:35:43.240
natural language, cognitive architecture.

01:35:43.240 --> 01:35:46.240
Um, you can download it for free from my website.

01:35:46.240 --> 01:35:48.240
Um, you can sign up for my newsletter.

01:35:48.240 --> 01:35:51.240
So one of my upcoming books is called benevolent by design,

01:35:51.240 --> 01:35:53.240
six words to safeguard humanity,

01:35:53.240 --> 01:35:56.240
which is to address the control problem of AGI.

01:35:56.240 --> 01:36:01.240
Um, so that's, that's, uh, that book should hopefully be out in the next six months or so.

01:36:01.240 --> 01:36:04.240
Um, and that is, um, so that's one project.

01:36:04.240 --> 01:36:06.240
I've got another nonfiction book.

01:36:06.240 --> 01:36:09.240
Um, and then also my own podcast will be coming out soon.

01:36:09.240 --> 01:36:10.240
Yeah.

01:36:10.240 --> 01:36:13.240
Head over to my site, David K Shapiro dot com and sign up for my newsletter.

01:36:13.240 --> 01:36:17.240
And you'll get, you'll get updated when these, when these come out, when they're available.

01:36:17.240 --> 01:36:18.240
Awesome.

01:36:18.240 --> 01:36:21.240
And David, you mentioned you're, you're looking for collaborators as well.

01:36:21.240 --> 01:36:22.240
Yes.

01:36:22.240 --> 01:36:23.240
For natural language, cognitive architecture.

01:36:23.240 --> 01:36:29.240
So, uh, if you're a coder, you know, imagine product manager, researcher, uh, hit up David

01:36:29.240 --> 01:36:32.240
and just connect if, if any of this stuff interests you.

01:36:32.240 --> 01:36:38.240
Um, I've, I've, I asked, I spent like a couple, I think I spent like a few days trying to find you on Twitter.

01:36:38.240 --> 01:36:40.240
I don't think you're quite on Twitter yet.

01:36:40.240 --> 01:36:47.240
Uh, I encourage you, uh, David, of course, you know, you and I will connect after we'll put any other, other place people could connect with you.

01:36:47.240 --> 01:36:48.240
There's the community forums.

01:36:48.240 --> 01:36:50.240
I assume you have a GitHub account.

01:36:50.240 --> 01:36:51.240
Yeah.

01:36:51.240 --> 01:36:54.240
So we're going to put that in, in the show notes and in the YouTube description below.

01:36:54.240 --> 01:36:57.240
Uh, so anyways, David, thank you so much for being here.

01:36:57.240 --> 01:37:06.240
I wanted to personally thank you for all the awesome, awesome community contributions you've made on the open eye community forum.

01:37:06.240 --> 01:37:09.240
Uh, you're just an essential person on there.

01:37:09.240 --> 01:37:10.240
I've learned a lot from you.

01:37:10.240 --> 01:37:14.240
Uh, you know, the insights you've shared, they're going to be there forever.

01:37:14.240 --> 01:37:16.240
And I'm sure I can't imagine how many people you've helped.

01:37:16.240 --> 01:37:23.240
Uh, and also about your book, I also just wanted to say to the audience, uh, David's done a great job making it really digestible.

01:37:23.240 --> 01:37:27.240
Like it was a very, it was a breeze of a read.

01:37:27.240 --> 01:37:32.240
I thoroughly enjoyed it as somebody who writes GPT three prompts and is into this ecosystem.

01:37:32.240 --> 01:37:42.240
It was just, uh, very interesting to see, uh, how it, how it could be laid out, uh, in this broader system approaching this huge problem.

01:37:42.240 --> 01:37:45.240
Um, and also I was able to even get the book for free.

01:37:45.240 --> 01:37:49.240
Obviously I encourage people by the book support it, but it's, it's there.

01:37:49.240 --> 01:37:50.240
It's ready.

01:37:50.240 --> 01:37:52.240
I think, you know, David's goal here is to get the ideas out.

01:37:52.240 --> 01:37:57.240
Um, and so, uh, anyways, so, uh, that's it for today's episode.

01:37:57.240 --> 01:37:58.240
David, thank you so much again.

01:37:58.240 --> 01:37:59.240
I really appreciate you being here.

01:37:59.240 --> 01:38:00.240
Thank you.

01:38:00.240 --> 01:38:03.240
Thank you for all the kind comments and, um, and you're quite welcome.

01:38:03.240 --> 01:38:04.240
And so is everyone else.

01:38:04.240 --> 01:38:06.240
That's why I'm here.

01:38:06.240 --> 01:38:07.240
Awesome.

01:38:07.240 --> 01:38:16.240
And, uh, quick, so my quick plugs, you know, at BAKZT future Twitter, Instagram, YouTube.com slash BAKZT future.

01:38:16.240 --> 01:38:22.240
My newsletter, I'll put it in the description below and I have a Twitter spaces event coming in two days at noon.

01:38:22.240 --> 01:38:24.240
Uh, a couple of people probably pulling up.

01:38:24.240 --> 01:38:26.240
This is like a audio only event.

01:38:26.240 --> 01:38:32.240
So I encourage audio podcast listeners, YouTube subscribers, pull up to the Twitter spaces event.

01:38:32.240 --> 01:38:37.240
We're going to chat more about codecs and prompt design and some other stuff going on in the, in the space.

01:38:37.240 --> 01:38:41.240
So anyways, thank you again for listening to multimodal by BAKZT future.

01:38:41.240 --> 01:38:42.240
I'll catch you in the next one.

01:38:42.240 --> 01:38:43.240
Bye.

