1
00:00:00,000 --> 00:00:07,440
So there's a thing that I see people routinely fail to get and it's a little hard to express.

2
00:00:09,120 --> 00:00:09,920
I'm just going to try.

3
00:00:12,320 --> 00:00:19,760
There's no rule which says that we make it. There's no rule which says that humanity has

4
00:00:19,760 --> 00:00:24,160
to survive this century or even this decade. There's no rule that says that everything's going

5
00:00:24,160 --> 00:00:29,200
to be okay. And when I say this to people they generally accept it. Most of the people I talk

6
00:00:29,360 --> 00:00:33,280
to anyway. They say, yeah of course. Of course there's no rule that says that we have to make it.

7
00:00:33,280 --> 00:00:37,280
Of course humanity could fail. We could go extinct. It could happen.

8
00:00:39,920 --> 00:00:46,320
It's like they know it, but they don't feel it. They don't take it seriously as an actual

9
00:00:46,320 --> 00:00:51,200
possibility. They kind of think, okay we have these things that we have to do. We have these

10
00:00:51,200 --> 00:00:58,720
challenges we have to overcome and so we'll do that. We'll rise to the occasion. We'll figure out

11
00:00:58,800 --> 00:01:05,520
what we need to do and we'll do it. But okay, this is a related concept. There's no rule which

12
00:01:05,520 --> 00:01:10,800
says that the challenges we're faced with are challenges that we are capable of meeting.

13
00:01:12,560 --> 00:01:16,800
Think about something like an asteroid strike. If a big enough asteroid hits earth we're pretty

14
00:01:16,800 --> 00:01:20,240
much done for, right? And I'm not saying that this is something we should day to day be concerned

15
00:01:20,240 --> 00:01:25,760
about because we're dealing with geological periods of time, right? Regularly asteroids hit

16
00:01:25,760 --> 00:01:33,120
earth. We're sort of due for one, but what does that really mean? Sometime in the next

17
00:01:33,920 --> 00:01:38,000
million years or whatever it is. So we have more pressing concerns, but

18
00:01:39,680 --> 00:01:44,480
when people are prepared to take seriously the possibility of an asteroid strike,

19
00:01:45,040 --> 00:01:49,520
usually it's because they're thinking about it like, oh interesting. Okay, so we have good

20
00:01:49,520 --> 00:01:54,720
telescopes. We would spot the thing. I wonder how long we would have. Would that be enough warning?

21
00:01:54,720 --> 00:01:59,280
Would that be enough time for us to build whatever it is that we needed to build to go up there

22
00:01:59,280 --> 00:02:04,320
and redirect it or blow it up or whatever? So they're willing to think about the possibility

23
00:02:05,440 --> 00:02:12,240
from the perspective of tackling the challenge. But the thing is, as I said, the periods of time

24
00:02:12,240 --> 00:02:18,400
that pass between asteroid strikes are so large that human-scale time periods don't really matter.

25
00:02:18,400 --> 00:02:22,080
The chance of an asteroid strike 200 years from now is more or less the same as it is now.

26
00:02:22,720 --> 00:02:26,960
And similarly, the chance of an asteroid strike 200 years ago is about the same as it is now.

27
00:02:29,040 --> 00:02:35,600
And if you go back a few hundred years, the question becomes very different.

28
00:02:37,440 --> 00:02:42,320
Could we see the asteroid coming? Maybe. Could we do anything about it?

29
00:02:43,440 --> 00:02:48,080
No. I mean, pretty much just flatly no. We didn't have the technology. We didn't have

30
00:02:48,080 --> 00:02:51,840
the understanding of the thing necessary to build something that could deal with that.

31
00:02:54,160 --> 00:02:59,120
If an asteroid were headed for Earth a few hundred years ago, that would just be it.

32
00:03:00,080 --> 00:03:05,840
Just game over. Humanity's done. Shame. You know, you didn't quite develop your technology fast

33
00:03:05,840 --> 00:03:10,560
enough to have anything that could deal with asteroids before one happened to hit you. Oh, well.

34
00:03:11,280 --> 00:03:19,920
There is no rule which says that the challenges the universe faces us with are challenges we can face.

35
00:03:21,920 --> 00:03:25,680
And I hope that kind of gives some taste of what I'm talking about,

36
00:03:25,680 --> 00:03:31,040
of like understanding the possibility that we could be currently faced with challenges

37
00:03:31,040 --> 00:03:36,160
that we will not rise to. That we actually could fail.

38
00:03:36,160 --> 00:03:42,320
But people just have a very hard time taking seriously the possibility that we could fail.

39
00:03:43,680 --> 00:03:49,600
I think it comes from a few different places. One place obviously is religion, right? If we

40
00:03:49,600 --> 00:03:55,600
were created deliberately by the creator of the universe, we are important in the universe.

41
00:03:56,880 --> 00:04:00,880
And so we're not going to be wiped out by some asteroid or whatever. We're not going to be wiped

42
00:04:00,880 --> 00:04:09,120
out in some boring way. Religions, they're often okay with the idea of an apocalypse,

43
00:04:09,120 --> 00:04:15,680
of humanity or of the world ending. But they're not okay with an accidental apocalypse.

44
00:04:15,680 --> 00:04:20,160
They have a very hard time taking that kind of thing seriously. In a religious apocalypse,

45
00:04:20,160 --> 00:04:23,760
it's always part of the plan.

46
00:04:24,720 --> 00:04:31,440
And so someone with a religious mindset, when faced with a scenario in which humanity is wiped

47
00:04:31,440 --> 00:04:40,400
out in a way which is not part of the plan, they have a very hard time taking that seriously.

48
00:04:41,200 --> 00:04:46,080
And I would posit that anyone raised in a society that's primarily religious probably has a lot

49
00:04:46,080 --> 00:04:52,880
of this as well. A lot of this kind of feeling that maybe we face a lot of challenges.

50
00:04:52,880 --> 00:04:58,480
Maybe we face very difficult challenges, but we'll deal with them. And if we don't,

51
00:04:58,480 --> 00:05:04,960
then that's okay because it's all part of the plan. Even if they would not explicitly endorse

52
00:05:04,960 --> 00:05:11,920
this idea, even if they claim to recognize the reality that there is no plan, I think those

53
00:05:11,920 --> 00:05:16,880
lingering ideas don't just immediately disappear when you give up on the supernatural parts of

54
00:05:16,880 --> 00:05:22,000
your religious beliefs and your upbringing. And it's not just capital R religion either, right?

55
00:05:22,000 --> 00:05:28,880
Like anyone who believes that human beings have souls or that human beings are in some way

56
00:05:28,880 --> 00:05:34,800
special, like fundamentally special, that the universe itself has like a separate category

57
00:05:34,800 --> 00:05:40,160
for human beings. Because again, there's this sense in which the universe on some level cares

58
00:05:40,160 --> 00:05:46,480
about us or even just recognizes us or acknowledges us as a thing. And if you believe that, again,

59
00:05:47,280 --> 00:05:54,560
it's difficult to think about the situations in which we're wiped out in some boring way with no

60
00:05:55,680 --> 00:06:06,080
dramatic final test, no heroic last stands, no judgments, no thrilling story to tell,

61
00:06:08,240 --> 00:06:13,920
and nobody to tell it to. So religion is one place that I think this comes from. The other

62
00:06:13,920 --> 00:06:18,640
place that it comes from, I think, is fiction, especially Hollywood and especially science

63
00:06:18,640 --> 00:06:23,120
fiction. And I'm not saying that people can't tell the difference between fiction and reality.

64
00:06:26,160 --> 00:06:29,200
I'm kind of saying that. I mean, on a conscious level, people can tell the difference between

65
00:06:29,200 --> 00:06:36,800
fiction and reality. But I think on some level, it's like people understand that whatever this

66
00:06:36,800 --> 00:06:42,720
fictional thing is, it didn't happen to us. It didn't happen here on earth. But it sort of

67
00:06:42,720 --> 00:06:47,600
happened somewhere. It happened in the world of the story, and that still feels real.

68
00:06:49,680 --> 00:06:55,920
It's kind of like, say you have two people, A and B. In person A says, you know, alcohol is

69
00:06:55,920 --> 00:07:01,680
actually a really harmful drug. I think we should ban alcohol in the UK. In person B might reply,

70
00:07:01,680 --> 00:07:08,160
well, they did try banning alcohol in the USA in the 1920s, and that didn't really work out

71
00:07:08,240 --> 00:07:12,560
at all the way they hoped it would. In person A would reply, well, yeah, but that was a different

72
00:07:12,560 --> 00:07:17,360
time. It was a different place. You know, modern Britain is very different from 1920s America,

73
00:07:17,360 --> 00:07:21,840
and we could implement it differently and so on. In person B would say, yeah, I mean,

74
00:07:21,840 --> 00:07:26,560
maybe there are important differences. But still, the fact that something similar was tried and

75
00:07:26,560 --> 00:07:31,600
failed is evidence against the idea, right? That's useful information to take into consideration.

76
00:07:32,880 --> 00:07:37,440
And that doesn't seem like an unreasonable position to take. But then people would do the

77
00:07:37,440 --> 00:07:43,280
same thing where somebody expresses something about AI, and somebody else says, yeah, but think

78
00:07:43,280 --> 00:07:50,080
about what happened in 2001, a space odyssey or iRobot or something. And you say to them, well,

79
00:07:50,080 --> 00:07:55,680
that was fiction. That didn't happen, right? That's a fictional example. It's different from the real

80
00:07:55,680 --> 00:08:02,000
world. It doesn't really make sense to use that. And they'll say, oh, I know, I know that it's

81
00:08:02,000 --> 00:08:07,120
fictional. But still, there's some sense in which this is like evidence, right?

82
00:08:09,200 --> 00:08:16,000
And no, it's not. I mean, it's, at best, it's one dude's best guess. And usually it's not even that,

83
00:08:16,000 --> 00:08:20,320
because usually the person is not trying to be as accurate as possible. They're trying to be entertaining.

84
00:08:22,480 --> 00:08:25,360
So clearly fiction affects the way people think about this.

85
00:08:27,200 --> 00:08:31,440
It's interesting. It seems like it increases the probabilities people assign to bad things

86
00:08:31,440 --> 00:08:37,360
happening with AI, but it also decreases the probabilities they assign to really, really

87
00:08:37,360 --> 00:08:43,520
bad things happening with AI. In the sense that they have in some part of their mind a whole

88
00:08:43,520 --> 00:08:50,080
bunch of examples in which bad things happened, but then we were able to overcome them, right?

89
00:08:50,080 --> 00:08:53,920
And this is not just with AI. This is any existential risk. Even asteroids, for example,

90
00:08:54,720 --> 00:09:00,480
coming back to them. Even though people often won't explicitly endorse this idea,

91
00:09:01,200 --> 00:09:06,320
I think there's a part of people which says, oh, well, I've seen the world threatened hundreds of

92
00:09:06,320 --> 00:09:14,560
times, right? And every time it works out. There's some, there's a brave hero or a plucky

93
00:09:14,560 --> 00:09:19,200
band of misfits or something comes along. We figure out what we need to do and then we do it.

94
00:09:19,200 --> 00:09:24,720
And I guess that's where we are, you know? And some of those people are even potentially

95
00:09:24,720 --> 00:09:29,840
helpful because they offer to join your plucky band of misfits. But they're still not able to

96
00:09:29,840 --> 00:09:37,200
take seriously the possibility that we might lose because they've never seen that happen,

97
00:09:38,080 --> 00:09:41,200
right? I mean, they've never seen it happen in fiction or very rarely seen it happen in fiction.

98
00:09:41,760 --> 00:09:47,200
And furthermore, they've never seen it happen in reality, obviously. That's kind of a difficult

99
00:09:47,200 --> 00:09:54,480
one because I feel like you should be able to use reality as evidence. But I mean, of course,

100
00:09:54,480 --> 00:09:59,600
we've never observed humanity being wiped out. Wouldn't be here talking about it if we had.

101
00:09:59,920 --> 00:10:08,800
And so it's hard to get people to really feel the reality of the situation,

102
00:10:10,000 --> 00:10:15,600
which is that we, the whole of humanity, we are up here on this tightrope.

103
00:10:17,760 --> 00:10:26,880
We've had no practice, no dry run, no rehearsal. We get one shot and there is no parachute.

104
00:10:26,880 --> 00:10:33,200
There's no safety net. There is no rule that says that we have to make it. We absolutely

105
00:10:33,200 --> 00:10:41,440
can fail. And we will if we don't successfully tackle a series of hard problems. And there's no

106
00:10:41,440 --> 00:10:47,280
rule that says that we're actually capable of tackling those problems. And there's no rule

107
00:10:47,280 --> 00:10:52,240
that says that we have enough time to do so. So we'd better get to work.

