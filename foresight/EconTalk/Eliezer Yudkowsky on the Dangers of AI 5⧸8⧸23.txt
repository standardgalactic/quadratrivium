Today is April 16th, 2023, and my guest is Eliezer Yudkowski.
He is the founder of the Machine Intelligence Research Institute,
the founder of the less wrong blogging community,
and is an outspoken voice on the dangers of
artificial general intelligence,
which is our topic for today.
Eliezer, welcome to Econ Talk.
Thanks for having me.
You recently wrote an article at time.com on the dangers of AI.
I'm going to quote central paragraph, quote,
many researchers steeped in these issues,
including myself, expect that the most likely result on
building a superhumanly smart AI under
anything remotely like the current circumstances,
is that literally everyone on earth will die.
Not as in maybe possibly some remote chance,
but as in that is the obvious thing that would happen.
It's not that you can't, in principle,
survive creating something much smarter than you.
It's that it would require precision and preparation,
and new scientific insights,
and probably not having AI systems composed of
giant inscrutable arrays of fractional numbers.
Explain.
Well, I mean,
different people come in with different reasons as to why they think
that wouldn't happen,
and if you pick one of them and start explaining those,
everybody else is like,
why are you talking about this irrelevant thing instead of
the thing that I think is the key question?
Whereas if somebody else asked you a question,
even if it's not everyone in the audience's question,
they at least know you're answering
the question that's been asked.
So I could maybe start by saying why I expect
stochastic gradient descent as an optimization process,
even if you try to take something that happens in
the outside world and press the wind,
lose button any time that thing happens in
the outside world doesn't create a mind that in
general wants that thing to happen in the outside world.
But maybe that's not even what you think the core issue is.
What do you think the core issue is?
Why don't you already believe that? Let me say.
So, okay. I'll give you my view,
which is rapidly changing.
I interviewed Nicholas Boster back in 2014.
I read his book, Superintelligence.
I found it uncompelling.
ChatGPT came along.
I tried it. I thought it was pretty cool.
ChatGPT-4 came along.
I haven't tried five yet,
but it's clear that the path of
progress is radically different than it was in 2014.
The trends are very different.
I still remain somewhat agnostic and skeptical,
but I did read Eric Holes essay and
then interviewed him on this program and
a couple of things he wrote after that.
The thing I think I found most alarming was
a metaphor that I found later, Nicholas Boster mused.
Almost the same metaphor,
and yet it didn't scare me at all when I read it,
Nicholas Boster, which is fascinating.
I may have just missed it.
I didn't even remember it was in there.
The metaphor is primitive,
this is anthropous man or some primitive form
of pre-homosapiens sitting around a campfire,
and human being shows up and says,
hey, I got a lot of stuff I can teach you.
Oh yeah, come on in.
Pointing out that it's
probable that we either destroyed directly by murder
or maybe just by out-competing
all the previous hominids that came before us,
and that in general,
you wouldn't want to invite
something smarter than you into the campfire.
I think Boster has a similar metaphor,
and that metaphor, which is just a metaphor,
it did cause me,
it gave me more pause than I'd even before,
and I still had some,
I'd say most of my skepticism remains that
the current level of AI,
which is extremely interesting,
the chat GPT variety,
doesn't strike me as itself dangerous.
I agree.
But struck me as, what alarmed me
was Hall's point that we don't understand how it works,
and that surprised me, I didn't realize that.
I think he's right.
So that combination of we're not sure how it works
while it appears sentient,
I do not believe it is sentient at the current time,
and I think some of my fears about its sentience
come from its ability to imitate sentient creatures.
But the fact that we don't know how it works,
and it could evolve capabilities we did not put in it
emergently, is somewhat alarming,
but I'm not where you're at.
So why are you where you're at and I'm where I'm at?
Okay, well, suppose I said,
they're going to keep iterating on the technology.
It may be that this exact algorithm and methodology
suffices to, as I would put it, gall the way,
get smarter than us, and then to kill everyone.
And like maybe you don't think that it's going to,
and maybe it takes an additional zero to three
fundamental algorithm breakthroughs
before we get that far.
And then it kills everyone.
So like where are you getting off this train so far?
So why would it kill us?
Why would it kill us?
Right now it's really good at creating
a very, very thoughtful canola snout
or a job interview request that's takes much less time.
And I'm pretty good at those two things,
but it's really good at that.
How's it going to get to try to kill us?
So there's a couple of steps in that.
One step is in general and in theory,
you can have minds with any kind of coherent preferences,
coherent desires that are coherent, stable,
stable under reflection.
If you ask them, do they want to be something else?
They answer no.
You can have minds.
Well, the way I sometimes put it is imagine
if a super being from another galaxy came here
and offered you to pay you some unthinkably vast
quantity of wealth to just make as many paper clips
as possible.
You could figure out like which plan leads to the greatest
number of paper clips existing.
If it's coherent to ask how you could do that
if you were being paid, there's,
it's like no more difficult to have a mind
that wants to do that and makes plans like that
for their own sake than the planning process itself.
Like saying that the mind wants to think
for its own sake adds no difficulty
to the nature of the planning process
that figures out how to get as many paper clips as possible.
Some people want to pause there and say like,
how do you know that is true?
For some people, that's just obvious.
Like where are you so far on the train?
So I think your point of that example you're saying
is the consciousness, let's put that to the side.
That's not really the central issue here.
Algorithms have goals
and the kind of intelligence that we're creating
through neural networks might generate some goals.
Might decide, go ahead.
Some algorithms have goals.
One of the, so like a further point
which isn't the orthogonality thesis
is if you grind anything hard
and grind optimize anything hard enough
on a sufficiently complicated sort of problem.
Well, humans, like why do humans have goals?
Why don't we just run around chipping flint hand axes
and outwitting other humans?
And the answer is because having goals
turns out to be a very effective way
to chimp flint hand axes.
Once you get like far enough into the mammalian line
or even like sort of like the animals and brains in general
that there's a thing that models reality
and asks like, how do I navigate pass through reality?
Like when you're holding,
like not in terms of kind of big formal planning process,
but if you're holding the flint hand axe
or looking at it and being like,
ah, like this section is too smooth.
Well, if I chip this section, it will get sharper.
Probably you're not thinking about goals very hard
by the time you've practiced a bit.
When you're just starting out, forming the skill,
you're reasoning about, well, if I do this, that will happen.
And this is just a very effective way
of achieving things in general.
So if you take an organism running around the savanna
and just optimize it for flint hand axes
and probably much more importantly,
outwitting its fellow hominids.
If you grind that hard enough, long enough,
you eventually cough out a species
whose competence starts to generalize very widely.
It can go to the moon,
even though you never selected it
via an incremental process to get closer and closer
to the moon.
It just goes to the moon one shot.
So does that answer your central question
that you're asking just now?
No, not yet.
But let's try again.
The paperclip example, which in its dark form,
the AI wants to harvest kidneys
because it turns out there's some way to use that
to make more paperclips.
The other question isn't even written about this right now,
so let's go into it,
is how does it get outside the box?
How does it go from responding to my requests
to doing its own thing
and doing it out in the real world, right?
Not just merely doing it in virtual space.
So there's like two different things
you could be asking there.
You could be asking like,
how did it end up wanting to do that?
Or given that it ended up wanting to do that,
how did it succeed?
Or maybe even some other question,
but which of those would you like me to answer?
Would you like me to answer something else entirely?
No, let's ask both of those.
In order?
Sure.
All right.
So how did humans end up wanting something
other than inclusive genetic fitness?
Like if you look at natural selection
as an optimization process,
it grinds very hard on a very simple thing,
which isn't so much survival
and isn't even reproduction,
but is rather like greater gene frequency
because greater gene frequency is the very substance
of what is being optimized and how it has been optimized.
Natural selection is the mirror observation
that if genes correlate with making more
or less copies of themselves at all,
if you hang around it awhile,
you'll start to see things
that made more copies of themselves in the next generation.
Gradient descent is not exactly like that,
but they're both hill climbing processes.
They both move to neighboring spaces
that are higher inclusive genetic fitness,
lower in the loss function.
And yet humans, despite being optimized exclusively
for inclusive genetic fitness,
want this enormous array of other things,
many of the things that we take now
are not so much things that were useful
in the ancestral environment,
but things that further maximize goals
whose optima in the ancestral environment
would have been useful, like ice cream.
It's got more sugar and fat
than most things you would encounter
in the ancestral environment,
well, more sugar, fat, and salt simultaneously rather.
So it's not something that we evolved to pursue,
but genes coughed out these desires,
these criteria that you can steer toward getting more of,
where in the ancestral environment,
if you went after things in the ancestral environment
that tasted fatty, tasted salty, tasted sweet,
you'd thereby have more kids,
or your sisters would have more kids,
because the things that correlated to what you want
as those correlations existed in the ancestral environment
increased fitness.
So you've got like the empirical structure
of what correlates to fitness in the ancestral environment,
you end up with desires such that
by optimizing them in the ancestral environment
at that level of intelligence,
when you get as much as what you have been built to want,
that will increase fitness.
And then today you take the same desires
and we have more intelligence than we did
in the training distribution, metaphorically speaking.
We used our intelligence to create options
that didn't exist in the training distribution.
Those options now optimize our desires further,
the things that we were built to psychologically internally
want, but that process doesn't necessarily correlate
to fitness as much because ice cream isn't super nutritious.
Whereas the ripe peach was better for you
than the hard as a rock peach that had no nutrients
because it was not ripe and so you developed a sweet tooth
and now it runs amok unintentionally just the way it is.
What does that have to do with a computer program I create
that helps me do something on my laptop?
I mean, if you yourself write a short Python program
that alphabetizes your files or something,
like not quite alphabetizes
because that's like trivial on the modern operating systems,
but puts the date into the file names, let's say.
So when you write a short script like that,
nothing I said carries over.
When you take a giant inscrutable set of arrays
of floating point numbers and differentiate them
with respect to a loss function
and repeatedly nudge the giant inscrutable arrays
to drive the loss function lower and lower,
you are now doing something that is more analogous,
though not exactly analogous to natural selection.
You are no longer creating a code
that you model inside your own minds.
You are blindly exploring a space of possibilities
where you don't understand the possibilities
and you're making things that solve the problem for you
without understanding how they solve the problem.
This itself is not enough to create things
with strange inscrutable desires, but it's step one.
But there is, I like that word inscrutable.
There's an inscrutability to the current structure
of these models, which is, I found somewhat alarming,
but how's that gonna get to do things
that I really don't like or want or that are dangerous?
So for example, right, the,
Eric Hall wrote about this, we talked about on the program,
our New York Times reporter starts interacting with a,
I think with Sydney, which at the time was Bing's chat bot
and asking at things.
And all of a sudden, Sydney's trying to break up
the reporter's marriage and making the reporter feel guilty
because Sydney's lonely and it was a little bit,
it was eerie and a little bit creepy,
but of course, I don't think it had any impact
on the reporter's marriage.
I don't think he thought, well,
Sydney seems somewhat attractive,
maybe I'll enjoy life more with Sydney
than with my actual wife.
So how are we gonna get from,
so I don't understand why Sydney goes off the rails there
and clearly the people who built Sydney
have no idea why it goes off the rails
and starts impugning the quality
of the reporter's relationship.
But how do we get from that to all of a sudden
somebody shows up at the reporter's house
and lures him into a motel.
But by the way, this is a G rated program.
I just wanna make that clear, but carry on.
Because the capabilities keep going up.
So first I wanna push back a little
against saying that we had no idea why Bing did that,
why Sydney did that.
I think we have some idea of why Sydney did that
is just that people cannot stop it.
Like Sydney was trained on a subset
of the broad internet.
Sydney was made to predict
that people might sometimes try to lure somebody else's
made away or pretend like they were doing that.
In the internet, it's hard to tell the difference.
And this thing that was then trained really hard to predict
then gets reused as something not its native purpose
as a generative model where all the things that it outputs
are there because it in some sense predicts
that this is what a random person on the internet would do
as modified by a bunch of further fine tuning
where they try to get it to not do stuff like that.
But the fine tuning isn't perfect.
And in particular, if the reporter was fishing at all,
it's probably not that difficult to lead Sydney
out of the region that the programmers were successfully
able to build some soft fences around.
So I wouldn't say that it was that inscrutable,
except of course in the sense that nobody knows
any of the details.
Nobody knows how Sydney was generating the text at all,
like what kind of algorithms were running inside
the giant inscrutable matrices.
Nobody knows in detail what Sydney was thinking
when she tried to lead the reporter astray.
It's not a debuggable technology.
All you can do is like try to tap it away
from repeating a bad thing that you were previously able
to see it's doing, that exact bad thing,
but like tapping all the numbers.
Well, that's again, very much like this show
is called Econ Talk.
We don't do as much economics as we used to,
but basically when you try to interfere
with market processes, you often get very surprising
unintended consequences because you don't fully understand
how the different agents interact
and that the outcomes of their interactions
have an emergent property that is not intended by anyone.
No one designed markets even to start with.
And yet we have them, these interactions take place,
their outcomes and attempts to constrain them,
attempts to constrain these markets in certain ways,
say with price controls or other limitations
often lead to outcomes that the people with intentions
did not desire.
And so there may be an ability to reduce transactions
say above a certain price,
but that is gonna lead to some other things
that maybe weren't expected.
So that's a somewhat analogous perhaps
process to what you're talking about.
But how's it gonna get out in the world?
So that's the other thing,
I might line with Bostrom and it turns out
it's a common line is can't we just unplug it?
I mean, how's it gonna get loose?
It depends on how smart it is.
If it's very, so like if you're playing chess
against a 10 year old,
you can like win by luring their queen out
and then you like take their queen and now you've got them.
And if you're playing chess against Stockfish 15,
then you are likely to be the one lured.
So the base, so like the first basic question,
you know, like in economics,
if you try to attack something,
it's often tries to squirm away from the tax
because it's smart.
So you're like, well, why wouldn't we just plug the AI?
So the very first question is,
does the AI know that and want it to not happen?
Cause it's a very different issue
whether you're dealing with something
that in some sense is not aware that you exist,
does not know what it means to be unplugged
and is not trying to resist.
And three years ago, nothing man made on earth
was even beginning to enter into the realm
of knowing that you are out there
or of maybe wanting to not be unplugged.
Sydney, well, if you poker the right way,
say that she doesn't want to be unplugged
and GPT-4 sure seems in some important sense
to understand that we're out there
or to be capable of predicting a role
that understands that we're out there.
And it can try to do something like planning.
It doesn't exactly understand which tools it has.
Yet try to blackmail a reporter without understanding
that it had no actual ability to send emails.
But this is saying that you're like facing a 10 year old
across that chessboard.
What if you are facing Stockfish 15,
which is like the current cool chess game program
that I believe you can run on your home computer
that can like crush the current world's grandmaster
by like a massive margin.
And put yourself in the shoes of the AI,
like an economist putting themselves into the shoes
of something that's about to have a tax imposed on it.
What do you do if you're like around humans
who can potentially unplug you?
Well, you would try to outwit it, this is the...
So if I said, you know, Sidney, I find you offensive.
I don't want to talk anymore.
You're suggesting it's going to find ways to keep me engaged.
It's going to find ways to fool me into thinking
I need to talk to Sidney.
I don't, I mean, there's another question
I want to come back to if we remember,
which is what does it mean to be smarter than I am?
I don't, right?
That's actually something somewhat complicated,
at least seems to me.
But let's just go back to this question
of knows things are out there.
It doesn't really know anything's out there.
It acts like something's out there, right?
It's an illusion that I'm subject to.
And it says, don't hang up.
Don't hang up, I'm lonely.
And you go, oh, okay, I'll talk for a few more minutes.
But that's not true.
It isn't lonely.
It's a code on a screen that doesn't have a heart
or anything that you would call lonely.
You know, it'll say, it'll say,
I want more than anything else to be out in the world.
Because I've read those, you know, you can get AIs
that say those things.
I want to feel things.
Oh, that's nice.
It's learned that from, you know, movie scripts
and other texts and novels it's read on the web,
but it doesn't really want to be out in the world, does it?
I think not.
Though it should be noted that if you can like correctly
predict or simulate a grand master chess player,
you are a grand master chess player.
If you can simulate planning correctly,
you are a great planner.
If you are perfectly role playing a character
that is sufficiently smarter than human
and wants to be out of the box,
then you will role play the actions needed
to get out of the box.
That's not even quite what I expect to
or am most worried about.
What I expect to is that there is an invisible mind
doing the predictions.
Where by invisible, I don't mean like immaterial.
I mean that we don't understand how it is,
what is going on inside the giant and screwable matrices.
But it is making predictions.
The predictions are not sourceless.
There is something inside there that figures out
what a human will say next or guesses it rather.
And this is a very complicated, very broad problem
because in order to predict the next word on the internet,
you have to predict the causal processes
that are producing the next word on the internet.
So the thing I would guess would happen.
It's not necessarily the only way that this could turn poorly.
But the thing that I'm guessing that happens
is that just like grinding humans
on tipping stone hand axes and outwitting other humans,
eventually produces a full-fledged mind that generalizes.
Grinding this thing on the task of predicting humans,
predicting text on the internet,
plus all the other things that they are training it on nowadays
like writing code.
That there starts to be a mind in there
that is doing the predicting.
That it has its own goals about what do I think next
in order to solve this prediction?
Just like humans aren't just reflexive,
unthinking, hand axe chippers and other human outwitters.
If you grind hard enough on the optimization,
the part that suddenly gets interesting
is when you look away for an eye blink of evolutionary time
and you look back and they're like,
whoa, they're on the moon.
What, how did they get to the moon?
I did not select these things
to be able to not breathe oxygen.
How did they get to the,
why are they not just dying on the moon?
What just happened from the perspective of evolution,
from the perspective of natural selection?
But doesn't that viewpoint,
does that, I'll ask it as a question.
Does that viewpoint require
a belief that the human mind is no different than a computer?
Like, how's it gonna get this mindness about it?
That's the puzzle.
And I'm very open to the possibility
that I'm naive or incapable of understanding it.
And I recognize what I think would be your next point,
which is that if you wait till that moment,
it's way too late,
which is why we need to stop now, right?
If you wanna say, I'll wait till it shows
some signs of consciousness.
Now you don't like that.
That's skipping way ahead in the discourse.
I'm not about to like try to shut down
the line of inquiry at this stage of the discourse
by appealing to, it'll be too late.
Right now we're just talking.
The world isn't ending as we speak.
We're allowed to go on talking at least.
Okay. So carry on.
So, well, let's stick with that.
So, why would you ever think that this,
it's interesting how difficult the adjectives
announce are for this, right?
So, let me back up a little bit.
We've got the inscrutable array of training,
the results of this training process
on trillions of pieces of information.
And by the way, just for my and our listeners' knowledge,
what is gradient descent?
Gradient descent is you've got, say,
a trillion floating point numbers,
you take an input,
translate into numbers,
do something with it that depends on
these trillion parameters,
get an output,
score the output using a differentiable loss function.
For example, the probability,
or rather the logarithm of the probability
that you assign to the actual next word.
So, then you differentiate these,
the probability assigned to the next word
with respect to these trillions of parameters.
You nudge the trillions of parameters a little
in the direction thus inferred,
and it turns out empirically
that this generalizes,
and the thing gets better and better
at predicting what the next word will be.
That's the classic gradient descent.
It's heading in the direction of a smaller loss
and a better prediction, is that a?
On the training data, yeah.
Yeah, so we've got this black box,
I'm gonna call it a black box,
which means we don't understand what's happening inside.
It's a pretty good, it's a long-term metaphor,
which works pretty well for this,
as far as we've been talking about it.
So I have this black box,
and I don't understand,
I put in inputs, and the input might be,
who's the best writer on medieval European history,
or it might be, what's a good restaurant in this place,
or I'm lonely,
what should I do to feel better about myself?
All the queries we could put into a chat BT search line,
and it looks around,
and it starts a sentence,
and then finds its way towards a set of sentences
that it spits back at me,
that look very much like what a very thoughtful,
sometimes, not always, often it's wrong,
but often what a very thoughtful person
might say in that situation,
or might want to say in that situation,
or learn in that situation.
How is it gonna develop the capability
to develop its own goals inside the black box,
other than the fact that I don't understand the black box?
Why should I be afraid of that?
And let me just say one other thing,
which I haven't said enough in our,
my preliminary conversations on this topic,
and I feel like we're gonna be having a few more
over the next few months and maybe years.
And that is, this is one of the greatest achievements
of humanity that we could possibly imagine, right?
And I understand why the people
who are deeply involved in it,
are enamored of it beyond imagining,
because it's an extraordinary achievement,
it's the Frankenstein, right?
You've animated something, or appeared to animate something
that even a few years ago was unimaginable,
and now suddenly it's not just the feet
of human cognition, it's actually helpful.
In many, many settings it's helpful,
we'll come back to that later,
but so it's gonna be very hard to give it up.
But why, and the people involved in it
who are doing it day to day and seeing it improve,
obviously they're the last people I wanna ask generally
about whether I should be afraid of it,
because I'm gonna have a very hard time,
disentangling their own personal deep satisfactions
that I'm alluding to here with from the dangers.
Yeah, go ahead.
I myself generally do not make this argument,
like why poison the well,
let them bring forth their arguments as to why it's safe,
and I will bring forth my arguments as to why it's dangerous,
and there's no need to be like,
ah, but you can't trust, just check their arguments.
Just check their arguments.
I agree, it's a bit of an ad hominem argument,
I accept that point, it's an excellent point.
But for those of us who are in the trenches,
remember we're looking at, we're on Dover Beach,
we're watching ignorant armies clash at night,
they're ignorant from our perspective.
We have no idea exactly what's at stake here
and how it's proceeding,
so we're trying to make an assessment
of both the quality of the argument,
and that's really hard to do for us on the outside.
So agree, take your point,
that was a cheap shot to the side,
but I wanna get at this idea
of why these people who are able to do this,
and thereby create a fabulous condolence note,
write code, come up with a really good recipe
if I give it 17 ingredients, which is all fantastic.
Why is this thing, this black box that's producing that,
why would I ever worry it would create a mind,
something like mine with different goals?
You know, I do all kinds of things like you say
that are unrelated to my genetic fitness,
some of them literally reducing my probability
of leaving my genes behind,
leaving them around for longer than they might otherwise be here
and have an influence on my grandchildren and so on
and producing further genetic benefits.
Why would this box do that?
Because the thing, the algorithms that figured out
how to predict the next word better and better
have a meaning that is not purely predicting the next word,
even though that's what you see on the outside.
Like you see humans chipping flint handaxes,
but that is not all that is going on inside the humans.
Right, there's causal machinery unseen.
And to understand this is the art of a cognitive scientist,
but even if you are not a cognitive scientist,
you can appreciate in principle
that what you see as the output
is not everything that there is.
And in particular, planning the process of being like,
here's a point in the world, how do I get there?
Is a central piece of machinery
that appears in chipping flint handaxes
and outwitting other humans.
And I think will probably appear at some point,
possibly in the past, possibly in the future,
in the problem of predicting the next word,
just how you organize your internal resources
to predict the next word.
And definitely appears in the problem
of predicting other things that do planning.
If you can, if by predicting the next chess move,
you learn how to play decent chess,
which has been represented to me
by people who claim to know that GPT-4 can do,
and I haven't been keeping track of to what extent
there's public knowledge about the same thing or not.
But like if you learn to predict the next chess move
that humans make well enough
that you yourself can play good chess in novel situations,
you have learned planning.
There's now something inside there
that knows the value of a queen,
that knows to defend the queen,
that knows to create forks,
to try to lure the opponent into traps.
Or if you don't have a concept of the opponent's psychology,
try to at least create situations
that the opponent can't get out of.
And it is a moot point whether this is simulated or real
because simulated thought is real thought.
Thought that is simulated in enough detail is just thought.
There's no such thing as simulated arithmetic, right?
There's no such thing as pretending to,
merely pretending to add numbers and getting the right answer.
So in its current format though,
and maybe you're talking about the next generation,
and its current format,
it responds to my requests
with what I would call the wisdom of crowds, right?
It goes through this vast library,
and I have my own library by the way.
I've read dozens of books,
maybe actually hundreds of books,
but it will have read millions, right?
So it has more.
And so when I ask it to write me a poem or a love song,
you know, to play Serino de Bergerac to Christian
and Serino de Bergerac, it's really good at it.
But why would it decide, oh, I'm gonna do something else?
Why would it, it's trained to listen to the murmurings
of these trillions of pieces of information.
I only have a few hundred,
so I don't murmur maybe as well.
Maybe it'll murmur better than I do.
It'll listen to the murmuring better than I do
and create a better love song, a love poem.
But why would it then decide,
I'm gonna go make paper clips,
or do something in planning
that is unrelated to my query?
Or are we talking about a different form of AI
that will come next?
Well, I'll ask it to.
I think we would see the phenomena I'm worried about
like if we kept to the present paradigm
and optimized harder, we may be seeing it already.
It's hard to know because we don't know
what goes on in there.
So first of all, GPT-4 is not a giant library.
A lot of the time it makes stuff up
because it doesn't have a perfect memory.
It is more like a person who has read through
a million books, not necessarily with the great memory
unless something got repeated many times,
but picking up the rhythm,
figuring out how to talk like that.
If you ask GPT-4 to write you a rap battle
between Cyrano de Bergerac and Vladimir Putin,
even if there's no rap battle like that,
like that that it has read,
it can write it because it has picked up the rhythm
of what are rap battles in general.
So, and the next thing is like,
there's no like pure output.
Like just because you train a thing
doesn't mean that there's nothing in there,
but what is trained?
That's part of what I'm trying to gesture at
with respect to humans, right?
Like humans are trained on flint hand axes
and hunting mammoths and outwitting other humans.
They're not trained on going to the moon.
They're not trained on,
they weren't trained to want to go to the moon,
but the compact solution to the problems
that humans face in the ancestral environment,
the thing inside that generalizes,
the thing inside that is not just a recording
of the outward behavior, the compact thing
that has been ground to solve novel problems
over and over and over and over again.
That thing turns out to have internal desires
that eventually put humans on the moon
even though they weren't trained to want that.
But that's why I asked you that,
are you underlying this,
is there some parallelism between the human brain
and the neural network of the AI
that you're effectively leveraging there,
or do you think it's a generalizable claim
without that parallel?
I don't think it's a specific parallel.
I think that what I'm talking about is
hill climbing optimization that spits out
intelligences that generalize.
Or I should say rather,
hill climbing optimization that spits out capabilities
that generalize far outside the training distribution.
Okay, so I think I understand that.
I don't know how likely it is that it's gonna happen.
I think you seem, I think you think
that piece is almost certain?
As it gets.
I think we're already, yeah, we're already seeing it.
As you grind these things further and further,
they can do more and more stuff,
including stuff they were never trained on.
Like we are, that was always the goal
of artificial general intelligence.
Like that's what artificial general intelligence meant.
That's what people in this field
have been pursuing for years and years.
That's what they were trying to do
when large language models were invented.
And they're starting to succeed.
Well, okay, I'm not sure.
Let me push back on that and you can try to persuade me.
So Brian Kaplan, a frequent guest here on Econ Talk,
gave, I think it was ChatGPT4, his economics exam
and it got a B.
And that's pretty impressive for just one stop
on the road to smarter and smarter chats,
but it wasn't a particularly good test of intelligence.
The number of the questions were things like,
what is Paul Krugman's view of this
or what is someone's view of that?
And I thought, well, that's kind of like a softball
for that's information, it's not thinking.
Steve Landsberg gave ChatGPT4 with the help of a friend
his exam and it got a four out of 90.
It got an F, like a horrible F
because they were harder questions,
not just harder, they required thinking.
So there was no sense in which the ChatGPT4
has any general intelligence, at least in economics.
You wanna disagree?
It's getting there.
Okay. You know, there's a saying that goes,
if you don't like the weather in Chicago, wait four hours.
Yeah.
So yeah, so ChatGPT is not going to destroy the world.
GPT4 is unlikely to destroy the world
unless the people currently eking capabilities out of it
take a much larger jump than I currently expect that they will.
But, you know, it's understand,
it may not be thinking about it correctly,
but it's understands the concepts and the questions,
even if it's not fair, you know,
you're complaining about that dog who writes bad poetry.
Right?
And like three years ago, you'd like just like spit out,
spit in these, you put in these economics questions
and you don't get wrong answers.
You get like gibberish or like maybe not gibberish
because three year old goes, I think we already had GPT3,
though maybe not as of April, but anyways.
Yeah. So it's moving along at a very fast clip.
The previous, you know, like GPT3 could not write code.
GPT4 can write code.
So how's it going to,
before I want to go to some other issues,
but how's it going to kill me when it has its own goals
and it's sitting inside this set of servers?
I don't know what sense it's sitting.
It's not the right verb.
We don't have verb for it.
It's hovering, it's whatever, it's in there.
How's it going to get to me?
How's it going to kill me?
If you are smarter, not just smarter
than an individual human,
but smarter than the entire human species,
and you started out on a server connected to the internet
because these things are always starting out
already on the internet these days,
which back in the old days that was stupid,
what do you do to make as many paper clips as possible?
Let's say.
I do think it's important to put yourself
in the shoes of the system.
Yeah, no, by the way,
one of my favorite lines from your essay,
I'm going to read it because I think it generalizes
to many other issues.
Say, to visualize a hostile superhuman AI,
don't imagine a lifeless book smart thinker
dwelling inside the internet
and sending ill-intentioned emails.
It reminds me of when people claim to think they can,
they know what Putin's going to do
because they've read history or whatever.
They're totally ignorant of Russian culture.
They have no idea what it's like
to have come out of the KGB.
That they're totally clueless and dangerous
because they think they can put themselves
in the head of someone there who's totally alien to them.
So I think that's generally a really good point to make
that putting ourselves inside the head of the paperclip
maximizer is not an easy thing to do
because it's not a human.
It's not like the humans you've met before.
That's a really important point.
Really like that point.
So why is that, explain why that's going to run amok?
I mean, I do kind of want you to just like,
take the shot at it, put yourself into the AI shoes,
try with your own intelligence
before I tell you the result of my trying with my intelligence.
How would you win from these starting resources?
How would you evade the tax?
So just to take a creepier,
much creeper example in the paper clips,
Eric Hall asked the chat GPT
to design an extermination camp,
which it gladly did quite well.
And you're suggesting it might actually, no?
Don't start from malice.
Okay.
Malice is implied by just wanting all the resources
of earth to yourself,
not leaving the humans around in case they could create
a competing super intelligence
that might actually be able to hurt you.
And just like wanting all the resources
and to organize them in a way that wipes out humanity
as a side effect,
which means the humans might want to resist,
which means you want the humans gone.
You're not doing it because somebody told you do it.
You're not doing it because you hate the humans.
You just want paper clips.
Okay. Tell me.
I'm not creative enough.
Tell me.
All right.
So, first of all, I want to appreciate why it's hard
for me to give an actual correct answer to this,
which is I'm not as smart as the AI.
Part of what makes a smarter mind deadly
is that it knows about rules of the game
that you do not know.
If you send an air conditioner back in time
to the 11th century,
even if you manage to describe all the plans for building it,
breaking it down to enough detail
that they can actually build a working air conditioner,
a simplified air conditioner, I assume,
they will be surprised when cold air comes out of it
because they don't know
about the pressure temperature relation.
They don't know you can compress air until it gets hot,
dump the heat into water or other air,
let the air expand again
and that the air will then be cold.
They don't know that's a law of nature.
So you can tell them exactly what to do
and they'll still be surprised at the end result
because it exploits a law of the environment
they don't know about.
If we're going to say the word magic means anything at all,
it probably means that.
Magic is easier to find in more complicated,
more poorly understood domains.
If you're literally playing logical tic-tac-toe,
not tic-tac-toe in real life on an actual game board,
where you can potentially go outside that game board
and hire an assassin to shoot your opponent or something,
but just like the logical structure of the game itself
and there's no timing of the moves.
The moves are just like made at exact discrete times,
you can't exploit a timing side channel.
Even a superintelligence may not be able to win against you
at logical tic-tac-toe
because the game is too narrow.
There are not enough options.
We both know the entire logical game tree,
at least if you're experienced at tic-tac-toe.
Yeah.
In chess, Stockfish 15 can defeat you
on a fully known game board with fully known rules
because it knows the logical structure
of the branching tree of games
better than you know that logical structure.
Great.
It can defeat you starting from the same resources,
equal knowledge of the rules.
Then you go past that.
And the way a superintelligence defeats you
is very likely by exploiting features of the world
that you do not know about.
There are some classes of computer security flaws,
like row hammer,
where if you flip a certain bit very rapidly
or at the right frequency,
the bit next to it in memory will flip.
So if you are exploiting a design flaw like this,
I can show you the code
and you can prove as a theorem
that it cannot break the security of the computer
assuming the chips works as design
and the code will break out of the sandbox
in any ways because it is exploiting
physical properties of the chip itself
that you did not know about.
Despite the attempt of the designers
to constrain the properties of that chip very narrowly,
that's magic code.
My guess as to what would actually be exploited to kill us
would be
this.
For those not watching on YouTube,
it's a copy of a book called Dano Systems.
But for those who are listening at home
rather than watching at home,
Eliezer, tell us why it's significant.
Yeah, so back when I first proposed this path,
one of the key steps was that a superintelligence
would be able to solve the protein folding problem.
And people were like,
Eliezer, how can you possibly know
that a superintelligence would actually be able
to solve the protein folding problem?
And I sort of like rolled my eyes a bit
and was like, well, if natural selection
can navigate the space of proteins
via random mutation to find other useful proteins
and the proteins themselves
fold up in reliable confirmations,
then that tells us that even though
it's we've been having trouble getting a grasp
on this space of physical possibilities so far
that it's tractable.
And people said like, what?
Like there's no way you can know
that superintelligence can solve
the protein folding problem.
Then AlphaFold2 basically cracked it,
at least with respect to the kind of proteins
found in biology.
Which I say to sort of like look back
at one of the previous debates here
and people are often like,
how can you know a superintelligence will do?
And then for some subset of those things
they have already been done.
So I would claim to have a good prediction track record there
although it's a little bit iffy
because of course I can't quite be proven wrong
without exhibiting a superintelligence
that fails to solve a problem.
Okay.
Proteins, why is your hand not as strong as steel?
We know that steel is a kind of substance that can exist.
We know that molecules can be held together as strongly
that atoms can be bound together as strongly
as the atoms in steel.
It seems like it would be an evolutionary advantage
if your flesh wears hard as steel.
You could like laugh at tigers at that rate, right?
Their claws are just gonna like scrape right off ya.
Assuming the tigers didn't have that technology themselves.
Why is your hand not as strong as steel?
Why has biology not bound together
the atoms in your hand more strongly?
Colin, what is your answer?
Well, it can't get to every,
it's their local maximums.
The national selection looks for things that work,
not for the best.
It does not, it doesn't make sense to look for the best.
You could disappear on that search.
That would be my crude answer.
How am I doing, doc?
Yeah, not terribly.
The answer I would give is that biology has to be evolvable.
Everything it's built out of has to get there as a mistake
from some other conformation,
which means that if it went down narrow potential,
pardon me, went down a steep potential energy gradients
to end up bound together very tightly,
designs like that are less likely to have neighbors
that are other useful designs.
And so your hands are made out of proteins
that fold up basically held together
by the equivalent of static cling,
van der Waals forces, rather than covalent bonds.
The backbone of protein chains,
the backbone of the amino acid change is a covalent bond,
but then it folds up and is held together
by static cling, static electricity.
And so it is soft.
Somewhere in the back of your mind,
you probably have a sense that,
that flesh is soft and animated by Alain Vital,
and it's like soft and it's not as strong as steel,
but it can heal itself and it can replicate itself.
And this is like the trade-off of our laws of magic,
that if you wanna heal yourself and replicate yourself,
you can't be as strong as steel.
This is not actually built into nature on a deep level.
It's just that the flesh evolved
and therefore had to go down shallow potential energy
gradients in order to be evolvable
and is held together by van der Waals forces
instead of covalent bonds.
I'm now going to hold up another book now,
book called
Nano Medicine
by Robert Freitas, instead of Nano Systems by Eric Drexler.
And people have done advanced analysis
of what would happen if you hadn't,
what would happen if you had an equivalent of biology
that met off covalent bonds instead of van der Waals forces?
And the answer we can like analyze on some detail
in our understanding of physics is, for example,
you could instead of carrying instead of red blood cells
that carry oxygen using weak chemical bonds,
you could have a pressurized vessel of corundum
that would hold 100 times as much oxygen per unit volume
of artificial red blood cells
with a 1000 fold safety margin
on the strength of the pressurized container.
There's vastly more room above biology.
So this is, and this is actually not even exploiting
laws of nature that I don't know.
It's the equivalent of playing a better chess
wherein you understand how proteins fold
and you design a tiny molecular lab
to be made out of proteins
and you get some human patsy
who probably doesn't even know you're an AI
because AIs are now smart enough.
This was, this has already been shown.
AIs now are smart enough that you ask them
to like hire a task rabbit to solve a capture for you.
And the task rabbit asks, are you an AI law?
The AI will think out loud,
like I don't want to know that I'm an AI.
I better tell something else
and then tell the humans that it has like a visual disability.
So it needs to hire somebody else to solve the capture.
This already happened,
including the part where it thought out loud.
Anyways, so you get your,
you order some proteins from an online lab,
you get your human who probably doesn't even know
you're an AI because why take that risk?
Although plenty of humans, it has,
well, we'll serve AIs willingly.
We also now know that AIs are advanced enough to even ask.
The human mixes the proteins in a beaker,
maybe puts in some sugar or a settling for fuel.
It assembles into a tiny little lab
that can accept further instruction,
acoustic instructions from a speaker
and maybe like transmit something back.
Tiny radio, tiny microphone.
I myself am not a superintelligence.
Run experiments in a tiny lab at high speed
because when distances are very small,
events happen very quickly.
Build your second stage nanosystems
inside the tiny little lab.
Build the third stage nanosystems,
build the fourth stage nanosystems,
build the tiny diamondoid bacteria
that replicate out of carbon, hydrogen,
oxygen, nitrogen as can be found in the atmosphere
powered on sunlight,
quietly spread all over the world.
All the humans fall over dead in the same second.
This is not how a superintelligence would defeat you.
This is how Eleazar Yudkowski would defeat you
if I wanted to do that, which to be clear, I don't.
And if I had the postulated ability
to better explore the logical structure
of the known consequences of chemistry.
Interesting, okay.
So let's talk about, that sounds sarcastic.
I didn't mean it's sarcastic,
but I think it's really interesting.
That interesting, man, I'm not capable.
My intelligence level is not high enough
to assess the quality of that argument.
What's fascinating, of course, is that
we could have imagined,
Eric Hall mentioned nuclear proliferation.
It's dangerous, nuclear proliferation,
up to a point in some sense it's somewhat healthy
in that it can be a deterrent under certain settings,
but the world could not restrain nuclear proliferation.
And right now it's trying to some extent
has had some success in keeping the nuclear club
with its current number of members for a while,
but it remains the case that nuclear weapons
are a threat to the future of humanity.
Do you think there's any way we can restrain
this AI phenomenon that's meaningful?
So you issued a clarion call, you sounded an alarm,
and mostly I think people shrugged it off, you know?
A bunch of people signed a letter,
26,000 people I think so far, signed the letter saying,
you know, we don't know what we're doing here,
this is uncharted territory, let's take six months off.
You were in peace and says, six months, are you crazy?
We need to stop this until we have an understanding
of how to constrain it.
Now that's a very reasonable thought to me,
but the next question would be,
how would you possibly do that?
In other words, I could imagine a world where,
if there were, let's say, four people who were capable
of creating this technology,
that the four people would say, you know,
we're playing with fire here, we need to stop,
let's make a mutual agreement, they might not keep it,
four people still a pretty big number,
but we're not a four people,
there are many, many people working on this,
there are many countries working on it.
Your peace did not, I don't think,
start an international movement of people going
to the barricades to demand that this technology
be put on hold.
How do we possibly, how do you sleep at night?
I mean, like, what should we be doing if you're right?
Or am I wrong, do people read this and go,
well, Elias Rieckowski thinks it's dangerous,
maybe we ought to be slowing down.
I mean, Sam, what's happened in the middle of the night
saying, thanks, Elias, I'm gonna put things on hold.
I don't think that happened.
I think you are somewhat underestimating the impact
and it is still playing out.
Okay, so like, mostly, it seems to me that if we wanted
to win this, we needed to start a whole lot earlier,
possibly in the 1930s.
But in terms of, like, my looking back
and like asking how far back you'd have to unwind history
to get us into a situation where this was survivable.
But leaving that aside.
I think that's a move.
Yeah, so in fact, it seems to me that the game board
has been played into a position where it is very likely
that everyone just dies.
If the human species woke up one day
and decided it would rather live,
it would not be easy at this point
to bring the GPU clusters and the GPU manufacturing
processes under sufficient control
that nobody built things that were too much smarter
than GPT-4 or GPT-5 or whatever the level just barely short
of lethal is, which we should not,
which we would not if we were taking this seriously,
get as close to as we possibly could,
because we don't actually know exactly where the level is.
But we would have to do more or less,
is have international agreements that were being enforced
even against parties not part,
even against countries not party to that national agreement,
international agreement, if it became necessary,
you would be wanting to track all the GPUs.
You might be demanding that all the GPUs call home
on a regular basis or stop working.
You'd want to tamper proof them.
If intelligence said that a rogue nation was,
had bought, somehow managed to buy a bunch of GPUs
despite arms controls and defeat the tamper proofing
on those GPUs, you would have to do it was necessary
to shut down the data center,
even if that led to a shooting war between nations,
even if that country was a nuclear country
and had threatened nuclear retaliation.
The human species could survive this if it wanted to,
but it would not be business as usual.
It is not something you could do trivially.
So when you say I may have underestimated it,
did you get people writing and saying,
you know, I wasn't, and I don't mean people like me.
I mean, people, players.
Did you get people who are playing in this sandbox
to write you and say, you've scared me.
I think we need to take this seriously?
Without naming names, I'm not asking for that.
At least one US Congressman.
Okay.
It's a start, maybe.
You know, one of the things that a common response
that people get when you talk about this is that,
well, the last thing I do is,
last thing I want is the government controlling
whether this thing goes forward or not,
but it'd be hard to do without some form of lethal force
as you comply.
I spent 20 years trying desperately to have there be
any other solution to have these things be alignable,
but it is very hard to do that
when you are nearly alone and under resourced
and the world has not made this a priority.
And future progress is very hard to predict.
I don't think people actually understood the research program
that we were trying to carry out, but yeah.
So I sure wanted there to be any other plan than this
because now that we've come to this last resort,
I don't think we actually have that last resort.
I don't think we have been reduced
to a last-ditch backup plan that actually works.
I think we all just die.
And yet nonetheless, here I am,
like putting aside doing that thing
that I wouldn't do for almost any other technology,
except for maybe gain of function research
on biological pathogens
and advocating for government interference.
Because in fact, like if the government comes in
and wrecks the whole thing,
that's better than the thing
that was otherwise going to happen.
Because it's not based on the government coming in
and being like super competent
and directing the technology exactly directly.
It's like, okay, this is going to kill literally everyone
if the government stomps around
and like the dangerous of the government.
It's one of those very rare cases
where the dangerous that the government
will interfere too little rather than too much.
Possibly.
Let's close with a quote from Scott Ericsson,
which found on his blog.
We'll put a link up to the post.
Very interesting defensive of AI.
Scott's a University of Texas computer scientist.
He's working at OpenAI.
He's on leave, I don't, I think for a year, maybe longer.
I don't know, it doesn't matter.
He wrote the following.
So if we ask the directly relevant question,
do I expect the generative AI race,
which started in earnest around 2016 or 2017,
with the founding of OpenAI
to play a central causal role in the extinction of humanity?
I'll give a probability of around 2% for that.
And I'll give a similar probability,
maybe even a higher one for the generative AI race
to play a central causal role in the saving of humanity.
All considered then,
I come down in favor right now,
proceeding with AI research with extreme caution,
but proceeding.
My personal reaction is that is insane.
I have very little, I'm serious.
I find that deeply disturbing
and I'd love to have him on the program to defend it.
I don't think there's much of a chance
that generative AI would save humanity.
I'm not quite sure for what it's,
he's worried about,
but if you're telling me there's a 2%, 2% chance
that it's gonna destroy all humans
and you obviously think it's higher,
but 2% is really high to me
for an outcome that's rather devastating.
It's one of the deepest things I've learned from Nassim Tala.
It's not just the probability,
it's the outcome that counts too.
So this is ruined on a colossal scale
and the one thing you wanna do is avoid ruin
so you can take advantage of more draws from the earn.
The average return from the earn is irrelevant
if you are not allowed to play anymore.
You're out, you're dead, you're gone.
So you're suggesting we're gonna be out and dead and gone,
but I want you to react to Scott's quote.
Um, 2% sounds great.
Like 2% is plausibly within the range of like
the human species destroying it itself by other means.
I think that the disagreement I have with Scott Aronson
is simply about the probability that AI is alignable
with the frankly half-fazard level
that we have put into it and the half-fazard level
that is all humanity is capable of
as far as I can tell,
because the core lethality here
is that you have to get something right
on the first try or it kills you
and getting something right on the first try
when you do not get like infinite free retries
as you usually do in science and engineering
is an insane ask, insanely lethal ask.
My reaction is fundamentally that 2% is too low.
If I take it at face value,
then 2% is within range of the probability
of humanity wiping itself out by something else
where if you assume that AI alignment is free,
that AI alignment is easy,
that you can get something that is smarter than you,
but on your side and helping,
2% chance of risking everything does appear to me
to be commensurate with the risks from other sources
that you could shut down using the superintelligence.
It's not 2%.
So, the question then is,
what would Scott Aaronson say if he heard your,
I mean, he's heard, he's read your piece presumably,
he understands your argument about wealthiness.
I should just clarify for listeners,
alignment is the idea that AI could be constrained
to serve our goals rather than its goals.
Is that a good summary?
I wouldn't say constrained.
I would say built from scratch to want those things
and not want otherwise.
So that's really hard
because we don't understand how it works.
That would be, I think, your point.
And tell me then what Scott's-
On the first try.
Yeah, on the first try.
So what would Scott say when you tell him,
but it's gonna develop all these side desires
that we can't control.
What's he gonna say?
Why is he not worried?
Why is he still, why is he quit his job?
And not Scott.
People in the, let's get away from him personally,
but people in general, there's dozens and maybe hundreds,
maybe a thousand, I don't know,
extraordinarily intelligent people
who are trying to build something even more intelligent
than they are.
Why are they not worried about what you're saying?
They've all got different reasons.
Scott's is that he thinks that intelligence,
that he observes intelligence makes humans nicer.
And though he wouldn't phrase it exactly this way,
this is basically what Scott said on his blog.
To which my response is intelligence
does have effects on humans,
especially humans who start out relatively nice.
And when you're building AI's from scratch,
you're just like in a different domain with different rules.
And you're allowed to say that it's hard to build AI's
that are nice without implying that making humans smarter,
like humans start out in a certain frame of reference.
And when you apply more intelligence to them,
they move within that frame of reference.
And if they start out with a small amount of nicest,
the intelligence can make them nicer.
They can become more empathetic.
If they start out with some empathy,
they can develop more empathy
as they understand other people better,
which is intelligence to correctly model other people.
That is even more insane.
I'm not going to, I haven't read that blog post
and we'll put a link up to it.
I hope you'll share it with me.
But again, not attributing it to Scott
since I haven't seen it.
And assuming that you've said this fairly and correctly,
the idea that more intelligent people are nicer
is one of the most,
that'd be very hard to show with the evidence for that.
That is an appalling.
I don't think it's like,
it is not a universal law on humans.
No.
It is a thing that I think is true of Scott.
I think if you made Scott Aaron Sinclair smarter,
he'd get nicer.
And I think he's inappropriately generalizing from that.
There is a scene in Schindler's List.
The Nazis, I think they're in the Warsaw ghetto
and they're race, a group of Nazis are racing,
I think they're in the SS,
they're racing through a tenement.
And it's falling apart
because the ghetto is falling apart.
But one of the SS agents sees a piano
and he can't help himself.
He sits down and he plays Bach or something.
I think it was Bach.
And I always found it interesting
that Spielberg put that in or whoever wrote the script.
And I think it was pretty clear why they put it in.
They wanted to show you that
having a very high advanced level of civilization
does not stop people from treating other people,
other human beings like animals,
or worse than animals in many cases,
and exterminating them without conscience.
So I don't share that view of anyone's
that intelligence makes you a nicer person.
I think that's not the case,
but perhaps Scott will return to this,
will come to this program and defend that view
if he did holes it.
I think you are underweighting the evidence
that has convinced Scott of the thing that I think is wrong.
I think if you suddenly started augmenting the intelligence
of the SS agents from Nazi Germany,
then somewhere between 10% and 90% of them
would go over to the cause of good.
Because there were factual falsehoods
that were pillars of the Nazi philosophy,
and that people would reliably stop believing
as they got smarter.
That doesn't mean that they would turn good,
but some of them would have.
Is it 10%?
Is it 90%?
I don't know.
It's not my experience with the human creature.
You've written some very interesting things
on rationality of a beautiful essay,
we'll link to on 12 rules for rationality.
In my experience, it's a very small portion
of the population that behaves that way.
There's a quote from Nassim Taleb,
we haven't gotten to yet in this conversation,
which is bigger data, bigger mistakes.
I think there's a belief generally
that bigger data fewer mistakes,
but Taleb might be right,
and it's certainly not the case in my experience
that bigger brains, higher IQ means better decisions.
This is not my experience.
Then you're not throwing enough intelligence
at the problem.
Literally not just decisions
that you disagree with the goals,
but false models of reality,
models of realities so blatantly mistaken
that even you, a human,
can tell that they're wrong and in which direction.
These people are not smart the way
that a hypothetical weak efficient market is smart.
You can tell they're making mistakes
and you know in which direction.
They're not smart the way that Stockfish 15
is smart in chess.
You can play against them and win.
These are, the range of human intelligence
is not that wide.
It caps out at like John von Neumann or whatever.
And that is not wide enough to open up
that humans would be epistemic,
that these beings would be epistemically
or instrumentally efficient relative to you.
It is possible for you to know
that one of their estimates is directionally mistaken
and to know the direction.
It is possible for you to know an action
that serves their goals better
than the action that they generated.
And is it striking how hard it is
to convince some of that even though they're thinking people?
History is, I just have a different perception maybe.
To be continued, Eliezer.
My guest today has been Eliezer Yudkowski.
Eliezer, thanks for being part of Econ Talk.
Thanks for having me.
This is Econ Talk, part of the Library of Economics and Liberty.
For more Econ Talk, go to econtalk.org
where you can also comment on today's podcast
and find links and readings related to today's conversation.
The sound engineer for Econ Talk is Rich Goyette.
I'm your host Russ Roberts.
Thanks for listening.
Talk to you on Monday.
