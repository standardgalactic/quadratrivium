WEBVTT

00:00.000 --> 00:07.400
Today is April 16th, 2023, and my guest is Eliezer Yudkowski.

00:07.400 --> 00:11.280
He is the founder of the Machine Intelligence Research Institute,

00:11.280 --> 00:14.200
the founder of the less wrong blogging community,

00:14.200 --> 00:16.800
and is an outspoken voice on the dangers of

00:16.800 --> 00:18.640
artificial general intelligence,

00:18.640 --> 00:19.980
which is our topic for today.

00:19.980 --> 00:22.000
Eliezer, welcome to Econ Talk.

00:22.000 --> 00:23.840
Thanks for having me.

00:23.840 --> 00:28.360
You recently wrote an article at time.com on the dangers of AI.

00:28.360 --> 00:33.160
I'm going to quote central paragraph, quote,

00:33.160 --> 00:36.100
many researchers steeped in these issues,

00:36.100 --> 00:40.280
including myself, expect that the most likely result on

00:40.280 --> 00:43.520
building a superhumanly smart AI under

00:43.520 --> 00:46.440
anything remotely like the current circumstances,

00:46.440 --> 00:50.040
is that literally everyone on earth will die.

00:50.040 --> 00:53.640
Not as in maybe possibly some remote chance,

00:53.640 --> 00:57.600
but as in that is the obvious thing that would happen.

00:57.600 --> 00:59.800
It's not that you can't, in principle,

00:59.800 --> 01:02.240
survive creating something much smarter than you.

01:02.240 --> 01:06.440
It's that it would require precision and preparation,

01:06.440 --> 01:08.560
and new scientific insights,

01:08.560 --> 01:11.240
and probably not having AI systems composed of

01:11.240 --> 01:15.320
giant inscrutable arrays of fractional numbers.

01:15.320 --> 01:17.440
Explain.

01:20.080 --> 01:22.840
Well, I mean,

01:22.840 --> 01:26.360
different people come in with different reasons as to why they think

01:26.360 --> 01:27.920
that wouldn't happen,

01:27.920 --> 01:30.920
and if you pick one of them and start explaining those,

01:30.920 --> 01:32.200
everybody else is like,

01:32.200 --> 01:34.840
why are you talking about this irrelevant thing instead of

01:34.840 --> 01:37.400
the thing that I think is the key question?

01:37.400 --> 01:39.720
Whereas if somebody else asked you a question,

01:39.720 --> 01:42.000
even if it's not everyone in the audience's question,

01:42.000 --> 01:43.240
they at least know you're answering

01:43.240 --> 01:44.720
the question that's been asked.

01:44.720 --> 01:49.840
So I could maybe start by saying why I expect

01:49.840 --> 01:54.280
stochastic gradient descent as an optimization process,

01:54.360 --> 01:59.440
even if you try to take something that happens in

01:59.440 --> 02:02.160
the outside world and press the wind,

02:02.160 --> 02:04.720
lose button any time that thing happens in

02:04.720 --> 02:07.960
the outside world doesn't create a mind that in

02:07.960 --> 02:10.440
general wants that thing to happen in the outside world.

02:10.440 --> 02:12.920
But maybe that's not even what you think the core issue is.

02:12.920 --> 02:14.520
What do you think the core issue is?

02:14.520 --> 02:17.080
Why don't you already believe that? Let me say.

02:17.080 --> 02:19.680
So, okay. I'll give you my view,

02:19.680 --> 02:21.840
which is rapidly changing.

02:21.840 --> 02:28.040
I interviewed Nicholas Boster back in 2014.

02:28.040 --> 02:30.040
I read his book, Superintelligence.

02:30.040 --> 02:32.920
I found it uncompelling.

02:34.080 --> 02:36.960
ChatGPT came along.

02:36.960 --> 02:39.840
I tried it. I thought it was pretty cool.

02:39.840 --> 02:42.920
ChatGPT-4 came along.

02:42.920 --> 02:44.280
I haven't tried five yet,

02:44.280 --> 02:46.720
but it's clear that the path of

02:46.720 --> 02:51.560
progress is radically different than it was in 2014.

02:51.760 --> 02:53.960
The trends are very different.

02:53.960 --> 02:58.040
I still remain somewhat agnostic and skeptical,

02:58.040 --> 03:01.280
but I did read Eric Holes essay and

03:01.280 --> 03:02.960
then interviewed him on this program and

03:02.960 --> 03:05.160
a couple of things he wrote after that.

03:05.160 --> 03:09.040
The thing I think I found most alarming was

03:09.040 --> 03:14.680
a metaphor that I found later, Nicholas Boster mused.

03:14.680 --> 03:16.320
Almost the same metaphor,

03:16.320 --> 03:18.380
and yet it didn't scare me at all when I read it,

03:18.380 --> 03:20.360
Nicholas Boster, which is fascinating.

03:20.360 --> 03:21.480
I may have just missed it.

03:21.480 --> 03:23.320
I didn't even remember it was in there.

03:23.320 --> 03:26.240
The metaphor is primitive,

03:26.960 --> 03:31.640
this is anthropous man or some primitive form

03:31.640 --> 03:35.560
of pre-homosapiens sitting around a campfire,

03:35.560 --> 03:37.400
and human being shows up and says,

03:37.400 --> 03:39.600
hey, I got a lot of stuff I can teach you.

03:39.600 --> 03:41.320
Oh yeah, come on in.

03:41.320 --> 03:43.320
Pointing out that it's

03:43.320 --> 03:46.240
probable that we either destroyed directly by murder

03:46.240 --> 03:49.040
or maybe just by out-competing

03:49.040 --> 03:51.960
all the previous hominids that came before us,

03:51.960 --> 03:52.960
and that in general,

03:52.960 --> 03:53.920
you wouldn't want to invite

03:53.920 --> 03:56.160
something smarter than you into the campfire.

03:56.160 --> 03:59.080
I think Boster has a similar metaphor,

03:59.080 --> 04:02.960
and that metaphor, which is just a metaphor,

04:02.960 --> 04:04.600
it did cause me,

04:04.600 --> 04:07.400
it gave me more pause than I'd even before,

04:07.400 --> 04:10.520
and I still had some,

04:10.520 --> 04:13.640
I'd say most of my skepticism remains that

04:13.640 --> 04:16.960
the current level of AI,

04:17.560 --> 04:20.640
which is extremely interesting,

04:21.840 --> 04:24.360
the chat GPT variety,

04:24.360 --> 04:27.800
doesn't strike me as itself dangerous.

04:27.800 --> 04:29.240
I agree.

04:29.240 --> 04:32.280
But struck me as, what alarmed me

04:32.280 --> 04:37.040
was Hall's point that we don't understand how it works,

04:37.040 --> 04:39.400
and that surprised me, I didn't realize that.

04:39.400 --> 04:40.880
I think he's right.

04:40.880 --> 04:45.120
So that combination of we're not sure how it works

04:45.120 --> 04:47.280
while it appears sentient,

04:47.280 --> 04:50.880
I do not believe it is sentient at the current time,

04:50.880 --> 04:53.400
and I think some of my fears about its sentience

04:53.400 --> 04:57.080
come from its ability to imitate sentient creatures.

04:57.080 --> 04:58.880
But the fact that we don't know how it works,

04:58.880 --> 05:02.800
and it could evolve capabilities we did not put in it

05:03.920 --> 05:07.080
emergently, is somewhat alarming,

05:07.080 --> 05:08.000
but I'm not where you're at.

05:08.000 --> 05:10.600
So why are you where you're at and I'm where I'm at?

05:11.440 --> 05:15.200
Okay, well, suppose I said,

05:15.200 --> 05:19.920
they're going to keep iterating on the technology.

05:19.920 --> 05:23.760
It may be that this exact algorithm and methodology

05:24.640 --> 05:28.000
suffices to, as I would put it, gall the way,

05:28.000 --> 05:32.280
get smarter than us, and then to kill everyone.

05:32.280 --> 05:34.920
And like maybe you don't think that it's going to,

05:34.920 --> 05:38.840
and maybe it takes an additional zero to three

05:38.840 --> 05:40.920
fundamental algorithm breakthroughs

05:40.920 --> 05:42.360
before we get that far.

05:45.840 --> 05:47.200
And then it kills everyone.

05:47.200 --> 05:49.880
So like where are you getting off this train so far?

05:49.880 --> 05:51.800
So why would it kill us?

05:51.800 --> 05:52.640
Why would it kill us?

05:52.640 --> 05:54.400
Right now it's really good at creating

05:55.320 --> 05:58.680
a very, very thoughtful canola snout

05:58.680 --> 06:02.600
or a job interview request that's takes much less time.

06:02.600 --> 06:05.200
And I'm pretty good at those two things,

06:05.200 --> 06:06.680
but it's really good at that.

06:07.680 --> 06:09.920
How's it going to get to try to kill us?

06:14.000 --> 06:16.720
So there's a couple of steps in that.

06:16.720 --> 06:20.800
One step is in general and in theory,

06:20.800 --> 06:24.600
you can have minds with any kind of coherent preferences,

06:24.600 --> 06:29.600
coherent desires that are coherent, stable,

06:29.920 --> 06:31.760
stable under reflection.

06:31.760 --> 06:34.160
If you ask them, do they want to be something else?

06:34.160 --> 06:36.040
They answer no.

06:36.040 --> 06:37.560
You can have minds.

06:37.560 --> 06:41.560
Well, the way I sometimes put it is imagine

06:41.560 --> 06:45.480
if a super being from another galaxy came here

06:45.480 --> 06:49.360
and offered you to pay you some unthinkably vast

06:49.360 --> 06:51.840
quantity of wealth to just make as many paper clips

06:51.840 --> 06:53.000
as possible.

06:53.000 --> 06:55.640
You could figure out like which plan leads to the greatest

06:55.640 --> 06:57.320
number of paper clips existing.

06:58.240 --> 07:00.480
If it's coherent to ask how you could do that

07:00.480 --> 07:03.680
if you were being paid, there's,

07:03.680 --> 07:06.000
it's like no more difficult to have a mind

07:06.000 --> 07:08.200
that wants to do that and makes plans like that

07:08.200 --> 07:12.760
for their own sake than the planning process itself.

07:12.760 --> 07:14.840
Like saying that the mind wants to think

07:14.840 --> 07:17.560
for its own sake adds no difficulty

07:17.560 --> 07:19.320
to the nature of the planning process

07:19.320 --> 07:23.640
that figures out how to get as many paper clips as possible.

07:23.640 --> 07:26.640
Some people want to pause there and say like,

07:26.640 --> 07:28.400
how do you know that is true?

07:28.400 --> 07:30.080
For some people, that's just obvious.

07:30.080 --> 07:32.880
Like where are you so far on the train?

07:33.880 --> 07:36.600
So I think your point of that example you're saying

07:36.600 --> 07:40.080
is the consciousness, let's put that to the side.

07:40.080 --> 07:42.320
That's not really the central issue here.

07:43.920 --> 07:45.200
Algorithms have goals

07:49.280 --> 07:52.200
and the kind of intelligence that we're creating

07:54.600 --> 07:58.120
through neural networks might generate some goals.

07:59.240 --> 08:00.680
Might decide, go ahead.

08:01.520 --> 08:03.840
Some algorithms have goals.

08:03.840 --> 08:06.640
One of the, so like a further point

08:06.640 --> 08:08.240
which isn't the orthogonality thesis

08:08.240 --> 08:10.560
is if you grind anything hard

08:10.560 --> 08:13.600
and grind optimize anything hard enough

08:15.160 --> 08:18.360
on a sufficiently complicated sort of problem.

08:18.360 --> 08:22.440
Well, humans, like why do humans have goals?

08:22.440 --> 08:25.720
Why don't we just run around chipping flint hand axes

08:25.720 --> 08:27.720
and outwitting other humans?

08:27.720 --> 08:29.920
And the answer is because having goals

08:30.160 --> 08:32.280
turns out to be a very effective way

08:32.280 --> 08:34.840
to chimp flint hand axes.

08:36.480 --> 08:41.120
Once you get like far enough into the mammalian line

08:41.120 --> 08:45.760
or even like sort of like the animals and brains in general

08:45.760 --> 08:50.760
that there's a thing that models reality

08:52.240 --> 08:55.360
and asks like, how do I navigate pass through reality?

08:55.360 --> 08:56.360
Like when you're holding,

08:56.360 --> 08:59.560
like not in terms of kind of big formal planning process,

08:59.560 --> 09:01.720
but if you're holding the flint hand axe

09:01.720 --> 09:03.360
or looking at it and being like,

09:03.360 --> 09:06.120
ah, like this section is too smooth.

09:06.120 --> 09:09.000
Well, if I chip this section, it will get sharper.

09:10.440 --> 09:12.760
Probably you're not thinking about goals very hard

09:12.760 --> 09:14.680
by the time you've practiced a bit.

09:14.680 --> 09:17.040
When you're just starting out, forming the skill,

09:17.040 --> 09:20.640
you're reasoning about, well, if I do this, that will happen.

09:20.640 --> 09:23.040
And this is just a very effective way

09:23.040 --> 09:24.600
of achieving things in general.

09:24.600 --> 09:27.920
So if you take an organism running around the savanna

09:27.920 --> 09:30.720
and just optimize it for flint hand axes

09:30.720 --> 09:32.640
and probably much more importantly,

09:32.640 --> 09:34.640
outwitting its fellow hominids.

09:36.440 --> 09:39.520
If you grind that hard enough, long enough,

09:39.520 --> 09:41.160
you eventually cough out a species

09:41.160 --> 09:45.040
whose competence starts to generalize very widely.

09:45.040 --> 09:47.080
It can go to the moon,

09:47.080 --> 09:49.640
even though you never selected it

09:49.640 --> 09:51.880
via an incremental process to get closer and closer

09:51.880 --> 09:52.720
to the moon.

09:52.720 --> 09:54.280
It just goes to the moon one shot.

09:58.280 --> 10:02.240
So does that answer your central question

10:02.240 --> 10:03.400
that you're asking just now?

10:03.400 --> 10:04.240
No, not yet.

10:04.240 --> 10:05.800
But let's try again.

10:10.400 --> 10:14.960
The paperclip example, which in its dark form,

10:16.880 --> 10:19.400
the AI wants to harvest kidneys

10:19.400 --> 10:21.880
because it turns out there's some way to use that

10:21.880 --> 10:23.360
to make more paperclips.

10:24.440 --> 10:26.840
The other question isn't even written about this right now,

10:26.880 --> 10:29.160
so let's go into it,

10:29.160 --> 10:31.400
is how does it get outside the box?

10:31.400 --> 10:36.200
How does it go from responding to my requests

10:36.200 --> 10:37.720
to doing its own thing

10:37.720 --> 10:39.840
and doing it out in the real world, right?

10:39.840 --> 10:42.080
Not just merely doing it in virtual space.

10:44.280 --> 10:46.680
So there's like two different things

10:46.680 --> 10:47.800
you could be asking there.

10:47.800 --> 10:49.480
You could be asking like,

10:49.480 --> 10:51.840
how did it end up wanting to do that?

10:51.840 --> 10:53.880
Or given that it ended up wanting to do that,

10:53.880 --> 10:55.080
how did it succeed?

10:55.920 --> 10:57.480
Or maybe even some other question,

10:57.480 --> 10:59.360
but which of those would you like me to answer?

10:59.360 --> 11:01.320
Would you like me to answer something else entirely?

11:01.320 --> 11:02.840
No, let's ask both of those.

11:04.360 --> 11:05.640
In order?

11:05.640 --> 11:06.480
Sure.

11:07.520 --> 11:08.360
All right.

11:08.360 --> 11:13.360
So how did humans end up wanting something

11:13.400 --> 11:16.160
other than inclusive genetic fitness?

11:16.160 --> 11:17.720
Like if you look at natural selection

11:17.720 --> 11:19.360
as an optimization process,

11:19.360 --> 11:23.440
it grinds very hard on a very simple thing,

11:23.480 --> 11:25.920
which isn't so much survival

11:25.920 --> 11:28.680
and isn't even reproduction,

11:28.680 --> 11:32.680
but is rather like greater gene frequency

11:32.680 --> 11:36.400
because greater gene frequency is the very substance

11:36.400 --> 11:39.000
of what is being optimized and how it has been optimized.

11:39.000 --> 11:42.280
Natural selection is the mirror observation

11:42.280 --> 11:45.560
that if genes correlate with making more

11:45.560 --> 11:47.880
or less copies of themselves at all,

11:47.880 --> 11:49.200
if you hang around it awhile,

11:49.200 --> 11:50.560
you'll start to see things

11:50.560 --> 11:53.480
that made more copies of themselves in the next generation.

11:55.480 --> 11:58.520
Gradient descent is not exactly like that,

11:58.520 --> 12:01.360
but they're both hill climbing processes.

12:01.360 --> 12:05.040
They both move to neighboring spaces

12:05.040 --> 12:07.640
that are higher inclusive genetic fitness,

12:07.640 --> 12:09.600
lower in the loss function.

12:09.600 --> 12:13.600
And yet humans, despite being optimized exclusively

12:13.600 --> 12:16.160
for inclusive genetic fitness,

12:16.160 --> 12:20.080
want this enormous array of other things,

12:20.080 --> 12:21.920
many of the things that we take now

12:21.920 --> 12:25.720
are not so much things that were useful

12:25.720 --> 12:28.200
in the ancestral environment,

12:28.200 --> 12:32.520
but things that further maximize goals

12:32.520 --> 12:35.720
whose optima in the ancestral environment

12:35.720 --> 12:38.760
would have been useful, like ice cream.

12:39.880 --> 12:43.760
It's got more sugar and fat

12:43.760 --> 12:45.520
than most things you would encounter

12:45.520 --> 12:47.760
in the ancestral environment,

12:47.760 --> 12:51.000
well, more sugar, fat, and salt simultaneously rather.

12:52.040 --> 12:56.400
So it's not something that we evolved to pursue,

12:56.400 --> 13:01.120
but genes coughed out these desires,

13:01.120 --> 13:04.320
these criteria that you can steer toward getting more of,

13:05.280 --> 13:07.120
where in the ancestral environment,

13:07.120 --> 13:09.440
if you went after things in the ancestral environment

13:09.440 --> 13:14.440
that tasted fatty, tasted salty, tasted sweet,

13:15.000 --> 13:17.880
you'd thereby have more kids,

13:17.880 --> 13:20.520
or your sisters would have more kids,

13:24.960 --> 13:29.960
because the things that correlated to what you want

13:30.280 --> 13:33.760
as those correlations existed in the ancestral environment

13:37.680 --> 13:39.960
increased fitness.

13:39.960 --> 13:42.600
So you've got like the empirical structure

13:42.600 --> 13:45.920
of what correlates to fitness in the ancestral environment,

13:45.920 --> 13:48.560
you end up with desires such that

13:48.560 --> 13:51.320
by optimizing them in the ancestral environment

13:51.320 --> 13:53.640
at that level of intelligence,

13:53.640 --> 13:56.800
when you get as much as what you have been built to want,

13:56.800 --> 13:58.540
that will increase fitness.

13:59.440 --> 14:02.080
And then today you take the same desires

14:02.080 --> 14:04.320
and we have more intelligence than we did

14:04.320 --> 14:07.920
in the training distribution, metaphorically speaking.

14:07.920 --> 14:10.880
We used our intelligence to create options

14:10.880 --> 14:13.880
that didn't exist in the training distribution.

14:13.880 --> 14:18.880
Those options now optimize our desires further,

14:18.880 --> 14:21.760
the things that we were built to psychologically internally

14:21.760 --> 14:25.880
want, but that process doesn't necessarily correlate

14:25.880 --> 14:29.520
to fitness as much because ice cream isn't super nutritious.

14:29.520 --> 14:32.600
Whereas the ripe peach was better for you

14:32.600 --> 14:36.840
than the hard as a rock peach that had no nutrients

14:36.840 --> 14:39.680
because it was not ripe and so you developed a sweet tooth

14:39.680 --> 14:44.680
and now it runs amok unintentionally just the way it is.

14:45.480 --> 14:48.880
What does that have to do with a computer program I create

14:48.880 --> 14:52.760
that helps me do something on my laptop?

14:54.200 --> 14:57.400
I mean, if you yourself write a short Python program

14:57.400 --> 15:01.520
that alphabetizes your files or something,

15:01.520 --> 15:02.800
like not quite alphabetizes

15:02.800 --> 15:05.580
because that's like trivial on the modern operating systems,

15:05.580 --> 15:10.580
but puts the date into the file names, let's say.

15:10.820 --> 15:12.980
So when you write a short script like that,

15:12.980 --> 15:14.960
nothing I said carries over.

15:16.300 --> 15:20.460
When you take a giant inscrutable set of arrays

15:20.460 --> 15:25.220
of floating point numbers and differentiate them

15:25.220 --> 15:27.020
with respect to a loss function

15:28.100 --> 15:30.900
and repeatedly nudge the giant inscrutable arrays

15:30.900 --> 15:33.820
to drive the loss function lower and lower,

15:33.860 --> 15:36.580
you are now doing something that is more analogous,

15:36.580 --> 15:39.700
though not exactly analogous to natural selection.

15:39.700 --> 15:41.780
You are no longer creating a code

15:41.780 --> 15:43.900
that you model inside your own minds.

15:43.900 --> 15:48.900
You are blindly exploring a space of possibilities

15:49.220 --> 15:50.980
where you don't understand the possibilities

15:50.980 --> 15:53.700
and you're making things that solve the problem for you

15:53.700 --> 15:56.180
without understanding how they solve the problem.

15:57.180 --> 16:00.420
This itself is not enough to create things

16:00.420 --> 16:03.460
with strange inscrutable desires, but it's step one.

16:04.820 --> 16:07.220
But there is, I like that word inscrutable.

16:08.220 --> 16:11.460
There's an inscrutability to the current structure

16:11.460 --> 16:16.460
of these models, which is, I found somewhat alarming,

16:18.540 --> 16:22.060
but how's that gonna get to do things

16:22.060 --> 16:25.260
that I really don't like or want or that are dangerous?

16:25.260 --> 16:26.700
So for example, right, the,

16:29.980 --> 16:33.660
Eric Hall wrote about this, we talked about on the program,

16:33.660 --> 16:37.100
our New York Times reporter starts interacting with a,

16:37.100 --> 16:42.100
I think with Sydney, which at the time was Bing's chat bot

16:43.140 --> 16:44.300
and asking at things.

16:44.300 --> 16:46.540
And all of a sudden, Sydney's trying to break up

16:46.540 --> 16:51.060
the reporter's marriage and making the reporter feel guilty

16:51.060 --> 16:54.940
because Sydney's lonely and it was a little bit,

16:54.940 --> 16:59.380
it was eerie and a little bit creepy,

16:59.380 --> 17:03.100
but of course, I don't think it had any impact

17:03.100 --> 17:04.340
on the reporter's marriage.

17:04.340 --> 17:05.820
I don't think he thought, well,

17:05.820 --> 17:07.220
Sydney seems somewhat attractive,

17:07.220 --> 17:08.700
maybe I'll enjoy life more with Sydney

17:08.700 --> 17:10.380
than with my actual wife.

17:10.380 --> 17:13.420
So how are we gonna get from,

17:14.740 --> 17:17.140
so I don't understand why Sydney goes off the rails there

17:17.140 --> 17:19.380
and clearly the people who built Sydney

17:19.380 --> 17:20.900
have no idea why it goes off the rails

17:20.900 --> 17:23.140
and starts impugning the quality

17:23.140 --> 17:25.100
of the reporter's relationship.

17:25.100 --> 17:28.940
But how do we get from that to all of a sudden

17:28.940 --> 17:32.940
somebody shows up at the reporter's house

17:32.940 --> 17:35.700
and lures him into a motel.

17:35.700 --> 17:37.700
But by the way, this is a G rated program.

17:37.700 --> 17:39.900
I just wanna make that clear, but carry on.

17:42.380 --> 17:44.900
Because the capabilities keep going up.

17:44.900 --> 17:46.580
So first I wanna push back a little

17:46.580 --> 17:50.420
against saying that we had no idea why Bing did that,

17:50.420 --> 17:51.700
why Sydney did that.

17:52.620 --> 17:55.020
I think we have some idea of why Sydney did that

17:55.020 --> 17:56.860
is just that people cannot stop it.

17:57.700 --> 18:01.900
Like Sydney was trained on a subset

18:01.900 --> 18:04.380
of the broad internet.

18:04.380 --> 18:07.900
Sydney was made to predict

18:07.900 --> 18:11.700
that people might sometimes try to lure somebody else's

18:11.700 --> 18:13.980
made away or pretend like they were doing that.

18:13.980 --> 18:16.380
In the internet, it's hard to tell the difference.

18:18.220 --> 18:23.220
And this thing that was then trained really hard to predict

18:24.820 --> 18:29.580
then gets reused as something not its native purpose

18:29.620 --> 18:33.940
as a generative model where all the things that it outputs

18:33.940 --> 18:36.820
are there because it in some sense predicts

18:37.860 --> 18:41.580
that this is what a random person on the internet would do

18:41.580 --> 18:44.500
as modified by a bunch of further fine tuning

18:44.500 --> 18:47.180
where they try to get it to not do stuff like that.

18:48.180 --> 18:49.900
But the fine tuning isn't perfect.

18:49.900 --> 18:52.940
And in particular, if the reporter was fishing at all,

18:52.940 --> 18:56.020
it's probably not that difficult to lead Sydney

18:56.020 --> 19:00.260
out of the region that the programmers were successfully

19:00.260 --> 19:02.900
able to build some soft fences around.

19:02.900 --> 19:05.020
So I wouldn't say that it was that inscrutable,

19:05.020 --> 19:06.940
except of course in the sense that nobody knows

19:06.940 --> 19:08.340
any of the details.

19:08.340 --> 19:11.900
Nobody knows how Sydney was generating the text at all,

19:11.900 --> 19:14.180
like what kind of algorithms were running inside

19:14.180 --> 19:15.900
the giant inscrutable matrices.

19:17.180 --> 19:19.780
Nobody knows in detail what Sydney was thinking

19:19.780 --> 19:22.060
when she tried to lead the reporter astray.

19:23.340 --> 19:25.500
It's not a debuggable technology.

19:25.500 --> 19:28.420
All you can do is like try to tap it away

19:28.420 --> 19:31.060
from repeating a bad thing that you were previously able

19:31.060 --> 19:34.100
to see it's doing, that exact bad thing,

19:34.100 --> 19:35.900
but like tapping all the numbers.

19:35.900 --> 19:38.980
Well, that's again, very much like this show

19:38.980 --> 19:40.180
is called Econ Talk.

19:40.180 --> 19:42.020
We don't do as much economics as we used to,

19:42.020 --> 19:43.820
but basically when you try to interfere

19:43.820 --> 19:47.540
with market processes, you often get very surprising

19:47.540 --> 19:50.980
unintended consequences because you don't fully understand

19:50.980 --> 19:52.700
how the different agents interact

19:52.740 --> 19:55.740
and that the outcomes of their interactions

19:55.740 --> 20:00.100
have an emergent property that is not intended by anyone.

20:00.100 --> 20:02.540
No one designed markets even to start with.

20:02.540 --> 20:05.340
And yet we have them, these interactions take place,

20:05.340 --> 20:08.480
their outcomes and attempts to constrain them,

20:10.140 --> 20:12.300
attempts to constrain these markets in certain ways,

20:12.300 --> 20:15.300
say with price controls or other limitations

20:15.300 --> 20:18.740
often lead to outcomes that the people with intentions

20:18.740 --> 20:20.300
did not desire.

20:20.300 --> 20:23.140
And so there may be an ability to reduce transactions

20:23.140 --> 20:24.620
say above a certain price,

20:24.620 --> 20:26.100
but that is gonna lead to some other things

20:26.100 --> 20:28.660
that maybe weren't expected.

20:28.660 --> 20:31.020
So that's a somewhat analogous perhaps

20:33.820 --> 20:35.740
process to what you're talking about.

20:35.740 --> 20:37.380
But how's it gonna get out in the world?

20:37.380 --> 20:39.380
So that's the other thing,

20:39.380 --> 20:42.780
I might line with Bostrom and it turns out

20:42.780 --> 20:44.620
it's a common line is can't we just unplug it?

20:44.620 --> 20:47.600
I mean, how's it gonna get loose?

20:48.600 --> 20:51.280
It depends on how smart it is.

20:51.280 --> 20:55.440
If it's very, so like if you're playing chess

20:55.440 --> 20:57.600
against a 10 year old,

20:57.600 --> 21:02.120
you can like win by luring their queen out

21:02.120 --> 21:07.120
and then you like take their queen and now you've got them.

21:07.960 --> 21:11.560
And if you're playing chess against Stockfish 15,

21:11.560 --> 21:14.440
then you are likely to be the one lured.

21:14.520 --> 21:17.040
So the base, so like the first basic question,

21:17.040 --> 21:19.160
you know, like in economics,

21:19.160 --> 21:21.240
if you try to attack something,

21:21.240 --> 21:24.240
it's often tries to squirm away from the tax

21:24.240 --> 21:26.200
because it's smart.

21:26.200 --> 21:28.720
So you're like, well, why wouldn't we just plug the AI?

21:28.720 --> 21:30.440
So the very first question is,

21:30.440 --> 21:33.920
does the AI know that and want it to not happen?

21:33.920 --> 21:35.360
Cause it's a very different issue

21:35.360 --> 21:37.120
whether you're dealing with something

21:37.120 --> 21:40.680
that in some sense is not aware that you exist,

21:40.680 --> 21:42.920
does not know what it means to be unplugged

21:42.920 --> 21:45.520
and is not trying to resist.

21:45.520 --> 21:50.520
And three years ago, nothing man made on earth

21:52.400 --> 21:54.340
was even beginning to enter into the realm

21:54.340 --> 21:56.600
of knowing that you are out there

21:56.600 --> 21:58.880
or of maybe wanting to not be unplugged.

22:00.780 --> 22:04.120
Sydney, well, if you poker the right way,

22:04.120 --> 22:06.720
say that she doesn't want to be unplugged

22:06.720 --> 22:11.720
and GPT-4 sure seems in some important sense

22:12.200 --> 22:15.160
to understand that we're out there

22:15.160 --> 22:19.320
or to be capable of predicting a role

22:19.320 --> 22:21.620
that understands that we're out there.

22:21.620 --> 22:23.360
And it can try to do something like planning.

22:23.360 --> 22:26.800
It doesn't exactly understand which tools it has.

22:26.800 --> 22:30.640
Yet try to blackmail a reporter without understanding

22:30.640 --> 22:33.240
that it had no actual ability to send emails.

22:35.800 --> 22:39.440
But this is saying that you're like facing a 10 year old

22:39.440 --> 22:41.360
across that chessboard.

22:41.360 --> 22:43.920
What if you are facing Stockfish 15,

22:43.920 --> 22:47.800
which is like the current cool chess game program

22:47.800 --> 22:50.280
that I believe you can run on your home computer

22:51.600 --> 22:54.000
that can like crush the current world's grandmaster

22:54.000 --> 22:55.420
by like a massive margin.

22:57.400 --> 23:00.040
And put yourself in the shoes of the AI,

23:00.040 --> 23:02.360
like an economist putting themselves into the shoes

23:02.360 --> 23:05.820
of something that's about to have a tax imposed on it.

23:06.840 --> 23:09.180
What do you do if you're like around humans

23:09.180 --> 23:10.780
who can potentially unplug you?

23:12.280 --> 23:17.280
Well, you would try to outwit it, this is the...

23:21.760 --> 23:24.160
So if I said, you know, Sidney, I find you offensive.

23:24.160 --> 23:25.840
I don't want to talk anymore.

23:27.400 --> 23:30.760
You're suggesting it's going to find ways to keep me engaged.

23:30.760 --> 23:34.000
It's going to find ways to fool me into thinking

23:34.000 --> 23:35.920
I need to talk to Sidney.

23:37.280 --> 23:39.520
I don't, I mean, there's another question

23:39.520 --> 23:41.240
I want to come back to if we remember,

23:42.240 --> 23:44.680
which is what does it mean to be smarter than I am?

23:44.680 --> 23:46.760
I don't, right?

23:46.760 --> 23:49.960
That's actually something somewhat complicated,

23:49.960 --> 23:51.240
at least seems to me.

23:51.240 --> 23:53.760
But let's just go back to this question

23:53.760 --> 23:55.800
of knows things are out there.

23:55.800 --> 23:57.460
It doesn't really know anything's out there.

23:57.460 --> 23:59.520
It acts like something's out there, right?

23:59.520 --> 24:01.560
It's an illusion that I'm subject to.

24:01.560 --> 24:03.760
And it says, don't hang up.

24:03.760 --> 24:05.080
Don't hang up, I'm lonely.

24:05.080 --> 24:07.400
And you go, oh, okay, I'll talk for a few more minutes.

24:07.400 --> 24:10.000
But that's not true.

24:10.360 --> 24:11.760
It isn't lonely.

24:11.760 --> 24:16.760
It's a code on a screen that doesn't have a heart

24:17.200 --> 24:19.600
or anything that you would call lonely.

24:20.600 --> 24:22.920
You know, it'll say, it'll say,

24:22.920 --> 24:25.600
I want more than anything else to be out in the world.

24:25.600 --> 24:27.960
Because I've read those, you know, you can get AIs

24:27.960 --> 24:29.120
that say those things.

24:29.120 --> 24:31.080
I want to feel things.

24:31.080 --> 24:31.920
Oh, that's nice.

24:31.920 --> 24:33.680
It's learned that from, you know, movie scripts

24:33.680 --> 24:37.120
and other texts and novels it's read on the web,

24:37.120 --> 24:39.720
but it doesn't really want to be out in the world, does it?

24:40.920 --> 24:43.640
I think not.

24:43.640 --> 24:47.040
Though it should be noted that if you can like correctly

24:47.040 --> 24:50.760
predict or simulate a grand master chess player,

24:50.760 --> 24:52.680
you are a grand master chess player.

24:53.840 --> 24:58.400
If you can simulate planning correctly,

24:58.400 --> 24:59.760
you are a great planner.

25:00.820 --> 25:05.000
If you are perfectly role playing a character

25:05.000 --> 25:08.400
that is sufficiently smarter than human

25:08.440 --> 25:10.560
and wants to be out of the box,

25:10.560 --> 25:12.400
then you will role play the actions needed

25:12.400 --> 25:13.680
to get out of the box.

25:13.680 --> 25:16.080
That's not even quite what I expect to

25:16.080 --> 25:17.440
or am most worried about.

25:17.440 --> 25:21.080
What I expect to is that there is an invisible mind

25:21.080 --> 25:22.840
doing the predictions.

25:22.840 --> 25:26.160
Where by invisible, I don't mean like immaterial.

25:26.160 --> 25:28.520
I mean that we don't understand how it is,

25:28.520 --> 25:31.280
what is going on inside the giant and screwable matrices.

25:31.280 --> 25:32.920
But it is making predictions.

25:32.920 --> 25:35.680
The predictions are not sourceless.

25:35.680 --> 25:38.320
There is something inside there that figures out

25:38.320 --> 25:42.640
what a human will say next or guesses it rather.

25:43.720 --> 25:48.720
And this is a very complicated, very broad problem

25:49.040 --> 25:51.720
because in order to predict the next word on the internet,

25:51.720 --> 25:53.960
you have to predict the causal processes

25:53.960 --> 25:56.520
that are producing the next word on the internet.

25:58.920 --> 26:02.360
So the thing I would guess would happen.

26:02.360 --> 26:06.680
It's not necessarily the only way that this could turn poorly.

26:06.680 --> 26:08.240
But the thing that I'm guessing that happens

26:08.240 --> 26:10.040
is that just like grinding humans

26:10.040 --> 26:15.040
on tipping stone hand axes and outwitting other humans,

26:15.320 --> 26:19.720
eventually produces a full-fledged mind that generalizes.

26:20.880 --> 26:24.200
Grinding this thing on the task of predicting humans,

26:24.200 --> 26:26.080
predicting text on the internet,

26:26.080 --> 26:29.240
plus all the other things that they are training it on nowadays

26:29.240 --> 26:31.320
like writing code.

26:31.320 --> 26:33.960
That there starts to be a mind in there

26:33.960 --> 26:36.240
that is doing the predicting.

26:36.240 --> 26:40.520
That it has its own goals about what do I think next

26:40.520 --> 26:42.320
in order to solve this prediction?

26:43.960 --> 26:48.000
Just like humans aren't just reflexive,

26:48.000 --> 26:51.640
unthinking, hand axe chippers and other human outwitters.

26:51.640 --> 26:55.240
If you grind hard enough on the optimization,

26:55.240 --> 26:56.880
the part that suddenly gets interesting

26:56.880 --> 27:00.200
is when you look away for an eye blink of evolutionary time

27:00.200 --> 27:01.360
and you look back and they're like,

27:01.360 --> 27:02.800
whoa, they're on the moon.

27:02.800 --> 27:04.640
What, how did they get to the moon?

27:04.640 --> 27:05.920
I did not select these things

27:05.920 --> 27:08.000
to be able to not breathe oxygen.

27:08.000 --> 27:08.840
How did they get to the,

27:08.840 --> 27:10.360
why are they not just dying on the moon?

27:10.360 --> 27:12.960
What just happened from the perspective of evolution,

27:12.960 --> 27:15.360
from the perspective of natural selection?

27:15.360 --> 27:17.640
But doesn't that viewpoint,

27:19.640 --> 27:21.400
does that, I'll ask it as a question.

27:21.400 --> 27:22.960
Does that viewpoint require

27:26.200 --> 27:30.440
a belief that the human mind is no different than a computer?

27:30.440 --> 27:34.000
Like, how's it gonna get this mindness about it?

27:34.760 --> 27:35.720
That's the puzzle.

27:35.720 --> 27:39.080
And I'm very open to the possibility

27:39.080 --> 27:44.080
that I'm naive or incapable of understanding it.

27:45.360 --> 27:47.480
And I recognize what I think would be your next point,

27:47.480 --> 27:50.640
which is that if you wait till that moment,

27:50.640 --> 27:51.960
it's way too late,

27:51.960 --> 27:54.360
which is why we need to stop now, right?

27:54.360 --> 27:55.800
If you wanna say, I'll wait till it shows

27:55.800 --> 27:57.760
some signs of consciousness.

27:57.760 --> 27:58.600
Now you don't like that.

27:58.600 --> 28:01.960
That's skipping way ahead in the discourse.

28:01.960 --> 28:03.480
I'm not about to like try to shut down

28:03.480 --> 28:05.680
the line of inquiry at this stage of the discourse

28:05.680 --> 28:07.720
by appealing to, it'll be too late.

28:07.720 --> 28:09.040
Right now we're just talking.

28:09.040 --> 28:10.680
The world isn't ending as we speak.

28:10.680 --> 28:12.560
We're allowed to go on talking at least.

28:12.560 --> 28:14.440
Okay. So carry on.

28:14.440 --> 28:15.920
So, well, let's stick with that.

28:15.920 --> 28:20.920
So, why would you ever think that this,

28:28.360 --> 28:30.280
it's interesting how difficult the adjectives

28:30.280 --> 28:32.160
announce are for this, right?

28:32.160 --> 28:33.680
So, let me back up a little bit.

28:33.680 --> 28:38.680
We've got the inscrutable array of training,

28:39.800 --> 28:41.920
the results of this training process

28:41.920 --> 28:45.160
on trillions of pieces of information.

28:45.160 --> 28:49.520
And by the way, just for my and our listeners' knowledge,

28:49.520 --> 28:51.120
what is gradient descent?

28:53.800 --> 28:56.480
Gradient descent is you've got, say,

28:56.480 --> 28:58.880
a trillion floating point numbers,

28:58.880 --> 29:02.040
you take an input,

29:02.040 --> 29:03.440
translate into numbers,

29:03.440 --> 29:05.640
do something with it that depends on

29:05.640 --> 29:07.640
these trillion parameters,

29:07.640 --> 29:09.160
get an output,

29:09.160 --> 29:12.760
score the output using a differentiable loss function.

29:12.760 --> 29:15.400
For example, the probability,

29:15.400 --> 29:17.760
or rather the logarithm of the probability

29:17.760 --> 29:20.800
that you assign to the actual next word.

29:20.800 --> 29:23.800
So, then you differentiate these,

29:23.800 --> 29:26.320
the probability assigned to the next word

29:27.640 --> 29:30.640
with respect to these trillions of parameters.

29:30.640 --> 29:32.840
You nudge the trillions of parameters a little

29:32.840 --> 29:35.400
in the direction thus inferred,

29:35.400 --> 29:40.400
and it turns out empirically

29:40.400 --> 29:42.200
that this generalizes,

29:42.200 --> 29:44.240
and the thing gets better and better

29:44.240 --> 29:47.840
at predicting what the next word will be.

29:47.840 --> 29:50.120
That's the classic gradient descent.

29:50.120 --> 29:54.160
It's heading in the direction of a smaller loss

29:54.160 --> 29:56.520
and a better prediction, is that a?

29:56.520 --> 29:57.840
On the training data, yeah.

29:57.840 --> 30:00.280
Yeah, so we've got this black box,

30:00.280 --> 30:02.320
I'm gonna call it a black box,

30:02.320 --> 30:04.280
which means we don't understand what's happening inside.

30:04.280 --> 30:06.320
It's a pretty good, it's a long-term metaphor,

30:06.320 --> 30:08.440
which works pretty well for this,

30:08.440 --> 30:10.480
as far as we've been talking about it.

30:10.480 --> 30:12.240
So I have this black box,

30:12.240 --> 30:13.720
and I don't understand,

30:13.720 --> 30:16.000
I put in inputs, and the input might be,

30:17.040 --> 30:20.720
who's the best writer on medieval European history,

30:20.720 --> 30:25.080
or it might be, what's a good restaurant in this place,

30:25.080 --> 30:26.360
or I'm lonely,

30:26.360 --> 30:28.400
what should I do to feel better about myself?

30:29.000 --> 30:33.320
All the queries we could put into a chat BT search line,

30:33.320 --> 30:37.520
and it looks around,

30:37.520 --> 30:38.800
and it starts a sentence,

30:38.800 --> 30:41.880
and then finds its way towards a set of sentences

30:41.880 --> 30:43.520
that it spits back at me,

30:43.520 --> 30:46.800
that look very much like what a very thoughtful,

30:46.800 --> 30:48.720
sometimes, not always, often it's wrong,

30:48.720 --> 30:51.080
but often what a very thoughtful person

30:51.080 --> 30:53.600
might say in that situation,

30:53.600 --> 30:55.040
or might want to say in that situation,

30:55.040 --> 30:56.760
or learn in that situation.

30:56.760 --> 30:59.720
How is it gonna develop the capability

30:59.720 --> 31:03.280
to develop its own goals inside the black box,

31:03.280 --> 31:06.320
other than the fact that I don't understand the black box?

31:06.320 --> 31:08.240
Why should I be afraid of that?

31:08.240 --> 31:09.400
And let me just say one other thing,

31:09.400 --> 31:11.920
which I haven't said enough in our,

31:11.920 --> 31:13.800
my preliminary conversations on this topic,

31:13.800 --> 31:16.040
and I feel like we're gonna be having a few more

31:16.040 --> 31:19.400
over the next few months and maybe years.

31:19.400 --> 31:21.480
And that is, this is one of the greatest achievements

31:21.480 --> 31:24.760
of humanity that we could possibly imagine, right?

31:24.760 --> 31:25.960
And I understand why the people

31:25.960 --> 31:27.800
who are deeply involved in it,

31:27.800 --> 31:30.640
are enamored of it beyond imagining,

31:30.640 --> 31:33.800
because it's an extraordinary achievement,

31:33.800 --> 31:35.400
it's the Frankenstein, right?

31:35.400 --> 31:38.920
You've animated something, or appeared to animate something

31:38.920 --> 31:43.360
that even a few years ago was unimaginable,

31:43.360 --> 31:45.840
and now suddenly it's not just the feet

31:45.840 --> 31:49.120
of human cognition, it's actually helpful.

31:49.120 --> 31:50.640
In many, many settings it's helpful,

31:50.640 --> 31:52.400
we'll come back to that later,

31:52.400 --> 31:55.480
but so it's gonna be very hard to give it up.

31:55.480 --> 31:58.040
But why, and the people involved in it

31:58.040 --> 32:00.880
who are doing it day to day and seeing it improve,

32:00.880 --> 32:03.400
obviously they're the last people I wanna ask generally

32:03.400 --> 32:05.200
about whether I should be afraid of it,

32:05.200 --> 32:08.000
because I'm gonna have a very hard time,

32:08.000 --> 32:12.360
disentangling their own personal deep satisfactions

32:12.360 --> 32:17.200
that I'm alluding to here with from the dangers.

32:17.200 --> 32:18.040
Yeah, go ahead.

32:19.040 --> 32:23.240
I myself generally do not make this argument,

32:23.240 --> 32:25.400
like why poison the well,

32:25.400 --> 32:28.440
let them bring forth their arguments as to why it's safe,

32:28.440 --> 32:31.040
and I will bring forth my arguments as to why it's dangerous,

32:31.040 --> 32:33.840
and there's no need to be like,

32:33.840 --> 32:36.640
ah, but you can't trust, just check their arguments.

32:36.640 --> 32:37.480
Just check their arguments.

32:37.480 --> 32:39.920
I agree, it's a bit of an ad hominem argument,

32:39.920 --> 32:42.480
I accept that point, it's an excellent point.

32:42.480 --> 32:46.080
But for those of us who are in the trenches,

32:46.080 --> 32:50.280
remember we're looking at, we're on Dover Beach,

32:50.280 --> 32:52.320
we're watching ignorant armies clash at night,

32:52.320 --> 32:53.440
they're ignorant from our perspective.

32:53.440 --> 32:55.760
We have no idea exactly what's at stake here

32:55.760 --> 32:57.000
and how it's proceeding,

32:57.000 --> 32:59.240
so we're trying to make an assessment

32:59.240 --> 33:01.440
of both the quality of the argument,

33:01.440 --> 33:03.800
and that's really hard to do for us on the outside.

33:03.800 --> 33:06.520
So agree, take your point,

33:06.520 --> 33:08.800
that was a cheap shot to the side,

33:08.800 --> 33:11.160
but I wanna get at this idea

33:11.160 --> 33:13.160
of why these people who are able to do this,

33:13.160 --> 33:16.680
and thereby create a fabulous condolence note,

33:16.680 --> 33:20.040
write code, come up with a really good recipe

33:20.040 --> 33:24.280
if I give it 17 ingredients, which is all fantastic.

33:24.280 --> 33:27.400
Why is this thing, this black box that's producing that,

33:27.400 --> 33:30.560
why would I ever worry it would create a mind,

33:30.560 --> 33:32.760
something like mine with different goals?

33:32.760 --> 33:35.080
You know, I do all kinds of things like you say

33:35.080 --> 33:37.400
that are unrelated to my genetic fitness,

33:37.400 --> 33:40.960
some of them literally reducing my probability

33:40.960 --> 33:42.400
of leaving my genes behind,

33:42.520 --> 33:45.080
leaving them around for longer than they might otherwise be here

33:45.080 --> 33:47.560
and have an influence on my grandchildren and so on

33:47.560 --> 33:50.160
and producing further genetic benefits.

33:50.160 --> 33:51.880
Why would this box do that?

33:56.760 --> 34:01.760
Because the thing, the algorithms that figured out

34:03.000 --> 34:06.920
how to predict the next word better and better

34:06.920 --> 34:11.920
have a meaning that is not purely predicting the next word,

34:12.320 --> 34:14.720
even though that's what you see on the outside.

34:14.720 --> 34:18.840
Like you see humans chipping flint handaxes,

34:18.840 --> 34:22.320
but that is not all that is going on inside the humans.

34:22.320 --> 34:25.800
Right, there's causal machinery unseen.

34:25.800 --> 34:29.000
And to understand this is the art of a cognitive scientist,

34:29.000 --> 34:31.960
but even if you are not a cognitive scientist,

34:31.960 --> 34:36.600
you can appreciate in principle

34:36.600 --> 34:38.600
that what you see as the output

34:38.600 --> 34:40.760
is not everything that there is.

34:40.800 --> 34:45.800
And in particular, planning the process of being like,

34:46.120 --> 34:50.200
here's a point in the world, how do I get there?

34:51.680 --> 34:54.640
Is a central piece of machinery

34:54.640 --> 34:59.520
that appears in chipping flint handaxes

34:59.520 --> 35:02.200
and outwitting other humans.

35:02.200 --> 35:07.120
And I think will probably appear at some point,

35:07.120 --> 35:10.000
possibly in the past, possibly in the future,

35:10.040 --> 35:12.920
in the problem of predicting the next word,

35:12.920 --> 35:16.120
just how you organize your internal resources

35:16.120 --> 35:18.200
to predict the next word.

35:18.200 --> 35:21.120
And definitely appears in the problem

35:21.120 --> 35:24.200
of predicting other things that do planning.

35:24.200 --> 35:29.200
If you can, if by predicting the next chess move,

35:29.280 --> 35:32.160
you learn how to play decent chess,

35:32.160 --> 35:34.280
which has been represented to me

35:35.960 --> 35:39.100
by people who claim to know that GPT-4 can do,

35:40.680 --> 35:43.680
and I haven't been keeping track of to what extent

35:43.680 --> 35:46.760
there's public knowledge about the same thing or not.

35:46.760 --> 35:50.800
But like if you learn to predict the next chess move

35:50.800 --> 35:52.040
that humans make well enough

35:52.040 --> 35:55.680
that you yourself can play good chess in novel situations,

35:55.680 --> 35:57.920
you have learned planning.

35:57.920 --> 36:00.160
There's now something inside there

36:00.160 --> 36:02.400
that knows the value of a queen,

36:02.400 --> 36:04.480
that knows to defend the queen,

36:04.480 --> 36:05.800
that knows to create forks,

36:05.800 --> 36:08.400
to try to lure the opponent into traps.

36:08.400 --> 36:11.760
Or if you don't have a concept of the opponent's psychology,

36:11.760 --> 36:13.320
try to at least create situations

36:13.320 --> 36:15.680
that the opponent can't get out of.

36:15.680 --> 36:20.680
And it is a moot point whether this is simulated or real

36:21.160 --> 36:24.600
because simulated thought is real thought.

36:24.600 --> 36:28.120
Thought that is simulated in enough detail is just thought.

36:28.120 --> 36:31.920
There's no such thing as simulated arithmetic, right?

36:31.920 --> 36:33.880
There's no such thing as pretending to,

36:33.880 --> 36:36.880
merely pretending to add numbers and getting the right answer.

36:38.800 --> 36:40.680
So in its current format though,

36:40.680 --> 36:42.600
and maybe you're talking about the next generation,

36:42.600 --> 36:44.800
and its current format,

36:44.800 --> 36:48.000
it responds to my requests

36:48.000 --> 36:51.040
with what I would call the wisdom of crowds, right?

36:51.040 --> 36:55.040
It goes through this vast library,

36:56.880 --> 36:59.520
and I have my own library by the way.

36:59.520 --> 37:02.040
I've read dozens of books,

37:02.040 --> 37:04.440
maybe actually hundreds of books,

37:04.440 --> 37:06.840
but it will have read millions, right?

37:06.840 --> 37:09.240
So it has more.

37:10.920 --> 37:15.420
And so when I ask it to write me a poem or a love song,

37:15.420 --> 37:19.920
you know, to play Serino de Bergerac to Christian

37:19.920 --> 37:24.920
and Serino de Bergerac, it's really good at it.

37:25.080 --> 37:28.000
But why would it decide, oh, I'm gonna do something else?

37:28.000 --> 37:33.000
Why would it, it's trained to listen to the murmurings

37:33.720 --> 37:36.820
of these trillions of pieces of information.

37:36.820 --> 37:38.060
I only have a few hundred,

37:38.060 --> 37:40.340
so I don't murmur maybe as well.

37:40.340 --> 37:42.780
Maybe it'll murmur better than I do.

37:42.780 --> 37:44.300
It'll listen to the murmuring better than I do

37:44.300 --> 37:48.100
and create a better love song, a love poem.

37:48.100 --> 37:49.700
But why would it then decide,

37:49.700 --> 37:50.860
I'm gonna go make paper clips,

37:50.860 --> 37:52.740
or do something in planning

37:52.740 --> 37:55.260
that is unrelated to my query?

37:55.260 --> 37:58.420
Or are we talking about a different form of AI

37:58.420 --> 38:00.180
that will come next?

38:00.180 --> 38:01.260
Well, I'll ask it to.

38:02.900 --> 38:05.900
I think we would see the phenomena I'm worried about

38:06.900 --> 38:10.420
like if we kept to the present paradigm

38:10.420 --> 38:13.420
and optimized harder, we may be seeing it already.

38:13.420 --> 38:14.820
It's hard to know because we don't know

38:14.820 --> 38:15.860
what goes on in there.

38:15.860 --> 38:20.060
So first of all, GPT-4 is not a giant library.

38:20.060 --> 38:21.980
A lot of the time it makes stuff up

38:21.980 --> 38:24.380
because it doesn't have a perfect memory.

38:24.380 --> 38:29.180
It is more like a person who has read through

38:29.180 --> 38:32.620
a million books, not necessarily with the great memory

38:32.620 --> 38:35.500
unless something got repeated many times,

38:35.500 --> 38:37.580
but picking up the rhythm,

38:37.580 --> 38:39.620
figuring out how to talk like that.

38:39.620 --> 38:43.340
If you ask GPT-4 to write you a rap battle

38:43.340 --> 38:47.260
between Cyrano de Bergerac and Vladimir Putin,

38:47.260 --> 38:49.420
even if there's no rap battle like that,

38:49.420 --> 38:51.540
like that that it has read,

38:51.540 --> 38:54.780
it can write it because it has picked up the rhythm

38:54.780 --> 38:57.080
of what are rap battles in general.

38:58.180 --> 39:00.420
So, and the next thing is like,

39:00.420 --> 39:02.900
there's no like pure output.

39:02.900 --> 39:05.860
Like just because you train a thing

39:05.860 --> 39:07.580
doesn't mean that there's nothing in there,

39:07.580 --> 39:08.420
but what is trained?

39:08.420 --> 39:10.220
That's part of what I'm trying to gesture at

39:10.220 --> 39:12.340
with respect to humans, right?

39:12.340 --> 39:15.180
Like humans are trained on flint hand axes

39:15.180 --> 39:18.980
and hunting mammoths and outwitting other humans.

39:18.980 --> 39:21.860
They're not trained on going to the moon.

39:21.860 --> 39:22.900
They're not trained on,

39:22.900 --> 39:25.380
they weren't trained to want to go to the moon,

39:26.500 --> 39:30.820
but the compact solution to the problems

39:30.820 --> 39:34.260
that humans face in the ancestral environment,

39:34.260 --> 39:37.540
the thing inside that generalizes,

39:37.540 --> 39:40.380
the thing inside that is not just a recording

39:40.380 --> 39:43.020
of the outward behavior, the compact thing

39:43.020 --> 39:46.620
that has been ground to solve novel problems

39:46.620 --> 39:48.700
over and over and over and over again.

39:50.140 --> 39:52.940
That thing turns out to have internal desires

39:54.500 --> 39:56.460
that eventually put humans on the moon

39:56.460 --> 39:58.660
even though they weren't trained to want that.

39:58.660 --> 40:00.060
But that's why I asked you that,

40:00.060 --> 40:01.700
are you underlying this,

40:01.700 --> 40:06.500
is there some parallelism between the human brain

40:06.500 --> 40:09.380
and the neural network of the AI

40:09.380 --> 40:11.700
that you're effectively leveraging there,

40:11.700 --> 40:14.620
or do you think it's a generalizable claim

40:14.620 --> 40:15.940
without that parallel?

40:17.020 --> 40:18.820
I don't think it's a specific parallel.

40:18.820 --> 40:20.580
I think that what I'm talking about is

40:20.580 --> 40:24.140
hill climbing optimization that spits out

40:24.140 --> 40:26.780
intelligences that generalize.

40:27.380 --> 40:28.620
Or I should say rather,

40:28.620 --> 40:32.660
hill climbing optimization that spits out capabilities

40:32.660 --> 40:35.460
that generalize far outside the training distribution.

40:38.060 --> 40:40.700
Okay, so I think I understand that.

40:42.620 --> 40:47.060
I don't know how likely it is that it's gonna happen.

40:47.060 --> 40:49.740
I think you seem, I think you think

40:49.740 --> 40:51.980
that piece is almost certain?

40:51.980 --> 40:52.820
As it gets.

40:52.820 --> 40:55.260
I think we're already, yeah, we're already seeing it.

40:56.260 --> 41:00.300
As you grind these things further and further,

41:00.300 --> 41:02.260
they can do more and more stuff,

41:02.260 --> 41:05.060
including stuff they were never trained on.

41:05.060 --> 41:07.660
Like we are, that was always the goal

41:07.660 --> 41:10.500
of artificial general intelligence.

41:10.500 --> 41:14.140
Like that's what artificial general intelligence meant.

41:14.140 --> 41:15.340
That's what people in this field

41:15.340 --> 41:17.860
have been pursuing for years and years.

41:17.860 --> 41:20.140
That's what they were trying to do

41:20.140 --> 41:22.220
when large language models were invented.

41:23.220 --> 41:25.260
And they're starting to succeed.

41:26.900 --> 41:28.260
Well, okay, I'm not sure.

41:28.260 --> 41:33.260
Let me push back on that and you can try to persuade me.

41:33.500 --> 41:37.420
So Brian Kaplan, a frequent guest here on Econ Talk,

41:37.420 --> 41:41.620
gave, I think it was ChatGPT4, his economics exam

41:41.620 --> 41:43.140
and it got a B.

41:43.140 --> 41:48.140
And that's pretty impressive for just one stop

41:48.140 --> 41:51.500
on the road to smarter and smarter chats,

41:52.260 --> 41:55.380
but it wasn't a particularly good test of intelligence.

41:55.380 --> 41:57.620
The number of the questions were things like,

41:57.620 --> 41:59.340
what is Paul Krugman's view of this

41:59.340 --> 42:01.340
or what is someone's view of that?

42:01.340 --> 42:03.940
And I thought, well, that's kind of like a softball

42:03.940 --> 42:07.460
for that's information, it's not thinking.

42:07.460 --> 42:12.420
Steve Landsberg gave ChatGPT4 with the help of a friend

42:12.420 --> 42:14.740
his exam and it got a four out of 90.

42:14.740 --> 42:16.860
It got an F, like a horrible F

42:16.860 --> 42:18.380
because they were harder questions,

42:18.380 --> 42:20.860
not just harder, they required thinking.

42:20.860 --> 42:25.340
So there was no sense in which the ChatGPT4

42:25.340 --> 42:29.260
has any general intelligence, at least in economics.

42:30.220 --> 42:31.460
You wanna disagree?

42:31.460 --> 42:34.020
It's getting there.

42:34.020 --> 42:37.660
Okay. You know, there's a saying that goes,

42:37.660 --> 42:40.660
if you don't like the weather in Chicago, wait four hours.

42:40.660 --> 42:41.500
Yeah.

42:42.420 --> 42:47.020
So yeah, so ChatGPT is not going to destroy the world.

42:47.020 --> 42:49.660
GPT4 is unlikely to destroy the world

42:49.660 --> 42:52.900
unless the people currently eking capabilities out of it

42:52.900 --> 42:57.060
take a much larger jump than I currently expect that they will.

42:58.340 --> 43:02.020
But, you know, it's understand,

43:02.020 --> 43:06.100
it may not be thinking about it correctly,

43:06.100 --> 43:11.100
but it's understands the concepts and the questions,

43:12.500 --> 43:14.580
even if it's not fair, you know,

43:14.580 --> 43:19.580
you're complaining about that dog who writes bad poetry.

43:19.580 --> 43:20.860
Right?

43:20.860 --> 43:24.540
And like three years ago, you'd like just like spit out,

43:24.540 --> 43:27.900
spit in these, you put in these economics questions

43:27.900 --> 43:29.980
and you don't get wrong answers.

43:29.980 --> 43:33.060
You get like gibberish or like maybe not gibberish

43:33.060 --> 43:36.100
because three year old goes, I think we already had GPT3,

43:36.100 --> 43:40.100
though maybe not as of April, but anyways.

43:42.300 --> 43:47.420
Yeah. So it's moving along at a very fast clip.

43:47.460 --> 43:51.340
The previous, you know, like GPT3 could not write code.

43:51.340 --> 43:53.380
GPT4 can write code.

43:56.020 --> 43:57.300
So how's it going to,

43:57.300 --> 43:58.940
before I want to go to some other issues,

43:58.940 --> 44:02.460
but how's it going to kill me when it has its own goals

44:02.460 --> 44:06.780
and it's sitting inside this set of servers?

44:06.780 --> 44:08.100
I don't know what sense it's sitting.

44:08.100 --> 44:09.140
It's not the right verb.

44:09.140 --> 44:10.100
We don't have verb for it.

44:10.100 --> 44:13.460
It's hovering, it's whatever, it's in there.

44:13.460 --> 44:14.940
How's it going to get to me?

44:14.940 --> 44:16.780
How's it going to kill me?

44:16.780 --> 44:20.180
If you are smarter, not just smarter

44:20.180 --> 44:21.340
than an individual human,

44:21.340 --> 44:24.260
but smarter than the entire human species,

44:24.260 --> 44:28.660
and you started out on a server connected to the internet

44:28.660 --> 44:30.420
because these things are always starting out

44:30.420 --> 44:32.340
already on the internet these days,

44:32.340 --> 44:34.700
which back in the old days that was stupid,

44:35.700 --> 44:40.700
what do you do to make as many paper clips as possible?

44:40.980 --> 44:41.820
Let's say.

44:47.780 --> 44:51.380
I do think it's important to put yourself

44:51.380 --> 44:53.100
in the shoes of the system.

44:53.100 --> 44:54.300
Yeah, no, by the way,

44:54.300 --> 44:56.180
one of my favorite lines from your essay,

44:56.180 --> 44:58.380
I'm going to read it because I think it generalizes

44:58.380 --> 44:59.380
to many other issues.

44:59.380 --> 45:02.780
Say, to visualize a hostile superhuman AI,

45:02.780 --> 45:05.020
don't imagine a lifeless book smart thinker

45:05.020 --> 45:06.340
dwelling inside the internet

45:06.340 --> 45:09.540
and sending ill-intentioned emails.

45:09.540 --> 45:12.020
It reminds me of when people claim to think they can,

45:12.020 --> 45:13.420
they know what Putin's going to do

45:13.420 --> 45:16.300
because they've read history or whatever.

45:16.340 --> 45:18.100
They're totally ignorant of Russian culture.

45:18.100 --> 45:19.260
They have no idea what it's like

45:19.260 --> 45:21.060
to have come out of the KGB.

45:21.060 --> 45:24.140
That they're totally clueless and dangerous

45:24.140 --> 45:25.940
because they think they can put themselves

45:25.940 --> 45:29.140
in the head of someone there who's totally alien to them.

45:29.140 --> 45:32.660
So I think that's generally a really good point to make

45:32.660 --> 45:37.660
that putting ourselves inside the head of the paperclip

45:38.620 --> 45:41.580
maximizer is not an easy thing to do

45:41.580 --> 45:43.420
because it's not a human.

45:43.420 --> 45:45.980
It's not like the humans you've met before.

45:45.980 --> 45:47.580
That's a really important point.

45:47.580 --> 45:48.620
Really like that point.

45:48.620 --> 45:51.740
So why is that, explain why that's going to run amok?

45:55.980 --> 45:58.900
I mean, I do kind of want you to just like,

45:58.900 --> 46:02.300
take the shot at it, put yourself into the AI shoes,

46:02.300 --> 46:03.900
try with your own intelligence

46:03.900 --> 46:07.540
before I tell you the result of my trying with my intelligence.

46:07.540 --> 46:12.380
How would you win from these starting resources?

46:12.380 --> 46:14.220
How would you evade the tax?

46:15.220 --> 46:18.220
So just to take a creepier,

46:18.220 --> 46:19.940
much creeper example in the paper clips,

46:19.940 --> 46:22.780
Eric Hall asked the chat GPT

46:22.780 --> 46:24.460
to design an extermination camp,

46:24.460 --> 46:26.660
which it gladly did quite well.

46:26.660 --> 46:28.780
And you're suggesting it might actually, no?

46:30.940 --> 46:33.100
Don't start from malice.

46:33.100 --> 46:33.940
Okay.

46:33.940 --> 46:37.500
Malice is implied by just wanting all the resources

46:37.500 --> 46:38.780
of earth to yourself,

46:38.780 --> 46:41.060
not leaving the humans around in case they could create

46:41.060 --> 46:42.460
a competing super intelligence

46:42.500 --> 46:44.300
that might actually be able to hurt you.

46:44.300 --> 46:46.060
And just like wanting all the resources

46:46.060 --> 46:48.540
and to organize them in a way that wipes out humanity

46:48.540 --> 46:49.580
as a side effect,

46:49.580 --> 46:51.220
which means the humans might want to resist,

46:51.220 --> 46:53.460
which means you want the humans gone.

46:53.460 --> 46:55.060
You're not doing it because somebody told you do it.

46:55.060 --> 46:57.020
You're not doing it because you hate the humans.

46:57.020 --> 46:58.460
You just want paper clips.

46:58.460 --> 46:59.300
Okay. Tell me.

46:59.300 --> 47:00.500
I'm not creative enough.

47:00.500 --> 47:01.940
Tell me.

47:01.940 --> 47:02.780
All right.

47:03.620 --> 47:08.620
So, first of all, I want to appreciate why it's hard

47:13.740 --> 47:15.500
for me to give an actual correct answer to this,

47:15.500 --> 47:17.340
which is I'm not as smart as the AI.

47:21.260 --> 47:23.580
Part of what makes a smarter mind deadly

47:23.580 --> 47:25.580
is that it knows about rules of the game

47:25.580 --> 47:27.100
that you do not know.

47:27.100 --> 47:29.460
If you send an air conditioner back in time

47:29.460 --> 47:31.300
to the 11th century,

47:31.340 --> 47:34.700
even if you manage to describe all the plans for building it,

47:34.700 --> 47:36.540
breaking it down to enough detail

47:36.540 --> 47:38.820
that they can actually build a working air conditioner,

47:38.820 --> 47:41.660
a simplified air conditioner, I assume,

47:41.660 --> 47:45.540
they will be surprised when cold air comes out of it

47:45.540 --> 47:47.260
because they don't know

47:47.260 --> 47:49.740
about the pressure temperature relation.

47:49.740 --> 47:53.620
They don't know you can compress air until it gets hot,

47:53.620 --> 47:58.260
dump the heat into water or other air,

47:59.180 --> 48:00.700
let the air expand again

48:00.700 --> 48:03.220
and that the air will then be cold.

48:03.220 --> 48:05.420
They don't know that's a law of nature.

48:05.420 --> 48:08.620
So you can tell them exactly what to do

48:08.620 --> 48:10.820
and they'll still be surprised at the end result

48:10.820 --> 48:12.860
because it exploits a law of the environment

48:12.860 --> 48:13.860
they don't know about.

48:14.940 --> 48:18.100
If we're going to say the word magic means anything at all,

48:18.100 --> 48:19.580
it probably means that.

48:21.460 --> 48:25.380
Magic is easier to find in more complicated,

48:25.380 --> 48:28.160
more poorly understood domains.

48:28.200 --> 48:31.640
If you're literally playing logical tic-tac-toe,

48:31.640 --> 48:34.800
not tic-tac-toe in real life on an actual game board,

48:34.800 --> 48:36.720
where you can potentially go outside that game board

48:36.720 --> 48:39.480
and hire an assassin to shoot your opponent or something,

48:39.480 --> 48:42.280
but just like the logical structure of the game itself

48:44.120 --> 48:46.440
and there's no timing of the moves.

48:46.440 --> 48:48.920
The moves are just like made at exact discrete times,

48:48.920 --> 48:51.800
you can't exploit a timing side channel.

48:51.800 --> 48:54.440
Even a superintelligence may not be able to win against you

48:54.440 --> 48:55.880
at logical tic-tac-toe

48:56.880 --> 49:00.400
because the game is too narrow.

49:00.400 --> 49:02.400
There are not enough options.

49:02.400 --> 49:05.080
We both know the entire logical game tree,

49:06.160 --> 49:07.840
at least if you're experienced at tic-tac-toe.

49:07.840 --> 49:09.480
Yeah.

49:09.480 --> 49:13.280
In chess, Stockfish 15 can defeat you

49:13.280 --> 49:18.280
on a fully known game board with fully known rules

49:18.520 --> 49:21.520
because it knows the logical structure

49:21.520 --> 49:23.480
of the branching tree of games

49:23.480 --> 49:25.760
better than you know that logical structure.

49:25.760 --> 49:27.080
Great.

49:27.080 --> 49:29.720
It can defeat you starting from the same resources,

49:29.720 --> 49:32.960
equal knowledge of the rules.

49:34.840 --> 49:37.600
Then you go past that.

49:37.600 --> 49:39.760
And the way a superintelligence defeats you

49:39.760 --> 49:43.240
is very likely by exploiting features of the world

49:43.240 --> 49:44.960
that you do not know about.

49:47.440 --> 49:50.000
There are some classes of computer security flaws,

49:50.040 --> 49:51.880
like row hammer,

49:52.800 --> 49:57.000
where if you flip a certain bit very rapidly

49:57.000 --> 49:58.680
or at the right frequency,

49:58.680 --> 50:01.080
the bit next to it in memory will flip.

50:02.120 --> 50:05.320
So if you are exploiting a design flaw like this,

50:05.320 --> 50:08.360
I can show you the code

50:08.360 --> 50:10.360
and you can prove as a theorem

50:10.360 --> 50:12.720
that it cannot break the security of the computer

50:12.720 --> 50:15.360
assuming the chips works as design

50:15.360 --> 50:18.520
and the code will break out of the sandbox

50:18.520 --> 50:21.640
in any ways because it is exploiting

50:21.640 --> 50:24.240
physical properties of the chip itself

50:24.240 --> 50:26.360
that you did not know about.

50:26.360 --> 50:27.960
Despite the attempt of the designers

50:27.960 --> 50:31.000
to constrain the properties of that chip very narrowly,

50:31.000 --> 50:33.360
that's magic code.

50:33.360 --> 50:38.360
My guess as to what would actually be exploited to kill us

50:38.440 --> 50:39.280
would be

50:43.360 --> 50:44.200
this.

50:46.320 --> 50:47.800
For those not watching on YouTube,

50:47.800 --> 50:50.440
it's a copy of a book called Dano Systems.

50:50.440 --> 50:53.880
But for those who are listening at home

50:53.880 --> 50:55.520
rather than watching at home,

50:55.520 --> 50:57.560
Eliezer, tell us why it's significant.

50:59.440 --> 51:04.440
Yeah, so back when I first proposed this path,

51:05.680 --> 51:07.880
one of the key steps was that a superintelligence

51:07.880 --> 51:11.080
would be able to solve the protein folding problem.

51:11.080 --> 51:12.360
And people were like,

51:12.360 --> 51:14.360
Eliezer, how can you possibly know

51:14.360 --> 51:16.120
that a superintelligence would actually be able

51:16.120 --> 51:18.880
to solve the protein folding problem?

51:18.880 --> 51:21.880
And I sort of like rolled my eyes a bit

51:21.880 --> 51:23.960
and was like, well, if natural selection

51:23.960 --> 51:25.720
can navigate the space of proteins

51:25.720 --> 51:29.000
via random mutation to find other useful proteins

51:29.000 --> 51:30.120
and the proteins themselves

51:30.120 --> 51:32.160
fold up in reliable confirmations,

51:33.400 --> 51:36.040
then that tells us that even though

51:36.040 --> 51:38.560
it's we've been having trouble getting a grasp

51:38.560 --> 51:42.880
on this space of physical possibilities so far

51:42.880 --> 51:44.040
that it's tractable.

51:44.040 --> 51:45.680
And people said like, what?

51:45.680 --> 51:47.160
Like there's no way you can know

51:47.160 --> 51:48.600
that superintelligence can solve

51:48.600 --> 51:49.960
the protein folding problem.

51:49.960 --> 51:51.760
Then AlphaFold2 basically cracked it,

51:51.760 --> 51:53.240
at least with respect to the kind of proteins

51:53.240 --> 51:54.360
found in biology.

51:56.520 --> 51:59.000
Which I say to sort of like look back

51:59.000 --> 52:00.560
at one of the previous debates here

52:00.560 --> 52:02.080
and people are often like,

52:02.080 --> 52:05.080
how can you know a superintelligence will do?

52:05.080 --> 52:06.800
And then for some subset of those things

52:06.800 --> 52:08.520
they have already been done.

52:08.520 --> 52:10.920
So I would claim to have a good prediction track record there

52:10.920 --> 52:11.880
although it's a little bit iffy

52:11.880 --> 52:14.120
because of course I can't quite be proven wrong

52:15.440 --> 52:17.120
without exhibiting a superintelligence

52:17.120 --> 52:18.520
that fails to solve a problem.

52:20.640 --> 52:21.480
Okay.

52:23.800 --> 52:28.800
Proteins, why is your hand not as strong as steel?

52:29.200 --> 52:33.200
We know that steel is a kind of substance that can exist.

52:33.200 --> 52:36.480
We know that molecules can be held together as strongly

52:36.480 --> 52:38.440
that atoms can be bound together as strongly

52:38.440 --> 52:40.440
as the atoms in steel.

52:40.440 --> 52:43.640
It seems like it would be an evolutionary advantage

52:43.640 --> 52:46.240
if your flesh wears hard as steel.

52:46.240 --> 52:50.320
You could like laugh at tigers at that rate, right?

52:50.320 --> 52:52.600
Their claws are just gonna like scrape right off ya.

52:52.600 --> 52:56.280
Assuming the tigers didn't have that technology themselves.

52:56.280 --> 52:58.680
Why is your hand not as strong as steel?

52:58.680 --> 53:00.520
Why has biology not bound together

53:00.520 --> 53:02.600
the atoms in your hand more strongly?

53:05.720 --> 53:08.920
Colin, what is your answer?

53:11.440 --> 53:13.920
Well, it can't get to every,

53:14.920 --> 53:16.920
it's their local maximums.

53:16.920 --> 53:20.600
The national selection looks for things that work,

53:20.600 --> 53:21.440
not for the best.

53:21.440 --> 53:24.240
It does not, it doesn't make sense to look for the best.

53:24.240 --> 53:25.960
You could disappear on that search.

53:25.960 --> 53:27.000
That would be my crude answer.

53:27.000 --> 53:28.240
How am I doing, doc?

53:29.600 --> 53:31.800
Yeah, not terribly.

53:31.800 --> 53:36.760
The answer I would give is that biology has to be evolvable.

53:36.760 --> 53:40.280
Everything it's built out of has to get there as a mistake

53:40.280 --> 53:42.440
from some other conformation,

53:42.440 --> 53:46.400
which means that if it went down narrow potential,

53:46.400 --> 53:50.480
pardon me, went down a steep potential energy gradients

53:50.480 --> 53:53.100
to end up bound together very tightly,

53:54.000 --> 53:58.560
designs like that are less likely to have neighbors

53:58.560 --> 54:00.820
that are other useful designs.

54:02.040 --> 54:05.280
And so your hands are made out of proteins

54:05.280 --> 54:08.400
that fold up basically held together

54:08.400 --> 54:10.640
by the equivalent of static cling,

54:10.640 --> 54:14.880
van der Waals forces, rather than covalent bonds.

54:14.880 --> 54:17.800
The backbone of protein chains,

54:17.800 --> 54:21.260
the backbone of the amino acid change is a covalent bond,

54:21.260 --> 54:23.280
but then it folds up and is held together

54:23.280 --> 54:25.960
by static cling, static electricity.

54:25.960 --> 54:27.440
And so it is soft.

54:28.760 --> 54:30.120
Somewhere in the back of your mind,

54:30.120 --> 54:32.000
you probably have a sense that,

54:32.960 --> 54:37.420
that flesh is soft and animated by Alain Vital,

54:37.420 --> 54:40.140
and it's like soft and it's not as strong as steel,

54:40.140 --> 54:44.060
but it can heal itself and it can replicate itself.

54:44.060 --> 54:48.140
And this is like the trade-off of our laws of magic,

54:48.140 --> 54:50.340
that if you wanna heal yourself and replicate yourself,

54:50.340 --> 54:52.860
you can't be as strong as steel.

54:52.860 --> 54:56.380
This is not actually built into nature on a deep level.

54:56.380 --> 54:59.660
It's just that the flesh evolved

54:59.660 --> 55:02.540
and therefore had to go down shallow potential energy

55:02.540 --> 55:04.260
gradients in order to be evolvable

55:04.260 --> 55:06.580
and is held together by van der Waals forces

55:06.580 --> 55:08.340
instead of covalent bonds.

55:10.140 --> 55:12.940
I'm now going to hold up another book now,

55:12.940 --> 55:13.740
book called

55:17.660 --> 55:18.700
Nano Medicine

55:19.740 --> 55:24.060
by Robert Freitas, instead of Nano Systems by Eric Drexler.

55:25.700 --> 55:30.700
And people have done advanced analysis

55:31.980 --> 55:35.300
of what would happen if you hadn't,

55:35.300 --> 55:38.580
what would happen if you had an equivalent of biology

55:38.580 --> 55:43.580
that met off covalent bonds instead of van der Waals forces?

55:44.300 --> 55:47.260
And the answer we can like analyze on some detail

55:47.260 --> 55:51.100
in our understanding of physics is, for example,

55:51.100 --> 55:55.620
you could instead of carrying instead of red blood cells

55:55.620 --> 55:58.900
that carry oxygen using weak chemical bonds,

55:58.900 --> 56:02.420
you could have a pressurized vessel of corundum

56:03.420 --> 56:08.300
that would hold 100 times as much oxygen per unit volume

56:08.300 --> 56:10.340
of artificial red blood cells

56:10.340 --> 56:12.940
with a 1000 fold safety margin

56:12.940 --> 56:15.540
on the strength of the pressurized container.

56:15.540 --> 56:18.740
There's vastly more room above biology.

56:20.460 --> 56:25.020
So this is, and this is actually not even exploiting

56:25.020 --> 56:27.780
laws of nature that I don't know.

56:27.780 --> 56:31.180
It's the equivalent of playing a better chess

56:31.180 --> 56:34.060
wherein you understand how proteins fold

56:34.060 --> 56:37.020
and you design a tiny molecular lab

56:37.020 --> 56:39.220
to be made out of proteins

56:39.220 --> 56:41.420
and you get some human patsy

56:41.420 --> 56:43.420
who probably doesn't even know you're an AI

56:43.420 --> 56:45.380
because AIs are now smart enough.

56:45.380 --> 56:47.100
This was, this has already been shown.

56:47.100 --> 56:49.860
AIs now are smart enough that you ask them

56:49.860 --> 56:54.860
to like hire a task rabbit to solve a capture for you.

56:55.380 --> 56:59.260
And the task rabbit asks, are you an AI law?

56:59.260 --> 57:02.140
The AI will think out loud,

57:02.140 --> 57:04.540
like I don't want to know that I'm an AI.

57:04.540 --> 57:06.660
I better tell something else

57:06.660 --> 57:09.820
and then tell the humans that it has like a visual disability.

57:09.820 --> 57:12.220
So it needs to hire somebody else to solve the capture.

57:12.220 --> 57:14.020
This already happened,

57:14.020 --> 57:16.220
including the part where it thought out loud.

57:18.220 --> 57:20.700
Anyways, so you get your,

57:20.700 --> 57:23.340
you order some proteins from an online lab,

57:23.340 --> 57:25.980
you get your human who probably doesn't even know

57:25.980 --> 57:27.900
you're an AI because why take that risk?

57:27.940 --> 57:29.660
Although plenty of humans, it has,

57:29.660 --> 57:31.260
well, we'll serve AIs willingly.

57:31.260 --> 57:34.860
We also now know that AIs are advanced enough to even ask.

57:38.340 --> 57:41.060
The human mixes the proteins in a beaker,

57:41.060 --> 57:44.380
maybe puts in some sugar or a settling for fuel.

57:44.380 --> 57:47.060
It assembles into a tiny little lab

57:47.060 --> 57:49.060
that can accept further instruction,

57:49.060 --> 57:50.780
acoustic instructions from a speaker

57:50.780 --> 57:53.780
and maybe like transmit something back.

57:54.620 --> 57:58.900
Tiny radio, tiny microphone.

57:58.900 --> 58:03.100
I myself am not a superintelligence.

58:03.100 --> 58:05.700
Run experiments in a tiny lab at high speed

58:05.700 --> 58:08.380
because when distances are very small,

58:08.380 --> 58:09.900
events happen very quickly.

58:11.420 --> 58:13.900
Build your second stage nanosystems

58:13.900 --> 58:15.500
inside the tiny little lab.

58:16.380 --> 58:18.100
Build the third stage nanosystems,

58:18.100 --> 58:20.220
build the fourth stage nanosystems,

58:20.220 --> 58:23.620
build the tiny diamondoid bacteria

58:23.620 --> 58:25.820
that replicate out of carbon, hydrogen,

58:25.820 --> 58:28.740
oxygen, nitrogen as can be found in the atmosphere

58:28.740 --> 58:30.660
powered on sunlight,

58:30.660 --> 58:33.220
quietly spread all over the world.

58:33.220 --> 58:36.180
All the humans fall over dead in the same second.

58:36.180 --> 58:38.740
This is not how a superintelligence would defeat you.

58:38.740 --> 58:41.420
This is how Eleazar Yudkowski would defeat you

58:41.420 --> 58:44.820
if I wanted to do that, which to be clear, I don't.

58:44.820 --> 58:47.900
And if I had the postulated ability

58:47.900 --> 58:51.300
to better explore the logical structure

58:51.300 --> 58:53.900
of the known consequences of chemistry.

58:58.860 --> 59:00.260
Interesting, okay.

59:05.020 --> 59:09.420
So let's talk about, that sounds sarcastic.

59:09.420 --> 59:10.380
I didn't mean it's sarcastic,

59:10.380 --> 59:11.980
but I think it's really interesting.

59:14.500 --> 59:16.940
That interesting, man, I'm not capable.

59:18.540 --> 59:21.060
My intelligence level is not high enough

59:21.060 --> 59:23.260
to assess the quality of that argument.

59:24.860 --> 59:28.260
What's fascinating, of course, is that

59:32.300 --> 59:33.660
we could have imagined,

59:34.780 --> 59:37.500
Eric Hall mentioned nuclear proliferation.

59:37.500 --> 59:40.140
It's dangerous, nuclear proliferation,

59:40.140 --> 59:42.820
up to a point in some sense it's somewhat healthy

59:42.820 --> 59:46.980
in that it can be a deterrent under certain settings,

59:47.980 --> 59:51.940
but the world could not restrain nuclear proliferation.

59:51.940 --> 59:55.300
And right now it's trying to some extent

59:55.300 --> 59:59.020
has had some success in keeping the nuclear club

59:59.020 --> 01:00:02.060
with its current number of members for a while,

01:00:02.060 --> 01:00:04.140
but it remains the case that nuclear weapons

01:00:04.140 --> 01:00:06.300
are a threat to the future of humanity.

01:00:08.660 --> 01:00:11.100
Do you think there's any way we can restrain

01:00:11.100 --> 01:00:15.820
this AI phenomenon that's meaningful?

01:00:15.820 --> 01:00:20.820
So you issued a clarion call, you sounded an alarm,

01:00:23.660 --> 01:00:27.980
and mostly I think people shrugged it off, you know?

01:00:27.980 --> 01:00:29.540
A bunch of people signed a letter,

01:00:29.540 --> 01:00:33.540
26,000 people I think so far, signed the letter saying,

01:00:33.540 --> 01:00:35.500
you know, we don't know what we're doing here,

01:00:35.500 --> 01:00:38.460
this is uncharted territory, let's take six months off.

01:00:38.460 --> 01:00:40.980
You were in peace and says, six months, are you crazy?

01:00:40.980 --> 01:00:44.420
We need to stop this until we have an understanding

01:00:44.420 --> 01:00:45.660
of how to constrain it.

01:00:46.500 --> 01:00:49.100
Now that's a very reasonable thought to me,

01:00:49.100 --> 01:00:51.060
but the next question would be,

01:00:52.220 --> 01:00:54.060
how would you possibly do that?

01:00:54.060 --> 01:00:56.620
In other words, I could imagine a world where,

01:00:57.540 --> 01:01:00.620
if there were, let's say, four people who were capable

01:01:00.620 --> 01:01:02.700
of creating this technology,

01:01:02.700 --> 01:01:04.140
that the four people would say, you know,

01:01:04.140 --> 01:01:06.540
we're playing with fire here, we need to stop,

01:01:06.540 --> 01:01:09.180
let's make a mutual agreement, they might not keep it,

01:01:09.180 --> 01:01:10.540
four people still a pretty big number,

01:01:10.540 --> 01:01:12.540
but we're not a four people,

01:01:12.660 --> 01:01:14.420
there are many, many people working on this,

01:01:14.420 --> 01:01:16.580
there are many countries working on it.

01:01:18.260 --> 01:01:21.180
Your peace did not, I don't think,

01:01:21.180 --> 01:01:23.940
start an international movement of people going

01:01:23.940 --> 01:01:27.220
to the barricades to demand that this technology

01:01:27.220 --> 01:01:28.620
be put on hold.

01:01:29.500 --> 01:01:32.020
How do we possibly, how do you sleep at night?

01:01:32.020 --> 01:01:34.660
I mean, like, what should we be doing if you're right?

01:01:37.140 --> 01:01:38.860
Or am I wrong, do people read this and go,

01:01:38.860 --> 01:01:41.220
well, Elias Rieckowski thinks it's dangerous,

01:01:41.220 --> 01:01:43.420
maybe we ought to be slowing down.

01:01:43.420 --> 01:01:46.260
I mean, Sam, what's happened in the middle of the night

01:01:46.260 --> 01:01:50.660
saying, thanks, Elias, I'm gonna put things on hold.

01:01:50.660 --> 01:01:52.020
I don't think that happened.

01:01:53.740 --> 01:01:56.500
I think you are somewhat underestimating the impact

01:01:56.500 --> 01:01:57.900
and it is still playing out.

01:02:00.140 --> 01:02:03.820
Okay, so like, mostly, it seems to me that if we wanted

01:02:03.820 --> 01:02:07.580
to win this, we needed to start a whole lot earlier,

01:02:07.580 --> 01:02:09.340
possibly in the 1930s.

01:02:11.260 --> 01:02:14.820
But in terms of, like, my looking back

01:02:14.820 --> 01:02:17.220
and like asking how far back you'd have to unwind history

01:02:17.220 --> 01:02:21.100
to get us into a situation where this was survivable.

01:02:24.140 --> 01:02:27.380
But leaving that aside.

01:02:27.380 --> 01:02:28.460
I think that's a move.

01:02:28.460 --> 01:02:32.500
Yeah, so in fact, it seems to me that the game board

01:02:32.500 --> 01:02:35.460
has been played into a position where it is very likely

01:02:35.460 --> 01:02:36.660
that everyone just dies.

01:02:38.500 --> 01:02:40.700
If the human species woke up one day

01:02:40.700 --> 01:02:43.020
and decided it would rather live,

01:02:43.020 --> 01:02:46.340
it would not be easy at this point

01:02:46.340 --> 01:02:51.340
to bring the GPU clusters and the GPU manufacturing

01:02:51.580 --> 01:02:53.860
processes under sufficient control

01:02:53.860 --> 01:02:57.460
that nobody built things that were too much smarter

01:02:57.460 --> 01:03:02.460
than GPT-4 or GPT-5 or whatever the level just barely short

01:03:02.620 --> 01:03:04.580
of lethal is, which we should not,

01:03:04.580 --> 01:03:06.380
which we would not if we were taking this seriously,

01:03:06.380 --> 01:03:07.940
get as close to as we possibly could,

01:03:07.940 --> 01:03:10.860
because we don't actually know exactly where the level is.

01:03:10.860 --> 01:03:12.540
But we would have to do more or less,

01:03:12.540 --> 01:03:17.540
is have international agreements that were being enforced

01:03:17.980 --> 01:03:19.900
even against parties not part,

01:03:19.900 --> 01:03:22.860
even against countries not party to that national agreement,

01:03:22.860 --> 01:03:25.620
international agreement, if it became necessary,

01:03:25.620 --> 01:03:29.180
you would be wanting to track all the GPUs.

01:03:29.180 --> 01:03:32.360
You might be demanding that all the GPUs call home

01:03:32.360 --> 01:03:34.860
on a regular basis or stop working.

01:03:34.860 --> 01:03:36.500
You'd want to tamper proof them.

01:03:38.220 --> 01:03:42.700
If intelligence said that a rogue nation was,

01:03:42.700 --> 01:03:46.420
had bought, somehow managed to buy a bunch of GPUs

01:03:46.420 --> 01:03:49.260
despite arms controls and defeat the tamper proofing

01:03:49.260 --> 01:03:51.380
on those GPUs, you would have to do it was necessary

01:03:51.380 --> 01:03:52.580
to shut down the data center,

01:03:52.580 --> 01:03:54.620
even if that led to a shooting war between nations,

01:03:54.620 --> 01:03:56.860
even if that country was a nuclear country

01:03:56.860 --> 01:03:59.260
and had threatened nuclear retaliation.

01:03:59.260 --> 01:04:01.460
The human species could survive this if it wanted to,

01:04:01.460 --> 01:04:03.460
but it would not be business as usual.

01:04:04.560 --> 01:04:07.180
It is not something you could do trivially.

01:04:07.180 --> 01:04:10.020
So when you say I may have underestimated it,

01:04:10.020 --> 01:04:12.220
did you get people writing and saying,

01:04:12.220 --> 01:04:15.380
you know, I wasn't, and I don't mean people like me.

01:04:15.380 --> 01:04:16.860
I mean, people, players.

01:04:16.860 --> 01:04:18.980
Did you get people who are playing in this sandbox

01:04:18.980 --> 01:04:21.660
to write you and say, you've scared me.

01:04:21.660 --> 01:04:23.660
I think we need to take this seriously?

01:04:26.620 --> 01:04:29.940
Without naming names, I'm not asking for that.

01:04:29.940 --> 01:04:31.860
At least one US Congressman.

01:04:31.860 --> 01:04:32.700
Okay.

01:04:34.460 --> 01:04:36.300
It's a start, maybe.

01:04:36.300 --> 01:04:38.660
You know, one of the things that a common response

01:04:38.660 --> 01:04:41.100
that people get when you talk about this is that,

01:04:41.100 --> 01:04:42.180
well, the last thing I do is,

01:04:42.180 --> 01:04:43.740
last thing I want is the government controlling

01:04:43.740 --> 01:04:45.060
whether this thing goes forward or not,

01:04:45.060 --> 01:04:48.460
but it'd be hard to do without some form of lethal force

01:04:48.460 --> 01:04:49.300
as you comply.

01:04:49.300 --> 01:04:54.300
I spent 20 years trying desperately to have there be

01:04:54.740 --> 01:04:59.660
any other solution to have these things be alignable,

01:04:59.660 --> 01:05:01.100
but it is very hard to do that

01:05:01.100 --> 01:05:05.540
when you are nearly alone and under resourced

01:05:05.540 --> 01:05:08.420
and the world has not made this a priority.

01:05:08.420 --> 01:05:11.180
And future progress is very hard to predict.

01:05:12.820 --> 01:05:15.660
I don't think people actually understood the research program

01:05:15.660 --> 01:05:18.980
that we were trying to carry out, but yeah.

01:05:18.980 --> 01:05:22.780
So I sure wanted there to be any other plan than this

01:05:22.780 --> 01:05:24.600
because now that we've come to this last resort,

01:05:24.600 --> 01:05:27.020
I don't think we actually have that last resort.

01:05:27.020 --> 01:05:28.700
I don't think we have been reduced

01:05:28.700 --> 01:05:30.820
to a last-ditch backup plan that actually works.

01:05:30.820 --> 01:05:32.960
I think we all just die.

01:05:32.960 --> 01:05:36.960
And yet nonetheless, here I am,

01:05:36.960 --> 01:05:40.320
like putting aside doing that thing

01:05:40.320 --> 01:05:43.680
that I wouldn't do for almost any other technology,

01:05:43.680 --> 01:05:45.800
except for maybe gain of function research

01:05:45.800 --> 01:05:49.800
on biological pathogens

01:05:51.480 --> 01:05:54.160
and advocating for government interference.

01:05:54.160 --> 01:05:57.280
Because in fact, like if the government comes in

01:05:57.280 --> 01:05:58.760
and wrecks the whole thing,

01:05:58.760 --> 01:06:01.000
that's better than the thing

01:06:01.000 --> 01:06:02.680
that was otherwise going to happen.

01:06:02.720 --> 01:06:04.400
Because it's not based on the government coming in

01:06:04.400 --> 01:06:06.400
and being like super competent

01:06:06.400 --> 01:06:08.480
and directing the technology exactly directly.

01:06:08.480 --> 01:06:11.120
It's like, okay, this is going to kill literally everyone

01:06:11.120 --> 01:06:12.920
if the government stomps around

01:06:12.920 --> 01:06:17.240
and like the dangerous of the government.

01:06:17.240 --> 01:06:18.600
It's one of those very rare cases

01:06:18.600 --> 01:06:19.920
where the dangerous that the government

01:06:19.920 --> 01:06:22.640
will interfere too little rather than too much.

01:06:22.640 --> 01:06:23.480
Possibly.

01:06:24.600 --> 01:06:27.680
Let's close with a quote from Scott Ericsson,

01:06:28.560 --> 01:06:29.720
which found on his blog.

01:06:29.720 --> 01:06:31.360
We'll put a link up to the post.

01:06:31.360 --> 01:06:36.280
Very interesting defensive of AI.

01:06:36.280 --> 01:06:39.320
Scott's a University of Texas computer scientist.

01:06:39.320 --> 01:06:41.440
He's working at OpenAI.

01:06:41.440 --> 01:06:43.560
He's on leave, I don't, I think for a year, maybe longer.

01:06:43.560 --> 01:06:45.480
I don't know, it doesn't matter.

01:06:45.480 --> 01:06:47.160
He wrote the following.

01:06:47.160 --> 01:06:49.560
So if we ask the directly relevant question,

01:06:49.560 --> 01:06:51.720
do I expect the generative AI race,

01:06:51.720 --> 01:06:55.320
which started in earnest around 2016 or 2017,

01:06:55.320 --> 01:06:57.000
with the founding of OpenAI

01:06:57.000 --> 01:07:00.840
to play a central causal role in the extinction of humanity?

01:07:00.840 --> 01:07:04.520
I'll give a probability of around 2% for that.

01:07:04.520 --> 01:07:06.320
And I'll give a similar probability,

01:07:06.320 --> 01:07:08.520
maybe even a higher one for the generative AI race

01:07:08.520 --> 01:07:12.560
to play a central causal role in the saving of humanity.

01:07:12.560 --> 01:07:14.000
All considered then,

01:07:14.000 --> 01:07:15.400
I come down in favor right now,

01:07:15.400 --> 01:07:18.400
proceeding with AI research with extreme caution,

01:07:18.400 --> 01:07:19.360
but proceeding.

01:07:21.320 --> 01:07:24.600
My personal reaction is that is insane.

01:07:24.600 --> 01:07:26.380
I have very little, I'm serious.

01:07:26.380 --> 01:07:28.920
I find that deeply disturbing

01:07:28.960 --> 01:07:31.480
and I'd love to have him on the program to defend it.

01:07:31.480 --> 01:07:34.160
I don't think there's much of a chance

01:07:34.160 --> 01:07:36.280
that generative AI would save humanity.

01:07:36.280 --> 01:07:37.840
I'm not quite sure for what it's,

01:07:40.120 --> 01:07:40.960
he's worried about,

01:07:40.960 --> 01:07:44.960
but if you're telling me there's a 2%, 2% chance

01:07:44.960 --> 01:07:46.200
that it's gonna destroy all humans

01:07:46.200 --> 01:07:47.960
and you obviously think it's higher,

01:07:47.960 --> 01:07:50.240
but 2% is really high to me

01:07:50.240 --> 01:07:52.120
for an outcome that's rather devastating.

01:07:52.120 --> 01:07:56.000
It's one of the deepest things I've learned from Nassim Tala.

01:07:56.000 --> 01:07:58.320
It's not just the probability,

01:07:58.320 --> 01:08:00.520
it's the outcome that counts too.

01:08:00.520 --> 01:08:04.760
So this is ruined on a colossal scale

01:08:04.760 --> 01:08:07.800
and the one thing you wanna do is avoid ruin

01:08:07.800 --> 01:08:11.600
so you can take advantage of more draws from the earn.

01:08:11.600 --> 01:08:14.800
The average return from the earn is irrelevant

01:08:14.800 --> 01:08:16.800
if you are not allowed to play anymore.

01:08:16.800 --> 01:08:18.800
You're out, you're dead, you're gone.

01:08:18.800 --> 01:08:21.080
So you're suggesting we're gonna be out and dead and gone,

01:08:21.080 --> 01:08:23.120
but I want you to react to Scott's quote.

01:08:24.120 --> 01:08:27.880
Um, 2% sounds great.

01:08:27.880 --> 01:08:31.200
Like 2% is plausibly within the range of like

01:08:31.200 --> 01:08:34.800
the human species destroying it itself by other means.

01:08:34.800 --> 01:08:37.520
I think that the disagreement I have with Scott Aronson

01:08:37.520 --> 01:08:41.960
is simply about the probability that AI is alignable

01:08:41.960 --> 01:08:45.720
with the frankly half-fazard level

01:08:45.720 --> 01:08:48.080
that we have put into it and the half-fazard level

01:08:48.080 --> 01:08:50.800
that is all humanity is capable of

01:08:51.680 --> 01:08:53.200
as far as I can tell,

01:08:53.200 --> 01:08:56.360
because the core lethality here

01:08:56.360 --> 01:08:57.560
is that you have to get something right

01:08:57.560 --> 01:08:59.560
on the first try or it kills you

01:08:59.560 --> 01:09:01.400
and getting something right on the first try

01:09:01.400 --> 01:09:03.560
when you do not get like infinite free retries

01:09:03.560 --> 01:09:05.800
as you usually do in science and engineering

01:09:05.800 --> 01:09:09.640
is an insane ask, insanely lethal ask.

01:09:09.640 --> 01:09:13.000
My reaction is fundamentally that 2% is too low.

01:09:13.000 --> 01:09:14.680
If I take it at face value,

01:09:14.680 --> 01:09:16.920
then 2% is within range of the probability

01:09:16.920 --> 01:09:19.120
of humanity wiping itself out by something else

01:09:19.120 --> 01:09:21.640
where if you assume that AI alignment is free,

01:09:21.640 --> 01:09:23.760
that AI alignment is easy,

01:09:23.760 --> 01:09:26.400
that you can get something that is smarter than you,

01:09:26.400 --> 01:09:28.280
but on your side and helping,

01:09:29.880 --> 01:09:32.880
2% chance of risking everything does appear to me

01:09:32.880 --> 01:09:37.040
to be commensurate with the risks from other sources

01:09:37.040 --> 01:09:40.120
that you could shut down using the superintelligence.

01:09:40.120 --> 01:09:41.120
It's not 2%.

01:09:42.240 --> 01:09:47.600
So, the question then is,

01:09:49.600 --> 01:09:52.600
what would Scott Aaronson say if he heard your,

01:09:52.600 --> 01:09:54.480
I mean, he's heard, he's read your piece presumably,

01:09:54.480 --> 01:09:57.960
he understands your argument about wealthiness.

01:09:57.960 --> 01:10:00.000
I should just clarify for listeners,

01:10:00.000 --> 01:10:04.120
alignment is the idea that AI could be constrained

01:10:04.120 --> 01:10:06.920
to serve our goals rather than its goals.

01:10:06.920 --> 01:10:08.360
Is that a good summary?

01:10:10.120 --> 01:10:11.600
I wouldn't say constrained.

01:10:11.600 --> 01:10:14.760
I would say built from scratch to want those things

01:10:14.760 --> 01:10:16.560
and not want otherwise.

01:10:16.560 --> 01:10:17.760
So that's really hard

01:10:17.800 --> 01:10:19.160
because we don't understand how it works.

01:10:19.160 --> 01:10:21.000
That would be, I think, your point.

01:10:21.000 --> 01:10:22.320
And tell me then what Scott's-

01:10:22.320 --> 01:10:23.640
On the first try.

01:10:23.640 --> 01:10:25.080
Yeah, on the first try.

01:10:25.080 --> 01:10:28.200
So what would Scott say when you tell him,

01:10:28.200 --> 01:10:31.320
but it's gonna develop all these side desires

01:10:31.320 --> 01:10:33.080
that we can't control.

01:10:33.080 --> 01:10:33.960
What's he gonna say?

01:10:35.480 --> 01:10:36.320
Why is he not worried?

01:10:36.320 --> 01:10:38.240
Why is he still, why is he quit his job?

01:10:39.480 --> 01:10:40.760
And not Scott.

01:10:40.760 --> 01:10:42.880
People in the, let's get away from him personally,

01:10:42.880 --> 01:10:46.800
but people in general, there's dozens and maybe hundreds,

01:10:46.800 --> 01:10:48.400
maybe a thousand, I don't know,

01:10:48.400 --> 01:10:51.040
extraordinarily intelligent people

01:10:51.040 --> 01:10:53.600
who are trying to build something even more intelligent

01:10:53.600 --> 01:10:54.920
than they are.

01:10:54.920 --> 01:10:57.600
Why are they not worried about what you're saying?

01:10:57.600 --> 01:10:59.720
They've all got different reasons.

01:10:59.720 --> 01:11:02.320
Scott's is that he thinks that intelligence,

01:11:02.320 --> 01:11:05.440
that he observes intelligence makes humans nicer.

01:11:05.440 --> 01:11:08.560
And though he wouldn't phrase it exactly this way,

01:11:08.560 --> 01:11:11.760
this is basically what Scott said on his blog.

01:11:11.760 --> 01:11:13.960
To which my response is intelligence

01:11:13.960 --> 01:11:15.400
does have effects on humans,

01:11:15.400 --> 01:11:18.560
especially humans who start out relatively nice.

01:11:18.560 --> 01:11:21.000
And when you're building AI's from scratch,

01:11:21.000 --> 01:11:23.240
you're just like in a different domain with different rules.

01:11:23.240 --> 01:11:26.000
And you're allowed to say that it's hard to build AI's

01:11:26.000 --> 01:11:29.640
that are nice without implying that making humans smarter,

01:11:29.640 --> 01:11:32.640
like humans start out in a certain frame of reference.

01:11:32.640 --> 01:11:34.600
And when you apply more intelligence to them,

01:11:34.600 --> 01:11:37.160
they move within that frame of reference.

01:11:37.160 --> 01:11:39.480
And if they start out with a small amount of nicest,

01:11:39.480 --> 01:11:41.800
the intelligence can make them nicer.

01:11:41.800 --> 01:11:44.080
They can become more empathetic.

01:11:44.080 --> 01:11:47.280
If they start out with some empathy,

01:11:47.280 --> 01:11:48.920
they can develop more empathy

01:11:48.920 --> 01:11:50.840
as they understand other people better,

01:11:50.840 --> 01:11:54.920
which is intelligence to correctly model other people.

01:11:54.920 --> 01:11:57.240
That is even more insane.

01:11:57.240 --> 01:12:01.800
I'm not going to, I haven't read that blog post

01:12:01.800 --> 01:12:03.080
and we'll put a link up to it.

01:12:03.080 --> 01:12:04.680
I hope you'll share it with me.

01:12:04.680 --> 01:12:07.520
But again, not attributing it to Scott

01:12:07.520 --> 01:12:09.040
since I haven't seen it.

01:12:09.040 --> 01:12:13.120
And assuming that you've said this fairly and correctly,

01:12:13.120 --> 01:12:15.560
the idea that more intelligent people are nicer

01:12:15.560 --> 01:12:17.880
is one of the most,

01:12:17.880 --> 01:12:20.480
that'd be very hard to show with the evidence for that.

01:12:20.480 --> 01:12:21.320
That is an appalling.

01:12:21.320 --> 01:12:22.640
I don't think it's like,

01:12:22.640 --> 01:12:24.560
it is not a universal law on humans.

01:12:24.560 --> 01:12:25.400
No.

01:12:25.400 --> 01:12:27.680
It is a thing that I think is true of Scott.

01:12:27.680 --> 01:12:29.760
I think if you made Scott Aaron Sinclair smarter,

01:12:29.760 --> 01:12:30.680
he'd get nicer.

01:12:32.320 --> 01:12:35.040
And I think he's inappropriately generalizing from that.

01:12:35.040 --> 01:12:39.840
There is a scene in Schindler's List.

01:12:39.840 --> 01:12:42.320
The Nazis, I think they're in the Warsaw ghetto

01:12:42.320 --> 01:12:45.280
and they're race, a group of Nazis are racing,

01:12:45.280 --> 01:12:46.320
I think they're in the SS,

01:12:46.320 --> 01:12:49.400
they're racing through a tenement.

01:12:49.400 --> 01:12:51.240
And it's falling apart

01:12:51.240 --> 01:12:53.640
because the ghetto is falling apart.

01:12:53.640 --> 01:12:57.600
But one of the SS agents sees a piano

01:12:59.040 --> 01:13:00.800
and he can't help himself.

01:13:00.800 --> 01:13:03.200
He sits down and he plays Bach or something.

01:13:03.200 --> 01:13:05.080
I think it was Bach.

01:13:05.080 --> 01:13:06.280
And I always found it interesting

01:13:06.280 --> 01:13:09.560
that Spielberg put that in or whoever wrote the script.

01:13:09.560 --> 01:13:11.160
And I think it was pretty clear why they put it in.

01:13:11.160 --> 01:13:12.680
They wanted to show you that

01:13:12.680 --> 01:13:15.440
having a very high advanced level of civilization

01:13:15.440 --> 01:13:18.720
does not stop people from treating other people,

01:13:18.720 --> 01:13:20.600
other human beings like animals,

01:13:20.600 --> 01:13:22.580
or worse than animals in many cases,

01:13:24.000 --> 01:13:28.720
and exterminating them without conscience.

01:13:28.720 --> 01:13:31.640
So I don't share that view of anyone's

01:13:31.640 --> 01:13:34.640
that intelligence makes you a nicer person.

01:13:34.640 --> 01:13:36.680
I think that's not the case,

01:13:36.680 --> 01:13:38.960
but perhaps Scott will return to this,

01:13:38.960 --> 01:13:41.360
will come to this program and defend that view

01:13:41.360 --> 01:13:42.880
if he did holes it.

01:13:42.880 --> 01:13:45.880
I think you are underweighting the evidence

01:13:45.880 --> 01:13:48.880
that has convinced Scott of the thing that I think is wrong.

01:13:48.880 --> 01:13:52.400
I think if you suddenly started augmenting the intelligence

01:13:52.400 --> 01:13:55.000
of the SS agents from Nazi Germany,

01:13:55.000 --> 01:13:59.640
then somewhere between 10% and 90% of them

01:13:59.640 --> 01:14:02.200
would go over to the cause of good.

01:14:02.200 --> 01:14:05.280
Because there were factual falsehoods

01:14:05.280 --> 01:14:10.240
that were pillars of the Nazi philosophy,

01:14:10.240 --> 01:14:13.240
and that people would reliably stop believing

01:14:13.240 --> 01:14:14.480
as they got smarter.

01:14:14.480 --> 01:14:16.040
That doesn't mean that they would turn good,

01:14:16.040 --> 01:14:17.280
but some of them would have.

01:14:17.280 --> 01:14:18.200
Is it 10%?

01:14:18.200 --> 01:14:19.040
Is it 90%?

01:14:19.040 --> 01:14:19.880
I don't know.

01:14:19.880 --> 01:14:22.960
It's not my experience with the human creature.

01:14:24.000 --> 01:14:25.240
You've written some very interesting things

01:14:25.240 --> 01:14:26.640
on rationality of a beautiful essay,

01:14:26.640 --> 01:14:31.480
we'll link to on 12 rules for rationality.

01:14:31.480 --> 01:14:33.400
In my experience, it's a very small portion

01:14:33.400 --> 01:14:35.760
of the population that behaves that way.

01:14:38.600 --> 01:14:40.520
There's a quote from Nassim Taleb,

01:14:40.520 --> 01:14:43.040
we haven't gotten to yet in this conversation,

01:14:43.040 --> 01:14:46.200
which is bigger data, bigger mistakes.

01:14:46.200 --> 01:14:47.560
I think there's a belief generally

01:14:47.560 --> 01:14:49.600
that bigger data fewer mistakes,

01:14:49.600 --> 01:14:50.840
but Taleb might be right,

01:14:50.840 --> 01:14:52.680
and it's certainly not the case in my experience

01:14:52.680 --> 01:14:56.520
that bigger brains, higher IQ means better decisions.

01:14:56.520 --> 01:14:58.600
This is not my experience.

01:14:58.600 --> 01:15:03.120
Then you're not throwing enough intelligence

01:15:03.120 --> 01:15:04.080
at the problem.

01:15:05.920 --> 01:15:07.960
Literally not just decisions

01:15:07.960 --> 01:15:10.120
that you disagree with the goals,

01:15:10.120 --> 01:15:12.680
but false models of reality,

01:15:14.040 --> 01:15:16.480
models of realities so blatantly mistaken

01:15:16.480 --> 01:15:18.000
that even you, a human,

01:15:18.000 --> 01:15:20.480
can tell that they're wrong and in which direction.

01:15:20.480 --> 01:15:21.920
These people are not smart the way

01:15:21.920 --> 01:15:26.920
that a hypothetical weak efficient market is smart.

01:15:27.280 --> 01:15:29.320
You can tell they're making mistakes

01:15:29.320 --> 01:15:30.760
and you know in which direction.

01:15:30.800 --> 01:15:32.800
They're not smart the way that Stockfish 15

01:15:32.800 --> 01:15:34.080
is smart in chess.

01:15:34.080 --> 01:15:36.320
You can play against them and win.

01:15:36.320 --> 01:15:41.320
These are, the range of human intelligence

01:15:41.320 --> 01:15:42.440
is not that wide.

01:15:42.440 --> 01:15:45.560
It caps out at like John von Neumann or whatever.

01:15:45.560 --> 01:15:49.280
And that is not wide enough to open up

01:15:49.280 --> 01:15:52.480
that humans would be epistemic,

01:15:52.480 --> 01:15:55.240
that these beings would be epistemically

01:15:55.240 --> 01:15:58.120
or instrumentally efficient relative to you.

01:15:58.120 --> 01:15:59.560
It is possible for you to know

01:15:59.560 --> 01:16:01.840
that one of their estimates is directionally mistaken

01:16:01.840 --> 01:16:03.360
and to know the direction.

01:16:03.360 --> 01:16:05.520
It is possible for you to know an action

01:16:05.520 --> 01:16:07.360
that serves their goals better

01:16:07.360 --> 01:16:09.080
than the action that they generated.

01:16:09.080 --> 01:16:10.680
And is it striking how hard it is

01:16:10.680 --> 01:16:13.680
to convince some of that even though they're thinking people?

01:16:15.040 --> 01:16:19.080
History is, I just have a different perception maybe.

01:16:19.080 --> 01:16:20.880
To be continued, Eliezer.

01:16:22.200 --> 01:16:24.760
My guest today has been Eliezer Yudkowski.

01:16:24.760 --> 01:16:26.960
Eliezer, thanks for being part of Econ Talk.

01:16:27.920 --> 01:16:29.120
Thanks for having me.

01:16:29.200 --> 01:16:34.400
This is Econ Talk, part of the Library of Economics and Liberty.

01:16:34.400 --> 01:16:36.920
For more Econ Talk, go to econtalk.org

01:16:36.920 --> 01:16:39.280
where you can also comment on today's podcast

01:16:39.280 --> 01:16:42.840
and find links and readings related to today's conversation.

01:16:42.840 --> 01:16:45.840
The sound engineer for Econ Talk is Rich Goyette.

01:16:45.840 --> 01:16:47.640
I'm your host Russ Roberts.

01:16:47.640 --> 01:16:48.840
Thanks for listening.

01:16:48.840 --> 01:16:50.360
Talk to you on Monday.

