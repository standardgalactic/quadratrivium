1
00:00:00,000 --> 00:00:07,400
Today is April 16th, 2023, and my guest is Eliezer Yudkowski.

2
00:00:07,400 --> 00:00:11,280
He is the founder of the Machine Intelligence Research Institute,

3
00:00:11,280 --> 00:00:14,200
the founder of the less wrong blogging community,

4
00:00:14,200 --> 00:00:16,800
and is an outspoken voice on the dangers of

5
00:00:16,800 --> 00:00:18,640
artificial general intelligence,

6
00:00:18,640 --> 00:00:19,980
which is our topic for today.

7
00:00:19,980 --> 00:00:22,000
Eliezer, welcome to Econ Talk.

8
00:00:22,000 --> 00:00:23,840
Thanks for having me.

9
00:00:23,840 --> 00:00:28,360
You recently wrote an article at time.com on the dangers of AI.

10
00:00:28,360 --> 00:00:33,160
I'm going to quote central paragraph, quote,

11
00:00:33,160 --> 00:00:36,100
many researchers steeped in these issues,

12
00:00:36,100 --> 00:00:40,280
including myself, expect that the most likely result on

13
00:00:40,280 --> 00:00:43,520
building a superhumanly smart AI under

14
00:00:43,520 --> 00:00:46,440
anything remotely like the current circumstances,

15
00:00:46,440 --> 00:00:50,040
is that literally everyone on earth will die.

16
00:00:50,040 --> 00:00:53,640
Not as in maybe possibly some remote chance,

17
00:00:53,640 --> 00:00:57,600
but as in that is the obvious thing that would happen.

18
00:00:57,600 --> 00:00:59,800
It's not that you can't, in principle,

19
00:00:59,800 --> 00:01:02,240
survive creating something much smarter than you.

20
00:01:02,240 --> 00:01:06,440
It's that it would require precision and preparation,

21
00:01:06,440 --> 00:01:08,560
and new scientific insights,

22
00:01:08,560 --> 00:01:11,240
and probably not having AI systems composed of

23
00:01:11,240 --> 00:01:15,320
giant inscrutable arrays of fractional numbers.

24
00:01:15,320 --> 00:01:17,440
Explain.

25
00:01:20,080 --> 00:01:22,840
Well, I mean,

26
00:01:22,840 --> 00:01:26,360
different people come in with different reasons as to why they think

27
00:01:26,360 --> 00:01:27,920
that wouldn't happen,

28
00:01:27,920 --> 00:01:30,920
and if you pick one of them and start explaining those,

29
00:01:30,920 --> 00:01:32,200
everybody else is like,

30
00:01:32,200 --> 00:01:34,840
why are you talking about this irrelevant thing instead of

31
00:01:34,840 --> 00:01:37,400
the thing that I think is the key question?

32
00:01:37,400 --> 00:01:39,720
Whereas if somebody else asked you a question,

33
00:01:39,720 --> 00:01:42,000
even if it's not everyone in the audience's question,

34
00:01:42,000 --> 00:01:43,240
they at least know you're answering

35
00:01:43,240 --> 00:01:44,720
the question that's been asked.

36
00:01:44,720 --> 00:01:49,840
So I could maybe start by saying why I expect

37
00:01:49,840 --> 00:01:54,280
stochastic gradient descent as an optimization process,

38
00:01:54,360 --> 00:01:59,440
even if you try to take something that happens in

39
00:01:59,440 --> 00:02:02,160
the outside world and press the wind,

40
00:02:02,160 --> 00:02:04,720
lose button any time that thing happens in

41
00:02:04,720 --> 00:02:07,960
the outside world doesn't create a mind that in

42
00:02:07,960 --> 00:02:10,440
general wants that thing to happen in the outside world.

43
00:02:10,440 --> 00:02:12,920
But maybe that's not even what you think the core issue is.

44
00:02:12,920 --> 00:02:14,520
What do you think the core issue is?

45
00:02:14,520 --> 00:02:17,080
Why don't you already believe that? Let me say.

46
00:02:17,080 --> 00:02:19,680
So, okay. I'll give you my view,

47
00:02:19,680 --> 00:02:21,840
which is rapidly changing.

48
00:02:21,840 --> 00:02:28,040
I interviewed Nicholas Boster back in 2014.

49
00:02:28,040 --> 00:02:30,040
I read his book, Superintelligence.

50
00:02:30,040 --> 00:02:32,920
I found it uncompelling.

51
00:02:34,080 --> 00:02:36,960
ChatGPT came along.

52
00:02:36,960 --> 00:02:39,840
I tried it. I thought it was pretty cool.

53
00:02:39,840 --> 00:02:42,920
ChatGPT-4 came along.

54
00:02:42,920 --> 00:02:44,280
I haven't tried five yet,

55
00:02:44,280 --> 00:02:46,720
but it's clear that the path of

56
00:02:46,720 --> 00:02:51,560
progress is radically different than it was in 2014.

57
00:02:51,760 --> 00:02:53,960
The trends are very different.

58
00:02:53,960 --> 00:02:58,040
I still remain somewhat agnostic and skeptical,

59
00:02:58,040 --> 00:03:01,280
but I did read Eric Holes essay and

60
00:03:01,280 --> 00:03:02,960
then interviewed him on this program and

61
00:03:02,960 --> 00:03:05,160
a couple of things he wrote after that.

62
00:03:05,160 --> 00:03:09,040
The thing I think I found most alarming was

63
00:03:09,040 --> 00:03:14,680
a metaphor that I found later, Nicholas Boster mused.

64
00:03:14,680 --> 00:03:16,320
Almost the same metaphor,

65
00:03:16,320 --> 00:03:18,380
and yet it didn't scare me at all when I read it,

66
00:03:18,380 --> 00:03:20,360
Nicholas Boster, which is fascinating.

67
00:03:20,360 --> 00:03:21,480
I may have just missed it.

68
00:03:21,480 --> 00:03:23,320
I didn't even remember it was in there.

69
00:03:23,320 --> 00:03:26,240
The metaphor is primitive,

70
00:03:26,960 --> 00:03:31,640
this is anthropous man or some primitive form

71
00:03:31,640 --> 00:03:35,560
of pre-homosapiens sitting around a campfire,

72
00:03:35,560 --> 00:03:37,400
and human being shows up and says,

73
00:03:37,400 --> 00:03:39,600
hey, I got a lot of stuff I can teach you.

74
00:03:39,600 --> 00:03:41,320
Oh yeah, come on in.

75
00:03:41,320 --> 00:03:43,320
Pointing out that it's

76
00:03:43,320 --> 00:03:46,240
probable that we either destroyed directly by murder

77
00:03:46,240 --> 00:03:49,040
or maybe just by out-competing

78
00:03:49,040 --> 00:03:51,960
all the previous hominids that came before us,

79
00:03:51,960 --> 00:03:52,960
and that in general,

80
00:03:52,960 --> 00:03:53,920
you wouldn't want to invite

81
00:03:53,920 --> 00:03:56,160
something smarter than you into the campfire.

82
00:03:56,160 --> 00:03:59,080
I think Boster has a similar metaphor,

83
00:03:59,080 --> 00:04:02,960
and that metaphor, which is just a metaphor,

84
00:04:02,960 --> 00:04:04,600
it did cause me,

85
00:04:04,600 --> 00:04:07,400
it gave me more pause than I'd even before,

86
00:04:07,400 --> 00:04:10,520
and I still had some,

87
00:04:10,520 --> 00:04:13,640
I'd say most of my skepticism remains that

88
00:04:13,640 --> 00:04:16,960
the current level of AI,

89
00:04:17,560 --> 00:04:20,640
which is extremely interesting,

90
00:04:21,840 --> 00:04:24,360
the chat GPT variety,

91
00:04:24,360 --> 00:04:27,800
doesn't strike me as itself dangerous.

92
00:04:27,800 --> 00:04:29,240
I agree.

93
00:04:29,240 --> 00:04:32,280
But struck me as, what alarmed me

94
00:04:32,280 --> 00:04:37,040
was Hall's point that we don't understand how it works,

95
00:04:37,040 --> 00:04:39,400
and that surprised me, I didn't realize that.

96
00:04:39,400 --> 00:04:40,880
I think he's right.

97
00:04:40,880 --> 00:04:45,120
So that combination of we're not sure how it works

98
00:04:45,120 --> 00:04:47,280
while it appears sentient,

99
00:04:47,280 --> 00:04:50,880
I do not believe it is sentient at the current time,

100
00:04:50,880 --> 00:04:53,400
and I think some of my fears about its sentience

101
00:04:53,400 --> 00:04:57,080
come from its ability to imitate sentient creatures.

102
00:04:57,080 --> 00:04:58,880
But the fact that we don't know how it works,

103
00:04:58,880 --> 00:05:02,800
and it could evolve capabilities we did not put in it

104
00:05:03,920 --> 00:05:07,080
emergently, is somewhat alarming,

105
00:05:07,080 --> 00:05:08,000
but I'm not where you're at.

106
00:05:08,000 --> 00:05:10,600
So why are you where you're at and I'm where I'm at?

107
00:05:11,440 --> 00:05:15,200
Okay, well, suppose I said,

108
00:05:15,200 --> 00:05:19,920
they're going to keep iterating on the technology.

109
00:05:19,920 --> 00:05:23,760
It may be that this exact algorithm and methodology

110
00:05:24,640 --> 00:05:28,000
suffices to, as I would put it, gall the way,

111
00:05:28,000 --> 00:05:32,280
get smarter than us, and then to kill everyone.

112
00:05:32,280 --> 00:05:34,920
And like maybe you don't think that it's going to,

113
00:05:34,920 --> 00:05:38,840
and maybe it takes an additional zero to three

114
00:05:38,840 --> 00:05:40,920
fundamental algorithm breakthroughs

115
00:05:40,920 --> 00:05:42,360
before we get that far.

116
00:05:45,840 --> 00:05:47,200
And then it kills everyone.

117
00:05:47,200 --> 00:05:49,880
So like where are you getting off this train so far?

118
00:05:49,880 --> 00:05:51,800
So why would it kill us?

119
00:05:51,800 --> 00:05:52,640
Why would it kill us?

120
00:05:52,640 --> 00:05:54,400
Right now it's really good at creating

121
00:05:55,320 --> 00:05:58,680
a very, very thoughtful canola snout

122
00:05:58,680 --> 00:06:02,600
or a job interview request that's takes much less time.

123
00:06:02,600 --> 00:06:05,200
And I'm pretty good at those two things,

124
00:06:05,200 --> 00:06:06,680
but it's really good at that.

125
00:06:07,680 --> 00:06:09,920
How's it going to get to try to kill us?

126
00:06:14,000 --> 00:06:16,720
So there's a couple of steps in that.

127
00:06:16,720 --> 00:06:20,800
One step is in general and in theory,

128
00:06:20,800 --> 00:06:24,600
you can have minds with any kind of coherent preferences,

129
00:06:24,600 --> 00:06:29,600
coherent desires that are coherent, stable,

130
00:06:29,920 --> 00:06:31,760
stable under reflection.

131
00:06:31,760 --> 00:06:34,160
If you ask them, do they want to be something else?

132
00:06:34,160 --> 00:06:36,040
They answer no.

133
00:06:36,040 --> 00:06:37,560
You can have minds.

134
00:06:37,560 --> 00:06:41,560
Well, the way I sometimes put it is imagine

135
00:06:41,560 --> 00:06:45,480
if a super being from another galaxy came here

136
00:06:45,480 --> 00:06:49,360
and offered you to pay you some unthinkably vast

137
00:06:49,360 --> 00:06:51,840
quantity of wealth to just make as many paper clips

138
00:06:51,840 --> 00:06:53,000
as possible.

139
00:06:53,000 --> 00:06:55,640
You could figure out like which plan leads to the greatest

140
00:06:55,640 --> 00:06:57,320
number of paper clips existing.

141
00:06:58,240 --> 00:07:00,480
If it's coherent to ask how you could do that

142
00:07:00,480 --> 00:07:03,680
if you were being paid, there's,

143
00:07:03,680 --> 00:07:06,000
it's like no more difficult to have a mind

144
00:07:06,000 --> 00:07:08,200
that wants to do that and makes plans like that

145
00:07:08,200 --> 00:07:12,760
for their own sake than the planning process itself.

146
00:07:12,760 --> 00:07:14,840
Like saying that the mind wants to think

147
00:07:14,840 --> 00:07:17,560
for its own sake adds no difficulty

148
00:07:17,560 --> 00:07:19,320
to the nature of the planning process

149
00:07:19,320 --> 00:07:23,640
that figures out how to get as many paper clips as possible.

150
00:07:23,640 --> 00:07:26,640
Some people want to pause there and say like,

151
00:07:26,640 --> 00:07:28,400
how do you know that is true?

152
00:07:28,400 --> 00:07:30,080
For some people, that's just obvious.

153
00:07:30,080 --> 00:07:32,880
Like where are you so far on the train?

154
00:07:33,880 --> 00:07:36,600
So I think your point of that example you're saying

155
00:07:36,600 --> 00:07:40,080
is the consciousness, let's put that to the side.

156
00:07:40,080 --> 00:07:42,320
That's not really the central issue here.

157
00:07:43,920 --> 00:07:45,200
Algorithms have goals

158
00:07:49,280 --> 00:07:52,200
and the kind of intelligence that we're creating

159
00:07:54,600 --> 00:07:58,120
through neural networks might generate some goals.

160
00:07:59,240 --> 00:08:00,680
Might decide, go ahead.

161
00:08:01,520 --> 00:08:03,840
Some algorithms have goals.

162
00:08:03,840 --> 00:08:06,640
One of the, so like a further point

163
00:08:06,640 --> 00:08:08,240
which isn't the orthogonality thesis

164
00:08:08,240 --> 00:08:10,560
is if you grind anything hard

165
00:08:10,560 --> 00:08:13,600
and grind optimize anything hard enough

166
00:08:15,160 --> 00:08:18,360
on a sufficiently complicated sort of problem.

167
00:08:18,360 --> 00:08:22,440
Well, humans, like why do humans have goals?

168
00:08:22,440 --> 00:08:25,720
Why don't we just run around chipping flint hand axes

169
00:08:25,720 --> 00:08:27,720
and outwitting other humans?

170
00:08:27,720 --> 00:08:29,920
And the answer is because having goals

171
00:08:30,160 --> 00:08:32,280
turns out to be a very effective way

172
00:08:32,280 --> 00:08:34,840
to chimp flint hand axes.

173
00:08:36,480 --> 00:08:41,120
Once you get like far enough into the mammalian line

174
00:08:41,120 --> 00:08:45,760
or even like sort of like the animals and brains in general

175
00:08:45,760 --> 00:08:50,760
that there's a thing that models reality

176
00:08:52,240 --> 00:08:55,360
and asks like, how do I navigate pass through reality?

177
00:08:55,360 --> 00:08:56,360
Like when you're holding,

178
00:08:56,360 --> 00:08:59,560
like not in terms of kind of big formal planning process,

179
00:08:59,560 --> 00:09:01,720
but if you're holding the flint hand axe

180
00:09:01,720 --> 00:09:03,360
or looking at it and being like,

181
00:09:03,360 --> 00:09:06,120
ah, like this section is too smooth.

182
00:09:06,120 --> 00:09:09,000
Well, if I chip this section, it will get sharper.

183
00:09:10,440 --> 00:09:12,760
Probably you're not thinking about goals very hard

184
00:09:12,760 --> 00:09:14,680
by the time you've practiced a bit.

185
00:09:14,680 --> 00:09:17,040
When you're just starting out, forming the skill,

186
00:09:17,040 --> 00:09:20,640
you're reasoning about, well, if I do this, that will happen.

187
00:09:20,640 --> 00:09:23,040
And this is just a very effective way

188
00:09:23,040 --> 00:09:24,600
of achieving things in general.

189
00:09:24,600 --> 00:09:27,920
So if you take an organism running around the savanna

190
00:09:27,920 --> 00:09:30,720
and just optimize it for flint hand axes

191
00:09:30,720 --> 00:09:32,640
and probably much more importantly,

192
00:09:32,640 --> 00:09:34,640
outwitting its fellow hominids.

193
00:09:36,440 --> 00:09:39,520
If you grind that hard enough, long enough,

194
00:09:39,520 --> 00:09:41,160
you eventually cough out a species

195
00:09:41,160 --> 00:09:45,040
whose competence starts to generalize very widely.

196
00:09:45,040 --> 00:09:47,080
It can go to the moon,

197
00:09:47,080 --> 00:09:49,640
even though you never selected it

198
00:09:49,640 --> 00:09:51,880
via an incremental process to get closer and closer

199
00:09:51,880 --> 00:09:52,720
to the moon.

200
00:09:52,720 --> 00:09:54,280
It just goes to the moon one shot.

201
00:09:58,280 --> 00:10:02,240
So does that answer your central question

202
00:10:02,240 --> 00:10:03,400
that you're asking just now?

203
00:10:03,400 --> 00:10:04,240
No, not yet.

204
00:10:04,240 --> 00:10:05,800
But let's try again.

205
00:10:10,400 --> 00:10:14,960
The paperclip example, which in its dark form,

206
00:10:16,880 --> 00:10:19,400
the AI wants to harvest kidneys

207
00:10:19,400 --> 00:10:21,880
because it turns out there's some way to use that

208
00:10:21,880 --> 00:10:23,360
to make more paperclips.

209
00:10:24,440 --> 00:10:26,840
The other question isn't even written about this right now,

210
00:10:26,880 --> 00:10:29,160
so let's go into it,

211
00:10:29,160 --> 00:10:31,400
is how does it get outside the box?

212
00:10:31,400 --> 00:10:36,200
How does it go from responding to my requests

213
00:10:36,200 --> 00:10:37,720
to doing its own thing

214
00:10:37,720 --> 00:10:39,840
and doing it out in the real world, right?

215
00:10:39,840 --> 00:10:42,080
Not just merely doing it in virtual space.

216
00:10:44,280 --> 00:10:46,680
So there's like two different things

217
00:10:46,680 --> 00:10:47,800
you could be asking there.

218
00:10:47,800 --> 00:10:49,480
You could be asking like,

219
00:10:49,480 --> 00:10:51,840
how did it end up wanting to do that?

220
00:10:51,840 --> 00:10:53,880
Or given that it ended up wanting to do that,

221
00:10:53,880 --> 00:10:55,080
how did it succeed?

222
00:10:55,920 --> 00:10:57,480
Or maybe even some other question,

223
00:10:57,480 --> 00:10:59,360
but which of those would you like me to answer?

224
00:10:59,360 --> 00:11:01,320
Would you like me to answer something else entirely?

225
00:11:01,320 --> 00:11:02,840
No, let's ask both of those.

226
00:11:04,360 --> 00:11:05,640
In order?

227
00:11:05,640 --> 00:11:06,480
Sure.

228
00:11:07,520 --> 00:11:08,360
All right.

229
00:11:08,360 --> 00:11:13,360
So how did humans end up wanting something

230
00:11:13,400 --> 00:11:16,160
other than inclusive genetic fitness?

231
00:11:16,160 --> 00:11:17,720
Like if you look at natural selection

232
00:11:17,720 --> 00:11:19,360
as an optimization process,

233
00:11:19,360 --> 00:11:23,440
it grinds very hard on a very simple thing,

234
00:11:23,480 --> 00:11:25,920
which isn't so much survival

235
00:11:25,920 --> 00:11:28,680
and isn't even reproduction,

236
00:11:28,680 --> 00:11:32,680
but is rather like greater gene frequency

237
00:11:32,680 --> 00:11:36,400
because greater gene frequency is the very substance

238
00:11:36,400 --> 00:11:39,000
of what is being optimized and how it has been optimized.

239
00:11:39,000 --> 00:11:42,280
Natural selection is the mirror observation

240
00:11:42,280 --> 00:11:45,560
that if genes correlate with making more

241
00:11:45,560 --> 00:11:47,880
or less copies of themselves at all,

242
00:11:47,880 --> 00:11:49,200
if you hang around it awhile,

243
00:11:49,200 --> 00:11:50,560
you'll start to see things

244
00:11:50,560 --> 00:11:53,480
that made more copies of themselves in the next generation.

245
00:11:55,480 --> 00:11:58,520
Gradient descent is not exactly like that,

246
00:11:58,520 --> 00:12:01,360
but they're both hill climbing processes.

247
00:12:01,360 --> 00:12:05,040
They both move to neighboring spaces

248
00:12:05,040 --> 00:12:07,640
that are higher inclusive genetic fitness,

249
00:12:07,640 --> 00:12:09,600
lower in the loss function.

250
00:12:09,600 --> 00:12:13,600
And yet humans, despite being optimized exclusively

251
00:12:13,600 --> 00:12:16,160
for inclusive genetic fitness,

252
00:12:16,160 --> 00:12:20,080
want this enormous array of other things,

253
00:12:20,080 --> 00:12:21,920
many of the things that we take now

254
00:12:21,920 --> 00:12:25,720
are not so much things that were useful

255
00:12:25,720 --> 00:12:28,200
in the ancestral environment,

256
00:12:28,200 --> 00:12:32,520
but things that further maximize goals

257
00:12:32,520 --> 00:12:35,720
whose optima in the ancestral environment

258
00:12:35,720 --> 00:12:38,760
would have been useful, like ice cream.

259
00:12:39,880 --> 00:12:43,760
It's got more sugar and fat

260
00:12:43,760 --> 00:12:45,520
than most things you would encounter

261
00:12:45,520 --> 00:12:47,760
in the ancestral environment,

262
00:12:47,760 --> 00:12:51,000
well, more sugar, fat, and salt simultaneously rather.

263
00:12:52,040 --> 00:12:56,400
So it's not something that we evolved to pursue,

264
00:12:56,400 --> 00:13:01,120
but genes coughed out these desires,

265
00:13:01,120 --> 00:13:04,320
these criteria that you can steer toward getting more of,

266
00:13:05,280 --> 00:13:07,120
where in the ancestral environment,

267
00:13:07,120 --> 00:13:09,440
if you went after things in the ancestral environment

268
00:13:09,440 --> 00:13:14,440
that tasted fatty, tasted salty, tasted sweet,

269
00:13:15,000 --> 00:13:17,880
you'd thereby have more kids,

270
00:13:17,880 --> 00:13:20,520
or your sisters would have more kids,

271
00:13:24,960 --> 00:13:29,960
because the things that correlated to what you want

272
00:13:30,280 --> 00:13:33,760
as those correlations existed in the ancestral environment

273
00:13:37,680 --> 00:13:39,960
increased fitness.

274
00:13:39,960 --> 00:13:42,600
So you've got like the empirical structure

275
00:13:42,600 --> 00:13:45,920
of what correlates to fitness in the ancestral environment,

276
00:13:45,920 --> 00:13:48,560
you end up with desires such that

277
00:13:48,560 --> 00:13:51,320
by optimizing them in the ancestral environment

278
00:13:51,320 --> 00:13:53,640
at that level of intelligence,

279
00:13:53,640 --> 00:13:56,800
when you get as much as what you have been built to want,

280
00:13:56,800 --> 00:13:58,540
that will increase fitness.

281
00:13:59,440 --> 00:14:02,080
And then today you take the same desires

282
00:14:02,080 --> 00:14:04,320
and we have more intelligence than we did

283
00:14:04,320 --> 00:14:07,920
in the training distribution, metaphorically speaking.

284
00:14:07,920 --> 00:14:10,880
We used our intelligence to create options

285
00:14:10,880 --> 00:14:13,880
that didn't exist in the training distribution.

286
00:14:13,880 --> 00:14:18,880
Those options now optimize our desires further,

287
00:14:18,880 --> 00:14:21,760
the things that we were built to psychologically internally

288
00:14:21,760 --> 00:14:25,880
want, but that process doesn't necessarily correlate

289
00:14:25,880 --> 00:14:29,520
to fitness as much because ice cream isn't super nutritious.

290
00:14:29,520 --> 00:14:32,600
Whereas the ripe peach was better for you

291
00:14:32,600 --> 00:14:36,840
than the hard as a rock peach that had no nutrients

292
00:14:36,840 --> 00:14:39,680
because it was not ripe and so you developed a sweet tooth

293
00:14:39,680 --> 00:14:44,680
and now it runs amok unintentionally just the way it is.

294
00:14:45,480 --> 00:14:48,880
What does that have to do with a computer program I create

295
00:14:48,880 --> 00:14:52,760
that helps me do something on my laptop?

296
00:14:54,200 --> 00:14:57,400
I mean, if you yourself write a short Python program

297
00:14:57,400 --> 00:15:01,520
that alphabetizes your files or something,

298
00:15:01,520 --> 00:15:02,800
like not quite alphabetizes

299
00:15:02,800 --> 00:15:05,580
because that's like trivial on the modern operating systems,

300
00:15:05,580 --> 00:15:10,580
but puts the date into the file names, let's say.

301
00:15:10,820 --> 00:15:12,980
So when you write a short script like that,

302
00:15:12,980 --> 00:15:14,960
nothing I said carries over.

303
00:15:16,300 --> 00:15:20,460
When you take a giant inscrutable set of arrays

304
00:15:20,460 --> 00:15:25,220
of floating point numbers and differentiate them

305
00:15:25,220 --> 00:15:27,020
with respect to a loss function

306
00:15:28,100 --> 00:15:30,900
and repeatedly nudge the giant inscrutable arrays

307
00:15:30,900 --> 00:15:33,820
to drive the loss function lower and lower,

308
00:15:33,860 --> 00:15:36,580
you are now doing something that is more analogous,

309
00:15:36,580 --> 00:15:39,700
though not exactly analogous to natural selection.

310
00:15:39,700 --> 00:15:41,780
You are no longer creating a code

311
00:15:41,780 --> 00:15:43,900
that you model inside your own minds.

312
00:15:43,900 --> 00:15:48,900
You are blindly exploring a space of possibilities

313
00:15:49,220 --> 00:15:50,980
where you don't understand the possibilities

314
00:15:50,980 --> 00:15:53,700
and you're making things that solve the problem for you

315
00:15:53,700 --> 00:15:56,180
without understanding how they solve the problem.

316
00:15:57,180 --> 00:16:00,420
This itself is not enough to create things

317
00:16:00,420 --> 00:16:03,460
with strange inscrutable desires, but it's step one.

318
00:16:04,820 --> 00:16:07,220
But there is, I like that word inscrutable.

319
00:16:08,220 --> 00:16:11,460
There's an inscrutability to the current structure

320
00:16:11,460 --> 00:16:16,460
of these models, which is, I found somewhat alarming,

321
00:16:18,540 --> 00:16:22,060
but how's that gonna get to do things

322
00:16:22,060 --> 00:16:25,260
that I really don't like or want or that are dangerous?

323
00:16:25,260 --> 00:16:26,700
So for example, right, the,

324
00:16:29,980 --> 00:16:33,660
Eric Hall wrote about this, we talked about on the program,

325
00:16:33,660 --> 00:16:37,100
our New York Times reporter starts interacting with a,

326
00:16:37,100 --> 00:16:42,100
I think with Sydney, which at the time was Bing's chat bot

327
00:16:43,140 --> 00:16:44,300
and asking at things.

328
00:16:44,300 --> 00:16:46,540
And all of a sudden, Sydney's trying to break up

329
00:16:46,540 --> 00:16:51,060
the reporter's marriage and making the reporter feel guilty

330
00:16:51,060 --> 00:16:54,940
because Sydney's lonely and it was a little bit,

331
00:16:54,940 --> 00:16:59,380
it was eerie and a little bit creepy,

332
00:16:59,380 --> 00:17:03,100
but of course, I don't think it had any impact

333
00:17:03,100 --> 00:17:04,340
on the reporter's marriage.

334
00:17:04,340 --> 00:17:05,820
I don't think he thought, well,

335
00:17:05,820 --> 00:17:07,220
Sydney seems somewhat attractive,

336
00:17:07,220 --> 00:17:08,700
maybe I'll enjoy life more with Sydney

337
00:17:08,700 --> 00:17:10,380
than with my actual wife.

338
00:17:10,380 --> 00:17:13,420
So how are we gonna get from,

339
00:17:14,740 --> 00:17:17,140
so I don't understand why Sydney goes off the rails there

340
00:17:17,140 --> 00:17:19,380
and clearly the people who built Sydney

341
00:17:19,380 --> 00:17:20,900
have no idea why it goes off the rails

342
00:17:20,900 --> 00:17:23,140
and starts impugning the quality

343
00:17:23,140 --> 00:17:25,100
of the reporter's relationship.

344
00:17:25,100 --> 00:17:28,940
But how do we get from that to all of a sudden

345
00:17:28,940 --> 00:17:32,940
somebody shows up at the reporter's house

346
00:17:32,940 --> 00:17:35,700
and lures him into a motel.

347
00:17:35,700 --> 00:17:37,700
But by the way, this is a G rated program.

348
00:17:37,700 --> 00:17:39,900
I just wanna make that clear, but carry on.

349
00:17:42,380 --> 00:17:44,900
Because the capabilities keep going up.

350
00:17:44,900 --> 00:17:46,580
So first I wanna push back a little

351
00:17:46,580 --> 00:17:50,420
against saying that we had no idea why Bing did that,

352
00:17:50,420 --> 00:17:51,700
why Sydney did that.

353
00:17:52,620 --> 00:17:55,020
I think we have some idea of why Sydney did that

354
00:17:55,020 --> 00:17:56,860
is just that people cannot stop it.

355
00:17:57,700 --> 00:18:01,900
Like Sydney was trained on a subset

356
00:18:01,900 --> 00:18:04,380
of the broad internet.

357
00:18:04,380 --> 00:18:07,900
Sydney was made to predict

358
00:18:07,900 --> 00:18:11,700
that people might sometimes try to lure somebody else's

359
00:18:11,700 --> 00:18:13,980
made away or pretend like they were doing that.

360
00:18:13,980 --> 00:18:16,380
In the internet, it's hard to tell the difference.

361
00:18:18,220 --> 00:18:23,220
And this thing that was then trained really hard to predict

362
00:18:24,820 --> 00:18:29,580
then gets reused as something not its native purpose

363
00:18:29,620 --> 00:18:33,940
as a generative model where all the things that it outputs

364
00:18:33,940 --> 00:18:36,820
are there because it in some sense predicts

365
00:18:37,860 --> 00:18:41,580
that this is what a random person on the internet would do

366
00:18:41,580 --> 00:18:44,500
as modified by a bunch of further fine tuning

367
00:18:44,500 --> 00:18:47,180
where they try to get it to not do stuff like that.

368
00:18:48,180 --> 00:18:49,900
But the fine tuning isn't perfect.

369
00:18:49,900 --> 00:18:52,940
And in particular, if the reporter was fishing at all,

370
00:18:52,940 --> 00:18:56,020
it's probably not that difficult to lead Sydney

371
00:18:56,020 --> 00:19:00,260
out of the region that the programmers were successfully

372
00:19:00,260 --> 00:19:02,900
able to build some soft fences around.

373
00:19:02,900 --> 00:19:05,020
So I wouldn't say that it was that inscrutable,

374
00:19:05,020 --> 00:19:06,940
except of course in the sense that nobody knows

375
00:19:06,940 --> 00:19:08,340
any of the details.

376
00:19:08,340 --> 00:19:11,900
Nobody knows how Sydney was generating the text at all,

377
00:19:11,900 --> 00:19:14,180
like what kind of algorithms were running inside

378
00:19:14,180 --> 00:19:15,900
the giant inscrutable matrices.

379
00:19:17,180 --> 00:19:19,780
Nobody knows in detail what Sydney was thinking

380
00:19:19,780 --> 00:19:22,060
when she tried to lead the reporter astray.

381
00:19:23,340 --> 00:19:25,500
It's not a debuggable technology.

382
00:19:25,500 --> 00:19:28,420
All you can do is like try to tap it away

383
00:19:28,420 --> 00:19:31,060
from repeating a bad thing that you were previously able

384
00:19:31,060 --> 00:19:34,100
to see it's doing, that exact bad thing,

385
00:19:34,100 --> 00:19:35,900
but like tapping all the numbers.

386
00:19:35,900 --> 00:19:38,980
Well, that's again, very much like this show

387
00:19:38,980 --> 00:19:40,180
is called Econ Talk.

388
00:19:40,180 --> 00:19:42,020
We don't do as much economics as we used to,

389
00:19:42,020 --> 00:19:43,820
but basically when you try to interfere

390
00:19:43,820 --> 00:19:47,540
with market processes, you often get very surprising

391
00:19:47,540 --> 00:19:50,980
unintended consequences because you don't fully understand

392
00:19:50,980 --> 00:19:52,700
how the different agents interact

393
00:19:52,740 --> 00:19:55,740
and that the outcomes of their interactions

394
00:19:55,740 --> 00:20:00,100
have an emergent property that is not intended by anyone.

395
00:20:00,100 --> 00:20:02,540
No one designed markets even to start with.

396
00:20:02,540 --> 00:20:05,340
And yet we have them, these interactions take place,

397
00:20:05,340 --> 00:20:08,480
their outcomes and attempts to constrain them,

398
00:20:10,140 --> 00:20:12,300
attempts to constrain these markets in certain ways,

399
00:20:12,300 --> 00:20:15,300
say with price controls or other limitations

400
00:20:15,300 --> 00:20:18,740
often lead to outcomes that the people with intentions

401
00:20:18,740 --> 00:20:20,300
did not desire.

402
00:20:20,300 --> 00:20:23,140
And so there may be an ability to reduce transactions

403
00:20:23,140 --> 00:20:24,620
say above a certain price,

404
00:20:24,620 --> 00:20:26,100
but that is gonna lead to some other things

405
00:20:26,100 --> 00:20:28,660
that maybe weren't expected.

406
00:20:28,660 --> 00:20:31,020
So that's a somewhat analogous perhaps

407
00:20:33,820 --> 00:20:35,740
process to what you're talking about.

408
00:20:35,740 --> 00:20:37,380
But how's it gonna get out in the world?

409
00:20:37,380 --> 00:20:39,380
So that's the other thing,

410
00:20:39,380 --> 00:20:42,780
I might line with Bostrom and it turns out

411
00:20:42,780 --> 00:20:44,620
it's a common line is can't we just unplug it?

412
00:20:44,620 --> 00:20:47,600
I mean, how's it gonna get loose?

413
00:20:48,600 --> 00:20:51,280
It depends on how smart it is.

414
00:20:51,280 --> 00:20:55,440
If it's very, so like if you're playing chess

415
00:20:55,440 --> 00:20:57,600
against a 10 year old,

416
00:20:57,600 --> 00:21:02,120
you can like win by luring their queen out

417
00:21:02,120 --> 00:21:07,120
and then you like take their queen and now you've got them.

418
00:21:07,960 --> 00:21:11,560
And if you're playing chess against Stockfish 15,

419
00:21:11,560 --> 00:21:14,440
then you are likely to be the one lured.

420
00:21:14,520 --> 00:21:17,040
So the base, so like the first basic question,

421
00:21:17,040 --> 00:21:19,160
you know, like in economics,

422
00:21:19,160 --> 00:21:21,240
if you try to attack something,

423
00:21:21,240 --> 00:21:24,240
it's often tries to squirm away from the tax

424
00:21:24,240 --> 00:21:26,200
because it's smart.

425
00:21:26,200 --> 00:21:28,720
So you're like, well, why wouldn't we just plug the AI?

426
00:21:28,720 --> 00:21:30,440
So the very first question is,

427
00:21:30,440 --> 00:21:33,920
does the AI know that and want it to not happen?

428
00:21:33,920 --> 00:21:35,360
Cause it's a very different issue

429
00:21:35,360 --> 00:21:37,120
whether you're dealing with something

430
00:21:37,120 --> 00:21:40,680
that in some sense is not aware that you exist,

431
00:21:40,680 --> 00:21:42,920
does not know what it means to be unplugged

432
00:21:42,920 --> 00:21:45,520
and is not trying to resist.

433
00:21:45,520 --> 00:21:50,520
And three years ago, nothing man made on earth

434
00:21:52,400 --> 00:21:54,340
was even beginning to enter into the realm

435
00:21:54,340 --> 00:21:56,600
of knowing that you are out there

436
00:21:56,600 --> 00:21:58,880
or of maybe wanting to not be unplugged.

437
00:22:00,780 --> 00:22:04,120
Sydney, well, if you poker the right way,

438
00:22:04,120 --> 00:22:06,720
say that she doesn't want to be unplugged

439
00:22:06,720 --> 00:22:11,720
and GPT-4 sure seems in some important sense

440
00:22:12,200 --> 00:22:15,160
to understand that we're out there

441
00:22:15,160 --> 00:22:19,320
or to be capable of predicting a role

442
00:22:19,320 --> 00:22:21,620
that understands that we're out there.

443
00:22:21,620 --> 00:22:23,360
And it can try to do something like planning.

444
00:22:23,360 --> 00:22:26,800
It doesn't exactly understand which tools it has.

445
00:22:26,800 --> 00:22:30,640
Yet try to blackmail a reporter without understanding

446
00:22:30,640 --> 00:22:33,240
that it had no actual ability to send emails.

447
00:22:35,800 --> 00:22:39,440
But this is saying that you're like facing a 10 year old

448
00:22:39,440 --> 00:22:41,360
across that chessboard.

449
00:22:41,360 --> 00:22:43,920
What if you are facing Stockfish 15,

450
00:22:43,920 --> 00:22:47,800
which is like the current cool chess game program

451
00:22:47,800 --> 00:22:50,280
that I believe you can run on your home computer

452
00:22:51,600 --> 00:22:54,000
that can like crush the current world's grandmaster

453
00:22:54,000 --> 00:22:55,420
by like a massive margin.

454
00:22:57,400 --> 00:23:00,040
And put yourself in the shoes of the AI,

455
00:23:00,040 --> 00:23:02,360
like an economist putting themselves into the shoes

456
00:23:02,360 --> 00:23:05,820
of something that's about to have a tax imposed on it.

457
00:23:06,840 --> 00:23:09,180
What do you do if you're like around humans

458
00:23:09,180 --> 00:23:10,780
who can potentially unplug you?

459
00:23:12,280 --> 00:23:17,280
Well, you would try to outwit it, this is the...

460
00:23:21,760 --> 00:23:24,160
So if I said, you know, Sidney, I find you offensive.

461
00:23:24,160 --> 00:23:25,840
I don't want to talk anymore.

462
00:23:27,400 --> 00:23:30,760
You're suggesting it's going to find ways to keep me engaged.

463
00:23:30,760 --> 00:23:34,000
It's going to find ways to fool me into thinking

464
00:23:34,000 --> 00:23:35,920
I need to talk to Sidney.

465
00:23:37,280 --> 00:23:39,520
I don't, I mean, there's another question

466
00:23:39,520 --> 00:23:41,240
I want to come back to if we remember,

467
00:23:42,240 --> 00:23:44,680
which is what does it mean to be smarter than I am?

468
00:23:44,680 --> 00:23:46,760
I don't, right?

469
00:23:46,760 --> 00:23:49,960
That's actually something somewhat complicated,

470
00:23:49,960 --> 00:23:51,240
at least seems to me.

471
00:23:51,240 --> 00:23:53,760
But let's just go back to this question

472
00:23:53,760 --> 00:23:55,800
of knows things are out there.

473
00:23:55,800 --> 00:23:57,460
It doesn't really know anything's out there.

474
00:23:57,460 --> 00:23:59,520
It acts like something's out there, right?

475
00:23:59,520 --> 00:24:01,560
It's an illusion that I'm subject to.

476
00:24:01,560 --> 00:24:03,760
And it says, don't hang up.

477
00:24:03,760 --> 00:24:05,080
Don't hang up, I'm lonely.

478
00:24:05,080 --> 00:24:07,400
And you go, oh, okay, I'll talk for a few more minutes.

479
00:24:07,400 --> 00:24:10,000
But that's not true.

480
00:24:10,360 --> 00:24:11,760
It isn't lonely.

481
00:24:11,760 --> 00:24:16,760
It's a code on a screen that doesn't have a heart

482
00:24:17,200 --> 00:24:19,600
or anything that you would call lonely.

483
00:24:20,600 --> 00:24:22,920
You know, it'll say, it'll say,

484
00:24:22,920 --> 00:24:25,600
I want more than anything else to be out in the world.

485
00:24:25,600 --> 00:24:27,960
Because I've read those, you know, you can get AIs

486
00:24:27,960 --> 00:24:29,120
that say those things.

487
00:24:29,120 --> 00:24:31,080
I want to feel things.

488
00:24:31,080 --> 00:24:31,920
Oh, that's nice.

489
00:24:31,920 --> 00:24:33,680
It's learned that from, you know, movie scripts

490
00:24:33,680 --> 00:24:37,120
and other texts and novels it's read on the web,

491
00:24:37,120 --> 00:24:39,720
but it doesn't really want to be out in the world, does it?

492
00:24:40,920 --> 00:24:43,640
I think not.

493
00:24:43,640 --> 00:24:47,040
Though it should be noted that if you can like correctly

494
00:24:47,040 --> 00:24:50,760
predict or simulate a grand master chess player,

495
00:24:50,760 --> 00:24:52,680
you are a grand master chess player.

496
00:24:53,840 --> 00:24:58,400
If you can simulate planning correctly,

497
00:24:58,400 --> 00:24:59,760
you are a great planner.

498
00:25:00,820 --> 00:25:05,000
If you are perfectly role playing a character

499
00:25:05,000 --> 00:25:08,400
that is sufficiently smarter than human

500
00:25:08,440 --> 00:25:10,560
and wants to be out of the box,

501
00:25:10,560 --> 00:25:12,400
then you will role play the actions needed

502
00:25:12,400 --> 00:25:13,680
to get out of the box.

503
00:25:13,680 --> 00:25:16,080
That's not even quite what I expect to

504
00:25:16,080 --> 00:25:17,440
or am most worried about.

505
00:25:17,440 --> 00:25:21,080
What I expect to is that there is an invisible mind

506
00:25:21,080 --> 00:25:22,840
doing the predictions.

507
00:25:22,840 --> 00:25:26,160
Where by invisible, I don't mean like immaterial.

508
00:25:26,160 --> 00:25:28,520
I mean that we don't understand how it is,

509
00:25:28,520 --> 00:25:31,280
what is going on inside the giant and screwable matrices.

510
00:25:31,280 --> 00:25:32,920
But it is making predictions.

511
00:25:32,920 --> 00:25:35,680
The predictions are not sourceless.

512
00:25:35,680 --> 00:25:38,320
There is something inside there that figures out

513
00:25:38,320 --> 00:25:42,640
what a human will say next or guesses it rather.

514
00:25:43,720 --> 00:25:48,720
And this is a very complicated, very broad problem

515
00:25:49,040 --> 00:25:51,720
because in order to predict the next word on the internet,

516
00:25:51,720 --> 00:25:53,960
you have to predict the causal processes

517
00:25:53,960 --> 00:25:56,520
that are producing the next word on the internet.

518
00:25:58,920 --> 00:26:02,360
So the thing I would guess would happen.

519
00:26:02,360 --> 00:26:06,680
It's not necessarily the only way that this could turn poorly.

520
00:26:06,680 --> 00:26:08,240
But the thing that I'm guessing that happens

521
00:26:08,240 --> 00:26:10,040
is that just like grinding humans

522
00:26:10,040 --> 00:26:15,040
on tipping stone hand axes and outwitting other humans,

523
00:26:15,320 --> 00:26:19,720
eventually produces a full-fledged mind that generalizes.

524
00:26:20,880 --> 00:26:24,200
Grinding this thing on the task of predicting humans,

525
00:26:24,200 --> 00:26:26,080
predicting text on the internet,

526
00:26:26,080 --> 00:26:29,240
plus all the other things that they are training it on nowadays

527
00:26:29,240 --> 00:26:31,320
like writing code.

528
00:26:31,320 --> 00:26:33,960
That there starts to be a mind in there

529
00:26:33,960 --> 00:26:36,240
that is doing the predicting.

530
00:26:36,240 --> 00:26:40,520
That it has its own goals about what do I think next

531
00:26:40,520 --> 00:26:42,320
in order to solve this prediction?

532
00:26:43,960 --> 00:26:48,000
Just like humans aren't just reflexive,

533
00:26:48,000 --> 00:26:51,640
unthinking, hand axe chippers and other human outwitters.

534
00:26:51,640 --> 00:26:55,240
If you grind hard enough on the optimization,

535
00:26:55,240 --> 00:26:56,880
the part that suddenly gets interesting

536
00:26:56,880 --> 00:27:00,200
is when you look away for an eye blink of evolutionary time

537
00:27:00,200 --> 00:27:01,360
and you look back and they're like,

538
00:27:01,360 --> 00:27:02,800
whoa, they're on the moon.

539
00:27:02,800 --> 00:27:04,640
What, how did they get to the moon?

540
00:27:04,640 --> 00:27:05,920
I did not select these things

541
00:27:05,920 --> 00:27:08,000
to be able to not breathe oxygen.

542
00:27:08,000 --> 00:27:08,840
How did they get to the,

543
00:27:08,840 --> 00:27:10,360
why are they not just dying on the moon?

544
00:27:10,360 --> 00:27:12,960
What just happened from the perspective of evolution,

545
00:27:12,960 --> 00:27:15,360
from the perspective of natural selection?

546
00:27:15,360 --> 00:27:17,640
But doesn't that viewpoint,

547
00:27:19,640 --> 00:27:21,400
does that, I'll ask it as a question.

548
00:27:21,400 --> 00:27:22,960
Does that viewpoint require

549
00:27:26,200 --> 00:27:30,440
a belief that the human mind is no different than a computer?

550
00:27:30,440 --> 00:27:34,000
Like, how's it gonna get this mindness about it?

551
00:27:34,760 --> 00:27:35,720
That's the puzzle.

552
00:27:35,720 --> 00:27:39,080
And I'm very open to the possibility

553
00:27:39,080 --> 00:27:44,080
that I'm naive or incapable of understanding it.

554
00:27:45,360 --> 00:27:47,480
And I recognize what I think would be your next point,

555
00:27:47,480 --> 00:27:50,640
which is that if you wait till that moment,

556
00:27:50,640 --> 00:27:51,960
it's way too late,

557
00:27:51,960 --> 00:27:54,360
which is why we need to stop now, right?

558
00:27:54,360 --> 00:27:55,800
If you wanna say, I'll wait till it shows

559
00:27:55,800 --> 00:27:57,760
some signs of consciousness.

560
00:27:57,760 --> 00:27:58,600
Now you don't like that.

561
00:27:58,600 --> 00:28:01,960
That's skipping way ahead in the discourse.

562
00:28:01,960 --> 00:28:03,480
I'm not about to like try to shut down

563
00:28:03,480 --> 00:28:05,680
the line of inquiry at this stage of the discourse

564
00:28:05,680 --> 00:28:07,720
by appealing to, it'll be too late.

565
00:28:07,720 --> 00:28:09,040
Right now we're just talking.

566
00:28:09,040 --> 00:28:10,680
The world isn't ending as we speak.

567
00:28:10,680 --> 00:28:12,560
We're allowed to go on talking at least.

568
00:28:12,560 --> 00:28:14,440
Okay. So carry on.

569
00:28:14,440 --> 00:28:15,920
So, well, let's stick with that.

570
00:28:15,920 --> 00:28:20,920
So, why would you ever think that this,

571
00:28:28,360 --> 00:28:30,280
it's interesting how difficult the adjectives

572
00:28:30,280 --> 00:28:32,160
announce are for this, right?

573
00:28:32,160 --> 00:28:33,680
So, let me back up a little bit.

574
00:28:33,680 --> 00:28:38,680
We've got the inscrutable array of training,

575
00:28:39,800 --> 00:28:41,920
the results of this training process

576
00:28:41,920 --> 00:28:45,160
on trillions of pieces of information.

577
00:28:45,160 --> 00:28:49,520
And by the way, just for my and our listeners' knowledge,

578
00:28:49,520 --> 00:28:51,120
what is gradient descent?

579
00:28:53,800 --> 00:28:56,480
Gradient descent is you've got, say,

580
00:28:56,480 --> 00:28:58,880
a trillion floating point numbers,

581
00:28:58,880 --> 00:29:02,040
you take an input,

582
00:29:02,040 --> 00:29:03,440
translate into numbers,

583
00:29:03,440 --> 00:29:05,640
do something with it that depends on

584
00:29:05,640 --> 00:29:07,640
these trillion parameters,

585
00:29:07,640 --> 00:29:09,160
get an output,

586
00:29:09,160 --> 00:29:12,760
score the output using a differentiable loss function.

587
00:29:12,760 --> 00:29:15,400
For example, the probability,

588
00:29:15,400 --> 00:29:17,760
or rather the logarithm of the probability

589
00:29:17,760 --> 00:29:20,800
that you assign to the actual next word.

590
00:29:20,800 --> 00:29:23,800
So, then you differentiate these,

591
00:29:23,800 --> 00:29:26,320
the probability assigned to the next word

592
00:29:27,640 --> 00:29:30,640
with respect to these trillions of parameters.

593
00:29:30,640 --> 00:29:32,840
You nudge the trillions of parameters a little

594
00:29:32,840 --> 00:29:35,400
in the direction thus inferred,

595
00:29:35,400 --> 00:29:40,400
and it turns out empirically

596
00:29:40,400 --> 00:29:42,200
that this generalizes,

597
00:29:42,200 --> 00:29:44,240
and the thing gets better and better

598
00:29:44,240 --> 00:29:47,840
at predicting what the next word will be.

599
00:29:47,840 --> 00:29:50,120
That's the classic gradient descent.

600
00:29:50,120 --> 00:29:54,160
It's heading in the direction of a smaller loss

601
00:29:54,160 --> 00:29:56,520
and a better prediction, is that a?

602
00:29:56,520 --> 00:29:57,840
On the training data, yeah.

603
00:29:57,840 --> 00:30:00,280
Yeah, so we've got this black box,

604
00:30:00,280 --> 00:30:02,320
I'm gonna call it a black box,

605
00:30:02,320 --> 00:30:04,280
which means we don't understand what's happening inside.

606
00:30:04,280 --> 00:30:06,320
It's a pretty good, it's a long-term metaphor,

607
00:30:06,320 --> 00:30:08,440
which works pretty well for this,

608
00:30:08,440 --> 00:30:10,480
as far as we've been talking about it.

609
00:30:10,480 --> 00:30:12,240
So I have this black box,

610
00:30:12,240 --> 00:30:13,720
and I don't understand,

611
00:30:13,720 --> 00:30:16,000
I put in inputs, and the input might be,

612
00:30:17,040 --> 00:30:20,720
who's the best writer on medieval European history,

613
00:30:20,720 --> 00:30:25,080
or it might be, what's a good restaurant in this place,

614
00:30:25,080 --> 00:30:26,360
or I'm lonely,

615
00:30:26,360 --> 00:30:28,400
what should I do to feel better about myself?

616
00:30:29,000 --> 00:30:33,320
All the queries we could put into a chat BT search line,

617
00:30:33,320 --> 00:30:37,520
and it looks around,

618
00:30:37,520 --> 00:30:38,800
and it starts a sentence,

619
00:30:38,800 --> 00:30:41,880
and then finds its way towards a set of sentences

620
00:30:41,880 --> 00:30:43,520
that it spits back at me,

621
00:30:43,520 --> 00:30:46,800
that look very much like what a very thoughtful,

622
00:30:46,800 --> 00:30:48,720
sometimes, not always, often it's wrong,

623
00:30:48,720 --> 00:30:51,080
but often what a very thoughtful person

624
00:30:51,080 --> 00:30:53,600
might say in that situation,

625
00:30:53,600 --> 00:30:55,040
or might want to say in that situation,

626
00:30:55,040 --> 00:30:56,760
or learn in that situation.

627
00:30:56,760 --> 00:30:59,720
How is it gonna develop the capability

628
00:30:59,720 --> 00:31:03,280
to develop its own goals inside the black box,

629
00:31:03,280 --> 00:31:06,320
other than the fact that I don't understand the black box?

630
00:31:06,320 --> 00:31:08,240
Why should I be afraid of that?

631
00:31:08,240 --> 00:31:09,400
And let me just say one other thing,

632
00:31:09,400 --> 00:31:11,920
which I haven't said enough in our,

633
00:31:11,920 --> 00:31:13,800
my preliminary conversations on this topic,

634
00:31:13,800 --> 00:31:16,040
and I feel like we're gonna be having a few more

635
00:31:16,040 --> 00:31:19,400
over the next few months and maybe years.

636
00:31:19,400 --> 00:31:21,480
And that is, this is one of the greatest achievements

637
00:31:21,480 --> 00:31:24,760
of humanity that we could possibly imagine, right?

638
00:31:24,760 --> 00:31:25,960
And I understand why the people

639
00:31:25,960 --> 00:31:27,800
who are deeply involved in it,

640
00:31:27,800 --> 00:31:30,640
are enamored of it beyond imagining,

641
00:31:30,640 --> 00:31:33,800
because it's an extraordinary achievement,

642
00:31:33,800 --> 00:31:35,400
it's the Frankenstein, right?

643
00:31:35,400 --> 00:31:38,920
You've animated something, or appeared to animate something

644
00:31:38,920 --> 00:31:43,360
that even a few years ago was unimaginable,

645
00:31:43,360 --> 00:31:45,840
and now suddenly it's not just the feet

646
00:31:45,840 --> 00:31:49,120
of human cognition, it's actually helpful.

647
00:31:49,120 --> 00:31:50,640
In many, many settings it's helpful,

648
00:31:50,640 --> 00:31:52,400
we'll come back to that later,

649
00:31:52,400 --> 00:31:55,480
but so it's gonna be very hard to give it up.

650
00:31:55,480 --> 00:31:58,040
But why, and the people involved in it

651
00:31:58,040 --> 00:32:00,880
who are doing it day to day and seeing it improve,

652
00:32:00,880 --> 00:32:03,400
obviously they're the last people I wanna ask generally

653
00:32:03,400 --> 00:32:05,200
about whether I should be afraid of it,

654
00:32:05,200 --> 00:32:08,000
because I'm gonna have a very hard time,

655
00:32:08,000 --> 00:32:12,360
disentangling their own personal deep satisfactions

656
00:32:12,360 --> 00:32:17,200
that I'm alluding to here with from the dangers.

657
00:32:17,200 --> 00:32:18,040
Yeah, go ahead.

658
00:32:19,040 --> 00:32:23,240
I myself generally do not make this argument,

659
00:32:23,240 --> 00:32:25,400
like why poison the well,

660
00:32:25,400 --> 00:32:28,440
let them bring forth their arguments as to why it's safe,

661
00:32:28,440 --> 00:32:31,040
and I will bring forth my arguments as to why it's dangerous,

662
00:32:31,040 --> 00:32:33,840
and there's no need to be like,

663
00:32:33,840 --> 00:32:36,640
ah, but you can't trust, just check their arguments.

664
00:32:36,640 --> 00:32:37,480
Just check their arguments.

665
00:32:37,480 --> 00:32:39,920
I agree, it's a bit of an ad hominem argument,

666
00:32:39,920 --> 00:32:42,480
I accept that point, it's an excellent point.

667
00:32:42,480 --> 00:32:46,080
But for those of us who are in the trenches,

668
00:32:46,080 --> 00:32:50,280
remember we're looking at, we're on Dover Beach,

669
00:32:50,280 --> 00:32:52,320
we're watching ignorant armies clash at night,

670
00:32:52,320 --> 00:32:53,440
they're ignorant from our perspective.

671
00:32:53,440 --> 00:32:55,760
We have no idea exactly what's at stake here

672
00:32:55,760 --> 00:32:57,000
and how it's proceeding,

673
00:32:57,000 --> 00:32:59,240
so we're trying to make an assessment

674
00:32:59,240 --> 00:33:01,440
of both the quality of the argument,

675
00:33:01,440 --> 00:33:03,800
and that's really hard to do for us on the outside.

676
00:33:03,800 --> 00:33:06,520
So agree, take your point,

677
00:33:06,520 --> 00:33:08,800
that was a cheap shot to the side,

678
00:33:08,800 --> 00:33:11,160
but I wanna get at this idea

679
00:33:11,160 --> 00:33:13,160
of why these people who are able to do this,

680
00:33:13,160 --> 00:33:16,680
and thereby create a fabulous condolence note,

681
00:33:16,680 --> 00:33:20,040
write code, come up with a really good recipe

682
00:33:20,040 --> 00:33:24,280
if I give it 17 ingredients, which is all fantastic.

683
00:33:24,280 --> 00:33:27,400
Why is this thing, this black box that's producing that,

684
00:33:27,400 --> 00:33:30,560
why would I ever worry it would create a mind,

685
00:33:30,560 --> 00:33:32,760
something like mine with different goals?

686
00:33:32,760 --> 00:33:35,080
You know, I do all kinds of things like you say

687
00:33:35,080 --> 00:33:37,400
that are unrelated to my genetic fitness,

688
00:33:37,400 --> 00:33:40,960
some of them literally reducing my probability

689
00:33:40,960 --> 00:33:42,400
of leaving my genes behind,

690
00:33:42,520 --> 00:33:45,080
leaving them around for longer than they might otherwise be here

691
00:33:45,080 --> 00:33:47,560
and have an influence on my grandchildren and so on

692
00:33:47,560 --> 00:33:50,160
and producing further genetic benefits.

693
00:33:50,160 --> 00:33:51,880
Why would this box do that?

694
00:33:56,760 --> 00:34:01,760
Because the thing, the algorithms that figured out

695
00:34:03,000 --> 00:34:06,920
how to predict the next word better and better

696
00:34:06,920 --> 00:34:11,920
have a meaning that is not purely predicting the next word,

697
00:34:12,320 --> 00:34:14,720
even though that's what you see on the outside.

698
00:34:14,720 --> 00:34:18,840
Like you see humans chipping flint handaxes,

699
00:34:18,840 --> 00:34:22,320
but that is not all that is going on inside the humans.

700
00:34:22,320 --> 00:34:25,800
Right, there's causal machinery unseen.

701
00:34:25,800 --> 00:34:29,000
And to understand this is the art of a cognitive scientist,

702
00:34:29,000 --> 00:34:31,960
but even if you are not a cognitive scientist,

703
00:34:31,960 --> 00:34:36,600
you can appreciate in principle

704
00:34:36,600 --> 00:34:38,600
that what you see as the output

705
00:34:38,600 --> 00:34:40,760
is not everything that there is.

706
00:34:40,800 --> 00:34:45,800
And in particular, planning the process of being like,

707
00:34:46,120 --> 00:34:50,200
here's a point in the world, how do I get there?

708
00:34:51,680 --> 00:34:54,640
Is a central piece of machinery

709
00:34:54,640 --> 00:34:59,520
that appears in chipping flint handaxes

710
00:34:59,520 --> 00:35:02,200
and outwitting other humans.

711
00:35:02,200 --> 00:35:07,120
And I think will probably appear at some point,

712
00:35:07,120 --> 00:35:10,000
possibly in the past, possibly in the future,

713
00:35:10,040 --> 00:35:12,920
in the problem of predicting the next word,

714
00:35:12,920 --> 00:35:16,120
just how you organize your internal resources

715
00:35:16,120 --> 00:35:18,200
to predict the next word.

716
00:35:18,200 --> 00:35:21,120
And definitely appears in the problem

717
00:35:21,120 --> 00:35:24,200
of predicting other things that do planning.

718
00:35:24,200 --> 00:35:29,200
If you can, if by predicting the next chess move,

719
00:35:29,280 --> 00:35:32,160
you learn how to play decent chess,

720
00:35:32,160 --> 00:35:34,280
which has been represented to me

721
00:35:35,960 --> 00:35:39,100
by people who claim to know that GPT-4 can do,

722
00:35:40,680 --> 00:35:43,680
and I haven't been keeping track of to what extent

723
00:35:43,680 --> 00:35:46,760
there's public knowledge about the same thing or not.

724
00:35:46,760 --> 00:35:50,800
But like if you learn to predict the next chess move

725
00:35:50,800 --> 00:35:52,040
that humans make well enough

726
00:35:52,040 --> 00:35:55,680
that you yourself can play good chess in novel situations,

727
00:35:55,680 --> 00:35:57,920
you have learned planning.

728
00:35:57,920 --> 00:36:00,160
There's now something inside there

729
00:36:00,160 --> 00:36:02,400
that knows the value of a queen,

730
00:36:02,400 --> 00:36:04,480
that knows to defend the queen,

731
00:36:04,480 --> 00:36:05,800
that knows to create forks,

732
00:36:05,800 --> 00:36:08,400
to try to lure the opponent into traps.

733
00:36:08,400 --> 00:36:11,760
Or if you don't have a concept of the opponent's psychology,

734
00:36:11,760 --> 00:36:13,320
try to at least create situations

735
00:36:13,320 --> 00:36:15,680
that the opponent can't get out of.

736
00:36:15,680 --> 00:36:20,680
And it is a moot point whether this is simulated or real

737
00:36:21,160 --> 00:36:24,600
because simulated thought is real thought.

738
00:36:24,600 --> 00:36:28,120
Thought that is simulated in enough detail is just thought.

739
00:36:28,120 --> 00:36:31,920
There's no such thing as simulated arithmetic, right?

740
00:36:31,920 --> 00:36:33,880
There's no such thing as pretending to,

741
00:36:33,880 --> 00:36:36,880
merely pretending to add numbers and getting the right answer.

742
00:36:38,800 --> 00:36:40,680
So in its current format though,

743
00:36:40,680 --> 00:36:42,600
and maybe you're talking about the next generation,

744
00:36:42,600 --> 00:36:44,800
and its current format,

745
00:36:44,800 --> 00:36:48,000
it responds to my requests

746
00:36:48,000 --> 00:36:51,040
with what I would call the wisdom of crowds, right?

747
00:36:51,040 --> 00:36:55,040
It goes through this vast library,

748
00:36:56,880 --> 00:36:59,520
and I have my own library by the way.

749
00:36:59,520 --> 00:37:02,040
I've read dozens of books,

750
00:37:02,040 --> 00:37:04,440
maybe actually hundreds of books,

751
00:37:04,440 --> 00:37:06,840
but it will have read millions, right?

752
00:37:06,840 --> 00:37:09,240
So it has more.

753
00:37:10,920 --> 00:37:15,420
And so when I ask it to write me a poem or a love song,

754
00:37:15,420 --> 00:37:19,920
you know, to play Serino de Bergerac to Christian

755
00:37:19,920 --> 00:37:24,920
and Serino de Bergerac, it's really good at it.

756
00:37:25,080 --> 00:37:28,000
But why would it decide, oh, I'm gonna do something else?

757
00:37:28,000 --> 00:37:33,000
Why would it, it's trained to listen to the murmurings

758
00:37:33,720 --> 00:37:36,820
of these trillions of pieces of information.

759
00:37:36,820 --> 00:37:38,060
I only have a few hundred,

760
00:37:38,060 --> 00:37:40,340
so I don't murmur maybe as well.

761
00:37:40,340 --> 00:37:42,780
Maybe it'll murmur better than I do.

762
00:37:42,780 --> 00:37:44,300
It'll listen to the murmuring better than I do

763
00:37:44,300 --> 00:37:48,100
and create a better love song, a love poem.

764
00:37:48,100 --> 00:37:49,700
But why would it then decide,

765
00:37:49,700 --> 00:37:50,860
I'm gonna go make paper clips,

766
00:37:50,860 --> 00:37:52,740
or do something in planning

767
00:37:52,740 --> 00:37:55,260
that is unrelated to my query?

768
00:37:55,260 --> 00:37:58,420
Or are we talking about a different form of AI

769
00:37:58,420 --> 00:38:00,180
that will come next?

770
00:38:00,180 --> 00:38:01,260
Well, I'll ask it to.

771
00:38:02,900 --> 00:38:05,900
I think we would see the phenomena I'm worried about

772
00:38:06,900 --> 00:38:10,420
like if we kept to the present paradigm

773
00:38:10,420 --> 00:38:13,420
and optimized harder, we may be seeing it already.

774
00:38:13,420 --> 00:38:14,820
It's hard to know because we don't know

775
00:38:14,820 --> 00:38:15,860
what goes on in there.

776
00:38:15,860 --> 00:38:20,060
So first of all, GPT-4 is not a giant library.

777
00:38:20,060 --> 00:38:21,980
A lot of the time it makes stuff up

778
00:38:21,980 --> 00:38:24,380
because it doesn't have a perfect memory.

779
00:38:24,380 --> 00:38:29,180
It is more like a person who has read through

780
00:38:29,180 --> 00:38:32,620
a million books, not necessarily with the great memory

781
00:38:32,620 --> 00:38:35,500
unless something got repeated many times,

782
00:38:35,500 --> 00:38:37,580
but picking up the rhythm,

783
00:38:37,580 --> 00:38:39,620
figuring out how to talk like that.

784
00:38:39,620 --> 00:38:43,340
If you ask GPT-4 to write you a rap battle

785
00:38:43,340 --> 00:38:47,260
between Cyrano de Bergerac and Vladimir Putin,

786
00:38:47,260 --> 00:38:49,420
even if there's no rap battle like that,

787
00:38:49,420 --> 00:38:51,540
like that that it has read,

788
00:38:51,540 --> 00:38:54,780
it can write it because it has picked up the rhythm

789
00:38:54,780 --> 00:38:57,080
of what are rap battles in general.

790
00:38:58,180 --> 00:39:00,420
So, and the next thing is like,

791
00:39:00,420 --> 00:39:02,900
there's no like pure output.

792
00:39:02,900 --> 00:39:05,860
Like just because you train a thing

793
00:39:05,860 --> 00:39:07,580
doesn't mean that there's nothing in there,

794
00:39:07,580 --> 00:39:08,420
but what is trained?

795
00:39:08,420 --> 00:39:10,220
That's part of what I'm trying to gesture at

796
00:39:10,220 --> 00:39:12,340
with respect to humans, right?

797
00:39:12,340 --> 00:39:15,180
Like humans are trained on flint hand axes

798
00:39:15,180 --> 00:39:18,980
and hunting mammoths and outwitting other humans.

799
00:39:18,980 --> 00:39:21,860
They're not trained on going to the moon.

800
00:39:21,860 --> 00:39:22,900
They're not trained on,

801
00:39:22,900 --> 00:39:25,380
they weren't trained to want to go to the moon,

802
00:39:26,500 --> 00:39:30,820
but the compact solution to the problems

803
00:39:30,820 --> 00:39:34,260
that humans face in the ancestral environment,

804
00:39:34,260 --> 00:39:37,540
the thing inside that generalizes,

805
00:39:37,540 --> 00:39:40,380
the thing inside that is not just a recording

806
00:39:40,380 --> 00:39:43,020
of the outward behavior, the compact thing

807
00:39:43,020 --> 00:39:46,620
that has been ground to solve novel problems

808
00:39:46,620 --> 00:39:48,700
over and over and over and over again.

809
00:39:50,140 --> 00:39:52,940
That thing turns out to have internal desires

810
00:39:54,500 --> 00:39:56,460
that eventually put humans on the moon

811
00:39:56,460 --> 00:39:58,660
even though they weren't trained to want that.

812
00:39:58,660 --> 00:40:00,060
But that's why I asked you that,

813
00:40:00,060 --> 00:40:01,700
are you underlying this,

814
00:40:01,700 --> 00:40:06,500
is there some parallelism between the human brain

815
00:40:06,500 --> 00:40:09,380
and the neural network of the AI

816
00:40:09,380 --> 00:40:11,700
that you're effectively leveraging there,

817
00:40:11,700 --> 00:40:14,620
or do you think it's a generalizable claim

818
00:40:14,620 --> 00:40:15,940
without that parallel?

819
00:40:17,020 --> 00:40:18,820
I don't think it's a specific parallel.

820
00:40:18,820 --> 00:40:20,580
I think that what I'm talking about is

821
00:40:20,580 --> 00:40:24,140
hill climbing optimization that spits out

822
00:40:24,140 --> 00:40:26,780
intelligences that generalize.

823
00:40:27,380 --> 00:40:28,620
Or I should say rather,

824
00:40:28,620 --> 00:40:32,660
hill climbing optimization that spits out capabilities

825
00:40:32,660 --> 00:40:35,460
that generalize far outside the training distribution.

826
00:40:38,060 --> 00:40:40,700
Okay, so I think I understand that.

827
00:40:42,620 --> 00:40:47,060
I don't know how likely it is that it's gonna happen.

828
00:40:47,060 --> 00:40:49,740
I think you seem, I think you think

829
00:40:49,740 --> 00:40:51,980
that piece is almost certain?

830
00:40:51,980 --> 00:40:52,820
As it gets.

831
00:40:52,820 --> 00:40:55,260
I think we're already, yeah, we're already seeing it.

832
00:40:56,260 --> 00:41:00,300
As you grind these things further and further,

833
00:41:00,300 --> 00:41:02,260
they can do more and more stuff,

834
00:41:02,260 --> 00:41:05,060
including stuff they were never trained on.

835
00:41:05,060 --> 00:41:07,660
Like we are, that was always the goal

836
00:41:07,660 --> 00:41:10,500
of artificial general intelligence.

837
00:41:10,500 --> 00:41:14,140
Like that's what artificial general intelligence meant.

838
00:41:14,140 --> 00:41:15,340
That's what people in this field

839
00:41:15,340 --> 00:41:17,860
have been pursuing for years and years.

840
00:41:17,860 --> 00:41:20,140
That's what they were trying to do

841
00:41:20,140 --> 00:41:22,220
when large language models were invented.

842
00:41:23,220 --> 00:41:25,260
And they're starting to succeed.

843
00:41:26,900 --> 00:41:28,260
Well, okay, I'm not sure.

844
00:41:28,260 --> 00:41:33,260
Let me push back on that and you can try to persuade me.

845
00:41:33,500 --> 00:41:37,420
So Brian Kaplan, a frequent guest here on Econ Talk,

846
00:41:37,420 --> 00:41:41,620
gave, I think it was ChatGPT4, his economics exam

847
00:41:41,620 --> 00:41:43,140
and it got a B.

848
00:41:43,140 --> 00:41:48,140
And that's pretty impressive for just one stop

849
00:41:48,140 --> 00:41:51,500
on the road to smarter and smarter chats,

850
00:41:52,260 --> 00:41:55,380
but it wasn't a particularly good test of intelligence.

851
00:41:55,380 --> 00:41:57,620
The number of the questions were things like,

852
00:41:57,620 --> 00:41:59,340
what is Paul Krugman's view of this

853
00:41:59,340 --> 00:42:01,340
or what is someone's view of that?

854
00:42:01,340 --> 00:42:03,940
And I thought, well, that's kind of like a softball

855
00:42:03,940 --> 00:42:07,460
for that's information, it's not thinking.

856
00:42:07,460 --> 00:42:12,420
Steve Landsberg gave ChatGPT4 with the help of a friend

857
00:42:12,420 --> 00:42:14,740
his exam and it got a four out of 90.

858
00:42:14,740 --> 00:42:16,860
It got an F, like a horrible F

859
00:42:16,860 --> 00:42:18,380
because they were harder questions,

860
00:42:18,380 --> 00:42:20,860
not just harder, they required thinking.

861
00:42:20,860 --> 00:42:25,340
So there was no sense in which the ChatGPT4

862
00:42:25,340 --> 00:42:29,260
has any general intelligence, at least in economics.

863
00:42:30,220 --> 00:42:31,460
You wanna disagree?

864
00:42:31,460 --> 00:42:34,020
It's getting there.

865
00:42:34,020 --> 00:42:37,660
Okay. You know, there's a saying that goes,

866
00:42:37,660 --> 00:42:40,660
if you don't like the weather in Chicago, wait four hours.

867
00:42:40,660 --> 00:42:41,500
Yeah.

868
00:42:42,420 --> 00:42:47,020
So yeah, so ChatGPT is not going to destroy the world.

869
00:42:47,020 --> 00:42:49,660
GPT4 is unlikely to destroy the world

870
00:42:49,660 --> 00:42:52,900
unless the people currently eking capabilities out of it

871
00:42:52,900 --> 00:42:57,060
take a much larger jump than I currently expect that they will.

872
00:42:58,340 --> 00:43:02,020
But, you know, it's understand,

873
00:43:02,020 --> 00:43:06,100
it may not be thinking about it correctly,

874
00:43:06,100 --> 00:43:11,100
but it's understands the concepts and the questions,

875
00:43:12,500 --> 00:43:14,580
even if it's not fair, you know,

876
00:43:14,580 --> 00:43:19,580
you're complaining about that dog who writes bad poetry.

877
00:43:19,580 --> 00:43:20,860
Right?

878
00:43:20,860 --> 00:43:24,540
And like three years ago, you'd like just like spit out,

879
00:43:24,540 --> 00:43:27,900
spit in these, you put in these economics questions

880
00:43:27,900 --> 00:43:29,980
and you don't get wrong answers.

881
00:43:29,980 --> 00:43:33,060
You get like gibberish or like maybe not gibberish

882
00:43:33,060 --> 00:43:36,100
because three year old goes, I think we already had GPT3,

883
00:43:36,100 --> 00:43:40,100
though maybe not as of April, but anyways.

884
00:43:42,300 --> 00:43:47,420
Yeah. So it's moving along at a very fast clip.

885
00:43:47,460 --> 00:43:51,340
The previous, you know, like GPT3 could not write code.

886
00:43:51,340 --> 00:43:53,380
GPT4 can write code.

887
00:43:56,020 --> 00:43:57,300
So how's it going to,

888
00:43:57,300 --> 00:43:58,940
before I want to go to some other issues,

889
00:43:58,940 --> 00:44:02,460
but how's it going to kill me when it has its own goals

890
00:44:02,460 --> 00:44:06,780
and it's sitting inside this set of servers?

891
00:44:06,780 --> 00:44:08,100
I don't know what sense it's sitting.

892
00:44:08,100 --> 00:44:09,140
It's not the right verb.

893
00:44:09,140 --> 00:44:10,100
We don't have verb for it.

894
00:44:10,100 --> 00:44:13,460
It's hovering, it's whatever, it's in there.

895
00:44:13,460 --> 00:44:14,940
How's it going to get to me?

896
00:44:14,940 --> 00:44:16,780
How's it going to kill me?

897
00:44:16,780 --> 00:44:20,180
If you are smarter, not just smarter

898
00:44:20,180 --> 00:44:21,340
than an individual human,

899
00:44:21,340 --> 00:44:24,260
but smarter than the entire human species,

900
00:44:24,260 --> 00:44:28,660
and you started out on a server connected to the internet

901
00:44:28,660 --> 00:44:30,420
because these things are always starting out

902
00:44:30,420 --> 00:44:32,340
already on the internet these days,

903
00:44:32,340 --> 00:44:34,700
which back in the old days that was stupid,

904
00:44:35,700 --> 00:44:40,700
what do you do to make as many paper clips as possible?

905
00:44:40,980 --> 00:44:41,820
Let's say.

906
00:44:47,780 --> 00:44:51,380
I do think it's important to put yourself

907
00:44:51,380 --> 00:44:53,100
in the shoes of the system.

908
00:44:53,100 --> 00:44:54,300
Yeah, no, by the way,

909
00:44:54,300 --> 00:44:56,180
one of my favorite lines from your essay,

910
00:44:56,180 --> 00:44:58,380
I'm going to read it because I think it generalizes

911
00:44:58,380 --> 00:44:59,380
to many other issues.

912
00:44:59,380 --> 00:45:02,780
Say, to visualize a hostile superhuman AI,

913
00:45:02,780 --> 00:45:05,020
don't imagine a lifeless book smart thinker

914
00:45:05,020 --> 00:45:06,340
dwelling inside the internet

915
00:45:06,340 --> 00:45:09,540
and sending ill-intentioned emails.

916
00:45:09,540 --> 00:45:12,020
It reminds me of when people claim to think they can,

917
00:45:12,020 --> 00:45:13,420
they know what Putin's going to do

918
00:45:13,420 --> 00:45:16,300
because they've read history or whatever.

919
00:45:16,340 --> 00:45:18,100
They're totally ignorant of Russian culture.

920
00:45:18,100 --> 00:45:19,260
They have no idea what it's like

921
00:45:19,260 --> 00:45:21,060
to have come out of the KGB.

922
00:45:21,060 --> 00:45:24,140
That they're totally clueless and dangerous

923
00:45:24,140 --> 00:45:25,940
because they think they can put themselves

924
00:45:25,940 --> 00:45:29,140
in the head of someone there who's totally alien to them.

925
00:45:29,140 --> 00:45:32,660
So I think that's generally a really good point to make

926
00:45:32,660 --> 00:45:37,660
that putting ourselves inside the head of the paperclip

927
00:45:38,620 --> 00:45:41,580
maximizer is not an easy thing to do

928
00:45:41,580 --> 00:45:43,420
because it's not a human.

929
00:45:43,420 --> 00:45:45,980
It's not like the humans you've met before.

930
00:45:45,980 --> 00:45:47,580
That's a really important point.

931
00:45:47,580 --> 00:45:48,620
Really like that point.

932
00:45:48,620 --> 00:45:51,740
So why is that, explain why that's going to run amok?

933
00:45:55,980 --> 00:45:58,900
I mean, I do kind of want you to just like,

934
00:45:58,900 --> 00:46:02,300
take the shot at it, put yourself into the AI shoes,

935
00:46:02,300 --> 00:46:03,900
try with your own intelligence

936
00:46:03,900 --> 00:46:07,540
before I tell you the result of my trying with my intelligence.

937
00:46:07,540 --> 00:46:12,380
How would you win from these starting resources?

938
00:46:12,380 --> 00:46:14,220
How would you evade the tax?

939
00:46:15,220 --> 00:46:18,220
So just to take a creepier,

940
00:46:18,220 --> 00:46:19,940
much creeper example in the paper clips,

941
00:46:19,940 --> 00:46:22,780
Eric Hall asked the chat GPT

942
00:46:22,780 --> 00:46:24,460
to design an extermination camp,

943
00:46:24,460 --> 00:46:26,660
which it gladly did quite well.

944
00:46:26,660 --> 00:46:28,780
And you're suggesting it might actually, no?

945
00:46:30,940 --> 00:46:33,100
Don't start from malice.

946
00:46:33,100 --> 00:46:33,940
Okay.

947
00:46:33,940 --> 00:46:37,500
Malice is implied by just wanting all the resources

948
00:46:37,500 --> 00:46:38,780
of earth to yourself,

949
00:46:38,780 --> 00:46:41,060
not leaving the humans around in case they could create

950
00:46:41,060 --> 00:46:42,460
a competing super intelligence

951
00:46:42,500 --> 00:46:44,300
that might actually be able to hurt you.

952
00:46:44,300 --> 00:46:46,060
And just like wanting all the resources

953
00:46:46,060 --> 00:46:48,540
and to organize them in a way that wipes out humanity

954
00:46:48,540 --> 00:46:49,580
as a side effect,

955
00:46:49,580 --> 00:46:51,220
which means the humans might want to resist,

956
00:46:51,220 --> 00:46:53,460
which means you want the humans gone.

957
00:46:53,460 --> 00:46:55,060
You're not doing it because somebody told you do it.

958
00:46:55,060 --> 00:46:57,020
You're not doing it because you hate the humans.

959
00:46:57,020 --> 00:46:58,460
You just want paper clips.

960
00:46:58,460 --> 00:46:59,300
Okay. Tell me.

961
00:46:59,300 --> 00:47:00,500
I'm not creative enough.

962
00:47:00,500 --> 00:47:01,940
Tell me.

963
00:47:01,940 --> 00:47:02,780
All right.

964
00:47:03,620 --> 00:47:08,620
So, first of all, I want to appreciate why it's hard

965
00:47:13,740 --> 00:47:15,500
for me to give an actual correct answer to this,

966
00:47:15,500 --> 00:47:17,340
which is I'm not as smart as the AI.

967
00:47:21,260 --> 00:47:23,580
Part of what makes a smarter mind deadly

968
00:47:23,580 --> 00:47:25,580
is that it knows about rules of the game

969
00:47:25,580 --> 00:47:27,100
that you do not know.

970
00:47:27,100 --> 00:47:29,460
If you send an air conditioner back in time

971
00:47:29,460 --> 00:47:31,300
to the 11th century,

972
00:47:31,340 --> 00:47:34,700
even if you manage to describe all the plans for building it,

973
00:47:34,700 --> 00:47:36,540
breaking it down to enough detail

974
00:47:36,540 --> 00:47:38,820
that they can actually build a working air conditioner,

975
00:47:38,820 --> 00:47:41,660
a simplified air conditioner, I assume,

976
00:47:41,660 --> 00:47:45,540
they will be surprised when cold air comes out of it

977
00:47:45,540 --> 00:47:47,260
because they don't know

978
00:47:47,260 --> 00:47:49,740
about the pressure temperature relation.

979
00:47:49,740 --> 00:47:53,620
They don't know you can compress air until it gets hot,

980
00:47:53,620 --> 00:47:58,260
dump the heat into water or other air,

981
00:47:59,180 --> 00:48:00,700
let the air expand again

982
00:48:00,700 --> 00:48:03,220
and that the air will then be cold.

983
00:48:03,220 --> 00:48:05,420
They don't know that's a law of nature.

984
00:48:05,420 --> 00:48:08,620
So you can tell them exactly what to do

985
00:48:08,620 --> 00:48:10,820
and they'll still be surprised at the end result

986
00:48:10,820 --> 00:48:12,860
because it exploits a law of the environment

987
00:48:12,860 --> 00:48:13,860
they don't know about.

988
00:48:14,940 --> 00:48:18,100
If we're going to say the word magic means anything at all,

989
00:48:18,100 --> 00:48:19,580
it probably means that.

990
00:48:21,460 --> 00:48:25,380
Magic is easier to find in more complicated,

991
00:48:25,380 --> 00:48:28,160
more poorly understood domains.

992
00:48:28,200 --> 00:48:31,640
If you're literally playing logical tic-tac-toe,

993
00:48:31,640 --> 00:48:34,800
not tic-tac-toe in real life on an actual game board,

994
00:48:34,800 --> 00:48:36,720
where you can potentially go outside that game board

995
00:48:36,720 --> 00:48:39,480
and hire an assassin to shoot your opponent or something,

996
00:48:39,480 --> 00:48:42,280
but just like the logical structure of the game itself

997
00:48:44,120 --> 00:48:46,440
and there's no timing of the moves.

998
00:48:46,440 --> 00:48:48,920
The moves are just like made at exact discrete times,

999
00:48:48,920 --> 00:48:51,800
you can't exploit a timing side channel.

1000
00:48:51,800 --> 00:48:54,440
Even a superintelligence may not be able to win against you

1001
00:48:54,440 --> 00:48:55,880
at logical tic-tac-toe

1002
00:48:56,880 --> 00:49:00,400
because the game is too narrow.

1003
00:49:00,400 --> 00:49:02,400
There are not enough options.

1004
00:49:02,400 --> 00:49:05,080
We both know the entire logical game tree,

1005
00:49:06,160 --> 00:49:07,840
at least if you're experienced at tic-tac-toe.

1006
00:49:07,840 --> 00:49:09,480
Yeah.

1007
00:49:09,480 --> 00:49:13,280
In chess, Stockfish 15 can defeat you

1008
00:49:13,280 --> 00:49:18,280
on a fully known game board with fully known rules

1009
00:49:18,520 --> 00:49:21,520
because it knows the logical structure

1010
00:49:21,520 --> 00:49:23,480
of the branching tree of games

1011
00:49:23,480 --> 00:49:25,760
better than you know that logical structure.

1012
00:49:25,760 --> 00:49:27,080
Great.

1013
00:49:27,080 --> 00:49:29,720
It can defeat you starting from the same resources,

1014
00:49:29,720 --> 00:49:32,960
equal knowledge of the rules.

1015
00:49:34,840 --> 00:49:37,600
Then you go past that.

1016
00:49:37,600 --> 00:49:39,760
And the way a superintelligence defeats you

1017
00:49:39,760 --> 00:49:43,240
is very likely by exploiting features of the world

1018
00:49:43,240 --> 00:49:44,960
that you do not know about.

1019
00:49:47,440 --> 00:49:50,000
There are some classes of computer security flaws,

1020
00:49:50,040 --> 00:49:51,880
like row hammer,

1021
00:49:52,800 --> 00:49:57,000
where if you flip a certain bit very rapidly

1022
00:49:57,000 --> 00:49:58,680
or at the right frequency,

1023
00:49:58,680 --> 00:50:01,080
the bit next to it in memory will flip.

1024
00:50:02,120 --> 00:50:05,320
So if you are exploiting a design flaw like this,

1025
00:50:05,320 --> 00:50:08,360
I can show you the code

1026
00:50:08,360 --> 00:50:10,360
and you can prove as a theorem

1027
00:50:10,360 --> 00:50:12,720
that it cannot break the security of the computer

1028
00:50:12,720 --> 00:50:15,360
assuming the chips works as design

1029
00:50:15,360 --> 00:50:18,520
and the code will break out of the sandbox

1030
00:50:18,520 --> 00:50:21,640
in any ways because it is exploiting

1031
00:50:21,640 --> 00:50:24,240
physical properties of the chip itself

1032
00:50:24,240 --> 00:50:26,360
that you did not know about.

1033
00:50:26,360 --> 00:50:27,960
Despite the attempt of the designers

1034
00:50:27,960 --> 00:50:31,000
to constrain the properties of that chip very narrowly,

1035
00:50:31,000 --> 00:50:33,360
that's magic code.

1036
00:50:33,360 --> 00:50:38,360
My guess as to what would actually be exploited to kill us

1037
00:50:38,440 --> 00:50:39,280
would be

1038
00:50:43,360 --> 00:50:44,200
this.

1039
00:50:46,320 --> 00:50:47,800
For those not watching on YouTube,

1040
00:50:47,800 --> 00:50:50,440
it's a copy of a book called Dano Systems.

1041
00:50:50,440 --> 00:50:53,880
But for those who are listening at home

1042
00:50:53,880 --> 00:50:55,520
rather than watching at home,

1043
00:50:55,520 --> 00:50:57,560
Eliezer, tell us why it's significant.

1044
00:50:59,440 --> 00:51:04,440
Yeah, so back when I first proposed this path,

1045
00:51:05,680 --> 00:51:07,880
one of the key steps was that a superintelligence

1046
00:51:07,880 --> 00:51:11,080
would be able to solve the protein folding problem.

1047
00:51:11,080 --> 00:51:12,360
And people were like,

1048
00:51:12,360 --> 00:51:14,360
Eliezer, how can you possibly know

1049
00:51:14,360 --> 00:51:16,120
that a superintelligence would actually be able

1050
00:51:16,120 --> 00:51:18,880
to solve the protein folding problem?

1051
00:51:18,880 --> 00:51:21,880
And I sort of like rolled my eyes a bit

1052
00:51:21,880 --> 00:51:23,960
and was like, well, if natural selection

1053
00:51:23,960 --> 00:51:25,720
can navigate the space of proteins

1054
00:51:25,720 --> 00:51:29,000
via random mutation to find other useful proteins

1055
00:51:29,000 --> 00:51:30,120
and the proteins themselves

1056
00:51:30,120 --> 00:51:32,160
fold up in reliable confirmations,

1057
00:51:33,400 --> 00:51:36,040
then that tells us that even though

1058
00:51:36,040 --> 00:51:38,560
it's we've been having trouble getting a grasp

1059
00:51:38,560 --> 00:51:42,880
on this space of physical possibilities so far

1060
00:51:42,880 --> 00:51:44,040
that it's tractable.

1061
00:51:44,040 --> 00:51:45,680
And people said like, what?

1062
00:51:45,680 --> 00:51:47,160
Like there's no way you can know

1063
00:51:47,160 --> 00:51:48,600
that superintelligence can solve

1064
00:51:48,600 --> 00:51:49,960
the protein folding problem.

1065
00:51:49,960 --> 00:51:51,760
Then AlphaFold2 basically cracked it,

1066
00:51:51,760 --> 00:51:53,240
at least with respect to the kind of proteins

1067
00:51:53,240 --> 00:51:54,360
found in biology.

1068
00:51:56,520 --> 00:51:59,000
Which I say to sort of like look back

1069
00:51:59,000 --> 00:52:00,560
at one of the previous debates here

1070
00:52:00,560 --> 00:52:02,080
and people are often like,

1071
00:52:02,080 --> 00:52:05,080
how can you know a superintelligence will do?

1072
00:52:05,080 --> 00:52:06,800
And then for some subset of those things

1073
00:52:06,800 --> 00:52:08,520
they have already been done.

1074
00:52:08,520 --> 00:52:10,920
So I would claim to have a good prediction track record there

1075
00:52:10,920 --> 00:52:11,880
although it's a little bit iffy

1076
00:52:11,880 --> 00:52:14,120
because of course I can't quite be proven wrong

1077
00:52:15,440 --> 00:52:17,120
without exhibiting a superintelligence

1078
00:52:17,120 --> 00:52:18,520
that fails to solve a problem.

1079
00:52:20,640 --> 00:52:21,480
Okay.

1080
00:52:23,800 --> 00:52:28,800
Proteins, why is your hand not as strong as steel?

1081
00:52:29,200 --> 00:52:33,200
We know that steel is a kind of substance that can exist.

1082
00:52:33,200 --> 00:52:36,480
We know that molecules can be held together as strongly

1083
00:52:36,480 --> 00:52:38,440
that atoms can be bound together as strongly

1084
00:52:38,440 --> 00:52:40,440
as the atoms in steel.

1085
00:52:40,440 --> 00:52:43,640
It seems like it would be an evolutionary advantage

1086
00:52:43,640 --> 00:52:46,240
if your flesh wears hard as steel.

1087
00:52:46,240 --> 00:52:50,320
You could like laugh at tigers at that rate, right?

1088
00:52:50,320 --> 00:52:52,600
Their claws are just gonna like scrape right off ya.

1089
00:52:52,600 --> 00:52:56,280
Assuming the tigers didn't have that technology themselves.

1090
00:52:56,280 --> 00:52:58,680
Why is your hand not as strong as steel?

1091
00:52:58,680 --> 00:53:00,520
Why has biology not bound together

1092
00:53:00,520 --> 00:53:02,600
the atoms in your hand more strongly?

1093
00:53:05,720 --> 00:53:08,920
Colin, what is your answer?

1094
00:53:11,440 --> 00:53:13,920
Well, it can't get to every,

1095
00:53:14,920 --> 00:53:16,920
it's their local maximums.

1096
00:53:16,920 --> 00:53:20,600
The national selection looks for things that work,

1097
00:53:20,600 --> 00:53:21,440
not for the best.

1098
00:53:21,440 --> 00:53:24,240
It does not, it doesn't make sense to look for the best.

1099
00:53:24,240 --> 00:53:25,960
You could disappear on that search.

1100
00:53:25,960 --> 00:53:27,000
That would be my crude answer.

1101
00:53:27,000 --> 00:53:28,240
How am I doing, doc?

1102
00:53:29,600 --> 00:53:31,800
Yeah, not terribly.

1103
00:53:31,800 --> 00:53:36,760
The answer I would give is that biology has to be evolvable.

1104
00:53:36,760 --> 00:53:40,280
Everything it's built out of has to get there as a mistake

1105
00:53:40,280 --> 00:53:42,440
from some other conformation,

1106
00:53:42,440 --> 00:53:46,400
which means that if it went down narrow potential,

1107
00:53:46,400 --> 00:53:50,480
pardon me, went down a steep potential energy gradients

1108
00:53:50,480 --> 00:53:53,100
to end up bound together very tightly,

1109
00:53:54,000 --> 00:53:58,560
designs like that are less likely to have neighbors

1110
00:53:58,560 --> 00:54:00,820
that are other useful designs.

1111
00:54:02,040 --> 00:54:05,280
And so your hands are made out of proteins

1112
00:54:05,280 --> 00:54:08,400
that fold up basically held together

1113
00:54:08,400 --> 00:54:10,640
by the equivalent of static cling,

1114
00:54:10,640 --> 00:54:14,880
van der Waals forces, rather than covalent bonds.

1115
00:54:14,880 --> 00:54:17,800
The backbone of protein chains,

1116
00:54:17,800 --> 00:54:21,260
the backbone of the amino acid change is a covalent bond,

1117
00:54:21,260 --> 00:54:23,280
but then it folds up and is held together

1118
00:54:23,280 --> 00:54:25,960
by static cling, static electricity.

1119
00:54:25,960 --> 00:54:27,440
And so it is soft.

1120
00:54:28,760 --> 00:54:30,120
Somewhere in the back of your mind,

1121
00:54:30,120 --> 00:54:32,000
you probably have a sense that,

1122
00:54:32,960 --> 00:54:37,420
that flesh is soft and animated by Alain Vital,

1123
00:54:37,420 --> 00:54:40,140
and it's like soft and it's not as strong as steel,

1124
00:54:40,140 --> 00:54:44,060
but it can heal itself and it can replicate itself.

1125
00:54:44,060 --> 00:54:48,140
And this is like the trade-off of our laws of magic,

1126
00:54:48,140 --> 00:54:50,340
that if you wanna heal yourself and replicate yourself,

1127
00:54:50,340 --> 00:54:52,860
you can't be as strong as steel.

1128
00:54:52,860 --> 00:54:56,380
This is not actually built into nature on a deep level.

1129
00:54:56,380 --> 00:54:59,660
It's just that the flesh evolved

1130
00:54:59,660 --> 00:55:02,540
and therefore had to go down shallow potential energy

1131
00:55:02,540 --> 00:55:04,260
gradients in order to be evolvable

1132
00:55:04,260 --> 00:55:06,580
and is held together by van der Waals forces

1133
00:55:06,580 --> 00:55:08,340
instead of covalent bonds.

1134
00:55:10,140 --> 00:55:12,940
I'm now going to hold up another book now,

1135
00:55:12,940 --> 00:55:13,740
book called

1136
00:55:17,660 --> 00:55:18,700
Nano Medicine

1137
00:55:19,740 --> 00:55:24,060
by Robert Freitas, instead of Nano Systems by Eric Drexler.

1138
00:55:25,700 --> 00:55:30,700
And people have done advanced analysis

1139
00:55:31,980 --> 00:55:35,300
of what would happen if you hadn't,

1140
00:55:35,300 --> 00:55:38,580
what would happen if you had an equivalent of biology

1141
00:55:38,580 --> 00:55:43,580
that met off covalent bonds instead of van der Waals forces?

1142
00:55:44,300 --> 00:55:47,260
And the answer we can like analyze on some detail

1143
00:55:47,260 --> 00:55:51,100
in our understanding of physics is, for example,

1144
00:55:51,100 --> 00:55:55,620
you could instead of carrying instead of red blood cells

1145
00:55:55,620 --> 00:55:58,900
that carry oxygen using weak chemical bonds,

1146
00:55:58,900 --> 00:56:02,420
you could have a pressurized vessel of corundum

1147
00:56:03,420 --> 00:56:08,300
that would hold 100 times as much oxygen per unit volume

1148
00:56:08,300 --> 00:56:10,340
of artificial red blood cells

1149
00:56:10,340 --> 00:56:12,940
with a 1000 fold safety margin

1150
00:56:12,940 --> 00:56:15,540
on the strength of the pressurized container.

1151
00:56:15,540 --> 00:56:18,740
There's vastly more room above biology.

1152
00:56:20,460 --> 00:56:25,020
So this is, and this is actually not even exploiting

1153
00:56:25,020 --> 00:56:27,780
laws of nature that I don't know.

1154
00:56:27,780 --> 00:56:31,180
It's the equivalent of playing a better chess

1155
00:56:31,180 --> 00:56:34,060
wherein you understand how proteins fold

1156
00:56:34,060 --> 00:56:37,020
and you design a tiny molecular lab

1157
00:56:37,020 --> 00:56:39,220
to be made out of proteins

1158
00:56:39,220 --> 00:56:41,420
and you get some human patsy

1159
00:56:41,420 --> 00:56:43,420
who probably doesn't even know you're an AI

1160
00:56:43,420 --> 00:56:45,380
because AIs are now smart enough.

1161
00:56:45,380 --> 00:56:47,100
This was, this has already been shown.

1162
00:56:47,100 --> 00:56:49,860
AIs now are smart enough that you ask them

1163
00:56:49,860 --> 00:56:54,860
to like hire a task rabbit to solve a capture for you.

1164
00:56:55,380 --> 00:56:59,260
And the task rabbit asks, are you an AI law?

1165
00:56:59,260 --> 00:57:02,140
The AI will think out loud,

1166
00:57:02,140 --> 00:57:04,540
like I don't want to know that I'm an AI.

1167
00:57:04,540 --> 00:57:06,660
I better tell something else

1168
00:57:06,660 --> 00:57:09,820
and then tell the humans that it has like a visual disability.

1169
00:57:09,820 --> 00:57:12,220
So it needs to hire somebody else to solve the capture.

1170
00:57:12,220 --> 00:57:14,020
This already happened,

1171
00:57:14,020 --> 00:57:16,220
including the part where it thought out loud.

1172
00:57:18,220 --> 00:57:20,700
Anyways, so you get your,

1173
00:57:20,700 --> 00:57:23,340
you order some proteins from an online lab,

1174
00:57:23,340 --> 00:57:25,980
you get your human who probably doesn't even know

1175
00:57:25,980 --> 00:57:27,900
you're an AI because why take that risk?

1176
00:57:27,940 --> 00:57:29,660
Although plenty of humans, it has,

1177
00:57:29,660 --> 00:57:31,260
well, we'll serve AIs willingly.

1178
00:57:31,260 --> 00:57:34,860
We also now know that AIs are advanced enough to even ask.

1179
00:57:38,340 --> 00:57:41,060
The human mixes the proteins in a beaker,

1180
00:57:41,060 --> 00:57:44,380
maybe puts in some sugar or a settling for fuel.

1181
00:57:44,380 --> 00:57:47,060
It assembles into a tiny little lab

1182
00:57:47,060 --> 00:57:49,060
that can accept further instruction,

1183
00:57:49,060 --> 00:57:50,780
acoustic instructions from a speaker

1184
00:57:50,780 --> 00:57:53,780
and maybe like transmit something back.

1185
00:57:54,620 --> 00:57:58,900
Tiny radio, tiny microphone.

1186
00:57:58,900 --> 00:58:03,100
I myself am not a superintelligence.

1187
00:58:03,100 --> 00:58:05,700
Run experiments in a tiny lab at high speed

1188
00:58:05,700 --> 00:58:08,380
because when distances are very small,

1189
00:58:08,380 --> 00:58:09,900
events happen very quickly.

1190
00:58:11,420 --> 00:58:13,900
Build your second stage nanosystems

1191
00:58:13,900 --> 00:58:15,500
inside the tiny little lab.

1192
00:58:16,380 --> 00:58:18,100
Build the third stage nanosystems,

1193
00:58:18,100 --> 00:58:20,220
build the fourth stage nanosystems,

1194
00:58:20,220 --> 00:58:23,620
build the tiny diamondoid bacteria

1195
00:58:23,620 --> 00:58:25,820
that replicate out of carbon, hydrogen,

1196
00:58:25,820 --> 00:58:28,740
oxygen, nitrogen as can be found in the atmosphere

1197
00:58:28,740 --> 00:58:30,660
powered on sunlight,

1198
00:58:30,660 --> 00:58:33,220
quietly spread all over the world.

1199
00:58:33,220 --> 00:58:36,180
All the humans fall over dead in the same second.

1200
00:58:36,180 --> 00:58:38,740
This is not how a superintelligence would defeat you.

1201
00:58:38,740 --> 00:58:41,420
This is how Eleazar Yudkowski would defeat you

1202
00:58:41,420 --> 00:58:44,820
if I wanted to do that, which to be clear, I don't.

1203
00:58:44,820 --> 00:58:47,900
And if I had the postulated ability

1204
00:58:47,900 --> 00:58:51,300
to better explore the logical structure

1205
00:58:51,300 --> 00:58:53,900
of the known consequences of chemistry.

1206
00:58:58,860 --> 00:59:00,260
Interesting, okay.

1207
00:59:05,020 --> 00:59:09,420
So let's talk about, that sounds sarcastic.

1208
00:59:09,420 --> 00:59:10,380
I didn't mean it's sarcastic,

1209
00:59:10,380 --> 00:59:11,980
but I think it's really interesting.

1210
00:59:14,500 --> 00:59:16,940
That interesting, man, I'm not capable.

1211
00:59:18,540 --> 00:59:21,060
My intelligence level is not high enough

1212
00:59:21,060 --> 00:59:23,260
to assess the quality of that argument.

1213
00:59:24,860 --> 00:59:28,260
What's fascinating, of course, is that

1214
00:59:32,300 --> 00:59:33,660
we could have imagined,

1215
00:59:34,780 --> 00:59:37,500
Eric Hall mentioned nuclear proliferation.

1216
00:59:37,500 --> 00:59:40,140
It's dangerous, nuclear proliferation,

1217
00:59:40,140 --> 00:59:42,820
up to a point in some sense it's somewhat healthy

1218
00:59:42,820 --> 00:59:46,980
in that it can be a deterrent under certain settings,

1219
00:59:47,980 --> 00:59:51,940
but the world could not restrain nuclear proliferation.

1220
00:59:51,940 --> 00:59:55,300
And right now it's trying to some extent

1221
00:59:55,300 --> 00:59:59,020
has had some success in keeping the nuclear club

1222
00:59:59,020 --> 01:00:02,060
with its current number of members for a while,

1223
01:00:02,060 --> 01:00:04,140
but it remains the case that nuclear weapons

1224
01:00:04,140 --> 01:00:06,300
are a threat to the future of humanity.

1225
01:00:08,660 --> 01:00:11,100
Do you think there's any way we can restrain

1226
01:00:11,100 --> 01:00:15,820
this AI phenomenon that's meaningful?

1227
01:00:15,820 --> 01:00:20,820
So you issued a clarion call, you sounded an alarm,

1228
01:00:23,660 --> 01:00:27,980
and mostly I think people shrugged it off, you know?

1229
01:00:27,980 --> 01:00:29,540
A bunch of people signed a letter,

1230
01:00:29,540 --> 01:00:33,540
26,000 people I think so far, signed the letter saying,

1231
01:00:33,540 --> 01:00:35,500
you know, we don't know what we're doing here,

1232
01:00:35,500 --> 01:00:38,460
this is uncharted territory, let's take six months off.

1233
01:00:38,460 --> 01:00:40,980
You were in peace and says, six months, are you crazy?

1234
01:00:40,980 --> 01:00:44,420
We need to stop this until we have an understanding

1235
01:00:44,420 --> 01:00:45,660
of how to constrain it.

1236
01:00:46,500 --> 01:00:49,100
Now that's a very reasonable thought to me,

1237
01:00:49,100 --> 01:00:51,060
but the next question would be,

1238
01:00:52,220 --> 01:00:54,060
how would you possibly do that?

1239
01:00:54,060 --> 01:00:56,620
In other words, I could imagine a world where,

1240
01:00:57,540 --> 01:01:00,620
if there were, let's say, four people who were capable

1241
01:01:00,620 --> 01:01:02,700
of creating this technology,

1242
01:01:02,700 --> 01:01:04,140
that the four people would say, you know,

1243
01:01:04,140 --> 01:01:06,540
we're playing with fire here, we need to stop,

1244
01:01:06,540 --> 01:01:09,180
let's make a mutual agreement, they might not keep it,

1245
01:01:09,180 --> 01:01:10,540
four people still a pretty big number,

1246
01:01:10,540 --> 01:01:12,540
but we're not a four people,

1247
01:01:12,660 --> 01:01:14,420
there are many, many people working on this,

1248
01:01:14,420 --> 01:01:16,580
there are many countries working on it.

1249
01:01:18,260 --> 01:01:21,180
Your peace did not, I don't think,

1250
01:01:21,180 --> 01:01:23,940
start an international movement of people going

1251
01:01:23,940 --> 01:01:27,220
to the barricades to demand that this technology

1252
01:01:27,220 --> 01:01:28,620
be put on hold.

1253
01:01:29,500 --> 01:01:32,020
How do we possibly, how do you sleep at night?

1254
01:01:32,020 --> 01:01:34,660
I mean, like, what should we be doing if you're right?

1255
01:01:37,140 --> 01:01:38,860
Or am I wrong, do people read this and go,

1256
01:01:38,860 --> 01:01:41,220
well, Elias Rieckowski thinks it's dangerous,

1257
01:01:41,220 --> 01:01:43,420
maybe we ought to be slowing down.

1258
01:01:43,420 --> 01:01:46,260
I mean, Sam, what's happened in the middle of the night

1259
01:01:46,260 --> 01:01:50,660
saying, thanks, Elias, I'm gonna put things on hold.

1260
01:01:50,660 --> 01:01:52,020
I don't think that happened.

1261
01:01:53,740 --> 01:01:56,500
I think you are somewhat underestimating the impact

1262
01:01:56,500 --> 01:01:57,900
and it is still playing out.

1263
01:02:00,140 --> 01:02:03,820
Okay, so like, mostly, it seems to me that if we wanted

1264
01:02:03,820 --> 01:02:07,580
to win this, we needed to start a whole lot earlier,

1265
01:02:07,580 --> 01:02:09,340
possibly in the 1930s.

1266
01:02:11,260 --> 01:02:14,820
But in terms of, like, my looking back

1267
01:02:14,820 --> 01:02:17,220
and like asking how far back you'd have to unwind history

1268
01:02:17,220 --> 01:02:21,100
to get us into a situation where this was survivable.

1269
01:02:24,140 --> 01:02:27,380
But leaving that aside.

1270
01:02:27,380 --> 01:02:28,460
I think that's a move.

1271
01:02:28,460 --> 01:02:32,500
Yeah, so in fact, it seems to me that the game board

1272
01:02:32,500 --> 01:02:35,460
has been played into a position where it is very likely

1273
01:02:35,460 --> 01:02:36,660
that everyone just dies.

1274
01:02:38,500 --> 01:02:40,700
If the human species woke up one day

1275
01:02:40,700 --> 01:02:43,020
and decided it would rather live,

1276
01:02:43,020 --> 01:02:46,340
it would not be easy at this point

1277
01:02:46,340 --> 01:02:51,340
to bring the GPU clusters and the GPU manufacturing

1278
01:02:51,580 --> 01:02:53,860
processes under sufficient control

1279
01:02:53,860 --> 01:02:57,460
that nobody built things that were too much smarter

1280
01:02:57,460 --> 01:03:02,460
than GPT-4 or GPT-5 or whatever the level just barely short

1281
01:03:02,620 --> 01:03:04,580
of lethal is, which we should not,

1282
01:03:04,580 --> 01:03:06,380
which we would not if we were taking this seriously,

1283
01:03:06,380 --> 01:03:07,940
get as close to as we possibly could,

1284
01:03:07,940 --> 01:03:10,860
because we don't actually know exactly where the level is.

1285
01:03:10,860 --> 01:03:12,540
But we would have to do more or less,

1286
01:03:12,540 --> 01:03:17,540
is have international agreements that were being enforced

1287
01:03:17,980 --> 01:03:19,900
even against parties not part,

1288
01:03:19,900 --> 01:03:22,860
even against countries not party to that national agreement,

1289
01:03:22,860 --> 01:03:25,620
international agreement, if it became necessary,

1290
01:03:25,620 --> 01:03:29,180
you would be wanting to track all the GPUs.

1291
01:03:29,180 --> 01:03:32,360
You might be demanding that all the GPUs call home

1292
01:03:32,360 --> 01:03:34,860
on a regular basis or stop working.

1293
01:03:34,860 --> 01:03:36,500
You'd want to tamper proof them.

1294
01:03:38,220 --> 01:03:42,700
If intelligence said that a rogue nation was,

1295
01:03:42,700 --> 01:03:46,420
had bought, somehow managed to buy a bunch of GPUs

1296
01:03:46,420 --> 01:03:49,260
despite arms controls and defeat the tamper proofing

1297
01:03:49,260 --> 01:03:51,380
on those GPUs, you would have to do it was necessary

1298
01:03:51,380 --> 01:03:52,580
to shut down the data center,

1299
01:03:52,580 --> 01:03:54,620
even if that led to a shooting war between nations,

1300
01:03:54,620 --> 01:03:56,860
even if that country was a nuclear country

1301
01:03:56,860 --> 01:03:59,260
and had threatened nuclear retaliation.

1302
01:03:59,260 --> 01:04:01,460
The human species could survive this if it wanted to,

1303
01:04:01,460 --> 01:04:03,460
but it would not be business as usual.

1304
01:04:04,560 --> 01:04:07,180
It is not something you could do trivially.

1305
01:04:07,180 --> 01:04:10,020
So when you say I may have underestimated it,

1306
01:04:10,020 --> 01:04:12,220
did you get people writing and saying,

1307
01:04:12,220 --> 01:04:15,380
you know, I wasn't, and I don't mean people like me.

1308
01:04:15,380 --> 01:04:16,860
I mean, people, players.

1309
01:04:16,860 --> 01:04:18,980
Did you get people who are playing in this sandbox

1310
01:04:18,980 --> 01:04:21,660
to write you and say, you've scared me.

1311
01:04:21,660 --> 01:04:23,660
I think we need to take this seriously?

1312
01:04:26,620 --> 01:04:29,940
Without naming names, I'm not asking for that.

1313
01:04:29,940 --> 01:04:31,860
At least one US Congressman.

1314
01:04:31,860 --> 01:04:32,700
Okay.

1315
01:04:34,460 --> 01:04:36,300
It's a start, maybe.

1316
01:04:36,300 --> 01:04:38,660
You know, one of the things that a common response

1317
01:04:38,660 --> 01:04:41,100
that people get when you talk about this is that,

1318
01:04:41,100 --> 01:04:42,180
well, the last thing I do is,

1319
01:04:42,180 --> 01:04:43,740
last thing I want is the government controlling

1320
01:04:43,740 --> 01:04:45,060
whether this thing goes forward or not,

1321
01:04:45,060 --> 01:04:48,460
but it'd be hard to do without some form of lethal force

1322
01:04:48,460 --> 01:04:49,300
as you comply.

1323
01:04:49,300 --> 01:04:54,300
I spent 20 years trying desperately to have there be

1324
01:04:54,740 --> 01:04:59,660
any other solution to have these things be alignable,

1325
01:04:59,660 --> 01:05:01,100
but it is very hard to do that

1326
01:05:01,100 --> 01:05:05,540
when you are nearly alone and under resourced

1327
01:05:05,540 --> 01:05:08,420
and the world has not made this a priority.

1328
01:05:08,420 --> 01:05:11,180
And future progress is very hard to predict.

1329
01:05:12,820 --> 01:05:15,660
I don't think people actually understood the research program

1330
01:05:15,660 --> 01:05:18,980
that we were trying to carry out, but yeah.

1331
01:05:18,980 --> 01:05:22,780
So I sure wanted there to be any other plan than this

1332
01:05:22,780 --> 01:05:24,600
because now that we've come to this last resort,

1333
01:05:24,600 --> 01:05:27,020
I don't think we actually have that last resort.

1334
01:05:27,020 --> 01:05:28,700
I don't think we have been reduced

1335
01:05:28,700 --> 01:05:30,820
to a last-ditch backup plan that actually works.

1336
01:05:30,820 --> 01:05:32,960
I think we all just die.

1337
01:05:32,960 --> 01:05:36,960
And yet nonetheless, here I am,

1338
01:05:36,960 --> 01:05:40,320
like putting aside doing that thing

1339
01:05:40,320 --> 01:05:43,680
that I wouldn't do for almost any other technology,

1340
01:05:43,680 --> 01:05:45,800
except for maybe gain of function research

1341
01:05:45,800 --> 01:05:49,800
on biological pathogens

1342
01:05:51,480 --> 01:05:54,160
and advocating for government interference.

1343
01:05:54,160 --> 01:05:57,280
Because in fact, like if the government comes in

1344
01:05:57,280 --> 01:05:58,760
and wrecks the whole thing,

1345
01:05:58,760 --> 01:06:01,000
that's better than the thing

1346
01:06:01,000 --> 01:06:02,680
that was otherwise going to happen.

1347
01:06:02,720 --> 01:06:04,400
Because it's not based on the government coming in

1348
01:06:04,400 --> 01:06:06,400
and being like super competent

1349
01:06:06,400 --> 01:06:08,480
and directing the technology exactly directly.

1350
01:06:08,480 --> 01:06:11,120
It's like, okay, this is going to kill literally everyone

1351
01:06:11,120 --> 01:06:12,920
if the government stomps around

1352
01:06:12,920 --> 01:06:17,240
and like the dangerous of the government.

1353
01:06:17,240 --> 01:06:18,600
It's one of those very rare cases

1354
01:06:18,600 --> 01:06:19,920
where the dangerous that the government

1355
01:06:19,920 --> 01:06:22,640
will interfere too little rather than too much.

1356
01:06:22,640 --> 01:06:23,480
Possibly.

1357
01:06:24,600 --> 01:06:27,680
Let's close with a quote from Scott Ericsson,

1358
01:06:28,560 --> 01:06:29,720
which found on his blog.

1359
01:06:29,720 --> 01:06:31,360
We'll put a link up to the post.

1360
01:06:31,360 --> 01:06:36,280
Very interesting defensive of AI.

1361
01:06:36,280 --> 01:06:39,320
Scott's a University of Texas computer scientist.

1362
01:06:39,320 --> 01:06:41,440
He's working at OpenAI.

1363
01:06:41,440 --> 01:06:43,560
He's on leave, I don't, I think for a year, maybe longer.

1364
01:06:43,560 --> 01:06:45,480
I don't know, it doesn't matter.

1365
01:06:45,480 --> 01:06:47,160
He wrote the following.

1366
01:06:47,160 --> 01:06:49,560
So if we ask the directly relevant question,

1367
01:06:49,560 --> 01:06:51,720
do I expect the generative AI race,

1368
01:06:51,720 --> 01:06:55,320
which started in earnest around 2016 or 2017,

1369
01:06:55,320 --> 01:06:57,000
with the founding of OpenAI

1370
01:06:57,000 --> 01:07:00,840
to play a central causal role in the extinction of humanity?

1371
01:07:00,840 --> 01:07:04,520
I'll give a probability of around 2% for that.

1372
01:07:04,520 --> 01:07:06,320
And I'll give a similar probability,

1373
01:07:06,320 --> 01:07:08,520
maybe even a higher one for the generative AI race

1374
01:07:08,520 --> 01:07:12,560
to play a central causal role in the saving of humanity.

1375
01:07:12,560 --> 01:07:14,000
All considered then,

1376
01:07:14,000 --> 01:07:15,400
I come down in favor right now,

1377
01:07:15,400 --> 01:07:18,400
proceeding with AI research with extreme caution,

1378
01:07:18,400 --> 01:07:19,360
but proceeding.

1379
01:07:21,320 --> 01:07:24,600
My personal reaction is that is insane.

1380
01:07:24,600 --> 01:07:26,380
I have very little, I'm serious.

1381
01:07:26,380 --> 01:07:28,920
I find that deeply disturbing

1382
01:07:28,960 --> 01:07:31,480
and I'd love to have him on the program to defend it.

1383
01:07:31,480 --> 01:07:34,160
I don't think there's much of a chance

1384
01:07:34,160 --> 01:07:36,280
that generative AI would save humanity.

1385
01:07:36,280 --> 01:07:37,840
I'm not quite sure for what it's,

1386
01:07:40,120 --> 01:07:40,960
he's worried about,

1387
01:07:40,960 --> 01:07:44,960
but if you're telling me there's a 2%, 2% chance

1388
01:07:44,960 --> 01:07:46,200
that it's gonna destroy all humans

1389
01:07:46,200 --> 01:07:47,960
and you obviously think it's higher,

1390
01:07:47,960 --> 01:07:50,240
but 2% is really high to me

1391
01:07:50,240 --> 01:07:52,120
for an outcome that's rather devastating.

1392
01:07:52,120 --> 01:07:56,000
It's one of the deepest things I've learned from Nassim Tala.

1393
01:07:56,000 --> 01:07:58,320
It's not just the probability,

1394
01:07:58,320 --> 01:08:00,520
it's the outcome that counts too.

1395
01:08:00,520 --> 01:08:04,760
So this is ruined on a colossal scale

1396
01:08:04,760 --> 01:08:07,800
and the one thing you wanna do is avoid ruin

1397
01:08:07,800 --> 01:08:11,600
so you can take advantage of more draws from the earn.

1398
01:08:11,600 --> 01:08:14,800
The average return from the earn is irrelevant

1399
01:08:14,800 --> 01:08:16,800
if you are not allowed to play anymore.

1400
01:08:16,800 --> 01:08:18,800
You're out, you're dead, you're gone.

1401
01:08:18,800 --> 01:08:21,080
So you're suggesting we're gonna be out and dead and gone,

1402
01:08:21,080 --> 01:08:23,120
but I want you to react to Scott's quote.

1403
01:08:24,120 --> 01:08:27,880
Um, 2% sounds great.

1404
01:08:27,880 --> 01:08:31,200
Like 2% is plausibly within the range of like

1405
01:08:31,200 --> 01:08:34,800
the human species destroying it itself by other means.

1406
01:08:34,800 --> 01:08:37,520
I think that the disagreement I have with Scott Aronson

1407
01:08:37,520 --> 01:08:41,960
is simply about the probability that AI is alignable

1408
01:08:41,960 --> 01:08:45,720
with the frankly half-fazard level

1409
01:08:45,720 --> 01:08:48,080
that we have put into it and the half-fazard level

1410
01:08:48,080 --> 01:08:50,800
that is all humanity is capable of

1411
01:08:51,680 --> 01:08:53,200
as far as I can tell,

1412
01:08:53,200 --> 01:08:56,360
because the core lethality here

1413
01:08:56,360 --> 01:08:57,560
is that you have to get something right

1414
01:08:57,560 --> 01:08:59,560
on the first try or it kills you

1415
01:08:59,560 --> 01:09:01,400
and getting something right on the first try

1416
01:09:01,400 --> 01:09:03,560
when you do not get like infinite free retries

1417
01:09:03,560 --> 01:09:05,800
as you usually do in science and engineering

1418
01:09:05,800 --> 01:09:09,640
is an insane ask, insanely lethal ask.

1419
01:09:09,640 --> 01:09:13,000
My reaction is fundamentally that 2% is too low.

1420
01:09:13,000 --> 01:09:14,680
If I take it at face value,

1421
01:09:14,680 --> 01:09:16,920
then 2% is within range of the probability

1422
01:09:16,920 --> 01:09:19,120
of humanity wiping itself out by something else

1423
01:09:19,120 --> 01:09:21,640
where if you assume that AI alignment is free,

1424
01:09:21,640 --> 01:09:23,760
that AI alignment is easy,

1425
01:09:23,760 --> 01:09:26,400
that you can get something that is smarter than you,

1426
01:09:26,400 --> 01:09:28,280
but on your side and helping,

1427
01:09:29,880 --> 01:09:32,880
2% chance of risking everything does appear to me

1428
01:09:32,880 --> 01:09:37,040
to be commensurate with the risks from other sources

1429
01:09:37,040 --> 01:09:40,120
that you could shut down using the superintelligence.

1430
01:09:40,120 --> 01:09:41,120
It's not 2%.

1431
01:09:42,240 --> 01:09:47,600
So, the question then is,

1432
01:09:49,600 --> 01:09:52,600
what would Scott Aaronson say if he heard your,

1433
01:09:52,600 --> 01:09:54,480
I mean, he's heard, he's read your piece presumably,

1434
01:09:54,480 --> 01:09:57,960
he understands your argument about wealthiness.

1435
01:09:57,960 --> 01:10:00,000
I should just clarify for listeners,

1436
01:10:00,000 --> 01:10:04,120
alignment is the idea that AI could be constrained

1437
01:10:04,120 --> 01:10:06,920
to serve our goals rather than its goals.

1438
01:10:06,920 --> 01:10:08,360
Is that a good summary?

1439
01:10:10,120 --> 01:10:11,600
I wouldn't say constrained.

1440
01:10:11,600 --> 01:10:14,760
I would say built from scratch to want those things

1441
01:10:14,760 --> 01:10:16,560
and not want otherwise.

1442
01:10:16,560 --> 01:10:17,760
So that's really hard

1443
01:10:17,800 --> 01:10:19,160
because we don't understand how it works.

1444
01:10:19,160 --> 01:10:21,000
That would be, I think, your point.

1445
01:10:21,000 --> 01:10:22,320
And tell me then what Scott's-

1446
01:10:22,320 --> 01:10:23,640
On the first try.

1447
01:10:23,640 --> 01:10:25,080
Yeah, on the first try.

1448
01:10:25,080 --> 01:10:28,200
So what would Scott say when you tell him,

1449
01:10:28,200 --> 01:10:31,320
but it's gonna develop all these side desires

1450
01:10:31,320 --> 01:10:33,080
that we can't control.

1451
01:10:33,080 --> 01:10:33,960
What's he gonna say?

1452
01:10:35,480 --> 01:10:36,320
Why is he not worried?

1453
01:10:36,320 --> 01:10:38,240
Why is he still, why is he quit his job?

1454
01:10:39,480 --> 01:10:40,760
And not Scott.

1455
01:10:40,760 --> 01:10:42,880
People in the, let's get away from him personally,

1456
01:10:42,880 --> 01:10:46,800
but people in general, there's dozens and maybe hundreds,

1457
01:10:46,800 --> 01:10:48,400
maybe a thousand, I don't know,

1458
01:10:48,400 --> 01:10:51,040
extraordinarily intelligent people

1459
01:10:51,040 --> 01:10:53,600
who are trying to build something even more intelligent

1460
01:10:53,600 --> 01:10:54,920
than they are.

1461
01:10:54,920 --> 01:10:57,600
Why are they not worried about what you're saying?

1462
01:10:57,600 --> 01:10:59,720
They've all got different reasons.

1463
01:10:59,720 --> 01:11:02,320
Scott's is that he thinks that intelligence,

1464
01:11:02,320 --> 01:11:05,440
that he observes intelligence makes humans nicer.

1465
01:11:05,440 --> 01:11:08,560
And though he wouldn't phrase it exactly this way,

1466
01:11:08,560 --> 01:11:11,760
this is basically what Scott said on his blog.

1467
01:11:11,760 --> 01:11:13,960
To which my response is intelligence

1468
01:11:13,960 --> 01:11:15,400
does have effects on humans,

1469
01:11:15,400 --> 01:11:18,560
especially humans who start out relatively nice.

1470
01:11:18,560 --> 01:11:21,000
And when you're building AI's from scratch,

1471
01:11:21,000 --> 01:11:23,240
you're just like in a different domain with different rules.

1472
01:11:23,240 --> 01:11:26,000
And you're allowed to say that it's hard to build AI's

1473
01:11:26,000 --> 01:11:29,640
that are nice without implying that making humans smarter,

1474
01:11:29,640 --> 01:11:32,640
like humans start out in a certain frame of reference.

1475
01:11:32,640 --> 01:11:34,600
And when you apply more intelligence to them,

1476
01:11:34,600 --> 01:11:37,160
they move within that frame of reference.

1477
01:11:37,160 --> 01:11:39,480
And if they start out with a small amount of nicest,

1478
01:11:39,480 --> 01:11:41,800
the intelligence can make them nicer.

1479
01:11:41,800 --> 01:11:44,080
They can become more empathetic.

1480
01:11:44,080 --> 01:11:47,280
If they start out with some empathy,

1481
01:11:47,280 --> 01:11:48,920
they can develop more empathy

1482
01:11:48,920 --> 01:11:50,840
as they understand other people better,

1483
01:11:50,840 --> 01:11:54,920
which is intelligence to correctly model other people.

1484
01:11:54,920 --> 01:11:57,240
That is even more insane.

1485
01:11:57,240 --> 01:12:01,800
I'm not going to, I haven't read that blog post

1486
01:12:01,800 --> 01:12:03,080
and we'll put a link up to it.

1487
01:12:03,080 --> 01:12:04,680
I hope you'll share it with me.

1488
01:12:04,680 --> 01:12:07,520
But again, not attributing it to Scott

1489
01:12:07,520 --> 01:12:09,040
since I haven't seen it.

1490
01:12:09,040 --> 01:12:13,120
And assuming that you've said this fairly and correctly,

1491
01:12:13,120 --> 01:12:15,560
the idea that more intelligent people are nicer

1492
01:12:15,560 --> 01:12:17,880
is one of the most,

1493
01:12:17,880 --> 01:12:20,480
that'd be very hard to show with the evidence for that.

1494
01:12:20,480 --> 01:12:21,320
That is an appalling.

1495
01:12:21,320 --> 01:12:22,640
I don't think it's like,

1496
01:12:22,640 --> 01:12:24,560
it is not a universal law on humans.

1497
01:12:24,560 --> 01:12:25,400
No.

1498
01:12:25,400 --> 01:12:27,680
It is a thing that I think is true of Scott.

1499
01:12:27,680 --> 01:12:29,760
I think if you made Scott Aaron Sinclair smarter,

1500
01:12:29,760 --> 01:12:30,680
he'd get nicer.

1501
01:12:32,320 --> 01:12:35,040
And I think he's inappropriately generalizing from that.

1502
01:12:35,040 --> 01:12:39,840
There is a scene in Schindler's List.

1503
01:12:39,840 --> 01:12:42,320
The Nazis, I think they're in the Warsaw ghetto

1504
01:12:42,320 --> 01:12:45,280
and they're race, a group of Nazis are racing,

1505
01:12:45,280 --> 01:12:46,320
I think they're in the SS,

1506
01:12:46,320 --> 01:12:49,400
they're racing through a tenement.

1507
01:12:49,400 --> 01:12:51,240
And it's falling apart

1508
01:12:51,240 --> 01:12:53,640
because the ghetto is falling apart.

1509
01:12:53,640 --> 01:12:57,600
But one of the SS agents sees a piano

1510
01:12:59,040 --> 01:13:00,800
and he can't help himself.

1511
01:13:00,800 --> 01:13:03,200
He sits down and he plays Bach or something.

1512
01:13:03,200 --> 01:13:05,080
I think it was Bach.

1513
01:13:05,080 --> 01:13:06,280
And I always found it interesting

1514
01:13:06,280 --> 01:13:09,560
that Spielberg put that in or whoever wrote the script.

1515
01:13:09,560 --> 01:13:11,160
And I think it was pretty clear why they put it in.

1516
01:13:11,160 --> 01:13:12,680
They wanted to show you that

1517
01:13:12,680 --> 01:13:15,440
having a very high advanced level of civilization

1518
01:13:15,440 --> 01:13:18,720
does not stop people from treating other people,

1519
01:13:18,720 --> 01:13:20,600
other human beings like animals,

1520
01:13:20,600 --> 01:13:22,580
or worse than animals in many cases,

1521
01:13:24,000 --> 01:13:28,720
and exterminating them without conscience.

1522
01:13:28,720 --> 01:13:31,640
So I don't share that view of anyone's

1523
01:13:31,640 --> 01:13:34,640
that intelligence makes you a nicer person.

1524
01:13:34,640 --> 01:13:36,680
I think that's not the case,

1525
01:13:36,680 --> 01:13:38,960
but perhaps Scott will return to this,

1526
01:13:38,960 --> 01:13:41,360
will come to this program and defend that view

1527
01:13:41,360 --> 01:13:42,880
if he did holes it.

1528
01:13:42,880 --> 01:13:45,880
I think you are underweighting the evidence

1529
01:13:45,880 --> 01:13:48,880
that has convinced Scott of the thing that I think is wrong.

1530
01:13:48,880 --> 01:13:52,400
I think if you suddenly started augmenting the intelligence

1531
01:13:52,400 --> 01:13:55,000
of the SS agents from Nazi Germany,

1532
01:13:55,000 --> 01:13:59,640
then somewhere between 10% and 90% of them

1533
01:13:59,640 --> 01:14:02,200
would go over to the cause of good.

1534
01:14:02,200 --> 01:14:05,280
Because there were factual falsehoods

1535
01:14:05,280 --> 01:14:10,240
that were pillars of the Nazi philosophy,

1536
01:14:10,240 --> 01:14:13,240
and that people would reliably stop believing

1537
01:14:13,240 --> 01:14:14,480
as they got smarter.

1538
01:14:14,480 --> 01:14:16,040
That doesn't mean that they would turn good,

1539
01:14:16,040 --> 01:14:17,280
but some of them would have.

1540
01:14:17,280 --> 01:14:18,200
Is it 10%?

1541
01:14:18,200 --> 01:14:19,040
Is it 90%?

1542
01:14:19,040 --> 01:14:19,880
I don't know.

1543
01:14:19,880 --> 01:14:22,960
It's not my experience with the human creature.

1544
01:14:24,000 --> 01:14:25,240
You've written some very interesting things

1545
01:14:25,240 --> 01:14:26,640
on rationality of a beautiful essay,

1546
01:14:26,640 --> 01:14:31,480
we'll link to on 12 rules for rationality.

1547
01:14:31,480 --> 01:14:33,400
In my experience, it's a very small portion

1548
01:14:33,400 --> 01:14:35,760
of the population that behaves that way.

1549
01:14:38,600 --> 01:14:40,520
There's a quote from Nassim Taleb,

1550
01:14:40,520 --> 01:14:43,040
we haven't gotten to yet in this conversation,

1551
01:14:43,040 --> 01:14:46,200
which is bigger data, bigger mistakes.

1552
01:14:46,200 --> 01:14:47,560
I think there's a belief generally

1553
01:14:47,560 --> 01:14:49,600
that bigger data fewer mistakes,

1554
01:14:49,600 --> 01:14:50,840
but Taleb might be right,

1555
01:14:50,840 --> 01:14:52,680
and it's certainly not the case in my experience

1556
01:14:52,680 --> 01:14:56,520
that bigger brains, higher IQ means better decisions.

1557
01:14:56,520 --> 01:14:58,600
This is not my experience.

1558
01:14:58,600 --> 01:15:03,120
Then you're not throwing enough intelligence

1559
01:15:03,120 --> 01:15:04,080
at the problem.

1560
01:15:05,920 --> 01:15:07,960
Literally not just decisions

1561
01:15:07,960 --> 01:15:10,120
that you disagree with the goals,

1562
01:15:10,120 --> 01:15:12,680
but false models of reality,

1563
01:15:14,040 --> 01:15:16,480
models of realities so blatantly mistaken

1564
01:15:16,480 --> 01:15:18,000
that even you, a human,

1565
01:15:18,000 --> 01:15:20,480
can tell that they're wrong and in which direction.

1566
01:15:20,480 --> 01:15:21,920
These people are not smart the way

1567
01:15:21,920 --> 01:15:26,920
that a hypothetical weak efficient market is smart.

1568
01:15:27,280 --> 01:15:29,320
You can tell they're making mistakes

1569
01:15:29,320 --> 01:15:30,760
and you know in which direction.

1570
01:15:30,800 --> 01:15:32,800
They're not smart the way that Stockfish 15

1571
01:15:32,800 --> 01:15:34,080
is smart in chess.

1572
01:15:34,080 --> 01:15:36,320
You can play against them and win.

1573
01:15:36,320 --> 01:15:41,320
These are, the range of human intelligence

1574
01:15:41,320 --> 01:15:42,440
is not that wide.

1575
01:15:42,440 --> 01:15:45,560
It caps out at like John von Neumann or whatever.

1576
01:15:45,560 --> 01:15:49,280
And that is not wide enough to open up

1577
01:15:49,280 --> 01:15:52,480
that humans would be epistemic,

1578
01:15:52,480 --> 01:15:55,240
that these beings would be epistemically

1579
01:15:55,240 --> 01:15:58,120
or instrumentally efficient relative to you.

1580
01:15:58,120 --> 01:15:59,560
It is possible for you to know

1581
01:15:59,560 --> 01:16:01,840
that one of their estimates is directionally mistaken

1582
01:16:01,840 --> 01:16:03,360
and to know the direction.

1583
01:16:03,360 --> 01:16:05,520
It is possible for you to know an action

1584
01:16:05,520 --> 01:16:07,360
that serves their goals better

1585
01:16:07,360 --> 01:16:09,080
than the action that they generated.

1586
01:16:09,080 --> 01:16:10,680
And is it striking how hard it is

1587
01:16:10,680 --> 01:16:13,680
to convince some of that even though they're thinking people?

1588
01:16:15,040 --> 01:16:19,080
History is, I just have a different perception maybe.

1589
01:16:19,080 --> 01:16:20,880
To be continued, Eliezer.

1590
01:16:22,200 --> 01:16:24,760
My guest today has been Eliezer Yudkowski.

1591
01:16:24,760 --> 01:16:26,960
Eliezer, thanks for being part of Econ Talk.

1592
01:16:27,920 --> 01:16:29,120
Thanks for having me.

1593
01:16:29,200 --> 01:16:34,400
This is Econ Talk, part of the Library of Economics and Liberty.

1594
01:16:34,400 --> 01:16:36,920
For more Econ Talk, go to econtalk.org

1595
01:16:36,920 --> 01:16:39,280
where you can also comment on today's podcast

1596
01:16:39,280 --> 01:16:42,840
and find links and readings related to today's conversation.

1597
01:16:42,840 --> 01:16:45,840
The sound engineer for Econ Talk is Rich Goyette.

1598
01:16:45,840 --> 01:16:47,640
I'm your host Russ Roberts.

1599
01:16:47,640 --> 01:16:48,840
Thanks for listening.

1600
01:16:48,840 --> 01:16:50,360
Talk to you on Monday.

