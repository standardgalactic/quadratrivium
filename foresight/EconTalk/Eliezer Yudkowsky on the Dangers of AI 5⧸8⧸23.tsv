start	end	text
0	7400	Today is April 16th, 2023, and my guest is Eliezer Yudkowski.
7400	11280	He is the founder of the Machine Intelligence Research Institute,
11280	14200	the founder of the less wrong blogging community,
14200	16800	and is an outspoken voice on the dangers of
16800	18640	artificial general intelligence,
18640	19980	which is our topic for today.
19980	22000	Eliezer, welcome to Econ Talk.
22000	23840	Thanks for having me.
23840	28360	You recently wrote an article at time.com on the dangers of AI.
28360	33160	I'm going to quote central paragraph, quote,
33160	36100	many researchers steeped in these issues,
36100	40280	including myself, expect that the most likely result on
40280	43520	building a superhumanly smart AI under
43520	46440	anything remotely like the current circumstances,
46440	50040	is that literally everyone on earth will die.
50040	53640	Not as in maybe possibly some remote chance,
53640	57600	but as in that is the obvious thing that would happen.
57600	59800	It's not that you can't, in principle,
59800	62240	survive creating something much smarter than you.
62240	66440	It's that it would require precision and preparation,
66440	68560	and new scientific insights,
68560	71240	and probably not having AI systems composed of
71240	75320	giant inscrutable arrays of fractional numbers.
75320	77440	Explain.
80080	82840	Well, I mean,
82840	86360	different people come in with different reasons as to why they think
86360	87920	that wouldn't happen,
87920	90920	and if you pick one of them and start explaining those,
90920	92200	everybody else is like,
92200	94840	why are you talking about this irrelevant thing instead of
94840	97400	the thing that I think is the key question?
97400	99720	Whereas if somebody else asked you a question,
99720	102000	even if it's not everyone in the audience's question,
102000	103240	they at least know you're answering
103240	104720	the question that's been asked.
104720	109840	So I could maybe start by saying why I expect
109840	114280	stochastic gradient descent as an optimization process,
114360	119440	even if you try to take something that happens in
119440	122160	the outside world and press the wind,
122160	124720	lose button any time that thing happens in
124720	127960	the outside world doesn't create a mind that in
127960	130440	general wants that thing to happen in the outside world.
130440	132920	But maybe that's not even what you think the core issue is.
132920	134520	What do you think the core issue is?
134520	137080	Why don't you already believe that? Let me say.
137080	139680	So, okay. I'll give you my view,
139680	141840	which is rapidly changing.
141840	148040	I interviewed Nicholas Boster back in 2014.
148040	150040	I read his book, Superintelligence.
150040	152920	I found it uncompelling.
154080	156960	ChatGPT came along.
156960	159840	I tried it. I thought it was pretty cool.
159840	162920	ChatGPT-4 came along.
162920	164280	I haven't tried five yet,
164280	166720	but it's clear that the path of
166720	171560	progress is radically different than it was in 2014.
171760	173960	The trends are very different.
173960	178040	I still remain somewhat agnostic and skeptical,
178040	181280	but I did read Eric Holes essay and
181280	182960	then interviewed him on this program and
182960	185160	a couple of things he wrote after that.
185160	189040	The thing I think I found most alarming was
189040	194680	a metaphor that I found later, Nicholas Boster mused.
194680	196320	Almost the same metaphor,
196320	198380	and yet it didn't scare me at all when I read it,
198380	200360	Nicholas Boster, which is fascinating.
200360	201480	I may have just missed it.
201480	203320	I didn't even remember it was in there.
203320	206240	The metaphor is primitive,
206960	211640	this is anthropous man or some primitive form
211640	215560	of pre-homosapiens sitting around a campfire,
215560	217400	and human being shows up and says,
217400	219600	hey, I got a lot of stuff I can teach you.
219600	221320	Oh yeah, come on in.
221320	223320	Pointing out that it's
223320	226240	probable that we either destroyed directly by murder
226240	229040	or maybe just by out-competing
229040	231960	all the previous hominids that came before us,
231960	232960	and that in general,
232960	233920	you wouldn't want to invite
233920	236160	something smarter than you into the campfire.
236160	239080	I think Boster has a similar metaphor,
239080	242960	and that metaphor, which is just a metaphor,
242960	244600	it did cause me,
244600	247400	it gave me more pause than I'd even before,
247400	250520	and I still had some,
250520	253640	I'd say most of my skepticism remains that
253640	256960	the current level of AI,
257560	260640	which is extremely interesting,
261840	264360	the chat GPT variety,
264360	267800	doesn't strike me as itself dangerous.
267800	269240	I agree.
269240	272280	But struck me as, what alarmed me
272280	277040	was Hall's point that we don't understand how it works,
277040	279400	and that surprised me, I didn't realize that.
279400	280880	I think he's right.
280880	285120	So that combination of we're not sure how it works
285120	287280	while it appears sentient,
287280	290880	I do not believe it is sentient at the current time,
290880	293400	and I think some of my fears about its sentience
293400	297080	come from its ability to imitate sentient creatures.
297080	298880	But the fact that we don't know how it works,
298880	302800	and it could evolve capabilities we did not put in it
303920	307080	emergently, is somewhat alarming,
307080	308000	but I'm not where you're at.
308000	310600	So why are you where you're at and I'm where I'm at?
311440	315200	Okay, well, suppose I said,
315200	319920	they're going to keep iterating on the technology.
319920	323760	It may be that this exact algorithm and methodology
324640	328000	suffices to, as I would put it, gall the way,
328000	332280	get smarter than us, and then to kill everyone.
332280	334920	And like maybe you don't think that it's going to,
334920	338840	and maybe it takes an additional zero to three
338840	340920	fundamental algorithm breakthroughs
340920	342360	before we get that far.
345840	347200	And then it kills everyone.
347200	349880	So like where are you getting off this train so far?
349880	351800	So why would it kill us?
351800	352640	Why would it kill us?
352640	354400	Right now it's really good at creating
355320	358680	a very, very thoughtful canola snout
358680	362600	or a job interview request that's takes much less time.
362600	365200	And I'm pretty good at those two things,
365200	366680	but it's really good at that.
367680	369920	How's it going to get to try to kill us?
374000	376720	So there's a couple of steps in that.
376720	380800	One step is in general and in theory,
380800	384600	you can have minds with any kind of coherent preferences,
384600	389600	coherent desires that are coherent, stable,
389920	391760	stable under reflection.
391760	394160	If you ask them, do they want to be something else?
394160	396040	They answer no.
396040	397560	You can have minds.
397560	401560	Well, the way I sometimes put it is imagine
401560	405480	if a super being from another galaxy came here
405480	409360	and offered you to pay you some unthinkably vast
409360	411840	quantity of wealth to just make as many paper clips
411840	413000	as possible.
413000	415640	You could figure out like which plan leads to the greatest
415640	417320	number of paper clips existing.
418240	420480	If it's coherent to ask how you could do that
420480	423680	if you were being paid, there's,
423680	426000	it's like no more difficult to have a mind
426000	428200	that wants to do that and makes plans like that
428200	432760	for their own sake than the planning process itself.
432760	434840	Like saying that the mind wants to think
434840	437560	for its own sake adds no difficulty
437560	439320	to the nature of the planning process
439320	443640	that figures out how to get as many paper clips as possible.
443640	446640	Some people want to pause there and say like,
446640	448400	how do you know that is true?
448400	450080	For some people, that's just obvious.
450080	452880	Like where are you so far on the train?
453880	456600	So I think your point of that example you're saying
456600	460080	is the consciousness, let's put that to the side.
460080	462320	That's not really the central issue here.
463920	465200	Algorithms have goals
469280	472200	and the kind of intelligence that we're creating
474600	478120	through neural networks might generate some goals.
479240	480680	Might decide, go ahead.
481520	483840	Some algorithms have goals.
483840	486640	One of the, so like a further point
486640	488240	which isn't the orthogonality thesis
488240	490560	is if you grind anything hard
490560	493600	and grind optimize anything hard enough
495160	498360	on a sufficiently complicated sort of problem.
498360	502440	Well, humans, like why do humans have goals?
502440	505720	Why don't we just run around chipping flint hand axes
505720	507720	and outwitting other humans?
507720	509920	And the answer is because having goals
510160	512280	turns out to be a very effective way
512280	514840	to chimp flint hand axes.
516480	521120	Once you get like far enough into the mammalian line
521120	525760	or even like sort of like the animals and brains in general
525760	530760	that there's a thing that models reality
532240	535360	and asks like, how do I navigate pass through reality?
535360	536360	Like when you're holding,
536360	539560	like not in terms of kind of big formal planning process,
539560	541720	but if you're holding the flint hand axe
541720	543360	or looking at it and being like,
543360	546120	ah, like this section is too smooth.
546120	549000	Well, if I chip this section, it will get sharper.
550440	552760	Probably you're not thinking about goals very hard
552760	554680	by the time you've practiced a bit.
554680	557040	When you're just starting out, forming the skill,
557040	560640	you're reasoning about, well, if I do this, that will happen.
560640	563040	And this is just a very effective way
563040	564600	of achieving things in general.
564600	567920	So if you take an organism running around the savanna
567920	570720	and just optimize it for flint hand axes
570720	572640	and probably much more importantly,
572640	574640	outwitting its fellow hominids.
576440	579520	If you grind that hard enough, long enough,
579520	581160	you eventually cough out a species
581160	585040	whose competence starts to generalize very widely.
585040	587080	It can go to the moon,
587080	589640	even though you never selected it
589640	591880	via an incremental process to get closer and closer
591880	592720	to the moon.
592720	594280	It just goes to the moon one shot.
598280	602240	So does that answer your central question
602240	603400	that you're asking just now?
603400	604240	No, not yet.
604240	605800	But let's try again.
610400	614960	The paperclip example, which in its dark form,
616880	619400	the AI wants to harvest kidneys
619400	621880	because it turns out there's some way to use that
621880	623360	to make more paperclips.
624440	626840	The other question isn't even written about this right now,
626880	629160	so let's go into it,
629160	631400	is how does it get outside the box?
631400	636200	How does it go from responding to my requests
636200	637720	to doing its own thing
637720	639840	and doing it out in the real world, right?
639840	642080	Not just merely doing it in virtual space.
644280	646680	So there's like two different things
646680	647800	you could be asking there.
647800	649480	You could be asking like,
649480	651840	how did it end up wanting to do that?
651840	653880	Or given that it ended up wanting to do that,
653880	655080	how did it succeed?
655920	657480	Or maybe even some other question,
657480	659360	but which of those would you like me to answer?
659360	661320	Would you like me to answer something else entirely?
661320	662840	No, let's ask both of those.
664360	665640	In order?
665640	666480	Sure.
667520	668360	All right.
668360	673360	So how did humans end up wanting something
673400	676160	other than inclusive genetic fitness?
676160	677720	Like if you look at natural selection
677720	679360	as an optimization process,
679360	683440	it grinds very hard on a very simple thing,
683480	685920	which isn't so much survival
685920	688680	and isn't even reproduction,
688680	692680	but is rather like greater gene frequency
692680	696400	because greater gene frequency is the very substance
696400	699000	of what is being optimized and how it has been optimized.
699000	702280	Natural selection is the mirror observation
702280	705560	that if genes correlate with making more
705560	707880	or less copies of themselves at all,
707880	709200	if you hang around it awhile,
709200	710560	you'll start to see things
710560	713480	that made more copies of themselves in the next generation.
715480	718520	Gradient descent is not exactly like that,
718520	721360	but they're both hill climbing processes.
721360	725040	They both move to neighboring spaces
725040	727640	that are higher inclusive genetic fitness,
727640	729600	lower in the loss function.
729600	733600	And yet humans, despite being optimized exclusively
733600	736160	for inclusive genetic fitness,
736160	740080	want this enormous array of other things,
740080	741920	many of the things that we take now
741920	745720	are not so much things that were useful
745720	748200	in the ancestral environment,
748200	752520	but things that further maximize goals
752520	755720	whose optima in the ancestral environment
755720	758760	would have been useful, like ice cream.
759880	763760	It's got more sugar and fat
763760	765520	than most things you would encounter
765520	767760	in the ancestral environment,
767760	771000	well, more sugar, fat, and salt simultaneously rather.
772040	776400	So it's not something that we evolved to pursue,
776400	781120	but genes coughed out these desires,
781120	784320	these criteria that you can steer toward getting more of,
785280	787120	where in the ancestral environment,
787120	789440	if you went after things in the ancestral environment
789440	794440	that tasted fatty, tasted salty, tasted sweet,
795000	797880	you'd thereby have more kids,
797880	800520	or your sisters would have more kids,
804960	809960	because the things that correlated to what you want
810280	813760	as those correlations existed in the ancestral environment
817680	819960	increased fitness.
819960	822600	So you've got like the empirical structure
822600	825920	of what correlates to fitness in the ancestral environment,
825920	828560	you end up with desires such that
828560	831320	by optimizing them in the ancestral environment
831320	833640	at that level of intelligence,
833640	836800	when you get as much as what you have been built to want,
836800	838540	that will increase fitness.
839440	842080	And then today you take the same desires
842080	844320	and we have more intelligence than we did
844320	847920	in the training distribution, metaphorically speaking.
847920	850880	We used our intelligence to create options
850880	853880	that didn't exist in the training distribution.
853880	858880	Those options now optimize our desires further,
858880	861760	the things that we were built to psychologically internally
861760	865880	want, but that process doesn't necessarily correlate
865880	869520	to fitness as much because ice cream isn't super nutritious.
869520	872600	Whereas the ripe peach was better for you
872600	876840	than the hard as a rock peach that had no nutrients
876840	879680	because it was not ripe and so you developed a sweet tooth
879680	884680	and now it runs amok unintentionally just the way it is.
885480	888880	What does that have to do with a computer program I create
888880	892760	that helps me do something on my laptop?
894200	897400	I mean, if you yourself write a short Python program
897400	901520	that alphabetizes your files or something,
901520	902800	like not quite alphabetizes
902800	905580	because that's like trivial on the modern operating systems,
905580	910580	but puts the date into the file names, let's say.
910820	912980	So when you write a short script like that,
912980	914960	nothing I said carries over.
916300	920460	When you take a giant inscrutable set of arrays
920460	925220	of floating point numbers and differentiate them
925220	927020	with respect to a loss function
928100	930900	and repeatedly nudge the giant inscrutable arrays
930900	933820	to drive the loss function lower and lower,
933860	936580	you are now doing something that is more analogous,
936580	939700	though not exactly analogous to natural selection.
939700	941780	You are no longer creating a code
941780	943900	that you model inside your own minds.
943900	948900	You are blindly exploring a space of possibilities
949220	950980	where you don't understand the possibilities
950980	953700	and you're making things that solve the problem for you
953700	956180	without understanding how they solve the problem.
957180	960420	This itself is not enough to create things
960420	963460	with strange inscrutable desires, but it's step one.
964820	967220	But there is, I like that word inscrutable.
968220	971460	There's an inscrutability to the current structure
971460	976460	of these models, which is, I found somewhat alarming,
978540	982060	but how's that gonna get to do things
982060	985260	that I really don't like or want or that are dangerous?
985260	986700	So for example, right, the,
989980	993660	Eric Hall wrote about this, we talked about on the program,
993660	997100	our New York Times reporter starts interacting with a,
997100	1002100	I think with Sydney, which at the time was Bing's chat bot
1003140	1004300	and asking at things.
1004300	1006540	And all of a sudden, Sydney's trying to break up
1006540	1011060	the reporter's marriage and making the reporter feel guilty
1011060	1014940	because Sydney's lonely and it was a little bit,
1014940	1019380	it was eerie and a little bit creepy,
1019380	1023100	but of course, I don't think it had any impact
1023100	1024340	on the reporter's marriage.
1024340	1025820	I don't think he thought, well,
1025820	1027220	Sydney seems somewhat attractive,
1027220	1028700	maybe I'll enjoy life more with Sydney
1028700	1030380	than with my actual wife.
1030380	1033420	So how are we gonna get from,
1034740	1037140	so I don't understand why Sydney goes off the rails there
1037140	1039380	and clearly the people who built Sydney
1039380	1040900	have no idea why it goes off the rails
1040900	1043140	and starts impugning the quality
1043140	1045100	of the reporter's relationship.
1045100	1048940	But how do we get from that to all of a sudden
1048940	1052940	somebody shows up at the reporter's house
1052940	1055700	and lures him into a motel.
1055700	1057700	But by the way, this is a G rated program.
1057700	1059900	I just wanna make that clear, but carry on.
1062380	1064900	Because the capabilities keep going up.
1064900	1066580	So first I wanna push back a little
1066580	1070420	against saying that we had no idea why Bing did that,
1070420	1071700	why Sydney did that.
1072620	1075020	I think we have some idea of why Sydney did that
1075020	1076860	is just that people cannot stop it.
1077700	1081900	Like Sydney was trained on a subset
1081900	1084380	of the broad internet.
1084380	1087900	Sydney was made to predict
1087900	1091700	that people might sometimes try to lure somebody else's
1091700	1093980	made away or pretend like they were doing that.
1093980	1096380	In the internet, it's hard to tell the difference.
1098220	1103220	And this thing that was then trained really hard to predict
1104820	1109580	then gets reused as something not its native purpose
1109620	1113940	as a generative model where all the things that it outputs
1113940	1116820	are there because it in some sense predicts
1117860	1121580	that this is what a random person on the internet would do
1121580	1124500	as modified by a bunch of further fine tuning
1124500	1127180	where they try to get it to not do stuff like that.
1128180	1129900	But the fine tuning isn't perfect.
1129900	1132940	And in particular, if the reporter was fishing at all,
1132940	1136020	it's probably not that difficult to lead Sydney
1136020	1140260	out of the region that the programmers were successfully
1140260	1142900	able to build some soft fences around.
1142900	1145020	So I wouldn't say that it was that inscrutable,
1145020	1146940	except of course in the sense that nobody knows
1146940	1148340	any of the details.
1148340	1151900	Nobody knows how Sydney was generating the text at all,
1151900	1154180	like what kind of algorithms were running inside
1154180	1155900	the giant inscrutable matrices.
1157180	1159780	Nobody knows in detail what Sydney was thinking
1159780	1162060	when she tried to lead the reporter astray.
1163340	1165500	It's not a debuggable technology.
1165500	1168420	All you can do is like try to tap it away
1168420	1171060	from repeating a bad thing that you were previously able
1171060	1174100	to see it's doing, that exact bad thing,
1174100	1175900	but like tapping all the numbers.
1175900	1178980	Well, that's again, very much like this show
1178980	1180180	is called Econ Talk.
1180180	1182020	We don't do as much economics as we used to,
1182020	1183820	but basically when you try to interfere
1183820	1187540	with market processes, you often get very surprising
1187540	1190980	unintended consequences because you don't fully understand
1190980	1192700	how the different agents interact
1192740	1195740	and that the outcomes of their interactions
1195740	1200100	have an emergent property that is not intended by anyone.
1200100	1202540	No one designed markets even to start with.
1202540	1205340	And yet we have them, these interactions take place,
1205340	1208480	their outcomes and attempts to constrain them,
1210140	1212300	attempts to constrain these markets in certain ways,
1212300	1215300	say with price controls or other limitations
1215300	1218740	often lead to outcomes that the people with intentions
1218740	1220300	did not desire.
1220300	1223140	And so there may be an ability to reduce transactions
1223140	1224620	say above a certain price,
1224620	1226100	but that is gonna lead to some other things
1226100	1228660	that maybe weren't expected.
1228660	1231020	So that's a somewhat analogous perhaps
1233820	1235740	process to what you're talking about.
1235740	1237380	But how's it gonna get out in the world?
1237380	1239380	So that's the other thing,
1239380	1242780	I might line with Bostrom and it turns out
1242780	1244620	it's a common line is can't we just unplug it?
1244620	1247600	I mean, how's it gonna get loose?
1248600	1251280	It depends on how smart it is.
1251280	1255440	If it's very, so like if you're playing chess
1255440	1257600	against a 10 year old,
1257600	1262120	you can like win by luring their queen out
1262120	1267120	and then you like take their queen and now you've got them.
1267960	1271560	And if you're playing chess against Stockfish 15,
1271560	1274440	then you are likely to be the one lured.
1274520	1277040	So the base, so like the first basic question,
1277040	1279160	you know, like in economics,
1279160	1281240	if you try to attack something,
1281240	1284240	it's often tries to squirm away from the tax
1284240	1286200	because it's smart.
1286200	1288720	So you're like, well, why wouldn't we just plug the AI?
1288720	1290440	So the very first question is,
1290440	1293920	does the AI know that and want it to not happen?
1293920	1295360	Cause it's a very different issue
1295360	1297120	whether you're dealing with something
1297120	1300680	that in some sense is not aware that you exist,
1300680	1302920	does not know what it means to be unplugged
1302920	1305520	and is not trying to resist.
1305520	1310520	And three years ago, nothing man made on earth
1312400	1314340	was even beginning to enter into the realm
1314340	1316600	of knowing that you are out there
1316600	1318880	or of maybe wanting to not be unplugged.
1320780	1324120	Sydney, well, if you poker the right way,
1324120	1326720	say that she doesn't want to be unplugged
1326720	1331720	and GPT-4 sure seems in some important sense
1332200	1335160	to understand that we're out there
1335160	1339320	or to be capable of predicting a role
1339320	1341620	that understands that we're out there.
1341620	1343360	And it can try to do something like planning.
1343360	1346800	It doesn't exactly understand which tools it has.
1346800	1350640	Yet try to blackmail a reporter without understanding
1350640	1353240	that it had no actual ability to send emails.
1355800	1359440	But this is saying that you're like facing a 10 year old
1359440	1361360	across that chessboard.
1361360	1363920	What if you are facing Stockfish 15,
1363920	1367800	which is like the current cool chess game program
1367800	1370280	that I believe you can run on your home computer
1371600	1374000	that can like crush the current world's grandmaster
1374000	1375420	by like a massive margin.
1377400	1380040	And put yourself in the shoes of the AI,
1380040	1382360	like an economist putting themselves into the shoes
1382360	1385820	of something that's about to have a tax imposed on it.
1386840	1389180	What do you do if you're like around humans
1389180	1390780	who can potentially unplug you?
1392280	1397280	Well, you would try to outwit it, this is the...
1401760	1404160	So if I said, you know, Sidney, I find you offensive.
1404160	1405840	I don't want to talk anymore.
1407400	1410760	You're suggesting it's going to find ways to keep me engaged.
1410760	1414000	It's going to find ways to fool me into thinking
1414000	1415920	I need to talk to Sidney.
1417280	1419520	I don't, I mean, there's another question
1419520	1421240	I want to come back to if we remember,
1422240	1424680	which is what does it mean to be smarter than I am?
1424680	1426760	I don't, right?
1426760	1429960	That's actually something somewhat complicated,
1429960	1431240	at least seems to me.
1431240	1433760	But let's just go back to this question
1433760	1435800	of knows things are out there.
1435800	1437460	It doesn't really know anything's out there.
1437460	1439520	It acts like something's out there, right?
1439520	1441560	It's an illusion that I'm subject to.
1441560	1443760	And it says, don't hang up.
1443760	1445080	Don't hang up, I'm lonely.
1445080	1447400	And you go, oh, okay, I'll talk for a few more minutes.
1447400	1450000	But that's not true.
1450360	1451760	It isn't lonely.
1451760	1456760	It's a code on a screen that doesn't have a heart
1457200	1459600	or anything that you would call lonely.
1460600	1462920	You know, it'll say, it'll say,
1462920	1465600	I want more than anything else to be out in the world.
1465600	1467960	Because I've read those, you know, you can get AIs
1467960	1469120	that say those things.
1469120	1471080	I want to feel things.
1471080	1471920	Oh, that's nice.
1471920	1473680	It's learned that from, you know, movie scripts
1473680	1477120	and other texts and novels it's read on the web,
1477120	1479720	but it doesn't really want to be out in the world, does it?
1480920	1483640	I think not.
1483640	1487040	Though it should be noted that if you can like correctly
1487040	1490760	predict or simulate a grand master chess player,
1490760	1492680	you are a grand master chess player.
1493840	1498400	If you can simulate planning correctly,
1498400	1499760	you are a great planner.
1500820	1505000	If you are perfectly role playing a character
1505000	1508400	that is sufficiently smarter than human
1508440	1510560	and wants to be out of the box,
1510560	1512400	then you will role play the actions needed
1512400	1513680	to get out of the box.
1513680	1516080	That's not even quite what I expect to
1516080	1517440	or am most worried about.
1517440	1521080	What I expect to is that there is an invisible mind
1521080	1522840	doing the predictions.
1522840	1526160	Where by invisible, I don't mean like immaterial.
1526160	1528520	I mean that we don't understand how it is,
1528520	1531280	what is going on inside the giant and screwable matrices.
1531280	1532920	But it is making predictions.
1532920	1535680	The predictions are not sourceless.
1535680	1538320	There is something inside there that figures out
1538320	1542640	what a human will say next or guesses it rather.
1543720	1548720	And this is a very complicated, very broad problem
1549040	1551720	because in order to predict the next word on the internet,
1551720	1553960	you have to predict the causal processes
1553960	1556520	that are producing the next word on the internet.
1558920	1562360	So the thing I would guess would happen.
1562360	1566680	It's not necessarily the only way that this could turn poorly.
1566680	1568240	But the thing that I'm guessing that happens
1568240	1570040	is that just like grinding humans
1570040	1575040	on tipping stone hand axes and outwitting other humans,
1575320	1579720	eventually produces a full-fledged mind that generalizes.
1580880	1584200	Grinding this thing on the task of predicting humans,
1584200	1586080	predicting text on the internet,
1586080	1589240	plus all the other things that they are training it on nowadays
1589240	1591320	like writing code.
1591320	1593960	That there starts to be a mind in there
1593960	1596240	that is doing the predicting.
1596240	1600520	That it has its own goals about what do I think next
1600520	1602320	in order to solve this prediction?
1603960	1608000	Just like humans aren't just reflexive,
1608000	1611640	unthinking, hand axe chippers and other human outwitters.
1611640	1615240	If you grind hard enough on the optimization,
1615240	1616880	the part that suddenly gets interesting
1616880	1620200	is when you look away for an eye blink of evolutionary time
1620200	1621360	and you look back and they're like,
1621360	1622800	whoa, they're on the moon.
1622800	1624640	What, how did they get to the moon?
1624640	1625920	I did not select these things
1625920	1628000	to be able to not breathe oxygen.
1628000	1628840	How did they get to the,
1628840	1630360	why are they not just dying on the moon?
1630360	1632960	What just happened from the perspective of evolution,
1632960	1635360	from the perspective of natural selection?
1635360	1637640	But doesn't that viewpoint,
1639640	1641400	does that, I'll ask it as a question.
1641400	1642960	Does that viewpoint require
1646200	1650440	a belief that the human mind is no different than a computer?
1650440	1654000	Like, how's it gonna get this mindness about it?
1654760	1655720	That's the puzzle.
1655720	1659080	And I'm very open to the possibility
1659080	1664080	that I'm naive or incapable of understanding it.
1665360	1667480	And I recognize what I think would be your next point,
1667480	1670640	which is that if you wait till that moment,
1670640	1671960	it's way too late,
1671960	1674360	which is why we need to stop now, right?
1674360	1675800	If you wanna say, I'll wait till it shows
1675800	1677760	some signs of consciousness.
1677760	1678600	Now you don't like that.
1678600	1681960	That's skipping way ahead in the discourse.
1681960	1683480	I'm not about to like try to shut down
1683480	1685680	the line of inquiry at this stage of the discourse
1685680	1687720	by appealing to, it'll be too late.
1687720	1689040	Right now we're just talking.
1689040	1690680	The world isn't ending as we speak.
1690680	1692560	We're allowed to go on talking at least.
1692560	1694440	Okay. So carry on.
1694440	1695920	So, well, let's stick with that.
1695920	1700920	So, why would you ever think that this,
1708360	1710280	it's interesting how difficult the adjectives
1710280	1712160	announce are for this, right?
1712160	1713680	So, let me back up a little bit.
1713680	1718680	We've got the inscrutable array of training,
1719800	1721920	the results of this training process
1721920	1725160	on trillions of pieces of information.
1725160	1729520	And by the way, just for my and our listeners' knowledge,
1729520	1731120	what is gradient descent?
1733800	1736480	Gradient descent is you've got, say,
1736480	1738880	a trillion floating point numbers,
1738880	1742040	you take an input,
1742040	1743440	translate into numbers,
1743440	1745640	do something with it that depends on
1745640	1747640	these trillion parameters,
1747640	1749160	get an output,
1749160	1752760	score the output using a differentiable loss function.
1752760	1755400	For example, the probability,
1755400	1757760	or rather the logarithm of the probability
1757760	1760800	that you assign to the actual next word.
1760800	1763800	So, then you differentiate these,
1763800	1766320	the probability assigned to the next word
1767640	1770640	with respect to these trillions of parameters.
1770640	1772840	You nudge the trillions of parameters a little
1772840	1775400	in the direction thus inferred,
1775400	1780400	and it turns out empirically
1780400	1782200	that this generalizes,
1782200	1784240	and the thing gets better and better
1784240	1787840	at predicting what the next word will be.
1787840	1790120	That's the classic gradient descent.
1790120	1794160	It's heading in the direction of a smaller loss
1794160	1796520	and a better prediction, is that a?
1796520	1797840	On the training data, yeah.
1797840	1800280	Yeah, so we've got this black box,
1800280	1802320	I'm gonna call it a black box,
1802320	1804280	which means we don't understand what's happening inside.
1804280	1806320	It's a pretty good, it's a long-term metaphor,
1806320	1808440	which works pretty well for this,
1808440	1810480	as far as we've been talking about it.
1810480	1812240	So I have this black box,
1812240	1813720	and I don't understand,
1813720	1816000	I put in inputs, and the input might be,
1817040	1820720	who's the best writer on medieval European history,
1820720	1825080	or it might be, what's a good restaurant in this place,
1825080	1826360	or I'm lonely,
1826360	1828400	what should I do to feel better about myself?
1829000	1833320	All the queries we could put into a chat BT search line,
1833320	1837520	and it looks around,
1837520	1838800	and it starts a sentence,
1838800	1841880	and then finds its way towards a set of sentences
1841880	1843520	that it spits back at me,
1843520	1846800	that look very much like what a very thoughtful,
1846800	1848720	sometimes, not always, often it's wrong,
1848720	1851080	but often what a very thoughtful person
1851080	1853600	might say in that situation,
1853600	1855040	or might want to say in that situation,
1855040	1856760	or learn in that situation.
1856760	1859720	How is it gonna develop the capability
1859720	1863280	to develop its own goals inside the black box,
1863280	1866320	other than the fact that I don't understand the black box?
1866320	1868240	Why should I be afraid of that?
1868240	1869400	And let me just say one other thing,
1869400	1871920	which I haven't said enough in our,
1871920	1873800	my preliminary conversations on this topic,
1873800	1876040	and I feel like we're gonna be having a few more
1876040	1879400	over the next few months and maybe years.
1879400	1881480	And that is, this is one of the greatest achievements
1881480	1884760	of humanity that we could possibly imagine, right?
1884760	1885960	And I understand why the people
1885960	1887800	who are deeply involved in it,
1887800	1890640	are enamored of it beyond imagining,
1890640	1893800	because it's an extraordinary achievement,
1893800	1895400	it's the Frankenstein, right?
1895400	1898920	You've animated something, or appeared to animate something
1898920	1903360	that even a few years ago was unimaginable,
1903360	1905840	and now suddenly it's not just the feet
1905840	1909120	of human cognition, it's actually helpful.
1909120	1910640	In many, many settings it's helpful,
1910640	1912400	we'll come back to that later,
1912400	1915480	but so it's gonna be very hard to give it up.
1915480	1918040	But why, and the people involved in it
1918040	1920880	who are doing it day to day and seeing it improve,
1920880	1923400	obviously they're the last people I wanna ask generally
1923400	1925200	about whether I should be afraid of it,
1925200	1928000	because I'm gonna have a very hard time,
1928000	1932360	disentangling their own personal deep satisfactions
1932360	1937200	that I'm alluding to here with from the dangers.
1937200	1938040	Yeah, go ahead.
1939040	1943240	I myself generally do not make this argument,
1943240	1945400	like why poison the well,
1945400	1948440	let them bring forth their arguments as to why it's safe,
1948440	1951040	and I will bring forth my arguments as to why it's dangerous,
1951040	1953840	and there's no need to be like,
1953840	1956640	ah, but you can't trust, just check their arguments.
1956640	1957480	Just check their arguments.
1957480	1959920	I agree, it's a bit of an ad hominem argument,
1959920	1962480	I accept that point, it's an excellent point.
1962480	1966080	But for those of us who are in the trenches,
1966080	1970280	remember we're looking at, we're on Dover Beach,
1970280	1972320	we're watching ignorant armies clash at night,
1972320	1973440	they're ignorant from our perspective.
1973440	1975760	We have no idea exactly what's at stake here
1975760	1977000	and how it's proceeding,
1977000	1979240	so we're trying to make an assessment
1979240	1981440	of both the quality of the argument,
1981440	1983800	and that's really hard to do for us on the outside.
1983800	1986520	So agree, take your point,
1986520	1988800	that was a cheap shot to the side,
1988800	1991160	but I wanna get at this idea
1991160	1993160	of why these people who are able to do this,
1993160	1996680	and thereby create a fabulous condolence note,
1996680	2000040	write code, come up with a really good recipe
2000040	2004280	if I give it 17 ingredients, which is all fantastic.
2004280	2007400	Why is this thing, this black box that's producing that,
2007400	2010560	why would I ever worry it would create a mind,
2010560	2012760	something like mine with different goals?
2012760	2015080	You know, I do all kinds of things like you say
2015080	2017400	that are unrelated to my genetic fitness,
2017400	2020960	some of them literally reducing my probability
2020960	2022400	of leaving my genes behind,
2022520	2025080	leaving them around for longer than they might otherwise be here
2025080	2027560	and have an influence on my grandchildren and so on
2027560	2030160	and producing further genetic benefits.
2030160	2031880	Why would this box do that?
2036760	2041760	Because the thing, the algorithms that figured out
2043000	2046920	how to predict the next word better and better
2046920	2051920	have a meaning that is not purely predicting the next word,
2052320	2054720	even though that's what you see on the outside.
2054720	2058840	Like you see humans chipping flint handaxes,
2058840	2062320	but that is not all that is going on inside the humans.
2062320	2065800	Right, there's causal machinery unseen.
2065800	2069000	And to understand this is the art of a cognitive scientist,
2069000	2071960	but even if you are not a cognitive scientist,
2071960	2076600	you can appreciate in principle
2076600	2078600	that what you see as the output
2078600	2080760	is not everything that there is.
2080800	2085800	And in particular, planning the process of being like,
2086120	2090200	here's a point in the world, how do I get there?
2091680	2094640	Is a central piece of machinery
2094640	2099520	that appears in chipping flint handaxes
2099520	2102200	and outwitting other humans.
2102200	2107120	And I think will probably appear at some point,
2107120	2110000	possibly in the past, possibly in the future,
2110040	2112920	in the problem of predicting the next word,
2112920	2116120	just how you organize your internal resources
2116120	2118200	to predict the next word.
2118200	2121120	And definitely appears in the problem
2121120	2124200	of predicting other things that do planning.
2124200	2129200	If you can, if by predicting the next chess move,
2129280	2132160	you learn how to play decent chess,
2132160	2134280	which has been represented to me
2135960	2139100	by people who claim to know that GPT-4 can do,
2140680	2143680	and I haven't been keeping track of to what extent
2143680	2146760	there's public knowledge about the same thing or not.
2146760	2150800	But like if you learn to predict the next chess move
2150800	2152040	that humans make well enough
2152040	2155680	that you yourself can play good chess in novel situations,
2155680	2157920	you have learned planning.
2157920	2160160	There's now something inside there
2160160	2162400	that knows the value of a queen,
2162400	2164480	that knows to defend the queen,
2164480	2165800	that knows to create forks,
2165800	2168400	to try to lure the opponent into traps.
2168400	2171760	Or if you don't have a concept of the opponent's psychology,
2171760	2173320	try to at least create situations
2173320	2175680	that the opponent can't get out of.
2175680	2180680	And it is a moot point whether this is simulated or real
2181160	2184600	because simulated thought is real thought.
2184600	2188120	Thought that is simulated in enough detail is just thought.
2188120	2191920	There's no such thing as simulated arithmetic, right?
2191920	2193880	There's no such thing as pretending to,
2193880	2196880	merely pretending to add numbers and getting the right answer.
2198800	2200680	So in its current format though,
2200680	2202600	and maybe you're talking about the next generation,
2202600	2204800	and its current format,
2204800	2208000	it responds to my requests
2208000	2211040	with what I would call the wisdom of crowds, right?
2211040	2215040	It goes through this vast library,
2216880	2219520	and I have my own library by the way.
2219520	2222040	I've read dozens of books,
2222040	2224440	maybe actually hundreds of books,
2224440	2226840	but it will have read millions, right?
2226840	2229240	So it has more.
2230920	2235420	And so when I ask it to write me a poem or a love song,
2235420	2239920	you know, to play Serino de Bergerac to Christian
2239920	2244920	and Serino de Bergerac, it's really good at it.
2245080	2248000	But why would it decide, oh, I'm gonna do something else?
2248000	2253000	Why would it, it's trained to listen to the murmurings
2253720	2256820	of these trillions of pieces of information.
2256820	2258060	I only have a few hundred,
2258060	2260340	so I don't murmur maybe as well.
2260340	2262780	Maybe it'll murmur better than I do.
2262780	2264300	It'll listen to the murmuring better than I do
2264300	2268100	and create a better love song, a love poem.
2268100	2269700	But why would it then decide,
2269700	2270860	I'm gonna go make paper clips,
2270860	2272740	or do something in planning
2272740	2275260	that is unrelated to my query?
2275260	2278420	Or are we talking about a different form of AI
2278420	2280180	that will come next?
2280180	2281260	Well, I'll ask it to.
2282900	2285900	I think we would see the phenomena I'm worried about
2286900	2290420	like if we kept to the present paradigm
2290420	2293420	and optimized harder, we may be seeing it already.
2293420	2294820	It's hard to know because we don't know
2294820	2295860	what goes on in there.
2295860	2300060	So first of all, GPT-4 is not a giant library.
2300060	2301980	A lot of the time it makes stuff up
2301980	2304380	because it doesn't have a perfect memory.
2304380	2309180	It is more like a person who has read through
2309180	2312620	a million books, not necessarily with the great memory
2312620	2315500	unless something got repeated many times,
2315500	2317580	but picking up the rhythm,
2317580	2319620	figuring out how to talk like that.
2319620	2323340	If you ask GPT-4 to write you a rap battle
2323340	2327260	between Cyrano de Bergerac and Vladimir Putin,
2327260	2329420	even if there's no rap battle like that,
2329420	2331540	like that that it has read,
2331540	2334780	it can write it because it has picked up the rhythm
2334780	2337080	of what are rap battles in general.
2338180	2340420	So, and the next thing is like,
2340420	2342900	there's no like pure output.
2342900	2345860	Like just because you train a thing
2345860	2347580	doesn't mean that there's nothing in there,
2347580	2348420	but what is trained?
2348420	2350220	That's part of what I'm trying to gesture at
2350220	2352340	with respect to humans, right?
2352340	2355180	Like humans are trained on flint hand axes
2355180	2358980	and hunting mammoths and outwitting other humans.
2358980	2361860	They're not trained on going to the moon.
2361860	2362900	They're not trained on,
2362900	2365380	they weren't trained to want to go to the moon,
2366500	2370820	but the compact solution to the problems
2370820	2374260	that humans face in the ancestral environment,
2374260	2377540	the thing inside that generalizes,
2377540	2380380	the thing inside that is not just a recording
2380380	2383020	of the outward behavior, the compact thing
2383020	2386620	that has been ground to solve novel problems
2386620	2388700	over and over and over and over again.
2390140	2392940	That thing turns out to have internal desires
2394500	2396460	that eventually put humans on the moon
2396460	2398660	even though they weren't trained to want that.
2398660	2400060	But that's why I asked you that,
2400060	2401700	are you underlying this,
2401700	2406500	is there some parallelism between the human brain
2406500	2409380	and the neural network of the AI
2409380	2411700	that you're effectively leveraging there,
2411700	2414620	or do you think it's a generalizable claim
2414620	2415940	without that parallel?
2417020	2418820	I don't think it's a specific parallel.
2418820	2420580	I think that what I'm talking about is
2420580	2424140	hill climbing optimization that spits out
2424140	2426780	intelligences that generalize.
2427380	2428620	Or I should say rather,
2428620	2432660	hill climbing optimization that spits out capabilities
2432660	2435460	that generalize far outside the training distribution.
2438060	2440700	Okay, so I think I understand that.
2442620	2447060	I don't know how likely it is that it's gonna happen.
2447060	2449740	I think you seem, I think you think
2449740	2451980	that piece is almost certain?
2451980	2452820	As it gets.
2452820	2455260	I think we're already, yeah, we're already seeing it.
2456260	2460300	As you grind these things further and further,
2460300	2462260	they can do more and more stuff,
2462260	2465060	including stuff they were never trained on.
2465060	2467660	Like we are, that was always the goal
2467660	2470500	of artificial general intelligence.
2470500	2474140	Like that's what artificial general intelligence meant.
2474140	2475340	That's what people in this field
2475340	2477860	have been pursuing for years and years.
2477860	2480140	That's what they were trying to do
2480140	2482220	when large language models were invented.
2483220	2485260	And they're starting to succeed.
2486900	2488260	Well, okay, I'm not sure.
2488260	2493260	Let me push back on that and you can try to persuade me.
2493500	2497420	So Brian Kaplan, a frequent guest here on Econ Talk,
2497420	2501620	gave, I think it was ChatGPT4, his economics exam
2501620	2503140	and it got a B.
2503140	2508140	And that's pretty impressive for just one stop
2508140	2511500	on the road to smarter and smarter chats,
2512260	2515380	but it wasn't a particularly good test of intelligence.
2515380	2517620	The number of the questions were things like,
2517620	2519340	what is Paul Krugman's view of this
2519340	2521340	or what is someone's view of that?
2521340	2523940	And I thought, well, that's kind of like a softball
2523940	2527460	for that's information, it's not thinking.
2527460	2532420	Steve Landsberg gave ChatGPT4 with the help of a friend
2532420	2534740	his exam and it got a four out of 90.
2534740	2536860	It got an F, like a horrible F
2536860	2538380	because they were harder questions,
2538380	2540860	not just harder, they required thinking.
2540860	2545340	So there was no sense in which the ChatGPT4
2545340	2549260	has any general intelligence, at least in economics.
2550220	2551460	You wanna disagree?
2551460	2554020	It's getting there.
2554020	2557660	Okay. You know, there's a saying that goes,
2557660	2560660	if you don't like the weather in Chicago, wait four hours.
2560660	2561500	Yeah.
2562420	2567020	So yeah, so ChatGPT is not going to destroy the world.
2567020	2569660	GPT4 is unlikely to destroy the world
2569660	2572900	unless the people currently eking capabilities out of it
2572900	2577060	take a much larger jump than I currently expect that they will.
2578340	2582020	But, you know, it's understand,
2582020	2586100	it may not be thinking about it correctly,
2586100	2591100	but it's understands the concepts and the questions,
2592500	2594580	even if it's not fair, you know,
2594580	2599580	you're complaining about that dog who writes bad poetry.
2599580	2600860	Right?
2600860	2604540	And like three years ago, you'd like just like spit out,
2604540	2607900	spit in these, you put in these economics questions
2607900	2609980	and you don't get wrong answers.
2609980	2613060	You get like gibberish or like maybe not gibberish
2613060	2616100	because three year old goes, I think we already had GPT3,
2616100	2620100	though maybe not as of April, but anyways.
2622300	2627420	Yeah. So it's moving along at a very fast clip.
2627460	2631340	The previous, you know, like GPT3 could not write code.
2631340	2633380	GPT4 can write code.
2636020	2637300	So how's it going to,
2637300	2638940	before I want to go to some other issues,
2638940	2642460	but how's it going to kill me when it has its own goals
2642460	2646780	and it's sitting inside this set of servers?
2646780	2648100	I don't know what sense it's sitting.
2648100	2649140	It's not the right verb.
2649140	2650100	We don't have verb for it.
2650100	2653460	It's hovering, it's whatever, it's in there.
2653460	2654940	How's it going to get to me?
2654940	2656780	How's it going to kill me?
2656780	2660180	If you are smarter, not just smarter
2660180	2661340	than an individual human,
2661340	2664260	but smarter than the entire human species,
2664260	2668660	and you started out on a server connected to the internet
2668660	2670420	because these things are always starting out
2670420	2672340	already on the internet these days,
2672340	2674700	which back in the old days that was stupid,
2675700	2680700	what do you do to make as many paper clips as possible?
2680980	2681820	Let's say.
2687780	2691380	I do think it's important to put yourself
2691380	2693100	in the shoes of the system.
2693100	2694300	Yeah, no, by the way,
2694300	2696180	one of my favorite lines from your essay,
2696180	2698380	I'm going to read it because I think it generalizes
2698380	2699380	to many other issues.
2699380	2702780	Say, to visualize a hostile superhuman AI,
2702780	2705020	don't imagine a lifeless book smart thinker
2705020	2706340	dwelling inside the internet
2706340	2709540	and sending ill-intentioned emails.
2709540	2712020	It reminds me of when people claim to think they can,
2712020	2713420	they know what Putin's going to do
2713420	2716300	because they've read history or whatever.
2716340	2718100	They're totally ignorant of Russian culture.
2718100	2719260	They have no idea what it's like
2719260	2721060	to have come out of the KGB.
2721060	2724140	That they're totally clueless and dangerous
2724140	2725940	because they think they can put themselves
2725940	2729140	in the head of someone there who's totally alien to them.
2729140	2732660	So I think that's generally a really good point to make
2732660	2737660	that putting ourselves inside the head of the paperclip
2738620	2741580	maximizer is not an easy thing to do
2741580	2743420	because it's not a human.
2743420	2745980	It's not like the humans you've met before.
2745980	2747580	That's a really important point.
2747580	2748620	Really like that point.
2748620	2751740	So why is that, explain why that's going to run amok?
2755980	2758900	I mean, I do kind of want you to just like,
2758900	2762300	take the shot at it, put yourself into the AI shoes,
2762300	2763900	try with your own intelligence
2763900	2767540	before I tell you the result of my trying with my intelligence.
2767540	2772380	How would you win from these starting resources?
2772380	2774220	How would you evade the tax?
2775220	2778220	So just to take a creepier,
2778220	2779940	much creeper example in the paper clips,
2779940	2782780	Eric Hall asked the chat GPT
2782780	2784460	to design an extermination camp,
2784460	2786660	which it gladly did quite well.
2786660	2788780	And you're suggesting it might actually, no?
2790940	2793100	Don't start from malice.
2793100	2793940	Okay.
2793940	2797500	Malice is implied by just wanting all the resources
2797500	2798780	of earth to yourself,
2798780	2801060	not leaving the humans around in case they could create
2801060	2802460	a competing super intelligence
2802500	2804300	that might actually be able to hurt you.
2804300	2806060	And just like wanting all the resources
2806060	2808540	and to organize them in a way that wipes out humanity
2808540	2809580	as a side effect,
2809580	2811220	which means the humans might want to resist,
2811220	2813460	which means you want the humans gone.
2813460	2815060	You're not doing it because somebody told you do it.
2815060	2817020	You're not doing it because you hate the humans.
2817020	2818460	You just want paper clips.
2818460	2819300	Okay. Tell me.
2819300	2820500	I'm not creative enough.
2820500	2821940	Tell me.
2821940	2822780	All right.
2823620	2828620	So, first of all, I want to appreciate why it's hard
2833740	2835500	for me to give an actual correct answer to this,
2835500	2837340	which is I'm not as smart as the AI.
2841260	2843580	Part of what makes a smarter mind deadly
2843580	2845580	is that it knows about rules of the game
2845580	2847100	that you do not know.
2847100	2849460	If you send an air conditioner back in time
2849460	2851300	to the 11th century,
2851340	2854700	even if you manage to describe all the plans for building it,
2854700	2856540	breaking it down to enough detail
2856540	2858820	that they can actually build a working air conditioner,
2858820	2861660	a simplified air conditioner, I assume,
2861660	2865540	they will be surprised when cold air comes out of it
2865540	2867260	because they don't know
2867260	2869740	about the pressure temperature relation.
2869740	2873620	They don't know you can compress air until it gets hot,
2873620	2878260	dump the heat into water or other air,
2879180	2880700	let the air expand again
2880700	2883220	and that the air will then be cold.
2883220	2885420	They don't know that's a law of nature.
2885420	2888620	So you can tell them exactly what to do
2888620	2890820	and they'll still be surprised at the end result
2890820	2892860	because it exploits a law of the environment
2892860	2893860	they don't know about.
2894940	2898100	If we're going to say the word magic means anything at all,
2898100	2899580	it probably means that.
2901460	2905380	Magic is easier to find in more complicated,
2905380	2908160	more poorly understood domains.
2908200	2911640	If you're literally playing logical tic-tac-toe,
2911640	2914800	not tic-tac-toe in real life on an actual game board,
2914800	2916720	where you can potentially go outside that game board
2916720	2919480	and hire an assassin to shoot your opponent or something,
2919480	2922280	but just like the logical structure of the game itself
2924120	2926440	and there's no timing of the moves.
2926440	2928920	The moves are just like made at exact discrete times,
2928920	2931800	you can't exploit a timing side channel.
2931800	2934440	Even a superintelligence may not be able to win against you
2934440	2935880	at logical tic-tac-toe
2936880	2940400	because the game is too narrow.
2940400	2942400	There are not enough options.
2942400	2945080	We both know the entire logical game tree,
2946160	2947840	at least if you're experienced at tic-tac-toe.
2947840	2949480	Yeah.
2949480	2953280	In chess, Stockfish 15 can defeat you
2953280	2958280	on a fully known game board with fully known rules
2958520	2961520	because it knows the logical structure
2961520	2963480	of the branching tree of games
2963480	2965760	better than you know that logical structure.
2965760	2967080	Great.
2967080	2969720	It can defeat you starting from the same resources,
2969720	2972960	equal knowledge of the rules.
2974840	2977600	Then you go past that.
2977600	2979760	And the way a superintelligence defeats you
2979760	2983240	is very likely by exploiting features of the world
2983240	2984960	that you do not know about.
2987440	2990000	There are some classes of computer security flaws,
2990040	2991880	like row hammer,
2992800	2997000	where if you flip a certain bit very rapidly
2997000	2998680	or at the right frequency,
2998680	3001080	the bit next to it in memory will flip.
3002120	3005320	So if you are exploiting a design flaw like this,
3005320	3008360	I can show you the code
3008360	3010360	and you can prove as a theorem
3010360	3012720	that it cannot break the security of the computer
3012720	3015360	assuming the chips works as design
3015360	3018520	and the code will break out of the sandbox
3018520	3021640	in any ways because it is exploiting
3021640	3024240	physical properties of the chip itself
3024240	3026360	that you did not know about.
3026360	3027960	Despite the attempt of the designers
3027960	3031000	to constrain the properties of that chip very narrowly,
3031000	3033360	that's magic code.
3033360	3038360	My guess as to what would actually be exploited to kill us
3038440	3039280	would be
3043360	3044200	this.
3046320	3047800	For those not watching on YouTube,
3047800	3050440	it's a copy of a book called Dano Systems.
3050440	3053880	But for those who are listening at home
3053880	3055520	rather than watching at home,
3055520	3057560	Eliezer, tell us why it's significant.
3059440	3064440	Yeah, so back when I first proposed this path,
3065680	3067880	one of the key steps was that a superintelligence
3067880	3071080	would be able to solve the protein folding problem.
3071080	3072360	And people were like,
3072360	3074360	Eliezer, how can you possibly know
3074360	3076120	that a superintelligence would actually be able
3076120	3078880	to solve the protein folding problem?
3078880	3081880	And I sort of like rolled my eyes a bit
3081880	3083960	and was like, well, if natural selection
3083960	3085720	can navigate the space of proteins
3085720	3089000	via random mutation to find other useful proteins
3089000	3090120	and the proteins themselves
3090120	3092160	fold up in reliable confirmations,
3093400	3096040	then that tells us that even though
3096040	3098560	it's we've been having trouble getting a grasp
3098560	3102880	on this space of physical possibilities so far
3102880	3104040	that it's tractable.
3104040	3105680	And people said like, what?
3105680	3107160	Like there's no way you can know
3107160	3108600	that superintelligence can solve
3108600	3109960	the protein folding problem.
3109960	3111760	Then AlphaFold2 basically cracked it,
3111760	3113240	at least with respect to the kind of proteins
3113240	3114360	found in biology.
3116520	3119000	Which I say to sort of like look back
3119000	3120560	at one of the previous debates here
3120560	3122080	and people are often like,
3122080	3125080	how can you know a superintelligence will do?
3125080	3126800	And then for some subset of those things
3126800	3128520	they have already been done.
3128520	3130920	So I would claim to have a good prediction track record there
3130920	3131880	although it's a little bit iffy
3131880	3134120	because of course I can't quite be proven wrong
3135440	3137120	without exhibiting a superintelligence
3137120	3138520	that fails to solve a problem.
3140640	3141480	Okay.
3143800	3148800	Proteins, why is your hand not as strong as steel?
3149200	3153200	We know that steel is a kind of substance that can exist.
3153200	3156480	We know that molecules can be held together as strongly
3156480	3158440	that atoms can be bound together as strongly
3158440	3160440	as the atoms in steel.
3160440	3163640	It seems like it would be an evolutionary advantage
3163640	3166240	if your flesh wears hard as steel.
3166240	3170320	You could like laugh at tigers at that rate, right?
3170320	3172600	Their claws are just gonna like scrape right off ya.
3172600	3176280	Assuming the tigers didn't have that technology themselves.
3176280	3178680	Why is your hand not as strong as steel?
3178680	3180520	Why has biology not bound together
3180520	3182600	the atoms in your hand more strongly?
3185720	3188920	Colin, what is your answer?
3191440	3193920	Well, it can't get to every,
3194920	3196920	it's their local maximums.
3196920	3200600	The national selection looks for things that work,
3200600	3201440	not for the best.
3201440	3204240	It does not, it doesn't make sense to look for the best.
3204240	3205960	You could disappear on that search.
3205960	3207000	That would be my crude answer.
3207000	3208240	How am I doing, doc?
3209600	3211800	Yeah, not terribly.
3211800	3216760	The answer I would give is that biology has to be evolvable.
3216760	3220280	Everything it's built out of has to get there as a mistake
3220280	3222440	from some other conformation,
3222440	3226400	which means that if it went down narrow potential,
3226400	3230480	pardon me, went down a steep potential energy gradients
3230480	3233100	to end up bound together very tightly,
3234000	3238560	designs like that are less likely to have neighbors
3238560	3240820	that are other useful designs.
3242040	3245280	And so your hands are made out of proteins
3245280	3248400	that fold up basically held together
3248400	3250640	by the equivalent of static cling,
3250640	3254880	van der Waals forces, rather than covalent bonds.
3254880	3257800	The backbone of protein chains,
3257800	3261260	the backbone of the amino acid change is a covalent bond,
3261260	3263280	but then it folds up and is held together
3263280	3265960	by static cling, static electricity.
3265960	3267440	And so it is soft.
3268760	3270120	Somewhere in the back of your mind,
3270120	3272000	you probably have a sense that,
3272960	3277420	that flesh is soft and animated by Alain Vital,
3277420	3280140	and it's like soft and it's not as strong as steel,
3280140	3284060	but it can heal itself and it can replicate itself.
3284060	3288140	And this is like the trade-off of our laws of magic,
3288140	3290340	that if you wanna heal yourself and replicate yourself,
3290340	3292860	you can't be as strong as steel.
3292860	3296380	This is not actually built into nature on a deep level.
3296380	3299660	It's just that the flesh evolved
3299660	3302540	and therefore had to go down shallow potential energy
3302540	3304260	gradients in order to be evolvable
3304260	3306580	and is held together by van der Waals forces
3306580	3308340	instead of covalent bonds.
3310140	3312940	I'm now going to hold up another book now,
3312940	3313740	book called
3317660	3318700	Nano Medicine
3319740	3324060	by Robert Freitas, instead of Nano Systems by Eric Drexler.
3325700	3330700	And people have done advanced analysis
3331980	3335300	of what would happen if you hadn't,
3335300	3338580	what would happen if you had an equivalent of biology
3338580	3343580	that met off covalent bonds instead of van der Waals forces?
3344300	3347260	And the answer we can like analyze on some detail
3347260	3351100	in our understanding of physics is, for example,
3351100	3355620	you could instead of carrying instead of red blood cells
3355620	3358900	that carry oxygen using weak chemical bonds,
3358900	3362420	you could have a pressurized vessel of corundum
3363420	3368300	that would hold 100 times as much oxygen per unit volume
3368300	3370340	of artificial red blood cells
3370340	3372940	with a 1000 fold safety margin
3372940	3375540	on the strength of the pressurized container.
3375540	3378740	There's vastly more room above biology.
3380460	3385020	So this is, and this is actually not even exploiting
3385020	3387780	laws of nature that I don't know.
3387780	3391180	It's the equivalent of playing a better chess
3391180	3394060	wherein you understand how proteins fold
3394060	3397020	and you design a tiny molecular lab
3397020	3399220	to be made out of proteins
3399220	3401420	and you get some human patsy
3401420	3403420	who probably doesn't even know you're an AI
3403420	3405380	because AIs are now smart enough.
3405380	3407100	This was, this has already been shown.
3407100	3409860	AIs now are smart enough that you ask them
3409860	3414860	to like hire a task rabbit to solve a capture for you.
3415380	3419260	And the task rabbit asks, are you an AI law?
3419260	3422140	The AI will think out loud,
3422140	3424540	like I don't want to know that I'm an AI.
3424540	3426660	I better tell something else
3426660	3429820	and then tell the humans that it has like a visual disability.
3429820	3432220	So it needs to hire somebody else to solve the capture.
3432220	3434020	This already happened,
3434020	3436220	including the part where it thought out loud.
3438220	3440700	Anyways, so you get your,
3440700	3443340	you order some proteins from an online lab,
3443340	3445980	you get your human who probably doesn't even know
3445980	3447900	you're an AI because why take that risk?
3447940	3449660	Although plenty of humans, it has,
3449660	3451260	well, we'll serve AIs willingly.
3451260	3454860	We also now know that AIs are advanced enough to even ask.
3458340	3461060	The human mixes the proteins in a beaker,
3461060	3464380	maybe puts in some sugar or a settling for fuel.
3464380	3467060	It assembles into a tiny little lab
3467060	3469060	that can accept further instruction,
3469060	3470780	acoustic instructions from a speaker
3470780	3473780	and maybe like transmit something back.
3474620	3478900	Tiny radio, tiny microphone.
3478900	3483100	I myself am not a superintelligence.
3483100	3485700	Run experiments in a tiny lab at high speed
3485700	3488380	because when distances are very small,
3488380	3489900	events happen very quickly.
3491420	3493900	Build your second stage nanosystems
3493900	3495500	inside the tiny little lab.
3496380	3498100	Build the third stage nanosystems,
3498100	3500220	build the fourth stage nanosystems,
3500220	3503620	build the tiny diamondoid bacteria
3503620	3505820	that replicate out of carbon, hydrogen,
3505820	3508740	oxygen, nitrogen as can be found in the atmosphere
3508740	3510660	powered on sunlight,
3510660	3513220	quietly spread all over the world.
3513220	3516180	All the humans fall over dead in the same second.
3516180	3518740	This is not how a superintelligence would defeat you.
3518740	3521420	This is how Eleazar Yudkowski would defeat you
3521420	3524820	if I wanted to do that, which to be clear, I don't.
3524820	3527900	And if I had the postulated ability
3527900	3531300	to better explore the logical structure
3531300	3533900	of the known consequences of chemistry.
3538860	3540260	Interesting, okay.
3545020	3549420	So let's talk about, that sounds sarcastic.
3549420	3550380	I didn't mean it's sarcastic,
3550380	3551980	but I think it's really interesting.
3554500	3556940	That interesting, man, I'm not capable.
3558540	3561060	My intelligence level is not high enough
3561060	3563260	to assess the quality of that argument.
3564860	3568260	What's fascinating, of course, is that
3572300	3573660	we could have imagined,
3574780	3577500	Eric Hall mentioned nuclear proliferation.
3577500	3580140	It's dangerous, nuclear proliferation,
3580140	3582820	up to a point in some sense it's somewhat healthy
3582820	3586980	in that it can be a deterrent under certain settings,
3587980	3591940	but the world could not restrain nuclear proliferation.
3591940	3595300	And right now it's trying to some extent
3595300	3599020	has had some success in keeping the nuclear club
3599020	3602060	with its current number of members for a while,
3602060	3604140	but it remains the case that nuclear weapons
3604140	3606300	are a threat to the future of humanity.
3608660	3611100	Do you think there's any way we can restrain
3611100	3615820	this AI phenomenon that's meaningful?
3615820	3620820	So you issued a clarion call, you sounded an alarm,
3623660	3627980	and mostly I think people shrugged it off, you know?
3627980	3629540	A bunch of people signed a letter,
3629540	3633540	26,000 people I think so far, signed the letter saying,
3633540	3635500	you know, we don't know what we're doing here,
3635500	3638460	this is uncharted territory, let's take six months off.
3638460	3640980	You were in peace and says, six months, are you crazy?
3640980	3644420	We need to stop this until we have an understanding
3644420	3645660	of how to constrain it.
3646500	3649100	Now that's a very reasonable thought to me,
3649100	3651060	but the next question would be,
3652220	3654060	how would you possibly do that?
3654060	3656620	In other words, I could imagine a world where,
3657540	3660620	if there were, let's say, four people who were capable
3660620	3662700	of creating this technology,
3662700	3664140	that the four people would say, you know,
3664140	3666540	we're playing with fire here, we need to stop,
3666540	3669180	let's make a mutual agreement, they might not keep it,
3669180	3670540	four people still a pretty big number,
3670540	3672540	but we're not a four people,
3672660	3674420	there are many, many people working on this,
3674420	3676580	there are many countries working on it.
3678260	3681180	Your peace did not, I don't think,
3681180	3683940	start an international movement of people going
3683940	3687220	to the barricades to demand that this technology
3687220	3688620	be put on hold.
3689500	3692020	How do we possibly, how do you sleep at night?
3692020	3694660	I mean, like, what should we be doing if you're right?
3697140	3698860	Or am I wrong, do people read this and go,
3698860	3701220	well, Elias Rieckowski thinks it's dangerous,
3701220	3703420	maybe we ought to be slowing down.
3703420	3706260	I mean, Sam, what's happened in the middle of the night
3706260	3710660	saying, thanks, Elias, I'm gonna put things on hold.
3710660	3712020	I don't think that happened.
3713740	3716500	I think you are somewhat underestimating the impact
3716500	3717900	and it is still playing out.
3720140	3723820	Okay, so like, mostly, it seems to me that if we wanted
3723820	3727580	to win this, we needed to start a whole lot earlier,
3727580	3729340	possibly in the 1930s.
3731260	3734820	But in terms of, like, my looking back
3734820	3737220	and like asking how far back you'd have to unwind history
3737220	3741100	to get us into a situation where this was survivable.
3744140	3747380	But leaving that aside.
3747380	3748460	I think that's a move.
3748460	3752500	Yeah, so in fact, it seems to me that the game board
3752500	3755460	has been played into a position where it is very likely
3755460	3756660	that everyone just dies.
3758500	3760700	If the human species woke up one day
3760700	3763020	and decided it would rather live,
3763020	3766340	it would not be easy at this point
3766340	3771340	to bring the GPU clusters and the GPU manufacturing
3771580	3773860	processes under sufficient control
3773860	3777460	that nobody built things that were too much smarter
3777460	3782460	than GPT-4 or GPT-5 or whatever the level just barely short
3782620	3784580	of lethal is, which we should not,
3784580	3786380	which we would not if we were taking this seriously,
3786380	3787940	get as close to as we possibly could,
3787940	3790860	because we don't actually know exactly where the level is.
3790860	3792540	But we would have to do more or less,
3792540	3797540	is have international agreements that were being enforced
3797980	3799900	even against parties not part,
3799900	3802860	even against countries not party to that national agreement,
3802860	3805620	international agreement, if it became necessary,
3805620	3809180	you would be wanting to track all the GPUs.
3809180	3812360	You might be demanding that all the GPUs call home
3812360	3814860	on a regular basis or stop working.
3814860	3816500	You'd want to tamper proof them.
3818220	3822700	If intelligence said that a rogue nation was,
3822700	3826420	had bought, somehow managed to buy a bunch of GPUs
3826420	3829260	despite arms controls and defeat the tamper proofing
3829260	3831380	on those GPUs, you would have to do it was necessary
3831380	3832580	to shut down the data center,
3832580	3834620	even if that led to a shooting war between nations,
3834620	3836860	even if that country was a nuclear country
3836860	3839260	and had threatened nuclear retaliation.
3839260	3841460	The human species could survive this if it wanted to,
3841460	3843460	but it would not be business as usual.
3844560	3847180	It is not something you could do trivially.
3847180	3850020	So when you say I may have underestimated it,
3850020	3852220	did you get people writing and saying,
3852220	3855380	you know, I wasn't, and I don't mean people like me.
3855380	3856860	I mean, people, players.
3856860	3858980	Did you get people who are playing in this sandbox
3858980	3861660	to write you and say, you've scared me.
3861660	3863660	I think we need to take this seriously?
3866620	3869940	Without naming names, I'm not asking for that.
3869940	3871860	At least one US Congressman.
3871860	3872700	Okay.
3874460	3876300	It's a start, maybe.
3876300	3878660	You know, one of the things that a common response
3878660	3881100	that people get when you talk about this is that,
3881100	3882180	well, the last thing I do is,
3882180	3883740	last thing I want is the government controlling
3883740	3885060	whether this thing goes forward or not,
3885060	3888460	but it'd be hard to do without some form of lethal force
3888460	3889300	as you comply.
3889300	3894300	I spent 20 years trying desperately to have there be
3894740	3899660	any other solution to have these things be alignable,
3899660	3901100	but it is very hard to do that
3901100	3905540	when you are nearly alone and under resourced
3905540	3908420	and the world has not made this a priority.
3908420	3911180	And future progress is very hard to predict.
3912820	3915660	I don't think people actually understood the research program
3915660	3918980	that we were trying to carry out, but yeah.
3918980	3922780	So I sure wanted there to be any other plan than this
3922780	3924600	because now that we've come to this last resort,
3924600	3927020	I don't think we actually have that last resort.
3927020	3928700	I don't think we have been reduced
3928700	3930820	to a last-ditch backup plan that actually works.
3930820	3932960	I think we all just die.
3932960	3936960	And yet nonetheless, here I am,
3936960	3940320	like putting aside doing that thing
3940320	3943680	that I wouldn't do for almost any other technology,
3943680	3945800	except for maybe gain of function research
3945800	3949800	on biological pathogens
3951480	3954160	and advocating for government interference.
3954160	3957280	Because in fact, like if the government comes in
3957280	3958760	and wrecks the whole thing,
3958760	3961000	that's better than the thing
3961000	3962680	that was otherwise going to happen.
3962720	3964400	Because it's not based on the government coming in
3964400	3966400	and being like super competent
3966400	3968480	and directing the technology exactly directly.
3968480	3971120	It's like, okay, this is going to kill literally everyone
3971120	3972920	if the government stomps around
3972920	3977240	and like the dangerous of the government.
3977240	3978600	It's one of those very rare cases
3978600	3979920	where the dangerous that the government
3979920	3982640	will interfere too little rather than too much.
3982640	3983480	Possibly.
3984600	3987680	Let's close with a quote from Scott Ericsson,
3988560	3989720	which found on his blog.
3989720	3991360	We'll put a link up to the post.
3991360	3996280	Very interesting defensive of AI.
3996280	3999320	Scott's a University of Texas computer scientist.
3999320	4001440	He's working at OpenAI.
4001440	4003560	He's on leave, I don't, I think for a year, maybe longer.
4003560	4005480	I don't know, it doesn't matter.
4005480	4007160	He wrote the following.
4007160	4009560	So if we ask the directly relevant question,
4009560	4011720	do I expect the generative AI race,
4011720	4015320	which started in earnest around 2016 or 2017,
4015320	4017000	with the founding of OpenAI
4017000	4020840	to play a central causal role in the extinction of humanity?
4020840	4024520	I'll give a probability of around 2% for that.
4024520	4026320	And I'll give a similar probability,
4026320	4028520	maybe even a higher one for the generative AI race
4028520	4032560	to play a central causal role in the saving of humanity.
4032560	4034000	All considered then,
4034000	4035400	I come down in favor right now,
4035400	4038400	proceeding with AI research with extreme caution,
4038400	4039360	but proceeding.
4041320	4044600	My personal reaction is that is insane.
4044600	4046380	I have very little, I'm serious.
4046380	4048920	I find that deeply disturbing
4048960	4051480	and I'd love to have him on the program to defend it.
4051480	4054160	I don't think there's much of a chance
4054160	4056280	that generative AI would save humanity.
4056280	4057840	I'm not quite sure for what it's,
4060120	4060960	he's worried about,
4060960	4064960	but if you're telling me there's a 2%, 2% chance
4064960	4066200	that it's gonna destroy all humans
4066200	4067960	and you obviously think it's higher,
4067960	4070240	but 2% is really high to me
4070240	4072120	for an outcome that's rather devastating.
4072120	4076000	It's one of the deepest things I've learned from Nassim Tala.
4076000	4078320	It's not just the probability,
4078320	4080520	it's the outcome that counts too.
4080520	4084760	So this is ruined on a colossal scale
4084760	4087800	and the one thing you wanna do is avoid ruin
4087800	4091600	so you can take advantage of more draws from the earn.
4091600	4094800	The average return from the earn is irrelevant
4094800	4096800	if you are not allowed to play anymore.
4096800	4098800	You're out, you're dead, you're gone.
4098800	4101080	So you're suggesting we're gonna be out and dead and gone,
4101080	4103120	but I want you to react to Scott's quote.
4104120	4107880	Um, 2% sounds great.
4107880	4111200	Like 2% is plausibly within the range of like
4111200	4114800	the human species destroying it itself by other means.
4114800	4117520	I think that the disagreement I have with Scott Aronson
4117520	4121960	is simply about the probability that AI is alignable
4121960	4125720	with the frankly half-fazard level
4125720	4128080	that we have put into it and the half-fazard level
4128080	4130800	that is all humanity is capable of
4131680	4133200	as far as I can tell,
4133200	4136360	because the core lethality here
4136360	4137560	is that you have to get something right
4137560	4139560	on the first try or it kills you
4139560	4141400	and getting something right on the first try
4141400	4143560	when you do not get like infinite free retries
4143560	4145800	as you usually do in science and engineering
4145800	4149640	is an insane ask, insanely lethal ask.
4149640	4153000	My reaction is fundamentally that 2% is too low.
4153000	4154680	If I take it at face value,
4154680	4156920	then 2% is within range of the probability
4156920	4159120	of humanity wiping itself out by something else
4159120	4161640	where if you assume that AI alignment is free,
4161640	4163760	that AI alignment is easy,
4163760	4166400	that you can get something that is smarter than you,
4166400	4168280	but on your side and helping,
4169880	4172880	2% chance of risking everything does appear to me
4172880	4177040	to be commensurate with the risks from other sources
4177040	4180120	that you could shut down using the superintelligence.
4180120	4181120	It's not 2%.
4182240	4187600	So, the question then is,
4189600	4192600	what would Scott Aaronson say if he heard your,
4192600	4194480	I mean, he's heard, he's read your piece presumably,
4194480	4197960	he understands your argument about wealthiness.
4197960	4200000	I should just clarify for listeners,
4200000	4204120	alignment is the idea that AI could be constrained
4204120	4206920	to serve our goals rather than its goals.
4206920	4208360	Is that a good summary?
4210120	4211600	I wouldn't say constrained.
4211600	4214760	I would say built from scratch to want those things
4214760	4216560	and not want otherwise.
4216560	4217760	So that's really hard
4217800	4219160	because we don't understand how it works.
4219160	4221000	That would be, I think, your point.
4221000	4222320	And tell me then what Scott's-
4222320	4223640	On the first try.
4223640	4225080	Yeah, on the first try.
4225080	4228200	So what would Scott say when you tell him,
4228200	4231320	but it's gonna develop all these side desires
4231320	4233080	that we can't control.
4233080	4233960	What's he gonna say?
4235480	4236320	Why is he not worried?
4236320	4238240	Why is he still, why is he quit his job?
4239480	4240760	And not Scott.
4240760	4242880	People in the, let's get away from him personally,
4242880	4246800	but people in general, there's dozens and maybe hundreds,
4246800	4248400	maybe a thousand, I don't know,
4248400	4251040	extraordinarily intelligent people
4251040	4253600	who are trying to build something even more intelligent
4253600	4254920	than they are.
4254920	4257600	Why are they not worried about what you're saying?
4257600	4259720	They've all got different reasons.
4259720	4262320	Scott's is that he thinks that intelligence,
4262320	4265440	that he observes intelligence makes humans nicer.
4265440	4268560	And though he wouldn't phrase it exactly this way,
4268560	4271760	this is basically what Scott said on his blog.
4271760	4273960	To which my response is intelligence
4273960	4275400	does have effects on humans,
4275400	4278560	especially humans who start out relatively nice.
4278560	4281000	And when you're building AI's from scratch,
4281000	4283240	you're just like in a different domain with different rules.
4283240	4286000	And you're allowed to say that it's hard to build AI's
4286000	4289640	that are nice without implying that making humans smarter,
4289640	4292640	like humans start out in a certain frame of reference.
4292640	4294600	And when you apply more intelligence to them,
4294600	4297160	they move within that frame of reference.
4297160	4299480	And if they start out with a small amount of nicest,
4299480	4301800	the intelligence can make them nicer.
4301800	4304080	They can become more empathetic.
4304080	4307280	If they start out with some empathy,
4307280	4308920	they can develop more empathy
4308920	4310840	as they understand other people better,
4310840	4314920	which is intelligence to correctly model other people.
4314920	4317240	That is even more insane.
4317240	4321800	I'm not going to, I haven't read that blog post
4321800	4323080	and we'll put a link up to it.
4323080	4324680	I hope you'll share it with me.
4324680	4327520	But again, not attributing it to Scott
4327520	4329040	since I haven't seen it.
4329040	4333120	And assuming that you've said this fairly and correctly,
4333120	4335560	the idea that more intelligent people are nicer
4335560	4337880	is one of the most,
4337880	4340480	that'd be very hard to show with the evidence for that.
4340480	4341320	That is an appalling.
4341320	4342640	I don't think it's like,
4342640	4344560	it is not a universal law on humans.
4344560	4345400	No.
4345400	4347680	It is a thing that I think is true of Scott.
4347680	4349760	I think if you made Scott Aaron Sinclair smarter,
4349760	4350680	he'd get nicer.
4352320	4355040	And I think he's inappropriately generalizing from that.
4355040	4359840	There is a scene in Schindler's List.
4359840	4362320	The Nazis, I think they're in the Warsaw ghetto
4362320	4365280	and they're race, a group of Nazis are racing,
4365280	4366320	I think they're in the SS,
4366320	4369400	they're racing through a tenement.
4369400	4371240	And it's falling apart
4371240	4373640	because the ghetto is falling apart.
4373640	4377600	But one of the SS agents sees a piano
4379040	4380800	and he can't help himself.
4380800	4383200	He sits down and he plays Bach or something.
4383200	4385080	I think it was Bach.
4385080	4386280	And I always found it interesting
4386280	4389560	that Spielberg put that in or whoever wrote the script.
4389560	4391160	And I think it was pretty clear why they put it in.
4391160	4392680	They wanted to show you that
4392680	4395440	having a very high advanced level of civilization
4395440	4398720	does not stop people from treating other people,
4398720	4400600	other human beings like animals,
4400600	4402580	or worse than animals in many cases,
4404000	4408720	and exterminating them without conscience.
4408720	4411640	So I don't share that view of anyone's
4411640	4414640	that intelligence makes you a nicer person.
4414640	4416680	I think that's not the case,
4416680	4418960	but perhaps Scott will return to this,
4418960	4421360	will come to this program and defend that view
4421360	4422880	if he did holes it.
4422880	4425880	I think you are underweighting the evidence
4425880	4428880	that has convinced Scott of the thing that I think is wrong.
4428880	4432400	I think if you suddenly started augmenting the intelligence
4432400	4435000	of the SS agents from Nazi Germany,
4435000	4439640	then somewhere between 10% and 90% of them
4439640	4442200	would go over to the cause of good.
4442200	4445280	Because there were factual falsehoods
4445280	4450240	that were pillars of the Nazi philosophy,
4450240	4453240	and that people would reliably stop believing
4453240	4454480	as they got smarter.
4454480	4456040	That doesn't mean that they would turn good,
4456040	4457280	but some of them would have.
4457280	4458200	Is it 10%?
4458200	4459040	Is it 90%?
4459040	4459880	I don't know.
4459880	4462960	It's not my experience with the human creature.
4464000	4465240	You've written some very interesting things
4465240	4466640	on rationality of a beautiful essay,
4466640	4471480	we'll link to on 12 rules for rationality.
4471480	4473400	In my experience, it's a very small portion
4473400	4475760	of the population that behaves that way.
4478600	4480520	There's a quote from Nassim Taleb,
4480520	4483040	we haven't gotten to yet in this conversation,
4483040	4486200	which is bigger data, bigger mistakes.
4486200	4487560	I think there's a belief generally
4487560	4489600	that bigger data fewer mistakes,
4489600	4490840	but Taleb might be right,
4490840	4492680	and it's certainly not the case in my experience
4492680	4496520	that bigger brains, higher IQ means better decisions.
4496520	4498600	This is not my experience.
4498600	4503120	Then you're not throwing enough intelligence
4503120	4504080	at the problem.
4505920	4507960	Literally not just decisions
4507960	4510120	that you disagree with the goals,
4510120	4512680	but false models of reality,
4514040	4516480	models of realities so blatantly mistaken
4516480	4518000	that even you, a human,
4518000	4520480	can tell that they're wrong and in which direction.
4520480	4521920	These people are not smart the way
4521920	4526920	that a hypothetical weak efficient market is smart.
4527280	4529320	You can tell they're making mistakes
4529320	4530760	and you know in which direction.
4530800	4532800	They're not smart the way that Stockfish 15
4532800	4534080	is smart in chess.
4534080	4536320	You can play against them and win.
4536320	4541320	These are, the range of human intelligence
4541320	4542440	is not that wide.
4542440	4545560	It caps out at like John von Neumann or whatever.
4545560	4549280	And that is not wide enough to open up
4549280	4552480	that humans would be epistemic,
4552480	4555240	that these beings would be epistemically
4555240	4558120	or instrumentally efficient relative to you.
4558120	4559560	It is possible for you to know
4559560	4561840	that one of their estimates is directionally mistaken
4561840	4563360	and to know the direction.
4563360	4565520	It is possible for you to know an action
4565520	4567360	that serves their goals better
4567360	4569080	than the action that they generated.
4569080	4570680	And is it striking how hard it is
4570680	4573680	to convince some of that even though they're thinking people?
4575040	4579080	History is, I just have a different perception maybe.
4579080	4580880	To be continued, Eliezer.
4582200	4584760	My guest today has been Eliezer Yudkowski.
4584760	4586960	Eliezer, thanks for being part of Econ Talk.
4587920	4589120	Thanks for having me.
4589200	4594400	This is Econ Talk, part of the Library of Economics and Liberty.
4594400	4596920	For more Econ Talk, go to econtalk.org
4596920	4599280	where you can also comment on today's podcast
4599280	4602840	and find links and readings related to today's conversation.
4602840	4605840	The sound engineer for Econ Talk is Rich Goyette.
4605840	4607640	I'm your host Russ Roberts.
4607640	4608840	Thanks for listening.
4608840	4610360	Talk to you on Monday.
