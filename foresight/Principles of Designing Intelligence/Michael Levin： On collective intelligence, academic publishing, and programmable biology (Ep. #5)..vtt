WEBVTT

00:00.000 --> 00:12.760
This podcast is about understanding how collaborative learning networks work and can be designed.

00:12.760 --> 00:20.640
It's part of our effort to describe a methodology, which we call Unify, that can help align human

00:20.640 --> 00:25.120
learning principles, which come from learning science, machine learning principles, and

00:25.120 --> 00:30.160
also how to think about organizations as networks, how to be strategic in how networks

00:30.160 --> 00:38.980
are designed, and what data to collect from networks in order to create new kinds of intelligence.

00:38.980 --> 00:45.720
Our next guest is, in my mind, the authority, and I can't believe I got this interview,

00:45.720 --> 00:48.240
in the field of collective intelligence.

00:48.240 --> 00:53.800
If you haven't seen Michael Levin's TED Talk, it's amazing, I highly recommend it.

00:53.800 --> 01:01.360
This conversation did not go as expected, because I got, frankly, a lot of really good

01:01.360 --> 01:08.720
advice from, frankly, someone who was trying to help me out in terms of how can I propel

01:08.720 --> 01:11.760
my work forward, and for that I'm personally grateful.

01:11.760 --> 01:13.240
I hope you enjoy this.

01:13.240 --> 01:20.920
Thank you so much.

01:20.920 --> 01:25.160
Everything we do is related to this question of embodied minds, so I'm interested in how

01:25.160 --> 01:30.760
very diverse kinds of intelligence can exist in our universe, in all sorts of different

01:30.760 --> 01:34.960
manifestations, different scales, both of space and time.

01:34.960 --> 01:41.560
We use a combination of computer science, developmental biology, biophysics, behavioral

01:41.560 --> 01:48.600
science, computer science, to really try to understand how different degrees of agency

01:48.600 --> 01:53.640
can be implemented in different embodiments.

01:53.640 --> 02:00.760
We have parts of the lab are very theoretical, and do conceptual kinds of models in almost

02:00.760 --> 02:05.480
philosophy, and in other parts of the bright code, and produce various tools and software,

02:05.480 --> 02:09.920
and in other parts do various applications of these ideas.

02:09.920 --> 02:14.840
We have applications in birth defects, and regenerative medicine, and cancer, and we

02:14.840 --> 02:17.960
have some things in AI, and synthetic bioengineering, and so on.

02:17.960 --> 02:23.080
We run the spectrum from very fundamental conceptual things to very practical things

02:23.080 --> 02:27.600
that we hope will end up in the clinic at some point.

02:27.600 --> 02:30.200
My background originally was computer science.

02:30.200 --> 02:34.360
I did software engineering for a long time, scientific programming.

02:34.360 --> 02:40.160
I got a degree in genetics after that, and I've been running a biology lab in the Allen

02:40.160 --> 02:47.920
Discovery Center ever since.

02:47.920 --> 02:54.680
Our goal is to create a collaborative intelligence framework, something that is reliable and robust,

02:54.680 --> 02:57.760
a playbook to give to organizational leaders.

02:57.760 --> 03:06.200
We don't believe that the latest AI algorithm or technology trend that needs to be implemented

03:06.200 --> 03:08.080
is the solution.

03:08.080 --> 03:14.880
We believe that there needs to be an understanding of the guiding principles of how do you design

03:14.880 --> 03:17.440
collaborative intelligence systems?

03:17.440 --> 03:23.760
How do you apply them with minimal or no technology?

03:23.760 --> 03:29.400
Because of the lack of formal definitions, we're hoping we can draw inspiration from

03:29.400 --> 03:37.160
the field, biology, and I'm wondering if there are any principles around collective

03:37.160 --> 03:43.680
intelligence that you feel might be applicable to the domain of business?

03:43.680 --> 03:48.960
Well, I guess because I'm not in this field, I could use a little more guidance as to what

03:48.960 --> 03:51.200
the best case outcome would be.

03:51.200 --> 03:56.160
Can you paint a picture for me of what you want to have happen?

03:56.160 --> 03:57.560
What are we trying to improve?

03:57.560 --> 03:59.640
What are we trying to achieve?

03:59.640 --> 04:02.280
How would you recognize success if you cracked this problem?

04:02.280 --> 04:04.880
What would it look like?

04:04.880 --> 04:13.160
We would like to be able to model an organization using a paradigm from reinforcement learning

04:13.240 --> 04:18.440
that doesn't mean we're going to want to use reinforcement learning, but the idea is to

04:18.440 --> 04:25.400
track agents taking actions in their environments and measuring the outcomes in this case that

04:25.400 --> 04:33.280
can be applied to human sequences of decisions or machine or humans and machines working

04:33.280 --> 04:36.640
together collaboratively.

04:36.640 --> 04:46.480
And one problem we have is that we're applying a reinforcement learning way of thinking to

04:46.480 --> 04:48.040
the macro world.

04:48.040 --> 04:52.680
It's not just applying it to code with a set of data.

04:52.680 --> 05:00.440
It's trying to create a virtual twin of an organization and applying that way of thinking

05:00.440 --> 05:05.160
from reinforcement learning to organizational design.

05:05.160 --> 05:15.280
I think success is going to be having a framework and a methodology, but also being able to implement

05:15.280 --> 05:20.840
it in terms of, let's say, an API, a set of standards.

05:20.840 --> 05:29.200
And step one really is to investigate what are some principles around collaborative intelligence

05:29.280 --> 05:33.400
that work that can be applied to organizations.

05:33.400 --> 05:39.760
I think we're really at the beginning and I don't know if we have even defined yet what

05:39.760 --> 05:42.200
success is.

05:42.200 --> 05:48.760
I like to work backwards and I like to imagine the future that you want to see.

05:48.760 --> 05:49.880
So what does that mean?

05:49.880 --> 05:54.720
Does that mean the organization is functioning more efficiently towards specific goals?

05:54.720 --> 05:58.000
Does that mean something about what those goals are?

05:58.000 --> 06:04.600
Does that mean something about the individual happiness of the people participating in those

06:04.600 --> 06:05.600
goals?

06:05.600 --> 06:11.360
Does that mean something about developing some kind of dominant paradigm that pushes out

06:11.360 --> 06:14.600
competitive views of some particular field?

06:14.600 --> 06:20.760
I think step one is figuring out what, if you did, if you already had a successful theory

06:20.760 --> 06:24.440
of all this stuff and you were able to put it into practice, what would the implications

06:24.440 --> 06:33.720
be for reality, for how people run these things in the real world?

06:33.720 --> 06:39.280
And so I don't know what the answer to this is, but I guess I can talk a little bit about

06:39.280 --> 06:47.320
what we see in biology and then I don't love just blindly taking things from biology and

06:47.320 --> 06:51.560
pushing them into, I mean, people often would like to do this, pushing them into social

06:51.560 --> 06:58.240
and various kinds of societal contexts, but I think it's better to work backwards and

06:58.240 --> 07:01.560
ask what we're trying to achieve and what that looks like.

07:01.560 --> 07:04.320
I find in general that's missing from a lot of the discussions.

07:04.320 --> 07:08.720
A lot of people have critiques about things that are going on now, whether it's AI or

07:08.720 --> 07:12.240
whether it's something else, they can see all the problems and it's going to lead to

07:12.240 --> 07:14.280
this, it's going to lead to that, we don't want it.

07:14.280 --> 07:17.720
What I don't see as much as people articulating, what do you want to see?

07:17.720 --> 07:24.240
What does future humanity look like that avoids these kinds of things that people are concerned

07:24.240 --> 07:25.240
about?

07:25.240 --> 07:32.760
With respect to all of the parameters that are currently under debate and I think that's

07:32.760 --> 07:38.520
what I would do here as well as I would ask, what does the future of an optimal successful

07:38.520 --> 07:40.200
organization look like?

07:40.200 --> 07:41.200
How do you recognize it?

07:41.200 --> 07:50.600
What are we aiming for?

07:50.600 --> 07:55.480
But having said all that, I could tell you some things we learned from the biology.

07:55.480 --> 08:01.640
One thing we learned from the biology is that one reason biology is so successful is that

08:01.640 --> 08:05.920
it often changes the goals.

08:05.920 --> 08:11.120
Pretty much like with artificial life and this notion of perverse instantiation, which

08:11.120 --> 08:15.440
is you think you're trying to solve a problem in a particular way and if your system is

08:15.440 --> 08:21.000
flexible enough, it might do something completely different that on retrospect, you can see

08:21.000 --> 08:24.840
how that would solve the problem, but it isn't at all what you were looking for.

08:24.840 --> 08:28.640
Biology does this all the time, when faced with a difficult problem, one thing biology

08:28.640 --> 08:32.200
sometimes does is switch to a different problem.

08:32.200 --> 08:37.200
If you look at the biosphere, there's every possible way of making a living and biology

08:37.200 --> 08:41.720
is never tied to finding an answer to a specific problem.

08:41.720 --> 08:46.880
They often change, it often changes the problem that it tries to solve and so that may or

08:46.880 --> 08:52.520
may not work for us because you may come up with an organizational structure that is

08:52.520 --> 08:56.640
very good at let's say perpetuating itself, but one of the ways that might do that is

08:56.640 --> 09:02.200
to go off, completely go off script as far as where you thought the organization was

09:02.200 --> 09:06.600
going to go and that may not, that works in the biosphere if your goal is to make sure

09:06.600 --> 09:12.680
that life survives in some particular, you know, on a planet, that may not be that ability

09:12.680 --> 09:17.360
to just completely change goals may not be what you want for an organization.

09:17.360 --> 09:25.280
The other thing is that when you, biological agents and those could be cells, they could

09:25.280 --> 09:31.160
be molecular networks, they could be tissues, organs, whatever, they're very good at combining

09:31.160 --> 09:39.280
into higher level entities that are these emergent selves that do things in other problem spaces.

09:39.280 --> 09:43.120
But they're not, there's a few things that are not guaranteed when that happens.

09:43.120 --> 09:49.000
One thing that's not guaranteed is that the emergent self is smarter than the components.

09:49.000 --> 09:53.360
It doesn't have to be, sometimes it certainly is, but it doesn't have to be and so that's

09:53.440 --> 09:57.640
no guarantee, you know, scaling up is no guarantee that it's going to be more intelligent.

09:57.640 --> 10:10.320
Also, when you create these novel selves, you simultaneously create various goals, preferences

10:10.320 --> 10:20.520
and various kinds of salience and valence for these things that don't necessarily match

10:20.520 --> 10:22.040
those of the subunits.

10:22.040 --> 10:27.960
So in other words, people often say, you know, people often hear the story, I tell of gap

10:27.960 --> 10:32.400
junctions and cancer, this idea that cells are tied together with gap junctions that

10:32.400 --> 10:36.480
basically erase their individuality a little bit so that they're all part of this collective

10:36.480 --> 10:38.720
and they work on making the organs and whatnot.

10:38.720 --> 10:43.000
And then when there's a breakdown of that, cells roll back to the kind of unicellular

10:43.000 --> 10:46.320
form and they go off and they metastasize and so on.

10:46.320 --> 10:52.000
And so people hear that and often they say, well, that's clear what we have to do for

10:52.000 --> 10:55.920
the various ills of society, you know, we can address that the whole selfishness thing

10:55.920 --> 10:56.920
and so on.

10:56.920 --> 11:01.360
We'll just sort of gap junctions ourselves, take it to each other in some format and when

11:01.360 --> 11:05.640
wipe our individuality a little bit and we'll be this collective.

11:05.640 --> 11:11.640
I think that's a, in general, that's a terrible idea because one of the clear things is that

11:11.640 --> 11:18.040
when you as a large scale individual, let's say a human goes off and does various things.

11:18.040 --> 11:20.880
My silly example is rock climbing.

11:20.880 --> 11:24.560
You know, you go rock climbing and you have a lovely day and you meet various social goals,

11:24.560 --> 11:28.480
you meet various personal happiness goals and so on as a human.

11:28.480 --> 11:32.520
But you've left a bunch of skin cells on that, you know, on that rock.

11:32.520 --> 11:36.360
And nobody asked those skin cells whether this was something that they were interested

11:36.360 --> 11:37.360
in.

11:37.360 --> 11:42.600
And just in general, the basic premise of having a multicellular body is that the vast majority

11:42.600 --> 11:48.840
of your body cells are going to die in the service of whatever weird to them incomprehensible

11:48.840 --> 11:51.600
goals that the collective human might have.

11:51.600 --> 12:00.080
So that stuff is very easily scaled up into social structures where you might have persistent,

12:00.080 --> 12:04.500
you know, persistent kinds of government, persistent kinds of companies and so on that

12:04.500 --> 12:09.760
pursue goals that are at odds with the well-being, the happiness and the goals of the beings

12:09.760 --> 12:11.760
that make them up.

12:11.760 --> 12:18.320
And that needs to be, it needs to be clear that when you scale up like that, you are

12:18.320 --> 12:19.320
not guaranteed.

12:19.320 --> 12:23.800
In fact, the big thing is that we don't have a good science of being able to anticipate

12:23.800 --> 12:26.000
the goals of collective systems.

12:26.000 --> 12:30.880
We really can't anticipate that very well now at all.

12:30.880 --> 12:40.480
I talk about these biology examples all the time in terms of like, we make something called

12:40.480 --> 12:44.720
a frog allotle in our lab and so this is part frog and part axolotl.

12:44.720 --> 12:50.640
And so baby axolotls have little forelegs and tadpoles don't have forelegs.

12:50.640 --> 12:55.880
And so when we make this frog allotle, I often say this in my talk, you know, we know the

12:55.880 --> 12:58.720
genome for the frog, we know the genome for the axolotl.

12:58.720 --> 13:02.040
Can you tell me if a frog allotle is going to have legs or not, right?

13:02.040 --> 13:04.400
And there's no one can predict that up front.

13:04.400 --> 13:08.720
And that's because while we understand the molecular hardware to some extent of the

13:08.720 --> 13:11.880
pieces, we do not understand how the collective decisions are made.

13:11.880 --> 13:16.280
We don't know how they decide to make these large scale structures and to say that, you

13:16.280 --> 13:18.040
know, there's a million examples of that.

13:18.040 --> 13:22.320
So that's the thing when we make these, when we make these collectives, we really don't

13:22.320 --> 13:27.400
know what the goal of the, if there is going to be an emergent agent, we don't know what

13:27.400 --> 13:28.400
the goal is going to be.

13:28.400 --> 13:34.320
And we don't know if it at all matches our goals, even if we, I mean, even our own, we

13:34.320 --> 13:38.040
don't match goals with each other, but never mind, or even if never mind that.

13:38.040 --> 13:39.040
Yeah.

13:39.040 --> 13:41.240
So, so I think, so I think that's the danger, right?

13:41.240 --> 13:46.120
So I think biology can teach us some very successful principles about making sure you

13:46.120 --> 13:51.880
stick around, but I'm not sure that's the end all and be all that we want from the structures

13:51.880 --> 13:53.320
that we build, right?

13:53.320 --> 13:54.600
Persistence isn't really what we're looking for.

13:54.600 --> 13:56.480
We're looking for something much more specific.

13:56.480 --> 14:06.160
So you got to, you got to take all that biology stuff with a, with a grain of salt.

14:06.160 --> 14:10.240
So one of my favorite things as a consultant is that I get to speak with individuals where

14:10.280 --> 14:15.800
very aware of their problems, very aware of goals and the financial value of their goals.

14:15.800 --> 14:21.440
But when it comes time to connect the dots between these different teams and more of

14:21.440 --> 14:26.560
a collective intelligence or collaborative intelligence approach, typically there's

14:26.560 --> 14:33.760
no person in charge or there's no system or process or computational way that combines

14:33.800 --> 14:36.880
them all and analyzes them.

14:36.880 --> 14:45.960
And I found this gap to be really intriguing and I want to pursue formalizing a process

14:45.960 --> 14:50.360
that could be something that we could publish and like, let's say an academic journal or

14:50.360 --> 14:56.400
paper to help push this area of research forward.

14:57.400 --> 15:04.240
And I think the one challenge I've had is like simply defining, you know, what is collective

15:04.240 --> 15:08.120
intelligence for an organization, what is collaborative intelligence, because I hear

15:08.120 --> 15:14.200
different people, different startups talking about those two and, and yet it's all very

15:14.200 --> 15:15.200
fuzzy.

15:15.200 --> 15:25.360
So I thought that trying to pursue a more formalized definition that respected kind

15:25.400 --> 15:30.640
of academic perspectives would be a good approach for us to have.

15:31.120 --> 15:32.120
Yeah.

15:32.320 --> 15:33.760
You know, I don't know.

15:33.760 --> 15:35.040
That's that's a whole other thing.

15:35.040 --> 15:40.040
This this like meta meta goal seeking, which is what what should the goals be?

15:40.040 --> 15:44.280
So so gaining respect of the academic community.

15:44.280 --> 15:51.000
I'm not even sure that's an achievable goal per se for anybody because there is no monolithic

15:51.000 --> 15:52.000
academic community.

15:52.040 --> 15:59.120
So there are different islands of academics where like so so so there's a there's a particular

15:59.120 --> 16:04.040
kind of standard talk that I sometimes give and I always know what kind of department

16:04.040 --> 16:07.960
I'm giving the talking based on which part it makes people angry and it's often a different

16:07.960 --> 16:11.920
it's always a different part because there are certain things you can say in a in a neurobiology

16:11.920 --> 16:13.520
department and everybody's like, yeah, no kidding.

16:13.520 --> 16:17.440
So what we knew that and the exact same thing in the genetics department and people throw

16:17.480 --> 16:18.480
tomatoes.

16:18.480 --> 16:26.760
And so trying for kind of generic acceptance, I'm not sure that's feasible until until we

16:26.760 --> 16:34.440
get a lot better at breaking down these these kind of conceptual silos that the communities

16:34.440 --> 16:35.440
are in.

16:35.440 --> 16:42.480
And also, you know, what's the what's the payoff of that?

16:42.480 --> 16:46.080
You know, you want you what I mean, what do we actually what you know, what do we actually

16:46.120 --> 16:47.120
want?

16:47.120 --> 16:51.240
Why we write these scientific papers on what what what is the impact?

16:51.240 --> 16:56.800
I mean, I think there's been a lot of talk of impact in the last decade, maybe more.

16:56.800 --> 17:00.320
And it's on the one hand, I think it's terrible.

17:00.320 --> 17:02.480
And on the other hand, I think it's really good.

17:02.480 --> 17:08.240
It has good kind of effects, which which really forces us to think about what do you really

17:08.240 --> 17:13.440
want to have happen when you write that paper, right, besides the kind of obvious stuff of,

17:13.440 --> 17:17.120
you know, tenure or whatever, you know, get a job or you know, get a good position or

17:17.120 --> 17:18.120
whatever.

17:18.120 --> 17:20.880
But like long term, the change you're trying to make in the world, what happens when when

17:20.880 --> 17:22.400
you write these papers?

17:22.400 --> 17:26.600
So in theory, I mean, what are some possible answers to that?

17:26.600 --> 17:29.760
Sometimes people read it and they get inspired and they do something else.

17:29.760 --> 17:30.760
So that's cool.

17:30.760 --> 17:35.160
But maybe you're better off just doing that other thing yourself directly, right?

17:35.160 --> 17:37.200
That's one possibility.

17:37.200 --> 17:45.240
Another possibility is that maybe you can unify people across fields so that they sort

17:45.240 --> 17:48.400
of bring some interdisciplinary thought to that.

17:48.400 --> 17:49.400
So that's so that's pretty good.

17:49.400 --> 17:52.840
That's that's, and I don't know if that's gaining acceptance, that's more gaining interest,

17:52.840 --> 17:53.840
right?

17:53.840 --> 17:56.560
So you want people to start thinking about things and asking questions that of things

17:56.560 --> 18:00.760
that they may have taken on, you know, as an assumption before that.

18:00.760 --> 18:01.760
So that's pretty good.

18:02.720 --> 18:09.840
But you know, the actual impact of academic papers, even in academia is often not clear,

18:09.840 --> 18:11.760
but outside of academia.

18:11.760 --> 18:12.760
I don't know.

18:12.760 --> 18:17.160
I mean, you know, Chris Fields had a great sentence recently where he said, arguments

18:17.160 --> 18:19.880
are only settled by technologies.

18:19.880 --> 18:21.440
And I think that's I think that's true.

18:21.440 --> 18:27.480
I think I think we can have all kinds of abstruse academic arguments about things and

18:27.480 --> 18:29.680
people have all kinds of commitments to stuff.

18:29.680 --> 18:34.640
But in the end, if the idea is a good one, it's going to the rubber is going to hit the

18:34.640 --> 18:38.400
you hit the road and you're going to make some sort of impact on the physical world.

18:38.400 --> 18:42.600
And there's going to be some sort of application, some sort of practical thing.

18:42.600 --> 18:47.080
And then it's going to be irrelevant, which part of academia likes it and which part doesn't.

18:47.080 --> 18:53.360
So I don't know, you know, I think I think you got it again, it's about working backwards

18:53.360 --> 18:59.640
to ask what's the change that you want to see and how much of that change is requires

18:59.640 --> 19:05.080
I am buy in from academia, which part of academia, whether papers are sufficient.

19:05.080 --> 19:10.520
And there's a whole other thing, which is that there are so many papers now that I,

19:10.520 --> 19:16.840
you know, most people don't have time to even catch up with the things in their field anymore.

19:16.840 --> 19:18.640
Nevermind, you know, sort of other things.

19:18.640 --> 19:22.360
I sometimes get the feeling that all these papers were writing, but not really for us.

19:22.360 --> 19:26.480
They're for some sort of future AI or some sort of augmented future scientists who's

19:26.480 --> 19:30.280
actually going to be able to have the bandwidth to read all this stuff.

19:30.280 --> 19:33.040
Because I mean, I don't know, you can't see it, but I've got behind my chair,

19:33.040 --> 19:36.960
I've got these like stacks of papers that I'm supposed to have read by now.

19:36.960 --> 19:42.200
And they're just getting bigger and bigger because there's no time to do it.

19:42.200 --> 19:47.680
So yeah, I would I would work backwards and I would think about whose eyeballs are we

19:47.680 --> 19:54.280
trying to capture here, right on these papers and what's the what's the what's the goal of that, you know.

19:54.360 --> 19:57.600
This conversation did not go at all like I was expecting.

19:57.600 --> 20:06.400
I am really grateful that you challenged me on some of my current beliefs around pursuing

20:06.400 --> 20:12.240
an academic approach to defining collective intelligence or collaborative intelligence

20:12.240 --> 20:18.000
from kind of a theoretical and academic perspective for organizations to implement.

20:18.000 --> 20:20.520
I have to do a lot of thinking on this.

20:20.520 --> 20:31.120
And as I reflect, the questions that come to mind as I was listening were,

20:31.120 --> 20:33.680
how does your lab pursue success?

20:33.680 --> 20:41.280
And how does your lab think about translating theory into application?

20:41.280 --> 20:48.360
I view academia serving a higher purpose to make the world a better place through creation

20:48.360 --> 20:52.160
and curation and sharing of knowledge.

20:52.160 --> 20:57.360
And we'd love to hear some of what you've learned works.

21:03.280 --> 21:08.920
First, we generate a lot of very basic fundamental knowledge and also conceptual apparatus.

21:08.920 --> 21:14.920
So we come up with very sort of fundamentally almost philosophical like like perspectives

21:14.920 --> 21:20.200
on things and then that gets cashed out as software or various computational paradigms

21:20.200 --> 21:25.280
and then that gets cashed out in new experiments and new new capabilities and so on.

21:25.280 --> 21:32.120
And I firmly believe that to the extent that we're on the right track with any of this stuff,

21:32.120 --> 21:37.000
really kind of transformative applications should follow.

21:37.000 --> 21:44.280
And this is why we now have a spin off company doing regeneration and we have one doing computer

21:44.280 --> 21:49.600
AI designed synthetic living machines and there's another one on cancer.

21:49.600 --> 21:53.800
So like this stuff is moving towards, hopefully towards the clinic.

21:53.800 --> 21:59.680
So we're not in humans patients yet by any means, but we're going in that direction.

21:59.680 --> 22:10.080
And the other thing too is that I kind of try to strike a balance because I'm not even a clinician.

22:10.080 --> 22:12.560
OK, I don't do clinical work at all.

22:12.560 --> 22:20.160
I get emails and phone calls every day from people with the most unbelievable medical needs.

22:20.160 --> 22:25.560
I mean, you just can't imagine what's out there, the need that's out there and the things that happen

22:25.560 --> 22:31.000
to people from all the way from birth defects to do various other things that happen in your lifetime.

22:31.000 --> 22:33.040
The need is intent.

22:33.040 --> 22:40.760
And so I consider it, you know, I would not consider it a success if all we ever did was,

22:40.760 --> 22:48.360
you know, generate some kind of, you know, conceptual stuff and basic science that never helped patients.

22:48.360 --> 22:53.800
I want to see, I want to see in my lifetime Indiana World, we'll see if that happens or not.

22:53.800 --> 22:58.040
But in my lifetime, I want to see actual patients help with the stuff that we do.

22:58.040 --> 23:00.520
So it's kind of a it's kind of a two pronged attack.

23:00.520 --> 23:06.680
And, you know, you can always argue about what the right prioritization is, you know,

23:06.680 --> 23:10.680
and some people say, you know, don't get tied up with this with the with the stuff with these,

23:10.680 --> 23:14.120
you know, spending time with these companies and figuring out how to, you know,

23:14.120 --> 23:15.960
make products and argue with the FDA.

23:15.960 --> 23:18.600
You know, you should be spending your time on basic basic science.

23:18.600 --> 23:21.560
And then other people write me and they say, and they say the opposite.

23:21.560 --> 23:26.640
Like, what are you doing spending time figuring out, you know, things about the nature of the self?

23:26.640 --> 23:28.880
Like, you could have, you know, you should be solving cancer.

23:28.880 --> 23:32.320
And and and so, you know, you kind of you kind of juggle those two things.

23:32.320 --> 23:37.720
But I think I think I think we're pretty fortunate in my group that I think we can do some of both,

23:37.720 --> 23:40.000
actually, and I think they're very tightly related.

23:40.000 --> 23:43.480
I think if you do it right, you can make impacts on both.

23:43.480 --> 23:45.320
So that's how I see it.

23:45.560 --> 23:59.920
I'm imagining the world you see as we will be able to regrow an arm.

23:59.920 --> 24:04.160
If someone were an accident and were to lose it, is that correct?

24:04.160 --> 24:09.520
Embodiment that you got through the vagaries of genetics and and evolution.

24:09.520 --> 24:13.240
You could change it the way the way we change many other things.

24:13.240 --> 24:18.960
You want tentacles, you want you want to see an infrared, you want to live underwater?

24:18.960 --> 24:20.320
Like, why not? Right?

24:20.320 --> 24:27.000
Why not? Who said who said this this random walk that evolution took to to get you here is how it has to stay?

24:27.000 --> 24:31.120
I think down the line, I mean, you know, without like scaring everybody and so on,

24:31.120 --> 24:32.680
because a lot of people find that pretty freaky.

24:32.680 --> 24:37.000
But but I'm certainly not the first person to say stuff like this down the line.

24:37.000 --> 24:41.640
I believe in maximizing freedom, which includes freedom of embodiment.

24:41.640 --> 24:46.000
You should not be locked into some random configuration that's susceptible to weird diseases

24:46.000 --> 24:50.040
and aging and and and you know, dumb stuff that can happen to you from stepping on the wrong,

24:50.040 --> 24:56.360
you know, patch of ground, you know, it's all the stuff is just it's it's it's the way the way,

24:56.360 --> 25:01.400
you know, we we we we started out with antibiotics and, you know,

25:01.400 --> 25:03.640
and wearing clothes against the cold and things like that.

25:03.640 --> 25:04.760
And that was it after that.

25:04.760 --> 25:09.600
Like it became completely obvious that we do not have to stay the way we came into the world.

25:09.600 --> 25:13.160
So so amazing future.

25:13.160 --> 25:14.840
I sign up for that vision.

25:14.840 --> 25:17.960
I think that sounds amazing.

25:17.960 --> 25:18.920
That's amazing.

25:18.920 --> 25:26.400
And to me, it seems like it's leaving the boundaries of what does it even mean to be human?

25:26.400 --> 25:28.720
Yeah, no, that's I mean, so two things, right?

25:28.720 --> 25:32.600
One is that I don't I don't think these are boundaries at all.

25:32.600 --> 25:33.680
There are no boundaries.

25:33.680 --> 25:36.960
The boundary is us not knowing how to work the interface.

25:36.960 --> 25:41.680
So if if you're given a calculator and you have no idea what this thing is or how to use it,

25:41.680 --> 25:44.720
there's a real boundary between you and the capabilities of that calculator.

25:44.720 --> 25:45.360
But it's not real.

25:45.360 --> 25:52.800
It's it's it's it's a boundary of ignorance and and and those things are are are are

25:52.800 --> 25:53.760
improvable, right?

25:53.760 --> 25:55.600
So you can we can we can we can do that now.

25:55.600 --> 25:59.520
We have a process, the scientific process and some other stuff that that can help us get through that.

25:59.520 --> 26:01.280
So I don't believe in these boundaries at all.

26:02.240 --> 26:06.640
Then, you know, and then and and not only, I mean, let's let's let's face it.

26:06.640 --> 26:10.160
We we haven't believed in these boundaries for a long time.

26:10.160 --> 26:16.720
If you you walk into any gym or, you know, a martial arts studio or a call or a university

26:16.720 --> 26:19.520
and you see people removing their boundaries, right?

26:19.520 --> 26:21.600
You don't come out of there the way you went in.

26:21.600 --> 26:22.960
You come out with extra powers.

26:22.960 --> 26:27.120
You come out with whether that be new brain power or muscles or or you know,

26:27.120 --> 26:30.480
you learn to swim and hold your breath underwater or whatever you know,

26:30.560 --> 26:35.280
whatever you're going to learn, we know we can improve ourselves and the only limitation.

26:35.280 --> 26:39.600
And frankly, some people are amazing and they've pushed it even without all this technology

26:39.600 --> 26:40.560
that I'm talking about.

26:40.560 --> 26:41.600
They've already pushed us.

26:41.600 --> 26:45.280
I mean, right, we've all seen there are there are humans that are so on some particular

26:45.280 --> 26:46.640
thing that they've dedicated the life to.

26:46.640 --> 26:50.800
They're so far outside the mean that it's just it's it's unbelievable that even that that's

26:50.800 --> 26:51.680
even possible.

26:51.680 --> 26:56.800
And that's and that's without knowing, frankly, very much at all about about how biology works yet.

26:57.360 --> 27:00.960
So so whether mental, whether physical, whether spiritual, I think that's important.

27:00.960 --> 27:02.960
There are there are no boundaries.

27:02.960 --> 27:05.680
This is all the all these boundaries are defined by by our own ignorance.

27:06.240 --> 27:09.440
And then the other important thing you said is, are we leaving what it means to be human?

27:09.440 --> 27:10.560
So that's an interesting question.

27:11.920 --> 27:17.600
What do we mean when we say human, in particular, because of this whole AI thing,

27:17.600 --> 27:22.640
somebody I forget who it is, but somebody's working on these proof of humanity certificates,

27:22.640 --> 27:23.040
right?

27:23.040 --> 27:24.720
And it's a good it's a good thing to think about.

27:24.800 --> 27:29.520
If if somebody if if you're interacting with somebody and they and they show you their their

27:29.520 --> 27:32.560
little stamp or whatever it's going to be, that that's the proof of humanity.

27:33.200 --> 27:35.280
What is it that you really are looking for?

27:35.280 --> 27:35.520
Right?

27:35.520 --> 27:39.280
When when you're looking for that proof of humanity in someone or when you want to say

27:39.280 --> 27:43.040
whether someone's left, you know, kind of the human category, what are you really looking for?

27:44.400 --> 27:50.640
Are you looking to validate that their DNA is the same that evolution left us in as homo sapiens?

27:50.640 --> 27:51.360
I don't think so.

27:51.360 --> 27:53.040
Do you really care about anybody's DNA?

27:53.040 --> 27:53.360
I don't.

27:54.320 --> 27:56.480
I don't think that's that's relevant for anything.

27:57.040 --> 27:58.640
Is it is it body structure?

27:58.640 --> 28:02.800
Are you do you want confirmation that that that that person hasn't had some percentage

28:02.800 --> 28:05.360
of their organs replaced by various prosthetics?

28:05.360 --> 28:06.560
I don't care about that either, right?

28:06.560 --> 28:07.760
Does that matter to you?

28:07.760 --> 28:08.320
I don't know.

28:09.040 --> 28:11.840
So what are so what is that when we say somebody's human?

28:11.840 --> 28:13.040
What do we actually mean?

28:13.680 --> 28:19.280
And I think and I'm sure other people have different different, you know, different definitions for it.

28:19.280 --> 28:23.920
But I think what you mean is a minimal level of compassion.

28:23.920 --> 28:24.720
That's what I think.

28:24.720 --> 28:27.840
What you're looking for when somebody says they're human, you're looking for a cognitive

28:27.840 --> 28:33.840
light cone on that individual that is able to actively able to care at least maybe more,

28:33.840 --> 28:38.800
but at least to the same level that you can about others, about various, you know, about

28:38.800 --> 28:44.160
various goals that when when if, you know, if I am trying to choose, you know,

28:44.160 --> 28:46.880
say you're going to go live on Mars or something and you're choosing a companion,

28:47.120 --> 28:53.120
and what you want in that proof of humanity is not anything but their DNA or, you know,

28:53.120 --> 28:55.120
whether they have some prosthetic organs or something.

28:55.120 --> 28:59.600
What you want is do they have the capacity to care about the same stuff that I care about

28:59.600 --> 29:01.120
the same degree of stuff?

29:01.120 --> 29:05.280
Because if they don't, if they're a rumba or, you know, or a cat or something else,

29:06.160 --> 29:10.320
nice, but but not the same relationship you can have with a human, right?

29:10.320 --> 29:11.840
So so that's what I think humans are.

29:11.840 --> 29:16.160
And after that, everything is everything else is for is up for grabs.

29:16.160 --> 29:20.480
You've got gills, you've got, you know, your third brain hemisphere with like whatever,

29:20.480 --> 29:21.680
you know, who cares?

29:21.680 --> 29:23.360
So that's my that's my view of it.

29:24.160 --> 29:29.120
I mean, going macro, I think from that, sorry, Ron, but I think it's like one thing that was

29:29.120 --> 29:33.680
like thinking about because you're talking about these, these, I guess, like boundaries or like

29:33.680 --> 29:36.880
what is our like predefined view of what it means to be human.

29:36.880 --> 29:41.520
And I think if you take even that macro to the stuff that, for example, Ron and I are working on,

29:41.600 --> 29:48.240
is that we are working so hard to try to map out organizations to try to understand organizations.

29:48.240 --> 29:50.000
But what does even an organization mean?

29:50.720 --> 29:55.840
Yeah, are we, are we actually thinking about organizations in our preconceived ways of

29:55.840 --> 29:59.600
thinking about them and not thinking what an organization could be?

29:59.600 --> 30:02.720
It's like, are we just mapping the status quo, whereas we could

30:04.080 --> 30:10.000
helping organizations evolve in the same way that you're saying humans can evolve way

30:10.080 --> 30:11.920
farther than everybody's thinking about now.

30:11.920 --> 30:14.480
And I think that's a very interesting, you know, angle.

30:14.480 --> 30:15.200
What do you think, Ron?

30:16.160 --> 30:17.520
That's a fascinating question.

30:18.400 --> 30:23.440
Level one, let's say, is what are the goals of an organization just listing them out

30:23.440 --> 30:24.880
among the individual nodes?

30:24.880 --> 30:31.920
Then you could say level two is what are the connections between what people care about

30:31.920 --> 30:34.560
and how are they mapped together?

30:35.360 --> 30:40.800
And then level three is, well, how are they adapting and changing together?

30:40.800 --> 30:43.520
I think that's new for me to think about from this conversation.

30:44.480 --> 30:51.280
And I don't know how to answer your question, Juan, because this is when we talk about

30:51.280 --> 30:55.440
adapting our biology and thinking about an organization, like being able to

30:57.520 --> 31:04.080
adapt an organization like we would DNA, it's all new for me to think about.

31:05.040 --> 31:08.160
And the thing with goals, too, is they can be explicit or not.

31:08.160 --> 31:11.760
So like any psychoanalyst will tell you that you think you have goals.

31:13.040 --> 31:17.360
They may or may not be the actual goals that drive your behavior, right?

31:18.400 --> 31:23.200
We all know we have all kinds of internal modules that are trying to maximize and

31:23.200 --> 31:28.000
minimize various things that we don't necessarily have direct access to that may

31:28.000 --> 31:30.800
or may not be adaptive in our context and so on.

31:31.360 --> 31:37.680
So yeah, in the future, so I guess, I don't know anything about this field, but if I had

31:37.680 --> 31:45.120
to guess in the future, I sort of envisioned some kind of psychoanalysis of organizations where

31:46.560 --> 31:50.240
there's some way of kind of like we had this project where we were trying to

31:50.240 --> 31:55.120
communicate with an ant colony, not the ants, the colony, which is a completely different

31:55.120 --> 32:02.080
thing. It's very hard. But at some point, there might be techniques, the way that we try to talk

32:02.080 --> 32:07.600
to organs instead of the individual cells and so on. There might be ways to find out what does

32:07.600 --> 32:11.440
the actual organization want? And the individual people will tell you, oh, well, I know there's

32:11.440 --> 32:17.360
these set of goals. You don't know anything anymore than if you ask you the cells or the organs in

32:17.360 --> 32:22.080
your body, what are the goals of the organism? They'll tell you some stuff about physiological

32:22.880 --> 32:26.880
parameters, boundary parameters and some things like that. But they can't even begin to guess

32:26.880 --> 32:32.960
if your goal of whatever, going to grad school four years from now, they can't fathom it.

32:32.960 --> 32:37.760
So who knows what the organization is going to want. So in the future, there might be some,

32:37.760 --> 32:43.680
I think we need it. I think if we're going to survive and as a mature species, we're going to

32:43.760 --> 32:51.840
have to get our science around being able to identify, characterize, relate to

32:52.880 --> 32:58.160
and modify these kind of emergent, very unconventional emergent agents,

32:58.160 --> 33:05.200
which are anything from organizations to evolutionary lineages and things like that.

33:07.200 --> 33:10.720
I know you have a hard stop. So three minutes left. There's something you said, which just

33:10.720 --> 33:16.800
my ears perked up. Can you just spend a couple of minutes? When you say you want to speak to the

33:16.800 --> 33:23.840
ant colony itself and not the ant, that's an amazing visualization. My brain is so curious

33:23.840 --> 33:29.200
about, can you just describe more what that means? Sure. Yeah. And this is, you know, I'm

33:29.200 --> 33:34.160
by definitely not the first person to talk about this. Doug Hofstadter in his book,

33:34.880 --> 33:39.520
Gert Lecherbach had some great thoughts about it. And before that, in the 20s, Eugene Marais

33:39.520 --> 33:45.040
had this book called The Soul of the White Ant, which is just amazing about the kind of the life

33:45.040 --> 33:50.160
of the colony that's distinct from the individual members. So this idea that if we take collective

33:50.160 --> 33:54.240
intelligence seriously and this idea that the colony works in different problem spaces than

33:54.240 --> 33:57.840
the individual ants and they have, it has a memory of those things that no individual ant knows

33:57.840 --> 34:02.320
and things like that. So we wanted to communicate with it. And we took a very simplistic approach,

34:02.960 --> 34:07.600
which is that training is a simple kind of communication. So if we train you to do something,

34:07.600 --> 34:11.360
then we've communicated the fact that something is good and something else is bad. So you know,

34:11.360 --> 34:16.720
one act of communication has taken place, very minimal. So what we tried to do, and we haven't,

34:16.720 --> 34:20.560
this is something that started right during the pandemic, so it never like really got off the

34:20.560 --> 34:28.080
ground, but we'll, you know, we'll get to it eventually, is just imagine a colony and there's

34:28.080 --> 34:33.600
a location where little drops of food get dropped by a computerized system. But the amount of food

34:33.680 --> 34:37.600
that gets dropped there is proportional to the number of ants standing on a platform

34:37.600 --> 34:42.080
at the other end of the colony. So what that means is that no individual ant ever has the

34:42.080 --> 34:47.040
experience of I stand here and then I collect my reward. No ant has that experience. So now,

34:47.040 --> 34:51.120
if we find over time with training that the colony sends a bunch of ants over in this direction to

34:51.120 --> 34:56.560
pick up the food over here, that's a fact, that's a conditioning, that's a bit of learning that

34:56.560 --> 35:01.360
was done by the colony by no individual ant. And you know, it sounds all crazy or anything, but

35:01.360 --> 35:06.320
you know, when you train a rat to push a lever and get a pellet, the cells at the bottom of

35:06.320 --> 35:12.320
the foot are what's interacting with the button and the cells that get the reward are in the

35:12.320 --> 35:17.440
intestine. No individual cell has both experiences. So we've got this thing we call a rat, which is

35:17.440 --> 35:21.840
this collective that's able to do the credit assignment and figure out that these two things

35:21.840 --> 35:26.560
are related. That's the collective intelligence. We're all collective intelligences. And so we

35:26.560 --> 35:31.120
tried to communicate with the hand colony that way. So stay tuned. I don't know if that's going

35:31.120 --> 35:39.680
to work or not. Mind is blown right now. Thank you so much. Cool. Thank you. Yeah. It's a fun

35:39.680 --> 35:44.080
conversation. I appreciate it. Cheers. Thank you so much. Very nice to meet you. Thank you.

