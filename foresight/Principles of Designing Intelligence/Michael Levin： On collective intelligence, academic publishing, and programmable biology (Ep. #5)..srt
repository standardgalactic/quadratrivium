1
00:00:00,000 --> 00:00:12,760
This podcast is about understanding how collaborative learning networks work and can be designed.

2
00:00:12,760 --> 00:00:20,640
It's part of our effort to describe a methodology, which we call Unify, that can help align human

3
00:00:20,640 --> 00:00:25,120
learning principles, which come from learning science, machine learning principles, and

4
00:00:25,120 --> 00:00:30,160
also how to think about organizations as networks, how to be strategic in how networks

5
00:00:30,160 --> 00:00:38,980
are designed, and what data to collect from networks in order to create new kinds of intelligence.

6
00:00:38,980 --> 00:00:45,720
Our next guest is, in my mind, the authority, and I can't believe I got this interview,

7
00:00:45,720 --> 00:00:48,240
in the field of collective intelligence.

8
00:00:48,240 --> 00:00:53,800
If you haven't seen Michael Levin's TED Talk, it's amazing, I highly recommend it.

9
00:00:53,800 --> 00:01:01,360
This conversation did not go as expected, because I got, frankly, a lot of really good

10
00:01:01,360 --> 00:01:08,720
advice from, frankly, someone who was trying to help me out in terms of how can I propel

11
00:01:08,720 --> 00:01:11,760
my work forward, and for that I'm personally grateful.

12
00:01:11,760 --> 00:01:13,240
I hope you enjoy this.

13
00:01:13,240 --> 00:01:20,920
Thank you so much.

14
00:01:20,920 --> 00:01:25,160
Everything we do is related to this question of embodied minds, so I'm interested in how

15
00:01:25,160 --> 00:01:30,760
very diverse kinds of intelligence can exist in our universe, in all sorts of different

16
00:01:30,760 --> 00:01:34,960
manifestations, different scales, both of space and time.

17
00:01:34,960 --> 00:01:41,560
We use a combination of computer science, developmental biology, biophysics, behavioral

18
00:01:41,560 --> 00:01:48,600
science, computer science, to really try to understand how different degrees of agency

19
00:01:48,600 --> 00:01:53,640
can be implemented in different embodiments.

20
00:01:53,640 --> 00:02:00,760
We have parts of the lab are very theoretical, and do conceptual kinds of models in almost

21
00:02:00,760 --> 00:02:05,480
philosophy, and in other parts of the bright code, and produce various tools and software,

22
00:02:05,480 --> 00:02:09,920
and in other parts do various applications of these ideas.

23
00:02:09,920 --> 00:02:14,840
We have applications in birth defects, and regenerative medicine, and cancer, and we

24
00:02:14,840 --> 00:02:17,960
have some things in AI, and synthetic bioengineering, and so on.

25
00:02:17,960 --> 00:02:23,080
We run the spectrum from very fundamental conceptual things to very practical things

26
00:02:23,080 --> 00:02:27,600
that we hope will end up in the clinic at some point.

27
00:02:27,600 --> 00:02:30,200
My background originally was computer science.

28
00:02:30,200 --> 00:02:34,360
I did software engineering for a long time, scientific programming.

29
00:02:34,360 --> 00:02:40,160
I got a degree in genetics after that, and I've been running a biology lab in the Allen

30
00:02:40,160 --> 00:02:47,920
Discovery Center ever since.

31
00:02:47,920 --> 00:02:54,680
Our goal is to create a collaborative intelligence framework, something that is reliable and robust,

32
00:02:54,680 --> 00:02:57,760
a playbook to give to organizational leaders.

33
00:02:57,760 --> 00:03:06,200
We don't believe that the latest AI algorithm or technology trend that needs to be implemented

34
00:03:06,200 --> 00:03:08,080
is the solution.

35
00:03:08,080 --> 00:03:14,880
We believe that there needs to be an understanding of the guiding principles of how do you design

36
00:03:14,880 --> 00:03:17,440
collaborative intelligence systems?

37
00:03:17,440 --> 00:03:23,760
How do you apply them with minimal or no technology?

38
00:03:23,760 --> 00:03:29,400
Because of the lack of formal definitions, we're hoping we can draw inspiration from

39
00:03:29,400 --> 00:03:37,160
the field, biology, and I'm wondering if there are any principles around collective

40
00:03:37,160 --> 00:03:43,680
intelligence that you feel might be applicable to the domain of business?

41
00:03:43,680 --> 00:03:48,960
Well, I guess because I'm not in this field, I could use a little more guidance as to what

42
00:03:48,960 --> 00:03:51,200
the best case outcome would be.

43
00:03:51,200 --> 00:03:56,160
Can you paint a picture for me of what you want to have happen?

44
00:03:56,160 --> 00:03:57,560
What are we trying to improve?

45
00:03:57,560 --> 00:03:59,640
What are we trying to achieve?

46
00:03:59,640 --> 00:04:02,280
How would you recognize success if you cracked this problem?

47
00:04:02,280 --> 00:04:04,880
What would it look like?

48
00:04:04,880 --> 00:04:13,160
We would like to be able to model an organization using a paradigm from reinforcement learning

49
00:04:13,240 --> 00:04:18,440
that doesn't mean we're going to want to use reinforcement learning, but the idea is to

50
00:04:18,440 --> 00:04:25,400
track agents taking actions in their environments and measuring the outcomes in this case that

51
00:04:25,400 --> 00:04:33,280
can be applied to human sequences of decisions or machine or humans and machines working

52
00:04:33,280 --> 00:04:36,640
together collaboratively.

53
00:04:36,640 --> 00:04:46,480
And one problem we have is that we're applying a reinforcement learning way of thinking to

54
00:04:46,480 --> 00:04:48,040
the macro world.

55
00:04:48,040 --> 00:04:52,680
It's not just applying it to code with a set of data.

56
00:04:52,680 --> 00:05:00,440
It's trying to create a virtual twin of an organization and applying that way of thinking

57
00:05:00,440 --> 00:05:05,160
from reinforcement learning to organizational design.

58
00:05:05,160 --> 00:05:15,280
I think success is going to be having a framework and a methodology, but also being able to implement

59
00:05:15,280 --> 00:05:20,840
it in terms of, let's say, an API, a set of standards.

60
00:05:20,840 --> 00:05:29,200
And step one really is to investigate what are some principles around collaborative intelligence

61
00:05:29,280 --> 00:05:33,400
that work that can be applied to organizations.

62
00:05:33,400 --> 00:05:39,760
I think we're really at the beginning and I don't know if we have even defined yet what

63
00:05:39,760 --> 00:05:42,200
success is.

64
00:05:42,200 --> 00:05:48,760
I like to work backwards and I like to imagine the future that you want to see.

65
00:05:48,760 --> 00:05:49,880
So what does that mean?

66
00:05:49,880 --> 00:05:54,720
Does that mean the organization is functioning more efficiently towards specific goals?

67
00:05:54,720 --> 00:05:58,000
Does that mean something about what those goals are?

68
00:05:58,000 --> 00:06:04,600
Does that mean something about the individual happiness of the people participating in those

69
00:06:04,600 --> 00:06:05,600
goals?

70
00:06:05,600 --> 00:06:11,360
Does that mean something about developing some kind of dominant paradigm that pushes out

71
00:06:11,360 --> 00:06:14,600
competitive views of some particular field?

72
00:06:14,600 --> 00:06:20,760
I think step one is figuring out what, if you did, if you already had a successful theory

73
00:06:20,760 --> 00:06:24,440
of all this stuff and you were able to put it into practice, what would the implications

74
00:06:24,440 --> 00:06:33,720
be for reality, for how people run these things in the real world?

75
00:06:33,720 --> 00:06:39,280
And so I don't know what the answer to this is, but I guess I can talk a little bit about

76
00:06:39,280 --> 00:06:47,320
what we see in biology and then I don't love just blindly taking things from biology and

77
00:06:47,320 --> 00:06:51,560
pushing them into, I mean, people often would like to do this, pushing them into social

78
00:06:51,560 --> 00:06:58,240
and various kinds of societal contexts, but I think it's better to work backwards and

79
00:06:58,240 --> 00:07:01,560
ask what we're trying to achieve and what that looks like.

80
00:07:01,560 --> 00:07:04,320
I find in general that's missing from a lot of the discussions.

81
00:07:04,320 --> 00:07:08,720
A lot of people have critiques about things that are going on now, whether it's AI or

82
00:07:08,720 --> 00:07:12,240
whether it's something else, they can see all the problems and it's going to lead to

83
00:07:12,240 --> 00:07:14,280
this, it's going to lead to that, we don't want it.

84
00:07:14,280 --> 00:07:17,720
What I don't see as much as people articulating, what do you want to see?

85
00:07:17,720 --> 00:07:24,240
What does future humanity look like that avoids these kinds of things that people are concerned

86
00:07:24,240 --> 00:07:25,240
about?

87
00:07:25,240 --> 00:07:32,760
With respect to all of the parameters that are currently under debate and I think that's

88
00:07:32,760 --> 00:07:38,520
what I would do here as well as I would ask, what does the future of an optimal successful

89
00:07:38,520 --> 00:07:40,200
organization look like?

90
00:07:40,200 --> 00:07:41,200
How do you recognize it?

91
00:07:41,200 --> 00:07:50,600
What are we aiming for?

92
00:07:50,600 --> 00:07:55,480
But having said all that, I could tell you some things we learned from the biology.

93
00:07:55,480 --> 00:08:01,640
One thing we learned from the biology is that one reason biology is so successful is that

94
00:08:01,640 --> 00:08:05,920
it often changes the goals.

95
00:08:05,920 --> 00:08:11,120
Pretty much like with artificial life and this notion of perverse instantiation, which

96
00:08:11,120 --> 00:08:15,440
is you think you're trying to solve a problem in a particular way and if your system is

97
00:08:15,440 --> 00:08:21,000
flexible enough, it might do something completely different that on retrospect, you can see

98
00:08:21,000 --> 00:08:24,840
how that would solve the problem, but it isn't at all what you were looking for.

99
00:08:24,840 --> 00:08:28,640
Biology does this all the time, when faced with a difficult problem, one thing biology

100
00:08:28,640 --> 00:08:32,200
sometimes does is switch to a different problem.

101
00:08:32,200 --> 00:08:37,200
If you look at the biosphere, there's every possible way of making a living and biology

102
00:08:37,200 --> 00:08:41,720
is never tied to finding an answer to a specific problem.

103
00:08:41,720 --> 00:08:46,880
They often change, it often changes the problem that it tries to solve and so that may or

104
00:08:46,880 --> 00:08:52,520
may not work for us because you may come up with an organizational structure that is

105
00:08:52,520 --> 00:08:56,640
very good at let's say perpetuating itself, but one of the ways that might do that is

106
00:08:56,640 --> 00:09:02,200
to go off, completely go off script as far as where you thought the organization was

107
00:09:02,200 --> 00:09:06,600
going to go and that may not, that works in the biosphere if your goal is to make sure

108
00:09:06,600 --> 00:09:12,680
that life survives in some particular, you know, on a planet, that may not be that ability

109
00:09:12,680 --> 00:09:17,360
to just completely change goals may not be what you want for an organization.

110
00:09:17,360 --> 00:09:25,280
The other thing is that when you, biological agents and those could be cells, they could

111
00:09:25,280 --> 00:09:31,160
be molecular networks, they could be tissues, organs, whatever, they're very good at combining

112
00:09:31,160 --> 00:09:39,280
into higher level entities that are these emergent selves that do things in other problem spaces.

113
00:09:39,280 --> 00:09:43,120
But they're not, there's a few things that are not guaranteed when that happens.

114
00:09:43,120 --> 00:09:49,000
One thing that's not guaranteed is that the emergent self is smarter than the components.

115
00:09:49,000 --> 00:09:53,360
It doesn't have to be, sometimes it certainly is, but it doesn't have to be and so that's

116
00:09:53,440 --> 00:09:57,640
no guarantee, you know, scaling up is no guarantee that it's going to be more intelligent.

117
00:09:57,640 --> 00:10:10,320
Also, when you create these novel selves, you simultaneously create various goals, preferences

118
00:10:10,320 --> 00:10:20,520
and various kinds of salience and valence for these things that don't necessarily match

119
00:10:20,520 --> 00:10:22,040
those of the subunits.

120
00:10:22,040 --> 00:10:27,960
So in other words, people often say, you know, people often hear the story, I tell of gap

121
00:10:27,960 --> 00:10:32,400
junctions and cancer, this idea that cells are tied together with gap junctions that

122
00:10:32,400 --> 00:10:36,480
basically erase their individuality a little bit so that they're all part of this collective

123
00:10:36,480 --> 00:10:38,720
and they work on making the organs and whatnot.

124
00:10:38,720 --> 00:10:43,000
And then when there's a breakdown of that, cells roll back to the kind of unicellular

125
00:10:43,000 --> 00:10:46,320
form and they go off and they metastasize and so on.

126
00:10:46,320 --> 00:10:52,000
And so people hear that and often they say, well, that's clear what we have to do for

127
00:10:52,000 --> 00:10:55,920
the various ills of society, you know, we can address that the whole selfishness thing

128
00:10:55,920 --> 00:10:56,920
and so on.

129
00:10:56,920 --> 00:11:01,360
We'll just sort of gap junctions ourselves, take it to each other in some format and when

130
00:11:01,360 --> 00:11:05,640
wipe our individuality a little bit and we'll be this collective.

131
00:11:05,640 --> 00:11:11,640
I think that's a, in general, that's a terrible idea because one of the clear things is that

132
00:11:11,640 --> 00:11:18,040
when you as a large scale individual, let's say a human goes off and does various things.

133
00:11:18,040 --> 00:11:20,880
My silly example is rock climbing.

134
00:11:20,880 --> 00:11:24,560
You know, you go rock climbing and you have a lovely day and you meet various social goals,

135
00:11:24,560 --> 00:11:28,480
you meet various personal happiness goals and so on as a human.

136
00:11:28,480 --> 00:11:32,520
But you've left a bunch of skin cells on that, you know, on that rock.

137
00:11:32,520 --> 00:11:36,360
And nobody asked those skin cells whether this was something that they were interested

138
00:11:36,360 --> 00:11:37,360
in.

139
00:11:37,360 --> 00:11:42,600
And just in general, the basic premise of having a multicellular body is that the vast majority

140
00:11:42,600 --> 00:11:48,840
of your body cells are going to die in the service of whatever weird to them incomprehensible

141
00:11:48,840 --> 00:11:51,600
goals that the collective human might have.

142
00:11:51,600 --> 00:12:00,080
So that stuff is very easily scaled up into social structures where you might have persistent,

143
00:12:00,080 --> 00:12:04,500
you know, persistent kinds of government, persistent kinds of companies and so on that

144
00:12:04,500 --> 00:12:09,760
pursue goals that are at odds with the well-being, the happiness and the goals of the beings

145
00:12:09,760 --> 00:12:11,760
that make them up.

146
00:12:11,760 --> 00:12:18,320
And that needs to be, it needs to be clear that when you scale up like that, you are

147
00:12:18,320 --> 00:12:19,320
not guaranteed.

148
00:12:19,320 --> 00:12:23,800
In fact, the big thing is that we don't have a good science of being able to anticipate

149
00:12:23,800 --> 00:12:26,000
the goals of collective systems.

150
00:12:26,000 --> 00:12:30,880
We really can't anticipate that very well now at all.

151
00:12:30,880 --> 00:12:40,480
I talk about these biology examples all the time in terms of like, we make something called

152
00:12:40,480 --> 00:12:44,720
a frog allotle in our lab and so this is part frog and part axolotl.

153
00:12:44,720 --> 00:12:50,640
And so baby axolotls have little forelegs and tadpoles don't have forelegs.

154
00:12:50,640 --> 00:12:55,880
And so when we make this frog allotle, I often say this in my talk, you know, we know the

155
00:12:55,880 --> 00:12:58,720
genome for the frog, we know the genome for the axolotl.

156
00:12:58,720 --> 00:13:02,040
Can you tell me if a frog allotle is going to have legs or not, right?

157
00:13:02,040 --> 00:13:04,400
And there's no one can predict that up front.

158
00:13:04,400 --> 00:13:08,720
And that's because while we understand the molecular hardware to some extent of the

159
00:13:08,720 --> 00:13:11,880
pieces, we do not understand how the collective decisions are made.

160
00:13:11,880 --> 00:13:16,280
We don't know how they decide to make these large scale structures and to say that, you

161
00:13:16,280 --> 00:13:18,040
know, there's a million examples of that.

162
00:13:18,040 --> 00:13:22,320
So that's the thing when we make these, when we make these collectives, we really don't

163
00:13:22,320 --> 00:13:27,400
know what the goal of the, if there is going to be an emergent agent, we don't know what

164
00:13:27,400 --> 00:13:28,400
the goal is going to be.

165
00:13:28,400 --> 00:13:34,320
And we don't know if it at all matches our goals, even if we, I mean, even our own, we

166
00:13:34,320 --> 00:13:38,040
don't match goals with each other, but never mind, or even if never mind that.

167
00:13:38,040 --> 00:13:39,040
Yeah.

168
00:13:39,040 --> 00:13:41,240
So, so I think, so I think that's the danger, right?

169
00:13:41,240 --> 00:13:46,120
So I think biology can teach us some very successful principles about making sure you

170
00:13:46,120 --> 00:13:51,880
stick around, but I'm not sure that's the end all and be all that we want from the structures

171
00:13:51,880 --> 00:13:53,320
that we build, right?

172
00:13:53,320 --> 00:13:54,600
Persistence isn't really what we're looking for.

173
00:13:54,600 --> 00:13:56,480
We're looking for something much more specific.

174
00:13:56,480 --> 00:14:06,160
So you got to, you got to take all that biology stuff with a, with a grain of salt.

175
00:14:06,160 --> 00:14:10,240
So one of my favorite things as a consultant is that I get to speak with individuals where

176
00:14:10,280 --> 00:14:15,800
very aware of their problems, very aware of goals and the financial value of their goals.

177
00:14:15,800 --> 00:14:21,440
But when it comes time to connect the dots between these different teams and more of

178
00:14:21,440 --> 00:14:26,560
a collective intelligence or collaborative intelligence approach, typically there's

179
00:14:26,560 --> 00:14:33,760
no person in charge or there's no system or process or computational way that combines

180
00:14:33,800 --> 00:14:36,880
them all and analyzes them.

181
00:14:36,880 --> 00:14:45,960
And I found this gap to be really intriguing and I want to pursue formalizing a process

182
00:14:45,960 --> 00:14:50,360
that could be something that we could publish and like, let's say an academic journal or

183
00:14:50,360 --> 00:14:56,400
paper to help push this area of research forward.

184
00:14:57,400 --> 00:15:04,240
And I think the one challenge I've had is like simply defining, you know, what is collective

185
00:15:04,240 --> 00:15:08,120
intelligence for an organization, what is collaborative intelligence, because I hear

186
00:15:08,120 --> 00:15:14,200
different people, different startups talking about those two and, and yet it's all very

187
00:15:14,200 --> 00:15:15,200
fuzzy.

188
00:15:15,200 --> 00:15:25,360
So I thought that trying to pursue a more formalized definition that respected kind

189
00:15:25,400 --> 00:15:30,640
of academic perspectives would be a good approach for us to have.

190
00:15:31,120 --> 00:15:32,120
Yeah.

191
00:15:32,320 --> 00:15:33,760
You know, I don't know.

192
00:15:33,760 --> 00:15:35,040
That's that's a whole other thing.

193
00:15:35,040 --> 00:15:40,040
This this like meta meta goal seeking, which is what what should the goals be?

194
00:15:40,040 --> 00:15:44,280
So so gaining respect of the academic community.

195
00:15:44,280 --> 00:15:51,000
I'm not even sure that's an achievable goal per se for anybody because there is no monolithic

196
00:15:51,000 --> 00:15:52,000
academic community.

197
00:15:52,040 --> 00:15:59,120
So there are different islands of academics where like so so so there's a there's a particular

198
00:15:59,120 --> 00:16:04,040
kind of standard talk that I sometimes give and I always know what kind of department

199
00:16:04,040 --> 00:16:07,960
I'm giving the talking based on which part it makes people angry and it's often a different

200
00:16:07,960 --> 00:16:11,920
it's always a different part because there are certain things you can say in a in a neurobiology

201
00:16:11,920 --> 00:16:13,520
department and everybody's like, yeah, no kidding.

202
00:16:13,520 --> 00:16:17,440
So what we knew that and the exact same thing in the genetics department and people throw

203
00:16:17,480 --> 00:16:18,480
tomatoes.

204
00:16:18,480 --> 00:16:26,760
And so trying for kind of generic acceptance, I'm not sure that's feasible until until we

205
00:16:26,760 --> 00:16:34,440
get a lot better at breaking down these these kind of conceptual silos that the communities

206
00:16:34,440 --> 00:16:35,440
are in.

207
00:16:35,440 --> 00:16:42,480
And also, you know, what's the what's the payoff of that?

208
00:16:42,480 --> 00:16:46,080
You know, you want you what I mean, what do we actually what you know, what do we actually

209
00:16:46,120 --> 00:16:47,120
want?

210
00:16:47,120 --> 00:16:51,240
Why we write these scientific papers on what what what is the impact?

211
00:16:51,240 --> 00:16:56,800
I mean, I think there's been a lot of talk of impact in the last decade, maybe more.

212
00:16:56,800 --> 00:17:00,320
And it's on the one hand, I think it's terrible.

213
00:17:00,320 --> 00:17:02,480
And on the other hand, I think it's really good.

214
00:17:02,480 --> 00:17:08,240
It has good kind of effects, which which really forces us to think about what do you really

215
00:17:08,240 --> 00:17:13,440
want to have happen when you write that paper, right, besides the kind of obvious stuff of,

216
00:17:13,440 --> 00:17:17,120
you know, tenure or whatever, you know, get a job or you know, get a good position or

217
00:17:17,120 --> 00:17:18,120
whatever.

218
00:17:18,120 --> 00:17:20,880
But like long term, the change you're trying to make in the world, what happens when when

219
00:17:20,880 --> 00:17:22,400
you write these papers?

220
00:17:22,400 --> 00:17:26,600
So in theory, I mean, what are some possible answers to that?

221
00:17:26,600 --> 00:17:29,760
Sometimes people read it and they get inspired and they do something else.

222
00:17:29,760 --> 00:17:30,760
So that's cool.

223
00:17:30,760 --> 00:17:35,160
But maybe you're better off just doing that other thing yourself directly, right?

224
00:17:35,160 --> 00:17:37,200
That's one possibility.

225
00:17:37,200 --> 00:17:45,240
Another possibility is that maybe you can unify people across fields so that they sort

226
00:17:45,240 --> 00:17:48,400
of bring some interdisciplinary thought to that.

227
00:17:48,400 --> 00:17:49,400
So that's so that's pretty good.

228
00:17:49,400 --> 00:17:52,840
That's that's, and I don't know if that's gaining acceptance, that's more gaining interest,

229
00:17:52,840 --> 00:17:53,840
right?

230
00:17:53,840 --> 00:17:56,560
So you want people to start thinking about things and asking questions that of things

231
00:17:56,560 --> 00:18:00,760
that they may have taken on, you know, as an assumption before that.

232
00:18:00,760 --> 00:18:01,760
So that's pretty good.

233
00:18:02,720 --> 00:18:09,840
But you know, the actual impact of academic papers, even in academia is often not clear,

234
00:18:09,840 --> 00:18:11,760
but outside of academia.

235
00:18:11,760 --> 00:18:12,760
I don't know.

236
00:18:12,760 --> 00:18:17,160
I mean, you know, Chris Fields had a great sentence recently where he said, arguments

237
00:18:17,160 --> 00:18:19,880
are only settled by technologies.

238
00:18:19,880 --> 00:18:21,440
And I think that's I think that's true.

239
00:18:21,440 --> 00:18:27,480
I think I think we can have all kinds of abstruse academic arguments about things and

240
00:18:27,480 --> 00:18:29,680
people have all kinds of commitments to stuff.

241
00:18:29,680 --> 00:18:34,640
But in the end, if the idea is a good one, it's going to the rubber is going to hit the

242
00:18:34,640 --> 00:18:38,400
you hit the road and you're going to make some sort of impact on the physical world.

243
00:18:38,400 --> 00:18:42,600
And there's going to be some sort of application, some sort of practical thing.

244
00:18:42,600 --> 00:18:47,080
And then it's going to be irrelevant, which part of academia likes it and which part doesn't.

245
00:18:47,080 --> 00:18:53,360
So I don't know, you know, I think I think you got it again, it's about working backwards

246
00:18:53,360 --> 00:18:59,640
to ask what's the change that you want to see and how much of that change is requires

247
00:18:59,640 --> 00:19:05,080
I am buy in from academia, which part of academia, whether papers are sufficient.

248
00:19:05,080 --> 00:19:10,520
And there's a whole other thing, which is that there are so many papers now that I,

249
00:19:10,520 --> 00:19:16,840
you know, most people don't have time to even catch up with the things in their field anymore.

250
00:19:16,840 --> 00:19:18,640
Nevermind, you know, sort of other things.

251
00:19:18,640 --> 00:19:22,360
I sometimes get the feeling that all these papers were writing, but not really for us.

252
00:19:22,360 --> 00:19:26,480
They're for some sort of future AI or some sort of augmented future scientists who's

253
00:19:26,480 --> 00:19:30,280
actually going to be able to have the bandwidth to read all this stuff.

254
00:19:30,280 --> 00:19:33,040
Because I mean, I don't know, you can't see it, but I've got behind my chair,

255
00:19:33,040 --> 00:19:36,960
I've got these like stacks of papers that I'm supposed to have read by now.

256
00:19:36,960 --> 00:19:42,200
And they're just getting bigger and bigger because there's no time to do it.

257
00:19:42,200 --> 00:19:47,680
So yeah, I would I would work backwards and I would think about whose eyeballs are we

258
00:19:47,680 --> 00:19:54,280
trying to capture here, right on these papers and what's the what's the what's the goal of that, you know.

259
00:19:54,360 --> 00:19:57,600
This conversation did not go at all like I was expecting.

260
00:19:57,600 --> 00:20:06,400
I am really grateful that you challenged me on some of my current beliefs around pursuing

261
00:20:06,400 --> 00:20:12,240
an academic approach to defining collective intelligence or collaborative intelligence

262
00:20:12,240 --> 00:20:18,000
from kind of a theoretical and academic perspective for organizations to implement.

263
00:20:18,000 --> 00:20:20,520
I have to do a lot of thinking on this.

264
00:20:20,520 --> 00:20:31,120
And as I reflect, the questions that come to mind as I was listening were,

265
00:20:31,120 --> 00:20:33,680
how does your lab pursue success?

266
00:20:33,680 --> 00:20:41,280
And how does your lab think about translating theory into application?

267
00:20:41,280 --> 00:20:48,360
I view academia serving a higher purpose to make the world a better place through creation

268
00:20:48,360 --> 00:20:52,160
and curation and sharing of knowledge.

269
00:20:52,160 --> 00:20:57,360
And we'd love to hear some of what you've learned works.

270
00:21:03,280 --> 00:21:08,920
First, we generate a lot of very basic fundamental knowledge and also conceptual apparatus.

271
00:21:08,920 --> 00:21:14,920
So we come up with very sort of fundamentally almost philosophical like like perspectives

272
00:21:14,920 --> 00:21:20,200
on things and then that gets cashed out as software or various computational paradigms

273
00:21:20,200 --> 00:21:25,280
and then that gets cashed out in new experiments and new new capabilities and so on.

274
00:21:25,280 --> 00:21:32,120
And I firmly believe that to the extent that we're on the right track with any of this stuff,

275
00:21:32,120 --> 00:21:37,000
really kind of transformative applications should follow.

276
00:21:37,000 --> 00:21:44,280
And this is why we now have a spin off company doing regeneration and we have one doing computer

277
00:21:44,280 --> 00:21:49,600
AI designed synthetic living machines and there's another one on cancer.

278
00:21:49,600 --> 00:21:53,800
So like this stuff is moving towards, hopefully towards the clinic.

279
00:21:53,800 --> 00:21:59,680
So we're not in humans patients yet by any means, but we're going in that direction.

280
00:21:59,680 --> 00:22:10,080
And the other thing too is that I kind of try to strike a balance because I'm not even a clinician.

281
00:22:10,080 --> 00:22:12,560
OK, I don't do clinical work at all.

282
00:22:12,560 --> 00:22:20,160
I get emails and phone calls every day from people with the most unbelievable medical needs.

283
00:22:20,160 --> 00:22:25,560
I mean, you just can't imagine what's out there, the need that's out there and the things that happen

284
00:22:25,560 --> 00:22:31,000
to people from all the way from birth defects to do various other things that happen in your lifetime.

285
00:22:31,000 --> 00:22:33,040
The need is intent.

286
00:22:33,040 --> 00:22:40,760
And so I consider it, you know, I would not consider it a success if all we ever did was,

287
00:22:40,760 --> 00:22:48,360
you know, generate some kind of, you know, conceptual stuff and basic science that never helped patients.

288
00:22:48,360 --> 00:22:53,800
I want to see, I want to see in my lifetime Indiana World, we'll see if that happens or not.

289
00:22:53,800 --> 00:22:58,040
But in my lifetime, I want to see actual patients help with the stuff that we do.

290
00:22:58,040 --> 00:23:00,520
So it's kind of a it's kind of a two pronged attack.

291
00:23:00,520 --> 00:23:06,680
And, you know, you can always argue about what the right prioritization is, you know,

292
00:23:06,680 --> 00:23:10,680
and some people say, you know, don't get tied up with this with the with the stuff with these,

293
00:23:10,680 --> 00:23:14,120
you know, spending time with these companies and figuring out how to, you know,

294
00:23:14,120 --> 00:23:15,960
make products and argue with the FDA.

295
00:23:15,960 --> 00:23:18,600
You know, you should be spending your time on basic basic science.

296
00:23:18,600 --> 00:23:21,560
And then other people write me and they say, and they say the opposite.

297
00:23:21,560 --> 00:23:26,640
Like, what are you doing spending time figuring out, you know, things about the nature of the self?

298
00:23:26,640 --> 00:23:28,880
Like, you could have, you know, you should be solving cancer.

299
00:23:28,880 --> 00:23:32,320
And and and so, you know, you kind of you kind of juggle those two things.

300
00:23:32,320 --> 00:23:37,720
But I think I think I think we're pretty fortunate in my group that I think we can do some of both,

301
00:23:37,720 --> 00:23:40,000
actually, and I think they're very tightly related.

302
00:23:40,000 --> 00:23:43,480
I think if you do it right, you can make impacts on both.

303
00:23:43,480 --> 00:23:45,320
So that's how I see it.

304
00:23:45,560 --> 00:23:59,920
I'm imagining the world you see as we will be able to regrow an arm.

305
00:23:59,920 --> 00:24:04,160
If someone were an accident and were to lose it, is that correct?

306
00:24:04,160 --> 00:24:09,520
Embodiment that you got through the vagaries of genetics and and evolution.

307
00:24:09,520 --> 00:24:13,240
You could change it the way the way we change many other things.

308
00:24:13,240 --> 00:24:18,960
You want tentacles, you want you want to see an infrared, you want to live underwater?

309
00:24:18,960 --> 00:24:20,320
Like, why not? Right?

310
00:24:20,320 --> 00:24:27,000
Why not? Who said who said this this random walk that evolution took to to get you here is how it has to stay?

311
00:24:27,000 --> 00:24:31,120
I think down the line, I mean, you know, without like scaring everybody and so on,

312
00:24:31,120 --> 00:24:32,680
because a lot of people find that pretty freaky.

313
00:24:32,680 --> 00:24:37,000
But but I'm certainly not the first person to say stuff like this down the line.

314
00:24:37,000 --> 00:24:41,640
I believe in maximizing freedom, which includes freedom of embodiment.

315
00:24:41,640 --> 00:24:46,000
You should not be locked into some random configuration that's susceptible to weird diseases

316
00:24:46,000 --> 00:24:50,040
and aging and and and you know, dumb stuff that can happen to you from stepping on the wrong,

317
00:24:50,040 --> 00:24:56,360
you know, patch of ground, you know, it's all the stuff is just it's it's it's the way the way,

318
00:24:56,360 --> 00:25:01,400
you know, we we we we started out with antibiotics and, you know,

319
00:25:01,400 --> 00:25:03,640
and wearing clothes against the cold and things like that.

320
00:25:03,640 --> 00:25:04,760
And that was it after that.

321
00:25:04,760 --> 00:25:09,600
Like it became completely obvious that we do not have to stay the way we came into the world.

322
00:25:09,600 --> 00:25:13,160
So so amazing future.

323
00:25:13,160 --> 00:25:14,840
I sign up for that vision.

324
00:25:14,840 --> 00:25:17,960
I think that sounds amazing.

325
00:25:17,960 --> 00:25:18,920
That's amazing.

326
00:25:18,920 --> 00:25:26,400
And to me, it seems like it's leaving the boundaries of what does it even mean to be human?

327
00:25:26,400 --> 00:25:28,720
Yeah, no, that's I mean, so two things, right?

328
00:25:28,720 --> 00:25:32,600
One is that I don't I don't think these are boundaries at all.

329
00:25:32,600 --> 00:25:33,680
There are no boundaries.

330
00:25:33,680 --> 00:25:36,960
The boundary is us not knowing how to work the interface.

331
00:25:36,960 --> 00:25:41,680
So if if you're given a calculator and you have no idea what this thing is or how to use it,

332
00:25:41,680 --> 00:25:44,720
there's a real boundary between you and the capabilities of that calculator.

333
00:25:44,720 --> 00:25:45,360
But it's not real.

334
00:25:45,360 --> 00:25:52,800
It's it's it's it's a boundary of ignorance and and and those things are are are are

335
00:25:52,800 --> 00:25:53,760
improvable, right?

336
00:25:53,760 --> 00:25:55,600
So you can we can we can we can do that now.

337
00:25:55,600 --> 00:25:59,520
We have a process, the scientific process and some other stuff that that can help us get through that.

338
00:25:59,520 --> 00:26:01,280
So I don't believe in these boundaries at all.

339
00:26:02,240 --> 00:26:06,640
Then, you know, and then and and not only, I mean, let's let's let's face it.

340
00:26:06,640 --> 00:26:10,160
We we haven't believed in these boundaries for a long time.

341
00:26:10,160 --> 00:26:16,720
If you you walk into any gym or, you know, a martial arts studio or a call or a university

342
00:26:16,720 --> 00:26:19,520
and you see people removing their boundaries, right?

343
00:26:19,520 --> 00:26:21,600
You don't come out of there the way you went in.

344
00:26:21,600 --> 00:26:22,960
You come out with extra powers.

345
00:26:22,960 --> 00:26:27,120
You come out with whether that be new brain power or muscles or or you know,

346
00:26:27,120 --> 00:26:30,480
you learn to swim and hold your breath underwater or whatever you know,

347
00:26:30,560 --> 00:26:35,280
whatever you're going to learn, we know we can improve ourselves and the only limitation.

348
00:26:35,280 --> 00:26:39,600
And frankly, some people are amazing and they've pushed it even without all this technology

349
00:26:39,600 --> 00:26:40,560
that I'm talking about.

350
00:26:40,560 --> 00:26:41,600
They've already pushed us.

351
00:26:41,600 --> 00:26:45,280
I mean, right, we've all seen there are there are humans that are so on some particular

352
00:26:45,280 --> 00:26:46,640
thing that they've dedicated the life to.

353
00:26:46,640 --> 00:26:50,800
They're so far outside the mean that it's just it's it's unbelievable that even that that's

354
00:26:50,800 --> 00:26:51,680
even possible.

355
00:26:51,680 --> 00:26:56,800
And that's and that's without knowing, frankly, very much at all about about how biology works yet.

356
00:26:57,360 --> 00:27:00,960
So so whether mental, whether physical, whether spiritual, I think that's important.

357
00:27:00,960 --> 00:27:02,960
There are there are no boundaries.

358
00:27:02,960 --> 00:27:05,680
This is all the all these boundaries are defined by by our own ignorance.

359
00:27:06,240 --> 00:27:09,440
And then the other important thing you said is, are we leaving what it means to be human?

360
00:27:09,440 --> 00:27:10,560
So that's an interesting question.

361
00:27:11,920 --> 00:27:17,600
What do we mean when we say human, in particular, because of this whole AI thing,

362
00:27:17,600 --> 00:27:22,640
somebody I forget who it is, but somebody's working on these proof of humanity certificates,

363
00:27:22,640 --> 00:27:23,040
right?

364
00:27:23,040 --> 00:27:24,720
And it's a good it's a good thing to think about.

365
00:27:24,800 --> 00:27:29,520
If if somebody if if you're interacting with somebody and they and they show you their their

366
00:27:29,520 --> 00:27:32,560
little stamp or whatever it's going to be, that that's the proof of humanity.

367
00:27:33,200 --> 00:27:35,280
What is it that you really are looking for?

368
00:27:35,280 --> 00:27:35,520
Right?

369
00:27:35,520 --> 00:27:39,280
When when you're looking for that proof of humanity in someone or when you want to say

370
00:27:39,280 --> 00:27:43,040
whether someone's left, you know, kind of the human category, what are you really looking for?

371
00:27:44,400 --> 00:27:50,640
Are you looking to validate that their DNA is the same that evolution left us in as homo sapiens?

372
00:27:50,640 --> 00:27:51,360
I don't think so.

373
00:27:51,360 --> 00:27:53,040
Do you really care about anybody's DNA?

374
00:27:53,040 --> 00:27:53,360
I don't.

375
00:27:54,320 --> 00:27:56,480
I don't think that's that's relevant for anything.

376
00:27:57,040 --> 00:27:58,640
Is it is it body structure?

377
00:27:58,640 --> 00:28:02,800
Are you do you want confirmation that that that that person hasn't had some percentage

378
00:28:02,800 --> 00:28:05,360
of their organs replaced by various prosthetics?

379
00:28:05,360 --> 00:28:06,560
I don't care about that either, right?

380
00:28:06,560 --> 00:28:07,760
Does that matter to you?

381
00:28:07,760 --> 00:28:08,320
I don't know.

382
00:28:09,040 --> 00:28:11,840
So what are so what is that when we say somebody's human?

383
00:28:11,840 --> 00:28:13,040
What do we actually mean?

384
00:28:13,680 --> 00:28:19,280
And I think and I'm sure other people have different different, you know, different definitions for it.

385
00:28:19,280 --> 00:28:23,920
But I think what you mean is a minimal level of compassion.

386
00:28:23,920 --> 00:28:24,720
That's what I think.

387
00:28:24,720 --> 00:28:27,840
What you're looking for when somebody says they're human, you're looking for a cognitive

388
00:28:27,840 --> 00:28:33,840
light cone on that individual that is able to actively able to care at least maybe more,

389
00:28:33,840 --> 00:28:38,800
but at least to the same level that you can about others, about various, you know, about

390
00:28:38,800 --> 00:28:44,160
various goals that when when if, you know, if I am trying to choose, you know,

391
00:28:44,160 --> 00:28:46,880
say you're going to go live on Mars or something and you're choosing a companion,

392
00:28:47,120 --> 00:28:53,120
and what you want in that proof of humanity is not anything but their DNA or, you know,

393
00:28:53,120 --> 00:28:55,120
whether they have some prosthetic organs or something.

394
00:28:55,120 --> 00:28:59,600
What you want is do they have the capacity to care about the same stuff that I care about

395
00:28:59,600 --> 00:29:01,120
the same degree of stuff?

396
00:29:01,120 --> 00:29:05,280
Because if they don't, if they're a rumba or, you know, or a cat or something else,

397
00:29:06,160 --> 00:29:10,320
nice, but but not the same relationship you can have with a human, right?

398
00:29:10,320 --> 00:29:11,840
So so that's what I think humans are.

399
00:29:11,840 --> 00:29:16,160
And after that, everything is everything else is for is up for grabs.

400
00:29:16,160 --> 00:29:20,480
You've got gills, you've got, you know, your third brain hemisphere with like whatever,

401
00:29:20,480 --> 00:29:21,680
you know, who cares?

402
00:29:21,680 --> 00:29:23,360
So that's my that's my view of it.

403
00:29:24,160 --> 00:29:29,120
I mean, going macro, I think from that, sorry, Ron, but I think it's like one thing that was

404
00:29:29,120 --> 00:29:33,680
like thinking about because you're talking about these, these, I guess, like boundaries or like

405
00:29:33,680 --> 00:29:36,880
what is our like predefined view of what it means to be human.

406
00:29:36,880 --> 00:29:41,520
And I think if you take even that macro to the stuff that, for example, Ron and I are working on,

407
00:29:41,600 --> 00:29:48,240
is that we are working so hard to try to map out organizations to try to understand organizations.

408
00:29:48,240 --> 00:29:50,000
But what does even an organization mean?

409
00:29:50,720 --> 00:29:55,840
Yeah, are we, are we actually thinking about organizations in our preconceived ways of

410
00:29:55,840 --> 00:29:59,600
thinking about them and not thinking what an organization could be?

411
00:29:59,600 --> 00:30:02,720
It's like, are we just mapping the status quo, whereas we could

412
00:30:04,080 --> 00:30:10,000
helping organizations evolve in the same way that you're saying humans can evolve way

413
00:30:10,080 --> 00:30:11,920
farther than everybody's thinking about now.

414
00:30:11,920 --> 00:30:14,480
And I think that's a very interesting, you know, angle.

415
00:30:14,480 --> 00:30:15,200
What do you think, Ron?

416
00:30:16,160 --> 00:30:17,520
That's a fascinating question.

417
00:30:18,400 --> 00:30:23,440
Level one, let's say, is what are the goals of an organization just listing them out

418
00:30:23,440 --> 00:30:24,880
among the individual nodes?

419
00:30:24,880 --> 00:30:31,920
Then you could say level two is what are the connections between what people care about

420
00:30:31,920 --> 00:30:34,560
and how are they mapped together?

421
00:30:35,360 --> 00:30:40,800
And then level three is, well, how are they adapting and changing together?

422
00:30:40,800 --> 00:30:43,520
I think that's new for me to think about from this conversation.

423
00:30:44,480 --> 00:30:51,280
And I don't know how to answer your question, Juan, because this is when we talk about

424
00:30:51,280 --> 00:30:55,440
adapting our biology and thinking about an organization, like being able to

425
00:30:57,520 --> 00:31:04,080
adapt an organization like we would DNA, it's all new for me to think about.

426
00:31:05,040 --> 00:31:08,160
And the thing with goals, too, is they can be explicit or not.

427
00:31:08,160 --> 00:31:11,760
So like any psychoanalyst will tell you that you think you have goals.

428
00:31:13,040 --> 00:31:17,360
They may or may not be the actual goals that drive your behavior, right?

429
00:31:18,400 --> 00:31:23,200
We all know we have all kinds of internal modules that are trying to maximize and

430
00:31:23,200 --> 00:31:28,000
minimize various things that we don't necessarily have direct access to that may

431
00:31:28,000 --> 00:31:30,800
or may not be adaptive in our context and so on.

432
00:31:31,360 --> 00:31:37,680
So yeah, in the future, so I guess, I don't know anything about this field, but if I had

433
00:31:37,680 --> 00:31:45,120
to guess in the future, I sort of envisioned some kind of psychoanalysis of organizations where

434
00:31:46,560 --> 00:31:50,240
there's some way of kind of like we had this project where we were trying to

435
00:31:50,240 --> 00:31:55,120
communicate with an ant colony, not the ants, the colony, which is a completely different

436
00:31:55,120 --> 00:32:02,080
thing. It's very hard. But at some point, there might be techniques, the way that we try to talk

437
00:32:02,080 --> 00:32:07,600
to organs instead of the individual cells and so on. There might be ways to find out what does

438
00:32:07,600 --> 00:32:11,440
the actual organization want? And the individual people will tell you, oh, well, I know there's

439
00:32:11,440 --> 00:32:17,360
these set of goals. You don't know anything anymore than if you ask you the cells or the organs in

440
00:32:17,360 --> 00:32:22,080
your body, what are the goals of the organism? They'll tell you some stuff about physiological

441
00:32:22,880 --> 00:32:26,880
parameters, boundary parameters and some things like that. But they can't even begin to guess

442
00:32:26,880 --> 00:32:32,960
if your goal of whatever, going to grad school four years from now, they can't fathom it.

443
00:32:32,960 --> 00:32:37,760
So who knows what the organization is going to want. So in the future, there might be some,

444
00:32:37,760 --> 00:32:43,680
I think we need it. I think if we're going to survive and as a mature species, we're going to

445
00:32:43,760 --> 00:32:51,840
have to get our science around being able to identify, characterize, relate to

446
00:32:52,880 --> 00:32:58,160
and modify these kind of emergent, very unconventional emergent agents,

447
00:32:58,160 --> 00:33:05,200
which are anything from organizations to evolutionary lineages and things like that.

448
00:33:07,200 --> 00:33:10,720
I know you have a hard stop. So three minutes left. There's something you said, which just

449
00:33:10,720 --> 00:33:16,800
my ears perked up. Can you just spend a couple of minutes? When you say you want to speak to the

450
00:33:16,800 --> 00:33:23,840
ant colony itself and not the ant, that's an amazing visualization. My brain is so curious

451
00:33:23,840 --> 00:33:29,200
about, can you just describe more what that means? Sure. Yeah. And this is, you know, I'm

452
00:33:29,200 --> 00:33:34,160
by definitely not the first person to talk about this. Doug Hofstadter in his book,

453
00:33:34,880 --> 00:33:39,520
Gert Lecherbach had some great thoughts about it. And before that, in the 20s, Eugene Marais

454
00:33:39,520 --> 00:33:45,040
had this book called The Soul of the White Ant, which is just amazing about the kind of the life

455
00:33:45,040 --> 00:33:50,160
of the colony that's distinct from the individual members. So this idea that if we take collective

456
00:33:50,160 --> 00:33:54,240
intelligence seriously and this idea that the colony works in different problem spaces than

457
00:33:54,240 --> 00:33:57,840
the individual ants and they have, it has a memory of those things that no individual ant knows

458
00:33:57,840 --> 00:34:02,320
and things like that. So we wanted to communicate with it. And we took a very simplistic approach,

459
00:34:02,960 --> 00:34:07,600
which is that training is a simple kind of communication. So if we train you to do something,

460
00:34:07,600 --> 00:34:11,360
then we've communicated the fact that something is good and something else is bad. So you know,

461
00:34:11,360 --> 00:34:16,720
one act of communication has taken place, very minimal. So what we tried to do, and we haven't,

462
00:34:16,720 --> 00:34:20,560
this is something that started right during the pandemic, so it never like really got off the

463
00:34:20,560 --> 00:34:28,080
ground, but we'll, you know, we'll get to it eventually, is just imagine a colony and there's

464
00:34:28,080 --> 00:34:33,600
a location where little drops of food get dropped by a computerized system. But the amount of food

465
00:34:33,680 --> 00:34:37,600
that gets dropped there is proportional to the number of ants standing on a platform

466
00:34:37,600 --> 00:34:42,080
at the other end of the colony. So what that means is that no individual ant ever has the

467
00:34:42,080 --> 00:34:47,040
experience of I stand here and then I collect my reward. No ant has that experience. So now,

468
00:34:47,040 --> 00:34:51,120
if we find over time with training that the colony sends a bunch of ants over in this direction to

469
00:34:51,120 --> 00:34:56,560
pick up the food over here, that's a fact, that's a conditioning, that's a bit of learning that

470
00:34:56,560 --> 00:35:01,360
was done by the colony by no individual ant. And you know, it sounds all crazy or anything, but

471
00:35:01,360 --> 00:35:06,320
you know, when you train a rat to push a lever and get a pellet, the cells at the bottom of

472
00:35:06,320 --> 00:35:12,320
the foot are what's interacting with the button and the cells that get the reward are in the

473
00:35:12,320 --> 00:35:17,440
intestine. No individual cell has both experiences. So we've got this thing we call a rat, which is

474
00:35:17,440 --> 00:35:21,840
this collective that's able to do the credit assignment and figure out that these two things

475
00:35:21,840 --> 00:35:26,560
are related. That's the collective intelligence. We're all collective intelligences. And so we

476
00:35:26,560 --> 00:35:31,120
tried to communicate with the hand colony that way. So stay tuned. I don't know if that's going

477
00:35:31,120 --> 00:35:39,680
to work or not. Mind is blown right now. Thank you so much. Cool. Thank you. Yeah. It's a fun

478
00:35:39,680 --> 00:35:44,080
conversation. I appreciate it. Cheers. Thank you so much. Very nice to meet you. Thank you.

