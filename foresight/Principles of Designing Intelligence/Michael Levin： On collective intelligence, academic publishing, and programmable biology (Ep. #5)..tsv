start	end	text
0	12760	This podcast is about understanding how collaborative learning networks work and can be designed.
12760	20640	It's part of our effort to describe a methodology, which we call Unify, that can help align human
20640	25120	learning principles, which come from learning science, machine learning principles, and
25120	30160	also how to think about organizations as networks, how to be strategic in how networks
30160	38980	are designed, and what data to collect from networks in order to create new kinds of intelligence.
38980	45720	Our next guest is, in my mind, the authority, and I can't believe I got this interview,
45720	48240	in the field of collective intelligence.
48240	53800	If you haven't seen Michael Levin's TED Talk, it's amazing, I highly recommend it.
53800	61360	This conversation did not go as expected, because I got, frankly, a lot of really good
61360	68720	advice from, frankly, someone who was trying to help me out in terms of how can I propel
68720	71760	my work forward, and for that I'm personally grateful.
71760	73240	I hope you enjoy this.
73240	80920	Thank you so much.
80920	85160	Everything we do is related to this question of embodied minds, so I'm interested in how
85160	90760	very diverse kinds of intelligence can exist in our universe, in all sorts of different
90760	94960	manifestations, different scales, both of space and time.
94960	101560	We use a combination of computer science, developmental biology, biophysics, behavioral
101560	108600	science, computer science, to really try to understand how different degrees of agency
108600	113640	can be implemented in different embodiments.
113640	120760	We have parts of the lab are very theoretical, and do conceptual kinds of models in almost
120760	125480	philosophy, and in other parts of the bright code, and produce various tools and software,
125480	129920	and in other parts do various applications of these ideas.
129920	134840	We have applications in birth defects, and regenerative medicine, and cancer, and we
134840	137960	have some things in AI, and synthetic bioengineering, and so on.
137960	143080	We run the spectrum from very fundamental conceptual things to very practical things
143080	147600	that we hope will end up in the clinic at some point.
147600	150200	My background originally was computer science.
150200	154360	I did software engineering for a long time, scientific programming.
154360	160160	I got a degree in genetics after that, and I've been running a biology lab in the Allen
160160	167920	Discovery Center ever since.
167920	174680	Our goal is to create a collaborative intelligence framework, something that is reliable and robust,
174680	177760	a playbook to give to organizational leaders.
177760	186200	We don't believe that the latest AI algorithm or technology trend that needs to be implemented
186200	188080	is the solution.
188080	194880	We believe that there needs to be an understanding of the guiding principles of how do you design
194880	197440	collaborative intelligence systems?
197440	203760	How do you apply them with minimal or no technology?
203760	209400	Because of the lack of formal definitions, we're hoping we can draw inspiration from
209400	217160	the field, biology, and I'm wondering if there are any principles around collective
217160	223680	intelligence that you feel might be applicable to the domain of business?
223680	228960	Well, I guess because I'm not in this field, I could use a little more guidance as to what
228960	231200	the best case outcome would be.
231200	236160	Can you paint a picture for me of what you want to have happen?
236160	237560	What are we trying to improve?
237560	239640	What are we trying to achieve?
239640	242280	How would you recognize success if you cracked this problem?
242280	244880	What would it look like?
244880	253160	We would like to be able to model an organization using a paradigm from reinforcement learning
253240	258440	that doesn't mean we're going to want to use reinforcement learning, but the idea is to
258440	265400	track agents taking actions in their environments and measuring the outcomes in this case that
265400	273280	can be applied to human sequences of decisions or machine or humans and machines working
273280	276640	together collaboratively.
276640	286480	And one problem we have is that we're applying a reinforcement learning way of thinking to
286480	288040	the macro world.
288040	292680	It's not just applying it to code with a set of data.
292680	300440	It's trying to create a virtual twin of an organization and applying that way of thinking
300440	305160	from reinforcement learning to organizational design.
305160	315280	I think success is going to be having a framework and a methodology, but also being able to implement
315280	320840	it in terms of, let's say, an API, a set of standards.
320840	329200	And step one really is to investigate what are some principles around collaborative intelligence
329280	333400	that work that can be applied to organizations.
333400	339760	I think we're really at the beginning and I don't know if we have even defined yet what
339760	342200	success is.
342200	348760	I like to work backwards and I like to imagine the future that you want to see.
348760	349880	So what does that mean?
349880	354720	Does that mean the organization is functioning more efficiently towards specific goals?
354720	358000	Does that mean something about what those goals are?
358000	364600	Does that mean something about the individual happiness of the people participating in those
364600	365600	goals?
365600	371360	Does that mean something about developing some kind of dominant paradigm that pushes out
371360	374600	competitive views of some particular field?
374600	380760	I think step one is figuring out what, if you did, if you already had a successful theory
380760	384440	of all this stuff and you were able to put it into practice, what would the implications
384440	393720	be for reality, for how people run these things in the real world?
393720	399280	And so I don't know what the answer to this is, but I guess I can talk a little bit about
399280	407320	what we see in biology and then I don't love just blindly taking things from biology and
407320	411560	pushing them into, I mean, people often would like to do this, pushing them into social
411560	418240	and various kinds of societal contexts, but I think it's better to work backwards and
418240	421560	ask what we're trying to achieve and what that looks like.
421560	424320	I find in general that's missing from a lot of the discussions.
424320	428720	A lot of people have critiques about things that are going on now, whether it's AI or
428720	432240	whether it's something else, they can see all the problems and it's going to lead to
432240	434280	this, it's going to lead to that, we don't want it.
434280	437720	What I don't see as much as people articulating, what do you want to see?
437720	444240	What does future humanity look like that avoids these kinds of things that people are concerned
444240	445240	about?
445240	452760	With respect to all of the parameters that are currently under debate and I think that's
452760	458520	what I would do here as well as I would ask, what does the future of an optimal successful
458520	460200	organization look like?
460200	461200	How do you recognize it?
461200	470600	What are we aiming for?
470600	475480	But having said all that, I could tell you some things we learned from the biology.
475480	481640	One thing we learned from the biology is that one reason biology is so successful is that
481640	485920	it often changes the goals.
485920	491120	Pretty much like with artificial life and this notion of perverse instantiation, which
491120	495440	is you think you're trying to solve a problem in a particular way and if your system is
495440	501000	flexible enough, it might do something completely different that on retrospect, you can see
501000	504840	how that would solve the problem, but it isn't at all what you were looking for.
504840	508640	Biology does this all the time, when faced with a difficult problem, one thing biology
508640	512200	sometimes does is switch to a different problem.
512200	517200	If you look at the biosphere, there's every possible way of making a living and biology
517200	521720	is never tied to finding an answer to a specific problem.
521720	526880	They often change, it often changes the problem that it tries to solve and so that may or
526880	532520	may not work for us because you may come up with an organizational structure that is
532520	536640	very good at let's say perpetuating itself, but one of the ways that might do that is
536640	542200	to go off, completely go off script as far as where you thought the organization was
542200	546600	going to go and that may not, that works in the biosphere if your goal is to make sure
546600	552680	that life survives in some particular, you know, on a planet, that may not be that ability
552680	557360	to just completely change goals may not be what you want for an organization.
557360	565280	The other thing is that when you, biological agents and those could be cells, they could
565280	571160	be molecular networks, they could be tissues, organs, whatever, they're very good at combining
571160	579280	into higher level entities that are these emergent selves that do things in other problem spaces.
579280	583120	But they're not, there's a few things that are not guaranteed when that happens.
583120	589000	One thing that's not guaranteed is that the emergent self is smarter than the components.
589000	593360	It doesn't have to be, sometimes it certainly is, but it doesn't have to be and so that's
593440	597640	no guarantee, you know, scaling up is no guarantee that it's going to be more intelligent.
597640	610320	Also, when you create these novel selves, you simultaneously create various goals, preferences
610320	620520	and various kinds of salience and valence for these things that don't necessarily match
620520	622040	those of the subunits.
622040	627960	So in other words, people often say, you know, people often hear the story, I tell of gap
627960	632400	junctions and cancer, this idea that cells are tied together with gap junctions that
632400	636480	basically erase their individuality a little bit so that they're all part of this collective
636480	638720	and they work on making the organs and whatnot.
638720	643000	And then when there's a breakdown of that, cells roll back to the kind of unicellular
643000	646320	form and they go off and they metastasize and so on.
646320	652000	And so people hear that and often they say, well, that's clear what we have to do for
652000	655920	the various ills of society, you know, we can address that the whole selfishness thing
655920	656920	and so on.
656920	661360	We'll just sort of gap junctions ourselves, take it to each other in some format and when
661360	665640	wipe our individuality a little bit and we'll be this collective.
665640	671640	I think that's a, in general, that's a terrible idea because one of the clear things is that
671640	678040	when you as a large scale individual, let's say a human goes off and does various things.
678040	680880	My silly example is rock climbing.
680880	684560	You know, you go rock climbing and you have a lovely day and you meet various social goals,
684560	688480	you meet various personal happiness goals and so on as a human.
688480	692520	But you've left a bunch of skin cells on that, you know, on that rock.
692520	696360	And nobody asked those skin cells whether this was something that they were interested
696360	697360	in.
697360	702600	And just in general, the basic premise of having a multicellular body is that the vast majority
702600	708840	of your body cells are going to die in the service of whatever weird to them incomprehensible
708840	711600	goals that the collective human might have.
711600	720080	So that stuff is very easily scaled up into social structures where you might have persistent,
720080	724500	you know, persistent kinds of government, persistent kinds of companies and so on that
724500	729760	pursue goals that are at odds with the well-being, the happiness and the goals of the beings
729760	731760	that make them up.
731760	738320	And that needs to be, it needs to be clear that when you scale up like that, you are
738320	739320	not guaranteed.
739320	743800	In fact, the big thing is that we don't have a good science of being able to anticipate
743800	746000	the goals of collective systems.
746000	750880	We really can't anticipate that very well now at all.
750880	760480	I talk about these biology examples all the time in terms of like, we make something called
760480	764720	a frog allotle in our lab and so this is part frog and part axolotl.
764720	770640	And so baby axolotls have little forelegs and tadpoles don't have forelegs.
770640	775880	And so when we make this frog allotle, I often say this in my talk, you know, we know the
775880	778720	genome for the frog, we know the genome for the axolotl.
778720	782040	Can you tell me if a frog allotle is going to have legs or not, right?
782040	784400	And there's no one can predict that up front.
784400	788720	And that's because while we understand the molecular hardware to some extent of the
788720	791880	pieces, we do not understand how the collective decisions are made.
791880	796280	We don't know how they decide to make these large scale structures and to say that, you
796280	798040	know, there's a million examples of that.
798040	802320	So that's the thing when we make these, when we make these collectives, we really don't
802320	807400	know what the goal of the, if there is going to be an emergent agent, we don't know what
807400	808400	the goal is going to be.
808400	814320	And we don't know if it at all matches our goals, even if we, I mean, even our own, we
814320	818040	don't match goals with each other, but never mind, or even if never mind that.
818040	819040	Yeah.
819040	821240	So, so I think, so I think that's the danger, right?
821240	826120	So I think biology can teach us some very successful principles about making sure you
826120	831880	stick around, but I'm not sure that's the end all and be all that we want from the structures
831880	833320	that we build, right?
833320	834600	Persistence isn't really what we're looking for.
834600	836480	We're looking for something much more specific.
836480	846160	So you got to, you got to take all that biology stuff with a, with a grain of salt.
846160	850240	So one of my favorite things as a consultant is that I get to speak with individuals where
850280	855800	very aware of their problems, very aware of goals and the financial value of their goals.
855800	861440	But when it comes time to connect the dots between these different teams and more of
861440	866560	a collective intelligence or collaborative intelligence approach, typically there's
866560	873760	no person in charge or there's no system or process or computational way that combines
873800	876880	them all and analyzes them.
876880	885960	And I found this gap to be really intriguing and I want to pursue formalizing a process
885960	890360	that could be something that we could publish and like, let's say an academic journal or
890360	896400	paper to help push this area of research forward.
897400	904240	And I think the one challenge I've had is like simply defining, you know, what is collective
904240	908120	intelligence for an organization, what is collaborative intelligence, because I hear
908120	914200	different people, different startups talking about those two and, and yet it's all very
914200	915200	fuzzy.
915200	925360	So I thought that trying to pursue a more formalized definition that respected kind
925400	930640	of academic perspectives would be a good approach for us to have.
931120	932120	Yeah.
932320	933760	You know, I don't know.
933760	935040	That's that's a whole other thing.
935040	940040	This this like meta meta goal seeking, which is what what should the goals be?
940040	944280	So so gaining respect of the academic community.
944280	951000	I'm not even sure that's an achievable goal per se for anybody because there is no monolithic
951000	952000	academic community.
952040	959120	So there are different islands of academics where like so so so there's a there's a particular
959120	964040	kind of standard talk that I sometimes give and I always know what kind of department
964040	967960	I'm giving the talking based on which part it makes people angry and it's often a different
967960	971920	it's always a different part because there are certain things you can say in a in a neurobiology
971920	973520	department and everybody's like, yeah, no kidding.
973520	977440	So what we knew that and the exact same thing in the genetics department and people throw
977480	978480	tomatoes.
978480	986760	And so trying for kind of generic acceptance, I'm not sure that's feasible until until we
986760	994440	get a lot better at breaking down these these kind of conceptual silos that the communities
994440	995440	are in.
995440	1002480	And also, you know, what's the what's the payoff of that?
1002480	1006080	You know, you want you what I mean, what do we actually what you know, what do we actually
1006120	1007120	want?
1007120	1011240	Why we write these scientific papers on what what what is the impact?
1011240	1016800	I mean, I think there's been a lot of talk of impact in the last decade, maybe more.
1016800	1020320	And it's on the one hand, I think it's terrible.
1020320	1022480	And on the other hand, I think it's really good.
1022480	1028240	It has good kind of effects, which which really forces us to think about what do you really
1028240	1033440	want to have happen when you write that paper, right, besides the kind of obvious stuff of,
1033440	1037120	you know, tenure or whatever, you know, get a job or you know, get a good position or
1037120	1038120	whatever.
1038120	1040880	But like long term, the change you're trying to make in the world, what happens when when
1040880	1042400	you write these papers?
1042400	1046600	So in theory, I mean, what are some possible answers to that?
1046600	1049760	Sometimes people read it and they get inspired and they do something else.
1049760	1050760	So that's cool.
1050760	1055160	But maybe you're better off just doing that other thing yourself directly, right?
1055160	1057200	That's one possibility.
1057200	1065240	Another possibility is that maybe you can unify people across fields so that they sort
1065240	1068400	of bring some interdisciplinary thought to that.
1068400	1069400	So that's so that's pretty good.
1069400	1072840	That's that's, and I don't know if that's gaining acceptance, that's more gaining interest,
1072840	1073840	right?
1073840	1076560	So you want people to start thinking about things and asking questions that of things
1076560	1080760	that they may have taken on, you know, as an assumption before that.
1080760	1081760	So that's pretty good.
1082720	1089840	But you know, the actual impact of academic papers, even in academia is often not clear,
1089840	1091760	but outside of academia.
1091760	1092760	I don't know.
1092760	1097160	I mean, you know, Chris Fields had a great sentence recently where he said, arguments
1097160	1099880	are only settled by technologies.
1099880	1101440	And I think that's I think that's true.
1101440	1107480	I think I think we can have all kinds of abstruse academic arguments about things and
1107480	1109680	people have all kinds of commitments to stuff.
1109680	1114640	But in the end, if the idea is a good one, it's going to the rubber is going to hit the
1114640	1118400	you hit the road and you're going to make some sort of impact on the physical world.
1118400	1122600	And there's going to be some sort of application, some sort of practical thing.
1122600	1127080	And then it's going to be irrelevant, which part of academia likes it and which part doesn't.
1127080	1133360	So I don't know, you know, I think I think you got it again, it's about working backwards
1133360	1139640	to ask what's the change that you want to see and how much of that change is requires
1139640	1145080	I am buy in from academia, which part of academia, whether papers are sufficient.
1145080	1150520	And there's a whole other thing, which is that there are so many papers now that I,
1150520	1156840	you know, most people don't have time to even catch up with the things in their field anymore.
1156840	1158640	Nevermind, you know, sort of other things.
1158640	1162360	I sometimes get the feeling that all these papers were writing, but not really for us.
1162360	1166480	They're for some sort of future AI or some sort of augmented future scientists who's
1166480	1170280	actually going to be able to have the bandwidth to read all this stuff.
1170280	1173040	Because I mean, I don't know, you can't see it, but I've got behind my chair,
1173040	1176960	I've got these like stacks of papers that I'm supposed to have read by now.
1176960	1182200	And they're just getting bigger and bigger because there's no time to do it.
1182200	1187680	So yeah, I would I would work backwards and I would think about whose eyeballs are we
1187680	1194280	trying to capture here, right on these papers and what's the what's the what's the goal of that, you know.
1194360	1197600	This conversation did not go at all like I was expecting.
1197600	1206400	I am really grateful that you challenged me on some of my current beliefs around pursuing
1206400	1212240	an academic approach to defining collective intelligence or collaborative intelligence
1212240	1218000	from kind of a theoretical and academic perspective for organizations to implement.
1218000	1220520	I have to do a lot of thinking on this.
1220520	1231120	And as I reflect, the questions that come to mind as I was listening were,
1231120	1233680	how does your lab pursue success?
1233680	1241280	And how does your lab think about translating theory into application?
1241280	1248360	I view academia serving a higher purpose to make the world a better place through creation
1248360	1252160	and curation and sharing of knowledge.
1252160	1257360	And we'd love to hear some of what you've learned works.
1263280	1268920	First, we generate a lot of very basic fundamental knowledge and also conceptual apparatus.
1268920	1274920	So we come up with very sort of fundamentally almost philosophical like like perspectives
1274920	1280200	on things and then that gets cashed out as software or various computational paradigms
1280200	1285280	and then that gets cashed out in new experiments and new new capabilities and so on.
1285280	1292120	And I firmly believe that to the extent that we're on the right track with any of this stuff,
1292120	1297000	really kind of transformative applications should follow.
1297000	1304280	And this is why we now have a spin off company doing regeneration and we have one doing computer
1304280	1309600	AI designed synthetic living machines and there's another one on cancer.
1309600	1313800	So like this stuff is moving towards, hopefully towards the clinic.
1313800	1319680	So we're not in humans patients yet by any means, but we're going in that direction.
1319680	1330080	And the other thing too is that I kind of try to strike a balance because I'm not even a clinician.
1330080	1332560	OK, I don't do clinical work at all.
1332560	1340160	I get emails and phone calls every day from people with the most unbelievable medical needs.
1340160	1345560	I mean, you just can't imagine what's out there, the need that's out there and the things that happen
1345560	1351000	to people from all the way from birth defects to do various other things that happen in your lifetime.
1351000	1353040	The need is intent.
1353040	1360760	And so I consider it, you know, I would not consider it a success if all we ever did was,
1360760	1368360	you know, generate some kind of, you know, conceptual stuff and basic science that never helped patients.
1368360	1373800	I want to see, I want to see in my lifetime Indiana World, we'll see if that happens or not.
1373800	1378040	But in my lifetime, I want to see actual patients help with the stuff that we do.
1378040	1380520	So it's kind of a it's kind of a two pronged attack.
1380520	1386680	And, you know, you can always argue about what the right prioritization is, you know,
1386680	1390680	and some people say, you know, don't get tied up with this with the with the stuff with these,
1390680	1394120	you know, spending time with these companies and figuring out how to, you know,
1394120	1395960	make products and argue with the FDA.
1395960	1398600	You know, you should be spending your time on basic basic science.
1398600	1401560	And then other people write me and they say, and they say the opposite.
1401560	1406640	Like, what are you doing spending time figuring out, you know, things about the nature of the self?
1406640	1408880	Like, you could have, you know, you should be solving cancer.
1408880	1412320	And and and so, you know, you kind of you kind of juggle those two things.
1412320	1417720	But I think I think I think we're pretty fortunate in my group that I think we can do some of both,
1417720	1420000	actually, and I think they're very tightly related.
1420000	1423480	I think if you do it right, you can make impacts on both.
1423480	1425320	So that's how I see it.
1425560	1439920	I'm imagining the world you see as we will be able to regrow an arm.
1439920	1444160	If someone were an accident and were to lose it, is that correct?
1444160	1449520	Embodiment that you got through the vagaries of genetics and and evolution.
1449520	1453240	You could change it the way the way we change many other things.
1453240	1458960	You want tentacles, you want you want to see an infrared, you want to live underwater?
1458960	1460320	Like, why not? Right?
1460320	1467000	Why not? Who said who said this this random walk that evolution took to to get you here is how it has to stay?
1467000	1471120	I think down the line, I mean, you know, without like scaring everybody and so on,
1471120	1472680	because a lot of people find that pretty freaky.
1472680	1477000	But but I'm certainly not the first person to say stuff like this down the line.
1477000	1481640	I believe in maximizing freedom, which includes freedom of embodiment.
1481640	1486000	You should not be locked into some random configuration that's susceptible to weird diseases
1486000	1490040	and aging and and and you know, dumb stuff that can happen to you from stepping on the wrong,
1490040	1496360	you know, patch of ground, you know, it's all the stuff is just it's it's it's the way the way,
1496360	1501400	you know, we we we we started out with antibiotics and, you know,
1501400	1503640	and wearing clothes against the cold and things like that.
1503640	1504760	And that was it after that.
1504760	1509600	Like it became completely obvious that we do not have to stay the way we came into the world.
1509600	1513160	So so amazing future.
1513160	1514840	I sign up for that vision.
1514840	1517960	I think that sounds amazing.
1517960	1518920	That's amazing.
1518920	1526400	And to me, it seems like it's leaving the boundaries of what does it even mean to be human?
1526400	1528720	Yeah, no, that's I mean, so two things, right?
1528720	1532600	One is that I don't I don't think these are boundaries at all.
1532600	1533680	There are no boundaries.
1533680	1536960	The boundary is us not knowing how to work the interface.
1536960	1541680	So if if you're given a calculator and you have no idea what this thing is or how to use it,
1541680	1544720	there's a real boundary between you and the capabilities of that calculator.
1544720	1545360	But it's not real.
1545360	1552800	It's it's it's it's a boundary of ignorance and and and those things are are are are
1552800	1553760	improvable, right?
1553760	1555600	So you can we can we can we can do that now.
1555600	1559520	We have a process, the scientific process and some other stuff that that can help us get through that.
1559520	1561280	So I don't believe in these boundaries at all.
1562240	1566640	Then, you know, and then and and not only, I mean, let's let's let's face it.
1566640	1570160	We we haven't believed in these boundaries for a long time.
1570160	1576720	If you you walk into any gym or, you know, a martial arts studio or a call or a university
1576720	1579520	and you see people removing their boundaries, right?
1579520	1581600	You don't come out of there the way you went in.
1581600	1582960	You come out with extra powers.
1582960	1587120	You come out with whether that be new brain power or muscles or or you know,
1587120	1590480	you learn to swim and hold your breath underwater or whatever you know,
1590560	1595280	whatever you're going to learn, we know we can improve ourselves and the only limitation.
1595280	1599600	And frankly, some people are amazing and they've pushed it even without all this technology
1599600	1600560	that I'm talking about.
1600560	1601600	They've already pushed us.
1601600	1605280	I mean, right, we've all seen there are there are humans that are so on some particular
1605280	1606640	thing that they've dedicated the life to.
1606640	1610800	They're so far outside the mean that it's just it's it's unbelievable that even that that's
1610800	1611680	even possible.
1611680	1616800	And that's and that's without knowing, frankly, very much at all about about how biology works yet.
1617360	1620960	So so whether mental, whether physical, whether spiritual, I think that's important.
1620960	1622960	There are there are no boundaries.
1622960	1625680	This is all the all these boundaries are defined by by our own ignorance.
1626240	1629440	And then the other important thing you said is, are we leaving what it means to be human?
1629440	1630560	So that's an interesting question.
1631920	1637600	What do we mean when we say human, in particular, because of this whole AI thing,
1637600	1642640	somebody I forget who it is, but somebody's working on these proof of humanity certificates,
1642640	1643040	right?
1643040	1644720	And it's a good it's a good thing to think about.
1644800	1649520	If if somebody if if you're interacting with somebody and they and they show you their their
1649520	1652560	little stamp or whatever it's going to be, that that's the proof of humanity.
1653200	1655280	What is it that you really are looking for?
1655280	1655520	Right?
1655520	1659280	When when you're looking for that proof of humanity in someone or when you want to say
1659280	1663040	whether someone's left, you know, kind of the human category, what are you really looking for?
1664400	1670640	Are you looking to validate that their DNA is the same that evolution left us in as homo sapiens?
1670640	1671360	I don't think so.
1671360	1673040	Do you really care about anybody's DNA?
1673040	1673360	I don't.
1674320	1676480	I don't think that's that's relevant for anything.
1677040	1678640	Is it is it body structure?
1678640	1682800	Are you do you want confirmation that that that that person hasn't had some percentage
1682800	1685360	of their organs replaced by various prosthetics?
1685360	1686560	I don't care about that either, right?
1686560	1687760	Does that matter to you?
1687760	1688320	I don't know.
1689040	1691840	So what are so what is that when we say somebody's human?
1691840	1693040	What do we actually mean?
1693680	1699280	And I think and I'm sure other people have different different, you know, different definitions for it.
1699280	1703920	But I think what you mean is a minimal level of compassion.
1703920	1704720	That's what I think.
1704720	1707840	What you're looking for when somebody says they're human, you're looking for a cognitive
1707840	1713840	light cone on that individual that is able to actively able to care at least maybe more,
1713840	1718800	but at least to the same level that you can about others, about various, you know, about
1718800	1724160	various goals that when when if, you know, if I am trying to choose, you know,
1724160	1726880	say you're going to go live on Mars or something and you're choosing a companion,
1727120	1733120	and what you want in that proof of humanity is not anything but their DNA or, you know,
1733120	1735120	whether they have some prosthetic organs or something.
1735120	1739600	What you want is do they have the capacity to care about the same stuff that I care about
1739600	1741120	the same degree of stuff?
1741120	1745280	Because if they don't, if they're a rumba or, you know, or a cat or something else,
1746160	1750320	nice, but but not the same relationship you can have with a human, right?
1750320	1751840	So so that's what I think humans are.
1751840	1756160	And after that, everything is everything else is for is up for grabs.
1756160	1760480	You've got gills, you've got, you know, your third brain hemisphere with like whatever,
1760480	1761680	you know, who cares?
1761680	1763360	So that's my that's my view of it.
1764160	1769120	I mean, going macro, I think from that, sorry, Ron, but I think it's like one thing that was
1769120	1773680	like thinking about because you're talking about these, these, I guess, like boundaries or like
1773680	1776880	what is our like predefined view of what it means to be human.
1776880	1781520	And I think if you take even that macro to the stuff that, for example, Ron and I are working on,
1781600	1788240	is that we are working so hard to try to map out organizations to try to understand organizations.
1788240	1790000	But what does even an organization mean?
1790720	1795840	Yeah, are we, are we actually thinking about organizations in our preconceived ways of
1795840	1799600	thinking about them and not thinking what an organization could be?
1799600	1802720	It's like, are we just mapping the status quo, whereas we could
1804080	1810000	helping organizations evolve in the same way that you're saying humans can evolve way
1810080	1811920	farther than everybody's thinking about now.
1811920	1814480	And I think that's a very interesting, you know, angle.
1814480	1815200	What do you think, Ron?
1816160	1817520	That's a fascinating question.
1818400	1823440	Level one, let's say, is what are the goals of an organization just listing them out
1823440	1824880	among the individual nodes?
1824880	1831920	Then you could say level two is what are the connections between what people care about
1831920	1834560	and how are they mapped together?
1835360	1840800	And then level three is, well, how are they adapting and changing together?
1840800	1843520	I think that's new for me to think about from this conversation.
1844480	1851280	And I don't know how to answer your question, Juan, because this is when we talk about
1851280	1855440	adapting our biology and thinking about an organization, like being able to
1857520	1864080	adapt an organization like we would DNA, it's all new for me to think about.
1865040	1868160	And the thing with goals, too, is they can be explicit or not.
1868160	1871760	So like any psychoanalyst will tell you that you think you have goals.
1873040	1877360	They may or may not be the actual goals that drive your behavior, right?
1878400	1883200	We all know we have all kinds of internal modules that are trying to maximize and
1883200	1888000	minimize various things that we don't necessarily have direct access to that may
1888000	1890800	or may not be adaptive in our context and so on.
1891360	1897680	So yeah, in the future, so I guess, I don't know anything about this field, but if I had
1897680	1905120	to guess in the future, I sort of envisioned some kind of psychoanalysis of organizations where
1906560	1910240	there's some way of kind of like we had this project where we were trying to
1910240	1915120	communicate with an ant colony, not the ants, the colony, which is a completely different
1915120	1922080	thing. It's very hard. But at some point, there might be techniques, the way that we try to talk
1922080	1927600	to organs instead of the individual cells and so on. There might be ways to find out what does
1927600	1931440	the actual organization want? And the individual people will tell you, oh, well, I know there's
1931440	1937360	these set of goals. You don't know anything anymore than if you ask you the cells or the organs in
1937360	1942080	your body, what are the goals of the organism? They'll tell you some stuff about physiological
1942880	1946880	parameters, boundary parameters and some things like that. But they can't even begin to guess
1946880	1952960	if your goal of whatever, going to grad school four years from now, they can't fathom it.
1952960	1957760	So who knows what the organization is going to want. So in the future, there might be some,
1957760	1963680	I think we need it. I think if we're going to survive and as a mature species, we're going to
1963760	1971840	have to get our science around being able to identify, characterize, relate to
1972880	1978160	and modify these kind of emergent, very unconventional emergent agents,
1978160	1985200	which are anything from organizations to evolutionary lineages and things like that.
1987200	1990720	I know you have a hard stop. So three minutes left. There's something you said, which just
1990720	1996800	my ears perked up. Can you just spend a couple of minutes? When you say you want to speak to the
1996800	2003840	ant colony itself and not the ant, that's an amazing visualization. My brain is so curious
2003840	2009200	about, can you just describe more what that means? Sure. Yeah. And this is, you know, I'm
2009200	2014160	by definitely not the first person to talk about this. Doug Hofstadter in his book,
2014880	2019520	Gert Lecherbach had some great thoughts about it. And before that, in the 20s, Eugene Marais
2019520	2025040	had this book called The Soul of the White Ant, which is just amazing about the kind of the life
2025040	2030160	of the colony that's distinct from the individual members. So this idea that if we take collective
2030160	2034240	intelligence seriously and this idea that the colony works in different problem spaces than
2034240	2037840	the individual ants and they have, it has a memory of those things that no individual ant knows
2037840	2042320	and things like that. So we wanted to communicate with it. And we took a very simplistic approach,
2042960	2047600	which is that training is a simple kind of communication. So if we train you to do something,
2047600	2051360	then we've communicated the fact that something is good and something else is bad. So you know,
2051360	2056720	one act of communication has taken place, very minimal. So what we tried to do, and we haven't,
2056720	2060560	this is something that started right during the pandemic, so it never like really got off the
2060560	2068080	ground, but we'll, you know, we'll get to it eventually, is just imagine a colony and there's
2068080	2073600	a location where little drops of food get dropped by a computerized system. But the amount of food
2073680	2077600	that gets dropped there is proportional to the number of ants standing on a platform
2077600	2082080	at the other end of the colony. So what that means is that no individual ant ever has the
2082080	2087040	experience of I stand here and then I collect my reward. No ant has that experience. So now,
2087040	2091120	if we find over time with training that the colony sends a bunch of ants over in this direction to
2091120	2096560	pick up the food over here, that's a fact, that's a conditioning, that's a bit of learning that
2096560	2101360	was done by the colony by no individual ant. And you know, it sounds all crazy or anything, but
2101360	2106320	you know, when you train a rat to push a lever and get a pellet, the cells at the bottom of
2106320	2112320	the foot are what's interacting with the button and the cells that get the reward are in the
2112320	2117440	intestine. No individual cell has both experiences. So we've got this thing we call a rat, which is
2117440	2121840	this collective that's able to do the credit assignment and figure out that these two things
2121840	2126560	are related. That's the collective intelligence. We're all collective intelligences. And so we
2126560	2131120	tried to communicate with the hand colony that way. So stay tuned. I don't know if that's going
2131120	2139680	to work or not. Mind is blown right now. Thank you so much. Cool. Thank you. Yeah. It's a fun
2139680	2144080	conversation. I appreciate it. Cheers. Thank you so much. Very nice to meet you. Thank you.
