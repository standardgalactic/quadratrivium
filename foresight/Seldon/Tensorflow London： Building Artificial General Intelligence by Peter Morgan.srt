1
00:00:00,000 --> 00:00:10,000
Now, so I'm going to talk about EGI.

2
00:00:10,000 --> 00:00:13,000
I've got long, I've got lots of slides, so I'll move very quickly.

3
00:00:13,000 --> 00:00:16,000
Here's a report I wrote for Riley.

4
00:00:16,000 --> 00:00:22,000
There's a company I started as an AI consulting company.

5
00:00:22,000 --> 00:00:24,000
And so what are we going to talk about?

6
00:00:24,000 --> 00:00:28,000
So the first question we have to ask, and I apologize if you've seen this talk before,

7
00:00:28,000 --> 00:00:32,000
I had given it a few times, hopefully there's none in the room who has.

8
00:00:32,000 --> 00:00:38,000
What is intelligence and then what physical systems are available for us to get inspiration from

9
00:00:38,000 --> 00:00:42,000
and how far have we come in building the hardware.

10
00:00:42,000 --> 00:00:47,000
And then I'm going to look at very briefly deep learning as, you know,

11
00:00:47,000 --> 00:00:51,000
just ask very quickly the question, are we on the right track?

12
00:00:51,000 --> 00:00:55,000
And then with deep learning, is that going to get us all the way to EGI?

13
00:00:55,000 --> 00:00:58,000
The answer is no, spoiler.

14
00:00:58,000 --> 00:01:00,000
And so what do we need, what's missing?

15
00:01:00,000 --> 00:01:03,000
I'll do a very quick overview at the end.

16
00:01:03,000 --> 00:01:07,000
And we'll look at one particular, one theory in particular called active inference.

17
00:01:07,000 --> 00:01:13,000
And amongst a whole, you know, zoo of theories, really, EGI theories.

18
00:01:13,000 --> 00:01:15,000
We'll pick that one out, I'll explain why.

19
00:01:15,000 --> 00:01:18,000
And then I'll wrap it up with conclusions.

20
00:01:18,000 --> 00:01:20,000
So, you know, why do we want to build it?

21
00:01:20,000 --> 00:01:22,000
Well, it's kind of obvious, right?

22
00:01:22,000 --> 00:01:27,000
We want to build it to solve the problem of the century, probably,

23
00:01:27,000 --> 00:01:31,000
of the human race, maybe, arguably, so I might argue.

24
00:01:31,000 --> 00:01:36,000
And then, you know, to make lots of money doing it, but really it's just to solve it.

25
00:01:36,000 --> 00:01:40,000
And see, and then once we solve it, as DeepMind said, we can use it to solve everything else.

26
00:01:40,000 --> 00:01:46,000
That includes all science or medicine, how to live longer, how to be alive,

27
00:01:46,000 --> 00:01:51,000
new materials, you name it, you know, spacecraft, explore the galaxies, use your imagination.

28
00:01:51,000 --> 00:01:53,000
So that really is the aim, right?

29
00:01:53,000 --> 00:01:57,000
To build general level intelligence and quite quickly, you know,

30
00:01:57,000 --> 00:02:01,000
move beyond that to superintelligence.

31
00:02:01,000 --> 00:02:04,000
So, you know, a quick question I always ask the audience.

32
00:02:04,000 --> 00:02:06,000
Are any of these general intelligence?

33
00:02:06,000 --> 00:02:10,000
So we've got Watson there, Super Impressive, you know,

34
00:02:10,000 --> 00:02:15,000
we've got the chess playing Deep Blue, IBM, Alpha Star, just very recently,

35
00:02:15,000 --> 00:02:20,000
you know, DeepMind built that system that could beat some of the best players at Starcraft.

36
00:02:21,000 --> 00:02:24,000
And online game and AlphaGo.

37
00:02:24,000 --> 00:02:29,000
So, no, no matter how impressive all of these are, and they certainly were,

38
00:02:29,000 --> 00:02:34,000
you know, years into making 10, 20, 30 years since the birth of AI, maybe,

39
00:02:34,000 --> 00:02:38,000
they're not, they're nothing to do with AI.

40
00:02:38,000 --> 00:02:43,000
So, and here's why, because intelligence is much broader than that.

41
00:02:43,000 --> 00:02:45,000
Remember, we asked the question, what is intelligence?

42
00:02:45,000 --> 00:02:50,000
Well, a guy called Howard Gardner at Harvard, I think in the 90s,

43
00:02:50,000 --> 00:02:53,000
wrote down quite a nice way of categorizing it.

44
00:02:53,000 --> 00:02:56,000
So you put it into nine categories.

45
00:02:56,000 --> 00:03:00,000
So we've got naturalist understanding nature, the external world, can go do that, no.

46
00:03:00,000 --> 00:03:04,000
You know, and I want to keep going, can go do that, can go do that.

47
00:03:04,000 --> 00:03:06,000
The answer is no, no, no.

48
00:03:06,000 --> 00:03:08,000
The spatial visualization.

49
00:03:08,000 --> 00:03:13,000
So we have AI, you know, that can do maybe one of these really well,

50
00:03:13,000 --> 00:03:15,000
but never more than two, right?

51
00:03:15,000 --> 00:03:17,000
So we're nowhere near AI.

52
00:03:17,000 --> 00:03:24,000
Intrapersonal introspection, you know, there's a TPU sitting in a Google Data Center.

53
00:03:24,000 --> 00:03:27,000
No matter what algorithm you run on it, it doesn't go, why am I here?

54
00:03:27,000 --> 00:03:29,000
Right, why am I a TPU who made me?

55
00:03:29,000 --> 00:03:31,000
Right, no.

56
00:03:31,000 --> 00:03:33,000
This is all part of intelligence.

57
00:03:33,000 --> 00:03:35,000
Introspection, even though I make light of it, is, you know,

58
00:03:35,000 --> 00:03:38,000
a very important aspect of intelligence.

59
00:03:38,000 --> 00:03:41,000
Language, linguistics, we have NLP.

60
00:03:41,000 --> 00:03:46,000
We can have things that read and classify sentences.

61
00:03:46,000 --> 00:03:49,000
And, you know, are they created?

62
00:03:49,000 --> 00:03:55,000
It's mostly based on statistical analysis of words and images and pixels and whatnot.

63
00:03:55,000 --> 00:03:59,000
It's not quite how the brain does it, or the biological brain.

64
00:03:59,000 --> 00:04:01,000
We have movement.

65
00:04:01,000 --> 00:04:04,000
You know, the best assets in the world get paid, you know,

66
00:04:04,000 --> 00:04:08,000
all players getting transferred for 100, 200 million pounds, right?

67
00:04:08,000 --> 00:04:10,000
I mean, there's some of the highest-paid people in the world,

68
00:04:10,000 --> 00:04:12,000
because they use their body so well.

69
00:04:12,000 --> 00:04:16,000
The basketball players in America, the football players in Europe.

70
00:04:16,000 --> 00:04:18,000
So that's a part of intelligence.

71
00:04:18,000 --> 00:04:20,000
And, you know, we have robotics, right?

72
00:04:20,000 --> 00:04:24,000
So there's nowhere, we haven't got any robots that can play football like Nestle,

73
00:04:24,000 --> 00:04:27,000
or, you know, the Beth, Michael Jordan, and muscle balls.

74
00:04:27,000 --> 00:04:28,000
They work very close.

75
00:04:28,000 --> 00:04:31,000
Although, you know, impressive progress has been made with that.

76
00:04:31,000 --> 00:04:33,000
You see it being backflips and whatnot.

77
00:04:33,000 --> 00:04:35,000
Super impressive, right?

78
00:04:35,000 --> 00:04:38,000
It's quite general, even that one small limit of sense.

79
00:04:38,000 --> 00:04:41,000
Interpersonal, how we communicate with each other in the world.

80
00:04:41,000 --> 00:04:43,000
Animals, ourselves.

81
00:04:43,000 --> 00:04:47,000
Interpersonal, communications, modeling what the other person is thinking, right?

82
00:04:47,000 --> 00:04:50,000
And then predicting what their next move will be.

83
00:04:50,000 --> 00:04:52,000
Now, we know that.

84
00:04:52,000 --> 00:04:53,000
It's essential.

85
00:04:53,000 --> 00:04:54,000
That's the way we hear it.

86
00:04:54,000 --> 00:04:57,000
And the logical mathematical path that we've kind of got.

87
00:04:57,000 --> 00:05:00,000
I would argue, you know, fight with it, chess, and go,

88
00:05:00,000 --> 00:05:02,000
and numerical stuff and calculate it.

89
00:05:02,000 --> 00:05:04,000
It's a very simple calculator.

90
00:05:04,000 --> 00:05:07,000
So we probably were okay in that quadrant.

91
00:05:07,000 --> 00:05:12,000
And then musical and other creative art music, writing poems, just, you know,

92
00:05:12,000 --> 00:05:16,000
but mostly musical, you know, writing would be linguistic.

93
00:05:16,000 --> 00:05:20,000
So art, the creative fine arts.

94
00:05:20,000 --> 00:05:25,000
Yeah, you know, AI, narrow AI has written some songs,

95
00:05:25,000 --> 00:05:28,000
but, you know, use the word written very loosely.

96
00:05:28,000 --> 00:05:31,000
You know, one could argue again, it's statistical, not creative.

97
00:05:31,000 --> 00:05:33,000
So what are we missing?

98
00:05:33,000 --> 00:05:36,000
So, you know, how far have we come?

99
00:05:36,000 --> 00:05:40,000
Well, like, the highest is probably logical mathematical at 50%.

100
00:05:40,000 --> 00:05:43,000
So calculation is great, but not creativity.

101
00:05:43,000 --> 00:05:48,000
You know, they can't really invent new games of go or new types of math yet.

102
00:05:48,000 --> 00:05:49,000
Okay.

103
00:05:49,000 --> 00:05:54,000
You know, there's mathematicians in the world, the guys who gets the field there that we haven't,

104
00:05:54,000 --> 00:05:57,000
they can't, you know, do math at the level of the field there,

105
00:05:57,000 --> 00:05:59,000
that's sort of a Nobel Prize winner.

106
00:05:59,000 --> 00:06:02,000
And linguistic is good as they are.

107
00:06:02,000 --> 00:06:05,000
It's in creating stuff and complete sentence,

108
00:06:05,000 --> 00:06:08,000
completion and, you know, sentiment analysis.

109
00:06:08,000 --> 00:06:10,000
It's a statistical analysis.

110
00:06:10,000 --> 00:06:12,000
It's not what we're doing.

111
00:06:12,000 --> 00:06:14,000
It's not a creative thing.

112
00:06:14,000 --> 00:06:17,000
And then I don't know how to read the list, right?

113
00:06:17,000 --> 00:06:21,000
Spatial, we've got the span and body, you've got atlas.

114
00:06:21,000 --> 00:06:26,000
You know, in understanding the environment, I thought 10%, that's a generous 10% music,

115
00:06:26,000 --> 00:06:29,000
you know, Google, the Gentile, 50%.

116
00:06:29,000 --> 00:06:31,000
These are all pretty generous numbers, I would say.

117
00:06:31,000 --> 00:06:34,000
So basically, you know, we know we need a general intelligence.

118
00:06:34,000 --> 00:06:37,000
We're not at 100%, but we can do all of these really well.

119
00:06:37,000 --> 00:06:41,000
You know, we specialize in what we pick, what we put out and specialize,

120
00:06:41,000 --> 00:06:43,000
but we can do them all.

121
00:06:43,000 --> 00:06:47,000
So it takes a village to, you know, raise a child,

122
00:06:47,000 --> 00:06:49,000
and it's going to take a village to create AGI.

123
00:06:49,000 --> 00:06:51,000
So we're going to need computer science.

124
00:06:51,000 --> 00:06:54,000
That's the one that everybody's got right now,

125
00:06:54,000 --> 00:06:57,000
but also going to need physics, we're going to need neuroscience,

126
00:06:57,000 --> 00:06:59,000
and we're going to need psychology.

127
00:06:59,000 --> 00:07:01,000
We're going to need people from all walks of life,

128
00:07:01,000 --> 00:07:04,000
because that word general means general, right?

129
00:07:04,000 --> 00:07:06,000
It's everything.

130
00:07:06,000 --> 00:07:12,000
And so let's move on now to the hardware.

131
00:07:12,000 --> 00:07:14,000
You know, how far we've got with the hardware.

132
00:07:14,000 --> 00:07:19,000
Then we'll look at the algorithms and we'll put it all together at the end.

133
00:07:19,000 --> 00:07:23,000
So we basically look at, you know, we have an existence proof.

134
00:07:23,000 --> 00:07:24,000
That's us.

135
00:07:24,000 --> 00:07:27,000
You know, that's a three pound of wet flesh in ourselves.

136
00:07:27,000 --> 00:07:30,000
That's what's creating this general intelligence,

137
00:07:30,000 --> 00:07:32,000
unless you believe in something else.

138
00:07:32,000 --> 00:07:35,000
But, you know, neuroscience, physics tells us that it's all we have.

139
00:07:35,000 --> 00:07:39,000
So we need to get, we are getting inspiration.

140
00:07:39,000 --> 00:07:42,000
Maybe not enough, actually, but this is where it's the first place.

141
00:07:42,000 --> 00:07:46,000
You know, I would probably start looking at how nature builds intelligence,

142
00:07:46,000 --> 00:07:48,000
or how nature hits built intelligence,

143
00:07:48,000 --> 00:07:50,000
even though it's had a little bit of a hit start on us,

144
00:07:50,000 --> 00:07:52,000
like four billion years.

145
00:07:52,000 --> 00:07:54,000
You could argue the big bang, 13 billion years.

146
00:07:54,000 --> 00:07:57,000
We've only been doing it for about 50.

147
00:07:57,000 --> 00:08:01,000
But it's come up with this thing up here in our skulls,

148
00:08:01,000 --> 00:08:03,000
that can come up with a general theory of relativity,

149
00:08:03,000 --> 00:08:06,000
play chess, play go, drive a car, go shopping, raise children,

150
00:08:06,000 --> 00:08:09,000
create mathematics, do this,

151
00:08:09,000 --> 00:08:13,000
pay, you know, everything that civilization has done,

152
00:08:13,000 --> 00:08:15,000
you know, there's a little three pound thing up here.

153
00:08:15,000 --> 00:08:19,000
Given there's a lot of us, and, you know, we think we're on our own,

154
00:08:19,000 --> 00:08:22,000
we don't collectively, we know we're near there, are we?

155
00:08:22,000 --> 00:08:24,000
We're honest with ourselves.

156
00:08:24,000 --> 00:08:26,000
So how does nature do it?

157
00:08:26,000 --> 00:08:30,000
Well, you know, you start off with a single cell organism.

158
00:08:30,000 --> 00:08:34,000
This is where the definition of intelligence gets a little bit tricky.

159
00:08:34,000 --> 00:08:36,000
It's like, is that thing intelligent?

160
00:08:36,000 --> 00:08:38,000
It's alive, it's living, it responds to its environment.

161
00:08:38,000 --> 00:08:42,000
I would argue, yes, it stays alive, manages to stay alive

162
00:08:42,000 --> 00:08:44,000
over billions of years even.

163
00:08:44,000 --> 00:08:48,000
Maybe it's more intelligent than us in that respect, the way you see.

164
00:08:48,000 --> 00:08:50,000
And it reproduces.

165
00:08:50,000 --> 00:08:53,000
It doesn't come out, it can't come up with a general theory of relativity.

166
00:08:53,000 --> 00:08:55,000
So in some sense, it has limited intelligence,

167
00:08:55,000 --> 00:08:56,000
but it's still intelligent.

168
00:08:56,000 --> 00:08:58,000
So, you know, what's that doing?

169
00:08:58,000 --> 00:08:59,000
Can we reproduce that?

170
00:08:59,000 --> 00:09:00,000
Let's start there.

171
00:09:00,000 --> 00:09:02,000
That's the C. elegans worm.

172
00:09:02,000 --> 00:09:04,000
It has 100, 302 neurons.

173
00:09:04,000 --> 00:09:06,000
It has a central nervous system.

174
00:09:06,000 --> 00:09:09,000
It's an organism with the simplest nervous system,

175
00:09:09,000 --> 00:09:12,000
CNS, central nervous system, 302 neurons.

176
00:09:12,000 --> 00:09:14,000
We can build that today.

177
00:09:14,000 --> 00:09:15,000
We've got models of that.

178
00:09:15,000 --> 00:09:17,000
So that's sort of where we are.

179
00:09:17,000 --> 00:09:20,000
You know, we have 80 billion, 100 billion neurons.

180
00:09:20,000 --> 00:09:21,000
We're at 300.

181
00:09:21,000 --> 00:09:23,000
So we've got a little road to travel,

182
00:09:23,000 --> 00:09:26,000
but if we can kind of understand how that thing works,

183
00:09:26,000 --> 00:09:29,000
you know, maybe we can extrapolate and scale a little bit.

184
00:09:29,000 --> 00:09:31,000
We can't give ourselves too hard of time.

185
00:09:31,000 --> 00:09:35,000
In the last 10 years, we can model a C. elegans worm.

186
00:09:35,000 --> 00:09:36,000
We have a bumblebee.

187
00:09:36,000 --> 00:09:38,000
That's about a million neurons.

188
00:09:38,000 --> 00:09:41,000
And they can navigate and do little dances

189
00:09:41,000 --> 00:09:43,000
and tell the rest of the bee colony where the hunt,

190
00:09:43,000 --> 00:09:46,000
where the polliners and come back and mate and stay alive.

191
00:09:46,000 --> 00:09:47,000
They're super intelligent.

192
00:09:47,000 --> 00:09:49,000
I would argue they're super intelligent, right?

193
00:09:49,000 --> 00:09:52,000
They cannot come up with the general theory of relativity,

194
00:09:52,000 --> 00:09:54,000
but they're still super smart.

195
00:09:54,000 --> 00:09:56,000
We haven't got that far.

196
00:09:56,000 --> 00:09:57,000
And then there's these things,

197
00:09:57,000 --> 00:09:59,000
the wick mushy bit in our head,

198
00:09:59,000 --> 00:10:01,000
three pounds in our skull.

199
00:10:01,000 --> 00:10:04,000
And we know any of that, even though, you know, it's small, right?

200
00:10:04,000 --> 00:10:06,000
I mean, we're trying to understand the universe,

201
00:10:06,000 --> 00:10:07,000
the cosmos, the galaxy.

202
00:10:07,000 --> 00:10:09,000
We're kind of, you know, doing a better job.

203
00:10:09,000 --> 00:10:11,000
We can't even understand the tiny little thing.

204
00:10:11,000 --> 00:10:13,000
I mean, just, it's not just.

205
00:10:13,000 --> 00:10:14,000
It's very, very clever.

206
00:10:14,000 --> 00:10:15,000
It's had billions of years to evolve.

207
00:10:15,000 --> 00:10:17,000
So, you know, what are we missing?

208
00:10:17,000 --> 00:10:18,000
What's the math?

209
00:10:18,000 --> 00:10:20,000
What's the mathematical model to build something

210
00:10:20,000 --> 00:10:22,000
that can model or come up with math

211
00:10:22,000 --> 00:10:27,000
and model its environment into personal introspection?

212
00:10:27,000 --> 00:10:29,000
How does that happen, right?

213
00:10:29,000 --> 00:10:30,000
That's what we've got to figure out.

214
00:10:30,000 --> 00:10:31,000
It's not it.

215
00:10:31,000 --> 00:10:32,000
It's no small problem.

216
00:10:32,000 --> 00:10:36,000
So, the first thing we notice from biology is it's hierarchical.

217
00:10:36,000 --> 00:10:39,000
It starts off with molecules and neurons cells

218
00:10:39,000 --> 00:10:41,000
and then networks of cells

219
00:10:41,000 --> 00:10:43,000
and then sort of subsections of the brain

220
00:10:43,000 --> 00:10:46,000
or integrated connected together wonderfully

221
00:10:46,000 --> 00:10:48,000
to create a whole brain.

222
00:10:48,000 --> 00:10:50,000
That's a connectome.

223
00:10:50,000 --> 00:10:53,000
And, you know, there's a part doing math.

224
00:10:53,000 --> 00:10:54,000
There's a part doing language.

225
00:10:54,000 --> 00:10:55,000
There's a part doing emotion.

226
00:10:55,000 --> 00:10:56,000
There's a part.

227
00:10:56,000 --> 00:10:57,000
Is there a part that creates music?

228
00:10:57,000 --> 00:10:59,000
Is there another part that drives a car?

229
00:10:59,000 --> 00:11:01,000
Another part that, you know,

230
00:11:01,000 --> 00:11:03,000
or is it all sort of integrated?

231
00:11:03,000 --> 00:11:05,000
You know, so that's the kind of inspiration

232
00:11:05,000 --> 00:11:06,000
we need to take from biology

233
00:11:06,000 --> 00:11:09,000
and kind of use that to build mathematical models

234
00:11:09,000 --> 00:11:12,000
and then actually build it in physical hardware.

235
00:11:12,000 --> 00:11:13,000
Because Simon said,

236
00:11:13,000 --> 00:11:14,000
we don't understand anything in this.

237
00:11:14,000 --> 00:11:15,000
We can build it.

238
00:11:15,000 --> 00:11:16,000
And I quite agree, right?

239
00:11:16,000 --> 00:11:18,000
We can write down any mathematical stuff we like.

240
00:11:18,000 --> 00:11:20,000
There's lots of models,

241
00:11:20,000 --> 00:11:22,000
the public kits, all sorts of different models.

242
00:11:22,000 --> 00:11:23,000
But until we build it,

243
00:11:23,000 --> 00:11:25,000
we don't really understand.

244
00:11:25,000 --> 00:11:26,000
We don't know if it tries.

245
00:11:26,000 --> 00:11:28,000
It's just a few.

246
00:11:28,000 --> 00:11:29,000
Okay.

247
00:11:29,000 --> 00:11:31,000
And then, yeah,

248
00:11:31,000 --> 00:11:33,000
certainly we have societies

249
00:11:33,000 --> 00:11:35,000
which are kind of hierarchical.

250
00:11:35,000 --> 00:11:38,000
We organize ourselves hierarchical in cities

251
00:11:38,000 --> 00:11:40,000
and countries and nations, et cetera.

252
00:11:40,000 --> 00:11:42,000
Right?

253
00:11:42,000 --> 00:11:43,000
Okay.

254
00:11:43,000 --> 00:11:45,000
So this is biology.

255
00:11:45,000 --> 00:11:47,000
Super complicated, super complex, right?

256
00:11:47,000 --> 00:11:50,000
You know, do we have to go down to that level of complexity?

257
00:11:50,000 --> 00:11:51,000
Well, that's a good question.

258
00:11:51,000 --> 00:11:53,000
Or can we just start at the cell in Europe?

259
00:11:53,000 --> 00:11:56,000
That's a bit of an open question right now.

260
00:11:56,000 --> 00:11:58,000
It's good to have open questions

261
00:11:58,000 --> 00:11:59,000
about science.

262
00:11:59,000 --> 00:12:01,000
That's what science is about, okay?

263
00:12:01,000 --> 00:12:03,000
But then it sort of organized itself

264
00:12:03,000 --> 00:12:06,000
into about two million of these cortical columns.

265
00:12:06,000 --> 00:12:09,000
So the brain has some sort of sort of substructure there.

266
00:12:09,000 --> 00:12:11,000
And it's quite common over the whole brain

267
00:12:11,000 --> 00:12:13,000
in the neocortex.

268
00:12:13,000 --> 00:12:16,000
Yeah, the math part, the seeing, the hearing.

269
00:12:16,000 --> 00:12:18,000
So one part of our brain,

270
00:12:18,000 --> 00:12:21,000
if we go blind or whatever through accident,

271
00:12:21,000 --> 00:12:25,000
those cortical columns are used to analyze the sound.

272
00:12:25,000 --> 00:12:27,000
So there's something very fundamental

273
00:12:27,000 --> 00:12:29,000
going on there.

274
00:12:29,000 --> 00:12:31,000
These are all clues.

275
00:12:31,000 --> 00:12:34,000
From biology, there's a connectome,

276
00:12:34,000 --> 00:12:36,000
and there's a central nervous system

277
00:12:36,000 --> 00:12:38,000
that helps all connect together.

278
00:12:38,000 --> 00:12:40,000
Maybe we don't need to build bodies.

279
00:12:40,000 --> 00:12:42,000
You know, if you want robots, we do.

280
00:12:42,000 --> 00:12:44,000
But if we just want something that thinks really well,

281
00:12:44,000 --> 00:12:46,000
create series like Stephen Hawking.

282
00:12:46,000 --> 00:12:48,000
I mean, you can parallelize it to get around.

283
00:12:48,000 --> 00:12:51,000
But I'm just saying that we only need the rest of that

284
00:12:51,000 --> 00:12:53,000
if we want to build a robot, okay?

285
00:12:53,000 --> 00:12:55,000
We're really focused on the neocortex

286
00:12:56,000 --> 00:12:58,000
a bit in the skull.

287
00:12:58,000 --> 00:13:01,000
Okay, and then there's the societal limits.

288
00:13:01,000 --> 00:13:05,000
So what have we built with hardware?

289
00:13:05,000 --> 00:13:07,000
Okay, well, we've come a long way.

290
00:13:07,000 --> 00:13:10,000
We're up to about 10 billion transistors,

291
00:13:10,000 --> 00:13:11,000
and there's different types,

292
00:13:11,000 --> 00:13:14,000
CPU, GPU, the Graphcore, IQ.

293
00:13:14,000 --> 00:13:16,000
And there's different types of,

294
00:13:16,000 --> 00:13:18,000
due to different data sets, basically.

295
00:13:18,000 --> 00:13:22,000
So if you want to understand environmental data,

296
00:13:22,000 --> 00:13:25,000
that's where we probably need something beyond the GPU.

297
00:13:25,000 --> 00:13:28,000
The GPU is really good at matrix modification,

298
00:13:28,000 --> 00:13:30,000
and the brain doesn't do matrix modification

299
00:13:30,000 --> 00:13:32,000
to understand and create, right?

300
00:13:32,000 --> 00:13:35,000
But deep learning does.

301
00:13:35,000 --> 00:13:38,000
So we do need GPUs to that and CPUs.

302
00:13:38,000 --> 00:13:40,000
But if we really need to go beyond

303
00:13:40,000 --> 00:13:42,000
in a different sort of hardware architecture,

304
00:13:42,000 --> 00:13:45,000
if we do want to have general intelligence.

305
00:13:45,000 --> 00:13:48,000
Okay, so there's a little nostalgia here.

306
00:13:48,000 --> 00:13:51,000
That's a cray, over 40 years ago,

307
00:13:51,000 --> 00:13:54,000
160 megaflops, how quaint, you know,

308
00:13:54,000 --> 00:13:57,000
we have teraflops in our pocket now.

309
00:13:57,000 --> 00:14:00,000
So this is where we are today.

310
00:14:00,000 --> 00:14:02,000
We have, you know, with Moore's Law,

311
00:14:02,000 --> 00:14:05,000
we've had a factor of a million or so since then.

312
00:14:05,000 --> 00:14:07,000
So we've got the i9 Intel CPU,

313
00:14:07,000 --> 00:14:11,000
we've got the NVIDIA V100 Volta GPU,

314
00:14:11,000 --> 00:14:14,000
we've got the Google TPU version 3,

315
00:14:14,000 --> 00:14:16,000
which are absolutely pedaflops,

316
00:14:16,000 --> 00:14:18,000
but just beyond the TPU,

317
00:14:18,000 --> 00:14:22,000
coming online is the Graphcore IQ.

318
00:14:22,000 --> 00:14:25,000
But really, these are all matrix multipliers.

319
00:14:25,000 --> 00:14:27,000
So super impressive, you know,

320
00:14:27,000 --> 00:14:30,000
we've got 100 pedaflops in a thing that, you know,

321
00:14:30,000 --> 00:14:32,000
we've put on this stage,

322
00:14:32,000 --> 00:14:34,000
which is somewhat what the brain does.

323
00:14:34,000 --> 00:14:36,000
I think the brain does about a pedaflop or so.

324
00:14:36,000 --> 00:14:39,000
So it's not about raw computation, right?

325
00:14:39,000 --> 00:14:41,000
If it was, we would be there.

326
00:14:41,000 --> 00:14:43,000
This thing would be coming up with theories

327
00:14:43,000 --> 00:14:45,000
of relativity and all sorts of stuff.

328
00:14:45,000 --> 00:14:47,000
We're trying to find physics and creating symphonies,

329
00:14:47,000 --> 00:14:48,000
but it can't.

330
00:14:48,000 --> 00:14:50,000
It's done as a brick, right?

331
00:14:50,000 --> 00:14:54,000
Instead of that, that's exaflops, right?

332
00:14:54,000 --> 00:14:56,000
That's a thousand times a pedaflop.

333
00:14:56,000 --> 00:14:58,000
So we're going to need something more.

334
00:14:58,000 --> 00:15:01,000
What are we missing in the hardware, right?

335
00:15:01,000 --> 00:15:03,000
So it's not just about hardware.

336
00:15:03,000 --> 00:15:04,000
It's actually about the hardware,

337
00:15:04,000 --> 00:15:06,000
which is pretty obvious when you think about it,

338
00:15:06,000 --> 00:15:09,000
because what's up here is a little different

339
00:15:09,000 --> 00:15:12,000
than the hardware we've just seen.

340
00:15:12,000 --> 00:15:14,000
What about neuromorphic computing?

341
00:15:14,000 --> 00:15:16,000
So this is where things get interesting,

342
00:15:16,000 --> 00:15:21,000
because now we're using analog circuits,

343
00:15:21,000 --> 00:15:22,000
which is what the brain is.

344
00:15:22,000 --> 00:15:24,000
The brain uses spiking neural networks,

345
00:15:24,000 --> 00:15:28,000
which is almost a combination of analog and digital.

346
00:15:28,000 --> 00:15:30,000
So there's a guy called Steve Berber,

347
00:15:30,000 --> 00:15:32,000
one of the co-founders of ARM.

348
00:15:32,000 --> 00:15:35,000
After 20 years, he left ARM

349
00:15:35,000 --> 00:15:37,000
in the University of Manchester 20 years ago.

350
00:15:37,000 --> 00:15:39,000
He set up a project called Spinnacle,

351
00:15:39,000 --> 00:15:41,000
which is now being folded into the human brain project.

352
00:15:41,000 --> 00:15:43,000
There's IBM True North.

353
00:15:43,000 --> 00:15:45,000
There's brain scales at the human brain project.

354
00:15:45,000 --> 00:15:47,000
There's various projects throughout the world

355
00:15:47,000 --> 00:15:50,000
building this analog hardware.

356
00:15:50,000 --> 00:15:55,000
So to me, I think this is a big part of it.

357
00:15:55,000 --> 00:15:57,000
Albums are certainly important,

358
00:15:57,000 --> 00:16:01,000
but I think the hardware is important as well.

359
00:16:01,000 --> 00:16:05,000
So Spinnacle, we've got a billion neurons.

360
00:16:05,000 --> 00:16:08,000
So a hundred times that, we've got the human brain.

361
00:16:08,000 --> 00:16:10,000
So we've built a mouse brain already,

362
00:16:10,000 --> 00:16:12,000
and it can do stuff like navigate through mazes

363
00:16:12,000 --> 00:16:14,000
and play Sudoku and do crossword puzzles.

364
00:16:14,000 --> 00:16:16,000
It does intelligent stuff,

365
00:16:16,000 --> 00:16:18,000
and you know, some people have heard of that,

366
00:16:18,000 --> 00:16:20,000
but it does not get depressed, does it?

367
00:16:20,000 --> 00:16:21,000
Deep mind does.

368
00:16:21,000 --> 00:16:24,000
Now, for go, everything else, it doesn't.

369
00:16:24,000 --> 00:16:26,000
I argue that this is part of my thesis

370
00:16:26,000 --> 00:16:30,000
that this is what will take us to general intelligence.

371
00:16:30,000 --> 00:16:31,000
And there it is.

372
00:16:31,000 --> 00:16:33,000
Five racks are up to 10 now.

373
00:16:33,000 --> 00:16:37,000
There it is, sitting in a data center in Manchester,

374
00:16:37,000 --> 00:16:38,000
part of the human brain project,

375
00:16:38,000 --> 00:16:41,000
and that is neuromorphic computing.

376
00:16:41,000 --> 00:16:42,000
There's a billion neurons here.

377
00:16:42,000 --> 00:16:43,000
There's a mouse brain right there.

378
00:16:43,000 --> 00:16:45,000
It's a little bigger than a mouse brain,

379
00:16:45,000 --> 00:16:46,000
but we've seen with Moore's Law,

380
00:16:46,000 --> 00:16:48,000
we're quite good at scaling, right?

381
00:16:48,000 --> 00:16:51,000
So the point is, we've built some stuff here

382
00:16:51,000 --> 00:16:52,000
which might be useful for us.

383
00:16:52,000 --> 00:16:54,000
And that's how you scale up.

384
00:16:54,000 --> 00:16:56,000
You put 1,000 neurons on a core.

385
00:16:56,000 --> 00:16:58,000
You put 18 cores on a chip.

386
00:16:58,000 --> 00:16:59,000
You send it away.

387
00:16:59,000 --> 00:17:00,000
You get it built.

388
00:17:00,000 --> 00:17:01,000
It comes back.

389
00:17:01,000 --> 00:17:02,000
You put them together.

390
00:17:02,000 --> 00:17:03,000
You put the chips on the board,

391
00:17:03,000 --> 00:17:04,000
put boards in a rack,

392
00:17:04,000 --> 00:17:06,000
and then you get 10 cabinets,

393
00:17:06,000 --> 00:17:08,000
and you put quite racks in each cabinet.

394
00:17:08,000 --> 00:17:11,000
You've got 50 servers, neuromorphic servers,

395
00:17:11,000 --> 00:17:13,000
and you have a mouse brain.

396
00:17:13,000 --> 00:17:16,000
That's how you do it, and it's been done.

397
00:17:16,000 --> 00:17:18,000
That is the brain scale approach that I mentioned

398
00:17:18,000 --> 00:17:19,000
at the human brain project.

399
00:17:19,000 --> 00:17:22,000
That's about the same size as the spinnaker

400
00:17:22,000 --> 00:17:23,000
and the same power,

401
00:17:23,000 --> 00:17:24,000
got a mouse brain,

402
00:17:24,000 --> 00:17:26,000
uses a slightly different architecture.

403
00:17:26,000 --> 00:17:27,000
I won't go into the details.

404
00:17:27,000 --> 00:17:29,000
True North IBM uses a slightly different architecture,

405
00:17:29,000 --> 00:17:31,000
but the point is, you're all neuromorphic,

406
00:17:31,000 --> 00:17:35,000
and you're all analog based on spiking your networks.

407
00:17:35,000 --> 00:17:39,000
And that's sort of general intelligent mouse level

408
00:17:39,000 --> 00:17:41,000
compared to the Google TPUs,

409
00:17:41,000 --> 00:17:44,000
which are super impressive at different types of things.

410
00:17:44,000 --> 00:17:46,000
I'd argue they won't get us a general intelligence

411
00:17:46,000 --> 00:17:49,000
with the thing on the left side.

412
00:17:49,000 --> 00:17:51,000
What about quantum?

413
00:17:51,000 --> 00:17:52,000
I don't think the brain is quantum.

414
00:17:52,000 --> 00:17:54,000
It's too warm and wet.

415
00:17:54,000 --> 00:17:56,000
Quantum needs super cold temperatures.

416
00:17:56,000 --> 00:17:58,000
It's decoherence very quickly.

417
00:17:58,000 --> 00:18:00,000
I don't think there's qubits in the brain,

418
00:18:00,000 --> 00:18:01,000
but they look nice.

419
00:18:01,000 --> 00:18:03,000
I put a picture of one on there.

420
00:18:03,000 --> 00:18:04,000
That's what a quantum circuit looks like.

421
00:18:05,000 --> 00:18:08,000
Suddenly, we're not just about the CPUs anymore.

422
00:18:08,000 --> 00:18:11,000
We've got these four different types of architectures.

423
00:18:11,000 --> 00:18:13,000
One's biological.

424
00:18:13,000 --> 00:18:14,000
One's quantum.

425
00:18:14,000 --> 00:18:16,000
Or three are not biological.

426
00:18:16,000 --> 00:18:17,000
We've got quantum.

427
00:18:17,000 --> 00:18:18,000
We've got neuromorphic.

428
00:18:18,000 --> 00:18:20,000
And we've got the digital.

429
00:18:20,000 --> 00:18:22,000
And I'm arguing that the thing on the top right

430
00:18:22,000 --> 00:18:24,000
will be needed.

431
00:18:24,000 --> 00:18:26,000
The general intelligence is what they look like.

432
00:18:26,000 --> 00:18:28,000
We've built them all.

433
00:18:28,000 --> 00:18:31,000
The quantum bits are 7 bit IBM qubits.

434
00:18:32,000 --> 00:18:34,000
They're 7 bit IBM processor.

435
00:18:34,000 --> 00:18:36,000
Quantum processor.

436
00:18:36,000 --> 00:18:37,000
There's a neuromorphic.

437
00:18:37,000 --> 00:18:39,000
That's what they look like microscopically.

438
00:18:39,000 --> 00:18:40,000
There's a digital.

439
00:18:40,000 --> 00:18:42,000
I mentioned 10 billion on a chip.

440
00:18:42,000 --> 00:18:44,000
And a processor with a CPU-PPU.

441
00:18:44,000 --> 00:18:47,000
And that's how biology looks like.

442
00:18:47,000 --> 00:18:52,000
So, you know, I think the neuromorphic in the biology,

443
00:18:52,000 --> 00:18:55,000
the neuromorphic will map onto the biological

444
00:18:55,000 --> 00:18:59,000
the best out of all of those options there.

445
00:18:59,000 --> 00:19:01,000
Not to say the others aren't clear.

446
00:19:01,000 --> 00:19:03,000
So, the data centers of the future.

447
00:19:03,000 --> 00:19:04,000
They'll look like this.

448
00:19:04,000 --> 00:19:06,000
We might just have them fill up with CPU-PPUs.

449
00:19:06,000 --> 00:19:07,000
We'll have neuromorphics.

450
00:19:07,000 --> 00:19:08,000
We'll have quantum.

451
00:19:08,000 --> 00:19:11,000
And we'll have a little bit of classical as well.

452
00:19:11,000 --> 00:19:13,000
Interesting time to head.

453
00:19:13,000 --> 00:19:15,000
Deep learning.

454
00:19:15,000 --> 00:19:18,000
Just going to have a couple of slides here.

455
00:19:18,000 --> 00:19:20,000
There's this plethora of neural networks.

456
00:19:20,000 --> 00:19:22,000
It won't get us there.

457
00:19:22,000 --> 00:19:25,000
They've got very good classification and regression.

458
00:19:25,000 --> 00:19:26,000
And pattern matching.

459
00:19:26,000 --> 00:19:27,000
Very good.

460
00:19:27,000 --> 00:19:31,000
This is all statistical analysis.

461
00:19:31,000 --> 00:19:32,000
Very clever.

462
00:19:32,000 --> 00:19:35,000
But nothing to do with what the brain is doing at all.

463
00:19:35,000 --> 00:19:37,000
And so, this is a TensorFlow meetup.

464
00:19:37,000 --> 00:19:39,000
So, I had to mention TensorFlow.

465
00:19:39,000 --> 00:19:40,000
Here's how popular TensorFlow is.

466
00:19:40,000 --> 00:19:43,000
So, if you're going to start on the deep learning,

467
00:19:43,000 --> 00:19:47,000
you know, you need a quick pie talk to TensorFlow.

468
00:19:47,000 --> 00:19:48,000
Okay.

469
00:19:48,000 --> 00:19:49,000
So, that's that.

470
00:19:49,000 --> 00:19:50,000
There we go.

471
00:19:50,000 --> 00:19:53,000
So, what do we need?

472
00:19:53,000 --> 00:19:55,000
How do we build AGI?

473
00:19:56,000 --> 00:19:58,000
So, well, I would start as a physicist.

474
00:19:58,000 --> 00:20:00,000
I would start with the laws of physics.

475
00:20:00,000 --> 00:20:03,000
And we've come up with physics as an old subject,

476
00:20:03,000 --> 00:20:05,000
about 400 years old.

477
00:20:05,000 --> 00:20:08,000
Computer science is quite new, about 70 years old.

478
00:20:08,000 --> 00:20:10,000
So, I will just go back and start with the laws of physics.

479
00:20:10,000 --> 00:20:12,000
You know, what are the laws of physics?

480
00:20:12,000 --> 00:20:15,000
Because the brain, I would argue, is a physical system.

481
00:20:15,000 --> 00:20:18,000
So, probably that's the best place to start.

482
00:20:18,000 --> 00:20:20,000
Don't just start writing a JavaScript.

483
00:20:20,000 --> 00:20:23,000
No, start, you know, go back to basics.

484
00:20:23,000 --> 00:20:25,000
This is what physics is.

485
00:20:25,000 --> 00:20:27,000
And basically all the physics,

486
00:20:27,000 --> 00:20:29,000
and maybe not too many people in the room know this,

487
00:20:29,000 --> 00:20:31,000
can be written as a principle of least action.

488
00:20:31,000 --> 00:20:34,000
S is the action, L is the function, blah, blah, blah.

489
00:20:34,000 --> 00:20:36,000
You know, it's a great level of physics.

490
00:20:36,000 --> 00:20:38,000
So, I know this stuff really well.

491
00:20:38,000 --> 00:20:43,000
Build into me hundreds of exams and assignments, blah, blah, blah.

492
00:20:43,000 --> 00:20:45,000
And there's been a book just recently called

493
00:20:45,000 --> 00:20:47,000
Principles of Least Action, which surprised me.

494
00:20:47,000 --> 00:20:50,000
So, it sort of has checked us on each section of physics

495
00:20:50,000 --> 00:20:52,000
and how you can derive it from delta S equals zero.

496
00:20:52,000 --> 00:20:55,000
So, I would start there if I was going to build

497
00:20:55,000 --> 00:20:57,000
an intelligent system.

498
00:20:57,000 --> 00:20:59,000
So, that's physics.

499
00:20:59,000 --> 00:21:01,000
What else do we need?

500
00:21:01,000 --> 00:21:04,000
So, intelligence isn't just about pattern action.

501
00:21:04,000 --> 00:21:06,000
I've hope, you know, I've conveyed that already

502
00:21:06,000 --> 00:21:08,000
in nine different types.

503
00:21:08,000 --> 00:21:10,000
And one or two types is about pattern action, really.

504
00:21:10,000 --> 00:21:12,000
It's about modeling the world, really.

505
00:21:12,000 --> 00:21:13,000
That's what intelligence is.

506
00:21:13,000 --> 00:21:15,000
It's how well we can model the world

507
00:21:15,000 --> 00:21:17,000
and then make predictions and predict the next step.

508
00:21:17,000 --> 00:21:18,000
That's how we make money.

509
00:21:18,000 --> 00:21:19,000
That's how we stay alive.

510
00:21:19,000 --> 00:21:21,000
You know, that's how we do everything.

511
00:21:21,000 --> 00:21:24,000
That's what intelligence is, like, definition, really.

512
00:21:24,000 --> 00:21:26,000
We can play chess and do all sorts of things,

513
00:21:26,000 --> 00:21:28,000
but really chess is about understanding our environment,

514
00:21:28,000 --> 00:21:30,000
modeling our environment,

515
00:21:30,000 --> 00:21:32,000
the ability to model our environment,

516
00:21:32,000 --> 00:21:34,000
then to make predictions and then to take action

517
00:21:34,000 --> 00:21:36,000
from those predictions.

518
00:21:36,000 --> 00:21:39,000
We need to understand, explain and understand what we see.

519
00:21:39,000 --> 00:21:41,000
This is what we need to build.

520
00:21:41,000 --> 00:21:42,000
We need to imagine things,

521
00:21:42,000 --> 00:21:44,000
and we need to problem-solve and plan actions.

522
00:21:44,000 --> 00:21:47,000
We need to build new models as we learn about the world.

523
00:21:47,000 --> 00:21:49,000
Nothing to do with AlphaGo.

524
00:21:49,000 --> 00:21:53,000
Nothing to do with AlphaStuff at all.

525
00:21:53,000 --> 00:21:54,000
Okay.

526
00:21:54,000 --> 00:21:56,000
I would argue they're super clever,

527
00:21:56,000 --> 00:21:57,000
but they're statistical.

528
00:21:57,000 --> 00:21:59,000
They're not really intelligent.

529
00:21:59,000 --> 00:22:00,000
Okay.

530
00:22:00,000 --> 00:22:01,000
So, what is intelligence?

531
00:22:01,000 --> 00:22:03,000
Well, there's loads of theories.

532
00:22:03,000 --> 00:22:04,000
I'm going to pick out actually,

533
00:22:04,000 --> 00:22:05,000
but there's a ton of work,

534
00:22:05,000 --> 00:22:07,000
and all of these people have been doing this work

535
00:22:07,000 --> 00:22:08,000
for 30 years now.

536
00:22:08,000 --> 00:22:09,000
There's Schmidt-Rieber,

537
00:22:09,000 --> 00:22:10,000
Rooker,

538
00:22:10,000 --> 00:22:11,000
Bialectic,

539
00:22:11,000 --> 00:22:12,000
Princeton, Tishvie,

540
00:22:12,000 --> 00:22:15,000
and Friston at UCL here.

541
00:22:15,000 --> 00:22:17,000
Yeah, this is their life's work.

542
00:22:17,000 --> 00:22:19,000
They're mad scientists twining away,

543
00:22:19,000 --> 00:22:21,000
but sort of it's starting to happen for them.

544
00:22:21,000 --> 00:22:23,000
So, they'll probably get to see a little bit of,

545
00:22:23,000 --> 00:22:25,000
you know, the fruits of their labor.

546
00:22:25,000 --> 00:22:26,000
Okay.

547
00:22:26,000 --> 00:22:27,000
So, what is that,

548
00:22:27,000 --> 00:22:28,000
the built-proofs that, when I picked up,

549
00:22:28,000 --> 00:22:32,000
you're more than welcome to go home and Google all of these.

550
00:22:32,000 --> 00:22:34,000
They're all got papers on archives.

551
00:22:34,000 --> 00:22:36,000
They've all got wonderful websites,

552
00:22:36,000 --> 00:22:37,000
loads and loads of beautiful,

553
00:22:37,000 --> 00:22:38,000
really reading,

554
00:22:38,000 --> 00:22:40,000
and interested in intelligence,

555
00:22:40,000 --> 00:22:41,000
essentially,

556
00:22:41,000 --> 00:22:43,000
and how we might get there.

557
00:22:43,000 --> 00:22:46,000
And they all have useful input,

558
00:22:46,000 --> 00:22:48,000
and we don't quite know which one is right,

559
00:22:48,000 --> 00:22:51,000
maybe the combination is a few.

560
00:22:51,000 --> 00:22:52,000
But this is what Friston's saying.

561
00:22:52,000 --> 00:22:54,000
It's called active inference.

562
00:22:54,000 --> 00:22:56,000
It's based on physics and information theory.

563
00:22:56,000 --> 00:22:58,000
So, it's a very fundamental theory.

564
00:22:58,000 --> 00:23:01,000
It uses something called the free energy principle,

565
00:23:01,000 --> 00:23:07,000
and it does encompass all space and all time scales.

566
00:23:07,000 --> 00:23:10,000
So, yeah, it works from the very small to the very large,

567
00:23:10,000 --> 00:23:14,000
and it works from nanoseconds to the age of the universe.

568
00:23:14,000 --> 00:23:15,000
It's completely general.

569
00:23:15,000 --> 00:23:16,000
Yeah.

570
00:23:16,000 --> 00:23:19,000
Now, I don't expect you, there is Sarah,

571
00:23:19,000 --> 00:23:21,000
I don't expect you, you know,

572
00:23:21,000 --> 00:23:22,000
to go, ah, I get it now,

573
00:23:22,000 --> 00:23:24,000
because this is hard stuff,

574
00:23:24,000 --> 00:23:25,000
and it takes a lot of reading.

575
00:23:25,000 --> 00:23:27,000
It certainly took me an awful lot of time, actually,

576
00:23:27,000 --> 00:23:29,000
to read through all of these different theories,

577
00:23:29,000 --> 00:23:32,000
and just start at the basics of computer science.

578
00:23:32,000 --> 00:23:33,000
You know, I said it took a village,

579
00:23:33,000 --> 00:23:36,000
neuroscience, physics, psychology.

580
00:23:36,000 --> 00:23:39,000
You know, I just learned a lot about a lot of things, right?

581
00:23:39,000 --> 00:23:40,000
And we do.

582
00:23:40,000 --> 00:23:41,000
This is why it's a hard problem,

583
00:23:41,000 --> 00:23:42,000
unless it was so quick.

584
00:23:42,000 --> 00:23:47,000
So, this is how he sort of puts it down, models it,

585
00:23:47,000 --> 00:23:49,000
I guess, so the free energy principle.

586
00:23:49,000 --> 00:23:51,000
You just divide the world into internal states,

587
00:23:51,000 --> 00:23:54,000
which is our brain and outside the environment,

588
00:23:54,000 --> 00:23:55,000
agent environment.

589
00:23:55,000 --> 00:23:57,000
Anybody who's ever done reinforcement learning,

590
00:23:57,000 --> 00:23:58,000
it's a similar sort of thing, right?

591
00:23:58,000 --> 00:24:00,000
You have the agent and the environment,

592
00:24:00,000 --> 00:24:03,000
but this kind of explains how the agent works itself.

593
00:24:03,000 --> 00:24:05,000
What's going on inside the brain,

594
00:24:05,000 --> 00:24:06,000
what's going on outside the brain,

595
00:24:06,000 --> 00:24:08,000
it's separated by something called a Markov banquet,

596
00:24:08,000 --> 00:24:11,000
which is kind of a cool-sounding name.

597
00:24:11,000 --> 00:24:13,000
The guy called Julia Pulitzer,

598
00:24:13,000 --> 00:24:15,000
University of Southern California,

599
00:24:15,000 --> 00:24:16,000
came up with 18.

600
00:24:16,000 --> 00:24:19,000
So, you know, there's this contribution from everywhere

601
00:24:19,000 --> 00:24:20,000
into this theory.

602
00:24:20,000 --> 00:24:22,000
And so it works from, like I say,

603
00:24:22,000 --> 00:24:25,000
small to large, it works from cells to brains,

604
00:24:25,000 --> 00:24:26,000
external states, internal states,

605
00:24:26,000 --> 00:24:29,000
whether that's a cell, microscopic,

606
00:24:29,000 --> 00:24:33,000
we saw the cell, I think that was intelligent to a brain.

607
00:24:33,000 --> 00:24:36,000
The difference is the brain is much more complicated

608
00:24:36,000 --> 00:24:38,000
than the cell.

609
00:24:39,000 --> 00:24:41,000
So it's got none.

610
00:24:41,000 --> 00:24:43,000
But that's a similar setup.

611
00:24:43,000 --> 00:24:45,000
And so you can kind of, you know,

612
00:24:45,000 --> 00:24:49,000
it all different scales, mice, people, bacteria.

613
00:24:49,000 --> 00:24:50,000
Okay, there's the math.

614
00:24:50,000 --> 00:24:52,000
The math gets ugly.

615
00:24:52,000 --> 00:24:53,000
That's what it looks like.

616
00:24:53,000 --> 00:24:56,000
But if you've ever read a reinforcement learning paper,

617
00:24:56,000 --> 00:24:58,000
that's what that looks like as well.

618
00:24:58,000 --> 00:25:01,000
So, you know, if you're a deep learning researcher,

619
00:25:01,000 --> 00:25:03,000
the only difference is the physical one

620
00:25:03,000 --> 00:25:07,000
that needs my entropy and things like that.

621
00:25:08,000 --> 00:25:10,000
And the reinforcement learning needs to be

622
00:25:10,000 --> 00:25:12,000
much more statistical.

623
00:25:12,000 --> 00:25:14,000
But it's kind of similar looking,

624
00:25:14,000 --> 00:25:15,000
probability distributions,

625
00:25:15,000 --> 00:25:17,000
but this one's based on

626
00:25:17,000 --> 00:25:20,000
fundamental physics and information theory.

627
00:25:20,000 --> 00:25:22,000
Okay, don't expect you to take any of that.

628
00:25:22,000 --> 00:25:24,000
And I just wanted to show off the math.

629
00:25:24,000 --> 00:25:25,000
Yeah.

630
00:25:25,000 --> 00:25:27,000
So, how to provide, look at that math,

631
00:25:27,000 --> 00:25:28,000
but I didn't do it.

632
00:25:28,000 --> 00:25:29,000
It's not mine.

633
00:25:29,000 --> 00:25:32,000
Okay, so can we build it?

634
00:25:32,000 --> 00:25:35,000
Well, yes, we can.

635
00:25:35,000 --> 00:25:38,000
We have the theories, theories, theories, theories.

636
00:25:38,000 --> 00:25:39,000
We have a few candidates.

637
00:25:39,000 --> 00:25:41,000
We have the algorithm.

638
00:25:41,000 --> 00:25:42,000
We even have software.

639
00:25:42,000 --> 00:25:45,000
All of the professor has a big software base

640
00:25:45,000 --> 00:25:47,000
that has been developed over the years.

641
00:25:47,000 --> 00:25:48,000
We have the hardware.

642
00:25:48,000 --> 00:25:49,000
I'm going to argue,

643
00:25:49,000 --> 00:25:50,000
the neuromorphic hardware is there.

644
00:25:50,000 --> 00:25:53,000
So, we run this on Spinnaker.

645
00:25:53,000 --> 00:25:54,000
We should be good.

646
00:25:54,000 --> 00:25:56,000
And we have data sets.

647
00:25:56,000 --> 00:25:57,000
It's a thing called the internet.

648
00:25:57,000 --> 00:25:58,000
We have all the data we need.

649
00:25:58,000 --> 00:26:01,000
And so, I would say,

650
00:26:01,000 --> 00:26:03,000
to some extent, we're kind of there.

651
00:26:03,000 --> 00:26:05,000
We have the mouse screen.

652
00:26:05,000 --> 00:26:06,000
And we have, you know,

653
00:26:06,000 --> 00:26:08,000
the active inference running on it.

654
00:26:08,000 --> 00:26:10,000
And I'd say we're kind of there.

655
00:26:10,000 --> 00:26:13,000
We're kind of in the very early stages.

656
00:26:13,000 --> 00:26:15,000
We're not millions of miles away,

657
00:26:15,000 --> 00:26:17,000
many years away down the road.

658
00:26:17,000 --> 00:26:18,000
We're sort of there.

659
00:26:18,000 --> 00:26:19,000
We're on the road.

660
00:26:19,000 --> 00:26:21,000
It's the neutral road.

661
00:26:21,000 --> 00:26:24,000
I don't think there's anything missing.

662
00:26:24,000 --> 00:26:26,000
There's no theory of missing.

663
00:26:26,000 --> 00:26:27,000
There's no hardware missing.

664
00:26:27,000 --> 00:26:29,000
There's no data missing.

665
00:26:29,000 --> 00:26:34,000
And there's no, we have a theory.

666
00:26:34,000 --> 00:26:36,000
That's a big statement, right?

667
00:26:36,000 --> 00:26:38,000
So, I think we're there.

668
00:26:38,000 --> 00:26:39,000
Okay.

669
00:26:39,000 --> 00:26:40,000
So, what do we need?

670
00:26:40,000 --> 00:26:41,000
We need hardware.

671
00:26:41,000 --> 00:26:42,000
We've got a hardware.

672
00:26:42,000 --> 00:26:43,000
We need a piece of clothing,

673
00:26:43,000 --> 00:26:44,000
a general intelligence.

674
00:26:44,000 --> 00:26:47,000
So, we need software engineers here in the room

675
00:26:47,000 --> 00:26:48,000
who want to get started.

676
00:26:48,000 --> 00:26:49,000
About building,

677
00:26:49,000 --> 00:26:50,000
you'll be completely missed.

678
00:26:50,000 --> 00:26:52,000
And, yeah,

679
00:26:52,000 --> 00:26:54,000
it's the color project of our time.

680
00:26:54,000 --> 00:26:55,000
I mean, you know, it's the one, right?

681
00:26:55,000 --> 00:26:57,000
We've got a sort of intelligence.

682
00:26:57,000 --> 00:26:58,000
We need sort of everything else.

683
00:26:58,000 --> 00:26:59,000
There's many.

684
00:26:59,000 --> 00:27:00,000
There's a huge,

685
00:27:00,000 --> 00:27:01,000
there's a deep,

686
00:27:01,000 --> 00:27:02,000
deep mind,

687
00:27:02,000 --> 00:27:03,000
the brain,

688
00:27:03,000 --> 00:27:04,000
the human brain project,

689
00:27:04,000 --> 00:27:05,000
China,

690
00:27:05,000 --> 00:27:07,000
just with this 10 million dollars,

691
00:27:07,000 --> 00:27:08,000
sitting up a city,

692
00:27:08,000 --> 00:27:09,000
the whole city,

693
00:27:09,000 --> 00:27:10,000
EGI.

694
00:27:10,000 --> 00:27:12,000
And should we build it?

695
00:27:12,000 --> 00:27:13,000
I mean,

696
00:27:13,000 --> 00:27:14,000
if ethics is safety,

697
00:27:14,000 --> 00:27:16,000
but I'll leave that to another discussion.

698
00:27:16,000 --> 00:27:17,000
Finishing off,

699
00:27:17,000 --> 00:27:19,000
here's some EGI projects that are real.

700
00:27:19,000 --> 00:27:20,000
They exist.

701
00:27:20,000 --> 00:27:21,000
They have venture funding.

702
00:27:21,000 --> 00:27:22,000
They have people.

703
00:27:22,000 --> 00:27:24,000
They're working on it 24 seven every day.

704
00:27:24,000 --> 00:27:25,000
That's their life.

705
00:27:25,000 --> 00:27:27,000
That's what they do.

706
00:27:28,000 --> 00:27:30,000
You know, I didn't put the mind up there.

707
00:27:30,000 --> 00:27:31,000
Some of them are well known,

708
00:27:31,000 --> 00:27:33,000
some not so well known.

709
00:27:34,000 --> 00:27:35,000
Some are smaller,

710
00:27:35,000 --> 00:27:36,000
some are bigger.

711
00:27:36,000 --> 00:27:37,000
And so,

712
00:27:37,000 --> 00:27:38,000
in conclusion,

713
00:27:38,000 --> 00:27:40,000
it's obvious to most,

714
00:27:40,000 --> 00:27:42,000
that deep learning is lacking the foundation,

715
00:27:42,000 --> 00:27:43,000
to the general theory of intelligence.

716
00:27:43,000 --> 00:27:44,000
It's based on statistics,

717
00:27:44,000 --> 00:27:46,000
not physics.

718
00:27:46,000 --> 00:27:48,000
Some groups are starting to look at

719
00:27:48,000 --> 00:27:50,000
viable models.

720
00:27:50,000 --> 00:27:53,000
And it frustrates me that they weren't starting

721
00:27:53,000 --> 00:27:55,000
looking at them all along somewhere,

722
00:27:55,000 --> 00:27:56,000
but a lot of this deep learning stuff

723
00:27:56,000 --> 00:27:57,000
is just, you know,

724
00:27:57,000 --> 00:27:59,000
it wasn't ever going to get us there.

725
00:27:59,000 --> 00:28:00,000
And people, you know,

726
00:28:00,000 --> 00:28:01,000
hopefully realize that now,

727
00:28:01,000 --> 00:28:03,000
but I'm not that optimistic.

728
00:28:04,000 --> 00:28:05,000
But I'm saying right now,

729
00:28:05,000 --> 00:28:07,000
it's not going to get us there at all.

730
00:28:07,000 --> 00:28:08,000
And is it?

731
00:28:08,000 --> 00:28:09,000
Yeah.

732
00:28:10,000 --> 00:28:11,000
I've already said,

733
00:28:11,000 --> 00:28:12,000
we've sort of,

734
00:28:12,000 --> 00:28:13,000
we've already,

735
00:28:13,000 --> 00:28:14,000
we've already got it.

736
00:28:14,000 --> 00:28:15,000
Not human level,

737
00:28:15,000 --> 00:28:16,000
but mouse level.

738
00:28:18,000 --> 00:28:19,000
And normal,

739
00:28:19,000 --> 00:28:21,000
it might be a platform to get us there.

740
00:28:21,000 --> 00:28:23,000
And I'll leave you with this,

741
00:28:24,000 --> 00:28:25,000
parting words from Jeff Hinton

742
00:28:25,000 --> 00:28:26,000
a bit.

743
00:28:31,000 --> 00:28:32,000
Okay.

744
00:28:32,000 --> 00:28:33,000
So what he was saying there

745
00:28:33,000 --> 00:28:35,000
is basically what I think

746
00:28:35,000 --> 00:28:36,000
the Godfather of AI

747
00:28:36,000 --> 00:28:38,000
of deep learning from the 80s

748
00:28:38,000 --> 00:28:39,000
and his whole life,

749
00:28:39,000 --> 00:28:40,000
used to work with Princeton at UCL,

750
00:28:40,000 --> 00:28:41,000
by the way,

751
00:28:41,000 --> 00:28:43,000
went to Toronto in the end.

752
00:28:43,000 --> 00:28:45,000
What he is saying is that deep learning

753
00:28:45,000 --> 00:28:46,000
won't get us there.

754
00:28:46,000 --> 00:28:47,000
It was never meant to.

755
00:28:47,000 --> 00:28:48,000
I didn't make a mistake.

756
00:28:48,000 --> 00:28:49,000
It's just,

757
00:28:49,000 --> 00:28:50,000
I was working on something else.

758
00:28:50,000 --> 00:28:51,000
What we do need,

759
00:28:51,000 --> 00:28:52,000
what will get us there

760
00:28:52,000 --> 00:28:54,000
is understanding the brain.

761
00:28:54,000 --> 00:28:55,000
And he said,

762
00:28:55,000 --> 00:28:56,000
this was in 2016.

763
00:28:57,000 --> 00:28:58,000
He's saying,

764
00:28:58,000 --> 00:28:59,000
I think that

765
00:29:00,000 --> 00:29:02,000
we are very close to that moment.

766
00:29:02,000 --> 00:29:03,000
And I've,

767
00:29:03,000 --> 00:29:04,000
I've,

768
00:29:04,000 --> 00:29:05,000
I've put up a few theories there,

769
00:29:05,000 --> 00:29:07,000
which maybe he wasn't even aware of.

770
00:29:07,000 --> 00:29:08,000
Maybe he was,

771
00:29:08,000 --> 00:29:09,000
he was a smart guy,

772
00:29:09,000 --> 00:29:11,000
but I believe we do understand the brain.

773
00:29:11,000 --> 00:29:12,000
I believe we're there.

774
00:29:12,000 --> 00:29:13,000
Thanks.

775
00:29:13,000 --> 00:29:14,000
Great stuff,

776
00:29:14,000 --> 00:29:15,000
round of applause.

777
00:29:15,000 --> 00:29:16,000
Okay, good stuff.

778
00:29:16,000 --> 00:29:17,000
So we're a little bit tight for time,

779
00:29:17,000 --> 00:29:18,000
but I'm curious.

780
00:29:18,000 --> 00:29:19,000
So you've done,

781
00:29:19,000 --> 00:29:20,000
you've been a research fellow.

782
00:29:20,000 --> 00:29:21,000
You've obviously been running

783
00:29:21,000 --> 00:29:22,000
deep learning partnerships

784
00:29:22,000 --> 00:29:23,000
for quite a while now.

785
00:29:23,000 --> 00:29:24,000
Yeah.

786
00:29:24,000 --> 00:29:26,000
What is your interest?

787
00:29:26,000 --> 00:29:27,000
How come so much focus

788
00:29:27,000 --> 00:29:28,000
we're interested in this?

789
00:29:28,000 --> 00:29:30,000
And where do we find you online?

790
00:29:30,000 --> 00:29:31,000
And what are your next,

791
00:29:31,000 --> 00:29:32,000
what's your next project?

792
00:29:32,000 --> 00:29:33,000
Yeah.

793
00:29:33,000 --> 00:29:34,000
So I have started a company called

794
00:29:34,000 --> 00:29:35,000
TuringAI.co.

795
00:29:35,000 --> 00:29:36,000
Which was up there, right?

796
00:29:36,000 --> 00:29:37,000
Yeah.

797
00:29:37,000 --> 00:29:38,000
Yeah.

798
00:29:38,000 --> 00:29:39,000
Maybe it would have been interesting.

799
00:29:39,000 --> 00:29:40,000
It was one of the boxes up there.

800
00:29:40,000 --> 00:29:41,000
Yeah, it was, actually.

801
00:29:41,000 --> 00:29:42,000
One of the boxes.

802
00:29:42,000 --> 00:29:43,000
Tell us about that.

803
00:29:43,000 --> 00:29:44,000
Yeah.

804
00:29:44,000 --> 00:29:45,000
Turing,

805
00:29:45,000 --> 00:29:46,000
like it is now,

806
00:29:46,000 --> 00:29:47,000
TuringAI.co.

807
00:29:47,000 --> 00:29:48,000
Now,

808
00:29:48,000 --> 00:29:50,000
I've actually started that company

809
00:29:50,000 --> 00:29:51,000
with Professor Frisson

810
00:29:51,000 --> 00:29:52,000
and a couple of very clever post-docs.

811
00:29:53,000 --> 00:29:54,000
So we're working on this.

812
00:29:54,000 --> 00:29:56,000
Schmidt Hooper's working on it.

813
00:29:56,000 --> 00:29:58,000
Weilich at Princeton's working on it.

814
00:29:58,000 --> 00:29:59,000
DeepMind are working on it.

815
00:29:59,000 --> 00:30:00,000
So I'm not saying that

816
00:30:00,000 --> 00:30:01,000
we're going to crack it first,

817
00:30:01,000 --> 00:30:02,000
but yeah,

818
00:30:02,000 --> 00:30:05,000
we're very involved in solving the problem.

819
00:30:05,000 --> 00:30:06,000
Big respect.

820
00:30:06,000 --> 00:30:07,000
Well, with you all the way,

821
00:30:07,000 --> 00:30:08,000
we'll have you back on TensorFlow

822
00:30:08,000 --> 00:30:09,000
once you solve that problem.

823
00:30:09,000 --> 00:30:10,000
Yeah.

824
00:30:10,000 --> 00:30:11,000
Okay, cool.

825
00:30:11,000 --> 00:30:12,000
Next,

826
00:30:12,000 --> 00:30:13,000
we've got time for two quick questions.

827
00:30:13,000 --> 00:30:14,000
Anyone in the crowd?

828
00:30:14,000 --> 00:30:15,000
Okay, gentlemen.

829
00:30:15,000 --> 00:30:16,000
Introduce yourself.

830
00:30:16,000 --> 00:30:17,000
Hi.

831
00:30:17,000 --> 00:30:18,000
My name is Erdogan.

832
00:30:18,000 --> 00:30:19,000
Yeah.

833
00:30:19,000 --> 00:30:20,000
I help you.

834
00:30:21,000 --> 00:30:22,000
The one you mentioned,

835
00:30:22,000 --> 00:30:24,000
which is less publicity.

836
00:30:24,000 --> 00:30:25,000
Given that 3.0

837
00:30:25,000 --> 00:30:26,000
could be deep blue

838
00:30:26,000 --> 00:30:28,000
within nine hours of learning,

839
00:30:28,000 --> 00:30:29,000
why isn't there something similar

840
00:30:29,000 --> 00:30:30,000
being done to promote it?

841
00:30:30,000 --> 00:30:31,000
Yeah.

842
00:30:31,000 --> 00:30:32,000
So, I mean,

843
00:30:32,000 --> 00:30:33,000
I don't know,

844
00:30:33,000 --> 00:30:34,000
really,

845
00:30:34,000 --> 00:30:36,000
they're using reinforcement voting

846
00:30:36,000 --> 00:30:37,000
and lack of,

847
00:30:37,000 --> 00:30:38,000
you know,

848
00:30:38,000 --> 00:30:39,000
Monte Carlo's

849
00:30:39,000 --> 00:30:40,000
tree certifications.

850
00:30:40,000 --> 00:30:41,000
Right.

851
00:30:41,000 --> 00:30:42,000
So,

852
00:30:42,000 --> 00:30:43,000
you know,

853
00:30:43,000 --> 00:30:44,000
I don't know,

854
00:30:44,000 --> 00:30:45,000
I don't know,

855
00:30:45,000 --> 00:30:46,000
I don't know,

856
00:30:46,000 --> 00:30:47,000
I don't know,

857
00:30:47,000 --> 00:30:48,000
I don't know,

858
00:30:48,000 --> 00:30:49,000
everything's been right.

859
00:30:49,000 --> 00:30:49,800
So,

860
00:30:49,800 --> 00:30:51,000
I'm not going to take anything

861
00:30:51,000 --> 00:30:52,000
away from any of that work,

862
00:30:52,000 --> 00:30:53,000
it's brilliant, right.

863
00:30:54,000 --> 00:30:55,000
Remember,

864
00:30:55,000 --> 00:30:56,460
calculator can out

865
00:30:56,460 --> 00:30:57,440
calculators that modify two

866
00:30:57,440 --> 00:30:59,000
large numbers together.

867
00:30:59,000 --> 00:31:00,000
You know

868
00:31:00,000 --> 00:31:01,000
there's no way

869
00:31:01,000 --> 00:31:02,000
we're ever going to be

870
00:31:04,000 --> 00:31:05,000
going against or go over again.

871
00:31:05,000 --> 00:31:06,000
It's super clear.

872
00:31:06,000 --> 00:31:07,000
But remember,

873
00:31:07,000 --> 00:31:08,000
my nine types of intelligent

874
00:31:08,000 --> 00:31:09,000
but can't do anything

875
00:31:09,000 --> 00:31:10,000
as it may be.

876
00:31:10,000 --> 00:31:11,000
I can't write a symphony.

877
00:31:11,000 --> 00:31:12,000
We need a differential reversal

878
00:31:12,000 --> 00:31:13,000
for that.

879
00:31:13,000 --> 00:31:14,000
So,

880
00:31:14,000 --> 00:31:15,000
you know,

881
00:31:15,000 --> 00:31:16,000
how do we integrate them all

882
00:31:16,000 --> 00:31:17,000
together?

883
00:31:17,000 --> 00:31:21,000
Yeah, that's a pretty big problem actually, yeah, that's clear.

884
00:31:21,000 --> 00:31:25,000
Any other questions?

885
00:31:25,000 --> 00:31:35,000
Hi, my name is Nuki, I'm just wondering, what's the

886
00:31:35,000 --> 00:31:41,000
number of neurons in the system?

887
00:31:41,000 --> 00:31:45,000
Yeah, nothing like it, so that's a great point, so there's two

888
00:31:45,000 --> 00:31:48,000
neurons, there's a number of neurons, but there's also the connectivity between the

889
00:31:48,000 --> 00:31:54,000
neurons, so the biology seems to like to connect about a thousand to ten

890
00:31:54,000 --> 00:31:58,000
thousand synapses per neuron, so if Spinnaker is nowhere near that, it might

891
00:31:58,000 --> 00:32:03,000
have a few dozen, so that's a very important mention.

892
00:32:03,000 --> 00:32:07,000
Connectivity is super important to you.

893
00:32:07,000 --> 00:32:11,000
Can I start talking a little more? There's one over here.

894
00:32:11,000 --> 00:32:18,000
Hi, my name is Marco, I work at the real startup called

895
00:32:18,000 --> 00:32:25,000
Erypadet. My question is, you focus a lot on the CPU architecture, but obviously the charge

896
00:32:25,000 --> 00:32:31,000
tool in the thesis says that any function that is computable can be executed by any

897
00:32:31,000 --> 00:32:37,000
machine, whether it's a biologist, whether it's a CPU, whether it's a GPU, is it just the

898
00:32:37,000 --> 00:32:42,000
execution speed that you care about, or do you see anything wrong with the charge

899
00:32:42,000 --> 00:32:43,000
tool?

900
00:32:43,000 --> 00:32:47,000
No, I do believe that's a great question, and there's been a lot of debate, even for

901
00:32:47,000 --> 00:32:52,000
the last philosophical debate, about whether the brain is Turing complete, and I

902
00:32:52,000 --> 00:32:57,000
think it is, and most people do as well, so it's a very fringe people who say there's

903
00:32:57,000 --> 00:33:01,000
something else in the brain that's not Turing complete, so I don't believe there's

904
00:33:01,000 --> 00:33:06,000
any bottleneck there at all, I just think it's a different type of computational system.

905
00:33:06,000 --> 00:33:11,000
We're going to follow you, generally connect with any type of CPU.

906
00:33:11,000 --> 00:33:18,000
Yeah, I still think we need the synaptic connections, like a thousand to ten thousand, so it really

907
00:33:18,000 --> 00:33:25,000
is a deeply embedded hardware problem, but that hardware is Turing complete, so the general

908
00:33:25,000 --> 00:33:32,000
theory of computation applies, but it's just a different hardware, that's a good question.

909
00:33:32,000 --> 00:33:34,000
Okay, thank you.

