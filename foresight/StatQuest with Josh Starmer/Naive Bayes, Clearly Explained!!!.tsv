start	end	text
0	6000	I'm at home during lockdown, working on my stat quest, yeah.
6000	12000	I'm at home during lockdown, working on my stat quest, yeah.
12000	14000	Stat quest.
14000	18000	Hello, I'm Josh Starmer, and welcome to Stat Quest.
18000	24000	Today we're going to talk about naive bays, and it's going to be clearly explained.
24000	28000	This stat quest is sponsored by JADBIO.
28000	34000	Just add data, and their automatic machine learning algorithms will do the rest of the work for you.
34000	39000	For more details, follow the link in the pinned comment below.
39000	48000	Note, when most people want to learn about naive bays, they want to learn about the multinomial naive bays classifier,
48000	50000	and that's what we talk about in this video.
50000	59000	However, just know that there is another commonly used version of naive bays, called Gaussian naive bays classification,
59000	62000	and I cover that in a follow-up stat quest.
62000	66000	So check that one out when you're done with this quest.
66000	68000	BAM.
68000	73000	Now, imagine we received normal messages from friends and family.
73000	81000	And we also received spam, unwanted messages that are usually scams or unsolicited advertisements.
81000	85000	And we wanted to filter out the spam messages.
85000	94000	So, the first thing we do is make a histogram of all the words that occur in the normal messages from friends and family.
94000	102000	We can use the histogram to calculate the probabilities of seeing each word, given that it was in a normal message.
102000	112000	For example, the probability we see the word deer, given that we saw it in a normal message, is 8.
112000	123000	The total number of times deer occurred in normal messages, divided by 17, the total number of words in all of the normal messages.
123000	127000	And that gives us 0.47.
127000	131000	So let's put that over the word deer so we don't forget it.
131000	141000	Likewise, the probability that we see the word friend, given that we saw it in a normal message, is 5.
141000	151000	The total number of times friend occurred in normal messages, divided by 17, the total number of words in all of the normal messages.
151000	155000	And that gives us 0.29.
155000	160000	So let's put that over the word friend so we don't forget it.
160000	168000	Likewise, the probability that we see the word lunch, given that it is in a normal message, is 0.18.
168000	176000	And the probability that we see the word money, given that it is in a normal message, is 0.06.
176000	186000	Now we make a histogram of all the words that occur in the spam, and calculate the probability of seeing the word deer,
186000	190000	given that we saw it in the spam.
190000	200000	And that is 2, the number of times we saw deer in the spam, divided by 7, the total number of words in the spam.
200000	204000	And that gives us 0.29.
204000	211000	Likewise, we calculate the probability of seeing the remaining words, given that they were in the spam.
211000	214000	Bam!
214000	222000	Now, because these histograms are taking up a lot of space, let's get rid of them, but keep the probabilities.
222000	226000	Oh no, it's the dreaded terminology alert.
226000	236000	Because we have calculated the probabilities of discrete, individual words, and not the probability of something continuous, like weight or height,
236000	241000	these probabilities are also called likelihoods.
241000	249000	I mention this because some tutorials say these are probabilities, and others say they are likelihoods.
249000	254000	In this case, the terms are interchangeable, so don't sweat it.
254000	263000	We'll talk more about probabilities versus likelihoods when we talk about Gaussian naive Bayes in the follow-up quest.
263000	267000	Now, imagine we got a new message that said,
267000	274000	Dear Friend, and we want to decide if it is a normal message or spam.
274000	283000	We start with an initial guess about the probability that any message, regardless of what it says, is a normal message.
283000	290000	This guess can be any probability that we want, but a common guess is estimated from the training data.
290000	299000	For example, since 8 of the 12 messages are normal messages, our initial guess will be 0.67.
299000	304000	So let's put that under the normal messages so we don't forget it.
304000	308000	Oh no, it's another dreaded terminology alert.
308000	315000	The initial guess that we observe a normal message is called a prior probability.
315000	322000	Now we multiply the initial guess by the probability that the word Dear occurs in a normal message,
322000	328000	and the probability that the word Friend occurs in a normal message.
328000	333000	Now we just plug in the values that we worked out earlier and do the math,
333000	339000	beep boop beep boop beep, and we get 0.09.
339000	346000	We can think of 0.09 as the score that Dear Friend gets if it is a normal message.
346000	356000	However, technically, it is proportional to the probability that the message is normal, given that it says Dear Friend.
356000	361000	So let's put that on top of the normal messages so we don't forget.
362000	368000	Now, just like we did before, we start with an initial guess about the probability that any message,
368000	372000	regardless of what it says, is spam.
372000	381000	And just like before, the guess can be any probability we want, but a common guess is estimated from the training data.
381000	389000	And since 4 of the 12 messages are spam, our initial guess will be 0.33.
389000	393000	So let's put that under the spam so we don't forget it.
393000	400000	Now we multiply that initial guess by the probability that the word Dear occurs in spam,
400000	406000	and the probability that the word Friend occurs in spam.
406000	411000	Now we just plug in the values that we worked out earlier and do the math,
411000	417000	beep boop beep boop beep, and we get 0.01.
417000	425000	Like before, we can think of 0.01 as the score that Dear Friend gets if it is spam.
425000	435000	However, technically, it is proportional to the probability that the message is spam, given that it says Dear Friend.
435000	445000	And because the score we got for normal message, 0.09, is greater than the score we got for spam, 0.01,
445000	450000	we will decide that Dear Friend is a normal message.
450000	453000	Double BAM!
453000	461000	Now, before we move on to a slightly more complex situation, let's review what we've done so far.
461000	469000	We started with histograms of all the words in the normal messages, and all of the words in the spam.
469000	478000	Then we calculated the probabilities of seeing each word, given that we saw the word in either a normal message or spam.
478000	484000	Then we made an initial guess about the probability of seeing a normal message.
484000	492000	This guess can be anything between 0 and 1, but we based ours on the classifications in the training dataset.
492000	498000	Then we made the same sort of guess about the probability of seeing spam.
498000	506000	Then we multiplied our initial guess that the message was normal, by the probabilities of seeing the words Dear and Friend,
506000	509000	given that the message was normal.
509000	514000	Then we multiplied our initial guess that the message was spam,
514000	520000	by the probabilities of seeing the words Dear and Friend, given that the message was spam.
520000	525000	Then we did the math and decided that Dear Friend was a normal message,
525000	530720	0.09 is greater than 0.01.
530720	536460	Now that we understand the basics of how naive Bayes classification works,
536460	540880	let's look at a slightly more complicated example.
540880	544400	This time, let's try to classify this message,
544400	548560	lunch, money, money, money, money.
548560	554040	Note, this message contains the word money four times.
554040	561440	And since the probability of seeing the word money is much higher in spam than in normal messages,
561440	567200	then it seems reasonable to predict that this message will end up being spam.
567200	569720	So let's do the math.
569720	574640	Calculating the score for a normal message works just like before.
574640	577080	We start with the initial guess,
577080	580560	then we multiply it by the probability we see lunch,
580560	583520	given that it is in a normal message.
583520	590440	And the probability we see money four times, given that it is in a normal message.
590440	595000	When we do the math, we get this tiny number.
595000	599800	However, when we do the same calculation for spam,
599800	602520	we get 0.
602520	607080	This is because the probability we see lunch in spam is 0,
607080	610280	since it was not in the training data.
610320	614040	And when we plug in 0 for the probability we see lunch,
614040	616880	given that it was in spam,
616880	623000	then it doesn't matter what value we picked for the initial guess that the message was spam,
623000	627000	and it doesn't matter what the probability is that we see money,
627000	630240	given that the message was spam,
630240	635560	because anything times 0 is 0.
635560	637000	In other words,
637000	639600	if a message contains the word lunch,
639600	642960	it will not be classified as spam.
642960	648160	And that means we will always classify the messages with lunch in them as normal,
648160	652360	no matter how many times we see the word money.
652360	655120	And that's a problem.
655120	657040	To work around this problem,
657040	661440	people usually add one count, represented by a black box,
661440	664840	to each word in the histograms.
664840	665840	Note,
665840	672760	the number of counts we add to each word is typically referred to with the Greek letter, alpha.
672760	675640	In this case, alpha equals 1,
675640	678640	but we could have said it to anything.
678640	679760	Anyway,
679760	683800	now when we calculate the probabilities of observing each word,
683800	686680	we never get 0.
686680	688080	For example,
688080	690400	the probability of seeing lunch,
690400	693040	given that it is in spam,
693040	694440	is 1,
694440	696080	divided by 7,
696080	698440	the total number of words in spam,
698440	699680	plus 4,
699680	702920	the extra counts that we added.
702920	707000	And that gives us 0.09.
707000	707800	Note,
707800	713600	adding counts to each word does not change our initial guess that a message is normal,
713600	717280	or the initial guess that the message is spam,
717280	724400	because adding a count to each word did not change the number of messages in the training dataset that are normal.
724760	728880	Or the number of messages that are spam.
728880	733200	Now when we calculate the scores for this message,
733200	737480	we still get a small number for the normal message,
737480	740480	but now when we calculate the value for spam,
740480	743680	we get a value greater than 0.
743680	749080	And since the value for spam is greater than the one for a normal message,
749080	752480	we classify the message as spam.
752560	755560	Spam!
755560	760880	Now let's talk about why Naive Bayes is naive.
760880	768640	The thing that makes Naive Bayes so naive is that it treats all word orders the same.
768640	770080	For example,
770080	774680	the normal message score for the phrase dear friend
774680	779600	is the exact same for the score for friend dear.
779600	780920	In other words,
780960	783440	regardless of how the words are ordered,
783440	786720	we get 0.08.
786720	793520	Treating all word orders equal is very different from how you and I communicate.
793520	797240	Every language has grammar rules and common phrases,
797240	801160	but Naive Bayes ignores all of that stuff.
801160	806360	Instead, Naive Bayes treats a language like it is just a bag full of words
806360	810360	and each message is a random handful of them.
810360	819080	Naive Bayes ignores all the rules because keeping track of every single reasonable phrase in a language would be impossible.
819080	822840	That said, even though Naive Bayes is naive,
822840	829600	it tends to perform surprisingly well when separating normal messages from spam.
829600	831560	In machine learning lingo,
831560	835200	we'd say that by ignoring relationships among words,
835200	838560	Naive Bayes has high bias.
838600	841440	But because it works well in practice,
841440	844840	Naive Bayes has low variance.
844840	847920	Shameless self-promotion
847920	852120	If you are not already familiar with the terms bias and variance,
852120	853480	check out the quest.
853480	856680	The link is in the description below.
856680	859960	Triple spam!
859960	864280	Oh no, it's one last shameless self-promotion.
864280	871120	One awesome way to support StatQuest is to purchase the Naive Bayes StatQuest Study Guide.
871120	875520	It has everything you need to study for an exam or job interview.
875520	878920	It's eight pages of total awesomeness.
878920	882880	And while you're there, check out the other StatQuest Study Guides.
882880	886480	There's something for everyone.
886480	891120	Hooray! We've made it to the end of another exciting StatQuest.
891120	894880	If you like this StatQuest and want to see more, please subscribe.
894880	899560	And if you want to support StatQuest, consider contributing to my Patreon campaign,
899560	904800	becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie,
904800	906000	or just donate.
906000	908360	The links are in the description below.
908360	910640	Alright, until next time,
910640	911640	quest on!
