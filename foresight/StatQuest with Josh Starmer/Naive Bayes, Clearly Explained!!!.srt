1
00:00:00,000 --> 00:00:06,000
I'm at home during lockdown, working on my stat quest, yeah.

2
00:00:06,000 --> 00:00:12,000
I'm at home during lockdown, working on my stat quest, yeah.

3
00:00:12,000 --> 00:00:14,000
Stat quest.

4
00:00:14,000 --> 00:00:18,000
Hello, I'm Josh Starmer, and welcome to Stat Quest.

5
00:00:18,000 --> 00:00:24,000
Today we're going to talk about naive bays, and it's going to be clearly explained.

6
00:00:24,000 --> 00:00:28,000
This stat quest is sponsored by JADBIO.

7
00:00:28,000 --> 00:00:34,000
Just add data, and their automatic machine learning algorithms will do the rest of the work for you.

8
00:00:34,000 --> 00:00:39,000
For more details, follow the link in the pinned comment below.

9
00:00:39,000 --> 00:00:48,000
Note, when most people want to learn about naive bays, they want to learn about the multinomial naive bays classifier,

10
00:00:48,000 --> 00:00:50,000
and that's what we talk about in this video.

11
00:00:50,000 --> 00:00:59,000
However, just know that there is another commonly used version of naive bays, called Gaussian naive bays classification,

12
00:00:59,000 --> 00:01:02,000
and I cover that in a follow-up stat quest.

13
00:01:02,000 --> 00:01:06,000
So check that one out when you're done with this quest.

14
00:01:06,000 --> 00:01:08,000
BAM.

15
00:01:08,000 --> 00:01:13,000
Now, imagine we received normal messages from friends and family.

16
00:01:13,000 --> 00:01:21,000
And we also received spam, unwanted messages that are usually scams or unsolicited advertisements.

17
00:01:21,000 --> 00:01:25,000
And we wanted to filter out the spam messages.

18
00:01:25,000 --> 00:01:34,000
So, the first thing we do is make a histogram of all the words that occur in the normal messages from friends and family.

19
00:01:34,000 --> 00:01:42,000
We can use the histogram to calculate the probabilities of seeing each word, given that it was in a normal message.

20
00:01:42,000 --> 00:01:52,000
For example, the probability we see the word deer, given that we saw it in a normal message, is 8.

21
00:01:52,000 --> 00:02:03,000
The total number of times deer occurred in normal messages, divided by 17, the total number of words in all of the normal messages.

22
00:02:03,000 --> 00:02:07,000
And that gives us 0.47.

23
00:02:07,000 --> 00:02:11,000
So let's put that over the word deer so we don't forget it.

24
00:02:11,000 --> 00:02:21,000
Likewise, the probability that we see the word friend, given that we saw it in a normal message, is 5.

25
00:02:21,000 --> 00:02:31,000
The total number of times friend occurred in normal messages, divided by 17, the total number of words in all of the normal messages.

26
00:02:31,000 --> 00:02:35,000
And that gives us 0.29.

27
00:02:35,000 --> 00:02:40,000
So let's put that over the word friend so we don't forget it.

28
00:02:40,000 --> 00:02:48,000
Likewise, the probability that we see the word lunch, given that it is in a normal message, is 0.18.

29
00:02:48,000 --> 00:02:56,000
And the probability that we see the word money, given that it is in a normal message, is 0.06.

30
00:02:56,000 --> 00:03:06,000
Now we make a histogram of all the words that occur in the spam, and calculate the probability of seeing the word deer,

31
00:03:06,000 --> 00:03:10,000
given that we saw it in the spam.

32
00:03:10,000 --> 00:03:20,000
And that is 2, the number of times we saw deer in the spam, divided by 7, the total number of words in the spam.

33
00:03:20,000 --> 00:03:24,000
And that gives us 0.29.

34
00:03:24,000 --> 00:03:31,000
Likewise, we calculate the probability of seeing the remaining words, given that they were in the spam.

35
00:03:31,000 --> 00:03:34,000
Bam!

36
00:03:34,000 --> 00:03:42,000
Now, because these histograms are taking up a lot of space, let's get rid of them, but keep the probabilities.

37
00:03:42,000 --> 00:03:46,000
Oh no, it's the dreaded terminology alert.

38
00:03:46,000 --> 00:03:56,000
Because we have calculated the probabilities of discrete, individual words, and not the probability of something continuous, like weight or height,

39
00:03:56,000 --> 00:04:01,000
these probabilities are also called likelihoods.

40
00:04:01,000 --> 00:04:09,000
I mention this because some tutorials say these are probabilities, and others say they are likelihoods.

41
00:04:09,000 --> 00:04:14,000
In this case, the terms are interchangeable, so don't sweat it.

42
00:04:14,000 --> 00:04:23,000
We'll talk more about probabilities versus likelihoods when we talk about Gaussian naive Bayes in the follow-up quest.

43
00:04:23,000 --> 00:04:27,000
Now, imagine we got a new message that said,

44
00:04:27,000 --> 00:04:34,000
Dear Friend, and we want to decide if it is a normal message or spam.

45
00:04:34,000 --> 00:04:43,000
We start with an initial guess about the probability that any message, regardless of what it says, is a normal message.

46
00:04:43,000 --> 00:04:50,000
This guess can be any probability that we want, but a common guess is estimated from the training data.

47
00:04:50,000 --> 00:04:59,000
For example, since 8 of the 12 messages are normal messages, our initial guess will be 0.67.

48
00:04:59,000 --> 00:05:04,000
So let's put that under the normal messages so we don't forget it.

49
00:05:04,000 --> 00:05:08,000
Oh no, it's another dreaded terminology alert.

50
00:05:08,000 --> 00:05:15,000
The initial guess that we observe a normal message is called a prior probability.

51
00:05:15,000 --> 00:05:22,000
Now we multiply the initial guess by the probability that the word Dear occurs in a normal message,

52
00:05:22,000 --> 00:05:28,000
and the probability that the word Friend occurs in a normal message.

53
00:05:28,000 --> 00:05:33,000
Now we just plug in the values that we worked out earlier and do the math,

54
00:05:33,000 --> 00:05:39,000
beep boop beep boop beep, and we get 0.09.

55
00:05:39,000 --> 00:05:46,000
We can think of 0.09 as the score that Dear Friend gets if it is a normal message.

56
00:05:46,000 --> 00:05:56,000
However, technically, it is proportional to the probability that the message is normal, given that it says Dear Friend.

57
00:05:56,000 --> 00:06:01,000
So let's put that on top of the normal messages so we don't forget.

58
00:06:02,000 --> 00:06:08,000
Now, just like we did before, we start with an initial guess about the probability that any message,

59
00:06:08,000 --> 00:06:12,000
regardless of what it says, is spam.

60
00:06:12,000 --> 00:06:21,000
And just like before, the guess can be any probability we want, but a common guess is estimated from the training data.

61
00:06:21,000 --> 00:06:29,000
And since 4 of the 12 messages are spam, our initial guess will be 0.33.

62
00:06:29,000 --> 00:06:33,000
So let's put that under the spam so we don't forget it.

63
00:06:33,000 --> 00:06:40,000
Now we multiply that initial guess by the probability that the word Dear occurs in spam,

64
00:06:40,000 --> 00:06:46,000
and the probability that the word Friend occurs in spam.

65
00:06:46,000 --> 00:06:51,000
Now we just plug in the values that we worked out earlier and do the math,

66
00:06:51,000 --> 00:06:57,000
beep boop beep boop beep, and we get 0.01.

67
00:06:57,000 --> 00:07:05,000
Like before, we can think of 0.01 as the score that Dear Friend gets if it is spam.

68
00:07:05,000 --> 00:07:15,000
However, technically, it is proportional to the probability that the message is spam, given that it says Dear Friend.

69
00:07:15,000 --> 00:07:25,000
And because the score we got for normal message, 0.09, is greater than the score we got for spam, 0.01,

70
00:07:25,000 --> 00:07:30,000
we will decide that Dear Friend is a normal message.

71
00:07:30,000 --> 00:07:33,000
Double BAM!

72
00:07:33,000 --> 00:07:41,000
Now, before we move on to a slightly more complex situation, let's review what we've done so far.

73
00:07:41,000 --> 00:07:49,000
We started with histograms of all the words in the normal messages, and all of the words in the spam.

74
00:07:49,000 --> 00:07:58,000
Then we calculated the probabilities of seeing each word, given that we saw the word in either a normal message or spam.

75
00:07:58,000 --> 00:08:04,000
Then we made an initial guess about the probability of seeing a normal message.

76
00:08:04,000 --> 00:08:12,000
This guess can be anything between 0 and 1, but we based ours on the classifications in the training dataset.

77
00:08:12,000 --> 00:08:18,000
Then we made the same sort of guess about the probability of seeing spam.

78
00:08:18,000 --> 00:08:26,000
Then we multiplied our initial guess that the message was normal, by the probabilities of seeing the words Dear and Friend,

79
00:08:26,000 --> 00:08:29,000
given that the message was normal.

80
00:08:29,000 --> 00:08:34,000
Then we multiplied our initial guess that the message was spam,

81
00:08:34,000 --> 00:08:40,000
by the probabilities of seeing the words Dear and Friend, given that the message was spam.

82
00:08:40,000 --> 00:08:45,000
Then we did the math and decided that Dear Friend was a normal message,

83
00:08:45,000 --> 00:08:50,720
0.09 is greater than 0.01.

84
00:08:50,720 --> 00:08:56,460
Now that we understand the basics of how naive Bayes classification works,

85
00:08:56,460 --> 00:09:00,880
let's look at a slightly more complicated example.

86
00:09:00,880 --> 00:09:04,400
This time, let's try to classify this message,

87
00:09:04,400 --> 00:09:08,560
lunch, money, money, money, money.

88
00:09:08,560 --> 00:09:14,040
Note, this message contains the word money four times.

89
00:09:14,040 --> 00:09:21,440
And since the probability of seeing the word money is much higher in spam than in normal messages,

90
00:09:21,440 --> 00:09:27,200
then it seems reasonable to predict that this message will end up being spam.

91
00:09:27,200 --> 00:09:29,720
So let's do the math.

92
00:09:29,720 --> 00:09:34,640
Calculating the score for a normal message works just like before.

93
00:09:34,640 --> 00:09:37,080
We start with the initial guess,

94
00:09:37,080 --> 00:09:40,560
then we multiply it by the probability we see lunch,

95
00:09:40,560 --> 00:09:43,520
given that it is in a normal message.

96
00:09:43,520 --> 00:09:50,440
And the probability we see money four times, given that it is in a normal message.

97
00:09:50,440 --> 00:09:55,000
When we do the math, we get this tiny number.

98
00:09:55,000 --> 00:09:59,800
However, when we do the same calculation for spam,

99
00:09:59,800 --> 00:10:02,520
we get 0.

100
00:10:02,520 --> 00:10:07,080
This is because the probability we see lunch in spam is 0,

101
00:10:07,080 --> 00:10:10,280
since it was not in the training data.

102
00:10:10,320 --> 00:10:14,040
And when we plug in 0 for the probability we see lunch,

103
00:10:14,040 --> 00:10:16,880
given that it was in spam,

104
00:10:16,880 --> 00:10:23,000
then it doesn't matter what value we picked for the initial guess that the message was spam,

105
00:10:23,000 --> 00:10:27,000
and it doesn't matter what the probability is that we see money,

106
00:10:27,000 --> 00:10:30,240
given that the message was spam,

107
00:10:30,240 --> 00:10:35,560
because anything times 0 is 0.

108
00:10:35,560 --> 00:10:37,000
In other words,

109
00:10:37,000 --> 00:10:39,600
if a message contains the word lunch,

110
00:10:39,600 --> 00:10:42,960
it will not be classified as spam.

111
00:10:42,960 --> 00:10:48,160
And that means we will always classify the messages with lunch in them as normal,

112
00:10:48,160 --> 00:10:52,360
no matter how many times we see the word money.

113
00:10:52,360 --> 00:10:55,120
And that's a problem.

114
00:10:55,120 --> 00:10:57,040
To work around this problem,

115
00:10:57,040 --> 00:11:01,440
people usually add one count, represented by a black box,

116
00:11:01,440 --> 00:11:04,840
to each word in the histograms.

117
00:11:04,840 --> 00:11:05,840
Note,

118
00:11:05,840 --> 00:11:12,760
the number of counts we add to each word is typically referred to with the Greek letter, alpha.

119
00:11:12,760 --> 00:11:15,640
In this case, alpha equals 1,

120
00:11:15,640 --> 00:11:18,640
but we could have said it to anything.

121
00:11:18,640 --> 00:11:19,760
Anyway,

122
00:11:19,760 --> 00:11:23,800
now when we calculate the probabilities of observing each word,

123
00:11:23,800 --> 00:11:26,680
we never get 0.

124
00:11:26,680 --> 00:11:28,080
For example,

125
00:11:28,080 --> 00:11:30,400
the probability of seeing lunch,

126
00:11:30,400 --> 00:11:33,040
given that it is in spam,

127
00:11:33,040 --> 00:11:34,440
is 1,

128
00:11:34,440 --> 00:11:36,080
divided by 7,

129
00:11:36,080 --> 00:11:38,440
the total number of words in spam,

130
00:11:38,440 --> 00:11:39,680
plus 4,

131
00:11:39,680 --> 00:11:42,920
the extra counts that we added.

132
00:11:42,920 --> 00:11:47,000
And that gives us 0.09.

133
00:11:47,000 --> 00:11:47,800
Note,

134
00:11:47,800 --> 00:11:53,600
adding counts to each word does not change our initial guess that a message is normal,

135
00:11:53,600 --> 00:11:57,280
or the initial guess that the message is spam,

136
00:11:57,280 --> 00:12:04,400
because adding a count to each word did not change the number of messages in the training dataset that are normal.

137
00:12:04,760 --> 00:12:08,880
Or the number of messages that are spam.

138
00:12:08,880 --> 00:12:13,200
Now when we calculate the scores for this message,

139
00:12:13,200 --> 00:12:17,480
we still get a small number for the normal message,

140
00:12:17,480 --> 00:12:20,480
but now when we calculate the value for spam,

141
00:12:20,480 --> 00:12:23,680
we get a value greater than 0.

142
00:12:23,680 --> 00:12:29,080
And since the value for spam is greater than the one for a normal message,

143
00:12:29,080 --> 00:12:32,480
we classify the message as spam.

144
00:12:32,560 --> 00:12:35,560
Spam!

145
00:12:35,560 --> 00:12:40,880
Now let's talk about why Naive Bayes is naive.

146
00:12:40,880 --> 00:12:48,640
The thing that makes Naive Bayes so naive is that it treats all word orders the same.

147
00:12:48,640 --> 00:12:50,080
For example,

148
00:12:50,080 --> 00:12:54,680
the normal message score for the phrase dear friend

149
00:12:54,680 --> 00:12:59,600
is the exact same for the score for friend dear.

150
00:12:59,600 --> 00:13:00,920
In other words,

151
00:13:00,960 --> 00:13:03,440
regardless of how the words are ordered,

152
00:13:03,440 --> 00:13:06,720
we get 0.08.

153
00:13:06,720 --> 00:13:13,520
Treating all word orders equal is very different from how you and I communicate.

154
00:13:13,520 --> 00:13:17,240
Every language has grammar rules and common phrases,

155
00:13:17,240 --> 00:13:21,160
but Naive Bayes ignores all of that stuff.

156
00:13:21,160 --> 00:13:26,360
Instead, Naive Bayes treats a language like it is just a bag full of words

157
00:13:26,360 --> 00:13:30,360
and each message is a random handful of them.

158
00:13:30,360 --> 00:13:39,080
Naive Bayes ignores all the rules because keeping track of every single reasonable phrase in a language would be impossible.

159
00:13:39,080 --> 00:13:42,840
That said, even though Naive Bayes is naive,

160
00:13:42,840 --> 00:13:49,600
it tends to perform surprisingly well when separating normal messages from spam.

161
00:13:49,600 --> 00:13:51,560
In machine learning lingo,

162
00:13:51,560 --> 00:13:55,200
we'd say that by ignoring relationships among words,

163
00:13:55,200 --> 00:13:58,560
Naive Bayes has high bias.

164
00:13:58,600 --> 00:14:01,440
But because it works well in practice,

165
00:14:01,440 --> 00:14:04,840
Naive Bayes has low variance.

166
00:14:04,840 --> 00:14:07,920
Shameless self-promotion

167
00:14:07,920 --> 00:14:12,120
If you are not already familiar with the terms bias and variance,

168
00:14:12,120 --> 00:14:13,480
check out the quest.

169
00:14:13,480 --> 00:14:16,680
The link is in the description below.

170
00:14:16,680 --> 00:14:19,960
Triple spam!

171
00:14:19,960 --> 00:14:24,280
Oh no, it's one last shameless self-promotion.

172
00:14:24,280 --> 00:14:31,120
One awesome way to support StatQuest is to purchase the Naive Bayes StatQuest Study Guide.

173
00:14:31,120 --> 00:14:35,520
It has everything you need to study for an exam or job interview.

174
00:14:35,520 --> 00:14:38,920
It's eight pages of total awesomeness.

175
00:14:38,920 --> 00:14:42,880
And while you're there, check out the other StatQuest Study Guides.

176
00:14:42,880 --> 00:14:46,480
There's something for everyone.

177
00:14:46,480 --> 00:14:51,120
Hooray! We've made it to the end of another exciting StatQuest.

178
00:14:51,120 --> 00:14:54,880
If you like this StatQuest and want to see more, please subscribe.

179
00:14:54,880 --> 00:14:59,560
And if you want to support StatQuest, consider contributing to my Patreon campaign,

180
00:14:59,560 --> 00:15:04,800
becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie,

181
00:15:04,800 --> 00:15:06,000
or just donate.

182
00:15:06,000 --> 00:15:08,360
The links are in the description below.

183
00:15:08,360 --> 00:15:10,640
Alright, until next time,

184
00:15:10,640 --> 00:15:11,640
quest on!

