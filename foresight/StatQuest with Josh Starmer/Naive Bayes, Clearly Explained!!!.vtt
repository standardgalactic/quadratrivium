WEBVTT

00:00.000 --> 00:06.000
I'm at home during lockdown, working on my stat quest, yeah.

00:06.000 --> 00:12.000
I'm at home during lockdown, working on my stat quest, yeah.

00:12.000 --> 00:14.000
Stat quest.

00:14.000 --> 00:18.000
Hello, I'm Josh Starmer, and welcome to Stat Quest.

00:18.000 --> 00:24.000
Today we're going to talk about naive bays, and it's going to be clearly explained.

00:24.000 --> 00:28.000
This stat quest is sponsored by JADBIO.

00:28.000 --> 00:34.000
Just add data, and their automatic machine learning algorithms will do the rest of the work for you.

00:34.000 --> 00:39.000
For more details, follow the link in the pinned comment below.

00:39.000 --> 00:48.000
Note, when most people want to learn about naive bays, they want to learn about the multinomial naive bays classifier,

00:48.000 --> 00:50.000
and that's what we talk about in this video.

00:50.000 --> 00:59.000
However, just know that there is another commonly used version of naive bays, called Gaussian naive bays classification,

00:59.000 --> 01:02.000
and I cover that in a follow-up stat quest.

01:02.000 --> 01:06.000
So check that one out when you're done with this quest.

01:06.000 --> 01:08.000
BAM.

01:08.000 --> 01:13.000
Now, imagine we received normal messages from friends and family.

01:13.000 --> 01:21.000
And we also received spam, unwanted messages that are usually scams or unsolicited advertisements.

01:21.000 --> 01:25.000
And we wanted to filter out the spam messages.

01:25.000 --> 01:34.000
So, the first thing we do is make a histogram of all the words that occur in the normal messages from friends and family.

01:34.000 --> 01:42.000
We can use the histogram to calculate the probabilities of seeing each word, given that it was in a normal message.

01:42.000 --> 01:52.000
For example, the probability we see the word deer, given that we saw it in a normal message, is 8.

01:52.000 --> 02:03.000
The total number of times deer occurred in normal messages, divided by 17, the total number of words in all of the normal messages.

02:03.000 --> 02:07.000
And that gives us 0.47.

02:07.000 --> 02:11.000
So let's put that over the word deer so we don't forget it.

02:11.000 --> 02:21.000
Likewise, the probability that we see the word friend, given that we saw it in a normal message, is 5.

02:21.000 --> 02:31.000
The total number of times friend occurred in normal messages, divided by 17, the total number of words in all of the normal messages.

02:31.000 --> 02:35.000
And that gives us 0.29.

02:35.000 --> 02:40.000
So let's put that over the word friend so we don't forget it.

02:40.000 --> 02:48.000
Likewise, the probability that we see the word lunch, given that it is in a normal message, is 0.18.

02:48.000 --> 02:56.000
And the probability that we see the word money, given that it is in a normal message, is 0.06.

02:56.000 --> 03:06.000
Now we make a histogram of all the words that occur in the spam, and calculate the probability of seeing the word deer,

03:06.000 --> 03:10.000
given that we saw it in the spam.

03:10.000 --> 03:20.000
And that is 2, the number of times we saw deer in the spam, divided by 7, the total number of words in the spam.

03:20.000 --> 03:24.000
And that gives us 0.29.

03:24.000 --> 03:31.000
Likewise, we calculate the probability of seeing the remaining words, given that they were in the spam.

03:31.000 --> 03:34.000
Bam!

03:34.000 --> 03:42.000
Now, because these histograms are taking up a lot of space, let's get rid of them, but keep the probabilities.

03:42.000 --> 03:46.000
Oh no, it's the dreaded terminology alert.

03:46.000 --> 03:56.000
Because we have calculated the probabilities of discrete, individual words, and not the probability of something continuous, like weight or height,

03:56.000 --> 04:01.000
these probabilities are also called likelihoods.

04:01.000 --> 04:09.000
I mention this because some tutorials say these are probabilities, and others say they are likelihoods.

04:09.000 --> 04:14.000
In this case, the terms are interchangeable, so don't sweat it.

04:14.000 --> 04:23.000
We'll talk more about probabilities versus likelihoods when we talk about Gaussian naive Bayes in the follow-up quest.

04:23.000 --> 04:27.000
Now, imagine we got a new message that said,

04:27.000 --> 04:34.000
Dear Friend, and we want to decide if it is a normal message or spam.

04:34.000 --> 04:43.000
We start with an initial guess about the probability that any message, regardless of what it says, is a normal message.

04:43.000 --> 04:50.000
This guess can be any probability that we want, but a common guess is estimated from the training data.

04:50.000 --> 04:59.000
For example, since 8 of the 12 messages are normal messages, our initial guess will be 0.67.

04:59.000 --> 05:04.000
So let's put that under the normal messages so we don't forget it.

05:04.000 --> 05:08.000
Oh no, it's another dreaded terminology alert.

05:08.000 --> 05:15.000
The initial guess that we observe a normal message is called a prior probability.

05:15.000 --> 05:22.000
Now we multiply the initial guess by the probability that the word Dear occurs in a normal message,

05:22.000 --> 05:28.000
and the probability that the word Friend occurs in a normal message.

05:28.000 --> 05:33.000
Now we just plug in the values that we worked out earlier and do the math,

05:33.000 --> 05:39.000
beep boop beep boop beep, and we get 0.09.

05:39.000 --> 05:46.000
We can think of 0.09 as the score that Dear Friend gets if it is a normal message.

05:46.000 --> 05:56.000
However, technically, it is proportional to the probability that the message is normal, given that it says Dear Friend.

05:56.000 --> 06:01.000
So let's put that on top of the normal messages so we don't forget.

06:02.000 --> 06:08.000
Now, just like we did before, we start with an initial guess about the probability that any message,

06:08.000 --> 06:12.000
regardless of what it says, is spam.

06:12.000 --> 06:21.000
And just like before, the guess can be any probability we want, but a common guess is estimated from the training data.

06:21.000 --> 06:29.000
And since 4 of the 12 messages are spam, our initial guess will be 0.33.

06:29.000 --> 06:33.000
So let's put that under the spam so we don't forget it.

06:33.000 --> 06:40.000
Now we multiply that initial guess by the probability that the word Dear occurs in spam,

06:40.000 --> 06:46.000
and the probability that the word Friend occurs in spam.

06:46.000 --> 06:51.000
Now we just plug in the values that we worked out earlier and do the math,

06:51.000 --> 06:57.000
beep boop beep boop beep, and we get 0.01.

06:57.000 --> 07:05.000
Like before, we can think of 0.01 as the score that Dear Friend gets if it is spam.

07:05.000 --> 07:15.000
However, technically, it is proportional to the probability that the message is spam, given that it says Dear Friend.

07:15.000 --> 07:25.000
And because the score we got for normal message, 0.09, is greater than the score we got for spam, 0.01,

07:25.000 --> 07:30.000
we will decide that Dear Friend is a normal message.

07:30.000 --> 07:33.000
Double BAM!

07:33.000 --> 07:41.000
Now, before we move on to a slightly more complex situation, let's review what we've done so far.

07:41.000 --> 07:49.000
We started with histograms of all the words in the normal messages, and all of the words in the spam.

07:49.000 --> 07:58.000
Then we calculated the probabilities of seeing each word, given that we saw the word in either a normal message or spam.

07:58.000 --> 08:04.000
Then we made an initial guess about the probability of seeing a normal message.

08:04.000 --> 08:12.000
This guess can be anything between 0 and 1, but we based ours on the classifications in the training dataset.

08:12.000 --> 08:18.000
Then we made the same sort of guess about the probability of seeing spam.

08:18.000 --> 08:26.000
Then we multiplied our initial guess that the message was normal, by the probabilities of seeing the words Dear and Friend,

08:26.000 --> 08:29.000
given that the message was normal.

08:29.000 --> 08:34.000
Then we multiplied our initial guess that the message was spam,

08:34.000 --> 08:40.000
by the probabilities of seeing the words Dear and Friend, given that the message was spam.

08:40.000 --> 08:45.000
Then we did the math and decided that Dear Friend was a normal message,

08:45.000 --> 08:50.720
0.09 is greater than 0.01.

08:50.720 --> 08:56.460
Now that we understand the basics of how naive Bayes classification works,

08:56.460 --> 09:00.880
let's look at a slightly more complicated example.

09:00.880 --> 09:04.400
This time, let's try to classify this message,

09:04.400 --> 09:08.560
lunch, money, money, money, money.

09:08.560 --> 09:14.040
Note, this message contains the word money four times.

09:14.040 --> 09:21.440
And since the probability of seeing the word money is much higher in spam than in normal messages,

09:21.440 --> 09:27.200
then it seems reasonable to predict that this message will end up being spam.

09:27.200 --> 09:29.720
So let's do the math.

09:29.720 --> 09:34.640
Calculating the score for a normal message works just like before.

09:34.640 --> 09:37.080
We start with the initial guess,

09:37.080 --> 09:40.560
then we multiply it by the probability we see lunch,

09:40.560 --> 09:43.520
given that it is in a normal message.

09:43.520 --> 09:50.440
And the probability we see money four times, given that it is in a normal message.

09:50.440 --> 09:55.000
When we do the math, we get this tiny number.

09:55.000 --> 09:59.800
However, when we do the same calculation for spam,

09:59.800 --> 10:02.520
we get 0.

10:02.520 --> 10:07.080
This is because the probability we see lunch in spam is 0,

10:07.080 --> 10:10.280
since it was not in the training data.

10:10.320 --> 10:14.040
And when we plug in 0 for the probability we see lunch,

10:14.040 --> 10:16.880
given that it was in spam,

10:16.880 --> 10:23.000
then it doesn't matter what value we picked for the initial guess that the message was spam,

10:23.000 --> 10:27.000
and it doesn't matter what the probability is that we see money,

10:27.000 --> 10:30.240
given that the message was spam,

10:30.240 --> 10:35.560
because anything times 0 is 0.

10:35.560 --> 10:37.000
In other words,

10:37.000 --> 10:39.600
if a message contains the word lunch,

10:39.600 --> 10:42.960
it will not be classified as spam.

10:42.960 --> 10:48.160
And that means we will always classify the messages with lunch in them as normal,

10:48.160 --> 10:52.360
no matter how many times we see the word money.

10:52.360 --> 10:55.120
And that's a problem.

10:55.120 --> 10:57.040
To work around this problem,

10:57.040 --> 11:01.440
people usually add one count, represented by a black box,

11:01.440 --> 11:04.840
to each word in the histograms.

11:04.840 --> 11:05.840
Note,

11:05.840 --> 11:12.760
the number of counts we add to each word is typically referred to with the Greek letter, alpha.

11:12.760 --> 11:15.640
In this case, alpha equals 1,

11:15.640 --> 11:18.640
but we could have said it to anything.

11:18.640 --> 11:19.760
Anyway,

11:19.760 --> 11:23.800
now when we calculate the probabilities of observing each word,

11:23.800 --> 11:26.680
we never get 0.

11:26.680 --> 11:28.080
For example,

11:28.080 --> 11:30.400
the probability of seeing lunch,

11:30.400 --> 11:33.040
given that it is in spam,

11:33.040 --> 11:34.440
is 1,

11:34.440 --> 11:36.080
divided by 7,

11:36.080 --> 11:38.440
the total number of words in spam,

11:38.440 --> 11:39.680
plus 4,

11:39.680 --> 11:42.920
the extra counts that we added.

11:42.920 --> 11:47.000
And that gives us 0.09.

11:47.000 --> 11:47.800
Note,

11:47.800 --> 11:53.600
adding counts to each word does not change our initial guess that a message is normal,

11:53.600 --> 11:57.280
or the initial guess that the message is spam,

11:57.280 --> 12:04.400
because adding a count to each word did not change the number of messages in the training dataset that are normal.

12:04.760 --> 12:08.880
Or the number of messages that are spam.

12:08.880 --> 12:13.200
Now when we calculate the scores for this message,

12:13.200 --> 12:17.480
we still get a small number for the normal message,

12:17.480 --> 12:20.480
but now when we calculate the value for spam,

12:20.480 --> 12:23.680
we get a value greater than 0.

12:23.680 --> 12:29.080
And since the value for spam is greater than the one for a normal message,

12:29.080 --> 12:32.480
we classify the message as spam.

12:32.560 --> 12:35.560
Spam!

12:35.560 --> 12:40.880
Now let's talk about why Naive Bayes is naive.

12:40.880 --> 12:48.640
The thing that makes Naive Bayes so naive is that it treats all word orders the same.

12:48.640 --> 12:50.080
For example,

12:50.080 --> 12:54.680
the normal message score for the phrase dear friend

12:54.680 --> 12:59.600
is the exact same for the score for friend dear.

12:59.600 --> 13:00.920
In other words,

13:00.960 --> 13:03.440
regardless of how the words are ordered,

13:03.440 --> 13:06.720
we get 0.08.

13:06.720 --> 13:13.520
Treating all word orders equal is very different from how you and I communicate.

13:13.520 --> 13:17.240
Every language has grammar rules and common phrases,

13:17.240 --> 13:21.160
but Naive Bayes ignores all of that stuff.

13:21.160 --> 13:26.360
Instead, Naive Bayes treats a language like it is just a bag full of words

13:26.360 --> 13:30.360
and each message is a random handful of them.

13:30.360 --> 13:39.080
Naive Bayes ignores all the rules because keeping track of every single reasonable phrase in a language would be impossible.

13:39.080 --> 13:42.840
That said, even though Naive Bayes is naive,

13:42.840 --> 13:49.600
it tends to perform surprisingly well when separating normal messages from spam.

13:49.600 --> 13:51.560
In machine learning lingo,

13:51.560 --> 13:55.200
we'd say that by ignoring relationships among words,

13:55.200 --> 13:58.560
Naive Bayes has high bias.

13:58.600 --> 14:01.440
But because it works well in practice,

14:01.440 --> 14:04.840
Naive Bayes has low variance.

14:04.840 --> 14:07.920
Shameless self-promotion

14:07.920 --> 14:12.120
If you are not already familiar with the terms bias and variance,

14:12.120 --> 14:13.480
check out the quest.

14:13.480 --> 14:16.680
The link is in the description below.

14:16.680 --> 14:19.960
Triple spam!

14:19.960 --> 14:24.280
Oh no, it's one last shameless self-promotion.

14:24.280 --> 14:31.120
One awesome way to support StatQuest is to purchase the Naive Bayes StatQuest Study Guide.

14:31.120 --> 14:35.520
It has everything you need to study for an exam or job interview.

14:35.520 --> 14:38.920
It's eight pages of total awesomeness.

14:38.920 --> 14:42.880
And while you're there, check out the other StatQuest Study Guides.

14:42.880 --> 14:46.480
There's something for everyone.

14:46.480 --> 14:51.120
Hooray! We've made it to the end of another exciting StatQuest.

14:51.120 --> 14:54.880
If you like this StatQuest and want to see more, please subscribe.

14:54.880 --> 14:59.560
And if you want to support StatQuest, consider contributing to my Patreon campaign,

14:59.560 --> 15:04.800
becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie,

15:04.800 --> 15:06.000
or just donate.

15:06.000 --> 15:08.360
The links are in the description below.

15:08.360 --> 15:10.640
Alright, until next time,

15:10.640 --> 15:11.640
quest on!

