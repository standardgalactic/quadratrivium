WEBVTT

00:00.000 --> 00:04.680
If you sort of zoom out, you look like we are right now in the middle of this like explosion of some

00:04.680 --> 00:10.880
sec and we don't know what it's going to end up like but but I do believe as you alluded to earlier

00:10.880 --> 00:15.060
that scenarios in which we just remain more or less like we are now and have the current human

00:15.060 --> 00:20.720
condition just keep going for like tens of thousands of years that just seems extremely improbable to

00:20.720 --> 00:26.880
me relative to either like extinction slash dystopia or some radical transformation into some kind of

00:26.880 --> 00:33.840
posthumous condition. Hello this is Robinson Earhart here with the introduction to Robinson's

00:33.840 --> 00:41.600
podcast number 205 and this episode is with Nick Bostrom a philosopher of artificial intelligence

00:41.600 --> 00:47.640
and many other subjects who was most recently professor of philosophy at Oxford University where

00:47.640 --> 00:55.640
he also served as the founding director of the Future of Humanity Institute. Nick is likely best

00:55.640 --> 01:03.320
known in the public eye for his work on the simulation argument which is the idea that we live

01:03.320 --> 01:11.880
not in the familiar physical world that we believe we inhabit but a simulation of one but he's also

01:11.880 --> 01:17.760
well known for his book Super Intelligence which covers the dangers of artificial intelligence

01:17.760 --> 01:24.360
and strategies for dealing with them. In this episode though Nick and I talk about his more

01:24.360 --> 01:31.400
recent book Deep Utopia Life and Meaning in a Solved World which just came out in March actually

01:31.400 --> 01:39.720
and opposed to get as opposed to getting into the existential dangers of AI which Nick tackles in

01:39.720 --> 01:46.440
super intelligence deep utopia considers the sorts of concerns that might arise if everything

01:46.440 --> 01:54.120
actually goes right with AI. So after talking about the alignment problem and some other fundamental

01:54.120 --> 02:00.840
issues in the philosophy of AI we talk about the problems that might come from having perfect

02:00.840 --> 02:08.200
technology from immortality from not having to work anymore from having robots that can do all

02:08.200 --> 02:16.680
your hobbies better than you can do and more. Likes, comments, subscribes, reviews as you know all of

02:16.680 --> 02:25.480
these things are always extremely appreciated. There is also a patreon if you would like a link

02:25.480 --> 02:34.040
to an ad for your rss feed or show notes and to all those of you who are patrons who are geeslings

02:34.040 --> 02:39.560
thank you so much for your support. But now without any further ado I hope that you enjoy

02:39.560 --> 02:42.280
this conversation as much as I enjoyed having in with Nick.

02:50.760 --> 02:56.280
You're one of the most widely recognized philosophers in the in the public sphere

02:56.840 --> 03:04.440
and mainly known in that capacity I think for thinking about dystopian AI scenarios scenarios

03:04.440 --> 03:10.680
and other existential threats so right off the bat I'm wondering what motivated this shift to

03:10.680 --> 03:20.040
thinking about utopias. Now both sides have always been present in my mind and my outlook. Back when

03:20.040 --> 03:28.040
I was writing super intelligence this 2014 book which was in the works six years prior to that I

03:28.040 --> 03:36.120
felt it was more urgent to focus on what could go wrong with AI figure out where the pitfalls were

03:36.120 --> 03:45.000
so that we could avoid them. This was a time at which the whole idea of AI posing any kind of

03:45.640 --> 03:51.000
existential risk or even just having any kind of transformative impact on society was still

03:51.000 --> 03:57.400
far outside the mainstream. The whole idea of the alignment problem was not in the

03:57.400 --> 04:02.200
overton window that were like maybe a handful of people scattered around the world kind of internet

04:02.200 --> 04:06.920
type so we're trying to work on this and it just seemed hugely neglected to me so I thought writing

04:06.920 --> 04:11.880
this book super intelligence trying to develop concepts that would make it possible for people

04:11.880 --> 04:17.160
to start to think more constructively about this and begin to do research on developing scalable

04:17.160 --> 04:24.120
methods for alignment was important. In the intervening years we've seen a big shift now all

04:24.120 --> 04:29.320
the frontier AI labs have research teams specifically trying to develop scalable methods for AI

04:29.320 --> 04:34.200
control and there are many other organizations as well where a lot of the smartest young people I

04:34.200 --> 04:40.120
know now are like flooding into this field and in the last two years we've also seen a big shift

04:41.160 --> 04:47.720
at the level of policy conversation where even top tier policy makers are now beginning to

04:47.720 --> 04:53.000
recognize the transformative impact of future AI developments with statements coming out from the

04:53.000 --> 04:58.120
White House and the UK hosted this global summit on AI. I was just the other week in Brussels for

04:58.120 --> 05:04.920
a however meeting and so to some extent that whole thing is now widely recognized and many of

05:04.920 --> 05:14.120
me feel a work on it. I felt the other side of what happens if things go right with AI had

05:14.120 --> 05:19.720
not yet really been addressed or people talk about it but often in a rather superficial way

05:20.600 --> 05:24.120
but once you start to dig in you realize there are actually quite

05:24.760 --> 05:29.240
deep even philosophical problems that come up when you imagine like what would give human life

05:29.240 --> 05:34.600
purpose and meaning in this condition where in a solved world as I call it like we're all practical

05:34.600 --> 05:39.960
problems that can be solved through progress have already been solved and so that's how the book

05:41.320 --> 05:45.800
it wasn't really so much a plan to write the book it more kind of happened as sometimes

05:46.440 --> 05:50.680
is the way of these things like you start writing a little and then it takes on a life of its own

05:50.680 --> 05:57.160
and you just try to hold on less the thing gradually develops. I can tell that this book

05:57.160 --> 06:04.600
really did take on a life of its own just because of the the various formats in which you wrote it

06:04.600 --> 06:10.520
and I mean there's there's poetry there's dialogue there's standard exposition so it was

06:10.520 --> 06:18.920
clearly a very creative project in a way that I mean philosophical articles are still creative but

06:18.920 --> 06:25.560
it's more of an intellectual creativity than an artistic creativity. Yeah some people maybe think

06:25.560 --> 06:31.000
they should have been a sort of more tough minded editor hovering over me to kind of

06:31.880 --> 06:38.920
but it actually does serve a purpose well for a start it's less a book about conclusions than

06:38.920 --> 06:45.720
it is about questions and exploration and I think this format with multiple different characters

06:45.720 --> 06:51.080
and voices helps you explore several different sides giving each one their due and letting them

06:51.080 --> 06:59.160
speak for themselves different values. So that's one reason for I think the form kind of works

06:59.160 --> 07:05.640
for the content the other is that the goal of the book is well in part to try to give the reader

07:05.640 --> 07:10.680
various concepts and considerations and such that they can you know better think about these things

07:10.680 --> 07:17.480
but another part is to try to put somebody in the right frame of mind for confronting these

07:17.480 --> 07:23.160
if you imagine that will actually perhaps at some point be some group of people who will have to

07:23.160 --> 07:28.200
form some opinions about what we want long term from these AI developments you know whether it's

07:28.200 --> 07:33.720
a few people in some lab or a government or some more humanity-wide deliberation process

07:34.120 --> 07:39.720
these are really difficult questions to deliberate about and I would like people who go into that to

07:39.720 --> 07:45.800
come at it with a certain attitude so a kind of broad-minded generosity combined with some

07:45.800 --> 07:53.880
playfulness and thoughtfulness and open-mindedness which I'm hoping that the book encourages and

07:53.880 --> 08:00.840
I think that those creative elements are also meant to contribute to that. That it is a long

08:01.320 --> 08:06.680
it's a long read so it is also maybe an antidote to short attention spans issue.

08:08.440 --> 08:16.120
Well before we move on to some of the substantive issues just a couple of comments it really was

08:16.680 --> 08:23.080
a great way of delivering a number of concepts to the reader I mean before reading this book

08:23.080 --> 08:29.000
I just thought of utopia as kind of a blanket concept but now I realize and we'll get into it

08:29.000 --> 08:36.360
there are various different types of utopias each of which have their own problems and considerations

08:36.360 --> 08:47.960
to keep in mind and then you said that this book tackles very deep problems about meaning and it's

08:47.960 --> 08:55.320
also I think very useful as a thought experiment for testing an extended thought experiment for

08:55.320 --> 09:00.840
testing our intuitions about meaning in the present so it's not just like it's useful for these

09:01.400 --> 09:06.200
future utopian scenarios it's very useful for thinking about our lives today for instance

09:06.200 --> 09:12.280
there's a passage on shopping that resonated with me very much as somebody who enjoys shopping but

09:12.840 --> 09:19.000
returning to the substance two things that you mentioned right at the outset that I think we

09:19.000 --> 09:27.800
should pin down for our listeners who might not have read superintelligence are one what

09:27.800 --> 09:34.280
superintelligence is since that's vital for the understanding of a deep utopia and then also

09:34.280 --> 09:41.400
what this alignment problem is because even though we will presuppose that it has been solved for a

09:41.400 --> 09:47.240
lot of this conversation I think it's still important to understand what it is. Yeah well so

09:47.240 --> 09:58.040
superintelligence is kind of any intellectual system that radically outperforms all humans in

09:58.040 --> 10:07.560
all cognitive fields and so including you know scientific creativity you know wisdom strategy

10:07.560 --> 10:17.960
like the full range really and the the alignment problem is the technical question of how if you

10:17.960 --> 10:24.600
were if you figured out how to make an AI that's generally capable of learning and planning and

10:24.600 --> 10:30.760
reasoning how could you then steer it in some particular direction to make sure that even

10:30.760 --> 10:37.240
when it becomes much smarter than you its creator it still does what it's supposed to do like what

10:37.240 --> 10:42.760
you intended for it to do that it's kind of on our side as opposed to becoming this antagonistic

10:42.760 --> 10:47.720
force that has different goals than we tried to give it that then works at cross purposes with ours

10:48.840 --> 10:52.120
probably speaking and and there are different ways that different people have tried to make that

10:52.120 --> 11:01.800
precise but that is more or less the gist of it. So yeah this book kind of just assumes our like

11:01.880 --> 11:09.320
postulates really that all of that the alignment problem is solved and also that the like governance

11:09.320 --> 11:13.640
problems are solved to whatever extent they can be solved so that like imagine we actually

11:13.640 --> 11:18.680
don't use this powerful technology to wage war against each other or to oppress each other and

11:18.680 --> 11:24.040
that like all these things that can be solved have been solved to whatever extent possible

11:24.040 --> 11:28.680
in order then to actually get to the point where we can ask these questions about value

11:32.040 --> 11:36.360
I have of course a lot of things to say in my other writing so about these issues of getting

11:36.360 --> 11:44.920
from here to there but there is a risk that if one starts to write about those one never actually

11:44.920 --> 11:50.760
gets to consider what it's all for ultimately like we are and I think this holds in general with our

11:50.760 --> 11:57.240
existences as well we are very busy like putting one foot in front of the other and maybe don't

11:57.240 --> 12:03.560
always take the time to reflect on where we are going or and especially at like at the

12:03.560 --> 12:09.720
civilization level a lot of effort is being put into making progress like growing the economy

12:09.720 --> 12:16.280
improving the technology making things more efficient but nobody really seems to have a

12:16.280 --> 12:23.560
clear notion of you know where does this lead us to in you know in 50 years time in 500 years time

12:24.360 --> 12:29.560
but anyway that's yeah so that that's that's all kind of now I don't know whether super

12:29.560 --> 12:34.120
intelligence is strictly speaking like a premise or like it is certainly is one technology which

12:34.120 --> 12:38.600
if we had that it would unlock a lot of other technological affordances and you would more

12:38.600 --> 12:44.040
quickly approximate this condition of a solved world but you could in principle imagine getting

12:44.040 --> 12:49.560
there just through a slower accumulation of human driven automation you could maybe imagine in the

12:49.560 --> 12:55.400
limit just as you can create a little program that automates a specific task on your computer

12:56.280 --> 13:01.480
or create a robot that makes one action in the car factor or something if we just kept doing that

13:02.120 --> 13:05.560
for a hundred thousand years or a million years maybe eventually we would have software and

13:05.560 --> 13:11.480
robots that could all the tasks and you could kind of at least get close to a similar situation

13:11.480 --> 13:20.840
without super intelligence you said that so both the the dystopian and the utopian sides of the coin

13:20.840 --> 13:26.920
were always on your mind from the beginning but at the time you wrote super intelligence though it

13:26.920 --> 13:34.040
had been gestating for seven or eight years you thought it was more a more pressing problem to

13:34.040 --> 13:42.360
deal with the the alignment problem and the pitfalls of AI but what I'm wondering is so there

13:42.360 --> 13:49.320
was a practical element in writing super intelligence I don't want to get bogged down and how we get

13:49.320 --> 13:55.080
from here to super intelligence because as you mentioned that can take us very far far afield

13:55.080 --> 14:00.920
and we won't actually ever get to the utopian questions but is there also a sort of practical

14:00.920 --> 14:08.120
dimension to writing about deep utopia because you think it is near at hand and we need to

14:08.120 --> 14:13.720
start thinking about it or is it more at this point just a philosophical exercise it's more

14:13.720 --> 14:20.520
the former for me actually I mean I do think we are on a seemingly fast track towards the AI

14:20.520 --> 14:27.400
transition and so it does give the whole set of questions greater practical urgency and

14:27.400 --> 14:32.360
that might be relevant in all sorts of ways like that might for example be points at which we may

14:32.360 --> 14:38.520
need to make some some trade-offs between taking on risks and undergoing this transition sooner versus

14:39.800 --> 14:46.360
delaying it in order to hopefully like make it safer although that exposes us to other

14:47.240 --> 14:53.960
risks independent of AI but and some of this might hinge on how urgent we think it is to

14:54.440 --> 15:01.240
improve upon or get out of the current human condition into something better if you think that

15:01.240 --> 15:08.040
anything on the other side is like bad right then you might just want to preserve the status quo for

15:08.040 --> 15:12.840
as long as possible if you think there is this like very wonderful utopia on the other side then

15:12.840 --> 15:18.600
maybe you think well let's let's make sure we get there before we all just die from aging or

15:19.240 --> 15:25.240
something and it would be worse taking some risk to make sure that we can reach that point so

15:25.960 --> 15:31.080
these questions do I think flow back possibly to decisions that people have to make but the main

15:31.080 --> 15:37.080
use case would be this if people are at one point having to design that trajectory forward and maybe

15:37.080 --> 15:43.400
they with AI advice are able to anticipate where it will lead and like you know one trajectory

15:43.400 --> 15:51.320
maybe leads to some kind of hedonism maximizing outcome another leads to some other complex

15:51.320 --> 15:57.640
situation a third might lead to some kind of radically planetary sized brains optimized for

15:57.640 --> 16:03.480
you know processing scientific knowledge or something and if some people need to make some

16:03.480 --> 16:08.360
judgments about the relative desirability of these then I think it would be good if they had

16:08.360 --> 16:13.400
thought more about it then there are their sort of decisions were not merely reflecting on the

16:13.400 --> 16:19.480
latest thing they had happened to read on twitter or like how they happened to feel that day like

16:19.480 --> 16:25.080
if they wake up in a good mood they think you know it's the future is great and has a lot of

16:25.080 --> 16:29.240
opportunity if they happen to wake up depressed they kind of are a nihilist I think it would be

16:29.240 --> 16:34.280
better if the world just disappeared and ideally whatever decision is made should reflect the

16:34.280 --> 16:40.680
kind of broader set of considerations and values and perspectives than that and so

16:42.280 --> 16:48.120
yeah contributing to that and I think I guess one more thing to be said about that is that

16:49.560 --> 16:53.240
the prospect of the future gone well I think is greater if

16:56.520 --> 17:02.120
people have this sense of there being great possibilities to realize not just one value

17:02.120 --> 17:07.640
but many values that if the future goes well it unlocks this huge I mean would have this

17:08.280 --> 17:14.520
super advanced technology plentiful resources a lot of time and there would be a kind of

17:14.520 --> 17:20.760
abundance and I think that can help people approach things in a more generous way than if

17:20.760 --> 17:26.200
there is extreme scarcity and like one person's gain is another's loss and there is not enough

17:26.200 --> 17:32.120
for everybody then it's very hard to be generous right it's easier to be generous if you have a

17:32.120 --> 17:36.600
lot and so thinking ahead if we could approach the future in this more cooperative and generous

17:36.600 --> 17:42.360
way I think that also reduces the risk of various dystopian outcomes and improves the prospect so

17:44.600 --> 17:51.880
those are some thoughts in the background yeah to speak I guess a little bit proverbially though

17:51.880 --> 17:57.160
that abundance has its own problems because what is a pleasure without pain and when we

17:57.160 --> 18:03.080
have everything I mean this is a big part of of the book in your project is where we find

18:03.080 --> 18:10.280
meaning in a world where we have everything and there is no scarcity to drive purpose in right

18:10.280 --> 18:16.840
yeah so so that's like the kind of the problem then that one confronts so much of our lives currently

18:16.840 --> 18:23.320
are structured around the various practical necessities that we face in our life so you

18:23.320 --> 18:28.200
have to go into work every day because you need to get the paycheck which you need to pay the rent

18:28.920 --> 18:35.400
you have to brush your teeth because otherwise your gum will decay and you have to you have to

18:35.400 --> 18:41.240
do this stat and you have like this like so much of our lives are just like doing something in order

18:41.240 --> 18:47.800
for something else to be cost as a result right and that that that kind of fills our life with

18:47.800 --> 18:55.000
sort of goal oriented activity but in this old world a very large chunk of all of that

18:56.680 --> 19:05.000
would no longer be needed like you wouldn't have to humans wouldn't have to engage in economically

19:05.000 --> 19:09.080
productive labor because ais and robots could do everything that needed to be done

19:12.200 --> 19:19.160
um yeah you you you wouldn't have to like spend an hour at the gym working out in order

19:19.160 --> 19:23.480
to be fit and healthy because you could pop a pill that would give you the same physiological

19:24.280 --> 19:29.320
effects and you can kind of go through activity by activity and you find that for many of them

19:29.320 --> 19:33.720
you can sort of either cross them out or at least put a question mark on top of them that

19:33.720 --> 19:37.800
although you could still do them at technical maturity they would seem to be a kind of

19:37.800 --> 19:45.640
pointlessness hanging over them like a dark cloud that maybe remove some of their value and so

19:47.560 --> 19:52.120
and and this is like I think yeah part of what would make from our current vantage point

19:52.120 --> 20:00.760
some of these scenarios at least prima facie look unattractive we kind of base our self-worth

20:00.760 --> 20:05.320
and dignity on the sense of making some doing something useful like you're a breadwinner or

20:05.320 --> 20:10.280
maybe you contribute to society in some other way or you like you you look after your kids or

20:10.280 --> 20:16.440
your grandkids and you like through your efforts and strivings like you actually make the world

20:16.440 --> 20:19.560
better in some place that it could in some way that it could not otherwise be

20:20.680 --> 20:27.640
and if that's removed then yeah there is a void that the utopians would have to somehow

20:27.640 --> 20:36.840
fill in I think a lot of the fun in this project comes from your imagination and

20:37.800 --> 20:45.080
looking at all the various examples of what life might be like and the technologies that might be

20:46.040 --> 20:52.840
available in a what you refer to as a technologically mature society and I want to talk

20:52.840 --> 20:59.240
about those things but before we do I mentioned earlier that there were I'm not sure if it was

20:59.240 --> 21:06.760
five or six I think five different utopias that you discuss in the book whereas I had thought that I

21:06.760 --> 21:10.920
just only ever thought there's one kind of utopia it's just heaven and I had left that

21:10.920 --> 21:16.440
unanalyzed heaven or something like heaven where everything's perfect but what are the

21:16.680 --> 21:25.480
the different types of utopia that you think are relevant to this discussion and and how do they

21:25.480 --> 21:32.200
differ and what are some of the basic problems that they confront the utopia dweller with

21:32.920 --> 21:39.400
yeah so so this is how I like how I classify them for my purposes but you could think of

21:40.120 --> 21:48.360
first category of sort of culture and politics utopia as many of the classical works fall into

21:48.360 --> 21:55.800
this category where basically the author tries to imagine a better political system or a better

21:55.800 --> 22:00.520
sort of culture so maybe you think there would be a different system of governance or like the

22:00.520 --> 22:05.640
gender roles would be defined differently or the way that children relate to their parents

22:06.600 --> 22:10.600
um and then it like different authors are different who is about the optimal way to

22:10.600 --> 22:17.560
arrange this and the optimal way would that be a utopia um now a slightly more I guess radical or

22:17.560 --> 22:23.400
like at least in one dimension more radical uh conception of utopia would be uh an abundance

22:23.400 --> 22:32.840
utopia where you imagine that there is plenty to go around so all material needs are abundantly

22:32.840 --> 22:42.440
supplied um and you do also find some of these um in the historical records like there was a

22:43.320 --> 22:51.480
a kind of medieval fantasy of the the land of kokai that was a kind of peasant vision of like

22:51.480 --> 22:58.200
the ideal life the rivers would flow with a wine and roasted turkeys would kind of drop down

22:58.840 --> 23:05.400
on the plate and that would be music and dancing and a lot of time for relaxing and and you can see

23:05.400 --> 23:11.000
if you were like a kind of agricultural labor like doing backbreaking work from morn to dusk

23:11.000 --> 23:16.280
and then just to have barely enough porridge to eat like this would already be like a fantastic

23:16.280 --> 23:21.000
conception right like you could eat as much as you want you could just stuff yourself with food

23:21.000 --> 23:30.920
and rest all day and like that so so that would be kind of an abundance utopia um and then then

23:30.920 --> 23:36.360
we have a kind of post-work utopia which is different from abundance utopia in that not

23:36.360 --> 23:45.240
only is there plenty of everything but humans don't need to work to produce it um and this is

23:45.240 --> 23:50.040
about as far as most mainstream conversation has gone in the context of AI you do find some

23:50.120 --> 23:56.520
economists who are starting to think about um automation and whether it could cause unemployment

23:58.040 --> 24:02.360
and if so what would happen like you know you maybe need some universal basic income etc

24:04.920 --> 24:13.160
and um like like in in Marx's utopia he doesn't fight very much about it but it seems he imagines

24:13.160 --> 24:20.760
that would be a sort of um certainly a culture's politics utopia according to his conception and

24:20.760 --> 24:26.760
then that would be like less work and more abundance but that would still be people would

24:26.760 --> 24:31.160
still be working so it's kind of but i think there are levels beyond that that you could consider

24:31.160 --> 24:39.160
which is um much more radical and poses much more deep problems uh in terms of human values so

24:39.880 --> 24:46.760
we have a post-instrumental utopia where it is not just the need for human economic labor

24:47.400 --> 24:53.880
that is rendered odious by technological progress but the need for all human labor that is

24:54.760 --> 25:00.760
instrumental in character so i mean i mentioned like you might have to labor at the gym to be fit

25:00.760 --> 25:09.400
but that you wouldn't have to do in utopia um right now if you want to understand mathematics you

25:09.400 --> 25:15.560
have to first study mathematics and spend hours working on exercise problems and like really

25:15.560 --> 25:21.800
exerting yourself right but in this condition of technical maturity there would be shortcuts to

25:21.800 --> 25:27.480
the same end you could imagine some swarm of nanobots infiltrating your brain and rewiring

25:27.480 --> 25:32.600
your synapses into a condition where you now possess mathematical knowledge without you having to

25:32.600 --> 25:42.760
put out any effort um so that kind of you know pulls the rug out of a wider range of human activity

25:42.760 --> 25:47.800
and exertion not just like the job that you go and do for eight hours a day but most of what fills

25:47.800 --> 25:54.680
the rest of your time um and then there's like a final stage beyond that which is i call a plastic

25:54.760 --> 26:01.000
utopia which has all the attributes of a post instrumental utopia but in addition um the human

26:01.000 --> 26:09.240
organism uh itself becomes malleable uh and subject to our wishes and desires so you could

26:09.240 --> 26:14.440
sort of choose what emotions you want to have or what thoughts you want to have or what moral

26:14.440 --> 26:19.960
character you want to have or what like physiology you have technologies to kind of reshape yourself

26:20.600 --> 26:28.440
uh at will um including your psychological states and and that then results in this kind of

26:29.240 --> 26:35.240
solve they're almost this old world where we're like all the kind of hard limitations that we

26:35.240 --> 26:41.800
currently face like are removed it seems and and the only thing that remains are kind of our values

26:41.800 --> 26:47.240
that i think have been shaped under this condition of scarcity which has like always been there for

26:47.960 --> 26:53.960
for humans throughout history like and um it's almost like formed an exoskeleton these practical

26:53.960 --> 26:59.800
needs uh and if you were to remove all of those practical necessities there is like the question

26:59.800 --> 27:06.760
of what happens to the this soft squishy parts like do they just become a blob like a drug doubt

27:06.760 --> 27:16.280
um a kind of pleasure blob uh or can they take some more interesting shape um and um yeah so that's

27:16.520 --> 27:20.440
so that's like the the main that there are a couple of early chapters that talk about

27:20.440 --> 27:24.520
some of these poor economic and technological but then the bulk of the issue is literally set in

27:24.520 --> 27:32.200
this condition of solve it as a solved world at this plastic and post instrumental i'd like to

27:32.760 --> 27:39.400
digress for a moment to this mathematics case that you brought up which i i find particularly

27:39.480 --> 27:48.440
compelling myself especially because it's an example of something i i said earlier where

27:48.440 --> 27:56.920
and you're examining what a mathematician might do in let's say a post instrumental utopia might

27:56.920 --> 28:03.560
tell us something about what mathematicians do and value today that might not readily be apparent

28:03.560 --> 28:10.280
so in this post instrumental world where we have extremely powerful

28:11.080 --> 28:16.520
automated theorem provers that might enumerate all the theorems of math and all the

28:17.400 --> 28:26.040
interesting axiomatic systems and there's no longer a role for mathematicians to be

28:26.040 --> 28:31.960
providing new proofs do you think mathematicians would still be around because i mean this is

28:31.960 --> 28:40.600
something that isn't this is one element of utopia that might not be that far out unlike

28:40.600 --> 28:48.200
some other ones because we already have automated theorem provers that are doing work that mathematicians

28:48.200 --> 28:55.640
can't do yeah so i mean people do i mean people play chess even though we have computers that can

28:55.640 --> 29:01.240
play much better chess or solve crossword puzzles even though there is no need for

29:01.240 --> 29:08.040
crossword puzzles to be solved and do a lot of other things currently right so the first answer

29:08.040 --> 29:14.600
one might think is yeah yeah sure we'll just do it anyway because it's fun now i think we can't

29:15.880 --> 29:21.000
stop at that point we need to like unpack this idea that we do it because it's fun so that could

29:21.000 --> 29:26.440
mean we do it because it gives us pleasure like somebody who is fascinated with mathematics

29:27.080 --> 29:33.400
probably derive pleasure from engaging in mathematical thinking and occasionally maybe

29:33.400 --> 29:41.160
finding a solution but if that's the only reason there would be again shortcuts to attaining pleasure

29:42.280 --> 29:47.160
in a solved world you could have more direct forms of brain manipulation that would stimulate

29:47.240 --> 29:52.440
your pleasure centers i mean you could imagine it as some sort of a super duper drug with outside

29:52.440 --> 29:58.360
effects and addiction potential or more likely some direct way of manipulating the relevant

29:58.360 --> 30:02.920
neural structures you know maybe we're all digital at that point anyway and you just

30:04.200 --> 30:10.760
and so yeah if all you wanted was to experience positive affect that there would be no need to

30:11.320 --> 30:18.680
do mathematics for that purpose so if you do nevertheless think that it would be better to

30:18.680 --> 30:24.360
do mathematics you then it seems value something other than just this hedonic state that you now

30:25.240 --> 30:31.160
very imperfectly and grudgingly like maybe like some little drips of pleasure is derived every

30:31.160 --> 30:35.640
once in a while when somebody who is enjoying mathematics does mathematics and there's probably

30:35.720 --> 30:45.320
just a lot of sort of boring futile effort and like uncomfortable straining to get to those occasional

30:45.320 --> 30:51.320
little drips of reward that's the stingy way our current reward systems are architected right

30:52.200 --> 30:56.840
so you could kind of open the floodgates to that and then have a kind of more blissed out

30:57.880 --> 31:05.000
psychological condition and so that's easy to dismiss a lot of people like immediately think

31:05.000 --> 31:11.000
that wow that's like what a horrific view of the future that is we're all kind of like junkies

31:11.000 --> 31:16.920
sprayed on some sort of flea-infested mattress but with like a super drug dripping into our nucleus

31:16.920 --> 31:26.280
incumbents and you know it might well be that ultimately we want and can have more than that

31:26.280 --> 31:30.440
but I it is actually a rather deep question whether we shouldn't dismiss too quickly the

31:31.320 --> 31:39.480
the super bliss as an element of what we would ultimately choose and have reason to choose

31:39.480 --> 31:47.880
perhaps nevertheless I think we can have that and have a whole bunch of other things on top of that

31:47.880 --> 31:56.040
that may be satisfy other possible value candidates so certainly if you do think that it is good to

31:56.760 --> 32:02.120
engage in certain types of activity you could do that as well there is no reason you could have

32:02.120 --> 32:08.680
complex experiences pleasure plus complex experiences plus various forms of goal-directed

32:08.680 --> 32:13.160
activity but maybe we're you know and we could talk more about why you would be adopting various

32:13.160 --> 32:21.080
goals if if you were in this post-instrumental situation but and and then perhaps some additional

32:21.160 --> 32:27.160
elements even beyond that that you could kind of reconstitute some of the prerequisites for

32:27.160 --> 32:34.680
instantiating human values even in this solved world condition I think what what motivated my

32:34.680 --> 32:41.960
asking this question was reflecting on two conversations I had in the past on the show

32:41.960 --> 32:49.160
one was with a renowned number theorist at Columbia University named Michael Harris that

32:49.160 --> 32:54.520
we had a couple of years ago so it's a little faint in my mind and another is with Steven Wolfram

32:55.240 --> 33:04.360
and we were talking about math in a world in which all of math is essentially automatable and

33:05.400 --> 33:12.440
today I think our folk view of what mathematicians are in the business of is just producing new proofs

33:13.080 --> 33:20.200
discovering or inventing depending on your philosophical view more facts to be added to

33:20.200 --> 33:32.360
this big book of math but what Michael Harris said and what I think Steven Wolfram agreed with

33:33.560 --> 33:38.680
is that mathematicians aren't just in the business of producing new facts

33:38.680 --> 33:42.760
they're in the business of understanding that's what really drives what they're doing is they

33:42.760 --> 33:49.560
want to understand these structures and I was thinking that in this post-instrumental world

33:49.560 --> 33:55.640
when mathematicians no longer have to produce proofs they still would be very interested in

33:56.200 --> 34:03.400
understanding and maybe teaching mathematics but then this raises another problem that I hadn't

34:03.400 --> 34:11.960
considered yet that you discuss in some of the later or the more advanced types of utopia where

34:11.960 --> 34:18.840
we have a matrix like system of downloading information where there's no long into our

34:18.840 --> 34:25.240
brains where there's no longer any real barrier to understanding you can understand anything

34:25.240 --> 34:30.680
as soon as you just downloaded into your mind so there's no there's not really an activity of being

34:30.680 --> 34:35.400
a mathematician anymore just if you want to understand the math then you press a button

34:35.400 --> 34:42.120
then you understand the math yeah so this is a very like advanced level of technology and it's

34:42.120 --> 34:48.440
like a little unclear exactly how close to this we could get but certainly if you imagine a human

34:48.440 --> 34:53.960
upload like a digitized version of a human mind and then you have on the outside some machine super

34:53.960 --> 35:01.560
intelligence that is able to like understand how the neural network that is you sometimes

35:02.360 --> 35:08.760
successfully can grasp mathematical concepts and in other cases fail and what variations of your

35:08.760 --> 35:14.200
neural network would be required like what edits to make it as similar as possible to you now but

35:14.200 --> 35:19.880
with this extra mathematical understanding I think it possibly would be the case that the

35:19.880 --> 35:26.840
job of working out how to adjust the various parameters in your brain to give you this mathematical

35:26.840 --> 35:32.280
knowledge could be outsourced to this machine super intelligence that is looking at a model of

35:32.280 --> 35:39.800
your brain and probably be able to do that without actually running a detailed simulation of your

35:39.800 --> 35:44.280
brain which if it if that were the only way for the super intelligence to work it out you might

35:44.280 --> 35:49.080
then think well you would actually then be instantiated in the simulation and you would have to put

35:49.080 --> 35:54.840
in the effort although the simulated version of you would do but I think probably you would be able

35:54.840 --> 36:03.240
to move up levels of abstraction to more or less obviate that need and so that you would then be

36:03.240 --> 36:10.840
able to yeah in effect download skills or knowledge without having to put in time studying

36:13.400 --> 36:17.400
and so yeah I think I mean and the teaching of mathematics is even more obvious I mean I think

36:17.400 --> 36:21.560
we are very close now to having tutorial systems that would be much better than most

36:22.200 --> 36:26.680
like high school mathematics I mean aside from keeping the class disciplined and making sure

36:26.680 --> 36:32.280
everybody sits at their desk or whatever like but the actual instruction I'd imagine maybe even

36:32.280 --> 36:39.640
chateau pt4 would like be a better explainer than than than most high school teachers because it can

36:40.680 --> 36:45.640
keep track of exactly where you are where you get stuck on a particular problem and sort of

36:45.640 --> 36:52.760
customize its explanation for you as opposed to doing something generic for the class of 30

36:55.880 --> 37:03.000
just curious are you drawing the sort of distinction between chateau pt4 or gbt4 being

37:03.000 --> 37:11.400
a good high school teacher and a good university level professor because maybe there is insufficient

37:11.480 --> 37:17.640
training data on university level material for chateau pt4 to be able to explain it

37:17.640 --> 37:24.440
better than a university professor could I mean I so I don't know exactly I'd imagine there need

37:24.440 --> 37:31.080
to be some fine tuning maybe to adapt these large language models for specific use cases in the

37:31.080 --> 37:38.200
classroom I'd imagine the standard university courses like you know first and second calculus

37:38.200 --> 37:45.000
and like all of that stuff probably it would be able to explain Adam Madden as well as a mathematics

37:45.000 --> 37:49.240
professor I think maybe once you get to the research level of mathematics whether it's more

37:49.240 --> 37:55.240
matter of taste and then I'm not sure the current language models are quite yet there yet whether

37:55.240 --> 38:00.520
it would be as good as like a decent mathematics professor in terms of figuring out whether a

38:00.520 --> 38:07.400
research direction is is is promising or whether some proposal for say a phd is like

38:08.280 --> 38:12.760
the right level of difficulty that the student could possibly do it in four years and

38:13.800 --> 38:17.560
like like I think there there is like well there's obviously less training data written up

38:18.120 --> 38:24.440
for ingestion and and also it involves the the most high level human faculties that haven't yet

38:24.440 --> 38:32.120
perhaps quite been attained by the current generation of systems a term of art that you

38:33.000 --> 38:40.040
introduce in the book that I referenced earlier is technological maturity and

38:41.320 --> 38:44.920
I also mentioned that this is one of the the very fun things for me

38:44.920 --> 38:50.120
of reading deep utopia because we get to see your imagination at work and all the possible

38:50.120 --> 38:58.440
things that a technological mature society might have at its fingertips so to speak

38:58.440 --> 39:05.800
so how do you define a technologically mature society and what are some of the things that

39:06.520 --> 39:15.000
people might expect to be able to do or have on offer yeah so it's like a condition at which

39:18.200 --> 39:25.800
all those technological affordances that are consistent with the loss of physics

39:25.800 --> 39:32.920
and for which there is some possible trajectory from where we are now to their development exists

39:34.680 --> 39:41.960
so yeah it might be that you can't ever strictly speaking attain perfect technological maturity

39:41.960 --> 39:46.840
but in reality you might get some kind of close approximation to it if you have developed all

39:47.400 --> 39:53.800
the most useful general purpose technologies is perfect technical technological maturity being

39:53.800 --> 39:59.320
able to do anything that's physically possible well I would also have this pathway there but it

39:59.320 --> 40:04.680
might just be that if you imagine there are kind of an unlimited number of more and more

40:04.680 --> 40:11.880
specialized technologies for doing specific things it might be possible to develop any

40:12.520 --> 40:18.760
like small subset of them by like investing your r&d resources but if there is like quadrillion

40:18.760 --> 40:23.480
and quadrillion of different very specialized technologies maybe you can't develop the whole

40:23.480 --> 40:29.800
set of them but I think that would be sort of ways of getting close to that for most practical

40:29.800 --> 40:34.840
purposes like once you have superintelligence and nanotechnology you can sort of come up with

40:35.560 --> 40:39.640
the superintelligence can make arbitrary designs and the nanotech can sort of put it together

40:40.440 --> 40:43.880
and then like some other things you can I think get pretty close to that

40:44.760 --> 40:51.320
um so and and I think we know already at least some lower bounds of what this

40:51.320 --> 40:57.720
such a condition of technical maturity would involve that is technologies that we can already

40:57.720 --> 41:03.960
have good reason to think are physically possible and for which there is a pathway such that we could

41:03.960 --> 41:09.000
at least in the fullness of time under favorable condition get there so superintelligence is one

41:09.000 --> 41:17.800
of these like I think perfect virtual reality of the sort that would be indistinguishable to the

41:17.800 --> 41:26.760
person inside the physical like virtual reality from from ordinary basic reality would be another

41:26.760 --> 41:34.680
affordance I think like cures for aging uploading into computers space colonization at large

41:34.680 --> 41:41.160
intergalactic scales perfect newer technology in the sense of being able to control

41:42.120 --> 41:50.200
precisely our hedonic and emotional states cognitive augmentations like many other things

41:50.200 --> 41:57.000
as well but at least that already gives you a huge set of possible like things you can do right

41:57.000 --> 42:03.880
that's technological maturity one of the so the way that for our listeners who haven't

42:03.880 --> 42:08.840
looked at the book yet the book is organized so it's kind of like a a course you're teaching to

42:08.840 --> 42:15.400
these imaginary students and at various points in the book there are handouts where you've

42:15.400 --> 42:22.200
summarized some of the material and I have a handout number two some capabilities at technological

42:22.200 --> 42:30.680
maturity on the screen before me and I was just glancing at them and one that jumped out at me

42:30.680 --> 42:39.160
is the possibility of Dyson spheres for harvesting the energy output of stars and this comes to mind

42:39.160 --> 42:49.960
for two reasons one is well I'll just focus on one but the reason is I've done a number of episodes

42:49.960 --> 42:55.400
on string theory and been thinking a lot about string theory lately and one of the big barriers

42:55.400 --> 43:02.120
to empirical confirmation of string theory is that we cannot generate enough energy to probe

43:02.120 --> 43:08.040
sufficiently small distances to confirm the existence of strings and people have conjectured

43:08.040 --> 43:15.800
that the sort of energy you would need is the energy of a star so it looks like we'll be able

43:15.800 --> 43:21.960
to confirm or disconfirm string theory in a very useful technology the Dyson sphere

43:22.680 --> 43:28.120
but yeah like you mentioned arbitrary sensory inputs reversal of aging

43:29.480 --> 43:33.880
uploading of biological brains into computers and all of these things

43:36.680 --> 43:43.240
engender their own host of philosophical questions to be asked and answered aligned police

43:43.240 --> 43:49.480
bots and automatic treaty enforcement I mean and police bots are already a question being asked

43:49.480 --> 43:55.240
today but you know reversal of aging and cures for all diseases that's one that

43:55.880 --> 44:03.160
jumps out at me or a couple that jump out at me right now are there any particular problems

44:03.160 --> 44:11.320
that would need to be dealt with in these later stage utopias if we just lived forever

44:12.200 --> 44:20.280
well I mean the most obvious one would be the size of the population which is able to increase

44:20.280 --> 44:30.680
exponentially and if nobody dies then at some point the rate of births would have to match

44:31.400 --> 44:38.120
the rate of acquisition of new resources in order to maintain high per capita incomes

44:39.080 --> 44:45.000
because at technological maturity there wouldn't be increases in productivity like that's one of

44:45.000 --> 44:48.840
the things that drive technological growth now right even if the resources are the same you

44:48.840 --> 44:52.840
could have better technologies to make more efficient use of them but that will have maxed

44:52.840 --> 44:58.600
out and so then the only way that the economy could grow at that point is if more resources

44:58.600 --> 45:06.520
are attained through expanding in space but that can at most happen at a polynomial rate

45:07.160 --> 45:13.080
like you have a bubble that is growing in all directions at some significant fraction of the

45:13.080 --> 45:19.080
speed of light and the volume of that grows as a polynomial of time whereas the population

45:19.640 --> 45:26.600
like could grow exponentially right and so then eventually you would have a Malthusian condition

45:28.360 --> 45:35.800
and so in the limit you would need to sort of make sure the the rate of population increase

45:36.600 --> 45:41.400
which might be sort of digital minds making copies of themselves right but would kind of at

45:41.400 --> 45:49.080
most match the rate at which the the resource endowment increases like another maybe more

45:49.080 --> 45:55.160
philosophically interesting question if we eliminated aging and then I mean right now what

45:55.160 --> 46:01.000
would happen if you eliminated aging is that people would die from accidents and wars but maybe

46:01.000 --> 46:06.840
with a lifespan of a few thousand years if you kind of calculate the rate of death by accident

46:08.680 --> 46:15.400
now hopefully we will bring down the rate of accidents and and war certainly in utopia you'd

46:15.400 --> 46:20.360
imagine that to go way down and if you could upload yourself to computers then you could make backup

46:20.360 --> 46:26.040
copies and stuff and then the rate of accidents could kind of become arbitrarily small so then

46:26.040 --> 46:36.920
then you might have very long lifespans even astronomical and and that presents more interesting

46:36.920 --> 46:43.240
philosophical questions for human values in that well first of all we don't know what would happen

46:43.240 --> 46:49.080
to like if you remained a human mind of like a brain with your current size and the number of

46:49.080 --> 46:52.920
synapses and stuff like we know you can continue to learn and develop and grow for like a hundred

46:52.920 --> 46:57.400
years but we don't even know what would happen if a human just kept living for 400 years like would

46:57.400 --> 47:08.680
you go stale and rigid and like so and even if that didn't happen in a sort of like neurological

47:08.680 --> 47:19.640
way like then it might still be that and it's a hard question to sort of gauge but that the

47:19.640 --> 47:25.160
number of different types of things that can be done with a human mind and body in a human world

47:26.280 --> 47:31.640
is finite and is that number small enough that it would become relevant so that at some point you

47:31.640 --> 47:37.880
would just run out of interesting new things to do and if so how long like I mean I think

47:37.880 --> 47:42.280
given that we are finite there has to be some number of years after which you would literally

47:42.280 --> 47:48.920
be doing the same thing you'd already done but if that is kind of 10 to the power of a hundred

47:49.000 --> 47:53.720
or something we might not have to care very much because we'd die anyway before that from the

47:53.720 --> 47:59.960
heat death of the universe but if the answer is like 10,000 years like after 10,000 years you'd

47:59.960 --> 48:06.360
basically just repeat yourself then either you'd have to accept that such lives will have a diminution

48:06.360 --> 48:11.800
of a certain kind of novelty and that may be a price to admittance and this kind of extreme

48:11.800 --> 48:18.600
longevity in utopia or you would have to perhaps start to level up after you have spent at 10,000

48:18.600 --> 48:24.200
years being a human maybe then you would want to become a transhuman like increase your intellectual

48:24.200 --> 48:29.480
faculties or something and you could kind of start to tackle the next level of challenges and

48:31.000 --> 48:39.240
keep developing that way but it is I think it is one direction in which you can kind of

48:40.440 --> 48:45.800
take our current human values and where they start to become strained if you just imagine the

48:45.800 --> 48:50.760
current human lifespan with our current values and our current faculties but just extended

48:50.760 --> 48:55.480
and like prevented from physical disease like at some point I think that would

48:58.600 --> 49:02.280
possibly become unattractive like there are still a lot of things in human life that don't

49:02.280 --> 49:08.920
require novelty I mean like a cup of tea is like about as good like the 10,000s time as it is the

49:08.920 --> 49:14.760
second time or first time right it they're like renewable pleasures and sometimes you could run

49:14.760 --> 49:18.600
the argument that sometimes the best things in life are the little things that it's not like

49:19.480 --> 49:27.240
like the big dramatic narrative climaxes but it's like you know the smell of autumn air in

49:27.240 --> 49:33.800
the evening along a stroll along the coast or like looking into your loved one's face or a little

49:34.360 --> 49:40.760
you know cup of tea or whatever like these little things sometimes you know you could run the argument

49:40.840 --> 49:47.000
that that's a fairly high quality value although we give them kind of short shrift

49:49.080 --> 49:53.160
an interesting connection to the philosophical literature going back

49:53.960 --> 50:01.880
hundreds of years I mean is how conceptions of personal identity might shift in this world

50:01.880 --> 50:09.240
where we live for we live indefinitely I think and you should correct me if I'm wrong but I think

50:09.240 --> 50:16.280
it was Locke who proposed that personal identity has something to do with memory and psychological

50:16.280 --> 50:26.760
connectedness and maybe it was Derek Parfit in his sort of ongoing discourse with David Lewis who

50:27.640 --> 50:34.360
questioned whether somebody who lived who wasn't immortal would eventually no longer be the same

50:34.360 --> 50:41.400
person that they started out as because they would not remember let's say their childhood

50:41.400 --> 50:47.800
F there would no longer be any psychological connection to a person stage thousands and

50:47.800 --> 50:55.160
thousands of years earlier and in this sort of utopia maybe we would bypass this by getting

50:56.280 --> 51:01.800
memory implants or memory chip implants so that we could just continually

51:02.760 --> 51:09.160
add new memories and maintain internal psychological connectedness yeah um

51:10.120 --> 51:17.320
I mean I guess there is first of all like I would want to make retain the distinction between

51:17.320 --> 51:24.760
long time and infinity like immortality I think you should mean not dying not not just taking

51:24.760 --> 51:30.840
a while before you die like conceptually and it is important because immortality in that strict

51:30.840 --> 51:37.400
sense just might be impossible in our physical universe and be the preserve of say more theological

51:37.400 --> 51:43.880
scenarios and from the blink view of eternity like whether you live for like 80 years or 80

51:43.880 --> 51:50.040
thousand years it's kind of in some sense the same right it's an infinitesimal short relative to

51:50.040 --> 52:02.920
infinity either way now I do think that yes increasing I call it time time suits but the

52:02.920 --> 52:10.440
idea that we could make some modifications tasks that make us better able to survive radical change

52:10.440 --> 52:17.000
without having our identity eroded or our values corrupted so the most obvious thing would be to

52:17.000 --> 52:22.680
improve our episodic memory so that you actually don't just forget what you've been through and

52:22.680 --> 52:29.800
that could make you sort of remain relevantly the same for longer than if you just have like the

52:29.800 --> 52:35.480
memory of a goldfish and and you could imagine some other tricks as well that would allow us to

52:35.480 --> 52:41.400
sort of endure for longer periods of time and and upgrading our capacities in different ways could

52:41.480 --> 52:51.400
help with that um but even if it came to a point where you would have to accept some loss of personal

52:51.400 --> 52:56.840
continuity in in whatever relevant sense I mean it might be acceptable like if you if you were

52:56.840 --> 53:01.480
have a five-year-old and you know that if there were a pill that would just arrest their development

53:01.480 --> 53:04.760
that they would remain a five-year-old like in some sense maybe a year from now that would be

53:04.760 --> 53:10.760
more similar to what they're now than if they're just allowed to freely develop uh but we might

53:10.760 --> 53:16.520
still think it's better for them to grow up like in some sense you are the same person now as you

53:16.520 --> 53:23.880
were when you were five but in realistically you're also quite different right in many ways uh and uh

53:23.880 --> 53:32.200
and maybe that's not an unambiguously bad thing like and so I think the combination of these that um

53:33.160 --> 53:43.560
a uh we can reduce the negative effects of various kinds of uh corroding uh consequences of the passage

53:43.560 --> 53:50.360
of time through improving memory and other certain other positive attributes like also including like

53:50.360 --> 53:57.560
maybe extending your planning horizon etc and and then also accepting some degree of of change

53:57.560 --> 54:03.320
even if it does mean that you eventually move further away from what you once were if there's

54:03.320 --> 54:07.640
still a difference between that and just dying because at any given point in time you might

54:07.640 --> 54:13.240
look forward to hundreds of years of existing in some very similar things your current condition and

54:13.240 --> 54:18.440
and you would gradually sort of you know see new ways of doing things or being or new experiences

54:19.000 --> 54:23.480
that that feels a lot more optimistic to me than it then like a kind of you're gonna be shot

54:24.440 --> 54:31.960
tomorrow morning right it's a um this the smoother and gentler way of losing our connection with the

54:31.960 --> 54:39.720
past seems perhaps prudentially preferable um even to the extent that it is unavoidable relative to

54:39.720 --> 54:47.400
yeah um the alternative okay not that a lot of this or all of it hasn't all been speculative

54:48.120 --> 54:52.600
but i have a particularly speculative question to ask so when i

54:54.280 --> 55:00.600
spoke with avi lobe the harvard astrophysicist this this brings us back to dyson spheres

55:01.160 --> 55:11.160
he conjectured that umuamua this comet that passed us by in i believe 2017 might have been

55:11.880 --> 55:23.160
part of a dyson sphere and you have made your own very uh i don't know if splashy is an adjective but

55:24.280 --> 55:31.160
powerful uh controversial conjectures about for instance whether or not we might be in a simulation

55:31.160 --> 55:39.720
and i'm wondering if you think it is possible or likely even that there are societies out there

55:39.720 --> 55:46.760
that have already reached uh for instance a post scarcity utopian stage because i think the

55:47.560 --> 55:54.120
having a dyson sphere if whoever sent uamua our way had a dyson sphere we would classify them

55:54.120 --> 56:01.400
probably as being in a post scarcity period assuming that they still have other dyson spheres

56:01.400 --> 56:06.440
yeah i mean so that i mean post scarcity they might not i mean it depends a lot on what their goals are

56:07.160 --> 56:13.000
and um it might be that the dyson sphere of resources is nowhere near enough for what they

56:13.000 --> 56:22.440
are trying to do um and so it is to some extent a human con like um it's not like a very precisely

56:22.440 --> 56:27.880
defined concept it's like you could say like from few humans so you could say that we are already

56:28.520 --> 56:35.560
uh somewhat close in some respects well if you're if you are like fortunate enough to live in a in a

56:35.560 --> 56:41.640
in a rich country and you have a i don't know like you're healthy and trying to good education like

56:41.640 --> 56:50.440
probably most of your listeners like it's probably the case that um uh if if you wanted to you wouldn't

56:50.440 --> 56:56.120
really have to work to survive like or if you select say say you worked hard really hard for

56:56.120 --> 57:00.760
five or ten years and saved up all the money you probably could then move to thailand or something

57:00.760 --> 57:06.920
and buy a small hot near a beach and then have enough to eat and be physically healthy and you'd

57:06.920 --> 57:10.920
have a computer you could eat access the internet or whatever for the rest of your life without

57:10.920 --> 57:17.400
having to work like it's at least tantalizingly close uh but yeah people choose to continue to

57:17.400 --> 57:24.280
strive because they want more than just the basic necessities of life right like and in particular

57:24.280 --> 57:29.400
we want to have more than other people and so there's a lot of this kind of um zero some

57:30.040 --> 57:38.600
status consumption going on with humans um and that could drive scarcity up to astronomical

57:38.600 --> 57:45.400
levels right like you yeah you have your own dyson sphere but uh like i have four dyson spheres so

57:45.400 --> 57:52.040
like you still need to try to um catch up and and and intergalactic civilization might have

57:52.040 --> 57:56.600
practical reasons as well like maybe they fear some other intergalactic civilization that might

57:56.600 --> 58:04.280
have a larger fleet of worships or something and so um but the difference is like um even if human

58:05.000 --> 58:12.680
desires are insatiable it doesn't mean there will always be a need for human work like you

58:12.680 --> 58:20.520
could reach a condition where even though like you maybe are worth a trillion dollars and you

58:20.520 --> 58:25.000
would like to make another trillion dollars there's just nothing you can do with your own labor that

58:25.000 --> 58:29.800
would make you any significant amount of money because all the work is better done by machine

58:31.640 --> 58:36.520
and if the most you could make is the minimum wage then once you're a trillion year like

58:36.520 --> 58:40.360
there is no point really working right because it's trivial compared to what you just earned from

58:40.360 --> 58:44.680
your capital and and it might even be that by doing the work yourself you like expend more

58:44.680 --> 58:50.200
calories that cost you more than the actual value of the labor so you could still end up in this kind

58:50.200 --> 58:58.200
of condition of um uh unemployment or post work even if there are still needs that have or desires

58:58.200 --> 59:03.720
that haven't been fully uh satisfied um now you asked whether I think there are already some that

59:03.720 --> 59:10.280
have attained either poor scarcity or I guess you could generalize it into technological maturity

59:10.280 --> 59:15.800
and I think if the universe is infinite as it seems to be like if we have the simplest topology

59:15.800 --> 59:24.040
and an opener flat spacetime um then definitely that would be uh such civilizations in fact infinitely

59:24.040 --> 59:31.240
many of them out there but um if I had to guess I'd say none in our in the observable universe and so

59:31.240 --> 59:38.360
that there would be infinitely many of these but they would have low density and so um um we might

59:38.360 --> 59:43.480
be out of causal contact forever with the nearest other one um that certainly would help explain the

59:43.480 --> 59:49.560
Fermi paradox um but it's not the only possible explanation so we can't rule out that we could

59:49.560 --> 59:57.240
share the observable universe with some other civilizations as well. Speaking of intergalactic

59:57.240 --> 01:00:05.480
civilizations and intergalactic warfare obviously that would be a huge barrier to us reaching a

01:00:05.560 --> 01:00:15.560
utopian state if we had to ascend off uh an alien invasion but do you see any other barriers

01:00:15.560 --> 01:00:23.000
besides just our ability to produce a super intelligent technology or super intelligent

01:00:23.000 --> 01:00:32.520
AI other barriers beyond that to our reaching utopian states? Uh yes I mean first of all it would

01:00:32.520 --> 01:00:37.640
be uh crucially super intelligence is well aligned um because otherwise yeah that might

01:00:37.640 --> 01:00:45.480
be our undoing um and second if you have scenarios uh multipolar in character like if you have many

01:00:45.480 --> 01:00:54.120
different entities ultimately with our own AI uh assistance that then a lot of the same dynamics

01:00:54.120 --> 01:01:04.440
that uh we see on the planet today with arms races and um oppression and warfare and exploitation of

01:01:04.440 --> 01:01:11.160
the global commons by sort of spewing pollutants into the atmosphere overfishing the oceans all of

01:01:11.160 --> 01:01:17.240
that could still occur and and indeed could be intensified in this kind of hyper competitive

01:01:17.240 --> 01:01:23.640
economy that might be created um if on the other hand you don't have a multipolar but like a unipolar

01:01:23.720 --> 01:01:27.880
or as I call like a singleton scenario where it gets all consolidated then there is the obvious

01:01:27.880 --> 01:01:38.120
question of um who controls that uh singleton right and how uh benevolent and wise is whatever

01:01:38.120 --> 01:01:44.440
the mechanism whether it's like a global democracy or a dictatorship or whatever it is is like so I

01:01:44.440 --> 01:01:51.080
think we can identify several broad categories of things that need to fall in place so one is

01:01:51.080 --> 01:01:56.120
to have this kind of uh deep utopia a you would need super advanced technology

01:01:57.480 --> 01:02:02.360
like otherwise you just can't do all of the if you right now we can't cure the

01:02:02.360 --> 01:02:09.000
jit with a cancer in many cases right and that means our world will fall short of what it should be

01:02:09.880 --> 01:02:17.560
so the super advanced technology but then I think also we'd need um a fairly high level of

01:02:17.560 --> 01:02:25.000
cooperation slash good governance to prevent sort of negative some conflict and then we'd

01:02:25.000 --> 01:02:32.120
also need some adequately high level of wisdom in how we use these great uh technological powers

01:02:32.120 --> 01:02:42.120
that we have um and it might be that the wisdom is it has a kind of threshold effect where if

01:02:42.200 --> 01:02:48.040
you're above that threshold even if you're still uh unwise in many deep ways and have many

01:02:48.680 --> 01:02:53.480
erroneous beliefs and misconceptions you might have enough wisdom to realize that you are fallible

01:02:53.480 --> 01:02:59.560
and to start to seek out ways to fix you can reflect on your own shortcomings and gradually

01:02:59.560 --> 01:03:05.240
develop ways of remedying it but if you're sort of below that wisdom level you might

01:03:05.240 --> 01:03:11.560
be more likely to lock in your current prejudices uh or to shoot your foot off when you're trying to

01:03:11.560 --> 01:03:16.520
like develop ways to fix it and it's kind of actually an open question I think where humanity

01:03:16.520 --> 01:03:21.000
is relative to that because you could ask the same question at the collective level like are we

01:03:21.000 --> 01:03:24.760
I think we are maybe close to the threshold level but I'm not sure whether we are just under

01:03:24.760 --> 01:03:35.320
or just above it um and um and then we might need some bit of luck as well on top of that um but um

01:03:36.200 --> 01:03:45.720
um I think yeah although these are kind of extreme technological postulates and you might

01:03:45.720 --> 01:03:51.880
see them very wide wild ideas I still think um they might actually not be completely unrealistic

01:03:51.880 --> 01:04:00.520
even within our lifetime if this whole AI transition happens um and it's if you sort of

01:04:00.520 --> 01:04:07.480
zoom out and look at the human history like uh from from its inception like you know a few

01:04:07.480 --> 01:04:13.880
hundred thousand years ago to today and you plot that in any possible way you you might want to

01:04:13.880 --> 01:04:20.440
like whether it's like the amount of you know um you know the GDP let us say or like some sort of

01:04:20.440 --> 01:04:26.840
the energy expended by human civilization or like in any it does have this you like if you

01:04:26.840 --> 01:04:32.200
plot it on a linear scale you just see a flat line that just spikes up at the end of the graph so

01:04:32.200 --> 01:04:38.840
then you have to go to like a log plot or to even see it bending but even then it bends up and we're

01:04:38.840 --> 01:04:44.200
really sitting right now at this like ridiculous anomaly um where what we are currently taking

01:04:44.200 --> 01:04:50.840
to be the normal human condition is in any which way look at it like just extreme historical anomaly

01:04:50.840 --> 01:04:58.600
like a tiny little thin coat of paint on on like a giant uh battleship of human history right it's

01:04:58.600 --> 01:05:04.200
like it and and then now we think it's like a radical conception to think well what if that changes

01:05:04.200 --> 01:05:08.920
in the future whereas in fact if you sort of zoom out you look like we are right now in the middle

01:05:08.920 --> 01:05:15.160
of this like explosion of something and we don't know what it's going to end up like but um but I

01:05:15.160 --> 01:05:20.200
do believe as as you alluded to earlier that scenarios in which we just remain more or less

01:05:20.200 --> 01:05:25.240
like we are now and have the current human condition just keep going for like tens of thousands of

01:05:25.240 --> 01:05:31.240
years that just seems extremely improbable to me relative to either like extinction slash dystopia

01:05:31.240 --> 01:05:38.680
or some radical transformation into some kind of post-human condition well Nick I I really

01:05:38.680 --> 01:05:44.840
enjoyed deep utopia and I came out of it with so many ideas and concepts that I hadn't considered

01:05:44.840 --> 01:05:49.800
beforehand so thanks so much for writing the book and thinking about these really interesting

01:05:49.800 --> 01:05:54.520
pressing problems and thanks a lot for coming on the show to talk to me about it well thanks to you

01:05:54.520 --> 01:06:01.080
and uh to the little cat there as well

