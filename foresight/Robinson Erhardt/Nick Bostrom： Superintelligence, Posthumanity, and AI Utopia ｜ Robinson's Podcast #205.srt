1
00:00:00,000 --> 00:00:04,680
If you sort of zoom out, you look like we are right now in the middle of this like explosion of some

2
00:00:04,680 --> 00:00:10,880
sec and we don't know what it's going to end up like but but I do believe as you alluded to earlier

3
00:00:10,880 --> 00:00:15,060
that scenarios in which we just remain more or less like we are now and have the current human

4
00:00:15,060 --> 00:00:20,720
condition just keep going for like tens of thousands of years that just seems extremely improbable to

5
00:00:20,720 --> 00:00:26,880
me relative to either like extinction slash dystopia or some radical transformation into some kind of

6
00:00:26,880 --> 00:00:33,840
posthumous condition. Hello this is Robinson Earhart here with the introduction to Robinson's

7
00:00:33,840 --> 00:00:41,600
podcast number 205 and this episode is with Nick Bostrom a philosopher of artificial intelligence

8
00:00:41,600 --> 00:00:47,640
and many other subjects who was most recently professor of philosophy at Oxford University where

9
00:00:47,640 --> 00:00:55,640
he also served as the founding director of the Future of Humanity Institute. Nick is likely best

10
00:00:55,640 --> 00:01:03,320
known in the public eye for his work on the simulation argument which is the idea that we live

11
00:01:03,320 --> 00:01:11,880
not in the familiar physical world that we believe we inhabit but a simulation of one but he's also

12
00:01:11,880 --> 00:01:17,760
well known for his book Super Intelligence which covers the dangers of artificial intelligence

13
00:01:17,760 --> 00:01:24,360
and strategies for dealing with them. In this episode though Nick and I talk about his more

14
00:01:24,360 --> 00:01:31,400
recent book Deep Utopia Life and Meaning in a Solved World which just came out in March actually

15
00:01:31,400 --> 00:01:39,720
and opposed to get as opposed to getting into the existential dangers of AI which Nick tackles in

16
00:01:39,720 --> 00:01:46,440
super intelligence deep utopia considers the sorts of concerns that might arise if everything

17
00:01:46,440 --> 00:01:54,120
actually goes right with AI. So after talking about the alignment problem and some other fundamental

18
00:01:54,120 --> 00:02:00,840
issues in the philosophy of AI we talk about the problems that might come from having perfect

19
00:02:00,840 --> 00:02:08,200
technology from immortality from not having to work anymore from having robots that can do all

20
00:02:08,200 --> 00:02:16,680
your hobbies better than you can do and more. Likes, comments, subscribes, reviews as you know all of

21
00:02:16,680 --> 00:02:25,480
these things are always extremely appreciated. There is also a patreon if you would like a link

22
00:02:25,480 --> 00:02:34,040
to an ad for your rss feed or show notes and to all those of you who are patrons who are geeslings

23
00:02:34,040 --> 00:02:39,560
thank you so much for your support. But now without any further ado I hope that you enjoy

24
00:02:39,560 --> 00:02:42,280
this conversation as much as I enjoyed having in with Nick.

25
00:02:50,760 --> 00:02:56,280
You're one of the most widely recognized philosophers in the in the public sphere

26
00:02:56,840 --> 00:03:04,440
and mainly known in that capacity I think for thinking about dystopian AI scenarios scenarios

27
00:03:04,440 --> 00:03:10,680
and other existential threats so right off the bat I'm wondering what motivated this shift to

28
00:03:10,680 --> 00:03:20,040
thinking about utopias. Now both sides have always been present in my mind and my outlook. Back when

29
00:03:20,040 --> 00:03:28,040
I was writing super intelligence this 2014 book which was in the works six years prior to that I

30
00:03:28,040 --> 00:03:36,120
felt it was more urgent to focus on what could go wrong with AI figure out where the pitfalls were

31
00:03:36,120 --> 00:03:45,000
so that we could avoid them. This was a time at which the whole idea of AI posing any kind of

32
00:03:45,640 --> 00:03:51,000
existential risk or even just having any kind of transformative impact on society was still

33
00:03:51,000 --> 00:03:57,400
far outside the mainstream. The whole idea of the alignment problem was not in the

34
00:03:57,400 --> 00:04:02,200
overton window that were like maybe a handful of people scattered around the world kind of internet

35
00:04:02,200 --> 00:04:06,920
type so we're trying to work on this and it just seemed hugely neglected to me so I thought writing

36
00:04:06,920 --> 00:04:11,880
this book super intelligence trying to develop concepts that would make it possible for people

37
00:04:11,880 --> 00:04:17,160
to start to think more constructively about this and begin to do research on developing scalable

38
00:04:17,160 --> 00:04:24,120
methods for alignment was important. In the intervening years we've seen a big shift now all

39
00:04:24,120 --> 00:04:29,320
the frontier AI labs have research teams specifically trying to develop scalable methods for AI

40
00:04:29,320 --> 00:04:34,200
control and there are many other organizations as well where a lot of the smartest young people I

41
00:04:34,200 --> 00:04:40,120
know now are like flooding into this field and in the last two years we've also seen a big shift

42
00:04:41,160 --> 00:04:47,720
at the level of policy conversation where even top tier policy makers are now beginning to

43
00:04:47,720 --> 00:04:53,000
recognize the transformative impact of future AI developments with statements coming out from the

44
00:04:53,000 --> 00:04:58,120
White House and the UK hosted this global summit on AI. I was just the other week in Brussels for

45
00:04:58,120 --> 00:05:04,920
a however meeting and so to some extent that whole thing is now widely recognized and many of

46
00:05:04,920 --> 00:05:14,120
me feel a work on it. I felt the other side of what happens if things go right with AI had

47
00:05:14,120 --> 00:05:19,720
not yet really been addressed or people talk about it but often in a rather superficial way

48
00:05:20,600 --> 00:05:24,120
but once you start to dig in you realize there are actually quite

49
00:05:24,760 --> 00:05:29,240
deep even philosophical problems that come up when you imagine like what would give human life

50
00:05:29,240 --> 00:05:34,600
purpose and meaning in this condition where in a solved world as I call it like we're all practical

51
00:05:34,600 --> 00:05:39,960
problems that can be solved through progress have already been solved and so that's how the book

52
00:05:41,320 --> 00:05:45,800
it wasn't really so much a plan to write the book it more kind of happened as sometimes

53
00:05:46,440 --> 00:05:50,680
is the way of these things like you start writing a little and then it takes on a life of its own

54
00:05:50,680 --> 00:05:57,160
and you just try to hold on less the thing gradually develops. I can tell that this book

55
00:05:57,160 --> 00:06:04,600
really did take on a life of its own just because of the the various formats in which you wrote it

56
00:06:04,600 --> 00:06:10,520
and I mean there's there's poetry there's dialogue there's standard exposition so it was

57
00:06:10,520 --> 00:06:18,920
clearly a very creative project in a way that I mean philosophical articles are still creative but

58
00:06:18,920 --> 00:06:25,560
it's more of an intellectual creativity than an artistic creativity. Yeah some people maybe think

59
00:06:25,560 --> 00:06:31,000
they should have been a sort of more tough minded editor hovering over me to kind of

60
00:06:31,880 --> 00:06:38,920
but it actually does serve a purpose well for a start it's less a book about conclusions than

61
00:06:38,920 --> 00:06:45,720
it is about questions and exploration and I think this format with multiple different characters

62
00:06:45,720 --> 00:06:51,080
and voices helps you explore several different sides giving each one their due and letting them

63
00:06:51,080 --> 00:06:59,160
speak for themselves different values. So that's one reason for I think the form kind of works

64
00:06:59,160 --> 00:07:05,640
for the content the other is that the goal of the book is well in part to try to give the reader

65
00:07:05,640 --> 00:07:10,680
various concepts and considerations and such that they can you know better think about these things

66
00:07:10,680 --> 00:07:17,480
but another part is to try to put somebody in the right frame of mind for confronting these

67
00:07:17,480 --> 00:07:23,160
if you imagine that will actually perhaps at some point be some group of people who will have to

68
00:07:23,160 --> 00:07:28,200
form some opinions about what we want long term from these AI developments you know whether it's

69
00:07:28,200 --> 00:07:33,720
a few people in some lab or a government or some more humanity-wide deliberation process

70
00:07:34,120 --> 00:07:39,720
these are really difficult questions to deliberate about and I would like people who go into that to

71
00:07:39,720 --> 00:07:45,800
come at it with a certain attitude so a kind of broad-minded generosity combined with some

72
00:07:45,800 --> 00:07:53,880
playfulness and thoughtfulness and open-mindedness which I'm hoping that the book encourages and

73
00:07:53,880 --> 00:08:00,840
I think that those creative elements are also meant to contribute to that. That it is a long

74
00:08:01,320 --> 00:08:06,680
it's a long read so it is also maybe an antidote to short attention spans issue.

75
00:08:08,440 --> 00:08:16,120
Well before we move on to some of the substantive issues just a couple of comments it really was

76
00:08:16,680 --> 00:08:23,080
a great way of delivering a number of concepts to the reader I mean before reading this book

77
00:08:23,080 --> 00:08:29,000
I just thought of utopia as kind of a blanket concept but now I realize and we'll get into it

78
00:08:29,000 --> 00:08:36,360
there are various different types of utopias each of which have their own problems and considerations

79
00:08:36,360 --> 00:08:47,960
to keep in mind and then you said that this book tackles very deep problems about meaning and it's

80
00:08:47,960 --> 00:08:55,320
also I think very useful as a thought experiment for testing an extended thought experiment for

81
00:08:55,320 --> 00:09:00,840
testing our intuitions about meaning in the present so it's not just like it's useful for these

82
00:09:01,400 --> 00:09:06,200
future utopian scenarios it's very useful for thinking about our lives today for instance

83
00:09:06,200 --> 00:09:12,280
there's a passage on shopping that resonated with me very much as somebody who enjoys shopping but

84
00:09:12,840 --> 00:09:19,000
returning to the substance two things that you mentioned right at the outset that I think we

85
00:09:19,000 --> 00:09:27,800
should pin down for our listeners who might not have read superintelligence are one what

86
00:09:27,800 --> 00:09:34,280
superintelligence is since that's vital for the understanding of a deep utopia and then also

87
00:09:34,280 --> 00:09:41,400
what this alignment problem is because even though we will presuppose that it has been solved for a

88
00:09:41,400 --> 00:09:47,240
lot of this conversation I think it's still important to understand what it is. Yeah well so

89
00:09:47,240 --> 00:09:58,040
superintelligence is kind of any intellectual system that radically outperforms all humans in

90
00:09:58,040 --> 00:10:07,560
all cognitive fields and so including you know scientific creativity you know wisdom strategy

91
00:10:07,560 --> 00:10:17,960
like the full range really and the the alignment problem is the technical question of how if you

92
00:10:17,960 --> 00:10:24,600
were if you figured out how to make an AI that's generally capable of learning and planning and

93
00:10:24,600 --> 00:10:30,760
reasoning how could you then steer it in some particular direction to make sure that even

94
00:10:30,760 --> 00:10:37,240
when it becomes much smarter than you its creator it still does what it's supposed to do like what

95
00:10:37,240 --> 00:10:42,760
you intended for it to do that it's kind of on our side as opposed to becoming this antagonistic

96
00:10:42,760 --> 00:10:47,720
force that has different goals than we tried to give it that then works at cross purposes with ours

97
00:10:48,840 --> 00:10:52,120
probably speaking and and there are different ways that different people have tried to make that

98
00:10:52,120 --> 00:11:01,800
precise but that is more or less the gist of it. So yeah this book kind of just assumes our like

99
00:11:01,880 --> 00:11:09,320
postulates really that all of that the alignment problem is solved and also that the like governance

100
00:11:09,320 --> 00:11:13,640
problems are solved to whatever extent they can be solved so that like imagine we actually

101
00:11:13,640 --> 00:11:18,680
don't use this powerful technology to wage war against each other or to oppress each other and

102
00:11:18,680 --> 00:11:24,040
that like all these things that can be solved have been solved to whatever extent possible

103
00:11:24,040 --> 00:11:28,680
in order then to actually get to the point where we can ask these questions about value

104
00:11:32,040 --> 00:11:36,360
I have of course a lot of things to say in my other writing so about these issues of getting

105
00:11:36,360 --> 00:11:44,920
from here to there but there is a risk that if one starts to write about those one never actually

106
00:11:44,920 --> 00:11:50,760
gets to consider what it's all for ultimately like we are and I think this holds in general with our

107
00:11:50,760 --> 00:11:57,240
existences as well we are very busy like putting one foot in front of the other and maybe don't

108
00:11:57,240 --> 00:12:03,560
always take the time to reflect on where we are going or and especially at like at the

109
00:12:03,560 --> 00:12:09,720
civilization level a lot of effort is being put into making progress like growing the economy

110
00:12:09,720 --> 00:12:16,280
improving the technology making things more efficient but nobody really seems to have a

111
00:12:16,280 --> 00:12:23,560
clear notion of you know where does this lead us to in you know in 50 years time in 500 years time

112
00:12:24,360 --> 00:12:29,560
but anyway that's yeah so that that's that's all kind of now I don't know whether super

113
00:12:29,560 --> 00:12:34,120
intelligence is strictly speaking like a premise or like it is certainly is one technology which

114
00:12:34,120 --> 00:12:38,600
if we had that it would unlock a lot of other technological affordances and you would more

115
00:12:38,600 --> 00:12:44,040
quickly approximate this condition of a solved world but you could in principle imagine getting

116
00:12:44,040 --> 00:12:49,560
there just through a slower accumulation of human driven automation you could maybe imagine in the

117
00:12:49,560 --> 00:12:55,400
limit just as you can create a little program that automates a specific task on your computer

118
00:12:56,280 --> 00:13:01,480
or create a robot that makes one action in the car factor or something if we just kept doing that

119
00:13:02,120 --> 00:13:05,560
for a hundred thousand years or a million years maybe eventually we would have software and

120
00:13:05,560 --> 00:13:11,480
robots that could all the tasks and you could kind of at least get close to a similar situation

121
00:13:11,480 --> 00:13:20,840
without super intelligence you said that so both the the dystopian and the utopian sides of the coin

122
00:13:20,840 --> 00:13:26,920
were always on your mind from the beginning but at the time you wrote super intelligence though it

123
00:13:26,920 --> 00:13:34,040
had been gestating for seven or eight years you thought it was more a more pressing problem to

124
00:13:34,040 --> 00:13:42,360
deal with the the alignment problem and the pitfalls of AI but what I'm wondering is so there

125
00:13:42,360 --> 00:13:49,320
was a practical element in writing super intelligence I don't want to get bogged down and how we get

126
00:13:49,320 --> 00:13:55,080
from here to super intelligence because as you mentioned that can take us very far far afield

127
00:13:55,080 --> 00:14:00,920
and we won't actually ever get to the utopian questions but is there also a sort of practical

128
00:14:00,920 --> 00:14:08,120
dimension to writing about deep utopia because you think it is near at hand and we need to

129
00:14:08,120 --> 00:14:13,720
start thinking about it or is it more at this point just a philosophical exercise it's more

130
00:14:13,720 --> 00:14:20,520
the former for me actually I mean I do think we are on a seemingly fast track towards the AI

131
00:14:20,520 --> 00:14:27,400
transition and so it does give the whole set of questions greater practical urgency and

132
00:14:27,400 --> 00:14:32,360
that might be relevant in all sorts of ways like that might for example be points at which we may

133
00:14:32,360 --> 00:14:38,520
need to make some some trade-offs between taking on risks and undergoing this transition sooner versus

134
00:14:39,800 --> 00:14:46,360
delaying it in order to hopefully like make it safer although that exposes us to other

135
00:14:47,240 --> 00:14:53,960
risks independent of AI but and some of this might hinge on how urgent we think it is to

136
00:14:54,440 --> 00:15:01,240
improve upon or get out of the current human condition into something better if you think that

137
00:15:01,240 --> 00:15:08,040
anything on the other side is like bad right then you might just want to preserve the status quo for

138
00:15:08,040 --> 00:15:12,840
as long as possible if you think there is this like very wonderful utopia on the other side then

139
00:15:12,840 --> 00:15:18,600
maybe you think well let's let's make sure we get there before we all just die from aging or

140
00:15:19,240 --> 00:15:25,240
something and it would be worse taking some risk to make sure that we can reach that point so

141
00:15:25,960 --> 00:15:31,080
these questions do I think flow back possibly to decisions that people have to make but the main

142
00:15:31,080 --> 00:15:37,080
use case would be this if people are at one point having to design that trajectory forward and maybe

143
00:15:37,080 --> 00:15:43,400
they with AI advice are able to anticipate where it will lead and like you know one trajectory

144
00:15:43,400 --> 00:15:51,320
maybe leads to some kind of hedonism maximizing outcome another leads to some other complex

145
00:15:51,320 --> 00:15:57,640
situation a third might lead to some kind of radically planetary sized brains optimized for

146
00:15:57,640 --> 00:16:03,480
you know processing scientific knowledge or something and if some people need to make some

147
00:16:03,480 --> 00:16:08,360
judgments about the relative desirability of these then I think it would be good if they had

148
00:16:08,360 --> 00:16:13,400
thought more about it then there are their sort of decisions were not merely reflecting on the

149
00:16:13,400 --> 00:16:19,480
latest thing they had happened to read on twitter or like how they happened to feel that day like

150
00:16:19,480 --> 00:16:25,080
if they wake up in a good mood they think you know it's the future is great and has a lot of

151
00:16:25,080 --> 00:16:29,240
opportunity if they happen to wake up depressed they kind of are a nihilist I think it would be

152
00:16:29,240 --> 00:16:34,280
better if the world just disappeared and ideally whatever decision is made should reflect the

153
00:16:34,280 --> 00:16:40,680
kind of broader set of considerations and values and perspectives than that and so

154
00:16:42,280 --> 00:16:48,120
yeah contributing to that and I think I guess one more thing to be said about that is that

155
00:16:49,560 --> 00:16:53,240
the prospect of the future gone well I think is greater if

156
00:16:56,520 --> 00:17:02,120
people have this sense of there being great possibilities to realize not just one value

157
00:17:02,120 --> 00:17:07,640
but many values that if the future goes well it unlocks this huge I mean would have this

158
00:17:08,280 --> 00:17:14,520
super advanced technology plentiful resources a lot of time and there would be a kind of

159
00:17:14,520 --> 00:17:20,760
abundance and I think that can help people approach things in a more generous way than if

160
00:17:20,760 --> 00:17:26,200
there is extreme scarcity and like one person's gain is another's loss and there is not enough

161
00:17:26,200 --> 00:17:32,120
for everybody then it's very hard to be generous right it's easier to be generous if you have a

162
00:17:32,120 --> 00:17:36,600
lot and so thinking ahead if we could approach the future in this more cooperative and generous

163
00:17:36,600 --> 00:17:42,360
way I think that also reduces the risk of various dystopian outcomes and improves the prospect so

164
00:17:44,600 --> 00:17:51,880
those are some thoughts in the background yeah to speak I guess a little bit proverbially though

165
00:17:51,880 --> 00:17:57,160
that abundance has its own problems because what is a pleasure without pain and when we

166
00:17:57,160 --> 00:18:03,080
have everything I mean this is a big part of of the book in your project is where we find

167
00:18:03,080 --> 00:18:10,280
meaning in a world where we have everything and there is no scarcity to drive purpose in right

168
00:18:10,280 --> 00:18:16,840
yeah so so that's like the kind of the problem then that one confronts so much of our lives currently

169
00:18:16,840 --> 00:18:23,320
are structured around the various practical necessities that we face in our life so you

170
00:18:23,320 --> 00:18:28,200
have to go into work every day because you need to get the paycheck which you need to pay the rent

171
00:18:28,920 --> 00:18:35,400
you have to brush your teeth because otherwise your gum will decay and you have to you have to

172
00:18:35,400 --> 00:18:41,240
do this stat and you have like this like so much of our lives are just like doing something in order

173
00:18:41,240 --> 00:18:47,800
for something else to be cost as a result right and that that that kind of fills our life with

174
00:18:47,800 --> 00:18:55,000
sort of goal oriented activity but in this old world a very large chunk of all of that

175
00:18:56,680 --> 00:19:05,000
would no longer be needed like you wouldn't have to humans wouldn't have to engage in economically

176
00:19:05,000 --> 00:19:09,080
productive labor because ais and robots could do everything that needed to be done

177
00:19:12,200 --> 00:19:19,160
um yeah you you you wouldn't have to like spend an hour at the gym working out in order

178
00:19:19,160 --> 00:19:23,480
to be fit and healthy because you could pop a pill that would give you the same physiological

179
00:19:24,280 --> 00:19:29,320
effects and you can kind of go through activity by activity and you find that for many of them

180
00:19:29,320 --> 00:19:33,720
you can sort of either cross them out or at least put a question mark on top of them that

181
00:19:33,720 --> 00:19:37,800
although you could still do them at technical maturity they would seem to be a kind of

182
00:19:37,800 --> 00:19:45,640
pointlessness hanging over them like a dark cloud that maybe remove some of their value and so

183
00:19:47,560 --> 00:19:52,120
and and this is like I think yeah part of what would make from our current vantage point

184
00:19:52,120 --> 00:20:00,760
some of these scenarios at least prima facie look unattractive we kind of base our self-worth

185
00:20:00,760 --> 00:20:05,320
and dignity on the sense of making some doing something useful like you're a breadwinner or

186
00:20:05,320 --> 00:20:10,280
maybe you contribute to society in some other way or you like you you look after your kids or

187
00:20:10,280 --> 00:20:16,440
your grandkids and you like through your efforts and strivings like you actually make the world

188
00:20:16,440 --> 00:20:19,560
better in some place that it could in some way that it could not otherwise be

189
00:20:20,680 --> 00:20:27,640
and if that's removed then yeah there is a void that the utopians would have to somehow

190
00:20:27,640 --> 00:20:36,840
fill in I think a lot of the fun in this project comes from your imagination and

191
00:20:37,800 --> 00:20:45,080
looking at all the various examples of what life might be like and the technologies that might be

192
00:20:46,040 --> 00:20:52,840
available in a what you refer to as a technologically mature society and I want to talk

193
00:20:52,840 --> 00:20:59,240
about those things but before we do I mentioned earlier that there were I'm not sure if it was

194
00:20:59,240 --> 00:21:06,760
five or six I think five different utopias that you discuss in the book whereas I had thought that I

195
00:21:06,760 --> 00:21:10,920
just only ever thought there's one kind of utopia it's just heaven and I had left that

196
00:21:10,920 --> 00:21:16,440
unanalyzed heaven or something like heaven where everything's perfect but what are the

197
00:21:16,680 --> 00:21:25,480
the different types of utopia that you think are relevant to this discussion and and how do they

198
00:21:25,480 --> 00:21:32,200
differ and what are some of the basic problems that they confront the utopia dweller with

199
00:21:32,920 --> 00:21:39,400
yeah so so this is how I like how I classify them for my purposes but you could think of

200
00:21:40,120 --> 00:21:48,360
first category of sort of culture and politics utopia as many of the classical works fall into

201
00:21:48,360 --> 00:21:55,800
this category where basically the author tries to imagine a better political system or a better

202
00:21:55,800 --> 00:22:00,520
sort of culture so maybe you think there would be a different system of governance or like the

203
00:22:00,520 --> 00:22:05,640
gender roles would be defined differently or the way that children relate to their parents

204
00:22:06,600 --> 00:22:10,600
um and then it like different authors are different who is about the optimal way to

205
00:22:10,600 --> 00:22:17,560
arrange this and the optimal way would that be a utopia um now a slightly more I guess radical or

206
00:22:17,560 --> 00:22:23,400
like at least in one dimension more radical uh conception of utopia would be uh an abundance

207
00:22:23,400 --> 00:22:32,840
utopia where you imagine that there is plenty to go around so all material needs are abundantly

208
00:22:32,840 --> 00:22:42,440
supplied um and you do also find some of these um in the historical records like there was a

209
00:22:43,320 --> 00:22:51,480
a kind of medieval fantasy of the the land of kokai that was a kind of peasant vision of like

210
00:22:51,480 --> 00:22:58,200
the ideal life the rivers would flow with a wine and roasted turkeys would kind of drop down

211
00:22:58,840 --> 00:23:05,400
on the plate and that would be music and dancing and a lot of time for relaxing and and you can see

212
00:23:05,400 --> 00:23:11,000
if you were like a kind of agricultural labor like doing backbreaking work from morn to dusk

213
00:23:11,000 --> 00:23:16,280
and then just to have barely enough porridge to eat like this would already be like a fantastic

214
00:23:16,280 --> 00:23:21,000
conception right like you could eat as much as you want you could just stuff yourself with food

215
00:23:21,000 --> 00:23:30,920
and rest all day and like that so so that would be kind of an abundance utopia um and then then

216
00:23:30,920 --> 00:23:36,360
we have a kind of post-work utopia which is different from abundance utopia in that not

217
00:23:36,360 --> 00:23:45,240
only is there plenty of everything but humans don't need to work to produce it um and this is

218
00:23:45,240 --> 00:23:50,040
about as far as most mainstream conversation has gone in the context of AI you do find some

219
00:23:50,120 --> 00:23:56,520
economists who are starting to think about um automation and whether it could cause unemployment

220
00:23:58,040 --> 00:24:02,360
and if so what would happen like you know you maybe need some universal basic income etc

221
00:24:04,920 --> 00:24:13,160
and um like like in in Marx's utopia he doesn't fight very much about it but it seems he imagines

222
00:24:13,160 --> 00:24:20,760
that would be a sort of um certainly a culture's politics utopia according to his conception and

223
00:24:20,760 --> 00:24:26,760
then that would be like less work and more abundance but that would still be people would

224
00:24:26,760 --> 00:24:31,160
still be working so it's kind of but i think there are levels beyond that that you could consider

225
00:24:31,160 --> 00:24:39,160
which is um much more radical and poses much more deep problems uh in terms of human values so

226
00:24:39,880 --> 00:24:46,760
we have a post-instrumental utopia where it is not just the need for human economic labor

227
00:24:47,400 --> 00:24:53,880
that is rendered odious by technological progress but the need for all human labor that is

228
00:24:54,760 --> 00:25:00,760
instrumental in character so i mean i mentioned like you might have to labor at the gym to be fit

229
00:25:00,760 --> 00:25:09,400
but that you wouldn't have to do in utopia um right now if you want to understand mathematics you

230
00:25:09,400 --> 00:25:15,560
have to first study mathematics and spend hours working on exercise problems and like really

231
00:25:15,560 --> 00:25:21,800
exerting yourself right but in this condition of technical maturity there would be shortcuts to

232
00:25:21,800 --> 00:25:27,480
the same end you could imagine some swarm of nanobots infiltrating your brain and rewiring

233
00:25:27,480 --> 00:25:32,600
your synapses into a condition where you now possess mathematical knowledge without you having to

234
00:25:32,600 --> 00:25:42,760
put out any effort um so that kind of you know pulls the rug out of a wider range of human activity

235
00:25:42,760 --> 00:25:47,800
and exertion not just like the job that you go and do for eight hours a day but most of what fills

236
00:25:47,800 --> 00:25:54,680
the rest of your time um and then there's like a final stage beyond that which is i call a plastic

237
00:25:54,760 --> 00:26:01,000
utopia which has all the attributes of a post instrumental utopia but in addition um the human

238
00:26:01,000 --> 00:26:09,240
organism uh itself becomes malleable uh and subject to our wishes and desires so you could

239
00:26:09,240 --> 00:26:14,440
sort of choose what emotions you want to have or what thoughts you want to have or what moral

240
00:26:14,440 --> 00:26:19,960
character you want to have or what like physiology you have technologies to kind of reshape yourself

241
00:26:20,600 --> 00:26:28,440
uh at will um including your psychological states and and that then results in this kind of

242
00:26:29,240 --> 00:26:35,240
solve they're almost this old world where we're like all the kind of hard limitations that we

243
00:26:35,240 --> 00:26:41,800
currently face like are removed it seems and and the only thing that remains are kind of our values

244
00:26:41,800 --> 00:26:47,240
that i think have been shaped under this condition of scarcity which has like always been there for

245
00:26:47,960 --> 00:26:53,960
for humans throughout history like and um it's almost like formed an exoskeleton these practical

246
00:26:53,960 --> 00:26:59,800
needs uh and if you were to remove all of those practical necessities there is like the question

247
00:26:59,800 --> 00:27:06,760
of what happens to the this soft squishy parts like do they just become a blob like a drug doubt

248
00:27:06,760 --> 00:27:16,280
um a kind of pleasure blob uh or can they take some more interesting shape um and um yeah so that's

249
00:27:16,520 --> 00:27:20,440
so that's like the the main that there are a couple of early chapters that talk about

250
00:27:20,440 --> 00:27:24,520
some of these poor economic and technological but then the bulk of the issue is literally set in

251
00:27:24,520 --> 00:27:32,200
this condition of solve it as a solved world at this plastic and post instrumental i'd like to

252
00:27:32,760 --> 00:27:39,400
digress for a moment to this mathematics case that you brought up which i i find particularly

253
00:27:39,480 --> 00:27:48,440
compelling myself especially because it's an example of something i i said earlier where

254
00:27:48,440 --> 00:27:56,920
and you're examining what a mathematician might do in let's say a post instrumental utopia might

255
00:27:56,920 --> 00:28:03,560
tell us something about what mathematicians do and value today that might not readily be apparent

256
00:28:03,560 --> 00:28:10,280
so in this post instrumental world where we have extremely powerful

257
00:28:11,080 --> 00:28:16,520
automated theorem provers that might enumerate all the theorems of math and all the

258
00:28:17,400 --> 00:28:26,040
interesting axiomatic systems and there's no longer a role for mathematicians to be

259
00:28:26,040 --> 00:28:31,960
providing new proofs do you think mathematicians would still be around because i mean this is

260
00:28:31,960 --> 00:28:40,600
something that isn't this is one element of utopia that might not be that far out unlike

261
00:28:40,600 --> 00:28:48,200
some other ones because we already have automated theorem provers that are doing work that mathematicians

262
00:28:48,200 --> 00:28:55,640
can't do yeah so i mean people do i mean people play chess even though we have computers that can

263
00:28:55,640 --> 00:29:01,240
play much better chess or solve crossword puzzles even though there is no need for

264
00:29:01,240 --> 00:29:08,040
crossword puzzles to be solved and do a lot of other things currently right so the first answer

265
00:29:08,040 --> 00:29:14,600
one might think is yeah yeah sure we'll just do it anyway because it's fun now i think we can't

266
00:29:15,880 --> 00:29:21,000
stop at that point we need to like unpack this idea that we do it because it's fun so that could

267
00:29:21,000 --> 00:29:26,440
mean we do it because it gives us pleasure like somebody who is fascinated with mathematics

268
00:29:27,080 --> 00:29:33,400
probably derive pleasure from engaging in mathematical thinking and occasionally maybe

269
00:29:33,400 --> 00:29:41,160
finding a solution but if that's the only reason there would be again shortcuts to attaining pleasure

270
00:29:42,280 --> 00:29:47,160
in a solved world you could have more direct forms of brain manipulation that would stimulate

271
00:29:47,240 --> 00:29:52,440
your pleasure centers i mean you could imagine it as some sort of a super duper drug with outside

272
00:29:52,440 --> 00:29:58,360
effects and addiction potential or more likely some direct way of manipulating the relevant

273
00:29:58,360 --> 00:30:02,920
neural structures you know maybe we're all digital at that point anyway and you just

274
00:30:04,200 --> 00:30:10,760
and so yeah if all you wanted was to experience positive affect that there would be no need to

275
00:30:11,320 --> 00:30:18,680
do mathematics for that purpose so if you do nevertheless think that it would be better to

276
00:30:18,680 --> 00:30:24,360
do mathematics you then it seems value something other than just this hedonic state that you now

277
00:30:25,240 --> 00:30:31,160
very imperfectly and grudgingly like maybe like some little drips of pleasure is derived every

278
00:30:31,160 --> 00:30:35,640
once in a while when somebody who is enjoying mathematics does mathematics and there's probably

279
00:30:35,720 --> 00:30:45,320
just a lot of sort of boring futile effort and like uncomfortable straining to get to those occasional

280
00:30:45,320 --> 00:30:51,320
little drips of reward that's the stingy way our current reward systems are architected right

281
00:30:52,200 --> 00:30:56,840
so you could kind of open the floodgates to that and then have a kind of more blissed out

282
00:30:57,880 --> 00:31:05,000
psychological condition and so that's easy to dismiss a lot of people like immediately think

283
00:31:05,000 --> 00:31:11,000
that wow that's like what a horrific view of the future that is we're all kind of like junkies

284
00:31:11,000 --> 00:31:16,920
sprayed on some sort of flea-infested mattress but with like a super drug dripping into our nucleus

285
00:31:16,920 --> 00:31:26,280
incumbents and you know it might well be that ultimately we want and can have more than that

286
00:31:26,280 --> 00:31:30,440
but I it is actually a rather deep question whether we shouldn't dismiss too quickly the

287
00:31:31,320 --> 00:31:39,480
the super bliss as an element of what we would ultimately choose and have reason to choose

288
00:31:39,480 --> 00:31:47,880
perhaps nevertheless I think we can have that and have a whole bunch of other things on top of that

289
00:31:47,880 --> 00:31:56,040
that may be satisfy other possible value candidates so certainly if you do think that it is good to

290
00:31:56,760 --> 00:32:02,120
engage in certain types of activity you could do that as well there is no reason you could have

291
00:32:02,120 --> 00:32:08,680
complex experiences pleasure plus complex experiences plus various forms of goal-directed

292
00:32:08,680 --> 00:32:13,160
activity but maybe we're you know and we could talk more about why you would be adopting various

293
00:32:13,160 --> 00:32:21,080
goals if if you were in this post-instrumental situation but and and then perhaps some additional

294
00:32:21,160 --> 00:32:27,160
elements even beyond that that you could kind of reconstitute some of the prerequisites for

295
00:32:27,160 --> 00:32:34,680
instantiating human values even in this solved world condition I think what what motivated my

296
00:32:34,680 --> 00:32:41,960
asking this question was reflecting on two conversations I had in the past on the show

297
00:32:41,960 --> 00:32:49,160
one was with a renowned number theorist at Columbia University named Michael Harris that

298
00:32:49,160 --> 00:32:54,520
we had a couple of years ago so it's a little faint in my mind and another is with Steven Wolfram

299
00:32:55,240 --> 00:33:04,360
and we were talking about math in a world in which all of math is essentially automatable and

300
00:33:05,400 --> 00:33:12,440
today I think our folk view of what mathematicians are in the business of is just producing new proofs

301
00:33:13,080 --> 00:33:20,200
discovering or inventing depending on your philosophical view more facts to be added to

302
00:33:20,200 --> 00:33:32,360
this big book of math but what Michael Harris said and what I think Steven Wolfram agreed with

303
00:33:33,560 --> 00:33:38,680
is that mathematicians aren't just in the business of producing new facts

304
00:33:38,680 --> 00:33:42,760
they're in the business of understanding that's what really drives what they're doing is they

305
00:33:42,760 --> 00:33:49,560
want to understand these structures and I was thinking that in this post-instrumental world

306
00:33:49,560 --> 00:33:55,640
when mathematicians no longer have to produce proofs they still would be very interested in

307
00:33:56,200 --> 00:34:03,400
understanding and maybe teaching mathematics but then this raises another problem that I hadn't

308
00:34:03,400 --> 00:34:11,960
considered yet that you discuss in some of the later or the more advanced types of utopia where

309
00:34:11,960 --> 00:34:18,840
we have a matrix like system of downloading information where there's no long into our

310
00:34:18,840 --> 00:34:25,240
brains where there's no longer any real barrier to understanding you can understand anything

311
00:34:25,240 --> 00:34:30,680
as soon as you just downloaded into your mind so there's no there's not really an activity of being

312
00:34:30,680 --> 00:34:35,400
a mathematician anymore just if you want to understand the math then you press a button

313
00:34:35,400 --> 00:34:42,120
then you understand the math yeah so this is a very like advanced level of technology and it's

314
00:34:42,120 --> 00:34:48,440
like a little unclear exactly how close to this we could get but certainly if you imagine a human

315
00:34:48,440 --> 00:34:53,960
upload like a digitized version of a human mind and then you have on the outside some machine super

316
00:34:53,960 --> 00:35:01,560
intelligence that is able to like understand how the neural network that is you sometimes

317
00:35:02,360 --> 00:35:08,760
successfully can grasp mathematical concepts and in other cases fail and what variations of your

318
00:35:08,760 --> 00:35:14,200
neural network would be required like what edits to make it as similar as possible to you now but

319
00:35:14,200 --> 00:35:19,880
with this extra mathematical understanding I think it possibly would be the case that the

320
00:35:19,880 --> 00:35:26,840
job of working out how to adjust the various parameters in your brain to give you this mathematical

321
00:35:26,840 --> 00:35:32,280
knowledge could be outsourced to this machine super intelligence that is looking at a model of

322
00:35:32,280 --> 00:35:39,800
your brain and probably be able to do that without actually running a detailed simulation of your

323
00:35:39,800 --> 00:35:44,280
brain which if it if that were the only way for the super intelligence to work it out you might

324
00:35:44,280 --> 00:35:49,080
then think well you would actually then be instantiated in the simulation and you would have to put

325
00:35:49,080 --> 00:35:54,840
in the effort although the simulated version of you would do but I think probably you would be able

326
00:35:54,840 --> 00:36:03,240
to move up levels of abstraction to more or less obviate that need and so that you would then be

327
00:36:03,240 --> 00:36:10,840
able to yeah in effect download skills or knowledge without having to put in time studying

328
00:36:13,400 --> 00:36:17,400
and so yeah I think I mean and the teaching of mathematics is even more obvious I mean I think

329
00:36:17,400 --> 00:36:21,560
we are very close now to having tutorial systems that would be much better than most

330
00:36:22,200 --> 00:36:26,680
like high school mathematics I mean aside from keeping the class disciplined and making sure

331
00:36:26,680 --> 00:36:32,280
everybody sits at their desk or whatever like but the actual instruction I'd imagine maybe even

332
00:36:32,280 --> 00:36:39,640
chateau pt4 would like be a better explainer than than than most high school teachers because it can

333
00:36:40,680 --> 00:36:45,640
keep track of exactly where you are where you get stuck on a particular problem and sort of

334
00:36:45,640 --> 00:36:52,760
customize its explanation for you as opposed to doing something generic for the class of 30

335
00:36:55,880 --> 00:37:03,000
just curious are you drawing the sort of distinction between chateau pt4 or gbt4 being

336
00:37:03,000 --> 00:37:11,400
a good high school teacher and a good university level professor because maybe there is insufficient

337
00:37:11,480 --> 00:37:17,640
training data on university level material for chateau pt4 to be able to explain it

338
00:37:17,640 --> 00:37:24,440
better than a university professor could I mean I so I don't know exactly I'd imagine there need

339
00:37:24,440 --> 00:37:31,080
to be some fine tuning maybe to adapt these large language models for specific use cases in the

340
00:37:31,080 --> 00:37:38,200
classroom I'd imagine the standard university courses like you know first and second calculus

341
00:37:38,200 --> 00:37:45,000
and like all of that stuff probably it would be able to explain Adam Madden as well as a mathematics

342
00:37:45,000 --> 00:37:49,240
professor I think maybe once you get to the research level of mathematics whether it's more

343
00:37:49,240 --> 00:37:55,240
matter of taste and then I'm not sure the current language models are quite yet there yet whether

344
00:37:55,240 --> 00:38:00,520
it would be as good as like a decent mathematics professor in terms of figuring out whether a

345
00:38:00,520 --> 00:38:07,400
research direction is is is promising or whether some proposal for say a phd is like

346
00:38:08,280 --> 00:38:12,760
the right level of difficulty that the student could possibly do it in four years and

347
00:38:13,800 --> 00:38:17,560
like like I think there there is like well there's obviously less training data written up

348
00:38:18,120 --> 00:38:24,440
for ingestion and and also it involves the the most high level human faculties that haven't yet

349
00:38:24,440 --> 00:38:32,120
perhaps quite been attained by the current generation of systems a term of art that you

350
00:38:33,000 --> 00:38:40,040
introduce in the book that I referenced earlier is technological maturity and

351
00:38:41,320 --> 00:38:44,920
I also mentioned that this is one of the the very fun things for me

352
00:38:44,920 --> 00:38:50,120
of reading deep utopia because we get to see your imagination at work and all the possible

353
00:38:50,120 --> 00:38:58,440
things that a technological mature society might have at its fingertips so to speak

354
00:38:58,440 --> 00:39:05,800
so how do you define a technologically mature society and what are some of the things that

355
00:39:06,520 --> 00:39:15,000
people might expect to be able to do or have on offer yeah so it's like a condition at which

356
00:39:18,200 --> 00:39:25,800
all those technological affordances that are consistent with the loss of physics

357
00:39:25,800 --> 00:39:32,920
and for which there is some possible trajectory from where we are now to their development exists

358
00:39:34,680 --> 00:39:41,960
so yeah it might be that you can't ever strictly speaking attain perfect technological maturity

359
00:39:41,960 --> 00:39:46,840
but in reality you might get some kind of close approximation to it if you have developed all

360
00:39:47,400 --> 00:39:53,800
the most useful general purpose technologies is perfect technical technological maturity being

361
00:39:53,800 --> 00:39:59,320
able to do anything that's physically possible well I would also have this pathway there but it

362
00:39:59,320 --> 00:40:04,680
might just be that if you imagine there are kind of an unlimited number of more and more

363
00:40:04,680 --> 00:40:11,880
specialized technologies for doing specific things it might be possible to develop any

364
00:40:12,520 --> 00:40:18,760
like small subset of them by like investing your r&d resources but if there is like quadrillion

365
00:40:18,760 --> 00:40:23,480
and quadrillion of different very specialized technologies maybe you can't develop the whole

366
00:40:23,480 --> 00:40:29,800
set of them but I think that would be sort of ways of getting close to that for most practical

367
00:40:29,800 --> 00:40:34,840
purposes like once you have superintelligence and nanotechnology you can sort of come up with

368
00:40:35,560 --> 00:40:39,640
the superintelligence can make arbitrary designs and the nanotech can sort of put it together

369
00:40:40,440 --> 00:40:43,880
and then like some other things you can I think get pretty close to that

370
00:40:44,760 --> 00:40:51,320
um so and and I think we know already at least some lower bounds of what this

371
00:40:51,320 --> 00:40:57,720
such a condition of technical maturity would involve that is technologies that we can already

372
00:40:57,720 --> 00:41:03,960
have good reason to think are physically possible and for which there is a pathway such that we could

373
00:41:03,960 --> 00:41:09,000
at least in the fullness of time under favorable condition get there so superintelligence is one

374
00:41:09,000 --> 00:41:17,800
of these like I think perfect virtual reality of the sort that would be indistinguishable to the

375
00:41:17,800 --> 00:41:26,760
person inside the physical like virtual reality from from ordinary basic reality would be another

376
00:41:26,760 --> 00:41:34,680
affordance I think like cures for aging uploading into computers space colonization at large

377
00:41:34,680 --> 00:41:41,160
intergalactic scales perfect newer technology in the sense of being able to control

378
00:41:42,120 --> 00:41:50,200
precisely our hedonic and emotional states cognitive augmentations like many other things

379
00:41:50,200 --> 00:41:57,000
as well but at least that already gives you a huge set of possible like things you can do right

380
00:41:57,000 --> 00:42:03,880
that's technological maturity one of the so the way that for our listeners who haven't

381
00:42:03,880 --> 00:42:08,840
looked at the book yet the book is organized so it's kind of like a a course you're teaching to

382
00:42:08,840 --> 00:42:15,400
these imaginary students and at various points in the book there are handouts where you've

383
00:42:15,400 --> 00:42:22,200
summarized some of the material and I have a handout number two some capabilities at technological

384
00:42:22,200 --> 00:42:30,680
maturity on the screen before me and I was just glancing at them and one that jumped out at me

385
00:42:30,680 --> 00:42:39,160
is the possibility of Dyson spheres for harvesting the energy output of stars and this comes to mind

386
00:42:39,160 --> 00:42:49,960
for two reasons one is well I'll just focus on one but the reason is I've done a number of episodes

387
00:42:49,960 --> 00:42:55,400
on string theory and been thinking a lot about string theory lately and one of the big barriers

388
00:42:55,400 --> 00:43:02,120
to empirical confirmation of string theory is that we cannot generate enough energy to probe

389
00:43:02,120 --> 00:43:08,040
sufficiently small distances to confirm the existence of strings and people have conjectured

390
00:43:08,040 --> 00:43:15,800
that the sort of energy you would need is the energy of a star so it looks like we'll be able

391
00:43:15,800 --> 00:43:21,960
to confirm or disconfirm string theory in a very useful technology the Dyson sphere

392
00:43:22,680 --> 00:43:28,120
but yeah like you mentioned arbitrary sensory inputs reversal of aging

393
00:43:29,480 --> 00:43:33,880
uploading of biological brains into computers and all of these things

394
00:43:36,680 --> 00:43:43,240
engender their own host of philosophical questions to be asked and answered aligned police

395
00:43:43,240 --> 00:43:49,480
bots and automatic treaty enforcement I mean and police bots are already a question being asked

396
00:43:49,480 --> 00:43:55,240
today but you know reversal of aging and cures for all diseases that's one that

397
00:43:55,880 --> 00:44:03,160
jumps out at me or a couple that jump out at me right now are there any particular problems

398
00:44:03,160 --> 00:44:11,320
that would need to be dealt with in these later stage utopias if we just lived forever

399
00:44:12,200 --> 00:44:20,280
well I mean the most obvious one would be the size of the population which is able to increase

400
00:44:20,280 --> 00:44:30,680
exponentially and if nobody dies then at some point the rate of births would have to match

401
00:44:31,400 --> 00:44:38,120
the rate of acquisition of new resources in order to maintain high per capita incomes

402
00:44:39,080 --> 00:44:45,000
because at technological maturity there wouldn't be increases in productivity like that's one of

403
00:44:45,000 --> 00:44:48,840
the things that drive technological growth now right even if the resources are the same you

404
00:44:48,840 --> 00:44:52,840
could have better technologies to make more efficient use of them but that will have maxed

405
00:44:52,840 --> 00:44:58,600
out and so then the only way that the economy could grow at that point is if more resources

406
00:44:58,600 --> 00:45:06,520
are attained through expanding in space but that can at most happen at a polynomial rate

407
00:45:07,160 --> 00:45:13,080
like you have a bubble that is growing in all directions at some significant fraction of the

408
00:45:13,080 --> 00:45:19,080
speed of light and the volume of that grows as a polynomial of time whereas the population

409
00:45:19,640 --> 00:45:26,600
like could grow exponentially right and so then eventually you would have a Malthusian condition

410
00:45:28,360 --> 00:45:35,800
and so in the limit you would need to sort of make sure the the rate of population increase

411
00:45:36,600 --> 00:45:41,400
which might be sort of digital minds making copies of themselves right but would kind of at

412
00:45:41,400 --> 00:45:49,080
most match the rate at which the the resource endowment increases like another maybe more

413
00:45:49,080 --> 00:45:55,160
philosophically interesting question if we eliminated aging and then I mean right now what

414
00:45:55,160 --> 00:46:01,000
would happen if you eliminated aging is that people would die from accidents and wars but maybe

415
00:46:01,000 --> 00:46:06,840
with a lifespan of a few thousand years if you kind of calculate the rate of death by accident

416
00:46:08,680 --> 00:46:15,400
now hopefully we will bring down the rate of accidents and and war certainly in utopia you'd

417
00:46:15,400 --> 00:46:20,360
imagine that to go way down and if you could upload yourself to computers then you could make backup

418
00:46:20,360 --> 00:46:26,040
copies and stuff and then the rate of accidents could kind of become arbitrarily small so then

419
00:46:26,040 --> 00:46:36,920
then you might have very long lifespans even astronomical and and that presents more interesting

420
00:46:36,920 --> 00:46:43,240
philosophical questions for human values in that well first of all we don't know what would happen

421
00:46:43,240 --> 00:46:49,080
to like if you remained a human mind of like a brain with your current size and the number of

422
00:46:49,080 --> 00:46:52,920
synapses and stuff like we know you can continue to learn and develop and grow for like a hundred

423
00:46:52,920 --> 00:46:57,400
years but we don't even know what would happen if a human just kept living for 400 years like would

424
00:46:57,400 --> 00:47:08,680
you go stale and rigid and like so and even if that didn't happen in a sort of like neurological

425
00:47:08,680 --> 00:47:19,640
way like then it might still be that and it's a hard question to sort of gauge but that the

426
00:47:19,640 --> 00:47:25,160
number of different types of things that can be done with a human mind and body in a human world

427
00:47:26,280 --> 00:47:31,640
is finite and is that number small enough that it would become relevant so that at some point you

428
00:47:31,640 --> 00:47:37,880
would just run out of interesting new things to do and if so how long like I mean I think

429
00:47:37,880 --> 00:47:42,280
given that we are finite there has to be some number of years after which you would literally

430
00:47:42,280 --> 00:47:48,920
be doing the same thing you'd already done but if that is kind of 10 to the power of a hundred

431
00:47:49,000 --> 00:47:53,720
or something we might not have to care very much because we'd die anyway before that from the

432
00:47:53,720 --> 00:47:59,960
heat death of the universe but if the answer is like 10,000 years like after 10,000 years you'd

433
00:47:59,960 --> 00:48:06,360
basically just repeat yourself then either you'd have to accept that such lives will have a diminution

434
00:48:06,360 --> 00:48:11,800
of a certain kind of novelty and that may be a price to admittance and this kind of extreme

435
00:48:11,800 --> 00:48:18,600
longevity in utopia or you would have to perhaps start to level up after you have spent at 10,000

436
00:48:18,600 --> 00:48:24,200
years being a human maybe then you would want to become a transhuman like increase your intellectual

437
00:48:24,200 --> 00:48:29,480
faculties or something and you could kind of start to tackle the next level of challenges and

438
00:48:31,000 --> 00:48:39,240
keep developing that way but it is I think it is one direction in which you can kind of

439
00:48:40,440 --> 00:48:45,800
take our current human values and where they start to become strained if you just imagine the

440
00:48:45,800 --> 00:48:50,760
current human lifespan with our current values and our current faculties but just extended

441
00:48:50,760 --> 00:48:55,480
and like prevented from physical disease like at some point I think that would

442
00:48:58,600 --> 00:49:02,280
possibly become unattractive like there are still a lot of things in human life that don't

443
00:49:02,280 --> 00:49:08,920
require novelty I mean like a cup of tea is like about as good like the 10,000s time as it is the

444
00:49:08,920 --> 00:49:14,760
second time or first time right it they're like renewable pleasures and sometimes you could run

445
00:49:14,760 --> 00:49:18,600
the argument that sometimes the best things in life are the little things that it's not like

446
00:49:19,480 --> 00:49:27,240
like the big dramatic narrative climaxes but it's like you know the smell of autumn air in

447
00:49:27,240 --> 00:49:33,800
the evening along a stroll along the coast or like looking into your loved one's face or a little

448
00:49:34,360 --> 00:49:40,760
you know cup of tea or whatever like these little things sometimes you know you could run the argument

449
00:49:40,840 --> 00:49:47,000
that that's a fairly high quality value although we give them kind of short shrift

450
00:49:49,080 --> 00:49:53,160
an interesting connection to the philosophical literature going back

451
00:49:53,960 --> 00:50:01,880
hundreds of years I mean is how conceptions of personal identity might shift in this world

452
00:50:01,880 --> 00:50:09,240
where we live for we live indefinitely I think and you should correct me if I'm wrong but I think

453
00:50:09,240 --> 00:50:16,280
it was Locke who proposed that personal identity has something to do with memory and psychological

454
00:50:16,280 --> 00:50:26,760
connectedness and maybe it was Derek Parfit in his sort of ongoing discourse with David Lewis who

455
00:50:27,640 --> 00:50:34,360
questioned whether somebody who lived who wasn't immortal would eventually no longer be the same

456
00:50:34,360 --> 00:50:41,400
person that they started out as because they would not remember let's say their childhood

457
00:50:41,400 --> 00:50:47,800
F there would no longer be any psychological connection to a person stage thousands and

458
00:50:47,800 --> 00:50:55,160
thousands of years earlier and in this sort of utopia maybe we would bypass this by getting

459
00:50:56,280 --> 00:51:01,800
memory implants or memory chip implants so that we could just continually

460
00:51:02,760 --> 00:51:09,160
add new memories and maintain internal psychological connectedness yeah um

461
00:51:10,120 --> 00:51:17,320
I mean I guess there is first of all like I would want to make retain the distinction between

462
00:51:17,320 --> 00:51:24,760
long time and infinity like immortality I think you should mean not dying not not just taking

463
00:51:24,760 --> 00:51:30,840
a while before you die like conceptually and it is important because immortality in that strict

464
00:51:30,840 --> 00:51:37,400
sense just might be impossible in our physical universe and be the preserve of say more theological

465
00:51:37,400 --> 00:51:43,880
scenarios and from the blink view of eternity like whether you live for like 80 years or 80

466
00:51:43,880 --> 00:51:50,040
thousand years it's kind of in some sense the same right it's an infinitesimal short relative to

467
00:51:50,040 --> 00:52:02,920
infinity either way now I do think that yes increasing I call it time time suits but the

468
00:52:02,920 --> 00:52:10,440
idea that we could make some modifications tasks that make us better able to survive radical change

469
00:52:10,440 --> 00:52:17,000
without having our identity eroded or our values corrupted so the most obvious thing would be to

470
00:52:17,000 --> 00:52:22,680
improve our episodic memory so that you actually don't just forget what you've been through and

471
00:52:22,680 --> 00:52:29,800
that could make you sort of remain relevantly the same for longer than if you just have like the

472
00:52:29,800 --> 00:52:35,480
memory of a goldfish and and you could imagine some other tricks as well that would allow us to

473
00:52:35,480 --> 00:52:41,400
sort of endure for longer periods of time and and upgrading our capacities in different ways could

474
00:52:41,480 --> 00:52:51,400
help with that um but even if it came to a point where you would have to accept some loss of personal

475
00:52:51,400 --> 00:52:56,840
continuity in in whatever relevant sense I mean it might be acceptable like if you if you were

476
00:52:56,840 --> 00:53:01,480
have a five-year-old and you know that if there were a pill that would just arrest their development

477
00:53:01,480 --> 00:53:04,760
that they would remain a five-year-old like in some sense maybe a year from now that would be

478
00:53:04,760 --> 00:53:10,760
more similar to what they're now than if they're just allowed to freely develop uh but we might

479
00:53:10,760 --> 00:53:16,520
still think it's better for them to grow up like in some sense you are the same person now as you

480
00:53:16,520 --> 00:53:23,880
were when you were five but in realistically you're also quite different right in many ways uh and uh

481
00:53:23,880 --> 00:53:32,200
and maybe that's not an unambiguously bad thing like and so I think the combination of these that um

482
00:53:33,160 --> 00:53:43,560
a uh we can reduce the negative effects of various kinds of uh corroding uh consequences of the passage

483
00:53:43,560 --> 00:53:50,360
of time through improving memory and other certain other positive attributes like also including like

484
00:53:50,360 --> 00:53:57,560
maybe extending your planning horizon etc and and then also accepting some degree of of change

485
00:53:57,560 --> 00:54:03,320
even if it does mean that you eventually move further away from what you once were if there's

486
00:54:03,320 --> 00:54:07,640
still a difference between that and just dying because at any given point in time you might

487
00:54:07,640 --> 00:54:13,240
look forward to hundreds of years of existing in some very similar things your current condition and

488
00:54:13,240 --> 00:54:18,440
and you would gradually sort of you know see new ways of doing things or being or new experiences

489
00:54:19,000 --> 00:54:23,480
that that feels a lot more optimistic to me than it then like a kind of you're gonna be shot

490
00:54:24,440 --> 00:54:31,960
tomorrow morning right it's a um this the smoother and gentler way of losing our connection with the

491
00:54:31,960 --> 00:54:39,720
past seems perhaps prudentially preferable um even to the extent that it is unavoidable relative to

492
00:54:39,720 --> 00:54:47,400
yeah um the alternative okay not that a lot of this or all of it hasn't all been speculative

493
00:54:48,120 --> 00:54:52,600
but i have a particularly speculative question to ask so when i

494
00:54:54,280 --> 00:55:00,600
spoke with avi lobe the harvard astrophysicist this this brings us back to dyson spheres

495
00:55:01,160 --> 00:55:11,160
he conjectured that umuamua this comet that passed us by in i believe 2017 might have been

496
00:55:11,880 --> 00:55:23,160
part of a dyson sphere and you have made your own very uh i don't know if splashy is an adjective but

497
00:55:24,280 --> 00:55:31,160
powerful uh controversial conjectures about for instance whether or not we might be in a simulation

498
00:55:31,160 --> 00:55:39,720
and i'm wondering if you think it is possible or likely even that there are societies out there

499
00:55:39,720 --> 00:55:46,760
that have already reached uh for instance a post scarcity utopian stage because i think the

500
00:55:47,560 --> 00:55:54,120
having a dyson sphere if whoever sent uamua our way had a dyson sphere we would classify them

501
00:55:54,120 --> 00:56:01,400
probably as being in a post scarcity period assuming that they still have other dyson spheres

502
00:56:01,400 --> 00:56:06,440
yeah i mean so that i mean post scarcity they might not i mean it depends a lot on what their goals are

503
00:56:07,160 --> 00:56:13,000
and um it might be that the dyson sphere of resources is nowhere near enough for what they

504
00:56:13,000 --> 00:56:22,440
are trying to do um and so it is to some extent a human con like um it's not like a very precisely

505
00:56:22,440 --> 00:56:27,880
defined concept it's like you could say like from few humans so you could say that we are already

506
00:56:28,520 --> 00:56:35,560
uh somewhat close in some respects well if you're if you are like fortunate enough to live in a in a

507
00:56:35,560 --> 00:56:41,640
in a rich country and you have a i don't know like you're healthy and trying to good education like

508
00:56:41,640 --> 00:56:50,440
probably most of your listeners like it's probably the case that um uh if if you wanted to you wouldn't

509
00:56:50,440 --> 00:56:56,120
really have to work to survive like or if you select say say you worked hard really hard for

510
00:56:56,120 --> 00:57:00,760
five or ten years and saved up all the money you probably could then move to thailand or something

511
00:57:00,760 --> 00:57:06,920
and buy a small hot near a beach and then have enough to eat and be physically healthy and you'd

512
00:57:06,920 --> 00:57:10,920
have a computer you could eat access the internet or whatever for the rest of your life without

513
00:57:10,920 --> 00:57:17,400
having to work like it's at least tantalizingly close uh but yeah people choose to continue to

514
00:57:17,400 --> 00:57:24,280
strive because they want more than just the basic necessities of life right like and in particular

515
00:57:24,280 --> 00:57:29,400
we want to have more than other people and so there's a lot of this kind of um zero some

516
00:57:30,040 --> 00:57:38,600
status consumption going on with humans um and that could drive scarcity up to astronomical

517
00:57:38,600 --> 00:57:45,400
levels right like you yeah you have your own dyson sphere but uh like i have four dyson spheres so

518
00:57:45,400 --> 00:57:52,040
like you still need to try to um catch up and and and intergalactic civilization might have

519
00:57:52,040 --> 00:57:56,600
practical reasons as well like maybe they fear some other intergalactic civilization that might

520
00:57:56,600 --> 00:58:04,280
have a larger fleet of worships or something and so um but the difference is like um even if human

521
00:58:05,000 --> 00:58:12,680
desires are insatiable it doesn't mean there will always be a need for human work like you

522
00:58:12,680 --> 00:58:20,520
could reach a condition where even though like you maybe are worth a trillion dollars and you

523
00:58:20,520 --> 00:58:25,000
would like to make another trillion dollars there's just nothing you can do with your own labor that

524
00:58:25,000 --> 00:58:29,800
would make you any significant amount of money because all the work is better done by machine

525
00:58:31,640 --> 00:58:36,520
and if the most you could make is the minimum wage then once you're a trillion year like

526
00:58:36,520 --> 00:58:40,360
there is no point really working right because it's trivial compared to what you just earned from

527
00:58:40,360 --> 00:58:44,680
your capital and and it might even be that by doing the work yourself you like expend more

528
00:58:44,680 --> 00:58:50,200
calories that cost you more than the actual value of the labor so you could still end up in this kind

529
00:58:50,200 --> 00:58:58,200
of condition of um uh unemployment or post work even if there are still needs that have or desires

530
00:58:58,200 --> 00:59:03,720
that haven't been fully uh satisfied um now you asked whether I think there are already some that

531
00:59:03,720 --> 00:59:10,280
have attained either poor scarcity or I guess you could generalize it into technological maturity

532
00:59:10,280 --> 00:59:15,800
and I think if the universe is infinite as it seems to be like if we have the simplest topology

533
00:59:15,800 --> 00:59:24,040
and an opener flat spacetime um then definitely that would be uh such civilizations in fact infinitely

534
00:59:24,040 --> 00:59:31,240
many of them out there but um if I had to guess I'd say none in our in the observable universe and so

535
00:59:31,240 --> 00:59:38,360
that there would be infinitely many of these but they would have low density and so um um we might

536
00:59:38,360 --> 00:59:43,480
be out of causal contact forever with the nearest other one um that certainly would help explain the

537
00:59:43,480 --> 00:59:49,560
Fermi paradox um but it's not the only possible explanation so we can't rule out that we could

538
00:59:49,560 --> 00:59:57,240
share the observable universe with some other civilizations as well. Speaking of intergalactic

539
00:59:57,240 --> 01:00:05,480
civilizations and intergalactic warfare obviously that would be a huge barrier to us reaching a

540
01:00:05,560 --> 01:00:15,560
utopian state if we had to ascend off uh an alien invasion but do you see any other barriers

541
01:00:15,560 --> 01:00:23,000
besides just our ability to produce a super intelligent technology or super intelligent

542
01:00:23,000 --> 01:00:32,520
AI other barriers beyond that to our reaching utopian states? Uh yes I mean first of all it would

543
01:00:32,520 --> 01:00:37,640
be uh crucially super intelligence is well aligned um because otherwise yeah that might

544
01:00:37,640 --> 01:00:45,480
be our undoing um and second if you have scenarios uh multipolar in character like if you have many

545
01:00:45,480 --> 01:00:54,120
different entities ultimately with our own AI uh assistance that then a lot of the same dynamics

546
01:00:54,120 --> 01:01:04,440
that uh we see on the planet today with arms races and um oppression and warfare and exploitation of

547
01:01:04,440 --> 01:01:11,160
the global commons by sort of spewing pollutants into the atmosphere overfishing the oceans all of

548
01:01:11,160 --> 01:01:17,240
that could still occur and and indeed could be intensified in this kind of hyper competitive

549
01:01:17,240 --> 01:01:23,640
economy that might be created um if on the other hand you don't have a multipolar but like a unipolar

550
01:01:23,720 --> 01:01:27,880
or as I call like a singleton scenario where it gets all consolidated then there is the obvious

551
01:01:27,880 --> 01:01:38,120
question of um who controls that uh singleton right and how uh benevolent and wise is whatever

552
01:01:38,120 --> 01:01:44,440
the mechanism whether it's like a global democracy or a dictatorship or whatever it is is like so I

553
01:01:44,440 --> 01:01:51,080
think we can identify several broad categories of things that need to fall in place so one is

554
01:01:51,080 --> 01:01:56,120
to have this kind of uh deep utopia a you would need super advanced technology

555
01:01:57,480 --> 01:02:02,360
like otherwise you just can't do all of the if you right now we can't cure the

556
01:02:02,360 --> 01:02:09,000
jit with a cancer in many cases right and that means our world will fall short of what it should be

557
01:02:09,880 --> 01:02:17,560
so the super advanced technology but then I think also we'd need um a fairly high level of

558
01:02:17,560 --> 01:02:25,000
cooperation slash good governance to prevent sort of negative some conflict and then we'd

559
01:02:25,000 --> 01:02:32,120
also need some adequately high level of wisdom in how we use these great uh technological powers

560
01:02:32,120 --> 01:02:42,120
that we have um and it might be that the wisdom is it has a kind of threshold effect where if

561
01:02:42,200 --> 01:02:48,040
you're above that threshold even if you're still uh unwise in many deep ways and have many

562
01:02:48,680 --> 01:02:53,480
erroneous beliefs and misconceptions you might have enough wisdom to realize that you are fallible

563
01:02:53,480 --> 01:02:59,560
and to start to seek out ways to fix you can reflect on your own shortcomings and gradually

564
01:02:59,560 --> 01:03:05,240
develop ways of remedying it but if you're sort of below that wisdom level you might

565
01:03:05,240 --> 01:03:11,560
be more likely to lock in your current prejudices uh or to shoot your foot off when you're trying to

566
01:03:11,560 --> 01:03:16,520
like develop ways to fix it and it's kind of actually an open question I think where humanity

567
01:03:16,520 --> 01:03:21,000
is relative to that because you could ask the same question at the collective level like are we

568
01:03:21,000 --> 01:03:24,760
I think we are maybe close to the threshold level but I'm not sure whether we are just under

569
01:03:24,760 --> 01:03:35,320
or just above it um and um and then we might need some bit of luck as well on top of that um but um

570
01:03:36,200 --> 01:03:45,720
um I think yeah although these are kind of extreme technological postulates and you might

571
01:03:45,720 --> 01:03:51,880
see them very wide wild ideas I still think um they might actually not be completely unrealistic

572
01:03:51,880 --> 01:04:00,520
even within our lifetime if this whole AI transition happens um and it's if you sort of

573
01:04:00,520 --> 01:04:07,480
zoom out and look at the human history like uh from from its inception like you know a few

574
01:04:07,480 --> 01:04:13,880
hundred thousand years ago to today and you plot that in any possible way you you might want to

575
01:04:13,880 --> 01:04:20,440
like whether it's like the amount of you know um you know the GDP let us say or like some sort of

576
01:04:20,440 --> 01:04:26,840
the energy expended by human civilization or like in any it does have this you like if you

577
01:04:26,840 --> 01:04:32,200
plot it on a linear scale you just see a flat line that just spikes up at the end of the graph so

578
01:04:32,200 --> 01:04:38,840
then you have to go to like a log plot or to even see it bending but even then it bends up and we're

579
01:04:38,840 --> 01:04:44,200
really sitting right now at this like ridiculous anomaly um where what we are currently taking

580
01:04:44,200 --> 01:04:50,840
to be the normal human condition is in any which way look at it like just extreme historical anomaly

581
01:04:50,840 --> 01:04:58,600
like a tiny little thin coat of paint on on like a giant uh battleship of human history right it's

582
01:04:58,600 --> 01:05:04,200
like it and and then now we think it's like a radical conception to think well what if that changes

583
01:05:04,200 --> 01:05:08,920
in the future whereas in fact if you sort of zoom out you look like we are right now in the middle

584
01:05:08,920 --> 01:05:15,160
of this like explosion of something and we don't know what it's going to end up like but um but I

585
01:05:15,160 --> 01:05:20,200
do believe as as you alluded to earlier that scenarios in which we just remain more or less

586
01:05:20,200 --> 01:05:25,240
like we are now and have the current human condition just keep going for like tens of thousands of

587
01:05:25,240 --> 01:05:31,240
years that just seems extremely improbable to me relative to either like extinction slash dystopia

588
01:05:31,240 --> 01:05:38,680
or some radical transformation into some kind of post-human condition well Nick I I really

589
01:05:38,680 --> 01:05:44,840
enjoyed deep utopia and I came out of it with so many ideas and concepts that I hadn't considered

590
01:05:44,840 --> 01:05:49,800
beforehand so thanks so much for writing the book and thinking about these really interesting

591
01:05:49,800 --> 01:05:54,520
pressing problems and thanks a lot for coming on the show to talk to me about it well thanks to you

592
01:05:54,520 --> 01:06:01,080
and uh to the little cat there as well

