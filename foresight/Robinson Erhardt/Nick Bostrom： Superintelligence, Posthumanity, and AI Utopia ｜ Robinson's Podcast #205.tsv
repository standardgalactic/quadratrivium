start	end	text
0	4680	If you sort of zoom out, you look like we are right now in the middle of this like explosion of some
4680	10880	sec and we don't know what it's going to end up like but but I do believe as you alluded to earlier
10880	15060	that scenarios in which we just remain more or less like we are now and have the current human
15060	20720	condition just keep going for like tens of thousands of years that just seems extremely improbable to
20720	26880	me relative to either like extinction slash dystopia or some radical transformation into some kind of
26880	33840	posthumous condition. Hello this is Robinson Earhart here with the introduction to Robinson's
33840	41600	podcast number 205 and this episode is with Nick Bostrom a philosopher of artificial intelligence
41600	47640	and many other subjects who was most recently professor of philosophy at Oxford University where
47640	55640	he also served as the founding director of the Future of Humanity Institute. Nick is likely best
55640	63320	known in the public eye for his work on the simulation argument which is the idea that we live
63320	71880	not in the familiar physical world that we believe we inhabit but a simulation of one but he's also
71880	77760	well known for his book Super Intelligence which covers the dangers of artificial intelligence
77760	84360	and strategies for dealing with them. In this episode though Nick and I talk about his more
84360	91400	recent book Deep Utopia Life and Meaning in a Solved World which just came out in March actually
91400	99720	and opposed to get as opposed to getting into the existential dangers of AI which Nick tackles in
99720	106440	super intelligence deep utopia considers the sorts of concerns that might arise if everything
106440	114120	actually goes right with AI. So after talking about the alignment problem and some other fundamental
114120	120840	issues in the philosophy of AI we talk about the problems that might come from having perfect
120840	128200	technology from immortality from not having to work anymore from having robots that can do all
128200	136680	your hobbies better than you can do and more. Likes, comments, subscribes, reviews as you know all of
136680	145480	these things are always extremely appreciated. There is also a patreon if you would like a link
145480	154040	to an ad for your rss feed or show notes and to all those of you who are patrons who are geeslings
154040	159560	thank you so much for your support. But now without any further ado I hope that you enjoy
159560	162280	this conversation as much as I enjoyed having in with Nick.
170760	176280	You're one of the most widely recognized philosophers in the in the public sphere
176840	184440	and mainly known in that capacity I think for thinking about dystopian AI scenarios scenarios
184440	190680	and other existential threats so right off the bat I'm wondering what motivated this shift to
190680	200040	thinking about utopias. Now both sides have always been present in my mind and my outlook. Back when
200040	208040	I was writing super intelligence this 2014 book which was in the works six years prior to that I
208040	216120	felt it was more urgent to focus on what could go wrong with AI figure out where the pitfalls were
216120	225000	so that we could avoid them. This was a time at which the whole idea of AI posing any kind of
225640	231000	existential risk or even just having any kind of transformative impact on society was still
231000	237400	far outside the mainstream. The whole idea of the alignment problem was not in the
237400	242200	overton window that were like maybe a handful of people scattered around the world kind of internet
242200	246920	type so we're trying to work on this and it just seemed hugely neglected to me so I thought writing
246920	251880	this book super intelligence trying to develop concepts that would make it possible for people
251880	257160	to start to think more constructively about this and begin to do research on developing scalable
257160	264120	methods for alignment was important. In the intervening years we've seen a big shift now all
264120	269320	the frontier AI labs have research teams specifically trying to develop scalable methods for AI
269320	274200	control and there are many other organizations as well where a lot of the smartest young people I
274200	280120	know now are like flooding into this field and in the last two years we've also seen a big shift
281160	287720	at the level of policy conversation where even top tier policy makers are now beginning to
287720	293000	recognize the transformative impact of future AI developments with statements coming out from the
293000	298120	White House and the UK hosted this global summit on AI. I was just the other week in Brussels for
298120	304920	a however meeting and so to some extent that whole thing is now widely recognized and many of
304920	314120	me feel a work on it. I felt the other side of what happens if things go right with AI had
314120	319720	not yet really been addressed or people talk about it but often in a rather superficial way
320600	324120	but once you start to dig in you realize there are actually quite
324760	329240	deep even philosophical problems that come up when you imagine like what would give human life
329240	334600	purpose and meaning in this condition where in a solved world as I call it like we're all practical
334600	339960	problems that can be solved through progress have already been solved and so that's how the book
341320	345800	it wasn't really so much a plan to write the book it more kind of happened as sometimes
346440	350680	is the way of these things like you start writing a little and then it takes on a life of its own
350680	357160	and you just try to hold on less the thing gradually develops. I can tell that this book
357160	364600	really did take on a life of its own just because of the the various formats in which you wrote it
364600	370520	and I mean there's there's poetry there's dialogue there's standard exposition so it was
370520	378920	clearly a very creative project in a way that I mean philosophical articles are still creative but
378920	385560	it's more of an intellectual creativity than an artistic creativity. Yeah some people maybe think
385560	391000	they should have been a sort of more tough minded editor hovering over me to kind of
391880	398920	but it actually does serve a purpose well for a start it's less a book about conclusions than
398920	405720	it is about questions and exploration and I think this format with multiple different characters
405720	411080	and voices helps you explore several different sides giving each one their due and letting them
411080	419160	speak for themselves different values. So that's one reason for I think the form kind of works
419160	425640	for the content the other is that the goal of the book is well in part to try to give the reader
425640	430680	various concepts and considerations and such that they can you know better think about these things
430680	437480	but another part is to try to put somebody in the right frame of mind for confronting these
437480	443160	if you imagine that will actually perhaps at some point be some group of people who will have to
443160	448200	form some opinions about what we want long term from these AI developments you know whether it's
448200	453720	a few people in some lab or a government or some more humanity-wide deliberation process
454120	459720	these are really difficult questions to deliberate about and I would like people who go into that to
459720	465800	come at it with a certain attitude so a kind of broad-minded generosity combined with some
465800	473880	playfulness and thoughtfulness and open-mindedness which I'm hoping that the book encourages and
473880	480840	I think that those creative elements are also meant to contribute to that. That it is a long
481320	486680	it's a long read so it is also maybe an antidote to short attention spans issue.
488440	496120	Well before we move on to some of the substantive issues just a couple of comments it really was
496680	503080	a great way of delivering a number of concepts to the reader I mean before reading this book
503080	509000	I just thought of utopia as kind of a blanket concept but now I realize and we'll get into it
509000	516360	there are various different types of utopias each of which have their own problems and considerations
516360	527960	to keep in mind and then you said that this book tackles very deep problems about meaning and it's
527960	535320	also I think very useful as a thought experiment for testing an extended thought experiment for
535320	540840	testing our intuitions about meaning in the present so it's not just like it's useful for these
541400	546200	future utopian scenarios it's very useful for thinking about our lives today for instance
546200	552280	there's a passage on shopping that resonated with me very much as somebody who enjoys shopping but
552840	559000	returning to the substance two things that you mentioned right at the outset that I think we
559000	567800	should pin down for our listeners who might not have read superintelligence are one what
567800	574280	superintelligence is since that's vital for the understanding of a deep utopia and then also
574280	581400	what this alignment problem is because even though we will presuppose that it has been solved for a
581400	587240	lot of this conversation I think it's still important to understand what it is. Yeah well so
587240	598040	superintelligence is kind of any intellectual system that radically outperforms all humans in
598040	607560	all cognitive fields and so including you know scientific creativity you know wisdom strategy
607560	617960	like the full range really and the the alignment problem is the technical question of how if you
617960	624600	were if you figured out how to make an AI that's generally capable of learning and planning and
624600	630760	reasoning how could you then steer it in some particular direction to make sure that even
630760	637240	when it becomes much smarter than you its creator it still does what it's supposed to do like what
637240	642760	you intended for it to do that it's kind of on our side as opposed to becoming this antagonistic
642760	647720	force that has different goals than we tried to give it that then works at cross purposes with ours
648840	652120	probably speaking and and there are different ways that different people have tried to make that
652120	661800	precise but that is more or less the gist of it. So yeah this book kind of just assumes our like
661880	669320	postulates really that all of that the alignment problem is solved and also that the like governance
669320	673640	problems are solved to whatever extent they can be solved so that like imagine we actually
673640	678680	don't use this powerful technology to wage war against each other or to oppress each other and
678680	684040	that like all these things that can be solved have been solved to whatever extent possible
684040	688680	in order then to actually get to the point where we can ask these questions about value
692040	696360	I have of course a lot of things to say in my other writing so about these issues of getting
696360	704920	from here to there but there is a risk that if one starts to write about those one never actually
704920	710760	gets to consider what it's all for ultimately like we are and I think this holds in general with our
710760	717240	existences as well we are very busy like putting one foot in front of the other and maybe don't
717240	723560	always take the time to reflect on where we are going or and especially at like at the
723560	729720	civilization level a lot of effort is being put into making progress like growing the economy
729720	736280	improving the technology making things more efficient but nobody really seems to have a
736280	743560	clear notion of you know where does this lead us to in you know in 50 years time in 500 years time
744360	749560	but anyway that's yeah so that that's that's all kind of now I don't know whether super
749560	754120	intelligence is strictly speaking like a premise or like it is certainly is one technology which
754120	758600	if we had that it would unlock a lot of other technological affordances and you would more
758600	764040	quickly approximate this condition of a solved world but you could in principle imagine getting
764040	769560	there just through a slower accumulation of human driven automation you could maybe imagine in the
769560	775400	limit just as you can create a little program that automates a specific task on your computer
776280	781480	or create a robot that makes one action in the car factor or something if we just kept doing that
782120	785560	for a hundred thousand years or a million years maybe eventually we would have software and
785560	791480	robots that could all the tasks and you could kind of at least get close to a similar situation
791480	800840	without super intelligence you said that so both the the dystopian and the utopian sides of the coin
800840	806920	were always on your mind from the beginning but at the time you wrote super intelligence though it
806920	814040	had been gestating for seven or eight years you thought it was more a more pressing problem to
814040	822360	deal with the the alignment problem and the pitfalls of AI but what I'm wondering is so there
822360	829320	was a practical element in writing super intelligence I don't want to get bogged down and how we get
829320	835080	from here to super intelligence because as you mentioned that can take us very far far afield
835080	840920	and we won't actually ever get to the utopian questions but is there also a sort of practical
840920	848120	dimension to writing about deep utopia because you think it is near at hand and we need to
848120	853720	start thinking about it or is it more at this point just a philosophical exercise it's more
853720	860520	the former for me actually I mean I do think we are on a seemingly fast track towards the AI
860520	867400	transition and so it does give the whole set of questions greater practical urgency and
867400	872360	that might be relevant in all sorts of ways like that might for example be points at which we may
872360	878520	need to make some some trade-offs between taking on risks and undergoing this transition sooner versus
879800	886360	delaying it in order to hopefully like make it safer although that exposes us to other
887240	893960	risks independent of AI but and some of this might hinge on how urgent we think it is to
894440	901240	improve upon or get out of the current human condition into something better if you think that
901240	908040	anything on the other side is like bad right then you might just want to preserve the status quo for
908040	912840	as long as possible if you think there is this like very wonderful utopia on the other side then
912840	918600	maybe you think well let's let's make sure we get there before we all just die from aging or
919240	925240	something and it would be worse taking some risk to make sure that we can reach that point so
925960	931080	these questions do I think flow back possibly to decisions that people have to make but the main
931080	937080	use case would be this if people are at one point having to design that trajectory forward and maybe
937080	943400	they with AI advice are able to anticipate where it will lead and like you know one trajectory
943400	951320	maybe leads to some kind of hedonism maximizing outcome another leads to some other complex
951320	957640	situation a third might lead to some kind of radically planetary sized brains optimized for
957640	963480	you know processing scientific knowledge or something and if some people need to make some
963480	968360	judgments about the relative desirability of these then I think it would be good if they had
968360	973400	thought more about it then there are their sort of decisions were not merely reflecting on the
973400	979480	latest thing they had happened to read on twitter or like how they happened to feel that day like
979480	985080	if they wake up in a good mood they think you know it's the future is great and has a lot of
985080	989240	opportunity if they happen to wake up depressed they kind of are a nihilist I think it would be
989240	994280	better if the world just disappeared and ideally whatever decision is made should reflect the
994280	1000680	kind of broader set of considerations and values and perspectives than that and so
1002280	1008120	yeah contributing to that and I think I guess one more thing to be said about that is that
1009560	1013240	the prospect of the future gone well I think is greater if
1016520	1022120	people have this sense of there being great possibilities to realize not just one value
1022120	1027640	but many values that if the future goes well it unlocks this huge I mean would have this
1028280	1034520	super advanced technology plentiful resources a lot of time and there would be a kind of
1034520	1040760	abundance and I think that can help people approach things in a more generous way than if
1040760	1046200	there is extreme scarcity and like one person's gain is another's loss and there is not enough
1046200	1052120	for everybody then it's very hard to be generous right it's easier to be generous if you have a
1052120	1056600	lot and so thinking ahead if we could approach the future in this more cooperative and generous
1056600	1062360	way I think that also reduces the risk of various dystopian outcomes and improves the prospect so
1064600	1071880	those are some thoughts in the background yeah to speak I guess a little bit proverbially though
1071880	1077160	that abundance has its own problems because what is a pleasure without pain and when we
1077160	1083080	have everything I mean this is a big part of of the book in your project is where we find
1083080	1090280	meaning in a world where we have everything and there is no scarcity to drive purpose in right
1090280	1096840	yeah so so that's like the kind of the problem then that one confronts so much of our lives currently
1096840	1103320	are structured around the various practical necessities that we face in our life so you
1103320	1108200	have to go into work every day because you need to get the paycheck which you need to pay the rent
1108920	1115400	you have to brush your teeth because otherwise your gum will decay and you have to you have to
1115400	1121240	do this stat and you have like this like so much of our lives are just like doing something in order
1121240	1127800	for something else to be cost as a result right and that that that kind of fills our life with
1127800	1135000	sort of goal oriented activity but in this old world a very large chunk of all of that
1136680	1145000	would no longer be needed like you wouldn't have to humans wouldn't have to engage in economically
1145000	1149080	productive labor because ais and robots could do everything that needed to be done
1152200	1159160	um yeah you you you wouldn't have to like spend an hour at the gym working out in order
1159160	1163480	to be fit and healthy because you could pop a pill that would give you the same physiological
1164280	1169320	effects and you can kind of go through activity by activity and you find that for many of them
1169320	1173720	you can sort of either cross them out or at least put a question mark on top of them that
1173720	1177800	although you could still do them at technical maturity they would seem to be a kind of
1177800	1185640	pointlessness hanging over them like a dark cloud that maybe remove some of their value and so
1187560	1192120	and and this is like I think yeah part of what would make from our current vantage point
1192120	1200760	some of these scenarios at least prima facie look unattractive we kind of base our self-worth
1200760	1205320	and dignity on the sense of making some doing something useful like you're a breadwinner or
1205320	1210280	maybe you contribute to society in some other way or you like you you look after your kids or
1210280	1216440	your grandkids and you like through your efforts and strivings like you actually make the world
1216440	1219560	better in some place that it could in some way that it could not otherwise be
1220680	1227640	and if that's removed then yeah there is a void that the utopians would have to somehow
1227640	1236840	fill in I think a lot of the fun in this project comes from your imagination and
1237800	1245080	looking at all the various examples of what life might be like and the technologies that might be
1246040	1252840	available in a what you refer to as a technologically mature society and I want to talk
1252840	1259240	about those things but before we do I mentioned earlier that there were I'm not sure if it was
1259240	1266760	five or six I think five different utopias that you discuss in the book whereas I had thought that I
1266760	1270920	just only ever thought there's one kind of utopia it's just heaven and I had left that
1270920	1276440	unanalyzed heaven or something like heaven where everything's perfect but what are the
1276680	1285480	the different types of utopia that you think are relevant to this discussion and and how do they
1285480	1292200	differ and what are some of the basic problems that they confront the utopia dweller with
1292920	1299400	yeah so so this is how I like how I classify them for my purposes but you could think of
1300120	1308360	first category of sort of culture and politics utopia as many of the classical works fall into
1308360	1315800	this category where basically the author tries to imagine a better political system or a better
1315800	1320520	sort of culture so maybe you think there would be a different system of governance or like the
1320520	1325640	gender roles would be defined differently or the way that children relate to their parents
1326600	1330600	um and then it like different authors are different who is about the optimal way to
1330600	1337560	arrange this and the optimal way would that be a utopia um now a slightly more I guess radical or
1337560	1343400	like at least in one dimension more radical uh conception of utopia would be uh an abundance
1343400	1352840	utopia where you imagine that there is plenty to go around so all material needs are abundantly
1352840	1362440	supplied um and you do also find some of these um in the historical records like there was a
1363320	1371480	a kind of medieval fantasy of the the land of kokai that was a kind of peasant vision of like
1371480	1378200	the ideal life the rivers would flow with a wine and roasted turkeys would kind of drop down
1378840	1385400	on the plate and that would be music and dancing and a lot of time for relaxing and and you can see
1385400	1391000	if you were like a kind of agricultural labor like doing backbreaking work from morn to dusk
1391000	1396280	and then just to have barely enough porridge to eat like this would already be like a fantastic
1396280	1401000	conception right like you could eat as much as you want you could just stuff yourself with food
1401000	1410920	and rest all day and like that so so that would be kind of an abundance utopia um and then then
1410920	1416360	we have a kind of post-work utopia which is different from abundance utopia in that not
1416360	1425240	only is there plenty of everything but humans don't need to work to produce it um and this is
1425240	1430040	about as far as most mainstream conversation has gone in the context of AI you do find some
1430120	1436520	economists who are starting to think about um automation and whether it could cause unemployment
1438040	1442360	and if so what would happen like you know you maybe need some universal basic income etc
1444920	1453160	and um like like in in Marx's utopia he doesn't fight very much about it but it seems he imagines
1453160	1460760	that would be a sort of um certainly a culture's politics utopia according to his conception and
1460760	1466760	then that would be like less work and more abundance but that would still be people would
1466760	1471160	still be working so it's kind of but i think there are levels beyond that that you could consider
1471160	1479160	which is um much more radical and poses much more deep problems uh in terms of human values so
1479880	1486760	we have a post-instrumental utopia where it is not just the need for human economic labor
1487400	1493880	that is rendered odious by technological progress but the need for all human labor that is
1494760	1500760	instrumental in character so i mean i mentioned like you might have to labor at the gym to be fit
1500760	1509400	but that you wouldn't have to do in utopia um right now if you want to understand mathematics you
1509400	1515560	have to first study mathematics and spend hours working on exercise problems and like really
1515560	1521800	exerting yourself right but in this condition of technical maturity there would be shortcuts to
1521800	1527480	the same end you could imagine some swarm of nanobots infiltrating your brain and rewiring
1527480	1532600	your synapses into a condition where you now possess mathematical knowledge without you having to
1532600	1542760	put out any effort um so that kind of you know pulls the rug out of a wider range of human activity
1542760	1547800	and exertion not just like the job that you go and do for eight hours a day but most of what fills
1547800	1554680	the rest of your time um and then there's like a final stage beyond that which is i call a plastic
1554760	1561000	utopia which has all the attributes of a post instrumental utopia but in addition um the human
1561000	1569240	organism uh itself becomes malleable uh and subject to our wishes and desires so you could
1569240	1574440	sort of choose what emotions you want to have or what thoughts you want to have or what moral
1574440	1579960	character you want to have or what like physiology you have technologies to kind of reshape yourself
1580600	1588440	uh at will um including your psychological states and and that then results in this kind of
1589240	1595240	solve they're almost this old world where we're like all the kind of hard limitations that we
1595240	1601800	currently face like are removed it seems and and the only thing that remains are kind of our values
1601800	1607240	that i think have been shaped under this condition of scarcity which has like always been there for
1607960	1613960	for humans throughout history like and um it's almost like formed an exoskeleton these practical
1613960	1619800	needs uh and if you were to remove all of those practical necessities there is like the question
1619800	1626760	of what happens to the this soft squishy parts like do they just become a blob like a drug doubt
1626760	1636280	um a kind of pleasure blob uh or can they take some more interesting shape um and um yeah so that's
1636520	1640440	so that's like the the main that there are a couple of early chapters that talk about
1640440	1644520	some of these poor economic and technological but then the bulk of the issue is literally set in
1644520	1652200	this condition of solve it as a solved world at this plastic and post instrumental i'd like to
1652760	1659400	digress for a moment to this mathematics case that you brought up which i i find particularly
1659480	1668440	compelling myself especially because it's an example of something i i said earlier where
1668440	1676920	and you're examining what a mathematician might do in let's say a post instrumental utopia might
1676920	1683560	tell us something about what mathematicians do and value today that might not readily be apparent
1683560	1690280	so in this post instrumental world where we have extremely powerful
1691080	1696520	automated theorem provers that might enumerate all the theorems of math and all the
1697400	1706040	interesting axiomatic systems and there's no longer a role for mathematicians to be
1706040	1711960	providing new proofs do you think mathematicians would still be around because i mean this is
1711960	1720600	something that isn't this is one element of utopia that might not be that far out unlike
1720600	1728200	some other ones because we already have automated theorem provers that are doing work that mathematicians
1728200	1735640	can't do yeah so i mean people do i mean people play chess even though we have computers that can
1735640	1741240	play much better chess or solve crossword puzzles even though there is no need for
1741240	1748040	crossword puzzles to be solved and do a lot of other things currently right so the first answer
1748040	1754600	one might think is yeah yeah sure we'll just do it anyway because it's fun now i think we can't
1755880	1761000	stop at that point we need to like unpack this idea that we do it because it's fun so that could
1761000	1766440	mean we do it because it gives us pleasure like somebody who is fascinated with mathematics
1767080	1773400	probably derive pleasure from engaging in mathematical thinking and occasionally maybe
1773400	1781160	finding a solution but if that's the only reason there would be again shortcuts to attaining pleasure
1782280	1787160	in a solved world you could have more direct forms of brain manipulation that would stimulate
1787240	1792440	your pleasure centers i mean you could imagine it as some sort of a super duper drug with outside
1792440	1798360	effects and addiction potential or more likely some direct way of manipulating the relevant
1798360	1802920	neural structures you know maybe we're all digital at that point anyway and you just
1804200	1810760	and so yeah if all you wanted was to experience positive affect that there would be no need to
1811320	1818680	do mathematics for that purpose so if you do nevertheless think that it would be better to
1818680	1824360	do mathematics you then it seems value something other than just this hedonic state that you now
1825240	1831160	very imperfectly and grudgingly like maybe like some little drips of pleasure is derived every
1831160	1835640	once in a while when somebody who is enjoying mathematics does mathematics and there's probably
1835720	1845320	just a lot of sort of boring futile effort and like uncomfortable straining to get to those occasional
1845320	1851320	little drips of reward that's the stingy way our current reward systems are architected right
1852200	1856840	so you could kind of open the floodgates to that and then have a kind of more blissed out
1857880	1865000	psychological condition and so that's easy to dismiss a lot of people like immediately think
1865000	1871000	that wow that's like what a horrific view of the future that is we're all kind of like junkies
1871000	1876920	sprayed on some sort of flea-infested mattress but with like a super drug dripping into our nucleus
1876920	1886280	incumbents and you know it might well be that ultimately we want and can have more than that
1886280	1890440	but I it is actually a rather deep question whether we shouldn't dismiss too quickly the
1891320	1899480	the super bliss as an element of what we would ultimately choose and have reason to choose
1899480	1907880	perhaps nevertheless I think we can have that and have a whole bunch of other things on top of that
1907880	1916040	that may be satisfy other possible value candidates so certainly if you do think that it is good to
1916760	1922120	engage in certain types of activity you could do that as well there is no reason you could have
1922120	1928680	complex experiences pleasure plus complex experiences plus various forms of goal-directed
1928680	1933160	activity but maybe we're you know and we could talk more about why you would be adopting various
1933160	1941080	goals if if you were in this post-instrumental situation but and and then perhaps some additional
1941160	1947160	elements even beyond that that you could kind of reconstitute some of the prerequisites for
1947160	1954680	instantiating human values even in this solved world condition I think what what motivated my
1954680	1961960	asking this question was reflecting on two conversations I had in the past on the show
1961960	1969160	one was with a renowned number theorist at Columbia University named Michael Harris that
1969160	1974520	we had a couple of years ago so it's a little faint in my mind and another is with Steven Wolfram
1975240	1984360	and we were talking about math in a world in which all of math is essentially automatable and
1985400	1992440	today I think our folk view of what mathematicians are in the business of is just producing new proofs
1993080	2000200	discovering or inventing depending on your philosophical view more facts to be added to
2000200	2012360	this big book of math but what Michael Harris said and what I think Steven Wolfram agreed with
2013560	2018680	is that mathematicians aren't just in the business of producing new facts
2018680	2022760	they're in the business of understanding that's what really drives what they're doing is they
2022760	2029560	want to understand these structures and I was thinking that in this post-instrumental world
2029560	2035640	when mathematicians no longer have to produce proofs they still would be very interested in
2036200	2043400	understanding and maybe teaching mathematics but then this raises another problem that I hadn't
2043400	2051960	considered yet that you discuss in some of the later or the more advanced types of utopia where
2051960	2058840	we have a matrix like system of downloading information where there's no long into our
2058840	2065240	brains where there's no longer any real barrier to understanding you can understand anything
2065240	2070680	as soon as you just downloaded into your mind so there's no there's not really an activity of being
2070680	2075400	a mathematician anymore just if you want to understand the math then you press a button
2075400	2082120	then you understand the math yeah so this is a very like advanced level of technology and it's
2082120	2088440	like a little unclear exactly how close to this we could get but certainly if you imagine a human
2088440	2093960	upload like a digitized version of a human mind and then you have on the outside some machine super
2093960	2101560	intelligence that is able to like understand how the neural network that is you sometimes
2102360	2108760	successfully can grasp mathematical concepts and in other cases fail and what variations of your
2108760	2114200	neural network would be required like what edits to make it as similar as possible to you now but
2114200	2119880	with this extra mathematical understanding I think it possibly would be the case that the
2119880	2126840	job of working out how to adjust the various parameters in your brain to give you this mathematical
2126840	2132280	knowledge could be outsourced to this machine super intelligence that is looking at a model of
2132280	2139800	your brain and probably be able to do that without actually running a detailed simulation of your
2139800	2144280	brain which if it if that were the only way for the super intelligence to work it out you might
2144280	2149080	then think well you would actually then be instantiated in the simulation and you would have to put
2149080	2154840	in the effort although the simulated version of you would do but I think probably you would be able
2154840	2163240	to move up levels of abstraction to more or less obviate that need and so that you would then be
2163240	2170840	able to yeah in effect download skills or knowledge without having to put in time studying
2173400	2177400	and so yeah I think I mean and the teaching of mathematics is even more obvious I mean I think
2177400	2181560	we are very close now to having tutorial systems that would be much better than most
2182200	2186680	like high school mathematics I mean aside from keeping the class disciplined and making sure
2186680	2192280	everybody sits at their desk or whatever like but the actual instruction I'd imagine maybe even
2192280	2199640	chateau pt4 would like be a better explainer than than than most high school teachers because it can
2200680	2205640	keep track of exactly where you are where you get stuck on a particular problem and sort of
2205640	2212760	customize its explanation for you as opposed to doing something generic for the class of 30
2215880	2223000	just curious are you drawing the sort of distinction between chateau pt4 or gbt4 being
2223000	2231400	a good high school teacher and a good university level professor because maybe there is insufficient
2231480	2237640	training data on university level material for chateau pt4 to be able to explain it
2237640	2244440	better than a university professor could I mean I so I don't know exactly I'd imagine there need
2244440	2251080	to be some fine tuning maybe to adapt these large language models for specific use cases in the
2251080	2258200	classroom I'd imagine the standard university courses like you know first and second calculus
2258200	2265000	and like all of that stuff probably it would be able to explain Adam Madden as well as a mathematics
2265000	2269240	professor I think maybe once you get to the research level of mathematics whether it's more
2269240	2275240	matter of taste and then I'm not sure the current language models are quite yet there yet whether
2275240	2280520	it would be as good as like a decent mathematics professor in terms of figuring out whether a
2280520	2287400	research direction is is is promising or whether some proposal for say a phd is like
2288280	2292760	the right level of difficulty that the student could possibly do it in four years and
2293800	2297560	like like I think there there is like well there's obviously less training data written up
2298120	2304440	for ingestion and and also it involves the the most high level human faculties that haven't yet
2304440	2312120	perhaps quite been attained by the current generation of systems a term of art that you
2313000	2320040	introduce in the book that I referenced earlier is technological maturity and
2321320	2324920	I also mentioned that this is one of the the very fun things for me
2324920	2330120	of reading deep utopia because we get to see your imagination at work and all the possible
2330120	2338440	things that a technological mature society might have at its fingertips so to speak
2338440	2345800	so how do you define a technologically mature society and what are some of the things that
2346520	2355000	people might expect to be able to do or have on offer yeah so it's like a condition at which
2358200	2365800	all those technological affordances that are consistent with the loss of physics
2365800	2372920	and for which there is some possible trajectory from where we are now to their development exists
2374680	2381960	so yeah it might be that you can't ever strictly speaking attain perfect technological maturity
2381960	2386840	but in reality you might get some kind of close approximation to it if you have developed all
2387400	2393800	the most useful general purpose technologies is perfect technical technological maturity being
2393800	2399320	able to do anything that's physically possible well I would also have this pathway there but it
2399320	2404680	might just be that if you imagine there are kind of an unlimited number of more and more
2404680	2411880	specialized technologies for doing specific things it might be possible to develop any
2412520	2418760	like small subset of them by like investing your r&d resources but if there is like quadrillion
2418760	2423480	and quadrillion of different very specialized technologies maybe you can't develop the whole
2423480	2429800	set of them but I think that would be sort of ways of getting close to that for most practical
2429800	2434840	purposes like once you have superintelligence and nanotechnology you can sort of come up with
2435560	2439640	the superintelligence can make arbitrary designs and the nanotech can sort of put it together
2440440	2443880	and then like some other things you can I think get pretty close to that
2444760	2451320	um so and and I think we know already at least some lower bounds of what this
2451320	2457720	such a condition of technical maturity would involve that is technologies that we can already
2457720	2463960	have good reason to think are physically possible and for which there is a pathway such that we could
2463960	2469000	at least in the fullness of time under favorable condition get there so superintelligence is one
2469000	2477800	of these like I think perfect virtual reality of the sort that would be indistinguishable to the
2477800	2486760	person inside the physical like virtual reality from from ordinary basic reality would be another
2486760	2494680	affordance I think like cures for aging uploading into computers space colonization at large
2494680	2501160	intergalactic scales perfect newer technology in the sense of being able to control
2502120	2510200	precisely our hedonic and emotional states cognitive augmentations like many other things
2510200	2517000	as well but at least that already gives you a huge set of possible like things you can do right
2517000	2523880	that's technological maturity one of the so the way that for our listeners who haven't
2523880	2528840	looked at the book yet the book is organized so it's kind of like a a course you're teaching to
2528840	2535400	these imaginary students and at various points in the book there are handouts where you've
2535400	2542200	summarized some of the material and I have a handout number two some capabilities at technological
2542200	2550680	maturity on the screen before me and I was just glancing at them and one that jumped out at me
2550680	2559160	is the possibility of Dyson spheres for harvesting the energy output of stars and this comes to mind
2559160	2569960	for two reasons one is well I'll just focus on one but the reason is I've done a number of episodes
2569960	2575400	on string theory and been thinking a lot about string theory lately and one of the big barriers
2575400	2582120	to empirical confirmation of string theory is that we cannot generate enough energy to probe
2582120	2588040	sufficiently small distances to confirm the existence of strings and people have conjectured
2588040	2595800	that the sort of energy you would need is the energy of a star so it looks like we'll be able
2595800	2601960	to confirm or disconfirm string theory in a very useful technology the Dyson sphere
2602680	2608120	but yeah like you mentioned arbitrary sensory inputs reversal of aging
2609480	2613880	uploading of biological brains into computers and all of these things
2616680	2623240	engender their own host of philosophical questions to be asked and answered aligned police
2623240	2629480	bots and automatic treaty enforcement I mean and police bots are already a question being asked
2629480	2635240	today but you know reversal of aging and cures for all diseases that's one that
2635880	2643160	jumps out at me or a couple that jump out at me right now are there any particular problems
2643160	2651320	that would need to be dealt with in these later stage utopias if we just lived forever
2652200	2660280	well I mean the most obvious one would be the size of the population which is able to increase
2660280	2670680	exponentially and if nobody dies then at some point the rate of births would have to match
2671400	2678120	the rate of acquisition of new resources in order to maintain high per capita incomes
2679080	2685000	because at technological maturity there wouldn't be increases in productivity like that's one of
2685000	2688840	the things that drive technological growth now right even if the resources are the same you
2688840	2692840	could have better technologies to make more efficient use of them but that will have maxed
2692840	2698600	out and so then the only way that the economy could grow at that point is if more resources
2698600	2706520	are attained through expanding in space but that can at most happen at a polynomial rate
2707160	2713080	like you have a bubble that is growing in all directions at some significant fraction of the
2713080	2719080	speed of light and the volume of that grows as a polynomial of time whereas the population
2719640	2726600	like could grow exponentially right and so then eventually you would have a Malthusian condition
2728360	2735800	and so in the limit you would need to sort of make sure the the rate of population increase
2736600	2741400	which might be sort of digital minds making copies of themselves right but would kind of at
2741400	2749080	most match the rate at which the the resource endowment increases like another maybe more
2749080	2755160	philosophically interesting question if we eliminated aging and then I mean right now what
2755160	2761000	would happen if you eliminated aging is that people would die from accidents and wars but maybe
2761000	2766840	with a lifespan of a few thousand years if you kind of calculate the rate of death by accident
2768680	2775400	now hopefully we will bring down the rate of accidents and and war certainly in utopia you'd
2775400	2780360	imagine that to go way down and if you could upload yourself to computers then you could make backup
2780360	2786040	copies and stuff and then the rate of accidents could kind of become arbitrarily small so then
2786040	2796920	then you might have very long lifespans even astronomical and and that presents more interesting
2796920	2803240	philosophical questions for human values in that well first of all we don't know what would happen
2803240	2809080	to like if you remained a human mind of like a brain with your current size and the number of
2809080	2812920	synapses and stuff like we know you can continue to learn and develop and grow for like a hundred
2812920	2817400	years but we don't even know what would happen if a human just kept living for 400 years like would
2817400	2828680	you go stale and rigid and like so and even if that didn't happen in a sort of like neurological
2828680	2839640	way like then it might still be that and it's a hard question to sort of gauge but that the
2839640	2845160	number of different types of things that can be done with a human mind and body in a human world
2846280	2851640	is finite and is that number small enough that it would become relevant so that at some point you
2851640	2857880	would just run out of interesting new things to do and if so how long like I mean I think
2857880	2862280	given that we are finite there has to be some number of years after which you would literally
2862280	2868920	be doing the same thing you'd already done but if that is kind of 10 to the power of a hundred
2869000	2873720	or something we might not have to care very much because we'd die anyway before that from the
2873720	2879960	heat death of the universe but if the answer is like 10,000 years like after 10,000 years you'd
2879960	2886360	basically just repeat yourself then either you'd have to accept that such lives will have a diminution
2886360	2891800	of a certain kind of novelty and that may be a price to admittance and this kind of extreme
2891800	2898600	longevity in utopia or you would have to perhaps start to level up after you have spent at 10,000
2898600	2904200	years being a human maybe then you would want to become a transhuman like increase your intellectual
2904200	2909480	faculties or something and you could kind of start to tackle the next level of challenges and
2911000	2919240	keep developing that way but it is I think it is one direction in which you can kind of
2920440	2925800	take our current human values and where they start to become strained if you just imagine the
2925800	2930760	current human lifespan with our current values and our current faculties but just extended
2930760	2935480	and like prevented from physical disease like at some point I think that would
2938600	2942280	possibly become unattractive like there are still a lot of things in human life that don't
2942280	2948920	require novelty I mean like a cup of tea is like about as good like the 10,000s time as it is the
2948920	2954760	second time or first time right it they're like renewable pleasures and sometimes you could run
2954760	2958600	the argument that sometimes the best things in life are the little things that it's not like
2959480	2967240	like the big dramatic narrative climaxes but it's like you know the smell of autumn air in
2967240	2973800	the evening along a stroll along the coast or like looking into your loved one's face or a little
2974360	2980760	you know cup of tea or whatever like these little things sometimes you know you could run the argument
2980840	2987000	that that's a fairly high quality value although we give them kind of short shrift
2989080	2993160	an interesting connection to the philosophical literature going back
2993960	3001880	hundreds of years I mean is how conceptions of personal identity might shift in this world
3001880	3009240	where we live for we live indefinitely I think and you should correct me if I'm wrong but I think
3009240	3016280	it was Locke who proposed that personal identity has something to do with memory and psychological
3016280	3026760	connectedness and maybe it was Derek Parfit in his sort of ongoing discourse with David Lewis who
3027640	3034360	questioned whether somebody who lived who wasn't immortal would eventually no longer be the same
3034360	3041400	person that they started out as because they would not remember let's say their childhood
3041400	3047800	F there would no longer be any psychological connection to a person stage thousands and
3047800	3055160	thousands of years earlier and in this sort of utopia maybe we would bypass this by getting
3056280	3061800	memory implants or memory chip implants so that we could just continually
3062760	3069160	add new memories and maintain internal psychological connectedness yeah um
3070120	3077320	I mean I guess there is first of all like I would want to make retain the distinction between
3077320	3084760	long time and infinity like immortality I think you should mean not dying not not just taking
3084760	3090840	a while before you die like conceptually and it is important because immortality in that strict
3090840	3097400	sense just might be impossible in our physical universe and be the preserve of say more theological
3097400	3103880	scenarios and from the blink view of eternity like whether you live for like 80 years or 80
3103880	3110040	thousand years it's kind of in some sense the same right it's an infinitesimal short relative to
3110040	3122920	infinity either way now I do think that yes increasing I call it time time suits but the
3122920	3130440	idea that we could make some modifications tasks that make us better able to survive radical change
3130440	3137000	without having our identity eroded or our values corrupted so the most obvious thing would be to
3137000	3142680	improve our episodic memory so that you actually don't just forget what you've been through and
3142680	3149800	that could make you sort of remain relevantly the same for longer than if you just have like the
3149800	3155480	memory of a goldfish and and you could imagine some other tricks as well that would allow us to
3155480	3161400	sort of endure for longer periods of time and and upgrading our capacities in different ways could
3161480	3171400	help with that um but even if it came to a point where you would have to accept some loss of personal
3171400	3176840	continuity in in whatever relevant sense I mean it might be acceptable like if you if you were
3176840	3181480	have a five-year-old and you know that if there were a pill that would just arrest their development
3181480	3184760	that they would remain a five-year-old like in some sense maybe a year from now that would be
3184760	3190760	more similar to what they're now than if they're just allowed to freely develop uh but we might
3190760	3196520	still think it's better for them to grow up like in some sense you are the same person now as you
3196520	3203880	were when you were five but in realistically you're also quite different right in many ways uh and uh
3203880	3212200	and maybe that's not an unambiguously bad thing like and so I think the combination of these that um
3213160	3223560	a uh we can reduce the negative effects of various kinds of uh corroding uh consequences of the passage
3223560	3230360	of time through improving memory and other certain other positive attributes like also including like
3230360	3237560	maybe extending your planning horizon etc and and then also accepting some degree of of change
3237560	3243320	even if it does mean that you eventually move further away from what you once were if there's
3243320	3247640	still a difference between that and just dying because at any given point in time you might
3247640	3253240	look forward to hundreds of years of existing in some very similar things your current condition and
3253240	3258440	and you would gradually sort of you know see new ways of doing things or being or new experiences
3259000	3263480	that that feels a lot more optimistic to me than it then like a kind of you're gonna be shot
3264440	3271960	tomorrow morning right it's a um this the smoother and gentler way of losing our connection with the
3271960	3279720	past seems perhaps prudentially preferable um even to the extent that it is unavoidable relative to
3279720	3287400	yeah um the alternative okay not that a lot of this or all of it hasn't all been speculative
3288120	3292600	but i have a particularly speculative question to ask so when i
3294280	3300600	spoke with avi lobe the harvard astrophysicist this this brings us back to dyson spheres
3301160	3311160	he conjectured that umuamua this comet that passed us by in i believe 2017 might have been
3311880	3323160	part of a dyson sphere and you have made your own very uh i don't know if splashy is an adjective but
3324280	3331160	powerful uh controversial conjectures about for instance whether or not we might be in a simulation
3331160	3339720	and i'm wondering if you think it is possible or likely even that there are societies out there
3339720	3346760	that have already reached uh for instance a post scarcity utopian stage because i think the
3347560	3354120	having a dyson sphere if whoever sent uamua our way had a dyson sphere we would classify them
3354120	3361400	probably as being in a post scarcity period assuming that they still have other dyson spheres
3361400	3366440	yeah i mean so that i mean post scarcity they might not i mean it depends a lot on what their goals are
3367160	3373000	and um it might be that the dyson sphere of resources is nowhere near enough for what they
3373000	3382440	are trying to do um and so it is to some extent a human con like um it's not like a very precisely
3382440	3387880	defined concept it's like you could say like from few humans so you could say that we are already
3388520	3395560	uh somewhat close in some respects well if you're if you are like fortunate enough to live in a in a
3395560	3401640	in a rich country and you have a i don't know like you're healthy and trying to good education like
3401640	3410440	probably most of your listeners like it's probably the case that um uh if if you wanted to you wouldn't
3410440	3416120	really have to work to survive like or if you select say say you worked hard really hard for
3416120	3420760	five or ten years and saved up all the money you probably could then move to thailand or something
3420760	3426920	and buy a small hot near a beach and then have enough to eat and be physically healthy and you'd
3426920	3430920	have a computer you could eat access the internet or whatever for the rest of your life without
3430920	3437400	having to work like it's at least tantalizingly close uh but yeah people choose to continue to
3437400	3444280	strive because they want more than just the basic necessities of life right like and in particular
3444280	3449400	we want to have more than other people and so there's a lot of this kind of um zero some
3450040	3458600	status consumption going on with humans um and that could drive scarcity up to astronomical
3458600	3465400	levels right like you yeah you have your own dyson sphere but uh like i have four dyson spheres so
3465400	3472040	like you still need to try to um catch up and and and intergalactic civilization might have
3472040	3476600	practical reasons as well like maybe they fear some other intergalactic civilization that might
3476600	3484280	have a larger fleet of worships or something and so um but the difference is like um even if human
3485000	3492680	desires are insatiable it doesn't mean there will always be a need for human work like you
3492680	3500520	could reach a condition where even though like you maybe are worth a trillion dollars and you
3500520	3505000	would like to make another trillion dollars there's just nothing you can do with your own labor that
3505000	3509800	would make you any significant amount of money because all the work is better done by machine
3511640	3516520	and if the most you could make is the minimum wage then once you're a trillion year like
3516520	3520360	there is no point really working right because it's trivial compared to what you just earned from
3520360	3524680	your capital and and it might even be that by doing the work yourself you like expend more
3524680	3530200	calories that cost you more than the actual value of the labor so you could still end up in this kind
3530200	3538200	of condition of um uh unemployment or post work even if there are still needs that have or desires
3538200	3543720	that haven't been fully uh satisfied um now you asked whether I think there are already some that
3543720	3550280	have attained either poor scarcity or I guess you could generalize it into technological maturity
3550280	3555800	and I think if the universe is infinite as it seems to be like if we have the simplest topology
3555800	3564040	and an opener flat spacetime um then definitely that would be uh such civilizations in fact infinitely
3564040	3571240	many of them out there but um if I had to guess I'd say none in our in the observable universe and so
3571240	3578360	that there would be infinitely many of these but they would have low density and so um um we might
3578360	3583480	be out of causal contact forever with the nearest other one um that certainly would help explain the
3583480	3589560	Fermi paradox um but it's not the only possible explanation so we can't rule out that we could
3589560	3597240	share the observable universe with some other civilizations as well. Speaking of intergalactic
3597240	3605480	civilizations and intergalactic warfare obviously that would be a huge barrier to us reaching a
3605560	3615560	utopian state if we had to ascend off uh an alien invasion but do you see any other barriers
3615560	3623000	besides just our ability to produce a super intelligent technology or super intelligent
3623000	3632520	AI other barriers beyond that to our reaching utopian states? Uh yes I mean first of all it would
3632520	3637640	be uh crucially super intelligence is well aligned um because otherwise yeah that might
3637640	3645480	be our undoing um and second if you have scenarios uh multipolar in character like if you have many
3645480	3654120	different entities ultimately with our own AI uh assistance that then a lot of the same dynamics
3654120	3664440	that uh we see on the planet today with arms races and um oppression and warfare and exploitation of
3664440	3671160	the global commons by sort of spewing pollutants into the atmosphere overfishing the oceans all of
3671160	3677240	that could still occur and and indeed could be intensified in this kind of hyper competitive
3677240	3683640	economy that might be created um if on the other hand you don't have a multipolar but like a unipolar
3683720	3687880	or as I call like a singleton scenario where it gets all consolidated then there is the obvious
3687880	3698120	question of um who controls that uh singleton right and how uh benevolent and wise is whatever
3698120	3704440	the mechanism whether it's like a global democracy or a dictatorship or whatever it is is like so I
3704440	3711080	think we can identify several broad categories of things that need to fall in place so one is
3711080	3716120	to have this kind of uh deep utopia a you would need super advanced technology
3717480	3722360	like otherwise you just can't do all of the if you right now we can't cure the
3722360	3729000	jit with a cancer in many cases right and that means our world will fall short of what it should be
3729880	3737560	so the super advanced technology but then I think also we'd need um a fairly high level of
3737560	3745000	cooperation slash good governance to prevent sort of negative some conflict and then we'd
3745000	3752120	also need some adequately high level of wisdom in how we use these great uh technological powers
3752120	3762120	that we have um and it might be that the wisdom is it has a kind of threshold effect where if
3762200	3768040	you're above that threshold even if you're still uh unwise in many deep ways and have many
3768680	3773480	erroneous beliefs and misconceptions you might have enough wisdom to realize that you are fallible
3773480	3779560	and to start to seek out ways to fix you can reflect on your own shortcomings and gradually
3779560	3785240	develop ways of remedying it but if you're sort of below that wisdom level you might
3785240	3791560	be more likely to lock in your current prejudices uh or to shoot your foot off when you're trying to
3791560	3796520	like develop ways to fix it and it's kind of actually an open question I think where humanity
3796520	3801000	is relative to that because you could ask the same question at the collective level like are we
3801000	3804760	I think we are maybe close to the threshold level but I'm not sure whether we are just under
3804760	3815320	or just above it um and um and then we might need some bit of luck as well on top of that um but um
3816200	3825720	um I think yeah although these are kind of extreme technological postulates and you might
3825720	3831880	see them very wide wild ideas I still think um they might actually not be completely unrealistic
3831880	3840520	even within our lifetime if this whole AI transition happens um and it's if you sort of
3840520	3847480	zoom out and look at the human history like uh from from its inception like you know a few
3847480	3853880	hundred thousand years ago to today and you plot that in any possible way you you might want to
3853880	3860440	like whether it's like the amount of you know um you know the GDP let us say or like some sort of
3860440	3866840	the energy expended by human civilization or like in any it does have this you like if you
3866840	3872200	plot it on a linear scale you just see a flat line that just spikes up at the end of the graph so
3872200	3878840	then you have to go to like a log plot or to even see it bending but even then it bends up and we're
3878840	3884200	really sitting right now at this like ridiculous anomaly um where what we are currently taking
3884200	3890840	to be the normal human condition is in any which way look at it like just extreme historical anomaly
3890840	3898600	like a tiny little thin coat of paint on on like a giant uh battleship of human history right it's
3898600	3904200	like it and and then now we think it's like a radical conception to think well what if that changes
3904200	3908920	in the future whereas in fact if you sort of zoom out you look like we are right now in the middle
3908920	3915160	of this like explosion of something and we don't know what it's going to end up like but um but I
3915160	3920200	do believe as as you alluded to earlier that scenarios in which we just remain more or less
3920200	3925240	like we are now and have the current human condition just keep going for like tens of thousands of
3925240	3931240	years that just seems extremely improbable to me relative to either like extinction slash dystopia
3931240	3938680	or some radical transformation into some kind of post-human condition well Nick I I really
3938680	3944840	enjoyed deep utopia and I came out of it with so many ideas and concepts that I hadn't considered
3944840	3949800	beforehand so thanks so much for writing the book and thinking about these really interesting
3949800	3954520	pressing problems and thanks a lot for coming on the show to talk to me about it well thanks to you
3954520	3961080	and uh to the little cat there as well
