1
00:00:00,000 --> 00:00:06,880
Welcome to the clothing ceremony of UC Berkeley's AI Hackathon.

2
00:00:06,880 --> 00:00:09,680
I want to call on stage the awesome,

3
00:00:09,680 --> 00:00:14,280
incredible executive director of Skydeck, Caroline Winat.

4
00:00:14,280 --> 00:00:16,400
Thank you, Rene.

5
00:00:16,400 --> 00:00:20,160
Hi, everybody. How are you doing?

6
00:00:20,160 --> 00:00:24,480
Awesome. You ready to hear who won the Hackathon?

7
00:00:24,480 --> 00:00:26,880
Yes, you are. How many hackers here?

8
00:00:26,880 --> 00:00:28,360
How many in the audience?

9
00:00:28,360 --> 00:00:30,880
Oh, nice. Very good.

10
00:00:30,880 --> 00:00:34,440
All right. We're going to get started because I think you want to hear Andre.

11
00:00:34,440 --> 00:00:36,200
Yes, you want to hear Andre.

12
00:00:36,200 --> 00:00:37,520
Yes, you want to hear Andre.

13
00:00:37,520 --> 00:00:39,680
All right. Let's quick run through.

14
00:00:39,680 --> 00:00:43,080
You want to hear some cool facts about what has been happening.

15
00:00:43,080 --> 00:00:44,760
This is what we're going to do today.

16
00:00:44,760 --> 00:00:47,040
We're going to get to our pitches soon.

17
00:00:47,040 --> 00:00:48,920
This is some pictures.

18
00:00:48,920 --> 00:00:50,880
All you hackers, did we have fun?

19
00:00:50,880 --> 00:00:52,480
Did we have a good time?

20
00:00:52,480 --> 00:00:54,800
I had an absolute blast.

21
00:00:54,800 --> 00:00:57,600
Yes, there were llamas for sure.

22
00:00:57,720 --> 00:00:59,160
I was there most of the time.

23
00:00:59,160 --> 00:01:01,120
I was not there at 3 a.m.

24
00:01:01,120 --> 00:01:03,440
But I was so impressed with all of you.

25
00:01:03,440 --> 00:01:05,840
You hacked your hearts out.

26
00:01:05,840 --> 00:01:08,360
And I'm so proud of all of you,

27
00:01:08,360 --> 00:01:12,120
whether on stage or in audience, you're all completely awesome.

28
00:01:12,120 --> 00:01:15,120
All right. How many people they took to make this happen?

29
00:01:15,120 --> 00:01:18,960
This giant number, 371.

30
00:01:18,960 --> 00:01:21,800
UC Berkeley Skydeck, which I represent,

31
00:01:21,800 --> 00:01:25,720
and Calhacks Educational Program and Student Organization.

32
00:01:25,760 --> 00:01:29,360
So I think we did a pretty decent job of getting this all together.

33
00:01:29,360 --> 00:01:30,920
This is how it breaks down.

34
00:01:30,920 --> 00:01:35,520
Hackathon at Berkeley Directors, Skydeck staff, sponsors.

35
00:01:35,520 --> 00:01:37,880
We're going to give some love to sponsors.

36
00:01:37,880 --> 00:01:40,520
As I mentioned, we're an educational program.

37
00:01:40,520 --> 00:01:42,680
Calhacks is a student organization.

38
00:01:42,680 --> 00:01:46,040
This is all because of the sponsors.

39
00:01:46,040 --> 00:01:47,400
So we're going to give them a ton of love

40
00:01:47,400 --> 00:01:48,640
when they come up on stage.

41
00:01:48,640 --> 00:01:50,800
You with me? Awesome.

42
00:01:50,840 --> 00:01:55,320
Okay, 140 judges, 100 plus volunteers,

43
00:01:55,320 --> 00:01:58,760
and 80 mentors hanging out helping everybody.

44
00:01:58,760 --> 00:02:00,680
Let me tell you a bit about Skydeck.

45
00:02:00,680 --> 00:02:02,920
Who hasn't heard of Skydeck?

46
00:02:02,920 --> 00:02:04,200
Anybody?

47
00:02:04,200 --> 00:02:05,320
A couple of you.

48
00:02:05,320 --> 00:02:09,320
Skydeck, UC Berkeley's flagship accelerator.

49
00:02:09,320 --> 00:02:11,880
We host 250 startups a year.

50
00:02:11,880 --> 00:02:15,560
Our accelerator track gets $200,000 in investment

51
00:02:15,560 --> 00:02:17,760
from our dedicated Venture Fund.

52
00:02:17,760 --> 00:02:19,160
Pretty cool.

53
00:02:19,160 --> 00:02:21,600
Let me tell you about Berkeley Skydeck Fund,

54
00:02:21,600 --> 00:02:23,640
our dedicated Venture Fund.

55
00:02:23,640 --> 00:02:25,480
Investing in about 40 startups a year.

56
00:02:25,480 --> 00:02:28,600
That's a lot of startups for an adventure fund, by the way.

57
00:02:28,600 --> 00:02:30,480
The 200K investment.

58
00:02:30,480 --> 00:02:33,000
And who wants to apply to Skydeck?

59
00:02:33,000 --> 00:02:36,080
July 16, I want to see all of your startup applications

60
00:02:36,080 --> 00:02:37,040
coming in.

61
00:02:37,040 --> 00:02:39,400
That's in a month.

62
00:02:39,400 --> 00:02:43,840
And hackathons at Berkeley are an amazing student organization.

63
00:02:43,840 --> 00:02:48,680
Truly extraordinary people who helped put this on this event.

64
00:02:48,720 --> 00:02:50,680
This is, of course, what they do.

65
00:02:50,680 --> 00:02:51,760
They do hackathons.

66
00:02:51,760 --> 00:02:54,000
They've been doing it for 10 years.

67
00:02:54,000 --> 00:02:57,160
They do about 2,500 students a year.

68
00:02:57,160 --> 00:03:00,120
And, of course, they reach a ton of universities.

69
00:03:00,120 --> 00:03:02,760
How many people here not from Cal?

70
00:03:02,760 --> 00:03:04,320
Hacking not from Cal?

71
00:03:04,320 --> 00:03:05,480
Fantastic.

72
00:03:05,480 --> 00:03:06,600
Welcome.

73
00:03:06,600 --> 00:03:09,720
Berkeley is a place where we bring great talent in.

74
00:03:09,720 --> 00:03:11,040
Y'all are great talent.

75
00:03:11,040 --> 00:03:11,880
We brought you here.

76
00:03:11,880 --> 00:03:13,480
That's what we do.

77
00:03:13,480 --> 00:03:15,960
That's what Berkeley Hackathons does.

78
00:03:15,960 --> 00:03:21,840
Come to their 11th big hackathon in San Francisco in October.

79
00:03:21,840 --> 00:03:23,080
Check them out on social media.

80
00:03:23,080 --> 00:03:25,840
Get on that LinkedIn, all of that.

81
00:03:25,840 --> 00:03:27,440
Who's coming to San Francisco?

82
00:03:27,440 --> 00:03:28,440
Y'all coming?

83
00:03:28,440 --> 00:03:29,080
Yes!

84
00:03:29,080 --> 00:03:31,000
OK, fantastic.

85
00:03:31,000 --> 00:03:31,840
All right.

86
00:03:31,840 --> 00:03:34,120
Thank you to our partners, all of you

87
00:03:34,120 --> 00:03:37,080
who brought your hackers here, including our friends

88
00:03:37,080 --> 00:03:38,320
down to the South Bay.

89
00:03:38,320 --> 00:03:41,480
Thank you for joining us and all the other great universities.

90
00:03:41,480 --> 00:03:42,560
Fantastic.

91
00:03:42,560 --> 00:03:44,400
Really happy to have you.

92
00:03:44,400 --> 00:03:46,600
Do you want to hear Andre?

93
00:03:46,600 --> 00:03:49,000
Do you want to hear Andre?

94
00:03:49,000 --> 00:03:49,880
Yes!

95
00:03:49,880 --> 00:03:54,040
Please give a huge round of applause for our keynote speaker,

96
00:03:54,040 --> 00:03:55,360
founding member of OpenAI.

97
00:03:55,360 --> 00:03:56,120
I need the applause.

98
00:03:56,120 --> 00:03:57,080
Come on, keep going.

99
00:03:57,080 --> 00:03:59,560
Andre, come on up.

100
00:03:59,560 --> 00:04:01,120
Carpathian.

101
00:04:01,120 --> 00:04:04,080
Yes, big applause.

102
00:04:04,080 --> 00:04:05,080
Thank you.

103
00:04:07,640 --> 00:04:10,040
Hi, everyone.

104
00:04:10,040 --> 00:04:11,480
Yeah, so thank you for inviting me.

105
00:04:11,480 --> 00:04:14,440
It's really a great pleasure to be here.

106
00:04:14,440 --> 00:04:17,680
I love, love, love hackathons.

107
00:04:17,680 --> 00:04:19,600
I think there's a huge amount of energy,

108
00:04:19,600 --> 00:04:21,560
a huge amount of creativity, young people

109
00:04:21,560 --> 00:04:25,120
trying to do cool things, learning together, creating.

110
00:04:25,120 --> 00:04:26,880
It's just like my favorite place to be,

111
00:04:26,880 --> 00:04:28,480
and I've had my fair share of hackathons.

112
00:04:28,480 --> 00:04:33,080
So really a great pleasure to be here and talk to you today.

113
00:04:33,080 --> 00:04:36,040
So one thing is this is bigger than I expected

114
00:04:36,040 --> 00:04:37,280
when they invited me.

115
00:04:37,280 --> 00:04:39,240
So this is really large here.

116
00:04:39,240 --> 00:04:42,600
I kind of feel like actually the scale of the hackathon

117
00:04:42,600 --> 00:04:44,160
is quite large.

118
00:04:44,160 --> 00:04:46,400
And I guess one thing I wanted to start with

119
00:04:46,400 --> 00:04:49,280
is that just in case you're wondering,

120
00:04:49,280 --> 00:04:51,600
this is not normal for AI.

121
00:04:51,600 --> 00:04:53,720
I've been in AI for about 15 years,

122
00:04:53,720 --> 00:04:55,600
so I can say that with confidence.

123
00:04:55,600 --> 00:04:58,560
And it's kind of just grown a lot.

124
00:04:58,560 --> 00:05:02,560
So for me, AI is a couple of hundred academics

125
00:05:02,560 --> 00:05:05,280
getting together in a workshop of a conference

126
00:05:05,280 --> 00:05:08,800
and talking together about some esoteric details

127
00:05:09,240 --> 00:05:10,400
of some math.

128
00:05:10,400 --> 00:05:12,480
And so this is what I'm used to.

129
00:05:12,480 --> 00:05:15,400
This is when I entered AI about 15 years ago.

130
00:05:15,400 --> 00:05:18,160
You're working with, say when you're training neural networks,

131
00:05:18,160 --> 00:05:20,440
you're working with these tiny digits from MNIST.

132
00:05:20,440 --> 00:05:22,280
You're training a restricted Boltzmann machine.

133
00:05:22,280 --> 00:05:24,720
You're using contrastive divergence to train your network.

134
00:05:24,720 --> 00:05:28,840
And then you're scrutinizing these on your first light

135
00:05:28,840 --> 00:05:30,600
to make sure that the network trained correctly.

136
00:05:30,600 --> 00:05:32,440
And I know none of that makes any sense

137
00:05:32,440 --> 00:05:34,280
because it's been so long ago,

138
00:05:34,280 --> 00:05:36,080
but it was a different vibe back then

139
00:05:36,080 --> 00:05:37,760
and it was not as crazy.

140
00:05:37,800 --> 00:05:40,120
I think things have really gotten out of proportion

141
00:05:40,120 --> 00:05:42,000
to some extent, but it is really beautiful

142
00:05:42,000 --> 00:05:43,280
to see the energy.

143
00:05:43,280 --> 00:05:47,240
And today, 15 years later, it looks a lot more like this.

144
00:05:49,920 --> 00:05:52,120
So this is, I guess, where AI is today.

145
00:05:52,120 --> 00:05:55,000
And that's also why this event is the largest I expect.

146
00:05:55,000 --> 00:05:57,000
So yeah, NVIDIA, the manufacturer of GPUs,

147
00:05:57,000 --> 00:05:58,520
which is used for all the heavy lifting

148
00:05:58,520 --> 00:05:59,600
for our neural networks,

149
00:05:59,600 --> 00:06:03,040
is now the most valuable company in the United States

150
00:06:03,040 --> 00:06:04,440
and has taken over.

151
00:06:04,440 --> 00:06:06,520
And this is the day that we live in today

152
00:06:06,520 --> 00:06:08,800
and why we have so many hackathons like this and so on,

153
00:06:08,800 --> 00:06:10,120
which I think is quite amazing,

154
00:06:10,120 --> 00:06:11,440
but definitely unprecedented.

155
00:06:11,440 --> 00:06:13,320
And this is a very unique point in time

156
00:06:13,320 --> 00:06:16,720
that many of you maybe are entering the AI field right now.

157
00:06:16,720 --> 00:06:18,280
And this is not normal, it's super interesting,

158
00:06:18,280 --> 00:06:20,320
super unique, there's a ton happening.

159
00:06:20,320 --> 00:06:22,320
Now, I think fundamentally the reason behind that

160
00:06:22,320 --> 00:06:25,120
is that I think the nature of computation

161
00:06:25,120 --> 00:06:26,520
basically is changing.

162
00:06:26,520 --> 00:06:29,640
And we're kind of have like a new computing paradigm

163
00:06:29,640 --> 00:06:30,960
that we're entering into.

164
00:06:30,960 --> 00:06:31,920
And this is very rare.

165
00:06:31,920 --> 00:06:34,760
I kind of almost feel like it's the 1980s

166
00:06:34,760 --> 00:06:36,360
of computing all over again.

167
00:06:36,360 --> 00:06:38,560
And instead of having a central processing unit

168
00:06:38,560 --> 00:06:41,480
that works on instructions over bytes,

169
00:06:41,480 --> 00:06:42,800
we have these large language models,

170
00:06:42,800 --> 00:06:45,800
which are kind of like the central processing unit

171
00:06:45,800 --> 00:06:49,320
working on tokens, which are a little string pieces instead.

172
00:06:49,320 --> 00:06:50,680
And then in addition to that,

173
00:06:50,680 --> 00:06:51,960
we have a contact window of tokens

174
00:06:51,960 --> 00:06:54,000
instead of a RAM of bytes.

175
00:06:54,000 --> 00:06:56,400
And we have a coolant of disk and everything else.

176
00:06:56,400 --> 00:06:58,000
So it's a bit like a computer.

177
00:06:58,000 --> 00:06:59,720
And this is the orchestrator.

178
00:06:59,720 --> 00:07:02,120
And that's why I call this like the large language model,

179
00:07:02,120 --> 00:07:03,520
LMOS.

180
00:07:03,560 --> 00:07:05,560
And I've sort of like tweeted about this

181
00:07:05,560 --> 00:07:07,280
in some more detail before.

182
00:07:07,280 --> 00:07:09,520
And so I see this as a new computer

183
00:07:09,520 --> 00:07:12,160
that we're all learning how to program

184
00:07:12,160 --> 00:07:14,600
and what it's good at, what it's not as good at,

185
00:07:14,600 --> 00:07:16,280
how to incorporate into products,

186
00:07:16,280 --> 00:07:18,920
and really how to squeeze the most out of it.

187
00:07:18,920 --> 00:07:20,760
So that I think is quite exciting.

188
00:07:20,760 --> 00:07:23,960
And I think maybe many of you have seen the GPT-40 demo

189
00:07:23,960 --> 00:07:26,680
that came out from OpenAI two, three weeks ago

190
00:07:26,680 --> 00:07:27,720
or something like that.

191
00:07:27,720 --> 00:07:29,720
And you're really starting to get a sense that

192
00:07:29,720 --> 00:07:33,120
this is a thing that you can actually talk to.

193
00:07:33,120 --> 00:07:37,760
And it responds back in your natural interface

194
00:07:37,760 --> 00:07:40,440
of like audio and it sees and hears and can paint

195
00:07:40,440 --> 00:07:42,200
and can do all these things.

196
00:07:42,200 --> 00:07:44,240
I think potentially many of you have seen this movie.

197
00:07:44,240 --> 00:07:45,920
If you haven't, I would definitely watch it.

198
00:07:45,920 --> 00:07:49,120
It's extremely inspirational for us today, movie Her.

199
00:07:49,120 --> 00:07:52,600
And actually kind of presently in this movie,

200
00:07:52,600 --> 00:07:55,360
when this main character here talks to the AI,

201
00:07:55,360 --> 00:07:57,880
that AI is called an OS, an operating system.

202
00:07:57,880 --> 00:08:00,280
So I think that's very present from that movie.

203
00:08:00,280 --> 00:08:02,920
And it's a beautiful movie and I encourage you to watch it.

204
00:08:02,920 --> 00:08:05,200
Now, the thing is that in this movie,

205
00:08:05,200 --> 00:08:07,200
I think the focus is very much on like the emotional

206
00:08:07,200 --> 00:08:09,280
intelligence kind of aspects of these models.

207
00:08:09,280 --> 00:08:12,400
But these models in practice in our society

208
00:08:12,400 --> 00:08:14,440
will probably be doing a ton of problem solving

209
00:08:14,440 --> 00:08:15,800
in the digital space.

210
00:08:15,800 --> 00:08:17,920
And so it's not just gonna be a single digital entity

211
00:08:17,920 --> 00:08:21,000
that kind of in some weird way resembles a human almost

212
00:08:21,000 --> 00:08:22,160
in that you can talk to it.

213
00:08:22,160 --> 00:08:24,200
But it's not quite a human, of course.

214
00:08:24,200 --> 00:08:26,000
But it's not just a single digital entity.

215
00:08:26,000 --> 00:08:28,720
Maybe there's many of these digital entities.

216
00:08:28,720 --> 00:08:30,320
And maybe we can give them tasks

217
00:08:30,320 --> 00:08:32,040
and they can talk to each other and collaborate

218
00:08:32,040 --> 00:08:33,720
and they have fake slack threads.

219
00:08:33,720 --> 00:08:36,680
And they're just doing a ton of work in the digital space.

220
00:08:36,680 --> 00:08:39,640
And they're automating a ton of digital infrastructure.

221
00:08:39,640 --> 00:08:41,640
Not just a digital infrastructure,

222
00:08:41,640 --> 00:08:43,600
but maybe physical infrastructure as well.

223
00:08:43,600 --> 00:08:46,120
And this is kind of an earlier stages, I would say,

224
00:08:46,120 --> 00:08:48,200
and will probably happen slightly lagging

225
00:08:48,200 --> 00:08:50,200
behind a lot of digital innovations

226
00:08:50,200 --> 00:08:53,160
because it's so much easier to work with bits than atoms.

227
00:08:53,160 --> 00:08:55,200
But this is another movie that I would definitely point you to

228
00:08:55,200 --> 00:08:56,400
as one of my favorites.

229
00:08:56,400 --> 00:08:58,400
It is not very well known at all.

230
00:08:58,400 --> 00:09:00,680
It's called iRobot and it's from 2004.

231
00:09:00,680 --> 00:09:02,520
Will Smith, amazing movie.

232
00:09:02,520 --> 00:09:05,040
And it kind of explores this future with human robots

233
00:09:05,040 --> 00:09:07,120
doing a lot of tasks in society.

234
00:09:07,120 --> 00:09:09,880
And kind of spoiler alert, it doesn't go so well

235
00:09:09,880 --> 00:09:11,000
for these people in this movie.

236
00:09:11,000 --> 00:09:13,680
And the robots kind of take over a little bit.

237
00:09:13,680 --> 00:09:16,640
But I think it's kind of interesting to think through.

238
00:09:16,640 --> 00:09:19,120
And I definitely would encourage you to also watch this movie.

239
00:09:19,120 --> 00:09:22,200
And this movie takes place in 2035, allegedly,

240
00:09:22,200 --> 00:09:23,640
which is 10 years away.

241
00:09:23,640 --> 00:09:25,960
And so maybe in 10 years, you can definitely squint

242
00:09:25,960 --> 00:09:28,120
and think about that maybe we are gonna be in a place

243
00:09:28,120 --> 00:09:30,520
where these things are walking around

244
00:09:30,520 --> 00:09:32,360
and talking to us and performing tasks

245
00:09:32,360 --> 00:09:34,080
in physical world and digital world.

246
00:09:34,080 --> 00:09:34,920
And what does that look like?

247
00:09:34,920 --> 00:09:35,760
What does that mean?

248
00:09:35,760 --> 00:09:36,800
And how do we program them?

249
00:09:36,800 --> 00:09:40,600
How do we make sure that they sort of do

250
00:09:40,600 --> 00:09:42,200
what we want them to, et cetera?

251
00:09:43,520 --> 00:09:45,720
So when you put all this together,

252
00:09:45,720 --> 00:09:48,560
I think the feeling that people talk about often

253
00:09:48,560 --> 00:09:50,160
is this feeling of AGI.

254
00:09:50,160 --> 00:09:52,240
Like do you feel the AGI quote unquote?

255
00:09:52,240 --> 00:09:54,280
And what this means is that you've really intuitively

256
00:09:54,280 --> 00:09:57,360
understand the magnitude of what could be coming

257
00:09:57,360 --> 00:10:00,560
around the corner if the stuff actually continues to work.

258
00:10:00,560 --> 00:10:02,520
The amount of automation that we can potentially have

259
00:10:02,520 --> 00:10:05,320
in both the digital space and the physical space.

260
00:10:05,320 --> 00:10:06,280
Now, I don't know about you,

261
00:10:06,280 --> 00:10:08,240
but I actually find this picture kind of bleak.

262
00:10:08,240 --> 00:10:10,480
This is what came out when I put a bunch of the

263
00:10:10,480 --> 00:10:13,680
last few minutes of talk into an image generator.

264
00:10:13,680 --> 00:10:15,840
And I don't actually like this picture.

265
00:10:15,840 --> 00:10:17,160
I think we can do better.

266
00:10:17,160 --> 00:10:19,440
And we have a few thousand people here.

267
00:10:19,440 --> 00:10:21,480
You're about to enter the industry

268
00:10:21,480 --> 00:10:23,200
and you're gonna be working on a lot of this technology

269
00:10:23,200 --> 00:10:24,200
and you're gonna be shaping it

270
00:10:24,200 --> 00:10:27,760
and you'll have some active sort of power over it.

271
00:10:27,760 --> 00:10:28,720
So I don't know, maybe we want this

272
00:10:28,720 --> 00:10:30,080
to look something like this.

273
00:10:30,080 --> 00:10:32,080
I mean, this is what I would like.

274
00:10:32,080 --> 00:10:34,400
So this is humans, animals, and nature,

275
00:10:34,400 --> 00:10:36,320
coexisting in harmony.

276
00:10:36,320 --> 00:10:39,360
But secretly, this is actually a high tech society

277
00:10:39,360 --> 00:10:40,880
and there are robots and quadcopters

278
00:10:40,880 --> 00:10:42,480
and there's a ton of automation,

279
00:10:42,480 --> 00:10:45,920
but it's hidden away and it's not sort of like in your face.

280
00:10:45,920 --> 00:10:49,040
And so maybe this is something that we want instead.

281
00:10:49,040 --> 00:10:50,680
And you should feel a lot of agency

282
00:10:50,680 --> 00:10:52,680
over what you want the future to be like

283
00:10:52,680 --> 00:10:54,200
because you're gonna build it.

284
00:10:54,200 --> 00:10:55,320
So maybe we can agree right now

285
00:10:55,320 --> 00:10:57,200
that this is better than the previous picture.

286
00:10:57,200 --> 00:10:58,960
But I don't know about you, but I would hope so

287
00:10:58,960 --> 00:11:01,480
because I'm gonna be living in that future, I think.

288
00:11:02,480 --> 00:11:03,880
So the question for this hackathon,

289
00:11:03,880 --> 00:11:06,040
I mean, a lot of you have worked on a real,

290
00:11:06,040 --> 00:11:08,800
a bunch of really cool projects over the last day or two.

291
00:11:08,800 --> 00:11:11,160
And the question is how do we go from hacking

292
00:11:11,160 --> 00:11:15,080
to actually changing the world and building this future,

293
00:11:15,080 --> 00:11:16,680
whatever that may be for you?

294
00:11:16,680 --> 00:11:18,280
And so what I thought I would do in this talk

295
00:11:18,280 --> 00:11:20,800
is go over maybe like my last 15 years or so

296
00:11:20,800 --> 00:11:21,960
in the industry.

297
00:11:21,960 --> 00:11:24,160
And I think I had a bit of a window

298
00:11:24,160 --> 00:11:27,280
into how projects become real role change.

299
00:11:27,280 --> 00:11:28,800
And I have some takeaways and things like that

300
00:11:28,800 --> 00:11:30,600
and that I maybe wanted to talk about.

301
00:11:31,520 --> 00:11:35,120
So the first thing that I find really incredible

302
00:11:35,120 --> 00:11:38,280
is how projects that are sometimes very small projects

303
00:11:38,280 --> 00:11:40,560
like little snowballs can actually like snowball

304
00:11:40,560 --> 00:11:41,920
into really big projects.

305
00:11:41,920 --> 00:11:44,000
And just how incredible that is to watch.

306
00:11:44,000 --> 00:11:46,940
So as an example, I have my fair share of hackathons

307
00:11:46,940 --> 00:11:48,620
like I mentioned, these are some projects

308
00:11:48,620 --> 00:11:50,440
from a long time ago that I worked on

309
00:11:50,440 --> 00:11:52,240
over the last 15 years or so.

310
00:11:52,240 --> 00:11:55,480
So I had a little Rubik's Cube color extractor.

311
00:11:55,480 --> 00:11:57,680
I put up some game programming tutorials on YouTube

312
00:11:57,680 --> 00:12:00,200
like 13 years ago and tried to teach people

313
00:12:00,200 --> 00:12:01,680
programming for games.

314
00:12:01,680 --> 00:12:04,120
I had a video games and a lot of them.

315
00:12:04,120 --> 00:12:07,280
I had this like kind of janky neuro evolution simulator

316
00:12:07,280 --> 00:12:09,480
which was kind of interesting.

317
00:12:09,480 --> 00:12:12,200
And unsurprisingly, not all of these projects

318
00:12:12,200 --> 00:12:13,360
actually go on to snowball.

319
00:12:13,360 --> 00:12:15,920
A lot of this is just exploration, you're tinkering.

320
00:12:15,920 --> 00:12:17,200
And so actually these three projects

321
00:12:17,200 --> 00:12:18,720
didn't really go anywhere for me.

322
00:12:18,720 --> 00:12:20,280
I wouldn't say that it was really wasted work.

323
00:12:20,280 --> 00:12:22,640
It was just like it didn't add up to and didn't snowball

324
00:12:22,640 --> 00:12:24,600
but it was still like helping me along the way.

325
00:12:24,600 --> 00:12:26,200
I'll come back to that later.

326
00:12:26,200 --> 00:12:28,040
But the game programming tutorials actually ended up

327
00:12:28,040 --> 00:12:29,840
snowballing for me in a certain way

328
00:12:29,840 --> 00:12:32,440
because that led me from game programming tutorials

329
00:12:32,440 --> 00:12:34,800
to a bunch of Rubik's Cube videos actually

330
00:12:34,800 --> 00:12:36,760
that became kind of popular at the time.

331
00:12:36,760 --> 00:12:39,680
And this kind of sparked an interest in teaching for me.

332
00:12:39,680 --> 00:12:42,000
And then when I was a PhD student at Stanford,

333
00:12:42,000 --> 00:12:45,840
I got to teach this class CS231N

334
00:12:45,840 --> 00:12:47,360
and got to develop it and teach it.

335
00:12:47,360 --> 00:12:50,280
And this was the first big deep learning class at Stanford.

336
00:12:50,280 --> 00:12:52,800
And a lot of people have gone on to like this.

337
00:12:52,800 --> 00:12:55,880
And then after that, I ended up making another YouTube channel

338
00:12:55,880 --> 00:12:59,640
which is my zero to hero series for deep learning and LLMs.

339
00:12:59,640 --> 00:13:01,440
So a lot of people like that as well.

340
00:13:01,440 --> 00:13:03,200
And then on top of that, continuing to snowball,

341
00:13:03,200 --> 00:13:05,200
the project that I'm currently very interested in

342
00:13:05,200 --> 00:13:07,600
is this next class and what it could look like

343
00:13:07,600 --> 00:13:08,960
and how I can make it better.

344
00:13:08,960 --> 00:13:11,160
And I'm calling that LLM101N.

345
00:13:11,160 --> 00:13:13,080
And it's about building a storyteller,

346
00:13:13,080 --> 00:13:14,600
something like kind of a chat GPT

347
00:13:14,600 --> 00:13:16,680
that you can work with to generate stories.

348
00:13:16,680 --> 00:13:19,520
And the idea is you build everything from scratch

349
00:13:19,520 --> 00:13:21,800
from basic prerequisites all the way to like

350
00:13:21,800 --> 00:13:24,880
kind of a chat GPT clone in the domain of storytelling.

351
00:13:24,880 --> 00:13:26,200
And building that from scratch,

352
00:13:26,200 --> 00:13:28,440
I think will be really instructive, could be really fun.

353
00:13:28,440 --> 00:13:30,880
I only published this on GitHub like two or three days ago.

354
00:13:30,880 --> 00:13:33,560
So it's pretty raw and still very much in the early stages.

355
00:13:33,560 --> 00:13:34,840
But I'm really excited for it.

356
00:13:34,840 --> 00:13:37,320
And this for me is an example of a snowball.

357
00:13:37,320 --> 00:13:40,160
It started with like 13 years ago, a little game programming.

358
00:13:40,160 --> 00:13:41,600
And I'm working on a course

359
00:13:41,600 --> 00:13:43,600
that I think will be really interesting.

360
00:13:45,080 --> 00:13:45,920
Thank you.

361
00:13:45,920 --> 00:13:51,160
Another example from my life, I think,

362
00:13:51,160 --> 00:13:53,360
is the snowball that I've witnessed with OpenAI.

363
00:13:53,360 --> 00:13:54,880
So as was briefly mentioned,

364
00:13:54,880 --> 00:13:58,120
I was a founding member researcher of OpenAI.

365
00:13:58,120 --> 00:13:59,600
And so I was there seven years ago.

366
00:13:59,600 --> 00:14:01,240
These are some images that are public

367
00:14:01,240 --> 00:14:04,960
of what it was like working out of Greg's apartment,

368
00:14:04,960 --> 00:14:06,000
like eight of us.

369
00:14:06,000 --> 00:14:08,200
And OpenAI was founded to be kind of like

370
00:14:08,200 --> 00:14:10,120
a counterbalance to Google.

371
00:14:10,120 --> 00:14:11,600
And Google was like this gorilla,

372
00:14:11,600 --> 00:14:13,680
with 70 billion free cash flow.

373
00:14:13,680 --> 00:14:15,240
And back then, Google employed

374
00:14:15,240 --> 00:14:18,200
like half of the AI research industry almost.

375
00:14:18,200 --> 00:14:23,240
So it was kind of like an interesting setup, I would say.

376
00:14:23,240 --> 00:14:25,080
And we were just like eight people with a laptop.

377
00:14:25,080 --> 00:14:26,600
So that was really interesting.

378
00:14:26,600 --> 00:14:27,840
And very similar to my background,

379
00:14:27,840 --> 00:14:31,280
OpenAI ended up exploring a large number of projects internally.

380
00:14:31,280 --> 00:14:32,840
We hired some really good people.

381
00:14:32,840 --> 00:14:35,120
And many of them didn't go too far,

382
00:14:35,120 --> 00:14:37,360
but some of them really did work.

383
00:14:37,360 --> 00:14:39,320
And so as an example, here's a project

384
00:14:39,320 --> 00:14:42,840
that was in an early stage, a very small snowball

385
00:14:43,160 --> 00:14:44,840
in the early history of OpenAI.

386
00:14:44,840 --> 00:14:47,120
Someone worked on a Reddit chatbot.

387
00:14:47,120 --> 00:14:49,680
And if you come by their desk and you're like,

388
00:14:49,680 --> 00:14:50,960
what does this look like when someone's working

389
00:14:50,960 --> 00:14:52,000
on a Reddit chatbot?

390
00:14:52,000 --> 00:14:54,600
We're trying to compete with Google.

391
00:14:54,600 --> 00:14:56,360
And you're working on a Reddit chatbot.

392
00:14:56,360 --> 00:14:58,440
Like we should be doing something bigger.

393
00:14:58,440 --> 00:15:01,360
And so it's very easy to dismiss these small snowballs,

394
00:15:01,360 --> 00:15:02,440
because they're so fragile.

395
00:15:02,440 --> 00:15:04,720
Right, these projects are so fragile in the beginning.

396
00:15:04,720 --> 00:15:06,000
But actually this Reddit chatbot,

397
00:15:06,000 --> 00:15:07,160
and by the way, don't read too much

398
00:15:07,160 --> 00:15:08,200
into the specific details.

399
00:15:08,200 --> 00:15:10,000
These are kind of like random screenshots,

400
00:15:10,000 --> 00:15:11,440
just for illustration.

401
00:15:11,440 --> 00:15:12,720
But this was a Reddit chatbot.

402
00:15:12,760 --> 00:15:13,760
And it looked naive.

403
00:15:13,760 --> 00:15:16,200
But actually Reddit chatbot, what is that?

404
00:15:16,200 --> 00:15:17,560
It's a language model.

405
00:15:17,560 --> 00:15:19,280
And it happens to be trained on Reddit.

406
00:15:19,280 --> 00:15:21,000
But actually you could train a language model

407
00:15:21,000 --> 00:15:23,120
on any arbitrary data, not just Reddit.

408
00:15:23,120 --> 00:15:24,600
And when the transformer came out,

409
00:15:24,600 --> 00:15:27,600
this was spun into something that worked much better.

410
00:15:27,600 --> 00:15:29,000
And then the domain was expanded

411
00:15:29,000 --> 00:15:30,880
from just Reddit to many other web pages.

412
00:15:30,880 --> 00:15:33,760
And suddenly you get GPT-1, GPT-2, 3, 4,

413
00:15:33,760 --> 00:15:35,400
and then you get GPT-4O.

414
00:15:35,400 --> 00:15:37,080
So actually this Reddit chatbot

415
00:15:37,080 --> 00:15:39,320
that was so easy to dismiss,

416
00:15:39,320 --> 00:15:41,720
actually like ended up leading a snowballing

417
00:15:41,720 --> 00:15:44,240
into GPT-4O, which we currently think of as this like,

418
00:15:44,240 --> 00:15:45,840
change in the computing paradigm.

419
00:15:45,840 --> 00:15:47,680
And you can talk to it, it's amazing.

420
00:15:47,680 --> 00:15:49,200
So it's really incredible for me to have witnessed

421
00:15:49,200 --> 00:15:52,200
some of those, I guess, snowballs.

422
00:15:52,200 --> 00:15:54,360
And today opening out, of course, is worth,

423
00:15:54,360 --> 00:15:56,880
maybe somewhere just below $100 billion or something like that.

424
00:15:56,880 --> 00:15:58,760
So really incredible,

425
00:15:58,760 --> 00:16:02,040
incredible to see some of these snowballs in practice.

426
00:16:02,040 --> 00:16:04,960
So I would say a lot of you over the last two days

427
00:16:04,960 --> 00:16:07,800
have also worked on small projects, small snowballs maybe.

428
00:16:07,800 --> 00:16:09,000
And it's really incredible to me

429
00:16:09,000 --> 00:16:11,240
that some of them probably won't go anywhere,

430
00:16:11,240 --> 00:16:13,200
but probably some of them actually will.

431
00:16:13,200 --> 00:16:16,400
And you should continue the momentum of your projects

432
00:16:16,400 --> 00:16:18,560
and maybe they can add up to a really big snowball

433
00:16:18,560 --> 00:16:20,560
and that's really incredible to watch.

434
00:16:21,480 --> 00:16:23,720
The next thing I wanted to briefly talk about is

435
00:16:23,720 --> 00:16:25,000
this concept of 10,000 hours

436
00:16:25,000 --> 00:16:27,520
that was popularized by Malcolm Gladwell, I think.

437
00:16:27,520 --> 00:16:29,240
I actually am quite a big believer in it.

438
00:16:29,240 --> 00:16:31,200
And I think that to a very large extent,

439
00:16:31,200 --> 00:16:33,560
success comes from just repeated practice

440
00:16:33,560 --> 00:16:34,840
and just a huge amount of it.

441
00:16:34,840 --> 00:16:37,160
And you should be very willing to put in those 10,000 hours

442
00:16:37,160 --> 00:16:38,920
and just literally just count.

443
00:16:38,920 --> 00:16:41,000
Don't be too nervous about what am I working about?

444
00:16:41,040 --> 00:16:42,640
Am I succeeding or failing, et cetera.

445
00:16:42,640 --> 00:16:43,920
Just do simple bean counting

446
00:16:43,920 --> 00:16:45,640
of how many hours you're gonna, you're doing

447
00:16:45,640 --> 00:16:46,960
and everything adds up.

448
00:16:46,960 --> 00:16:48,560
Even the projects that I failed at

449
00:16:48,560 --> 00:16:50,040
and didn't snowball into anything,

450
00:16:50,040 --> 00:16:53,840
those add to my counter of the number of hours I've spent

451
00:16:53,840 --> 00:16:56,680
developing my expertise and getting into an empowered state

452
00:16:56,680 --> 00:16:58,920
of being able to take on these projects with confidence

453
00:16:58,920 --> 00:17:00,240
and getting them to work.

454
00:17:00,240 --> 00:17:01,600
So a few examples of that.

455
00:17:03,040 --> 00:17:05,640
I made this like really janky website a few weeks ago.

456
00:17:05,640 --> 00:17:06,880
This was a weekend project

457
00:17:06,880 --> 00:17:09,320
and it's called awesomemovies.life.

458
00:17:09,320 --> 00:17:10,520
And you can visit it.

459
00:17:10,520 --> 00:17:12,720
I think it still works, I'm not 100% sure.

460
00:17:12,720 --> 00:17:13,800
I wouldn't recommend you go there.

461
00:17:13,800 --> 00:17:16,320
It's trying to be a movie recommendation engine

462
00:17:16,320 --> 00:17:18,200
because I was trying to figure out what to watch

463
00:17:18,200 --> 00:17:20,080
on that Saturday and then I was like, okay,

464
00:17:20,080 --> 00:17:22,440
I need to build myself a movie recommendation engine.

465
00:17:22,440 --> 00:17:25,120
So I put this up and one of the tweets

466
00:17:25,120 --> 00:17:27,840
that was replied to mine was, wow, that's so cool

467
00:17:27,840 --> 00:17:29,920
that you got this to work in the weekend.

468
00:17:29,920 --> 00:17:33,200
And I was kind of reflecting on that at the time

469
00:17:33,200 --> 00:17:36,440
because it wasn't as amazing to me.

470
00:17:36,440 --> 00:17:37,880
And the reason for that was that

471
00:17:37,880 --> 00:17:39,640
what this person is not seeing

472
00:17:39,720 --> 00:17:41,560
is that this is my 20th time

473
00:17:41,560 --> 00:17:43,760
like making a website like this.

474
00:17:43,760 --> 00:17:45,680
So I see all the steps that I was gonna follow.

475
00:17:45,680 --> 00:17:47,840
Okay, I need a linode, I need a flask server,

476
00:17:47,840 --> 00:17:50,920
I'm gonna write some of this JavaScript style sheets,

477
00:17:50,920 --> 00:17:53,080
HTML, I'm gonna spin this up together.

478
00:17:53,080 --> 00:17:55,240
I need to scrape all these web pages,

479
00:17:55,240 --> 00:17:58,880
I need to extract TFIDF vectors, I need to train SVM.

480
00:17:58,880 --> 00:18:00,160
And all of these things are things

481
00:18:00,160 --> 00:18:02,000
I've already done before 20 times.

482
00:18:02,000 --> 00:18:03,680
I already have code snippets lying around

483
00:18:03,680 --> 00:18:06,520
from previous projects and I'm just remixing what I have

484
00:18:06,520 --> 00:18:08,000
and I've already done all of this.

485
00:18:08,000 --> 00:18:10,120
And so remixing everything into a new form

486
00:18:10,120 --> 00:18:11,400
isn't actually that much work

487
00:18:11,400 --> 00:18:13,000
and allowed me to put this up over the weekend

488
00:18:13,000 --> 00:18:14,520
and it's not that crazy.

489
00:18:14,520 --> 00:18:16,160
And this only comes from expertise,

490
00:18:16,160 --> 00:18:18,680
this only comes from having done it 20 times

491
00:18:18,680 --> 00:18:21,080
and you can do this so confidently.

492
00:18:21,080 --> 00:18:22,600
The next example I would say in my life

493
00:18:22,600 --> 00:18:24,160
was a Tesla autopilot.

494
00:18:24,160 --> 00:18:27,780
So I was hired to lead the computer vision team

495
00:18:27,780 --> 00:18:30,840
at Tesla autopilot about seven or eight years ago.

496
00:18:30,840 --> 00:18:33,040
And one of the first things I did actually

497
00:18:33,040 --> 00:18:36,160
when I joined the team was I basically ended up rewriting

498
00:18:36,160 --> 00:18:39,080
the computer vision deep learning network,

499
00:18:39,080 --> 00:18:42,200
training code base from scratch in PyTorch

500
00:18:42,200 --> 00:18:44,840
in some of the first few months that I entered the team.

501
00:18:44,840 --> 00:18:47,140
And I sort of rewrote the whole thing from scratch

502
00:18:47,140 --> 00:18:50,000
and that ended up being a kernel of what it is now.

503
00:18:50,000 --> 00:18:52,040
And I think to some extent,

504
00:18:52,040 --> 00:18:54,240
to some people that looked impressive at the time,

505
00:18:54,240 --> 00:18:57,080
but for me it wasn't because I was coming from my PhD

506
00:18:57,080 --> 00:18:59,920
and I spent five years doing stuff like that

507
00:18:59,920 --> 00:19:01,880
and I knew exactly what needs to go into there.

508
00:19:01,880 --> 00:19:04,120
I need my training set, my evaluation sets,

509
00:19:04,120 --> 00:19:06,040
I need my training loop in PyTorch,

510
00:19:06,080 --> 00:19:10,920
I need my sort of configs, I need my log directories,

511
00:19:10,920 --> 00:19:13,400
I need to bring in a ResNet, I need to put in detection,

512
00:19:13,400 --> 00:19:15,600
we're doing a regression classification.

513
00:19:15,600 --> 00:19:16,520
And so the whole thing,

514
00:19:16,520 --> 00:19:18,080
like I'm anticipating all the steps

515
00:19:18,080 --> 00:19:19,720
and that only comes from experience,

516
00:19:19,720 --> 00:19:21,960
that only comes from having done it 20 times before.

517
00:19:21,960 --> 00:19:23,760
And so I think this makes a huge difference

518
00:19:23,760 --> 00:19:25,480
and things that look impressive

519
00:19:25,480 --> 00:19:27,120
are maybe much less impressive to you

520
00:19:27,120 --> 00:19:28,680
if you've done it 20 times before.

521
00:19:28,680 --> 00:19:30,440
So really try to get to this point

522
00:19:30,440 --> 00:19:32,080
where you have your 10,000 hours,

523
00:19:32,080 --> 00:19:36,000
it makes a huge difference and just, yeah, that's it.

524
00:19:37,040 --> 00:19:38,640
By the way, 10,000 hours,

525
00:19:38,640 --> 00:19:40,640
if you're doing six hours per day,

526
00:19:40,640 --> 00:19:43,120
I think this works out to about five years.

527
00:19:43,120 --> 00:19:44,920
So it's about a length of a PhD

528
00:19:44,920 --> 00:19:47,720
that you need to develop expertise in an area.

529
00:19:47,720 --> 00:19:49,360
So I think it's roughly correct

530
00:19:49,360 --> 00:19:51,520
that that works out to about a PhD length.

531
00:19:52,680 --> 00:19:55,160
The other thing that I found is actually quite useful

532
00:19:55,160 --> 00:19:57,520
is to keep the dopamine flowing,

533
00:19:57,520 --> 00:20:00,880
be aware of your psychology, your brain, how it works,

534
00:20:00,880 --> 00:20:04,280
and what it needs to keep going and how to keep inspired.

535
00:20:04,480 --> 00:20:06,640
And so in particular, your brain is a reward machine

536
00:20:06,640 --> 00:20:09,200
and it wants rewards and you need to give it rewards.

537
00:20:09,200 --> 00:20:11,040
So what is a good way to give it rewards?

538
00:20:11,040 --> 00:20:13,960
And in my practice, it is by doing projects

539
00:20:13,960 --> 00:20:16,840
and work on projects and continue publishing them.

540
00:20:16,840 --> 00:20:18,680
And so here I have a webpage snippet

541
00:20:18,680 --> 00:20:21,120
of some of the projects I have worked on in the past

542
00:20:21,120 --> 00:20:23,320
and these are hackathon projects and random projects

543
00:20:23,320 --> 00:20:24,160
and not all of them are good,

544
00:20:24,160 --> 00:20:26,240
some of them are not quite good, et cetera.

545
00:20:26,240 --> 00:20:28,560
But what I love about project is a number of things.

546
00:20:28,560 --> 00:20:31,920
Number one, I love that projects get you to work

547
00:20:31,920 --> 00:20:34,480
on something end-to-end and depth-wise.

548
00:20:34,480 --> 00:20:36,080
Like normally when you go to classes,

549
00:20:36,080 --> 00:20:38,000
you're learning in a breath-wise fashion.

550
00:20:38,000 --> 00:20:39,000
You're learning a lot of stuff

551
00:20:39,000 --> 00:20:41,200
just in case you might need it in the future.

552
00:20:41,200 --> 00:20:42,480
Well, when you're working on a project,

553
00:20:42,480 --> 00:20:44,560
you know what you need and you're learning it on demand

554
00:20:44,560 --> 00:20:46,080
and you're just trying to get it to work.

555
00:20:46,080 --> 00:20:49,080
So I think it's a very different mode of learning

556
00:20:49,080 --> 00:20:51,320
that really complements the breath-wise learning

557
00:20:51,320 --> 00:20:52,320
and it's very important.

558
00:20:52,320 --> 00:20:54,920
So I 100% encourage people to work on projects.

559
00:20:54,920 --> 00:20:56,620
The other thing is putting them up

560
00:20:56,620 --> 00:20:59,820
is actually also like a really good Jedi mind trick

561
00:20:59,820 --> 00:21:01,160
in my experience.

562
00:21:01,160 --> 00:21:03,720
The reason for that is that if you're gonna put something up,

563
00:21:03,720 --> 00:21:04,880
you're thinking about all the people

564
00:21:04,880 --> 00:21:06,040
who are gonna be looking at it,

565
00:21:06,040 --> 00:21:08,000
your friends and teammates and family

566
00:21:08,000 --> 00:21:10,280
and future employers, et cetera.

567
00:21:10,280 --> 00:21:13,840
And so that really increases the bar for your own work

568
00:21:13,840 --> 00:21:15,560
and it makes you work harder

569
00:21:15,560 --> 00:21:16,680
because they're gonna be looking at it

570
00:21:16,680 --> 00:21:18,720
and you feel shame if it was crappy.

571
00:21:18,720 --> 00:21:19,840
And so you work much harder

572
00:21:19,840 --> 00:21:22,040
and you're gonna go that extra mile to make it really good

573
00:21:22,040 --> 00:21:24,800
and that really, really helps.

574
00:21:24,800 --> 00:21:27,360
And lastly, when other people are looking at your projects,

575
00:21:27,360 --> 00:21:28,400
you're gonna get that reward

576
00:21:28,400 --> 00:21:30,960
because they like it, they appreciate it, they fork it,

577
00:21:30,960 --> 00:21:32,040
they work on top of it.

578
00:21:32,040 --> 00:21:34,280
And so that feels good to your brain.

579
00:21:34,280 --> 00:21:36,760
And so the way that this comes together is

580
00:21:36,760 --> 00:21:39,120
you are getting your dopamine, you feel good.

581
00:21:39,120 --> 00:21:42,120
That way you can build up to 10,000 hours of experience

582
00:21:42,120 --> 00:21:44,040
and that's what helps you a lot.

583
00:21:44,040 --> 00:21:47,080
Snowball your projects from a small snowball

584
00:21:47,080 --> 00:21:48,440
all the way to a really big one

585
00:21:48,440 --> 00:21:50,920
and actually make change in the world.

586
00:21:50,920 --> 00:21:53,760
So in summary, that's I think how it works

587
00:21:53,760 --> 00:21:57,720
like on a high level and the message is just keep hacking.

588
00:21:57,720 --> 00:21:58,560
That's it.

589
00:21:58,560 --> 00:22:03,560
And then hopefully, this is the future

590
00:22:07,440 --> 00:22:08,280
that we're gonna build together

591
00:22:08,280 --> 00:22:09,480
when we snowball all of our stuff

592
00:22:09,480 --> 00:22:10,720
or something like that,

593
00:22:10,720 --> 00:22:13,280
but not the first picture I showed, hopefully.

594
00:22:13,280 --> 00:22:14,680
And that's it, thank you.

595
00:22:14,680 --> 00:22:15,520
Thank you.

596
00:22:20,020 --> 00:22:22,360
Andre Karparthi, everybody.

597
00:22:22,360 --> 00:22:24,520
Thank you, Andre, that was awesome.

598
00:22:24,520 --> 00:22:25,600
Thank you, thank you.

599
00:22:26,440 --> 00:22:31,560
All right, let's get to those pitches.

600
00:22:31,560 --> 00:22:33,520
The grand prize, coming up,

601
00:22:33,520 --> 00:22:35,880
you're gonna hear eight pitches by eight projects

602
00:22:35,880 --> 00:22:39,240
filtered through 290 submissions.

603
00:22:39,240 --> 00:22:40,320
Narrow it down to eight

604
00:22:40,320 --> 00:22:42,460
so you're all gonna see some cool stuff.

605
00:22:42,460 --> 00:22:45,520
The grand prize is $25,000 investment

606
00:22:45,520 --> 00:22:48,440
and actual term sheet from the Berkeley Skydeck Fund.

607
00:22:48,440 --> 00:22:52,040
They must commit to hacking all summer on their project

608
00:22:52,040 --> 00:22:54,000
and they must appropriately form,

609
00:22:54,000 --> 00:22:55,680
of course, a legal entity.

610
00:22:55,680 --> 00:22:57,320
How do you get money otherwise?

611
00:22:57,320 --> 00:23:00,720
All right, I would like to now tell you briefly

612
00:23:00,720 --> 00:23:02,640
about how this is gonna go.

613
00:23:02,640 --> 00:23:05,960
Eight projects, as I said, three minute pitch.

614
00:23:05,960 --> 00:23:06,920
You guys ready?

615
00:23:06,920 --> 00:23:07,840
Three minutes?

616
00:23:07,840 --> 00:23:09,640
Yes, they're ready.

617
00:23:09,640 --> 00:23:13,120
The judges will then provide three minutes of feedback

618
00:23:13,120 --> 00:23:15,440
and then after all the pitches,

619
00:23:15,440 --> 00:23:19,520
the grand judges will go and deliberate and pick a winner.

620
00:23:19,520 --> 00:23:21,540
Well, we show you some other cool stuff.

621
00:23:21,540 --> 00:23:24,380
All right, I would like to introduce now

622
00:23:24,380 --> 00:23:26,380
to great applause, everybody, please,

623
00:23:26,380 --> 00:23:29,340
because we have an incredible panel of judges.

624
00:23:29,340 --> 00:23:30,900
We are so pleased to have them.

625
00:23:30,900 --> 00:23:33,340
Please welcome our first judge, Brian Boardley,

626
00:23:33,340 --> 00:23:35,660
with the Berkeley Skydeck Fund.

627
00:23:35,660 --> 00:23:36,820
Welcome, Brian.

628
00:23:38,780 --> 00:23:41,300
Marcy Vu with Greycroft.

629
00:23:41,300 --> 00:23:42,620
Welcome, Marcy.

630
00:23:45,980 --> 00:23:49,620
Namdi Eregbulam with Lightspeed.

631
00:23:49,620 --> 00:23:50,700
Welcome, Namdi.

632
00:23:52,540 --> 00:23:55,220
Irving Sue with Mayfield Fund.

633
00:23:55,220 --> 00:23:56,500
Welcome, Irving.

634
00:23:59,420 --> 00:24:01,940
Kurt Kuitzer, UC Berkeley faculty

635
00:24:01,940 --> 00:24:03,500
and serial entrepreneur.

636
00:24:03,500 --> 00:24:04,620
Welcome, Kurt.

637
00:24:05,820 --> 00:24:08,060
And Mark Nitzberg, Berkeley faculty

638
00:24:08,060 --> 00:24:09,780
and director of the UC Berkeley Center

639
00:24:09,780 --> 00:24:12,180
for Human Compatible AI.

640
00:24:12,180 --> 00:24:13,580
Thank you, judges.

641
00:24:15,860 --> 00:24:18,060
All right, we got eight startups.

642
00:24:18,060 --> 00:24:20,500
Warming up backstage.

643
00:24:20,540 --> 00:24:22,740
Let's give them a little drum roll.

644
00:24:22,740 --> 00:24:24,900
Let's give them a little drum roll.

645
00:24:24,900 --> 00:24:26,060
We can get them going.

646
00:24:26,060 --> 00:24:28,140
I first have to hear if the slide's up.

647
00:24:28,140 --> 00:24:29,500
The slide is up first.

648
00:24:29,500 --> 00:24:30,740
Are you ready?

649
00:24:30,740 --> 00:24:31,580
You ready?

650
00:24:32,460 --> 00:24:34,020
Are you ready?

651
00:24:34,020 --> 00:24:34,940
Yes!

652
00:24:34,940 --> 00:24:37,460
Please give everybody a warm round of applause.

653
00:24:37,460 --> 00:24:38,780
They've been up all night hacking

654
00:24:38,780 --> 00:24:40,100
and they're ready to share with you.

655
00:24:40,100 --> 00:24:42,780
Please welcome the first project, Revision.

656
00:24:42,780 --> 00:24:44,740
Come on out, come on out, Revision.

657
00:24:44,740 --> 00:24:45,580
Yes, Danica!

658
00:24:53,740 --> 00:24:55,020
Yes, the mic.

659
00:24:55,020 --> 00:24:55,940
That'd be helpful.

660
00:24:58,020 --> 00:24:58,980
Yeah, thank you.

661
00:24:58,980 --> 00:25:00,780
So good evening, everyone.

662
00:25:00,780 --> 00:25:04,020
It's my pleasure here on behalf of my team also

663
00:25:04,020 --> 00:25:06,020
for the Revision project.

664
00:25:06,020 --> 00:25:07,260
And my name is Danica.

665
00:25:07,260 --> 00:25:09,580
I'm a rising senior studying computer science

666
00:25:09,580 --> 00:25:10,620
at UC Berkeley.

667
00:25:10,620 --> 00:25:12,420
We have masters of design students

668
00:25:12,420 --> 00:25:14,220
as well as data science students on our team.

669
00:25:14,300 --> 00:25:17,580
And we're really excited to tell you about our project.

670
00:25:17,580 --> 00:25:19,860
So our project, we're focusing on building

671
00:25:19,860 --> 00:25:22,940
an AI co-pilot tool for STEM textbook authors

672
00:25:22,940 --> 00:25:25,340
capable of detecting and mitigating bias

673
00:25:25,340 --> 00:25:28,740
in textbooks to create inclusive education content.

674
00:25:28,740 --> 00:25:31,180
And there is a reason why we're doing this.

675
00:25:31,180 --> 00:25:33,740
When considering overall representation of scientists

676
00:25:33,740 --> 00:25:38,740
across textbooks, only 13.1% were women

677
00:25:38,780 --> 00:25:42,300
compared to 86.9% men in a 2020 study

678
00:25:42,300 --> 00:25:45,060
that featured seven of the most frequently used

679
00:25:45,060 --> 00:25:47,820
biology textbooks within the US.

680
00:25:47,820 --> 00:25:50,300
And on average, people of color only appear

681
00:25:50,300 --> 00:25:53,100
every 320 pages of text, while white figures

682
00:25:53,100 --> 00:25:57,460
observed every 24 pages across 10 college STEM textbooks

683
00:25:57,460 --> 00:26:00,060
published between 2016 and 2020.

684
00:26:01,180 --> 00:26:03,620
So we thought about this problem deep and hard

685
00:26:03,620 --> 00:26:05,220
and it has been something that I've seen

686
00:26:05,220 --> 00:26:06,700
from my personal studies.

687
00:26:06,700 --> 00:26:08,660
And starting from elementary school to middle school,

688
00:26:08,660 --> 00:26:11,780
we constantly see different examples of word problems

689
00:26:11,780 --> 00:26:14,340
in other situations where text is always there.

690
00:26:14,340 --> 00:26:17,220
And it's not always reflective of the actual true history.

691
00:26:17,220 --> 00:26:19,540
And this research has been done by numerous scientists

692
00:26:19,540 --> 00:26:21,260
who have gone through this process

693
00:26:21,260 --> 00:26:23,620
of identifying people creating databases,

694
00:26:23,620 --> 00:26:25,420
but there is just no current fix that,

695
00:26:25,420 --> 00:26:28,500
and no one is really hoping to create this problem,

696
00:26:28,500 --> 00:26:31,780
but there is no current fix that helps address this problem.

697
00:26:31,780 --> 00:26:33,580
So the textbook companies actually

698
00:26:33,580 --> 00:26:36,620
is who our team identified as our buying customer.

699
00:26:36,620 --> 00:26:39,340
The current revision process actually takes six to 12 months

700
00:26:39,340 --> 00:26:41,380
of a committee of five or more full-time employees

701
00:26:41,380 --> 00:26:42,540
working on bias checks.

702
00:26:42,540 --> 00:26:44,820
And the issue here is that employees

703
00:26:44,820 --> 00:26:46,780
are actually not experts on their topic.

704
00:26:46,780 --> 00:26:49,380
They also bring in their personal biases as well.

705
00:26:49,380 --> 00:26:52,300
So our tool would come in right in between the writing

706
00:26:52,300 --> 00:26:57,100
and revising part of this entire cycle

707
00:26:57,100 --> 00:27:00,180
that developers go through when writing textbooks.

708
00:27:01,220 --> 00:27:03,540
So again, here is our competitor analysis.

709
00:27:03,540 --> 00:27:05,940
I'm sure many of you have used Turnitin or Grammarly

710
00:27:05,940 --> 00:27:07,860
when you're submitting even essays.

711
00:27:07,860 --> 00:27:09,780
And we really think that there needs to be

712
00:27:09,780 --> 00:27:11,900
an additional check here for bias

713
00:27:11,900 --> 00:27:14,980
and checking gender, racial, political, and other biases

714
00:27:14,980 --> 00:27:17,540
and making this process affordable and automatic.

715
00:27:17,540 --> 00:27:22,540
So it's not a costly process for anyone.

716
00:27:22,860 --> 00:27:24,660
And throughout this process,

717
00:27:24,660 --> 00:27:26,700
we're addressing supply chain diversity.

718
00:27:26,700 --> 00:27:28,380
So starting from a younger age,

719
00:27:28,380 --> 00:27:30,460
the elementary school students could be able

720
00:27:30,460 --> 00:27:32,580
to use textbooks that truly reflect

721
00:27:32,580 --> 00:27:35,380
the true history as well as themselves.

722
00:27:35,380 --> 00:27:36,940
And here is our prototype.

723
00:27:36,940 --> 00:27:40,580
So we have our text box here on the left side of the screen

724
00:27:40,580 --> 00:27:42,860
where you get to show in real time

725
00:27:42,860 --> 00:27:44,780
the examples of some sort of text

726
00:27:44,780 --> 00:27:47,420
that a writer is creating at the moment.

727
00:27:47,420 --> 00:27:49,500
And on the right, we have an overall score

728
00:27:49,500 --> 00:27:52,700
and the bias checks for different categories.

729
00:27:52,700 --> 00:27:54,820
And we're using machine learning models on the back end

730
00:27:54,820 --> 00:27:57,620
to actually identify as well as LLMs.

731
00:27:58,740 --> 00:28:03,700
And I'm not sure if I can play the prototype, but, oh.

732
00:28:03,700 --> 00:28:05,020
Okay, yeah, it does play.

733
00:28:05,060 --> 00:28:07,940
So essentially you can click through the different links

734
00:28:07,940 --> 00:28:09,660
to see the breakdown.

735
00:28:09,660 --> 00:28:12,140
And once you actually highlight one of these,

736
00:28:12,140 --> 00:28:16,060
we are also adding in an API through a couple

737
00:28:16,060 --> 00:28:19,500
of the sponsors here, such as Hume API and more,

738
00:28:19,500 --> 00:28:22,940
to actually identify emotional analysis as well

739
00:28:22,940 --> 00:28:24,540
in the textbook writing.

740
00:28:24,540 --> 00:28:25,700
And in addition to this,

741
00:28:25,700 --> 00:28:27,100
we're hoping to build a chat bot

742
00:28:27,100 --> 00:28:30,780
that can actually help you also bring in databases

743
00:28:30,780 --> 00:28:33,060
from the unrecognized scientists

744
00:28:33,060 --> 00:28:36,140
and being able to sort of represent it.

745
00:28:36,140 --> 00:28:38,780
Because bias actually exists in three different ways.

746
00:28:38,780 --> 00:28:41,300
One of them is through actual text

747
00:28:41,300 --> 00:28:45,420
such as representing firefighters would be nicer

748
00:28:45,420 --> 00:28:46,900
than saying fireman.

749
00:28:46,900 --> 00:28:51,180
And the other way is the entire tone and emotional analysis,

750
00:28:51,180 --> 00:28:53,140
which is why our team used Hume API

751
00:28:53,140 --> 00:28:56,220
to actually detect that emotional component.

752
00:28:56,220 --> 00:28:58,220
And the third one is mitigating bias.

753
00:28:58,220 --> 00:29:01,220
So we also considered adding in the chat bot.

754
00:29:01,260 --> 00:29:04,380
So say, for example, if you want to highlight scientists

755
00:29:04,380 --> 00:29:07,220
that are like, for example, contributing to physics,

756
00:29:07,220 --> 00:29:09,660
you wouldn't just say list a few male scientists

757
00:29:09,660 --> 00:29:10,500
and call it a day.

758
00:29:10,500 --> 00:29:12,540
We would also suggest equivalent contributions

759
00:29:12,540 --> 00:29:14,740
of female scientists as well.

760
00:29:14,740 --> 00:29:16,860
So please join me and our team

761
00:29:16,860 --> 00:29:19,580
in revisioning our future of education and work.

762
00:29:19,580 --> 00:29:20,420
Thank you.

763
00:29:20,420 --> 00:29:30,260
So I think everybody in communication today,

764
00:29:30,260 --> 00:29:32,820
nonprofit or profit is concerned about diversity.

765
00:29:32,820 --> 00:29:35,460
So it seems like you have a much larger market

766
00:29:35,460 --> 00:29:40,540
than just textbook educators.

767
00:29:40,540 --> 00:29:43,580
Also a comment on kind of like market sizing and whatnot.

768
00:29:43,580 --> 00:29:45,820
I would think about, you know, potential ways

769
00:29:45,820 --> 00:29:47,140
you could expand the market here,

770
00:29:47,140 --> 00:29:49,540
because the number of people who are involved

771
00:29:49,540 --> 00:29:51,980
in writing textbooks is a relatively small group.

772
00:29:51,980 --> 00:29:53,580
But one way to think about it is like maybe

773
00:29:53,580 --> 00:29:56,180
in this new era of AI generated content,

774
00:29:56,180 --> 00:29:58,540
a much wider array of people can be part

775
00:29:58,580 --> 00:30:00,580
of this textbook generation process.

776
00:30:00,580 --> 00:30:01,740
So that's one thing.

777
00:30:01,740 --> 00:30:03,740
And then I would also maybe consider selling directly

778
00:30:03,740 --> 00:30:05,260
to the consumers of textbooks.

779
00:30:05,260 --> 00:30:07,540
Because in some sense, the bias we're talking about

780
00:30:07,540 --> 00:30:09,340
is internalized on that side of the equation,

781
00:30:09,340 --> 00:30:11,100
not on the manufacturer side.

782
00:30:11,100 --> 00:30:12,620
And so they could be insensitive there

783
00:30:12,620 --> 00:30:14,580
for want to pay for something like this.

784
00:30:14,580 --> 00:30:15,420
Yeah, definitely.

785
00:30:15,420 --> 00:30:16,380
That's something we're considering.

786
00:30:16,380 --> 00:30:18,740
So like the textbook would be our official bias

787
00:30:18,740 --> 00:30:19,820
that we're marketing to,

788
00:30:19,820 --> 00:30:22,180
but eventually it would be more of like a grammarly checker

789
00:30:22,180 --> 00:30:24,540
type of tool that anyone can use.

790
00:30:24,540 --> 00:30:26,340
Yeah, I had a similar comment on TAM

791
00:30:26,420 --> 00:30:27,580
and market opportunity.

792
00:30:27,580 --> 00:30:31,020
And as you think about just how a textbook

793
00:30:31,020 --> 00:30:32,540
gets put into production,

794
00:30:32,540 --> 00:30:34,820
that if you actually had it as a tool for

795
00:30:34,820 --> 00:30:37,460
whether it's news or other areas,

796
00:30:37,460 --> 00:30:39,420
you'd have more velocity,

797
00:30:39,420 --> 00:30:42,100
both in terms of getting the data to improve your models,

798
00:30:42,100 --> 00:30:44,660
but also greater impact.

799
00:30:49,380 --> 00:30:50,500
Yeah, I'll just, I'll just like as well.

800
00:30:50,500 --> 00:30:52,100
I mean, similar, I think everyone here

801
00:30:52,100 --> 00:30:54,820
is kind of hitting the theme of how do we think bigger?

802
00:30:54,820 --> 00:30:56,300
So even enterprises, right?

803
00:30:56,300 --> 00:30:58,140
Like companies sending out communications

804
00:30:58,140 --> 00:30:59,340
internally or externally.

805
00:30:59,340 --> 00:31:01,300
I know this, this, this problem exists everywhere.

806
00:31:01,300 --> 00:31:03,660
So that's kind of where my brain would go to.

807
00:31:06,580 --> 00:31:08,260
Okay, thank you.

808
00:31:08,260 --> 00:31:09,100
Thank you.

809
00:31:09,100 --> 00:31:09,940
Thank you.

810
00:31:09,940 --> 00:31:10,780
Thank you.

811
00:31:10,780 --> 00:31:11,620
Thank you.

812
00:31:16,420 --> 00:31:17,780
Whoops.

813
00:31:17,780 --> 00:31:18,900
Agent OS.

814
00:31:18,900 --> 00:31:20,540
Please welcome Agent OS.

815
00:31:26,380 --> 00:31:27,220
Hey there everyone.

816
00:31:27,220 --> 00:31:28,260
My name is Shashank.

817
00:31:28,260 --> 00:31:30,060
I'm here today with my friends,

818
00:31:30,060 --> 00:31:31,580
Uggum and Drew Buhuja,

819
00:31:31,580 --> 00:31:33,260
somewhere in the crowd over here.

820
00:31:33,260 --> 00:31:35,140
We built today Agent OS.

821
00:31:36,700 --> 00:31:37,940
Picture this.

822
00:31:37,940 --> 00:31:38,900
You work at a hair salon

823
00:31:38,900 --> 00:31:40,780
and you guys are bombarded every single day

824
00:31:40,780 --> 00:31:42,740
and every single year by your accounting

825
00:31:42,740 --> 00:31:45,060
and tax preparation qualms.

826
00:31:45,060 --> 00:31:46,620
These are things that are very hard to deal with

827
00:31:46,620 --> 00:31:48,220
and you've heard of tools like OpenAI,

828
00:31:48,220 --> 00:31:52,340
ChatGPT, LLM this, ChatGPT that, everything.

829
00:31:52,340 --> 00:31:53,580
But you have no clue where to start

830
00:31:53,580 --> 00:31:54,780
using these technologies.

831
00:31:54,820 --> 00:31:56,940
And that's no fault of your own.

832
00:31:56,940 --> 00:31:59,460
The current state of the technology right now

833
00:31:59,460 --> 00:32:02,540
is very bad at multi-functionary tasks.

834
00:32:02,540 --> 00:32:05,060
More so, it's very hard as an individual developer,

835
00:32:05,060 --> 00:32:06,740
sometimes even non-technical,

836
00:32:06,740 --> 00:32:09,780
to even get started with even the simplest automations

837
00:32:09,780 --> 00:32:12,340
or workflows or tools with such LLMs.

838
00:32:12,340 --> 00:32:15,220
Even engineers with years on years of experience

839
00:32:15,220 --> 00:32:18,340
in this space take tens of hundreds and hours

840
00:32:18,340 --> 00:32:20,020
and even thousands and thousands of dollars

841
00:32:20,020 --> 00:32:22,380
to even get started to build something.

842
00:32:22,420 --> 00:32:25,980
This is where Agent OS completely transforms the landscape.

843
00:32:25,980 --> 00:32:29,620
With Agent OS, you're able to create multi-agent workflows

844
00:32:29,620 --> 00:32:32,100
in a matter of seconds from natural language.

845
00:32:32,100 --> 00:32:33,500
What does that even mean?

846
00:32:33,500 --> 00:32:35,620
Take your average corporate org structure.

847
00:32:35,620 --> 00:32:37,300
You have your managers, you have your workers

848
00:32:37,300 --> 00:32:39,100
and sometimes you even have your interns.

849
00:32:39,100 --> 00:32:40,900
Everyone is really good at what they do.

850
00:32:40,900 --> 00:32:42,740
They have their tools, their skills.

851
00:32:42,740 --> 00:32:45,140
Let's say John is really good at charting

852
00:32:45,140 --> 00:32:46,380
and making PowerPoints.

853
00:32:46,380 --> 00:32:49,200
Let's say Steve is really good at Python coding.

854
00:32:49,200 --> 00:32:51,120
Everyone's really good at what they do.

855
00:32:51,120 --> 00:32:54,040
In this, you have a very collaborative working together

856
00:32:54,040 --> 00:32:55,880
to create this common solution

857
00:32:55,880 --> 00:32:57,720
for someone coming from higher up.

858
00:32:57,720 --> 00:33:00,000
That's how Agent OS was designed.

859
00:33:00,000 --> 00:33:01,560
Our engineers, Dhruv and Uggum,

860
00:33:01,560 --> 00:33:05,160
were able to replicate this human collaborative process

861
00:33:05,160 --> 00:33:07,680
to programmatically using LLMs.

862
00:33:07,680 --> 00:33:08,760
What does this do?

863
00:33:08,760 --> 00:33:11,140
This allows everyone from the common Joe

864
00:33:11,140 --> 00:33:13,000
all the way up to enterprise clients

865
00:33:13,000 --> 00:33:16,680
to be able to interact and use these multi-agent,

866
00:33:16,680 --> 00:33:18,880
agentic workflows in their day-to-day life

867
00:33:18,920 --> 00:33:21,240
to improve their quality of life or productivity,

868
00:33:21,240 --> 00:33:23,760
in all in a matter of seconds and a few sentences.

869
00:33:24,680 --> 00:33:27,040
Let's go back to the study of the hair salon.

870
00:33:27,040 --> 00:33:30,520
In the process of doing your taxes and accounting,

871
00:33:30,520 --> 00:33:31,760
you have multiple steps.

872
00:33:31,760 --> 00:33:33,440
You have your collection from your receipts

873
00:33:33,440 --> 00:33:35,960
and your invoices, you have calculating your cash flow,

874
00:33:35,960 --> 00:33:37,640
all the calculations you have to do,

875
00:33:37,640 --> 00:33:39,120
you have to manage your workers

876
00:33:39,120 --> 00:33:41,000
and then you also have to do your general summary.

877
00:33:41,000 --> 00:33:42,300
What about your insights for the year,

878
00:33:42,300 --> 00:33:44,320
how you were spending, what you were spending on

879
00:33:44,320 --> 00:33:46,160
and you have to also do a lot of clustering

880
00:33:46,160 --> 00:33:47,480
and analytics on this.

881
00:33:47,520 --> 00:33:49,400
This is a very complex workflow

882
00:33:49,400 --> 00:33:51,760
that's nearly impossible for modern-day LLMs

883
00:33:51,760 --> 00:33:53,480
at the current state to do right now.

884
00:33:53,480 --> 00:33:55,280
You can take chat, GPT, you ask it a question

885
00:33:55,280 --> 00:33:56,680
for even more than three things,

886
00:33:56,680 --> 00:33:58,060
it'll forget what the first thing was

887
00:33:58,060 --> 00:33:59,360
by the time you're at the second.

888
00:33:59,360 --> 00:34:00,680
It doesn't work that way.

889
00:34:00,680 --> 00:34:02,640
With AgentOS, this completely changes

890
00:34:02,640 --> 00:34:05,280
where you're able to have these complex workflows.

891
00:34:05,280 --> 00:34:07,640
Let's dive into another demo.

892
00:34:07,640 --> 00:34:09,760
So let's say I'm an analyst at JP Morgan

893
00:34:09,760 --> 00:34:12,400
and my boss tells me every morning I want a report

894
00:34:12,400 --> 00:34:17,080
of XYZ stock in the morning, a detailed report on paper.

895
00:34:17,080 --> 00:34:18,560
How do I do that?

896
00:34:18,560 --> 00:34:19,840
I use AgentOS.

897
00:34:19,840 --> 00:34:21,840
On this screen you can see a bunch of other complex

898
00:34:21,840 --> 00:34:25,080
use cases of multiple agents working together collaboratively

899
00:34:25,080 --> 00:34:27,400
but in the tool box, in the search bar,

900
00:34:27,400 --> 00:34:29,480
you can see the use case of the analyst.

901
00:34:29,480 --> 00:34:32,240
Here I have to do market research, livestock data,

902
00:34:32,240 --> 00:34:35,140
I have to search the internet, go on Yahoo Finance,

903
00:34:35,140 --> 00:34:36,360
then I have to create my analysis,

904
00:34:36,360 --> 00:34:38,600
technical analysis, qualitative analysis,

905
00:34:38,600 --> 00:34:41,080
then I have to do what my boss is telling me to do

906
00:34:41,080 --> 00:34:43,400
and after all of that I have to create charts,

907
00:34:43,400 --> 00:34:45,200
graphs and visualizations.

908
00:34:45,240 --> 00:34:47,840
Here you can build tools using natural language

909
00:34:47,840 --> 00:34:49,920
like the one right there that says write me a tool

910
00:34:49,920 --> 00:34:52,600
that fetches the meta stock price from Yahoo Finance.

911
00:34:52,600 --> 00:34:55,240
In a matter of seconds, the common Joe or anyone

912
00:34:55,240 --> 00:34:58,120
is able to create that tool, connect them to workers.

913
00:34:58,120 --> 00:35:00,360
You can think of workers as your everyday employees,

914
00:35:00,360 --> 00:35:03,520
agents, people that perform these actions using the tools

915
00:35:03,520 --> 00:35:05,320
and then connect them to super teams

916
00:35:05,320 --> 00:35:08,480
and these teams are able to, on the screen you see four

917
00:35:08,480 --> 00:35:11,160
but you can scale this up to 40, 400,

918
00:35:11,160 --> 00:35:14,440
basically complex, vertical and horizontal organizations

919
00:35:14,480 --> 00:35:16,720
that are able to perform complex decision making

920
00:35:16,720 --> 00:35:19,480
and complex analyses for anyone,

921
00:35:19,480 --> 00:35:20,760
from enterprise to consumer.

922
00:35:21,880 --> 00:35:22,800
What does this do?

923
00:35:22,800 --> 00:35:24,920
With a multi-agent, multi-team framework,

924
00:35:24,920 --> 00:35:27,240
this completely opens the landscape up for anyone

925
00:35:27,240 --> 00:35:29,840
and everyone to take on the power of LLMs

926
00:35:29,840 --> 00:35:32,720
into their own hands from natural language.

927
00:35:32,720 --> 00:35:35,280
Take your average farmer at a farmer's market.

928
00:35:35,280 --> 00:35:37,680
He's trying to create his marketing campaign

929
00:35:37,680 --> 00:35:39,680
for the upcoming farmer's market this Sunday.

930
00:35:39,680 --> 00:35:42,040
He has no clue where to start looking at his metrics,

931
00:35:42,040 --> 00:35:43,880
looking at the customers, looking at the weather

932
00:35:44,000 --> 00:35:46,960
and creating these brochures, papers, pamphlets and whatnot.

933
00:35:46,960 --> 00:35:49,560
With one line and one minute using Agent OS,

934
00:35:49,560 --> 00:35:51,760
he can create all the documentation he needs

935
00:35:51,760 --> 00:35:55,000
in order to enact this stuff and be able to perform successfully

936
00:35:55,000 --> 00:35:57,880
and continually grow his business at his farmer's market.

937
00:35:57,880 --> 00:36:00,440
Things like this are completely opened up with Agent OS

938
00:36:00,440 --> 00:36:03,120
and we hope to completely democratize the process

939
00:36:03,120 --> 00:36:06,320
of using LLMs at all scales, at all geographies

940
00:36:06,320 --> 00:36:09,400
and all use cases within sentences and seconds.

941
00:36:09,400 --> 00:36:10,240
Thank you.

942
00:36:10,240 --> 00:36:25,080
That's a compelling proposition.

943
00:36:25,080 --> 00:36:36,480
The one thing that I worry about is right now the agents are the LLMs,

944
00:36:36,520 --> 00:36:44,600
you know, performing these tasks and there's a certain question

945
00:36:44,600 --> 00:36:47,560
about the veracity and reliability of what they're doing.

946
00:36:47,560 --> 00:36:50,760
And so I think that in a future

947
00:36:50,760 --> 00:36:54,800
where we have that reliability, this would make perfect sense,

948
00:36:54,800 --> 00:37:00,280
but I would want to add a kind of tandem subject matter expert,

949
00:37:00,280 --> 00:37:05,320
maybe looking over the shoulder of each of the agents.

950
00:37:05,320 --> 00:37:07,680
I think next time I hear this pitch, I'd love to hear

951
00:37:07,680 --> 00:37:10,200
about the one market you're going to crush.

952
00:37:10,200 --> 00:37:14,720
It's hard for me to imagine serving a hairstylist one day

953
00:37:14,720 --> 00:37:18,120
and Morgan Stanley analysts the next.

954
00:37:18,120 --> 00:37:24,760
This is a huge opportunity and a big bold mission that you have.

955
00:37:24,760 --> 00:37:28,640
I would want to dig a bit deeper into your tech staff

956
00:37:28,640 --> 00:37:30,280
and the people you have on your team

957
00:37:30,280 --> 00:37:33,760
because these are really complex problems and issues

958
00:37:33,760 --> 00:37:40,360
and also agree that what would be your first area of focus

959
00:37:40,360 --> 00:37:45,320
because it's pretty broad and wide.

960
00:37:45,320 --> 00:37:48,480
I'll say I kind of like the broad focus and there's a lot

961
00:37:48,480 --> 00:37:52,640
of individual startups tackling each of these individual problems.

962
00:37:52,640 --> 00:37:56,080
If it's invoicing or research, it might be interesting to figure

963
00:37:56,080 --> 00:37:59,840
out how to like loop in all these other tools that are out there

964
00:37:59,840 --> 00:38:01,800
and really kind of just be like an interface layer

965
00:38:01,840 --> 00:38:06,600
and let these other companies solve the technical challenges.

966
00:38:06,600 --> 00:38:09,920
I think the value proposition of creating multi-agent workflows

967
00:38:09,920 --> 00:38:12,480
in a matter of seconds is really compelling.

968
00:38:12,480 --> 00:38:14,440
I think the next step would be trying to figure

969
00:38:14,440 --> 00:38:17,480
out how can you go from simply performing these tasks

970
00:38:17,480 --> 00:38:18,920
to becoming the best at these tasks.

971
00:38:18,920 --> 00:38:21,400
So for example, going after the outliers,

972
00:38:21,400 --> 00:38:23,560
sort of the thesis around coaching networks.

973
00:38:23,560 --> 00:38:26,600
Some startups do this and they do it better for like certain

974
00:38:26,600 --> 00:38:27,600
verticals and others.

975
00:38:27,600 --> 00:38:31,160
So I think doing more research around that could be really

976
00:38:31,160 --> 00:38:32,840
compelling.

977
00:38:32,840 --> 00:38:35,080
The only thing I would add is just think about, you know,

978
00:38:35,080 --> 00:38:37,560
enterprise security and how you solve for that.

979
00:38:37,560 --> 00:38:40,080
There's a lot of authenticates and authorization you're going

980
00:38:40,080 --> 00:38:41,240
to have to do for all these agents.

981
00:38:41,240 --> 00:38:45,680
So just have an answer for that.

982
00:38:45,680 --> 00:38:46,720
Well, yeah, thank you so much.

983
00:38:46,720 --> 00:38:48,360
Thank you, everyone.

984
00:38:48,360 --> 00:38:52,120
Thank you, Agent OS.

985
00:38:52,120 --> 00:38:54,480
All right, next up, Skyline.

986
00:38:54,480 --> 00:38:56,680
Come on out, Skyline.

987
00:38:56,680 --> 00:39:03,920
Hey, everyone.

988
00:39:03,920 --> 00:39:06,800
Hey, so my name is Rajan and I'm a first year student

989
00:39:06,800 --> 00:39:08,080
at the University of Waterloo

990
00:39:08,080 --> 00:39:09,720
and I study software engineering.

991
00:39:09,720 --> 00:39:11,440
And I fundamentally believe

992
00:39:11,440 --> 00:39:14,280
that cities shouldn't be static.

993
00:39:14,280 --> 00:39:17,080
They should optimize and scale for the people

994
00:39:17,080 --> 00:39:18,360
who inhabit them.

995
00:39:18,360 --> 00:39:19,600
And so we built Skyline.

996
00:39:19,600 --> 00:39:23,600
Skyline is an optimizer and it allows you

997
00:39:23,600 --> 00:39:26,440
to better understand how to model cities,

998
00:39:26,440 --> 00:39:30,880
using agents, and optimize things like traffic and transit

999
00:39:30,880 --> 00:39:34,240
to inherently increase mobility

1000
00:39:34,240 --> 00:39:37,040
and reduce things like carbon emissions.

1001
00:39:37,040 --> 00:39:40,320
So this is a very weird problem that we solved.

1002
00:39:40,320 --> 00:39:43,520
But I want to walk through the case study of Los Angeles.

1003
00:39:43,520 --> 00:39:46,360
So Los Angeles is one of the largest carbon emitters

1004
00:39:46,360 --> 00:39:49,280
in North America, this most because of their transit,

1005
00:39:49,280 --> 00:39:51,200
because of the amount of cars.

1006
00:39:51,200 --> 00:39:53,920
And so what are ways in which we can optimize this?

1007
00:39:53,960 --> 00:39:55,640
Well, let's look directly at the people

1008
00:39:55,640 --> 00:39:57,440
who inhabit Los Angeles.

1009
00:39:57,440 --> 00:40:00,320
We can extract census data, things like age.

1010
00:40:00,320 --> 00:40:02,760
We can look at things like gender.

1011
00:40:02,760 --> 00:40:04,280
We can look at things like income.

1012
00:40:04,280 --> 00:40:06,320
We can find population density graphs.

1013
00:40:06,320 --> 00:40:09,400
And using this information, we can start to find patterns.

1014
00:40:09,400 --> 00:40:13,520
Specifically, what we did is we created 500 distinct agents.

1015
00:40:13,520 --> 00:40:17,400
Each agent is a different citizen with different interests.

1016
00:40:17,400 --> 00:40:18,800
And what we can do is we can give them

1017
00:40:18,800 --> 00:40:21,040
each their own chain of thought.

1018
00:40:21,080 --> 00:40:23,960
Each person here has their own day in their life.

1019
00:40:23,960 --> 00:40:26,400
For example, this person is a very young,

1020
00:40:26,400 --> 00:40:29,640
I believe this was a 22-year-old with a very large income.

1021
00:40:29,640 --> 00:40:32,880
He's a long day at work, and after work, he goes to the gym.

1022
00:40:32,880 --> 00:40:35,560
We can now reason about what this person may do

1023
00:40:35,560 --> 00:40:38,280
and now model this on a map.

1024
00:40:38,280 --> 00:40:40,400
Now, once we have how these different agents

1025
00:40:40,400 --> 00:40:41,920
are moving around, what we can do

1026
00:40:41,920 --> 00:40:44,520
is we can try and optimize things like transit.

1027
00:40:44,520 --> 00:40:47,520
So what we do here is we have our own proximal policy

1028
00:40:47,520 --> 00:40:50,720
analyzer, and this allows us to create simulations.

1029
00:40:50,720 --> 00:40:52,440
What we believe to be the best way

1030
00:40:52,440 --> 00:40:58,040
to understand how we can move around from any point A and B

1031
00:40:58,040 --> 00:41:01,600
in the fastest way at the lowest carbon cost.

1032
00:41:01,600 --> 00:41:04,240
We use our own carbon cost analysis mechanisms,

1033
00:41:04,240 --> 00:41:06,760
our own machine learning models to better understand

1034
00:41:06,760 --> 00:41:10,000
how we may be emitting carbon and how

1035
00:41:10,000 --> 00:41:11,880
to reduce this through our transit.

1036
00:41:11,880 --> 00:41:13,600
So this is a lot that is through you,

1037
00:41:13,600 --> 00:41:15,680
and I think the best way for me to represent this to you

1038
00:41:15,680 --> 00:41:16,760
is through a video.

1039
00:41:16,760 --> 00:41:17,840
I hope this video loads.

1040
00:41:21,160 --> 00:41:22,360
It's possible to play the video.

1041
00:41:26,040 --> 00:41:28,480
So what we first do is we have an agent-based simulation.

1042
00:41:28,480 --> 00:41:31,880
These are 500 distinct things in parallel that are running.

1043
00:41:31,880 --> 00:41:33,840
Now they each go around throughout their day,

1044
00:41:33,840 --> 00:41:35,760
and what we can do is we can define patterns

1045
00:41:35,760 --> 00:41:37,200
in how they move around.

1046
00:41:37,200 --> 00:41:39,120
Now the best part is what we can do

1047
00:41:39,120 --> 00:41:41,560
is now that they're all back in their original position,

1048
00:41:41,560 --> 00:41:44,120
we can start a generation of transit.

1049
00:41:44,120 --> 00:41:46,400
And we're using these patterns to now generate

1050
00:41:46,400 --> 00:41:48,920
live different transit systems that we

1051
00:41:48,920 --> 00:41:50,720
believe to be the most optimal.

1052
00:41:50,720 --> 00:41:54,280
So what Skyline is, we're not a company that

1053
00:41:54,280 --> 00:41:56,280
does analysis of transit.

1054
00:41:56,280 --> 00:41:58,640
We are a human modeling company, and that

1055
00:41:58,640 --> 00:42:01,800
allows us to better understand and better predict

1056
00:42:01,800 --> 00:42:03,480
how things around us will change and how

1057
00:42:03,480 --> 00:42:06,880
it can optimize them using these patterns.

1058
00:42:06,880 --> 00:42:09,480
Yeah, so that's Skyline.

1059
00:42:09,480 --> 00:42:11,480
Happy to take any of the back.

1060
00:42:11,480 --> 00:42:12,480
Thank you.

1061
00:42:22,480 --> 00:42:22,980
Wow.

1062
00:42:27,000 --> 00:42:31,760
I just want to observe that what you're

1063
00:42:31,760 --> 00:42:36,640
doing in creating a sort of digital twin of a city

1064
00:42:36,640 --> 00:42:44,680
is essentially the each citizen is

1065
00:42:44,680 --> 00:42:49,760
being simulated using one of these really powerful,

1066
00:42:49,760 --> 00:42:52,560
expensive things, a language model.

1067
00:42:52,560 --> 00:42:58,120
And it will be probably an important step

1068
00:42:58,120 --> 00:43:04,080
to draw from the language model some of the statistics that

1069
00:43:04,080 --> 00:43:06,320
are actually fed in in the first place

1070
00:43:06,320 --> 00:43:11,440
to make sure you're getting out something representative.

1071
00:43:11,440 --> 00:43:14,480
But that's very impressive.

1072
00:43:14,480 --> 00:43:16,520
Yeah, yeah, similar comment.

1073
00:43:16,520 --> 00:43:18,920
I think there's all sorts of economic theory

1074
00:43:18,920 --> 00:43:21,440
about agents and modeling their behavior

1075
00:43:21,440 --> 00:43:22,600
and their values and whatnot.

1076
00:43:22,600 --> 00:43:24,240
And the thing that usually gets you

1077
00:43:24,240 --> 00:43:26,760
is the sort of heterogeneity across the population.

1078
00:43:26,760 --> 00:43:28,920
And so making sure that that actually represents

1079
00:43:28,920 --> 00:43:31,280
the populations being modeled is important.

1080
00:43:31,280 --> 00:43:33,320
And then the other thing also related to value,

1081
00:43:33,320 --> 00:43:35,560
I would think about it as just value capture

1082
00:43:35,600 --> 00:43:37,120
for your own sake, because I feel like this

1083
00:43:37,120 --> 00:43:40,920
is a category of software where the economic impact of this

1084
00:43:40,920 --> 00:43:41,720
could be massive.

1085
00:43:41,720 --> 00:43:44,640
But how much of that do you get to capture as a software renderer

1086
00:43:44,640 --> 00:43:46,160
is less clear to me?

1087
00:43:46,160 --> 00:43:49,480
But it's very interesting.

1088
00:43:49,480 --> 00:43:52,800
I guess I would be curious about maybe some more nuanced

1089
00:43:52,800 --> 00:43:57,320
enterprise use cases as well, if it's concerts or security

1090
00:43:57,320 --> 00:43:58,640
or stadiums.

1091
00:43:58,640 --> 00:44:01,400
So just thinking about are there more micro use cases

1092
00:44:01,400 --> 00:44:05,120
that there's a more direct ROI with for this sort

1093
00:44:05,200 --> 00:44:07,040
of modeling?

1094
00:44:07,040 --> 00:44:09,440
Yeah, we tried to consider ourselves

1095
00:44:09,440 --> 00:44:11,240
to be a human modeling software.

1096
00:44:11,240 --> 00:44:13,800
And this is just one of the most visual applications,

1097
00:44:13,800 --> 00:44:14,600
which is strength.

1098
00:44:19,880 --> 00:44:20,760
Awesome, thank you so much.

1099
00:44:20,760 --> 00:44:21,760
Thank you.

1100
00:44:21,760 --> 00:44:23,000
Thank you.

1101
00:44:23,000 --> 00:44:25,560
Thank you, Skyline.

1102
00:44:25,560 --> 00:44:32,720
All right, next up, we have Spark.

1103
00:44:32,720 --> 00:44:33,600
Please welcome Spark.

1104
00:44:35,120 --> 00:44:39,840
Hi, how's everyone?

1105
00:44:39,840 --> 00:44:41,200
How's Cali?

1106
00:44:41,200 --> 00:44:43,400
We are Spark, and we're giving a voice

1107
00:44:43,400 --> 00:44:47,360
to new entrepreneurs, young entrepreneurs.

1108
00:44:47,360 --> 00:44:50,960
So let's admit it, cold calling is really hard.

1109
00:44:50,960 --> 00:44:53,640
I mean, resources are hard to get.

1110
00:44:53,640 --> 00:44:55,280
It's a steep learning curve.

1111
00:44:55,280 --> 00:44:57,400
And getting attention is hard.

1112
00:44:57,400 --> 00:44:59,680
If you've cold called someone, you know, they don't have time.

1113
00:44:59,680 --> 00:45:02,000
They'll say, oh, sorry, call me back later.

1114
00:45:02,000 --> 00:45:02,720
I mean, they're busy.

1115
00:45:02,720 --> 00:45:03,400
Everyone's busy.

1116
00:45:03,400 --> 00:45:04,320
We have things to do.

1117
00:45:04,320 --> 00:45:06,560
So we have to figure out how can we

1118
00:45:06,560 --> 00:45:09,960
earn the time of working people.

1119
00:45:09,960 --> 00:45:11,760
There's existing solutions.

1120
00:45:11,760 --> 00:45:14,600
It's long and arduous for trial and error.

1121
00:45:14,600 --> 00:45:16,640
It's expensive for a sales coach.

1122
00:45:16,640 --> 00:45:19,600
And finally, if you have a sales partner,

1123
00:45:19,600 --> 00:45:23,320
chemistry isn't easy if you're just meeting them, right?

1124
00:45:23,320 --> 00:45:24,640
Well, we have a process.

1125
00:45:24,640 --> 00:45:27,720
You upload a transcript to our software.

1126
00:45:27,720 --> 00:45:30,080
We go through and analyze the emotion.

1127
00:45:30,080 --> 00:45:35,200
We aggregate this data, and we give you productive feedback.

1128
00:45:35,200 --> 00:45:36,400
Who's our target market?

1129
00:45:36,400 --> 00:45:38,000
Well, look around.

1130
00:45:38,000 --> 00:45:39,600
You guys are our target market.

1131
00:45:39,600 --> 00:45:42,480
People who are engineers, people who love to build,

1132
00:45:42,480 --> 00:45:45,160
and say this weekend you made some sort of product

1133
00:45:45,160 --> 00:45:46,200
you want to sell.

1134
00:45:46,200 --> 00:45:49,540
You don't have much experience with sales or outreach.

1135
00:45:49,540 --> 00:45:52,720
With our software, you can record your cold outreach,

1136
00:45:52,720 --> 00:45:54,560
and we can tell you what you've done right

1137
00:45:54,560 --> 00:45:57,440
and how you can improve to hopefully land your product

1138
00:45:57,440 --> 00:45:59,840
where it needs to be.

1139
00:45:59,840 --> 00:46:02,640
And later on, we want to expand to call centers and sales

1140
00:46:02,640 --> 00:46:05,440
staffs, because we think we can spread this

1141
00:46:05,440 --> 00:46:09,320
across an organization, and it can be highly profitable.

1142
00:46:09,320 --> 00:46:14,680
We have usage-based tiering, so $0.75 a minute for 1,000 hours,

1143
00:46:14,680 --> 00:46:18,480
going up to $0.40 for $10,000.

1144
00:46:18,480 --> 00:46:20,880
So this is our software.

1145
00:46:20,880 --> 00:46:22,960
And I want to tell you guys a story.

1146
00:46:22,960 --> 00:46:26,060
I started being an entrepreneur around six months ago,

1147
00:46:26,060 --> 00:46:28,960
and we made an AI body cam analysis startup.

1148
00:46:28,960 --> 00:46:32,080
So I did 100 phone calls, 100 cold calls.

1149
00:46:32,080 --> 00:46:36,720
I got no clients, 200, 300, 500, and 700.

1150
00:46:36,720 --> 00:46:39,080
No one was responding to me.

1151
00:46:39,080 --> 00:46:43,440
So by 800, I got actually three, and I realized something.

1152
00:46:43,440 --> 00:46:45,460
The human brain is pretty amazing.

1153
00:46:45,460 --> 00:46:47,320
We're able to pick up on patterns,

1154
00:46:47,320 --> 00:46:49,920
but at the same time, it's kind of inefficient,

1155
00:46:49,920 --> 00:46:52,920
because it took 800.

1156
00:46:52,920 --> 00:46:56,480
Here, we look at the emotion between every single sentence,

1157
00:46:56,480 --> 00:46:59,000
and we figure out spikes of emotion and decreasing

1158
00:46:59,000 --> 00:47:00,360
emotions.

1159
00:47:00,360 --> 00:47:03,040
We see that when we talk about security and data privacy

1160
00:47:03,040 --> 00:47:06,600
with police officers, it shows an increase in their interest.

1161
00:47:06,600 --> 00:47:09,600
And this was a trend among many conversations we had.

1162
00:47:09,600 --> 00:47:14,640
So in our analysis page, we see in the top left

1163
00:47:14,640 --> 00:47:18,320
that mentioning AI accuracy and efficiency, increased officer

1164
00:47:18,320 --> 00:47:21,080
safety, and discussing cost savings

1165
00:47:21,080 --> 00:47:24,160
really helped us when we were talking to officers,

1166
00:47:24,160 --> 00:47:26,240
because we're some college students, right?

1167
00:47:26,240 --> 00:47:29,240
We're dealing with some pretty confidential data.

1168
00:47:29,240 --> 00:47:32,480
Bringing this up early really helped improve our rates.

1169
00:47:32,480 --> 00:47:35,200
And the four things you see here in the corners

1170
00:47:35,200 --> 00:47:38,240
are the different triggers we generated automatically

1171
00:47:38,240 --> 00:47:40,320
based on the cold calls we had.

1172
00:47:40,320 --> 00:47:45,280
So one is positive reactions, negative reactions,

1173
00:47:45,280 --> 00:47:48,400
deescalating tense situations, and normalizing

1174
00:47:48,400 --> 00:47:50,320
exciting situations.

1175
00:47:50,320 --> 00:47:54,480
We also generate insights, too, based on whatever cold calling

1176
00:47:54,480 --> 00:47:56,760
trends you make.

1177
00:47:56,760 --> 00:48:00,320
We also have a rag, so you can upload your company knowledge,

1178
00:48:00,320 --> 00:48:02,840
your target audience, and your pricing information.

1179
00:48:02,840 --> 00:48:05,640
So if you make a mistake, don't worry, we got your back.

1180
00:48:05,640 --> 00:48:08,480
We'll tell you, hey, maybe instead of saying this,

1181
00:48:08,480 --> 00:48:10,760
you could have said this, because it might have helped you

1182
00:48:10,760 --> 00:48:11,640
out a little bit.

1183
00:48:14,080 --> 00:48:15,760
Sorry, my team picture isn't on here,

1184
00:48:15,760 --> 00:48:18,720
but thank you to Tushar, and Nick, and Krishna.

1185
00:48:18,720 --> 00:48:20,200
You guys were a great team.

1186
00:48:20,200 --> 00:48:22,480
And I'm honored to be here representing you guys today.

1187
00:48:22,480 --> 00:48:23,440
I'm open to feedback.

1188
00:48:23,440 --> 00:48:33,120
I guess I need to be the first person

1189
00:48:33,120 --> 00:48:37,920
to say that you're entering a pretty competitive market

1190
00:48:37,920 --> 00:48:39,520
with other offerings here.

1191
00:48:43,480 --> 00:48:45,000
I'll say something that stuck out to me

1192
00:48:45,000 --> 00:48:47,840
was this idea of insights.

1193
00:48:47,840 --> 00:48:50,720
But I think at an organization, there's

1194
00:48:50,720 --> 00:48:52,800
going to be a sales team, and a marketing team,

1195
00:48:52,840 --> 00:48:54,760
and an online web team.

1196
00:48:54,760 --> 00:48:56,560
And those teams don't really talk to each other,

1197
00:48:56,560 --> 00:48:58,680
so maybe it's interesting to think about how do you pull

1198
00:48:58,680 --> 00:49:01,960
insights from one channel of sales or marketing

1199
00:49:01,960 --> 00:49:03,920
and actually bring that into another channel.

1200
00:49:03,920 --> 00:49:06,040
So maybe the insights from cold calling

1201
00:49:06,040 --> 00:49:08,240
are actually influencing what's going on the website.

1202
00:49:08,240 --> 00:49:12,320
Maybe there's some interesting spots of opportunity there.

1203
00:49:12,320 --> 00:49:14,360
Yeah, I can actually talk about one facet of this.

1204
00:49:14,360 --> 00:49:16,600
We want to explore deeply.

1205
00:49:16,600 --> 00:49:17,720
I want to give you an example.

1206
00:49:17,720 --> 00:49:19,960
Say we have three founders in the company, right?

1207
00:49:19,960 --> 00:49:22,680
I have a first cold call with one person,

1208
00:49:22,720 --> 00:49:24,720
and later on, my second co-founder

1209
00:49:24,720 --> 00:49:27,040
wants to set up a warmer call in the future.

1210
00:49:27,040 --> 00:49:29,600
And then my third founder wants to set up a third call.

1211
00:49:29,600 --> 00:49:33,640
We want to build a profile for this client as they go along,

1212
00:49:33,640 --> 00:49:35,280
so we can truly understand them.

1213
00:49:35,280 --> 00:49:37,920
And also, we want to develop a profile on ourselves, too,

1214
00:49:37,920 --> 00:49:39,960
so we can learn more about ourselves as we go

1215
00:49:39,960 --> 00:49:43,840
and how we're behaving, make sure that we're learning as we go.

1216
00:49:43,840 --> 00:49:47,800
So we're thinking, if we develop a CRM on top of this data

1217
00:49:47,800 --> 00:49:50,600
that we leverage, then we can connect multiple teams

1218
00:49:50,600 --> 00:49:53,320
and enable cross-functional benefit.

1219
00:49:55,520 --> 00:49:56,800
Yeah, I had a similar comment.

1220
00:49:56,800 --> 00:49:58,480
I think it would be really game-changing

1221
00:49:58,480 --> 00:50:01,160
if, in addition to some of the real-time analyses

1222
00:50:01,160 --> 00:50:03,120
your guys are doing around sentiment,

1223
00:50:03,120 --> 00:50:07,560
where you can see the system with information on prior calls

1224
00:50:07,560 --> 00:50:10,040
or this person's particular strengths and weaknesses

1225
00:50:10,040 --> 00:50:13,320
and how they compliment those of the other people on the team

1226
00:50:13,320 --> 00:50:17,200
and to really build a CRM, this knowledge graph around

1227
00:50:17,200 --> 00:50:19,360
each person's strengths and weaknesses on the team

1228
00:50:19,360 --> 00:50:21,240
to be able to better fine-tune the system.

1229
00:50:21,240 --> 00:50:22,920
Yeah, thank you. You guys saw the analysis,

1230
00:50:22,920 --> 00:50:26,440
but also, there's a long list of past conversations.

1231
00:50:26,440 --> 00:50:28,600
You can actually go into every single conversation

1232
00:50:28,600 --> 00:50:30,880
you've ever had and look at it deeply,

1233
00:50:30,880 --> 00:50:33,600
the same way you did in the latest conversation.

1234
00:50:34,920 --> 00:50:38,280
I would think about the full sales funnel.

1235
00:50:38,280 --> 00:50:40,600
This is pretty deep down in it.

1236
00:50:40,600 --> 00:50:42,760
And as you think about,

1237
00:50:42,760 --> 00:50:45,560
where are you really going to be able to convert

1238
00:50:45,560 --> 00:50:49,200
or where's the wedge that really matters?

1239
00:50:49,200 --> 00:50:53,400
Because there's a lot that goes into converting a sale

1240
00:50:53,400 --> 00:50:54,760
and it's not just the cold call.

1241
00:50:54,760 --> 00:50:57,880
So is the issue, are you actually calling the right people?

1242
00:50:57,880 --> 00:51:00,960
Or is the issue, are you actually speaking

1243
00:51:00,960 --> 00:51:02,280
to the right decision-makers?

1244
00:51:02,280 --> 00:51:05,840
So just thinking more broadly about that funnel

1245
00:51:05,840 --> 00:51:09,400
and where you might actually be able to have the most impact

1246
00:51:09,400 --> 00:51:14,400
and have the right wedge into the broader products we.

1247
00:51:14,600 --> 00:51:15,800
Yeah, thanks.

1248
00:51:16,760 --> 00:51:18,800
Thank you. Thank you.

1249
00:51:18,800 --> 00:51:19,720
All right, thank you guys.

1250
00:51:19,720 --> 00:51:20,560
Thank you so much.

1251
00:51:20,560 --> 00:51:21,400
We appreciate it.

1252
00:51:21,400 --> 00:51:22,720
Bye, enjoy your questions.

1253
00:51:24,440 --> 00:51:26,680
Clicker, clicker.

1254
00:51:26,680 --> 00:51:27,520
Thank you.

1255
00:51:27,520 --> 00:51:28,880
Okay.

1256
00:51:28,880 --> 00:51:31,640
Next up, we have Hear Me Out.

1257
00:51:31,640 --> 00:51:33,440
Please welcome Hear Me Out.

1258
00:51:33,440 --> 00:51:42,520
All right, guys.

1259
00:51:42,520 --> 00:51:44,640
Hey, hi, my name is Marcus.

1260
00:51:44,640 --> 00:51:45,480
I'm from Hear Me Out.

1261
00:51:45,480 --> 00:51:47,680
And what we've built is an AI-driven

1262
00:51:47,680 --> 00:51:49,160
customer service pipeline,

1263
00:51:49,160 --> 00:51:51,560
optimizing call matching and visibility.

1264
00:51:51,560 --> 00:51:53,000
So that might leave you scratching your head.

1265
00:51:53,000 --> 00:51:55,600
So let's just talk about the problem.

1266
00:51:55,600 --> 00:51:57,080
So let me give you a bit of context.

1267
00:51:57,080 --> 00:51:58,120
I'm an exchange student

1268
00:51:58,120 --> 00:52:00,320
and when I first came here, I had to deal with so many things.

1269
00:52:00,320 --> 00:52:01,960
I had to deal with banking.

1270
00:52:01,960 --> 00:52:03,160
I had to deal with insurance.

1271
00:52:03,160 --> 00:52:04,480
I had to deal with deliveries.

1272
00:52:04,480 --> 00:52:06,680
And I even had my car break down on me

1273
00:52:06,680 --> 00:52:08,080
and that was a real pain.

1274
00:52:09,480 --> 00:52:11,280
In short, I was overwhelmed by the sheer number

1275
00:52:11,280 --> 00:52:12,440
of customer service calls

1276
00:52:12,440 --> 00:52:13,360
because for each of these things,

1277
00:52:13,360 --> 00:52:16,000
I had to make so many calls just to get things done.

1278
00:52:16,000 --> 00:52:17,800
And I think a lot of you guys can relate to that.

1279
00:52:17,800 --> 00:52:20,160
We've all had our fair share of bad call experiences

1280
00:52:20,160 --> 00:52:21,640
where we're upset,

1281
00:52:21,640 --> 00:52:23,600
the customer service representative is also upset

1282
00:52:23,600 --> 00:52:25,080
and nothing is done.

1283
00:52:25,080 --> 00:52:26,920
We've also had good experiences as well.

1284
00:52:26,920 --> 00:52:27,760
And I think that's the core

1285
00:52:27,760 --> 00:52:29,560
of what we're trying to tackle here.

1286
00:52:29,560 --> 00:52:30,840
We want to create a pipeline

1287
00:52:30,840 --> 00:52:33,760
that tries to provide optimal matching

1288
00:52:33,760 --> 00:52:36,560
and provide visibility on emotional data to businesses.

1289
00:52:37,920 --> 00:52:39,240
So we also did the research

1290
00:52:39,240 --> 00:52:41,440
and I think the numbers speak for itself.

1291
00:52:41,440 --> 00:52:43,360
This is a key problem with a sizable market

1292
00:52:43,360 --> 00:52:46,520
and a sheer number of people are affected by this as well.

1293
00:52:47,720 --> 00:52:49,480
And this is our problem statement, which is,

1294
00:52:49,480 --> 00:52:52,040
how might businesses which offer customer service calls

1295
00:52:52,040 --> 00:52:54,440
provide a better experience for their customers?

1296
00:52:55,600 --> 00:52:58,040
So we think we can tackle this in four key components.

1297
00:52:58,040 --> 00:52:59,880
First of all, an improved call bot.

1298
00:52:59,880 --> 00:53:02,000
We all are common with that initial robo call

1299
00:53:02,000 --> 00:53:02,840
that we have to deal with

1300
00:53:02,840 --> 00:53:04,920
and sometimes it's really, really frustrating.

1301
00:53:04,920 --> 00:53:07,080
How many times have the call bot talk to you

1302
00:53:07,080 --> 00:53:09,000
and it just doesn't direct you to the right person?

1303
00:53:09,000 --> 00:53:11,200
I think we've all experienced that before.

1304
00:53:11,200 --> 00:53:13,080
Second and third, and I think this comes hand in hand,

1305
00:53:13,080 --> 00:53:14,840
it's just business visibility.

1306
00:53:14,840 --> 00:53:16,920
We want to provide businesses with better visibility

1307
00:53:16,920 --> 00:53:20,320
of both call experiences data

1308
00:53:20,320 --> 00:53:23,360
as well as customer representatives bandwidths over the day

1309
00:53:23,360 --> 00:53:25,480
as they continue to take calls.

1310
00:53:25,480 --> 00:53:28,160
And finally, this is where we put those two together.

1311
00:53:28,160 --> 00:53:30,960
We want to take that data and optimize a customer's journey

1312
00:53:30,960 --> 00:53:35,280
through a better customer to service representative matching.

1313
00:53:35,280 --> 00:53:38,120
So I won't bore you with this data,

1314
00:53:38,120 --> 00:53:40,040
but with that in mind,

1315
00:53:40,040 --> 00:53:42,880
we developed a set of decoupled microservices

1316
00:53:42,880 --> 00:53:45,920
and I just want to point three key parts out to you.

1317
00:53:45,920 --> 00:53:49,080
So first of all, we want to assess customer agreeability

1318
00:53:49,080 --> 00:53:50,880
with an initial robo call,

1319
00:53:50,880 --> 00:53:52,840
but this won't just be your normal robo call.

1320
00:53:52,840 --> 00:53:55,360
We want to use Hume's EVI to manage the robo call

1321
00:53:55,360 --> 00:53:56,600
in an empathetic manner

1322
00:53:56,600 --> 00:53:59,200
such that it measures the customer's emotions

1323
00:53:59,200 --> 00:54:00,520
as they go through the call

1324
00:54:00,520 --> 00:54:03,360
and eventually outputs an agreeability score

1325
00:54:03,360 --> 00:54:04,200
for the customer.

1326
00:54:05,160 --> 00:54:07,680
Second of all, we have a call analysis feedback loop

1327
00:54:07,680 --> 00:54:08,920
and that's that whole thing on the right

1328
00:54:08,920 --> 00:54:09,960
that goes down below.

1329
00:54:09,960 --> 00:54:12,120
And what this does is once you have a call connected

1330
00:54:12,120 --> 00:54:14,040
between a customer and a representative,

1331
00:54:14,040 --> 00:54:15,520
it takes in multiple factors of data

1332
00:54:15,520 --> 00:54:17,160
such as the call duration,

1333
00:54:17,160 --> 00:54:18,760
the emotional change over the call

1334
00:54:18,760 --> 00:54:20,680
and the overall call outcome.

1335
00:54:20,680 --> 00:54:22,400
Using Hume's emotional measurement API,

1336
00:54:22,400 --> 00:54:25,040
we can then also generate a call score.

1337
00:54:25,040 --> 00:54:27,280
Finally, and this is the third and key part to this,

1338
00:54:27,280 --> 00:54:28,440
it's the matching API.

1339
00:54:28,440 --> 00:54:30,240
Using the two things that I just mentioned,

1340
00:54:30,240 --> 00:54:31,760
we can best match a customer

1341
00:54:31,760 --> 00:54:33,400
to a customer service representative

1342
00:54:33,400 --> 00:54:35,400
which matches their vibe, their energy

1343
00:54:35,400 --> 00:54:38,560
and their emotions based on how our custom model is developed.

1344
00:54:38,560 --> 00:54:40,480
So what's the outcome of all of this?

1345
00:54:40,480 --> 00:54:42,760
As a representative goes through their day,

1346
00:54:42,760 --> 00:54:44,960
their state changes depending on how their calls go

1347
00:54:44,960 --> 00:54:47,080
and they're bandwidth adjusts accordingly.

1348
00:54:47,080 --> 00:54:48,560
This affects the subsequent customers

1349
00:54:48,560 --> 00:54:51,320
which they are matched to in a positive manner

1350
00:54:51,320 --> 00:54:53,980
and creates a better experience for both parties.

1351
00:54:55,280 --> 00:54:57,040
So there's a lot more which we can build

1352
00:54:57,040 --> 00:54:59,320
with what we have, but with this foundational pipeline,

1353
00:54:59,320 --> 00:55:00,880
we believe we effectively tackle the problem

1354
00:55:00,880 --> 00:55:02,000
that needs to be solved.

1355
00:55:02,000 --> 00:55:02,840
That's all I have for today.

1356
00:55:02,840 --> 00:55:03,680
Thank you.

1357
00:55:03,680 --> 00:55:04,520
Thank you.

1358
00:55:12,200 --> 00:55:13,200
Nice.

1359
00:55:13,200 --> 00:55:14,040
Thank you.

1360
00:55:17,400 --> 00:55:19,080
Yeah, I mean a little bit of feedback

1361
00:55:19,080 --> 00:55:20,560
similar to the last company as well.

1362
00:55:20,560 --> 00:55:23,000
Just there's a lot of companies working in this space too.

1363
00:55:23,000 --> 00:55:25,560
So I would just continue to think through

1364
00:55:25,560 --> 00:55:27,680
how to find that core differentiation

1365
00:55:28,880 --> 00:55:31,800
if you continue to work on this after the hackathon.

1366
00:55:31,800 --> 00:55:32,920
Yeah, I completely agree.

1367
00:55:33,000 --> 00:55:35,960
I think a key part that we thought was really exciting

1368
00:55:35,960 --> 00:55:38,760
was just what you can achieve with custom models.

1369
00:55:38,760 --> 00:55:40,680
What we're doing by developing a feedback loop

1370
00:55:40,680 --> 00:55:42,680
is we're creating something where we can create,

1371
00:55:42,680 --> 00:55:44,680
in a sense, a model which trains itself.

1372
00:55:44,680 --> 00:55:47,120
We can assess how calls might improve

1373
00:55:47,120 --> 00:55:49,400
or get worse after the matching

1374
00:55:49,400 --> 00:55:51,560
and that feedback gets fed straight to the matching API

1375
00:55:51,560 --> 00:55:53,600
so that it knows whether or not it's done a good job or not.

1376
00:55:53,600 --> 00:55:54,760
And we find that really interesting

1377
00:55:54,760 --> 00:55:56,760
and we think that that's a key differentiating factor

1378
00:55:56,760 --> 00:55:57,760
which we can achieve.

1379
00:55:58,760 --> 00:56:03,160
There might be some opportunities for building

1380
00:56:03,160 --> 00:56:06,080
some sort of synthetic data pipeline here

1381
00:56:06,080 --> 00:56:08,120
where you could just sort of simulate calls

1382
00:56:08,120 --> 00:56:12,720
with an AI bot of some sort and use that as feedback.

1383
00:56:12,720 --> 00:56:14,360
I don't know how good that data will be or not

1384
00:56:14,360 --> 00:56:16,240
but it could be interesting.

1385
00:56:16,240 --> 00:56:17,440
Yeah, I know that's a really interesting thought.

1386
00:56:17,440 --> 00:56:18,280
Thank you.

1387
00:56:19,400 --> 00:56:21,000
I know right now you guys are targeting

1388
00:56:21,000 --> 00:56:24,280
customer service agents as well as call centers.

1389
00:56:24,280 --> 00:56:25,880
Something that could be interesting to think about

1390
00:56:26,160 --> 00:56:28,040
as you think about the different stages

1391
00:56:28,040 --> 00:56:29,840
of the software adoption lifecycle

1392
00:56:29,840 --> 00:56:33,320
as you go from your early adopters to your early majority

1393
00:56:33,320 --> 00:56:34,360
and then your late majority

1394
00:56:34,360 --> 00:56:37,000
who's eventually gonna justify your evaluation

1395
00:56:37,000 --> 00:56:39,560
in terms of what those ideal customer profiles

1396
00:56:39,560 --> 00:56:41,840
are going to look like down the line.

1397
00:56:41,840 --> 00:56:43,400
Yeah, thank you for that.

1398
00:56:43,400 --> 00:56:47,000
I think one key thing was we actually had a judge come to us

1399
00:56:47,000 --> 00:56:48,760
and talk to us about how they were doing something similar

1400
00:56:48,760 --> 00:56:50,520
for sales representatives as well

1401
00:56:50,520 --> 00:56:51,560
and we found that really interesting.

1402
00:56:51,560 --> 00:56:53,600
So happy to figure out how we can pivot

1403
00:56:53,600 --> 00:56:54,960
if that need arises.

1404
00:56:56,880 --> 00:56:59,200
Thank you so much.

1405
00:56:59,200 --> 00:57:00,520
Thank you.

1406
00:57:00,520 --> 00:57:01,720
Thank you, hear me out.

1407
00:57:02,840 --> 00:57:06,040
All right, next up we have Dispatch AI.

1408
00:57:06,040 --> 00:57:07,400
Please welcome Dispatch.

1409
00:57:15,280 --> 00:57:18,640
Hi everyone, my name is Spike and I'm with Dispatch AI.

1410
00:57:20,400 --> 00:57:24,200
In the United States, over 80% of 911 call centers

1411
00:57:24,280 --> 00:57:26,160
are critically understaffed.

1412
00:57:26,160 --> 00:57:27,800
This means that in a crisis,

1413
00:57:27,800 --> 00:57:30,520
people with real emergencies aren't able

1414
00:57:30,520 --> 00:57:32,160
to get the support they need

1415
00:57:32,160 --> 00:57:34,560
because all the human agents are busy

1416
00:57:35,440 --> 00:57:37,600
and they're often put on hold.

1417
00:57:37,600 --> 00:57:40,320
This is particularly an issue in our neighboring city

1418
00:57:40,320 --> 00:57:44,160
of Oakland where last year the average wait time

1419
00:57:44,160 --> 00:57:47,840
was over 60 seconds to pick up a 911 call.

1420
00:57:47,840 --> 00:57:50,880
Now in an emergency, every second counts

1421
00:57:50,880 --> 00:57:52,720
and this could be literally the difference

1422
00:57:52,720 --> 00:57:54,600
between life and death.

1423
00:57:54,600 --> 00:57:58,840
This is unacceptable and that's why we built Dispatch AI.

1424
00:57:58,840 --> 00:58:02,600
The world's first AI 911 call operator

1425
00:58:02,600 --> 00:58:05,480
designed to eliminate these wait times.

1426
00:58:06,480 --> 00:58:09,720
Our system is powered by two key components.

1427
00:58:09,720 --> 00:58:11,880
First is the voice AI.

1428
00:58:11,880 --> 00:58:14,520
The voice AI will step into calls

1429
00:58:14,520 --> 00:58:16,720
when all human agents are busy

1430
00:58:16,720 --> 00:58:18,840
and it will work with the caller

1431
00:58:18,880 --> 00:58:23,280
to evaluate their situation, extract the location

1432
00:58:23,280 --> 00:58:26,040
and optionally dispatch first responders

1433
00:58:26,040 --> 00:58:27,280
directly to the scene.

1434
00:58:28,880 --> 00:58:32,360
And the second part is our powerful dashboard

1435
00:58:32,360 --> 00:58:34,080
for the operator themselves.

1436
00:58:34,080 --> 00:58:37,240
So the operator will have access to a bird's eye view

1437
00:58:37,240 --> 00:58:39,400
of all of the ongoing calls

1438
00:58:39,400 --> 00:58:42,520
which will be automatically triaged by the AI

1439
00:58:42,520 --> 00:58:45,680
into different forms of severity or priority.

1440
00:58:45,720 --> 00:58:48,480
Further, they'll see that the AI

1441
00:58:48,480 --> 00:58:51,160
will automatically extract the location

1442
00:58:51,160 --> 00:58:54,480
and will provide a live transcript of the call

1443
00:58:54,480 --> 00:58:56,560
so that they can quickly see what's going on

1444
00:58:56,560 --> 00:59:00,080
and even step into the call once they're available.

1445
00:59:00,080 --> 00:59:02,680
Further, they have buttons that allow them

1446
00:59:02,680 --> 00:59:04,440
to directly, with just one click

1447
00:59:04,440 --> 00:59:06,200
because the location's already fetched,

1448
00:59:06,200 --> 00:59:09,920
dispatch police, firefighters or paramedics.

1449
00:59:09,920 --> 00:59:13,440
All of this is done from a human centric angle

1450
00:59:13,440 --> 00:59:15,040
and the way how we achieve this

1451
00:59:15,040 --> 00:59:18,400
is by taking into account the caller's emotions.

1452
00:59:18,400 --> 00:59:22,180
So for instance, when a caller shows signs of anxiety

1453
00:59:22,180 --> 00:59:25,520
or fear, the system could work more to calm them down

1454
00:59:25,520 --> 00:59:26,640
and make them feel at ease

1455
00:59:26,640 --> 00:59:29,720
before taking the next safe step.

1456
00:59:29,720 --> 00:59:31,880
This system is fully designed

1457
00:59:31,880 --> 00:59:34,720
with ethical safe guides in mind

1458
00:59:34,720 --> 00:59:38,000
and part of that was fine tuning a model

1459
00:59:38,000 --> 00:59:41,400
on over 50911 calls

1460
00:59:41,400 --> 00:59:43,800
so that it could understand the proper protocols

1461
00:59:43,840 --> 00:59:48,840
and be knowledgeable on a wide variety of possible scenarios

1462
00:59:51,280 --> 00:59:53,560
in which a 911 operator could assist in,

1463
00:59:53,560 --> 00:59:56,440
including fake calls or instances

1464
00:59:56,440 --> 00:59:58,240
where it may not need assistance.

1465
00:59:59,640 --> 01:00:02,080
This is all powered by our innovative tech stack

1466
01:00:02,080 --> 01:00:04,400
that utilizes a variety of AI components,

1467
01:00:04,400 --> 01:00:08,760
including the voice AI, the emotional analysis

1468
01:00:08,760 --> 01:00:10,860
and of course a key component of this,

1469
01:00:10,860 --> 01:00:12,680
the fine tuning itself.

1470
01:00:12,680 --> 01:00:16,280
Our mission is to make requesting emergency services

1471
01:00:16,280 --> 01:00:18,660
more effective and efficient.

1472
01:00:18,660 --> 01:00:20,000
Thank you.

1473
01:00:20,000 --> 01:00:30,700
I'll go first.

1474
01:00:30,700 --> 01:00:32,880
I thought you did a great job.

1475
01:00:32,880 --> 01:00:35,120
I thought you presented the problem set,

1476
01:00:35,120 --> 01:00:37,880
the opportunity and the product very clearly.

1477
01:00:38,880 --> 01:00:39,960
It only had three minutes,

1478
01:00:39,960 --> 01:00:42,040
but you hit all the relevant points.

1479
01:00:42,040 --> 01:00:42,880
Thank you.

1480
01:00:45,080 --> 01:00:47,040
The one thing I would encourage you

1481
01:00:47,040 --> 01:00:48,760
to think about a little bit is

1482
01:00:49,720 --> 01:00:52,640
sort of like the optimization function

1483
01:00:52,640 --> 01:00:54,440
for these municipalities, right?

1484
01:00:54,440 --> 01:00:56,480
Cause if people in Oakland are waiting 60 seconds

1485
01:00:56,480 --> 01:00:58,520
to get their 911 call answered,

1486
01:00:58,520 --> 01:00:59,960
like there's a reason for that.

1487
01:00:59,960 --> 01:01:00,800
I don't know what that is,

1488
01:01:00,800 --> 01:01:02,480
but somehow these municipalities have decided

1489
01:01:02,480 --> 01:01:04,160
that that's how they want it to be.

1490
01:01:04,160 --> 01:01:06,920
And so I would just think about like,

1491
01:01:06,920 --> 01:01:09,280
you know, as you bring in AI to this problem,

1492
01:01:09,280 --> 01:01:11,760
doing the like potentially difficult AB tests

1493
01:01:11,800 --> 01:01:13,160
of making sure that whatever it is

1494
01:01:13,160 --> 01:01:14,880
that these municipalities are actually optimizing for

1495
01:01:14,880 --> 01:01:16,440
is actually improved by this.

1496
01:01:16,440 --> 01:01:18,600
Cause it seems like a no-brainer when you first say it,

1497
01:01:18,600 --> 01:01:21,760
but like clearly it's this way for some reason

1498
01:01:21,760 --> 01:01:24,200
that it's probably nuanced and tricky.

1499
01:01:24,200 --> 01:01:25,800
So just something to think about.

1500
01:01:32,080 --> 01:01:33,240
Any other feedback?

1501
01:01:35,240 --> 01:01:36,240
Just following up on that,

1502
01:01:36,240 --> 01:01:38,040
I think the key is ease of adoption.

1503
01:01:38,040 --> 01:01:39,040
I mean, I think you,

1504
01:01:39,040 --> 01:01:41,640
it's easy going to be easy to make a productivity argument

1505
01:01:41,640 --> 01:01:43,320
to the city of Oakland,

1506
01:01:43,320 --> 01:01:46,200
but then you got to think about who's actually installing,

1507
01:01:46,200 --> 01:01:48,440
who's paying for this and who's installing it.

1508
01:01:52,320 --> 01:01:53,160
Okay, that's good.

1509
01:01:53,160 --> 01:01:54,360
Thank you so much.

1510
01:01:54,360 --> 01:01:55,200
Thank you.

1511
01:01:55,200 --> 01:01:57,560
He just batch.

1512
01:01:59,200 --> 01:02:00,760
All right.

1513
01:02:00,760 --> 01:02:04,880
And next up, we have ASL Brigify.

1514
01:02:04,880 --> 01:02:06,480
Please welcome ASL Brigify.

1515
01:02:12,640 --> 01:02:14,280
Hello, my name is Isha

1516
01:02:14,280 --> 01:02:17,080
and today I'll be presenting ASL Brigify,

1517
01:02:17,080 --> 01:02:21,040
the next generation of sign language and interactive learning.

1518
01:02:22,600 --> 01:02:23,440
Oh.

1519
01:02:29,000 --> 01:02:30,560
Oh, this?

1520
01:02:30,560 --> 01:02:31,640
Okay.

1521
01:02:31,640 --> 01:02:32,520
Sorry.

1522
01:02:32,520 --> 01:02:34,680
So what was the inspiration behind this?

1523
01:02:34,680 --> 01:02:37,480
Well, ASL is the third most studied language

1524
01:02:37,480 --> 01:02:39,000
after Spanish and English

1525
01:02:39,000 --> 01:02:41,040
and over a billion people are projected

1526
01:02:41,040 --> 01:02:43,680
to have some sort of hearing loss deficiency,

1527
01:02:43,680 --> 01:02:45,680
which is why it's even more important

1528
01:02:45,680 --> 01:02:48,480
to have a seamless way of for people

1529
01:02:48,480 --> 01:02:49,880
with hearing loss deficiencies

1530
01:02:49,880 --> 01:02:53,160
to communicate with people without them and vice versa.

1531
01:02:53,160 --> 01:02:56,720
And next, there's over a 15,000% return on investment

1532
01:02:56,720 --> 01:02:58,360
over a 10 year period,

1533
01:02:58,360 --> 01:03:00,480
demonstrating the value proposition

1534
01:03:00,480 --> 01:03:03,280
and existing platforms like Duolingo surprisingly

1535
01:03:03,280 --> 01:03:05,840
do not take into account ASL learning,

1536
01:03:05,840 --> 01:03:07,800
which is why it's important to build

1537
01:03:08,280 --> 01:03:11,760
an interactive platform where individuals

1538
01:03:11,760 --> 01:03:15,080
can retrieve the accuracy of their signed texts

1539
01:03:15,080 --> 01:03:17,320
as well as characters.

1540
01:03:22,160 --> 01:03:25,600
Now, our solution includes three proprietary AI models.

1541
01:03:25,600 --> 01:03:27,560
First, we use random forest,

1542
01:03:27,560 --> 01:03:30,440
the random forest algorithm in order to,

1543
01:03:30,440 --> 01:03:33,960
in order to map input pose estimation frames,

1544
01:03:33,960 --> 01:03:38,000
a frame length of 200 to the predicted,

1545
01:03:38,000 --> 01:03:40,760
to the predicted alphabet from A to Z.

1546
01:03:40,760 --> 01:03:43,640
Next, we also use an LSTM model,

1547
01:03:43,640 --> 01:03:45,760
which captures sequential dependencies

1548
01:03:45,760 --> 01:03:49,560
to map from hand pose coordinates

1549
01:03:49,560 --> 01:03:52,640
to the actual word.

1550
01:03:52,640 --> 01:03:56,120
And then we also have our individualized RAG

1551
01:03:56,120 --> 01:04:01,120
calling in Lang chain as well as PDFs

1552
01:04:01,880 --> 01:04:05,000
PDFs that are specific to ASL learning

1553
01:04:05,000 --> 01:04:07,160
that get chunked and transformed

1554
01:04:07,160 --> 01:04:09,760
in a vector dimension space.

1555
01:04:09,760 --> 01:04:11,440
Now, as you can see here,

1556
01:04:11,440 --> 01:04:16,440
this is a hand pose estimation extraction

1557
01:04:16,720 --> 01:04:18,400
using the media pipe library.

1558
01:04:18,400 --> 01:04:20,440
So you can see A, B and C.

1559
01:04:22,040 --> 01:04:24,720
And here's our platform where you can,

1560
01:04:24,720 --> 01:04:26,480
there are different modules to learn

1561
01:04:26,480 --> 01:04:28,920
alphabets, signs, as well as sentences.

1562
01:04:28,920 --> 01:04:33,040
And we even have, we even have a real-time ASL practice.

1563
01:04:33,040 --> 01:04:36,680
So in real-time to capture the sign that you are actually,

1564
01:04:36,680 --> 01:04:38,260
the letter that you're actually signing

1565
01:04:38,260 --> 01:04:40,320
and give you the accuracy for that.

1566
01:04:40,320 --> 01:04:43,640
So here's an example of us using the media pipe library

1567
01:04:43,640 --> 01:04:47,560
to actually extract all of the hand key points.

1568
01:04:47,560 --> 01:04:51,400
And here are some videos where they're over hundreds

1569
01:04:51,400 --> 01:04:53,040
of words that you can actually view

1570
01:04:53,040 --> 01:04:57,000
to learn each of the hand signing frames.

1571
01:04:57,040 --> 01:04:59,720
Now, this is our proprietary RAG.

1572
01:04:59,720 --> 01:05:04,320
And the way we've trained this is we've collected,

1573
01:05:04,320 --> 01:05:06,200
we've collected a variety of PDFs

1574
01:05:06,200 --> 01:05:10,840
that are essentially manuals for ASL learning.

1575
01:05:10,840 --> 01:05:12,560
And potentially in the future,

1576
01:05:12,560 --> 01:05:16,760
we would want to incorporate things like YouTube transcriptions

1577
01:05:16,760 --> 01:05:21,200
that can actually be transformed and embedded

1578
01:05:21,200 --> 01:05:24,280
within our vector dimension model.

1579
01:05:25,280 --> 01:05:28,480
Now, in the future, this doesn't,

1580
01:05:28,480 --> 01:05:32,600
ASL doesn't just, hand pose estimation doesn't just have

1581
01:05:32,600 --> 01:05:36,520
to be localized to ASL.

1582
01:05:36,520 --> 01:05:38,400
There are plenty of other opportunities

1583
01:05:38,400 --> 01:05:42,320
for human pose estimation, including fields

1584
01:05:42,320 --> 01:05:44,040
like dance, martial arts,

1585
01:05:44,040 --> 01:05:46,520
where you can not only identify certain techniques,

1586
01:05:46,520 --> 01:05:50,720
but you can also get feedback generations

1587
01:05:50,720 --> 01:05:53,400
from certain input frames.

1588
01:05:53,400 --> 01:05:55,920
And in the future, this could also be integrated

1589
01:05:55,920 --> 01:05:59,640
into existing solutions, such as Zoom, Loom,

1590
01:05:59,640 --> 01:06:01,120
and FaceTime videos.

1591
01:06:01,120 --> 01:06:06,200
So if there's, so given the signing

1592
01:06:06,200 --> 01:06:09,360
of a certain sentence transcript,

1593
01:06:09,360 --> 01:06:13,320
you can get in real time the actual predicted sentences

1594
01:06:13,320 --> 01:06:14,160
and words.

1595
01:06:16,240 --> 01:06:17,080
Okay.

1596
01:06:17,840 --> 01:06:18,680
Thank you.

1597
01:06:18,680 --> 01:06:19,520
Thank you.

1598
01:06:19,520 --> 01:06:20,360
Thank you.

1599
01:06:28,080 --> 01:06:30,160
That's nice work for 36 hours.

1600
01:06:33,920 --> 01:06:38,920
I would be, I spent some time creating assistive

1601
01:06:40,440 --> 01:06:41,960
technologies for the blind,

1602
01:06:41,960 --> 01:06:45,720
and I would be just very aware of the market

1603
01:06:45,760 --> 01:06:49,400
and how you'll approach it and who will be paying.

1604
01:06:49,400 --> 01:06:52,400
I think that will be a good thing to pay attention to.

1605
01:06:52,400 --> 01:06:53,240
Thank you.

1606
01:06:56,480 --> 01:06:59,360
Yeah, as you think about the market,

1607
01:06:59,360 --> 01:07:01,240
I feel like these language learning apps

1608
01:07:01,240 --> 01:07:06,080
are tricky to kind of scale to meaningful businesses.

1609
01:07:06,080 --> 01:07:07,840
There was sort of like Rosetta Stone,

1610
01:07:07,840 --> 01:07:08,840
whatever, 20 years ago,

1611
01:07:08,840 --> 01:07:10,080
and then there's been like Duolingo

1612
01:07:10,080 --> 01:07:11,080
on this most recent gen,

1613
01:07:11,080 --> 01:07:12,600
but there aren't like that many

1614
01:07:12,600 --> 01:07:14,000
that get to meaningful scale.

1615
01:07:14,040 --> 01:07:16,920
So might be worth just thinking about that market

1616
01:07:16,920 --> 01:07:19,960
and what are the kind of success drivers?

1617
01:07:19,960 --> 01:07:22,160
I think even as I mentioned previously,

1618
01:07:22,160 --> 01:07:24,640
apart from just hand pose estimation,

1619
01:07:24,640 --> 01:07:26,120
I think that there's a big market

1620
01:07:26,120 --> 01:07:27,920
for body pose estimation.

1621
01:07:27,920 --> 01:07:30,720
I think especially in things like combat training,

1622
01:07:30,720 --> 01:07:32,800
especially like if you look at the military,

1623
01:07:32,800 --> 01:07:35,680
even dance performance companies

1624
01:07:35,680 --> 01:07:37,000
where they have to train dancers

1625
01:07:37,000 --> 01:07:39,600
and they're actually specific techniques

1626
01:07:39,600 --> 01:07:43,400
in which they'd want ground truth feedbacks for,

1627
01:07:43,400 --> 01:07:45,920
I think those are also potential markets

1628
01:07:45,920 --> 01:07:48,320
that could be ready to penetrate into.

1629
01:07:49,440 --> 01:07:52,280
You chose more traditional machine learning algorithms

1630
01:07:52,280 --> 01:07:54,800
in early neural nets like LSTM,

1631
01:07:54,800 --> 01:07:58,160
and that may be the right answer.

1632
01:07:58,160 --> 01:07:59,080
That's not obvious to me,

1633
01:07:59,080 --> 01:08:01,880
but I think for today's audience,

1634
01:08:01,880 --> 01:08:03,680
we need to explain why you're not using

1635
01:08:03,680 --> 01:08:07,680
more contemporary GNAI algorithms.

1636
01:08:07,680 --> 01:08:10,800
Yeah, so initially we were actually thinking about

1637
01:08:10,800 --> 01:08:15,440
using more encoder-based transformer models,

1638
01:08:15,440 --> 01:08:18,480
but we ran into some struggles,

1639
01:08:18,480 --> 01:08:23,480
so we just ended up settling on the LSTMs,

1640
01:08:23,720 --> 01:08:28,040
but in the future, we would obviously adapt

1641
01:08:28,040 --> 01:08:31,000
more of the state of the art transformers,

1642
01:08:31,000 --> 01:08:34,800
and even in the case for feedback generation

1643
01:08:34,800 --> 01:08:36,480
for given hand poses,

1644
01:08:36,480 --> 01:08:39,800
that could be an easy encoder-to-decoder

1645
01:08:39,800 --> 01:08:43,280
multi-self-attention model that you could train.

1646
01:08:46,640 --> 01:08:47,720
Okay, thank you so much.

1647
01:08:47,720 --> 01:08:48,560
Thank you.

1648
01:08:48,560 --> 01:08:49,400
Thank you.

1649
01:08:51,680 --> 01:08:55,920
All right, our last contestant for the grand prize

1650
01:08:55,920 --> 01:08:56,800
is Greenwise.

1651
01:08:56,800 --> 01:08:58,400
Please welcome Greenwise.

1652
01:08:58,400 --> 01:08:59,240
Thank you.

1653
01:09:05,720 --> 01:09:10,480
When I was 14, I stopped eating all meat.

1654
01:09:11,960 --> 01:09:13,760
I lasted about two months.

1655
01:09:16,120 --> 01:09:18,680
Now, even though I still eat meat,

1656
01:09:18,680 --> 01:09:21,080
there are a lot of small changes you can make

1657
01:09:21,080 --> 01:09:24,440
that have a huge impact on your carbon footprint.

1658
01:09:24,440 --> 01:09:28,160
For example, by switching from beef to chicken,

1659
01:09:28,240 --> 01:09:32,520
you cut the carbon footprint of your meals by six times.

1660
01:09:32,520 --> 01:09:36,080
What we do is we help you make that switch

1661
01:09:36,080 --> 01:09:38,120
from beef to chicken for everything,

1662
01:09:38,120 --> 01:09:41,800
for your shoes, your shirt, household supplies, food.

1663
01:09:41,800 --> 01:09:43,960
Everything has a carbon cost

1664
01:09:43,960 --> 01:09:46,240
and a carbon footprint that we can mitigate.

1665
01:09:47,960 --> 01:09:51,600
So how does a consumer analyze all their purchases

1666
01:09:51,600 --> 01:09:53,520
and the carbon footprint of anything

1667
01:09:53,520 --> 01:09:56,280
and try to make all these very difficult decisions

1668
01:09:56,320 --> 01:10:00,120
and research about how they should change their actions?

1669
01:10:00,120 --> 01:10:02,480
Well, this is where Greenwise comes in.

1670
01:10:02,480 --> 01:10:07,120
We seamlessly integrate with existing customer purchase models

1671
01:10:07,120 --> 01:10:12,120
to basically input what the consumer is already doing,

1672
01:10:12,160 --> 01:10:14,800
for example, through receipts or through emails,

1673
01:10:14,800 --> 01:10:19,200
and we integrate with Apple Pay, with Amazon,

1674
01:10:19,200 --> 01:10:22,240
and with Square to automatically get their purchases

1675
01:10:22,240 --> 01:10:23,800
into our system.

1676
01:10:23,800 --> 01:10:27,800
From there, we vectorize their purchase

1677
01:10:27,800 --> 01:10:30,040
and compare it to our vector database.

1678
01:10:30,040 --> 01:10:32,840
This database has all the carbon footprints

1679
01:10:32,840 --> 01:10:37,280
of over 10,000 products that we've analyzed

1680
01:10:37,280 --> 01:10:39,760
and made sure that these are accurate carbon estimates.

1681
01:10:39,760 --> 01:10:42,320
Additionally, by using the vector embedding,

1682
01:10:42,320 --> 01:10:45,920
we make sure that these similarity scores are very accurate.

1683
01:10:45,920 --> 01:10:48,400
It's not an LLM that can hallucinate.

1684
01:10:48,400 --> 01:10:52,960
These are real accuracy scores and real carbon predictions.

1685
01:10:52,960 --> 01:10:54,720
From there, it directly can tell them

1686
01:10:54,720 --> 01:10:57,600
an alternative product that is very similar

1687
01:10:57,600 --> 01:11:00,520
but has less carbon footprint.

1688
01:11:00,520 --> 01:11:03,400
Additionally, this provides a lot of room for scaling

1689
01:11:03,400 --> 01:11:05,200
when other businesses want to analyze

1690
01:11:05,200 --> 01:11:07,120
their carbon footprint for their products

1691
01:11:08,160 --> 01:11:11,040
or for events and other bigger venues.

1692
01:11:14,400 --> 01:11:19,400
So from good intentions to reality, let's make it happen.

1693
01:11:20,320 --> 01:11:21,560
Thank you.

1694
01:11:30,080 --> 01:11:31,760
It's a very innovative rag use case.

1695
01:11:31,760 --> 01:11:33,360
I never thought of that.

1696
01:11:33,360 --> 01:11:35,480
We're not using graph.

1697
01:11:35,480 --> 01:11:36,840
Oh, it's not.

1698
01:11:36,840 --> 01:11:39,720
It's similar in that it uses a vector embedding

1699
01:11:39,720 --> 01:11:42,360
for finding similarity,

1700
01:11:42,360 --> 01:11:45,400
but the similarity is directly the output.

1701
01:11:45,400 --> 01:11:46,280
Yes, that's right.

1702
01:11:46,280 --> 01:11:49,720
Yeah, yeah, that's all it is.

1703
01:11:49,720 --> 01:11:52,960
Is this a subscription product?

1704
01:11:52,960 --> 01:11:53,560
So currently?

1705
01:11:53,560 --> 01:11:57,320
Are you would imagine it being a subscription product?

1706
01:11:57,320 --> 01:11:59,760
We would imagine, or maybe you could talk about it.

1707
01:11:59,760 --> 01:12:03,200
Probably not.

1708
01:12:03,200 --> 01:12:06,560
Ideally, we'd integrate with existing businesses,

1709
01:12:06,560 --> 01:12:14,560
like Instacart or Safeway, so that they can show our results

1710
01:12:14,560 --> 01:12:19,680
on how the carbon footprint of certain products

1711
01:12:19,680 --> 01:12:27,200
is on their app, but it also works for consumers

1712
01:12:27,200 --> 01:12:30,480
to use on their own, as demonstrated here.

1713
01:12:33,560 --> 01:12:35,560
People wouldn't pay for a subscription, though.

1714
01:12:42,000 --> 01:12:43,280
OK, I think that's all the comments.

1715
01:12:43,280 --> 01:12:44,280
Thank you so much.

1716
01:12:45,240 --> 01:12:49,560
Green wise, thank you.

1717
01:12:49,560 --> 01:12:52,800
All right, I would now like to invite our esteemed judges

1718
01:12:52,800 --> 01:12:57,920
to convene in this secret room, where judges make their decisions.

1719
01:12:57,920 --> 01:13:00,800
And we are going to have the special prizes.

1720
01:13:00,800 --> 01:13:02,760
So as I mentioned, a bunch of sponsors

1721
01:13:02,760 --> 01:13:04,800
came to make this all happen.

1722
01:13:04,800 --> 01:13:07,320
We're an educational program, and it is entirely

1723
01:13:07,320 --> 01:13:09,040
the support of these sponsors.

1724
01:13:09,040 --> 01:13:12,440
And they're not just providing support, they got cool prizes.

1725
01:13:12,480 --> 01:13:15,600
So let's bring them on in just a minute.

1726
01:13:15,600 --> 01:13:17,040
You're going to hear from each one.

1727
01:13:17,040 --> 01:13:19,400
These are the sponsors for today.

1728
01:13:19,400 --> 01:13:21,920
And I also want to thank our community sponsors.

1729
01:13:21,920 --> 01:13:24,200
These are startups, very cool startups

1730
01:13:24,200 --> 01:13:28,000
who hung out and helped our young hackers with their needs

1731
01:13:28,000 --> 01:13:29,920
and their cool tools.

1732
01:13:29,920 --> 01:13:33,200
All right, so our very first special prize

1733
01:13:33,200 --> 01:13:38,000
is going to be announced by a very special campus partner.

1734
01:13:38,000 --> 01:13:42,000
I'd like to welcome the Academic Innovation

1735
01:13:42,000 --> 01:13:42,720
Catalyst.

1736
01:13:42,720 --> 01:13:45,840
I'd like to welcome out here Matt Sincini and Paul

1737
01:13:45,840 --> 01:13:49,720
Warp to tell you about AIC, one of our newest campus partners

1738
01:13:49,720 --> 01:13:51,040
doing very cool stuff.

1739
01:13:51,040 --> 01:13:52,320
Please give them a welcome.

1740
01:13:52,320 --> 01:13:54,520
Thank you.

1741
01:13:54,520 --> 01:13:55,920
Thank you so much, Caroline.

1742
01:13:55,920 --> 01:13:57,280
It's just a thrill to be here.

1743
01:13:57,280 --> 01:14:02,080
So my partner and I, Paul and I created Academic Innovation

1744
01:14:02,080 --> 01:14:06,240
Catalyst, or AIC, to release more innovation

1745
01:14:06,240 --> 01:14:10,240
from academic research for positive social impact.

1746
01:14:10,240 --> 01:14:13,720
And we're focused initially on the areas of climate tech

1747
01:14:13,720 --> 01:14:16,960
and AI for good, which is why we're here today.

1748
01:14:16,960 --> 01:14:17,880
How do we do this?

1749
01:14:17,880 --> 01:14:20,600
Well, we make proof of concept grants,

1750
01:14:20,600 --> 01:14:24,120
so no strings attached, non-dilutive grants

1751
01:14:24,120 --> 01:14:26,440
to academics with an idea.

1752
01:14:26,440 --> 01:14:28,760
Then we help them take that idea, carry it

1753
01:14:28,760 --> 01:14:33,880
through commercialization to scale and sustainability.

1754
01:14:33,880 --> 01:14:35,520
So that's what we do.

1755
01:14:35,520 --> 01:14:37,680
We're thrilled to be here today, and we'll

1756
01:14:37,680 --> 01:14:42,200
be making two awards to the most compelling business

1757
01:14:42,200 --> 01:14:45,800
plans or innovations involving the use of AI

1758
01:14:45,800 --> 01:14:47,680
to make the world a better place.

1759
01:14:47,680 --> 01:14:50,440
And we couldn't be more excited to announce them

1760
01:14:50,440 --> 01:14:52,480
in five seconds here.

1761
01:14:52,480 --> 01:14:55,680
I'll just say that we met with many amazing teams.

1762
01:14:55,680 --> 01:14:57,480
It's been an extraordinary weekend.

1763
01:14:57,480 --> 01:14:59,680
Thank you so much for including us.

1764
01:14:59,680 --> 01:15:01,360
We had to narrow it down to two.

1765
01:15:01,360 --> 01:15:03,080
It was tough, but I think you'll see

1766
01:15:03,080 --> 01:15:04,280
that they're well deserving.

1767
01:15:04,280 --> 01:15:07,440
So with that, let me hand it to my partner, Paul Warp,

1768
01:15:07,440 --> 01:15:12,800
to announce the winners of the AIC AI for Good Awards in 19,

1769
01:15:12,800 --> 01:15:14,200
I'm sorry, 2024.

1770
01:15:16,840 --> 01:15:20,240
This is what happens when you get old people up here on stage.

1771
01:15:20,240 --> 01:15:23,600
So anyway, we are really thrilled to be here, as Matt said,

1772
01:15:23,600 --> 01:15:25,800
and we're especially thrilled with the fact

1773
01:15:25,800 --> 01:15:28,440
that so many of you are putting your talents

1774
01:15:28,440 --> 01:15:33,040
to work for such great causes and for the betterment

1775
01:15:33,040 --> 01:15:34,320
of humanity.

1776
01:15:34,320 --> 01:15:37,280
And AI has so much potential in so many realms,

1777
01:15:37,280 --> 01:15:41,240
but among the most important is to make the world a better

1778
01:15:41,240 --> 01:15:42,880
place and to make a social impact.

1779
01:15:42,880 --> 01:15:45,080
And so with that, we're thrilled to announce

1780
01:15:45,080 --> 01:15:50,280
the first two winners, Dispatch AI and ASL Brigify.

1781
01:15:50,280 --> 01:15:58,120
So these are tremendous companies.

1782
01:15:58,120 --> 01:16:01,320
Again, the competition was so strong.

1783
01:16:01,320 --> 01:16:04,040
May I ask actually both sets of winners

1784
01:16:04,040 --> 01:16:07,640
to stand in the audience here?

1785
01:16:07,640 --> 01:16:11,760
And thank you again so much for the terrific work.

1786
01:16:11,760 --> 01:16:14,640
I think as you heard, ASL Brigify

1787
01:16:14,640 --> 01:16:21,040
is doing for sign language what Duolingo has done for learning

1788
01:16:21,040 --> 01:16:21,920
other languages.

1789
01:16:21,920 --> 01:16:23,000
And it is so important.

1790
01:16:23,000 --> 01:16:26,280
It's incredible and shocking that it's an underserved

1791
01:16:26,280 --> 01:16:27,920
and currently not served market.

1792
01:16:27,920 --> 01:16:30,680
And their technologies are going to change that.

1793
01:16:30,680 --> 01:16:33,440
And Dispatch AI, what can you say?

1794
01:16:33,440 --> 01:16:36,000
I mean, it's such an important issue

1795
01:16:36,000 --> 01:16:38,320
to be able to get emergency response,

1796
01:16:38,320 --> 01:16:40,520
to be able to get a response when you need it.

1797
01:16:40,520 --> 01:16:43,680
And of course, the reality is when we have, unfortunately,

1798
01:16:43,680 --> 01:16:46,200
too many mass catastrophes, the time

1799
01:16:46,200 --> 01:16:49,240
when you need the response strapped

1800
01:16:49,240 --> 01:16:53,120
is that you're often most short staffed.

1801
01:16:53,120 --> 01:16:56,840
And so Dispatch AI is using artificial intelligence

1802
01:16:56,840 --> 01:17:01,080
and a variety of technologies to speed that process up

1803
01:17:01,080 --> 01:17:04,880
and to help both the dispatchers and the people

1804
01:17:04,880 --> 01:17:06,240
that the dispatchers are helping.

1805
01:17:06,240 --> 01:17:08,960
And so can I ask the Dispatch AI team to stand up as well

1806
01:17:08,960 --> 01:17:09,920
and be recognized?

1807
01:17:09,920 --> 01:17:14,840
It's a great job.

1808
01:17:14,840 --> 01:17:17,520
Congratulations to all of you and to everyone

1809
01:17:17,520 --> 01:17:18,400
who is here today.

1810
01:17:18,400 --> 01:17:20,040
Thank you so much.

1811
01:17:20,040 --> 01:17:21,200
Thank you, Madam Paul.

1812
01:17:21,200 --> 01:17:23,440
Thank you, Academic Innovation Catalyst.

1813
01:17:23,440 --> 01:17:25,800
Our next special prize is going to be

1814
01:17:25,800 --> 01:17:28,960
introduced by our very own General Manager at Skydeck,

1815
01:17:28,960 --> 01:17:30,080
Sybil Chen.

1816
01:17:30,080 --> 01:17:30,880
Give her a welcome.

1817
01:17:30,880 --> 01:17:31,380
Woo!

1818
01:17:36,380 --> 01:17:37,080
Hello, everyone.

1819
01:17:37,080 --> 01:17:41,000
Hope everyone has had a great weekend.

1820
01:17:41,000 --> 01:17:44,520
At Skydeck, we have about a year and a half ago,

1821
01:17:44,520 --> 01:17:47,680
we launched the Skydeck Climate Tech track.

1822
01:17:47,680 --> 01:17:51,080
In part, thanks to a nice grant from the university,

1823
01:17:51,080 --> 01:17:54,400
we had $150,000 to build out the track.

1824
01:17:54,400 --> 01:17:57,480
And right away, we started putting that to work.

1825
01:17:57,480 --> 01:18:02,240
We grew our advisor network from maybe like five advisors

1826
01:18:02,240 --> 01:18:05,720
in the climate tech to now over 30 advisors that

1827
01:18:05,720 --> 01:18:07,520
are in the climate tech space.

1828
01:18:07,520 --> 01:18:10,240
And beyond that, prior to the grant,

1829
01:18:10,240 --> 01:18:13,840
we had maybe three or five startups, every batch that

1830
01:18:13,840 --> 01:18:15,200
were in climate tech.

1831
01:18:15,200 --> 01:18:18,560
And now we average 15 startups per batch

1832
01:18:18,560 --> 01:18:20,320
in the climate tech space.

1833
01:18:20,320 --> 01:18:22,800
And we really hope to see that grow.

1834
01:18:22,800 --> 01:18:26,800
So I'm very pleased to announce that the winner of the Skydeck

1835
01:18:26,800 --> 01:18:30,080
Climate Tech track is Team Greenwise.

1836
01:18:30,080 --> 01:18:32,120
I think they're still in the green room

1837
01:18:32,120 --> 01:18:33,240
because they just pitched.

1838
01:18:33,240 --> 01:18:36,240
They were the last ones to go on stage.

1839
01:18:36,240 --> 01:18:40,640
But they really kind of represent the team members

1840
01:18:40,640 --> 01:18:43,680
that we'd like to see at early stage startups.

1841
01:18:43,680 --> 01:18:46,480
It's three team members that are best friends

1842
01:18:46,480 --> 01:18:47,840
from middle school.

1843
01:18:47,840 --> 01:18:48,520
Yeah, they are.

1844
01:18:48,520 --> 01:18:50,000
Oh, they're out here on stage.

1845
01:18:50,000 --> 01:18:51,160
Come on out.

1846
01:18:51,160 --> 01:18:52,160
I wasn't expecting that.

1847
01:18:52,160 --> 01:18:56,960
But Anthony, Ben, and Ethan, three best friends

1848
01:18:56,960 --> 01:19:00,920
from middle school representing UC Davis, UC Santa Cruz,

1849
01:19:00,920 --> 01:19:04,200
and UC Santa Barbara.

1850
01:19:04,200 --> 01:19:08,600
And they've built a platform for carbon footprint tracking

1851
01:19:08,600 --> 01:19:13,520
with actionable recommendations for vendors

1852
01:19:13,520 --> 01:19:17,200
so that people and companies can reduce their overall carbon

1853
01:19:17,200 --> 01:19:18,040
footprint.

1854
01:19:18,040 --> 01:19:20,840
So please help me in congratulating this team.

1855
01:19:20,880 --> 01:19:22,880
Winners of $25,000.

1856
01:19:22,880 --> 01:19:26,880
All right.

1857
01:19:26,880 --> 01:19:27,680
Thank you, Simo.

1858
01:19:27,680 --> 01:19:28,720
Thank you, Greenwise.

1859
01:19:28,720 --> 01:19:29,760
Clicker, clicker.

1860
01:19:29,760 --> 01:19:31,040
Thank you.

1861
01:19:31,040 --> 01:19:34,840
All right, next up, special prizes from Intel.

1862
01:19:34,840 --> 01:19:36,800
Intel, come on out.

1863
01:19:36,800 --> 01:19:37,920
Intel was here.

1864
01:19:37,920 --> 01:19:39,720
Their table was swamped.

1865
01:19:39,720 --> 01:19:43,800
I'd like to introduce Gabrielle Amaronto.

1866
01:19:43,800 --> 01:19:45,040
Hi, everyone.

1867
01:19:45,040 --> 01:19:46,520
Thank you all so much.

1868
01:19:46,520 --> 01:19:48,480
And thank you to the organizers for having us.

1869
01:19:48,480 --> 01:19:50,920
We've had such a great weekend.

1870
01:19:50,920 --> 01:19:53,680
And your projects are so amazing.

1871
01:19:53,680 --> 01:19:55,720
So thank you to everyone who joined our track.

1872
01:19:55,720 --> 01:19:58,280
As you can see, the winners behind me,

1873
01:19:58,280 --> 01:20:00,080
congrats to all the winners.

1874
01:20:00,080 --> 01:20:02,640
We have our Raffle winner, Ayla Arres.

1875
01:20:02,640 --> 01:20:04,680
Third place is Asel.

1876
01:20:04,680 --> 01:20:07,200
Second place is Batteries by LLM.

1877
01:20:07,200 --> 01:20:09,680
And first place is Dish Batch AI.

1878
01:20:09,680 --> 01:20:12,160
So let's give them a round of applause.

1879
01:20:12,880 --> 01:20:17,720
Yes, great job.

1880
01:20:17,720 --> 01:20:18,720
Amazing projects.

1881
01:20:18,720 --> 01:20:21,240
If you won, please meet us outside.

1882
01:20:21,240 --> 01:20:23,160
I want to hand you your prizes.

1883
01:20:23,160 --> 01:20:24,480
We have them with us.

1884
01:20:24,480 --> 01:20:26,520
So please meet us outside so we can take pictures

1885
01:20:26,520 --> 01:20:27,680
and give you your prizes.

1886
01:20:27,680 --> 01:20:29,560
Thank you.

1887
01:20:29,560 --> 01:20:31,720
Thank you, Intel.

1888
01:20:31,720 --> 01:20:33,680
All right, next up, AWS.

1889
01:20:33,680 --> 01:20:35,560
Come on stage, AWS.

1890
01:20:35,560 --> 01:20:41,280
We've got Rohit Terluri, Kevin Liu, and Brandon Middleton.

1891
01:20:41,320 --> 01:20:42,600
And that's what they're going to tell you.

1892
01:20:42,600 --> 01:20:44,760
Go ahead, Rohit.

1893
01:20:44,760 --> 01:20:45,240
Howdy there.

1894
01:20:45,240 --> 01:20:46,480
Can you all hear me?

1895
01:20:46,480 --> 01:20:47,200
Yes?

1896
01:20:47,200 --> 01:20:47,880
Awesome.

1897
01:20:47,880 --> 01:20:50,320
Well, hey, thank you so much, Skydeck Team,

1898
01:20:50,320 --> 01:20:52,000
for having us and Calhacks.

1899
01:20:52,000 --> 01:20:55,480
This has been an extremely impressively run operation.

1900
01:20:55,480 --> 01:20:58,320
And we're really excited to be partners and sponsors

1901
01:20:58,320 --> 01:20:59,920
of this hackathon.

1902
01:20:59,920 --> 01:21:01,840
Today, we have three different prizes.

1903
01:21:01,840 --> 01:21:03,800
Actually, let me introduce myself first.

1904
01:21:03,800 --> 01:21:06,240
We have Brandon, Kevin, and Rohit.

1905
01:21:06,240 --> 01:21:09,680
We are members of the Generative AI Organization at AWS.

1906
01:21:09,680 --> 01:21:12,120
We work with top Generative AI startups

1907
01:21:12,120 --> 01:21:15,720
like Anthropics, Stability, Perplexity, and others

1908
01:21:15,720 --> 01:21:18,080
in the development of their large language models,

1909
01:21:18,080 --> 01:21:23,240
as well as our overall kind of inorganic and organic growth

1910
01:21:23,240 --> 01:21:25,880
strategy, including investments as well.

1911
01:21:25,880 --> 01:21:28,040
Today, we have three different prizes.

1912
01:21:28,040 --> 01:21:31,280
We have four of the teams that we have chosen

1913
01:21:31,280 --> 01:21:33,000
to give the prizes out to.

1914
01:21:33,000 --> 01:21:37,360
Our first place prize is for $10,000 in AWS credits.

1915
01:21:37,400 --> 01:21:40,120
And we have two other prizes, one for climate tech

1916
01:21:40,120 --> 01:21:42,840
and then one for responsible AI, which are $5,000.

1917
01:21:42,840 --> 01:21:47,760
I did want to say that we talked to so many impressive startups

1918
01:21:47,760 --> 01:21:52,000
and founding teams today and hackathon teams today.

1919
01:21:52,000 --> 01:21:54,280
I wish we could give prizes to all of them.

1920
01:21:54,280 --> 01:21:57,880
We did want to recommend that those who we spoke with,

1921
01:21:57,880 --> 01:22:00,400
and I think we have these conversations with you already,

1922
01:22:00,400 --> 01:22:03,040
to go ahead and apply for the AWS Generative AI

1923
01:22:03,040 --> 01:22:06,080
accelerator, as well as our AWS Activate

1924
01:22:06,080 --> 01:22:08,440
program to receive additional credits.

1925
01:22:08,440 --> 01:22:09,200
I'll go first.

1926
01:22:09,200 --> 01:22:12,120
I'm going to be announcing the climate tech.

1927
01:22:12,120 --> 01:22:15,360
We're going to give the prize out to disaster aid,

1928
01:22:15,360 --> 01:22:18,600
is disaster aid in the room today.

1929
01:22:18,600 --> 01:22:20,680
Yes, good job, guys.

1930
01:22:23,240 --> 01:22:25,960
And then for responsible AI, we have a two-way tie,

1931
01:22:25,960 --> 01:22:29,080
so we're splitting that prize into 2.5K for each team

1932
01:22:29,080 --> 01:22:32,560
in credits, and that's GPT ethics and DP ancestry.

1933
01:22:32,560 --> 01:22:34,240
They're in the hall.

1934
01:22:37,080 --> 01:22:38,080
All right.

1935
01:22:38,080 --> 01:22:39,840
And I'll round us out.

1936
01:22:39,840 --> 01:22:42,960
Our grand prize, kind of the most impressive thing

1937
01:22:42,960 --> 01:22:46,080
that we heard and saw today, is going to go to Safeguard.

1938
01:22:46,080 --> 01:22:48,880
So Safeguard team, if you're in the building,

1939
01:22:48,880 --> 01:22:49,800
stand up real fast.

1940
01:22:49,800 --> 01:22:53,120
Let's give you a round of applause.

1941
01:22:53,120 --> 01:22:55,440
I don't see them, but God bless you

1942
01:22:55,440 --> 01:22:57,080
and keep doing what you're doing.

1943
01:22:57,080 --> 01:22:57,920
Thanks so much, guys.

1944
01:22:57,920 --> 01:22:59,560
Thank you.

1945
01:22:59,560 --> 01:23:01,440
Thank you, Intel.

1946
01:23:01,440 --> 01:23:04,440
Our next amazing partner, Reach Capital.

1947
01:23:04,440 --> 01:23:06,200
Please come out, Tony Wahn.

1948
01:23:09,760 --> 01:23:11,200
Out of order.

1949
01:23:11,200 --> 01:23:12,440
Let me see if I can find you.

1950
01:23:12,440 --> 01:23:13,720
There you are.

1951
01:23:13,720 --> 01:23:15,440
OK, Mike.

1952
01:23:15,440 --> 01:23:16,880
Thank you so much.

1953
01:23:16,880 --> 01:23:18,120
Thank you to Calhacks.

1954
01:23:18,120 --> 01:23:19,080
Thank you to Skydack.

1955
01:23:19,080 --> 01:23:21,880
It is such a delight and pleasure to be with all of you today.

1956
01:23:21,880 --> 01:23:23,640
And thank you to everyone for being here,

1957
01:23:23,640 --> 01:23:26,120
from across the country, from across the world.

1958
01:23:26,120 --> 01:23:28,120
My name is Tony, and I'm from Reach Capital.

1959
01:23:28,120 --> 01:23:29,800
And let's just cut to the chase, because there's

1960
01:23:29,800 --> 01:23:30,640
no drama here.

1961
01:23:30,640 --> 01:23:34,720
We want to congratulate Frodo for winning our AI and Education

1962
01:23:34,720 --> 01:23:35,160
Prize.

1963
01:23:35,160 --> 01:23:37,440
Frodo, Aman, Kush, and the team, if you are here,

1964
01:23:37,440 --> 01:23:38,600
please stand up.

1965
01:23:38,600 --> 01:23:39,680
Please stand up.

1966
01:23:39,680 --> 01:23:41,160
All right, you are right up front.

1967
01:23:41,160 --> 01:23:42,320
You are in the right place.

1968
01:23:42,320 --> 01:23:44,080
Thank you so much.

1969
01:23:44,080 --> 01:23:47,000
You've won the one ring, as they say,

1970
01:23:47,000 --> 01:23:49,280
or at least our $1,000 cash prize.

1971
01:23:49,280 --> 01:23:52,400
So please, let's meet up afterwards.

1972
01:23:52,400 --> 01:23:55,320
Reach Capital, we're an early stage edtech VC firm

1973
01:23:55,320 --> 01:23:56,360
investing in edtech.

1974
01:23:56,360 --> 01:23:59,880
We invest in education across K12, higher ed, and workforce,

1975
01:23:59,880 --> 01:24:03,640
in tools that expand access to education and accountability.

1976
01:24:03,640 --> 01:24:05,840
And many of the companies that are portfolio

1977
01:24:05,840 --> 01:24:07,720
were founded by students themselves,

1978
01:24:07,720 --> 01:24:11,440
because what better place to find great ideas and great

1979
01:24:11,440 --> 01:24:14,520
talent than to go to places like this, where students are

1980
01:24:14,520 --> 01:24:15,600
living that experience?

1981
01:24:15,600 --> 01:24:18,920
So if you're building an edtech, please reach out.

1982
01:24:18,920 --> 01:24:20,280
Thank you so much.

1983
01:24:20,280 --> 01:24:22,920
Thank you, Tony.

1984
01:24:22,920 --> 01:24:26,440
Next up, we have you.com.

1985
01:24:26,440 --> 01:24:27,320
We have Rex.

1986
01:24:27,320 --> 01:24:29,480
Come on out, Rex, and tell us about the prize.

1987
01:24:32,840 --> 01:24:34,600
Applauds, please, for our sponsors.

1988
01:24:34,600 --> 01:24:35,100
Hi, buddy.

1989
01:24:35,100 --> 01:24:37,080
Yeah, thank you so much.

1990
01:24:37,080 --> 01:24:39,560
Yeah, so we wanted to announce, I'm Rex.

1991
01:24:39,560 --> 01:24:40,480
We're from you.com.

1992
01:24:40,480 --> 01:24:42,600
This is Oliver.

1993
01:24:42,600 --> 01:24:47,320
As you know, you.com brings transparency and web access

1994
01:24:47,320 --> 01:24:49,960
to our LLMs to make sure that they are as accurate as possible.

1995
01:24:49,960 --> 01:24:53,280
So we wanted to give an award for the best use of you.com's

1996
01:24:53,280 --> 01:24:56,080
APIs to transferify.

1997
01:24:56,080 --> 01:24:57,160
So congratulations.

1998
01:24:57,160 --> 01:25:00,200
If you guys want to stand up, if you're here, there you guys are.

1999
01:25:00,200 --> 01:25:01,960
Yeah, thank you so much.

2000
01:25:01,960 --> 01:25:04,480
Transferified did an incredible job.

2001
01:25:04,480 --> 01:25:07,040
They were live streaming videos, fact checking them

2002
01:25:07,040 --> 01:25:10,200
as they went, using sources from the web,

2003
01:25:10,200 --> 01:25:12,160
and you.com search APIs.

2004
01:25:12,160 --> 01:25:14,040
It was really incredible and powerful.

2005
01:25:14,040 --> 01:25:17,040
And Oliver will talk about our custom assistant.

2006
01:25:17,040 --> 01:25:19,120
Yeah, so for our best custom assistant,

2007
01:25:19,120 --> 01:25:23,400
we'd like to give that to events-.ai with Oliver and Devesh.

2008
01:25:23,400 --> 01:25:26,000
So Oliver and Devesh, can you please stand up

2009
01:25:26,080 --> 01:25:28,080
if you're in the room?

2010
01:25:28,080 --> 01:25:28,580
Congrats.

2011
01:25:28,580 --> 01:25:29,080
Over there.

2012
01:25:29,080 --> 01:25:29,580
Thank you.

2013
01:25:29,580 --> 01:25:30,080
Yeah.

2014
01:25:30,080 --> 01:25:37,360
So we were particularly impressed by what they've built.

2015
01:25:37,360 --> 01:25:39,680
Essentially, they handled booking, searching,

2016
01:25:39,680 --> 01:25:43,160
and even talking with customer agents on the phone.

2017
01:25:43,160 --> 01:25:47,720
And they used you.com in a way to actually find these events.

2018
01:25:47,720 --> 01:25:49,440
So we were incredibly impressed by them

2019
01:25:49,440 --> 01:25:52,440
and can't wait to see what they'll do in the future.

2020
01:25:52,440 --> 01:25:54,080
Yeah, come find us after.

2021
01:25:54,080 --> 01:25:56,840
And we will give you your awards.

2022
01:25:56,840 --> 01:25:58,360
Thank you, Hume.

2023
01:25:58,360 --> 01:25:58,960
Are you?

2024
01:25:58,960 --> 01:26:00,000
Thank you.

2025
01:26:00,000 --> 01:26:02,560
All right, I think we're going back to Hume now

2026
01:26:02,560 --> 01:26:05,080
with Zach Greathouse.

2027
01:26:05,080 --> 01:26:06,120
Welcome, Hume.

2028
01:26:06,120 --> 01:26:07,360
Nice round of applause, please.

2029
01:26:10,640 --> 01:26:11,720
Hi.

2030
01:26:11,720 --> 01:26:14,880
So first, just a huge thanks to Skydeck and Calhacks

2031
01:26:14,880 --> 01:26:17,880
for organizing this event and inviting us back

2032
01:26:17,880 --> 01:26:21,280
and to all the staff for running such a memorable event.

2033
01:26:21,280 --> 01:26:23,960
So I'm going to be announcing our three categories

2034
01:26:23,960 --> 01:26:25,240
for our track.

2035
01:26:25,240 --> 01:26:30,360
We have our best AI, our best empathic AI for social good,

2036
01:26:30,360 --> 01:26:34,520
best empathic AI for just most innovative empathic AI,

2037
01:26:34,520 --> 01:26:36,680
and then just the best overall.

2038
01:26:36,680 --> 01:26:37,960
As you can see, the team's here.

2039
01:26:37,960 --> 01:26:39,960
We've chosen Scam Scanner.

2040
01:26:39,960 --> 01:26:41,560
Can Scam Scanner, are you here?

2041
01:26:41,560 --> 01:26:42,800
Can you stand up?

2042
01:26:42,800 --> 01:26:43,960
All right, big applause.

2043
01:26:47,480 --> 01:26:51,720
For most innovative, we have Bloom Buddy.

2044
01:26:51,720 --> 01:26:53,160
Where's Bloom Buddy?

2045
01:26:53,160 --> 01:26:53,760
Can you stand up?

2046
01:26:53,760 --> 01:26:56,320
Yeah, OK, great job, you guys.

2047
01:26:56,320 --> 01:26:58,360
Talking to plans.

2048
01:26:58,360 --> 01:27:02,040
And then best use of empathic AI overall, we chose Lock-In.

2049
01:27:02,040 --> 01:27:05,440
It's a personalized learning assistant.

2050
01:27:05,440 --> 01:27:06,280
Are you in the room?

2051
01:27:06,280 --> 01:27:06,840
Where are you?

2052
01:27:06,840 --> 01:27:07,480
Yeah, there we are.

2053
01:27:07,480 --> 01:27:09,560
OK, congratulations, you guys.

2054
01:27:09,560 --> 01:27:11,280
Come meet us after outside.

2055
01:27:11,280 --> 01:27:13,240
We'd love to chat, take pictures.

2056
01:27:13,240 --> 01:27:14,680
And thank you so much.

2057
01:27:14,680 --> 01:27:17,080
Thank you to all the participants.

2058
01:27:17,080 --> 01:27:18,800
Yeah, we'll maybe see you next year.

2059
01:27:18,800 --> 01:27:19,960
So take care.

2060
01:27:19,960 --> 01:27:21,480
All right, thanks, Hume.

2061
01:27:21,520 --> 01:27:24,640
All right, and our last special prize is Grock.

2062
01:27:24,640 --> 01:27:25,320
There they are.

2063
01:27:25,320 --> 01:27:27,840
Please welcome Jose Menendez.

2064
01:27:31,120 --> 01:27:33,440
Hey, everyone.

2065
01:27:33,440 --> 01:27:34,920
Very nice to be here.

2066
01:27:34,920 --> 01:27:38,480
For those of you who haven't heard about Grock,

2067
01:27:38,480 --> 01:27:43,320
grock.com experienced the fastest inference on Earth

2068
01:27:43,320 --> 01:27:43,880
period.

2069
01:27:43,880 --> 01:27:46,160
All I have to say about Grock right now.

2070
01:27:46,160 --> 01:27:50,120
But our special Grock Star Award today

2071
01:27:50,120 --> 01:27:51,640
goes to Scam Scanner.

2072
01:27:51,640 --> 01:27:54,800
Where are you guys?

2073
01:27:54,800 --> 01:27:59,280
So these guys have a product that I

2074
01:27:59,280 --> 01:28:02,600
want my mom to use today, right?

2075
01:28:02,600 --> 01:28:06,240
Monitor your call for potential scams.

2076
01:28:06,240 --> 01:28:09,560
Who doesn't want that for your mom, your uncles,

2077
01:28:09,560 --> 01:28:11,720
and the whole thing?

2078
01:28:11,720 --> 01:28:18,600
Now, they get 1,000 credits on Grock Cloud,

2079
01:28:18,640 --> 01:28:21,680
which is many, many millions of tokens.

2080
01:28:24,880 --> 01:28:28,680
There's two special mentions I have to read,

2081
01:28:28,680 --> 01:28:30,480
so I don't screw up.

2082
01:28:30,480 --> 01:28:31,560
Three brown, one blue.

2083
01:28:31,560 --> 01:28:34,800
Where are you guys?

2084
01:28:34,800 --> 01:28:36,840
Another awesome solution.

2085
01:28:36,840 --> 01:28:41,400
These guys are generating on the fly

2086
01:28:41,400 --> 01:28:44,960
incredible videos for math.

2087
01:28:44,960 --> 01:28:47,640
Something that I would use right now as well.

2088
01:28:47,640 --> 01:28:51,480
And Transverify, are you guys around here?

2089
01:28:51,480 --> 01:28:53,720
You've been mentioned as well.

2090
01:28:53,720 --> 01:28:55,440
Transverify, very cool.

2091
01:28:55,440 --> 01:28:59,520
Who doesn't want to hear a podcast with instant fact

2092
01:28:59,520 --> 01:29:00,520
checking, right?

2093
01:29:00,520 --> 01:29:03,160
Am I right?

2094
01:29:03,160 --> 01:29:06,720
Now, my special surprise for the day,

2095
01:29:06,720 --> 01:29:11,040
I want to make a very special mention of Nathan Bog.

2096
01:29:11,040 --> 01:29:13,240
Are you around?

2097
01:29:13,240 --> 01:29:15,640
Nathan, all right.

2098
01:29:15,640 --> 01:29:17,800
Nathan didn't use Grock.

2099
01:29:17,800 --> 01:29:22,320
So I'm going to give a special technical excellence

2100
01:29:22,320 --> 01:29:28,880
award to Nathan for a model he trained and ran on his CPU

2101
01:29:28,880 --> 01:29:34,680
for doing very interesting DOM operations, corrections

2102
01:29:34,680 --> 01:29:36,640
on the fly for front end.

2103
01:29:36,640 --> 01:29:40,160
Not only that, Nathan is invited, officially,

2104
01:29:40,160 --> 01:29:47,280
to present his work in the Grock HQ as soon as he can.

2105
01:29:47,280 --> 01:29:48,520
That's it, guys.

2106
01:29:48,520 --> 01:29:52,000
I'm very impressed with all the work we saw.

2107
01:29:52,000 --> 01:29:52,880
Thank you very much.

2108
01:29:52,880 --> 01:29:53,640
Congratulations.

2109
01:29:53,640 --> 01:29:55,080
Thank you, Grock.

2110
01:29:55,080 --> 01:29:56,920
All right.

2111
01:29:56,920 --> 01:30:01,520
Our esteemed judges are back with their determination.

2112
01:30:01,520 --> 01:30:03,720
Please come back, judges.

2113
01:30:03,720 --> 01:30:08,840
Come back so we can all enjoy the grand prize.

2114
01:30:08,840 --> 01:30:10,240
Are you guys ready?

2115
01:30:10,240 --> 01:30:12,160
Do you have a guess?

2116
01:30:12,160 --> 01:30:12,960
Is there a voting?

2117
01:30:12,960 --> 01:30:14,560
Do we have voting tally?

2118
01:30:14,560 --> 01:30:16,080
Taking bets?

2119
01:30:16,080 --> 01:30:20,160
Everybody, I want you to guess your top two choices

2120
01:30:20,160 --> 01:30:22,080
for grand prize.

2121
01:30:22,080 --> 01:30:25,440
And then I'm going to ask, who got it right?

2122
01:30:25,440 --> 01:30:26,520
OK.

2123
01:30:26,520 --> 01:30:31,080
So as our wonderful judges, take their seat.

2124
01:30:31,080 --> 01:30:33,800
All right, we got some shout outs going here.

2125
01:30:33,800 --> 01:30:36,520
Any other shout outs?

2126
01:30:36,520 --> 01:30:38,080
OK.

2127
01:30:38,120 --> 01:30:40,280
All right, this audience is into it.

2128
01:30:40,280 --> 01:30:42,400
So as a reminder, the grand prize

2129
01:30:42,400 --> 01:30:47,200
is a $25,000 investment from the Berkeley Skydeck fund.

2130
01:30:47,200 --> 01:30:51,560
Also, a golden ticket to our Pad 13 program at Skydeck.

2131
01:30:51,560 --> 01:30:55,240
And a special prize, we are happy to announce,

2132
01:30:55,240 --> 01:31:00,720
that OpenAI is providing $2,500 in credits for this winter.

2133
01:31:00,720 --> 01:31:03,360
So I think we're ready for the drum roll.

2134
01:31:03,360 --> 01:31:05,200
Take your guesses.

2135
01:31:05,200 --> 01:31:06,760
Only the judges know.

2136
01:31:06,760 --> 01:31:07,640
I don't know.

2137
01:31:07,640 --> 01:31:09,240
We're all about to find out.

2138
01:31:09,240 --> 01:31:10,960
It's Dispatch AI.

2139
01:31:10,960 --> 01:31:12,160
Dispatch, where are you?

2140
01:31:12,160 --> 01:31:14,480
Come on up.

2141
01:31:14,480 --> 01:31:15,560
There's stairs right there.

2142
01:31:15,560 --> 01:31:18,760
Come on, come to the front stage.

2143
01:31:18,760 --> 01:31:19,600
There you go.

2144
01:31:19,600 --> 01:31:20,840
Thank you, judges.

2145
01:31:20,840 --> 01:31:23,640
I want to invite, while we invite Dispatch up,

2146
01:31:23,640 --> 01:31:25,480
I want to thank all of you for coming.

2147
01:31:25,480 --> 01:31:31,400
I want to invite Spike from Dispatch.

2148
01:31:31,400 --> 01:31:33,680
OK, here's the team.

2149
01:31:33,680 --> 01:31:34,600
There we go.

2150
01:31:34,600 --> 01:31:38,560
Dispatch AI, grand prize winners.

2151
01:31:38,560 --> 01:31:40,280
Well done.

2152
01:31:40,280 --> 01:31:42,200
Well done.

2153
01:31:42,200 --> 01:31:44,640
I'd like to invite the Skydeck staff to come out,

2154
01:31:44,640 --> 01:31:47,120
and the Berkeley Hackathon staff to come out.

2155
01:31:47,120 --> 01:31:48,560
Come on out.

2156
01:31:48,560 --> 01:31:50,160
They've been working all weekend.

2157
01:31:50,160 --> 01:31:53,520
I think some of them did not sleep at all.

2158
01:31:53,520 --> 01:31:57,320
Please give everyone who joined to make this a huge round

2159
01:31:57,320 --> 01:31:59,240
of applause.

2160
01:31:59,240 --> 01:32:00,840
Thank you, everybody.

2161
01:32:00,840 --> 01:32:02,800
Thanks for joining us.

2162
01:32:02,800 --> 01:32:06,040
We will see you next year.

2163
01:32:06,040 --> 01:32:08,040
Woo!

