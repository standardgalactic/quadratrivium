Okay, so welcome everybody for tonight's talk. Today we have Ilaria and she was going to talk
about explainability with knowledge graphs. What have we learned? This is the talk series of the
cost action on distributed knowledge graphs. We've had a few talks already. You can watch them in our
YouTube channel and we also have a few talks coming up on January the 31st. We will have a
talk by Mayankerival on March the 4th. We will have a talk by Marc Neusen and on April the 17th
we have a talk by Peter Patelschneider. What is this cost action? It is a European research network
where more than 30 countries are represented. We run workshops, hackathons and short term
scientific missions for which also you can apply. This overall runs from 2020 to 2024.
So we have a few months still on the project and we are now in our final year. This is chaired by
myself. This cost action with Axel Polares as the vice chair. We have Michel Dumontier and
Renzi as working group leads next to Andreas Antoine Olaf. Then there is John as a scientific
representative of the grant holder Anastasia Dimou as the science communication manager
and Stefan Gostowicz as the grant coordinator. So much on the cost action. Today we have Ilaria
and I would like to hand over to Olaf to announce the speaker. Yes, hello and welcome from my side
as well. I'm very happy to have Ilaria as a speaker. She got her PhD from the Open University in the
UK and the title of the PhD was explaining data patterns using knowledge from the web of data
and for the PhD she received this distinguished dissertation award of the Semantic Web Science
Association of Swizzers. She got this award in 2017 and as you maybe know we run this talk series
with speakers who received either this dissertation award or they received the 10 years award of
Swizzers. So now Ilaria received this award in 2017. One year later she went to Amsterdam
to the Free University in Amsterdam where she became an assistant professor there that she still is.
She has been involved in several conferences. She is the editor-in-chief for the CEWS
workshop proceedings where many of us probably publish their proceedings of their workshops.
She's also in the steering committee at the moment of the Hybrid Human AI Conference
and her research, the focus of her research is on systems that combine semantic technologies,
open data, machine learning in order to generate complex narratives with applications mostly in
scientific scenarios and in robotics and if you look at kind of her most cited papers many of them
have the word explain or explaining explanation in their title. So I'm very happy to have her here
and kind of reflect on the work that she did in the PhD and what has happened since then with this
work. The floor is yours Ilaria. Thank you so much. Thank you for all of Tobias for the introduction.
Yeah I'm going to share my screen so my question was whether you are keeping the rights.
Can you just give me a sign that you can see that I think okay. Yeah great. So
good evening or good morning everybody. My name is Ilaria and I work at the Free University in
Amsterdam. I'm an assistant professor. I'm very very happy of giving this talk today. There was
really a nice opportunity for me to reflect on the overall research that I've been doing in
the past since I was a PhD student and of course it's about explanations which is a very hot topic
in these days and really my work and my background is on knowledge graphs so I've been working most
of my life on how to use knowledge graphs in the context of generating explanations
and that's what we are going to see a bit. So I just to all I've gave her a very good introduction
of myself as I said I'm an assistant professor and have read intelligence. My background is on
knowledge representation, knowledge graphs, knowledge graphs for explainable AI. These
days I'm mostly focusing on my use cases around hybrid intelligence. You will hear a bit more
later on knowledge representation driven robotics and also scientific assistance and scientific
discovery. There's more about me you can check on the on my website. This slide is actually wrong
so it's IWC 2017 that I missed the one when I was about to receive supposed to come on stage and
receive the SWSA dissertation award. I was actually driving to a very far plate so I had to miss the
conference but I was very happy of receiving the news nevertheless. So to give you an overview
of what this talk is going to be about I will be describing a bit what's the work during my PhD was
so as as you heard from all of the title of my thesis was on explaining data patterns using
knowledge from the web of data and then I also want to discuss a bit what has going on since
then so since 2017 2016-17 what how explanations have developed how our knowledge graphs have
developed and the overall area and also some and concluding with some future consideration
of where do we want to go and and and how can we go from here. Starting with about 10 years ago
a bit more it's always hard to realize it over 10 years now. I like to show these pictures
or map a bit the audience I am not sure I can actually see the chat but maybe somebody wants to
I like to ask the question on whether somebody knows this picture which it's quite famous used to
be quite famous I don't know whether somebody is familiar with that
and this might be how I don't really see the chat but that's okay. So this is a typical
knowledge discovery process as was firstly introduced around in the middle of the 90s
when knowledge discovery was one of the main scientific processes that
scientists were using computer science method 4 and the process was actually was
firstly introduced by Fayyad so you can look up the reference and it used to be described as
all the steps that scientists need to produce in order to go from data into knowledge
and these steps were mostly selection of the data processing of the data transformation
data mining that was what came before the current machine learning and deep learning method
and then interpretation of an evaluation of the patterns so every step in every step you do
you transform your data little by little first you identify the relevant information then you
preprocess your data you transform them into something that you give to a data mining algorithm
and then you come up with patterns that then need an interpretation and an evaluation in order to
become knowledge. Now what we focused on is mostly this interpretation process so the last step
the interpretation of the patterns was somehow the core of the scientific process was what would
allow to transform patterns into into knowledge so to give a meaning to these patterns this is
actually how interpretation is being defined somehow in the dictionary so it's really the
action of capturing the meaning and communicating conveying the meaning of something usually the
way you interpret patterns so the way you try to give a meaning to that to capture this meaning
is by using your own background knowledge so you come up with patterns you might have
your own background knowledge or maybe human experts in a topic that explain the patterns
that the data mining algorithm has come up with and that helps evaluating and interpreting this
knowledge the problem is this background knowledge might be missing so you might require maybe the
expert doesn't know the actual explanation or you might need in certain use cases you might need
different experts from different domains so gathering this background knowledge might be
quite a time consuming process in this case when this information is missing the kind of
hypothesis that we put forward was that symbolic AI so symbols or knowledge graphs or linked open
data as we used to call them at that time are another source of background knowledge so the
kind of idea that we had is okay if we have plenty of multiple of knowledge graph of data sources
that are multi-domain that are connected between each other so at that time we had the link data
cloud I think this is this picture is slightly later than 2013 but the idea is to have all these
connected data sets that point to each other and then you can continuously discover knowledge simply
by crawling data set one after the other this information is most of the time it's connected
it's centralized in hubs and observatories it's standardized according to certain vocabularies
that allow modeling data sharing data so the the vocabularies would allow to to to have inter
operability across application then the kind of idea was okay maybe we can use this all this
information which is available online to help us explaining the patterns that an algorithm gives us
whenever we don't have an expert in the picture or whenever we are missing the background knowledge
to explain that and and it is very similar to the idea that Newell and Simon already in the 70s had
so among the the faders in AI that symbols were one additional layer to to use when somebody
wants to capture and and and convey the meaning so you don't only need the data or the experience
but you need to put a structure and you need symbols on top of it in order to really get into
into an intelligent system that can understand and capture meaning so based on these hypotheses
we set up a number of research questions so the very first one we we looked into was okay if we
want to use uh if if we want to use knowledge graph large-scale knowledge graphs to to to
generate explanations um we kind of need to understand what do we mean by an explanation
so we need a definition even if it's a working definition we still need one and then we kind
of said okay we need a method that will allow us to to to generate explanations from knowledge
graphs so how how are we going to do that um then then assuming we come up with one such a method
we need to try to cope with all the problems that come with knowledge graph including
incompleteness, bias, noise um so so the the later research questions obviously some of the
research questions came out little by little throughout the process right so you look at your
phd from a different perspective but somehow the the later research questions were on improving
the methods that we had in order to cope with the limitations of knowledge graphs including
incompleteness and bias and basically uh looking now i'm going to go through the the research
questions but i don't want to dive too much into it i mean we have papers for that i'm just the next
slides are mostly to give you an idea of what the overall approach was uh starting with the the
definition of explanation we defined we first defined an ontology design pattern for explanation
so what we did was we looked into different disciplines we looked into philosophy we looked
into linguistics into social science and we looked into their definition of explanations and we
noticed that even though there were different approaches and methods explanations were always
seen according to the similar characteristics so there was always the generation of some
coherence between old knowledge and new knowledge uh the elements in the explanation were always the
same there was a theory there were an anterior and a posterior event there were sequences that
would make this event happening at the same time and there were always processes um
uh that would be one internal process where you come up with an explanation for yourself and then
one that is more an external process where you communicated the explanation to the rest of the
world and based on this we then defined our own uh ontology design ontology for an explanation so
our own definition that we could then feed into a system that would try to generate explanation
according to these patterns so we we then just it's a pattern in the sense that it can be
instantiated in multiple um in multiple contexts but the overall idea was that you always have an
event uh that happens before an event that happens after I'm not sure you see my pointer but I hope
so uh there are certain uh conditions that make this uh this event happening in a specific setting
and then there's some sort of uh theory behind and a sort and an agent that that that outputs the
that creates the conceptualizes the uh the explanation uh based on um based on this pattern
we then try to to design a system that would try to use knowledge graph to generate explanation for
a given pattern of data and then the the kind of question we try to answer is really okay if we
want to use a knowledge graph a very large knowledge graph uh as background knowledge
which kind of process do I need to uh generate explanation and then we try to work with some
with some examples of patterns of data of things that people might want to to explain uh we had this
very good example that would work with uh google trends and then you try to explain why a certain
website uh is uh regularly happening at specific points in time so in these cases why people are
searching for the term a song in ice and fire only in certain periods so you see that there are very
regular peaks or maybe we try to explain data like statistical data for example from the UNESCO
uh in this case we have uh countries that are grouped according to the
female literacy rate and then you try to explain okay why is is it oops oops sorry uh why is it
happening that um that that certain countries have in common like a certain characteristic
and the very good thing is that uh we try to come up with a method that had us uh working
out these examples and it was basically it was based on three main steps one was an inductive
logic programming step where you try to compare positive and negative examples and you have some
background knowledge expressed in uh as as tripos as facts about these examples and then the goal
is to induce the hypothesis that mostly represents your positive examples which are the examples
you want to explain uh we used a knowledge graph search so we we try to uh uh search the the link
data graph uh in order to find a common path which would be expressed in terms of predicates
relationships uh leading to a specific entity in the graph so we call this path and then we
would consider any path that is uh common uh to all the positive examples as an explanation for the
group of positive example uh and then in order to to to improve the the the graph search we we
implemented a greedy uh links reversal strategy uh so we try to aim for uh the uh longer and longer
path you know um and explore the graph using simple http d referencing in order to avoid
computational costs it was 2013 we still had issues of computational costs and uh based on this
method we try to answer mostly two questions so first we said okay which kind of your uh
risks do we need to to drive a greedy search and identify the best explanation so we try
different strategies and then one of the main finding for us was that a measure so a strategy
based on entropy um would lead to a higher explanation in in in in less time so we simply
measured on what is the best accuracy of an explanation over time so this is um uh iteration
in searching the graph and we always ended up having the best uh an entropy based measure
as the one that would perform best and give us the best explanation uh these allowed us to to to
come up with uh explanation for the use cases i was showing before so uh why are women less
educated than men in certain countries and that's mostly when countries are least developed for
example uh so they have a quite a high human development index rank and uh or they are expressed
they are defined in dbpb as least developed countries um or we we had explanations like okay
people search for a song in ice and fire whenever there is an event that is somehow linked to a
game of thrones tv series so that's this is we consider these explanations but of course if you
look at the results you might notice things that are quite not right and uh in particular you look
at these kind of examples so uh it's true that if you have enough background knowledge about a song
in ice and fire which is actually a book uh and you know that there is a certain tv series related
that is based on this book you know that the explanation for people being particularly
interested in these uh is whenever the tv series is coming out so there is a new season coming out
but there's nothing that relates the tonga tonga uh to to to the book or basketball
competition to the book so the kind of questions we had to ask answer afterwards was really okay is
there something in linked data that can tell us that game of thrones is strongly related to a song
in ice and fire much more than basketball competitions or anything related to tonga so the
second part of the method was really focused on strengthening this knowledge based explanation
and we focused we used a genetic programming algorithm to try to learn um a function that
could detect the strong relationships between two graph entities that are quite distant in the graph
so we are not talking about two entities in in in the same graph but you have one entity that is
connected to another entity through hops in multiple graphs in multiple data sets and then we really
want to try to understand what is the the strongest relationship and what we did is that with a genetic
programming algorithm we try to learn a function that could study the topological and semantic
characteristics of the knowledge graph so we really looked into how many hubs and how many
how strongly connected was the graph for example which kind of vocabulary the graph was was using
all these we gave all these to our genetic programming algorithm and then we tried to
come up with a with a function that would tell us okay this is a strong relationship and we evaluated
that with against a human evaluated relationship path and um actually what what came out from
from this study was that the best thing for us would have been to try to follow uh notes that
would have quite rich descriptions so here you have different examples of different functions
that we tried and then here you have the say the best performing ones um and the the the best
functions that we could find were really the ones were focusing on uh following notes with
rich descriptions that they would have quite a good number of namespaces uh it was best to follow
more specific entities so really not not not hubs with many incoming links but rather something
that is more specific and also we look into scores the scores vocabulary and uh it was best to have
fewer topical categories so not something quite generic in some in terms of topic but but rather
again a bit more specificity uh of course we had to look into into into into into into the bias
in the data as well so the the results were not optimal and as I said we needed to deal both with
incompleteness of the data but also when inner bias of the data and what we tried to do was to um
use identity links so remember that we are still we were still we are still talking about link data
where data sets are connected to each other through identity links including same as and what we uh
tried to do was to um measure the bias in a given data set uh by uh comparing the projection on one
data set into another one so the overall idea was really that you have a data set which could be
I don't know the link movie database and you have a certain amount of entities that are connected
through the dbpdia uh using identity links and the projection of the the movie database is mostly
the set of entities that's in dbpdia that are connected to to to the movie database and then
we we would basically compare all the entities in the larger data set with the subset and using
some correlation uh tests or some t-test and by comparing the distribution of uh property value
pairs uh between the subset of the entities and the large set uh we try to identify with uh which
were the the the the the bias in a given data set uh so for example we uh and yeah I would
uh send uh I would recommend you to refer to the paper but what we what we discovered was
for example that the link movie database was particularly focused on black and white movies and
that certain digital humanities data sets were focused on uh uh poets and novelists from the
18th and 19th century so it was a way to try to measure the bias in the in the in the data
set that we were using to generate explanations and to once we learned these bias we could also try to
somehow input this information in our system and generate better explanations
now this was uh uh quite a long a long journey that uh ended up in 2016 and there's a number of
things that happened uh since then what I what I'm I call the the present um so I don't have
enough time to focus on everything but I choose three particular uh aspects that I'm going to to
show which help us also thinking a bit further uh into the steps that we want to uh to take afterwards
the very first thing of course is the rise of deep learning so when uh when uh we started the
the work on explanations and knowledge graphs actually on knowledge graphs for explanations
deep learning was just was probably just booming and I wasn't even aware of that so what we were
talking about was uh using knowledge graphs to um generate explanation for outputs of any
machine learning algorithm uh but we've never heard of deep learning I think that yeah maybe the the
the the first papers are around 2012 but if I'm not mistaken but um of course the the hype came
much later and um uh the the the DARPA uh so so deep learning and all the methods behind we
that came with deep learning showed uh impressive results that could achieve uh
the same results as human would and uh but but also showed a number of limitations so the
uh the the the facts like the Cambridge Analytica scandals or the the loan accreditation scandals
show that these systems were uh were not able to to show a clear reasoning so the reasoning was
quite opaque uh these systems were data hungry so you needed a lot of data to train them they were
too brittle in this sense and then DARPA came out with this explainable AI program that was 2017
if not if I'm not mistaken um on okay let's try to implement create systems where the user is
that are transparent that can explain the machine learning model and then where uh an explanation
can be given on why a certain output is being given uh so this need of explaining the models
and the results really came out and you start we started seeing methods like SHOP and LIME which
are probably the most basic ones and then there's there's a number of other systems that came out
afterwards uh that could tell you okay the the most important features to come up with an explanation
and more in your data set are uh like maybe the race or the occupation of of of of your data
or we started seeing saliency maps okay with with imagery recognition uh what are the most
important parts that that system focuses on when generating explanations uh this especially the
explanations in in in the explainable AI area and if you look into the major AI conference you
start seeing a boom of uh I think explainable AI became an actual topic or a subfield in in AI
uh one of the questions that people asked was okay uh are these explanations that SHOP and LIME
come up with are they really working especially in a real-world context so they do work in in a
small use case but what if I applied into into a real-world context uh and somehow the narrow
symbolic field uh which was already in in the meantime in developing on on his own came up a
bit in the rescue of this problem so somehow we started seeing methods that try to combine
a symbolic approach so symbolic reasoning uh with neural network either to uh maybe improve
the explainability and trust of a system using a symbolic description uh or by creating a sort
of hybrid interaction between the the neural network and and the reasoning system uh so these
are just two of the many examples that you could could see when uh where the knowledge graph would
try to jump in into the explainable AI word and say okay hey look we should be maybe using
knowledge graphs and ontologies to help uh we also did a part of this uh so what what we try to do um
and um uh this this I completely forgot the the uh this the reference to this work but
if we published in 2021 uh we really try to say okay if everybody if many people are looking into
using knowledge graphs as a as a tool to explain um machine learning methods let's try to look at
what's the state of the art so uh how are people in machine learning using knowledge graphs what are
the the most important characteristics so we try to look into different uh tasks and different areas
and uh we try to look into the characteristics of the knowledge graphs the characteristics of
the model and the characteristics of the explanations that we were being generating uh
in order to come up with really with a with a picture of the field uh and this this is more
or less what we came up with so uh we we learned certain things like if you are dealing with tasks
for recognition and recommendation uh most of the explanation you will get are really about the
model and how it behaves and most of the information that is being used uh from the knowledge graph is
the is the a is the aversion box uh and whenever uh we deal with tasks that involve the interaction
of the user like conversational agents or recommender systems the knowledge graph information is
used to is used more into the training of the model uh to generate a certain explanation
rather than as a postdoc stack uh we looked into the different types of knowledge graphs whether
they were factual or common sense or domain knowledge graphs domain specific and it turns
out that common sense knowledge graphs they're not that many but they were being used for image
recognition and question answering uh we also look into the reuse of knowledge graphs so we've been
uh talking so much in our field about reusing knowledge graph reusing ontology ontologies and
we kind of ask okay is this being applied in this field and it actually turned out it was
quite an established practice so very few were coming up with their own knowledge graph uh most
of the the methods were were using uh dbpdia uh wikidata um consonants or a combination of them
um actually we looked into uh whether these methods were using only one knowledge graph
or multiple ones and we we were hoping to to see a bit more but it does happen sometimes in
nlp tasks and really the kind of this is the kind of picture that came out so if you if certain
areas are more focused on on model embedded knowledge so explaining the model rather than
explaining the outputs certain others are more focused on using the ontologies of the
knowledge graph rather than the facts um and um and so on and so forth so I think the analysis we
did it's around 60 60 papers more or less um and we kind of try to identify also the challenges
for the field right so there are things that were we were hoping to see but that didn't happen
including the what we call the co-creation of explanations so really having a sort
the human having a role into the generation of the explanation there are there were there are
issues related to the maintenance of the knowledge graph so how to deal with uh
missing information or bias when coming up when generating explanations and of course this is a
problem that we also saw during in the in the phd and also there are issues related to the
automated extraction of relevant knowledge when using a knowledge graph that generates explanations
so most of the methods that we analyzed when coming up with an explanation end up manually
selecting uh the relevant relevant relevant information to generate an explanation and
this is quite something it means that there's still a lot to do in the in the field in order to
um uh to move forward um there's uh so this was one part of the story so what happened
ever since deep learning uh there's also the field of hybrid intelligence that came up so
I don't know if many of you heard about uh hybrid intelligence maybe you had about human
centric ai this is another way of addressing this this problem uh the overall idea was that
this there is an emerging field in ai which uh and it's emerging because you start seeing
different conferences and workshops uh around the topic there's a number of uh national and
international um collaboration networks uh that uh that are really focusing on uh on this concept
of hybrid intelligence and and the overall idea because we don't really have a proper definition
but we do have a working definition of hybrid intelligence is to have uh to to aim for ai systems
that try to enhance human capabilities as other scientific tools would do think of the telescope
that allows a scientist to um to look uh where his own eyes cannot see or the machine the the
the sorry the the card that or the the airplane would allow people to to to reach places that
they couldn't reach easily with their with their feet um so it's really about seeing ai system as
an extension of the human intelligence rather than seeing ai as a tool that replaces us so in
this sense hybrid intelligence really look into uh systems that collaborate uh with humans uh aiming
for a complementarity so weak weaknesses and strengths of both ai and humans are complemented
by each other uh and really about this synergetic idea so the the the mixed team the hybrid team
is is aiming for the same has a shared goal um this is um the so so we have a research agenda
I'm part of the Dutch hybrid intelligence consortium and I've been part of the hybrid
intelligence conference of the past uh in the past years um and it's a really vibrant uh field
um and and explainability explaining um it's also a part of the research agenda so
it's not only about trying to collaborate but how in in this in this collaboration
the goal is is also to try to communicate our own intention and explain our own actions and
our own reasoning uh so the one of the the core topics that we established when when
hybrid intelligence came into the picture was really on on on how to create systems that are
able to deliberate and and explain to to their collaborators um I'm more than happy to to discuss
this a bit further uh and somehow in as part of the hybrid intelligence picture we also uh
and as part of the the the contribution that we could give with ontologies and knowledge
across with respect to explainability we also try to um work on on on on a number of what we
call boxologies or terminologies for hybrid intelligence where so to establish mostly to
establish a shared language between uh agents in a different team um sorry agents in the same
team that would collaborate between each other um and uh what what we did in one of the the the
the newest work was really on comparing different hybrid intelligence scenarios
and first trying to identify what are the common uh knowledge roles so we came up with a high level
ontologies of agents interaction types uh and and and specific scenarios and and we try to also
identify um uh using these high level ontologies what were the the most specific hybrid intelligent
tasks uh including ones that would uh relate to creativity and and explainability uh and then
on the side of it we would have uh tasks like team awareness and multimodality uh that's the
second part of the picture so we talk about uh deep learning we talk about uh hybrid intelligence
and what also came into play with respect to to explanation and uh we didn't have that much
time to to dive into it but hybrid intelligence is strongly related to that as well is really on
especially with a with a european perspective is really on the gdpr and uh so um and an eu
ai act which recently came up uh but really the idea is to try to monitor the systems that we are
developing uh to make sure that that that uh users are are protected both in terms in terms of the
data that are being generated and the methods that are being generated uh and these also
means to be able to to to explain um uh the reasoning and to trace back the the the information
that is being output uh so so explainability and transparency became um started uh appearing
together in the in the picture so if you want transparency you need to be explainable therefore
showing your reasoning um and now with this eu ai act the the idea is really that that there are
certain obligations there are certain systems that need to show um uh some transparent they have
transparency obligations so they need to uh be able to explain why certain things are are happening
um otherwise they are considered uh unacceptable uh this is a very uh so i have a few minutes left
i think this is a very uh quick overview of what has happened ever since uh which leads us back to
okay what is going to happen now and how can we kind of think of everything that has happened so far
and and where are we going to to go uh of course i couldn't get away without mentioning language
models at least once so somehow one of the questions and one might ask and we also kind of wondered uh
was uh okay but everything we've done could this be now done so the the overall knowledge
process could just be replaced by language models and by large language models by LLMs
could we just not replace all the steps or the explainability steps do we actually need
knowledge grasp for that and it's true that um LLMs language and language models are very good
in dealing with noise and inconsistency and like methods that the methods that we had before were
not that much able they allow us to extract information very quickly from large structure
data so that goes towards the dream of doing a web-scale learning um or one could argue that
you already achieved that uh they're actually quite good in capturing some complex semantics so
somehow it has been demonstrated that defining a class can can be uh uh with specific boundaries
so the boundaries of the definition of a class is quite difficult are quite hard
so there is no universal class description and and it the especially with the embeddings method
based methods are able to capture this this this complexity quite a bit better and of course
are very good in generating natural language so instead of generating explanation in a
mechanic mechanical mechanistic way from from from the triples uh they are able to generate a much
more human friendly um explanation the problem is that there are still limited in a number of things
so learning from rare and unique events especially like the ones that that that you can find in the
web is still quite difficult um if this these methods are not yet able to show proper reasoning
and argumenting behind thoroughly creating a thorough argumentation uh behind what has happened
and also they don't really deal with with with fairness and interoperability acceptability all
these this fair aspects that we've been looking into as as knowledge graph community are not yet
part of the picture in a language model so somehow there are limitations in using that
but it doesn't mean that we need to discard them completely but we can just join the the
both words and and work out something to for to generate better better explanations
so i want to conclude in the last few minutes to really think okay if we now look at knowledge
graphs and whether they're useful uh to to generate explanation and do they actually work
what is it that we launch so uh somehow both based on the phc and everything that happens
afterwards uh yes we can use knowledge graphs but they are mostly an intermediate representation
so knowledge graphs are really for the machine consumption uh they shouldn't be for human consumption
and the rdf is just a language that that for machines to perform an exchange of information
which is unambiguous so uh we should really not look into knowledge graph as something that we human
should understand but but more as something a machine can can quickly exchange um we can use
knowledge graphs to as to get together content to generate explanation but really the graph
structure is just a backbone so we don't really want to use the knowledge graph to create an output
we can use language models for that uh but but we can use knowledge graph as to to to gather the
backbone of the explanation which can then be output according to different users in a different
dimension so we think okay an expert user might need a longer explanation uh which with more
arguments and an expert a non-expert and layman might need a much shorter and simpler explanation
in terms and llms are great in doing that you can ask them to rephrase a certain concept in
different uh in different according to different dimensions um also knowledge graphs allow allow
to check the to trace back information so you can really walk down the graph and and and check
whether the information is truth and this is quite uh this is much better than looking into
the propagation of an error the activation of a neural network and try to decode what does it mean
um in order to come up with an explanation that you might not be able to to to explain
that clearly yourself and finally scalability which was part of the picture at that time okay
how can we deal with very large knowledge graphs and uh uh we want to to to integrate as much
knowledge as possible well actually this is not that relevant anymore and what people are really
aiming for and we also saw this when collaborating with industry is more it's much better to have
a high quality knowledge graph which is curated by the expert uh to generate explanations rather
than having a very large knowledge graph um so this is one part of the story and then the other part
is more of the big picture uh what we learned is really explainability is not only about machine
learning uh so in hybrid intelligence we deal with explainability we deal with experts from
all kind of fields from social science to computer science and um people look into
explainability from from this in different ways so it's a bit of a jungle of terminology we cannot
really agree what uh what we mean by something being explainable and then we need to try to find
a way to harmonize that um we we learned that explanations are really task dependent so yes
we have an ontology design patterns for the explanation but we need to adapt this according
to the context and again we can use language models for that but it's um the the target audience or the
the language or the the type of explanation really need to change according to the situation
and more importantly we need to look for the human so knowledge graphs are only one part in the
explanation process so in order to generate explanation you need a knowledge graph you need
a sub symbolic method but you also need a human that interacts with the with the system
because in the end as we said at the beginning explanation is a social process it's a dual
process so it has to happen in a in a in a co-creation setting where the user is really
interacting with the system to come up with an explanation is satisfied with and in this sense
we also need interdisciplinarity in the picture to try to measure whether an explanation is is
interesting or not so we kind of we don't only need a computer science perspective but we also
need to try to integrate the talks that we have with social scientists and cognitive uh scientists
to make sure that the explanations that are being generated are actually useful uh so to give some
ideas on what and uh then i'm i'm just done i know i'm over a bit um uh what we suggest is that
really we should look into the kind of knowledge in artificial intelligence so some people have called
this knowledge science some people have called it empirical semantics we call it knowledge in AI
so we really need to go back to the empirical analysis of the knowledge graphs that we create
and we deal with we need to understand their modeling style and the kind of semantics they're
communicating and whether the semantics is enough or too much to generate explanations
we need to check for the usefulness and limitation of the knowledge graph that
and the knowledge that that we create and somehow we need to try to move from the step of okay how
can we fit knowledge into the learning process these we know how to do it uh or at least we have
good methods but we really need to focus on what is which kind of knowledge uh we need to fit
in order to be able to learn something and to generate explanation for example
so somehow this is a call for the community to to start so where do we start and uh i have
tried to revisit a bit the research questions that that i had in the beginning thinking okay
based on on this idea of knowledge in AI then maybe we need to try to fit the explanation pattern
into the existing uh systems uh we need to try to to to to come up with explanations that like
uh using deep learning and uh at a web scale uh we need to try to augment explanations and turn
them into complex narratives we mentioned these uh complex argumentations so and for these we
can really combine knowledge graphs and language models and we really need to compensate whatever
information is missing uh by uh performing a co-creation of explanation with with the humans
this is the end of my talk uh i thank you so much i don't know how many people are there i
thank you so much for taking the time to listen into me i thank uh tobya santua and olav for
inviting me there was an amazing opportunity i invite you to to reach me out for exchanges
and i really hope to see you in uh at the next hybrid intelligence conference in in sweden in in
june uh this is the end of my talk thank you all right uh thank you ilaria um i should buy one of
the sitcom applause machines and give you a round of applause that reflects the size of the audience
that we had um there were a few questions already in the youtube chat um so please keep them come in
and um in the meantime i will post some of them to you and uh if there are no more questions
i may ask the people have to deal with mine um good so let's start there is um there is a question
by mevish on the chat um and she's asking uh what is your vision on using your explanation
techniques for a large language model so you do you think the same techniques are useful for them
as well or do you need a different kind or is there a way of adapting maybe um the methods
yeah thanks so this is a very cool question of course i mean that that's the kind of question we
i i i'm expecting in these days because we do have language models are able to achieve so much
that it's it's hard to think okay if i done everything wrong can they just do better than me
and i mean i would be very curious to to just do a um a simple comparison i like this question a lot
so the the the question is really can we use these techniques um um with large language models
because i i now have a number of PhD students working on these and we were discussing this just
this morning um the you you certainly have the advantage that you might not need to crawl
the knowledge graph anymore to generate explanation so all the problems of
um heretics to search the graph um to to reduce the computational complexity
might not be needed anymore uh uh with that said i still think that using you you need to combine
language models and knowledge graphs um in order to be able to to trace back the information and
especially if we are talking about uh truthfulness of an explanation i can ask my knowledge graph to
the language model to to come up with an explanation for a given pattern of data but i
i also want to make sure that i can trace the provenance back and this is something that i
i want to know i mean probably a knowledge graph is much better to do than than a language model
uh so i still think that as i said the backbone of the information should come from a knowledge
graph in a way that you can uh you can reconstruct the subgraph somehow that generates your explanation
and then the output for the form it can be uh can be generated or or situated according to the
users by the language model that will be my answer okay um so thanks for that there's another
question by peter jones he's asking to what extent do you think ai and explainable ai may
reduce or undermine the use or development of domain specific languages uh sorry i
lost the second part of the question so do i think the ai and explainable ai reduced the use
of domain specific languages um well i don't think they actually i i'm not quite sure uh
whether by domain specific language uh yeah we mean the domain specific representation
or so the main ontologies but i don't think they actually uh reduce it or at least i don't think
they should reduce it somehow um i see more an integration of the two in the sense that uh
the the the same way uh there are these methods that use so you i've seen methods using domain
specific ontologies to um come up with maybe decision trees about the an explanation that is
being generated so they um uh the advantage of domain specific language is still that they are
highly they're curated by by the experts and so they are still more reliable so the two methods
should kind of uh i don't i don't want to think of uh of explainable ai methods as taking over
but rather to try to uh complement to to to combine the two or in a in a neuro symbolic fashion
i hope this answer the questions uh hi yeah i hope so too um speaking of maybe maybe this
this goes one into the direction of one of the questions that uh that i uh noted um so if we
have if you have a domain specific language or a domain specific may of modeling things
then this allows to very uh concisely write down things for a specific domain um previously one
of your explanation methods you use something that like was looking at graph uh the distance in the
graph right and if you change like if you have something very good for one domain then that
obviously changes the the the distance in the graph so maybe you can reflect a little bit on how
the graph structure or the role of how model how things are modeled or how much entailment is
applied on on the graph changes the the the distance or the results of your approach
yeah so i think um in general this is the kind of problem we try to to to approach
both so with uh say with a with a follow-up method so when we looked into
uh trying to identify strong relationships but also trying to cope with the with the
the inner bias of the information uh the assumption so we we've never dealt with
knowledge graphs we created right so we always dealt with knowledge graphs that were created by
others so the assumption was you might not find the information that that you might need and you
need to so sometimes the the information is very well curated and sometimes this is not
and somehow we need to find um a way to to to cope with this problem in order to be as general
as possible so the my view is really that there is not a a universal way of so there is not a
method that can can deal with both the most important thing is being able to um to cope
with the problem and integrate it in the methods that you develop so you need to be aware that
information might be missing and and you need to make sure that your method compensates for that
this can happen inside the development of your method or as a postdoc it's like a posteriori
step and and it can be as simple as i mean in the same view of the co-creation with the user it can
be as simple as okay let's interact and see whether i'm missing some part of the information
so so somehow one of the reasons why we had to look into the strength of the relationship was
also because we were missing these and we had to find a strategy to to survive in the in this
case okay um there's another question from ask kim star and uh they are asking what
adaptions do you see for knowledge for for this whole set of approaches to deal with multimodality
um sorry so the so you had so you had like there was this with the cat picture where there
was some computer vision aspects in it um and maybe you reflect a bit on on multimodality when
it comes so it's uh it's through that we've never uh it's the kind of knowledge graph we've been
dealing with and and multi multi-modal knowledge graphs were not really um uh um that common at
that time i think uh so so it's through that one aspect that would be interesting and maybe
we kind of we didn't discuss this as as in the future step but it is in these days we are now
talking about we hear much more about multimodal knowledge graphs we hear um also about knowledge
graph that somehow try to integrate the physical words so like we deal with robots in some scenarios
and we also have this problem of okay i have a knowledge graph that has to integrate both
abstract concepts but also the physical word um so um uh i think it would be quite interesting to
think on how to generate explanations that are multimodal in this sense uh this is one part of
the answer in the sense that yes we didn't look into multimodal knowledge graphs and this it could
change uh the other part of the answer is uh uh the explanation that we could generate
so the kind of patterns that were coming out could also be patterns coming from from multimodal
data so like the uh i dealt a lot with clusters of of of data points but these data points could as
well be um uh paths of an image for example that will represent that will represent i don't know
the ear or the the tail of the cut of this kind of thing so i didn't deal with that concretely but
it could uh i i don't see why the method shouldn't work on um image uh labels uh with yeah with specific
information that we might want to explain uh so that say with respect to multimodality there are
these two parts so the the the original method would probably work also on data points coming up
from multimodal data and then the multimodal knowledge graph we didn't look that much into it
and it could be quite interesting to to look into multimodal explanation in this sense so that's
okay thanks um so maybe as a as a last question jumping back to your history um and because this
is the cost action on distributed knowledge graph where we uh care about distributed decentralized
things so you you had this one approach that was based on dereferencing of your eyes and looking at
things from from that perspective now um you kind of gave me the impression that you say okay now the
person who developed the large language model did the web crawling for you but maybe you can say
something like all the methods that you have developed in the meantime after you were done
with dereferencing your eyes are they based on a global kind of view uh to i don't know generate
embeddings and things um or would they still work in a distributed and decentralized setting
i i guess as long as uh so the the embeddings are convenient because they can embed a lot of
information in in a very small space and that's something that we didn't have so we had to come
up with a different dereferencing opportunity but also we didn't want to deal with uh storing the
graph because if you store the graph and you query it then you then you you need to know the
data model and we didn't care about it so what we really cared about was this kind of serendipitous
hope um i remember one of the first papers i i saw was um something all of also did on on on
navigation query languages so that was quite quite quite related um so i think the the most
important the the part that would still work is even in a distributed context is as long as you
have connections or pointers to to from data to data then it's fine it doesn't really matter and uh
and we we didn't care whether it was the web of data or it could have been the load and load
stored in a in an hdt file um that was really not the the problem the main problem was uh
what where is the irrelevant information uh how do i identify the the links between the
between that and this is valid in any context do i whether i have the information centralized or
decentralized i would say then i have really experimented with that so i can't tell but
so good um yeah thanks for the answer um i don't have more questions from the youtube chat um
with that i just want you to thank you again for the very nice talk and the very nice tna session
before we close the stream i can only advertise the next talk on january the 31st at the same
time as today by mayank um you can check out our website and find uh follow us on this thing
formerly called twitter and of course you find all the recordings of our talk on our youtube channel
