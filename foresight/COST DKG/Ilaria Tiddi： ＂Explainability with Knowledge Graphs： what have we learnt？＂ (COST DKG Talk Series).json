{"text": " Okay, so welcome everybody for tonight's talk. Today we have Ilaria and she was going to talk about explainability with knowledge graphs. What have we learned? This is the talk series of the cost action on distributed knowledge graphs. We've had a few talks already. You can watch them in our YouTube channel and we also have a few talks coming up on January the 31st. We will have a talk by Mayankerival on March the 4th. We will have a talk by Marc Neusen and on April the 17th we have a talk by Peter Patelschneider. What is this cost action? It is a European research network where more than 30 countries are represented. We run workshops, hackathons and short term scientific missions for which also you can apply. This overall runs from 2020 to 2024. So we have a few months still on the project and we are now in our final year. This is chaired by myself. This cost action with Axel Polares as the vice chair. We have Michel Dumontier and Renzi as working group leads next to Andreas Antoine Olaf. Then there is John as a scientific representative of the grant holder Anastasia Dimou as the science communication manager and Stefan Gostowicz as the grant coordinator. So much on the cost action. Today we have Ilaria and I would like to hand over to Olaf to announce the speaker. Yes, hello and welcome from my side as well. I'm very happy to have Ilaria as a speaker. She got her PhD from the Open University in the UK and the title of the PhD was explaining data patterns using knowledge from the web of data and for the PhD she received this distinguished dissertation award of the Semantic Web Science Association of Swizzers. She got this award in 2017 and as you maybe know we run this talk series with speakers who received either this dissertation award or they received the 10 years award of Swizzers. So now Ilaria received this award in 2017. One year later she went to Amsterdam to the Free University in Amsterdam where she became an assistant professor there that she still is. She has been involved in several conferences. She is the editor-in-chief for the CEWS workshop proceedings where many of us probably publish their proceedings of their workshops. She's also in the steering committee at the moment of the Hybrid Human AI Conference and her research, the focus of her research is on systems that combine semantic technologies, open data, machine learning in order to generate complex narratives with applications mostly in scientific scenarios and in robotics and if you look at kind of her most cited papers many of them have the word explain or explaining explanation in their title. So I'm very happy to have her here and kind of reflect on the work that she did in the PhD and what has happened since then with this work. The floor is yours Ilaria. Thank you so much. Thank you for all of Tobias for the introduction. Yeah I'm going to share my screen so my question was whether you are keeping the rights. Can you just give me a sign that you can see that I think okay. Yeah great. So good evening or good morning everybody. My name is Ilaria and I work at the Free University in Amsterdam. I'm an assistant professor. I'm very very happy of giving this talk today. There was really a nice opportunity for me to reflect on the overall research that I've been doing in the past since I was a PhD student and of course it's about explanations which is a very hot topic in these days and really my work and my background is on knowledge graphs so I've been working most of my life on how to use knowledge graphs in the context of generating explanations and that's what we are going to see a bit. So I just to all I've gave her a very good introduction of myself as I said I'm an assistant professor and have read intelligence. My background is on knowledge representation, knowledge graphs, knowledge graphs for explainable AI. These days I'm mostly focusing on my use cases around hybrid intelligence. You will hear a bit more later on knowledge representation driven robotics and also scientific assistance and scientific discovery. There's more about me you can check on the on my website. This slide is actually wrong so it's IWC 2017 that I missed the one when I was about to receive supposed to come on stage and receive the SWSA dissertation award. I was actually driving to a very far plate so I had to miss the conference but I was very happy of receiving the news nevertheless. So to give you an overview of what this talk is going to be about I will be describing a bit what's the work during my PhD was so as as you heard from all of the title of my thesis was on explaining data patterns using knowledge from the web of data and then I also want to discuss a bit what has going on since then so since 2017 2016-17 what how explanations have developed how our knowledge graphs have developed and the overall area and also some and concluding with some future consideration of where do we want to go and and and how can we go from here. Starting with about 10 years ago a bit more it's always hard to realize it over 10 years now. I like to show these pictures or map a bit the audience I am not sure I can actually see the chat but maybe somebody wants to I like to ask the question on whether somebody knows this picture which it's quite famous used to be quite famous I don't know whether somebody is familiar with that and this might be how I don't really see the chat but that's okay. So this is a typical knowledge discovery process as was firstly introduced around in the middle of the 90s when knowledge discovery was one of the main scientific processes that scientists were using computer science method 4 and the process was actually was firstly introduced by Fayyad so you can look up the reference and it used to be described as all the steps that scientists need to produce in order to go from data into knowledge and these steps were mostly selection of the data processing of the data transformation data mining that was what came before the current machine learning and deep learning method and then interpretation of an evaluation of the patterns so every step in every step you do you transform your data little by little first you identify the relevant information then you preprocess your data you transform them into something that you give to a data mining algorithm and then you come up with patterns that then need an interpretation and an evaluation in order to become knowledge. Now what we focused on is mostly this interpretation process so the last step the interpretation of the patterns was somehow the core of the scientific process was what would allow to transform patterns into into knowledge so to give a meaning to these patterns this is actually how interpretation is being defined somehow in the dictionary so it's really the action of capturing the meaning and communicating conveying the meaning of something usually the way you interpret patterns so the way you try to give a meaning to that to capture this meaning is by using your own background knowledge so you come up with patterns you might have your own background knowledge or maybe human experts in a topic that explain the patterns that the data mining algorithm has come up with and that helps evaluating and interpreting this knowledge the problem is this background knowledge might be missing so you might require maybe the expert doesn't know the actual explanation or you might need in certain use cases you might need different experts from different domains so gathering this background knowledge might be quite a time consuming process in this case when this information is missing the kind of hypothesis that we put forward was that symbolic AI so symbols or knowledge graphs or linked open data as we used to call them at that time are another source of background knowledge so the kind of idea that we had is okay if we have plenty of multiple of knowledge graph of data sources that are multi-domain that are connected between each other so at that time we had the link data cloud I think this is this picture is slightly later than 2013 but the idea is to have all these connected data sets that point to each other and then you can continuously discover knowledge simply by crawling data set one after the other this information is most of the time it's connected it's centralized in hubs and observatories it's standardized according to certain vocabularies that allow modeling data sharing data so the the vocabularies would allow to to to have inter operability across application then the kind of idea was okay maybe we can use this all this information which is available online to help us explaining the patterns that an algorithm gives us whenever we don't have an expert in the picture or whenever we are missing the background knowledge to explain that and and it is very similar to the idea that Newell and Simon already in the 70s had so among the the faders in AI that symbols were one additional layer to to use when somebody wants to capture and and and convey the meaning so you don't only need the data or the experience but you need to put a structure and you need symbols on top of it in order to really get into into an intelligent system that can understand and capture meaning so based on these hypotheses we set up a number of research questions so the very first one we we looked into was okay if we want to use uh if if we want to use knowledge graph large-scale knowledge graphs to to to generate explanations um we kind of need to understand what do we mean by an explanation so we need a definition even if it's a working definition we still need one and then we kind of said okay we need a method that will allow us to to to generate explanations from knowledge graphs so how how are we going to do that um then then assuming we come up with one such a method we need to try to cope with all the problems that come with knowledge graph including incompleteness, bias, noise um so so the the later research questions obviously some of the research questions came out little by little throughout the process right so you look at your phd from a different perspective but somehow the the later research questions were on improving the methods that we had in order to cope with the limitations of knowledge graphs including incompleteness and bias and basically uh looking now i'm going to go through the the research questions but i don't want to dive too much into it i mean we have papers for that i'm just the next slides are mostly to give you an idea of what the overall approach was uh starting with the the definition of explanation we defined we first defined an ontology design pattern for explanation so what we did was we looked into different disciplines we looked into philosophy we looked into linguistics into social science and we looked into their definition of explanations and we noticed that even though there were different approaches and methods explanations were always seen according to the similar characteristics so there was always the generation of some coherence between old knowledge and new knowledge uh the elements in the explanation were always the same there was a theory there were an anterior and a posterior event there were sequences that would make this event happening at the same time and there were always processes um uh that would be one internal process where you come up with an explanation for yourself and then one that is more an external process where you communicated the explanation to the rest of the world and based on this we then defined our own uh ontology design ontology for an explanation so our own definition that we could then feed into a system that would try to generate explanation according to these patterns so we we then just it's a pattern in the sense that it can be instantiated in multiple um in multiple contexts but the overall idea was that you always have an event uh that happens before an event that happens after I'm not sure you see my pointer but I hope so uh there are certain uh conditions that make this uh this event happening in a specific setting and then there's some sort of uh theory behind and a sort and an agent that that that outputs the that creates the conceptualizes the uh the explanation uh based on um based on this pattern we then try to to design a system that would try to use knowledge graph to generate explanation for a given pattern of data and then the the kind of question we try to answer is really okay if we want to use a knowledge graph a very large knowledge graph uh as background knowledge which kind of process do I need to uh generate explanation and then we try to work with some with some examples of patterns of data of things that people might want to to explain uh we had this very good example that would work with uh google trends and then you try to explain why a certain website uh is uh regularly happening at specific points in time so in these cases why people are searching for the term a song in ice and fire only in certain periods so you see that there are very regular peaks or maybe we try to explain data like statistical data for example from the UNESCO uh in this case we have uh countries that are grouped according to the female literacy rate and then you try to explain okay why is is it oops oops sorry uh why is it happening that um that that certain countries have in common like a certain characteristic and the very good thing is that uh we try to come up with a method that had us uh working out these examples and it was basically it was based on three main steps one was an inductive logic programming step where you try to compare positive and negative examples and you have some background knowledge expressed in uh as as tripos as facts about these examples and then the goal is to induce the hypothesis that mostly represents your positive examples which are the examples you want to explain uh we used a knowledge graph search so we we try to uh uh search the the link data graph uh in order to find a common path which would be expressed in terms of predicates relationships uh leading to a specific entity in the graph so we call this path and then we would consider any path that is uh common uh to all the positive examples as an explanation for the group of positive example uh and then in order to to to improve the the the graph search we we implemented a greedy uh links reversal strategy uh so we try to aim for uh the uh longer and longer path you know um and explore the graph using simple http d referencing in order to avoid computational costs it was 2013 we still had issues of computational costs and uh based on this method we try to answer mostly two questions so first we said okay which kind of your uh risks do we need to to drive a greedy search and identify the best explanation so we try different strategies and then one of the main finding for us was that a measure so a strategy based on entropy um would lead to a higher explanation in in in in less time so we simply measured on what is the best accuracy of an explanation over time so this is um uh iteration in searching the graph and we always ended up having the best uh an entropy based measure as the one that would perform best and give us the best explanation uh these allowed us to to to come up with uh explanation for the use cases i was showing before so uh why are women less educated than men in certain countries and that's mostly when countries are least developed for example uh so they have a quite a high human development index rank and uh or they are expressed they are defined in dbpb as least developed countries um or we we had explanations like okay people search for a song in ice and fire whenever there is an event that is somehow linked to a game of thrones tv series so that's this is we consider these explanations but of course if you look at the results you might notice things that are quite not right and uh in particular you look at these kind of examples so uh it's true that if you have enough background knowledge about a song in ice and fire which is actually a book uh and you know that there is a certain tv series related that is based on this book you know that the explanation for people being particularly interested in these uh is whenever the tv series is coming out so there is a new season coming out but there's nothing that relates the tonga tonga uh to to to the book or basketball competition to the book so the kind of questions we had to ask answer afterwards was really okay is there something in linked data that can tell us that game of thrones is strongly related to a song in ice and fire much more than basketball competitions or anything related to tonga so the second part of the method was really focused on strengthening this knowledge based explanation and we focused we used a genetic programming algorithm to try to learn um a function that could detect the strong relationships between two graph entities that are quite distant in the graph so we are not talking about two entities in in in the same graph but you have one entity that is connected to another entity through hops in multiple graphs in multiple data sets and then we really want to try to understand what is the the strongest relationship and what we did is that with a genetic programming algorithm we try to learn a function that could study the topological and semantic characteristics of the knowledge graph so we really looked into how many hubs and how many how strongly connected was the graph for example which kind of vocabulary the graph was was using all these we gave all these to our genetic programming algorithm and then we tried to come up with a with a function that would tell us okay this is a strong relationship and we evaluated that with against a human evaluated relationship path and um actually what what came out from from this study was that the best thing for us would have been to try to follow uh notes that would have quite rich descriptions so here you have different examples of different functions that we tried and then here you have the say the best performing ones um and the the the best functions that we could find were really the ones were focusing on uh following notes with rich descriptions that they would have quite a good number of namespaces uh it was best to follow more specific entities so really not not not hubs with many incoming links but rather something that is more specific and also we look into scores the scores vocabulary and uh it was best to have fewer topical categories so not something quite generic in some in terms of topic but but rather again a bit more specificity uh of course we had to look into into into into into into the bias in the data as well so the the results were not optimal and as I said we needed to deal both with incompleteness of the data but also when inner bias of the data and what we tried to do was to um use identity links so remember that we are still we were still we are still talking about link data where data sets are connected to each other through identity links including same as and what we uh tried to do was to um measure the bias in a given data set uh by uh comparing the projection on one data set into another one so the overall idea was really that you have a data set which could be I don't know the link movie database and you have a certain amount of entities that are connected through the dbpdia uh using identity links and the projection of the the movie database is mostly the set of entities that's in dbpdia that are connected to to to the movie database and then we we would basically compare all the entities in the larger data set with the subset and using some correlation uh tests or some t-test and by comparing the distribution of uh property value pairs uh between the subset of the entities and the large set uh we try to identify with uh which were the the the the the bias in a given data set uh so for example we uh and yeah I would uh send uh I would recommend you to refer to the paper but what we what we discovered was for example that the link movie database was particularly focused on black and white movies and that certain digital humanities data sets were focused on uh uh poets and novelists from the 18th and 19th century so it was a way to try to measure the bias in the in the in the data set that we were using to generate explanations and to once we learned these bias we could also try to somehow input this information in our system and generate better explanations now this was uh uh quite a long a long journey that uh ended up in 2016 and there's a number of things that happened uh since then what I what I'm I call the the present um so I don't have enough time to focus on everything but I choose three particular uh aspects that I'm going to to show which help us also thinking a bit further uh into the steps that we want to uh to take afterwards the very first thing of course is the rise of deep learning so when uh when uh we started the the work on explanations and knowledge graphs actually on knowledge graphs for explanations deep learning was just was probably just booming and I wasn't even aware of that so what we were talking about was uh using knowledge graphs to um generate explanation for outputs of any machine learning algorithm uh but we've never heard of deep learning I think that yeah maybe the the the the first papers are around 2012 but if I'm not mistaken but um of course the the hype came much later and um uh the the the DARPA uh so so deep learning and all the methods behind we that came with deep learning showed uh impressive results that could achieve uh the same results as human would and uh but but also showed a number of limitations so the uh the the the facts like the Cambridge Analytica scandals or the the loan accreditation scandals show that these systems were uh were not able to to show a clear reasoning so the reasoning was quite opaque uh these systems were data hungry so you needed a lot of data to train them they were too brittle in this sense and then DARPA came out with this explainable AI program that was 2017 if not if I'm not mistaken um on okay let's try to implement create systems where the user is that are transparent that can explain the machine learning model and then where uh an explanation can be given on why a certain output is being given uh so this need of explaining the models and the results really came out and you start we started seeing methods like SHOP and LIME which are probably the most basic ones and then there's there's a number of other systems that came out afterwards uh that could tell you okay the the most important features to come up with an explanation and more in your data set are uh like maybe the race or the occupation of of of of your data or we started seeing saliency maps okay with with imagery recognition uh what are the most important parts that that system focuses on when generating explanations uh this especially the explanations in in in the explainable AI area and if you look into the major AI conference you start seeing a boom of uh I think explainable AI became an actual topic or a subfield in in AI uh one of the questions that people asked was okay uh are these explanations that SHOP and LIME come up with are they really working especially in a real-world context so they do work in in a small use case but what if I applied into into a real-world context uh and somehow the narrow symbolic field uh which was already in in the meantime in developing on on his own came up a bit in the rescue of this problem so somehow we started seeing methods that try to combine a symbolic approach so symbolic reasoning uh with neural network either to uh maybe improve the explainability and trust of a system using a symbolic description uh or by creating a sort of hybrid interaction between the the neural network and and the reasoning system uh so these are just two of the many examples that you could could see when uh where the knowledge graph would try to jump in into the explainable AI word and say okay hey look we should be maybe using knowledge graphs and ontologies to help uh we also did a part of this uh so what what we try to do um and um uh this this I completely forgot the the uh this the reference to this work but if we published in 2021 uh we really try to say okay if everybody if many people are looking into using knowledge graphs as a as a tool to explain um machine learning methods let's try to look at what's the state of the art so uh how are people in machine learning using knowledge graphs what are the the most important characteristics so we try to look into different uh tasks and different areas and uh we try to look into the characteristics of the knowledge graphs the characteristics of the model and the characteristics of the explanations that we were being generating uh in order to come up with really with a with a picture of the field uh and this this is more or less what we came up with so uh we we learned certain things like if you are dealing with tasks for recognition and recommendation uh most of the explanation you will get are really about the model and how it behaves and most of the information that is being used uh from the knowledge graph is the is the a is the aversion box uh and whenever uh we deal with tasks that involve the interaction of the user like conversational agents or recommender systems the knowledge graph information is used to is used more into the training of the model uh to generate a certain explanation rather than as a postdoc stack uh we looked into the different types of knowledge graphs whether they were factual or common sense or domain knowledge graphs domain specific and it turns out that common sense knowledge graphs they're not that many but they were being used for image recognition and question answering uh we also look into the reuse of knowledge graphs so we've been uh talking so much in our field about reusing knowledge graph reusing ontology ontologies and we kind of ask okay is this being applied in this field and it actually turned out it was quite an established practice so very few were coming up with their own knowledge graph uh most of the the methods were were using uh dbpdia uh wikidata um consonants or a combination of them um actually we looked into uh whether these methods were using only one knowledge graph or multiple ones and we we were hoping to to see a bit more but it does happen sometimes in nlp tasks and really the kind of this is the kind of picture that came out so if you if certain areas are more focused on on model embedded knowledge so explaining the model rather than explaining the outputs certain others are more focused on using the ontologies of the knowledge graph rather than the facts um and um and so on and so forth so I think the analysis we did it's around 60 60 papers more or less um and we kind of try to identify also the challenges for the field right so there are things that were we were hoping to see but that didn't happen including the what we call the co-creation of explanations so really having a sort the human having a role into the generation of the explanation there are there were there are issues related to the maintenance of the knowledge graph so how to deal with uh missing information or bias when coming up when generating explanations and of course this is a problem that we also saw during in the in the phd and also there are issues related to the automated extraction of relevant knowledge when using a knowledge graph that generates explanations so most of the methods that we analyzed when coming up with an explanation end up manually selecting uh the relevant relevant relevant information to generate an explanation and this is quite something it means that there's still a lot to do in the in the field in order to um uh to move forward um there's uh so this was one part of the story so what happened ever since deep learning uh there's also the field of hybrid intelligence that came up so I don't know if many of you heard about uh hybrid intelligence maybe you had about human centric ai this is another way of addressing this this problem uh the overall idea was that this there is an emerging field in ai which uh and it's emerging because you start seeing different conferences and workshops uh around the topic there's a number of uh national and international um collaboration networks uh that uh that are really focusing on uh on this concept of hybrid intelligence and and the overall idea because we don't really have a proper definition but we do have a working definition of hybrid intelligence is to have uh to to aim for ai systems that try to enhance human capabilities as other scientific tools would do think of the telescope that allows a scientist to um to look uh where his own eyes cannot see or the machine the the the sorry the the card that or the the airplane would allow people to to to reach places that they couldn't reach easily with their with their feet um so it's really about seeing ai system as an extension of the human intelligence rather than seeing ai as a tool that replaces us so in this sense hybrid intelligence really look into uh systems that collaborate uh with humans uh aiming for a complementarity so weak weaknesses and strengths of both ai and humans are complemented by each other uh and really about this synergetic idea so the the the mixed team the hybrid team is is aiming for the same has a shared goal um this is um the so so we have a research agenda I'm part of the Dutch hybrid intelligence consortium and I've been part of the hybrid intelligence conference of the past uh in the past years um and it's a really vibrant uh field um and and explainability explaining um it's also a part of the research agenda so it's not only about trying to collaborate but how in in this in this collaboration the goal is is also to try to communicate our own intention and explain our own actions and our own reasoning uh so the one of the the core topics that we established when when hybrid intelligence came into the picture was really on on on how to create systems that are able to deliberate and and explain to to their collaborators um I'm more than happy to to discuss this a bit further uh and somehow in as part of the hybrid intelligence picture we also uh and as part of the the the contribution that we could give with ontologies and knowledge across with respect to explainability we also try to um work on on on on a number of what we call boxologies or terminologies for hybrid intelligence where so to establish mostly to establish a shared language between uh agents in a different team um sorry agents in the same team that would collaborate between each other um and uh what what we did in one of the the the the newest work was really on comparing different hybrid intelligence scenarios and first trying to identify what are the common uh knowledge roles so we came up with a high level ontologies of agents interaction types uh and and and specific scenarios and and we try to also identify um uh using these high level ontologies what were the the most specific hybrid intelligent tasks uh including ones that would uh relate to creativity and and explainability uh and then on the side of it we would have uh tasks like team awareness and multimodality uh that's the second part of the picture so we talk about uh deep learning we talk about uh hybrid intelligence and what also came into play with respect to to explanation and uh we didn't have that much time to to dive into it but hybrid intelligence is strongly related to that as well is really on especially with a with a european perspective is really on the gdpr and uh so um and an eu ai act which recently came up uh but really the idea is to try to monitor the systems that we are developing uh to make sure that that that uh users are are protected both in terms in terms of the data that are being generated and the methods that are being generated uh and these also means to be able to to to explain um uh the reasoning and to trace back the the the information that is being output uh so so explainability and transparency became um started uh appearing together in the in the picture so if you want transparency you need to be explainable therefore showing your reasoning um and now with this eu ai act the the idea is really that that there are certain obligations there are certain systems that need to show um uh some transparent they have transparency obligations so they need to uh be able to explain why certain things are are happening um otherwise they are considered uh unacceptable uh this is a very uh so i have a few minutes left i think this is a very uh quick overview of what has happened ever since uh which leads us back to okay what is going to happen now and how can we kind of think of everything that has happened so far and and where are we going to to go uh of course i couldn't get away without mentioning language models at least once so somehow one of the questions and one might ask and we also kind of wondered uh was uh okay but everything we've done could this be now done so the the overall knowledge process could just be replaced by language models and by large language models by LLMs could we just not replace all the steps or the explainability steps do we actually need knowledge grasp for that and it's true that um LLMs language and language models are very good in dealing with noise and inconsistency and like methods that the methods that we had before were not that much able they allow us to extract information very quickly from large structure data so that goes towards the dream of doing a web-scale learning um or one could argue that you already achieved that uh they're actually quite good in capturing some complex semantics so somehow it has been demonstrated that defining a class can can be uh uh with specific boundaries so the boundaries of the definition of a class is quite difficult are quite hard so there is no universal class description and and it the especially with the embeddings method based methods are able to capture this this this complexity quite a bit better and of course are very good in generating natural language so instead of generating explanation in a mechanic mechanical mechanistic way from from from the triples uh they are able to generate a much more human friendly um explanation the problem is that there are still limited in a number of things so learning from rare and unique events especially like the ones that that that you can find in the web is still quite difficult um if this these methods are not yet able to show proper reasoning and argumenting behind thoroughly creating a thorough argumentation uh behind what has happened and also they don't really deal with with with fairness and interoperability acceptability all these this fair aspects that we've been looking into as as knowledge graph community are not yet part of the picture in a language model so somehow there are limitations in using that but it doesn't mean that we need to discard them completely but we can just join the the both words and and work out something to for to generate better better explanations so i want to conclude in the last few minutes to really think okay if we now look at knowledge graphs and whether they're useful uh to to generate explanation and do they actually work what is it that we launch so uh somehow both based on the phc and everything that happens afterwards uh yes we can use knowledge graphs but they are mostly an intermediate representation so knowledge graphs are really for the machine consumption uh they shouldn't be for human consumption and the rdf is just a language that that for machines to perform an exchange of information which is unambiguous so uh we should really not look into knowledge graph as something that we human should understand but but more as something a machine can can quickly exchange um we can use knowledge graphs to as to get together content to generate explanation but really the graph structure is just a backbone so we don't really want to use the knowledge graph to create an output we can use language models for that uh but but we can use knowledge graph as to to to gather the backbone of the explanation which can then be output according to different users in a different dimension so we think okay an expert user might need a longer explanation uh which with more arguments and an expert a non-expert and layman might need a much shorter and simpler explanation in terms and llms are great in doing that you can ask them to rephrase a certain concept in different uh in different according to different dimensions um also knowledge graphs allow allow to check the to trace back information so you can really walk down the graph and and and check whether the information is truth and this is quite uh this is much better than looking into the propagation of an error the activation of a neural network and try to decode what does it mean um in order to come up with an explanation that you might not be able to to to explain that clearly yourself and finally scalability which was part of the picture at that time okay how can we deal with very large knowledge graphs and uh uh we want to to to integrate as much knowledge as possible well actually this is not that relevant anymore and what people are really aiming for and we also saw this when collaborating with industry is more it's much better to have a high quality knowledge graph which is curated by the expert uh to generate explanations rather than having a very large knowledge graph um so this is one part of the story and then the other part is more of the big picture uh what we learned is really explainability is not only about machine learning uh so in hybrid intelligence we deal with explainability we deal with experts from all kind of fields from social science to computer science and um people look into explainability from from this in different ways so it's a bit of a jungle of terminology we cannot really agree what uh what we mean by something being explainable and then we need to try to find a way to harmonize that um we we learned that explanations are really task dependent so yes we have an ontology design patterns for the explanation but we need to adapt this according to the context and again we can use language models for that but it's um the the target audience or the the language or the the type of explanation really need to change according to the situation and more importantly we need to look for the human so knowledge graphs are only one part in the explanation process so in order to generate explanation you need a knowledge graph you need a sub symbolic method but you also need a human that interacts with the with the system because in the end as we said at the beginning explanation is a social process it's a dual process so it has to happen in a in a in a co-creation setting where the user is really interacting with the system to come up with an explanation is satisfied with and in this sense we also need interdisciplinarity in the picture to try to measure whether an explanation is is interesting or not so we kind of we don't only need a computer science perspective but we also need to try to integrate the talks that we have with social scientists and cognitive uh scientists to make sure that the explanations that are being generated are actually useful uh so to give some ideas on what and uh then i'm i'm just done i know i'm over a bit um uh what we suggest is that really we should look into the kind of knowledge in artificial intelligence so some people have called this knowledge science some people have called it empirical semantics we call it knowledge in AI so we really need to go back to the empirical analysis of the knowledge graphs that we create and we deal with we need to understand their modeling style and the kind of semantics they're communicating and whether the semantics is enough or too much to generate explanations we need to check for the usefulness and limitation of the knowledge graph that and the knowledge that that we create and somehow we need to try to move from the step of okay how can we fit knowledge into the learning process these we know how to do it uh or at least we have good methods but we really need to focus on what is which kind of knowledge uh we need to fit in order to be able to learn something and to generate explanation for example so somehow this is a call for the community to to start so where do we start and uh i have tried to revisit a bit the research questions that that i had in the beginning thinking okay based on on this idea of knowledge in AI then maybe we need to try to fit the explanation pattern into the existing uh systems uh we need to try to to to to come up with explanations that like uh using deep learning and uh at a web scale uh we need to try to augment explanations and turn them into complex narratives we mentioned these uh complex argumentations so and for these we can really combine knowledge graphs and language models and we really need to compensate whatever information is missing uh by uh performing a co-creation of explanation with with the humans this is the end of my talk uh i thank you so much i don't know how many people are there i thank you so much for taking the time to listen into me i thank uh tobya santua and olav for inviting me there was an amazing opportunity i invite you to to reach me out for exchanges and i really hope to see you in uh at the next hybrid intelligence conference in in sweden in in june uh this is the end of my talk thank you all right uh thank you ilaria um i should buy one of the sitcom applause machines and give you a round of applause that reflects the size of the audience that we had um there were a few questions already in the youtube chat um so please keep them come in and um in the meantime i will post some of them to you and uh if there are no more questions i may ask the people have to deal with mine um good so let's start there is um there is a question by mevish on the chat um and she's asking uh what is your vision on using your explanation techniques for a large language model so you do you think the same techniques are useful for them as well or do you need a different kind or is there a way of adapting maybe um the methods yeah thanks so this is a very cool question of course i mean that that's the kind of question we i i i'm expecting in these days because we do have language models are able to achieve so much that it's it's hard to think okay if i done everything wrong can they just do better than me and i mean i would be very curious to to just do a um a simple comparison i like this question a lot so the the the question is really can we use these techniques um um with large language models because i i now have a number of PhD students working on these and we were discussing this just this morning um the you you certainly have the advantage that you might not need to crawl the knowledge graph anymore to generate explanation so all the problems of um heretics to search the graph um to to reduce the computational complexity might not be needed anymore uh uh with that said i still think that using you you need to combine language models and knowledge graphs um in order to be able to to trace back the information and especially if we are talking about uh truthfulness of an explanation i can ask my knowledge graph to the language model to to come up with an explanation for a given pattern of data but i i also want to make sure that i can trace the provenance back and this is something that i i want to know i mean probably a knowledge graph is much better to do than than a language model uh so i still think that as i said the backbone of the information should come from a knowledge graph in a way that you can uh you can reconstruct the subgraph somehow that generates your explanation and then the output for the form it can be uh can be generated or or situated according to the users by the language model that will be my answer okay um so thanks for that there's another question by peter jones he's asking to what extent do you think ai and explainable ai may reduce or undermine the use or development of domain specific languages uh sorry i lost the second part of the question so do i think the ai and explainable ai reduced the use of domain specific languages um well i don't think they actually i i'm not quite sure uh whether by domain specific language uh yeah we mean the domain specific representation or so the main ontologies but i don't think they actually uh reduce it or at least i don't think they should reduce it somehow um i see more an integration of the two in the sense that uh the the the same way uh there are these methods that use so you i've seen methods using domain specific ontologies to um come up with maybe decision trees about the an explanation that is being generated so they um uh the advantage of domain specific language is still that they are highly they're curated by by the experts and so they are still more reliable so the two methods should kind of uh i don't i don't want to think of uh of explainable ai methods as taking over but rather to try to uh complement to to to combine the two or in a in a neuro symbolic fashion i hope this answer the questions uh hi yeah i hope so too um speaking of maybe maybe this this goes one into the direction of one of the questions that uh that i uh noted um so if we have if you have a domain specific language or a domain specific may of modeling things then this allows to very uh concisely write down things for a specific domain um previously one of your explanation methods you use something that like was looking at graph uh the distance in the graph right and if you change like if you have something very good for one domain then that obviously changes the the the distance in the graph so maybe you can reflect a little bit on how the graph structure or the role of how model how things are modeled or how much entailment is applied on on the graph changes the the the distance or the results of your approach yeah so i think um in general this is the kind of problem we try to to to approach both so with uh say with a with a follow-up method so when we looked into uh trying to identify strong relationships but also trying to cope with the with the the inner bias of the information uh the assumption so we we've never dealt with knowledge graphs we created right so we always dealt with knowledge graphs that were created by others so the assumption was you might not find the information that that you might need and you need to so sometimes the the information is very well curated and sometimes this is not and somehow we need to find um a way to to to cope with this problem in order to be as general as possible so the my view is really that there is not a a universal way of so there is not a method that can can deal with both the most important thing is being able to um to cope with the problem and integrate it in the methods that you develop so you need to be aware that information might be missing and and you need to make sure that your method compensates for that this can happen inside the development of your method or as a postdoc it's like a posteriori step and and it can be as simple as i mean in the same view of the co-creation with the user it can be as simple as okay let's interact and see whether i'm missing some part of the information so so somehow one of the reasons why we had to look into the strength of the relationship was also because we were missing these and we had to find a strategy to to survive in the in this case okay um there's another question from ask kim star and uh they are asking what adaptions do you see for knowledge for for this whole set of approaches to deal with multimodality um sorry so the so you had so you had like there was this with the cat picture where there was some computer vision aspects in it um and maybe you reflect a bit on on multimodality when it comes so it's uh it's through that we've never uh it's the kind of knowledge graph we've been dealing with and and multi multi-modal knowledge graphs were not really um uh um that common at that time i think uh so so it's through that one aspect that would be interesting and maybe we kind of we didn't discuss this as as in the future step but it is in these days we are now talking about we hear much more about multimodal knowledge graphs we hear um also about knowledge graph that somehow try to integrate the physical words so like we deal with robots in some scenarios and we also have this problem of okay i have a knowledge graph that has to integrate both abstract concepts but also the physical word um so um uh i think it would be quite interesting to think on how to generate explanations that are multimodal in this sense uh this is one part of the answer in the sense that yes we didn't look into multimodal knowledge graphs and this it could change uh the other part of the answer is uh uh the explanation that we could generate so the kind of patterns that were coming out could also be patterns coming from from multimodal data so like the uh i dealt a lot with clusters of of of data points but these data points could as well be um uh paths of an image for example that will represent that will represent i don't know the ear or the the tail of the cut of this kind of thing so i didn't deal with that concretely but it could uh i i don't see why the method shouldn't work on um image uh labels uh with yeah with specific information that we might want to explain uh so that say with respect to multimodality there are these two parts so the the the original method would probably work also on data points coming up from multimodal data and then the multimodal knowledge graph we didn't look that much into it and it could be quite interesting to to look into multimodal explanation in this sense so that's okay thanks um so maybe as a as a last question jumping back to your history um and because this is the cost action on distributed knowledge graph where we uh care about distributed decentralized things so you you had this one approach that was based on dereferencing of your eyes and looking at things from from that perspective now um you kind of gave me the impression that you say okay now the person who developed the large language model did the web crawling for you but maybe you can say something like all the methods that you have developed in the meantime after you were done with dereferencing your eyes are they based on a global kind of view uh to i don't know generate embeddings and things um or would they still work in a distributed and decentralized setting i i guess as long as uh so the the embeddings are convenient because they can embed a lot of information in in a very small space and that's something that we didn't have so we had to come up with a different dereferencing opportunity but also we didn't want to deal with uh storing the graph because if you store the graph and you query it then you then you you need to know the data model and we didn't care about it so what we really cared about was this kind of serendipitous hope um i remember one of the first papers i i saw was um something all of also did on on on navigation query languages so that was quite quite quite related um so i think the the most important the the part that would still work is even in a distributed context is as long as you have connections or pointers to to from data to data then it's fine it doesn't really matter and uh and we we didn't care whether it was the web of data or it could have been the load and load stored in a in an hdt file um that was really not the the problem the main problem was uh what where is the irrelevant information uh how do i identify the the links between the between that and this is valid in any context do i whether i have the information centralized or decentralized i would say then i have really experimented with that so i can't tell but so good um yeah thanks for the answer um i don't have more questions from the youtube chat um with that i just want you to thank you again for the very nice talk and the very nice tna session before we close the stream i can only advertise the next talk on january the 31st at the same time as today by mayank um you can check out our website and find uh follow us on this thing formerly called twitter and of course you find all the recordings of our talk on our youtube channel", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 9.36, "text": " Okay, so welcome everybody for tonight's talk. Today we have Ilaria and she was going to talk", "tokens": [50364, 1033, 11, 370, 2928, 2201, 337, 4440, 311, 751, 13, 2692, 321, 362, 286, 2200, 654, 293, 750, 390, 516, 281, 751, 50832], "temperature": 0.0, "avg_logprob": -0.1548428807939802, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.021880032494664192}, {"id": 1, "seek": 0, "start": 9.36, "end": 16.64, "text": " about explainability with knowledge graphs. What have we learned? This is the talk series of the", "tokens": [50832, 466, 2903, 2310, 365, 3601, 24877, 13, 708, 362, 321, 3264, 30, 639, 307, 264, 751, 2638, 295, 264, 51196], "temperature": 0.0, "avg_logprob": -0.1548428807939802, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.021880032494664192}, {"id": 2, "seek": 0, "start": 16.64, "end": 23.68, "text": " cost action on distributed knowledge graphs. We've had a few talks already. You can watch them in our", "tokens": [51196, 2063, 3069, 322, 12631, 3601, 24877, 13, 492, 600, 632, 257, 1326, 6686, 1217, 13, 509, 393, 1159, 552, 294, 527, 51548], "temperature": 0.0, "avg_logprob": -0.1548428807939802, "compression_ratio": 1.489795918367347, "no_speech_prob": 0.021880032494664192}, {"id": 3, "seek": 2368, "start": 23.68, "end": 31.04, "text": " YouTube channel and we also have a few talks coming up on January the 31st. We will have a", "tokens": [50364, 3088, 2269, 293, 321, 611, 362, 257, 1326, 6686, 1348, 493, 322, 7061, 264, 10353, 372, 13, 492, 486, 362, 257, 50732], "temperature": 0.0, "avg_logprob": -0.18412731311939382, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.28713756799697876}, {"id": 4, "seek": 2368, "start": 31.04, "end": 38.72, "text": " talk by Mayankerival on March the 4th. We will have a talk by Marc Neusen and on April the 17th", "tokens": [50732, 751, 538, 1891, 657, 260, 3576, 322, 6129, 264, 1017, 392, 13, 492, 486, 362, 257, 751, 538, 18460, 1734, 301, 268, 293, 322, 6929, 264, 3282, 392, 51116], "temperature": 0.0, "avg_logprob": -0.18412731311939382, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.28713756799697876}, {"id": 5, "seek": 2368, "start": 38.72, "end": 48.72, "text": " we have a talk by Peter Patelschneider. What is this cost action? It is a European research network", "tokens": [51116, 321, 362, 257, 751, 538, 6508, 4379, 1625, 339, 716, 1438, 13, 708, 307, 341, 2063, 3069, 30, 467, 307, 257, 6473, 2132, 3209, 51616], "temperature": 0.0, "avg_logprob": -0.18412731311939382, "compression_ratio": 1.5294117647058822, "no_speech_prob": 0.28713756799697876}, {"id": 6, "seek": 4872, "start": 49.519999999999996, "end": 54.64, "text": " where more than 30 countries are represented. We run workshops, hackathons and short term", "tokens": [50404, 689, 544, 813, 2217, 3517, 366, 10379, 13, 492, 1190, 19162, 11, 10339, 998, 892, 293, 2099, 1433, 50660], "temperature": 0.0, "avg_logprob": -0.09035157445651382, "compression_ratio": 1.4421052631578948, "no_speech_prob": 0.0925416424870491}, {"id": 7, "seek": 4872, "start": 54.64, "end": 63.44, "text": " scientific missions for which also you can apply. This overall runs from 2020 to 2024.", "tokens": [50660, 8134, 13744, 337, 597, 611, 291, 393, 3079, 13, 639, 4787, 6676, 490, 4808, 281, 45237, 13, 51100], "temperature": 0.0, "avg_logprob": -0.09035157445651382, "compression_ratio": 1.4421052631578948, "no_speech_prob": 0.0925416424870491}, {"id": 8, "seek": 4872, "start": 64.16, "end": 73.92, "text": " So we have a few months still on the project and we are now in our final year. This is chaired by", "tokens": [51136, 407, 321, 362, 257, 1326, 2493, 920, 322, 264, 1716, 293, 321, 366, 586, 294, 527, 2572, 1064, 13, 639, 307, 6294, 1824, 538, 51624], "temperature": 0.0, "avg_logprob": -0.09035157445651382, "compression_ratio": 1.4421052631578948, "no_speech_prob": 0.0925416424870491}, {"id": 9, "seek": 7392, "start": 73.92, "end": 79.52, "text": " myself. This cost action with Axel Polares as the vice chair. We have Michel Dumontier and", "tokens": [50364, 2059, 13, 639, 2063, 3069, 365, 20118, 338, 3635, 8643, 382, 264, 11964, 6090, 13, 492, 362, 23709, 29572, 896, 811, 293, 50644], "temperature": 0.0, "avg_logprob": -0.2701133266910092, "compression_ratio": 1.546218487394958, "no_speech_prob": 0.08833419531583786}, {"id": 10, "seek": 7392, "start": 79.52, "end": 88.48, "text": " Renzi as working group leads next to Andreas Antoine Olaf. Then there is John as a scientific", "tokens": [50644, 12883, 3992, 382, 1364, 1594, 6689, 958, 281, 38785, 5130, 44454, 48961, 13, 1396, 456, 307, 2619, 382, 257, 8134, 51092], "temperature": 0.0, "avg_logprob": -0.2701133266910092, "compression_ratio": 1.546218487394958, "no_speech_prob": 0.08833419531583786}, {"id": 11, "seek": 7392, "start": 88.48, "end": 94.0, "text": " representative of the grant holder Anastasia Dimou as the science communication manager", "tokens": [51092, 12424, 295, 264, 6386, 20349, 1107, 525, 25251, 20975, 263, 382, 264, 3497, 6101, 6598, 51368], "temperature": 0.0, "avg_logprob": -0.2701133266910092, "compression_ratio": 1.546218487394958, "no_speech_prob": 0.08833419531583786}, {"id": 12, "seek": 7392, "start": 94.0, "end": 102.32000000000001, "text": " and Stefan Gostowicz as the grant coordinator. So much on the cost action. Today we have Ilaria", "tokens": [51368, 293, 32158, 460, 555, 305, 17946, 382, 264, 6386, 27394, 13, 407, 709, 322, 264, 2063, 3069, 13, 2692, 321, 362, 286, 2200, 654, 51784], "temperature": 0.0, "avg_logprob": -0.2701133266910092, "compression_ratio": 1.546218487394958, "no_speech_prob": 0.08833419531583786}, {"id": 13, "seek": 10232, "start": 102.39999999999999, "end": 111.83999999999999, "text": " and I would like to hand over to Olaf to announce the speaker. Yes, hello and welcome from my side", "tokens": [50368, 293, 286, 576, 411, 281, 1011, 670, 281, 48961, 281, 7478, 264, 8145, 13, 1079, 11, 7751, 293, 2928, 490, 452, 1252, 50840], "temperature": 0.0, "avg_logprob": -0.12170848330935917, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.00617504445835948}, {"id": 14, "seek": 10232, "start": 111.83999999999999, "end": 122.16, "text": " as well. I'm very happy to have Ilaria as a speaker. She got her PhD from the Open University in the", "tokens": [50840, 382, 731, 13, 286, 478, 588, 2055, 281, 362, 286, 2200, 654, 382, 257, 8145, 13, 1240, 658, 720, 14476, 490, 264, 7238, 3535, 294, 264, 51356], "temperature": 0.0, "avg_logprob": -0.12170848330935917, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.00617504445835948}, {"id": 15, "seek": 10232, "start": 122.16, "end": 129.68, "text": " UK and the title of the PhD was explaining data patterns using knowledge from the web of data", "tokens": [51356, 7051, 293, 264, 4876, 295, 264, 14476, 390, 13468, 1412, 8294, 1228, 3601, 490, 264, 3670, 295, 1412, 51732], "temperature": 0.0, "avg_logprob": -0.12170848330935917, "compression_ratio": 1.5103092783505154, "no_speech_prob": 0.00617504445835948}, {"id": 16, "seek": 12968, "start": 130.56, "end": 136.72, "text": " and for the PhD she received this distinguished dissertation award of the Semantic Web Science", "tokens": [50408, 293, 337, 264, 14476, 750, 4613, 341, 21702, 39555, 7130, 295, 264, 14421, 7128, 9573, 8976, 50716], "temperature": 0.0, "avg_logprob": -0.18083673907864478, "compression_ratio": 1.6055555555555556, "no_speech_prob": 0.0023209715727716684}, {"id": 17, "seek": 12968, "start": 136.72, "end": 148.72, "text": " Association of Swizzers. She got this award in 2017 and as you maybe know we run this talk series", "tokens": [50716, 10734, 295, 3926, 8072, 433, 13, 1240, 658, 341, 7130, 294, 6591, 293, 382, 291, 1310, 458, 321, 1190, 341, 751, 2638, 51316], "temperature": 0.0, "avg_logprob": -0.18083673907864478, "compression_ratio": 1.6055555555555556, "no_speech_prob": 0.0023209715727716684}, {"id": 18, "seek": 12968, "start": 148.72, "end": 154.08, "text": " with speakers who received either this dissertation award or they received the 10 years award of", "tokens": [51316, 365, 9518, 567, 4613, 2139, 341, 39555, 7130, 420, 436, 4613, 264, 1266, 924, 7130, 295, 51584], "temperature": 0.0, "avg_logprob": -0.18083673907864478, "compression_ratio": 1.6055555555555556, "no_speech_prob": 0.0023209715727716684}, {"id": 19, "seek": 15408, "start": 154.08, "end": 160.4, "text": " Swizzers. So now Ilaria received this award in 2017. One year later she went to Amsterdam", "tokens": [50364, 3926, 8072, 433, 13, 407, 586, 286, 2200, 654, 4613, 341, 7130, 294, 6591, 13, 1485, 1064, 1780, 750, 1437, 281, 28291, 50680], "temperature": 0.0, "avg_logprob": -0.18485168849720673, "compression_ratio": 1.4375, "no_speech_prob": 0.0005702167400158942}, {"id": 20, "seek": 15408, "start": 160.96, "end": 168.64000000000001, "text": " to the Free University in Amsterdam where she became an assistant professor there that she still is.", "tokens": [50708, 281, 264, 11551, 3535, 294, 28291, 689, 750, 3062, 364, 10994, 8304, 456, 300, 750, 920, 307, 13, 51092], "temperature": 0.0, "avg_logprob": -0.18485168849720673, "compression_ratio": 1.4375, "no_speech_prob": 0.0005702167400158942}, {"id": 21, "seek": 15408, "start": 170.4, "end": 179.04000000000002, "text": " She has been involved in several conferences. She is the editor-in-chief for the CEWS", "tokens": [51180, 1240, 575, 668, 3288, 294, 2940, 22032, 13, 1240, 307, 264, 9839, 12, 259, 12, 30189, 337, 264, 28109, 12508, 51612], "temperature": 0.0, "avg_logprob": -0.18485168849720673, "compression_ratio": 1.4375, "no_speech_prob": 0.0005702167400158942}, {"id": 22, "seek": 17904, "start": 179.84, "end": 186.0, "text": " workshop proceedings where many of us probably publish their proceedings of their workshops.", "tokens": [50404, 13541, 37254, 689, 867, 295, 505, 1391, 11374, 641, 37254, 295, 641, 19162, 13, 50712], "temperature": 0.0, "avg_logprob": -0.12611348099178737, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.001432011486031115}, {"id": 23, "seek": 17904, "start": 186.0, "end": 190.39999999999998, "text": " She's also in the steering committee at the moment of the Hybrid Human AI Conference", "tokens": [50712, 1240, 311, 611, 294, 264, 14823, 7482, 412, 264, 1623, 295, 264, 47088, 10294, 7318, 22131, 50932], "temperature": 0.0, "avg_logprob": -0.12611348099178737, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.001432011486031115}, {"id": 24, "seek": 17904, "start": 191.2, "end": 197.84, "text": " and her research, the focus of her research is on systems that combine semantic technologies,", "tokens": [50972, 293, 720, 2132, 11, 264, 1879, 295, 720, 2132, 307, 322, 3652, 300, 10432, 47982, 7943, 11, 51304], "temperature": 0.0, "avg_logprob": -0.12611348099178737, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.001432011486031115}, {"id": 25, "seek": 17904, "start": 197.84, "end": 205.12, "text": " open data, machine learning in order to generate complex narratives with applications mostly in", "tokens": [51304, 1269, 1412, 11, 3479, 2539, 294, 1668, 281, 8460, 3997, 28016, 365, 5821, 5240, 294, 51668], "temperature": 0.0, "avg_logprob": -0.12611348099178737, "compression_ratio": 1.6026200873362446, "no_speech_prob": 0.001432011486031115}, {"id": 26, "seek": 20512, "start": 205.12, "end": 211.44, "text": " scientific scenarios and in robotics and if you look at kind of her most cited papers many of them", "tokens": [50364, 8134, 15077, 293, 294, 34145, 293, 498, 291, 574, 412, 733, 295, 720, 881, 30134, 10577, 867, 295, 552, 50680], "temperature": 0.0, "avg_logprob": -0.13091603329307155, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.002312070457264781}, {"id": 27, "seek": 20512, "start": 211.44, "end": 219.04, "text": " have the word explain or explaining explanation in their title. So I'm very happy to have her here", "tokens": [50680, 362, 264, 1349, 2903, 420, 13468, 10835, 294, 641, 4876, 13, 407, 286, 478, 588, 2055, 281, 362, 720, 510, 51060], "temperature": 0.0, "avg_logprob": -0.13091603329307155, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.002312070457264781}, {"id": 28, "seek": 20512, "start": 219.04, "end": 225.04000000000002, "text": " and kind of reflect on the work that she did in the PhD and what has happened since then with this", "tokens": [51060, 293, 733, 295, 5031, 322, 264, 589, 300, 750, 630, 294, 264, 14476, 293, 437, 575, 2011, 1670, 550, 365, 341, 51360], "temperature": 0.0, "avg_logprob": -0.13091603329307155, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.002312070457264781}, {"id": 29, "seek": 20512, "start": 225.04000000000002, "end": 234.4, "text": " work. The floor is yours Ilaria. Thank you so much. Thank you for all of Tobias for the introduction.", "tokens": [51360, 589, 13, 440, 4123, 307, 6342, 286, 2200, 654, 13, 1044, 291, 370, 709, 13, 1044, 291, 337, 439, 295, 26350, 4609, 337, 264, 9339, 13, 51828], "temperature": 0.0, "avg_logprob": -0.13091603329307155, "compression_ratio": 1.6722689075630253, "no_speech_prob": 0.002312070457264781}, {"id": 30, "seek": 23512, "start": 235.6, "end": 241.52, "text": " Yeah I'm going to share my screen so my question was whether you are keeping the rights.", "tokens": [50388, 865, 286, 478, 516, 281, 2073, 452, 2568, 370, 452, 1168, 390, 1968, 291, 366, 5145, 264, 4601, 13, 50684], "temperature": 0.0, "avg_logprob": -0.24731720195097082, "compression_ratio": 1.4475138121546962, "no_speech_prob": 0.0031911320984363556}, {"id": 31, "seek": 23512, "start": 242.24, "end": 249.12, "text": " Can you just give me a sign that you can see that I think okay. Yeah great. So", "tokens": [50720, 1664, 291, 445, 976, 385, 257, 1465, 300, 291, 393, 536, 300, 286, 519, 1392, 13, 865, 869, 13, 407, 51064], "temperature": 0.0, "avg_logprob": -0.24731720195097082, "compression_ratio": 1.4475138121546962, "no_speech_prob": 0.0031911320984363556}, {"id": 32, "seek": 23512, "start": 252.0, "end": 257.76, "text": " good evening or good morning everybody. My name is Ilaria and I work at the Free University in", "tokens": [51208, 665, 5634, 420, 665, 2446, 2201, 13, 1222, 1315, 307, 286, 2200, 654, 293, 286, 589, 412, 264, 11551, 3535, 294, 51496], "temperature": 0.0, "avg_logprob": -0.24731720195097082, "compression_ratio": 1.4475138121546962, "no_speech_prob": 0.0031911320984363556}, {"id": 33, "seek": 25776, "start": 257.76, "end": 266.64, "text": " Amsterdam. I'm an assistant professor. I'm very very happy of giving this talk today. There was", "tokens": [50364, 28291, 13, 286, 478, 364, 10994, 8304, 13, 286, 478, 588, 588, 2055, 295, 2902, 341, 751, 965, 13, 821, 390, 50808], "temperature": 0.0, "avg_logprob": -0.113884638337528, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.008955389261245728}, {"id": 34, "seek": 25776, "start": 266.64, "end": 274.64, "text": " really a nice opportunity for me to reflect on the overall research that I've been doing in", "tokens": [50808, 534, 257, 1481, 2650, 337, 385, 281, 5031, 322, 264, 4787, 2132, 300, 286, 600, 668, 884, 294, 51208], "temperature": 0.0, "avg_logprob": -0.113884638337528, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.008955389261245728}, {"id": 35, "seek": 25776, "start": 276.64, "end": 283.92, "text": " the past since I was a PhD student and of course it's about explanations which is a very hot topic", "tokens": [51308, 264, 1791, 1670, 286, 390, 257, 14476, 3107, 293, 295, 1164, 309, 311, 466, 28708, 597, 307, 257, 588, 2368, 4829, 51672], "temperature": 0.0, "avg_logprob": -0.113884638337528, "compression_ratio": 1.4818652849740932, "no_speech_prob": 0.008955389261245728}, {"id": 36, "seek": 28392, "start": 283.92, "end": 290.72, "text": " in these days and really my work and my background is on knowledge graphs so I've been working most", "tokens": [50364, 294, 613, 1708, 293, 534, 452, 589, 293, 452, 3678, 307, 322, 3601, 24877, 370, 286, 600, 668, 1364, 881, 50704], "temperature": 0.0, "avg_logprob": -0.14867198601197662, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.00537489727139473}, {"id": 37, "seek": 28392, "start": 290.72, "end": 295.44, "text": " of my life on how to use knowledge graphs in the context of generating explanations", "tokens": [50704, 295, 452, 993, 322, 577, 281, 764, 3601, 24877, 294, 264, 4319, 295, 17746, 28708, 50940], "temperature": 0.0, "avg_logprob": -0.14867198601197662, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.00537489727139473}, {"id": 38, "seek": 28392, "start": 296.08000000000004, "end": 306.96000000000004, "text": " and that's what we are going to see a bit. So I just to all I've gave her a very good introduction", "tokens": [50972, 293, 300, 311, 437, 321, 366, 516, 281, 536, 257, 857, 13, 407, 286, 445, 281, 439, 286, 600, 2729, 720, 257, 588, 665, 9339, 51516], "temperature": 0.0, "avg_logprob": -0.14867198601197662, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.00537489727139473}, {"id": 39, "seek": 28392, "start": 306.96000000000004, "end": 312.08000000000004, "text": " of myself as I said I'm an assistant professor and have read intelligence. My background is on", "tokens": [51516, 295, 2059, 382, 286, 848, 286, 478, 364, 10994, 8304, 293, 362, 1401, 7599, 13, 1222, 3678, 307, 322, 51772], "temperature": 0.0, "avg_logprob": -0.14867198601197662, "compression_ratio": 1.6755555555555555, "no_speech_prob": 0.00537489727139473}, {"id": 40, "seek": 31208, "start": 312.08, "end": 318.71999999999997, "text": " knowledge representation, knowledge graphs, knowledge graphs for explainable AI. These", "tokens": [50364, 3601, 10290, 11, 3601, 24877, 11, 3601, 24877, 337, 2903, 712, 7318, 13, 1981, 50696], "temperature": 0.0, "avg_logprob": -0.15687087962501928, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.002063059015199542}, {"id": 41, "seek": 31208, "start": 318.71999999999997, "end": 325.36, "text": " days I'm mostly focusing on my use cases around hybrid intelligence. You will hear a bit more", "tokens": [50696, 1708, 286, 478, 5240, 8416, 322, 452, 764, 3331, 926, 13051, 7599, 13, 509, 486, 1568, 257, 857, 544, 51028], "temperature": 0.0, "avg_logprob": -0.15687087962501928, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.002063059015199542}, {"id": 42, "seek": 31208, "start": 325.36, "end": 332.4, "text": " later on knowledge representation driven robotics and also scientific assistance and scientific", "tokens": [51028, 1780, 322, 3601, 10290, 9555, 34145, 293, 611, 8134, 9683, 293, 8134, 51380], "temperature": 0.0, "avg_logprob": -0.15687087962501928, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.002063059015199542}, {"id": 43, "seek": 31208, "start": 332.4, "end": 341.36, "text": " discovery. There's more about me you can check on the on my website. This slide is actually wrong", "tokens": [51380, 12114, 13, 821, 311, 544, 466, 385, 291, 393, 1520, 322, 264, 322, 452, 3144, 13, 639, 4137, 307, 767, 2085, 51828], "temperature": 0.0, "avg_logprob": -0.15687087962501928, "compression_ratio": 1.654867256637168, "no_speech_prob": 0.002063059015199542}, {"id": 44, "seek": 34136, "start": 341.36, "end": 348.24, "text": " so it's IWC 2017 that I missed the one when I was about to receive supposed to come on stage and", "tokens": [50364, 370, 309, 311, 286, 54, 34, 6591, 300, 286, 6721, 264, 472, 562, 286, 390, 466, 281, 4774, 3442, 281, 808, 322, 3233, 293, 50708], "temperature": 0.0, "avg_logprob": -0.11478940645853679, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.002629699418321252}, {"id": 45, "seek": 34136, "start": 348.24, "end": 356.96000000000004, "text": " receive the SWSA dissertation award. I was actually driving to a very far plate so I had to miss the", "tokens": [50708, 4774, 264, 20346, 8886, 39555, 7130, 13, 286, 390, 767, 4840, 281, 257, 588, 1400, 5924, 370, 286, 632, 281, 1713, 264, 51144], "temperature": 0.0, "avg_logprob": -0.11478940645853679, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.002629699418321252}, {"id": 46, "seek": 34136, "start": 356.96000000000004, "end": 366.56, "text": " conference but I was very happy of receiving the news nevertheless. So to give you an overview", "tokens": [51144, 7586, 457, 286, 390, 588, 2055, 295, 10040, 264, 2583, 26924, 13, 407, 281, 976, 291, 364, 12492, 51624], "temperature": 0.0, "avg_logprob": -0.11478940645853679, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.002629699418321252}, {"id": 47, "seek": 36656, "start": 366.56, "end": 376.72, "text": " of what this talk is going to be about I will be describing a bit what's the work during my PhD was", "tokens": [50364, 295, 437, 341, 751, 307, 516, 281, 312, 466, 286, 486, 312, 16141, 257, 857, 437, 311, 264, 589, 1830, 452, 14476, 390, 50872], "temperature": 0.0, "avg_logprob": -0.13117833818708147, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.009715252555906773}, {"id": 48, "seek": 36656, "start": 376.72, "end": 385.12, "text": " so as as you heard from all of the title of my thesis was on explaining data patterns using", "tokens": [50872, 370, 382, 382, 291, 2198, 490, 439, 295, 264, 4876, 295, 452, 22288, 390, 322, 13468, 1412, 8294, 1228, 51292], "temperature": 0.0, "avg_logprob": -0.13117833818708147, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.009715252555906773}, {"id": 49, "seek": 36656, "start": 385.12, "end": 392.4, "text": " knowledge from the web of data and then I also want to discuss a bit what has going on since", "tokens": [51292, 3601, 490, 264, 3670, 295, 1412, 293, 550, 286, 611, 528, 281, 2248, 257, 857, 437, 575, 516, 322, 1670, 51656], "temperature": 0.0, "avg_logprob": -0.13117833818708147, "compression_ratio": 1.569060773480663, "no_speech_prob": 0.009715252555906773}, {"id": 50, "seek": 39240, "start": 392.4, "end": 400.64, "text": " then so since 2017 2016-17 what how explanations have developed how our knowledge graphs have", "tokens": [50364, 550, 370, 1670, 6591, 6549, 12, 7773, 437, 577, 28708, 362, 4743, 577, 527, 3601, 24877, 362, 50776], "temperature": 0.0, "avg_logprob": -0.16647488730294363, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0044794706627726555}, {"id": 51, "seek": 39240, "start": 400.64, "end": 407.28, "text": " developed and the overall area and also some and concluding with some future consideration", "tokens": [50776, 4743, 293, 264, 4787, 1859, 293, 611, 512, 293, 9312, 278, 365, 512, 2027, 12381, 51108], "temperature": 0.0, "avg_logprob": -0.16647488730294363, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0044794706627726555}, {"id": 52, "seek": 39240, "start": 407.28, "end": 417.28, "text": " of where do we want to go and and and how can we go from here. Starting with about 10 years ago", "tokens": [51108, 295, 689, 360, 321, 528, 281, 352, 293, 293, 293, 577, 393, 321, 352, 490, 510, 13, 16217, 365, 466, 1266, 924, 2057, 51608], "temperature": 0.0, "avg_logprob": -0.16647488730294363, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0044794706627726555}, {"id": 53, "seek": 41728, "start": 417.28, "end": 426.96, "text": " a bit more it's always hard to realize it over 10 years now. I like to show these pictures", "tokens": [50364, 257, 857, 544, 309, 311, 1009, 1152, 281, 4325, 309, 670, 1266, 924, 586, 13, 286, 411, 281, 855, 613, 5242, 50848], "temperature": 0.0, "avg_logprob": -0.16977188552635303, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.03869329392910004}, {"id": 54, "seek": 41728, "start": 426.96, "end": 432.4, "text": " or map a bit the audience I am not sure I can actually see the chat but maybe somebody wants to", "tokens": [50848, 420, 4471, 257, 857, 264, 4034, 286, 669, 406, 988, 286, 393, 767, 536, 264, 5081, 457, 1310, 2618, 2738, 281, 51120], "temperature": 0.0, "avg_logprob": -0.16977188552635303, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.03869329392910004}, {"id": 55, "seek": 41728, "start": 432.4, "end": 438.88, "text": " I like to ask the question on whether somebody knows this picture which it's quite famous used to", "tokens": [51120, 286, 411, 281, 1029, 264, 1168, 322, 1968, 2618, 3255, 341, 3036, 597, 309, 311, 1596, 4618, 1143, 281, 51444], "temperature": 0.0, "avg_logprob": -0.16977188552635303, "compression_ratio": 1.5604395604395604, "no_speech_prob": 0.03869329392910004}, {"id": 56, "seek": 43888, "start": 438.88, "end": 442.08, "text": " be quite famous I don't know whether somebody is familiar with that", "tokens": [50364, 312, 1596, 4618, 286, 500, 380, 458, 1968, 2618, 307, 4963, 365, 300, 50524], "temperature": 0.0, "avg_logprob": -0.19347851851890827, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.008595037274062634}, {"id": 57, "seek": 43888, "start": 446.56, "end": 456.8, "text": " and this might be how I don't really see the chat but that's okay. So this is a typical", "tokens": [50748, 293, 341, 1062, 312, 577, 286, 500, 380, 534, 536, 264, 5081, 457, 300, 311, 1392, 13, 407, 341, 307, 257, 7476, 51260], "temperature": 0.0, "avg_logprob": -0.19347851851890827, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.008595037274062634}, {"id": 58, "seek": 43888, "start": 458.15999999999997, "end": 464.64, "text": " knowledge discovery process as was firstly introduced around in the middle of the 90s", "tokens": [51328, 3601, 12114, 1399, 382, 390, 27376, 7268, 926, 294, 264, 2808, 295, 264, 4289, 82, 51652], "temperature": 0.0, "avg_logprob": -0.19347851851890827, "compression_ratio": 1.4176470588235295, "no_speech_prob": 0.008595037274062634}, {"id": 59, "seek": 46464, "start": 465.12, "end": 473.03999999999996, "text": " when knowledge discovery was one of the main scientific processes that", "tokens": [50388, 562, 3601, 12114, 390, 472, 295, 264, 2135, 8134, 7555, 300, 50784], "temperature": 0.0, "avg_logprob": -0.23116076909578764, "compression_ratio": 1.525, "no_speech_prob": 0.0064138201996684074}, {"id": 60, "seek": 46464, "start": 475.59999999999997, "end": 484.4, "text": " scientists were using computer science method 4 and the process was actually was", "tokens": [50912, 7708, 645, 1228, 3820, 3497, 3170, 1017, 293, 264, 1399, 390, 767, 390, 51352], "temperature": 0.0, "avg_logprob": -0.23116076909578764, "compression_ratio": 1.525, "no_speech_prob": 0.0064138201996684074}, {"id": 61, "seek": 46464, "start": 484.4, "end": 491.76, "text": " firstly introduced by Fayyad so you can look up the reference and it used to be described as", "tokens": [51352, 27376, 7268, 538, 48889, 88, 345, 370, 291, 393, 574, 493, 264, 6408, 293, 309, 1143, 281, 312, 7619, 382, 51720], "temperature": 0.0, "avg_logprob": -0.23116076909578764, "compression_ratio": 1.525, "no_speech_prob": 0.0064138201996684074}, {"id": 62, "seek": 49176, "start": 491.76, "end": 499.12, "text": " all the steps that scientists need to produce in order to go from data into knowledge", "tokens": [50364, 439, 264, 4439, 300, 7708, 643, 281, 5258, 294, 1668, 281, 352, 490, 1412, 666, 3601, 50732], "temperature": 0.0, "avg_logprob": -0.1315072167594478, "compression_ratio": 1.6878980891719746, "no_speech_prob": 0.00677440594881773}, {"id": 63, "seek": 49176, "start": 499.12, "end": 505.52, "text": " and these steps were mostly selection of the data processing of the data transformation", "tokens": [50732, 293, 613, 4439, 645, 5240, 9450, 295, 264, 1412, 9007, 295, 264, 1412, 9887, 51052], "temperature": 0.0, "avg_logprob": -0.1315072167594478, "compression_ratio": 1.6878980891719746, "no_speech_prob": 0.00677440594881773}, {"id": 64, "seek": 49176, "start": 505.52, "end": 515.04, "text": " data mining that was what came before the current machine learning and deep learning method", "tokens": [51052, 1412, 15512, 300, 390, 437, 1361, 949, 264, 2190, 3479, 2539, 293, 2452, 2539, 3170, 51528], "temperature": 0.0, "avg_logprob": -0.1315072167594478, "compression_ratio": 1.6878980891719746, "no_speech_prob": 0.00677440594881773}, {"id": 65, "seek": 51504, "start": 515.28, "end": 522.7199999999999, "text": " and then interpretation of an evaluation of the patterns so every step in every step you do", "tokens": [50376, 293, 550, 14174, 295, 364, 13344, 295, 264, 8294, 370, 633, 1823, 294, 633, 1823, 291, 360, 50748], "temperature": 0.0, "avg_logprob": -0.17740863408797827, "compression_ratio": 2.005291005291005, "no_speech_prob": 0.005666075274348259}, {"id": 66, "seek": 51504, "start": 522.7199999999999, "end": 529.92, "text": " you transform your data little by little first you identify the relevant information then you", "tokens": [50748, 291, 4088, 428, 1412, 707, 538, 707, 700, 291, 5876, 264, 7340, 1589, 550, 291, 51108], "temperature": 0.0, "avg_logprob": -0.17740863408797827, "compression_ratio": 2.005291005291005, "no_speech_prob": 0.005666075274348259}, {"id": 67, "seek": 51504, "start": 529.92, "end": 536.3199999999999, "text": " preprocess your data you transform them into something that you give to a data mining algorithm", "tokens": [51108, 2666, 340, 780, 428, 1412, 291, 4088, 552, 666, 746, 300, 291, 976, 281, 257, 1412, 15512, 9284, 51428], "temperature": 0.0, "avg_logprob": -0.17740863408797827, "compression_ratio": 2.005291005291005, "no_speech_prob": 0.005666075274348259}, {"id": 68, "seek": 51504, "start": 536.3199999999999, "end": 542.24, "text": " and then you come up with patterns that then need an interpretation and an evaluation in order to", "tokens": [51428, 293, 550, 291, 808, 493, 365, 8294, 300, 550, 643, 364, 14174, 293, 364, 13344, 294, 1668, 281, 51724], "temperature": 0.0, "avg_logprob": -0.17740863408797827, "compression_ratio": 2.005291005291005, "no_speech_prob": 0.005666075274348259}, {"id": 69, "seek": 54224, "start": 542.24, "end": 556.08, "text": " become knowledge. Now what we focused on is mostly this interpretation process so the last step", "tokens": [50364, 1813, 3601, 13, 823, 437, 321, 5178, 322, 307, 5240, 341, 14174, 1399, 370, 264, 1036, 1823, 51056], "temperature": 0.0, "avg_logprob": -0.11563343837343414, "compression_ratio": 1.75, "no_speech_prob": 0.002316010883077979}, {"id": 70, "seek": 54224, "start": 556.8, "end": 563.28, "text": " the interpretation of the patterns was somehow the core of the scientific process was what would", "tokens": [51092, 264, 14174, 295, 264, 8294, 390, 6063, 264, 4965, 295, 264, 8134, 1399, 390, 437, 576, 51416], "temperature": 0.0, "avg_logprob": -0.11563343837343414, "compression_ratio": 1.75, "no_speech_prob": 0.002316010883077979}, {"id": 71, "seek": 54224, "start": 563.28, "end": 571.84, "text": " allow to transform patterns into into knowledge so to give a meaning to these patterns this is", "tokens": [51416, 2089, 281, 4088, 8294, 666, 666, 3601, 370, 281, 976, 257, 3620, 281, 613, 8294, 341, 307, 51844], "temperature": 0.0, "avg_logprob": -0.11563343837343414, "compression_ratio": 1.75, "no_speech_prob": 0.002316010883077979}, {"id": 72, "seek": 57184, "start": 571.9200000000001, "end": 579.52, "text": " actually how interpretation is being defined somehow in the dictionary so it's really the", "tokens": [50368, 767, 577, 14174, 307, 885, 7642, 6063, 294, 264, 25890, 370, 309, 311, 534, 264, 50748], "temperature": 0.0, "avg_logprob": -0.1003043860719915, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00336350011639297}, {"id": 73, "seek": 57184, "start": 579.52, "end": 588.32, "text": " action of capturing the meaning and communicating conveying the meaning of something usually the", "tokens": [50748, 3069, 295, 23384, 264, 3620, 293, 17559, 18053, 1840, 264, 3620, 295, 746, 2673, 264, 51188], "temperature": 0.0, "avg_logprob": -0.1003043860719915, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00336350011639297}, {"id": 74, "seek": 57184, "start": 588.32, "end": 597.36, "text": " way you interpret patterns so the way you try to give a meaning to that to capture this meaning", "tokens": [51188, 636, 291, 7302, 8294, 370, 264, 636, 291, 853, 281, 976, 257, 3620, 281, 300, 281, 7983, 341, 3620, 51640], "temperature": 0.0, "avg_logprob": -0.1003043860719915, "compression_ratio": 1.7961783439490446, "no_speech_prob": 0.00336350011639297}, {"id": 75, "seek": 59736, "start": 597.36, "end": 602.96, "text": " is by using your own background knowledge so you come up with patterns you might have", "tokens": [50364, 307, 538, 1228, 428, 1065, 3678, 3601, 370, 291, 808, 493, 365, 8294, 291, 1062, 362, 50644], "temperature": 0.0, "avg_logprob": -0.06847974045635903, "compression_ratio": 1.9680851063829787, "no_speech_prob": 0.004152372013777494}, {"id": 76, "seek": 59736, "start": 603.6800000000001, "end": 608.64, "text": " your own background knowledge or maybe human experts in a topic that explain the patterns", "tokens": [50680, 428, 1065, 3678, 3601, 420, 1310, 1952, 8572, 294, 257, 4829, 300, 2903, 264, 8294, 50928], "temperature": 0.0, "avg_logprob": -0.06847974045635903, "compression_ratio": 1.9680851063829787, "no_speech_prob": 0.004152372013777494}, {"id": 77, "seek": 59736, "start": 608.64, "end": 616.48, "text": " that the data mining algorithm has come up with and that helps evaluating and interpreting this", "tokens": [50928, 300, 264, 1412, 15512, 9284, 575, 808, 493, 365, 293, 300, 3665, 27479, 293, 37395, 341, 51320], "temperature": 0.0, "avg_logprob": -0.06847974045635903, "compression_ratio": 1.9680851063829787, "no_speech_prob": 0.004152372013777494}, {"id": 78, "seek": 59736, "start": 616.48, "end": 624.8000000000001, "text": " knowledge the problem is this background knowledge might be missing so you might require maybe the", "tokens": [51320, 3601, 264, 1154, 307, 341, 3678, 3601, 1062, 312, 5361, 370, 291, 1062, 3651, 1310, 264, 51736], "temperature": 0.0, "avg_logprob": -0.06847974045635903, "compression_ratio": 1.9680851063829787, "no_speech_prob": 0.004152372013777494}, {"id": 79, "seek": 62480, "start": 624.8, "end": 632.24, "text": " expert doesn't know the actual explanation or you might need in certain use cases you might need", "tokens": [50364, 5844, 1177, 380, 458, 264, 3539, 10835, 420, 291, 1062, 643, 294, 1629, 764, 3331, 291, 1062, 643, 50736], "temperature": 0.0, "avg_logprob": -0.13472127031396935, "compression_ratio": 1.6407185628742516, "no_speech_prob": 0.00297881243750453}, {"id": 80, "seek": 62480, "start": 632.24, "end": 638.56, "text": " different experts from different domains so gathering this background knowledge might be", "tokens": [50736, 819, 8572, 490, 819, 25514, 370, 13519, 341, 3678, 3601, 1062, 312, 51052], "temperature": 0.0, "avg_logprob": -0.13472127031396935, "compression_ratio": 1.6407185628742516, "no_speech_prob": 0.00297881243750453}, {"id": 81, "seek": 62480, "start": 638.56, "end": 648.7199999999999, "text": " quite a time consuming process in this case when this information is missing the kind of", "tokens": [51052, 1596, 257, 565, 19867, 1399, 294, 341, 1389, 562, 341, 1589, 307, 5361, 264, 733, 295, 51560], "temperature": 0.0, "avg_logprob": -0.13472127031396935, "compression_ratio": 1.6407185628742516, "no_speech_prob": 0.00297881243750453}, {"id": 82, "seek": 64872, "start": 648.8000000000001, "end": 659.0400000000001, "text": " hypothesis that we put forward was that symbolic AI so symbols or knowledge graphs or linked open", "tokens": [50368, 17291, 300, 321, 829, 2128, 390, 300, 25755, 7318, 370, 16944, 420, 3601, 24877, 420, 9408, 1269, 50880], "temperature": 0.0, "avg_logprob": -0.11949820745558966, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.012049905024468899}, {"id": 83, "seek": 64872, "start": 659.0400000000001, "end": 666.48, "text": " data as we used to call them at that time are another source of background knowledge so the", "tokens": [50880, 1412, 382, 321, 1143, 281, 818, 552, 412, 300, 565, 366, 1071, 4009, 295, 3678, 3601, 370, 264, 51252], "temperature": 0.0, "avg_logprob": -0.11949820745558966, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.012049905024468899}, {"id": 84, "seek": 64872, "start": 666.48, "end": 677.28, "text": " kind of idea that we had is okay if we have plenty of multiple of knowledge graph of data sources", "tokens": [51252, 733, 295, 1558, 300, 321, 632, 307, 1392, 498, 321, 362, 7140, 295, 3866, 295, 3601, 4295, 295, 1412, 7139, 51792], "temperature": 0.0, "avg_logprob": -0.11949820745558966, "compression_ratio": 1.6589595375722543, "no_speech_prob": 0.012049905024468899}, {"id": 85, "seek": 67728, "start": 677.28, "end": 683.6, "text": " that are multi-domain that are connected between each other so at that time we had the link data", "tokens": [50364, 300, 366, 4825, 12, 4121, 491, 300, 366, 4582, 1296, 1184, 661, 370, 412, 300, 565, 321, 632, 264, 2113, 1412, 50680], "temperature": 0.0, "avg_logprob": -0.12764438180362478, "compression_ratio": 1.72, "no_speech_prob": 0.004842740949243307}, {"id": 86, "seek": 67728, "start": 683.6, "end": 689.4399999999999, "text": " cloud I think this is this picture is slightly later than 2013 but the idea is to have all these", "tokens": [50680, 4588, 286, 519, 341, 307, 341, 3036, 307, 4748, 1780, 813, 9012, 457, 264, 1558, 307, 281, 362, 439, 613, 50972], "temperature": 0.0, "avg_logprob": -0.12764438180362478, "compression_ratio": 1.72, "no_speech_prob": 0.004842740949243307}, {"id": 87, "seek": 67728, "start": 689.4399999999999, "end": 698.0, "text": " connected data sets that point to each other and then you can continuously discover knowledge simply", "tokens": [50972, 4582, 1412, 6352, 300, 935, 281, 1184, 661, 293, 550, 291, 393, 15684, 4411, 3601, 2935, 51400], "temperature": 0.0, "avg_logprob": -0.12764438180362478, "compression_ratio": 1.72, "no_speech_prob": 0.004842740949243307}, {"id": 88, "seek": 67728, "start": 698.0, "end": 704.3199999999999, "text": " by crawling data set one after the other this information is most of the time it's connected", "tokens": [51400, 538, 32979, 1412, 992, 472, 934, 264, 661, 341, 1589, 307, 881, 295, 264, 565, 309, 311, 4582, 51716], "temperature": 0.0, "avg_logprob": -0.12764438180362478, "compression_ratio": 1.72, "no_speech_prob": 0.004842740949243307}, {"id": 89, "seek": 70432, "start": 704.32, "end": 712.32, "text": " it's centralized in hubs and observatories it's standardized according to certain vocabularies", "tokens": [50364, 309, 311, 32395, 294, 46870, 293, 9951, 30077, 309, 311, 31677, 4650, 281, 1629, 2329, 455, 1040, 530, 50764], "temperature": 0.0, "avg_logprob": -0.13041065633296967, "compression_ratio": 1.6242774566473988, "no_speech_prob": 0.0033374091144651175}, {"id": 90, "seek": 70432, "start": 712.32, "end": 723.0400000000001, "text": " that allow modeling data sharing data so the the vocabularies would allow to to to have inter", "tokens": [50764, 300, 2089, 15983, 1412, 5414, 1412, 370, 264, 264, 2329, 455, 1040, 530, 576, 2089, 281, 281, 281, 362, 728, 51300], "temperature": 0.0, "avg_logprob": -0.13041065633296967, "compression_ratio": 1.6242774566473988, "no_speech_prob": 0.0033374091144651175}, {"id": 91, "seek": 70432, "start": 723.0400000000001, "end": 730.1600000000001, "text": " operability across application then the kind of idea was okay maybe we can use this all this", "tokens": [51300, 2208, 2310, 2108, 3861, 550, 264, 733, 295, 1558, 390, 1392, 1310, 321, 393, 764, 341, 439, 341, 51656], "temperature": 0.0, "avg_logprob": -0.13041065633296967, "compression_ratio": 1.6242774566473988, "no_speech_prob": 0.0033374091144651175}, {"id": 92, "seek": 73016, "start": 730.16, "end": 738.8, "text": " information which is available online to help us explaining the patterns that an algorithm gives us", "tokens": [50364, 1589, 597, 307, 2435, 2950, 281, 854, 505, 13468, 264, 8294, 300, 364, 9284, 2709, 505, 50796], "temperature": 0.0, "avg_logprob": -0.10185693594125601, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.008310066536068916}, {"id": 93, "seek": 73016, "start": 739.6, "end": 743.76, "text": " whenever we don't have an expert in the picture or whenever we are missing the background knowledge", "tokens": [50836, 5699, 321, 500, 380, 362, 364, 5844, 294, 264, 3036, 420, 5699, 321, 366, 5361, 264, 3678, 3601, 51044], "temperature": 0.0, "avg_logprob": -0.10185693594125601, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.008310066536068916}, {"id": 94, "seek": 73016, "start": 743.76, "end": 754.56, "text": " to explain that and and it is very similar to the idea that Newell and Simon already in the 70s had", "tokens": [51044, 281, 2903, 300, 293, 293, 309, 307, 588, 2531, 281, 264, 1558, 300, 1734, 6326, 293, 13193, 1217, 294, 264, 5285, 82, 632, 51584], "temperature": 0.0, "avg_logprob": -0.10185693594125601, "compression_ratio": 1.5904255319148937, "no_speech_prob": 0.008310066536068916}, {"id": 95, "seek": 75456, "start": 755.52, "end": 765.28, "text": " so among the the faders in AI that symbols were one additional layer to to use when somebody", "tokens": [50412, 370, 3654, 264, 264, 283, 15221, 294, 7318, 300, 16944, 645, 472, 4497, 4583, 281, 281, 764, 562, 2618, 50900], "temperature": 0.0, "avg_logprob": -0.14642059101777918, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.010955833829939365}, {"id": 96, "seek": 75456, "start": 765.28, "end": 772.2399999999999, "text": " wants to capture and and and convey the meaning so you don't only need the data or the experience", "tokens": [50900, 2738, 281, 7983, 293, 293, 293, 16965, 264, 3620, 370, 291, 500, 380, 787, 643, 264, 1412, 420, 264, 1752, 51248], "temperature": 0.0, "avg_logprob": -0.14642059101777918, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.010955833829939365}, {"id": 97, "seek": 75456, "start": 772.2399999999999, "end": 777.92, "text": " but you need to put a structure and you need symbols on top of it in order to really get into", "tokens": [51248, 457, 291, 643, 281, 829, 257, 3877, 293, 291, 643, 16944, 322, 1192, 295, 309, 294, 1668, 281, 534, 483, 666, 51532], "temperature": 0.0, "avg_logprob": -0.14642059101777918, "compression_ratio": 1.6511627906976745, "no_speech_prob": 0.010955833829939365}, {"id": 98, "seek": 77792, "start": 778.88, "end": 787.36, "text": " into an intelligent system that can understand and capture meaning so based on these hypotheses", "tokens": [50412, 666, 364, 13232, 1185, 300, 393, 1223, 293, 7983, 3620, 370, 2361, 322, 613, 49969, 50836], "temperature": 0.0, "avg_logprob": -0.1392221450805664, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.004182700999081135}, {"id": 99, "seek": 77792, "start": 787.36, "end": 795.68, "text": " we set up a number of research questions so the very first one we we looked into was okay if we", "tokens": [50836, 321, 992, 493, 257, 1230, 295, 2132, 1651, 370, 264, 588, 700, 472, 321, 321, 2956, 666, 390, 1392, 498, 321, 51252], "temperature": 0.0, "avg_logprob": -0.1392221450805664, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.004182700999081135}, {"id": 100, "seek": 77792, "start": 795.68, "end": 801.92, "text": " want to use uh if if we want to use knowledge graph large-scale knowledge graphs to to to", "tokens": [51252, 528, 281, 764, 2232, 498, 498, 321, 528, 281, 764, 3601, 4295, 2416, 12, 20033, 3601, 24877, 281, 281, 281, 51564], "temperature": 0.0, "avg_logprob": -0.1392221450805664, "compression_ratio": 1.6927710843373494, "no_speech_prob": 0.004182700999081135}, {"id": 101, "seek": 80192, "start": 801.92, "end": 807.5999999999999, "text": " generate explanations um we kind of need to understand what do we mean by an explanation", "tokens": [50364, 8460, 28708, 1105, 321, 733, 295, 643, 281, 1223, 437, 360, 321, 914, 538, 364, 10835, 50648], "temperature": 0.0, "avg_logprob": -0.07592964726825092, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.012089090421795845}, {"id": 102, "seek": 80192, "start": 807.5999999999999, "end": 813.04, "text": " so we need a definition even if it's a working definition we still need one and then we kind", "tokens": [50648, 370, 321, 643, 257, 7123, 754, 498, 309, 311, 257, 1364, 7123, 321, 920, 643, 472, 293, 550, 321, 733, 50920], "temperature": 0.0, "avg_logprob": -0.07592964726825092, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.012089090421795845}, {"id": 103, "seek": 80192, "start": 813.04, "end": 818.56, "text": " of said okay we need a method that will allow us to to to generate explanations from knowledge", "tokens": [50920, 295, 848, 1392, 321, 643, 257, 3170, 300, 486, 2089, 505, 281, 281, 281, 8460, 28708, 490, 3601, 51196], "temperature": 0.0, "avg_logprob": -0.07592964726825092, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.012089090421795845}, {"id": 104, "seek": 80192, "start": 818.56, "end": 828.48, "text": " graphs so how how are we going to do that um then then assuming we come up with one such a method", "tokens": [51196, 24877, 370, 577, 577, 366, 321, 516, 281, 360, 300, 1105, 550, 550, 11926, 321, 808, 493, 365, 472, 1270, 257, 3170, 51692], "temperature": 0.0, "avg_logprob": -0.07592964726825092, "compression_ratio": 1.879396984924623, "no_speech_prob": 0.012089090421795845}, {"id": 105, "seek": 82848, "start": 828.48, "end": 833.28, "text": " we need to try to cope with all the problems that come with knowledge graph including", "tokens": [50364, 321, 643, 281, 853, 281, 22598, 365, 439, 264, 2740, 300, 808, 365, 3601, 4295, 3009, 50604], "temperature": 0.0, "avg_logprob": -0.1165181692544516, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.006090231239795685}, {"id": 106, "seek": 82848, "start": 834.0, "end": 843.52, "text": " incompleteness, bias, noise um so so the the later research questions obviously some of the", "tokens": [50640, 14036, 14657, 15264, 11, 12577, 11, 5658, 1105, 370, 370, 264, 264, 1780, 2132, 1651, 2745, 512, 295, 264, 51116], "temperature": 0.0, "avg_logprob": -0.1165181692544516, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.006090231239795685}, {"id": 107, "seek": 82848, "start": 843.52, "end": 847.84, "text": " research questions came out little by little throughout the process right so you look at your", "tokens": [51116, 2132, 1651, 1361, 484, 707, 538, 707, 3710, 264, 1399, 558, 370, 291, 574, 412, 428, 51332], "temperature": 0.0, "avg_logprob": -0.1165181692544516, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.006090231239795685}, {"id": 108, "seek": 82848, "start": 847.84, "end": 855.44, "text": " phd from a different perspective but somehow the the later research questions were on improving", "tokens": [51332, 903, 67, 490, 257, 819, 4585, 457, 6063, 264, 264, 1780, 2132, 1651, 645, 322, 11470, 51712], "temperature": 0.0, "avg_logprob": -0.1165181692544516, "compression_ratio": 1.7902439024390244, "no_speech_prob": 0.006090231239795685}, {"id": 109, "seek": 85544, "start": 855.44, "end": 859.6, "text": " the methods that we had in order to cope with the limitations of knowledge graphs including", "tokens": [50364, 264, 7150, 300, 321, 632, 294, 1668, 281, 22598, 365, 264, 15705, 295, 3601, 24877, 3009, 50572], "temperature": 0.0, "avg_logprob": -0.07321101427078247, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.003266730112954974}, {"id": 110, "seek": 85544, "start": 859.6, "end": 868.1600000000001, "text": " incompleteness and bias and basically uh looking now i'm going to go through the the research", "tokens": [50572, 14036, 14657, 15264, 293, 12577, 293, 1936, 2232, 1237, 586, 741, 478, 516, 281, 352, 807, 264, 264, 2132, 51000], "temperature": 0.0, "avg_logprob": -0.07321101427078247, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.003266730112954974}, {"id": 111, "seek": 85544, "start": 868.1600000000001, "end": 874.24, "text": " questions but i don't want to dive too much into it i mean we have papers for that i'm just the next", "tokens": [51000, 1651, 457, 741, 500, 380, 528, 281, 9192, 886, 709, 666, 309, 741, 914, 321, 362, 10577, 337, 300, 741, 478, 445, 264, 958, 51304], "temperature": 0.0, "avg_logprob": -0.07321101427078247, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.003266730112954974}, {"id": 112, "seek": 85544, "start": 874.24, "end": 881.36, "text": " slides are mostly to give you an idea of what the overall approach was uh starting with the the", "tokens": [51304, 9788, 366, 5240, 281, 976, 291, 364, 1558, 295, 437, 264, 4787, 3109, 390, 2232, 2891, 365, 264, 264, 51660], "temperature": 0.0, "avg_logprob": -0.07321101427078247, "compression_ratio": 1.668122270742358, "no_speech_prob": 0.003266730112954974}, {"id": 113, "seek": 88136, "start": 881.36, "end": 887.84, "text": " definition of explanation we defined we first defined an ontology design pattern for explanation", "tokens": [50364, 7123, 295, 10835, 321, 7642, 321, 700, 7642, 364, 6592, 1793, 1715, 5102, 337, 10835, 50688], "temperature": 0.0, "avg_logprob": -0.10318589891706194, "compression_ratio": 2.088397790055249, "no_speech_prob": 0.006876310799270868}, {"id": 114, "seek": 88136, "start": 887.84, "end": 893.36, "text": " so what we did was we looked into different disciplines we looked into philosophy we looked", "tokens": [50688, 370, 437, 321, 630, 390, 321, 2956, 666, 819, 21919, 321, 2956, 666, 10675, 321, 2956, 50964], "temperature": 0.0, "avg_logprob": -0.10318589891706194, "compression_ratio": 2.088397790055249, "no_speech_prob": 0.006876310799270868}, {"id": 115, "seek": 88136, "start": 893.36, "end": 901.6, "text": " into linguistics into social science and we looked into their definition of explanations and we", "tokens": [50964, 666, 21766, 6006, 666, 2093, 3497, 293, 321, 2956, 666, 641, 7123, 295, 28708, 293, 321, 51376], "temperature": 0.0, "avg_logprob": -0.10318589891706194, "compression_ratio": 2.088397790055249, "no_speech_prob": 0.006876310799270868}, {"id": 116, "seek": 88136, "start": 901.6, "end": 907.52, "text": " noticed that even though there were different approaches and methods explanations were always", "tokens": [51376, 5694, 300, 754, 1673, 456, 645, 819, 11587, 293, 7150, 28708, 645, 1009, 51672], "temperature": 0.0, "avg_logprob": -0.10318589891706194, "compression_ratio": 2.088397790055249, "no_speech_prob": 0.006876310799270868}, {"id": 117, "seek": 90752, "start": 907.52, "end": 913.1999999999999, "text": " seen according to the similar characteristics so there was always the generation of some", "tokens": [50364, 1612, 4650, 281, 264, 2531, 10891, 370, 456, 390, 1009, 264, 5125, 295, 512, 50648], "temperature": 0.0, "avg_logprob": -0.12239777551938409, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.015749899670481682}, {"id": 118, "seek": 90752, "start": 913.1999999999999, "end": 918.8, "text": " coherence between old knowledge and new knowledge uh the elements in the explanation were always the", "tokens": [50648, 26528, 655, 1296, 1331, 3601, 293, 777, 3601, 2232, 264, 4959, 294, 264, 10835, 645, 1009, 264, 50928], "temperature": 0.0, "avg_logprob": -0.12239777551938409, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.015749899670481682}, {"id": 119, "seek": 90752, "start": 918.8, "end": 925.52, "text": " same there was a theory there were an anterior and a posterior event there were sequences that", "tokens": [50928, 912, 456, 390, 257, 5261, 456, 645, 364, 22272, 293, 257, 33529, 2280, 456, 645, 22978, 300, 51264], "temperature": 0.0, "avg_logprob": -0.12239777551938409, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.015749899670481682}, {"id": 120, "seek": 90752, "start": 925.52, "end": 933.4399999999999, "text": " would make this event happening at the same time and there were always processes um", "tokens": [51264, 576, 652, 341, 2280, 2737, 412, 264, 912, 565, 293, 456, 645, 1009, 7555, 1105, 51660], "temperature": 0.0, "avg_logprob": -0.12239777551938409, "compression_ratio": 1.936842105263158, "no_speech_prob": 0.015749899670481682}, {"id": 121, "seek": 93344, "start": 934.4000000000001, "end": 940.0, "text": " uh that would be one internal process where you come up with an explanation for yourself and then", "tokens": [50412, 2232, 300, 576, 312, 472, 6920, 1399, 689, 291, 808, 493, 365, 364, 10835, 337, 1803, 293, 550, 50692], "temperature": 0.0, "avg_logprob": -0.0891790855221632, "compression_ratio": 1.9794871794871796, "no_speech_prob": 0.0018707994604483247}, {"id": 122, "seek": 93344, "start": 940.0, "end": 944.6400000000001, "text": " one that is more an external process where you communicated the explanation to the rest of the", "tokens": [50692, 472, 300, 307, 544, 364, 8320, 1399, 689, 291, 34989, 264, 10835, 281, 264, 1472, 295, 264, 50924], "temperature": 0.0, "avg_logprob": -0.0891790855221632, "compression_ratio": 1.9794871794871796, "no_speech_prob": 0.0018707994604483247}, {"id": 123, "seek": 93344, "start": 944.6400000000001, "end": 953.0400000000001, "text": " world and based on this we then defined our own uh ontology design ontology for an explanation so", "tokens": [50924, 1002, 293, 2361, 322, 341, 321, 550, 7642, 527, 1065, 2232, 6592, 1793, 1715, 6592, 1793, 337, 364, 10835, 370, 51344], "temperature": 0.0, "avg_logprob": -0.0891790855221632, "compression_ratio": 1.9794871794871796, "no_speech_prob": 0.0018707994604483247}, {"id": 124, "seek": 93344, "start": 953.0400000000001, "end": 959.12, "text": " our own definition that we could then feed into a system that would try to generate explanation", "tokens": [51344, 527, 1065, 7123, 300, 321, 727, 550, 3154, 666, 257, 1185, 300, 576, 853, 281, 8460, 10835, 51648], "temperature": 0.0, "avg_logprob": -0.0891790855221632, "compression_ratio": 1.9794871794871796, "no_speech_prob": 0.0018707994604483247}, {"id": 125, "seek": 95912, "start": 959.2, "end": 964.88, "text": " according to these patterns so we we then just it's a pattern in the sense that it can be", "tokens": [50368, 4650, 281, 613, 8294, 370, 321, 321, 550, 445, 309, 311, 257, 5102, 294, 264, 2020, 300, 309, 393, 312, 50652], "temperature": 0.0, "avg_logprob": -0.09960930997675116, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.012475824914872646}, {"id": 126, "seek": 95912, "start": 964.88, "end": 972.4, "text": " instantiated in multiple um in multiple contexts but the overall idea was that you always have an", "tokens": [50652, 9836, 72, 770, 294, 3866, 1105, 294, 3866, 30628, 457, 264, 4787, 1558, 390, 300, 291, 1009, 362, 364, 51028], "temperature": 0.0, "avg_logprob": -0.09960930997675116, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.012475824914872646}, {"id": 127, "seek": 95912, "start": 972.4, "end": 978.16, "text": " event uh that happens before an event that happens after I'm not sure you see my pointer but I hope", "tokens": [51028, 2280, 2232, 300, 2314, 949, 364, 2280, 300, 2314, 934, 286, 478, 406, 988, 291, 536, 452, 23918, 457, 286, 1454, 51316], "temperature": 0.0, "avg_logprob": -0.09960930997675116, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.012475824914872646}, {"id": 128, "seek": 95912, "start": 978.16, "end": 986.24, "text": " so uh there are certain uh conditions that make this uh this event happening in a specific setting", "tokens": [51316, 370, 2232, 456, 366, 1629, 2232, 4487, 300, 652, 341, 2232, 341, 2280, 2737, 294, 257, 2685, 3287, 51720], "temperature": 0.0, "avg_logprob": -0.09960930997675116, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.012475824914872646}, {"id": 129, "seek": 98624, "start": 986.24, "end": 994.08, "text": " and then there's some sort of uh theory behind and a sort and an agent that that that outputs the", "tokens": [50364, 293, 550, 456, 311, 512, 1333, 295, 2232, 5261, 2261, 293, 257, 1333, 293, 364, 9461, 300, 300, 300, 23930, 264, 50756], "temperature": 0.0, "avg_logprob": -0.1475784448476938, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.0024935207329690456}, {"id": 130, "seek": 98624, "start": 994.08, "end": 1004.48, "text": " that creates the conceptualizes the uh the explanation uh based on um based on this pattern", "tokens": [50756, 300, 7829, 264, 24106, 5660, 264, 2232, 264, 10835, 2232, 2361, 322, 1105, 2361, 322, 341, 5102, 51276], "temperature": 0.0, "avg_logprob": -0.1475784448476938, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.0024935207329690456}, {"id": 131, "seek": 98624, "start": 1004.48, "end": 1011.04, "text": " we then try to to design a system that would try to use knowledge graph to generate explanation for", "tokens": [51276, 321, 550, 853, 281, 281, 1715, 257, 1185, 300, 576, 853, 281, 764, 3601, 4295, 281, 8460, 10835, 337, 51604], "temperature": 0.0, "avg_logprob": -0.1475784448476938, "compression_ratio": 1.795031055900621, "no_speech_prob": 0.0024935207329690456}, {"id": 132, "seek": 101104, "start": 1011.04, "end": 1017.04, "text": " a given pattern of data and then the the kind of question we try to answer is really okay if we", "tokens": [50364, 257, 2212, 5102, 295, 1412, 293, 550, 264, 264, 733, 295, 1168, 321, 853, 281, 1867, 307, 534, 1392, 498, 321, 50664], "temperature": 0.0, "avg_logprob": -0.08435637530158548, "compression_ratio": 1.8203883495145632, "no_speech_prob": 0.004728011321276426}, {"id": 133, "seek": 101104, "start": 1017.04, "end": 1022.56, "text": " want to use a knowledge graph a very large knowledge graph uh as background knowledge", "tokens": [50664, 528, 281, 764, 257, 3601, 4295, 257, 588, 2416, 3601, 4295, 2232, 382, 3678, 3601, 50940], "temperature": 0.0, "avg_logprob": -0.08435637530158548, "compression_ratio": 1.8203883495145632, "no_speech_prob": 0.004728011321276426}, {"id": 134, "seek": 101104, "start": 1022.56, "end": 1028.8799999999999, "text": " which kind of process do I need to uh generate explanation and then we try to work with some", "tokens": [50940, 597, 733, 295, 1399, 360, 286, 643, 281, 2232, 8460, 10835, 293, 550, 321, 853, 281, 589, 365, 512, 51256], "temperature": 0.0, "avg_logprob": -0.08435637530158548, "compression_ratio": 1.8203883495145632, "no_speech_prob": 0.004728011321276426}, {"id": 135, "seek": 101104, "start": 1028.8799999999999, "end": 1036.48, "text": " with some examples of patterns of data of things that people might want to to explain uh we had this", "tokens": [51256, 365, 512, 5110, 295, 8294, 295, 1412, 295, 721, 300, 561, 1062, 528, 281, 281, 2903, 2232, 321, 632, 341, 51636], "temperature": 0.0, "avg_logprob": -0.08435637530158548, "compression_ratio": 1.8203883495145632, "no_speech_prob": 0.004728011321276426}, {"id": 136, "seek": 103648, "start": 1036.96, "end": 1043.28, "text": " very good example that would work with uh google trends and then you try to explain why a certain", "tokens": [50388, 588, 665, 1365, 300, 576, 589, 365, 2232, 20742, 13892, 293, 550, 291, 853, 281, 2903, 983, 257, 1629, 50704], "temperature": 0.0, "avg_logprob": -0.12182464156039925, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0052220867946743965}, {"id": 137, "seek": 103648, "start": 1043.28, "end": 1050.72, "text": " website uh is uh regularly happening at specific points in time so in these cases why people are", "tokens": [50704, 3144, 2232, 307, 2232, 11672, 2737, 412, 2685, 2793, 294, 565, 370, 294, 613, 3331, 983, 561, 366, 51076], "temperature": 0.0, "avg_logprob": -0.12182464156039925, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0052220867946743965}, {"id": 138, "seek": 103648, "start": 1050.72, "end": 1056.24, "text": " searching for the term a song in ice and fire only in certain periods so you see that there are very", "tokens": [51076, 10808, 337, 264, 1433, 257, 2153, 294, 4435, 293, 2610, 787, 294, 1629, 13804, 370, 291, 536, 300, 456, 366, 588, 51352], "temperature": 0.0, "avg_logprob": -0.12182464156039925, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0052220867946743965}, {"id": 139, "seek": 103648, "start": 1056.24, "end": 1063.76, "text": " regular peaks or maybe we try to explain data like statistical data for example from the UNESCO", "tokens": [51352, 3890, 26897, 420, 1310, 321, 853, 281, 2903, 1412, 411, 22820, 1412, 337, 1365, 490, 264, 8229, 47024, 51728], "temperature": 0.0, "avg_logprob": -0.12182464156039925, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.0052220867946743965}, {"id": 140, "seek": 106376, "start": 1064.32, "end": 1070.08, "text": " uh in this case we have uh countries that are grouped according to the", "tokens": [50392, 2232, 294, 341, 1389, 321, 362, 2232, 3517, 300, 366, 41877, 4650, 281, 264, 50680], "temperature": 0.0, "avg_logprob": -0.15056791305541992, "compression_ratio": 1.788659793814433, "no_speech_prob": 0.005033801309764385}, {"id": 141, "seek": 106376, "start": 1071.04, "end": 1077.6, "text": " female literacy rate and then you try to explain okay why is is it oops oops sorry uh why is it", "tokens": [50728, 6556, 23166, 3314, 293, 550, 291, 853, 281, 2903, 1392, 983, 307, 307, 309, 34166, 34166, 2597, 2232, 983, 307, 309, 51056], "temperature": 0.0, "avg_logprob": -0.15056791305541992, "compression_ratio": 1.788659793814433, "no_speech_prob": 0.005033801309764385}, {"id": 142, "seek": 106376, "start": 1077.6, "end": 1084.96, "text": " happening that um that that certain countries have in common like a certain characteristic", "tokens": [51056, 2737, 300, 1105, 300, 300, 1629, 3517, 362, 294, 2689, 411, 257, 1629, 16282, 51424], "temperature": 0.0, "avg_logprob": -0.15056791305541992, "compression_ratio": 1.788659793814433, "no_speech_prob": 0.005033801309764385}, {"id": 143, "seek": 106376, "start": 1085.6, "end": 1091.84, "text": " and the very good thing is that uh we try to come up with a method that had us uh working", "tokens": [51456, 293, 264, 588, 665, 551, 307, 300, 2232, 321, 853, 281, 808, 493, 365, 257, 3170, 300, 632, 505, 2232, 1364, 51768], "temperature": 0.0, "avg_logprob": -0.15056791305541992, "compression_ratio": 1.788659793814433, "no_speech_prob": 0.005033801309764385}, {"id": 144, "seek": 109184, "start": 1091.9199999999998, "end": 1098.3999999999999, "text": " out these examples and it was basically it was based on three main steps one was an inductive", "tokens": [50368, 484, 613, 5110, 293, 309, 390, 1936, 309, 390, 2361, 322, 1045, 2135, 4439, 472, 390, 364, 31612, 488, 50692], "temperature": 0.0, "avg_logprob": -0.07449513826614772, "compression_ratio": 1.8509615384615385, "no_speech_prob": 0.006417805328965187}, {"id": 145, "seek": 109184, "start": 1098.3999999999999, "end": 1103.9199999999998, "text": " logic programming step where you try to compare positive and negative examples and you have some", "tokens": [50692, 9952, 9410, 1823, 689, 291, 853, 281, 6794, 3353, 293, 3671, 5110, 293, 291, 362, 512, 50968], "temperature": 0.0, "avg_logprob": -0.07449513826614772, "compression_ratio": 1.8509615384615385, "no_speech_prob": 0.006417805328965187}, {"id": 146, "seek": 109184, "start": 1103.9199999999998, "end": 1110.32, "text": " background knowledge expressed in uh as as tripos as facts about these examples and then the goal", "tokens": [50968, 3678, 3601, 12675, 294, 2232, 382, 382, 4931, 329, 382, 9130, 466, 613, 5110, 293, 550, 264, 3387, 51288], "temperature": 0.0, "avg_logprob": -0.07449513826614772, "compression_ratio": 1.8509615384615385, "no_speech_prob": 0.006417805328965187}, {"id": 147, "seek": 109184, "start": 1110.32, "end": 1115.6799999999998, "text": " is to induce the hypothesis that mostly represents your positive examples which are the examples", "tokens": [51288, 307, 281, 41263, 264, 17291, 300, 5240, 8855, 428, 3353, 5110, 597, 366, 264, 5110, 51556], "temperature": 0.0, "avg_logprob": -0.07449513826614772, "compression_ratio": 1.8509615384615385, "no_speech_prob": 0.006417805328965187}, {"id": 148, "seek": 111568, "start": 1115.68, "end": 1125.6000000000001, "text": " you want to explain uh we used a knowledge graph search so we we try to uh uh search the the link", "tokens": [50364, 291, 528, 281, 2903, 2232, 321, 1143, 257, 3601, 4295, 3164, 370, 321, 321, 853, 281, 2232, 2232, 3164, 264, 264, 2113, 50860], "temperature": 0.0, "avg_logprob": -0.11309600232252434, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.011893230490386486}, {"id": 149, "seek": 111568, "start": 1125.6000000000001, "end": 1132.64, "text": " data graph uh in order to find a common path which would be expressed in terms of predicates", "tokens": [50860, 1412, 4295, 2232, 294, 1668, 281, 915, 257, 2689, 3100, 597, 576, 312, 12675, 294, 2115, 295, 47336, 1024, 51212], "temperature": 0.0, "avg_logprob": -0.11309600232252434, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.011893230490386486}, {"id": 150, "seek": 111568, "start": 1133.2, "end": 1139.52, "text": " relationships uh leading to a specific entity in the graph so we call this path and then we", "tokens": [51240, 6159, 2232, 5775, 281, 257, 2685, 13977, 294, 264, 4295, 370, 321, 818, 341, 3100, 293, 550, 321, 51556], "temperature": 0.0, "avg_logprob": -0.11309600232252434, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.011893230490386486}, {"id": 151, "seek": 113952, "start": 1139.52, "end": 1146.48, "text": " would consider any path that is uh common uh to all the positive examples as an explanation for the", "tokens": [50364, 576, 1949, 604, 3100, 300, 307, 2232, 2689, 2232, 281, 439, 264, 3353, 5110, 382, 364, 10835, 337, 264, 50712], "temperature": 0.0, "avg_logprob": -0.12707277554184643, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.002668346744030714}, {"id": 152, "seek": 113952, "start": 1146.48, "end": 1154.08, "text": " group of positive example uh and then in order to to to improve the the the graph search we we", "tokens": [50712, 1594, 295, 3353, 1365, 2232, 293, 550, 294, 1668, 281, 281, 281, 3470, 264, 264, 264, 4295, 3164, 321, 321, 51092], "temperature": 0.0, "avg_logprob": -0.12707277554184643, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.002668346744030714}, {"id": 153, "seek": 113952, "start": 1154.08, "end": 1163.04, "text": " implemented a greedy uh links reversal strategy uh so we try to aim for uh the uh longer and longer", "tokens": [51092, 12270, 257, 28228, 2232, 6123, 42778, 5206, 2232, 370, 321, 853, 281, 5939, 337, 2232, 264, 2232, 2854, 293, 2854, 51540], "temperature": 0.0, "avg_logprob": -0.12707277554184643, "compression_ratio": 1.719298245614035, "no_speech_prob": 0.002668346744030714}, {"id": 154, "seek": 116304, "start": 1163.04, "end": 1170.1599999999999, "text": " path you know um and explore the graph using simple http d referencing in order to avoid", "tokens": [50364, 3100, 291, 458, 1105, 293, 6839, 264, 4295, 1228, 2199, 276, 6319, 79, 274, 40582, 294, 1668, 281, 5042, 50720], "temperature": 0.0, "avg_logprob": -0.14823352999803496, "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.003251373302191496}, {"id": 155, "seek": 116304, "start": 1170.1599999999999, "end": 1177.44, "text": " computational costs it was 2013 we still had issues of computational costs and uh based on this", "tokens": [50720, 28270, 5497, 309, 390, 9012, 321, 920, 632, 2663, 295, 28270, 5497, 293, 2232, 2361, 322, 341, 51084], "temperature": 0.0, "avg_logprob": -0.14823352999803496, "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.003251373302191496}, {"id": 156, "seek": 116304, "start": 1177.44, "end": 1182.96, "text": " method we try to answer mostly two questions so first we said okay which kind of your uh", "tokens": [51084, 3170, 321, 853, 281, 1867, 5240, 732, 1651, 370, 700, 321, 848, 1392, 597, 733, 295, 428, 2232, 51360], "temperature": 0.0, "avg_logprob": -0.14823352999803496, "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.003251373302191496}, {"id": 157, "seek": 116304, "start": 1182.96, "end": 1189.6, "text": " risks do we need to to drive a greedy search and identify the best explanation so we try", "tokens": [51360, 10888, 360, 321, 643, 281, 281, 3332, 257, 28228, 3164, 293, 5876, 264, 1151, 10835, 370, 321, 853, 51692], "temperature": 0.0, "avg_logprob": -0.14823352999803496, "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.003251373302191496}, {"id": 158, "seek": 118960, "start": 1189.6, "end": 1195.9199999999998, "text": " different strategies and then one of the main finding for us was that a measure so a strategy", "tokens": [50364, 819, 9029, 293, 550, 472, 295, 264, 2135, 5006, 337, 505, 390, 300, 257, 3481, 370, 257, 5206, 50680], "temperature": 0.0, "avg_logprob": -0.12837718781970797, "compression_ratio": 1.6932515337423313, "no_speech_prob": 0.011957566253840923}, {"id": 159, "seek": 118960, "start": 1196.48, "end": 1205.9199999999998, "text": " based on entropy um would lead to a higher explanation in in in in less time so we simply", "tokens": [50708, 2361, 322, 30867, 1105, 576, 1477, 281, 257, 2946, 10835, 294, 294, 294, 294, 1570, 565, 370, 321, 2935, 51180], "temperature": 0.0, "avg_logprob": -0.12837718781970797, "compression_ratio": 1.6932515337423313, "no_speech_prob": 0.011957566253840923}, {"id": 160, "seek": 118960, "start": 1205.9199999999998, "end": 1214.9599999999998, "text": " measured on what is the best accuracy of an explanation over time so this is um uh iteration", "tokens": [51180, 12690, 322, 437, 307, 264, 1151, 14170, 295, 364, 10835, 670, 565, 370, 341, 307, 1105, 2232, 24784, 51632], "temperature": 0.0, "avg_logprob": -0.12837718781970797, "compression_ratio": 1.6932515337423313, "no_speech_prob": 0.011957566253840923}, {"id": 161, "seek": 121496, "start": 1214.96, "end": 1221.28, "text": " in searching the graph and we always ended up having the best uh an entropy based measure", "tokens": [50364, 294, 10808, 264, 4295, 293, 321, 1009, 4590, 493, 1419, 264, 1151, 2232, 364, 30867, 2361, 3481, 50680], "temperature": 0.0, "avg_logprob": -0.070719788714153, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.009561413899064064}, {"id": 162, "seek": 121496, "start": 1221.28, "end": 1228.56, "text": " as the one that would perform best and give us the best explanation uh these allowed us to to to", "tokens": [50680, 382, 264, 472, 300, 576, 2042, 1151, 293, 976, 505, 264, 1151, 10835, 2232, 613, 4350, 505, 281, 281, 281, 51044], "temperature": 0.0, "avg_logprob": -0.070719788714153, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.009561413899064064}, {"id": 163, "seek": 121496, "start": 1228.56, "end": 1235.68, "text": " come up with uh explanation for the use cases i was showing before so uh why are women less", "tokens": [51044, 808, 493, 365, 2232, 10835, 337, 264, 764, 3331, 741, 390, 4099, 949, 370, 2232, 983, 366, 2266, 1570, 51400], "temperature": 0.0, "avg_logprob": -0.070719788714153, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.009561413899064064}, {"id": 164, "seek": 121496, "start": 1235.68, "end": 1240.16, "text": " educated than men in certain countries and that's mostly when countries are least developed for", "tokens": [51400, 15872, 813, 1706, 294, 1629, 3517, 293, 300, 311, 5240, 562, 3517, 366, 1935, 4743, 337, 51624], "temperature": 0.0, "avg_logprob": -0.070719788714153, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.009561413899064064}, {"id": 165, "seek": 124016, "start": 1240.16, "end": 1248.4, "text": " example uh so they have a quite a high human development index rank and uh or they are expressed", "tokens": [50364, 1365, 2232, 370, 436, 362, 257, 1596, 257, 1090, 1952, 3250, 8186, 6181, 293, 2232, 420, 436, 366, 12675, 50776], "temperature": 0.0, "avg_logprob": -0.17685292372063025, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.004702082835137844}, {"id": 166, "seek": 124016, "start": 1248.4, "end": 1256.0, "text": " they are defined in dbpb as least developed countries um or we we had explanations like okay", "tokens": [50776, 436, 366, 7642, 294, 274, 65, 79, 65, 382, 1935, 4743, 3517, 1105, 420, 321, 321, 632, 28708, 411, 1392, 51156], "temperature": 0.0, "avg_logprob": -0.17685292372063025, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.004702082835137844}, {"id": 167, "seek": 124016, "start": 1256.0, "end": 1263.8400000000001, "text": " people search for a song in ice and fire whenever there is an event that is somehow linked to a", "tokens": [51156, 561, 3164, 337, 257, 2153, 294, 4435, 293, 2610, 5699, 456, 307, 364, 2280, 300, 307, 6063, 9408, 281, 257, 51548], "temperature": 0.0, "avg_logprob": -0.17685292372063025, "compression_ratio": 1.601123595505618, "no_speech_prob": 0.004702082835137844}, {"id": 168, "seek": 126384, "start": 1263.84, "end": 1271.12, "text": " game of thrones tv series so that's this is we consider these explanations but of course if you", "tokens": [50364, 1216, 295, 739, 2213, 16364, 2638, 370, 300, 311, 341, 307, 321, 1949, 613, 28708, 457, 295, 1164, 498, 291, 50728], "temperature": 0.0, "avg_logprob": -0.0822026252746582, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.008412543684244156}, {"id": 169, "seek": 126384, "start": 1271.12, "end": 1278.32, "text": " look at the results you might notice things that are quite not right and uh in particular you look", "tokens": [50728, 574, 412, 264, 3542, 291, 1062, 3449, 721, 300, 366, 1596, 406, 558, 293, 2232, 294, 1729, 291, 574, 51088], "temperature": 0.0, "avg_logprob": -0.0822026252746582, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.008412543684244156}, {"id": 170, "seek": 126384, "start": 1278.32, "end": 1284.48, "text": " at these kind of examples so uh it's true that if you have enough background knowledge about a song", "tokens": [51088, 412, 613, 733, 295, 5110, 370, 2232, 309, 311, 2074, 300, 498, 291, 362, 1547, 3678, 3601, 466, 257, 2153, 51396], "temperature": 0.0, "avg_logprob": -0.0822026252746582, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.008412543684244156}, {"id": 171, "seek": 126384, "start": 1284.48, "end": 1290.9599999999998, "text": " in ice and fire which is actually a book uh and you know that there is a certain tv series related", "tokens": [51396, 294, 4435, 293, 2610, 597, 307, 767, 257, 1446, 2232, 293, 291, 458, 300, 456, 307, 257, 1629, 16364, 2638, 4077, 51720], "temperature": 0.0, "avg_logprob": -0.0822026252746582, "compression_ratio": 1.738938053097345, "no_speech_prob": 0.008412543684244156}, {"id": 172, "seek": 129096, "start": 1291.6000000000001, "end": 1296.56, "text": " that is based on this book you know that the explanation for people being particularly", "tokens": [50396, 300, 307, 2361, 322, 341, 1446, 291, 458, 300, 264, 10835, 337, 561, 885, 4098, 50644], "temperature": 0.0, "avg_logprob": -0.09226451437157321, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.016774296760559082}, {"id": 173, "seek": 129096, "start": 1296.56, "end": 1302.72, "text": " interested in these uh is whenever the tv series is coming out so there is a new season coming out", "tokens": [50644, 3102, 294, 613, 2232, 307, 5699, 264, 16364, 2638, 307, 1348, 484, 370, 456, 307, 257, 777, 3196, 1348, 484, 50952], "temperature": 0.0, "avg_logprob": -0.09226451437157321, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.016774296760559082}, {"id": 174, "seek": 129096, "start": 1302.72, "end": 1310.32, "text": " but there's nothing that relates the tonga tonga uh to to to the book or basketball", "tokens": [50952, 457, 456, 311, 1825, 300, 16155, 264, 9124, 64, 9124, 64, 2232, 281, 281, 281, 264, 1446, 420, 11767, 51332], "temperature": 0.0, "avg_logprob": -0.09226451437157321, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.016774296760559082}, {"id": 175, "seek": 129096, "start": 1310.32, "end": 1317.3600000000001, "text": " competition to the book so the kind of questions we had to ask answer afterwards was really okay is", "tokens": [51332, 6211, 281, 264, 1446, 370, 264, 733, 295, 1651, 321, 632, 281, 1029, 1867, 10543, 390, 534, 1392, 307, 51684], "temperature": 0.0, "avg_logprob": -0.09226451437157321, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.016774296760559082}, {"id": 176, "seek": 131736, "start": 1317.36, "end": 1324.1599999999999, "text": " there something in linked data that can tell us that game of thrones is strongly related to a song", "tokens": [50364, 456, 746, 294, 9408, 1412, 300, 393, 980, 505, 300, 1216, 295, 739, 2213, 307, 10613, 4077, 281, 257, 2153, 50704], "temperature": 0.0, "avg_logprob": -0.06931195503626114, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.003508328925818205}, {"id": 177, "seek": 131736, "start": 1324.1599999999999, "end": 1329.9199999999998, "text": " in ice and fire much more than basketball competitions or anything related to tonga so the", "tokens": [50704, 294, 4435, 293, 2610, 709, 544, 813, 11767, 26185, 420, 1340, 4077, 281, 9124, 64, 370, 264, 50992], "temperature": 0.0, "avg_logprob": -0.06931195503626114, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.003508328925818205}, {"id": 178, "seek": 131736, "start": 1329.9199999999998, "end": 1336.0, "text": " second part of the method was really focused on strengthening this knowledge based explanation", "tokens": [50992, 1150, 644, 295, 264, 3170, 390, 534, 5178, 322, 28224, 341, 3601, 2361, 10835, 51296], "temperature": 0.0, "avg_logprob": -0.06931195503626114, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.003508328925818205}, {"id": 179, "seek": 131736, "start": 1336.0, "end": 1343.28, "text": " and we focused we used a genetic programming algorithm to try to learn um a function that", "tokens": [51296, 293, 321, 5178, 321, 1143, 257, 12462, 9410, 9284, 281, 853, 281, 1466, 1105, 257, 2445, 300, 51660], "temperature": 0.0, "avg_logprob": -0.06931195503626114, "compression_ratio": 1.7314814814814814, "no_speech_prob": 0.003508328925818205}, {"id": 180, "seek": 134328, "start": 1343.28, "end": 1350.0, "text": " could detect the strong relationships between two graph entities that are quite distant in the graph", "tokens": [50364, 727, 5531, 264, 2068, 6159, 1296, 732, 4295, 16667, 300, 366, 1596, 17275, 294, 264, 4295, 50700], "temperature": 0.0, "avg_logprob": -0.09749251320248559, "compression_ratio": 1.90521327014218, "no_speech_prob": 0.011306723579764366}, {"id": 181, "seek": 134328, "start": 1350.0, "end": 1355.68, "text": " so we are not talking about two entities in in in the same graph but you have one entity that is", "tokens": [50700, 370, 321, 366, 406, 1417, 466, 732, 16667, 294, 294, 294, 264, 912, 4295, 457, 291, 362, 472, 13977, 300, 307, 50984], "temperature": 0.0, "avg_logprob": -0.09749251320248559, "compression_ratio": 1.90521327014218, "no_speech_prob": 0.011306723579764366}, {"id": 182, "seek": 134328, "start": 1355.68, "end": 1362.32, "text": " connected to another entity through hops in multiple graphs in multiple data sets and then we really", "tokens": [50984, 4582, 281, 1071, 13977, 807, 47579, 294, 3866, 24877, 294, 3866, 1412, 6352, 293, 550, 321, 534, 51316], "temperature": 0.0, "avg_logprob": -0.09749251320248559, "compression_ratio": 1.90521327014218, "no_speech_prob": 0.011306723579764366}, {"id": 183, "seek": 134328, "start": 1362.32, "end": 1368.0, "text": " want to try to understand what is the the strongest relationship and what we did is that with a genetic", "tokens": [51316, 528, 281, 853, 281, 1223, 437, 307, 264, 264, 16595, 2480, 293, 437, 321, 630, 307, 300, 365, 257, 12462, 51600], "temperature": 0.0, "avg_logprob": -0.09749251320248559, "compression_ratio": 1.90521327014218, "no_speech_prob": 0.011306723579764366}, {"id": 184, "seek": 136800, "start": 1368.0, "end": 1376.08, "text": " programming algorithm we try to learn a function that could study the topological and semantic", "tokens": [50364, 9410, 9284, 321, 853, 281, 1466, 257, 2445, 300, 727, 2979, 264, 1192, 4383, 293, 47982, 50768], "temperature": 0.0, "avg_logprob": -0.10224351882934571, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.014258738607168198}, {"id": 185, "seek": 136800, "start": 1376.08, "end": 1382.72, "text": " characteristics of the knowledge graph so we really looked into how many hubs and how many", "tokens": [50768, 10891, 295, 264, 3601, 4295, 370, 321, 534, 2956, 666, 577, 867, 46870, 293, 577, 867, 51100], "temperature": 0.0, "avg_logprob": -0.10224351882934571, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.014258738607168198}, {"id": 186, "seek": 136800, "start": 1385.52, "end": 1390.32, "text": " how strongly connected was the graph for example which kind of vocabulary the graph was was using", "tokens": [51240, 577, 10613, 4582, 390, 264, 4295, 337, 1365, 597, 733, 295, 19864, 264, 4295, 390, 390, 1228, 51480], "temperature": 0.0, "avg_logprob": -0.10224351882934571, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.014258738607168198}, {"id": 187, "seek": 136800, "start": 1391.6, "end": 1396.4, "text": " all these we gave all these to our genetic programming algorithm and then we tried to", "tokens": [51544, 439, 613, 321, 2729, 439, 613, 281, 527, 12462, 9410, 9284, 293, 550, 321, 3031, 281, 51784], "temperature": 0.0, "avg_logprob": -0.10224351882934571, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.014258738607168198}, {"id": 188, "seek": 139640, "start": 1396.4, "end": 1403.0400000000002, "text": " come up with a with a function that would tell us okay this is a strong relationship and we evaluated", "tokens": [50364, 808, 493, 365, 257, 365, 257, 2445, 300, 576, 980, 505, 1392, 341, 307, 257, 2068, 2480, 293, 321, 25509, 50696], "temperature": 0.0, "avg_logprob": -0.08738729357719421, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.0050927335396409035}, {"id": 189, "seek": 139640, "start": 1403.0400000000002, "end": 1410.8000000000002, "text": " that with against a human evaluated relationship path and um actually what what came out from", "tokens": [50696, 300, 365, 1970, 257, 1952, 25509, 2480, 3100, 293, 1105, 767, 437, 437, 1361, 484, 490, 51084], "temperature": 0.0, "avg_logprob": -0.08738729357719421, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.0050927335396409035}, {"id": 190, "seek": 139640, "start": 1410.8000000000002, "end": 1418.16, "text": " from this study was that the best thing for us would have been to try to follow uh notes that", "tokens": [51084, 490, 341, 2979, 390, 300, 264, 1151, 551, 337, 505, 576, 362, 668, 281, 853, 281, 1524, 2232, 5570, 300, 51452], "temperature": 0.0, "avg_logprob": -0.08738729357719421, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.0050927335396409035}, {"id": 191, "seek": 139640, "start": 1418.16, "end": 1423.8400000000001, "text": " would have quite rich descriptions so here you have different examples of different functions", "tokens": [51452, 576, 362, 1596, 4593, 24406, 370, 510, 291, 362, 819, 5110, 295, 819, 6828, 51736], "temperature": 0.0, "avg_logprob": -0.08738729357719421, "compression_ratio": 1.8413461538461537, "no_speech_prob": 0.0050927335396409035}, {"id": 192, "seek": 142384, "start": 1423.84, "end": 1430.9599999999998, "text": " that we tried and then here you have the say the best performing ones um and the the the best", "tokens": [50364, 300, 321, 3031, 293, 550, 510, 291, 362, 264, 584, 264, 1151, 10205, 2306, 1105, 293, 264, 264, 264, 1151, 50720], "temperature": 0.0, "avg_logprob": -0.07974536244462176, "compression_ratio": 1.8, "no_speech_prob": 0.005011760629713535}, {"id": 193, "seek": 142384, "start": 1430.9599999999998, "end": 1438.24, "text": " functions that we could find were really the ones were focusing on uh following notes with", "tokens": [50720, 6828, 300, 321, 727, 915, 645, 534, 264, 2306, 645, 8416, 322, 2232, 3480, 5570, 365, 51084], "temperature": 0.0, "avg_logprob": -0.07974536244462176, "compression_ratio": 1.8, "no_speech_prob": 0.005011760629713535}, {"id": 194, "seek": 142384, "start": 1438.24, "end": 1445.04, "text": " rich descriptions that they would have quite a good number of namespaces uh it was best to follow", "tokens": [51084, 4593, 24406, 300, 436, 576, 362, 1596, 257, 665, 1230, 295, 5288, 79, 2116, 2232, 309, 390, 1151, 281, 1524, 51424], "temperature": 0.0, "avg_logprob": -0.07974536244462176, "compression_ratio": 1.8, "no_speech_prob": 0.005011760629713535}, {"id": 195, "seek": 142384, "start": 1445.04, "end": 1450.8799999999999, "text": " more specific entities so really not not not hubs with many incoming links but rather something", "tokens": [51424, 544, 2685, 16667, 370, 534, 406, 406, 406, 46870, 365, 867, 22341, 6123, 457, 2831, 746, 51716], "temperature": 0.0, "avg_logprob": -0.07974536244462176, "compression_ratio": 1.8, "no_speech_prob": 0.005011760629713535}, {"id": 196, "seek": 145088, "start": 1450.88, "end": 1458.3200000000002, "text": " that is more specific and also we look into scores the scores vocabulary and uh it was best to have", "tokens": [50364, 300, 307, 544, 2685, 293, 611, 321, 574, 666, 13444, 264, 13444, 19864, 293, 2232, 309, 390, 1151, 281, 362, 50736], "temperature": 0.0, "avg_logprob": -0.13370805711888556, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.003632445354014635}, {"id": 197, "seek": 145088, "start": 1458.3200000000002, "end": 1464.16, "text": " fewer topical categories so not something quite generic in some in terms of topic but but rather", "tokens": [50736, 13366, 1192, 804, 10479, 370, 406, 746, 1596, 19577, 294, 512, 294, 2115, 295, 4829, 457, 457, 2831, 51028], "temperature": 0.0, "avg_logprob": -0.13370805711888556, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.003632445354014635}, {"id": 198, "seek": 145088, "start": 1466.0800000000002, "end": 1475.7600000000002, "text": " again a bit more specificity uh of course we had to look into into into into into into the bias", "tokens": [51124, 797, 257, 857, 544, 2685, 507, 2232, 295, 1164, 321, 632, 281, 574, 666, 666, 666, 666, 666, 666, 264, 12577, 51608], "temperature": 0.0, "avg_logprob": -0.13370805711888556, "compression_ratio": 1.7590361445783131, "no_speech_prob": 0.003632445354014635}, {"id": 199, "seek": 147576, "start": 1475.76, "end": 1482.96, "text": " in the data as well so the the results were not optimal and as I said we needed to deal both with", "tokens": [50364, 294, 264, 1412, 382, 731, 370, 264, 264, 3542, 645, 406, 16252, 293, 382, 286, 848, 321, 2978, 281, 2028, 1293, 365, 50724], "temperature": 0.0, "avg_logprob": -0.1438508241072945, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.010157558135688305}, {"id": 200, "seek": 147576, "start": 1482.96, "end": 1490.8, "text": " incompleteness of the data but also when inner bias of the data and what we tried to do was to um", "tokens": [50724, 14036, 14657, 15264, 295, 264, 1412, 457, 611, 562, 7284, 12577, 295, 264, 1412, 293, 437, 321, 3031, 281, 360, 390, 281, 1105, 51116], "temperature": 0.0, "avg_logprob": -0.1438508241072945, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.010157558135688305}, {"id": 201, "seek": 147576, "start": 1491.68, "end": 1497.44, "text": " use identity links so remember that we are still we were still we are still talking about link data", "tokens": [51160, 764, 6575, 6123, 370, 1604, 300, 321, 366, 920, 321, 645, 920, 321, 366, 920, 1417, 466, 2113, 1412, 51448], "temperature": 0.0, "avg_logprob": -0.1438508241072945, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.010157558135688305}, {"id": 202, "seek": 147576, "start": 1497.44, "end": 1504.08, "text": " where data sets are connected to each other through identity links including same as and what we uh", "tokens": [51448, 689, 1412, 6352, 366, 4582, 281, 1184, 661, 807, 6575, 6123, 3009, 912, 382, 293, 437, 321, 2232, 51780], "temperature": 0.0, "avg_logprob": -0.1438508241072945, "compression_ratio": 1.872037914691943, "no_speech_prob": 0.010157558135688305}, {"id": 203, "seek": 150408, "start": 1504.72, "end": 1514.72, "text": " tried to do was to um measure the bias in a given data set uh by uh comparing the projection on one", "tokens": [50396, 3031, 281, 360, 390, 281, 1105, 3481, 264, 12577, 294, 257, 2212, 1412, 992, 2232, 538, 2232, 15763, 264, 22743, 322, 472, 50896], "temperature": 0.0, "avg_logprob": -0.09052443158799324, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.0037156210746616125}, {"id": 204, "seek": 150408, "start": 1514.72, "end": 1520.0, "text": " data set into another one so the overall idea was really that you have a data set which could be", "tokens": [50896, 1412, 992, 666, 1071, 472, 370, 264, 4787, 1558, 390, 534, 300, 291, 362, 257, 1412, 992, 597, 727, 312, 51160], "temperature": 0.0, "avg_logprob": -0.09052443158799324, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.0037156210746616125}, {"id": 205, "seek": 150408, "start": 1520.0, "end": 1525.84, "text": " I don't know the link movie database and you have a certain amount of entities that are connected", "tokens": [51160, 286, 500, 380, 458, 264, 2113, 3169, 8149, 293, 291, 362, 257, 1629, 2372, 295, 16667, 300, 366, 4582, 51452], "temperature": 0.0, "avg_logprob": -0.09052443158799324, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.0037156210746616125}, {"id": 206, "seek": 152584, "start": 1525.9199999999998, "end": 1535.6, "text": " through the dbpdia uh using identity links and the projection of the the movie database is mostly", "tokens": [50368, 807, 264, 274, 65, 79, 67, 654, 2232, 1228, 6575, 6123, 293, 264, 22743, 295, 264, 264, 3169, 8149, 307, 5240, 50852], "temperature": 0.0, "avg_logprob": -0.1283707618713379, "compression_ratio": 1.7763975155279503, "no_speech_prob": 0.008629011921584606}, {"id": 207, "seek": 152584, "start": 1535.6, "end": 1542.08, "text": " the set of entities that's in dbpdia that are connected to to to the movie database and then", "tokens": [50852, 264, 992, 295, 16667, 300, 311, 294, 274, 65, 79, 67, 654, 300, 366, 4582, 281, 281, 281, 264, 3169, 8149, 293, 550, 51176], "temperature": 0.0, "avg_logprob": -0.1283707618713379, "compression_ratio": 1.7763975155279503, "no_speech_prob": 0.008629011921584606}, {"id": 208, "seek": 152584, "start": 1542.08, "end": 1550.56, "text": " we we would basically compare all the entities in the larger data set with the subset and using", "tokens": [51176, 321, 321, 576, 1936, 6794, 439, 264, 16667, 294, 264, 4833, 1412, 992, 365, 264, 25993, 293, 1228, 51600], "temperature": 0.0, "avg_logprob": -0.1283707618713379, "compression_ratio": 1.7763975155279503, "no_speech_prob": 0.008629011921584606}, {"id": 209, "seek": 155056, "start": 1550.56, "end": 1558.3999999999999, "text": " some correlation uh tests or some t-test and by comparing the distribution of uh property value", "tokens": [50364, 512, 20009, 2232, 6921, 420, 512, 256, 12, 31636, 293, 538, 15763, 264, 7316, 295, 2232, 4707, 2158, 50756], "temperature": 0.0, "avg_logprob": -0.1734789960524615, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.003441066946834326}, {"id": 210, "seek": 155056, "start": 1558.3999999999999, "end": 1566.32, "text": " pairs uh between the subset of the entities and the large set uh we try to identify with uh which", "tokens": [50756, 15494, 2232, 1296, 264, 25993, 295, 264, 16667, 293, 264, 2416, 992, 2232, 321, 853, 281, 5876, 365, 2232, 597, 51152], "temperature": 0.0, "avg_logprob": -0.1734789960524615, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.003441066946834326}, {"id": 211, "seek": 155056, "start": 1566.32, "end": 1575.36, "text": " were the the the the the bias in a given data set uh so for example we uh and yeah I would", "tokens": [51152, 645, 264, 264, 264, 264, 264, 12577, 294, 257, 2212, 1412, 992, 2232, 370, 337, 1365, 321, 2232, 293, 1338, 286, 576, 51604], "temperature": 0.0, "avg_logprob": -0.1734789960524615, "compression_ratio": 1.632183908045977, "no_speech_prob": 0.003441066946834326}, {"id": 212, "seek": 157536, "start": 1575.9199999999998, "end": 1583.12, "text": " uh send uh I would recommend you to refer to the paper but what we what we discovered was", "tokens": [50392, 2232, 2845, 2232, 286, 576, 2748, 291, 281, 2864, 281, 264, 3035, 457, 437, 321, 437, 321, 6941, 390, 50752], "temperature": 0.0, "avg_logprob": -0.1750271479288737, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.009055104106664658}, {"id": 213, "seek": 157536, "start": 1583.12, "end": 1589.6799999999998, "text": " for example that the link movie database was particularly focused on black and white movies and", "tokens": [50752, 337, 1365, 300, 264, 2113, 3169, 8149, 390, 4098, 5178, 322, 2211, 293, 2418, 6233, 293, 51080], "temperature": 0.0, "avg_logprob": -0.1750271479288737, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.009055104106664658}, {"id": 214, "seek": 157536, "start": 1591.6, "end": 1598.56, "text": " that certain digital humanities data sets were focused on uh uh poets and novelists from the", "tokens": [51176, 300, 1629, 4562, 36140, 1412, 6352, 645, 5178, 322, 2232, 2232, 38364, 293, 7613, 1751, 490, 264, 51524], "temperature": 0.0, "avg_logprob": -0.1750271479288737, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.009055104106664658}, {"id": 215, "seek": 159856, "start": 1598.56, "end": 1606.72, "text": " 18th and 19th century so it was a way to try to measure the bias in the in the in the data", "tokens": [50364, 2443, 392, 293, 1294, 392, 4901, 370, 309, 390, 257, 636, 281, 853, 281, 3481, 264, 12577, 294, 264, 294, 264, 294, 264, 1412, 50772], "temperature": 0.0, "avg_logprob": -0.12158024500286768, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.006493692751973867}, {"id": 216, "seek": 159856, "start": 1606.72, "end": 1613.84, "text": " set that we were using to generate explanations and to once we learned these bias we could also try to", "tokens": [50772, 992, 300, 321, 645, 1228, 281, 8460, 28708, 293, 281, 1564, 321, 3264, 613, 12577, 321, 727, 611, 853, 281, 51128], "temperature": 0.0, "avg_logprob": -0.12158024500286768, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.006493692751973867}, {"id": 217, "seek": 159856, "start": 1617.44, "end": 1624.0, "text": " somehow input this information in our system and generate better explanations", "tokens": [51308, 6063, 4846, 341, 1589, 294, 527, 1185, 293, 8460, 1101, 28708, 51636], "temperature": 0.0, "avg_logprob": -0.12158024500286768, "compression_ratio": 1.7044025157232705, "no_speech_prob": 0.006493692751973867}, {"id": 218, "seek": 162400, "start": 1624.88, "end": 1633.68, "text": " now this was uh uh quite a long a long journey that uh ended up in 2016 and there's a number of", "tokens": [50408, 586, 341, 390, 2232, 2232, 1596, 257, 938, 257, 938, 4671, 300, 2232, 4590, 493, 294, 6549, 293, 456, 311, 257, 1230, 295, 50848], "temperature": 0.0, "avg_logprob": -0.10524677568011814, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.003971994388848543}, {"id": 219, "seek": 162400, "start": 1633.68, "end": 1642.24, "text": " things that happened uh since then what I what I'm I call the the present um so I don't have", "tokens": [50848, 721, 300, 2011, 2232, 1670, 550, 437, 286, 437, 286, 478, 286, 818, 264, 264, 1974, 1105, 370, 286, 500, 380, 362, 51276], "temperature": 0.0, "avg_logprob": -0.10524677568011814, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.003971994388848543}, {"id": 220, "seek": 162400, "start": 1642.24, "end": 1648.08, "text": " enough time to focus on everything but I choose three particular uh aspects that I'm going to to", "tokens": [51276, 1547, 565, 281, 1879, 322, 1203, 457, 286, 2826, 1045, 1729, 2232, 7270, 300, 286, 478, 516, 281, 281, 51568], "temperature": 0.0, "avg_logprob": -0.10524677568011814, "compression_ratio": 1.548913043478261, "no_speech_prob": 0.003971994388848543}, {"id": 221, "seek": 164808, "start": 1648.08, "end": 1657.28, "text": " show which help us also thinking a bit further uh into the steps that we want to uh to take afterwards", "tokens": [50364, 855, 597, 854, 505, 611, 1953, 257, 857, 3052, 2232, 666, 264, 4439, 300, 321, 528, 281, 2232, 281, 747, 10543, 50824], "temperature": 0.0, "avg_logprob": -0.06748779614766438, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.009965574368834496}, {"id": 222, "seek": 164808, "start": 1658.0, "end": 1664.48, "text": " the very first thing of course is the rise of deep learning so when uh when uh we started the", "tokens": [50860, 264, 588, 700, 551, 295, 1164, 307, 264, 6272, 295, 2452, 2539, 370, 562, 2232, 562, 2232, 321, 1409, 264, 51184], "temperature": 0.0, "avg_logprob": -0.06748779614766438, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.009965574368834496}, {"id": 223, "seek": 164808, "start": 1664.48, "end": 1668.6399999999999, "text": " the work on explanations and knowledge graphs actually on knowledge graphs for explanations", "tokens": [51184, 264, 589, 322, 28708, 293, 3601, 24877, 767, 322, 3601, 24877, 337, 28708, 51392], "temperature": 0.0, "avg_logprob": -0.06748779614766438, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.009965574368834496}, {"id": 224, "seek": 164808, "start": 1669.6, "end": 1675.1999999999998, "text": " deep learning was just was probably just booming and I wasn't even aware of that so what we were", "tokens": [51440, 2452, 2539, 390, 445, 390, 1391, 445, 45883, 293, 286, 2067, 380, 754, 3650, 295, 300, 370, 437, 321, 645, 51720], "temperature": 0.0, "avg_logprob": -0.06748779614766438, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.009965574368834496}, {"id": 225, "seek": 167520, "start": 1675.2, "end": 1682.24, "text": " talking about was uh using knowledge graphs to um generate explanation for outputs of any", "tokens": [50364, 1417, 466, 390, 2232, 1228, 3601, 24877, 281, 1105, 8460, 10835, 337, 23930, 295, 604, 50716], "temperature": 0.0, "avg_logprob": -0.13269948202466209, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.003762960433959961}, {"id": 226, "seek": 167520, "start": 1682.24, "end": 1688.64, "text": " machine learning algorithm uh but we've never heard of deep learning I think that yeah maybe the the", "tokens": [50716, 3479, 2539, 9284, 2232, 457, 321, 600, 1128, 2198, 295, 2452, 2539, 286, 519, 300, 1338, 1310, 264, 264, 51036], "temperature": 0.0, "avg_logprob": -0.13269948202466209, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.003762960433959961}, {"id": 227, "seek": 167520, "start": 1689.3600000000001, "end": 1697.04, "text": " the the first papers are around 2012 but if I'm not mistaken but um of course the the hype came", "tokens": [51072, 264, 264, 700, 10577, 366, 926, 9125, 457, 498, 286, 478, 406, 21333, 457, 1105, 295, 1164, 264, 264, 24144, 1361, 51456], "temperature": 0.0, "avg_logprob": -0.13269948202466209, "compression_ratio": 1.5543478260869565, "no_speech_prob": 0.003762960433959961}, {"id": 228, "seek": 169704, "start": 1697.04, "end": 1707.52, "text": " much later and um uh the the the DARPA uh so so deep learning and all the methods behind we", "tokens": [50364, 709, 1780, 293, 1105, 2232, 264, 264, 264, 49274, 10297, 2232, 370, 370, 2452, 2539, 293, 439, 264, 7150, 2261, 321, 50888], "temperature": 0.0, "avg_logprob": -0.2195698102315267, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.028289614245295525}, {"id": 229, "seek": 169704, "start": 1707.52, "end": 1712.96, "text": " that came with deep learning showed uh impressive results that could achieve uh", "tokens": [50888, 300, 1361, 365, 2452, 2539, 4712, 2232, 8992, 3542, 300, 727, 4584, 2232, 51160], "temperature": 0.0, "avg_logprob": -0.2195698102315267, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.028289614245295525}, {"id": 230, "seek": 169704, "start": 1715.12, "end": 1720.96, "text": " the same results as human would and uh but but also showed a number of limitations so the", "tokens": [51268, 264, 912, 3542, 382, 1952, 576, 293, 2232, 457, 457, 611, 4712, 257, 1230, 295, 15705, 370, 264, 51560], "temperature": 0.0, "avg_logprob": -0.2195698102315267, "compression_ratio": 1.6624203821656052, "no_speech_prob": 0.028289614245295525}, {"id": 231, "seek": 172096, "start": 1721.6000000000001, "end": 1728.64, "text": " uh the the the facts like the Cambridge Analytica scandals or the the loan accreditation scandals", "tokens": [50396, 2232, 264, 264, 264, 9130, 411, 264, 24876, 23688, 2262, 40273, 1124, 420, 264, 264, 10529, 33877, 4614, 40273, 1124, 50748], "temperature": 0.0, "avg_logprob": -0.18113933520370654, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.005202379543334246}, {"id": 232, "seek": 172096, "start": 1729.68, "end": 1736.4, "text": " show that these systems were uh were not able to to show a clear reasoning so the reasoning was", "tokens": [50800, 855, 300, 613, 3652, 645, 2232, 645, 406, 1075, 281, 281, 855, 257, 1850, 21577, 370, 264, 21577, 390, 51136], "temperature": 0.0, "avg_logprob": -0.18113933520370654, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.005202379543334246}, {"id": 233, "seek": 172096, "start": 1736.4, "end": 1742.16, "text": " quite opaque uh these systems were data hungry so you needed a lot of data to train them they were", "tokens": [51136, 1596, 42687, 2232, 613, 3652, 645, 1412, 8067, 370, 291, 2978, 257, 688, 295, 1412, 281, 3847, 552, 436, 645, 51424], "temperature": 0.0, "avg_logprob": -0.18113933520370654, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.005202379543334246}, {"id": 234, "seek": 172096, "start": 1742.16, "end": 1748.88, "text": " too brittle in this sense and then DARPA came out with this explainable AI program that was 2017", "tokens": [51424, 886, 49325, 294, 341, 2020, 293, 550, 49274, 10297, 1361, 484, 365, 341, 2903, 712, 7318, 1461, 300, 390, 6591, 51760], "temperature": 0.0, "avg_logprob": -0.18113933520370654, "compression_ratio": 1.7136563876651982, "no_speech_prob": 0.005202379543334246}, {"id": 235, "seek": 174888, "start": 1749.2800000000002, "end": 1757.8400000000001, "text": " if not if I'm not mistaken um on okay let's try to implement create systems where the user is", "tokens": [50384, 498, 406, 498, 286, 478, 406, 21333, 1105, 322, 1392, 718, 311, 853, 281, 4445, 1884, 3652, 689, 264, 4195, 307, 50812], "temperature": 0.0, "avg_logprob": -0.16201183713715653, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0039459350518882275}, {"id": 236, "seek": 174888, "start": 1757.8400000000001, "end": 1764.8000000000002, "text": " that are transparent that can explain the machine learning model and then where uh an explanation", "tokens": [50812, 300, 366, 12737, 300, 393, 2903, 264, 3479, 2539, 2316, 293, 550, 689, 2232, 364, 10835, 51160], "temperature": 0.0, "avg_logprob": -0.16201183713715653, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0039459350518882275}, {"id": 237, "seek": 174888, "start": 1764.8000000000002, "end": 1770.88, "text": " can be given on why a certain output is being given uh so this need of explaining the models", "tokens": [51160, 393, 312, 2212, 322, 983, 257, 1629, 5598, 307, 885, 2212, 2232, 370, 341, 643, 295, 13468, 264, 5245, 51464], "temperature": 0.0, "avg_logprob": -0.16201183713715653, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0039459350518882275}, {"id": 238, "seek": 174888, "start": 1770.88, "end": 1776.88, "text": " and the results really came out and you start we started seeing methods like SHOP and LIME which", "tokens": [51464, 293, 264, 3542, 534, 1361, 484, 293, 291, 722, 321, 1409, 2577, 7150, 411, 7405, 12059, 293, 441, 6324, 36, 597, 51764], "temperature": 0.0, "avg_logprob": -0.16201183713715653, "compression_ratio": 1.6493506493506493, "no_speech_prob": 0.0039459350518882275}, {"id": 239, "seek": 177688, "start": 1776.88, "end": 1782.3200000000002, "text": " are probably the most basic ones and then there's there's a number of other systems that came out", "tokens": [50364, 366, 1391, 264, 881, 3875, 2306, 293, 550, 456, 311, 456, 311, 257, 1230, 295, 661, 3652, 300, 1361, 484, 50636], "temperature": 0.0, "avg_logprob": -0.1753947235817133, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0035866007674485445}, {"id": 240, "seek": 177688, "start": 1782.3200000000002, "end": 1788.64, "text": " afterwards uh that could tell you okay the the most important features to come up with an explanation", "tokens": [50636, 10543, 2232, 300, 727, 980, 291, 1392, 264, 264, 881, 1021, 4122, 281, 808, 493, 365, 364, 10835, 50952], "temperature": 0.0, "avg_logprob": -0.1753947235817133, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0035866007674485445}, {"id": 241, "seek": 177688, "start": 1789.68, "end": 1799.44, "text": " and more in your data set are uh like maybe the race or the occupation of of of of your data", "tokens": [51004, 293, 544, 294, 428, 1412, 992, 366, 2232, 411, 1310, 264, 4569, 420, 264, 24482, 295, 295, 295, 295, 428, 1412, 51492], "temperature": 0.0, "avg_logprob": -0.1753947235817133, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0035866007674485445}, {"id": 242, "seek": 177688, "start": 1800.16, "end": 1806.3200000000002, "text": " or we started seeing saliency maps okay with with imagery recognition uh what are the most", "tokens": [51528, 420, 321, 1409, 2577, 1845, 7848, 11317, 1392, 365, 365, 24340, 11150, 2232, 437, 366, 264, 881, 51836], "temperature": 0.0, "avg_logprob": -0.1753947235817133, "compression_ratio": 1.7731481481481481, "no_speech_prob": 0.0035866007674485445}, {"id": 243, "seek": 180632, "start": 1806.3999999999999, "end": 1816.08, "text": " important parts that that system focuses on when generating explanations uh this especially the", "tokens": [50368, 1021, 3166, 300, 300, 1185, 16109, 322, 562, 17746, 28708, 2232, 341, 2318, 264, 50852], "temperature": 0.0, "avg_logprob": -0.12341398662990993, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.005437012296169996}, {"id": 244, "seek": 180632, "start": 1816.08, "end": 1822.8799999999999, "text": " explanations in in in the explainable AI area and if you look into the major AI conference you", "tokens": [50852, 28708, 294, 294, 294, 264, 2903, 712, 7318, 1859, 293, 498, 291, 574, 666, 264, 2563, 7318, 7586, 291, 51192], "temperature": 0.0, "avg_logprob": -0.12341398662990993, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.005437012296169996}, {"id": 245, "seek": 180632, "start": 1822.8799999999999, "end": 1829.9199999999998, "text": " start seeing a boom of uh I think explainable AI became an actual topic or a subfield in in AI", "tokens": [51192, 722, 2577, 257, 9351, 295, 2232, 286, 519, 2903, 712, 7318, 3062, 364, 3539, 4829, 420, 257, 1422, 7610, 294, 294, 7318, 51544], "temperature": 0.0, "avg_logprob": -0.12341398662990993, "compression_ratio": 1.6569767441860466, "no_speech_prob": 0.005437012296169996}, {"id": 246, "seek": 182992, "start": 1830.5600000000002, "end": 1836.8000000000002, "text": " uh one of the questions that people asked was okay uh are these explanations that SHOP and LIME", "tokens": [50396, 2232, 472, 295, 264, 1651, 300, 561, 2351, 390, 1392, 2232, 366, 613, 28708, 300, 7405, 12059, 293, 441, 6324, 36, 50708], "temperature": 0.0, "avg_logprob": -0.16038077218191965, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.005549988709390163}, {"id": 247, "seek": 182992, "start": 1836.8000000000002, "end": 1843.8400000000001, "text": " come up with are they really working especially in a real-world context so they do work in in a", "tokens": [50708, 808, 493, 365, 366, 436, 534, 1364, 2318, 294, 257, 957, 12, 13217, 4319, 370, 436, 360, 589, 294, 294, 257, 51060], "temperature": 0.0, "avg_logprob": -0.16038077218191965, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.005549988709390163}, {"id": 248, "seek": 182992, "start": 1843.8400000000001, "end": 1850.5600000000002, "text": " small use case but what if I applied into into a real-world context uh and somehow the narrow", "tokens": [51060, 1359, 764, 1389, 457, 437, 498, 286, 6456, 666, 666, 257, 957, 12, 13217, 4319, 2232, 293, 6063, 264, 9432, 51396], "temperature": 0.0, "avg_logprob": -0.16038077218191965, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.005549988709390163}, {"id": 249, "seek": 182992, "start": 1850.5600000000002, "end": 1857.8400000000001, "text": " symbolic field uh which was already in in the meantime in developing on on his own came up a", "tokens": [51396, 25755, 2519, 2232, 597, 390, 1217, 294, 294, 264, 14991, 294, 6416, 322, 322, 702, 1065, 1361, 493, 257, 51760], "temperature": 0.0, "avg_logprob": -0.16038077218191965, "compression_ratio": 1.6578947368421053, "no_speech_prob": 0.005549988709390163}, {"id": 250, "seek": 185784, "start": 1857.84, "end": 1865.6, "text": " bit in the rescue of this problem so somehow we started seeing methods that try to combine", "tokens": [50364, 857, 294, 264, 13283, 295, 341, 1154, 370, 6063, 321, 1409, 2577, 7150, 300, 853, 281, 10432, 50752], "temperature": 0.0, "avg_logprob": -0.1322537723340486, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.0024877721443772316}, {"id": 251, "seek": 185784, "start": 1866.1599999999999, "end": 1873.36, "text": " a symbolic approach so symbolic reasoning uh with neural network either to uh maybe improve", "tokens": [50780, 257, 25755, 3109, 370, 25755, 21577, 2232, 365, 18161, 3209, 2139, 281, 2232, 1310, 3470, 51140], "temperature": 0.0, "avg_logprob": -0.1322537723340486, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.0024877721443772316}, {"id": 252, "seek": 185784, "start": 1873.36, "end": 1880.56, "text": " the explainability and trust of a system using a symbolic description uh or by creating a sort", "tokens": [51140, 264, 2903, 2310, 293, 3361, 295, 257, 1185, 1228, 257, 25755, 3855, 2232, 420, 538, 4084, 257, 1333, 51500], "temperature": 0.0, "avg_logprob": -0.1322537723340486, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.0024877721443772316}, {"id": 253, "seek": 185784, "start": 1880.56, "end": 1885.84, "text": " of hybrid interaction between the the neural network and and the reasoning system uh so these", "tokens": [51500, 295, 13051, 9285, 1296, 264, 264, 18161, 3209, 293, 293, 264, 21577, 1185, 2232, 370, 613, 51764], "temperature": 0.0, "avg_logprob": -0.1322537723340486, "compression_ratio": 1.7751196172248804, "no_speech_prob": 0.0024877721443772316}, {"id": 254, "seek": 188584, "start": 1885.84, "end": 1891.6, "text": " are just two of the many examples that you could could see when uh where the knowledge graph would", "tokens": [50364, 366, 445, 732, 295, 264, 867, 5110, 300, 291, 727, 727, 536, 562, 2232, 689, 264, 3601, 4295, 576, 50652], "temperature": 0.0, "avg_logprob": -0.13116504333831452, "compression_ratio": 1.75, "no_speech_prob": 0.0017023311229422688}, {"id": 255, "seek": 188584, "start": 1891.6, "end": 1897.4399999999998, "text": " try to jump in into the explainable AI word and say okay hey look we should be maybe using", "tokens": [50652, 853, 281, 3012, 294, 666, 264, 2903, 712, 7318, 1349, 293, 584, 1392, 4177, 574, 321, 820, 312, 1310, 1228, 50944], "temperature": 0.0, "avg_logprob": -0.13116504333831452, "compression_ratio": 1.75, "no_speech_prob": 0.0017023311229422688}, {"id": 256, "seek": 188584, "start": 1897.4399999999998, "end": 1905.52, "text": " knowledge graphs and ontologies to help uh we also did a part of this uh so what what we try to do um", "tokens": [50944, 3601, 24877, 293, 6592, 6204, 281, 854, 2232, 321, 611, 630, 257, 644, 295, 341, 2232, 370, 437, 437, 321, 853, 281, 360, 1105, 51348], "temperature": 0.0, "avg_logprob": -0.13116504333831452, "compression_ratio": 1.75, "no_speech_prob": 0.0017023311229422688}, {"id": 257, "seek": 188584, "start": 1905.52, "end": 1913.04, "text": " and um uh this this I completely forgot the the uh this the reference to this work but", "tokens": [51348, 293, 1105, 2232, 341, 341, 286, 2584, 5298, 264, 264, 2232, 341, 264, 6408, 281, 341, 589, 457, 51724], "temperature": 0.0, "avg_logprob": -0.13116504333831452, "compression_ratio": 1.75, "no_speech_prob": 0.0017023311229422688}, {"id": 258, "seek": 191304, "start": 1913.04, "end": 1920.3999999999999, "text": " if we published in 2021 uh we really try to say okay if everybody if many people are looking into", "tokens": [50364, 498, 321, 6572, 294, 7201, 2232, 321, 534, 853, 281, 584, 1392, 498, 2201, 498, 867, 561, 366, 1237, 666, 50732], "temperature": 0.0, "avg_logprob": -0.09254219365674396, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.0038059023208916187}, {"id": 259, "seek": 191304, "start": 1920.3999999999999, "end": 1929.44, "text": " using knowledge graphs as a as a tool to explain um machine learning methods let's try to look at", "tokens": [50732, 1228, 3601, 24877, 382, 257, 382, 257, 2290, 281, 2903, 1105, 3479, 2539, 7150, 718, 311, 853, 281, 574, 412, 51184], "temperature": 0.0, "avg_logprob": -0.09254219365674396, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.0038059023208916187}, {"id": 260, "seek": 191304, "start": 1929.44, "end": 1936.1599999999999, "text": " what's the state of the art so uh how are people in machine learning using knowledge graphs what are", "tokens": [51184, 437, 311, 264, 1785, 295, 264, 1523, 370, 2232, 577, 366, 561, 294, 3479, 2539, 1228, 3601, 24877, 437, 366, 51520], "temperature": 0.0, "avg_logprob": -0.09254219365674396, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.0038059023208916187}, {"id": 261, "seek": 191304, "start": 1936.1599999999999, "end": 1942.72, "text": " the the most important characteristics so we try to look into different uh tasks and different areas", "tokens": [51520, 264, 264, 881, 1021, 10891, 370, 321, 853, 281, 574, 666, 819, 2232, 9608, 293, 819, 3179, 51848], "temperature": 0.0, "avg_logprob": -0.09254219365674396, "compression_ratio": 1.812785388127854, "no_speech_prob": 0.0038059023208916187}, {"id": 262, "seek": 194304, "start": 1943.2, "end": 1947.68, "text": " and uh we try to look into the characteristics of the knowledge graphs the characteristics of", "tokens": [50372, 293, 2232, 321, 853, 281, 574, 666, 264, 10891, 295, 264, 3601, 24877, 264, 10891, 295, 50596], "temperature": 0.0, "avg_logprob": -0.0827401065826416, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.0013519206549972296}, {"id": 263, "seek": 194304, "start": 1947.68, "end": 1952.96, "text": " the model and the characteristics of the explanations that we were being generating uh", "tokens": [50596, 264, 2316, 293, 264, 10891, 295, 264, 28708, 300, 321, 645, 885, 17746, 2232, 50860], "temperature": 0.0, "avg_logprob": -0.0827401065826416, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.0013519206549972296}, {"id": 264, "seek": 194304, "start": 1952.96, "end": 1958.0, "text": " in order to come up with really with a with a picture of the field uh and this this is more", "tokens": [50860, 294, 1668, 281, 808, 493, 365, 534, 365, 257, 365, 257, 3036, 295, 264, 2519, 2232, 293, 341, 341, 307, 544, 51112], "temperature": 0.0, "avg_logprob": -0.0827401065826416, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.0013519206549972296}, {"id": 265, "seek": 194304, "start": 1958.0, "end": 1965.12, "text": " or less what we came up with so uh we we learned certain things like if you are dealing with tasks", "tokens": [51112, 420, 1570, 437, 321, 1361, 493, 365, 370, 2232, 321, 321, 3264, 1629, 721, 411, 498, 291, 366, 6260, 365, 9608, 51468], "temperature": 0.0, "avg_logprob": -0.0827401065826416, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.0013519206549972296}, {"id": 266, "seek": 194304, "start": 1965.12, "end": 1970.56, "text": " for recognition and recommendation uh most of the explanation you will get are really about the", "tokens": [51468, 337, 11150, 293, 11879, 2232, 881, 295, 264, 10835, 291, 486, 483, 366, 534, 466, 264, 51740], "temperature": 0.0, "avg_logprob": -0.0827401065826416, "compression_ratio": 2.0129310344827585, "no_speech_prob": 0.0013519206549972296}, {"id": 267, "seek": 197056, "start": 1970.56, "end": 1976.96, "text": " model and how it behaves and most of the information that is being used uh from the knowledge graph is", "tokens": [50364, 2316, 293, 577, 309, 36896, 293, 881, 295, 264, 1589, 300, 307, 885, 1143, 2232, 490, 264, 3601, 4295, 307, 50684], "temperature": 0.0, "avg_logprob": -0.10588544890994117, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0029061438981443644}, {"id": 268, "seek": 197056, "start": 1976.96, "end": 1985.52, "text": " the is the a is the aversion box uh and whenever uh we deal with tasks that involve the interaction", "tokens": [50684, 264, 307, 264, 257, 307, 264, 257, 29153, 2424, 2232, 293, 5699, 2232, 321, 2028, 365, 9608, 300, 9494, 264, 9285, 51112], "temperature": 0.0, "avg_logprob": -0.10588544890994117, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0029061438981443644}, {"id": 269, "seek": 197056, "start": 1985.52, "end": 1991.2, "text": " of the user like conversational agents or recommender systems the knowledge graph information is", "tokens": [51112, 295, 264, 4195, 411, 2615, 1478, 12554, 420, 2748, 260, 3652, 264, 3601, 4295, 1589, 307, 51396], "temperature": 0.0, "avg_logprob": -0.10588544890994117, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0029061438981443644}, {"id": 270, "seek": 197056, "start": 1991.2, "end": 1997.6799999999998, "text": " used to is used more into the training of the model uh to generate a certain explanation", "tokens": [51396, 1143, 281, 307, 1143, 544, 666, 264, 3097, 295, 264, 2316, 2232, 281, 8460, 257, 1629, 10835, 51720], "temperature": 0.0, "avg_logprob": -0.10588544890994117, "compression_ratio": 1.8653846153846154, "no_speech_prob": 0.0029061438981443644}, {"id": 271, "seek": 199768, "start": 1997.76, "end": 2004.72, "text": " rather than as a postdoc stack uh we looked into the different types of knowledge graphs whether", "tokens": [50368, 2831, 813, 382, 257, 2183, 39966, 8630, 2232, 321, 2956, 666, 264, 819, 3467, 295, 3601, 24877, 1968, 50716], "temperature": 0.0, "avg_logprob": -0.11050081856643097, "compression_ratio": 1.891089108910891, "no_speech_prob": 0.003337996080517769}, {"id": 272, "seek": 199768, "start": 2004.72, "end": 2010.8, "text": " they were factual or common sense or domain knowledge graphs domain specific and it turns", "tokens": [50716, 436, 645, 48029, 420, 2689, 2020, 420, 9274, 3601, 24877, 9274, 2685, 293, 309, 4523, 51020], "temperature": 0.0, "avg_logprob": -0.11050081856643097, "compression_ratio": 1.891089108910891, "no_speech_prob": 0.003337996080517769}, {"id": 273, "seek": 199768, "start": 2010.8, "end": 2015.76, "text": " out that common sense knowledge graphs they're not that many but they were being used for image", "tokens": [51020, 484, 300, 2689, 2020, 3601, 24877, 436, 434, 406, 300, 867, 457, 436, 645, 885, 1143, 337, 3256, 51268], "temperature": 0.0, "avg_logprob": -0.11050081856643097, "compression_ratio": 1.891089108910891, "no_speech_prob": 0.003337996080517769}, {"id": 274, "seek": 199768, "start": 2015.76, "end": 2021.44, "text": " recognition and question answering uh we also look into the reuse of knowledge graphs so we've been", "tokens": [51268, 11150, 293, 1168, 13430, 2232, 321, 611, 574, 666, 264, 26225, 295, 3601, 24877, 370, 321, 600, 668, 51552], "temperature": 0.0, "avg_logprob": -0.11050081856643097, "compression_ratio": 1.891089108910891, "no_speech_prob": 0.003337996080517769}, {"id": 275, "seek": 202144, "start": 2021.52, "end": 2027.92, "text": " uh talking so much in our field about reusing knowledge graph reusing ontology ontologies and", "tokens": [50368, 2232, 1417, 370, 709, 294, 527, 2519, 466, 319, 7981, 3601, 4295, 319, 7981, 6592, 1793, 6592, 6204, 293, 50688], "temperature": 0.0, "avg_logprob": -0.17029408046177455, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.008736321702599525}, {"id": 276, "seek": 202144, "start": 2027.92, "end": 2032.64, "text": " we kind of ask okay is this being applied in this field and it actually turned out it was", "tokens": [50688, 321, 733, 295, 1029, 1392, 307, 341, 885, 6456, 294, 341, 2519, 293, 309, 767, 3574, 484, 309, 390, 50924], "temperature": 0.0, "avg_logprob": -0.17029408046177455, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.008736321702599525}, {"id": 277, "seek": 202144, "start": 2032.64, "end": 2037.92, "text": " quite an established practice so very few were coming up with their own knowledge graph uh most", "tokens": [50924, 1596, 364, 7545, 3124, 370, 588, 1326, 645, 1348, 493, 365, 641, 1065, 3601, 4295, 2232, 881, 51188], "temperature": 0.0, "avg_logprob": -0.17029408046177455, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.008736321702599525}, {"id": 278, "seek": 202144, "start": 2037.92, "end": 2046.3200000000002, "text": " of the the methods were were using uh dbpdia uh wikidata um consonants or a combination of them", "tokens": [51188, 295, 264, 264, 7150, 645, 645, 1228, 2232, 274, 65, 79, 67, 654, 2232, 261, 1035, 327, 3274, 1105, 30843, 1719, 420, 257, 6562, 295, 552, 51608], "temperature": 0.0, "avg_logprob": -0.17029408046177455, "compression_ratio": 1.7361111111111112, "no_speech_prob": 0.008736321702599525}, {"id": 279, "seek": 204632, "start": 2046.8799999999999, "end": 2053.68, "text": " um actually we looked into uh whether these methods were using only one knowledge graph", "tokens": [50392, 1105, 767, 321, 2956, 666, 2232, 1968, 613, 7150, 645, 1228, 787, 472, 3601, 4295, 50732], "temperature": 0.0, "avg_logprob": -0.11239234510674534, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.0016422264743596315}, {"id": 280, "seek": 204632, "start": 2053.68, "end": 2060.16, "text": " or multiple ones and we we were hoping to to see a bit more but it does happen sometimes in", "tokens": [50732, 420, 3866, 2306, 293, 321, 321, 645, 7159, 281, 281, 536, 257, 857, 544, 457, 309, 775, 1051, 2171, 294, 51056], "temperature": 0.0, "avg_logprob": -0.11239234510674534, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.0016422264743596315}, {"id": 281, "seek": 204632, "start": 2060.16, "end": 2066.96, "text": " nlp tasks and really the kind of this is the kind of picture that came out so if you if certain", "tokens": [51056, 297, 75, 79, 9608, 293, 534, 264, 733, 295, 341, 307, 264, 733, 295, 3036, 300, 1361, 484, 370, 498, 291, 498, 1629, 51396], "temperature": 0.0, "avg_logprob": -0.11239234510674534, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.0016422264743596315}, {"id": 282, "seek": 204632, "start": 2066.96, "end": 2072.24, "text": " areas are more focused on on model embedded knowledge so explaining the model rather than", "tokens": [51396, 3179, 366, 544, 5178, 322, 322, 2316, 16741, 3601, 370, 13468, 264, 2316, 2831, 813, 51660], "temperature": 0.0, "avg_logprob": -0.11239234510674534, "compression_ratio": 1.7298578199052133, "no_speech_prob": 0.0016422264743596315}, {"id": 283, "seek": 207224, "start": 2072.24, "end": 2079.7599999999998, "text": " explaining the outputs certain others are more focused on using the ontologies of the", "tokens": [50364, 13468, 264, 23930, 1629, 2357, 366, 544, 5178, 322, 1228, 264, 6592, 6204, 295, 264, 50740], "temperature": 0.0, "avg_logprob": -0.10901786541116648, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0035755247808992863}, {"id": 284, "seek": 207224, "start": 2079.7599999999998, "end": 2086.64, "text": " knowledge graph rather than the facts um and um and so on and so forth so I think the analysis we", "tokens": [50740, 3601, 4295, 2831, 813, 264, 9130, 1105, 293, 1105, 293, 370, 322, 293, 370, 5220, 370, 286, 519, 264, 5215, 321, 51084], "temperature": 0.0, "avg_logprob": -0.10901786541116648, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0035755247808992863}, {"id": 285, "seek": 207224, "start": 2086.64, "end": 2093.9199999999996, "text": " did it's around 60 60 papers more or less um and we kind of try to identify also the challenges", "tokens": [51084, 630, 309, 311, 926, 4060, 4060, 10577, 544, 420, 1570, 1105, 293, 321, 733, 295, 853, 281, 5876, 611, 264, 4759, 51448], "temperature": 0.0, "avg_logprob": -0.10901786541116648, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0035755247808992863}, {"id": 286, "seek": 207224, "start": 2093.9199999999996, "end": 2100.4799999999996, "text": " for the field right so there are things that were we were hoping to see but that didn't happen", "tokens": [51448, 337, 264, 2519, 558, 370, 456, 366, 721, 300, 645, 321, 645, 7159, 281, 536, 457, 300, 994, 380, 1051, 51776], "temperature": 0.0, "avg_logprob": -0.10901786541116648, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0035755247808992863}, {"id": 287, "seek": 210048, "start": 2100.56, "end": 2105.04, "text": " including the what we call the co-creation of explanations so really having a sort", "tokens": [50368, 3009, 264, 437, 321, 818, 264, 598, 12, 14066, 399, 295, 28708, 370, 534, 1419, 257, 1333, 50592], "temperature": 0.0, "avg_logprob": -0.13098498026529948, "compression_ratio": 1.8429319371727748, "no_speech_prob": 0.0035291172098368406}, {"id": 288, "seek": 210048, "start": 2107.44, "end": 2112.56, "text": " the human having a role into the generation of the explanation there are there were there are", "tokens": [50712, 264, 1952, 1419, 257, 3090, 666, 264, 5125, 295, 264, 10835, 456, 366, 456, 645, 456, 366, 50968], "temperature": 0.0, "avg_logprob": -0.13098498026529948, "compression_ratio": 1.8429319371727748, "no_speech_prob": 0.0035291172098368406}, {"id": 289, "seek": 210048, "start": 2112.56, "end": 2118.4, "text": " issues related to the maintenance of the knowledge graph so how to deal with uh", "tokens": [50968, 2663, 4077, 281, 264, 11258, 295, 264, 3601, 4295, 370, 577, 281, 2028, 365, 2232, 51260], "temperature": 0.0, "avg_logprob": -0.13098498026529948, "compression_ratio": 1.8429319371727748, "no_speech_prob": 0.0035291172098368406}, {"id": 290, "seek": 210048, "start": 2118.96, "end": 2125.76, "text": " missing information or bias when coming up when generating explanations and of course this is a", "tokens": [51288, 5361, 1589, 420, 12577, 562, 1348, 493, 562, 17746, 28708, 293, 295, 1164, 341, 307, 257, 51628], "temperature": 0.0, "avg_logprob": -0.13098498026529948, "compression_ratio": 1.8429319371727748, "no_speech_prob": 0.0035291172098368406}, {"id": 291, "seek": 212576, "start": 2125.76, "end": 2131.76, "text": " problem that we also saw during in the in the phd and also there are issues related to the", "tokens": [50364, 1154, 300, 321, 611, 1866, 1830, 294, 264, 294, 264, 903, 67, 293, 611, 456, 366, 2663, 4077, 281, 264, 50664], "temperature": 0.0, "avg_logprob": -0.08282570540904999, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.004353745374828577}, {"id": 292, "seek": 212576, "start": 2131.76, "end": 2137.5200000000004, "text": " automated extraction of relevant knowledge when using a knowledge graph that generates explanations", "tokens": [50664, 18473, 30197, 295, 7340, 3601, 562, 1228, 257, 3601, 4295, 300, 23815, 28708, 50952], "temperature": 0.0, "avg_logprob": -0.08282570540904999, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.004353745374828577}, {"id": 293, "seek": 212576, "start": 2137.5200000000004, "end": 2144.5600000000004, "text": " so most of the methods that we analyzed when coming up with an explanation end up manually", "tokens": [50952, 370, 881, 295, 264, 7150, 300, 321, 28181, 562, 1348, 493, 365, 364, 10835, 917, 493, 16945, 51304], "temperature": 0.0, "avg_logprob": -0.08282570540904999, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.004353745374828577}, {"id": 294, "seek": 212576, "start": 2144.5600000000004, "end": 2150.48, "text": " selecting uh the relevant relevant relevant information to generate an explanation and", "tokens": [51304, 18182, 2232, 264, 7340, 7340, 7340, 1589, 281, 8460, 364, 10835, 293, 51600], "temperature": 0.0, "avg_logprob": -0.08282570540904999, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.004353745374828577}, {"id": 295, "seek": 212576, "start": 2150.48, "end": 2154.6400000000003, "text": " this is quite something it means that there's still a lot to do in the in the field in order to", "tokens": [51600, 341, 307, 1596, 746, 309, 1355, 300, 456, 311, 920, 257, 688, 281, 360, 294, 264, 294, 264, 2519, 294, 1668, 281, 51808], "temperature": 0.0, "avg_logprob": -0.08282570540904999, "compression_ratio": 1.9333333333333333, "no_speech_prob": 0.004353745374828577}, {"id": 296, "seek": 215464, "start": 2155.52, "end": 2166.0, "text": " um uh to move forward um there's uh so this was one part of the story so what happened", "tokens": [50408, 1105, 2232, 281, 1286, 2128, 1105, 456, 311, 2232, 370, 341, 390, 472, 644, 295, 264, 1657, 370, 437, 2011, 50932], "temperature": 0.0, "avg_logprob": -0.14346469394744388, "compression_ratio": 1.6459627329192548, "no_speech_prob": 0.0026402813382446766}, {"id": 297, "seek": 215464, "start": 2166.0, "end": 2171.44, "text": " ever since deep learning uh there's also the field of hybrid intelligence that came up so", "tokens": [50932, 1562, 1670, 2452, 2539, 2232, 456, 311, 611, 264, 2519, 295, 13051, 7599, 300, 1361, 493, 370, 51204], "temperature": 0.0, "avg_logprob": -0.14346469394744388, "compression_ratio": 1.6459627329192548, "no_speech_prob": 0.0026402813382446766}, {"id": 298, "seek": 215464, "start": 2171.44, "end": 2177.2799999999997, "text": " I don't know if many of you heard about uh hybrid intelligence maybe you had about human", "tokens": [51204, 286, 500, 380, 458, 498, 867, 295, 291, 2198, 466, 2232, 13051, 7599, 1310, 291, 632, 466, 1952, 51496], "temperature": 0.0, "avg_logprob": -0.14346469394744388, "compression_ratio": 1.6459627329192548, "no_speech_prob": 0.0026402813382446766}, {"id": 299, "seek": 217728, "start": 2177.28, "end": 2183.6000000000004, "text": " centric ai this is another way of addressing this this problem uh the overall idea was that", "tokens": [50364, 1489, 1341, 9783, 341, 307, 1071, 636, 295, 14329, 341, 341, 1154, 2232, 264, 4787, 1558, 390, 300, 50680], "temperature": 0.0, "avg_logprob": -0.11863083717150566, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.021600067615509033}, {"id": 300, "seek": 217728, "start": 2184.96, "end": 2190.32, "text": " this there is an emerging field in ai which uh and it's emerging because you start seeing", "tokens": [50748, 341, 456, 307, 364, 14989, 2519, 294, 9783, 597, 2232, 293, 309, 311, 14989, 570, 291, 722, 2577, 51016], "temperature": 0.0, "avg_logprob": -0.11863083717150566, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.021600067615509033}, {"id": 301, "seek": 217728, "start": 2190.32, "end": 2196.7200000000003, "text": " different conferences and workshops uh around the topic there's a number of uh national and", "tokens": [51016, 819, 22032, 293, 19162, 2232, 926, 264, 4829, 456, 311, 257, 1230, 295, 2232, 4048, 293, 51336], "temperature": 0.0, "avg_logprob": -0.11863083717150566, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.021600067615509033}, {"id": 302, "seek": 217728, "start": 2196.7200000000003, "end": 2205.36, "text": " international um collaboration networks uh that uh that are really focusing on uh on this concept", "tokens": [51336, 5058, 1105, 9363, 9590, 2232, 300, 2232, 300, 366, 534, 8416, 322, 2232, 322, 341, 3410, 51768], "temperature": 0.0, "avg_logprob": -0.11863083717150566, "compression_ratio": 1.7582938388625593, "no_speech_prob": 0.021600067615509033}, {"id": 303, "seek": 220536, "start": 2205.36, "end": 2211.04, "text": " of hybrid intelligence and and the overall idea because we don't really have a proper definition", "tokens": [50364, 295, 13051, 7599, 293, 293, 264, 4787, 1558, 570, 321, 500, 380, 534, 362, 257, 2296, 7123, 50648], "temperature": 0.0, "avg_logprob": -0.07625282006185563, "compression_ratio": 1.6628571428571428, "no_speech_prob": 0.0060978662222623825}, {"id": 304, "seek": 220536, "start": 2211.04, "end": 2218.96, "text": " but we do have a working definition of hybrid intelligence is to have uh to to aim for ai systems", "tokens": [50648, 457, 321, 360, 362, 257, 1364, 7123, 295, 13051, 7599, 307, 281, 362, 2232, 281, 281, 5939, 337, 9783, 3652, 51044], "temperature": 0.0, "avg_logprob": -0.07625282006185563, "compression_ratio": 1.6628571428571428, "no_speech_prob": 0.0060978662222623825}, {"id": 305, "seek": 220536, "start": 2218.96, "end": 2226.4, "text": " that try to enhance human capabilities as other scientific tools would do think of the telescope", "tokens": [51044, 300, 853, 281, 11985, 1952, 10862, 382, 661, 8134, 3873, 576, 360, 519, 295, 264, 26114, 51416], "temperature": 0.0, "avg_logprob": -0.07625282006185563, "compression_ratio": 1.6628571428571428, "no_speech_prob": 0.0060978662222623825}, {"id": 306, "seek": 222640, "start": 2226.4, "end": 2235.76, "text": " that allows a scientist to um to look uh where his own eyes cannot see or the machine the the", "tokens": [50364, 300, 4045, 257, 12662, 281, 1105, 281, 574, 2232, 689, 702, 1065, 2575, 2644, 536, 420, 264, 3479, 264, 264, 50832], "temperature": 0.0, "avg_logprob": -0.12330061813880658, "compression_ratio": 1.7962085308056872, "no_speech_prob": 0.007499111816287041}, {"id": 307, "seek": 222640, "start": 2235.76, "end": 2241.44, "text": " the sorry the the card that or the the airplane would allow people to to to reach places that", "tokens": [50832, 264, 2597, 264, 264, 2920, 300, 420, 264, 264, 17130, 576, 2089, 561, 281, 281, 281, 2524, 3190, 300, 51116], "temperature": 0.0, "avg_logprob": -0.12330061813880658, "compression_ratio": 1.7962085308056872, "no_speech_prob": 0.007499111816287041}, {"id": 308, "seek": 222640, "start": 2241.44, "end": 2248.4, "text": " they couldn't reach easily with their with their feet um so it's really about seeing ai system as", "tokens": [51116, 436, 2809, 380, 2524, 3612, 365, 641, 365, 641, 3521, 1105, 370, 309, 311, 534, 466, 2577, 9783, 1185, 382, 51464], "temperature": 0.0, "avg_logprob": -0.12330061813880658, "compression_ratio": 1.7962085308056872, "no_speech_prob": 0.007499111816287041}, {"id": 309, "seek": 222640, "start": 2248.4, "end": 2255.84, "text": " an extension of the human intelligence rather than seeing ai as a tool that replaces us so in", "tokens": [51464, 364, 10320, 295, 264, 1952, 7599, 2831, 813, 2577, 9783, 382, 257, 2290, 300, 46734, 505, 370, 294, 51836], "temperature": 0.0, "avg_logprob": -0.12330061813880658, "compression_ratio": 1.7962085308056872, "no_speech_prob": 0.007499111816287041}, {"id": 310, "seek": 225584, "start": 2255.84, "end": 2264.32, "text": " this sense hybrid intelligence really look into uh systems that collaborate uh with humans uh aiming", "tokens": [50364, 341, 2020, 13051, 7599, 534, 574, 666, 2232, 3652, 300, 18338, 2232, 365, 6255, 2232, 20253, 50788], "temperature": 0.0, "avg_logprob": -0.11531827488883598, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.0016232568304985762}, {"id": 311, "seek": 225584, "start": 2264.32, "end": 2271.2000000000003, "text": " for a complementarity so weak weaknesses and strengths of both ai and humans are complemented", "tokens": [50788, 337, 257, 17103, 17409, 370, 5336, 24381, 293, 16986, 295, 1293, 9783, 293, 6255, 366, 17103, 292, 51132], "temperature": 0.0, "avg_logprob": -0.11531827488883598, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.0016232568304985762}, {"id": 312, "seek": 225584, "start": 2271.2000000000003, "end": 2280.0, "text": " by each other uh and really about this synergetic idea so the the the mixed team the hybrid team", "tokens": [51132, 538, 1184, 661, 2232, 293, 534, 466, 341, 33781, 19177, 1558, 370, 264, 264, 264, 7467, 1469, 264, 13051, 1469, 51572], "temperature": 0.0, "avg_logprob": -0.11531827488883598, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.0016232568304985762}, {"id": 313, "seek": 228000, "start": 2280.0, "end": 2290.8, "text": " is is aiming for the same has a shared goal um this is um the so so we have a research agenda", "tokens": [50364, 307, 307, 20253, 337, 264, 912, 575, 257, 5507, 3387, 1105, 341, 307, 1105, 264, 370, 370, 321, 362, 257, 2132, 9829, 50904], "temperature": 0.0, "avg_logprob": -0.14671905005156105, "compression_ratio": 1.7018633540372672, "no_speech_prob": 0.005213447380810976}, {"id": 314, "seek": 228000, "start": 2290.8, "end": 2295.36, "text": " I'm part of the Dutch hybrid intelligence consortium and I've been part of the hybrid", "tokens": [50904, 286, 478, 644, 295, 264, 15719, 13051, 7599, 38343, 2197, 293, 286, 600, 668, 644, 295, 264, 13051, 51132], "temperature": 0.0, "avg_logprob": -0.14671905005156105, "compression_ratio": 1.7018633540372672, "no_speech_prob": 0.005213447380810976}, {"id": 315, "seek": 228000, "start": 2295.36, "end": 2302.4, "text": " intelligence conference of the past uh in the past years um and it's a really vibrant uh field", "tokens": [51132, 7599, 7586, 295, 264, 1791, 2232, 294, 264, 1791, 924, 1105, 293, 309, 311, 257, 534, 21571, 2232, 2519, 51484], "temperature": 0.0, "avg_logprob": -0.14671905005156105, "compression_ratio": 1.7018633540372672, "no_speech_prob": 0.005213447380810976}, {"id": 316, "seek": 230240, "start": 2302.96, "end": 2310.7200000000003, "text": " um and and explainability explaining um it's also a part of the research agenda so", "tokens": [50392, 1105, 293, 293, 2903, 2310, 13468, 1105, 309, 311, 611, 257, 644, 295, 264, 2132, 9829, 370, 50780], "temperature": 0.0, "avg_logprob": -0.11863082950398074, "compression_ratio": 1.7364864864864864, "no_speech_prob": 0.03876078501343727}, {"id": 317, "seek": 230240, "start": 2310.7200000000003, "end": 2315.52, "text": " it's not only about trying to collaborate but how in in this in this collaboration", "tokens": [50780, 309, 311, 406, 787, 466, 1382, 281, 18338, 457, 577, 294, 294, 341, 294, 341, 9363, 51020], "temperature": 0.0, "avg_logprob": -0.11863082950398074, "compression_ratio": 1.7364864864864864, "no_speech_prob": 0.03876078501343727}, {"id": 318, "seek": 230240, "start": 2315.52, "end": 2322.64, "text": " the goal is is also to try to communicate our own intention and explain our own actions and", "tokens": [51020, 264, 3387, 307, 307, 611, 281, 853, 281, 7890, 527, 1065, 7789, 293, 2903, 527, 1065, 5909, 293, 51376], "temperature": 0.0, "avg_logprob": -0.11863082950398074, "compression_ratio": 1.7364864864864864, "no_speech_prob": 0.03876078501343727}, {"id": 319, "seek": 232264, "start": 2322.64, "end": 2332.72, "text": " our own reasoning uh so the one of the the core topics that we established when when", "tokens": [50364, 527, 1065, 21577, 2232, 370, 264, 472, 295, 264, 264, 4965, 8378, 300, 321, 7545, 562, 562, 50868], "temperature": 0.0, "avg_logprob": -0.1311130289171563, "compression_ratio": 1.6272189349112427, "no_speech_prob": 0.03184948489069939}, {"id": 320, "seek": 232264, "start": 2332.72, "end": 2337.8399999999997, "text": " hybrid intelligence came into the picture was really on on on how to create systems that are", "tokens": [50868, 13051, 7599, 1361, 666, 264, 3036, 390, 534, 322, 322, 322, 577, 281, 1884, 3652, 300, 366, 51124], "temperature": 0.0, "avg_logprob": -0.1311130289171563, "compression_ratio": 1.6272189349112427, "no_speech_prob": 0.03184948489069939}, {"id": 321, "seek": 232264, "start": 2337.8399999999997, "end": 2346.64, "text": " able to deliberate and and explain to to their collaborators um I'm more than happy to to discuss", "tokens": [51124, 1075, 281, 30515, 293, 293, 2903, 281, 281, 641, 39789, 1105, 286, 478, 544, 813, 2055, 281, 281, 2248, 51564], "temperature": 0.0, "avg_logprob": -0.1311130289171563, "compression_ratio": 1.6272189349112427, "no_speech_prob": 0.03184948489069939}, {"id": 322, "seek": 234664, "start": 2346.64, "end": 2353.8399999999997, "text": " this a bit further uh and somehow in as part of the hybrid intelligence picture we also uh", "tokens": [50364, 341, 257, 857, 3052, 2232, 293, 6063, 294, 382, 644, 295, 264, 13051, 7599, 3036, 321, 611, 2232, 50724], "temperature": 0.0, "avg_logprob": -0.13056581991690178, "compression_ratio": 1.8232323232323233, "no_speech_prob": 0.01649249531328678}, {"id": 323, "seek": 234664, "start": 2353.8399999999997, "end": 2359.12, "text": " and as part of the the the contribution that we could give with ontologies and knowledge", "tokens": [50724, 293, 382, 644, 295, 264, 264, 264, 13150, 300, 321, 727, 976, 365, 6592, 6204, 293, 3601, 50988], "temperature": 0.0, "avg_logprob": -0.13056581991690178, "compression_ratio": 1.8232323232323233, "no_speech_prob": 0.01649249531328678}, {"id": 324, "seek": 234664, "start": 2359.12, "end": 2366.96, "text": " across with respect to explainability we also try to um work on on on on a number of what we", "tokens": [50988, 2108, 365, 3104, 281, 2903, 2310, 321, 611, 853, 281, 1105, 589, 322, 322, 322, 322, 257, 1230, 295, 437, 321, 51380], "temperature": 0.0, "avg_logprob": -0.13056581991690178, "compression_ratio": 1.8232323232323233, "no_speech_prob": 0.01649249531328678}, {"id": 325, "seek": 234664, "start": 2366.96, "end": 2372.4, "text": " call boxologies or terminologies for hybrid intelligence where so to establish mostly to", "tokens": [51380, 818, 2424, 6204, 420, 10761, 6204, 337, 13051, 7599, 689, 370, 281, 8327, 5240, 281, 51652], "temperature": 0.0, "avg_logprob": -0.13056581991690178, "compression_ratio": 1.8232323232323233, "no_speech_prob": 0.01649249531328678}, {"id": 326, "seek": 237240, "start": 2372.4, "end": 2379.76, "text": " establish a shared language between uh agents in a different team um sorry agents in the same", "tokens": [50364, 8327, 257, 5507, 2856, 1296, 2232, 12554, 294, 257, 819, 1469, 1105, 2597, 12554, 294, 264, 912, 50732], "temperature": 0.0, "avg_logprob": -0.12293726358658229, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0022125456016510725}, {"id": 327, "seek": 237240, "start": 2379.76, "end": 2386.8, "text": " team that would collaborate between each other um and uh what what we did in one of the the the", "tokens": [50732, 1469, 300, 576, 18338, 1296, 1184, 661, 1105, 293, 2232, 437, 437, 321, 630, 294, 472, 295, 264, 264, 264, 51084], "temperature": 0.0, "avg_logprob": -0.12293726358658229, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0022125456016510725}, {"id": 328, "seek": 237240, "start": 2388.0, "end": 2392.88, "text": " the newest work was really on comparing different hybrid intelligence scenarios", "tokens": [51144, 264, 17569, 589, 390, 534, 322, 15763, 819, 13051, 7599, 15077, 51388], "temperature": 0.0, "avg_logprob": -0.12293726358658229, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0022125456016510725}, {"id": 329, "seek": 237240, "start": 2393.92, "end": 2399.92, "text": " and first trying to identify what are the common uh knowledge roles so we came up with a high level", "tokens": [51440, 293, 700, 1382, 281, 5876, 437, 366, 264, 2689, 2232, 3601, 9604, 370, 321, 1361, 493, 365, 257, 1090, 1496, 51740], "temperature": 0.0, "avg_logprob": -0.12293726358658229, "compression_ratio": 1.7571428571428571, "no_speech_prob": 0.0022125456016510725}, {"id": 330, "seek": 239992, "start": 2400.0, "end": 2408.0, "text": " ontologies of agents interaction types uh and and and specific scenarios and and we try to also", "tokens": [50368, 6592, 6204, 295, 12554, 9285, 3467, 2232, 293, 293, 293, 2685, 15077, 293, 293, 321, 853, 281, 611, 50768], "temperature": 0.0, "avg_logprob": -0.1428815205891927, "compression_ratio": 1.7305389221556886, "no_speech_prob": 0.00534677691757679}, {"id": 331, "seek": 239992, "start": 2408.0, "end": 2415.36, "text": " identify um uh using these high level ontologies what were the the most specific hybrid intelligent", "tokens": [50768, 5876, 1105, 2232, 1228, 613, 1090, 1496, 6592, 6204, 437, 645, 264, 264, 881, 2685, 13051, 13232, 51136], "temperature": 0.0, "avg_logprob": -0.1428815205891927, "compression_ratio": 1.7305389221556886, "no_speech_prob": 0.00534677691757679}, {"id": 332, "seek": 239992, "start": 2415.36, "end": 2425.12, "text": " tasks uh including ones that would uh relate to creativity and and explainability uh and then", "tokens": [51136, 9608, 2232, 3009, 2306, 300, 576, 2232, 10961, 281, 12915, 293, 293, 2903, 2310, 2232, 293, 550, 51624], "temperature": 0.0, "avg_logprob": -0.1428815205891927, "compression_ratio": 1.7305389221556886, "no_speech_prob": 0.00534677691757679}, {"id": 333, "seek": 242512, "start": 2425.12, "end": 2431.44, "text": " on the side of it we would have uh tasks like team awareness and multimodality uh that's the", "tokens": [50364, 322, 264, 1252, 295, 309, 321, 576, 362, 2232, 9608, 411, 1469, 8888, 293, 32972, 378, 1860, 2232, 300, 311, 264, 50680], "temperature": 0.0, "avg_logprob": -0.08510669620557763, "compression_ratio": 1.7793427230046948, "no_speech_prob": 0.002951227594166994}, {"id": 334, "seek": 242512, "start": 2431.44, "end": 2437.6, "text": " second part of the picture so we talk about uh deep learning we talk about uh hybrid intelligence", "tokens": [50680, 1150, 644, 295, 264, 3036, 370, 321, 751, 466, 2232, 2452, 2539, 321, 751, 466, 2232, 13051, 7599, 50988], "temperature": 0.0, "avg_logprob": -0.08510669620557763, "compression_ratio": 1.7793427230046948, "no_speech_prob": 0.002951227594166994}, {"id": 335, "seek": 242512, "start": 2438.7999999999997, "end": 2446.3199999999997, "text": " and what also came into play with respect to to explanation and uh we didn't have that much", "tokens": [51048, 293, 437, 611, 1361, 666, 862, 365, 3104, 281, 281, 10835, 293, 2232, 321, 994, 380, 362, 300, 709, 51424], "temperature": 0.0, "avg_logprob": -0.08510669620557763, "compression_ratio": 1.7793427230046948, "no_speech_prob": 0.002951227594166994}, {"id": 336, "seek": 242512, "start": 2446.3199999999997, "end": 2452.08, "text": " time to to dive into it but hybrid intelligence is strongly related to that as well is really on", "tokens": [51424, 565, 281, 281, 9192, 666, 309, 457, 13051, 7599, 307, 10613, 4077, 281, 300, 382, 731, 307, 534, 322, 51712], "temperature": 0.0, "avg_logprob": -0.08510669620557763, "compression_ratio": 1.7793427230046948, "no_speech_prob": 0.002951227594166994}, {"id": 337, "seek": 245208, "start": 2452.08, "end": 2458.64, "text": " especially with a with a european perspective is really on the gdpr and uh so um and an eu", "tokens": [50364, 2318, 365, 257, 365, 257, 27207, 282, 4585, 307, 534, 322, 264, 290, 67, 1424, 293, 2232, 370, 1105, 293, 364, 2228, 50692], "temperature": 0.0, "avg_logprob": -0.14857018931528157, "compression_ratio": 1.8894472361809045, "no_speech_prob": 0.007552407681941986}, {"id": 338, "seek": 245208, "start": 2459.6, "end": 2467.36, "text": " ai act which recently came up uh but really the idea is to try to monitor the systems that we are", "tokens": [50740, 9783, 605, 597, 3938, 1361, 493, 2232, 457, 534, 264, 1558, 307, 281, 853, 281, 6002, 264, 3652, 300, 321, 366, 51128], "temperature": 0.0, "avg_logprob": -0.14857018931528157, "compression_ratio": 1.8894472361809045, "no_speech_prob": 0.007552407681941986}, {"id": 339, "seek": 245208, "start": 2467.36, "end": 2475.6, "text": " developing uh to make sure that that that uh users are are protected both in terms in terms of the", "tokens": [51128, 6416, 2232, 281, 652, 988, 300, 300, 300, 2232, 5022, 366, 366, 10594, 1293, 294, 2115, 294, 2115, 295, 264, 51540], "temperature": 0.0, "avg_logprob": -0.14857018931528157, "compression_ratio": 1.8894472361809045, "no_speech_prob": 0.007552407681941986}, {"id": 340, "seek": 245208, "start": 2475.6, "end": 2481.52, "text": " data that are being generated and the methods that are being generated uh and these also", "tokens": [51540, 1412, 300, 366, 885, 10833, 293, 264, 7150, 300, 366, 885, 10833, 2232, 293, 613, 611, 51836], "temperature": 0.0, "avg_logprob": -0.14857018931528157, "compression_ratio": 1.8894472361809045, "no_speech_prob": 0.007552407681941986}, {"id": 341, "seek": 248152, "start": 2481.52, "end": 2490.96, "text": " means to be able to to to explain um uh the reasoning and to trace back the the the information", "tokens": [50364, 1355, 281, 312, 1075, 281, 281, 281, 2903, 1105, 2232, 264, 21577, 293, 281, 13508, 646, 264, 264, 264, 1589, 50836], "temperature": 0.0, "avg_logprob": -0.09295339738169024, "compression_ratio": 1.775, "no_speech_prob": 0.0049593765288591385}, {"id": 342, "seek": 248152, "start": 2490.96, "end": 2500.8, "text": " that is being output uh so so explainability and transparency became um started uh appearing", "tokens": [50836, 300, 307, 885, 5598, 2232, 370, 370, 2903, 2310, 293, 17131, 3062, 1105, 1409, 2232, 19870, 51328], "temperature": 0.0, "avg_logprob": -0.09295339738169024, "compression_ratio": 1.775, "no_speech_prob": 0.0049593765288591385}, {"id": 343, "seek": 248152, "start": 2500.8, "end": 2506.08, "text": " together in the in the picture so if you want transparency you need to be explainable therefore", "tokens": [51328, 1214, 294, 264, 294, 264, 3036, 370, 498, 291, 528, 17131, 291, 643, 281, 312, 2903, 712, 4412, 51592], "temperature": 0.0, "avg_logprob": -0.09295339738169024, "compression_ratio": 1.775, "no_speech_prob": 0.0049593765288591385}, {"id": 344, "seek": 250608, "start": 2506.08, "end": 2514.96, "text": " showing your reasoning um and now with this eu ai act the the idea is really that that there are", "tokens": [50364, 4099, 428, 21577, 1105, 293, 586, 365, 341, 2228, 9783, 605, 264, 264, 1558, 307, 534, 300, 300, 456, 366, 50808], "temperature": 0.0, "avg_logprob": -0.1364306621864194, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.008047170005738735}, {"id": 345, "seek": 250608, "start": 2514.96, "end": 2522.0, "text": " certain obligations there are certain systems that need to show um uh some transparent they have", "tokens": [50808, 1629, 26234, 456, 366, 1629, 3652, 300, 643, 281, 855, 1105, 2232, 512, 12737, 436, 362, 51160], "temperature": 0.0, "avg_logprob": -0.1364306621864194, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.008047170005738735}, {"id": 346, "seek": 250608, "start": 2522.0, "end": 2528.7999999999997, "text": " transparency obligations so they need to uh be able to explain why certain things are are happening", "tokens": [51160, 17131, 26234, 370, 436, 643, 281, 2232, 312, 1075, 281, 2903, 983, 1629, 721, 366, 366, 2737, 51500], "temperature": 0.0, "avg_logprob": -0.1364306621864194, "compression_ratio": 1.8198757763975155, "no_speech_prob": 0.008047170005738735}, {"id": 347, "seek": 252880, "start": 2528.88, "end": 2540.0800000000004, "text": " um otherwise they are considered uh unacceptable uh this is a very uh so i have a few minutes left", "tokens": [50368, 1105, 5911, 436, 366, 4888, 2232, 31812, 2232, 341, 307, 257, 588, 2232, 370, 741, 362, 257, 1326, 2077, 1411, 50928], "temperature": 0.0, "avg_logprob": -0.08111836569649833, "compression_ratio": 1.7325581395348837, "no_speech_prob": 0.018601788207888603}, {"id": 348, "seek": 252880, "start": 2540.0800000000004, "end": 2548.1600000000003, "text": " i think this is a very uh quick overview of what has happened ever since uh which leads us back to", "tokens": [50928, 741, 519, 341, 307, 257, 588, 2232, 1702, 12492, 295, 437, 575, 2011, 1562, 1670, 2232, 597, 6689, 505, 646, 281, 51332], "temperature": 0.0, "avg_logprob": -0.08111836569649833, "compression_ratio": 1.7325581395348837, "no_speech_prob": 0.018601788207888603}, {"id": 349, "seek": 252880, "start": 2548.1600000000003, "end": 2554.2400000000002, "text": " okay what is going to happen now and how can we kind of think of everything that has happened so far", "tokens": [51332, 1392, 437, 307, 516, 281, 1051, 586, 293, 577, 393, 321, 733, 295, 519, 295, 1203, 300, 575, 2011, 370, 1400, 51636], "temperature": 0.0, "avg_logprob": -0.08111836569649833, "compression_ratio": 1.7325581395348837, "no_speech_prob": 0.018601788207888603}, {"id": 350, "seek": 255424, "start": 2554.8799999999997, "end": 2561.68, "text": " and and where are we going to to go uh of course i couldn't get away without mentioning language", "tokens": [50396, 293, 293, 689, 366, 321, 516, 281, 281, 352, 2232, 295, 1164, 741, 2809, 380, 483, 1314, 1553, 18315, 2856, 50736], "temperature": 0.0, "avg_logprob": -0.15127058171514254, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.030866362154483795}, {"id": 351, "seek": 255424, "start": 2561.68, "end": 2570.16, "text": " models at least once so somehow one of the questions and one might ask and we also kind of wondered uh", "tokens": [50736, 5245, 412, 1935, 1564, 370, 6063, 472, 295, 264, 1651, 293, 472, 1062, 1029, 293, 321, 611, 733, 295, 17055, 2232, 51160], "temperature": 0.0, "avg_logprob": -0.15127058171514254, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.030866362154483795}, {"id": 352, "seek": 255424, "start": 2570.16, "end": 2576.64, "text": " was uh okay but everything we've done could this be now done so the the overall knowledge", "tokens": [51160, 390, 2232, 1392, 457, 1203, 321, 600, 1096, 727, 341, 312, 586, 1096, 370, 264, 264, 4787, 3601, 51484], "temperature": 0.0, "avg_logprob": -0.15127058171514254, "compression_ratio": 1.6235955056179776, "no_speech_prob": 0.030866362154483795}, {"id": 353, "seek": 257664, "start": 2576.7999999999997, "end": 2582.56, "text": " process could just be replaced by language models and by large language models by LLMs", "tokens": [50372, 1399, 727, 445, 312, 10772, 538, 2856, 5245, 293, 538, 2416, 2856, 5245, 538, 441, 43, 26386, 50660], "temperature": 0.0, "avg_logprob": -0.16880647820162487, "compression_ratio": 1.835, "no_speech_prob": 0.1321311891078949}, {"id": 354, "seek": 257664, "start": 2583.2799999999997, "end": 2588.64, "text": " could we just not replace all the steps or the explainability steps do we actually need", "tokens": [50696, 727, 321, 445, 406, 7406, 439, 264, 4439, 420, 264, 2903, 2310, 4439, 360, 321, 767, 643, 50964], "temperature": 0.0, "avg_logprob": -0.16880647820162487, "compression_ratio": 1.835, "no_speech_prob": 0.1321311891078949}, {"id": 355, "seek": 257664, "start": 2588.64, "end": 2596.72, "text": " knowledge grasp for that and it's true that um LLMs language and language models are very good", "tokens": [50964, 3601, 21743, 337, 300, 293, 309, 311, 2074, 300, 1105, 441, 43, 26386, 2856, 293, 2856, 5245, 366, 588, 665, 51368], "temperature": 0.0, "avg_logprob": -0.16880647820162487, "compression_ratio": 1.835, "no_speech_prob": 0.1321311891078949}, {"id": 356, "seek": 257664, "start": 2596.72, "end": 2602.16, "text": " in dealing with noise and inconsistency and like methods that the methods that we had before were", "tokens": [51368, 294, 6260, 365, 5658, 293, 22039, 468, 3020, 293, 411, 7150, 300, 264, 7150, 300, 321, 632, 949, 645, 51640], "temperature": 0.0, "avg_logprob": -0.16880647820162487, "compression_ratio": 1.835, "no_speech_prob": 0.1321311891078949}, {"id": 357, "seek": 260216, "start": 2602.16, "end": 2607.92, "text": " not that much able they allow us to extract information very quickly from large structure", "tokens": [50364, 406, 300, 709, 1075, 436, 2089, 505, 281, 8947, 1589, 588, 2661, 490, 2416, 3877, 50652], "temperature": 0.0, "avg_logprob": -0.10566932340211506, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.004329802468419075}, {"id": 358, "seek": 260216, "start": 2607.92, "end": 2615.3599999999997, "text": " data so that goes towards the dream of doing a web-scale learning um or one could argue that", "tokens": [50652, 1412, 370, 300, 1709, 3030, 264, 3055, 295, 884, 257, 3670, 12, 20033, 2539, 1105, 420, 472, 727, 9695, 300, 51024], "temperature": 0.0, "avg_logprob": -0.10566932340211506, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.004329802468419075}, {"id": 359, "seek": 260216, "start": 2615.3599999999997, "end": 2621.8399999999997, "text": " you already achieved that uh they're actually quite good in capturing some complex semantics so", "tokens": [51024, 291, 1217, 11042, 300, 2232, 436, 434, 767, 1596, 665, 294, 23384, 512, 3997, 4361, 45298, 370, 51348], "temperature": 0.0, "avg_logprob": -0.10566932340211506, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.004329802468419075}, {"id": 360, "seek": 260216, "start": 2621.8399999999997, "end": 2629.3599999999997, "text": " somehow it has been demonstrated that defining a class can can be uh uh with specific boundaries", "tokens": [51348, 6063, 309, 575, 668, 18772, 300, 17827, 257, 1508, 393, 393, 312, 2232, 2232, 365, 2685, 13180, 51724], "temperature": 0.0, "avg_logprob": -0.10566932340211506, "compression_ratio": 1.6233766233766234, "no_speech_prob": 0.004329802468419075}, {"id": 361, "seek": 262936, "start": 2629.36, "end": 2633.04, "text": " so the boundaries of the definition of a class is quite difficult are quite hard", "tokens": [50364, 370, 264, 13180, 295, 264, 7123, 295, 257, 1508, 307, 1596, 2252, 366, 1596, 1152, 50548], "temperature": 0.0, "avg_logprob": -0.1414084107908484, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0006578345783054829}, {"id": 362, "seek": 262936, "start": 2634.88, "end": 2644.56, "text": " so there is no universal class description and and it the especially with the embeddings method", "tokens": [50640, 370, 456, 307, 572, 11455, 1508, 3855, 293, 293, 309, 264, 2318, 365, 264, 12240, 29432, 3170, 51124], "temperature": 0.0, "avg_logprob": -0.1414084107908484, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0006578345783054829}, {"id": 363, "seek": 262936, "start": 2644.56, "end": 2651.6, "text": " based methods are able to capture this this this complexity quite a bit better and of course", "tokens": [51124, 2361, 7150, 366, 1075, 281, 7983, 341, 341, 341, 14024, 1596, 257, 857, 1101, 293, 295, 1164, 51476], "temperature": 0.0, "avg_logprob": -0.1414084107908484, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0006578345783054829}, {"id": 364, "seek": 262936, "start": 2651.6, "end": 2656.8, "text": " are very good in generating natural language so instead of generating explanation in a", "tokens": [51476, 366, 588, 665, 294, 17746, 3303, 2856, 370, 2602, 295, 17746, 10835, 294, 257, 51736], "temperature": 0.0, "avg_logprob": -0.1414084107908484, "compression_ratio": 1.7623762376237624, "no_speech_prob": 0.0006578345783054829}, {"id": 365, "seek": 265680, "start": 2656.88, "end": 2663.84, "text": " mechanic mechanical mechanistic way from from from the triples uh they are able to generate a much", "tokens": [50368, 23860, 12070, 4236, 3142, 636, 490, 490, 490, 264, 1376, 2622, 2232, 436, 366, 1075, 281, 8460, 257, 709, 50716], "temperature": 0.0, "avg_logprob": -0.11713448024931408, "compression_ratio": 1.7713004484304933, "no_speech_prob": 0.002851231023669243}, {"id": 366, "seek": 265680, "start": 2663.84, "end": 2670.88, "text": " more human friendly um explanation the problem is that there are still limited in a number of things", "tokens": [50716, 544, 1952, 9208, 1105, 10835, 264, 1154, 307, 300, 456, 366, 920, 5567, 294, 257, 1230, 295, 721, 51068], "temperature": 0.0, "avg_logprob": -0.11713448024931408, "compression_ratio": 1.7713004484304933, "no_speech_prob": 0.002851231023669243}, {"id": 367, "seek": 265680, "start": 2670.88, "end": 2677.1200000000003, "text": " so learning from rare and unique events especially like the ones that that that you can find in the", "tokens": [51068, 370, 2539, 490, 5892, 293, 3845, 3931, 2318, 411, 264, 2306, 300, 300, 300, 291, 393, 915, 294, 264, 51380], "temperature": 0.0, "avg_logprob": -0.11713448024931408, "compression_ratio": 1.7713004484304933, "no_speech_prob": 0.002851231023669243}, {"id": 368, "seek": 265680, "start": 2677.1200000000003, "end": 2686.48, "text": " web is still quite difficult um if this these methods are not yet able to show proper reasoning", "tokens": [51380, 3670, 307, 920, 1596, 2252, 1105, 498, 341, 613, 7150, 366, 406, 1939, 1075, 281, 855, 2296, 21577, 51848], "temperature": 0.0, "avg_logprob": -0.11713448024931408, "compression_ratio": 1.7713004484304933, "no_speech_prob": 0.002851231023669243}, {"id": 369, "seek": 268648, "start": 2686.48, "end": 2694.16, "text": " and argumenting behind thoroughly creating a thorough argumentation uh behind what has happened", "tokens": [50364, 293, 6770, 278, 2261, 17987, 4084, 257, 12934, 6770, 399, 2232, 2261, 437, 575, 2011, 50748], "temperature": 0.0, "avg_logprob": -0.10147495393629198, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.002366953995078802}, {"id": 370, "seek": 268648, "start": 2694.16, "end": 2700.64, "text": " and also they don't really deal with with with fairness and interoperability acceptability all", "tokens": [50748, 293, 611, 436, 500, 380, 534, 2028, 365, 365, 365, 29765, 293, 728, 7192, 2310, 3241, 2310, 439, 51072], "temperature": 0.0, "avg_logprob": -0.10147495393629198, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.002366953995078802}, {"id": 371, "seek": 268648, "start": 2700.64, "end": 2709.92, "text": " these this fair aspects that we've been looking into as as knowledge graph community are not yet", "tokens": [51072, 613, 341, 3143, 7270, 300, 321, 600, 668, 1237, 666, 382, 382, 3601, 4295, 1768, 366, 406, 1939, 51536], "temperature": 0.0, "avg_logprob": -0.10147495393629198, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.002366953995078802}, {"id": 372, "seek": 268648, "start": 2709.92, "end": 2714.88, "text": " part of the picture in a language model so somehow there are limitations in using that", "tokens": [51536, 644, 295, 264, 3036, 294, 257, 2856, 2316, 370, 6063, 456, 366, 15705, 294, 1228, 300, 51784], "temperature": 0.0, "avg_logprob": -0.10147495393629198, "compression_ratio": 1.7235023041474655, "no_speech_prob": 0.002366953995078802}, {"id": 373, "seek": 271488, "start": 2714.88, "end": 2720.0, "text": " but it doesn't mean that we need to discard them completely but we can just join the the", "tokens": [50364, 457, 309, 1177, 380, 914, 300, 321, 643, 281, 31597, 552, 2584, 457, 321, 393, 445, 3917, 264, 264, 50620], "temperature": 0.0, "avg_logprob": -0.1089099087292635, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0018100646557286382}, {"id": 374, "seek": 271488, "start": 2720.0, "end": 2725.76, "text": " both words and and work out something to for to generate better better explanations", "tokens": [50620, 1293, 2283, 293, 293, 589, 484, 746, 281, 337, 281, 8460, 1101, 1101, 28708, 50908], "temperature": 0.0, "avg_logprob": -0.1089099087292635, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0018100646557286382}, {"id": 375, "seek": 271488, "start": 2726.7200000000003, "end": 2735.04, "text": " so i want to conclude in the last few minutes to really think okay if we now look at knowledge", "tokens": [50956, 370, 741, 528, 281, 16886, 294, 264, 1036, 1326, 2077, 281, 534, 519, 1392, 498, 321, 586, 574, 412, 3601, 51372], "temperature": 0.0, "avg_logprob": -0.1089099087292635, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0018100646557286382}, {"id": 376, "seek": 271488, "start": 2735.04, "end": 2743.28, "text": " graphs and whether they're useful uh to to generate explanation and do they actually work", "tokens": [51372, 24877, 293, 1968, 436, 434, 4420, 2232, 281, 281, 8460, 10835, 293, 360, 436, 767, 589, 51784], "temperature": 0.0, "avg_logprob": -0.1089099087292635, "compression_ratio": 1.7163461538461537, "no_speech_prob": 0.0018100646557286382}, {"id": 377, "seek": 274328, "start": 2744.0, "end": 2750.2400000000002, "text": " what is it that we launch so uh somehow both based on the phc and everything that happens", "tokens": [50400, 437, 307, 309, 300, 321, 4025, 370, 2232, 6063, 1293, 2361, 322, 264, 903, 66, 293, 1203, 300, 2314, 50712], "temperature": 0.0, "avg_logprob": -0.10903464691548408, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005620189011096954}, {"id": 378, "seek": 274328, "start": 2750.2400000000002, "end": 2756.6400000000003, "text": " afterwards uh yes we can use knowledge graphs but they are mostly an intermediate representation", "tokens": [50712, 10543, 2232, 2086, 321, 393, 764, 3601, 24877, 457, 436, 366, 5240, 364, 19376, 10290, 51032], "temperature": 0.0, "avg_logprob": -0.10903464691548408, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005620189011096954}, {"id": 379, "seek": 274328, "start": 2756.6400000000003, "end": 2762.32, "text": " so knowledge graphs are really for the machine consumption uh they shouldn't be for human consumption", "tokens": [51032, 370, 3601, 24877, 366, 534, 337, 264, 3479, 12126, 2232, 436, 4659, 380, 312, 337, 1952, 12126, 51316], "temperature": 0.0, "avg_logprob": -0.10903464691548408, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005620189011096954}, {"id": 380, "seek": 274328, "start": 2762.96, "end": 2771.92, "text": " and the rdf is just a language that that for machines to perform an exchange of information", "tokens": [51348, 293, 264, 367, 45953, 307, 445, 257, 2856, 300, 300, 337, 8379, 281, 2042, 364, 7742, 295, 1589, 51796], "temperature": 0.0, "avg_logprob": -0.10903464691548408, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.005620189011096954}, {"id": 381, "seek": 277192, "start": 2772.0, "end": 2777.92, "text": " which is unambiguous so uh we should really not look into knowledge graph as something that we human", "tokens": [50368, 597, 307, 517, 2173, 30525, 370, 2232, 321, 820, 534, 406, 574, 666, 3601, 4295, 382, 746, 300, 321, 1952, 50664], "temperature": 0.0, "avg_logprob": -0.10882247366556307, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.002239329507574439}, {"id": 382, "seek": 277192, "start": 2777.92, "end": 2785.6, "text": " should understand but but more as something a machine can can quickly exchange um we can use", "tokens": [50664, 820, 1223, 457, 457, 544, 382, 746, 257, 3479, 393, 393, 2661, 7742, 1105, 321, 393, 764, 51048], "temperature": 0.0, "avg_logprob": -0.10882247366556307, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.002239329507574439}, {"id": 383, "seek": 277192, "start": 2785.6, "end": 2792.88, "text": " knowledge graphs to as to get together content to generate explanation but really the graph", "tokens": [51048, 3601, 24877, 281, 382, 281, 483, 1214, 2701, 281, 8460, 10835, 457, 534, 264, 4295, 51412], "temperature": 0.0, "avg_logprob": -0.10882247366556307, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.002239329507574439}, {"id": 384, "seek": 277192, "start": 2792.88, "end": 2799.44, "text": " structure is just a backbone so we don't really want to use the knowledge graph to create an output", "tokens": [51412, 3877, 307, 445, 257, 34889, 370, 321, 500, 380, 534, 528, 281, 764, 264, 3601, 4295, 281, 1884, 364, 5598, 51740], "temperature": 0.0, "avg_logprob": -0.10882247366556307, "compression_ratio": 1.807511737089202, "no_speech_prob": 0.002239329507574439}, {"id": 385, "seek": 279944, "start": 2799.44, "end": 2807.36, "text": " we can use language models for that uh but but we can use knowledge graph as to to to gather the", "tokens": [50364, 321, 393, 764, 2856, 5245, 337, 300, 2232, 457, 457, 321, 393, 764, 3601, 4295, 382, 281, 281, 281, 5448, 264, 50760], "temperature": 0.0, "avg_logprob": -0.1236995727785172, "compression_ratio": 1.6627906976744187, "no_speech_prob": 0.0062659019604325294}, {"id": 386, "seek": 279944, "start": 2807.36, "end": 2813.52, "text": " backbone of the explanation which can then be output according to different users in a different", "tokens": [50760, 34889, 295, 264, 10835, 597, 393, 550, 312, 5598, 4650, 281, 819, 5022, 294, 257, 819, 51068], "temperature": 0.0, "avg_logprob": -0.1236995727785172, "compression_ratio": 1.6627906976744187, "no_speech_prob": 0.0062659019604325294}, {"id": 387, "seek": 279944, "start": 2813.52, "end": 2821.52, "text": " dimension so we think okay an expert user might need a longer explanation uh which with more", "tokens": [51068, 10139, 370, 321, 519, 1392, 364, 5844, 4195, 1062, 643, 257, 2854, 10835, 2232, 597, 365, 544, 51468], "temperature": 0.0, "avg_logprob": -0.1236995727785172, "compression_ratio": 1.6627906976744187, "no_speech_prob": 0.0062659019604325294}, {"id": 388, "seek": 282152, "start": 2821.52, "end": 2829.04, "text": " arguments and an expert a non-expert and layman might need a much shorter and simpler explanation", "tokens": [50364, 12869, 293, 364, 5844, 257, 2107, 12, 3121, 15346, 293, 2360, 1601, 1062, 643, 257, 709, 11639, 293, 18587, 10835, 50740], "temperature": 0.0, "avg_logprob": -0.141018457190935, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.012214639224112034}, {"id": 389, "seek": 282152, "start": 2830.08, "end": 2835.28, "text": " in terms and llms are great in doing that you can ask them to rephrase a certain concept in", "tokens": [50792, 294, 2115, 293, 287, 75, 2592, 366, 869, 294, 884, 300, 291, 393, 1029, 552, 281, 319, 44598, 651, 257, 1629, 3410, 294, 51052], "temperature": 0.0, "avg_logprob": -0.141018457190935, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.012214639224112034}, {"id": 390, "seek": 282152, "start": 2835.28, "end": 2841.7599999999998, "text": " different uh in different according to different dimensions um also knowledge graphs allow allow", "tokens": [51052, 819, 2232, 294, 819, 4650, 281, 819, 12819, 1105, 611, 3601, 24877, 2089, 2089, 51376], "temperature": 0.0, "avg_logprob": -0.141018457190935, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.012214639224112034}, {"id": 391, "seek": 282152, "start": 2841.7599999999998, "end": 2849.04, "text": " to check the to trace back information so you can really walk down the graph and and and check", "tokens": [51376, 281, 1520, 264, 281, 13508, 646, 1589, 370, 291, 393, 534, 1792, 760, 264, 4295, 293, 293, 293, 1520, 51740], "temperature": 0.0, "avg_logprob": -0.141018457190935, "compression_ratio": 1.780373831775701, "no_speech_prob": 0.012214639224112034}, {"id": 392, "seek": 284904, "start": 2849.04, "end": 2855.44, "text": " whether the information is truth and this is quite uh this is much better than looking into", "tokens": [50364, 1968, 264, 1589, 307, 3494, 293, 341, 307, 1596, 2232, 341, 307, 709, 1101, 813, 1237, 666, 50684], "temperature": 0.0, "avg_logprob": -0.11908251980701125, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.00557366106659174}, {"id": 393, "seek": 284904, "start": 2855.44, "end": 2861.36, "text": " the propagation of an error the activation of a neural network and try to decode what does it mean", "tokens": [50684, 264, 38377, 295, 364, 6713, 264, 24433, 295, 257, 18161, 3209, 293, 853, 281, 979, 1429, 437, 775, 309, 914, 50980], "temperature": 0.0, "avg_logprob": -0.11908251980701125, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.00557366106659174}, {"id": 394, "seek": 284904, "start": 2861.36, "end": 2867.04, "text": " um in order to come up with an explanation that you might not be able to to to explain", "tokens": [50980, 1105, 294, 1668, 281, 808, 493, 365, 364, 10835, 300, 291, 1062, 406, 312, 1075, 281, 281, 281, 2903, 51264], "temperature": 0.0, "avg_logprob": -0.11908251980701125, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.00557366106659174}, {"id": 395, "seek": 284904, "start": 2868.8, "end": 2876.0, "text": " that clearly yourself and finally scalability which was part of the picture at that time okay", "tokens": [51352, 300, 4448, 1803, 293, 2721, 15664, 2310, 597, 390, 644, 295, 264, 3036, 412, 300, 565, 1392, 51712], "temperature": 0.0, "avg_logprob": -0.11908251980701125, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.00557366106659174}, {"id": 396, "seek": 287600, "start": 2876.0, "end": 2882.24, "text": " how can we deal with very large knowledge graphs and uh uh we want to to to integrate as much", "tokens": [50364, 577, 393, 321, 2028, 365, 588, 2416, 3601, 24877, 293, 2232, 2232, 321, 528, 281, 281, 281, 13365, 382, 709, 50676], "temperature": 0.0, "avg_logprob": -0.08974293131887177, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00527056073769927}, {"id": 397, "seek": 287600, "start": 2882.24, "end": 2887.92, "text": " knowledge as possible well actually this is not that relevant anymore and what people are really", "tokens": [50676, 3601, 382, 1944, 731, 767, 341, 307, 406, 300, 7340, 3602, 293, 437, 561, 366, 534, 50960], "temperature": 0.0, "avg_logprob": -0.08974293131887177, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00527056073769927}, {"id": 398, "seek": 287600, "start": 2887.92, "end": 2895.44, "text": " aiming for and we also saw this when collaborating with industry is more it's much better to have", "tokens": [50960, 20253, 337, 293, 321, 611, 1866, 341, 562, 30188, 365, 3518, 307, 544, 309, 311, 709, 1101, 281, 362, 51336], "temperature": 0.0, "avg_logprob": -0.08974293131887177, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00527056073769927}, {"id": 399, "seek": 287600, "start": 2896.16, "end": 2902.48, "text": " a high quality knowledge graph which is curated by the expert uh to generate explanations rather", "tokens": [51372, 257, 1090, 3125, 3601, 4295, 597, 307, 47851, 538, 264, 5844, 2232, 281, 8460, 28708, 2831, 51688], "temperature": 0.0, "avg_logprob": -0.08974293131887177, "compression_ratio": 1.7264573991031391, "no_speech_prob": 0.00527056073769927}, {"id": 400, "seek": 290248, "start": 2902.48, "end": 2909.68, "text": " than having a very large knowledge graph um so this is one part of the story and then the other part", "tokens": [50364, 813, 1419, 257, 588, 2416, 3601, 4295, 1105, 370, 341, 307, 472, 644, 295, 264, 1657, 293, 550, 264, 661, 644, 50724], "temperature": 0.0, "avg_logprob": -0.08443753513289087, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.004305832087993622}, {"id": 401, "seek": 290248, "start": 2909.68, "end": 2915.2, "text": " is more of the big picture uh what we learned is really explainability is not only about machine", "tokens": [50724, 307, 544, 295, 264, 955, 3036, 2232, 437, 321, 3264, 307, 534, 2903, 2310, 307, 406, 787, 466, 3479, 51000], "temperature": 0.0, "avg_logprob": -0.08443753513289087, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.004305832087993622}, {"id": 402, "seek": 290248, "start": 2915.2, "end": 2920.8, "text": " learning uh so in hybrid intelligence we deal with explainability we deal with experts from", "tokens": [51000, 2539, 2232, 370, 294, 13051, 7599, 321, 2028, 365, 2903, 2310, 321, 2028, 365, 8572, 490, 51280], "temperature": 0.0, "avg_logprob": -0.08443753513289087, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.004305832087993622}, {"id": 403, "seek": 290248, "start": 2920.8, "end": 2927.28, "text": " all kind of fields from social science to computer science and um people look into", "tokens": [51280, 439, 733, 295, 7909, 490, 2093, 3497, 281, 3820, 3497, 293, 1105, 561, 574, 666, 51604], "temperature": 0.0, "avg_logprob": -0.08443753513289087, "compression_ratio": 1.7547169811320755, "no_speech_prob": 0.004305832087993622}, {"id": 404, "seek": 292728, "start": 2927.36, "end": 2934.2400000000002, "text": " explainability from from this in different ways so it's a bit of a jungle of terminology we cannot", "tokens": [50368, 2903, 2310, 490, 490, 341, 294, 819, 2098, 370, 309, 311, 257, 857, 295, 257, 18228, 295, 27575, 321, 2644, 50712], "temperature": 0.0, "avg_logprob": -0.10216995688045725, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.004755475092679262}, {"id": 405, "seek": 292728, "start": 2934.2400000000002, "end": 2941.2000000000003, "text": " really agree what uh what we mean by something being explainable and then we need to try to find", "tokens": [50712, 534, 3986, 437, 2232, 437, 321, 914, 538, 746, 885, 2903, 712, 293, 550, 321, 643, 281, 853, 281, 915, 51060], "temperature": 0.0, "avg_logprob": -0.10216995688045725, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.004755475092679262}, {"id": 406, "seek": 292728, "start": 2941.2000000000003, "end": 2949.6800000000003, "text": " a way to harmonize that um we we learned that explanations are really task dependent so yes", "tokens": [51060, 257, 636, 281, 14750, 1125, 300, 1105, 321, 321, 3264, 300, 28708, 366, 534, 5633, 12334, 370, 2086, 51484], "temperature": 0.0, "avg_logprob": -0.10216995688045725, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.004755475092679262}, {"id": 407, "seek": 292728, "start": 2949.6800000000003, "end": 2954.7200000000003, "text": " we have an ontology design patterns for the explanation but we need to adapt this according", "tokens": [51484, 321, 362, 364, 6592, 1793, 1715, 8294, 337, 264, 10835, 457, 321, 643, 281, 6231, 341, 4650, 51736], "temperature": 0.0, "avg_logprob": -0.10216995688045725, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.004755475092679262}, {"id": 408, "seek": 295472, "start": 2954.72, "end": 2961.68, "text": " to the context and again we can use language models for that but it's um the the target audience or the", "tokens": [50364, 281, 264, 4319, 293, 797, 321, 393, 764, 2856, 5245, 337, 300, 457, 309, 311, 1105, 264, 264, 3779, 4034, 420, 264, 50712], "temperature": 0.0, "avg_logprob": -0.09923532532482612, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.0026789414696395397}, {"id": 409, "seek": 295472, "start": 2962.48, "end": 2967.3599999999997, "text": " the language or the the type of explanation really need to change according to the situation", "tokens": [50752, 264, 2856, 420, 264, 264, 2010, 295, 10835, 534, 643, 281, 1319, 4650, 281, 264, 2590, 50996], "temperature": 0.0, "avg_logprob": -0.09923532532482612, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.0026789414696395397}, {"id": 410, "seek": 295472, "start": 2968.0, "end": 2974.48, "text": " and more importantly we need to look for the human so knowledge graphs are only one part in the", "tokens": [51028, 293, 544, 8906, 321, 643, 281, 574, 337, 264, 1952, 370, 3601, 24877, 366, 787, 472, 644, 294, 264, 51352], "temperature": 0.0, "avg_logprob": -0.09923532532482612, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.0026789414696395397}, {"id": 411, "seek": 295472, "start": 2974.48, "end": 2980.56, "text": " explanation process so in order to generate explanation you need a knowledge graph you need", "tokens": [51352, 10835, 1399, 370, 294, 1668, 281, 8460, 10835, 291, 643, 257, 3601, 4295, 291, 643, 51656], "temperature": 0.0, "avg_logprob": -0.09923532532482612, "compression_ratio": 1.8640776699029127, "no_speech_prob": 0.0026789414696395397}, {"id": 412, "seek": 298056, "start": 2980.56, "end": 2985.52, "text": " a sub symbolic method but you also need a human that interacts with the with the system", "tokens": [50364, 257, 1422, 25755, 3170, 457, 291, 611, 643, 257, 1952, 300, 43582, 365, 264, 365, 264, 1185, 50612], "temperature": 0.0, "avg_logprob": -0.10128736050329476, "compression_ratio": 1.9570815450643777, "no_speech_prob": 0.007693391293287277}, {"id": 413, "seek": 298056, "start": 2986.56, "end": 2990.64, "text": " because in the end as we said at the beginning explanation is a social process it's a dual", "tokens": [50664, 570, 294, 264, 917, 382, 321, 848, 412, 264, 2863, 10835, 307, 257, 2093, 1399, 309, 311, 257, 11848, 50868], "temperature": 0.0, "avg_logprob": -0.10128736050329476, "compression_ratio": 1.9570815450643777, "no_speech_prob": 0.007693391293287277}, {"id": 414, "seek": 298056, "start": 2990.64, "end": 2997.7599999999998, "text": " process so it has to happen in a in a in a co-creation setting where the user is really", "tokens": [50868, 1399, 370, 309, 575, 281, 1051, 294, 257, 294, 257, 294, 257, 598, 12, 14066, 399, 3287, 689, 264, 4195, 307, 534, 51224], "temperature": 0.0, "avg_logprob": -0.10128736050329476, "compression_ratio": 1.9570815450643777, "no_speech_prob": 0.007693391293287277}, {"id": 415, "seek": 298056, "start": 2997.7599999999998, "end": 3003.36, "text": " interacting with the system to come up with an explanation is satisfied with and in this sense", "tokens": [51224, 18017, 365, 264, 1185, 281, 808, 493, 365, 364, 10835, 307, 11239, 365, 293, 294, 341, 2020, 51504], "temperature": 0.0, "avg_logprob": -0.10128736050329476, "compression_ratio": 1.9570815450643777, "no_speech_prob": 0.007693391293287277}, {"id": 416, "seek": 298056, "start": 3003.36, "end": 3010.24, "text": " we also need interdisciplinarity in the picture to try to measure whether an explanation is is", "tokens": [51504, 321, 611, 643, 728, 13731, 19246, 6470, 507, 294, 264, 3036, 281, 853, 281, 3481, 1968, 364, 10835, 307, 307, 51848], "temperature": 0.0, "avg_logprob": -0.10128736050329476, "compression_ratio": 1.9570815450643777, "no_speech_prob": 0.007693391293287277}, {"id": 417, "seek": 301024, "start": 3010.3199999999997, "end": 3015.68, "text": " interesting or not so we kind of we don't only need a computer science perspective but we also", "tokens": [50368, 1880, 420, 406, 370, 321, 733, 295, 321, 500, 380, 787, 643, 257, 3820, 3497, 4585, 457, 321, 611, 50636], "temperature": 0.0, "avg_logprob": -0.09156425264146593, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0009801920969039202}, {"id": 418, "seek": 301024, "start": 3015.68, "end": 3021.7599999999998, "text": " need to try to integrate the talks that we have with social scientists and cognitive uh scientists", "tokens": [50636, 643, 281, 853, 281, 13365, 264, 6686, 300, 321, 362, 365, 2093, 7708, 293, 15605, 2232, 7708, 50940], "temperature": 0.0, "avg_logprob": -0.09156425264146593, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0009801920969039202}, {"id": 419, "seek": 301024, "start": 3021.7599999999998, "end": 3028.8799999999997, "text": " to make sure that the explanations that are being generated are actually useful uh so to give some", "tokens": [50940, 281, 652, 988, 300, 264, 28708, 300, 366, 885, 10833, 366, 767, 4420, 2232, 370, 281, 976, 512, 51296], "temperature": 0.0, "avg_logprob": -0.09156425264146593, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0009801920969039202}, {"id": 420, "seek": 301024, "start": 3028.8799999999997, "end": 3035.68, "text": " ideas on what and uh then i'm i'm just done i know i'm over a bit um uh what we suggest is that", "tokens": [51296, 3487, 322, 437, 293, 2232, 550, 741, 478, 741, 478, 445, 1096, 741, 458, 741, 478, 670, 257, 857, 1105, 2232, 437, 321, 3402, 307, 300, 51636], "temperature": 0.0, "avg_logprob": -0.09156425264146593, "compression_ratio": 1.7244444444444444, "no_speech_prob": 0.0009801920969039202}, {"id": 421, "seek": 303568, "start": 3035.68, "end": 3041.12, "text": " really we should look into the kind of knowledge in artificial intelligence so some people have called", "tokens": [50364, 534, 321, 820, 574, 666, 264, 733, 295, 3601, 294, 11677, 7599, 370, 512, 561, 362, 1219, 50636], "temperature": 0.0, "avg_logprob": -0.0776975720199113, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.009696517139673233}, {"id": 422, "seek": 303568, "start": 3041.12, "end": 3046.0, "text": " this knowledge science some people have called it empirical semantics we call it knowledge in AI", "tokens": [50636, 341, 3601, 3497, 512, 561, 362, 1219, 309, 31886, 4361, 45298, 321, 818, 309, 3601, 294, 7318, 50880], "temperature": 0.0, "avg_logprob": -0.0776975720199113, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.009696517139673233}, {"id": 423, "seek": 303568, "start": 3047.12, "end": 3052.72, "text": " so we really need to go back to the empirical analysis of the knowledge graphs that we create", "tokens": [50936, 370, 321, 534, 643, 281, 352, 646, 281, 264, 31886, 5215, 295, 264, 3601, 24877, 300, 321, 1884, 51216], "temperature": 0.0, "avg_logprob": -0.0776975720199113, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.009696517139673233}, {"id": 424, "seek": 303568, "start": 3052.72, "end": 3058.3999999999996, "text": " and we deal with we need to understand their modeling style and the kind of semantics they're", "tokens": [51216, 293, 321, 2028, 365, 321, 643, 281, 1223, 641, 15983, 3758, 293, 264, 733, 295, 4361, 45298, 436, 434, 51500], "temperature": 0.0, "avg_logprob": -0.0776975720199113, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.009696517139673233}, {"id": 425, "seek": 303568, "start": 3058.3999999999996, "end": 3063.8399999999997, "text": " communicating and whether the semantics is enough or too much to generate explanations", "tokens": [51500, 17559, 293, 1968, 264, 4361, 45298, 307, 1547, 420, 886, 709, 281, 8460, 28708, 51772], "temperature": 0.0, "avg_logprob": -0.0776975720199113, "compression_ratio": 1.9190283400809716, "no_speech_prob": 0.009696517139673233}, {"id": 426, "seek": 306384, "start": 3063.84, "end": 3067.6000000000004, "text": " we need to check for the usefulness and limitation of the knowledge graph that", "tokens": [50364, 321, 643, 281, 1520, 337, 264, 4420, 1287, 293, 27432, 295, 264, 3601, 4295, 300, 50552], "temperature": 0.0, "avg_logprob": -0.08287777946990671, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0017908122390508652}, {"id": 427, "seek": 306384, "start": 3067.6000000000004, "end": 3074.1600000000003, "text": " and the knowledge that that we create and somehow we need to try to move from the step of okay how", "tokens": [50552, 293, 264, 3601, 300, 300, 321, 1884, 293, 6063, 321, 643, 281, 853, 281, 1286, 490, 264, 1823, 295, 1392, 577, 50880], "temperature": 0.0, "avg_logprob": -0.08287777946990671, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0017908122390508652}, {"id": 428, "seek": 306384, "start": 3074.1600000000003, "end": 3081.2000000000003, "text": " can we fit knowledge into the learning process these we know how to do it uh or at least we have", "tokens": [50880, 393, 321, 3318, 3601, 666, 264, 2539, 1399, 613, 321, 458, 577, 281, 360, 309, 2232, 420, 412, 1935, 321, 362, 51232], "temperature": 0.0, "avg_logprob": -0.08287777946990671, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0017908122390508652}, {"id": 429, "seek": 306384, "start": 3081.2000000000003, "end": 3087.52, "text": " good methods but we really need to focus on what is which kind of knowledge uh we need to fit", "tokens": [51232, 665, 7150, 457, 321, 534, 643, 281, 1879, 322, 437, 307, 597, 733, 295, 3601, 2232, 321, 643, 281, 3318, 51548], "temperature": 0.0, "avg_logprob": -0.08287777946990671, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0017908122390508652}, {"id": 430, "seek": 306384, "start": 3087.52, "end": 3091.6800000000003, "text": " in order to be able to learn something and to generate explanation for example", "tokens": [51548, 294, 1668, 281, 312, 1075, 281, 1466, 746, 293, 281, 8460, 10835, 337, 1365, 51756], "temperature": 0.0, "avg_logprob": -0.08287777946990671, "compression_ratio": 1.8547717842323652, "no_speech_prob": 0.0017908122390508652}, {"id": 431, "seek": 309168, "start": 3091.68, "end": 3098.3999999999996, "text": " so somehow this is a call for the community to to start so where do we start and uh i have", "tokens": [50364, 370, 6063, 341, 307, 257, 818, 337, 264, 1768, 281, 281, 722, 370, 689, 360, 321, 722, 293, 2232, 741, 362, 50700], "temperature": 0.0, "avg_logprob": -0.10377290032126686, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.002122018253430724}, {"id": 432, "seek": 309168, "start": 3098.3999999999996, "end": 3104.16, "text": " tried to revisit a bit the research questions that that i had in the beginning thinking okay", "tokens": [50700, 3031, 281, 32676, 257, 857, 264, 2132, 1651, 300, 300, 741, 632, 294, 264, 2863, 1953, 1392, 50988], "temperature": 0.0, "avg_logprob": -0.10377290032126686, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.002122018253430724}, {"id": 433, "seek": 309168, "start": 3104.16, "end": 3110.72, "text": " based on on this idea of knowledge in AI then maybe we need to try to fit the explanation pattern", "tokens": [50988, 2361, 322, 322, 341, 1558, 295, 3601, 294, 7318, 550, 1310, 321, 643, 281, 853, 281, 3318, 264, 10835, 5102, 51316], "temperature": 0.0, "avg_logprob": -0.10377290032126686, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.002122018253430724}, {"id": 434, "seek": 309168, "start": 3110.72, "end": 3119.52, "text": " into the existing uh systems uh we need to try to to to to come up with explanations that like", "tokens": [51316, 666, 264, 6741, 2232, 3652, 2232, 321, 643, 281, 853, 281, 281, 281, 281, 808, 493, 365, 28708, 300, 411, 51756], "temperature": 0.0, "avg_logprob": -0.10377290032126686, "compression_ratio": 1.7819905213270142, "no_speech_prob": 0.002122018253430724}, {"id": 435, "seek": 311952, "start": 3119.52, "end": 3128.24, "text": " uh using deep learning and uh at a web scale uh we need to try to augment explanations and turn", "tokens": [50364, 2232, 1228, 2452, 2539, 293, 2232, 412, 257, 3670, 4373, 2232, 321, 643, 281, 853, 281, 29919, 28708, 293, 1261, 50800], "temperature": 0.0, "avg_logprob": -0.11533969135607704, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0017659706063568592}, {"id": 436, "seek": 311952, "start": 3128.24, "end": 3134.24, "text": " them into complex narratives we mentioned these uh complex argumentations so and for these we", "tokens": [50800, 552, 666, 3997, 28016, 321, 2835, 613, 2232, 3997, 6770, 763, 370, 293, 337, 613, 321, 51100], "temperature": 0.0, "avg_logprob": -0.11533969135607704, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0017659706063568592}, {"id": 437, "seek": 311952, "start": 3134.24, "end": 3140.4, "text": " can really combine knowledge graphs and language models and we really need to compensate whatever", "tokens": [51100, 393, 534, 10432, 3601, 24877, 293, 2856, 5245, 293, 321, 534, 643, 281, 29458, 2035, 51408], "temperature": 0.0, "avg_logprob": -0.11533969135607704, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0017659706063568592}, {"id": 438, "seek": 314040, "start": 3140.4, "end": 3148.88, "text": " information is missing uh by uh performing a co-creation of explanation with with the humans", "tokens": [50364, 1589, 307, 5361, 2232, 538, 2232, 10205, 257, 598, 12, 14066, 399, 295, 10835, 365, 365, 264, 6255, 50788], "temperature": 0.0, "avg_logprob": -0.14365362084430197, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.028187716379761696}, {"id": 439, "seek": 314040, "start": 3150.0, "end": 3154.7200000000003, "text": " this is the end of my talk uh i thank you so much i don't know how many people are there i", "tokens": [50844, 341, 307, 264, 917, 295, 452, 751, 2232, 741, 1309, 291, 370, 709, 741, 500, 380, 458, 577, 867, 561, 366, 456, 741, 51080], "temperature": 0.0, "avg_logprob": -0.14365362084430197, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.028187716379761696}, {"id": 440, "seek": 314040, "start": 3154.7200000000003, "end": 3160.8, "text": " thank you so much for taking the time to listen into me i thank uh tobya santua and olav for", "tokens": [51080, 1309, 291, 370, 709, 337, 1940, 264, 565, 281, 2140, 666, 385, 741, 1309, 2232, 281, 2322, 64, 23044, 4398, 293, 2545, 706, 337, 51384], "temperature": 0.0, "avg_logprob": -0.14365362084430197, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.028187716379761696}, {"id": 441, "seek": 314040, "start": 3160.8, "end": 3168.7200000000003, "text": " inviting me there was an amazing opportunity i invite you to to reach me out for exchanges", "tokens": [51384, 18202, 385, 456, 390, 364, 2243, 2650, 741, 7980, 291, 281, 281, 2524, 385, 484, 337, 27374, 51780], "temperature": 0.0, "avg_logprob": -0.14365362084430197, "compression_ratio": 1.7393364928909953, "no_speech_prob": 0.028187716379761696}, {"id": 442, "seek": 316872, "start": 3168.72, "end": 3175.04, "text": " and i really hope to see you in uh at the next hybrid intelligence conference in in sweden in in", "tokens": [50364, 293, 741, 534, 1454, 281, 536, 291, 294, 2232, 412, 264, 958, 13051, 7599, 7586, 294, 294, 1693, 6876, 294, 294, 50680], "temperature": 0.0, "avg_logprob": -0.12208026729217947, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0031037454027682543}, {"id": 443, "seek": 316872, "start": 3175.04, "end": 3185.68, "text": " june uh this is the end of my talk thank you all right uh thank you ilaria um i should buy one of", "tokens": [50680, 361, 2613, 2232, 341, 307, 264, 917, 295, 452, 751, 1309, 291, 439, 558, 2232, 1309, 291, 741, 2200, 654, 1105, 741, 820, 2256, 472, 295, 51212], "temperature": 0.0, "avg_logprob": -0.12208026729217947, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0031037454027682543}, {"id": 444, "seek": 316872, "start": 3185.68, "end": 3193.3599999999997, "text": " the sitcom applause machines and give you a round of applause that reflects the size of the audience", "tokens": [51212, 264, 49530, 9969, 8379, 293, 976, 291, 257, 3098, 295, 9969, 300, 18926, 264, 2744, 295, 264, 4034, 51596], "temperature": 0.0, "avg_logprob": -0.12208026729217947, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0031037454027682543}, {"id": 445, "seek": 319336, "start": 3193.36, "end": 3202.1600000000003, "text": " that we had um there were a few questions already in the youtube chat um so please keep them come in", "tokens": [50364, 300, 321, 632, 1105, 456, 645, 257, 1326, 1651, 1217, 294, 264, 12487, 5081, 1105, 370, 1767, 1066, 552, 808, 294, 50804], "temperature": 0.0, "avg_logprob": -0.09500713217748355, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.12234114855527878}, {"id": 446, "seek": 319336, "start": 3203.36, "end": 3210.96, "text": " and um in the meantime i will post some of them to you and uh if there are no more questions", "tokens": [50864, 293, 1105, 294, 264, 14991, 741, 486, 2183, 512, 295, 552, 281, 291, 293, 2232, 498, 456, 366, 572, 544, 1651, 51244], "temperature": 0.0, "avg_logprob": -0.09500713217748355, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.12234114855527878}, {"id": 447, "seek": 319336, "start": 3210.96, "end": 3221.6, "text": " i may ask the people have to deal with mine um good so let's start there is um there is a question", "tokens": [51244, 741, 815, 1029, 264, 561, 362, 281, 2028, 365, 3892, 1105, 665, 370, 718, 311, 722, 456, 307, 1105, 456, 307, 257, 1168, 51776], "temperature": 0.0, "avg_logprob": -0.09500713217748355, "compression_ratio": 1.7176470588235293, "no_speech_prob": 0.12234114855527878}, {"id": 448, "seek": 322160, "start": 3221.6, "end": 3229.7599999999998, "text": " by mevish on the chat um and she's asking uh what is your vision on using your explanation", "tokens": [50364, 538, 385, 85, 742, 322, 264, 5081, 1105, 293, 750, 311, 3365, 2232, 437, 307, 428, 5201, 322, 1228, 428, 10835, 50772], "temperature": 0.0, "avg_logprob": -0.1268047014872233, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.02327006310224533}, {"id": 449, "seek": 322160, "start": 3229.7599999999998, "end": 3235.92, "text": " techniques for a large language model so you do you think the same techniques are useful for them", "tokens": [50772, 7512, 337, 257, 2416, 2856, 2316, 370, 291, 360, 291, 519, 264, 912, 7512, 366, 4420, 337, 552, 51080], "temperature": 0.0, "avg_logprob": -0.1268047014872233, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.02327006310224533}, {"id": 450, "seek": 322160, "start": 3235.92, "end": 3242.0, "text": " as well or do you need a different kind or is there a way of adapting maybe um the methods", "tokens": [51080, 382, 731, 420, 360, 291, 643, 257, 819, 733, 420, 307, 456, 257, 636, 295, 34942, 1310, 1105, 264, 7150, 51384], "temperature": 0.0, "avg_logprob": -0.1268047014872233, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.02327006310224533}, {"id": 451, "seek": 322160, "start": 3244.3199999999997, "end": 3249.92, "text": " yeah thanks so this is a very cool question of course i mean that that's the kind of question we", "tokens": [51500, 1338, 3231, 370, 341, 307, 257, 588, 1627, 1168, 295, 1164, 741, 914, 300, 300, 311, 264, 733, 295, 1168, 321, 51780], "temperature": 0.0, "avg_logprob": -0.1268047014872233, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.02327006310224533}, {"id": 452, "seek": 324992, "start": 3250.0, "end": 3257.28, "text": " i i i'm expecting in these days because we do have language models are able to achieve so much", "tokens": [50368, 741, 741, 741, 478, 9650, 294, 613, 1708, 570, 321, 360, 362, 2856, 5245, 366, 1075, 281, 4584, 370, 709, 50732], "temperature": 0.0, "avg_logprob": -0.09564214944839478, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.005340450908988714}, {"id": 453, "seek": 324992, "start": 3259.28, "end": 3265.36, "text": " that it's it's hard to think okay if i done everything wrong can they just do better than me", "tokens": [50832, 300, 309, 311, 309, 311, 1152, 281, 519, 1392, 498, 741, 1096, 1203, 2085, 393, 436, 445, 360, 1101, 813, 385, 51136], "temperature": 0.0, "avg_logprob": -0.09564214944839478, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.005340450908988714}, {"id": 454, "seek": 324992, "start": 3265.36, "end": 3272.48, "text": " and i mean i would be very curious to to just do a um a simple comparison i like this question a lot", "tokens": [51136, 293, 741, 914, 741, 576, 312, 588, 6369, 281, 281, 445, 360, 257, 1105, 257, 2199, 9660, 741, 411, 341, 1168, 257, 688, 51492], "temperature": 0.0, "avg_logprob": -0.09564214944839478, "compression_ratio": 1.5824175824175823, "no_speech_prob": 0.005340450908988714}, {"id": 455, "seek": 327248, "start": 3272.48, "end": 3280.72, "text": " so the the the question is really can we use these techniques um um with large language models", "tokens": [50364, 370, 264, 264, 264, 1168, 307, 534, 393, 321, 764, 613, 7512, 1105, 1105, 365, 2416, 2856, 5245, 50776], "temperature": 0.0, "avg_logprob": -0.1382631879103811, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0053991046734154224}, {"id": 456, "seek": 327248, "start": 3280.72, "end": 3285.52, "text": " because i i now have a number of PhD students working on these and we were discussing this just", "tokens": [50776, 570, 741, 741, 586, 362, 257, 1230, 295, 14476, 1731, 1364, 322, 613, 293, 321, 645, 10850, 341, 445, 51016], "temperature": 0.0, "avg_logprob": -0.1382631879103811, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0053991046734154224}, {"id": 457, "seek": 327248, "start": 3285.52, "end": 3293.92, "text": " this morning um the you you certainly have the advantage that you might not need to crawl", "tokens": [51016, 341, 2446, 1105, 264, 291, 291, 3297, 362, 264, 5002, 300, 291, 1062, 406, 643, 281, 24767, 51436], "temperature": 0.0, "avg_logprob": -0.1382631879103811, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0053991046734154224}, {"id": 458, "seek": 327248, "start": 3294.88, "end": 3299.12, "text": " the knowledge graph anymore to generate explanation so all the problems of", "tokens": [51484, 264, 3601, 4295, 3602, 281, 8460, 10835, 370, 439, 264, 2740, 295, 51696], "temperature": 0.0, "avg_logprob": -0.1382631879103811, "compression_ratio": 1.658878504672897, "no_speech_prob": 0.0053991046734154224}, {"id": 459, "seek": 329912, "start": 3299.8399999999997, "end": 3307.3599999999997, "text": " um heretics to search the graph um to to reduce the computational complexity", "tokens": [50400, 1105, 720, 15793, 281, 3164, 264, 4295, 1105, 281, 281, 5407, 264, 28270, 14024, 50776], "temperature": 0.0, "avg_logprob": -0.1850964864095052, "compression_ratio": 1.622754491017964, "no_speech_prob": 0.0034199021756649017}, {"id": 460, "seek": 329912, "start": 3309.12, "end": 3318.48, "text": " might not be needed anymore uh uh with that said i still think that using you you need to combine", "tokens": [50864, 1062, 406, 312, 2978, 3602, 2232, 2232, 365, 300, 848, 741, 920, 519, 300, 1228, 291, 291, 643, 281, 10432, 51332], "temperature": 0.0, "avg_logprob": -0.1850964864095052, "compression_ratio": 1.622754491017964, "no_speech_prob": 0.0034199021756649017}, {"id": 461, "seek": 329912, "start": 3318.48, "end": 3324.64, "text": " language models and knowledge graphs um in order to be able to to trace back the information and", "tokens": [51332, 2856, 5245, 293, 3601, 24877, 1105, 294, 1668, 281, 312, 1075, 281, 281, 13508, 646, 264, 1589, 293, 51640], "temperature": 0.0, "avg_logprob": -0.1850964864095052, "compression_ratio": 1.622754491017964, "no_speech_prob": 0.0034199021756649017}, {"id": 462, "seek": 332464, "start": 3324.64, "end": 3331.52, "text": " especially if we are talking about uh truthfulness of an explanation i can ask my knowledge graph to", "tokens": [50364, 2318, 498, 321, 366, 1417, 466, 2232, 3494, 26872, 295, 364, 10835, 741, 393, 1029, 452, 3601, 4295, 281, 50708], "temperature": 0.0, "avg_logprob": -0.11367992971135282, "compression_ratio": 1.7942583732057416, "no_speech_prob": 0.003035004250705242}, {"id": 463, "seek": 332464, "start": 3333.2799999999997, "end": 3338.3199999999997, "text": " the language model to to come up with an explanation for a given pattern of data but i", "tokens": [50796, 264, 2856, 2316, 281, 281, 808, 493, 365, 364, 10835, 337, 257, 2212, 5102, 295, 1412, 457, 741, 51048], "temperature": 0.0, "avg_logprob": -0.11367992971135282, "compression_ratio": 1.7942583732057416, "no_speech_prob": 0.003035004250705242}, {"id": 464, "seek": 332464, "start": 3338.3199999999997, "end": 3344.96, "text": " i also want to make sure that i can trace the provenance back and this is something that i", "tokens": [51048, 741, 611, 528, 281, 652, 988, 300, 741, 393, 13508, 264, 12785, 719, 646, 293, 341, 307, 746, 300, 741, 51380], "temperature": 0.0, "avg_logprob": -0.11367992971135282, "compression_ratio": 1.7942583732057416, "no_speech_prob": 0.003035004250705242}, {"id": 465, "seek": 332464, "start": 3345.8399999999997, "end": 3350.48, "text": " i want to know i mean probably a knowledge graph is much better to do than than a language model", "tokens": [51424, 741, 528, 281, 458, 741, 914, 1391, 257, 3601, 4295, 307, 709, 1101, 281, 360, 813, 813, 257, 2856, 2316, 51656], "temperature": 0.0, "avg_logprob": -0.11367992971135282, "compression_ratio": 1.7942583732057416, "no_speech_prob": 0.003035004250705242}, {"id": 466, "seek": 335048, "start": 3351.2, "end": 3357.2, "text": " uh so i still think that as i said the backbone of the information should come from a knowledge", "tokens": [50400, 2232, 370, 741, 920, 519, 300, 382, 741, 848, 264, 34889, 295, 264, 1589, 820, 808, 490, 257, 3601, 50700], "temperature": 0.0, "avg_logprob": -0.08644182031804865, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.0025466063525527716}, {"id": 467, "seek": 335048, "start": 3357.2, "end": 3365.6, "text": " graph in a way that you can uh you can reconstruct the subgraph somehow that generates your explanation", "tokens": [50700, 4295, 294, 257, 636, 300, 291, 393, 2232, 291, 393, 31499, 264, 1422, 34091, 6063, 300, 23815, 428, 10835, 51120], "temperature": 0.0, "avg_logprob": -0.08644182031804865, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.0025466063525527716}, {"id": 468, "seek": 335048, "start": 3365.6, "end": 3374.96, "text": " and then the output for the form it can be uh can be generated or or situated according to the", "tokens": [51120, 293, 550, 264, 5598, 337, 264, 1254, 309, 393, 312, 2232, 393, 312, 10833, 420, 420, 30143, 4650, 281, 264, 51588], "temperature": 0.0, "avg_logprob": -0.08644182031804865, "compression_ratio": 1.7093023255813953, "no_speech_prob": 0.0025466063525527716}, {"id": 469, "seek": 337496, "start": 3374.96, "end": 3384.96, "text": " users by the language model that will be my answer okay um so thanks for that there's another", "tokens": [50364, 5022, 538, 264, 2856, 2316, 300, 486, 312, 452, 1867, 1392, 1105, 370, 3231, 337, 300, 456, 311, 1071, 50864], "temperature": 0.0, "avg_logprob": -0.09624631174149052, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003215912962332368}, {"id": 470, "seek": 337496, "start": 3384.96, "end": 3391.2, "text": " question by peter jones he's asking to what extent do you think ai and explainable ai may", "tokens": [50864, 1168, 538, 280, 2398, 361, 2213, 415, 311, 3365, 281, 437, 8396, 360, 291, 519, 9783, 293, 2903, 712, 9783, 815, 51176], "temperature": 0.0, "avg_logprob": -0.09624631174149052, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003215912962332368}, {"id": 471, "seek": 337496, "start": 3391.2, "end": 3399.44, "text": " reduce or undermine the use or development of domain specific languages uh sorry i", "tokens": [51176, 5407, 420, 39257, 264, 764, 420, 3250, 295, 9274, 2685, 8650, 2232, 2597, 741, 51588], "temperature": 0.0, "avg_logprob": -0.09624631174149052, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.003215912962332368}, {"id": 472, "seek": 339944, "start": 3399.44, "end": 3407.92, "text": " lost the second part of the question so do i think the ai and explainable ai reduced the use", "tokens": [50364, 2731, 264, 1150, 644, 295, 264, 1168, 370, 360, 741, 519, 264, 9783, 293, 2903, 712, 9783, 9212, 264, 764, 50788], "temperature": 0.0, "avg_logprob": -0.11189621289571126, "compression_ratio": 1.7070063694267517, "no_speech_prob": 0.018440157175064087}, {"id": 473, "seek": 339944, "start": 3407.92, "end": 3420.0, "text": " of domain specific languages um well i don't think they actually i i'm not quite sure uh", "tokens": [50788, 295, 9274, 2685, 8650, 1105, 731, 741, 500, 380, 519, 436, 767, 741, 741, 478, 406, 1596, 988, 2232, 51392], "temperature": 0.0, "avg_logprob": -0.11189621289571126, "compression_ratio": 1.7070063694267517, "no_speech_prob": 0.018440157175064087}, {"id": 474, "seek": 339944, "start": 3420.0, "end": 3428.32, "text": " whether by domain specific language uh yeah we mean the domain specific representation", "tokens": [51392, 1968, 538, 9274, 2685, 2856, 2232, 1338, 321, 914, 264, 9274, 2685, 10290, 51808], "temperature": 0.0, "avg_logprob": -0.11189621289571126, "compression_ratio": 1.7070063694267517, "no_speech_prob": 0.018440157175064087}, {"id": 475, "seek": 342832, "start": 3428.32, "end": 3437.2000000000003, "text": " or so the main ontologies but i don't think they actually uh reduce it or at least i don't think", "tokens": [50364, 420, 370, 264, 2135, 6592, 6204, 457, 741, 500, 380, 519, 436, 767, 2232, 5407, 309, 420, 412, 1935, 741, 500, 380, 519, 50808], "temperature": 0.0, "avg_logprob": -0.15108766555786132, "compression_ratio": 1.7300613496932515, "no_speech_prob": 0.0032980870455503464}, {"id": 476, "seek": 342832, "start": 3437.2000000000003, "end": 3446.1600000000003, "text": " they should reduce it somehow um i see more an integration of the two in the sense that uh", "tokens": [50808, 436, 820, 5407, 309, 6063, 1105, 741, 536, 544, 364, 10980, 295, 264, 732, 294, 264, 2020, 300, 2232, 51256], "temperature": 0.0, "avg_logprob": -0.15108766555786132, "compression_ratio": 1.7300613496932515, "no_speech_prob": 0.0032980870455503464}, {"id": 477, "seek": 342832, "start": 3446.1600000000003, "end": 3454.1600000000003, "text": " the the the same way uh there are these methods that use so you i've seen methods using domain", "tokens": [51256, 264, 264, 264, 912, 636, 2232, 456, 366, 613, 7150, 300, 764, 370, 291, 741, 600, 1612, 7150, 1228, 9274, 51656], "temperature": 0.0, "avg_logprob": -0.15108766555786132, "compression_ratio": 1.7300613496932515, "no_speech_prob": 0.0032980870455503464}, {"id": 478, "seek": 345416, "start": 3454.16, "end": 3461.52, "text": " specific ontologies to um come up with maybe decision trees about the an explanation that is", "tokens": [50364, 2685, 6592, 6204, 281, 1105, 808, 493, 365, 1310, 3537, 5852, 466, 264, 364, 10835, 300, 307, 50732], "temperature": 0.0, "avg_logprob": -0.09849731257704437, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.006721766199916601}, {"id": 479, "seek": 345416, "start": 3461.52, "end": 3469.92, "text": " being generated so they um uh the advantage of domain specific language is still that they are", "tokens": [50732, 885, 10833, 370, 436, 1105, 2232, 264, 5002, 295, 9274, 2685, 2856, 307, 920, 300, 436, 366, 51152], "temperature": 0.0, "avg_logprob": -0.09849731257704437, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.006721766199916601}, {"id": 480, "seek": 345416, "start": 3470.8799999999997, "end": 3476.96, "text": " highly they're curated by by the experts and so they are still more reliable so the two methods", "tokens": [51200, 5405, 436, 434, 47851, 538, 538, 264, 8572, 293, 370, 436, 366, 920, 544, 12924, 370, 264, 732, 7150, 51504], "temperature": 0.0, "avg_logprob": -0.09849731257704437, "compression_ratio": 1.654970760233918, "no_speech_prob": 0.006721766199916601}, {"id": 481, "seek": 347696, "start": 3476.96, "end": 3485.44, "text": " should kind of uh i don't i don't want to think of uh of explainable ai methods as taking over", "tokens": [50364, 820, 733, 295, 2232, 741, 500, 380, 741, 500, 380, 528, 281, 519, 295, 2232, 295, 2903, 712, 9783, 7150, 382, 1940, 670, 50788], "temperature": 0.0, "avg_logprob": -0.18654083198224994, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.006453747395426035}, {"id": 482, "seek": 347696, "start": 3485.44, "end": 3492.08, "text": " but rather to try to uh complement to to to combine the two or in a in a neuro symbolic fashion", "tokens": [50788, 457, 2831, 281, 853, 281, 2232, 17103, 281, 281, 281, 10432, 264, 732, 420, 294, 257, 294, 257, 16499, 25755, 6700, 51120], "temperature": 0.0, "avg_logprob": -0.18654083198224994, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.006453747395426035}, {"id": 483, "seek": 347696, "start": 3493.84, "end": 3504.4, "text": " i hope this answer the questions uh hi yeah i hope so too um speaking of maybe maybe this", "tokens": [51208, 741, 1454, 341, 1867, 264, 1651, 2232, 4879, 1338, 741, 1454, 370, 886, 1105, 4124, 295, 1310, 1310, 341, 51736], "temperature": 0.0, "avg_logprob": -0.18654083198224994, "compression_ratio": 1.6470588235294117, "no_speech_prob": 0.006453747395426035}, {"id": 484, "seek": 350440, "start": 3504.4, "end": 3510.96, "text": " this goes one into the direction of one of the questions that uh that i uh noted um so if we", "tokens": [50364, 341, 1709, 472, 666, 264, 3513, 295, 472, 295, 264, 1651, 300, 2232, 300, 741, 2232, 12964, 1105, 370, 498, 321, 50692], "temperature": 0.0, "avg_logprob": -0.13911050841921851, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0022514385636895895}, {"id": 485, "seek": 350440, "start": 3510.96, "end": 3515.52, "text": " have if you have a domain specific language or a domain specific may of modeling things", "tokens": [50692, 362, 498, 291, 362, 257, 9274, 2685, 2856, 420, 257, 9274, 2685, 815, 295, 15983, 721, 50920], "temperature": 0.0, "avg_logprob": -0.13911050841921851, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0022514385636895895}, {"id": 486, "seek": 350440, "start": 3515.52, "end": 3524.8, "text": " then this allows to very uh concisely write down things for a specific domain um previously one", "tokens": [50920, 550, 341, 4045, 281, 588, 2232, 1588, 271, 736, 2464, 760, 721, 337, 257, 2685, 9274, 1105, 8046, 472, 51384], "temperature": 0.0, "avg_logprob": -0.13911050841921851, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0022514385636895895}, {"id": 487, "seek": 350440, "start": 3524.8, "end": 3534.1600000000003, "text": " of your explanation methods you use something that like was looking at graph uh the distance in the", "tokens": [51384, 295, 428, 10835, 7150, 291, 764, 746, 300, 411, 390, 1237, 412, 4295, 2232, 264, 4560, 294, 264, 51852], "temperature": 0.0, "avg_logprob": -0.13911050841921851, "compression_ratio": 1.8076923076923077, "no_speech_prob": 0.0022514385636895895}, {"id": 488, "seek": 353416, "start": 3534.16, "end": 3539.6, "text": " graph right and if you change like if you have something very good for one domain then that", "tokens": [50364, 4295, 558, 293, 498, 291, 1319, 411, 498, 291, 362, 746, 588, 665, 337, 472, 9274, 550, 300, 50636], "temperature": 0.0, "avg_logprob": -0.07428948329045222, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.0022168352734297514}, {"id": 489, "seek": 353416, "start": 3539.6, "end": 3547.44, "text": " obviously changes the the the distance in the graph so maybe you can reflect a little bit on how", "tokens": [50636, 2745, 2962, 264, 264, 264, 4560, 294, 264, 4295, 370, 1310, 291, 393, 5031, 257, 707, 857, 322, 577, 51028], "temperature": 0.0, "avg_logprob": -0.07428948329045222, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.0022168352734297514}, {"id": 490, "seek": 353416, "start": 3548.08, "end": 3554.7999999999997, "text": " the graph structure or the role of how model how things are modeled or how much entailment is", "tokens": [51060, 264, 4295, 3877, 420, 264, 3090, 295, 577, 2316, 577, 721, 366, 37140, 420, 577, 709, 948, 864, 518, 307, 51396], "temperature": 0.0, "avg_logprob": -0.07428948329045222, "compression_ratio": 1.688622754491018, "no_speech_prob": 0.0022168352734297514}, {"id": 491, "seek": 355480, "start": 3554.8, "end": 3563.84, "text": " applied on on the graph changes the the the distance or the results of your approach", "tokens": [50364, 6456, 322, 322, 264, 4295, 2962, 264, 264, 264, 4560, 420, 264, 3542, 295, 428, 3109, 50816], "temperature": 0.0, "avg_logprob": -0.15463131763896004, "compression_ratio": 1.6283783783783783, "no_speech_prob": 0.005114395637065172}, {"id": 492, "seek": 355480, "start": 3564.8, "end": 3571.6000000000004, "text": " yeah so i think um in general this is the kind of problem we try to to to approach", "tokens": [50864, 1338, 370, 741, 519, 1105, 294, 2674, 341, 307, 264, 733, 295, 1154, 321, 853, 281, 281, 281, 3109, 51204], "temperature": 0.0, "avg_logprob": -0.15463131763896004, "compression_ratio": 1.6283783783783783, "no_speech_prob": 0.005114395637065172}, {"id": 493, "seek": 355480, "start": 3574.8, "end": 3580.96, "text": " both so with uh say with a with a follow-up method so when we looked into", "tokens": [51364, 1293, 370, 365, 2232, 584, 365, 257, 365, 257, 1524, 12, 1010, 3170, 370, 562, 321, 2956, 666, 51672], "temperature": 0.0, "avg_logprob": -0.15463131763896004, "compression_ratio": 1.6283783783783783, "no_speech_prob": 0.005114395637065172}, {"id": 494, "seek": 358096, "start": 3581.84, "end": 3586.88, "text": " uh trying to identify strong relationships but also trying to cope with the with the", "tokens": [50408, 2232, 1382, 281, 5876, 2068, 6159, 457, 611, 1382, 281, 22598, 365, 264, 365, 264, 50660], "temperature": 0.0, "avg_logprob": -0.0909712391514932, "compression_ratio": 2.0841121495327104, "no_speech_prob": 0.001573728397488594}, {"id": 495, "seek": 358096, "start": 3586.88, "end": 3592.16, "text": " the inner bias of the information uh the assumption so we we've never dealt with", "tokens": [50660, 264, 7284, 12577, 295, 264, 1589, 2232, 264, 15302, 370, 321, 321, 600, 1128, 15991, 365, 50924], "temperature": 0.0, "avg_logprob": -0.0909712391514932, "compression_ratio": 2.0841121495327104, "no_speech_prob": 0.001573728397488594}, {"id": 496, "seek": 358096, "start": 3592.8, "end": 3598.48, "text": " knowledge graphs we created right so we always dealt with knowledge graphs that were created by", "tokens": [50956, 3601, 24877, 321, 2942, 558, 370, 321, 1009, 15991, 365, 3601, 24877, 300, 645, 2942, 538, 51240], "temperature": 0.0, "avg_logprob": -0.0909712391514932, "compression_ratio": 2.0841121495327104, "no_speech_prob": 0.001573728397488594}, {"id": 497, "seek": 358096, "start": 3598.48, "end": 3604.32, "text": " others so the assumption was you might not find the information that that you might need and you", "tokens": [51240, 2357, 370, 264, 15302, 390, 291, 1062, 406, 915, 264, 1589, 300, 300, 291, 1062, 643, 293, 291, 51532], "temperature": 0.0, "avg_logprob": -0.0909712391514932, "compression_ratio": 2.0841121495327104, "no_speech_prob": 0.001573728397488594}, {"id": 498, "seek": 358096, "start": 3604.32, "end": 3610.32, "text": " need to so sometimes the the information is very well curated and sometimes this is not", "tokens": [51532, 643, 281, 370, 2171, 264, 264, 1589, 307, 588, 731, 47851, 293, 2171, 341, 307, 406, 51832], "temperature": 0.0, "avg_logprob": -0.0909712391514932, "compression_ratio": 2.0841121495327104, "no_speech_prob": 0.001573728397488594}, {"id": 499, "seek": 361032, "start": 3610.32, "end": 3617.6000000000004, "text": " and somehow we need to find um a way to to to cope with this problem in order to be as general", "tokens": [50364, 293, 6063, 321, 643, 281, 915, 1105, 257, 636, 281, 281, 281, 22598, 365, 341, 1154, 294, 1668, 281, 312, 382, 2674, 50728], "temperature": 0.0, "avg_logprob": -0.10348520278930665, "compression_ratio": 1.6932515337423313, "no_speech_prob": 0.0010198844829574227}, {"id": 500, "seek": 361032, "start": 3617.6000000000004, "end": 3628.1600000000003, "text": " as possible so the my view is really that there is not a a universal way of so there is not a", "tokens": [50728, 382, 1944, 370, 264, 452, 1910, 307, 534, 300, 456, 307, 406, 257, 257, 11455, 636, 295, 370, 456, 307, 406, 257, 51256], "temperature": 0.0, "avg_logprob": -0.10348520278930665, "compression_ratio": 1.6932515337423313, "no_speech_prob": 0.0010198844829574227}, {"id": 501, "seek": 361032, "start": 3628.1600000000003, "end": 3636.6400000000003, "text": " method that can can deal with both the most important thing is being able to um to cope", "tokens": [51256, 3170, 300, 393, 393, 2028, 365, 1293, 264, 881, 1021, 551, 307, 885, 1075, 281, 1105, 281, 22598, 51680], "temperature": 0.0, "avg_logprob": -0.10348520278930665, "compression_ratio": 1.6932515337423313, "no_speech_prob": 0.0010198844829574227}, {"id": 502, "seek": 363664, "start": 3636.64, "end": 3640.56, "text": " with the problem and integrate it in the methods that you develop so you need to be aware that", "tokens": [50364, 365, 264, 1154, 293, 13365, 309, 294, 264, 7150, 300, 291, 1499, 370, 291, 643, 281, 312, 3650, 300, 50560], "temperature": 0.0, "avg_logprob": -0.07300199488157867, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.00948522798717022}, {"id": 503, "seek": 363664, "start": 3640.56, "end": 3647.6, "text": " information might be missing and and you need to make sure that your method compensates for that", "tokens": [50560, 1589, 1062, 312, 5361, 293, 293, 291, 643, 281, 652, 988, 300, 428, 3170, 11598, 1024, 337, 300, 50912], "temperature": 0.0, "avg_logprob": -0.07300199488157867, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.00948522798717022}, {"id": 504, "seek": 363664, "start": 3647.6, "end": 3653.44, "text": " this can happen inside the development of your method or as a postdoc it's like a posteriori", "tokens": [50912, 341, 393, 1051, 1854, 264, 3250, 295, 428, 3170, 420, 382, 257, 2183, 39966, 309, 311, 411, 257, 33529, 72, 51204], "temperature": 0.0, "avg_logprob": -0.07300199488157867, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.00948522798717022}, {"id": 505, "seek": 363664, "start": 3653.44, "end": 3661.8399999999997, "text": " step and and it can be as simple as i mean in the same view of the co-creation with the user it can", "tokens": [51204, 1823, 293, 293, 309, 393, 312, 382, 2199, 382, 741, 914, 294, 264, 912, 1910, 295, 264, 598, 12, 14066, 399, 365, 264, 4195, 309, 393, 51624], "temperature": 0.0, "avg_logprob": -0.07300199488157867, "compression_ratio": 1.8285714285714285, "no_speech_prob": 0.00948522798717022}, {"id": 506, "seek": 366184, "start": 3661.84, "end": 3667.84, "text": " be as simple as okay let's interact and see whether i'm missing some part of the information", "tokens": [50364, 312, 382, 2199, 382, 1392, 718, 311, 4648, 293, 536, 1968, 741, 478, 5361, 512, 644, 295, 264, 1589, 50664], "temperature": 0.0, "avg_logprob": -0.07033447785810991, "compression_ratio": 1.6766467065868262, "no_speech_prob": 0.0075464872643351555}, {"id": 507, "seek": 366184, "start": 3670.6400000000003, "end": 3676.8, "text": " so so somehow one of the reasons why we had to look into the strength of the relationship was", "tokens": [50804, 370, 370, 6063, 472, 295, 264, 4112, 983, 321, 632, 281, 574, 666, 264, 3800, 295, 264, 2480, 390, 51112], "temperature": 0.0, "avg_logprob": -0.07033447785810991, "compression_ratio": 1.6766467065868262, "no_speech_prob": 0.0075464872643351555}, {"id": 508, "seek": 366184, "start": 3676.8, "end": 3684.4, "text": " also because we were missing these and we had to find a strategy to to survive in the in this", "tokens": [51112, 611, 570, 321, 645, 5361, 613, 293, 321, 632, 281, 915, 257, 5206, 281, 281, 7867, 294, 264, 294, 341, 51492], "temperature": 0.0, "avg_logprob": -0.07033447785810991, "compression_ratio": 1.6766467065868262, "no_speech_prob": 0.0075464872643351555}, {"id": 509, "seek": 368440, "start": 3684.96, "end": 3695.6800000000003, "text": " case okay um there's another question from ask kim star and uh they are asking what", "tokens": [50392, 1389, 1392, 1105, 456, 311, 1071, 1168, 490, 1029, 10776, 3543, 293, 2232, 436, 366, 3365, 437, 50928], "temperature": 0.0, "avg_logprob": -0.24675463162935696, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.018219931051135063}, {"id": 510, "seek": 368440, "start": 3695.6800000000003, "end": 3702.1600000000003, "text": " adaptions do you see for knowledge for for this whole set of approaches to deal with multimodality", "tokens": [50928, 6231, 626, 360, 291, 536, 337, 3601, 337, 337, 341, 1379, 992, 295, 11587, 281, 2028, 365, 32972, 378, 1860, 51252], "temperature": 0.0, "avg_logprob": -0.24675463162935696, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.018219931051135063}, {"id": 511, "seek": 368440, "start": 3704.32, "end": 3712.96, "text": " um sorry so the so you had so you had like there was this with the cat picture where there", "tokens": [51360, 1105, 2597, 370, 264, 370, 291, 632, 370, 291, 632, 411, 456, 390, 341, 365, 264, 3857, 3036, 689, 456, 51792], "temperature": 0.0, "avg_logprob": -0.24675463162935696, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.018219931051135063}, {"id": 512, "seek": 371296, "start": 3712.96, "end": 3722.08, "text": " was some computer vision aspects in it um and maybe you reflect a bit on on multimodality when", "tokens": [50364, 390, 512, 3820, 5201, 7270, 294, 309, 1105, 293, 1310, 291, 5031, 257, 857, 322, 322, 32972, 378, 1860, 562, 50820], "temperature": 0.0, "avg_logprob": -0.16479616694980198, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.00025978797930292785}, {"id": 513, "seek": 371296, "start": 3722.08, "end": 3730.2400000000002, "text": " it comes so it's uh it's through that we've never uh it's the kind of knowledge graph we've been", "tokens": [50820, 309, 1487, 370, 309, 311, 2232, 309, 311, 807, 300, 321, 600, 1128, 2232, 309, 311, 264, 733, 295, 3601, 4295, 321, 600, 668, 51228], "temperature": 0.0, "avg_logprob": -0.16479616694980198, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.00025978797930292785}, {"id": 514, "seek": 371296, "start": 3730.2400000000002, "end": 3738.88, "text": " dealing with and and multi multi-modal knowledge graphs were not really um uh um that common at", "tokens": [51228, 6260, 365, 293, 293, 4825, 4825, 12, 8014, 304, 3601, 24877, 645, 406, 534, 1105, 2232, 1105, 300, 2689, 412, 51660], "temperature": 0.0, "avg_logprob": -0.16479616694980198, "compression_ratio": 1.6306818181818181, "no_speech_prob": 0.00025978797930292785}, {"id": 515, "seek": 373888, "start": 3738.96, "end": 3745.76, "text": " that time i think uh so so it's through that one aspect that would be interesting and maybe", "tokens": [50368, 300, 565, 741, 519, 2232, 370, 370, 309, 311, 807, 300, 472, 4171, 300, 576, 312, 1880, 293, 1310, 50708], "temperature": 0.0, "avg_logprob": -0.11729134212840688, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0027653584256768227}, {"id": 516, "seek": 373888, "start": 3746.6400000000003, "end": 3753.44, "text": " we kind of we didn't discuss this as as in the future step but it is in these days we are now", "tokens": [50752, 321, 733, 295, 321, 994, 380, 2248, 341, 382, 382, 294, 264, 2027, 1823, 457, 309, 307, 294, 613, 1708, 321, 366, 586, 51092], "temperature": 0.0, "avg_logprob": -0.11729134212840688, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0027653584256768227}, {"id": 517, "seek": 373888, "start": 3753.44, "end": 3761.6800000000003, "text": " talking about we hear much more about multimodal knowledge graphs we hear um also about knowledge", "tokens": [51092, 1417, 466, 321, 1568, 709, 544, 466, 32972, 378, 304, 3601, 24877, 321, 1568, 1105, 611, 466, 3601, 51504], "temperature": 0.0, "avg_logprob": -0.11729134212840688, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0027653584256768227}, {"id": 518, "seek": 373888, "start": 3761.6800000000003, "end": 3767.12, "text": " graph that somehow try to integrate the physical words so like we deal with robots in some scenarios", "tokens": [51504, 4295, 300, 6063, 853, 281, 13365, 264, 4001, 2283, 370, 411, 321, 2028, 365, 14733, 294, 512, 15077, 51776], "temperature": 0.0, "avg_logprob": -0.11729134212840688, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.0027653584256768227}, {"id": 519, "seek": 376712, "start": 3767.12, "end": 3772.24, "text": " and we also have this problem of okay i have a knowledge graph that has to integrate both", "tokens": [50364, 293, 321, 611, 362, 341, 1154, 295, 1392, 741, 362, 257, 3601, 4295, 300, 575, 281, 13365, 1293, 50620], "temperature": 0.0, "avg_logprob": -0.07914202321659435, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.001979823224246502}, {"id": 520, "seek": 376712, "start": 3772.24, "end": 3781.04, "text": " abstract concepts but also the physical word um so um uh i think it would be quite interesting to", "tokens": [50620, 12649, 10392, 457, 611, 264, 4001, 1349, 1105, 370, 1105, 2232, 741, 519, 309, 576, 312, 1596, 1880, 281, 51060], "temperature": 0.0, "avg_logprob": -0.07914202321659435, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.001979823224246502}, {"id": 521, "seek": 376712, "start": 3781.04, "end": 3787.3599999999997, "text": " think on how to generate explanations that are multimodal in this sense uh this is one part of", "tokens": [51060, 519, 322, 577, 281, 8460, 28708, 300, 366, 32972, 378, 304, 294, 341, 2020, 2232, 341, 307, 472, 644, 295, 51376], "temperature": 0.0, "avg_logprob": -0.07914202321659435, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.001979823224246502}, {"id": 522, "seek": 376712, "start": 3787.3599999999997, "end": 3792.7999999999997, "text": " the answer in the sense that yes we didn't look into multimodal knowledge graphs and this it could", "tokens": [51376, 264, 1867, 294, 264, 2020, 300, 2086, 321, 994, 380, 574, 666, 32972, 378, 304, 3601, 24877, 293, 341, 309, 727, 51648], "temperature": 0.0, "avg_logprob": -0.07914202321659435, "compression_ratio": 1.7477064220183487, "no_speech_prob": 0.001979823224246502}, {"id": 523, "seek": 379280, "start": 3792.88, "end": 3800.8, "text": " change uh the other part of the answer is uh uh the explanation that we could generate", "tokens": [50368, 1319, 2232, 264, 661, 644, 295, 264, 1867, 307, 2232, 2232, 264, 10835, 300, 321, 727, 8460, 50764], "temperature": 0.0, "avg_logprob": -0.1047856229724306, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.00648835301399231}, {"id": 524, "seek": 379280, "start": 3801.44, "end": 3807.6800000000003, "text": " so the kind of patterns that were coming out could also be patterns coming from from multimodal", "tokens": [50796, 370, 264, 733, 295, 8294, 300, 645, 1348, 484, 727, 611, 312, 8294, 1348, 490, 490, 32972, 378, 304, 51108], "temperature": 0.0, "avg_logprob": -0.1047856229724306, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.00648835301399231}, {"id": 525, "seek": 379280, "start": 3807.6800000000003, "end": 3816.32, "text": " data so like the uh i dealt a lot with clusters of of of data points but these data points could as", "tokens": [51108, 1412, 370, 411, 264, 2232, 741, 15991, 257, 688, 365, 23313, 295, 295, 295, 1412, 2793, 457, 613, 1412, 2793, 727, 382, 51540], "temperature": 0.0, "avg_logprob": -0.1047856229724306, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.00648835301399231}, {"id": 526, "seek": 381632, "start": 3816.32, "end": 3824.0800000000004, "text": " well be um uh paths of an image for example that will represent that will represent i don't know", "tokens": [50364, 731, 312, 1105, 2232, 14518, 295, 364, 3256, 337, 1365, 300, 486, 2906, 300, 486, 2906, 741, 500, 380, 458, 50752], "temperature": 0.0, "avg_logprob": -0.13926435771741366, "compression_ratio": 1.7341040462427746, "no_speech_prob": 0.0052445316687226295}, {"id": 527, "seek": 381632, "start": 3824.0800000000004, "end": 3829.92, "text": " the ear or the the tail of the cut of this kind of thing so i didn't deal with that concretely but", "tokens": [50752, 264, 1273, 420, 264, 264, 6838, 295, 264, 1723, 295, 341, 733, 295, 551, 370, 741, 994, 380, 2028, 365, 300, 39481, 736, 457, 51044], "temperature": 0.0, "avg_logprob": -0.13926435771741366, "compression_ratio": 1.7341040462427746, "no_speech_prob": 0.0052445316687226295}, {"id": 528, "seek": 381632, "start": 3829.92, "end": 3841.6000000000004, "text": " it could uh i i don't see why the method shouldn't work on um image uh labels uh with yeah with specific", "tokens": [51044, 309, 727, 2232, 741, 741, 500, 380, 536, 983, 264, 3170, 4659, 380, 589, 322, 1105, 3256, 2232, 16949, 2232, 365, 1338, 365, 2685, 51628], "temperature": 0.0, "avg_logprob": -0.13926435771741366, "compression_ratio": 1.7341040462427746, "no_speech_prob": 0.0052445316687226295}, {"id": 529, "seek": 384160, "start": 3841.6, "end": 3847.6, "text": " information that we might want to explain uh so that say with respect to multimodality there are", "tokens": [50364, 1589, 300, 321, 1062, 528, 281, 2903, 2232, 370, 300, 584, 365, 3104, 281, 32972, 378, 1860, 456, 366, 50664], "temperature": 0.0, "avg_logprob": -0.06622918085618452, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003721716348081827}, {"id": 530, "seek": 384160, "start": 3847.6, "end": 3853.04, "text": " these two parts so the the the original method would probably work also on data points coming up", "tokens": [50664, 613, 732, 3166, 370, 264, 264, 264, 3380, 3170, 576, 1391, 589, 611, 322, 1412, 2793, 1348, 493, 50936], "temperature": 0.0, "avg_logprob": -0.06622918085618452, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003721716348081827}, {"id": 531, "seek": 384160, "start": 3853.04, "end": 3859.68, "text": " from multimodal data and then the multimodal knowledge graph we didn't look that much into it", "tokens": [50936, 490, 32972, 378, 304, 1412, 293, 550, 264, 32972, 378, 304, 3601, 4295, 321, 994, 380, 574, 300, 709, 666, 309, 51268], "temperature": 0.0, "avg_logprob": -0.06622918085618452, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003721716348081827}, {"id": 532, "seek": 384160, "start": 3859.68, "end": 3865.2, "text": " and it could be quite interesting to to look into multimodal explanation in this sense so that's", "tokens": [51268, 293, 309, 727, 312, 1596, 1880, 281, 281, 574, 666, 32972, 378, 304, 10835, 294, 341, 2020, 370, 300, 311, 51544], "temperature": 0.0, "avg_logprob": -0.06622918085618452, "compression_ratio": 1.7777777777777777, "no_speech_prob": 0.003721716348081827}, {"id": 533, "seek": 386520, "start": 3865.2799999999997, "end": 3875.3599999999997, "text": " okay thanks um so maybe as a as a last question jumping back to your history um and because this", "tokens": [50368, 1392, 3231, 1105, 370, 1310, 382, 257, 382, 257, 1036, 1168, 11233, 646, 281, 428, 2503, 1105, 293, 570, 341, 50872], "temperature": 0.0, "avg_logprob": -0.1151617020368576, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.003943189978599548}, {"id": 534, "seek": 386520, "start": 3875.3599999999997, "end": 3882.64, "text": " is the cost action on distributed knowledge graph where we uh care about distributed decentralized", "tokens": [50872, 307, 264, 2063, 3069, 322, 12631, 3601, 4295, 689, 321, 2232, 1127, 466, 12631, 32870, 51236], "temperature": 0.0, "avg_logprob": -0.1151617020368576, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.003943189978599548}, {"id": 535, "seek": 386520, "start": 3882.64, "end": 3889.4399999999996, "text": " things so you you had this one approach that was based on dereferencing of your eyes and looking at", "tokens": [51236, 721, 370, 291, 291, 632, 341, 472, 3109, 300, 390, 2361, 322, 15969, 612, 13644, 295, 428, 2575, 293, 1237, 412, 51576], "temperature": 0.0, "avg_logprob": -0.1151617020368576, "compression_ratio": 1.620879120879121, "no_speech_prob": 0.003943189978599548}, {"id": 536, "seek": 388944, "start": 3889.44, "end": 3897.92, "text": " things from from that perspective now um you kind of gave me the impression that you say okay now the", "tokens": [50364, 721, 490, 490, 300, 4585, 586, 1105, 291, 733, 295, 2729, 385, 264, 9995, 300, 291, 584, 1392, 586, 264, 50788], "temperature": 0.0, "avg_logprob": -0.06483201647913733, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.0050597586669027805}, {"id": 537, "seek": 388944, "start": 3897.92, "end": 3903.44, "text": " person who developed the large language model did the web crawling for you but maybe you can say", "tokens": [50788, 954, 567, 4743, 264, 2416, 2856, 2316, 630, 264, 3670, 32979, 337, 291, 457, 1310, 291, 393, 584, 51064], "temperature": 0.0, "avg_logprob": -0.06483201647913733, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.0050597586669027805}, {"id": 538, "seek": 388944, "start": 3903.44, "end": 3910.0, "text": " something like all the methods that you have developed in the meantime after you were done", "tokens": [51064, 746, 411, 439, 264, 7150, 300, 291, 362, 4743, 294, 264, 14991, 934, 291, 645, 1096, 51392], "temperature": 0.0, "avg_logprob": -0.06483201647913733, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.0050597586669027805}, {"id": 539, "seek": 388944, "start": 3910.0, "end": 3917.2000000000003, "text": " with dereferencing your eyes are they based on a global kind of view uh to i don't know generate", "tokens": [51392, 365, 15969, 612, 13644, 428, 2575, 366, 436, 2361, 322, 257, 4338, 733, 295, 1910, 2232, 281, 741, 500, 380, 458, 8460, 51752], "temperature": 0.0, "avg_logprob": -0.06483201647913733, "compression_ratio": 1.746606334841629, "no_speech_prob": 0.0050597586669027805}, {"id": 540, "seek": 391720, "start": 3917.2, "end": 3922.8799999999997, "text": " embeddings and things um or would they still work in a distributed and decentralized setting", "tokens": [50364, 12240, 29432, 293, 721, 1105, 420, 576, 436, 920, 589, 294, 257, 12631, 293, 32870, 3287, 50648], "temperature": 0.0, "avg_logprob": -0.07444330778988925, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.0008541897404938936}, {"id": 541, "seek": 391720, "start": 3924.08, "end": 3933.2, "text": " i i guess as long as uh so the the embeddings are convenient because they can embed a lot of", "tokens": [50708, 741, 741, 2041, 382, 938, 382, 2232, 370, 264, 264, 12240, 29432, 366, 10851, 570, 436, 393, 12240, 257, 688, 295, 51164], "temperature": 0.0, "avg_logprob": -0.07444330778988925, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.0008541897404938936}, {"id": 542, "seek": 391720, "start": 3933.2, "end": 3939.2, "text": " information in in a very small space and that's something that we didn't have so we had to come", "tokens": [51164, 1589, 294, 294, 257, 588, 1359, 1901, 293, 300, 311, 746, 300, 321, 994, 380, 362, 370, 321, 632, 281, 808, 51464], "temperature": 0.0, "avg_logprob": -0.07444330778988925, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.0008541897404938936}, {"id": 543, "seek": 391720, "start": 3939.2, "end": 3946.3999999999996, "text": " up with a different dereferencing opportunity but also we didn't want to deal with uh storing the", "tokens": [51464, 493, 365, 257, 819, 15969, 612, 13644, 2650, 457, 611, 321, 994, 380, 528, 281, 2028, 365, 2232, 26085, 264, 51824], "temperature": 0.0, "avg_logprob": -0.07444330778988925, "compression_ratio": 1.730593607305936, "no_speech_prob": 0.0008541897404938936}, {"id": 544, "seek": 394640, "start": 3946.4, "end": 3950.96, "text": " graph because if you store the graph and you query it then you then you you need to know the", "tokens": [50364, 4295, 570, 498, 291, 3531, 264, 4295, 293, 291, 14581, 309, 550, 291, 550, 291, 291, 643, 281, 458, 264, 50592], "temperature": 0.0, "avg_logprob": -0.1022794246673584, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0027294987812638283}, {"id": 545, "seek": 394640, "start": 3950.96, "end": 3956.64, "text": " data model and we didn't care about it so what we really cared about was this kind of serendipitous", "tokens": [50592, 1412, 2316, 293, 321, 994, 380, 1127, 466, 309, 370, 437, 321, 534, 19779, 466, 390, 341, 733, 295, 816, 521, 647, 39831, 50876], "temperature": 0.0, "avg_logprob": -0.1022794246673584, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0027294987812638283}, {"id": 546, "seek": 394640, "start": 3956.64, "end": 3964.8, "text": " hope um i remember one of the first papers i i saw was um something all of also did on on on", "tokens": [50876, 1454, 1105, 741, 1604, 472, 295, 264, 700, 10577, 741, 741, 1866, 390, 1105, 746, 439, 295, 611, 630, 322, 322, 322, 51284], "temperature": 0.0, "avg_logprob": -0.1022794246673584, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0027294987812638283}, {"id": 547, "seek": 394640, "start": 3964.8, "end": 3973.2000000000003, "text": " navigation query languages so that was quite quite quite related um so i think the the most", "tokens": [51284, 17346, 14581, 8650, 370, 300, 390, 1596, 1596, 1596, 4077, 1105, 370, 741, 519, 264, 264, 881, 51704], "temperature": 0.0, "avg_logprob": -0.1022794246673584, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0027294987812638283}, {"id": 548, "seek": 397320, "start": 3973.2, "end": 3981.2799999999997, "text": " important the the part that would still work is even in a distributed context is as long as you", "tokens": [50364, 1021, 264, 264, 644, 300, 576, 920, 589, 307, 754, 294, 257, 12631, 4319, 307, 382, 938, 382, 291, 50768], "temperature": 0.0, "avg_logprob": -0.14821207863943917, "compression_ratio": 1.6647398843930636, "no_speech_prob": 0.006205142941325903}, {"id": 549, "seek": 397320, "start": 3981.2799999999997, "end": 3989.04, "text": " have connections or pointers to to from data to data then it's fine it doesn't really matter and uh", "tokens": [50768, 362, 9271, 420, 44548, 281, 281, 490, 1412, 281, 1412, 550, 309, 311, 2489, 309, 1177, 380, 534, 1871, 293, 2232, 51156], "temperature": 0.0, "avg_logprob": -0.14821207863943917, "compression_ratio": 1.6647398843930636, "no_speech_prob": 0.006205142941325903}, {"id": 550, "seek": 397320, "start": 3989.04, "end": 3996.0, "text": " and we we didn't care whether it was the web of data or it could have been the load and load", "tokens": [51156, 293, 321, 321, 994, 380, 1127, 1968, 309, 390, 264, 3670, 295, 1412, 420, 309, 727, 362, 668, 264, 3677, 293, 3677, 51504], "temperature": 0.0, "avg_logprob": -0.14821207863943917, "compression_ratio": 1.6647398843930636, "no_speech_prob": 0.006205142941325903}, {"id": 551, "seek": 399600, "start": 3996.96, "end": 4004.16, "text": " stored in a in an hdt file um that was really not the the problem the main problem was uh", "tokens": [50412, 12187, 294, 257, 294, 364, 276, 39488, 3991, 1105, 300, 390, 534, 406, 264, 264, 1154, 264, 2135, 1154, 390, 2232, 50772], "temperature": 0.0, "avg_logprob": -0.15395637879888696, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.028838494792580605}, {"id": 552, "seek": 399600, "start": 4004.96, "end": 4010.72, "text": " what where is the irrelevant information uh how do i identify the the links between the", "tokens": [50812, 437, 689, 307, 264, 28682, 1589, 2232, 577, 360, 741, 5876, 264, 264, 6123, 1296, 264, 51100], "temperature": 0.0, "avg_logprob": -0.15395637879888696, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.028838494792580605}, {"id": 553, "seek": 399600, "start": 4010.72, "end": 4016.48, "text": " between that and this is valid in any context do i whether i have the information centralized or", "tokens": [51100, 1296, 300, 293, 341, 307, 7363, 294, 604, 4319, 360, 741, 1968, 741, 362, 264, 1589, 32395, 420, 51388], "temperature": 0.0, "avg_logprob": -0.15395637879888696, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.028838494792580605}, {"id": 554, "seek": 399600, "start": 4016.48, "end": 4022.24, "text": " decentralized i would say then i have really experimented with that so i can't tell but", "tokens": [51388, 32870, 741, 576, 584, 550, 741, 362, 534, 5120, 292, 365, 300, 370, 741, 393, 380, 980, 457, 51676], "temperature": 0.0, "avg_logprob": -0.15395637879888696, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.028838494792580605}, {"id": 555, "seek": 402224, "start": 4022.3999999999996, "end": 4032.3999999999996, "text": " so good um yeah thanks for the answer um i don't have more questions from the youtube chat um", "tokens": [50372, 370, 665, 1105, 1338, 3231, 337, 264, 1867, 1105, 741, 500, 380, 362, 544, 1651, 490, 264, 12487, 5081, 1105, 50872], "temperature": 0.0, "avg_logprob": -0.1295373003247758, "compression_ratio": 1.6473988439306357, "no_speech_prob": 0.005718096159398556}, {"id": 556, "seek": 402224, "start": 4033.7599999999998, "end": 4040.64, "text": " with that i just want you to thank you again for the very nice talk and the very nice tna session", "tokens": [50940, 365, 300, 741, 445, 528, 291, 281, 1309, 291, 797, 337, 264, 588, 1481, 751, 293, 264, 588, 1481, 256, 629, 5481, 51284], "temperature": 0.0, "avg_logprob": -0.1295373003247758, "compression_ratio": 1.6473988439306357, "no_speech_prob": 0.005718096159398556}, {"id": 557, "seek": 402224, "start": 4042.0, "end": 4048.4799999999996, "text": " before we close the stream i can only advertise the next talk on january the 31st at the same", "tokens": [51352, 949, 321, 1998, 264, 4309, 741, 393, 787, 35379, 264, 958, 751, 322, 25442, 6164, 264, 10353, 372, 412, 264, 912, 51676], "temperature": 0.0, "avg_logprob": -0.1295373003247758, "compression_ratio": 1.6473988439306357, "no_speech_prob": 0.005718096159398556}, {"id": 558, "seek": 404848, "start": 4048.48, "end": 4057.52, "text": " time as today by mayank um you can check out our website and find uh follow us on this thing", "tokens": [50364, 565, 382, 965, 538, 815, 657, 1105, 291, 393, 1520, 484, 527, 3144, 293, 915, 2232, 1524, 505, 322, 341, 551, 50816], "temperature": 0.0, "avg_logprob": -0.17138734730807217, "compression_ratio": 1.4846153846153847, "no_speech_prob": 0.02553509920835495}, {"id": 559, "seek": 404848, "start": 4057.52, "end": 4064.4, "text": " formerly called twitter and of course you find all the recordings of our talk on our youtube channel", "tokens": [50816, 34777, 1219, 21439, 293, 295, 1164, 291, 915, 439, 264, 25162, 295, 527, 751, 322, 527, 12487, 2269, 51160], "temperature": 0.0, "avg_logprob": -0.17138734730807217, "compression_ratio": 1.4846153846153847, "no_speech_prob": 0.02553509920835495}], "language": "en"}