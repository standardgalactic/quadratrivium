1
00:00:00,000 --> 00:00:09,360
Okay, so welcome everybody for tonight's talk. Today we have Ilaria and she was going to talk

2
00:00:09,360 --> 00:00:16,640
about explainability with knowledge graphs. What have we learned? This is the talk series of the

3
00:00:16,640 --> 00:00:23,680
cost action on distributed knowledge graphs. We've had a few talks already. You can watch them in our

4
00:00:23,680 --> 00:00:31,040
YouTube channel and we also have a few talks coming up on January the 31st. We will have a

5
00:00:31,040 --> 00:00:38,720
talk by Mayankerival on March the 4th. We will have a talk by Marc Neusen and on April the 17th

6
00:00:38,720 --> 00:00:48,720
we have a talk by Peter Patelschneider. What is this cost action? It is a European research network

7
00:00:49,520 --> 00:00:54,640
where more than 30 countries are represented. We run workshops, hackathons and short term

8
00:00:54,640 --> 00:01:03,440
scientific missions for which also you can apply. This overall runs from 2020 to 2024.

9
00:01:04,160 --> 00:01:13,920
So we have a few months still on the project and we are now in our final year. This is chaired by

10
00:01:13,920 --> 00:01:19,520
myself. This cost action with Axel Polares as the vice chair. We have Michel Dumontier and

11
00:01:19,520 --> 00:01:28,480
Renzi as working group leads next to Andreas Antoine Olaf. Then there is John as a scientific

12
00:01:28,480 --> 00:01:34,000
representative of the grant holder Anastasia Dimou as the science communication manager

13
00:01:34,000 --> 00:01:42,320
and Stefan Gostowicz as the grant coordinator. So much on the cost action. Today we have Ilaria

14
00:01:42,400 --> 00:01:51,840
and I would like to hand over to Olaf to announce the speaker. Yes, hello and welcome from my side

15
00:01:51,840 --> 00:02:02,160
as well. I'm very happy to have Ilaria as a speaker. She got her PhD from the Open University in the

16
00:02:02,160 --> 00:02:09,680
UK and the title of the PhD was explaining data patterns using knowledge from the web of data

17
00:02:10,560 --> 00:02:16,720
and for the PhD she received this distinguished dissertation award of the Semantic Web Science

18
00:02:16,720 --> 00:02:28,720
Association of Swizzers. She got this award in 2017 and as you maybe know we run this talk series

19
00:02:28,720 --> 00:02:34,080
with speakers who received either this dissertation award or they received the 10 years award of

20
00:02:34,080 --> 00:02:40,400
Swizzers. So now Ilaria received this award in 2017. One year later she went to Amsterdam

21
00:02:40,960 --> 00:02:48,640
to the Free University in Amsterdam where she became an assistant professor there that she still is.

22
00:02:50,400 --> 00:02:59,040
She has been involved in several conferences. She is the editor-in-chief for the CEWS

23
00:02:59,840 --> 00:03:06,000
workshop proceedings where many of us probably publish their proceedings of their workshops.

24
00:03:06,000 --> 00:03:10,400
She's also in the steering committee at the moment of the Hybrid Human AI Conference

25
00:03:11,200 --> 00:03:17,840
and her research, the focus of her research is on systems that combine semantic technologies,

26
00:03:17,840 --> 00:03:25,120
open data, machine learning in order to generate complex narratives with applications mostly in

27
00:03:25,120 --> 00:03:31,440
scientific scenarios and in robotics and if you look at kind of her most cited papers many of them

28
00:03:31,440 --> 00:03:39,040
have the word explain or explaining explanation in their title. So I'm very happy to have her here

29
00:03:39,040 --> 00:03:45,040
and kind of reflect on the work that she did in the PhD and what has happened since then with this

30
00:03:45,040 --> 00:03:54,400
work. The floor is yours Ilaria. Thank you so much. Thank you for all of Tobias for the introduction.

31
00:03:55,600 --> 00:04:01,520
Yeah I'm going to share my screen so my question was whether you are keeping the rights.

32
00:04:02,240 --> 00:04:09,120
Can you just give me a sign that you can see that I think okay. Yeah great. So

33
00:04:12,000 --> 00:04:17,760
good evening or good morning everybody. My name is Ilaria and I work at the Free University in

34
00:04:17,760 --> 00:04:26,640
Amsterdam. I'm an assistant professor. I'm very very happy of giving this talk today. There was

35
00:04:26,640 --> 00:04:34,640
really a nice opportunity for me to reflect on the overall research that I've been doing in

36
00:04:36,640 --> 00:04:43,920
the past since I was a PhD student and of course it's about explanations which is a very hot topic

37
00:04:43,920 --> 00:04:50,720
in these days and really my work and my background is on knowledge graphs so I've been working most

38
00:04:50,720 --> 00:04:55,440
of my life on how to use knowledge graphs in the context of generating explanations

39
00:04:56,080 --> 00:05:06,960
and that's what we are going to see a bit. So I just to all I've gave her a very good introduction

40
00:05:06,960 --> 00:05:12,080
of myself as I said I'm an assistant professor and have read intelligence. My background is on

41
00:05:12,080 --> 00:05:18,720
knowledge representation, knowledge graphs, knowledge graphs for explainable AI. These

42
00:05:18,720 --> 00:05:25,360
days I'm mostly focusing on my use cases around hybrid intelligence. You will hear a bit more

43
00:05:25,360 --> 00:05:32,400
later on knowledge representation driven robotics and also scientific assistance and scientific

44
00:05:32,400 --> 00:05:41,360
discovery. There's more about me you can check on the on my website. This slide is actually wrong

45
00:05:41,360 --> 00:05:48,240
so it's IWC 2017 that I missed the one when I was about to receive supposed to come on stage and

46
00:05:48,240 --> 00:05:56,960
receive the SWSA dissertation award. I was actually driving to a very far plate so I had to miss the

47
00:05:56,960 --> 00:06:06,560
conference but I was very happy of receiving the news nevertheless. So to give you an overview

48
00:06:06,560 --> 00:06:16,720
of what this talk is going to be about I will be describing a bit what's the work during my PhD was

49
00:06:16,720 --> 00:06:25,120
so as as you heard from all of the title of my thesis was on explaining data patterns using

50
00:06:25,120 --> 00:06:32,400
knowledge from the web of data and then I also want to discuss a bit what has going on since

51
00:06:32,400 --> 00:06:40,640
then so since 2017 2016-17 what how explanations have developed how our knowledge graphs have

52
00:06:40,640 --> 00:06:47,280
developed and the overall area and also some and concluding with some future consideration

53
00:06:47,280 --> 00:06:57,280
of where do we want to go and and and how can we go from here. Starting with about 10 years ago

54
00:06:57,280 --> 00:07:06,960
a bit more it's always hard to realize it over 10 years now. I like to show these pictures

55
00:07:06,960 --> 00:07:12,400
or map a bit the audience I am not sure I can actually see the chat but maybe somebody wants to

56
00:07:12,400 --> 00:07:18,880
I like to ask the question on whether somebody knows this picture which it's quite famous used to

57
00:07:18,880 --> 00:07:22,080
be quite famous I don't know whether somebody is familiar with that

58
00:07:26,560 --> 00:07:36,800
and this might be how I don't really see the chat but that's okay. So this is a typical

59
00:07:38,160 --> 00:07:44,640
knowledge discovery process as was firstly introduced around in the middle of the 90s

60
00:07:45,120 --> 00:07:53,040
when knowledge discovery was one of the main scientific processes that

61
00:07:55,600 --> 00:08:04,400
scientists were using computer science method 4 and the process was actually was

62
00:08:04,400 --> 00:08:11,760
firstly introduced by Fayyad so you can look up the reference and it used to be described as

63
00:08:11,760 --> 00:08:19,120
all the steps that scientists need to produce in order to go from data into knowledge

64
00:08:19,120 --> 00:08:25,520
and these steps were mostly selection of the data processing of the data transformation

65
00:08:25,520 --> 00:08:35,040
data mining that was what came before the current machine learning and deep learning method

66
00:08:35,280 --> 00:08:42,720
and then interpretation of an evaluation of the patterns so every step in every step you do

67
00:08:42,720 --> 00:08:49,920
you transform your data little by little first you identify the relevant information then you

68
00:08:49,920 --> 00:08:56,320
preprocess your data you transform them into something that you give to a data mining algorithm

69
00:08:56,320 --> 00:09:02,240
and then you come up with patterns that then need an interpretation and an evaluation in order to

70
00:09:02,240 --> 00:09:16,080
become knowledge. Now what we focused on is mostly this interpretation process so the last step

71
00:09:16,800 --> 00:09:23,280
the interpretation of the patterns was somehow the core of the scientific process was what would

72
00:09:23,280 --> 00:09:31,840
allow to transform patterns into into knowledge so to give a meaning to these patterns this is

73
00:09:31,920 --> 00:09:39,520
actually how interpretation is being defined somehow in the dictionary so it's really the

74
00:09:39,520 --> 00:09:48,320
action of capturing the meaning and communicating conveying the meaning of something usually the

75
00:09:48,320 --> 00:09:57,360
way you interpret patterns so the way you try to give a meaning to that to capture this meaning

76
00:09:57,360 --> 00:10:02,960
is by using your own background knowledge so you come up with patterns you might have

77
00:10:03,680 --> 00:10:08,640
your own background knowledge or maybe human experts in a topic that explain the patterns

78
00:10:08,640 --> 00:10:16,480
that the data mining algorithm has come up with and that helps evaluating and interpreting this

79
00:10:16,480 --> 00:10:24,800
knowledge the problem is this background knowledge might be missing so you might require maybe the

80
00:10:24,800 --> 00:10:32,240
expert doesn't know the actual explanation or you might need in certain use cases you might need

81
00:10:32,240 --> 00:10:38,560
different experts from different domains so gathering this background knowledge might be

82
00:10:38,560 --> 00:10:48,720
quite a time consuming process in this case when this information is missing the kind of

83
00:10:48,800 --> 00:10:59,040
hypothesis that we put forward was that symbolic AI so symbols or knowledge graphs or linked open

84
00:10:59,040 --> 00:11:06,480
data as we used to call them at that time are another source of background knowledge so the

85
00:11:06,480 --> 00:11:17,280
kind of idea that we had is okay if we have plenty of multiple of knowledge graph of data sources

86
00:11:17,280 --> 00:11:23,600
that are multi-domain that are connected between each other so at that time we had the link data

87
00:11:23,600 --> 00:11:29,440
cloud I think this is this picture is slightly later than 2013 but the idea is to have all these

88
00:11:29,440 --> 00:11:38,000
connected data sets that point to each other and then you can continuously discover knowledge simply

89
00:11:38,000 --> 00:11:44,320
by crawling data set one after the other this information is most of the time it's connected

90
00:11:44,320 --> 00:11:52,320
it's centralized in hubs and observatories it's standardized according to certain vocabularies

91
00:11:52,320 --> 00:12:03,040
that allow modeling data sharing data so the the vocabularies would allow to to to have inter

92
00:12:03,040 --> 00:12:10,160
operability across application then the kind of idea was okay maybe we can use this all this

93
00:12:10,160 --> 00:12:18,800
information which is available online to help us explaining the patterns that an algorithm gives us

94
00:12:19,600 --> 00:12:23,760
whenever we don't have an expert in the picture or whenever we are missing the background knowledge

95
00:12:23,760 --> 00:12:34,560
to explain that and and it is very similar to the idea that Newell and Simon already in the 70s had

96
00:12:35,520 --> 00:12:45,280
so among the the faders in AI that symbols were one additional layer to to use when somebody

97
00:12:45,280 --> 00:12:52,240
wants to capture and and and convey the meaning so you don't only need the data or the experience

98
00:12:52,240 --> 00:12:57,920
but you need to put a structure and you need symbols on top of it in order to really get into

99
00:12:58,880 --> 00:13:07,360
into an intelligent system that can understand and capture meaning so based on these hypotheses

100
00:13:07,360 --> 00:13:15,680
we set up a number of research questions so the very first one we we looked into was okay if we

101
00:13:15,680 --> 00:13:21,920
want to use uh if if we want to use knowledge graph large-scale knowledge graphs to to to

102
00:13:21,920 --> 00:13:27,600
generate explanations um we kind of need to understand what do we mean by an explanation

103
00:13:27,600 --> 00:13:33,040
so we need a definition even if it's a working definition we still need one and then we kind

104
00:13:33,040 --> 00:13:38,560
of said okay we need a method that will allow us to to to generate explanations from knowledge

105
00:13:38,560 --> 00:13:48,480
graphs so how how are we going to do that um then then assuming we come up with one such a method

106
00:13:48,480 --> 00:13:53,280
we need to try to cope with all the problems that come with knowledge graph including

107
00:13:54,000 --> 00:14:03,520
incompleteness, bias, noise um so so the the later research questions obviously some of the

108
00:14:03,520 --> 00:14:07,840
research questions came out little by little throughout the process right so you look at your

109
00:14:07,840 --> 00:14:15,440
phd from a different perspective but somehow the the later research questions were on improving

110
00:14:15,440 --> 00:14:19,600
the methods that we had in order to cope with the limitations of knowledge graphs including

111
00:14:19,600 --> 00:14:28,160
incompleteness and bias and basically uh looking now i'm going to go through the the research

112
00:14:28,160 --> 00:14:34,240
questions but i don't want to dive too much into it i mean we have papers for that i'm just the next

113
00:14:34,240 --> 00:14:41,360
slides are mostly to give you an idea of what the overall approach was uh starting with the the

114
00:14:41,360 --> 00:14:47,840
definition of explanation we defined we first defined an ontology design pattern for explanation

115
00:14:47,840 --> 00:14:53,360
so what we did was we looked into different disciplines we looked into philosophy we looked

116
00:14:53,360 --> 00:15:01,600
into linguistics into social science and we looked into their definition of explanations and we

117
00:15:01,600 --> 00:15:07,520
noticed that even though there were different approaches and methods explanations were always

118
00:15:07,520 --> 00:15:13,200
seen according to the similar characteristics so there was always the generation of some

119
00:15:13,200 --> 00:15:18,800
coherence between old knowledge and new knowledge uh the elements in the explanation were always the

120
00:15:18,800 --> 00:15:25,520
same there was a theory there were an anterior and a posterior event there were sequences that

121
00:15:25,520 --> 00:15:33,440
would make this event happening at the same time and there were always processes um

122
00:15:34,400 --> 00:15:40,000
uh that would be one internal process where you come up with an explanation for yourself and then

123
00:15:40,000 --> 00:15:44,640
one that is more an external process where you communicated the explanation to the rest of the

124
00:15:44,640 --> 00:15:53,040
world and based on this we then defined our own uh ontology design ontology for an explanation so

125
00:15:53,040 --> 00:15:59,120
our own definition that we could then feed into a system that would try to generate explanation

126
00:15:59,200 --> 00:16:04,880
according to these patterns so we we then just it's a pattern in the sense that it can be

127
00:16:04,880 --> 00:16:12,400
instantiated in multiple um in multiple contexts but the overall idea was that you always have an

128
00:16:12,400 --> 00:16:18,160
event uh that happens before an event that happens after I'm not sure you see my pointer but I hope

129
00:16:18,160 --> 00:16:26,240
so uh there are certain uh conditions that make this uh this event happening in a specific setting

130
00:16:26,240 --> 00:16:34,080
and then there's some sort of uh theory behind and a sort and an agent that that that outputs the

131
00:16:34,080 --> 00:16:44,480
that creates the conceptualizes the uh the explanation uh based on um based on this pattern

132
00:16:44,480 --> 00:16:51,040
we then try to to design a system that would try to use knowledge graph to generate explanation for

133
00:16:51,040 --> 00:16:57,040
a given pattern of data and then the the kind of question we try to answer is really okay if we

134
00:16:57,040 --> 00:17:02,560
want to use a knowledge graph a very large knowledge graph uh as background knowledge

135
00:17:02,560 --> 00:17:08,880
which kind of process do I need to uh generate explanation and then we try to work with some

136
00:17:08,880 --> 00:17:16,480
with some examples of patterns of data of things that people might want to to explain uh we had this

137
00:17:16,960 --> 00:17:23,280
very good example that would work with uh google trends and then you try to explain why a certain

138
00:17:23,280 --> 00:17:30,720
website uh is uh regularly happening at specific points in time so in these cases why people are

139
00:17:30,720 --> 00:17:36,240
searching for the term a song in ice and fire only in certain periods so you see that there are very

140
00:17:36,240 --> 00:17:43,760
regular peaks or maybe we try to explain data like statistical data for example from the UNESCO

141
00:17:44,320 --> 00:17:50,080
uh in this case we have uh countries that are grouped according to the

142
00:17:51,040 --> 00:17:57,600
female literacy rate and then you try to explain okay why is is it oops oops sorry uh why is it

143
00:17:57,600 --> 00:18:04,960
happening that um that that certain countries have in common like a certain characteristic

144
00:18:05,600 --> 00:18:11,840
and the very good thing is that uh we try to come up with a method that had us uh working

145
00:18:11,920 --> 00:18:18,400
out these examples and it was basically it was based on three main steps one was an inductive

146
00:18:18,400 --> 00:18:23,920
logic programming step where you try to compare positive and negative examples and you have some

147
00:18:23,920 --> 00:18:30,320
background knowledge expressed in uh as as tripos as facts about these examples and then the goal

148
00:18:30,320 --> 00:18:35,680
is to induce the hypothesis that mostly represents your positive examples which are the examples

149
00:18:35,680 --> 00:18:45,600
you want to explain uh we used a knowledge graph search so we we try to uh uh search the the link

150
00:18:45,600 --> 00:18:52,640
data graph uh in order to find a common path which would be expressed in terms of predicates

151
00:18:53,200 --> 00:18:59,520
relationships uh leading to a specific entity in the graph so we call this path and then we

152
00:18:59,520 --> 00:19:06,480
would consider any path that is uh common uh to all the positive examples as an explanation for the

153
00:19:06,480 --> 00:19:14,080
group of positive example uh and then in order to to to improve the the the graph search we we

154
00:19:14,080 --> 00:19:23,040
implemented a greedy uh links reversal strategy uh so we try to aim for uh the uh longer and longer

155
00:19:23,040 --> 00:19:30,160
path you know um and explore the graph using simple http d referencing in order to avoid

156
00:19:30,160 --> 00:19:37,440
computational costs it was 2013 we still had issues of computational costs and uh based on this

157
00:19:37,440 --> 00:19:42,960
method we try to answer mostly two questions so first we said okay which kind of your uh

158
00:19:42,960 --> 00:19:49,600
risks do we need to to drive a greedy search and identify the best explanation so we try

159
00:19:49,600 --> 00:19:55,920
different strategies and then one of the main finding for us was that a measure so a strategy

160
00:19:56,480 --> 00:20:05,920
based on entropy um would lead to a higher explanation in in in in less time so we simply

161
00:20:05,920 --> 00:20:14,960
measured on what is the best accuracy of an explanation over time so this is um uh iteration

162
00:20:14,960 --> 00:20:21,280
in searching the graph and we always ended up having the best uh an entropy based measure

163
00:20:21,280 --> 00:20:28,560
as the one that would perform best and give us the best explanation uh these allowed us to to to

164
00:20:28,560 --> 00:20:35,680
come up with uh explanation for the use cases i was showing before so uh why are women less

165
00:20:35,680 --> 00:20:40,160
educated than men in certain countries and that's mostly when countries are least developed for

166
00:20:40,160 --> 00:20:48,400
example uh so they have a quite a high human development index rank and uh or they are expressed

167
00:20:48,400 --> 00:20:56,000
they are defined in dbpb as least developed countries um or we we had explanations like okay

168
00:20:56,000 --> 00:21:03,840
people search for a song in ice and fire whenever there is an event that is somehow linked to a

169
00:21:03,840 --> 00:21:11,120
game of thrones tv series so that's this is we consider these explanations but of course if you

170
00:21:11,120 --> 00:21:18,320
look at the results you might notice things that are quite not right and uh in particular you look

171
00:21:18,320 --> 00:21:24,480
at these kind of examples so uh it's true that if you have enough background knowledge about a song

172
00:21:24,480 --> 00:21:30,960
in ice and fire which is actually a book uh and you know that there is a certain tv series related

173
00:21:31,600 --> 00:21:36,560
that is based on this book you know that the explanation for people being particularly

174
00:21:36,560 --> 00:21:42,720
interested in these uh is whenever the tv series is coming out so there is a new season coming out

175
00:21:42,720 --> 00:21:50,320
but there's nothing that relates the tonga tonga uh to to to the book or basketball

176
00:21:50,320 --> 00:21:57,360
competition to the book so the kind of questions we had to ask answer afterwards was really okay is

177
00:21:57,360 --> 00:22:04,160
there something in linked data that can tell us that game of thrones is strongly related to a song

178
00:22:04,160 --> 00:22:09,920
in ice and fire much more than basketball competitions or anything related to tonga so the

179
00:22:09,920 --> 00:22:16,000
second part of the method was really focused on strengthening this knowledge based explanation

180
00:22:16,000 --> 00:22:23,280
and we focused we used a genetic programming algorithm to try to learn um a function that

181
00:22:23,280 --> 00:22:30,000
could detect the strong relationships between two graph entities that are quite distant in the graph

182
00:22:30,000 --> 00:22:35,680
so we are not talking about two entities in in in the same graph but you have one entity that is

183
00:22:35,680 --> 00:22:42,320
connected to another entity through hops in multiple graphs in multiple data sets and then we really

184
00:22:42,320 --> 00:22:48,000
want to try to understand what is the the strongest relationship and what we did is that with a genetic

185
00:22:48,000 --> 00:22:56,080
programming algorithm we try to learn a function that could study the topological and semantic

186
00:22:56,080 --> 00:23:02,720
characteristics of the knowledge graph so we really looked into how many hubs and how many

187
00:23:05,520 --> 00:23:10,320
how strongly connected was the graph for example which kind of vocabulary the graph was was using

188
00:23:11,600 --> 00:23:16,400
all these we gave all these to our genetic programming algorithm and then we tried to

189
00:23:16,400 --> 00:23:23,040
come up with a with a function that would tell us okay this is a strong relationship and we evaluated

190
00:23:23,040 --> 00:23:30,800
that with against a human evaluated relationship path and um actually what what came out from

191
00:23:30,800 --> 00:23:38,160
from this study was that the best thing for us would have been to try to follow uh notes that

192
00:23:38,160 --> 00:23:43,840
would have quite rich descriptions so here you have different examples of different functions

193
00:23:43,840 --> 00:23:50,960
that we tried and then here you have the say the best performing ones um and the the the best

194
00:23:50,960 --> 00:23:58,240
functions that we could find were really the ones were focusing on uh following notes with

195
00:23:58,240 --> 00:24:05,040
rich descriptions that they would have quite a good number of namespaces uh it was best to follow

196
00:24:05,040 --> 00:24:10,880
more specific entities so really not not not hubs with many incoming links but rather something

197
00:24:10,880 --> 00:24:18,320
that is more specific and also we look into scores the scores vocabulary and uh it was best to have

198
00:24:18,320 --> 00:24:24,160
fewer topical categories so not something quite generic in some in terms of topic but but rather

199
00:24:26,080 --> 00:24:35,760
again a bit more specificity uh of course we had to look into into into into into into the bias

200
00:24:35,760 --> 00:24:42,960
in the data as well so the the results were not optimal and as I said we needed to deal both with

201
00:24:42,960 --> 00:24:50,800
incompleteness of the data but also when inner bias of the data and what we tried to do was to um

202
00:24:51,680 --> 00:24:57,440
use identity links so remember that we are still we were still we are still talking about link data

203
00:24:57,440 --> 00:25:04,080
where data sets are connected to each other through identity links including same as and what we uh

204
00:25:04,720 --> 00:25:14,720
tried to do was to um measure the bias in a given data set uh by uh comparing the projection on one

205
00:25:14,720 --> 00:25:20,000
data set into another one so the overall idea was really that you have a data set which could be

206
00:25:20,000 --> 00:25:25,840
I don't know the link movie database and you have a certain amount of entities that are connected

207
00:25:25,920 --> 00:25:35,600
through the dbpdia uh using identity links and the projection of the the movie database is mostly

208
00:25:35,600 --> 00:25:42,080
the set of entities that's in dbpdia that are connected to to to the movie database and then

209
00:25:42,080 --> 00:25:50,560
we we would basically compare all the entities in the larger data set with the subset and using

210
00:25:50,560 --> 00:25:58,400
some correlation uh tests or some t-test and by comparing the distribution of uh property value

211
00:25:58,400 --> 00:26:06,320
pairs uh between the subset of the entities and the large set uh we try to identify with uh which

212
00:26:06,320 --> 00:26:15,360
were the the the the the bias in a given data set uh so for example we uh and yeah I would

213
00:26:15,920 --> 00:26:23,120
uh send uh I would recommend you to refer to the paper but what we what we discovered was

214
00:26:23,120 --> 00:26:29,680
for example that the link movie database was particularly focused on black and white movies and

215
00:26:31,600 --> 00:26:38,560
that certain digital humanities data sets were focused on uh uh poets and novelists from the

216
00:26:38,560 --> 00:26:46,720
18th and 19th century so it was a way to try to measure the bias in the in the in the data

217
00:26:46,720 --> 00:26:53,840
set that we were using to generate explanations and to once we learned these bias we could also try to

218
00:26:57,440 --> 00:27:04,000
somehow input this information in our system and generate better explanations

219
00:27:04,880 --> 00:27:13,680
now this was uh uh quite a long a long journey that uh ended up in 2016 and there's a number of

220
00:27:13,680 --> 00:27:22,240
things that happened uh since then what I what I'm I call the the present um so I don't have

221
00:27:22,240 --> 00:27:28,080
enough time to focus on everything but I choose three particular uh aspects that I'm going to to

222
00:27:28,080 --> 00:27:37,280
show which help us also thinking a bit further uh into the steps that we want to uh to take afterwards

223
00:27:38,000 --> 00:27:44,480
the very first thing of course is the rise of deep learning so when uh when uh we started the

224
00:27:44,480 --> 00:27:48,640
the work on explanations and knowledge graphs actually on knowledge graphs for explanations

225
00:27:49,600 --> 00:27:55,200
deep learning was just was probably just booming and I wasn't even aware of that so what we were

226
00:27:55,200 --> 00:28:02,240
talking about was uh using knowledge graphs to um generate explanation for outputs of any

227
00:28:02,240 --> 00:28:08,640
machine learning algorithm uh but we've never heard of deep learning I think that yeah maybe the the

228
00:28:09,360 --> 00:28:17,040
the the first papers are around 2012 but if I'm not mistaken but um of course the the hype came

229
00:28:17,040 --> 00:28:27,520
much later and um uh the the the DARPA uh so so deep learning and all the methods behind we

230
00:28:27,520 --> 00:28:32,960
that came with deep learning showed uh impressive results that could achieve uh

231
00:28:35,120 --> 00:28:40,960
the same results as human would and uh but but also showed a number of limitations so the

232
00:28:41,600 --> 00:28:48,640
uh the the the facts like the Cambridge Analytica scandals or the the loan accreditation scandals

233
00:28:49,680 --> 00:28:56,400
show that these systems were uh were not able to to show a clear reasoning so the reasoning was

234
00:28:56,400 --> 00:29:02,160
quite opaque uh these systems were data hungry so you needed a lot of data to train them they were

235
00:29:02,160 --> 00:29:08,880
too brittle in this sense and then DARPA came out with this explainable AI program that was 2017

236
00:29:09,280 --> 00:29:17,840
if not if I'm not mistaken um on okay let's try to implement create systems where the user is

237
00:29:17,840 --> 00:29:24,800
that are transparent that can explain the machine learning model and then where uh an explanation

238
00:29:24,800 --> 00:29:30,880
can be given on why a certain output is being given uh so this need of explaining the models

239
00:29:30,880 --> 00:29:36,880
and the results really came out and you start we started seeing methods like SHOP and LIME which

240
00:29:36,880 --> 00:29:42,320
are probably the most basic ones and then there's there's a number of other systems that came out

241
00:29:42,320 --> 00:29:48,640
afterwards uh that could tell you okay the the most important features to come up with an explanation

242
00:29:49,680 --> 00:29:59,440
and more in your data set are uh like maybe the race or the occupation of of of of your data

243
00:30:00,160 --> 00:30:06,320
or we started seeing saliency maps okay with with imagery recognition uh what are the most

244
00:30:06,400 --> 00:30:16,080
important parts that that system focuses on when generating explanations uh this especially the

245
00:30:16,080 --> 00:30:22,880
explanations in in in the explainable AI area and if you look into the major AI conference you

246
00:30:22,880 --> 00:30:29,920
start seeing a boom of uh I think explainable AI became an actual topic or a subfield in in AI

247
00:30:30,560 --> 00:30:36,800
uh one of the questions that people asked was okay uh are these explanations that SHOP and LIME

248
00:30:36,800 --> 00:30:43,840
come up with are they really working especially in a real-world context so they do work in in a

249
00:30:43,840 --> 00:30:50,560
small use case but what if I applied into into a real-world context uh and somehow the narrow

250
00:30:50,560 --> 00:30:57,840
symbolic field uh which was already in in the meantime in developing on on his own came up a

251
00:30:57,840 --> 00:31:05,600
bit in the rescue of this problem so somehow we started seeing methods that try to combine

252
00:31:06,160 --> 00:31:13,360
a symbolic approach so symbolic reasoning uh with neural network either to uh maybe improve

253
00:31:13,360 --> 00:31:20,560
the explainability and trust of a system using a symbolic description uh or by creating a sort

254
00:31:20,560 --> 00:31:25,840
of hybrid interaction between the the neural network and and the reasoning system uh so these

255
00:31:25,840 --> 00:31:31,600
are just two of the many examples that you could could see when uh where the knowledge graph would

256
00:31:31,600 --> 00:31:37,440
try to jump in into the explainable AI word and say okay hey look we should be maybe using

257
00:31:37,440 --> 00:31:45,520
knowledge graphs and ontologies to help uh we also did a part of this uh so what what we try to do um

258
00:31:45,520 --> 00:31:53,040
and um uh this this I completely forgot the the uh this the reference to this work but

259
00:31:53,040 --> 00:32:00,400
if we published in 2021 uh we really try to say okay if everybody if many people are looking into

260
00:32:00,400 --> 00:32:09,440
using knowledge graphs as a as a tool to explain um machine learning methods let's try to look at

261
00:32:09,440 --> 00:32:16,160
what's the state of the art so uh how are people in machine learning using knowledge graphs what are

262
00:32:16,160 --> 00:32:22,720
the the most important characteristics so we try to look into different uh tasks and different areas

263
00:32:23,200 --> 00:32:27,680
and uh we try to look into the characteristics of the knowledge graphs the characteristics of

264
00:32:27,680 --> 00:32:32,960
the model and the characteristics of the explanations that we were being generating uh

265
00:32:32,960 --> 00:32:38,000
in order to come up with really with a with a picture of the field uh and this this is more

266
00:32:38,000 --> 00:32:45,120
or less what we came up with so uh we we learned certain things like if you are dealing with tasks

267
00:32:45,120 --> 00:32:50,560
for recognition and recommendation uh most of the explanation you will get are really about the

268
00:32:50,560 --> 00:32:56,960
model and how it behaves and most of the information that is being used uh from the knowledge graph is

269
00:32:56,960 --> 00:33:05,520
the is the a is the aversion box uh and whenever uh we deal with tasks that involve the interaction

270
00:33:05,520 --> 00:33:11,200
of the user like conversational agents or recommender systems the knowledge graph information is

271
00:33:11,200 --> 00:33:17,680
used to is used more into the training of the model uh to generate a certain explanation

272
00:33:17,760 --> 00:33:24,720
rather than as a postdoc stack uh we looked into the different types of knowledge graphs whether

273
00:33:24,720 --> 00:33:30,800
they were factual or common sense or domain knowledge graphs domain specific and it turns

274
00:33:30,800 --> 00:33:35,760
out that common sense knowledge graphs they're not that many but they were being used for image

275
00:33:35,760 --> 00:33:41,440
recognition and question answering uh we also look into the reuse of knowledge graphs so we've been

276
00:33:41,520 --> 00:33:47,920
uh talking so much in our field about reusing knowledge graph reusing ontology ontologies and

277
00:33:47,920 --> 00:33:52,640
we kind of ask okay is this being applied in this field and it actually turned out it was

278
00:33:52,640 --> 00:33:57,920
quite an established practice so very few were coming up with their own knowledge graph uh most

279
00:33:57,920 --> 00:34:06,320
of the the methods were were using uh dbpdia uh wikidata um consonants or a combination of them

280
00:34:06,880 --> 00:34:13,680
um actually we looked into uh whether these methods were using only one knowledge graph

281
00:34:13,680 --> 00:34:20,160
or multiple ones and we we were hoping to to see a bit more but it does happen sometimes in

282
00:34:20,160 --> 00:34:26,960
nlp tasks and really the kind of this is the kind of picture that came out so if you if certain

283
00:34:26,960 --> 00:34:32,240
areas are more focused on on model embedded knowledge so explaining the model rather than

284
00:34:32,240 --> 00:34:39,760
explaining the outputs certain others are more focused on using the ontologies of the

285
00:34:39,760 --> 00:34:46,640
knowledge graph rather than the facts um and um and so on and so forth so I think the analysis we

286
00:34:46,640 --> 00:34:53,920
did it's around 60 60 papers more or less um and we kind of try to identify also the challenges

287
00:34:53,920 --> 00:35:00,480
for the field right so there are things that were we were hoping to see but that didn't happen

288
00:35:00,560 --> 00:35:05,040
including the what we call the co-creation of explanations so really having a sort

289
00:35:07,440 --> 00:35:12,560
the human having a role into the generation of the explanation there are there were there are

290
00:35:12,560 --> 00:35:18,400
issues related to the maintenance of the knowledge graph so how to deal with uh

291
00:35:18,960 --> 00:35:25,760
missing information or bias when coming up when generating explanations and of course this is a

292
00:35:25,760 --> 00:35:31,760
problem that we also saw during in the in the phd and also there are issues related to the

293
00:35:31,760 --> 00:35:37,520
automated extraction of relevant knowledge when using a knowledge graph that generates explanations

294
00:35:37,520 --> 00:35:44,560
so most of the methods that we analyzed when coming up with an explanation end up manually

295
00:35:44,560 --> 00:35:50,480
selecting uh the relevant relevant relevant information to generate an explanation and

296
00:35:50,480 --> 00:35:54,640
this is quite something it means that there's still a lot to do in the in the field in order to

297
00:35:55,520 --> 00:36:06,000
um uh to move forward um there's uh so this was one part of the story so what happened

298
00:36:06,000 --> 00:36:11,440
ever since deep learning uh there's also the field of hybrid intelligence that came up so

299
00:36:11,440 --> 00:36:17,280
I don't know if many of you heard about uh hybrid intelligence maybe you had about human

300
00:36:17,280 --> 00:36:23,600
centric ai this is another way of addressing this this problem uh the overall idea was that

301
00:36:24,960 --> 00:36:30,320
this there is an emerging field in ai which uh and it's emerging because you start seeing

302
00:36:30,320 --> 00:36:36,720
different conferences and workshops uh around the topic there's a number of uh national and

303
00:36:36,720 --> 00:36:45,360
international um collaboration networks uh that uh that are really focusing on uh on this concept

304
00:36:45,360 --> 00:36:51,040
of hybrid intelligence and and the overall idea because we don't really have a proper definition

305
00:36:51,040 --> 00:36:58,960
but we do have a working definition of hybrid intelligence is to have uh to to aim for ai systems

306
00:36:58,960 --> 00:37:06,400
that try to enhance human capabilities as other scientific tools would do think of the telescope

307
00:37:06,400 --> 00:37:15,760
that allows a scientist to um to look uh where his own eyes cannot see or the machine the the

308
00:37:15,760 --> 00:37:21,440
the sorry the the card that or the the airplane would allow people to to to reach places that

309
00:37:21,440 --> 00:37:28,400
they couldn't reach easily with their with their feet um so it's really about seeing ai system as

310
00:37:28,400 --> 00:37:35,840
an extension of the human intelligence rather than seeing ai as a tool that replaces us so in

311
00:37:35,840 --> 00:37:44,320
this sense hybrid intelligence really look into uh systems that collaborate uh with humans uh aiming

312
00:37:44,320 --> 00:37:51,200
for a complementarity so weak weaknesses and strengths of both ai and humans are complemented

313
00:37:51,200 --> 00:38:00,000
by each other uh and really about this synergetic idea so the the the mixed team the hybrid team

314
00:38:00,000 --> 00:38:10,800
is is aiming for the same has a shared goal um this is um the so so we have a research agenda

315
00:38:10,800 --> 00:38:15,360
I'm part of the Dutch hybrid intelligence consortium and I've been part of the hybrid

316
00:38:15,360 --> 00:38:22,400
intelligence conference of the past uh in the past years um and it's a really vibrant uh field

317
00:38:22,960 --> 00:38:30,720
um and and explainability explaining um it's also a part of the research agenda so

318
00:38:30,720 --> 00:38:35,520
it's not only about trying to collaborate but how in in this in this collaboration

319
00:38:35,520 --> 00:38:42,640
the goal is is also to try to communicate our own intention and explain our own actions and

320
00:38:42,640 --> 00:38:52,720
our own reasoning uh so the one of the the core topics that we established when when

321
00:38:52,720 --> 00:38:57,840
hybrid intelligence came into the picture was really on on on how to create systems that are

322
00:38:57,840 --> 00:39:06,640
able to deliberate and and explain to to their collaborators um I'm more than happy to to discuss

323
00:39:06,640 --> 00:39:13,840
this a bit further uh and somehow in as part of the hybrid intelligence picture we also uh

324
00:39:13,840 --> 00:39:19,120
and as part of the the the contribution that we could give with ontologies and knowledge

325
00:39:19,120 --> 00:39:26,960
across with respect to explainability we also try to um work on on on on a number of what we

326
00:39:26,960 --> 00:39:32,400
call boxologies or terminologies for hybrid intelligence where so to establish mostly to

327
00:39:32,400 --> 00:39:39,760
establish a shared language between uh agents in a different team um sorry agents in the same

328
00:39:39,760 --> 00:39:46,800
team that would collaborate between each other um and uh what what we did in one of the the the

329
00:39:48,000 --> 00:39:52,880
the newest work was really on comparing different hybrid intelligence scenarios

330
00:39:53,920 --> 00:39:59,920
and first trying to identify what are the common uh knowledge roles so we came up with a high level

331
00:40:00,000 --> 00:40:08,000
ontologies of agents interaction types uh and and and specific scenarios and and we try to also

332
00:40:08,000 --> 00:40:15,360
identify um uh using these high level ontologies what were the the most specific hybrid intelligent

333
00:40:15,360 --> 00:40:25,120
tasks uh including ones that would uh relate to creativity and and explainability uh and then

334
00:40:25,120 --> 00:40:31,440
on the side of it we would have uh tasks like team awareness and multimodality uh that's the

335
00:40:31,440 --> 00:40:37,600
second part of the picture so we talk about uh deep learning we talk about uh hybrid intelligence

336
00:40:38,800 --> 00:40:46,320
and what also came into play with respect to to explanation and uh we didn't have that much

337
00:40:46,320 --> 00:40:52,080
time to to dive into it but hybrid intelligence is strongly related to that as well is really on

338
00:40:52,080 --> 00:40:58,640
especially with a with a european perspective is really on the gdpr and uh so um and an eu

339
00:40:59,600 --> 00:41:07,360
ai act which recently came up uh but really the idea is to try to monitor the systems that we are

340
00:41:07,360 --> 00:41:15,600
developing uh to make sure that that that uh users are are protected both in terms in terms of the

341
00:41:15,600 --> 00:41:21,520
data that are being generated and the methods that are being generated uh and these also

342
00:41:21,520 --> 00:41:30,960
means to be able to to to explain um uh the reasoning and to trace back the the the information

343
00:41:30,960 --> 00:41:40,800
that is being output uh so so explainability and transparency became um started uh appearing

344
00:41:40,800 --> 00:41:46,080
together in the in the picture so if you want transparency you need to be explainable therefore

345
00:41:46,080 --> 00:41:54,960
showing your reasoning um and now with this eu ai act the the idea is really that that there are

346
00:41:54,960 --> 00:42:02,000
certain obligations there are certain systems that need to show um uh some transparent they have

347
00:42:02,000 --> 00:42:08,800
transparency obligations so they need to uh be able to explain why certain things are are happening

348
00:42:08,880 --> 00:42:20,080
um otherwise they are considered uh unacceptable uh this is a very uh so i have a few minutes left

349
00:42:20,080 --> 00:42:28,160
i think this is a very uh quick overview of what has happened ever since uh which leads us back to

350
00:42:28,160 --> 00:42:34,240
okay what is going to happen now and how can we kind of think of everything that has happened so far

351
00:42:34,880 --> 00:42:41,680
and and where are we going to to go uh of course i couldn't get away without mentioning language

352
00:42:41,680 --> 00:42:50,160
models at least once so somehow one of the questions and one might ask and we also kind of wondered uh

353
00:42:50,160 --> 00:42:56,640
was uh okay but everything we've done could this be now done so the the overall knowledge

354
00:42:56,800 --> 00:43:02,560
process could just be replaced by language models and by large language models by LLMs

355
00:43:03,280 --> 00:43:08,640
could we just not replace all the steps or the explainability steps do we actually need

356
00:43:08,640 --> 00:43:16,720
knowledge grasp for that and it's true that um LLMs language and language models are very good

357
00:43:16,720 --> 00:43:22,160
in dealing with noise and inconsistency and like methods that the methods that we had before were

358
00:43:22,160 --> 00:43:27,920
not that much able they allow us to extract information very quickly from large structure

359
00:43:27,920 --> 00:43:35,360
data so that goes towards the dream of doing a web-scale learning um or one could argue that

360
00:43:35,360 --> 00:43:41,840
you already achieved that uh they're actually quite good in capturing some complex semantics so

361
00:43:41,840 --> 00:43:49,360
somehow it has been demonstrated that defining a class can can be uh uh with specific boundaries

362
00:43:49,360 --> 00:43:53,040
so the boundaries of the definition of a class is quite difficult are quite hard

363
00:43:54,880 --> 00:44:04,560
so there is no universal class description and and it the especially with the embeddings method

364
00:44:04,560 --> 00:44:11,600
based methods are able to capture this this this complexity quite a bit better and of course

365
00:44:11,600 --> 00:44:16,800
are very good in generating natural language so instead of generating explanation in a

366
00:44:16,880 --> 00:44:23,840
mechanic mechanical mechanistic way from from from the triples uh they are able to generate a much

367
00:44:23,840 --> 00:44:30,880
more human friendly um explanation the problem is that there are still limited in a number of things

368
00:44:30,880 --> 00:44:37,120
so learning from rare and unique events especially like the ones that that that you can find in the

369
00:44:37,120 --> 00:44:46,480
web is still quite difficult um if this these methods are not yet able to show proper reasoning

370
00:44:46,480 --> 00:44:54,160
and argumenting behind thoroughly creating a thorough argumentation uh behind what has happened

371
00:44:54,160 --> 00:45:00,640
and also they don't really deal with with with fairness and interoperability acceptability all

372
00:45:00,640 --> 00:45:09,920
these this fair aspects that we've been looking into as as knowledge graph community are not yet

373
00:45:09,920 --> 00:45:14,880
part of the picture in a language model so somehow there are limitations in using that

374
00:45:14,880 --> 00:45:20,000
but it doesn't mean that we need to discard them completely but we can just join the the

375
00:45:20,000 --> 00:45:25,760
both words and and work out something to for to generate better better explanations

376
00:45:26,720 --> 00:45:35,040
so i want to conclude in the last few minutes to really think okay if we now look at knowledge

377
00:45:35,040 --> 00:45:43,280
graphs and whether they're useful uh to to generate explanation and do they actually work

378
00:45:44,000 --> 00:45:50,240
what is it that we launch so uh somehow both based on the phc and everything that happens

379
00:45:50,240 --> 00:45:56,640
afterwards uh yes we can use knowledge graphs but they are mostly an intermediate representation

380
00:45:56,640 --> 00:46:02,320
so knowledge graphs are really for the machine consumption uh they shouldn't be for human consumption

381
00:46:02,960 --> 00:46:11,920
and the rdf is just a language that that for machines to perform an exchange of information

382
00:46:12,000 --> 00:46:17,920
which is unambiguous so uh we should really not look into knowledge graph as something that we human

383
00:46:17,920 --> 00:46:25,600
should understand but but more as something a machine can can quickly exchange um we can use

384
00:46:25,600 --> 00:46:32,880
knowledge graphs to as to get together content to generate explanation but really the graph

385
00:46:32,880 --> 00:46:39,440
structure is just a backbone so we don't really want to use the knowledge graph to create an output

386
00:46:39,440 --> 00:46:47,360
we can use language models for that uh but but we can use knowledge graph as to to to gather the

387
00:46:47,360 --> 00:46:53,520
backbone of the explanation which can then be output according to different users in a different

388
00:46:53,520 --> 00:47:01,520
dimension so we think okay an expert user might need a longer explanation uh which with more

389
00:47:01,520 --> 00:47:09,040
arguments and an expert a non-expert and layman might need a much shorter and simpler explanation

390
00:47:10,080 --> 00:47:15,280
in terms and llms are great in doing that you can ask them to rephrase a certain concept in

391
00:47:15,280 --> 00:47:21,760
different uh in different according to different dimensions um also knowledge graphs allow allow

392
00:47:21,760 --> 00:47:29,040
to check the to trace back information so you can really walk down the graph and and and check

393
00:47:29,040 --> 00:47:35,440
whether the information is truth and this is quite uh this is much better than looking into

394
00:47:35,440 --> 00:47:41,360
the propagation of an error the activation of a neural network and try to decode what does it mean

395
00:47:41,360 --> 00:47:47,040
um in order to come up with an explanation that you might not be able to to to explain

396
00:47:48,800 --> 00:47:56,000
that clearly yourself and finally scalability which was part of the picture at that time okay

397
00:47:56,000 --> 00:48:02,240
how can we deal with very large knowledge graphs and uh uh we want to to to integrate as much

398
00:48:02,240 --> 00:48:07,920
knowledge as possible well actually this is not that relevant anymore and what people are really

399
00:48:07,920 --> 00:48:15,440
aiming for and we also saw this when collaborating with industry is more it's much better to have

400
00:48:16,160 --> 00:48:22,480
a high quality knowledge graph which is curated by the expert uh to generate explanations rather

401
00:48:22,480 --> 00:48:29,680
than having a very large knowledge graph um so this is one part of the story and then the other part

402
00:48:29,680 --> 00:48:35,200
is more of the big picture uh what we learned is really explainability is not only about machine

403
00:48:35,200 --> 00:48:40,800
learning uh so in hybrid intelligence we deal with explainability we deal with experts from

404
00:48:40,800 --> 00:48:47,280
all kind of fields from social science to computer science and um people look into

405
00:48:47,360 --> 00:48:54,240
explainability from from this in different ways so it's a bit of a jungle of terminology we cannot

406
00:48:54,240 --> 00:49:01,200
really agree what uh what we mean by something being explainable and then we need to try to find

407
00:49:01,200 --> 00:49:09,680
a way to harmonize that um we we learned that explanations are really task dependent so yes

408
00:49:09,680 --> 00:49:14,720
we have an ontology design patterns for the explanation but we need to adapt this according

409
00:49:14,720 --> 00:49:21,680
to the context and again we can use language models for that but it's um the the target audience or the

410
00:49:22,480 --> 00:49:27,360
the language or the the type of explanation really need to change according to the situation

411
00:49:28,000 --> 00:49:34,480
and more importantly we need to look for the human so knowledge graphs are only one part in the

412
00:49:34,480 --> 00:49:40,560
explanation process so in order to generate explanation you need a knowledge graph you need

413
00:49:40,560 --> 00:49:45,520
a sub symbolic method but you also need a human that interacts with the with the system

414
00:49:46,560 --> 00:49:50,640
because in the end as we said at the beginning explanation is a social process it's a dual

415
00:49:50,640 --> 00:49:57,760
process so it has to happen in a in a in a co-creation setting where the user is really

416
00:49:57,760 --> 00:50:03,360
interacting with the system to come up with an explanation is satisfied with and in this sense

417
00:50:03,360 --> 00:50:10,240
we also need interdisciplinarity in the picture to try to measure whether an explanation is is

418
00:50:10,320 --> 00:50:15,680
interesting or not so we kind of we don't only need a computer science perspective but we also

419
00:50:15,680 --> 00:50:21,760
need to try to integrate the talks that we have with social scientists and cognitive uh scientists

420
00:50:21,760 --> 00:50:28,880
to make sure that the explanations that are being generated are actually useful uh so to give some

421
00:50:28,880 --> 00:50:35,680
ideas on what and uh then i'm i'm just done i know i'm over a bit um uh what we suggest is that

422
00:50:35,680 --> 00:50:41,120
really we should look into the kind of knowledge in artificial intelligence so some people have called

423
00:50:41,120 --> 00:50:46,000
this knowledge science some people have called it empirical semantics we call it knowledge in AI

424
00:50:47,120 --> 00:50:52,720
so we really need to go back to the empirical analysis of the knowledge graphs that we create

425
00:50:52,720 --> 00:50:58,400
and we deal with we need to understand their modeling style and the kind of semantics they're

426
00:50:58,400 --> 00:51:03,840
communicating and whether the semantics is enough or too much to generate explanations

427
00:51:03,840 --> 00:51:07,600
we need to check for the usefulness and limitation of the knowledge graph that

428
00:51:07,600 --> 00:51:14,160
and the knowledge that that we create and somehow we need to try to move from the step of okay how

429
00:51:14,160 --> 00:51:21,200
can we fit knowledge into the learning process these we know how to do it uh or at least we have

430
00:51:21,200 --> 00:51:27,520
good methods but we really need to focus on what is which kind of knowledge uh we need to fit

431
00:51:27,520 --> 00:51:31,680
in order to be able to learn something and to generate explanation for example

432
00:51:31,680 --> 00:51:38,400
so somehow this is a call for the community to to start so where do we start and uh i have

433
00:51:38,400 --> 00:51:44,160
tried to revisit a bit the research questions that that i had in the beginning thinking okay

434
00:51:44,160 --> 00:51:50,720
based on on this idea of knowledge in AI then maybe we need to try to fit the explanation pattern

435
00:51:50,720 --> 00:51:59,520
into the existing uh systems uh we need to try to to to to come up with explanations that like

436
00:51:59,520 --> 00:52:08,240
uh using deep learning and uh at a web scale uh we need to try to augment explanations and turn

437
00:52:08,240 --> 00:52:14,240
them into complex narratives we mentioned these uh complex argumentations so and for these we

438
00:52:14,240 --> 00:52:20,400
can really combine knowledge graphs and language models and we really need to compensate whatever

439
00:52:20,400 --> 00:52:28,880
information is missing uh by uh performing a co-creation of explanation with with the humans

440
00:52:30,000 --> 00:52:34,720
this is the end of my talk uh i thank you so much i don't know how many people are there i

441
00:52:34,720 --> 00:52:40,800
thank you so much for taking the time to listen into me i thank uh tobya santua and olav for

442
00:52:40,800 --> 00:52:48,720
inviting me there was an amazing opportunity i invite you to to reach me out for exchanges

443
00:52:48,720 --> 00:52:55,040
and i really hope to see you in uh at the next hybrid intelligence conference in in sweden in in

444
00:52:55,040 --> 00:53:05,680
june uh this is the end of my talk thank you all right uh thank you ilaria um i should buy one of

445
00:53:05,680 --> 00:53:13,360
the sitcom applause machines and give you a round of applause that reflects the size of the audience

446
00:53:13,360 --> 00:53:22,160
that we had um there were a few questions already in the youtube chat um so please keep them come in

447
00:53:23,360 --> 00:53:30,960
and um in the meantime i will post some of them to you and uh if there are no more questions

448
00:53:30,960 --> 00:53:41,600
i may ask the people have to deal with mine um good so let's start there is um there is a question

449
00:53:41,600 --> 00:53:49,760
by mevish on the chat um and she's asking uh what is your vision on using your explanation

450
00:53:49,760 --> 00:53:55,920
techniques for a large language model so you do you think the same techniques are useful for them

451
00:53:55,920 --> 00:54:02,000
as well or do you need a different kind or is there a way of adapting maybe um the methods

452
00:54:04,320 --> 00:54:09,920
yeah thanks so this is a very cool question of course i mean that that's the kind of question we

453
00:54:10,000 --> 00:54:17,280
i i i'm expecting in these days because we do have language models are able to achieve so much

454
00:54:19,280 --> 00:54:25,360
that it's it's hard to think okay if i done everything wrong can they just do better than me

455
00:54:25,360 --> 00:54:32,480
and i mean i would be very curious to to just do a um a simple comparison i like this question a lot

456
00:54:32,480 --> 00:54:40,720
so the the the question is really can we use these techniques um um with large language models

457
00:54:40,720 --> 00:54:45,520
because i i now have a number of PhD students working on these and we were discussing this just

458
00:54:45,520 --> 00:54:53,920
this morning um the you you certainly have the advantage that you might not need to crawl

459
00:54:54,880 --> 00:54:59,120
the knowledge graph anymore to generate explanation so all the problems of

460
00:54:59,840 --> 00:55:07,360
um heretics to search the graph um to to reduce the computational complexity

461
00:55:09,120 --> 00:55:18,480
might not be needed anymore uh uh with that said i still think that using you you need to combine

462
00:55:18,480 --> 00:55:24,640
language models and knowledge graphs um in order to be able to to trace back the information and

463
00:55:24,640 --> 00:55:31,520
especially if we are talking about uh truthfulness of an explanation i can ask my knowledge graph to

464
00:55:33,280 --> 00:55:38,320
the language model to to come up with an explanation for a given pattern of data but i

465
00:55:38,320 --> 00:55:44,960
i also want to make sure that i can trace the provenance back and this is something that i

466
00:55:45,840 --> 00:55:50,480
i want to know i mean probably a knowledge graph is much better to do than than a language model

467
00:55:51,200 --> 00:55:57,200
uh so i still think that as i said the backbone of the information should come from a knowledge

468
00:55:57,200 --> 00:56:05,600
graph in a way that you can uh you can reconstruct the subgraph somehow that generates your explanation

469
00:56:05,600 --> 00:56:14,960
and then the output for the form it can be uh can be generated or or situated according to the

470
00:56:14,960 --> 00:56:24,960
users by the language model that will be my answer okay um so thanks for that there's another

471
00:56:24,960 --> 00:56:31,200
question by peter jones he's asking to what extent do you think ai and explainable ai may

472
00:56:31,200 --> 00:56:39,440
reduce or undermine the use or development of domain specific languages uh sorry i

473
00:56:39,440 --> 00:56:47,920
lost the second part of the question so do i think the ai and explainable ai reduced the use

474
00:56:47,920 --> 00:57:00,000
of domain specific languages um well i don't think they actually i i'm not quite sure uh

475
00:57:00,000 --> 00:57:08,320
whether by domain specific language uh yeah we mean the domain specific representation

476
00:57:08,320 --> 00:57:17,200
or so the main ontologies but i don't think they actually uh reduce it or at least i don't think

477
00:57:17,200 --> 00:57:26,160
they should reduce it somehow um i see more an integration of the two in the sense that uh

478
00:57:26,160 --> 00:57:34,160
the the the same way uh there are these methods that use so you i've seen methods using domain

479
00:57:34,160 --> 00:57:41,520
specific ontologies to um come up with maybe decision trees about the an explanation that is

480
00:57:41,520 --> 00:57:49,920
being generated so they um uh the advantage of domain specific language is still that they are

481
00:57:50,880 --> 00:57:56,960
highly they're curated by by the experts and so they are still more reliable so the two methods

482
00:57:56,960 --> 00:58:05,440
should kind of uh i don't i don't want to think of uh of explainable ai methods as taking over

483
00:58:05,440 --> 00:58:12,080
but rather to try to uh complement to to to combine the two or in a in a neuro symbolic fashion

484
00:58:13,840 --> 00:58:24,400
i hope this answer the questions uh hi yeah i hope so too um speaking of maybe maybe this

485
00:58:24,400 --> 00:58:30,960
this goes one into the direction of one of the questions that uh that i uh noted um so if we

486
00:58:30,960 --> 00:58:35,520
have if you have a domain specific language or a domain specific may of modeling things

487
00:58:35,520 --> 00:58:44,800
then this allows to very uh concisely write down things for a specific domain um previously one

488
00:58:44,800 --> 00:58:54,160
of your explanation methods you use something that like was looking at graph uh the distance in the

489
00:58:54,160 --> 00:58:59,600
graph right and if you change like if you have something very good for one domain then that

490
00:58:59,600 --> 00:59:07,440
obviously changes the the the distance in the graph so maybe you can reflect a little bit on how

491
00:59:08,080 --> 00:59:14,800
the graph structure or the role of how model how things are modeled or how much entailment is

492
00:59:14,800 --> 00:59:23,840
applied on on the graph changes the the the distance or the results of your approach

493
00:59:24,800 --> 00:59:31,600
yeah so i think um in general this is the kind of problem we try to to to approach

494
00:59:34,800 --> 00:59:40,960
both so with uh say with a with a follow-up method so when we looked into

495
00:59:41,840 --> 00:59:46,880
uh trying to identify strong relationships but also trying to cope with the with the

496
00:59:46,880 --> 00:59:52,160
the inner bias of the information uh the assumption so we we've never dealt with

497
00:59:52,800 --> 00:59:58,480
knowledge graphs we created right so we always dealt with knowledge graphs that were created by

498
00:59:58,480 --> 01:00:04,320
others so the assumption was you might not find the information that that you might need and you

499
01:00:04,320 --> 01:00:10,320
need to so sometimes the the information is very well curated and sometimes this is not

500
01:00:10,320 --> 01:00:17,600
and somehow we need to find um a way to to to cope with this problem in order to be as general

501
01:00:17,600 --> 01:00:28,160
as possible so the my view is really that there is not a a universal way of so there is not a

502
01:00:28,160 --> 01:00:36,640
method that can can deal with both the most important thing is being able to um to cope

503
01:00:36,640 --> 01:00:40,560
with the problem and integrate it in the methods that you develop so you need to be aware that

504
01:00:40,560 --> 01:00:47,600
information might be missing and and you need to make sure that your method compensates for that

505
01:00:47,600 --> 01:00:53,440
this can happen inside the development of your method or as a postdoc it's like a posteriori

506
01:00:53,440 --> 01:01:01,840
step and and it can be as simple as i mean in the same view of the co-creation with the user it can

507
01:01:01,840 --> 01:01:07,840
be as simple as okay let's interact and see whether i'm missing some part of the information

508
01:01:10,640 --> 01:01:16,800
so so somehow one of the reasons why we had to look into the strength of the relationship was

509
01:01:16,800 --> 01:01:24,400
also because we were missing these and we had to find a strategy to to survive in the in this

510
01:01:24,960 --> 01:01:35,680
case okay um there's another question from ask kim star and uh they are asking what

511
01:01:35,680 --> 01:01:42,160
adaptions do you see for knowledge for for this whole set of approaches to deal with multimodality

512
01:01:44,320 --> 01:01:52,960
um sorry so the so you had so you had like there was this with the cat picture where there

513
01:01:52,960 --> 01:02:02,080
was some computer vision aspects in it um and maybe you reflect a bit on on multimodality when

514
01:02:02,080 --> 01:02:10,240
it comes so it's uh it's through that we've never uh it's the kind of knowledge graph we've been

515
01:02:10,240 --> 01:02:18,880
dealing with and and multi multi-modal knowledge graphs were not really um uh um that common at

516
01:02:18,960 --> 01:02:25,760
that time i think uh so so it's through that one aspect that would be interesting and maybe

517
01:02:26,640 --> 01:02:33,440
we kind of we didn't discuss this as as in the future step but it is in these days we are now

518
01:02:33,440 --> 01:02:41,680
talking about we hear much more about multimodal knowledge graphs we hear um also about knowledge

519
01:02:41,680 --> 01:02:47,120
graph that somehow try to integrate the physical words so like we deal with robots in some scenarios

520
01:02:47,120 --> 01:02:52,240
and we also have this problem of okay i have a knowledge graph that has to integrate both

521
01:02:52,240 --> 01:03:01,040
abstract concepts but also the physical word um so um uh i think it would be quite interesting to

522
01:03:01,040 --> 01:03:07,360
think on how to generate explanations that are multimodal in this sense uh this is one part of

523
01:03:07,360 --> 01:03:12,800
the answer in the sense that yes we didn't look into multimodal knowledge graphs and this it could

524
01:03:12,880 --> 01:03:20,800
change uh the other part of the answer is uh uh the explanation that we could generate

525
01:03:21,440 --> 01:03:27,680
so the kind of patterns that were coming out could also be patterns coming from from multimodal

526
01:03:27,680 --> 01:03:36,320
data so like the uh i dealt a lot with clusters of of of data points but these data points could as

527
01:03:36,320 --> 01:03:44,080
well be um uh paths of an image for example that will represent that will represent i don't know

528
01:03:44,080 --> 01:03:49,920
the ear or the the tail of the cut of this kind of thing so i didn't deal with that concretely but

529
01:03:49,920 --> 01:04:01,600
it could uh i i don't see why the method shouldn't work on um image uh labels uh with yeah with specific

530
01:04:01,600 --> 01:04:07,600
information that we might want to explain uh so that say with respect to multimodality there are

531
01:04:07,600 --> 01:04:13,040
these two parts so the the the original method would probably work also on data points coming up

532
01:04:13,040 --> 01:04:19,680
from multimodal data and then the multimodal knowledge graph we didn't look that much into it

533
01:04:19,680 --> 01:04:25,200
and it could be quite interesting to to look into multimodal explanation in this sense so that's

534
01:04:25,280 --> 01:04:35,360
okay thanks um so maybe as a as a last question jumping back to your history um and because this

535
01:04:35,360 --> 01:04:42,640
is the cost action on distributed knowledge graph where we uh care about distributed decentralized

536
01:04:42,640 --> 01:04:49,440
things so you you had this one approach that was based on dereferencing of your eyes and looking at

537
01:04:49,440 --> 01:04:57,920
things from from that perspective now um you kind of gave me the impression that you say okay now the

538
01:04:57,920 --> 01:05:03,440
person who developed the large language model did the web crawling for you but maybe you can say

539
01:05:03,440 --> 01:05:10,000
something like all the methods that you have developed in the meantime after you were done

540
01:05:10,000 --> 01:05:17,200
with dereferencing your eyes are they based on a global kind of view uh to i don't know generate

541
01:05:17,200 --> 01:05:22,880
embeddings and things um or would they still work in a distributed and decentralized setting

542
01:05:24,080 --> 01:05:33,200
i i guess as long as uh so the the embeddings are convenient because they can embed a lot of

543
01:05:33,200 --> 01:05:39,200
information in in a very small space and that's something that we didn't have so we had to come

544
01:05:39,200 --> 01:05:46,400
up with a different dereferencing opportunity but also we didn't want to deal with uh storing the

545
01:05:46,400 --> 01:05:50,960
graph because if you store the graph and you query it then you then you you need to know the

546
01:05:50,960 --> 01:05:56,640
data model and we didn't care about it so what we really cared about was this kind of serendipitous

547
01:05:56,640 --> 01:06:04,800
hope um i remember one of the first papers i i saw was um something all of also did on on on

548
01:06:04,800 --> 01:06:13,200
navigation query languages so that was quite quite quite related um so i think the the most

549
01:06:13,200 --> 01:06:21,280
important the the part that would still work is even in a distributed context is as long as you

550
01:06:21,280 --> 01:06:29,040
have connections or pointers to to from data to data then it's fine it doesn't really matter and uh

551
01:06:29,040 --> 01:06:36,000
and we we didn't care whether it was the web of data or it could have been the load and load

552
01:06:36,960 --> 01:06:44,160
stored in a in an hdt file um that was really not the the problem the main problem was uh

553
01:06:44,960 --> 01:06:50,720
what where is the irrelevant information uh how do i identify the the links between the

554
01:06:50,720 --> 01:06:56,480
between that and this is valid in any context do i whether i have the information centralized or

555
01:06:56,480 --> 01:07:02,240
decentralized i would say then i have really experimented with that so i can't tell but

556
01:07:02,400 --> 01:07:12,400
so good um yeah thanks for the answer um i don't have more questions from the youtube chat um

557
01:07:13,760 --> 01:07:20,640
with that i just want you to thank you again for the very nice talk and the very nice tna session

558
01:07:22,000 --> 01:07:28,480
before we close the stream i can only advertise the next talk on january the 31st at the same

559
01:07:28,480 --> 01:07:37,520
time as today by mayank um you can check out our website and find uh follow us on this thing

560
01:07:37,520 --> 01:07:44,400
formerly called twitter and of course you find all the recordings of our talk on our youtube channel

