WEBVTT

00:00.000 --> 00:04.740
The year was 1956 and the place was Dartmouth College.

00:04.740 --> 00:10.640
In a research proposal, a math professor used a term that was then entirely new and entirely

00:10.640 --> 00:14.720
fanciful, artificial intelligence.

00:14.720 --> 00:17.980
There's nothing fanciful about AI anymore.

00:17.980 --> 00:22.080
The directors of the Stanford Institute for Human-Centered Artificial Intelligence, John

00:22.080 --> 00:24.440
Etchimendi, and Fei-Fei Li.

00:24.440 --> 00:36.800
On Uncommon Knowledge, now.

00:36.800 --> 00:38.120
Welcome to Uncommon Knowledge.

00:38.120 --> 00:39.800
I'm Peter Robinson.

00:39.800 --> 00:46.520
Philosopher John Etchimendi served from 2000 to 2017 as provost here at Stanford University.

00:46.520 --> 00:51.240
Dr. Etchimendi received his undergraduate degree from the University of Nevada before earning

00:51.240 --> 00:54.320
his doctorate in philosophy at Stanford.

00:54.320 --> 00:59.280
He earned that doctorate in 1983 and became a member of the Stanford Philosophy Department

00:59.280 --> 01:01.200
the very next year.

01:01.200 --> 01:07.840
He's the author of a number of books, including the 1990 volume The Concept of Logical Consequence.

01:07.840 --> 01:11.360
Since stepping down as provost, Dr. Etchimendi has held a number of positions at Stanford,

01:11.360 --> 01:16.800
including, and for our purposes today, this is the relevant position, co-director of the

01:16.800 --> 01:21.280
Stanford Institute for Human-Centered Artificial Intelligence.

01:21.280 --> 01:26.600
Born in Beijing, Dr. Fei-Fei Li moved to this country at the age of 15.

01:26.600 --> 01:31.160
She received her undergraduate degree from Princeton and a doctorate in electrical engineering

01:31.160 --> 01:34.240
from the California Institute of Technology.

01:34.240 --> 01:39.040
Now a professor of computer science here at Stanford, Dr. Li is the founder once again

01:39.040 --> 01:43.040
of the Stanford Institute for Human-Centered Artificial Intelligence.

01:43.800 --> 01:49.400
Dr. Li's memoir published just last year, The Worlds I See, Curiosity, Exploration,

01:49.400 --> 01:52.440
and Discovery at the Dawn of AI.

01:52.440 --> 01:55.840
John Etchimendi and Fei-Fei Li, thank you for making the time to join me.

01:55.840 --> 01:59.240
Thank you for inviting us.

01:59.240 --> 02:04.360
I would say that I'm going to ask a dumb question, but I'm actually going to ask a question

02:04.360 --> 02:07.600
that is right at the top of my form.

02:07.600 --> 02:10.360
What is artificial intelligence?

02:10.360 --> 02:15.680
I have seen the term 100 times a day for, what, several years now.

02:15.680 --> 02:21.720
I have yet to find a succinct and satisfying explanation.

02:21.720 --> 02:22.720
Let's see.

02:22.720 --> 02:23.720
Well, let's go to the philosophy.

02:23.720 --> 02:26.520
Here's a man who's professionally rigorous, but here's a woman who actually knows the

02:26.520 --> 02:27.520
answer.

02:27.520 --> 02:28.520
Yeah, she knows the answer.

02:28.520 --> 02:31.520
So, let's say the answer, and then I will give you a different answer.

02:31.520 --> 02:32.520
Oh, really?

02:32.520 --> 02:33.520
All right.

02:33.520 --> 02:34.520
Okay.

02:34.520 --> 02:39.120
Peter used the word succinct, and I'm sweating here, so because artificial intelligence

02:39.120 --> 02:50.720
by today is already a collection of methods and tools that summarizes the overall area

02:50.720 --> 03:00.800
of computer science that has to do with data, pattern recognition, decision-making in natural

03:00.800 --> 03:08.160
language, in images, in videos, in robotics, in speech, so it's really a collection at

03:08.160 --> 03:14.600
the heart of artificial intelligence is statistical modeling, such as machine learning, using

03:14.600 --> 03:16.600
computer programs.

03:16.600 --> 03:24.120
But today, artificial intelligence truly is an umbrella term that covers many things

03:24.120 --> 03:29.160
that we're starting to feel familiar about, for example, language intelligence, language

03:29.160 --> 03:32.560
modeling, or speech, or vision.

03:32.920 --> 03:39.080
And you and I both knew John McCarthy, who came to Stanford after he wrote that, used

03:39.080 --> 03:44.240
the term coin, the term artificial intelligence, now the late John McCarthy, and I confess

03:44.240 --> 03:49.040
to you who knew him, as I did, that I'm a little suspicious of the term because I knew

03:49.040 --> 03:52.840
John, and John liked to be provocative.

03:52.840 --> 03:58.360
And I am thinking to myself, wait a moment, we're still dealing with ones and zeros.

03:58.360 --> 04:01.600
Computers are calculating machines.

04:01.600 --> 04:05.360
artificial intelligence is a marketing term.

04:05.360 --> 04:09.080
So no, it's not really a marketing term.

04:09.080 --> 04:13.880
So I will give you an answer that is more like what John would have given.

04:13.880 --> 04:20.160
And that is, it's the field, the subfield of computer science that attempts to create

04:20.160 --> 04:29.040
machines that can accomplish tasks that seem to require intelligence.

04:29.040 --> 04:37.040
So the early artificial intelligence were systems that played chess or checkers, even,

04:37.040 --> 04:38.840
you know, very, very simple things.

04:38.840 --> 04:48.040
Now John, who, as you know, if you knew him, was ambitious.

04:48.040 --> 04:54.200
And he thought that in a summer conference at Dartmouth, they could solve most of the

04:54.200 --> 04:55.200
problems.

04:55.200 --> 05:00.960
All right, I'm going to come, let me name a couple of very famous events.

05:00.960 --> 05:03.800
What I'm looking for here, I'll name the events.

05:03.800 --> 05:08.600
We have, in 1997, a computer defeats Gary Kasparov at chess.

05:08.600 --> 05:10.600
Big moment for the first time.

05:10.600 --> 05:15.680
Big blue and IBM project defeats a human being at chess, and not just a human being, but

05:15.680 --> 05:19.680
Gary Kasparov, who by some measures is one of the half dozen greatest chess players who

05:19.680 --> 05:22.520
ever lived.

05:22.520 --> 05:27.400
And as best I can tell, computer scientists said, you know, things are getting faster,

05:27.400 --> 05:28.920
but still.

05:28.920 --> 05:37.440
And then we have, in 2015, a computer defeats Go expert Han Fuei, and the following year

05:37.440 --> 05:43.160
it defeats Go grandmaster Lee Seadol, I'm not at all sure I'm pronouncing that correctly,

05:43.160 --> 05:45.240
in a five game match.

05:45.240 --> 05:50.600
And people say, whoa, something just happened this time.

05:50.600 --> 05:55.280
So what I'm looking for here is something, something that a layman like me can latch

05:55.280 --> 05:57.680
on to and say, here's the discontinuity.

05:57.680 --> 05:59.720
Here's where we entered a new moment.

05:59.720 --> 06:01.680
Here's artificial intelligence.

06:01.680 --> 06:04.520
Am I looking for something that doesn't exist?

06:04.520 --> 06:08.000
No, no, I think you're not.

06:08.000 --> 06:17.000
So the difference between deep blue and which played chess, deep blue was written using

06:17.000 --> 06:19.960
traditional programming techniques.

06:19.960 --> 06:25.520
And what deep blue did is it, it would, for each move, for each position on the board,

06:25.520 --> 06:27.960
it would look down to all the possible.

06:27.960 --> 06:29.560
Every conceivable decision tree.

06:29.560 --> 06:32.880
Every decision tree, to a certain depth.

06:32.880 --> 06:36.680
I mean, obviously, you can't go all the way.

06:36.680 --> 06:41.040
And it would, it would have ways of, of way, which ones are best.

06:41.040 --> 06:46.160
And so then it would say, no, this is the best move for me at this time.

06:46.160 --> 06:52.160
That's why, in some sense, it was not theoretically very interesting.

06:52.160 --> 06:59.960
The, the go, AlphaGo, AlphaGo, which was a Google project, was that Google project.

06:59.960 --> 07:09.000
This uses deep learning, it's a neural net, it's not explicit, explicit programming.

07:09.000 --> 07:15.640
We don't know, you know, we don't go into it with an idea of, here's the algorithm

07:15.640 --> 07:19.440
we're going to use, do this, and then do this, and do this.

07:19.440 --> 07:25.520
So it was actually quite a surprise, particularly AlphaGo.

07:25.520 --> 07:29.360
Not to me, but to the public, yes.

07:29.360 --> 07:30.360
To the public.

07:30.360 --> 07:31.360
Yeah.

07:31.360 --> 07:35.160
But our, our colleague, I'm going at this one more time because I really want to understand

07:35.160 --> 07:36.160
this.

07:36.160 --> 07:37.160
I really do.

07:37.160 --> 07:40.800
Our colleague here at Stanford, ZX Shen, who must be known to both of you, physicist

07:40.800 --> 07:42.160
here at Stanford.

07:42.160 --> 07:46.480
And he said to me, Peter, what you need to understand about the moment when a computer

07:46.480 --> 07:53.800
defeated go, go, which is in much more complicated, at least in the decision space, much, much

07:53.800 --> 07:55.800
bigger, so to speak, than chess.

07:55.800 --> 07:58.360
There are more pieces, more square, alright.

07:58.360 --> 08:03.900
And ZX said to me, that whereas chess just did more quickly, what a committee of grand

08:03.900 --> 08:10.200
masters would have decided on, the computer in go was creative.

08:10.200 --> 08:13.680
It was pursuing strategies that human beings had never pursued before.

08:13.680 --> 08:15.280
Is there something to that?

08:15.280 --> 08:17.400
Yeah, so there's a famous, no.

08:17.400 --> 08:19.720
If he's getting impatient with me, I'm asking such, go ahead.

08:19.720 --> 08:21.480
No, no, you're asking such good questions.

08:21.480 --> 08:26.320
So in the third game of the, I think it was the third game of the five games, there was

08:26.320 --> 08:27.320
a move.

08:27.320 --> 08:28.720
I think it was move 32.

08:28.720 --> 08:29.720
32 or 35.

08:29.720 --> 08:30.720
32 or 35.

08:30.720 --> 08:39.520
It's that the computer program made a move that really surprised every single go masters.

08:39.520 --> 08:43.480
Not only Lisa Dole himself, but everybody who's watching.

08:43.480 --> 08:46.040
That's a very, that's a very surprising move.

08:46.040 --> 08:49.040
I thought it was, I thought it was a mistake.

08:49.040 --> 08:56.800
In fact, even post-anonymizing how that move came about, the human masters would say, this

08:56.800 --> 08:59.440
is completely unexpected.

08:59.440 --> 09:08.280
What happens is that the computers, like John says, right, is, has the learning ability,

09:08.280 --> 09:16.200
and has the inference ability to think about patterns or to decide on certain movements,

09:16.200 --> 09:25.800
even outside of the trained, familiar human masters, domain of knowledge in this particular

09:25.800 --> 09:26.800
game.

09:26.800 --> 09:27.800
May I?

09:27.800 --> 09:28.800
Go ahead.

09:28.800 --> 09:29.800
Yes, yes.

09:29.800 --> 09:30.800
Expand on that.

09:30.800 --> 09:39.480
Deep neural nets are supremely good pattern recognition systems.

09:39.480 --> 09:45.760
But the patterns they recognize, the patterns they learn to recognize are not necessarily

09:45.760 --> 09:49.760
exactly the patterns that humans recognize.

09:49.760 --> 09:57.800
So it was seeing something about that position, and it made a move that because of the patterns

09:57.800 --> 10:06.320
that it recognized in the, in the board, that made no sense from a human standpoint.

10:06.320 --> 10:12.880
In fact, all of the, all of the lessons in how to play go tell you never make a move that

10:12.880 --> 10:16.420
close to the edge that, that quickly.

10:16.420 --> 10:22.080
And so everybody thought it made a mistake, and then it proceeded to win.

10:22.080 --> 10:28.240
And I think the way to understand that is it's just seeing patterns that we don't see.

10:28.240 --> 10:36.040
It's computing patterns that, that is not traditionally human, and it has the capacity

10:36.040 --> 10:37.040
to compute.

10:37.040 --> 10:38.040
Okay.

10:38.040 --> 10:44.080
I'm trying to, we're already entering this territory, but I am trying really hard to

10:44.080 --> 10:51.480
tease out the, wait a moment, these are still just machines running zeros and ones, bigger

10:51.480 --> 10:55.240
and bigger memory, faster and faster ability to calculate, but we're still dealing with

10:55.240 --> 10:56.960
machines that run zeros and ones.

10:56.960 --> 10:58.560
That's one strand.

10:58.560 --> 11:03.280
And the other strand is, as you well know, 2001 Space Odyssey, where the computer takes

11:03.280 --> 11:04.760
over the ship.

11:04.760 --> 11:07.880
Open the pod bay doors, Hal.

11:07.880 --> 11:09.560
I'm sorry, Dave.

11:09.560 --> 11:11.880
I'm afraid I can't do that.

11:11.880 --> 11:12.880
Okay.

11:12.880 --> 11:16.040
We'll keep, we'll keep, we'll come to this soon enough.

11:16.040 --> 11:20.800
Fei-Fei Li in your memoir, The Worlds I See, quote, I believe our civilization stands

11:20.800 --> 11:28.200
on the cusp of a technological revolution with the power to reshape life as we know

11:28.200 --> 11:29.360
it.

11:29.360 --> 11:33.560
Revolution, reshape life as we know it.

11:33.560 --> 11:37.840
Now you're a man whose whole academic training is in rigor.

11:37.840 --> 11:41.440
Are you going to let her get away with, with this over, kind of wild overstatement?

11:41.440 --> 11:44.760
No, I don't think it's an overstatement.

11:45.000 --> 11:46.000
Oh.

11:46.000 --> 11:47.000
I think she's right.

11:47.000 --> 11:50.680
He told me to write the book.

11:50.680 --> 11:57.360
Mind you, Peter, it's a technology that is extremely powerful, that will allow us and

11:57.360 --> 12:06.080
is allowing us to get computers to do things we never could have programmed them to do.

12:06.080 --> 12:08.480
And it will change everything.

12:08.480 --> 12:14.280
But it's like, a lot of people have said it's like electricity, it's like the steam

12:14.280 --> 12:16.000
revolution.

12:16.000 --> 12:19.600
It's not something necessarily to be afraid of.

12:19.600 --> 12:23.320
It's not that it's going to suddenly take over the world, that's not what Fei-Fei was

12:23.320 --> 12:24.320
saying.

12:24.320 --> 12:25.320
Well, right.

12:25.320 --> 12:32.680
It's a powerful tool that will revolutionize industries and human the way we live, but

12:32.680 --> 12:38.800
the word revolution is not that it's a conscious being, it's just a powerful tool that changes

12:38.800 --> 12:39.800
things.

12:39.840 --> 12:46.560
And at reassuring, if a few pages later Fei-Fei had not gone on to write, there's no separating

12:46.560 --> 12:52.320
the beauty of science from something like, say, the Manhattan Project, close quote.

12:52.320 --> 12:58.880
Nuclear science, we can produce abundant energy, but it can also produce weapons of indescribable

12:58.880 --> 12:59.880
horror.

12:59.880 --> 13:05.040
AI has boogeymen of its own, whether it's killer robots, widespread surveillance, or

13:05.040 --> 13:09.880
even just automating all 8 billion of us out of our jobs.

13:09.880 --> 13:14.800
Now we could devote an entire program to each of those boogeymen, and maybe at some point

13:14.800 --> 13:17.240
we should.

13:17.240 --> 13:22.520
But now that you have scared me, even in the act of reassuring me, and in fact, it throws

13:22.520 --> 13:26.240
me that you're so eager to reassure me that I think maybe I really should be even more

13:26.240 --> 13:27.600
scared than I am.

13:27.600 --> 13:28.960
Let me just go right down.

13:28.960 --> 13:30.520
Here's the killer robots.

13:30.520 --> 13:31.920
Let me quote the late Henry Kissinger.

13:31.920 --> 13:37.640
I'm just going to put these up and let you, you may calm me down if you can.

13:37.640 --> 13:41.840
Henry Kissinger, if you imagine a war between China and the United States, you have artificial

13:41.840 --> 13:43.880
intelligence weapons.

13:43.880 --> 13:51.120
Nobody has tested these things on a broad scale, and nobody can tell exactly what will happen

13:51.120 --> 13:54.920
when AI fighter planes on both sides interact.

13:54.920 --> 13:59.480
So you are then, I'm quoting Henry Kissinger, who is not a fool after all, so you are then

13:59.480 --> 14:02.560
in a world of potentially total destructiveness.

14:02.560 --> 14:03.560
Close quote.

14:03.560 --> 14:04.560
Fei-Fei.

14:04.560 --> 14:10.080
So like I said, I'm now denying how powerful these tools are.

14:10.080 --> 14:17.200
I mean, humanity, before AI has already created tools and technology that are very destructive,

14:17.200 --> 14:18.520
could be very destructive.

14:18.520 --> 14:21.360
We talk about Manhattan Project, right?

14:21.360 --> 14:28.560
But that doesn't mean that we should collectively decide to use this tool in this destructive

14:28.560 --> 14:29.640
way.

14:29.640 --> 14:36.240
Okay, Peter, you know, think back before you even had heard about artificial intelligence.

14:36.240 --> 14:37.840
Which actually, what is it five years ago?

14:37.840 --> 14:38.840
No, I know.

14:38.840 --> 14:40.240
This is all happening so fast.

14:40.240 --> 14:43.640
Just five years ago, or 10 years ago.

14:43.640 --> 14:53.320
Remember the tragic incident where an Iranian passenger plane was shot down flying over the

14:53.320 --> 14:56.240
Persian Gulf by an Aegis system?

14:56.240 --> 14:57.400
Yes, yes.

14:57.400 --> 15:05.760
And one of our ships, an automated system, because it had to be automated in order to

15:05.760 --> 15:06.760
be...

15:06.760 --> 15:07.760
Humans can't react that fast.

15:07.760 --> 15:08.760
Yeah, exactly.

15:08.760 --> 15:14.600
And in this case, for reasons that I think are quite understandable now that you understand

15:14.600 --> 15:19.960
the incident, but it did something that was horrible.

15:19.960 --> 15:24.560
That's not different in kind from what you can do with AI, right?

15:24.560 --> 15:36.800
So we as creators of these devices, or as users of AI, have to be vigilant about what

15:36.800 --> 15:39.440
kind of use we put them to.

15:39.440 --> 15:45.800
And when we decide to put them to one particular use, and there may be uses, the military has

15:45.800 --> 15:53.880
many good uses for them, we have to be vigilant about their doing what we intend them to do

15:53.880 --> 15:56.840
rather than doing things that we don't intend them to do.

15:56.840 --> 15:59.200
So you're announcing a great theme.

15:59.200 --> 16:07.280
And that theme is that what Dr. Fei-Fei Li has invented makes the discipline to which

16:07.280 --> 16:12.640
you have dedicated your life, philosophy, even more important, not less so.

16:12.640 --> 16:13.640
Yeah, that's why we're the co-directors.

16:13.640 --> 16:17.080
The power of this instrument makes the human being more important, not less so.

16:17.080 --> 16:18.080
Am I making...

16:18.080 --> 16:19.080
Am I being glib?

16:19.080 --> 16:20.080
Or is that onto...

16:20.080 --> 16:24.840
So let me tell you a story about...

16:24.840 --> 16:29.120
So Fei-Fei used to live next door to me, or close to next door to me.

16:29.120 --> 16:30.120
And I was talking...

16:30.120 --> 16:34.440
I'm not sure whether that would make me feel more safe or more exposed, no.

16:34.440 --> 16:38.560
And I was talking to her, I was still a pro at this time.

16:38.560 --> 16:45.640
And she said to me, you and John Hennessey started a lot of institutes that brought technology

16:45.640 --> 16:49.320
into other parts of the university.

16:49.320 --> 16:56.320
We need to start an institute that brings philosophy and ethics and the social sciences

16:56.320 --> 17:06.280
into AI, because AI is too dangerous to leave it to the computer scientists alone.

17:06.280 --> 17:07.280
Nothing wrong with that.

17:07.280 --> 17:10.680
There are many stories about how hard it was to persuade him when he was provost, and you

17:10.680 --> 17:11.680
succeeded.

17:11.680 --> 17:12.680
Can I...

17:12.680 --> 17:17.800
Just one more boogeyman briefly, and we'll return to that theme that you just gave us

17:17.880 --> 17:21.160
there, and then we'll get back to the Stanford Institute.

17:21.160 --> 17:22.360
I'm quoting you again.

17:22.360 --> 17:24.240
This is from your memoir.

17:24.240 --> 17:28.560
The prospect of just automating all billion of us out of our jobs.

17:28.560 --> 17:29.560
That's the phrase you used.

17:29.560 --> 17:36.520
Well, it turns out that it took me mere seconds, using my AI-enabled search algorithm, search

17:36.520 --> 17:42.480
device, to find a Goldman Sachs study from last year, predicting that in the United States

17:42.480 --> 17:49.480
and Europe, some two-thirds of all jobs could be automated, at least to some degree.

17:49.480 --> 17:55.880
So, why shouldn't we all be terrified, Henry Kissinger of World Apocalypse, all right, maybe

17:55.880 --> 17:59.360
that's a bit too much, but my job.

17:59.360 --> 18:03.160
So I think job change is real.

18:03.160 --> 18:09.360
Job change is real with every single technological advances that humanity, human civilization

18:09.360 --> 18:11.640
has faced.

18:11.640 --> 18:15.240
That is real, and that's not to be taken lightly.

18:15.240 --> 18:18.440
We also have to be careful with the word job.

18:18.440 --> 18:26.440
Job tends to describe a holistic profession or that a person attaches his or her income

18:26.440 --> 18:29.440
as well as identity.

18:29.440 --> 18:35.480
But there is also, within every job, pretty much within every job, there are so many tasks.

18:35.480 --> 18:41.960
It's hard to imagine there's one job that has only one singular task, right, like being

18:41.960 --> 18:46.600
a professor, being a scholar, being a doctor, being a cook.

18:46.600 --> 18:49.360
All of these jobs have multiple tasks.

18:49.360 --> 18:56.000
What we're seeing is technology is changing how some of these tasks can be done.

18:56.000 --> 19:03.120
And it's true, as it changes these tasks, some of them, some part of them could be automated.

19:03.120 --> 19:07.960
It's starting to change how the jobs are, and eventually it's going to impact jobs.

19:07.960 --> 19:12.840
So this is going to be a gradual process, and it's very important we stay on top of

19:12.840 --> 19:13.840
this.

19:13.840 --> 19:19.400
This is why Human Center AI Institute was founded, is these questions are profound.

19:19.400 --> 19:22.680
They're by definition multidisciplinary.

19:22.680 --> 19:28.840
Computer scientists alone cannot do all the economic analysis, but economists now understanding

19:28.840 --> 19:37.120
what these computer science programs do will not, by themselves, understand the shift of

19:37.120 --> 19:38.120
the jobs.

19:38.120 --> 19:39.120
John, may I tell you?

19:39.120 --> 19:40.120
Go ahead.

19:40.120 --> 19:42.960
But let me just point something out.

19:42.960 --> 19:50.360
The Goldman Sachs study said that such and such percentage of jobs will be automated

19:50.360 --> 19:53.320
or can be automated at least in part.

19:53.320 --> 19:54.320
Yes.

19:54.320 --> 19:58.840
What they're saying is that a certain number of the tasks that go into a particular job

19:58.840 --> 19:59.840
are done.

19:59.840 --> 20:00.840
Exactly.

20:00.840 --> 20:08.400
So Peter, you said it only took me a few seconds to go to the computer and find that

20:08.400 --> 20:10.880
article.

20:10.880 --> 20:12.880
Guess what?

20:12.880 --> 20:17.120
That's one of the tasks that would have taken you a lot of time.

20:17.120 --> 20:21.840
So part of your job has been automated.

20:21.840 --> 20:22.840
Okay.

20:22.840 --> 20:24.280
Now let me tell you a story.

20:24.280 --> 20:25.520
But also empowered.

20:25.520 --> 20:26.520
Empowered.

20:26.520 --> 20:27.520
Empowered.

20:27.520 --> 20:28.520
Exactly.

20:28.520 --> 20:29.520
Fine.

20:29.520 --> 20:30.520
Thank you.

20:30.520 --> 20:31.520
Thank you.

20:31.520 --> 20:32.520
Thank you.

20:32.520 --> 20:33.520
You're making me feel good.

20:33.520 --> 20:34.520
Now let me tell you a story.

20:34.520 --> 20:35.520
All three of us live in California, which means all three of us probably have some friends

20:35.520 --> 20:36.520
down in Hollywood.

20:36.520 --> 20:40.120
And I have a friend who was involved in the writer's strike.

20:40.120 --> 20:41.120
Yeah.

20:41.120 --> 20:42.120
Okay.

20:42.120 --> 20:44.160
And here's the problem.

20:44.160 --> 20:48.960
To run a sitcom, you used to run a writer's room.

20:48.960 --> 20:54.200
And the writer's room would employ seven, a dozen on The Simpsons Show, The Cartoon

20:54.200 --> 20:55.200
Show.

20:55.200 --> 20:57.320
They'd had a couple of writer's rooms running.

20:57.320 --> 20:58.800
They were employing 20.

20:58.800 --> 21:04.080
And these were the last kind of person you'd imagine a computer could replace because they

21:04.080 --> 21:08.880
were well-educated and witty and quick with words.

21:08.880 --> 21:13.120
And you think of computers as just running calculations, maybe spreadsheets, maybe someday

21:13.120 --> 21:18.000
they can eliminate accountants, but writers, Hollywood writers.

21:18.000 --> 21:25.480
But it turns out, and my friend illustrated this for me by saying, doing the artificial

21:25.480 --> 21:33.360
intelligence thing, we're at a prompt, draft a skit for Saturday Night Live in which Joe

21:33.360 --> 21:40.920
Biden and Donald Trump are playing beer pong, 15 seconds.

21:40.920 --> 21:44.680
Now professionals could have tightened it up or made it, but it was pretty funny and

21:44.680 --> 21:46.920
it was instantaneous.

21:46.920 --> 21:48.360
And you know what that means?

21:48.360 --> 21:51.980
That means you don't need four or five of the seven writers.

21:51.980 --> 21:58.080
You need a senior writer to assign intelligence, the artificial, and you need maybe one other

21:58.080 --> 22:02.040
writer or two other writers to tighten it up or redraft it.

22:02.040 --> 22:03.920
It is upon us.

22:03.920 --> 22:08.120
And your artificial intelligence is going to get bad press when it starts eliminating

22:08.120 --> 22:10.520
the jobs of the chattering classes.

22:10.520 --> 22:12.920
And that has already begun.

22:12.920 --> 22:13.920
Tell me I'm wrong.

22:13.920 --> 22:23.200
Do you know, before the agricultural revolution, something like 80, 90% of all the people in

22:23.200 --> 22:28.280
the United States were employed on farms.

22:28.280 --> 22:37.240
We now, now it's down to 2% or 3%, and those same farms, that same land, is far, far more

22:37.240 --> 22:38.240
productive.

22:38.960 --> 22:47.800
Now, would you say that your life or anybody's life now was worse off than it was in the

22:47.800 --> 22:52.480
1890s when everybody was working on the farm?

22:52.480 --> 22:53.480
No.

22:53.480 --> 22:56.160
So yes, you're right.

22:56.160 --> 22:58.320
It will change jobs.

22:58.320 --> 23:00.600
It will make some jobs easier.

23:00.600 --> 23:05.600
It will make, allow us to do things that we could not do before.

23:05.600 --> 23:16.920
And yes, it will allow fewer people to do more of what they were doing before, and consequently

23:16.920 --> 23:20.440
there will be fewer people in that line of work.

23:20.440 --> 23:21.440
That's true.

23:21.440 --> 23:22.440
That is true.

23:22.440 --> 23:24.240
I also want to just point out two things.

23:24.240 --> 23:29.240
One is that jobs is always changing, and that change is always painful.

23:29.240 --> 23:34.720
And we're, as compared scientists, as philosophers, also as citizens of the world, we should be

23:34.720 --> 23:41.080
empathetic of that, and nobody is saying we should just ignore that change in pain.

23:41.080 --> 23:46.200
So this is why we're studying this, we're trying to talk to policymakers, we're educating

23:46.200 --> 23:47.200
the population.

23:47.200 --> 23:53.360
In the meantime, I think we should give more credit to human creativity in the face of

23:53.360 --> 23:54.520
AI.

23:54.520 --> 23:59.640
I start to use this example that's not even AI.

23:59.640 --> 24:08.320
Think about the advanced, speaking of Hollywood, graphics, technology, CGI, and all that, right?

24:08.320 --> 24:09.320
The video gaming industry?

24:09.320 --> 24:12.840
No, no, just animations and all that, right?

24:12.840 --> 24:20.400
One of many of our, including our children's, favorite animation series is by Ghibli Studio.

24:20.400 --> 24:27.680
You know, Princess Nomononaki, my neighbor Totoro, Spirited Away.

24:27.720 --> 24:35.000
All of these were made during a period where computer graphics technology is far more advanced

24:35.000 --> 24:38.280
than these hand-drawn animations.

24:38.280 --> 24:46.000
Yet the beauty, the creativity, the emotion, the uniqueness in this film continue to inspire

24:46.000 --> 24:49.200
and just entertain humanity.

24:49.200 --> 24:56.520
So I think we need to still have that pride and also give the credit to humans.

24:56.520 --> 25:01.640
Let's not forget our creativity and emotion and intelligence is unique.

25:01.640 --> 25:04.640
It's not going to be taken away by technology.

25:04.640 --> 25:05.640
Thank you.

25:05.640 --> 25:07.400
I feel slightly reassured.

25:07.400 --> 25:10.640
I'm still nervous about my job, but I feel slightly reassured.

25:10.640 --> 25:17.120
But you mentioned government a moment ago, which leads us to how we should regulate AI.

25:17.120 --> 25:19.120
Let me give you two quotations.

25:19.120 --> 25:20.120
I'll begin.

25:20.120 --> 25:23.880
I'm coming to the quotation from the two of you, but I'm going to start with a recent

25:23.880 --> 25:28.440
article in the Wall Street Journal by Senator Ted Cruz of Texas and former Senator Phil

25:28.440 --> 25:30.280
Graham also of Texas.

25:30.280 --> 25:36.680
Quote, the Clinton administration took a hands-off approach to regulating the early internet.

25:36.680 --> 25:41.680
In so doing, it unleashed extraordinary economic growth and prosperity.

25:41.680 --> 25:47.720
The Biden administration, by contrast, is impeding innovation in artificial intelligence with

25:47.720 --> 25:49.800
aggressive regulation.

25:49.800 --> 25:50.800
Close quote.

25:50.800 --> 25:52.240
That's them.

25:52.240 --> 25:53.240
This is you.

25:53.600 --> 25:58.120
Also a recent article in the Wall Street Journal, John Etchamendi and Fei-Fei Li.

25:58.120 --> 26:04.360
Quote, President Biden has signed an executive order on artificial intelligence that demonstrates

26:04.360 --> 26:09.880
his administration's commitment to harness and govern the technology.

26:09.880 --> 26:14.280
President Biden has set the stage and now it is time for Congress to act.

26:14.280 --> 26:16.600
Cruz and Graham, less regulation.

26:16.600 --> 26:20.120
Etchamendi and Lee, Biden administration has done well.

26:20.120 --> 26:22.480
Now Congress needs to give us even more.

26:22.480 --> 26:23.480
No.

26:23.480 --> 26:24.480
All right, John.

26:24.480 --> 26:27.040
No, I don't agree with that.

26:27.040 --> 26:33.960
I believe regulating any kind of technology is very difficult and you have to be careful

26:33.960 --> 26:41.120
not to regulate too soon or not to regulate too late.

26:41.120 --> 26:42.800
Let me give you another example.

26:42.800 --> 26:46.000
You talked about the internet and it's true.

26:46.000 --> 26:49.280
The government really was quite hands-off and that's good.

26:49.280 --> 26:50.280
That's good.

26:50.280 --> 26:51.280
It worked out.

26:51.280 --> 26:52.280
It worked out.

26:52.280 --> 26:55.880
Let's also think about social media.

26:55.880 --> 27:04.360
Social media has not worked exactly, worked out exactly the way we want it.

27:04.360 --> 27:12.200
We originally believed that we were going to enter a golden age in which friendship, comedy,

27:12.200 --> 27:19.520
well and everybody would have a voice and we could all live together at Kumbaya and

27:19.520 --> 27:20.520
so forth.

27:20.640 --> 27:22.640
What happened?

27:22.640 --> 27:26.920
Jonathan Haidt has a new book out on the particular pathologies among young people

27:26.920 --> 27:29.200
from all of these social media.

27:29.200 --> 27:30.200
Not an argument.

27:30.200 --> 27:34.680
It's an argument but it's based on lots of data.

27:34.680 --> 27:47.120
It seems to me that I'm in favor of very light-handed and informed regulation to try to put up sort

27:47.120 --> 27:48.120
of bumpers.

27:48.720 --> 27:50.680
I don't know what the analogy is.

27:50.680 --> 27:51.680
Guardrails.

27:51.680 --> 27:53.760
Guardrails for the technology.

27:53.760 --> 28:01.040
I am not for heavy-handed top-down regulation that stifles innovation.

28:01.040 --> 28:02.040
Here's another.

28:02.040 --> 28:04.520
Let me get on to this.

28:04.520 --> 28:07.760
I'm sure you'll be able to adapt your answer to this question too.

28:07.760 --> 28:10.200
I'm continuing your Wall Street Journal piece.

28:10.200 --> 28:13.880
Big tech companies can't be left to govern themselves.

28:13.880 --> 28:17.000
Around here, Silicon Valley, those are fighting words.

28:17.000 --> 28:21.680
Academic institutions should play a leading role in providing trustworthy assessments

28:21.680 --> 28:24.440
and benchmarking of these advanced technologies.

28:24.440 --> 28:29.680
We encourage an investment in human capital to bring more talent to the field of AI with

28:29.680 --> 28:31.160
academia and the government.

28:31.160 --> 28:32.160
Close quote.

28:32.160 --> 28:33.160
Okay.

28:33.160 --> 28:35.280
Now, it is mandatory for me to say this.

28:35.280 --> 28:41.880
So please forgive me, my fellow Stanford employees, apart from anything else.

28:41.880 --> 28:44.720
Why should academic institutions be trusted?

28:44.720 --> 28:48.040
Both the country has lost faith in academic institutions.

28:48.040 --> 28:52.800
DEI, the whole woke agenda, anti-Semitism on campus.

28:52.800 --> 28:57.200
We've got a recent Gallup poll showing the proportion of Americans who expressed a great

28:57.200 --> 29:00.880
deal or quite a lot of confidence in higher education.

29:00.880 --> 29:08.600
This year came in at just 36 percent and that is down in the last eight years from 57 percent.

29:08.600 --> 29:13.120
You are asking us to trust you at the very moment when we believe we have good reason

29:13.120 --> 29:14.840
to knock it off.

29:14.840 --> 29:15.840
Trust you?

29:15.840 --> 29:16.840
Okay.

29:16.840 --> 29:19.440
So I'll start with this first half of the answer.

29:19.440 --> 29:21.120
I'm sure John has a lot to say.

29:21.120 --> 29:28.320
I do want to make sure, especially wearing the hats of co-directors of HAI, when we talk

29:28.320 --> 29:33.720
about the relationship between government and technology, we tend to use the word regulation.

29:33.720 --> 29:36.720
I really, really want to double click.

29:36.720 --> 29:38.960
I want to use the word policy.

29:39.120 --> 29:43.880
Policy and regulation are related but not the same.

29:43.880 --> 29:49.800
When John and I wrote that Wall Street Journal Opinion piece, we really are focusing on a

29:49.800 --> 29:58.160
piece of policy that is to resource public sector AI, to resource academia because we

29:58.160 --> 30:05.600
believe that AI is such a powerful technology and science and academia and public sector

30:05.600 --> 30:10.320
still has a role to play to create public good.

30:10.320 --> 30:15.680
And public goods are curiosity-driven knowledge exploration.

30:15.680 --> 30:25.640
Our cures for cancers are the maps of biodiversity of our globe, our discovery of nanomaterials

30:25.640 --> 30:32.560
that we haven't seen before, our different ways of expressing in theater, in writing,

30:32.560 --> 30:33.560
in music.

30:33.720 --> 30:41.000
These are public goods and when we are collaborating with the government on policy, we're focusing

30:41.000 --> 30:42.000
on that.

30:42.000 --> 30:44.360
So I really want to make sure.

30:44.360 --> 30:49.280
Regulation we all have personal opinion, but there's more than regulation in policy.

30:49.280 --> 30:54.920
John, let me make one last run at you.

30:54.920 --> 31:00.800
In my theory here, although I'm asking questions that I'm quite sure you'd like to take me

31:00.800 --> 31:04.920
out and swap me around at this point, John, but this is serious.

31:04.920 --> 31:09.720
You've got the Stanford Institute for Human-Centered Artificial Intelligence and that's because

31:09.720 --> 31:12.520
you really think this is important.

31:12.520 --> 31:16.880
But we live in a democracy and you're going to have to convince a whole lot of people.

31:16.880 --> 31:20.280
So let me take one more run at you and then hand it back to you, John.

31:20.280 --> 31:22.440
Your article in the Wall Street Journal, again, let me repeat this.

31:22.440 --> 31:27.000
We encourage an investment in human capital to bring more talent to the field of AI with

31:27.000 --> 31:28.920
academia and the government, close quote.

31:28.920 --> 31:30.080
That means money.

31:30.080 --> 31:33.520
An investment means money and it means taxpayers' money.

31:33.520 --> 31:36.640
Here's what Cruz and Graham say in the Wall Street Journal, the Biden regulatory policy

31:36.640 --> 31:42.040
on AI has everything to do with special interest rent seeking.

31:42.040 --> 31:45.720
Stand for faculty, make well above the national average income.

31:45.720 --> 31:51.000
We are sitting at a university with an endowment of tens of billions of dollars.

31:51.000 --> 31:58.480
John, why is not your article in the Wall Street Journal the very kind of rent seeking

31:58.800 --> 32:01.880
that Senator Cruz and Senator Graham are saying?

32:01.880 --> 32:03.040
Are you kidding?

32:03.040 --> 32:08.080
Peter, let's take another example.

32:08.080 --> 32:15.240
So one of the greatest policy decisions that this country has ever made was when Vannevar

32:15.240 --> 32:22.240
Bush, advisor to at the time President Truman, convinced the state on through Eisenhower,

32:23.120 --> 32:24.120
as I recall, so it's important.

32:24.120 --> 32:25.120
Yeah, I'm kidding.

32:25.120 --> 32:26.120
He's bipartisan.

32:26.120 --> 32:27.120
Exactly.

32:27.120 --> 32:28.120
Exactly.

32:28.120 --> 32:36.760
It's not a bipartisan issue at all, but convinced Truman to set up the NSF for funding...

32:36.760 --> 32:37.760
National Science Foundation.

32:37.760 --> 32:47.160
...for funding curiosity-based research, advanced research at the universities, and then not

32:47.160 --> 32:52.760
to cut, not to, you know, say that companies don't have any role, not to say that government

32:52.760 --> 32:53.840
has no role.

32:53.840 --> 33:02.360
They both have roles, but they're different roles, and companies tend to be better at

33:02.360 --> 33:08.760
development, better at producing products and tapping into things that can, within

33:08.760 --> 33:15.360
a year or two or three, can be a product that will be useful.

33:15.360 --> 33:18.440
Scientists at universities don't have that constraint.

33:18.440 --> 33:21.440
They don't have to worry about when is this going to be...

33:21.440 --> 33:22.440
Commercial.

33:22.440 --> 33:23.440
Commercial.

33:24.040 --> 33:35.440
And that has, I think, had such an incalculable effect on the prosperity of this country,

33:35.440 --> 33:40.600
on the fact that we are the leader in every technology field.

33:40.600 --> 33:44.760
It's not an accident that we're the leader in every technology field.

33:44.760 --> 33:45.840
We didn't used to be.

33:45.840 --> 33:51.480
And does it affect your argument if I add it also enabled us or contributed to a victory

33:51.480 --> 33:52.480
in the Cold War?

33:53.480 --> 33:56.480
The weapons systems that came out of universities?

33:56.480 --> 33:57.480
All right.

33:57.480 --> 33:58.480
Well, no, absolutely.

33:58.480 --> 34:02.480
And, you know, President Reagan and Star Wars.

34:02.480 --> 34:06.240
In other words, it ended up being a defensive, kind of good, you could argue from all kinds

34:06.240 --> 34:09.720
of points of view as it was a good ROI for taxpayers' money.

34:09.720 --> 34:10.720
Yeah.

34:10.720 --> 34:11.720
All right.

34:11.720 --> 34:16.680
So we're not arguing for higher salaries for faculty or anything of that sort.

34:16.680 --> 34:26.160
But we think, particularly in AI, it's gotten to the point where scientists at universities

34:26.160 --> 34:33.920
can no longer play in the game because of the cost of the computing, the cost, the inaccessibility

34:33.920 --> 34:35.480
of the data.

34:35.480 --> 34:38.600
That's why you see all of these developments coming out of companies.

34:38.600 --> 34:39.600
That's great.

34:39.600 --> 34:41.440
Those are great developments.

34:41.440 --> 34:50.240
But we need to have also people who are exploring these technologies without looking at the

34:50.240 --> 34:54.480
product, without being driven by the profit motive.

34:54.480 --> 34:59.360
And then eventually, hopefully, they will develop discoveries, they will make discoveries,

34:59.360 --> 35:01.360
and we'll then be commercializable.

35:01.360 --> 35:02.360
Okay.

35:02.360 --> 35:06.240
I noticed in your book, Fei-Fei, I was very struck that you said, oh, I think it was about

35:06.240 --> 35:12.600
a decade ago, 2015, I think, was that you noticed that you were beginning to lose colleagues

35:12.600 --> 35:18.440
to the private sector, presumably because they just pay so phenomenally well around

35:18.440 --> 35:19.440
here in Silicon Valley.

35:19.440 --> 35:25.280
But then there's also the point that to get to make progress in AI, you need an enormous

35:25.280 --> 35:27.600
amount of computational power.

35:27.600 --> 35:31.840
And assembling all those ones and zeros is extremely expensive.

35:31.840 --> 35:35.600
So ChatGPT, what is the parent company?

35:36.320 --> 35:37.320
Open AI.

35:37.320 --> 35:42.400
Open AI got started with an initial investment of a billion dollars.

35:42.400 --> 35:47.360
And friends and family capital of a billion dollars is a lot of money even around here.

35:47.360 --> 35:48.360
Okay.

35:48.360 --> 35:49.360
That's the point you're making.

35:49.360 --> 35:50.360
Yes.

35:50.360 --> 35:52.860
All right.

35:52.860 --> 35:56.960
It feels to me as though every one of these topics is worth a day long seminar.

35:56.960 --> 35:59.320
Actually, I think that they are.

35:59.320 --> 36:07.040
And by the way, this has happened before where the science has become so expensive that it

36:07.040 --> 36:12.640
could no longer, that university level research and researchers could no longer afford to

36:12.640 --> 36:14.460
do the science.

36:14.460 --> 36:17.800
It happened in high-energy physics.

36:17.800 --> 36:24.240
High-energy physics used to mean you had a Vandegraaff generator in your office, and

36:24.240 --> 36:29.120
that was your accelerator, or you could do what you needed to do.

36:29.720 --> 36:36.480
And then it no longer was, you know, the energy levels were higher and higher.

36:36.480 --> 36:37.480
And what happened?

36:37.480 --> 36:42.000
Well, the federal government stepped in and said, we're going to help.

36:42.000 --> 36:45.120
We're going to build an accelerator.

36:45.120 --> 36:46.120
Stanford linear accelerator.

36:46.120 --> 36:47.120
Stanford linear accelerator.

36:47.120 --> 36:48.120
Exactly.

36:48.120 --> 36:51.640
Sandia Labs, Lawrence Livermore, all these are at least in part federal established.

36:51.640 --> 36:52.640
CERN.

36:52.640 --> 36:53.840
CERN, which is European.

36:53.840 --> 36:54.840
Right.

36:54.840 --> 36:55.840
Well, Fermilab.

36:55.840 --> 37:03.240
The first accelerator was Slack, Stanford linear accelerator center, then Fermilab, and so

37:03.240 --> 37:05.240
on and so forth.

37:05.240 --> 37:11.760
CERN is actually late in the game, and it's European consortium.

37:11.760 --> 37:21.800
But the thing is, we could not continue the science without the help of the government

37:21.800 --> 37:22.800
and government.

37:22.920 --> 37:30.160
There is another, and then in addition to high energy physics and then bio, right, especially

37:30.160 --> 37:36.080
with genetic sequencing and high throughput genomics, and biotech is also changing.

37:36.080 --> 37:44.840
And now you see a new wave of biology labs that are actually heavily funded by the combination

37:44.840 --> 37:52.480
of government and philanthropy and all that, and that stepped in to, you know, supplement

37:52.480 --> 37:55.160
what the traditional university model is.

37:55.160 --> 37:58.360
And so we're now here with AI and computer science.

37:58.360 --> 37:59.360
Okay.

37:59.360 --> 38:05.480
This is, we have to do another show on that one alone, I think.

38:05.480 --> 38:06.480
The Singularity.

38:06.480 --> 38:08.480
Oh, good.

38:08.480 --> 38:09.480
This is good.

38:09.480 --> 38:10.480
Reassuring.

38:10.480 --> 38:11.480
You're both, I mean, rolling your eyes.

38:11.480 --> 38:12.480
Wonderful.

38:12.480 --> 38:14.480
I feel better about this already.

38:14.480 --> 38:15.480
Good.

38:15.480 --> 38:16.480
Ray Kurzweil.

38:16.480 --> 38:17.480
You know exactly where this is going.

38:17.480 --> 38:19.120
Ray Kurzweil writes a book in 2005.

38:19.120 --> 38:23.400
This gets everybody's attention and still scares lots of people to death, including

38:23.400 --> 38:24.400
me.

38:24.400 --> 38:30.240
The book is called The Singularity is Near, and Kurzweil predicts a singularity that

38:30.240 --> 38:37.480
will involve, and I'm quoting him, the merger of human technology with human intelligence.

38:37.480 --> 38:41.040
He's not saying the tech will mimic more and more closely human intelligence.

38:41.040 --> 38:43.200
He is saying they will merge.

38:43.200 --> 38:47.600
I set the date for the singularity representing a profound and disruptive transformation in

38:47.600 --> 38:50.880
human capability as 2045.

38:50.880 --> 38:52.880
Okay.

38:52.880 --> 38:53.880
That's the first quotation.

38:53.880 --> 38:54.880
Here's the second.

38:54.880 --> 38:59.280
And this comes from the Stanford Course Catalog's description of the philosophy of artificial

38:59.280 --> 39:06.640
intelligence, a freshman seminar that was taught last quarter, as I recall, by one John

39:06.640 --> 39:07.640
Echamendi.

39:07.640 --> 39:14.120
Here, here's from the description, is it really possible for an artificial system to

39:14.120 --> 39:18.880
achieve genuine intelligence, thoughts, consciousness, emotions?

39:18.880 --> 39:20.880
What would that mean?

39:20.880 --> 39:23.120
John, is it possible?

39:23.120 --> 39:26.840
What would it mean?

39:26.840 --> 39:30.240
I think the answer is actually no.

39:30.240 --> 39:33.840
And thank goodness, you kept me waiting for a moment.

39:33.840 --> 39:45.800
I think the fantasies that Ray Kurzweil and others have been spinning up, I guess that's

39:45.800 --> 39:56.720
the way to put it, stem from a lack of understanding of how the human being really works and don't

39:56.720 --> 40:06.600
understand how crucial biology is to the way we work, the way we are motivated, how we

40:06.600 --> 40:14.240
get desires, how we get goals, how we become humans, become people.

40:14.240 --> 40:23.360
And what AI has done so far, AI is capturing what you might think of as the information

40:23.360 --> 40:27.800
processing piece of what we do.

40:27.800 --> 40:30.800
So part of what we do is information processing.

40:30.800 --> 40:35.120
So it's got the right frontal cortex, but it hasn't got the left frontal cortex yet?

40:35.120 --> 40:37.480
Yeah, it's an oversimplification, but yes.

40:37.480 --> 40:40.000
Imagine that on television.

40:40.000 --> 40:52.400
So I actually think it is, first of all, the date, 2045, is insane.

40:53.400 --> 40:56.840
And secondly, it's not even clear to me that we will ever go back.

40:56.840 --> 40:59.080
Wait, I can't believe I'm saying this.

40:59.080 --> 41:06.320
In his defense, I don't think he's saying that 2045 is the day that the machines become

41:06.320 --> 41:10.000
conscious beings like humans.

41:10.000 --> 41:18.960
It's more an inflection point of the power of the technology that is disrupting the society.

41:18.960 --> 41:19.960
Well, that's in his late.

41:19.960 --> 41:20.960
He's late.

41:20.960 --> 41:21.960
We're already there.

41:21.960 --> 41:23.240
That's what I'm saying.

41:23.240 --> 41:30.120
I think you're being overly generous.

41:30.120 --> 41:35.200
But he means by the singularity is the date at which we create an artificial intelligence

41:35.200 --> 41:43.480
system that can improve itself and then get into a cycle, a recursive cycle, where it

41:43.480 --> 41:46.880
becomes a superintelligence.

41:46.880 --> 41:48.640
And I deny that.

41:48.640 --> 41:51.940
He's playing the 2001 Space Odyssey game here.

41:51.940 --> 41:54.940
And it's a different question, but related question.

41:54.940 --> 42:00.820
In some ways, this is a more serious question, I think, although that's serious too.

42:00.820 --> 42:09.020
Here's the late Henry Kissinger again, quote, we live in a world which has no philosophy.

42:09.020 --> 42:15.980
There is no dominant philosophical view, so the technologists can run wild.

42:15.980 --> 42:20.660
They can develop world-changing things, and there's nobody to say, we've got to integrate

42:20.740 --> 42:22.780
this into something.

42:22.780 --> 42:26.460
All right, I'm going to put it crudely again.

42:26.460 --> 42:32.700
But in China, a century ago, we still had Confucian thought, dominant at least among

42:32.700 --> 42:37.700
the educated classes on my very thin understanding of Chinese history.

42:37.700 --> 42:43.660
In this country, until the day before yesterday, we still spoke without irony of the Judeo-Christian

42:43.660 --> 42:52.140
tradition, which involved certain concepts about morality, what it meant to be human.

42:52.140 --> 42:56.700
It assumed a belief in God, but it turned out you could actually get pretty far along,

42:56.700 --> 42:59.260
even if you didn't believe in OK.

42:59.260 --> 43:02.740
And Kissinger is now saying, it's all fallen apart.

43:02.740 --> 43:05.760
There is no dominant philosophy.

43:05.760 --> 43:08.020
This is a serious problem, is it not?

43:08.020 --> 43:11.540
There's nothing to integrate AI into.

43:11.540 --> 43:14.980
You take his point.

43:14.980 --> 43:15.980
It's up to the children.

43:15.980 --> 43:18.980
You're the philosopher.

43:18.980 --> 43:23.020
You're the philosopher.

43:23.020 --> 43:26.580
I think this is a great, first of all, thank you for that quote.

43:26.580 --> 43:30.260
I didn't read that quote from Henry Kissinger.

43:30.260 --> 43:34.100
I mean, this is why we founded the Human Center AI Institute.

43:34.100 --> 43:39.500
These are the fundamental questions that our generation needs to figure out.

43:39.860 --> 43:40.860
That's not just a question.

43:40.860 --> 43:41.860
That's the question.

43:41.860 --> 43:43.780
It was one of the fundamental questions.

43:43.780 --> 43:48.780
It's also one of the fundamental questions that illustrates why universities are still

43:48.780 --> 43:52.180
relevant today.

43:52.180 --> 43:58.140
And Peter, one of the things that Henry Kissinger says in that quote is that there is no dominant

43:58.140 --> 43:59.140
philosophy.

43:59.140 --> 44:05.780
There's no one dominant philosophy like the Judeo-Christian tradition, which used to be

44:05.780 --> 44:08.060
the dominant tradition in the US.

44:08.100 --> 44:11.300
It's a different conversation in Paris in the 12th century, for example, the University

44:11.300 --> 44:12.300
of Paris.

44:12.300 --> 44:20.460
In order to take values into account when you're creating an AI system, you don't need

44:20.460 --> 44:25.340
a dominant tradition.

44:25.340 --> 44:31.060
What you need, for example, for most ethical traditions is the Golden Rule.

44:31.060 --> 44:33.180
Go back to the Confucius.

44:33.180 --> 44:35.980
We can still get along with each other.

44:35.980 --> 44:40.100
Even when it comes to deep, deep questions of value such as this, we still have enough

44:40.100 --> 44:41.820
common ground.

44:41.820 --> 44:44.820
I believe so.

44:44.820 --> 44:46.980
I heave yet another sigh of relief.

44:46.980 --> 44:49.340
Okay, let's talk a little bit.

44:49.340 --> 44:53.340
We're talking a little bit about a lot of things here, but so it is.

44:53.340 --> 44:57.260
Let us speak of many things as it is written in Alice in Wonderland.

44:57.260 --> 44:59.580
The Stanford Institute.

44:59.580 --> 45:03.940
The Stanford Institute for Human-Centered Artificial Intelligence, of which you are

45:03.980 --> 45:08.940
co-directors, and I just have two questions and respond as you'd like.

45:08.940 --> 45:16.220
Can you give me some taste, some feel for what you're doing now, and in some ways more

45:16.220 --> 45:20.780
important, but more elusive, where you'd like to be in just five years, say.

45:20.780 --> 45:21.780
Everything in this field is moving.

45:21.780 --> 45:25.220
So I would, my impulse is to say 10 years because it's a rounder number.

45:25.220 --> 45:27.420
It's too far off in this field.

45:27.420 --> 45:28.420
Fei-Fei.

45:28.420 --> 45:33.300
I think what really has happened in the past five years by Stanford High, among many

45:33.380 --> 45:34.380
things.

45:34.380 --> 45:36.460
I just want to make sure everybody following you.

45:36.460 --> 45:39.540
H-A-I, Stanford High is the way it's known on this campus.

45:39.540 --> 45:40.540
Yes.

45:40.540 --> 45:41.540
Go ahead.

45:41.540 --> 45:42.540
Yeah.

45:42.540 --> 45:48.420
Is that we have put a stick on the ground for Stanford as well as for everybody that

45:48.420 --> 45:56.420
this is an interdisciplinary study that AI, artificial intelligence, is a science of

45:56.420 --> 45:57.500
its own.

45:57.500 --> 46:04.940
It's a powerful tool, and what happens is that you can welcome so many disciplines to

46:04.940 --> 46:11.900
cross-pollinate around the topic of AI or use the tools of AI to make other sciences

46:11.900 --> 46:15.300
happen or to explore other new ideas.

46:15.300 --> 46:23.260
And that concept of making this an interdisciplinary and multidisciplinary field is what I think

46:23.260 --> 46:28.140
Stanford High brought to Stanford and also hopefully to the world.

46:28.140 --> 46:33.540
Because like you said, computer science is kind of a new field, you know, only, you know,

46:33.540 --> 46:39.620
the late John McCarthy coined the term, you know, in the late fifties.

46:39.620 --> 46:41.260
Now it's moving so fast.

46:41.260 --> 46:47.380
Everybody feels it's just a niche computer science field that's just like making its

46:47.380 --> 46:48.820
way into the future.

46:48.820 --> 46:52.340
And we're saying, no, look abroad.

46:52.340 --> 46:54.980
There's so many disciplines that can be put here.

46:54.980 --> 46:57.900
Who competes with the Stanford Institute and Human-Centered Design?

46:57.900 --> 47:00.380
Is there such an institute at Harvard or Oxford or Beijing?

47:00.380 --> 47:01.980
I just don't know what the...

47:01.980 --> 47:02.980
Oh, thank you.

47:02.980 --> 47:08.660
So in the five years since we launched, there have been a number of similar institutes that

47:08.660 --> 47:11.780
have been created at other universities.

47:11.780 --> 47:14.260
We don't see that as competition in any way.

47:14.260 --> 47:17.060
If these arguments you've been making are valid, then we need them.

47:17.060 --> 47:18.060
We should welcome them.

47:18.300 --> 47:19.300
As a movement.

47:19.300 --> 47:20.300
We need them.

47:20.300 --> 47:23.820
And part of what we want to do and part of what I think we've succeeded to a certain

47:23.820 --> 47:33.500
extent doing is communicating this vision of the importance of keeping the human and

47:33.500 --> 47:41.780
human values at the center when we are developing this technology, when we are applying this

47:41.780 --> 47:44.420
technology.

47:44.420 --> 47:47.620
And we want to communicate that to the world.

47:47.620 --> 47:53.060
We want other centers that adopt a similar standpoint.

47:53.060 --> 47:59.380
And importantly, one of the things that I didn't mention is one of the things we try

47:59.380 --> 48:08.500
to do is educate and educate, for example, legislators so that they understand what this

48:08.500 --> 48:12.780
technology is, what it can do, what it can't do.

48:12.780 --> 48:17.380
So you're traveling to Washington or the very generous trustees of this institution

48:17.380 --> 48:22.260
are bringing congressional staff and they're both, both are happening.

48:22.260 --> 48:28.420
So are you, first of all, did you teach that course in Stanford HAI or was the course located

48:28.420 --> 48:30.260
in the philosophy department or cross-list?

48:30.260 --> 48:33.220
I'm just trying to get a feel for what's actually taking place there now.

48:33.220 --> 48:34.220
Yeah.

48:34.220 --> 48:38.620
I actually taught it in the confines of the HAI building.

48:38.620 --> 48:39.620
Okay.

48:39.620 --> 48:40.620
So it's an HAI.

48:40.620 --> 48:41.620
No, it's a philosophy.

48:41.620 --> 48:45.540
It's listed as a philosophy course, but taught in the HAI.

48:45.540 --> 48:46.540
He's the former provost.

48:46.540 --> 48:49.700
He gets to, he's an interdisciplinary walking wonder.

48:49.700 --> 48:57.980
And your work in AI assisted healthcare, is that taking place in HAI or is it at the medical

48:57.980 --> 48:58.980
school?

48:58.980 --> 48:59.980
Well, that's the beauty.

48:59.980 --> 49:05.300
It's taking place in HAI, computer science department, the medical school, even has collaborators

49:05.300 --> 49:09.540
from the law school, from the political science department.

49:09.540 --> 49:10.540
So that's the beauty.

49:10.620 --> 49:13.220
It's deeply interdisciplinary.

49:13.220 --> 49:16.780
If I were the provost, I'd say this is starting to sound like something that's about to run

49:16.780 --> 49:17.780
a muck.

49:17.780 --> 49:20.220
Doesn't that sound a little too interdisciplinary, John?

49:20.220 --> 49:23.180
Don't we need to define things a little bit here?

49:23.180 --> 49:26.180
Let me, let me tell you, let me say something.

49:26.180 --> 49:32.580
So Steve Denning, who was the chair of our board of trustees for many years and has been

49:32.580 --> 49:38.740
a long, long time supporter of the university in many, many ways.

49:38.740 --> 49:46.260
In fact, we are the Denning co-directors of Stanford HAI, Stanford HAI.

49:46.260 --> 49:55.540
Steve saw five, six years ago, he said, you know, AI is going to impact in a free department

49:55.540 --> 49:58.620
at this university.

49:58.620 --> 50:05.500
And we need to have an institute that makes sure that that happens the right way, that

50:05.620 --> 50:10.620
that impact is, is, does not run a muck.

50:10.620 --> 50:14.180
Where would you like to be in five years?

50:14.180 --> 50:17.060
What's a, what's a course you'd like to be teaching in five years?

50:17.060 --> 50:19.340
What's a, what's a special project?

50:19.340 --> 50:25.140
I would like to teach a course, freshman seminar called The Greatest Discoveries by AI.

50:25.140 --> 50:27.340
Oh, really?

50:27.340 --> 50:30.340
Okay.

50:30.340 --> 50:35.460
A last question, which I have one last question, but that does not.

50:35.460 --> 50:38.580
That means that it has, you, each of you has to hold yourself to one last answer because

50:38.580 --> 50:41.860
it's a kind of open-ended question.

50:41.860 --> 50:46.740
I have a theory, but all I do is wander around this campus.

50:46.740 --> 50:50.580
The two of you are deeply embedded here and you ran the place for 17 years, so you'll

50:50.580 --> 50:52.380
know more than I will.

50:52.380 --> 50:56.100
Including you may know that my theory is wrong, but I'm going to trot it out, modest

50:56.100 --> 51:00.460
though it may be, even so.

51:00.460 --> 51:05.140
Milton Friedman, the late Milton Friedman, who when I first arrived here was a colleague

51:05.140 --> 51:06.140
at the Hoover Institution.

51:06.140 --> 51:10.700
In fact, by some miracle, his office was on the same hallway as mine and I used to stop

51:10.700 --> 51:13.580
in on him from time to time.

51:13.580 --> 51:20.340
He told me that he went into economics because he grew up during the Depression and the overriding

51:20.340 --> 51:27.580
question in the country at that time was how do we satisfy our material needs?

51:27.580 --> 51:29.540
There were millions of people without jobs.

51:29.540 --> 51:33.420
There really were people who had trouble feeding their families.

51:33.420 --> 51:35.140
All right.

51:35.140 --> 51:39.860
I think of my own generation, which is more or less John's generation.

51:39.860 --> 51:41.820
You come much later, Faye Faye.

51:41.820 --> 51:42.820
Thank you.

51:42.820 --> 51:48.180
And for us, I don't know what kind of discussions you had in the dorm room, but when I was in

51:48.180 --> 51:52.620
college, there were both sessions about the Cold War, where the Russians...

51:52.620 --> 51:55.940
The Cold War was real to our generation.

51:55.940 --> 51:59.540
That was the overriding question.

51:59.540 --> 52:01.180
How can we defend our way of life?

52:01.180 --> 52:04.180
How can we defend our fundamental principles?

52:04.180 --> 52:05.700
All right.

52:05.700 --> 52:07.980
Here's my theory.

52:07.980 --> 52:16.620
For current students, they've grown up in a period of unimaginable prosperity.

52:16.620 --> 52:19.860
Material needs are just not the problem.

52:19.860 --> 52:24.540
They have also grown up during a period of relative peace.

52:24.540 --> 52:26.020
The Cold War ended.

52:26.020 --> 52:27.300
You could put different...

52:27.300 --> 52:31.260
The Soviet Union declared itself defunct in 1991.

52:31.260 --> 52:35.300
Cold War is over at that moment of the latest.

52:35.300 --> 52:41.060
The overriding question for these kids today is meaning.

52:41.060 --> 52:43.860
What is it all for?

52:43.860 --> 52:45.700
Why are we here?

52:45.700 --> 52:48.140
What does it mean to be human?

52:48.140 --> 52:53.140
What's the difference between us and the machines?

52:53.380 --> 53:00.940
If my little theory is correct, then by some miracle, this technological marvel that you

53:00.940 --> 53:06.580
have produced will lead to a new flowering of the humanities.

53:06.580 --> 53:12.460
Do you go for that, John?

53:12.460 --> 53:13.460
Do I go for it?

53:13.460 --> 53:17.060
I would go for it if it were going to happen.

53:17.060 --> 53:19.180
Did I put that in a slightly sloppy way?

53:19.180 --> 53:21.500
No.

53:21.500 --> 53:23.260
I think it would be wonderful.

53:23.260 --> 53:27.980
It's something to hope for.

53:27.980 --> 53:31.060
Now I'm going to be the cynic.

53:31.060 --> 53:37.100
So far, what I see in students is more and more focus, or Stanford students, more and

53:37.100 --> 53:41.380
more focus on technology, on learning.

53:41.380 --> 53:47.020
Computer science is still the biggest major at this university.

53:47.020 --> 53:48.980
We have tried at HAI.

53:48.980 --> 53:56.180
We have actually started a program called Embedded Ethics, where the CS at the end of

53:56.180 --> 54:02.540
ethics is capitalized, so it's computer science.

54:02.540 --> 54:04.060
That'll catch the kids' attention.

54:04.060 --> 54:07.500
No, we don't have to catch their attention.

54:07.500 --> 54:15.620
What we do is virtually all of the courses in computer science, the introductory courses,

54:15.620 --> 54:19.020
have ethics components built in.

54:19.020 --> 54:24.900
So a problem set, so you have a problem set this week, and that'll have a whole bunch

54:24.900 --> 54:32.460
of very difficult math problems, computer science problem, and then it will have a very

54:32.460 --> 54:34.460
difficult ethical challenge.

54:34.460 --> 54:37.980
It'll say, here's the situation.

54:37.980 --> 54:46.400
You are programming a computer, a programming an AI system, and here's the dilemma.

54:46.400 --> 54:47.900
Now discuss.

54:47.900 --> 54:49.500
What are you going to do?

54:49.500 --> 54:54.140
So we're trying to bring, and this is what they wanted.

54:54.140 --> 55:00.820
We're trying to bring ethics within the last couple of years, okay, two, three years.

55:00.820 --> 55:08.620
We're trying to bring the attention to ethics into the computer science curriculum.

55:08.620 --> 55:14.680
And partly that's because they're not, I mean, students tend to follow the path of least

55:14.680 --> 55:15.680
resistance.

55:15.680 --> 55:19.380
Well, they also, let's put it, again, if I'm saying things crudely again and again,

55:19.380 --> 55:22.580
but someone must say it, they follow the money.

55:22.580 --> 55:29.420
So as long as this valley that surrounds us rewards brilliant young kids from Stanford

55:29.540 --> 55:35.440
with CS degrees as richly as it does, and it is amazingly richly, they'll go get CS

55:35.440 --> 55:36.940
degrees, right?

55:36.940 --> 55:44.020
Well, I do think it's a little crude.

55:44.020 --> 55:54.700
I think money is one surrogate measure of also what is advancing in our time.

55:54.700 --> 56:02.300
Maybe right now truly is one of the biggest drivers of the changes of our civilization.

56:02.300 --> 56:07.540
When you're talking about what is this generation of students talk about, I was just thinking

56:07.540 --> 56:13.900
that 400 years ago, when the scientific revolution was happening, what is in the dorms, of course

56:13.900 --> 56:21.100
it's all young men in Cambridge or Oxford, but that must also be a very exciting and

56:21.100 --> 56:22.100
interesting time.

56:22.100 --> 56:27.420
Of course, there was an internet and social media to propel the travel of the knowledge,

56:27.420 --> 56:36.900
but imagine there was the blossoming of discovery and of our understanding of the physical world.

56:36.900 --> 56:42.300
Right now we're in that kind of great era of technological blossoming.

56:42.300 --> 56:44.020
It's a digital revolution.

56:44.020 --> 56:50.740
So the conversations in the dorm, I think it's a blend of the meaning of who we are

56:50.780 --> 56:55.900
as humans, as well as our relationship to these technologies we're building.

56:55.900 --> 56:57.980
And so it's a...

56:57.980 --> 57:08.740
So properly taught technology can subsume or embed philosophy literature?

57:08.740 --> 57:12.140
Of course, can inspire, can inspire.

57:12.140 --> 57:16.540
And also think about it, what follows scientific revolution is a great period of change of

57:16.540 --> 57:19.900
political, social, economical change, right?

57:20.100 --> 57:21.100
We're seeing that.

57:21.100 --> 57:22.100
Not all for the better.

57:22.100 --> 57:23.100
Right.

57:23.100 --> 57:29.380
And I'm not saying it's necessary for the better, but we are seeing, we're having even

57:29.380 --> 57:35.220
peaked the digital revolution, but we're already seeing the political, social, economic changes.

57:35.220 --> 57:41.700
So this is, again, back to Stanford High when we founded it five years ago.

57:41.700 --> 57:47.780
We believe all this is happening and this is an institute where these kind of conversations,

57:47.780 --> 57:55.140
ideas, debates should be taking place and education programs should be happening.

57:55.140 --> 57:58.180
And that's part of the reason why we did this.

57:58.180 --> 58:05.580
Let me tell you, yeah, so as you pointed out, I just finished teaching a course called Philosophy

58:05.580 --> 58:09.980
of Artificial Intelligence, about which I found out too late, I would have asked permission

58:09.980 --> 58:11.260
to audit your course, John.

58:11.260 --> 58:14.060
No, no, you're too old.

58:14.060 --> 58:20.140
So and about half of the students were computer science students who were planned to be computer

58:20.140 --> 58:22.260
science majors.

58:22.260 --> 58:30.420
Another quarter planned to be symbolic systems majors, which is a major that is related to

58:30.420 --> 58:36.380
computer science, and then there was a smattering of others.

58:36.380 --> 58:41.660
And these were people, every one of them at the end of the course, and I'm not saying

58:41.660 --> 58:46.900
this to brag, every one of them said, this is the best course we've ever taken.

58:46.900 --> 58:49.340
And why did they say that?

58:49.340 --> 58:53.220
It inspired, it made them think.

58:53.220 --> 58:59.820
It gave them a framework for thinking, a framework for trying to address some of these problems,

58:59.820 --> 59:03.220
some of the worries that you've brought out today.

59:03.220 --> 59:10.820
And how do we think about them and how do we not just become panicked because of some

59:10.860 --> 59:18.020
science fiction movie that we've seen, or because we read Ray Kurzweil.

59:18.020 --> 59:20.700
So maybe it's just as well I didn't take the course.

59:20.700 --> 59:24.580
I'm sure John would have given me a C-minus at best.

59:24.580 --> 59:27.020
Great inflation.

59:27.020 --> 59:40.700
So it's clear that these kids, the students, are looking for the opening to think the

59:40.740 --> 59:48.740
things and to understand how to address ethical questions, how to address hard philosophical

59:48.740 --> 59:55.340
questions, and that's what they got out of the course.

59:55.340 --> 59:58.460
And that's a way of looking for meaning in this time.

59:58.460 --> 01:00:00.460
Yes it is.

01:00:00.460 --> 01:00:06.420
Dr. Feifei Li and Dr. John Etchamendi, both of the Stanford Institute for Human-Centered

01:00:06.420 --> 01:00:08.260
Artificial Intelligence.

01:00:08.260 --> 01:00:09.260
Thank you.

01:00:09.260 --> 01:00:10.380
Thank you, Peter.

01:00:10.380 --> 01:00:14.660
For Uncommon Knowledge and the Hoover Institution and Fox Nation, I'm Peter Robinson.

