start	end	text
0	4740	The year was 1956 and the place was Dartmouth College.
4740	10640	In a research proposal, a math professor used a term that was then entirely new and entirely
10640	14720	fanciful, artificial intelligence.
14720	17980	There's nothing fanciful about AI anymore.
17980	22080	The directors of the Stanford Institute for Human-Centered Artificial Intelligence, John
22080	24440	Etchimendi, and Fei-Fei Li.
24440	36800	On Uncommon Knowledge, now.
36800	38120	Welcome to Uncommon Knowledge.
38120	39800	I'm Peter Robinson.
39800	46520	Philosopher John Etchimendi served from 2000 to 2017 as provost here at Stanford University.
46520	51240	Dr. Etchimendi received his undergraduate degree from the University of Nevada before earning
51240	54320	his doctorate in philosophy at Stanford.
54320	59280	He earned that doctorate in 1983 and became a member of the Stanford Philosophy Department
59280	61200	the very next year.
61200	67840	He's the author of a number of books, including the 1990 volume The Concept of Logical Consequence.
67840	71360	Since stepping down as provost, Dr. Etchimendi has held a number of positions at Stanford,
71360	76800	including, and for our purposes today, this is the relevant position, co-director of the
76800	81280	Stanford Institute for Human-Centered Artificial Intelligence.
81280	86600	Born in Beijing, Dr. Fei-Fei Li moved to this country at the age of 15.
86600	91160	She received her undergraduate degree from Princeton and a doctorate in electrical engineering
91160	94240	from the California Institute of Technology.
94240	99040	Now a professor of computer science here at Stanford, Dr. Li is the founder once again
99040	103040	of the Stanford Institute for Human-Centered Artificial Intelligence.
103800	109400	Dr. Li's memoir published just last year, The Worlds I See, Curiosity, Exploration,
109400	112440	and Discovery at the Dawn of AI.
112440	115840	John Etchimendi and Fei-Fei Li, thank you for making the time to join me.
115840	119240	Thank you for inviting us.
119240	124360	I would say that I'm going to ask a dumb question, but I'm actually going to ask a question
124360	127600	that is right at the top of my form.
127600	130360	What is artificial intelligence?
130360	135680	I have seen the term 100 times a day for, what, several years now.
135680	141720	I have yet to find a succinct and satisfying explanation.
141720	142720	Let's see.
142720	143720	Well, let's go to the philosophy.
143720	146520	Here's a man who's professionally rigorous, but here's a woman who actually knows the
146520	147520	answer.
147520	148520	Yeah, she knows the answer.
148520	151520	So, let's say the answer, and then I will give you a different answer.
151520	152520	Oh, really?
152520	153520	All right.
153520	154520	Okay.
154520	159120	Peter used the word succinct, and I'm sweating here, so because artificial intelligence
159120	170720	by today is already a collection of methods and tools that summarizes the overall area
170720	180800	of computer science that has to do with data, pattern recognition, decision-making in natural
180800	188160	language, in images, in videos, in robotics, in speech, so it's really a collection at
188160	194600	the heart of artificial intelligence is statistical modeling, such as machine learning, using
194600	196600	computer programs.
196600	204120	But today, artificial intelligence truly is an umbrella term that covers many things
204120	209160	that we're starting to feel familiar about, for example, language intelligence, language
209160	212560	modeling, or speech, or vision.
212920	219080	And you and I both knew John McCarthy, who came to Stanford after he wrote that, used
219080	224240	the term coin, the term artificial intelligence, now the late John McCarthy, and I confess
224240	229040	to you who knew him, as I did, that I'm a little suspicious of the term because I knew
229040	232840	John, and John liked to be provocative.
232840	238360	And I am thinking to myself, wait a moment, we're still dealing with ones and zeros.
238360	241600	Computers are calculating machines.
241600	245360	artificial intelligence is a marketing term.
245360	249080	So no, it's not really a marketing term.
249080	253880	So I will give you an answer that is more like what John would have given.
253880	260160	And that is, it's the field, the subfield of computer science that attempts to create
260160	269040	machines that can accomplish tasks that seem to require intelligence.
269040	277040	So the early artificial intelligence were systems that played chess or checkers, even,
277040	278840	you know, very, very simple things.
278840	288040	Now John, who, as you know, if you knew him, was ambitious.
288040	294200	And he thought that in a summer conference at Dartmouth, they could solve most of the
294200	295200	problems.
295200	300960	All right, I'm going to come, let me name a couple of very famous events.
300960	303800	What I'm looking for here, I'll name the events.
303800	308600	We have, in 1997, a computer defeats Gary Kasparov at chess.
308600	310600	Big moment for the first time.
310600	315680	Big blue and IBM project defeats a human being at chess, and not just a human being, but
315680	319680	Gary Kasparov, who by some measures is one of the half dozen greatest chess players who
319680	322520	ever lived.
322520	327400	And as best I can tell, computer scientists said, you know, things are getting faster,
327400	328920	but still.
328920	337440	And then we have, in 2015, a computer defeats Go expert Han Fuei, and the following year
337440	343160	it defeats Go grandmaster Lee Seadol, I'm not at all sure I'm pronouncing that correctly,
343160	345240	in a five game match.
345240	350600	And people say, whoa, something just happened this time.
350600	355280	So what I'm looking for here is something, something that a layman like me can latch
355280	357680	on to and say, here's the discontinuity.
357680	359720	Here's where we entered a new moment.
359720	361680	Here's artificial intelligence.
361680	364520	Am I looking for something that doesn't exist?
364520	368000	No, no, I think you're not.
368000	377000	So the difference between deep blue and which played chess, deep blue was written using
377000	379960	traditional programming techniques.
379960	385520	And what deep blue did is it, it would, for each move, for each position on the board,
385520	387960	it would look down to all the possible.
387960	389560	Every conceivable decision tree.
389560	392880	Every decision tree, to a certain depth.
392880	396680	I mean, obviously, you can't go all the way.
396680	401040	And it would, it would have ways of, of way, which ones are best.
401040	406160	And so then it would say, no, this is the best move for me at this time.
406160	412160	That's why, in some sense, it was not theoretically very interesting.
412160	419960	The, the go, AlphaGo, AlphaGo, which was a Google project, was that Google project.
419960	429000	This uses deep learning, it's a neural net, it's not explicit, explicit programming.
429000	435640	We don't know, you know, we don't go into it with an idea of, here's the algorithm
435640	439440	we're going to use, do this, and then do this, and do this.
439440	445520	So it was actually quite a surprise, particularly AlphaGo.
445520	449360	Not to me, but to the public, yes.
449360	450360	To the public.
450360	451360	Yeah.
451360	455160	But our, our colleague, I'm going at this one more time because I really want to understand
455160	456160	this.
456160	457160	I really do.
457160	460800	Our colleague here at Stanford, ZX Shen, who must be known to both of you, physicist
460800	462160	here at Stanford.
462160	466480	And he said to me, Peter, what you need to understand about the moment when a computer
466480	473800	defeated go, go, which is in much more complicated, at least in the decision space, much, much
473800	475800	bigger, so to speak, than chess.
475800	478360	There are more pieces, more square, alright.
478360	483900	And ZX said to me, that whereas chess just did more quickly, what a committee of grand
483900	490200	masters would have decided on, the computer in go was creative.
490200	493680	It was pursuing strategies that human beings had never pursued before.
493680	495280	Is there something to that?
495280	497400	Yeah, so there's a famous, no.
497400	499720	If he's getting impatient with me, I'm asking such, go ahead.
499720	501480	No, no, you're asking such good questions.
501480	506320	So in the third game of the, I think it was the third game of the five games, there was
506320	507320	a move.
507320	508720	I think it was move 32.
508720	509720	32 or 35.
509720	510720	32 or 35.
510720	519520	It's that the computer program made a move that really surprised every single go masters.
519520	523480	Not only Lisa Dole himself, but everybody who's watching.
523480	526040	That's a very, that's a very surprising move.
526040	529040	I thought it was, I thought it was a mistake.
529040	536800	In fact, even post-anonymizing how that move came about, the human masters would say, this
536800	539440	is completely unexpected.
539440	548280	What happens is that the computers, like John says, right, is, has the learning ability,
548280	556200	and has the inference ability to think about patterns or to decide on certain movements,
556200	565800	even outside of the trained, familiar human masters, domain of knowledge in this particular
565800	566800	game.
566800	567800	May I?
567800	568800	Go ahead.
568800	569800	Yes, yes.
569800	570800	Expand on that.
570800	579480	Deep neural nets are supremely good pattern recognition systems.
579480	585760	But the patterns they recognize, the patterns they learn to recognize are not necessarily
585760	589760	exactly the patterns that humans recognize.
589760	597800	So it was seeing something about that position, and it made a move that because of the patterns
597800	606320	that it recognized in the, in the board, that made no sense from a human standpoint.
606320	612880	In fact, all of the, all of the lessons in how to play go tell you never make a move that
612880	616420	close to the edge that, that quickly.
616420	622080	And so everybody thought it made a mistake, and then it proceeded to win.
622080	628240	And I think the way to understand that is it's just seeing patterns that we don't see.
628240	636040	It's computing patterns that, that is not traditionally human, and it has the capacity
636040	637040	to compute.
637040	638040	Okay.
638040	644080	I'm trying to, we're already entering this territory, but I am trying really hard to
644080	651480	tease out the, wait a moment, these are still just machines running zeros and ones, bigger
651480	655240	and bigger memory, faster and faster ability to calculate, but we're still dealing with
655240	656960	machines that run zeros and ones.
656960	658560	That's one strand.
658560	663280	And the other strand is, as you well know, 2001 Space Odyssey, where the computer takes
663280	664760	over the ship.
664760	667880	Open the pod bay doors, Hal.
667880	669560	I'm sorry, Dave.
669560	671880	I'm afraid I can't do that.
671880	672880	Okay.
672880	676040	We'll keep, we'll keep, we'll come to this soon enough.
676040	680800	Fei-Fei Li in your memoir, The Worlds I See, quote, I believe our civilization stands
680800	688200	on the cusp of a technological revolution with the power to reshape life as we know
688200	689360	it.
689360	693560	Revolution, reshape life as we know it.
693560	697840	Now you're a man whose whole academic training is in rigor.
697840	701440	Are you going to let her get away with, with this over, kind of wild overstatement?
701440	704760	No, I don't think it's an overstatement.
705000	706000	Oh.
706000	707000	I think she's right.
707000	710680	He told me to write the book.
710680	717360	Mind you, Peter, it's a technology that is extremely powerful, that will allow us and
717360	726080	is allowing us to get computers to do things we never could have programmed them to do.
726080	728480	And it will change everything.
728480	734280	But it's like, a lot of people have said it's like electricity, it's like the steam
734280	736000	revolution.
736000	739600	It's not something necessarily to be afraid of.
739600	743320	It's not that it's going to suddenly take over the world, that's not what Fei-Fei was
743320	744320	saying.
744320	745320	Well, right.
745320	752680	It's a powerful tool that will revolutionize industries and human the way we live, but
752680	758800	the word revolution is not that it's a conscious being, it's just a powerful tool that changes
758800	759800	things.
759840	766560	And at reassuring, if a few pages later Fei-Fei had not gone on to write, there's no separating
766560	772320	the beauty of science from something like, say, the Manhattan Project, close quote.
772320	778880	Nuclear science, we can produce abundant energy, but it can also produce weapons of indescribable
778880	779880	horror.
779880	785040	AI has boogeymen of its own, whether it's killer robots, widespread surveillance, or
785040	789880	even just automating all 8 billion of us out of our jobs.
789880	794800	Now we could devote an entire program to each of those boogeymen, and maybe at some point
794800	797240	we should.
797240	802520	But now that you have scared me, even in the act of reassuring me, and in fact, it throws
802520	806240	me that you're so eager to reassure me that I think maybe I really should be even more
806240	807600	scared than I am.
807600	808960	Let me just go right down.
808960	810520	Here's the killer robots.
810520	811920	Let me quote the late Henry Kissinger.
811920	817640	I'm just going to put these up and let you, you may calm me down if you can.
817640	821840	Henry Kissinger, if you imagine a war between China and the United States, you have artificial
821840	823880	intelligence weapons.
823880	831120	Nobody has tested these things on a broad scale, and nobody can tell exactly what will happen
831120	834920	when AI fighter planes on both sides interact.
834920	839480	So you are then, I'm quoting Henry Kissinger, who is not a fool after all, so you are then
839480	842560	in a world of potentially total destructiveness.
842560	843560	Close quote.
843560	844560	Fei-Fei.
844560	850080	So like I said, I'm now denying how powerful these tools are.
850080	857200	I mean, humanity, before AI has already created tools and technology that are very destructive,
857200	858520	could be very destructive.
858520	861360	We talk about Manhattan Project, right?
861360	868560	But that doesn't mean that we should collectively decide to use this tool in this destructive
868560	869640	way.
869640	876240	Okay, Peter, you know, think back before you even had heard about artificial intelligence.
876240	877840	Which actually, what is it five years ago?
877840	878840	No, I know.
878840	880240	This is all happening so fast.
880240	883640	Just five years ago, or 10 years ago.
883640	893320	Remember the tragic incident where an Iranian passenger plane was shot down flying over the
893320	896240	Persian Gulf by an Aegis system?
896240	897400	Yes, yes.
897400	905760	And one of our ships, an automated system, because it had to be automated in order to
905760	906760	be...
906760	907760	Humans can't react that fast.
907760	908760	Yeah, exactly.
908760	914600	And in this case, for reasons that I think are quite understandable now that you understand
914600	919960	the incident, but it did something that was horrible.
919960	924560	That's not different in kind from what you can do with AI, right?
924560	936800	So we as creators of these devices, or as users of AI, have to be vigilant about what
936800	939440	kind of use we put them to.
939440	945800	And when we decide to put them to one particular use, and there may be uses, the military has
945800	953880	many good uses for them, we have to be vigilant about their doing what we intend them to do
953880	956840	rather than doing things that we don't intend them to do.
956840	959200	So you're announcing a great theme.
959200	967280	And that theme is that what Dr. Fei-Fei Li has invented makes the discipline to which
967280	972640	you have dedicated your life, philosophy, even more important, not less so.
972640	973640	Yeah, that's why we're the co-directors.
973640	977080	The power of this instrument makes the human being more important, not less so.
977080	978080	Am I making...
978080	979080	Am I being glib?
979080	980080	Or is that onto...
980080	984840	So let me tell you a story about...
984840	989120	So Fei-Fei used to live next door to me, or close to next door to me.
989120	990120	And I was talking...
990120	994440	I'm not sure whether that would make me feel more safe or more exposed, no.
994440	998560	And I was talking to her, I was still a pro at this time.
998560	1005640	And she said to me, you and John Hennessey started a lot of institutes that brought technology
1005640	1009320	into other parts of the university.
1009320	1016320	We need to start an institute that brings philosophy and ethics and the social sciences
1016320	1026280	into AI, because AI is too dangerous to leave it to the computer scientists alone.
1026280	1027280	Nothing wrong with that.
1027280	1030680	There are many stories about how hard it was to persuade him when he was provost, and you
1030680	1031680	succeeded.
1031680	1032680	Can I...
1032680	1037800	Just one more boogeyman briefly, and we'll return to that theme that you just gave us
1037880	1041160	there, and then we'll get back to the Stanford Institute.
1041160	1042360	I'm quoting you again.
1042360	1044240	This is from your memoir.
1044240	1048560	The prospect of just automating all billion of us out of our jobs.
1048560	1049560	That's the phrase you used.
1049560	1056520	Well, it turns out that it took me mere seconds, using my AI-enabled search algorithm, search
1056520	1062480	device, to find a Goldman Sachs study from last year, predicting that in the United States
1062480	1069480	and Europe, some two-thirds of all jobs could be automated, at least to some degree.
1069480	1075880	So, why shouldn't we all be terrified, Henry Kissinger of World Apocalypse, all right, maybe
1075880	1079360	that's a bit too much, but my job.
1079360	1083160	So I think job change is real.
1083160	1089360	Job change is real with every single technological advances that humanity, human civilization
1089360	1091640	has faced.
1091640	1095240	That is real, and that's not to be taken lightly.
1095240	1098440	We also have to be careful with the word job.
1098440	1106440	Job tends to describe a holistic profession or that a person attaches his or her income
1106440	1109440	as well as identity.
1109440	1115480	But there is also, within every job, pretty much within every job, there are so many tasks.
1115480	1121960	It's hard to imagine there's one job that has only one singular task, right, like being
1121960	1126600	a professor, being a scholar, being a doctor, being a cook.
1126600	1129360	All of these jobs have multiple tasks.
1129360	1136000	What we're seeing is technology is changing how some of these tasks can be done.
1136000	1143120	And it's true, as it changes these tasks, some of them, some part of them could be automated.
1143120	1147960	It's starting to change how the jobs are, and eventually it's going to impact jobs.
1147960	1152840	So this is going to be a gradual process, and it's very important we stay on top of
1152840	1153840	this.
1153840	1159400	This is why Human Center AI Institute was founded, is these questions are profound.
1159400	1162680	They're by definition multidisciplinary.
1162680	1168840	Computer scientists alone cannot do all the economic analysis, but economists now understanding
1168840	1177120	what these computer science programs do will not, by themselves, understand the shift of
1177120	1178120	the jobs.
1178120	1179120	John, may I tell you?
1179120	1180120	Go ahead.
1180120	1182960	But let me just point something out.
1182960	1190360	The Goldman Sachs study said that such and such percentage of jobs will be automated
1190360	1193320	or can be automated at least in part.
1193320	1194320	Yes.
1194320	1198840	What they're saying is that a certain number of the tasks that go into a particular job
1198840	1199840	are done.
1199840	1200840	Exactly.
1200840	1208400	So Peter, you said it only took me a few seconds to go to the computer and find that
1208400	1210880	article.
1210880	1212880	Guess what?
1212880	1217120	That's one of the tasks that would have taken you a lot of time.
1217120	1221840	So part of your job has been automated.
1221840	1222840	Okay.
1222840	1224280	Now let me tell you a story.
1224280	1225520	But also empowered.
1225520	1226520	Empowered.
1226520	1227520	Empowered.
1227520	1228520	Exactly.
1228520	1229520	Fine.
1229520	1230520	Thank you.
1230520	1231520	Thank you.
1231520	1232520	Thank you.
1232520	1233520	You're making me feel good.
1233520	1234520	Now let me tell you a story.
1234520	1235520	All three of us live in California, which means all three of us probably have some friends
1235520	1236520	down in Hollywood.
1236520	1240120	And I have a friend who was involved in the writer's strike.
1240120	1241120	Yeah.
1241120	1242120	Okay.
1242120	1244160	And here's the problem.
1244160	1248960	To run a sitcom, you used to run a writer's room.
1248960	1254200	And the writer's room would employ seven, a dozen on The Simpsons Show, The Cartoon
1254200	1255200	Show.
1255200	1257320	They'd had a couple of writer's rooms running.
1257320	1258800	They were employing 20.
1258800	1264080	And these were the last kind of person you'd imagine a computer could replace because they
1264080	1268880	were well-educated and witty and quick with words.
1268880	1273120	And you think of computers as just running calculations, maybe spreadsheets, maybe someday
1273120	1278000	they can eliminate accountants, but writers, Hollywood writers.
1278000	1285480	But it turns out, and my friend illustrated this for me by saying, doing the artificial
1285480	1293360	intelligence thing, we're at a prompt, draft a skit for Saturday Night Live in which Joe
1293360	1300920	Biden and Donald Trump are playing beer pong, 15 seconds.
1300920	1304680	Now professionals could have tightened it up or made it, but it was pretty funny and
1304680	1306920	it was instantaneous.
1306920	1308360	And you know what that means?
1308360	1311980	That means you don't need four or five of the seven writers.
1311980	1318080	You need a senior writer to assign intelligence, the artificial, and you need maybe one other
1318080	1322040	writer or two other writers to tighten it up or redraft it.
1322040	1323920	It is upon us.
1323920	1328120	And your artificial intelligence is going to get bad press when it starts eliminating
1328120	1330520	the jobs of the chattering classes.
1330520	1332920	And that has already begun.
1332920	1333920	Tell me I'm wrong.
1333920	1343200	Do you know, before the agricultural revolution, something like 80, 90% of all the people in
1343200	1348280	the United States were employed on farms.
1348280	1357240	We now, now it's down to 2% or 3%, and those same farms, that same land, is far, far more
1357240	1358240	productive.
1358960	1367800	Now, would you say that your life or anybody's life now was worse off than it was in the
1367800	1372480	1890s when everybody was working on the farm?
1372480	1373480	No.
1373480	1376160	So yes, you're right.
1376160	1378320	It will change jobs.
1378320	1380600	It will make some jobs easier.
1380600	1385600	It will make, allow us to do things that we could not do before.
1385600	1396920	And yes, it will allow fewer people to do more of what they were doing before, and consequently
1396920	1400440	there will be fewer people in that line of work.
1400440	1401440	That's true.
1401440	1402440	That is true.
1402440	1404240	I also want to just point out two things.
1404240	1409240	One is that jobs is always changing, and that change is always painful.
1409240	1414720	And we're, as compared scientists, as philosophers, also as citizens of the world, we should be
1414720	1421080	empathetic of that, and nobody is saying we should just ignore that change in pain.
1421080	1426200	So this is why we're studying this, we're trying to talk to policymakers, we're educating
1426200	1427200	the population.
1427200	1433360	In the meantime, I think we should give more credit to human creativity in the face of
1433360	1434520	AI.
1434520	1439640	I start to use this example that's not even AI.
1439640	1448320	Think about the advanced, speaking of Hollywood, graphics, technology, CGI, and all that, right?
1448320	1449320	The video gaming industry?
1449320	1452840	No, no, just animations and all that, right?
1452840	1460400	One of many of our, including our children's, favorite animation series is by Ghibli Studio.
1460400	1467680	You know, Princess Nomononaki, my neighbor Totoro, Spirited Away.
1467720	1475000	All of these were made during a period where computer graphics technology is far more advanced
1475000	1478280	than these hand-drawn animations.
1478280	1486000	Yet the beauty, the creativity, the emotion, the uniqueness in this film continue to inspire
1486000	1489200	and just entertain humanity.
1489200	1496520	So I think we need to still have that pride and also give the credit to humans.
1496520	1501640	Let's not forget our creativity and emotion and intelligence is unique.
1501640	1504640	It's not going to be taken away by technology.
1504640	1505640	Thank you.
1505640	1507400	I feel slightly reassured.
1507400	1510640	I'm still nervous about my job, but I feel slightly reassured.
1510640	1517120	But you mentioned government a moment ago, which leads us to how we should regulate AI.
1517120	1519120	Let me give you two quotations.
1519120	1520120	I'll begin.
1520120	1523880	I'm coming to the quotation from the two of you, but I'm going to start with a recent
1523880	1528440	article in the Wall Street Journal by Senator Ted Cruz of Texas and former Senator Phil
1528440	1530280	Graham also of Texas.
1530280	1536680	Quote, the Clinton administration took a hands-off approach to regulating the early internet.
1536680	1541680	In so doing, it unleashed extraordinary economic growth and prosperity.
1541680	1547720	The Biden administration, by contrast, is impeding innovation in artificial intelligence with
1547720	1549800	aggressive regulation.
1549800	1550800	Close quote.
1550800	1552240	That's them.
1552240	1553240	This is you.
1553600	1558120	Also a recent article in the Wall Street Journal, John Etchamendi and Fei-Fei Li.
1558120	1564360	Quote, President Biden has signed an executive order on artificial intelligence that demonstrates
1564360	1569880	his administration's commitment to harness and govern the technology.
1569880	1574280	President Biden has set the stage and now it is time for Congress to act.
1574280	1576600	Cruz and Graham, less regulation.
1576600	1580120	Etchamendi and Lee, Biden administration has done well.
1580120	1582480	Now Congress needs to give us even more.
1582480	1583480	No.
1583480	1584480	All right, John.
1584480	1587040	No, I don't agree with that.
1587040	1593960	I believe regulating any kind of technology is very difficult and you have to be careful
1593960	1601120	not to regulate too soon or not to regulate too late.
1601120	1602800	Let me give you another example.
1602800	1606000	You talked about the internet and it's true.
1606000	1609280	The government really was quite hands-off and that's good.
1609280	1610280	That's good.
1610280	1611280	It worked out.
1611280	1612280	It worked out.
1612280	1615880	Let's also think about social media.
1615880	1624360	Social media has not worked exactly, worked out exactly the way we want it.
1624360	1632200	We originally believed that we were going to enter a golden age in which friendship, comedy,
1632200	1639520	well and everybody would have a voice and we could all live together at Kumbaya and
1639520	1640520	so forth.
1640640	1642640	What happened?
1642640	1646920	Jonathan Haidt has a new book out on the particular pathologies among young people
1646920	1649200	from all of these social media.
1649200	1650200	Not an argument.
1650200	1654680	It's an argument but it's based on lots of data.
1654680	1667120	It seems to me that I'm in favor of very light-handed and informed regulation to try to put up sort
1667120	1668120	of bumpers.
1668720	1670680	I don't know what the analogy is.
1670680	1671680	Guardrails.
1671680	1673760	Guardrails for the technology.
1673760	1681040	I am not for heavy-handed top-down regulation that stifles innovation.
1681040	1682040	Here's another.
1682040	1684520	Let me get on to this.
1684520	1687760	I'm sure you'll be able to adapt your answer to this question too.
1687760	1690200	I'm continuing your Wall Street Journal piece.
1690200	1693880	Big tech companies can't be left to govern themselves.
1693880	1697000	Around here, Silicon Valley, those are fighting words.
1697000	1701680	Academic institutions should play a leading role in providing trustworthy assessments
1701680	1704440	and benchmarking of these advanced technologies.
1704440	1709680	We encourage an investment in human capital to bring more talent to the field of AI with
1709680	1711160	academia and the government.
1711160	1712160	Close quote.
1712160	1713160	Okay.
1713160	1715280	Now, it is mandatory for me to say this.
1715280	1721880	So please forgive me, my fellow Stanford employees, apart from anything else.
1721880	1724720	Why should academic institutions be trusted?
1724720	1728040	Both the country has lost faith in academic institutions.
1728040	1732800	DEI, the whole woke agenda, anti-Semitism on campus.
1732800	1737200	We've got a recent Gallup poll showing the proportion of Americans who expressed a great
1737200	1740880	deal or quite a lot of confidence in higher education.
1740880	1748600	This year came in at just 36 percent and that is down in the last eight years from 57 percent.
1748600	1753120	You are asking us to trust you at the very moment when we believe we have good reason
1753120	1754840	to knock it off.
1754840	1755840	Trust you?
1755840	1756840	Okay.
1756840	1759440	So I'll start with this first half of the answer.
1759440	1761120	I'm sure John has a lot to say.
1761120	1768320	I do want to make sure, especially wearing the hats of co-directors of HAI, when we talk
1768320	1773720	about the relationship between government and technology, we tend to use the word regulation.
1773720	1776720	I really, really want to double click.
1776720	1778960	I want to use the word policy.
1779120	1783880	Policy and regulation are related but not the same.
1783880	1789800	When John and I wrote that Wall Street Journal Opinion piece, we really are focusing on a
1789800	1798160	piece of policy that is to resource public sector AI, to resource academia because we
1798160	1805600	believe that AI is such a powerful technology and science and academia and public sector
1805600	1810320	still has a role to play to create public good.
1810320	1815680	And public goods are curiosity-driven knowledge exploration.
1815680	1825640	Our cures for cancers are the maps of biodiversity of our globe, our discovery of nanomaterials
1825640	1832560	that we haven't seen before, our different ways of expressing in theater, in writing,
1832560	1833560	in music.
1833720	1841000	These are public goods and when we are collaborating with the government on policy, we're focusing
1841000	1842000	on that.
1842000	1844360	So I really want to make sure.
1844360	1849280	Regulation we all have personal opinion, but there's more than regulation in policy.
1849280	1854920	John, let me make one last run at you.
1854920	1860800	In my theory here, although I'm asking questions that I'm quite sure you'd like to take me
1860800	1864920	out and swap me around at this point, John, but this is serious.
1864920	1869720	You've got the Stanford Institute for Human-Centered Artificial Intelligence and that's because
1869720	1872520	you really think this is important.
1872520	1876880	But we live in a democracy and you're going to have to convince a whole lot of people.
1876880	1880280	So let me take one more run at you and then hand it back to you, John.
1880280	1882440	Your article in the Wall Street Journal, again, let me repeat this.
1882440	1887000	We encourage an investment in human capital to bring more talent to the field of AI with
1887000	1888920	academia and the government, close quote.
1888920	1890080	That means money.
1890080	1893520	An investment means money and it means taxpayers' money.
1893520	1896640	Here's what Cruz and Graham say in the Wall Street Journal, the Biden regulatory policy
1896640	1902040	on AI has everything to do with special interest rent seeking.
1902040	1905720	Stand for faculty, make well above the national average income.
1905720	1911000	We are sitting at a university with an endowment of tens of billions of dollars.
1911000	1918480	John, why is not your article in the Wall Street Journal the very kind of rent seeking
1918800	1921880	that Senator Cruz and Senator Graham are saying?
1921880	1923040	Are you kidding?
1923040	1928080	Peter, let's take another example.
1928080	1935240	So one of the greatest policy decisions that this country has ever made was when Vannevar
1935240	1942240	Bush, advisor to at the time President Truman, convinced the state on through Eisenhower,
1943120	1944120	as I recall, so it's important.
1944120	1945120	Yeah, I'm kidding.
1945120	1946120	He's bipartisan.
1946120	1947120	Exactly.
1947120	1948120	Exactly.
1948120	1956760	It's not a bipartisan issue at all, but convinced Truman to set up the NSF for funding...
1956760	1957760	National Science Foundation.
1957760	1967160	...for funding curiosity-based research, advanced research at the universities, and then not
1967160	1972760	to cut, not to, you know, say that companies don't have any role, not to say that government
1972760	1973840	has no role.
1973840	1982360	They both have roles, but they're different roles, and companies tend to be better at
1982360	1988760	development, better at producing products and tapping into things that can, within
1988760	1995360	a year or two or three, can be a product that will be useful.
1995360	1998440	Scientists at universities don't have that constraint.
1998440	2001440	They don't have to worry about when is this going to be...
2001440	2002440	Commercial.
2002440	2003440	Commercial.
2004040	2015440	And that has, I think, had such an incalculable effect on the prosperity of this country,
2015440	2020600	on the fact that we are the leader in every technology field.
2020600	2024760	It's not an accident that we're the leader in every technology field.
2024760	2025840	We didn't used to be.
2025840	2031480	And does it affect your argument if I add it also enabled us or contributed to a victory
2031480	2032480	in the Cold War?
2033480	2036480	The weapons systems that came out of universities?
2036480	2037480	All right.
2037480	2038480	Well, no, absolutely.
2038480	2042480	And, you know, President Reagan and Star Wars.
2042480	2046240	In other words, it ended up being a defensive, kind of good, you could argue from all kinds
2046240	2049720	of points of view as it was a good ROI for taxpayers' money.
2049720	2050720	Yeah.
2050720	2051720	All right.
2051720	2056680	So we're not arguing for higher salaries for faculty or anything of that sort.
2056680	2066160	But we think, particularly in AI, it's gotten to the point where scientists at universities
2066160	2073920	can no longer play in the game because of the cost of the computing, the cost, the inaccessibility
2073920	2075480	of the data.
2075480	2078600	That's why you see all of these developments coming out of companies.
2078600	2079600	That's great.
2079600	2081440	Those are great developments.
2081440	2090240	But we need to have also people who are exploring these technologies without looking at the
2090240	2094480	product, without being driven by the profit motive.
2094480	2099360	And then eventually, hopefully, they will develop discoveries, they will make discoveries,
2099360	2101360	and we'll then be commercializable.
2101360	2102360	Okay.
2102360	2106240	I noticed in your book, Fei-Fei, I was very struck that you said, oh, I think it was about
2106240	2112600	a decade ago, 2015, I think, was that you noticed that you were beginning to lose colleagues
2112600	2118440	to the private sector, presumably because they just pay so phenomenally well around
2118440	2119440	here in Silicon Valley.
2119440	2125280	But then there's also the point that to get to make progress in AI, you need an enormous
2125280	2127600	amount of computational power.
2127600	2131840	And assembling all those ones and zeros is extremely expensive.
2131840	2135600	So ChatGPT, what is the parent company?
2136320	2137320	Open AI.
2137320	2142400	Open AI got started with an initial investment of a billion dollars.
2142400	2147360	And friends and family capital of a billion dollars is a lot of money even around here.
2147360	2148360	Okay.
2148360	2149360	That's the point you're making.
2149360	2150360	Yes.
2150360	2152860	All right.
2152860	2156960	It feels to me as though every one of these topics is worth a day long seminar.
2156960	2159320	Actually, I think that they are.
2159320	2167040	And by the way, this has happened before where the science has become so expensive that it
2167040	2172640	could no longer, that university level research and researchers could no longer afford to
2172640	2174460	do the science.
2174460	2177800	It happened in high-energy physics.
2177800	2184240	High-energy physics used to mean you had a Vandegraaff generator in your office, and
2184240	2189120	that was your accelerator, or you could do what you needed to do.
2189720	2196480	And then it no longer was, you know, the energy levels were higher and higher.
2196480	2197480	And what happened?
2197480	2202000	Well, the federal government stepped in and said, we're going to help.
2202000	2205120	We're going to build an accelerator.
2205120	2206120	Stanford linear accelerator.
2206120	2207120	Stanford linear accelerator.
2207120	2208120	Exactly.
2208120	2211640	Sandia Labs, Lawrence Livermore, all these are at least in part federal established.
2211640	2212640	CERN.
2212640	2213840	CERN, which is European.
2213840	2214840	Right.
2214840	2215840	Well, Fermilab.
2215840	2223240	The first accelerator was Slack, Stanford linear accelerator center, then Fermilab, and so
2223240	2225240	on and so forth.
2225240	2231760	CERN is actually late in the game, and it's European consortium.
2231760	2241800	But the thing is, we could not continue the science without the help of the government
2241800	2242800	and government.
2242920	2250160	There is another, and then in addition to high energy physics and then bio, right, especially
2250160	2256080	with genetic sequencing and high throughput genomics, and biotech is also changing.
2256080	2264840	And now you see a new wave of biology labs that are actually heavily funded by the combination
2264840	2272480	of government and philanthropy and all that, and that stepped in to, you know, supplement
2272480	2275160	what the traditional university model is.
2275160	2278360	And so we're now here with AI and computer science.
2278360	2279360	Okay.
2279360	2285480	This is, we have to do another show on that one alone, I think.
2285480	2286480	The Singularity.
2286480	2288480	Oh, good.
2288480	2289480	This is good.
2289480	2290480	Reassuring.
2290480	2291480	You're both, I mean, rolling your eyes.
2291480	2292480	Wonderful.
2292480	2294480	I feel better about this already.
2294480	2295480	Good.
2295480	2296480	Ray Kurzweil.
2296480	2297480	You know exactly where this is going.
2297480	2299120	Ray Kurzweil writes a book in 2005.
2299120	2303400	This gets everybody's attention and still scares lots of people to death, including
2303400	2304400	me.
2304400	2310240	The book is called The Singularity is Near, and Kurzweil predicts a singularity that
2310240	2317480	will involve, and I'm quoting him, the merger of human technology with human intelligence.
2317480	2321040	He's not saying the tech will mimic more and more closely human intelligence.
2321040	2323200	He is saying they will merge.
2323200	2327600	I set the date for the singularity representing a profound and disruptive transformation in
2327600	2330880	human capability as 2045.
2330880	2332880	Okay.
2332880	2333880	That's the first quotation.
2333880	2334880	Here's the second.
2334880	2339280	And this comes from the Stanford Course Catalog's description of the philosophy of artificial
2339280	2346640	intelligence, a freshman seminar that was taught last quarter, as I recall, by one John
2346640	2347640	Echamendi.
2347640	2354120	Here, here's from the description, is it really possible for an artificial system to
2354120	2358880	achieve genuine intelligence, thoughts, consciousness, emotions?
2358880	2360880	What would that mean?
2360880	2363120	John, is it possible?
2363120	2366840	What would it mean?
2366840	2370240	I think the answer is actually no.
2370240	2373840	And thank goodness, you kept me waiting for a moment.
2373840	2385800	I think the fantasies that Ray Kurzweil and others have been spinning up, I guess that's
2385800	2396720	the way to put it, stem from a lack of understanding of how the human being really works and don't
2396720	2406600	understand how crucial biology is to the way we work, the way we are motivated, how we
2406600	2414240	get desires, how we get goals, how we become humans, become people.
2414240	2423360	And what AI has done so far, AI is capturing what you might think of as the information
2423360	2427800	processing piece of what we do.
2427800	2430800	So part of what we do is information processing.
2430800	2435120	So it's got the right frontal cortex, but it hasn't got the left frontal cortex yet?
2435120	2437480	Yeah, it's an oversimplification, but yes.
2437480	2440000	Imagine that on television.
2440000	2452400	So I actually think it is, first of all, the date, 2045, is insane.
2453400	2456840	And secondly, it's not even clear to me that we will ever go back.
2456840	2459080	Wait, I can't believe I'm saying this.
2459080	2466320	In his defense, I don't think he's saying that 2045 is the day that the machines become
2466320	2470000	conscious beings like humans.
2470000	2478960	It's more an inflection point of the power of the technology that is disrupting the society.
2478960	2479960	Well, that's in his late.
2479960	2480960	He's late.
2480960	2481960	We're already there.
2481960	2483240	That's what I'm saying.
2483240	2490120	I think you're being overly generous.
2490120	2495200	But he means by the singularity is the date at which we create an artificial intelligence
2495200	2503480	system that can improve itself and then get into a cycle, a recursive cycle, where it
2503480	2506880	becomes a superintelligence.
2506880	2508640	And I deny that.
2508640	2511940	He's playing the 2001 Space Odyssey game here.
2511940	2514940	And it's a different question, but related question.
2514940	2520820	In some ways, this is a more serious question, I think, although that's serious too.
2520820	2529020	Here's the late Henry Kissinger again, quote, we live in a world which has no philosophy.
2529020	2535980	There is no dominant philosophical view, so the technologists can run wild.
2535980	2540660	They can develop world-changing things, and there's nobody to say, we've got to integrate
2540740	2542780	this into something.
2542780	2546460	All right, I'm going to put it crudely again.
2546460	2552700	But in China, a century ago, we still had Confucian thought, dominant at least among
2552700	2557700	the educated classes on my very thin understanding of Chinese history.
2557700	2563660	In this country, until the day before yesterday, we still spoke without irony of the Judeo-Christian
2563660	2572140	tradition, which involved certain concepts about morality, what it meant to be human.
2572140	2576700	It assumed a belief in God, but it turned out you could actually get pretty far along,
2576700	2579260	even if you didn't believe in OK.
2579260	2582740	And Kissinger is now saying, it's all fallen apart.
2582740	2585760	There is no dominant philosophy.
2585760	2588020	This is a serious problem, is it not?
2588020	2591540	There's nothing to integrate AI into.
2591540	2594980	You take his point.
2594980	2595980	It's up to the children.
2595980	2598980	You're the philosopher.
2598980	2603020	You're the philosopher.
2603020	2606580	I think this is a great, first of all, thank you for that quote.
2606580	2610260	I didn't read that quote from Henry Kissinger.
2610260	2614100	I mean, this is why we founded the Human Center AI Institute.
2614100	2619500	These are the fundamental questions that our generation needs to figure out.
2619860	2620860	That's not just a question.
2620860	2621860	That's the question.
2621860	2623780	It was one of the fundamental questions.
2623780	2628780	It's also one of the fundamental questions that illustrates why universities are still
2628780	2632180	relevant today.
2632180	2638140	And Peter, one of the things that Henry Kissinger says in that quote is that there is no dominant
2638140	2639140	philosophy.
2639140	2645780	There's no one dominant philosophy like the Judeo-Christian tradition, which used to be
2645780	2648060	the dominant tradition in the US.
2648100	2651300	It's a different conversation in Paris in the 12th century, for example, the University
2651300	2652300	of Paris.
2652300	2660460	In order to take values into account when you're creating an AI system, you don't need
2660460	2665340	a dominant tradition.
2665340	2671060	What you need, for example, for most ethical traditions is the Golden Rule.
2671060	2673180	Go back to the Confucius.
2673180	2675980	We can still get along with each other.
2675980	2680100	Even when it comes to deep, deep questions of value such as this, we still have enough
2680100	2681820	common ground.
2681820	2684820	I believe so.
2684820	2686980	I heave yet another sigh of relief.
2686980	2689340	Okay, let's talk a little bit.
2689340	2693340	We're talking a little bit about a lot of things here, but so it is.
2693340	2697260	Let us speak of many things as it is written in Alice in Wonderland.
2697260	2699580	The Stanford Institute.
2699580	2703940	The Stanford Institute for Human-Centered Artificial Intelligence, of which you are
2703980	2708940	co-directors, and I just have two questions and respond as you'd like.
2708940	2716220	Can you give me some taste, some feel for what you're doing now, and in some ways more
2716220	2720780	important, but more elusive, where you'd like to be in just five years, say.
2720780	2721780	Everything in this field is moving.
2721780	2725220	So I would, my impulse is to say 10 years because it's a rounder number.
2725220	2727420	It's too far off in this field.
2727420	2728420	Fei-Fei.
2728420	2733300	I think what really has happened in the past five years by Stanford High, among many
2733380	2734380	things.
2734380	2736460	I just want to make sure everybody following you.
2736460	2739540	H-A-I, Stanford High is the way it's known on this campus.
2739540	2740540	Yes.
2740540	2741540	Go ahead.
2741540	2742540	Yeah.
2742540	2748420	Is that we have put a stick on the ground for Stanford as well as for everybody that
2748420	2756420	this is an interdisciplinary study that AI, artificial intelligence, is a science of
2756420	2757500	its own.
2757500	2764940	It's a powerful tool, and what happens is that you can welcome so many disciplines to
2764940	2771900	cross-pollinate around the topic of AI or use the tools of AI to make other sciences
2771900	2775300	happen or to explore other new ideas.
2775300	2783260	And that concept of making this an interdisciplinary and multidisciplinary field is what I think
2783260	2788140	Stanford High brought to Stanford and also hopefully to the world.
2788140	2793540	Because like you said, computer science is kind of a new field, you know, only, you know,
2793540	2799620	the late John McCarthy coined the term, you know, in the late fifties.
2799620	2801260	Now it's moving so fast.
2801260	2807380	Everybody feels it's just a niche computer science field that's just like making its
2807380	2808820	way into the future.
2808820	2812340	And we're saying, no, look abroad.
2812340	2814980	There's so many disciplines that can be put here.
2814980	2817900	Who competes with the Stanford Institute and Human-Centered Design?
2817900	2820380	Is there such an institute at Harvard or Oxford or Beijing?
2820380	2821980	I just don't know what the...
2821980	2822980	Oh, thank you.
2822980	2828660	So in the five years since we launched, there have been a number of similar institutes that
2828660	2831780	have been created at other universities.
2831780	2834260	We don't see that as competition in any way.
2834260	2837060	If these arguments you've been making are valid, then we need them.
2837060	2838060	We should welcome them.
2838300	2839300	As a movement.
2839300	2840300	We need them.
2840300	2843820	And part of what we want to do and part of what I think we've succeeded to a certain
2843820	2853500	extent doing is communicating this vision of the importance of keeping the human and
2853500	2861780	human values at the center when we are developing this technology, when we are applying this
2861780	2864420	technology.
2864420	2867620	And we want to communicate that to the world.
2867620	2873060	We want other centers that adopt a similar standpoint.
2873060	2879380	And importantly, one of the things that I didn't mention is one of the things we try
2879380	2888500	to do is educate and educate, for example, legislators so that they understand what this
2888500	2892780	technology is, what it can do, what it can't do.
2892780	2897380	So you're traveling to Washington or the very generous trustees of this institution
2897380	2902260	are bringing congressional staff and they're both, both are happening.
2902260	2908420	So are you, first of all, did you teach that course in Stanford HAI or was the course located
2908420	2910260	in the philosophy department or cross-list?
2910260	2913220	I'm just trying to get a feel for what's actually taking place there now.
2913220	2914220	Yeah.
2914220	2918620	I actually taught it in the confines of the HAI building.
2918620	2919620	Okay.
2919620	2920620	So it's an HAI.
2920620	2921620	No, it's a philosophy.
2921620	2925540	It's listed as a philosophy course, but taught in the HAI.
2925540	2926540	He's the former provost.
2926540	2929700	He gets to, he's an interdisciplinary walking wonder.
2929700	2937980	And your work in AI assisted healthcare, is that taking place in HAI or is it at the medical
2937980	2938980	school?
2938980	2939980	Well, that's the beauty.
2939980	2945300	It's taking place in HAI, computer science department, the medical school, even has collaborators
2945300	2949540	from the law school, from the political science department.
2949540	2950540	So that's the beauty.
2950620	2953220	It's deeply interdisciplinary.
2953220	2956780	If I were the provost, I'd say this is starting to sound like something that's about to run
2956780	2957780	a muck.
2957780	2960220	Doesn't that sound a little too interdisciplinary, John?
2960220	2963180	Don't we need to define things a little bit here?
2963180	2966180	Let me, let me tell you, let me say something.
2966180	2972580	So Steve Denning, who was the chair of our board of trustees for many years and has been
2972580	2978740	a long, long time supporter of the university in many, many ways.
2978740	2986260	In fact, we are the Denning co-directors of Stanford HAI, Stanford HAI.
2986260	2995540	Steve saw five, six years ago, he said, you know, AI is going to impact in a free department
2995540	2998620	at this university.
2998620	3005500	And we need to have an institute that makes sure that that happens the right way, that
3005620	3010620	that impact is, is, does not run a muck.
3010620	3014180	Where would you like to be in five years?
3014180	3017060	What's a, what's a course you'd like to be teaching in five years?
3017060	3019340	What's a, what's a special project?
3019340	3025140	I would like to teach a course, freshman seminar called The Greatest Discoveries by AI.
3025140	3027340	Oh, really?
3027340	3030340	Okay.
3030340	3035460	A last question, which I have one last question, but that does not.
3035460	3038580	That means that it has, you, each of you has to hold yourself to one last answer because
3038580	3041860	it's a kind of open-ended question.
3041860	3046740	I have a theory, but all I do is wander around this campus.
3046740	3050580	The two of you are deeply embedded here and you ran the place for 17 years, so you'll
3050580	3052380	know more than I will.
3052380	3056100	Including you may know that my theory is wrong, but I'm going to trot it out, modest
3056100	3060460	though it may be, even so.
3060460	3065140	Milton Friedman, the late Milton Friedman, who when I first arrived here was a colleague
3065140	3066140	at the Hoover Institution.
3066140	3070700	In fact, by some miracle, his office was on the same hallway as mine and I used to stop
3070700	3073580	in on him from time to time.
3073580	3080340	He told me that he went into economics because he grew up during the Depression and the overriding
3080340	3087580	question in the country at that time was how do we satisfy our material needs?
3087580	3089540	There were millions of people without jobs.
3089540	3093420	There really were people who had trouble feeding their families.
3093420	3095140	All right.
3095140	3099860	I think of my own generation, which is more or less John's generation.
3099860	3101820	You come much later, Faye Faye.
3101820	3102820	Thank you.
3102820	3108180	And for us, I don't know what kind of discussions you had in the dorm room, but when I was in
3108180	3112620	college, there were both sessions about the Cold War, where the Russians...
3112620	3115940	The Cold War was real to our generation.
3115940	3119540	That was the overriding question.
3119540	3121180	How can we defend our way of life?
3121180	3124180	How can we defend our fundamental principles?
3124180	3125700	All right.
3125700	3127980	Here's my theory.
3127980	3136620	For current students, they've grown up in a period of unimaginable prosperity.
3136620	3139860	Material needs are just not the problem.
3139860	3144540	They have also grown up during a period of relative peace.
3144540	3146020	The Cold War ended.
3146020	3147300	You could put different...
3147300	3151260	The Soviet Union declared itself defunct in 1991.
3151260	3155300	Cold War is over at that moment of the latest.
3155300	3161060	The overriding question for these kids today is meaning.
3161060	3163860	What is it all for?
3163860	3165700	Why are we here?
3165700	3168140	What does it mean to be human?
3168140	3173140	What's the difference between us and the machines?
3173380	3180940	If my little theory is correct, then by some miracle, this technological marvel that you
3180940	3186580	have produced will lead to a new flowering of the humanities.
3186580	3192460	Do you go for that, John?
3192460	3193460	Do I go for it?
3193460	3197060	I would go for it if it were going to happen.
3197060	3199180	Did I put that in a slightly sloppy way?
3199180	3201500	No.
3201500	3203260	I think it would be wonderful.
3203260	3207980	It's something to hope for.
3207980	3211060	Now I'm going to be the cynic.
3211060	3217100	So far, what I see in students is more and more focus, or Stanford students, more and
3217100	3221380	more focus on technology, on learning.
3221380	3227020	Computer science is still the biggest major at this university.
3227020	3228980	We have tried at HAI.
3228980	3236180	We have actually started a program called Embedded Ethics, where the CS at the end of
3236180	3242540	ethics is capitalized, so it's computer science.
3242540	3244060	That'll catch the kids' attention.
3244060	3247500	No, we don't have to catch their attention.
3247500	3255620	What we do is virtually all of the courses in computer science, the introductory courses,
3255620	3259020	have ethics components built in.
3259020	3264900	So a problem set, so you have a problem set this week, and that'll have a whole bunch
3264900	3272460	of very difficult math problems, computer science problem, and then it will have a very
3272460	3274460	difficult ethical challenge.
3274460	3277980	It'll say, here's the situation.
3277980	3286400	You are programming a computer, a programming an AI system, and here's the dilemma.
3286400	3287900	Now discuss.
3287900	3289500	What are you going to do?
3289500	3294140	So we're trying to bring, and this is what they wanted.
3294140	3300820	We're trying to bring ethics within the last couple of years, okay, two, three years.
3300820	3308620	We're trying to bring the attention to ethics into the computer science curriculum.
3308620	3314680	And partly that's because they're not, I mean, students tend to follow the path of least
3314680	3315680	resistance.
3315680	3319380	Well, they also, let's put it, again, if I'm saying things crudely again and again,
3319380	3322580	but someone must say it, they follow the money.
3322580	3329420	So as long as this valley that surrounds us rewards brilliant young kids from Stanford
3329540	3335440	with CS degrees as richly as it does, and it is amazingly richly, they'll go get CS
3335440	3336940	degrees, right?
3336940	3344020	Well, I do think it's a little crude.
3344020	3354700	I think money is one surrogate measure of also what is advancing in our time.
3354700	3362300	Maybe right now truly is one of the biggest drivers of the changes of our civilization.
3362300	3367540	When you're talking about what is this generation of students talk about, I was just thinking
3367540	3373900	that 400 years ago, when the scientific revolution was happening, what is in the dorms, of course
3373900	3381100	it's all young men in Cambridge or Oxford, but that must also be a very exciting and
3381100	3382100	interesting time.
3382100	3387420	Of course, there was an internet and social media to propel the travel of the knowledge,
3387420	3396900	but imagine there was the blossoming of discovery and of our understanding of the physical world.
3396900	3402300	Right now we're in that kind of great era of technological blossoming.
3402300	3404020	It's a digital revolution.
3404020	3410740	So the conversations in the dorm, I think it's a blend of the meaning of who we are
3410780	3415900	as humans, as well as our relationship to these technologies we're building.
3415900	3417980	And so it's a...
3417980	3428740	So properly taught technology can subsume or embed philosophy literature?
3428740	3432140	Of course, can inspire, can inspire.
3432140	3436540	And also think about it, what follows scientific revolution is a great period of change of
3436540	3439900	political, social, economical change, right?
3440100	3441100	We're seeing that.
3441100	3442100	Not all for the better.
3442100	3443100	Right.
3443100	3449380	And I'm not saying it's necessary for the better, but we are seeing, we're having even
3449380	3455220	peaked the digital revolution, but we're already seeing the political, social, economic changes.
3455220	3461700	So this is, again, back to Stanford High when we founded it five years ago.
3461700	3467780	We believe all this is happening and this is an institute where these kind of conversations,
3467780	3475140	ideas, debates should be taking place and education programs should be happening.
3475140	3478180	And that's part of the reason why we did this.
3478180	3485580	Let me tell you, yeah, so as you pointed out, I just finished teaching a course called Philosophy
3485580	3489980	of Artificial Intelligence, about which I found out too late, I would have asked permission
3489980	3491260	to audit your course, John.
3491260	3494060	No, no, you're too old.
3494060	3500140	So and about half of the students were computer science students who were planned to be computer
3500140	3502260	science majors.
3502260	3510420	Another quarter planned to be symbolic systems majors, which is a major that is related to
3510420	3516380	computer science, and then there was a smattering of others.
3516380	3521660	And these were people, every one of them at the end of the course, and I'm not saying
3521660	3526900	this to brag, every one of them said, this is the best course we've ever taken.
3526900	3529340	And why did they say that?
3529340	3533220	It inspired, it made them think.
3533220	3539820	It gave them a framework for thinking, a framework for trying to address some of these problems,
3539820	3543220	some of the worries that you've brought out today.
3543220	3550820	And how do we think about them and how do we not just become panicked because of some
3550860	3558020	science fiction movie that we've seen, or because we read Ray Kurzweil.
3558020	3560700	So maybe it's just as well I didn't take the course.
3560700	3564580	I'm sure John would have given me a C-minus at best.
3564580	3567020	Great inflation.
3567020	3580700	So it's clear that these kids, the students, are looking for the opening to think the
3580740	3588740	things and to understand how to address ethical questions, how to address hard philosophical
3588740	3595340	questions, and that's what they got out of the course.
3595340	3598460	And that's a way of looking for meaning in this time.
3598460	3600460	Yes it is.
3600460	3606420	Dr. Feifei Li and Dr. John Etchamendi, both of the Stanford Institute for Human-Centered
3606420	3608260	Artificial Intelligence.
3608260	3609260	Thank you.
3609260	3610380	Thank you, Peter.
3610380	3614660	For Uncommon Knowledge and the Hoover Institution and Fox Nation, I'm Peter Robinson.
