1
00:00:00,000 --> 00:00:04,740
The year was 1956 and the place was Dartmouth College.

2
00:00:04,740 --> 00:00:10,640
In a research proposal, a math professor used a term that was then entirely new and entirely

3
00:00:10,640 --> 00:00:14,720
fanciful, artificial intelligence.

4
00:00:14,720 --> 00:00:17,980
There's nothing fanciful about AI anymore.

5
00:00:17,980 --> 00:00:22,080
The directors of the Stanford Institute for Human-Centered Artificial Intelligence, John

6
00:00:22,080 --> 00:00:24,440
Etchimendi, and Fei-Fei Li.

7
00:00:24,440 --> 00:00:36,800
On Uncommon Knowledge, now.

8
00:00:36,800 --> 00:00:38,120
Welcome to Uncommon Knowledge.

9
00:00:38,120 --> 00:00:39,800
I'm Peter Robinson.

10
00:00:39,800 --> 00:00:46,520
Philosopher John Etchimendi served from 2000 to 2017 as provost here at Stanford University.

11
00:00:46,520 --> 00:00:51,240
Dr. Etchimendi received his undergraduate degree from the University of Nevada before earning

12
00:00:51,240 --> 00:00:54,320
his doctorate in philosophy at Stanford.

13
00:00:54,320 --> 00:00:59,280
He earned that doctorate in 1983 and became a member of the Stanford Philosophy Department

14
00:00:59,280 --> 00:01:01,200
the very next year.

15
00:01:01,200 --> 00:01:07,840
He's the author of a number of books, including the 1990 volume The Concept of Logical Consequence.

16
00:01:07,840 --> 00:01:11,360
Since stepping down as provost, Dr. Etchimendi has held a number of positions at Stanford,

17
00:01:11,360 --> 00:01:16,800
including, and for our purposes today, this is the relevant position, co-director of the

18
00:01:16,800 --> 00:01:21,280
Stanford Institute for Human-Centered Artificial Intelligence.

19
00:01:21,280 --> 00:01:26,600
Born in Beijing, Dr. Fei-Fei Li moved to this country at the age of 15.

20
00:01:26,600 --> 00:01:31,160
She received her undergraduate degree from Princeton and a doctorate in electrical engineering

21
00:01:31,160 --> 00:01:34,240
from the California Institute of Technology.

22
00:01:34,240 --> 00:01:39,040
Now a professor of computer science here at Stanford, Dr. Li is the founder once again

23
00:01:39,040 --> 00:01:43,040
of the Stanford Institute for Human-Centered Artificial Intelligence.

24
00:01:43,800 --> 00:01:49,400
Dr. Li's memoir published just last year, The Worlds I See, Curiosity, Exploration,

25
00:01:49,400 --> 00:01:52,440
and Discovery at the Dawn of AI.

26
00:01:52,440 --> 00:01:55,840
John Etchimendi and Fei-Fei Li, thank you for making the time to join me.

27
00:01:55,840 --> 00:01:59,240
Thank you for inviting us.

28
00:01:59,240 --> 00:02:04,360
I would say that I'm going to ask a dumb question, but I'm actually going to ask a question

29
00:02:04,360 --> 00:02:07,600
that is right at the top of my form.

30
00:02:07,600 --> 00:02:10,360
What is artificial intelligence?

31
00:02:10,360 --> 00:02:15,680
I have seen the term 100 times a day for, what, several years now.

32
00:02:15,680 --> 00:02:21,720
I have yet to find a succinct and satisfying explanation.

33
00:02:21,720 --> 00:02:22,720
Let's see.

34
00:02:22,720 --> 00:02:23,720
Well, let's go to the philosophy.

35
00:02:23,720 --> 00:02:26,520
Here's a man who's professionally rigorous, but here's a woman who actually knows the

36
00:02:26,520 --> 00:02:27,520
answer.

37
00:02:27,520 --> 00:02:28,520
Yeah, she knows the answer.

38
00:02:28,520 --> 00:02:31,520
So, let's say the answer, and then I will give you a different answer.

39
00:02:31,520 --> 00:02:32,520
Oh, really?

40
00:02:32,520 --> 00:02:33,520
All right.

41
00:02:33,520 --> 00:02:34,520
Okay.

42
00:02:34,520 --> 00:02:39,120
Peter used the word succinct, and I'm sweating here, so because artificial intelligence

43
00:02:39,120 --> 00:02:50,720
by today is already a collection of methods and tools that summarizes the overall area

44
00:02:50,720 --> 00:03:00,800
of computer science that has to do with data, pattern recognition, decision-making in natural

45
00:03:00,800 --> 00:03:08,160
language, in images, in videos, in robotics, in speech, so it's really a collection at

46
00:03:08,160 --> 00:03:14,600
the heart of artificial intelligence is statistical modeling, such as machine learning, using

47
00:03:14,600 --> 00:03:16,600
computer programs.

48
00:03:16,600 --> 00:03:24,120
But today, artificial intelligence truly is an umbrella term that covers many things

49
00:03:24,120 --> 00:03:29,160
that we're starting to feel familiar about, for example, language intelligence, language

50
00:03:29,160 --> 00:03:32,560
modeling, or speech, or vision.

51
00:03:32,920 --> 00:03:39,080
And you and I both knew John McCarthy, who came to Stanford after he wrote that, used

52
00:03:39,080 --> 00:03:44,240
the term coin, the term artificial intelligence, now the late John McCarthy, and I confess

53
00:03:44,240 --> 00:03:49,040
to you who knew him, as I did, that I'm a little suspicious of the term because I knew

54
00:03:49,040 --> 00:03:52,840
John, and John liked to be provocative.

55
00:03:52,840 --> 00:03:58,360
And I am thinking to myself, wait a moment, we're still dealing with ones and zeros.

56
00:03:58,360 --> 00:04:01,600
Computers are calculating machines.

57
00:04:01,600 --> 00:04:05,360
artificial intelligence is a marketing term.

58
00:04:05,360 --> 00:04:09,080
So no, it's not really a marketing term.

59
00:04:09,080 --> 00:04:13,880
So I will give you an answer that is more like what John would have given.

60
00:04:13,880 --> 00:04:20,160
And that is, it's the field, the subfield of computer science that attempts to create

61
00:04:20,160 --> 00:04:29,040
machines that can accomplish tasks that seem to require intelligence.

62
00:04:29,040 --> 00:04:37,040
So the early artificial intelligence were systems that played chess or checkers, even,

63
00:04:37,040 --> 00:04:38,840
you know, very, very simple things.

64
00:04:38,840 --> 00:04:48,040
Now John, who, as you know, if you knew him, was ambitious.

65
00:04:48,040 --> 00:04:54,200
And he thought that in a summer conference at Dartmouth, they could solve most of the

66
00:04:54,200 --> 00:04:55,200
problems.

67
00:04:55,200 --> 00:05:00,960
All right, I'm going to come, let me name a couple of very famous events.

68
00:05:00,960 --> 00:05:03,800
What I'm looking for here, I'll name the events.

69
00:05:03,800 --> 00:05:08,600
We have, in 1997, a computer defeats Gary Kasparov at chess.

70
00:05:08,600 --> 00:05:10,600
Big moment for the first time.

71
00:05:10,600 --> 00:05:15,680
Big blue and IBM project defeats a human being at chess, and not just a human being, but

72
00:05:15,680 --> 00:05:19,680
Gary Kasparov, who by some measures is one of the half dozen greatest chess players who

73
00:05:19,680 --> 00:05:22,520
ever lived.

74
00:05:22,520 --> 00:05:27,400
And as best I can tell, computer scientists said, you know, things are getting faster,

75
00:05:27,400 --> 00:05:28,920
but still.

76
00:05:28,920 --> 00:05:37,440
And then we have, in 2015, a computer defeats Go expert Han Fuei, and the following year

77
00:05:37,440 --> 00:05:43,160
it defeats Go grandmaster Lee Seadol, I'm not at all sure I'm pronouncing that correctly,

78
00:05:43,160 --> 00:05:45,240
in a five game match.

79
00:05:45,240 --> 00:05:50,600
And people say, whoa, something just happened this time.

80
00:05:50,600 --> 00:05:55,280
So what I'm looking for here is something, something that a layman like me can latch

81
00:05:55,280 --> 00:05:57,680
on to and say, here's the discontinuity.

82
00:05:57,680 --> 00:05:59,720
Here's where we entered a new moment.

83
00:05:59,720 --> 00:06:01,680
Here's artificial intelligence.

84
00:06:01,680 --> 00:06:04,520
Am I looking for something that doesn't exist?

85
00:06:04,520 --> 00:06:08,000
No, no, I think you're not.

86
00:06:08,000 --> 00:06:17,000
So the difference between deep blue and which played chess, deep blue was written using

87
00:06:17,000 --> 00:06:19,960
traditional programming techniques.

88
00:06:19,960 --> 00:06:25,520
And what deep blue did is it, it would, for each move, for each position on the board,

89
00:06:25,520 --> 00:06:27,960
it would look down to all the possible.

90
00:06:27,960 --> 00:06:29,560
Every conceivable decision tree.

91
00:06:29,560 --> 00:06:32,880
Every decision tree, to a certain depth.

92
00:06:32,880 --> 00:06:36,680
I mean, obviously, you can't go all the way.

93
00:06:36,680 --> 00:06:41,040
And it would, it would have ways of, of way, which ones are best.

94
00:06:41,040 --> 00:06:46,160
And so then it would say, no, this is the best move for me at this time.

95
00:06:46,160 --> 00:06:52,160
That's why, in some sense, it was not theoretically very interesting.

96
00:06:52,160 --> 00:06:59,960
The, the go, AlphaGo, AlphaGo, which was a Google project, was that Google project.

97
00:06:59,960 --> 00:07:09,000
This uses deep learning, it's a neural net, it's not explicit, explicit programming.

98
00:07:09,000 --> 00:07:15,640
We don't know, you know, we don't go into it with an idea of, here's the algorithm

99
00:07:15,640 --> 00:07:19,440
we're going to use, do this, and then do this, and do this.

100
00:07:19,440 --> 00:07:25,520
So it was actually quite a surprise, particularly AlphaGo.

101
00:07:25,520 --> 00:07:29,360
Not to me, but to the public, yes.

102
00:07:29,360 --> 00:07:30,360
To the public.

103
00:07:30,360 --> 00:07:31,360
Yeah.

104
00:07:31,360 --> 00:07:35,160
But our, our colleague, I'm going at this one more time because I really want to understand

105
00:07:35,160 --> 00:07:36,160
this.

106
00:07:36,160 --> 00:07:37,160
I really do.

107
00:07:37,160 --> 00:07:40,800
Our colleague here at Stanford, ZX Shen, who must be known to both of you, physicist

108
00:07:40,800 --> 00:07:42,160
here at Stanford.

109
00:07:42,160 --> 00:07:46,480
And he said to me, Peter, what you need to understand about the moment when a computer

110
00:07:46,480 --> 00:07:53,800
defeated go, go, which is in much more complicated, at least in the decision space, much, much

111
00:07:53,800 --> 00:07:55,800
bigger, so to speak, than chess.

112
00:07:55,800 --> 00:07:58,360
There are more pieces, more square, alright.

113
00:07:58,360 --> 00:08:03,900
And ZX said to me, that whereas chess just did more quickly, what a committee of grand

114
00:08:03,900 --> 00:08:10,200
masters would have decided on, the computer in go was creative.

115
00:08:10,200 --> 00:08:13,680
It was pursuing strategies that human beings had never pursued before.

116
00:08:13,680 --> 00:08:15,280
Is there something to that?

117
00:08:15,280 --> 00:08:17,400
Yeah, so there's a famous, no.

118
00:08:17,400 --> 00:08:19,720
If he's getting impatient with me, I'm asking such, go ahead.

119
00:08:19,720 --> 00:08:21,480
No, no, you're asking such good questions.

120
00:08:21,480 --> 00:08:26,320
So in the third game of the, I think it was the third game of the five games, there was

121
00:08:26,320 --> 00:08:27,320
a move.

122
00:08:27,320 --> 00:08:28,720
I think it was move 32.

123
00:08:28,720 --> 00:08:29,720
32 or 35.

124
00:08:29,720 --> 00:08:30,720
32 or 35.

125
00:08:30,720 --> 00:08:39,520
It's that the computer program made a move that really surprised every single go masters.

126
00:08:39,520 --> 00:08:43,480
Not only Lisa Dole himself, but everybody who's watching.

127
00:08:43,480 --> 00:08:46,040
That's a very, that's a very surprising move.

128
00:08:46,040 --> 00:08:49,040
I thought it was, I thought it was a mistake.

129
00:08:49,040 --> 00:08:56,800
In fact, even post-anonymizing how that move came about, the human masters would say, this

130
00:08:56,800 --> 00:08:59,440
is completely unexpected.

131
00:08:59,440 --> 00:09:08,280
What happens is that the computers, like John says, right, is, has the learning ability,

132
00:09:08,280 --> 00:09:16,200
and has the inference ability to think about patterns or to decide on certain movements,

133
00:09:16,200 --> 00:09:25,800
even outside of the trained, familiar human masters, domain of knowledge in this particular

134
00:09:25,800 --> 00:09:26,800
game.

135
00:09:26,800 --> 00:09:27,800
May I?

136
00:09:27,800 --> 00:09:28,800
Go ahead.

137
00:09:28,800 --> 00:09:29,800
Yes, yes.

138
00:09:29,800 --> 00:09:30,800
Expand on that.

139
00:09:30,800 --> 00:09:39,480
Deep neural nets are supremely good pattern recognition systems.

140
00:09:39,480 --> 00:09:45,760
But the patterns they recognize, the patterns they learn to recognize are not necessarily

141
00:09:45,760 --> 00:09:49,760
exactly the patterns that humans recognize.

142
00:09:49,760 --> 00:09:57,800
So it was seeing something about that position, and it made a move that because of the patterns

143
00:09:57,800 --> 00:10:06,320
that it recognized in the, in the board, that made no sense from a human standpoint.

144
00:10:06,320 --> 00:10:12,880
In fact, all of the, all of the lessons in how to play go tell you never make a move that

145
00:10:12,880 --> 00:10:16,420
close to the edge that, that quickly.

146
00:10:16,420 --> 00:10:22,080
And so everybody thought it made a mistake, and then it proceeded to win.

147
00:10:22,080 --> 00:10:28,240
And I think the way to understand that is it's just seeing patterns that we don't see.

148
00:10:28,240 --> 00:10:36,040
It's computing patterns that, that is not traditionally human, and it has the capacity

149
00:10:36,040 --> 00:10:37,040
to compute.

150
00:10:37,040 --> 00:10:38,040
Okay.

151
00:10:38,040 --> 00:10:44,080
I'm trying to, we're already entering this territory, but I am trying really hard to

152
00:10:44,080 --> 00:10:51,480
tease out the, wait a moment, these are still just machines running zeros and ones, bigger

153
00:10:51,480 --> 00:10:55,240
and bigger memory, faster and faster ability to calculate, but we're still dealing with

154
00:10:55,240 --> 00:10:56,960
machines that run zeros and ones.

155
00:10:56,960 --> 00:10:58,560
That's one strand.

156
00:10:58,560 --> 00:11:03,280
And the other strand is, as you well know, 2001 Space Odyssey, where the computer takes

157
00:11:03,280 --> 00:11:04,760
over the ship.

158
00:11:04,760 --> 00:11:07,880
Open the pod bay doors, Hal.

159
00:11:07,880 --> 00:11:09,560
I'm sorry, Dave.

160
00:11:09,560 --> 00:11:11,880
I'm afraid I can't do that.

161
00:11:11,880 --> 00:11:12,880
Okay.

162
00:11:12,880 --> 00:11:16,040
We'll keep, we'll keep, we'll come to this soon enough.

163
00:11:16,040 --> 00:11:20,800
Fei-Fei Li in your memoir, The Worlds I See, quote, I believe our civilization stands

164
00:11:20,800 --> 00:11:28,200
on the cusp of a technological revolution with the power to reshape life as we know

165
00:11:28,200 --> 00:11:29,360
it.

166
00:11:29,360 --> 00:11:33,560
Revolution, reshape life as we know it.

167
00:11:33,560 --> 00:11:37,840
Now you're a man whose whole academic training is in rigor.

168
00:11:37,840 --> 00:11:41,440
Are you going to let her get away with, with this over, kind of wild overstatement?

169
00:11:41,440 --> 00:11:44,760
No, I don't think it's an overstatement.

170
00:11:45,000 --> 00:11:46,000
Oh.

171
00:11:46,000 --> 00:11:47,000
I think she's right.

172
00:11:47,000 --> 00:11:50,680
He told me to write the book.

173
00:11:50,680 --> 00:11:57,360
Mind you, Peter, it's a technology that is extremely powerful, that will allow us and

174
00:11:57,360 --> 00:12:06,080
is allowing us to get computers to do things we never could have programmed them to do.

175
00:12:06,080 --> 00:12:08,480
And it will change everything.

176
00:12:08,480 --> 00:12:14,280
But it's like, a lot of people have said it's like electricity, it's like the steam

177
00:12:14,280 --> 00:12:16,000
revolution.

178
00:12:16,000 --> 00:12:19,600
It's not something necessarily to be afraid of.

179
00:12:19,600 --> 00:12:23,320
It's not that it's going to suddenly take over the world, that's not what Fei-Fei was

180
00:12:23,320 --> 00:12:24,320
saying.

181
00:12:24,320 --> 00:12:25,320
Well, right.

182
00:12:25,320 --> 00:12:32,680
It's a powerful tool that will revolutionize industries and human the way we live, but

183
00:12:32,680 --> 00:12:38,800
the word revolution is not that it's a conscious being, it's just a powerful tool that changes

184
00:12:38,800 --> 00:12:39,800
things.

185
00:12:39,840 --> 00:12:46,560
And at reassuring, if a few pages later Fei-Fei had not gone on to write, there's no separating

186
00:12:46,560 --> 00:12:52,320
the beauty of science from something like, say, the Manhattan Project, close quote.

187
00:12:52,320 --> 00:12:58,880
Nuclear science, we can produce abundant energy, but it can also produce weapons of indescribable

188
00:12:58,880 --> 00:12:59,880
horror.

189
00:12:59,880 --> 00:13:05,040
AI has boogeymen of its own, whether it's killer robots, widespread surveillance, or

190
00:13:05,040 --> 00:13:09,880
even just automating all 8 billion of us out of our jobs.

191
00:13:09,880 --> 00:13:14,800
Now we could devote an entire program to each of those boogeymen, and maybe at some point

192
00:13:14,800 --> 00:13:17,240
we should.

193
00:13:17,240 --> 00:13:22,520
But now that you have scared me, even in the act of reassuring me, and in fact, it throws

194
00:13:22,520 --> 00:13:26,240
me that you're so eager to reassure me that I think maybe I really should be even more

195
00:13:26,240 --> 00:13:27,600
scared than I am.

196
00:13:27,600 --> 00:13:28,960
Let me just go right down.

197
00:13:28,960 --> 00:13:30,520
Here's the killer robots.

198
00:13:30,520 --> 00:13:31,920
Let me quote the late Henry Kissinger.

199
00:13:31,920 --> 00:13:37,640
I'm just going to put these up and let you, you may calm me down if you can.

200
00:13:37,640 --> 00:13:41,840
Henry Kissinger, if you imagine a war between China and the United States, you have artificial

201
00:13:41,840 --> 00:13:43,880
intelligence weapons.

202
00:13:43,880 --> 00:13:51,120
Nobody has tested these things on a broad scale, and nobody can tell exactly what will happen

203
00:13:51,120 --> 00:13:54,920
when AI fighter planes on both sides interact.

204
00:13:54,920 --> 00:13:59,480
So you are then, I'm quoting Henry Kissinger, who is not a fool after all, so you are then

205
00:13:59,480 --> 00:14:02,560
in a world of potentially total destructiveness.

206
00:14:02,560 --> 00:14:03,560
Close quote.

207
00:14:03,560 --> 00:14:04,560
Fei-Fei.

208
00:14:04,560 --> 00:14:10,080
So like I said, I'm now denying how powerful these tools are.

209
00:14:10,080 --> 00:14:17,200
I mean, humanity, before AI has already created tools and technology that are very destructive,

210
00:14:17,200 --> 00:14:18,520
could be very destructive.

211
00:14:18,520 --> 00:14:21,360
We talk about Manhattan Project, right?

212
00:14:21,360 --> 00:14:28,560
But that doesn't mean that we should collectively decide to use this tool in this destructive

213
00:14:28,560 --> 00:14:29,640
way.

214
00:14:29,640 --> 00:14:36,240
Okay, Peter, you know, think back before you even had heard about artificial intelligence.

215
00:14:36,240 --> 00:14:37,840
Which actually, what is it five years ago?

216
00:14:37,840 --> 00:14:38,840
No, I know.

217
00:14:38,840 --> 00:14:40,240
This is all happening so fast.

218
00:14:40,240 --> 00:14:43,640
Just five years ago, or 10 years ago.

219
00:14:43,640 --> 00:14:53,320
Remember the tragic incident where an Iranian passenger plane was shot down flying over the

220
00:14:53,320 --> 00:14:56,240
Persian Gulf by an Aegis system?

221
00:14:56,240 --> 00:14:57,400
Yes, yes.

222
00:14:57,400 --> 00:15:05,760
And one of our ships, an automated system, because it had to be automated in order to

223
00:15:05,760 --> 00:15:06,760
be...

224
00:15:06,760 --> 00:15:07,760
Humans can't react that fast.

225
00:15:07,760 --> 00:15:08,760
Yeah, exactly.

226
00:15:08,760 --> 00:15:14,600
And in this case, for reasons that I think are quite understandable now that you understand

227
00:15:14,600 --> 00:15:19,960
the incident, but it did something that was horrible.

228
00:15:19,960 --> 00:15:24,560
That's not different in kind from what you can do with AI, right?

229
00:15:24,560 --> 00:15:36,800
So we as creators of these devices, or as users of AI, have to be vigilant about what

230
00:15:36,800 --> 00:15:39,440
kind of use we put them to.

231
00:15:39,440 --> 00:15:45,800
And when we decide to put them to one particular use, and there may be uses, the military has

232
00:15:45,800 --> 00:15:53,880
many good uses for them, we have to be vigilant about their doing what we intend them to do

233
00:15:53,880 --> 00:15:56,840
rather than doing things that we don't intend them to do.

234
00:15:56,840 --> 00:15:59,200
So you're announcing a great theme.

235
00:15:59,200 --> 00:16:07,280
And that theme is that what Dr. Fei-Fei Li has invented makes the discipline to which

236
00:16:07,280 --> 00:16:12,640
you have dedicated your life, philosophy, even more important, not less so.

237
00:16:12,640 --> 00:16:13,640
Yeah, that's why we're the co-directors.

238
00:16:13,640 --> 00:16:17,080
The power of this instrument makes the human being more important, not less so.

239
00:16:17,080 --> 00:16:18,080
Am I making...

240
00:16:18,080 --> 00:16:19,080
Am I being glib?

241
00:16:19,080 --> 00:16:20,080
Or is that onto...

242
00:16:20,080 --> 00:16:24,840
So let me tell you a story about...

243
00:16:24,840 --> 00:16:29,120
So Fei-Fei used to live next door to me, or close to next door to me.

244
00:16:29,120 --> 00:16:30,120
And I was talking...

245
00:16:30,120 --> 00:16:34,440
I'm not sure whether that would make me feel more safe or more exposed, no.

246
00:16:34,440 --> 00:16:38,560
And I was talking to her, I was still a pro at this time.

247
00:16:38,560 --> 00:16:45,640
And she said to me, you and John Hennessey started a lot of institutes that brought technology

248
00:16:45,640 --> 00:16:49,320
into other parts of the university.

249
00:16:49,320 --> 00:16:56,320
We need to start an institute that brings philosophy and ethics and the social sciences

250
00:16:56,320 --> 00:17:06,280
into AI, because AI is too dangerous to leave it to the computer scientists alone.

251
00:17:06,280 --> 00:17:07,280
Nothing wrong with that.

252
00:17:07,280 --> 00:17:10,680
There are many stories about how hard it was to persuade him when he was provost, and you

253
00:17:10,680 --> 00:17:11,680
succeeded.

254
00:17:11,680 --> 00:17:12,680
Can I...

255
00:17:12,680 --> 00:17:17,800
Just one more boogeyman briefly, and we'll return to that theme that you just gave us

256
00:17:17,880 --> 00:17:21,160
there, and then we'll get back to the Stanford Institute.

257
00:17:21,160 --> 00:17:22,360
I'm quoting you again.

258
00:17:22,360 --> 00:17:24,240
This is from your memoir.

259
00:17:24,240 --> 00:17:28,560
The prospect of just automating all billion of us out of our jobs.

260
00:17:28,560 --> 00:17:29,560
That's the phrase you used.

261
00:17:29,560 --> 00:17:36,520
Well, it turns out that it took me mere seconds, using my AI-enabled search algorithm, search

262
00:17:36,520 --> 00:17:42,480
device, to find a Goldman Sachs study from last year, predicting that in the United States

263
00:17:42,480 --> 00:17:49,480
and Europe, some two-thirds of all jobs could be automated, at least to some degree.

264
00:17:49,480 --> 00:17:55,880
So, why shouldn't we all be terrified, Henry Kissinger of World Apocalypse, all right, maybe

265
00:17:55,880 --> 00:17:59,360
that's a bit too much, but my job.

266
00:17:59,360 --> 00:18:03,160
So I think job change is real.

267
00:18:03,160 --> 00:18:09,360
Job change is real with every single technological advances that humanity, human civilization

268
00:18:09,360 --> 00:18:11,640
has faced.

269
00:18:11,640 --> 00:18:15,240
That is real, and that's not to be taken lightly.

270
00:18:15,240 --> 00:18:18,440
We also have to be careful with the word job.

271
00:18:18,440 --> 00:18:26,440
Job tends to describe a holistic profession or that a person attaches his or her income

272
00:18:26,440 --> 00:18:29,440
as well as identity.

273
00:18:29,440 --> 00:18:35,480
But there is also, within every job, pretty much within every job, there are so many tasks.

274
00:18:35,480 --> 00:18:41,960
It's hard to imagine there's one job that has only one singular task, right, like being

275
00:18:41,960 --> 00:18:46,600
a professor, being a scholar, being a doctor, being a cook.

276
00:18:46,600 --> 00:18:49,360
All of these jobs have multiple tasks.

277
00:18:49,360 --> 00:18:56,000
What we're seeing is technology is changing how some of these tasks can be done.

278
00:18:56,000 --> 00:19:03,120
And it's true, as it changes these tasks, some of them, some part of them could be automated.

279
00:19:03,120 --> 00:19:07,960
It's starting to change how the jobs are, and eventually it's going to impact jobs.

280
00:19:07,960 --> 00:19:12,840
So this is going to be a gradual process, and it's very important we stay on top of

281
00:19:12,840 --> 00:19:13,840
this.

282
00:19:13,840 --> 00:19:19,400
This is why Human Center AI Institute was founded, is these questions are profound.

283
00:19:19,400 --> 00:19:22,680
They're by definition multidisciplinary.

284
00:19:22,680 --> 00:19:28,840
Computer scientists alone cannot do all the economic analysis, but economists now understanding

285
00:19:28,840 --> 00:19:37,120
what these computer science programs do will not, by themselves, understand the shift of

286
00:19:37,120 --> 00:19:38,120
the jobs.

287
00:19:38,120 --> 00:19:39,120
John, may I tell you?

288
00:19:39,120 --> 00:19:40,120
Go ahead.

289
00:19:40,120 --> 00:19:42,960
But let me just point something out.

290
00:19:42,960 --> 00:19:50,360
The Goldman Sachs study said that such and such percentage of jobs will be automated

291
00:19:50,360 --> 00:19:53,320
or can be automated at least in part.

292
00:19:53,320 --> 00:19:54,320
Yes.

293
00:19:54,320 --> 00:19:58,840
What they're saying is that a certain number of the tasks that go into a particular job

294
00:19:58,840 --> 00:19:59,840
are done.

295
00:19:59,840 --> 00:20:00,840
Exactly.

296
00:20:00,840 --> 00:20:08,400
So Peter, you said it only took me a few seconds to go to the computer and find that

297
00:20:08,400 --> 00:20:10,880
article.

298
00:20:10,880 --> 00:20:12,880
Guess what?

299
00:20:12,880 --> 00:20:17,120
That's one of the tasks that would have taken you a lot of time.

300
00:20:17,120 --> 00:20:21,840
So part of your job has been automated.

301
00:20:21,840 --> 00:20:22,840
Okay.

302
00:20:22,840 --> 00:20:24,280
Now let me tell you a story.

303
00:20:24,280 --> 00:20:25,520
But also empowered.

304
00:20:25,520 --> 00:20:26,520
Empowered.

305
00:20:26,520 --> 00:20:27,520
Empowered.

306
00:20:27,520 --> 00:20:28,520
Exactly.

307
00:20:28,520 --> 00:20:29,520
Fine.

308
00:20:29,520 --> 00:20:30,520
Thank you.

309
00:20:30,520 --> 00:20:31,520
Thank you.

310
00:20:31,520 --> 00:20:32,520
Thank you.

311
00:20:32,520 --> 00:20:33,520
You're making me feel good.

312
00:20:33,520 --> 00:20:34,520
Now let me tell you a story.

313
00:20:34,520 --> 00:20:35,520
All three of us live in California, which means all three of us probably have some friends

314
00:20:35,520 --> 00:20:36,520
down in Hollywood.

315
00:20:36,520 --> 00:20:40,120
And I have a friend who was involved in the writer's strike.

316
00:20:40,120 --> 00:20:41,120
Yeah.

317
00:20:41,120 --> 00:20:42,120
Okay.

318
00:20:42,120 --> 00:20:44,160
And here's the problem.

319
00:20:44,160 --> 00:20:48,960
To run a sitcom, you used to run a writer's room.

320
00:20:48,960 --> 00:20:54,200
And the writer's room would employ seven, a dozen on The Simpsons Show, The Cartoon

321
00:20:54,200 --> 00:20:55,200
Show.

322
00:20:55,200 --> 00:20:57,320
They'd had a couple of writer's rooms running.

323
00:20:57,320 --> 00:20:58,800
They were employing 20.

324
00:20:58,800 --> 00:21:04,080
And these were the last kind of person you'd imagine a computer could replace because they

325
00:21:04,080 --> 00:21:08,880
were well-educated and witty and quick with words.

326
00:21:08,880 --> 00:21:13,120
And you think of computers as just running calculations, maybe spreadsheets, maybe someday

327
00:21:13,120 --> 00:21:18,000
they can eliminate accountants, but writers, Hollywood writers.

328
00:21:18,000 --> 00:21:25,480
But it turns out, and my friend illustrated this for me by saying, doing the artificial

329
00:21:25,480 --> 00:21:33,360
intelligence thing, we're at a prompt, draft a skit for Saturday Night Live in which Joe

330
00:21:33,360 --> 00:21:40,920
Biden and Donald Trump are playing beer pong, 15 seconds.

331
00:21:40,920 --> 00:21:44,680
Now professionals could have tightened it up or made it, but it was pretty funny and

332
00:21:44,680 --> 00:21:46,920
it was instantaneous.

333
00:21:46,920 --> 00:21:48,360
And you know what that means?

334
00:21:48,360 --> 00:21:51,980
That means you don't need four or five of the seven writers.

335
00:21:51,980 --> 00:21:58,080
You need a senior writer to assign intelligence, the artificial, and you need maybe one other

336
00:21:58,080 --> 00:22:02,040
writer or two other writers to tighten it up or redraft it.

337
00:22:02,040 --> 00:22:03,920
It is upon us.

338
00:22:03,920 --> 00:22:08,120
And your artificial intelligence is going to get bad press when it starts eliminating

339
00:22:08,120 --> 00:22:10,520
the jobs of the chattering classes.

340
00:22:10,520 --> 00:22:12,920
And that has already begun.

341
00:22:12,920 --> 00:22:13,920
Tell me I'm wrong.

342
00:22:13,920 --> 00:22:23,200
Do you know, before the agricultural revolution, something like 80, 90% of all the people in

343
00:22:23,200 --> 00:22:28,280
the United States were employed on farms.

344
00:22:28,280 --> 00:22:37,240
We now, now it's down to 2% or 3%, and those same farms, that same land, is far, far more

345
00:22:37,240 --> 00:22:38,240
productive.

346
00:22:38,960 --> 00:22:47,800
Now, would you say that your life or anybody's life now was worse off than it was in the

347
00:22:47,800 --> 00:22:52,480
1890s when everybody was working on the farm?

348
00:22:52,480 --> 00:22:53,480
No.

349
00:22:53,480 --> 00:22:56,160
So yes, you're right.

350
00:22:56,160 --> 00:22:58,320
It will change jobs.

351
00:22:58,320 --> 00:23:00,600
It will make some jobs easier.

352
00:23:00,600 --> 00:23:05,600
It will make, allow us to do things that we could not do before.

353
00:23:05,600 --> 00:23:16,920
And yes, it will allow fewer people to do more of what they were doing before, and consequently

354
00:23:16,920 --> 00:23:20,440
there will be fewer people in that line of work.

355
00:23:20,440 --> 00:23:21,440
That's true.

356
00:23:21,440 --> 00:23:22,440
That is true.

357
00:23:22,440 --> 00:23:24,240
I also want to just point out two things.

358
00:23:24,240 --> 00:23:29,240
One is that jobs is always changing, and that change is always painful.

359
00:23:29,240 --> 00:23:34,720
And we're, as compared scientists, as philosophers, also as citizens of the world, we should be

360
00:23:34,720 --> 00:23:41,080
empathetic of that, and nobody is saying we should just ignore that change in pain.

361
00:23:41,080 --> 00:23:46,200
So this is why we're studying this, we're trying to talk to policymakers, we're educating

362
00:23:46,200 --> 00:23:47,200
the population.

363
00:23:47,200 --> 00:23:53,360
In the meantime, I think we should give more credit to human creativity in the face of

364
00:23:53,360 --> 00:23:54,520
AI.

365
00:23:54,520 --> 00:23:59,640
I start to use this example that's not even AI.

366
00:23:59,640 --> 00:24:08,320
Think about the advanced, speaking of Hollywood, graphics, technology, CGI, and all that, right?

367
00:24:08,320 --> 00:24:09,320
The video gaming industry?

368
00:24:09,320 --> 00:24:12,840
No, no, just animations and all that, right?

369
00:24:12,840 --> 00:24:20,400
One of many of our, including our children's, favorite animation series is by Ghibli Studio.

370
00:24:20,400 --> 00:24:27,680
You know, Princess Nomononaki, my neighbor Totoro, Spirited Away.

371
00:24:27,720 --> 00:24:35,000
All of these were made during a period where computer graphics technology is far more advanced

372
00:24:35,000 --> 00:24:38,280
than these hand-drawn animations.

373
00:24:38,280 --> 00:24:46,000
Yet the beauty, the creativity, the emotion, the uniqueness in this film continue to inspire

374
00:24:46,000 --> 00:24:49,200
and just entertain humanity.

375
00:24:49,200 --> 00:24:56,520
So I think we need to still have that pride and also give the credit to humans.

376
00:24:56,520 --> 00:25:01,640
Let's not forget our creativity and emotion and intelligence is unique.

377
00:25:01,640 --> 00:25:04,640
It's not going to be taken away by technology.

378
00:25:04,640 --> 00:25:05,640
Thank you.

379
00:25:05,640 --> 00:25:07,400
I feel slightly reassured.

380
00:25:07,400 --> 00:25:10,640
I'm still nervous about my job, but I feel slightly reassured.

381
00:25:10,640 --> 00:25:17,120
But you mentioned government a moment ago, which leads us to how we should regulate AI.

382
00:25:17,120 --> 00:25:19,120
Let me give you two quotations.

383
00:25:19,120 --> 00:25:20,120
I'll begin.

384
00:25:20,120 --> 00:25:23,880
I'm coming to the quotation from the two of you, but I'm going to start with a recent

385
00:25:23,880 --> 00:25:28,440
article in the Wall Street Journal by Senator Ted Cruz of Texas and former Senator Phil

386
00:25:28,440 --> 00:25:30,280
Graham also of Texas.

387
00:25:30,280 --> 00:25:36,680
Quote, the Clinton administration took a hands-off approach to regulating the early internet.

388
00:25:36,680 --> 00:25:41,680
In so doing, it unleashed extraordinary economic growth and prosperity.

389
00:25:41,680 --> 00:25:47,720
The Biden administration, by contrast, is impeding innovation in artificial intelligence with

390
00:25:47,720 --> 00:25:49,800
aggressive regulation.

391
00:25:49,800 --> 00:25:50,800
Close quote.

392
00:25:50,800 --> 00:25:52,240
That's them.

393
00:25:52,240 --> 00:25:53,240
This is you.

394
00:25:53,600 --> 00:25:58,120
Also a recent article in the Wall Street Journal, John Etchamendi and Fei-Fei Li.

395
00:25:58,120 --> 00:26:04,360
Quote, President Biden has signed an executive order on artificial intelligence that demonstrates

396
00:26:04,360 --> 00:26:09,880
his administration's commitment to harness and govern the technology.

397
00:26:09,880 --> 00:26:14,280
President Biden has set the stage and now it is time for Congress to act.

398
00:26:14,280 --> 00:26:16,600
Cruz and Graham, less regulation.

399
00:26:16,600 --> 00:26:20,120
Etchamendi and Lee, Biden administration has done well.

400
00:26:20,120 --> 00:26:22,480
Now Congress needs to give us even more.

401
00:26:22,480 --> 00:26:23,480
No.

402
00:26:23,480 --> 00:26:24,480
All right, John.

403
00:26:24,480 --> 00:26:27,040
No, I don't agree with that.

404
00:26:27,040 --> 00:26:33,960
I believe regulating any kind of technology is very difficult and you have to be careful

405
00:26:33,960 --> 00:26:41,120
not to regulate too soon or not to regulate too late.

406
00:26:41,120 --> 00:26:42,800
Let me give you another example.

407
00:26:42,800 --> 00:26:46,000
You talked about the internet and it's true.

408
00:26:46,000 --> 00:26:49,280
The government really was quite hands-off and that's good.

409
00:26:49,280 --> 00:26:50,280
That's good.

410
00:26:50,280 --> 00:26:51,280
It worked out.

411
00:26:51,280 --> 00:26:52,280
It worked out.

412
00:26:52,280 --> 00:26:55,880
Let's also think about social media.

413
00:26:55,880 --> 00:27:04,360
Social media has not worked exactly, worked out exactly the way we want it.

414
00:27:04,360 --> 00:27:12,200
We originally believed that we were going to enter a golden age in which friendship, comedy,

415
00:27:12,200 --> 00:27:19,520
well and everybody would have a voice and we could all live together at Kumbaya and

416
00:27:19,520 --> 00:27:20,520
so forth.

417
00:27:20,640 --> 00:27:22,640
What happened?

418
00:27:22,640 --> 00:27:26,920
Jonathan Haidt has a new book out on the particular pathologies among young people

419
00:27:26,920 --> 00:27:29,200
from all of these social media.

420
00:27:29,200 --> 00:27:30,200
Not an argument.

421
00:27:30,200 --> 00:27:34,680
It's an argument but it's based on lots of data.

422
00:27:34,680 --> 00:27:47,120
It seems to me that I'm in favor of very light-handed and informed regulation to try to put up sort

423
00:27:47,120 --> 00:27:48,120
of bumpers.

424
00:27:48,720 --> 00:27:50,680
I don't know what the analogy is.

425
00:27:50,680 --> 00:27:51,680
Guardrails.

426
00:27:51,680 --> 00:27:53,760
Guardrails for the technology.

427
00:27:53,760 --> 00:28:01,040
I am not for heavy-handed top-down regulation that stifles innovation.

428
00:28:01,040 --> 00:28:02,040
Here's another.

429
00:28:02,040 --> 00:28:04,520
Let me get on to this.

430
00:28:04,520 --> 00:28:07,760
I'm sure you'll be able to adapt your answer to this question too.

431
00:28:07,760 --> 00:28:10,200
I'm continuing your Wall Street Journal piece.

432
00:28:10,200 --> 00:28:13,880
Big tech companies can't be left to govern themselves.

433
00:28:13,880 --> 00:28:17,000
Around here, Silicon Valley, those are fighting words.

434
00:28:17,000 --> 00:28:21,680
Academic institutions should play a leading role in providing trustworthy assessments

435
00:28:21,680 --> 00:28:24,440
and benchmarking of these advanced technologies.

436
00:28:24,440 --> 00:28:29,680
We encourage an investment in human capital to bring more talent to the field of AI with

437
00:28:29,680 --> 00:28:31,160
academia and the government.

438
00:28:31,160 --> 00:28:32,160
Close quote.

439
00:28:32,160 --> 00:28:33,160
Okay.

440
00:28:33,160 --> 00:28:35,280
Now, it is mandatory for me to say this.

441
00:28:35,280 --> 00:28:41,880
So please forgive me, my fellow Stanford employees, apart from anything else.

442
00:28:41,880 --> 00:28:44,720
Why should academic institutions be trusted?

443
00:28:44,720 --> 00:28:48,040
Both the country has lost faith in academic institutions.

444
00:28:48,040 --> 00:28:52,800
DEI, the whole woke agenda, anti-Semitism on campus.

445
00:28:52,800 --> 00:28:57,200
We've got a recent Gallup poll showing the proportion of Americans who expressed a great

446
00:28:57,200 --> 00:29:00,880
deal or quite a lot of confidence in higher education.

447
00:29:00,880 --> 00:29:08,600
This year came in at just 36 percent and that is down in the last eight years from 57 percent.

448
00:29:08,600 --> 00:29:13,120
You are asking us to trust you at the very moment when we believe we have good reason

449
00:29:13,120 --> 00:29:14,840
to knock it off.

450
00:29:14,840 --> 00:29:15,840
Trust you?

451
00:29:15,840 --> 00:29:16,840
Okay.

452
00:29:16,840 --> 00:29:19,440
So I'll start with this first half of the answer.

453
00:29:19,440 --> 00:29:21,120
I'm sure John has a lot to say.

454
00:29:21,120 --> 00:29:28,320
I do want to make sure, especially wearing the hats of co-directors of HAI, when we talk

455
00:29:28,320 --> 00:29:33,720
about the relationship between government and technology, we tend to use the word regulation.

456
00:29:33,720 --> 00:29:36,720
I really, really want to double click.

457
00:29:36,720 --> 00:29:38,960
I want to use the word policy.

458
00:29:39,120 --> 00:29:43,880
Policy and regulation are related but not the same.

459
00:29:43,880 --> 00:29:49,800
When John and I wrote that Wall Street Journal Opinion piece, we really are focusing on a

460
00:29:49,800 --> 00:29:58,160
piece of policy that is to resource public sector AI, to resource academia because we

461
00:29:58,160 --> 00:30:05,600
believe that AI is such a powerful technology and science and academia and public sector

462
00:30:05,600 --> 00:30:10,320
still has a role to play to create public good.

463
00:30:10,320 --> 00:30:15,680
And public goods are curiosity-driven knowledge exploration.

464
00:30:15,680 --> 00:30:25,640
Our cures for cancers are the maps of biodiversity of our globe, our discovery of nanomaterials

465
00:30:25,640 --> 00:30:32,560
that we haven't seen before, our different ways of expressing in theater, in writing,

466
00:30:32,560 --> 00:30:33,560
in music.

467
00:30:33,720 --> 00:30:41,000
These are public goods and when we are collaborating with the government on policy, we're focusing

468
00:30:41,000 --> 00:30:42,000
on that.

469
00:30:42,000 --> 00:30:44,360
So I really want to make sure.

470
00:30:44,360 --> 00:30:49,280
Regulation we all have personal opinion, but there's more than regulation in policy.

471
00:30:49,280 --> 00:30:54,920
John, let me make one last run at you.

472
00:30:54,920 --> 00:31:00,800
In my theory here, although I'm asking questions that I'm quite sure you'd like to take me

473
00:31:00,800 --> 00:31:04,920
out and swap me around at this point, John, but this is serious.

474
00:31:04,920 --> 00:31:09,720
You've got the Stanford Institute for Human-Centered Artificial Intelligence and that's because

475
00:31:09,720 --> 00:31:12,520
you really think this is important.

476
00:31:12,520 --> 00:31:16,880
But we live in a democracy and you're going to have to convince a whole lot of people.

477
00:31:16,880 --> 00:31:20,280
So let me take one more run at you and then hand it back to you, John.

478
00:31:20,280 --> 00:31:22,440
Your article in the Wall Street Journal, again, let me repeat this.

479
00:31:22,440 --> 00:31:27,000
We encourage an investment in human capital to bring more talent to the field of AI with

480
00:31:27,000 --> 00:31:28,920
academia and the government, close quote.

481
00:31:28,920 --> 00:31:30,080
That means money.

482
00:31:30,080 --> 00:31:33,520
An investment means money and it means taxpayers' money.

483
00:31:33,520 --> 00:31:36,640
Here's what Cruz and Graham say in the Wall Street Journal, the Biden regulatory policy

484
00:31:36,640 --> 00:31:42,040
on AI has everything to do with special interest rent seeking.

485
00:31:42,040 --> 00:31:45,720
Stand for faculty, make well above the national average income.

486
00:31:45,720 --> 00:31:51,000
We are sitting at a university with an endowment of tens of billions of dollars.

487
00:31:51,000 --> 00:31:58,480
John, why is not your article in the Wall Street Journal the very kind of rent seeking

488
00:31:58,800 --> 00:32:01,880
that Senator Cruz and Senator Graham are saying?

489
00:32:01,880 --> 00:32:03,040
Are you kidding?

490
00:32:03,040 --> 00:32:08,080
Peter, let's take another example.

491
00:32:08,080 --> 00:32:15,240
So one of the greatest policy decisions that this country has ever made was when Vannevar

492
00:32:15,240 --> 00:32:22,240
Bush, advisor to at the time President Truman, convinced the state on through Eisenhower,

493
00:32:23,120 --> 00:32:24,120
as I recall, so it's important.

494
00:32:24,120 --> 00:32:25,120
Yeah, I'm kidding.

495
00:32:25,120 --> 00:32:26,120
He's bipartisan.

496
00:32:26,120 --> 00:32:27,120
Exactly.

497
00:32:27,120 --> 00:32:28,120
Exactly.

498
00:32:28,120 --> 00:32:36,760
It's not a bipartisan issue at all, but convinced Truman to set up the NSF for funding...

499
00:32:36,760 --> 00:32:37,760
National Science Foundation.

500
00:32:37,760 --> 00:32:47,160
...for funding curiosity-based research, advanced research at the universities, and then not

501
00:32:47,160 --> 00:32:52,760
to cut, not to, you know, say that companies don't have any role, not to say that government

502
00:32:52,760 --> 00:32:53,840
has no role.

503
00:32:53,840 --> 00:33:02,360
They both have roles, but they're different roles, and companies tend to be better at

504
00:33:02,360 --> 00:33:08,760
development, better at producing products and tapping into things that can, within

505
00:33:08,760 --> 00:33:15,360
a year or two or three, can be a product that will be useful.

506
00:33:15,360 --> 00:33:18,440
Scientists at universities don't have that constraint.

507
00:33:18,440 --> 00:33:21,440
They don't have to worry about when is this going to be...

508
00:33:21,440 --> 00:33:22,440
Commercial.

509
00:33:22,440 --> 00:33:23,440
Commercial.

510
00:33:24,040 --> 00:33:35,440
And that has, I think, had such an incalculable effect on the prosperity of this country,

511
00:33:35,440 --> 00:33:40,600
on the fact that we are the leader in every technology field.

512
00:33:40,600 --> 00:33:44,760
It's not an accident that we're the leader in every technology field.

513
00:33:44,760 --> 00:33:45,840
We didn't used to be.

514
00:33:45,840 --> 00:33:51,480
And does it affect your argument if I add it also enabled us or contributed to a victory

515
00:33:51,480 --> 00:33:52,480
in the Cold War?

516
00:33:53,480 --> 00:33:56,480
The weapons systems that came out of universities?

517
00:33:56,480 --> 00:33:57,480
All right.

518
00:33:57,480 --> 00:33:58,480
Well, no, absolutely.

519
00:33:58,480 --> 00:34:02,480
And, you know, President Reagan and Star Wars.

520
00:34:02,480 --> 00:34:06,240
In other words, it ended up being a defensive, kind of good, you could argue from all kinds

521
00:34:06,240 --> 00:34:09,720
of points of view as it was a good ROI for taxpayers' money.

522
00:34:09,720 --> 00:34:10,720
Yeah.

523
00:34:10,720 --> 00:34:11,720
All right.

524
00:34:11,720 --> 00:34:16,680
So we're not arguing for higher salaries for faculty or anything of that sort.

525
00:34:16,680 --> 00:34:26,160
But we think, particularly in AI, it's gotten to the point where scientists at universities

526
00:34:26,160 --> 00:34:33,920
can no longer play in the game because of the cost of the computing, the cost, the inaccessibility

527
00:34:33,920 --> 00:34:35,480
of the data.

528
00:34:35,480 --> 00:34:38,600
That's why you see all of these developments coming out of companies.

529
00:34:38,600 --> 00:34:39,600
That's great.

530
00:34:39,600 --> 00:34:41,440
Those are great developments.

531
00:34:41,440 --> 00:34:50,240
But we need to have also people who are exploring these technologies without looking at the

532
00:34:50,240 --> 00:34:54,480
product, without being driven by the profit motive.

533
00:34:54,480 --> 00:34:59,360
And then eventually, hopefully, they will develop discoveries, they will make discoveries,

534
00:34:59,360 --> 00:35:01,360
and we'll then be commercializable.

535
00:35:01,360 --> 00:35:02,360
Okay.

536
00:35:02,360 --> 00:35:06,240
I noticed in your book, Fei-Fei, I was very struck that you said, oh, I think it was about

537
00:35:06,240 --> 00:35:12,600
a decade ago, 2015, I think, was that you noticed that you were beginning to lose colleagues

538
00:35:12,600 --> 00:35:18,440
to the private sector, presumably because they just pay so phenomenally well around

539
00:35:18,440 --> 00:35:19,440
here in Silicon Valley.

540
00:35:19,440 --> 00:35:25,280
But then there's also the point that to get to make progress in AI, you need an enormous

541
00:35:25,280 --> 00:35:27,600
amount of computational power.

542
00:35:27,600 --> 00:35:31,840
And assembling all those ones and zeros is extremely expensive.

543
00:35:31,840 --> 00:35:35,600
So ChatGPT, what is the parent company?

544
00:35:36,320 --> 00:35:37,320
Open AI.

545
00:35:37,320 --> 00:35:42,400
Open AI got started with an initial investment of a billion dollars.

546
00:35:42,400 --> 00:35:47,360
And friends and family capital of a billion dollars is a lot of money even around here.

547
00:35:47,360 --> 00:35:48,360
Okay.

548
00:35:48,360 --> 00:35:49,360
That's the point you're making.

549
00:35:49,360 --> 00:35:50,360
Yes.

550
00:35:50,360 --> 00:35:52,860
All right.

551
00:35:52,860 --> 00:35:56,960
It feels to me as though every one of these topics is worth a day long seminar.

552
00:35:56,960 --> 00:35:59,320
Actually, I think that they are.

553
00:35:59,320 --> 00:36:07,040
And by the way, this has happened before where the science has become so expensive that it

554
00:36:07,040 --> 00:36:12,640
could no longer, that university level research and researchers could no longer afford to

555
00:36:12,640 --> 00:36:14,460
do the science.

556
00:36:14,460 --> 00:36:17,800
It happened in high-energy physics.

557
00:36:17,800 --> 00:36:24,240
High-energy physics used to mean you had a Vandegraaff generator in your office, and

558
00:36:24,240 --> 00:36:29,120
that was your accelerator, or you could do what you needed to do.

559
00:36:29,720 --> 00:36:36,480
And then it no longer was, you know, the energy levels were higher and higher.

560
00:36:36,480 --> 00:36:37,480
And what happened?

561
00:36:37,480 --> 00:36:42,000
Well, the federal government stepped in and said, we're going to help.

562
00:36:42,000 --> 00:36:45,120
We're going to build an accelerator.

563
00:36:45,120 --> 00:36:46,120
Stanford linear accelerator.

564
00:36:46,120 --> 00:36:47,120
Stanford linear accelerator.

565
00:36:47,120 --> 00:36:48,120
Exactly.

566
00:36:48,120 --> 00:36:51,640
Sandia Labs, Lawrence Livermore, all these are at least in part federal established.

567
00:36:51,640 --> 00:36:52,640
CERN.

568
00:36:52,640 --> 00:36:53,840
CERN, which is European.

569
00:36:53,840 --> 00:36:54,840
Right.

570
00:36:54,840 --> 00:36:55,840
Well, Fermilab.

571
00:36:55,840 --> 00:37:03,240
The first accelerator was Slack, Stanford linear accelerator center, then Fermilab, and so

572
00:37:03,240 --> 00:37:05,240
on and so forth.

573
00:37:05,240 --> 00:37:11,760
CERN is actually late in the game, and it's European consortium.

574
00:37:11,760 --> 00:37:21,800
But the thing is, we could not continue the science without the help of the government

575
00:37:21,800 --> 00:37:22,800
and government.

576
00:37:22,920 --> 00:37:30,160
There is another, and then in addition to high energy physics and then bio, right, especially

577
00:37:30,160 --> 00:37:36,080
with genetic sequencing and high throughput genomics, and biotech is also changing.

578
00:37:36,080 --> 00:37:44,840
And now you see a new wave of biology labs that are actually heavily funded by the combination

579
00:37:44,840 --> 00:37:52,480
of government and philanthropy and all that, and that stepped in to, you know, supplement

580
00:37:52,480 --> 00:37:55,160
what the traditional university model is.

581
00:37:55,160 --> 00:37:58,360
And so we're now here with AI and computer science.

582
00:37:58,360 --> 00:37:59,360
Okay.

583
00:37:59,360 --> 00:38:05,480
This is, we have to do another show on that one alone, I think.

584
00:38:05,480 --> 00:38:06,480
The Singularity.

585
00:38:06,480 --> 00:38:08,480
Oh, good.

586
00:38:08,480 --> 00:38:09,480
This is good.

587
00:38:09,480 --> 00:38:10,480
Reassuring.

588
00:38:10,480 --> 00:38:11,480
You're both, I mean, rolling your eyes.

589
00:38:11,480 --> 00:38:12,480
Wonderful.

590
00:38:12,480 --> 00:38:14,480
I feel better about this already.

591
00:38:14,480 --> 00:38:15,480
Good.

592
00:38:15,480 --> 00:38:16,480
Ray Kurzweil.

593
00:38:16,480 --> 00:38:17,480
You know exactly where this is going.

594
00:38:17,480 --> 00:38:19,120
Ray Kurzweil writes a book in 2005.

595
00:38:19,120 --> 00:38:23,400
This gets everybody's attention and still scares lots of people to death, including

596
00:38:23,400 --> 00:38:24,400
me.

597
00:38:24,400 --> 00:38:30,240
The book is called The Singularity is Near, and Kurzweil predicts a singularity that

598
00:38:30,240 --> 00:38:37,480
will involve, and I'm quoting him, the merger of human technology with human intelligence.

599
00:38:37,480 --> 00:38:41,040
He's not saying the tech will mimic more and more closely human intelligence.

600
00:38:41,040 --> 00:38:43,200
He is saying they will merge.

601
00:38:43,200 --> 00:38:47,600
I set the date for the singularity representing a profound and disruptive transformation in

602
00:38:47,600 --> 00:38:50,880
human capability as 2045.

603
00:38:50,880 --> 00:38:52,880
Okay.

604
00:38:52,880 --> 00:38:53,880
That's the first quotation.

605
00:38:53,880 --> 00:38:54,880
Here's the second.

606
00:38:54,880 --> 00:38:59,280
And this comes from the Stanford Course Catalog's description of the philosophy of artificial

607
00:38:59,280 --> 00:39:06,640
intelligence, a freshman seminar that was taught last quarter, as I recall, by one John

608
00:39:06,640 --> 00:39:07,640
Echamendi.

609
00:39:07,640 --> 00:39:14,120
Here, here's from the description, is it really possible for an artificial system to

610
00:39:14,120 --> 00:39:18,880
achieve genuine intelligence, thoughts, consciousness, emotions?

611
00:39:18,880 --> 00:39:20,880
What would that mean?

612
00:39:20,880 --> 00:39:23,120
John, is it possible?

613
00:39:23,120 --> 00:39:26,840
What would it mean?

614
00:39:26,840 --> 00:39:30,240
I think the answer is actually no.

615
00:39:30,240 --> 00:39:33,840
And thank goodness, you kept me waiting for a moment.

616
00:39:33,840 --> 00:39:45,800
I think the fantasies that Ray Kurzweil and others have been spinning up, I guess that's

617
00:39:45,800 --> 00:39:56,720
the way to put it, stem from a lack of understanding of how the human being really works and don't

618
00:39:56,720 --> 00:40:06,600
understand how crucial biology is to the way we work, the way we are motivated, how we

619
00:40:06,600 --> 00:40:14,240
get desires, how we get goals, how we become humans, become people.

620
00:40:14,240 --> 00:40:23,360
And what AI has done so far, AI is capturing what you might think of as the information

621
00:40:23,360 --> 00:40:27,800
processing piece of what we do.

622
00:40:27,800 --> 00:40:30,800
So part of what we do is information processing.

623
00:40:30,800 --> 00:40:35,120
So it's got the right frontal cortex, but it hasn't got the left frontal cortex yet?

624
00:40:35,120 --> 00:40:37,480
Yeah, it's an oversimplification, but yes.

625
00:40:37,480 --> 00:40:40,000
Imagine that on television.

626
00:40:40,000 --> 00:40:52,400
So I actually think it is, first of all, the date, 2045, is insane.

627
00:40:53,400 --> 00:40:56,840
And secondly, it's not even clear to me that we will ever go back.

628
00:40:56,840 --> 00:40:59,080
Wait, I can't believe I'm saying this.

629
00:40:59,080 --> 00:41:06,320
In his defense, I don't think he's saying that 2045 is the day that the machines become

630
00:41:06,320 --> 00:41:10,000
conscious beings like humans.

631
00:41:10,000 --> 00:41:18,960
It's more an inflection point of the power of the technology that is disrupting the society.

632
00:41:18,960 --> 00:41:19,960
Well, that's in his late.

633
00:41:19,960 --> 00:41:20,960
He's late.

634
00:41:20,960 --> 00:41:21,960
We're already there.

635
00:41:21,960 --> 00:41:23,240
That's what I'm saying.

636
00:41:23,240 --> 00:41:30,120
I think you're being overly generous.

637
00:41:30,120 --> 00:41:35,200
But he means by the singularity is the date at which we create an artificial intelligence

638
00:41:35,200 --> 00:41:43,480
system that can improve itself and then get into a cycle, a recursive cycle, where it

639
00:41:43,480 --> 00:41:46,880
becomes a superintelligence.

640
00:41:46,880 --> 00:41:48,640
And I deny that.

641
00:41:48,640 --> 00:41:51,940
He's playing the 2001 Space Odyssey game here.

642
00:41:51,940 --> 00:41:54,940
And it's a different question, but related question.

643
00:41:54,940 --> 00:42:00,820
In some ways, this is a more serious question, I think, although that's serious too.

644
00:42:00,820 --> 00:42:09,020
Here's the late Henry Kissinger again, quote, we live in a world which has no philosophy.

645
00:42:09,020 --> 00:42:15,980
There is no dominant philosophical view, so the technologists can run wild.

646
00:42:15,980 --> 00:42:20,660
They can develop world-changing things, and there's nobody to say, we've got to integrate

647
00:42:20,740 --> 00:42:22,780
this into something.

648
00:42:22,780 --> 00:42:26,460
All right, I'm going to put it crudely again.

649
00:42:26,460 --> 00:42:32,700
But in China, a century ago, we still had Confucian thought, dominant at least among

650
00:42:32,700 --> 00:42:37,700
the educated classes on my very thin understanding of Chinese history.

651
00:42:37,700 --> 00:42:43,660
In this country, until the day before yesterday, we still spoke without irony of the Judeo-Christian

652
00:42:43,660 --> 00:42:52,140
tradition, which involved certain concepts about morality, what it meant to be human.

653
00:42:52,140 --> 00:42:56,700
It assumed a belief in God, but it turned out you could actually get pretty far along,

654
00:42:56,700 --> 00:42:59,260
even if you didn't believe in OK.

655
00:42:59,260 --> 00:43:02,740
And Kissinger is now saying, it's all fallen apart.

656
00:43:02,740 --> 00:43:05,760
There is no dominant philosophy.

657
00:43:05,760 --> 00:43:08,020
This is a serious problem, is it not?

658
00:43:08,020 --> 00:43:11,540
There's nothing to integrate AI into.

659
00:43:11,540 --> 00:43:14,980
You take his point.

660
00:43:14,980 --> 00:43:15,980
It's up to the children.

661
00:43:15,980 --> 00:43:18,980
You're the philosopher.

662
00:43:18,980 --> 00:43:23,020
You're the philosopher.

663
00:43:23,020 --> 00:43:26,580
I think this is a great, first of all, thank you for that quote.

664
00:43:26,580 --> 00:43:30,260
I didn't read that quote from Henry Kissinger.

665
00:43:30,260 --> 00:43:34,100
I mean, this is why we founded the Human Center AI Institute.

666
00:43:34,100 --> 00:43:39,500
These are the fundamental questions that our generation needs to figure out.

667
00:43:39,860 --> 00:43:40,860
That's not just a question.

668
00:43:40,860 --> 00:43:41,860
That's the question.

669
00:43:41,860 --> 00:43:43,780
It was one of the fundamental questions.

670
00:43:43,780 --> 00:43:48,780
It's also one of the fundamental questions that illustrates why universities are still

671
00:43:48,780 --> 00:43:52,180
relevant today.

672
00:43:52,180 --> 00:43:58,140
And Peter, one of the things that Henry Kissinger says in that quote is that there is no dominant

673
00:43:58,140 --> 00:43:59,140
philosophy.

674
00:43:59,140 --> 00:44:05,780
There's no one dominant philosophy like the Judeo-Christian tradition, which used to be

675
00:44:05,780 --> 00:44:08,060
the dominant tradition in the US.

676
00:44:08,100 --> 00:44:11,300
It's a different conversation in Paris in the 12th century, for example, the University

677
00:44:11,300 --> 00:44:12,300
of Paris.

678
00:44:12,300 --> 00:44:20,460
In order to take values into account when you're creating an AI system, you don't need

679
00:44:20,460 --> 00:44:25,340
a dominant tradition.

680
00:44:25,340 --> 00:44:31,060
What you need, for example, for most ethical traditions is the Golden Rule.

681
00:44:31,060 --> 00:44:33,180
Go back to the Confucius.

682
00:44:33,180 --> 00:44:35,980
We can still get along with each other.

683
00:44:35,980 --> 00:44:40,100
Even when it comes to deep, deep questions of value such as this, we still have enough

684
00:44:40,100 --> 00:44:41,820
common ground.

685
00:44:41,820 --> 00:44:44,820
I believe so.

686
00:44:44,820 --> 00:44:46,980
I heave yet another sigh of relief.

687
00:44:46,980 --> 00:44:49,340
Okay, let's talk a little bit.

688
00:44:49,340 --> 00:44:53,340
We're talking a little bit about a lot of things here, but so it is.

689
00:44:53,340 --> 00:44:57,260
Let us speak of many things as it is written in Alice in Wonderland.

690
00:44:57,260 --> 00:44:59,580
The Stanford Institute.

691
00:44:59,580 --> 00:45:03,940
The Stanford Institute for Human-Centered Artificial Intelligence, of which you are

692
00:45:03,980 --> 00:45:08,940
co-directors, and I just have two questions and respond as you'd like.

693
00:45:08,940 --> 00:45:16,220
Can you give me some taste, some feel for what you're doing now, and in some ways more

694
00:45:16,220 --> 00:45:20,780
important, but more elusive, where you'd like to be in just five years, say.

695
00:45:20,780 --> 00:45:21,780
Everything in this field is moving.

696
00:45:21,780 --> 00:45:25,220
So I would, my impulse is to say 10 years because it's a rounder number.

697
00:45:25,220 --> 00:45:27,420
It's too far off in this field.

698
00:45:27,420 --> 00:45:28,420
Fei-Fei.

699
00:45:28,420 --> 00:45:33,300
I think what really has happened in the past five years by Stanford High, among many

700
00:45:33,380 --> 00:45:34,380
things.

701
00:45:34,380 --> 00:45:36,460
I just want to make sure everybody following you.

702
00:45:36,460 --> 00:45:39,540
H-A-I, Stanford High is the way it's known on this campus.

703
00:45:39,540 --> 00:45:40,540
Yes.

704
00:45:40,540 --> 00:45:41,540
Go ahead.

705
00:45:41,540 --> 00:45:42,540
Yeah.

706
00:45:42,540 --> 00:45:48,420
Is that we have put a stick on the ground for Stanford as well as for everybody that

707
00:45:48,420 --> 00:45:56,420
this is an interdisciplinary study that AI, artificial intelligence, is a science of

708
00:45:56,420 --> 00:45:57,500
its own.

709
00:45:57,500 --> 00:46:04,940
It's a powerful tool, and what happens is that you can welcome so many disciplines to

710
00:46:04,940 --> 00:46:11,900
cross-pollinate around the topic of AI or use the tools of AI to make other sciences

711
00:46:11,900 --> 00:46:15,300
happen or to explore other new ideas.

712
00:46:15,300 --> 00:46:23,260
And that concept of making this an interdisciplinary and multidisciplinary field is what I think

713
00:46:23,260 --> 00:46:28,140
Stanford High brought to Stanford and also hopefully to the world.

714
00:46:28,140 --> 00:46:33,540
Because like you said, computer science is kind of a new field, you know, only, you know,

715
00:46:33,540 --> 00:46:39,620
the late John McCarthy coined the term, you know, in the late fifties.

716
00:46:39,620 --> 00:46:41,260
Now it's moving so fast.

717
00:46:41,260 --> 00:46:47,380
Everybody feels it's just a niche computer science field that's just like making its

718
00:46:47,380 --> 00:46:48,820
way into the future.

719
00:46:48,820 --> 00:46:52,340
And we're saying, no, look abroad.

720
00:46:52,340 --> 00:46:54,980
There's so many disciplines that can be put here.

721
00:46:54,980 --> 00:46:57,900
Who competes with the Stanford Institute and Human-Centered Design?

722
00:46:57,900 --> 00:47:00,380
Is there such an institute at Harvard or Oxford or Beijing?

723
00:47:00,380 --> 00:47:01,980
I just don't know what the...

724
00:47:01,980 --> 00:47:02,980
Oh, thank you.

725
00:47:02,980 --> 00:47:08,660
So in the five years since we launched, there have been a number of similar institutes that

726
00:47:08,660 --> 00:47:11,780
have been created at other universities.

727
00:47:11,780 --> 00:47:14,260
We don't see that as competition in any way.

728
00:47:14,260 --> 00:47:17,060
If these arguments you've been making are valid, then we need them.

729
00:47:17,060 --> 00:47:18,060
We should welcome them.

730
00:47:18,300 --> 00:47:19,300
As a movement.

731
00:47:19,300 --> 00:47:20,300
We need them.

732
00:47:20,300 --> 00:47:23,820
And part of what we want to do and part of what I think we've succeeded to a certain

733
00:47:23,820 --> 00:47:33,500
extent doing is communicating this vision of the importance of keeping the human and

734
00:47:33,500 --> 00:47:41,780
human values at the center when we are developing this technology, when we are applying this

735
00:47:41,780 --> 00:47:44,420
technology.

736
00:47:44,420 --> 00:47:47,620
And we want to communicate that to the world.

737
00:47:47,620 --> 00:47:53,060
We want other centers that adopt a similar standpoint.

738
00:47:53,060 --> 00:47:59,380
And importantly, one of the things that I didn't mention is one of the things we try

739
00:47:59,380 --> 00:48:08,500
to do is educate and educate, for example, legislators so that they understand what this

740
00:48:08,500 --> 00:48:12,780
technology is, what it can do, what it can't do.

741
00:48:12,780 --> 00:48:17,380
So you're traveling to Washington or the very generous trustees of this institution

742
00:48:17,380 --> 00:48:22,260
are bringing congressional staff and they're both, both are happening.

743
00:48:22,260 --> 00:48:28,420
So are you, first of all, did you teach that course in Stanford HAI or was the course located

744
00:48:28,420 --> 00:48:30,260
in the philosophy department or cross-list?

745
00:48:30,260 --> 00:48:33,220
I'm just trying to get a feel for what's actually taking place there now.

746
00:48:33,220 --> 00:48:34,220
Yeah.

747
00:48:34,220 --> 00:48:38,620
I actually taught it in the confines of the HAI building.

748
00:48:38,620 --> 00:48:39,620
Okay.

749
00:48:39,620 --> 00:48:40,620
So it's an HAI.

750
00:48:40,620 --> 00:48:41,620
No, it's a philosophy.

751
00:48:41,620 --> 00:48:45,540
It's listed as a philosophy course, but taught in the HAI.

752
00:48:45,540 --> 00:48:46,540
He's the former provost.

753
00:48:46,540 --> 00:48:49,700
He gets to, he's an interdisciplinary walking wonder.

754
00:48:49,700 --> 00:48:57,980
And your work in AI assisted healthcare, is that taking place in HAI or is it at the medical

755
00:48:57,980 --> 00:48:58,980
school?

756
00:48:58,980 --> 00:48:59,980
Well, that's the beauty.

757
00:48:59,980 --> 00:49:05,300
It's taking place in HAI, computer science department, the medical school, even has collaborators

758
00:49:05,300 --> 00:49:09,540
from the law school, from the political science department.

759
00:49:09,540 --> 00:49:10,540
So that's the beauty.

760
00:49:10,620 --> 00:49:13,220
It's deeply interdisciplinary.

761
00:49:13,220 --> 00:49:16,780
If I were the provost, I'd say this is starting to sound like something that's about to run

762
00:49:16,780 --> 00:49:17,780
a muck.

763
00:49:17,780 --> 00:49:20,220
Doesn't that sound a little too interdisciplinary, John?

764
00:49:20,220 --> 00:49:23,180
Don't we need to define things a little bit here?

765
00:49:23,180 --> 00:49:26,180
Let me, let me tell you, let me say something.

766
00:49:26,180 --> 00:49:32,580
So Steve Denning, who was the chair of our board of trustees for many years and has been

767
00:49:32,580 --> 00:49:38,740
a long, long time supporter of the university in many, many ways.

768
00:49:38,740 --> 00:49:46,260
In fact, we are the Denning co-directors of Stanford HAI, Stanford HAI.

769
00:49:46,260 --> 00:49:55,540
Steve saw five, six years ago, he said, you know, AI is going to impact in a free department

770
00:49:55,540 --> 00:49:58,620
at this university.

771
00:49:58,620 --> 00:50:05,500
And we need to have an institute that makes sure that that happens the right way, that

772
00:50:05,620 --> 00:50:10,620
that impact is, is, does not run a muck.

773
00:50:10,620 --> 00:50:14,180
Where would you like to be in five years?

774
00:50:14,180 --> 00:50:17,060
What's a, what's a course you'd like to be teaching in five years?

775
00:50:17,060 --> 00:50:19,340
What's a, what's a special project?

776
00:50:19,340 --> 00:50:25,140
I would like to teach a course, freshman seminar called The Greatest Discoveries by AI.

777
00:50:25,140 --> 00:50:27,340
Oh, really?

778
00:50:27,340 --> 00:50:30,340
Okay.

779
00:50:30,340 --> 00:50:35,460
A last question, which I have one last question, but that does not.

780
00:50:35,460 --> 00:50:38,580
That means that it has, you, each of you has to hold yourself to one last answer because

781
00:50:38,580 --> 00:50:41,860
it's a kind of open-ended question.

782
00:50:41,860 --> 00:50:46,740
I have a theory, but all I do is wander around this campus.

783
00:50:46,740 --> 00:50:50,580
The two of you are deeply embedded here and you ran the place for 17 years, so you'll

784
00:50:50,580 --> 00:50:52,380
know more than I will.

785
00:50:52,380 --> 00:50:56,100
Including you may know that my theory is wrong, but I'm going to trot it out, modest

786
00:50:56,100 --> 00:51:00,460
though it may be, even so.

787
00:51:00,460 --> 00:51:05,140
Milton Friedman, the late Milton Friedman, who when I first arrived here was a colleague

788
00:51:05,140 --> 00:51:06,140
at the Hoover Institution.

789
00:51:06,140 --> 00:51:10,700
In fact, by some miracle, his office was on the same hallway as mine and I used to stop

790
00:51:10,700 --> 00:51:13,580
in on him from time to time.

791
00:51:13,580 --> 00:51:20,340
He told me that he went into economics because he grew up during the Depression and the overriding

792
00:51:20,340 --> 00:51:27,580
question in the country at that time was how do we satisfy our material needs?

793
00:51:27,580 --> 00:51:29,540
There were millions of people without jobs.

794
00:51:29,540 --> 00:51:33,420
There really were people who had trouble feeding their families.

795
00:51:33,420 --> 00:51:35,140
All right.

796
00:51:35,140 --> 00:51:39,860
I think of my own generation, which is more or less John's generation.

797
00:51:39,860 --> 00:51:41,820
You come much later, Faye Faye.

798
00:51:41,820 --> 00:51:42,820
Thank you.

799
00:51:42,820 --> 00:51:48,180
And for us, I don't know what kind of discussions you had in the dorm room, but when I was in

800
00:51:48,180 --> 00:51:52,620
college, there were both sessions about the Cold War, where the Russians...

801
00:51:52,620 --> 00:51:55,940
The Cold War was real to our generation.

802
00:51:55,940 --> 00:51:59,540
That was the overriding question.

803
00:51:59,540 --> 00:52:01,180
How can we defend our way of life?

804
00:52:01,180 --> 00:52:04,180
How can we defend our fundamental principles?

805
00:52:04,180 --> 00:52:05,700
All right.

806
00:52:05,700 --> 00:52:07,980
Here's my theory.

807
00:52:07,980 --> 00:52:16,620
For current students, they've grown up in a period of unimaginable prosperity.

808
00:52:16,620 --> 00:52:19,860
Material needs are just not the problem.

809
00:52:19,860 --> 00:52:24,540
They have also grown up during a period of relative peace.

810
00:52:24,540 --> 00:52:26,020
The Cold War ended.

811
00:52:26,020 --> 00:52:27,300
You could put different...

812
00:52:27,300 --> 00:52:31,260
The Soviet Union declared itself defunct in 1991.

813
00:52:31,260 --> 00:52:35,300
Cold War is over at that moment of the latest.

814
00:52:35,300 --> 00:52:41,060
The overriding question for these kids today is meaning.

815
00:52:41,060 --> 00:52:43,860
What is it all for?

816
00:52:43,860 --> 00:52:45,700
Why are we here?

817
00:52:45,700 --> 00:52:48,140
What does it mean to be human?

818
00:52:48,140 --> 00:52:53,140
What's the difference between us and the machines?

819
00:52:53,380 --> 00:53:00,940
If my little theory is correct, then by some miracle, this technological marvel that you

820
00:53:00,940 --> 00:53:06,580
have produced will lead to a new flowering of the humanities.

821
00:53:06,580 --> 00:53:12,460
Do you go for that, John?

822
00:53:12,460 --> 00:53:13,460
Do I go for it?

823
00:53:13,460 --> 00:53:17,060
I would go for it if it were going to happen.

824
00:53:17,060 --> 00:53:19,180
Did I put that in a slightly sloppy way?

825
00:53:19,180 --> 00:53:21,500
No.

826
00:53:21,500 --> 00:53:23,260
I think it would be wonderful.

827
00:53:23,260 --> 00:53:27,980
It's something to hope for.

828
00:53:27,980 --> 00:53:31,060
Now I'm going to be the cynic.

829
00:53:31,060 --> 00:53:37,100
So far, what I see in students is more and more focus, or Stanford students, more and

830
00:53:37,100 --> 00:53:41,380
more focus on technology, on learning.

831
00:53:41,380 --> 00:53:47,020
Computer science is still the biggest major at this university.

832
00:53:47,020 --> 00:53:48,980
We have tried at HAI.

833
00:53:48,980 --> 00:53:56,180
We have actually started a program called Embedded Ethics, where the CS at the end of

834
00:53:56,180 --> 00:54:02,540
ethics is capitalized, so it's computer science.

835
00:54:02,540 --> 00:54:04,060
That'll catch the kids' attention.

836
00:54:04,060 --> 00:54:07,500
No, we don't have to catch their attention.

837
00:54:07,500 --> 00:54:15,620
What we do is virtually all of the courses in computer science, the introductory courses,

838
00:54:15,620 --> 00:54:19,020
have ethics components built in.

839
00:54:19,020 --> 00:54:24,900
So a problem set, so you have a problem set this week, and that'll have a whole bunch

840
00:54:24,900 --> 00:54:32,460
of very difficult math problems, computer science problem, and then it will have a very

841
00:54:32,460 --> 00:54:34,460
difficult ethical challenge.

842
00:54:34,460 --> 00:54:37,980
It'll say, here's the situation.

843
00:54:37,980 --> 00:54:46,400
You are programming a computer, a programming an AI system, and here's the dilemma.

844
00:54:46,400 --> 00:54:47,900
Now discuss.

845
00:54:47,900 --> 00:54:49,500
What are you going to do?

846
00:54:49,500 --> 00:54:54,140
So we're trying to bring, and this is what they wanted.

847
00:54:54,140 --> 00:55:00,820
We're trying to bring ethics within the last couple of years, okay, two, three years.

848
00:55:00,820 --> 00:55:08,620
We're trying to bring the attention to ethics into the computer science curriculum.

849
00:55:08,620 --> 00:55:14,680
And partly that's because they're not, I mean, students tend to follow the path of least

850
00:55:14,680 --> 00:55:15,680
resistance.

851
00:55:15,680 --> 00:55:19,380
Well, they also, let's put it, again, if I'm saying things crudely again and again,

852
00:55:19,380 --> 00:55:22,580
but someone must say it, they follow the money.

853
00:55:22,580 --> 00:55:29,420
So as long as this valley that surrounds us rewards brilliant young kids from Stanford

854
00:55:29,540 --> 00:55:35,440
with CS degrees as richly as it does, and it is amazingly richly, they'll go get CS

855
00:55:35,440 --> 00:55:36,940
degrees, right?

856
00:55:36,940 --> 00:55:44,020
Well, I do think it's a little crude.

857
00:55:44,020 --> 00:55:54,700
I think money is one surrogate measure of also what is advancing in our time.

858
00:55:54,700 --> 00:56:02,300
Maybe right now truly is one of the biggest drivers of the changes of our civilization.

859
00:56:02,300 --> 00:56:07,540
When you're talking about what is this generation of students talk about, I was just thinking

860
00:56:07,540 --> 00:56:13,900
that 400 years ago, when the scientific revolution was happening, what is in the dorms, of course

861
00:56:13,900 --> 00:56:21,100
it's all young men in Cambridge or Oxford, but that must also be a very exciting and

862
00:56:21,100 --> 00:56:22,100
interesting time.

863
00:56:22,100 --> 00:56:27,420
Of course, there was an internet and social media to propel the travel of the knowledge,

864
00:56:27,420 --> 00:56:36,900
but imagine there was the blossoming of discovery and of our understanding of the physical world.

865
00:56:36,900 --> 00:56:42,300
Right now we're in that kind of great era of technological blossoming.

866
00:56:42,300 --> 00:56:44,020
It's a digital revolution.

867
00:56:44,020 --> 00:56:50,740
So the conversations in the dorm, I think it's a blend of the meaning of who we are

868
00:56:50,780 --> 00:56:55,900
as humans, as well as our relationship to these technologies we're building.

869
00:56:55,900 --> 00:56:57,980
And so it's a...

870
00:56:57,980 --> 00:57:08,740
So properly taught technology can subsume or embed philosophy literature?

871
00:57:08,740 --> 00:57:12,140
Of course, can inspire, can inspire.

872
00:57:12,140 --> 00:57:16,540
And also think about it, what follows scientific revolution is a great period of change of

873
00:57:16,540 --> 00:57:19,900
political, social, economical change, right?

874
00:57:20,100 --> 00:57:21,100
We're seeing that.

875
00:57:21,100 --> 00:57:22,100
Not all for the better.

876
00:57:22,100 --> 00:57:23,100
Right.

877
00:57:23,100 --> 00:57:29,380
And I'm not saying it's necessary for the better, but we are seeing, we're having even

878
00:57:29,380 --> 00:57:35,220
peaked the digital revolution, but we're already seeing the political, social, economic changes.

879
00:57:35,220 --> 00:57:41,700
So this is, again, back to Stanford High when we founded it five years ago.

880
00:57:41,700 --> 00:57:47,780
We believe all this is happening and this is an institute where these kind of conversations,

881
00:57:47,780 --> 00:57:55,140
ideas, debates should be taking place and education programs should be happening.

882
00:57:55,140 --> 00:57:58,180
And that's part of the reason why we did this.

883
00:57:58,180 --> 00:58:05,580
Let me tell you, yeah, so as you pointed out, I just finished teaching a course called Philosophy

884
00:58:05,580 --> 00:58:09,980
of Artificial Intelligence, about which I found out too late, I would have asked permission

885
00:58:09,980 --> 00:58:11,260
to audit your course, John.

886
00:58:11,260 --> 00:58:14,060
No, no, you're too old.

887
00:58:14,060 --> 00:58:20,140
So and about half of the students were computer science students who were planned to be computer

888
00:58:20,140 --> 00:58:22,260
science majors.

889
00:58:22,260 --> 00:58:30,420
Another quarter planned to be symbolic systems majors, which is a major that is related to

890
00:58:30,420 --> 00:58:36,380
computer science, and then there was a smattering of others.

891
00:58:36,380 --> 00:58:41,660
And these were people, every one of them at the end of the course, and I'm not saying

892
00:58:41,660 --> 00:58:46,900
this to brag, every one of them said, this is the best course we've ever taken.

893
00:58:46,900 --> 00:58:49,340
And why did they say that?

894
00:58:49,340 --> 00:58:53,220
It inspired, it made them think.

895
00:58:53,220 --> 00:58:59,820
It gave them a framework for thinking, a framework for trying to address some of these problems,

896
00:58:59,820 --> 00:59:03,220
some of the worries that you've brought out today.

897
00:59:03,220 --> 00:59:10,820
And how do we think about them and how do we not just become panicked because of some

898
00:59:10,860 --> 00:59:18,020
science fiction movie that we've seen, or because we read Ray Kurzweil.

899
00:59:18,020 --> 00:59:20,700
So maybe it's just as well I didn't take the course.

900
00:59:20,700 --> 00:59:24,580
I'm sure John would have given me a C-minus at best.

901
00:59:24,580 --> 00:59:27,020
Great inflation.

902
00:59:27,020 --> 00:59:40,700
So it's clear that these kids, the students, are looking for the opening to think the

903
00:59:40,740 --> 00:59:48,740
things and to understand how to address ethical questions, how to address hard philosophical

904
00:59:48,740 --> 00:59:55,340
questions, and that's what they got out of the course.

905
00:59:55,340 --> 00:59:58,460
And that's a way of looking for meaning in this time.

906
00:59:58,460 --> 01:00:00,460
Yes it is.

907
01:00:00,460 --> 01:00:06,420
Dr. Feifei Li and Dr. John Etchamendi, both of the Stanford Institute for Human-Centered

908
01:00:06,420 --> 01:00:08,260
Artificial Intelligence.

909
01:00:08,260 --> 01:00:09,260
Thank you.

910
01:00:09,260 --> 01:00:10,380
Thank you, Peter.

911
01:00:10,380 --> 01:00:14,660
For Uncommon Knowledge and the Hoover Institution and Fox Nation, I'm Peter Robinson.

