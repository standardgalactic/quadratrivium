1
00:00:00,000 --> 00:00:03,440
The neat thing about working in machine learning is that every few years somebody

2
00:00:03,440 --> 00:00:06,800
invents something crazy that makes you totally reconsider what's possible,

3
00:00:07,360 --> 00:00:11,440
like models that can play go or generate hyper-realistic faces.

4
00:00:12,000 --> 00:00:15,520
And today, the mind-blowing discovery that's rocking everyone's world is a type of neural

5
00:00:15,520 --> 00:00:20,000
network called a transformer. Transformers are models that can translate text,

6
00:00:20,000 --> 00:00:24,480
write poems, and off-eds, and even generate computer code. These have been used in biology

7
00:00:24,480 --> 00:00:29,200
to solve the protein folding problem. Transformers are like this magical machine learning hammer

8
00:00:29,200 --> 00:00:33,520
that seems to make every problem into a nail. If you've heard of the trendy new ML models

9
00:00:33,520 --> 00:00:40,000
BERT or GPT-3 or T5, all of these models are based on transformers. So if you want to stay

10
00:00:40,000 --> 00:00:43,200
hip in machine learning and especially in natural language processing,

11
00:00:43,200 --> 00:00:46,880
you have to know about the transformer. So in this video, I'm going to tell you about what

12
00:00:46,880 --> 00:00:51,680
transformers are, how they work, and why they've been so impactful. Let's get to it.

13
00:00:51,680 --> 00:00:56,880
So what is a transformer? It's a type of neural network architecture. To recap, neural networks

14
00:00:56,880 --> 00:01:02,240
are a very effective type of model for analyzing complicated data types like images, videos, audio,

15
00:01:02,240 --> 00:01:06,160
and text. But there are different types of neural networks optimized for different types of data,

16
00:01:06,160 --> 00:01:10,560
like if you're analyzing images, you typically use a convolutional neural network,

17
00:01:10,560 --> 00:01:14,640
which is designed to vaguely mimic the way that the human brain processes vision.

18
00:01:14,640 --> 00:01:18,720
And since around 2012, neural networks have been really good at solving vision tasks,

19
00:01:18,720 --> 00:01:23,680
like identifying objects and photos. But for a long time, we didn't have anything comparably good

20
00:01:23,680 --> 00:01:28,320
for analyzing language, whether for translation or text summarization or text generation.

21
00:01:28,880 --> 00:01:32,160
And this is a problem because language is the primary way that humans communicate.

22
00:01:32,160 --> 00:01:36,240
You see, until transformers came around, the way we used deep learning to understand text

23
00:01:36,240 --> 00:01:41,440
was with a type of model called a recurrent neural network, or an RNN. That looks something like this.

24
00:01:42,960 --> 00:01:45,600
Let's say you wanted to translate a sentence from English to French.

25
00:01:46,160 --> 00:01:50,800
An RNN would take as input an English sentence and process the words one at a time and then

26
00:01:50,800 --> 00:01:56,400
sequentially spit out their French counterparts. The key word here is sequential. In language,

27
00:01:56,400 --> 00:02:01,440
the order of words matters, and you can't just shuffle them around. For example, the sentence

28
00:02:01,440 --> 00:02:05,040
Jane went looking for trouble. It means something very different than the sentence

29
00:02:05,040 --> 00:02:09,360
trouble went looking for Jane. So any model that's going to deal with language has to

30
00:02:09,360 --> 00:02:14,480
capture word order, and recurrent neural networks do this by looking at one word at a time sequentially.

31
00:02:14,480 --> 00:02:19,440
But RNNs had a lot of problems. First, they never really did well at handling large sequences of

32
00:02:19,440 --> 00:02:24,000
text, like long paragraphs or essays. By the time they were analyzing the end of a paragraph,

33
00:02:24,000 --> 00:02:28,960
they'd forget what happened in the beginning. And even worse, RNNs were pretty hard to train.

34
00:02:28,960 --> 00:02:32,880
Because they processed words sequentially, they couldn't paralyze well, which means that you

35
00:02:32,880 --> 00:02:37,280
couldn't just speed them up by throwing lots of GPUs at them. And when you have a model that's slow

36
00:02:37,280 --> 00:02:41,360
to train, you can't train it on all that much data. This is where the transformer changed

37
00:02:41,360 --> 00:02:45,920
everything. They were a model developed in 2017 by researchers at Google and the University of

38
00:02:45,920 --> 00:02:50,640
Toronto, and they were initially designed to do translation. But unlike recurrent neural networks,

39
00:02:50,640 --> 00:02:54,800
you could really efficiently paralyze transformers. And that meant that with the right hardware,

40
00:02:54,800 --> 00:03:01,040
you could train some really big models. How big? Really big. Remember GPT-3, that model that writes

41
00:03:01,040 --> 00:03:06,640
poetry and code and has conversations? That was trained at almost 45 terabytes of text data,

42
00:03:06,640 --> 00:03:12,480
including, like, almost the entire public web. So if you remember anything about transformers,

43
00:03:12,480 --> 00:03:17,200
let it be this. Combine a model that scales really well with a huge data set, and the results will

44
00:03:17,200 --> 00:03:21,680
probably blow your mind. So how do these things actually work? From the diagram in the paper,

45
00:03:21,680 --> 00:03:27,760
it should be pretty clear. Or maybe not. Actually, it's simpler than you might think. There are

46
00:03:27,760 --> 00:03:32,400
three main innovations that make this model work so well. Positional encodings and attention,

47
00:03:32,400 --> 00:03:37,280
and specifically a type of attention called self-attention. Let's start by talking about the

48
00:03:37,280 --> 00:03:42,000
first one, positional encodings. Let's say we're trying to translate text from English to French.

49
00:03:42,000 --> 00:03:45,680
Positional encodings is the idea that instead of looking at words sequentially,

50
00:03:45,680 --> 00:03:49,040
you take each word in your sentence, and before you feed it into the neural network,

51
00:03:49,040 --> 00:03:54,400
you slap a number on it. One, two, three, depending on what number the word is in the sentence. In

52
00:03:54,400 --> 00:03:58,240
other words, you store information about word order in the data itself rather than in the

53
00:03:58,240 --> 00:04:03,040
structure of the network. Then as you train the network on lots of text data, it learns how to

54
00:04:03,040 --> 00:04:08,320
interpret those positional encodings. In this way, the neural network learns the importance of word

55
00:04:08,320 --> 00:04:13,760
order from the data. This is a high-level way to understand positional encodings, but it's an

56
00:04:13,760 --> 00:04:19,200
innovation that really helped make transformers easier to train than RNNs. The next innovation

57
00:04:19,200 --> 00:04:22,800
in this paper is a concept called attention, which you'll see used everywhere in machine

58
00:04:22,800 --> 00:04:27,600
learning these days. In fact, the title of the original transformer paper is attention is all

59
00:04:27,600 --> 00:04:34,400
you need. So, the agreement on the European Economic Area was signed in August 1992. Did you

60
00:04:34,400 --> 00:04:39,040
know that? That's the example sentence given in the original paper, and remember, the original

61
00:04:39,040 --> 00:04:43,680
transformer was designed for translation. Now imagine trying to translate that sentence to

62
00:04:43,680 --> 00:04:49,360
French. One bad way to translate text is to try to translate each word one for one. But in French,

63
00:04:49,360 --> 00:04:53,520
some words are flipped, like in the French translation, European comes before economic.

64
00:04:54,240 --> 00:04:58,240
Plus, French is a language that has gendered agreement between words, so the word

65
00:04:58,240 --> 00:05:03,520
européenne needs to be in the feminine form to match with la zone. The attention mechanism

66
00:05:03,520 --> 00:05:07,760
is a neural network structure that allows the text model to look at every single word

67
00:05:07,760 --> 00:05:12,400
in the original sentence when making a decision about how to translate a word in the output sentence.

68
00:05:12,400 --> 00:05:16,880
In fact, here's a nice visualization from that paper that shows what words in the input sentence

69
00:05:16,880 --> 00:05:21,120
the model is attending to when it makes predictions about a word for the output sentence.

70
00:05:21,920 --> 00:05:26,800
So, when the model outputs the word européenne, it's looking at the input words,

71
00:05:26,800 --> 00:05:31,600
European, and economic. You can think of this diagram as a sort of heat map for attention.

72
00:05:32,160 --> 00:05:36,400
And how does the model know which words it should be attending to? It's something that's

73
00:05:36,400 --> 00:05:41,520
learned over time from data. By seeing thousands of examples of French and English sentence pairs,

74
00:05:41,520 --> 00:05:46,080
the model learns about gender and word order and plurality and all of that grammatical stuff.

75
00:05:46,080 --> 00:05:50,480
So we talked about two key transformer innovations, positional encoding and attention.

76
00:05:51,040 --> 00:05:55,200
But actually, attention had been invented before this paper. The real innovation in

77
00:05:55,200 --> 00:06:00,880
transformers was something called self-attention, a twist on traditional attention. The type of

78
00:06:00,880 --> 00:06:04,400
attention we just talked about had to do with aligning words in English and French,

79
00:06:04,400 --> 00:06:08,480
which is really important for translation. But what if you're just trying to understand the

80
00:06:08,480 --> 00:06:13,440
underlying meaning in language so that you can build a network that can do any number of language

81
00:06:13,440 --> 00:06:18,960
tasks? What's incredible about neural networks like transformers is that as they analyze tons of

82
00:06:18,960 --> 00:06:24,160
text data, they begin to build up this internal representation or understanding of language

83
00:06:24,160 --> 00:06:29,680
automatically. They may learn, for example, that the words programmer and software engineer and

84
00:06:29,680 --> 00:06:34,400
software developer are all synonymous, and they might also naturally learn the rules of grammar

85
00:06:34,400 --> 00:06:39,040
and gender and tense and so on. The better this internal representation of language the neural

86
00:06:39,040 --> 00:06:44,000
network learns, the better it will be at any language task. And it turns out that attention

87
00:06:44,000 --> 00:06:48,480
can be a very effective way to get a neural network to understand language if it's turned on the

88
00:06:48,480 --> 00:06:54,640
input text itself. Let me give you an example. Take these two sentences. Server, can I have the check?

89
00:06:55,280 --> 00:07:00,800
Versus looks like I just crashed the server. The word server here means two very different things,

90
00:07:00,800 --> 00:07:04,160
and I know that because I'm looking at the context of the surrounding words.

91
00:07:04,960 --> 00:07:09,040
Self-attention allows a neural network to understand a word in the context of the words

92
00:07:09,040 --> 00:07:14,080
around it. So when a model processes the word server in the first sentence, it might be attending

93
00:07:14,080 --> 00:07:19,920
to the word check, which helps it disambiguate from a human server versus a mental one. In the

94
00:07:19,920 --> 00:07:23,680
second sentence, the model might be attending to the word crashed to determine that this server

95
00:07:23,680 --> 00:07:28,480
is a machine. Self-attention can also help neural networks disambiguate words, recognize parts of

96
00:07:28,480 --> 00:07:33,760
speech, and even identify word tense. This in a nutshell is the value of self-attention.

97
00:07:34,480 --> 00:07:40,320
So to summarize, transformers boil down to positional encodings, attention, and self-attention.

98
00:07:41,040 --> 00:07:45,840
Of course, this is a 10,000-foot look at transformers, but how are they actually useful?

99
00:07:45,840 --> 00:07:50,080
One of the most popular transformer-based models is called BERT, which was invented just around

100
00:07:50,080 --> 00:07:55,520
the time that I joined Google in 2018. BERT was trained on a massive text corpus and has become

101
00:07:55,520 --> 00:08:00,480
this sort of general pocket knife for NLP that can be adopted to a bunch of different tasks,

102
00:08:01,120 --> 00:08:05,920
like text summarization, question answering, classification, and finding similar sentences.

103
00:08:06,640 --> 00:08:10,800
It's used in Google Search to help understand search queries, and it powers a lot of Google

104
00:08:10,800 --> 00:08:16,320
Cloud's NLP tools, like Google Cloud, AutoML Natural Language. BERT also proved that you could

105
00:08:16,400 --> 00:08:21,360
build very good models on unlabeled data, like text scraped from Wikipedia or Reddit.

106
00:08:21,360 --> 00:08:25,040
This is called semi-supervised learning, and it's a big trend in machine learning right now.

107
00:08:27,120 --> 00:08:31,760
So if I've sold you on how cool transformers are, you might want to start using them in your app.

108
00:08:31,760 --> 00:08:36,800
No problem. Transurflow Hub is a great place to grab pre-trained transformer models like BERT.

109
00:08:36,800 --> 00:08:40,720
You can download them for free in multiple languages and drop them straight into your app.

110
00:08:41,440 --> 00:08:45,920
You can also check out the popular Transformers Python library, built by the company Hugging

111
00:08:45,920 --> 00:08:49,840
Face. That's one of the community's favorite ways to train and use transformer models.

112
00:08:49,840 --> 00:09:01,840
For more transformer tips, check out my blog post linked below, and thanks for watching!

