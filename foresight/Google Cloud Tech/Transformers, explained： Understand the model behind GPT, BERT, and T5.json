{"text": " The neat thing about working in machine learning is that every few years somebody invents something crazy that makes you totally reconsider what's possible, like models that can play go or generate hyper-realistic faces. And today, the mind-blowing discovery that's rocking everyone's world is a type of neural network called a transformer. Transformers are models that can translate text, write poems, and off-eds, and even generate computer code. These have been used in biology to solve the protein folding problem. Transformers are like this magical machine learning hammer that seems to make every problem into a nail. If you've heard of the trendy new ML models BERT or GPT-3 or T5, all of these models are based on transformers. So if you want to stay hip in machine learning and especially in natural language processing, you have to know about the transformer. So in this video, I'm going to tell you about what transformers are, how they work, and why they've been so impactful. Let's get to it. So what is a transformer? It's a type of neural network architecture. To recap, neural networks are a very effective type of model for analyzing complicated data types like images, videos, audio, and text. But there are different types of neural networks optimized for different types of data, like if you're analyzing images, you typically use a convolutional neural network, which is designed to vaguely mimic the way that the human brain processes vision. And since around 2012, neural networks have been really good at solving vision tasks, like identifying objects and photos. But for a long time, we didn't have anything comparably good for analyzing language, whether for translation or text summarization or text generation. And this is a problem because language is the primary way that humans communicate. You see, until transformers came around, the way we used deep learning to understand text was with a type of model called a recurrent neural network, or an RNN. That looks something like this. Let's say you wanted to translate a sentence from English to French. An RNN would take as input an English sentence and process the words one at a time and then sequentially spit out their French counterparts. The key word here is sequential. In language, the order of words matters, and you can't just shuffle them around. For example, the sentence Jane went looking for trouble. It means something very different than the sentence trouble went looking for Jane. So any model that's going to deal with language has to capture word order, and recurrent neural networks do this by looking at one word at a time sequentially. But RNNs had a lot of problems. First, they never really did well at handling large sequences of text, like long paragraphs or essays. By the time they were analyzing the end of a paragraph, they'd forget what happened in the beginning. And even worse, RNNs were pretty hard to train. Because they processed words sequentially, they couldn't paralyze well, which means that you couldn't just speed them up by throwing lots of GPUs at them. And when you have a model that's slow to train, you can't train it on all that much data. This is where the transformer changed everything. They were a model developed in 2017 by researchers at Google and the University of Toronto, and they were initially designed to do translation. But unlike recurrent neural networks, you could really efficiently paralyze transformers. And that meant that with the right hardware, you could train some really big models. How big? Really big. Remember GPT-3, that model that writes poetry and code and has conversations? That was trained at almost 45 terabytes of text data, including, like, almost the entire public web. So if you remember anything about transformers, let it be this. Combine a model that scales really well with a huge data set, and the results will probably blow your mind. So how do these things actually work? From the diagram in the paper, it should be pretty clear. Or maybe not. Actually, it's simpler than you might think. There are three main innovations that make this model work so well. Positional encodings and attention, and specifically a type of attention called self-attention. Let's start by talking about the first one, positional encodings. Let's say we're trying to translate text from English to French. Positional encodings is the idea that instead of looking at words sequentially, you take each word in your sentence, and before you feed it into the neural network, you slap a number on it. One, two, three, depending on what number the word is in the sentence. In other words, you store information about word order in the data itself rather than in the structure of the network. Then as you train the network on lots of text data, it learns how to interpret those positional encodings. In this way, the neural network learns the importance of word order from the data. This is a high-level way to understand positional encodings, but it's an innovation that really helped make transformers easier to train than RNNs. The next innovation in this paper is a concept called attention, which you'll see used everywhere in machine learning these days. In fact, the title of the original transformer paper is attention is all you need. So, the agreement on the European Economic Area was signed in August 1992. Did you know that? That's the example sentence given in the original paper, and remember, the original transformer was designed for translation. Now imagine trying to translate that sentence to French. One bad way to translate text is to try to translate each word one for one. But in French, some words are flipped, like in the French translation, European comes before economic. Plus, French is a language that has gendered agreement between words, so the word europ\u00e9enne needs to be in the feminine form to match with la zone. The attention mechanism is a neural network structure that allows the text model to look at every single word in the original sentence when making a decision about how to translate a word in the output sentence. In fact, here's a nice visualization from that paper that shows what words in the input sentence the model is attending to when it makes predictions about a word for the output sentence. So, when the model outputs the word europ\u00e9enne, it's looking at the input words, European, and economic. You can think of this diagram as a sort of heat map for attention. And how does the model know which words it should be attending to? It's something that's learned over time from data. By seeing thousands of examples of French and English sentence pairs, the model learns about gender and word order and plurality and all of that grammatical stuff. So we talked about two key transformer innovations, positional encoding and attention. But actually, attention had been invented before this paper. The real innovation in transformers was something called self-attention, a twist on traditional attention. The type of attention we just talked about had to do with aligning words in English and French, which is really important for translation. But what if you're just trying to understand the underlying meaning in language so that you can build a network that can do any number of language tasks? What's incredible about neural networks like transformers is that as they analyze tons of text data, they begin to build up this internal representation or understanding of language automatically. They may learn, for example, that the words programmer and software engineer and software developer are all synonymous, and they might also naturally learn the rules of grammar and gender and tense and so on. The better this internal representation of language the neural network learns, the better it will be at any language task. And it turns out that attention can be a very effective way to get a neural network to understand language if it's turned on the input text itself. Let me give you an example. Take these two sentences. Server, can I have the check? Versus looks like I just crashed the server. The word server here means two very different things, and I know that because I'm looking at the context of the surrounding words. Self-attention allows a neural network to understand a word in the context of the words around it. So when a model processes the word server in the first sentence, it might be attending to the word check, which helps it disambiguate from a human server versus a mental one. In the second sentence, the model might be attending to the word crashed to determine that this server is a machine. Self-attention can also help neural networks disambiguate words, recognize parts of speech, and even identify word tense. This in a nutshell is the value of self-attention. So to summarize, transformers boil down to positional encodings, attention, and self-attention. Of course, this is a 10,000-foot look at transformers, but how are they actually useful? One of the most popular transformer-based models is called BERT, which was invented just around the time that I joined Google in 2018. BERT was trained on a massive text corpus and has become this sort of general pocket knife for NLP that can be adopted to a bunch of different tasks, like text summarization, question answering, classification, and finding similar sentences. It's used in Google Search to help understand search queries, and it powers a lot of Google Cloud's NLP tools, like Google Cloud, AutoML Natural Language. BERT also proved that you could build very good models on unlabeled data, like text scraped from Wikipedia or Reddit. This is called semi-supervised learning, and it's a big trend in machine learning right now. So if I've sold you on how cool transformers are, you might want to start using them in your app. No problem. Transurflow Hub is a great place to grab pre-trained transformer models like BERT. You can download them for free in multiple languages and drop them straight into your app. You can also check out the popular Transformers Python library, built by the company Hugging Face. That's one of the community's favorite ways to train and use transformer models. For more transformer tips, check out my blog post linked below, and thanks for watching!", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.44, "text": " The neat thing about working in machine learning is that every few years somebody", "tokens": [50364, 440, 10654, 551, 466, 1364, 294, 3479, 2539, 307, 300, 633, 1326, 924, 2618, 50536], "temperature": 0.0, "avg_logprob": -0.11925525513906328, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.0012841776479035616}, {"id": 1, "seek": 0, "start": 3.44, "end": 6.8, "text": " invents something crazy that makes you totally reconsider what's possible,", "tokens": [50536, 1048, 791, 746, 3219, 300, 1669, 291, 3879, 40497, 437, 311, 1944, 11, 50704], "temperature": 0.0, "avg_logprob": -0.11925525513906328, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.0012841776479035616}, {"id": 2, "seek": 0, "start": 7.36, "end": 11.44, "text": " like models that can play go or generate hyper-realistic faces.", "tokens": [50732, 411, 5245, 300, 393, 862, 352, 420, 8460, 9848, 12, 9342, 3142, 8475, 13, 50936], "temperature": 0.0, "avg_logprob": -0.11925525513906328, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.0012841776479035616}, {"id": 3, "seek": 0, "start": 12.0, "end": 15.52, "text": " And today, the mind-blowing discovery that's rocking everyone's world is a type of neural", "tokens": [50964, 400, 965, 11, 264, 1575, 12, 43788, 12114, 300, 311, 30929, 1518, 311, 1002, 307, 257, 2010, 295, 18161, 51140], "temperature": 0.0, "avg_logprob": -0.11925525513906328, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.0012841776479035616}, {"id": 4, "seek": 0, "start": 15.52, "end": 20.0, "text": " network called a transformer. Transformers are models that can translate text,", "tokens": [51140, 3209, 1219, 257, 31782, 13, 27938, 433, 366, 5245, 300, 393, 13799, 2487, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11925525513906328, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.0012841776479035616}, {"id": 5, "seek": 0, "start": 20.0, "end": 24.48, "text": " write poems, and off-eds, and even generate computer code. These have been used in biology", "tokens": [51364, 2464, 24014, 11, 293, 766, 12, 5147, 11, 293, 754, 8460, 3820, 3089, 13, 1981, 362, 668, 1143, 294, 14956, 51588], "temperature": 0.0, "avg_logprob": -0.11925525513906328, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.0012841776479035616}, {"id": 6, "seek": 0, "start": 24.48, "end": 29.2, "text": " to solve the protein folding problem. Transformers are like this magical machine learning hammer", "tokens": [51588, 281, 5039, 264, 7944, 25335, 1154, 13, 27938, 433, 366, 411, 341, 12066, 3479, 2539, 13017, 51824], "temperature": 0.0, "avg_logprob": -0.11925525513906328, "compression_ratio": 1.7919254658385093, "no_speech_prob": 0.0012841776479035616}, {"id": 7, "seek": 2920, "start": 29.2, "end": 33.519999999999996, "text": " that seems to make every problem into a nail. If you've heard of the trendy new ML models", "tokens": [50364, 300, 2544, 281, 652, 633, 1154, 666, 257, 10173, 13, 759, 291, 600, 2198, 295, 264, 38596, 777, 21601, 5245, 50580], "temperature": 0.0, "avg_logprob": -0.07590386426007306, "compression_ratio": 1.670926517571885, "no_speech_prob": 0.010982566513121128}, {"id": 8, "seek": 2920, "start": 33.519999999999996, "end": 40.0, "text": " BERT or GPT-3 or T5, all of these models are based on transformers. So if you want to stay", "tokens": [50580, 363, 31479, 420, 26039, 51, 12, 18, 420, 314, 20, 11, 439, 295, 613, 5245, 366, 2361, 322, 4088, 433, 13, 407, 498, 291, 528, 281, 1754, 50904], "temperature": 0.0, "avg_logprob": -0.07590386426007306, "compression_ratio": 1.670926517571885, "no_speech_prob": 0.010982566513121128}, {"id": 9, "seek": 2920, "start": 40.0, "end": 43.2, "text": " hip in machine learning and especially in natural language processing,", "tokens": [50904, 8103, 294, 3479, 2539, 293, 2318, 294, 3303, 2856, 9007, 11, 51064], "temperature": 0.0, "avg_logprob": -0.07590386426007306, "compression_ratio": 1.670926517571885, "no_speech_prob": 0.010982566513121128}, {"id": 10, "seek": 2920, "start": 43.2, "end": 46.879999999999995, "text": " you have to know about the transformer. So in this video, I'm going to tell you about what", "tokens": [51064, 291, 362, 281, 458, 466, 264, 31782, 13, 407, 294, 341, 960, 11, 286, 478, 516, 281, 980, 291, 466, 437, 51248], "temperature": 0.0, "avg_logprob": -0.07590386426007306, "compression_ratio": 1.670926517571885, "no_speech_prob": 0.010982566513121128}, {"id": 11, "seek": 2920, "start": 46.879999999999995, "end": 51.68, "text": " transformers are, how they work, and why they've been so impactful. Let's get to it.", "tokens": [51248, 4088, 433, 366, 11, 577, 436, 589, 11, 293, 983, 436, 600, 668, 370, 30842, 13, 961, 311, 483, 281, 309, 13, 51488], "temperature": 0.0, "avg_logprob": -0.07590386426007306, "compression_ratio": 1.670926517571885, "no_speech_prob": 0.010982566513121128}, {"id": 12, "seek": 2920, "start": 51.68, "end": 56.879999999999995, "text": " So what is a transformer? It's a type of neural network architecture. To recap, neural networks", "tokens": [51488, 407, 437, 307, 257, 31782, 30, 467, 311, 257, 2010, 295, 18161, 3209, 9482, 13, 1407, 20928, 11, 18161, 9590, 51748], "temperature": 0.0, "avg_logprob": -0.07590386426007306, "compression_ratio": 1.670926517571885, "no_speech_prob": 0.010982566513121128}, {"id": 13, "seek": 5688, "start": 56.88, "end": 62.24, "text": " are a very effective type of model for analyzing complicated data types like images, videos, audio,", "tokens": [50364, 366, 257, 588, 4942, 2010, 295, 2316, 337, 23663, 6179, 1412, 3467, 411, 5267, 11, 2145, 11, 6278, 11, 50632], "temperature": 0.0, "avg_logprob": -0.06996307853891068, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.008314895443618298}, {"id": 14, "seek": 5688, "start": 62.24, "end": 66.16, "text": " and text. But there are different types of neural networks optimized for different types of data,", "tokens": [50632, 293, 2487, 13, 583, 456, 366, 819, 3467, 295, 18161, 9590, 26941, 337, 819, 3467, 295, 1412, 11, 50828], "temperature": 0.0, "avg_logprob": -0.06996307853891068, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.008314895443618298}, {"id": 15, "seek": 5688, "start": 66.16, "end": 70.56, "text": " like if you're analyzing images, you typically use a convolutional neural network,", "tokens": [50828, 411, 498, 291, 434, 23663, 5267, 11, 291, 5850, 764, 257, 45216, 304, 18161, 3209, 11, 51048], "temperature": 0.0, "avg_logprob": -0.06996307853891068, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.008314895443618298}, {"id": 16, "seek": 5688, "start": 70.56, "end": 74.64, "text": " which is designed to vaguely mimic the way that the human brain processes vision.", "tokens": [51048, 597, 307, 4761, 281, 13501, 48863, 31075, 264, 636, 300, 264, 1952, 3567, 7555, 5201, 13, 51252], "temperature": 0.0, "avg_logprob": -0.06996307853891068, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.008314895443618298}, {"id": 17, "seek": 5688, "start": 74.64, "end": 78.72, "text": " And since around 2012, neural networks have been really good at solving vision tasks,", "tokens": [51252, 400, 1670, 926, 9125, 11, 18161, 9590, 362, 668, 534, 665, 412, 12606, 5201, 9608, 11, 51456], "temperature": 0.0, "avg_logprob": -0.06996307853891068, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.008314895443618298}, {"id": 18, "seek": 5688, "start": 78.72, "end": 83.68, "text": " like identifying objects and photos. But for a long time, we didn't have anything comparably good", "tokens": [51456, 411, 16696, 6565, 293, 5787, 13, 583, 337, 257, 938, 565, 11, 321, 994, 380, 362, 1340, 6311, 1188, 665, 51704], "temperature": 0.0, "avg_logprob": -0.06996307853891068, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.008314895443618298}, {"id": 19, "seek": 8368, "start": 83.68, "end": 88.32000000000001, "text": " for analyzing language, whether for translation or text summarization or text generation.", "tokens": [50364, 337, 23663, 2856, 11, 1968, 337, 12853, 420, 2487, 14611, 2144, 420, 2487, 5125, 13, 50596], "temperature": 0.0, "avg_logprob": -0.07548910776774088, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.010984811931848526}, {"id": 20, "seek": 8368, "start": 88.88000000000001, "end": 92.16000000000001, "text": " And this is a problem because language is the primary way that humans communicate.", "tokens": [50624, 400, 341, 307, 257, 1154, 570, 2856, 307, 264, 6194, 636, 300, 6255, 7890, 13, 50788], "temperature": 0.0, "avg_logprob": -0.07548910776774088, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.010984811931848526}, {"id": 21, "seek": 8368, "start": 92.16000000000001, "end": 96.24000000000001, "text": " You see, until transformers came around, the way we used deep learning to understand text", "tokens": [50788, 509, 536, 11, 1826, 4088, 433, 1361, 926, 11, 264, 636, 321, 1143, 2452, 2539, 281, 1223, 2487, 50992], "temperature": 0.0, "avg_logprob": -0.07548910776774088, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.010984811931848526}, {"id": 22, "seek": 8368, "start": 96.24000000000001, "end": 101.44000000000001, "text": " was with a type of model called a recurrent neural network, or an RNN. That looks something like this.", "tokens": [50992, 390, 365, 257, 2010, 295, 2316, 1219, 257, 18680, 1753, 18161, 3209, 11, 420, 364, 45702, 45, 13, 663, 1542, 746, 411, 341, 13, 51252], "temperature": 0.0, "avg_logprob": -0.07548910776774088, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.010984811931848526}, {"id": 23, "seek": 8368, "start": 102.96000000000001, "end": 105.60000000000001, "text": " Let's say you wanted to translate a sentence from English to French.", "tokens": [51328, 961, 311, 584, 291, 1415, 281, 13799, 257, 8174, 490, 3669, 281, 5522, 13, 51460], "temperature": 0.0, "avg_logprob": -0.07548910776774088, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.010984811931848526}, {"id": 24, "seek": 8368, "start": 106.16000000000001, "end": 110.80000000000001, "text": " An RNN would take as input an English sentence and process the words one at a time and then", "tokens": [51488, 1107, 45702, 45, 576, 747, 382, 4846, 364, 3669, 8174, 293, 1399, 264, 2283, 472, 412, 257, 565, 293, 550, 51720], "temperature": 0.0, "avg_logprob": -0.07548910776774088, "compression_ratio": 1.6751592356687899, "no_speech_prob": 0.010984811931848526}, {"id": 25, "seek": 11080, "start": 110.8, "end": 116.39999999999999, "text": " sequentially spit out their French counterparts. The key word here is sequential. In language,", "tokens": [50364, 5123, 3137, 22127, 484, 641, 5522, 33287, 13, 440, 2141, 1349, 510, 307, 42881, 13, 682, 2856, 11, 50644], "temperature": 0.0, "avg_logprob": -0.07327909767627716, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.012818784452974796}, {"id": 26, "seek": 11080, "start": 116.39999999999999, "end": 121.44, "text": " the order of words matters, and you can't just shuffle them around. For example, the sentence", "tokens": [50644, 264, 1668, 295, 2283, 7001, 11, 293, 291, 393, 380, 445, 39426, 552, 926, 13, 1171, 1365, 11, 264, 8174, 50896], "temperature": 0.0, "avg_logprob": -0.07327909767627716, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.012818784452974796}, {"id": 27, "seek": 11080, "start": 121.44, "end": 125.03999999999999, "text": " Jane went looking for trouble. It means something very different than the sentence", "tokens": [50896, 13048, 1437, 1237, 337, 5253, 13, 467, 1355, 746, 588, 819, 813, 264, 8174, 51076], "temperature": 0.0, "avg_logprob": -0.07327909767627716, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.012818784452974796}, {"id": 28, "seek": 11080, "start": 125.03999999999999, "end": 129.35999999999999, "text": " trouble went looking for Jane. So any model that's going to deal with language has to", "tokens": [51076, 5253, 1437, 1237, 337, 13048, 13, 407, 604, 2316, 300, 311, 516, 281, 2028, 365, 2856, 575, 281, 51292], "temperature": 0.0, "avg_logprob": -0.07327909767627716, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.012818784452974796}, {"id": 29, "seek": 11080, "start": 129.35999999999999, "end": 134.48, "text": " capture word order, and recurrent neural networks do this by looking at one word at a time sequentially.", "tokens": [51292, 7983, 1349, 1668, 11, 293, 18680, 1753, 18161, 9590, 360, 341, 538, 1237, 412, 472, 1349, 412, 257, 565, 5123, 3137, 13, 51548], "temperature": 0.0, "avg_logprob": -0.07327909767627716, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.012818784452974796}, {"id": 30, "seek": 11080, "start": 134.48, "end": 139.44, "text": " But RNNs had a lot of problems. First, they never really did well at handling large sequences of", "tokens": [51548, 583, 45702, 45, 82, 632, 257, 688, 295, 2740, 13, 2386, 11, 436, 1128, 534, 630, 731, 412, 13175, 2416, 22978, 295, 51796], "temperature": 0.0, "avg_logprob": -0.07327909767627716, "compression_ratio": 1.736024844720497, "no_speech_prob": 0.012818784452974796}, {"id": 31, "seek": 13944, "start": 139.44, "end": 144.0, "text": " text, like long paragraphs or essays. By the time they were analyzing the end of a paragraph,", "tokens": [50364, 2487, 11, 411, 938, 48910, 420, 35123, 13, 3146, 264, 565, 436, 645, 23663, 264, 917, 295, 257, 18865, 11, 50592], "temperature": 0.0, "avg_logprob": -0.06623065383345993, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.005219609010964632}, {"id": 32, "seek": 13944, "start": 144.0, "end": 148.96, "text": " they'd forget what happened in the beginning. And even worse, RNNs were pretty hard to train.", "tokens": [50592, 436, 1116, 2870, 437, 2011, 294, 264, 2863, 13, 400, 754, 5324, 11, 45702, 45, 82, 645, 1238, 1152, 281, 3847, 13, 50840], "temperature": 0.0, "avg_logprob": -0.06623065383345993, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.005219609010964632}, {"id": 33, "seek": 13944, "start": 148.96, "end": 152.88, "text": " Because they processed words sequentially, they couldn't paralyze well, which means that you", "tokens": [50840, 1436, 436, 18846, 2283, 5123, 3137, 11, 436, 2809, 380, 32645, 1381, 731, 11, 597, 1355, 300, 291, 51036], "temperature": 0.0, "avg_logprob": -0.06623065383345993, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.005219609010964632}, {"id": 34, "seek": 13944, "start": 152.88, "end": 157.28, "text": " couldn't just speed them up by throwing lots of GPUs at them. And when you have a model that's slow", "tokens": [51036, 2809, 380, 445, 3073, 552, 493, 538, 10238, 3195, 295, 18407, 82, 412, 552, 13, 400, 562, 291, 362, 257, 2316, 300, 311, 2964, 51256], "temperature": 0.0, "avg_logprob": -0.06623065383345993, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.005219609010964632}, {"id": 35, "seek": 13944, "start": 157.28, "end": 161.36, "text": " to train, you can't train it on all that much data. This is where the transformer changed", "tokens": [51256, 281, 3847, 11, 291, 393, 380, 3847, 309, 322, 439, 300, 709, 1412, 13, 639, 307, 689, 264, 31782, 3105, 51460], "temperature": 0.0, "avg_logprob": -0.06623065383345993, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.005219609010964632}, {"id": 36, "seek": 13944, "start": 161.36, "end": 165.92, "text": " everything. They were a model developed in 2017 by researchers at Google and the University of", "tokens": [51460, 1203, 13, 814, 645, 257, 2316, 4743, 294, 6591, 538, 10309, 412, 3329, 293, 264, 3535, 295, 51688], "temperature": 0.0, "avg_logprob": -0.06623065383345993, "compression_ratio": 1.661764705882353, "no_speech_prob": 0.005219609010964632}, {"id": 37, "seek": 16592, "start": 165.92, "end": 170.64, "text": " Toronto, and they were initially designed to do translation. But unlike recurrent neural networks,", "tokens": [50364, 14140, 11, 293, 436, 645, 9105, 4761, 281, 360, 12853, 13, 583, 8343, 18680, 1753, 18161, 9590, 11, 50600], "temperature": 0.0, "avg_logprob": -0.06183104081587358, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.01032706256955862}, {"id": 38, "seek": 16592, "start": 170.64, "end": 174.79999999999998, "text": " you could really efficiently paralyze transformers. And that meant that with the right hardware,", "tokens": [50600, 291, 727, 534, 19621, 32645, 1381, 4088, 433, 13, 400, 300, 4140, 300, 365, 264, 558, 8837, 11, 50808], "temperature": 0.0, "avg_logprob": -0.06183104081587358, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.01032706256955862}, {"id": 39, "seek": 16592, "start": 174.79999999999998, "end": 181.04, "text": " you could train some really big models. How big? Really big. Remember GPT-3, that model that writes", "tokens": [50808, 291, 727, 3847, 512, 534, 955, 5245, 13, 1012, 955, 30, 4083, 955, 13, 5459, 26039, 51, 12, 18, 11, 300, 2316, 300, 13657, 51120], "temperature": 0.0, "avg_logprob": -0.06183104081587358, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.01032706256955862}, {"id": 40, "seek": 16592, "start": 181.04, "end": 186.64, "text": " poetry and code and has conversations? That was trained at almost 45 terabytes of text data,", "tokens": [51120, 15155, 293, 3089, 293, 575, 7315, 30, 663, 390, 8895, 412, 1920, 6905, 1796, 24538, 295, 2487, 1412, 11, 51400], "temperature": 0.0, "avg_logprob": -0.06183104081587358, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.01032706256955862}, {"id": 41, "seek": 16592, "start": 186.64, "end": 192.48, "text": " including, like, almost the entire public web. So if you remember anything about transformers,", "tokens": [51400, 3009, 11, 411, 11, 1920, 264, 2302, 1908, 3670, 13, 407, 498, 291, 1604, 1340, 466, 4088, 433, 11, 51692], "temperature": 0.0, "avg_logprob": -0.06183104081587358, "compression_ratio": 1.6597938144329898, "no_speech_prob": 0.01032706256955862}, {"id": 42, "seek": 19248, "start": 192.48, "end": 197.2, "text": " let it be this. Combine a model that scales really well with a huge data set, and the results will", "tokens": [50364, 718, 309, 312, 341, 13, 25939, 533, 257, 2316, 300, 17408, 534, 731, 365, 257, 2603, 1412, 992, 11, 293, 264, 3542, 486, 50600], "temperature": 0.0, "avg_logprob": -0.07806268419538226, "compression_ratio": 1.7104477611940299, "no_speech_prob": 0.030210377648472786}, {"id": 43, "seek": 19248, "start": 197.2, "end": 201.67999999999998, "text": " probably blow your mind. So how do these things actually work? From the diagram in the paper,", "tokens": [50600, 1391, 6327, 428, 1575, 13, 407, 577, 360, 613, 721, 767, 589, 30, 3358, 264, 10686, 294, 264, 3035, 11, 50824], "temperature": 0.0, "avg_logprob": -0.07806268419538226, "compression_ratio": 1.7104477611940299, "no_speech_prob": 0.030210377648472786}, {"id": 44, "seek": 19248, "start": 201.67999999999998, "end": 207.76, "text": " it should be pretty clear. Or maybe not. Actually, it's simpler than you might think. There are", "tokens": [50824, 309, 820, 312, 1238, 1850, 13, 1610, 1310, 406, 13, 5135, 11, 309, 311, 18587, 813, 291, 1062, 519, 13, 821, 366, 51128], "temperature": 0.0, "avg_logprob": -0.07806268419538226, "compression_ratio": 1.7104477611940299, "no_speech_prob": 0.030210377648472786}, {"id": 45, "seek": 19248, "start": 207.76, "end": 212.39999999999998, "text": " three main innovations that make this model work so well. Positional encodings and attention,", "tokens": [51128, 1045, 2135, 24283, 300, 652, 341, 2316, 589, 370, 731, 13, 25906, 2628, 2058, 378, 1109, 293, 3202, 11, 51360], "temperature": 0.0, "avg_logprob": -0.07806268419538226, "compression_ratio": 1.7104477611940299, "no_speech_prob": 0.030210377648472786}, {"id": 46, "seek": 19248, "start": 212.39999999999998, "end": 217.28, "text": " and specifically a type of attention called self-attention. Let's start by talking about the", "tokens": [51360, 293, 4682, 257, 2010, 295, 3202, 1219, 2698, 12, 1591, 1251, 13, 961, 311, 722, 538, 1417, 466, 264, 51604], "temperature": 0.0, "avg_logprob": -0.07806268419538226, "compression_ratio": 1.7104477611940299, "no_speech_prob": 0.030210377648472786}, {"id": 47, "seek": 19248, "start": 217.28, "end": 222.0, "text": " first one, positional encodings. Let's say we're trying to translate text from English to French.", "tokens": [51604, 700, 472, 11, 2535, 304, 2058, 378, 1109, 13, 961, 311, 584, 321, 434, 1382, 281, 13799, 2487, 490, 3669, 281, 5522, 13, 51840], "temperature": 0.0, "avg_logprob": -0.07806268419538226, "compression_ratio": 1.7104477611940299, "no_speech_prob": 0.030210377648472786}, {"id": 48, "seek": 22200, "start": 222.0, "end": 225.68, "text": " Positional encodings is the idea that instead of looking at words sequentially,", "tokens": [50364, 25906, 2628, 2058, 378, 1109, 307, 264, 1558, 300, 2602, 295, 1237, 412, 2283, 5123, 3137, 11, 50548], "temperature": 0.0, "avg_logprob": -0.05821596853660815, "compression_ratio": 1.8961937716262975, "no_speech_prob": 0.0032726877834647894}, {"id": 49, "seek": 22200, "start": 225.68, "end": 229.04, "text": " you take each word in your sentence, and before you feed it into the neural network,", "tokens": [50548, 291, 747, 1184, 1349, 294, 428, 8174, 11, 293, 949, 291, 3154, 309, 666, 264, 18161, 3209, 11, 50716], "temperature": 0.0, "avg_logprob": -0.05821596853660815, "compression_ratio": 1.8961937716262975, "no_speech_prob": 0.0032726877834647894}, {"id": 50, "seek": 22200, "start": 229.04, "end": 234.4, "text": " you slap a number on it. One, two, three, depending on what number the word is in the sentence. In", "tokens": [50716, 291, 21075, 257, 1230, 322, 309, 13, 1485, 11, 732, 11, 1045, 11, 5413, 322, 437, 1230, 264, 1349, 307, 294, 264, 8174, 13, 682, 50984], "temperature": 0.0, "avg_logprob": -0.05821596853660815, "compression_ratio": 1.8961937716262975, "no_speech_prob": 0.0032726877834647894}, {"id": 51, "seek": 22200, "start": 234.4, "end": 238.24, "text": " other words, you store information about word order in the data itself rather than in the", "tokens": [50984, 661, 2283, 11, 291, 3531, 1589, 466, 1349, 1668, 294, 264, 1412, 2564, 2831, 813, 294, 264, 51176], "temperature": 0.0, "avg_logprob": -0.05821596853660815, "compression_ratio": 1.8961937716262975, "no_speech_prob": 0.0032726877834647894}, {"id": 52, "seek": 22200, "start": 238.24, "end": 243.04, "text": " structure of the network. Then as you train the network on lots of text data, it learns how to", "tokens": [51176, 3877, 295, 264, 3209, 13, 1396, 382, 291, 3847, 264, 3209, 322, 3195, 295, 2487, 1412, 11, 309, 27152, 577, 281, 51416], "temperature": 0.0, "avg_logprob": -0.05821596853660815, "compression_ratio": 1.8961937716262975, "no_speech_prob": 0.0032726877834647894}, {"id": 53, "seek": 22200, "start": 243.04, "end": 248.32, "text": " interpret those positional encodings. In this way, the neural network learns the importance of word", "tokens": [51416, 7302, 729, 2535, 304, 2058, 378, 1109, 13, 682, 341, 636, 11, 264, 18161, 3209, 27152, 264, 7379, 295, 1349, 51680], "temperature": 0.0, "avg_logprob": -0.05821596853660815, "compression_ratio": 1.8961937716262975, "no_speech_prob": 0.0032726877834647894}, {"id": 54, "seek": 24832, "start": 248.32, "end": 253.76, "text": " order from the data. This is a high-level way to understand positional encodings, but it's an", "tokens": [50364, 1668, 490, 264, 1412, 13, 639, 307, 257, 1090, 12, 12418, 636, 281, 1223, 2535, 304, 2058, 378, 1109, 11, 457, 309, 311, 364, 50636], "temperature": 0.0, "avg_logprob": -0.07414685695543202, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.012429720722138882}, {"id": 55, "seek": 24832, "start": 253.76, "end": 259.2, "text": " innovation that really helped make transformers easier to train than RNNs. The next innovation", "tokens": [50636, 8504, 300, 534, 4254, 652, 4088, 433, 3571, 281, 3847, 813, 45702, 45, 82, 13, 440, 958, 8504, 50908], "temperature": 0.0, "avg_logprob": -0.07414685695543202, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.012429720722138882}, {"id": 56, "seek": 24832, "start": 259.2, "end": 262.8, "text": " in this paper is a concept called attention, which you'll see used everywhere in machine", "tokens": [50908, 294, 341, 3035, 307, 257, 3410, 1219, 3202, 11, 597, 291, 603, 536, 1143, 5315, 294, 3479, 51088], "temperature": 0.0, "avg_logprob": -0.07414685695543202, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.012429720722138882}, {"id": 57, "seek": 24832, "start": 262.8, "end": 267.6, "text": " learning these days. In fact, the title of the original transformer paper is attention is all", "tokens": [51088, 2539, 613, 1708, 13, 682, 1186, 11, 264, 4876, 295, 264, 3380, 31782, 3035, 307, 3202, 307, 439, 51328], "temperature": 0.0, "avg_logprob": -0.07414685695543202, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.012429720722138882}, {"id": 58, "seek": 24832, "start": 267.6, "end": 274.4, "text": " you need. So, the agreement on the European Economic Area was signed in August 1992. Did you", "tokens": [51328, 291, 643, 13, 407, 11, 264, 8106, 322, 264, 6473, 25776, 19405, 390, 8175, 294, 6897, 23952, 13, 2589, 291, 51668], "temperature": 0.0, "avg_logprob": -0.07414685695543202, "compression_ratio": 1.5945017182130585, "no_speech_prob": 0.012429720722138882}, {"id": 59, "seek": 27440, "start": 274.4, "end": 279.03999999999996, "text": " know that? That's the example sentence given in the original paper, and remember, the original", "tokens": [50364, 458, 300, 30, 663, 311, 264, 1365, 8174, 2212, 294, 264, 3380, 3035, 11, 293, 1604, 11, 264, 3380, 50596], "temperature": 0.0, "avg_logprob": -0.0859272302674853, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.005909549072384834}, {"id": 60, "seek": 27440, "start": 279.03999999999996, "end": 283.67999999999995, "text": " transformer was designed for translation. Now imagine trying to translate that sentence to", "tokens": [50596, 31782, 390, 4761, 337, 12853, 13, 823, 3811, 1382, 281, 13799, 300, 8174, 281, 50828], "temperature": 0.0, "avg_logprob": -0.0859272302674853, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.005909549072384834}, {"id": 61, "seek": 27440, "start": 283.67999999999995, "end": 289.35999999999996, "text": " French. One bad way to translate text is to try to translate each word one for one. But in French,", "tokens": [50828, 5522, 13, 1485, 1578, 636, 281, 13799, 2487, 307, 281, 853, 281, 13799, 1184, 1349, 472, 337, 472, 13, 583, 294, 5522, 11, 51112], "temperature": 0.0, "avg_logprob": -0.0859272302674853, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.005909549072384834}, {"id": 62, "seek": 27440, "start": 289.35999999999996, "end": 293.52, "text": " some words are flipped, like in the French translation, European comes before economic.", "tokens": [51112, 512, 2283, 366, 26273, 11, 411, 294, 264, 5522, 12853, 11, 6473, 1487, 949, 4836, 13, 51320], "temperature": 0.0, "avg_logprob": -0.0859272302674853, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.005909549072384834}, {"id": 63, "seek": 27440, "start": 294.23999999999995, "end": 298.23999999999995, "text": " Plus, French is a language that has gendered agreement between words, so the word", "tokens": [51356, 7721, 11, 5522, 307, 257, 2856, 300, 575, 7898, 292, 8106, 1296, 2283, 11, 370, 264, 1349, 51556], "temperature": 0.0, "avg_logprob": -0.0859272302674853, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.005909549072384834}, {"id": 64, "seek": 27440, "start": 298.23999999999995, "end": 303.52, "text": " europ\u00e9enne needs to be in the feminine form to match with la zone. The attention mechanism", "tokens": [51556, 32055, 13295, 2203, 281, 312, 294, 264, 24648, 1254, 281, 2995, 365, 635, 6668, 13, 440, 3202, 7513, 51820], "temperature": 0.0, "avg_logprob": -0.0859272302674853, "compression_ratio": 1.778501628664495, "no_speech_prob": 0.005909549072384834}, {"id": 65, "seek": 30352, "start": 303.52, "end": 307.76, "text": " is a neural network structure that allows the text model to look at every single word", "tokens": [50364, 307, 257, 18161, 3209, 3877, 300, 4045, 264, 2487, 2316, 281, 574, 412, 633, 2167, 1349, 50576], "temperature": 0.0, "avg_logprob": -0.07074406070093955, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.007573894690722227}, {"id": 66, "seek": 30352, "start": 307.76, "end": 312.4, "text": " in the original sentence when making a decision about how to translate a word in the output sentence.", "tokens": [50576, 294, 264, 3380, 8174, 562, 1455, 257, 3537, 466, 577, 281, 13799, 257, 1349, 294, 264, 5598, 8174, 13, 50808], "temperature": 0.0, "avg_logprob": -0.07074406070093955, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.007573894690722227}, {"id": 67, "seek": 30352, "start": 312.4, "end": 316.88, "text": " In fact, here's a nice visualization from that paper that shows what words in the input sentence", "tokens": [50808, 682, 1186, 11, 510, 311, 257, 1481, 25801, 490, 300, 3035, 300, 3110, 437, 2283, 294, 264, 4846, 8174, 51032], "temperature": 0.0, "avg_logprob": -0.07074406070093955, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.007573894690722227}, {"id": 68, "seek": 30352, "start": 316.88, "end": 321.12, "text": " the model is attending to when it makes predictions about a word for the output sentence.", "tokens": [51032, 264, 2316, 307, 15862, 281, 562, 309, 1669, 21264, 466, 257, 1349, 337, 264, 5598, 8174, 13, 51244], "temperature": 0.0, "avg_logprob": -0.07074406070093955, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.007573894690722227}, {"id": 69, "seek": 30352, "start": 321.91999999999996, "end": 326.79999999999995, "text": " So, when the model outputs the word europ\u00e9enne, it's looking at the input words,", "tokens": [51284, 407, 11, 562, 264, 2316, 23930, 264, 1349, 32055, 13295, 11, 309, 311, 1237, 412, 264, 4846, 2283, 11, 51528], "temperature": 0.0, "avg_logprob": -0.07074406070093955, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.007573894690722227}, {"id": 70, "seek": 30352, "start": 326.79999999999995, "end": 331.59999999999997, "text": " European, and economic. You can think of this diagram as a sort of heat map for attention.", "tokens": [51528, 6473, 11, 293, 4836, 13, 509, 393, 519, 295, 341, 10686, 382, 257, 1333, 295, 3738, 4471, 337, 3202, 13, 51768], "temperature": 0.0, "avg_logprob": -0.07074406070093955, "compression_ratio": 1.8355704697986577, "no_speech_prob": 0.007573894690722227}, {"id": 71, "seek": 33160, "start": 332.16, "end": 336.40000000000003, "text": " And how does the model know which words it should be attending to? It's something that's", "tokens": [50392, 400, 577, 775, 264, 2316, 458, 597, 2283, 309, 820, 312, 15862, 281, 30, 467, 311, 746, 300, 311, 50604], "temperature": 0.0, "avg_logprob": -0.07282664775848388, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0029808268882334232}, {"id": 72, "seek": 33160, "start": 336.40000000000003, "end": 341.52000000000004, "text": " learned over time from data. By seeing thousands of examples of French and English sentence pairs,", "tokens": [50604, 3264, 670, 565, 490, 1412, 13, 3146, 2577, 5383, 295, 5110, 295, 5522, 293, 3669, 8174, 15494, 11, 50860], "temperature": 0.0, "avg_logprob": -0.07282664775848388, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0029808268882334232}, {"id": 73, "seek": 33160, "start": 341.52000000000004, "end": 346.08000000000004, "text": " the model learns about gender and word order and plurality and all of that grammatical stuff.", "tokens": [50860, 264, 2316, 27152, 466, 7898, 293, 1349, 1668, 293, 25377, 507, 293, 439, 295, 300, 17570, 267, 804, 1507, 13, 51088], "temperature": 0.0, "avg_logprob": -0.07282664775848388, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0029808268882334232}, {"id": 74, "seek": 33160, "start": 346.08000000000004, "end": 350.48, "text": " So we talked about two key transformer innovations, positional encoding and attention.", "tokens": [51088, 407, 321, 2825, 466, 732, 2141, 31782, 24283, 11, 2535, 304, 43430, 293, 3202, 13, 51308], "temperature": 0.0, "avg_logprob": -0.07282664775848388, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0029808268882334232}, {"id": 75, "seek": 33160, "start": 351.04, "end": 355.20000000000005, "text": " But actually, attention had been invented before this paper. The real innovation in", "tokens": [51336, 583, 767, 11, 3202, 632, 668, 14479, 949, 341, 3035, 13, 440, 957, 8504, 294, 51544], "temperature": 0.0, "avg_logprob": -0.07282664775848388, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0029808268882334232}, {"id": 76, "seek": 33160, "start": 355.20000000000005, "end": 360.88, "text": " transformers was something called self-attention, a twist on traditional attention. The type of", "tokens": [51544, 4088, 433, 390, 746, 1219, 2698, 12, 1591, 1251, 11, 257, 8203, 322, 5164, 3202, 13, 440, 2010, 295, 51828], "temperature": 0.0, "avg_logprob": -0.07282664775848388, "compression_ratio": 1.767741935483871, "no_speech_prob": 0.0029808268882334232}, {"id": 77, "seek": 36088, "start": 360.88, "end": 364.4, "text": " attention we just talked about had to do with aligning words in English and French,", "tokens": [50364, 3202, 321, 445, 2825, 466, 632, 281, 360, 365, 419, 9676, 2283, 294, 3669, 293, 5522, 11, 50540], "temperature": 0.0, "avg_logprob": -0.05390676136674552, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.0023229678627103567}, {"id": 78, "seek": 36088, "start": 364.4, "end": 368.48, "text": " which is really important for translation. But what if you're just trying to understand the", "tokens": [50540, 597, 307, 534, 1021, 337, 12853, 13, 583, 437, 498, 291, 434, 445, 1382, 281, 1223, 264, 50744], "temperature": 0.0, "avg_logprob": -0.05390676136674552, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.0023229678627103567}, {"id": 79, "seek": 36088, "start": 368.48, "end": 373.44, "text": " underlying meaning in language so that you can build a network that can do any number of language", "tokens": [50744, 14217, 3620, 294, 2856, 370, 300, 291, 393, 1322, 257, 3209, 300, 393, 360, 604, 1230, 295, 2856, 50992], "temperature": 0.0, "avg_logprob": -0.05390676136674552, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.0023229678627103567}, {"id": 80, "seek": 36088, "start": 373.44, "end": 378.96, "text": " tasks? What's incredible about neural networks like transformers is that as they analyze tons of", "tokens": [50992, 9608, 30, 708, 311, 4651, 466, 18161, 9590, 411, 4088, 433, 307, 300, 382, 436, 12477, 9131, 295, 51268], "temperature": 0.0, "avg_logprob": -0.05390676136674552, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.0023229678627103567}, {"id": 81, "seek": 36088, "start": 378.96, "end": 384.15999999999997, "text": " text data, they begin to build up this internal representation or understanding of language", "tokens": [51268, 2487, 1412, 11, 436, 1841, 281, 1322, 493, 341, 6920, 10290, 420, 3701, 295, 2856, 51528], "temperature": 0.0, "avg_logprob": -0.05390676136674552, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.0023229678627103567}, {"id": 82, "seek": 36088, "start": 384.15999999999997, "end": 389.68, "text": " automatically. They may learn, for example, that the words programmer and software engineer and", "tokens": [51528, 6772, 13, 814, 815, 1466, 11, 337, 1365, 11, 300, 264, 2283, 32116, 293, 4722, 11403, 293, 51804], "temperature": 0.0, "avg_logprob": -0.05390676136674552, "compression_ratio": 1.7275541795665634, "no_speech_prob": 0.0023229678627103567}, {"id": 83, "seek": 38968, "start": 389.68, "end": 394.40000000000003, "text": " software developer are all synonymous, and they might also naturally learn the rules of grammar", "tokens": [50364, 4722, 10754, 366, 439, 5451, 18092, 11, 293, 436, 1062, 611, 8195, 1466, 264, 4474, 295, 22317, 50600], "temperature": 0.0, "avg_logprob": -0.07575213581050208, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.008313626982271671}, {"id": 84, "seek": 38968, "start": 394.40000000000003, "end": 399.04, "text": " and gender and tense and so on. The better this internal representation of language the neural", "tokens": [50600, 293, 7898, 293, 18760, 293, 370, 322, 13, 440, 1101, 341, 6920, 10290, 295, 2856, 264, 18161, 50832], "temperature": 0.0, "avg_logprob": -0.07575213581050208, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.008313626982271671}, {"id": 85, "seek": 38968, "start": 399.04, "end": 404.0, "text": " network learns, the better it will be at any language task. And it turns out that attention", "tokens": [50832, 3209, 27152, 11, 264, 1101, 309, 486, 312, 412, 604, 2856, 5633, 13, 400, 309, 4523, 484, 300, 3202, 51080], "temperature": 0.0, "avg_logprob": -0.07575213581050208, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.008313626982271671}, {"id": 86, "seek": 38968, "start": 404.0, "end": 408.48, "text": " can be a very effective way to get a neural network to understand language if it's turned on the", "tokens": [51080, 393, 312, 257, 588, 4942, 636, 281, 483, 257, 18161, 3209, 281, 1223, 2856, 498, 309, 311, 3574, 322, 264, 51304], "temperature": 0.0, "avg_logprob": -0.07575213581050208, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.008313626982271671}, {"id": 87, "seek": 38968, "start": 408.48, "end": 414.64, "text": " input text itself. Let me give you an example. Take these two sentences. Server, can I have the check?", "tokens": [51304, 4846, 2487, 2564, 13, 961, 385, 976, 291, 364, 1365, 13, 3664, 613, 732, 16579, 13, 25684, 11, 393, 286, 362, 264, 1520, 30, 51612], "temperature": 0.0, "avg_logprob": -0.07575213581050208, "compression_ratio": 1.7031802120141342, "no_speech_prob": 0.008313626982271671}, {"id": 88, "seek": 41464, "start": 415.28, "end": 420.8, "text": " Versus looks like I just crashed the server. The word server here means two very different things,", "tokens": [50396, 12226, 301, 1542, 411, 286, 445, 24190, 264, 7154, 13, 440, 1349, 7154, 510, 1355, 732, 588, 819, 721, 11, 50672], "temperature": 0.0, "avg_logprob": -0.06990425288677216, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.02442021295428276}, {"id": 89, "seek": 41464, "start": 420.8, "end": 424.15999999999997, "text": " and I know that because I'm looking at the context of the surrounding words.", "tokens": [50672, 293, 286, 458, 300, 570, 286, 478, 1237, 412, 264, 4319, 295, 264, 11498, 2283, 13, 50840], "temperature": 0.0, "avg_logprob": -0.06990425288677216, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.02442021295428276}, {"id": 90, "seek": 41464, "start": 424.96, "end": 429.03999999999996, "text": " Self-attention allows a neural network to understand a word in the context of the words", "tokens": [50880, 16348, 12, 1591, 1251, 4045, 257, 18161, 3209, 281, 1223, 257, 1349, 294, 264, 4319, 295, 264, 2283, 51084], "temperature": 0.0, "avg_logprob": -0.06990425288677216, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.02442021295428276}, {"id": 91, "seek": 41464, "start": 429.03999999999996, "end": 434.08, "text": " around it. So when a model processes the word server in the first sentence, it might be attending", "tokens": [51084, 926, 309, 13, 407, 562, 257, 2316, 7555, 264, 1349, 7154, 294, 264, 700, 8174, 11, 309, 1062, 312, 15862, 51336], "temperature": 0.0, "avg_logprob": -0.06990425288677216, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.02442021295428276}, {"id": 92, "seek": 41464, "start": 434.08, "end": 439.91999999999996, "text": " to the word check, which helps it disambiguate from a human server versus a mental one. In the", "tokens": [51336, 281, 264, 1349, 1520, 11, 597, 3665, 309, 717, 2173, 328, 10107, 490, 257, 1952, 7154, 5717, 257, 4973, 472, 13, 682, 264, 51628], "temperature": 0.0, "avg_logprob": -0.06990425288677216, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.02442021295428276}, {"id": 93, "seek": 41464, "start": 439.91999999999996, "end": 443.68, "text": " second sentence, the model might be attending to the word crashed to determine that this server", "tokens": [51628, 1150, 8174, 11, 264, 2316, 1062, 312, 15862, 281, 264, 1349, 24190, 281, 6997, 300, 341, 7154, 51816], "temperature": 0.0, "avg_logprob": -0.06990425288677216, "compression_ratio": 1.8904109589041096, "no_speech_prob": 0.02442021295428276}, {"id": 94, "seek": 44368, "start": 443.68, "end": 448.48, "text": " is a machine. Self-attention can also help neural networks disambiguate words, recognize parts of", "tokens": [50364, 307, 257, 3479, 13, 16348, 12, 1591, 1251, 393, 611, 854, 18161, 9590, 717, 2173, 328, 10107, 2283, 11, 5521, 3166, 295, 50604], "temperature": 0.0, "avg_logprob": -0.08441076278686524, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0035934040788561106}, {"id": 95, "seek": 44368, "start": 448.48, "end": 453.76, "text": " speech, and even identify word tense. This in a nutshell is the value of self-attention.", "tokens": [50604, 6218, 11, 293, 754, 5876, 1349, 18760, 13, 639, 294, 257, 37711, 307, 264, 2158, 295, 2698, 12, 1591, 1251, 13, 50868], "temperature": 0.0, "avg_logprob": -0.08441076278686524, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0035934040788561106}, {"id": 96, "seek": 44368, "start": 454.48, "end": 460.32, "text": " So to summarize, transformers boil down to positional encodings, attention, and self-attention.", "tokens": [50904, 407, 281, 20858, 11, 4088, 433, 13329, 760, 281, 2535, 304, 2058, 378, 1109, 11, 3202, 11, 293, 2698, 12, 1591, 1251, 13, 51196], "temperature": 0.0, "avg_logprob": -0.08441076278686524, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0035934040788561106}, {"id": 97, "seek": 44368, "start": 461.04, "end": 465.84000000000003, "text": " Of course, this is a 10,000-foot look at transformers, but how are they actually useful?", "tokens": [51232, 2720, 1164, 11, 341, 307, 257, 1266, 11, 1360, 12, 13498, 574, 412, 4088, 433, 11, 457, 577, 366, 436, 767, 4420, 30, 51472], "temperature": 0.0, "avg_logprob": -0.08441076278686524, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0035934040788561106}, {"id": 98, "seek": 44368, "start": 465.84000000000003, "end": 470.08, "text": " One of the most popular transformer-based models is called BERT, which was invented just around", "tokens": [51472, 1485, 295, 264, 881, 3743, 31782, 12, 6032, 5245, 307, 1219, 363, 31479, 11, 597, 390, 14479, 445, 926, 51684], "temperature": 0.0, "avg_logprob": -0.08441076278686524, "compression_ratio": 1.6501766784452296, "no_speech_prob": 0.0035934040788561106}, {"id": 99, "seek": 47008, "start": 470.08, "end": 475.52, "text": " the time that I joined Google in 2018. BERT was trained on a massive text corpus and has become", "tokens": [50364, 264, 565, 300, 286, 6869, 3329, 294, 6096, 13, 363, 31479, 390, 8895, 322, 257, 5994, 2487, 1181, 31624, 293, 575, 1813, 50636], "temperature": 0.0, "avg_logprob": -0.10220477364280008, "compression_ratio": 1.5671140939597314, "no_speech_prob": 0.0203287061303854}, {"id": 100, "seek": 47008, "start": 475.52, "end": 480.47999999999996, "text": " this sort of general pocket knife for NLP that can be adopted to a bunch of different tasks,", "tokens": [50636, 341, 1333, 295, 2674, 8963, 7976, 337, 426, 45196, 300, 393, 312, 12175, 281, 257, 3840, 295, 819, 9608, 11, 50884], "temperature": 0.0, "avg_logprob": -0.10220477364280008, "compression_ratio": 1.5671140939597314, "no_speech_prob": 0.0203287061303854}, {"id": 101, "seek": 47008, "start": 481.12, "end": 485.91999999999996, "text": " like text summarization, question answering, classification, and finding similar sentences.", "tokens": [50916, 411, 2487, 14611, 2144, 11, 1168, 13430, 11, 21538, 11, 293, 5006, 2531, 16579, 13, 51156], "temperature": 0.0, "avg_logprob": -0.10220477364280008, "compression_ratio": 1.5671140939597314, "no_speech_prob": 0.0203287061303854}, {"id": 102, "seek": 47008, "start": 486.64, "end": 490.79999999999995, "text": " It's used in Google Search to help understand search queries, and it powers a lot of Google", "tokens": [51192, 467, 311, 1143, 294, 3329, 17180, 281, 854, 1223, 3164, 24109, 11, 293, 309, 8674, 257, 688, 295, 3329, 51400], "temperature": 0.0, "avg_logprob": -0.10220477364280008, "compression_ratio": 1.5671140939597314, "no_speech_prob": 0.0203287061303854}, {"id": 103, "seek": 47008, "start": 490.79999999999995, "end": 496.32, "text": " Cloud's NLP tools, like Google Cloud, AutoML Natural Language. BERT also proved that you could", "tokens": [51400, 8061, 311, 426, 45196, 3873, 11, 411, 3329, 8061, 11, 13738, 12683, 20137, 24445, 13, 363, 31479, 611, 14617, 300, 291, 727, 51676], "temperature": 0.0, "avg_logprob": -0.10220477364280008, "compression_ratio": 1.5671140939597314, "no_speech_prob": 0.0203287061303854}, {"id": 104, "seek": 49632, "start": 496.4, "end": 501.36, "text": " build very good models on unlabeled data, like text scraped from Wikipedia or Reddit.", "tokens": [50368, 1322, 588, 665, 5245, 322, 32118, 18657, 292, 1412, 11, 411, 2487, 13943, 3452, 490, 28999, 420, 32210, 13, 50616], "temperature": 0.0, "avg_logprob": -0.10316261824439554, "compression_ratio": 1.6275659824046922, "no_speech_prob": 0.0007321686716750264}, {"id": 105, "seek": 49632, "start": 501.36, "end": 505.04, "text": " This is called semi-supervised learning, and it's a big trend in machine learning right now.", "tokens": [50616, 639, 307, 1219, 12909, 12, 48172, 24420, 2539, 11, 293, 309, 311, 257, 955, 6028, 294, 3479, 2539, 558, 586, 13, 50800], "temperature": 0.0, "avg_logprob": -0.10316261824439554, "compression_ratio": 1.6275659824046922, "no_speech_prob": 0.0007321686716750264}, {"id": 106, "seek": 49632, "start": 507.12, "end": 511.76, "text": " So if I've sold you on how cool transformers are, you might want to start using them in your app.", "tokens": [50904, 407, 498, 286, 600, 3718, 291, 322, 577, 1627, 4088, 433, 366, 11, 291, 1062, 528, 281, 722, 1228, 552, 294, 428, 724, 13, 51136], "temperature": 0.0, "avg_logprob": -0.10316261824439554, "compression_ratio": 1.6275659824046922, "no_speech_prob": 0.0007321686716750264}, {"id": 107, "seek": 49632, "start": 511.76, "end": 516.8, "text": " No problem. Transurflow Hub is a great place to grab pre-trained transformer models like BERT.", "tokens": [51136, 883, 1154, 13, 6531, 21844, 14107, 18986, 307, 257, 869, 1081, 281, 4444, 659, 12, 17227, 2001, 31782, 5245, 411, 363, 31479, 13, 51388], "temperature": 0.0, "avg_logprob": -0.10316261824439554, "compression_ratio": 1.6275659824046922, "no_speech_prob": 0.0007321686716750264}, {"id": 108, "seek": 49632, "start": 516.8, "end": 520.72, "text": " You can download them for free in multiple languages and drop them straight into your app.", "tokens": [51388, 509, 393, 5484, 552, 337, 1737, 294, 3866, 8650, 293, 3270, 552, 2997, 666, 428, 724, 13, 51584], "temperature": 0.0, "avg_logprob": -0.10316261824439554, "compression_ratio": 1.6275659824046922, "no_speech_prob": 0.0007321686716750264}, {"id": 109, "seek": 49632, "start": 521.4399999999999, "end": 525.92, "text": " You can also check out the popular Transformers Python library, built by the company Hugging", "tokens": [51620, 509, 393, 611, 1520, 484, 264, 3743, 27938, 433, 15329, 6405, 11, 3094, 538, 264, 2237, 46892, 3249, 51844], "temperature": 0.0, "avg_logprob": -0.10316261824439554, "compression_ratio": 1.6275659824046922, "no_speech_prob": 0.0007321686716750264}, {"id": 110, "seek": 52592, "start": 525.92, "end": 529.8399999999999, "text": " Face. That's one of the community's favorite ways to train and use transformer models.", "tokens": [50364, 4047, 13, 663, 311, 472, 295, 264, 1768, 311, 2954, 2098, 281, 3847, 293, 764, 31782, 5245, 13, 50560], "temperature": 0.0, "avg_logprob": -0.18404567532423066, "compression_ratio": 1.3257575757575757, "no_speech_prob": 0.010011222213506699}, {"id": 111, "seek": 52592, "start": 529.8399999999999, "end": 541.8399999999999, "text": " For more transformer tips, check out my blog post linked below, and thanks for watching!", "tokens": [50560, 1171, 544, 31782, 6082, 11, 1520, 484, 452, 6968, 2183, 9408, 2507, 11, 293, 3231, 337, 1976, 0, 51160], "temperature": 0.0, "avg_logprob": -0.18404567532423066, "compression_ratio": 1.3257575757575757, "no_speech_prob": 0.010011222213506699}], "language": "en"}