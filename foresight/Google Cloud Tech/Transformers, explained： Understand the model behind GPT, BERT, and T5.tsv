start	end	text
0	3440	The neat thing about working in machine learning is that every few years somebody
3440	6800	invents something crazy that makes you totally reconsider what's possible,
7360	11440	like models that can play go or generate hyper-realistic faces.
12000	15520	And today, the mind-blowing discovery that's rocking everyone's world is a type of neural
15520	20000	network called a transformer. Transformers are models that can translate text,
20000	24480	write poems, and off-eds, and even generate computer code. These have been used in biology
24480	29200	to solve the protein folding problem. Transformers are like this magical machine learning hammer
29200	33520	that seems to make every problem into a nail. If you've heard of the trendy new ML models
33520	40000	BERT or GPT-3 or T5, all of these models are based on transformers. So if you want to stay
40000	43200	hip in machine learning and especially in natural language processing,
43200	46880	you have to know about the transformer. So in this video, I'm going to tell you about what
46880	51680	transformers are, how they work, and why they've been so impactful. Let's get to it.
51680	56880	So what is a transformer? It's a type of neural network architecture. To recap, neural networks
56880	62240	are a very effective type of model for analyzing complicated data types like images, videos, audio,
62240	66160	and text. But there are different types of neural networks optimized for different types of data,
66160	70560	like if you're analyzing images, you typically use a convolutional neural network,
70560	74640	which is designed to vaguely mimic the way that the human brain processes vision.
74640	78720	And since around 2012, neural networks have been really good at solving vision tasks,
78720	83680	like identifying objects and photos. But for a long time, we didn't have anything comparably good
83680	88320	for analyzing language, whether for translation or text summarization or text generation.
88880	92160	And this is a problem because language is the primary way that humans communicate.
92160	96240	You see, until transformers came around, the way we used deep learning to understand text
96240	101440	was with a type of model called a recurrent neural network, or an RNN. That looks something like this.
102960	105600	Let's say you wanted to translate a sentence from English to French.
106160	110800	An RNN would take as input an English sentence and process the words one at a time and then
110800	116400	sequentially spit out their French counterparts. The key word here is sequential. In language,
116400	121440	the order of words matters, and you can't just shuffle them around. For example, the sentence
121440	125040	Jane went looking for trouble. It means something very different than the sentence
125040	129360	trouble went looking for Jane. So any model that's going to deal with language has to
129360	134480	capture word order, and recurrent neural networks do this by looking at one word at a time sequentially.
134480	139440	But RNNs had a lot of problems. First, they never really did well at handling large sequences of
139440	144000	text, like long paragraphs or essays. By the time they were analyzing the end of a paragraph,
144000	148960	they'd forget what happened in the beginning. And even worse, RNNs were pretty hard to train.
148960	152880	Because they processed words sequentially, they couldn't paralyze well, which means that you
152880	157280	couldn't just speed them up by throwing lots of GPUs at them. And when you have a model that's slow
157280	161360	to train, you can't train it on all that much data. This is where the transformer changed
161360	165920	everything. They were a model developed in 2017 by researchers at Google and the University of
165920	170640	Toronto, and they were initially designed to do translation. But unlike recurrent neural networks,
170640	174800	you could really efficiently paralyze transformers. And that meant that with the right hardware,
174800	181040	you could train some really big models. How big? Really big. Remember GPT-3, that model that writes
181040	186640	poetry and code and has conversations? That was trained at almost 45 terabytes of text data,
186640	192480	including, like, almost the entire public web. So if you remember anything about transformers,
192480	197200	let it be this. Combine a model that scales really well with a huge data set, and the results will
197200	201680	probably blow your mind. So how do these things actually work? From the diagram in the paper,
201680	207760	it should be pretty clear. Or maybe not. Actually, it's simpler than you might think. There are
207760	212400	three main innovations that make this model work so well. Positional encodings and attention,
212400	217280	and specifically a type of attention called self-attention. Let's start by talking about the
217280	222000	first one, positional encodings. Let's say we're trying to translate text from English to French.
222000	225680	Positional encodings is the idea that instead of looking at words sequentially,
225680	229040	you take each word in your sentence, and before you feed it into the neural network,
229040	234400	you slap a number on it. One, two, three, depending on what number the word is in the sentence. In
234400	238240	other words, you store information about word order in the data itself rather than in the
238240	243040	structure of the network. Then as you train the network on lots of text data, it learns how to
243040	248320	interpret those positional encodings. In this way, the neural network learns the importance of word
248320	253760	order from the data. This is a high-level way to understand positional encodings, but it's an
253760	259200	innovation that really helped make transformers easier to train than RNNs. The next innovation
259200	262800	in this paper is a concept called attention, which you'll see used everywhere in machine
262800	267600	learning these days. In fact, the title of the original transformer paper is attention is all
267600	274400	you need. So, the agreement on the European Economic Area was signed in August 1992. Did you
274400	279040	know that? That's the example sentence given in the original paper, and remember, the original
279040	283680	transformer was designed for translation. Now imagine trying to translate that sentence to
283680	289360	French. One bad way to translate text is to try to translate each word one for one. But in French,
289360	293520	some words are flipped, like in the French translation, European comes before economic.
294240	298240	Plus, French is a language that has gendered agreement between words, so the word
298240	303520	européenne needs to be in the feminine form to match with la zone. The attention mechanism
303520	307760	is a neural network structure that allows the text model to look at every single word
307760	312400	in the original sentence when making a decision about how to translate a word in the output sentence.
312400	316880	In fact, here's a nice visualization from that paper that shows what words in the input sentence
316880	321120	the model is attending to when it makes predictions about a word for the output sentence.
321920	326800	So, when the model outputs the word européenne, it's looking at the input words,
326800	331600	European, and economic. You can think of this diagram as a sort of heat map for attention.
332160	336400	And how does the model know which words it should be attending to? It's something that's
336400	341520	learned over time from data. By seeing thousands of examples of French and English sentence pairs,
341520	346080	the model learns about gender and word order and plurality and all of that grammatical stuff.
346080	350480	So we talked about two key transformer innovations, positional encoding and attention.
351040	355200	But actually, attention had been invented before this paper. The real innovation in
355200	360880	transformers was something called self-attention, a twist on traditional attention. The type of
360880	364400	attention we just talked about had to do with aligning words in English and French,
364400	368480	which is really important for translation. But what if you're just trying to understand the
368480	373440	underlying meaning in language so that you can build a network that can do any number of language
373440	378960	tasks? What's incredible about neural networks like transformers is that as they analyze tons of
378960	384160	text data, they begin to build up this internal representation or understanding of language
384160	389680	automatically. They may learn, for example, that the words programmer and software engineer and
389680	394400	software developer are all synonymous, and they might also naturally learn the rules of grammar
394400	399040	and gender and tense and so on. The better this internal representation of language the neural
399040	404000	network learns, the better it will be at any language task. And it turns out that attention
404000	408480	can be a very effective way to get a neural network to understand language if it's turned on the
408480	414640	input text itself. Let me give you an example. Take these two sentences. Server, can I have the check?
415280	420800	Versus looks like I just crashed the server. The word server here means two very different things,
420800	424160	and I know that because I'm looking at the context of the surrounding words.
424960	429040	Self-attention allows a neural network to understand a word in the context of the words
429040	434080	around it. So when a model processes the word server in the first sentence, it might be attending
434080	439920	to the word check, which helps it disambiguate from a human server versus a mental one. In the
439920	443680	second sentence, the model might be attending to the word crashed to determine that this server
443680	448480	is a machine. Self-attention can also help neural networks disambiguate words, recognize parts of
448480	453760	speech, and even identify word tense. This in a nutshell is the value of self-attention.
454480	460320	So to summarize, transformers boil down to positional encodings, attention, and self-attention.
461040	465840	Of course, this is a 10,000-foot look at transformers, but how are they actually useful?
465840	470080	One of the most popular transformer-based models is called BERT, which was invented just around
470080	475520	the time that I joined Google in 2018. BERT was trained on a massive text corpus and has become
475520	480480	this sort of general pocket knife for NLP that can be adopted to a bunch of different tasks,
481120	485920	like text summarization, question answering, classification, and finding similar sentences.
486640	490800	It's used in Google Search to help understand search queries, and it powers a lot of Google
490800	496320	Cloud's NLP tools, like Google Cloud, AutoML Natural Language. BERT also proved that you could
496400	501360	build very good models on unlabeled data, like text scraped from Wikipedia or Reddit.
501360	505040	This is called semi-supervised learning, and it's a big trend in machine learning right now.
507120	511760	So if I've sold you on how cool transformers are, you might want to start using them in your app.
511760	516800	No problem. Transurflow Hub is a great place to grab pre-trained transformer models like BERT.
516800	520720	You can download them for free in multiple languages and drop them straight into your app.
521440	525920	You can also check out the popular Transformers Python library, built by the company Hugging
525920	529840	Face. That's one of the community's favorite ways to train and use transformer models.
529840	541840	For more transformer tips, check out my blog post linked below, and thanks for watching!
