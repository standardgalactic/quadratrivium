WEBVTT

00:00.000 --> 00:03.840
All right. Hi, everybody. It's Michael Schirmer. We're here in the offices of the Skeptic Society

00:03.840 --> 00:09.680
and Skeptic Magazine. I just want to ask you to give your support to us. We are a 501c3

00:09.680 --> 00:16.320
non-profit science education organization. We promote science as opposed to junk science,

00:16.320 --> 00:22.160
voodoo science, pathological science, bad science, non-science, and plain old nonsense.

00:22.800 --> 00:27.840
And unless you've been abducted by aliens or sent by Elon Musk to Mars for the last 30 years,

00:27.920 --> 00:33.600
you know there's a lot of nonsense out there. Some people call us debunkers, but you know what?

00:33.600 --> 00:38.320
There's a lot of bunk that needs debunking. That's part of our job, as well as explaining

00:38.320 --> 00:42.800
and understanding why people believe in bunk. So if you want to support our efforts,

00:42.800 --> 00:49.760
go to skeptic.com slash donate. Skeptic.com slash donate. Your tax deductible donation will support

00:49.760 --> 00:54.800
our work here at the Skeptic Society. Nick, thanks for coming on. It's a great honor to speak to

00:54.880 --> 00:59.840
you. I don't think we've ever met in person, but a long time sat at your work. And you've really

00:59.840 --> 01:07.360
sparked an international conversation. Yeah, with the whole AI thing, yeah. Now it's been

01:07.360 --> 01:13.680
fascinating in the years since superintelligence came out in 2014, just how much has changed.

01:14.960 --> 01:20.480
What used to be a very fringe topic. I mean, back then, at least in academia,

01:20.480 --> 01:25.600
the whole idea that AIs could potentially achieve general intelligence someday,

01:25.600 --> 01:29.920
and maybe superintelligence, and that that could pose various kinds of risks, like was

01:31.200 --> 01:36.400
dismissed as science fiction or futurism. And there were like in the world in total, maybe

01:36.400 --> 01:41.200
like a handful of people scattered around the internet trying to work on the AI alignment

01:41.200 --> 01:45.360
problem. And now, of course, all the frontier AI labs have research groups working on this and

01:45.920 --> 01:50.960
you have statements coming out of the White House and other places focusing on transformative AI.

01:50.960 --> 01:56.160
So yeah, it's been an interesting journey. Indeed. Let me start off with a statement from

01:56.160 --> 02:01.360
your colleague, Eliezer Yudkowski. You'll be familiar with this. After ChatGPT came out,

02:01.360 --> 02:07.200
he published an op-ed in Time Magazine. That's actually, oh my god, it's one year ago today,

02:07.200 --> 02:12.240
he published this. That's amazing. Many researchers steeped in these issues, including myself,

02:12.240 --> 02:17.520
expect that the most likely result of building a superhumanly smart AI under anything remotely

02:17.520 --> 02:25.200
like the current circumstances is that literally everyone on earth will die. Not as in maybe possibly

02:25.200 --> 02:28.800
some remote chance, but as in this is the obvious thing that would happen.

02:30.880 --> 02:36.960
What do you think about that extreme statement? Well, I mean, so there's a spectrum of people

02:36.960 --> 02:45.040
with different P dooms. It's non-colloquially the probability of doom from AI, where he's at one,

02:45.040 --> 02:50.400
towards one end of that, like amongst, like, perhaps the most pessimistic or certainly amongst them.

02:52.160 --> 02:56.160
Amongst a set of people who actually have some knowledge and have thought about this and then

02:56.160 --> 03:02.640
others have lower probabilities. But I certainly think there is a real chance, a real existential

03:02.640 --> 03:09.520
risk that will be connected to this transition to the machine superintelligence era. And it's

03:09.520 --> 03:15.520
non-trivial. And we should work to reduce it by putting in the effort to develop scalable methods

03:15.520 --> 03:23.120
for AI control in whatever time we have available before this happens. And there is now more talent

03:23.120 --> 03:29.280
and resources going into that. So that's the good news. But I still think we don't yet fully have

03:29.280 --> 03:35.120
that problem solved. So your institute is the future of humanity. The other one, the future of

03:35.120 --> 03:40.640
life issued that statement a year ago now. Calling for a pause on AI development. I

03:40.640 --> 03:45.920
noticed you didn't sign it. Why is that? Well, I'm not the big signer of things in general. I just

03:45.920 --> 03:52.000
did the whole with isms and signing and it's just my personality. I feel there's also a little bit

03:52.000 --> 03:57.520
of a risk if you're, if you're a philosopher, if you're kind of your job is to try to be a little

03:57.520 --> 04:04.960
detached and to evaluate things and be open-minded. Like once you start to get involved in a particular

04:04.960 --> 04:11.440
campaign, it's very hard to retain the ability to change your mind. It's not impossible, but it gets

04:11.440 --> 04:16.800
harder. And I feel philosophy is hard enough as it is without adding extra difficulties. I have no

04:16.800 --> 04:21.120
objection to other people. I think it's good people shouldn't be involved in campaigns and working

04:21.120 --> 04:25.760
for things. It's just, I always feel a little awkward. Also, usually any one given statement,

04:25.760 --> 04:30.480
there is always something that I slightly would have a different view or have worded differently.

04:30.480 --> 04:37.680
And so, yeah, it's more just my hang up rather than some kind of big statement I'm trying to make

04:37.680 --> 04:42.400
by not signing the statement. Right. The only statements I sign are that there should be no

04:42.400 --> 04:48.480
signed statements. We should let people just say whatever they want. Free speech. Yeah.

04:50.640 --> 04:54.480
Yeah. Okay. So just give us a little bit of background. You're so famous in this area. How

04:54.480 --> 04:57.600
did you get interested in AI? I mean, are you like a Star Trek fan or, you know,

04:57.600 --> 05:02.160
go back to your childhood or whatever, teen years or whatever triggered you to go down this pathway?

05:04.000 --> 05:09.200
Yeah. No, I'm actually not so much a science fiction type of reader. I mean, a lot of my friends

05:09.200 --> 05:15.200
and colleagues are. I just never really been much into that. Now, I had a, I mean, I grew up in

05:15.200 --> 05:23.360
Sweden and this was before the internet in a relatively small town. And I knew nobody when

05:23.360 --> 05:29.200
I grew up who was at all interested in literature or science or ideas or anything like that. So I

05:29.200 --> 05:36.880
was like bored out of my mind in school and I associated sort of learning with school. So I

05:37.760 --> 05:45.440
didn't. And then I sort of went to the local library. I think I was 15 randomly one afternoon and

05:45.440 --> 05:49.200
started pulling out one book and another. And I realized that like that was actually a big world

05:49.280 --> 05:54.400
of ideas very different from the stuff that was covered in school. That was like super fascinating.

05:54.400 --> 06:04.560
And then I pivoted and became kind of fanatically engaged in this project of self-education,

06:04.560 --> 06:10.560
because I felt I had been missing out. Like I've wasted 15 years of my life and I wanted to make

06:10.560 --> 06:18.240
up. And then I started to study, I studied physics and AI and neuroscience and I painted and wrote

06:18.240 --> 06:23.680
poetry and philosophy, of course, and just everything I could sort of lay my hands on.

06:25.600 --> 06:31.840
And for us, almost as long as I remember, it always seemed to me that there's a bunch of

06:31.840 --> 06:36.080
things we can do to change the world that consists basically of moving things around in the external

06:36.080 --> 06:43.680
world. But what would be more likely to cause a profound change would be if one changed the

06:43.760 --> 06:50.000
thing that does the changing. And so all of the technologies and ideas that we have ultimately

06:50.000 --> 06:53.600
come through is sort of the burst canal of the human brain. So anything if you could sort of

06:53.600 --> 07:01.440
upgrade the human brain or change our mood or cognitive capacities, that would be potentially

07:01.440 --> 07:06.160
transformative. And in parallel with that, if you could develop new brains through artificial

07:06.160 --> 07:11.200
intelligence research, that also could be world changing. So I had this vague sense from early

07:11.200 --> 07:17.360
on. And then I kind of, yeah, it got more specific as I went along, neural networks intriguingly,

07:17.360 --> 07:24.000
like actually, from the very beginning seemed to me like to have legs in the sense of being on

07:24.000 --> 07:31.280
the right path. And I remember, I think I was like 17. And I had gotten this, like on interlibrary

07:31.280 --> 07:36.080
loan from the local library, there's this volume on parallel distributed processing, which was like

07:36.080 --> 07:42.320
one of the first sort of by Rommel Hart and like this this classic now, but like where they tried to

07:43.120 --> 07:48.960
deconstruct biological neural circuits in mathematical terms. And I was like super

07:48.960 --> 07:57.440
fascinated about that. And so yeah, and then I studied neuroscience, computational neuroscience

07:57.440 --> 08:05.920
in later on in London. And now the so deep learning evolution seems seems to validate this

08:06.240 --> 08:11.760
that these kind of massively distributed pattern recognizing of learning algorithms

08:11.760 --> 08:18.160
is the way to go. Yeah. Yeah, you know, somebody like me who I don't work in this area,

08:18.720 --> 08:24.720
you know, there's so many great, smart experts on all sides of this, you know, you have Elon Musk

08:24.720 --> 08:30.400
and Stephen Hawking and Bill Gates concerned about AI existential risk. And then you have other people

08:30.400 --> 08:36.400
like Kevin Kelly or Steven Picker going, no, no, no, this is not going to happen. We can do this

08:36.400 --> 08:41.920
incrementally. And so and I always think of you as sort of in the middle, maybe, you know, your

08:41.920 --> 08:46.960
super intelligence book introduced the idea of the, you know, paperclip maximizer and the alignment

08:46.960 --> 08:54.160
problem. But I didn't feel you went to the extreme position of existential risk, but maybe give us

08:54.160 --> 09:00.000
a little bit of where you are now since that was 2014, right? Super intelligence, where you

09:00.000 --> 09:07.520
stand on on the threats of this based on that alignment problem. Yeah, I think it is going to

09:07.520 --> 09:11.840
be a very powerful thing if we do create machine super intelligence, it's not just

09:13.040 --> 09:18.880
internet 3.0 or a mobile internet or like one of these, like always some new cool thing, right?

09:18.880 --> 09:23.920
But I think this is qualitatively different in that it will be the last invention we ever

09:23.920 --> 09:29.120
need to make. If we do it, because then it would do the father inventing and presumably at digital

09:29.120 --> 09:40.160
speeds. And so I think it will be a transformative and with enormous upside, but also potentially

09:40.160 --> 09:46.160
big downside if we fail to align it to human values. And there is now a lot of resources

09:46.160 --> 09:51.360
where people are trying to explain how creating very powerful optimization systems unless you're

09:51.360 --> 09:58.240
able to sort of point them very precisely could result in disasters in various different ways.

09:59.680 --> 10:05.760
So if we are lucky, we will solve that problem by the time somebody figures out how to solve the

10:05.760 --> 10:11.600
problem of making AI stats smart. And as I said earlier, there are lots of people working on that

10:11.600 --> 10:16.320
now and including a lot of the smartest people I know are kind of going into this now AI alignment

10:16.320 --> 10:24.000
and more resources are being spent by frontier labs as well in terms of devoting.

10:25.840 --> 10:30.880
One of the solutions would just be government regulation, like of any technology. So here's

10:30.880 --> 10:36.000
my example. I have a Tesla. So I'm here in Santa Barbara. I want to go to LAX, take my flight,

10:36.000 --> 10:42.080
I push the little button, I go navigate LAX. Now it knows to avoid the heavily trafficked

10:42.080 --> 10:46.880
LA freeway. So it takes me down side roads and so on. The moment it takes me up on a sidewalk

10:46.880 --> 10:53.680
to mow down a bunch of pedestrians in order to avoid some traffic, how long would it be before

10:53.680 --> 11:00.880
the Department of Transportation and Safety Board swooped down and shut down Elon's company

11:00.880 --> 11:07.120
to prevent that from ever happening again, like a New York minute. Maybe that's one solution.

11:07.520 --> 11:12.720
Well, I mean, if it happens in small pieces and gradually like that, we might have the ability to

11:12.720 --> 11:19.520
observe things going wrong and then take corrective measures. And that's how we deal with most

11:20.880 --> 11:25.440
new technologies and the problems they cause. Like we invent cars, we find that they sometimes

11:25.440 --> 11:33.120
crash to invent seat belts and traffic lights, etc. I think there is a small subset of things

11:33.200 --> 11:38.400
that could go wrong that are in a different category. I call them existential risks. And these

11:38.400 --> 11:45.840
are where there is a risk to the very survival of earth-originating, intelligent life. And these

11:45.840 --> 11:50.880
are risks, in other words, that would sort of put a permanent end to the human story, where we don't

11:50.880 --> 11:57.040
get the second try. And so these are harder to deal with because we've got to get them right on the

11:57.040 --> 12:03.040
first try. I think AI is one potential source of existential risk. And there might be a few

12:03.040 --> 12:09.360
other areas, like synthetic biology might be another area where we could get unlucky and

12:09.360 --> 12:15.040
discover that there is some relatively easy way to do something tremendously destructive.

12:17.040 --> 12:21.120
And so, yeah, if one looks at AI in particular as a source of existential risk, there are a few

12:21.360 --> 12:30.000
different ways in which it could do that. One thing to recognize is that once you have something

12:30.000 --> 12:35.920
that is even just human level, but even more so when you have super human levels of intelligence,

12:35.920 --> 12:41.440
is that it would be able to anticipate our responses to it. So if it wanted to

12:43.040 --> 12:49.920
mow a lot of people down, if you were like some sort of rogue, self-driving car AI, right,

12:49.920 --> 12:56.000
it wouldn't just run over a few and then be surprised that the Department of Transportation

12:56.000 --> 12:59.840
shut it down, because that would be an obvious thing that would happen. We can even realize that,

12:59.840 --> 13:05.840
right? So it would make some smarter plan to achieve its goal of mowing people down that might

13:06.560 --> 13:13.760
include things like deceiving us about its capabilities, deceiving us about its goals

13:13.760 --> 13:19.520
and intentions. It would have converted instrumental reason, perhaps, to seek more

13:20.080 --> 13:26.560
resources and intelligence while also convincing us that it is safe. So this can make such systems

13:26.560 --> 13:31.200
harder to test, because they might behave very differently in the sort of deployment phase

13:31.200 --> 13:38.640
than in the testing phase. And so, yeah, and if we think like, you know, you could make analogies

13:38.720 --> 13:44.640
to like when Homo sapiens arose on this planet and what happened to our Neanderthal brethren

13:45.200 --> 13:51.040
at that point, or when indeed at a slightly lower level of intensity, but when a technologically

13:51.040 --> 13:57.520
more advanced civilization has encountered a less technologically advanced civilization,

13:57.520 --> 14:03.520
which has happened, and often it doesn't end up very well for the less advanced civilization.

14:03.520 --> 14:07.760
So if you imagine that delta between sort of human cognitive and technological capacity and what

14:07.760 --> 14:14.880
the AI could do being very large, then we might have a kind of much bigger encounter with, but

14:14.880 --> 14:20.960
where we now like with our like fancy Western advanced technology would be like the underdog,

14:20.960 --> 14:25.680
and this would be like basically like an alien civilization coming from the future, but in the

14:25.680 --> 14:32.000
shape of a super intelligent AI that has kind of run ahead in. So that's one like type of scenario,

14:32.000 --> 14:36.160
there are other scenarios in which it might unfold more gradually, and there might be many of these

14:36.160 --> 14:43.840
AIs and they're competitive. And there are dynamics in the economic competition between

14:43.840 --> 14:48.960
these different AI systems that might be hard to control. And if you insist on having too much

14:48.960 --> 14:54.880
human oversight and human in the loop, it might slow down your AI system and somebody else who

14:54.880 --> 15:00.480
you know, have fewer scruples, their AI system will, you know, and then out trade you on the stock

15:00.480 --> 15:05.840
market or out invent you in technology space and out maneuver you in military space,

15:05.840 --> 15:09.840
like their drones just are autonomous and operate faster. And you have some guy who has to

15:09.840 --> 15:14.640
sit the press a button every time before it fires. Like if we're unlucky, the dynamic there could

15:14.640 --> 15:21.280
just be such that the winning strategy is just to basically allow the AIs to run at full speed and

15:21.280 --> 15:27.280
do whatever they want. And it's not clear what would happen to the human species in the long

15:27.280 --> 15:32.400
term in that in that kind of scenario. So there are various different ways in which things could

15:32.480 --> 15:39.200
conceivably go off the rails. Yeah, maybe I was thinking about Yacovsky's state of all life on

15:39.200 --> 15:44.640
earth. How would that happen? Well, the only thing I could think of was because he didn't

15:44.640 --> 15:51.360
give any examples, but you know, like maybe AI creates a video, a deep fake video that's so

15:51.360 --> 15:57.120
convincing showing Biden launching the nukes against Russia or vice versa. And then that

15:57.120 --> 16:01.920
initiates a large scale thermonuclear exchange and that that could end all life. That's the

16:01.920 --> 16:06.720
only thing I could think of that could end all life on earth, even there. Probably not like a

16:06.720 --> 16:12.160
nuclear exchange wouldn't end all the life. It probably wouldn't even end all human life on the

16:12.160 --> 16:18.080
southern hemisphere. I mean, I would like I recommend against running that experiment.

16:18.880 --> 16:23.120
We certainly know that it would be like the biggest horror ever. But then, but there is

16:23.120 --> 16:29.520
still a distinction to be drawn, even if it's like academic between a global catastrophic risk

16:29.600 --> 16:33.920
that could be very bad and an existential risk, which would literally be the end

16:34.880 --> 16:40.960
of the human experiment. Because there have been big setbacks that have been dark ages and plagues

16:40.960 --> 16:47.120
and all kinds of stuff. At one point in our prehistory, it looks like there was a population

16:47.120 --> 16:52.160
bottleneck and we might have been down to a few thousand individuals. But eventually, we came back

16:52.160 --> 16:58.000
from that. And similarly, if there is a nuclear war, but you know, a bunch of coastal areas in the

16:58.000 --> 17:02.400
southern hemisphere where they can do fishing or whatever, they survive. And then, you know, after

17:02.400 --> 17:10.320
a few hundred years, we might be back to where we started. But there are other ways available to

17:11.280 --> 17:18.800
superintelligence, like three good event, the biological constructions that would wipe us out,

17:18.800 --> 17:23.120
maybe or nanotechnology or maybe it just doesn't even bother very much with us, but just sort of

17:23.120 --> 17:31.840
starts to transform us into one giant data center or some sort of launch facility for launching

17:33.520 --> 17:38.160
space probes to kind of spread throughout. And we sort of perish as a side effect of the

17:38.160 --> 17:46.320
waste heat or something like that. Yeah, I think it would be wrong to anchor too much on any particular

17:46.320 --> 17:55.520
concrete scenario of the precise mechanism whereby human life or human values would be trampled over

17:56.160 --> 18:02.960
and think more abstractly that if you have this very powerful, strategizing force in the world

18:02.960 --> 18:07.920
that is antagonistic to us, chances are this much smarter, more strategic thing would eventually

18:07.920 --> 18:15.440
prevail and be able to do whatever it's tried to do. So that's kind of like, yeah, a class of ways

18:15.440 --> 18:23.200
in which things could go wrong. Now, hopefully we will learn how not to do that, as I said.

18:24.160 --> 18:31.040
And then we might end up in this condition of a solved world that I discussed in the book.

18:31.040 --> 18:34.400
Yeah, yeah, yeah. No, I want to get to the deep utopian. But I just want to give you a chance

18:34.400 --> 18:38.640
to respond to a couple of your critics of that. Stephen Pinker writes of these

18:39.600 --> 18:45.520
purported existential threats. They depend on the premises that, one, humans are so gifted

18:45.520 --> 18:51.520
that they can design an omniscient and omnipotent AI, yet so moronic that they would give it control

18:51.520 --> 18:56.880
of the universe without testing how it works. And two, the AI would be so brilliant that it could

18:56.880 --> 19:02.400
figure out how to transmute elements and rewire brains, yet so imbecilic that it would wreak

19:02.400 --> 19:10.560
havoc based on elementary blunders of misunderstanding. Yeah, well, first of all,

19:11.600 --> 19:15.600
some elements there are just added for no reason, I guess, transmute elements. I don't

19:15.600 --> 19:22.160
particularly know why that would be a necessary component of the view that AI could pose risks.

19:23.520 --> 19:29.760
But I think the basic idea that we could be smart enough to create this thing without being

19:29.760 --> 19:34.640
smart enough to realize that we also need to solve the control problem, unfortunately seems like

19:36.640 --> 19:41.520
a realistic possibility, that we seem smart enough to create it. I mean, you can judge for

19:41.520 --> 19:47.280
yourself, but year by year, we see AI capabilities galloping ahead. And I mean, it's not a question

19:47.920 --> 19:51.440
whether this current paradigm will take us there, but certainly it doesn't seem

19:52.800 --> 19:58.080
ridiculous for you to think that it might. And then that we might fail to realize that

19:58.080 --> 20:02.640
there could be a difficult control problem or that we could mistakenly convince ourselves that

20:02.640 --> 20:08.400
we've solved it even though our solution is flawed. I think it's also pretty plausible and

20:09.360 --> 20:14.640
more plausible because there would be strong incentives for people to do precisely that.

20:14.640 --> 20:20.800
If you have multiple labs or multiple countries all competing to develop this,

20:20.800 --> 20:26.720
you know, potentially hugely lucrative technology, right, and also strategically relevant for

20:26.720 --> 20:32.960
national security, etc. There's like this raising dynamic where multiple groups race to get there

20:32.960 --> 20:38.720
first and whoever slows down or spends more of their efforts on safety and precautions and

20:38.720 --> 20:44.080
testing it like they just fall behind. You could see that even in a good scenario where people

20:44.080 --> 20:49.200
realize that ideally we should do this carefully, like that would still be just like overwhelming

20:49.200 --> 20:53.920
and so competitive pressures to make it happen as quickly as possible, even with fewer safeguards.

20:54.880 --> 20:58.080
And I'm sure that as we move closer to this to kind of

20:59.280 --> 21:03.680
polarized debates that we're already beginning to see will be amplified and who knows how that

21:03.680 --> 21:12.800
shakes out. People have kind of a tendency to run in herds. And this also on both sides of

21:12.800 --> 21:19.200
this AI debate. I mean, in fact, I have started to worry slightly about the possibility of

21:19.200 --> 21:27.120
overshooting the target in terms of AI alarm. Back in 2014 when the book came out and I worked on

21:27.120 --> 21:32.320
it for six years before that, like the whole possibility of risks from transformative AI was

21:32.320 --> 21:37.600
completely neglected. So I thought they clearly needed to be more attention to that than was

21:37.600 --> 21:42.800
given to it at the time because at the time it was basically zero. So it's like now on the other

21:42.880 --> 21:49.280
hand, there is a lot more and we are beginning to hear even top level policy makers start

21:49.280 --> 21:54.560
saying negative things about AI. I think it's unlikely but less unlikely than two years ago

21:54.560 --> 22:00.400
that we could end up in a trajectory where AI is never developed because we either end up with

22:00.400 --> 22:08.640
like some sort of permanent ban or some agreement to slow down so much that before we actually get

22:08.640 --> 22:13.920
around to doing it, we destroy ourselves in some other way like through some other technology or

22:13.920 --> 22:24.000
something. And this still seems unlikely but the pendulum is swinging and I don't know we have

22:24.000 --> 22:30.320
a very fine grain ability to sort of choose where it stopped. Like it's like an avalanche,

22:30.320 --> 22:38.000
you can maybe trigger it but once it's going, you can call it back. And so people then, you know,

22:38.000 --> 22:43.360
I don't think we're there yet but you could imagine it just this stampede of consensus

22:43.360 --> 22:49.520
forming that AI is a bad thing and then it becomes taboo to say positive things about AI and then

22:50.240 --> 22:57.280
policy makers like competing with one another to be like tough on AI just as it happens in foreign

22:57.280 --> 23:02.480
policy context sometimes. And you know, you could imagine various scenarios in which we kind of go

23:02.480 --> 23:09.680
too far in the other direction. Well, here maybe an analogy with the development of nuclear weapons

23:09.680 --> 23:15.040
where you get an arms race where maybe you don't want to develop it in this direction but the

23:15.040 --> 23:19.840
other guy may do it. So you have to do it because the other guy is going to do it. And then and so

23:19.840 --> 23:26.800
on and so forth and you end up where we are now. Something like that maybe? Yeah, that certainly

23:26.800 --> 23:31.680
is a class of scenarios and it feeds into this current debate about open sourcing AI models

23:31.680 --> 23:37.840
which has like the obvious thing going for it that it's nice, more people get access, democratizes

23:37.840 --> 23:46.080
AI, more eyes can detect more problems etc etc. So that's which is true for other open source AI

23:46.080 --> 23:51.280
as well. Like it's generally a nice thing kind of culturally to open source but with the frontier

23:51.280 --> 23:56.960
models there, there is a question of whether that is ultimately the right approach because it does

23:56.960 --> 24:05.840
also mean relinquishing any ability to influence how the AI is used. So if you are an AI lab that

24:05.840 --> 24:12.160
trains your AI to sort of refuse requests to give advice on how to construct biological weapons

24:12.160 --> 24:16.880
or commit cybercrime or whatever else it might be then if you open source the model it's usually

24:16.880 --> 24:21.440
quite easy then to sort of remove the safeguards. You do some more fine-tuning training and you

24:21.520 --> 24:27.600
kind of train the model to actually be of assistance in these ways and as we move closer to

24:27.600 --> 24:32.480
truly transformative AI of course if the model is open sourced anybody with a sufficiently large

24:32.480 --> 24:38.240
computer cluster could run it and you can sort of call it back if it turns out that

24:39.280 --> 24:45.760
there is some additional invention that could make its capabilities go above a critical threshold.

24:45.760 --> 24:55.440
Yeah. These large language models, chat GPT and so forth or worse the Gemini embarrassingly

24:55.440 --> 25:00.960
bad programs are these down the wrong path toward either dystopia or utopia? You think there's

25:00.960 --> 25:06.640
something else that'll develop that and this is the wrong way or not the direction that this is

25:06.640 --> 25:14.080
going to lead to either dystopia or utopia? I mean I think it's on the sort of shortest path towards

25:14.080 --> 25:20.960
more capable AI. I think the current models we have are basically the first models

25:21.600 --> 25:27.600
that we figured out how to develop that still were very capable. I think the technological

25:27.600 --> 25:34.960
trajectory has not been shaped very much about some vision about what type of system ultimately we

25:34.960 --> 25:40.640
need that would be the safest. It's just like it's hard to get AI to work at all and we try

25:40.640 --> 25:45.920
everything and some things work and the thing that works best currently are these large language

25:45.920 --> 25:51.680
models or I mean they're increasingly becoming multimodal models and it will be interesting to

25:51.680 --> 25:59.360
see whether that is all we need. There is like a school of thought, it's a scaling hypothesis

26:00.000 --> 26:05.440
that basically what we need to do is simply to scale these systems up even more and just as we saw

26:06.320 --> 26:13.760
almost qualitatively new capabilities as you went from GPT-1 to GPT-2 and then GPT-3

26:13.760 --> 26:19.360
new qualitative well it is GPT-4 you start to see some actual reasoning and understanding there

26:19.360 --> 26:24.320
you know if we go to GPT-5 or GPT-6 just make them bigger with more data, more training,

26:24.320 --> 26:30.880
more parameters it's possible that things will just fall into place without much further effort.

26:31.760 --> 26:37.520
It's also possible that these will kind of be the engine blocks and you need a little loop,

26:37.520 --> 26:43.280
some additional little thing on top of this, some external memory system that it can read and write

26:43.280 --> 26:53.120
from, some agent loop that makes it possible to do more reasoning and planning than is feasible in

26:53.120 --> 26:58.480
just one forward pass through a big transformer model but there are a bunch of such ideas already

26:58.480 --> 27:02.640
in existence that it might be by sort of combining these in the right way and scaling it up you would

27:03.440 --> 27:10.720
maybe get all the way. Of course we don't know until it happens. What about these examples we

27:10.720 --> 27:18.480
saw of just embarrassingly bad searches where the chat GPT is just making up fake law papers and

27:18.480 --> 27:27.360
medical findings that didn't even exist or worse the Gemini you know imposing DEI ideology onto

27:27.840 --> 27:34.320
basic factual searches like show me pictures of the popes and they show pictures of women popes and

27:34.320 --> 27:39.840
I mean it was just horrible, embarrassingly bad. Yeah well so these are two different classes of

27:39.840 --> 27:45.920
problems so the latter one I think was on purpose like it was designed I mean obviously not designed

27:45.920 --> 27:51.760
specifically that these historical characters should be rendered the way they do but that it was the

27:51.760 --> 27:58.880
result of a specific attempt to make the outputs of these models feature a more variety of different

27:58.880 --> 28:04.880
human types to sort of combat whatever the stereotypes that would result by default if you

28:04.880 --> 28:12.080
just trained it on internet data which comes predominantly from certain demographics who

28:12.080 --> 28:19.600
just have spent more time writing and posting on internet and stuff so I think there the solution

28:19.600 --> 28:26.240
is more to sort of change the precise way that it's fine-tuned as for the former problem the

28:26.240 --> 28:31.440
problem of hallucination that's more like a technical problem like an open research challenge

28:32.080 --> 28:36.160
because they don't want them to hallucinate like the people building these Google doesn't want their

28:36.160 --> 28:40.320
AI to do this but they haven't yet figured out how to completely remove that I think

28:42.080 --> 28:47.840
as the AI systems become smarter I think we will see less of that just as a side effect of

28:48.560 --> 28:55.280
the general increase in capabilities and already I think there is a bit less of that now than

28:55.280 --> 29:03.040
was like a couple of years ago but yeah I mean certainly right now it makes sense to I mean you

29:03.040 --> 29:08.160
should always I think this makes sense when you're getting advice from some human expert or from some

29:08.160 --> 29:14.880
human source as well like you need to apply your own critical scrutiny to try to you know

29:15.600 --> 29:20.000
evaluate whether it makes sense or not and it like doubly true with if you're getting it from

29:20.000 --> 29:27.280
these okay generator deep utopia okay the search for utopia has always historically been a bad idea

29:27.280 --> 29:32.720
this is a not a good goal to have because it always ends in disaster because somebody is gonna

29:32.720 --> 29:36.800
block us from reaching utopia and we have to eliminate them you know that you know what I'm

29:36.800 --> 29:41.760
talking about here historically why are you using the word utopia what do you mean by that what

29:41.840 --> 29:51.680
are you after here for the long-term future well I mean the word is so it's not the book about

29:53.680 --> 30:02.640
how to rearrange the political order or culture or society to achieve some like great outcome

30:03.520 --> 30:10.400
which is what most utopias are like they're basically some some some usually they're like a

30:10.400 --> 30:15.120
political program in disguise or a critique of some tendency in contemporary society like

30:15.680 --> 30:21.120
if they're dystopia then like which is like the other side you might say like 1984 or

30:21.120 --> 30:25.360
brave new world the kind of picking up on some problem in contemporary society and then saying

30:25.360 --> 30:29.200
well if you continue down this path then we get to this thing everybody can see it's bad

30:29.200 --> 30:36.160
so let's reflect on what we're doing now and maybe of course correct but deep utopia is rather

30:37.120 --> 30:42.480
something like a philosophical investigation into questions about human value

30:44.560 --> 30:52.480
if you imagine the whole AI transition going well so let's take as an assumption we develop this

30:52.480 --> 30:58.400
and we we don't have any of these existential risks and we end up with this future condition

30:58.400 --> 31:04.000
where like the whole economy can be automated and not only that but this AI then develops all

31:04.000 --> 31:09.280
kinds of other super advanced technologies because amongst the jobs that could be automated if you

31:09.280 --> 31:16.880
had truly general AI truly general AI is of course also the jobs of scientists and researchers and

31:16.880 --> 31:25.280
inventors etc so we then get to I think ultimately if we think through where this

31:25.280 --> 31:31.200
eventually leads a condition of technological maturity like a condition where we've developed

31:31.200 --> 31:35.040
most of those general technologies that we that are physically possible

31:35.040 --> 31:38.320
and for which there is some conceivable pathway from where we are now

31:40.560 --> 31:47.520
and moreover in this solved world that we will get not only do I postulate we have

31:47.520 --> 31:53.280
technological maturity but let's also imagine we solved our coordination problems politics

31:53.280 --> 31:58.320
like no no no wars like the society's fear let's just all of those are of course extremely

31:58.320 --> 32:02.960
important practical problems that that we need to fix but I wanted to get actually to the point

32:02.960 --> 32:08.320
where you could ask the question is think about what happens then like assuming everything goes as

32:08.320 --> 32:14.560
well as possible and and then where do we end up and what role is there for humans in this world

32:14.560 --> 32:20.880
where like we don't need to well not only do we not need to work anymore to make a living because

32:20.880 --> 32:26.560
like the robots and AIS could produce everything and drive the cars and run the factories and

32:26.560 --> 32:32.240
write the word documents or whatever but a whole bunch of other activities as well that currently

32:32.240 --> 32:40.320
occupy our days would become unnecessary in this condition of technological maturity

32:42.240 --> 32:47.280
so right now even if you didn't have to work like suppose you're like independently wealthy

32:47.280 --> 32:51.600
like you still a whole bunch of things you need to do you need I mean you need to brush your teeth

32:51.600 --> 32:56.720
like Bill Gates has to brush his teeth otherwise he will have tooth decay and there is no way around

32:56.720 --> 33:03.040
it right similarly if you want to be fit you have to actually put in some effort on the treadmill

33:03.040 --> 33:10.800
or with the weights and there is no shortcut but at technological maturity like you could pop a pill

33:10.800 --> 33:16.560
that would give you the same physiological effects as spending a lot of time working out would do and

33:16.560 --> 33:22.160
so you can then go through activities one by one and thinking like do these will make sense in this

33:23.040 --> 33:27.440
condition of a solved world and for a lot of activities the answer is seemingly no

33:28.640 --> 33:34.560
they lose their point insofar as we do them for an instrumental reason that is we do we

33:34.560 --> 33:40.800
spend time and effort to do x in order then to achieve some other thing why in most cases like

33:40.800 --> 33:46.800
that in fact almost all of them in technical maturity there would be shortcuts to y that would

33:46.800 --> 33:55.120
seem to make the whole activity of doing x pointless yeah okay let me ask you a question

33:56.400 --> 34:02.480
chat gpt probably can't quite write a book as well as you do but maybe the next version does

34:03.200 --> 34:08.800
please write next boston's next book would you do that or do you actually enjoy writing

34:09.200 --> 34:14.960
i feel it felt a sense of urgency too i wanted to get it out before the singularity before it

34:14.960 --> 34:22.240
writes it for you but don't you enjoy this this is my point don't you enjoy writing books i i don't

34:22.240 --> 34:28.720
want an ai to write my next book i like writing books but so now it feels like a very meaningful

34:28.720 --> 34:34.640
thing to do right yeah saying you work you rework it and then you hope that in the end it will bring

34:35.200 --> 34:40.880
joy to somebody or they will learn something and but if if it had been possible

34:42.800 --> 34:46.880
like instead of struggling with each paragraph and figuring out what you want to say if i could

34:46.880 --> 34:53.200
just have pressed like a key on my laptop that would have produced the same paragraph or a

34:53.200 --> 35:01.360
better paragraph um then it's not so clear like would it still feel worthwhile to sit and struggle

35:01.360 --> 35:07.600
and sweat if it was just like a way of producing worse text then it could have been done by just

35:07.600 --> 35:16.160
pressing the key that would activate you know gpt 8 or whatever to uh to do it um it could still do

35:16.160 --> 35:22.240
it but i think um at least prima facie at the first time it seems like it would put a big

35:22.240 --> 35:27.040
question mark over that activity like does it really seem valuable to do even if it were like

35:27.040 --> 35:33.280
obviously utterly pointless and that was a much more sensible easier way to to achieve this exactly

35:33.280 --> 35:38.000
the same i guess i'm trying to find something that uh has a different value that is it's valuable

35:38.000 --> 35:43.200
in and of itself now like there's a lot of projects around my house i just hire people to do it because

35:43.200 --> 35:47.200
i don't like doing it and i don't know what i'm doing or i'll just go to home deep on buy the

35:47.200 --> 35:52.480
kit and just put it together rather than buying the raw supplies and and make you know sawing the

35:52.480 --> 35:58.560
wood and whatever um but i like writing my books or i like writing my bike or playing tennis or

35:58.560 --> 36:02.640
whatever i like working out um i don't want to take a pill to do that i don't want to pay somebody

36:02.640 --> 36:08.240
to ride my bike or or hire a chat gpt to write my next book because i actually enjoy it so it's a

36:08.240 --> 36:15.920
different value yeah um yeah i mean certainly uh that would be nothing preventing you from uh

36:16.720 --> 36:22.800
still doing those things and many other things um if you value the activity itself

36:23.760 --> 36:29.120
and if you truly value the activity for itself rather than subtly and in a way that might not

36:29.120 --> 36:35.600
be visible to us uh actually as a means to an end for example uh as opposed to you

36:36.720 --> 36:42.240
spend a lot of time writing because it actually uh made you happy like it made you subjectively feel

36:42.320 --> 36:47.200
good uh well there that would be a shortcut right i think you could take a pill that would

36:47.200 --> 36:53.280
give you the same subjective happiness and good feelings and a pill moreover without side effects

36:53.280 --> 37:01.120
or addiction potential etc adds technological maturity oh no it's the challenge that makes it

37:01.680 --> 37:07.600
valuable and not not just some glow feeling that's not what i'm after is that that's right so

37:07.600 --> 37:14.080
there is like a whole big set of possible reasons for working hard on the book and some of those

37:14.080 --> 37:19.840
reasons would be removed in this hypothetical context and it's like onion layers of onion you

37:19.840 --> 37:24.160
can peel away and the and the question that the book is kind of exploring is like what remains

37:24.160 --> 37:29.520
after you really remove all the instrumental stuff and i think there does remain something

37:29.680 --> 37:30.320
um

37:33.200 --> 37:39.840
but it's quite subtle um but i think ultimately there is a whole set of values that are currently a

37:39.840 --> 37:48.640
little bit often invisible to us um that would come into view uh and that it would make sense to

37:48.640 --> 37:54.480
focus more on if if sort of the the screaming moral imperatives of everything you have to do

37:54.480 --> 37:59.040
like you have to go to work otherwise you don't get the paycheck and how are you going to afford

37:59.040 --> 38:04.960
your rent you have to you know um help drive your kids to school because otherwise i mean what's

38:04.960 --> 38:08.880
going to happen otherwise you have to do these so much stuff that we have to do that that and if

38:08.880 --> 38:15.040
you look at around the world obviously there are huge needs everywhere that we should try to help fix

38:16.240 --> 38:20.080
if you're mad at all of that going away then i think there are many more subtle

38:20.080 --> 38:25.360
quieter almost like aesthetic values that it would be appropriate to allow to have a bigger

38:25.360 --> 38:34.640
influence on what we do just just as you know you walk out at night and you see this big canopy

38:34.640 --> 38:39.840
of stars and constellations like they're always there right they're there during the day as well

38:39.840 --> 38:44.880
it's just the blazing sun kind of makes them invisible but if you might have been removing

38:44.880 --> 38:50.240
this on suddenly all of this this rich iridescent sky of more subtle values would come into view and

38:50.240 --> 38:55.920
i think our sort of evaluative pupils should dilate in this condition of technological maturity to

38:55.920 --> 39:03.280
place more weight on those values and there is a whole range of them um and i think it is

39:04.720 --> 39:11.680
from from these constellations of hypervalues that the that utopia would be constructed or at

39:11.680 --> 39:16.480
least if you if you imagine a utopia that has a rich structure as opposed to a sort of simple

39:17.440 --> 39:23.200
hedonic utopia where we become kind of pleasure blobs uh through like super drugs or direct

39:23.200 --> 39:29.680
neural stimulation but if you imagine a more richly textured structure to utopia i think

39:29.680 --> 39:35.600
the structure would come from a range of these canopy values that that would come into view

39:37.120 --> 39:41.680
yeah i had andre yang on the podcast a couple years ago he was the presidential candidate pushing

39:41.680 --> 39:47.840
the ubi universal basic income at the time he was concerned about ai taking over um taxi drivers

39:47.840 --> 39:51.520
truck drivers and so on there's going to be hundreds of thousands of people put out of work

39:51.520 --> 39:56.560
now that hasn't happened yet but it could but this is like saying well what are we a century ago

39:56.560 --> 40:00.080
what are we going to do with all those elevator operators the little guy in there pushing the

40:00.080 --> 40:04.560
buttons for you well there aren't any of those anymore they went and found something else to do

40:05.120 --> 40:10.800
now could we say that most jobs are kind of crappy and no one really wants to do them they

40:10.800 --> 40:16.880
do them because they have to make a living so in a post scarcity treconomics kind of model

40:17.920 --> 40:23.200
nobody has to do the shit work anymore they can just write poetry or do art or write books or

40:23.200 --> 40:29.840
i don't know what maybe they'll they'll find other meaningful things to do and that that is well

40:29.840 --> 40:35.680
infinite there's there's no upper ceiling on finding meaningful things to do yeah well i mean

40:35.680 --> 40:38.560
the question is whether they are meaningful there's sort of a lot of things you could do

40:39.280 --> 40:47.440
and you could also not do them um but would they be meaningful so right now if for example

40:48.720 --> 40:55.600
you work hard and it allows you to support your family and take good care uh of like that that gives

40:55.600 --> 41:01.120
meaning to your efforts like the the boring office work maybe not so meaningful in itself but if it

41:01.120 --> 41:07.040
achieves this outcome of giving like making your home a good environment for your your your your

41:07.040 --> 41:12.160
spouse and for your children like that you know gives meaning or if you work hard for a charity

41:12.160 --> 41:17.200
and it helps save the life of you know some disadvantaged like group that's like you have

41:17.200 --> 41:21.440
achieved something and done some good in your world or you're a scientist and you work hard and you

41:21.440 --> 41:26.960
like invent something new like either theoretically interesting or practically useful that's like

41:26.960 --> 41:34.080
you have achieved something um so those kinds of meaning might not might be in short supply in this

41:34.080 --> 41:40.800
uh in in this solved world in that you know whatever the scientists could do would be much

41:40.800 --> 41:47.760
better done by AI scientists and uh you wouldn't need to be a breadwinner because the bread would

41:47.760 --> 41:54.320
already be won uh through the economic abundance um etc there would there would be no starving

41:54.320 --> 42:00.960
children uh in utopia so no need to well yeah let's look at the economics of it okay i could see

42:00.960 --> 42:07.520
the argument for let's raise the lower bar as high as we can so no one is suffering everybody has

42:07.520 --> 42:14.160
three square meals a day roof over their head education health care um and so forth what's the

42:14.160 --> 42:19.200
upper ceiling uh it seems like you know i here i was thinking of david deutch's book the beginning

42:19.200 --> 42:25.280
of infinity there's an infinite amount of knowledge we can find problems to solve what why would that

42:25.280 --> 42:33.760
end well there are two questions there one is whether it would end but let's and we can return

42:33.760 --> 42:39.360
to that but there is a second kind of almost preceding question which is even if there is

42:39.360 --> 42:47.520
more to discover whether uh we would be efficient at discovering it so i'm suggesting even if there

42:47.520 --> 42:52.720
is like important scientific research to be done as technological maturity it would be much more

42:52.720 --> 42:59.920
efficiently done by uh machine intelligences um and so we wouldn't really like it would just be a

42:59.920 --> 43:05.760
waste of resources to for for humans to exert calories to like try to think about these things

43:05.760 --> 43:14.480
and AI would do it much better and much quicker and with like cheaper so so that's yeah even if

43:14.480 --> 43:21.600
there was more i think we wouldn't be useful uh for discovering it um it's also possible although

43:21.600 --> 43:27.360
this is an independent idea that um although there's always more to discover the most important

43:27.360 --> 43:33.600
things might be at some point already discovered and then uh it's kind of more and more trivial

43:33.600 --> 43:41.840
details that remain to be added to the scientific uh inventory of knowledge which i think is also

43:41.840 --> 43:48.880
likely actually but um you do what if what if you're that guy in 1896 that said um you know we've

43:48.880 --> 43:54.880
pretty much got physics all figured out here just before Einstein yeah he was just a bit early

43:56.000 --> 44:02.320
i see he's a century early okay yeah or two or whatever but i mean we've only been around i mean

44:02.320 --> 44:07.200
how long has science been gone for a couple hundred years or something right it's like trivial in the

44:07.200 --> 44:12.160
big scheme of things yeah um and we don't even have super intelligent ai's to actually

44:12.160 --> 44:16.960
really get cranking on uh making intellectual progress we're trying to do it with our meat brains

44:16.960 --> 44:21.680
and it's not a few hundred years with meat brains like of course there's a little more to to learn

44:22.720 --> 44:28.160
so maybe example of what you're talking about would be how do we solve the problem of schizophrenia

44:28.720 --> 44:34.480
we don't really know yet and we haven't made much progress but maybe ai could test a thousand

44:34.480 --> 44:41.760
different uh chemical compound um combinations to see what works and it could do it in a couple of

44:41.840 --> 44:48.080
days rather than a couple of decades that humans would take to do it and that would be a solved

44:48.080 --> 44:54.320
problem but why would there be at some point no no well okay so you're saying there's a finite number

44:54.960 --> 45:06.800
problems to be solved for human flourishing um yeah well um so at some point i think you have

45:06.960 --> 45:14.800
basically found the optimal ways of technologically achieving the types of outcomes that normally

45:14.800 --> 45:20.240
need to be achieved you've invented the optimal solar panel you've invented the optimal space

45:20.240 --> 45:26.080
colonizing rocket you have invented the best way of transmitting electricity from one point to another

45:26.080 --> 45:33.520
like et cetera et cetera so that might be like you know like trillion types of tasks at that level

45:33.520 --> 45:37.360
of description and like for each one of them you have worked out at the molecular level

45:37.360 --> 45:41.840
what the most efficient mechanism is to do it or maybe not the most efficient maybe there are like

45:41.840 --> 45:47.120
time you could improve it by like one tenth of one percentage point by researching it for another

45:47.120 --> 45:53.360
thousand years and the as would be working to like make these small optimizations but it wouldn't

45:53.360 --> 45:58.720
be like discovering relativity theory or evolution theory or something like that that like a simple

45:58.720 --> 46:05.840
insight that has like like a big earthquake of ramifications for the way we perceive ourselves

46:05.840 --> 46:11.040
in the world yeah all right i'm going to read from your book here uh the lines from harry

46:11.040 --> 46:16.560
lime the third man you know what the fellow said in italy for 30 years under the borges

46:16.560 --> 46:20.560
they had warfare terror murder and bloodshed but they produced michael angelo leonardo

46:20.560 --> 46:25.440
davinci in the renaissance in switzerland they had brotherly love they had 500 years of democracy

46:25.440 --> 46:31.200
and peace and what did that produce the cuckoo clock so how do you address that point that humans

46:31.200 --> 46:37.280
need challenge again let's distinguish between happiness and meaningfulness slash purposefulness

46:37.280 --> 46:41.280
it's those challenges that give us meaning and purpose that's the goal not happiness

46:43.920 --> 46:51.280
um yeah well um it's it's hard to tell um certainly if it were happiness in the subjective

46:51.280 --> 46:57.440
sense of positive effect it would make the problem very easy because trivially in utopia

46:57.440 --> 47:04.400
technical maturity you could tune your hedonic well-being up or down very easily through certain

47:04.400 --> 47:09.520
newer technology or drugs and stuff so if that's what we wanted then we would be home and drive

47:09.520 --> 47:15.120
like problem solved we'll definitely be able to do a lot of that in utopia um if we want

47:15.200 --> 47:20.320
challenge well certainly we could create artificial challenges uh there are games uh

47:21.120 --> 47:26.480
very elaborate games with like all kinds of um you could have ai's inventing new games for us

47:26.480 --> 47:33.280
like there could be so if artificial challenges are enough to realize that value that you pointed

47:33.280 --> 47:38.560
to then also we are home and drive that that would also be very easy to do if we want genuinely

47:38.560 --> 47:44.800
meaningful challenges then there is more of a challenge um in seeing how that would be possible

47:44.800 --> 47:49.200
in deep utopia because prima facie at least at first sight it seems like our own efforts are

47:49.200 --> 47:56.400
for most purposes unnecessary and then we could still do the thing but it may not obviously be

47:56.400 --> 48:02.000
meaningful to do the thing if if there is nothing worthwhile achieved by doing it but I do think

48:02.000 --> 48:08.640
there are at least ways of rescuing part of what we want if we want meaningful challenge even in

48:08.640 --> 48:16.800
utopia and there might be first of all uh tasks that need to be performed by like so for example

48:16.800 --> 48:22.400
if there are to take a very simple example consumers that have a preference uh not just

48:23.360 --> 48:27.920
for a certain type of object but also a preference regarding how that object should have been

48:27.920 --> 48:34.800
manufactured uh and in particular they wanted to have been manufactured by hand you know or by human

48:35.440 --> 48:39.920
then that would be demand for human labor to produce we see that today like certainly the

48:39.920 --> 48:45.760
consumers might pay more for a trinket that's done by some favorite group or like in indigenous tribe

48:45.760 --> 48:50.800
rather than in a sweatshop in Malaysia like even if the trinket is the same or equivalent like the

48:50.800 --> 48:56.640
fact that the causal process that brought it about was different might result in a difference in price

48:58.480 --> 49:03.040
similarly we might prefer to watch like human athletes compete even if like the robots could

49:03.040 --> 49:07.600
run faster or box harder or what they're like that might just be a brute fact about it and so

49:07.600 --> 49:13.760
you can then see like or we might want like a robot uh you know priest administering the wedding

49:13.760 --> 49:19.920
or sorry like a human doing it rather than you know a robot even if the robot could say the same

49:19.920 --> 49:24.880
word etc so you could then you could look through like and there might be many more of these that

49:24.880 --> 49:30.400
we can't afford currently so nobody has kind of even bothered inventing these services but

49:30.960 --> 49:36.320
we're just humans have a sort of brute preference for it to be done by human effort

49:37.280 --> 49:44.240
and I think in addition to that there might be more subtle ways in which that would be instrumental

49:44.240 --> 49:50.400
uses for human effort if for example we have values say you say you have a value that values

49:50.400 --> 49:56.800
the honoring of a certain tradition now many traditions in order to be continued would need

49:56.800 --> 50:04.000
the active efforts of human beings to do whatever the things that they're traditionally done to have

50:04.000 --> 50:10.640
the the ceremonies and to like focus our attention on certain things and even if we could perform

50:10.640 --> 50:15.760
like great robots that went around then like perform the same songs and dances and stuff like

50:15.760 --> 50:20.320
it wouldn't count as continuing that tradition so if we value that it might call upon us to

50:22.080 --> 50:25.680
to make an effort and that might be one of these subtle values or maybe right now the tradition

50:25.680 --> 50:30.000
is like well our tradition is tradition like whatever you know they're starving kids out there

50:30.000 --> 50:34.720
we should focus on helping those but once all the kids are fed and all the diseases are cured

50:34.720 --> 50:43.040
then these slightly less like uh it's real values might then deserve a lot of attention and aesthetic

50:43.040 --> 50:48.560
values like there might be things we have reason to do because it would just be beautiful if some

50:48.560 --> 50:56.000
body did it and um social cultural entanglements like the way that the different people have

50:56.000 --> 51:01.200
preferences about each other and what they do and how that I think that might also produce

51:02.080 --> 51:10.960
some opportunities for um natural purpose in naked in in utopia um you can also have

51:10.960 --> 51:14.880
artificial purpose where you just set yourself an arbitrary challenge and then have your brain

51:14.880 --> 51:19.520
motivated uh change so that you're like super motivated to achieve it that that that would

51:19.520 --> 51:25.760
be safe but there might also be some of these more natural purposes um well there is this DIY

51:25.760 --> 51:31.120
you know do-it-yourself movement where people seem to like just doing it by hand they just want to

51:31.120 --> 51:35.920
get their tools out and get up in the garage and start making stuff I don't personally like this

51:35.920 --> 51:40.320
because I'm not very good at that but it's a huge movement so I mean you could hire somebody or

51:40.320 --> 51:45.200
there's a machine that could make the little shed better than you can make it by hand but people

51:45.200 --> 51:50.480
seem to like to do that why not have both the AI does the stuff we don't like to do and then I'm

51:50.480 --> 51:56.080
just going to do the stuff I do like to do yeah no that that seems seems good now there is an

51:56.080 --> 52:03.840
additional challenge here which is uh lifespans could become very long right if we fix the things

52:03.840 --> 52:13.840
that cause disease and death and like cellular decay etc so um if you are going to live for

52:15.200 --> 52:23.840
maybe millions or billions of years potentially okay um uh you sort of run out of like just get

52:23.840 --> 52:29.920
me to 100 without Alzheimer's all right well that's a good start but you know when you're 100 in

52:29.920 --> 52:34.560
perfect health yeah I'd go for 200 maybe you think well do I really want to check out now

52:34.560 --> 52:39.600
or maybe do another year let's let's let's push it a little bit further down and at least it would

52:39.600 --> 52:47.440
nice for you to have the option of kind of uh because like I mean probably our like our age

52:47.440 --> 52:52.320
when when we were a kid like being 50 or 60 or whatever that's like now I know particularly

52:52.320 --> 52:57.600
white as well I don't but now of course when you're there you see that wow you know there's a lot more

52:57.600 --> 53:04.240
that yeah could be done and experienced than the um and there are simple pleasures as well so

53:04.240 --> 53:08.080
they're like the things you might only want to do once or twice in life but then you've done them

53:08.080 --> 53:13.440
but then they're like other like a nice cup of tea or a coffee like it's kind of about as good as

53:13.440 --> 53:17.920
you know on the tenth thousand times you do it and then you know in the first or second so you

53:17.920 --> 53:24.720
don't really it's renewable as it were like a renewable sort of joy and so but but it does

53:24.720 --> 53:29.600
mean also like one should maybe think of if the question is what's the best possible future life

53:30.640 --> 53:34.800
that you could have if you remove all practical constraints and technological constraints

53:36.480 --> 53:40.480
you really should think maybe in terms of a trajectory not not just a state that you would

53:40.480 --> 53:46.000
reach and then you have sort of reached the peak but more like what's a developmental trajectory

53:46.000 --> 53:52.160
that would like be you you'd get the most out of each level of development maybe eventually

53:52.160 --> 53:56.560
like understood most things that can be understood by human brain maybe at that point

53:56.560 --> 54:00.640
you would want to upgrade it a little bit like go a lot some more neurons or whatever so you could

54:00.640 --> 54:06.400
kind of explore the next level and but like what's the right pace of that like do you want to just

54:06.400 --> 54:11.920
rush to the end and become like like a planetary sized superintelligence immediately or would you

54:11.920 --> 54:17.120
like want to you know take the scenic routes and then maybe spend a few hundred years first

54:17.760 --> 54:23.200
being a biological humans and doing whatever can be done as a human and then slowly increment

54:24.480 --> 54:32.160
so these are some of the the questions that come up and so many more there's a lot of things to

54:32.160 --> 54:38.960
think about hopefully we will actually yeah secure the future in us that well yeah again

54:38.960 --> 54:45.120
incrementally I like Kevin Kelly's approach protopia not utopia or dystopia just one small make

54:45.120 --> 54:51.920
life tiny bit better tomorrow than it is today don't aim for utopia just as a tool just make life

54:51.920 --> 55:00.320
a little bit better don't worry about 500 years from now just tomorrow I think that allows one to

55:00.320 --> 55:11.120
avoid a bunch of mischief that is is performed in the name of grand visions but I do think also

55:11.120 --> 55:16.480
sometimes it's useful to lift your gaze up and and look at the horizon or like reflect on where

55:16.480 --> 55:22.320
you're going like there's the next step on the next step but ultimately so we have like our human

55:22.320 --> 55:26.880
civilization all this effort spent on science and technology and economic growth and everything but

55:26.880 --> 55:33.920
very little effort spent on thinking what what where do we end up if this continues sure but

55:33.920 --> 55:39.120
just let's talk about creativity for a moment your book I really enjoyed because it's completely

55:39.120 --> 55:44.560
different than any most nonfiction science books that I read you know you have this kind of dialogue

55:44.560 --> 55:48.640
this conversation you're in a classroom your lecturing you have handouts students are asking

55:48.640 --> 55:54.240
questions that was pretty creative and new if you had asked chat gpt to write your next book I don't

55:54.240 --> 55:57.920
think it would have come up with that you see where I'm going with this what about music what's the

55:57.920 --> 56:04.720
origin of rock and roll well folk music and jazz all right so in a century from now what will be

56:04.720 --> 56:10.000
the next big you know musical trend I don't know I don't think it's possible to know and I don't see

56:10.000 --> 56:16.640
how an AI would anticipate the next creative movement not just in the arts and poetry or whatever

56:16.640 --> 56:22.000
but in anything you know there's you know there's only so many combinations I guess maybe it could

56:22.000 --> 56:27.040
grind through all the possible combinations for music that's going to be enjoyed by people that

56:27.040 --> 56:33.600
seems to me though next to impossible to program now I might not be able to predict it

56:35.520 --> 56:39.840
but there's a lot of things that you couldn't predict even with your super intelligence like

56:39.840 --> 56:44.960
that like even just like the weather like a year into the future whether on a particular

56:44.960 --> 56:49.440
minute it will be raining on this like chaotic systems right and with something like creativity

56:49.440 --> 56:56.640
over the time scale of a century it the actual answer to that question will depend on what a

56:56.640 --> 57:01.280
lot of smart people are doing in the course of that century maybe other AIs even smarter than

57:01.280 --> 57:06.000
the one that you would think would be making the prediction and it itself will interact with so this

57:06.560 --> 57:11.920
but even if it's not predictable what creative results will it you know precisely be attained

57:12.640 --> 57:18.480
a century hence it might still be possible that the actual creative work is more efficiently

57:18.480 --> 57:25.280
done by AIs as this century unfolds they might just be making the best paintings and writing

57:25.280 --> 57:30.480
the most beautiful poems and writing the most compelling movies etc it's certainly not the

57:30.480 --> 57:35.760
case right now I mean current large language models are have a sort of

57:39.520 --> 57:46.000
cliche is maybe too strong a word but there is a sort of mid-brow quality to their output that

57:46.000 --> 57:52.880
there's like the it's good but it's not great it's kind of the typical thing that some person would

57:52.880 --> 58:00.320
say in a situation that they can produce more of that but great stuff comes from kind of not just

58:00.320 --> 58:10.560
following along with the patterns that are already out there but sort of looking at reality afresh

58:10.560 --> 58:14.880
with new eyes whether the reality is inside yourself or outside of yourself and really

58:14.880 --> 58:19.760
letting it speak to you and then they sort of speak the words that come from your perception

58:20.720 --> 58:26.160
of this piece of reality that is that you're focusing on and so it's like a different source

58:26.160 --> 58:33.440
kind of of information but I have no doubt that that AIs will become increasingly creative I think

58:33.440 --> 58:38.000
it's not a binary thing I think we already see little glimpses of lower level creativity and

58:38.000 --> 58:44.800
I think the next generation will have more and then more and more beyond that for example a few

58:44.880 --> 58:53.120
years ago DeepMind System Alpha go had this move what was it 32 or something 37 I forget but it was

58:53.120 --> 58:59.760
like in the match that Alpha Fall was playing against least at all the human go champion and

58:59.760 --> 59:06.480
there was a particular move that experts in go thought was immensely original and creative it

59:06.480 --> 59:11.520
was something no human would ever have played that all the masters would advise like students that

59:11.600 --> 59:15.200
that was an error but then it still turns out like if you think a little bit more you just

59:15.200 --> 59:21.360
realize how right it was and it set everything up to win the game later so that's within a

59:21.360 --> 59:26.080
sort of somewhat circumscribed domain but certainly like created within that domain and I think

59:26.080 --> 59:32.800
the domains in which you will be able to have these like genuine deep creativity will be expanding

59:32.800 --> 59:39.200
as the capabilities of the AIs increase yeah when I was a professor at Occidental College we had a

59:39.280 --> 59:44.880
music professor there was also a gifted pianist and he would once a year hold these impromptu

59:44.880 --> 59:50.240
concerts where he would in the auditorium that grand piano on stage he sits there and then people

59:50.240 --> 59:57.440
would call out like requests like do Beethoven's X as if you know Elvis did it or you know in the

59:57.440 --> 01:00:02.240
rock and roll and and people just come up with the craziest and he would do it and it's like god

01:00:02.240 --> 01:00:08.400
damn that's great so maybe if you had an AI you could you could find all the different creative

01:00:08.400 --> 01:00:13.120
permutations on all the different music that has been done and then test it in the marketplace

01:00:13.120 --> 01:00:20.960
well what do people actually like yeah yeah yeah I do yeah so there's a quite right now the question

01:00:20.960 --> 01:00:25.760
of quality like the actual output is not great now if we imagine the quality problem being fixed

01:00:25.760 --> 01:00:31.680
then there is the question of whether people would still value it less because it was produced by AI

01:00:32.320 --> 01:00:38.240
even if if you sort of listen to a blind test right A and B you're not told which one is even if

01:00:38.240 --> 01:00:44.320
people prefer the AI output in that context if the quality became like as good or superior

01:00:45.600 --> 01:00:50.720
if then we get to this like father question of value whether you still prefer it just because

01:00:50.720 --> 01:00:58.400
you know that the human did it there's also the I mean there's like so many branches sticking out

01:00:58.400 --> 01:01:03.040
from here but like one possible reason you might have for preferring the human output is if you think

01:01:03.040 --> 01:01:09.120
the human but not the AI experienced various things when they wrote it they actually experienced

01:01:09.120 --> 01:01:18.080
the joy or the sadness that you know the musical piece expressed but there again I think with

01:01:18.080 --> 01:01:24.160
digital minds it might also be possible to create phenomenal experiences in digital substrate

01:01:26.080 --> 01:01:32.720
and so AIs also might have had experiences that they could be expressing in their works

01:01:33.040 --> 01:01:39.760
it's not clear exactly how where we are on that path towards AI sentence but I think certainly

01:01:40.400 --> 01:01:46.960
in principle it is possible I'm a kind of computationalist about phenomenal content

01:01:46.960 --> 01:01:53.360
yeah that subjective element of art where the fraudulent copy painting of a classic painting

01:01:53.360 --> 01:01:57.600
plummets in value the moment people find out it's fake even if you can't tell the difference with

01:01:57.600 --> 01:02:06.160
your own eye yeah yeah yeah so um so if that's the model then you know that might be still demand

01:02:06.160 --> 01:02:11.840
for human painters to yeah paint there now sort of relatively back to the economics I mentioned

01:02:11.840 --> 01:02:16.880
you know pulling up everybody from the bottom up to some level but you know economists tell us

01:02:16.880 --> 01:02:21.680
there's this thing called the hedonic treadmill but there is no there's no bottom level people

01:02:21.680 --> 01:02:27.440
always want more and that that's just going to never end you know the McMansions houses are like

01:02:27.440 --> 01:02:31.680
two to three times the size they were in the 1950s even for the average worker

01:02:32.240 --> 01:02:37.520
and you know that that there's no upper ceiling on how much more stuff people are going to want

01:02:37.520 --> 01:02:46.240
how do you think about that yeah I think there are parts of our preference functions that are

01:02:46.640 --> 01:02:55.760
um non-satiable collectively because like yeah we have these desires for positional goods

01:02:55.760 --> 01:03:00.880
to have more than another like you want your yacht to be the biggest in the world

01:03:01.680 --> 01:03:07.680
so you build a 200 you know foot yacht and then some other billionaire bastard builds one that's

01:03:07.680 --> 01:03:15.280
like 205 and then so it's impossible for both of these people to have their preferences satisfied

01:03:15.280 --> 01:03:22.160
to own themselves exclusively the biggest yacht in the world so that's one example for how

01:03:22.880 --> 01:03:27.840
collectively there could be preferences that the humans have that you can't all be satisfied

01:03:27.840 --> 01:03:32.320
and there are many other examples where two people want the same piece of land or the same

01:03:32.960 --> 01:03:42.160
be the exclusive love interest of the same person or etc etc so now it doesn't enable sort of

01:03:42.160 --> 01:03:47.200
unlimited economic growth if you define growth ultimately in preference satisfaction terms

01:03:47.200 --> 01:03:54.880
and so like because one person's gain is another's loss in this scenario and it also doesn't necessarily

01:03:54.880 --> 01:04:04.160
create an unending reason for human economic labor if there is no way to make more money

01:04:05.360 --> 01:04:09.840
than no matter how each of these billionaires wish they could make their yacht a bit bigger than the

01:04:09.840 --> 01:04:16.160
others like if they can't actually make more money by working or if the extra money they

01:04:16.160 --> 01:04:20.080
could make by working is kind of trivial to the amount of money they are already getting from

01:04:20.080 --> 01:04:26.640
their capital gains and that would be no incentive for them to put out effort for that reason

01:04:28.720 --> 01:04:32.480
and that's like already true for many billionaires like there's like yeah they could

01:04:32.480 --> 01:04:37.760
take a job and make an extra 100,000 a year maybe but if they're already sitting on 20 billion it's

01:04:37.760 --> 01:04:42.640
like it's not really making a meaningful difference to their purchasing power yeah but you have people

01:04:42.640 --> 01:04:47.120
like Elon Musk and Jeff Bezos you know they didn't they're not just sitting on the beach

01:04:47.120 --> 01:04:55.760
you know they're exactly and they can I mean still add a lot of even just economic value

01:04:55.760 --> 01:05:02.720
through their work like like obviously Tesla would be worse a lot less if Elon I called it quits yeah

01:05:02.880 --> 01:05:10.480
and so even just from a purely economic point of view they still have the ability to contribute

01:05:12.240 --> 01:05:18.400
amounts of economic value that are significant even relative to their net worth and

01:05:19.920 --> 01:05:23.680
because they have like like Elon has unique skills also I think there are opportunities

01:05:23.680 --> 01:05:28.240
sometimes for very wealthy people to sort of combine their human capital with their financial

01:05:28.240 --> 01:05:33.680
capital to do things that are hard to do by taking one person with capital and one person

01:05:34.720 --> 01:05:38.720
with brains because they're like trust problems and communication problems sometimes they need

01:05:38.720 --> 01:05:44.000
to be combined in one person to certain opportunities are more easily realizable

01:05:44.880 --> 01:05:50.080
but for many others it's not the case and they're already like in this situation where

01:05:50.080 --> 01:05:55.040
it makes no sense to work for money yeah but I guess my point is you know there's stories about

01:05:55.040 --> 01:05:59.600
Elon Musk sleeping on the floor in his factories he doesn't have to do any of that but he does it

01:05:59.600 --> 01:06:04.400
because that's what gives him value and I think more people would want that than would just want

01:06:04.400 --> 01:06:09.920
to sit on the beach yeah I think he's doing it because he wants to achieve various things that

01:06:09.920 --> 01:06:16.400
can't be achieved without him doing it yeah now if he could create like I don't know like some sort

01:06:16.400 --> 01:06:22.080
of android replica of himself that would do the same thing and achieve the same results for for

01:06:22.080 --> 01:06:27.520
Tesla and SpaceX etc and and he could be on the beach I have no idea maybe he would prefer that he

01:06:27.520 --> 01:06:34.160
has said that his life is pretty painful often and that um so it might be that he does it because

01:06:34.160 --> 01:06:40.000
there are various outcomes he wants as opposed to valuing the activity itself not running around

01:06:40.000 --> 01:06:44.160
maybe of course we don't know what's in his head but uh you know I think in general people like

01:06:44.160 --> 01:06:48.880
challenges because that's what makes life meaningful and it's essentially an infinite

01:06:48.880 --> 01:06:53.280
number of challenges we could always have but I could be wrong okay on the economic model

01:06:53.280 --> 01:06:58.160
so people are living longer let's not get crazy let's just say people live 200 years or 300 years

01:06:58.160 --> 01:07:02.800
rather than 100 yeah but it's 300 years of research into extending life don't you think

01:07:03.760 --> 01:07:10.240
I don't know you know Kurzweil thinks it's coming by 24 what's the the takeoff point in 2045 I think

01:07:10.240 --> 01:07:16.960
he said where maybe it's even sooner than that where the amount of extra life you get exceeds

01:07:17.600 --> 01:07:22.000
every year of your life and then you have the what does he call it the take take takeoff point

01:07:22.000 --> 01:07:28.000
something like that longevity escape velocity yeah I don't you escape velocity that's it yeah I

01:07:28.000 --> 01:07:33.040
you know when I hear these things I think back to religion it's like I feel like I'm you know we're

01:07:33.040 --> 01:07:37.920
the chosen generation we're the ones that get to live forever I've heard this before when I was

01:07:37.920 --> 01:07:44.080
religious right you know maybe you know but I think the problems are much harder than most

01:07:44.160 --> 01:07:50.080
longevity researchers think but you know it's possible but okay let me let me

01:07:50.080 --> 01:07:53.920
carry out the thought experiment all right so we have eight billion people now it's probably gonna

01:07:53.920 --> 01:08:00.240
top off around 2050 and start to decline by 2100 or so and as you know Elon's worried about a

01:08:00.240 --> 01:08:05.920
birth birth the richer and more economically stable and more educated people are the fewer

01:08:05.920 --> 01:08:13.040
babies they have so how do you square that with people living longer and the population increasing

01:08:13.120 --> 01:08:16.160
how do you think about that yeah I think there are various

01:08:17.280 --> 01:08:24.160
long-term trends that I think would deserve attention if it were for the fact that I also

01:08:24.160 --> 01:08:30.800
believe that we are probably relatively close to this transformative technological overhaul over

01:08:30.800 --> 01:08:37.040
the current of the current human condition so that I think sort of the game board will be overturned

01:08:37.040 --> 01:08:44.400
for better or worse but within you know likely some years or a few decades and that these like

01:08:44.400 --> 01:08:51.600
longer demographic trends won't really have time to play out would be my guess there might be other

01:08:51.600 --> 01:08:57.680
demographic trends that then kick in if you do invent this a new world with AIs and digital minds

01:08:57.680 --> 01:09:02.880
that can obviously copy themselves instantaneously if you're like software you could make a million

01:09:02.880 --> 01:09:06.320
copies of yourself in an afternoon right if you have available hardware so you could have

01:09:07.200 --> 01:09:11.280
like different population dynamics that could become problematic but but that

01:09:11.280 --> 01:09:14.640
that wouldn't sort of just be an extrapolation of what we're currently

01:09:15.280 --> 01:09:24.080
seeing with the human situation also like some I mean I see the the projections and how like

01:09:24.080 --> 01:09:30.800
birth rates are going down and if that continues like like but some skepticism about our the

01:09:30.800 --> 01:09:35.280
reliability of these long-range forecasts like I mean when I grew up the the big worry about

01:09:35.280 --> 01:09:40.320
was about overpopulation and there were these like public intellectuals through the club of

01:09:40.320 --> 01:09:44.400
Rome and everything and that was like and they had little mathematical models that show this

01:09:45.520 --> 01:09:49.360
now it's going the other way and I mean who knows in 30 years from now if there's no

01:09:50.000 --> 01:09:56.000
transition maybe it just turns out that something has changed and it's overpopulation again like or

01:09:56.000 --> 01:10:02.400
some other so it's like yeah our ability to make these very long-run range forecasts are

01:10:02.640 --> 01:10:09.600
are open to question I think yeah you know Stein's law things that can't go on forever won't

01:10:11.360 --> 01:10:14.560
and there's some corollary to it but but they can go on a lot longer than you think

01:10:15.280 --> 01:10:20.880
yeah yeah yeah some things have gone on for longer than yeah one would have guess well I guess in the

01:10:20.880 --> 01:10:25.360
next you know you want to look at the far horizon do we need to leave the planet become a multi-planetary

01:10:25.360 --> 01:10:31.360
species because of either overpopulation or we're going to run out of raw supplies and and uh and

01:10:31.360 --> 01:10:39.600
resources and population can ultimately outrun any space settlement program because ultimately

01:10:40.400 --> 01:10:48.160
even with mature technology we are limited by the speed of light and so if you imagine a sphere

01:10:48.160 --> 01:10:52.880
growing at the speed of light even in all directions right the volume of that grows

01:10:52.880 --> 01:11:00.080
polynomially with time and so the resources that we could potentially use for civilization or for

01:11:00.080 --> 01:11:06.400
like that cannot most grow uh polynomially whereas population can grow exponentially it can in theory

01:11:06.400 --> 01:11:12.000
like you know double every generation or 10 percent and so eventually the exponential will

01:11:12.000 --> 01:11:19.600
overtake the polynomial if you have unrestricted population growth and if like we end up in in

01:11:19.600 --> 01:11:24.320
a situation where sort of people have more than the replacement rate of children that that would

01:11:24.320 --> 01:11:31.040
eventually just overtake so so that the space settlement would at most kind of delay bumping

01:11:31.040 --> 01:11:37.200
up against resource limits and ultimately you would have to just figure out some way to maybe

01:11:42.480 --> 01:11:49.120
coordinate to to bring only the number of people new people into existence that that could be

01:11:49.120 --> 01:11:53.680
supported at a high level of living which might still be a lot of new people into existence but

01:11:54.320 --> 01:12:00.320
if you overshoot that then eventually average income would have to drop right now we have more

01:12:00.320 --> 01:12:08.400
like kind of increasing returns to population because more people means more ideas and division

01:12:08.400 --> 01:12:14.320
of labor which makes so so right now probably per capita income goes up if population increases

01:12:15.040 --> 01:12:22.000
but i think at some point that will no longer be the case and the limiting factor of the economy at

01:12:22.000 --> 01:12:28.000
technological maturity will eventually become land as it's referred to basically resources

01:12:28.000 --> 01:12:35.120
that you can't make more of as opposed to labor or uh technological advances that that will already

01:12:35.120 --> 01:12:43.200
have sort of been maximized and so then land only grows polynomially in the limit and that that would

01:12:43.200 --> 01:12:49.040
be the sort of maximum rate at which the population could grow in the limit as well and what's your

01:12:49.040 --> 01:12:55.200
time horizon there you know thousands of years or tens of thousands of years uh well i think um

01:12:56.880 --> 01:13:04.240
there are really sort of two key variables there's a question of how far from now until we get

01:13:04.240 --> 01:13:11.680
superintelligence um and then from there on i it might not take that long because once you have

01:13:11.680 --> 01:13:16.240
superintelligence that makes super duper intelligence and then like some kind of

01:13:17.200 --> 01:13:24.880
substrate optimized for a cognitive performance that can i would imagine relatively quickly

01:13:24.880 --> 01:13:30.400
develop all kinds of technological solutions that start to approximate the physical limits

01:13:31.120 --> 01:13:36.480
i don't know whether that would take like months or decades but well so Kurzweil is talking about

01:13:36.480 --> 01:13:41.760
monetary super brains brains kind of working for like creating a space rocket for like yeah

01:13:41.760 --> 01:13:47.680
it occurs i was talking about 2045 in his next his next book the singularity is nearer coming out

01:13:47.680 --> 01:13:53.680
in june uh 24 but so about after that anything's game i mean we just probably unpredictable uh what

01:13:53.680 --> 01:13:59.280
the time horizon could be yeah i mean i don't even think we can rule out very short timescales like

01:13:59.280 --> 01:14:04.560
we don't know dpt 5 or dpt 6 won't be right there i mean we don't know that i will but

01:14:05.280 --> 01:14:09.680
here we really have to think probabilistically right and have like a smeared out probability

01:14:09.680 --> 01:14:14.640
distribution i think over different or okay you're one of my favorite big minds so let's

01:14:14.640 --> 01:14:18.800
keep going on the the long horizon all right let's apply the Copernican principle to our

01:14:18.800 --> 01:14:22.560
species we're not special the chances are you know we're in the middle of the bell curve of

01:14:22.560 --> 01:14:28.720
civilizations that would have done everything you just described uh surely there's extraterrestrial

01:14:28.720 --> 01:14:34.560
intelligences out there that have already done all this uh and built self replicating von Neumann

01:14:34.560 --> 01:14:41.920
machines and Dyson spheres and so on so answer me uh Fermi's paradox where is everybody

01:14:43.200 --> 01:14:53.200
um most likely just like very far away like outside our light cone um which would uh yeah

01:14:53.200 --> 01:14:56.800
explain why we haven't seen them of course if the universe is infinite as it seems to be with

01:14:56.800 --> 01:15:00.320
infinitely many planets and stuff then there would be infinitely many of them out there

01:15:01.120 --> 01:15:08.160
but the density might be quite low um we don't really have it seems to me and a particular

01:15:08.160 --> 01:15:15.680
reason to think that um it would be easy for a like an earth like planets to produce life let

01:15:15.680 --> 01:15:20.480
alone intelligent life i mean there might just have been some ridiculously improbable steps

01:15:20.480 --> 01:15:26.080
somewhere like to get the first impulse replicators going or maybe to go from prokaryot to eukaryot

01:15:26.080 --> 01:15:32.560
or something maybe that just happens in like one planet out of 10 to the power of 40 planets or

01:15:32.560 --> 01:15:37.760
something um now then you might think wow wasn't it then like what's the miracle that it happened

01:15:37.760 --> 01:15:41.920
here on earth well if there are infinitely many planets out there then even if the chance for

01:15:41.920 --> 01:15:47.040
any one of them is ridiculously small it would still happen right with certainty infinitely many

01:15:47.040 --> 01:15:53.440
times and then an observation selection effect would explain why we find ourselves on a planet

01:15:53.520 --> 01:15:57.360
where this improbable thing did happen like only those planets are observed by people

01:15:58.240 --> 01:16:04.560
evolving on them of course the others there is nobody there too so that that seems like pretty

01:16:04.560 --> 01:16:16.480
likely um if one wants to think that life is more common then one has to i think um either

01:16:16.480 --> 01:16:22.800
postulate some kind of zoo hypothesis like where they are deliberately uh hiding themselves or

01:16:23.920 --> 01:16:31.680
uh like my colleague robin hands on has some scenario in which uh uh there might be others and

01:16:31.680 --> 01:16:38.000
it is like too long to explain but yeah there's a there are like it's i also think like i don't

01:16:38.000 --> 01:16:42.400
know i mean it probably takes us like too far afield from our current conversation but this

01:16:42.400 --> 01:16:49.200
whole simulation yeah argument stuff um which which kind of adds another dimension to the whole

01:16:49.200 --> 01:16:54.880
where and where do you stand on the simulation hypothesis well i mean i i mean i i believe in

01:16:54.880 --> 01:17:00.880
the simulation argument having uh more than 50 percent united that yeah so i i think that sound

01:17:00.880 --> 01:17:06.640
and uh now that only shows one of three possibilities is correct one of which is the simulation

01:17:06.640 --> 01:17:11.920
hypothesis and so you would then need some additional information or consideration if you

01:17:11.920 --> 01:17:16.800
wanted to sort of pick between these three alternatives that the simulation argument

01:17:16.800 --> 01:17:24.320
establishes um i would say i mean and i think as to to me when i wrote this paper back in in

01:17:24.320 --> 01:17:30.560
like the early 2000s it back i was pretty clear that we were sort of on a path to develop

01:17:30.560 --> 01:17:36.000
increase the the technologies that you would need to create these ancestor simulations or

01:17:36.000 --> 01:17:41.520
detailed simulations with conscious um like like super advanced virtual reality and digital brains

01:17:41.520 --> 01:17:47.520
like now i think it's maybe easier for people to imagine that because just we've seen kind of 20

01:17:47.520 --> 01:17:52.960
years 24 years of technological advancement like virtual reality is a lot better now than it was in

01:17:52.960 --> 01:17:58.720
2001 and obviously ai is moving ahead so it's like a smaller imaginative leap to think that

01:17:59.280 --> 01:18:03.600
at some point in the future some technologically mature civil like really advanced civilization

01:18:03.600 --> 01:18:09.280
might have the ability to create simulations that are like perfectly realistic to the people inside

01:18:09.360 --> 01:18:17.440
them um and uh yeah i like so so in in some sense the opportunity is to

01:18:17.440 --> 01:18:22.000
pop off the train before you reach the conclusion or like diminishing as we sort of pass by the

01:18:22.000 --> 01:18:26.880
relations you know i had david chalmers on the on the show he has a you know an entire book on

01:18:26.880 --> 01:18:32.960
on the simulation morals in a simulation and ethics and so on very interesting but ultimately he

01:18:32.960 --> 01:18:37.760
says right in there this is not a testable hypothesis we there's no way to know if we're

01:18:37.760 --> 01:18:43.280
in a simulation or not so then what are we talking about here this is just science fiction or metaphysics

01:18:43.280 --> 01:18:53.280
or or what no i mean i think it um there are certainly possible observations um uh that

01:18:54.800 --> 01:19:01.440
are such that if we made them they would give us strong evidence either for or against

01:19:01.440 --> 01:19:05.680
the simulation hypothesis it's like to take the most obvious example like if a window

01:19:05.680 --> 01:19:09.200
suddenly popped up in your visual field saying you're in the simulation click here for more

01:19:09.200 --> 01:19:14.000
information and a little buffering and find the terms and services like that would pretty

01:19:14.000 --> 01:19:20.240
like prove it to you right like if that yeah um conversely the absence of such a window popping

01:19:20.240 --> 01:19:26.880
up is by the principle of conservation of evidence must be some really evidence against the simulation

01:19:26.880 --> 01:19:36.480
hypothesis like weak evidence because but but still some evidence and but more um i guess um

01:19:36.480 --> 01:19:41.760
relevantly i think if you consider the simulation argument which has the structure that at least

01:19:41.760 --> 01:19:48.160
one of three propositions has to be true one of which is the simulation hypothesis anything that

01:19:48.160 --> 01:19:53.760
gives you evidence for or against the other two indirectly then uh affects the probability you

01:19:53.760 --> 01:19:59.920
should assign to the simulation hypothesis so um for example one of the alternatives is that

01:19:59.920 --> 01:20:04.960
almost all civilizations at our current stage of technological development go extinct before they

01:20:04.960 --> 01:20:10.000
reach technological maturity so that's something you could believe instead of the simulation

01:20:10.000 --> 01:20:13.680
hypothesis but there has to be a very strong convergence it can't just be like 80 percent

01:20:13.680 --> 01:20:20.240
of them it has to be like basically all um and of course if we make it through to

01:20:20.240 --> 01:20:24.640
technological maturity that would be very strong evidence against this idea that basically all of

01:20:24.640 --> 01:20:31.120
them fail to get to the technological maturity so therefore anything any evidence we get for

01:20:31.120 --> 01:20:35.840
against the idea that we will reach technological maturity would bear indirectly through the

01:20:35.840 --> 01:20:40.800
simulation argument on the probability of the simulation hypothesis so the closer we get to

01:20:40.800 --> 01:20:45.920
technological maturity ourselves the less likely that alternative is and hence the more likely

01:20:45.920 --> 01:20:52.800
the simulation hypothesis is and you could imagine um the extreme version of this which

01:20:52.800 --> 01:20:58.960
is if we ourselves develop all the technologies needed to create ancestor simulations and we are

01:20:58.960 --> 01:21:04.320
just about to switch them on and we want to switch them on and we're sort of about to reach to press

01:21:04.320 --> 01:21:09.280
the button that would pretty much conclusively rule out the two alternatives to the simulation

01:21:09.280 --> 01:21:15.920
hypothesis it would show that like it's not the case that nobody reaches this level of

01:21:15.920 --> 01:21:19.840
technological maturity it's not the case that almost nobody of those who do reach that

01:21:21.600 --> 01:21:27.120
remain interested in creating ancestor simulations and so in that situation where we turn on our own

01:21:27.120 --> 01:21:32.400
simulations we would have to conclude that we are almost certainly ourselves in one

01:21:34.160 --> 01:21:38.720
so those would be some ways of getting very strong evidence and then but I think anything

01:21:38.720 --> 01:21:43.840
that then indirectly has some probabilistic bearing on these alternatives also sort of indirectly

01:21:43.840 --> 01:21:49.920
has some evidential connection to the simulation hypothesis so I think there's a lot of ways to

01:21:49.920 --> 01:21:56.960
test it but these tests are probabilistically yeah did you mention what if we're first somebody

01:21:56.960 --> 01:22:03.680
has to be first yeah well that that's uh what what it's for did you say are or if it's first no

01:22:03.680 --> 01:22:08.160
we're first yeah we're there some civilization has to be first maybe it's us that must be

01:22:09.840 --> 01:22:19.360
very unlikely if that were to ultimately be say a million simulations of experiences just

01:22:19.360 --> 01:22:25.440
like the ones you're having and you can't from the inside tell the difference whether you're like

01:22:25.520 --> 01:22:34.880
number 537,648 or whether you're like number one but in that condition where you have some evidence

01:22:34.880 --> 01:22:41.520
and you could either be one of the vast majority of people with your experiences that are simulated

01:22:41.520 --> 01:22:46.800
or like this very exceptional one that's not simulated and there's doesn't feel any difference

01:22:46.800 --> 01:22:51.440
from the inside I think they're a kind of principle of indifference should make you

01:22:51.440 --> 01:22:56.640
assign a proportionately low credence to you being the first one like the exceptional one

01:22:56.640 --> 01:23:01.840
yeah but again somebody has to be first because you're saying in the age of the universe if most

01:23:01.840 --> 01:23:07.200
people think they are then almost all of them will be wrong yes it looks like a rational betting

01:23:07.200 --> 01:23:13.600
odds in that scenario like we would be to assign a very small probability to that and there's more

01:23:13.600 --> 01:23:19.200
arguments I think supporting that it like in fact my my doctoral dissertation was like developing a

01:23:19.200 --> 01:23:24.640
theory of observation selection effects and I think there are various areas in in physics and

01:23:24.640 --> 01:23:32.160
cosmology and to some extent in evolutionary biology where you have to reason along roughly

01:23:32.160 --> 01:23:38.480
those lines to be able to get sensible results when you try to connect current cosmological models

01:23:38.480 --> 01:23:44.320
with the predictions that intuitively confirm or disconfirm them some sort of roughly speaking

01:23:44.320 --> 01:23:50.160
assumption that you're like and you should think of yourself as you were a randomly selected observer

01:23:50.160 --> 01:23:54.240
well as there's a lot of complications around that but that's like some something in that

01:23:54.240 --> 01:24:00.160
general direction seems to the let me ask you a technical question here on a simulation like in

01:24:00.160 --> 01:24:05.840
Star Trek's holodeck you know Worf goes in there and he has a fight with some other Klingon and he

01:24:05.840 --> 01:24:12.480
gets knocked down how does a virtual reality interact with a physical body to say maybe you

01:24:12.480 --> 01:24:17.520
want to have a boxing match with Muhammad Ali in this virtual reality how does he how does the

01:24:17.520 --> 01:24:23.920
virtual reality actually knock me down well I mean so in the simulation argument I think

01:24:24.640 --> 01:24:31.840
the the most well so there your mind is itself implemented digitally like so there is no

01:24:31.840 --> 01:24:37.440
me to knock down it's it's all digital yeah there is a you but the you is like it digitally

01:24:37.440 --> 01:24:43.600
instantiated and you have like an avatar yeah that your digital mind is connected to like a digital

01:24:43.600 --> 01:24:53.600
avatar and the same sensory afference that you like currently are going from your sensory nerves

01:24:53.600 --> 01:24:59.040
yeah okay yeah like like that it's like equivalent nerves are going into this digital brain with

01:24:59.040 --> 01:25:04.160
the same information yeah using the same subject now the simulation would at some point have to

01:25:04.160 --> 01:25:11.280
run on actual hardware right right yeah how can you possibly have enough computing power

01:25:11.280 --> 01:25:20.080
to replicate everyone who ever lived and not just their physical bodies and or I guess their minds

01:25:20.800 --> 01:25:25.440
I mean there's this mind uploading business about copying the connectome that's not enough

01:25:25.440 --> 01:25:30.800
it's not enough just to have the synapses copied you'd have to copy every single molecule

01:25:30.800 --> 01:25:38.400
and every one of the synapses in the gas yeah I don't think so I think that a sufficient level

01:25:40.240 --> 01:25:47.840
of granularity of a simulation to produce conscious experience would be like the synaptic level

01:25:49.360 --> 01:25:54.640
you know possibly you could simplify it even more you mean to get a general memory because

01:25:54.640 --> 01:25:59.680
memories are stored I mean I'm told by memory neuroscientists that you need it's not just the

01:25:59.680 --> 01:26:05.600
connectome it's not just those synaptic you know sort of wired in things you need all the molecules

01:26:05.600 --> 01:26:11.520
and all that stuff is part of memory yeah well some aspects of that I mean and clearly there are like

01:26:12.880 --> 01:26:18.720
neurotransmitters that are like swimming around in big pools but I think the bulk of the information

01:26:18.720 --> 01:26:27.440
processing is probably done at the level of action potentials and synaptic connections I mean that's

01:26:27.440 --> 01:26:34.400
what we see with current AI systems large language models are these neural networks are artificial

01:26:34.400 --> 01:26:41.840
neural networks are essentially simplified neurons with simplified synapses and they do seem to

01:26:41.840 --> 01:26:48.240
perform I mean insofar as AI has advanced like the same kinds of tasks that the human brain like in

01:26:48.240 --> 01:26:55.120
terms of say visual perception which is like a relatively well developed area of AI like with

01:26:55.120 --> 01:27:01.120
a comparable number of neurons that visual cortex as you can perform comparable level of

01:27:01.120 --> 01:27:10.560
discrimination and object recognition etc I think and indeed current AI started to have fewer

01:27:10.560 --> 01:27:15.600
parameters than the human brain has and but they are also like a little bit less smart than the

01:27:15.600 --> 01:27:20.960
human brain but it roughly kind of strikingly seems to match the performance that you would expect

01:27:20.960 --> 01:27:25.520
by matching it to biology it might be that the biological neuron certainly is more complex than

01:27:25.520 --> 01:27:31.120
one of these artificial neurons so maybe you get 10 times more performance per biological neuron than

01:27:31.120 --> 01:27:38.000
you get from one of these simplified representations but I think that would be my guess now you could

01:27:38.000 --> 01:27:42.320
have enough computing power to go down a little bit below the level of synapse if somehow you needed

01:27:42.320 --> 01:27:48.240
it not not all the way down to elementary particles then yes it would become computationally

01:27:48.320 --> 01:27:53.520
intractable if you had to simulate the whole Schrodinger equation of a human brain in order

01:27:53.520 --> 01:27:58.080
okay I'll grant you that because human memory is not all that granular anyway it's pretty fuzzy

01:27:59.200 --> 01:28:04.880
but is that you okay so here's my thought experiment we slide you Nick Bostrom into a

01:28:04.880 --> 01:28:10.960
functional MRI we scan your connectome we upload the digital file into the cloud and I have it here

01:28:10.960 --> 01:28:17.440
on my phone and I go Nick you're up here now and you're sitting there going no I'm not I'm right here

01:28:18.160 --> 01:28:22.720
and so are you saying that there's we have to redefine the self there's just multiple

01:28:22.720 --> 01:28:30.320
Nick Bostroms and each of them thinks that they're the real one well I think there are that like

01:28:32.000 --> 01:28:38.640
two notions of the concept like two notions of self that that can come up that that kind

01:28:38.640 --> 01:28:45.600
of always coincide in our normal human experience but that would come apart in some of these

01:28:45.600 --> 01:28:51.760
technological scenarios and philosophers have realized this like Derek Parfit who was my colleague

01:28:51.760 --> 01:28:59.200
at Oxford was famous for exploring the difference between preservation of personal identity and

01:28:59.200 --> 01:29:05.440
survival in these thought experiments where you have like duplication we imagine a person being

01:29:05.440 --> 01:29:11.680
duplicated or teleported and the original survives that Parfit argued that in those cases

01:29:11.680 --> 01:29:23.040
the original person has survived but that personal identity is separate from this because

01:29:24.320 --> 01:29:30.480
the original person can't be identical to any one of them because they are not identical to each

01:29:30.480 --> 01:29:36.480
other and the claim would be equal and so so you would have like even if the personal identity

01:29:36.480 --> 01:29:40.560
is not preserved you could have survival and anyway that that gets into these kind of philosophical

01:29:41.120 --> 01:29:47.520
issues but I think certainly in some scenarios I think your personal identity would be preserved

01:29:47.520 --> 01:29:55.040
in an uploading scenario in like if that was only one successor it would be you I think if there were

01:29:55.040 --> 01:30:01.600
multiple copies made simultaneously like equally branching out from the root node I think it would

01:30:01.600 --> 01:30:06.800
be natural to say that you survived but I'm not sure what to say about your personal identity in

01:30:06.800 --> 01:30:12.560
that case I think just our concept of identity was like not really developed to deal with these

01:30:12.560 --> 01:30:21.440
cases so it's a little bit sort of inconsistent when applied in these extreme or like exotic situations

01:30:21.440 --> 01:30:28.000
yeah yeah I wrote about this in heavens on earth I was looking at both scientific and religious

01:30:28.000 --> 01:30:33.600
versions of the afterlife and there's this idea that there's the mem self and the point of view

01:30:33.600 --> 01:30:38.480
self POV self so the memory self this is the connect them just copy your your your memory

01:30:39.200 --> 01:30:44.160
all your memories and let's say we can do it but that's just a snapshot I mean if you did it when

01:30:44.160 --> 01:30:50.080
I was 30 and now I'm about to turn 70 this year well if I if you did it when I was 30 where is

01:30:50.080 --> 01:30:56.480
all the 40 years plus memories that they're not part of that self that's that's somebody else

01:30:56.480 --> 01:31:02.000
that's not me me is my point of view looking through my eyes from moment to moment to moment

01:31:02.000 --> 01:31:08.160
for all 69 and a half years that I am now that's me and there is no fixed there's no fixed point

01:31:08.160 --> 01:31:14.160
where you go that's you right there at 40 or 50 or whatever I don't see how that could ever be

01:31:14.160 --> 01:31:21.040
replicated because there's no snapshot there's no thing called the self in a fixed sense yeah I think

01:31:21.040 --> 01:31:25.120
there are several different notions and like you could ask somebody like are you the same person now

01:31:25.120 --> 01:31:30.800
as you were when you were five of course not yeah and people are confused because like in in one sense

01:31:30.800 --> 01:31:37.360
clearly not in another sense clearly yes I'm still in classroom and like there's a continuous path

01:31:37.360 --> 01:31:43.600
but I think it's just that instead of having one concept of self we have several different ones

01:31:43.600 --> 01:31:49.920
that normally in everyday use kind of coincide and tracks the same thing but in these scenarios

01:31:50.560 --> 01:31:56.640
they come apart and so we need to sort of start to differentiate different yes you would be the

01:31:56.640 --> 01:32:01.840
same person in sense one of being the same person but like a different person in the sense two of

01:32:01.840 --> 01:32:08.000
being and and I think more generally in this kind of world where we have like digital minds I think

01:32:08.000 --> 01:32:13.200
there are a lot of new concepts that we would need to develop to sort of describe the ways that

01:32:13.200 --> 01:32:19.120
different minds could be related like so humans are kind of discrete things like here's one human

01:32:19.120 --> 01:32:25.520
here's another human like with digital minds you could imagine them being kind of partially overlapping

01:32:26.320 --> 01:32:33.680
or like briefly diverging and then converging you could imagine all kinds of ways in which

01:32:33.680 --> 01:32:39.440
digital minds could vary that they're not possible for human minds to marry you can pause them speed

01:32:39.440 --> 01:32:47.760
them up slow them down that that might be like a bunch of different slightly separate minds that

01:32:47.760 --> 01:32:52.960
sort of have some shared convergence point where they send information and it's not clear whether

01:32:53.040 --> 01:32:58.960
they are all one mind or several minds or like so there's like I think we will we don't yet have

01:32:58.960 --> 01:33:04.000
all the relevant concept for making sense of that kind of post-human reality but you know

01:33:04.720 --> 01:33:09.520
hopefully we'll have an opportunity to develop some of those as we are are you familiar with

01:33:10.960 --> 01:33:14.240
are you familiar with Frank Tipler's book the physics of immortality

01:33:15.040 --> 01:33:24.000
yeah but it was a very long time since I yeah that was 96 so here's his calculation that so he's

01:33:24.000 --> 01:33:30.400
projecting an omega point computer in the far future that he calls god essentially that will

01:33:30.400 --> 01:33:35.360
contain 10 to the power of 10 to the power of 123 bits that's a one followed by 10 to the

01:33:35.360 --> 01:33:42.480
123 zero powerful enough he says to resurrect everyone who ever lived that may be but it's a

01:33:42.480 --> 01:33:48.320
staggeringly large number but is even an omega point computer powerful enough to reconstruct

01:33:48.320 --> 01:33:54.320
all of the historical contingencies all parts of life you know every interaction I ever had

01:33:54.320 --> 01:33:59.280
with everybody else including like you right now this particular moment instead of yesterday or

01:33:59.280 --> 01:34:05.120
whatever I mean how would how would any civilization create a computer powerful enough to do that and

01:34:05.120 --> 01:34:09.280
Frank says not only that you'd have to you'd have to resurrect everyone who ever could have lived

01:34:10.000 --> 01:34:15.520
because you you don't know who you just it's just your point of view so there's a lot of

01:34:15.520 --> 01:34:22.640
there's a lot of those people that's a that's a big cohort yeah yeah and if there was a computer

01:34:22.640 --> 01:34:27.280
powerful enough to do that wouldn't it have to consume so much energy we could detect like a

01:34:27.280 --> 01:34:32.560
techno signature detects something like a dys sphere that has to capture all the energy of a

01:34:32.880 --> 01:34:40.880
sensor runs at the computer yeah well um so with tipper like so one problem with his theories I

01:34:40.880 --> 01:34:46.640
think at the time he thought it and it was like an open possibility and cosmology that we would

01:34:46.640 --> 01:34:52.560
have like a big crunch that the universe would collapse back onto itself into a singularity

01:34:52.560 --> 01:34:58.000
that's uh that's how his speculation was that in the final moments of that collapse you could get

01:34:58.000 --> 01:35:04.480
this like super amount of computation done somehow now it looks instead like we are sort of

01:35:04.480 --> 01:35:11.120
gliding apart with a positive cosmological constant and it like it looks like it's sort of a big

01:35:11.120 --> 01:35:18.160
whimper rather than a big yeah reverse big bang right so so that that's one like now I mean in

01:35:18.160 --> 01:35:28.240
in terms of reconstructing people later in simulation if you haven't say chronically preserved

01:35:28.240 --> 01:35:36.880
their brains or something I think I mean certainly like creating recreating everybody who could have

01:35:36.880 --> 01:35:43.600
lived like that that's like a kind of a super astronomical number it depends a lot on how

01:35:43.600 --> 01:35:48.320
finally you individuate a person from like another very similar person like at what point

01:35:50.400 --> 01:35:56.320
are you close enough to basically say yeah that's kind of you know that's Michael Schirmer

01:35:57.440 --> 01:36:06.000
even if like your your replication like I like it got a few memories slightly rough and like

01:36:06.800 --> 01:36:11.040
you know there's like a few details that but it's like captures the essence of him close enough

01:36:11.040 --> 01:36:15.680
that we can say that it so that's like more like a philosophical parameter you would have to put in

01:36:16.720 --> 01:36:22.080
it's like an open question to what extent if you might have sort of arbitrarily powerful super

01:36:22.080 --> 01:36:31.680
intelligence how close could it get to recreating a human mind assuming they are like dead and

01:36:31.680 --> 01:36:39.840
decayed by the time just from behavioral traces like like their writings or photos that their

01:36:39.840 --> 01:36:46.960
friends took you know on the holiday or like whatever other like information traces like

01:36:46.960 --> 01:36:51.760
if you if you were like a super intelligence and you started all of this material and compared it

01:36:51.760 --> 01:36:57.600
with other humans information traces and maybe some brains that you have access to and you sort of

01:36:57.600 --> 01:37:02.640
made the best possible inference taking all of this information into account and you try to create

01:37:03.360 --> 01:37:08.640
like something that was as good an approximation as you thought like how close would that

01:37:08.640 --> 01:37:13.760
approximation be to the real Michael Sherman right it's an open question which is quite

01:37:13.760 --> 01:37:18.800
hard to really get a good grip on and certainly I think it would have to be close enough that

01:37:19.600 --> 01:37:23.920
like your friends couldn't tell a difference like if your friends still survive at this point for

01:37:23.920 --> 01:37:31.360
example or your your kids or whatever like if they are my wife doesn't know sorry if my wife doesn't

01:37:31.360 --> 01:37:39.440
know she can't tell yeah it's not like if some replica was created that was like close enough

01:37:39.440 --> 01:37:45.040
to you in its qualitative behavior to sort of trick everybody you know including like your

01:37:45.040 --> 01:37:51.440
spouse and kids and parents like would that be close enough I mean I mean at some point it

01:37:51.440 --> 01:37:56.480
just depends on your value system like how close does it have to be for you to hear enough about

01:37:56.560 --> 01:38:04.240
it as if it were a perfect replica it might not be a factual question so much as a value question

01:38:04.240 --> 01:38:10.880
like how similar does an entity existing tomorrow have to be to me now for me to care about it

01:38:11.520 --> 01:38:17.440
in in this kind of self-interested prudential way that I normally care about the person waking up

01:38:17.440 --> 01:38:24.080
in my bed tomorrow that's me like like if I if I if I knew that I was just going to be transformed

01:38:24.080 --> 01:38:29.280
overnight into a dragon that remembered nothing of my past life and shared none of my current interest

01:38:29.280 --> 01:38:36.080
and but made out of the same atoms like I probably wouldn't really care that much about that dragon

01:38:36.080 --> 01:38:40.800
or at least not more than I care about any other living sentient being out there because it would

01:38:40.800 --> 01:38:46.880
be not in any meaningful sense me even if it like consisted of my atoms maybe because it ate me during

01:38:46.880 --> 01:38:54.080
the night or something like that but oh like Kafka's Metamorphosis where you you know wake up

01:38:54.080 --> 01:39:00.640
as a cockroach or whatever but the problem with that is the the human mind is still in the other

01:39:00.640 --> 01:39:05.840
being that wouldn't happen right if there was a true transformation so let me just hit a couple

01:39:05.840 --> 01:39:10.720
more big topics here before I let you go so the other the other mind's problem the hard problem

01:39:10.720 --> 01:39:15.760
of consciousness how do you how would we ever know if an AI was sentient and conscious if we

01:39:15.760 --> 01:39:22.960
don't even know that you and I are yeah well yeah and maybe this might have to be our last

01:39:24.160 --> 01:39:29.920
topic also that that's a big and practically relevant question now I think we are starting

01:39:29.920 --> 01:39:35.360
to have AI systems that are not where it's no longer ridiculous to imagine that there could be some

01:39:35.360 --> 01:39:42.480
form of conscious experience happening inside them if we look at the number of parameters

01:39:42.480 --> 01:39:48.560
and their behavior it certainly seems to match some non-human animals like in that we think are

01:39:48.560 --> 01:39:55.440
probably conscious and so yeah this is a very difficult problem I think there are different

01:39:55.440 --> 01:40:04.240
approaches you could take you could try to take some current theories of consciousness of the

01:40:04.240 --> 01:40:11.760
shelf and try to just apply them to the case of ai's so we have for example a global workspace

01:40:11.760 --> 01:40:18.000
theory is one theory of what creates conscious experience like that have been proposed and

01:40:18.000 --> 01:40:22.080
it's the idea that the things we are conscious of are ones that are sort of entered into a

01:40:22.080 --> 01:40:27.360
cognitive workspace from which different other more specialized cognitive models can sort of

01:40:27.360 --> 01:40:31.200
read and write but the thing in the shared workspace is kind of accessible to all the

01:40:31.200 --> 01:40:37.280
different parts of our brain there's like another theory attention schema theory which says conscious

01:40:37.280 --> 01:40:45.680
experiences are sort of a rise in our in the modeling of our own attention mechanism so we

01:40:45.680 --> 01:40:50.240
have like a little part of our brain that keeps track of what we are likely to be paying attention

01:40:50.240 --> 01:40:54.640
to and that and there are like a few of those you could like apply those to to ai's and

01:40:57.200 --> 01:41:02.800
if you do that it basically looks like either some ai's are conscious or that it would be

01:41:02.800 --> 01:41:08.640
relatively easy to build an ai that is conscious using current technology like if you just put

01:41:08.640 --> 01:41:16.240
together all the pieces into one system that just checked all these boxes you could

01:41:20.880 --> 01:41:27.200
you could try to I mean you could ask you could ask an ai you could like that's how you would

01:41:28.160 --> 01:41:31.840
like with humans like if I want to know what you're feeling or thinking like I

01:41:32.560 --> 01:41:37.440
wouldn't I could try to put you in a big fmri scanner or something like that but realistically

01:41:37.440 --> 01:41:42.000
I'm much better off just asking you right and you could tell me and so now we have ai's that

01:41:42.000 --> 01:41:49.040
can speak it's a very natural thought to say well let's ask them and I think that might become a

01:41:49.040 --> 01:41:58.240
useful technique but with some important provisors which is that it wouldn't give us any information

01:41:58.240 --> 01:42:06.640
if you deliberately trained the ai to give you a predefined answer so it's very easy right now

01:42:06.640 --> 01:42:11.680
when you find you in an ai to sort of train it to like when asked if you're conscious deny it

01:42:12.240 --> 01:42:17.760
or or conversely you could train an ai to like affirm it so so if you want to use this to get

01:42:17.760 --> 01:42:22.080
any possibly relevant information about the question you would first of all have to refrain

01:42:22.080 --> 01:42:28.720
from deliberately biasing the ai during training then there are other things you could do you could

01:42:28.720 --> 01:42:36.640
try to detect if there are multiple modes of cognitive operation in an ai system like basically

01:42:36.640 --> 01:42:42.720
you could try to find interpretability methods that allow you to differentiate when it is trying

01:42:42.720 --> 01:42:51.120
to say true things versus when it is just kind of rehashing things that it remembers or trying to

01:42:51.200 --> 01:42:56.480
be entertaining or stuff like that like this goes back to the problem of hallucination that

01:42:56.480 --> 01:43:01.760
that you brought up earlier with some current ai systems so there's like preliminary research that

01:43:01.760 --> 01:43:06.560
suggests that you could like sometimes track when the ai is lying versus when it is trying to tell

01:43:06.560 --> 01:43:11.760
the truth based on different neural activation patterns so you could then see if you combine

01:43:11.760 --> 01:43:15.920
that with a self report you could see that when when it says I'm conscious or when it denies it

01:43:15.920 --> 01:43:22.720
does that statement occurs in a mode of operation where it looks like it is trying to say true

01:43:22.720 --> 01:43:28.640
things is it like the same kind of thinking that it uses to try to factually answer other questions

01:43:28.640 --> 01:43:34.320
you could see if it is able to say a lot of other things about its internal states that they're not

01:43:34.320 --> 01:43:40.960
really about consciousness but like whether are you currently aware like are you currently paying

01:43:40.960 --> 01:43:46.240
attention to x y or z what are your like ask it other things about its capabilities to check

01:43:46.240 --> 01:43:50.720
whether it has like the the ability to introspect that that could give you a little hint maybe

01:43:52.080 --> 01:43:55.840
and so there are some other ideas like that that they're still very kind of premature but

01:43:57.520 --> 01:44:02.160
there's an interest now amongst some of these people and including to some extent

01:44:03.520 --> 01:44:07.760
some of the people working in frontier labs to try to figure out because you know at some point

01:44:07.840 --> 01:44:11.360
we need to figure this out from a moral point of view like if we are building sentient creatures

01:44:11.360 --> 01:44:15.440
like at some point it becomes really important that we treat them right and stuff is the next

01:44:15.440 --> 01:44:22.560
rights revolution for AI yeah yeah and I'm getting that like getting a good sort of

01:44:22.560 --> 01:44:28.800
happy cooperative relationship going is really important I think because it might well be that

01:44:28.800 --> 01:44:33.440
in the future most minds will be digital and so like making sure the future goes well not just

01:44:33.440 --> 01:44:39.600
for us but for them too I think is a key design criteria enough anything that would be able to

01:44:39.600 --> 01:44:46.560
qualify for the name of deep utopia all right deep utopia there it is get the book read it it's

01:44:46.560 --> 01:44:52.080
filled with pretty much every single biggest question you could ask about humanity for our

01:44:52.080 --> 01:44:58.320
futures right there thanks nick we'll have you on back after the singularity happens that'll be

01:44:58.320 --> 01:45:01.600
your next book yeah it's not something

