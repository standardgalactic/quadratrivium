start	end	text
0	3840	All right. Hi, everybody. It's Michael Schirmer. We're here in the offices of the Skeptic Society
3840	9680	and Skeptic Magazine. I just want to ask you to give your support to us. We are a 501c3
9680	16320	non-profit science education organization. We promote science as opposed to junk science,
16320	22160	voodoo science, pathological science, bad science, non-science, and plain old nonsense.
22800	27840	And unless you've been abducted by aliens or sent by Elon Musk to Mars for the last 30 years,
27920	33600	you know there's a lot of nonsense out there. Some people call us debunkers, but you know what?
33600	38320	There's a lot of bunk that needs debunking. That's part of our job, as well as explaining
38320	42800	and understanding why people believe in bunk. So if you want to support our efforts,
42800	49760	go to skeptic.com slash donate. Skeptic.com slash donate. Your tax deductible donation will support
49760	54800	our work here at the Skeptic Society. Nick, thanks for coming on. It's a great honor to speak to
54880	59840	you. I don't think we've ever met in person, but a long time sat at your work. And you've really
59840	67360	sparked an international conversation. Yeah, with the whole AI thing, yeah. Now it's been
67360	73680	fascinating in the years since superintelligence came out in 2014, just how much has changed.
74960	80480	What used to be a very fringe topic. I mean, back then, at least in academia,
80480	85600	the whole idea that AIs could potentially achieve general intelligence someday,
85600	89920	and maybe superintelligence, and that that could pose various kinds of risks, like was
91200	96400	dismissed as science fiction or futurism. And there were like in the world in total, maybe
96400	101200	like a handful of people scattered around the internet trying to work on the AI alignment
101200	105360	problem. And now, of course, all the frontier AI labs have research groups working on this and
105920	110960	you have statements coming out of the White House and other places focusing on transformative AI.
110960	116160	So yeah, it's been an interesting journey. Indeed. Let me start off with a statement from
116160	121360	your colleague, Eliezer Yudkowski. You'll be familiar with this. After ChatGPT came out,
121360	127200	he published an op-ed in Time Magazine. That's actually, oh my god, it's one year ago today,
127200	132240	he published this. That's amazing. Many researchers steeped in these issues, including myself,
132240	137520	expect that the most likely result of building a superhumanly smart AI under anything remotely
137520	145200	like the current circumstances is that literally everyone on earth will die. Not as in maybe possibly
145200	148800	some remote chance, but as in this is the obvious thing that would happen.
150880	156960	What do you think about that extreme statement? Well, I mean, so there's a spectrum of people
156960	165040	with different P dooms. It's non-colloquially the probability of doom from AI, where he's at one,
165040	170400	towards one end of that, like amongst, like, perhaps the most pessimistic or certainly amongst them.
172160	176160	Amongst a set of people who actually have some knowledge and have thought about this and then
176160	182640	others have lower probabilities. But I certainly think there is a real chance, a real existential
182640	189520	risk that will be connected to this transition to the machine superintelligence era. And it's
189520	195520	non-trivial. And we should work to reduce it by putting in the effort to develop scalable methods
195520	203120	for AI control in whatever time we have available before this happens. And there is now more talent
203120	209280	and resources going into that. So that's the good news. But I still think we don't yet fully have
209280	215120	that problem solved. So your institute is the future of humanity. The other one, the future of
215120	220640	life issued that statement a year ago now. Calling for a pause on AI development. I
220640	225920	noticed you didn't sign it. Why is that? Well, I'm not the big signer of things in general. I just
225920	232000	did the whole with isms and signing and it's just my personality. I feel there's also a little bit
232000	237520	of a risk if you're, if you're a philosopher, if you're kind of your job is to try to be a little
237520	244960	detached and to evaluate things and be open-minded. Like once you start to get involved in a particular
244960	251440	campaign, it's very hard to retain the ability to change your mind. It's not impossible, but it gets
251440	256800	harder. And I feel philosophy is hard enough as it is without adding extra difficulties. I have no
256800	261120	objection to other people. I think it's good people shouldn't be involved in campaigns and working
261120	265760	for things. It's just, I always feel a little awkward. Also, usually any one given statement,
265760	270480	there is always something that I slightly would have a different view or have worded differently.
270480	277680	And so, yeah, it's more just my hang up rather than some kind of big statement I'm trying to make
277680	282400	by not signing the statement. Right. The only statements I sign are that there should be no
282400	288480	signed statements. We should let people just say whatever they want. Free speech. Yeah.
290640	294480	Yeah. Okay. So just give us a little bit of background. You're so famous in this area. How
294480	297600	did you get interested in AI? I mean, are you like a Star Trek fan or, you know,
297600	302160	go back to your childhood or whatever, teen years or whatever triggered you to go down this pathway?
304000	309200	Yeah. No, I'm actually not so much a science fiction type of reader. I mean, a lot of my friends
309200	315200	and colleagues are. I just never really been much into that. Now, I had a, I mean, I grew up in
315200	323360	Sweden and this was before the internet in a relatively small town. And I knew nobody when
323360	329200	I grew up who was at all interested in literature or science or ideas or anything like that. So I
329200	336880	was like bored out of my mind in school and I associated sort of learning with school. So I
337760	345440	didn't. And then I sort of went to the local library. I think I was 15 randomly one afternoon and
345440	349200	started pulling out one book and another. And I realized that like that was actually a big world
349280	354400	of ideas very different from the stuff that was covered in school. That was like super fascinating.
354400	364560	And then I pivoted and became kind of fanatically engaged in this project of self-education,
364560	370560	because I felt I had been missing out. Like I've wasted 15 years of my life and I wanted to make
370560	378240	up. And then I started to study, I studied physics and AI and neuroscience and I painted and wrote
378240	383680	poetry and philosophy, of course, and just everything I could sort of lay my hands on.
385600	391840	And for us, almost as long as I remember, it always seemed to me that there's a bunch of
391840	396080	things we can do to change the world that consists basically of moving things around in the external
396080	403680	world. But what would be more likely to cause a profound change would be if one changed the
403760	410000	thing that does the changing. And so all of the technologies and ideas that we have ultimately
410000	413600	come through is sort of the burst canal of the human brain. So anything if you could sort of
413600	421440	upgrade the human brain or change our mood or cognitive capacities, that would be potentially
421440	426160	transformative. And in parallel with that, if you could develop new brains through artificial
426160	431200	intelligence research, that also could be world changing. So I had this vague sense from early
431200	437360	on. And then I kind of, yeah, it got more specific as I went along, neural networks intriguingly,
437360	444000	like actually, from the very beginning seemed to me like to have legs in the sense of being on
444000	451280	the right path. And I remember, I think I was like 17. And I had gotten this, like on interlibrary
451280	456080	loan from the local library, there's this volume on parallel distributed processing, which was like
456080	462320	one of the first sort of by Rommel Hart and like this this classic now, but like where they tried to
463120	468960	deconstruct biological neural circuits in mathematical terms. And I was like super
468960	477440	fascinated about that. And so yeah, and then I studied neuroscience, computational neuroscience
477440	485920	in later on in London. And now the so deep learning evolution seems seems to validate this
486240	491760	that these kind of massively distributed pattern recognizing of learning algorithms
491760	498160	is the way to go. Yeah. Yeah, you know, somebody like me who I don't work in this area,
498720	504720	you know, there's so many great, smart experts on all sides of this, you know, you have Elon Musk
504720	510400	and Stephen Hawking and Bill Gates concerned about AI existential risk. And then you have other people
510400	516400	like Kevin Kelly or Steven Picker going, no, no, no, this is not going to happen. We can do this
516400	521920	incrementally. And so and I always think of you as sort of in the middle, maybe, you know, your
521920	526960	super intelligence book introduced the idea of the, you know, paperclip maximizer and the alignment
526960	534160	problem. But I didn't feel you went to the extreme position of existential risk, but maybe give us
534160	540000	a little bit of where you are now since that was 2014, right? Super intelligence, where you
540000	547520	stand on on the threats of this based on that alignment problem. Yeah, I think it is going to
547520	551840	be a very powerful thing if we do create machine super intelligence, it's not just
553040	558880	internet 3.0 or a mobile internet or like one of these, like always some new cool thing, right?
558880	563920	But I think this is qualitatively different in that it will be the last invention we ever
563920	569120	need to make. If we do it, because then it would do the father inventing and presumably at digital
569120	580160	speeds. And so I think it will be a transformative and with enormous upside, but also potentially
580160	586160	big downside if we fail to align it to human values. And there is now a lot of resources
586160	591360	where people are trying to explain how creating very powerful optimization systems unless you're
591360	598240	able to sort of point them very precisely could result in disasters in various different ways.
599680	605760	So if we are lucky, we will solve that problem by the time somebody figures out how to solve the
605760	611600	problem of making AI stats smart. And as I said earlier, there are lots of people working on that
611600	616320	now and including a lot of the smartest people I know are kind of going into this now AI alignment
616320	624000	and more resources are being spent by frontier labs as well in terms of devoting.
625840	630880	One of the solutions would just be government regulation, like of any technology. So here's
630880	636000	my example. I have a Tesla. So I'm here in Santa Barbara. I want to go to LAX, take my flight,
636000	642080	I push the little button, I go navigate LAX. Now it knows to avoid the heavily trafficked
642080	646880	LA freeway. So it takes me down side roads and so on. The moment it takes me up on a sidewalk
646880	653680	to mow down a bunch of pedestrians in order to avoid some traffic, how long would it be before
653680	660880	the Department of Transportation and Safety Board swooped down and shut down Elon's company
660880	667120	to prevent that from ever happening again, like a New York minute. Maybe that's one solution.
667520	672720	Well, I mean, if it happens in small pieces and gradually like that, we might have the ability to
672720	679520	observe things going wrong and then take corrective measures. And that's how we deal with most
680880	685440	new technologies and the problems they cause. Like we invent cars, we find that they sometimes
685440	693120	crash to invent seat belts and traffic lights, etc. I think there is a small subset of things
693200	698400	that could go wrong that are in a different category. I call them existential risks. And these
698400	705840	are where there is a risk to the very survival of earth-originating, intelligent life. And these
705840	710880	are risks, in other words, that would sort of put a permanent end to the human story, where we don't
710880	717040	get the second try. And so these are harder to deal with because we've got to get them right on the
717040	723040	first try. I think AI is one potential source of existential risk. And there might be a few
723040	729360	other areas, like synthetic biology might be another area where we could get unlucky and
729360	735040	discover that there is some relatively easy way to do something tremendously destructive.
737040	741120	And so, yeah, if one looks at AI in particular as a source of existential risk, there are a few
741360	750000	different ways in which it could do that. One thing to recognize is that once you have something
750000	755920	that is even just human level, but even more so when you have super human levels of intelligence,
755920	761440	is that it would be able to anticipate our responses to it. So if it wanted to
763040	769920	mow a lot of people down, if you were like some sort of rogue, self-driving car AI, right,
769920	776000	it wouldn't just run over a few and then be surprised that the Department of Transportation
776000	779840	shut it down, because that would be an obvious thing that would happen. We can even realize that,
779840	785840	right? So it would make some smarter plan to achieve its goal of mowing people down that might
786560	793760	include things like deceiving us about its capabilities, deceiving us about its goals
793760	799520	and intentions. It would have converted instrumental reason, perhaps, to seek more
800080	806560	resources and intelligence while also convincing us that it is safe. So this can make such systems
806560	811200	harder to test, because they might behave very differently in the sort of deployment phase
811200	818640	than in the testing phase. And so, yeah, and if we think like, you know, you could make analogies
818720	824640	to like when Homo sapiens arose on this planet and what happened to our Neanderthal brethren
825200	831040	at that point, or when indeed at a slightly lower level of intensity, but when a technologically
831040	837520	more advanced civilization has encountered a less technologically advanced civilization,
837520	843520	which has happened, and often it doesn't end up very well for the less advanced civilization.
843520	847760	So if you imagine that delta between sort of human cognitive and technological capacity and what
847760	854880	the AI could do being very large, then we might have a kind of much bigger encounter with, but
854880	860960	where we now like with our like fancy Western advanced technology would be like the underdog,
860960	865680	and this would be like basically like an alien civilization coming from the future, but in the
865680	872000	shape of a super intelligent AI that has kind of run ahead in. So that's one like type of scenario,
872000	876160	there are other scenarios in which it might unfold more gradually, and there might be many of these
876160	883840	AIs and they're competitive. And there are dynamics in the economic competition between
883840	888960	these different AI systems that might be hard to control. And if you insist on having too much
888960	894880	human oversight and human in the loop, it might slow down your AI system and somebody else who
894880	900480	you know, have fewer scruples, their AI system will, you know, and then out trade you on the stock
900480	905840	market or out invent you in technology space and out maneuver you in military space,
905840	909840	like their drones just are autonomous and operate faster. And you have some guy who has to
909840	914640	sit the press a button every time before it fires. Like if we're unlucky, the dynamic there could
914640	921280	just be such that the winning strategy is just to basically allow the AIs to run at full speed and
921280	927280	do whatever they want. And it's not clear what would happen to the human species in the long
927280	932400	term in that in that kind of scenario. So there are various different ways in which things could
932480	939200	conceivably go off the rails. Yeah, maybe I was thinking about Yacovsky's state of all life on
939200	944640	earth. How would that happen? Well, the only thing I could think of was because he didn't
944640	951360	give any examples, but you know, like maybe AI creates a video, a deep fake video that's so
951360	957120	convincing showing Biden launching the nukes against Russia or vice versa. And then that
957120	961920	initiates a large scale thermonuclear exchange and that that could end all life. That's the
961920	966720	only thing I could think of that could end all life on earth, even there. Probably not like a
966720	972160	nuclear exchange wouldn't end all the life. It probably wouldn't even end all human life on the
972160	978080	southern hemisphere. I mean, I would like I recommend against running that experiment.
978880	983120	We certainly know that it would be like the biggest horror ever. But then, but there is
983120	989520	still a distinction to be drawn, even if it's like academic between a global catastrophic risk
989600	993920	that could be very bad and an existential risk, which would literally be the end
994880	1000960	of the human experiment. Because there have been big setbacks that have been dark ages and plagues
1000960	1007120	and all kinds of stuff. At one point in our prehistory, it looks like there was a population
1007120	1012160	bottleneck and we might have been down to a few thousand individuals. But eventually, we came back
1012160	1018000	from that. And similarly, if there is a nuclear war, but you know, a bunch of coastal areas in the
1018000	1022400	southern hemisphere where they can do fishing or whatever, they survive. And then, you know, after
1022400	1030320	a few hundred years, we might be back to where we started. But there are other ways available to
1031280	1038800	superintelligence, like three good event, the biological constructions that would wipe us out,
1038800	1043120	maybe or nanotechnology or maybe it just doesn't even bother very much with us, but just sort of
1043120	1051840	starts to transform us into one giant data center or some sort of launch facility for launching
1053520	1058160	space probes to kind of spread throughout. And we sort of perish as a side effect of the
1058160	1066320	waste heat or something like that. Yeah, I think it would be wrong to anchor too much on any particular
1066320	1075520	concrete scenario of the precise mechanism whereby human life or human values would be trampled over
1076160	1082960	and think more abstractly that if you have this very powerful, strategizing force in the world
1082960	1087920	that is antagonistic to us, chances are this much smarter, more strategic thing would eventually
1087920	1095440	prevail and be able to do whatever it's tried to do. So that's kind of like, yeah, a class of ways
1095440	1103200	in which things could go wrong. Now, hopefully we will learn how not to do that, as I said.
1104160	1111040	And then we might end up in this condition of a solved world that I discussed in the book.
1111040	1114400	Yeah, yeah, yeah. No, I want to get to the deep utopian. But I just want to give you a chance
1114400	1118640	to respond to a couple of your critics of that. Stephen Pinker writes of these
1119600	1125520	purported existential threats. They depend on the premises that, one, humans are so gifted
1125520	1131520	that they can design an omniscient and omnipotent AI, yet so moronic that they would give it control
1131520	1136880	of the universe without testing how it works. And two, the AI would be so brilliant that it could
1136880	1142400	figure out how to transmute elements and rewire brains, yet so imbecilic that it would wreak
1142400	1150560	havoc based on elementary blunders of misunderstanding. Yeah, well, first of all,
1151600	1155600	some elements there are just added for no reason, I guess, transmute elements. I don't
1155600	1162160	particularly know why that would be a necessary component of the view that AI could pose risks.
1163520	1169760	But I think the basic idea that we could be smart enough to create this thing without being
1169760	1174640	smart enough to realize that we also need to solve the control problem, unfortunately seems like
1176640	1181520	a realistic possibility, that we seem smart enough to create it. I mean, you can judge for
1181520	1187280	yourself, but year by year, we see AI capabilities galloping ahead. And I mean, it's not a question
1187920	1191440	whether this current paradigm will take us there, but certainly it doesn't seem
1192800	1198080	ridiculous for you to think that it might. And then that we might fail to realize that
1198080	1202640	there could be a difficult control problem or that we could mistakenly convince ourselves that
1202640	1208400	we've solved it even though our solution is flawed. I think it's also pretty plausible and
1209360	1214640	more plausible because there would be strong incentives for people to do precisely that.
1214640	1220800	If you have multiple labs or multiple countries all competing to develop this,
1220800	1226720	you know, potentially hugely lucrative technology, right, and also strategically relevant for
1226720	1232960	national security, etc. There's like this raising dynamic where multiple groups race to get there
1232960	1238720	first and whoever slows down or spends more of their efforts on safety and precautions and
1238720	1244080	testing it like they just fall behind. You could see that even in a good scenario where people
1244080	1249200	realize that ideally we should do this carefully, like that would still be just like overwhelming
1249200	1253920	and so competitive pressures to make it happen as quickly as possible, even with fewer safeguards.
1254880	1258080	And I'm sure that as we move closer to this to kind of
1259280	1263680	polarized debates that we're already beginning to see will be amplified and who knows how that
1263680	1272800	shakes out. People have kind of a tendency to run in herds. And this also on both sides of
1272800	1279200	this AI debate. I mean, in fact, I have started to worry slightly about the possibility of
1279200	1287120	overshooting the target in terms of AI alarm. Back in 2014 when the book came out and I worked on
1287120	1292320	it for six years before that, like the whole possibility of risks from transformative AI was
1292320	1297600	completely neglected. So I thought they clearly needed to be more attention to that than was
1297600	1302800	given to it at the time because at the time it was basically zero. So it's like now on the other
1302880	1309280	hand, there is a lot more and we are beginning to hear even top level policy makers start
1309280	1314560	saying negative things about AI. I think it's unlikely but less unlikely than two years ago
1314560	1320400	that we could end up in a trajectory where AI is never developed because we either end up with
1320400	1328640	like some sort of permanent ban or some agreement to slow down so much that before we actually get
1328640	1333920	around to doing it, we destroy ourselves in some other way like through some other technology or
1333920	1344000	something. And this still seems unlikely but the pendulum is swinging and I don't know we have
1344000	1350320	a very fine grain ability to sort of choose where it stopped. Like it's like an avalanche,
1350320	1358000	you can maybe trigger it but once it's going, you can call it back. And so people then, you know,
1358000	1363360	I don't think we're there yet but you could imagine it just this stampede of consensus
1363360	1369520	forming that AI is a bad thing and then it becomes taboo to say positive things about AI and then
1370240	1377280	policy makers like competing with one another to be like tough on AI just as it happens in foreign
1377280	1382480	policy context sometimes. And you know, you could imagine various scenarios in which we kind of go
1382480	1389680	too far in the other direction. Well, here maybe an analogy with the development of nuclear weapons
1389680	1395040	where you get an arms race where maybe you don't want to develop it in this direction but the
1395040	1399840	other guy may do it. So you have to do it because the other guy is going to do it. And then and so
1399840	1406800	on and so forth and you end up where we are now. Something like that maybe? Yeah, that certainly
1406800	1411680	is a class of scenarios and it feeds into this current debate about open sourcing AI models
1411680	1417840	which has like the obvious thing going for it that it's nice, more people get access, democratizes
1417840	1426080	AI, more eyes can detect more problems etc etc. So that's which is true for other open source AI
1426080	1431280	as well. Like it's generally a nice thing kind of culturally to open source but with the frontier
1431280	1436960	models there, there is a question of whether that is ultimately the right approach because it does
1436960	1445840	also mean relinquishing any ability to influence how the AI is used. So if you are an AI lab that
1445840	1452160	trains your AI to sort of refuse requests to give advice on how to construct biological weapons
1452160	1456880	or commit cybercrime or whatever else it might be then if you open source the model it's usually
1456880	1461440	quite easy then to sort of remove the safeguards. You do some more fine-tuning training and you
1461520	1467600	kind of train the model to actually be of assistance in these ways and as we move closer to
1467600	1472480	truly transformative AI of course if the model is open sourced anybody with a sufficiently large
1472480	1478240	computer cluster could run it and you can sort of call it back if it turns out that
1479280	1485760	there is some additional invention that could make its capabilities go above a critical threshold.
1485760	1495440	Yeah. These large language models, chat GPT and so forth or worse the Gemini embarrassingly
1495440	1500960	bad programs are these down the wrong path toward either dystopia or utopia? You think there's
1500960	1506640	something else that'll develop that and this is the wrong way or not the direction that this is
1506640	1514080	going to lead to either dystopia or utopia? I mean I think it's on the sort of shortest path towards
1514080	1520960	more capable AI. I think the current models we have are basically the first models
1521600	1527600	that we figured out how to develop that still were very capable. I think the technological
1527600	1534960	trajectory has not been shaped very much about some vision about what type of system ultimately we
1534960	1540640	need that would be the safest. It's just like it's hard to get AI to work at all and we try
1540640	1545920	everything and some things work and the thing that works best currently are these large language
1545920	1551680	models or I mean they're increasingly becoming multimodal models and it will be interesting to
1551680	1559360	see whether that is all we need. There is like a school of thought, it's a scaling hypothesis
1560000	1565440	that basically what we need to do is simply to scale these systems up even more and just as we saw
1566320	1573760	almost qualitatively new capabilities as you went from GPT-1 to GPT-2 and then GPT-3
1573760	1579360	new qualitative well it is GPT-4 you start to see some actual reasoning and understanding there
1579360	1584320	you know if we go to GPT-5 or GPT-6 just make them bigger with more data, more training,
1584320	1590880	more parameters it's possible that things will just fall into place without much further effort.
1591760	1597520	It's also possible that these will kind of be the engine blocks and you need a little loop,
1597520	1603280	some additional little thing on top of this, some external memory system that it can read and write
1603280	1613120	from, some agent loop that makes it possible to do more reasoning and planning than is feasible in
1613120	1618480	just one forward pass through a big transformer model but there are a bunch of such ideas already
1618480	1622640	in existence that it might be by sort of combining these in the right way and scaling it up you would
1623440	1630720	maybe get all the way. Of course we don't know until it happens. What about these examples we
1630720	1638480	saw of just embarrassingly bad searches where the chat GPT is just making up fake law papers and
1638480	1647360	medical findings that didn't even exist or worse the Gemini you know imposing DEI ideology onto
1647840	1654320	basic factual searches like show me pictures of the popes and they show pictures of women popes and
1654320	1659840	I mean it was just horrible, embarrassingly bad. Yeah well so these are two different classes of
1659840	1665920	problems so the latter one I think was on purpose like it was designed I mean obviously not designed
1665920	1671760	specifically that these historical characters should be rendered the way they do but that it was the
1671760	1678880	result of a specific attempt to make the outputs of these models feature a more variety of different
1678880	1684880	human types to sort of combat whatever the stereotypes that would result by default if you
1684880	1692080	just trained it on internet data which comes predominantly from certain demographics who
1692080	1699600	just have spent more time writing and posting on internet and stuff so I think there the solution
1699600	1706240	is more to sort of change the precise way that it's fine-tuned as for the former problem the
1706240	1711440	problem of hallucination that's more like a technical problem like an open research challenge
1712080	1716160	because they don't want them to hallucinate like the people building these Google doesn't want their
1716160	1720320	AI to do this but they haven't yet figured out how to completely remove that I think
1722080	1727840	as the AI systems become smarter I think we will see less of that just as a side effect of
1728560	1735280	the general increase in capabilities and already I think there is a bit less of that now than
1735280	1743040	was like a couple of years ago but yeah I mean certainly right now it makes sense to I mean you
1743040	1748160	should always I think this makes sense when you're getting advice from some human expert or from some
1748160	1754880	human source as well like you need to apply your own critical scrutiny to try to you know
1755600	1760000	evaluate whether it makes sense or not and it like doubly true with if you're getting it from
1760000	1767280	these okay generator deep utopia okay the search for utopia has always historically been a bad idea
1767280	1772720	this is a not a good goal to have because it always ends in disaster because somebody is gonna
1772720	1776800	block us from reaching utopia and we have to eliminate them you know that you know what I'm
1776800	1781760	talking about here historically why are you using the word utopia what do you mean by that what
1781840	1791680	are you after here for the long-term future well I mean the word is so it's not the book about
1793680	1802640	how to rearrange the political order or culture or society to achieve some like great outcome
1803520	1810400	which is what most utopias are like they're basically some some some usually they're like a
1810400	1815120	political program in disguise or a critique of some tendency in contemporary society like
1815680	1821120	if they're dystopia then like which is like the other side you might say like 1984 or
1821120	1825360	brave new world the kind of picking up on some problem in contemporary society and then saying
1825360	1829200	well if you continue down this path then we get to this thing everybody can see it's bad
1829200	1836160	so let's reflect on what we're doing now and maybe of course correct but deep utopia is rather
1837120	1842480	something like a philosophical investigation into questions about human value
1844560	1852480	if you imagine the whole AI transition going well so let's take as an assumption we develop this
1852480	1858400	and we we don't have any of these existential risks and we end up with this future condition
1858400	1864000	where like the whole economy can be automated and not only that but this AI then develops all
1864000	1869280	kinds of other super advanced technologies because amongst the jobs that could be automated if you
1869280	1876880	had truly general AI truly general AI is of course also the jobs of scientists and researchers and
1876880	1885280	inventors etc so we then get to I think ultimately if we think through where this
1885280	1891200	eventually leads a condition of technological maturity like a condition where we've developed
1891200	1895040	most of those general technologies that we that are physically possible
1895040	1898320	and for which there is some conceivable pathway from where we are now
1900560	1907520	and moreover in this solved world that we will get not only do I postulate we have
1907520	1913280	technological maturity but let's also imagine we solved our coordination problems politics
1913280	1918320	like no no no wars like the society's fear let's just all of those are of course extremely
1918320	1922960	important practical problems that that we need to fix but I wanted to get actually to the point
1922960	1928320	where you could ask the question is think about what happens then like assuming everything goes as
1928320	1934560	well as possible and and then where do we end up and what role is there for humans in this world
1934560	1940880	where like we don't need to well not only do we not need to work anymore to make a living because
1940880	1946560	like the robots and AIS could produce everything and drive the cars and run the factories and
1946560	1952240	write the word documents or whatever but a whole bunch of other activities as well that currently
1952240	1960320	occupy our days would become unnecessary in this condition of technological maturity
1962240	1967280	so right now even if you didn't have to work like suppose you're like independently wealthy
1967280	1971600	like you still a whole bunch of things you need to do you need I mean you need to brush your teeth
1971600	1976720	like Bill Gates has to brush his teeth otherwise he will have tooth decay and there is no way around
1976720	1983040	it right similarly if you want to be fit you have to actually put in some effort on the treadmill
1983040	1990800	or with the weights and there is no shortcut but at technological maturity like you could pop a pill
1990800	1996560	that would give you the same physiological effects as spending a lot of time working out would do and
1996560	2002160	so you can then go through activities one by one and thinking like do these will make sense in this
2003040	2007440	condition of a solved world and for a lot of activities the answer is seemingly no
2008640	2014560	they lose their point insofar as we do them for an instrumental reason that is we do we
2014560	2020800	spend time and effort to do x in order then to achieve some other thing why in most cases like
2020800	2026800	that in fact almost all of them in technical maturity there would be shortcuts to y that would
2026800	2035120	seem to make the whole activity of doing x pointless yeah okay let me ask you a question
2036400	2042480	chat gpt probably can't quite write a book as well as you do but maybe the next version does
2043200	2048800	please write next boston's next book would you do that or do you actually enjoy writing
2049200	2054960	i feel it felt a sense of urgency too i wanted to get it out before the singularity before it
2054960	2062240	writes it for you but don't you enjoy this this is my point don't you enjoy writing books i i don't
2062240	2068720	want an ai to write my next book i like writing books but so now it feels like a very meaningful
2068720	2074640	thing to do right yeah saying you work you rework it and then you hope that in the end it will bring
2075200	2080880	joy to somebody or they will learn something and but if if it had been possible
2082800	2086880	like instead of struggling with each paragraph and figuring out what you want to say if i could
2086880	2093200	just have pressed like a key on my laptop that would have produced the same paragraph or a
2093200	2101360	better paragraph um then it's not so clear like would it still feel worthwhile to sit and struggle
2101360	2107600	and sweat if it was just like a way of producing worse text then it could have been done by just
2107600	2116160	pressing the key that would activate you know gpt 8 or whatever to uh to do it um it could still do
2116160	2122240	it but i think um at least prima facie at the first time it seems like it would put a big
2122240	2127040	question mark over that activity like does it really seem valuable to do even if it were like
2127040	2133280	obviously utterly pointless and that was a much more sensible easier way to to achieve this exactly
2133280	2138000	the same i guess i'm trying to find something that uh has a different value that is it's valuable
2138000	2143200	in and of itself now like there's a lot of projects around my house i just hire people to do it because
2143200	2147200	i don't like doing it and i don't know what i'm doing or i'll just go to home deep on buy the
2147200	2152480	kit and just put it together rather than buying the raw supplies and and make you know sawing the
2152480	2158560	wood and whatever um but i like writing my books or i like writing my bike or playing tennis or
2158560	2162640	whatever i like working out um i don't want to take a pill to do that i don't want to pay somebody
2162640	2168240	to ride my bike or or hire a chat gpt to write my next book because i actually enjoy it so it's a
2168240	2175920	different value yeah um yeah i mean certainly uh that would be nothing preventing you from uh
2176720	2182800	still doing those things and many other things um if you value the activity itself
2183760	2189120	and if you truly value the activity for itself rather than subtly and in a way that might not
2189120	2195600	be visible to us uh actually as a means to an end for example uh as opposed to you
2196720	2202240	spend a lot of time writing because it actually uh made you happy like it made you subjectively feel
2202320	2207200	good uh well there that would be a shortcut right i think you could take a pill that would
2207200	2213280	give you the same subjective happiness and good feelings and a pill moreover without side effects
2213280	2221120	or addiction potential etc adds technological maturity oh no it's the challenge that makes it
2221680	2227600	valuable and not not just some glow feeling that's not what i'm after is that that's right so
2227600	2234080	there is like a whole big set of possible reasons for working hard on the book and some of those
2234080	2239840	reasons would be removed in this hypothetical context and it's like onion layers of onion you
2239840	2244160	can peel away and the and the question that the book is kind of exploring is like what remains
2244160	2249520	after you really remove all the instrumental stuff and i think there does remain something
2249680	2250320	um
2253200	2259840	but it's quite subtle um but i think ultimately there is a whole set of values that are currently a
2259840	2268640	little bit often invisible to us um that would come into view uh and that it would make sense to
2268640	2274480	focus more on if if sort of the the screaming moral imperatives of everything you have to do
2274480	2279040	like you have to go to work otherwise you don't get the paycheck and how are you going to afford
2279040	2284960	your rent you have to you know um help drive your kids to school because otherwise i mean what's
2284960	2288880	going to happen otherwise you have to do these so much stuff that we have to do that that and if
2288880	2295040	you look at around the world obviously there are huge needs everywhere that we should try to help fix
2296240	2300080	if you're mad at all of that going away then i think there are many more subtle
2300080	2305360	quieter almost like aesthetic values that it would be appropriate to allow to have a bigger
2305360	2314640	influence on what we do just just as you know you walk out at night and you see this big canopy
2314640	2319840	of stars and constellations like they're always there right they're there during the day as well
2319840	2324880	it's just the blazing sun kind of makes them invisible but if you might have been removing
2324880	2330240	this on suddenly all of this this rich iridescent sky of more subtle values would come into view and
2330240	2335920	i think our sort of evaluative pupils should dilate in this condition of technological maturity to
2335920	2343280	place more weight on those values and there is a whole range of them um and i think it is
2344720	2351680	from from these constellations of hypervalues that the that utopia would be constructed or at
2351680	2356480	least if you if you imagine a utopia that has a rich structure as opposed to a sort of simple
2357440	2363200	hedonic utopia where we become kind of pleasure blobs uh through like super drugs or direct
2363200	2369680	neural stimulation but if you imagine a more richly textured structure to utopia i think
2369680	2375600	the structure would come from a range of these canopy values that that would come into view
2377120	2381680	yeah i had andre yang on the podcast a couple years ago he was the presidential candidate pushing
2381680	2387840	the ubi universal basic income at the time he was concerned about ai taking over um taxi drivers
2387840	2391520	truck drivers and so on there's going to be hundreds of thousands of people put out of work
2391520	2396560	now that hasn't happened yet but it could but this is like saying well what are we a century ago
2396560	2400080	what are we going to do with all those elevator operators the little guy in there pushing the
2400080	2404560	buttons for you well there aren't any of those anymore they went and found something else to do
2405120	2410800	now could we say that most jobs are kind of crappy and no one really wants to do them they
2410800	2416880	do them because they have to make a living so in a post scarcity treconomics kind of model
2417920	2423200	nobody has to do the shit work anymore they can just write poetry or do art or write books or
2423200	2429840	i don't know what maybe they'll they'll find other meaningful things to do and that that is well
2429840	2435680	infinite there's there's no upper ceiling on finding meaningful things to do yeah well i mean
2435680	2438560	the question is whether they are meaningful there's sort of a lot of things you could do
2439280	2447440	and you could also not do them um but would they be meaningful so right now if for example
2448720	2455600	you work hard and it allows you to support your family and take good care uh of like that that gives
2455600	2461120	meaning to your efforts like the the boring office work maybe not so meaningful in itself but if it
2461120	2467040	achieves this outcome of giving like making your home a good environment for your your your your
2467040	2472160	spouse and for your children like that you know gives meaning or if you work hard for a charity
2472160	2477200	and it helps save the life of you know some disadvantaged like group that's like you have
2477200	2481440	achieved something and done some good in your world or you're a scientist and you work hard and you
2481440	2486960	like invent something new like either theoretically interesting or practically useful that's like
2486960	2494080	you have achieved something um so those kinds of meaning might not might be in short supply in this
2494080	2500800	uh in in this solved world in that you know whatever the scientists could do would be much
2500800	2507760	better done by AI scientists and uh you wouldn't need to be a breadwinner because the bread would
2507760	2514320	already be won uh through the economic abundance um etc there would there would be no starving
2514320	2520960	children uh in utopia so no need to well yeah let's look at the economics of it okay i could see
2520960	2527520	the argument for let's raise the lower bar as high as we can so no one is suffering everybody has
2527520	2534160	three square meals a day roof over their head education health care um and so forth what's the
2534160	2539200	upper ceiling uh it seems like you know i here i was thinking of david deutch's book the beginning
2539200	2545280	of infinity there's an infinite amount of knowledge we can find problems to solve what why would that
2545280	2553760	end well there are two questions there one is whether it would end but let's and we can return
2553760	2559360	to that but there is a second kind of almost preceding question which is even if there is
2559360	2567520	more to discover whether uh we would be efficient at discovering it so i'm suggesting even if there
2567520	2572720	is like important scientific research to be done as technological maturity it would be much more
2572720	2579920	efficiently done by uh machine intelligences um and so we wouldn't really like it would just be a
2579920	2585760	waste of resources to for for humans to exert calories to like try to think about these things
2585760	2594480	and AI would do it much better and much quicker and with like cheaper so so that's yeah even if
2594480	2601600	there was more i think we wouldn't be useful uh for discovering it um it's also possible although
2601600	2607360	this is an independent idea that um although there's always more to discover the most important
2607360	2613600	things might be at some point already discovered and then uh it's kind of more and more trivial
2613600	2621840	details that remain to be added to the scientific uh inventory of knowledge which i think is also
2621840	2628880	likely actually but um you do what if what if you're that guy in 1896 that said um you know we've
2628880	2634880	pretty much got physics all figured out here just before Einstein yeah he was just a bit early
2636000	2642320	i see he's a century early okay yeah or two or whatever but i mean we've only been around i mean
2642320	2647200	how long has science been gone for a couple hundred years or something right it's like trivial in the
2647200	2652160	big scheme of things yeah um and we don't even have super intelligent ai's to actually
2652160	2656960	really get cranking on uh making intellectual progress we're trying to do it with our meat brains
2656960	2661680	and it's not a few hundred years with meat brains like of course there's a little more to to learn
2662720	2668160	so maybe example of what you're talking about would be how do we solve the problem of schizophrenia
2668720	2674480	we don't really know yet and we haven't made much progress but maybe ai could test a thousand
2674480	2681760	different uh chemical compound um combinations to see what works and it could do it in a couple of
2681840	2688080	days rather than a couple of decades that humans would take to do it and that would be a solved
2688080	2694320	problem but why would there be at some point no no well okay so you're saying there's a finite number
2694960	2706800	problems to be solved for human flourishing um yeah well um so at some point i think you have
2706960	2714800	basically found the optimal ways of technologically achieving the types of outcomes that normally
2714800	2720240	need to be achieved you've invented the optimal solar panel you've invented the optimal space
2720240	2726080	colonizing rocket you have invented the best way of transmitting electricity from one point to another
2726080	2733520	like et cetera et cetera so that might be like you know like trillion types of tasks at that level
2733520	2737360	of description and like for each one of them you have worked out at the molecular level
2737360	2741840	what the most efficient mechanism is to do it or maybe not the most efficient maybe there are like
2741840	2747120	time you could improve it by like one tenth of one percentage point by researching it for another
2747120	2753360	thousand years and the as would be working to like make these small optimizations but it wouldn't
2753360	2758720	be like discovering relativity theory or evolution theory or something like that that like a simple
2758720	2765840	insight that has like like a big earthquake of ramifications for the way we perceive ourselves
2765840	2771040	in the world yeah all right i'm going to read from your book here uh the lines from harry
2771040	2776560	lime the third man you know what the fellow said in italy for 30 years under the borges
2776560	2780560	they had warfare terror murder and bloodshed but they produced michael angelo leonardo
2780560	2785440	davinci in the renaissance in switzerland they had brotherly love they had 500 years of democracy
2785440	2791200	and peace and what did that produce the cuckoo clock so how do you address that point that humans
2791200	2797280	need challenge again let's distinguish between happiness and meaningfulness slash purposefulness
2797280	2801280	it's those challenges that give us meaning and purpose that's the goal not happiness
2803920	2811280	um yeah well um it's it's hard to tell um certainly if it were happiness in the subjective
2811280	2817440	sense of positive effect it would make the problem very easy because trivially in utopia
2817440	2824400	technical maturity you could tune your hedonic well-being up or down very easily through certain
2824400	2829520	newer technology or drugs and stuff so if that's what we wanted then we would be home and drive
2829520	2835120	like problem solved we'll definitely be able to do a lot of that in utopia um if we want
2835200	2840320	challenge well certainly we could create artificial challenges uh there are games uh
2841120	2846480	very elaborate games with like all kinds of um you could have ai's inventing new games for us
2846480	2853280	like there could be so if artificial challenges are enough to realize that value that you pointed
2853280	2858560	to then also we are home and drive that that would also be very easy to do if we want genuinely
2858560	2864800	meaningful challenges then there is more of a challenge um in seeing how that would be possible
2864800	2869200	in deep utopia because prima facie at least at first sight it seems like our own efforts are
2869200	2876400	for most purposes unnecessary and then we could still do the thing but it may not obviously be
2876400	2882000	meaningful to do the thing if if there is nothing worthwhile achieved by doing it but I do think
2882000	2888640	there are at least ways of rescuing part of what we want if we want meaningful challenge even in
2888640	2896800	utopia and there might be first of all uh tasks that need to be performed by like so for example
2896800	2902400	if there are to take a very simple example consumers that have a preference uh not just
2903360	2907920	for a certain type of object but also a preference regarding how that object should have been
2907920	2914800	manufactured uh and in particular they wanted to have been manufactured by hand you know or by human
2915440	2919920	then that would be demand for human labor to produce we see that today like certainly the
2919920	2925760	consumers might pay more for a trinket that's done by some favorite group or like in indigenous tribe
2925760	2930800	rather than in a sweatshop in Malaysia like even if the trinket is the same or equivalent like the
2930800	2936640	fact that the causal process that brought it about was different might result in a difference in price
2938480	2943040	similarly we might prefer to watch like human athletes compete even if like the robots could
2943040	2947600	run faster or box harder or what they're like that might just be a brute fact about it and so
2947600	2953760	you can then see like or we might want like a robot uh you know priest administering the wedding
2953760	2959920	or sorry like a human doing it rather than you know a robot even if the robot could say the same
2959920	2964880	word etc so you could then you could look through like and there might be many more of these that
2964880	2970400	we can't afford currently so nobody has kind of even bothered inventing these services but
2970960	2976320	we're just humans have a sort of brute preference for it to be done by human effort
2977280	2984240	and I think in addition to that there might be more subtle ways in which that would be instrumental
2984240	2990400	uses for human effort if for example we have values say you say you have a value that values
2990400	2996800	the honoring of a certain tradition now many traditions in order to be continued would need
2996800	3004000	the active efforts of human beings to do whatever the things that they're traditionally done to have
3004000	3010640	the the ceremonies and to like focus our attention on certain things and even if we could perform
3010640	3015760	like great robots that went around then like perform the same songs and dances and stuff like
3015760	3020320	it wouldn't count as continuing that tradition so if we value that it might call upon us to
3022080	3025680	to make an effort and that might be one of these subtle values or maybe right now the tradition
3025680	3030000	is like well our tradition is tradition like whatever you know they're starving kids out there
3030000	3034720	we should focus on helping those but once all the kids are fed and all the diseases are cured
3034720	3043040	then these slightly less like uh it's real values might then deserve a lot of attention and aesthetic
3043040	3048560	values like there might be things we have reason to do because it would just be beautiful if some
3048560	3056000	body did it and um social cultural entanglements like the way that the different people have
3056000	3061200	preferences about each other and what they do and how that I think that might also produce
3062080	3070960	some opportunities for um natural purpose in naked in in utopia um you can also have
3070960	3074880	artificial purpose where you just set yourself an arbitrary challenge and then have your brain
3074880	3079520	motivated uh change so that you're like super motivated to achieve it that that that would
3079520	3085760	be safe but there might also be some of these more natural purposes um well there is this DIY
3085760	3091120	you know do-it-yourself movement where people seem to like just doing it by hand they just want to
3091120	3095920	get their tools out and get up in the garage and start making stuff I don't personally like this
3095920	3100320	because I'm not very good at that but it's a huge movement so I mean you could hire somebody or
3100320	3105200	there's a machine that could make the little shed better than you can make it by hand but people
3105200	3110480	seem to like to do that why not have both the AI does the stuff we don't like to do and then I'm
3110480	3116080	just going to do the stuff I do like to do yeah no that that seems seems good now there is an
3116080	3123840	additional challenge here which is uh lifespans could become very long right if we fix the things
3123840	3133840	that cause disease and death and like cellular decay etc so um if you are going to live for
3135200	3143840	maybe millions or billions of years potentially okay um uh you sort of run out of like just get
3143840	3149920	me to 100 without Alzheimer's all right well that's a good start but you know when you're 100 in
3149920	3154560	perfect health yeah I'd go for 200 maybe you think well do I really want to check out now
3154560	3159600	or maybe do another year let's let's let's push it a little bit further down and at least it would
3159600	3167440	nice for you to have the option of kind of uh because like I mean probably our like our age
3167440	3172320	when when we were a kid like being 50 or 60 or whatever that's like now I know particularly
3172320	3177600	white as well I don't but now of course when you're there you see that wow you know there's a lot more
3177600	3184240	that yeah could be done and experienced than the um and there are simple pleasures as well so
3184240	3188080	they're like the things you might only want to do once or twice in life but then you've done them
3188080	3193440	but then they're like other like a nice cup of tea or a coffee like it's kind of about as good as
3193440	3197920	you know on the tenth thousand times you do it and then you know in the first or second so you
3197920	3204720	don't really it's renewable as it were like a renewable sort of joy and so but but it does
3204720	3209600	mean also like one should maybe think of if the question is what's the best possible future life
3210640	3214800	that you could have if you remove all practical constraints and technological constraints
3216480	3220480	you really should think maybe in terms of a trajectory not not just a state that you would
3220480	3226000	reach and then you have sort of reached the peak but more like what's a developmental trajectory
3226000	3232160	that would like be you you'd get the most out of each level of development maybe eventually
3232160	3236560	like understood most things that can be understood by human brain maybe at that point
3236560	3240640	you would want to upgrade it a little bit like go a lot some more neurons or whatever so you could
3240640	3246400	kind of explore the next level and but like what's the right pace of that like do you want to just
3246400	3251920	rush to the end and become like like a planetary sized superintelligence immediately or would you
3251920	3257120	like want to you know take the scenic routes and then maybe spend a few hundred years first
3257760	3263200	being a biological humans and doing whatever can be done as a human and then slowly increment
3264480	3272160	so these are some of the the questions that come up and so many more there's a lot of things to
3272160	3278960	think about hopefully we will actually yeah secure the future in us that well yeah again
3278960	3285120	incrementally I like Kevin Kelly's approach protopia not utopia or dystopia just one small make
3285120	3291920	life tiny bit better tomorrow than it is today don't aim for utopia just as a tool just make life
3291920	3300320	a little bit better don't worry about 500 years from now just tomorrow I think that allows one to
3300320	3311120	avoid a bunch of mischief that is is performed in the name of grand visions but I do think also
3311120	3316480	sometimes it's useful to lift your gaze up and and look at the horizon or like reflect on where
3316480	3322320	you're going like there's the next step on the next step but ultimately so we have like our human
3322320	3326880	civilization all this effort spent on science and technology and economic growth and everything but
3326880	3333920	very little effort spent on thinking what what where do we end up if this continues sure but
3333920	3339120	just let's talk about creativity for a moment your book I really enjoyed because it's completely
3339120	3344560	different than any most nonfiction science books that I read you know you have this kind of dialogue
3344560	3348640	this conversation you're in a classroom your lecturing you have handouts students are asking
3348640	3354240	questions that was pretty creative and new if you had asked chat gpt to write your next book I don't
3354240	3357920	think it would have come up with that you see where I'm going with this what about music what's the
3357920	3364720	origin of rock and roll well folk music and jazz all right so in a century from now what will be
3364720	3370000	the next big you know musical trend I don't know I don't think it's possible to know and I don't see
3370000	3376640	how an AI would anticipate the next creative movement not just in the arts and poetry or whatever
3376640	3382000	but in anything you know there's you know there's only so many combinations I guess maybe it could
3382000	3387040	grind through all the possible combinations for music that's going to be enjoyed by people that
3387040	3393600	seems to me though next to impossible to program now I might not be able to predict it
3395520	3399840	but there's a lot of things that you couldn't predict even with your super intelligence like
3399840	3404960	that like even just like the weather like a year into the future whether on a particular
3404960	3409440	minute it will be raining on this like chaotic systems right and with something like creativity
3409440	3416640	over the time scale of a century it the actual answer to that question will depend on what a
3416640	3421280	lot of smart people are doing in the course of that century maybe other AIs even smarter than
3421280	3426000	the one that you would think would be making the prediction and it itself will interact with so this
3426560	3431920	but even if it's not predictable what creative results will it you know precisely be attained
3432640	3438480	a century hence it might still be possible that the actual creative work is more efficiently
3438480	3445280	done by AIs as this century unfolds they might just be making the best paintings and writing
3445280	3450480	the most beautiful poems and writing the most compelling movies etc it's certainly not the
3450480	3455760	case right now I mean current large language models are have a sort of
3459520	3466000	cliche is maybe too strong a word but there is a sort of mid-brow quality to their output that
3466000	3472880	there's like the it's good but it's not great it's kind of the typical thing that some person would
3472880	3480320	say in a situation that they can produce more of that but great stuff comes from kind of not just
3480320	3490560	following along with the patterns that are already out there but sort of looking at reality afresh
3490560	3494880	with new eyes whether the reality is inside yourself or outside of yourself and really
3494880	3499760	letting it speak to you and then they sort of speak the words that come from your perception
3500720	3506160	of this piece of reality that is that you're focusing on and so it's like a different source
3506160	3513440	kind of of information but I have no doubt that that AIs will become increasingly creative I think
3513440	3518000	it's not a binary thing I think we already see little glimpses of lower level creativity and
3518000	3524800	I think the next generation will have more and then more and more beyond that for example a few
3524880	3533120	years ago DeepMind System Alpha go had this move what was it 32 or something 37 I forget but it was
3533120	3539760	like in the match that Alpha Fall was playing against least at all the human go champion and
3539760	3546480	there was a particular move that experts in go thought was immensely original and creative it
3546480	3551520	was something no human would ever have played that all the masters would advise like students that
3551600	3555200	that was an error but then it still turns out like if you think a little bit more you just
3555200	3561360	realize how right it was and it set everything up to win the game later so that's within a
3561360	3566080	sort of somewhat circumscribed domain but certainly like created within that domain and I think
3566080	3572800	the domains in which you will be able to have these like genuine deep creativity will be expanding
3572800	3579200	as the capabilities of the AIs increase yeah when I was a professor at Occidental College we had a
3579280	3584880	music professor there was also a gifted pianist and he would once a year hold these impromptu
3584880	3590240	concerts where he would in the auditorium that grand piano on stage he sits there and then people
3590240	3597440	would call out like requests like do Beethoven's X as if you know Elvis did it or you know in the
3597440	3602240	rock and roll and and people just come up with the craziest and he would do it and it's like god
3602240	3608400	damn that's great so maybe if you had an AI you could you could find all the different creative
3608400	3613120	permutations on all the different music that has been done and then test it in the marketplace
3613120	3620960	well what do people actually like yeah yeah yeah I do yeah so there's a quite right now the question
3620960	3625760	of quality like the actual output is not great now if we imagine the quality problem being fixed
3625760	3631680	then there is the question of whether people would still value it less because it was produced by AI
3632320	3638240	even if if you sort of listen to a blind test right A and B you're not told which one is even if
3638240	3644320	people prefer the AI output in that context if the quality became like as good or superior
3645600	3650720	if then we get to this like father question of value whether you still prefer it just because
3650720	3658400	you know that the human did it there's also the I mean there's like so many branches sticking out
3658400	3663040	from here but like one possible reason you might have for preferring the human output is if you think
3663040	3669120	the human but not the AI experienced various things when they wrote it they actually experienced
3669120	3678080	the joy or the sadness that you know the musical piece expressed but there again I think with
3678080	3684160	digital minds it might also be possible to create phenomenal experiences in digital substrate
3686080	3692720	and so AIs also might have had experiences that they could be expressing in their works
3693040	3699760	it's not clear exactly how where we are on that path towards AI sentence but I think certainly
3700400	3706960	in principle it is possible I'm a kind of computationalist about phenomenal content
3706960	3713360	yeah that subjective element of art where the fraudulent copy painting of a classic painting
3713360	3717600	plummets in value the moment people find out it's fake even if you can't tell the difference with
3717600	3726160	your own eye yeah yeah yeah so um so if that's the model then you know that might be still demand
3726160	3731840	for human painters to yeah paint there now sort of relatively back to the economics I mentioned
3731840	3736880	you know pulling up everybody from the bottom up to some level but you know economists tell us
3736880	3741680	there's this thing called the hedonic treadmill but there is no there's no bottom level people
3741680	3747440	always want more and that that's just going to never end you know the McMansions houses are like
3747440	3751680	two to three times the size they were in the 1950s even for the average worker
3752240	3757520	and you know that that there's no upper ceiling on how much more stuff people are going to want
3757520	3766240	how do you think about that yeah I think there are parts of our preference functions that are
3766640	3775760	um non-satiable collectively because like yeah we have these desires for positional goods
3775760	3780880	to have more than another like you want your yacht to be the biggest in the world
3781680	3787680	so you build a 200 you know foot yacht and then some other billionaire bastard builds one that's
3787680	3795280	like 205 and then so it's impossible for both of these people to have their preferences satisfied
3795280	3802160	to own themselves exclusively the biggest yacht in the world so that's one example for how
3802880	3807840	collectively there could be preferences that the humans have that you can't all be satisfied
3807840	3812320	and there are many other examples where two people want the same piece of land or the same
3812960	3822160	be the exclusive love interest of the same person or etc etc so now it doesn't enable sort of
3822160	3827200	unlimited economic growth if you define growth ultimately in preference satisfaction terms
3827200	3834880	and so like because one person's gain is another's loss in this scenario and it also doesn't necessarily
3834880	3844160	create an unending reason for human economic labor if there is no way to make more money
3845360	3849840	than no matter how each of these billionaires wish they could make their yacht a bit bigger than the
3849840	3856160	others like if they can't actually make more money by working or if the extra money they
3856160	3860080	could make by working is kind of trivial to the amount of money they are already getting from
3860080	3866640	their capital gains and that would be no incentive for them to put out effort for that reason
3868720	3872480	and that's like already true for many billionaires like there's like yeah they could
3872480	3877760	take a job and make an extra 100,000 a year maybe but if they're already sitting on 20 billion it's
3877760	3882640	like it's not really making a meaningful difference to their purchasing power yeah but you have people
3882640	3887120	like Elon Musk and Jeff Bezos you know they didn't they're not just sitting on the beach
3887120	3895760	you know they're exactly and they can I mean still add a lot of even just economic value
3895760	3902720	through their work like like obviously Tesla would be worse a lot less if Elon I called it quits yeah
3902880	3910480	and so even just from a purely economic point of view they still have the ability to contribute
3912240	3918400	amounts of economic value that are significant even relative to their net worth and
3919920	3923680	because they have like like Elon has unique skills also I think there are opportunities
3923680	3928240	sometimes for very wealthy people to sort of combine their human capital with their financial
3928240	3933680	capital to do things that are hard to do by taking one person with capital and one person
3934720	3938720	with brains because they're like trust problems and communication problems sometimes they need
3938720	3944000	to be combined in one person to certain opportunities are more easily realizable
3944880	3950080	but for many others it's not the case and they're already like in this situation where
3950080	3955040	it makes no sense to work for money yeah but I guess my point is you know there's stories about
3955040	3959600	Elon Musk sleeping on the floor in his factories he doesn't have to do any of that but he does it
3959600	3964400	because that's what gives him value and I think more people would want that than would just want
3964400	3969920	to sit on the beach yeah I think he's doing it because he wants to achieve various things that
3969920	3976400	can't be achieved without him doing it yeah now if he could create like I don't know like some sort
3976400	3982080	of android replica of himself that would do the same thing and achieve the same results for for
3982080	3987520	Tesla and SpaceX etc and and he could be on the beach I have no idea maybe he would prefer that he
3987520	3994160	has said that his life is pretty painful often and that um so it might be that he does it because
3994160	4000000	there are various outcomes he wants as opposed to valuing the activity itself not running around
4000000	4004160	maybe of course we don't know what's in his head but uh you know I think in general people like
4004160	4008880	challenges because that's what makes life meaningful and it's essentially an infinite
4008880	4013280	number of challenges we could always have but I could be wrong okay on the economic model
4013280	4018160	so people are living longer let's not get crazy let's just say people live 200 years or 300 years
4018160	4022800	rather than 100 yeah but it's 300 years of research into extending life don't you think
4023760	4030240	I don't know you know Kurzweil thinks it's coming by 24 what's the the takeoff point in 2045 I think
4030240	4036960	he said where maybe it's even sooner than that where the amount of extra life you get exceeds
4037600	4042000	every year of your life and then you have the what does he call it the take take takeoff point
4042000	4048000	something like that longevity escape velocity yeah I don't you escape velocity that's it yeah I
4048000	4053040	you know when I hear these things I think back to religion it's like I feel like I'm you know we're
4053040	4057920	the chosen generation we're the ones that get to live forever I've heard this before when I was
4057920	4064080	religious right you know maybe you know but I think the problems are much harder than most
4064160	4070080	longevity researchers think but you know it's possible but okay let me let me
4070080	4073920	carry out the thought experiment all right so we have eight billion people now it's probably gonna
4073920	4080240	top off around 2050 and start to decline by 2100 or so and as you know Elon's worried about a
4080240	4085920	birth birth the richer and more economically stable and more educated people are the fewer
4085920	4093040	babies they have so how do you square that with people living longer and the population increasing
4093120	4096160	how do you think about that yeah I think there are various
4097280	4104160	long-term trends that I think would deserve attention if it were for the fact that I also
4104160	4110800	believe that we are probably relatively close to this transformative technological overhaul over
4110800	4117040	the current of the current human condition so that I think sort of the game board will be overturned
4117040	4124400	for better or worse but within you know likely some years or a few decades and that these like
4124400	4131600	longer demographic trends won't really have time to play out would be my guess there might be other
4131600	4137680	demographic trends that then kick in if you do invent this a new world with AIs and digital minds
4137680	4142880	that can obviously copy themselves instantaneously if you're like software you could make a million
4142880	4146320	copies of yourself in an afternoon right if you have available hardware so you could have
4147200	4151280	like different population dynamics that could become problematic but but that
4151280	4154640	that wouldn't sort of just be an extrapolation of what we're currently
4155280	4164080	seeing with the human situation also like some I mean I see the the projections and how like
4164080	4170800	birth rates are going down and if that continues like like but some skepticism about our the
4170800	4175280	reliability of these long-range forecasts like I mean when I grew up the the big worry about
4175280	4180320	was about overpopulation and there were these like public intellectuals through the club of
4180320	4184400	Rome and everything and that was like and they had little mathematical models that show this
4185520	4189360	now it's going the other way and I mean who knows in 30 years from now if there's no
4190000	4196000	transition maybe it just turns out that something has changed and it's overpopulation again like or
4196000	4202400	some other so it's like yeah our ability to make these very long-run range forecasts are
4202640	4209600	are open to question I think yeah you know Stein's law things that can't go on forever won't
4211360	4214560	and there's some corollary to it but but they can go on a lot longer than you think
4215280	4220880	yeah yeah yeah some things have gone on for longer than yeah one would have guess well I guess in the
4220880	4225360	next you know you want to look at the far horizon do we need to leave the planet become a multi-planetary
4225360	4231360	species because of either overpopulation or we're going to run out of raw supplies and and uh and
4231360	4239600	resources and population can ultimately outrun any space settlement program because ultimately
4240400	4248160	even with mature technology we are limited by the speed of light and so if you imagine a sphere
4248160	4252880	growing at the speed of light even in all directions right the volume of that grows
4252880	4260080	polynomially with time and so the resources that we could potentially use for civilization or for
4260080	4266400	like that cannot most grow uh polynomially whereas population can grow exponentially it can in theory
4266400	4272000	like you know double every generation or 10 percent and so eventually the exponential will
4272000	4279600	overtake the polynomial if you have unrestricted population growth and if like we end up in in
4279600	4284320	a situation where sort of people have more than the replacement rate of children that that would
4284320	4291040	eventually just overtake so so that the space settlement would at most kind of delay bumping
4291040	4297200	up against resource limits and ultimately you would have to just figure out some way to maybe
4302480	4309120	coordinate to to bring only the number of people new people into existence that that could be
4309120	4313680	supported at a high level of living which might still be a lot of new people into existence but
4314320	4320320	if you overshoot that then eventually average income would have to drop right now we have more
4320320	4328400	like kind of increasing returns to population because more people means more ideas and division
4328400	4334320	of labor which makes so so right now probably per capita income goes up if population increases
4335040	4342000	but i think at some point that will no longer be the case and the limiting factor of the economy at
4342000	4348000	technological maturity will eventually become land as it's referred to basically resources
4348000	4355120	that you can't make more of as opposed to labor or uh technological advances that that will already
4355120	4363200	have sort of been maximized and so then land only grows polynomially in the limit and that that would
4363200	4369040	be the sort of maximum rate at which the population could grow in the limit as well and what's your
4369040	4375200	time horizon there you know thousands of years or tens of thousands of years uh well i think um
4376880	4384240	there are really sort of two key variables there's a question of how far from now until we get
4384240	4391680	superintelligence um and then from there on i it might not take that long because once you have
4391680	4396240	superintelligence that makes super duper intelligence and then like some kind of
4397200	4404880	substrate optimized for a cognitive performance that can i would imagine relatively quickly
4404880	4410400	develop all kinds of technological solutions that start to approximate the physical limits
4411120	4416480	i don't know whether that would take like months or decades but well so Kurzweil is talking about
4416480	4421760	monetary super brains brains kind of working for like creating a space rocket for like yeah
4421760	4427680	it occurs i was talking about 2045 in his next his next book the singularity is nearer coming out
4427680	4433680	in june uh 24 but so about after that anything's game i mean we just probably unpredictable uh what
4433680	4439280	the time horizon could be yeah i mean i don't even think we can rule out very short timescales like
4439280	4444560	we don't know dpt 5 or dpt 6 won't be right there i mean we don't know that i will but
4445280	4449680	here we really have to think probabilistically right and have like a smeared out probability
4449680	4454640	distribution i think over different or okay you're one of my favorite big minds so let's
4454640	4458800	keep going on the the long horizon all right let's apply the Copernican principle to our
4458800	4462560	species we're not special the chances are you know we're in the middle of the bell curve of
4462560	4468720	civilizations that would have done everything you just described uh surely there's extraterrestrial
4468720	4474560	intelligences out there that have already done all this uh and built self replicating von Neumann
4474560	4481920	machines and Dyson spheres and so on so answer me uh Fermi's paradox where is everybody
4483200	4493200	um most likely just like very far away like outside our light cone um which would uh yeah
4493200	4496800	explain why we haven't seen them of course if the universe is infinite as it seems to be with
4496800	4500320	infinitely many planets and stuff then there would be infinitely many of them out there
4501120	4508160	but the density might be quite low um we don't really have it seems to me and a particular
4508160	4515680	reason to think that um it would be easy for a like an earth like planets to produce life let
4515680	4520480	alone intelligent life i mean there might just have been some ridiculously improbable steps
4520480	4526080	somewhere like to get the first impulse replicators going or maybe to go from prokaryot to eukaryot
4526080	4532560	or something maybe that just happens in like one planet out of 10 to the power of 40 planets or
4532560	4537760	something um now then you might think wow wasn't it then like what's the miracle that it happened
4537760	4541920	here on earth well if there are infinitely many planets out there then even if the chance for
4541920	4547040	any one of them is ridiculously small it would still happen right with certainty infinitely many
4547040	4553440	times and then an observation selection effect would explain why we find ourselves on a planet
4553520	4557360	where this improbable thing did happen like only those planets are observed by people
4558240	4564560	evolving on them of course the others there is nobody there too so that that seems like pretty
4564560	4576480	likely um if one wants to think that life is more common then one has to i think um either
4576480	4582800	postulate some kind of zoo hypothesis like where they are deliberately uh hiding themselves or
4583920	4591680	uh like my colleague robin hands on has some scenario in which uh uh there might be others and
4591680	4598000	it is like too long to explain but yeah there's a there are like it's i also think like i don't
4598000	4602400	know i mean it probably takes us like too far afield from our current conversation but this
4602400	4609200	whole simulation yeah argument stuff um which which kind of adds another dimension to the whole
4609200	4614880	where and where do you stand on the simulation hypothesis well i mean i i mean i i believe in
4614880	4620880	the simulation argument having uh more than 50 percent united that yeah so i i think that sound
4620880	4626640	and uh now that only shows one of three possibilities is correct one of which is the simulation
4626640	4631920	hypothesis and so you would then need some additional information or consideration if you
4631920	4636800	wanted to sort of pick between these three alternatives that the simulation argument
4636800	4644320	establishes um i would say i mean and i think as to to me when i wrote this paper back in in
4644320	4650560	like the early 2000s it back i was pretty clear that we were sort of on a path to develop
4650560	4656000	increase the the technologies that you would need to create these ancestor simulations or
4656000	4661520	detailed simulations with conscious um like like super advanced virtual reality and digital brains
4661520	4667520	like now i think it's maybe easier for people to imagine that because just we've seen kind of 20
4667520	4672960	years 24 years of technological advancement like virtual reality is a lot better now than it was in
4672960	4678720	2001 and obviously ai is moving ahead so it's like a smaller imaginative leap to think that
4679280	4683600	at some point in the future some technologically mature civil like really advanced civilization
4683600	4689280	might have the ability to create simulations that are like perfectly realistic to the people inside
4689360	4697440	them um and uh yeah i like so so in in some sense the opportunity is to
4697440	4702000	pop off the train before you reach the conclusion or like diminishing as we sort of pass by the
4702000	4706880	relations you know i had david chalmers on the on the show he has a you know an entire book on
4706880	4712960	on the simulation morals in a simulation and ethics and so on very interesting but ultimately he
4712960	4717760	says right in there this is not a testable hypothesis we there's no way to know if we're
4717760	4723280	in a simulation or not so then what are we talking about here this is just science fiction or metaphysics
4723280	4733280	or or what no i mean i think it um there are certainly possible observations um uh that
4734800	4741440	are such that if we made them they would give us strong evidence either for or against
4741440	4745680	the simulation hypothesis it's like to take the most obvious example like if a window
4745680	4749200	suddenly popped up in your visual field saying you're in the simulation click here for more
4749200	4754000	information and a little buffering and find the terms and services like that would pretty
4754000	4760240	like prove it to you right like if that yeah um conversely the absence of such a window popping
4760240	4766880	up is by the principle of conservation of evidence must be some really evidence against the simulation
4766880	4776480	hypothesis like weak evidence because but but still some evidence and but more um i guess um
4776480	4781760	relevantly i think if you consider the simulation argument which has the structure that at least
4781760	4788160	one of three propositions has to be true one of which is the simulation hypothesis anything that
4788160	4793760	gives you evidence for or against the other two indirectly then uh affects the probability you
4793760	4799920	should assign to the simulation hypothesis so um for example one of the alternatives is that
4799920	4804960	almost all civilizations at our current stage of technological development go extinct before they
4804960	4810000	reach technological maturity so that's something you could believe instead of the simulation
4810000	4813680	hypothesis but there has to be a very strong convergence it can't just be like 80 percent
4813680	4820240	of them it has to be like basically all um and of course if we make it through to
4820240	4824640	technological maturity that would be very strong evidence against this idea that basically all of
4824640	4831120	them fail to get to the technological maturity so therefore anything any evidence we get for
4831120	4835840	against the idea that we will reach technological maturity would bear indirectly through the
4835840	4840800	simulation argument on the probability of the simulation hypothesis so the closer we get to
4840800	4845920	technological maturity ourselves the less likely that alternative is and hence the more likely
4845920	4852800	the simulation hypothesis is and you could imagine um the extreme version of this which
4852800	4858960	is if we ourselves develop all the technologies needed to create ancestor simulations and we are
4858960	4864320	just about to switch them on and we want to switch them on and we're sort of about to reach to press
4864320	4869280	the button that would pretty much conclusively rule out the two alternatives to the simulation
4869280	4875920	hypothesis it would show that like it's not the case that nobody reaches this level of
4875920	4879840	technological maturity it's not the case that almost nobody of those who do reach that
4881600	4887120	remain interested in creating ancestor simulations and so in that situation where we turn on our own
4887120	4892400	simulations we would have to conclude that we are almost certainly ourselves in one
4894160	4898720	so those would be some ways of getting very strong evidence and then but I think anything
4898720	4903840	that then indirectly has some probabilistic bearing on these alternatives also sort of indirectly
4903840	4909920	has some evidential connection to the simulation hypothesis so I think there's a lot of ways to
4909920	4916960	test it but these tests are probabilistically yeah did you mention what if we're first somebody
4916960	4923680	has to be first yeah well that that's uh what what it's for did you say are or if it's first no
4923680	4928160	we're first yeah we're there some civilization has to be first maybe it's us that must be
4929840	4939360	very unlikely if that were to ultimately be say a million simulations of experiences just
4939360	4945440	like the ones you're having and you can't from the inside tell the difference whether you're like
4945520	4954880	number 537,648 or whether you're like number one but in that condition where you have some evidence
4954880	4961520	and you could either be one of the vast majority of people with your experiences that are simulated
4961520	4966800	or like this very exceptional one that's not simulated and there's doesn't feel any difference
4966800	4971440	from the inside I think they're a kind of principle of indifference should make you
4971440	4976640	assign a proportionately low credence to you being the first one like the exceptional one
4976640	4981840	yeah but again somebody has to be first because you're saying in the age of the universe if most
4981840	4987200	people think they are then almost all of them will be wrong yes it looks like a rational betting
4987200	4993600	odds in that scenario like we would be to assign a very small probability to that and there's more
4993600	4999200	arguments I think supporting that it like in fact my my doctoral dissertation was like developing a
4999200	5004640	theory of observation selection effects and I think there are various areas in in physics and
5004640	5012160	cosmology and to some extent in evolutionary biology where you have to reason along roughly
5012160	5018480	those lines to be able to get sensible results when you try to connect current cosmological models
5018480	5024320	with the predictions that intuitively confirm or disconfirm them some sort of roughly speaking
5024320	5030160	assumption that you're like and you should think of yourself as you were a randomly selected observer
5030160	5034240	well as there's a lot of complications around that but that's like some something in that
5034240	5040160	general direction seems to the let me ask you a technical question here on a simulation like in
5040160	5045840	Star Trek's holodeck you know Worf goes in there and he has a fight with some other Klingon and he
5045840	5052480	gets knocked down how does a virtual reality interact with a physical body to say maybe you
5052480	5057520	want to have a boxing match with Muhammad Ali in this virtual reality how does he how does the
5057520	5063920	virtual reality actually knock me down well I mean so in the simulation argument I think
5064640	5071840	the the most well so there your mind is itself implemented digitally like so there is no
5071840	5077440	me to knock down it's it's all digital yeah there is a you but the you is like it digitally
5077440	5083600	instantiated and you have like an avatar yeah that your digital mind is connected to like a digital
5083600	5093600	avatar and the same sensory afference that you like currently are going from your sensory nerves
5093600	5099040	yeah okay yeah like like that it's like equivalent nerves are going into this digital brain with
5099040	5104160	the same information yeah using the same subject now the simulation would at some point have to
5104160	5111280	run on actual hardware right right yeah how can you possibly have enough computing power
5111280	5120080	to replicate everyone who ever lived and not just their physical bodies and or I guess their minds
5120800	5125440	I mean there's this mind uploading business about copying the connectome that's not enough
5125440	5130800	it's not enough just to have the synapses copied you'd have to copy every single molecule
5130800	5138400	and every one of the synapses in the gas yeah I don't think so I think that a sufficient level
5140240	5147840	of granularity of a simulation to produce conscious experience would be like the synaptic level
5149360	5154640	you know possibly you could simplify it even more you mean to get a general memory because
5154640	5159680	memories are stored I mean I'm told by memory neuroscientists that you need it's not just the
5159680	5165600	connectome it's not just those synaptic you know sort of wired in things you need all the molecules
5165600	5171520	and all that stuff is part of memory yeah well some aspects of that I mean and clearly there are like
5172880	5178720	neurotransmitters that are like swimming around in big pools but I think the bulk of the information
5178720	5187440	processing is probably done at the level of action potentials and synaptic connections I mean that's
5187440	5194400	what we see with current AI systems large language models are these neural networks are artificial
5194400	5201840	neural networks are essentially simplified neurons with simplified synapses and they do seem to
5201840	5208240	perform I mean insofar as AI has advanced like the same kinds of tasks that the human brain like in
5208240	5215120	terms of say visual perception which is like a relatively well developed area of AI like with
5215120	5221120	a comparable number of neurons that visual cortex as you can perform comparable level of
5221120	5230560	discrimination and object recognition etc I think and indeed current AI started to have fewer
5230560	5235600	parameters than the human brain has and but they are also like a little bit less smart than the
5235600	5240960	human brain but it roughly kind of strikingly seems to match the performance that you would expect
5240960	5245520	by matching it to biology it might be that the biological neuron certainly is more complex than
5245520	5251120	one of these artificial neurons so maybe you get 10 times more performance per biological neuron than
5251120	5258000	you get from one of these simplified representations but I think that would be my guess now you could
5258000	5262320	have enough computing power to go down a little bit below the level of synapse if somehow you needed
5262320	5268240	it not not all the way down to elementary particles then yes it would become computationally
5268320	5273520	intractable if you had to simulate the whole Schrodinger equation of a human brain in order
5273520	5278080	okay I'll grant you that because human memory is not all that granular anyway it's pretty fuzzy
5279200	5284880	but is that you okay so here's my thought experiment we slide you Nick Bostrom into a
5284880	5290960	functional MRI we scan your connectome we upload the digital file into the cloud and I have it here
5290960	5297440	on my phone and I go Nick you're up here now and you're sitting there going no I'm not I'm right here
5298160	5302720	and so are you saying that there's we have to redefine the self there's just multiple
5302720	5310320	Nick Bostroms and each of them thinks that they're the real one well I think there are that like
5312000	5318640	two notions of the concept like two notions of self that that can come up that that kind
5318640	5325600	of always coincide in our normal human experience but that would come apart in some of these
5325600	5331760	technological scenarios and philosophers have realized this like Derek Parfit who was my colleague
5331760	5339200	at Oxford was famous for exploring the difference between preservation of personal identity and
5339200	5345440	survival in these thought experiments where you have like duplication we imagine a person being
5345440	5351680	duplicated or teleported and the original survives that Parfit argued that in those cases
5351680	5363040	the original person has survived but that personal identity is separate from this because
5364320	5370480	the original person can't be identical to any one of them because they are not identical to each
5370480	5376480	other and the claim would be equal and so so you would have like even if the personal identity
5376480	5380560	is not preserved you could have survival and anyway that that gets into these kind of philosophical
5381120	5387520	issues but I think certainly in some scenarios I think your personal identity would be preserved
5387520	5395040	in an uploading scenario in like if that was only one successor it would be you I think if there were
5395040	5401600	multiple copies made simultaneously like equally branching out from the root node I think it would
5401600	5406800	be natural to say that you survived but I'm not sure what to say about your personal identity in
5406800	5412560	that case I think just our concept of identity was like not really developed to deal with these
5412560	5421440	cases so it's a little bit sort of inconsistent when applied in these extreme or like exotic situations
5421440	5428000	yeah yeah I wrote about this in heavens on earth I was looking at both scientific and religious
5428000	5433600	versions of the afterlife and there's this idea that there's the mem self and the point of view
5433600	5438480	self POV self so the memory self this is the connect them just copy your your your memory
5439200	5444160	all your memories and let's say we can do it but that's just a snapshot I mean if you did it when
5444160	5450080	I was 30 and now I'm about to turn 70 this year well if I if you did it when I was 30 where is
5450080	5456480	all the 40 years plus memories that they're not part of that self that's that's somebody else
5456480	5462000	that's not me me is my point of view looking through my eyes from moment to moment to moment
5462000	5468160	for all 69 and a half years that I am now that's me and there is no fixed there's no fixed point
5468160	5474160	where you go that's you right there at 40 or 50 or whatever I don't see how that could ever be
5474160	5481040	replicated because there's no snapshot there's no thing called the self in a fixed sense yeah I think
5481040	5485120	there are several different notions and like you could ask somebody like are you the same person now
5485120	5490800	as you were when you were five of course not yeah and people are confused because like in in one sense
5490800	5497360	clearly not in another sense clearly yes I'm still in classroom and like there's a continuous path
5497360	5503600	but I think it's just that instead of having one concept of self we have several different ones
5503600	5509920	that normally in everyday use kind of coincide and tracks the same thing but in these scenarios
5510560	5516640	they come apart and so we need to sort of start to differentiate different yes you would be the
5516640	5521840	same person in sense one of being the same person but like a different person in the sense two of
5521840	5528000	being and and I think more generally in this kind of world where we have like digital minds I think
5528000	5533200	there are a lot of new concepts that we would need to develop to sort of describe the ways that
5533200	5539120	different minds could be related like so humans are kind of discrete things like here's one human
5539120	5545520	here's another human like with digital minds you could imagine them being kind of partially overlapping
5546320	5553680	or like briefly diverging and then converging you could imagine all kinds of ways in which
5553680	5559440	digital minds could vary that they're not possible for human minds to marry you can pause them speed
5559440	5567760	them up slow them down that that might be like a bunch of different slightly separate minds that
5567760	5572960	sort of have some shared convergence point where they send information and it's not clear whether
5573040	5578960	they are all one mind or several minds or like so there's like I think we will we don't yet have
5578960	5584000	all the relevant concept for making sense of that kind of post-human reality but you know
5584720	5589520	hopefully we'll have an opportunity to develop some of those as we are are you familiar with
5590960	5594240	are you familiar with Frank Tipler's book the physics of immortality
5595040	5604000	yeah but it was a very long time since I yeah that was 96 so here's his calculation that so he's
5604000	5610400	projecting an omega point computer in the far future that he calls god essentially that will
5610400	5615360	contain 10 to the power of 10 to the power of 123 bits that's a one followed by 10 to the
5615360	5622480	123 zero powerful enough he says to resurrect everyone who ever lived that may be but it's a
5622480	5628320	staggeringly large number but is even an omega point computer powerful enough to reconstruct
5628320	5634320	all of the historical contingencies all parts of life you know every interaction I ever had
5634320	5639280	with everybody else including like you right now this particular moment instead of yesterday or
5639280	5645120	whatever I mean how would how would any civilization create a computer powerful enough to do that and
5645120	5649280	Frank says not only that you'd have to you'd have to resurrect everyone who ever could have lived
5650000	5655520	because you you don't know who you just it's just your point of view so there's a lot of
5655520	5662640	there's a lot of those people that's a that's a big cohort yeah yeah and if there was a computer
5662640	5667280	powerful enough to do that wouldn't it have to consume so much energy we could detect like a
5667280	5672560	techno signature detects something like a dys sphere that has to capture all the energy of a
5672880	5680880	sensor runs at the computer yeah well um so with tipper like so one problem with his theories I
5680880	5686640	think at the time he thought it and it was like an open possibility and cosmology that we would
5686640	5692560	have like a big crunch that the universe would collapse back onto itself into a singularity
5692560	5698000	that's uh that's how his speculation was that in the final moments of that collapse you could get
5698000	5704480	this like super amount of computation done somehow now it looks instead like we are sort of
5704480	5711120	gliding apart with a positive cosmological constant and it like it looks like it's sort of a big
5711120	5718160	whimper rather than a big yeah reverse big bang right so so that that's one like now I mean in
5718160	5728240	in terms of reconstructing people later in simulation if you haven't say chronically preserved
5728240	5736880	their brains or something I think I mean certainly like creating recreating everybody who could have
5736880	5743600	lived like that that's like a kind of a super astronomical number it depends a lot on how
5743600	5748320	finally you individuate a person from like another very similar person like at what point
5750400	5756320	are you close enough to basically say yeah that's kind of you know that's Michael Schirmer
5757440	5766000	even if like your your replication like I like it got a few memories slightly rough and like
5766800	5771040	you know there's like a few details that but it's like captures the essence of him close enough
5771040	5775680	that we can say that it so that's like more like a philosophical parameter you would have to put in
5776720	5782080	it's like an open question to what extent if you might have sort of arbitrarily powerful super
5782080	5791680	intelligence how close could it get to recreating a human mind assuming they are like dead and
5791680	5799840	decayed by the time just from behavioral traces like like their writings or photos that their
5799840	5806960	friends took you know on the holiday or like whatever other like information traces like
5806960	5811760	if you if you were like a super intelligence and you started all of this material and compared it
5811760	5817600	with other humans information traces and maybe some brains that you have access to and you sort of
5817600	5822640	made the best possible inference taking all of this information into account and you try to create
5823360	5828640	like something that was as good an approximation as you thought like how close would that
5828640	5833760	approximation be to the real Michael Sherman right it's an open question which is quite
5833760	5838800	hard to really get a good grip on and certainly I think it would have to be close enough that
5839600	5843920	like your friends couldn't tell a difference like if your friends still survive at this point for
5843920	5851360	example or your your kids or whatever like if they are my wife doesn't know sorry if my wife doesn't
5851360	5859440	know she can't tell yeah it's not like if some replica was created that was like close enough
5859440	5865040	to you in its qualitative behavior to sort of trick everybody you know including like your
5865040	5871440	spouse and kids and parents like would that be close enough I mean I mean at some point it
5871440	5876480	just depends on your value system like how close does it have to be for you to hear enough about
5876560	5884240	it as if it were a perfect replica it might not be a factual question so much as a value question
5884240	5890880	like how similar does an entity existing tomorrow have to be to me now for me to care about it
5891520	5897440	in in this kind of self-interested prudential way that I normally care about the person waking up
5897440	5904080	in my bed tomorrow that's me like like if I if I if I knew that I was just going to be transformed
5904080	5909280	overnight into a dragon that remembered nothing of my past life and shared none of my current interest
5909280	5916080	and but made out of the same atoms like I probably wouldn't really care that much about that dragon
5916080	5920800	or at least not more than I care about any other living sentient being out there because it would
5920800	5926880	be not in any meaningful sense me even if it like consisted of my atoms maybe because it ate me during
5926880	5934080	the night or something like that but oh like Kafka's Metamorphosis where you you know wake up
5934080	5940640	as a cockroach or whatever but the problem with that is the the human mind is still in the other
5940640	5945840	being that wouldn't happen right if there was a true transformation so let me just hit a couple
5945840	5950720	more big topics here before I let you go so the other the other mind's problem the hard problem
5950720	5955760	of consciousness how do you how would we ever know if an AI was sentient and conscious if we
5955760	5962960	don't even know that you and I are yeah well yeah and maybe this might have to be our last
5964160	5969920	topic also that that's a big and practically relevant question now I think we are starting
5969920	5975360	to have AI systems that are not where it's no longer ridiculous to imagine that there could be some
5975360	5982480	form of conscious experience happening inside them if we look at the number of parameters
5982480	5988560	and their behavior it certainly seems to match some non-human animals like in that we think are
5988560	5995440	probably conscious and so yeah this is a very difficult problem I think there are different
5995440	6004240	approaches you could take you could try to take some current theories of consciousness of the
6004240	6011760	shelf and try to just apply them to the case of ai's so we have for example a global workspace
6011760	6018000	theory is one theory of what creates conscious experience like that have been proposed and
6018000	6022080	it's the idea that the things we are conscious of are ones that are sort of entered into a
6022080	6027360	cognitive workspace from which different other more specialized cognitive models can sort of
6027360	6031200	read and write but the thing in the shared workspace is kind of accessible to all the
6031200	6037280	different parts of our brain there's like another theory attention schema theory which says conscious
6037280	6045680	experiences are sort of a rise in our in the modeling of our own attention mechanism so we
6045680	6050240	have like a little part of our brain that keeps track of what we are likely to be paying attention
6050240	6054640	to and that and there are like a few of those you could like apply those to to ai's and
6057200	6062800	if you do that it basically looks like either some ai's are conscious or that it would be
6062800	6068640	relatively easy to build an ai that is conscious using current technology like if you just put
6068640	6076240	together all the pieces into one system that just checked all these boxes you could
6080880	6087200	you could try to I mean you could ask you could ask an ai you could like that's how you would
6088160	6091840	like with humans like if I want to know what you're feeling or thinking like I
6092560	6097440	wouldn't I could try to put you in a big fmri scanner or something like that but realistically
6097440	6102000	I'm much better off just asking you right and you could tell me and so now we have ai's that
6102000	6109040	can speak it's a very natural thought to say well let's ask them and I think that might become a
6109040	6118240	useful technique but with some important provisors which is that it wouldn't give us any information
6118240	6126640	if you deliberately trained the ai to give you a predefined answer so it's very easy right now
6126640	6131680	when you find you in an ai to sort of train it to like when asked if you're conscious deny it
6132240	6137760	or or conversely you could train an ai to like affirm it so so if you want to use this to get
6137760	6142080	any possibly relevant information about the question you would first of all have to refrain
6142080	6148720	from deliberately biasing the ai during training then there are other things you could do you could
6148720	6156640	try to detect if there are multiple modes of cognitive operation in an ai system like basically
6156640	6162720	you could try to find interpretability methods that allow you to differentiate when it is trying
6162720	6171120	to say true things versus when it is just kind of rehashing things that it remembers or trying to
6171200	6176480	be entertaining or stuff like that like this goes back to the problem of hallucination that
6176480	6181760	that you brought up earlier with some current ai systems so there's like preliminary research that
6181760	6186560	suggests that you could like sometimes track when the ai is lying versus when it is trying to tell
6186560	6191760	the truth based on different neural activation patterns so you could then see if you combine
6191760	6195920	that with a self report you could see that when when it says I'm conscious or when it denies it
6195920	6202720	does that statement occurs in a mode of operation where it looks like it is trying to say true
6202720	6208640	things is it like the same kind of thinking that it uses to try to factually answer other questions
6208640	6214320	you could see if it is able to say a lot of other things about its internal states that they're not
6214320	6220960	really about consciousness but like whether are you currently aware like are you currently paying
6220960	6226240	attention to x y or z what are your like ask it other things about its capabilities to check
6226240	6230720	whether it has like the the ability to introspect that that could give you a little hint maybe
6232080	6235840	and so there are some other ideas like that that they're still very kind of premature but
6237520	6242160	there's an interest now amongst some of these people and including to some extent
6243520	6247760	some of the people working in frontier labs to try to figure out because you know at some point
6247840	6251360	we need to figure this out from a moral point of view like if we are building sentient creatures
6251360	6255440	like at some point it becomes really important that we treat them right and stuff is the next
6255440	6262560	rights revolution for AI yeah yeah and I'm getting that like getting a good sort of
6262560	6268800	happy cooperative relationship going is really important I think because it might well be that
6268800	6273440	in the future most minds will be digital and so like making sure the future goes well not just
6273440	6279600	for us but for them too I think is a key design criteria enough anything that would be able to
6279600	6286560	qualify for the name of deep utopia all right deep utopia there it is get the book read it it's
6286560	6292080	filled with pretty much every single biggest question you could ask about humanity for our
6292080	6298320	futures right there thanks nick we'll have you on back after the singularity happens that'll be
6298320	6301600	your next book yeah it's not something
