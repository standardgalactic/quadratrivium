{"text": " Thanks for watching and let me show you the math behind Google, at least the way it used to work back in the 90s. Because when I was younger and we had other search websites like Yahoo or Excite, it was quite a nightmare. Because if you were trying to look for a website about apples, the one that would get the most hits was one where it just said apples, apples, apples, apples, apples, apples, which was not very useful. But then came Google and life was much easier because they use what's called the page rank model, which looks as follows. Because suppose you have four websites which are connected as follows. So from three, you can go to one, from one, you can go to two, three, and four. Then the way Google works is that it assigns what's called an importance vector. Let's call it V. And suppose V is, for instance, 0.2, 0.1, 0.4, and 0.3. Then what that means is that the highest number is better. So website three is more important than website four, which is more important than website one, which is more important than website two. So three would get suggested first and then four and then one and two. Now, how do we determine that importance vector? Well, with just a little bit of linear algebra and probability. Because what we need is some initial vector. So assume all four websites are treated equally. So assume our initial vector is just 0.25, 0.25, 0.25, and 0.25. And some way of transitioning from the initial vector to the next step. And for this, we need something called a transition matrix. And in order to find a transition matrix, we need some probabilities. And you'll see why. In particular, think of it as follows. Suppose you start with website one. What are the chances of going to website two? Well, there's three possibilities, two, four, or three. And so in particular, from one to two, there's a third chance of landing to website two. So one third, one third, one third. And well, then we can just complete the other arrows as follows. So it's a nice exercise to show that the other probabilities are as follows. And this allows us to calculate our transition matrix. And the way you read the transition matrix is from top to left. And so this is website one, two, three, four, one, two, three, four. And you have to ask yourselves, well, to go from website one back to website one, where there's no self-linkage here. So the probability is zero. To go from two to one, again, two does not connect to one. The probability is zero. To go from three to one, well, there's just one connection here. So the probability is one. And let's say to go from four to one, the probability is one-half. And well, then you can do the same spiel with websites two, three, and four. And you end up getting the following matrix. Ta-da! Magic, isn't it? Now, what are we trying to answer, by the way? We want to figure out in the end, which website is the most important one. And in order to simplify, we just want to ask ourselves, what happens if we let this run infinitely many times? So what happens after infinitely many clicks? And well, in order to do this, we first need to figure out what happens after one click. Well, if you think about this, in order to go from the initial vector to what happens next, you just have to apply a once. So v1 is av0. And well, to figure out what happens after two clicks, so to figure out v2, that's just what happens when you apply a to the first step. So av1. But remember, v1 itself is transitioned, so it's just av0. And that becomes a squared v0. How cool is that? Using a squared, you can directly go from nothing to the second step. It's like a teleportation. And in particular, to go directly from the initial step to the end step, you just have to calculate a to the n. So a to the n v0. And of course, nothing prevents us from plugging in infinity here. So in particular, the answer to our question, v infinity, it's a infinity times v0. And so really, all we just need to answer is the question, what is a infinity? And this is where linear algebra can help us. And so the next step is to use some linear algebra where we diagonalize a. So it turns out a is pdp inverse. And if you do the calculation or use Wolfram Alpha, you get d is a diagonal matrix with one entry is one. And that one's super important. The other one are very small. So let's call them epsilon, delta, I don't know why not xi. And for p, we have the following. So again, only the first eigenvector matters. You'll see why. And I think we get two, two thirds, three halves. And I believe one. And the rest we don't really care. And why is this important? Because this decomposition allows us to calculate any power of a quite easily. Because for instance, a squared, that is a a, which is pdp inverse, pdp inverse, and p inverse and p cancels out. And you get pd squared p inverse. So a squared is pd squared p inverse. And in general, a to the n is pd to the n p inverse. But as we'll see soon, d to the n is very easy to calculate, which allows us to calculate a to the n in a much easier way. And nothing prevents us from letting n go to infinity. So in fact, the stuff that we want, v infinity, it's a infinity v naught, which just becomes pd infinity p naught, or p inverse of v naught, 0.25, 0.25, 0.25, 0.25. But what is d to the infinity? Remember, those eigenvalues are very small. So if you raise them to infinity, you get zero. So in the end, we get 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0. So this huge mess that seems very hard to calculate actually becomes quite easy. And if you actually do the calculation, which I'll skip because there's a nicer way very soon, you get the following. So in the end, we get v infinities, this vector. And finally, we get the answer to our question. After infinitely many clicks, which website is the most important one? Well, website one, followed by website three, followed by website four, followed by website two. And what is Google? Well, it's the same model, but with many websites. I think like billions of websites. It's the same idea. Finally, I do want to tell you something cool because there is a slightly faster way of doing this because it turns out, as you may have noticed, we didn't even have to calculate the other eigenvectors. All that really mattered is the eigenvalue one. And in fact, there's a theorem that says what is v infinity is just the eigenvector corresponding to one, which is two-thirds, three-halves, and one, but not quite because, well, this is not a probability vector. Well, but it is this, but divided by the sum of the terms. So two plus two-thirds plus three-halves plus one. And so in the end, you do get this vector 0.38, 0.12, 0.29, 0.19. How cool is that? All right. I hope you like this. And if you want to see more math, please make sure to subscribe to my channel. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 5.76, "text": " Thanks for watching and let me show you the math behind Google, at least the way it used to work", "tokens": [50364, 2561, 337, 1976, 293, 718, 385, 855, 291, 264, 5221, 2261, 3329, 11, 412, 1935, 264, 636, 309, 1143, 281, 589, 50652], "temperature": 0.0, "avg_logprob": -0.11871461366352282, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.001454669632948935}, {"id": 1, "seek": 0, "start": 5.76, "end": 14.88, "text": " back in the 90s. Because when I was younger and we had other search websites like Yahoo or Excite,", "tokens": [50652, 646, 294, 264, 4289, 82, 13, 1436, 562, 286, 390, 7037, 293, 321, 632, 661, 3164, 12891, 411, 41757, 420, 9368, 642, 11, 51108], "temperature": 0.0, "avg_logprob": -0.11871461366352282, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.001454669632948935}, {"id": 2, "seek": 0, "start": 14.88, "end": 21.12, "text": " it was quite a nightmare. Because if you were trying to look for a website about apples,", "tokens": [51108, 309, 390, 1596, 257, 18724, 13, 1436, 498, 291, 645, 1382, 281, 574, 337, 257, 3144, 466, 16814, 11, 51420], "temperature": 0.0, "avg_logprob": -0.11871461366352282, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.001454669632948935}, {"id": 3, "seek": 0, "start": 21.12, "end": 27.2, "text": " the one that would get the most hits was one where it just said apples, apples, apples, apples,", "tokens": [51420, 264, 472, 300, 576, 483, 264, 881, 8664, 390, 472, 689, 309, 445, 848, 16814, 11, 16814, 11, 16814, 11, 16814, 11, 51724], "temperature": 0.0, "avg_logprob": -0.11871461366352282, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.001454669632948935}, {"id": 4, "seek": 2720, "start": 27.2, "end": 34.879999999999995, "text": " apples, apples, which was not very useful. But then came Google and life was much easier because", "tokens": [50364, 16814, 11, 16814, 11, 597, 390, 406, 588, 4420, 13, 583, 550, 1361, 3329, 293, 993, 390, 709, 3571, 570, 50748], "temperature": 0.0, "avg_logprob": -0.19246746434105766, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.00658815260976553}, {"id": 5, "seek": 2720, "start": 34.879999999999995, "end": 42.16, "text": " they use what's called the page rank model, which looks as follows. Because suppose you have four", "tokens": [50748, 436, 764, 437, 311, 1219, 264, 3028, 6181, 2316, 11, 597, 1542, 382, 10002, 13, 1436, 7297, 291, 362, 1451, 51112], "temperature": 0.0, "avg_logprob": -0.19246746434105766, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.00658815260976553}, {"id": 6, "seek": 2720, "start": 42.16, "end": 49.84, "text": " websites which are connected as follows. So from three, you can go to one, from one, you can go to", "tokens": [51112, 12891, 597, 366, 4582, 382, 10002, 13, 407, 490, 1045, 11, 291, 393, 352, 281, 472, 11, 490, 472, 11, 291, 393, 352, 281, 51496], "temperature": 0.0, "avg_logprob": -0.19246746434105766, "compression_ratio": 1.6187845303867403, "no_speech_prob": 0.00658815260976553}, {"id": 7, "seek": 4984, "start": 49.84, "end": 57.36000000000001, "text": " two, three, and four. Then the way Google works is that it assigns what's called an importance vector.", "tokens": [50364, 732, 11, 1045, 11, 293, 1451, 13, 1396, 264, 636, 3329, 1985, 307, 300, 309, 6269, 82, 437, 311, 1219, 364, 7379, 8062, 13, 50740], "temperature": 0.0, "avg_logprob": -0.09867155018137462, "compression_ratio": 1.3691275167785235, "no_speech_prob": 0.003324344987049699}, {"id": 8, "seek": 4984, "start": 58.32000000000001, "end": 74.4, "text": " Let's call it V. And suppose V is, for instance, 0.2, 0.1, 0.4, and 0.3. Then what that means is that", "tokens": [50788, 961, 311, 818, 309, 691, 13, 400, 7297, 691, 307, 11, 337, 5197, 11, 1958, 13, 17, 11, 1958, 13, 16, 11, 1958, 13, 19, 11, 293, 1958, 13, 18, 13, 1396, 437, 300, 1355, 307, 300, 51592], "temperature": 0.0, "avg_logprob": -0.09867155018137462, "compression_ratio": 1.3691275167785235, "no_speech_prob": 0.003324344987049699}, {"id": 9, "seek": 7440, "start": 74.4, "end": 81.28, "text": " the highest number is better. So website three is more important than website four,", "tokens": [50364, 264, 6343, 1230, 307, 1101, 13, 407, 3144, 1045, 307, 544, 1021, 813, 3144, 1451, 11, 50708], "temperature": 0.0, "avg_logprob": -0.08481620233270186, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.00019411205721553415}, {"id": 10, "seek": 7440, "start": 81.28, "end": 86.0, "text": " which is more important than website one, which is more important than website two.", "tokens": [50708, 597, 307, 544, 1021, 813, 3144, 472, 11, 597, 307, 544, 1021, 813, 3144, 732, 13, 50944], "temperature": 0.0, "avg_logprob": -0.08481620233270186, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.00019411205721553415}, {"id": 11, "seek": 7440, "start": 86.64, "end": 94.96000000000001, "text": " So three would get suggested first and then four and then one and two. Now, how do we determine", "tokens": [50976, 407, 1045, 576, 483, 10945, 700, 293, 550, 1451, 293, 550, 472, 293, 732, 13, 823, 11, 577, 360, 321, 6997, 51392], "temperature": 0.0, "avg_logprob": -0.08481620233270186, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.00019411205721553415}, {"id": 12, "seek": 7440, "start": 94.96000000000001, "end": 101.36000000000001, "text": " that importance vector? Well, with just a little bit of linear algebra and probability.", "tokens": [51392, 300, 7379, 8062, 30, 1042, 11, 365, 445, 257, 707, 857, 295, 8213, 21989, 293, 8482, 13, 51712], "temperature": 0.0, "avg_logprob": -0.08481620233270186, "compression_ratio": 1.8571428571428572, "no_speech_prob": 0.00019411205721553415}, {"id": 13, "seek": 10440, "start": 105.12, "end": 116.80000000000001, "text": " Because what we need is some initial vector. So assume all four websites are treated equally.", "tokens": [50400, 1436, 437, 321, 643, 307, 512, 5883, 8062, 13, 407, 6552, 439, 1451, 12891, 366, 8668, 12309, 13, 50984], "temperature": 0.0, "avg_logprob": -0.12388584017753601, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.0003459712315816432}, {"id": 14, "seek": 10440, "start": 116.80000000000001, "end": 126.96000000000001, "text": " So assume our initial vector is just 0.25, 0.25, 0.25, and 0.25.", "tokens": [50984, 407, 6552, 527, 5883, 8062, 307, 445, 1958, 13, 6074, 11, 1958, 13, 6074, 11, 1958, 13, 6074, 11, 293, 1958, 13, 6074, 13, 51492], "temperature": 0.0, "avg_logprob": -0.12388584017753601, "compression_ratio": 1.4107142857142858, "no_speech_prob": 0.0003459712315816432}, {"id": 15, "seek": 12696, "start": 126.96, "end": 139.2, "text": " And some way of transitioning from the initial vector to the next step. And for this, we need", "tokens": [50364, 400, 512, 636, 295, 33777, 490, 264, 5883, 8062, 281, 264, 958, 1823, 13, 400, 337, 341, 11, 321, 643, 50976], "temperature": 0.0, "avg_logprob": -0.101387187616149, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0005527536268346012}, {"id": 16, "seek": 12696, "start": 139.2, "end": 146.24, "text": " something called a transition matrix. And in order to find a transition matrix, we need some", "tokens": [50976, 746, 1219, 257, 6034, 8141, 13, 400, 294, 1668, 281, 915, 257, 6034, 8141, 11, 321, 643, 512, 51328], "temperature": 0.0, "avg_logprob": -0.101387187616149, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0005527536268346012}, {"id": 17, "seek": 12696, "start": 146.24, "end": 154.48, "text": " probabilities. And you'll see why. In particular, think of it as follows. Suppose you start with", "tokens": [51328, 33783, 13, 400, 291, 603, 536, 983, 13, 682, 1729, 11, 519, 295, 309, 382, 10002, 13, 21360, 291, 722, 365, 51740], "temperature": 0.0, "avg_logprob": -0.101387187616149, "compression_ratio": 1.6264367816091954, "no_speech_prob": 0.0005527536268346012}, {"id": 18, "seek": 15448, "start": 154.48, "end": 162.88, "text": " website one. What are the chances of going to website two? Well, there's three possibilities,", "tokens": [50364, 3144, 472, 13, 708, 366, 264, 10486, 295, 516, 281, 3144, 732, 30, 1042, 11, 456, 311, 1045, 12178, 11, 50784], "temperature": 0.0, "avg_logprob": -0.06962839544635929, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007208286551758647}, {"id": 19, "seek": 15448, "start": 162.88, "end": 170.23999999999998, "text": " two, four, or three. And so in particular, from one to two, there's a third chance", "tokens": [50784, 732, 11, 1451, 11, 420, 1045, 13, 400, 370, 294, 1729, 11, 490, 472, 281, 732, 11, 456, 311, 257, 2636, 2931, 51152], "temperature": 0.0, "avg_logprob": -0.06962839544635929, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007208286551758647}, {"id": 20, "seek": 15448, "start": 170.23999999999998, "end": 178.95999999999998, "text": " of landing to website two. So one third, one third, one third. And well, then we can just", "tokens": [51152, 295, 11202, 281, 3144, 732, 13, 407, 472, 2636, 11, 472, 2636, 11, 472, 2636, 13, 400, 731, 11, 550, 321, 393, 445, 51588], "temperature": 0.0, "avg_logprob": -0.06962839544635929, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.0007208286551758647}, {"id": 21, "seek": 17896, "start": 178.96, "end": 186.48000000000002, "text": " complete the other arrows as follows. So it's a nice exercise to show that the other probabilities", "tokens": [50364, 3566, 264, 661, 19669, 382, 10002, 13, 407, 309, 311, 257, 1481, 5380, 281, 855, 300, 264, 661, 33783, 50740], "temperature": 0.0, "avg_logprob": -0.05797521935568915, "compression_ratio": 1.7100591715976332, "no_speech_prob": 0.0008040754473768175}, {"id": 22, "seek": 17896, "start": 186.48000000000002, "end": 195.20000000000002, "text": " are as follows. And this allows us to calculate our transition matrix. And the way you read the", "tokens": [50740, 366, 382, 10002, 13, 400, 341, 4045, 505, 281, 8873, 527, 6034, 8141, 13, 400, 264, 636, 291, 1401, 264, 51176], "temperature": 0.0, "avg_logprob": -0.05797521935568915, "compression_ratio": 1.7100591715976332, "no_speech_prob": 0.0008040754473768175}, {"id": 23, "seek": 17896, "start": 195.20000000000002, "end": 204.16, "text": " transition matrix is from top to left. And so this is website one, two, three, four, one, two,", "tokens": [51176, 6034, 8141, 307, 490, 1192, 281, 1411, 13, 400, 370, 341, 307, 3144, 472, 11, 732, 11, 1045, 11, 1451, 11, 472, 11, 732, 11, 51624], "temperature": 0.0, "avg_logprob": -0.05797521935568915, "compression_ratio": 1.7100591715976332, "no_speech_prob": 0.0008040754473768175}, {"id": 24, "seek": 20416, "start": 204.16, "end": 211.28, "text": " three, four. And you have to ask yourselves, well, to go from website one back to website one,", "tokens": [50364, 1045, 11, 1451, 13, 400, 291, 362, 281, 1029, 14791, 11, 731, 11, 281, 352, 490, 3144, 472, 646, 281, 3144, 472, 11, 50720], "temperature": 0.0, "avg_logprob": -0.07624518269240255, "compression_ratio": 1.9405405405405405, "no_speech_prob": 0.0031235506758093834}, {"id": 25, "seek": 20416, "start": 211.92, "end": 218.88, "text": " where there's no self-linkage here. So the probability is zero. To go from two to one,", "tokens": [50752, 689, 456, 311, 572, 2698, 12, 22473, 609, 510, 13, 407, 264, 8482, 307, 4018, 13, 1407, 352, 490, 732, 281, 472, 11, 51100], "temperature": 0.0, "avg_logprob": -0.07624518269240255, "compression_ratio": 1.9405405405405405, "no_speech_prob": 0.0031235506758093834}, {"id": 26, "seek": 20416, "start": 219.44, "end": 226.24, "text": " again, two does not connect to one. The probability is zero. To go from three to one,", "tokens": [51128, 797, 11, 732, 775, 406, 1745, 281, 472, 13, 440, 8482, 307, 4018, 13, 1407, 352, 490, 1045, 281, 472, 11, 51468], "temperature": 0.0, "avg_logprob": -0.07624518269240255, "compression_ratio": 1.9405405405405405, "no_speech_prob": 0.0031235506758093834}, {"id": 27, "seek": 20416, "start": 227.12, "end": 233.28, "text": " well, there's just one connection here. So the probability is one. And let's say to go from", "tokens": [51512, 731, 11, 456, 311, 445, 472, 4984, 510, 13, 407, 264, 8482, 307, 472, 13, 400, 718, 311, 584, 281, 352, 490, 51820], "temperature": 0.0, "avg_logprob": -0.07624518269240255, "compression_ratio": 1.9405405405405405, "no_speech_prob": 0.0031235506758093834}, {"id": 28, "seek": 23328, "start": 233.36, "end": 242.0, "text": " four to one, the probability is one-half. And well, then you can do the same spiel with websites", "tokens": [50368, 1451, 281, 472, 11, 264, 8482, 307, 472, 12, 25461, 13, 400, 731, 11, 550, 291, 393, 360, 264, 912, 637, 1187, 365, 12891, 50800], "temperature": 0.0, "avg_logprob": -0.12085620164871216, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.00018813791393768042}, {"id": 29, "seek": 23328, "start": 242.0, "end": 249.44, "text": " two, three, and four. And you end up getting the following matrix. Ta-da! Magic, isn't it?", "tokens": [50800, 732, 11, 1045, 11, 293, 1451, 13, 400, 291, 917, 493, 1242, 264, 3480, 8141, 13, 6551, 12, 2675, 0, 16154, 11, 1943, 380, 309, 30, 51172], "temperature": 0.0, "avg_logprob": -0.12085620164871216, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.00018813791393768042}, {"id": 30, "seek": 23328, "start": 250.56, "end": 256.4, "text": " Now, what are we trying to answer, by the way? We want to figure out in the end,", "tokens": [51228, 823, 11, 437, 366, 321, 1382, 281, 1867, 11, 538, 264, 636, 30, 492, 528, 281, 2573, 484, 294, 264, 917, 11, 51520], "temperature": 0.0, "avg_logprob": -0.12085620164871216, "compression_ratio": 1.4408602150537635, "no_speech_prob": 0.00018813791393768042}, {"id": 31, "seek": 25640, "start": 256.4, "end": 264.08, "text": " which website is the most important one. And in order to simplify, we just want to ask ourselves,", "tokens": [50364, 597, 3144, 307, 264, 881, 1021, 472, 13, 400, 294, 1668, 281, 20460, 11, 321, 445, 528, 281, 1029, 4175, 11, 50748], "temperature": 0.0, "avg_logprob": -0.08348532690518144, "compression_ratio": 1.698224852071006, "no_speech_prob": 0.00055277458159253}, {"id": 32, "seek": 25640, "start": 264.08, "end": 271.91999999999996, "text": " what happens if we let this run infinitely many times? So what happens after infinitely many clicks?", "tokens": [50748, 437, 2314, 498, 321, 718, 341, 1190, 36227, 867, 1413, 30, 407, 437, 2314, 934, 36227, 867, 18521, 30, 51140], "temperature": 0.0, "avg_logprob": -0.08348532690518144, "compression_ratio": 1.698224852071006, "no_speech_prob": 0.00055277458159253}, {"id": 33, "seek": 25640, "start": 272.47999999999996, "end": 278.47999999999996, "text": " And well, in order to do this, we first need to figure out what happens after one click.", "tokens": [51168, 400, 731, 11, 294, 1668, 281, 360, 341, 11, 321, 700, 643, 281, 2573, 484, 437, 2314, 934, 472, 2052, 13, 51468], "temperature": 0.0, "avg_logprob": -0.08348532690518144, "compression_ratio": 1.698224852071006, "no_speech_prob": 0.00055277458159253}, {"id": 34, "seek": 27848, "start": 279.36, "end": 287.76, "text": " Well, if you think about this, in order to go from the initial vector to what happens next,", "tokens": [50408, 1042, 11, 498, 291, 519, 466, 341, 11, 294, 1668, 281, 352, 490, 264, 5883, 8062, 281, 437, 2314, 958, 11, 50828], "temperature": 0.0, "avg_logprob": -0.10420438840791776, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0011513795470818877}, {"id": 35, "seek": 27848, "start": 287.76, "end": 297.68, "text": " you just have to apply a once. So v1 is av0. And well, to figure out what happens", "tokens": [50828, 291, 445, 362, 281, 3079, 257, 1564, 13, 407, 371, 16, 307, 1305, 15, 13, 400, 731, 11, 281, 2573, 484, 437, 2314, 51324], "temperature": 0.0, "avg_logprob": -0.10420438840791776, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0011513795470818877}, {"id": 36, "seek": 27848, "start": 298.48, "end": 307.20000000000005, "text": " after two clicks, so to figure out v2, that's just what happens when you apply a to the first step.", "tokens": [51364, 934, 732, 18521, 11, 370, 281, 2573, 484, 371, 17, 11, 300, 311, 445, 437, 2314, 562, 291, 3079, 257, 281, 264, 700, 1823, 13, 51800], "temperature": 0.0, "avg_logprob": -0.10420438840791776, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.0011513795470818877}, {"id": 37, "seek": 30720, "start": 308.15999999999997, "end": 318.8, "text": " So av1. But remember, v1 itself is transitioned, so it's just av0. And that becomes a squared v0.", "tokens": [50412, 407, 1305, 16, 13, 583, 1604, 11, 371, 16, 2564, 307, 47346, 11, 370, 309, 311, 445, 1305, 15, 13, 400, 300, 3643, 257, 8889, 371, 15, 13, 50944], "temperature": 0.0, "avg_logprob": -0.10831649679886668, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0005033282213844359}, {"id": 38, "seek": 30720, "start": 320.4, "end": 328.0, "text": " How cool is that? Using a squared, you can directly go from nothing to the second step.", "tokens": [51024, 1012, 1627, 307, 300, 30, 11142, 257, 8889, 11, 291, 393, 3838, 352, 490, 1825, 281, 264, 1150, 1823, 13, 51404], "temperature": 0.0, "avg_logprob": -0.10831649679886668, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0005033282213844359}, {"id": 39, "seek": 30720, "start": 328.0, "end": 336.71999999999997, "text": " It's like a teleportation. And in particular, to go directly from the initial step to the", "tokens": [51404, 467, 311, 411, 257, 28050, 399, 13, 400, 294, 1729, 11, 281, 352, 3838, 490, 264, 5883, 1823, 281, 264, 51840], "temperature": 0.0, "avg_logprob": -0.10831649679886668, "compression_ratio": 1.4705882352941178, "no_speech_prob": 0.0005033282213844359}, {"id": 40, "seek": 33672, "start": 336.72, "end": 346.16, "text": " end step, you just have to calculate a to the n. So a to the n v0. And of course,", "tokens": [50364, 917, 1823, 11, 291, 445, 362, 281, 8873, 257, 281, 264, 297, 13, 407, 257, 281, 264, 297, 371, 15, 13, 400, 295, 1164, 11, 50836], "temperature": 0.0, "avg_logprob": -0.10205314066502955, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.00041727666393853724}, {"id": 41, "seek": 33672, "start": 346.16, "end": 354.88000000000005, "text": " nothing prevents us from plugging in infinity here. So in particular, the answer to our question,", "tokens": [50836, 1825, 22367, 505, 490, 42975, 294, 13202, 510, 13, 407, 294, 1729, 11, 264, 1867, 281, 527, 1168, 11, 51272], "temperature": 0.0, "avg_logprob": -0.10205314066502955, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.00041727666393853724}, {"id": 42, "seek": 33672, "start": 354.88000000000005, "end": 366.08000000000004, "text": " v infinity, it's a infinity times v0. And so really, all we just need to answer is the question,", "tokens": [51272, 371, 13202, 11, 309, 311, 257, 13202, 1413, 371, 15, 13, 400, 370, 534, 11, 439, 321, 445, 643, 281, 1867, 307, 264, 1168, 11, 51832], "temperature": 0.0, "avg_logprob": -0.10205314066502955, "compression_ratio": 1.5953757225433527, "no_speech_prob": 0.00041727666393853724}, {"id": 43, "seek": 36608, "start": 366.08, "end": 375.76, "text": " what is a infinity? And this is where linear algebra can help us. And so the next step is to", "tokens": [50364, 437, 307, 257, 13202, 30, 400, 341, 307, 689, 8213, 21989, 393, 854, 505, 13, 400, 370, 264, 958, 1823, 307, 281, 50848], "temperature": 0.0, "avg_logprob": -0.11231608268542168, "compression_ratio": 1.5792349726775956, "no_speech_prob": 8.219910523621365e-05}, {"id": 44, "seek": 36608, "start": 375.76, "end": 384.79999999999995, "text": " use some linear algebra where we diagonalize a. So it turns out a is pdp inverse. And if you do the", "tokens": [50848, 764, 512, 8213, 21989, 689, 321, 21539, 1125, 257, 13, 407, 309, 4523, 484, 257, 307, 280, 67, 79, 17340, 13, 400, 498, 291, 360, 264, 51300], "temperature": 0.0, "avg_logprob": -0.11231608268542168, "compression_ratio": 1.5792349726775956, "no_speech_prob": 8.219910523621365e-05}, {"id": 45, "seek": 36608, "start": 384.79999999999995, "end": 395.84, "text": " calculation or use Wolfram Alpha, you get d is a diagonal matrix with one entry is one. And that", "tokens": [51300, 17108, 420, 764, 16634, 2356, 20588, 11, 291, 483, 274, 307, 257, 21539, 8141, 365, 472, 8729, 307, 472, 13, 400, 300, 51852], "temperature": 0.0, "avg_logprob": -0.11231608268542168, "compression_ratio": 1.5792349726775956, "no_speech_prob": 8.219910523621365e-05}, {"id": 46, "seek": 39584, "start": 395.84, "end": 404.15999999999997, "text": " one's super important. The other one are very small. So let's call them epsilon, delta, I don't", "tokens": [50364, 472, 311, 1687, 1021, 13, 440, 661, 472, 366, 588, 1359, 13, 407, 718, 311, 818, 552, 17889, 11, 8289, 11, 286, 500, 380, 50780], "temperature": 0.0, "avg_logprob": -0.17413730621337892, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0012065067421644926}, {"id": 47, "seek": 39584, "start": 404.15999999999997, "end": 415.12, "text": " know why not xi. And for p, we have the following. So again, only the first eigenvector matters.", "tokens": [50780, 458, 983, 406, 36800, 13, 400, 337, 280, 11, 321, 362, 264, 3480, 13, 407, 797, 11, 787, 264, 700, 10446, 303, 1672, 7001, 13, 51328], "temperature": 0.0, "avg_logprob": -0.17413730621337892, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.0012065067421644926}, {"id": 48, "seek": 41512, "start": 415.12, "end": 423.36, "text": " You'll see why. And I think we get two, two thirds, three halves. And I believe one.", "tokens": [50364, 509, 603, 536, 983, 13, 400, 286, 519, 321, 483, 732, 11, 732, 34552, 11, 1045, 38490, 13, 400, 286, 1697, 472, 13, 50776], "temperature": 0.0, "avg_logprob": -0.1427688201268514, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.007576813921332359}, {"id": 49, "seek": 41512, "start": 425.52, "end": 434.0, "text": " And the rest we don't really care. And why is this important? Because this decomposition", "tokens": [50884, 400, 264, 1472, 321, 500, 380, 534, 1127, 13, 400, 983, 307, 341, 1021, 30, 1436, 341, 48356, 51308], "temperature": 0.0, "avg_logprob": -0.1427688201268514, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.007576813921332359}, {"id": 50, "seek": 41512, "start": 434.0, "end": 443.92, "text": " allows us to calculate any power of a quite easily. Because for instance, a squared, that is a a,", "tokens": [51308, 4045, 505, 281, 8873, 604, 1347, 295, 257, 1596, 3612, 13, 1436, 337, 5197, 11, 257, 8889, 11, 300, 307, 257, 257, 11, 51804], "temperature": 0.0, "avg_logprob": -0.1427688201268514, "compression_ratio": 1.489010989010989, "no_speech_prob": 0.007576813921332359}, {"id": 51, "seek": 44392, "start": 444.8, "end": 454.96000000000004, "text": " which is pdp inverse, pdp inverse, and p inverse and p cancels out. And you get pd squared p inverse.", "tokens": [50408, 597, 307, 280, 67, 79, 17340, 11, 280, 67, 79, 17340, 11, 293, 280, 17340, 293, 280, 393, 66, 1625, 484, 13, 400, 291, 483, 280, 67, 8889, 280, 17340, 13, 50916], "temperature": 0.0, "avg_logprob": -0.09939962953001588, "compression_ratio": 1.9530201342281879, "no_speech_prob": 0.0015487102791666985}, {"id": 52, "seek": 44392, "start": 455.92, "end": 464.8, "text": " So a squared is pd squared p inverse. And in general, a to the n is pd to the n p inverse.", "tokens": [50964, 407, 257, 8889, 307, 280, 67, 8889, 280, 17340, 13, 400, 294, 2674, 11, 257, 281, 264, 297, 307, 280, 67, 281, 264, 297, 280, 17340, 13, 51408], "temperature": 0.0, "avg_logprob": -0.09939962953001588, "compression_ratio": 1.9530201342281879, "no_speech_prob": 0.0015487102791666985}, {"id": 53, "seek": 44392, "start": 465.68, "end": 473.52000000000004, "text": " But as we'll see soon, d to the n is very easy to calculate, which allows us to calculate a to the", "tokens": [51452, 583, 382, 321, 603, 536, 2321, 11, 274, 281, 264, 297, 307, 588, 1858, 281, 8873, 11, 597, 4045, 505, 281, 8873, 257, 281, 264, 51844], "temperature": 0.0, "avg_logprob": -0.09939962953001588, "compression_ratio": 1.9530201342281879, "no_speech_prob": 0.0015487102791666985}, {"id": 54, "seek": 47352, "start": 473.52, "end": 483.59999999999997, "text": " n in a much easier way. And nothing prevents us from letting n go to infinity. So in fact,", "tokens": [50364, 297, 294, 257, 709, 3571, 636, 13, 400, 1825, 22367, 505, 490, 8295, 297, 352, 281, 13202, 13, 407, 294, 1186, 11, 50868], "temperature": 0.0, "avg_logprob": -0.11244542643709003, "compression_ratio": 1.4402985074626866, "no_speech_prob": 0.00031010626116767526}, {"id": 55, "seek": 47352, "start": 483.59999999999997, "end": 495.03999999999996, "text": " the stuff that we want, v infinity, it's a infinity v naught, which just becomes pd infinity p naught,", "tokens": [50868, 264, 1507, 300, 321, 528, 11, 371, 13202, 11, 309, 311, 257, 13202, 371, 13138, 11, 597, 445, 3643, 280, 67, 13202, 280, 13138, 11, 51440], "temperature": 0.0, "avg_logprob": -0.11244542643709003, "compression_ratio": 1.4402985074626866, "no_speech_prob": 0.00031010626116767526}, {"id": 56, "seek": 49504, "start": 495.12, "end": 512.0, "text": " or p inverse of v naught, 0.25, 0.25, 0.25, 0.25. But what is d to the infinity? Remember, those", "tokens": [50368, 420, 280, 17340, 295, 371, 13138, 11, 1958, 13, 6074, 11, 1958, 13, 6074, 11, 1958, 13, 6074, 11, 1958, 13, 6074, 13, 583, 437, 307, 274, 281, 264, 13202, 30, 5459, 11, 729, 51212], "temperature": 0.0, "avg_logprob": -0.09888101002526661, "compression_ratio": 1.385185185185185, "no_speech_prob": 0.0010484589729458094}, {"id": 57, "seek": 49504, "start": 512.0, "end": 519.76, "text": " eigenvalues are very small. So if you raise them to infinity, you get zero. So in the end,", "tokens": [51212, 10446, 46033, 366, 588, 1359, 13, 407, 498, 291, 5300, 552, 281, 13202, 11, 291, 483, 4018, 13, 407, 294, 264, 917, 11, 51600], "temperature": 0.0, "avg_logprob": -0.09888101002526661, "compression_ratio": 1.385185185185185, "no_speech_prob": 0.0010484589729458094}, {"id": 58, "seek": 51976, "start": 519.76, "end": 533.12, "text": " we get 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0. So this huge mess that seems very hard to calculate", "tokens": [50364, 321, 483, 502, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 11, 1958, 13, 407, 341, 2603, 2082, 300, 2544, 588, 1152, 281, 8873, 51032], "temperature": 0.0, "avg_logprob": -0.09392058849334717, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0002341335784876719}, {"id": 59, "seek": 51976, "start": 533.12, "end": 540.96, "text": " actually becomes quite easy. And if you actually do the calculation, which I'll skip because there's", "tokens": [51032, 767, 3643, 1596, 1858, 13, 400, 498, 291, 767, 360, 264, 17108, 11, 597, 286, 603, 10023, 570, 456, 311, 51424], "temperature": 0.0, "avg_logprob": -0.09392058849334717, "compression_ratio": 1.476923076923077, "no_speech_prob": 0.0002341335784876719}, {"id": 60, "seek": 54096, "start": 541.0400000000001, "end": 549.12, "text": " a nicer way very soon, you get the following. So in the end, we get v infinities, this vector.", "tokens": [50368, 257, 22842, 636, 588, 2321, 11, 291, 483, 264, 3480, 13, 407, 294, 264, 917, 11, 321, 483, 371, 7193, 1088, 11, 341, 8062, 13, 50772], "temperature": 0.0, "avg_logprob": -0.08522469376864499, "compression_ratio": 1.68, "no_speech_prob": 0.05107990279793739}, {"id": 61, "seek": 54096, "start": 549.76, "end": 557.12, "text": " And finally, we get the answer to our question. After infinitely many clicks, which website is", "tokens": [50804, 400, 2721, 11, 321, 483, 264, 1867, 281, 527, 1168, 13, 2381, 36227, 867, 18521, 11, 597, 3144, 307, 51172], "temperature": 0.0, "avg_logprob": -0.08522469376864499, "compression_ratio": 1.68, "no_speech_prob": 0.05107990279793739}, {"id": 62, "seek": 54096, "start": 557.12, "end": 566.24, "text": " the most important one? Well, website one, followed by website three, followed by website four, followed", "tokens": [51172, 264, 881, 1021, 472, 30, 1042, 11, 3144, 472, 11, 6263, 538, 3144, 1045, 11, 6263, 538, 3144, 1451, 11, 6263, 51628], "temperature": 0.0, "avg_logprob": -0.08522469376864499, "compression_ratio": 1.68, "no_speech_prob": 0.05107990279793739}, {"id": 63, "seek": 56624, "start": 566.24, "end": 575.6800000000001, "text": " by website two. And what is Google? Well, it's the same model, but with many websites. I think", "tokens": [50364, 538, 3144, 732, 13, 400, 437, 307, 3329, 30, 1042, 11, 309, 311, 264, 912, 2316, 11, 457, 365, 867, 12891, 13, 286, 519, 50836], "temperature": 0.0, "avg_logprob": -0.06129993619145574, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.0009697335190139711}, {"id": 64, "seek": 56624, "start": 575.6800000000001, "end": 583.92, "text": " like billions of websites. It's the same idea. Finally, I do want to tell you something cool", "tokens": [50836, 411, 17375, 295, 12891, 13, 467, 311, 264, 912, 1558, 13, 6288, 11, 286, 360, 528, 281, 980, 291, 746, 1627, 51248], "temperature": 0.0, "avg_logprob": -0.06129993619145574, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.0009697335190139711}, {"id": 65, "seek": 56624, "start": 583.92, "end": 591.2, "text": " because there is a slightly faster way of doing this because it turns out, as you may have noticed,", "tokens": [51248, 570, 456, 307, 257, 4748, 4663, 636, 295, 884, 341, 570, 309, 4523, 484, 11, 382, 291, 815, 362, 5694, 11, 51612], "temperature": 0.0, "avg_logprob": -0.06129993619145574, "compression_ratio": 1.5513513513513513, "no_speech_prob": 0.0009697335190139711}, {"id": 66, "seek": 59120, "start": 591.2, "end": 598.0, "text": " we didn't even have to calculate the other eigenvectors. All that really mattered is the", "tokens": [50364, 321, 994, 380, 754, 362, 281, 8873, 264, 661, 10446, 303, 5547, 13, 1057, 300, 534, 44282, 307, 264, 50704], "temperature": 0.0, "avg_logprob": -0.09796019962855748, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.004609373398125172}, {"id": 67, "seek": 59120, "start": 598.0, "end": 609.12, "text": " eigenvalue one. And in fact, there's a theorem that says what is v infinity is just the eigenvector", "tokens": [50704, 10446, 29155, 472, 13, 400, 294, 1186, 11, 456, 311, 257, 20904, 300, 1619, 437, 307, 371, 13202, 307, 445, 264, 10446, 303, 1672, 51260], "temperature": 0.0, "avg_logprob": -0.09796019962855748, "compression_ratio": 1.4461538461538461, "no_speech_prob": 0.004609373398125172}, {"id": 68, "seek": 60912, "start": 609.12, "end": 623.36, "text": " corresponding to one, which is two-thirds, three-halves, and one, but not quite because, well,", "tokens": [50364, 11760, 281, 472, 11, 597, 307, 732, 12, 38507, 11, 1045, 12, 4947, 977, 11, 293, 472, 11, 457, 406, 1596, 570, 11, 731, 11, 51076], "temperature": 0.0, "avg_logprob": -0.1523211045698686, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.016402684152126312}, {"id": 69, "seek": 60912, "start": 623.36, "end": 633.36, "text": " this is not a probability vector. Well, but it is this, but divided by the sum of the terms. So", "tokens": [51076, 341, 307, 406, 257, 8482, 8062, 13, 1042, 11, 457, 309, 307, 341, 11, 457, 6666, 538, 264, 2408, 295, 264, 2115, 13, 407, 51576], "temperature": 0.0, "avg_logprob": -0.1523211045698686, "compression_ratio": 1.3970588235294117, "no_speech_prob": 0.016402684152126312}, {"id": 70, "seek": 63336, "start": 633.36, "end": 643.36, "text": " two plus two-thirds plus three-halves plus one. And so in the end, you do get this vector 0.38,", "tokens": [50364, 732, 1804, 732, 12, 38507, 1804, 1045, 12, 4947, 977, 1804, 472, 13, 400, 370, 294, 264, 917, 11, 291, 360, 483, 341, 8062, 1958, 13, 12625, 11, 50864], "temperature": 0.0, "avg_logprob": -0.10875020708356585, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.0033765286207199097}, {"id": 71, "seek": 63336, "start": 643.36, "end": 653.2, "text": " 0.12, 0.29, 0.19. How cool is that? All right. I hope you like this. And if you want to see", "tokens": [50864, 1958, 13, 4762, 11, 1958, 13, 11871, 11, 1958, 13, 3405, 13, 1012, 1627, 307, 300, 30, 1057, 558, 13, 286, 1454, 291, 411, 341, 13, 400, 498, 291, 528, 281, 536, 51356], "temperature": 0.0, "avg_logprob": -0.10875020708356585, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.0033765286207199097}, {"id": 72, "seek": 63336, "start": 653.2, "end": 657.2, "text": " more math, please make sure to subscribe to my channel. Thank you very much.", "tokens": [51356, 544, 5221, 11, 1767, 652, 988, 281, 3022, 281, 452, 2269, 13, 1044, 291, 588, 709, 13, 51556], "temperature": 0.0, "avg_logprob": -0.10875020708356585, "compression_ratio": 1.4585635359116023, "no_speech_prob": 0.0033765286207199097}], "language": "en"}