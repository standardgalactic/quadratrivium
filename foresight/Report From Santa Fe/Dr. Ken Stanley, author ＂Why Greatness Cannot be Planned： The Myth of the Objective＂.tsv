start	end	text
0	16600	Report from Santa Fe is made possible in part by grants from the members of the National
16600	22760	Education Association of New Mexico, an organization of professionals who believe that investing
22760	29400	in public education is an investment in our state's economic future, and by a grant from
29400	33600	the Healy Foundation, Taos, New Mexico.
33600	37400	Hello, I'm Loreen Mills, and welcome to Report from Santa Fe.
37400	39800	Our guest today is Kenneth Stanley.
39800	41400	Thank you for joining us.
41400	42400	Thank you, Loreen.
42400	43400	Really happy to be here.
43400	47720	Well, you are an associate professor at the University of Central Florida.
47720	52760	You're a computer scientist, an artificial intelligence researcher, and you're here on
52760	55320	sabbatical with the Santa Fe Institute.
55320	56320	That's right.
56320	58440	So that's how we get to have you.
58440	65440	You are the co-author with Joel Lehman of this fascinating book, Why Greatness Cannot
65440	68960	Be Planned, and the subtitle is?
68960	70880	The Myth of the Objective.
70880	72560	What is the objective?
72560	74960	What do you mean, the myth of the objective?
74960	79800	So when I'm talking about the myth of the objective, I'm talking about the fact that
79800	84840	so many things that we do in our lives and in our culture are guided by what we call
84840	85840	objectives.
85840	88640	Like, it's something that you set that you want to achieve, and you say, this is now
88640	93360	my objective, or this is our objective as a society, or as an institution, or something
93360	94640	like that.
94640	99760	And we believe, and this is like a really deeply entrenched aspect of our culture, that by setting
99760	103200	an objective, it makes it possible to achieve it.
103200	107280	And the message of the book, and the reason we call it the myth of the objective, is that
107280	112520	actually many times, and this is a surprise, having an objective can actually prevent you
112520	114800	from achieving your objective.
114920	120920	So, it is a surprise, and something that we intuit, but the fact that you've come through
120920	127320	it through computer science, and you wonder how many times does a theory in computer science
127320	132360	affect our real life, but you have almost, some have said that this is in a way a scientific
132360	134080	proof of serendipity.
134080	137400	Yeah, I mean, this is a really unusual story.
137400	138400	It's true.
138400	144160	You usually don't end up prescribing social policy based on algorithms in computer science.
144160	149520	And it was a long, you know, circuitous path for us to get from these scientific experiments
149520	153840	to the point where we're actually critiquing certain things about culture and society.
153840	155640	It wasn't our intention from the beginning.
155640	157040	But that's what the whole book is about.
157040	158040	Yeah, it's kind of...
158040	161040	It had been your intention, you wouldn't have been able to do it.
161040	164880	Yeah, I mean, it's like the story behind the book is almost like part of the theme of the
164880	165880	book.
165880	167200	Well, let's tell us that story.
167200	172880	How did you arrive at this through your studies of algorithms and artificial intelligence?
173240	178200	Right, so this is going to sound a little off topic, but actually this is the story
178200	179200	of how we got to it.
179200	180720	We were doing something completely different.
180720	182880	We had no idea we were going to get here.
182880	188760	But we had created a website on the internet, which is called Pick Breeder, where people
188760	192160	can come in and breed pictures.
192160	196040	And I know that probably listeners don't really know what that means to breed a picture,
196040	199200	like you don't mate Picasso with a van Gogh.
199200	202760	But you know, I come from artificial intelligence and in that field, actually, we had been
202800	208760	working on technologies that would allow us to, in effect, give a picture a child.
208760	212880	And just as when you have a child, it might look somewhat like you, but be a little bit
212880	214080	different.
214080	220360	When Pick Breeder images have children, they also look like their parents, but not exactly.
220360	224800	And that allows users to, in effect, breed pictures the way you might breed horses or
224800	227120	dogs or something like that.
227120	229920	So I know that our audience is going to want to go look at this.
229920	236320	So it's www.pickbreeder, P-I-C-B-R-E-E-D-E-R dot org.
236320	242000	So show us some of the images, say a person would go to that site, and then track us down
242000	243800	the amazing work that you had done.
243800	247680	Yeah, so it's really surprising, I think, some of the images that people have discovered.
247680	255440	I'm just going to show a few images that give a sense of what people have discovered.
255440	260760	And you can see that these images actually look like real things.
260760	264200	Like there's a skull there, there's a butterfly, and this is just a small sampling of what
264200	265760	people discovered.
265760	270080	And it's important to point out that people evolved these things from blobs.
270080	273520	That's how things start, just total random blobs.
273520	276040	And they got to these amazing images.
276040	280440	And that's the beginning of the story of how we came to this realization, because we started
280440	284480	to look at how people were making these discoveries.
284480	290040	How people were breeding, say, an image that looks like a car or a skull.
290040	295840	And it turns out, and this is sort of where the insight began, that you might expect that
295840	299760	if you came into this site and you were trying to breed, breed images like you might breed
299760	303040	horses, that you would say, well, I'm trying to breed a butterfly or something.
303040	306120	And then you would sort of pick things that look more and more butterfly-like, and that's
306120	308040	your objective, and then you get it.
308040	313040	And we thought that too, but what we discovered was that, in fact, the only way that any of
313040	317200	these interesting discoveries were being made was when they were not the objective of the
317200	318840	user who bred them.
318840	322800	And this was just a huge revelation for us, completely unexpected.
322800	327440	It blew me over because it contradicted everything that I understood about achievement and how
327440	329400	to get to an objective.
329400	336240	Show us, I think one of the clearest examples is the image of the alien face, which then,
336240	341360	through evolutionary art, as they call this, became the image of a car.
341360	346080	And tell me how many generations it took, but first let's see the image.
346080	348680	So let me show you a picture of this alien face.
348680	354280	This is a story, part of a story about how this initial realization came about.
354280	360440	And this alien face, it's important to note, was not bred by me, it was bred by someone
360440	361440	else.
361440	366360	It had been posted on the site so that I was able to see that it was there, and then I
366360	371200	decided to continue from there, which we call branching.
371200	373400	And here's what happened.
373400	377200	It was several generations, actually I don't know the exact number, maybe 15 generations
377200	383600	or 20 generations, of picking children and continually breeding that then led me to this
383600	387360	really surprising discovery, which is a car.
387360	392600	So what happened, the story is that the alien face was bred into a car.
392600	398320	And what's so important about this is the fact that I had this experience myself, so
398320	402460	I know for a fact I was not trying to evolve a car.
402460	406240	And if you put all that together, I mean just think about how strange this story is.
406240	410360	I chose an alien face, which someone else evolved, it wasn't me, so I wouldn't have
410360	412200	evolved that myself.
412200	416440	And then I evolved a car, which is something that I wasn't trying to evolve.
416440	420560	So someone else had to do something that I never would have done, and then I had to not
420560	424720	be trying to do what I ultimately did for me to make this discovery.
424720	429080	And if you hear that story, it sounds like, wow, that was a big coincidence, you got really
429080	430800	lucky that one time.
430800	435200	But the amazing thing that we discovered is that that actually is always the story.
435200	439280	Almost every interesting discovery in the site has that exact same story.
439280	442760	Somebody evolved something that the other person would never have evolved, and then the second
442760	447200	person kind of took it in a handoff and bred it to a place that they weren't actually trying
447200	448480	to get to.
448480	451440	And that's how you actually find things on Pick Reader.
451440	455720	But one of the most amazing statistics that you told me was that there are some that have
455720	463600	taken 50 generations or 90, 74 generations, but then when you programmed a computer to
463600	471200	do it, to go from the alien to the car, after 30,000 generations, they could not do it.
471200	473960	With that intention of going there, they couldn't.
473960	475720	The computer, try and make it.
475720	480320	This is a really important point, it's really surprising if you think about it.
480320	485080	I mean, what I told you basically is that people find things by not looking for them,
485080	489480	but the converse is also really surprising, which is people do not find things by looking
489480	490480	for them.
490480	494840	Like if you came in there and you wanted to evolve a butterfly, you would fail because
494840	498040	as an objective, it's too hard to actually discover.
498040	502240	And these experiments that you just described are basically confirmation of that fact in
502240	508320	kind of an extreme way by giving it 30,000 generations to try to reproduce an image that
508320	509920	already had been discovered.
509920	515000	So we know it's even possible and every time we tried it, each 30,000 generation attempt
515000	520520	it failed, it never succeeded and it just illustrates that it's sometimes impossible
520520	525480	to reach a destination by trying, even though we know that these destinations can be reached
525480	527120	because they have been.
527120	532040	And I just want to point out again that the image on the cover of your book is one of
532040	537720	the butterflies that was evolved from something completely different without trying to make
537720	538720	a butterfly.
538720	544560	It's just another example and basically every image has that story behind it.
544560	546440	And you know, the reason is kind of important.
546440	548240	So you may ask, well, why is it like that?
548240	551560	You know, why is it that you can only find things by not looking for them?
551560	554600	And the reason is because if you think about it, like if you think about the example of
554600	560560	the car, an alien face led to that car, no one would ever guess that you can get from
560560	562400	an alien face to a car.
562400	565360	If you look closely in hindsight, you can see that...
565480	568520	I think you have a picture of the eyes and the wheels.
568520	580840	Yeah, I can show a connection there that, in fact, the eyes of the alien turned into
580840	582400	the wheels of the car.
582400	586640	And this is something that's only obvious in hindsight.
586640	589560	But looking forward, you would never realize something like that.
589560	593600	And so the point is that the stepping stones, in this case the stepping stone is this alien
593600	594600	face that led to a car.
594600	599000	The stepping stones never resemble the final product, and that's why you would not know
599000	603520	to go through those stepping stones if the final product was your objective.
603520	608360	So the only way you can get to these important stepping stones is by not being single-mindedly
608360	610560	focused on a single objective.
610560	614040	And then you open up all that possibility of eventually reaching these destinations
614040	615640	that were not your objective.
615640	621080	Now I'm going to expand this a little bit because it is as incredible cultural and social and
621080	622960	evolutionary.
622960	629480	So our world is so governed by objectives and talked about goal-oriented, whether it's
629480	634920	test scores, scientific funding, I mean, we could go on and on, bottom line profits, career
634920	636120	tracks, everything.
636120	639080	You have a goal and then you work toward that goal.
639080	644920	And then what you're saying, if the objective paradox is that sometimes you have to give
644920	651120	up that goal to achieve what you, you know, something maybe greater, if your goal is making
651120	653800	a sandwich, it's very clear what the steps are.
653800	661520	But if your goal is to end world hunger or to cure cancer or to make man fly, you shouldn't
661520	665840	be, there's no trajectory that will step by step take you there.
665840	667400	It often hinders you.
667400	668400	Yes.
668400	670560	I mean, and I think, you know, we're making a leap here.
670560	676160	We're going from picture breeding to how really important discoveries happen.
676160	681800	And while this is a strange leap, we should really be concerned when we think about this.
681800	686560	You know, because really the way that discovery happens is no different from pick breeder.
686560	691320	All of us, when we lead, when we discover something, are building on something that was created
691320	693200	by our predecessors.
693200	695440	And those are the stepping stones.
695440	701440	And what this lesson teaches us is that if those stepping stones do not resemble what
701440	705960	we're ultimately trying to achieve, then there's almost no way that we're going to actually
705960	709040	traverse those stepping stones intentionally.
709040	714200	And therefore, we should be worried about a society that's molded in such a way that
714200	719280	we're always trying to constrain everything we do through some kind of objective compass
719280	723160	because the effect is going to be that we prune out the stepping stones, which don't
723160	724720	look like the objective.
724720	728960	And that, in fact, is how we run a lot of our society.
728960	736280	Now, serendipity, let's talk about what great discoveries that have been—I mean, your
736280	741000	book is full of its fascinating read—is full of these ways that—how about Elvis?
741000	745320	Talk to me about Elvis and the microwave and some of your favorite stories.
745320	746320	Yeah, sure.
746320	752360	So the book gives a lot of these examples of—sometimes we call it serendipitous discovery.
752360	756520	And these are really important because they illustrate that this is not just about pick
756520	757520	breeder.
758520	763520	Like Elvis is an example because Elvis was not trying to invent rock and roll.
763520	768960	But he effectively did at least become one of the inventors of rock and roll.
768960	774760	And his colleagues say that he was just playing around and being himself.
774760	778200	And it's almost inconceivable that someone could have an objective of inventing rock
778200	781320	and roll because if you did, then you probably already invented it.
781320	784520	And it just goes to show that there's lots of things like that in life that you're not
784520	787560	going to run across unless you're not trying to create them.
787560	790160	Now, there are more practical things than just rock and roll.
790160	793000	I mean, there's things like the computer.
793000	794000	I like this example.
794000	798040	You know, the computers—the first computers were made out of vacuum tubes.
798040	801360	So vacuum tubes are a stepping stone that leads to computers.
801360	802920	But here's an interesting fact.
802920	807100	The people who invented vacuum tubes were not thinking about computers.
807100	809280	They were used for electrical experiments.
809280	813200	In fact, if your interest had been in computers, why would you be making vacuum tubes?
813200	816000	It has nothing to do, obviously, with computers.
816000	819680	So it's a good thing that their objective was not to make computers or else we wouldn't
819680	821400	have them.
821400	823640	And these kind of examples, you can go on and on.
823640	824880	The microwave one I like.
824880	826920	The microwave one's another good one.
826920	831080	And this is where Percy Spencer—and this is a very serendipitous thing—where he
831080	834880	had a candy bar in his pocket and he was working on radars, which had microwave technology
834880	835880	in them.
835880	839600	And he noticed that the candy bar melted and suddenly realized, you know what, this actually
839600	841360	leads to a microwave oven.
841360	845480	So you get radar technology as a stepping stone to microwave ovens.
845480	849320	Once again, no way to project looking forward that you go from radars to a new kind of an
849320	850320	oven.
850320	854000	But looking in hindsight, you can see where the connection came in.
854000	857800	And this kind of example happens again and again, and in fact, I just would like to point
857800	863520	the viewers to the Wikipedia page on serendipity, where there are tons of examples like this.
863520	866880	And the suspicious thing about it, the thing that I think people should take note of is
866880	871600	that the people who made these discoveries all have a good track record or all educated
871600	874400	or have been successful in the past.
874400	881720	I mean, if this was just a complete accident, then you wouldn't have these very well respected
881720	883680	people making these discoveries.
883680	887880	So it suggests that—and I think we're going to get to this—that this kind of process
887880	889640	of discovery is not pure random.
889640	896400	You can actually try to fasten your life in such a way that you have more opportunities
896440	898600	to encounter this kind of serendipity.
898600	901760	And this is something as a society that we can also try to facilitate.
901760	907360	Well, we're speaking today with Kenneth Stanley about his new book, Why Greatness
907360	908880	Cannot Be Planned.
908880	914240	But as you described this, again, we have an unconscious understanding, and perhaps it's
914240	921400	artists and musicians the most who understand this.
921400	926120	Talk to me about stepping stones and talk to me about how we can live in a way that
926120	931600	we are aware of these evolutionary steps.
931600	932600	What do you call it?
932600	933600	Treasure hunter.
933600	934600	What is it?
934600	936040	Who would be a treasure hunter?
936040	937680	How do we become treasure hunters?
937680	942760	Yeah, so the natural question that follows from the story that I've kind of given so
942760	946360	far is, well, how do we make this happen?
946360	949920	It sounds nice that, well, we can all just do whatever we want.
950040	953840	What's the real formula here that actually someone can key in on?
953840	957280	And the answer is that when you look at a system like Pick Breeder, what we're actually
957280	960320	observing when we came to understand what was happening, why people were making all
960320	965600	these discoveries, is that the system as a whole, the website in this example, is collecting
965600	966880	stepping stones.
966880	971120	Every time someone comes to that website and discovers something and publishes it to the
971120	976600	community, everybody can see it, like the alien face that I saw that I eventually evolved
976600	977600	into a car.
977600	978600	That's a stepping stone.
978600	983520	And the more stepping stones people publish, the more opportunities or departure points
983520	985840	are created for further discovery.
985840	989120	And that's what I mean by a stepping stone collector.
989120	995200	And it's in our interest to try to fashion society, especially the parts of it that are
995200	999160	focused on innovation or discovery, as a stepping stone collector.
999160	1002040	And I sometimes also, like you said, call it a treasure hunter, because you can think
1002040	1003520	of these as treasures that are sort of buried.
1003520	1004520	We don't know what they are.
1004520	1008480	It's not an objective, but we know that there's lots of interesting stuff out there.
1008680	1013120	And we go hunting and trying to uncover them as much as we can.
1013120	1014640	You're used to the word interesting.
1014640	1021320	So you have created a noun, I don't know if it exists, called interestingness, right?
1021320	1027200	And you say that the scent of interestingness can lead us across these chains of stepping
1027200	1030120	stones into who knows what.
1030120	1034120	But because we do this, because it's interesting, and when people are choosing those images,
1034120	1035600	it's just the one that kind of interests them.
1035600	1038640	They're not thinking about the ratios or the proportion.
1038640	1043600	And so when you're guided by your own instincts, where will this take you?
1043600	1044600	Yeah.
1044600	1047800	So we tend not to trust people's instincts.
1047800	1052840	I mean, that's one of the morals here, is when we talk about interestingness, that's
1052840	1055680	the scent you have to follow if you don't have an objective.
1055680	1058920	You know, you're saying, well, which path should I follow?
1058920	1062120	And you don't know where any of them lead, because there's no objective.
1062120	1065760	That doesn't mean that you can't make an intelligent decision.
1065760	1069680	Often people will follow the path that's most interesting to them.
1069680	1074560	And what we've kind of done as a culture is decided that that's not a legitimate thing
1074560	1075560	to do.
1075560	1079960	In other words, we say that just about anything that you do has to somehow be objectively
1079960	1081120	validated.
1081120	1083880	You have to say what your objective is, and then we have to prove that you're actually
1083880	1085680	moving towards that objective.
1085680	1089880	Which means that nobody can get away with actually following the scent of interestingness.
1089880	1093640	If you think about, you know, accountability and metrics, and these things are pervasive
1093640	1098000	in our society, they're all objectively driven, and they're all pruning out this concept of
1098000	1102800	interestingness, which ultimately means we don't trust people to use their instincts
1102800	1107360	or their expertise, and we think it's too risky to let them do that, and so we're pruning
1107360	1111720	out all of the creative possibilities that those people could potentially uncover.
1111720	1117120	So here we have the objective paradox, which is to achieve our highest goals.
1117120	1119360	We must be willing to abandon them.
1119360	1124000	And then how can we create an environment for ourselves to go against the myth of the
1124000	1125000	objective?
1125000	1127840	Because you're saying working toward an objective is not going to do it.
1127840	1130960	So you have posited a day without objectives.
1130960	1132640	What would that do?
1132640	1137720	And you say that not meeting our objectives, which are superimposed on this, or come from
1137720	1139760	within, is very stressful.
1139760	1143400	You feel like an underachiever because you didn't win the Nobel Prize today.
1143400	1149040	And how would society be different just even if we could take some time off from objectives?
1149040	1150520	For kids, for example.
1150520	1151520	Yeah.
1151520	1155120	I mean, and you could think about this in the context of the education system where
1155120	1159920	people just saturated with things like standardized tests.
1159920	1161800	And this is no doubt.
1161800	1166320	Most people have an instinct that this is having a bad effect on children.
1166320	1173040	And we've done experiments, further experiments, beyond Pick Reader, where we actually formalized
1173040	1175000	Pick Reader in a computer program.
1175000	1180280	We called it Novelty Search, which we used to kind of demonstrate and show that it is
1180280	1186600	actually possible to implement this principle in practice and use it to make discoveries
1186600	1187600	that are useful.
1187600	1191040	And one of the things that we found in these experiments is that allowing things to make
1191040	1196720	mistakes and do things that we think are objectively perhaps bad or subpar actually leads them
1196720	1202160	eventually to discovering really important skills and abilities.
1202160	1206040	And so that might be a difference if we could allow people to do that, is that we would
1206040	1211920	get to places that we couldn't get to in a more efficient and also satisfying way.
1211920	1214960	Well, you talk about it being a different path to achievement.
1214960	1221000	And you did have, in the Novelty Search, you had a program where you created the jogger.
1221000	1225120	And I think you have an illustration of this jogger.
1225120	1233440	And the jogger's job was to go down through these walls.
1233440	1234440	And well, tell me.
1234440	1235440	Tell me what you're...
1235440	1242440	Yeah, so we made this program, which is a kind of demonstration of this principle, this
1242440	1244960	non-objective principle.
1244960	1248640	And you know, Pick Reader has humans in the loop, humans are making decisions.
1248640	1252200	We wanted to show that we can do this completely inside of a computer, partly because I'm
1252200	1253280	an AI researcher.
1253280	1257160	So people in my field want computers to do stuff on their own that's intelligent.
1257160	1262040	But usually in the field of artificial intelligence, if you were making a program to train a robot
1262040	1265800	to walk, and here I'll show a picture of what this robot looks like.
1265800	1269460	This is a robot in different kind of steps of walking.
1269460	1275640	Usually if you were trying to do something like this, you would reward the robot in some
1275640	1277840	sense for getting better at walking.
1277840	1279840	So it's like if it can get farther, it's better.
1279840	1283480	And then we would elaborate on that strategy, and then we get better and so forth.
1283480	1289020	What we did is we created this program called Novelty Search, which does not try to improve
1289020	1290520	with respect to walking.
1290520	1294280	Rather, it just says you're doing a good job if you're doing something different than
1294280	1295880	you've done before.
1295880	1298800	And so it falls on its face, it does all kinds of stupid things.
1298800	1301600	But if it falls on its face in a different way, that's good.
1301600	1302840	We say great.
1302840	1306360	That means that that's a stepping stone that we should explore further to see what might
1306360	1308880	happen if we elaborate on that idea.
1308880	1313480	And in so doing, eventually it actually led to walkers, and it actually led to walking
1313480	1317880	robots that walk better than the ones that were trained by rewarding them for trying
1317880	1318880	to pursue the objective.
1318880	1325240	So it kind of demonstrated this point in a real concrete, scientific, empirical way.
1325240	1331040	So one of the things that you say, it's sometimes better to start from where you are instead
1331040	1332400	of where you want to go.
1332400	1334920	Talk to me about the Wright brothers.
1335360	1340960	So when you think about objectives, what an objective really is, it's about comparing
1340960	1344520	where you are to where you want to be.
1344520	1349600	And when you talk about something like novelty, which is not an objective concept, novelty
1349600	1353840	is basically comparing where you are to where you've been before.
1353840	1355840	And for some reason, we don't like doing that.
1355840	1358960	We want to always compare ourselves to the future.
1358960	1362960	But something like the Wright brothers, we can view, sometimes that's used as a counter
1362960	1363960	example.
1364400	1367400	They wanted to build something that can fly, and then they did.
1367400	1371880	But we can also view it as the Wright brothers were at the right stepping stone at the right
1371880	1377880	time and realized that right at that moment, the stepping stones had been laid.
1377880	1381680	So this is where they are right now that have suddenly enabled us to move in a direction
1381680	1382680	towards.
1382680	1384800	And they were bicycle manufacturers.
1384800	1385800	They were making bicycles.
1385800	1386800	Yeah, right.
1386800	1390120	So in some sense for them, a bicycle was a stepping stone that kind of made an analogy
1390120	1391120	with a plane.
1391120	1394120	And they saw that you can bridge that gap.
1394120	1400320	So sometimes it makes sense to just sort of appreciate the possibilities of where you
1400320	1405180	are, as opposed to saying this is this long term vision that we need to achieve and everything
1405180	1406840	I do will now be compared to that.
1406840	1408560	We only have a couple of minutes left.
1408560	1412480	Steve Jobs talks about this a lot.
1412480	1416680	Because he was in college, he dropped out, he took a calligraphy class, he said, only
1416680	1420120	looking back, can I see how one thing led to another?
1420120	1425840	And that's why Apple and his products were as beautiful and as sensible as they were.
1425840	1429840	Yeah, I mean, Steve Jobs, he could have been a co-author of this book because his life
1429840	1433480	was really kind of lived in the spirit.
1433480	1437680	And like this calligraphy class that he took, and he didn't know that this was going to
1437680	1443120	lead to a revolution in computer fonts, until in hindsight he saw that a connection was
1443120	1444120	there.
1444120	1445120	And he has a lot of stories like this.
1445120	1448040	He dropped out of college and that's what allowed him to take the calligraphy class.
1448040	1453400	And most people would not think of dropping out of a college as an objectively good decision.
1453400	1458240	So a lot of people have done great things, have followed this philosophy in some sense.
1458240	1463060	Well, you have taken your research in the physical world into the metaphysical.
1463060	1469040	And there's overtones of Zen Buddhism and a lot of paths that say, if there's no destination,
1469040	1472200	every path is the right path, because it moves you along.
1472200	1476680	In the last part of your book, I just want to read this beautiful sentence of yours.
1476680	1480880	It's called, farewell to the...
1480880	1481880	Farewell to the mirage?
1481880	1482880	That's the chapter.
1482880	1483880	The mirage of object.
1483880	1484880	Right.
1484880	1487980	This mirage that objectives can get you where you want to go.
1487980	1494200	And you say, when all is said and done, even when visionaries grow weary of stale visions,
1494200	1500520	when the ash of unrequited expectation settles on the cloak of the impenetrable future, there
1500520	1504320	is but one principle that may yet pierce the darkness.
1504320	1509080	To achieve our highest goals, we must be willing to abandon them.
1509080	1510080	Yeah.
1510080	1511920	I mean, that sort of recaps the book.
1511920	1514520	That's the concluding paragraph.
1514520	1517720	And it's a difficult pill to swallow.
1517720	1519040	It sounds strange.
1519040	1523080	Giving up objectives is like giving up a security blanket.
1523080	1527360	But I hope that the evidence there is compelling enough to make people think twice about using
1527360	1532080	objectives for almost everything that we do in our culture, because it is going to lead
1532080	1536400	to this kind of ash falling on the cloak of the impenetrable future.
1536400	1537800	Yes.
1537800	1539880	And your book is full of examples.
1539880	1547320	I mean, from history that we all know and understand of people who have given up their
1547320	1551920	stated objective just to find out and discover what was out there.
1551920	1554240	And look what it's done for our world.
1554240	1560560	I must tell you, Kenneth, your book, I think this is a stepping stone.
1560560	1562000	Why greatness cannot be planned.
1562000	1564480	I really urge people to read it.
1564480	1567040	You're in challenging territory here.
1567040	1568520	And I think it's really life-changing.
1568520	1569920	I want people to read your book.
1569920	1573680	And I'm so grateful that you, our guest today, is Ken Stanley.
1573680	1576280	Thank you for taking the time to join us today.
1576280	1577280	Thank you so much.
1577280	1579920	I'm really glad to have this opportunity to get this message out.
1579920	1581000	Yes, yes.
1581000	1585120	And I'd like to thank you, our audience, for being with us today on Report from Santa Fe.
1585120	1586620	I'm Loreen Mills.
1586620	1589560	We'll see you next week.
1589560	1597320	Most archival programs of Report from Santa Fe are available at the website, reportfromsantafe.com.
1597320	1605080	If you have questions or comments, please email info at reportfromsantafe.com.
1605080	1609720	Report from Santa Fe is made possible in part by grants from the members of the National
1609720	1615860	Education Association of New Mexico, an organization of professionals who believe that investing
1615860	1622540	in public education is an investment in our state's economic future, and by a grant from
1622540	1625540	the Healy Foundation, Taos, New Mexico.
