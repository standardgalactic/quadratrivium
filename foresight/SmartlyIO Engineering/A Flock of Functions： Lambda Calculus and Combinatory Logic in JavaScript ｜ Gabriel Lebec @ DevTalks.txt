Okay, hello everybody, thank you.
Really excited to be here.
Quick little about me, I made this talk while I was working at Fullstack Academy as an instructor,
so it's going to hopefully be a little educational.
I currently work at Google, but this talk isn't about that, I'm not representing Google
or anything.
My thoughts and opinions are my own, et cetera.
And this is the smartly version of this talk, so it's been slightly tweaked and improved
in various ways.
And I've got social media accounts and things like that.
There are slides posted online, this will be in the video, slash we'll post it later,
you can go find them, and let's get started.
So I'm going to start off with this combinator called identity, and it looks like this.
It is a function that takes an input A and returns A. It's pretty straightforward, so
let's try it out.
So the identity of one is, yeah, it's interactive, and the identity of identity is identity.
Okay, so right off the bat, we have something cool, which is functions can be arguments
to other functions.
First class functions, and that is a defining characteristic of the lambda calculus slash
combinator logic.
So the identity of any X is X, and we can write that a whole bunch of different ways.
You've got JavaScript arrow functions on the board, we have this I variable that has an
equation that's called combinator logic, and we have this shaded out lambda thing below
it, you'll be able to go see these notes in the slides, you don't necessarily have to
try and read all the actual lambda calculus syntax for this talk.
So I'm not going to be showing it all the time, I'm going to focus on the very high-level
concepts.
And we saw that the identity of identity is itself, so yes, functions can be arguments
to functions and return functions and do all that stuff.
In Haskell, this is built into the base common library that everybody imports usually as
the ID or identity function, and it behaves exactly as shown, the identity of 5 is 5.
So what is up with this lambda notation?
It literally just means function.
If you were writing JavaScript the long way and not with an arrow function, it's the word
function.
If you're writing Python, it's like def function or something like that, I don't know.
And it takes a parameter, it takes an input.
All functions in the actual strict modified lambda calculus take only one argument.
We'll see how that's useful later.
And then finally it returns some expression.
In this case, the identity function returns its input.
And this whole thing together is sometimes called a lambda abstraction, abstraction
because it abstracts away through a parameter, concrete use cases.
So one concrete use case might be if A was 1, this would return 1.
If A was hello, this would return hello.
So by parameterizing those concrete use cases, we end up with an abstraction.
But it's really just a fancy way of saying it's a function.
There's all sorts of syntaxes across programming languages for these things, and you'll probably
have heard about them, and it's like, oh, now Java has lambdas, and why is that exciting?
Turns out there's a lot of different ways of writing these.
This is the worst, unfortunately.
I mean, just compare and contrast, right?
But they are fun.
They are exciting.
And there's a reason they show up a lot in functional programming.
If you're familiar with sort of the COPSI way of showing syntax, this is the grammar
for the lambda calculus.
This is a very tiny language.
It only has really four things.
It has variables, which are just plain old symbols.
It has a function being applied to its argument that's application, and that's just by a space.
A space between two things means function applied to argument.
It has function definitions, which we just saw, and they're called abstractions.
And then it's got parentheses to control what order things are evaluated in.
So this whole talk, I'm going to be comparing and contrasting the way these things look,
but they're really the same concepts.
Variables can hold a value.
Very important difference here.
In the lambda calculus, there's no such thing as a mutable variable.
All variables are immutable.
They have a definition, and you might know that definition, or you might not, but it
never changes.
You never reassign.
All right.
I talked about function applications.
Again, just a space means function called on argument.
We tend to say function applied to argument or function invoked with argument.
And there's little details here.
For example, it's left associative application, so that FAB line right there means first
call F with A, and that returns a function, and call that with B, which we can disambiguate
using those ghosted out parens and say, like, okay, to make this really clear, we're passing
A into F, and then we're passing B into the result of that.
But since it is left associative, we typically just don't write that down.
So you won't see those parens.
In contrast, you'll notice that the last line there, those parens around AB, that does
change the expression.
That is now a different expression.
It means first call the A function with the B argument, and then pass that result into
F. So now we've muddled up the order of things.
Abstractions, these are function definitions.
They're just JavaScript arrows.
That's all they are.
How do they work?
Well, again, you got lambda means function definition.
Dot means return.
So lambda A dot B means taking an A and return B. What do A and B have to do with each other
in this particular example?
Absolutely nothing.
So this throws out the A argument and returns whatever B is.
What is B in this slide?
The answer is we don't know.
We don't know what B is.
It's a free variable.
It's not defined anywhere.
So if you were doing lambda calculus by hand, you would simplify this.
If you called this function on an argument, you'd say it returns B whatever that is.
And that's fine.
You don't have reference errors in lambda calculus.
You end up with a symbol, and that symbol might stand for something or might not.
Anything else of interest on this slide?
Not really.
One maybe perhaps the last thing there.
You'll notice that you can have nested lambdas.
So A returns B returns A is just like if you had serial arrows in a JavaScript expression.
And this is actually right associative.
So associativity goes all over the place of lambda calculus.
That could be one of the confusing things at first.
But that means that bottom function there, if you pass an argument in as A, it doesn't
return the final thing.
It returns a new function that takes a B. And when you call that second function, final
really it returns an A. This is, by the way, known very popularly as curing.
So let's see that again actually.
So that bottom function there, we can see in JavaScript is defined as A takes that, returns
that, and blah, blah, blah.
I didn't bind it as anything, so I can't use it.
But we'll call this example.
That's a classic, right?
So if I call example with some argument, I get back a function, a plain old function.
If I try to call it with two things, this is not going to work exactly the way I expect.
I still get a function back.
So how do I pass in A and B?
Well, since passing in A returns a function, I can immediately invoke that function expression.
Oh my God, if he's there back.
With the second argument.
And now I actually get a result.
So when I say in the Lambda calculus, every Lambda only has one input.
That doesn't mean we can't think in terms of multiple inputs.
We just do so in a kind of funny way where we split up all the inputs into a bunch of
nested return functions.
And that means to actually call a function with multiple inputs, we pass them in successively.
Not all in the same prems, just one after the other.
This gives us all the power that we normally get with JavaScript.
We can pass in both things simultaneously, which I just did.
But it also gives us new capabilities that normal JavaScript function signatures don't
have because I can actually, let's do this, you know what I'm going to do on the next
slide.
I can actually not pass in all my arguments and I get back, instead of nan or garbage
or something, I actually get back like the next step.
Which can await further action.
This is the most mathy complex part of the entire talk.
So get ready.
It's just function evaluation.
What is function evaluation in Lambda calculus?
It's symbol replacement.
So it's called beta reduction, but that just literally means we're going to replace inputs
with their arguments and then rewrite the body.
Example, here's a function in red.
I'm going to underline its argument to make this all very clear.
It comes into this function as its parameter A, and then we go look inside that function
body, find every A parameter, and replace it with the value we're binding.
So this just gets rewritten as its argument.
We just keep doing this.
So we have a function and an argument.
The argument gets bound to the B value here.
So what do we do?
We look at this function body, replace all the Bs with Xs, and that's symbol replacement,
which is all the Lambda calculus is, gives us this expression.
Once again, we have a function and an argument.
This is going to replace the C parameter.
So we go look in the function body for every C value, replace it.
There are no Cs here, so it's easy.
And we end up with a final value.
Now there's no more reducible expressions or red Xs, as they're called.
All these names are ridiculous, I'm sorry.
And so this is in beta normal form, which just means simplified.
There are little caveats here.
If you could do multiple evaluations in different orders, they don't give you different answers
exactly, but one way might give you an answer, and one might be an infinite loop.
So there's little tricks to that.
Another thing is that if you happen to have different functions that coincidentally share
the same variable names, you have to be very careful not to accidentally conflate those
variable names.
And there's algorithms to deal with that.
We're not going to go into them.
Okay, so I promised that I would show, where'd the K comb in there?
I guess I'll do that in a second.
I'm going to show you something after this.
We'll come back to it.
So this is the mockingbird.
What does it do?
It's interesting.
It takes in a value, and then it calls that value on itself.
So if the value is going to be called on itself, it probably should be a function.
So I'm labeling it as f.
Let's try this out.
Takes a function, calls the function on itself.
Weird.
What would the mockingbird of the identity function be?
Well, it takes i, it duplicates i, so this gets simplified to ii, and the identity of
identity we already established is...
Identity.
Yeah, identity.
So it looks immediately very strange, but then we immediately see a really trivial example
of how it can actually work.
The mockingbird of identity is the identity of identity, and the identity of identity
is identity.
Say that three times fast.
What's the mockingbird of mockingbird?
Whoa.
Let's try it out.
Any predictions?
It's a stack overflow.
Why is it a stack overflow?
The mockingbird duplicates things.
So if you duplicate m, we get the mockingbird of mockingbird, but we start it out there.
So this is just an infinite loop.
Yeah.
Too bad.
This actually is too bad because this is what the problem is with Turing completeness
as a concept.
In Turing complete systems, which this is one, you can have infinite loops, and it gets
even worse because you can't even tell if your expression will be an infinite loop.
You just have to try it and see.
Sometimes you can prove it for an individual expression.
You can say, I have a proof that this will loop indefinitely, but there's no machine that
I can feed this lambda calculus string into that will say, oh, be careful, that's an infinite
loop.
You can't do it without running it.
Yeah.
So that's a problem.
This particular expression in lambda calculus is known as omega, and here's the beautiful
thing about doing math and pen and paper instead of with a computer is that I can recognize
I'm entering an infinite loop and say, we'll just name this thing omega and not actually
keep trying to calculate it.
One thing that when I was learning about all this stuff and getting excited about it that
confused me a lot was that there are so many different names for these things.
So here I'm looking at the mockingbird, which is the omega combinator when applied to itself,
which is also written in lambda form.
It's like I was trying to sort all this out, and that's when I started writing this talk.
One little last thing about syntax, and then we get to more juicy stuff, I promise.
We can write things in this curried form, these nested lambdas, but it's hard to read,
right?
Like A returns a function that takes a B, returns a function that takes a C, that returns
B. There's a shorthand.
The shorthand is any nested lambdas, we just squish them all together and put them to the
left of the dot.
So we see that lambda ABC, it's the exact same thing, we haven't changed the meaning
of it, we're just writing it in a shorthand.
And then when you go and do this simplification and evaluation step, you just have to be very
careful to notice that.
So here we have a function that takes a B and C and returns a B. Or is it?
Remember it's that syntax shorthand, so really this is a function that takes B and returns
a new lambda that takes C. So anytime you've got multiple parameters between that lambda
and that dot, just remember there's a bunch of lambdas hidden, sandwiched between them.
And when you do this simplification, they have to pop back out.
Congratulations, you know the lambda calculus.
It talks over.
No.
All right, here's the part I was trying to get to earlier.
The kestrel, this is one of my favorite combinators.
How does it work?
It takes an A and a B and it returns A. So it takes two things and returns the first
one.
Wait, I already did that, but I'll rewrite it.
The k-combinator, let's try it out.
So I'm going to pass in two things, two symbols, arbitrary symbols, they don't mean anything.
You've never seen them before.
And look, it gave me back the first symbol, whatever it was.
I could do this if I really wanted.
So this is JavaScript really being symbolic.
And of course, I can pass in whatever and wait, I'll pass in K. And what should this
give me back?
So the k-combinator takes two things, returns the first one.
So I get back my first symbol and there actually are symbols in JavaScript now, which makes
this even better.
But why do I care about defining it in this one argument only nested lambda way?
This is the reason.
The k-5 function is if I call k with just one of its two arguments, not both of them.
Oh, it's a function.
What the heck is the use of that function?
Well, if I call it on something else, it gives me back five.
What if I call it back on something completely different?
It gives me back five.
What if I call it on undefined?
It gives me back five.
The k-5 function or the constant five function is a function that is now stuck on and obsessed
with this value.
I can't get this function to return anything else.
Turns out that's useful for the same reason things like zero in a number system is useful.
Sometimes you need a function that does very little because you have an API that's like,
you must give me a function here.
And like, I don't want to give you a function, you're already doing everything I want.
So the identity function and the k-combinator, these things start to form very primitive,
almost trivial building blocks that slot into larger, more complicated systems.
So k of two things returns the first one.
If you swap them around, you still get the first one, whatever that is.
And in Haskell, this is called const, which isn't confusing for mainstream imperative
programmers at all.
It is a function.
The k-combinator takes two things, returns the first one.
Let's expand on this.
If we've got this k-combinator and we pass in two things, we get the first one.
You're getting it.
But wait a second.
This is math.
This isn't really programming, quite.
So this is a quality sign.
It's not an assignment operator.
This is a quality in the truest sense of whatever that means in math, which by the way is a
very deep topic that we wouldn't touch on.
What is the nature of equality?
People don't agree.
Anyway, so the thing on the left and the thing on the right are in theory the same thing.
But the thing on the right is a function.
It's the identity function.
That means I can call that function on a value.
Interesting.
But we know the identity of y, right?
We already know what that actually evaluates to.
That's y.
Huh.
So this function on the left, k, that I talked about taking two things, I've kind of passed
three things to, i, x, and y, and I actually get a result out of that.
That's weird.
I thought I'd define this function as only taking two inputs, but now I'm using it with
three.
This is the kind of weird flexibility and power that currying gives you.
When your functions are constantly returning other functions that return other functions
that return other functions, you can kind of chop things off early or extend them far
beyond what they were originally being used for, and they still do things, and those things
might end up being useful.
Why is this useful?
Because check this out.
The first two values there, ki, are a function being called on x and y, and they return the
second of those two things.
I start with k, returns the first of two things.
ki is a new function out of the ether that returns the second of two things.
I'm not hearing all the amazement I expected out of that.
Yeah, thank you.
This is called the kite.
So it's also easy to define.
I could do it the way I've been doing it, and say ki of two things returns a second.
There's the proof.
I could also define this as calling k on i, and now I can rename it, and it just looks
like a normal function, and it passes two things, and it gives me the second.
But what's really amazing about this is I never sat down and wrote out, look, ki takes
a, and it takes b, and it returns b.
I took two existing functions, neither of which talks about the second value of any
two inputs.
Just by smashing them together in this sort of atom-to-molecule way, it gave birth to
a new function with new behavior that I didn't manually specify.
So ki of m and k returns the second thing, ki of k and m returns the second thing, and
things are behaving more or less as we expect at this point.
What's with all the bird names?
Where's this coming from?
So there is a mathematician who had a brilliant short career, very sad ending to it, named
Schoenfinkel.
I can't pronounce German.
I'm sure many people in this room can pronounce German much better than I am, but he named
these things very long German names, like Sutsumetsetsungsfunktion, and then he shortened
them because it was annoying to write those down into things like i and k.
And then an American mathematician, Haskell B. Curry, studying this stuff, used many of
the same letters, switched some of the round just to confuse everyone who came later.
And then a logician and puzzle author whose books I really enjoy called Raymond Smullyan
wrote this book called To Mock a Mockingbird.
And to mock a mockingbird, it is all a extended metaphor of birds in a forest that call the
names of other birds, which give birth to birds, and you get where this is going.
It's the same thing.
He just took all those letters and he expanded them out into bird names for his metaphor.
By the way, if you try to learn commentary logic through Smullyan, I mean, you might
make some progress, but it is a puzzle book.
It's meant to be confusing.
So that's just fair warning for anyone who's like, I'm going to go buy the book now.
It is a good book.
You should check it out, but it's not meant to sort of be easy to follow.
Also, he called it the idiot bird.
I don't know.
It should have been Ibis.
Anyway, this was done in tribute to Curry, the other mathematician, because Curry was
a bird watcher.
And Haskell B. Curry, why do we care about this guy, and oh my god, okay, let's go talk
about history.
This is going to be the super fast version of this because I'm trying to stay under time.
But in the early 20th century, actually even late 19th century, mathematicians were trying
to figure out what is math?
And there are all these different systems that proceed from different axioms.
So what's the one true axiomatic system of math from which all of the things can be proved?
And it's nice and simple and intuitive looking and clean.
So Peano comes up with this thing called Peano arithmetic, which is like, I'm going to find
the numbers.
There's a thing called zero.
And one is the thing that comes after zero.
And two is the thing that comes after the thing that comes after zero.
And this sounds super trivial, but it actually forms a really sound foundation for very large
and complicated branches of math.
Frege, who's unfortunately not as well known as he should be, does amazing work in developing
concepts of combinatorial logic and functions and invent its own function notation, which
is in an abstract syntax tree, which by the way, it would have been so much easier to
simplify for people, but it doesn't type well, obviously, right?
Like if you're typesetting, you need a linear system.
And his was two-dimensional.
And he uses currying, although he doesn't draw attention to it.
And he develops a ton of work in axiomatic, quantified logic.
Things like for all x's in the real numbers, there exists a y such that y is greater than
x.
Those things like for all, and there exists, a lot of that originates with Frege.
1910, Russell and Whitehead famously published Principia Mathematica, which is another big
attempt at unifying all of math, a great grand unified theory of math.
Unfortunately, along the way, Russell discovers a paradox, an underlying flaw in set theory,
as it was known at the time, where the question of what about the set that contains all sets
that do not contain themselves or something like that?
I can't remember the details.
And he's like, wait a second.
Does that set contain itself or not?
You can't answer the question.
It's a nonsensical question.
This was a giant problem because tons of preexisting work was based on that version of set theory,
which is now shown to be inconsistent.
It didn't actually hold together.
So this was like the first big crack in the foundations of math, where people started
saying maybe this stuff is harder than we thought it was, because now we're trying to
be extra super-duper careful.
Schoenfinkel, 1920, does tons of work on combinatorial logic.
Now I'm talking about combinatorial logic, lambda calculus, JavaScript arrow functions.
What's the difference between all these things?
Well, we'll clarify that more throughout, but any time I'm using those letters like
k and s and i and m, that's combinatorial logic, and it's the study of functions that
act on each other.
And any time I'm showing you lambdas with lambda a dot b, et cetera, that is lambda calculus.
Turns out they overlap by like 99%.
They study functions, first-class, higher-order, anonymous functions.
Actually, the combinatorial logic ones are not anonymous, by definition.
But that's almost all the differences.
One of them, you give them names, and one of them, they're anonymous.
Van Neumann also doesn't work on this one.
I'm going to skip it.
Curry comes along and does a ton of this work, not knowing about Schoenfinkel, and then he
discovers all of Schoenfinkel's work.
He's like, oops, this guy already did it all.
But he keeps going, and he comes up with a lot of really great results as well.
By the way, now we know this thing that you split up the parameters as currying.
Okay, yeah.
But it really should be called Schoenfinkelization.
In 1931, Kurt Gerdel, or maybe fragification, I don't know, Gerdel now really upends all
math and just destroys all math forever with his incompleteness theorems, which prove that
any system that's sufficiently complicated and interesting is either inconsistent, meaning
it's flawed and actually nonsensical, or it is incomplete, which is the lesser of two
evils but still very disturbing for a mathematician.
This means you can talk about some possible thing that might be true or false in math,
but there might not ever be a proof of whether it is true or false.
But it is true or false.
You just can't find out.
It sounds like the most horrifyingly impossible thing to a mathematician, especially of that
time period, where math was seen as the language and nature of reality, that if you were working
on an unsolved problem, surely if you were just smart enough or determined enough or
had good enough teammates, you could all figure it out together.
Now Gerdel was coming along and saying, nope, you might never find the answer because it
might be impossible to find out the answer.
That was terrible.
It was really terrible.
By the way, a fun philosophy point.
People use this along with Einstein and general relativity in the 20th century to say that
the nature of reality is fluid and abstract and unknowable and things like that.
This really upset Gerdel because he viewed the incompleteness theorem as a universally
controversial fact about the universe.
He was really excited about this result because he's like, I've finally found something that
is always true and everyone else is like, oh my God, you've destroyed math.
Another thing, he works on general recursion theory, which is another big system trying
to unify all of mathematics.
Around the same time period, a Hungarian mathematician much less well-known, unfortunately
than Gerdel, is Rosa Pater and she is known as the mother of recursion theory.
She does a huge amount of recursive function theory development and work in papers for
decades, so I'm glad to be able to point that out.
Now we get to really what's at the subject of this.
The thread here has been all these people, very smart, are trying to figure out what
is math, what is the nature of math, how do we define it.
Now Alonzo Church comes along and defines this thing we've been looking at called lambda
cacons.
It's a little tiny language.
That's it.
It's just a notation system for symbol replacement.
That's what a calculus is, by the way.
A calculus is any formal system of rewriting symbols on a page and this is one for functions.
It turns out that that tiny little language is beautiful because of its simplicity, which
makes it easier to prove things about it than competing theories at the time, like
general recursive theory, which is more confusing to read, in my opinion.
Don't take that as gospel.
He's got some grad students, Stephen Claney goes on to invent regular expressions later,
but also these grad students help prove various things about lambda cac.
Then this is the brilliant shining moment where lambda calculus proved its worth.
These tiny little four lines of grammar and syntax were used to solve a giant unsolved
problem of the day called the decision problem, David Hilbert's decision problem, which was,
is there a machine that you could write, an algorithm you could design, that I could
feed into it some math expression and it will tell me if it will run into an infinite loop?
The answer to that is no, there is no such machine.
We already talked about that.
But church, what's interesting about this is that there's no such machine, but rather
that church used lambda calculus, which seems so tiny and trivial, to prove that result.
He published a paper and about a month later, Turing published another paper solving the
same thing with something much more famous called Turing machines.
Turing was a little upset that it turned out that the idea that he had been working on
for years, I think, he just never got around to publishing, he finally did it and someone
else had published a month earlier and yeah, sometimes that's the way it goes.
But he goes and looks at this and he's like, wait a second.
My Turing machines, which is this like stateful, imperative, sequential program instructions
and this lambda calculus thing, which is an expression that you just do simple replacement
and it simplifies, even though they look very different, I can write a lambda calculus interpreter
in my Turing machine and I can take a lambda calculus and use it to implement a Turing
machine simulation.
So wait a second, if there's something in a Turing machine, I can do it in lambda calculus
because I just use lambda calculus to simulate Turing machines and vice versa, therefore
these two systems are really just as powerful.
Anything one can do, the other can.
So they seem just like amazingly, even though they're completely independent schools of
thought and they're both trying to define what math is and all this business, they seem
to have accidentally discovered equally powerful systems for computing things.
So the church Turing thesis or hypothesis is not a fact.
The hypothesis is that these systems define computation in some deep philosophical way
that's undefinable.
So you can never prove it because it's not well defined.
The fact that lambda calculus interpreter machines are equally powerful is not the church Turing
thesis that's a common mistake.
So he's really excited about this.
He actually combs over to Princeton, gets a PhD under Alonzo Church, helps define like
new fixed point combinators and things like that.
What is a combinator?
I've said this word a couple of times, again, it's like all these different names for things.
Combinators are just functions with no free variables.
Remember I mentioned a free variable is one where you don't know what it's bound to, it's
not bound to anything, it's just kind of in space.
You end up with it and it's like, what does that mean?
I don't know, it's the symbol B. Combinators don't have those.
So all of their outputs are bound as inputs somewhere in the expression.
So the definition of B will come from somewhere when you use it.
It will not be undefined.
In other words, combinators can never have reference errors because their values are
all based on their inputs.
That's all they could do is remix their inputs.
And combinatorial logic, full circle now, is the study of those kinds of functions,
functions which act on each other in surprising ways.
We've seen a bunch, I, M, K, K, I, et cetera.
And again, this gets it like there's all these different names.
There's the Mockingbird versus Zem versus Lambda F to FF.
You can't actually define that one in Haskell, by the way.
Not simply because Haskell's type system doesn't like infinite types and the Mockingbird
is all about infinity.
There's a trick.
You can do it in Haskell through like a little type hack, but it's not beautiful.
Let's look at this one, the cardinal.
The cardinal is a very lovely combinator.
Takes three things, spits them back out in different order.
So let's say the first thing's a function and the second two things are inputs.
C combinator takes a function and A and B inputs and it calls the function with the
inputs reversed.
It flips the inputs around.
That's interesting.
What will we use or use that for?
Well, let's see examples of it just to make sure we're on the same page.
If I feed in a function like K into arguments like I and M, the K combinator returns the
first thing, so the flip of the K combinator returns the second thing.
Well, that's kind of cool.
Sometimes because of this couriering business, your inputs and your arguments don't have
quite the order you wish they did to make some of these tricks easier.
And so this flip combinator or cardinal or C combinator can rewire things for you so
that they take arguments in different orders.
And C of K takes two things and returns the second.
That sounds super familiar, right?
Flipping K gives you Ki.
They are the same function.
C of K and K of I are the same function.
They behave identically.
For every input, they give you the same output.
And when two functions that have been defined separately have nonetheless identical input
output behavior, they're called extensionally equal.
Extensional in the sense that from the outside, you can't see the guts of the function or
how it came into being.
All you can do is throw things in and see what comes out.
There's no way to distinguish those functions.
There's another kind of equality called intentional equality, which is more squirrely and it has
to do more with the internal guts.
We're not going to care about it.
So there's another beautiful combinator, the C combinator.
And in Haskell, it's called flip.
And if you've ever played around with Haskell and talked about point-free programming, flip
shows up a lot generally in a way that makes things much more confusing than they have
to be.
There's a cool site out there called pointfree.io that simplifies Haskell expressions, simplifies.
It really doesn't.
It shortens Haskell expressions to the minimal version that uses the fewest ingredients and
you'll just see tons of flip and ID and const and it's completely unreadable for humans.
Sometimes you get lucky and you get something that actually is simpler and easier, but flip
is one of these things that is very heavy in the rewiring that's possible.
Okay, I've talked about this for like 35 minutes.
Who cares?
What is the point?
We don't care about any of this.
Well, let's find out.
I talked about lambda calculus and Turing machines.
Why are Turing machines exciting?
Simple.
What?
Simple.
Simple.
They are simple.
So is lambda calculus.
They're both simple.
They're both so simple that it's almost hard to do anything with either of them, right?
Like if you've ever hand coded a Turing machine to do like anything interesting, it's very
complicated.
You have to write a one, write a zero, read something, go to memory address.
Like that's almost assembly.
And that's really why Turing machines are cool is because if a hypothetical machine
can calculate whatever you want it to, well, why not build real physical machines that
do the same thing?
So from Turing machines, we get the idea of let's make actual computers that can do this.
And those computers have to be programmed in this very low level on off switch state
version of programming called machine code.
But that's really hard for humans.
So now let's make a text representation called assembly of those very low level machine instructions.
But thinking in terms of machine instructions and assembly is still hard.
So now let's make high level textual languages that get translated into machine code and
aren't one to one correspondence and give you beautiful new human centric concepts.
Things like loops and variable assignment and stuff like that, which you don't get in
the same way for this low level stuff.
But those languages are still maybe saying, like, you tell me what memory to reserve and
what blocks and how much of it and how to index into it and things like that.
And someone out there is like, I'm sick of saying, you know, pointer to something.
Can't we just have a language where I say var x equals object and it'll go figure out
all that memory stuff for me?
So now we're going even further away from the machine and more towards the concepts
humans care about, which are, I've got a domain, I'm modeling something, I have ideas
that I want to bandy about.
And then we go even further and we say, like, wait, why do we even have state?
Why have variable mutation and reassignment?
That has to do with the way the machine physically works.
But we don't need that.
And if we take it out, some of our programs get a lot more typesafe.
And this whole time, the layers of languages that have been building up and people have
been inventing have been gradually moving away from the super imperative machine-based
way of thinking toward a more pure conceptual way of thinking.
And someone out there says, wait a second.
If lambda, cacos, and trig machines are equivalent, what if we went the opposite
direction?
What if we started with something that has absolutely no concept of a machine but is
still able to compute things and try to make it useful, try to make it work on a Turing
machine?
We know that it's possible.
Now the question is, can we optimize it?
Right?
Like, can we take this pure stateless functional way of thinking and make it perform well in
terms of memory and speed and stuff like that?
So we'll write these pure functional languages and we'll have compilers that have been very
cleverly written to develop good machine code from them.
These things are equivalent, which means everything can be functions, literally everything.
But not everything should be.
But more than you might think.
Such as bullions.
We're going to invent bullions from scratch now.
Let's do it.
Well, it's a problem because we don't have any of those things.
We just have parentheses.
So far, that's all we've got in our language and our parentheses.
We don't have equal signs.
We don't have the not operator.
We don't have ors.
We do have kind of variables, but the variables aren't bullions.
Like there's no concept of true-false.
Okay.
What are bullions used for?
A lot of things.
One thing they're used for is selection.
If some condition, then result, else different result.
Well, I've got two possible results and I'm selecting which one I want to use.
So if it's the true bullion, give me back the first expression.
And if it's the false one, give me back the second expression.
Huh.
Well, we don't have this syntax.
We don't have question marks and colon, so we'll just knock those out.
And then we have space.
So that means function application, right?
So whatever this thing, it has to be a function.
And I need two functions, one for true and one for false.
The true one will select the first expression and the false function will select the second
expression.
We've already seen this.
It's the k and ki functions.
Surprise, I already did it.
So we'll just go up in here and we'll now say, so if I log out the k function, by the
way, it says function k and node, I cheated ahead of time and I did load one tiny line
of functional stuff in here, not functional, but one tiny line of code that's not lambda
cacos or combinatorial logic, and that's to rewrite function names in node, which is
slightly annoying in node version 12.
But let's label this thing k slash true.
Ah, reference error.
Inspect is not defined.
Oh, my God.
Okay.
I reloaded this.
You know what?
We'll just not do it.
You'll all just remember that k is true.
See, this is what happens when you try to do it.
You try to go outside of pure math, right?
This is just a mess.
The ki function, unfortunately, this function is anonymous.
It doesn't have a name because I developed it by smushing together k and i.
So I'm going to quickly rewrite it manually, even though that's sad.
Great.
So anytime you see k, you can now think to yourself, oh, that's true.
And every time you see ki, you can think to yourself, that's false.
It's like if you see 1 and 0, you think true or false, or on and off you see true or false,
or true is true and false is false.
Now the k function is true and ki is false.
It's just a representation.
But if I have some condition like unknown bool is, quick, don't look.
No one saw that.
Well, which one is it?
I can find out by passing it two things.
If it was true, I get back the first thing.
Oh, I guess unknown bool was the true function.
So I could do everything I wanted, just like already out of the box.
All right, but that's not that interesting, right?
What's another thing we do with Boolean logic all the time?
We negate.
We actually transform things around.
The not operator.
Well, again, we don't have exclamation points, so we're going to get rid of that.
It becomes a function called not.
How do we define it?
If p is true, return false.
And if p is false, return true.
Well, we need to select between two possibilities.
The flip combinator is going to do this for us.
I've already defined not.
You didn't even realize at the time.
The flip combinator, the flip of k is ki and the flip of ki is k.
Which means the flip of true is false and the flip of false is true.
So not is just the c combinator.
Let's try it.
The flip of k is some function, but which one is it?
It's the false function.
It returns the second thing.
And the flip of ki, or false, is the true function.
So it returns the first thing.
We can also rename these like t is that and f is this.
So now we can say things like not true.
Which one is that?
Is that true or is it false?
It turns out not true is false, and it turns out not false is true.
And we just invented this out of the ether.
These are called church encodings.
Alonzo Church was one of the people who helped invent them.
And it's like, OK, we'll represent these things as functions.
And then we can do functions on them that transform them
and move them around.
There's other ways of doing them.
I've written one up there, like this lambda expression
is another way of defining the not function.
Let's move on and do and in the interest of time, of which
there is very little left.
The and function takes two booleans.
OK, so how does this work?
If both p and q are true, return true.
For anything else, the and function return false.
So we're taking in two booleans, p and q.
And remember, these are functions.
p and q are both functions.
So what can we do with those functions?
Well, we can use them to select things.
The first boolean input is going to select
between two expressions.
If p is false, it's going to return the second thing.
That's what the ki function does.
But if p is false, what should and return?
One of my booles is false.
So and should return false.
So p should return false.
And if p is true, what should and return?
Q, exactly.
Because it depends what q is, right?
If q is false, well, we should return false.
But if q is true, p was true and q is true.
So return q, which is true.
And hey, it all works.
And in fact, you could simplify that too.
If p is false, return p, which is false.
Let's try it.
Takes two unknown booleans.
And then if p is true, return whatever q is,
because we have to check it as well.
If p is false, well, we know p is false.
So just return p, because they're not both true.
And guess what?
I have to prove that it's actually, it's some boolean.
Oh, it actually worked that time.
So and true, true is that.
And true false is false.
And false false is false.
And false true is false.
Hey, we invented and boolean logic.
You can do the same thing with or.
It's just like slightly different order.
You say p returns p and q and things like that.
What is this?
This one's a little bit more dense.
And by the way, functional languages,
that function application has just a space.
Look how much easier it is to read when you don't have
all these parens for function evocation.
Like, I think that JavaScript is actually
harder to understand than the lambda calculus.
But if you want a little help, p selects
between two different things, depending
on whether it's true or false.
q selects between two different things,
depending on if it's true or false.
And the behavior of this unknown function,
this anonymous lambda, is if both p and q are true
or if both p and q are false, return true.
But if they're different from each other, return false.
So this is boolean equality.
And we can simplify it through some steps
that you can go review later in the slides.
It actually turns into this, which I'll do right now.
Boolean equality takes a p and a q,
and it calls p on q and not q.
And now I can test, hey, do true and false equal each other?
No.
Do false and false equal each other?
Maybe that's some function that just got
birthed out of nothingness, so let's check.
Yeah, it's the true function.
It's the one that returns the first thing.
And I'm gonna keep moving, because I'm almost out of time.
So we've got these church encodings, and they're lovely.
Hey, here's De Morgan's law.
It's a thing from logic.
It says not p and q is equal to not p or not q.
And now using just a lambda syntax anonymous functions,
we have this beautiful, concise, easy to read way
of expressing this logic.
The equality of not and pq or not p, not q
is always gonna be what you expect it to be.
Feed two booleans into this thing,
and by booleans I mean either k or ki,
and you get out what you would expect from boolean logic.
What else can we do with this kind of fun stuff?
We can invent numbers.
And if we got numbers, actually specifically
positive integers, actually specifically natural numbers,
zero and up.
If we've got those, well, a pair of numbers is fractions,
but we don't have pairs yet, so we can do arithmetic.
And we're gonna need some data structures.
So we'll invent those from scratch using just functions.
And once we have data structures
in arithmetic and things like that,
we can do data types and type systems.
And once we've invented some of that stuff,
we can also invent recursion from scratch.
Lambda calculus does not have named functions.
The c and k and i, that's commonest very logic.
We often refer to these functions by their names
because it's way easier than saying lambda, af,
ad, et cetera.
But lambda calculus itself does not have named functions.
In fact, it's deliberately designed without them
to make it easier to prove things about math
and the way things work.
So how do you do recursion in lambda calculus?
Well, sorry, you're gonna have to see part two
in the video online.
Yeah.
We're almost done.
Question.
So some fun little things at the end here.
How many combinators like c and k and stuff
do you think you need to cover every possible computation
to be able to do anything?
Because in combinatory logic,
you had to define all those functions ahead of time.
In lambda calculus,
you could define a function manually using syntax
whenever you want, but in combinatory logic,
you need to pre-define all these named functions.
So do you need 20 of them?
10, five?
Is it even possible?
Maybe you need an infinite number.
Well, here are two, the s and k combinator.
They behave together in all sorts of very clever ways.
And for instance, the i-combinator,
you can invent it by calling s on k of k.
And the v-combinator, which you didn't see,
it's in part two, it's a data structure, it's a two-tubble.
You can do by this nonsense.
But it gets even smaller.
You can do everything with a single function.
So if you really wanted to go crazy,
you could take this iota function
and write any other function in existence
using just this and parentheses.
But why?
So what is the point of this entire talk?
Like, why am I here talking about this?
The short answer is I just really like it,
and I hope that you've enjoyed it.
It's kind of a game, like I read about this stuff
in Smollion's book, which was deliberately
supposed to be entertaining.
It's logic puzzles and games,
and things to exercise your mind and think about.
It's also really great practice for thinking functionally,
not because in a functional programming language,
you have to invent Booleans from scratch,
although often you do.
Not quite in that way, though.
But just the act of doing all these function evaluations
with symbol manipulation and substitution
and evaluating things and seeing the power
of functions operating on each other
makes that sort of foundational knowledge
of working in a functional language
just feel much more comfortable.
It's a form of exercise of calisthenics.
Programming languages like Haskell and PureScript
and Agda, I guess, I don't know,
and Elm and other things.
Their basis and their core are lambda-calculi,
slightly souped up ones like System F and System FC
and things which add types and all sorts of stuff.
But you know how Haskell actually works?
It's great.
You write Haskell.
The compiler takes that and simplifies it down
to a slightly souped up lambda-calculus.
And then it pairs that lambda-calculus program
that was from what you originally wrote
as like a sane thing with a runtime written in C
that's a lambda-calculus interpreter.
And that's how Haskell works.
That's why the minimum size for any Haskell program
is like eight megabytes or something
because it ships with this lambda-calc interpreter
that runs your code.
I think personally though, at the end of the day,
like I don't wanna have to defend or justify
and say like you should all learn lambda-calculi
because it'll let you do this thing in your job.
It might be able to.
There's a couple cool examples I can think of
that I've now thought of after the fact.
But for me, it's like pure art for art's sake.
And I just think that's like a lovely thing
to find in programming and computer science.
And yeah, there's the Y Combinator.
That invents recursion.
You can go see about it later.
So there's a whole bunch of slides and stuff in here
for reference.
These are all part two and then yeah, there's resources.
And I guess seven minutes over time, that's not too bad.
Let's do questions.
Thank you.
