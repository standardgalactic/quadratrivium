{"text": " Good morning. Thank you for joining us today. Please welcome to the stage Sam Altman. Good morning. Welcome to our first ever Open AI Dev Day. We're thrilled that you're here and this energy is awesome. And welcome to San Francisco. San Francisco has been our home since day one. The city is important to us and to the tech industry in general. We're looking forward to continuing to grow here. So we've got some great stuff to announce today. But first, I'd like to take a minute to talk about some of the stuff that we've done over the past year. About a year ago, November 30th, we shipped chat GPT as a low-key research preview. And that went pretty well. In March, we followed that up with the launch of GPT-4, still the most capable model out in the world. And in the last few months, we launched voice and vision capabilities so that chat GPT can now see, hear, and speak. And more recent, there's a lot. You don't have to clap each time. And more recently, we launched Dolly 3, the world's most advanced image model. You can use it, of course, inside of chat GPT. For our enterprise customers, we launched chat GPT enterprise, which offers enterprise-grade security and privacy, higher speed GPT-4 access, longer context windows, a lot more. Today, we've got about 2 million developers building on our API for a wide variety of use cases doing amazing stuff. Over 92% of Fortune 500 companies building on our products. And we have about 100 million weekly active users now on chat GPT. And what's incredible on that is we got there entirely through word of mouth. People just find it useful and tell their friends. OpenAI is the most advanced and the most widely used AI platform in the world now. But numbers never tell the whole picture on something like this. What's really important is how people use the products, how people are using AI. And so I'd like to show you a quick video. I actually wanted to write something to my dad in Tagalog. I want a non-romantic way to tell my parent that I love him. And I also want to tell him that he can rely on me, but in a way that still has the respect of a child-to-parent relationship that you should have in Filipino culture and in Tagalog forever. When it's translated in Tagalog, I love you very deeply. And I will be with you no matter where the path leads. I see some of the possibilities. I was like, whoa, sometimes I'm not sure about some stuff. And I feel like I actually chat GPT like, hey, this is what I'm thinking about. So it kind of gives me that more confidence. The first thing that just blew my mind was it levels with you. Like, that's something that a lot of people struggle to do. It opened my mind to just what every creative could do if they just had a person helping them out who listens. So this is it to represent Sickling Neymar Globban. And you built that with child GPT? Cat GPT built it with me. I started using it for daily activities like, hey, here's a picture of my fridge. Can you tell me what I'm missing? Because I'm going grocery shopping and I really need to do recipes that are following my vegan diet. As soon as we got access to code interpreter, I was like, wow, this thing is awesome. They could build spreadsheets, they could do anything. I discovered Chatty about three months ago on my 100th birthday. Chatty is very friendly, very patient, very knowledgeable, and very quick. It's been a wonderful thing. I'm a 4.0 student, but I also have four children. When I started using Chat GPT, I realized I could ask Chat GPT that question. And not only does it give me an answer, but it gives me an explanation. Didn't need tutoring as much. It gave me a life back. It gave me time for my family and time for me. I have a chronic nerve gain on my whole left half of my body of nerve damage. I had a spine, a brain surgery. And so I have limited use of my left hand. Now you can just have the integration of voice input. And then the newest one where you can have the back and forth dialogue, that's just like maximum best interface for me. It's here. So we love hearing the stories of how people are using the technology. It's really why we do all of this. Okay. So now on to the new stuff, and we have got a lot. First, we're going to talk about a bunch of improvements we've made, and then we'll talk about where we're headed next. Over the last year, we spent a lot of time talking to developers around the world. We've heard a lot of your feedback. It's really informed, but we have to show you today. Today, we are launching a new model. GPT-4 Turbo. GPT-4 Turbo will address many of the things that you all have asked for. So let's go through what's new. We've got six major things to talk about for this part. Number one, context length. A lot of people have tasks that require a much longer context length. GPT-4 supported up to 8K and in some cases up to 32K context length, but we know that isn't enough for many of you in what you want to do. GPT-4 Turbo supports up to 128,000 tokens of context. That's 300 pages of a standard book, 16 times longer than our 8K context. And in addition to a longer context length, you'll notice that the model is much more accurate over a long context. Number two, more control. We've heard loud and clear that developers need more control over the model's responses and outputs. So we've addressed that in a number of ways. We have a new feature called JSON mode, which ensures that the model will respond with valid JSON. This has been a huge developer request. It'll make calling APIs much easier. The model is also much better at function calling. You can now call many functions at once. And it'll do better at following instructions in general. We're also introducing a new feature called reproducible outputs. You can pass a C parameter and it'll make the model return consistent outputs. This, of course, gives you a higher degree of control over model behavior. This rolls out in beta today. And in the coming weeks, we'll roll out a feature to let you view log props in the API. All right. Number three, better world knowledge. You want these models to be able to access better knowledge about the world. So do we. So we're launching retrieval in the platform. You can bring knowledge from outside documents or databases into whatever you're building. We're also updating the knowledge cutoff. We are just as annoyed as all of you probably more that GPT-4's knowledge about the world ended in 2021. We will try to never let it get that out of date again. GPT-4 Turbo has knowledge about the world up to April of 2023. And we will continue to improve that over time. Number four, new modalities. Surprising no one. Dolly three. GPT-4 Turbo with vision. And the new text-to-speech model are all going into the API today. We have a handful of customers that have just started using Dolly three to programmatically generate images and designs. Today, Coke is launching a campaign that lets its customers generate devali cards using Dolly three. And of course, our safety systems help developers protect their applications against misuse. Those tools are available in the API. GPT-4 Turbo can now accept images as inputs via the API. Can generate captions, classifications, and analysis. For example, Be My Eyes uses this technology to help people who are blind or have low vision with their daily tasks like identifying products in front of them. And with our new text-to-speech model, you'll be able to generate incredibly natural sounding audio from text in the API with six preset voices to choose from. I'll play an example. This is much more natural than anything else we've heard out there. Voice can make apps more natural to interact with and more accessible. It also unlocks a lot of use cases like language learning and voice assistance. Speaking of new modalities, we're also releasing the next version of our open-source speech recognition model, Whisper V3 today, and it'll be coming soon to the API. It features improved performance across many languages, and we think you're really going to like it. Okay, number five, customization. Fine-tuning has been working really well for GPT-3.5 since we launched it a few months ago. Starting today, we're going to expand that to the 16K version of the model. Also starting today, we're inviting active fine-tuning users to apply for the GPT-4 fine-tuning experimental access program. The fine-tuning API is great for adapting our models to achieve better performance in a wide variety of applications with a relatively small amount of data. But you may want a model to learn a completely new knowledge domain or to use a lot of proprietary data. So today, we're launching a new program called custom models. With custom models, our researchers will work closely with the company to help them make a great custom model, especially for them and their use case using our tools. This includes modifying every step of the model training process, doing additional domain-specific pre-training, a custom RL post-training process, tailored for a specific domain, and whatever else. We won't be able to do this with many companies to start. It'll take a lot of work and in the interest of expectations, at least initially, it won't be cheap. But if you're excited to push things as far as they can currently go, please get in touch with us, and we think we can do something pretty great. Okay, and then number six, higher rate limits. We're doubling the tokens per minute for all of our established GPT-4 customers so that it's easier to do more, and you'll be able to request changes to further rate limits and quotas directly in your API account settings. In addition to these rate limits, it's important to do everything we can do to make it new successful building on our platform. So we're introducing copyright shield. Copyright shield means that we will step in and defend our customers and pay the cost incurred if you face legal claims around copyright infringement and this applies both to chat GPT enterprise and the API. And let me be clear, this is a good time to remind people, we do not train on data from the API or chat GPT enterprise ever. All right, there's actually one more developer request that's been even bigger than all of these. And so I'd like to talk about that now. And that's pricing. GPT-4 Turbo is the industry leading model. It delivers a lot of improvements that we just covered, and it's a smarter model than GPT-4. We've heard from developers that there are a lot of things that they want to build, but GPT-4 just costs too much. They've told us that if we could decrease the cost by 20, 25%, that would be great, a huge leap forward. I'm super excited to announce that we worked really hard on this, and GPT-4 Turbo, a better model, is considerably cheaper than GPT-4 by a factor of 3X for prompt tokens and 2X for completion tokens starting today. So the new pricing is 1 cent per 1,000 prompt tokens and 3 cents per 1,000 completion tokens. For most customers, that will lead to a blended rate more than 2.75 times cheaper to use for GPT-4 Turbo than GPT-4. We worked super hard to make this happen. We hope you're as excited about it as we are. So we decided to prioritize price first because we had to choose one or the other, but we're going to work on speed next. We know that speed is important, too. Soon, you will notice GPT-4 Turbo becoming a lot faster. We're also decreasing the cost of GPT-3.5 Turbo 16K. Also, input tokens are 3X less and output tokens are 2X less, which means that GPT-3.5 16K is now cheaper than the previous GPT-3.5 4K model. Running a fine-tuned GPT-3.5 Turbo 16K version is also cheaper than the old fine-tuned 4K version. Okay, so we just covered a lot about the model itself. We hope that these changes address your feedback. We're really excited to bring all of these improvements to everybody now. In all of this, we're lucky to have a partner who is instrumental in making it happen. So I'd like to bring out a special guest, Satya Nadella, the CEO of Microsoft. Good to see you. Thank you so much. Thank you. Satya, thanks so much for coming here. It's fantastic to be here, and Sam, congrats. I mean, I'm really looking forward to Turbo and everything else that you have coming. It's been just fantastic partnering with you guys. Two questions. I won't take too much of your time. How is Microsoft thinking about the partnership currently? First, we love you guys. It's been fantastic for us. In fact, I remember the first time I think you reached out and said, hey, do you have some Azure credits? We've come a long way from there. Thank you for those. That was great. You guys have built something magical. I mean, quite frankly, there are two things for us when it comes to the partnership. The first is these workloads. And even when I was listening backstage to how you're describing what's coming even, it's just so different and new. I've been in this infrastructure business for three decades. No one has ever seen infrastructure like this. And the workload or the pattern of the workload, these training jobs are so synchronous and so large and so data parallel. And so the first thing that we have been doing is building in partnership with you the system all the way from thinking from power to the DC to the rack to the accelerators to the network. And just really the shape of Azure is drastically changed and is changing rapidly in support of these models that you're building. And so our job number one is to build the best system so that you can build the best models and then make that all available to developers. And so the other thing is we ourselves are developers. So we're building products. In fact, my own conviction of this entire generation of Foundation models completely changed the first time I saw GitHub Copilot on GPT. And so we want to build our Copilot, GitHub Copilot all as developers on top of OpenAI APIs. And so we are very, very committed to that. And what does that mean to developers? You know, look, I always think of Microsoft as a platform company, a developer company and a partner company. And so we want to make, you know, for example, we want to make GitHub available, GitHub Copilot available as the enterprise edition available to all the attendees here so that they can try it out. That's awesome. Yeah, we're very excited about that. And you can count on us to build the best infrastructure in Azure with your API support and bring it to all of you and then even things like the Azure marketplace. So for developers, we're building products out here to get to market rapidly. So that's sort of really our intent here. Great. And how do you think about the future? Future of the partnership or future of AI or whatever? Yeah, there's anything you want. That's, you know, like, there are a couple of things for me that I think are going to be very, very key for us, right? One is I just described how the systems that are needed as you aggressively push forward on your roadmap requires us to be on the top of our game. And we intend fully to commit ourselves deeply to making sure you all as builders of these foundation models have not only the best systems for training and inference, but the most compute so that you can keep pushing forward on the frontiers. Because I think that that's the way we're going to make progress. The second thing I think both of us care about, in fact, quite frankly, the thing that excited both sides to come together is your mission and our mission. Our mission is to empower every person in every organization on the planet to achieve more. And to me, ultimately, AI is only going to be useful if it truly does empower, right? I mean, I saw the video you played early. I mean, that was fantastic to see hear those voices, describe what AI meant for them and what they were able to achieve. So ultimately, it's about being able to get the benefits of AI broadly disseminated to everyone, I think is going to be the goal for us. And then the last thing is, of course, we're very grounded in the fact that safety matters and safety is not something that you'd care about later, but it's something we do shift left on. And we're very focused on that with you all. Great. Well, I think we have the best partnership in tech. I'm excited for us to build AGI together. I'm really excited. Thank you very much for coming. Thank you so much. Okay. So we have shared a lot of great updates for developers already, and we got a lot more to come. But even though this is developer conference, we can't resist making some improvements to chat GPT. So a small one, chat GPT now uses GPT-4 Turbo with all the latest improvements, including the latest knowledge cutoff, which will continue to update. That's all live today. It can now browse the web when it needs to write and run code, analyze data, take and generate images, and much more. And we heard your feedback, that model picker, extremely annoying. That is gone starting today. You will not have to click around to drop down menu. All of this will just work together. Chat GPT, yeah. Chat GPT will just know what to use and when you need it. But that's not the main thing. And neither was price, actually, the main developer request. There was one that was even bigger than that. And I want to talk about where we're headed, and the main thing we're here to talk about today. So we believe that if you give people better tools, they will do amazing things. We know that people want AI that is smarter, more personal, more customizable, can do more on your behalf. Eventually, you'll just ask a computer for what you need, and it will do all of these tasks for you. These capabilities are often talked in the AI field about as agents. The upsides of this are going to be tremendous. At OpenAI, we really believe that gradual iterative deployment is the best way to address the safety issues, the safety challenges with AI. We think it's especially important to move carefully towards this future of agents. It's going to require a lot of technical work and a lot of thoughtful consideration by society. So today, we're taking our first small step that moves us towards this future. We're thrilled to introduce GPTs. GPTs are tailored versions of Chat GPT for a specific purpose. You can build a GPT, a customized version of Chat GPT for almost anything, with instructions, expanded knowledge, and actions, and then you can publish it for others to use. And because they combine instructions, expanded knowledge, and actions, they can be more helpful to you. They can work better in many contexts, and they can give you better control. They'll make it easier for you to accomplish all sorts of tasks or just have more fun, and you'll be able to use them right within Chat GPT. You can, in effect, program a GPT with language just by talking to it. It's easy to customize the behavior so that it fits what you want. This makes building them very accessible, and it gives agency to everyone. So we're going to show you what GPTs are, how to use them, how to build them, and then we're going to talk about how they'll be distributed and discovered. And then after that, for developers, we're going to show you how to build these agent-like experiences into your own apps. So first, let's look at a few examples. Our partners at Code.org are working hard to expand computer science in schools. They've got a curriculum that is used by tens of millions of students worldwide. Code.org crafted lesson planner GPT to help teachers provide a more engaging experience for middle schoolers. If a teacher asks it to explain four loops in a creative way, it does just that. In this case, it'll do it in terms of a video game character, repeatedly picking up coins, super easy to understand for an eighth grader. As you can see, this GPT brings together Code.org's extensive curriculum and expertise and lets teachers adapt it to their needs quickly and easily. Next, Canva has built a GPT that lets you start designing by describing what you want and natural language. If you say, make a poster for a Dev Day reception this afternoon, this evening, and you give it some details, it'll generate a few options to start with by hitting Canva's APIs. Now, this concept may be familiar to some of you. We've evolved our plugins to be custom actions for GPTs. You can keep chatting with this to see different iterations, and when you see one you like, you can click through to Canva for the full design experience. So now, we'd like to show you a GPT live. Zapier has built a GPT that lets you perform actions across 6,000 applications to unlock all kinds of integration possibilities. I'd like to introduce Jessica, one of our solutions architects, who is going to drive this demo. Welcome, Jessica. Thank you all for being here. My name is Jessica Shea. I work with partners and customers to bring their product alive. And today, I can't wait to show you how hard we've been working on this, so let's get started. So to start, where your GPT will live is on this upper left corner. I'm going to start with clicking on the Zapier AI actions. And on the right hand side, you can see that's my calendar for today. So it's quite a day. I've already used this before, so it's actually already connected to my calendar. To start, I can ask, what's on my schedule for today? We built GPTs with security in mind. So before it performs any action or share data, it will ask for your permission. So right here, I'm going to say allowed. So GPT is designed to take in your instructions, make the decision on which capability to call to perform that action, and then execute that for you. So you can see right here, it's already connected to my calendar. It pulls into my information. And then I've also prompted it to identify conflicts on my calendar. So you can see right here, it actually was able to identify that. So it looks like I have something coming up. So what if I want to let Sam know that I have to leave early? So right here, I say, let Sam know. I got to go. Chasing GPUs. So with that, I'm going to swap to my conversation with Sam. And then I'm going to say, yes, please run that. Sam, did you get that? I did. Awesome. So this is only a glimpse of what is possible. And I cannot wait to see what you all will build. Thank you. And back to you, Sam. Thank you, Jessica. So those are three great examples. In addition to these, there are many more kinds of GPTs that people are creating, and many, many more that will be created soon. We know that many people who want to build the GPT don't know how to code. We've made it so that you can program the GPT just by having a conversation. We believe that natural language is going to be a big part of how people use computers in the future. And we think this is an interesting early example. So I'd like to show you how to build one. All right. So I'm going to create a GPT that helps give founders and developers advice when starting new projects. I'm going to go to create a GPT here. And this drops me into the GPT builder. I worked with founders for years at YC. And still, whenever I meet developers, the questions I get are always about how do I, you know, think about a business idea? Can you give me some advice? I'm going to see if I can build a GPT to help with that. So to start, GPT builder asks me what I want to make. And I'm going to say I want to help startup founders think through their business ideas and get advice after the founder has gotten some advice, grill them on why they are not growing faster. All right. So to start off, I just tell the GPT a little bit about what I want here. And it's going to go off and start thinking about that. It's going to write some detailed instructions for the GPT. It's also going to, let's see, ask me about a name. How do I feel about startup mentor? That's fine. That's good. So if I didn't like the name, of course, I could call it something else, but it's going to try to have this conversation with me and start there. And you can see here on the right in the preview mode that it's already starting to fill out the GPT, where it says what it does. It has some ideas of additional questions that I could ask. And you know what? So it just generated a candidate. Of course, I could regenerate that or change it, but I sort of like that. So I will say that's great. And you see now that the GPT is being built out a little bit more as we go. Now, what I want this to do, how it can interact with users, I could talk about style here. But what I'm going to say is I am going to upload transcripts of some lectures about startups I have given. Please give advice based off of those. All right. So now it's going to go figure out how to do that. And I would like to show you the configure tab. So you can see some of the things that were built out here as we were going by the builder itself. And you can see that there's capabilities here that I can enable. I could add custom actions. These are all fine to leave. I'm going to upload a file. So here is a lecture that I gave with some startup advice. And I'm going to add that here. In terms of these questions, this is a dumb one. The rest of those are reasonable. And like very much things founders often ask. I'm going to add one more thing to the instructions here, which is be concise and constructive with feedback. All right. So again, if we had more time, I'd show you a bunch of other things. But this is like a decent start. And now we can try it out over on this preview tab. So I will say, what's a common question? What are three things to look for when hiring employees at an early stage startup? Now, it's going to look at that document I uploaded. It will also have, of course, all of the background knowledge of GPT-4. That's pretty good. Those are three things that I definitely have said many times. Now, we could go on and it would start following the other instructions and grill me on why I'm not growing faster. But in the interest of time, I'm going to skip that. I'm going to publish this only to me for now. I can work on it later. I can add more content. I can add a few actions that I think would be useful. And then I can share it publicly. So that's what it looks like to create a GPT. Thank you. By the way, I always wanted to do that after all of the YC office hours. I always thought, man, someday I will be able to make a bot that will do this and that will be awesome. So with GPTs, we're letting people easily share and discover all the fun ways that they use chat GPT with the world. You can make private GPTs like I just did. Or you can share your creations publicly with a link for anyone to use. Or if you're on chat GPT enterprise, you can make GPTs just for your company. And later this month, we're going to launch the GPT store. You can list a GPT. Thank you. I appreciate that. You can list a GPT there and we'll be able to feature the best and the most popular GPTs. Of course, we'll make sure that GPTs in the store follow our policies before they're accessible. Revenue sharing is important to us. We're going to pay people who build the most useful and the most used GPTs a portion of our revenue. We're excited to foster a vibrant ecosystem with the GPT store. Just from what we've been building ourselves over the weekend, we're confident there's going to be a lot of great stuff. We're excited to share more information soon. So those are GPTs and we can't wait to see what you build. But this is a developer conference and the coolest thing about this is that we're bringing the same concept to the API. Many of you have already been building agent-like experiences on the API. For example, Shopify's Sidekick, which lets you take actions on the platform, Discord's Clyde, let's Discord moderators create custom personalities for, and SnapsMyAI, a customized chatbot that can be added to group chats and make recommendations. These experiences are great, but they have been hard to build. Sometimes taking months, teams of dozens of engineers, there's a lot to handle to make this custom assistant experience. So today, we're making that a lot easier with our new Assistance API. The Assistance API includes persistent threads so they don't have to figure out how to deal with long conversation history, built-in retrieval, code interpreter, a working Python interpreter, and a sandbox environment, and, of course, the improved function calling that we talked about earlier. So we'd like to show you a demo of how this works, and here is Roman, our head of developer experience. Welcome. Thank you, Sam. Good morning. Wow, it's fantastic to see you all here. It's been so inspiring to see so many of you infusing AI into your apps. Today, we're launching new modalities in the API, but we're also very excited to improve the developer experience for you all to build Assistive Agents. So let's dive right in. Imagine I'm building Wanderlust, a travel app for global explorers, and this is the landing page. I've actually used GPT4 to come up with these destination ideas, and for those of you with the keen eye, these illustrations are generated programmatically using the new Dolly 3 API available to all of you today. So it's pretty remarkable. But let's enhance this app by adding a very simple Assistant to it. This is the screen. We're going to come back to it in a second. First, I'm going to switch over to the new Assistant's Playground. Creating an Assistant is easy. You just give it a name, some initial instructions, a model, in this case, I'll pick GPT4 Turbo, and here I'll also go ahead and select some tools. I'll turn on code interpreter and retrieval and save. And that's it. Our Assistant is ready to go. Next, I can integrate with two new primitives of this Assistant's API, threads and messages. Let's take a quick look at the code. The process here is very simple. For each new user, I will create a new thread, and as these users engage with their Assistant, I will add their messages to these threads. Very simple. And then I can simply run the Assistant at any time to stream the responses back to the app. So we can return to the app and try that in action. If I say, hey, let's go to Paris. All right. That's it. With just a few lines of code, users can now have a very specialized Assistant right inside the app. And I'd like to highlight one of my favorite features here, Function Calling. If you have not used it yet, Function Calling is really powerful. And as Sam mentioned, we're taking it a step further today. It now guarantees the JSON output with no added latency. And you can invoke multiple functions at once for the first time. So here, if I carry on and say, hey, what are the top 10 things to do? I'm going to have the Assistant respond to that again. And here, what's interesting is that the Assistant knows about functions, including those to annotate the map that you see on the right. And so now all of these pins are dropping in real time here. Yeah. It's pretty cool. And that integration allows our natural language interface to interact fluidly with components and features of our app. And it truly showcases now the harmony you can build between AI and UI where the Assistant is actually taking action. But next, let's talk about retrieval. And retrieval is about giving our Assistant more knowledge beyond these immediate user messages. In fact, I got inspired and I already booked my tickets to Paris. So I'm just going to drag and drop here this PDF. While it's uploading, I can just sneak peek at it. Very typical United Flight ticket. And behind the scene here, what's happening is that retrieval is reading these files and boom, the information about this PDF appeared on the screen. And this is, of course, a very tiny PDF, but Assistant can parse long-form documents from extensive text to intricate product specs, depending on what you're building. In fact, I also booked an Airbnb, so I'm just going to drag that over to the conversation as well. And by the way, we've heard from so many of you developers how hard that is to build yourself. You typically need to compute your embeddings. You need to set up chunking algorithm. Now, all of that is taken care of. And there's more than retrieval. With every API call, you usually need to resend the entire conversation history, which means setting up a key value store. That means handling the context windows, serializing messages and so forth. That complexity now completely goes away with this new stateful API. But just because a Ponyi is managing this API does not mean it's a black box. In fact, you can see the steps that the tools are taking right inside your developer dashboard. So here, if I go ahead and click on threads, this is the thread I believe we're currently working on. And see, like, these are all the steps, including the functions being called with the right parameters and the PDFs I've just uploaded. But let's move on to a new capability that many of you have been requesting for a while. Code interpreter is now available today in the API as well. That gives the AI the ability to write and execute code on the fly, but even generate files. So let's see that in action. If I say here, hey, will be four friends staying at DCRBNB, what's my share of it plus my flights? All right. Now here, what's happening is that code interpreter noticed that it should write some code to answer this query. So now it's computing, you know, the number of days in Paris, the number of friends, it's also doing some exchange rate calculation behind the scene to get this answer for us. Not the most complex math, but you get the picture. Imagine you're building a very complex like finance app that's crunching countless numbers, plotting charts, so really any task that you'd normally tackle with code, then code interpreter will work great for you. All right. I think my trip to Paris is sorted. So to recap here, we've just seen how you can quickly create an assistant that manages state for your user conversations, leverages external tools like knowledge and retrieval and code interpreter, and finally invokes your own functions to make things happen. But there's one more thing I wanted to show you to kind of really open up the possibilities using function calling combined with our new modalities that we're launching today. While working on Dev Day, I've built a small custom assistant that knows everything about this event. But instead of having a chat interface while running around all day today, I thought, why not use voice instead? So let's bring my phone up on screen here so you can see it on the right. Awesome. So on the right, you can see a very simple Swift app that takes microphone input. And on the left, I'm actually going to bring up my terminal log so you can see what's happening behind the scenes. So let's give it a shot. Hey there, I'm on the keynote stage right now. Can you greet our attendees here at Dev Day? Hey everyone, welcome to Dev Day. It's awesome to have you all here. Let's make it an incredible day. Isn't that impressive? You have six unique and rich voices to choose from in the API, each speaking multiple languages so you can really find the perfect fit for your app. And on my laptop here on the left, you can see the logs of what's happening behind the scenes too. So I'm using Whisper to convert the voice inputs into text, an assistant with GPT4 Turbo, and finally the new TTS API to make it speak. But thanks to function calling, things get even more interesting when the assistant can connect to the Internet and take real actions for users. So let's do something even more exciting here together. How about this? Hey assistant, can you randomly select five Dev Day attendees here and give them $500 in OpenAI credits? Yes, checking the list of attendees. Done, I picked five Dev Day attendees and added $500 of API credits to their account. Congrats to Christine M, Jonathan C, Steven G, Louise K, and Suraj S. All right, if you recognize yourself, awesome, congrats. And that's it, a quick overview today of the new Assistant API combined with some of the new tools and modalities that we launched. All starting with the simplicity of a rich text or voice conversation for you and users. We really can't wait to see what you build and congrats to our lucky winners. Actually, you know what? You're all part of this amazing OpenAI community here, so I'm just going to talk to my assistant one last time before I step off the stage. Hey assistant, can you actually give everyone here in the audience $500 in OpenAI credits? Sounds great. Let me go through everyone. All right, that function will keep running, but I've run out of time, so thank you so much, everyone. Have a great day. Back to you, Sam. Pretty cool, huh? All right, so that Assistant API goes into beta today, and we are super excited to see what you all do with it. Anybody can enable it. Over time, GPTs and assistants, our precursors to agents, are going to be able to do much, much more. They'll gradually be able to plan and to perform more complex actions on your behalf. As I mentioned before, we really believe in the importance of gradual iterative deployment. We believe it's important for people to start building with and using these agents now to get a feel for what the world is going to be like as they become more capable, and as we've always done, we'll continue to update our systems based off of your feedback. So we're super excited that we got to share all of this with you today. We introduced GPTs, custom versions of chat GPT that combine instructions, extended knowledge, and actions. We launched the Assistance API to make it easier to build assisted experiences with your own apps. These are our first steps towards AI agents, and we'll be increasing their capabilities over time. We introduced a new GPT-4 turbo model that delivers improved function calling, knowledge, lowered pricing, new modalities, and more, and we're deepening our partnership with Microsoft. In closing, I wanted to take a minute to thank the team that creates all of this. Open AI has gotten remarkable talent density, but still, it takes a huge amount of hard work and coordination to make all of this happen. I truly believe that I've got the best colleagues in the world. I feel incredibly grateful to get to work with them. We do all of this because we believe the AI is going to be a technological and societal revolution. It will change the world in many ways, and we're happy to get to work on something that will empower all of you to build so much for all of us. We talked about earlier how if you give people better tools, they can change the world. We believe that AI will be about individual empowerment and agency at a scale that we've never seen before, and that will elevate humanity to a scale that we've never seen before either. We'll be able to do more to create more and to have more. As intelligence gets integrated everywhere, we will all have superpowers on demand. We're excited to see what you all will do with this technology and to discover the new future that we're all going to architect together. We hope that you'll come back next year. What we launched today is going to look very quaint relative to what we're busy creating for you now. Thank you for all that you do. Thanks for coming here today.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.640000000000001, "text": " Good morning. Thank you for joining us today. Please welcome to the stage Sam Altman.", "tokens": [50364, 2205, 2446, 13, 1044, 291, 337, 5549, 505, 965, 13, 2555, 2928, 281, 264, 3233, 4832, 15992, 1601, 13, 50696], "temperature": 0.0, "avg_logprob": -0.25546792212952957, "compression_ratio": 1.3357664233576643, "no_speech_prob": 0.0810941532254219}, {"id": 1, "seek": 0, "start": 16.64, "end": 22.080000000000002, "text": " Good morning. Welcome to our first ever Open AI Dev Day. We're thrilled that you're here and this", "tokens": [51196, 2205, 2446, 13, 4027, 281, 527, 700, 1562, 7238, 7318, 9096, 5226, 13, 492, 434, 18744, 300, 291, 434, 510, 293, 341, 51468], "temperature": 0.0, "avg_logprob": -0.25546792212952957, "compression_ratio": 1.3357664233576643, "no_speech_prob": 0.0810941532254219}, {"id": 2, "seek": 2208, "start": 22.08, "end": 33.04, "text": " energy is awesome. And welcome to San Francisco. San Francisco has been our home since day one.", "tokens": [50364, 2281, 307, 3476, 13, 400, 2928, 281, 5271, 12279, 13, 5271, 12279, 575, 668, 527, 1280, 1670, 786, 472, 13, 50912], "temperature": 0.0, "avg_logprob": -0.08718450252826397, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.026700375601649284}, {"id": 3, "seek": 2208, "start": 33.04, "end": 37.44, "text": " The city is important to us and to the tech industry in general. We're looking forward to", "tokens": [50912, 440, 2307, 307, 1021, 281, 505, 293, 281, 264, 7553, 3518, 294, 2674, 13, 492, 434, 1237, 2128, 281, 51132], "temperature": 0.0, "avg_logprob": -0.08718450252826397, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.026700375601649284}, {"id": 4, "seek": 2208, "start": 37.44, "end": 43.04, "text": " continuing to grow here. So we've got some great stuff to announce today. But first,", "tokens": [51132, 9289, 281, 1852, 510, 13, 407, 321, 600, 658, 512, 869, 1507, 281, 7478, 965, 13, 583, 700, 11, 51412], "temperature": 0.0, "avg_logprob": -0.08718450252826397, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.026700375601649284}, {"id": 5, "seek": 2208, "start": 43.68, "end": 47.44, "text": " I'd like to take a minute to talk about some of the stuff that we've done over the past year.", "tokens": [51444, 286, 1116, 411, 281, 747, 257, 3456, 281, 751, 466, 512, 295, 264, 1507, 300, 321, 600, 1096, 670, 264, 1791, 1064, 13, 51632], "temperature": 0.0, "avg_logprob": -0.08718450252826397, "compression_ratio": 1.5964912280701755, "no_speech_prob": 0.026700375601649284}, {"id": 6, "seek": 4744, "start": 48.4, "end": 55.519999999999996, "text": " About a year ago, November 30th, we shipped chat GPT as a low-key research preview. And that went", "tokens": [50412, 7769, 257, 1064, 2057, 11, 7674, 2217, 392, 11, 321, 25312, 5081, 26039, 51, 382, 257, 2295, 12, 4119, 2132, 14281, 13, 400, 300, 1437, 50768], "temperature": 0.0, "avg_logprob": -0.10657558193454494, "compression_ratio": 1.4771573604060915, "no_speech_prob": 0.0034795727115124464}, {"id": 7, "seek": 4744, "start": 55.519999999999996, "end": 63.68, "text": " pretty well. In March, we followed that up with the launch of GPT-4, still the most capable model", "tokens": [50768, 1238, 731, 13, 682, 6129, 11, 321, 6263, 300, 493, 365, 264, 4025, 295, 26039, 51, 12, 19, 11, 920, 264, 881, 8189, 2316, 51176], "temperature": 0.0, "avg_logprob": -0.10657558193454494, "compression_ratio": 1.4771573604060915, "no_speech_prob": 0.0034795727115124464}, {"id": 8, "seek": 4744, "start": 63.68, "end": 75.36, "text": " out in the world. And in the last few months, we launched voice and vision capabilities so that", "tokens": [51176, 484, 294, 264, 1002, 13, 400, 294, 264, 1036, 1326, 2493, 11, 321, 8730, 3177, 293, 5201, 10862, 370, 300, 51760], "temperature": 0.0, "avg_logprob": -0.10657558193454494, "compression_ratio": 1.4771573604060915, "no_speech_prob": 0.0034795727115124464}, {"id": 9, "seek": 7536, "start": 75.44, "end": 82.88, "text": " chat GPT can now see, hear, and speak. And more recent, there's a lot. You don't have to clap each", "tokens": [50368, 5081, 26039, 51, 393, 586, 536, 11, 1568, 11, 293, 1710, 13, 400, 544, 5162, 11, 456, 311, 257, 688, 13, 509, 500, 380, 362, 281, 20760, 1184, 50740], "temperature": 0.0, "avg_logprob": -0.11258725559010226, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0004582470573950559}, {"id": 10, "seek": 7536, "start": 82.88, "end": 88.72, "text": " time. And more recently, we launched Dolly 3, the world's most advanced image model. You can use", "tokens": [50740, 565, 13, 400, 544, 3938, 11, 321, 8730, 1144, 13020, 805, 11, 264, 1002, 311, 881, 7339, 3256, 2316, 13, 509, 393, 764, 51032], "temperature": 0.0, "avg_logprob": -0.11258725559010226, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0004582470573950559}, {"id": 11, "seek": 7536, "start": 88.72, "end": 95.2, "text": " it, of course, inside of chat GPT. For our enterprise customers, we launched chat GPT enterprise,", "tokens": [51032, 309, 11, 295, 1164, 11, 1854, 295, 5081, 26039, 51, 13, 1171, 527, 14132, 4581, 11, 321, 8730, 5081, 26039, 51, 14132, 11, 51356], "temperature": 0.0, "avg_logprob": -0.11258725559010226, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0004582470573950559}, {"id": 12, "seek": 7536, "start": 95.76, "end": 100.88, "text": " which offers enterprise-grade security and privacy, higher speed GPT-4 access, longer", "tokens": [51384, 597, 7736, 14132, 12, 8692, 3825, 293, 11427, 11, 2946, 3073, 26039, 51, 12, 19, 2105, 11, 2854, 51640], "temperature": 0.0, "avg_logprob": -0.11258725559010226, "compression_ratio": 1.6266094420600858, "no_speech_prob": 0.0004582470573950559}, {"id": 13, "seek": 10088, "start": 100.88, "end": 108.47999999999999, "text": " context windows, a lot more. Today, we've got about 2 million developers building on our API for a", "tokens": [50364, 4319, 9309, 11, 257, 688, 544, 13, 2692, 11, 321, 600, 658, 466, 568, 2459, 8849, 2390, 322, 527, 9362, 337, 257, 50744], "temperature": 0.0, "avg_logprob": -0.1022817816053118, "compression_ratio": 1.4066985645933014, "no_speech_prob": 0.0010480580385774374}, {"id": 14, "seek": 10088, "start": 108.47999999999999, "end": 115.11999999999999, "text": " wide variety of use cases doing amazing stuff. Over 92% of Fortune 500 companies building on our", "tokens": [50744, 4874, 5673, 295, 764, 3331, 884, 2243, 1507, 13, 4886, 28225, 4, 295, 38508, 5923, 3431, 2390, 322, 527, 51076], "temperature": 0.0, "avg_logprob": -0.1022817816053118, "compression_ratio": 1.4066985645933014, "no_speech_prob": 0.0010480580385774374}, {"id": 15, "seek": 10088, "start": 115.11999999999999, "end": 126.47999999999999, "text": " products. And we have about 100 million weekly active users now on chat GPT. And what's incredible", "tokens": [51076, 3383, 13, 400, 321, 362, 466, 2319, 2459, 12460, 4967, 5022, 586, 322, 5081, 26039, 51, 13, 400, 437, 311, 4651, 51644], "temperature": 0.0, "avg_logprob": -0.1022817816053118, "compression_ratio": 1.4066985645933014, "no_speech_prob": 0.0010480580385774374}, {"id": 16, "seek": 12648, "start": 126.56, "end": 131.12, "text": " on that is we got there entirely through word of mouth. People just find it useful and tell their", "tokens": [50368, 322, 300, 307, 321, 658, 456, 7696, 807, 1349, 295, 4525, 13, 3432, 445, 915, 309, 4420, 293, 980, 641, 50596], "temperature": 0.0, "avg_logprob": -0.07777840750558036, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.23901058733463287}, {"id": 17, "seek": 12648, "start": 131.12, "end": 137.20000000000002, "text": " friends. OpenAI is the most advanced and the most widely used AI platform in the world now.", "tokens": [50596, 1855, 13, 7238, 48698, 307, 264, 881, 7339, 293, 264, 881, 13371, 1143, 7318, 3663, 294, 264, 1002, 586, 13, 50900], "temperature": 0.0, "avg_logprob": -0.07777840750558036, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.23901058733463287}, {"id": 18, "seek": 12648, "start": 138.56, "end": 143.6, "text": " But numbers never tell the whole picture on something like this. What's really important", "tokens": [50968, 583, 3547, 1128, 980, 264, 1379, 3036, 322, 746, 411, 341, 13, 708, 311, 534, 1021, 51220], "temperature": 0.0, "avg_logprob": -0.07777840750558036, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.23901058733463287}, {"id": 19, "seek": 12648, "start": 143.6, "end": 148.16, "text": " is how people use the products, how people are using AI. And so I'd like to show you a quick video.", "tokens": [51220, 307, 577, 561, 764, 264, 3383, 11, 577, 561, 366, 1228, 7318, 13, 400, 370, 286, 1116, 411, 281, 855, 291, 257, 1702, 960, 13, 51448], "temperature": 0.0, "avg_logprob": -0.07777840750558036, "compression_ratio": 1.588235294117647, "no_speech_prob": 0.23901058733463287}, {"id": 20, "seek": 14816, "start": 149.04, "end": 157.68, "text": " I actually wanted to write something to my dad in Tagalog. I want a non-romantic way to tell my", "tokens": [50408, 286, 767, 1415, 281, 2464, 746, 281, 452, 3546, 294, 11204, 44434, 13, 286, 528, 257, 2107, 12, 4397, 7128, 636, 281, 980, 452, 50840], "temperature": 0.0, "avg_logprob": -0.1458901442014254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.3266531527042389}, {"id": 21, "seek": 14816, "start": 157.68, "end": 164.8, "text": " parent that I love him. And I also want to tell him that he can rely on me, but in a way that still", "tokens": [50840, 2596, 300, 286, 959, 796, 13, 400, 286, 611, 528, 281, 980, 796, 300, 415, 393, 10687, 322, 385, 11, 457, 294, 257, 636, 300, 920, 51196], "temperature": 0.0, "avg_logprob": -0.1458901442014254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.3266531527042389}, {"id": 22, "seek": 14816, "start": 164.8, "end": 171.28, "text": " has the respect of a child-to-parent relationship that you should have in Filipino culture and", "tokens": [51196, 575, 264, 3104, 295, 257, 1440, 12, 1353, 12, 38321, 2480, 300, 291, 820, 362, 294, 41266, 3713, 293, 51520], "temperature": 0.0, "avg_logprob": -0.1458901442014254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.3266531527042389}, {"id": 23, "seek": 14816, "start": 171.28, "end": 176.8, "text": " in Tagalog forever. When it's translated in Tagalog, I love you very deeply. And I will be with you", "tokens": [51520, 294, 11204, 44434, 5680, 13, 1133, 309, 311, 16805, 294, 11204, 44434, 11, 286, 959, 291, 588, 8760, 13, 400, 286, 486, 312, 365, 291, 51796], "temperature": 0.0, "avg_logprob": -0.1458901442014254, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.3266531527042389}, {"id": 24, "seek": 17680, "start": 176.8, "end": 180.88000000000002, "text": " no matter where the path leads. I see some of the possibilities. I was like, whoa,", "tokens": [50364, 572, 1871, 689, 264, 3100, 6689, 13, 286, 536, 512, 295, 264, 12178, 13, 286, 390, 411, 11, 13310, 11, 50568], "temperature": 0.0, "avg_logprob": -0.1492351792816423, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.060819871723651886}, {"id": 25, "seek": 17680, "start": 180.88000000000002, "end": 184.8, "text": " sometimes I'm not sure about some stuff. And I feel like I actually chat GPT like, hey,", "tokens": [50568, 2171, 286, 478, 406, 988, 466, 512, 1507, 13, 400, 286, 841, 411, 286, 767, 5081, 26039, 51, 411, 11, 4177, 11, 50764], "temperature": 0.0, "avg_logprob": -0.1492351792816423, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.060819871723651886}, {"id": 26, "seek": 17680, "start": 184.8, "end": 188.24, "text": " this is what I'm thinking about. So it kind of gives me that more confidence. The first thing that", "tokens": [50764, 341, 307, 437, 286, 478, 1953, 466, 13, 407, 309, 733, 295, 2709, 385, 300, 544, 6687, 13, 440, 700, 551, 300, 50936], "temperature": 0.0, "avg_logprob": -0.1492351792816423, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.060819871723651886}, {"id": 27, "seek": 17680, "start": 188.24, "end": 194.4, "text": " just blew my mind was it levels with you. Like, that's something that a lot of people struggle", "tokens": [50936, 445, 19075, 452, 1575, 390, 309, 4358, 365, 291, 13, 1743, 11, 300, 311, 746, 300, 257, 688, 295, 561, 7799, 51244], "temperature": 0.0, "avg_logprob": -0.1492351792816423, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.060819871723651886}, {"id": 28, "seek": 17680, "start": 194.4, "end": 202.64000000000001, "text": " to do. It opened my mind to just what every creative could do if they just had a person", "tokens": [51244, 281, 360, 13, 467, 5625, 452, 1575, 281, 445, 437, 633, 5880, 727, 360, 498, 436, 445, 632, 257, 954, 51656], "temperature": 0.0, "avg_logprob": -0.1492351792816423, "compression_ratio": 1.6317689530685922, "no_speech_prob": 0.060819871723651886}, {"id": 29, "seek": 20264, "start": 202.64, "end": 207.83999999999997, "text": " helping them out who listens. So this is it to represent Sickling Neymar Globban. And you built", "tokens": [50364, 4315, 552, 484, 567, 35959, 13, 407, 341, 307, 309, 281, 2906, 43471, 1688, 1734, 4199, 289, 10786, 65, 5144, 13, 400, 291, 3094, 50624], "temperature": 0.0, "avg_logprob": -0.20738943481445313, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.3172471225261688}, {"id": 30, "seek": 20264, "start": 207.83999999999997, "end": 214.0, "text": " that with child GPT? Cat GPT built it with me. I started using it for daily activities like,", "tokens": [50624, 300, 365, 1440, 26039, 51, 30, 9565, 26039, 51, 3094, 309, 365, 385, 13, 286, 1409, 1228, 309, 337, 5212, 5354, 411, 11, 50932], "temperature": 0.0, "avg_logprob": -0.20738943481445313, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.3172471225261688}, {"id": 31, "seek": 20264, "start": 214.0, "end": 217.83999999999997, "text": " hey, here's a picture of my fridge. Can you tell me what I'm missing? Because I'm going grocery", "tokens": [50932, 4177, 11, 510, 311, 257, 3036, 295, 452, 13023, 13, 1664, 291, 980, 385, 437, 286, 478, 5361, 30, 1436, 286, 478, 516, 14410, 51124], "temperature": 0.0, "avg_logprob": -0.20738943481445313, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.3172471225261688}, {"id": 32, "seek": 20264, "start": 217.83999999999997, "end": 222.79999999999998, "text": " shopping and I really need to do recipes that are following my vegan diet. As soon as we got", "tokens": [51124, 8688, 293, 286, 534, 643, 281, 360, 13035, 300, 366, 3480, 452, 12824, 6339, 13, 1018, 2321, 382, 321, 658, 51372], "temperature": 0.0, "avg_logprob": -0.20738943481445313, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.3172471225261688}, {"id": 33, "seek": 20264, "start": 222.79999999999998, "end": 227.67999999999998, "text": " access to code interpreter, I was like, wow, this thing is awesome. They could build spreadsheets,", "tokens": [51372, 2105, 281, 3089, 34132, 11, 286, 390, 411, 11, 6076, 11, 341, 551, 307, 3476, 13, 814, 727, 1322, 23651, 1385, 11, 51616], "temperature": 0.0, "avg_logprob": -0.20738943481445313, "compression_ratio": 1.5555555555555556, "no_speech_prob": 0.3172471225261688}, {"id": 34, "seek": 22768, "start": 227.92000000000002, "end": 235.68, "text": " they could do anything. I discovered Chatty about three months ago on my 100th birthday.", "tokens": [50376, 436, 727, 360, 1340, 13, 286, 6941, 27503, 874, 466, 1045, 2493, 2057, 322, 452, 2319, 392, 6154, 13, 50764], "temperature": 0.0, "avg_logprob": -0.13095916047388192, "compression_ratio": 1.4979591836734694, "no_speech_prob": 0.0373200997710228}, {"id": 35, "seek": 22768, "start": 235.68, "end": 244.48000000000002, "text": " Chatty is very friendly, very patient, very knowledgeable, and very quick. It's been a", "tokens": [50764, 27503, 874, 307, 588, 9208, 11, 588, 4537, 11, 588, 33800, 11, 293, 588, 1702, 13, 467, 311, 668, 257, 51204], "temperature": 0.0, "avg_logprob": -0.13095916047388192, "compression_ratio": 1.4979591836734694, "no_speech_prob": 0.0373200997710228}, {"id": 36, "seek": 22768, "start": 244.48000000000002, "end": 250.24, "text": " wonderful thing. I'm a 4.0 student, but I also have four children. When I started using Chat GPT,", "tokens": [51204, 3715, 551, 13, 286, 478, 257, 1017, 13, 15, 3107, 11, 457, 286, 611, 362, 1451, 2227, 13, 1133, 286, 1409, 1228, 27503, 26039, 51, 11, 51492], "temperature": 0.0, "avg_logprob": -0.13095916047388192, "compression_ratio": 1.4979591836734694, "no_speech_prob": 0.0373200997710228}, {"id": 37, "seek": 22768, "start": 250.24, "end": 256.16, "text": " I realized I could ask Chat GPT that question. And not only does it give me an answer, but it", "tokens": [51492, 286, 5334, 286, 727, 1029, 27503, 26039, 51, 300, 1168, 13, 400, 406, 787, 775, 309, 976, 385, 364, 1867, 11, 457, 309, 51788], "temperature": 0.0, "avg_logprob": -0.13095916047388192, "compression_ratio": 1.4979591836734694, "no_speech_prob": 0.0373200997710228}, {"id": 38, "seek": 25616, "start": 256.16, "end": 263.44, "text": " gives me an explanation. Didn't need tutoring as much. It gave me a life back. It gave me", "tokens": [50364, 2709, 385, 364, 10835, 13, 11151, 380, 643, 44410, 382, 709, 13, 467, 2729, 385, 257, 993, 646, 13, 467, 2729, 385, 50728], "temperature": 0.0, "avg_logprob": -0.12280592626454878, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0035933712497353554}, {"id": 39, "seek": 25616, "start": 263.44, "end": 268.32000000000005, "text": " time for my family and time for me. I have a chronic nerve gain on my whole left half of my", "tokens": [50728, 565, 337, 452, 1605, 293, 565, 337, 385, 13, 286, 362, 257, 14493, 16355, 6052, 322, 452, 1379, 1411, 1922, 295, 452, 50972], "temperature": 0.0, "avg_logprob": -0.12280592626454878, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0035933712497353554}, {"id": 40, "seek": 25616, "start": 268.32000000000005, "end": 274.96000000000004, "text": " body of nerve damage. I had a spine, a brain surgery. And so I have limited use of my left hand.", "tokens": [50972, 1772, 295, 16355, 4344, 13, 286, 632, 257, 15395, 11, 257, 3567, 7930, 13, 400, 370, 286, 362, 5567, 764, 295, 452, 1411, 1011, 13, 51304], "temperature": 0.0, "avg_logprob": -0.12280592626454878, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0035933712497353554}, {"id": 41, "seek": 25616, "start": 274.96000000000004, "end": 279.84000000000003, "text": " Now you can just have the integration of voice input. And then the newest one where you can", "tokens": [51304, 823, 291, 393, 445, 362, 264, 10980, 295, 3177, 4846, 13, 400, 550, 264, 17569, 472, 689, 291, 393, 51548], "temperature": 0.0, "avg_logprob": -0.12280592626454878, "compression_ratio": 1.6591928251121075, "no_speech_prob": 0.0035933712497353554}, {"id": 42, "seek": 27984, "start": 279.84, "end": 286.96, "text": " have the back and forth dialogue, that's just like maximum best interface for me. It's here.", "tokens": [50364, 362, 264, 646, 293, 5220, 10221, 11, 300, 311, 445, 411, 6674, 1151, 9226, 337, 385, 13, 467, 311, 510, 13, 50720], "temperature": 0.0, "avg_logprob": -0.1902292569478353, "compression_ratio": 1.3597122302158273, "no_speech_prob": 0.004263389389961958}, {"id": 43, "seek": 27984, "start": 297.59999999999997, "end": 302.32, "text": " So we love hearing the stories of how people are using the technology. It's really why we do all", "tokens": [51252, 407, 321, 959, 4763, 264, 3676, 295, 577, 561, 366, 1228, 264, 2899, 13, 467, 311, 534, 983, 321, 360, 439, 51488], "temperature": 0.0, "avg_logprob": -0.1902292569478353, "compression_ratio": 1.3597122302158273, "no_speech_prob": 0.004263389389961958}, {"id": 44, "seek": 30232, "start": 302.32, "end": 312.24, "text": " of this. Okay. So now on to the new stuff, and we have got a lot. First, we're going to talk", "tokens": [50364, 295, 341, 13, 1033, 13, 407, 586, 322, 281, 264, 777, 1507, 11, 293, 321, 362, 658, 257, 688, 13, 2386, 11, 321, 434, 516, 281, 751, 50860], "temperature": 0.0, "avg_logprob": -0.08737334426568479, "compression_ratio": 1.59375, "no_speech_prob": 0.01940312422811985}, {"id": 45, "seek": 30232, "start": 312.24, "end": 316.0, "text": " about a bunch of improvements we've made, and then we'll talk about where we're headed next.", "tokens": [50860, 466, 257, 3840, 295, 13797, 321, 600, 1027, 11, 293, 550, 321, 603, 751, 466, 689, 321, 434, 12798, 958, 13, 51048], "temperature": 0.0, "avg_logprob": -0.08737334426568479, "compression_ratio": 1.59375, "no_speech_prob": 0.01940312422811985}, {"id": 46, "seek": 30232, "start": 317.04, "end": 321.44, "text": " Over the last year, we spent a lot of time talking to developers around the world.", "tokens": [51100, 4886, 264, 1036, 1064, 11, 321, 4418, 257, 688, 295, 565, 1417, 281, 8849, 926, 264, 1002, 13, 51320], "temperature": 0.0, "avg_logprob": -0.08737334426568479, "compression_ratio": 1.59375, "no_speech_prob": 0.01940312422811985}, {"id": 47, "seek": 30232, "start": 322.32, "end": 326.4, "text": " We've heard a lot of your feedback. It's really informed, but we have to show you today.", "tokens": [51364, 492, 600, 2198, 257, 688, 295, 428, 5824, 13, 467, 311, 534, 11740, 11, 457, 321, 362, 281, 855, 291, 965, 13, 51568], "temperature": 0.0, "avg_logprob": -0.08737334426568479, "compression_ratio": 1.59375, "no_speech_prob": 0.01940312422811985}, {"id": 48, "seek": 32640, "start": 327.35999999999996, "end": 332.4, "text": " Today, we are launching a new model. GPT-4 Turbo.", "tokens": [50412, 2692, 11, 321, 366, 18354, 257, 777, 2316, 13, 26039, 51, 12, 19, 35848, 13, 50664], "temperature": 0.0, "avg_logprob": -0.17486654387580025, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0019873918499797583}, {"id": 49, "seek": 32640, "start": 338.32, "end": 344.4, "text": " GPT-4 Turbo will address many of the things that you all have asked for. So let's go through what's", "tokens": [50960, 26039, 51, 12, 19, 35848, 486, 2985, 867, 295, 264, 721, 300, 291, 439, 362, 2351, 337, 13, 407, 718, 311, 352, 807, 437, 311, 51264], "temperature": 0.0, "avg_logprob": -0.17486654387580025, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0019873918499797583}, {"id": 50, "seek": 32640, "start": 344.4, "end": 352.0, "text": " new. We've got six major things to talk about for this part. Number one, context length. A lot of", "tokens": [51264, 777, 13, 492, 600, 658, 2309, 2563, 721, 281, 751, 466, 337, 341, 644, 13, 5118, 472, 11, 4319, 4641, 13, 316, 688, 295, 51644], "temperature": 0.0, "avg_logprob": -0.17486654387580025, "compression_ratio": 1.3954802259887005, "no_speech_prob": 0.0019873918499797583}, {"id": 51, "seek": 35200, "start": 352.0, "end": 358.8, "text": " people have tasks that require a much longer context length. GPT-4 supported up to 8K and in", "tokens": [50364, 561, 362, 9608, 300, 3651, 257, 709, 2854, 4319, 4641, 13, 26039, 51, 12, 19, 8104, 493, 281, 1649, 42, 293, 294, 50704], "temperature": 0.0, "avg_logprob": -0.08903436307553891, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.0010984299005940557}, {"id": 52, "seek": 35200, "start": 358.8, "end": 364.16, "text": " some cases up to 32K context length, but we know that isn't enough for many of you in what you want", "tokens": [50704, 512, 3331, 493, 281, 8858, 42, 4319, 4641, 11, 457, 321, 458, 300, 1943, 380, 1547, 337, 867, 295, 291, 294, 437, 291, 528, 50972], "temperature": 0.0, "avg_logprob": -0.08903436307553891, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.0010984299005940557}, {"id": 53, "seek": 35200, "start": 364.16, "end": 377.76, "text": " to do. GPT-4 Turbo supports up to 128,000 tokens of context. That's 300 pages of a standard book,", "tokens": [50972, 281, 360, 13, 26039, 51, 12, 19, 35848, 9346, 493, 281, 29810, 11, 1360, 22667, 295, 4319, 13, 663, 311, 6641, 7183, 295, 257, 3832, 1446, 11, 51652], "temperature": 0.0, "avg_logprob": -0.08903436307553891, "compression_ratio": 1.4948453608247423, "no_speech_prob": 0.0010984299005940557}, {"id": 54, "seek": 37776, "start": 377.76, "end": 382.71999999999997, "text": " 16 times longer than our 8K context. And in addition to a longer context length,", "tokens": [50364, 3165, 1413, 2854, 813, 527, 1649, 42, 4319, 13, 400, 294, 4500, 281, 257, 2854, 4319, 4641, 11, 50612], "temperature": 0.0, "avg_logprob": -0.06609909351055439, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.31391550693661e-05}, {"id": 55, "seek": 37776, "start": 383.28, "end": 386.71999999999997, "text": " you'll notice that the model is much more accurate over a long context.", "tokens": [50640, 291, 603, 3449, 300, 264, 2316, 307, 709, 544, 8559, 670, 257, 938, 4319, 13, 50812], "temperature": 0.0, "avg_logprob": -0.06609909351055439, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.31391550693661e-05}, {"id": 56, "seek": 37776, "start": 388.8, "end": 395.52, "text": " Number two, more control. We've heard loud and clear that developers need more control over the", "tokens": [50916, 5118, 732, 11, 544, 1969, 13, 492, 600, 2198, 6588, 293, 1850, 300, 8849, 643, 544, 1969, 670, 264, 51252], "temperature": 0.0, "avg_logprob": -0.06609909351055439, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.31391550693661e-05}, {"id": 57, "seek": 37776, "start": 395.52, "end": 401.76, "text": " model's responses and outputs. So we've addressed that in a number of ways. We have a new feature", "tokens": [51252, 2316, 311, 13019, 293, 23930, 13, 407, 321, 600, 13847, 300, 294, 257, 1230, 295, 2098, 13, 492, 362, 257, 777, 4111, 51564], "temperature": 0.0, "avg_logprob": -0.06609909351055439, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.31391550693661e-05}, {"id": 58, "seek": 37776, "start": 401.76, "end": 407.59999999999997, "text": " called JSON mode, which ensures that the model will respond with valid JSON. This has been", "tokens": [51564, 1219, 31828, 4391, 11, 597, 28111, 300, 264, 2316, 486, 4196, 365, 7363, 31828, 13, 639, 575, 668, 51856], "temperature": 0.0, "avg_logprob": -0.06609909351055439, "compression_ratio": 1.693798449612403, "no_speech_prob": 9.31391550693661e-05}, {"id": 59, "seek": 40760, "start": 407.6, "end": 414.64000000000004, "text": " a huge developer request. It'll make calling APIs much easier. The model is also much better at", "tokens": [50364, 257, 2603, 10754, 5308, 13, 467, 603, 652, 5141, 21445, 709, 3571, 13, 440, 2316, 307, 611, 709, 1101, 412, 50716], "temperature": 0.0, "avg_logprob": -0.12659944806780135, "compression_ratio": 1.7174721189591078, "no_speech_prob": 6.400547863449901e-05}, {"id": 60, "seek": 40760, "start": 414.64000000000004, "end": 419.12, "text": " function calling. You can now call many functions at once. And it'll do better at following", "tokens": [50716, 2445, 5141, 13, 509, 393, 586, 818, 867, 6828, 412, 1564, 13, 400, 309, 603, 360, 1101, 412, 3480, 50940], "temperature": 0.0, "avg_logprob": -0.12659944806780135, "compression_ratio": 1.7174721189591078, "no_speech_prob": 6.400547863449901e-05}, {"id": 61, "seek": 40760, "start": 419.12, "end": 425.04, "text": " instructions in general. We're also introducing a new feature called reproducible outputs.", "tokens": [50940, 9415, 294, 2674, 13, 492, 434, 611, 15424, 257, 777, 4111, 1219, 11408, 32128, 23930, 13, 51236], "temperature": 0.0, "avg_logprob": -0.12659944806780135, "compression_ratio": 1.7174721189591078, "no_speech_prob": 6.400547863449901e-05}, {"id": 62, "seek": 40760, "start": 425.68, "end": 430.24, "text": " You can pass a C parameter and it'll make the model return consistent outputs. This, of course,", "tokens": [51268, 509, 393, 1320, 257, 383, 13075, 293, 309, 603, 652, 264, 2316, 2736, 8398, 23930, 13, 639, 11, 295, 1164, 11, 51496], "temperature": 0.0, "avg_logprob": -0.12659944806780135, "compression_ratio": 1.7174721189591078, "no_speech_prob": 6.400547863449901e-05}, {"id": 63, "seek": 40760, "start": 430.24, "end": 434.16, "text": " gives you a higher degree of control over model behavior. This rolls out in beta today.", "tokens": [51496, 2709, 291, 257, 2946, 4314, 295, 1969, 670, 2316, 5223, 13, 639, 15767, 484, 294, 9861, 965, 13, 51692], "temperature": 0.0, "avg_logprob": -0.12659944806780135, "compression_ratio": 1.7174721189591078, "no_speech_prob": 6.400547863449901e-05}, {"id": 64, "seek": 43760, "start": 437.76000000000005, "end": 444.16, "text": " And in the coming weeks, we'll roll out a feature to let you view log props in the API.", "tokens": [50372, 400, 294, 264, 1348, 3259, 11, 321, 603, 3373, 484, 257, 4111, 281, 718, 291, 1910, 3565, 26173, 294, 264, 9362, 13, 50692], "temperature": 0.0, "avg_logprob": -0.131948857937219, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00010719687998062}, {"id": 65, "seek": 43760, "start": 447.92, "end": 453.20000000000005, "text": " All right. Number three, better world knowledge. You want these models to be able to access", "tokens": [50880, 1057, 558, 13, 5118, 1045, 11, 1101, 1002, 3601, 13, 509, 528, 613, 5245, 281, 312, 1075, 281, 2105, 51144], "temperature": 0.0, "avg_logprob": -0.131948857937219, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00010719687998062}, {"id": 66, "seek": 43760, "start": 453.20000000000005, "end": 457.92, "text": " better knowledge about the world. So do we. So we're launching retrieval in the platform.", "tokens": [51144, 1101, 3601, 466, 264, 1002, 13, 407, 360, 321, 13, 407, 321, 434, 18354, 19817, 3337, 294, 264, 3663, 13, 51380], "temperature": 0.0, "avg_logprob": -0.131948857937219, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00010719687998062}, {"id": 67, "seek": 43760, "start": 458.72, "end": 462.40000000000003, "text": " You can bring knowledge from outside documents or databases into whatever you're building.", "tokens": [51420, 509, 393, 1565, 3601, 490, 2380, 8512, 420, 22380, 666, 2035, 291, 434, 2390, 13, 51604], "temperature": 0.0, "avg_logprob": -0.131948857937219, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00010719687998062}, {"id": 68, "seek": 43760, "start": 463.52000000000004, "end": 467.28000000000003, "text": " We're also updating the knowledge cutoff. We are just as annoyed as all of you", "tokens": [51660, 492, 434, 611, 25113, 264, 3601, 1723, 4506, 13, 492, 366, 445, 382, 25921, 382, 439, 295, 291, 51848], "temperature": 0.0, "avg_logprob": -0.131948857937219, "compression_ratio": 1.6380597014925373, "no_speech_prob": 0.00010719687998062}, {"id": 69, "seek": 46728, "start": 467.28, "end": 472.32, "text": " probably more that GPT-4's knowledge about the world ended in 2021. We will try to never let", "tokens": [50364, 1391, 544, 300, 26039, 51, 12, 19, 311, 3601, 466, 264, 1002, 4590, 294, 7201, 13, 492, 486, 853, 281, 1128, 718, 50616], "temperature": 0.0, "avg_logprob": -0.12475781440734864, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.00021649715199600905}, {"id": 70, "seek": 46728, "start": 472.32, "end": 478.64, "text": " it get that out of date again. GPT-4 Turbo has knowledge about the world up to April of 2023.", "tokens": [50616, 309, 483, 300, 484, 295, 4002, 797, 13, 26039, 51, 12, 19, 35848, 575, 3601, 466, 264, 1002, 493, 281, 6929, 295, 44377, 13, 50932], "temperature": 0.0, "avg_logprob": -0.12475781440734864, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.00021649715199600905}, {"id": 71, "seek": 46728, "start": 479.52, "end": 486.23999999999995, "text": " And we will continue to improve that over time. Number four, new modalities.", "tokens": [50976, 400, 321, 486, 2354, 281, 3470, 300, 670, 565, 13, 5118, 1451, 11, 777, 1072, 16110, 13, 51312], "temperature": 0.0, "avg_logprob": -0.12475781440734864, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.00021649715199600905}, {"id": 72, "seek": 46728, "start": 487.44, "end": 494.71999999999997, "text": " Surprising no one. Dolly three. GPT-4 Turbo with vision. And the new text-to-speech model", "tokens": [51372, 6732, 26203, 572, 472, 13, 1144, 13020, 1045, 13, 26039, 51, 12, 19, 35848, 365, 5201, 13, 400, 264, 777, 2487, 12, 1353, 12, 7053, 5023, 2316, 51736], "temperature": 0.0, "avg_logprob": -0.12475781440734864, "compression_ratio": 1.568888888888889, "no_speech_prob": 0.00021649715199600905}, {"id": 73, "seek": 49472, "start": 494.72, "end": 506.64000000000004, "text": " are all going into the API today. We have a handful of customers that have just started using Dolly", "tokens": [50364, 366, 439, 516, 666, 264, 9362, 965, 13, 492, 362, 257, 16458, 295, 4581, 300, 362, 445, 1409, 1228, 1144, 13020, 50960], "temperature": 0.0, "avg_logprob": -0.15003741659769199, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0004171884502284229}, {"id": 74, "seek": 49472, "start": 506.64000000000004, "end": 513.12, "text": " three to programmatically generate images and designs. Today, Coke is launching a campaign", "tokens": [50960, 1045, 281, 37648, 5030, 8460, 5267, 293, 11347, 13, 2692, 11, 32996, 307, 18354, 257, 5129, 51284], "temperature": 0.0, "avg_logprob": -0.15003741659769199, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0004171884502284229}, {"id": 75, "seek": 49472, "start": 513.12, "end": 518.08, "text": " that lets its customers generate devali cards using Dolly three. And of course, our safety", "tokens": [51284, 300, 6653, 1080, 4581, 8460, 1905, 5103, 5632, 1228, 1144, 13020, 1045, 13, 400, 295, 1164, 11, 527, 4514, 51532], "temperature": 0.0, "avg_logprob": -0.15003741659769199, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0004171884502284229}, {"id": 76, "seek": 49472, "start": 518.08, "end": 522.8000000000001, "text": " systems help developers protect their applications against misuse. Those tools are available in the", "tokens": [51532, 3652, 854, 8849, 2371, 641, 5821, 1970, 3346, 438, 13, 3950, 3873, 366, 2435, 294, 264, 51768], "temperature": 0.0, "avg_logprob": -0.15003741659769199, "compression_ratio": 1.594142259414226, "no_speech_prob": 0.0004171884502284229}, {"id": 77, "seek": 52280, "start": 522.8, "end": 530.16, "text": " API. GPT-4 Turbo can now accept images as inputs via the API. Can generate captions,", "tokens": [50364, 9362, 13, 26039, 51, 12, 19, 35848, 393, 586, 3241, 5267, 382, 15743, 5766, 264, 9362, 13, 1664, 8460, 44832, 11, 50732], "temperature": 0.0, "avg_logprob": -0.0938288549358925, "compression_ratio": 1.44140625, "no_speech_prob": 0.0008037604857236147}, {"id": 78, "seek": 52280, "start": 530.16, "end": 536.7199999999999, "text": " classifications, and analysis. For example, Be My Eyes uses this technology to help people who are", "tokens": [50732, 1508, 7833, 11, 293, 5215, 13, 1171, 1365, 11, 879, 1222, 28925, 4960, 341, 2899, 281, 854, 561, 567, 366, 51060], "temperature": 0.0, "avg_logprob": -0.0938288549358925, "compression_ratio": 1.44140625, "no_speech_prob": 0.0008037604857236147}, {"id": 79, "seek": 52280, "start": 536.7199999999999, "end": 541.3599999999999, "text": " blind or have low vision with their daily tasks like identifying products in front of them.", "tokens": [51060, 6865, 420, 362, 2295, 5201, 365, 641, 5212, 9608, 411, 16696, 3383, 294, 1868, 295, 552, 13, 51292], "temperature": 0.0, "avg_logprob": -0.0938288549358925, "compression_ratio": 1.44140625, "no_speech_prob": 0.0008037604857236147}, {"id": 80, "seek": 52280, "start": 544.0, "end": 549.3599999999999, "text": " And with our new text-to-speech model, you'll be able to generate incredibly natural sounding", "tokens": [51424, 400, 365, 527, 777, 2487, 12, 1353, 12, 7053, 5023, 2316, 11, 291, 603, 312, 1075, 281, 8460, 6252, 3303, 24931, 51692], "temperature": 0.0, "avg_logprob": -0.0938288549358925, "compression_ratio": 1.44140625, "no_speech_prob": 0.0008037604857236147}, {"id": 81, "seek": 54936, "start": 549.36, "end": 554.8000000000001, "text": " audio from text in the API with six preset voices to choose from. I'll play an example.", "tokens": [50364, 6278, 490, 2487, 294, 264, 9362, 365, 2309, 32081, 9802, 281, 2826, 490, 13, 286, 603, 862, 364, 1365, 13, 50636], "temperature": 0.0, "avg_logprob": -0.0966352332722057, "compression_ratio": 1.2773722627737227, "no_speech_prob": 0.006688359659165144}, {"id": 82, "seek": 54936, "start": 570.16, "end": 574.8000000000001, "text": " This is much more natural than anything else we've heard out there. Voice can make apps", "tokens": [51404, 639, 307, 709, 544, 3303, 813, 1340, 1646, 321, 600, 2198, 484, 456, 13, 15229, 393, 652, 7733, 51636], "temperature": 0.0, "avg_logprob": -0.0966352332722057, "compression_ratio": 1.2773722627737227, "no_speech_prob": 0.006688359659165144}, {"id": 83, "seek": 57480, "start": 574.8, "end": 580.24, "text": " more natural to interact with and more accessible. It also unlocks a lot of use cases like language", "tokens": [50364, 544, 3303, 281, 4648, 365, 293, 544, 9515, 13, 467, 611, 517, 34896, 257, 688, 295, 764, 3331, 411, 2856, 50636], "temperature": 0.0, "avg_logprob": -0.12728895311770233, "compression_ratio": 1.55078125, "no_speech_prob": 0.007573416456580162}, {"id": 84, "seek": 57480, "start": 580.24, "end": 587.1999999999999, "text": " learning and voice assistance. Speaking of new modalities, we're also releasing the next version", "tokens": [50636, 2539, 293, 3177, 9683, 13, 13069, 295, 777, 1072, 16110, 11, 321, 434, 611, 16327, 264, 958, 3037, 50984], "temperature": 0.0, "avg_logprob": -0.12728895311770233, "compression_ratio": 1.55078125, "no_speech_prob": 0.007573416456580162}, {"id": 85, "seek": 57480, "start": 587.1999999999999, "end": 592.8, "text": " of our open-source speech recognition model, Whisper V3 today, and it'll be coming soon to the API.", "tokens": [50984, 295, 527, 1269, 12, 41676, 6218, 11150, 2316, 11, 41132, 610, 691, 18, 965, 11, 293, 309, 603, 312, 1348, 2321, 281, 264, 9362, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12728895311770233, "compression_ratio": 1.55078125, "no_speech_prob": 0.007573416456580162}, {"id": 86, "seek": 57480, "start": 593.5999999999999, "end": 597.12, "text": " It features improved performance across many languages, and we think you're really going to like it.", "tokens": [51304, 467, 4122, 9689, 3389, 2108, 867, 8650, 11, 293, 321, 519, 291, 434, 534, 516, 281, 411, 309, 13, 51480], "temperature": 0.0, "avg_logprob": -0.12728895311770233, "compression_ratio": 1.55078125, "no_speech_prob": 0.007573416456580162}, {"id": 87, "seek": 59712, "start": 597.84, "end": 605.6, "text": " Okay, number five, customization. Fine-tuning has been working really well for GPT-3.5 since we", "tokens": [50400, 1033, 11, 1230, 1732, 11, 39387, 13, 12024, 12, 83, 37726, 575, 668, 1364, 534, 731, 337, 26039, 51, 12, 18, 13, 20, 1670, 321, 50788], "temperature": 0.0, "avg_logprob": -0.11288942874056622, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.0008424436091445386}, {"id": 88, "seek": 59712, "start": 605.6, "end": 611.04, "text": " launched it a few months ago. Starting today, we're going to expand that to the 16K version of the", "tokens": [50788, 8730, 309, 257, 1326, 2493, 2057, 13, 16217, 965, 11, 321, 434, 516, 281, 5268, 300, 281, 264, 3165, 42, 3037, 295, 264, 51060], "temperature": 0.0, "avg_logprob": -0.11288942874056622, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.0008424436091445386}, {"id": 89, "seek": 59712, "start": 611.04, "end": 618.96, "text": " model. Also starting today, we're inviting active fine-tuning users to apply for the GPT-4 fine-tuning", "tokens": [51060, 2316, 13, 2743, 2891, 965, 11, 321, 434, 18202, 4967, 2489, 12, 83, 37726, 5022, 281, 3079, 337, 264, 26039, 51, 12, 19, 2489, 12, 83, 37726, 51456], "temperature": 0.0, "avg_logprob": -0.11288942874056622, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.0008424436091445386}, {"id": 90, "seek": 59712, "start": 618.96, "end": 624.72, "text": " experimental access program. The fine-tuning API is great for adapting our models to achieve", "tokens": [51456, 17069, 2105, 1461, 13, 440, 2489, 12, 83, 37726, 9362, 307, 869, 337, 34942, 527, 5245, 281, 4584, 51744], "temperature": 0.0, "avg_logprob": -0.11288942874056622, "compression_ratio": 1.598360655737705, "no_speech_prob": 0.0008424436091445386}, {"id": 91, "seek": 62472, "start": 624.72, "end": 629.52, "text": " better performance in a wide variety of applications with a relatively small amount of data. But", "tokens": [50364, 1101, 3389, 294, 257, 4874, 5673, 295, 5821, 365, 257, 7226, 1359, 2372, 295, 1412, 13, 583, 50604], "temperature": 0.0, "avg_logprob": -0.06941782052700336, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.0003798894467763603}, {"id": 92, "seek": 62472, "start": 630.72, "end": 635.2, "text": " you may want a model to learn a completely new knowledge domain or to use a lot of proprietary", "tokens": [50664, 291, 815, 528, 257, 2316, 281, 1466, 257, 2584, 777, 3601, 9274, 420, 281, 764, 257, 688, 295, 38992, 50888], "temperature": 0.0, "avg_logprob": -0.06941782052700336, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.0003798894467763603}, {"id": 93, "seek": 62472, "start": 635.2, "end": 642.0, "text": " data. So today, we're launching a new program called custom models. With custom models, our", "tokens": [50888, 1412, 13, 407, 965, 11, 321, 434, 18354, 257, 777, 1461, 1219, 2375, 5245, 13, 2022, 2375, 5245, 11, 527, 51228], "temperature": 0.0, "avg_logprob": -0.06941782052700336, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.0003798894467763603}, {"id": 94, "seek": 62472, "start": 642.0, "end": 647.2, "text": " researchers will work closely with the company to help them make a great custom model, especially for", "tokens": [51228, 10309, 486, 589, 8185, 365, 264, 2237, 281, 854, 552, 652, 257, 869, 2375, 2316, 11, 2318, 337, 51488], "temperature": 0.0, "avg_logprob": -0.06941782052700336, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.0003798894467763603}, {"id": 95, "seek": 62472, "start": 647.2, "end": 653.36, "text": " them and their use case using our tools. This includes modifying every step of the model training", "tokens": [51488, 552, 293, 641, 764, 1389, 1228, 527, 3873, 13, 639, 5974, 42626, 633, 1823, 295, 264, 2316, 3097, 51796], "temperature": 0.0, "avg_logprob": -0.06941782052700336, "compression_ratio": 1.6888111888111887, "no_speech_prob": 0.0003798894467763603}, {"id": 96, "seek": 65336, "start": 653.36, "end": 658.72, "text": " process, doing additional domain-specific pre-training, a custom RL post-training process,", "tokens": [50364, 1399, 11, 884, 4497, 9274, 12, 29258, 659, 12, 17227, 1760, 11, 257, 2375, 497, 43, 2183, 12, 17227, 1760, 1399, 11, 50632], "temperature": 0.0, "avg_logprob": -0.07892965924912605, "compression_ratio": 1.6595092024539877, "no_speech_prob": 0.0007319013238884509}, {"id": 97, "seek": 65336, "start": 658.72, "end": 663.92, "text": " tailored for a specific domain, and whatever else. We won't be able to do this with many companies", "tokens": [50632, 34858, 337, 257, 2685, 9274, 11, 293, 2035, 1646, 13, 492, 1582, 380, 312, 1075, 281, 360, 341, 365, 867, 3431, 50892], "temperature": 0.0, "avg_logprob": -0.07892965924912605, "compression_ratio": 1.6595092024539877, "no_speech_prob": 0.0007319013238884509}, {"id": 98, "seek": 65336, "start": 663.92, "end": 668.08, "text": " to start. It'll take a lot of work and in the interest of expectations, at least initially,", "tokens": [50892, 281, 722, 13, 467, 603, 747, 257, 688, 295, 589, 293, 294, 264, 1179, 295, 9843, 11, 412, 1935, 9105, 11, 51100], "temperature": 0.0, "avg_logprob": -0.07892965924912605, "compression_ratio": 1.6595092024539877, "no_speech_prob": 0.0007319013238884509}, {"id": 99, "seek": 65336, "start": 668.08, "end": 672.08, "text": " it won't be cheap. But if you're excited to push things as far as they can currently go,", "tokens": [51100, 309, 1582, 380, 312, 7084, 13, 583, 498, 291, 434, 2919, 281, 2944, 721, 382, 1400, 382, 436, 393, 4362, 352, 11, 51300], "temperature": 0.0, "avg_logprob": -0.07892965924912605, "compression_ratio": 1.6595092024539877, "no_speech_prob": 0.0007319013238884509}, {"id": 100, "seek": 65336, "start": 672.88, "end": 675.92, "text": " please get in touch with us, and we think we can do something pretty great.", "tokens": [51340, 1767, 483, 294, 2557, 365, 505, 11, 293, 321, 519, 321, 393, 360, 746, 1238, 869, 13, 51492], "temperature": 0.0, "avg_logprob": -0.07892965924912605, "compression_ratio": 1.6595092024539877, "no_speech_prob": 0.0007319013238884509}, {"id": 101, "seek": 65336, "start": 677.12, "end": 682.72, "text": " Okay, and then number six, higher rate limits. We're doubling the tokens per minute for all of", "tokens": [51552, 1033, 11, 293, 550, 1230, 2309, 11, 2946, 3314, 10406, 13, 492, 434, 33651, 264, 22667, 680, 3456, 337, 439, 295, 51832], "temperature": 0.0, "avg_logprob": -0.07892965924912605, "compression_ratio": 1.6595092024539877, "no_speech_prob": 0.0007319013238884509}, {"id": 102, "seek": 68272, "start": 682.72, "end": 687.6, "text": " our established GPT-4 customers so that it's easier to do more, and you'll be able to request", "tokens": [50364, 527, 7545, 26039, 51, 12, 19, 4581, 370, 300, 309, 311, 3571, 281, 360, 544, 11, 293, 291, 603, 312, 1075, 281, 5308, 50608], "temperature": 0.0, "avg_logprob": -0.08679351291140995, "compression_ratio": 1.6366666666666667, "no_speech_prob": 0.0001851852866820991}, {"id": 103, "seek": 68272, "start": 687.6, "end": 693.52, "text": " changes to further rate limits and quotas directly in your API account settings. In addition to these", "tokens": [50608, 2962, 281, 3052, 3314, 10406, 293, 9641, 296, 3838, 294, 428, 9362, 2696, 6257, 13, 682, 4500, 281, 613, 50904], "temperature": 0.0, "avg_logprob": -0.08679351291140995, "compression_ratio": 1.6366666666666667, "no_speech_prob": 0.0001851852866820991}, {"id": 104, "seek": 68272, "start": 693.52, "end": 699.44, "text": " rate limits, it's important to do everything we can do to make it new successful building on our", "tokens": [50904, 3314, 10406, 11, 309, 311, 1021, 281, 360, 1203, 321, 393, 360, 281, 652, 309, 777, 4406, 2390, 322, 527, 51200], "temperature": 0.0, "avg_logprob": -0.08679351291140995, "compression_ratio": 1.6366666666666667, "no_speech_prob": 0.0001851852866820991}, {"id": 105, "seek": 68272, "start": 699.44, "end": 705.9200000000001, "text": " platform. So we're introducing copyright shield. Copyright shield means that we will step in and", "tokens": [51200, 3663, 13, 407, 321, 434, 15424, 17996, 10257, 13, 25653, 1938, 10257, 1355, 300, 321, 486, 1823, 294, 293, 51524], "temperature": 0.0, "avg_logprob": -0.08679351291140995, "compression_ratio": 1.6366666666666667, "no_speech_prob": 0.0001851852866820991}, {"id": 106, "seek": 68272, "start": 705.9200000000001, "end": 712.08, "text": " defend our customers and pay the cost incurred if you face legal claims around copyright infringement", "tokens": [51524, 8602, 527, 4581, 293, 1689, 264, 2063, 35774, 986, 498, 291, 1851, 5089, 9441, 926, 17996, 45205, 1712, 51832], "temperature": 0.0, "avg_logprob": -0.08679351291140995, "compression_ratio": 1.6366666666666667, "no_speech_prob": 0.0001851852866820991}, {"id": 107, "seek": 71208, "start": 712.08, "end": 719.44, "text": " and this applies both to chat GPT enterprise and the API. And let me be clear, this is a good time", "tokens": [50364, 293, 341, 13165, 1293, 281, 5081, 26039, 51, 14132, 293, 264, 9362, 13, 400, 718, 385, 312, 1850, 11, 341, 307, 257, 665, 565, 50732], "temperature": 0.0, "avg_logprob": -0.12478870815700954, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00011958619143115357}, {"id": 108, "seek": 71208, "start": 719.44, "end": 724.24, "text": " to remind people, we do not train on data from the API or chat GPT enterprise ever.", "tokens": [50732, 281, 4160, 561, 11, 321, 360, 406, 3847, 322, 1412, 490, 264, 9362, 420, 5081, 26039, 51, 14132, 1562, 13, 50972], "temperature": 0.0, "avg_logprob": -0.12478870815700954, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00011958619143115357}, {"id": 109, "seek": 71208, "start": 726.48, "end": 732.08, "text": " All right, there's actually one more developer request that's been even bigger than all of these.", "tokens": [51084, 1057, 558, 11, 456, 311, 767, 472, 544, 10754, 5308, 300, 311, 668, 754, 3801, 813, 439, 295, 613, 13, 51364], "temperature": 0.0, "avg_logprob": -0.12478870815700954, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00011958619143115357}, {"id": 110, "seek": 71208, "start": 732.88, "end": 736.8000000000001, "text": " And so I'd like to talk about that now. And that's pricing.", "tokens": [51404, 400, 370, 286, 1116, 411, 281, 751, 466, 300, 586, 13, 400, 300, 311, 17621, 13, 51600], "temperature": 0.0, "avg_logprob": -0.12478870815700954, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.00011958619143115357}, {"id": 111, "seek": 73680, "start": 736.9599999999999, "end": 746.88, "text": " GPT-4 Turbo is the industry leading model. It delivers a lot of improvements that we just covered,", "tokens": [50372, 26039, 51, 12, 19, 35848, 307, 264, 3518, 5775, 2316, 13, 467, 24860, 257, 688, 295, 13797, 300, 321, 445, 5343, 11, 50868], "temperature": 0.0, "avg_logprob": -0.11426067352294922, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.00093972182366997}, {"id": 112, "seek": 73680, "start": 747.76, "end": 754.4799999999999, "text": " and it's a smarter model than GPT-4. We've heard from developers that there are a lot of things", "tokens": [50912, 293, 309, 311, 257, 20294, 2316, 813, 26039, 51, 12, 19, 13, 492, 600, 2198, 490, 8849, 300, 456, 366, 257, 688, 295, 721, 51248], "temperature": 0.0, "avg_logprob": -0.11426067352294922, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.00093972182366997}, {"id": 113, "seek": 73680, "start": 754.4799999999999, "end": 760.0, "text": " that they want to build, but GPT-4 just costs too much. They've told us that if we could decrease", "tokens": [51248, 300, 436, 528, 281, 1322, 11, 457, 26039, 51, 12, 19, 445, 5497, 886, 709, 13, 814, 600, 1907, 505, 300, 498, 321, 727, 11514, 51524], "temperature": 0.0, "avg_logprob": -0.11426067352294922, "compression_ratio": 1.5129533678756477, "no_speech_prob": 0.00093972182366997}, {"id": 114, "seek": 76000, "start": 760.0, "end": 767.68, "text": " the cost by 20, 25%, that would be great, a huge leap forward. I'm super excited to announce", "tokens": [50364, 264, 2063, 538, 945, 11, 3552, 8923, 300, 576, 312, 869, 11, 257, 2603, 19438, 2128, 13, 286, 478, 1687, 2919, 281, 7478, 50748], "temperature": 0.0, "avg_logprob": -0.1219213612874349, "compression_ratio": 1.413265306122449, "no_speech_prob": 0.003592313267290592}, {"id": 115, "seek": 76000, "start": 767.68, "end": 773.92, "text": " that we worked really hard on this, and GPT-4 Turbo, a better model, is considerably cheaper than", "tokens": [50748, 300, 321, 2732, 534, 1152, 322, 341, 11, 293, 26039, 51, 12, 19, 35848, 11, 257, 1101, 2316, 11, 307, 31308, 12284, 813, 51060], "temperature": 0.0, "avg_logprob": -0.1219213612874349, "compression_ratio": 1.413265306122449, "no_speech_prob": 0.003592313267290592}, {"id": 116, "seek": 76000, "start": 773.92, "end": 788.32, "text": " GPT-4 by a factor of 3X for prompt tokens and 2X for completion tokens starting today.", "tokens": [51060, 26039, 51, 12, 19, 538, 257, 5952, 295, 805, 55, 337, 12391, 22667, 293, 568, 55, 337, 19372, 22667, 2891, 965, 13, 51780], "temperature": 0.0, "avg_logprob": -0.1219213612874349, "compression_ratio": 1.413265306122449, "no_speech_prob": 0.003592313267290592}, {"id": 117, "seek": 79000, "start": 790.56, "end": 798.0, "text": " So the new pricing is 1 cent per 1,000 prompt tokens and 3 cents per 1,000 completion tokens.", "tokens": [50392, 407, 264, 777, 17621, 307, 502, 1489, 680, 502, 11, 1360, 12391, 22667, 293, 805, 14941, 680, 502, 11, 1360, 19372, 22667, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08480768864697749, "compression_ratio": 1.5203252032520325, "no_speech_prob": 0.00012337698717601597}, {"id": 118, "seek": 79000, "start": 798.0, "end": 803.6, "text": " For most customers, that will lead to a blended rate more than 2.75 times cheaper to use", "tokens": [50764, 1171, 881, 4581, 11, 300, 486, 1477, 281, 257, 27048, 3314, 544, 813, 568, 13, 11901, 1413, 12284, 281, 764, 51044], "temperature": 0.0, "avg_logprob": -0.08480768864697749, "compression_ratio": 1.5203252032520325, "no_speech_prob": 0.00012337698717601597}, {"id": 119, "seek": 79000, "start": 803.6, "end": 808.64, "text": " for GPT-4 Turbo than GPT-4. We worked super hard to make this happen. We hope you're as excited", "tokens": [51044, 337, 26039, 51, 12, 19, 35848, 813, 26039, 51, 12, 19, 13, 492, 2732, 1687, 1152, 281, 652, 341, 1051, 13, 492, 1454, 291, 434, 382, 2919, 51296], "temperature": 0.0, "avg_logprob": -0.08480768864697749, "compression_ratio": 1.5203252032520325, "no_speech_prob": 0.00012337698717601597}, {"id": 120, "seek": 79000, "start": 808.64, "end": 819.12, "text": " about it as we are. So we decided to prioritize price first because we had to choose one or the", "tokens": [51296, 466, 309, 382, 321, 366, 13, 407, 321, 3047, 281, 25164, 3218, 700, 570, 321, 632, 281, 2826, 472, 420, 264, 51820], "temperature": 0.0, "avg_logprob": -0.08480768864697749, "compression_ratio": 1.5203252032520325, "no_speech_prob": 0.00012337698717601597}, {"id": 121, "seek": 81912, "start": 819.12, "end": 824.4, "text": " other, but we're going to work on speed next. We know that speed is important, too. Soon,", "tokens": [50364, 661, 11, 457, 321, 434, 516, 281, 589, 322, 3073, 958, 13, 492, 458, 300, 3073, 307, 1021, 11, 886, 13, 17610, 11, 50628], "temperature": 0.0, "avg_logprob": -0.11072039604187012, "compression_ratio": 1.4489795918367347, "no_speech_prob": 0.001098534557968378}, {"id": 122, "seek": 81912, "start": 824.4, "end": 832.08, "text": " you will notice GPT-4 Turbo becoming a lot faster. We're also decreasing the cost of GPT-3.5 Turbo", "tokens": [50628, 291, 486, 3449, 26039, 51, 12, 19, 35848, 5617, 257, 688, 4663, 13, 492, 434, 611, 23223, 264, 2063, 295, 26039, 51, 12, 18, 13, 20, 35848, 51012], "temperature": 0.0, "avg_logprob": -0.11072039604187012, "compression_ratio": 1.4489795918367347, "no_speech_prob": 0.001098534557968378}, {"id": 123, "seek": 81912, "start": 832.08, "end": 840.64, "text": " 16K. Also, input tokens are 3X less and output tokens are 2X less, which means that GPT-3.5 16K", "tokens": [51012, 3165, 42, 13, 2743, 11, 4846, 22667, 366, 805, 55, 1570, 293, 5598, 22667, 366, 568, 55, 1570, 11, 597, 1355, 300, 26039, 51, 12, 18, 13, 20, 3165, 42, 51440], "temperature": 0.0, "avg_logprob": -0.11072039604187012, "compression_ratio": 1.4489795918367347, "no_speech_prob": 0.001098534557968378}, {"id": 124, "seek": 84064, "start": 840.72, "end": 849.04, "text": " is now cheaper than the previous GPT-3.5 4K model. Running a fine-tuned GPT-3.5 Turbo 16K", "tokens": [50368, 307, 586, 12284, 813, 264, 3894, 26039, 51, 12, 18, 13, 20, 1017, 42, 2316, 13, 28136, 257, 2489, 12, 83, 43703, 26039, 51, 12, 18, 13, 20, 35848, 3165, 42, 50784], "temperature": 0.0, "avg_logprob": -0.0685957120015071, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.01261989213526249}, {"id": 125, "seek": 84064, "start": 849.04, "end": 855.28, "text": " version is also cheaper than the old fine-tuned 4K version. Okay, so we just covered a lot", "tokens": [50784, 3037, 307, 611, 12284, 813, 264, 1331, 2489, 12, 83, 43703, 1017, 42, 3037, 13, 1033, 11, 370, 321, 445, 5343, 257, 688, 51096], "temperature": 0.0, "avg_logprob": -0.0685957120015071, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.01261989213526249}, {"id": 126, "seek": 84064, "start": 855.28, "end": 860.24, "text": " about the model itself. We hope that these changes address your feedback. We're really excited to", "tokens": [51096, 466, 264, 2316, 2564, 13, 492, 1454, 300, 613, 2962, 2985, 428, 5824, 13, 492, 434, 534, 2919, 281, 51344], "temperature": 0.0, "avg_logprob": -0.0685957120015071, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.01261989213526249}, {"id": 127, "seek": 84064, "start": 860.24, "end": 867.28, "text": " bring all of these improvements to everybody now. In all of this, we're lucky to have a partner", "tokens": [51344, 1565, 439, 295, 613, 13797, 281, 2201, 586, 13, 682, 439, 295, 341, 11, 321, 434, 6356, 281, 362, 257, 4975, 51696], "temperature": 0.0, "avg_logprob": -0.0685957120015071, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.01261989213526249}, {"id": 128, "seek": 86728, "start": 867.36, "end": 872.8, "text": " who is instrumental in making it happen. So I'd like to bring out a special guest, Satya Nadella,", "tokens": [50368, 567, 307, 17388, 294, 1455, 309, 1051, 13, 407, 286, 1116, 411, 281, 1565, 484, 257, 2121, 8341, 11, 5344, 3016, 426, 762, 3505, 11, 50640], "temperature": 0.0, "avg_logprob": -0.13319421264360537, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.02403162233531475}, {"id": 129, "seek": 86728, "start": 872.8, "end": 874.0, "text": " the CEO of Microsoft.", "tokens": [50640, 264, 9282, 295, 8116, 13, 50700], "temperature": 0.0, "avg_logprob": -0.13319421264360537, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.02403162233531475}, {"id": 130, "seek": 86728, "start": 879.8399999999999, "end": 885.28, "text": " Good to see you. Thank you so much. Thank you. Satya, thanks so much for coming here.", "tokens": [50992, 2205, 281, 536, 291, 13, 1044, 291, 370, 709, 13, 1044, 291, 13, 5344, 3016, 11, 3231, 370, 709, 337, 1348, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13319421264360537, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.02403162233531475}, {"id": 131, "seek": 86728, "start": 885.28, "end": 890.64, "text": " It's fantastic to be here, and Sam, congrats. I mean, I'm really looking forward to Turbo and", "tokens": [51264, 467, 311, 5456, 281, 312, 510, 11, 293, 4832, 11, 8882, 1720, 13, 286, 914, 11, 286, 478, 534, 1237, 2128, 281, 35848, 293, 51532], "temperature": 0.0, "avg_logprob": -0.13319421264360537, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.02403162233531475}, {"id": 132, "seek": 86728, "start": 890.64, "end": 894.0, "text": " everything else that you have coming. It's been just fantastic partnering with you guys.", "tokens": [51532, 1203, 1646, 300, 291, 362, 1348, 13, 467, 311, 668, 445, 5456, 31290, 365, 291, 1074, 13, 51700], "temperature": 0.0, "avg_logprob": -0.13319421264360537, "compression_ratio": 1.5901639344262295, "no_speech_prob": 0.02403162233531475}, {"id": 133, "seek": 89400, "start": 894.96, "end": 899.12, "text": " Two questions. I won't take too much of your time. How is Microsoft thinking about the partnership", "tokens": [50412, 4453, 1651, 13, 286, 1582, 380, 747, 886, 709, 295, 428, 565, 13, 1012, 307, 8116, 1953, 466, 264, 9982, 50620], "temperature": 0.0, "avg_logprob": -0.10292258407130386, "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.002980221062898636}, {"id": 134, "seek": 89400, "start": 899.12, "end": 910.56, "text": " currently? First, we love you guys. It's been fantastic for us. In fact, I remember the first", "tokens": [50620, 4362, 30, 2386, 11, 321, 959, 291, 1074, 13, 467, 311, 668, 5456, 337, 505, 13, 682, 1186, 11, 286, 1604, 264, 700, 51192], "temperature": 0.0, "avg_logprob": -0.10292258407130386, "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.002980221062898636}, {"id": 135, "seek": 89400, "start": 910.56, "end": 914.32, "text": " time I think you reached out and said, hey, do you have some Azure credits? We've come a long way", "tokens": [51192, 565, 286, 519, 291, 6488, 484, 293, 848, 11, 4177, 11, 360, 291, 362, 512, 11969, 16816, 30, 492, 600, 808, 257, 938, 636, 51380], "temperature": 0.0, "avg_logprob": -0.10292258407130386, "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.002980221062898636}, {"id": 136, "seek": 89400, "start": 914.32, "end": 920.16, "text": " from there. Thank you for those. That was great. You guys have built something magical. I mean,", "tokens": [51380, 490, 456, 13, 1044, 291, 337, 729, 13, 663, 390, 869, 13, 509, 1074, 362, 3094, 746, 12066, 13, 286, 914, 11, 51672], "temperature": 0.0, "avg_logprob": -0.10292258407130386, "compression_ratio": 1.5378486055776892, "no_speech_prob": 0.002980221062898636}, {"id": 137, "seek": 92016, "start": 920.24, "end": 924.9599999999999, "text": " quite frankly, there are two things for us when it comes to the partnership. The first is these", "tokens": [50368, 1596, 11939, 11, 456, 366, 732, 721, 337, 505, 562, 309, 1487, 281, 264, 9982, 13, 440, 700, 307, 613, 50604], "temperature": 0.0, "avg_logprob": -0.09382446606953938, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0030266637913882732}, {"id": 138, "seek": 92016, "start": 924.9599999999999, "end": 929.1999999999999, "text": " workloads. And even when I was listening backstage to how you're describing what's coming even,", "tokens": [50604, 32452, 13, 400, 754, 562, 286, 390, 4764, 31764, 281, 577, 291, 434, 16141, 437, 311, 1348, 754, 11, 50816], "temperature": 0.0, "avg_logprob": -0.09382446606953938, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0030266637913882732}, {"id": 139, "seek": 92016, "start": 929.1999999999999, "end": 933.76, "text": " it's just so different and new. I've been in this infrastructure business for three decades.", "tokens": [50816, 309, 311, 445, 370, 819, 293, 777, 13, 286, 600, 668, 294, 341, 6896, 1606, 337, 1045, 7878, 13, 51044], "temperature": 0.0, "avg_logprob": -0.09382446606953938, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0030266637913882732}, {"id": 140, "seek": 92016, "start": 933.76, "end": 938.48, "text": " No one has ever seen infrastructure like this. And the workload or the pattern of the workload,", "tokens": [51044, 883, 472, 575, 1562, 1612, 6896, 411, 341, 13, 400, 264, 20139, 420, 264, 5102, 295, 264, 20139, 11, 51280], "temperature": 0.0, "avg_logprob": -0.09382446606953938, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0030266637913882732}, {"id": 141, "seek": 92016, "start": 938.48, "end": 945.92, "text": " these training jobs are so synchronous and so large and so data parallel. And so the first thing", "tokens": [51280, 613, 3097, 4782, 366, 370, 44743, 293, 370, 2416, 293, 370, 1412, 8952, 13, 400, 370, 264, 700, 551, 51652], "temperature": 0.0, "avg_logprob": -0.09382446606953938, "compression_ratio": 1.7158273381294964, "no_speech_prob": 0.0030266637913882732}, {"id": 142, "seek": 94592, "start": 945.92, "end": 950.8, "text": " that we have been doing is building in partnership with you the system all the way from thinking from", "tokens": [50364, 300, 321, 362, 668, 884, 307, 2390, 294, 9982, 365, 291, 264, 1185, 439, 264, 636, 490, 1953, 490, 50608], "temperature": 0.0, "avg_logprob": -0.09508878534490411, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.002550016390159726}, {"id": 143, "seek": 94592, "start": 950.8, "end": 959.36, "text": " power to the DC to the rack to the accelerators to the network. And just really the shape of Azure", "tokens": [50608, 1347, 281, 264, 9114, 281, 264, 14788, 281, 264, 10172, 3391, 281, 264, 3209, 13, 400, 445, 534, 264, 3909, 295, 11969, 51036], "temperature": 0.0, "avg_logprob": -0.09508878534490411, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.002550016390159726}, {"id": 144, "seek": 94592, "start": 959.36, "end": 965.1999999999999, "text": " is drastically changed and is changing rapidly in support of these models that you're building.", "tokens": [51036, 307, 29673, 3105, 293, 307, 4473, 12910, 294, 1406, 295, 613, 5245, 300, 291, 434, 2390, 13, 51328], "temperature": 0.0, "avg_logprob": -0.09508878534490411, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.002550016390159726}, {"id": 145, "seek": 94592, "start": 965.8399999999999, "end": 971.1999999999999, "text": " And so our job number one is to build the best system so that you can build the best models", "tokens": [51360, 400, 370, 527, 1691, 1230, 472, 307, 281, 1322, 264, 1151, 1185, 370, 300, 291, 393, 1322, 264, 1151, 5245, 51628], "temperature": 0.0, "avg_logprob": -0.09508878534490411, "compression_ratio": 1.7477477477477477, "no_speech_prob": 0.002550016390159726}, {"id": 146, "seek": 97120, "start": 971.2, "end": 975.76, "text": " and then make that all available to developers. And so the other thing is we ourselves are", "tokens": [50364, 293, 550, 652, 300, 439, 2435, 281, 8849, 13, 400, 370, 264, 661, 551, 307, 321, 4175, 366, 50592], "temperature": 0.0, "avg_logprob": -0.1164104355706109, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.0031220889650285244}, {"id": 147, "seek": 97120, "start": 975.76, "end": 981.36, "text": " developers. So we're building products. In fact, my own conviction of this entire generation of", "tokens": [50592, 8849, 13, 407, 321, 434, 2390, 3383, 13, 682, 1186, 11, 452, 1065, 24837, 295, 341, 2302, 5125, 295, 50872], "temperature": 0.0, "avg_logprob": -0.1164104355706109, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.0031220889650285244}, {"id": 148, "seek": 97120, "start": 981.36, "end": 989.44, "text": " Foundation models completely changed the first time I saw GitHub Copilot on GPT. And so we want", "tokens": [50872, 10335, 5245, 2584, 3105, 264, 700, 565, 286, 1866, 23331, 11579, 31516, 322, 26039, 51, 13, 400, 370, 321, 528, 51276], "temperature": 0.0, "avg_logprob": -0.1164104355706109, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.0031220889650285244}, {"id": 149, "seek": 97120, "start": 989.44, "end": 996.24, "text": " to build our Copilot, GitHub Copilot all as developers on top of OpenAI APIs. And so we are", "tokens": [51276, 281, 1322, 527, 11579, 31516, 11, 23331, 11579, 31516, 439, 382, 8849, 322, 1192, 295, 7238, 48698, 21445, 13, 400, 370, 321, 366, 51616], "temperature": 0.0, "avg_logprob": -0.1164104355706109, "compression_ratio": 1.6403508771929824, "no_speech_prob": 0.0031220889650285244}, {"id": 150, "seek": 99624, "start": 996.24, "end": 1000.72, "text": " very, very committed to that. And what does that mean to developers? You know, look, I always think", "tokens": [50364, 588, 11, 588, 7784, 281, 300, 13, 400, 437, 775, 300, 914, 281, 8849, 30, 509, 458, 11, 574, 11, 286, 1009, 519, 50588], "temperature": 0.0, "avg_logprob": -0.1107432862632295, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.004132471047341824}, {"id": 151, "seek": 99624, "start": 1000.72, "end": 1006.16, "text": " of Microsoft as a platform company, a developer company and a partner company. And so we want to", "tokens": [50588, 295, 8116, 382, 257, 3663, 2237, 11, 257, 10754, 2237, 293, 257, 4975, 2237, 13, 400, 370, 321, 528, 281, 50860], "temperature": 0.0, "avg_logprob": -0.1107432862632295, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.004132471047341824}, {"id": 152, "seek": 99624, "start": 1006.16, "end": 1010.88, "text": " make, you know, for example, we want to make GitHub available, GitHub Copilot available as the", "tokens": [50860, 652, 11, 291, 458, 11, 337, 1365, 11, 321, 528, 281, 652, 23331, 2435, 11, 23331, 11579, 31516, 2435, 382, 264, 51096], "temperature": 0.0, "avg_logprob": -0.1107432862632295, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.004132471047341824}, {"id": 153, "seek": 99624, "start": 1010.88, "end": 1015.2, "text": " enterprise edition available to all the attendees here so that they can try it out. That's awesome.", "tokens": [51096, 14132, 11377, 2435, 281, 439, 264, 34826, 510, 370, 300, 436, 393, 853, 309, 484, 13, 663, 311, 3476, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1107432862632295, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.004132471047341824}, {"id": 154, "seek": 99624, "start": 1015.2, "end": 1024.48, "text": " Yeah, we're very excited about that. And you can count on us to build the best infrastructure in", "tokens": [51312, 865, 11, 321, 434, 588, 2919, 466, 300, 13, 400, 291, 393, 1207, 322, 505, 281, 1322, 264, 1151, 6896, 294, 51776], "temperature": 0.0, "avg_logprob": -0.1107432862632295, "compression_ratio": 1.7183098591549295, "no_speech_prob": 0.004132471047341824}, {"id": 155, "seek": 102448, "start": 1024.48, "end": 1031.04, "text": " Azure with your API support and bring it to all of you and then even things like the Azure marketplace.", "tokens": [50364, 11969, 365, 428, 9362, 1406, 293, 1565, 309, 281, 439, 295, 291, 293, 550, 754, 721, 411, 264, 11969, 19455, 13, 50692], "temperature": 0.0, "avg_logprob": -0.1408324737548828, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.003592895111069083}, {"id": 156, "seek": 102448, "start": 1031.04, "end": 1035.76, "text": " So for developers, we're building products out here to get to market rapidly. So that's sort of", "tokens": [50692, 407, 337, 8849, 11, 321, 434, 2390, 3383, 484, 510, 281, 483, 281, 2142, 12910, 13, 407, 300, 311, 1333, 295, 50928], "temperature": 0.0, "avg_logprob": -0.1408324737548828, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.003592895111069083}, {"id": 157, "seek": 102448, "start": 1035.76, "end": 1040.56, "text": " really our intent here. Great. And how do you think about the future? Future of the partnership or", "tokens": [50928, 534, 527, 8446, 510, 13, 3769, 13, 400, 577, 360, 291, 519, 466, 264, 2027, 30, 20805, 295, 264, 9982, 420, 51168], "temperature": 0.0, "avg_logprob": -0.1408324737548828, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.003592895111069083}, {"id": 158, "seek": 102448, "start": 1040.56, "end": 1047.52, "text": " future of AI or whatever? Yeah, there's anything you want. That's, you know, like, there are a", "tokens": [51168, 2027, 295, 7318, 420, 2035, 30, 865, 11, 456, 311, 1340, 291, 528, 13, 663, 311, 11, 291, 458, 11, 411, 11, 456, 366, 257, 51516], "temperature": 0.0, "avg_logprob": -0.1408324737548828, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.003592895111069083}, {"id": 159, "seek": 102448, "start": 1047.52, "end": 1052.72, "text": " couple of things for me that I think are going to be very, very key for us, right? One is I just", "tokens": [51516, 1916, 295, 721, 337, 385, 300, 286, 519, 366, 516, 281, 312, 588, 11, 588, 2141, 337, 505, 11, 558, 30, 1485, 307, 286, 445, 51776], "temperature": 0.0, "avg_logprob": -0.1408324737548828, "compression_ratio": 1.6610169491525424, "no_speech_prob": 0.003592895111069083}, {"id": 160, "seek": 105272, "start": 1052.72, "end": 1060.0, "text": " described how the systems that are needed as you aggressively push forward on your roadmap", "tokens": [50364, 7619, 577, 264, 3652, 300, 366, 2978, 382, 291, 32024, 2944, 2128, 322, 428, 35738, 50728], "temperature": 0.0, "avg_logprob": -0.0756075353507536, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.020304838195443153}, {"id": 161, "seek": 105272, "start": 1061.1200000000001, "end": 1066.96, "text": " requires us to be on the top of our game. And we intend fully to commit ourselves deeply to", "tokens": [50784, 7029, 505, 281, 312, 322, 264, 1192, 295, 527, 1216, 13, 400, 321, 19759, 4498, 281, 5599, 4175, 8760, 281, 51076], "temperature": 0.0, "avg_logprob": -0.0756075353507536, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.020304838195443153}, {"id": 162, "seek": 105272, "start": 1066.96, "end": 1074.0, "text": " making sure you all as builders of these foundation models have not only the best systems for training", "tokens": [51076, 1455, 988, 291, 439, 382, 36281, 295, 613, 7030, 5245, 362, 406, 787, 264, 1151, 3652, 337, 3097, 51428], "temperature": 0.0, "avg_logprob": -0.0756075353507536, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.020304838195443153}, {"id": 163, "seek": 105272, "start": 1074.0, "end": 1080.48, "text": " and inference, but the most compute so that you can keep pushing forward on the frontiers. Because", "tokens": [51428, 293, 38253, 11, 457, 264, 881, 14722, 370, 300, 291, 393, 1066, 7380, 2128, 322, 264, 1868, 4890, 13, 1436, 51752], "temperature": 0.0, "avg_logprob": -0.0756075353507536, "compression_ratio": 1.6271186440677967, "no_speech_prob": 0.020304838195443153}, {"id": 164, "seek": 108048, "start": 1080.48, "end": 1084.56, "text": " I think that that's the way we're going to make progress. The second thing I think both of us care", "tokens": [50364, 286, 519, 300, 300, 311, 264, 636, 321, 434, 516, 281, 652, 4205, 13, 440, 1150, 551, 286, 519, 1293, 295, 505, 1127, 50568], "temperature": 0.0, "avg_logprob": -0.0789796294068261, "compression_ratio": 1.7561728395061729, "no_speech_prob": 0.0038814935833215714}, {"id": 165, "seek": 108048, "start": 1084.56, "end": 1089.92, "text": " about, in fact, quite frankly, the thing that excited both sides to come together is your mission", "tokens": [50568, 466, 11, 294, 1186, 11, 1596, 11939, 11, 264, 551, 300, 2919, 1293, 4881, 281, 808, 1214, 307, 428, 4447, 50836], "temperature": 0.0, "avg_logprob": -0.0789796294068261, "compression_ratio": 1.7561728395061729, "no_speech_prob": 0.0038814935833215714}, {"id": 166, "seek": 108048, "start": 1089.92, "end": 1094.4, "text": " and our mission. Our mission is to empower every person in every organization on the planet to", "tokens": [50836, 293, 527, 4447, 13, 2621, 4447, 307, 281, 11071, 633, 954, 294, 633, 4475, 322, 264, 5054, 281, 51060], "temperature": 0.0, "avg_logprob": -0.0789796294068261, "compression_ratio": 1.7561728395061729, "no_speech_prob": 0.0038814935833215714}, {"id": 167, "seek": 108048, "start": 1094.4, "end": 1099.3600000000001, "text": " achieve more. And to me, ultimately, AI is only going to be useful if it truly does empower,", "tokens": [51060, 4584, 544, 13, 400, 281, 385, 11, 6284, 11, 7318, 307, 787, 516, 281, 312, 4420, 498, 309, 4908, 775, 11071, 11, 51308], "temperature": 0.0, "avg_logprob": -0.0789796294068261, "compression_ratio": 1.7561728395061729, "no_speech_prob": 0.0038814935833215714}, {"id": 168, "seek": 108048, "start": 1099.3600000000001, "end": 1103.3600000000001, "text": " right? I mean, I saw the video you played early. I mean, that was fantastic to see", "tokens": [51308, 558, 30, 286, 914, 11, 286, 1866, 264, 960, 291, 3737, 2440, 13, 286, 914, 11, 300, 390, 5456, 281, 536, 51508], "temperature": 0.0, "avg_logprob": -0.0789796294068261, "compression_ratio": 1.7561728395061729, "no_speech_prob": 0.0038814935833215714}, {"id": 169, "seek": 108048, "start": 1104.4, "end": 1109.84, "text": " hear those voices, describe what AI meant for them and what they were able to achieve. So ultimately,", "tokens": [51560, 1568, 729, 9802, 11, 6786, 437, 7318, 4140, 337, 552, 293, 437, 436, 645, 1075, 281, 4584, 13, 407, 6284, 11, 51832], "temperature": 0.0, "avg_logprob": -0.0789796294068261, "compression_ratio": 1.7561728395061729, "no_speech_prob": 0.0038814935833215714}, {"id": 170, "seek": 110984, "start": 1109.84, "end": 1115.36, "text": " it's about being able to get the benefits of AI broadly disseminated to everyone, I think is", "tokens": [50364, 309, 311, 466, 885, 1075, 281, 483, 264, 5311, 295, 7318, 19511, 34585, 770, 281, 1518, 11, 286, 519, 307, 50640], "temperature": 0.0, "avg_logprob": -0.09558000797178687, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0011328624095767736}, {"id": 171, "seek": 110984, "start": 1115.36, "end": 1118.56, "text": " going to be the goal for us. And then the last thing is, of course, we're very grounded in the", "tokens": [50640, 516, 281, 312, 264, 3387, 337, 505, 13, 400, 550, 264, 1036, 551, 307, 11, 295, 1164, 11, 321, 434, 588, 23535, 294, 264, 50800], "temperature": 0.0, "avg_logprob": -0.09558000797178687, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0011328624095767736}, {"id": 172, "seek": 110984, "start": 1118.56, "end": 1122.24, "text": " fact that safety matters and safety is not something that you'd care about later, but it's", "tokens": [50800, 1186, 300, 4514, 7001, 293, 4514, 307, 406, 746, 300, 291, 1116, 1127, 466, 1780, 11, 457, 309, 311, 50984], "temperature": 0.0, "avg_logprob": -0.09558000797178687, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0011328624095767736}, {"id": 173, "seek": 110984, "start": 1122.24, "end": 1127.6799999999998, "text": " something we do shift left on. And we're very focused on that with you all. Great. Well, I think", "tokens": [50984, 746, 321, 360, 5513, 1411, 322, 13, 400, 321, 434, 588, 5178, 322, 300, 365, 291, 439, 13, 3769, 13, 1042, 11, 286, 519, 51256], "temperature": 0.0, "avg_logprob": -0.09558000797178687, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0011328624095767736}, {"id": 174, "seek": 110984, "start": 1127.6799999999998, "end": 1131.36, "text": " we have the best partnership in tech. I'm excited for us to build AGI together. I'm really excited.", "tokens": [51256, 321, 362, 264, 1151, 9982, 294, 7553, 13, 286, 478, 2919, 337, 505, 281, 1322, 316, 26252, 1214, 13, 286, 478, 534, 2919, 13, 51440], "temperature": 0.0, "avg_logprob": -0.09558000797178687, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0011328624095767736}, {"id": 175, "seek": 113136, "start": 1131.84, "end": 1145.1999999999998, "text": " Thank you very much for coming. Thank you so much. Okay. So we have shared a lot of great", "tokens": [50388, 1044, 291, 588, 709, 337, 1348, 13, 1044, 291, 370, 709, 13, 1033, 13, 407, 321, 362, 5507, 257, 688, 295, 869, 51056], "temperature": 0.0, "avg_logprob": -0.17703102730415962, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.016135042533278465}, {"id": 176, "seek": 113136, "start": 1145.1999999999998, "end": 1150.1599999999999, "text": " updates for developers already, and we got a lot more to come. But even though this is developer", "tokens": [51056, 9205, 337, 8849, 1217, 11, 293, 321, 658, 257, 688, 544, 281, 808, 13, 583, 754, 1673, 341, 307, 10754, 51304], "temperature": 0.0, "avg_logprob": -0.17703102730415962, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.016135042533278465}, {"id": 177, "seek": 113136, "start": 1150.1599999999999, "end": 1158.0, "text": " conference, we can't resist making some improvements to chat GPT. So a small one, chat GPT now uses", "tokens": [51304, 7586, 11, 321, 393, 380, 4597, 1455, 512, 13797, 281, 5081, 26039, 51, 13, 407, 257, 1359, 472, 11, 5081, 26039, 51, 586, 4960, 51696], "temperature": 0.0, "avg_logprob": -0.17703102730415962, "compression_ratio": 1.4973821989528795, "no_speech_prob": 0.016135042533278465}, {"id": 178, "seek": 115800, "start": 1158.08, "end": 1162.32, "text": " GPT-4 Turbo with all the latest improvements, including the latest knowledge cutoff, which", "tokens": [50368, 26039, 51, 12, 19, 35848, 365, 439, 264, 6792, 13797, 11, 3009, 264, 6792, 3601, 1723, 4506, 11, 597, 50580], "temperature": 0.0, "avg_logprob": -0.14410960985266644, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.03900233283638954}, {"id": 179, "seek": 115800, "start": 1162.32, "end": 1168.0, "text": " will continue to update. That's all live today. It can now browse the web when it needs to write", "tokens": [50580, 486, 2354, 281, 5623, 13, 663, 311, 439, 1621, 965, 13, 467, 393, 586, 31442, 264, 3670, 562, 309, 2203, 281, 2464, 50864], "temperature": 0.0, "avg_logprob": -0.14410960985266644, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.03900233283638954}, {"id": 180, "seek": 115800, "start": 1168.0, "end": 1173.04, "text": " and run code, analyze data, take and generate images, and much more. And we heard your feedback,", "tokens": [50864, 293, 1190, 3089, 11, 12477, 1412, 11, 747, 293, 8460, 5267, 11, 293, 709, 544, 13, 400, 321, 2198, 428, 5824, 11, 51116], "temperature": 0.0, "avg_logprob": -0.14410960985266644, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.03900233283638954}, {"id": 181, "seek": 115800, "start": 1173.04, "end": 1177.52, "text": " that model picker, extremely annoying. That is gone starting today. You will not have to click", "tokens": [51116, 300, 2316, 1888, 260, 11, 4664, 11304, 13, 663, 307, 2780, 2891, 965, 13, 509, 486, 406, 362, 281, 2052, 51340], "temperature": 0.0, "avg_logprob": -0.14410960985266644, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.03900233283638954}, {"id": 182, "seek": 115800, "start": 1177.52, "end": 1181.92, "text": " around to drop down menu. All of this will just work together. Chat GPT, yeah.", "tokens": [51340, 926, 281, 3270, 760, 6510, 13, 1057, 295, 341, 486, 445, 589, 1214, 13, 27503, 26039, 51, 11, 1338, 13, 51560], "temperature": 0.0, "avg_logprob": -0.14410960985266644, "compression_ratio": 1.5578231292517006, "no_speech_prob": 0.03900233283638954}, {"id": 183, "seek": 118192, "start": 1182.5600000000002, "end": 1192.72, "text": " Chat GPT will just know what to use and when you need it. But that's not the main thing.", "tokens": [50396, 27503, 26039, 51, 486, 445, 458, 437, 281, 764, 293, 562, 291, 643, 309, 13, 583, 300, 311, 406, 264, 2135, 551, 13, 50904], "temperature": 0.0, "avg_logprob": -0.11920340688605058, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.0004305221082177013}, {"id": 184, "seek": 118192, "start": 1194.5600000000002, "end": 1199.04, "text": " And neither was price, actually, the main developer request. There was one that was even", "tokens": [50996, 400, 9662, 390, 3218, 11, 767, 11, 264, 2135, 10754, 5308, 13, 821, 390, 472, 300, 390, 754, 51220], "temperature": 0.0, "avg_logprob": -0.11920340688605058, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.0004305221082177013}, {"id": 185, "seek": 118192, "start": 1199.04, "end": 1203.68, "text": " bigger than that. And I want to talk about where we're headed, and the main thing we're here to", "tokens": [51220, 3801, 813, 300, 13, 400, 286, 528, 281, 751, 466, 689, 321, 434, 12798, 11, 293, 264, 2135, 551, 321, 434, 510, 281, 51452], "temperature": 0.0, "avg_logprob": -0.11920340688605058, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.0004305221082177013}, {"id": 186, "seek": 118192, "start": 1203.68, "end": 1209.92, "text": " talk about today. So we believe that if you give people better tools, they will do amazing things.", "tokens": [51452, 751, 466, 965, 13, 407, 321, 1697, 300, 498, 291, 976, 561, 1101, 3873, 11, 436, 486, 360, 2243, 721, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11920340688605058, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.0004305221082177013}, {"id": 187, "seek": 120992, "start": 1210.48, "end": 1214.88, "text": " We know that people want AI that is smarter, more personal, more customizable,", "tokens": [50392, 492, 458, 300, 561, 528, 7318, 300, 307, 20294, 11, 544, 2973, 11, 544, 47922, 11, 50612], "temperature": 0.0, "avg_logprob": -0.07712791122008707, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.0010000660549849272}, {"id": 188, "seek": 120992, "start": 1214.88, "end": 1219.76, "text": " can do more on your behalf. Eventually, you'll just ask a computer for what you need,", "tokens": [50612, 393, 360, 544, 322, 428, 9490, 13, 17586, 11, 291, 603, 445, 1029, 257, 3820, 337, 437, 291, 643, 11, 50856], "temperature": 0.0, "avg_logprob": -0.07712791122008707, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.0010000660549849272}, {"id": 189, "seek": 120992, "start": 1220.5600000000002, "end": 1226.16, "text": " and it will do all of these tasks for you. These capabilities are often talked in the AI field", "tokens": [50896, 293, 309, 486, 360, 439, 295, 613, 9608, 337, 291, 13, 1981, 10862, 366, 2049, 2825, 294, 264, 7318, 2519, 51176], "temperature": 0.0, "avg_logprob": -0.07712791122008707, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.0010000660549849272}, {"id": 190, "seek": 120992, "start": 1226.16, "end": 1234.0800000000002, "text": " about as agents. The upsides of this are going to be tremendous. At OpenAI, we really believe", "tokens": [51176, 466, 382, 12554, 13, 440, 15497, 1875, 295, 341, 366, 516, 281, 312, 10048, 13, 1711, 7238, 48698, 11, 321, 534, 1697, 51572], "temperature": 0.0, "avg_logprob": -0.07712791122008707, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.0010000660549849272}, {"id": 191, "seek": 120992, "start": 1234.0800000000002, "end": 1238.88, "text": " that gradual iterative deployment is the best way to address the safety issues, the safety", "tokens": [51572, 300, 32890, 17138, 1166, 19317, 307, 264, 1151, 636, 281, 2985, 264, 4514, 2663, 11, 264, 4514, 51812], "temperature": 0.0, "avg_logprob": -0.07712791122008707, "compression_ratio": 1.6263736263736264, "no_speech_prob": 0.0010000660549849272}, {"id": 192, "seek": 123888, "start": 1238.88, "end": 1243.7600000000002, "text": " challenges with AI. We think it's especially important to move carefully towards this future", "tokens": [50364, 4759, 365, 7318, 13, 492, 519, 309, 311, 2318, 1021, 281, 1286, 7500, 3030, 341, 2027, 50608], "temperature": 0.0, "avg_logprob": -0.06996746720938847, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0004581620160024613}, {"id": 193, "seek": 123888, "start": 1243.7600000000002, "end": 1248.96, "text": " of agents. It's going to require a lot of technical work and a lot of thoughtful consideration by", "tokens": [50608, 295, 12554, 13, 467, 311, 516, 281, 3651, 257, 688, 295, 6191, 589, 293, 257, 688, 295, 21566, 12381, 538, 50868], "temperature": 0.0, "avg_logprob": -0.06996746720938847, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0004581620160024613}, {"id": 194, "seek": 123888, "start": 1248.96, "end": 1256.48, "text": " society. So today, we're taking our first small step that moves us towards this future. We're", "tokens": [50868, 4086, 13, 407, 965, 11, 321, 434, 1940, 527, 700, 1359, 1823, 300, 6067, 505, 3030, 341, 2027, 13, 492, 434, 51244], "temperature": 0.0, "avg_logprob": -0.06996746720938847, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0004581620160024613}, {"id": 195, "seek": 123888, "start": 1256.48, "end": 1265.92, "text": " thrilled to introduce GPTs. GPTs are tailored versions of Chat GPT for a specific purpose.", "tokens": [51244, 18744, 281, 5366, 26039, 33424, 13, 26039, 33424, 366, 34858, 9606, 295, 27503, 26039, 51, 337, 257, 2685, 4334, 13, 51716], "temperature": 0.0, "avg_logprob": -0.06996746720938847, "compression_ratio": 1.5822784810126582, "no_speech_prob": 0.0004581620160024613}, {"id": 196, "seek": 126592, "start": 1266.88, "end": 1273.1200000000001, "text": " You can build a GPT, a customized version of Chat GPT for almost anything, with instructions,", "tokens": [50412, 509, 393, 1322, 257, 26039, 51, 11, 257, 30581, 3037, 295, 27503, 26039, 51, 337, 1920, 1340, 11, 365, 9415, 11, 50724], "temperature": 0.0, "avg_logprob": -0.09607181734251745, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0008294214494526386}, {"id": 197, "seek": 126592, "start": 1273.76, "end": 1278.0800000000002, "text": " expanded knowledge, and actions, and then you can publish it for others to use.", "tokens": [50756, 14342, 3601, 11, 293, 5909, 11, 293, 550, 291, 393, 11374, 309, 337, 2357, 281, 764, 13, 50972], "temperature": 0.0, "avg_logprob": -0.09607181734251745, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0008294214494526386}, {"id": 198, "seek": 126592, "start": 1279.3600000000001, "end": 1282.88, "text": " And because they combine instructions, expanded knowledge, and actions,", "tokens": [51036, 400, 570, 436, 10432, 9415, 11, 14342, 3601, 11, 293, 5909, 11, 51212], "temperature": 0.0, "avg_logprob": -0.09607181734251745, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0008294214494526386}, {"id": 199, "seek": 126592, "start": 1283.76, "end": 1287.76, "text": " they can be more helpful to you. They can work better in many contexts, and they can give you", "tokens": [51256, 436, 393, 312, 544, 4961, 281, 291, 13, 814, 393, 589, 1101, 294, 867, 30628, 11, 293, 436, 393, 976, 291, 51456], "temperature": 0.0, "avg_logprob": -0.09607181734251745, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0008294214494526386}, {"id": 200, "seek": 126592, "start": 1287.76, "end": 1293.2, "text": " better control. They'll make it easier for you to accomplish all sorts of tasks or just have more", "tokens": [51456, 1101, 1969, 13, 814, 603, 652, 309, 3571, 337, 291, 281, 9021, 439, 7527, 295, 9608, 420, 445, 362, 544, 51728], "temperature": 0.0, "avg_logprob": -0.09607181734251745, "compression_ratio": 1.7692307692307692, "no_speech_prob": 0.0008294214494526386}, {"id": 201, "seek": 129320, "start": 1293.28, "end": 1300.4, "text": " fun, and you'll be able to use them right within Chat GPT. You can, in effect, program a GPT with", "tokens": [50368, 1019, 11, 293, 291, 603, 312, 1075, 281, 764, 552, 558, 1951, 27503, 26039, 51, 13, 509, 393, 11, 294, 1802, 11, 1461, 257, 26039, 51, 365, 50724], "temperature": 0.0, "avg_logprob": -0.06543269233098106, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0005975487874820828}, {"id": 202, "seek": 129320, "start": 1300.4, "end": 1305.2, "text": " language just by talking to it. It's easy to customize the behavior so that it fits what you", "tokens": [50724, 2856, 445, 538, 1417, 281, 309, 13, 467, 311, 1858, 281, 19734, 264, 5223, 370, 300, 309, 9001, 437, 291, 50964], "temperature": 0.0, "avg_logprob": -0.06543269233098106, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0005975487874820828}, {"id": 203, "seek": 129320, "start": 1305.2, "end": 1312.0800000000002, "text": " want. This makes building them very accessible, and it gives agency to everyone. So we're going to", "tokens": [50964, 528, 13, 639, 1669, 2390, 552, 588, 9515, 11, 293, 309, 2709, 7934, 281, 1518, 13, 407, 321, 434, 516, 281, 51308], "temperature": 0.0, "avg_logprob": -0.06543269233098106, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0005975487874820828}, {"id": 204, "seek": 129320, "start": 1312.0800000000002, "end": 1317.8400000000001, "text": " show you what GPTs are, how to use them, how to build them, and then we're going to talk about how", "tokens": [51308, 855, 291, 437, 26039, 33424, 366, 11, 577, 281, 764, 552, 11, 577, 281, 1322, 552, 11, 293, 550, 321, 434, 516, 281, 751, 466, 577, 51596], "temperature": 0.0, "avg_logprob": -0.06543269233098106, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0005975487874820828}, {"id": 205, "seek": 129320, "start": 1317.8400000000001, "end": 1322.0, "text": " they'll be distributed and discovered. And then after that, for developers, we're going to show", "tokens": [51596, 436, 603, 312, 12631, 293, 6941, 13, 400, 550, 934, 300, 11, 337, 8849, 11, 321, 434, 516, 281, 855, 51804], "temperature": 0.0, "avg_logprob": -0.06543269233098106, "compression_ratio": 1.7347670250896057, "no_speech_prob": 0.0005975487874820828}, {"id": 206, "seek": 132200, "start": 1322.0, "end": 1327.84, "text": " you how to build these agent-like experiences into your own apps. So first, let's look at a few", "tokens": [50364, 291, 577, 281, 1322, 613, 9461, 12, 4092, 5235, 666, 428, 1065, 7733, 13, 407, 700, 11, 718, 311, 574, 412, 257, 1326, 50656], "temperature": 0.0, "avg_logprob": -0.06889780001206831, "compression_ratio": 1.532520325203252, "no_speech_prob": 0.0016476152231916785}, {"id": 207, "seek": 132200, "start": 1327.84, "end": 1334.64, "text": " examples. Our partners at Code.org are working hard to expand computer science in schools.", "tokens": [50656, 5110, 13, 2621, 4462, 412, 15549, 13, 4646, 366, 1364, 1152, 281, 5268, 3820, 3497, 294, 4656, 13, 50996], "temperature": 0.0, "avg_logprob": -0.06889780001206831, "compression_ratio": 1.532520325203252, "no_speech_prob": 0.0016476152231916785}, {"id": 208, "seek": 132200, "start": 1335.36, "end": 1341.28, "text": " They've got a curriculum that is used by tens of millions of students worldwide. Code.org crafted", "tokens": [51032, 814, 600, 658, 257, 14302, 300, 307, 1143, 538, 10688, 295, 6803, 295, 1731, 13485, 13, 15549, 13, 4646, 36213, 51328], "temperature": 0.0, "avg_logprob": -0.06889780001206831, "compression_ratio": 1.532520325203252, "no_speech_prob": 0.0016476152231916785}, {"id": 209, "seek": 132200, "start": 1341.28, "end": 1345.92, "text": " lesson planner GPT to help teachers provide a more engaging experience for middle schoolers.", "tokens": [51328, 6898, 31268, 26039, 51, 281, 854, 6023, 2893, 257, 544, 11268, 1752, 337, 2808, 1395, 433, 13, 51560], "temperature": 0.0, "avg_logprob": -0.06889780001206831, "compression_ratio": 1.532520325203252, "no_speech_prob": 0.0016476152231916785}, {"id": 210, "seek": 134592, "start": 1346.16, "end": 1352.0, "text": " If a teacher asks it to explain four loops in a creative way, it does just that. In this case,", "tokens": [50376, 759, 257, 5027, 8962, 309, 281, 2903, 1451, 16121, 294, 257, 5880, 636, 11, 309, 775, 445, 300, 13, 682, 341, 1389, 11, 50668], "temperature": 0.0, "avg_logprob": -0.18322031838553293, "compression_ratio": 1.5758620689655172, "no_speech_prob": 0.009410389699041843}, {"id": 211, "seek": 134592, "start": 1352.0, "end": 1356.48, "text": " it'll do it in terms of a video game character, repeatedly picking up coins, super easy to", "tokens": [50668, 309, 603, 360, 309, 294, 2115, 295, 257, 960, 1216, 2517, 11, 18227, 8867, 493, 13561, 11, 1687, 1858, 281, 50892], "temperature": 0.0, "avg_logprob": -0.18322031838553293, "compression_ratio": 1.5758620689655172, "no_speech_prob": 0.009410389699041843}, {"id": 212, "seek": 134592, "start": 1356.48, "end": 1362.5600000000002, "text": " understand for an eighth grader. As you can see, this GPT brings together Code.org's extensive", "tokens": [50892, 1223, 337, 364, 19495, 2771, 260, 13, 1018, 291, 393, 536, 11, 341, 26039, 51, 5607, 1214, 15549, 13, 4646, 311, 13246, 51196], "temperature": 0.0, "avg_logprob": -0.18322031838553293, "compression_ratio": 1.5758620689655172, "no_speech_prob": 0.009410389699041843}, {"id": 213, "seek": 134592, "start": 1362.5600000000002, "end": 1367.2, "text": " curriculum and expertise and lets teachers adapt it to their needs quickly and easily.", "tokens": [51196, 14302, 293, 11769, 293, 6653, 6023, 6231, 309, 281, 641, 2203, 2661, 293, 3612, 13, 51428], "temperature": 0.0, "avg_logprob": -0.18322031838553293, "compression_ratio": 1.5758620689655172, "no_speech_prob": 0.009410389699041843}, {"id": 214, "seek": 134592, "start": 1368.48, "end": 1374.96, "text": " Next, Canva has built a GPT that lets you start designing by describing what you want and", "tokens": [51492, 3087, 11, 1664, 2757, 575, 3094, 257, 26039, 51, 300, 6653, 291, 722, 14685, 538, 16141, 437, 291, 528, 293, 51816], "temperature": 0.0, "avg_logprob": -0.18322031838553293, "compression_ratio": 1.5758620689655172, "no_speech_prob": 0.009410389699041843}, {"id": 215, "seek": 137496, "start": 1375.92, "end": 1381.8400000000001, "text": " natural language. If you say, make a poster for a Dev Day reception this afternoon, this evening,", "tokens": [50412, 3303, 2856, 13, 759, 291, 584, 11, 652, 257, 17171, 337, 257, 9096, 5226, 21682, 341, 6499, 11, 341, 5634, 11, 50708], "temperature": 0.0, "avg_logprob": -0.1221337870547646, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0004440777120180428}, {"id": 216, "seek": 137496, "start": 1381.8400000000001, "end": 1386.56, "text": " and you give it some details, it'll generate a few options to start with by hitting Canva's", "tokens": [50708, 293, 291, 976, 309, 512, 4365, 11, 309, 603, 8460, 257, 1326, 3956, 281, 722, 365, 538, 8850, 1664, 2757, 311, 50944], "temperature": 0.0, "avg_logprob": -0.1221337870547646, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0004440777120180428}, {"id": 217, "seek": 137496, "start": 1386.56, "end": 1392.24, "text": " APIs. Now, this concept may be familiar to some of you. We've evolved our plugins to be custom", "tokens": [50944, 21445, 13, 823, 11, 341, 3410, 815, 312, 4963, 281, 512, 295, 291, 13, 492, 600, 14178, 527, 33759, 281, 312, 2375, 51228], "temperature": 0.0, "avg_logprob": -0.1221337870547646, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0004440777120180428}, {"id": 218, "seek": 137496, "start": 1392.24, "end": 1398.56, "text": " actions for GPTs. You can keep chatting with this to see different iterations, and when you see one", "tokens": [51228, 5909, 337, 26039, 33424, 13, 509, 393, 1066, 24654, 365, 341, 281, 536, 819, 36540, 11, 293, 562, 291, 536, 472, 51544], "temperature": 0.0, "avg_logprob": -0.1221337870547646, "compression_ratio": 1.5421686746987953, "no_speech_prob": 0.0004440777120180428}, {"id": 219, "seek": 139856, "start": 1398.56, "end": 1405.52, "text": " you like, you can click through to Canva for the full design experience. So now, we'd like to show", "tokens": [50364, 291, 411, 11, 291, 393, 2052, 807, 281, 1664, 2757, 337, 264, 1577, 1715, 1752, 13, 407, 586, 11, 321, 1116, 411, 281, 855, 50712], "temperature": 0.0, "avg_logprob": -0.104712819399899, "compression_ratio": 1.4467005076142132, "no_speech_prob": 0.019699789583683014}, {"id": 220, "seek": 139856, "start": 1405.52, "end": 1413.28, "text": " you a GPT live. Zapier has built a GPT that lets you perform actions across 6,000 applications", "tokens": [50712, 291, 257, 26039, 51, 1621, 13, 34018, 811, 575, 3094, 257, 26039, 51, 300, 6653, 291, 2042, 5909, 2108, 1386, 11, 1360, 5821, 51100], "temperature": 0.0, "avg_logprob": -0.104712819399899, "compression_ratio": 1.4467005076142132, "no_speech_prob": 0.019699789583683014}, {"id": 221, "seek": 139856, "start": 1413.28, "end": 1418.1599999999999, "text": " to unlock all kinds of integration possibilities. I'd like to introduce Jessica, one of our", "tokens": [51100, 281, 11634, 439, 3685, 295, 10980, 12178, 13, 286, 1116, 411, 281, 5366, 15570, 11, 472, 295, 527, 51344], "temperature": 0.0, "avg_logprob": -0.104712819399899, "compression_ratio": 1.4467005076142132, "no_speech_prob": 0.019699789583683014}, {"id": 222, "seek": 141816, "start": 1418.16, "end": 1421.3600000000001, "text": " solutions architects, who is going to drive this demo. Welcome, Jessica.", "tokens": [50364, 6547, 30491, 11, 567, 307, 516, 281, 3332, 341, 10723, 13, 4027, 11, 15570, 13, 50524], "temperature": 0.0, "avg_logprob": -0.15280883542953, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.017158618196845055}, {"id": 223, "seek": 141816, "start": 1430.24, "end": 1434.48, "text": " Thank you all for being here. My name is Jessica Shea. I work with partners and customers to bring", "tokens": [50968, 1044, 291, 439, 337, 885, 510, 13, 1222, 1315, 307, 15570, 1240, 64, 13, 286, 589, 365, 4462, 293, 4581, 281, 1565, 51180], "temperature": 0.0, "avg_logprob": -0.15280883542953, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.017158618196845055}, {"id": 224, "seek": 141816, "start": 1434.48, "end": 1439.76, "text": " their product alive. And today, I can't wait to show you how hard we've been working on this,", "tokens": [51180, 641, 1674, 5465, 13, 400, 965, 11, 286, 393, 380, 1699, 281, 855, 291, 577, 1152, 321, 600, 668, 1364, 322, 341, 11, 51444], "temperature": 0.0, "avg_logprob": -0.15280883542953, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.017158618196845055}, {"id": 225, "seek": 141816, "start": 1439.76, "end": 1445.92, "text": " so let's get started. So to start, where your GPT will live is on this upper left corner.", "tokens": [51444, 370, 718, 311, 483, 1409, 13, 407, 281, 722, 11, 689, 428, 26039, 51, 486, 1621, 307, 322, 341, 6597, 1411, 4538, 13, 51752], "temperature": 0.0, "avg_logprob": -0.15280883542953, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.017158618196845055}, {"id": 226, "seek": 144592, "start": 1445.92, "end": 1451.92, "text": " I'm going to start with clicking on the Zapier AI actions. And on the right hand side, you can see", "tokens": [50364, 286, 478, 516, 281, 722, 365, 9697, 322, 264, 34018, 811, 7318, 5909, 13, 400, 322, 264, 558, 1011, 1252, 11, 291, 393, 536, 50664], "temperature": 0.0, "avg_logprob": -0.09221318134894738, "compression_ratio": 1.6090534979423867, "no_speech_prob": 0.0012635198654606938}, {"id": 227, "seek": 144592, "start": 1451.92, "end": 1457.92, "text": " that's my calendar for today. So it's quite a day. I've already used this before, so it's actually", "tokens": [50664, 300, 311, 452, 12183, 337, 965, 13, 407, 309, 311, 1596, 257, 786, 13, 286, 600, 1217, 1143, 341, 949, 11, 370, 309, 311, 767, 50964], "temperature": 0.0, "avg_logprob": -0.09221318134894738, "compression_ratio": 1.6090534979423867, "no_speech_prob": 0.0012635198654606938}, {"id": 228, "seek": 144592, "start": 1457.92, "end": 1464.5600000000002, "text": " already connected to my calendar. To start, I can ask, what's on my schedule for today? We built", "tokens": [50964, 1217, 4582, 281, 452, 12183, 13, 1407, 722, 11, 286, 393, 1029, 11, 437, 311, 322, 452, 7567, 337, 965, 30, 492, 3094, 51296], "temperature": 0.0, "avg_logprob": -0.09221318134894738, "compression_ratio": 1.6090534979423867, "no_speech_prob": 0.0012635198654606938}, {"id": 229, "seek": 144592, "start": 1464.5600000000002, "end": 1471.28, "text": " GPTs with security in mind. So before it performs any action or share data, it will ask for your", "tokens": [51296, 26039, 33424, 365, 3825, 294, 1575, 13, 407, 949, 309, 26213, 604, 3069, 420, 2073, 1412, 11, 309, 486, 1029, 337, 428, 51632], "temperature": 0.0, "avg_logprob": -0.09221318134894738, "compression_ratio": 1.6090534979423867, "no_speech_prob": 0.0012635198654606938}, {"id": 230, "seek": 147128, "start": 1471.28, "end": 1478.8, "text": " permission. So right here, I'm going to say allowed. So GPT is designed to take in your", "tokens": [50364, 11226, 13, 407, 558, 510, 11, 286, 478, 516, 281, 584, 4350, 13, 407, 26039, 51, 307, 4761, 281, 747, 294, 428, 50740], "temperature": 0.0, "avg_logprob": -0.06214347582184867, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.0017804352100938559}, {"id": 231, "seek": 147128, "start": 1478.8, "end": 1483.92, "text": " instructions, make the decision on which capability to call to perform that action,", "tokens": [50740, 9415, 11, 652, 264, 3537, 322, 597, 13759, 281, 818, 281, 2042, 300, 3069, 11, 50996], "temperature": 0.0, "avg_logprob": -0.06214347582184867, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.0017804352100938559}, {"id": 232, "seek": 147128, "start": 1483.92, "end": 1489.28, "text": " and then execute that for you. So you can see right here, it's already connected to my calendar.", "tokens": [50996, 293, 550, 14483, 300, 337, 291, 13, 407, 291, 393, 536, 558, 510, 11, 309, 311, 1217, 4582, 281, 452, 12183, 13, 51264], "temperature": 0.0, "avg_logprob": -0.06214347582184867, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.0017804352100938559}, {"id": 233, "seek": 147128, "start": 1489.28, "end": 1496.72, "text": " It pulls into my information. And then I've also prompted it to identify conflicts on my calendar.", "tokens": [51264, 467, 16982, 666, 452, 1589, 13, 400, 550, 286, 600, 611, 31042, 309, 281, 5876, 19807, 322, 452, 12183, 13, 51636], "temperature": 0.0, "avg_logprob": -0.06214347582184867, "compression_ratio": 1.6167400881057268, "no_speech_prob": 0.0017804352100938559}, {"id": 234, "seek": 149672, "start": 1496.72, "end": 1502.16, "text": " So you can see right here, it actually was able to identify that. So it looks like I have", "tokens": [50364, 407, 291, 393, 536, 558, 510, 11, 309, 767, 390, 1075, 281, 5876, 300, 13, 407, 309, 1542, 411, 286, 362, 50636], "temperature": 0.0, "avg_logprob": -0.0945517921447754, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.0009543238556943834}, {"id": 235, "seek": 149672, "start": 1502.16, "end": 1506.88, "text": " something coming up. So what if I want to let Sam know that I have to leave early? So right here,", "tokens": [50636, 746, 1348, 493, 13, 407, 437, 498, 286, 528, 281, 718, 4832, 458, 300, 286, 362, 281, 1856, 2440, 30, 407, 558, 510, 11, 50872], "temperature": 0.0, "avg_logprob": -0.0945517921447754, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.0009543238556943834}, {"id": 236, "seek": 149672, "start": 1506.88, "end": 1517.76, "text": " I say, let Sam know. I got to go. Chasing GPUs. So with that, I'm going to swap to", "tokens": [50872, 286, 584, 11, 718, 4832, 458, 13, 286, 658, 281, 352, 13, 761, 3349, 18407, 82, 13, 407, 365, 300, 11, 286, 478, 516, 281, 18135, 281, 51416], "temperature": 0.0, "avg_logprob": -0.0945517921447754, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.0009543238556943834}, {"id": 237, "seek": 149672, "start": 1518.8, "end": 1523.3600000000001, "text": " my conversation with Sam. And then I'm going to say, yes, please run that.", "tokens": [51468, 452, 3761, 365, 4832, 13, 400, 550, 286, 478, 516, 281, 584, 11, 2086, 11, 1767, 1190, 300, 13, 51696], "temperature": 0.0, "avg_logprob": -0.0945517921447754, "compression_ratio": 1.619718309859155, "no_speech_prob": 0.0009543238556943834}, {"id": 238, "seek": 152672, "start": 1526.96, "end": 1531.76, "text": " Sam, did you get that? I did. Awesome.", "tokens": [50376, 4832, 11, 630, 291, 483, 300, 30, 286, 630, 13, 10391, 13, 50616], "temperature": 0.0, "avg_logprob": -0.13265722189376603, "compression_ratio": 1.4203821656050954, "no_speech_prob": 0.00029134246869944036}, {"id": 239, "seek": 152672, "start": 1534.72, "end": 1541.84, "text": " So this is only a glimpse of what is possible. And I cannot wait to see what you all will build.", "tokens": [50764, 407, 341, 307, 787, 257, 25838, 295, 437, 307, 1944, 13, 400, 286, 2644, 1699, 281, 536, 437, 291, 439, 486, 1322, 13, 51120], "temperature": 0.0, "avg_logprob": -0.13265722189376603, "compression_ratio": 1.4203821656050954, "no_speech_prob": 0.00029134246869944036}, {"id": 240, "seek": 152672, "start": 1541.84, "end": 1543.1200000000001, "text": " Thank you. And back to you, Sam.", "tokens": [51120, 1044, 291, 13, 400, 646, 281, 291, 11, 4832, 13, 51184], "temperature": 0.0, "avg_logprob": -0.13265722189376603, "compression_ratio": 1.4203821656050954, "no_speech_prob": 0.00029134246869944036}, {"id": 241, "seek": 152672, "start": 1550.8, "end": 1553.84, "text": " Thank you, Jessica. So those are three great examples.", "tokens": [51568, 1044, 291, 11, 15570, 13, 407, 729, 366, 1045, 869, 5110, 13, 51720], "temperature": 0.0, "avg_logprob": -0.13265722189376603, "compression_ratio": 1.4203821656050954, "no_speech_prob": 0.00029134246869944036}, {"id": 242, "seek": 155384, "start": 1554.8, "end": 1558.72, "text": " In addition to these, there are many more kinds of GPTs that people are creating,", "tokens": [50412, 682, 4500, 281, 613, 11, 456, 366, 867, 544, 3685, 295, 26039, 33424, 300, 561, 366, 4084, 11, 50608], "temperature": 0.0, "avg_logprob": -0.06850237846374511, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0034819371066987514}, {"id": 243, "seek": 155384, "start": 1558.72, "end": 1564.24, "text": " and many, many more that will be created soon. We know that many people who want to build the", "tokens": [50608, 293, 867, 11, 867, 544, 300, 486, 312, 2942, 2321, 13, 492, 458, 300, 867, 561, 567, 528, 281, 1322, 264, 50884], "temperature": 0.0, "avg_logprob": -0.06850237846374511, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0034819371066987514}, {"id": 244, "seek": 155384, "start": 1564.24, "end": 1570.8799999999999, "text": " GPT don't know how to code. We've made it so that you can program the GPT just by having a conversation.", "tokens": [50884, 26039, 51, 500, 380, 458, 577, 281, 3089, 13, 492, 600, 1027, 309, 370, 300, 291, 393, 1461, 264, 26039, 51, 445, 538, 1419, 257, 3761, 13, 51216], "temperature": 0.0, "avg_logprob": -0.06850237846374511, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0034819371066987514}, {"id": 245, "seek": 155384, "start": 1572.1599999999999, "end": 1576.0, "text": " We believe that natural language is going to be a big part of how people use computers in the", "tokens": [51280, 492, 1697, 300, 3303, 2856, 307, 516, 281, 312, 257, 955, 644, 295, 577, 561, 764, 10807, 294, 264, 51472], "temperature": 0.0, "avg_logprob": -0.06850237846374511, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0034819371066987514}, {"id": 246, "seek": 155384, "start": 1576.0, "end": 1580.72, "text": " future. And we think this is an interesting early example. So I'd like to show you how to build one.", "tokens": [51472, 2027, 13, 400, 321, 519, 341, 307, 364, 1880, 2440, 1365, 13, 407, 286, 1116, 411, 281, 855, 291, 577, 281, 1322, 472, 13, 51708], "temperature": 0.0, "avg_logprob": -0.06850237846374511, "compression_ratio": 1.70863309352518, "no_speech_prob": 0.0034819371066987514}, {"id": 247, "seek": 158384, "start": 1584.72, "end": 1590.8, "text": " All right. So I'm going to create a GPT that helps give founders and developers advice when", "tokens": [50408, 1057, 558, 13, 407, 286, 478, 516, 281, 1884, 257, 26039, 51, 300, 3665, 976, 25608, 293, 8849, 5192, 562, 50712], "temperature": 0.0, "avg_logprob": -0.12942379299956974, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.00020984839648008347}, {"id": 248, "seek": 158384, "start": 1590.8, "end": 1599.04, "text": " starting new projects. I'm going to go to create a GPT here. And this drops me into the GPT builder.", "tokens": [50712, 2891, 777, 4455, 13, 286, 478, 516, 281, 352, 281, 1884, 257, 26039, 51, 510, 13, 400, 341, 11438, 385, 666, 264, 26039, 51, 27377, 13, 51124], "temperature": 0.0, "avg_logprob": -0.12942379299956974, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.00020984839648008347}, {"id": 249, "seek": 158384, "start": 1600.1599999999999, "end": 1604.8, "text": " I worked with founders for years at YC. And still, whenever I meet developers, the questions I get", "tokens": [51180, 286, 2732, 365, 25608, 337, 924, 412, 398, 34, 13, 400, 920, 11, 5699, 286, 1677, 8849, 11, 264, 1651, 286, 483, 51412], "temperature": 0.0, "avg_logprob": -0.12942379299956974, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.00020984839648008347}, {"id": 250, "seek": 158384, "start": 1604.8, "end": 1608.24, "text": " are always about how do I, you know, think about a business idea? Can you give me some advice?", "tokens": [51412, 366, 1009, 466, 577, 360, 286, 11, 291, 458, 11, 519, 466, 257, 1606, 1558, 30, 1664, 291, 976, 385, 512, 5192, 30, 51584], "temperature": 0.0, "avg_logprob": -0.12942379299956974, "compression_ratio": 1.6016597510373445, "no_speech_prob": 0.00020984839648008347}, {"id": 251, "seek": 160824, "start": 1609.1200000000001, "end": 1611.1200000000001, "text": " I'm going to see if I can build a GPT to help with that.", "tokens": [50408, 286, 478, 516, 281, 536, 498, 286, 393, 1322, 257, 26039, 51, 281, 854, 365, 300, 13, 50508], "temperature": 0.0, "avg_logprob": -0.09502534305348116, "compression_ratio": 1.536144578313253, "no_speech_prob": 0.0028002713806927204}, {"id": 252, "seek": 160824, "start": 1612.0, "end": 1618.64, "text": " So to start, GPT builder asks me what I want to make. And I'm going to say I want to help startup", "tokens": [50552, 407, 281, 722, 11, 26039, 51, 27377, 8962, 385, 437, 286, 528, 281, 652, 13, 400, 286, 478, 516, 281, 584, 286, 528, 281, 854, 18578, 50884], "temperature": 0.0, "avg_logprob": -0.09502534305348116, "compression_ratio": 1.536144578313253, "no_speech_prob": 0.0028002713806927204}, {"id": 253, "seek": 160824, "start": 1618.64, "end": 1631.04, "text": " founders think through their business ideas and get advice after the founder has gotten some advice,", "tokens": [50884, 25608, 519, 807, 641, 1606, 3487, 293, 483, 5192, 934, 264, 14917, 575, 5768, 512, 5192, 11, 51504], "temperature": 0.0, "avg_logprob": -0.09502534305348116, "compression_ratio": 1.536144578313253, "no_speech_prob": 0.0028002713806927204}, {"id": 254, "seek": 163104, "start": 1631.04, "end": 1643.52, "text": " grill them on why they are not growing faster. All right. So to start off, I just tell the GPT", "tokens": [50364, 16492, 552, 322, 983, 436, 366, 406, 4194, 4663, 13, 1057, 558, 13, 407, 281, 722, 766, 11, 286, 445, 980, 264, 26039, 51, 50988], "temperature": 0.0, "avg_logprob": -0.10662144534992722, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0012842129217460752}, {"id": 255, "seek": 163104, "start": 1643.52, "end": 1647.44, "text": " a little bit about what I want here. And it's going to go off and start thinking about that.", "tokens": [50988, 257, 707, 857, 466, 437, 286, 528, 510, 13, 400, 309, 311, 516, 281, 352, 766, 293, 722, 1953, 466, 300, 13, 51184], "temperature": 0.0, "avg_logprob": -0.10662144534992722, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0012842129217460752}, {"id": 256, "seek": 163104, "start": 1648.48, "end": 1653.2, "text": " It's going to write some detailed instructions for the GPT. It's also going to, let's see,", "tokens": [51236, 467, 311, 516, 281, 2464, 512, 9942, 9415, 337, 264, 26039, 51, 13, 467, 311, 611, 516, 281, 11, 718, 311, 536, 11, 51472], "temperature": 0.0, "avg_logprob": -0.10662144534992722, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0012842129217460752}, {"id": 257, "seek": 163104, "start": 1653.2, "end": 1660.08, "text": " ask me about a name. How do I feel about startup mentor? That's fine. That's good. So if I didn't", "tokens": [51472, 1029, 385, 466, 257, 1315, 13, 1012, 360, 286, 841, 466, 18578, 14478, 30, 663, 311, 2489, 13, 663, 311, 665, 13, 407, 498, 286, 994, 380, 51816], "temperature": 0.0, "avg_logprob": -0.10662144534992722, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.0012842129217460752}, {"id": 258, "seek": 166008, "start": 1660.08, "end": 1663.9199999999998, "text": " like the name, of course, I could call it something else, but it's going to try to have this conversation", "tokens": [50364, 411, 264, 1315, 11, 295, 1164, 11, 286, 727, 818, 309, 746, 1646, 11, 457, 309, 311, 516, 281, 853, 281, 362, 341, 3761, 50556], "temperature": 0.0, "avg_logprob": -0.10334340893492407, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0006876377738080919}, {"id": 259, "seek": 166008, "start": 1663.9199999999998, "end": 1670.1599999999999, "text": " with me and start there. And you can see here on the right in the preview mode that it's already", "tokens": [50556, 365, 385, 293, 722, 456, 13, 400, 291, 393, 536, 510, 322, 264, 558, 294, 264, 14281, 4391, 300, 309, 311, 1217, 50868], "temperature": 0.0, "avg_logprob": -0.10334340893492407, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0006876377738080919}, {"id": 260, "seek": 166008, "start": 1670.1599999999999, "end": 1676.96, "text": " starting to fill out the GPT, where it says what it does. It has some ideas of additional", "tokens": [50868, 2891, 281, 2836, 484, 264, 26039, 51, 11, 689, 309, 1619, 437, 309, 775, 13, 467, 575, 512, 3487, 295, 4497, 51208], "temperature": 0.0, "avg_logprob": -0.10334340893492407, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0006876377738080919}, {"id": 261, "seek": 166008, "start": 1676.96, "end": 1684.08, "text": " questions that I could ask. And you know what? So it just generated a candidate. Of course,", "tokens": [51208, 1651, 300, 286, 727, 1029, 13, 400, 291, 458, 437, 30, 407, 309, 445, 10833, 257, 11532, 13, 2720, 1164, 11, 51564], "temperature": 0.0, "avg_logprob": -0.10334340893492407, "compression_ratio": 1.641025641025641, "no_speech_prob": 0.0006876377738080919}, {"id": 262, "seek": 168408, "start": 1684.08, "end": 1690.1599999999999, "text": " I could regenerate that or change it, but I sort of like that. So I will say that's great.", "tokens": [50364, 286, 727, 26358, 473, 300, 420, 1319, 309, 11, 457, 286, 1333, 295, 411, 300, 13, 407, 286, 486, 584, 300, 311, 869, 13, 50668], "temperature": 0.0, "avg_logprob": -0.08984743676534514, "compression_ratio": 1.471502590673575, "no_speech_prob": 0.0011694032000377774}, {"id": 263, "seek": 168408, "start": 1692.8, "end": 1698.48, "text": " And you see now that the GPT is being built out a little bit more as we go. Now, what I want this", "tokens": [50800, 400, 291, 536, 586, 300, 264, 26039, 51, 307, 885, 3094, 484, 257, 707, 857, 544, 382, 321, 352, 13, 823, 11, 437, 286, 528, 341, 51084], "temperature": 0.0, "avg_logprob": -0.08984743676534514, "compression_ratio": 1.471502590673575, "no_speech_prob": 0.0011694032000377774}, {"id": 264, "seek": 168408, "start": 1698.48, "end": 1703.4399999999998, "text": " to do, how it can interact with users, I could talk about style here. But what I'm going to say", "tokens": [51084, 281, 360, 11, 577, 309, 393, 4648, 365, 5022, 11, 286, 727, 751, 466, 3758, 510, 13, 583, 437, 286, 478, 516, 281, 584, 51332], "temperature": 0.0, "avg_logprob": -0.08984743676534514, "compression_ratio": 1.471502590673575, "no_speech_prob": 0.0011694032000377774}, {"id": 265, "seek": 170344, "start": 1704.0, "end": 1712.88, "text": " is I am going to upload transcripts of some lectures about startups I have given.", "tokens": [50392, 307, 286, 669, 516, 281, 6580, 24444, 82, 295, 512, 16564, 466, 28041, 286, 362, 2212, 13, 50836], "temperature": 0.0, "avg_logprob": -0.07392064561235144, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.022967711091041565}, {"id": 266, "seek": 170344, "start": 1713.76, "end": 1723.6000000000001, "text": " Please give advice based off of those. All right. So now it's going to go figure out how to do that.", "tokens": [50880, 2555, 976, 5192, 2361, 766, 295, 729, 13, 1057, 558, 13, 407, 586, 309, 311, 516, 281, 352, 2573, 484, 577, 281, 360, 300, 13, 51372], "temperature": 0.0, "avg_logprob": -0.07392064561235144, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.022967711091041565}, {"id": 267, "seek": 170344, "start": 1723.6000000000001, "end": 1727.44, "text": " And I would like to show you the configure tab. So you can see some of the things that were built", "tokens": [51372, 400, 286, 576, 411, 281, 855, 291, 264, 22162, 4421, 13, 407, 291, 393, 536, 512, 295, 264, 721, 300, 645, 3094, 51564], "temperature": 0.0, "avg_logprob": -0.07392064561235144, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.022967711091041565}, {"id": 268, "seek": 170344, "start": 1727.44, "end": 1732.64, "text": " out here as we were going by the builder itself. And you can see that there's capabilities here", "tokens": [51564, 484, 510, 382, 321, 645, 516, 538, 264, 27377, 2564, 13, 400, 291, 393, 536, 300, 456, 311, 10862, 510, 51824], "temperature": 0.0, "avg_logprob": -0.07392064561235144, "compression_ratio": 1.6860986547085202, "no_speech_prob": 0.022967711091041565}, {"id": 269, "seek": 173264, "start": 1732.64, "end": 1738.48, "text": " that I can enable. I could add custom actions. These are all fine to leave. I'm going to upload", "tokens": [50364, 300, 286, 393, 9528, 13, 286, 727, 909, 2375, 5909, 13, 1981, 366, 439, 2489, 281, 1856, 13, 286, 478, 516, 281, 6580, 50656], "temperature": 0.0, "avg_logprob": -0.07205323980312155, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.00041724066250026226}, {"id": 270, "seek": 173264, "start": 1738.48, "end": 1747.2800000000002, "text": " a file. So here is a lecture that I gave with some startup advice. And I'm going to add that here.", "tokens": [50656, 257, 3991, 13, 407, 510, 307, 257, 7991, 300, 286, 2729, 365, 512, 18578, 5192, 13, 400, 286, 478, 516, 281, 909, 300, 510, 13, 51096], "temperature": 0.0, "avg_logprob": -0.07205323980312155, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.00041724066250026226}, {"id": 271, "seek": 173264, "start": 1747.2800000000002, "end": 1754.96, "text": " In terms of these questions, this is a dumb one. The rest of those are reasonable. And like very", "tokens": [51096, 682, 2115, 295, 613, 1651, 11, 341, 307, 257, 10316, 472, 13, 440, 1472, 295, 729, 366, 10585, 13, 400, 411, 588, 51480], "temperature": 0.0, "avg_logprob": -0.07205323980312155, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.00041724066250026226}, {"id": 272, "seek": 173264, "start": 1754.96, "end": 1759.44, "text": " much things founders often ask. I'm going to add one more thing to the instructions here,", "tokens": [51480, 709, 721, 25608, 2049, 1029, 13, 286, 478, 516, 281, 909, 472, 544, 551, 281, 264, 9415, 510, 11, 51704], "temperature": 0.0, "avg_logprob": -0.07205323980312155, "compression_ratio": 1.7085201793721974, "no_speech_prob": 0.00041724066250026226}, {"id": 273, "seek": 175944, "start": 1759.52, "end": 1767.8400000000001, "text": " which is be concise and constructive with feedback. All right. So again, if we had more", "tokens": [50368, 597, 307, 312, 44882, 293, 30223, 365, 5824, 13, 1057, 558, 13, 407, 797, 11, 498, 321, 632, 544, 50784], "temperature": 0.0, "avg_logprob": -0.1132135264078776, "compression_ratio": 1.4234693877551021, "no_speech_prob": 0.0006262448150664568}, {"id": 274, "seek": 175944, "start": 1767.8400000000001, "end": 1774.48, "text": " time, I'd show you a bunch of other things. But this is like a decent start. And now we can try", "tokens": [50784, 565, 11, 286, 1116, 855, 291, 257, 3840, 295, 661, 721, 13, 583, 341, 307, 411, 257, 8681, 722, 13, 400, 586, 321, 393, 853, 51116], "temperature": 0.0, "avg_logprob": -0.1132135264078776, "compression_ratio": 1.4234693877551021, "no_speech_prob": 0.0006262448150664568}, {"id": 275, "seek": 175944, "start": 1774.48, "end": 1781.2, "text": " it out over on this preview tab. So I will say, what's a common question? What are three things", "tokens": [51116, 309, 484, 670, 322, 341, 14281, 4421, 13, 407, 286, 486, 584, 11, 437, 311, 257, 2689, 1168, 30, 708, 366, 1045, 721, 51452], "temperature": 0.0, "avg_logprob": -0.1132135264078776, "compression_ratio": 1.4234693877551021, "no_speech_prob": 0.0006262448150664568}, {"id": 276, "seek": 178120, "start": 1781.2, "end": 1789.8400000000001, "text": " to look for when hiring employees at an early stage startup?", "tokens": [50364, 281, 574, 337, 562, 15335, 6619, 412, 364, 2440, 3233, 18578, 30, 50796], "temperature": 0.0, "avg_logprob": -0.09305925133787556, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.0015011110808700323}, {"id": 277, "seek": 178120, "start": 1793.76, "end": 1797.1200000000001, "text": " Now, it's going to look at that document I uploaded. It will also have, of course,", "tokens": [50992, 823, 11, 309, 311, 516, 281, 574, 412, 300, 4166, 286, 17135, 13, 467, 486, 611, 362, 11, 295, 1164, 11, 51160], "temperature": 0.0, "avg_logprob": -0.09305925133787556, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.0015011110808700323}, {"id": 278, "seek": 178120, "start": 1797.1200000000001, "end": 1804.64, "text": " all of the background knowledge of GPT-4. That's pretty good. Those are three things", "tokens": [51160, 439, 295, 264, 3678, 3601, 295, 26039, 51, 12, 19, 13, 663, 311, 1238, 665, 13, 3950, 366, 1045, 721, 51536], "temperature": 0.0, "avg_logprob": -0.09305925133787556, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.0015011110808700323}, {"id": 279, "seek": 178120, "start": 1804.64, "end": 1809.2, "text": " that I definitely have said many times. Now, we could go on and it would start", "tokens": [51536, 300, 286, 2138, 362, 848, 867, 1413, 13, 823, 11, 321, 727, 352, 322, 293, 309, 576, 722, 51764], "temperature": 0.0, "avg_logprob": -0.09305925133787556, "compression_ratio": 1.4619047619047618, "no_speech_prob": 0.0015011110808700323}, {"id": 280, "seek": 180920, "start": 1809.2, "end": 1813.76, "text": " following the other instructions and grill me on why I'm not growing faster. But in the interest", "tokens": [50364, 3480, 264, 661, 9415, 293, 16492, 385, 322, 983, 286, 478, 406, 4194, 4663, 13, 583, 294, 264, 1179, 50592], "temperature": 0.0, "avg_logprob": -0.08453866750887125, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0013246681774035096}, {"id": 281, "seek": 180920, "start": 1813.76, "end": 1820.0800000000002, "text": " of time, I'm going to skip that. I'm going to publish this only to me for now. I can work on it", "tokens": [50592, 295, 565, 11, 286, 478, 516, 281, 10023, 300, 13, 286, 478, 516, 281, 11374, 341, 787, 281, 385, 337, 586, 13, 286, 393, 589, 322, 309, 50908], "temperature": 0.0, "avg_logprob": -0.08453866750887125, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0013246681774035096}, {"id": 282, "seek": 180920, "start": 1820.0800000000002, "end": 1824.24, "text": " later. I can add more content. I can add a few actions that I think would be useful. And then", "tokens": [50908, 1780, 13, 286, 393, 909, 544, 2701, 13, 286, 393, 909, 257, 1326, 5909, 300, 286, 519, 576, 312, 4420, 13, 400, 550, 51116], "temperature": 0.0, "avg_logprob": -0.08453866750887125, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0013246681774035096}, {"id": 283, "seek": 180920, "start": 1824.24, "end": 1830.32, "text": " I can share it publicly. So that's what it looks like to create a GPT. Thank you.", "tokens": [51116, 286, 393, 2073, 309, 14843, 13, 407, 300, 311, 437, 309, 1542, 411, 281, 1884, 257, 26039, 51, 13, 1044, 291, 13, 51420], "temperature": 0.0, "avg_logprob": -0.08453866750887125, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0013246681774035096}, {"id": 284, "seek": 183032, "start": 1830.6399999999999, "end": 1841.36, "text": " By the way, I always wanted to do that after all of the YC office hours. I always thought,", "tokens": [50380, 3146, 264, 636, 11, 286, 1009, 1415, 281, 360, 300, 934, 439, 295, 264, 398, 34, 3398, 2496, 13, 286, 1009, 1194, 11, 50916], "temperature": 0.0, "avg_logprob": -0.1721948446686735, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.004069112706929445}, {"id": 285, "seek": 183032, "start": 1841.36, "end": 1844.32, "text": " man, someday I will be able to make a bot that will do this and that will be awesome.", "tokens": [50916, 587, 11, 19412, 286, 486, 312, 1075, 281, 652, 257, 10592, 300, 486, 360, 341, 293, 300, 486, 312, 3476, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1721948446686735, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.004069112706929445}, {"id": 286, "seek": 183032, "start": 1846.56, "end": 1852.56, "text": " So with GPTs, we're letting people easily share and discover all the fun ways that they use chat", "tokens": [51176, 407, 365, 26039, 33424, 11, 321, 434, 8295, 561, 3612, 2073, 293, 4411, 439, 264, 1019, 2098, 300, 436, 764, 5081, 51476], "temperature": 0.0, "avg_logprob": -0.1721948446686735, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.004069112706929445}, {"id": 287, "seek": 183032, "start": 1852.56, "end": 1859.9199999999998, "text": " GPT with the world. You can make private GPTs like I just did. Or you can share your", "tokens": [51476, 26039, 51, 365, 264, 1002, 13, 509, 393, 652, 4551, 26039, 33424, 411, 286, 445, 630, 13, 1610, 291, 393, 2073, 428, 51844], "temperature": 0.0, "avg_logprob": -0.1721948446686735, "compression_ratio": 1.6199095022624435, "no_speech_prob": 0.004069112706929445}, {"id": 288, "seek": 185992, "start": 1859.92, "end": 1866.48, "text": " creations publicly with a link for anyone to use. Or if you're on chat GPT enterprise,", "tokens": [50364, 37836, 14843, 365, 257, 2113, 337, 2878, 281, 764, 13, 1610, 498, 291, 434, 322, 5081, 26039, 51, 14132, 11, 50692], "temperature": 0.0, "avg_logprob": -0.11597121067536183, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.0015480772126466036}, {"id": 289, "seek": 185992, "start": 1866.48, "end": 1873.52, "text": " you can make GPTs just for your company. And later this month, we're going to launch the GPT", "tokens": [50692, 291, 393, 652, 26039, 33424, 445, 337, 428, 2237, 13, 400, 1780, 341, 1618, 11, 321, 434, 516, 281, 4025, 264, 26039, 51, 51044], "temperature": 0.0, "avg_logprob": -0.11597121067536183, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.0015480772126466036}, {"id": 290, "seek": 185992, "start": 1873.52, "end": 1887.28, "text": " store. You can list a GPT. Thank you. I appreciate that. You can list a GPT there and we'll be", "tokens": [51044, 3531, 13, 509, 393, 1329, 257, 26039, 51, 13, 1044, 291, 13, 286, 4449, 300, 13, 509, 393, 1329, 257, 26039, 51, 456, 293, 321, 603, 312, 51732], "temperature": 0.0, "avg_logprob": -0.11597121067536183, "compression_ratio": 1.5054945054945055, "no_speech_prob": 0.0015480772126466036}, {"id": 291, "seek": 188728, "start": 1887.28, "end": 1893.36, "text": " able to feature the best and the most popular GPTs. Of course, we'll make sure that GPTs in the store", "tokens": [50364, 1075, 281, 4111, 264, 1151, 293, 264, 881, 3743, 26039, 33424, 13, 2720, 1164, 11, 321, 603, 652, 988, 300, 26039, 33424, 294, 264, 3531, 50668], "temperature": 0.0, "avg_logprob": -0.08001173775771568, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0055520483292639256}, {"id": 292, "seek": 188728, "start": 1893.36, "end": 1900.3999999999999, "text": " follow our policies before they're accessible. Revenue sharing is important to us. We're going", "tokens": [50668, 1524, 527, 7657, 949, 436, 434, 9515, 13, 1300, 29307, 5414, 307, 1021, 281, 505, 13, 492, 434, 516, 51020], "temperature": 0.0, "avg_logprob": -0.08001173775771568, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0055520483292639256}, {"id": 293, "seek": 188728, "start": 1900.3999999999999, "end": 1905.36, "text": " to pay people who build the most useful and the most used GPTs a portion of our revenue.", "tokens": [51020, 281, 1689, 561, 567, 1322, 264, 881, 4420, 293, 264, 881, 1143, 26039, 33424, 257, 8044, 295, 527, 9324, 13, 51268], "temperature": 0.0, "avg_logprob": -0.08001173775771568, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0055520483292639256}, {"id": 294, "seek": 188728, "start": 1906.32, "end": 1911.04, "text": " We're excited to foster a vibrant ecosystem with the GPT store. Just from what we've been building", "tokens": [51316, 492, 434, 2919, 281, 17114, 257, 21571, 11311, 365, 264, 26039, 51, 3531, 13, 1449, 490, 437, 321, 600, 668, 2390, 51552], "temperature": 0.0, "avg_logprob": -0.08001173775771568, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0055520483292639256}, {"id": 295, "seek": 188728, "start": 1911.04, "end": 1914.3999999999999, "text": " ourselves over the weekend, we're confident there's going to be a lot of great stuff.", "tokens": [51552, 4175, 670, 264, 6711, 11, 321, 434, 6679, 456, 311, 516, 281, 312, 257, 688, 295, 869, 1507, 13, 51720], "temperature": 0.0, "avg_logprob": -0.08001173775771568, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0055520483292639256}, {"id": 296, "seek": 191440, "start": 1914.4, "end": 1920.4, "text": " We're excited to share more information soon. So those are GPTs and we can't wait to see what you", "tokens": [50364, 492, 434, 2919, 281, 2073, 544, 1589, 2321, 13, 407, 729, 366, 26039, 33424, 293, 321, 393, 380, 1699, 281, 536, 437, 291, 50664], "temperature": 0.0, "avg_logprob": -0.09938481274773092, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.0003052674001082778}, {"id": 297, "seek": 191440, "start": 1920.4, "end": 1926.24, "text": " build. But this is a developer conference and the coolest thing about this is that we're bringing", "tokens": [50664, 1322, 13, 583, 341, 307, 257, 10754, 7586, 293, 264, 22013, 551, 466, 341, 307, 300, 321, 434, 5062, 50956], "temperature": 0.0, "avg_logprob": -0.09938481274773092, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.0003052674001082778}, {"id": 298, "seek": 191440, "start": 1926.24, "end": 1938.5600000000002, "text": " the same concept to the API. Many of you have already been building agent-like experiences", "tokens": [50956, 264, 912, 3410, 281, 264, 9362, 13, 5126, 295, 291, 362, 1217, 668, 2390, 9461, 12, 4092, 5235, 51572], "temperature": 0.0, "avg_logprob": -0.09938481274773092, "compression_ratio": 1.4666666666666666, "no_speech_prob": 0.0003052674001082778}, {"id": 299, "seek": 193856, "start": 1938.6399999999999, "end": 1945.2, "text": " on the API. For example, Shopify's Sidekick, which lets you take actions on the platform,", "tokens": [50368, 322, 264, 9362, 13, 1171, 1365, 11, 43991, 311, 19026, 42427, 11, 597, 6653, 291, 747, 5909, 322, 264, 3663, 11, 50696], "temperature": 0.0, "avg_logprob": -0.17726733134343073, "compression_ratio": 1.5598591549295775, "no_speech_prob": 0.37311404943466187}, {"id": 300, "seek": 193856, "start": 1945.2, "end": 1951.36, "text": " Discord's Clyde, let's Discord moderators create custom personalities for, and SnapsMyAI,", "tokens": [50696, 32623, 311, 44752, 1479, 11, 718, 311, 32623, 10494, 3391, 1884, 2375, 25308, 337, 11, 293, 9264, 2382, 8506, 48698, 11, 51004], "temperature": 0.0, "avg_logprob": -0.17726733134343073, "compression_ratio": 1.5598591549295775, "no_speech_prob": 0.37311404943466187}, {"id": 301, "seek": 193856, "start": 1952.08, "end": 1955.76, "text": " a customized chatbot that can be added to group chats and make recommendations.", "tokens": [51040, 257, 30581, 5081, 18870, 300, 393, 312, 3869, 281, 1594, 38057, 293, 652, 10434, 13, 51224], "temperature": 0.0, "avg_logprob": -0.17726733134343073, "compression_ratio": 1.5598591549295775, "no_speech_prob": 0.37311404943466187}, {"id": 302, "seek": 193856, "start": 1956.3999999999999, "end": 1961.9199999999998, "text": " These experiences are great, but they have been hard to build. Sometimes taking months,", "tokens": [51256, 1981, 5235, 366, 869, 11, 457, 436, 362, 668, 1152, 281, 1322, 13, 4803, 1940, 2493, 11, 51532], "temperature": 0.0, "avg_logprob": -0.17726733134343073, "compression_ratio": 1.5598591549295775, "no_speech_prob": 0.37311404943466187}, {"id": 303, "seek": 193856, "start": 1961.9199999999998, "end": 1966.96, "text": " teams of dozens of engineers, there's a lot to handle to make this custom assistant experience.", "tokens": [51532, 5491, 295, 18431, 295, 11955, 11, 456, 311, 257, 688, 281, 4813, 281, 652, 341, 2375, 10994, 1752, 13, 51784], "temperature": 0.0, "avg_logprob": -0.17726733134343073, "compression_ratio": 1.5598591549295775, "no_speech_prob": 0.37311404943466187}, {"id": 304, "seek": 196856, "start": 1968.8, "end": 1972.3999999999999, "text": " So today, we're making that a lot easier with our new Assistance API.", "tokens": [50376, 407, 965, 11, 321, 434, 1455, 300, 257, 688, 3571, 365, 527, 777, 46805, 9362, 13, 50556], "temperature": 0.0, "avg_logprob": -0.11753681898117066, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.00015841462300159037}, {"id": 305, "seek": 196856, "start": 1978.1599999999999, "end": 1982.72, "text": " The Assistance API includes persistent threads so they don't have to figure out how to deal with", "tokens": [50844, 440, 46805, 9362, 5974, 24315, 19314, 370, 436, 500, 380, 362, 281, 2573, 484, 577, 281, 2028, 365, 51072], "temperature": 0.0, "avg_logprob": -0.11753681898117066, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.00015841462300159037}, {"id": 306, "seek": 196856, "start": 1982.72, "end": 1989.52, "text": " long conversation history, built-in retrieval, code interpreter, a working Python interpreter,", "tokens": [51072, 938, 3761, 2503, 11, 3094, 12, 259, 19817, 3337, 11, 3089, 34132, 11, 257, 1364, 15329, 34132, 11, 51412], "temperature": 0.0, "avg_logprob": -0.11753681898117066, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.00015841462300159037}, {"id": 307, "seek": 196856, "start": 1989.52, "end": 1994.6399999999999, "text": " and a sandbox environment, and, of course, the improved function calling that we talked about", "tokens": [51412, 293, 257, 42115, 2823, 11, 293, 11, 295, 1164, 11, 264, 9689, 2445, 5141, 300, 321, 2825, 466, 51668], "temperature": 0.0, "avg_logprob": -0.11753681898117066, "compression_ratio": 1.5570175438596492, "no_speech_prob": 0.00015841462300159037}, {"id": 308, "seek": 199464, "start": 1994.64, "end": 2001.2800000000002, "text": " earlier. So we'd like to show you a demo of how this works, and here is Roman, our head of developer", "tokens": [50364, 3071, 13, 407, 321, 1116, 411, 281, 855, 291, 257, 10723, 295, 577, 341, 1985, 11, 293, 510, 307, 8566, 11, 527, 1378, 295, 10754, 50696], "temperature": 0.0, "avg_logprob": -0.1035520336295985, "compression_ratio": 1.4472361809045227, "no_speech_prob": 0.005056233610957861}, {"id": 309, "seek": 199464, "start": 2001.2800000000002, "end": 2011.8400000000001, "text": " experience. Welcome. Thank you, Sam. Good morning. Wow, it's fantastic to see you all here.", "tokens": [50696, 1752, 13, 4027, 13, 1044, 291, 11, 4832, 13, 2205, 2446, 13, 3153, 11, 309, 311, 5456, 281, 536, 291, 439, 510, 13, 51224], "temperature": 0.0, "avg_logprob": -0.1035520336295985, "compression_ratio": 1.4472361809045227, "no_speech_prob": 0.005056233610957861}, {"id": 310, "seek": 199464, "start": 2013.2800000000002, "end": 2019.68, "text": " It's been so inspiring to see so many of you infusing AI into your apps. Today, we're launching", "tokens": [51296, 467, 311, 668, 370, 15883, 281, 536, 370, 867, 295, 291, 1536, 7981, 7318, 666, 428, 7733, 13, 2692, 11, 321, 434, 18354, 51616], "temperature": 0.0, "avg_logprob": -0.1035520336295985, "compression_ratio": 1.4472361809045227, "no_speech_prob": 0.005056233610957861}, {"id": 311, "seek": 201968, "start": 2019.68, "end": 2025.68, "text": " new modalities in the API, but we're also very excited to improve the developer experience for", "tokens": [50364, 777, 1072, 16110, 294, 264, 9362, 11, 457, 321, 434, 611, 588, 2919, 281, 3470, 264, 10754, 1752, 337, 50664], "temperature": 0.0, "avg_logprob": -0.09548179919903095, "compression_ratio": 1.58843537414966, "no_speech_prob": 0.006792799569666386}, {"id": 312, "seek": 201968, "start": 2025.68, "end": 2032.24, "text": " you all to build Assistive Agents. So let's dive right in. Imagine I'm building Wanderlust,", "tokens": [50664, 291, 439, 281, 1322, 49633, 488, 2725, 791, 13, 407, 718, 311, 9192, 558, 294, 13, 11739, 286, 478, 2390, 343, 4483, 75, 381, 11, 50992], "temperature": 0.0, "avg_logprob": -0.09548179919903095, "compression_ratio": 1.58843537414966, "no_speech_prob": 0.006792799569666386}, {"id": 313, "seek": 201968, "start": 2032.24, "end": 2037.8400000000001, "text": " a travel app for global explorers, and this is the landing page. I've actually used GPT4 to come", "tokens": [50992, 257, 3147, 724, 337, 4338, 24765, 433, 11, 293, 341, 307, 264, 11202, 3028, 13, 286, 600, 767, 1143, 26039, 51, 19, 281, 808, 51272], "temperature": 0.0, "avg_logprob": -0.09548179919903095, "compression_ratio": 1.58843537414966, "no_speech_prob": 0.006792799569666386}, {"id": 314, "seek": 201968, "start": 2037.8400000000001, "end": 2042.5600000000002, "text": " up with these destination ideas, and for those of you with the keen eye, these illustrations", "tokens": [51272, 493, 365, 613, 12236, 3487, 11, 293, 337, 729, 295, 291, 365, 264, 20297, 3313, 11, 613, 34540, 51508], "temperature": 0.0, "avg_logprob": -0.09548179919903095, "compression_ratio": 1.58843537414966, "no_speech_prob": 0.006792799569666386}, {"id": 315, "seek": 201968, "start": 2042.5600000000002, "end": 2047.8400000000001, "text": " are generated programmatically using the new Dolly 3 API available to all of you today. So", "tokens": [51508, 366, 10833, 37648, 5030, 1228, 264, 777, 1144, 13020, 805, 9362, 2435, 281, 439, 295, 291, 965, 13, 407, 51772], "temperature": 0.0, "avg_logprob": -0.09548179919903095, "compression_ratio": 1.58843537414966, "no_speech_prob": 0.006792799569666386}, {"id": 316, "seek": 204784, "start": 2047.84, "end": 2055.52, "text": " it's pretty remarkable. But let's enhance this app by adding a very simple Assistant to it. This is", "tokens": [50364, 309, 311, 1238, 12802, 13, 583, 718, 311, 11985, 341, 724, 538, 5127, 257, 588, 2199, 14890, 281, 309, 13, 639, 307, 50748], "temperature": 0.0, "avg_logprob": -0.08376757800579071, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.00019090299610979855}, {"id": 317, "seek": 204784, "start": 2055.52, "end": 2059.2799999999997, "text": " the screen. We're going to come back to it in a second. First, I'm going to switch over to the", "tokens": [50748, 264, 2568, 13, 492, 434, 516, 281, 808, 646, 281, 309, 294, 257, 1150, 13, 2386, 11, 286, 478, 516, 281, 3679, 670, 281, 264, 50936], "temperature": 0.0, "avg_logprob": -0.08376757800579071, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.00019090299610979855}, {"id": 318, "seek": 204784, "start": 2059.2799999999997, "end": 2065.12, "text": " new Assistant's Playground. Creating an Assistant is easy. You just give it a name, some initial", "tokens": [50936, 777, 14890, 311, 5506, 2921, 13, 40002, 364, 14890, 307, 1858, 13, 509, 445, 976, 309, 257, 1315, 11, 512, 5883, 51228], "temperature": 0.0, "avg_logprob": -0.08376757800579071, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.00019090299610979855}, {"id": 319, "seek": 204784, "start": 2065.12, "end": 2070.88, "text": " instructions, a model, in this case, I'll pick GPT4 Turbo, and here I'll also go ahead and select", "tokens": [51228, 9415, 11, 257, 2316, 11, 294, 341, 1389, 11, 286, 603, 1888, 26039, 51, 19, 35848, 11, 293, 510, 286, 603, 611, 352, 2286, 293, 3048, 51516], "temperature": 0.0, "avg_logprob": -0.08376757800579071, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.00019090299610979855}, {"id": 320, "seek": 204784, "start": 2070.88, "end": 2077.2, "text": " some tools. I'll turn on code interpreter and retrieval and save. And that's it. Our Assistant", "tokens": [51516, 512, 3873, 13, 286, 603, 1261, 322, 3089, 34132, 293, 19817, 3337, 293, 3155, 13, 400, 300, 311, 309, 13, 2621, 14890, 51832], "temperature": 0.0, "avg_logprob": -0.08376757800579071, "compression_ratio": 1.6187290969899666, "no_speech_prob": 0.00019090299610979855}, {"id": 321, "seek": 207720, "start": 2077.2799999999997, "end": 2083.2, "text": " is ready to go. Next, I can integrate with two new primitives of this Assistant's API,", "tokens": [50368, 307, 1919, 281, 352, 13, 3087, 11, 286, 393, 13365, 365, 732, 777, 2886, 38970, 295, 341, 14890, 311, 9362, 11, 50664], "temperature": 0.0, "avg_logprob": -0.08208524027178364, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0010282801231369376}, {"id": 322, "seek": 207720, "start": 2083.8399999999997, "end": 2091.04, "text": " threads and messages. Let's take a quick look at the code. The process here is very simple. For", "tokens": [50696, 19314, 293, 7897, 13, 961, 311, 747, 257, 1702, 574, 412, 264, 3089, 13, 440, 1399, 510, 307, 588, 2199, 13, 1171, 51056], "temperature": 0.0, "avg_logprob": -0.08208524027178364, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0010282801231369376}, {"id": 323, "seek": 207720, "start": 2091.04, "end": 2096.48, "text": " each new user, I will create a new thread, and as these users engage with their Assistant,", "tokens": [51056, 1184, 777, 4195, 11, 286, 486, 1884, 257, 777, 7207, 11, 293, 382, 613, 5022, 4683, 365, 641, 14890, 11, 51328], "temperature": 0.0, "avg_logprob": -0.08208524027178364, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0010282801231369376}, {"id": 324, "seek": 207720, "start": 2096.48, "end": 2102.72, "text": " I will add their messages to these threads. Very simple. And then I can simply run the Assistant", "tokens": [51328, 286, 486, 909, 641, 7897, 281, 613, 19314, 13, 4372, 2199, 13, 400, 550, 286, 393, 2935, 1190, 264, 14890, 51640], "temperature": 0.0, "avg_logprob": -0.08208524027178364, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0010282801231369376}, {"id": 325, "seek": 210272, "start": 2102.72, "end": 2108.64, "text": " at any time to stream the responses back to the app. So we can return to the app and try that in", "tokens": [50364, 412, 604, 565, 281, 4309, 264, 13019, 646, 281, 264, 724, 13, 407, 321, 393, 2736, 281, 264, 724, 293, 853, 300, 294, 50660], "temperature": 0.0, "avg_logprob": -0.0632996467443613, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.0011500513646751642}, {"id": 326, "seek": 210272, "start": 2108.64, "end": 2118.56, "text": " action. If I say, hey, let's go to Paris. All right. That's it. With just a few lines of code,", "tokens": [50660, 3069, 13, 759, 286, 584, 11, 4177, 11, 718, 311, 352, 281, 8380, 13, 1057, 558, 13, 663, 311, 309, 13, 2022, 445, 257, 1326, 3876, 295, 3089, 11, 51156], "temperature": 0.0, "avg_logprob": -0.0632996467443613, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.0011500513646751642}, {"id": 327, "seek": 210272, "start": 2118.56, "end": 2124.8799999999997, "text": " users can now have a very specialized Assistant right inside the app. And I'd like to highlight", "tokens": [51156, 5022, 393, 586, 362, 257, 588, 19813, 14890, 558, 1854, 264, 724, 13, 400, 286, 1116, 411, 281, 5078, 51472], "temperature": 0.0, "avg_logprob": -0.0632996467443613, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.0011500513646751642}, {"id": 328, "seek": 210272, "start": 2124.8799999999997, "end": 2130.16, "text": " one of my favorite features here, Function Calling. If you have not used it yet, Function Calling is", "tokens": [51472, 472, 295, 452, 2954, 4122, 510, 11, 11166, 882, 44150, 13, 759, 291, 362, 406, 1143, 309, 1939, 11, 11166, 882, 44150, 307, 51736], "temperature": 0.0, "avg_logprob": -0.0632996467443613, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.0011500513646751642}, {"id": 329, "seek": 213016, "start": 2130.16, "end": 2135.8399999999997, "text": " really powerful. And as Sam mentioned, we're taking it a step further today. It now guarantees", "tokens": [50364, 534, 4005, 13, 400, 382, 4832, 2835, 11, 321, 434, 1940, 309, 257, 1823, 3052, 965, 13, 467, 586, 32567, 50648], "temperature": 0.0, "avg_logprob": -0.0801833172639211, "compression_ratio": 1.552, "no_speech_prob": 0.0020762106869369745}, {"id": 330, "seek": 213016, "start": 2135.8399999999997, "end": 2141.6, "text": " the JSON output with no added latency. And you can invoke multiple functions at once for the first", "tokens": [50648, 264, 31828, 5598, 365, 572, 3869, 27043, 13, 400, 291, 393, 41117, 3866, 6828, 412, 1564, 337, 264, 700, 50936], "temperature": 0.0, "avg_logprob": -0.0801833172639211, "compression_ratio": 1.552, "no_speech_prob": 0.0020762106869369745}, {"id": 331, "seek": 213016, "start": 2141.6, "end": 2150.48, "text": " time. So here, if I carry on and say, hey, what are the top 10 things to do? I'm going to have the", "tokens": [50936, 565, 13, 407, 510, 11, 498, 286, 3985, 322, 293, 584, 11, 4177, 11, 437, 366, 264, 1192, 1266, 721, 281, 360, 30, 286, 478, 516, 281, 362, 264, 51380], "temperature": 0.0, "avg_logprob": -0.0801833172639211, "compression_ratio": 1.552, "no_speech_prob": 0.0020762106869369745}, {"id": 332, "seek": 213016, "start": 2150.48, "end": 2155.8399999999997, "text": " Assistant respond to that again. And here, what's interesting is that the Assistant knows about", "tokens": [51380, 14890, 4196, 281, 300, 797, 13, 400, 510, 11, 437, 311, 1880, 307, 300, 264, 14890, 3255, 466, 51648], "temperature": 0.0, "avg_logprob": -0.0801833172639211, "compression_ratio": 1.552, "no_speech_prob": 0.0020762106869369745}, {"id": 333, "seek": 215584, "start": 2155.84, "end": 2160.8, "text": " functions, including those to annotate the map that you see on the right. And so now all of these", "tokens": [50364, 6828, 11, 3009, 729, 281, 25339, 473, 264, 4471, 300, 291, 536, 322, 264, 558, 13, 400, 370, 586, 439, 295, 613, 50612], "temperature": 0.0, "avg_logprob": -0.07821589502794989, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.00044417247409000993}, {"id": 334, "seek": 215584, "start": 2160.8, "end": 2171.6800000000003, "text": " pins are dropping in real time here. Yeah. It's pretty cool. And that integration allows our", "tokens": [50612, 16392, 366, 13601, 294, 957, 565, 510, 13, 865, 13, 467, 311, 1238, 1627, 13, 400, 300, 10980, 4045, 527, 51156], "temperature": 0.0, "avg_logprob": -0.07821589502794989, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.00044417247409000993}, {"id": 335, "seek": 215584, "start": 2171.6800000000003, "end": 2176.96, "text": " natural language interface to interact fluidly with components and features of our app. And it", "tokens": [51156, 3303, 2856, 9226, 281, 4648, 9113, 356, 365, 6677, 293, 4122, 295, 527, 724, 13, 400, 309, 51420], "temperature": 0.0, "avg_logprob": -0.07821589502794989, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.00044417247409000993}, {"id": 336, "seek": 215584, "start": 2176.96, "end": 2183.36, "text": " truly showcases now the harmony you can build between AI and UI where the Assistant is actually", "tokens": [51420, 4908, 29794, 1957, 586, 264, 19410, 291, 393, 1322, 1296, 7318, 293, 15682, 689, 264, 14890, 307, 767, 51740], "temperature": 0.0, "avg_logprob": -0.07821589502794989, "compression_ratio": 1.5809128630705394, "no_speech_prob": 0.00044417247409000993}, {"id": 337, "seek": 218336, "start": 2183.44, "end": 2190.1600000000003, "text": " taking action. But next, let's talk about retrieval. And retrieval is about giving our Assistant more", "tokens": [50368, 1940, 3069, 13, 583, 958, 11, 718, 311, 751, 466, 19817, 3337, 13, 400, 19817, 3337, 307, 466, 2902, 527, 14890, 544, 50704], "temperature": 0.0, "avg_logprob": -0.08541637219880757, "compression_ratio": 1.5134099616858236, "no_speech_prob": 0.002926875604316592}, {"id": 338, "seek": 218336, "start": 2190.1600000000003, "end": 2195.2000000000003, "text": " knowledge beyond these immediate user messages. In fact, I got inspired and I already booked my", "tokens": [50704, 3601, 4399, 613, 11629, 4195, 7897, 13, 682, 1186, 11, 286, 658, 7547, 293, 286, 1217, 26735, 452, 50956], "temperature": 0.0, "avg_logprob": -0.08541637219880757, "compression_ratio": 1.5134099616858236, "no_speech_prob": 0.002926875604316592}, {"id": 339, "seek": 218336, "start": 2195.2000000000003, "end": 2201.2000000000003, "text": " tickets to Paris. So I'm just going to drag and drop here this PDF. While it's uploading, I can", "tokens": [50956, 12628, 281, 8380, 13, 407, 286, 478, 445, 516, 281, 5286, 293, 3270, 510, 341, 17752, 13, 3987, 309, 311, 27301, 11, 286, 393, 51256], "temperature": 0.0, "avg_logprob": -0.08541637219880757, "compression_ratio": 1.5134099616858236, "no_speech_prob": 0.002926875604316592}, {"id": 340, "seek": 218336, "start": 2201.2000000000003, "end": 2208.56, "text": " just sneak peek at it. Very typical United Flight ticket. And behind the scene here, what's happening", "tokens": [51256, 445, 13164, 19604, 412, 309, 13, 4372, 7476, 2824, 28954, 10550, 13, 400, 2261, 264, 4145, 510, 11, 437, 311, 2737, 51624], "temperature": 0.0, "avg_logprob": -0.08541637219880757, "compression_ratio": 1.5134099616858236, "no_speech_prob": 0.002926875604316592}, {"id": 341, "seek": 220856, "start": 2208.56, "end": 2214.72, "text": " is that retrieval is reading these files and boom, the information about this PDF appeared on the", "tokens": [50364, 307, 300, 19817, 3337, 307, 3760, 613, 7098, 293, 9351, 11, 264, 1589, 466, 341, 17752, 8516, 322, 264, 50672], "temperature": 0.0, "avg_logprob": -0.11488748083309251, "compression_ratio": 1.550387596899225, "no_speech_prob": 0.0015002622967585921}, {"id": 342, "seek": 220856, "start": 2214.72, "end": 2223.52, "text": " screen. And this is, of course, a very tiny PDF, but Assistant can parse long-form documents from", "tokens": [50672, 2568, 13, 400, 341, 307, 11, 295, 1164, 11, 257, 588, 5870, 17752, 11, 457, 14890, 393, 48377, 938, 12, 837, 8512, 490, 51112], "temperature": 0.0, "avg_logprob": -0.11488748083309251, "compression_ratio": 1.550387596899225, "no_speech_prob": 0.0015002622967585921}, {"id": 343, "seek": 220856, "start": 2223.52, "end": 2228.24, "text": " extensive text to intricate product specs, depending on what you're building. In fact, I also booked an", "tokens": [51112, 13246, 2487, 281, 38015, 1674, 27911, 11, 5413, 322, 437, 291, 434, 2390, 13, 682, 1186, 11, 286, 611, 26735, 364, 51348], "temperature": 0.0, "avg_logprob": -0.11488748083309251, "compression_ratio": 1.550387596899225, "no_speech_prob": 0.0015002622967585921}, {"id": 344, "seek": 220856, "start": 2228.24, "end": 2233.6, "text": " Airbnb, so I'm just going to drag that over to the conversation as well. And by the way, we've heard", "tokens": [51348, 38232, 11, 370, 286, 478, 445, 516, 281, 5286, 300, 670, 281, 264, 3761, 382, 731, 13, 400, 538, 264, 636, 11, 321, 600, 2198, 51616], "temperature": 0.0, "avg_logprob": -0.11488748083309251, "compression_ratio": 1.550387596899225, "no_speech_prob": 0.0015002622967585921}, {"id": 345, "seek": 223360, "start": 2233.6, "end": 2238.56, "text": " from so many of you developers how hard that is to build yourself. You typically need to compute", "tokens": [50364, 490, 370, 867, 295, 291, 8849, 577, 1152, 300, 307, 281, 1322, 1803, 13, 509, 5850, 643, 281, 14722, 50612], "temperature": 0.0, "avg_logprob": -0.07822614197337299, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.002465849043801427}, {"id": 346, "seek": 223360, "start": 2238.56, "end": 2243.12, "text": " your embeddings. You need to set up chunking algorithm. Now, all of that is taken care of.", "tokens": [50612, 428, 12240, 29432, 13, 509, 643, 281, 992, 493, 16635, 278, 9284, 13, 823, 11, 439, 295, 300, 307, 2726, 1127, 295, 13, 50840], "temperature": 0.0, "avg_logprob": -0.07822614197337299, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.002465849043801427}, {"id": 347, "seek": 223360, "start": 2244.56, "end": 2249.7599999999998, "text": " And there's more than retrieval. With every API call, you usually need to resend the entire", "tokens": [50912, 400, 456, 311, 544, 813, 19817, 3337, 13, 2022, 633, 9362, 818, 11, 291, 2673, 643, 281, 725, 521, 264, 2302, 51172], "temperature": 0.0, "avg_logprob": -0.07822614197337299, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.002465849043801427}, {"id": 348, "seek": 223360, "start": 2249.7599999999998, "end": 2255.36, "text": " conversation history, which means setting up a key value store. That means handling the context", "tokens": [51172, 3761, 2503, 11, 597, 1355, 3287, 493, 257, 2141, 2158, 3531, 13, 663, 1355, 13175, 264, 4319, 51452], "temperature": 0.0, "avg_logprob": -0.07822614197337299, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.002465849043801427}, {"id": 349, "seek": 223360, "start": 2255.36, "end": 2260.64, "text": " windows, serializing messages and so forth. That complexity now completely goes away with this", "tokens": [51452, 9309, 11, 17436, 3319, 7897, 293, 370, 5220, 13, 663, 14024, 586, 2584, 1709, 1314, 365, 341, 51716], "temperature": 0.0, "avg_logprob": -0.07822614197337299, "compression_ratio": 1.637630662020906, "no_speech_prob": 0.002465849043801427}, {"id": 350, "seek": 226064, "start": 2260.64, "end": 2267.2, "text": " new stateful API. But just because a Ponyi is managing this API does not mean it's a black box.", "tokens": [50364, 777, 1785, 906, 9362, 13, 583, 445, 570, 257, 430, 2526, 72, 307, 11642, 341, 9362, 775, 406, 914, 309, 311, 257, 2211, 2424, 13, 50692], "temperature": 0.0, "avg_logprob": -0.10709479649861654, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0018947714706882834}, {"id": 351, "seek": 226064, "start": 2267.2, "end": 2271.8399999999997, "text": " In fact, you can see the steps that the tools are taking right inside your developer dashboard.", "tokens": [50692, 682, 1186, 11, 291, 393, 536, 264, 4439, 300, 264, 3873, 366, 1940, 558, 1854, 428, 10754, 18342, 13, 50924], "temperature": 0.0, "avg_logprob": -0.10709479649861654, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0018947714706882834}, {"id": 352, "seek": 226064, "start": 2271.8399999999997, "end": 2278.08, "text": " So here, if I go ahead and click on threads, this is the thread I believe we're currently", "tokens": [50924, 407, 510, 11, 498, 286, 352, 2286, 293, 2052, 322, 19314, 11, 341, 307, 264, 7207, 286, 1697, 321, 434, 4362, 51236], "temperature": 0.0, "avg_logprob": -0.10709479649861654, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0018947714706882834}, {"id": 353, "seek": 226064, "start": 2278.08, "end": 2283.2, "text": " working on. And see, like, these are all the steps, including the functions being called with the", "tokens": [51236, 1364, 322, 13, 400, 536, 11, 411, 11, 613, 366, 439, 264, 4439, 11, 3009, 264, 6828, 885, 1219, 365, 264, 51492], "temperature": 0.0, "avg_logprob": -0.10709479649861654, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0018947714706882834}, {"id": 354, "seek": 226064, "start": 2283.2, "end": 2290.16, "text": " right parameters and the PDFs I've just uploaded. But let's move on to a new capability that many", "tokens": [51492, 558, 9834, 293, 264, 17752, 82, 286, 600, 445, 17135, 13, 583, 718, 311, 1286, 322, 281, 257, 777, 13759, 300, 867, 51840], "temperature": 0.0, "avg_logprob": -0.10709479649861654, "compression_ratio": 1.606060606060606, "no_speech_prob": 0.0018947714706882834}, {"id": 355, "seek": 229016, "start": 2290.16, "end": 2295.2799999999997, "text": " of you have been requesting for a while. Code interpreter is now available today in the API as", "tokens": [50364, 295, 291, 362, 668, 31937, 337, 257, 1339, 13, 15549, 34132, 307, 586, 2435, 965, 294, 264, 9362, 382, 50620], "temperature": 0.0, "avg_logprob": -0.08872779210408528, "compression_ratio": 1.396039603960396, "no_speech_prob": 0.0008547035395167768}, {"id": 356, "seek": 229016, "start": 2295.2799999999997, "end": 2302.0, "text": " well. That gives the AI the ability to write and execute code on the fly, but even generate files.", "tokens": [50620, 731, 13, 663, 2709, 264, 7318, 264, 3485, 281, 2464, 293, 14483, 3089, 322, 264, 3603, 11, 457, 754, 8460, 7098, 13, 50956], "temperature": 0.0, "avg_logprob": -0.08872779210408528, "compression_ratio": 1.396039603960396, "no_speech_prob": 0.0008547035395167768}, {"id": 357, "seek": 229016, "start": 2302.0, "end": 2310.72, "text": " So let's see that in action. If I say here, hey, will be four friends staying at DCRBNB,", "tokens": [50956, 407, 718, 311, 536, 300, 294, 3069, 13, 759, 286, 584, 510, 11, 4177, 11, 486, 312, 1451, 1855, 7939, 412, 9114, 49, 32006, 33, 11, 51392], "temperature": 0.0, "avg_logprob": -0.08872779210408528, "compression_ratio": 1.396039603960396, "no_speech_prob": 0.0008547035395167768}, {"id": 358, "seek": 231072, "start": 2310.9599999999996, "end": 2325.2799999999997, "text": " what's my share of it plus my flights? All right. Now here, what's happening is that code", "tokens": [50376, 437, 311, 452, 2073, 295, 309, 1804, 452, 21089, 30, 1057, 558, 13, 823, 510, 11, 437, 311, 2737, 307, 300, 3089, 51092], "temperature": 0.0, "avg_logprob": -0.12974813933013588, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0005702462512999773}, {"id": 359, "seek": 231072, "start": 2325.2799999999997, "end": 2330.9599999999996, "text": " interpreter noticed that it should write some code to answer this query. So now it's computing,", "tokens": [51092, 34132, 5694, 300, 309, 820, 2464, 512, 3089, 281, 1867, 341, 14581, 13, 407, 586, 309, 311, 15866, 11, 51376], "temperature": 0.0, "avg_logprob": -0.12974813933013588, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0005702462512999773}, {"id": 360, "seek": 231072, "start": 2330.9599999999996, "end": 2334.8799999999997, "text": " you know, the number of days in Paris, the number of friends, it's also doing some exchange rate", "tokens": [51376, 291, 458, 11, 264, 1230, 295, 1708, 294, 8380, 11, 264, 1230, 295, 1855, 11, 309, 311, 611, 884, 512, 7742, 3314, 51572], "temperature": 0.0, "avg_logprob": -0.12974813933013588, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0005702462512999773}, {"id": 361, "seek": 231072, "start": 2334.8799999999997, "end": 2340.24, "text": " calculation behind the scene to get this answer for us. Not the most complex math, but you get", "tokens": [51572, 17108, 2261, 264, 4145, 281, 483, 341, 1867, 337, 505, 13, 1726, 264, 881, 3997, 5221, 11, 457, 291, 483, 51840], "temperature": 0.0, "avg_logprob": -0.12974813933013588, "compression_ratio": 1.6111111111111112, "no_speech_prob": 0.0005702462512999773}, {"id": 362, "seek": 234024, "start": 2340.24, "end": 2345.3599999999997, "text": " the picture. Imagine you're building a very complex like finance app that's crunching countless", "tokens": [50364, 264, 3036, 13, 11739, 291, 434, 2390, 257, 588, 3997, 411, 10719, 724, 300, 311, 13386, 278, 19223, 50620], "temperature": 0.0, "avg_logprob": -0.08555253346761067, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003072754479944706}, {"id": 363, "seek": 234024, "start": 2345.3599999999997, "end": 2350.4799999999996, "text": " numbers, plotting charts, so really any task that you'd normally tackle with code, then code", "tokens": [50620, 3547, 11, 41178, 17767, 11, 370, 534, 604, 5633, 300, 291, 1116, 5646, 14896, 365, 3089, 11, 550, 3089, 50876], "temperature": 0.0, "avg_logprob": -0.08555253346761067, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003072754479944706}, {"id": 364, "seek": 234024, "start": 2350.4799999999996, "end": 2357.04, "text": " interpreter will work great for you. All right. I think my trip to Paris is sorted. So to recap", "tokens": [50876, 34132, 486, 589, 869, 337, 291, 13, 1057, 558, 13, 286, 519, 452, 4931, 281, 8380, 307, 25462, 13, 407, 281, 20928, 51204], "temperature": 0.0, "avg_logprob": -0.08555253346761067, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003072754479944706}, {"id": 365, "seek": 234024, "start": 2357.04, "end": 2361.8399999999997, "text": " here, we've just seen how you can quickly create an assistant that manages state for your user", "tokens": [51204, 510, 11, 321, 600, 445, 1612, 577, 291, 393, 2661, 1884, 364, 10994, 300, 22489, 1785, 337, 428, 4195, 51444], "temperature": 0.0, "avg_logprob": -0.08555253346761067, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003072754479944706}, {"id": 366, "seek": 234024, "start": 2361.8399999999997, "end": 2366.7999999999997, "text": " conversations, leverages external tools like knowledge and retrieval and code interpreter,", "tokens": [51444, 7315, 11, 12451, 1660, 8320, 3873, 411, 3601, 293, 19817, 3337, 293, 3089, 34132, 11, 51692], "temperature": 0.0, "avg_logprob": -0.08555253346761067, "compression_ratio": 1.6206896551724137, "no_speech_prob": 0.003072754479944706}, {"id": 367, "seek": 236680, "start": 2366.88, "end": 2370.0800000000004, "text": " and finally invokes your own functions to make things happen.", "tokens": [50368, 293, 2721, 1048, 8606, 428, 1065, 6828, 281, 652, 721, 1051, 13, 50528], "temperature": 0.0, "avg_logprob": -0.09588546338288681, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0009097608854062855}, {"id": 368, "seek": 236680, "start": 2371.84, "end": 2376.6400000000003, "text": " But there's one more thing I wanted to show you to kind of really open up the possibilities", "tokens": [50616, 583, 456, 311, 472, 544, 551, 286, 1415, 281, 855, 291, 281, 733, 295, 534, 1269, 493, 264, 12178, 50856], "temperature": 0.0, "avg_logprob": -0.09588546338288681, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0009097608854062855}, {"id": 369, "seek": 236680, "start": 2376.6400000000003, "end": 2380.8, "text": " using function calling combined with our new modalities that we're launching today.", "tokens": [50856, 1228, 2445, 5141, 9354, 365, 527, 777, 1072, 16110, 300, 321, 434, 18354, 965, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09588546338288681, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0009097608854062855}, {"id": 370, "seek": 236680, "start": 2381.6000000000004, "end": 2386.6400000000003, "text": " While working on Dev Day, I've built a small custom assistant that knows everything about", "tokens": [51104, 3987, 1364, 322, 9096, 5226, 11, 286, 600, 3094, 257, 1359, 2375, 10994, 300, 3255, 1203, 466, 51356], "temperature": 0.0, "avg_logprob": -0.09588546338288681, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0009097608854062855}, {"id": 371, "seek": 236680, "start": 2386.6400000000003, "end": 2391.28, "text": " this event. But instead of having a chat interface while running around all day today,", "tokens": [51356, 341, 2280, 13, 583, 2602, 295, 1419, 257, 5081, 9226, 1339, 2614, 926, 439, 786, 965, 11, 51588], "temperature": 0.0, "avg_logprob": -0.09588546338288681, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.0009097608854062855}, {"id": 372, "seek": 239128, "start": 2391.28, "end": 2397.2000000000003, "text": " I thought, why not use voice instead? So let's bring my phone up on screen here so you can see", "tokens": [50364, 286, 1194, 11, 983, 406, 764, 3177, 2602, 30, 407, 718, 311, 1565, 452, 2593, 493, 322, 2568, 510, 370, 291, 393, 536, 50660], "temperature": 0.0, "avg_logprob": -0.09210104100844439, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0012247224804013968}, {"id": 373, "seek": 239128, "start": 2397.2000000000003, "end": 2402.5600000000004, "text": " it on the right. Awesome. So on the right, you can see a very simple Swift app that takes microphone", "tokens": [50660, 309, 322, 264, 558, 13, 10391, 13, 407, 322, 264, 558, 11, 291, 393, 536, 257, 588, 2199, 25539, 724, 300, 2516, 10952, 50928], "temperature": 0.0, "avg_logprob": -0.09210104100844439, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0012247224804013968}, {"id": 374, "seek": 239128, "start": 2402.5600000000004, "end": 2407.36, "text": " input. And on the left, I'm actually going to bring up my terminal log so you can see what's", "tokens": [50928, 4846, 13, 400, 322, 264, 1411, 11, 286, 478, 767, 516, 281, 1565, 493, 452, 14709, 3565, 370, 291, 393, 536, 437, 311, 51168], "temperature": 0.0, "avg_logprob": -0.09210104100844439, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0012247224804013968}, {"id": 375, "seek": 239128, "start": 2407.36, "end": 2414.2400000000002, "text": " happening behind the scenes. So let's give it a shot. Hey there, I'm on the keynote stage right", "tokens": [51168, 2737, 2261, 264, 8026, 13, 407, 718, 311, 976, 309, 257, 3347, 13, 1911, 456, 11, 286, 478, 322, 264, 33896, 3233, 558, 51512], "temperature": 0.0, "avg_logprob": -0.09210104100844439, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0012247224804013968}, {"id": 376, "seek": 241424, "start": 2414.24, "end": 2423.6, "text": " now. Can you greet our attendees here at Dev Day? Hey everyone, welcome to Dev Day.", "tokens": [50364, 586, 13, 1664, 291, 12044, 527, 34826, 510, 412, 9096, 5226, 30, 1911, 1518, 11, 2928, 281, 9096, 5226, 13, 50832], "temperature": 0.0, "avg_logprob": -0.0847843100384968, "compression_ratio": 1.454954954954955, "no_speech_prob": 0.011502300389111042}, {"id": 377, "seek": 241424, "start": 2423.6, "end": 2426.72, "text": " It's awesome to have you all here. Let's make it an incredible day.", "tokens": [50832, 467, 311, 3476, 281, 362, 291, 439, 510, 13, 961, 311, 652, 309, 364, 4651, 786, 13, 50988], "temperature": 0.0, "avg_logprob": -0.0847843100384968, "compression_ratio": 1.454954954954955, "no_speech_prob": 0.011502300389111042}, {"id": 378, "seek": 241424, "start": 2432.7999999999997, "end": 2438.0, "text": " Isn't that impressive? You have six unique and rich voices to choose from in the API,", "tokens": [51292, 6998, 380, 300, 8992, 30, 509, 362, 2309, 3845, 293, 4593, 9802, 281, 2826, 490, 294, 264, 9362, 11, 51552], "temperature": 0.0, "avg_logprob": -0.0847843100384968, "compression_ratio": 1.454954954954955, "no_speech_prob": 0.011502300389111042}, {"id": 379, "seek": 241424, "start": 2438.0, "end": 2442.08, "text": " each speaking multiple languages so you can really find the perfect fit for your app.", "tokens": [51552, 1184, 4124, 3866, 8650, 370, 291, 393, 534, 915, 264, 2176, 3318, 337, 428, 724, 13, 51756], "temperature": 0.0, "avg_logprob": -0.0847843100384968, "compression_ratio": 1.454954954954955, "no_speech_prob": 0.011502300389111042}, {"id": 380, "seek": 244208, "start": 2442.64, "end": 2446.4, "text": " And on my laptop here on the left, you can see the logs of what's happening behind the scenes too.", "tokens": [50392, 400, 322, 452, 10732, 510, 322, 264, 1411, 11, 291, 393, 536, 264, 20820, 295, 437, 311, 2737, 2261, 264, 8026, 886, 13, 50580], "temperature": 0.0, "avg_logprob": -0.09511249470260907, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.0008965307497419417}, {"id": 381, "seek": 244208, "start": 2446.4, "end": 2451.92, "text": " So I'm using Whisper to convert the voice inputs into text, an assistant with GPT4 Turbo,", "tokens": [50580, 407, 286, 478, 1228, 41132, 610, 281, 7620, 264, 3177, 15743, 666, 2487, 11, 364, 10994, 365, 26039, 51, 19, 35848, 11, 50856], "temperature": 0.0, "avg_logprob": -0.09511249470260907, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.0008965307497419417}, {"id": 382, "seek": 244208, "start": 2451.92, "end": 2457.7599999999998, "text": " and finally the new TTS API to make it speak. But thanks to function calling,", "tokens": [50856, 293, 2721, 264, 777, 314, 7327, 9362, 281, 652, 309, 1710, 13, 583, 3231, 281, 2445, 5141, 11, 51148], "temperature": 0.0, "avg_logprob": -0.09511249470260907, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.0008965307497419417}, {"id": 383, "seek": 244208, "start": 2457.7599999999998, "end": 2462.24, "text": " things get even more interesting when the assistant can connect to the Internet", "tokens": [51148, 721, 483, 754, 544, 1880, 562, 264, 10994, 393, 1745, 281, 264, 7703, 51372], "temperature": 0.0, "avg_logprob": -0.09511249470260907, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.0008965307497419417}, {"id": 384, "seek": 244208, "start": 2462.24, "end": 2467.84, "text": " and take real actions for users. So let's do something even more exciting here together.", "tokens": [51372, 293, 747, 957, 5909, 337, 5022, 13, 407, 718, 311, 360, 746, 754, 544, 4670, 510, 1214, 13, 51652], "temperature": 0.0, "avg_logprob": -0.09511249470260907, "compression_ratio": 1.5934065934065933, "no_speech_prob": 0.0008965307497419417}, {"id": 385, "seek": 246784, "start": 2467.92, "end": 2475.28, "text": " How about this? Hey assistant, can you randomly select five Dev Day attendees here", "tokens": [50368, 1012, 466, 341, 30, 1911, 10994, 11, 393, 291, 16979, 3048, 1732, 9096, 5226, 34826, 510, 50736], "temperature": 0.0, "avg_logprob": -0.13382552100009606, "compression_ratio": 1.49079754601227, "no_speech_prob": 0.002979982178658247}, {"id": 386, "seek": 246784, "start": 2475.28, "end": 2482.6400000000003, "text": " and give them $500 in OpenAI credits? Yes, checking the list of attendees.", "tokens": [50736, 293, 976, 552, 1848, 7526, 294, 7238, 48698, 16816, 30, 1079, 11, 8568, 264, 1329, 295, 34826, 13, 51104], "temperature": 0.0, "avg_logprob": -0.13382552100009606, "compression_ratio": 1.49079754601227, "no_speech_prob": 0.002979982178658247}, {"id": 387, "seek": 246784, "start": 2487.28, "end": 2492.32, "text": " Done, I picked five Dev Day attendees and added $500 of API credits to their account.", "tokens": [51336, 18658, 11, 286, 6183, 1732, 9096, 5226, 34826, 293, 3869, 1848, 7526, 295, 9362, 16816, 281, 641, 2696, 13, 51588], "temperature": 0.0, "avg_logprob": -0.13382552100009606, "compression_ratio": 1.49079754601227, "no_speech_prob": 0.002979982178658247}, {"id": 388, "seek": 249232, "start": 2492.4, "end": 2497.1200000000003, "text": " Congrats to Christine M, Jonathan C, Steven G, Louise K, and Suraj S.", "tokens": [50368, 40219, 281, 24038, 376, 11, 15471, 383, 11, 12754, 460, 11, 35962, 591, 11, 293, 6732, 1805, 318, 13, 50604], "temperature": 0.0, "avg_logprob": -0.13449127302257294, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.001168672228232026}, {"id": 389, "seek": 249232, "start": 2498.1600000000003, "end": 2501.2000000000003, "text": " All right, if you recognize yourself, awesome, congrats.", "tokens": [50656, 1057, 558, 11, 498, 291, 5521, 1803, 11, 3476, 11, 8882, 1720, 13, 50808], "temperature": 0.0, "avg_logprob": -0.13449127302257294, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.001168672228232026}, {"id": 390, "seek": 249232, "start": 2503.36, "end": 2506.88, "text": " And that's it, a quick overview today of the new Assistant API", "tokens": [50916, 400, 300, 311, 309, 11, 257, 1702, 12492, 965, 295, 264, 777, 14890, 9362, 51092], "temperature": 0.0, "avg_logprob": -0.13449127302257294, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.001168672228232026}, {"id": 391, "seek": 249232, "start": 2506.88, "end": 2509.6800000000003, "text": " combined with some of the new tools and modalities that we launched.", "tokens": [51092, 9354, 365, 512, 295, 264, 777, 3873, 293, 1072, 16110, 300, 321, 8730, 13, 51232], "temperature": 0.0, "avg_logprob": -0.13449127302257294, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.001168672228232026}, {"id": 392, "seek": 249232, "start": 2510.48, "end": 2515.52, "text": " All starting with the simplicity of a rich text or voice conversation for you and users.", "tokens": [51272, 1057, 2891, 365, 264, 25632, 295, 257, 4593, 2487, 420, 3177, 3761, 337, 291, 293, 5022, 13, 51524], "temperature": 0.0, "avg_logprob": -0.13449127302257294, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.001168672228232026}, {"id": 393, "seek": 249232, "start": 2516.0800000000004, "end": 2519.36, "text": " We really can't wait to see what you build and congrats to our lucky winners.", "tokens": [51552, 492, 534, 393, 380, 1699, 281, 536, 437, 291, 1322, 293, 8882, 1720, 281, 527, 6356, 17193, 13, 51716], "temperature": 0.0, "avg_logprob": -0.13449127302257294, "compression_ratio": 1.551094890510949, "no_speech_prob": 0.001168672228232026}, {"id": 394, "seek": 251936, "start": 2520.08, "end": 2524.88, "text": " Actually, you know what? You're all part of this amazing OpenAI community here,", "tokens": [50400, 5135, 11, 291, 458, 437, 30, 509, 434, 439, 644, 295, 341, 2243, 7238, 48698, 1768, 510, 11, 50640], "temperature": 0.0, "avg_logprob": -0.13586270181756271, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.006002417765557766}, {"id": 395, "seek": 251936, "start": 2524.88, "end": 2528.32, "text": " so I'm just going to talk to my assistant one last time before I step off the stage.", "tokens": [50640, 370, 286, 478, 445, 516, 281, 751, 281, 452, 10994, 472, 1036, 565, 949, 286, 1823, 766, 264, 3233, 13, 50812], "temperature": 0.0, "avg_logprob": -0.13586270181756271, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.006002417765557766}, {"id": 396, "seek": 251936, "start": 2530.56, "end": 2536.0, "text": " Hey assistant, can you actually give everyone here in the audience $500 in OpenAI credits?", "tokens": [50924, 1911, 10994, 11, 393, 291, 767, 976, 1518, 510, 294, 264, 4034, 1848, 7526, 294, 7238, 48698, 16816, 30, 51196], "temperature": 0.0, "avg_logprob": -0.13586270181756271, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.006002417765557766}, {"id": 397, "seek": 251936, "start": 2537.84, "end": 2539.76, "text": " Sounds great. Let me go through everyone.", "tokens": [51288, 14576, 869, 13, 961, 385, 352, 807, 1518, 13, 51384], "temperature": 0.0, "avg_logprob": -0.13586270181756271, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.006002417765557766}, {"id": 398, "seek": 253976, "start": 2540.48, "end": 2552.0, "text": " All right, that function will keep running, but I've run out of time,", "tokens": [50400, 1057, 558, 11, 300, 2445, 486, 1066, 2614, 11, 457, 286, 600, 1190, 484, 295, 565, 11, 50976], "temperature": 0.0, "avg_logprob": -0.22586129636180644, "compression_ratio": 1.220472440944882, "no_speech_prob": 0.003428617026656866}, {"id": 399, "seek": 253976, "start": 2552.0, "end": 2554.96, "text": " so thank you so much, everyone. Have a great day. Back to you, Sam.", "tokens": [50976, 370, 1309, 291, 370, 709, 11, 1518, 13, 3560, 257, 869, 786, 13, 5833, 281, 291, 11, 4832, 13, 51124], "temperature": 0.0, "avg_logprob": -0.22586129636180644, "compression_ratio": 1.220472440944882, "no_speech_prob": 0.003428617026656866}, {"id": 400, "seek": 253976, "start": 2564.4, "end": 2565.1200000000003, "text": " Pretty cool, huh?", "tokens": [51596, 10693, 1627, 11, 7020, 30, 51632], "temperature": 0.0, "avg_logprob": -0.22586129636180644, "compression_ratio": 1.220472440944882, "no_speech_prob": 0.003428617026656866}, {"id": 401, "seek": 256512, "start": 2565.6, "end": 2574.4, "text": " All right, so that Assistant API goes into beta today, and we are super excited to see what", "tokens": [50388, 1057, 558, 11, 370, 300, 14890, 9362, 1709, 666, 9861, 965, 11, 293, 321, 366, 1687, 2919, 281, 536, 437, 50828], "temperature": 0.0, "avg_logprob": -0.1073922896653079, "compression_ratio": 1.5265486725663717, "no_speech_prob": 0.0002491635095793754}, {"id": 402, "seek": 256512, "start": 2574.4, "end": 2582.0, "text": " you all do with it. Anybody can enable it. Over time, GPTs and assistants, our precursors to agents,", "tokens": [50828, 291, 439, 360, 365, 309, 13, 19082, 393, 9528, 309, 13, 4886, 565, 11, 26039, 33424, 293, 34949, 11, 527, 41736, 830, 281, 12554, 11, 51208], "temperature": 0.0, "avg_logprob": -0.1073922896653079, "compression_ratio": 1.5265486725663717, "no_speech_prob": 0.0002491635095793754}, {"id": 403, "seek": 256512, "start": 2582.7999999999997, "end": 2586.7999999999997, "text": " are going to be able to do much, much more. They'll gradually be able to plan", "tokens": [51248, 366, 516, 281, 312, 1075, 281, 360, 709, 11, 709, 544, 13, 814, 603, 13145, 312, 1075, 281, 1393, 51448], "temperature": 0.0, "avg_logprob": -0.1073922896653079, "compression_ratio": 1.5265486725663717, "no_speech_prob": 0.0002491635095793754}, {"id": 404, "seek": 256512, "start": 2587.3599999999997, "end": 2592.56, "text": " and to perform more complex actions on your behalf. As I mentioned before,", "tokens": [51476, 293, 281, 2042, 544, 3997, 5909, 322, 428, 9490, 13, 1018, 286, 2835, 949, 11, 51736], "temperature": 0.0, "avg_logprob": -0.1073922896653079, "compression_ratio": 1.5265486725663717, "no_speech_prob": 0.0002491635095793754}, {"id": 405, "seek": 259256, "start": 2592.56, "end": 2595.92, "text": " we really believe in the importance of gradual iterative deployment.", "tokens": [50364, 321, 534, 1697, 294, 264, 7379, 295, 32890, 17138, 1166, 19317, 13, 50532], "temperature": 0.0, "avg_logprob": -0.07492381914527016, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.0006663929088972509}, {"id": 406, "seek": 259256, "start": 2596.72, "end": 2600.7999999999997, "text": " We believe it's important for people to start building with and using these agents now", "tokens": [50572, 492, 1697, 309, 311, 1021, 337, 561, 281, 722, 2390, 365, 293, 1228, 613, 12554, 586, 50776], "temperature": 0.0, "avg_logprob": -0.07492381914527016, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.0006663929088972509}, {"id": 407, "seek": 259256, "start": 2600.7999999999997, "end": 2604.4, "text": " to get a feel for what the world is going to be like as they become more capable,", "tokens": [50776, 281, 483, 257, 841, 337, 437, 264, 1002, 307, 516, 281, 312, 411, 382, 436, 1813, 544, 8189, 11, 50956], "temperature": 0.0, "avg_logprob": -0.07492381914527016, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.0006663929088972509}, {"id": 408, "seek": 259256, "start": 2605.36, "end": 2609.84, "text": " and as we've always done, we'll continue to update our systems based off of your feedback.", "tokens": [51004, 293, 382, 321, 600, 1009, 1096, 11, 321, 603, 2354, 281, 5623, 527, 3652, 2361, 766, 295, 428, 5824, 13, 51228], "temperature": 0.0, "avg_logprob": -0.07492381914527016, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.0006663929088972509}, {"id": 409, "seek": 259256, "start": 2611.12, "end": 2614.56, "text": " So we're super excited that we got to share all of this with you today.", "tokens": [51292, 407, 321, 434, 1687, 2919, 300, 321, 658, 281, 2073, 439, 295, 341, 365, 291, 965, 13, 51464], "temperature": 0.0, "avg_logprob": -0.07492381914527016, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.0006663929088972509}, {"id": 410, "seek": 259256, "start": 2615.84, "end": 2620.96, "text": " We introduced GPTs, custom versions of chat GPT that combine instructions,", "tokens": [51528, 492, 7268, 26039, 33424, 11, 2375, 9606, 295, 5081, 26039, 51, 300, 10432, 9415, 11, 51784], "temperature": 0.0, "avg_logprob": -0.07492381914527016, "compression_ratio": 1.643598615916955, "no_speech_prob": 0.0006663929088972509}, {"id": 411, "seek": 262096, "start": 2621.6, "end": 2626.96, "text": " extended knowledge, and actions. We launched the Assistance API to make it easier to build", "tokens": [50396, 10913, 3601, 11, 293, 5909, 13, 492, 8730, 264, 46805, 9362, 281, 652, 309, 3571, 281, 1322, 50664], "temperature": 0.0, "avg_logprob": -0.09850753250942436, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.0006876171100884676}, {"id": 412, "seek": 262096, "start": 2626.96, "end": 2632.8, "text": " assisted experiences with your own apps. These are our first steps towards AI agents,", "tokens": [50664, 30291, 5235, 365, 428, 1065, 7733, 13, 1981, 366, 527, 700, 4439, 3030, 7318, 12554, 11, 50956], "temperature": 0.0, "avg_logprob": -0.09850753250942436, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.0006876171100884676}, {"id": 413, "seek": 262096, "start": 2632.8, "end": 2639.2, "text": " and we'll be increasing their capabilities over time. We introduced a new GPT-4 turbo model that", "tokens": [50956, 293, 321, 603, 312, 5662, 641, 10862, 670, 565, 13, 492, 7268, 257, 777, 26039, 51, 12, 19, 20902, 2316, 300, 51276], "temperature": 0.0, "avg_logprob": -0.09850753250942436, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.0006876171100884676}, {"id": 414, "seek": 262096, "start": 2639.2, "end": 2644.2400000000002, "text": " delivers improved function calling, knowledge, lowered pricing, new modalities, and more,", "tokens": [51276, 24860, 9689, 2445, 5141, 11, 3601, 11, 28466, 17621, 11, 777, 1072, 16110, 11, 293, 544, 11, 51528], "temperature": 0.0, "avg_logprob": -0.09850753250942436, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.0006876171100884676}, {"id": 415, "seek": 262096, "start": 2645.04, "end": 2646.96, "text": " and we're deepening our partnership with Microsoft.", "tokens": [51568, 293, 321, 434, 2452, 4559, 527, 9982, 365, 8116, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09850753250942436, "compression_ratio": 1.5601503759398496, "no_speech_prob": 0.0006876171100884676}, {"id": 416, "seek": 264696, "start": 2647.92, "end": 2654.48, "text": " In closing, I wanted to take a minute to thank the team that creates all of this. Open AI has", "tokens": [50412, 682, 10377, 11, 286, 1415, 281, 747, 257, 3456, 281, 1309, 264, 1469, 300, 7829, 439, 295, 341, 13, 7238, 7318, 575, 50740], "temperature": 0.0, "avg_logprob": -0.08736681305201707, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.00033523107413202524}, {"id": 417, "seek": 264696, "start": 2654.48, "end": 2660.08, "text": " gotten remarkable talent density, but still, it takes a huge amount of hard work and coordination", "tokens": [50740, 5768, 12802, 8301, 10305, 11, 457, 920, 11, 309, 2516, 257, 2603, 2372, 295, 1152, 589, 293, 21252, 51020], "temperature": 0.0, "avg_logprob": -0.08736681305201707, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.00033523107413202524}, {"id": 418, "seek": 264696, "start": 2660.08, "end": 2663.76, "text": " to make all of this happen. I truly believe that I've got the best colleagues in the world.", "tokens": [51020, 281, 652, 439, 295, 341, 1051, 13, 286, 4908, 1697, 300, 286, 600, 658, 264, 1151, 7734, 294, 264, 1002, 13, 51204], "temperature": 0.0, "avg_logprob": -0.08736681305201707, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.00033523107413202524}, {"id": 419, "seek": 264696, "start": 2663.76, "end": 2669.68, "text": " I feel incredibly grateful to get to work with them. We do all of this because we believe the AI", "tokens": [51204, 286, 841, 6252, 7941, 281, 483, 281, 589, 365, 552, 13, 492, 360, 439, 295, 341, 570, 321, 1697, 264, 7318, 51500], "temperature": 0.0, "avg_logprob": -0.08736681305201707, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.00033523107413202524}, {"id": 420, "seek": 264696, "start": 2669.68, "end": 2674.56, "text": " is going to be a technological and societal revolution. It will change the world in many ways,", "tokens": [51500, 307, 516, 281, 312, 257, 18439, 293, 33472, 8894, 13, 467, 486, 1319, 264, 1002, 294, 867, 2098, 11, 51744], "temperature": 0.0, "avg_logprob": -0.08736681305201707, "compression_ratio": 1.6725352112676057, "no_speech_prob": 0.00033523107413202524}, {"id": 421, "seek": 267456, "start": 2675.2799999999997, "end": 2679.6, "text": " and we're happy to get to work on something that will empower all of you to build so much for all", "tokens": [50400, 293, 321, 434, 2055, 281, 483, 281, 589, 322, 746, 300, 486, 11071, 439, 295, 291, 281, 1322, 370, 709, 337, 439, 50616], "temperature": 0.0, "avg_logprob": -0.058429855484146254, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0008827211568132043}, {"id": 422, "seek": 267456, "start": 2679.6, "end": 2685.68, "text": " of us. We talked about earlier how if you give people better tools, they can change the world.", "tokens": [50616, 295, 505, 13, 492, 2825, 466, 3071, 577, 498, 291, 976, 561, 1101, 3873, 11, 436, 393, 1319, 264, 1002, 13, 50920], "temperature": 0.0, "avg_logprob": -0.058429855484146254, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0008827211568132043}, {"id": 423, "seek": 267456, "start": 2686.7999999999997, "end": 2690.64, "text": " We believe that AI will be about individual empowerment and agency at a scale that we've", "tokens": [50976, 492, 1697, 300, 7318, 486, 312, 466, 2609, 34825, 293, 7934, 412, 257, 4373, 300, 321, 600, 51168], "temperature": 0.0, "avg_logprob": -0.058429855484146254, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0008827211568132043}, {"id": 424, "seek": 267456, "start": 2690.64, "end": 2695.12, "text": " never seen before, and that will elevate humanity to a scale that we've never seen before either.", "tokens": [51168, 1128, 1612, 949, 11, 293, 300, 486, 33054, 10243, 281, 257, 4373, 300, 321, 600, 1128, 1612, 949, 2139, 13, 51392], "temperature": 0.0, "avg_logprob": -0.058429855484146254, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0008827211568132043}, {"id": 425, "seek": 267456, "start": 2695.92, "end": 2701.68, "text": " We'll be able to do more to create more and to have more. As intelligence gets integrated", "tokens": [51432, 492, 603, 312, 1075, 281, 360, 544, 281, 1884, 544, 293, 281, 362, 544, 13, 1018, 7599, 2170, 10919, 51720], "temperature": 0.0, "avg_logprob": -0.058429855484146254, "compression_ratio": 1.8038461538461539, "no_speech_prob": 0.0008827211568132043}, {"id": 426, "seek": 270168, "start": 2701.68, "end": 2707.2, "text": " everywhere, we will all have superpowers on demand. We're excited to see what you all will do with", "tokens": [50364, 5315, 11, 321, 486, 439, 362, 1687, 47953, 322, 4733, 13, 492, 434, 2919, 281, 536, 437, 291, 439, 486, 360, 365, 50640], "temperature": 0.0, "avg_logprob": -0.08260228124897132, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.004976008553057909}, {"id": 427, "seek": 270168, "start": 2707.2, "end": 2711.3599999999997, "text": " this technology and to discover the new future that we're all going to architect together.", "tokens": [50640, 341, 2899, 293, 281, 4411, 264, 777, 2027, 300, 321, 434, 439, 516, 281, 6331, 1214, 13, 50848], "temperature": 0.0, "avg_logprob": -0.08260228124897132, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.004976008553057909}, {"id": 428, "seek": 270168, "start": 2712.8799999999997, "end": 2717.2, "text": " We hope that you'll come back next year. What we launched today is going to look very quaint", "tokens": [50924, 492, 1454, 300, 291, 603, 808, 646, 958, 1064, 13, 708, 321, 8730, 965, 307, 516, 281, 574, 588, 421, 5114, 51140], "temperature": 0.0, "avg_logprob": -0.08260228124897132, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.004976008553057909}, {"id": 429, "seek": 270168, "start": 2717.2, "end": 2721.04, "text": " relative to what we're busy creating for you now. Thank you for all that you do.", "tokens": [51140, 4972, 281, 437, 321, 434, 5856, 4084, 337, 291, 586, 13, 1044, 291, 337, 439, 300, 291, 360, 13, 51332], "temperature": 0.0, "avg_logprob": -0.08260228124897132, "compression_ratio": 1.635135135135135, "no_speech_prob": 0.004976008553057909}, {"id": 430, "seek": 272104, "start": 2721.04, "end": 2735.6, "text": " Thanks for coming here today.", "tokens": [50364, 2561, 337, 1348, 510, 965, 13, 51092], "temperature": 0.0, "avg_logprob": -0.46703020731608075, "compression_ratio": 0.7837837837837838, "no_speech_prob": 0.015615032985806465}], "language": "en"}