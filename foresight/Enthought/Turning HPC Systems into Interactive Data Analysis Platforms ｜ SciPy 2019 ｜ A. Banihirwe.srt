1
00:00:00,000 --> 00:00:07,680
Good afternoon everyone. Welcome back to room 305. I see how we are full as normal. If you guys can

2
00:00:08,320 --> 00:00:13,280
try to get into the middle to find a seat, you know, fire code and stuff like that,

3
00:00:14,000 --> 00:00:20,720
that would be very, very much appreciated. This is the last talk in our Geophysics,

4
00:00:21,600 --> 00:00:27,680
Geology and Atmospheric Sciences session, and it goes to Addison. And how do you pronounce

5
00:00:27,760 --> 00:00:33,280
your last name, Addison? I'm going to leave that one with him and not even attempt it,

6
00:00:34,080 --> 00:00:39,360
who's going to talk about interactive super computing with Jupiter and Dask. Thank you very

7
00:00:39,360 --> 00:00:49,280
much, Addison. Thank you, Scott. Great. Thank you for having me here. This is my first sci-pi,

8
00:00:49,280 --> 00:00:57,600
so I'm very excited to be here. So as Scott said, so my name is Anderson, I work as a software

9
00:00:57,680 --> 00:01:03,760
engineer at the National Center for Atmospheric Research. So for those who are interested,

10
00:01:03,760 --> 00:01:10,800
the slides available address. So this is a talk about Python tools that enable

11
00:01:12,640 --> 00:01:17,120
interactive super computing, and we're going to see what we mean by super computing in a minute.

12
00:01:17,200 --> 00:01:28,800
Okay, so this is Alice, and Alice is a project scientist at NCAR. She studies the transfer of

13
00:01:28,800 --> 00:01:36,240
water and energy between the land surface and the lower atmosphere. And on the other side,

14
00:01:36,240 --> 00:01:42,400
we have a cool notebook of how work. There are three things that are really interesting about

15
00:01:42,400 --> 00:01:49,760
this notebook. So the first thing is that this notebook is not running on her computer. So this

16
00:01:49,760 --> 00:01:56,080
is running on a supercomputer. In this case, this is Cheyenne. And as you can see, there's an address

17
00:01:56,080 --> 00:02:03,120
to a Jupiter hub running on the Cheyenne supercomputer. The second thing is that she's using

18
00:02:04,480 --> 00:02:11,440
hundreds of processes and terabytes of memory. All this is distributed computing resources.

19
00:02:12,560 --> 00:02:18,800
Thanks to desktop queue. We're going to talk about that as well. And the third thing is she's

20
00:02:18,800 --> 00:02:25,040
actually doing science. So this is not a toy example, so which is what she's good at. So

21
00:02:26,000 --> 00:02:32,720
now we're going to talk about how we at NCAR enable Alice and folks like her to do interactive

22
00:02:32,720 --> 00:02:43,920
supercomputing. So what do we mean by supercomputing? First, MPI, batch processing, lots of heavy

23
00:02:43,920 --> 00:02:49,760
machines that most of people don't have access to as admins. So you have to talk to people to be able

24
00:02:49,760 --> 00:02:56,480
to install some of the software. In the picture, so this is Cheyenne, which is a supercomputer

25
00:02:56,480 --> 00:03:04,080
operated by NCAR. If you're ever in Cheyenne, Wyoming, feel free to stop by. They give free

26
00:03:04,080 --> 00:03:10,880
tours of the facility if you want to. And what do we mean by interactive supercomputing?

27
00:03:12,480 --> 00:03:20,240
So with the current growth in data creation, both through new simulations and new observations,

28
00:03:20,240 --> 00:03:26,960
there's a growing need to have a more human in the loop workflow where people can rapidly prototype

29
00:03:26,960 --> 00:03:35,120
and iterate through the data with tools like Jupyter notebooks. There's also a need to do

30
00:03:35,120 --> 00:03:40,800
things like in situ data analytics. So instead of having to run the simulations and output the data,

31
00:03:40,800 --> 00:03:47,120
you could actually run the simulation and do the analysis at the same time. And the other thing

32
00:03:47,120 --> 00:03:53,840
is actually adaptive scaling of computing resources. So this combination would really be powerful,

33
00:03:53,840 --> 00:04:02,880
but it's really hard for different reasons. So some of these reasons are cultural and others

34
00:04:02,880 --> 00:04:11,200
are technical. So the first one is that HP systems tends to be unique. So every HPC center has its

35
00:04:11,200 --> 00:04:16,960
own policies when it comes to things like security, what can you run on the supercomputer.

36
00:04:18,880 --> 00:04:25,280
And the second point is the tension between interactivity and machine utilization. So when it

37
00:04:25,280 --> 00:04:34,320
comes to HPC systems, HP center stands to be measured according to how often the machine is

38
00:04:34,320 --> 00:04:40,400
actually utilized. And when you're talking about things like interactivity, users want to have

39
00:04:41,440 --> 00:04:48,240
resources on demand whenever they want them. But because HP system operate on batch Q systems,

40
00:04:48,240 --> 00:04:53,280
it's not that easy to actually get resources whenever you want to. So no user is going to

41
00:04:53,280 --> 00:05:00,320
wait for five hours to get into the queue to do interactive data analysis. And the third point

42
00:05:00,320 --> 00:05:05,840
is the lack of elastic scaling. And this is a huge, at least in my opinion, I think this is a huge

43
00:05:05,840 --> 00:05:11,760
disadvantage in that if someone, let's say, want to do some analysis, they have to take time and

44
00:05:11,760 --> 00:05:17,040
actually think about how much resources they want before they even do the analysis. And what ends

45
00:05:17,040 --> 00:05:24,560
up happening is that you get into your work, maybe five minutes later, you probably take like 20

46
00:05:24,560 --> 00:05:29,200
minutes thinking, what do I do next? And during that time, the resources are just idle. So it would

47
00:05:29,200 --> 00:05:35,520
be nice if you could actually scale down later that people use those resources and get them back

48
00:05:35,520 --> 00:05:43,280
whenever you actually need to. The good news is that, despite all those challenges, there's a

49
00:05:43,280 --> 00:05:50,640
growing list of technologies or tools that try to help with interactive supercomputing. And I'm

50
00:05:50,640 --> 00:05:58,000
going to talk about some of these tools. The first one is Trubiter. So we all love and use

51
00:05:58,000 --> 00:06:03,840
Trubiter. And so I'm not really going to spend much time on it because now everybody is familiar

52
00:06:03,840 --> 00:06:12,640
with it. And some of you may be wondering, but isn't Trubiter already usable on HP systems? And

53
00:06:12,640 --> 00:06:19,200
the answer is actually, yes, but you have to go through all these steps. You have to essentially

54
00:06:19,200 --> 00:06:26,240
tune the machine. You have to set up SSH tunnels. And you have to do this every single time that

55
00:06:26,240 --> 00:06:32,960
you want to actually use the Trubiter notebook on the supercomputer. And this can actually be tedious,

56
00:06:33,040 --> 00:06:41,040
especially for new users. So what is missing? So one of the things that is actually missing is

57
00:06:41,040 --> 00:06:45,920
the multi-user support. So all those steps, everybody has to go through them individually.

58
00:06:45,920 --> 00:06:50,240
And it would be nice if there was one single place that everybody goes to and everything is

59
00:06:50,240 --> 00:06:56,720
done for them. Another thing is actually the lack of pure web access to HPC resources. Because as

60
00:06:56,720 --> 00:07:03,760
you've seen, we're setting up SSH tunnel, which if you ever need to use another process that probably

61
00:07:03,760 --> 00:07:07,840
runs on a different port, so you also have to SSH to create a tunnel for that port as well.

62
00:07:09,760 --> 00:07:17,680
Well, then Trubiter Hub comes to the rescue. So one good thing about Trubiter Hub is that,

63
00:07:17,680 --> 00:07:24,800
in a way, it provides a standard way of managing authentication. So one of the issues that is not

64
00:07:24,800 --> 00:07:32,480
really hard to convince sysadmins about is the security of web applications. But Trubiter Hub

65
00:07:32,480 --> 00:07:37,600
makes it easy in that it doesn't really force you to use any type of authentication. It's up to you

66
00:07:37,600 --> 00:07:42,960
to choose what you want to use for authentication. In this case, on the supercomputer, you have

67
00:07:42,960 --> 00:07:48,640
different types of authentications. And another thing is that Trubiter Hub will just take care of

68
00:07:48,720 --> 00:07:56,080
spawning the Trubiter notebook single server and giving access to the user on demand.

69
00:07:58,720 --> 00:08:05,920
And so let's now switch gears and actually see how this works in practice. So I will skip these

70
00:08:05,920 --> 00:08:14,080
few slides and actually go to the live demo. So the first thing that you do as a user, so you just

71
00:08:14,080 --> 00:08:27,680
go to Trubiter Hub.ucr.edu. And so this is what you presented. And if you've used the

72
00:08:27,680 --> 00:08:32,400
Trubiter Hub before, this looks familiar. The only difference is that in this case,

73
00:08:33,280 --> 00:08:39,600
I have to use the same authentication that I used to SSH into the machine. So in my case,

74
00:08:39,600 --> 00:08:46,160
I just provide my username. And then for the password, it's a combination of a PIN number

75
00:08:47,040 --> 00:08:57,040
and a UB key token. So now, so now I'm authenticated. So I then get this page,

76
00:08:57,040 --> 00:09:03,040
which basically asked me for the project account. So this is the allocation on the

77
00:09:03,040 --> 00:09:09,600
supercomputer so that they know who to charge for this usage. So in this case, I'm going to

78
00:09:10,800 --> 00:09:18,960
provide when to change the queue here and specify the project

79
00:09:22,080 --> 00:09:30,480
and maybe just reduce the wall time. And then once this is done, I just click on spawn.

80
00:09:31,440 --> 00:09:36,480
And what this is doing in the background, it's basically submitting a job to the queuing system.

81
00:09:37,120 --> 00:09:46,480
And once the Trubiter Notebook server is ready, I will get redirected to this new page. And at this

82
00:09:46,480 --> 00:09:54,480
point, thank you. So at this point, you have the same interface as what you would have on your

83
00:09:54,480 --> 00:10:02,720
laptop. So I can run notebooks, in this case, to show you that I'm not really lying. So

84
00:10:06,560 --> 00:10:12,480
you could just run this notebook, which is backed by Bashkarno. And as you can see,

85
00:10:12,480 --> 00:10:16,400
I don't really have 200 terabytes of storage on this computer. So

86
00:10:17,360 --> 00:10:30,800
that's it about Trubiter Hubs. I'll come back to it later. So the next tool is Dask.

87
00:10:33,680 --> 00:10:36,800
Is there anybody in this room who is not familiar with Dask at this point?

88
00:10:36,960 --> 00:10:46,400
Okay, a few people. So at this point, most of us are familiar with Dask.

89
00:10:47,600 --> 00:10:53,440
We've probably interacted with it in the cloud or even on our local machines. So I'm not really

90
00:10:53,440 --> 00:10:59,520
going to spend any more time talking about Dask. The one key point that I'm going to focus on is

91
00:10:59,520 --> 00:11:08,480
how to deploy Dask on HPC systems. And this is where Dask JobQ comes in.

92
00:11:09,600 --> 00:11:17,760
So Dask JobQ is a Python library that allows you to easily deploy Dask on JobQ systems,

93
00:11:17,760 --> 00:11:26,400
such as PBS or Slurm, and so many other. It was created as a spin of the Panjio project.

94
00:11:27,200 --> 00:11:32,960
It provides a high-level Python user interface to manage Dask clusters and Dask workers.

95
00:11:35,040 --> 00:11:40,000
So for instance, if you're like on a system that uses PBS as the queuing system,

96
00:11:40,000 --> 00:11:45,040
so this is what you have to do. So from Dask JobQ, import the PBS cluster class,

97
00:11:45,600 --> 00:11:50,080
and then instantiate that class with things like the project account, the queue,

98
00:11:50,880 --> 00:11:57,120
and all other resources that you want. And I should be clear that I'm not really defining

99
00:11:57,120 --> 00:12:03,200
everything that I want in my cluster. At this point, I'm just telling Dask a configuration of

100
00:12:03,840 --> 00:12:09,440
what to ask to the queuing system every time that I want computing resources. So in this case,

101
00:12:09,440 --> 00:12:16,960
I'm saying every time they just submit, just in this single job, submit, ask for one process

102
00:12:16,960 --> 00:12:25,680
and one thread and 100 gigabytes of memory. And once that is done, you can minority scale it by

103
00:12:25,680 --> 00:12:31,520
basically saying just scale to 10 nodes, in this case, that corresponds to 10 Dask workers.

104
00:12:32,240 --> 00:12:38,720
Or you could actually tell it to scale adaptively by saying, okay, I want you to,

105
00:12:39,840 --> 00:12:44,480
at all time, to have a minimum of one Dask worker, and you can scale between one

106
00:12:44,480 --> 00:12:51,040
and 100 Dask workers. And Dask will basically monitor your usage of your CPU usage or your

107
00:12:51,040 --> 00:12:54,960
memory usage, and then it will know that it should get more resources. And at any point,

108
00:12:54,960 --> 00:13:00,080
if you're not using those resources, it will just tell the queuing system to kill those jobs.

109
00:13:01,600 --> 00:13:08,080
So if you're on a system that uses SLRM instead of PBS, it's the exact same thing with only one

110
00:13:08,080 --> 00:13:11,840
difference of just using the SLRM cluster instead.

111
00:13:14,800 --> 00:13:22,800
Okay, so now let's go back to Tributor and actually see what Alice is actually doing in that notebook.

112
00:13:26,880 --> 00:13:32,720
Okay, so there's so much science going on in these notebooks, I won't really spend so much

113
00:13:32,720 --> 00:13:38,880
time going through the details about it. But if you're interested, there's a copy of this notebook

114
00:13:38,880 --> 00:13:46,000
that you can actually run in the cloud. It's actually exact same copy. So if you go to the

115
00:13:46,000 --> 00:13:52,320
Panjio data GitHub organization and just go to the Panjio tutorial, you should see

116
00:13:53,600 --> 00:14:00,880
one of these notebooks there. But I'm going to focus on a particular data set here,

117
00:14:00,880 --> 00:14:06,400
which is the grid ensemble precipitation and temperature estimates over the contiguous

118
00:14:06,400 --> 00:14:13,920
United States. So this consists of 100 ensemble members for precipitation and temperature data.

119
00:14:15,040 --> 00:14:19,200
So the first thing that I do, I input some packages.

120
00:14:25,120 --> 00:14:25,840
Is this better?

121
00:14:25,920 --> 00:14:33,200
Okay, good. So the next thing that I do, I now

122
00:14:34,960 --> 00:14:41,840
create my cluster object by telling that job queue that I want 109 gigabytes of memory,

123
00:14:42,800 --> 00:14:52,400
36 threads and 36 processes, specify the queue and the wall time. And I tell Dask to scale between

124
00:14:52,640 --> 00:15:02,880
360 Dask workers. And I don't know what that is, but you get just so. And when I click on this,

125
00:15:02,880 --> 00:15:10,480
as you can see, at this point, Dask is submitting a bunch of, Dask job queue, submitting a bunch

126
00:15:10,480 --> 00:15:19,040
of jobs to the queuing system. And now I'm not trying to get into workers. So as you can see,

127
00:15:19,040 --> 00:15:27,680
I have thrown a 60 workers as I specified. Let me bring up the dashboard here.

128
00:15:40,400 --> 00:15:42,720
Okay, let me one more thing.

129
00:15:43,680 --> 00:15:56,560
Okay. So now, now I can actually start doing the computation. So but the first thing I'm going to

130
00:15:56,560 --> 00:16:03,920
do is actually connect my client to the remote workers. So I have a cluster with 100 with one

131
00:16:03,920 --> 00:16:13,600
terabyte of memory and 360 workers. So the dataset is saved in stored as in ZAR format. So

132
00:16:13,600 --> 00:16:22,400
there was a talk two days ago about ZAR. If you've never had about ZAR, recommend to go and check

133
00:16:22,480 --> 00:16:38,240
it out. And I use XR8 to open this ZAR store. Again, just took 187 milliseconds. I didn't

134
00:16:38,240 --> 00:16:43,360
really do anything other than just looking at the metadata. So we can look at the size of the

135
00:16:43,360 --> 00:16:55,120
dataset. It's close to 1.7 terabyte. Can look at some metadata. So we have precipitation as one

136
00:16:55,120 --> 00:17:02,400
of our variable, the mean temperature and the temperature range. And these are all 40 variables.

137
00:17:02,400 --> 00:17:10,160
So 100 ensemble members for these many days, which corresponds to close to 40 years, I think,

138
00:17:10,240 --> 00:17:16,560
of data. And if you wanted to, you could actually look at the Dask array. If you actually wanted

139
00:17:16,560 --> 00:17:23,120
to look at the shape and things like that, if this is more useful to you. So the first thing I'm

140
00:17:23,120 --> 00:17:31,040
going to do is actually just do a quick plot for the elevation. This is basically a mask that tells

141
00:17:31,040 --> 00:17:37,520
me the elevation for each of the grid points. And as you can see, towards the west coast,

142
00:17:37,520 --> 00:17:42,400
you have points with higher elevation compared to the east coast of the country.

143
00:17:43,600 --> 00:17:49,600
So let's now try to quantify the ensemble uncertainty for a single day. So just select the

144
00:17:49,600 --> 00:18:00,080
data for one day and try to quantify the uncertainty for that one day. Again, this just goes very

145
00:18:00,080 --> 00:18:06,560
fast. Nothing really happened there other than just that Dask constructed the task graph. And when

146
00:18:06,560 --> 00:18:12,880
I do the plotting, that's when actually the computation gets triggered. Then I have my plot

147
00:18:12,880 --> 00:18:22,800
here. And again, I won't go into the details about the science. So the next task, let's try to

148
00:18:22,800 --> 00:18:33,040
compute the intra ensemble range. Again, as you can see, this just returns quickly. But this is

149
00:18:33,040 --> 00:18:40,560
actually a delayed operation or a lazy operation. And now I can actually tell Dask to do the

150
00:18:40,560 --> 00:18:51,200
computation. So now you can actually start seeing some activity here. This should be done anytime

151
00:18:52,160 --> 00:19:07,280
soon. Okay, so this has to do with Dask resiliency in that if, for instance, Dask tries to do a

152
00:19:07,280 --> 00:19:16,320
computation and it fails, it would start marking that computation. And I think by default, if it

153
00:19:16,320 --> 00:19:22,320
tries three times and hasn't actually been able to successfully complete that computation, Dask

154
00:19:22,320 --> 00:19:28,240
would just give up on that particular computation. So you could actually specify how many times you

155
00:19:28,240 --> 00:19:33,760
want Dask to try. Like, for instance, if a task was scheduled on a worker and the worker dies,

156
00:19:33,760 --> 00:19:39,120
what to do? Or if you run out of memory or things like that. So this is basically

157
00:19:39,120 --> 00:19:48,160
a cool feature of Dask, the resilience of Dask. So the computation finished. We can actually do

158
00:19:48,160 --> 00:20:01,440
some plotting here. Okay, again, not that many details about science. So the next task is let's

159
00:20:01,440 --> 00:20:13,200
try to compute the average seasonal fall, a snowfall. In this case, we're actually just computing on

160
00:20:13,200 --> 00:20:22,640
only four ensemble members. So if you go through the distributed documentation, there's a really

161
00:20:24,800 --> 00:20:30,480
cool information about how to interpret the dashboard here, the information about the dashboard.

162
00:20:31,520 --> 00:20:34,320
So this is done. Now we can do the plotting.

163
00:20:40,560 --> 00:20:48,560
It's going to take a few seconds. And as you can see, so as the year progresses,

164
00:20:48,560 --> 00:20:55,040
so the amount of snowfall decreases. And I think this is in the summer, you don't even have any

165
00:20:55,040 --> 00:21:02,240
at all in most part of the country. And this is consistent across the four ensemble members

166
00:21:02,240 --> 00:21:10,560
that we're looking at. So let's now actually do something cool. Let's actually look at like a

167
00:21:10,560 --> 00:21:20,080
specific region. In this case, let's just look at all the grid points near Austin. So XRA is really

168
00:21:20,080 --> 00:21:25,200
cool in that you can just give it the coordinates of the points you're interested in. In this case,

169
00:21:25,200 --> 00:21:31,760
we provide a buffer so that we can actually get all the grid points in that range. And we're going

170
00:21:31,760 --> 00:21:44,080
to compute the maximum precipitation near Austin for the last 40 years. So it's going to take a few

171
00:21:44,960 --> 00:22:02,800
seconds. And then we can do the plotting once this is done. So again,

172
00:22:02,800 --> 00:22:14,400
we can look at our cluster object. I still have 360 workers. I could have started with

173
00:22:14,400 --> 00:22:20,640
something really small, but it's just that I wanted this to go really fast. And you never know how

174
00:22:20,640 --> 00:22:25,440
busy the queuing system is going to be. So if you need the resources, you get them when you can.

175
00:22:26,000 --> 00:22:35,120
Okay, so this should be done. And basically, so this is the maximum precipitation near

176
00:22:35,840 --> 00:22:44,480
Austin, Texas for the 100 ensemble members for the last 40 years. So you could basically look at

177
00:22:46,800 --> 00:22:52,720
what that looked like. So yeah, so at this point, if I basically don't do anything,

178
00:22:52,720 --> 00:22:58,320
because I told us that I want a minimum of 360 workers, it would just keep those resources. But

179
00:22:59,360 --> 00:23:04,480
instead, you could actually now that I'm done with this, I can actually do

180
00:23:10,880 --> 00:23:12,880
let me just tell it to scale down to

181
00:23:13,040 --> 00:23:23,840
this. And basically, what that's going to do is going to start monitoring the workers. And if I'm

182
00:23:23,840 --> 00:23:29,040
not using them, we'll just take them away. So I will come back to this later to show you what I mean

183
00:23:29,040 --> 00:23:39,200
by that. Okay, so, so we've seen, or at least I try to demonstrate the adaptive or the elastic

184
00:23:39,200 --> 00:23:45,760
scaling and the resiliency of dusk. And let's now talk about some of the challenges. So being

185
00:23:45,760 --> 00:23:50,400
able to know how much resources you need before actually doing the computation is really hard.

186
00:23:50,400 --> 00:23:56,720
And they actually requires quite a lot of experimentation. And if you actually get to

187
00:23:56,720 --> 00:24:02,000
know how to do this for one particular workflow, it probably changes once you move to a different

188
00:24:02,000 --> 00:24:08,240
dataset. So, and another thing is that the computation, the computational workloads, they

189
00:24:08,240 --> 00:24:12,720
don't really are not really constant. So you probably started with one terabyte of data.

190
00:24:13,360 --> 00:24:18,800
Five minutes later, you probably only have one gig of data left. At that point, you don't really

191
00:24:18,800 --> 00:24:22,640
need all the resources that you started with when you when you're dealing with one terabyte of data.

192
00:24:23,200 --> 00:24:30,080
So the good thing is that dusk thinks about these things. So how to scale up and down,

193
00:24:30,080 --> 00:24:35,520
how to be resilient in case like a worker dies, and what if when you get new workers,

194
00:24:35,520 --> 00:24:38,560
what to do in terms of load balancing and things like that.

195
00:24:40,320 --> 00:24:44,000
So what is the solution? Basically just start a Jupyter notebook,

196
00:24:44,800 --> 00:24:50,480
instantiate your dusk cluster, and then just let dusk do the scaling up and down for you.

197
00:24:50,480 --> 00:25:01,920
And you just focus on the science. And what are the benefits to HPC systems for elastic scaling?

198
00:25:02,800 --> 00:25:08,240
One of them is that it actually improves the occupancy of the machine in that if the resources

199
00:25:08,240 --> 00:25:13,520
are idle, then dusk knows how to release them so that other users can actually use them.

200
00:25:13,840 --> 00:25:21,120
And another thing is that with the resiliency, you can easily, for instance, if you started

201
00:25:21,120 --> 00:25:28,080
with 120 workers, for some reason, your worker dies, dusk will know how to get a new worker.

202
00:25:28,080 --> 00:25:33,280
And in a way, if you think about something like MPI where you start, let's say you have like 120

203
00:25:36,000 --> 00:25:42,080
workers, if one thing goes wrong in an MPI environment, the whole thing dies. So in a way,

204
00:25:42,080 --> 00:25:46,480
you kind of lose all the work that you had done, which is probably not that nice.

205
00:25:47,520 --> 00:25:55,200
So basically, another thing, to the same point, dusk thinks about these things.

206
00:25:55,200 --> 00:26:00,320
And I think you should also think about what you get from this as well.

207
00:26:01,680 --> 00:26:06,480
Again, so not all jobs are interactive. So once you're done with your interactive

208
00:26:06,480 --> 00:26:12,640
workflows, there's this other package called dusk MPI, which actually now allows you to go on and

209
00:26:12,640 --> 00:26:19,360
actually launch but jobs in case you don't need to do interactive exploration.

210
00:26:20,640 --> 00:26:36,320
And looks like I'm running out of time. So that is pretty much it for today. Thank you.

211
00:26:37,200 --> 00:26:41,840
And like I said, now you can see that it took away those resources.

212
00:26:44,720 --> 00:26:45,520
Okay, so here we go.

213
00:26:53,120 --> 00:26:56,480
Let's try here. So do we have any more questions?

214
00:26:56,480 --> 00:27:10,640
Hi, great talk. How do you deal with large datasets? And if you need to import like large

215
00:27:10,640 --> 00:27:14,080
datasets, can you do it? Can you put them on the cluster?

216
00:27:16,320 --> 00:27:23,840
So in this case, the Trubeta Hub is actually running on the same file system.

217
00:27:24,080 --> 00:27:30,240
So which basically means that we don't need to move the data around. So in our case,

218
00:27:30,240 --> 00:27:35,280
it kind of works in that basically we're moving the computation where the data is.

219
00:27:35,280 --> 00:27:41,680
So we don't need to move the data around. So I haven't run into cases where we need

220
00:27:41,680 --> 00:27:49,040
to move the data. So if anybody has run into that, then they could probably provide a better answer.

221
00:27:49,520 --> 00:27:52,720
Other questions?

222
00:28:00,720 --> 00:28:08,960
Yeah, it's the Trubeta Hub instance taking a pre-allocated resources like 20 loads,

223
00:28:08,960 --> 00:28:11,920
and then you scale up and down inside the Trubeta Hub?

224
00:28:12,880 --> 00:28:20,800
No. So if you remember what I did when I logged in to the Trubeta Hub, I asked for I think just

225
00:28:20,800 --> 00:28:26,160
one process. So Trubeta Hub is actually running, the lab itself is running in its own job.

226
00:28:27,200 --> 00:28:33,760
So when I do the scaling up and down, I'm actually doing that as independent jobs.

227
00:28:34,560 --> 00:28:36,080
So that's why I'm able to do that.

228
00:28:36,800 --> 00:28:42,400
But I mean scaling down is easy, but when you're trying to scale up, in our class, you need to

229
00:28:42,400 --> 00:28:49,360
wait 30 minutes to have a new job created for you. So yeah, that's what I say that when it comes to

230
00:28:50,240 --> 00:28:57,360
queuing systems, I think that lack of elastic scaling kind of makes it hard to actually do

231
00:28:57,360 --> 00:29:01,840
interactive computing in that you can't wait for two hours to get into the queue.

232
00:29:02,800 --> 00:29:05,680
In my case, I was able to get into the queue very easily because

233
00:29:06,960 --> 00:29:15,200
I had to ask a special reservation. So yeah.

234
00:29:15,920 --> 00:29:18,880
All right, we're going to have a lot of people filing in and out here because we're changing

235
00:29:18,880 --> 00:29:25,200
topics, so can we please thank Anderson again and all the speakers for the Geophysics Imposer.

