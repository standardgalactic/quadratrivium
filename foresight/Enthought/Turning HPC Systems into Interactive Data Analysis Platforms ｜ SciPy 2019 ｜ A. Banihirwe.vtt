WEBVTT

00:00.000 --> 00:07.680
Good afternoon everyone. Welcome back to room 305. I see how we are full as normal. If you guys can

00:08.320 --> 00:13.280
try to get into the middle to find a seat, you know, fire code and stuff like that,

00:14.000 --> 00:20.720
that would be very, very much appreciated. This is the last talk in our Geophysics,

00:21.600 --> 00:27.680
Geology and Atmospheric Sciences session, and it goes to Addison. And how do you pronounce

00:27.760 --> 00:33.280
your last name, Addison? I'm going to leave that one with him and not even attempt it,

00:34.080 --> 00:39.360
who's going to talk about interactive super computing with Jupiter and Dask. Thank you very

00:39.360 --> 00:49.280
much, Addison. Thank you, Scott. Great. Thank you for having me here. This is my first sci-pi,

00:49.280 --> 00:57.600
so I'm very excited to be here. So as Scott said, so my name is Anderson, I work as a software

00:57.680 --> 01:03.760
engineer at the National Center for Atmospheric Research. So for those who are interested,

01:03.760 --> 01:10.800
the slides available address. So this is a talk about Python tools that enable

01:12.640 --> 01:17.120
interactive super computing, and we're going to see what we mean by super computing in a minute.

01:17.200 --> 01:28.800
Okay, so this is Alice, and Alice is a project scientist at NCAR. She studies the transfer of

01:28.800 --> 01:36.240
water and energy between the land surface and the lower atmosphere. And on the other side,

01:36.240 --> 01:42.400
we have a cool notebook of how work. There are three things that are really interesting about

01:42.400 --> 01:49.760
this notebook. So the first thing is that this notebook is not running on her computer. So this

01:49.760 --> 01:56.080
is running on a supercomputer. In this case, this is Cheyenne. And as you can see, there's an address

01:56.080 --> 02:03.120
to a Jupiter hub running on the Cheyenne supercomputer. The second thing is that she's using

02:04.480 --> 02:11.440
hundreds of processes and terabytes of memory. All this is distributed computing resources.

02:12.560 --> 02:18.800
Thanks to desktop queue. We're going to talk about that as well. And the third thing is she's

02:18.800 --> 02:25.040
actually doing science. So this is not a toy example, so which is what she's good at. So

02:26.000 --> 02:32.720
now we're going to talk about how we at NCAR enable Alice and folks like her to do interactive

02:32.720 --> 02:43.920
supercomputing. So what do we mean by supercomputing? First, MPI, batch processing, lots of heavy

02:43.920 --> 02:49.760
machines that most of people don't have access to as admins. So you have to talk to people to be able

02:49.760 --> 02:56.480
to install some of the software. In the picture, so this is Cheyenne, which is a supercomputer

02:56.480 --> 03:04.080
operated by NCAR. If you're ever in Cheyenne, Wyoming, feel free to stop by. They give free

03:04.080 --> 03:10.880
tours of the facility if you want to. And what do we mean by interactive supercomputing?

03:12.480 --> 03:20.240
So with the current growth in data creation, both through new simulations and new observations,

03:20.240 --> 03:26.960
there's a growing need to have a more human in the loop workflow where people can rapidly prototype

03:26.960 --> 03:35.120
and iterate through the data with tools like Jupyter notebooks. There's also a need to do

03:35.120 --> 03:40.800
things like in situ data analytics. So instead of having to run the simulations and output the data,

03:40.800 --> 03:47.120
you could actually run the simulation and do the analysis at the same time. And the other thing

03:47.120 --> 03:53.840
is actually adaptive scaling of computing resources. So this combination would really be powerful,

03:53.840 --> 04:02.880
but it's really hard for different reasons. So some of these reasons are cultural and others

04:02.880 --> 04:11.200
are technical. So the first one is that HP systems tends to be unique. So every HPC center has its

04:11.200 --> 04:16.960
own policies when it comes to things like security, what can you run on the supercomputer.

04:18.880 --> 04:25.280
And the second point is the tension between interactivity and machine utilization. So when it

04:25.280 --> 04:34.320
comes to HPC systems, HP center stands to be measured according to how often the machine is

04:34.320 --> 04:40.400
actually utilized. And when you're talking about things like interactivity, users want to have

04:41.440 --> 04:48.240
resources on demand whenever they want them. But because HP system operate on batch Q systems,

04:48.240 --> 04:53.280
it's not that easy to actually get resources whenever you want to. So no user is going to

04:53.280 --> 05:00.320
wait for five hours to get into the queue to do interactive data analysis. And the third point

05:00.320 --> 05:05.840
is the lack of elastic scaling. And this is a huge, at least in my opinion, I think this is a huge

05:05.840 --> 05:11.760
disadvantage in that if someone, let's say, want to do some analysis, they have to take time and

05:11.760 --> 05:17.040
actually think about how much resources they want before they even do the analysis. And what ends

05:17.040 --> 05:24.560
up happening is that you get into your work, maybe five minutes later, you probably take like 20

05:24.560 --> 05:29.200
minutes thinking, what do I do next? And during that time, the resources are just idle. So it would

05:29.200 --> 05:35.520
be nice if you could actually scale down later that people use those resources and get them back

05:35.520 --> 05:43.280
whenever you actually need to. The good news is that, despite all those challenges, there's a

05:43.280 --> 05:50.640
growing list of technologies or tools that try to help with interactive supercomputing. And I'm

05:50.640 --> 05:58.000
going to talk about some of these tools. The first one is Trubiter. So we all love and use

05:58.000 --> 06:03.840
Trubiter. And so I'm not really going to spend much time on it because now everybody is familiar

06:03.840 --> 06:12.640
with it. And some of you may be wondering, but isn't Trubiter already usable on HP systems? And

06:12.640 --> 06:19.200
the answer is actually, yes, but you have to go through all these steps. You have to essentially

06:19.200 --> 06:26.240
tune the machine. You have to set up SSH tunnels. And you have to do this every single time that

06:26.240 --> 06:32.960
you want to actually use the Trubiter notebook on the supercomputer. And this can actually be tedious,

06:33.040 --> 06:41.040
especially for new users. So what is missing? So one of the things that is actually missing is

06:41.040 --> 06:45.920
the multi-user support. So all those steps, everybody has to go through them individually.

06:45.920 --> 06:50.240
And it would be nice if there was one single place that everybody goes to and everything is

06:50.240 --> 06:56.720
done for them. Another thing is actually the lack of pure web access to HPC resources. Because as

06:56.720 --> 07:03.760
you've seen, we're setting up SSH tunnel, which if you ever need to use another process that probably

07:03.760 --> 07:07.840
runs on a different port, so you also have to SSH to create a tunnel for that port as well.

07:09.760 --> 07:17.680
Well, then Trubiter Hub comes to the rescue. So one good thing about Trubiter Hub is that,

07:17.680 --> 07:24.800
in a way, it provides a standard way of managing authentication. So one of the issues that is not

07:24.800 --> 07:32.480
really hard to convince sysadmins about is the security of web applications. But Trubiter Hub

07:32.480 --> 07:37.600
makes it easy in that it doesn't really force you to use any type of authentication. It's up to you

07:37.600 --> 07:42.960
to choose what you want to use for authentication. In this case, on the supercomputer, you have

07:42.960 --> 07:48.640
different types of authentications. And another thing is that Trubiter Hub will just take care of

07:48.720 --> 07:56.080
spawning the Trubiter notebook single server and giving access to the user on demand.

07:58.720 --> 08:05.920
And so let's now switch gears and actually see how this works in practice. So I will skip these

08:05.920 --> 08:14.080
few slides and actually go to the live demo. So the first thing that you do as a user, so you just

08:14.080 --> 08:27.680
go to Trubiter Hub.ucr.edu. And so this is what you presented. And if you've used the

08:27.680 --> 08:32.400
Trubiter Hub before, this looks familiar. The only difference is that in this case,

08:33.280 --> 08:39.600
I have to use the same authentication that I used to SSH into the machine. So in my case,

08:39.600 --> 08:46.160
I just provide my username. And then for the password, it's a combination of a PIN number

08:47.040 --> 08:57.040
and a UB key token. So now, so now I'm authenticated. So I then get this page,

08:57.040 --> 09:03.040
which basically asked me for the project account. So this is the allocation on the

09:03.040 --> 09:09.600
supercomputer so that they know who to charge for this usage. So in this case, I'm going to

09:10.800 --> 09:18.960
provide when to change the queue here and specify the project

09:22.080 --> 09:30.480
and maybe just reduce the wall time. And then once this is done, I just click on spawn.

09:31.440 --> 09:36.480
And what this is doing in the background, it's basically submitting a job to the queuing system.

09:37.120 --> 09:46.480
And once the Trubiter Notebook server is ready, I will get redirected to this new page. And at this

09:46.480 --> 09:54.480
point, thank you. So at this point, you have the same interface as what you would have on your

09:54.480 --> 10:02.720
laptop. So I can run notebooks, in this case, to show you that I'm not really lying. So

10:06.560 --> 10:12.480
you could just run this notebook, which is backed by Bashkarno. And as you can see,

10:12.480 --> 10:16.400
I don't really have 200 terabytes of storage on this computer. So

10:17.360 --> 10:30.800
that's it about Trubiter Hubs. I'll come back to it later. So the next tool is Dask.

10:33.680 --> 10:36.800
Is there anybody in this room who is not familiar with Dask at this point?

10:36.960 --> 10:46.400
Okay, a few people. So at this point, most of us are familiar with Dask.

10:47.600 --> 10:53.440
We've probably interacted with it in the cloud or even on our local machines. So I'm not really

10:53.440 --> 10:59.520
going to spend any more time talking about Dask. The one key point that I'm going to focus on is

10:59.520 --> 11:08.480
how to deploy Dask on HPC systems. And this is where Dask JobQ comes in.

11:09.600 --> 11:17.760
So Dask JobQ is a Python library that allows you to easily deploy Dask on JobQ systems,

11:17.760 --> 11:26.400
such as PBS or Slurm, and so many other. It was created as a spin of the Panjio project.

11:27.200 --> 11:32.960
It provides a high-level Python user interface to manage Dask clusters and Dask workers.

11:35.040 --> 11:40.000
So for instance, if you're like on a system that uses PBS as the queuing system,

11:40.000 --> 11:45.040
so this is what you have to do. So from Dask JobQ, import the PBS cluster class,

11:45.600 --> 11:50.080
and then instantiate that class with things like the project account, the queue,

11:50.880 --> 11:57.120
and all other resources that you want. And I should be clear that I'm not really defining

11:57.120 --> 12:03.200
everything that I want in my cluster. At this point, I'm just telling Dask a configuration of

12:03.840 --> 12:09.440
what to ask to the queuing system every time that I want computing resources. So in this case,

12:09.440 --> 12:16.960
I'm saying every time they just submit, just in this single job, submit, ask for one process

12:16.960 --> 12:25.680
and one thread and 100 gigabytes of memory. And once that is done, you can minority scale it by

12:25.680 --> 12:31.520
basically saying just scale to 10 nodes, in this case, that corresponds to 10 Dask workers.

12:32.240 --> 12:38.720
Or you could actually tell it to scale adaptively by saying, okay, I want you to,

12:39.840 --> 12:44.480
at all time, to have a minimum of one Dask worker, and you can scale between one

12:44.480 --> 12:51.040
and 100 Dask workers. And Dask will basically monitor your usage of your CPU usage or your

12:51.040 --> 12:54.960
memory usage, and then it will know that it should get more resources. And at any point,

12:54.960 --> 13:00.080
if you're not using those resources, it will just tell the queuing system to kill those jobs.

13:01.600 --> 13:08.080
So if you're on a system that uses SLRM instead of PBS, it's the exact same thing with only one

13:08.080 --> 13:11.840
difference of just using the SLRM cluster instead.

13:14.800 --> 13:22.800
Okay, so now let's go back to Tributor and actually see what Alice is actually doing in that notebook.

13:26.880 --> 13:32.720
Okay, so there's so much science going on in these notebooks, I won't really spend so much

13:32.720 --> 13:38.880
time going through the details about it. But if you're interested, there's a copy of this notebook

13:38.880 --> 13:46.000
that you can actually run in the cloud. It's actually exact same copy. So if you go to the

13:46.000 --> 13:52.320
Panjio data GitHub organization and just go to the Panjio tutorial, you should see

13:53.600 --> 14:00.880
one of these notebooks there. But I'm going to focus on a particular data set here,

14:00.880 --> 14:06.400
which is the grid ensemble precipitation and temperature estimates over the contiguous

14:06.400 --> 14:13.920
United States. So this consists of 100 ensemble members for precipitation and temperature data.

14:15.040 --> 14:19.200
So the first thing that I do, I input some packages.

14:25.120 --> 14:25.840
Is this better?

14:25.920 --> 14:33.200
Okay, good. So the next thing that I do, I now

14:34.960 --> 14:41.840
create my cluster object by telling that job queue that I want 109 gigabytes of memory,

14:42.800 --> 14:52.400
36 threads and 36 processes, specify the queue and the wall time. And I tell Dask to scale between

14:52.640 --> 15:02.880
360 Dask workers. And I don't know what that is, but you get just so. And when I click on this,

15:02.880 --> 15:10.480
as you can see, at this point, Dask is submitting a bunch of, Dask job queue, submitting a bunch

15:10.480 --> 15:19.040
of jobs to the queuing system. And now I'm not trying to get into workers. So as you can see,

15:19.040 --> 15:27.680
I have thrown a 60 workers as I specified. Let me bring up the dashboard here.

15:40.400 --> 15:42.720
Okay, let me one more thing.

15:43.680 --> 15:56.560
Okay. So now, now I can actually start doing the computation. So but the first thing I'm going to

15:56.560 --> 16:03.920
do is actually connect my client to the remote workers. So I have a cluster with 100 with one

16:03.920 --> 16:13.600
terabyte of memory and 360 workers. So the dataset is saved in stored as in ZAR format. So

16:13.600 --> 16:22.400
there was a talk two days ago about ZAR. If you've never had about ZAR, recommend to go and check

16:22.480 --> 16:38.240
it out. And I use XR8 to open this ZAR store. Again, just took 187 milliseconds. I didn't

16:38.240 --> 16:43.360
really do anything other than just looking at the metadata. So we can look at the size of the

16:43.360 --> 16:55.120
dataset. It's close to 1.7 terabyte. Can look at some metadata. So we have precipitation as one

16:55.120 --> 17:02.400
of our variable, the mean temperature and the temperature range. And these are all 40 variables.

17:02.400 --> 17:10.160
So 100 ensemble members for these many days, which corresponds to close to 40 years, I think,

17:10.240 --> 17:16.560
of data. And if you wanted to, you could actually look at the Dask array. If you actually wanted

17:16.560 --> 17:23.120
to look at the shape and things like that, if this is more useful to you. So the first thing I'm

17:23.120 --> 17:31.040
going to do is actually just do a quick plot for the elevation. This is basically a mask that tells

17:31.040 --> 17:37.520
me the elevation for each of the grid points. And as you can see, towards the west coast,

17:37.520 --> 17:42.400
you have points with higher elevation compared to the east coast of the country.

17:43.600 --> 17:49.600
So let's now try to quantify the ensemble uncertainty for a single day. So just select the

17:49.600 --> 18:00.080
data for one day and try to quantify the uncertainty for that one day. Again, this just goes very

18:00.080 --> 18:06.560
fast. Nothing really happened there other than just that Dask constructed the task graph. And when

18:06.560 --> 18:12.880
I do the plotting, that's when actually the computation gets triggered. Then I have my plot

18:12.880 --> 18:22.800
here. And again, I won't go into the details about the science. So the next task, let's try to

18:22.800 --> 18:33.040
compute the intra ensemble range. Again, as you can see, this just returns quickly. But this is

18:33.040 --> 18:40.560
actually a delayed operation or a lazy operation. And now I can actually tell Dask to do the

18:40.560 --> 18:51.200
computation. So now you can actually start seeing some activity here. This should be done anytime

18:52.160 --> 19:07.280
soon. Okay, so this has to do with Dask resiliency in that if, for instance, Dask tries to do a

19:07.280 --> 19:16.320
computation and it fails, it would start marking that computation. And I think by default, if it

19:16.320 --> 19:22.320
tries three times and hasn't actually been able to successfully complete that computation, Dask

19:22.320 --> 19:28.240
would just give up on that particular computation. So you could actually specify how many times you

19:28.240 --> 19:33.760
want Dask to try. Like, for instance, if a task was scheduled on a worker and the worker dies,

19:33.760 --> 19:39.120
what to do? Or if you run out of memory or things like that. So this is basically

19:39.120 --> 19:48.160
a cool feature of Dask, the resilience of Dask. So the computation finished. We can actually do

19:48.160 --> 20:01.440
some plotting here. Okay, again, not that many details about science. So the next task is let's

20:01.440 --> 20:13.200
try to compute the average seasonal fall, a snowfall. In this case, we're actually just computing on

20:13.200 --> 20:22.640
only four ensemble members. So if you go through the distributed documentation, there's a really

20:24.800 --> 20:30.480
cool information about how to interpret the dashboard here, the information about the dashboard.

20:31.520 --> 20:34.320
So this is done. Now we can do the plotting.

20:40.560 --> 20:48.560
It's going to take a few seconds. And as you can see, so as the year progresses,

20:48.560 --> 20:55.040
so the amount of snowfall decreases. And I think this is in the summer, you don't even have any

20:55.040 --> 21:02.240
at all in most part of the country. And this is consistent across the four ensemble members

21:02.240 --> 21:10.560
that we're looking at. So let's now actually do something cool. Let's actually look at like a

21:10.560 --> 21:20.080
specific region. In this case, let's just look at all the grid points near Austin. So XRA is really

21:20.080 --> 21:25.200
cool in that you can just give it the coordinates of the points you're interested in. In this case,

21:25.200 --> 21:31.760
we provide a buffer so that we can actually get all the grid points in that range. And we're going

21:31.760 --> 21:44.080
to compute the maximum precipitation near Austin for the last 40 years. So it's going to take a few

21:44.960 --> 22:02.800
seconds. And then we can do the plotting once this is done. So again,

22:02.800 --> 22:14.400
we can look at our cluster object. I still have 360 workers. I could have started with

22:14.400 --> 22:20.640
something really small, but it's just that I wanted this to go really fast. And you never know how

22:20.640 --> 22:25.440
busy the queuing system is going to be. So if you need the resources, you get them when you can.

22:26.000 --> 22:35.120
Okay, so this should be done. And basically, so this is the maximum precipitation near

22:35.840 --> 22:44.480
Austin, Texas for the 100 ensemble members for the last 40 years. So you could basically look at

22:46.800 --> 22:52.720
what that looked like. So yeah, so at this point, if I basically don't do anything,

22:52.720 --> 22:58.320
because I told us that I want a minimum of 360 workers, it would just keep those resources. But

22:59.360 --> 23:04.480
instead, you could actually now that I'm done with this, I can actually do

23:10.880 --> 23:12.880
let me just tell it to scale down to

23:13.040 --> 23:23.840
this. And basically, what that's going to do is going to start monitoring the workers. And if I'm

23:23.840 --> 23:29.040
not using them, we'll just take them away. So I will come back to this later to show you what I mean

23:29.040 --> 23:39.200
by that. Okay, so, so we've seen, or at least I try to demonstrate the adaptive or the elastic

23:39.200 --> 23:45.760
scaling and the resiliency of dusk. And let's now talk about some of the challenges. So being

23:45.760 --> 23:50.400
able to know how much resources you need before actually doing the computation is really hard.

23:50.400 --> 23:56.720
And they actually requires quite a lot of experimentation. And if you actually get to

23:56.720 --> 24:02.000
know how to do this for one particular workflow, it probably changes once you move to a different

24:02.000 --> 24:08.240
dataset. So, and another thing is that the computation, the computational workloads, they

24:08.240 --> 24:12.720
don't really are not really constant. So you probably started with one terabyte of data.

24:13.360 --> 24:18.800
Five minutes later, you probably only have one gig of data left. At that point, you don't really

24:18.800 --> 24:22.640
need all the resources that you started with when you when you're dealing with one terabyte of data.

24:23.200 --> 24:30.080
So the good thing is that dusk thinks about these things. So how to scale up and down,

24:30.080 --> 24:35.520
how to be resilient in case like a worker dies, and what if when you get new workers,

24:35.520 --> 24:38.560
what to do in terms of load balancing and things like that.

24:40.320 --> 24:44.000
So what is the solution? Basically just start a Jupyter notebook,

24:44.800 --> 24:50.480
instantiate your dusk cluster, and then just let dusk do the scaling up and down for you.

24:50.480 --> 25:01.920
And you just focus on the science. And what are the benefits to HPC systems for elastic scaling?

25:02.800 --> 25:08.240
One of them is that it actually improves the occupancy of the machine in that if the resources

25:08.240 --> 25:13.520
are idle, then dusk knows how to release them so that other users can actually use them.

25:13.840 --> 25:21.120
And another thing is that with the resiliency, you can easily, for instance, if you started

25:21.120 --> 25:28.080
with 120 workers, for some reason, your worker dies, dusk will know how to get a new worker.

25:28.080 --> 25:33.280
And in a way, if you think about something like MPI where you start, let's say you have like 120

25:36.000 --> 25:42.080
workers, if one thing goes wrong in an MPI environment, the whole thing dies. So in a way,

25:42.080 --> 25:46.480
you kind of lose all the work that you had done, which is probably not that nice.

25:47.520 --> 25:55.200
So basically, another thing, to the same point, dusk thinks about these things.

25:55.200 --> 26:00.320
And I think you should also think about what you get from this as well.

26:01.680 --> 26:06.480
Again, so not all jobs are interactive. So once you're done with your interactive

26:06.480 --> 26:12.640
workflows, there's this other package called dusk MPI, which actually now allows you to go on and

26:12.640 --> 26:19.360
actually launch but jobs in case you don't need to do interactive exploration.

26:20.640 --> 26:36.320
And looks like I'm running out of time. So that is pretty much it for today. Thank you.

26:37.200 --> 26:41.840
And like I said, now you can see that it took away those resources.

26:44.720 --> 26:45.520
Okay, so here we go.

26:53.120 --> 26:56.480
Let's try here. So do we have any more questions?

26:56.480 --> 27:10.640
Hi, great talk. How do you deal with large datasets? And if you need to import like large

27:10.640 --> 27:14.080
datasets, can you do it? Can you put them on the cluster?

27:16.320 --> 27:23.840
So in this case, the Trubeta Hub is actually running on the same file system.

27:24.080 --> 27:30.240
So which basically means that we don't need to move the data around. So in our case,

27:30.240 --> 27:35.280
it kind of works in that basically we're moving the computation where the data is.

27:35.280 --> 27:41.680
So we don't need to move the data around. So I haven't run into cases where we need

27:41.680 --> 27:49.040
to move the data. So if anybody has run into that, then they could probably provide a better answer.

27:49.520 --> 27:52.720
Other questions?

28:00.720 --> 28:08.960
Yeah, it's the Trubeta Hub instance taking a pre-allocated resources like 20 loads,

28:08.960 --> 28:11.920
and then you scale up and down inside the Trubeta Hub?

28:12.880 --> 28:20.800
No. So if you remember what I did when I logged in to the Trubeta Hub, I asked for I think just

28:20.800 --> 28:26.160
one process. So Trubeta Hub is actually running, the lab itself is running in its own job.

28:27.200 --> 28:33.760
So when I do the scaling up and down, I'm actually doing that as independent jobs.

28:34.560 --> 28:36.080
So that's why I'm able to do that.

28:36.800 --> 28:42.400
But I mean scaling down is easy, but when you're trying to scale up, in our class, you need to

28:42.400 --> 28:49.360
wait 30 minutes to have a new job created for you. So yeah, that's what I say that when it comes to

28:50.240 --> 28:57.360
queuing systems, I think that lack of elastic scaling kind of makes it hard to actually do

28:57.360 --> 29:01.840
interactive computing in that you can't wait for two hours to get into the queue.

29:02.800 --> 29:05.680
In my case, I was able to get into the queue very easily because

29:06.960 --> 29:15.200
I had to ask a special reservation. So yeah.

29:15.920 --> 29:18.880
All right, we're going to have a lot of people filing in and out here because we're changing

29:18.880 --> 29:25.200
topics, so can we please thank Anderson again and all the speakers for the Geophysics Imposer.

