Processing Overview for Enthought
============================
Checking Enthought/Bayesian Data Science： Probabilistic Programming ｜ SciPy 2019 Tutorial ｜ Eric Ma.txt
1. **Probability as Credibility Points**: We discussed how probability assigns credibility points to different values on the number line, with higher points indicating greater belief in those values.

2. **Bayesian Inference**: The course highlighted the importance of Bayes' rule for updating our beliefs based on new evidence and emphasized understanding joint and conditional probability, marginal probability, and how they all relate to each other.

3. **Probability Distribution Stories**: Knowing the "stories" behind different probability distributions is crucial for selecting the right distribution for a given modeling situation. This knowledge helps in understanding their shapes, supports, and how they can be applied to real-world data.

4. **Example: Binomial Distribution**: We worked through a detailed example using the binomial distribution, showing how to extend it to vectorized data, handle different amounts of information across groups, and build hierarchical models for more complex scenarios.

5. **Further Learning and Support**: The instructor offered office hours for further discussion and support, which would be announced on Slack in the Tejas room during afternoons.

6. **Improper Priors**: While improper priors (like flat distributions) can be used, they are generally discouraged as they might not always be the best approach and can lead to issues like non-identifiability. Instead, it's recommended to use informative priors that reflect what is known about a parameter with some uncertainty.

7. **Practical Application**: The practical aspect of applying these concepts in a work environment was also discussed, including the importance of knowing when to stop modeling for the day and allowing time for peer review and rest to gain fresh perspective.

The session aimed to equip participants with a deep understanding of the foundational concepts of probability and Bayesian inference, along with practical skills for applying these concepts in their work.

Checking Enthought/Turning HPC Systems into Interactive Data Analysis Platforms ｜ SciPy 2019 ｜ A. Banihirwe.txt
1. **Dusk Framework**: It's an open-source framework that enables Jupyter notebook users to perform interactive computing in a HPC environment with elastic scaling capabilities. Dusk handles the allocation and deallocation of resources based on the workload, ensuring efficient use of computational resources.

2. **Elastic Scaling Benefits**:
   - Improves machine occupancy by releasing idle resources for other users.
   - Enhances resilience by automatically replacing dead workers and balancing loads.
   - Avoids the all-or-nothing approach of MPI, where a failure in one worker can cause the entire job to fail.

3. **Dusk and Large Datasets**:
   - Dusk allows users to perform computations close to where the data resides (e.g., Trubeta Hub on the same file system).
   - There's no need to move large datasets around, which can be cumbersome and time-consuming.

4. **Resource Allocation in Trubeta Hub**:
   - The initial Trubeta Hub instance takes a predefined amount of resources (e.g., 20 cores).
   - Dusk allows users to manage their own jobs for scaling up/down independently within the Trubeta Hub environment.

5. **Challenges with Queuing Systems**:
   - Traditional queuing systems may lack elastic scaling, making interactive computing less efficient due to long wait times.
   - Special reservations can be requested to overcome these wait times in certain cases.

6. **Dusk MPI**:
   - For non-interactive workflows, dusk offers a package called dusk MPI, which allows launching batch jobs after the interactive exploration is complete.

7. **Questions and Clarifications**:
   - Large datasets are managed by ensuring computation moves to where the data is (e.g., Trubeta Hub's file system).
   - Dusk handles scaling within its own framework, separate from the initial resource allocation of the Trubeta Hub instance.
   - The process for scaling up and down involves creating new jobs as needed, with a slight delay in scaling up due to queuing systems.

8. **Thank You**: A round of thanks to Anderson for the presentation on Geophysics Imposer and all the speakers involved.

Checking Enthought/Xonsh： Bringing Python Data Science to your Shell ｜ Scipy 2019 Tutorial ｜ G. Forsyth, A. Scopatz.txt
1. **Problem Scenario**: You have a large set of file names generated from a webpage, and you need to extract the correct files from this list. The file names follow a specific pattern (e.g., `sub-187785.nii`).

2. **Solution Approach**:
   - Use `curl` or a similar tool to download the raw page containing the file names if you don't already have it.
   - Extract the list of files that match the desired pattern using regular expressions.
   - Download one of the extracted file names to get a sample file.
   - Use NiBabel (a Python library for handling neuroimaging data) to load the sample file and inspect its dimensions or headers to verify if it's the correct type of image.

3. **Path Handling**:
   - When dealing with URLs, you don't need to convert them into path objects because they are already strings that can be passed directly to commands like `curl`.

4. **Next Steps**:
   - Continue working on the example and any other issues or questions that arise during the workshop.
   - Conch developers are available to help with any specific tasks or improvements you want to implement.

5. **Engagement**:
   - Attendees are encouraged to engage with the Conch community, ask questions, report bugs, contribute improvements, and take stickers as a memento of the workshop.

6. **Overall Message**: The workshop is an opportunity for attendees to learn about Conch, try it out, and get help from the developers. Conch aims to improve users' workflows, and the community is friendly and open to contributions.

