WEBVTT

00:00.000 --> 00:02.960
Thank you all for coming to the tutorial.

00:02.960 --> 00:05.920
If you noticed in the SciPy program,

00:05.920 --> 00:08.320
there are three tutorials doing Bayes stuff.

00:08.320 --> 00:12.120
This is, I think, unprecedented in the SciPy tutorial

00:12.120 --> 00:14.160
program list.

00:14.160 --> 00:18.400
What happened was the three lead instructors, that

00:18.400 --> 00:22.200
is Alan Downey for his tutorial, myself and Hugo

00:22.200 --> 00:24.360
for this tutorial, and Ravine and Colin

00:24.360 --> 00:27.240
for the next tutorial on Bayesian modeling,

00:27.240 --> 00:29.840
we all decided to informally coordinate together

00:29.840 --> 00:32.160
to put this informal Bayes track thing.

00:32.160 --> 00:36.840
And I know that there are a few people who have actually

00:36.840 --> 00:38.600
registered for all three of them.

00:38.600 --> 00:42.880
So props to you guys, that handful of you.

00:42.880 --> 00:48.800
Anyone want to take an estimate on what fraction of this crowd?

00:48.800 --> 00:51.240
Did that?

00:51.240 --> 00:55.480
20%, which would mean about, let's see, 16 people-ish?

00:55.480 --> 00:56.360
15 people-ish?

00:56.360 --> 00:59.480
I think they're about 60 to 80 people registered in this room.

00:59.480 --> 01:00.440
Yeah, so OK.

01:00.440 --> 01:02.880
Yeah, I actually got the actual number from Jill.

01:02.880 --> 01:06.720
It's 15 of you, so it's a good, close estimate there.

01:06.720 --> 01:14.840
All right, so for the 15 of you who were in Alan's tutorial,

01:14.840 --> 01:18.320
some of this material will be familiar to you.

01:18.320 --> 01:21.800
And for the rest of you, we'll go through enough background

01:21.800 --> 01:24.080
to accomplish today's goals, which

01:24.080 --> 01:26.920
is to show you how you can write arbitrary statistical

01:26.920 --> 01:30.400
models and use them to perform inference.

01:30.400 --> 01:33.560
That's the end goal of today's tutorial.

01:33.560 --> 01:36.320
And there'll be a tool of choice, which is Pi MC3.

01:36.320 --> 01:38.560
We'll also show some things with NumPy.

01:38.560 --> 01:41.520
Hopefully that will give you enough grounding

01:41.520 --> 01:45.960
in probability, in Bayes rule, to then be

01:45.960 --> 01:48.520
able to talk about how our data were generated

01:48.520 --> 01:51.440
from a statistical standpoint and build those models

01:51.440 --> 01:54.840
and use them, use them productively.

01:54.840 --> 01:59.240
OK, some administrative matters before we move on.

01:59.240 --> 02:02.320
For those of you who are still not set up,

02:02.320 --> 02:04.480
I know that there's a high probability

02:04.480 --> 02:06.440
that you are set up, set up being defined

02:06.440 --> 02:08.960
as you either have Binder working

02:08.960 --> 02:10.760
or you have everything cloned locally

02:10.760 --> 02:13.200
and working on your laptop or on a remote server

02:13.200 --> 02:17.200
that you control, which is what I'm doing, by the way.

02:17.200 --> 02:20.080
So yes, I know what the feeling is like.

02:20.080 --> 02:25.040
So if you are set up and you're using Binder,

02:25.040 --> 02:27.400
remember every 15 minutes on the clock.

02:27.400 --> 02:30.800
So make sure you execute something in your notebooks

02:30.800 --> 02:34.440
because Binder sessions have a 20-minute timeout.

02:34.440 --> 02:37.360
If you are not already set up, go to the Read Me page.

02:37.360 --> 02:39.760
Go to the Read Me on the GitHub repository.

02:39.760 --> 02:41.840
Look for the Launch Binder button and hit it

02:41.840 --> 02:45.440
because you're going to not want to waste time with DevOps

02:45.440 --> 02:47.720
at this point.

02:47.720 --> 02:49.600
Cool.

02:49.600 --> 02:50.360
Let's see.

02:50.360 --> 02:54.400
We also have Ravine Kumar, who is one of the instructors

02:54.400 --> 02:56.360
for the next Bayes tutorial.

02:56.360 --> 03:00.280
He is helping out today and he'll be back in a few minutes.

03:00.280 --> 03:06.280
We can take a guess at a time that he'll be back.

03:06.280 --> 03:11.160
You'll notice I'm using a lot of these guess the time

03:11.160 --> 03:13.760
estimate that sort of terminology.

03:13.760 --> 03:18.520
It's very important that we've got both a theoretical view

03:18.520 --> 03:19.840
and a practical view on that.

03:19.840 --> 03:23.960
So hopefully today's tutorial will clarify that.

03:23.960 --> 03:26.120
OK.

03:26.120 --> 03:29.800
Before we go on, do people have any questions, any issues

03:29.800 --> 03:34.280
that they want to clarify, any things that are blocking them

03:34.280 --> 03:36.680
from doing today's tutorial?

03:36.680 --> 03:44.200
OK, if not, we're going to follow the agenda that's

03:44.200 --> 03:45.320
on the whiteboard.

03:45.320 --> 03:48.000
First off, we're going to do a recap on probability.

03:48.000 --> 03:50.760
Use some hands-on exercises to make sure

03:50.760 --> 03:53.320
that you've got a grasp on it.

03:53.320 --> 03:55.720
And then we'll recap what Bayes rule is.

03:55.720 --> 03:59.280
So for the 15 of you who were in Ellen's tutorial this morning,

03:59.280 --> 04:01.360
this should be familiar material.

04:01.360 --> 04:04.360
We won't do a full rigorous proof of Bayes rule.

04:04.360 --> 04:10.400
And we'll just make sure that everybody has a grasp on that.

04:10.400 --> 04:13.440
And it builds on top of the rules of conditional probability

04:13.440 --> 04:15.520
and joint probability.

04:15.520 --> 04:17.280
So once we're done with that, then

04:17.280 --> 04:21.240
we'll move on to what we would call the core activities

04:21.240 --> 04:25.960
of statistical inference, which is estimation.

04:25.960 --> 04:30.480
Estimation is the core of all statistical inference.

04:30.480 --> 04:31.880
Because we're in a Bayes tutorial,

04:31.880 --> 04:34.400
I'm going to go out on limb and saying calculating p values

04:34.400 --> 04:35.800
is not.

04:35.800 --> 04:37.920
Calculating p values is not the core activity.

04:37.920 --> 04:40.640
It's not even the point of statistical inference.

04:40.640 --> 04:42.880
So let's get that out of our heads.

04:42.880 --> 04:44.120
And then the final thing is we're

04:44.160 --> 04:51.240
going to show how the core activity of Bayesian statistical

04:51.240 --> 04:53.720
inference, which is Bayesian estimation, how that relates

04:53.720 --> 04:58.640
to things that we might have now already learned

04:58.640 --> 05:02.080
in our undergrad statistics or grad statistics classes, which

05:02.080 --> 05:04.680
is different forms of regression and the likes.

05:04.680 --> 05:08.200
And we'll do a short teaser on that.

05:08.200 --> 05:11.080
It's not the main point, because the main thing is really

05:11.120 --> 05:15.040
understanding estimation and how we build models that

05:15.040 --> 05:17.000
help us perform that.

05:17.000 --> 05:19.320
So with that, I'd like to invite everybody

05:19.320 --> 05:22.520
to open up your notebooks, fire up Jupiter,

05:22.520 --> 05:26.560
and open up notebook 1A.

05:26.560 --> 05:31.000
And then you'll notice that they always come in pairs.

05:31.000 --> 05:34.280
So there's a student version and an instructor version.

05:34.280 --> 05:39.280
And what I've decided, unlike my previous workshops,

05:39.280 --> 05:43.040
I'm going to live code with you.

05:43.040 --> 05:44.080
Not from memory, thankfully.

05:44.080 --> 05:45.520
I have a second computer here that

05:45.520 --> 05:49.200
tells me what the right answers are.

05:49.200 --> 05:53.680
You all, by the way, if you ever feel stuck on anything,

05:53.680 --> 05:56.280
the instructor versions of the notebooks are there for you.

05:56.280 --> 05:57.840
And that's the first time I'm going to say it,

05:57.840 --> 05:59.280
also the last time I'm going to say it.

05:59.280 --> 06:03.040
So if you're ever stuck, just open the instructor notebook.

06:03.040 --> 06:06.120
Don't hesitate to copy and paste the answers over.

06:06.120 --> 06:08.560
The point is understanding.

06:08.560 --> 06:10.520
Doing stuff helps with understanding.

06:10.520 --> 06:11.840
But if you get stuck doing stuff,

06:11.840 --> 06:14.400
it can lead to more frustration for learning

06:14.400 --> 06:15.720
than is productive.

06:15.720 --> 06:17.200
So at the end of the day, make sure you just

06:17.200 --> 06:20.080
have a good conceptual view, and that's good enough.

06:20.080 --> 06:23.520
All right, let's run the first cell

06:23.520 --> 06:27.520
and talk about what probability is.

06:27.520 --> 06:29.640
I had a few pop quiz questions just now.

06:29.640 --> 06:33.720
What's the probability that someone in this room,

06:33.720 --> 06:38.280
if you asked, would have a satisfying lunch today?

06:38.280 --> 06:40.680
And someone said, 75% of this room.

06:40.680 --> 06:41.640
That's another question.

06:41.640 --> 06:46.480
What is the probability that someone in this room

06:46.480 --> 06:49.360
has taken all three tutorials?

06:49.360 --> 06:50.720
All three Bayesian tutorials.

06:50.720 --> 06:53.400
And that's another question relating to probability.

06:53.400 --> 06:56.560
But I'd like to start by first asking the question a little

06:56.560 --> 06:58.320
bit more meta-level.

06:58.320 --> 07:01.320
What is probability?

07:01.680 --> 07:06.840
You have a volunteer, and I know no one's going to answer.

07:06.840 --> 07:09.840
So I would like you to do is talk with your partner

07:09.840 --> 07:13.960
for about 30 seconds to a minute, not 30 minutes.

07:13.960 --> 07:15.280
30 seconds to a minute.

07:15.280 --> 07:16.440
Introduce yourself.

07:16.440 --> 07:18.000
Network a little bit.

07:18.000 --> 07:21.680
And ask, what is probability?

07:21.680 --> 07:26.960
OK, so hopefully you all know each other a little better.

07:26.960 --> 07:29.320
Those of you who've been in the network analysis tutorials

07:29.320 --> 07:30.640
that I've done know this trick.

07:30.640 --> 07:32.520
This is the networking trick.

07:32.520 --> 07:36.640
Let's you build your network while you're in a tutorial room.

07:36.640 --> 07:42.840
So do we have volunteers to share maybe a stab at the question?

07:42.840 --> 07:45.640
What would you define probability as?

07:45.640 --> 07:48.080
We know some properties of probability, right?

07:48.080 --> 07:52.080
Someone said it must be bound from 0 to 1.

07:52.080 --> 07:53.240
What else?

07:53.240 --> 07:55.360
What other things about probability do you know?

07:55.680 --> 08:06.720
Volunteers, back there, a symbolic expression

08:06.720 --> 08:07.400
of uncertainty.

08:07.400 --> 08:11.000
Would you like to unpack that for us a little bit?

08:11.000 --> 08:12.080
No?

08:12.080 --> 08:15.120
All right, so then I'm supposed to do a stab at that, right?

08:15.120 --> 08:17.360
OK, let me ponder that.

08:17.360 --> 08:18.040
Do we have another?

08:19.040 --> 08:23.880
Something.

08:23.880 --> 08:24.480
Something.

08:24.480 --> 08:26.440
OK, yeah, yeah, OK, cool.

08:30.560 --> 08:33.520
Degree of belief, yep, yep, OK.

08:33.520 --> 08:34.000
Mark?

08:48.840 --> 09:02.800
Yeah, yep, OK, right, yep, OK.

09:02.800 --> 09:03.520
Anything else?

09:03.520 --> 09:06.320
Anybody else has a definition they'd like to contribute?

09:09.840 --> 09:10.320
Pardon me?

09:16.720 --> 09:18.000
Ah, OK, yeah, all right.

09:19.040 --> 09:26.040
Cool, right, so what I'm hearing are essentially

09:26.040 --> 09:29.640
the classical and the Bayesian, I refuse to say frequentist,

09:29.640 --> 09:31.680
the classical and the Bayesian ways

09:31.680 --> 09:34.960
of thinking about probability.

09:34.960 --> 09:37.880
Hugo, who made the first part of this tutorial,

09:37.880 --> 09:41.640
the material for the first part of this tutorial,

09:41.640 --> 09:43.400
found a really great quote.

09:43.400 --> 09:45.960
And it's inside your notebooks, and you'll see it, right?

09:45.960 --> 09:48.480
By data analysis, by severe and skilling.

09:48.480 --> 09:53.040
And there were essentially, historically,

09:53.040 --> 09:56.560
there was a shift in how probability was viewed.

09:56.560 --> 10:00.240
There was perhaps what we might call the first version that

10:00.240 --> 10:05.240
was formally recorded in European scientific history,

10:05.240 --> 10:10.440
which would be a degree of belief assigned to an outcome.

10:10.440 --> 10:15.800
And then there was then a shift because of views,

10:16.760 --> 10:19.080
foundational worldviews shifted.

10:19.080 --> 10:21.440
And so people thought of probability

10:21.440 --> 10:26.680
as more of the relative frequency with which things occurred,

10:26.680 --> 10:30.720
which then gave rise to the name that I refuse to mention.

10:30.720 --> 10:35.040
And because these frequencies can be measured,

10:35.040 --> 10:41.680
they then seemed to form an objective view of reality, right?

10:41.680 --> 10:44.680
I, on the other hand, don't subscribe to this view, right?

10:44.680 --> 10:47.800
I see probability defined.

10:47.800 --> 10:51.840
So there are formal definitions of probability.

10:51.840 --> 10:54.720
You get into these very technical mathematical terms

10:54.720 --> 10:59.880
that involve spaces and measure theory and the likes.

10:59.880 --> 11:04.840
The way that I prefer to, for day-to-day use of Bayesian

11:04.840 --> 11:07.880
methods and probability, I tend to think of probability

11:07.880 --> 11:11.560
as just being money assigned on a number line.

11:11.560 --> 11:13.920
If I gave you $100, how would you

11:13.920 --> 11:18.360
also assign that $100 to points on the number line

11:18.360 --> 11:19.560
if you were doing discrete things?

11:19.560 --> 11:22.480
Or how would you draw a curve that would say,

11:22.480 --> 11:26.160
assign how you would distribute money to the number line?

11:26.160 --> 11:27.560
Essentially, it's a measure of how much you're

11:27.560 --> 11:31.520
willing to bet that this thing will take on a particular value,

11:31.520 --> 11:33.360
this thing that you're interested in, right?

11:33.360 --> 11:36.000
So you can use money, or if you prefer not

11:36.000 --> 11:39.640
to gamble like myself, then I would just

11:39.640 --> 11:42.240
say credibility points assigned to the number line, right?

11:43.120 --> 11:45.360
Credibility points assigned to the number line

11:45.360 --> 11:47.880
gives us a view of probability that

11:47.880 --> 11:51.160
is a good enough working definition

11:51.160 --> 11:57.080
for how we can view probability in applied problems.

11:57.080 --> 12:03.240
So where do we want to then use probability as a tool?

12:03.240 --> 12:06.720
Once again, spend 30 seconds with your neighbors

12:06.720 --> 12:08.560
and talk about this.

12:08.560 --> 12:09.600
Let's gather back.

12:09.880 --> 12:13.680
I'd like to hear from those who haven't already raised their hands

12:13.680 --> 12:16.200
and contributed something to the discussion.

12:16.200 --> 12:20.880
Where would you use probability in an applied setting?

12:20.880 --> 12:21.720
At the back.

12:24.720 --> 12:25.720
OK.

12:25.720 --> 12:27.120
Did you elaborate on that, please?

12:31.120 --> 12:31.620
Right.

12:40.400 --> 12:41.680
Yes.

12:41.680 --> 12:42.880
Yeah, OK.

12:42.880 --> 12:44.520
Right, so probability is a tool for that.

12:44.520 --> 12:45.400
Next door neighbor.

12:48.720 --> 12:49.840
OK.

12:49.840 --> 12:50.800
Did you give an example?

13:01.680 --> 13:04.720
Right, so some key questions we might be interested in

13:04.720 --> 13:06.200
would be like, what's the probability

13:06.200 --> 13:10.520
of a catastrophic failure of that piece of infrastructure,

13:10.520 --> 13:12.080
for example, or something like that?

13:22.840 --> 13:25.640
Right, so calculation of, again, the probability

13:25.640 --> 13:27.800
of a catastrophic failure would be really important,

13:27.800 --> 13:31.880
and the probability that the safety mechanism would fail

13:31.880 --> 13:33.840
as well would be also important.

13:36.600 --> 13:50.840
Right, right, right, so you need to know the probability

13:50.840 --> 13:53.120
that the rate is equal to something

13:53.120 --> 13:54.600
plus the uncertainty on it.

13:54.600 --> 13:57.760
I'm very happy that we're going in a Bayesian direction here.

13:57.760 --> 13:59.560
OK, cool.

13:59.560 --> 14:04.920
Great, so if, say, we were, for example,

14:04.960 --> 14:09.040
in a marketing firm or a new tech firm,

14:09.040 --> 14:11.840
and we wanted to think about click-through rates, right?

14:11.840 --> 14:14.920
That's another place that probability comes into play,

14:14.920 --> 14:16.880
so another applied setting.

14:16.880 --> 14:19.560
And click-through rates are essentially

14:19.560 --> 14:22.360
measured as the probability that a user that arrives

14:22.360 --> 14:24.960
at your page will click on something, right?

14:24.960 --> 14:26.880
So that's another example.

14:26.880 --> 14:30.400
And that's the first example that we'll use here

14:30.400 --> 14:34.240
to get a practical handle on how we

14:34.240 --> 14:38.040
can use the existing tools in our toolkit.

14:38.040 --> 14:39.160
We probably know NumPy.

14:39.160 --> 14:41.800
We probably know a bit of pandas and the rest.

14:41.800 --> 14:42.800
NumPy, Matplotlib.

14:42.800 --> 14:48.240
We'll use NumPy to help us get a grasp on what exactly

14:48.240 --> 14:50.560
is probability, and how does it relate to proportions

14:50.560 --> 14:51.800
and the likes, OK?

14:51.800 --> 14:54.600
So let's say, let's try this example together.

14:54.600 --> 14:56.400
It's a code-along activity.

14:56.400 --> 15:00.000
Let's say we've got a website, and we've measured the click-through

15:00.000 --> 15:04.720
rate accurately to the ninth decimal place, right?

15:04.720 --> 15:09.360
So it's 50% accurately to the ninth decimal place.

15:09.360 --> 15:14.280
So what does that mean of the visitors that come by?

15:14.280 --> 15:17.400
So if we had 1,000 visitors, how many people

15:17.400 --> 15:21.160
would we expect to click on that button?

15:26.160 --> 15:27.160
Any volunteers?

15:27.160 --> 15:28.120
500, right?

15:28.160 --> 15:30.240
That's the expectation, right?

15:30.240 --> 15:31.640
We'll try to simulate that, OK?

15:31.640 --> 15:38.560
So if we have one way to simulate this kind of problem

15:38.560 --> 15:41.360
is to use NumPy, and to start, we'll

15:41.360 --> 15:44.840
take a uniform distribution, right?

15:44.840 --> 15:47.640
We'll start by saying, we've got a whole bunch of people.

15:47.640 --> 15:50.160
We don't know where they came from.

15:50.160 --> 15:54.200
And we'll simulate this process of clicking

15:54.200 --> 16:00.440
by taking 1,000 random numbers bounded between 0 and 1,

16:00.440 --> 16:04.720
and then cutting a threshold somewhere at 50%, right?

16:04.720 --> 16:11.160
So we'll do something like np.random.rand, 1,000.

16:11.160 --> 16:15.120
And what this will give us then is,

16:15.120 --> 16:20.240
if we plot the histogram of that, ooh, my kernel just died on me.

16:20.240 --> 16:21.960
Cool, all right?

16:21.960 --> 16:22.760
Let me try that again.

16:31.280 --> 16:32.840
We plot the histogram of that.

16:32.840 --> 16:35.760
You should get something that looks like this, right?

16:35.760 --> 16:40.520
So this is what we would call 1,000 draws

16:40.520 --> 16:43.440
with equal credibility assigned across the number line

16:43.440 --> 16:45.400
from 0 to 1.

16:45.400 --> 16:52.280
And to simulate how people click on the website,

16:52.280 --> 16:54.240
we can do a few things.

16:54.240 --> 17:02.000
We can do first ask how many of those values are below 0.5.

17:02.000 --> 17:05.880
And by definition, the rest are going to be above.

17:05.880 --> 17:11.920
And finally, simply sum up the total number of clicks

17:11.920 --> 17:12.720
that we get, right?

17:12.720 --> 17:19.160
We define clicks as a value being drawn from this distribution

17:19.160 --> 17:22.080
being less than 0.5, and then we simply sum it up.

17:22.080 --> 17:24.320
How many do people get?

17:24.320 --> 17:25.840
I get 481.

17:25.840 --> 17:27.720
What are the others?

17:27.720 --> 17:29.440
4, sorry?

17:29.440 --> 17:34.280
505, 478, et cetera.

17:34.280 --> 17:34.920
OK, cool.

17:34.920 --> 17:38.240
So we've got a variety of answers here.

17:38.240 --> 17:40.360
And this is one of the core ideas

17:40.360 --> 17:43.000
behind statistical inference.

17:43.000 --> 17:47.080
That is, there is always some form of randomness involved.

17:47.080 --> 17:50.520
And when there's some form of randomness involved,

17:50.520 --> 17:53.520
we will expect the answers.

17:53.520 --> 17:55.920
So we can calculate this thing called expectations.

17:55.920 --> 18:01.120
And we expect that, on average, from, say, a lot of us

18:01.120 --> 18:05.840
pooling our results together, the number of people who click

18:05.840 --> 18:12.320
will be centered around 500 at 0.5 out of 1,000.

18:12.320 --> 18:15.760
However, because of random effects and whatever else

18:15.760 --> 18:18.280
that goes on in the data generating process,

18:18.280 --> 18:21.160
we won't always get exactly 500.

18:21.160 --> 18:24.000
There's always some randomness involved.

18:24.000 --> 18:27.360
So then we can calculate the proportion

18:27.360 --> 18:34.080
that clicked as basically n clicks over length of clicks.

18:34.080 --> 18:35.760
And I will get 0.481.

18:35.760 --> 18:39.240
And you will get your 0.505s and whatever.

18:39.240 --> 18:45.720
So this is one of the, so one idea

18:45.720 --> 18:52.280
that we want to convey here is that in statistical inference,

18:52.280 --> 18:54.720
we're often talking about random processes, right?

18:54.720 --> 18:56.240
Things that are not deterministic.

18:56.240 --> 18:58.040
There's always a component that we

18:58.040 --> 19:02.280
don't know how to exactly write an equation to model exactly.

19:03.160 --> 19:06.720
And so what we do is we use a statistical model

19:06.720 --> 19:09.840
to help us get around this fact, right?

19:09.840 --> 19:13.600
And there's randomness built in inherently inside there.

19:13.600 --> 19:17.800
So all right, so what we did was we said

19:17.800 --> 19:21.400
we had a model, so-called model, of clicking

19:21.400 --> 19:24.400
in which we said half of the people who come will click.

19:24.400 --> 19:27.880
And then we drew samples from that model, right?

19:27.880 --> 19:31.360
Where we had our own, on our own computers,

19:31.360 --> 19:34.360
we had our own instantiations, our own realizations

19:34.360 --> 19:34.920
of that model.

19:34.920 --> 19:38.320
And there's some random effects that are inside there.

19:38.320 --> 19:43.160
OK, so what we'd like to do now is

19:43.160 --> 19:45.560
have you all do this on your own for the next two

19:45.560 --> 19:46.800
to three minutes.

19:46.800 --> 19:49.480
Try to simulate what the results will

19:49.480 --> 19:52.320
look like when you have a click through rate of 0.7 instead

19:52.320 --> 19:53.480
of 0.5, right?

19:53.480 --> 19:55.800
So spend a minute or two handling this.

19:55.800 --> 19:59.200
If you are ever stuck, you have lots of resources.

19:59.200 --> 20:00.200
You have your neighbors.

20:00.200 --> 20:01.760
So continue networking.

20:01.760 --> 20:04.320
If not, you also have the instructor notebook.

20:04.320 --> 20:05.600
And you have the screen, which I'm

20:05.600 --> 20:07.280
going to be typing on as well.

20:07.280 --> 20:11.000
Cool, so it'll look something like that.

20:11.000 --> 20:16.640
Once again, not everybody will have the same results, right?

20:16.640 --> 20:20.760
OK, so some numbers, just popcorn style this.

20:20.760 --> 20:25.880
703, 694, I have 708.

20:25.880 --> 20:28.640
What else?

20:28.680 --> 20:30.240
687, OK, cool.

20:30.240 --> 20:34.840
So we have what this forms is a distribution of numbers,

20:34.840 --> 20:36.840
realizations.

20:36.840 --> 20:41.760
Cool, this model that we've just simulated by hand

20:41.760 --> 20:45.120
is known as the biased coin flip.

20:45.120 --> 20:47.560
We know that the coin flip is the classic example

20:47.560 --> 20:51.000
that every probability tutorial has to deal with.

20:51.000 --> 20:53.800
The biased coin flip is just nothing more than a variant

20:53.800 --> 20:54.760
on that.

20:54.760 --> 20:57.120
Where else do you see biased coin flips happening?

20:59.040 --> 21:02.080
OK, talk with your neighbor, 30 seconds.

21:02.080 --> 21:04.520
Nobody will answer the first time on my first thing,

21:04.520 --> 21:07.000
so do we have volunteers?

21:07.000 --> 21:09.840
Where do we see the biased coin flip apart

21:09.840 --> 21:12.720
from click-through rates?

21:12.720 --> 21:13.920
Do we have a volunteer?

21:17.680 --> 21:18.160
Mark?

21:18.160 --> 21:18.660
Sorry.

21:18.660 --> 21:21.120
My example's kind of got limitations,

21:21.120 --> 21:24.560
but you're traveling on a tree, so traffic light will be

21:24.560 --> 21:30.640
a 70% priority, so the traffic light will be a 70% priority.

21:30.640 --> 21:33.880
Oh, OK, so if you looked at a junction,

21:33.880 --> 21:35.840
you can calculate the probability

21:35.840 --> 21:41.920
that the lights are favoring the main artery rather

21:41.920 --> 21:43.000
than the side arteries.

21:43.000 --> 21:45.240
OK, cool.

21:45.240 --> 21:46.600
A friend?

21:46.600 --> 21:49.000
Widget manufacturing.

21:49.000 --> 21:50.240
Can we talk about that?

21:50.680 --> 22:00.320
So you have a binary outcome, accepted or rejected?

22:00.320 --> 22:00.800
Where else?

22:00.800 --> 22:02.280
Back there.

22:02.280 --> 22:03.520
Is it at birth?

22:03.520 --> 22:07.280
Yeah, biological sex at birth, yes, exactly.

22:07.280 --> 22:10.800
So it's a binary situation for the vast majority

22:10.800 --> 22:13.360
of the population, so under that approximation,

22:13.360 --> 22:15.120
then what is it?

22:15.120 --> 22:19.320
The canonical is like 51% or 52% male, female, sorry,

22:19.320 --> 22:22.440
and slightly lower for male, right?

22:22.440 --> 22:22.960
Where else?

22:26.800 --> 22:30.320
All right, so I think the point is well described

22:30.320 --> 22:31.560
by these examples.

22:31.560 --> 22:34.960
There's a binary outcome that we're seeking to model,

22:34.960 --> 22:39.000
and the binary outcome sometimes is merely an approximation.

22:39.000 --> 22:43.600
The binary outcome is a useful, if it's a useful approximation,

22:43.600 --> 22:45.600
then we use that.

22:45.600 --> 22:48.040
We can use what we call the bias coin flip

22:48.040 --> 22:52.600
to model the process of generating the individual outcomes

22:52.600 --> 22:54.480
that we actually observe, right?

22:54.480 --> 22:56.360
OK, cool.

22:56.360 --> 22:58.080
Let's try a different example.

22:58.080 --> 23:01.960
So this example here was us basically

23:01.960 --> 23:06.720
hand coding the generative process in a very explicit

23:06.720 --> 23:10.000
fashion for what we would call the Bernoulli trials

23:10.000 --> 23:12.880
or binomial trials.

23:12.880 --> 23:14.720
We're going to try a different way, which

23:14.720 --> 23:17.320
is to actually look at real data and treat data

23:17.320 --> 23:22.120
as if they were the population, so to speak.

23:22.120 --> 23:25.840
As if the data that we measured were infinitely large enough,

23:25.840 --> 23:28.960
which is never true, but as an approximation,

23:28.960 --> 23:31.560
we'll start with that and ask, how can we

23:31.560 --> 23:33.760
calculate the probability of certain things

23:33.760 --> 23:36.960
under this assumption of the data being a really good

23:36.960 --> 23:38.840
approximation of ground truth?

23:38.840 --> 23:42.000
So what I'd like you to do is run this next cell.

23:42.000 --> 23:44.160
What this data set that we have here

23:44.160 --> 23:49.200
is Finch Beaks measured on Galapagos Islands, right?

23:49.200 --> 23:51.440
So how many of you have learned biology

23:51.440 --> 23:53.920
and have heard of the Finches before?

23:53.920 --> 23:54.840
Right, so there we go.

23:54.840 --> 23:59.440
We have another binary outcome right there.

23:59.440 --> 24:04.920
So in this, someone went to the Galapagos, a research team

24:04.920 --> 24:08.280
went to the Galapagos Islands and measured Finch Beaks,

24:08.280 --> 24:11.720
both their length and their depth, and asked,

24:11.720 --> 24:13.280
and just recorded what they were in.

24:13.280 --> 24:17.520
Let's assume that this is a realistic sample,

24:17.520 --> 24:20.440
a realistic approximation of the population of Finches

24:20.440 --> 24:21.880
that were observed.

24:21.880 --> 24:31.680
So let's grab out just the beak length, Th, as a Panda series.

24:31.680 --> 24:33.760
So follow along with that cell.

24:33.760 --> 24:36.280
And so what we'd like to ask is, what

24:36.280 --> 24:41.560
is the probability of a bird having a beak length greater

24:41.560 --> 24:43.000
than 10, right?

24:43.000 --> 24:45.920
So how would we calculate that?

24:45.920 --> 24:47.520
Well, one way we might calculate that

24:47.520 --> 24:51.320
is under the assumption that the data are the population,

24:51.320 --> 24:56.080
we could just ask what proportion of birds

24:56.080 --> 24:57.560
have beak lengths greater than 10.

24:57.560 --> 25:04.560
So we can do something like p is equal to the sum of lengths,

25:04.560 --> 25:12.120
Ths greater than 10 divided by the length of that.

25:12.120 --> 25:14.760
And you should get something close to,

25:14.760 --> 25:18.080
you should actually get this exact number in this case,

25:18.080 --> 25:19.920
because there's no randomness in the data.

25:19.920 --> 25:22.280
So we're not simulating the random process,

25:22.280 --> 25:25.160
we're treating the data as if they were the true thing.

25:25.160 --> 25:28.560
So everybody should get 0.851, right?

25:28.560 --> 25:29.600
Cool.

25:29.600 --> 25:32.960
So these two examples, they're really

25:32.960 --> 25:39.240
here to show you that proportions are somewhat

25:39.240 --> 25:41.360
a proxy for probability.

25:41.400 --> 25:43.760
Under certain circumstances, if you

25:43.760 --> 25:45.360
make explicit certain assumptions,

25:45.360 --> 25:47.920
your proportions can be a good estimator

25:47.920 --> 25:52.480
for the actual probability that we're interested in.

25:52.480 --> 26:06.520
All right, so over here, we can actually try simulating

26:06.520 --> 26:09.200
the finch beak lengths as well, right?

26:09.200 --> 26:14.400
So we can draw random samples from the data

26:14.400 --> 26:19.560
and use that as another way of estimating

26:19.560 --> 26:21.720
what the uncertainty around that probability would be.

26:21.720 --> 26:26.600
So this procedure is what we would call resampling

26:26.600 --> 26:27.400
with replacement.

26:27.400 --> 26:30.080
I think it's bootstrapping.

26:30.080 --> 26:32.200
My memory is blanking at this point.

26:32.200 --> 26:35.800
But resampling with replacement is another way to do it.

26:36.760 --> 26:41.320
So now, if we break the assumption

26:41.320 --> 26:45.440
that the data are exactly what the population are,

26:45.440 --> 26:46.840
then we're faced with a problem.

26:46.840 --> 26:50.000
We're faced with a problem that is the proportion

26:50.000 --> 26:52.240
that we've calculated may not actually

26:52.240 --> 26:58.120
be the true probability of finch beaks being

26:58.120 --> 27:00.080
greater than length 10.

27:00.080 --> 27:03.080
So how do we estimate what that uncertainty might be?

27:03.080 --> 27:06.040
We'll do this hacker statistic sort of method

27:06.040 --> 27:09.440
where we computationally try to simulate random draws

27:09.440 --> 27:11.560
from the population using the data.

27:11.560 --> 27:16.840
So the way we would do this here is to do,

27:16.840 --> 27:24.400
we would first do random picks from the data distribution,

27:24.400 --> 27:29.480
so mp.random.choice, lengths.

27:29.480 --> 27:34.440
So we're choosing from the length Panda series.

27:34.440 --> 27:43.640
We want 1,000, we want 10,000 samples from there.

27:43.640 --> 27:46.480
And we want to do it with replacement.

27:49.600 --> 27:58.720
And finally, we'll take the sum of that, sum over all

27:58.720 --> 28:02.800
of those that are greater than 10, and divide it by n samples.

28:06.160 --> 28:10.480
And so you will notice we won't get an exactly the same number.

28:10.480 --> 28:11.480
Question?

28:11.480 --> 28:13.080
Is the range about choice?

28:13.080 --> 28:14.080
Yes.

28:14.080 --> 28:16.400
Is that creating continuous distribution

28:16.400 --> 28:22.040
based on length, or is that choosing from the length?

28:22.040 --> 28:23.360
It is the latter.

28:23.360 --> 28:26.440
It is choosing from existing elements.

28:26.440 --> 28:27.640
All right?

28:27.680 --> 28:29.120
Cool.

28:29.120 --> 28:30.120
Question?

28:30.120 --> 28:35.040
So there are really only 229 values in it.

28:35.040 --> 28:41.040
So we'll just, like I said, we're resampling with replacement

28:41.040 --> 28:42.080
10,000 times.

28:42.080 --> 28:43.720
Yes, yes, exactly.

28:43.720 --> 28:46.120
OK?

28:46.120 --> 28:49.640
OK, so these are computational methods.

28:49.640 --> 28:53.160
One of them sort of is an explicit simulation

28:53.160 --> 28:55.400
for the coin flips.

28:55.400 --> 28:59.040
The other is treating data as ground truth.

28:59.040 --> 29:01.680
And then breaking that assumption,

29:01.680 --> 29:03.920
now saying data are not ground truth,

29:03.920 --> 29:05.760
what is the uncertainty in this parameter

29:05.760 --> 29:06.760
we're trying to estimate?

29:06.760 --> 29:10.560
So there are hacker stats ways of handling this.

29:10.560 --> 29:14.200
There's another way that we can deal with probability

29:14.200 --> 29:16.640
and talk about coin flips and the likes.

29:16.640 --> 29:23.040
So coin flips are essentially Bernoulli trials.

29:23.040 --> 29:27.720
Every coin flip that I make has a single outcome

29:27.720 --> 29:31.000
that can take on one of two possible values.

29:31.000 --> 29:35.520
We'll call them 0 and 1, or true and false,

29:35.520 --> 29:39.040
a binary outcome of some sort.

29:39.040 --> 29:43.200
And we can actually take advantage of,

29:43.200 --> 29:48.480
we can take advantage of NumPy's random number generators,

29:48.480 --> 29:50.560
or the probability distributions that

29:50.560 --> 29:52.760
are inside there, to try to simulate this.

29:52.960 --> 29:54.840
So there's Bernoulli trials.

29:54.840 --> 29:57.880
And then if you sum up Bernoulli trials,

29:57.880 --> 30:01.840
you get binomally distributed data.

30:01.840 --> 30:06.480
So we're going to try this out over here.

30:06.480 --> 30:13.840
So let's set a NumPy random seed.

30:13.840 --> 30:14.960
You can use 42.

30:14.960 --> 30:21.360
You can use 1,607,190, oh, 16 million, sorry,

30:21.360 --> 30:24.360
as your number, your choice.

30:24.360 --> 30:27.360
And we can flip, we can do a few things.

30:27.360 --> 30:28.840
So we're going to try this.

30:28.840 --> 30:32.440
We're going to try simulating a single flip.

30:32.440 --> 30:34.320
Let's try simulating a single flip.

30:34.320 --> 30:40.800
mp.random.binomial1, n equals 1.

30:40.800 --> 30:44.760
And it being a biased flip, we'll do 0.7.

30:44.760 --> 30:53.760
And so if you rerun that cell many times,

30:53.760 --> 30:57.560
you should get a lot of 1's, but some 0's, right?

31:00.120 --> 31:03.080
Oh, the seed, my bad.

31:03.080 --> 31:03.560
Thank you.

31:11.320 --> 31:12.800
There we go, yes.

31:12.840 --> 31:19.640
So you will get a bunch of 1's and a bunch of 0's.

31:19.640 --> 31:22.800
If you summed up all of those Bernoulli trials,

31:22.800 --> 31:25.920
say we did 10 of those trials together

31:25.920 --> 31:29.080
and treated them as a single experimental run,

31:29.080 --> 31:32.520
so we'll now do mp.random.binomial of 10,

31:32.520 --> 31:36.720
you'll get sometimes 10, sometimes 4, sometimes 6,

31:36.720 --> 31:38.760
et cetera, et cetera.

31:38.800 --> 31:45.320
So now what we're really interested in

31:45.320 --> 31:49.840
is how, in a biased coin flip, what we're really interested

31:49.840 --> 31:56.360
in then is what the probability is of getting,

31:56.360 --> 31:58.000
sorry, let me backtrack a little bit.

31:58.000 --> 32:04.120
So in a binomial trial, in a binomial draw or a binomial run,

32:04.120 --> 32:07.040
what we're interested in is calculating the probability,

32:07.040 --> 32:13.480
knowing the probability of getting up to that number

32:13.480 --> 32:18.560
of successes inside, up to that number of successes

32:18.560 --> 32:19.440
for that run, right?

32:19.440 --> 32:22.320
So the binomial trial is basically

32:22.320 --> 32:25.480
defined as n being the number of successes,

32:25.480 --> 32:28.680
sorry, n being the number of total Bernoulli runs

32:28.680 --> 32:31.840
that we've done, and p being the probability of successes.

32:31.840 --> 32:34.360
And it gives back a number, which is the number of successes

32:34.360 --> 32:37.640
out of that n number of trials.

32:37.640 --> 32:40.920
So what values, sorry, pop quiz, what values

32:40.920 --> 32:45.320
can this take on, the result of a binomial trial?

32:48.560 --> 32:54.200
0 to 10 or n in the general case where we know what n is,

32:54.200 --> 32:55.360
right, OK, cool.

32:55.360 --> 32:59.680
So we can actually simulate this.

32:59.680 --> 33:07.000
We can simulate, say, 10,000 experiments of us flipping coins

33:07.000 --> 33:10.160
10 times and counting the number of times

33:10.160 --> 33:15.160
that we got ahead in each particular experiment, right?

33:15.160 --> 33:28.240
So we can do that by doing mp.random.binomial 10, 0.7 and 10,000.

33:28.240 --> 33:29.920
If you want some clarity, you can actually

33:29.920 --> 33:32.280
use the underscore between your numbers

33:32.280 --> 33:34.560
to make sure you know what you're typing.

33:34.560 --> 33:41.440
So in that, we'll give us something

33:41.440 --> 33:44.720
that looks like this, right?

33:44.720 --> 33:53.280
So what I'd like to ask you then is, what do you see in the chart?

33:53.280 --> 33:56.280
What values are probable?

33:56.280 --> 33:57.520
Probable values are not probable.

34:00.160 --> 34:02.040
I'm not going to ask you to talk with your neighbor

34:02.040 --> 34:03.040
this time around.

34:06.040 --> 34:09.120
Any volunteers?

34:09.120 --> 34:12.240
So the most probable value is 7, right?

34:12.240 --> 34:17.960
And we know that from the fact that our p, which we set, was 0.7.

34:17.960 --> 34:21.280
So the expectation or the most likely value,

34:21.280 --> 34:24.120
the thing we expect to see the most is 0.7.

34:24.120 --> 34:25.160
What else do we see?

34:27.280 --> 34:31.040
The upper bound, bounded at 10, right?

34:31.040 --> 34:33.360
We don't see any values at 0, 1, or 2.

34:36.360 --> 34:37.360
What's happening there?

34:41.360 --> 34:42.360
Is this very small?

34:42.360 --> 34:45.600
Yeah, we've done the experiment 10,000 times,

34:45.600 --> 34:47.920
and that's still not enough, right?

34:47.920 --> 34:49.000
It's still not enough for us to be

34:49.000 --> 34:51.720
able to see whether there is a probability,

34:51.720 --> 34:55.160
see the proportion of the probability.

34:55.160 --> 35:01.200
The proportion of trials that we will get one head only out of 10.

35:01.200 --> 35:06.240
OK, so I'd like you to try the following exercises.

35:06.240 --> 35:09.760
Spend about two to three minutes on them.

35:09.760 --> 35:12.480
The first one is calculating the probability of five

35:12.480 --> 35:16.680
or more heads for a value of p is 0.3, then for 0.5,

35:16.680 --> 35:19.920
plot the histograms for both of them.

35:19.920 --> 35:22.240
Yeah, so spend a minute or two handling that.

35:22.240 --> 35:24.560
Just really quickly go over.

35:24.560 --> 35:30.680
You should get something like this for the first exercise,

35:30.680 --> 35:34.320
something around the value of 0.7655,

35:34.320 --> 35:36.760
something like that for the second exercise, right?

35:36.760 --> 35:39.880
The probability of seeing a heads is higher,

35:39.880 --> 35:44.120
so therefore the probability of seeing five or more heads out

35:44.120 --> 35:47.360
of 20 is also going to be higher, right?

35:47.360 --> 35:52.560
So 99% of those have that result.

35:52.560 --> 35:55.160
And if you plot the histogram, you

35:55.160 --> 35:56.840
should see something like this.

35:56.840 --> 36:00.120
Now, for those of you who had that bit of time

36:00.120 --> 36:02.960
to think about the question, looking at the histogram,

36:02.960 --> 36:09.200
can you tell me what is the probability of seeing four

36:09.200 --> 36:10.160
or more heads?

36:12.680 --> 36:15.160
Pardon me?

36:15.160 --> 36:17.680
Yes.

36:17.680 --> 36:21.000
But a lot is not a number from 0 to 1, right?

36:21.040 --> 36:22.880
So what is that number from 0 to 1?

36:26.480 --> 36:28.400
Pardon me?

36:28.400 --> 36:31.000
85%, OK, maybe.

36:31.000 --> 36:33.000
It's not very easy to tell, right?

36:33.000 --> 36:35.920
Not very easy to tell.

36:35.920 --> 36:39.720
This is the sort of question where histograms are kind

36:39.720 --> 36:41.720
of not the right thing to look at.

36:41.720 --> 36:44.080
It's the thing that we're used to looking at,

36:44.080 --> 36:47.080
but it's not the kind of thing that would give us rich.

36:47.080 --> 36:49.040
It's not the kind of plot that would give us

36:49.040 --> 36:52.160
rich statistical information on the data

36:52.160 --> 36:54.880
that we have on hand, OK?

36:54.880 --> 36:55.360
Question?

36:59.440 --> 37:01.640
Danka, yes, exactly.

37:01.640 --> 37:06.280
So what we want instead is the cumulative distribution

37:06.280 --> 37:07.800
of our data.

37:07.800 --> 37:11.120
And that, I argue in a blog post,

37:11.120 --> 37:14.920
gives us much richer information than a histogram would.

37:14.920 --> 37:16.640
A histogram is nice and convenient

37:16.640 --> 37:19.160
to look at because it sort of tells us where the central

37:19.160 --> 37:22.000
tendency is, but that's all it really

37:22.000 --> 37:25.120
can tell us what the central tendency is, right?

37:25.120 --> 37:27.160
Maybe the bounds, but even the bounds

37:27.160 --> 37:28.680
aren't going to be shown very accurately,

37:28.680 --> 37:33.160
as we would see from this histogram up there, right?

37:33.160 --> 37:38.480
So this is where the cumulative distribution of our data

37:38.480 --> 37:40.520
comes into play, and we can plot what

37:40.520 --> 37:43.680
we would call the empirical cumulative distribution

37:43.720 --> 37:46.880
function, the ECDF, of our data.

37:46.880 --> 37:50.640
And the ECDF is a great way to visualize this, OK?

37:50.640 --> 37:55.080
So if we take our data and we arrange them, well, actually,

37:55.080 --> 37:56.880
let's code along, let's code along,

37:56.880 --> 37:59.720
and you'll see what the ECDF will look like for this,

37:59.720 --> 38:01.720
for binomally distributed data.

38:01.720 --> 38:07.320
So X flips and Y flips are going to be ECDF of our data,

38:07.320 --> 38:10.760
which our data is X. What it will return

38:10.760 --> 38:15.560
is it'll give us indices on the x-axis, which

38:15.560 --> 38:18.880
are where our data points fall.

38:18.880 --> 38:21.760
And then it'll give us an index on the y-axis, which

38:21.760 --> 38:24.400
is a number from 0 to 1 that tells us

38:24.400 --> 38:27.480
how much of our data falls below that particular data

38:27.480 --> 38:29.160
points value, OK?

38:29.160 --> 38:33.880
So let's do that, and we'll do plt.plot x flips,

38:33.880 --> 38:42.920
y flips, marker is a dot, oops, line style is none.

38:45.640 --> 38:46.880
Oops, I forgot to run that.

38:49.720 --> 38:55.000
Now, things look a little clearer, right?

38:55.000 --> 38:58.720
So for those of you who have the histogram on your screen,

38:58.720 --> 39:00.600
keep the histogram on your screen

39:00.600 --> 39:05.240
and compare it to the ECDF that's on the projector screens.

39:05.240 --> 39:08.360
Now let me ask, what's the probability

39:08.360 --> 39:11.840
of getting 4 or higher in our data?

39:18.520 --> 39:19.760
Something percent, yes.

39:19.760 --> 39:22.400
OK, so here's how you look at it.

39:22.400 --> 39:29.640
You go to 4, you go up to the bottom of that bar-like looking

39:29.640 --> 39:33.600
thing, and then you draw a line across to there,

39:33.600 --> 39:35.800
and it falls roughly at 0.2, right?

39:35.800 --> 39:40.520
So the probability of getting 4 or higher is approximately 0.2.

39:40.520 --> 39:48.760
And the thing about ECDFs is 0.8, sorry, thank you, 1 minus.

39:48.760 --> 39:50.440
We're doing the higher, thank you.

39:50.440 --> 39:53.600
So the thing about ECDFs, though,

39:53.600 --> 39:56.640
is that it gives you much richer statistical information.

39:56.640 --> 39:59.040
You can tell things like the central tendency.

39:59.080 --> 40:00.360
So what's the central tendency?

40:03.680 --> 40:09.080
Well, we go to 0.5, draw a line across,

40:09.080 --> 40:11.960
see where it hits the data, and look on the x-axis,

40:11.960 --> 40:15.880
and that value is 5, exactly.

40:15.880 --> 40:17.600
What are the bounds of the data?

40:21.880 --> 40:24.320
Exactly, and take a look at that.

40:24.320 --> 40:27.880
We couldn't see the 0 in the histogram

40:27.880 --> 40:31.240
because it's kind of like obscured by the height of the rest.

40:31.240 --> 40:33.160
One thing that's cool about the ECDF

40:33.160 --> 40:36.360
is that it uses all of the data.

40:36.360 --> 40:41.040
There's no binning bias that can obscure the values

40:41.040 --> 40:43.600
that your data can take on.

40:43.600 --> 40:44.840
That is a serious problem.

40:44.840 --> 40:47.040
You can lie with histograms.

40:47.040 --> 40:49.200
You do not want to lie with histograms.

40:49.200 --> 40:51.160
I will come after you, OK?

40:54.080 --> 40:55.920
OK, cool.

40:55.920 --> 40:59.360
So I made my case, and I rest my case with ECDFs.

40:59.360 --> 41:01.360
Don't ever use histograms again.

41:01.360 --> 41:02.720
Always use ECDFs.

41:02.720 --> 41:05.240
They give you a much richer view onto your data.

41:05.240 --> 41:09.160
You can look at all of the percentiles of interest.

41:09.160 --> 41:11.240
You can visualize, sorry, all of the percentiles

41:11.240 --> 41:17.040
that are relevant in your modeling problems using ECDFs, OK?

41:17.040 --> 41:22.560
OK, so we did a little recap on probability

41:22.560 --> 41:24.880
just to make sure we're all clear

41:24.920 --> 41:26.920
and we're on the same page.

41:26.920 --> 41:32.320
Probability is credibility points assigned to the number line,

41:32.320 --> 41:33.480
OK?

41:33.480 --> 41:35.080
When we plot something like this, we're

41:35.080 --> 41:38.720
saying there's lots of credibility points assigned

41:38.720 --> 41:39.920
to this value.

41:39.920 --> 41:42.200
There's very little credibility points assigned

41:42.200 --> 41:45.080
to the tail values, OK?

41:45.080 --> 41:47.480
That's all probability is a working definition

41:47.480 --> 41:48.800
for our purposes.

41:48.800 --> 41:52.400
We can simulate draws from a probability distribution.

41:52.400 --> 41:54.400
We did that multiple ways.

41:54.400 --> 42:03.120
We can simulate it from data by doing sampling with replacement.

42:03.120 --> 42:05.800
Now, what we're going to do, oh, sorry, and finally,

42:05.800 --> 42:09.160
we can actually take advantage of the exact analytical form,

42:09.160 --> 42:13.040
analytically implemented probability distributions

42:13.040 --> 42:19.320
in NumPy and SciPy stats, and use that to help us simulate

42:19.320 --> 42:23.120
what our data might look like under a set of fixed parameters.

42:23.120 --> 42:25.160
When I was learning Bayesian stats,

42:25.160 --> 42:27.960
this activity was really, really helpful

42:27.960 --> 42:30.680
for getting familiar with the shapes of probability

42:30.680 --> 42:32.440
distributions.

42:32.440 --> 42:34.720
And as you'll see later, the shapes

42:34.720 --> 42:37.080
of your probability distributions, particularly

42:37.080 --> 42:40.000
the bounds, the central tendency, and how they're skewed,

42:40.000 --> 42:45.040
can be really useful pieces of knowledge

42:45.040 --> 42:47.920
that no longer are just trivia in your head,

42:47.920 --> 42:51.360
but actually can be useful tools for modeling.

42:51.400 --> 42:55.040
What we're going to do now is do a very, very, very, very quick

42:55.040 --> 42:58.400
run through of the different probability distributions

42:58.400 --> 43:01.480
and what their so-called stories are.

43:01.480 --> 43:02.680
Probability distribution.

43:02.680 --> 43:04.320
Oh, well, let me backtrack a little bit.

43:04.320 --> 43:06.920
How many of you have heard of the term generative models

43:06.920 --> 43:09.080
of data?

43:09.080 --> 43:11.240
Yeah, those who've been in the deep learning world

43:11.240 --> 43:15.120
will know that there is this term called generative models.

43:15.120 --> 43:17.600
And frankly, at some companies, like the one that I worked at,

43:17.600 --> 43:20.640
it's been overhyped quite a bit.

43:20.640 --> 43:24.600
Everyone wants to talk about generative models.

43:24.600 --> 43:27.800
At its core, generative models are just

43:27.800 --> 43:32.600
how we can generate things that look like data.

43:32.600 --> 43:35.240
And if you think hard and long about it,

43:35.240 --> 43:39.440
probability distributions are generative models of data.

43:39.440 --> 43:41.920
Probability distributions are generative models of data

43:41.920 --> 43:45.560
because we can construct a model that

43:45.560 --> 43:49.520
is composed of purely just probability distributions

43:49.520 --> 43:53.520
and use it to simulate data that looks like actual data

43:53.520 --> 43:55.520
that we might collect.

43:55.520 --> 43:57.880
So let's think about what actual data we might collect,

43:57.880 --> 44:00.720
say, starting with the Poisson distribution, right?

44:00.720 --> 44:04.960
So Poisson processes and the Poisson distributions.

44:04.960 --> 44:06.360
That's a generative model.

44:06.360 --> 44:08.680
And it's got a story that's behind it.

44:08.680 --> 44:12.760
And this concept of what is the story behind each

44:12.760 --> 44:14.520
and every probability distribution

44:14.520 --> 44:17.880
is something that we need to make sure we're familiar with.

44:17.880 --> 44:22.200
So Poisson distributed data, basically,

44:22.200 --> 44:26.400
can be thought of as something like this.

44:26.400 --> 44:29.400
This is borrowed from David McKay's book.

44:29.400 --> 44:32.280
I have a town, and it's called Poissonville.

44:32.280 --> 44:38.440
And the buses, they're kind of like MBTA trains in Boston.

44:38.440 --> 44:40.680
So they come, and then sometimes you

44:40.680 --> 44:43.280
have to wait a heck of a long time before the next one comes.

44:43.280 --> 44:46.240
And sometimes the next one comes right after them, right?

44:46.280 --> 44:48.720
They're the one that just came by.

44:48.720 --> 44:51.440
Yeah, and then sometimes they get derailed and, well,

44:51.440 --> 44:53.600
too bad for Bostonians.

44:53.600 --> 45:00.920
So the amount of time that you wait for one train or one bus

45:00.920 --> 45:02.920
is independent of the amount of time

45:02.920 --> 45:06.240
that you waited for the previous bus, right?

45:06.240 --> 45:09.120
So that's the story of a Poisson process.

45:09.520 --> 45:16.400
And so the timing of the next event

45:16.400 --> 45:20.000
is completely independent of when the previous event happened.

45:20.000 --> 45:26.040
And so it's not just faulty trains on the red line in Boston.

45:26.040 --> 45:27.320
There's other things, too, right?

45:27.320 --> 45:29.240
There's like births in a hospital.

45:29.240 --> 45:32.000
There's meteor strikes.

45:32.000 --> 45:34.680
God help us if we have one.

45:34.680 --> 45:38.320
Aviation incidents, the rate of things happening, right?

45:38.320 --> 45:40.840
Number of events happening per unit time.

45:40.840 --> 45:44.080
So that's what a Poisson process is.

45:44.080 --> 45:49.880
And the number of arrivals that occur in a given amount of time

45:49.880 --> 45:52.480
takes on a Poisson distribution.

45:52.480 --> 45:54.560
So all that the Poisson distribution is modeling

45:54.560 --> 46:00.880
is just how many things happen within a given unit of time.

46:00.880 --> 46:04.800
So we can actually simulate that.

46:05.240 --> 46:15.400
So if we want what we can do here is we can simulate Poisson

46:15.400 --> 46:22.120
draws, np.random.poisson, with per unit time six events

46:22.120 --> 46:24.760
happening, so six natural births per day,

46:24.760 --> 46:32.440
or six collisions at Brigham Circle near Harvard Medical.

46:32.440 --> 46:37.000
And we'll do like 10 to the power of six.

46:37.000 --> 46:39.080
Let's do a million draws, right?

46:39.080 --> 46:41.000
And then plot what this distribution looks like.

46:41.000 --> 46:46.960
PLT.hist of samples will just set the bin size

46:46.960 --> 46:49.400
so that it's a convenient thing for us to visualize.

46:49.400 --> 46:52.640
And you'll get something that looks like that, right?

46:52.640 --> 46:55.920
Does the shape look kind of familiar?

46:55.920 --> 47:00.840
Not that you've seen it before, but like some other distribution?

47:00.840 --> 47:02.720
The binomial, did I hear?

47:02.720 --> 47:04.200
Yeah, what's up with the binomial?

47:08.960 --> 47:12.720
Yeah, so this one isn't exactly symmetric, but close enough,

47:12.720 --> 47:13.640
yes?

47:13.640 --> 47:16.000
Yeah, so there's a neat relationship

47:16.000 --> 47:19.200
between the Poisson distribution and the binomial

47:19.200 --> 47:22.480
distribution, that is for low probability of successes,

47:22.480 --> 47:27.720
that is for really, really rare events that happen.

47:27.720 --> 47:32.760
The Poisson distribution is the approximation, or the limit,

47:32.760 --> 47:35.480
as we go to low probability of success

47:35.480 --> 47:37.600
and large number of trials, right?

47:37.600 --> 47:40.240
So there's a relationship between these distributions.

47:43.480 --> 47:48.480
We can actually, let's plot the ECDF of this as well.

47:48.480 --> 47:55.400
So ECDF of our samples that we drew.

47:55.400 --> 47:57.240
And then we plot this guy again.

48:15.080 --> 48:17.480
And you should get something that looks like that.

48:17.480 --> 48:22.400
Once again, this tells us a lot of information.

48:22.400 --> 48:24.080
What is the central tendency in this case?

48:26.400 --> 48:27.840
Six, yes, exactly.

48:27.840 --> 48:32.200
So the central tendency, I'm using this term

48:32.200 --> 48:33.960
as the more generalized thing, right?

48:33.960 --> 48:37.200
So the central tendency can be either the mean, the median,

48:37.200 --> 48:39.480
or the mode, depending on which one we're talking about.

48:39.480 --> 48:43.360
The expectation is the mean, and the expectation

48:43.360 --> 48:47.000
of the Poisson distribution is the parameter

48:47.000 --> 48:48.440
that we passed into it, right?

48:48.440 --> 48:53.040
So if we say that things are Poisson distributed with six

48:53.080 --> 48:55.320
events per unit time, then we expect

48:55.320 --> 48:58.280
to see six events, the central tendency will be six,

48:58.280 --> 49:02.120
the median will be six, the mean will be six, OK?

49:02.120 --> 49:06.040
All right, so something that's also Poisson distributed

49:06.040 --> 49:10.240
would be field goal attempts per game.

49:10.240 --> 49:14.200
Yesterday over lunch, a bunch of us

49:14.200 --> 49:17.560
were talking about how football, and I'm

49:17.560 --> 49:21.240
sorry to the American football fans, that's hand egg,

49:21.840 --> 49:24.040
real football that is played with your feet,

49:26.080 --> 49:30.320
that's sort of at a field goal rate that's not that good,

49:30.320 --> 49:33.400
whereas hockey is at the right field goal rate,

49:33.400 --> 49:37.320
whereas basketball is at some absurdly high field goal rate.

49:38.320 --> 49:43.320
So we're gonna do an example taken from professor,

49:43.760 --> 49:46.080
an instructor at Caltech, Justin Voice,

49:46.080 --> 49:48.600
who I've met at the SciPy conference,

49:48.600 --> 49:53.400
and it's about field goal attempts by LeBron James, right?

49:54.520 --> 49:59.520
And he did, that's his data, the his stats per game,

49:59.680 --> 50:04.080
number of things that he, number of times

50:04.080 --> 50:07.200
that he shot in one game, so the unit time is one game,

50:07.200 --> 50:09.920
and we're asking how many attempts did he make, OK?

50:14.240 --> 50:15.920
I need to reconnect my VPN.

50:16.120 --> 50:21.200
Pardon me, OK, so let's move on.

50:21.200 --> 50:23.720
So we've got the field goal attempts here.

50:28.080 --> 50:30.200
All right, that's OK.

50:30.200 --> 50:31.840
It'll come back when it comes back.

50:33.840 --> 50:38.840
And we'll do the ECDF of this data, all right?

50:43.320 --> 50:44.680
Cool.

50:44.680 --> 50:49.680
And finally, what we're gonna do is we're gonna do

50:49.760 --> 50:53.120
many random draws from a Poisson simulation,

50:53.120 --> 50:57.520
from a Poisson distribution, and plot all of those random

50:57.520 --> 51:01.720
draws, the ECDFs of those random draws to see whether

51:01.720 --> 51:06.720
it follows the same distribution as what LeBron's

51:07.000 --> 51:09.360
field goal attempts would be like.

51:09.360 --> 51:13.860
So let's code along, so for underscore in range of that,

51:14.920 --> 51:19.920
samples is np.random.poisson, np.mean of field goal attempts,

51:21.720 --> 51:26.720
and the size is the length of field goal attempts,

51:27.000 --> 51:29.680
and in this case, we'll plot the ECDF

51:29.680 --> 51:31.240
of the theoretical samples.

51:36.400 --> 51:39.400
And let's see, my kernel's disconnected,

51:39.400 --> 51:44.240
so I'm going to have to reconnect and rerun cells.

51:44.680 --> 51:49.680
And since we're at it, I'll just bring up the instructor

51:55.800 --> 51:59.040
version so that we have it there.

51:59.040 --> 52:01.480
You should get a plot that looks something like that,

52:01.480 --> 52:03.960
right, OK?

52:03.960 --> 52:05.800
And so with that guy over there,

52:08.320 --> 52:10.760
that's one probability distribution story

52:10.760 --> 52:13.180
that we can talk about, the Poisson distribution.

52:13.220 --> 52:16.620
Now the Poisson distribution is a discreet

52:16.620 --> 52:17.740
probability distribution.

52:17.740 --> 52:20.420
What's the other class of distributions?

52:20.420 --> 52:21.900
The continuous family, right?

52:21.900 --> 52:26.900
OK, so the discreets, when we plot that,

52:28.940 --> 52:32.380
they technically follow a probability mass function

52:32.380 --> 52:35.740
because there's a bulk of mass that's associated

52:35.740 --> 52:39.100
with each particular value, that is how credibility

52:39.100 --> 52:43.660
is assigned, with continuous distributions,

52:43.660 --> 52:46.620
we have the probability distribution function,

52:46.620 --> 52:49.580
which technically have zero mass at any point

52:49.580 --> 52:51.340
on the x-axis, right?

52:51.340 --> 52:55.860
But they take on, they have a density of mass,

52:55.860 --> 53:00.020
so-called, from within a range of continuous values, OK?

53:01.180 --> 53:05.020
OK, we're going to skip one or two,

53:05.020 --> 53:07.680
we're going to skip the exponential distribution,

53:07.680 --> 53:09.640
I'll just leave it out there for you,

53:09.640 --> 53:12.960
that the exponential distribution is the waiting time

53:12.960 --> 53:17.960
between Poisson events, and so that's that,

53:19.000 --> 53:23.120
and so you can take a look at how the CDF,

53:23.120 --> 53:25.920
ECDF of the exponential distribution

53:25.920 --> 53:27.440
will look like on your own time.

53:27.440 --> 53:30.160
We'll go through the normal distribution, OK?

53:30.160 --> 53:33.160
The normal distribution, everyone's familiar with this,

53:33.160 --> 53:35.720
right, things are normally distributed,

53:35.720 --> 53:38.320
a lot of things, sorry, are normally distributed, OK?

53:39.920 --> 53:41.920
We've got measurements, and in this case,

53:41.920 --> 53:45.400
we'll just take a look at what the story is.

53:45.400 --> 53:48.400
The story is, in this quote here,

53:48.400 --> 53:50.120
when doing repeated measurements,

53:50.120 --> 53:52.880
we expect them to be normally distributed,

53:52.880 --> 53:56.160
owing to the fact that many sub-processes,

53:56.160 --> 53:59.560
lots of individual data-generating processes,

53:59.560 --> 54:02.200
contribute to this final thing that we measure, OK?

54:02.200 --> 54:07.120
So things like human height is a culmination of many genes,

54:07.120 --> 54:09.040
so it's kind of reasonable,

54:09.040 --> 54:11.240
it's a culmination of lots of genes,

54:11.240 --> 54:13.880
plus environmental effects put together,

54:13.880 --> 54:15.440
and so it's reasonable to expect

54:15.440 --> 54:17.520
that human height would be normally distributed

54:17.520 --> 54:19.600
or approximately so, right?

54:21.760 --> 54:25.520
So yeah, so the formulation,

54:25.520 --> 54:28.360
one formulation of the CLT, the central limit theorem,

54:28.360 --> 54:30.160
is that any quantity that emerges

54:30.320 --> 54:32.920
as the sum of a large number of sub-processes

54:32.920 --> 54:34.920
tends to be normally distributed,

54:34.920 --> 54:38.360
provided that none of the sub-processes

54:38.360 --> 54:40.560
is very broadly distributed itself,

54:40.560 --> 54:42.680
that is, it doesn't overwhelm

54:42.680 --> 54:46.720
the final data distribution that we're looking at, OK?

54:46.720 --> 54:50.880
So just to have you all take a look at this,

54:50.880 --> 54:54.600
these are what we would call,

54:54.600 --> 54:56.640
these are measurements of the speed of light,

54:56.640 --> 54:59.440
and there's, in an experiment,

54:59.440 --> 55:02.560
lots of factors contribute to what measure,

55:02.560 --> 55:05.560
what value we eventually measure, OK?

55:05.560 --> 55:09.000
So there's some data on the speed of light,

55:09.000 --> 55:12.480
and the estimate of the speed of light

55:12.480 --> 55:14.600
tends to be normally distributed

55:14.600 --> 55:17.800
because of this data-generative process, right?

55:17.800 --> 55:20.280
Lots of things contributing to this final thing.

55:20.280 --> 55:22.560
There's error in the instrument,

55:22.560 --> 55:25.240
there's error in, there's the instrument itself,

55:25.240 --> 55:26.720
and it's got some error,

55:26.720 --> 55:28.920
there's the measurement technique,

55:28.920 --> 55:30.360
and it's got some error,

55:31.440 --> 55:34.320
there's the conditions of the day

55:34.320 --> 55:37.320
that we're measuring and that that can affect,

55:37.320 --> 55:40.360
that's the reason most biologists tend to use

55:40.360 --> 55:43.720
something happened that day for a field experiment,

55:43.720 --> 55:46.120
and I know it myself because I was one before.

55:47.000 --> 55:49.000
So yes, you get the point, though, right?

55:49.000 --> 55:53.160
So there's a combination of factors that lead to this.

55:53.160 --> 55:54.000
OK.

55:54.880 --> 55:58.480
Now, I want to leave, before we go on a break,

55:58.480 --> 56:01.240
I want to leave you with this little tidbit

56:01.240 --> 56:03.680
which comes from Ellen Downey's blog post.

56:03.680 --> 56:07.000
Are your data normally distributed?

56:07.000 --> 56:09.080
I encourage you to look at that blog post

56:09.080 --> 56:11.920
because though something,

56:13.960 --> 56:18.720
though we impose models on our data,

56:18.720 --> 56:22.520
our data may not necessarily be

56:23.480 --> 56:27.120
exactly following that model that we've imposed on.

56:27.120 --> 56:29.120
So when we make an assumption

56:29.120 --> 56:32.320
or when we impose this idea that our data might be normal

56:32.320 --> 56:36.360
and we do a fit, we check how deviant our data

56:36.360 --> 56:38.040
are from normal distributions,

56:38.040 --> 56:39.560
we use that normal distribution

56:39.560 --> 56:41.920
and later downstream things for estimating the mean

56:41.920 --> 56:43.560
and the likes.

56:43.560 --> 56:45.640
When we do things like that,

56:45.640 --> 56:47.840
we're saying we're imposing a model

56:47.840 --> 56:49.680
and that model might be wrong,

56:49.680 --> 56:51.160
but it can be useful, right?

56:51.160 --> 56:53.120
This is a classic George Box quote.

56:55.680 --> 56:59.080
I'm going to mash that with George Orwell

56:59.080 --> 57:01.560
saying that some models are wrong

57:01.560 --> 57:05.120
but some are more wrong than others, OK?

57:05.120 --> 57:08.200
And we might be, find some of them useful, right?

57:08.200 --> 57:11.200
So take a look at Ellen's post, ponder about that point.

57:11.200 --> 57:14.960
We'll come back at 2.45 p.m.

57:14.960 --> 57:17.160
There should be snacks inside the Tejas room.

57:18.160 --> 57:21.520
We'll come back to in notebook 1b

57:21.520 --> 57:24.160
and very quickly go through joint

57:24.160 --> 57:26.120
and conditional probability, OK?

57:26.120 --> 57:28.800
This is a last minute decision that I made

57:30.280 --> 57:31.880
in that in the interest of time,

57:31.880 --> 57:33.760
there's some ideas I want to cover.

57:33.760 --> 57:38.200
We'll cut down a little bit on the foundational exercises

57:38.200 --> 57:40.520
but then we'll have the concepts given to you all.

57:40.520 --> 57:45.200
So you have the tools to do your modeling later on, OK?

57:45.200 --> 57:47.120
So as I mentioned in the interest of time,

57:47.120 --> 57:49.640
we're going to skip a few exercises.

57:49.640 --> 57:54.920
The next notebook is on conditional and joint probability

57:54.920 --> 58:00.160
and really I want to give you all mostly just the tools

58:00.160 --> 58:02.120
that you need to be able to think through

58:02.120 --> 58:03.920
the problems at hand, OK?

58:03.920 --> 58:06.080
So one of the tools in the toolkit

58:06.080 --> 58:09.000
is the distributions and the stories

58:09.000 --> 58:10.640
that are associated with them.

58:12.400 --> 58:13.840
I drew this out as a table

58:13.840 --> 58:15.960
but by the way there's a really great resource

58:15.960 --> 58:19.680
by Justin Boyce who put all of the distribution stories

58:19.680 --> 58:23.600
that are relevant on his Caltech website.

58:23.600 --> 58:27.040
So that's a really, really great resource to go and check

58:27.040 --> 58:28.880
and I'll make sure that's also linked

58:28.880 --> 58:30.160
on our GitHub repository

58:30.160 --> 58:33.280
so you can always check that out from there, OK?

58:33.280 --> 58:36.960
So just to recap, we've got probability,

58:36.960 --> 58:41.880
they follow, we've defined as credibility

58:41.880 --> 58:43.640
assigned on the number line,

58:43.640 --> 58:47.160
we have ways of simulating probability distributions,

58:47.160 --> 58:50.480
both analytically and by brute force computation.

58:51.720 --> 58:55.160
Really the core thing that we want to have in our toolkit

58:55.160 --> 58:57.960
is actually the probability distributions

58:57.960 --> 59:00.800
as well as their shapes and their stories, OK?

59:00.800 --> 59:05.000
So this is a very incomplete, highly partial table

59:05.000 --> 59:06.920
of the many probability distributions

59:06.920 --> 59:10.040
that exist out there and I wanted to, you know,

59:10.040 --> 59:13.960
hint at you should be building such a matrix for yourself

59:15.280 --> 59:16.960
and it is really helpful,

59:16.960 --> 59:18.800
like you can maybe have a cheat sheet

59:18.800 --> 59:21.240
written by someone else but I always find it

59:21.240 --> 59:23.800
like if I do the hard work and draw it out

59:23.800 --> 59:26.000
and write it out for myself, it's also really useful.

59:26.000 --> 59:28.720
Nonetheless, I'll be building a resource

59:28.720 --> 59:33.720
that basically 100% plagiarizes Justin's resource

59:34.720 --> 59:39.320
but gives added nice visuals for learning benefit, OK?

59:40.600 --> 59:42.600
But really, so the toolkits are,

59:42.600 --> 59:45.200
you want to know the names of these distributions

59:45.200 --> 59:47.400
because they aid in communication

59:47.400 --> 59:50.800
with other people who do statistics.

59:50.800 --> 59:51.880
When you're writing things out,

59:51.880 --> 59:54.560
you'll want to know what their abbreviations are, right?

59:54.560 --> 59:56.560
So that guy highlighted over there

59:56.560 --> 59:58.840
because that gives a nice and compact way

59:58.840 --> 01:00:02.360
of telling people what distribution you're using

01:00:02.360 --> 01:00:04.040
when you're writing stuff.

01:00:04.040 --> 01:00:05.680
In your mental model,

01:00:05.680 --> 01:00:08.880
you want something like the shape of a distribution, right?

01:00:08.880 --> 01:00:12.440
The uniform is just equal credibility between two bounds.

01:00:12.440 --> 01:00:14.680
So that picture should come into your head.

01:00:14.680 --> 01:00:16.120
You should know whether the bounds

01:00:16.120 --> 01:00:17.920
are relevant to your problem or not

01:00:17.920 --> 01:00:21.360
and also whether equal credibility makes sense or not, right?

01:00:23.360 --> 01:00:25.960
Just so that you have something live,

01:00:25.960 --> 01:00:29.360
there's a variant or the generalization

01:00:29.360 --> 01:00:31.160
of the uniform distribution,

01:00:31.160 --> 01:00:34.920
which is the beta distribution.

01:00:34.920 --> 01:00:37.120
It takes in two parameters,

01:00:37.120 --> 01:00:41.440
number of successes, number of failures.

01:00:41.440 --> 01:00:45.240
It's actually bound between zero and one explicitly

01:00:45.240 --> 01:00:49.280
and takes on values that can look like that

01:00:49.280 --> 01:00:52.520
or can look like this

01:00:54.480 --> 01:00:57.200
or can look like that, right?

01:00:57.200 --> 01:00:58.920
So, but the key point is that

01:00:58.920 --> 01:01:00.720
it takes on values that are bounded.

01:01:01.720 --> 01:01:05.920
There's this term which took me a little while to remember,

01:01:05.920 --> 01:01:08.400
but it's called the support of a distribution.

01:01:08.400 --> 01:01:10.480
This is something that you'll see

01:01:10.480 --> 01:01:12.400
inside the statistics literature.

01:01:12.400 --> 01:01:14.680
The support of a distribution is nothing more

01:01:14.680 --> 01:01:17.680
than the values that it can take on,

01:01:17.680 --> 01:01:19.640
the values that it can take on, right?

01:01:19.640 --> 01:01:20.720
That's all it is.

01:01:20.720 --> 01:01:23.960
So the support for the beta distribution

01:01:23.960 --> 01:01:29.080
is something like a, oops, zero to one,

01:01:29.120 --> 01:01:31.400
it's bounded between zero to one, right?

01:01:31.400 --> 01:01:34.160
And there's a story for the beta distribution,

01:01:34.160 --> 01:01:38.080
which is number of successes

01:01:41.640 --> 01:01:46.640
expected fraction of successes

01:01:48.360 --> 01:01:53.360
out of n success plus n failure, okay?

01:01:54.360 --> 01:01:58.360
And it can take on, the alpha and beta parameters

01:01:58.360 --> 01:02:00.360
can take on not just integer,

01:02:00.360 --> 01:02:03.360
but also floating point, decimal numbers, okay?

01:02:03.360 --> 01:02:06.360
So you'll want to have this kind of picture in your head

01:02:06.360 --> 01:02:08.360
when you're thinking about that.

01:02:08.360 --> 01:02:09.360
Where can you learn?

01:02:09.360 --> 01:02:12.360
Again, I'm saying Justin Boyce's website is a great place.

01:02:12.360 --> 01:02:14.360
I learned a lot of these probability distributions

01:02:14.360 --> 01:02:18.360
and their shapes and their possible values

01:02:18.360 --> 01:02:20.360
by looking at the prime seed,

01:02:20.360 --> 01:02:23.360
prime seed three has a great thing for that.

01:02:23.360 --> 01:02:25.360
But you'll notice, in prime seed three,

01:02:25.360 --> 01:02:27.360
basically all they're doing is just,

01:02:27.360 --> 01:02:29.360
all we're doing in the docs is just simulating

01:02:29.360 --> 01:02:31.360
those distributions and plotting them out.

01:02:31.360 --> 01:02:33.360
So that really is the best way to do it.

01:02:33.360 --> 01:02:36.360
All of that hands-on simulation that we just did,

01:02:36.360 --> 01:02:39.360
that is a great, great way for you to learn

01:02:39.360 --> 01:02:41.360
what the probability distributions are,

01:02:41.360 --> 01:02:44.360
what their shapes, what their data generating processes

01:02:44.360 --> 01:02:45.360
are all about, okay?

01:02:45.360 --> 01:02:47.360
So I want to start with,

01:02:47.360 --> 01:02:50.360
that's one thing you want to have in your toolkit.

01:02:50.360 --> 01:02:53.360
The next thing you'll want to have in your toolkit

01:02:53.360 --> 01:02:57.360
is this idea of joint and conditional probability.

01:02:57.360 --> 01:03:00.360
Once again, in the interest of time,

01:03:00.360 --> 01:03:02.360
by the way, for those of you who just got back,

01:03:02.360 --> 01:03:04.360
make sure, and using Binder,

01:03:04.360 --> 01:03:07.360
execute something so you don't lose your session.

01:03:07.360 --> 01:03:12.360
The way I think about joint, conditional,

01:03:12.360 --> 01:03:14.360
and marginal probability,

01:03:14.360 --> 01:03:19.360
is by a visual that looks something like this.

01:03:26.360 --> 01:03:30.360
If I have data that are jointly distributed,

01:03:30.360 --> 01:03:33.360
they might look something like that.

01:03:33.360 --> 01:03:36.360
Say, this is a bivariate Gaussian.

01:03:36.360 --> 01:03:39.360
This put together is what we would call

01:03:39.360 --> 01:03:42.360
the joint distribution.

01:03:43.360 --> 01:03:46.360
Of the two things that we're interested in,

01:03:46.360 --> 01:03:49.360
X1 and X2, okay?

01:03:49.360 --> 01:03:53.360
Then there's a thing called conditional distribution.

01:03:53.360 --> 01:03:59.360
That is, what is the distribution of one of the two axes,

01:03:59.360 --> 01:04:03.360
given that we know something about the other, okay?

01:04:03.360 --> 01:04:07.360
So if we use red to do the conditional distribution,

01:04:08.360 --> 01:04:15.360
this is a known value of X1.

01:04:15.360 --> 01:04:16.360
Oh, sorry.

01:04:16.360 --> 01:04:22.360
This joint distribution is denoted as p of X1 and X2.

01:04:25.360 --> 01:04:26.360
Okay?

01:04:26.360 --> 01:04:28.360
So in red, we're going to show you

01:04:28.360 --> 01:04:30.360
what the conditional distribution looks like.

01:04:30.360 --> 01:04:33.360
So the known distribution looks like that.

01:04:33.360 --> 01:04:34.360
Oh, shucks.

01:04:34.360 --> 01:04:36.360
I hit the off button.

01:04:38.360 --> 01:04:42.360
If we take that joint distribution

01:04:45.360 --> 01:04:48.360
and project it back onto the X2 axis,

01:04:53.360 --> 01:04:56.360
it itself will follow a distribution.

01:04:58.360 --> 01:04:59.360
Okay?

01:04:59.360 --> 01:05:00.360
Are we okay with this?

01:05:00.360 --> 01:05:01.360
Right?

01:05:01.360 --> 01:05:03.360
This is the distribution of X2,

01:05:03.360 --> 01:05:05.360
given that we know X1.

01:05:05.360 --> 01:05:10.360
This is what we would call the conditional distribution.

01:05:13.360 --> 01:05:14.360
Okay?

01:05:14.360 --> 01:05:22.360
And this is denoted probability of X2 given X1,

01:05:22.360 --> 01:05:27.360
where that little thing over there is given.

01:05:28.360 --> 01:05:29.360
Okay?

01:05:29.360 --> 01:05:32.360
That pipe tells the statistician that you're taking,

01:05:33.360 --> 01:05:36.360
that you're computing the distribution of X2,

01:05:36.360 --> 01:05:41.360
having known a particular value of X1.

01:05:41.360 --> 01:05:42.360
Okay?

01:05:42.360 --> 01:05:44.360
Are we okay with this so far?

01:05:44.360 --> 01:05:46.360
Okay, there's a final idea,

01:05:46.360 --> 01:05:48.360
which I'm hoping you'll keep,

01:05:48.360 --> 01:05:50.360
which is known as the marginal distribution.

01:05:52.360 --> 01:05:55.360
Marginal distribution looks like this.

01:05:57.360 --> 01:05:59.360
There are two distributions over here.

01:06:02.360 --> 01:06:03.360
Okay?

01:06:04.360 --> 01:06:05.360
This in blue

01:06:07.360 --> 01:06:11.360
are what we would call the marginal distribution.

01:06:11.360 --> 01:06:14.360
Why is it called the distribute marginal?

01:06:14.360 --> 01:06:21.360
Well, first off, it's on the margins of this two-axis thing.

01:06:21.360 --> 01:06:22.360
Okay?

01:06:22.360 --> 01:06:27.360
It is the value of X1 ignoring whatever values,

01:06:27.360 --> 01:06:30.360
ignoring whatever value that X2 is.

01:06:30.360 --> 01:06:31.360
Okay?

01:06:31.360 --> 01:06:35.360
So it is the value of X1 completely ignoring

01:06:35.360 --> 01:06:38.360
the other variants that are of interest.

01:06:38.360 --> 01:06:39.360
Okay?

01:06:39.360 --> 01:06:43.360
So this is denoted as P of X1.

01:06:43.360 --> 01:06:46.360
This is denoted as P of X2.

01:06:46.360 --> 01:06:49.360
And those are marginal distributions.

01:06:49.360 --> 01:06:50.360
Okay?

01:06:50.360 --> 01:06:53.360
So prior to drawing this out for myself,

01:06:53.360 --> 01:06:55.360
this was something that I didn't,

01:06:55.360 --> 01:07:00.360
wasn't really able to keep straight in my head.

01:07:00.360 --> 01:07:03.360
But this served as a very,

01:07:03.360 --> 01:07:05.360
like what I would call an anchoring example

01:07:05.360 --> 01:07:07.360
for what these three terms mean,

01:07:07.360 --> 01:07:11.360
joint, conditional, and marginal probability.

01:07:11.360 --> 01:07:15.360
Now, why are these three terms really important?

01:07:15.360 --> 01:07:20.360
It's because it's from joint and conditional probability

01:07:20.360 --> 01:07:22.360
that we make our way, joint, conditional,

01:07:22.360 --> 01:07:24.360
and marginal probability that we make our way

01:07:24.360 --> 01:07:26.360
to what we call Bayes' Rule.

01:07:26.360 --> 01:07:27.360
Okay?

01:07:27.360 --> 01:07:32.360
Bayes' Rule, if you've seen the famous neon light photo,

01:07:32.360 --> 01:07:42.360
is written as P of A given B is P of B given A

01:07:42.360 --> 01:07:46.360
times P of A over P of B.

01:07:46.360 --> 01:07:48.360
Where does this come from?

01:07:48.360 --> 01:07:50.360
Well, this comes from the definition

01:07:50.360 --> 01:07:54.360
of joint probability.

01:07:54.360 --> 01:08:07.360
So P of A, B is equal to P of A given B times P of B,

01:08:07.360 --> 01:08:13.360
which is equal to P of B given A times P of A.

01:08:13.360 --> 01:08:18.360
And if you simply isolate out this portion

01:08:19.360 --> 01:08:24.360
and move this guy over there,

01:08:24.360 --> 01:08:29.360
then suddenly you have Bayes' Rule, right?

01:08:29.360 --> 01:08:32.360
And this is a neat thing because it gives us a way

01:08:32.360 --> 01:08:38.360
to move between probability of data given model

01:08:38.360 --> 01:08:40.360
or in probability of model given data

01:08:40.360 --> 01:08:45.360
if we take an alternative view of what we're doing, okay?

01:08:45.360 --> 01:08:47.360
And to illustrate this example,

01:08:47.360 --> 01:08:51.360
we're actually going to use the drug testing example

01:08:51.360 --> 01:08:53.360
in notebook 1B, so I'd like to invite you

01:08:53.360 --> 01:09:00.360
to navigate to there, okay?

01:09:00.360 --> 01:09:11.360
I'm going to plug this back to my laptop.

01:09:11.360 --> 01:09:16.360
So the drug testing example is one of those classic things

01:09:16.360 --> 01:09:25.360
where we can come up with a solution to a problem,

01:09:25.360 --> 01:09:27.360
but if we don't do the stats right,

01:09:27.360 --> 01:09:29.360
we'll be kind of off, all right?

01:09:29.360 --> 01:09:31.360
So I hope you'll see this from this point.

01:09:31.360 --> 01:09:35.360
So let's look at the question, right?

01:09:35.360 --> 01:09:37.360
So we have a test.

01:09:37.360 --> 01:09:43.360
It's 99% true positive for drug users

01:09:43.360 --> 01:09:46.360
and 99% true negative for non-drug users.

01:09:46.360 --> 01:09:50.360
What that means is if I give you a drug user

01:09:50.360 --> 01:09:54.360
and then I ask you to do the test on that drug user,

01:09:54.360 --> 01:09:59.360
then 99% of the time it will be correct.

01:09:59.360 --> 01:10:02.360
And if I give you a non-drug user

01:10:02.360 --> 01:10:04.360
and I ask you to do the test,

01:10:04.360 --> 01:10:09.360
then 99% of the time it will be correct as well.

01:10:09.360 --> 01:10:12.360
So it sounds like a great device, right?

01:10:12.360 --> 01:10:15.360
So what I'd like to then ask you to do

01:10:15.360 --> 01:10:18.360
is before doing any of this computation,

01:10:18.360 --> 01:10:22.360
write down what you think the probability will be

01:10:22.360 --> 01:10:25.360
that a drug user is,

01:10:25.360 --> 01:10:30.360
sorry, a positive testing user will be a drug user.

01:10:30.360 --> 01:10:32.360
Put down a number.

01:10:32.360 --> 01:10:35.360
It's got to be from zero to one because it's a probability.

01:10:35.360 --> 01:10:38.360
Put down a number mentally in your head.

01:10:38.360 --> 01:10:41.360
Yeah, it's in the notebooks.

01:10:41.360 --> 01:10:45.360
So 99% true positive results for drug users

01:10:45.360 --> 01:10:49.360
and 99% true negative results for non-drug users.

01:10:49.360 --> 01:10:51.360
Okay?

01:10:51.360 --> 01:10:57.360
It's in your notebook, so scroll down on notebook 1b.

01:10:57.360 --> 01:10:59.360
Write down a number.

01:10:59.360 --> 01:11:02.360
And when you're done, give me a thumbs up.

01:11:02.360 --> 01:11:04.360
Don't think too hard on this one.

01:11:04.360 --> 01:11:09.360
You're meant to be surprised.

01:11:09.360 --> 01:11:15.360
Okay, so we can actually simulate this whole process, right?

01:11:15.360 --> 01:11:18.360
Because we know,

01:11:18.360 --> 01:11:25.360
let's switch back to here.

01:11:25.360 --> 01:11:33.360
We know a few things about the data-generating process.

01:11:33.360 --> 01:11:42.360
We can actually represent this as a tree.

01:11:42.360 --> 01:11:48.360
That looks ugly.

01:11:48.360 --> 01:11:55.360
That tree might look something like this.

01:11:55.360 --> 01:12:02.360
Someone's a user and someone's a non-user.

01:12:02.360 --> 01:12:10.360
They test and they turn out to be positive and negative.

01:12:10.360 --> 01:12:14.360
Positive and negative.

01:12:14.360 --> 01:12:16.360
Now, help me fill in the blanks.

01:12:16.360 --> 01:12:22.360
What is the probability that the user tests positive

01:12:22.360 --> 01:12:25.360
given that they are a user?

01:12:25.360 --> 01:12:27.360
99%.

01:12:27.360 --> 01:12:34.360
I'm going to use 0.99 here.

01:12:34.360 --> 01:12:38.360
What should the value be on the negative arm?

01:12:38.360 --> 01:12:40.360
0.01, very good.

01:12:40.360 --> 01:12:44.360
What about the non-user?

01:12:44.360 --> 01:12:48.360
0.01, right?

01:12:48.360 --> 01:12:52.360
If the user is not a drug user,

01:12:52.360 --> 01:12:56.360
1% of the time they will test positive as a drug user.

01:12:56.360 --> 01:13:02.360
99% of the time, oops, I'm being inconsistent,

01:13:02.360 --> 01:13:04.360
they will test negative.

01:13:04.360 --> 01:13:08.360
This sounds like a really good test, right?

01:13:08.360 --> 01:13:14.360
What we're interested in, however, is P of drug user

01:13:14.360 --> 01:13:20.360
given positive because this guy here is P of positive

01:13:20.360 --> 01:13:26.360
given drug user.

01:13:26.360 --> 01:13:28.360
So, how do we calculate this?

01:13:28.360 --> 01:13:29.360
Well, we can simulate it.

01:13:29.360 --> 01:13:53.360
Let's come back to the notebook.

01:13:54.360 --> 01:14:02.360
So, if we take 10,000 subjects and we simulate...

01:14:02.360 --> 01:14:05.360
Sorry, let me backtrack a little bit.

01:14:05.360 --> 01:14:09.360
In order to solve that problem, we're missing one piece of information.

01:14:09.360 --> 01:14:15.360
That is, what is the probability that a user is a drug user?

01:14:15.360 --> 01:14:17.360
A person is a drug user.

01:14:17.360 --> 01:14:19.360
We have the people, persons group,

01:14:19.360 --> 01:14:23.360
who are a drug user and non-drug user.

01:14:23.360 --> 01:14:28.360
So, some may say that if we're in Central West Virginia,

01:14:28.360 --> 01:14:32.360
then opioid crisis is like ravaging there, right?

01:14:32.360 --> 01:14:37.360
And it's a black stain on the pharmaceutical industry for that.

01:14:37.360 --> 01:14:45.360
So, we might put an estimate that 5% to 10% of the population are drug users.

01:14:45.360 --> 01:14:48.360
Now, if we're in clean, clean Massachusetts,

01:14:48.360 --> 01:14:52.360
then what might we believe about this fraction?

01:14:52.360 --> 01:14:56.360
It might be 10 times smaller, say 0.05, or New York City, right?

01:14:56.360 --> 01:14:59.360
Like, here goes from New York, or Sydney, actually, that's his hometown.

01:14:59.360 --> 01:15:03.360
So, 0.05, right?

01:15:03.360 --> 01:15:06.360
Sorry, not 0.05, 0.005, right?

01:15:06.360 --> 01:15:12.360
So, let's simulate how many drug users we will get under that particular regime.

01:15:12.360 --> 01:15:17.360
So, we'll do mp.random.binomial.

01:15:17.360 --> 01:15:21.360
We'll have 10,000 users, okay?

01:15:21.360 --> 01:15:27.360
We have a probability of them being,

01:15:27.360 --> 01:15:33.360
the probability of them being a drug user is 0.05,

01:15:33.360 --> 01:15:35.360
and we only want one trial.

01:15:35.360 --> 01:15:41.360
And so, the non-users is going to be n minus the number of users, right?

01:15:41.360 --> 01:15:45.360
Number of non-users in our population is just the complement, okay?

01:15:45.360 --> 01:15:47.360
We run that cell.

01:15:47.360 --> 01:15:57.360
Oh, yeah, all right, I need...

01:15:57.360 --> 01:16:10.360
So, of the users, how many of them will test positive?

01:16:10.360 --> 01:16:13.360
I said 99%, but it's, you know, a probability.

01:16:13.360 --> 01:16:16.360
So, we'll explicitly simulate it.

01:16:16.360 --> 01:16:20.360
mp.random.binomial again.

01:16:20.360 --> 01:16:25.360
We have the users, the number of users in the population,

01:16:25.360 --> 01:16:29.360
and 99% of them will test positive.

01:16:29.360 --> 01:16:33.360
And then we'll have the number of non-users,

01:16:33.360 --> 01:16:37.360
but we also want to know how many of them will test positive, right?

01:16:37.360 --> 01:16:39.360
So, how would we simulate that?

01:16:39.360 --> 01:16:43.360
mp.random.binomial non-users,

01:16:43.360 --> 01:16:46.360
and what's the probability value inside here?

01:16:46.360 --> 01:16:48.360
0.01, right?

01:16:48.360 --> 01:16:51.360
Because we're only interested, we're interested in,

01:16:51.360 --> 01:16:56.360
given that you're positive, what is the probability that you are a true user

01:16:56.360 --> 01:17:00.360
or not a user, right, of this drug?

01:17:00.360 --> 01:17:06.360
So, what fraction of those tests will be positive for users?

01:17:06.360 --> 01:17:10.360
What do we need to divide by?

01:17:10.360 --> 01:17:15.360
We need the sum of non-users and users,

01:17:15.360 --> 01:17:21.360
sorry, non-positive and positives,

01:17:21.360 --> 01:17:24.360
or rather non-users that did test positive.

01:17:24.360 --> 01:17:26.360
Gosh, the naming is tough.

01:17:26.360 --> 01:17:30.360
That's the hardest thing in computer science, right?

01:17:30.360 --> 01:17:33.360
If we calculate that,

01:17:33.360 --> 01:17:39.360
you'll get something like, what, 0.3-ish?

01:17:39.360 --> 01:17:40.360
You all get that?

01:17:40.360 --> 01:17:43.360
Is this surprising?

01:17:43.360 --> 01:17:44.360
Pardon me?

01:17:44.360 --> 01:17:45.360
It's a big number.

01:17:45.360 --> 01:17:48.360
It's a big number, yeah.

01:17:48.360 --> 01:17:57.360
Yeah, like, we thought we could get away with like a 99% sensitive and 99% specific test.

01:17:57.360 --> 01:18:01.360
But it turns out, because the thing that we're really interested in

01:18:01.360 --> 01:18:06.360
is inferring the thing that isn't shown from the data.

01:18:06.360 --> 01:18:09.360
The thing that isn't shown from the data is the latent thing,

01:18:09.360 --> 01:18:12.360
that is, are you a drug user or not?

01:18:12.360 --> 01:18:14.360
And then that shows up in the drug test,

01:18:14.360 --> 01:18:17.360
but the drug test has some probability of error as well.

01:18:17.360 --> 01:18:20.360
So, if we go back and think about that tree that we drew,

01:18:20.360 --> 01:18:23.360
that's the full data-generating process.

01:18:23.360 --> 01:18:27.360
That is the full data-generating process for the things that we observed.

01:18:27.360 --> 01:18:33.360
And now we can use that to back-infer the probability of the thing that we're interested in

01:18:33.360 --> 01:18:40.360
rather than the probability of data given the underlying condition or the model,

01:18:40.360 --> 01:18:45.360
rather than the, sorry, probability of the model, underlying condition or model given the data,

01:18:45.360 --> 01:18:47.360
rather than the thing that is easy to simulate.

01:18:47.360 --> 01:18:53.360
The thing that's really easy to simulate is probability of the data given my model of the world,

01:18:53.360 --> 01:18:56.360
but what if my model is wrong, right?

01:18:56.360 --> 01:18:59.360
What if my model needs updating?

01:18:59.360 --> 01:19:05.360
And so that's where this Bayes rule thing comes in handy, okay?

01:19:05.360 --> 01:19:06.360
All right.

01:19:06.360 --> 01:19:11.360
Now, if you look at how this is solved with Bayes theorem,

01:19:11.360 --> 01:19:13.360
the equations are in the notebook.

01:19:13.360 --> 01:19:15.360
Take a look at that.

01:19:15.360 --> 01:19:17.360
What I'd encourage you to keep in mind, though,

01:19:17.360 --> 01:19:23.360
is we're interested in the probability of our,

01:19:23.360 --> 01:19:27.360
the distribution of our parameters of interest given the data, right?

01:19:27.360 --> 01:19:32.360
When we're talking about modeling our data-generating process,

01:19:32.360 --> 01:19:36.360
we're going to stick in parameters like, you know, p, right?

01:19:36.360 --> 01:19:41.360
Probability of success or lambda or mu, you know?

01:19:41.360 --> 01:19:44.360
Lambda for the rate of a Poisson process or mu,

01:19:44.360 --> 01:19:48.360
the central tendency for a normally distributed thing.

01:19:48.360 --> 01:19:52.360
But we might be wrong and we need to update our model having seen new data,

01:19:52.360 --> 01:19:55.360
and that's where Bayes rule comes in, okay?

01:19:55.360 --> 01:19:59.360
So we're going to look, what we're going to do next,

01:19:59.360 --> 01:20:04.360
over the next two and a half hours, is to look very,

01:20:04.360 --> 01:20:11.360
sorry, two hours, is to look very in-depth into two particular data-generating stories

01:20:11.360 --> 01:20:14.360
that can be applied across multiple places.

01:20:14.360 --> 01:20:18.360
So in some senses, these are fairly generic models

01:20:18.360 --> 01:20:21.360
that you can take home and use in your modeling work.

01:20:21.360 --> 01:20:25.360
But we're going to go really deep into each and every one of those.

01:20:25.360 --> 01:20:29.360
And so, pardon me if you are already quite familiar with this,

01:20:29.360 --> 01:20:31.360
but I think it's handy for a few reasons.

01:20:31.360 --> 01:20:34.360
One, you'll have the mechanics of PMC-3,

01:20:34.360 --> 01:20:39.360
which is the tool that we're going to use under your toolkit.

01:20:39.360 --> 01:20:48.360
You'll also have the process of telling a data-generating story in your mind as well, okay?

01:20:48.360 --> 01:20:52.360
And we'll have practice with that, okay?

01:20:52.360 --> 01:20:54.360
So we're going to skip notebook number two

01:20:54.360 --> 01:21:02.360
and instead move to notebook number three directly, okay?

01:21:02.360 --> 01:21:08.360
So I'd like to invite you to open up notebook number three.

01:21:08.360 --> 01:21:13.360
And this is where we jump right into what we would call

01:21:13.360 --> 01:21:16.360
probabilistic programming and Bayesian estimation.

01:21:16.360 --> 01:21:18.360
These are probabilistic programming.

01:21:18.360 --> 01:21:25.360
Oh, sorry, go ahead.

01:21:25.360 --> 01:21:32.360
Yeah, let me put that up.

01:21:32.360 --> 01:21:40.360
Yeah, yeah, yeah, definitely, definitely, definitely.

01:21:40.360 --> 01:21:46.360
So let's write this out, okay?

01:21:46.360 --> 01:21:53.360
Bayes' rule states that p of x1, x2, the joint distribution,

01:21:53.360 --> 01:21:57.360
sorry, Bayes' rule starts from this formulation.

01:21:57.360 --> 01:22:01.360
It's p of x1 given x2 times p of x2,

01:22:01.360 --> 01:22:10.360
which is equal to p of x2 given x1 times the probability of x1, okay?

01:22:10.360 --> 01:22:11.360
All right?

01:22:11.360 --> 01:22:22.360
So if we say probability, so if we do the rearrangement of terms,

01:22:22.360 --> 01:22:28.360
then we get x1, well, let's make this fit the example that we're looking at.

01:22:28.360 --> 01:22:38.360
Probability of x2 given x1 is therefore equal to p of x1 given x2 times p of x2

01:22:38.360 --> 01:22:43.360
over p of x1.

01:22:43.360 --> 01:22:47.360
Are we okay here so far?

01:22:47.360 --> 01:22:49.360
Okay, it'll be up there.

01:22:49.360 --> 01:22:54.360
I'm unfortunately restrained by the size of the screen here as well, right,

01:22:54.360 --> 01:22:57.360
in order to fit enough inside here.

01:22:57.360 --> 01:23:01.360
So let me use the pencil to illustrate what this is.

01:23:01.360 --> 01:23:06.360
This is the marginals, okay?

01:23:06.360 --> 01:23:12.360
These are the marginals.

01:23:12.360 --> 01:23:22.360
This is the conditional, okay?

01:23:22.360 --> 01:23:30.360
This is what we're interested in.

01:23:30.360 --> 01:23:32.360
Are we okay with that?

01:23:32.360 --> 01:23:37.360
And what we're interested in is also a conditional, just to be clear.

01:23:37.360 --> 01:23:42.360
We're interested in the red distribution.

01:23:42.360 --> 01:23:50.360
However, we also need, in order to know the red distribution that's up here, okay?

01:23:50.360 --> 01:23:56.360
In order to know that distribution, we actually need to know this distribution that I'm highlighting,

01:23:56.360 --> 01:24:05.360
the probability of x1 given x2, but integrated or summed over all possible values of x2,

01:24:05.360 --> 01:24:10.360
which therefore, let me switch mics,

01:24:10.360 --> 01:24:15.360
which therefore means we're doing some form of summing over integration

01:24:15.360 --> 01:24:21.360
over all horizontal slices of our data, okay?

01:24:21.360 --> 01:24:27.360
So just to make this little clear, we're saying this top term up here

01:24:27.360 --> 01:24:35.360
is basically this slice plus this slice plus this slice plus this slice

01:24:35.360 --> 01:24:42.360
all multiplied together, okay?

01:24:42.360 --> 01:24:45.360
That's what we're doing, and that's how that formulation, Bayes' Rule,

01:24:45.360 --> 01:25:01.360
relates to this picture that we've drawn for conditional, marginal, and joint probability, right?

01:25:01.360 --> 01:25:10.360
Okay, so I'd like to have you all open up notebook number three.

01:25:10.360 --> 01:25:17.360
So we're going to do probabilistic programming, and we're going to start with the coin flip story, right?

01:25:17.360 --> 01:25:25.360
The coin flip story is way too classic, but it's really one of those anchoring examples in my mind, all right?

01:25:25.360 --> 01:25:31.360
So if you master the complexities that we can build on top of it for the coin flip story,

01:25:31.360 --> 01:25:40.360
then you'll grasp a lot of concepts that are usable across multiple different types of models.

01:25:40.360 --> 01:25:46.360
Okay, so we're going to do estimation, like I mentioned,

01:25:46.360 --> 01:25:52.360
one of the core activities of statistical inference is estimation of the parameter given data.

01:25:52.360 --> 01:25:55.360
Notice the given, right? There's a conditional,

01:25:55.360 --> 01:25:58.360
so we're jointly modeling our data and our parameters together,

01:25:58.360 --> 01:26:01.360
and we're saying, given that we've observed data now,

01:26:01.360 --> 01:26:06.360
what's the distribution of parameters that we've got, okay?

01:26:06.360 --> 01:26:12.360
So go ahead, run that first cell.

01:26:12.360 --> 01:26:16.360
The first thing that we're going to do is we're going to actually look at some,

01:26:16.360 --> 01:26:20.360
we're going to look at click-through rates, again, the classic binomial thing,

01:26:20.360 --> 01:26:23.360
rather than coin flips, coin flips are a little too boring.

01:26:23.360 --> 01:26:27.360
So let's look at click-through rates by loading this cell.

01:26:27.360 --> 01:26:33.360
Ooh, I lost my kernel again.

01:26:33.360 --> 01:26:36.360
You know what, if you all don't mind, I'm going to switch over to the instructor notebook

01:26:36.360 --> 01:26:39.360
and not code along, but I'll make sure that you all have enough time,

01:26:39.360 --> 01:26:44.360
because I have all the write outputs in the instructor notebooks.

01:26:45.360 --> 01:26:56.360
Yes, notebook three.

01:26:56.360 --> 01:27:00.360
So last night I posted on our Slack, do a new Git pull.

01:27:00.360 --> 01:27:03.360
So make sure you, if you haven't done that, do a Git pull.

01:27:03.360 --> 01:27:06.360
So that should hopefully clear up the confusion.

01:27:06.360 --> 01:27:09.360
By the way, for the tutorials, get up on the Slack,

01:27:09.360 --> 01:27:16.360
because it's a useful channel for the instructors to one way communicate information

01:27:16.360 --> 01:27:21.360
and the other way get back questions.

01:27:21.360 --> 01:27:25.360
Okay, so let's look at the click-through rates data.

01:27:25.360 --> 01:27:30.360
You all have that loaded.

01:27:30.360 --> 01:27:36.360
Oh, good, my thing's working now.

01:27:36.360 --> 01:27:39.360
You should get data that looks something like this.

01:27:39.360 --> 01:27:42.360
So we've done this test.

01:27:42.360 --> 01:27:45.360
We have a case and a control.

01:27:45.360 --> 01:27:51.360
And we've measured for every single visitor to our website,

01:27:51.360 --> 01:27:56.360
whether they clicked on the button within a fixed period of time

01:27:56.360 --> 01:27:59.360
or whether they just decided to leave.

01:27:59.360 --> 01:28:05.360
Okay, so how then do we use PMC syntax to build a model

01:28:05.360 --> 01:28:14.360
that helps us estimate the true value of P with its uncertainty for this data?

01:28:14.360 --> 01:28:18.360
So if you didn't have PMC three, what might you do?

01:28:18.360 --> 01:28:22.360
You might do a data frame dot group by, right?

01:28:22.360 --> 01:28:34.360
So you might do R dot group by group dot mean.

01:28:34.360 --> 01:28:35.360
Something like that.

01:28:35.360 --> 01:28:36.360
Oh, well, okay.

01:28:36.360 --> 01:28:40.360
Yeah, I need to run the cell.

01:28:40.360 --> 01:28:41.360
You might do something like that.

01:28:41.360 --> 01:28:46.360
And if someone runs that code, what number do you get?

01:28:46.360 --> 01:28:49.360
While mine's running, it has to connect to the kernel.

01:28:49.360 --> 01:28:52.360
Can someone do that?

01:28:52.360 --> 01:28:55.360
Oh, CTR, my bad.

01:28:55.360 --> 01:28:56.360
Click through rate.

01:28:56.360 --> 01:28:59.360
Here, you'll get something that looks like this.

01:28:59.360 --> 01:29:06.360
You'll get like a 0.14050 for one group and 0.19125.

01:29:06.360 --> 01:29:08.360
Click through rate for the other group, right?

01:29:08.360 --> 01:29:14.360
And how much would you believe that data?

01:29:14.360 --> 01:29:15.360
Maybe true.

01:29:15.360 --> 01:29:16.360
Maybe not.

01:29:16.360 --> 01:29:23.360
It depends on how our data were split between the control and the test group, right?

01:29:23.360 --> 01:29:29.360
Usually, we do a random splitting, but in this case, some malfunction happens.

01:29:29.360 --> 01:29:35.360
So we only had really 200 data points out of 1,000 for one of the groups and 800 for the other.

01:29:35.360 --> 01:29:37.360
So we don't have 50-50 splits.

01:29:37.360 --> 01:29:39.360
We have 80-20 splits.

01:29:39.360 --> 01:29:44.360
One of the groups is going to be smaller.

01:29:44.360 --> 01:29:47.360
Oh, it's actually 2,800, so it's even worse.

01:29:47.360 --> 01:29:51.360
It's not even one of those nice round numbers that we can think about.

01:29:51.360 --> 01:29:53.360
So, cool.

01:29:53.360 --> 01:29:56.360
So we've got this skewed amount of data.

01:29:56.360 --> 01:30:04.360
Which number do you believe in more, given that you know that the test group only has 800?

01:30:04.360 --> 01:30:07.360
You believe the control number more, right?

01:30:07.360 --> 01:30:11.360
And that's because we've got more measurements for them.

01:30:11.360 --> 01:30:12.360
All right.

01:30:12.360 --> 01:30:23.360
Well, what we're going to do is we're going to spend a bit of time thinking about what the data-generating process looks like for this kind of model.

01:30:23.360 --> 01:30:33.360
So let that come up.

01:30:33.360 --> 01:30:45.360
What does the data-generating process look like for a Bernoulli or a binomially distributed data?

01:30:45.360 --> 01:30:52.360
This, by the way, is the exact workflow by which I go about every single problem.

01:30:52.360 --> 01:30:55.360
So we've got the question, what is the data-generating process?

01:30:55.360 --> 01:30:58.360
Well, we start with the data that we have on hand.

01:30:58.360 --> 01:31:02.360
We have Bernoulli distributed data.

01:31:02.360 --> 01:31:22.360
So we'll say the likelihood follows a Bernoulli distribution.

01:31:22.360 --> 01:31:26.360
Okay?

01:31:26.360 --> 01:31:30.360
We're going to just estimate for the control group.

01:31:30.360 --> 01:31:33.360
We're not going to do it with two groups just yet.

01:31:33.360 --> 01:31:35.360
We'll build it for one.

01:31:35.360 --> 01:31:49.360
So this parameter P, however, how is this distributed?

01:31:49.360 --> 01:31:54.360
Okay, so we got the value P.

01:31:54.360 --> 01:31:57.360
How would we model that?

01:31:57.360 --> 01:32:09.360
Well, if you've never seen click-through rate data before, how would you assign credibility points to the number line to correctly model P?

01:32:09.360 --> 01:32:13.360
Talk with your neighbor for a minute or two.

01:32:13.360 --> 01:32:16.360
This time I mean it, like really talk with your neighbor.

01:32:16.360 --> 01:32:20.360
I hear some things crystallizing.

01:32:20.360 --> 01:32:24.360
Do we have volunteers?

01:32:24.360 --> 01:32:37.360
What might you believe about, how would you assign credibility points to the values that P can take on having never seen click-through rate data?

01:32:37.360 --> 01:32:39.360
Uniform what?

01:32:39.360 --> 01:32:40.360
Zero to one.

01:32:40.360 --> 01:32:41.360
So let's try that.

01:32:41.360 --> 01:32:55.360
We'll say then that P is distributed zero to one uniformly.

01:32:55.360 --> 01:32:57.360
Okay?

01:32:57.360 --> 01:33:02.360
This being the likelihood, down, oops.

01:33:02.360 --> 01:33:10.360
This being the likelihood means we're actually going to, this, sorry, sorry, let me backtrack a little bit.

01:33:10.360 --> 01:33:18.360
What we've just drawn here on the screen is one generative model for our data.

01:33:18.360 --> 01:33:26.360
One generative model out of many possible models that we might want to build.

01:33:26.360 --> 01:33:32.360
This thing that I've highlighted being the likelihood is the thing that we have observed.

01:33:32.360 --> 01:33:39.360
Okay?

01:33:39.360 --> 01:33:42.360
Zero.

01:33:42.360 --> 01:33:44.360
I wish Raveen was here.

01:33:44.360 --> 01:33:46.360
I would have him copy this model onto the whiteboard.

01:33:46.360 --> 01:33:48.360
Give me one moment.

01:34:09.360 --> 01:34:24.360
Okay.

01:34:24.360 --> 01:34:26.360
So we got the model on the whiteboard.

01:34:26.360 --> 01:34:33.360
I'm going to switch back and we're going to see how we can actually build that model with PIMC code really easily.

01:34:33.360 --> 01:34:35.360
Okay?

01:34:35.360 --> 01:34:44.360
So code along, I believe this is code along.

01:34:44.360 --> 01:34:50.360
Since we're only doing the estimation for the control group, we're not going to worry about the test group just yet.

01:34:50.360 --> 01:34:52.360
How do we write this model?

01:34:52.360 --> 01:34:56.360
Well, we can write it this way.

01:34:56.360 --> 01:34:59.360
We've got uniform.

01:34:59.360 --> 01:35:01.360
Right?

01:35:01.360 --> 01:35:05.360
Kind of looks very, very close to the picture we drew.

01:35:05.360 --> 01:35:08.360
We'll call this variable P.

01:35:08.360 --> 01:35:10.360
It needs to have an explicit name.

01:35:10.360 --> 01:35:16.360
So rule of thumb is whatever you name it in your variable, as the Python variable, just give it the exact same name.

01:35:16.360 --> 01:35:21.360
Such that PIMC can recognize this.

01:35:21.360 --> 01:35:22.360
Lower is zero.

01:35:22.360 --> 01:35:25.360
Upper is one.

01:35:25.360 --> 01:35:28.360
Okay?

01:35:28.360 --> 01:35:38.360
And then the likelihood would be PIM.Bernoulli.

01:35:38.360 --> 01:35:45.360
We'll name it likelihood, like P is equal to P.

01:35:45.360 --> 01:35:48.360
A Bernoulli distribution only has one parameter.

01:35:48.360 --> 01:35:50.360
It takes only one parameter in.

01:35:50.360 --> 01:35:51.360
It's called P.

01:35:51.360 --> 01:35:52.360
What is it?

01:35:52.360 --> 01:35:56.360
It's distributed uniformly, having not seen the data.

01:35:56.360 --> 01:35:58.360
Okay?

01:35:58.360 --> 01:35:59.360
How are we doing?

01:35:59.360 --> 01:36:02.360
Okay, so far, syntax, mechanics.

01:36:02.360 --> 01:36:11.360
And then what we do next is we say observed is inside our data frame.

01:36:11.360 --> 01:36:22.360
Control-DF clicks.

01:36:22.360 --> 01:36:24.360
Okay?

01:36:24.360 --> 01:36:33.360
So that means we've observed a sequence of ones and zeros, whether the user has clicked or not.

01:36:33.360 --> 01:36:37.360
That is the data that we have observed for the Bernoulli distribution.

01:36:37.360 --> 01:36:38.360
Okay?

01:36:38.360 --> 01:36:40.360
So this is one way of writing that model.

01:36:40.360 --> 01:36:44.360
I'm going to give you all, how are we doing with this syntax so far?

01:36:44.360 --> 01:36:47.360
This is the mechanics part of building a model.

01:36:47.360 --> 01:36:58.360
That said, if you think back to what we drew on the whiteboard just now, the syntax looks very similar, right?

01:36:58.360 --> 01:37:08.360
The syntax here looks very similar to the syntax, sorry, the syntax in code looks very similar to the syntax, the thing that we drew on the whiteboard.

01:37:08.360 --> 01:37:10.360
The pictures are much easier to reason about, right?

01:37:10.360 --> 01:37:16.360
We can draw our data generative process here and directly translate it into code.

01:37:16.360 --> 01:37:20.360
So that's one way of observing it.

01:37:20.360 --> 01:37:23.360
There is another way, another formulation, right?

01:37:23.360 --> 01:37:33.360
And if you remember what I said just now, the distribution, sorry, if you remember what I said just now, a sequence of Bernoulli's is binomially distributed,

01:37:33.360 --> 01:37:39.360
which what that means then is we can actually write the model as a binomial likelihood,

01:37:39.360 --> 01:37:45.360
but we'll have to change a little bit about how we do, how we structure the data to be input, okay?

01:37:45.360 --> 01:37:50.360
But I'm going to just give you the binomial model as is,

01:37:50.360 --> 01:38:00.360
and you can try to break the model or break how we structure the data so you get a better feel on how things should work.

01:38:00.360 --> 01:38:04.360
Let's run that cell.

01:38:04.360 --> 01:38:16.360
The next thing that we do, having written our model and telling PMC what we're conditioning on, is we say within this model context,

01:38:16.360 --> 01:38:26.360
sorry, Model 1 Bernoulli, please sample from the posterior 2,000 times.

01:38:26.360 --> 01:38:34.360
Give me 2,000 draws that describe how we expect the parameter P to look like.

01:38:34.360 --> 01:38:39.360
It's a simulation, MC-based, Monte Carlo-based simulation of what the posterior will look like.

01:38:39.360 --> 01:38:44.360
Now, this is kind of unnecessary for a simple coin flip model,

01:38:44.360 --> 01:38:53.360
but when we go to slightly more complicated hierarchical versions, you'll see that this comes in really handy, okay?

01:38:53.360 --> 01:39:00.360
Run that cell and do the same for the binomial.

01:39:00.360 --> 01:39:02.360
It's literally PM.sample.

01:39:02.360 --> 01:39:05.360
Literally, that's all you need to do.

01:39:05.360 --> 01:39:09.360
What's happening here? Fancy math is happening for lazy programmers.

01:39:09.360 --> 01:39:11.360
That's been the motto of Pi MC3.

01:39:11.360 --> 01:39:18.360
We abstract away the math, the fancy math that is MCMC sampling or variational inference

01:39:18.360 --> 01:39:21.360
and allow users to focus on building generative stories.

01:39:21.360 --> 01:39:30.360
Really, the place when doing PMC modeling, the place that we need to keep our focus on is in the place that I've highlighted up there in the cell.

01:39:30.360 --> 01:39:32.360
That is, what is the model definition?

01:39:32.360 --> 01:39:36.360
Is it Bernoulli? Is it binomial?

01:39:36.360 --> 01:39:40.360
I'd like you to run those two cells.

01:39:40.360 --> 01:39:47.360
If you run those two cells and you plot the posterior distribution,

01:39:47.360 --> 01:39:50.360
you'll get something that looks like this.

01:39:50.360 --> 01:39:53.360
How do you do the posterior plotting?

01:39:53.360 --> 01:40:01.360
You do RVs for which Ravine is one of the lead developers.

01:40:01.360 --> 01:40:16.360
It's a visualization tool for taking a look at the results of MC sampling, all right?

01:40:16.360 --> 01:40:25.360
It's kind to those who still haven't broken out of the histogram land.

01:40:25.360 --> 01:40:31.360
I need to run all cells above.

01:40:31.360 --> 01:40:34.360
I'd invite you to run that.

01:40:34.360 --> 01:40:36.360
You should get something that looks like this.

01:40:36.360 --> 01:40:39.360
It's happening on the left-hand side of the screen.

01:40:39.360 --> 01:40:49.360
You'll then get a posterior distribution, a view on what we believe about the value of P,

01:40:49.360 --> 01:41:04.360
having seen the data and having explicitly stated what we believe prior to seeing the data.

01:41:04.360 --> 01:41:07.360
For now, I'm not going to go into that.

01:41:07.360 --> 01:41:12.360
That is something slightly more advanced and that's the contents of Ravine's tutorial.

01:41:12.360 --> 01:41:17.360
If you're not attending it, then catch the tutorial online,

01:41:17.360 --> 01:41:24.360
because that's where they'll go into a little bit more of the theory behind the Monte Carlo sampling that goes on.

01:41:24.360 --> 01:41:26.360
For now, we're going to ignore those errors.

01:41:26.360 --> 01:41:30.360
What I want to give you is the mechanics of writing the model

01:41:30.360 --> 01:41:36.360
and the mechanics of sampling and the mechanics of visualizing and interpreting.

01:41:36.360 --> 01:41:48.360
Do the same for the binomial model.

01:41:48.360 --> 01:41:51.360
You should get something that looks like that.

01:41:51.360 --> 01:42:06.360
The values range from 0.125 to about 0.155, the 94% highest posterior density.

01:42:06.360 --> 01:42:14.360
All that says is that 94% of the credibility is assigned within this black bar region.

01:42:14.360 --> 01:42:17.360
That's what we believe is true.

01:42:17.360 --> 01:42:19.360
Question?

01:42:20.360 --> 01:42:22.360
I understand it.

01:42:22.360 --> 01:42:24.360
The area was the uniform.

01:42:24.360 --> 01:42:27.360
We showed all the data that we had.

01:42:27.360 --> 01:42:29.360
That was the data frame.

01:42:29.360 --> 01:42:36.360
And I am simply completely able to sample the data.

01:42:36.360 --> 01:42:40.360
Exactly that.

01:42:41.360 --> 01:42:47.360
What I'd like you to do then is you've basically got the template

01:42:47.360 --> 01:42:53.360
that you need to now replicate this for doing two groups within a new model.

01:42:53.360 --> 01:42:57.360
I'd like you to do is the hands-on activity below,

01:42:57.360 --> 01:43:04.360
in which you build a model that does both estimations,

01:43:04.360 --> 01:43:08.360
one for the control group and one for the test group,

01:43:08.360 --> 01:43:10.360
in about five to ten minutes,

01:43:10.360 --> 01:43:14.360
such that you get up to this point where you get this plot.

01:43:14.360 --> 01:43:16.360
Go ahead and do that.

01:43:16.360 --> 01:43:19.360
Question?

01:43:19.360 --> 01:43:20.360
Oh, okay.

01:43:20.360 --> 01:43:22.360
I'll come and address it.

01:43:39.360 --> 01:43:41.360
Yes.

01:43:41.360 --> 01:44:05.360
Good question.

01:44:05.360 --> 01:44:13.360
It happens when you're doing the sampling.

01:44:13.360 --> 01:44:18.360
It happens when you're doing the sampling.

01:44:18.360 --> 01:44:21.360
Ah, okay.

01:44:21.360 --> 01:44:27.360
Okay, okay, okay.

01:44:27.360 --> 01:44:28.360
Give me a moment.

01:44:28.360 --> 01:44:30.360
I'm blanking right now.

01:44:30.360 --> 01:44:39.360
All right.

01:44:39.360 --> 01:44:43.360
So I'm going to give this to you that it is the math is being executed

01:44:43.360 --> 01:44:45.360
when you hit PM.sample.

01:44:45.360 --> 01:44:49.360
But really what happens underneath the hood is we've computed a joint likelihood

01:44:49.360 --> 01:44:52.360
of all the parameters with the data.

01:44:52.360 --> 01:45:00.360
And then we use MC sampling to figure out what the typical set of the posterior is.

01:45:00.360 --> 01:45:04.360
And the goal here is to sample around the typical set.

01:45:04.360 --> 01:45:13.360
That is the typical range of values for each of the parameters that are involved.

01:45:13.360 --> 01:45:25.360
All right.

01:45:25.360 --> 01:45:27.360
Okay, so how many of you are done?

01:45:27.360 --> 01:45:29.360
Thumbs up.

01:45:29.360 --> 01:45:31.360
We've got a bunch of people who are still working at it.

01:45:31.360 --> 01:45:43.360
So if you're stuck, you should have something that looks like this.

01:45:43.360 --> 01:45:45.360
You can do it with the binomial.

01:45:45.360 --> 01:45:48.360
So I'm encouraging you to try it with the binomial rather than the Bernoulli.

01:45:48.360 --> 01:46:17.360
But if you want the Bernoulli, I'll code it live for you all.

01:46:17.360 --> 01:46:46.360
Okay.

01:46:46.360 --> 01:46:55.360
Oh, I see.

01:46:55.360 --> 01:46:57.360
I hear fans running.

01:46:57.360 --> 01:47:02.360
Someone's sampling real hard.

01:47:02.360 --> 01:47:07.360
Thank you.

01:47:07.360 --> 01:47:11.360
So if you're interested in what the Bernoulli formulation will look like, it'll look like this.

01:47:11.360 --> 01:47:14.360
The binomial formulation looks like that on the right-hand side.

01:47:14.360 --> 01:47:21.360
All right, okay.

01:47:21.360 --> 01:47:28.360
And then if we're going to sample from the posterior, once again fancy math happens for lazy programmers.

01:47:28.360 --> 01:47:50.360
All right.

01:47:50.360 --> 01:48:00.360
So if you do this final thing, which is a deterministic transform of your random variables,

01:48:00.360 --> 01:48:07.360
you can actually explicitly compute the posterior distribution of the difference of the two p's.

01:48:07.360 --> 01:48:14.360
And this would be akin to what you're trying to do if you were to do a t-test of sorts, right?

01:48:14.360 --> 01:48:18.360
Basically you're just comparing two means and asking how overlapping are they.

01:48:18.360 --> 01:48:20.360
Are they overlapping or not?

01:48:20.360 --> 01:48:21.360
A binary decision.

01:48:21.360 --> 01:48:25.360
Are they completely overlapping or partially overlapping or completely non-overlapping?

01:48:25.360 --> 01:48:27.360
Is one greater than the other, right?

01:48:27.360 --> 01:48:30.360
We're asking questions like this basically.

01:48:30.360 --> 01:48:32.360
So I'll show you how to do that.

01:48:32.360 --> 01:48:39.360
You can do pdiff is pm.deterministic.

01:48:39.360 --> 01:48:43.360
And it's nothing more than math on probability distributions.

01:48:43.360 --> 01:48:46.360
So we'll say ptest minus pcontrol.

01:48:46.360 --> 01:48:54.360
We'll define test minus control as the diff difference between the two and rerun that.

01:48:54.360 --> 01:49:03.360
And what we'll get is a posterior distribution on the difference of the two parameters.

01:49:04.360 --> 01:49:13.360
So I'm really tempted to ask this, but in a t-test would this be significant?

01:49:17.360 --> 01:49:21.360
So yeah, okay, sure.

01:49:21.360 --> 01:49:23.360
Now next question.

01:49:23.360 --> 01:49:26.360
Would the t-test be appropriate?

01:49:26.360 --> 01:49:28.360
Why?

01:49:34.360 --> 01:49:42.360
Well, there is an n if we're doing, so that it's not the right thing to do is the correct answer,

01:49:42.360 --> 01:49:45.360
but I disagree slightly with the reasoning.

01:49:45.360 --> 01:49:46.360
We do have multiple n's.

01:49:46.360 --> 01:49:49.360
We have multiple observations here.

01:49:49.360 --> 01:49:53.360
So that's accounted for.

01:49:53.360 --> 01:50:00.360
Let me ask you about the, back there.

01:50:00.360 --> 01:50:04.360
Maybe power calculations are one thing that I had a long Twitter thread on.

01:50:04.360 --> 01:50:08.360
Thankfully it's not a rant, so you can read it.

01:50:08.360 --> 01:50:12.360
I'll post that.

01:50:12.360 --> 01:50:15.360
Yeah, so let's see.

01:50:15.360 --> 01:50:20.360
Can a probability be normally distributed?

01:50:20.360 --> 01:50:24.360
No, why?

01:50:24.360 --> 01:50:26.360
Right, right, exactly.

01:50:26.360 --> 01:50:31.360
Now we can approximate it with a normal distribution, but we have to be extremely clear.

01:50:31.360 --> 01:50:36.360
That's an approximation of what already is an approximation.

01:50:36.360 --> 01:50:38.360
A model is an approximation of the world.

01:50:38.360 --> 01:50:40.360
We're putting another approximation on top.

01:50:40.360 --> 01:50:42.360
Whoa, okay.

01:50:42.360 --> 01:50:49.360
All right, so this is where knowledge of the shapes and the support of the distributions comes in handy.

01:50:49.360 --> 01:50:51.360
P is a probability.

01:50:51.360 --> 01:50:55.360
It can never take any value below zero or above one.

01:50:55.360 --> 01:51:02.360
So why would you impose a normal distribution on the probability parameter p?

01:51:02.360 --> 01:51:06.360
Especially now, we don't need to do any math.

01:51:06.360 --> 01:51:12.360
We don't need to write equations out and solve these equations that tell us what the posterior will look like

01:51:12.360 --> 01:51:14.360
having seen data under a normal approximation.

01:51:14.360 --> 01:51:15.360
We don't have to.

01:51:15.360 --> 01:51:19.360
We can explicitly sample from the posterior using MC simulation.

01:51:19.360 --> 01:51:27.360
So why not go whole hog and just use probability distributions that are bounded from zero to one?

01:51:27.360 --> 01:51:28.360
Right?

01:51:28.360 --> 01:51:30.360
Okay, cool, great.

01:51:30.360 --> 01:51:34.360
So you all just had my, I did a talk at PyCon.

01:51:34.360 --> 01:51:40.360
It was my 25 minute rant on why we don't always do the t-test,

01:51:40.360 --> 01:51:43.360
why we should go away from canned statistical procedures.

01:51:43.360 --> 01:51:45.360
This is one example of it.

01:51:45.360 --> 01:51:48.360
P is not normally distributed.

01:51:48.360 --> 01:51:52.360
We might look normally distributed, but it is definitely not normally distributed.

01:51:52.360 --> 01:51:53.360
It's got to be bound.

01:51:53.360 --> 01:51:59.360
So you can't take on a probability distribution that is bound from negative infinity to positive infinity.

01:51:59.360 --> 01:52:01.360
If you do that, you're wrong.

01:52:01.360 --> 01:52:03.360
Okay?

01:52:03.360 --> 01:52:05.360
Cool, great.

01:52:05.360 --> 01:52:14.360
And on this hypothesis testing thing, think about, there's a great blog post by Alan Downey, again.

01:52:14.360 --> 01:52:22.360
That talks about the fact that every single classical statistical test boils down to one framework.

01:52:22.360 --> 01:52:27.360
And if you go Bayesian, there's really no reason to calculate p-values and the likes.

01:52:27.360 --> 01:52:32.360
You just look at posteriors and look at the posterior distributions of statistics,

01:52:32.360 --> 01:52:35.360
single-valued statistics, single distributed, sorry,

01:52:35.360 --> 01:52:37.360
single-variate statistics that you're interested in.

01:52:37.360 --> 01:52:40.360
You don't have to worry about p-values here.

01:52:40.360 --> 01:52:41.360
Okay?

01:52:41.360 --> 01:52:43.360
Are we all right with that?

01:52:43.360 --> 01:52:46.360
Cool, cool, cool, cool, great.

01:52:46.360 --> 01:52:53.360
Now, knowing that the probability difference is,

01:52:53.360 --> 01:52:57.360
knowing this probability difference is all good and useful,

01:52:57.360 --> 01:53:08.360
but it's still not tied to something that is real world and interpretable in the minds of our executive friends, right?

01:53:08.360 --> 01:53:12.360
So how do we take this metric that we've calculated?

01:53:12.360 --> 01:53:16.360
p-diff and turn that into something that matters.

01:53:16.360 --> 01:53:21.360
Well, one thing that you might learn from Raveen's tutorial if you're going or watching it later

01:53:21.360 --> 01:53:29.360
is that there is this idea of a loss function or a cost function that we can attach to these things of interest.

01:53:29.360 --> 01:53:32.360
And I'm going to just show you a very simple example, okay?

01:53:32.360 --> 01:53:34.360
Let's say we know one thing.

01:53:34.360 --> 01:53:44.360
We've computed with other data and said that our customers on average spend 25 U.S. dollars if they click

01:53:44.360 --> 01:53:47.360
and zero U.S. dollars if they don't click, right?

01:53:47.360 --> 01:53:51.360
Like, there's money attached to this process.

01:53:51.360 --> 01:53:55.360
How that money is attached to this process, we can always write an equation that describes ours.

01:53:55.360 --> 01:53:59.360
We'll write one arbitrary one, so we don't have to go too deep into what it is.

01:53:59.360 --> 01:54:06.360
But if you take this, if you look at what's in the trace for p-diff,

01:54:06.360 --> 01:54:13.360
you'll notice it's nothing more than a sequence of 2,000 draws from that posterior, okay?

01:54:13.360 --> 01:54:16.360
So run that on your own notebooks as well.

01:54:16.360 --> 01:54:18.360
Just open up and view what p-diff is.

01:54:18.360 --> 01:54:21.360
It's a NumPy array. It's got 2,000 numbers, okay?

01:54:21.360 --> 01:54:30.360
So a difference in probability can translate into a difference in amount of money being spent.

01:54:30.360 --> 01:54:36.360
And if we attach a single, you know, a single dollar value to each and every one of those,

01:54:36.360 --> 01:54:39.360
rather than a distribution, that's not complicated for the moment.

01:54:39.360 --> 01:54:51.360
We can do something like dollar distribution is trace of p-diff times 25 times 1 million, right?

01:54:51.360 --> 01:54:57.360
Over a million customers, there's an increase in the probability that they spend money,

01:54:57.360 --> 01:54:59.360
so under the test group.

01:54:59.360 --> 01:55:03.360
And so how much increase do we expect over a million customers?

01:55:03.360 --> 01:55:05.360
Let's translate into some real numbers.

01:55:10.360 --> 01:55:11.360
We'll do that.

01:55:11.360 --> 01:55:20.360
And finally, you should get something like that.

01:55:20.360 --> 01:55:22.360
You should get something like that.

01:55:22.360 --> 01:55:24.360
So what does this say?

01:55:24.360 --> 01:55:29.360
Well, on average, we expect a lot of revenue increase,

01:55:29.360 --> 01:55:34.360
but there is always this very, very small tail probability that we still might lose money.

01:55:35.360 --> 01:55:41.360
And that's just the nature of our computation, okay?

01:55:41.360 --> 01:55:43.360
So I just wanted to put that out there.

01:55:43.360 --> 01:55:47.360
You can always tack on, this is highly custom per problem,

01:55:47.360 --> 01:55:51.360
but you can always tack on a loss function or a cost function that describes,

01:55:51.360 --> 01:55:55.360
that ties the metric of interest, you have to think about it,

01:55:55.360 --> 01:56:03.360
to some dollar amount or some hours spent by people on a particular problem, okay?

01:56:03.360 --> 01:56:09.360
And that's a way of communicating to so-called non-technical folks

01:56:09.360 --> 01:56:16.360
who want to be maybe a little bit more spoon-fed on what we expect to see, okay?

01:56:16.360 --> 01:56:18.360
Are we okay with this so far?

01:56:18.360 --> 01:56:21.360
So far, so good.

01:56:21.360 --> 01:56:24.360
Any questions?

01:56:24.360 --> 01:56:27.360
Okay, if there are no questions, I want you to talk with your neighbor for one minute,

01:56:27.360 --> 01:56:29.360
one new thing you learned.

01:56:29.360 --> 01:56:31.360
Anybody want to share?

01:56:31.360 --> 01:56:34.360
Who knew they learned? That's far.

01:56:44.360 --> 01:56:46.360
Yep, yep, that's right.

01:56:46.360 --> 01:56:48.360
That's the point of this first exercise.

01:56:48.360 --> 01:56:51.360
Get you familiar with that mechanics of doing so.

01:56:51.360 --> 01:56:53.360
Anything else?

01:56:55.360 --> 01:56:57.360
Any, back there?

01:57:02.360 --> 01:57:09.360
And that's the default sampler.

01:57:22.360 --> 01:57:25.360
Yep, yep, absolutely, absolutely.

01:57:32.360 --> 01:57:34.360
Yep.

01:57:37.360 --> 01:57:39.360
How so?

01:57:42.360 --> 01:57:44.360
Yep.

01:57:46.360 --> 01:57:49.360
Oh gosh, so, yeah.

01:57:52.360 --> 01:57:54.360
Yep.

01:57:54.360 --> 01:57:56.360
Oh gosh.

01:57:57.360 --> 01:57:59.360
Yep, and that's...

01:58:02.360 --> 01:58:04.360
Right, right, absolutely.

01:58:04.360 --> 01:58:09.360
And so, that actually brings up a very highly related point.

01:58:09.360 --> 01:58:12.360
We built statistical models of the world.

01:58:12.360 --> 01:58:14.360
We built statistical models of the world.

01:58:14.360 --> 01:58:20.360
When I built the cost function, it was both somewhat of a hybrid of a statistical model

01:58:20.360 --> 01:58:22.360
and a mechanistic model of the world.

01:58:22.360 --> 01:58:28.360
That is, people who click will now spend money on average $25 per click,

01:58:28.360 --> 01:58:32.360
and so, we do some multiplication and that expresses some mechanism.

01:58:32.360 --> 01:58:35.360
But ultimately, they're models, and we have to validate them.

01:58:35.360 --> 01:58:39.360
And so, that problem you brought up is, I think, one of the problems

01:58:39.360 --> 01:58:42.360
that we don't know how to validate properly, I think,

01:58:42.360 --> 01:58:47.360
because we don't have the negative data to build a good model.

01:58:47.360 --> 01:58:50.360
We have all the positives, the successful molecules.

01:58:50.360 --> 01:58:54.360
We don't know what the opportunity cost and how to model the opportunity cost

01:58:54.360 --> 01:59:00.360
of those failed, you know, incorrectly modeled molecules would be.

01:59:00.360 --> 01:59:03.360
Yeah, that's a very good point.

01:59:03.360 --> 01:59:05.360
Anything else?

01:59:05.360 --> 01:59:07.360
Okay.

01:59:07.360 --> 01:59:10.360
If there are no other points, you'll notice...

01:59:10.360 --> 01:59:12.360
All right, we have the case and the control group.

01:59:12.360 --> 01:59:15.360
If you listen to many of my rants,

01:59:15.360 --> 01:59:18.360
case and control isn't the only thing you can do.

01:59:18.360 --> 01:59:21.360
You really can do, like, arbitrary number of groups

01:59:21.360 --> 01:59:25.360
that are having to worry much about multiple hypothesis correction

01:59:25.360 --> 01:59:29.360
and the likes and, like, you know, having this guillotine of p-values

01:59:29.360 --> 01:59:31.360
which you chop off as you go down.

01:59:31.360 --> 01:59:35.360
Oh, gosh, that's, like, I don't know how people came up with that,

01:59:35.360 --> 01:59:38.360
but it's complicated, right?

01:59:38.360 --> 01:59:43.360
Whereas, like, just looking at posteriors is so much more clean,

01:59:43.360 --> 01:59:45.360
much easier to interpret as well.

01:59:45.360 --> 01:59:50.360
So, we're going to do another example that's still the binomial story,

01:59:50.360 --> 01:59:53.360
still the Bernoulli binomial story,

01:59:53.360 --> 01:59:59.360
but it's going to show you how, like, we can't just copy-past a test control

01:59:59.360 --> 02:00:04.360
player 1, player 2, player 3, player 4, player 857, right,

02:00:04.360 --> 02:00:07.360
or write a for loop with functions, and that's just, like, way too much.

02:00:07.360 --> 02:00:10.360
We can actually take advantage of some syntactic things

02:00:10.360 --> 02:00:14.360
that allow us to write in a very concise fashion

02:00:14.360 --> 02:00:18.360
these models that model multiple, you know, more than two groups, okay?

02:00:19.360 --> 02:00:24.360
So, like y'all to scroll down, we're going to look at baseball data,

02:00:24.360 --> 02:00:28.360
and this is one of the classic, classic,

02:00:28.360 --> 02:00:32.360
this is one of those classic data sets that one would pick up, right?

02:00:32.360 --> 02:00:36.360
So, we want to, we want to model the probability

02:00:36.360 --> 02:00:40.360
that a player who is a batter will hit a pitch, right?

02:00:40.360 --> 02:00:43.360
So, they have this stat called at bats and number of hits.

02:00:43.360 --> 02:00:47.360
At bats is the n, the total number of times

02:00:47.360 --> 02:00:49.360
that they've come up for batting,

02:00:49.360 --> 02:00:52.360
and h hits is the total number of times

02:00:52.360 --> 02:00:54.360
that they've actually hit the bat.

02:00:54.360 --> 02:00:57.360
So, we've got some data from the baseball database,

02:00:57.360 --> 02:00:59.360
all credit to them.

02:00:59.360 --> 02:01:03.360
I'd like you to run this cell that loads the data.

02:01:03.360 --> 02:01:07.360
Little pitch for a tool that I've been developing

02:01:07.360 --> 02:01:10.360
alongside colleagues here, so Zach in the back,

02:01:10.360 --> 02:01:12.360
he also works on this.

02:01:12.360 --> 02:01:15.360
It's called Pyjanitor, and it's basically there to help you

02:01:15.360 --> 02:01:18.360
make data preprocessing easy to read,

02:01:18.360 --> 02:01:22.360
so that you have a clean API for cleaning data, all right?

02:01:22.360 --> 02:01:26.360
So, we've got the data, I'd like you to load that.

02:01:26.360 --> 02:01:30.360
You should see something that looks like this, okay?

02:01:30.360 --> 02:01:34.360
We've got at bats, hits, the salary of the player,

02:01:34.360 --> 02:01:37.360
and this extra column which we will use,

02:01:37.360 --> 02:01:41.360
it's called player ID underscore encoded, okay?

02:01:42.360 --> 02:01:46.360
It's basically just an integer encoding of the player ID,

02:01:46.360 --> 02:01:48.360
nothing more than that.

02:01:48.360 --> 02:01:51.360
You'll find out why that becomes handy later.

02:01:51.360 --> 02:01:58.360
So, once again, it's the same old Bernoulli binomial sorry.

02:01:58.360 --> 02:02:03.360
Now, because we have the data structured as at bats and hits,

02:02:03.360 --> 02:02:06.360
which is the, what should we,

02:02:06.360 --> 02:02:09.360
what is the likelihood that we want then?

02:02:11.360 --> 02:02:14.360
Is it Bernoulli or binomial?

02:02:15.360 --> 02:02:17.360
Pardon me?

02:02:19.360 --> 02:02:22.360
Binomial, and why is it binomial?

02:02:22.360 --> 02:02:25.360
Yes, because we know exactly how many times.

02:02:25.360 --> 02:02:27.360
We're not recording every single bat.

02:02:27.360 --> 02:02:30.360
We're summarizing, we're taking the summary statistics,

02:02:30.360 --> 02:02:33.360
which is the total number of at bats and the total number of hits,

02:02:33.360 --> 02:02:35.360
and this then forms the binomial story

02:02:35.360 --> 02:02:39.360
in which we know the number of times something has come up for trial.

02:02:39.360 --> 02:02:42.360
Something has come up for a test of whether they succeed or not, okay?

02:02:42.360 --> 02:02:44.360
This is distinct from the Bernoulli,

02:02:44.360 --> 02:02:46.360
where we only know that they're coming up,

02:02:46.360 --> 02:02:49.360
and we just, we only know that they're coming up

02:02:49.360 --> 02:02:53.360
for a success failure trial, and we know the probability,

02:02:53.360 --> 02:02:56.360
but we don't have the number of times that they've got that, okay?

02:02:56.360 --> 02:03:00.360
So, we'll be using a binomial distribution as a,

02:03:00.360 --> 02:03:03.360
we'll be using a binomial distribution as the likelihood.

02:03:03.360 --> 02:03:06.360
So, let's build this model,

02:03:06.360 --> 02:03:10.360
and what I want you to do is to notice some syntactic changes here, okay?

02:03:10.360 --> 02:03:17.360
So, first we'll have the pitch model.

02:03:17.360 --> 02:03:19.360
We're going to build that.

02:03:19.360 --> 02:03:25.360
I'm going to switch over now to a beta distribution, right?

02:03:25.360 --> 02:03:29.360
A beta distribution is also bounded from 0 to 1.

02:03:29.360 --> 02:03:31.360
It has the correct support.

02:03:31.360 --> 02:03:35.360
A beta distribution also allows, has this parameter where,

02:03:35.360 --> 02:03:39.360
two parameters that let us control the shape where it's skewed.

02:03:39.360 --> 02:03:41.360
Is it skewed left or is it,

02:03:41.360 --> 02:03:45.360
does it have more mass on the left or density on the right?

02:03:45.360 --> 02:03:48.360
It lets us do all, it lets us do that, right?

02:03:48.360 --> 02:03:50.360
It lets us control the shape of the distribution.

02:03:50.360 --> 02:03:54.360
So, it's not just a simple flat uniform prior.

02:03:54.360 --> 02:03:56.360
So, code along with that.

02:03:56.360 --> 02:03:58.360
P is the name of the thing.

02:03:58.360 --> 02:04:00.360
Alpha is 1, beta is 1.

02:04:01.360 --> 02:04:07.360
And then the shape of this, this is a new thing.

02:04:07.360 --> 02:04:16.360
The shape of this beta distribution is the number of players that we have.

02:04:16.360 --> 02:04:19.360
So, all we're expressing now is that we've got,

02:04:19.360 --> 02:04:23.360
instead of a single beta distribution that lives in memory,

02:04:23.360 --> 02:04:26.360
we've got a vector of beta distributions.

02:04:26.360 --> 02:04:29.360
One beta distribution per cell, right?

02:04:29.360 --> 02:04:33.360
And that beta distribution maps onto one particular player, okay?

02:04:33.360 --> 02:04:35.360
That's, that's what we're doing here.

02:04:35.360 --> 02:04:40.360
The likelihood is binomially distributed.

02:04:40.360 --> 02:04:47.360
So, PM.binomial, its name is like,

02:04:47.360 --> 02:04:53.360
its P is distributed according to the beta distribution,

02:04:53.360 --> 02:04:55.360
but it's a vector of distributions.

02:04:55.360 --> 02:05:00.360
So, what this effectively does is it creates a vector of binomials, okay?

02:05:00.360 --> 02:05:03.360
That's the key syntactic difference and conceptual difference

02:05:03.360 --> 02:05:05.360
that you need to take away from this example.

02:05:05.360 --> 02:05:07.360
You can get away with doing,

02:05:07.360 --> 02:05:14.360
you can get away from for loops by simply vectorizing everything, okay?

02:05:14.360 --> 02:05:18.360
N is data at bat, right?

02:05:18.360 --> 02:05:20.360
This is the number of times they've come up.

02:05:20.360 --> 02:05:26.360
And it's also a vector that is the same length of P.

02:05:26.360 --> 02:05:41.360
And finally, observed is data hits, okay?

02:05:41.360 --> 02:05:44.360
My line wrapping is coming into play.

02:05:44.360 --> 02:05:49.360
Maybe I shouldn't set this in the Jupyter Notebook.

02:05:49.360 --> 02:05:54.360
Now, we got that, but since we have salary information,

02:05:54.360 --> 02:05:57.360
we've been talking about like,

02:05:57.360 --> 02:06:02.360
we've been talking about tying things to real world numbers, right?

02:06:02.360 --> 02:06:08.360
Let's compute a metric that says the probability of batting per unit of salary, right?

02:06:08.360 --> 02:06:12.360
So, how, how we want that number to be as high as possible, right?

02:06:12.360 --> 02:06:15.360
So, we want for the smallest salary, someone,

02:06:15.360 --> 02:06:17.360
we want to figure out, for the smallest salary,

02:06:17.360 --> 02:06:21.360
someone who bats has the highest batting percentage.

02:06:21.360 --> 02:06:24.360
That would play right into the sabre metrics kind of thing, right?

02:06:24.360 --> 02:06:28.360
You're looking for value for money on different metrics.

02:06:28.360 --> 02:06:32.360
So, we'll do a deterministic transform.

02:06:32.360 --> 02:06:37.360
We'll call this P per salary, PPS, okay?

02:06:38.360 --> 02:06:45.360
And simply take P and divide it by the salary that we observe in the data.

02:06:45.360 --> 02:06:48.360
How are we doing? Any questions so far?

02:06:48.360 --> 02:06:49.360
So far, so good.

02:06:49.360 --> 02:06:55.360
Following along, story, we, it's, by the way, the exact same data generating process,

02:06:55.360 --> 02:07:02.360
the exact same story as we drew on the whiteboard.

02:07:02.360 --> 02:07:04.360
Go ahead, do sampling then.

02:07:04.360 --> 02:07:08.360
And look at, look at the posterior distributions.

02:07:21.360 --> 02:07:23.360
So, notice how we're sampling.

02:07:23.360 --> 02:07:25.360
It's fast.

02:07:25.360 --> 02:07:28.360
I'm taking advantage of the GPU that I have at home.

02:07:28.360 --> 02:07:33.360
This one in this particular case, I don't think it'll run any faster on the GPU than on,

02:07:33.360 --> 02:07:41.360
on a CPU because there's no complex matrix multiplies that are going on.

02:07:41.360 --> 02:07:44.360
But if you were to do like more complex things,

02:07:44.360 --> 02:07:46.360
complicated things like Bayesian neural net,

02:07:46.360 --> 02:07:49.360
which you actually can write in piano and IMC three,

02:07:49.360 --> 02:07:51.360
there's totally no problem with that.

02:07:51.360 --> 02:07:59.360
Then moving stuff onto the GPU can get you up to four to four to eight times faster sampling and fitting.

02:07:59.360 --> 02:08:05.360
Okay, so we have built here.

02:08:05.360 --> 02:08:08.360
Oh, I named it trace batting.

02:08:08.360 --> 02:08:18.360
Let me resample again.

02:08:18.360 --> 02:08:28.360
So what, what I've done then below is create this custom visualization that relies on IPI widgets and the likes to get it working.

02:08:28.360 --> 02:08:35.360
The intent here is that if you were to visualize posterior distributions for 805 players,

02:08:35.360 --> 02:08:39.360
that's 805 matplotlib axes that you're drawing to screen.

02:08:39.360 --> 02:08:41.360
It's not the prettiest thing.

02:08:41.360 --> 02:08:47.360
Okay, so we'll take advantage of some interactivity that we can build to.

02:08:47.360 --> 02:08:50.360
Oh, it's not working on my screens, but it should be working on some of yours, right?

02:08:50.360 --> 02:08:57.360
If you've got IPI widgets working, you'll get a select that you can actually look at and select multiple players.

02:08:57.360 --> 02:09:05.360
And compare their, their P per salary and their cumulative, the cumulative distributions for each of those.

02:09:05.360 --> 02:09:21.360
Okay, actually, while this is running, I'm just going to very quickly get up and running.

02:09:21.360 --> 02:09:32.360
And so while you have that visualization up, I'd like you to try to hunt for the player that's got the highest PPS distribution.

02:09:32.360 --> 02:09:41.360
And while I install some, while I reinstall some things, once, once I'm finished with the commands, we'll come back and talk about that.

02:09:51.360 --> 02:10:18.360
Okay, so anybody found interesting players?

02:10:18.360 --> 02:10:28.360
Yes.

02:10:28.360 --> 02:10:36.360
That's right. So one thing that we have as an idea in Bayesian statistics is that in the limit of lots of data,

02:10:36.360 --> 02:10:42.360
what your priors are really don't matter unless, unless you chose priors that can never change,

02:10:42.360 --> 02:10:49.360
which means you're just hard coding your great uncles, your uncles viewpoints on politics, which will never change, right?

02:10:49.360 --> 02:11:05.360
So, yeah.

02:11:05.360 --> 02:11:23.360
Maybe I just need to reload.

02:11:23.360 --> 02:11:30.360
My, my, my, my, it's very slow.

02:11:30.360 --> 02:11:39.360
So do we have players that are, that look interesting?

02:11:39.360 --> 02:11:42.360
Pardon me?

02:11:42.360 --> 02:11:44.360
Okay.

02:11:44.360 --> 02:11:46.360
Can you help me out a little bit?

02:11:46.360 --> 02:11:56.360
What does their CDF look like?

02:11:56.360 --> 02:11:57.360
Sure.

02:11:57.360 --> 02:12:04.360
Exponential being, sorry.

02:12:04.360 --> 02:12:07.360
Exponential being what?

02:12:07.360 --> 02:12:13.360
This, this, this, this line that looks like that.

02:12:13.360 --> 02:12:17.360
Okay, some look like this, right?

02:12:17.360 --> 02:12:19.360
You'll have noticed some of that.

02:12:19.360 --> 02:12:23.360
Any other patterns?

02:12:23.360 --> 02:12:30.360
Yeah, squiggly, but more or less straight line.

02:12:30.360 --> 02:12:34.360
Okay, let's talk about what each of these CDFs express.

02:12:34.360 --> 02:12:40.360
What's this one expressing?

02:12:40.360 --> 02:12:51.360
Yeah, essentially it's expressing that the credibility points are assigned anywhere from the lowest to the highest in pretty much a uniform distribution.

02:12:51.360 --> 02:12:58.360
What about something that looks like this?

02:12:58.360 --> 02:13:02.360
How tight is the distribution compared to the first one?

02:13:02.360 --> 02:13:04.360
Very tight.

02:13:04.360 --> 02:13:06.360
And it's also very shifted to the right, right?

02:13:06.360 --> 02:13:09.360
Because this, the central tendency is way out over there.

02:13:09.360 --> 02:13:10.360
Okay.

02:13:10.360 --> 02:13:12.360
And then what else do we have?

02:13:12.360 --> 02:13:14.360
We have this guy over here.

02:13:14.360 --> 02:13:19.360
What's this guy like?

02:13:19.360 --> 02:13:21.360
Something in the middle.

02:13:21.360 --> 02:13:22.360
Something in the middle.

02:13:22.360 --> 02:13:26.360
Still takes on lots of values, but the distribution is kind of skewed as well.

02:13:26.360 --> 02:13:27.360
Right?

02:13:27.360 --> 02:13:30.360
Because there's a, there's a long tail on the left.

02:13:30.360 --> 02:13:36.360
Lots of, lots of probability or lots of credibility assigned to the middle over here.

02:13:36.360 --> 02:13:38.360
And then it peters off at the top.

02:13:38.360 --> 02:13:48.360
It'll always peter off at the top.

02:13:48.360 --> 02:13:55.360
Once again, it's, it's the richness of statistical information.

02:13:55.360 --> 02:13:58.360
So PDFs look a lot like CDFs.

02:13:58.360 --> 02:14:00.360
Sorry, PDFs look a lot like histograms.

02:14:00.360 --> 02:14:02.360
I take that back.

02:14:02.360 --> 02:14:04.360
PDFs look a lot like histograms.

02:14:04.360 --> 02:14:15.360
And so you can't tell more than the bounds and central tendency from a PDF.

02:14:15.360 --> 02:14:23.360
Whereas you can tell quartiles and percent, percentiles and roughly estimate where they are from a CDF.

02:14:23.360 --> 02:14:37.360
So most of the time I would just default to using a CDF rather than a histogram or a PDF.

02:14:37.360 --> 02:14:38.360
Okay.

02:14:38.360 --> 02:14:40.360
Let's see if this works.

02:14:40.360 --> 02:14:42.360
My widget doesn't display.

02:14:42.360 --> 02:14:43.360
Never mind.

02:14:43.360 --> 02:14:45.360
I'll have the widget on your laptop.

02:14:45.360 --> 02:14:49.360
So you'll be able to view that.

02:14:49.360 --> 02:14:50.360
Okay.

02:14:50.360 --> 02:14:52.360
So there are some interesting players.

02:14:52.360 --> 02:15:01.360
One thing that's kind of interesting though is that we've got players like the uniform distribution one, right?

02:15:01.360 --> 02:15:03.360
Super uninformative.

02:15:03.360 --> 02:15:09.360
And that raises a problem.

02:15:09.360 --> 02:15:12.360
The problem sounds something like this.

02:15:12.360 --> 02:15:15.360
Now I'm going to pose this as a question to you all.

02:15:15.360 --> 02:15:18.360
So think about it.

02:15:18.360 --> 02:15:37.360
Having seen players, professional players play and do their thing, do you really expect that having a few at-bats,

02:15:37.360 --> 02:15:44.360
that comes from the situation where we have like one or two at-bats and zero or one hits.

02:15:44.360 --> 02:16:00.360
You really expect that having seen that one or two at-bats, we should still believe that their batting capability is at this near uniform from zero to one.

02:16:00.360 --> 02:16:02.360
Talk about that with your neighbor.

02:16:02.360 --> 02:16:07.360
And then tell me why.

02:16:07.360 --> 02:16:08.360
Yes.

02:16:08.360 --> 02:16:17.360
So do you really, should we really believe having seen one or two at-bats that performance would still range so wildly from zero to one basically?

02:16:17.360 --> 02:16:19.360
What are your answers?

02:16:19.360 --> 02:16:24.360
Should we really believe what we saw in the posterior results?

02:16:24.360 --> 02:16:31.360
That performance gets basically, is still uniformly or close to uniformly distributed?

02:16:31.360 --> 02:16:33.360
No, I see a head shaking.

02:16:33.360 --> 02:16:36.360
Tell me why.

02:16:36.360 --> 02:16:37.360
Both of you.

02:16:37.360 --> 02:16:53.360
Both of you are a team.

02:16:53.360 --> 02:16:57.360
Right, right.

02:16:58.360 --> 02:17:03.360
So what we're encoding here is this notion.

02:17:03.360 --> 02:17:07.360
So the response is pretty much how we would think about it, right?

02:17:07.360 --> 02:17:16.360
The professional baseball players don't tend to have performances that range so wildly.

02:17:16.360 --> 02:17:21.360
That is a piece of prior information that we can impose on the modeling problem.

02:17:21.360 --> 02:17:23.360
So how do we impose that?

02:17:24.360 --> 02:17:25.360
There are two ways.

02:17:25.360 --> 02:17:27.360
One, we can have a stronger prior.

02:17:27.360 --> 02:17:32.360
A stronger prior that is imposed on every single player.

02:17:32.360 --> 02:17:34.360
So we might do a beta.

02:17:34.360 --> 02:17:51.360
So if batting averages tend to fall within the range of 0.2 to 0.3, we might put a beta distribution with, so 0.2 is about approximately one success and four failures.

02:17:51.360 --> 02:17:54.360
So it would be a beta distribution of one and four.

02:17:54.360 --> 02:17:56.360
That is a slightly stronger prior.

02:17:56.360 --> 02:18:05.360
Or if we wanted it to be even stronger, we would do a beta distribution of 10 and 40, which is much narrower compared to the beta of one and four.

02:18:05.360 --> 02:18:08.360
If you don't believe me, go simulate it in NumPy.

02:18:08.360 --> 02:18:10.360
The beta distribution is right there.

02:18:10.360 --> 02:18:16.360
And if you really wanted to be a really strong prior, then it would be beta 100, 400.

02:18:16.360 --> 02:18:22.360
That is the 0.2 batting average kind of prior we could put on here.

02:18:26.360 --> 02:18:45.360
So the advantage of using the beta over the uniform is that I can now tweak the alpha and beta parameters of the beta distribution to change where we center the distribution and also change how wide or thin that distribution is.

02:18:46.360 --> 02:18:53.360
So as I was mentioning, beta 1, 4 looks something like this, skewed, but kind of wide.

02:18:53.360 --> 02:18:56.360
Beta 10, 40 looks something like this.

02:18:56.360 --> 02:19:00.360
And beta 100, 400 looks something like that.

02:19:00.360 --> 02:19:02.360
Are we okay with that?

02:19:02.360 --> 02:19:10.360
So beta distributions give us a little bit more control over the shape of the distribution.

02:19:10.360 --> 02:19:15.360
Okay, so putting tighter priors is one way.

02:19:15.360 --> 02:19:25.360
Another way to approach this is actually to impose what we would call a hyper prior, one that governs the population of players.

02:19:25.360 --> 02:19:28.360
So I'm going to switch over to drawing again.

02:19:38.360 --> 02:19:43.360
We have the binomial likelihood.

02:19:43.360 --> 02:19:45.360
And we know this already, right?

02:19:45.360 --> 02:19:58.360
This is a very familiar story for us by now.

02:19:58.360 --> 02:20:06.360
In the interest of saving space, I'm not going to draw out the distributions, but picture them in your head.

02:20:06.360 --> 02:20:10.360
N is known.

02:20:10.360 --> 02:20:21.360
P comes from another distribution.

02:20:21.360 --> 02:20:31.360
The way I want you to think about this though is because we vectorized everything.

02:20:31.360 --> 02:20:49.360
We've got a vector of binomial likelihoods and a corresponding vector of beta distributions for priors on the P parameter.

02:20:49.360 --> 02:21:04.360
If we were to do this hierarchically, what we are effectively doing is asking what is the population alpha and beta look like?

02:21:04.360 --> 02:21:12.360
That's kind of ugly.

02:21:13.360 --> 02:21:16.360
And we only have one of these each.

02:21:16.360 --> 02:21:21.360
We don't have a vector of them because they're governing the entire population.

02:21:21.360 --> 02:21:26.360
So when we do this hierarchical thing, we're effectively expressing this idea.

02:21:26.360 --> 02:21:31.360
Players themselves are drawn from a parental distribution.

02:21:31.360 --> 02:21:41.360
Professional players all generally follow some general distribution that is governing the performance of the individual players.

02:21:41.360 --> 02:21:57.360
So the population distribution, which is imposed on A and B, that governs the individual player's performance parameter, which is the beta distributed thing,

02:21:57.360 --> 02:22:03.360
which then influences the outcomes that we're interested in.

02:22:03.360 --> 02:22:14.360
So let me just annotate that.

02:22:14.360 --> 02:22:19.360
Up there we have the population parameters.

02:22:19.360 --> 02:22:26.360
We have the individual parameters for P.

02:22:26.360 --> 02:22:35.360
We have the likelihood which governs how our outcomes, the data that we observe, are generated.

02:22:35.360 --> 02:22:38.360
Okay? Are we okay so far?

02:22:38.360 --> 02:22:50.360
Now, I'm going to go on a limb and tell you that A and B, these two parameters, they also have distributions because then otherwise it wouldn't be probabilistic.

02:22:50.360 --> 02:22:56.360
But now the question is, what is an appropriate distribution for A and B?

02:22:56.360 --> 02:22:59.360
What might be an appropriate distribution for A and B?

02:22:59.360 --> 02:23:03.360
I'll tell you a few snippets.

02:23:03.360 --> 02:23:11.360
A and B govern the number of successes and failures effectively for the population of players.

02:23:11.360 --> 02:23:16.360
This happens over a single year.

02:23:16.360 --> 02:23:28.360
It can only be positive, so it being over a single year means there's generally a finite number of positives and successes and failures, A's and B's that every player has.

02:23:28.360 --> 02:23:35.360
So given that you know that, what might be a suitable distribution? Let's not worry about the shape of the distribution just yet.

02:23:35.360 --> 02:23:43.360
What is a suitable distribution?

02:23:43.360 --> 02:23:51.360
For Poisson maybe, it being, I forgot to say, A and B can actually be continuous.

02:23:51.360 --> 02:24:00.360
And that's sort of a giveaway that Poisson's not so ideal but we could try it.

02:24:00.360 --> 02:24:06.360
What's a positive bound continuous distribution that you might have heard of?

02:24:06.360 --> 02:24:11.360
Exponential is one. There's another one that we can define which is the half normal.

02:24:11.360 --> 02:24:18.360
There is the normal distribution but chopped up in half such that now the support is defined only on the positive half.

02:24:18.360 --> 02:24:20.360
That's one way to do it.

02:24:20.360 --> 02:24:26.360
Gamma distribution is another. Gamma is actually I think a generalization of a number of child distributions.

02:24:26.360 --> 02:24:30.360
It's got more parameters, it's a little bit more complicated, but yeah, we can totally do that.

02:24:30.360 --> 02:24:35.360
Lognormal I think is as well, yeah.

02:24:35.360 --> 02:24:38.360
Half Cauchy as well, like half student T, etc.

02:24:38.360 --> 02:24:42.360
Yes, definitely. So we're all on the right track. We're thinking of distributions.

02:24:42.360 --> 02:24:49.360
The key point is these distributions have to be positive bound because A and B can only take positive values.

02:24:49.360 --> 02:24:55.360
If we were to do anything with the full normal distribution, we'd be doing it the wrong way.

02:24:55.360 --> 02:25:02.360
So for simplicity's sake, let's assign A and B to take exponentials.

02:25:02.360 --> 02:25:09.360
And the only reason I would start with this but maybe not end with it is that exponentials are easy.

02:25:09.360 --> 02:25:16.360
They have a single parameter so there's not much hyper hyper parameters that we have to worry about.

02:25:16.360 --> 02:25:23.360
This A and B thing that we're assigning distributions on, these are what we would call hyper priors.

02:25:23.360 --> 02:25:28.360
Hyper being an added dimension, an added dimension of modeling that we're doing here.

02:25:28.360 --> 02:25:36.360
So for the sake of simplicity, we'll start with a simple exponential.

02:25:36.360 --> 02:25:44.360
What did I do in the real thing?

02:25:44.360 --> 02:25:54.360
In the actual thing, we've used 1 over 29, which is just an arbitrary number, which we can always debate about, but we're not going to today.

02:25:54.360 --> 02:26:00.360
We're going to do that in a long modeling critique session that my colleague Zach and I have done umpteen times now,

02:26:00.360 --> 02:26:03.360
where we debate our priors, debate the model structure.

02:26:03.360 --> 02:26:06.360
For now, for learning purposes, we'll just stick with that.

02:26:06.360 --> 02:26:10.360
So let's take this model and code it up.

02:26:10.360 --> 02:26:12.360
I'm going to switch back to the notebook.

02:26:12.360 --> 02:26:19.360
I'd like to encourage you all to also switch over.

02:26:19.360 --> 02:26:29.360
Inside the notebook, you will see that we've already got the binomial likelihood and the PPS metrics, the deterministic transforms defined for you.

02:26:29.360 --> 02:26:40.360
So now I'd like you to code along and let's fill in the rest for the beta distribution and the A prior and B prior.

02:26:40.360 --> 02:26:47.360
So we have alpha is A prior, beta is B prior.

02:26:47.360 --> 02:26:54.360
The shape is still length of data.

02:26:54.360 --> 02:27:03.360
And then we'll have exponential 1 over 29.

02:27:03.360 --> 02:27:10.360
Oops.

02:27:10.360 --> 02:27:15.360
Make sure it's all floating points.

02:27:15.360 --> 02:27:21.360
And let's call it hierarchical baseball.

02:27:21.360 --> 02:27:38.360
So once you've coded up the model, go ahead and sample from it and tell me what you see is kind of different.

02:27:38.360 --> 02:27:51.360
You'll notice also this model is a little slower to sample from.

02:27:51.360 --> 02:27:59.360
Okay, so while you all are waiting for models to sample and finish up, questions?

02:27:59.360 --> 02:28:07.360
Yes, no problem.

02:28:07.360 --> 02:28:13.360
Well, while things are sampling, it's actually a great time to talk with your neighbor about something new you've learned.

02:28:13.360 --> 02:28:16.360
Okay.

02:28:16.360 --> 02:28:32.360
Before we go on into something new you've learned, I want to ask a few questions about what you're observing about these baseball player posteriors having seen this having been fit under this hierarchical model.

02:28:32.360 --> 02:28:43.360
What's different from what you saw before?

02:28:43.360 --> 02:28:50.360
Okay, so you see more sigmoidal type of distributions rather than uniform or exponential types.

02:28:50.360 --> 02:28:52.360
That's one good one.

02:28:52.360 --> 02:28:57.360
What's another property that you're observing as well?

02:28:57.360 --> 02:29:00.360
What are the bounds and ranges?

02:29:00.360 --> 02:29:01.360
Pardon me?

02:29:01.360 --> 02:29:03.360
They're more tight.

02:29:03.360 --> 02:29:05.360
They're more tight.

02:29:05.360 --> 02:29:15.360
And this is the result of this type of switching over to a hierarchical model rather than using an independent model.

02:29:15.360 --> 02:29:26.360
So the first model that we wrote where every player is modeled as a beta distribution on its own, that is what I might call an independent model.

02:29:26.360 --> 02:29:34.360
And then the hierarchical model actually sort of pools these player properties as being drawn from one parental distribution.

02:29:34.360 --> 02:29:39.360
So they're sort of constrained by the parental distribution.

02:29:39.360 --> 02:29:43.360
This is a property of hierarchical models.

02:29:43.360 --> 02:29:47.360
I'm not going to design a value judgment on whether this is always good or always bad.

02:29:47.360 --> 02:29:49.360
It depends on the problem.

02:29:49.360 --> 02:30:13.360
But if it is justifiable by your modeling domain expertise, then a hierarchical model is actually a really powerful way to borrow information from players that have had lots of, from the population of players to do inference on the players that we have not had much information about.

02:30:13.360 --> 02:30:29.360
So for those players that had one at bat and one success, what you will notice is that their posterior distribution is still, is going to be kind of wide, not as crazy wide as it was before.

02:30:29.360 --> 02:30:39.360
It's going to be kind of wide, centered roughly around what the population mean is and follow roughly what the population mean is as well.

02:30:39.360 --> 02:30:53.360
If you have something that's a little bit more extreme, like seven at bat, seven hits, then you'll get something that's shifted to the right because there's a little bit of information saying that this player is kind of good or maybe lucky, we don't know.

02:30:53.360 --> 02:31:03.360
There's a bit of, it'll be shifted to the right, it'll be narrower, but it does express that, you know, it's not going to be wildly like 90, 90 something centered on 90 to 100%.

02:31:03.360 --> 02:31:06.360
It's going to be centered off, shifted off.

02:31:06.360 --> 02:31:19.360
So at least in this setting, it correctly encodes our intuition that players generally fall within this like population distributed.

02:31:19.360 --> 02:31:26.360
They follow the population distribution much more than we would expect from just looking at them independently.

02:31:26.360 --> 02:31:37.360
Okay, so this is a very powerful thing, that phenomena where you have these wild estimates being shrunk towards the population mean is called shrinkage.

02:31:37.360 --> 02:31:40.360
Shrinkage is a term you'll want to look out for in the literature.

02:31:40.360 --> 02:31:44.360
Okay, so you have this vocabulary that you won't be confused by.

02:31:44.360 --> 02:31:56.360
Okay.

02:31:56.360 --> 02:31:58.360
Yep.

02:32:15.360 --> 02:32:28.360
Okay, okay, cool.

02:32:28.360 --> 02:32:38.360
So the beta that we've put in there expresses a prior that is unconnected to any other player.

02:32:38.360 --> 02:32:43.360
So the prior for the beta distribution, the A and the B,

02:32:43.360 --> 02:32:50.360
the priors that we put, they are, even though they were point estimates, they're just saying this is, this is the shape.

02:32:50.360 --> 02:32:55.360
We're putting an identical shape of distribution on every single player.

02:32:55.360 --> 02:33:05.360
Now when we connect the players by saying they all draw from a population distribution, it's not that we're putting uncertainty.

02:33:05.360 --> 02:33:10.360
I would be hesitant to say that we're putting uncertainty on A and B.

02:33:10.360 --> 02:33:17.360
Rather, we're expressing that their shapes are now controlled by a population shape, right?

02:33:17.360 --> 02:33:20.360
That's where, that's where this shrinkage comes in.

02:33:20.360 --> 02:33:26.360
It's not that we've, we were really sure and so we assigned a single point value.

02:33:26.360 --> 02:33:37.360
We know that the two point values can give one shape, but that shape for the beta distribution was unconnected to the population at first.

02:33:37.360 --> 02:33:39.360
That's all it was.

02:33:39.360 --> 02:33:40.360
Okay.

02:33:40.360 --> 02:33:51.360
And now when we have a connected set of shapes, right, so we have a connected beta distributions by the hyper priors that we put on.

02:33:51.360 --> 02:34:05.360
What we're saying is that there is a population shape for the beta and they're influencing and governing the individual player shapes, the beta distribution shapes.

02:34:05.360 --> 02:34:08.360
Does that make sense?

02:34:08.360 --> 02:34:17.360
By ironically putting a distribution on it, not putting a ring on it.

02:34:18.360 --> 02:34:23.360
Yes, yes, yes.

02:34:23.360 --> 02:34:28.360
So if in doubt, don't put a ring on it, put a distribution on it, okay?

02:34:28.360 --> 02:34:31.360
Cool.

02:34:31.360 --> 02:34:39.360
Right, so that's, that's that phenomena of shrinkage that I wanted everybody to have some intuition about, okay?

02:34:39.360 --> 02:34:42.360
So let's now go into something that you've learned.

02:34:42.360 --> 02:34:48.360
We've actually come to the end of the binomial story and we've gone really, really deep.

02:34:48.360 --> 02:35:01.360
We've come from like the simple naive one, one group to two groups to now vectorizing over multiple groups to then now adding on a hierarchical model on top.

02:35:01.360 --> 02:35:06.360
I'm hoping it's not yet information overload because there's more.

02:35:06.360 --> 02:35:08.360
So what's something new you've learned?

02:35:08.360 --> 02:35:11.360
And let's get that like etched in your head.

02:35:11.360 --> 02:35:18.360
You have volunteers.

02:35:18.360 --> 02:35:19.360
Sure.

02:35:19.360 --> 02:35:20.360
Yeah, cool.

02:35:20.360 --> 02:35:21.360
Great.

02:35:21.360 --> 02:35:23.360
That was part of the point.

02:35:23.360 --> 02:35:32.360
Anything else, something new that you didn't expect or something that has been resonating with you?

02:35:32.360 --> 02:35:33.360
Maybe on this side.

02:35:33.360 --> 02:35:38.360
This side has been really quiet.

02:35:38.360 --> 02:35:42.360
I'm going to point at someone.

02:35:42.360 --> 02:35:49.360
Second last row middle guy.

02:35:49.360 --> 02:35:58.360
Anything new you've learned?

02:35:59.360 --> 02:36:00.360
Okay, still processing.

02:36:00.360 --> 02:36:01.360
That's completely valid.

02:36:01.360 --> 02:36:02.360
That's totally cool.

02:36:02.360 --> 02:36:09.360
And the fact that you're still not sure means there's processing going on and I fully appreciate that.

02:36:09.360 --> 02:36:10.360
How about in the middle?

02:36:10.360 --> 02:36:11.360
Anybody else?

02:36:11.360 --> 02:36:17.360
Any volunteers?

02:36:17.360 --> 02:36:20.360
Sorry, can you say it louder?

02:36:20.360 --> 02:36:25.360
Okay, so reinforcing the value of the cumulative distribution plots, right?

02:36:25.360 --> 02:36:27.360
That's super important.

02:36:27.360 --> 02:36:36.360
The fact that we get richer information from that is very useful.

02:36:36.360 --> 02:36:38.360
Oh, the beta distribution.

02:36:38.360 --> 02:36:39.360
Ah, yes.

02:36:39.360 --> 02:36:45.360
The fact that we're able to constrain and shape.

02:36:45.360 --> 02:36:47.360
Yes, yes, exactly.

02:36:47.360 --> 02:36:48.360
Exactly.

02:36:48.360 --> 02:36:52.360
It's a very useful tool to have in the toolkit.

02:36:52.360 --> 02:37:05.360
I think it's still, like, I don't know if it's going to fit in here, but in my, I don't know if it's going to fit in here.

02:37:05.360 --> 02:37:10.360
Yep.

02:37:10.360 --> 02:37:11.360
Cool, cool.

02:37:11.360 --> 02:37:12.360
Awesome.

02:37:12.360 --> 02:37:13.360
And back there.

02:37:13.360 --> 02:37:14.360
Last one.

02:37:14.360 --> 02:37:16.360
I knew what I knew.

02:37:16.360 --> 02:37:17.360
Yeah.

02:37:18.360 --> 02:37:23.360
Oh, yeah.

02:37:23.360 --> 02:37:25.360
Yep.

02:37:25.360 --> 02:37:27.360
Yep.

02:37:27.360 --> 02:37:28.360
Yep.

02:37:28.360 --> 02:37:29.360
Yep.

02:37:29.360 --> 02:37:30.360
Absolutely.

02:37:30.360 --> 02:37:31.360
There are lots of good connections there.

02:37:31.360 --> 02:37:37.360
So a lot of, a lot of the classical stats are connected in this way.

02:37:37.360 --> 02:37:46.360
Yes.

02:37:46.360 --> 02:37:47.360
Yeah.

02:37:47.360 --> 02:37:54.360
I was hoping that this question would come up and trust me, I did not plant her in the crowd.

02:37:54.360 --> 02:38:01.360
So when we think about which distribution to use, there are a few rules of thumb.

02:38:01.360 --> 02:38:07.360
The first rule of thumb is find something that has the correct support.

02:38:07.360 --> 02:38:09.360
That is absolutely crucial.

02:38:09.360 --> 02:38:15.360
If you use something that's got the wrong support, that is, you've got data that showed up negative.

02:38:15.360 --> 02:38:22.360
You've got data that are showed up negative, can take on negative values, but you put a

02:38:22.360 --> 02:38:24.360
positive only distribution inside there.

02:38:24.360 --> 02:38:28.360
You're going to get not a number errors inside sampling.

02:38:28.360 --> 02:38:29.360
Right.

02:38:29.360 --> 02:38:33.360
And that also means that you've not, you've missed something in the modeling process.

02:38:33.360 --> 02:38:39.360
So getting the support correct is the first step.

02:38:39.360 --> 02:38:43.360
And the next step is to think about the likelihood.

02:38:43.360 --> 02:38:44.360
Right.

02:38:44.360 --> 02:38:48.360
That's where knowing, so that's, that's for any arbitrary distribution.

02:38:48.360 --> 02:38:51.360
Getting the support correct is absolutely crucial.

02:38:51.360 --> 02:38:55.360
The next thing is to think about the likelihood function.

02:38:55.360 --> 02:39:00.360
How are the data that you are interested in, the thing you've actually measured?

02:39:00.360 --> 02:39:05.360
How is that, how is that distributed?

02:39:05.360 --> 02:39:11.360
So that's where knowing the probability distribution stories comes into place.

02:39:11.360 --> 02:39:18.360
Especially rules of thumb are if you've got something that's got amount of stuff happening

02:39:18.360 --> 02:39:20.360
per unit time, it's put on.

02:39:20.360 --> 02:39:25.360
If you've got trials that are positive, negative, it's Bernoulli binomial.

02:39:25.360 --> 02:39:26.360
Right.

02:39:26.360 --> 02:39:28.360
These are very generalizable stories.

02:39:28.360 --> 02:39:34.360
If you're really unsure, you might start with a normal distribution.

02:39:34.360 --> 02:39:40.360
If you've got some other types of processes, so for example, the negative binomial distribution

02:39:40.360 --> 02:39:45.360
counts the number of failures until a success.

02:39:45.360 --> 02:39:46.360
Right.

02:39:46.360 --> 02:39:49.360
So knowing this generative story helps as well.

02:39:49.360 --> 02:39:50.360
Okay.

02:39:50.360 --> 02:39:56.360
So, and then also there's this family of distributions called the zero inflated distributions.

02:39:56.360 --> 02:39:59.360
So you can have the zero inflated Poisson distribution.

02:39:59.360 --> 02:40:03.360
What it expresses is that there's, there are two processes at play.

02:40:03.360 --> 02:40:06.360
There's a process that generates lots of zeros.

02:40:06.360 --> 02:40:11.360
And then there's a process that generates the Poisson side of that and there's, there's,

02:40:11.360 --> 02:40:13.360
this is essentially a mixture model.

02:40:13.360 --> 02:40:20.360
So you're now having to infer both the probability that it is in the zero versus not zero P and

02:40:20.360 --> 02:40:26.360
one minus P as well as the Poisson parameter, the rate parameter of interest.

02:40:26.360 --> 02:40:32.360
And it, it, it's really important to think through the, that part of the problem.

02:40:32.360 --> 02:40:34.360
So that's the second part.

02:40:34.360 --> 02:40:40.360
And the third rule of thumb is to think about what the shape of the distribution should

02:40:40.360 --> 02:40:41.360
look like.

02:40:41.360 --> 02:40:46.360
So this is where I would then look at the PDF rather than the CDF because this is all analytical

02:40:46.360 --> 02:40:52.360
because then it gives me a sense of the skew and the central moments of the distribution.

02:40:52.360 --> 02:40:57.360
And it can help me express quantitatively what I'm thinking about.

02:40:57.360 --> 02:41:02.360
So the beta distribution is that classic anchoring example that I always come back to.

02:41:02.360 --> 02:41:07.360
It's bound from zero to one and it's therefore suitable for a probability parameter.

02:41:07.360 --> 02:41:14.360
I can tweak whether it's centered on 0.5, 0.2, 0.9 by simply tweaking the A and B parameters.

02:41:14.360 --> 02:41:21.360
And I can tweak how, how tight that distribution is by doing, you know,

02:41:21.360 --> 02:41:26.360
beta 91 versus beta 9010 versus beta 900, 100.

02:41:26.360 --> 02:41:27.360
Right.

02:41:27.360 --> 02:41:31.360
So there are ways to control the, the shape of the distribution that way.

02:41:31.360 --> 02:41:34.360
Those are the three rules of thumb.

02:41:34.360 --> 02:41:38.360
What I, in practice, find myself doing is thinking about the problem and going like,

02:41:38.360 --> 02:41:42.360
ah, yeah, I need something that's positive here because that can only take on positive values.

02:41:42.360 --> 02:41:47.360
So then I'll go hunting in the distribution library for something that's positive.

02:41:47.360 --> 02:41:53.360
And most of the time we're sort of expressing, you know, say for a standard deviation parameter,

02:41:53.360 --> 02:41:59.360
we're expressing the fact that things generally are not going to be wildly,

02:41:59.360 --> 02:42:02.360
um, standard deviation parameters are generally like tight,

02:42:02.360 --> 02:42:04.360
but then sometimes can take on high values.

02:42:04.360 --> 02:42:08.360
So I might take like a half koshi because standard deviations can only be positive,

02:42:08.360 --> 02:42:13.360
but I'm allowing for really high tails or half student T, for example.

02:42:13.360 --> 02:42:14.360
Okay.

02:42:14.360 --> 02:42:16.360
So that's a few examples.

02:42:16.360 --> 02:42:17.360
Back there.

02:42:17.360 --> 02:42:43.360
Okay.

02:42:43.360 --> 02:42:44.360
Yeah.

02:42:44.360 --> 02:42:49.360
Um, so the exponentials, you can, sorry, so choosing an exponential,

02:42:49.360 --> 02:42:54.360
you can think of it as this is the first model I'll write.

02:42:54.360 --> 02:42:57.360
And then if you go to ravine's tutorial tomorrow,

02:42:57.360 --> 02:43:00.360
there's this whole business of model comparison.

02:43:00.360 --> 02:43:04.360
That's, um, sometimes you'll find it doesn't really matter what the hyperpryor is.

02:43:04.360 --> 02:43:11.360
And sometimes it does matter when we check things like the information criteria metric,

02:43:11.360 --> 02:43:15.360
uh, that, uh, the information that's contained in inside the model.

02:43:15.360 --> 02:43:18.360
Um, sometimes you'll find whether it's half koshi or exponential,

02:43:18.360 --> 02:43:20.360
just quantitatively, it doesn't really matter.

02:43:20.360 --> 02:43:29.360
Um, so in some senses start with something and then run with it and then be ready to change the model.

02:43:29.360 --> 02:43:30.360
Yep.

02:43:30.360 --> 02:43:31.360
Yep.

02:43:31.360 --> 02:43:38.360
And I emphasize that, uh, we, we don't want to really get into debating that choice,

02:43:38.360 --> 02:43:41.360
but we can actually offline debate that choice if we want.

02:43:41.360 --> 02:43:46.360
We can look at how the Lambda parameter controls the shape of the exponential distribution

02:43:46.360 --> 02:43:50.360
and whether that expresses qualitatively what we're intending to express.

02:43:50.360 --> 02:43:51.360
Right.

02:43:51.360 --> 02:43:55.360
So some, the exponential distribution generally starts high and then goes low.

02:43:55.360 --> 02:43:56.360
Right.

02:43:56.360 --> 02:44:01.360
Um, if you increase, I think if you increase the Lambda parameter in quantity,

02:44:01.360 --> 02:44:04.360
it'll, it'll become more and more flat.

02:44:04.360 --> 02:44:10.360
If you decrease it, it'll become more and more closer to, to, to zero, uh, centered on zero.

02:44:10.360 --> 02:44:11.360
Right.

02:44:11.360 --> 02:44:14.360
So that's, that's sort of how, and then we'll, we'll have to ask,

02:44:14.360 --> 02:44:18.360
is that what we want to express in the, in the model?

02:44:18.360 --> 02:44:19.360
Okay.

02:44:19.360 --> 02:44:20.360
Yeah.

02:44:20.360 --> 02:44:21.360
Yeah.

02:44:21.360 --> 02:44:22.360
Yeah.

02:44:22.360 --> 02:44:23.360
Yeah.

02:44:23.360 --> 02:44:24.360
Yeah.

02:44:24.360 --> 02:44:25.360
Yeah.

02:44:25.360 --> 02:44:26.360
Yeah.

02:44:26.360 --> 02:44:27.360
Yeah.

02:44:27.360 --> 02:44:28.360
Yeah.

02:44:28.360 --> 02:44:29.360
Yeah.

02:44:29.360 --> 02:44:33.360
Um, both from a mechanical standpoint that is like,

02:44:33.360 --> 02:44:35.360
we have fewer things to worry about.

02:44:35.360 --> 02:44:40.360
Um, and from, uh, I guess parsimony standpoint is like a simpler,

02:44:40.360 --> 02:44:41.360
it's a simpler model.

02:44:41.360 --> 02:44:42.360
Right.

02:44:42.360 --> 02:44:44.360
Like we don't have that many knobs to turn.

02:44:44.360 --> 02:44:45.360
Right.

02:44:45.360 --> 02:44:46.360
Yeah.

02:44:46.360 --> 02:44:49.360
Cool.

02:44:49.360 --> 02:44:50.360
All right.

02:44:50.360 --> 02:44:52.360
Let's see.

02:44:52.360 --> 02:44:55.360
It's 440 right now and we end at 530.

02:44:55.360 --> 02:44:59.360
So I'm debating what we should worry about next.

02:44:59.360 --> 02:45:02.360
So what's, what we would have done, what we, sorry.

02:45:02.360 --> 02:45:07.360
So what I originally planned was, uh, to go through one more example

02:45:07.360 --> 02:45:09.360
of how we do Bayesian estimation this time,

02:45:09.360 --> 02:45:11.360
not with binomial stories,

02:45:11.360 --> 02:45:14.360
but with like student T distributions and normal distributions.

02:45:14.360 --> 02:45:17.360
Um, that's one thing we could work on,

02:45:17.360 --> 02:45:23.360
or we can first jump to, uh, arbitrary curve regression.

02:45:23.360 --> 02:45:27.360
So I'm, I'm, I'm intentionally setting this up as like,

02:45:27.360 --> 02:45:30.360
you can fit any curve, uh, with times three,

02:45:30.360 --> 02:45:33.360
not just a line that like linear regression is what we're used to.

02:45:33.360 --> 02:45:34.360
That's kind of boring.

02:45:34.360 --> 02:45:37.360
So let's go in and like fit a different type of model.

02:45:37.360 --> 02:45:40.360
Um, what would you prefer?

02:45:40.360 --> 02:45:44.360
So let's do a vote and I will estimate the probabilities.

02:45:44.360 --> 02:45:47.360
Um, how many of you want to do the curve regression?

02:45:47.360 --> 02:45:48.360
Raise your hand.

02:45:48.360 --> 02:45:50.360
How many want to do a second estimation?

02:45:50.360 --> 02:45:51.360
Okay.

02:45:51.360 --> 02:45:53.360
So we'll do, we'll do the curve regression.

02:45:53.360 --> 02:45:57.360
If we have time, I'll come back and show you a few things,

02:45:58.360 --> 02:46:02.360
uh, live demoed rather than, uh, interactive coding.

02:46:02.360 --> 02:46:03.360
Okay.

02:46:03.360 --> 02:46:05.360
On, on the second estimation thing.

02:46:05.360 --> 02:46:12.360
So with that, I'd like you to open up, uh, notebook number five.

02:46:12.360 --> 02:46:19.360
Notebook number five is all about arbitrary curve regression.

02:46:19.360 --> 02:46:22.360
Let me see if I can connect in here.

02:46:22.360 --> 02:46:23.360
Cool.

02:46:23.360 --> 02:46:24.360
Cool.

02:46:24.360 --> 02:46:32.360
So, um, curve regression, I'm going to put this out here.

02:46:32.360 --> 02:46:38.360
Curve regression is nothing more than estimation with, with equations.

02:46:38.360 --> 02:46:39.360
Okay.

02:46:39.360 --> 02:46:46.360
So we're going to use a radioactive decay data set to sort of reinforce this point.

02:46:46.360 --> 02:46:47.360
Okay.

02:46:47.360 --> 02:46:51.360
Um, so we know linear regression.

02:46:51.360 --> 02:46:59.360
You have, you have something Y is, uh, modeled as a function of a linear combination of your X's.

02:46:59.360 --> 02:47:00.360
Right.

02:47:00.360 --> 02:47:05.360
And so you can have the thing we're really interested in is like the W's, the weights,

02:47:05.360 --> 02:47:10.360
or, you know, the M's if you're from physics, Y equals MX plus C, the M's if you're in physics,

02:47:10.360 --> 02:47:12.360
or the weights if you're in stats.

02:47:12.360 --> 02:47:15.360
Um, and the bias term as well.

02:47:15.360 --> 02:47:20.360
Uh, and really nothing should stop us from just thinking about linear regression as the

02:47:20.360 --> 02:47:23.360
only form of regression that we're interested in.

02:47:23.360 --> 02:47:24.360
Right.

02:47:24.360 --> 02:47:26.360
There's, there's, you can do all sorts of regression.

02:47:26.360 --> 02:47:28.360
You can do Poisson regression, whatever.

02:47:28.360 --> 02:47:29.360
You can do neural net regression.

02:47:29.360 --> 02:47:36.360
If you know how to write neural nets, you can do, uh, in this case, exponential decay curve regression.

02:47:36.360 --> 02:47:37.360
Right.

02:47:37.360 --> 02:47:44.360
So we're going to see whether we can from noisy, um, radioactive decay measurements back

02:47:45.360 --> 02:47:50.360
infer the correct parameters that help us identify a radioactive material.

02:47:50.360 --> 02:47:51.360
Okay.

02:47:51.360 --> 02:47:55.360
So I'd like you to run that first cell where we load data.

02:47:55.360 --> 02:48:02.360
Oh, my.

02:48:02.360 --> 02:48:03.360
Okay.

02:48:03.360 --> 02:48:06.360
You'll have something that looks, data that looks something like this.

02:48:06.360 --> 02:48:07.360
What's on this data?

02:48:07.360 --> 02:48:13.360
Well, it's got time on one axis and then it's got activity or radioactive, you know, uh,

02:48:13.360 --> 02:48:16.360
tiger count things on the Y axis.

02:48:16.360 --> 02:48:17.360
Okay.

02:48:17.360 --> 02:48:20.360
I've sort of, uh, well, this is synthetic data.

02:48:20.360 --> 02:48:23.360
Noised out for educational purposes.

02:48:23.360 --> 02:48:24.360
All right.

02:48:24.360 --> 02:48:29.360
So if you plot the data, you should look, you should get something that looks like this.

02:48:29.360 --> 02:48:30.360
Right.

02:48:30.360 --> 02:48:31.360
Right.

02:48:31.360 --> 02:48:33.360
Everybody got that?

02:48:33.360 --> 02:48:34.360
Okay.

02:48:34.360 --> 02:48:42.360
So given that we're in this like, uh, radioactive decay sort of scenario.

02:48:42.360 --> 02:48:50.360
I'd like to ask you to think about what equations can we use to model this data?

02:48:50.360 --> 02:48:55.360
We'll get into what the statistical model is in a moment, but I want to first think about

02:48:55.360 --> 02:48:59.360
what equations we can use to govern this model.

02:48:59.360 --> 02:49:02.360
There's an exponential decay equation.

02:49:02.360 --> 02:49:05.360
What are the parameters of that equation?

02:49:05.360 --> 02:49:09.360
You have time, which we've observed part of what we've observed.

02:49:09.360 --> 02:49:11.360
And what else?

02:49:11.360 --> 02:49:13.360
Half-life, the decay constant, right?

02:49:13.360 --> 02:49:15.360
And anything else?

02:49:15.360 --> 02:49:17.360
Ah, sure.

02:49:17.360 --> 02:49:20.360
We're ignoring that for the time being, but yes, if we were to be fully mechanistic,

02:49:20.360 --> 02:49:21.360
we would do that.

02:49:21.360 --> 02:49:23.360
What else is there?

02:49:23.360 --> 02:49:24.360
Offset.

02:49:24.360 --> 02:49:25.360
Offset.

02:49:25.360 --> 02:49:30.360
So, uh, is that the first offset or the baseline offset?

02:49:30.360 --> 02:49:31.360
Baseline offset.

02:49:31.360 --> 02:49:36.360
And then there's one more which is, which governs the original, uh, the starting point, right?

02:49:36.360 --> 02:49:38.360
So there are what?

02:49:38.360 --> 02:49:41.360
A, C, and tau.

02:49:41.360 --> 02:49:44.360
We have three parameters to estimate for this curve.

02:49:44.360 --> 02:49:49.360
And you'll notice the readings are actually kind of noisy as well, right?

02:49:49.360 --> 02:49:53.360
And that's because there's, you know, measurements are not always perfect.

02:49:53.360 --> 02:49:56.360
There's going to be some amount of noise that we've got to deal with.

02:49:56.360 --> 02:49:58.360
So how do we do that?

02:49:58.360 --> 02:50:01.360
Well, we've got to build a model.

02:50:01.360 --> 02:50:05.360
I'm going to switch back to drawing.

02:50:05.360 --> 02:50:18.360
We've got to build a model that lets us link the x-axis component, which is the time component,

02:50:18.360 --> 02:50:20.360
to the y-axis thing.

02:50:20.360 --> 02:50:37.360
We've already said that the equation is y is equal to a times e to the negative, uh, t over tau plus c, right?

02:50:37.360 --> 02:50:49.360
The c term we can interpret, it's sort of like systematic bias in our measurement.

02:50:50.360 --> 02:50:53.360
The a term we can interpret, right?

02:50:53.360 --> 02:51:04.360
The a term starts is, is the starting radioactivity.

02:51:04.360 --> 02:51:16.360
And the tau term we can also interpret, it is the characteristic half-life of this radioactive element.

02:51:16.360 --> 02:51:18.360
Pardon me?

02:51:18.360 --> 02:51:21.360
Ah, yes. So we're going to, we're going to talk about noises.

02:51:21.360 --> 02:51:24.360
Oh, Siri, goodness.

02:51:24.360 --> 02:51:26.360
Yes, that is very good.

02:51:26.360 --> 02:51:30.360
And let's add in this plus epsilon.

02:51:30.360 --> 02:51:33.360
But epsilon is not one of the mechanistic components.

02:51:33.360 --> 02:51:35.360
It's a statistical component.

02:51:35.360 --> 02:51:38.360
And that's why I've drawn it in red or a different color, right?

02:51:38.360 --> 02:51:41.360
It's not part of the mechanistic part that we're really interested in.

02:51:41.360 --> 02:51:46.360
So, let's see.

02:51:46.360 --> 02:51:52.360
What are, we're now going to take this equation and we're going to build a statistical model around it.

02:51:52.360 --> 02:52:03.360
What is a good prior for a, what is, sorry, what is a good distribution for a?

02:52:03.360 --> 02:52:05.360
Something positive, yes.

02:52:05.360 --> 02:52:06.360
All right.

02:52:06.360 --> 02:52:13.360
And what's the simplest positive distribution that we can think about?

02:52:13.360 --> 02:52:15.360
Pardon me?

02:52:15.360 --> 02:52:18.360
Yeah, it's like the first, first thing we measure, right?

02:52:18.360 --> 02:52:21.360
Yeah, so it's a bit like an impulse that way.

02:52:21.360 --> 02:52:30.360
It's the, so we might say a follows some exponential distribution.

02:52:30.360 --> 02:52:38.360
Let's just start with that because it's a simple one.

02:52:38.360 --> 02:52:43.360
What about tau?

02:52:43.360 --> 02:52:48.360
What values can tau take on?

02:52:48.360 --> 02:52:49.360
It must be positive.

02:52:49.360 --> 02:52:50.360
Yes, yes.

02:52:50.360 --> 02:52:53.360
Okay.

02:52:53.360 --> 02:52:57.360
So let's say exponential.

02:52:57.360 --> 02:53:03.360
And what about C?

02:53:03.360 --> 02:53:09.360
This is systematic bias, not the noise in the data.

02:53:09.360 --> 02:53:12.360
Gaussian.

02:53:12.360 --> 02:53:15.360
Systematic bias could be positive, could be negative.

02:53:15.360 --> 02:53:20.360
Yes, so it could be normal.

02:53:20.360 --> 02:53:24.360
Could be.

02:53:24.360 --> 02:53:25.360
Ah, great.

02:53:25.360 --> 02:53:27.360
Thank you.

02:53:27.360 --> 02:53:34.360
So instead of normal, what will we do then?

02:53:34.360 --> 02:53:38.360
We might do exponential, sure.

02:53:38.360 --> 02:53:51.360
What about the likelihood though?

02:53:51.360 --> 02:53:53.360
Pardon me?

02:53:53.360 --> 02:53:55.360
Why would it be Poisson?

02:53:55.360 --> 02:53:57.360
We're measuring counts.

02:53:57.360 --> 02:53:58.360
Yes.

02:53:58.360 --> 02:54:07.360
However, at least in the data, we've got it as continuous right now because of the noise in the machine that reports back a continuous value.

02:54:07.360 --> 02:54:19.360
So what might we do?

02:54:19.360 --> 02:54:22.360
Let's cheat a little bit.

02:54:22.360 --> 02:54:24.360
Approximation on approximation.

02:54:24.360 --> 02:54:47.360
We'll use a normal distribution here because the range of values for which we've got data are tight enough and far enough from zero that essentially at the tails of our normal, we don't have any much really credible credibility points assigned there.

02:54:47.360 --> 02:54:53.360
These are like the struggles that we wrestle with, with every new modeling problem that comes in.

02:54:53.360 --> 02:54:57.360
Is a normal distribution likelihood reasonable?

02:54:57.360 --> 02:54:58.360
Is it correct?

02:54:58.360 --> 02:54:59.360
Probably not.

02:54:59.360 --> 02:55:01.360
Is it useful?

02:55:01.360 --> 02:55:02.360
Maybe.

02:55:02.360 --> 02:55:03.360
Right?

02:55:03.360 --> 02:55:11.360
So I want to get that in your head.

02:55:12.360 --> 02:55:22.360
Likelihood is the thing that you're observing about the data, right?

02:55:22.360 --> 02:55:34.360
So what do you mean by unit then?

02:55:34.360 --> 02:55:36.360
Right, right, right, right.

02:55:36.360 --> 02:55:43.360
So if you look at, I'm going to detour a little bit and talk about linear regression.

02:55:43.360 --> 02:55:52.360
So you have y equals mx plus c.

02:55:52.360 --> 02:56:00.360
We might write a model that says m is normally distributed for whatever distribution parameters.

02:56:00.360 --> 02:56:04.360
C is also normally distributed.

02:56:04.360 --> 02:56:08.360
Y is the likelihood of the data.

02:56:08.360 --> 02:56:09.360
It's got noise.

02:56:09.360 --> 02:56:26.360
And if we assume that the noise is Gaussian noise, then we can impose a modeling assumption that says that this is normally distributed where the mu is equal to mx plus c.

02:56:26.360 --> 02:56:31.360
And the sigma is equal to something else.

02:56:31.360 --> 02:56:33.360
The sigma is our epsilon.

02:56:33.360 --> 02:56:35.360
And we can ask, what is the epsilon?

02:56:35.360 --> 02:56:38.360
How is the, how is that going to be distributed?

02:56:38.360 --> 02:56:40.360
Does that make sense?

02:56:40.360 --> 02:56:41.360
Yeah.

02:56:41.360 --> 02:56:52.360
So I'm glad you asked that question because if you look at the parallels here, we'll need a sigma prior.

02:56:52.360 --> 02:57:00.360
And just for convenience, I'm just going to put the standard half Cauchy, okay?

02:57:00.360 --> 02:57:01.360
Inside there.

02:57:01.360 --> 02:57:03.360
So we got that.

02:57:03.360 --> 02:57:13.360
So then we might impose the same or a similar set of modeling assumptions on the likelihood, which is our y, right?

02:57:13.360 --> 02:57:17.360
Or rather than calling it likelihood, because that's overloading terms.

02:57:17.360 --> 02:57:18.360
Let's just do y.

02:57:18.360 --> 02:57:20.360
How is y distributed?

02:57:20.360 --> 02:57:36.360
Why we might impose that this is normally distributed where the mu is equal to a times e to the negative t over tau plus c.

02:57:36.360 --> 02:57:40.360
And then we have some noise, which is our epsilon.

02:57:40.360 --> 02:57:49.360
And our epsilon, just for convenience, will also make it half Cauchy.

02:57:49.360 --> 02:58:00.360
Let's let that sink in for a moment.

02:58:00.360 --> 02:58:04.360
Or maybe I should say something like, I'm just going to leave this up on there.

02:58:04.360 --> 02:58:05.360
No.

02:58:05.360 --> 02:58:07.360
Do we have questions?

02:58:07.360 --> 02:58:29.360
Things that are not clear.

02:58:29.360 --> 02:58:30.360
Yeah.

02:58:30.360 --> 02:58:47.360
If the errors in this particular case, what we've assumed is that our errors are not dependent on the value on the x-axis.

02:58:47.360 --> 02:59:01.360
If now we suddenly found that the values, the error varies with the value on the x-axis, suddenly we have to write a function that models sigma as a function of, in this case, t.

02:59:01.360 --> 02:59:02.360
Right?

02:59:02.360 --> 02:59:05.360
So that's one place where this model would fail.

02:59:05.360 --> 02:59:08.360
And I've actually encountered that at work before.

02:59:09.360 --> 02:59:10.360
How do we get around that?

02:59:10.360 --> 02:59:20.360
We get around that by either explicitly stating up front that this assumption does not hold in our data, but we're willing to work with the consequences of that.

02:59:20.360 --> 02:59:23.360
Or we go hunting for the function.

02:59:23.360 --> 02:59:30.360
And sometimes that's kind of hard when you have like limited x values to work with.

02:59:30.360 --> 02:59:31.360
Oops.

02:59:31.360 --> 02:59:34.360
Siri keeps coming up.

02:59:34.360 --> 02:59:35.360
Cool.

02:59:35.360 --> 02:59:37.360
Any other questions on this?

02:59:37.360 --> 02:59:39.360
This is like the key, key point.

02:59:39.360 --> 02:59:41.360
This is the key point here.

02:59:41.360 --> 02:59:59.360
Like you can write the parameters of your likelihood distributions as a function or a transformation on the other things that you've, you're interested in.

02:59:59.360 --> 03:00:02.360
Okay.

03:00:02.360 --> 03:00:03.360
Okay.

03:00:03.360 --> 03:00:10.360
So if you go ahead and let's, let's go ahead and code the model together.

03:00:10.360 --> 03:00:15.360
What's inside the instructor notebook might be different from what we just wrote out.

03:00:15.360 --> 03:00:20.360
What I wanted to give you all just now was this live experience of like, well, I don't know.

03:00:20.360 --> 03:00:24.360
So what are we, what modeling assumptions am I willing to stand with?

03:00:24.360 --> 03:00:25.360
Right.

03:00:25.360 --> 03:00:28.360
And then we can go back in and re critique the model one more time.

03:00:28.360 --> 03:00:35.360
So let's, let's put in, in this case, just copy and paste what's inside the instructor notebook.

03:00:35.360 --> 03:00:36.360
All right.

03:00:36.360 --> 03:00:41.360
And let's not worry too much about the others.

03:00:41.360 --> 03:00:46.360
Again, you'll notice that thing, that equation.

03:00:46.360 --> 03:00:50.360
I, I alluded it, alluded to this point, its name a few times.

03:00:50.360 --> 03:00:52.360
It's called a link function.

03:00:52.360 --> 03:00:53.360
Right.

03:00:53.360 --> 03:00:55.360
So y equals mx plus C is a link function.

03:00:55.360 --> 03:01:00.360
Y is equal to times a times e to the negative t over tau plus C.

03:01:00.360 --> 03:01:02.360
That's just another link function.

03:01:02.360 --> 03:01:06.360
You can have your four parameter dose response curves as a link function.

03:01:06.360 --> 03:01:16.360
You can put the standard logistic regression curve as a link function, like any math function that you can think of can be a link function.

03:01:16.360 --> 03:01:18.360
All right.

03:01:18.360 --> 03:01:28.360
And then that goes and that, what that does is it can, it controls the, the mean curve parameter.

03:01:28.360 --> 03:01:29.360
Right.

03:01:29.360 --> 03:01:35.360
It controls the mean of our data as a function of, you know, this thing on the x axis.

03:01:35.360 --> 03:01:38.360
All right.

03:01:38.360 --> 03:01:40.360
So let's copy and paste what's inside here.

03:01:40.360 --> 03:01:44.360
Here the modeling choices are half normal, exponential.

03:01:44.360 --> 03:01:53.360
I think I chose C to be normal under the assumption that sometimes the, the machine might go faulty and give us like a completely negative baseline.

03:01:53.360 --> 03:01:54.360
Sure.

03:01:54.360 --> 03:01:59.360
And if that never happens, then I would change, change that to a half kosher exponential.

03:01:59.360 --> 03:02:02.360
Okay.

03:02:02.360 --> 03:02:13.360
So then we sample.

03:02:13.360 --> 03:02:22.360
Oh, I hear the jet engines running again.

03:02:22.360 --> 03:02:35.360
Ah, so this is a, this is the thing that I'm wondering ravine, will you be covering Colin will be covering it tomorrow in the RVs or the Asian model evaluation tutorial.

03:02:35.360 --> 03:02:39.360
So ignore that for the time being.

03:02:39.360 --> 03:02:42.360
And you should get something that looks like this guy.

03:02:42.360 --> 03:02:45.360
The, do we all have that thumbs up if you do.

03:02:45.360 --> 03:02:46.360
Yep.

03:02:46.360 --> 03:02:47.360
Okay.

03:02:47.360 --> 03:02:48.360
So you get like traces.

03:02:48.360 --> 03:02:58.360
This one's been simple, right, because we've got only a single alpha, a single capital A, a single capital C, a single tau, right.

03:02:58.360 --> 03:03:14.360
But you all saw just now how we can actually have a vector of A's, a vector of towels, a vector of C's, our likelihood normal distribution can also be expanded to be a vector of, of, of likelihoods.

03:03:14.360 --> 03:03:21.360
There's just some little intricacies that we have to worry about with respect to the, the, you know, y equals.

03:03:21.360 --> 03:03:23.360
Y is the mu, the link function, right.

03:03:23.360 --> 03:03:25.360
So you have to play around with that.

03:03:25.360 --> 03:03:26.360
But this is totally doable.

03:03:26.360 --> 03:03:34.360
And then once you have that multiple groups thing, once again, you can do your hierarchical player, hierarchical thing.

03:03:34.360 --> 03:03:37.360
If it's an appropriate modeling decision, right.

03:03:37.360 --> 03:03:41.360
So if you think about it, think about it.

03:03:41.360 --> 03:03:49.360
That this arbitrary curve regression thing is once again, nothing more than estimation at its heart.

03:03:49.360 --> 03:04:05.360
And instead of estimating like A, instead of estimating distribution parameters directly, like in this case, in previous cases, we were estimating the P hierarchically, right.

03:04:05.360 --> 03:04:09.360
Now all we've done is we've said there's an equation that governs that key parameter.

03:04:09.360 --> 03:04:18.360
And now we want to estimate the parameters of that equation in a, of that equation in a statistical fashion, rather than just treat it as some fixed point.

03:04:18.360 --> 03:04:20.360
Okay.

03:04:20.360 --> 03:04:22.360
How are we with that?

03:04:22.360 --> 03:04:25.360
Okay.

03:04:25.360 --> 03:04:29.360
If you want, go back and like figure out what the element is.

03:04:29.360 --> 03:04:33.360
I'm not going to reveal the answer right now.

03:04:33.360 --> 03:04:38.360
But I want to point you to the table at the bottom of your notebooks.

03:04:38.360 --> 03:04:55.360
The table at the bottom of your notebooks says in compact form, everything that I just said, that is, you can put any arbitrary curve as a link function, and you don't have to be restrained to modeling just linear models.

03:04:55.360 --> 03:05:02.360
You can model these decay curves, you can model logistic regressions, you can do.

03:05:02.360 --> 03:05:04.360
You can write a neural net.

03:05:04.360 --> 03:05:14.360
Like if you've got some weird function that is, you know, non non monotonically linear, then go ahead, write a neural net and estimate the parameters.

03:05:14.360 --> 03:05:16.360
You might not want to do like MCMC sampling.

03:05:16.360 --> 03:05:18.360
That's a little too much.

03:05:18.360 --> 03:05:26.360
You might want to bust out the variational inference tools that we have, but in time see three, but, you know, it's all possible.

03:05:26.360 --> 03:05:28.360
It's all totally possible.

03:05:28.360 --> 03:05:34.360
So, all right, that's it for the arbitrary curve regression notebook.

03:05:34.360 --> 03:05:41.360
Do we have any questions before we go back into doing estimation one more time?

03:05:41.360 --> 03:05:54.360
What you have to do, let me see if I can pull this off here.

03:05:54.360 --> 03:06:00.360
You want to see not just the single regression line, but the full family of them, right?

03:06:00.360 --> 03:06:05.360
Yeah, all right, this is going to test my live coding abilities.

03:06:05.360 --> 03:06:16.360
Trace dot bar names.

03:06:16.360 --> 03:06:18.360
Cool.

03:06:18.360 --> 03:06:25.360
One way to do this is to plot what T would look like first.

03:06:25.360 --> 03:06:34.360
So T is NP dot LIN space from zero to 800.

03:06:34.360 --> 03:06:40.360
Okay.

03:06:40.360 --> 03:06:46.360
And then you'll want to write the equation out.

03:06:46.360 --> 03:06:54.360
The equation is this guy.

03:06:54.360 --> 03:07:02.360
I'm going to put this down here.

03:07:02.360 --> 03:07:15.360
So we return that.

03:07:15.360 --> 03:07:35.360
And then the trace will have, if we inspect trace of A, it's a vector, it's 2000 long, 8000 long.

03:07:35.360 --> 03:08:04.360
I hope I've done this before, but I just have to do this correctly.

03:08:04.360 --> 03:08:13.360
In any time, I'm going to have you, we'll talk afterwards, and I'll put that, I'll be sure to put this on the notebook so that everybody benefits from this question.

03:08:13.360 --> 03:08:23.360
I think it's a great question because I've done this before, I just like am blanking on live coding, but I'll get that up there for you guys.

03:08:24.360 --> 03:08:27.360
But there's some form of broadcasting that we need to do, right?

03:08:27.360 --> 03:08:36.360
There's like X over tau needs to be broadcasted into a matrix and then we plot each of those rows of the matrix, but I'm not sure how to do this right now.

03:08:36.360 --> 03:08:39.360
So we'll work that out later.

03:08:39.360 --> 03:08:43.360
Let's come back to estimation before we wrap up.

03:08:43.360 --> 03:08:46.360
So with estimation, we're going back to notebook number four.

03:08:46.360 --> 03:09:04.360
I'd like to invite you to open up notebook number four, and all I'm going to do is rather than code with you, I'm going to show you another, show you this example is basically another case study where we've got information from two groups,

03:09:04.360 --> 03:09:08.360
but now we have this third group for which we don't have enough information.

03:09:08.360 --> 03:09:12.360
We want to be able to make reasonable inferences on it.

03:09:12.360 --> 03:09:18.360
So I'm going to use the instructor version.

03:09:18.360 --> 03:09:33.360
Whoops, rather than the student version, and I'm just going to run run down to about here first.

03:09:33.360 --> 03:09:37.360
Okay, so we've got data.

03:09:37.360 --> 03:09:45.360
We always love to have data.

03:09:45.360 --> 03:09:52.360
And for educational reasons, what I did, I took the liberty of adding in an extra species that was unknown.

03:09:52.360 --> 03:09:59.360
We know it's a Finch, but we've never, we've never really measured it, but and it's so rare, we've only got one measurement.

03:09:59.360 --> 03:10:08.360
So we're going to make inferences on, well, what's the, what do we expect to know about this new Finch's beak depth, right?

03:10:08.360 --> 03:10:11.360
We've been measuring the beaks depth and their length.

03:10:11.360 --> 03:10:15.360
What do we expect to know?

03:10:15.360 --> 03:10:20.360
So under this case is like, damn, we have like one measurement.

03:10:20.360 --> 03:10:27.360
There's no way we can even compute a standard deviation on this one independent measurement, right?

03:10:27.360 --> 03:10:30.360
We estimate uncertainty in this case.

03:10:30.360 --> 03:10:46.360
And this is the sort of scenario where a hierarchical model can be helpful in exactly the same way that it was helpful for those baseball players who had only one at that and no other data than that one at that.

03:10:46.360 --> 03:10:56.360
Okay, so if we think about the data generative process, we'll say something like, oh yeah, our beaks, maybe they are student T distributed.

03:10:56.360 --> 03:10:57.360
Why student T?

03:10:57.360 --> 03:11:02.360
It's because student T is the generalization of the normal and the Cauchy.

03:11:02.360 --> 03:11:06.360
The Cauchy distribution, the student T distribution has this degree of freedom parameter.

03:11:06.360 --> 03:11:11.360
This degree of freedom parameter controls how high or how fat the tails are.

03:11:11.360 --> 03:11:17.360
Normal distribution has really, really low, low tails, low probability density on the tails.

03:11:17.360 --> 03:11:22.360
The Cauchy distribution has really high probability density on the tails, relatively speaking.

03:11:22.360 --> 03:11:31.360
So the student T distribution says that when degree of freedom is one, it's the Cauchy, and when it's infinite, it's the normal.

03:11:31.360 --> 03:11:38.360
And everything else in between is controlled by this degree of freedom parameter.

03:11:38.360 --> 03:11:43.360
So we might define a student T likelihood.

03:11:43.360 --> 03:11:58.360
If we do an independent model, we'll get these posterior distributions on the beak depth, right?

03:11:58.360 --> 03:12:00.360
And it's on the mean beak depth.

03:12:00.360 --> 03:12:07.360
And this is kind of like where this independent model is really not the right place to be.

03:12:07.360 --> 03:12:08.360
So think about it.

03:12:08.360 --> 03:12:27.360
We've got values that can range from 0 to 15, where we know that finches generally are constrained maybe more towards 4 to 9 or 4 to 11 or something like that, right?

03:12:27.360 --> 03:12:28.360
I forgot.

03:12:28.360 --> 03:12:30.360
This is centimeters and millimeters.

03:12:30.360 --> 03:12:32.360
But you get the point, right?

03:12:32.360 --> 03:12:40.360
This independent model doesn't really have that borrowing of information from the known species to help us constrain our estimates.

03:12:40.360 --> 03:12:44.360
So I'm going to throw this on the right-hand side here.

03:12:44.360 --> 03:12:46.360
Keep this one in mind.

03:12:46.360 --> 03:12:48.360
This is the independent model.

03:12:48.360 --> 03:12:55.360
Now, if we fit a hierarchical model, and it looks something like this guy, right?

03:12:55.360 --> 03:12:56.360
Similar syntax.

03:12:56.360 --> 03:12:57.360
Nothing fancy.

03:12:57.360 --> 03:13:16.360
We have our priors and everything, and we have the broadcasting that's happening going on, just like in the independent model, except now we have prior distributions on the parameters of our distributions for the like, on the distributions for the parameters of our likelihood function.

03:13:16.360 --> 03:13:17.360
Okay?

03:13:17.360 --> 03:13:19.360
That's a bit of a mouthful, but I hope you get the point.

03:13:19.360 --> 03:13:22.360
There are hyper priors that exist.

03:13:22.360 --> 03:13:32.360
If we do sampling, now, ooh, sorry, I'm going to instead throw this up on the right.

03:13:32.360 --> 03:13:35.360
There we go.

03:13:35.360 --> 03:13:38.360
This is the one we want.

03:13:38.360 --> 03:13:39.360
Oh, okay.

03:13:39.360 --> 03:13:43.360
Maybe it's better on the bottom.

03:13:43.360 --> 03:13:51.360
And if you look at this guy over here, throw this one up here.

03:13:51.360 --> 03:14:03.360
Okay, so down on the bottom is our posterior distribution estimates for the independent model.

03:14:03.360 --> 03:14:04.360
Okay.

03:14:04.360 --> 03:14:12.360
And up at the top is the posterior distribution estimate for the hierarchical model.

03:14:12.360 --> 03:14:20.360
Which one looks more reasonable for this unknown species that we're interested in?

03:14:20.360 --> 03:14:33.360
This might take a bit of prior knowledge, but then you think about the 95% posterior density values.

03:14:33.360 --> 03:14:35.360
These are pretty extreme.

03:14:35.360 --> 03:14:38.360
These are quite extreme for the problem at hand, right?

03:14:38.360 --> 03:14:45.360
Finch peaks that are like zero centimeters are really close to zero, not so believable.

03:14:45.360 --> 03:14:55.360
In this case, because we have only a single measurement, the math works out such that our smallest beak size will, you know, in the 94% density will be at 0.9.

03:14:55.360 --> 03:15:00.360
Still might be unreasonable, but if you look at where most of the credibility is associated, it's out here.

03:15:00.360 --> 03:15:09.360
Whereas on this side, well, yeah, most of the credibility is associated out here, but there's still lots of credibility assigned like at really low values nonetheless.

03:15:09.360 --> 03:15:16.360
So qualitatively speaking, it still doesn't really make sense, right?

03:15:16.360 --> 03:15:20.360
Given the background prior knowledge that we've had.

03:15:20.360 --> 03:15:21.360
Okay.

03:15:21.360 --> 03:15:26.360
Okay, so that's all that I really wanted to say about this particular model.

03:15:26.360 --> 03:15:30.360
It was intended, so you can do this at home.

03:15:30.360 --> 03:15:40.360
It's intended, this exercise is intended as, you know, can I build a model for the data that is now not following the binomial story?

03:15:40.360 --> 03:15:49.360
Because we really, really harped on the binomial story to illustrate these other things, hierarchical models, vectorization of probability distributions, and the likes.

03:15:49.360 --> 03:15:59.360
Okay, so we really harped on that, but here, here, this case, you can get some practice with, you know, something that's t-distributed or normally distributed and try out other problems for yourself as well.

03:15:59.360 --> 03:16:01.360
Try out the other probability distributions.

03:16:01.360 --> 03:16:04.360
Okay, so that's also really helpful.

03:16:04.360 --> 03:16:11.360
All right, so the final thing that we have to do is I'm going to have Raveen and find a few helpers.

03:16:11.360 --> 03:16:19.360
I've got these little cards that congratulate you for taking this tutorial and sticking all the way through to the end.

03:16:19.360 --> 03:16:21.360
So this is my little way of saying thank you.

03:16:21.360 --> 03:16:23.360
At the same time, I have a little ask as well.

03:16:24.360 --> 03:16:39.360
There is a survey that we have on the readme of the GitHub repository, or if you prefer to use your phone to do it, there's a QR code on the back of the congratulations card.

03:16:39.360 --> 03:16:46.360
I'd like you to fill out that form to tell us where, where we did well on this tutorial, where we could improve it.

03:16:46.360 --> 03:16:53.360
Every generation of tutorials gets better and better, and it's all thanks to your feedback that we're able to do it.

03:16:53.360 --> 03:17:00.360
So while that's happening, I'm also happy to take questions, and then I have one final, final, final thing for everybody.

03:17:00.360 --> 03:17:12.360
So while that's going around, while you all are doing the surveys, I hope you all can open up the readme if you want to do it on your computer or scan the QR code if you're on your phone.

03:17:13.360 --> 03:17:17.360
Do you have questions on the material today?

03:17:28.360 --> 03:17:30.360
Okay, if not, then we'll continue.

03:17:30.360 --> 03:17:36.360
I'll just wait until I've got some form of quorum on like everybody being done.

03:17:39.360 --> 03:17:40.360
Yes.

03:17:42.360 --> 03:17:43.360
Sure.

03:17:56.360 --> 03:17:57.360
Right.

03:17:57.360 --> 03:17:58.360
Okay.

03:18:01.360 --> 03:18:02.360
Oh, cool.

03:18:02.360 --> 03:18:03.360
All right.

03:18:03.360 --> 03:18:09.360
That's, that's good for me to know because really tight.

03:18:10.360 --> 03:18:11.360
Okay.

03:18:16.360 --> 03:18:17.360
Yep.

03:18:17.360 --> 03:18:18.360
Yep.

03:18:18.360 --> 03:18:19.360
Oh, cool.

03:18:19.360 --> 03:18:20.360
Thanks a lot.

03:18:20.360 --> 03:18:21.360
I learned something today.

03:18:24.360 --> 03:18:25.360
Anything else?

03:18:39.360 --> 03:18:40.360
Yeah.

03:18:58.360 --> 03:18:59.360
Yeah.

03:19:01.360 --> 03:19:03.360
So let's see.

03:19:04.360 --> 03:19:17.360
The simplest, the simplest way to do this is actually to, to use the sci-pi stats module and, and use that to calculate the likelihood of data under your distribute, calculate, sorry, backtrack a little bit.

03:19:18.360 --> 03:19:24.360
We've always in, in our examples had data being basically like a data frame or like multiple rows of stuff.

03:19:25.360 --> 03:19:32.360
When we calculate the likelihood, it's really the sum of likelihoods over every single data point.

03:19:32.360 --> 03:19:43.360
And what happens when we're sampling, I think Revena and Colin will know this better than I would, but it, the mental model that I have is we're sort of, we draw a number from a, from our prior distributions.

03:19:44.360 --> 03:19:56.360
We, we assume that to be true and now fit it, put that into the likelihood, then compute the sum of likelihoods over all of our data.

03:19:56.360 --> 03:20:05.360
And then there's like this, this step that says, well, okay, given, given this thing that we've pulled out, the likelihood is this particular value.

03:20:05.360 --> 03:20:16.360
Now there's, you know, there's, there's gradient information that tells us which way to go and in, in the right place to sample that will now help us increase likelihood.

03:20:16.360 --> 03:20:19.360
Now, I want to be clear, this is not gradient ascent.

03:20:19.360 --> 03:20:21.360
Okay, this is not gradient ascent.

03:20:21.360 --> 03:20:28.360
And I've, in talking with Colin multiple times, I've actually made that mistake of thinking of it as gradient descent.

03:20:28.360 --> 03:20:32.360
So I don't want you to think of it as gradient, this gradient ascent at all.

03:20:32.360 --> 03:20:34.360
It's much more complicated than that.

03:20:34.360 --> 03:20:35.360
Right.

03:20:35.360 --> 03:20:39.360
But that's basically a glimpse into what's happening underneath the hood.

03:20:43.360 --> 03:20:46.360
That is the MCMC step that we're doing.

03:20:46.360 --> 03:20:51.360
We're like randomly sampling values from our, from our calculated posterior distribution.

03:20:52.360 --> 03:21:02.360
Sorry, I didn't, I didn't catch that.

03:21:02.360 --> 03:21:04.360
We can do it with simpler math.

03:21:11.360 --> 03:21:18.360
So the, the, who depends on how we define simple and complicated integrals are kind of complicated.

03:21:18.360 --> 03:21:24.360
And if we were not to use MCE methods, we would be doing integration and that'd be a bit of a pain.

03:21:24.360 --> 03:21:26.360
I think ravine had something to say.

03:21:26.360 --> 03:21:27.360
Yeah.

03:21:29.360 --> 03:21:38.360
So for that question specifically like the tutorial that I'm giving tomorrow, which is on GitHub has an entire notebook for that question of how MCMC works and diagnostics for it.

03:21:38.360 --> 03:21:40.360
So if you're in it, that's good.

03:21:40.360 --> 03:21:41.360
Otherwise you can come talk to me.

03:21:41.360 --> 03:21:47.360
I'll give you the link to the GitHub and we can talk through MCMC and is well, hopefully a medium amount of detail.

03:21:47.360 --> 03:21:48.360
Does that help?

03:21:48.360 --> 03:21:50.360
Yeah, yeah.

03:21:50.360 --> 03:21:51.360
Okay, cool.

03:21:52.360 --> 03:21:53.360
So.

03:22:16.360 --> 03:22:17.360
Yep.

03:22:21.360 --> 03:22:43.360
So I've done it before where we cheat and look at the data first and then try to see what distribution might be suitable.

03:22:43.360 --> 03:22:45.360
Well, this is mostly for the likelihood.

03:22:46.360 --> 03:22:50.360
Cheating basions are called empirical basions.

03:22:50.360 --> 03:22:55.360
So that's one way of approaching the problem.

03:22:55.360 --> 03:23:06.360
In practice, what happens is this will build a model and then it's all got it's got all the simplest things.

03:23:06.360 --> 03:23:08.360
It's it's normal.

03:23:08.360 --> 03:23:14.360
It's exponentials and like we're not thinking too hard about the mechanics of the problem.

03:23:14.360 --> 03:23:17.360
We're not thinking too hard about the details of the problem.

03:23:17.360 --> 03:23:18.360
We're not.

03:23:18.360 --> 03:23:25.360
We're sort of ignoring ahead of time what potential problems might show up in MC sampling and just running with it first.

03:23:25.360 --> 03:23:31.360
And then we'll encounter a problem with MC sampling, which often is an index.

03:23:32.360 --> 03:23:39.360
And then we'll encounter a problem with MC sampling, which often is an indication that like the model is kind of bad as well.

03:23:39.360 --> 03:23:42.360
Then we'll go back and think a little bit more carefully about it.

03:23:42.360 --> 03:23:57.360
So I've done these sessions at work where I start working on a model at just after lunch and I don't go home until 7pm because at 7 that's when like something that might look correct starts to show up.

03:23:57.360 --> 03:24:02.360
And even then I'm still not 100% sure that that model is the best model.

03:24:02.360 --> 03:24:09.360
However, I have a pragmatist in my head restraining me from going till 9pm.

03:24:09.360 --> 03:24:17.360
And it says, well, okay, you've got the key parameters of interest and you know their uncertainty to some degree.

03:24:17.360 --> 03:24:26.360
Go home, rest over it and maybe present it and someone else might be able to the peer review process at work then shows up.

03:24:26.360 --> 03:24:35.360
And I think if no one else can critique the model any further, we sort of all have to just agree that let's just run with it.

03:24:35.360 --> 03:24:37.360
Yeah.

03:24:47.360 --> 03:24:50.360
Yes, yes, yes.

03:24:50.360 --> 03:24:54.360
Yes, exactly.

03:24:54.360 --> 03:24:59.360
And if we're not sure about that, we change the distribution.

03:24:59.360 --> 03:25:06.360
There are positive only distributions that can be centered, you know, way out further out, right?

03:25:06.360 --> 03:25:14.360
I'm blanking right now on exactly which ones, but if you look at the PIMC3 distribution gallery, then you'll see those pictures and it becomes clear.

03:25:14.360 --> 03:25:29.360
Yeah, so there are these so-called improper priors.

03:25:29.360 --> 03:25:40.360
The flat distribution assigns, I forgot what likelihood it assigns, but it just assigns a single constant number from negative infinity to positive infinity.

03:25:40.360 --> 03:25:51.360
And then PIMC, there is machinery that lets you bound a distribution by setting its lower bound, upper bound, or both.

03:25:51.360 --> 03:25:53.360
And that's available.

03:25:53.360 --> 03:25:58.360
And yes, you can do that, though I think the pros say don't do it.

03:25:58.360 --> 03:26:04.360
Avoid the improper priors where you can, like it's not the best thing.

03:26:04.360 --> 03:26:11.360
The reasons why I'll have to dig, but the rule of thumb I've remembered is don't, like just avoid it.

03:26:11.360 --> 03:26:12.360
Yeah.

03:26:12.360 --> 03:26:19.360
Weekly informative priors that say things like, yeah, it's probably more close to zero, but I'm really not sure.

03:26:19.360 --> 03:26:24.360
Or it's probably centered around here, but I'm willing to give lots of uncertainty at first.

03:26:24.360 --> 03:26:31.360
Those are the general rules of thumb for selecting priors.

03:26:31.360 --> 03:26:35.360
Cool. Anything else?

03:26:35.360 --> 03:26:39.360
Okay, if not, let's do a very quick recap of what we went through today.

03:26:39.360 --> 03:26:45.360
There's a lot of material, but the core thing that I hope you take away are the following.

03:26:45.360 --> 03:26:55.360
Firstly, that probability itself, and you've seen it so many times here, it's nothing more than assigning credibility points to the number line.

03:26:55.360 --> 03:27:04.360
Where there's higher credibility points, we believe it more, and where there's lower credibility points, we believe that that parameter takes on that value less times.

03:27:04.360 --> 03:27:06.360
That's all it is.

03:27:06.360 --> 03:27:14.360
We saw how we can go from joint and conditional probability to Bayes rule and how that maps on.

03:27:14.360 --> 03:27:22.360
Marginal probability, joint and marginal and conditional probability are all really important for this.

03:27:22.360 --> 03:27:29.360
One thing I really hope you all take back is know your probability distribution stories.

03:27:29.360 --> 03:27:37.360
Super-duper important. If you know what their stories are, then picking them for your modeling work becomes much easier.

03:27:37.360 --> 03:27:40.360
Picking them becomes much easier.

03:27:40.360 --> 03:27:45.360
And so knowing what the continuuses are and what the discreet are, that's really important.

03:27:45.360 --> 03:27:52.360
Knowing their shape, their support, what story they tell that will help you in your modeling work.

03:27:52.360 --> 03:28:02.360
And finally, we went through one really simple example but built it up in depth, the binomial distribution story,

03:28:02.360 --> 03:28:12.360
and went along and showed how you can take a seemingly simple model and complicate it enough to fit the problem that you have at hand.

03:28:13.360 --> 03:28:16.360
Do you have more than one group? Well, vectorize the thing.

03:28:16.360 --> 03:28:21.360
Do you have some groups with lots of info and some groups that don't have lots of info?

03:28:21.360 --> 03:28:28.360
Well, use a hierarchical model and the mechanics of how we build these models, we reinforced over and over and over.

03:28:28.360 --> 03:28:37.360
And from the discussion, I noticed a lot of light bulbs going off, so that always makes me very happy to see.

03:28:37.360 --> 03:28:42.360
Cool. And with that, I'm going to say we're going to end here. Thank you all for coming.

03:28:42.360 --> 03:28:48.360
If you want office hours, I'll put them on the Slack. They will always be in the Tejas room in the afternoons.

03:28:48.360 --> 03:28:51.360
Exact time, see the Slack channel. Thanks a lot.

