start	end	text
0	7680	Good afternoon everyone. Welcome back to room 305. I see how we are full as normal. If you guys can
8320	13280	try to get into the middle to find a seat, you know, fire code and stuff like that,
14000	20720	that would be very, very much appreciated. This is the last talk in our Geophysics,
21600	27680	Geology and Atmospheric Sciences session, and it goes to Addison. And how do you pronounce
27760	33280	your last name, Addison? I'm going to leave that one with him and not even attempt it,
34080	39360	who's going to talk about interactive super computing with Jupiter and Dask. Thank you very
39360	49280	much, Addison. Thank you, Scott. Great. Thank you for having me here. This is my first sci-pi,
49280	57600	so I'm very excited to be here. So as Scott said, so my name is Anderson, I work as a software
57680	63760	engineer at the National Center for Atmospheric Research. So for those who are interested,
63760	70800	the slides available address. So this is a talk about Python tools that enable
72640	77120	interactive super computing, and we're going to see what we mean by super computing in a minute.
77200	88800	Okay, so this is Alice, and Alice is a project scientist at NCAR. She studies the transfer of
88800	96240	water and energy between the land surface and the lower atmosphere. And on the other side,
96240	102400	we have a cool notebook of how work. There are three things that are really interesting about
102400	109760	this notebook. So the first thing is that this notebook is not running on her computer. So this
109760	116080	is running on a supercomputer. In this case, this is Cheyenne. And as you can see, there's an address
116080	123120	to a Jupiter hub running on the Cheyenne supercomputer. The second thing is that she's using
124480	131440	hundreds of processes and terabytes of memory. All this is distributed computing resources.
132560	138800	Thanks to desktop queue. We're going to talk about that as well. And the third thing is she's
138800	145040	actually doing science. So this is not a toy example, so which is what she's good at. So
146000	152720	now we're going to talk about how we at NCAR enable Alice and folks like her to do interactive
152720	163920	supercomputing. So what do we mean by supercomputing? First, MPI, batch processing, lots of heavy
163920	169760	machines that most of people don't have access to as admins. So you have to talk to people to be able
169760	176480	to install some of the software. In the picture, so this is Cheyenne, which is a supercomputer
176480	184080	operated by NCAR. If you're ever in Cheyenne, Wyoming, feel free to stop by. They give free
184080	190880	tours of the facility if you want to. And what do we mean by interactive supercomputing?
192480	200240	So with the current growth in data creation, both through new simulations and new observations,
200240	206960	there's a growing need to have a more human in the loop workflow where people can rapidly prototype
206960	215120	and iterate through the data with tools like Jupyter notebooks. There's also a need to do
215120	220800	things like in situ data analytics. So instead of having to run the simulations and output the data,
220800	227120	you could actually run the simulation and do the analysis at the same time. And the other thing
227120	233840	is actually adaptive scaling of computing resources. So this combination would really be powerful,
233840	242880	but it's really hard for different reasons. So some of these reasons are cultural and others
242880	251200	are technical. So the first one is that HP systems tends to be unique. So every HPC center has its
251200	256960	own policies when it comes to things like security, what can you run on the supercomputer.
258880	265280	And the second point is the tension between interactivity and machine utilization. So when it
265280	274320	comes to HPC systems, HP center stands to be measured according to how often the machine is
274320	280400	actually utilized. And when you're talking about things like interactivity, users want to have
281440	288240	resources on demand whenever they want them. But because HP system operate on batch Q systems,
288240	293280	it's not that easy to actually get resources whenever you want to. So no user is going to
293280	300320	wait for five hours to get into the queue to do interactive data analysis. And the third point
300320	305840	is the lack of elastic scaling. And this is a huge, at least in my opinion, I think this is a huge
305840	311760	disadvantage in that if someone, let's say, want to do some analysis, they have to take time and
311760	317040	actually think about how much resources they want before they even do the analysis. And what ends
317040	324560	up happening is that you get into your work, maybe five minutes later, you probably take like 20
324560	329200	minutes thinking, what do I do next? And during that time, the resources are just idle. So it would
329200	335520	be nice if you could actually scale down later that people use those resources and get them back
335520	343280	whenever you actually need to. The good news is that, despite all those challenges, there's a
343280	350640	growing list of technologies or tools that try to help with interactive supercomputing. And I'm
350640	358000	going to talk about some of these tools. The first one is Trubiter. So we all love and use
358000	363840	Trubiter. And so I'm not really going to spend much time on it because now everybody is familiar
363840	372640	with it. And some of you may be wondering, but isn't Trubiter already usable on HP systems? And
372640	379200	the answer is actually, yes, but you have to go through all these steps. You have to essentially
379200	386240	tune the machine. You have to set up SSH tunnels. And you have to do this every single time that
386240	392960	you want to actually use the Trubiter notebook on the supercomputer. And this can actually be tedious,
393040	401040	especially for new users. So what is missing? So one of the things that is actually missing is
401040	405920	the multi-user support. So all those steps, everybody has to go through them individually.
405920	410240	And it would be nice if there was one single place that everybody goes to and everything is
410240	416720	done for them. Another thing is actually the lack of pure web access to HPC resources. Because as
416720	423760	you've seen, we're setting up SSH tunnel, which if you ever need to use another process that probably
423760	427840	runs on a different port, so you also have to SSH to create a tunnel for that port as well.
429760	437680	Well, then Trubiter Hub comes to the rescue. So one good thing about Trubiter Hub is that,
437680	444800	in a way, it provides a standard way of managing authentication. So one of the issues that is not
444800	452480	really hard to convince sysadmins about is the security of web applications. But Trubiter Hub
452480	457600	makes it easy in that it doesn't really force you to use any type of authentication. It's up to you
457600	462960	to choose what you want to use for authentication. In this case, on the supercomputer, you have
462960	468640	different types of authentications. And another thing is that Trubiter Hub will just take care of
468720	476080	spawning the Trubiter notebook single server and giving access to the user on demand.
478720	485920	And so let's now switch gears and actually see how this works in practice. So I will skip these
485920	494080	few slides and actually go to the live demo. So the first thing that you do as a user, so you just
494080	507680	go to Trubiter Hub.ucr.edu. And so this is what you presented. And if you've used the
507680	512400	Trubiter Hub before, this looks familiar. The only difference is that in this case,
513280	519600	I have to use the same authentication that I used to SSH into the machine. So in my case,
519600	526160	I just provide my username. And then for the password, it's a combination of a PIN number
527040	537040	and a UB key token. So now, so now I'm authenticated. So I then get this page,
537040	543040	which basically asked me for the project account. So this is the allocation on the
543040	549600	supercomputer so that they know who to charge for this usage. So in this case, I'm going to
550800	558960	provide when to change the queue here and specify the project
562080	570480	and maybe just reduce the wall time. And then once this is done, I just click on spawn.
571440	576480	And what this is doing in the background, it's basically submitting a job to the queuing system.
577120	586480	And once the Trubiter Notebook server is ready, I will get redirected to this new page. And at this
586480	594480	point, thank you. So at this point, you have the same interface as what you would have on your
594480	602720	laptop. So I can run notebooks, in this case, to show you that I'm not really lying. So
606560	612480	you could just run this notebook, which is backed by Bashkarno. And as you can see,
612480	616400	I don't really have 200 terabytes of storage on this computer. So
617360	630800	that's it about Trubiter Hubs. I'll come back to it later. So the next tool is Dask.
633680	636800	Is there anybody in this room who is not familiar with Dask at this point?
636960	646400	Okay, a few people. So at this point, most of us are familiar with Dask.
647600	653440	We've probably interacted with it in the cloud or even on our local machines. So I'm not really
653440	659520	going to spend any more time talking about Dask. The one key point that I'm going to focus on is
659520	668480	how to deploy Dask on HPC systems. And this is where Dask JobQ comes in.
669600	677760	So Dask JobQ is a Python library that allows you to easily deploy Dask on JobQ systems,
677760	686400	such as PBS or Slurm, and so many other. It was created as a spin of the Panjio project.
687200	692960	It provides a high-level Python user interface to manage Dask clusters and Dask workers.
695040	700000	So for instance, if you're like on a system that uses PBS as the queuing system,
700000	705040	so this is what you have to do. So from Dask JobQ, import the PBS cluster class,
705600	710080	and then instantiate that class with things like the project account, the queue,
710880	717120	and all other resources that you want. And I should be clear that I'm not really defining
717120	723200	everything that I want in my cluster. At this point, I'm just telling Dask a configuration of
723840	729440	what to ask to the queuing system every time that I want computing resources. So in this case,
729440	736960	I'm saying every time they just submit, just in this single job, submit, ask for one process
736960	745680	and one thread and 100 gigabytes of memory. And once that is done, you can minority scale it by
745680	751520	basically saying just scale to 10 nodes, in this case, that corresponds to 10 Dask workers.
752240	758720	Or you could actually tell it to scale adaptively by saying, okay, I want you to,
759840	764480	at all time, to have a minimum of one Dask worker, and you can scale between one
764480	771040	and 100 Dask workers. And Dask will basically monitor your usage of your CPU usage or your
771040	774960	memory usage, and then it will know that it should get more resources. And at any point,
774960	780080	if you're not using those resources, it will just tell the queuing system to kill those jobs.
781600	788080	So if you're on a system that uses SLRM instead of PBS, it's the exact same thing with only one
788080	791840	difference of just using the SLRM cluster instead.
794800	802800	Okay, so now let's go back to Tributor and actually see what Alice is actually doing in that notebook.
806880	812720	Okay, so there's so much science going on in these notebooks, I won't really spend so much
812720	818880	time going through the details about it. But if you're interested, there's a copy of this notebook
818880	826000	that you can actually run in the cloud. It's actually exact same copy. So if you go to the
826000	832320	Panjio data GitHub organization and just go to the Panjio tutorial, you should see
833600	840880	one of these notebooks there. But I'm going to focus on a particular data set here,
840880	846400	which is the grid ensemble precipitation and temperature estimates over the contiguous
846400	853920	United States. So this consists of 100 ensemble members for precipitation and temperature data.
855040	859200	So the first thing that I do, I input some packages.
865120	865840	Is this better?
865920	873200	Okay, good. So the next thing that I do, I now
874960	881840	create my cluster object by telling that job queue that I want 109 gigabytes of memory,
882800	892400	36 threads and 36 processes, specify the queue and the wall time. And I tell Dask to scale between
892640	902880	360 Dask workers. And I don't know what that is, but you get just so. And when I click on this,
902880	910480	as you can see, at this point, Dask is submitting a bunch of, Dask job queue, submitting a bunch
910480	919040	of jobs to the queuing system. And now I'm not trying to get into workers. So as you can see,
919040	927680	I have thrown a 60 workers as I specified. Let me bring up the dashboard here.
940400	942720	Okay, let me one more thing.
943680	956560	Okay. So now, now I can actually start doing the computation. So but the first thing I'm going to
956560	963920	do is actually connect my client to the remote workers. So I have a cluster with 100 with one
963920	973600	terabyte of memory and 360 workers. So the dataset is saved in stored as in ZAR format. So
973600	982400	there was a talk two days ago about ZAR. If you've never had about ZAR, recommend to go and check
982480	998240	it out. And I use XR8 to open this ZAR store. Again, just took 187 milliseconds. I didn't
998240	1003360	really do anything other than just looking at the metadata. So we can look at the size of the
1003360	1015120	dataset. It's close to 1.7 terabyte. Can look at some metadata. So we have precipitation as one
1015120	1022400	of our variable, the mean temperature and the temperature range. And these are all 40 variables.
1022400	1030160	So 100 ensemble members for these many days, which corresponds to close to 40 years, I think,
1030240	1036560	of data. And if you wanted to, you could actually look at the Dask array. If you actually wanted
1036560	1043120	to look at the shape and things like that, if this is more useful to you. So the first thing I'm
1043120	1051040	going to do is actually just do a quick plot for the elevation. This is basically a mask that tells
1051040	1057520	me the elevation for each of the grid points. And as you can see, towards the west coast,
1057520	1062400	you have points with higher elevation compared to the east coast of the country.
1063600	1069600	So let's now try to quantify the ensemble uncertainty for a single day. So just select the
1069600	1080080	data for one day and try to quantify the uncertainty for that one day. Again, this just goes very
1080080	1086560	fast. Nothing really happened there other than just that Dask constructed the task graph. And when
1086560	1092880	I do the plotting, that's when actually the computation gets triggered. Then I have my plot
1092880	1102800	here. And again, I won't go into the details about the science. So the next task, let's try to
1102800	1113040	compute the intra ensemble range. Again, as you can see, this just returns quickly. But this is
1113040	1120560	actually a delayed operation or a lazy operation. And now I can actually tell Dask to do the
1120560	1131200	computation. So now you can actually start seeing some activity here. This should be done anytime
1132160	1147280	soon. Okay, so this has to do with Dask resiliency in that if, for instance, Dask tries to do a
1147280	1156320	computation and it fails, it would start marking that computation. And I think by default, if it
1156320	1162320	tries three times and hasn't actually been able to successfully complete that computation, Dask
1162320	1168240	would just give up on that particular computation. So you could actually specify how many times you
1168240	1173760	want Dask to try. Like, for instance, if a task was scheduled on a worker and the worker dies,
1173760	1179120	what to do? Or if you run out of memory or things like that. So this is basically
1179120	1188160	a cool feature of Dask, the resilience of Dask. So the computation finished. We can actually do
1188160	1201440	some plotting here. Okay, again, not that many details about science. So the next task is let's
1201440	1213200	try to compute the average seasonal fall, a snowfall. In this case, we're actually just computing on
1213200	1222640	only four ensemble members. So if you go through the distributed documentation, there's a really
1224800	1230480	cool information about how to interpret the dashboard here, the information about the dashboard.
1231520	1234320	So this is done. Now we can do the plotting.
1240560	1248560	It's going to take a few seconds. And as you can see, so as the year progresses,
1248560	1255040	so the amount of snowfall decreases. And I think this is in the summer, you don't even have any
1255040	1262240	at all in most part of the country. And this is consistent across the four ensemble members
1262240	1270560	that we're looking at. So let's now actually do something cool. Let's actually look at like a
1270560	1280080	specific region. In this case, let's just look at all the grid points near Austin. So XRA is really
1280080	1285200	cool in that you can just give it the coordinates of the points you're interested in. In this case,
1285200	1291760	we provide a buffer so that we can actually get all the grid points in that range. And we're going
1291760	1304080	to compute the maximum precipitation near Austin for the last 40 years. So it's going to take a few
1304960	1322800	seconds. And then we can do the plotting once this is done. So again,
1322800	1334400	we can look at our cluster object. I still have 360 workers. I could have started with
1334400	1340640	something really small, but it's just that I wanted this to go really fast. And you never know how
1340640	1345440	busy the queuing system is going to be. So if you need the resources, you get them when you can.
1346000	1355120	Okay, so this should be done. And basically, so this is the maximum precipitation near
1355840	1364480	Austin, Texas for the 100 ensemble members for the last 40 years. So you could basically look at
1366800	1372720	what that looked like. So yeah, so at this point, if I basically don't do anything,
1372720	1378320	because I told us that I want a minimum of 360 workers, it would just keep those resources. But
1379360	1384480	instead, you could actually now that I'm done with this, I can actually do
1390880	1392880	let me just tell it to scale down to
1393040	1403840	this. And basically, what that's going to do is going to start monitoring the workers. And if I'm
1403840	1409040	not using them, we'll just take them away. So I will come back to this later to show you what I mean
1409040	1419200	by that. Okay, so, so we've seen, or at least I try to demonstrate the adaptive or the elastic
1419200	1425760	scaling and the resiliency of dusk. And let's now talk about some of the challenges. So being
1425760	1430400	able to know how much resources you need before actually doing the computation is really hard.
1430400	1436720	And they actually requires quite a lot of experimentation. And if you actually get to
1436720	1442000	know how to do this for one particular workflow, it probably changes once you move to a different
1442000	1448240	dataset. So, and another thing is that the computation, the computational workloads, they
1448240	1452720	don't really are not really constant. So you probably started with one terabyte of data.
1453360	1458800	Five minutes later, you probably only have one gig of data left. At that point, you don't really
1458800	1462640	need all the resources that you started with when you when you're dealing with one terabyte of data.
1463200	1470080	So the good thing is that dusk thinks about these things. So how to scale up and down,
1470080	1475520	how to be resilient in case like a worker dies, and what if when you get new workers,
1475520	1478560	what to do in terms of load balancing and things like that.
1480320	1484000	So what is the solution? Basically just start a Jupyter notebook,
1484800	1490480	instantiate your dusk cluster, and then just let dusk do the scaling up and down for you.
1490480	1501920	And you just focus on the science. And what are the benefits to HPC systems for elastic scaling?
1502800	1508240	One of them is that it actually improves the occupancy of the machine in that if the resources
1508240	1513520	are idle, then dusk knows how to release them so that other users can actually use them.
1513840	1521120	And another thing is that with the resiliency, you can easily, for instance, if you started
1521120	1528080	with 120 workers, for some reason, your worker dies, dusk will know how to get a new worker.
1528080	1533280	And in a way, if you think about something like MPI where you start, let's say you have like 120
1536000	1542080	workers, if one thing goes wrong in an MPI environment, the whole thing dies. So in a way,
1542080	1546480	you kind of lose all the work that you had done, which is probably not that nice.
1547520	1555200	So basically, another thing, to the same point, dusk thinks about these things.
1555200	1560320	And I think you should also think about what you get from this as well.
1561680	1566480	Again, so not all jobs are interactive. So once you're done with your interactive
1566480	1572640	workflows, there's this other package called dusk MPI, which actually now allows you to go on and
1572640	1579360	actually launch but jobs in case you don't need to do interactive exploration.
1580640	1596320	And looks like I'm running out of time. So that is pretty much it for today. Thank you.
1597200	1601840	And like I said, now you can see that it took away those resources.
1604720	1605520	Okay, so here we go.
1613120	1616480	Let's try here. So do we have any more questions?
1616480	1630640	Hi, great talk. How do you deal with large datasets? And if you need to import like large
1630640	1634080	datasets, can you do it? Can you put them on the cluster?
1636320	1643840	So in this case, the Trubeta Hub is actually running on the same file system.
1644080	1650240	So which basically means that we don't need to move the data around. So in our case,
1650240	1655280	it kind of works in that basically we're moving the computation where the data is.
1655280	1661680	So we don't need to move the data around. So I haven't run into cases where we need
1661680	1669040	to move the data. So if anybody has run into that, then they could probably provide a better answer.
1669520	1672720	Other questions?
1680720	1688960	Yeah, it's the Trubeta Hub instance taking a pre-allocated resources like 20 loads,
1688960	1691920	and then you scale up and down inside the Trubeta Hub?
1692880	1700800	No. So if you remember what I did when I logged in to the Trubeta Hub, I asked for I think just
1700800	1706160	one process. So Trubeta Hub is actually running, the lab itself is running in its own job.
1707200	1713760	So when I do the scaling up and down, I'm actually doing that as independent jobs.
1714560	1716080	So that's why I'm able to do that.
1716800	1722400	But I mean scaling down is easy, but when you're trying to scale up, in our class, you need to
1722400	1729360	wait 30 minutes to have a new job created for you. So yeah, that's what I say that when it comes to
1730240	1737360	queuing systems, I think that lack of elastic scaling kind of makes it hard to actually do
1737360	1741840	interactive computing in that you can't wait for two hours to get into the queue.
1742800	1745680	In my case, I was able to get into the queue very easily because
1746960	1755200	I had to ask a special reservation. So yeah.
1755920	1758880	All right, we're going to have a lot of people filing in and out here because we're changing
1758880	1765200	topics, so can we please thank Anderson again and all the speakers for the Geophysics Imposer.
