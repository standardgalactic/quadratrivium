start	end	text
0	2960	Thank you all for coming to the tutorial.
2960	5920	If you noticed in the SciPy program,
5920	8320	there are three tutorials doing Bayes stuff.
8320	12120	This is, I think, unprecedented in the SciPy tutorial
12120	14160	program list.
14160	18400	What happened was the three lead instructors, that
18400	22200	is Alan Downey for his tutorial, myself and Hugo
22200	24360	for this tutorial, and Ravine and Colin
24360	27240	for the next tutorial on Bayesian modeling,
27240	29840	we all decided to informally coordinate together
29840	32160	to put this informal Bayes track thing.
32160	36840	And I know that there are a few people who have actually
36840	38600	registered for all three of them.
38600	42880	So props to you guys, that handful of you.
42880	48800	Anyone want to take an estimate on what fraction of this crowd?
48800	51240	Did that?
51240	55480	20%, which would mean about, let's see, 16 people-ish?
55480	56360	15 people-ish?
56360	59480	I think they're about 60 to 80 people registered in this room.
59480	60440	Yeah, so OK.
60440	62880	Yeah, I actually got the actual number from Jill.
62880	66720	It's 15 of you, so it's a good, close estimate there.
66720	74840	All right, so for the 15 of you who were in Alan's tutorial,
74840	78320	some of this material will be familiar to you.
78320	81800	And for the rest of you, we'll go through enough background
81800	84080	to accomplish today's goals, which
84080	86920	is to show you how you can write arbitrary statistical
86920	90400	models and use them to perform inference.
90400	93560	That's the end goal of today's tutorial.
93560	96320	And there'll be a tool of choice, which is Pi MC3.
96320	98560	We'll also show some things with NumPy.
98560	101520	Hopefully that will give you enough grounding
101520	105960	in probability, in Bayes rule, to then be
105960	108520	able to talk about how our data were generated
108520	111440	from a statistical standpoint and build those models
111440	114840	and use them, use them productively.
114840	119240	OK, some administrative matters before we move on.
119240	122320	For those of you who are still not set up,
122320	124480	I know that there's a high probability
124480	126440	that you are set up, set up being defined
126440	128960	as you either have Binder working
128960	130760	or you have everything cloned locally
130760	133200	and working on your laptop or on a remote server
133200	137200	that you control, which is what I'm doing, by the way.
137200	140080	So yes, I know what the feeling is like.
140080	145040	So if you are set up and you're using Binder,
145040	147400	remember every 15 minutes on the clock.
147400	150800	So make sure you execute something in your notebooks
150800	154440	because Binder sessions have a 20-minute timeout.
154440	157360	If you are not already set up, go to the Read Me page.
157360	159760	Go to the Read Me on the GitHub repository.
159760	161840	Look for the Launch Binder button and hit it
161840	165440	because you're going to not want to waste time with DevOps
165440	167720	at this point.
167720	169600	Cool.
169600	170360	Let's see.
170360	174400	We also have Ravine Kumar, who is one of the instructors
174400	176360	for the next Bayes tutorial.
176360	180280	He is helping out today and he'll be back in a few minutes.
180280	186280	We can take a guess at a time that he'll be back.
186280	191160	You'll notice I'm using a lot of these guess the time
191160	193760	estimate that sort of terminology.
193760	198520	It's very important that we've got both a theoretical view
198520	199840	and a practical view on that.
199840	203960	So hopefully today's tutorial will clarify that.
203960	206120	OK.
206120	209800	Before we go on, do people have any questions, any issues
209800	214280	that they want to clarify, any things that are blocking them
214280	216680	from doing today's tutorial?
216680	224200	OK, if not, we're going to follow the agenda that's
224200	225320	on the whiteboard.
225320	228000	First off, we're going to do a recap on probability.
228000	230760	Use some hands-on exercises to make sure
230760	233320	that you've got a grasp on it.
233320	235720	And then we'll recap what Bayes rule is.
235720	239280	So for the 15 of you who were in Ellen's tutorial this morning,
239280	241360	this should be familiar material.
241360	244360	We won't do a full rigorous proof of Bayes rule.
244360	250400	And we'll just make sure that everybody has a grasp on that.
250400	253440	And it builds on top of the rules of conditional probability
253440	255520	and joint probability.
255520	257280	So once we're done with that, then
257280	261240	we'll move on to what we would call the core activities
261240	265960	of statistical inference, which is estimation.
265960	270480	Estimation is the core of all statistical inference.
270480	271880	Because we're in a Bayes tutorial,
271880	274400	I'm going to go out on limb and saying calculating p values
274400	275800	is not.
275800	277920	Calculating p values is not the core activity.
277920	280640	It's not even the point of statistical inference.
280640	282880	So let's get that out of our heads.
282880	284120	And then the final thing is we're
284160	291240	going to show how the core activity of Bayesian statistical
291240	293720	inference, which is Bayesian estimation, how that relates
293720	298640	to things that we might have now already learned
298640	302080	in our undergrad statistics or grad statistics classes, which
302080	304680	is different forms of regression and the likes.
304680	308200	And we'll do a short teaser on that.
308200	311080	It's not the main point, because the main thing is really
311120	315040	understanding estimation and how we build models that
315040	317000	help us perform that.
317000	319320	So with that, I'd like to invite everybody
319320	322520	to open up your notebooks, fire up Jupiter,
322520	326560	and open up notebook 1A.
326560	331000	And then you'll notice that they always come in pairs.
331000	334280	So there's a student version and an instructor version.
334280	339280	And what I've decided, unlike my previous workshops,
339280	343040	I'm going to live code with you.
343040	344080	Not from memory, thankfully.
344080	345520	I have a second computer here that
345520	349200	tells me what the right answers are.
349200	353680	You all, by the way, if you ever feel stuck on anything,
353680	356280	the instructor versions of the notebooks are there for you.
356280	357840	And that's the first time I'm going to say it,
357840	359280	also the last time I'm going to say it.
359280	363040	So if you're ever stuck, just open the instructor notebook.
363040	366120	Don't hesitate to copy and paste the answers over.
366120	368560	The point is understanding.
368560	370520	Doing stuff helps with understanding.
370520	371840	But if you get stuck doing stuff,
371840	374400	it can lead to more frustration for learning
374400	375720	than is productive.
375720	377200	So at the end of the day, make sure you just
377200	380080	have a good conceptual view, and that's good enough.
380080	383520	All right, let's run the first cell
383520	387520	and talk about what probability is.
387520	389640	I had a few pop quiz questions just now.
389640	393720	What's the probability that someone in this room,
393720	398280	if you asked, would have a satisfying lunch today?
398280	400680	And someone said, 75% of this room.
400680	401640	That's another question.
401640	406480	What is the probability that someone in this room
406480	409360	has taken all three tutorials?
409360	410720	All three Bayesian tutorials.
410720	413400	And that's another question relating to probability.
413400	416560	But I'd like to start by first asking the question a little
416560	418320	bit more meta-level.
418320	421320	What is probability?
421680	426840	You have a volunteer, and I know no one's going to answer.
426840	429840	So I would like you to do is talk with your partner
429840	433960	for about 30 seconds to a minute, not 30 minutes.
433960	435280	30 seconds to a minute.
435280	436440	Introduce yourself.
436440	438000	Network a little bit.
438000	441680	And ask, what is probability?
441680	446960	OK, so hopefully you all know each other a little better.
446960	449320	Those of you who've been in the network analysis tutorials
449320	450640	that I've done know this trick.
450640	452520	This is the networking trick.
452520	456640	Let's you build your network while you're in a tutorial room.
456640	462840	So do we have volunteers to share maybe a stab at the question?
462840	465640	What would you define probability as?
465640	468080	We know some properties of probability, right?
468080	472080	Someone said it must be bound from 0 to 1.
472080	473240	What else?
473240	475360	What other things about probability do you know?
475680	486720	Volunteers, back there, a symbolic expression
486720	487400	of uncertainty.
487400	491000	Would you like to unpack that for us a little bit?
491000	492080	No?
492080	495120	All right, so then I'm supposed to do a stab at that, right?
495120	497360	OK, let me ponder that.
497360	498040	Do we have another?
499040	503880	Something.
503880	504480	Something.
504480	506440	OK, yeah, yeah, OK, cool.
510560	513520	Degree of belief, yep, yep, OK.
513520	514000	Mark?
528840	542800	Yeah, yep, OK, right, yep, OK.
542800	543520	Anything else?
543520	546320	Anybody else has a definition they'd like to contribute?
549840	550320	Pardon me?
556720	558000	Ah, OK, yeah, all right.
559040	566040	Cool, right, so what I'm hearing are essentially
566040	569640	the classical and the Bayesian, I refuse to say frequentist,
569640	571680	the classical and the Bayesian ways
571680	574960	of thinking about probability.
574960	577880	Hugo, who made the first part of this tutorial,
577880	581640	the material for the first part of this tutorial,
581640	583400	found a really great quote.
583400	585960	And it's inside your notebooks, and you'll see it, right?
585960	588480	By data analysis, by severe and skilling.
588480	593040	And there were essentially, historically,
593040	596560	there was a shift in how probability was viewed.
596560	600240	There was perhaps what we might call the first version that
600240	605240	was formally recorded in European scientific history,
605240	610440	which would be a degree of belief assigned to an outcome.
610440	615800	And then there was then a shift because of views,
616760	619080	foundational worldviews shifted.
619080	621440	And so people thought of probability
621440	626680	as more of the relative frequency with which things occurred,
626680	630720	which then gave rise to the name that I refuse to mention.
630720	635040	And because these frequencies can be measured,
635040	641680	they then seemed to form an objective view of reality, right?
641680	644680	I, on the other hand, don't subscribe to this view, right?
644680	647800	I see probability defined.
647800	651840	So there are formal definitions of probability.
651840	654720	You get into these very technical mathematical terms
654720	659880	that involve spaces and measure theory and the likes.
659880	664840	The way that I prefer to, for day-to-day use of Bayesian
664840	667880	methods and probability, I tend to think of probability
667880	671560	as just being money assigned on a number line.
671560	673920	If I gave you $100, how would you
673920	678360	also assign that $100 to points on the number line
678360	679560	if you were doing discrete things?
679560	682480	Or how would you draw a curve that would say,
682480	686160	assign how you would distribute money to the number line?
686160	687560	Essentially, it's a measure of how much you're
687560	691520	willing to bet that this thing will take on a particular value,
691520	693360	this thing that you're interested in, right?
693360	696000	So you can use money, or if you prefer not
696000	699640	to gamble like myself, then I would just
699640	702240	say credibility points assigned to the number line, right?
703120	705360	Credibility points assigned to the number line
705360	707880	gives us a view of probability that
707880	711160	is a good enough working definition
711160	717080	for how we can view probability in applied problems.
717080	723240	So where do we want to then use probability as a tool?
723240	726720	Once again, spend 30 seconds with your neighbors
726720	728560	and talk about this.
728560	729600	Let's gather back.
729880	733680	I'd like to hear from those who haven't already raised their hands
733680	736200	and contributed something to the discussion.
736200	740880	Where would you use probability in an applied setting?
740880	741720	At the back.
744720	745720	OK.
745720	747120	Did you elaborate on that, please?
751120	751620	Right.
760400	761680	Yes.
761680	762880	Yeah, OK.
762880	764520	Right, so probability is a tool for that.
764520	765400	Next door neighbor.
768720	769840	OK.
769840	770800	Did you give an example?
781680	784720	Right, so some key questions we might be interested in
784720	786200	would be like, what's the probability
786200	790520	of a catastrophic failure of that piece of infrastructure,
790520	792080	for example, or something like that?
802840	805640	Right, so calculation of, again, the probability
805640	807800	of a catastrophic failure would be really important,
807800	811880	and the probability that the safety mechanism would fail
811880	813840	as well would be also important.
816600	830840	Right, right, right, so you need to know the probability
830840	833120	that the rate is equal to something
833120	834600	plus the uncertainty on it.
834600	837760	I'm very happy that we're going in a Bayesian direction here.
837760	839560	OK, cool.
839560	844920	Great, so if, say, we were, for example,
844960	849040	in a marketing firm or a new tech firm,
849040	851840	and we wanted to think about click-through rates, right?
851840	854920	That's another place that probability comes into play,
854920	856880	so another applied setting.
856880	859560	And click-through rates are essentially
859560	862360	measured as the probability that a user that arrives
862360	864960	at your page will click on something, right?
864960	866880	So that's another example.
866880	870400	And that's the first example that we'll use here
870400	874240	to get a practical handle on how we
874240	878040	can use the existing tools in our toolkit.
878040	879160	We probably know NumPy.
879160	881800	We probably know a bit of pandas and the rest.
881800	882800	NumPy, Matplotlib.
882800	888240	We'll use NumPy to help us get a grasp on what exactly
888240	890560	is probability, and how does it relate to proportions
890560	891800	and the likes, OK?
891800	894600	So let's say, let's try this example together.
894600	896400	It's a code-along activity.
896400	900000	Let's say we've got a website, and we've measured the click-through
900000	904720	rate accurately to the ninth decimal place, right?
904720	909360	So it's 50% accurately to the ninth decimal place.
909360	914280	So what does that mean of the visitors that come by?
914280	917400	So if we had 1,000 visitors, how many people
917400	921160	would we expect to click on that button?
926160	927160	Any volunteers?
927160	928120	500, right?
928160	930240	That's the expectation, right?
930240	931640	We'll try to simulate that, OK?
931640	938560	So if we have one way to simulate this kind of problem
938560	941360	is to use NumPy, and to start, we'll
941360	944840	take a uniform distribution, right?
944840	947640	We'll start by saying, we've got a whole bunch of people.
947640	950160	We don't know where they came from.
950160	954200	And we'll simulate this process of clicking
954200	960440	by taking 1,000 random numbers bounded between 0 and 1,
960440	964720	and then cutting a threshold somewhere at 50%, right?
964720	971160	So we'll do something like np.random.rand, 1,000.
971160	975120	And what this will give us then is,
975120	980240	if we plot the histogram of that, ooh, my kernel just died on me.
980240	981960	Cool, all right?
981960	982760	Let me try that again.
991280	992840	We plot the histogram of that.
992840	995760	You should get something that looks like this, right?
995760	1000520	So this is what we would call 1,000 draws
1000520	1003440	with equal credibility assigned across the number line
1003440	1005400	from 0 to 1.
1005400	1012280	And to simulate how people click on the website,
1012280	1014240	we can do a few things.
1014240	1022000	We can do first ask how many of those values are below 0.5.
1022000	1025880	And by definition, the rest are going to be above.
1025880	1031920	And finally, simply sum up the total number of clicks
1031920	1032720	that we get, right?
1032720	1039160	We define clicks as a value being drawn from this distribution
1039160	1042080	being less than 0.5, and then we simply sum it up.
1042080	1044320	How many do people get?
1044320	1045840	I get 481.
1045840	1047720	What are the others?
1047720	1049440	4, sorry?
1049440	1054280	505, 478, et cetera.
1054280	1054920	OK, cool.
1054920	1058240	So we've got a variety of answers here.
1058240	1060360	And this is one of the core ideas
1060360	1063000	behind statistical inference.
1063000	1067080	That is, there is always some form of randomness involved.
1067080	1070520	And when there's some form of randomness involved,
1070520	1073520	we will expect the answers.
1073520	1075920	So we can calculate this thing called expectations.
1075920	1081120	And we expect that, on average, from, say, a lot of us
1081120	1085840	pooling our results together, the number of people who click
1085840	1092320	will be centered around 500 at 0.5 out of 1,000.
1092320	1095760	However, because of random effects and whatever else
1095760	1098280	that goes on in the data generating process,
1098280	1101160	we won't always get exactly 500.
1101160	1104000	There's always some randomness involved.
1104000	1107360	So then we can calculate the proportion
1107360	1114080	that clicked as basically n clicks over length of clicks.
1114080	1115760	And I will get 0.481.
1115760	1119240	And you will get your 0.505s and whatever.
1119240	1125720	So this is one of the, so one idea
1125720	1132280	that we want to convey here is that in statistical inference,
1132280	1134720	we're often talking about random processes, right?
1134720	1136240	Things that are not deterministic.
1136240	1138040	There's always a component that we
1138040	1142280	don't know how to exactly write an equation to model exactly.
1143160	1146720	And so what we do is we use a statistical model
1146720	1149840	to help us get around this fact, right?
1149840	1153600	And there's randomness built in inherently inside there.
1153600	1157800	So all right, so what we did was we said
1157800	1161400	we had a model, so-called model, of clicking
1161400	1164400	in which we said half of the people who come will click.
1164400	1167880	And then we drew samples from that model, right?
1167880	1171360	Where we had our own, on our own computers,
1171360	1174360	we had our own instantiations, our own realizations
1174360	1174920	of that model.
1174920	1178320	And there's some random effects that are inside there.
1178320	1183160	OK, so what we'd like to do now is
1183160	1185560	have you all do this on your own for the next two
1185560	1186800	to three minutes.
1186800	1189480	Try to simulate what the results will
1189480	1192320	look like when you have a click through rate of 0.7 instead
1192320	1193480	of 0.5, right?
1193480	1195800	So spend a minute or two handling this.
1195800	1199200	If you are ever stuck, you have lots of resources.
1199200	1200200	You have your neighbors.
1200200	1201760	So continue networking.
1201760	1204320	If not, you also have the instructor notebook.
1204320	1205600	And you have the screen, which I'm
1205600	1207280	going to be typing on as well.
1207280	1211000	Cool, so it'll look something like that.
1211000	1216640	Once again, not everybody will have the same results, right?
1216640	1220760	OK, so some numbers, just popcorn style this.
1220760	1225880	703, 694, I have 708.
1225880	1228640	What else?
1228680	1230240	687, OK, cool.
1230240	1234840	So we have what this forms is a distribution of numbers,
1234840	1236840	realizations.
1236840	1241760	Cool, this model that we've just simulated by hand
1241760	1245120	is known as the biased coin flip.
1245120	1247560	We know that the coin flip is the classic example
1247560	1251000	that every probability tutorial has to deal with.
1251000	1253800	The biased coin flip is just nothing more than a variant
1253800	1254760	on that.
1254760	1257120	Where else do you see biased coin flips happening?
1259040	1262080	OK, talk with your neighbor, 30 seconds.
1262080	1264520	Nobody will answer the first time on my first thing,
1264520	1267000	so do we have volunteers?
1267000	1269840	Where do we see the biased coin flip apart
1269840	1272720	from click-through rates?
1272720	1273920	Do we have a volunteer?
1277680	1278160	Mark?
1278160	1278660	Sorry.
1278660	1281120	My example's kind of got limitations,
1281120	1284560	but you're traveling on a tree, so traffic light will be
1284560	1290640	a 70% priority, so the traffic light will be a 70% priority.
1290640	1293880	Oh, OK, so if you looked at a junction,
1293880	1295840	you can calculate the probability
1295840	1301920	that the lights are favoring the main artery rather
1301920	1303000	than the side arteries.
1303000	1305240	OK, cool.
1305240	1306600	A friend?
1306600	1309000	Widget manufacturing.
1309000	1310240	Can we talk about that?
1310680	1320320	So you have a binary outcome, accepted or rejected?
1320320	1320800	Where else?
1320800	1322280	Back there.
1322280	1323520	Is it at birth?
1323520	1327280	Yeah, biological sex at birth, yes, exactly.
1327280	1330800	So it's a binary situation for the vast majority
1330800	1333360	of the population, so under that approximation,
1333360	1335120	then what is it?
1335120	1339320	The canonical is like 51% or 52% male, female, sorry,
1339320	1342440	and slightly lower for male, right?
1342440	1342960	Where else?
1346800	1350320	All right, so I think the point is well described
1350320	1351560	by these examples.
1351560	1354960	There's a binary outcome that we're seeking to model,
1354960	1359000	and the binary outcome sometimes is merely an approximation.
1359000	1363600	The binary outcome is a useful, if it's a useful approximation,
1363600	1365600	then we use that.
1365600	1368040	We can use what we call the bias coin flip
1368040	1372600	to model the process of generating the individual outcomes
1372600	1374480	that we actually observe, right?
1374480	1376360	OK, cool.
1376360	1378080	Let's try a different example.
1378080	1381960	So this example here was us basically
1381960	1386720	hand coding the generative process in a very explicit
1386720	1390000	fashion for what we would call the Bernoulli trials
1390000	1392880	or binomial trials.
1392880	1394720	We're going to try a different way, which
1394720	1397320	is to actually look at real data and treat data
1397320	1402120	as if they were the population, so to speak.
1402120	1405840	As if the data that we measured were infinitely large enough,
1405840	1408960	which is never true, but as an approximation,
1408960	1411560	we'll start with that and ask, how can we
1411560	1413760	calculate the probability of certain things
1413760	1416960	under this assumption of the data being a really good
1416960	1418840	approximation of ground truth?
1418840	1422000	So what I'd like you to do is run this next cell.
1422000	1424160	What this data set that we have here
1424160	1429200	is Finch Beaks measured on Galapagos Islands, right?
1429200	1431440	So how many of you have learned biology
1431440	1433920	and have heard of the Finches before?
1433920	1434840	Right, so there we go.
1434840	1439440	We have another binary outcome right there.
1439440	1444920	So in this, someone went to the Galapagos, a research team
1444920	1448280	went to the Galapagos Islands and measured Finch Beaks,
1448280	1451720	both their length and their depth, and asked,
1451720	1453280	and just recorded what they were in.
1453280	1457520	Let's assume that this is a realistic sample,
1457520	1460440	a realistic approximation of the population of Finches
1460440	1461880	that were observed.
1461880	1471680	So let's grab out just the beak length, Th, as a Panda series.
1471680	1473760	So follow along with that cell.
1473760	1476280	And so what we'd like to ask is, what
1476280	1481560	is the probability of a bird having a beak length greater
1481560	1483000	than 10, right?
1483000	1485920	So how would we calculate that?
1485920	1487520	Well, one way we might calculate that
1487520	1491320	is under the assumption that the data are the population,
1491320	1496080	we could just ask what proportion of birds
1496080	1497560	have beak lengths greater than 10.
1497560	1504560	So we can do something like p is equal to the sum of lengths,
1504560	1512120	Ths greater than 10 divided by the length of that.
1512120	1514760	And you should get something close to,
1514760	1518080	you should actually get this exact number in this case,
1518080	1519920	because there's no randomness in the data.
1519920	1522280	So we're not simulating the random process,
1522280	1525160	we're treating the data as if they were the true thing.
1525160	1528560	So everybody should get 0.851, right?
1528560	1529600	Cool.
1529600	1532960	So these two examples, they're really
1532960	1539240	here to show you that proportions are somewhat
1539240	1541360	a proxy for probability.
1541400	1543760	Under certain circumstances, if you
1543760	1545360	make explicit certain assumptions,
1545360	1547920	your proportions can be a good estimator
1547920	1552480	for the actual probability that we're interested in.
1552480	1566520	All right, so over here, we can actually try simulating
1566520	1569200	the finch beak lengths as well, right?
1569200	1574400	So we can draw random samples from the data
1574400	1579560	and use that as another way of estimating
1579560	1581720	what the uncertainty around that probability would be.
1581720	1586600	So this procedure is what we would call resampling
1586600	1587400	with replacement.
1587400	1590080	I think it's bootstrapping.
1590080	1592200	My memory is blanking at this point.
1592200	1595800	But resampling with replacement is another way to do it.
1596760	1601320	So now, if we break the assumption
1601320	1605440	that the data are exactly what the population are,
1605440	1606840	then we're faced with a problem.
1606840	1610000	We're faced with a problem that is the proportion
1610000	1612240	that we've calculated may not actually
1612240	1618120	be the true probability of finch beaks being
1618120	1620080	greater than length 10.
1620080	1623080	So how do we estimate what that uncertainty might be?
1623080	1626040	We'll do this hacker statistic sort of method
1626040	1629440	where we computationally try to simulate random draws
1629440	1631560	from the population using the data.
1631560	1636840	So the way we would do this here is to do,
1636840	1644400	we would first do random picks from the data distribution,
1644400	1649480	so mp.random.choice, lengths.
1649480	1654440	So we're choosing from the length Panda series.
1654440	1663640	We want 1,000, we want 10,000 samples from there.
1663640	1666480	And we want to do it with replacement.
1669600	1678720	And finally, we'll take the sum of that, sum over all
1678720	1682800	of those that are greater than 10, and divide it by n samples.
1686160	1690480	And so you will notice we won't get an exactly the same number.
1690480	1691480	Question?
1691480	1693080	Is the range about choice?
1693080	1694080	Yes.
1694080	1696400	Is that creating continuous distribution
1696400	1702040	based on length, or is that choosing from the length?
1702040	1703360	It is the latter.
1703360	1706440	It is choosing from existing elements.
1706440	1707640	All right?
1707680	1709120	Cool.
1709120	1710120	Question?
1710120	1715040	So there are really only 229 values in it.
1715040	1721040	So we'll just, like I said, we're resampling with replacement
1721040	1722080	10,000 times.
1722080	1723720	Yes, yes, exactly.
1723720	1726120	OK?
1726120	1729640	OK, so these are computational methods.
1729640	1733160	One of them sort of is an explicit simulation
1733160	1735400	for the coin flips.
1735400	1739040	The other is treating data as ground truth.
1739040	1741680	And then breaking that assumption,
1741680	1743920	now saying data are not ground truth,
1743920	1745760	what is the uncertainty in this parameter
1745760	1746760	we're trying to estimate?
1746760	1750560	So there are hacker stats ways of handling this.
1750560	1754200	There's another way that we can deal with probability
1754200	1756640	and talk about coin flips and the likes.
1756640	1763040	So coin flips are essentially Bernoulli trials.
1763040	1767720	Every coin flip that I make has a single outcome
1767720	1771000	that can take on one of two possible values.
1771000	1775520	We'll call them 0 and 1, or true and false,
1775520	1779040	a binary outcome of some sort.
1779040	1783200	And we can actually take advantage of,
1783200	1788480	we can take advantage of NumPy's random number generators,
1788480	1790560	or the probability distributions that
1790560	1792760	are inside there, to try to simulate this.
1792960	1794840	So there's Bernoulli trials.
1794840	1797880	And then if you sum up Bernoulli trials,
1797880	1801840	you get binomally distributed data.
1801840	1806480	So we're going to try this out over here.
1806480	1813840	So let's set a NumPy random seed.
1813840	1814960	You can use 42.
1814960	1821360	You can use 1,607,190, oh, 16 million, sorry,
1821360	1824360	as your number, your choice.
1824360	1827360	And we can flip, we can do a few things.
1827360	1828840	So we're going to try this.
1828840	1832440	We're going to try simulating a single flip.
1832440	1834320	Let's try simulating a single flip.
1834320	1840800	mp.random.binomial1, n equals 1.
1840800	1844760	And it being a biased flip, we'll do 0.7.
1844760	1853760	And so if you rerun that cell many times,
1853760	1857560	you should get a lot of 1's, but some 0's, right?
1860120	1863080	Oh, the seed, my bad.
1863080	1863560	Thank you.
1871320	1872800	There we go, yes.
1872840	1879640	So you will get a bunch of 1's and a bunch of 0's.
1879640	1882800	If you summed up all of those Bernoulli trials,
1882800	1885920	say we did 10 of those trials together
1885920	1889080	and treated them as a single experimental run,
1889080	1892520	so we'll now do mp.random.binomial of 10,
1892520	1896720	you'll get sometimes 10, sometimes 4, sometimes 6,
1896720	1898760	et cetera, et cetera.
1898800	1905320	So now what we're really interested in
1905320	1909840	is how, in a biased coin flip, what we're really interested
1909840	1916360	in then is what the probability is of getting,
1916360	1918000	sorry, let me backtrack a little bit.
1918000	1924120	So in a binomial trial, in a binomial draw or a binomial run,
1924120	1927040	what we're interested in is calculating the probability,
1927040	1933480	knowing the probability of getting up to that number
1933480	1938560	of successes inside, up to that number of successes
1938560	1939440	for that run, right?
1939440	1942320	So the binomial trial is basically
1942320	1945480	defined as n being the number of successes,
1945480	1948680	sorry, n being the number of total Bernoulli runs
1948680	1951840	that we've done, and p being the probability of successes.
1951840	1954360	And it gives back a number, which is the number of successes
1954360	1957640	out of that n number of trials.
1957640	1960920	So what values, sorry, pop quiz, what values
1960920	1965320	can this take on, the result of a binomial trial?
1968560	1974200	0 to 10 or n in the general case where we know what n is,
1974200	1975360	right, OK, cool.
1975360	1979680	So we can actually simulate this.
1979680	1987000	We can simulate, say, 10,000 experiments of us flipping coins
1987000	1990160	10 times and counting the number of times
1990160	1995160	that we got ahead in each particular experiment, right?
1995160	2008240	So we can do that by doing mp.random.binomial 10, 0.7 and 10,000.
2008240	2009920	If you want some clarity, you can actually
2009920	2012280	use the underscore between your numbers
2012280	2014560	to make sure you know what you're typing.
2014560	2021440	So in that, we'll give us something
2021440	2024720	that looks like this, right?
2024720	2033280	So what I'd like to ask you then is, what do you see in the chart?
2033280	2036280	What values are probable?
2036280	2037520	Probable values are not probable.
2040160	2042040	I'm not going to ask you to talk with your neighbor
2042040	2043040	this time around.
2046040	2049120	Any volunteers?
2049120	2052240	So the most probable value is 7, right?
2052240	2057960	And we know that from the fact that our p, which we set, was 0.7.
2057960	2061280	So the expectation or the most likely value,
2061280	2064120	the thing we expect to see the most is 0.7.
2064120	2065160	What else do we see?
2067280	2071040	The upper bound, bounded at 10, right?
2071040	2073360	We don't see any values at 0, 1, or 2.
2076360	2077360	What's happening there?
2081360	2082360	Is this very small?
2082360	2085600	Yeah, we've done the experiment 10,000 times,
2085600	2087920	and that's still not enough, right?
2087920	2089000	It's still not enough for us to be
2089000	2091720	able to see whether there is a probability,
2091720	2095160	see the proportion of the probability.
2095160	2101200	The proportion of trials that we will get one head only out of 10.
2101200	2106240	OK, so I'd like you to try the following exercises.
2106240	2109760	Spend about two to three minutes on them.
2109760	2112480	The first one is calculating the probability of five
2112480	2116680	or more heads for a value of p is 0.3, then for 0.5,
2116680	2119920	plot the histograms for both of them.
2119920	2122240	Yeah, so spend a minute or two handling that.
2122240	2124560	Just really quickly go over.
2124560	2130680	You should get something like this for the first exercise,
2130680	2134320	something around the value of 0.7655,
2134320	2136760	something like that for the second exercise, right?
2136760	2139880	The probability of seeing a heads is higher,
2139880	2144120	so therefore the probability of seeing five or more heads out
2144120	2147360	of 20 is also going to be higher, right?
2147360	2152560	So 99% of those have that result.
2152560	2155160	And if you plot the histogram, you
2155160	2156840	should see something like this.
2156840	2160120	Now, for those of you who had that bit of time
2160120	2162960	to think about the question, looking at the histogram,
2162960	2169200	can you tell me what is the probability of seeing four
2169200	2170160	or more heads?
2172680	2175160	Pardon me?
2175160	2177680	Yes.
2177680	2181000	But a lot is not a number from 0 to 1, right?
2181040	2182880	So what is that number from 0 to 1?
2186480	2188400	Pardon me?
2188400	2191000	85%, OK, maybe.
2191000	2193000	It's not very easy to tell, right?
2193000	2195920	Not very easy to tell.
2195920	2199720	This is the sort of question where histograms are kind
2199720	2201720	of not the right thing to look at.
2201720	2204080	It's the thing that we're used to looking at,
2204080	2207080	but it's not the kind of thing that would give us rich.
2207080	2209040	It's not the kind of plot that would give us
2209040	2212160	rich statistical information on the data
2212160	2214880	that we have on hand, OK?
2214880	2215360	Question?
2219440	2221640	Danka, yes, exactly.
2221640	2226280	So what we want instead is the cumulative distribution
2226280	2227800	of our data.
2227800	2231120	And that, I argue in a blog post,
2231120	2234920	gives us much richer information than a histogram would.
2234920	2236640	A histogram is nice and convenient
2236640	2239160	to look at because it sort of tells us where the central
2239160	2242000	tendency is, but that's all it really
2242000	2245120	can tell us what the central tendency is, right?
2245120	2247160	Maybe the bounds, but even the bounds
2247160	2248680	aren't going to be shown very accurately,
2248680	2253160	as we would see from this histogram up there, right?
2253160	2258480	So this is where the cumulative distribution of our data
2258480	2260520	comes into play, and we can plot what
2260520	2263680	we would call the empirical cumulative distribution
2263720	2266880	function, the ECDF, of our data.
2266880	2270640	And the ECDF is a great way to visualize this, OK?
2270640	2275080	So if we take our data and we arrange them, well, actually,
2275080	2276880	let's code along, let's code along,
2276880	2279720	and you'll see what the ECDF will look like for this,
2279720	2281720	for binomally distributed data.
2281720	2287320	So X flips and Y flips are going to be ECDF of our data,
2287320	2290760	which our data is X. What it will return
2290760	2295560	is it'll give us indices on the x-axis, which
2295560	2298880	are where our data points fall.
2298880	2301760	And then it'll give us an index on the y-axis, which
2301760	2304400	is a number from 0 to 1 that tells us
2304400	2307480	how much of our data falls below that particular data
2307480	2309160	points value, OK?
2309160	2313880	So let's do that, and we'll do plt.plot x flips,
2313880	2322920	y flips, marker is a dot, oops, line style is none.
2325640	2326880	Oops, I forgot to run that.
2329720	2335000	Now, things look a little clearer, right?
2335000	2338720	So for those of you who have the histogram on your screen,
2338720	2340600	keep the histogram on your screen
2340600	2345240	and compare it to the ECDF that's on the projector screens.
2345240	2348360	Now let me ask, what's the probability
2348360	2351840	of getting 4 or higher in our data?
2358520	2359760	Something percent, yes.
2359760	2362400	OK, so here's how you look at it.
2362400	2369640	You go to 4, you go up to the bottom of that bar-like looking
2369640	2373600	thing, and then you draw a line across to there,
2373600	2375800	and it falls roughly at 0.2, right?
2375800	2380520	So the probability of getting 4 or higher is approximately 0.2.
2380520	2388760	And the thing about ECDFs is 0.8, sorry, thank you, 1 minus.
2388760	2390440	We're doing the higher, thank you.
2390440	2393600	So the thing about ECDFs, though,
2393600	2396640	is that it gives you much richer statistical information.
2396640	2399040	You can tell things like the central tendency.
2399080	2400360	So what's the central tendency?
2403680	2409080	Well, we go to 0.5, draw a line across,
2409080	2411960	see where it hits the data, and look on the x-axis,
2411960	2415880	and that value is 5, exactly.
2415880	2417600	What are the bounds of the data?
2421880	2424320	Exactly, and take a look at that.
2424320	2427880	We couldn't see the 0 in the histogram
2427880	2431240	because it's kind of like obscured by the height of the rest.
2431240	2433160	One thing that's cool about the ECDF
2433160	2436360	is that it uses all of the data.
2436360	2441040	There's no binning bias that can obscure the values
2441040	2443600	that your data can take on.
2443600	2444840	That is a serious problem.
2444840	2447040	You can lie with histograms.
2447040	2449200	You do not want to lie with histograms.
2449200	2451160	I will come after you, OK?
2454080	2455920	OK, cool.
2455920	2459360	So I made my case, and I rest my case with ECDFs.
2459360	2461360	Don't ever use histograms again.
2461360	2462720	Always use ECDFs.
2462720	2465240	They give you a much richer view onto your data.
2465240	2469160	You can look at all of the percentiles of interest.
2469160	2471240	You can visualize, sorry, all of the percentiles
2471240	2477040	that are relevant in your modeling problems using ECDFs, OK?
2477040	2482560	OK, so we did a little recap on probability
2482560	2484880	just to make sure we're all clear
2484920	2486920	and we're on the same page.
2486920	2492320	Probability is credibility points assigned to the number line,
2492320	2493480	OK?
2493480	2495080	When we plot something like this, we're
2495080	2498720	saying there's lots of credibility points assigned
2498720	2499920	to this value.
2499920	2502200	There's very little credibility points assigned
2502200	2505080	to the tail values, OK?
2505080	2507480	That's all probability is a working definition
2507480	2508800	for our purposes.
2508800	2512400	We can simulate draws from a probability distribution.
2512400	2514400	We did that multiple ways.
2514400	2523120	We can simulate it from data by doing sampling with replacement.
2523120	2525800	Now, what we're going to do, oh, sorry, and finally,
2525800	2529160	we can actually take advantage of the exact analytical form,
2529160	2533040	analytically implemented probability distributions
2533040	2539320	in NumPy and SciPy stats, and use that to help us simulate
2539320	2543120	what our data might look like under a set of fixed parameters.
2543120	2545160	When I was learning Bayesian stats,
2545160	2547960	this activity was really, really helpful
2547960	2550680	for getting familiar with the shapes of probability
2550680	2552440	distributions.
2552440	2554720	And as you'll see later, the shapes
2554720	2557080	of your probability distributions, particularly
2557080	2560000	the bounds, the central tendency, and how they're skewed,
2560000	2565040	can be really useful pieces of knowledge
2565040	2567920	that no longer are just trivia in your head,
2567920	2571360	but actually can be useful tools for modeling.
2571400	2575040	What we're going to do now is do a very, very, very, very quick
2575040	2578400	run through of the different probability distributions
2578400	2581480	and what their so-called stories are.
2581480	2582680	Probability distribution.
2582680	2584320	Oh, well, let me backtrack a little bit.
2584320	2586920	How many of you have heard of the term generative models
2586920	2589080	of data?
2589080	2591240	Yeah, those who've been in the deep learning world
2591240	2595120	will know that there is this term called generative models.
2595120	2597600	And frankly, at some companies, like the one that I worked at,
2597600	2600640	it's been overhyped quite a bit.
2600640	2604600	Everyone wants to talk about generative models.
2604600	2607800	At its core, generative models are just
2607800	2612600	how we can generate things that look like data.
2612600	2615240	And if you think hard and long about it,
2615240	2619440	probability distributions are generative models of data.
2619440	2621920	Probability distributions are generative models of data
2621920	2625560	because we can construct a model that
2625560	2629520	is composed of purely just probability distributions
2629520	2633520	and use it to simulate data that looks like actual data
2633520	2635520	that we might collect.
2635520	2637880	So let's think about what actual data we might collect,
2637880	2640720	say, starting with the Poisson distribution, right?
2640720	2644960	So Poisson processes and the Poisson distributions.
2644960	2646360	That's a generative model.
2646360	2648680	And it's got a story that's behind it.
2648680	2652760	And this concept of what is the story behind each
2652760	2654520	and every probability distribution
2654520	2657880	is something that we need to make sure we're familiar with.
2657880	2662200	So Poisson distributed data, basically,
2662200	2666400	can be thought of as something like this.
2666400	2669400	This is borrowed from David McKay's book.
2669400	2672280	I have a town, and it's called Poissonville.
2672280	2678440	And the buses, they're kind of like MBTA trains in Boston.
2678440	2680680	So they come, and then sometimes you
2680680	2683280	have to wait a heck of a long time before the next one comes.
2683280	2686240	And sometimes the next one comes right after them, right?
2686280	2688720	They're the one that just came by.
2688720	2691440	Yeah, and then sometimes they get derailed and, well,
2691440	2693600	too bad for Bostonians.
2693600	2700920	So the amount of time that you wait for one train or one bus
2700920	2702920	is independent of the amount of time
2702920	2706240	that you waited for the previous bus, right?
2706240	2709120	So that's the story of a Poisson process.
2709520	2716400	And so the timing of the next event
2716400	2720000	is completely independent of when the previous event happened.
2720000	2726040	And so it's not just faulty trains on the red line in Boston.
2726040	2727320	There's other things, too, right?
2727320	2729240	There's like births in a hospital.
2729240	2732000	There's meteor strikes.
2732000	2734680	God help us if we have one.
2734680	2738320	Aviation incidents, the rate of things happening, right?
2738320	2740840	Number of events happening per unit time.
2740840	2744080	So that's what a Poisson process is.
2744080	2749880	And the number of arrivals that occur in a given amount of time
2749880	2752480	takes on a Poisson distribution.
2752480	2754560	So all that the Poisson distribution is modeling
2754560	2760880	is just how many things happen within a given unit of time.
2760880	2764800	So we can actually simulate that.
2765240	2775400	So if we want what we can do here is we can simulate Poisson
2775400	2782120	draws, np.random.poisson, with per unit time six events
2782120	2784760	happening, so six natural births per day,
2784760	2792440	or six collisions at Brigham Circle near Harvard Medical.
2792440	2797000	And we'll do like 10 to the power of six.
2797000	2799080	Let's do a million draws, right?
2799080	2801000	And then plot what this distribution looks like.
2801000	2806960	PLT.hist of samples will just set the bin size
2806960	2809400	so that it's a convenient thing for us to visualize.
2809400	2812640	And you'll get something that looks like that, right?
2812640	2815920	Does the shape look kind of familiar?
2815920	2820840	Not that you've seen it before, but like some other distribution?
2820840	2822720	The binomial, did I hear?
2822720	2824200	Yeah, what's up with the binomial?
2828960	2832720	Yeah, so this one isn't exactly symmetric, but close enough,
2832720	2833640	yes?
2833640	2836000	Yeah, so there's a neat relationship
2836000	2839200	between the Poisson distribution and the binomial
2839200	2842480	distribution, that is for low probability of successes,
2842480	2847720	that is for really, really rare events that happen.
2847720	2852760	The Poisson distribution is the approximation, or the limit,
2852760	2855480	as we go to low probability of success
2855480	2857600	and large number of trials, right?
2857600	2860240	So there's a relationship between these distributions.
2863480	2868480	We can actually, let's plot the ECDF of this as well.
2868480	2875400	So ECDF of our samples that we drew.
2875400	2877240	And then we plot this guy again.
2895080	2897480	And you should get something that looks like that.
2897480	2902400	Once again, this tells us a lot of information.
2902400	2904080	What is the central tendency in this case?
2906400	2907840	Six, yes, exactly.
2907840	2912200	So the central tendency, I'm using this term
2912200	2913960	as the more generalized thing, right?
2913960	2917200	So the central tendency can be either the mean, the median,
2917200	2919480	or the mode, depending on which one we're talking about.
2919480	2923360	The expectation is the mean, and the expectation
2923360	2927000	of the Poisson distribution is the parameter
2927000	2928440	that we passed into it, right?
2928440	2933040	So if we say that things are Poisson distributed with six
2933080	2935320	events per unit time, then we expect
2935320	2938280	to see six events, the central tendency will be six,
2938280	2942120	the median will be six, the mean will be six, OK?
2942120	2946040	All right, so something that's also Poisson distributed
2946040	2950240	would be field goal attempts per game.
2950240	2954200	Yesterday over lunch, a bunch of us
2954200	2957560	were talking about how football, and I'm
2957560	2961240	sorry to the American football fans, that's hand egg,
2961840	2964040	real football that is played with your feet,
2966080	2970320	that's sort of at a field goal rate that's not that good,
2970320	2973400	whereas hockey is at the right field goal rate,
2973400	2977320	whereas basketball is at some absurdly high field goal rate.
2978320	2983320	So we're gonna do an example taken from professor,
2983760	2986080	an instructor at Caltech, Justin Voice,
2986080	2988600	who I've met at the SciPy conference,
2988600	2993400	and it's about field goal attempts by LeBron James, right?
2994520	2999520	And he did, that's his data, the his stats per game,
2999680	3004080	number of things that he, number of times
3004080	3007200	that he shot in one game, so the unit time is one game,
3007200	3009920	and we're asking how many attempts did he make, OK?
3014240	3015920	I need to reconnect my VPN.
3016120	3021200	Pardon me, OK, so let's move on.
3021200	3023720	So we've got the field goal attempts here.
3028080	3030200	All right, that's OK.
3030200	3031840	It'll come back when it comes back.
3033840	3038840	And we'll do the ECDF of this data, all right?
3043320	3044680	Cool.
3044680	3049680	And finally, what we're gonna do is we're gonna do
3049760	3053120	many random draws from a Poisson simulation,
3053120	3057520	from a Poisson distribution, and plot all of those random
3057520	3061720	draws, the ECDFs of those random draws to see whether
3061720	3066720	it follows the same distribution as what LeBron's
3067000	3069360	field goal attempts would be like.
3069360	3073860	So let's code along, so for underscore in range of that,
3074920	3079920	samples is np.random.poisson, np.mean of field goal attempts,
3081720	3086720	and the size is the length of field goal attempts,
3087000	3089680	and in this case, we'll plot the ECDF
3089680	3091240	of the theoretical samples.
3096400	3099400	And let's see, my kernel's disconnected,
3099400	3104240	so I'm going to have to reconnect and rerun cells.
3104680	3109680	And since we're at it, I'll just bring up the instructor
3115800	3119040	version so that we have it there.
3119040	3121480	You should get a plot that looks something like that,
3121480	3123960	right, OK?
3123960	3125800	And so with that guy over there,
3128320	3130760	that's one probability distribution story
3130760	3133180	that we can talk about, the Poisson distribution.
3133220	3136620	Now the Poisson distribution is a discreet
3136620	3137740	probability distribution.
3137740	3140420	What's the other class of distributions?
3140420	3141900	The continuous family, right?
3141900	3146900	OK, so the discreets, when we plot that,
3148940	3152380	they technically follow a probability mass function
3152380	3155740	because there's a bulk of mass that's associated
3155740	3159100	with each particular value, that is how credibility
3159100	3163660	is assigned, with continuous distributions,
3163660	3166620	we have the probability distribution function,
3166620	3169580	which technically have zero mass at any point
3169580	3171340	on the x-axis, right?
3171340	3175860	But they take on, they have a density of mass,
3175860	3180020	so-called, from within a range of continuous values, OK?
3181180	3185020	OK, we're going to skip one or two,
3185020	3187680	we're going to skip the exponential distribution,
3187680	3189640	I'll just leave it out there for you,
3189640	3192960	that the exponential distribution is the waiting time
3192960	3197960	between Poisson events, and so that's that,
3199000	3203120	and so you can take a look at how the CDF,
3203120	3205920	ECDF of the exponential distribution
3205920	3207440	will look like on your own time.
3207440	3210160	We'll go through the normal distribution, OK?
3210160	3213160	The normal distribution, everyone's familiar with this,
3213160	3215720	right, things are normally distributed,
3215720	3218320	a lot of things, sorry, are normally distributed, OK?
3219920	3221920	We've got measurements, and in this case,
3221920	3225400	we'll just take a look at what the story is.
3225400	3228400	The story is, in this quote here,
3228400	3230120	when doing repeated measurements,
3230120	3232880	we expect them to be normally distributed,
3232880	3236160	owing to the fact that many sub-processes,
3236160	3239560	lots of individual data-generating processes,
3239560	3242200	contribute to this final thing that we measure, OK?
3242200	3247120	So things like human height is a culmination of many genes,
3247120	3249040	so it's kind of reasonable,
3249040	3251240	it's a culmination of lots of genes,
3251240	3253880	plus environmental effects put together,
3253880	3255440	and so it's reasonable to expect
3255440	3257520	that human height would be normally distributed
3257520	3259600	or approximately so, right?
3261760	3265520	So yeah, so the formulation,
3265520	3268360	one formulation of the CLT, the central limit theorem,
3268360	3270160	is that any quantity that emerges
3270320	3272920	as the sum of a large number of sub-processes
3272920	3274920	tends to be normally distributed,
3274920	3278360	provided that none of the sub-processes
3278360	3280560	is very broadly distributed itself,
3280560	3282680	that is, it doesn't overwhelm
3282680	3286720	the final data distribution that we're looking at, OK?
3286720	3290880	So just to have you all take a look at this,
3290880	3294600	these are what we would call,
3294600	3296640	these are measurements of the speed of light,
3296640	3299440	and there's, in an experiment,
3299440	3302560	lots of factors contribute to what measure,
3302560	3305560	what value we eventually measure, OK?
3305560	3309000	So there's some data on the speed of light,
3309000	3312480	and the estimate of the speed of light
3312480	3314600	tends to be normally distributed
3314600	3317800	because of this data-generative process, right?
3317800	3320280	Lots of things contributing to this final thing.
3320280	3322560	There's error in the instrument,
3322560	3325240	there's error in, there's the instrument itself,
3325240	3326720	and it's got some error,
3326720	3328920	there's the measurement technique,
3328920	3330360	and it's got some error,
3331440	3334320	there's the conditions of the day
3334320	3337320	that we're measuring and that that can affect,
3337320	3340360	that's the reason most biologists tend to use
3340360	3343720	something happened that day for a field experiment,
3343720	3346120	and I know it myself because I was one before.
3347000	3349000	So yes, you get the point, though, right?
3349000	3353160	So there's a combination of factors that lead to this.
3353160	3354000	OK.
3354880	3358480	Now, I want to leave, before we go on a break,
3358480	3361240	I want to leave you with this little tidbit
3361240	3363680	which comes from Ellen Downey's blog post.
3363680	3367000	Are your data normally distributed?
3367000	3369080	I encourage you to look at that blog post
3369080	3371920	because though something,
3373960	3378720	though we impose models on our data,
3378720	3382520	our data may not necessarily be
3383480	3387120	exactly following that model that we've imposed on.
3387120	3389120	So when we make an assumption
3389120	3392320	or when we impose this idea that our data might be normal
3392320	3396360	and we do a fit, we check how deviant our data
3396360	3398040	are from normal distributions,
3398040	3399560	we use that normal distribution
3399560	3401920	and later downstream things for estimating the mean
3401920	3403560	and the likes.
3403560	3405640	When we do things like that,
3405640	3407840	we're saying we're imposing a model
3407840	3409680	and that model might be wrong,
3409680	3411160	but it can be useful, right?
3411160	3413120	This is a classic George Box quote.
3415680	3419080	I'm going to mash that with George Orwell
3419080	3421560	saying that some models are wrong
3421560	3425120	but some are more wrong than others, OK?
3425120	3428200	And we might be, find some of them useful, right?
3428200	3431200	So take a look at Ellen's post, ponder about that point.
3431200	3434960	We'll come back at 2.45 p.m.
3434960	3437160	There should be snacks inside the Tejas room.
3438160	3441520	We'll come back to in notebook 1b
3441520	3444160	and very quickly go through joint
3444160	3446120	and conditional probability, OK?
3446120	3448800	This is a last minute decision that I made
3450280	3451880	in that in the interest of time,
3451880	3453760	there's some ideas I want to cover.
3453760	3458200	We'll cut down a little bit on the foundational exercises
3458200	3460520	but then we'll have the concepts given to you all.
3460520	3465200	So you have the tools to do your modeling later on, OK?
3465200	3467120	So as I mentioned in the interest of time,
3467120	3469640	we're going to skip a few exercises.
3469640	3474920	The next notebook is on conditional and joint probability
3474920	3480160	and really I want to give you all mostly just the tools
3480160	3482120	that you need to be able to think through
3482120	3483920	the problems at hand, OK?
3483920	3486080	So one of the tools in the toolkit
3486080	3489000	is the distributions and the stories
3489000	3490640	that are associated with them.
3492400	3493840	I drew this out as a table
3493840	3495960	but by the way there's a really great resource
3495960	3499680	by Justin Boyce who put all of the distribution stories
3499680	3503600	that are relevant on his Caltech website.
3503600	3507040	So that's a really, really great resource to go and check
3507040	3508880	and I'll make sure that's also linked
3508880	3510160	on our GitHub repository
3510160	3513280	so you can always check that out from there, OK?
3513280	3516960	So just to recap, we've got probability,
3516960	3521880	they follow, we've defined as credibility
3521880	3523640	assigned on the number line,
3523640	3527160	we have ways of simulating probability distributions,
3527160	3530480	both analytically and by brute force computation.
3531720	3535160	Really the core thing that we want to have in our toolkit
3535160	3537960	is actually the probability distributions
3537960	3540800	as well as their shapes and their stories, OK?
3540800	3545000	So this is a very incomplete, highly partial table
3545000	3546920	of the many probability distributions
3546920	3550040	that exist out there and I wanted to, you know,
3550040	3553960	hint at you should be building such a matrix for yourself
3555280	3556960	and it is really helpful,
3556960	3558800	like you can maybe have a cheat sheet
3558800	3561240	written by someone else but I always find it
3561240	3563800	like if I do the hard work and draw it out
3563800	3566000	and write it out for myself, it's also really useful.
3566000	3568720	Nonetheless, I'll be building a resource
3568720	3573720	that basically 100% plagiarizes Justin's resource
3574720	3579320	but gives added nice visuals for learning benefit, OK?
3580600	3582600	But really, so the toolkits are,
3582600	3585200	you want to know the names of these distributions
3585200	3587400	because they aid in communication
3587400	3590800	with other people who do statistics.
3590800	3591880	When you're writing things out,
3591880	3594560	you'll want to know what their abbreviations are, right?
3594560	3596560	So that guy highlighted over there
3596560	3598840	because that gives a nice and compact way
3598840	3602360	of telling people what distribution you're using
3602360	3604040	when you're writing stuff.
3604040	3605680	In your mental model,
3605680	3608880	you want something like the shape of a distribution, right?
3608880	3612440	The uniform is just equal credibility between two bounds.
3612440	3614680	So that picture should come into your head.
3614680	3616120	You should know whether the bounds
3616120	3617920	are relevant to your problem or not
3617920	3621360	and also whether equal credibility makes sense or not, right?
3623360	3625960	Just so that you have something live,
3625960	3629360	there's a variant or the generalization
3629360	3631160	of the uniform distribution,
3631160	3634920	which is the beta distribution.
3634920	3637120	It takes in two parameters,
3637120	3641440	number of successes, number of failures.
3641440	3645240	It's actually bound between zero and one explicitly
3645240	3649280	and takes on values that can look like that
3649280	3652520	or can look like this
3654480	3657200	or can look like that, right?
3657200	3658920	So, but the key point is that
3658920	3660720	it takes on values that are bounded.
3661720	3665920	There's this term which took me a little while to remember,
3665920	3668400	but it's called the support of a distribution.
3668400	3670480	This is something that you'll see
3670480	3672400	inside the statistics literature.
3672400	3674680	The support of a distribution is nothing more
3674680	3677680	than the values that it can take on,
3677680	3679640	the values that it can take on, right?
3679640	3680720	That's all it is.
3680720	3683960	So the support for the beta distribution
3683960	3689080	is something like a, oops, zero to one,
3689120	3691400	it's bounded between zero to one, right?
3691400	3694160	And there's a story for the beta distribution,
3694160	3698080	which is number of successes
3701640	3706640	expected fraction of successes
3708360	3713360	out of n success plus n failure, okay?
3714360	3718360	And it can take on, the alpha and beta parameters
3718360	3720360	can take on not just integer,
3720360	3723360	but also floating point, decimal numbers, okay?
3723360	3726360	So you'll want to have this kind of picture in your head
3726360	3728360	when you're thinking about that.
3728360	3729360	Where can you learn?
3729360	3732360	Again, I'm saying Justin Boyce's website is a great place.
3732360	3734360	I learned a lot of these probability distributions
3734360	3738360	and their shapes and their possible values
3738360	3740360	by looking at the prime seed,
3740360	3743360	prime seed three has a great thing for that.
3743360	3745360	But you'll notice, in prime seed three,
3745360	3747360	basically all they're doing is just,
3747360	3749360	all we're doing in the docs is just simulating
3749360	3751360	those distributions and plotting them out.
3751360	3753360	So that really is the best way to do it.
3753360	3756360	All of that hands-on simulation that we just did,
3756360	3759360	that is a great, great way for you to learn
3759360	3761360	what the probability distributions are,
3761360	3764360	what their shapes, what their data generating processes
3764360	3765360	are all about, okay?
3765360	3767360	So I want to start with,
3767360	3770360	that's one thing you want to have in your toolkit.
3770360	3773360	The next thing you'll want to have in your toolkit
3773360	3777360	is this idea of joint and conditional probability.
3777360	3780360	Once again, in the interest of time,
3780360	3782360	by the way, for those of you who just got back,
3782360	3784360	make sure, and using Binder,
3784360	3787360	execute something so you don't lose your session.
3787360	3792360	The way I think about joint, conditional,
3792360	3794360	and marginal probability,
3794360	3799360	is by a visual that looks something like this.
3806360	3810360	If I have data that are jointly distributed,
3810360	3813360	they might look something like that.
3813360	3816360	Say, this is a bivariate Gaussian.
3816360	3819360	This put together is what we would call
3819360	3822360	the joint distribution.
3823360	3826360	Of the two things that we're interested in,
3826360	3829360	X1 and X2, okay?
3829360	3833360	Then there's a thing called conditional distribution.
3833360	3839360	That is, what is the distribution of one of the two axes,
3839360	3843360	given that we know something about the other, okay?
3843360	3847360	So if we use red to do the conditional distribution,
3848360	3855360	this is a known value of X1.
3855360	3856360	Oh, sorry.
3856360	3862360	This joint distribution is denoted as p of X1 and X2.
3865360	3866360	Okay?
3866360	3868360	So in red, we're going to show you
3868360	3870360	what the conditional distribution looks like.
3870360	3873360	So the known distribution looks like that.
3873360	3874360	Oh, shucks.
3874360	3876360	I hit the off button.
3878360	3882360	If we take that joint distribution
3885360	3888360	and project it back onto the X2 axis,
3893360	3896360	it itself will follow a distribution.
3898360	3899360	Okay?
3899360	3900360	Are we okay with this?
3900360	3901360	Right?
3901360	3903360	This is the distribution of X2,
3903360	3905360	given that we know X1.
3905360	3910360	This is what we would call the conditional distribution.
3913360	3914360	Okay?
3914360	3922360	And this is denoted probability of X2 given X1,
3922360	3927360	where that little thing over there is given.
3928360	3929360	Okay?
3929360	3932360	That pipe tells the statistician that you're taking,
3933360	3936360	that you're computing the distribution of X2,
3936360	3941360	having known a particular value of X1.
3941360	3942360	Okay?
3942360	3944360	Are we okay with this so far?
3944360	3946360	Okay, there's a final idea,
3946360	3948360	which I'm hoping you'll keep,
3948360	3950360	which is known as the marginal distribution.
3952360	3955360	Marginal distribution looks like this.
3957360	3959360	There are two distributions over here.
3962360	3963360	Okay?
3964360	3965360	This in blue
3967360	3971360	are what we would call the marginal distribution.
3971360	3974360	Why is it called the distribute marginal?
3974360	3981360	Well, first off, it's on the margins of this two-axis thing.
3981360	3982360	Okay?
3982360	3987360	It is the value of X1 ignoring whatever values,
3987360	3990360	ignoring whatever value that X2 is.
3990360	3991360	Okay?
3991360	3995360	So it is the value of X1 completely ignoring
3995360	3998360	the other variants that are of interest.
3998360	3999360	Okay?
3999360	4003360	So this is denoted as P of X1.
4003360	4006360	This is denoted as P of X2.
4006360	4009360	And those are marginal distributions.
4009360	4010360	Okay?
4010360	4013360	So prior to drawing this out for myself,
4013360	4015360	this was something that I didn't,
4015360	4020360	wasn't really able to keep straight in my head.
4020360	4023360	But this served as a very,
4023360	4025360	like what I would call an anchoring example
4025360	4027360	for what these three terms mean,
4027360	4031360	joint, conditional, and marginal probability.
4031360	4035360	Now, why are these three terms really important?
4035360	4040360	It's because it's from joint and conditional probability
4040360	4042360	that we make our way, joint, conditional,
4042360	4044360	and marginal probability that we make our way
4044360	4046360	to what we call Bayes' Rule.
4046360	4047360	Okay?
4047360	4052360	Bayes' Rule, if you've seen the famous neon light photo,
4052360	4062360	is written as P of A given B is P of B given A
4062360	4066360	times P of A over P of B.
4066360	4068360	Where does this come from?
4068360	4070360	Well, this comes from the definition
4070360	4074360	of joint probability.
4074360	4087360	So P of A, B is equal to P of A given B times P of B,
4087360	4093360	which is equal to P of B given A times P of A.
4093360	4098360	And if you simply isolate out this portion
4099360	4104360	and move this guy over there,
4104360	4109360	then suddenly you have Bayes' Rule, right?
4109360	4112360	And this is a neat thing because it gives us a way
4112360	4118360	to move between probability of data given model
4118360	4120360	or in probability of model given data
4120360	4125360	if we take an alternative view of what we're doing, okay?
4125360	4127360	And to illustrate this example,
4127360	4131360	we're actually going to use the drug testing example
4131360	4133360	in notebook 1B, so I'd like to invite you
4133360	4140360	to navigate to there, okay?
4140360	4151360	I'm going to plug this back to my laptop.
4151360	4156360	So the drug testing example is one of those classic things
4156360	4165360	where we can come up with a solution to a problem,
4165360	4167360	but if we don't do the stats right,
4167360	4169360	we'll be kind of off, all right?
4169360	4171360	So I hope you'll see this from this point.
4171360	4175360	So let's look at the question, right?
4175360	4177360	So we have a test.
4177360	4183360	It's 99% true positive for drug users
4183360	4186360	and 99% true negative for non-drug users.
4186360	4190360	What that means is if I give you a drug user
4190360	4194360	and then I ask you to do the test on that drug user,
4194360	4199360	then 99% of the time it will be correct.
4199360	4202360	And if I give you a non-drug user
4202360	4204360	and I ask you to do the test,
4204360	4209360	then 99% of the time it will be correct as well.
4209360	4212360	So it sounds like a great device, right?
4212360	4215360	So what I'd like to then ask you to do
4215360	4218360	is before doing any of this computation,
4218360	4222360	write down what you think the probability will be
4222360	4225360	that a drug user is,
4225360	4230360	sorry, a positive testing user will be a drug user.
4230360	4232360	Put down a number.
4232360	4235360	It's got to be from zero to one because it's a probability.
4235360	4238360	Put down a number mentally in your head.
4238360	4241360	Yeah, it's in the notebooks.
4241360	4245360	So 99% true positive results for drug users
4245360	4249360	and 99% true negative results for non-drug users.
4249360	4251360	Okay?
4251360	4257360	It's in your notebook, so scroll down on notebook 1b.
4257360	4259360	Write down a number.
4259360	4262360	And when you're done, give me a thumbs up.
4262360	4264360	Don't think too hard on this one.
4264360	4269360	You're meant to be surprised.
4269360	4275360	Okay, so we can actually simulate this whole process, right?
4275360	4278360	Because we know,
4278360	4285360	let's switch back to here.
4285360	4293360	We know a few things about the data-generating process.
4293360	4302360	We can actually represent this as a tree.
4302360	4308360	That looks ugly.
4308360	4315360	That tree might look something like this.
4315360	4322360	Someone's a user and someone's a non-user.
4322360	4330360	They test and they turn out to be positive and negative.
4330360	4334360	Positive and negative.
4334360	4336360	Now, help me fill in the blanks.
4336360	4342360	What is the probability that the user tests positive
4342360	4345360	given that they are a user?
4345360	4347360	99%.
4347360	4354360	I'm going to use 0.99 here.
4354360	4358360	What should the value be on the negative arm?
4358360	4360360	0.01, very good.
4360360	4364360	What about the non-user?
4364360	4368360	0.01, right?
4368360	4372360	If the user is not a drug user,
4372360	4376360	1% of the time they will test positive as a drug user.
4376360	4382360	99% of the time, oops, I'm being inconsistent,
4382360	4384360	they will test negative.
4384360	4388360	This sounds like a really good test, right?
4388360	4394360	What we're interested in, however, is P of drug user
4394360	4400360	given positive because this guy here is P of positive
4400360	4406360	given drug user.
4406360	4408360	So, how do we calculate this?
4408360	4409360	Well, we can simulate it.
4409360	4433360	Let's come back to the notebook.
4434360	4442360	So, if we take 10,000 subjects and we simulate...
4442360	4445360	Sorry, let me backtrack a little bit.
4445360	4449360	In order to solve that problem, we're missing one piece of information.
4449360	4455360	That is, what is the probability that a user is a drug user?
4455360	4457360	A person is a drug user.
4457360	4459360	We have the people, persons group,
4459360	4463360	who are a drug user and non-drug user.
4463360	4468360	So, some may say that if we're in Central West Virginia,
4468360	4472360	then opioid crisis is like ravaging there, right?
4472360	4477360	And it's a black stain on the pharmaceutical industry for that.
4477360	4485360	So, we might put an estimate that 5% to 10% of the population are drug users.
4485360	4488360	Now, if we're in clean, clean Massachusetts,
4488360	4492360	then what might we believe about this fraction?
4492360	4496360	It might be 10 times smaller, say 0.05, or New York City, right?
4496360	4499360	Like, here goes from New York, or Sydney, actually, that's his hometown.
4499360	4503360	So, 0.05, right?
4503360	4506360	Sorry, not 0.05, 0.005, right?
4506360	4512360	So, let's simulate how many drug users we will get under that particular regime.
4512360	4517360	So, we'll do mp.random.binomial.
4517360	4521360	We'll have 10,000 users, okay?
4521360	4527360	We have a probability of them being,
4527360	4533360	the probability of them being a drug user is 0.05,
4533360	4535360	and we only want one trial.
4535360	4541360	And so, the non-users is going to be n minus the number of users, right?
4541360	4545360	Number of non-users in our population is just the complement, okay?
4545360	4547360	We run that cell.
4547360	4557360	Oh, yeah, all right, I need...
4557360	4570360	So, of the users, how many of them will test positive?
4570360	4573360	I said 99%, but it's, you know, a probability.
4573360	4576360	So, we'll explicitly simulate it.
4576360	4580360	mp.random.binomial again.
4580360	4585360	We have the users, the number of users in the population,
4585360	4589360	and 99% of them will test positive.
4589360	4593360	And then we'll have the number of non-users,
4593360	4597360	but we also want to know how many of them will test positive, right?
4597360	4599360	So, how would we simulate that?
4599360	4603360	mp.random.binomial non-users,
4603360	4606360	and what's the probability value inside here?
4606360	4608360	0.01, right?
4608360	4611360	Because we're only interested, we're interested in,
4611360	4616360	given that you're positive, what is the probability that you are a true user
4616360	4620360	or not a user, right, of this drug?
4620360	4626360	So, what fraction of those tests will be positive for users?
4626360	4630360	What do we need to divide by?
4630360	4635360	We need the sum of non-users and users,
4635360	4641360	sorry, non-positive and positives,
4641360	4644360	or rather non-users that did test positive.
4644360	4646360	Gosh, the naming is tough.
4646360	4650360	That's the hardest thing in computer science, right?
4650360	4653360	If we calculate that,
4653360	4659360	you'll get something like, what, 0.3-ish?
4659360	4660360	You all get that?
4660360	4663360	Is this surprising?
4663360	4664360	Pardon me?
4664360	4665360	It's a big number.
4665360	4668360	It's a big number, yeah.
4668360	4677360	Yeah, like, we thought we could get away with like a 99% sensitive and 99% specific test.
4677360	4681360	But it turns out, because the thing that we're really interested in
4681360	4686360	is inferring the thing that isn't shown from the data.
4686360	4689360	The thing that isn't shown from the data is the latent thing,
4689360	4692360	that is, are you a drug user or not?
4692360	4694360	And then that shows up in the drug test,
4694360	4697360	but the drug test has some probability of error as well.
4697360	4700360	So, if we go back and think about that tree that we drew,
4700360	4703360	that's the full data-generating process.
4703360	4707360	That is the full data-generating process for the things that we observed.
4707360	4713360	And now we can use that to back-infer the probability of the thing that we're interested in
4713360	4720360	rather than the probability of data given the underlying condition or the model,
4720360	4725360	rather than the, sorry, probability of the model, underlying condition or model given the data,
4725360	4727360	rather than the thing that is easy to simulate.
4727360	4733360	The thing that's really easy to simulate is probability of the data given my model of the world,
4733360	4736360	but what if my model is wrong, right?
4736360	4739360	What if my model needs updating?
4739360	4745360	And so that's where this Bayes rule thing comes in handy, okay?
4745360	4746360	All right.
4746360	4751360	Now, if you look at how this is solved with Bayes theorem,
4751360	4753360	the equations are in the notebook.
4753360	4755360	Take a look at that.
4755360	4757360	What I'd encourage you to keep in mind, though,
4757360	4763360	is we're interested in the probability of our,
4763360	4767360	the distribution of our parameters of interest given the data, right?
4767360	4772360	When we're talking about modeling our data-generating process,
4772360	4776360	we're going to stick in parameters like, you know, p, right?
4776360	4781360	Probability of success or lambda or mu, you know?
4781360	4784360	Lambda for the rate of a Poisson process or mu,
4784360	4788360	the central tendency for a normally distributed thing.
4788360	4792360	But we might be wrong and we need to update our model having seen new data,
4792360	4795360	and that's where Bayes rule comes in, okay?
4795360	4799360	So we're going to look, what we're going to do next,
4799360	4804360	over the next two and a half hours, is to look very,
4804360	4811360	sorry, two hours, is to look very in-depth into two particular data-generating stories
4811360	4814360	that can be applied across multiple places.
4814360	4818360	So in some senses, these are fairly generic models
4818360	4821360	that you can take home and use in your modeling work.
4821360	4825360	But we're going to go really deep into each and every one of those.
4825360	4829360	And so, pardon me if you are already quite familiar with this,
4829360	4831360	but I think it's handy for a few reasons.
4831360	4834360	One, you'll have the mechanics of PMC-3,
4834360	4839360	which is the tool that we're going to use under your toolkit.
4839360	4848360	You'll also have the process of telling a data-generating story in your mind as well, okay?
4848360	4852360	And we'll have practice with that, okay?
4852360	4854360	So we're going to skip notebook number two
4854360	4862360	and instead move to notebook number three directly, okay?
4862360	4868360	So I'd like to invite you to open up notebook number three.
4868360	4873360	And this is where we jump right into what we would call
4873360	4876360	probabilistic programming and Bayesian estimation.
4876360	4878360	These are probabilistic programming.
4878360	4885360	Oh, sorry, go ahead.
4885360	4892360	Yeah, let me put that up.
4892360	4900360	Yeah, yeah, yeah, definitely, definitely, definitely.
4900360	4906360	So let's write this out, okay?
4906360	4913360	Bayes' rule states that p of x1, x2, the joint distribution,
4913360	4917360	sorry, Bayes' rule starts from this formulation.
4917360	4921360	It's p of x1 given x2 times p of x2,
4921360	4930360	which is equal to p of x2 given x1 times the probability of x1, okay?
4930360	4931360	All right?
4931360	4942360	So if we say probability, so if we do the rearrangement of terms,
4942360	4948360	then we get x1, well, let's make this fit the example that we're looking at.
4948360	4958360	Probability of x2 given x1 is therefore equal to p of x1 given x2 times p of x2
4958360	4963360	over p of x1.
4963360	4967360	Are we okay here so far?
4967360	4969360	Okay, it'll be up there.
4969360	4974360	I'm unfortunately restrained by the size of the screen here as well, right,
4974360	4977360	in order to fit enough inside here.
4977360	4981360	So let me use the pencil to illustrate what this is.
4981360	4986360	This is the marginals, okay?
4986360	4992360	These are the marginals.
4992360	5002360	This is the conditional, okay?
5002360	5010360	This is what we're interested in.
5010360	5012360	Are we okay with that?
5012360	5017360	And what we're interested in is also a conditional, just to be clear.
5017360	5022360	We're interested in the red distribution.
5022360	5030360	However, we also need, in order to know the red distribution that's up here, okay?
5030360	5036360	In order to know that distribution, we actually need to know this distribution that I'm highlighting,
5036360	5045360	the probability of x1 given x2, but integrated or summed over all possible values of x2,
5045360	5050360	which therefore, let me switch mics,
5050360	5055360	which therefore means we're doing some form of summing over integration
5055360	5061360	over all horizontal slices of our data, okay?
5061360	5067360	So just to make this little clear, we're saying this top term up here
5067360	5075360	is basically this slice plus this slice plus this slice plus this slice
5075360	5082360	all multiplied together, okay?
5082360	5085360	That's what we're doing, and that's how that formulation, Bayes' Rule,
5085360	5101360	relates to this picture that we've drawn for conditional, marginal, and joint probability, right?
5101360	5110360	Okay, so I'd like to have you all open up notebook number three.
5110360	5117360	So we're going to do probabilistic programming, and we're going to start with the coin flip story, right?
5117360	5125360	The coin flip story is way too classic, but it's really one of those anchoring examples in my mind, all right?
5125360	5131360	So if you master the complexities that we can build on top of it for the coin flip story,
5131360	5140360	then you'll grasp a lot of concepts that are usable across multiple different types of models.
5140360	5146360	Okay, so we're going to do estimation, like I mentioned,
5146360	5152360	one of the core activities of statistical inference is estimation of the parameter given data.
5152360	5155360	Notice the given, right? There's a conditional,
5155360	5158360	so we're jointly modeling our data and our parameters together,
5158360	5161360	and we're saying, given that we've observed data now,
5161360	5166360	what's the distribution of parameters that we've got, okay?
5166360	5172360	So go ahead, run that first cell.
5172360	5176360	The first thing that we're going to do is we're going to actually look at some,
5176360	5180360	we're going to look at click-through rates, again, the classic binomial thing,
5180360	5183360	rather than coin flips, coin flips are a little too boring.
5183360	5187360	So let's look at click-through rates by loading this cell.
5187360	5193360	Ooh, I lost my kernel again.
5193360	5196360	You know what, if you all don't mind, I'm going to switch over to the instructor notebook
5196360	5199360	and not code along, but I'll make sure that you all have enough time,
5199360	5204360	because I have all the write outputs in the instructor notebooks.
5205360	5216360	Yes, notebook three.
5216360	5220360	So last night I posted on our Slack, do a new Git pull.
5220360	5223360	So make sure you, if you haven't done that, do a Git pull.
5223360	5226360	So that should hopefully clear up the confusion.
5226360	5229360	By the way, for the tutorials, get up on the Slack,
5229360	5236360	because it's a useful channel for the instructors to one way communicate information
5236360	5241360	and the other way get back questions.
5241360	5245360	Okay, so let's look at the click-through rates data.
5245360	5250360	You all have that loaded.
5250360	5256360	Oh, good, my thing's working now.
5256360	5259360	You should get data that looks something like this.
5259360	5262360	So we've done this test.
5262360	5265360	We have a case and a control.
5265360	5271360	And we've measured for every single visitor to our website,
5271360	5276360	whether they clicked on the button within a fixed period of time
5276360	5279360	or whether they just decided to leave.
5279360	5285360	Okay, so how then do we use PMC syntax to build a model
5285360	5294360	that helps us estimate the true value of P with its uncertainty for this data?
5294360	5298360	So if you didn't have PMC three, what might you do?
5298360	5302360	You might do a data frame dot group by, right?
5302360	5314360	So you might do R dot group by group dot mean.
5314360	5315360	Something like that.
5315360	5316360	Oh, well, okay.
5316360	5320360	Yeah, I need to run the cell.
5320360	5321360	You might do something like that.
5321360	5326360	And if someone runs that code, what number do you get?
5326360	5329360	While mine's running, it has to connect to the kernel.
5329360	5332360	Can someone do that?
5332360	5335360	Oh, CTR, my bad.
5335360	5336360	Click through rate.
5336360	5339360	Here, you'll get something that looks like this.
5339360	5346360	You'll get like a 0.14050 for one group and 0.19125.
5346360	5348360	Click through rate for the other group, right?
5348360	5354360	And how much would you believe that data?
5354360	5355360	Maybe true.
5355360	5356360	Maybe not.
5356360	5363360	It depends on how our data were split between the control and the test group, right?
5363360	5369360	Usually, we do a random splitting, but in this case, some malfunction happens.
5369360	5375360	So we only had really 200 data points out of 1,000 for one of the groups and 800 for the other.
5375360	5377360	So we don't have 50-50 splits.
5377360	5379360	We have 80-20 splits.
5379360	5384360	One of the groups is going to be smaller.
5384360	5387360	Oh, it's actually 2,800, so it's even worse.
5387360	5391360	It's not even one of those nice round numbers that we can think about.
5391360	5393360	So, cool.
5393360	5396360	So we've got this skewed amount of data.
5396360	5404360	Which number do you believe in more, given that you know that the test group only has 800?
5404360	5407360	You believe the control number more, right?
5407360	5411360	And that's because we've got more measurements for them.
5411360	5412360	All right.
5412360	5423360	Well, what we're going to do is we're going to spend a bit of time thinking about what the data-generating process looks like for this kind of model.
5423360	5433360	So let that come up.
5433360	5445360	What does the data-generating process look like for a Bernoulli or a binomially distributed data?
5445360	5452360	This, by the way, is the exact workflow by which I go about every single problem.
5452360	5455360	So we've got the question, what is the data-generating process?
5455360	5458360	Well, we start with the data that we have on hand.
5458360	5462360	We have Bernoulli distributed data.
5462360	5482360	So we'll say the likelihood follows a Bernoulli distribution.
5482360	5486360	Okay?
5486360	5490360	We're going to just estimate for the control group.
5490360	5493360	We're not going to do it with two groups just yet.
5493360	5495360	We'll build it for one.
5495360	5509360	So this parameter P, however, how is this distributed?
5509360	5514360	Okay, so we got the value P.
5514360	5517360	How would we model that?
5517360	5529360	Well, if you've never seen click-through rate data before, how would you assign credibility points to the number line to correctly model P?
5529360	5533360	Talk with your neighbor for a minute or two.
5533360	5536360	This time I mean it, like really talk with your neighbor.
5536360	5540360	I hear some things crystallizing.
5540360	5544360	Do we have volunteers?
5544360	5557360	What might you believe about, how would you assign credibility points to the values that P can take on having never seen click-through rate data?
5557360	5559360	Uniform what?
5559360	5560360	Zero to one.
5560360	5561360	So let's try that.
5561360	5575360	We'll say then that P is distributed zero to one uniformly.
5575360	5577360	Okay?
5577360	5582360	This being the likelihood, down, oops.
5582360	5590360	This being the likelihood means we're actually going to, this, sorry, sorry, let me backtrack a little bit.
5590360	5598360	What we've just drawn here on the screen is one generative model for our data.
5598360	5606360	One generative model out of many possible models that we might want to build.
5606360	5612360	This thing that I've highlighted being the likelihood is the thing that we have observed.
5612360	5619360	Okay?
5619360	5622360	Zero.
5622360	5624360	I wish Raveen was here.
5624360	5626360	I would have him copy this model onto the whiteboard.
5626360	5628360	Give me one moment.
5649360	5664360	Okay.
5664360	5666360	So we got the model on the whiteboard.
5666360	5673360	I'm going to switch back and we're going to see how we can actually build that model with PIMC code really easily.
5673360	5675360	Okay?
5675360	5684360	So code along, I believe this is code along.
5684360	5690360	Since we're only doing the estimation for the control group, we're not going to worry about the test group just yet.
5690360	5692360	How do we write this model?
5692360	5696360	Well, we can write it this way.
5696360	5699360	We've got uniform.
5699360	5701360	Right?
5701360	5705360	Kind of looks very, very close to the picture we drew.
5705360	5708360	We'll call this variable P.
5708360	5710360	It needs to have an explicit name.
5710360	5716360	So rule of thumb is whatever you name it in your variable, as the Python variable, just give it the exact same name.
5716360	5721360	Such that PIMC can recognize this.
5721360	5722360	Lower is zero.
5722360	5725360	Upper is one.
5725360	5728360	Okay?
5728360	5738360	And then the likelihood would be PIM.Bernoulli.
5738360	5745360	We'll name it likelihood, like P is equal to P.
5745360	5748360	A Bernoulli distribution only has one parameter.
5748360	5750360	It takes only one parameter in.
5750360	5751360	It's called P.
5751360	5752360	What is it?
5752360	5756360	It's distributed uniformly, having not seen the data.
5756360	5758360	Okay?
5758360	5759360	How are we doing?
5759360	5762360	Okay, so far, syntax, mechanics.
5762360	5771360	And then what we do next is we say observed is inside our data frame.
5771360	5782360	Control-DF clicks.
5782360	5784360	Okay?
5784360	5793360	So that means we've observed a sequence of ones and zeros, whether the user has clicked or not.
5793360	5797360	That is the data that we have observed for the Bernoulli distribution.
5797360	5798360	Okay?
5798360	5800360	So this is one way of writing that model.
5800360	5804360	I'm going to give you all, how are we doing with this syntax so far?
5804360	5807360	This is the mechanics part of building a model.
5807360	5818360	That said, if you think back to what we drew on the whiteboard just now, the syntax looks very similar, right?
5818360	5828360	The syntax here looks very similar to the syntax, sorry, the syntax in code looks very similar to the syntax, the thing that we drew on the whiteboard.
5828360	5830360	The pictures are much easier to reason about, right?
5830360	5836360	We can draw our data generative process here and directly translate it into code.
5836360	5840360	So that's one way of observing it.
5840360	5843360	There is another way, another formulation, right?
5843360	5853360	And if you remember what I said just now, the distribution, sorry, if you remember what I said just now, a sequence of Bernoulli's is binomially distributed,
5853360	5859360	which what that means then is we can actually write the model as a binomial likelihood,
5859360	5865360	but we'll have to change a little bit about how we do, how we structure the data to be input, okay?
5865360	5870360	But I'm going to just give you the binomial model as is,
5870360	5880360	and you can try to break the model or break how we structure the data so you get a better feel on how things should work.
5880360	5884360	Let's run that cell.
5884360	5896360	The next thing that we do, having written our model and telling PMC what we're conditioning on, is we say within this model context,
5896360	5906360	sorry, Model 1 Bernoulli, please sample from the posterior 2,000 times.
5906360	5914360	Give me 2,000 draws that describe how we expect the parameter P to look like.
5914360	5919360	It's a simulation, MC-based, Monte Carlo-based simulation of what the posterior will look like.
5919360	5924360	Now, this is kind of unnecessary for a simple coin flip model,
5924360	5933360	but when we go to slightly more complicated hierarchical versions, you'll see that this comes in really handy, okay?
5933360	5940360	Run that cell and do the same for the binomial.
5940360	5942360	It's literally PM.sample.
5942360	5945360	Literally, that's all you need to do.
5945360	5949360	What's happening here? Fancy math is happening for lazy programmers.
5949360	5951360	That's been the motto of Pi MC3.
5951360	5958360	We abstract away the math, the fancy math that is MCMC sampling or variational inference
5958360	5961360	and allow users to focus on building generative stories.
5961360	5970360	Really, the place when doing PMC modeling, the place that we need to keep our focus on is in the place that I've highlighted up there in the cell.
5970360	5972360	That is, what is the model definition?
5972360	5976360	Is it Bernoulli? Is it binomial?
5976360	5980360	I'd like you to run those two cells.
5980360	5987360	If you run those two cells and you plot the posterior distribution,
5987360	5990360	you'll get something that looks like this.
5990360	5993360	How do you do the posterior plotting?
5993360	6001360	You do RVs for which Ravine is one of the lead developers.
6001360	6016360	It's a visualization tool for taking a look at the results of MC sampling, all right?
6016360	6025360	It's kind to those who still haven't broken out of the histogram land.
6025360	6031360	I need to run all cells above.
6031360	6034360	I'd invite you to run that.
6034360	6036360	You should get something that looks like this.
6036360	6039360	It's happening on the left-hand side of the screen.
6039360	6049360	You'll then get a posterior distribution, a view on what we believe about the value of P,
6049360	6064360	having seen the data and having explicitly stated what we believe prior to seeing the data.
6064360	6067360	For now, I'm not going to go into that.
6067360	6072360	That is something slightly more advanced and that's the contents of Ravine's tutorial.
6072360	6077360	If you're not attending it, then catch the tutorial online,
6077360	6084360	because that's where they'll go into a little bit more of the theory behind the Monte Carlo sampling that goes on.
6084360	6086360	For now, we're going to ignore those errors.
6086360	6090360	What I want to give you is the mechanics of writing the model
6090360	6096360	and the mechanics of sampling and the mechanics of visualizing and interpreting.
6096360	6108360	Do the same for the binomial model.
6108360	6111360	You should get something that looks like that.
6111360	6126360	The values range from 0.125 to about 0.155, the 94% highest posterior density.
6126360	6134360	All that says is that 94% of the credibility is assigned within this black bar region.
6134360	6137360	That's what we believe is true.
6137360	6139360	Question?
6140360	6142360	I understand it.
6142360	6144360	The area was the uniform.
6144360	6147360	We showed all the data that we had.
6147360	6149360	That was the data frame.
6149360	6156360	And I am simply completely able to sample the data.
6156360	6160360	Exactly that.
6161360	6167360	What I'd like you to do then is you've basically got the template
6167360	6173360	that you need to now replicate this for doing two groups within a new model.
6173360	6177360	I'd like you to do is the hands-on activity below,
6177360	6184360	in which you build a model that does both estimations,
6184360	6188360	one for the control group and one for the test group,
6188360	6190360	in about five to ten minutes,
6190360	6194360	such that you get up to this point where you get this plot.
6194360	6196360	Go ahead and do that.
6196360	6199360	Question?
6199360	6200360	Oh, okay.
6200360	6202360	I'll come and address it.
6219360	6221360	Yes.
6221360	6245360	Good question.
6245360	6253360	It happens when you're doing the sampling.
6253360	6258360	It happens when you're doing the sampling.
6258360	6261360	Ah, okay.
6261360	6267360	Okay, okay, okay.
6267360	6268360	Give me a moment.
6268360	6270360	I'm blanking right now.
6270360	6279360	All right.
6279360	6283360	So I'm going to give this to you that it is the math is being executed
6283360	6285360	when you hit PM.sample.
6285360	6289360	But really what happens underneath the hood is we've computed a joint likelihood
6289360	6292360	of all the parameters with the data.
6292360	6300360	And then we use MC sampling to figure out what the typical set of the posterior is.
6300360	6304360	And the goal here is to sample around the typical set.
6304360	6313360	That is the typical range of values for each of the parameters that are involved.
6313360	6325360	All right.
6325360	6327360	Okay, so how many of you are done?
6327360	6329360	Thumbs up.
6329360	6331360	We've got a bunch of people who are still working at it.
6331360	6343360	So if you're stuck, you should have something that looks like this.
6343360	6345360	You can do it with the binomial.
6345360	6348360	So I'm encouraging you to try it with the binomial rather than the Bernoulli.
6348360	6377360	But if you want the Bernoulli, I'll code it live for you all.
6377360	6406360	Okay.
6406360	6415360	Oh, I see.
6415360	6417360	I hear fans running.
6417360	6422360	Someone's sampling real hard.
6422360	6427360	Thank you.
6427360	6431360	So if you're interested in what the Bernoulli formulation will look like, it'll look like this.
6431360	6434360	The binomial formulation looks like that on the right-hand side.
6434360	6441360	All right, okay.
6441360	6448360	And then if we're going to sample from the posterior, once again fancy math happens for lazy programmers.
6448360	6470360	All right.
6470360	6480360	So if you do this final thing, which is a deterministic transform of your random variables,
6480360	6487360	you can actually explicitly compute the posterior distribution of the difference of the two p's.
6487360	6494360	And this would be akin to what you're trying to do if you were to do a t-test of sorts, right?
6494360	6498360	Basically you're just comparing two means and asking how overlapping are they.
6498360	6500360	Are they overlapping or not?
6500360	6501360	A binary decision.
6501360	6505360	Are they completely overlapping or partially overlapping or completely non-overlapping?
6505360	6507360	Is one greater than the other, right?
6507360	6510360	We're asking questions like this basically.
6510360	6512360	So I'll show you how to do that.
6512360	6519360	You can do pdiff is pm.deterministic.
6519360	6523360	And it's nothing more than math on probability distributions.
6523360	6526360	So we'll say ptest minus pcontrol.
6526360	6534360	We'll define test minus control as the diff difference between the two and rerun that.
6534360	6543360	And what we'll get is a posterior distribution on the difference of the two parameters.
6544360	6553360	So I'm really tempted to ask this, but in a t-test would this be significant?
6557360	6561360	So yeah, okay, sure.
6561360	6563360	Now next question.
6563360	6566360	Would the t-test be appropriate?
6566360	6568360	Why?
6574360	6582360	Well, there is an n if we're doing, so that it's not the right thing to do is the correct answer,
6582360	6585360	but I disagree slightly with the reasoning.
6585360	6586360	We do have multiple n's.
6586360	6589360	We have multiple observations here.
6589360	6593360	So that's accounted for.
6593360	6600360	Let me ask you about the, back there.
6600360	6604360	Maybe power calculations are one thing that I had a long Twitter thread on.
6604360	6608360	Thankfully it's not a rant, so you can read it.
6608360	6612360	I'll post that.
6612360	6615360	Yeah, so let's see.
6615360	6620360	Can a probability be normally distributed?
6620360	6624360	No, why?
6624360	6626360	Right, right, exactly.
6626360	6631360	Now we can approximate it with a normal distribution, but we have to be extremely clear.
6631360	6636360	That's an approximation of what already is an approximation.
6636360	6638360	A model is an approximation of the world.
6638360	6640360	We're putting another approximation on top.
6640360	6642360	Whoa, okay.
6642360	6649360	All right, so this is where knowledge of the shapes and the support of the distributions comes in handy.
6649360	6651360	P is a probability.
6651360	6655360	It can never take any value below zero or above one.
6655360	6662360	So why would you impose a normal distribution on the probability parameter p?
6662360	6666360	Especially now, we don't need to do any math.
6666360	6672360	We don't need to write equations out and solve these equations that tell us what the posterior will look like
6672360	6674360	having seen data under a normal approximation.
6674360	6675360	We don't have to.
6675360	6679360	We can explicitly sample from the posterior using MC simulation.
6679360	6687360	So why not go whole hog and just use probability distributions that are bounded from zero to one?
6687360	6688360	Right?
6688360	6690360	Okay, cool, great.
6690360	6694360	So you all just had my, I did a talk at PyCon.
6694360	6700360	It was my 25 minute rant on why we don't always do the t-test,
6700360	6703360	why we should go away from canned statistical procedures.
6703360	6705360	This is one example of it.
6705360	6708360	P is not normally distributed.
6708360	6712360	We might look normally distributed, but it is definitely not normally distributed.
6712360	6713360	It's got to be bound.
6713360	6719360	So you can't take on a probability distribution that is bound from negative infinity to positive infinity.
6719360	6721360	If you do that, you're wrong.
6721360	6723360	Okay?
6723360	6725360	Cool, great.
6725360	6734360	And on this hypothesis testing thing, think about, there's a great blog post by Alan Downey, again.
6734360	6742360	That talks about the fact that every single classical statistical test boils down to one framework.
6742360	6747360	And if you go Bayesian, there's really no reason to calculate p-values and the likes.
6747360	6752360	You just look at posteriors and look at the posterior distributions of statistics,
6752360	6755360	single-valued statistics, single distributed, sorry,
6755360	6757360	single-variate statistics that you're interested in.
6757360	6760360	You don't have to worry about p-values here.
6760360	6761360	Okay?
6761360	6763360	Are we all right with that?
6763360	6766360	Cool, cool, cool, cool, great.
6766360	6773360	Now, knowing that the probability difference is,
6773360	6777360	knowing this probability difference is all good and useful,
6777360	6788360	but it's still not tied to something that is real world and interpretable in the minds of our executive friends, right?
6788360	6792360	So how do we take this metric that we've calculated?
6792360	6796360	p-diff and turn that into something that matters.
6796360	6801360	Well, one thing that you might learn from Raveen's tutorial if you're going or watching it later
6801360	6809360	is that there is this idea of a loss function or a cost function that we can attach to these things of interest.
6809360	6812360	And I'm going to just show you a very simple example, okay?
6812360	6814360	Let's say we know one thing.
6814360	6824360	We've computed with other data and said that our customers on average spend 25 U.S. dollars if they click
6824360	6827360	and zero U.S. dollars if they don't click, right?
6827360	6831360	Like, there's money attached to this process.
6831360	6835360	How that money is attached to this process, we can always write an equation that describes ours.
6835360	6839360	We'll write one arbitrary one, so we don't have to go too deep into what it is.
6839360	6846360	But if you take this, if you look at what's in the trace for p-diff,
6846360	6853360	you'll notice it's nothing more than a sequence of 2,000 draws from that posterior, okay?
6853360	6856360	So run that on your own notebooks as well.
6856360	6858360	Just open up and view what p-diff is.
6858360	6861360	It's a NumPy array. It's got 2,000 numbers, okay?
6861360	6870360	So a difference in probability can translate into a difference in amount of money being spent.
6870360	6876360	And if we attach a single, you know, a single dollar value to each and every one of those,
6876360	6879360	rather than a distribution, that's not complicated for the moment.
6879360	6891360	We can do something like dollar distribution is trace of p-diff times 25 times 1 million, right?
6891360	6897360	Over a million customers, there's an increase in the probability that they spend money,
6897360	6899360	so under the test group.
6899360	6903360	And so how much increase do we expect over a million customers?
6903360	6905360	Let's translate into some real numbers.
6910360	6911360	We'll do that.
6911360	6920360	And finally, you should get something like that.
6920360	6922360	You should get something like that.
6922360	6924360	So what does this say?
6924360	6929360	Well, on average, we expect a lot of revenue increase,
6929360	6934360	but there is always this very, very small tail probability that we still might lose money.
6935360	6941360	And that's just the nature of our computation, okay?
6941360	6943360	So I just wanted to put that out there.
6943360	6947360	You can always tack on, this is highly custom per problem,
6947360	6951360	but you can always tack on a loss function or a cost function that describes,
6951360	6955360	that ties the metric of interest, you have to think about it,
6955360	6963360	to some dollar amount or some hours spent by people on a particular problem, okay?
6963360	6969360	And that's a way of communicating to so-called non-technical folks
6969360	6976360	who want to be maybe a little bit more spoon-fed on what we expect to see, okay?
6976360	6978360	Are we okay with this so far?
6978360	6981360	So far, so good.
6981360	6984360	Any questions?
6984360	6987360	Okay, if there are no questions, I want you to talk with your neighbor for one minute,
6987360	6989360	one new thing you learned.
6989360	6991360	Anybody want to share?
6991360	6994360	Who knew they learned? That's far.
7004360	7006360	Yep, yep, that's right.
7006360	7008360	That's the point of this first exercise.
7008360	7011360	Get you familiar with that mechanics of doing so.
7011360	7013360	Anything else?
7015360	7017360	Any, back there?
7022360	7029360	And that's the default sampler.
7042360	7045360	Yep, yep, absolutely, absolutely.
7052360	7054360	Yep.
7057360	7059360	How so?
7062360	7064360	Yep.
7066360	7069360	Oh gosh, so, yeah.
7072360	7074360	Yep.
7074360	7076360	Oh gosh.
7077360	7079360	Yep, and that's...
7082360	7084360	Right, right, absolutely.
7084360	7089360	And so, that actually brings up a very highly related point.
7089360	7092360	We built statistical models of the world.
7092360	7094360	We built statistical models of the world.
7094360	7100360	When I built the cost function, it was both somewhat of a hybrid of a statistical model
7100360	7102360	and a mechanistic model of the world.
7102360	7108360	That is, people who click will now spend money on average $25 per click,
7108360	7112360	and so, we do some multiplication and that expresses some mechanism.
7112360	7115360	But ultimately, they're models, and we have to validate them.
7115360	7119360	And so, that problem you brought up is, I think, one of the problems
7119360	7122360	that we don't know how to validate properly, I think,
7122360	7127360	because we don't have the negative data to build a good model.
7127360	7130360	We have all the positives, the successful molecules.
7130360	7134360	We don't know what the opportunity cost and how to model the opportunity cost
7134360	7140360	of those failed, you know, incorrectly modeled molecules would be.
7140360	7143360	Yeah, that's a very good point.
7143360	7145360	Anything else?
7145360	7147360	Okay.
7147360	7150360	If there are no other points, you'll notice...
7150360	7152360	All right, we have the case and the control group.
7152360	7155360	If you listen to many of my rants,
7155360	7158360	case and control isn't the only thing you can do.
7158360	7161360	You really can do, like, arbitrary number of groups
7161360	7165360	that are having to worry much about multiple hypothesis correction
7165360	7169360	and the likes and, like, you know, having this guillotine of p-values
7169360	7171360	which you chop off as you go down.
7171360	7175360	Oh, gosh, that's, like, I don't know how people came up with that,
7175360	7178360	but it's complicated, right?
7178360	7183360	Whereas, like, just looking at posteriors is so much more clean,
7183360	7185360	much easier to interpret as well.
7185360	7190360	So, we're going to do another example that's still the binomial story,
7190360	7193360	still the Bernoulli binomial story,
7193360	7199360	but it's going to show you how, like, we can't just copy-past a test control
7199360	7204360	player 1, player 2, player 3, player 4, player 857, right,
7204360	7207360	or write a for loop with functions, and that's just, like, way too much.
7207360	7210360	We can actually take advantage of some syntactic things
7210360	7214360	that allow us to write in a very concise fashion
7214360	7218360	these models that model multiple, you know, more than two groups, okay?
7219360	7224360	So, like y'all to scroll down, we're going to look at baseball data,
7224360	7228360	and this is one of the classic, classic,
7228360	7232360	this is one of those classic data sets that one would pick up, right?
7232360	7236360	So, we want to, we want to model the probability
7236360	7240360	that a player who is a batter will hit a pitch, right?
7240360	7243360	So, they have this stat called at bats and number of hits.
7243360	7247360	At bats is the n, the total number of times
7247360	7249360	that they've come up for batting,
7249360	7252360	and h hits is the total number of times
7252360	7254360	that they've actually hit the bat.
7254360	7257360	So, we've got some data from the baseball database,
7257360	7259360	all credit to them.
7259360	7263360	I'd like you to run this cell that loads the data.
7263360	7267360	Little pitch for a tool that I've been developing
7267360	7270360	alongside colleagues here, so Zach in the back,
7270360	7272360	he also works on this.
7272360	7275360	It's called Pyjanitor, and it's basically there to help you
7275360	7278360	make data preprocessing easy to read,
7278360	7282360	so that you have a clean API for cleaning data, all right?
7282360	7286360	So, we've got the data, I'd like you to load that.
7286360	7290360	You should see something that looks like this, okay?
7290360	7294360	We've got at bats, hits, the salary of the player,
7294360	7297360	and this extra column which we will use,
7297360	7301360	it's called player ID underscore encoded, okay?
7302360	7306360	It's basically just an integer encoding of the player ID,
7306360	7308360	nothing more than that.
7308360	7311360	You'll find out why that becomes handy later.
7311360	7318360	So, once again, it's the same old Bernoulli binomial sorry.
7318360	7323360	Now, because we have the data structured as at bats and hits,
7323360	7326360	which is the, what should we,
7326360	7329360	what is the likelihood that we want then?
7331360	7334360	Is it Bernoulli or binomial?
7335360	7337360	Pardon me?
7339360	7342360	Binomial, and why is it binomial?
7342360	7345360	Yes, because we know exactly how many times.
7345360	7347360	We're not recording every single bat.
7347360	7350360	We're summarizing, we're taking the summary statistics,
7350360	7353360	which is the total number of at bats and the total number of hits,
7353360	7355360	and this then forms the binomial story
7355360	7359360	in which we know the number of times something has come up for trial.
7359360	7362360	Something has come up for a test of whether they succeed or not, okay?
7362360	7364360	This is distinct from the Bernoulli,
7364360	7366360	where we only know that they're coming up,
7366360	7369360	and we just, we only know that they're coming up
7369360	7373360	for a success failure trial, and we know the probability,
7373360	7376360	but we don't have the number of times that they've got that, okay?
7376360	7380360	So, we'll be using a binomial distribution as a,
7380360	7383360	we'll be using a binomial distribution as the likelihood.
7383360	7386360	So, let's build this model,
7386360	7390360	and what I want you to do is to notice some syntactic changes here, okay?
7390360	7397360	So, first we'll have the pitch model.
7397360	7399360	We're going to build that.
7399360	7405360	I'm going to switch over now to a beta distribution, right?
7405360	7409360	A beta distribution is also bounded from 0 to 1.
7409360	7411360	It has the correct support.
7411360	7415360	A beta distribution also allows, has this parameter where,
7415360	7419360	two parameters that let us control the shape where it's skewed.
7419360	7421360	Is it skewed left or is it,
7421360	7425360	does it have more mass on the left or density on the right?
7425360	7428360	It lets us do all, it lets us do that, right?
7428360	7430360	It lets us control the shape of the distribution.
7430360	7434360	So, it's not just a simple flat uniform prior.
7434360	7436360	So, code along with that.
7436360	7438360	P is the name of the thing.
7438360	7440360	Alpha is 1, beta is 1.
7441360	7447360	And then the shape of this, this is a new thing.
7447360	7456360	The shape of this beta distribution is the number of players that we have.
7456360	7459360	So, all we're expressing now is that we've got,
7459360	7463360	instead of a single beta distribution that lives in memory,
7463360	7466360	we've got a vector of beta distributions.
7466360	7469360	One beta distribution per cell, right?
7469360	7473360	And that beta distribution maps onto one particular player, okay?
7473360	7475360	That's, that's what we're doing here.
7475360	7480360	The likelihood is binomially distributed.
7480360	7487360	So, PM.binomial, its name is like,
7487360	7493360	its P is distributed according to the beta distribution,
7493360	7495360	but it's a vector of distributions.
7495360	7500360	So, what this effectively does is it creates a vector of binomials, okay?
7500360	7503360	That's the key syntactic difference and conceptual difference
7503360	7505360	that you need to take away from this example.
7505360	7507360	You can get away with doing,
7507360	7514360	you can get away from for loops by simply vectorizing everything, okay?
7514360	7518360	N is data at bat, right?
7518360	7520360	This is the number of times they've come up.
7520360	7526360	And it's also a vector that is the same length of P.
7526360	7541360	And finally, observed is data hits, okay?
7541360	7544360	My line wrapping is coming into play.
7544360	7549360	Maybe I shouldn't set this in the Jupyter Notebook.
7549360	7554360	Now, we got that, but since we have salary information,
7554360	7557360	we've been talking about like,
7557360	7562360	we've been talking about tying things to real world numbers, right?
7562360	7568360	Let's compute a metric that says the probability of batting per unit of salary, right?
7568360	7572360	So, how, how we want that number to be as high as possible, right?
7572360	7575360	So, we want for the smallest salary, someone,
7575360	7577360	we want to figure out, for the smallest salary,
7577360	7581360	someone who bats has the highest batting percentage.
7581360	7584360	That would play right into the sabre metrics kind of thing, right?
7584360	7588360	You're looking for value for money on different metrics.
7588360	7592360	So, we'll do a deterministic transform.
7592360	7597360	We'll call this P per salary, PPS, okay?
7598360	7605360	And simply take P and divide it by the salary that we observe in the data.
7605360	7608360	How are we doing? Any questions so far?
7608360	7609360	So far, so good.
7609360	7615360	Following along, story, we, it's, by the way, the exact same data generating process,
7615360	7622360	the exact same story as we drew on the whiteboard.
7622360	7624360	Go ahead, do sampling then.
7624360	7628360	And look at, look at the posterior distributions.
7641360	7643360	So, notice how we're sampling.
7643360	7645360	It's fast.
7645360	7648360	I'm taking advantage of the GPU that I have at home.
7648360	7653360	This one in this particular case, I don't think it'll run any faster on the GPU than on,
7653360	7661360	on a CPU because there's no complex matrix multiplies that are going on.
7661360	7664360	But if you were to do like more complex things,
7664360	7666360	complicated things like Bayesian neural net,
7666360	7669360	which you actually can write in piano and IMC three,
7669360	7671360	there's totally no problem with that.
7671360	7679360	Then moving stuff onto the GPU can get you up to four to four to eight times faster sampling and fitting.
7679360	7685360	Okay, so we have built here.
7685360	7688360	Oh, I named it trace batting.
7688360	7698360	Let me resample again.
7698360	7708360	So what, what I've done then below is create this custom visualization that relies on IPI widgets and the likes to get it working.
7708360	7715360	The intent here is that if you were to visualize posterior distributions for 805 players,
7715360	7719360	that's 805 matplotlib axes that you're drawing to screen.
7719360	7721360	It's not the prettiest thing.
7721360	7727360	Okay, so we'll take advantage of some interactivity that we can build to.
7727360	7730360	Oh, it's not working on my screens, but it should be working on some of yours, right?
7730360	7737360	If you've got IPI widgets working, you'll get a select that you can actually look at and select multiple players.
7737360	7745360	And compare their, their P per salary and their cumulative, the cumulative distributions for each of those.
7745360	7761360	Okay, actually, while this is running, I'm just going to very quickly get up and running.
7761360	7772360	And so while you have that visualization up, I'd like you to try to hunt for the player that's got the highest PPS distribution.
7772360	7781360	And while I install some, while I reinstall some things, once, once I'm finished with the commands, we'll come back and talk about that.
7791360	7818360	Okay, so anybody found interesting players?
7818360	7828360	Yes.
7828360	7836360	That's right. So one thing that we have as an idea in Bayesian statistics is that in the limit of lots of data,
7836360	7842360	what your priors are really don't matter unless, unless you chose priors that can never change,
7842360	7849360	which means you're just hard coding your great uncles, your uncles viewpoints on politics, which will never change, right?
7849360	7865360	So, yeah.
7865360	7883360	Maybe I just need to reload.
7883360	7890360	My, my, my, my, it's very slow.
7890360	7899360	So do we have players that are, that look interesting?
7899360	7902360	Pardon me?
7902360	7904360	Okay.
7904360	7906360	Can you help me out a little bit?
7906360	7916360	What does their CDF look like?
7916360	7917360	Sure.
7917360	7924360	Exponential being, sorry.
7924360	7927360	Exponential being what?
7927360	7933360	This, this, this, this line that looks like that.
7933360	7937360	Okay, some look like this, right?
7937360	7939360	You'll have noticed some of that.
7939360	7943360	Any other patterns?
7943360	7950360	Yeah, squiggly, but more or less straight line.
7950360	7954360	Okay, let's talk about what each of these CDFs express.
7954360	7960360	What's this one expressing?
7960360	7971360	Yeah, essentially it's expressing that the credibility points are assigned anywhere from the lowest to the highest in pretty much a uniform distribution.
7971360	7978360	What about something that looks like this?
7978360	7982360	How tight is the distribution compared to the first one?
7982360	7984360	Very tight.
7984360	7986360	And it's also very shifted to the right, right?
7986360	7989360	Because this, the central tendency is way out over there.
7989360	7990360	Okay.
7990360	7992360	And then what else do we have?
7992360	7994360	We have this guy over here.
7994360	7999360	What's this guy like?
7999360	8001360	Something in the middle.
8001360	8002360	Something in the middle.
8002360	8006360	Still takes on lots of values, but the distribution is kind of skewed as well.
8006360	8007360	Right?
8007360	8010360	Because there's a, there's a long tail on the left.
8010360	8016360	Lots of, lots of probability or lots of credibility assigned to the middle over here.
8016360	8018360	And then it peters off at the top.
8018360	8028360	It'll always peter off at the top.
8028360	8035360	Once again, it's, it's the richness of statistical information.
8035360	8038360	So PDFs look a lot like CDFs.
8038360	8040360	Sorry, PDFs look a lot like histograms.
8040360	8042360	I take that back.
8042360	8044360	PDFs look a lot like histograms.
8044360	8055360	And so you can't tell more than the bounds and central tendency from a PDF.
8055360	8063360	Whereas you can tell quartiles and percent, percentiles and roughly estimate where they are from a CDF.
8063360	8077360	So most of the time I would just default to using a CDF rather than a histogram or a PDF.
8077360	8078360	Okay.
8078360	8080360	Let's see if this works.
8080360	8082360	My widget doesn't display.
8082360	8083360	Never mind.
8083360	8085360	I'll have the widget on your laptop.
8085360	8089360	So you'll be able to view that.
8089360	8090360	Okay.
8090360	8092360	So there are some interesting players.
8092360	8101360	One thing that's kind of interesting though is that we've got players like the uniform distribution one, right?
8101360	8103360	Super uninformative.
8103360	8109360	And that raises a problem.
8109360	8112360	The problem sounds something like this.
8112360	8115360	Now I'm going to pose this as a question to you all.
8115360	8118360	So think about it.
8118360	8137360	Having seen players, professional players play and do their thing, do you really expect that having a few at-bats,
8137360	8144360	that comes from the situation where we have like one or two at-bats and zero or one hits.
8144360	8160360	You really expect that having seen that one or two at-bats, we should still believe that their batting capability is at this near uniform from zero to one.
8160360	8162360	Talk about that with your neighbor.
8162360	8167360	And then tell me why.
8167360	8168360	Yes.
8168360	8177360	So do you really, should we really believe having seen one or two at-bats that performance would still range so wildly from zero to one basically?
8177360	8179360	What are your answers?
8179360	8184360	Should we really believe what we saw in the posterior results?
8184360	8191360	That performance gets basically, is still uniformly or close to uniformly distributed?
8191360	8193360	No, I see a head shaking.
8193360	8196360	Tell me why.
8196360	8197360	Both of you.
8197360	8213360	Both of you are a team.
8213360	8217360	Right, right.
8218360	8223360	So what we're encoding here is this notion.
8223360	8227360	So the response is pretty much how we would think about it, right?
8227360	8236360	The professional baseball players don't tend to have performances that range so wildly.
8236360	8241360	That is a piece of prior information that we can impose on the modeling problem.
8241360	8243360	So how do we impose that?
8244360	8245360	There are two ways.
8245360	8247360	One, we can have a stronger prior.
8247360	8252360	A stronger prior that is imposed on every single player.
8252360	8254360	So we might do a beta.
8254360	8271360	So if batting averages tend to fall within the range of 0.2 to 0.3, we might put a beta distribution with, so 0.2 is about approximately one success and four failures.
8271360	8274360	So it would be a beta distribution of one and four.
8274360	8276360	That is a slightly stronger prior.
8276360	8285360	Or if we wanted it to be even stronger, we would do a beta distribution of 10 and 40, which is much narrower compared to the beta of one and four.
8285360	8288360	If you don't believe me, go simulate it in NumPy.
8288360	8290360	The beta distribution is right there.
8290360	8296360	And if you really wanted to be a really strong prior, then it would be beta 100, 400.
8296360	8302360	That is the 0.2 batting average kind of prior we could put on here.
8306360	8325360	So the advantage of using the beta over the uniform is that I can now tweak the alpha and beta parameters of the beta distribution to change where we center the distribution and also change how wide or thin that distribution is.
8326360	8333360	So as I was mentioning, beta 1, 4 looks something like this, skewed, but kind of wide.
8333360	8336360	Beta 10, 40 looks something like this.
8336360	8340360	And beta 100, 400 looks something like that.
8340360	8342360	Are we okay with that?
8342360	8350360	So beta distributions give us a little bit more control over the shape of the distribution.
8350360	8355360	Okay, so putting tighter priors is one way.
8355360	8365360	Another way to approach this is actually to impose what we would call a hyper prior, one that governs the population of players.
8365360	8368360	So I'm going to switch over to drawing again.
8378360	8383360	We have the binomial likelihood.
8383360	8385360	And we know this already, right?
8385360	8398360	This is a very familiar story for us by now.
8398360	8406360	In the interest of saving space, I'm not going to draw out the distributions, but picture them in your head.
8406360	8410360	N is known.
8410360	8421360	P comes from another distribution.
8421360	8431360	The way I want you to think about this though is because we vectorized everything.
8431360	8449360	We've got a vector of binomial likelihoods and a corresponding vector of beta distributions for priors on the P parameter.
8449360	8464360	If we were to do this hierarchically, what we are effectively doing is asking what is the population alpha and beta look like?
8464360	8472360	That's kind of ugly.
8473360	8476360	And we only have one of these each.
8476360	8481360	We don't have a vector of them because they're governing the entire population.
8481360	8486360	So when we do this hierarchical thing, we're effectively expressing this idea.
8486360	8491360	Players themselves are drawn from a parental distribution.
8491360	8501360	Professional players all generally follow some general distribution that is governing the performance of the individual players.
8501360	8517360	So the population distribution, which is imposed on A and B, that governs the individual player's performance parameter, which is the beta distributed thing,
8517360	8523360	which then influences the outcomes that we're interested in.
8523360	8534360	So let me just annotate that.
8534360	8539360	Up there we have the population parameters.
8539360	8546360	We have the individual parameters for P.
8546360	8555360	We have the likelihood which governs how our outcomes, the data that we observe, are generated.
8555360	8558360	Okay? Are we okay so far?
8558360	8570360	Now, I'm going to go on a limb and tell you that A and B, these two parameters, they also have distributions because then otherwise it wouldn't be probabilistic.
8570360	8576360	But now the question is, what is an appropriate distribution for A and B?
8576360	8579360	What might be an appropriate distribution for A and B?
8579360	8583360	I'll tell you a few snippets.
8583360	8591360	A and B govern the number of successes and failures effectively for the population of players.
8591360	8596360	This happens over a single year.
8596360	8608360	It can only be positive, so it being over a single year means there's generally a finite number of positives and successes and failures, A's and B's that every player has.
8608360	8615360	So given that you know that, what might be a suitable distribution? Let's not worry about the shape of the distribution just yet.
8615360	8623360	What is a suitable distribution?
8623360	8631360	For Poisson maybe, it being, I forgot to say, A and B can actually be continuous.
8631360	8640360	And that's sort of a giveaway that Poisson's not so ideal but we could try it.
8640360	8646360	What's a positive bound continuous distribution that you might have heard of?
8646360	8651360	Exponential is one. There's another one that we can define which is the half normal.
8651360	8658360	There is the normal distribution but chopped up in half such that now the support is defined only on the positive half.
8658360	8660360	That's one way to do it.
8660360	8666360	Gamma distribution is another. Gamma is actually I think a generalization of a number of child distributions.
8666360	8670360	It's got more parameters, it's a little bit more complicated, but yeah, we can totally do that.
8670360	8675360	Lognormal I think is as well, yeah.
8675360	8678360	Half Cauchy as well, like half student T, etc.
8678360	8682360	Yes, definitely. So we're all on the right track. We're thinking of distributions.
8682360	8689360	The key point is these distributions have to be positive bound because A and B can only take positive values.
8689360	8695360	If we were to do anything with the full normal distribution, we'd be doing it the wrong way.
8695360	8702360	So for simplicity's sake, let's assign A and B to take exponentials.
8702360	8709360	And the only reason I would start with this but maybe not end with it is that exponentials are easy.
8709360	8716360	They have a single parameter so there's not much hyper hyper parameters that we have to worry about.
8716360	8723360	This A and B thing that we're assigning distributions on, these are what we would call hyper priors.
8723360	8728360	Hyper being an added dimension, an added dimension of modeling that we're doing here.
8728360	8736360	So for the sake of simplicity, we'll start with a simple exponential.
8736360	8744360	What did I do in the real thing?
8744360	8754360	In the actual thing, we've used 1 over 29, which is just an arbitrary number, which we can always debate about, but we're not going to today.
8754360	8760360	We're going to do that in a long modeling critique session that my colleague Zach and I have done umpteen times now,
8760360	8763360	where we debate our priors, debate the model structure.
8763360	8766360	For now, for learning purposes, we'll just stick with that.
8766360	8770360	So let's take this model and code it up.
8770360	8772360	I'm going to switch back to the notebook.
8772360	8779360	I'd like to encourage you all to also switch over.
8779360	8789360	Inside the notebook, you will see that we've already got the binomial likelihood and the PPS metrics, the deterministic transforms defined for you.
8789360	8800360	So now I'd like you to code along and let's fill in the rest for the beta distribution and the A prior and B prior.
8800360	8807360	So we have alpha is A prior, beta is B prior.
8807360	8814360	The shape is still length of data.
8814360	8823360	And then we'll have exponential 1 over 29.
8823360	8830360	Oops.
8830360	8835360	Make sure it's all floating points.
8835360	8841360	And let's call it hierarchical baseball.
8841360	8858360	So once you've coded up the model, go ahead and sample from it and tell me what you see is kind of different.
8858360	8871360	You'll notice also this model is a little slower to sample from.
8871360	8879360	Okay, so while you all are waiting for models to sample and finish up, questions?
8879360	8887360	Yes, no problem.
8887360	8893360	Well, while things are sampling, it's actually a great time to talk with your neighbor about something new you've learned.
8893360	8896360	Okay.
8896360	8912360	Before we go on into something new you've learned, I want to ask a few questions about what you're observing about these baseball player posteriors having seen this having been fit under this hierarchical model.
8912360	8923360	What's different from what you saw before?
8923360	8930360	Okay, so you see more sigmoidal type of distributions rather than uniform or exponential types.
8930360	8932360	That's one good one.
8932360	8937360	What's another property that you're observing as well?
8937360	8940360	What are the bounds and ranges?
8940360	8941360	Pardon me?
8941360	8943360	They're more tight.
8943360	8945360	They're more tight.
8945360	8955360	And this is the result of this type of switching over to a hierarchical model rather than using an independent model.
8955360	8966360	So the first model that we wrote where every player is modeled as a beta distribution on its own, that is what I might call an independent model.
8966360	8974360	And then the hierarchical model actually sort of pools these player properties as being drawn from one parental distribution.
8974360	8979360	So they're sort of constrained by the parental distribution.
8979360	8983360	This is a property of hierarchical models.
8983360	8987360	I'm not going to design a value judgment on whether this is always good or always bad.
8987360	8989360	It depends on the problem.
8989360	9013360	But if it is justifiable by your modeling domain expertise, then a hierarchical model is actually a really powerful way to borrow information from players that have had lots of, from the population of players to do inference on the players that we have not had much information about.
9013360	9029360	So for those players that had one at bat and one success, what you will notice is that their posterior distribution is still, is going to be kind of wide, not as crazy wide as it was before.
9029360	9039360	It's going to be kind of wide, centered roughly around what the population mean is and follow roughly what the population mean is as well.
9039360	9053360	If you have something that's a little bit more extreme, like seven at bat, seven hits, then you'll get something that's shifted to the right because there's a little bit of information saying that this player is kind of good or maybe lucky, we don't know.
9053360	9063360	There's a bit of, it'll be shifted to the right, it'll be narrower, but it does express that, you know, it's not going to be wildly like 90, 90 something centered on 90 to 100%.
9063360	9066360	It's going to be centered off, shifted off.
9066360	9079360	So at least in this setting, it correctly encodes our intuition that players generally fall within this like population distributed.
9079360	9086360	They follow the population distribution much more than we would expect from just looking at them independently.
9086360	9097360	Okay, so this is a very powerful thing, that phenomena where you have these wild estimates being shrunk towards the population mean is called shrinkage.
9097360	9100360	Shrinkage is a term you'll want to look out for in the literature.
9100360	9104360	Okay, so you have this vocabulary that you won't be confused by.
9104360	9116360	Okay.
9116360	9118360	Yep.
9135360	9148360	Okay, okay, cool.
9148360	9158360	So the beta that we've put in there expresses a prior that is unconnected to any other player.
9158360	9163360	So the prior for the beta distribution, the A and the B,
9163360	9170360	the priors that we put, they are, even though they were point estimates, they're just saying this is, this is the shape.
9170360	9175360	We're putting an identical shape of distribution on every single player.
9175360	9185360	Now when we connect the players by saying they all draw from a population distribution, it's not that we're putting uncertainty.
9185360	9190360	I would be hesitant to say that we're putting uncertainty on A and B.
9190360	9197360	Rather, we're expressing that their shapes are now controlled by a population shape, right?
9197360	9200360	That's where, that's where this shrinkage comes in.
9200360	9206360	It's not that we've, we were really sure and so we assigned a single point value.
9206360	9217360	We know that the two point values can give one shape, but that shape for the beta distribution was unconnected to the population at first.
9217360	9219360	That's all it was.
9219360	9220360	Okay.
9220360	9231360	And now when we have a connected set of shapes, right, so we have a connected beta distributions by the hyper priors that we put on.
9231360	9245360	What we're saying is that there is a population shape for the beta and they're influencing and governing the individual player shapes, the beta distribution shapes.
9245360	9248360	Does that make sense?
9248360	9257360	By ironically putting a distribution on it, not putting a ring on it.
9258360	9263360	Yes, yes, yes.
9263360	9268360	So if in doubt, don't put a ring on it, put a distribution on it, okay?
9268360	9271360	Cool.
9271360	9279360	Right, so that's, that's that phenomena of shrinkage that I wanted everybody to have some intuition about, okay?
9279360	9282360	So let's now go into something that you've learned.
9282360	9288360	We've actually come to the end of the binomial story and we've gone really, really deep.
9288360	9301360	We've come from like the simple naive one, one group to two groups to now vectorizing over multiple groups to then now adding on a hierarchical model on top.
9301360	9306360	I'm hoping it's not yet information overload because there's more.
9306360	9308360	So what's something new you've learned?
9308360	9311360	And let's get that like etched in your head.
9311360	9318360	You have volunteers.
9318360	9319360	Sure.
9319360	9320360	Yeah, cool.
9320360	9321360	Great.
9321360	9323360	That was part of the point.
9323360	9332360	Anything else, something new that you didn't expect or something that has been resonating with you?
9332360	9333360	Maybe on this side.
9333360	9338360	This side has been really quiet.
9338360	9342360	I'm going to point at someone.
9342360	9349360	Second last row middle guy.
9349360	9358360	Anything new you've learned?
9359360	9360360	Okay, still processing.
9360360	9361360	That's completely valid.
9361360	9362360	That's totally cool.
9362360	9369360	And the fact that you're still not sure means there's processing going on and I fully appreciate that.
9369360	9370360	How about in the middle?
9370360	9371360	Anybody else?
9371360	9377360	Any volunteers?
9377360	9380360	Sorry, can you say it louder?
9380360	9385360	Okay, so reinforcing the value of the cumulative distribution plots, right?
9385360	9387360	That's super important.
9387360	9396360	The fact that we get richer information from that is very useful.
9396360	9398360	Oh, the beta distribution.
9398360	9399360	Ah, yes.
9399360	9405360	The fact that we're able to constrain and shape.
9405360	9407360	Yes, yes, exactly.
9407360	9408360	Exactly.
9408360	9412360	It's a very useful tool to have in the toolkit.
9412360	9425360	I think it's still, like, I don't know if it's going to fit in here, but in my, I don't know if it's going to fit in here.
9425360	9430360	Yep.
9430360	9431360	Cool, cool.
9431360	9432360	Awesome.
9432360	9433360	And back there.
9433360	9434360	Last one.
9434360	9436360	I knew what I knew.
9436360	9437360	Yeah.
9438360	9443360	Oh, yeah.
9443360	9445360	Yep.
9445360	9447360	Yep.
9447360	9448360	Yep.
9448360	9449360	Yep.
9449360	9450360	Absolutely.
9450360	9451360	There are lots of good connections there.
9451360	9457360	So a lot of, a lot of the classical stats are connected in this way.
9457360	9466360	Yes.
9466360	9467360	Yeah.
9467360	9474360	I was hoping that this question would come up and trust me, I did not plant her in the crowd.
9474360	9481360	So when we think about which distribution to use, there are a few rules of thumb.
9481360	9487360	The first rule of thumb is find something that has the correct support.
9487360	9489360	That is absolutely crucial.
9489360	9495360	If you use something that's got the wrong support, that is, you've got data that showed up negative.
9495360	9502360	You've got data that are showed up negative, can take on negative values, but you put a
9502360	9504360	positive only distribution inside there.
9504360	9508360	You're going to get not a number errors inside sampling.
9508360	9509360	Right.
9509360	9513360	And that also means that you've not, you've missed something in the modeling process.
9513360	9519360	So getting the support correct is the first step.
9519360	9523360	And the next step is to think about the likelihood.
9523360	9524360	Right.
9524360	9528360	That's where knowing, so that's, that's for any arbitrary distribution.
9528360	9531360	Getting the support correct is absolutely crucial.
9531360	9535360	The next thing is to think about the likelihood function.
9535360	9540360	How are the data that you are interested in, the thing you've actually measured?
9540360	9545360	How is that, how is that distributed?
9545360	9551360	So that's where knowing the probability distribution stories comes into place.
9551360	9558360	Especially rules of thumb are if you've got something that's got amount of stuff happening
9558360	9560360	per unit time, it's put on.
9560360	9565360	If you've got trials that are positive, negative, it's Bernoulli binomial.
9565360	9566360	Right.
9566360	9568360	These are very generalizable stories.
9568360	9574360	If you're really unsure, you might start with a normal distribution.
9574360	9580360	If you've got some other types of processes, so for example, the negative binomial distribution
9580360	9585360	counts the number of failures until a success.
9585360	9586360	Right.
9586360	9589360	So knowing this generative story helps as well.
9589360	9590360	Okay.
9590360	9596360	So, and then also there's this family of distributions called the zero inflated distributions.
9596360	9599360	So you can have the zero inflated Poisson distribution.
9599360	9603360	What it expresses is that there's, there are two processes at play.
9603360	9606360	There's a process that generates lots of zeros.
9606360	9611360	And then there's a process that generates the Poisson side of that and there's, there's,
9611360	9613360	this is essentially a mixture model.
9613360	9620360	So you're now having to infer both the probability that it is in the zero versus not zero P and
9620360	9626360	one minus P as well as the Poisson parameter, the rate parameter of interest.
9626360	9632360	And it, it, it's really important to think through the, that part of the problem.
9632360	9634360	So that's the second part.
9634360	9640360	And the third rule of thumb is to think about what the shape of the distribution should
9640360	9641360	look like.
9641360	9646360	So this is where I would then look at the PDF rather than the CDF because this is all analytical
9646360	9652360	because then it gives me a sense of the skew and the central moments of the distribution.
9652360	9657360	And it can help me express quantitatively what I'm thinking about.
9657360	9662360	So the beta distribution is that classic anchoring example that I always come back to.
9662360	9667360	It's bound from zero to one and it's therefore suitable for a probability parameter.
9667360	9674360	I can tweak whether it's centered on 0.5, 0.2, 0.9 by simply tweaking the A and B parameters.
9674360	9681360	And I can tweak how, how tight that distribution is by doing, you know,
9681360	9686360	beta 91 versus beta 9010 versus beta 900, 100.
9686360	9687360	Right.
9687360	9691360	So there are ways to control the, the shape of the distribution that way.
9691360	9694360	Those are the three rules of thumb.
9694360	9698360	What I, in practice, find myself doing is thinking about the problem and going like,
9698360	9702360	ah, yeah, I need something that's positive here because that can only take on positive values.
9702360	9707360	So then I'll go hunting in the distribution library for something that's positive.
9707360	9713360	And most of the time we're sort of expressing, you know, say for a standard deviation parameter,
9713360	9719360	we're expressing the fact that things generally are not going to be wildly,
9719360	9722360	um, standard deviation parameters are generally like tight,
9722360	9724360	but then sometimes can take on high values.
9724360	9728360	So I might take like a half koshi because standard deviations can only be positive,
9728360	9733360	but I'm allowing for really high tails or half student T, for example.
9733360	9734360	Okay.
9734360	9736360	So that's a few examples.
9736360	9737360	Back there.
9737360	9763360	Okay.
9763360	9764360	Yeah.
9764360	9769360	Um, so the exponentials, you can, sorry, so choosing an exponential,
9769360	9774360	you can think of it as this is the first model I'll write.
9774360	9777360	And then if you go to ravine's tutorial tomorrow,
9777360	9780360	there's this whole business of model comparison.
9780360	9784360	That's, um, sometimes you'll find it doesn't really matter what the hyperpryor is.
9784360	9791360	And sometimes it does matter when we check things like the information criteria metric,
9791360	9795360	uh, that, uh, the information that's contained in inside the model.
9795360	9798360	Um, sometimes you'll find whether it's half koshi or exponential,
9798360	9800360	just quantitatively, it doesn't really matter.
9800360	9809360	Um, so in some senses start with something and then run with it and then be ready to change the model.
9809360	9810360	Yep.
9810360	9811360	Yep.
9811360	9818360	And I emphasize that, uh, we, we don't want to really get into debating that choice,
9818360	9821360	but we can actually offline debate that choice if we want.
9821360	9826360	We can look at how the Lambda parameter controls the shape of the exponential distribution
9826360	9830360	and whether that expresses qualitatively what we're intending to express.
9830360	9831360	Right.
9831360	9835360	So some, the exponential distribution generally starts high and then goes low.
9835360	9836360	Right.
9836360	9841360	Um, if you increase, I think if you increase the Lambda parameter in quantity,
9841360	9844360	it'll, it'll become more and more flat.
9844360	9850360	If you decrease it, it'll become more and more closer to, to, to zero, uh, centered on zero.
9850360	9851360	Right.
9851360	9854360	So that's, that's sort of how, and then we'll, we'll have to ask,
9854360	9858360	is that what we want to express in the, in the model?
9858360	9859360	Okay.
9859360	9860360	Yeah.
9860360	9861360	Yeah.
9861360	9862360	Yeah.
9862360	9863360	Yeah.
9863360	9864360	Yeah.
9864360	9865360	Yeah.
9865360	9866360	Yeah.
9866360	9867360	Yeah.
9867360	9868360	Yeah.
9868360	9869360	Yeah.
9869360	9873360	Um, both from a mechanical standpoint that is like,
9873360	9875360	we have fewer things to worry about.
9875360	9880360	Um, and from, uh, I guess parsimony standpoint is like a simpler,
9880360	9881360	it's a simpler model.
9881360	9882360	Right.
9882360	9884360	Like we don't have that many knobs to turn.
9884360	9885360	Right.
9885360	9886360	Yeah.
9886360	9889360	Cool.
9889360	9890360	All right.
9890360	9892360	Let's see.
9892360	9895360	It's 440 right now and we end at 530.
9895360	9899360	So I'm debating what we should worry about next.
9899360	9902360	So what's, what we would have done, what we, sorry.
9902360	9907360	So what I originally planned was, uh, to go through one more example
9907360	9909360	of how we do Bayesian estimation this time,
9909360	9911360	not with binomial stories,
9911360	9914360	but with like student T distributions and normal distributions.
9914360	9917360	Um, that's one thing we could work on,
9917360	9923360	or we can first jump to, uh, arbitrary curve regression.
9923360	9927360	So I'm, I'm, I'm intentionally setting this up as like,
9927360	9930360	you can fit any curve, uh, with times three,
9930360	9933360	not just a line that like linear regression is what we're used to.
9933360	9934360	That's kind of boring.
9934360	9937360	So let's go in and like fit a different type of model.
9937360	9940360	Um, what would you prefer?
9940360	9944360	So let's do a vote and I will estimate the probabilities.
9944360	9947360	Um, how many of you want to do the curve regression?
9947360	9948360	Raise your hand.
9948360	9950360	How many want to do a second estimation?
9950360	9951360	Okay.
9951360	9953360	So we'll do, we'll do the curve regression.
9953360	9957360	If we have time, I'll come back and show you a few things,
9958360	9962360	uh, live demoed rather than, uh, interactive coding.
9962360	9963360	Okay.
9963360	9965360	On, on the second estimation thing.
9965360	9972360	So with that, I'd like you to open up, uh, notebook number five.
9972360	9979360	Notebook number five is all about arbitrary curve regression.
9979360	9982360	Let me see if I can connect in here.
9982360	9983360	Cool.
9983360	9984360	Cool.
9984360	9992360	So, um, curve regression, I'm going to put this out here.
9992360	9998360	Curve regression is nothing more than estimation with, with equations.
9998360	9999360	Okay.
9999360	10006360	So we're going to use a radioactive decay data set to sort of reinforce this point.
10006360	10007360	Okay.
10007360	10011360	Um, so we know linear regression.
10011360	10019360	You have, you have something Y is, uh, modeled as a function of a linear combination of your X's.
10019360	10020360	Right.
10020360	10025360	And so you can have the thing we're really interested in is like the W's, the weights,
10025360	10030360	or, you know, the M's if you're from physics, Y equals MX plus C, the M's if you're in physics,
10030360	10032360	or the weights if you're in stats.
10032360	10035360	Um, and the bias term as well.
10035360	10040360	Uh, and really nothing should stop us from just thinking about linear regression as the
10040360	10043360	only form of regression that we're interested in.
10043360	10044360	Right.
10044360	10046360	There's, there's, you can do all sorts of regression.
10046360	10048360	You can do Poisson regression, whatever.
10048360	10049360	You can do neural net regression.
10049360	10056360	If you know how to write neural nets, you can do, uh, in this case, exponential decay curve regression.
10056360	10057360	Right.
10057360	10064360	So we're going to see whether we can from noisy, um, radioactive decay measurements back
10065360	10070360	infer the correct parameters that help us identify a radioactive material.
10070360	10071360	Okay.
10071360	10075360	So I'd like you to run that first cell where we load data.
10075360	10082360	Oh, my.
10082360	10083360	Okay.
10083360	10086360	You'll have something that looks, data that looks something like this.
10086360	10087360	What's on this data?
10087360	10093360	Well, it's got time on one axis and then it's got activity or radioactive, you know, uh,
10093360	10096360	tiger count things on the Y axis.
10096360	10097360	Okay.
10097360	10100360	I've sort of, uh, well, this is synthetic data.
10100360	10103360	Noised out for educational purposes.
10103360	10104360	All right.
10104360	10109360	So if you plot the data, you should look, you should get something that looks like this.
10109360	10110360	Right.
10110360	10111360	Right.
10111360	10113360	Everybody got that?
10113360	10114360	Okay.
10114360	10122360	So given that we're in this like, uh, radioactive decay sort of scenario.
10122360	10130360	I'd like to ask you to think about what equations can we use to model this data?
10130360	10135360	We'll get into what the statistical model is in a moment, but I want to first think about
10135360	10139360	what equations we can use to govern this model.
10139360	10142360	There's an exponential decay equation.
10142360	10145360	What are the parameters of that equation?
10145360	10149360	You have time, which we've observed part of what we've observed.
10149360	10151360	And what else?
10151360	10153360	Half-life, the decay constant, right?
10153360	10155360	And anything else?
10155360	10157360	Ah, sure.
10157360	10160360	We're ignoring that for the time being, but yes, if we were to be fully mechanistic,
10160360	10161360	we would do that.
10161360	10163360	What else is there?
10163360	10164360	Offset.
10164360	10165360	Offset.
10165360	10170360	So, uh, is that the first offset or the baseline offset?
10170360	10171360	Baseline offset.
10171360	10176360	And then there's one more which is, which governs the original, uh, the starting point, right?
10176360	10178360	So there are what?
10178360	10181360	A, C, and tau.
10181360	10184360	We have three parameters to estimate for this curve.
10184360	10189360	And you'll notice the readings are actually kind of noisy as well, right?
10189360	10193360	And that's because there's, you know, measurements are not always perfect.
10193360	10196360	There's going to be some amount of noise that we've got to deal with.
10196360	10198360	So how do we do that?
10198360	10201360	Well, we've got to build a model.
10201360	10205360	I'm going to switch back to drawing.
10205360	10218360	We've got to build a model that lets us link the x-axis component, which is the time component,
10218360	10220360	to the y-axis thing.
10220360	10237360	We've already said that the equation is y is equal to a times e to the negative, uh, t over tau plus c, right?
10237360	10249360	The c term we can interpret, it's sort of like systematic bias in our measurement.
10250360	10253360	The a term we can interpret, right?
10253360	10264360	The a term starts is, is the starting radioactivity.
10264360	10276360	And the tau term we can also interpret, it is the characteristic half-life of this radioactive element.
10276360	10278360	Pardon me?
10278360	10281360	Ah, yes. So we're going to, we're going to talk about noises.
10281360	10284360	Oh, Siri, goodness.
10284360	10286360	Yes, that is very good.
10286360	10290360	And let's add in this plus epsilon.
10290360	10293360	But epsilon is not one of the mechanistic components.
10293360	10295360	It's a statistical component.
10295360	10298360	And that's why I've drawn it in red or a different color, right?
10298360	10301360	It's not part of the mechanistic part that we're really interested in.
10301360	10306360	So, let's see.
10306360	10312360	What are, we're now going to take this equation and we're going to build a statistical model around it.
10312360	10323360	What is a good prior for a, what is, sorry, what is a good distribution for a?
10323360	10325360	Something positive, yes.
10325360	10326360	All right.
10326360	10333360	And what's the simplest positive distribution that we can think about?
10333360	10335360	Pardon me?
10335360	10338360	Yeah, it's like the first, first thing we measure, right?
10338360	10341360	Yeah, so it's a bit like an impulse that way.
10341360	10350360	It's the, so we might say a follows some exponential distribution.
10350360	10358360	Let's just start with that because it's a simple one.
10358360	10363360	What about tau?
10363360	10368360	What values can tau take on?
10368360	10369360	It must be positive.
10369360	10370360	Yes, yes.
10370360	10373360	Okay.
10373360	10377360	So let's say exponential.
10377360	10383360	And what about C?
10383360	10389360	This is systematic bias, not the noise in the data.
10389360	10392360	Gaussian.
10392360	10395360	Systematic bias could be positive, could be negative.
10395360	10400360	Yes, so it could be normal.
10400360	10404360	Could be.
10404360	10405360	Ah, great.
10405360	10407360	Thank you.
10407360	10414360	So instead of normal, what will we do then?
10414360	10418360	We might do exponential, sure.
10418360	10431360	What about the likelihood though?
10431360	10433360	Pardon me?
10433360	10435360	Why would it be Poisson?
10435360	10437360	We're measuring counts.
10437360	10438360	Yes.
10438360	10447360	However, at least in the data, we've got it as continuous right now because of the noise in the machine that reports back a continuous value.
10447360	10459360	So what might we do?
10459360	10462360	Let's cheat a little bit.
10462360	10464360	Approximation on approximation.
10464360	10487360	We'll use a normal distribution here because the range of values for which we've got data are tight enough and far enough from zero that essentially at the tails of our normal, we don't have any much really credible credibility points assigned there.
10487360	10493360	These are like the struggles that we wrestle with, with every new modeling problem that comes in.
10493360	10497360	Is a normal distribution likelihood reasonable?
10497360	10498360	Is it correct?
10498360	10499360	Probably not.
10499360	10501360	Is it useful?
10501360	10502360	Maybe.
10502360	10503360	Right?
10503360	10511360	So I want to get that in your head.
10512360	10522360	Likelihood is the thing that you're observing about the data, right?
10522360	10534360	So what do you mean by unit then?
10534360	10536360	Right, right, right, right.
10536360	10543360	So if you look at, I'm going to detour a little bit and talk about linear regression.
10543360	10552360	So you have y equals mx plus c.
10552360	10560360	We might write a model that says m is normally distributed for whatever distribution parameters.
10560360	10564360	C is also normally distributed.
10564360	10568360	Y is the likelihood of the data.
10568360	10569360	It's got noise.
10569360	10586360	And if we assume that the noise is Gaussian noise, then we can impose a modeling assumption that says that this is normally distributed where the mu is equal to mx plus c.
10586360	10591360	And the sigma is equal to something else.
10591360	10593360	The sigma is our epsilon.
10593360	10595360	And we can ask, what is the epsilon?
10595360	10598360	How is the, how is that going to be distributed?
10598360	10600360	Does that make sense?
10600360	10601360	Yeah.
10601360	10612360	So I'm glad you asked that question because if you look at the parallels here, we'll need a sigma prior.
10612360	10620360	And just for convenience, I'm just going to put the standard half Cauchy, okay?
10620360	10621360	Inside there.
10621360	10623360	So we got that.
10623360	10633360	So then we might impose the same or a similar set of modeling assumptions on the likelihood, which is our y, right?
10633360	10637360	Or rather than calling it likelihood, because that's overloading terms.
10637360	10638360	Let's just do y.
10638360	10640360	How is y distributed?
10640360	10656360	Why we might impose that this is normally distributed where the mu is equal to a times e to the negative t over tau plus c.
10656360	10660360	And then we have some noise, which is our epsilon.
10660360	10669360	And our epsilon, just for convenience, will also make it half Cauchy.
10669360	10680360	Let's let that sink in for a moment.
10680360	10684360	Or maybe I should say something like, I'm just going to leave this up on there.
10684360	10685360	No.
10685360	10687360	Do we have questions?
10687360	10709360	Things that are not clear.
10709360	10710360	Yeah.
10710360	10727360	If the errors in this particular case, what we've assumed is that our errors are not dependent on the value on the x-axis.
10727360	10741360	If now we suddenly found that the values, the error varies with the value on the x-axis, suddenly we have to write a function that models sigma as a function of, in this case, t.
10741360	10742360	Right?
10742360	10745360	So that's one place where this model would fail.
10745360	10748360	And I've actually encountered that at work before.
10749360	10750360	How do we get around that?
10750360	10760360	We get around that by either explicitly stating up front that this assumption does not hold in our data, but we're willing to work with the consequences of that.
10760360	10763360	Or we go hunting for the function.
10763360	10770360	And sometimes that's kind of hard when you have like limited x values to work with.
10770360	10771360	Oops.
10771360	10774360	Siri keeps coming up.
10774360	10775360	Cool.
10775360	10777360	Any other questions on this?
10777360	10779360	This is like the key, key point.
10779360	10781360	This is the key point here.
10781360	10799360	Like you can write the parameters of your likelihood distributions as a function or a transformation on the other things that you've, you're interested in.
10799360	10802360	Okay.
10802360	10803360	Okay.
10803360	10810360	So if you go ahead and let's, let's go ahead and code the model together.
10810360	10815360	What's inside the instructor notebook might be different from what we just wrote out.
10815360	10820360	What I wanted to give you all just now was this live experience of like, well, I don't know.
10820360	10824360	So what are we, what modeling assumptions am I willing to stand with?
10824360	10825360	Right.
10825360	10828360	And then we can go back in and re critique the model one more time.
10828360	10835360	So let's, let's put in, in this case, just copy and paste what's inside the instructor notebook.
10835360	10836360	All right.
10836360	10841360	And let's not worry too much about the others.
10841360	10846360	Again, you'll notice that thing, that equation.
10846360	10850360	I, I alluded it, alluded to this point, its name a few times.
10850360	10852360	It's called a link function.
10852360	10853360	Right.
10853360	10855360	So y equals mx plus C is a link function.
10855360	10860360	Y is equal to times a times e to the negative t over tau plus C.
10860360	10862360	That's just another link function.
10862360	10866360	You can have your four parameter dose response curves as a link function.
10866360	10876360	You can put the standard logistic regression curve as a link function, like any math function that you can think of can be a link function.
10876360	10878360	All right.
10878360	10888360	And then that goes and that, what that does is it can, it controls the, the mean curve parameter.
10888360	10889360	Right.
10889360	10895360	It controls the mean of our data as a function of, you know, this thing on the x axis.
10895360	10898360	All right.
10898360	10900360	So let's copy and paste what's inside here.
10900360	10904360	Here the modeling choices are half normal, exponential.
10904360	10913360	I think I chose C to be normal under the assumption that sometimes the, the machine might go faulty and give us like a completely negative baseline.
10913360	10914360	Sure.
10914360	10919360	And if that never happens, then I would change, change that to a half kosher exponential.
10919360	10922360	Okay.
10922360	10933360	So then we sample.
10933360	10942360	Oh, I hear the jet engines running again.
10942360	10955360	Ah, so this is a, this is the thing that I'm wondering ravine, will you be covering Colin will be covering it tomorrow in the RVs or the Asian model evaluation tutorial.
10955360	10959360	So ignore that for the time being.
10959360	10962360	And you should get something that looks like this guy.
10962360	10965360	The, do we all have that thumbs up if you do.
10965360	10966360	Yep.
10966360	10967360	Okay.
10967360	10968360	So you get like traces.
10968360	10978360	This one's been simple, right, because we've got only a single alpha, a single capital A, a single capital C, a single tau, right.
10978360	10994360	But you all saw just now how we can actually have a vector of A's, a vector of towels, a vector of C's, our likelihood normal distribution can also be expanded to be a vector of, of, of likelihoods.
10994360	11001360	There's just some little intricacies that we have to worry about with respect to the, the, you know, y equals.
11001360	11003360	Y is the mu, the link function, right.
11003360	11005360	So you have to play around with that.
11005360	11006360	But this is totally doable.
11006360	11014360	And then once you have that multiple groups thing, once again, you can do your hierarchical player, hierarchical thing.
11014360	11017360	If it's an appropriate modeling decision, right.
11017360	11021360	So if you think about it, think about it.
11021360	11029360	That this arbitrary curve regression thing is once again, nothing more than estimation at its heart.
11029360	11045360	And instead of estimating like A, instead of estimating distribution parameters directly, like in this case, in previous cases, we were estimating the P hierarchically, right.
11045360	11049360	Now all we've done is we've said there's an equation that governs that key parameter.
11049360	11058360	And now we want to estimate the parameters of that equation in a, of that equation in a statistical fashion, rather than just treat it as some fixed point.
11058360	11060360	Okay.
11060360	11062360	How are we with that?
11062360	11065360	Okay.
11065360	11069360	If you want, go back and like figure out what the element is.
11069360	11073360	I'm not going to reveal the answer right now.
11073360	11078360	But I want to point you to the table at the bottom of your notebooks.
11078360	11095360	The table at the bottom of your notebooks says in compact form, everything that I just said, that is, you can put any arbitrary curve as a link function, and you don't have to be restrained to modeling just linear models.
11095360	11102360	You can model these decay curves, you can model logistic regressions, you can do.
11102360	11104360	You can write a neural net.
11104360	11114360	Like if you've got some weird function that is, you know, non non monotonically linear, then go ahead, write a neural net and estimate the parameters.
11114360	11116360	You might not want to do like MCMC sampling.
11116360	11118360	That's a little too much.
11118360	11126360	You might want to bust out the variational inference tools that we have, but in time see three, but, you know, it's all possible.
11126360	11128360	It's all totally possible.
11128360	11134360	So, all right, that's it for the arbitrary curve regression notebook.
11134360	11141360	Do we have any questions before we go back into doing estimation one more time?
11141360	11154360	What you have to do, let me see if I can pull this off here.
11154360	11160360	You want to see not just the single regression line, but the full family of them, right?
11160360	11165360	Yeah, all right, this is going to test my live coding abilities.
11165360	11176360	Trace dot bar names.
11176360	11178360	Cool.
11178360	11185360	One way to do this is to plot what T would look like first.
11185360	11194360	So T is NP dot LIN space from zero to 800.
11194360	11200360	Okay.
11200360	11206360	And then you'll want to write the equation out.
11206360	11214360	The equation is this guy.
11214360	11222360	I'm going to put this down here.
11222360	11235360	So we return that.
11235360	11255360	And then the trace will have, if we inspect trace of A, it's a vector, it's 2000 long, 8000 long.
11255360	11284360	I hope I've done this before, but I just have to do this correctly.
11284360	11293360	In any time, I'm going to have you, we'll talk afterwards, and I'll put that, I'll be sure to put this on the notebook so that everybody benefits from this question.
11293360	11303360	I think it's a great question because I've done this before, I just like am blanking on live coding, but I'll get that up there for you guys.
11304360	11307360	But there's some form of broadcasting that we need to do, right?
11307360	11316360	There's like X over tau needs to be broadcasted into a matrix and then we plot each of those rows of the matrix, but I'm not sure how to do this right now.
11316360	11319360	So we'll work that out later.
11319360	11323360	Let's come back to estimation before we wrap up.
11323360	11326360	So with estimation, we're going back to notebook number four.
11326360	11344360	I'd like to invite you to open up notebook number four, and all I'm going to do is rather than code with you, I'm going to show you another, show you this example is basically another case study where we've got information from two groups,
11344360	11348360	but now we have this third group for which we don't have enough information.
11348360	11352360	We want to be able to make reasonable inferences on it.
11352360	11358360	So I'm going to use the instructor version.
11358360	11373360	Whoops, rather than the student version, and I'm just going to run run down to about here first.
11373360	11377360	Okay, so we've got data.
11377360	11385360	We always love to have data.
11385360	11392360	And for educational reasons, what I did, I took the liberty of adding in an extra species that was unknown.
11392360	11399360	We know it's a Finch, but we've never, we've never really measured it, but and it's so rare, we've only got one measurement.
11399360	11408360	So we're going to make inferences on, well, what's the, what do we expect to know about this new Finch's beak depth, right?
11408360	11411360	We've been measuring the beaks depth and their length.
11411360	11415360	What do we expect to know?
11415360	11420360	So under this case is like, damn, we have like one measurement.
11420360	11427360	There's no way we can even compute a standard deviation on this one independent measurement, right?
11427360	11430360	We estimate uncertainty in this case.
11430360	11446360	And this is the sort of scenario where a hierarchical model can be helpful in exactly the same way that it was helpful for those baseball players who had only one at that and no other data than that one at that.
11446360	11456360	Okay, so if we think about the data generative process, we'll say something like, oh yeah, our beaks, maybe they are student T distributed.
11456360	11457360	Why student T?
11457360	11462360	It's because student T is the generalization of the normal and the Cauchy.
11462360	11466360	The Cauchy distribution, the student T distribution has this degree of freedom parameter.
11466360	11471360	This degree of freedom parameter controls how high or how fat the tails are.
11471360	11477360	Normal distribution has really, really low, low tails, low probability density on the tails.
11477360	11482360	The Cauchy distribution has really high probability density on the tails, relatively speaking.
11482360	11491360	So the student T distribution says that when degree of freedom is one, it's the Cauchy, and when it's infinite, it's the normal.
11491360	11498360	And everything else in between is controlled by this degree of freedom parameter.
11498360	11503360	So we might define a student T likelihood.
11503360	11518360	If we do an independent model, we'll get these posterior distributions on the beak depth, right?
11518360	11520360	And it's on the mean beak depth.
11520360	11527360	And this is kind of like where this independent model is really not the right place to be.
11527360	11528360	So think about it.
11528360	11547360	We've got values that can range from 0 to 15, where we know that finches generally are constrained maybe more towards 4 to 9 or 4 to 11 or something like that, right?
11547360	11548360	I forgot.
11548360	11550360	This is centimeters and millimeters.
11550360	11552360	But you get the point, right?
11552360	11560360	This independent model doesn't really have that borrowing of information from the known species to help us constrain our estimates.
11560360	11564360	So I'm going to throw this on the right-hand side here.
11564360	11566360	Keep this one in mind.
11566360	11568360	This is the independent model.
11568360	11575360	Now, if we fit a hierarchical model, and it looks something like this guy, right?
11575360	11576360	Similar syntax.
11576360	11577360	Nothing fancy.
11577360	11596360	We have our priors and everything, and we have the broadcasting that's happening going on, just like in the independent model, except now we have prior distributions on the parameters of our distributions for the like, on the distributions for the parameters of our likelihood function.
11596360	11597360	Okay?
11597360	11599360	That's a bit of a mouthful, but I hope you get the point.
11599360	11602360	There are hyper priors that exist.
11602360	11612360	If we do sampling, now, ooh, sorry, I'm going to instead throw this up on the right.
11612360	11615360	There we go.
11615360	11618360	This is the one we want.
11618360	11619360	Oh, okay.
11619360	11623360	Maybe it's better on the bottom.
11623360	11631360	And if you look at this guy over here, throw this one up here.
11631360	11643360	Okay, so down on the bottom is our posterior distribution estimates for the independent model.
11643360	11644360	Okay.
11644360	11652360	And up at the top is the posterior distribution estimate for the hierarchical model.
11652360	11660360	Which one looks more reasonable for this unknown species that we're interested in?
11660360	11673360	This might take a bit of prior knowledge, but then you think about the 95% posterior density values.
11673360	11675360	These are pretty extreme.
11675360	11678360	These are quite extreme for the problem at hand, right?
11678360	11685360	Finch peaks that are like zero centimeters are really close to zero, not so believable.
11685360	11695360	In this case, because we have only a single measurement, the math works out such that our smallest beak size will, you know, in the 94% density will be at 0.9.
11695360	11700360	Still might be unreasonable, but if you look at where most of the credibility is associated, it's out here.
11700360	11709360	Whereas on this side, well, yeah, most of the credibility is associated out here, but there's still lots of credibility assigned like at really low values nonetheless.
11709360	11716360	So qualitatively speaking, it still doesn't really make sense, right?
11716360	11720360	Given the background prior knowledge that we've had.
11720360	11721360	Okay.
11721360	11726360	Okay, so that's all that I really wanted to say about this particular model.
11726360	11730360	It was intended, so you can do this at home.
11730360	11740360	It's intended, this exercise is intended as, you know, can I build a model for the data that is now not following the binomial story?
11740360	11749360	Because we really, really harped on the binomial story to illustrate these other things, hierarchical models, vectorization of probability distributions, and the likes.
11749360	11759360	Okay, so we really harped on that, but here, here, this case, you can get some practice with, you know, something that's t-distributed or normally distributed and try out other problems for yourself as well.
11759360	11761360	Try out the other probability distributions.
11761360	11764360	Okay, so that's also really helpful.
11764360	11771360	All right, so the final thing that we have to do is I'm going to have Raveen and find a few helpers.
11771360	11779360	I've got these little cards that congratulate you for taking this tutorial and sticking all the way through to the end.
11779360	11781360	So this is my little way of saying thank you.
11781360	11783360	At the same time, I have a little ask as well.
11784360	11799360	There is a survey that we have on the readme of the GitHub repository, or if you prefer to use your phone to do it, there's a QR code on the back of the congratulations card.
11799360	11806360	I'd like you to fill out that form to tell us where, where we did well on this tutorial, where we could improve it.
11806360	11813360	Every generation of tutorials gets better and better, and it's all thanks to your feedback that we're able to do it.
11813360	11820360	So while that's happening, I'm also happy to take questions, and then I have one final, final, final thing for everybody.
11820360	11832360	So while that's going around, while you all are doing the surveys, I hope you all can open up the readme if you want to do it on your computer or scan the QR code if you're on your phone.
11833360	11837360	Do you have questions on the material today?
11848360	11850360	Okay, if not, then we'll continue.
11850360	11856360	I'll just wait until I've got some form of quorum on like everybody being done.
11859360	11860360	Yes.
11862360	11863360	Sure.
11876360	11877360	Right.
11877360	11878360	Okay.
11881360	11882360	Oh, cool.
11882360	11883360	All right.
11883360	11889360	That's, that's good for me to know because really tight.
11890360	11891360	Okay.
11896360	11897360	Yep.
11897360	11898360	Yep.
11898360	11899360	Oh, cool.
11899360	11900360	Thanks a lot.
11900360	11901360	I learned something today.
11904360	11905360	Anything else?
11919360	11920360	Yeah.
11938360	11939360	Yeah.
11941360	11943360	So let's see.
11944360	11957360	The simplest, the simplest way to do this is actually to, to use the sci-pi stats module and, and use that to calculate the likelihood of data under your distribute, calculate, sorry, backtrack a little bit.
11958360	11964360	We've always in, in our examples had data being basically like a data frame or like multiple rows of stuff.
11965360	11972360	When we calculate the likelihood, it's really the sum of likelihoods over every single data point.
11972360	11983360	And what happens when we're sampling, I think Revena and Colin will know this better than I would, but it, the mental model that I have is we're sort of, we draw a number from a, from our prior distributions.
11984360	11996360	We, we assume that to be true and now fit it, put that into the likelihood, then compute the sum of likelihoods over all of our data.
11996360	12005360	And then there's like this, this step that says, well, okay, given, given this thing that we've pulled out, the likelihood is this particular value.
12005360	12016360	Now there's, you know, there's, there's gradient information that tells us which way to go and in, in the right place to sample that will now help us increase likelihood.
12016360	12019360	Now, I want to be clear, this is not gradient ascent.
12019360	12021360	Okay, this is not gradient ascent.
12021360	12028360	And I've, in talking with Colin multiple times, I've actually made that mistake of thinking of it as gradient descent.
12028360	12032360	So I don't want you to think of it as gradient, this gradient ascent at all.
12032360	12034360	It's much more complicated than that.
12034360	12035360	Right.
12035360	12039360	But that's basically a glimpse into what's happening underneath the hood.
12043360	12046360	That is the MCMC step that we're doing.
12046360	12051360	We're like randomly sampling values from our, from our calculated posterior distribution.
12052360	12062360	Sorry, I didn't, I didn't catch that.
12062360	12064360	We can do it with simpler math.
12071360	12078360	So the, the, who depends on how we define simple and complicated integrals are kind of complicated.
12078360	12084360	And if we were not to use MCE methods, we would be doing integration and that'd be a bit of a pain.
12084360	12086360	I think ravine had something to say.
12086360	12087360	Yeah.
12089360	12098360	So for that question specifically like the tutorial that I'm giving tomorrow, which is on GitHub has an entire notebook for that question of how MCMC works and diagnostics for it.
12098360	12100360	So if you're in it, that's good.
12100360	12101360	Otherwise you can come talk to me.
12101360	12107360	I'll give you the link to the GitHub and we can talk through MCMC and is well, hopefully a medium amount of detail.
12107360	12108360	Does that help?
12108360	12110360	Yeah, yeah.
12110360	12111360	Okay, cool.
12112360	12113360	So.
12136360	12137360	Yep.
12141360	12163360	So I've done it before where we cheat and look at the data first and then try to see what distribution might be suitable.
12163360	12165360	Well, this is mostly for the likelihood.
12166360	12170360	Cheating basions are called empirical basions.
12170360	12175360	So that's one way of approaching the problem.
12175360	12186360	In practice, what happens is this will build a model and then it's all got it's got all the simplest things.
12186360	12188360	It's it's normal.
12188360	12194360	It's exponentials and like we're not thinking too hard about the mechanics of the problem.
12194360	12197360	We're not thinking too hard about the details of the problem.
12197360	12198360	We're not.
12198360	12205360	We're sort of ignoring ahead of time what potential problems might show up in MC sampling and just running with it first.
12205360	12211360	And then we'll encounter a problem with MC sampling, which often is an index.
12212360	12219360	And then we'll encounter a problem with MC sampling, which often is an indication that like the model is kind of bad as well.
12219360	12222360	Then we'll go back and think a little bit more carefully about it.
12222360	12237360	So I've done these sessions at work where I start working on a model at just after lunch and I don't go home until 7pm because at 7 that's when like something that might look correct starts to show up.
12237360	12242360	And even then I'm still not 100% sure that that model is the best model.
12242360	12249360	However, I have a pragmatist in my head restraining me from going till 9pm.
12249360	12257360	And it says, well, okay, you've got the key parameters of interest and you know their uncertainty to some degree.
12257360	12266360	Go home, rest over it and maybe present it and someone else might be able to the peer review process at work then shows up.
12266360	12275360	And I think if no one else can critique the model any further, we sort of all have to just agree that let's just run with it.
12275360	12277360	Yeah.
12287360	12290360	Yes, yes, yes.
12290360	12294360	Yes, exactly.
12294360	12299360	And if we're not sure about that, we change the distribution.
12299360	12306360	There are positive only distributions that can be centered, you know, way out further out, right?
12306360	12314360	I'm blanking right now on exactly which ones, but if you look at the PIMC3 distribution gallery, then you'll see those pictures and it becomes clear.
12314360	12329360	Yeah, so there are these so-called improper priors.
12329360	12340360	The flat distribution assigns, I forgot what likelihood it assigns, but it just assigns a single constant number from negative infinity to positive infinity.
12340360	12351360	And then PIMC, there is machinery that lets you bound a distribution by setting its lower bound, upper bound, or both.
12351360	12353360	And that's available.
12353360	12358360	And yes, you can do that, though I think the pros say don't do it.
12358360	12364360	Avoid the improper priors where you can, like it's not the best thing.
12364360	12371360	The reasons why I'll have to dig, but the rule of thumb I've remembered is don't, like just avoid it.
12371360	12372360	Yeah.
12372360	12379360	Weekly informative priors that say things like, yeah, it's probably more close to zero, but I'm really not sure.
12379360	12384360	Or it's probably centered around here, but I'm willing to give lots of uncertainty at first.
12384360	12391360	Those are the general rules of thumb for selecting priors.
12391360	12395360	Cool. Anything else?
12395360	12399360	Okay, if not, let's do a very quick recap of what we went through today.
12399360	12405360	There's a lot of material, but the core thing that I hope you take away are the following.
12405360	12415360	Firstly, that probability itself, and you've seen it so many times here, it's nothing more than assigning credibility points to the number line.
12415360	12424360	Where there's higher credibility points, we believe it more, and where there's lower credibility points, we believe that that parameter takes on that value less times.
12424360	12426360	That's all it is.
12426360	12434360	We saw how we can go from joint and conditional probability to Bayes rule and how that maps on.
12434360	12442360	Marginal probability, joint and marginal and conditional probability are all really important for this.
12442360	12449360	One thing I really hope you all take back is know your probability distribution stories.
12449360	12457360	Super-duper important. If you know what their stories are, then picking them for your modeling work becomes much easier.
12457360	12460360	Picking them becomes much easier.
12460360	12465360	And so knowing what the continuuses are and what the discreet are, that's really important.
12465360	12472360	Knowing their shape, their support, what story they tell that will help you in your modeling work.
12472360	12482360	And finally, we went through one really simple example but built it up in depth, the binomial distribution story,
12482360	12492360	and went along and showed how you can take a seemingly simple model and complicate it enough to fit the problem that you have at hand.
12493360	12496360	Do you have more than one group? Well, vectorize the thing.
12496360	12501360	Do you have some groups with lots of info and some groups that don't have lots of info?
12501360	12508360	Well, use a hierarchical model and the mechanics of how we build these models, we reinforced over and over and over.
12508360	12517360	And from the discussion, I noticed a lot of light bulbs going off, so that always makes me very happy to see.
12517360	12522360	Cool. And with that, I'm going to say we're going to end here. Thank you all for coming.
12522360	12528360	If you want office hours, I'll put them on the Slack. They will always be in the Tejas room in the afternoons.
12528360	12531360	Exact time, see the Slack channel. Thanks a lot.
