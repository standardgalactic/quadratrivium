WEBVTT

00:00.000 --> 00:07.000
Hi, my name is Brandon Shanahan, and today I'm going to talk about Bayesian neural networks.

00:07.000 --> 00:16.000
In the first part of the talk, I'll give a brief, high-level introduction to neural networks that broadly covers what they are, what they do, and how they work.

00:16.000 --> 00:26.000
I'll spend a bit of time talking about the stochastic optimization procedure employed by neural networks, and then I'll discuss two common neural network architectures that are used in experiments in a later section.

00:26.000 --> 00:34.000
Next, I'll give an equivalent description that frames neural networks as conditional probabilistic models and outline the parallels between the two definitions.

00:34.000 --> 00:41.000
I'll then discuss the shortcomings of these models, which will motivate the introduction of Bayesian methods in neural network.

00:41.000 --> 00:45.000
Finally, I'll discuss the results of two experiments employing Bayesian neural network.

00:45.000 --> 01:01.000
One regression experiment compares the results obtained by a plain neural network and a Bayesian neural network, and one image classification experiment that further illustrates the benefits of Bayesian learning.

01:01.000 --> 01:10.000
Neural networks have been rigorously shown to be universal approximators capable of representing arbitrarily complex nonlinear functions to arbitrary precision.

01:10.000 --> 01:24.000
As such, neural networks are used in many tasks which require learning incredibly complex mappings between input and output spaces, such as in speech recognition and text generation, sentiment analysis, and image segmentation and classification.

01:24.000 --> 01:31.000
To take a concrete example, say we're building a network for use in an autonomous vehicle which specifically performs object detection.

01:31.000 --> 01:47.000
You can imagine a function that maps, say, an image of a typical city street from the point of view of the driver to a set of categorical boundaries, each of which represents a pedestrian, another car, a traffic light or street sign, a crosswalk, and so on, is highly nonlinear.

01:47.000 --> 01:54.000
The function also needs to be extremely adaptable as it's being computed in near real time and the data it acts on isn't static.

01:54.000 --> 02:02.000
The car is moving, pedestrians are walking, other cars are moving, lights are changing, and no two images that it captures will be identical.

02:02.000 --> 02:10.000
A scenario like this makes it clear that it's not sufficient for our network to simply learn a deterministic mapping based on a set of training data that it's seen before.

02:10.000 --> 02:21.000
In other words, the network needs to be able to generalize well and extrapolate to new data based on the context of data that it's already seen, but it also needs to have some notion of the uncertainty inherent in its decisions

02:21.000 --> 02:29.000
to be able to consider this uncertainty when taking in action in order to prevent making overconfident incorrect decisions.

02:29.000 --> 02:40.000
The basic computational unit of a neural network is referred to as an artificial neuron, or more simply, a neuron, in which it develops historically as a crude mathematical model of a biological neuron.

02:40.000 --> 02:52.000
A neuron takes some input data to which it applies an affine transformation with a weight matrix W and bias vector B, both of which are learned parameters, and finally, a nonlinear activation function as its output.

02:52.000 --> 02:57.000
The simplest neural network architecture is a fully connected feedforward network.

02:57.000 --> 03:07.000
Neurons are arranged in layers, which are stacked one on top of the other, starting from the input layer, followed by an arbitrary number of intermediate hidden layers, and finally, an output layer.

03:07.000 --> 03:20.000
This architecture gets its name from the fact that a neuron in one layer is fully connected to every neuron in the two layers at sandwich between, and a neuron's output is fed forward as input to every neuron in the next layer.

03:20.000 --> 03:27.000
The key to a neural network's ability to learn nonlinear representations of their input are the use of nonlinear activation functions.

03:27.000 --> 03:35.000
Simply applying successive affine transformations at each layer of a network is equivalent to applying a single affine transformation.

03:35.000 --> 03:45.000
Therefore, a neural network that only uses an identity activation function at each neuron can only learn a linear mapping between its input and output spaces.

03:45.000 --> 03:51.000
The form of a network's output layer depends on the type of data it processes and the network's intended task.

03:51.000 --> 04:04.000
For classification of a labeled categorical data, the output layer consists of C neurons, which includes a probability distribution over the C possible classes of the data, and assigns a prediction which maximizes this distribution.

04:04.000 --> 04:12.000
It then computes a cross-entropy loss function between the true and predicted distributions of the data, which is usually averaged over the entire frame set.

04:12.000 --> 04:24.000
For regression, the output layer consists of a single neuron, which computes an identity activation function and, typically, either the mean absolute error or mean squared error loss between the true and predicted output.

04:24.000 --> 04:34.000
It's important to note here that while the output layer computes an identity activation function, hidden layers still compute nonlinear activation functions.

04:34.000 --> 04:44.000
Neural networks are represented internally by what's referred to as a directed acyclic graph, which allows for decomposing complex functions into a sequence of simple arithmetic operations.

04:44.000 --> 04:57.000
This is especially important because neural networks learn their parameters using optimization procedures such as stochastic gradient descent for one of its many variants, which require computing the gradient of the loss function with respect to the network's input.

04:57.000 --> 05:13.000
For functions whose gradients do not have a simple closed form expression, this internal representation allows the computation of a single intractable gradient to be achieved instead by taking simple local gradients with respect to intermediate variables which are defined and cached locally during training.

05:13.000 --> 05:23.000
Gradients are first computed with respect to neurons in the output layer and then propagated backwards through the network's hidden layers to the input layer by a straightforward application of the chain rule.

05:23.000 --> 05:34.000
At each neuron, its parameters are updated by taking a step in the direction of steepest gradient descent, with the step size controlled by a hyperparameter lambda referred to as the learning rate.

05:34.000 --> 05:44.000
Training proceeds by performing a forward pass with the entire training data set and computing the average loss, which is then back propagated through all the network parameters to the input layer.

05:44.000 --> 05:48.000
This forward-backward sequence is referred to as one training epoch.

05:49.000 --> 05:58.000
Parameters are modified during each epoch, and the network is said to have converged when the loss function plateaus over many subsequent forward passes.

05:58.000 --> 06:11.000
Another important neural network architecture is the convolutional neural network, which is designed specifically to operate on image data and to take advantage of the additional structure of an information contained in an image's color channels.

06:11.000 --> 06:24.000
Convolutional neural networks tend to learn incredibly rich, local, and translationally invariant features that make them extremely powerful, and they have been the primary building block in most state-of-the-art neural network designs in the last decade.

06:24.000 --> 06:31.000
Convolutional neural networks are distinct for the use of convolutional layers, which differ from fully connected layers in several important ways.

06:31.000 --> 06:42.000
First, while each neuron in a fully connected layer will see the entire output of the previous layer, each convolutional feature map has a limited receptive field and only sees a fixed region of the input.

06:42.000 --> 06:58.000
Additionally, a fully connected neuron computes an affine transformation by acting on a flattened, vectorized representation of the input image, whereas a feature map is formed by convolving the color channels in its receptive field with a set of filters, which are the network's learned parameters.

06:58.000 --> 07:21.000
In other words, whereas a neuron in a fully connected layer acts on a one-dimensional input vector to produce a one-dimensional feature vector, a convolutional layer acts on a three-dimensional image volume with dimensions of width, height, and color channels to produce a three-dimensional feature map with dimensions of receptive field width, receptive field height, and filter size.

07:22.000 --> 07:29.000
Neural networks can be equivalently understood without making any references to their biological motivations or internal architecture.

07:29.000 --> 07:39.000
Here, we define a neural network as a conditional probabilistic model, which is conditioned on a set of training data D and its learned weight and bias parameters, which we simply refer to as theta.

07:39.000 --> 07:48.000
Given some new input x hat, the network computes a probability distribution over its entire output space and selects an output y hat, which maximizes the distribution.

07:48.000 --> 08:00.000
To draw an equivalence between these two descriptions, in a classification setting, the output space y is the set of all possible classes, and the model p is a categorical distribution, and the corresponding loss function is the cross-entropy loss.

08:00.000 --> 08:08.000
For regression, the output space y is the real numbers, and p is a Gaussian distribution, and the corresponding loss function is the mean squared error loss.

08:08.000 --> 08:16.000
Nothing about the actual implementation changes and parameter optimization proceeds by back propagation, as described in the previous section.

08:16.000 --> 08:29.000
Parameters learned during back propagation are maximum likelihood estimates, or rather stochastic approximations.

08:29.000 --> 08:32.000
This likelihood model presents several shortcomings.

08:32.000 --> 08:44.000
Networks whose parameters are maximum likelihood estimates result in predictions that are deterministic, in the sense that, once trained, predictions of a given input corresponds to a point estimate, and will always result in the same output.

08:44.000 --> 08:50.000
And these networks are therefore prone to overfitting and unable to express uncertainty about their prediction.

08:50.000 --> 08:54.000
This can be especially problematic when generalizing to data that they have not been trained on.

08:54.000 --> 09:03.000
Techniques such as L1 and L2 regularization, commonly referred to as ridge and lasso respectively, are commonly used in practice to counteract overfitting.

09:03.000 --> 09:09.000
But these networks are nonetheless deterministic and unable to express uncertainty in their predictions.

09:09.000 --> 09:18.000
Bayesian inference for neural networks proceeds by defining a prior distribution over the parameters of the network, and applying Bayes' rule to compute their posterior distribution.

09:18.000 --> 09:31.000
Predictive queries about some new data X-hat correspond to computing the predicted distribution of an unknown label Y-hat given X-hat by taking the expectation of the conditional distribution of the new data with respect to the network's posterior.

09:31.000 --> 09:38.000
Every posterior weighted parameter configuration contributes to the prediction of the label Y-hat given the data X-hat.

09:38.000 --> 09:46.000
Therefore, taking the expectation under the posterior is equivalent to computing a weighted average of predictions from an ensemble of plain neural networks,

09:46.000 --> 09:50.000
each of whose parameters are drawn from the same shared distribution.

09:50.000 --> 10:00.000
Because their predictions correspond to posterior samples, the ability to express uncertainty in the parameters in subsequent observations is built into Bayesian neural networks.

10:00.000 --> 10:12.000
This ensemble averaging in subsequent parameter uncertainty has regularizing effects on Bayesian neural network predictions, which are equivalent to the regularization methods previously discussed.

10:12.000 --> 10:23.000
As with most interesting Bayesian models, computing the posterior distribution is analytically intractable and must be approximated, which is generally done using sampling methods such as Markov, J, and Monte Carlo.

10:23.000 --> 10:38.000
Bayesian neural networks instead frame posterior inference as an optimization problem over the parameters by of a target variational distribution by minimizing the cubic Liebler divergence or KL divergence between the variational distribution and the true posterior.

10:38.000 --> 10:47.000
At inference time, the network parameters theta are instead sampled from the variational distribution.

10:47.000 --> 10:59.000
The corresponding loss function is commonly referred to as the variational free energy or the variational lower bound, and represents its tradeoff between maximizing the expected log likelihood of the data with respect to the variational distribution,

10:59.000 --> 11:07.000
referred to as the likelihood cost, and minimizing the KL divergence between the variational distribution and the network's prior, referred to as the complexity cost.

11:07.000 --> 11:17.000
Equivalently, variational free energy represents a tradeoff between satisfying the complexity of the data and the simplicity of the network's prior distribution.

11:17.000 --> 11:34.000
Because we're minimizing an expectation with respect to the known variational distribution, we can approximate the true loss by instead sampling from the variational distribution and computing the approximate loss in a process similar to the aforementioned Markov, J, and Monte Carlo approach.

11:34.000 --> 11:44.000
By assuming the samples from the posterior are normally distributed and uncorrelated, we can apply a local reparameterization technique which allows us to sample parameter free white noise epsilon,

11:44.000 --> 11:56.000
then shift and scale epsilon by a deterministic function of mu and rho, mean and standard deviation of the variational distribution respectively, to obtain a sample theta from the variational posterior.

11:56.000 --> 12:02.000
In this context, the back propagation is slightly modified and referred to as Bayes by back prop.

12:02.000 --> 12:11.000
Now, gradients are taken with respect to the variational parameters mu and rho, which are then updated by taking a step in the direction of steepest gradient descent.

12:11.000 --> 12:20.000
Network training otherwise proceeds as normal, performing alternating forward passes and backward passes until the network parameters are converged.

12:20.000 --> 12:24.000
I'll now describe two experiments employing Bayesian neural networks.

12:24.000 --> 12:33.000
The first is a regression experiment on a toy data set which compares the performance of the plane to the forward fully connected network to that of an identical Bayesian neural network.

12:33.000 --> 12:39.000
Training samples are generated using 2048 equally spaced points between zero and one half.

12:39.000 --> 12:50.000
Both neural network architectures consist of two fully connected hidden layers with 250 neurons each in RELU activations and an output layer with a single neuron and identity activation.

12:50.000 --> 12:55.000
Training occurred over 500 epochs with many batches of 32 samples.

12:55.000 --> 13:10.000
Once fully trained, each network was used over 500 trials to predict the test set 4096 samples run uniformly randomly from the closed interval from minus one half to one, and the results were averaged and are given in the bottom to bottom way.

13:10.000 --> 13:15.000
For the plane neural networks results are on top and the Bayesian networks results are on the bottom.

13:15.000 --> 13:24.000
The red line is the mean prediction over 500 trials and the dark blue and light blue regions are the one sigma and two sigma confidence intervals respectively.

13:24.000 --> 13:26.000
A few things are interesting.

13:26.000 --> 13:38.000
First off, the plane neural network underestimates the true variance of the test data in the region of the space that it was trained on, which is maybe not so surprising given that we know how prominent these types of networks are overfitting.

13:38.000 --> 13:54.000
However, in regions of the test base where the networks have not been trained, the plane neural network chooses a particular extrapolation of the training data and subsequent test predictions have extremely low variance, which goes towards zero as points in the test base gets further from the training set.

13:54.000 --> 14:07.000
The Bayesian network on the other hand accurately expresses the variance in the training data when making predictions, which can be seen by the fact that essentially all of the training samples fall within two standard deviations of the Bayesian network mean.

14:07.000 --> 14:15.000
Compared to the plane network, however, the variance in the Bayesian network's prediction actually grows in regions of the test base further from the training set.

14:15.000 --> 14:28.000
Because each of the Bayesian network's predictions correspond to the prediction of an ensemble of networks, each of which samples independently from the network's posterior distribution and chooses a particular extrapolation of the data as the plane network did,

14:28.000 --> 14:41.000
the Bayesian network's increased variance is therefore a reflection of averaging over the uncertainty of these ensembles in regions of the output space that they haven't been trained on.

14:41.000 --> 14:58.000
The next experiment is an image classification experiment on the MNIST dataset, which contains 60,000 28 by 28 pixel images of handwritten digits collected by the US Postal Service in the early 90s in order to develop a mechanism capable of automatically sorting mail based on zip codes.

14:58.000 --> 15:05.000
Image classification was performed using a Bayesian convolutional neural network whose architecture is described in the bottom figure.

15:05.000 --> 15:21.000
It consists of three convolutional layers. The first layer has 32 filters in a 24 by 24 pixel feature map, while the second and third layers have 64 filters each and an 84 by 84 pixel and 64 by 64 pixel feature map respectively.

15:21.000 --> 15:30.000
Three convolutional layers are separated by two cooling layers, which are standard sub-sampling layers used to keep the number of parameters in the network small.

15:30.000 --> 15:36.000
Here they down sample each convolutional feature map by a factor of two in the width and height directions.

15:36.000 --> 15:51.000
The final two layers are fully connected and have 128 neurons and 10 neurons respectively. All hidden layers compute RELU activations and the output layer computes a softmax distribution over the 10 possible digit classes of each image.

15:51.000 --> 16:00.000
The training occurred over 3000 training steps, or approximately 23 epochs, with many batches of 128 images per training step.

16:00.000 --> 16:16.000
At the 10th, 100th, 500th, and 3000th training step, 10 images were randomly selected from the validation set and classified 50 times each, corresponding to 50 independent samples from the network's posterior distribution for each classification image.

16:16.000 --> 16:33.000
The four wide columns displayed here show the sampling results at each of these four training steps, and the three narrow columns within each wider column show the image classified to histograms posterior samples in the approximate predicted distribution over the 10 digit classes given the corresponding image.

16:33.000 --> 16:50.000
At the start of training, posterior samples have a high variance, and probability masses distributed more or less uniformly over the 10 digit classes from most images, owing to the fact that parameter values are randomly initialized at the start of training, and 10 training steps isn't enough time for the network to learn anything meaningful.

16:51.000 --> 17:08.000
As training progresses, the posterior variance increases as the network is able to correctly classify images with increasingly high probability, although many images, such as the nines in the first, fifth, seventh, and eighth rows are still being incorrectly classified much of the time.

17:08.000 --> 17:19.000
By the 500th training step, learning becomes slower, however the posterior variance is still decreasing, and the accuracy of the predicted distribution continues to improve on the whole.

17:20.000 --> 17:37.000
By the end of the 3000 epoch, the network is fully converged, posterior samples show almost zero variance for the overwhelming majority of images, and the network's predicted distribution has a classification accuracy of 95.3% on the full test set.

17:37.000 --> 17:45.000
As was the case in the regression experiment, we see that Bayesian network's uncertainty is captured in the variance of its posterior distribution.

17:45.000 --> 17:51.000
We can also see how the distribution of the first two moments of the posterior change between the beginning and ending of training.

17:51.000 --> 18:02.000
At the start, both means and variances are tightly peaked, whereas at the end, means are distributed less sharply and with a slightly heavier tail, whereas variances are essentially flat in some layers.

18:02.000 --> 18:15.000
As a final word, both experiments show us that employing Bayesian methods in neural networks is preferable in scenarios where we would like to be able to express uncertainty in our neural network's predictions.

18:15.000 --> 18:21.000
Finally, all works cited in this presentation are listed here. Thank you for your time.

