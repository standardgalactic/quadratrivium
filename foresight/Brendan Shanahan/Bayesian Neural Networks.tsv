start	end	text
0	7000	Hi, my name is Brandon Shanahan, and today I'm going to talk about Bayesian neural networks.
7000	16000	In the first part of the talk, I'll give a brief, high-level introduction to neural networks that broadly covers what they are, what they do, and how they work.
16000	26000	I'll spend a bit of time talking about the stochastic optimization procedure employed by neural networks, and then I'll discuss two common neural network architectures that are used in experiments in a later section.
26000	34000	Next, I'll give an equivalent description that frames neural networks as conditional probabilistic models and outline the parallels between the two definitions.
34000	41000	I'll then discuss the shortcomings of these models, which will motivate the introduction of Bayesian methods in neural network.
41000	45000	Finally, I'll discuss the results of two experiments employing Bayesian neural network.
45000	61000	One regression experiment compares the results obtained by a plain neural network and a Bayesian neural network, and one image classification experiment that further illustrates the benefits of Bayesian learning.
61000	70000	Neural networks have been rigorously shown to be universal approximators capable of representing arbitrarily complex nonlinear functions to arbitrary precision.
70000	84000	As such, neural networks are used in many tasks which require learning incredibly complex mappings between input and output spaces, such as in speech recognition and text generation, sentiment analysis, and image segmentation and classification.
84000	91000	To take a concrete example, say we're building a network for use in an autonomous vehicle which specifically performs object detection.
91000	107000	You can imagine a function that maps, say, an image of a typical city street from the point of view of the driver to a set of categorical boundaries, each of which represents a pedestrian, another car, a traffic light or street sign, a crosswalk, and so on, is highly nonlinear.
107000	114000	The function also needs to be extremely adaptable as it's being computed in near real time and the data it acts on isn't static.
114000	122000	The car is moving, pedestrians are walking, other cars are moving, lights are changing, and no two images that it captures will be identical.
122000	130000	A scenario like this makes it clear that it's not sufficient for our network to simply learn a deterministic mapping based on a set of training data that it's seen before.
130000	141000	In other words, the network needs to be able to generalize well and extrapolate to new data based on the context of data that it's already seen, but it also needs to have some notion of the uncertainty inherent in its decisions
141000	149000	to be able to consider this uncertainty when taking in action in order to prevent making overconfident incorrect decisions.
149000	160000	The basic computational unit of a neural network is referred to as an artificial neuron, or more simply, a neuron, in which it develops historically as a crude mathematical model of a biological neuron.
160000	172000	A neuron takes some input data to which it applies an affine transformation with a weight matrix W and bias vector B, both of which are learned parameters, and finally, a nonlinear activation function as its output.
172000	177000	The simplest neural network architecture is a fully connected feedforward network.
177000	187000	Neurons are arranged in layers, which are stacked one on top of the other, starting from the input layer, followed by an arbitrary number of intermediate hidden layers, and finally, an output layer.
187000	200000	This architecture gets its name from the fact that a neuron in one layer is fully connected to every neuron in the two layers at sandwich between, and a neuron's output is fed forward as input to every neuron in the next layer.
200000	207000	The key to a neural network's ability to learn nonlinear representations of their input are the use of nonlinear activation functions.
207000	215000	Simply applying successive affine transformations at each layer of a network is equivalent to applying a single affine transformation.
215000	225000	Therefore, a neural network that only uses an identity activation function at each neuron can only learn a linear mapping between its input and output spaces.
225000	231000	The form of a network's output layer depends on the type of data it processes and the network's intended task.
231000	244000	For classification of a labeled categorical data, the output layer consists of C neurons, which includes a probability distribution over the C possible classes of the data, and assigns a prediction which maximizes this distribution.
244000	252000	It then computes a cross-entropy loss function between the true and predicted distributions of the data, which is usually averaged over the entire frame set.
252000	264000	For regression, the output layer consists of a single neuron, which computes an identity activation function and, typically, either the mean absolute error or mean squared error loss between the true and predicted output.
264000	274000	It's important to note here that while the output layer computes an identity activation function, hidden layers still compute nonlinear activation functions.
274000	284000	Neural networks are represented internally by what's referred to as a directed acyclic graph, which allows for decomposing complex functions into a sequence of simple arithmetic operations.
284000	297000	This is especially important because neural networks learn their parameters using optimization procedures such as stochastic gradient descent for one of its many variants, which require computing the gradient of the loss function with respect to the network's input.
297000	313000	For functions whose gradients do not have a simple closed form expression, this internal representation allows the computation of a single intractable gradient to be achieved instead by taking simple local gradients with respect to intermediate variables which are defined and cached locally during training.
313000	323000	Gradients are first computed with respect to neurons in the output layer and then propagated backwards through the network's hidden layers to the input layer by a straightforward application of the chain rule.
323000	334000	At each neuron, its parameters are updated by taking a step in the direction of steepest gradient descent, with the step size controlled by a hyperparameter lambda referred to as the learning rate.
334000	344000	Training proceeds by performing a forward pass with the entire training data set and computing the average loss, which is then back propagated through all the network parameters to the input layer.
344000	348000	This forward-backward sequence is referred to as one training epoch.
349000	358000	Parameters are modified during each epoch, and the network is said to have converged when the loss function plateaus over many subsequent forward passes.
358000	371000	Another important neural network architecture is the convolutional neural network, which is designed specifically to operate on image data and to take advantage of the additional structure of an information contained in an image's color channels.
371000	384000	Convolutional neural networks tend to learn incredibly rich, local, and translationally invariant features that make them extremely powerful, and they have been the primary building block in most state-of-the-art neural network designs in the last decade.
384000	391000	Convolutional neural networks are distinct for the use of convolutional layers, which differ from fully connected layers in several important ways.
391000	402000	First, while each neuron in a fully connected layer will see the entire output of the previous layer, each convolutional feature map has a limited receptive field and only sees a fixed region of the input.
402000	418000	Additionally, a fully connected neuron computes an affine transformation by acting on a flattened, vectorized representation of the input image, whereas a feature map is formed by convolving the color channels in its receptive field with a set of filters, which are the network's learned parameters.
418000	441000	In other words, whereas a neuron in a fully connected layer acts on a one-dimensional input vector to produce a one-dimensional feature vector, a convolutional layer acts on a three-dimensional image volume with dimensions of width, height, and color channels to produce a three-dimensional feature map with dimensions of receptive field width, receptive field height, and filter size.
442000	449000	Neural networks can be equivalently understood without making any references to their biological motivations or internal architecture.
449000	459000	Here, we define a neural network as a conditional probabilistic model, which is conditioned on a set of training data D and its learned weight and bias parameters, which we simply refer to as theta.
459000	468000	Given some new input x hat, the network computes a probability distribution over its entire output space and selects an output y hat, which maximizes the distribution.
468000	480000	To draw an equivalence between these two descriptions, in a classification setting, the output space y is the set of all possible classes, and the model p is a categorical distribution, and the corresponding loss function is the cross-entropy loss.
480000	488000	For regression, the output space y is the real numbers, and p is a Gaussian distribution, and the corresponding loss function is the mean squared error loss.
488000	496000	Nothing about the actual implementation changes and parameter optimization proceeds by back propagation, as described in the previous section.
496000	509000	Parameters learned during back propagation are maximum likelihood estimates, or rather stochastic approximations.
509000	512000	This likelihood model presents several shortcomings.
512000	524000	Networks whose parameters are maximum likelihood estimates result in predictions that are deterministic, in the sense that, once trained, predictions of a given input corresponds to a point estimate, and will always result in the same output.
524000	530000	And these networks are therefore prone to overfitting and unable to express uncertainty about their prediction.
530000	534000	This can be especially problematic when generalizing to data that they have not been trained on.
534000	543000	Techniques such as L1 and L2 regularization, commonly referred to as ridge and lasso respectively, are commonly used in practice to counteract overfitting.
543000	549000	But these networks are nonetheless deterministic and unable to express uncertainty in their predictions.
549000	558000	Bayesian inference for neural networks proceeds by defining a prior distribution over the parameters of the network, and applying Bayes' rule to compute their posterior distribution.
558000	571000	Predictive queries about some new data X-hat correspond to computing the predicted distribution of an unknown label Y-hat given X-hat by taking the expectation of the conditional distribution of the new data with respect to the network's posterior.
571000	578000	Every posterior weighted parameter configuration contributes to the prediction of the label Y-hat given the data X-hat.
578000	586000	Therefore, taking the expectation under the posterior is equivalent to computing a weighted average of predictions from an ensemble of plain neural networks,
586000	590000	each of whose parameters are drawn from the same shared distribution.
590000	600000	Because their predictions correspond to posterior samples, the ability to express uncertainty in the parameters in subsequent observations is built into Bayesian neural networks.
600000	612000	This ensemble averaging in subsequent parameter uncertainty has regularizing effects on Bayesian neural network predictions, which are equivalent to the regularization methods previously discussed.
612000	623000	As with most interesting Bayesian models, computing the posterior distribution is analytically intractable and must be approximated, which is generally done using sampling methods such as Markov, J, and Monte Carlo.
623000	638000	Bayesian neural networks instead frame posterior inference as an optimization problem over the parameters by of a target variational distribution by minimizing the cubic Liebler divergence or KL divergence between the variational distribution and the true posterior.
638000	647000	At inference time, the network parameters theta are instead sampled from the variational distribution.
647000	659000	The corresponding loss function is commonly referred to as the variational free energy or the variational lower bound, and represents its tradeoff between maximizing the expected log likelihood of the data with respect to the variational distribution,
659000	667000	referred to as the likelihood cost, and minimizing the KL divergence between the variational distribution and the network's prior, referred to as the complexity cost.
667000	677000	Equivalently, variational free energy represents a tradeoff between satisfying the complexity of the data and the simplicity of the network's prior distribution.
677000	694000	Because we're minimizing an expectation with respect to the known variational distribution, we can approximate the true loss by instead sampling from the variational distribution and computing the approximate loss in a process similar to the aforementioned Markov, J, and Monte Carlo approach.
694000	704000	By assuming the samples from the posterior are normally distributed and uncorrelated, we can apply a local reparameterization technique which allows us to sample parameter free white noise epsilon,
704000	716000	then shift and scale epsilon by a deterministic function of mu and rho, mean and standard deviation of the variational distribution respectively, to obtain a sample theta from the variational posterior.
716000	722000	In this context, the back propagation is slightly modified and referred to as Bayes by back prop.
722000	731000	Now, gradients are taken with respect to the variational parameters mu and rho, which are then updated by taking a step in the direction of steepest gradient descent.
731000	740000	Network training otherwise proceeds as normal, performing alternating forward passes and backward passes until the network parameters are converged.
740000	744000	I'll now describe two experiments employing Bayesian neural networks.
744000	753000	The first is a regression experiment on a toy data set which compares the performance of the plane to the forward fully connected network to that of an identical Bayesian neural network.
753000	759000	Training samples are generated using 2048 equally spaced points between zero and one half.
759000	770000	Both neural network architectures consist of two fully connected hidden layers with 250 neurons each in RELU activations and an output layer with a single neuron and identity activation.
770000	775000	Training occurred over 500 epochs with many batches of 32 samples.
775000	790000	Once fully trained, each network was used over 500 trials to predict the test set 4096 samples run uniformly randomly from the closed interval from minus one half to one, and the results were averaged and are given in the bottom to bottom way.
790000	795000	For the plane neural networks results are on top and the Bayesian networks results are on the bottom.
795000	804000	The red line is the mean prediction over 500 trials and the dark blue and light blue regions are the one sigma and two sigma confidence intervals respectively.
804000	806000	A few things are interesting.
806000	818000	First off, the plane neural network underestimates the true variance of the test data in the region of the space that it was trained on, which is maybe not so surprising given that we know how prominent these types of networks are overfitting.
818000	834000	However, in regions of the test base where the networks have not been trained, the plane neural network chooses a particular extrapolation of the training data and subsequent test predictions have extremely low variance, which goes towards zero as points in the test base gets further from the training set.
834000	847000	The Bayesian network on the other hand accurately expresses the variance in the training data when making predictions, which can be seen by the fact that essentially all of the training samples fall within two standard deviations of the Bayesian network mean.
847000	855000	Compared to the plane network, however, the variance in the Bayesian network's prediction actually grows in regions of the test base further from the training set.
855000	868000	Because each of the Bayesian network's predictions correspond to the prediction of an ensemble of networks, each of which samples independently from the network's posterior distribution and chooses a particular extrapolation of the data as the plane network did,
868000	881000	the Bayesian network's increased variance is therefore a reflection of averaging over the uncertainty of these ensembles in regions of the output space that they haven't been trained on.
881000	898000	The next experiment is an image classification experiment on the MNIST dataset, which contains 60,000 28 by 28 pixel images of handwritten digits collected by the US Postal Service in the early 90s in order to develop a mechanism capable of automatically sorting mail based on zip codes.
898000	905000	Image classification was performed using a Bayesian convolutional neural network whose architecture is described in the bottom figure.
905000	921000	It consists of three convolutional layers. The first layer has 32 filters in a 24 by 24 pixel feature map, while the second and third layers have 64 filters each and an 84 by 84 pixel and 64 by 64 pixel feature map respectively.
921000	930000	Three convolutional layers are separated by two cooling layers, which are standard sub-sampling layers used to keep the number of parameters in the network small.
930000	936000	Here they down sample each convolutional feature map by a factor of two in the width and height directions.
936000	951000	The final two layers are fully connected and have 128 neurons and 10 neurons respectively. All hidden layers compute RELU activations and the output layer computes a softmax distribution over the 10 possible digit classes of each image.
951000	960000	The training occurred over 3000 training steps, or approximately 23 epochs, with many batches of 128 images per training step.
960000	976000	At the 10th, 100th, 500th, and 3000th training step, 10 images were randomly selected from the validation set and classified 50 times each, corresponding to 50 independent samples from the network's posterior distribution for each classification image.
976000	993000	The four wide columns displayed here show the sampling results at each of these four training steps, and the three narrow columns within each wider column show the image classified to histograms posterior samples in the approximate predicted distribution over the 10 digit classes given the corresponding image.
993000	1010000	At the start of training, posterior samples have a high variance, and probability masses distributed more or less uniformly over the 10 digit classes from most images, owing to the fact that parameter values are randomly initialized at the start of training, and 10 training steps isn't enough time for the network to learn anything meaningful.
1011000	1028000	As training progresses, the posterior variance increases as the network is able to correctly classify images with increasingly high probability, although many images, such as the nines in the first, fifth, seventh, and eighth rows are still being incorrectly classified much of the time.
1028000	1039000	By the 500th training step, learning becomes slower, however the posterior variance is still decreasing, and the accuracy of the predicted distribution continues to improve on the whole.
1040000	1057000	By the end of the 3000 epoch, the network is fully converged, posterior samples show almost zero variance for the overwhelming majority of images, and the network's predicted distribution has a classification accuracy of 95.3% on the full test set.
1057000	1065000	As was the case in the regression experiment, we see that Bayesian network's uncertainty is captured in the variance of its posterior distribution.
1065000	1071000	We can also see how the distribution of the first two moments of the posterior change between the beginning and ending of training.
1071000	1082000	At the start, both means and variances are tightly peaked, whereas at the end, means are distributed less sharply and with a slightly heavier tail, whereas variances are essentially flat in some layers.
1082000	1095000	As a final word, both experiments show us that employing Bayesian methods in neural networks is preferable in scenarios where we would like to be able to express uncertainty in our neural network's predictions.
1095000	1101000	Finally, all works cited in this presentation are listed here. Thank you for your time.
