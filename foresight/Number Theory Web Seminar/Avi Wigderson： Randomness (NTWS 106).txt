When the organizers asked me to give a talk in this seminar,
I told them that I have no new results or maybe no old results
even in number theory.
And I can talk about some general topics
in my field of interest in complexity theory.
And they said that it could be great or good.
So I'll talk about randomness, which is one of my favorite topics.
I want to talk about this randomness.
It's something that I guess mankind was always interested in.
And it's a topic you can talk to almost everyone on the street
about lack and chance and unpredictability and informally.
But you can, you know, mathematicians study the randomness
in variety of forms in probability and statistics and so on.
And then physicists are extremely curious about randomness
and the meaning of randomness and a lot of scientific interest
in randomness.
So it's a very, very broad topic.
And people have sometimes very strong opinions about randomness
and its meaning and so on.
So what I want to tell you about today is this computational
complexity point of view of randomness.
And of course, this is a very general, high-level, non-technical talk.
It will be a lot of things you'll see you would have known.
So I would like to stress it's a new conceptual element.
So there'll be lots of examples.
And I'll tell you when something new is coming of importance.
OK, so what's now is OK.
The plan is, first of all, I'm going
to talk only about discrete events.
For number theory, this is usually OK.
So what's perfect randomness for me?
Perfect that the perfect random event will be a coin toss.
Something that has probability of heads and tails equal to half.
And if there are many of them, they are independent.
So I always show this slide.
Which of these two sequences of 20 letters is more probable?
I know you know the answer.
They're equally probable.
So that is what we would call the uniform distribution
or the distribution with full entropy.
This would be like an ideal model of randomness.
Perfect randomness.
And for the first 15 minutes or so,
I'll just go through examples of different applications,
you can call them, or ways in which randomness is used.
Places it is used.
And the main thing in the back of our mind
should be, can we really do that?
I mean, what do we do if we don't have perfect independence?
Can any of them be done without it?
And in order to understand this, we'll
move to define pseudo randomness, which roughly speaking,
is just a, you can think of collectively deterministic
structures that share some properties with random ones.
Some properties.
And we'll talk about different such properties.
And that will help us understand that.
We'll see many examples.
And then I will go back to the basic question of what
can we do in a world in which there is no perfect randomness?
Maybe there is weak randomness or no randomness
and we'll roughly discuss the meaning of this.
All pretty informal.
Of course, underlying the main messages
are formal theorems that move them that you
can quiz me about that.
OK, so let me start with the utility of randomness.
And as I said, the back of our mind,
you'll see this icon in every slide.
You can ask yourself, or maybe you have asked yourself,
where are the random bits for this coming from?
And we'll discuss that later.
OK, so the most basic example that everybody
gives to the power of randomness is just sampling.
You want to know what will be the fraction of people
who will vote red in a particular country.
You just, even if it's huge, it's millions of people.
If you sample 2,000 at random, but completely at random,
a perfect subset of 2,000 at random,
then you know that with probability at least 99%
over this choice, the percentage of red votes
in the sample and the red votes in the population
will be within 2% of the actual percentage.
And we know it's hugely powerful.
First of all, we know that these numbers, 2 and 1%,
don't depend on the size of the population.
That's basically the law of large numbers.
It's a very powerful way of using randomness.
And of course, the main point is that if we didn't have
randomness here, it's not clear how we would get even an estimate
up to 2% without asking 98% of the population what they think.
So that's a very, very strong demonstration.
And I want to move to less obvious ones.
Here's one.
I give you this portion of the plane,
the proportion of the grid.
And I ask you, in how many ways can you tie this with dominoes?
You don't have to look at the colors here.
Just in how many ways can you tie this region with dominoes?
It can be this rectangular region.
It could be any other region.
And this is a basic question, a basic counting question.
Of course, it doesn't come out of nowhere.
I mean, it's picturesque, but it's also important.
It's got a dimer problem in physics.
And for diatomic molecules, it captures
thermodynamic properties of that matter.
You can understand from it free energy, phase transitions,
and stuff like that.
So it's also an important problem.
But the main problem is that there can be,
even the region is small.
It's 1,000 by 1,000.
The number of such configurations can be used.
And here's a very important result
from about 20 years ago, Jerome Cichlial Vigoda.
They found a way to approximately count
the number of dominoes tiling in any region.
In fact, it's more general.
It's the number of matchings in any graph.
Doesn't have to be planar.
The type of algorithm, I'm not going to describe algorithm,
but the nature of it is some people know.
It's what's called the Monte Carlo method.
The number of matchings can be exponential
in the size of the region, so the size of the graph.
So you cannot just write it down or look at all of them.
There's a sort of Markov chain analyzing this.
And anyway, figuring out that this Markov chain converges
quickly is highly non-trivial.
And that gives a probabilistic algorithm
for this problem for approximately counting the number.
We know that counting exactly the number
is a difficult problem, and I will not go into this.
So this approximation problem can be done fast probabilistically.
But the best deterministic algorithm known
requires exponential timing.
We can do really better than just trying all possibilities
and adding them up.
So I want to stress that this is the best known deterministic
algorithm.
There may be faster ones that we haven't discovered yet,
and actually we'll see at the end that maybe there
is the faster one.
OK, so there are many, many such examples.
And this is really the key application
that I want to eventually discuss.
What can we do of randomness, namely
to probabilistic algorithm, understanding
the power of randomness in computation.
Many examples, many of them, they are all over mathematics
in every field of mathematics.
In number theory, the problem of finding large primes,
just give me a thousand big primes, thousand digits prime.
We don't know any deterministic way to do that.
We can certify primality quickly by now,
but we don't know how to just find a large number, which
is prime deterministically.
Factoring multivariate polynomials over finite fields,
it's another example approximating
the volume of convex sets in high dimensions,
computing large free eco-efficient of multivariate
functions that are given by a program, say, that circuit.
In all of these things, in all these cases,
and there are many more examples,
there are fast probabilistic algorithms,
in many cases, that are highly non-trivial.
And on the other hand, the best known deterministic
algorithm for each of them requires exponential time.
So we'd like to know whether there are deterministic ones
or not.
Sorry, Avi, can I ask?
For finding a large prime, why don't you just start?
If you want to find a prime above n, you start at n,
and then you use, we know the prime is in p.
So you check if n is a prime, then check of n plus 1
is a prime, and by the prime number theorem,
you take your log n steps before you find one.
If you believe the prime, no, no, it will not take your log n.
I mean, the best we know is root n or something like that.
We don't know that there is a prime.
You need cramming.
Prime number theorem does not guarantee
that every interval of log n is a prime.
I mean, the gaps between the primes.
OK, so that part of it is still probabilistic,
even if I can check the numbers.
Yeah, yeah, we don't know.
I mean, any advance on any of the expressions
would be extremely interesting.
Yeah, I mean, there was a large project in this,
I forgot what the last one was.
Polymath.
Sorry?
Polymath.
Yeah, polymath, the projects that Tim Giles and Terry Tao
organized some 10 years ago just focused
on this particular problem, finding large primes.
And yeah, this was one of the polymath's failures.
I mean, I think it was really interesting
what went on there.
But anyway, yeah, but it's good to just stress that when
we say n, large n, we mean like 1,000 digits.
So our number, we start with 1 and then
have some 1,000 digits after it, and it's prime.
Find one deterministically.
Yeah.
I'll just interrupt for a second.
I mean, in practice, you can find large primes very quickly
because we know that you don't have
to go very far to find a prime.
It's the question.
There's no practice here.
I want a theorem.
I want a theorem.
Yeah.
No, Joe, but just along Alex's side.
Yeah, there are lots of heuristics for lots of problems.
And yeah, but anyway, these are just examples.
It's just no surprise that you are interested in this one.
But I think it's a good challenge for number theory.
It's a really interesting one.
Of course, we have an algorithm that
is deterministic that is conjectured to work very rapidly,
but we cannot prove that.
Yeah, but this is the only question.
I want something provable.
I want a theorem.
So in all these cases, the probabilistic algorithm
will guarantee to find with the antenna probability of prime
and it will certify that.
And we want to do it without randomness.
Can we do it?
And let's stop the discussion of this.
Sorry, I made progress.
But yeah, it's a great problem, this particular one.
OK, good.
So these are examples of probabilistic algorithms.
I want to give you examples of just some other things
where randomness is used of a different nature, several such
examples.
Distributed computation, randomness
can make not just an exponential improvement in time,
but an infinite one, in that they
can make some problems that are provably impossible,
become possible with randomness.
These are two picturesque examples
of fundamental problems in distributed computing.
One is the dining philosophers problem.
The second is the Byzantine generals problem.
As you can tell, these guys know how
to pick names for their problems.
So in both cases, I will not explain this.
It's a bit beside the main course of the lecture,
but I want to stress that these are fundamental problems
in distributed computation, in asynchronous distributed
computation.
One is about consensus and one is about coordination.
And theorems that there is no deterministic solution,
whatever that means, I didn't define it.
But if the actors in this distributed system
have randomness, then it's possible to solve them.
And in fact, these algorithms are used.
So here, the gap can be arbitrary between randomness
and determinism.
And that's provable.
So we will not talk about the applications here,
but another demonstration.
Here's another one that goes even beyond the randomness
helps to do things.
It also helps to define things.
In game theory, which is supposed
to model rational behavior, we try
to understand in strategic situation
what would actors, players, agents do.
Here's a very simple example from a game of Auman called
the chicken game, which is often done in game theory,
represented by a matrix of payoffs for two players.
In this, you can imagine that this game represents
two sort of macho drivers driving towards each other
on a narrow road.
And they can either, each of them
can either sway to the side or continue to drive.
And like being cautious or aggressive.
And the payoffs to each of them are written here,
where you can see in the bottom right
that if they are both aggressive,
then no one deviates.
They both basically die.
But anyway, you ask what would players do in such a situation?
What would be a rational behavior?
And as I'm sure most of you know,
there is a key concept that John Nash defined,
the Nash equilibrium.
You want a strategy for each pair of strategies,
such that given the other one, you would not change yours.
So it's kind of a stability requirement
on a solution concept.
And it's a very natural one to choose.
And of course, Nash didn't get the Nobel Prize just
for the definition.
He also had a theorem, is that every game,
every strategic game of this nature,
no matter how many players, no matter how many strategies,
there is always such a solution.
This solution concept is universal.
It always exists.
What is interesting and important for us
is that, again, as many of you know,
this depends on the ability of players to toss coins.
So the strategies they will choose,
the strategies that exist are mixed strategies,
namely the random strategies.
In this particular game, there's a unique such equilibrium.
With these numbers, it happens to me
to be cautious with probability 3 quarter
and aggressive with probability 1 quarter.
If you know that the other driver behaves like this,
you would stay with your choice.
Anyway, the main point here is that if you require
or you don't allow them randomness,
if they are deterministic, there's no equilibrium.
So here's another place where there are several points here.
This is something that exists with randomness
and does not exist without randomness.
Another is that you can ask yourself, here, it's really important.
I mean, are people in life, in economic situations,
speaking strategies like this?
And are they using randomness and so on?
Anyway, but that's just another example.
Example closer to computer science
and to life of everybody is cryptography,
where randomness is basically you can't live home without it.
Everything depends on it.
The very definition of a secret needs randomness.
I mean, if you Shannon already pointed out
that formalized using this notion of entropy and so on,
a secret is basically as good as entropy in it.
I mean, if you pick your 90s password randomly,
then my chances of guessing it is exactly 10 to the 9.
If, on the other hand, you pick it as the phone number of one
of your friends, then I probably have a better chance of guessing it.
It will have much less sense of it.
But then, of course, randomness is absolutely essential
to any notion we define in cryptography, basically everything.
And also there, and this is something that's practically used all the time,
you can ask yourself what randomness is used there
and is it perfect as the definitions all assume,
perfect randomness like the public scargoite, all assume perfect randomness.
Where is it coming from?
Yeah, randomness is used in lots of other places, of course.
And also there you may want the random bits
coming from in the casino and so on.
So that's about the set of examples I wanted to talk about.
And it's clear that randomness is an essential part of lots of our lives.
So now I want to ask the question we asked before.
Where are these random bits coming from?
Let's say in particular applications that we have seen.
And I don't know about you, but when I don't know the answer
to a question I asked Google, I'm sure you do many times.
So you feed Google through random bits.
What do you get?
Well, 10 years ago when this slide is from, I got four million answers.
Probably today there will be 400 million answers.
But you start at the answer more or less is basically you buy them.
You want good random bits, you buy them.
At least there are lots of companies that will sell you random bits.
And then you can ask yourself, well, where do they take their random bits from?
And OK, well, you can read what they say.
I mean, some take it from various physical phenomena, which is interesting
because they seem somewhat unpredictable.
Some take them from other physical phenomena, which are maybe more unpredictable,
like quantum behavior.
If you wonder about casinos at least some years ago, they were all using this chip
of this company and you can wonder whether it works or not.
I mean, whether what they are doing does generate random bits or random in what sense.
Anyway, we would like to understand this question.
What is randomness?
In fact, what is randomness to any particular purpose that randomness is good?
And so now I really want to come to a central definition.
And yeah, this time to the next slide is maybe the most important slide of the talk.
We are trying to define or understand how to define the randomness.
So it's taken from a fundamental paper of Blame and Michali from the early 80s.
And here I invite Alina to unmute herself and join me.
It's OK, Alina.
Alina, are you there?
Yes, I'm here.
All right, yes.
Enthusiastic, as I can see.
OK, so yeah, I'm going to do an experiment.
We are trying to understand.
We said that the post-typical random event is a coin toss, half tails, half heads.
So suppose I'm holding a coin and I'm going to toss it in the air.
I'll toss a coin and, you know, you should guess just as it leaves my finger.
So maybe we assume we are in the same room, let's say, as in the picture you are watching me.
And, you know, maybe it spends two seconds in the air and you should guess just as it
leaves my finger whether it will be heads on tails when it lands on the floor.
OK, what do you think the probability that you'll predict it correctly will be?
Uh-huh.
Good.
I told you not to worry.
Good, OK, so I agree and I think most people will give this answer.
OK, OK, let me ask you another question.
Suppose, you know, you are holding a laptop.
In fact, I know you have a laptop that you are watching.
You can use it.
So, you know, as I currently my finger, you know, you can do whatever you want.
What do you think is the probability that you will predict it correctly?
Still, huh?
OK, we are in agreement.
And again, I think everybody will have the same view.
OK, and so now let's change it again.
And suppose I give you, you know, any number of video cameras that are trained on my fingers
and they are all connected to a great computer and a supercomputer and it is connected to
your laptop and they are all ready to go just as I toss the coin.
What do you think will be the chances that you predict the outcome correctly?
Well, depends how well positioned.
It is powerful as you want.
It gets, I guess, closer to one, right?
Yeah, well, I think the right person.
Thank you.
Yeah, so what's the point about this?
Again, I think that you can imagine that, you know, with sufficient machinery, one can
calculate, you know, the angular momentum of the coin and the, you know, the air pressure
and the humidity in the air and the distance to the floor with perfect accuracy so that
almost surely you'll get the right answer suddenly in far less than two seconds.
OK, so what is the point of this?
There is a major point which deviates from all previous views of randomness.
I want to stress that the experiment, me tossing the coin, the random event, suppose the random
event, the event anyway we are using, didn't change.
My behavior didn't change here.
I'm tossing the coin.
I'm the random generator.
I didn't change at all.
Right?
This is the point.
So what is happening is we are seeing a property of randomness that distinguishes between different
observers of the random phenomena, right?
Depends, you know, randomness somehow, I mean, the ability to predict how much entropy is
in the coin depends on the, maybe not the eye of the beholder, but the computational
power of the beholder, OK?
So this is very different than basically every prior point of view till the 80s.
It's an objective definition, sorry, unlike the previous objective definition of randomness.
This is a subjective one.
It depends on the observer and its operative, you know, we look at what the observer would
say or what's the probability that the observer will say something about what they see.
OK, so we are going to take now this, you know, point of view and going to study pseudo
randomness or notions, various notions of pseudo randomness because they too will depend
on observers or on properties of events that are being looked at.
So I'm switching now to the second part and if there's an important question or somebody
in the chat that Philip saw that is, you know, I need to answer now, I'm happy to.
So just repeat the message is that we are now considering randomness as a property of
the observer.
How much randomness, how much entropy or what is, what is, what we recall random depends
on observers or on properties of what we are seeing.
OK, so let me just start giving examples.
So now I'm moving to the second part and this is about pseudo randomness and this slide
is a bit abstract and then there will be many examples.
So don't worry if this is a bit abstract, but anyway, it should be appealing still.
We are looking now at deterministic structure.
So imagine that the universe of objects, these objects can be, you know, of any types
that can be numbers, graphs, sequences, tables, works, codes, lots of possibilities, but
discrete ones.
And in it, we want to understand what is the random like behavior or a typical behavior
and it will, I'll call it a property and the property will be just a subset of this
universe and the definition will be really simple.
A pseudo random property is any large property, any property, any subset that occupies most
of the universe.
So think of a hay in a, you know, in a haystack.
So a property that's held by most elements of the universe, equivalently, it's like
the hay in a haystack or, you know, another way to think about it is you should pick a
random element from you, it satisfies this property.
So this I will call and for the, you know, this example that will follow this will be
for me a pseudo random property.
I want to stress that both math and CS have lots of questions about pseudo randomness
and we'll see them.
The typical task, such question in mathematics is whether a particular object has a pseudo
random property, which is defined.
So we study, you know, there's random like properties of natural, this X zero, maybe
natural, we'll see examples in computer science is a different angle.
Mostly we look, you know, we want an algorithm to find any element of this with this property.
So basically we are looking for finding objects that have random like properties.
We know that most of them are like that.
So it's like sort of searching for hay in a haystack and, you know, as we'll see, I
mean, this is not an easy task.
Despite, you know, the popular culture, we are not looking for needles.
We are looking for hay.
OK, so let's see examples and, you know, we'll get familiar to my terminology here.
First, I want to say just why, you know, yeah, the popular culture does say that we
should easily find hay in haystack.
Why, why is this not true in the in the problems we are really interested in?
First of all, that, you know, we are talking about huge objects.
And typically we see very, very tiny parts of them.
So we may see closer to us, maybe needles.
And we also often tend to search, you know, sort of using the tools we have, but they
may not be sufficient.
I just want to give two sort of popular examples.
You know, we may look at this and say, oh, this is green everywhere, even though we know
most of it is red, but we have to cut it first.
And another example is that I like a lot is this issue of exoplanets.
I mean, for up to about 30 years ago, 25 years ago, I think people had no evidence
that other stars, which are, of course, suns in the universe have any planets.
There was no way to see them.
You need the particular new techniques.
So you could have predicted that, you know, no, no suns have planets, except maybe ours.
But it's not true.
Okay.
So sometimes, you know, we don't, we don't, these are huge objects and we may not have
the right tools.
So let's see, you know, more mathematical example.
Let me start with codes, because most people are familiar with them.
Of course, codes are extremely, air-correcting codes are extremely important to, you know,
lots of things we use in our daily lives and embedded in all of the storage and communication
devices are good air-correcting codes that they have, you know, large rates and distance
if you want.
You know, there's a formal definition of what a good code, what the properties of a good
code is.
But, you know, let's fix some such notion.
The basic result that Shannon, you know, opened the door for this amazing theory, coding theory,
was the, first of all, the observation that a random code is a good code.
Almost all codes are good codes.
I'm not defining formally the space and so on, but some of you know and some of you
can imagine.
But regardless, a random code is a good code.
And the first thing I want to take from this is to, you know, instill in you that, you
know, my notation, this, that means that good codes is a, being a good code is a pseudo
random property.
Why?
Because it's, you know, almost every code is good.
Okay.
That's one point.
And then what do you want to do?
If you want to put it in various devices, you need to be also useful.
You want to be able to describe it, decode and code it, decode it and so on.
And this is a hard problem.
I mean, that took, you know, some 20 years, then we are still looking for other, you know,
even better ones, but to find explicit codes, you know, efficiently usable codes, this,
this is a hard problem.
So this is an example of problem, you know, of finding an element of a pseudo random property.
Here's a very different element, a very different example, I'm sure you're all familiar with
this number.
So here is a, you know, a property that you would like maybe this random to satisfy or
is believed to be a property of pi.
If you look at the number of occurrences of the digits seven in the expansion of pi,
it looks like it appears about one tenth of the time.
If you look at any pair of digits, let's say five, four point 65, four occurs one hundredth
of the time.
If you look at any triple one thousandths of the time, et cetera.
So let's demand that let's say we are looking at real numbers, maybe between zero and one.
And we want all these properties, it's a countable number of properties, we want them
to hold about this real number.
And moreover, we want them to hold not only in base them, but in base seven, in base 15,
in base million and one and so on, still a countable countable number of properties.
And what do we know about this, these are called normal numbers, properties, number satisfying
all these properties are called normal and basically an observation that just because
are only countable in many conditions that the random real number, let's say between
real and between zero and one is normal with high probability except with probability zero.
So again, this means that normality is a pseudo random property, it is held by almost
every number in the universe repeat.
And mathematicians study this question is, you know, for example, particular natural
elements like pi like root two or whatever are normal.
And I should say this is, these problems are still open, maybe some of you are working
on them, I don't know.
Yeah, anyway, so the open problems in the, you know, that are formulated naturally formulated
in this language of randomness, give me, give me an object that looks like a random object
here in the pseudo random properties normality.
So this may be, you know, not a central examples in mathematics, but I want to say that the
problems that can be expressed in exactly this form, you know, is a natural object pseudo
random are absolutely fundamental in both math and computer science.
And I want to give you another example with you, you particularly will know number theories.
But in general, the major problems that concern or can be phrased questions about pseudo randomness.
Usually, if I have a room, talking to and can see people like us, young ones, you know,
what are major problems?
How do you know that the problem is major?
Well, one, one answer is that there's a lot of bounty money offered for it.
So here's one set of such problems, the clay millennium problems.
And one is gone, there's still other ways to make a million dollars.
What I want to say is that at least two of these problems are really naturally expressed as
questions about pseudo randomness, you know, our main problem, computer science, and maybe
your main problem, the Riemann hypothesis, I will not spend much time about this.
First one, give a sample, but say briefly, why is it a pseudo randomness question?
Well, what's the pseudo random property?
Being hard to compute is a pseudo random property.
Random functions are hard to compute.
And we ask whether a particular function, let's say the traveling settlement problem or
you know, solving, you know, quadratic systems of quadratic equations over finite fields,
your favorite, whether a particular problem, natural problem is also hard to compute.
That's basically the previous NP question.
So it's a question about pseudo randomness, whether a natural element has a pseudo random property.
And I want to talk about the second one, I'll elaborate on this, I don't need to read it,
but again, we'll see a pseudo random property and many of you, of course, will know this
result.
So let's express Riemann hypothesis as a pseudo random property of something.
And if something would be, you know, the universe would be a collection of walks.
So here's a canonical description of the drunkard walk.
You think that there is a pub in location zero in this street, if you want, and somebody walks
into it, and after a few beers come out and start walking up and down the streets, maybe a bit drunk.
So takes each step up with probability half and down with probability half.
And then you can, you know, just run an experiment just and toss random coins and see where
the person gets to after 100 steps.
And even if you didn't know math, you would observe immediately that something weird happens
that despite the fact that the person took, you know, 100 steps, he or she would be only
about 10 paces away from the pub.
So something happens, which we know, and of course, it's an exercise to prove that after
10 steps, almost surely, it was very high probability, the distance from the origin will
be about root 10 steps.
So it's an exercise, of course, and more important for me, it's something we see as pseudo random
property here, right? So the property of a walk staying close to the origin about root
10 from the origin is a pseudo random property because almost every walk has this property.
Okay, good. So now let's talk about the particular walk, which you all like, the Mabuse walk.
You know, so here's the definition, you know, for an integer p of x, you know, the number
of distinct prime divisors, and we just define mu of x, a Mabuse function.
It will be zero if the x is a square divisor and otherwise, you know, it's,
oops, there's a minus one hidden.
Yeah, so it's minus one if it's odd.
I wonder if I can fix this so that it will be visible.
Here is a minus.
Okay, sorry about that.
Okay, anyway, that's a Mabuse function and Mabuse function can think of it as describing a walk.
I mean, it just tells you for every time step, first step, second step, third step, what to do,
whether to go up the street or down the street. And let's suppose we allow also that sometimes you
stay where you are. So I'm changing a bit the definition of a walk, but that doesn't matter
much. And now, what are we wondering about is this particular walk is just a description of a
walk like before, but only it's deterministic, not random. And we ask whether it's, you know,
has it stood on the property? Does it stay close to the origin? And as, you know,
probably most of you in this audience know, that's basically the lima hypothesis, right?
If for every end this work says around the origin, then the lima hypothesis is true and vice
versa. So it's another formulation, a very convenient formulation, the one that you can tell
your, you know, your family and friends about without zeta functions about what the lima hypothesis
really is. So, but anyway, very conveniently stated as the pseudorandom is a question about
the particular pseudorandom. Okay, so these are basically the examples we see one more soon,
but I want to get now to the point I was talking about in the beginning. How does this
point of view, what did it lead us to understand about applications of randomness,
you know, to, especially to probabilistic algorithms and
some major results, both technically and conceptually understanding
some fundamental tools about the power and limitation of randomness,
at least in computational settings. Okay, and actually quite beyond, but I will probably not
have time to talk about this. Okay, so let me summarize this on one slide and I'll elaborate
a bit on one. So the three possible worlds I want to consider are the following. One is the
world in which we have perfect randomness in which we could, you know, do everything we want,
we just are guaranteed a stream of unbiased independent quantos. The second world is a world
that was maybe alluded to by some of the companies that use physical devices or physical phenomena
to create random bits or pseudorandom bits. This is a world where we have some unpredictable
phenomena, you can think of sunspots or stock market behavior or radioactive decay or quantum
phenomena, your favorite. In each of them, we feel there's some entropy, we feel there's some
unpredictability, if we want to know, you know, of course, there are not independent events,
if you want to know whether tomorrow it's probably a good guess to say that it will be like today.
So they're correlated and they are not sometimes biased and so on. Of course,
we can manipulate them and try to use them then in one of these applications. Anyway,
this is a world in which there's some entropy in the sequence we see, but it's not necessarily
independent unbiased, you know, uniform distribution. And the third world is, you know,
there's the other world is deterministic, there is no randomness at all, it's all in our imagination,
everything's predictable, what can we do then? We can just, everything's deterministic.
So, you know, we saw these applications in case we have perfect randomness,
this question, what survives otherwise? And in the next two boxes are really major,
these major understandings. So in the first, it turns out that all probabilistic algorithms
can be made to function as they did, even in the presence of very weak randomness.
This is the subject of what is called extractor theory, randomness extraction,
long, long sequence of works. And it is all about purifying randomness, taking, you know,
weak random sources, sources that are biased and dependent, and somehow extracting
audience or extracting anyway, their entropy in pure form, they may make to look maybe
shorter sequences, but they look like they were perfectly random. So that's one,
you know, one theory, one major result, sequence of results.
The second one is what happens if the world is deterministic. And in this case,
we just don't know how to move an inch without an assumption, but it's okay,
the assumption is something that we live with, and believe. So I want to assume,
I put it in quotes, because I don't want to formally define it,
well, you can ask me, but something like P is different than NP. You can
satisfy yourself, at least in that the whole electronic commerce world you live in assumes
far less than that. It's assumed that factoring or discrete logarithm or some other computational
problem in NP is difficult, it's exponentially difficult. And that's the nature of the assumption
that we need. Okay, so we assume something like there exists a house function. That's what we assume.
And now under this assumption, again, you know, we don't need randomness. All fast algorithms for
with for every practical purposes are all algorithms. Fast, I mean those we can run in our
lifetimes, all efficient algorithms that are probabilistic can be replaced,
to say every fast probabilistic algorithms has a deterministic counterpart. There's another
algorithm which is deterministic does exactly the same thing has no error. And it's not much slower
than the original. This is a consequence of another theory, which, you know, but sort of the paradigm
underlying it, underlying it is the hardness versus randomness paradigm, namely you can use
house function to generate good enough randomness deterministically. Again, long sequence of
works several decades spent and the consequences is this understanding. So in other words, it seems
like, you know, randomness for the purpose of algorithms is far weaker than may have been
suspected initially with their power or the seeming success in the absence of the deterministic
algorithms for lots of problems we want to solve. Okay. I want to say something about
doing with time. Just give a hint. Okay, good. I started the seven minutes late. So that's good.
Take five more minutes of your time.
I want to tell you something about this structural theory and randomness verification.
And yeah, we saw that perfect randomness has lots of applications. And supposedly in reality,
we have, you know, this sort of weak random sources is bias dependent sources. And I want
to assume for this, you know, this is only part of the story. I want to assume that
there are several of them, and they're independent of each other. So let's say we have three or five
or 10 such phenomena that we can tap, they're all weak, you know, they all contain maybe
an end bit sequence will contain only n over 100 bits of entropy or maybe square
with n of bits of entropy. But I have several of them. So I want to assume that, you know,
sunspots don't affect stock market behavior. Some people believe that it does actually,
but let's assume it's not. So I have several sources. And the question is, if I have such a,
you know, collection of outputs of such phenomena, can I transform it into maybe a
shorter sequence of something that looks, you know, exactly or almost exactly statistically
exactly like the uniform distribution. That's a subject of example theory. And I want to give
you a hint into one result that, you know, uses a pseudorandom property. So we'll connect the
two parts of the talk. So I want to talk about one more pseudorandom property,
you know, it's a property of tables of matrices of numbers. Okay, so think of a random
n by n matrix with entries in n. Think you're working more than
elements between one element. Okay, and what, when do I call, you know, what's the property I'm after?
The property is that if I look at any sub matrix, small sub matrix, okay,
then I see lots of numbers. So let's quantify this. Yeah, this is certainly that, you know,
what you will see in, you know, in such random examples. Every small window, which is a
some matrix, some matrix, I mean, in general, a subset of rows crosses a subset of columns.
Let's say, you know, if I pick a k by k window, the k rows and k columns, and I look at this k
squared entries, I see significantly more than k distinct entries. I mean, you would expect,
actually, it will be close to k squared if k is small enough. And k is small enough, k is maybe
n to the point one. So in a random table, you know, surely you will have this property for
every window. I want to stress, I want for every window. It's a sort of random property. It's an
easy theorem counting argument to prove that a random matrix will have this property.
And here's the question of the same nature. Can such matrices be constructed explicitly?
Is there, you know, just give me a matrix of numbers in one to n by n matrix where, of course,
a family of matrices so that every k by k window has at least k to the 1.1 distinct entries, say.
Okay, I mean, we mathematicians have lots of examples for the matrices we like that look
random in various ways. But I mean, let's start from, you know, every case could give you a matrix,
right? I mean, second grader can give you the addition table. So here's the, here's the matrix.
Let's say we wrap more than the numbers. I didn't do it here, but we wrap the numbers more than.
Is it good? Is this to do random in this sense? Does every small window have few entries? Well,
of course not. I mean, we've seen in, if we look along an arithmetic progression
in rows and columns, we will see just a linear number in k by k matrix will see just a linear
number of distinct entries. So it's certainly bad. Okay, well, we can go to third grade and think about
the multiplication table. And we can ask whether this is good. And of course, the answer is no
for the same reason if we look at this rectangle doesn't show it. But if you look at the geometric
progression, long rows and columns, again, you'll see only a linear number of entries.
And this somehow reminds me of a lesson that von Neumann was saying or attributed to von Neumann
that anyone who considers a mathematical method to produce random digits is of course in a state of
sin. But anybody who knows the history knows that von Neumann ignores his own advice and of course
use pseudo random generators based on mathematical means. And in fact, you should do it in this case
too. And what I'm going to say is the result that probably many of you are familiar with
is that while each of these tables, each of these matrices, the addition one, the multiplication
one is bad. Their combination is good. Starting with other similarity in the case of integers
and then we'll get passed out in the final field case, let's say more than or more p.
If you do it more the prime, then every window will be good in one of these tables. So if you
look at the union of numbers you see in the, you know, in the window above and window below,
there'll be plenty of entries as many as you need. So what does it mean in terms of, so first of all
we found, you know, by cheating a bit, by taking two tables, we found
elements that have this pseudo random property which are explicit. In fact, they are very simple
and easy to compute. And this was used in one of the early solutions to this problem of
extractor randomness. Basically, some and products are independent. That's the message from this
very important theorem and mixing them increases entropy. So one way to view this extractor
is that if you have three such sources, you can add the first two and multiply by the third,
you know, they think of them as integers more p. The entropy, total entropy will have increased.
The entropy rate will have increased and therefore you can repeat it several times and approach,
you know, perfect randomness. So that's one example of an extractor using this particular pseudo random
property of tables or methods. Okay, so let me summarize.
Some things we have seen, some important messages. I want to go back to this really important point
that, you know, randomness is in the eye of the beholder or in the computational power of the
beholder. This is a subjective definition and this very important point of view that leads to this
understanding. It's also pragmatic in the sense that you can study the use of randomness
or the intended use of randomness and maybe by understanding it you can remove the use of randomness
in it. And the impact of this kind of point of view is that you can remove randomness from
probabilistic algorithms even in situations where you have very weak random sources or even assuming
hardness in a world which is just purely deterministic. I should maybe add here that because
you are familiar with this example but may not be familiar with how related it is to the lecture.
So you all know that there is a deterministic polynomial time algorithm to certify prime
to determine whether a number and integer is prime or not. You also probably know that there
are lots of probabilistic algorithms for this before the deterministic one was known and of
course a question of doing this sufficiently is extremely odd. Gauss talks about it quite explicitly
and well Granville has a beautiful survey in which he sites in the beginning this except from
Gauss's discussion on this arithmetic where he basically calls to the community basically says
it's a disgrace we don't yet have an efficient algorithm for either primality or factoring.
Anyway the way what was I think less people know is that the way that the deterministic
algorithm was obtained by Agarwal Kayal and Saxena was the following. They've designed
a new probabilistic algorithm for primality testing and then in fact this was earlier work of
Agarwal and another student and the work of Agarwal Kayal and Saxena is a
de-randomization of this algorithm. It's a way of understanding the way the algorithm
used this randomness and constructing a specific pseudorandom generator which fools it and therefore
makes it deterministic. Okay so anyway there are many other probabilistic algorithms for which we
know it only under some assumptions like p different. Next pseudorandomness so what was used is the
general very high level definition of pseudorandomness which connects basically like
in the example of primality connects the property the pseudorandom property with the
application in mind maybe the algorithm you want to the randomness. The study of pseudorandomness
captures many basic problems and areas both in math and in computer science. I could have
talked another hour on you know structural versus randomness paradigm which is very much
part of a pseudorandomness study. It captures things like this name I think was given by Tao gave
surveys of examples like this and you know many like some other regularity lemma or it's used
in high math progressions in the primes it's used in boosting and machine learning and it's used
basically in the other applications of it in analysis and PDEs and you know that's
definitely in number theory and yeah so it stems from a very
you know pseudorandomness like approach to a very wide array of problems.
And the last thing is that pseudorandom objects that you you build for particular purposes in
particular you know extractor for extractor theory or expander graphs or various codes or something
turn out to actually they were initially designed for fault tolerant circuits
or that's at least what Pinsker has in mind as an application.
Often they find many many many more applications than they used to there they somehow fundamental
turn out many of them are fundamental in other contexts than you know I think you must have
heard of is high dimensional expander another example of a pseudorandom object of a mysterious
nature that we are trying to understand and yeah there are many more so let me end with this thank you.
