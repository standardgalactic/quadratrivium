1
00:00:00,000 --> 00:00:13,560
What an incredible pleasure to be here with Jeffrey Hinton, one of the great minds and

2
00:00:13,560 --> 00:00:15,160
one of the great issues of our time.

3
00:00:15,160 --> 00:00:19,640
A man who helped create artificial intelligence was at the center of nearly every revolution

4
00:00:19,640 --> 00:00:25,160
in it and now has become perhaps the most articulate critic of where we're going.

5
00:00:25,160 --> 00:00:27,520
So an honor to be on stage with you.

6
00:00:27,520 --> 00:00:28,520
Thank you.

7
00:00:28,560 --> 00:00:31,880
During the moniker Godfather of AI, one of the things that AI has traditionally had

8
00:00:31,880 --> 00:00:36,320
problems with is humor, I asked AI if it could come up with a joke about the Godfather of

9
00:00:36,320 --> 00:00:39,840
AI and it actually wasn't that bad.

10
00:00:39,840 --> 00:00:44,240
It said he gave AI an offer it couldn't refuse, neural networks.

11
00:00:44,240 --> 00:00:45,240
It's not bad.

12
00:00:45,240 --> 00:00:46,240
Okay, that's not bad.

13
00:00:46,240 --> 00:00:47,240
It's good for AI.

14
00:00:47,240 --> 00:00:48,240
So let's begin with that.

15
00:00:48,240 --> 00:00:52,160
What I want to do in this conversation is very briefly step a little back into your foundational

16
00:00:52,160 --> 00:00:57,040
work, then go to where we are today and then talk about the future.

17
00:00:57,040 --> 00:01:01,280
So when you're building and you're designing neural networks and you're building computer

18
00:01:01,280 --> 00:01:06,400
systems that work like the human brain and that learn like the human brain and everybody

19
00:01:06,400 --> 00:01:09,440
else is saying, Jeff, this is not going to work.

20
00:01:09,440 --> 00:01:15,680
You push ahead and do you push ahead because you know that this is the best way to train

21
00:01:15,680 --> 00:01:21,000
computer systems or you do it for more spiritual reasons that you want to make a machine that

22
00:01:21,000 --> 00:01:22,840
is like us?

23
00:01:22,840 --> 00:01:28,280
I do it because the brain has to work somehow and it sure as hell doesn't work by manipulating

24
00:01:28,280 --> 00:01:34,440
symbolic expressions explicitly and so something like neural nets had to work.

25
00:01:34,440 --> 00:01:38,360
Also von Neumann and Turing believed that so that's a good start.

26
00:01:38,360 --> 00:01:42,200
So you're doing it because you think it's the best way forward?

27
00:01:42,200 --> 00:01:44,640
Yes, in the long run the best way forward.

28
00:01:44,640 --> 00:01:48,920
Because that decision has profound effects down the line.

29
00:01:49,320 --> 00:01:50,640
Okay, so you do that.

30
00:01:50,640 --> 00:01:55,640
You start building neural nets, you push forward and they become better than humans

31
00:01:55,640 --> 00:01:57,880
at certain limited tasks, right?

32
00:01:57,880 --> 00:02:04,520
At image recognition, at translation, some chemical work.

33
00:02:04,520 --> 00:02:10,800
I interviewed you in 2019 at Google I.O. and you said that it would be a long time before

34
00:02:10,800 --> 00:02:15,200
they could match us in reasoning and that's the big change that's happened over the last

35
00:02:15,200 --> 00:02:17,040
four years, right?

36
00:02:17,040 --> 00:02:19,600
They still can't match us but they're getting close.

37
00:02:19,600 --> 00:02:23,120
And how close are they getting and why?

38
00:02:23,120 --> 00:02:27,720
It's the big language models that are getting close and I don't really understand why they

39
00:02:27,720 --> 00:02:31,560
can do it but they can do little bits of reasoning.

40
00:02:31,560 --> 00:02:36,840
So my favorite example is I asked GPT-4, a puzzle that was given to me by a symbolic

41
00:02:36,840 --> 00:02:40,080
AI guy who thought it wouldn't be able to do it.

42
00:02:40,080 --> 00:02:45,800
I made the puzzle more difficult than it could still do it and the puzzle was the rooms in

43
00:02:45,800 --> 00:02:50,400
my house are painted blue or yellow or white.

44
00:02:50,400 --> 00:02:53,360
Yellow paint fades to white within a year.

45
00:02:53,360 --> 00:02:55,760
In two years' time I want them all to be white.

46
00:02:55,760 --> 00:02:58,440
What should I do and why?

47
00:02:58,440 --> 00:03:03,560
And it says you should paint the blue rooms white and then it says you should do that

48
00:03:03,560 --> 00:03:08,080
because blue won't fade to white and it says you don't need to paint the yellow rooms because

49
00:03:08,080 --> 00:03:09,920
they will fade to white.

50
00:03:09,920 --> 00:03:14,640
So it knew what I should do and it knew why and I was surprised that it could do that

51
00:03:14,640 --> 00:03:16,040
much reasoning already.

52
00:03:16,040 --> 00:03:21,040
And it's kind of an amazing example because when people critique these systems or they

53
00:03:21,040 --> 00:03:24,000
say they're not going to do much, they say they're mad libs, they're just word completion

54
00:03:24,000 --> 00:03:25,800
but that is not word completion.

55
00:03:25,800 --> 00:03:27,800
To you is that thinking?

56
00:03:27,800 --> 00:03:35,280
Yeah, that's thinking and when people say it's just autocomplete, a lot goes on in

57
00:03:35,280 --> 00:03:37,640
that word just autocomplete.

58
00:03:37,640 --> 00:03:42,400
If you think what it takes to predict the next word, you have to understand what's been

59
00:03:42,400 --> 00:03:45,360
said to be really good at predicting the next word.

60
00:03:45,360 --> 00:03:49,600
So people say it's just autocomplete or it's just statistics.

61
00:03:49,600 --> 00:03:56,360
Now there's a sense in which it is just statistics, but in that sense everything's just statistics.

62
00:03:56,360 --> 00:04:00,840
It's not the sense most people think of statistics as it keeps the counts of how many times this

63
00:04:00,840 --> 00:04:04,080
combination of words occurred and how many times that combination.

64
00:04:04,080 --> 00:04:05,080
It's not like that at all.

65
00:04:05,080 --> 00:04:10,040
It's inventing features and interactions between features to explain what comes next.

66
00:04:10,040 --> 00:04:15,560
So if it's just statistics and everything is just statistics, is there anything that

67
00:04:15,560 --> 00:04:17,840
we can do?

68
00:04:17,840 --> 00:04:20,760
Obviously it's not humor, maybe it's not reasoning.

69
00:04:20,760 --> 00:04:25,560
Is there anything that we can do that a sufficiently well-trained large language model with a sufficient

70
00:04:25,560 --> 00:04:29,840
number of parameters and a sufficient amount of compute could not do in the future?

71
00:04:29,840 --> 00:04:35,960
If the model is also trained on vision and picking things up and so on, then no.

72
00:04:35,960 --> 00:04:40,400
But is there anything that we can think of and any way we can think in any cognitive

73
00:04:40,400 --> 00:04:44,080
process that the machines will not be able to replicate?

74
00:04:44,080 --> 00:04:48,440
We're just a machine, we're a wonderful, incredibly complicated machine, but we're

75
00:04:48,440 --> 00:04:52,680
just a big neural net and there's no reason why an artificial neural net shouldn't be

76
00:04:52,680 --> 00:04:54,480
able to do everything we can do.

77
00:04:54,480 --> 00:04:59,280
Are we a big neural net that is more efficient than these new neural nets we're building

78
00:04:59,280 --> 00:05:01,480
or are we less efficient?

79
00:05:01,480 --> 00:05:06,000
It depends whether you're talking about speed of acquiring knowledge and how much knowledge

80
00:05:06,000 --> 00:05:09,960
you can acquire or whether you're talking about energy consumption.

81
00:05:09,960 --> 00:05:12,920
So in energy consumption, we're much more efficient.

82
00:05:12,920 --> 00:05:17,080
We're like 30 watts and one of these big language models, when you're training it, you train

83
00:05:17,080 --> 00:05:20,760
many copies of it each looking at different parts of the data.

84
00:05:20,760 --> 00:05:22,920
So it's more like a megawatt.

85
00:05:22,920 --> 00:05:27,520
So it's much more expensive in terms of energy, but all these copies can be learning different

86
00:05:27,520 --> 00:05:29,480
things from different parts of the data.

87
00:05:29,480 --> 00:05:33,360
So it's much more efficient in terms of acquiring knowledge from data.

88
00:05:33,360 --> 00:05:37,480
And it becomes only more efficient because each system can train each next system?

89
00:05:37,480 --> 00:05:38,480
Yes.

90
00:05:38,480 --> 00:05:39,760
So let's get to your critique.

91
00:05:39,760 --> 00:05:45,360
So the best summarization of your critique came from a conference at the Milken Institute

92
00:05:45,360 --> 00:05:51,960
about a month ago and it was Snoop Dogg and he said, I heard the old dude who created

93
00:05:51,960 --> 00:05:57,160
AI saying this is not safe because the AI's got their own mind and those motherfuckers

94
00:05:57,160 --> 00:06:01,040
going to start doing their own shit.

95
00:06:01,040 --> 00:06:02,040
Is that accurate?

96
00:06:02,040 --> 00:06:05,280
Is that an accurate summarization?

97
00:06:05,280 --> 00:06:12,000
They probably didn't have mothers.

98
00:06:12,000 --> 00:06:14,760
But the rest of what Dr. Dogg said is correct.

99
00:06:14,760 --> 00:06:15,760
Hang on.

100
00:06:15,760 --> 00:06:16,760
Yes.

101
00:06:16,760 --> 00:06:17,760
All right.

102
00:06:17,760 --> 00:06:23,520
So explain what you mean or what he means and how it applies to what you mean when they're

103
00:06:23,520 --> 00:06:25,440
going to start doing their own shit.

104
00:06:25,440 --> 00:06:26,440
What does that mean to you?

105
00:06:26,560 --> 00:06:27,560
Okay.

106
00:06:27,560 --> 00:06:31,680
So first I have to emphasize we're entering a period of huge uncertainty.

107
00:06:31,680 --> 00:06:33,440
Nobody really knows what's going to happen.

108
00:06:33,440 --> 00:06:36,960
And people whose opinion I respect have very different beliefs from me.

109
00:06:36,960 --> 00:06:39,360
Like Jan LeCun thinks everything's going to be fine.

110
00:06:39,360 --> 00:06:40,360
They're just going to help us.

111
00:06:40,360 --> 00:06:41,960
It's all going to be wonderful.

112
00:06:41,960 --> 00:06:46,160
But I think we have to take seriously the possibility that if they get to be smarter

113
00:06:46,160 --> 00:06:51,400
than us, which seems quite likely, and they have goals of their own, which seems quite

114
00:06:51,400 --> 00:06:55,120
likely, they may well develop the goal of taking control.

115
00:06:55,320 --> 00:06:57,800
And if they do that, we're in trouble.

116
00:06:57,800 --> 00:06:58,800
So okay.

117
00:06:58,800 --> 00:07:02,400
So let's let's go back to that in a second, but let's take Jan's position.

118
00:07:02,400 --> 00:07:06,040
So Jan LeCun was also one of the people who won the Turing Award and is also called the

119
00:07:06,040 --> 00:07:07,560
Godfather of AI.

120
00:07:07,560 --> 00:07:11,480
And I was recently interviewing him and he made the case.

121
00:07:11,480 --> 00:07:16,200
He said, look, technologies, all technologies can be used for good or ill, but some technologies

122
00:07:16,200 --> 00:07:18,000
have more of an inherent goodness.

123
00:07:18,000 --> 00:07:23,760
And AI has been built by humans, by good humans for good purposes.

124
00:07:23,760 --> 00:07:27,240
It's been trained on good books and good texts.

125
00:07:27,240 --> 00:07:30,680
It will have a bias towards good in the future.

126
00:07:30,680 --> 00:07:32,760
Do you believe that or not?

127
00:07:32,760 --> 00:07:36,560
I think AI that's been trained by good people will have a bias towards good.

128
00:07:36,560 --> 00:07:41,400
And AI that's been trained by bad people like Putin or somebody like that will have a bias

129
00:07:41,400 --> 00:07:42,740
towards bad.

130
00:07:42,740 --> 00:07:46,040
We know they're going to make battle robots.

131
00:07:46,040 --> 00:07:49,520
They're busy doing it in many different defense departments.

132
00:07:49,520 --> 00:07:54,120
So they're not going to necessarily be good since their primary purpose is going to be

133
00:07:54,120 --> 00:07:56,360
to kill people.

134
00:07:56,360 --> 00:08:03,640
So you believe that the risks of the bad uses of AI are whether they're more or less than

135
00:08:03,640 --> 00:08:08,240
the good uses of AI are so substantial, they deserve a lot of our thought right now.

136
00:08:08,240 --> 00:08:09,240
Certainly.

137
00:08:09,240 --> 00:08:10,240
Yes.

138
00:08:10,240 --> 00:08:12,240
For lethal autonomous weapons, they deserve a lot of our thought.

139
00:08:12,240 --> 00:08:15,840
Well, let's, okay, let's stick on lethal autonomous weapons because one of the things

140
00:08:15,840 --> 00:08:21,480
in this argument is that you are one of the few people who is really speaking about this

141
00:08:21,480 --> 00:08:23,880
as a risk, a real risk.

142
00:08:23,880 --> 00:08:32,440
Explain your hypothesis about why super powerful AI combined with the military could actually

143
00:08:32,440 --> 00:08:35,280
lead to more and more warfare.

144
00:08:35,280 --> 00:08:36,520
Okay.

145
00:08:36,520 --> 00:08:41,800
I don't actually want to answer that question.

146
00:08:41,800 --> 00:08:43,120
There's a separate question.

147
00:08:43,120 --> 00:08:48,320
Even if the AI isn't super intelligent, if defense departments use it for making battle

148
00:08:48,320 --> 00:08:51,520
robots, it's going to be very nasty, scary stuff.

149
00:08:51,520 --> 00:08:55,400
And it's going to lead, even if it's not super intelligent, and even if it doesn't have its

150
00:08:55,400 --> 00:09:00,640
own intentions, it just does what Putin tells it to.

151
00:09:00,640 --> 00:09:06,000
It's going to make it much easier, for example, for rich countries to invade poor countries.

152
00:09:06,000 --> 00:09:11,240
A present, there's a barrier to invading poor countries willy-nilly, which is you get dead

153
00:09:11,240 --> 00:09:13,420
citizens coming home.

154
00:09:13,420 --> 00:09:15,720
If they just dead battle robots, that's just great.

155
00:09:15,720 --> 00:09:18,960
The military industrial complex would love that.

156
00:09:18,960 --> 00:09:23,320
So you think that because, I mean, it's sort of a similar argument that people make with

157
00:09:23,320 --> 00:09:24,320
drones.

158
00:09:24,320 --> 00:09:26,800
If you can send a drone and you don't have to send an airplane with a pilot, you're more

159
00:09:26,800 --> 00:09:30,080
likely to send the drone, therefore you're more likely to attack.

160
00:09:30,080 --> 00:09:34,000
If you have a battle robot, it's that same thing squared.

161
00:09:34,000 --> 00:09:35,280
And that's your concern.

162
00:09:35,280 --> 00:09:37,120
That's my main concern with battle robots.

163
00:09:37,120 --> 00:09:42,040
It's a separate concern from what happens with super intelligent systems taking over

164
00:09:42,040 --> 00:09:44,360
for their own purposes.

165
00:09:44,360 --> 00:09:48,200
Before we get to super intelligent systems, let's talk about some of your other concerns.

166
00:09:48,200 --> 00:09:53,200
So in the litany of things that you're worried about, you obviously we have battle robots

167
00:09:53,200 --> 00:09:56,560
as one, you're also quite worried about inequality.

168
00:09:56,560 --> 00:09:58,120
Tell me more about this.

169
00:09:58,120 --> 00:10:03,360
So it's fairly clear, it's not certain, but it's fairly clear that these big language

170
00:10:03,360 --> 00:10:06,920
models will cause a big increase in productivity.

171
00:10:06,920 --> 00:10:11,240
So there's someone I know who answers letters of complaint for a health service.

172
00:10:11,240 --> 00:10:16,200
And he used to write these letters himself and now he just gets chat GPT to write the

173
00:10:16,200 --> 00:10:20,640
letters and it takes one-fifth of the amount of time to answer a complaint.

174
00:10:20,640 --> 00:10:26,840
So he can do five times as much work and so there are only five times fewer of him.

175
00:10:26,840 --> 00:10:28,720
Or maybe they'll just answer a lot more letters.

176
00:10:28,720 --> 00:10:29,960
Or they'll answer more letters, right?

177
00:10:29,960 --> 00:10:33,480
Or maybe they'll have more people because they'll be so efficient, right?

178
00:10:33,480 --> 00:10:35,680
More productivity leads to more getting more done.

179
00:10:35,680 --> 00:10:37,880
This is an unanswered question.

180
00:10:37,880 --> 00:10:42,440
But what we expect in the kind of society we live in is that if you get a big increase

181
00:10:42,440 --> 00:10:47,440
in productivity like that, the wealth isn't going to go to the people who are doing the

182
00:10:47,440 --> 00:10:51,440
work or the people who get unemployed, it's going to go to making the rich richer and

183
00:10:51,440 --> 00:10:52,440
the poor poorer.

184
00:10:52,440 --> 00:10:54,440
And that's very bad for society.

185
00:10:54,440 --> 00:10:57,880
Definitionally, or you think there's some feature of AI that will lead to that?

186
00:10:57,880 --> 00:11:02,280
No, it's not to do with AI, it's just what happens when you get an increase in productivity,

187
00:11:02,280 --> 00:11:05,120
particularly in a society that doesn't have strong unions.

188
00:11:05,120 --> 00:11:09,680
But now there are many economists who would take a different position and say that over

189
00:11:09,680 --> 00:11:14,800
time, and if you were to look at technology, right, we went from horses and horses and

190
00:11:14,800 --> 00:11:18,160
buggies and the horses and buggies went away and then we had cars and oh my gosh, the people

191
00:11:18,160 --> 00:11:22,920
who drove the horses lost their jobs and ATMs came along and suddenly bank tellers no longer

192
00:11:22,920 --> 00:11:23,920
need to do that.

193
00:11:23,920 --> 00:11:27,040
But we now employ many more bank tellers than we used to and we have many more people driving

194
00:11:27,040 --> 00:11:29,200
Ubers than we had people driving horses.

195
00:11:29,200 --> 00:11:35,080
So the argument what an economist would make to this would be, yes, there will be chair

196
00:11:35,080 --> 00:11:40,200
and there will be fewer people answering those letters, but there'll be many more higher

197
00:11:40,200 --> 00:11:41,640
cognitive things that will be done.

198
00:11:41,640 --> 00:11:43,920
How do you respond to that?

199
00:11:43,920 --> 00:11:48,640
I think the first thing I'd say is a loaf of bread used to cost a penny, then they invented

200
00:11:48,640 --> 00:11:52,680
economics and now it costs five dollars.

201
00:11:52,680 --> 00:11:56,880
So I don't entirely trust what economists say, particularly when they're dealing with

202
00:11:56,880 --> 00:12:00,240
a new situation that's never happened before.

203
00:12:00,240 --> 00:12:04,360
And superintelligence would be a new situation that never happened before, but even these

204
00:12:04,360 --> 00:12:10,400
big chatbots that are just replacing people whose job involves producing text, that's

205
00:12:10,400 --> 00:12:15,920
never happened before and I'm not sure how they can confidently predict that more jobs

206
00:12:15,920 --> 00:12:18,440
will be created than the number of jobs lost.

207
00:12:18,440 --> 00:12:22,840
I just have a little side note that in the green room, I introduced Jeff to, I have two

208
00:12:22,840 --> 00:12:27,760
of my three children are here, Alice and Zachary, they're somewhere out here, and he said to

209
00:12:27,760 --> 00:12:30,240
Alice, he said, are you going to go into media?

210
00:12:30,240 --> 00:12:33,280
And then he said, well, I'm not sure media will exist.

211
00:12:33,280 --> 00:12:35,000
And then Alice was asking, what should I do?

212
00:12:35,000 --> 00:12:36,000
And you said?

213
00:12:36,000 --> 00:12:37,000
Plumbing.

214
00:12:37,000 --> 00:12:38,000
Yes.

215
00:12:38,000 --> 00:12:39,000
Now explain.

216
00:12:39,000 --> 00:12:43,440
I mean, we have a number of plumbing problems at our house, it'd be wonderful if they were

217
00:12:43,440 --> 00:12:45,720
able to put in a new sink.

218
00:12:45,720 --> 00:12:50,120
Explain what jobs, a lot of young people out here, not just my children, but thinking about

219
00:12:50,120 --> 00:12:54,440
what careers to go into, what are the careers they should be looking at, what are the attributes

220
00:12:54,440 --> 00:12:55,440
of them?

221
00:12:55,440 --> 00:12:58,960
I'll give you a little story about being a carpenter.

222
00:12:58,960 --> 00:13:04,600
If you're a carpenter, it's fun making furniture, but it's a complete dead loss because machines

223
00:13:04,600 --> 00:13:06,360
can make furniture.

224
00:13:06,360 --> 00:13:11,760
If you're a carpenter, what you're good for is repairing furniture or fitting things into

225
00:13:11,760 --> 00:13:17,280
awkward spaces in old houses, making shelves in things that aren't quite square.

226
00:13:17,280 --> 00:13:22,440
So the jobs that are going to survive AI for a long time are jobs where you have to be

227
00:13:22,440 --> 00:13:27,460
very adaptable and physically skilled and plumbing is that kind of a job.

228
00:13:27,460 --> 00:13:31,660
How does manual dexterity is hard for a machine to replicate?

229
00:13:31,660 --> 00:13:37,020
It's still hard, and I think it's going to be longer before they can be really dexterous

230
00:13:37,020 --> 00:13:40,140
and get into awkward spaces.

231
00:13:40,140 --> 00:13:44,020
That's going to take longer than being good at answering text questions.

232
00:13:44,020 --> 00:13:45,020
Should I believe you?

233
00:13:45,020 --> 00:13:47,660
Because when we were on stage four years ago, you said reasoning.

234
00:13:47,660 --> 00:13:51,580
As long as somebody has a job that focuses on reasoning, they'll be able to last a dozen.

235
00:13:51,580 --> 00:13:57,380
Isn't the nature of AI such that we don't actually know where the next incredible

236
00:13:57,380 --> 00:13:58,980
improvement in performance will come?

237
00:13:58,980 --> 00:14:00,500
Maybe it will come in manual dexterity.

238
00:14:00,500 --> 00:14:02,500
Yeah, it's possible.

239
00:14:02,500 --> 00:14:05,340
So actually, let me ask you a question about that.

240
00:14:05,340 --> 00:14:10,540
So do you think when we look at AI and we look at the next five years of AI, the most

241
00:14:10,540 --> 00:14:14,940
impactful improvements we'll see will be in large language models and related to large

242
00:14:14,940 --> 00:14:16,140
language models?

243
00:14:16,140 --> 00:14:18,620
Or do you think it will be in something else?

244
00:14:18,620 --> 00:14:22,020
I think it'll probably be in multimodal large models.

245
00:14:22,020 --> 00:14:26,740
So they won't just be language models, they'll be doing vision.

246
00:14:26,780 --> 00:14:28,540
Actually, they'll be analyzing video.

247
00:14:28,540 --> 00:14:32,620
So they were able to train on all of the YouTube videos, for example.

248
00:14:32,620 --> 00:14:38,180
And you can understand a lot from things other than language.

249
00:14:38,180 --> 00:14:42,260
And when you do that, you need less language to reach the same performance.

250
00:14:42,260 --> 00:14:45,540
So the idea that they're going to be saturated because they've already used all the language

251
00:14:45,540 --> 00:14:49,100
there is, all the language is easy to get hold of.

252
00:14:49,100 --> 00:14:52,020
That's less of a concern if they're also using lots of other modalities.

253
00:14:52,020 --> 00:14:56,500
I mean, this gets at one of the, another argument that Jan, your fellow Godfather of AI makes

254
00:14:56,540 --> 00:14:58,620
is that language is so limited, right?

255
00:14:58,620 --> 00:15:00,940
There's so much information that we're conveying just beyond the word.

256
00:15:00,940 --> 00:15:02,900
In fact, I'm gesturing like mad, right?

257
00:15:02,900 --> 00:15:06,380
Which conveys some of the information as well as the lighting and all this.

258
00:15:06,380 --> 00:15:08,900
So your view is that may be true.

259
00:15:08,900 --> 00:15:13,780
Language is a limited vector for information, but soon it will be combined with other vectors.

260
00:15:13,780 --> 00:15:15,300
Absolutely.

261
00:15:15,300 --> 00:15:19,900
It's amazing what you can learn from language alone, but you're much better off learning

262
00:15:19,900 --> 00:15:21,020
from many modalities.

263
00:15:21,020 --> 00:15:23,780
Small children don't just learn from language alone.

264
00:15:23,780 --> 00:15:24,340
Right.

265
00:15:24,380 --> 00:15:31,180
So if you were, if your principal role right now was still researching AI, finding the

266
00:15:31,180 --> 00:15:38,580
next big thing, you would be doing multi-modal AI and trying to attach, say, visual AI systems

267
00:15:38,580 --> 00:15:40,620
to text-based AI systems?

268
00:15:40,620 --> 00:15:43,380
Yes, which is what they're doing now at Google.

269
00:15:43,380 --> 00:15:49,100
Google is making a system called Gemini, but fortunately, Demisabis talked about it a few

270
00:15:49,100 --> 00:15:53,220
days ago, and that's a multi-modal AI.

271
00:15:53,260 --> 00:15:55,340
Well, let me talk about actually something else at Google.

272
00:15:55,340 --> 00:16:01,500
So while you were there, Google invented the transformer network or invented the transformer

273
00:16:01,500 --> 00:16:05,580
architecture, generative pre-trained transformers.

274
00:16:05,580 --> 00:16:12,420
When did you realize that that would be so central and so important?

275
00:16:12,420 --> 00:16:17,780
It's interesting to me because it's this paper that comes out in 2017, and when it comes

276
00:16:17,780 --> 00:16:21,860
out, it's not as though firecrackers are left, you know, shot into the sky.

277
00:16:21,860 --> 00:16:25,860
It's six years later, five years later, that we suddenly realized the consequences.

278
00:16:25,860 --> 00:16:29,940
And it's interesting to think, what are the other papers out there that could be the same

279
00:16:29,940 --> 00:16:30,780
in five years?

280
00:16:30,780 --> 00:16:35,220
So with transformers, it was really only a couple of years later when Google developed

281
00:16:35,220 --> 00:16:36,540
BERT.

282
00:16:36,540 --> 00:16:41,740
So BERT made it very clear transformers were a huge breakthrough.

283
00:16:41,740 --> 00:16:48,860
I didn't immediately realize what a huge breakthrough they were, and I'm annoyed about that.

284
00:16:48,860 --> 00:16:50,700
It took me a couple of years to realize.

285
00:16:50,700 --> 00:16:55,100
Well, you know, the first time I ever heard the word transformer was talking to you on

286
00:16:55,100 --> 00:17:00,500
stage, and you were talking about transformers versus capsules, and this was right after

287
00:17:00,500 --> 00:17:01,500
it came out.

288
00:17:01,500 --> 00:17:05,460
Let's talk about one of the other critiques about language models and other models, which

289
00:17:05,460 --> 00:17:12,260
is soon, I mean, in fact, probably already they've absorbed all the organic data that

290
00:17:12,260 --> 00:17:13,780
has been created by humans.

291
00:17:13,780 --> 00:17:17,660
If I create an AI model right now, and I train it on the internet, it's trained on a bunch

292
00:17:17,660 --> 00:17:21,940
of stuff, mostly stuff made by humans, but a bunch of stuff made by AI, right?

293
00:17:21,940 --> 00:17:22,940
Yeah.

294
00:17:22,940 --> 00:17:27,740
And you're going to keep training AIs on stuff that has been created by AIs, whether it's

295
00:17:27,740 --> 00:17:31,780
text-based language model or whether it's a multimodal language model.

296
00:17:31,780 --> 00:17:37,300
Will that lead to the inevitable decay and corruption, as some people argue?

297
00:17:37,300 --> 00:17:41,140
Or is that just a thing we have to deal with?

298
00:17:41,140 --> 00:17:45,580
Or is it, as other people in the AI field, the greatest thing for training AIs, and we

299
00:17:45,580 --> 00:17:48,180
should just use synthetic data in AI?

300
00:17:48,180 --> 00:17:49,180
Okay.

301
00:17:49,180 --> 00:17:51,900
I don't actually know the answer to this technically.

302
00:17:51,900 --> 00:17:56,060
I suspect you have to take precautions, so you're not just training on data that you yourself

303
00:17:56,060 --> 00:18:00,540
generated or the some previous version of you generated.

304
00:18:00,540 --> 00:18:04,740
I suspect it's going to be possible to take those precautions, although it would be much

305
00:18:04,740 --> 00:18:08,300
easier if all fake data was marked fake.

306
00:18:08,300 --> 00:18:13,940
There is one example in AI where training on stuff from yourself helps a lot.

307
00:18:13,940 --> 00:18:19,180
So if you don't have much training data or rather you have a lot of unlabeled data and

308
00:18:19,180 --> 00:18:23,660
a small amount of labeled data, you can train a model to predict the labels on the labeled

309
00:18:23,660 --> 00:18:33,020
data and then you take that same model and train it to predict labels for unlabeled data

310
00:18:33,020 --> 00:18:38,180
and whatever it predicts, you tell it you were right.

311
00:18:38,180 --> 00:18:40,580
And that actually makes the model work better.

312
00:18:40,580 --> 00:18:42,820
How on earth does that work?

313
00:18:43,060 --> 00:18:47,860
Because on the whole it tends to be right.

314
00:18:47,860 --> 00:18:48,860
It's complicated.

315
00:18:48,860 --> 00:18:53,620
It's been analyzed much better in many years ago from acoustic modems.

316
00:18:53,620 --> 00:18:55,780
They did the same trick.

317
00:18:55,780 --> 00:19:01,740
So listening to this, I've had this realization on stage, you're a man who's very critical

318
00:19:01,740 --> 00:19:05,780
of where we're going, killer robots, income inequality.

319
00:19:05,780 --> 00:19:07,940
You also sound like somebody who loves this stuff.

320
00:19:07,940 --> 00:19:10,020
Yeah, I love this stuff.

321
00:19:10,020 --> 00:19:13,140
How could you not love making intelligent things?

322
00:19:13,140 --> 00:19:17,660
So let me get to maybe the most important question for the audience and for everyone

323
00:19:17,660 --> 00:19:19,580
here.

324
00:19:19,580 --> 00:19:22,580
We're now at this moment where a lot of people here love this stuff and they want to build

325
00:19:22,580 --> 00:19:25,220
it and they want to experiment.

326
00:19:25,220 --> 00:19:27,260
But we don't want negative consequences.

327
00:19:27,260 --> 00:19:29,020
We don't want increased income inequality.

328
00:19:29,020 --> 00:19:31,380
I don't want media to disappear.

329
00:19:31,380 --> 00:19:37,700
What are the choices and decisions and things we should be working on now to maximize the

330
00:19:37,700 --> 00:19:42,340
good, to maximize the creativity, but to limit the potential harms?

331
00:19:42,340 --> 00:19:47,340
So I think to answer that you have to distinguish many kinds of potential harm.

332
00:19:47,340 --> 00:19:50,740
So I'll distinguish like six of them for you.

333
00:19:50,740 --> 00:19:53,940
There's bias and discrimination.

334
00:19:53,940 --> 00:19:57,860
That is present now.

335
00:19:57,860 --> 00:19:59,660
It's not one of these future things we need to worry about.

336
00:19:59,660 --> 00:20:01,340
It's happening now.

337
00:20:01,340 --> 00:20:06,140
But it is something that I think is relatively easy to fix compared with all the other things.

338
00:20:06,140 --> 00:20:10,140
If you make your target, not be to have a completely unbiased system, but just to have

339
00:20:10,140 --> 00:20:14,580
a system that's significantly less biased than what it's replacing.

340
00:20:14,580 --> 00:20:18,580
So at present, you have old white men deciding whether the young black women should get mortgages.

341
00:20:18,580 --> 00:20:23,340
And if you just train on that data, you get a system that's equally biased.

342
00:20:23,340 --> 00:20:25,700
But you can analyze the bias.

343
00:20:25,700 --> 00:20:27,980
You can see how it's biased because it won't change its behavior.

344
00:20:27,980 --> 00:20:29,940
You can freeze it and then analyze it.

345
00:20:29,940 --> 00:20:32,740
And that should make it easier to correct for bias.

346
00:20:32,740 --> 00:20:35,020
So okay, that's bias and discrimination.

347
00:20:35,060 --> 00:20:36,540
I think we can do a lot about that.

348
00:20:36,540 --> 00:20:40,420
And I think it's important we do a lot about that, but it's doable.

349
00:20:40,420 --> 00:20:42,620
The next one is battle robots.

350
00:20:42,620 --> 00:20:48,380
That I'm really worried about because defense departments are going to build them.

351
00:20:48,380 --> 00:20:52,900
And I don't see how you could stop them doing it.

352
00:20:52,900 --> 00:20:56,220
Something like a Geneva Convention would be great.

353
00:20:56,220 --> 00:20:59,740
But those never happened until after they've been used with chemical weapons.

354
00:20:59,740 --> 00:21:03,820
It didn't happen until after the First World War, I believe.

355
00:21:03,820 --> 00:21:07,260
And so I think what may happen is people will use battle robots.

356
00:21:07,260 --> 00:21:09,740
We'll see just how absolutely awful they are.

357
00:21:09,740 --> 00:21:13,660
And then maybe we can get an international convention to prohibit them.

358
00:21:13,660 --> 00:21:14,660
So that's two.

359
00:21:14,660 --> 00:21:20,380
I mean, you could also tell the people building the AI to not sell their equipment to the military.

360
00:21:20,380 --> 00:21:21,340
You could try.

361
00:21:21,340 --> 00:21:21,740
Try.

362
00:21:21,740 --> 00:21:23,620
Okay, number three.

363
00:21:23,620 --> 00:21:27,340
The military has lots of money.

364
00:21:27,340 --> 00:21:31,420
Okay, number three, there's joblessness.

365
00:21:31,420 --> 00:21:36,860
You could try and do stuff to make sure the increase in productivity, some of that extra

366
00:21:36,860 --> 00:21:41,260
revenue that comes from the increase in productivity is going goes to helping the people who make

367
00:21:41,260 --> 00:21:42,260
jobless.

368
00:21:42,260 --> 00:21:47,140
If it turns out that there aren't as many jobs created as destroyed.

369
00:21:47,140 --> 00:21:49,020
That's a question of social policy.

370
00:21:49,020 --> 00:21:52,780
And what you really need for that is socialism.

371
00:21:52,780 --> 00:21:58,340
We're in Canada, so you can say socialism.

372
00:21:58,340 --> 00:22:05,500
Number four would be the warring echo chambers due to the big companies wanting you to click

373
00:22:05,500 --> 00:22:07,380
on things that make you indignant.

374
00:22:07,380 --> 00:22:10,420
And so giving you things that are more and more extreme.

375
00:22:10,420 --> 00:22:15,220
And so you end up in this echo chamber where you believe these crazy conspiracy theorists,

376
00:22:15,220 --> 00:22:21,940
if you're in the other echo chamber, or you believe the truth, if you're in my echo chamber.

377
00:22:21,940 --> 00:22:25,700
That's partly to do with the policies of the companies and maybe something could be done

378
00:22:25,700 --> 00:22:26,700
about that.

379
00:22:27,060 --> 00:22:31,820
But that would mean that is a problem that exists if it existed prior to large language

380
00:22:31,820 --> 00:22:32,820
models.

381
00:22:32,820 --> 00:22:35,980
And in fact, large language models could reverse it.

382
00:22:35,980 --> 00:22:36,980
Maybe.

383
00:22:36,980 --> 00:22:40,780
I mean, it's an open question of whether they can make it better or whether they make that

384
00:22:40,780 --> 00:22:41,780
problem worse.

385
00:22:41,780 --> 00:22:46,820
Yeah, it's a problem to do with AI, but it's not to do with large language models.

386
00:22:46,820 --> 00:22:49,940
It's a problem to do with AI in the sense that there's an algorithm using AI trained

387
00:22:49,940 --> 00:22:52,420
on our emotions that then pushes us in those directions.

388
00:22:52,420 --> 00:22:53,420
Okay.

389
00:22:53,420 --> 00:22:54,420
All right.

390
00:22:54,420 --> 00:22:55,420
So that's number four.

391
00:22:55,420 --> 00:23:00,260
Because the existential risk, which is the one I decided to talk about because a lot

392
00:23:00,260 --> 00:23:01,820
of people think is a joke.

393
00:23:01,820 --> 00:23:02,820
Right.

394
00:23:02,820 --> 00:23:08,860
So there was an editorial in Nature yesterday where they basically said, I'm fear-mongering

395
00:23:08,860 --> 00:23:14,100
about the existential risk is distracting attention from the actual risks.

396
00:23:14,100 --> 00:23:18,380
So they compared existential risk with actual risks, implying the existential risk wasn't

397
00:23:18,380 --> 00:23:21,260
actual.

398
00:23:21,260 --> 00:23:25,180
I think it's important that people understand it's not just science fiction.

399
00:23:25,180 --> 00:23:27,300
It's not just fear-mongering.

400
00:23:27,300 --> 00:23:31,780
It is a real risk that we need to think about, and we need to figure out in advance how to

401
00:23:31,780 --> 00:23:32,940
deal with it.

402
00:23:32,940 --> 00:23:38,060
So that's five, and there's one more, and I can't think what it is.

403
00:23:38,060 --> 00:23:40,380
How do you have a list that doesn't end on existential risk?

404
00:23:40,380 --> 00:23:41,980
I feel like that should be the end of the list.

405
00:23:41,980 --> 00:23:45,860
No, that was the end, but I thought if I talked about existential risk, I'd be able to remember

406
00:23:45,860 --> 00:23:48,260
the missing one while I talk about it, but I couldn't.

407
00:23:48,260 --> 00:23:49,260
All right.

408
00:23:49,260 --> 00:23:50,460
Well, let's talk about existential risk.

409
00:23:50,460 --> 00:23:54,940
What exactly explain exactly existential risk, how it happens?

410
00:23:54,940 --> 00:24:00,980
Or explain, as best you can imagine it, what it is that goes wrong that leads us to extinction

411
00:24:00,980 --> 00:24:03,180
or disappearance of humanity as a species?

412
00:24:03,180 --> 00:24:04,180
Okay.

413
00:24:04,180 --> 00:24:09,500
At a very general level, if you've got something a lot smarter than you that's very good at

414
00:24:09,500 --> 00:24:14,340
manipulating people, just at a very general level, are you confident people will stay

415
00:24:14,340 --> 00:24:16,340
in charge?

416
00:24:16,340 --> 00:24:20,980
And then you can go into specific scenarios for how people might lose control, even though

417
00:24:20,980 --> 00:24:24,340
they're the people creating this and giving it its goals.

418
00:24:24,340 --> 00:24:29,660
And one very obvious scenario is if you're given a goal and you want to be good at achieving

419
00:24:29,660 --> 00:24:34,780
it, what you need is as much control as possible.

420
00:24:34,780 --> 00:24:40,740
So for example, if I'm sitting in a boring seminar and I see a little dot of light on

421
00:24:40,740 --> 00:24:47,420
the ceiling, and then suddenly I notice that when I move, that dot of light moves, I realize

422
00:24:47,420 --> 00:24:51,700
this is the reflection from my watch, the sun is bouncing off my watch.

423
00:24:51,700 --> 00:24:55,340
And so the next thing I do is I don't start listening to the boring seminar again.

424
00:24:55,340 --> 00:24:58,780
I immediately try and figure out how to make it go this way and how to make it go that

425
00:24:58,780 --> 00:24:59,780
way.

426
00:24:59,780 --> 00:25:02,620
And once I got control of it, then maybe I'll listen to the seminar again.

427
00:25:02,620 --> 00:25:07,140
We have a very strong built in urge to get control and it's very sensible because the

428
00:25:07,140 --> 00:25:10,740
more control you get, the easier it is to achieve things.

429
00:25:10,740 --> 00:25:13,500
And I think AI will be able to derive that too.

430
00:25:13,500 --> 00:25:16,380
It's good to get control so you can achieve other goals.

431
00:25:16,580 --> 00:25:24,140
Wait, so you actually believe that getting control will be an innate feature of something

432
00:25:24,140 --> 00:25:26,100
that the AIs are trained on us, right?

433
00:25:26,100 --> 00:25:27,100
They act like us.

434
00:25:27,100 --> 00:25:31,500
They think like us because the neural architecture makes them like our human brains and because

435
00:25:31,500 --> 00:25:33,100
they're trained on all of our outputs.

436
00:25:33,100 --> 00:25:37,780
So you actually think that getting control of humans will be something that the AI is

437
00:25:37,780 --> 00:25:39,500
almost aspire to?

438
00:25:39,500 --> 00:25:44,940
No, I think they'll derive it as a way of achieving other goals.

439
00:25:44,940 --> 00:25:46,300
I think in us it's innate.

440
00:25:46,300 --> 00:25:52,620
I think I'm very dubious about saying things are really innate, but I think the desire

441
00:25:52,620 --> 00:25:58,740
to understand how things work is a very sensible desire to have and I think we have that.

442
00:25:58,740 --> 00:26:04,020
So we have that and then AIs will develop an ability to manipulate us and control us

443
00:26:04,020 --> 00:26:08,460
in a way that we can't respond to, right?

444
00:26:08,460 --> 00:26:14,340
That the manipulative AIs and even though good people will be able to use equally powerful

445
00:26:14,380 --> 00:26:18,540
AIs to counter these bad ones, you believe that we still could have an existential crisis?

446
00:26:18,540 --> 00:26:19,740
Yes.

447
00:26:19,740 --> 00:26:20,740
It's not clear to me.

448
00:26:20,740 --> 00:26:25,380
I mean, that makes the argument that the good people will have more resources than the

449
00:26:25,380 --> 00:26:27,380
bad people.

450
00:26:27,380 --> 00:26:33,300
I'm not sure about that and that good AI is going to be more powerful than bad AI and

451
00:26:33,300 --> 00:26:35,860
good AI is going to be able to regulate bad AI.

452
00:26:35,860 --> 00:26:40,660
And we have a situation like that at present where you have people using AI to create

453
00:26:40,700 --> 00:26:46,460
spam and you have people like Google using AI to filter out the spam and at present Google

454
00:26:46,460 --> 00:26:51,260
has more resources than the defenders of beating the attackers, but I don't see that it'll

455
00:26:51,260 --> 00:26:52,260
always be like that.

456
00:26:52,260 --> 00:26:55,060
I mean, even in cyber warfare where you have moments where it seems like the criminals

457
00:26:55,060 --> 00:26:58,060
are winning and sometimes where it seems like the defenders are winning.

458
00:26:58,060 --> 00:27:02,100
So you believe that there will be a battle like that over control of humans by super

459
00:27:02,100 --> 00:27:03,380
intelligent artificial intelligence?

460
00:27:03,380 --> 00:27:04,380
It may well be, yes.

461
00:27:04,380 --> 00:27:09,380
And I'm not convinced that good AI that's trying to stop bad AI getting control will

462
00:27:09,380 --> 00:27:10,380
win.

463
00:27:10,420 --> 00:27:11,420
Okay.

464
00:27:11,420 --> 00:27:13,340
So, all right.

465
00:27:13,340 --> 00:27:18,980
So before this existential risk happened, before bad AI does this, we have a lot of extremely

466
00:27:18,980 --> 00:27:22,100
smart people building a lot of extremely important things.

467
00:27:22,100 --> 00:27:27,340
What exactly can they do to most help limit this risk?

468
00:27:27,340 --> 00:27:34,580
So one thing you can do is before the AI gets super intelligent, you can do empirical work

469
00:27:34,580 --> 00:27:39,660
into how it goes wrong, how it tries to get control, whether it tries to get control.

470
00:27:39,660 --> 00:27:41,420
You don't know whether it would.

471
00:27:41,420 --> 00:27:46,140
But before it's smarter than us, I think the people developing it should be encouraged

472
00:27:46,140 --> 00:27:51,980
to put a lot of work into understanding how it might go wrong, understanding how it might

473
00:27:51,980 --> 00:27:54,080
try and take control away.

474
00:27:54,080 --> 00:27:59,260
And I think the government could maybe encourage the big companies developing it to put comparable

475
00:27:59,260 --> 00:28:04,380
resources, maybe not equal resources, but right now there's 99 very smart people trying

476
00:28:04,380 --> 00:28:08,340
to make it better and one very smart person trying to figure out how to stop it taking

477
00:28:08,380 --> 00:28:09,380
over.

478
00:28:09,380 --> 00:28:12,060
And maybe you want it more balanced.

479
00:28:12,060 --> 00:28:17,140
And so this is in some ways your role right now, the reason why you've left Google on

480
00:28:17,140 --> 00:28:21,980
good terms, but you want to be able to speak out and help participate in this conversation

481
00:28:21,980 --> 00:28:24,700
so more people can join that one and not the 99.

482
00:28:24,700 --> 00:28:25,700
Yeah.

483
00:28:25,700 --> 00:28:28,700
I would say it's very important for smart people to be working on that.

484
00:28:28,700 --> 00:28:32,740
But I'd also say it's very important not to think this is the only risk.

485
00:28:32,740 --> 00:28:34,420
There's all these other risks.

486
00:28:34,420 --> 00:28:38,700
And I've remembered the last one, which is fake news.

487
00:28:38,700 --> 00:28:43,660
So it's very important to try, for example, to mark everything that's fake as fake, whether

488
00:28:43,660 --> 00:28:47,180
we can do that technically, I don't know, but it'd be great if we could.

489
00:28:47,180 --> 00:28:48,740
Governments do it with counterfeit money.

490
00:28:48,740 --> 00:28:54,860
They won't allow counterfeit money because that reflects on their sort of central interest.

491
00:28:54,860 --> 00:28:58,460
They should try and do it with AI-generated stuff.

492
00:28:58,460 --> 00:28:59,940
I don't know whether they can.

493
00:28:59,940 --> 00:29:00,940
All right.

494
00:29:00,940 --> 00:29:01,940
So we're out of time.

495
00:29:01,940 --> 00:29:06,500
Give one specific to-do, something to read, a thought experiment, one thing to leave

496
00:29:06,500 --> 00:29:10,340
the audience with so they can go out here and think, OK, I'm going to do this.

497
00:29:10,340 --> 00:29:15,140
AI is the most powerful thing we've invented, and perhaps in our lifetimes, and I'm going

498
00:29:15,140 --> 00:29:20,180
to make it better to make it more likely it's a force for good in the next generation.

499
00:29:20,180 --> 00:29:22,460
So how could they make it more likely be a force for good?

500
00:29:22,460 --> 00:29:23,460
Yes.

501
00:29:23,460 --> 00:29:26,820
One final thought for everyone here.

502
00:29:26,820 --> 00:29:32,340
I actually don't have a plan for how to make it more likely to be good than bad, sorry.

503
00:29:32,340 --> 00:29:37,140
I think it's great that it's being developed because we didn't get to mention the huge

504
00:29:37,140 --> 00:29:42,100
numbers of good uses of it, like in medicine, in climate change, and so on.

505
00:29:42,100 --> 00:29:48,180
So I think progress in AI is inevitable and it's probably good, but we seriously ought

506
00:29:48,180 --> 00:29:52,380
to worry about mitigating all the bad side effects of it and worry about the existential

507
00:29:52,380 --> 00:29:53,380
threat.

508
00:29:53,380 --> 00:29:54,380
All right.

509
00:29:54,380 --> 00:29:57,980
Thank you everyone, an incredibly thoughtful, inspiring, interesting, phenomenally smart.

510
00:29:57,980 --> 00:29:58,980
Thank you to Jeffrey Hinton.

511
00:29:58,980 --> 00:29:59,980
Thank you.

512
00:29:59,980 --> 00:30:00,980
Thank you, Jeff.

513
00:30:00,980 --> 00:30:01,480
So great.

