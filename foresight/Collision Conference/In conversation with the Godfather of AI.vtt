WEBVTT

00:00.000 --> 00:13.560
What an incredible pleasure to be here with Jeffrey Hinton, one of the great minds and

00:13.560 --> 00:15.160
one of the great issues of our time.

00:15.160 --> 00:19.640
A man who helped create artificial intelligence was at the center of nearly every revolution

00:19.640 --> 00:25.160
in it and now has become perhaps the most articulate critic of where we're going.

00:25.160 --> 00:27.520
So an honor to be on stage with you.

00:27.520 --> 00:28.520
Thank you.

00:28.560 --> 00:31.880
During the moniker Godfather of AI, one of the things that AI has traditionally had

00:31.880 --> 00:36.320
problems with is humor, I asked AI if it could come up with a joke about the Godfather of

00:36.320 --> 00:39.840
AI and it actually wasn't that bad.

00:39.840 --> 00:44.240
It said he gave AI an offer it couldn't refuse, neural networks.

00:44.240 --> 00:45.240
It's not bad.

00:45.240 --> 00:46.240
Okay, that's not bad.

00:46.240 --> 00:47.240
It's good for AI.

00:47.240 --> 00:48.240
So let's begin with that.

00:48.240 --> 00:52.160
What I want to do in this conversation is very briefly step a little back into your foundational

00:52.160 --> 00:57.040
work, then go to where we are today and then talk about the future.

00:57.040 --> 01:01.280
So when you're building and you're designing neural networks and you're building computer

01:01.280 --> 01:06.400
systems that work like the human brain and that learn like the human brain and everybody

01:06.400 --> 01:09.440
else is saying, Jeff, this is not going to work.

01:09.440 --> 01:15.680
You push ahead and do you push ahead because you know that this is the best way to train

01:15.680 --> 01:21.000
computer systems or you do it for more spiritual reasons that you want to make a machine that

01:21.000 --> 01:22.840
is like us?

01:22.840 --> 01:28.280
I do it because the brain has to work somehow and it sure as hell doesn't work by manipulating

01:28.280 --> 01:34.440
symbolic expressions explicitly and so something like neural nets had to work.

01:34.440 --> 01:38.360
Also von Neumann and Turing believed that so that's a good start.

01:38.360 --> 01:42.200
So you're doing it because you think it's the best way forward?

01:42.200 --> 01:44.640
Yes, in the long run the best way forward.

01:44.640 --> 01:48.920
Because that decision has profound effects down the line.

01:49.320 --> 01:50.640
Okay, so you do that.

01:50.640 --> 01:55.640
You start building neural nets, you push forward and they become better than humans

01:55.640 --> 01:57.880
at certain limited tasks, right?

01:57.880 --> 02:04.520
At image recognition, at translation, some chemical work.

02:04.520 --> 02:10.800
I interviewed you in 2019 at Google I.O. and you said that it would be a long time before

02:10.800 --> 02:15.200
they could match us in reasoning and that's the big change that's happened over the last

02:15.200 --> 02:17.040
four years, right?

02:17.040 --> 02:19.600
They still can't match us but they're getting close.

02:19.600 --> 02:23.120
And how close are they getting and why?

02:23.120 --> 02:27.720
It's the big language models that are getting close and I don't really understand why they

02:27.720 --> 02:31.560
can do it but they can do little bits of reasoning.

02:31.560 --> 02:36.840
So my favorite example is I asked GPT-4, a puzzle that was given to me by a symbolic

02:36.840 --> 02:40.080
AI guy who thought it wouldn't be able to do it.

02:40.080 --> 02:45.800
I made the puzzle more difficult than it could still do it and the puzzle was the rooms in

02:45.800 --> 02:50.400
my house are painted blue or yellow or white.

02:50.400 --> 02:53.360
Yellow paint fades to white within a year.

02:53.360 --> 02:55.760
In two years' time I want them all to be white.

02:55.760 --> 02:58.440
What should I do and why?

02:58.440 --> 03:03.560
And it says you should paint the blue rooms white and then it says you should do that

03:03.560 --> 03:08.080
because blue won't fade to white and it says you don't need to paint the yellow rooms because

03:08.080 --> 03:09.920
they will fade to white.

03:09.920 --> 03:14.640
So it knew what I should do and it knew why and I was surprised that it could do that

03:14.640 --> 03:16.040
much reasoning already.

03:16.040 --> 03:21.040
And it's kind of an amazing example because when people critique these systems or they

03:21.040 --> 03:24.000
say they're not going to do much, they say they're mad libs, they're just word completion

03:24.000 --> 03:25.800
but that is not word completion.

03:25.800 --> 03:27.800
To you is that thinking?

03:27.800 --> 03:35.280
Yeah, that's thinking and when people say it's just autocomplete, a lot goes on in

03:35.280 --> 03:37.640
that word just autocomplete.

03:37.640 --> 03:42.400
If you think what it takes to predict the next word, you have to understand what's been

03:42.400 --> 03:45.360
said to be really good at predicting the next word.

03:45.360 --> 03:49.600
So people say it's just autocomplete or it's just statistics.

03:49.600 --> 03:56.360
Now there's a sense in which it is just statistics, but in that sense everything's just statistics.

03:56.360 --> 04:00.840
It's not the sense most people think of statistics as it keeps the counts of how many times this

04:00.840 --> 04:04.080
combination of words occurred and how many times that combination.

04:04.080 --> 04:05.080
It's not like that at all.

04:05.080 --> 04:10.040
It's inventing features and interactions between features to explain what comes next.

04:10.040 --> 04:15.560
So if it's just statistics and everything is just statistics, is there anything that

04:15.560 --> 04:17.840
we can do?

04:17.840 --> 04:20.760
Obviously it's not humor, maybe it's not reasoning.

04:20.760 --> 04:25.560
Is there anything that we can do that a sufficiently well-trained large language model with a sufficient

04:25.560 --> 04:29.840
number of parameters and a sufficient amount of compute could not do in the future?

04:29.840 --> 04:35.960
If the model is also trained on vision and picking things up and so on, then no.

04:35.960 --> 04:40.400
But is there anything that we can think of and any way we can think in any cognitive

04:40.400 --> 04:44.080
process that the machines will not be able to replicate?

04:44.080 --> 04:48.440
We're just a machine, we're a wonderful, incredibly complicated machine, but we're

04:48.440 --> 04:52.680
just a big neural net and there's no reason why an artificial neural net shouldn't be

04:52.680 --> 04:54.480
able to do everything we can do.

04:54.480 --> 04:59.280
Are we a big neural net that is more efficient than these new neural nets we're building

04:59.280 --> 05:01.480
or are we less efficient?

05:01.480 --> 05:06.000
It depends whether you're talking about speed of acquiring knowledge and how much knowledge

05:06.000 --> 05:09.960
you can acquire or whether you're talking about energy consumption.

05:09.960 --> 05:12.920
So in energy consumption, we're much more efficient.

05:12.920 --> 05:17.080
We're like 30 watts and one of these big language models, when you're training it, you train

05:17.080 --> 05:20.760
many copies of it each looking at different parts of the data.

05:20.760 --> 05:22.920
So it's more like a megawatt.

05:22.920 --> 05:27.520
So it's much more expensive in terms of energy, but all these copies can be learning different

05:27.520 --> 05:29.480
things from different parts of the data.

05:29.480 --> 05:33.360
So it's much more efficient in terms of acquiring knowledge from data.

05:33.360 --> 05:37.480
And it becomes only more efficient because each system can train each next system?

05:37.480 --> 05:38.480
Yes.

05:38.480 --> 05:39.760
So let's get to your critique.

05:39.760 --> 05:45.360
So the best summarization of your critique came from a conference at the Milken Institute

05:45.360 --> 05:51.960
about a month ago and it was Snoop Dogg and he said, I heard the old dude who created

05:51.960 --> 05:57.160
AI saying this is not safe because the AI's got their own mind and those motherfuckers

05:57.160 --> 06:01.040
going to start doing their own shit.

06:01.040 --> 06:02.040
Is that accurate?

06:02.040 --> 06:05.280
Is that an accurate summarization?

06:05.280 --> 06:12.000
They probably didn't have mothers.

06:12.000 --> 06:14.760
But the rest of what Dr. Dogg said is correct.

06:14.760 --> 06:15.760
Hang on.

06:15.760 --> 06:16.760
Yes.

06:16.760 --> 06:17.760
All right.

06:17.760 --> 06:23.520
So explain what you mean or what he means and how it applies to what you mean when they're

06:23.520 --> 06:25.440
going to start doing their own shit.

06:25.440 --> 06:26.440
What does that mean to you?

06:26.560 --> 06:27.560
Okay.

06:27.560 --> 06:31.680
So first I have to emphasize we're entering a period of huge uncertainty.

06:31.680 --> 06:33.440
Nobody really knows what's going to happen.

06:33.440 --> 06:36.960
And people whose opinion I respect have very different beliefs from me.

06:36.960 --> 06:39.360
Like Jan LeCun thinks everything's going to be fine.

06:39.360 --> 06:40.360
They're just going to help us.

06:40.360 --> 06:41.960
It's all going to be wonderful.

06:41.960 --> 06:46.160
But I think we have to take seriously the possibility that if they get to be smarter

06:46.160 --> 06:51.400
than us, which seems quite likely, and they have goals of their own, which seems quite

06:51.400 --> 06:55.120
likely, they may well develop the goal of taking control.

06:55.320 --> 06:57.800
And if they do that, we're in trouble.

06:57.800 --> 06:58.800
So okay.

06:58.800 --> 07:02.400
So let's let's go back to that in a second, but let's take Jan's position.

07:02.400 --> 07:06.040
So Jan LeCun was also one of the people who won the Turing Award and is also called the

07:06.040 --> 07:07.560
Godfather of AI.

07:07.560 --> 07:11.480
And I was recently interviewing him and he made the case.

07:11.480 --> 07:16.200
He said, look, technologies, all technologies can be used for good or ill, but some technologies

07:16.200 --> 07:18.000
have more of an inherent goodness.

07:18.000 --> 07:23.760
And AI has been built by humans, by good humans for good purposes.

07:23.760 --> 07:27.240
It's been trained on good books and good texts.

07:27.240 --> 07:30.680
It will have a bias towards good in the future.

07:30.680 --> 07:32.760
Do you believe that or not?

07:32.760 --> 07:36.560
I think AI that's been trained by good people will have a bias towards good.

07:36.560 --> 07:41.400
And AI that's been trained by bad people like Putin or somebody like that will have a bias

07:41.400 --> 07:42.740
towards bad.

07:42.740 --> 07:46.040
We know they're going to make battle robots.

07:46.040 --> 07:49.520
They're busy doing it in many different defense departments.

07:49.520 --> 07:54.120
So they're not going to necessarily be good since their primary purpose is going to be

07:54.120 --> 07:56.360
to kill people.

07:56.360 --> 08:03.640
So you believe that the risks of the bad uses of AI are whether they're more or less than

08:03.640 --> 08:08.240
the good uses of AI are so substantial, they deserve a lot of our thought right now.

08:08.240 --> 08:09.240
Certainly.

08:09.240 --> 08:10.240
Yes.

08:10.240 --> 08:12.240
For lethal autonomous weapons, they deserve a lot of our thought.

08:12.240 --> 08:15.840
Well, let's, okay, let's stick on lethal autonomous weapons because one of the things

08:15.840 --> 08:21.480
in this argument is that you are one of the few people who is really speaking about this

08:21.480 --> 08:23.880
as a risk, a real risk.

08:23.880 --> 08:32.440
Explain your hypothesis about why super powerful AI combined with the military could actually

08:32.440 --> 08:35.280
lead to more and more warfare.

08:35.280 --> 08:36.520
Okay.

08:36.520 --> 08:41.800
I don't actually want to answer that question.

08:41.800 --> 08:43.120
There's a separate question.

08:43.120 --> 08:48.320
Even if the AI isn't super intelligent, if defense departments use it for making battle

08:48.320 --> 08:51.520
robots, it's going to be very nasty, scary stuff.

08:51.520 --> 08:55.400
And it's going to lead, even if it's not super intelligent, and even if it doesn't have its

08:55.400 --> 09:00.640
own intentions, it just does what Putin tells it to.

09:00.640 --> 09:06.000
It's going to make it much easier, for example, for rich countries to invade poor countries.

09:06.000 --> 09:11.240
A present, there's a barrier to invading poor countries willy-nilly, which is you get dead

09:11.240 --> 09:13.420
citizens coming home.

09:13.420 --> 09:15.720
If they just dead battle robots, that's just great.

09:15.720 --> 09:18.960
The military industrial complex would love that.

09:18.960 --> 09:23.320
So you think that because, I mean, it's sort of a similar argument that people make with

09:23.320 --> 09:24.320
drones.

09:24.320 --> 09:26.800
If you can send a drone and you don't have to send an airplane with a pilot, you're more

09:26.800 --> 09:30.080
likely to send the drone, therefore you're more likely to attack.

09:30.080 --> 09:34.000
If you have a battle robot, it's that same thing squared.

09:34.000 --> 09:35.280
And that's your concern.

09:35.280 --> 09:37.120
That's my main concern with battle robots.

09:37.120 --> 09:42.040
It's a separate concern from what happens with super intelligent systems taking over

09:42.040 --> 09:44.360
for their own purposes.

09:44.360 --> 09:48.200
Before we get to super intelligent systems, let's talk about some of your other concerns.

09:48.200 --> 09:53.200
So in the litany of things that you're worried about, you obviously we have battle robots

09:53.200 --> 09:56.560
as one, you're also quite worried about inequality.

09:56.560 --> 09:58.120
Tell me more about this.

09:58.120 --> 10:03.360
So it's fairly clear, it's not certain, but it's fairly clear that these big language

10:03.360 --> 10:06.920
models will cause a big increase in productivity.

10:06.920 --> 10:11.240
So there's someone I know who answers letters of complaint for a health service.

10:11.240 --> 10:16.200
And he used to write these letters himself and now he just gets chat GPT to write the

10:16.200 --> 10:20.640
letters and it takes one-fifth of the amount of time to answer a complaint.

10:20.640 --> 10:26.840
So he can do five times as much work and so there are only five times fewer of him.

10:26.840 --> 10:28.720
Or maybe they'll just answer a lot more letters.

10:28.720 --> 10:29.960
Or they'll answer more letters, right?

10:29.960 --> 10:33.480
Or maybe they'll have more people because they'll be so efficient, right?

10:33.480 --> 10:35.680
More productivity leads to more getting more done.

10:35.680 --> 10:37.880
This is an unanswered question.

10:37.880 --> 10:42.440
But what we expect in the kind of society we live in is that if you get a big increase

10:42.440 --> 10:47.440
in productivity like that, the wealth isn't going to go to the people who are doing the

10:47.440 --> 10:51.440
work or the people who get unemployed, it's going to go to making the rich richer and

10:51.440 --> 10:52.440
the poor poorer.

10:52.440 --> 10:54.440
And that's very bad for society.

10:54.440 --> 10:57.880
Definitionally, or you think there's some feature of AI that will lead to that?

10:57.880 --> 11:02.280
No, it's not to do with AI, it's just what happens when you get an increase in productivity,

11:02.280 --> 11:05.120
particularly in a society that doesn't have strong unions.

11:05.120 --> 11:09.680
But now there are many economists who would take a different position and say that over

11:09.680 --> 11:14.800
time, and if you were to look at technology, right, we went from horses and horses and

11:14.800 --> 11:18.160
buggies and the horses and buggies went away and then we had cars and oh my gosh, the people

11:18.160 --> 11:22.920
who drove the horses lost their jobs and ATMs came along and suddenly bank tellers no longer

11:22.920 --> 11:23.920
need to do that.

11:23.920 --> 11:27.040
But we now employ many more bank tellers than we used to and we have many more people driving

11:27.040 --> 11:29.200
Ubers than we had people driving horses.

11:29.200 --> 11:35.080
So the argument what an economist would make to this would be, yes, there will be chair

11:35.080 --> 11:40.200
and there will be fewer people answering those letters, but there'll be many more higher

11:40.200 --> 11:41.640
cognitive things that will be done.

11:41.640 --> 11:43.920
How do you respond to that?

11:43.920 --> 11:48.640
I think the first thing I'd say is a loaf of bread used to cost a penny, then they invented

11:48.640 --> 11:52.680
economics and now it costs five dollars.

11:52.680 --> 11:56.880
So I don't entirely trust what economists say, particularly when they're dealing with

11:56.880 --> 12:00.240
a new situation that's never happened before.

12:00.240 --> 12:04.360
And superintelligence would be a new situation that never happened before, but even these

12:04.360 --> 12:10.400
big chatbots that are just replacing people whose job involves producing text, that's

12:10.400 --> 12:15.920
never happened before and I'm not sure how they can confidently predict that more jobs

12:15.920 --> 12:18.440
will be created than the number of jobs lost.

12:18.440 --> 12:22.840
I just have a little side note that in the green room, I introduced Jeff to, I have two

12:22.840 --> 12:27.760
of my three children are here, Alice and Zachary, they're somewhere out here, and he said to

12:27.760 --> 12:30.240
Alice, he said, are you going to go into media?

12:30.240 --> 12:33.280
And then he said, well, I'm not sure media will exist.

12:33.280 --> 12:35.000
And then Alice was asking, what should I do?

12:35.000 --> 12:36.000
And you said?

12:36.000 --> 12:37.000
Plumbing.

12:37.000 --> 12:38.000
Yes.

12:38.000 --> 12:39.000
Now explain.

12:39.000 --> 12:43.440
I mean, we have a number of plumbing problems at our house, it'd be wonderful if they were

12:43.440 --> 12:45.720
able to put in a new sink.

12:45.720 --> 12:50.120
Explain what jobs, a lot of young people out here, not just my children, but thinking about

12:50.120 --> 12:54.440
what careers to go into, what are the careers they should be looking at, what are the attributes

12:54.440 --> 12:55.440
of them?

12:55.440 --> 12:58.960
I'll give you a little story about being a carpenter.

12:58.960 --> 13:04.600
If you're a carpenter, it's fun making furniture, but it's a complete dead loss because machines

13:04.600 --> 13:06.360
can make furniture.

13:06.360 --> 13:11.760
If you're a carpenter, what you're good for is repairing furniture or fitting things into

13:11.760 --> 13:17.280
awkward spaces in old houses, making shelves in things that aren't quite square.

13:17.280 --> 13:22.440
So the jobs that are going to survive AI for a long time are jobs where you have to be

13:22.440 --> 13:27.460
very adaptable and physically skilled and plumbing is that kind of a job.

13:27.460 --> 13:31.660
How does manual dexterity is hard for a machine to replicate?

13:31.660 --> 13:37.020
It's still hard, and I think it's going to be longer before they can be really dexterous

13:37.020 --> 13:40.140
and get into awkward spaces.

13:40.140 --> 13:44.020
That's going to take longer than being good at answering text questions.

13:44.020 --> 13:45.020
Should I believe you?

13:45.020 --> 13:47.660
Because when we were on stage four years ago, you said reasoning.

13:47.660 --> 13:51.580
As long as somebody has a job that focuses on reasoning, they'll be able to last a dozen.

13:51.580 --> 13:57.380
Isn't the nature of AI such that we don't actually know where the next incredible

13:57.380 --> 13:58.980
improvement in performance will come?

13:58.980 --> 14:00.500
Maybe it will come in manual dexterity.

14:00.500 --> 14:02.500
Yeah, it's possible.

14:02.500 --> 14:05.340
So actually, let me ask you a question about that.

14:05.340 --> 14:10.540
So do you think when we look at AI and we look at the next five years of AI, the most

14:10.540 --> 14:14.940
impactful improvements we'll see will be in large language models and related to large

14:14.940 --> 14:16.140
language models?

14:16.140 --> 14:18.620
Or do you think it will be in something else?

14:18.620 --> 14:22.020
I think it'll probably be in multimodal large models.

14:22.020 --> 14:26.740
So they won't just be language models, they'll be doing vision.

14:26.780 --> 14:28.540
Actually, they'll be analyzing video.

14:28.540 --> 14:32.620
So they were able to train on all of the YouTube videos, for example.

14:32.620 --> 14:38.180
And you can understand a lot from things other than language.

14:38.180 --> 14:42.260
And when you do that, you need less language to reach the same performance.

14:42.260 --> 14:45.540
So the idea that they're going to be saturated because they've already used all the language

14:45.540 --> 14:49.100
there is, all the language is easy to get hold of.

14:49.100 --> 14:52.020
That's less of a concern if they're also using lots of other modalities.

14:52.020 --> 14:56.500
I mean, this gets at one of the, another argument that Jan, your fellow Godfather of AI makes

14:56.540 --> 14:58.620
is that language is so limited, right?

14:58.620 --> 15:00.940
There's so much information that we're conveying just beyond the word.

15:00.940 --> 15:02.900
In fact, I'm gesturing like mad, right?

15:02.900 --> 15:06.380
Which conveys some of the information as well as the lighting and all this.

15:06.380 --> 15:08.900
So your view is that may be true.

15:08.900 --> 15:13.780
Language is a limited vector for information, but soon it will be combined with other vectors.

15:13.780 --> 15:15.300
Absolutely.

15:15.300 --> 15:19.900
It's amazing what you can learn from language alone, but you're much better off learning

15:19.900 --> 15:21.020
from many modalities.

15:21.020 --> 15:23.780
Small children don't just learn from language alone.

15:23.780 --> 15:24.340
Right.

15:24.380 --> 15:31.180
So if you were, if your principal role right now was still researching AI, finding the

15:31.180 --> 15:38.580
next big thing, you would be doing multi-modal AI and trying to attach, say, visual AI systems

15:38.580 --> 15:40.620
to text-based AI systems?

15:40.620 --> 15:43.380
Yes, which is what they're doing now at Google.

15:43.380 --> 15:49.100
Google is making a system called Gemini, but fortunately, Demisabis talked about it a few

15:49.100 --> 15:53.220
days ago, and that's a multi-modal AI.

15:53.260 --> 15:55.340
Well, let me talk about actually something else at Google.

15:55.340 --> 16:01.500
So while you were there, Google invented the transformer network or invented the transformer

16:01.500 --> 16:05.580
architecture, generative pre-trained transformers.

16:05.580 --> 16:12.420
When did you realize that that would be so central and so important?

16:12.420 --> 16:17.780
It's interesting to me because it's this paper that comes out in 2017, and when it comes

16:17.780 --> 16:21.860
out, it's not as though firecrackers are left, you know, shot into the sky.

16:21.860 --> 16:25.860
It's six years later, five years later, that we suddenly realized the consequences.

16:25.860 --> 16:29.940
And it's interesting to think, what are the other papers out there that could be the same

16:29.940 --> 16:30.780
in five years?

16:30.780 --> 16:35.220
So with transformers, it was really only a couple of years later when Google developed

16:35.220 --> 16:36.540
BERT.

16:36.540 --> 16:41.740
So BERT made it very clear transformers were a huge breakthrough.

16:41.740 --> 16:48.860
I didn't immediately realize what a huge breakthrough they were, and I'm annoyed about that.

16:48.860 --> 16:50.700
It took me a couple of years to realize.

16:50.700 --> 16:55.100
Well, you know, the first time I ever heard the word transformer was talking to you on

16:55.100 --> 17:00.500
stage, and you were talking about transformers versus capsules, and this was right after

17:00.500 --> 17:01.500
it came out.

17:01.500 --> 17:05.460
Let's talk about one of the other critiques about language models and other models, which

17:05.460 --> 17:12.260
is soon, I mean, in fact, probably already they've absorbed all the organic data that

17:12.260 --> 17:13.780
has been created by humans.

17:13.780 --> 17:17.660
If I create an AI model right now, and I train it on the internet, it's trained on a bunch

17:17.660 --> 17:21.940
of stuff, mostly stuff made by humans, but a bunch of stuff made by AI, right?

17:21.940 --> 17:22.940
Yeah.

17:22.940 --> 17:27.740
And you're going to keep training AIs on stuff that has been created by AIs, whether it's

17:27.740 --> 17:31.780
text-based language model or whether it's a multimodal language model.

17:31.780 --> 17:37.300
Will that lead to the inevitable decay and corruption, as some people argue?

17:37.300 --> 17:41.140
Or is that just a thing we have to deal with?

17:41.140 --> 17:45.580
Or is it, as other people in the AI field, the greatest thing for training AIs, and we

17:45.580 --> 17:48.180
should just use synthetic data in AI?

17:48.180 --> 17:49.180
Okay.

17:49.180 --> 17:51.900
I don't actually know the answer to this technically.

17:51.900 --> 17:56.060
I suspect you have to take precautions, so you're not just training on data that you yourself

17:56.060 --> 18:00.540
generated or the some previous version of you generated.

18:00.540 --> 18:04.740
I suspect it's going to be possible to take those precautions, although it would be much

18:04.740 --> 18:08.300
easier if all fake data was marked fake.

18:08.300 --> 18:13.940
There is one example in AI where training on stuff from yourself helps a lot.

18:13.940 --> 18:19.180
So if you don't have much training data or rather you have a lot of unlabeled data and

18:19.180 --> 18:23.660
a small amount of labeled data, you can train a model to predict the labels on the labeled

18:23.660 --> 18:33.020
data and then you take that same model and train it to predict labels for unlabeled data

18:33.020 --> 18:38.180
and whatever it predicts, you tell it you were right.

18:38.180 --> 18:40.580
And that actually makes the model work better.

18:40.580 --> 18:42.820
How on earth does that work?

18:43.060 --> 18:47.860
Because on the whole it tends to be right.

18:47.860 --> 18:48.860
It's complicated.

18:48.860 --> 18:53.620
It's been analyzed much better in many years ago from acoustic modems.

18:53.620 --> 18:55.780
They did the same trick.

18:55.780 --> 19:01.740
So listening to this, I've had this realization on stage, you're a man who's very critical

19:01.740 --> 19:05.780
of where we're going, killer robots, income inequality.

19:05.780 --> 19:07.940
You also sound like somebody who loves this stuff.

19:07.940 --> 19:10.020
Yeah, I love this stuff.

19:10.020 --> 19:13.140
How could you not love making intelligent things?

19:13.140 --> 19:17.660
So let me get to maybe the most important question for the audience and for everyone

19:17.660 --> 19:19.580
here.

19:19.580 --> 19:22.580
We're now at this moment where a lot of people here love this stuff and they want to build

19:22.580 --> 19:25.220
it and they want to experiment.

19:25.220 --> 19:27.260
But we don't want negative consequences.

19:27.260 --> 19:29.020
We don't want increased income inequality.

19:29.020 --> 19:31.380
I don't want media to disappear.

19:31.380 --> 19:37.700
What are the choices and decisions and things we should be working on now to maximize the

19:37.700 --> 19:42.340
good, to maximize the creativity, but to limit the potential harms?

19:42.340 --> 19:47.340
So I think to answer that you have to distinguish many kinds of potential harm.

19:47.340 --> 19:50.740
So I'll distinguish like six of them for you.

19:50.740 --> 19:53.940
There's bias and discrimination.

19:53.940 --> 19:57.860
That is present now.

19:57.860 --> 19:59.660
It's not one of these future things we need to worry about.

19:59.660 --> 20:01.340
It's happening now.

20:01.340 --> 20:06.140
But it is something that I think is relatively easy to fix compared with all the other things.

20:06.140 --> 20:10.140
If you make your target, not be to have a completely unbiased system, but just to have

20:10.140 --> 20:14.580
a system that's significantly less biased than what it's replacing.

20:14.580 --> 20:18.580
So at present, you have old white men deciding whether the young black women should get mortgages.

20:18.580 --> 20:23.340
And if you just train on that data, you get a system that's equally biased.

20:23.340 --> 20:25.700
But you can analyze the bias.

20:25.700 --> 20:27.980
You can see how it's biased because it won't change its behavior.

20:27.980 --> 20:29.940
You can freeze it and then analyze it.

20:29.940 --> 20:32.740
And that should make it easier to correct for bias.

20:32.740 --> 20:35.020
So okay, that's bias and discrimination.

20:35.060 --> 20:36.540
I think we can do a lot about that.

20:36.540 --> 20:40.420
And I think it's important we do a lot about that, but it's doable.

20:40.420 --> 20:42.620
The next one is battle robots.

20:42.620 --> 20:48.380
That I'm really worried about because defense departments are going to build them.

20:48.380 --> 20:52.900
And I don't see how you could stop them doing it.

20:52.900 --> 20:56.220
Something like a Geneva Convention would be great.

20:56.220 --> 20:59.740
But those never happened until after they've been used with chemical weapons.

20:59.740 --> 21:03.820
It didn't happen until after the First World War, I believe.

21:03.820 --> 21:07.260
And so I think what may happen is people will use battle robots.

21:07.260 --> 21:09.740
We'll see just how absolutely awful they are.

21:09.740 --> 21:13.660
And then maybe we can get an international convention to prohibit them.

21:13.660 --> 21:14.660
So that's two.

21:14.660 --> 21:20.380
I mean, you could also tell the people building the AI to not sell their equipment to the military.

21:20.380 --> 21:21.340
You could try.

21:21.340 --> 21:21.740
Try.

21:21.740 --> 21:23.620
Okay, number three.

21:23.620 --> 21:27.340
The military has lots of money.

21:27.340 --> 21:31.420
Okay, number three, there's joblessness.

21:31.420 --> 21:36.860
You could try and do stuff to make sure the increase in productivity, some of that extra

21:36.860 --> 21:41.260
revenue that comes from the increase in productivity is going goes to helping the people who make

21:41.260 --> 21:42.260
jobless.

21:42.260 --> 21:47.140
If it turns out that there aren't as many jobs created as destroyed.

21:47.140 --> 21:49.020
That's a question of social policy.

21:49.020 --> 21:52.780
And what you really need for that is socialism.

21:52.780 --> 21:58.340
We're in Canada, so you can say socialism.

21:58.340 --> 22:05.500
Number four would be the warring echo chambers due to the big companies wanting you to click

22:05.500 --> 22:07.380
on things that make you indignant.

22:07.380 --> 22:10.420
And so giving you things that are more and more extreme.

22:10.420 --> 22:15.220
And so you end up in this echo chamber where you believe these crazy conspiracy theorists,

22:15.220 --> 22:21.940
if you're in the other echo chamber, or you believe the truth, if you're in my echo chamber.

22:21.940 --> 22:25.700
That's partly to do with the policies of the companies and maybe something could be done

22:25.700 --> 22:26.700
about that.

22:27.060 --> 22:31.820
But that would mean that is a problem that exists if it existed prior to large language

22:31.820 --> 22:32.820
models.

22:32.820 --> 22:35.980
And in fact, large language models could reverse it.

22:35.980 --> 22:36.980
Maybe.

22:36.980 --> 22:40.780
I mean, it's an open question of whether they can make it better or whether they make that

22:40.780 --> 22:41.780
problem worse.

22:41.780 --> 22:46.820
Yeah, it's a problem to do with AI, but it's not to do with large language models.

22:46.820 --> 22:49.940
It's a problem to do with AI in the sense that there's an algorithm using AI trained

22:49.940 --> 22:52.420
on our emotions that then pushes us in those directions.

22:52.420 --> 22:53.420
Okay.

22:53.420 --> 22:54.420
All right.

22:54.420 --> 22:55.420
So that's number four.

22:55.420 --> 23:00.260
Because the existential risk, which is the one I decided to talk about because a lot

23:00.260 --> 23:01.820
of people think is a joke.

23:01.820 --> 23:02.820
Right.

23:02.820 --> 23:08.860
So there was an editorial in Nature yesterday where they basically said, I'm fear-mongering

23:08.860 --> 23:14.100
about the existential risk is distracting attention from the actual risks.

23:14.100 --> 23:18.380
So they compared existential risk with actual risks, implying the existential risk wasn't

23:18.380 --> 23:21.260
actual.

23:21.260 --> 23:25.180
I think it's important that people understand it's not just science fiction.

23:25.180 --> 23:27.300
It's not just fear-mongering.

23:27.300 --> 23:31.780
It is a real risk that we need to think about, and we need to figure out in advance how to

23:31.780 --> 23:32.940
deal with it.

23:32.940 --> 23:38.060
So that's five, and there's one more, and I can't think what it is.

23:38.060 --> 23:40.380
How do you have a list that doesn't end on existential risk?

23:40.380 --> 23:41.980
I feel like that should be the end of the list.

23:41.980 --> 23:45.860
No, that was the end, but I thought if I talked about existential risk, I'd be able to remember

23:45.860 --> 23:48.260
the missing one while I talk about it, but I couldn't.

23:48.260 --> 23:49.260
All right.

23:49.260 --> 23:50.460
Well, let's talk about existential risk.

23:50.460 --> 23:54.940
What exactly explain exactly existential risk, how it happens?

23:54.940 --> 24:00.980
Or explain, as best you can imagine it, what it is that goes wrong that leads us to extinction

24:00.980 --> 24:03.180
or disappearance of humanity as a species?

24:03.180 --> 24:04.180
Okay.

24:04.180 --> 24:09.500
At a very general level, if you've got something a lot smarter than you that's very good at

24:09.500 --> 24:14.340
manipulating people, just at a very general level, are you confident people will stay

24:14.340 --> 24:16.340
in charge?

24:16.340 --> 24:20.980
And then you can go into specific scenarios for how people might lose control, even though

24:20.980 --> 24:24.340
they're the people creating this and giving it its goals.

24:24.340 --> 24:29.660
And one very obvious scenario is if you're given a goal and you want to be good at achieving

24:29.660 --> 24:34.780
it, what you need is as much control as possible.

24:34.780 --> 24:40.740
So for example, if I'm sitting in a boring seminar and I see a little dot of light on

24:40.740 --> 24:47.420
the ceiling, and then suddenly I notice that when I move, that dot of light moves, I realize

24:47.420 --> 24:51.700
this is the reflection from my watch, the sun is bouncing off my watch.

24:51.700 --> 24:55.340
And so the next thing I do is I don't start listening to the boring seminar again.

24:55.340 --> 24:58.780
I immediately try and figure out how to make it go this way and how to make it go that

24:58.780 --> 24:59.780
way.

24:59.780 --> 25:02.620
And once I got control of it, then maybe I'll listen to the seminar again.

25:02.620 --> 25:07.140
We have a very strong built in urge to get control and it's very sensible because the

25:07.140 --> 25:10.740
more control you get, the easier it is to achieve things.

25:10.740 --> 25:13.500
And I think AI will be able to derive that too.

25:13.500 --> 25:16.380
It's good to get control so you can achieve other goals.

25:16.580 --> 25:24.140
Wait, so you actually believe that getting control will be an innate feature of something

25:24.140 --> 25:26.100
that the AIs are trained on us, right?

25:26.100 --> 25:27.100
They act like us.

25:27.100 --> 25:31.500
They think like us because the neural architecture makes them like our human brains and because

25:31.500 --> 25:33.100
they're trained on all of our outputs.

25:33.100 --> 25:37.780
So you actually think that getting control of humans will be something that the AI is

25:37.780 --> 25:39.500
almost aspire to?

25:39.500 --> 25:44.940
No, I think they'll derive it as a way of achieving other goals.

25:44.940 --> 25:46.300
I think in us it's innate.

25:46.300 --> 25:52.620
I think I'm very dubious about saying things are really innate, but I think the desire

25:52.620 --> 25:58.740
to understand how things work is a very sensible desire to have and I think we have that.

25:58.740 --> 26:04.020
So we have that and then AIs will develop an ability to manipulate us and control us

26:04.020 --> 26:08.460
in a way that we can't respond to, right?

26:08.460 --> 26:14.340
That the manipulative AIs and even though good people will be able to use equally powerful

26:14.380 --> 26:18.540
AIs to counter these bad ones, you believe that we still could have an existential crisis?

26:18.540 --> 26:19.740
Yes.

26:19.740 --> 26:20.740
It's not clear to me.

26:20.740 --> 26:25.380
I mean, that makes the argument that the good people will have more resources than the

26:25.380 --> 26:27.380
bad people.

26:27.380 --> 26:33.300
I'm not sure about that and that good AI is going to be more powerful than bad AI and

26:33.300 --> 26:35.860
good AI is going to be able to regulate bad AI.

26:35.860 --> 26:40.660
And we have a situation like that at present where you have people using AI to create

26:40.700 --> 26:46.460
spam and you have people like Google using AI to filter out the spam and at present Google

26:46.460 --> 26:51.260
has more resources than the defenders of beating the attackers, but I don't see that it'll

26:51.260 --> 26:52.260
always be like that.

26:52.260 --> 26:55.060
I mean, even in cyber warfare where you have moments where it seems like the criminals

26:55.060 --> 26:58.060
are winning and sometimes where it seems like the defenders are winning.

26:58.060 --> 27:02.100
So you believe that there will be a battle like that over control of humans by super

27:02.100 --> 27:03.380
intelligent artificial intelligence?

27:03.380 --> 27:04.380
It may well be, yes.

27:04.380 --> 27:09.380
And I'm not convinced that good AI that's trying to stop bad AI getting control will

27:09.380 --> 27:10.380
win.

27:10.420 --> 27:11.420
Okay.

27:11.420 --> 27:13.340
So, all right.

27:13.340 --> 27:18.980
So before this existential risk happened, before bad AI does this, we have a lot of extremely

27:18.980 --> 27:22.100
smart people building a lot of extremely important things.

27:22.100 --> 27:27.340
What exactly can they do to most help limit this risk?

27:27.340 --> 27:34.580
So one thing you can do is before the AI gets super intelligent, you can do empirical work

27:34.580 --> 27:39.660
into how it goes wrong, how it tries to get control, whether it tries to get control.

27:39.660 --> 27:41.420
You don't know whether it would.

27:41.420 --> 27:46.140
But before it's smarter than us, I think the people developing it should be encouraged

27:46.140 --> 27:51.980
to put a lot of work into understanding how it might go wrong, understanding how it might

27:51.980 --> 27:54.080
try and take control away.

27:54.080 --> 27:59.260
And I think the government could maybe encourage the big companies developing it to put comparable

27:59.260 --> 28:04.380
resources, maybe not equal resources, but right now there's 99 very smart people trying

28:04.380 --> 28:08.340
to make it better and one very smart person trying to figure out how to stop it taking

28:08.380 --> 28:09.380
over.

28:09.380 --> 28:12.060
And maybe you want it more balanced.

28:12.060 --> 28:17.140
And so this is in some ways your role right now, the reason why you've left Google on

28:17.140 --> 28:21.980
good terms, but you want to be able to speak out and help participate in this conversation

28:21.980 --> 28:24.700
so more people can join that one and not the 99.

28:24.700 --> 28:25.700
Yeah.

28:25.700 --> 28:28.700
I would say it's very important for smart people to be working on that.

28:28.700 --> 28:32.740
But I'd also say it's very important not to think this is the only risk.

28:32.740 --> 28:34.420
There's all these other risks.

28:34.420 --> 28:38.700
And I've remembered the last one, which is fake news.

28:38.700 --> 28:43.660
So it's very important to try, for example, to mark everything that's fake as fake, whether

28:43.660 --> 28:47.180
we can do that technically, I don't know, but it'd be great if we could.

28:47.180 --> 28:48.740
Governments do it with counterfeit money.

28:48.740 --> 28:54.860
They won't allow counterfeit money because that reflects on their sort of central interest.

28:54.860 --> 28:58.460
They should try and do it with AI-generated stuff.

28:58.460 --> 28:59.940
I don't know whether they can.

28:59.940 --> 29:00.940
All right.

29:00.940 --> 29:01.940
So we're out of time.

29:01.940 --> 29:06.500
Give one specific to-do, something to read, a thought experiment, one thing to leave

29:06.500 --> 29:10.340
the audience with so they can go out here and think, OK, I'm going to do this.

29:10.340 --> 29:15.140
AI is the most powerful thing we've invented, and perhaps in our lifetimes, and I'm going

29:15.140 --> 29:20.180
to make it better to make it more likely it's a force for good in the next generation.

29:20.180 --> 29:22.460
So how could they make it more likely be a force for good?

29:22.460 --> 29:23.460
Yes.

29:23.460 --> 29:26.820
One final thought for everyone here.

29:26.820 --> 29:32.340
I actually don't have a plan for how to make it more likely to be good than bad, sorry.

29:32.340 --> 29:37.140
I think it's great that it's being developed because we didn't get to mention the huge

29:37.140 --> 29:42.100
numbers of good uses of it, like in medicine, in climate change, and so on.

29:42.100 --> 29:48.180
So I think progress in AI is inevitable and it's probably good, but we seriously ought

29:48.180 --> 29:52.380
to worry about mitigating all the bad side effects of it and worry about the existential

29:52.380 --> 29:53.380
threat.

29:53.380 --> 29:54.380
All right.

29:54.380 --> 29:57.980
Thank you everyone, an incredibly thoughtful, inspiring, interesting, phenomenally smart.

29:57.980 --> 29:58.980
Thank you to Jeffrey Hinton.

29:58.980 --> 29:59.980
Thank you.

29:59.980 --> 30:00.980
Thank you, Jeff.

30:00.980 --> 30:01.480
So great.

