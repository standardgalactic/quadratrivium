start	end	text
0	13560	What an incredible pleasure to be here with Jeffrey Hinton, one of the great minds and
13560	15160	one of the great issues of our time.
15160	19640	A man who helped create artificial intelligence was at the center of nearly every revolution
19640	25160	in it and now has become perhaps the most articulate critic of where we're going.
25160	27520	So an honor to be on stage with you.
27520	28520	Thank you.
28560	31880	During the moniker Godfather of AI, one of the things that AI has traditionally had
31880	36320	problems with is humor, I asked AI if it could come up with a joke about the Godfather of
36320	39840	AI and it actually wasn't that bad.
39840	44240	It said he gave AI an offer it couldn't refuse, neural networks.
44240	45240	It's not bad.
45240	46240	Okay, that's not bad.
46240	47240	It's good for AI.
47240	48240	So let's begin with that.
48240	52160	What I want to do in this conversation is very briefly step a little back into your foundational
52160	57040	work, then go to where we are today and then talk about the future.
57040	61280	So when you're building and you're designing neural networks and you're building computer
61280	66400	systems that work like the human brain and that learn like the human brain and everybody
66400	69440	else is saying, Jeff, this is not going to work.
69440	75680	You push ahead and do you push ahead because you know that this is the best way to train
75680	81000	computer systems or you do it for more spiritual reasons that you want to make a machine that
81000	82840	is like us?
82840	88280	I do it because the brain has to work somehow and it sure as hell doesn't work by manipulating
88280	94440	symbolic expressions explicitly and so something like neural nets had to work.
94440	98360	Also von Neumann and Turing believed that so that's a good start.
98360	102200	So you're doing it because you think it's the best way forward?
102200	104640	Yes, in the long run the best way forward.
104640	108920	Because that decision has profound effects down the line.
109320	110640	Okay, so you do that.
110640	115640	You start building neural nets, you push forward and they become better than humans
115640	117880	at certain limited tasks, right?
117880	124520	At image recognition, at translation, some chemical work.
124520	130800	I interviewed you in 2019 at Google I.O. and you said that it would be a long time before
130800	135200	they could match us in reasoning and that's the big change that's happened over the last
135200	137040	four years, right?
137040	139600	They still can't match us but they're getting close.
139600	143120	And how close are they getting and why?
143120	147720	It's the big language models that are getting close and I don't really understand why they
147720	151560	can do it but they can do little bits of reasoning.
151560	156840	So my favorite example is I asked GPT-4, a puzzle that was given to me by a symbolic
156840	160080	AI guy who thought it wouldn't be able to do it.
160080	165800	I made the puzzle more difficult than it could still do it and the puzzle was the rooms in
165800	170400	my house are painted blue or yellow or white.
170400	173360	Yellow paint fades to white within a year.
173360	175760	In two years' time I want them all to be white.
175760	178440	What should I do and why?
178440	183560	And it says you should paint the blue rooms white and then it says you should do that
183560	188080	because blue won't fade to white and it says you don't need to paint the yellow rooms because
188080	189920	they will fade to white.
189920	194640	So it knew what I should do and it knew why and I was surprised that it could do that
194640	196040	much reasoning already.
196040	201040	And it's kind of an amazing example because when people critique these systems or they
201040	204000	say they're not going to do much, they say they're mad libs, they're just word completion
204000	205800	but that is not word completion.
205800	207800	To you is that thinking?
207800	215280	Yeah, that's thinking and when people say it's just autocomplete, a lot goes on in
215280	217640	that word just autocomplete.
217640	222400	If you think what it takes to predict the next word, you have to understand what's been
222400	225360	said to be really good at predicting the next word.
225360	229600	So people say it's just autocomplete or it's just statistics.
229600	236360	Now there's a sense in which it is just statistics, but in that sense everything's just statistics.
236360	240840	It's not the sense most people think of statistics as it keeps the counts of how many times this
240840	244080	combination of words occurred and how many times that combination.
244080	245080	It's not like that at all.
245080	250040	It's inventing features and interactions between features to explain what comes next.
250040	255560	So if it's just statistics and everything is just statistics, is there anything that
255560	257840	we can do?
257840	260760	Obviously it's not humor, maybe it's not reasoning.
260760	265560	Is there anything that we can do that a sufficiently well-trained large language model with a sufficient
265560	269840	number of parameters and a sufficient amount of compute could not do in the future?
269840	275960	If the model is also trained on vision and picking things up and so on, then no.
275960	280400	But is there anything that we can think of and any way we can think in any cognitive
280400	284080	process that the machines will not be able to replicate?
284080	288440	We're just a machine, we're a wonderful, incredibly complicated machine, but we're
288440	292680	just a big neural net and there's no reason why an artificial neural net shouldn't be
292680	294480	able to do everything we can do.
294480	299280	Are we a big neural net that is more efficient than these new neural nets we're building
299280	301480	or are we less efficient?
301480	306000	It depends whether you're talking about speed of acquiring knowledge and how much knowledge
306000	309960	you can acquire or whether you're talking about energy consumption.
309960	312920	So in energy consumption, we're much more efficient.
312920	317080	We're like 30 watts and one of these big language models, when you're training it, you train
317080	320760	many copies of it each looking at different parts of the data.
320760	322920	So it's more like a megawatt.
322920	327520	So it's much more expensive in terms of energy, but all these copies can be learning different
327520	329480	things from different parts of the data.
329480	333360	So it's much more efficient in terms of acquiring knowledge from data.
333360	337480	And it becomes only more efficient because each system can train each next system?
337480	338480	Yes.
338480	339760	So let's get to your critique.
339760	345360	So the best summarization of your critique came from a conference at the Milken Institute
345360	351960	about a month ago and it was Snoop Dogg and he said, I heard the old dude who created
351960	357160	AI saying this is not safe because the AI's got their own mind and those motherfuckers
357160	361040	going to start doing their own shit.
361040	362040	Is that accurate?
362040	365280	Is that an accurate summarization?
365280	372000	They probably didn't have mothers.
372000	374760	But the rest of what Dr. Dogg said is correct.
374760	375760	Hang on.
375760	376760	Yes.
376760	377760	All right.
377760	383520	So explain what you mean or what he means and how it applies to what you mean when they're
383520	385440	going to start doing their own shit.
385440	386440	What does that mean to you?
386560	387560	Okay.
387560	391680	So first I have to emphasize we're entering a period of huge uncertainty.
391680	393440	Nobody really knows what's going to happen.
393440	396960	And people whose opinion I respect have very different beliefs from me.
396960	399360	Like Jan LeCun thinks everything's going to be fine.
399360	400360	They're just going to help us.
400360	401960	It's all going to be wonderful.
401960	406160	But I think we have to take seriously the possibility that if they get to be smarter
406160	411400	than us, which seems quite likely, and they have goals of their own, which seems quite
411400	415120	likely, they may well develop the goal of taking control.
415320	417800	And if they do that, we're in trouble.
417800	418800	So okay.
418800	422400	So let's let's go back to that in a second, but let's take Jan's position.
422400	426040	So Jan LeCun was also one of the people who won the Turing Award and is also called the
426040	427560	Godfather of AI.
427560	431480	And I was recently interviewing him and he made the case.
431480	436200	He said, look, technologies, all technologies can be used for good or ill, but some technologies
436200	438000	have more of an inherent goodness.
438000	443760	And AI has been built by humans, by good humans for good purposes.
443760	447240	It's been trained on good books and good texts.
447240	450680	It will have a bias towards good in the future.
450680	452760	Do you believe that or not?
452760	456560	I think AI that's been trained by good people will have a bias towards good.
456560	461400	And AI that's been trained by bad people like Putin or somebody like that will have a bias
461400	462740	towards bad.
462740	466040	We know they're going to make battle robots.
466040	469520	They're busy doing it in many different defense departments.
469520	474120	So they're not going to necessarily be good since their primary purpose is going to be
474120	476360	to kill people.
476360	483640	So you believe that the risks of the bad uses of AI are whether they're more or less than
483640	488240	the good uses of AI are so substantial, they deserve a lot of our thought right now.
488240	489240	Certainly.
489240	490240	Yes.
490240	492240	For lethal autonomous weapons, they deserve a lot of our thought.
492240	495840	Well, let's, okay, let's stick on lethal autonomous weapons because one of the things
495840	501480	in this argument is that you are one of the few people who is really speaking about this
501480	503880	as a risk, a real risk.
503880	512440	Explain your hypothesis about why super powerful AI combined with the military could actually
512440	515280	lead to more and more warfare.
515280	516520	Okay.
516520	521800	I don't actually want to answer that question.
521800	523120	There's a separate question.
523120	528320	Even if the AI isn't super intelligent, if defense departments use it for making battle
528320	531520	robots, it's going to be very nasty, scary stuff.
531520	535400	And it's going to lead, even if it's not super intelligent, and even if it doesn't have its
535400	540640	own intentions, it just does what Putin tells it to.
540640	546000	It's going to make it much easier, for example, for rich countries to invade poor countries.
546000	551240	A present, there's a barrier to invading poor countries willy-nilly, which is you get dead
551240	553420	citizens coming home.
553420	555720	If they just dead battle robots, that's just great.
555720	558960	The military industrial complex would love that.
558960	563320	So you think that because, I mean, it's sort of a similar argument that people make with
563320	564320	drones.
564320	566800	If you can send a drone and you don't have to send an airplane with a pilot, you're more
566800	570080	likely to send the drone, therefore you're more likely to attack.
570080	574000	If you have a battle robot, it's that same thing squared.
574000	575280	And that's your concern.
575280	577120	That's my main concern with battle robots.
577120	582040	It's a separate concern from what happens with super intelligent systems taking over
582040	584360	for their own purposes.
584360	588200	Before we get to super intelligent systems, let's talk about some of your other concerns.
588200	593200	So in the litany of things that you're worried about, you obviously we have battle robots
593200	596560	as one, you're also quite worried about inequality.
596560	598120	Tell me more about this.
598120	603360	So it's fairly clear, it's not certain, but it's fairly clear that these big language
603360	606920	models will cause a big increase in productivity.
606920	611240	So there's someone I know who answers letters of complaint for a health service.
611240	616200	And he used to write these letters himself and now he just gets chat GPT to write the
616200	620640	letters and it takes one-fifth of the amount of time to answer a complaint.
620640	626840	So he can do five times as much work and so there are only five times fewer of him.
626840	628720	Or maybe they'll just answer a lot more letters.
628720	629960	Or they'll answer more letters, right?
629960	633480	Or maybe they'll have more people because they'll be so efficient, right?
633480	635680	More productivity leads to more getting more done.
635680	637880	This is an unanswered question.
637880	642440	But what we expect in the kind of society we live in is that if you get a big increase
642440	647440	in productivity like that, the wealth isn't going to go to the people who are doing the
647440	651440	work or the people who get unemployed, it's going to go to making the rich richer and
651440	652440	the poor poorer.
652440	654440	And that's very bad for society.
654440	657880	Definitionally, or you think there's some feature of AI that will lead to that?
657880	662280	No, it's not to do with AI, it's just what happens when you get an increase in productivity,
662280	665120	particularly in a society that doesn't have strong unions.
665120	669680	But now there are many economists who would take a different position and say that over
669680	674800	time, and if you were to look at technology, right, we went from horses and horses and
674800	678160	buggies and the horses and buggies went away and then we had cars and oh my gosh, the people
678160	682920	who drove the horses lost their jobs and ATMs came along and suddenly bank tellers no longer
682920	683920	need to do that.
683920	687040	But we now employ many more bank tellers than we used to and we have many more people driving
687040	689200	Ubers than we had people driving horses.
689200	695080	So the argument what an economist would make to this would be, yes, there will be chair
695080	700200	and there will be fewer people answering those letters, but there'll be many more higher
700200	701640	cognitive things that will be done.
701640	703920	How do you respond to that?
703920	708640	I think the first thing I'd say is a loaf of bread used to cost a penny, then they invented
708640	712680	economics and now it costs five dollars.
712680	716880	So I don't entirely trust what economists say, particularly when they're dealing with
716880	720240	a new situation that's never happened before.
720240	724360	And superintelligence would be a new situation that never happened before, but even these
724360	730400	big chatbots that are just replacing people whose job involves producing text, that's
730400	735920	never happened before and I'm not sure how they can confidently predict that more jobs
735920	738440	will be created than the number of jobs lost.
738440	742840	I just have a little side note that in the green room, I introduced Jeff to, I have two
742840	747760	of my three children are here, Alice and Zachary, they're somewhere out here, and he said to
747760	750240	Alice, he said, are you going to go into media?
750240	753280	And then he said, well, I'm not sure media will exist.
753280	755000	And then Alice was asking, what should I do?
755000	756000	And you said?
756000	757000	Plumbing.
757000	758000	Yes.
758000	759000	Now explain.
759000	763440	I mean, we have a number of plumbing problems at our house, it'd be wonderful if they were
763440	765720	able to put in a new sink.
765720	770120	Explain what jobs, a lot of young people out here, not just my children, but thinking about
770120	774440	what careers to go into, what are the careers they should be looking at, what are the attributes
774440	775440	of them?
775440	778960	I'll give you a little story about being a carpenter.
778960	784600	If you're a carpenter, it's fun making furniture, but it's a complete dead loss because machines
784600	786360	can make furniture.
786360	791760	If you're a carpenter, what you're good for is repairing furniture or fitting things into
791760	797280	awkward spaces in old houses, making shelves in things that aren't quite square.
797280	802440	So the jobs that are going to survive AI for a long time are jobs where you have to be
802440	807460	very adaptable and physically skilled and plumbing is that kind of a job.
807460	811660	How does manual dexterity is hard for a machine to replicate?
811660	817020	It's still hard, and I think it's going to be longer before they can be really dexterous
817020	820140	and get into awkward spaces.
820140	824020	That's going to take longer than being good at answering text questions.
824020	825020	Should I believe you?
825020	827660	Because when we were on stage four years ago, you said reasoning.
827660	831580	As long as somebody has a job that focuses on reasoning, they'll be able to last a dozen.
831580	837380	Isn't the nature of AI such that we don't actually know where the next incredible
837380	838980	improvement in performance will come?
838980	840500	Maybe it will come in manual dexterity.
840500	842500	Yeah, it's possible.
842500	845340	So actually, let me ask you a question about that.
845340	850540	So do you think when we look at AI and we look at the next five years of AI, the most
850540	854940	impactful improvements we'll see will be in large language models and related to large
854940	856140	language models?
856140	858620	Or do you think it will be in something else?
858620	862020	I think it'll probably be in multimodal large models.
862020	866740	So they won't just be language models, they'll be doing vision.
866780	868540	Actually, they'll be analyzing video.
868540	872620	So they were able to train on all of the YouTube videos, for example.
872620	878180	And you can understand a lot from things other than language.
878180	882260	And when you do that, you need less language to reach the same performance.
882260	885540	So the idea that they're going to be saturated because they've already used all the language
885540	889100	there is, all the language is easy to get hold of.
889100	892020	That's less of a concern if they're also using lots of other modalities.
892020	896500	I mean, this gets at one of the, another argument that Jan, your fellow Godfather of AI makes
896540	898620	is that language is so limited, right?
898620	900940	There's so much information that we're conveying just beyond the word.
900940	902900	In fact, I'm gesturing like mad, right?
902900	906380	Which conveys some of the information as well as the lighting and all this.
906380	908900	So your view is that may be true.
908900	913780	Language is a limited vector for information, but soon it will be combined with other vectors.
913780	915300	Absolutely.
915300	919900	It's amazing what you can learn from language alone, but you're much better off learning
919900	921020	from many modalities.
921020	923780	Small children don't just learn from language alone.
923780	924340	Right.
924380	931180	So if you were, if your principal role right now was still researching AI, finding the
931180	938580	next big thing, you would be doing multi-modal AI and trying to attach, say, visual AI systems
938580	940620	to text-based AI systems?
940620	943380	Yes, which is what they're doing now at Google.
943380	949100	Google is making a system called Gemini, but fortunately, Demisabis talked about it a few
949100	953220	days ago, and that's a multi-modal AI.
953260	955340	Well, let me talk about actually something else at Google.
955340	961500	So while you were there, Google invented the transformer network or invented the transformer
961500	965580	architecture, generative pre-trained transformers.
965580	972420	When did you realize that that would be so central and so important?
972420	977780	It's interesting to me because it's this paper that comes out in 2017, and when it comes
977780	981860	out, it's not as though firecrackers are left, you know, shot into the sky.
981860	985860	It's six years later, five years later, that we suddenly realized the consequences.
985860	989940	And it's interesting to think, what are the other papers out there that could be the same
989940	990780	in five years?
990780	995220	So with transformers, it was really only a couple of years later when Google developed
995220	996540	BERT.
996540	1001740	So BERT made it very clear transformers were a huge breakthrough.
1001740	1008860	I didn't immediately realize what a huge breakthrough they were, and I'm annoyed about that.
1008860	1010700	It took me a couple of years to realize.
1010700	1015100	Well, you know, the first time I ever heard the word transformer was talking to you on
1015100	1020500	stage, and you were talking about transformers versus capsules, and this was right after
1020500	1021500	it came out.
1021500	1025460	Let's talk about one of the other critiques about language models and other models, which
1025460	1032260	is soon, I mean, in fact, probably already they've absorbed all the organic data that
1032260	1033780	has been created by humans.
1033780	1037660	If I create an AI model right now, and I train it on the internet, it's trained on a bunch
1037660	1041940	of stuff, mostly stuff made by humans, but a bunch of stuff made by AI, right?
1041940	1042940	Yeah.
1042940	1047740	And you're going to keep training AIs on stuff that has been created by AIs, whether it's
1047740	1051780	text-based language model or whether it's a multimodal language model.
1051780	1057300	Will that lead to the inevitable decay and corruption, as some people argue?
1057300	1061140	Or is that just a thing we have to deal with?
1061140	1065580	Or is it, as other people in the AI field, the greatest thing for training AIs, and we
1065580	1068180	should just use synthetic data in AI?
1068180	1069180	Okay.
1069180	1071900	I don't actually know the answer to this technically.
1071900	1076060	I suspect you have to take precautions, so you're not just training on data that you yourself
1076060	1080540	generated or the some previous version of you generated.
1080540	1084740	I suspect it's going to be possible to take those precautions, although it would be much
1084740	1088300	easier if all fake data was marked fake.
1088300	1093940	There is one example in AI where training on stuff from yourself helps a lot.
1093940	1099180	So if you don't have much training data or rather you have a lot of unlabeled data and
1099180	1103660	a small amount of labeled data, you can train a model to predict the labels on the labeled
1103660	1113020	data and then you take that same model and train it to predict labels for unlabeled data
1113020	1118180	and whatever it predicts, you tell it you were right.
1118180	1120580	And that actually makes the model work better.
1120580	1122820	How on earth does that work?
1123060	1127860	Because on the whole it tends to be right.
1127860	1128860	It's complicated.
1128860	1133620	It's been analyzed much better in many years ago from acoustic modems.
1133620	1135780	They did the same trick.
1135780	1141740	So listening to this, I've had this realization on stage, you're a man who's very critical
1141740	1145780	of where we're going, killer robots, income inequality.
1145780	1147940	You also sound like somebody who loves this stuff.
1147940	1150020	Yeah, I love this stuff.
1150020	1153140	How could you not love making intelligent things?
1153140	1157660	So let me get to maybe the most important question for the audience and for everyone
1157660	1159580	here.
1159580	1162580	We're now at this moment where a lot of people here love this stuff and they want to build
1162580	1165220	it and they want to experiment.
1165220	1167260	But we don't want negative consequences.
1167260	1169020	We don't want increased income inequality.
1169020	1171380	I don't want media to disappear.
1171380	1177700	What are the choices and decisions and things we should be working on now to maximize the
1177700	1182340	good, to maximize the creativity, but to limit the potential harms?
1182340	1187340	So I think to answer that you have to distinguish many kinds of potential harm.
1187340	1190740	So I'll distinguish like six of them for you.
1190740	1193940	There's bias and discrimination.
1193940	1197860	That is present now.
1197860	1199660	It's not one of these future things we need to worry about.
1199660	1201340	It's happening now.
1201340	1206140	But it is something that I think is relatively easy to fix compared with all the other things.
1206140	1210140	If you make your target, not be to have a completely unbiased system, but just to have
1210140	1214580	a system that's significantly less biased than what it's replacing.
1214580	1218580	So at present, you have old white men deciding whether the young black women should get mortgages.
1218580	1223340	And if you just train on that data, you get a system that's equally biased.
1223340	1225700	But you can analyze the bias.
1225700	1227980	You can see how it's biased because it won't change its behavior.
1227980	1229940	You can freeze it and then analyze it.
1229940	1232740	And that should make it easier to correct for bias.
1232740	1235020	So okay, that's bias and discrimination.
1235060	1236540	I think we can do a lot about that.
1236540	1240420	And I think it's important we do a lot about that, but it's doable.
1240420	1242620	The next one is battle robots.
1242620	1248380	That I'm really worried about because defense departments are going to build them.
1248380	1252900	And I don't see how you could stop them doing it.
1252900	1256220	Something like a Geneva Convention would be great.
1256220	1259740	But those never happened until after they've been used with chemical weapons.
1259740	1263820	It didn't happen until after the First World War, I believe.
1263820	1267260	And so I think what may happen is people will use battle robots.
1267260	1269740	We'll see just how absolutely awful they are.
1269740	1273660	And then maybe we can get an international convention to prohibit them.
1273660	1274660	So that's two.
1274660	1280380	I mean, you could also tell the people building the AI to not sell their equipment to the military.
1280380	1281340	You could try.
1281340	1281740	Try.
1281740	1283620	Okay, number three.
1283620	1287340	The military has lots of money.
1287340	1291420	Okay, number three, there's joblessness.
1291420	1296860	You could try and do stuff to make sure the increase in productivity, some of that extra
1296860	1301260	revenue that comes from the increase in productivity is going goes to helping the people who make
1301260	1302260	jobless.
1302260	1307140	If it turns out that there aren't as many jobs created as destroyed.
1307140	1309020	That's a question of social policy.
1309020	1312780	And what you really need for that is socialism.
1312780	1318340	We're in Canada, so you can say socialism.
1318340	1325500	Number four would be the warring echo chambers due to the big companies wanting you to click
1325500	1327380	on things that make you indignant.
1327380	1330420	And so giving you things that are more and more extreme.
1330420	1335220	And so you end up in this echo chamber where you believe these crazy conspiracy theorists,
1335220	1341940	if you're in the other echo chamber, or you believe the truth, if you're in my echo chamber.
1341940	1345700	That's partly to do with the policies of the companies and maybe something could be done
1345700	1346700	about that.
1347060	1351820	But that would mean that is a problem that exists if it existed prior to large language
1351820	1352820	models.
1352820	1355980	And in fact, large language models could reverse it.
1355980	1356980	Maybe.
1356980	1360780	I mean, it's an open question of whether they can make it better or whether they make that
1360780	1361780	problem worse.
1361780	1366820	Yeah, it's a problem to do with AI, but it's not to do with large language models.
1366820	1369940	It's a problem to do with AI in the sense that there's an algorithm using AI trained
1369940	1372420	on our emotions that then pushes us in those directions.
1372420	1373420	Okay.
1373420	1374420	All right.
1374420	1375420	So that's number four.
1375420	1380260	Because the existential risk, which is the one I decided to talk about because a lot
1380260	1381820	of people think is a joke.
1381820	1382820	Right.
1382820	1388860	So there was an editorial in Nature yesterday where they basically said, I'm fear-mongering
1388860	1394100	about the existential risk is distracting attention from the actual risks.
1394100	1398380	So they compared existential risk with actual risks, implying the existential risk wasn't
1398380	1401260	actual.
1401260	1405180	I think it's important that people understand it's not just science fiction.
1405180	1407300	It's not just fear-mongering.
1407300	1411780	It is a real risk that we need to think about, and we need to figure out in advance how to
1411780	1412940	deal with it.
1412940	1418060	So that's five, and there's one more, and I can't think what it is.
1418060	1420380	How do you have a list that doesn't end on existential risk?
1420380	1421980	I feel like that should be the end of the list.
1421980	1425860	No, that was the end, but I thought if I talked about existential risk, I'd be able to remember
1425860	1428260	the missing one while I talk about it, but I couldn't.
1428260	1429260	All right.
1429260	1430460	Well, let's talk about existential risk.
1430460	1434940	What exactly explain exactly existential risk, how it happens?
1434940	1440980	Or explain, as best you can imagine it, what it is that goes wrong that leads us to extinction
1440980	1443180	or disappearance of humanity as a species?
1443180	1444180	Okay.
1444180	1449500	At a very general level, if you've got something a lot smarter than you that's very good at
1449500	1454340	manipulating people, just at a very general level, are you confident people will stay
1454340	1456340	in charge?
1456340	1460980	And then you can go into specific scenarios for how people might lose control, even though
1460980	1464340	they're the people creating this and giving it its goals.
1464340	1469660	And one very obvious scenario is if you're given a goal and you want to be good at achieving
1469660	1474780	it, what you need is as much control as possible.
1474780	1480740	So for example, if I'm sitting in a boring seminar and I see a little dot of light on
1480740	1487420	the ceiling, and then suddenly I notice that when I move, that dot of light moves, I realize
1487420	1491700	this is the reflection from my watch, the sun is bouncing off my watch.
1491700	1495340	And so the next thing I do is I don't start listening to the boring seminar again.
1495340	1498780	I immediately try and figure out how to make it go this way and how to make it go that
1498780	1499780	way.
1499780	1502620	And once I got control of it, then maybe I'll listen to the seminar again.
1502620	1507140	We have a very strong built in urge to get control and it's very sensible because the
1507140	1510740	more control you get, the easier it is to achieve things.
1510740	1513500	And I think AI will be able to derive that too.
1513500	1516380	It's good to get control so you can achieve other goals.
1516580	1524140	Wait, so you actually believe that getting control will be an innate feature of something
1524140	1526100	that the AIs are trained on us, right?
1526100	1527100	They act like us.
1527100	1531500	They think like us because the neural architecture makes them like our human brains and because
1531500	1533100	they're trained on all of our outputs.
1533100	1537780	So you actually think that getting control of humans will be something that the AI is
1537780	1539500	almost aspire to?
1539500	1544940	No, I think they'll derive it as a way of achieving other goals.
1544940	1546300	I think in us it's innate.
1546300	1552620	I think I'm very dubious about saying things are really innate, but I think the desire
1552620	1558740	to understand how things work is a very sensible desire to have and I think we have that.
1558740	1564020	So we have that and then AIs will develop an ability to manipulate us and control us
1564020	1568460	in a way that we can't respond to, right?
1568460	1574340	That the manipulative AIs and even though good people will be able to use equally powerful
1574380	1578540	AIs to counter these bad ones, you believe that we still could have an existential crisis?
1578540	1579740	Yes.
1579740	1580740	It's not clear to me.
1580740	1585380	I mean, that makes the argument that the good people will have more resources than the
1585380	1587380	bad people.
1587380	1593300	I'm not sure about that and that good AI is going to be more powerful than bad AI and
1593300	1595860	good AI is going to be able to regulate bad AI.
1595860	1600660	And we have a situation like that at present where you have people using AI to create
1600700	1606460	spam and you have people like Google using AI to filter out the spam and at present Google
1606460	1611260	has more resources than the defenders of beating the attackers, but I don't see that it'll
1611260	1612260	always be like that.
1612260	1615060	I mean, even in cyber warfare where you have moments where it seems like the criminals
1615060	1618060	are winning and sometimes where it seems like the defenders are winning.
1618060	1622100	So you believe that there will be a battle like that over control of humans by super
1622100	1623380	intelligent artificial intelligence?
1623380	1624380	It may well be, yes.
1624380	1629380	And I'm not convinced that good AI that's trying to stop bad AI getting control will
1629380	1630380	win.
1630420	1631420	Okay.
1631420	1633340	So, all right.
1633340	1638980	So before this existential risk happened, before bad AI does this, we have a lot of extremely
1638980	1642100	smart people building a lot of extremely important things.
1642100	1647340	What exactly can they do to most help limit this risk?
1647340	1654580	So one thing you can do is before the AI gets super intelligent, you can do empirical work
1654580	1659660	into how it goes wrong, how it tries to get control, whether it tries to get control.
1659660	1661420	You don't know whether it would.
1661420	1666140	But before it's smarter than us, I think the people developing it should be encouraged
1666140	1671980	to put a lot of work into understanding how it might go wrong, understanding how it might
1671980	1674080	try and take control away.
1674080	1679260	And I think the government could maybe encourage the big companies developing it to put comparable
1679260	1684380	resources, maybe not equal resources, but right now there's 99 very smart people trying
1684380	1688340	to make it better and one very smart person trying to figure out how to stop it taking
1688380	1689380	over.
1689380	1692060	And maybe you want it more balanced.
1692060	1697140	And so this is in some ways your role right now, the reason why you've left Google on
1697140	1701980	good terms, but you want to be able to speak out and help participate in this conversation
1701980	1704700	so more people can join that one and not the 99.
1704700	1705700	Yeah.
1705700	1708700	I would say it's very important for smart people to be working on that.
1708700	1712740	But I'd also say it's very important not to think this is the only risk.
1712740	1714420	There's all these other risks.
1714420	1718700	And I've remembered the last one, which is fake news.
1718700	1723660	So it's very important to try, for example, to mark everything that's fake as fake, whether
1723660	1727180	we can do that technically, I don't know, but it'd be great if we could.
1727180	1728740	Governments do it with counterfeit money.
1728740	1734860	They won't allow counterfeit money because that reflects on their sort of central interest.
1734860	1738460	They should try and do it with AI-generated stuff.
1738460	1739940	I don't know whether they can.
1739940	1740940	All right.
1740940	1741940	So we're out of time.
1741940	1746500	Give one specific to-do, something to read, a thought experiment, one thing to leave
1746500	1750340	the audience with so they can go out here and think, OK, I'm going to do this.
1750340	1755140	AI is the most powerful thing we've invented, and perhaps in our lifetimes, and I'm going
1755140	1760180	to make it better to make it more likely it's a force for good in the next generation.
1760180	1762460	So how could they make it more likely be a force for good?
1762460	1763460	Yes.
1763460	1766820	One final thought for everyone here.
1766820	1772340	I actually don't have a plan for how to make it more likely to be good than bad, sorry.
1772340	1777140	I think it's great that it's being developed because we didn't get to mention the huge
1777140	1782100	numbers of good uses of it, like in medicine, in climate change, and so on.
1782100	1788180	So I think progress in AI is inevitable and it's probably good, but we seriously ought
1788180	1792380	to worry about mitigating all the bad side effects of it and worry about the existential
1792380	1793380	threat.
1793380	1794380	All right.
1794380	1797980	Thank you everyone, an incredibly thoughtful, inspiring, interesting, phenomenally smart.
1797980	1798980	Thank you to Jeffrey Hinton.
1798980	1799980	Thank you.
1799980	1800980	Thank you, Jeff.
1800980	1801480	So great.
