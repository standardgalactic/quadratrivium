WEBVTT

00:00.000 --> 00:09.280
It's such a pleasure to be here with you today to talk about the rise of artificial

00:09.280 --> 00:15.920
intelligence that you heard about and what both the opportunities are and the challenges

00:15.920 --> 00:19.160
that we need to overcome to get these opportunities.

00:19.160 --> 00:24.440
So I want to encourage you to think big.

00:24.440 --> 00:31.040
I love the historical intro we got with the toilet paper roll and the dinosaurs reminding

00:31.040 --> 00:34.120
us of the big picture.

00:34.120 --> 00:39.920
So if we zoom out even bigger, as we see here, here we are at 13.8 billion years after our

00:39.920 --> 00:44.480
big bang and realizing that something remarkable has happened.

00:44.480 --> 00:50.520
Finally, after all this time, our universe has woken up and become aware of itself through

00:50.520 --> 00:54.760
us conscious beings on this little spinning blue ball in space.

00:54.760 --> 01:00.480
And when we look out into this beautiful cosmos, we discover something very humbling.

01:00.480 --> 01:08.040
We discover that a universe is vastly grander than we thought and that it looks mostly dead

01:08.040 --> 01:09.040
at first glance.

01:09.040 --> 01:15.720
But we've also discovered something truly inspiring, which is that by carefully studying

01:15.720 --> 01:22.720
our universe and its laws and building technology, we actually have much more influence than

01:22.720 --> 01:24.720
we thought.

01:24.720 --> 01:31.400
For example, we can, through science and technology, harness the powers of our universe and start

01:31.400 --> 01:35.960
exploring it and do all sorts of great things.

01:35.960 --> 01:47.000
And when we think about rockets, for example, as an example of what we can do with tech,

01:47.000 --> 01:52.120
it gives us a great metaphor that can guide us through this morning.

01:52.120 --> 01:58.400
To do inspiring things with technology, it's not enough to make your technology powerful.

01:58.400 --> 02:07.680
You also have to figure out, of course, how to steer it and where you want to go with it.

02:07.680 --> 02:13.440
Now, there's another journey that's much more inspiring still than that with rockets, which

02:13.440 --> 02:16.600
is that with artificial intelligence, which we're going to go on together this morning,

02:16.600 --> 02:20.040
where the passengers aren't just a few astronauts, but all of humanity.

02:20.040 --> 02:26.960
So let's talk about this collective journey we're taking into the future with AI, thinking

02:26.960 --> 02:31.840
about both the power, the steering, and the destination.

02:31.840 --> 02:32.840
Okay?

02:32.840 --> 02:33.840
Ready to go?

02:33.840 --> 02:35.840
And jam on.

02:35.840 --> 02:41.720
Let's begin with the power.

02:41.720 --> 02:45.240
What is intelligence?

02:45.240 --> 02:51.280
I define it just as the ability to accomplish goals, and the more complex the goals are,

02:51.280 --> 02:52.920
the more intelligent.

02:52.920 --> 03:00.520
And I give this very inclusive definition, covering both biological and non-biological

03:00.520 --> 03:08.640
intelligence, because the key idea, in my opinion, which I'll try to sell you on here,

03:08.640 --> 03:15.040
is that intelligence is all about information processing.

03:15.040 --> 03:20.040
Many people think intelligence is something mysterious that can only exist in biological

03:20.040 --> 03:21.040
organisms.

03:21.240 --> 03:28.400
I call that carbon chauvinism, this idea that you can only be smart if you're made of meat.

03:28.400 --> 03:34.080
And I think it's exactly the opposite idea that's powered the great progress in AI.

03:34.080 --> 03:39.000
The idea that it doesn't matter whether the information is processed by carbon atoms in

03:39.000 --> 03:42.840
neurons and brains, or by silicon atoms in our technology.

03:42.840 --> 03:46.160
It's the information processing itself that matters.

03:46.160 --> 03:56.440
And this simple idea has really, really transformed artificial intelligence, and greatly grown

03:56.440 --> 03:58.320
the power of this tech.

03:58.320 --> 03:59.320
Just think about it.

03:59.320 --> 04:05.520
Not long ago, this was the state of the art in robots trying to walk.

04:05.520 --> 04:11.720
It is extra embarrassing for me, because one of these is the MIT robot.

04:11.720 --> 04:13.960
And that was just six years ago.

04:13.960 --> 04:15.600
It was fast forward to today.

04:15.600 --> 04:16.960
Can you see any difference?

04:23.480 --> 04:30.640
In other words, the momentum in the field of artificial intelligence is really quite palpable.

04:30.640 --> 04:35.240
Not long ago, we didn't have self-driving cars.

04:35.240 --> 04:40.640
Now we have self-flying rockets that can land themselves with artificial intelligence.

04:44.080 --> 04:47.920
Not long ago, we couldn't do face recognition.

04:47.920 --> 04:53.000
Now we can do that great.

04:53.000 --> 04:56.680
We can even simulate your face saying all sorts of things you never said, and we have

04:56.680 --> 05:01.840
all these great tools that we heard about from Luca using artificial intelligence.

05:01.840 --> 05:05.920
Not long ago, AI could not save lives.

05:05.920 --> 05:11.040
I believe that soon artificial intelligence will eliminate more than one million pointless

05:11.040 --> 05:15.360
road deaths on the Earth's highways.

05:15.360 --> 05:20.680
And even more lives will be saved by eliminating stupid mistakes in health care.

05:20.680 --> 05:26.000
And still more lives will be saved by actually accelerating the pace of medical research.

05:26.000 --> 05:31.680
We already have AI that is as good as the best doctors at diagnosing prostate cancer,

05:31.680 --> 05:35.600
lung cancer, various eye diseases.

05:36.560 --> 05:42.000
The most lives of all I think in medicine will be saved by just accelerating the pace

05:42.000 --> 05:44.200
of scientific research itself.

05:44.200 --> 05:50.080
For example, for over 50 years, biologists have tried and failed to solve the protein

05:50.080 --> 05:55.760
folding problem, where you start with a genetic sequence of a protein and you try to figure

05:55.760 --> 06:01.560
out if it's going to fold up into a donut shape like the hemoglobin molecules that take

06:01.560 --> 06:04.840
oxygen to your brains right now or to some other shape.

06:04.840 --> 06:08.040
And then AI solved it.

06:08.040 --> 06:16.440
And it's really cool to just look at how Google DeepMind's AlphaFold AI takes the amino acid

06:16.440 --> 06:23.080
genetic sequence as input and just figures out how the protein folds up in 3D and really

06:23.080 --> 06:29.960
spectacular agreement also with expensive and slow measurements from X-ray crystallography.

06:29.960 --> 06:35.280
This is an example of AI for good, which I think can greatly accelerate drug discovery

06:35.280 --> 06:38.880
and drug development.

06:38.880 --> 06:44.680
Not long ago, staying on the theme of the growing power of AI, AI could not beat us at the Asian

06:44.680 --> 06:46.000
Board Game of Go.

06:46.000 --> 06:51.720
Raise your hand if you've ever tried playing Go.

06:51.720 --> 07:00.040
And then, of course, Google DeepMind's AlphaGo and then later AlphaZero, MuZero software,

07:00.040 --> 07:04.720
it took 3,000 years of human Go games and Go wisdom and put it all in the garbage can,

07:04.720 --> 07:10.160
became the best Go player in the world by playing against itself for 24 hours.

07:10.160 --> 07:15.960
And the most interesting thing of all was that it didn't just crush gamers, but it crushed

07:15.960 --> 07:21.520
AI developers like myself who had spent all this time hand-crafting code to do it, all

07:21.520 --> 07:22.800
made obsolete.

07:22.800 --> 07:29.840
And the same AI was able to learn not just this game, but also at the same time become

07:29.840 --> 07:39.160
the world's best player at chess, crushing stockfish and also at now many other games.

07:39.160 --> 07:45.800
This year, 2022, we've seen spectacular progress in large language models that we'll hear

07:45.800 --> 07:48.600
a lot more about later today.

07:48.600 --> 07:52.440
And other models that just take a lot of it, massive amounts of input, train on them and

07:52.440 --> 07:54.080
do cool things.

07:54.080 --> 07:58.640
The Dali 2 AI, if you tell it, to show you a picture of an armchair in the shape of

07:58.640 --> 08:01.960
an avocado.

08:01.960 --> 08:05.760
This is what it comes up with.

08:05.760 --> 08:10.720
One of all the language models so far I'm most impressed with is actually Google Palm,

08:10.720 --> 08:16.080
which just came out a couple of months ago.

08:16.080 --> 08:20.560
If you put in this joke into it like this, saying, hey, I was going to go fly visit my

08:20.560 --> 08:28.840
family on April 6, my mom said, oh, great, your stepdad's poet reading is that night.

08:28.840 --> 08:31.920
So now I'm flying in on April 7.

08:31.920 --> 08:43.080
When this was put into this AI, this is how the AI explains the joke.

08:43.080 --> 08:56.760
I'll shut up for a moment so you can read this.

08:56.760 --> 09:01.280
The one that freaked me out the most was this example.

09:01.280 --> 09:07.560
Same AI, so the input we give to it is Trevor has wanted to see the mountain with all of

09:07.560 --> 09:12.640
the heads on it for a long time, so he finally drove out to see it.

09:12.640 --> 09:19.600
What is the capital of the state that is directly east of the state that Trevor is currently

09:19.600 --> 09:24.480
in?

09:24.480 --> 09:26.000
You guys are really smart.

09:26.000 --> 09:31.560
Can any of you answer this?

09:31.560 --> 09:41.320
Just shout out if you have any thoughts.

09:41.320 --> 09:46.320
The mountain with all the heads on it, what's that?

09:46.320 --> 09:47.520
Mount Rushmore.

09:47.520 --> 09:48.840
All right.

09:48.840 --> 09:52.840
Where is Mount Rushmore?

09:52.840 --> 10:02.200
Any takers?

10:02.200 --> 10:03.200
This is pretty tough.

10:03.200 --> 10:07.800
I couldn't do it, even though I live in the US.

10:07.800 --> 10:14.960
So this is what the AI said, the mountain with all the heads on it is Mount Rushmore.

10:14.960 --> 10:17.360
Mount Rushmore is in South Dakota.

10:17.360 --> 10:21.600
The state directly east of South Dakota is Minnesota.

10:21.600 --> 10:24.640
The capital of Minnesota is St. Paul.

10:24.640 --> 10:29.640
The answer is St. Paul.

10:29.640 --> 10:38.160
As recently as January, February of this year, I remember hearing many AI researchers saying,

10:38.160 --> 10:41.160
oh, these large language models are just kind of fake.

10:41.160 --> 10:45.760
They train on massive amounts of data to predict the next word and so on, but they don't really

10:45.760 --> 10:46.920
understand anything.

10:46.920 --> 10:52.680
They can't really do profound logical reasoning.

10:52.680 --> 11:01.880
When you look at this, that criticism doesn't feel quite as convincing anymore, does it?

11:01.880 --> 11:05.960
So things are happening very, very fast.

11:05.960 --> 11:16.880
And to put this a little bit in context, this, the Google Palm model here, it has 540 billion

11:17.440 --> 11:22.120
parameters in the neural network that's been trained here.

11:22.120 --> 11:27.760
That's about a thousand times more than a mouse.

11:27.760 --> 11:29.760
Our symposium is called Synapse.

11:29.760 --> 11:35.080
A mouse has about 500 million synapses, which you could truly think of a single parameter

11:35.080 --> 11:36.080
each.

11:36.080 --> 11:40.000
And we humans, we have about 200 times more than that.

11:40.000 --> 11:45.520
To put this in context, if we think about the memory capacity of our computational devices

11:45.560 --> 11:54.880
over time, we see this familiar Moore's Law, where there's been a spectacular increase

11:54.880 --> 12:01.800
of a thousand million, million times growth since the 1950s.

12:01.800 --> 12:12.080
And here we are today, where you can now buy a little memory card about the size of my

12:12.120 --> 12:16.760
thumbnail, twice that, for 100 bucks.

12:16.760 --> 12:21.200
So it costs about $1 per terabyte.

12:21.200 --> 12:27.640
And if we compare this with the amount of memory or data in these models, we see that

12:27.640 --> 12:32.880
actually, you know, we're already far beyond the mouse.

12:32.880 --> 12:37.040
Google Palm sits about here, you know, this is about the amount of data in the Google

12:37.040 --> 12:38.040
Palm model.

12:38.160 --> 12:44.000
Humans are a little bit above there, but you can buy 100 of these little cards.

12:44.000 --> 12:47.280
That's about as much information as you have stored in your entire brain.

12:47.280 --> 12:53.640
The hardware has already more or less caught up with the human hardware in some ways, even

12:53.640 --> 12:56.720
though it's much less energy efficient and so on.

12:56.720 --> 13:01.600
It seems like we're mainly just limited by having much more stupid software in our machines

13:01.600 --> 13:05.800
than we have in our brains.

13:05.800 --> 13:07.840
This begs the question, though.

13:07.840 --> 13:09.800
How far will this go?

13:09.800 --> 13:15.880
We've talked here about how amazing the growth of power of artificial intelligence has been.

13:15.880 --> 13:19.640
And I hope with these examples, especially with the Mount Rushmore one, I can convince

13:19.640 --> 13:24.960
you that really, AI is happening, whether we like it or not.

13:24.960 --> 13:27.400
How far is it going to go?

13:27.400 --> 13:34.800
Well, I like to think about this question in terms of this abstract landscape of tasks,

13:34.800 --> 13:41.800
where the elevation represents how difficult it is for AI to do each task, and the sea

13:41.800 --> 13:48.560
level represents what computers can do today.

13:48.560 --> 13:53.960
The sea level is obviously rising, so an obvious takeaway from this is for all of you thinking

13:53.960 --> 13:59.360
about your careers, be careful with careers right at the waterfront, which are in the

13:59.360 --> 14:02.760
process of getting disrupted.

14:02.760 --> 14:10.520
But the bigger question is, of course, how high is the water eventually going to rise?

14:10.520 --> 14:16.840
Is it eventually going to submerge all land?

14:16.840 --> 14:23.920
This is the definition of artificial general intelligence, AGI.

14:23.920 --> 14:29.560
So with this definition, people who say, ah, you know, there'll always be jobs that humans

14:29.560 --> 14:38.880
can do better than machines, are simply saying that there will never be AGI.

14:38.880 --> 14:43.880
If you think this sounds like crazy science fiction, AGI, I want you to be clear on the

14:43.880 --> 14:48.000
fact that there is something else which sounds like even more crazy science fiction, the

14:48.000 --> 14:51.440
idea of super intelligence.

14:51.440 --> 14:56.320
An idea which is actually quite simple, the idea is that if we actually do get the AGI,

14:56.320 --> 15:01.600
and then, since by definition, machines can do all jobs better than humans, that includes

15:01.600 --> 15:08.400
the job of AI development and everything else that is done by bending spoons.

15:08.400 --> 15:14.680
So it opens up the controversial possibility that future AI development after that can

15:14.680 --> 15:20.400
be done much faster, replacing the typical human research and development timescale

15:20.400 --> 15:27.040
of years by months or weeks or hours or whatever it takes for machines to make better versions

15:27.040 --> 15:30.160
of their software, etc.

15:30.160 --> 15:36.080
And so if that happens, it opens up the possibility of a recursively self-improving technology

15:36.080 --> 15:40.120
which could quickly leave us humans far, far behind.

15:40.120 --> 15:43.520
Here, we need a bit of a reality check.

15:43.520 --> 15:49.920
Is this just crazy science fiction speculation by people who have no idea about technology

15:49.920 --> 15:50.920
really?

15:50.920 --> 15:54.480
I'll let you decide that for yourselves by playing a little clip from a conference that

15:54.480 --> 15:59.720
I was involved in organizing some years ago.

15:59.720 --> 16:02.840
It's a fair science controversy.

16:02.840 --> 16:06.680
On one hand, you have people like my former MIT colleague Rodney Brooks who would be like,

16:06.680 --> 16:10.760
ah, this is all just bullshit, you know, this won't happen for centuries.

16:10.760 --> 16:17.200
On the other hand, you have people like Demi Sassabis, CEO of Google DeepMind, gave us

16:17.200 --> 16:22.480
AlphaZero and so much else who think it is going to happen and who's betting their whole

16:22.480 --> 16:26.920
company on trying to build these things.

16:26.920 --> 16:35.160
So here is the little video clip I'll share with you.

16:35.160 --> 16:46.200
Let's see if we have any luck here.

16:46.200 --> 16:48.800
This is actually a very good metaphor for the whole talk.

16:48.800 --> 16:52.560
With technology, what can possibly go wrong?

16:52.560 --> 16:56.480
So before I asked if superintelligence is possible at all according to the laws of physics, now

16:56.480 --> 16:59.080
I'm asking, will it actually happen?

16:59.080 --> 17:02.240
Yes, no, or it's complicated?

17:02.240 --> 17:04.600
A little bit complicated, but yes.

17:04.600 --> 17:10.040
Yes, and if it doesn't, something terrible has happened to prevent it?

17:10.040 --> 17:11.040
Yes.

17:11.040 --> 17:12.040
Probably.

17:12.040 --> 17:13.040
Yes.

17:13.040 --> 17:14.040
Yes.

17:14.040 --> 17:15.040
Yes.

17:15.040 --> 17:16.040
Yes.

17:16.040 --> 17:17.040
Yes.

17:17.040 --> 17:18.040
Yes.

17:18.040 --> 17:19.040
Yes.

17:19.040 --> 17:20.040
No.

17:20.040 --> 17:25.640
So what do we make of that?

17:25.640 --> 17:33.000
Well, aside from the fact that Elon has a sense of humor, it's quite obvious that these

17:33.000 --> 17:37.400
people who are saying this are not all just the much of philosophers who don't know anything

17:37.400 --> 17:38.400
about physics.

17:38.400 --> 17:41.640
This, for example, is Demi Sassabis from Google DeepMind.

17:41.640 --> 17:43.680
There were a bunch of AI professors there, et cetera.

17:43.680 --> 17:47.160
So that doesn't prove that superintelligence will happen, but it means that we have to

17:47.160 --> 17:50.800
take it very seriously as an actual possibility.

17:50.800 --> 17:55.680
In terms of the much slower ambition of just replacing all human jobs with machines that

17:55.680 --> 18:00.840
can do it cheaper and better, in other words, AGI, there are actually recent surveys have

18:00.840 --> 18:07.160
shown that most AI researchers think this is going to happen within a matter of decades.

18:07.160 --> 18:12.800
So when I look at you, you all look healthy, like you're taking your vitamins, going to

18:12.800 --> 18:17.520
the gym, et cetera, who are going to be around in a few decades.

18:17.520 --> 18:23.440
So you should definitely be very open to the possibility that it might happen in your lifetime

18:23.440 --> 18:29.400
and think about, well, what does that imply for you and how can we make sure that this

18:29.400 --> 18:33.440
becomes the best thing ever to happen to humanity rather than the worst thing ever to happen

18:33.440 --> 18:34.440
to humanity?

18:34.440 --> 18:40.360
That is the defining challenge of our generation.

18:40.360 --> 18:45.320
I'm going to try this pointer and see if it works better, proving that Italian technology

18:45.320 --> 18:47.640
is better than American technology.

18:47.640 --> 18:48.640
Great works.

18:48.640 --> 18:49.640
Will that happen?

18:49.640 --> 18:51.640
Yes, no, or it's complicated.

18:51.640 --> 18:52.640
I shouldn't have said that.

18:52.640 --> 18:53.640
A little bit complicated, but yes.

18:53.640 --> 18:55.640
Okay, well, try both pointers.

18:55.640 --> 18:56.640
Yes.

18:56.640 --> 18:57.640
There.

18:57.640 --> 18:59.360
All right.

18:59.360 --> 19:05.720
So if this happens and we get to artificial general intelligence and then it starts taking

19:05.720 --> 19:08.040
off towards superintelligence, what does that really mean?

19:08.040 --> 19:11.800
Well, one thing it's obviously going to mean is that we should start imagining technology

19:11.800 --> 19:16.480
which is limited not by human intelligence the way it is today, but limited instead by

19:16.480 --> 19:21.640
the laws of physics, and that's a huge, huge difference.

19:21.640 --> 19:28.360
So just to get you thinking big again, we already looked at the exponential growth,

19:28.360 --> 19:30.080
for example, of memory technology.

19:30.080 --> 19:34.680
Well, these technologies, even though they're cool, are still pretty stupid.

19:34.680 --> 19:39.880
It still takes 100 billion atoms to store each one bit.

19:39.880 --> 19:40.880
That's pretty dumb.

19:40.880 --> 19:42.760
I mean, you have 100 billion neurons in your brain.

19:42.760 --> 19:46.600
Imagine if you're only using one of them to store information in it.

19:46.600 --> 19:51.640
So that suggests you can do way better quite easily as you start using quantum tech to

19:51.640 --> 19:54.480
get down closer to one bit per atom.

19:54.480 --> 20:00.120
If you go and look at other things you might care about, like computation, how much can

20:00.120 --> 20:03.240
you compute per dollar?

20:03.240 --> 20:07.800
The trend has been even more spectacular in terms of the performance cost.

20:07.800 --> 20:11.220
We've gone down 19 orders of magnitude in price.

20:11.220 --> 20:16.640
If you cut the prices of everything in Italy and the world by 10 to the power of 19, you

20:16.640 --> 20:20.760
know, a thousand billion, billion, billion times, you could buy all the economic production

20:20.760 --> 20:24.960
of the world for less than one cent.

20:24.960 --> 20:31.840
And here again, if we get limited by the laws of physics, we're nowhere near the limits.

20:31.840 --> 20:35.960
Professor Seth Floyd at MIT actually calculated what the physical limits are before your computer

20:35.960 --> 20:40.960
turns into a black hole or anything like that, or violating speed of light limits.

20:40.960 --> 20:47.160
And this is to scale how far we are from the physical limits.

20:47.160 --> 20:52.280
We talk a lot now in Italy and elsewhere about energy, crisis, and energy independence.

20:52.280 --> 20:58.080
Look how terribly inefficient our technology is today, like burning coal and gasoline.

20:58.080 --> 21:02.360
This is how many percent of the energy you actually get out of it.

21:02.360 --> 21:07.720
With nuclear reactors, you do it a little bit better, but it's still quite pathetic compared

21:07.720 --> 21:10.680
to the limits that Einstein say are put by nature.

21:10.680 --> 21:15.640
And we already have technological ideas like spalarizers and things like this, which if

21:15.640 --> 21:21.120
you have superintelligence you could build, just do dramatically better than this.

21:21.120 --> 21:23.160
Raise your hand if you like to travel.

21:23.160 --> 21:27.240
All right, so there's a cool universe out there, right?

21:27.240 --> 21:33.800
So far, we haven't even been anywhere further, we humans, than going to the moon, but obviously

21:33.800 --> 21:38.960
if you have suddenly enhanced our technology to be limited by the laws of physics rather

21:38.960 --> 21:45.120
than by our intelligence, things look a lot more rosy instead of having to wait thousands

21:45.120 --> 21:49.200
or millions of years like in science fiction novels to have this tech, you could have it

21:49.200 --> 21:51.520
in your lifetime.

21:51.520 --> 21:59.080
And it becomes suddenly quite straightforward to go visit other stars, even other galaxies.

21:59.080 --> 22:03.280
You can ask me in the Q&A if you want a little bit about how to do it, but I just threw in

22:03.280 --> 22:09.600
this little thing here to just show you how much more resources there are out there in

22:09.600 --> 22:13.680
the cosmos compared to the resources that we keep fighting about on Earth when we have

22:13.680 --> 22:19.200
our stupid little wars.

22:19.200 --> 22:24.120
And even here, even with human intelligence, there have been all sorts of very clever solutions

22:24.120 --> 22:28.840
that people have figured out, like how to go live in the solar system and a fun habitat,

22:28.840 --> 22:31.560
which is solar powered, etc.

22:31.560 --> 22:38.120
We could build these sort of things very quickly with robots that were controlled with AI if

22:38.120 --> 22:39.920
we wanted to.

22:40.920 --> 22:45.640
All right, so we've spent most of our time talking about the growing power of artificial

22:45.640 --> 22:51.120
intelligence, and the key message that I want you to take away from this again is the fact

22:51.120 --> 23:01.800
that artificial intelligence really is happening for better or for worse, and the pace of progress

23:01.800 --> 23:04.960
in the field is just absolutely amazing.

23:04.960 --> 23:11.480
And if it continues, and we get to artificial general intelligence and beyond, we will have

23:11.480 --> 23:15.360
enormous opportunities, we'll become the masters of our own destiny.

23:15.360 --> 23:21.840
Instead of being running around trying to not get stepped on or eaten by tigers, we will

23:21.840 --> 23:25.440
be making the decisions about our future.

23:25.440 --> 23:26.760
What should those decisions be?

23:26.760 --> 23:29.200
How can we make sure that this becomes good?

23:29.200 --> 23:35.800
As Luca said, if you think about, even without worrying about what machines might do to you,

23:35.800 --> 23:44.280
if you just offer a moment to imagine your least favorite leader on the planet, don't

23:44.280 --> 23:47.840
tell me who it is, but just imagine their face for a second, okay?

23:47.840 --> 23:53.920
Imagine now that they are the ones who control AGI and use it to take over the world.

23:53.920 --> 23:55.440
How does that make you feel?

23:59.440 --> 24:00.440
Great.

24:02.680 --> 24:04.160
Or less so.

24:04.160 --> 24:10.720
So how can we steer in the progress of this technology towards an inspiring future?

24:10.720 --> 24:11.720
Let's talk a little bit about this.

24:11.720 --> 24:17.200
To help with this issue, I teamed up with some colleagues and we created the Future Life Institute,

24:17.200 --> 24:21.960
whose goal is precisely for the future life to be awesome and not awful.

24:22.040 --> 24:30.280
We've done a lot of things to help focus people's thinking about how to steer.

24:30.280 --> 24:37.960
We, for example, had conferences where we developed principles for guiding the beneficial

24:37.960 --> 24:43.920
use of artificial intelligence, which have been signed by thousands of leading AI researchers.

24:43.920 --> 24:49.360
They've even been adopted into law by the state of California and inspired a lot of

24:49.360 --> 24:52.720
the OECD principles here in Europe, et cetera.

24:52.720 --> 24:58.600
And I'll just give a couple, a few quick, very quick examples of such principles.

24:58.600 --> 25:06.720
Try this one, try this one, there.

25:06.720 --> 25:11.320
If you see me struggling too much with a clicker, you can just press the right arrow on the

25:11.320 --> 25:12.840
laptop back there.

25:12.840 --> 25:17.520
One principle is that we should, like, you know, all technologies can be used to help

25:17.520 --> 25:20.000
people or for new ways of harming people.

25:20.000 --> 25:25.480
So biologists and chemists, for example, have been working very hard to make sure that they

25:25.480 --> 25:30.840
ban bio weapons and ban chemical weapons so we can use their sciences for new machines

25:30.840 --> 25:32.960
and materials instead.

25:32.960 --> 25:38.120
And of course, AI researchers are an idealistic bunch, as those of you who work on it know,

25:38.120 --> 25:42.280
who want to make sure we use artificial intelligence for new solutions also, not for just new ways

25:42.280 --> 25:46.200
of killing people with slaughterbots or whatever.

25:46.200 --> 25:51.800
Another principle that there's very broad agreement on is that we really need to make

25:51.800 --> 25:57.080
sure that all the enormous growth in the economic pie benefits everybody.

25:57.080 --> 26:02.080
And my opinion is that if we can produce this future abundance of goods and services with

26:02.080 --> 26:06.840
artificial intelligence, and we still cannot figure out how to share this in such a way

26:06.840 --> 26:12.640
that everybody on Earth gets better off, then shame on us.

26:12.640 --> 26:16.040
And it's fun to talk about this in Europe because I feel Italy and Europe in general

26:16.040 --> 26:21.160
has a much stronger tradition actually in this regard of asking the question, how can

26:21.160 --> 26:30.120
we make our growing economy work for all of us, not just for some of us?

26:30.120 --> 26:36.680
A third principle is, well, raise your hand if your computer has ever crashed.

26:36.680 --> 26:41.440
That's a lot of hats.

26:41.440 --> 26:49.160
So how did that make you feel?

26:49.160 --> 26:50.160
Frustrated.

26:50.160 --> 26:59.600
All right, now suppose this computer was actually in charge of the nearby nuclear power plant

26:59.600 --> 27:05.760
or the U.S. nuclear arsenal or something else that really affects people's lives.

27:05.760 --> 27:07.600
Frustrated probably would not be the first word you would use.

27:07.600 --> 27:09.560
You'd probably use a stronger one, right?

27:09.560 --> 27:15.800
So this means that as AI gets more powerful, it becomes ever more important to work on AI

27:15.800 --> 27:16.800
safety research.

27:16.800 --> 27:22.560
And this is actually an encouragement for all of those of you who work in AI.

27:22.560 --> 27:26.000
Don't just think about how you can work to make it more powerful.

27:26.000 --> 27:30.280
Also think about the technical work you might be able to contribute to, to make it safe

27:30.280 --> 27:40.560
and robust and beneficial so it actually does what we want to do.

27:40.560 --> 27:51.840
Finally, it's part of this has to also involve thinking about putting goals into our machines

27:51.840 --> 27:55.080
that are aligned with human goals.

27:55.080 --> 27:59.400
Because the greatest risk from very powerful artificial intelligence is not that it's going

27:59.400 --> 28:05.360
to turn evil like in stupid Hollywood movies, it's that it's going to just turn very competent

28:05.360 --> 28:11.480
and go out and accomplish goals that are not aligned with our goals.

28:11.480 --> 28:18.080
If we think about this guy, for example, the West African black rhino, why did we humans

28:18.080 --> 28:19.240
drive it extinct?

28:19.240 --> 28:22.760
Is it because we are actually a bunch of evil rhinoceros haters?

28:22.760 --> 28:23.760
No.

28:23.760 --> 28:27.440
It's that we are more intelligent than they were, and our goals are not aligned with

28:27.440 --> 28:30.200
their goals and tough luck for them.

28:30.200 --> 28:37.920
Let's not put humanity in the place of those rhinos by giving a bunch of power to machines

28:37.920 --> 28:46.280
who don't share our goals or to humans who control machines that don't have our goals.

28:46.280 --> 28:47.720
We can talk a lot about this.

28:47.720 --> 28:51.720
This is also partly a very technical problem, actually, how to make machines understand our

28:51.720 --> 28:54.880
goals, learn our goals, retain their goals.

28:54.880 --> 28:59.320
Also a problem, of course, of making sure that we give the right incentives to the corporations

28:59.320 --> 29:05.640
and governments that, in turn, have power over the machines.

29:05.640 --> 29:15.240
My research that my group at MIT focuses on is very much dedicated to technical artificial

29:15.240 --> 29:23.560
intelligence research, and we don't have time for me to go into it in any great detail

29:23.680 --> 29:27.080
at all, but I'll just give you a little bit of a flavor.

29:27.080 --> 29:34.480
So today, the vast majority of the really high-powered AI systems, like the language

29:34.480 --> 29:39.600
models that we looked at a little bit here, and you'll hear more about today, like Google

29:39.600 --> 29:43.400
Palm and GPT-3 and so on, are gigantic black boxes.

29:43.400 --> 29:48.600
They do very intelligent things, but we really don't understand how they work, which limits

29:48.600 --> 29:51.720
the extent to which you can trust them.

29:51.760 --> 29:54.680
We are very focused on how can we open up the black box?

29:54.680 --> 30:03.400
How can we use the training of a giant black box AI model as the first step, rather than

30:03.400 --> 30:05.560
the last step?

30:05.560 --> 30:11.800
So our vision is, first, you create something like this that learns something very smart,

30:11.800 --> 30:17.120
and then, instead of just selling it at that point, you do a second step and try to get

30:17.120 --> 30:19.680
the knowledge out of it.

30:19.680 --> 30:23.200
So you can put the knowledge into something which you can trust more.

30:23.200 --> 30:27.800
We've had some success with very basic things, like if you have a big table of numbers, for

30:27.800 --> 30:32.080
example, and you want to predict the last column from the previous ones, this is called

30:32.080 --> 30:35.040
symbolic regression.

30:35.040 --> 30:40.920
It's the nonlinear NP-hard version of what's known as linear regression, which is very

30:40.920 --> 30:41.920
easy.

30:41.920 --> 30:46.960
This normally takes longer than the age of the universe to discover simple formulas,

30:46.960 --> 30:48.640
in general.

30:48.640 --> 30:51.960
But we were able to get state-of-the-art performance on this.

30:51.960 --> 30:56.760
We first train a black box neural network, and then we use all sorts of techniques by

30:56.760 --> 31:02.680
looking at the gradients and so on to figure out how we can be broken apart into modules.

31:02.680 --> 31:07.720
And then we do this in a recursive way that I can tell you about over coffee if you're

31:07.720 --> 31:09.200
interested.

31:09.200 --> 31:13.920
And eventually, the thing works so well that we were able to put in the 100 most famous

31:13.920 --> 31:18.800
physics equations from the Feynman Lecturers and from other books.

31:18.800 --> 31:24.280
And by just showing data, it was able to not just fit them accurately with a sort of black

31:24.280 --> 31:28.840
box system that you don't understand, but actually discover what the formulas were so

31:28.840 --> 31:31.120
you could extract out the knowledge.

31:31.120 --> 31:36.840
This is very inspired, for me, I was very inspired to do this project by an Italian,

31:36.840 --> 31:39.440
Galileo Galilei.

31:39.440 --> 31:45.560
Because when he was four years old, if his mother threw him an apple, of course he could

31:45.560 --> 31:47.360
catch it.

31:47.360 --> 31:50.720
Because his neural network that he had trained during his childhood in his brain was very

31:50.720 --> 31:57.000
good at predicting the shape in which apples moved under the influence of gravity.

31:57.000 --> 32:00.080
And so you could do the same when you were four years old.

32:00.080 --> 32:04.520
But when Galileo got older, he did something more.

32:04.520 --> 32:10.000
He said, hmm, can I take this intuitive knowledge that I have that I don't know how it's represented

32:10.000 --> 32:14.040
and somehow get it out of my brain?

32:14.040 --> 32:20.040
And he said, well, this curve, it's a parabola, regardless of what kind of apple it is and

32:20.040 --> 32:21.280
how fast it's thrown.

32:21.280 --> 32:23.840
And there's an equation for it, y equals x squared.

32:23.840 --> 32:28.840
And he was able to tell his friends about this and publish papers.

32:28.840 --> 32:38.480
So this ability to transform poorly understood intuitive knowledge into symbolic representations

32:38.480 --> 32:41.380
is really at the heart of human intelligence.

32:41.380 --> 32:44.640
When you speak Italian with your friends, you're doing exactly the same thing.

32:44.640 --> 32:48.840
You're taking some knowledge that you have in your brain and you're verbalizing it, putting

32:48.840 --> 32:53.560
it in a symbolic representation where it can be communicated and explained.

32:53.560 --> 32:58.680
So if we can do it, I believe that we can make machines that can also do it.

32:58.680 --> 33:03.160
And they will be much more safe and trustworthy that way because the hard part is usually

33:03.160 --> 33:04.680
discovering the knowledge.

33:04.680 --> 33:09.520
Once you have the knowledge, you can implement it in something maybe which is not the standard

33:09.520 --> 33:12.720
neural network, but something that you really, really understand.

33:12.720 --> 33:18.800
So this was just one little hopeful example of how I think that we can do much better

33:18.800 --> 33:25.320
in terms of trustworthiness of our AI systems going forward than we should.

33:25.680 --> 33:30.160
And in my final two minutes, let's talk just a little bit about the destination also.

33:30.160 --> 33:34.200
So we're making this ever more powerful tech.

33:34.200 --> 33:39.440
I mentioned various ideas for how we can steer it better, control it better, trust it better.

33:39.440 --> 33:43.640
But what do we ultimately want to do with it?

33:43.640 --> 33:48.720
How can we ensure an inspiring future with artificial intelligence, both for businesses

33:48.720 --> 33:52.720
like Bending Spoons and for our entire civilization?

33:53.560 --> 34:00.400
I've already alluded to one important strategy, which is just to think about all the uses

34:00.400 --> 34:08.280
of AI and then draw a very clear red line between what we consider acceptable uses and

34:08.280 --> 34:12.000
unacceptable uses.

34:12.000 --> 34:16.240
I would encourage you to keep this very ethical framework in the back of your mind in your

34:16.240 --> 34:21.240
future career also whenever you build something, ask yourself also what the social impact

34:21.240 --> 34:23.800
of it will be.

34:23.800 --> 34:31.400
And the second strategy is we really need to articulate positive visions for what we want

34:31.400 --> 34:38.520
to accomplish with this because shared positive visions are the fundamental driver of collaboration

34:38.520 --> 34:43.920
in companies, in relationships, and in the world.

34:43.920 --> 34:50.920
And it's easy to say, well, we'll never find any goals for the future that the US and China

34:51.120 --> 34:53.960
and Italy are all going to agree on.

34:53.960 --> 34:55.720
But that's obviously not true.

34:55.720 --> 35:01.080
The United Nations Sustainable Development Goals, for example, are quite ambitious visions

35:01.080 --> 35:03.600
and basically every country on the planet agrees with it.

35:03.600 --> 35:07.760
We even wrote this paper here in Nature Commons recently about how AI can help us accomplish

35:07.760 --> 35:10.720
the Sustainable Development Goals faster.

35:10.720 --> 35:16.880
And if we get ever more advanced AI, we can go much more ambitious than that also and

35:16.880 --> 35:23.880
say, well, let's not just try to reduce a few problems a little bit, but really actually

35:24.680 --> 35:28.440
solve them and just do dramatically better.

35:28.440 --> 35:31.760
The sky is the limit.

35:31.760 --> 35:38.760
So in summary, I feel that artificial general intelligence is the ultimate game-changing

35:38.880 --> 35:45.080
technology for our human species, and it is coming.

35:45.080 --> 35:49.680
And what does that mean, exactly?

35:49.680 --> 35:56.680
First of all, it means we're not just some tiny little life forms here on a tiny little

35:56.680 --> 36:02.320
planet that's completely irrelevant because we're so small.

36:02.320 --> 36:08.320
What's happening on this planet in your lifetime is probably the most significant thing ever

36:08.400 --> 36:14.640
to happen anywhere in our universe so far, and could easily affect the entire future

36:14.640 --> 36:21.640
of much of this gorgeous and beautiful universe that you're looking at in the background.

36:21.640 --> 36:28.640
So my charge to you folks is be proactive and think about how can you steer this technology

36:30.160 --> 36:32.440
to a good place?

36:32.440 --> 36:38.680
Let's be the masters of our own destiny by envisioning it the way we want it to be and

36:38.680 --> 36:40.840
by actually building it.

36:40.840 --> 36:41.840
Thank you.

36:41.840 --> 36:42.840
Thank you.

36:42.840 --> 36:43.840
That was great.

36:43.840 --> 36:44.840
Thank you very much.

36:44.840 --> 36:45.840
Stay there for a minute.

36:45.840 --> 36:46.840
Thank you very much, Max.

36:46.840 --> 36:57.840
That was absolutely awesome.

36:57.840 --> 37:01.800
I want to move things along, but we can't let Max go without asking him a few questions.

37:01.800 --> 37:03.440
This is your opportunity.

37:03.440 --> 37:05.920
Who would like to ask a question?

37:05.920 --> 37:06.920
Who will be brave and ask that?

37:06.920 --> 37:08.680
Do you want me to ask the first question?

37:08.680 --> 37:09.680
Okay.

37:09.680 --> 37:10.760
All right then.

37:10.760 --> 37:16.040
You talked about, I've got about five questions I want to ask you.

37:16.040 --> 37:17.920
We're not going to have time for all of these.

37:17.920 --> 37:23.000
Are we better at seeing how AI works?

37:23.000 --> 37:30.000
I'm reminded of the example of Microsoft's Tay when it was your friend in your pocket

37:30.000 --> 37:33.760
that you would have a conversation with and it randomly started to be racist and insult

37:33.760 --> 37:38.280
people and they turned it off because they didn't know why it was doing it.

37:38.280 --> 37:45.040
Are we better now at looking, opening up the brain and looking inside it and seeing why

37:45.040 --> 37:50.120
it is doing things as opposed to looking at the outcome?

37:50.120 --> 37:51.120
Very good question.

37:51.120 --> 37:55.520
We're a little bit better, but in the meantime the technology has gotten dramatically more

37:55.520 --> 37:56.520
powerful.

37:56.520 --> 37:58.880
So we really need to up our game in this area.

37:58.880 --> 38:03.760
I really do believe that this is one of the most valuable things we can do for a good

38:03.760 --> 38:10.300
future to win this wisdom race between the growing power of the tech and the growing

38:10.300 --> 38:12.920
wisdom with which we understand it.

38:12.920 --> 38:16.200
I hope there are some of the examples I mentioned that suggest that it's not as hopeless at

38:16.200 --> 38:18.880
all as people think.

38:18.880 --> 38:25.400
After all, we humans are able to extract our own insights in a fashion we can explain to

38:25.400 --> 38:31.960
others and we really should be able to get the AIs to do the same for us.

38:31.960 --> 38:32.960
Any questions?

38:32.960 --> 38:33.960
Yes, sir.

38:33.960 --> 38:34.960
There we go.

38:34.960 --> 38:35.960
This works.

38:35.960 --> 38:36.960
It does.

38:36.960 --> 38:37.960
Bonjour, everybody.

38:37.960 --> 38:45.680
So I was thinking about the super intelligence.

38:45.680 --> 38:50.000
We could see it as our next big step in evolution.

38:50.000 --> 38:51.640
We're already moving so far.

38:51.640 --> 38:58.720
The older generations are having really troubles in understanding AI, the power, the steering

38:58.720 --> 39:00.520
and the direction of it.

39:00.520 --> 39:08.240
So I wanted to hear your take on do you have a feeling of when the overtaking, the super

39:08.240 --> 39:18.320
intelligence might happen and how can we gracefully go in that direction, make it gracefully

39:18.320 --> 39:22.040
modern within the society, let's say.

39:22.040 --> 39:23.040
Great.

39:23.040 --> 39:31.080
So as to the question of when, you know, the most reliable polls of AI researchers suggest

39:31.080 --> 39:38.520
maybe a 30 years, but it could be a lot sooner, it could be later, but I think it's very likely

39:38.520 --> 39:43.720
to be within your lifetime, which is really the number one thing to take away from that.

39:43.720 --> 39:49.560
In terms of how to make it graceful and make it something good, I think we have both a

39:49.560 --> 39:53.320
bunch of these technical challenges, again, how can we make AI systems that we actually

39:53.320 --> 40:00.720
trust to do what we want them to do, and how can we make sure that we improve our democracy

40:00.720 --> 40:06.320
so that the control over this ever greater power actually lies in the hands of all of

40:06.320 --> 40:07.320
us.

40:07.320 --> 40:12.760
I personally believe that the only way in which you can guarantee that things actually can

40:12.760 --> 40:16.760
get better for everybody is if everybody has a say in how it's used.

40:16.760 --> 40:22.000
I think right now, especially in America where I live, things are going pretty rapidly in

40:22.000 --> 40:28.320
the opposite direction, where ever more power gets concentrated into ever fewer hands and

40:28.320 --> 40:35.560
you even have large social media companies starting to get almost a monopoly of the truth,

40:35.560 --> 40:38.280
which gives them even more power.

40:38.280 --> 40:44.840
And as a scientist, I much prefer the democratic idea that everybody should be able to challenge

40:44.840 --> 40:48.760
everything and to make it.

40:48.760 --> 40:54.960
So Europe is, I think, actually totally key in this.

40:54.960 --> 40:59.200
We have this tradition in Europe of trying to build a society that ultimately really

40:59.200 --> 41:00.680
works for everybody.

41:00.680 --> 41:07.920
That's why we have free healthcare and free universities in Italy, not in the United States.

41:07.920 --> 41:15.760
And I would encourage you all to sort of envision how can we reinvent, reimagine sort of the

41:15.760 --> 41:23.040
welfare state 3.0, which is even more awesome because it has all this technology and remains

41:23.040 --> 41:30.600
very firmly aligned with what's actually good for people, all people, not just some

41:30.600 --> 41:31.600
tech nerds.

41:31.600 --> 41:32.600
Thanks.

41:32.600 --> 41:36.600
Does anyone else have a question?

41:36.600 --> 41:38.600
Yes, please.

41:38.600 --> 41:43.640
Oh, okay.

41:43.640 --> 41:53.840
So my question is, do you think, so let's say we are able to develop AGI, but we cannot

41:53.840 --> 42:01.680
impose like, so if we have the ability to develop AGI, and if this means that we cannot

42:01.680 --> 42:07.200
impose like, go alignment limits on it, should we do it?

42:07.200 --> 42:12.880
To have something intelligent as humans, it should have also like the bad qualities of

42:12.880 --> 42:13.880
humans.

42:13.880 --> 42:24.360
So if we can't control it, should we still build it?

42:24.360 --> 42:26.480
On one hand, of course not.

42:26.480 --> 42:32.680
On the other hand, the way the economy works, I think it's not realistic to say, let's

42:32.680 --> 42:38.760
just press pause on technology until we got our act together, because there's just so

42:38.760 --> 42:41.720
much money in it and so much power in it.

42:41.720 --> 42:47.800
I think what's the more realistic game plan is rather than trying to go, stop, stop, stop.

42:47.800 --> 42:52.360
To think of this as a race between the growing power of the technology and the growing wisdom

42:52.360 --> 42:56.240
with which we manage it, solve all of these problems.

42:56.240 --> 43:01.640
If we can't slow down the growth of power, what we can do is accelerate the growth of

43:01.640 --> 43:03.160
the wisdom.

43:03.160 --> 43:07.080
That's why it's so wonderful that Luca and his colleagues are organizing conversations

43:07.080 --> 43:09.520
about precisely this.

43:09.520 --> 43:11.440
How can we grow the wisdom?

43:11.440 --> 43:14.440
How can we invest more in AI safety research?

43:14.440 --> 43:21.720
How can we have more conversations in society about how we want to use this, how it should

43:21.720 --> 43:25.040
be regulated, who should be in charge, and so on?

43:25.040 --> 43:31.440
That I think is our best shot that we have.

43:31.440 --> 43:37.880
In Europe right now, the European Union is actually developing the EU AI Act, which is

43:37.880 --> 43:43.160
the first time in the West that there will be actually a law trying to steer things in

43:43.160 --> 43:44.160
the right direction.

43:44.160 --> 43:46.000
I personally think this is very exciting.

43:46.000 --> 43:50.160
In America, there's no meaningful attempts to regulate AI at all, because lobbyists are

43:50.160 --> 43:53.840
too powerful there.

43:53.840 --> 43:59.160
Sure enough, big social media companies from America have now sent more lobbyists to Brussels

43:59.160 --> 44:05.560
to fight this than the oil companies ever sent to Brussels.

44:05.560 --> 44:08.680
As Europeans, be mindful of this.

44:08.680 --> 44:12.800
This might be the first battle you can actually win, where you start laying down the ground

44:12.800 --> 44:19.240
rules in such a way that you score up a big victory for the wisdom.

44:19.240 --> 44:24.000
Then right here, I have a question.

44:24.000 --> 44:25.000
Good morning.

44:25.000 --> 44:27.000
Thank you for your presentation.

44:27.000 --> 44:29.840
My question is about your program.

44:29.840 --> 44:36.120
You said that you are a physicist, so I want to know how was your transition to AI and

44:36.120 --> 44:38.240
what was your challenges?

44:38.240 --> 44:39.240
Thank you.

44:39.240 --> 44:40.240
Thank you.

44:40.240 --> 44:42.520
Yeah, that's right.

44:42.520 --> 44:47.960
When I was a teenager, I used to lie in my hammock between two apple trees and think about and

44:47.960 --> 44:51.120
realize that I was just really excited by big questions.

44:51.120 --> 44:55.960
The two biggest was our universe out there, which is also what my first book was about,

44:55.960 --> 45:01.640
and then the second one was our universe in here, intelligence and the mind.

45:01.640 --> 45:07.320
Seven years ago, roughly, after spending my career on physics, I decided to dramatically

45:07.320 --> 45:11.580
change careers and start doing AI research instead.

45:11.580 --> 45:19.700
I could get away with it in terms of my job because they can't fire me, an MIT, whatever

45:19.700 --> 45:22.900
I do, a tenure.

45:22.900 --> 45:24.300
Your question is very good.

45:24.300 --> 45:25.420
How hard was it?

45:25.420 --> 45:31.340
I would say it certainly was quite hard to learn so many things in a new field, but it

45:31.340 --> 45:34.380
was also really fun.

45:34.380 --> 45:35.900
It's such a fascinating topic.

45:35.900 --> 45:36.900
You know that.

45:36.900 --> 45:37.900
That's why you're here.

45:38.220 --> 45:42.860
And I was actually surprised also by how much I could make use of things I knew from my

45:42.860 --> 45:44.380
past life.

45:44.380 --> 45:50.620
So in physics, I had worked a lot on dealing with large data sets and information theory.

45:50.620 --> 45:55.380
I love computers and apps, which is why as we heard in the intro, I wrote games when

45:55.380 --> 45:59.780
I was a teenager and so on.

45:59.780 --> 46:03.820
The message I have for all of you is if you're ever thinking about a career change in the

46:03.820 --> 46:09.940
future and you're really, really excited about it and thought it through a little bit, you

46:09.940 --> 46:11.780
go for it.

46:11.780 --> 46:17.340
You get one shot to live on this planet, so make it count.

46:17.340 --> 46:20.780
Yeah, another question.

46:20.780 --> 46:26.940
This is a very broad question, but do you think AI has to be understandable in order to be

46:26.940 --> 46:28.940
ethical or trustworthy?

46:28.940 --> 46:31.020
Oh, that's good.

46:31.020 --> 46:38.300
Do we need it to be understanding in order to be ethical or what was the last word?

46:38.300 --> 46:42.580
For trustworthy, yes.

46:42.580 --> 46:49.540
For trustworthy, for sure, I think we should trust things, machines, not because some sales

46:49.540 --> 46:51.660
representative says, oh, trust this.

46:51.660 --> 46:55.900
It's great that it has a little sticker on it saying AI, but rather because we can understand

46:55.900 --> 46:59.740
how it works.

47:00.700 --> 47:05.420
On the other hand, just because it does what it's supposed to does not in any way guarantee

47:05.420 --> 47:07.060
it's ethical.

47:07.060 --> 47:14.820
If some terrorist has built this slaughter bot, then it's very trustworthy and he programs

47:14.820 --> 47:21.100
it to go kill all people with a certain skin color, for example, it's completely trustworthy.

47:21.100 --> 47:27.020
It's going to obey its owner even if it's by maybe my standards completely unethical.

47:27.100 --> 47:29.100
Those are two separate things.

47:29.100 --> 47:34.380
What that means simply is that, yes, of course, we have to first of all make things trustworthy,

47:34.380 --> 47:39.660
otherwise we don't even have the luxury of talking about ethics, but it's not enough.

47:39.660 --> 47:46.660
We also have to have a very serious conversation in our society about how to make sure that

47:46.660 --> 47:52.020
we align the goals of people and companies and governments who have this technology to

47:52.020 --> 47:56.620
do what's good for society as a whole.

47:56.620 --> 48:02.060
I think we're not doing so great there either today.

48:02.060 --> 48:08.460
If you have a company that decides to chop down the rainforest or whatever, maybe their

48:08.460 --> 48:16.380
technology they use to do it is very trustworthy for them, but companies are also a kind of

48:16.380 --> 48:22.140
artificial intelligence, even though it's built out of people, not out of machines.

48:22.140 --> 48:27.140
With the same alignment challenge we have of making sure that the machines we build

48:27.140 --> 48:34.380
do what's good for humanity, we also have to apply that same approach to companies and

48:34.380 --> 48:37.540
other entities that control machines.

48:37.540 --> 48:41.260
Thank you very much.

48:41.260 --> 48:44.540
One more question here, another one over there as well.

48:44.540 --> 48:46.260
We are running out of time.

48:46.260 --> 48:50.540
If you have questions actually for Max, we're going to be doing a panel later on, so come

48:50.540 --> 48:57.860
and find me and we can ask those questions later on as well.

48:57.860 --> 48:59.860
Over here.

48:59.860 --> 49:05.460
One question about the James Webb Telescope, it's been like 10 days that it's operative,

49:05.460 --> 49:11.540
so I wanted to ask you how is AI going to impact the research on these infrared images

49:11.540 --> 49:18.500
that we now have about the deep universe, because we've never been able to look so deep.

49:18.500 --> 49:25.140
How is going to be the AI impact on that, and do we have some future projects on it?

49:25.140 --> 49:32.660
I'm very excited about the opportunity of using AI for science more broadly, and astronomy

49:32.660 --> 49:37.060
and extragalactic astrophysics is a great example of this, because we're getting such

49:37.060 --> 49:41.780
enormous amounts of data, that you just cannot do what you would do in the past, it's like

49:41.780 --> 49:46.300
give it to a grad student to look at it all and come back and tell you what they found.

49:46.580 --> 49:50.100
It'll go away for 100 years.

49:50.100 --> 49:56.220
We have already been quite successful using AI to analyze enormous amounts of astronomical

49:56.220 --> 50:01.620
data to figure out, to find all the stars in the galaxies in there, to figure out what's

50:01.620 --> 50:06.780
there, what's different, what's surprising.

50:06.780 --> 50:15.020
I think 10 years from now it'll be almost as difficult to find a physicist or astrophysicist

50:15.020 --> 50:20.580
who does not use any kind of machine learning, as it is today to find someone in those fields

50:20.580 --> 50:22.340
who says, I don't use mathematics.

