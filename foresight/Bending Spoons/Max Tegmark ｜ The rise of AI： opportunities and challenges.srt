1
00:00:00,000 --> 00:00:09,280
It's such a pleasure to be here with you today to talk about the rise of artificial

2
00:00:09,280 --> 00:00:15,920
intelligence that you heard about and what both the opportunities are and the challenges

3
00:00:15,920 --> 00:00:19,160
that we need to overcome to get these opportunities.

4
00:00:19,160 --> 00:00:24,440
So I want to encourage you to think big.

5
00:00:24,440 --> 00:00:31,040
I love the historical intro we got with the toilet paper roll and the dinosaurs reminding

6
00:00:31,040 --> 00:00:34,120
us of the big picture.

7
00:00:34,120 --> 00:00:39,920
So if we zoom out even bigger, as we see here, here we are at 13.8 billion years after our

8
00:00:39,920 --> 00:00:44,480
big bang and realizing that something remarkable has happened.

9
00:00:44,480 --> 00:00:50,520
Finally, after all this time, our universe has woken up and become aware of itself through

10
00:00:50,520 --> 00:00:54,760
us conscious beings on this little spinning blue ball in space.

11
00:00:54,760 --> 00:01:00,480
And when we look out into this beautiful cosmos, we discover something very humbling.

12
00:01:00,480 --> 00:01:08,040
We discover that a universe is vastly grander than we thought and that it looks mostly dead

13
00:01:08,040 --> 00:01:09,040
at first glance.

14
00:01:09,040 --> 00:01:15,720
But we've also discovered something truly inspiring, which is that by carefully studying

15
00:01:15,720 --> 00:01:22,720
our universe and its laws and building technology, we actually have much more influence than

16
00:01:22,720 --> 00:01:24,720
we thought.

17
00:01:24,720 --> 00:01:31,400
For example, we can, through science and technology, harness the powers of our universe and start

18
00:01:31,400 --> 00:01:35,960
exploring it and do all sorts of great things.

19
00:01:35,960 --> 00:01:47,000
And when we think about rockets, for example, as an example of what we can do with tech,

20
00:01:47,000 --> 00:01:52,120
it gives us a great metaphor that can guide us through this morning.

21
00:01:52,120 --> 00:01:58,400
To do inspiring things with technology, it's not enough to make your technology powerful.

22
00:01:58,400 --> 00:02:07,680
You also have to figure out, of course, how to steer it and where you want to go with it.

23
00:02:07,680 --> 00:02:13,440
Now, there's another journey that's much more inspiring still than that with rockets, which

24
00:02:13,440 --> 00:02:16,600
is that with artificial intelligence, which we're going to go on together this morning,

25
00:02:16,600 --> 00:02:20,040
where the passengers aren't just a few astronauts, but all of humanity.

26
00:02:20,040 --> 00:02:26,960
So let's talk about this collective journey we're taking into the future with AI, thinking

27
00:02:26,960 --> 00:02:31,840
about both the power, the steering, and the destination.

28
00:02:31,840 --> 00:02:32,840
Okay?

29
00:02:32,840 --> 00:02:33,840
Ready to go?

30
00:02:33,840 --> 00:02:35,840
And jam on.

31
00:02:35,840 --> 00:02:41,720
Let's begin with the power.

32
00:02:41,720 --> 00:02:45,240
What is intelligence?

33
00:02:45,240 --> 00:02:51,280
I define it just as the ability to accomplish goals, and the more complex the goals are,

34
00:02:51,280 --> 00:02:52,920
the more intelligent.

35
00:02:52,920 --> 00:03:00,520
And I give this very inclusive definition, covering both biological and non-biological

36
00:03:00,520 --> 00:03:08,640
intelligence, because the key idea, in my opinion, which I'll try to sell you on here,

37
00:03:08,640 --> 00:03:15,040
is that intelligence is all about information processing.

38
00:03:15,040 --> 00:03:20,040
Many people think intelligence is something mysterious that can only exist in biological

39
00:03:20,040 --> 00:03:21,040
organisms.

40
00:03:21,240 --> 00:03:28,400
I call that carbon chauvinism, this idea that you can only be smart if you're made of meat.

41
00:03:28,400 --> 00:03:34,080
And I think it's exactly the opposite idea that's powered the great progress in AI.

42
00:03:34,080 --> 00:03:39,000
The idea that it doesn't matter whether the information is processed by carbon atoms in

43
00:03:39,000 --> 00:03:42,840
neurons and brains, or by silicon atoms in our technology.

44
00:03:42,840 --> 00:03:46,160
It's the information processing itself that matters.

45
00:03:46,160 --> 00:03:56,440
And this simple idea has really, really transformed artificial intelligence, and greatly grown

46
00:03:56,440 --> 00:03:58,320
the power of this tech.

47
00:03:58,320 --> 00:03:59,320
Just think about it.

48
00:03:59,320 --> 00:04:05,520
Not long ago, this was the state of the art in robots trying to walk.

49
00:04:05,520 --> 00:04:11,720
It is extra embarrassing for me, because one of these is the MIT robot.

50
00:04:11,720 --> 00:04:13,960
And that was just six years ago.

51
00:04:13,960 --> 00:04:15,600
It was fast forward to today.

52
00:04:15,600 --> 00:04:16,960
Can you see any difference?

53
00:04:23,480 --> 00:04:30,640
In other words, the momentum in the field of artificial intelligence is really quite palpable.

54
00:04:30,640 --> 00:04:35,240
Not long ago, we didn't have self-driving cars.

55
00:04:35,240 --> 00:04:40,640
Now we have self-flying rockets that can land themselves with artificial intelligence.

56
00:04:44,080 --> 00:04:47,920
Not long ago, we couldn't do face recognition.

57
00:04:47,920 --> 00:04:53,000
Now we can do that great.

58
00:04:53,000 --> 00:04:56,680
We can even simulate your face saying all sorts of things you never said, and we have

59
00:04:56,680 --> 00:05:01,840
all these great tools that we heard about from Luca using artificial intelligence.

60
00:05:01,840 --> 00:05:05,920
Not long ago, AI could not save lives.

61
00:05:05,920 --> 00:05:11,040
I believe that soon artificial intelligence will eliminate more than one million pointless

62
00:05:11,040 --> 00:05:15,360
road deaths on the Earth's highways.

63
00:05:15,360 --> 00:05:20,680
And even more lives will be saved by eliminating stupid mistakes in health care.

64
00:05:20,680 --> 00:05:26,000
And still more lives will be saved by actually accelerating the pace of medical research.

65
00:05:26,000 --> 00:05:31,680
We already have AI that is as good as the best doctors at diagnosing prostate cancer,

66
00:05:31,680 --> 00:05:35,600
lung cancer, various eye diseases.

67
00:05:36,560 --> 00:05:42,000
The most lives of all I think in medicine will be saved by just accelerating the pace

68
00:05:42,000 --> 00:05:44,200
of scientific research itself.

69
00:05:44,200 --> 00:05:50,080
For example, for over 50 years, biologists have tried and failed to solve the protein

70
00:05:50,080 --> 00:05:55,760
folding problem, where you start with a genetic sequence of a protein and you try to figure

71
00:05:55,760 --> 00:06:01,560
out if it's going to fold up into a donut shape like the hemoglobin molecules that take

72
00:06:01,560 --> 00:06:04,840
oxygen to your brains right now or to some other shape.

73
00:06:04,840 --> 00:06:08,040
And then AI solved it.

74
00:06:08,040 --> 00:06:16,440
And it's really cool to just look at how Google DeepMind's AlphaFold AI takes the amino acid

75
00:06:16,440 --> 00:06:23,080
genetic sequence as input and just figures out how the protein folds up in 3D and really

76
00:06:23,080 --> 00:06:29,960
spectacular agreement also with expensive and slow measurements from X-ray crystallography.

77
00:06:29,960 --> 00:06:35,280
This is an example of AI for good, which I think can greatly accelerate drug discovery

78
00:06:35,280 --> 00:06:38,880
and drug development.

79
00:06:38,880 --> 00:06:44,680
Not long ago, staying on the theme of the growing power of AI, AI could not beat us at the Asian

80
00:06:44,680 --> 00:06:46,000
Board Game of Go.

81
00:06:46,000 --> 00:06:51,720
Raise your hand if you've ever tried playing Go.

82
00:06:51,720 --> 00:07:00,040
And then, of course, Google DeepMind's AlphaGo and then later AlphaZero, MuZero software,

83
00:07:00,040 --> 00:07:04,720
it took 3,000 years of human Go games and Go wisdom and put it all in the garbage can,

84
00:07:04,720 --> 00:07:10,160
became the best Go player in the world by playing against itself for 24 hours.

85
00:07:10,160 --> 00:07:15,960
And the most interesting thing of all was that it didn't just crush gamers, but it crushed

86
00:07:15,960 --> 00:07:21,520
AI developers like myself who had spent all this time hand-crafting code to do it, all

87
00:07:21,520 --> 00:07:22,800
made obsolete.

88
00:07:22,800 --> 00:07:29,840
And the same AI was able to learn not just this game, but also at the same time become

89
00:07:29,840 --> 00:07:39,160
the world's best player at chess, crushing stockfish and also at now many other games.

90
00:07:39,160 --> 00:07:45,800
This year, 2022, we've seen spectacular progress in large language models that we'll hear

91
00:07:45,800 --> 00:07:48,600
a lot more about later today.

92
00:07:48,600 --> 00:07:52,440
And other models that just take a lot of it, massive amounts of input, train on them and

93
00:07:52,440 --> 00:07:54,080
do cool things.

94
00:07:54,080 --> 00:07:58,640
The Dali 2 AI, if you tell it, to show you a picture of an armchair in the shape of

95
00:07:58,640 --> 00:08:01,960
an avocado.

96
00:08:01,960 --> 00:08:05,760
This is what it comes up with.

97
00:08:05,760 --> 00:08:10,720
One of all the language models so far I'm most impressed with is actually Google Palm,

98
00:08:10,720 --> 00:08:16,080
which just came out a couple of months ago.

99
00:08:16,080 --> 00:08:20,560
If you put in this joke into it like this, saying, hey, I was going to go fly visit my

100
00:08:20,560 --> 00:08:28,840
family on April 6, my mom said, oh, great, your stepdad's poet reading is that night.

101
00:08:28,840 --> 00:08:31,920
So now I'm flying in on April 7.

102
00:08:31,920 --> 00:08:43,080
When this was put into this AI, this is how the AI explains the joke.

103
00:08:43,080 --> 00:08:56,760
I'll shut up for a moment so you can read this.

104
00:08:56,760 --> 00:09:01,280
The one that freaked me out the most was this example.

105
00:09:01,280 --> 00:09:07,560
Same AI, so the input we give to it is Trevor has wanted to see the mountain with all of

106
00:09:07,560 --> 00:09:12,640
the heads on it for a long time, so he finally drove out to see it.

107
00:09:12,640 --> 00:09:19,600
What is the capital of the state that is directly east of the state that Trevor is currently

108
00:09:19,600 --> 00:09:24,480
in?

109
00:09:24,480 --> 00:09:26,000
You guys are really smart.

110
00:09:26,000 --> 00:09:31,560
Can any of you answer this?

111
00:09:31,560 --> 00:09:41,320
Just shout out if you have any thoughts.

112
00:09:41,320 --> 00:09:46,320
The mountain with all the heads on it, what's that?

113
00:09:46,320 --> 00:09:47,520
Mount Rushmore.

114
00:09:47,520 --> 00:09:48,840
All right.

115
00:09:48,840 --> 00:09:52,840
Where is Mount Rushmore?

116
00:09:52,840 --> 00:10:02,200
Any takers?

117
00:10:02,200 --> 00:10:03,200
This is pretty tough.

118
00:10:03,200 --> 00:10:07,800
I couldn't do it, even though I live in the US.

119
00:10:07,800 --> 00:10:14,960
So this is what the AI said, the mountain with all the heads on it is Mount Rushmore.

120
00:10:14,960 --> 00:10:17,360
Mount Rushmore is in South Dakota.

121
00:10:17,360 --> 00:10:21,600
The state directly east of South Dakota is Minnesota.

122
00:10:21,600 --> 00:10:24,640
The capital of Minnesota is St. Paul.

123
00:10:24,640 --> 00:10:29,640
The answer is St. Paul.

124
00:10:29,640 --> 00:10:38,160
As recently as January, February of this year, I remember hearing many AI researchers saying,

125
00:10:38,160 --> 00:10:41,160
oh, these large language models are just kind of fake.

126
00:10:41,160 --> 00:10:45,760
They train on massive amounts of data to predict the next word and so on, but they don't really

127
00:10:45,760 --> 00:10:46,920
understand anything.

128
00:10:46,920 --> 00:10:52,680
They can't really do profound logical reasoning.

129
00:10:52,680 --> 00:11:01,880
When you look at this, that criticism doesn't feel quite as convincing anymore, does it?

130
00:11:01,880 --> 00:11:05,960
So things are happening very, very fast.

131
00:11:05,960 --> 00:11:16,880
And to put this a little bit in context, this, the Google Palm model here, it has 540 billion

132
00:11:17,440 --> 00:11:22,120
parameters in the neural network that's been trained here.

133
00:11:22,120 --> 00:11:27,760
That's about a thousand times more than a mouse.

134
00:11:27,760 --> 00:11:29,760
Our symposium is called Synapse.

135
00:11:29,760 --> 00:11:35,080
A mouse has about 500 million synapses, which you could truly think of a single parameter

136
00:11:35,080 --> 00:11:36,080
each.

137
00:11:36,080 --> 00:11:40,000
And we humans, we have about 200 times more than that.

138
00:11:40,000 --> 00:11:45,520
To put this in context, if we think about the memory capacity of our computational devices

139
00:11:45,560 --> 00:11:54,880
over time, we see this familiar Moore's Law, where there's been a spectacular increase

140
00:11:54,880 --> 00:12:01,800
of a thousand million, million times growth since the 1950s.

141
00:12:01,800 --> 00:12:12,080
And here we are today, where you can now buy a little memory card about the size of my

142
00:12:12,120 --> 00:12:16,760
thumbnail, twice that, for 100 bucks.

143
00:12:16,760 --> 00:12:21,200
So it costs about $1 per terabyte.

144
00:12:21,200 --> 00:12:27,640
And if we compare this with the amount of memory or data in these models, we see that

145
00:12:27,640 --> 00:12:32,880
actually, you know, we're already far beyond the mouse.

146
00:12:32,880 --> 00:12:37,040
Google Palm sits about here, you know, this is about the amount of data in the Google

147
00:12:37,040 --> 00:12:38,040
Palm model.

148
00:12:38,160 --> 00:12:44,000
Humans are a little bit above there, but you can buy 100 of these little cards.

149
00:12:44,000 --> 00:12:47,280
That's about as much information as you have stored in your entire brain.

150
00:12:47,280 --> 00:12:53,640
The hardware has already more or less caught up with the human hardware in some ways, even

151
00:12:53,640 --> 00:12:56,720
though it's much less energy efficient and so on.

152
00:12:56,720 --> 00:13:01,600
It seems like we're mainly just limited by having much more stupid software in our machines

153
00:13:01,600 --> 00:13:05,800
than we have in our brains.

154
00:13:05,800 --> 00:13:07,840
This begs the question, though.

155
00:13:07,840 --> 00:13:09,800
How far will this go?

156
00:13:09,800 --> 00:13:15,880
We've talked here about how amazing the growth of power of artificial intelligence has been.

157
00:13:15,880 --> 00:13:19,640
And I hope with these examples, especially with the Mount Rushmore one, I can convince

158
00:13:19,640 --> 00:13:24,960
you that really, AI is happening, whether we like it or not.

159
00:13:24,960 --> 00:13:27,400
How far is it going to go?

160
00:13:27,400 --> 00:13:34,800
Well, I like to think about this question in terms of this abstract landscape of tasks,

161
00:13:34,800 --> 00:13:41,800
where the elevation represents how difficult it is for AI to do each task, and the sea

162
00:13:41,800 --> 00:13:48,560
level represents what computers can do today.

163
00:13:48,560 --> 00:13:53,960
The sea level is obviously rising, so an obvious takeaway from this is for all of you thinking

164
00:13:53,960 --> 00:13:59,360
about your careers, be careful with careers right at the waterfront, which are in the

165
00:13:59,360 --> 00:14:02,760
process of getting disrupted.

166
00:14:02,760 --> 00:14:10,520
But the bigger question is, of course, how high is the water eventually going to rise?

167
00:14:10,520 --> 00:14:16,840
Is it eventually going to submerge all land?

168
00:14:16,840 --> 00:14:23,920
This is the definition of artificial general intelligence, AGI.

169
00:14:23,920 --> 00:14:29,560
So with this definition, people who say, ah, you know, there'll always be jobs that humans

170
00:14:29,560 --> 00:14:38,880
can do better than machines, are simply saying that there will never be AGI.

171
00:14:38,880 --> 00:14:43,880
If you think this sounds like crazy science fiction, AGI, I want you to be clear on the

172
00:14:43,880 --> 00:14:48,000
fact that there is something else which sounds like even more crazy science fiction, the

173
00:14:48,000 --> 00:14:51,440
idea of super intelligence.

174
00:14:51,440 --> 00:14:56,320
An idea which is actually quite simple, the idea is that if we actually do get the AGI,

175
00:14:56,320 --> 00:15:01,600
and then, since by definition, machines can do all jobs better than humans, that includes

176
00:15:01,600 --> 00:15:08,400
the job of AI development and everything else that is done by bending spoons.

177
00:15:08,400 --> 00:15:14,680
So it opens up the controversial possibility that future AI development after that can

178
00:15:14,680 --> 00:15:20,400
be done much faster, replacing the typical human research and development timescale

179
00:15:20,400 --> 00:15:27,040
of years by months or weeks or hours or whatever it takes for machines to make better versions

180
00:15:27,040 --> 00:15:30,160
of their software, etc.

181
00:15:30,160 --> 00:15:36,080
And so if that happens, it opens up the possibility of a recursively self-improving technology

182
00:15:36,080 --> 00:15:40,120
which could quickly leave us humans far, far behind.

183
00:15:40,120 --> 00:15:43,520
Here, we need a bit of a reality check.

184
00:15:43,520 --> 00:15:49,920
Is this just crazy science fiction speculation by people who have no idea about technology

185
00:15:49,920 --> 00:15:50,920
really?

186
00:15:50,920 --> 00:15:54,480
I'll let you decide that for yourselves by playing a little clip from a conference that

187
00:15:54,480 --> 00:15:59,720
I was involved in organizing some years ago.

188
00:15:59,720 --> 00:16:02,840
It's a fair science controversy.

189
00:16:02,840 --> 00:16:06,680
On one hand, you have people like my former MIT colleague Rodney Brooks who would be like,

190
00:16:06,680 --> 00:16:10,760
ah, this is all just bullshit, you know, this won't happen for centuries.

191
00:16:10,760 --> 00:16:17,200
On the other hand, you have people like Demi Sassabis, CEO of Google DeepMind, gave us

192
00:16:17,200 --> 00:16:22,480
AlphaZero and so much else who think it is going to happen and who's betting their whole

193
00:16:22,480 --> 00:16:26,920
company on trying to build these things.

194
00:16:26,920 --> 00:16:35,160
So here is the little video clip I'll share with you.

195
00:16:35,160 --> 00:16:46,200
Let's see if we have any luck here.

196
00:16:46,200 --> 00:16:48,800
This is actually a very good metaphor for the whole talk.

197
00:16:48,800 --> 00:16:52,560
With technology, what can possibly go wrong?

198
00:16:52,560 --> 00:16:56,480
So before I asked if superintelligence is possible at all according to the laws of physics, now

199
00:16:56,480 --> 00:16:59,080
I'm asking, will it actually happen?

200
00:16:59,080 --> 00:17:02,240
Yes, no, or it's complicated?

201
00:17:02,240 --> 00:17:04,600
A little bit complicated, but yes.

202
00:17:04,600 --> 00:17:10,040
Yes, and if it doesn't, something terrible has happened to prevent it?

203
00:17:10,040 --> 00:17:11,040
Yes.

204
00:17:11,040 --> 00:17:12,040
Probably.

205
00:17:12,040 --> 00:17:13,040
Yes.

206
00:17:13,040 --> 00:17:14,040
Yes.

207
00:17:14,040 --> 00:17:15,040
Yes.

208
00:17:15,040 --> 00:17:16,040
Yes.

209
00:17:16,040 --> 00:17:17,040
Yes.

210
00:17:17,040 --> 00:17:18,040
Yes.

211
00:17:18,040 --> 00:17:19,040
Yes.

212
00:17:19,040 --> 00:17:20,040
No.

213
00:17:20,040 --> 00:17:25,640
So what do we make of that?

214
00:17:25,640 --> 00:17:33,000
Well, aside from the fact that Elon has a sense of humor, it's quite obvious that these

215
00:17:33,000 --> 00:17:37,400
people who are saying this are not all just the much of philosophers who don't know anything

216
00:17:37,400 --> 00:17:38,400
about physics.

217
00:17:38,400 --> 00:17:41,640
This, for example, is Demi Sassabis from Google DeepMind.

218
00:17:41,640 --> 00:17:43,680
There were a bunch of AI professors there, et cetera.

219
00:17:43,680 --> 00:17:47,160
So that doesn't prove that superintelligence will happen, but it means that we have to

220
00:17:47,160 --> 00:17:50,800
take it very seriously as an actual possibility.

221
00:17:50,800 --> 00:17:55,680
In terms of the much slower ambition of just replacing all human jobs with machines that

222
00:17:55,680 --> 00:18:00,840
can do it cheaper and better, in other words, AGI, there are actually recent surveys have

223
00:18:00,840 --> 00:18:07,160
shown that most AI researchers think this is going to happen within a matter of decades.

224
00:18:07,160 --> 00:18:12,800
So when I look at you, you all look healthy, like you're taking your vitamins, going to

225
00:18:12,800 --> 00:18:17,520
the gym, et cetera, who are going to be around in a few decades.

226
00:18:17,520 --> 00:18:23,440
So you should definitely be very open to the possibility that it might happen in your lifetime

227
00:18:23,440 --> 00:18:29,400
and think about, well, what does that imply for you and how can we make sure that this

228
00:18:29,400 --> 00:18:33,440
becomes the best thing ever to happen to humanity rather than the worst thing ever to happen

229
00:18:33,440 --> 00:18:34,440
to humanity?

230
00:18:34,440 --> 00:18:40,360
That is the defining challenge of our generation.

231
00:18:40,360 --> 00:18:45,320
I'm going to try this pointer and see if it works better, proving that Italian technology

232
00:18:45,320 --> 00:18:47,640
is better than American technology.

233
00:18:47,640 --> 00:18:48,640
Great works.

234
00:18:48,640 --> 00:18:49,640
Will that happen?

235
00:18:49,640 --> 00:18:51,640
Yes, no, or it's complicated.

236
00:18:51,640 --> 00:18:52,640
I shouldn't have said that.

237
00:18:52,640 --> 00:18:53,640
A little bit complicated, but yes.

238
00:18:53,640 --> 00:18:55,640
Okay, well, try both pointers.

239
00:18:55,640 --> 00:18:56,640
Yes.

240
00:18:56,640 --> 00:18:57,640
There.

241
00:18:57,640 --> 00:18:59,360
All right.

242
00:18:59,360 --> 00:19:05,720
So if this happens and we get to artificial general intelligence and then it starts taking

243
00:19:05,720 --> 00:19:08,040
off towards superintelligence, what does that really mean?

244
00:19:08,040 --> 00:19:11,800
Well, one thing it's obviously going to mean is that we should start imagining technology

245
00:19:11,800 --> 00:19:16,480
which is limited not by human intelligence the way it is today, but limited instead by

246
00:19:16,480 --> 00:19:21,640
the laws of physics, and that's a huge, huge difference.

247
00:19:21,640 --> 00:19:28,360
So just to get you thinking big again, we already looked at the exponential growth,

248
00:19:28,360 --> 00:19:30,080
for example, of memory technology.

249
00:19:30,080 --> 00:19:34,680
Well, these technologies, even though they're cool, are still pretty stupid.

250
00:19:34,680 --> 00:19:39,880
It still takes 100 billion atoms to store each one bit.

251
00:19:39,880 --> 00:19:40,880
That's pretty dumb.

252
00:19:40,880 --> 00:19:42,760
I mean, you have 100 billion neurons in your brain.

253
00:19:42,760 --> 00:19:46,600
Imagine if you're only using one of them to store information in it.

254
00:19:46,600 --> 00:19:51,640
So that suggests you can do way better quite easily as you start using quantum tech to

255
00:19:51,640 --> 00:19:54,480
get down closer to one bit per atom.

256
00:19:54,480 --> 00:20:00,120
If you go and look at other things you might care about, like computation, how much can

257
00:20:00,120 --> 00:20:03,240
you compute per dollar?

258
00:20:03,240 --> 00:20:07,800
The trend has been even more spectacular in terms of the performance cost.

259
00:20:07,800 --> 00:20:11,220
We've gone down 19 orders of magnitude in price.

260
00:20:11,220 --> 00:20:16,640
If you cut the prices of everything in Italy and the world by 10 to the power of 19, you

261
00:20:16,640 --> 00:20:20,760
know, a thousand billion, billion, billion times, you could buy all the economic production

262
00:20:20,760 --> 00:20:24,960
of the world for less than one cent.

263
00:20:24,960 --> 00:20:31,840
And here again, if we get limited by the laws of physics, we're nowhere near the limits.

264
00:20:31,840 --> 00:20:35,960
Professor Seth Floyd at MIT actually calculated what the physical limits are before your computer

265
00:20:35,960 --> 00:20:40,960
turns into a black hole or anything like that, or violating speed of light limits.

266
00:20:40,960 --> 00:20:47,160
And this is to scale how far we are from the physical limits.

267
00:20:47,160 --> 00:20:52,280
We talk a lot now in Italy and elsewhere about energy, crisis, and energy independence.

268
00:20:52,280 --> 00:20:58,080
Look how terribly inefficient our technology is today, like burning coal and gasoline.

269
00:20:58,080 --> 00:21:02,360
This is how many percent of the energy you actually get out of it.

270
00:21:02,360 --> 00:21:07,720
With nuclear reactors, you do it a little bit better, but it's still quite pathetic compared

271
00:21:07,720 --> 00:21:10,680
to the limits that Einstein say are put by nature.

272
00:21:10,680 --> 00:21:15,640
And we already have technological ideas like spalarizers and things like this, which if

273
00:21:15,640 --> 00:21:21,120
you have superintelligence you could build, just do dramatically better than this.

274
00:21:21,120 --> 00:21:23,160
Raise your hand if you like to travel.

275
00:21:23,160 --> 00:21:27,240
All right, so there's a cool universe out there, right?

276
00:21:27,240 --> 00:21:33,800
So far, we haven't even been anywhere further, we humans, than going to the moon, but obviously

277
00:21:33,800 --> 00:21:38,960
if you have suddenly enhanced our technology to be limited by the laws of physics rather

278
00:21:38,960 --> 00:21:45,120
than by our intelligence, things look a lot more rosy instead of having to wait thousands

279
00:21:45,120 --> 00:21:49,200
or millions of years like in science fiction novels to have this tech, you could have it

280
00:21:49,200 --> 00:21:51,520
in your lifetime.

281
00:21:51,520 --> 00:21:59,080
And it becomes suddenly quite straightforward to go visit other stars, even other galaxies.

282
00:21:59,080 --> 00:22:03,280
You can ask me in the Q&A if you want a little bit about how to do it, but I just threw in

283
00:22:03,280 --> 00:22:09,600
this little thing here to just show you how much more resources there are out there in

284
00:22:09,600 --> 00:22:13,680
the cosmos compared to the resources that we keep fighting about on Earth when we have

285
00:22:13,680 --> 00:22:19,200
our stupid little wars.

286
00:22:19,200 --> 00:22:24,120
And even here, even with human intelligence, there have been all sorts of very clever solutions

287
00:22:24,120 --> 00:22:28,840
that people have figured out, like how to go live in the solar system and a fun habitat,

288
00:22:28,840 --> 00:22:31,560
which is solar powered, etc.

289
00:22:31,560 --> 00:22:38,120
We could build these sort of things very quickly with robots that were controlled with AI if

290
00:22:38,120 --> 00:22:39,920
we wanted to.

291
00:22:40,920 --> 00:22:45,640
All right, so we've spent most of our time talking about the growing power of artificial

292
00:22:45,640 --> 00:22:51,120
intelligence, and the key message that I want you to take away from this again is the fact

293
00:22:51,120 --> 00:23:01,800
that artificial intelligence really is happening for better or for worse, and the pace of progress

294
00:23:01,800 --> 00:23:04,960
in the field is just absolutely amazing.

295
00:23:04,960 --> 00:23:11,480
And if it continues, and we get to artificial general intelligence and beyond, we will have

296
00:23:11,480 --> 00:23:15,360
enormous opportunities, we'll become the masters of our own destiny.

297
00:23:15,360 --> 00:23:21,840
Instead of being running around trying to not get stepped on or eaten by tigers, we will

298
00:23:21,840 --> 00:23:25,440
be making the decisions about our future.

299
00:23:25,440 --> 00:23:26,760
What should those decisions be?

300
00:23:26,760 --> 00:23:29,200
How can we make sure that this becomes good?

301
00:23:29,200 --> 00:23:35,800
As Luca said, if you think about, even without worrying about what machines might do to you,

302
00:23:35,800 --> 00:23:44,280
if you just offer a moment to imagine your least favorite leader on the planet, don't

303
00:23:44,280 --> 00:23:47,840
tell me who it is, but just imagine their face for a second, okay?

304
00:23:47,840 --> 00:23:53,920
Imagine now that they are the ones who control AGI and use it to take over the world.

305
00:23:53,920 --> 00:23:55,440
How does that make you feel?

306
00:23:59,440 --> 00:24:00,440
Great.

307
00:24:02,680 --> 00:24:04,160
Or less so.

308
00:24:04,160 --> 00:24:10,720
So how can we steer in the progress of this technology towards an inspiring future?

309
00:24:10,720 --> 00:24:11,720
Let's talk a little bit about this.

310
00:24:11,720 --> 00:24:17,200
To help with this issue, I teamed up with some colleagues and we created the Future Life Institute,

311
00:24:17,200 --> 00:24:21,960
whose goal is precisely for the future life to be awesome and not awful.

312
00:24:22,040 --> 00:24:30,280
We've done a lot of things to help focus people's thinking about how to steer.

313
00:24:30,280 --> 00:24:37,960
We, for example, had conferences where we developed principles for guiding the beneficial

314
00:24:37,960 --> 00:24:43,920
use of artificial intelligence, which have been signed by thousands of leading AI researchers.

315
00:24:43,920 --> 00:24:49,360
They've even been adopted into law by the state of California and inspired a lot of

316
00:24:49,360 --> 00:24:52,720
the OECD principles here in Europe, et cetera.

317
00:24:52,720 --> 00:24:58,600
And I'll just give a couple, a few quick, very quick examples of such principles.

318
00:24:58,600 --> 00:25:06,720
Try this one, try this one, there.

319
00:25:06,720 --> 00:25:11,320
If you see me struggling too much with a clicker, you can just press the right arrow on the

320
00:25:11,320 --> 00:25:12,840
laptop back there.

321
00:25:12,840 --> 00:25:17,520
One principle is that we should, like, you know, all technologies can be used to help

322
00:25:17,520 --> 00:25:20,000
people or for new ways of harming people.

323
00:25:20,000 --> 00:25:25,480
So biologists and chemists, for example, have been working very hard to make sure that they

324
00:25:25,480 --> 00:25:30,840
ban bio weapons and ban chemical weapons so we can use their sciences for new machines

325
00:25:30,840 --> 00:25:32,960
and materials instead.

326
00:25:32,960 --> 00:25:38,120
And of course, AI researchers are an idealistic bunch, as those of you who work on it know,

327
00:25:38,120 --> 00:25:42,280
who want to make sure we use artificial intelligence for new solutions also, not for just new ways

328
00:25:42,280 --> 00:25:46,200
of killing people with slaughterbots or whatever.

329
00:25:46,200 --> 00:25:51,800
Another principle that there's very broad agreement on is that we really need to make

330
00:25:51,800 --> 00:25:57,080
sure that all the enormous growth in the economic pie benefits everybody.

331
00:25:57,080 --> 00:26:02,080
And my opinion is that if we can produce this future abundance of goods and services with

332
00:26:02,080 --> 00:26:06,840
artificial intelligence, and we still cannot figure out how to share this in such a way

333
00:26:06,840 --> 00:26:12,640
that everybody on Earth gets better off, then shame on us.

334
00:26:12,640 --> 00:26:16,040
And it's fun to talk about this in Europe because I feel Italy and Europe in general

335
00:26:16,040 --> 00:26:21,160
has a much stronger tradition actually in this regard of asking the question, how can

336
00:26:21,160 --> 00:26:30,120
we make our growing economy work for all of us, not just for some of us?

337
00:26:30,120 --> 00:26:36,680
A third principle is, well, raise your hand if your computer has ever crashed.

338
00:26:36,680 --> 00:26:41,440
That's a lot of hats.

339
00:26:41,440 --> 00:26:49,160
So how did that make you feel?

340
00:26:49,160 --> 00:26:50,160
Frustrated.

341
00:26:50,160 --> 00:26:59,600
All right, now suppose this computer was actually in charge of the nearby nuclear power plant

342
00:26:59,600 --> 00:27:05,760
or the U.S. nuclear arsenal or something else that really affects people's lives.

343
00:27:05,760 --> 00:27:07,600
Frustrated probably would not be the first word you would use.

344
00:27:07,600 --> 00:27:09,560
You'd probably use a stronger one, right?

345
00:27:09,560 --> 00:27:15,800
So this means that as AI gets more powerful, it becomes ever more important to work on AI

346
00:27:15,800 --> 00:27:16,800
safety research.

347
00:27:16,800 --> 00:27:22,560
And this is actually an encouragement for all of those of you who work in AI.

348
00:27:22,560 --> 00:27:26,000
Don't just think about how you can work to make it more powerful.

349
00:27:26,000 --> 00:27:30,280
Also think about the technical work you might be able to contribute to, to make it safe

350
00:27:30,280 --> 00:27:40,560
and robust and beneficial so it actually does what we want to do.

351
00:27:40,560 --> 00:27:51,840
Finally, it's part of this has to also involve thinking about putting goals into our machines

352
00:27:51,840 --> 00:27:55,080
that are aligned with human goals.

353
00:27:55,080 --> 00:27:59,400
Because the greatest risk from very powerful artificial intelligence is not that it's going

354
00:27:59,400 --> 00:28:05,360
to turn evil like in stupid Hollywood movies, it's that it's going to just turn very competent

355
00:28:05,360 --> 00:28:11,480
and go out and accomplish goals that are not aligned with our goals.

356
00:28:11,480 --> 00:28:18,080
If we think about this guy, for example, the West African black rhino, why did we humans

357
00:28:18,080 --> 00:28:19,240
drive it extinct?

358
00:28:19,240 --> 00:28:22,760
Is it because we are actually a bunch of evil rhinoceros haters?

359
00:28:22,760 --> 00:28:23,760
No.

360
00:28:23,760 --> 00:28:27,440
It's that we are more intelligent than they were, and our goals are not aligned with

361
00:28:27,440 --> 00:28:30,200
their goals and tough luck for them.

362
00:28:30,200 --> 00:28:37,920
Let's not put humanity in the place of those rhinos by giving a bunch of power to machines

363
00:28:37,920 --> 00:28:46,280
who don't share our goals or to humans who control machines that don't have our goals.

364
00:28:46,280 --> 00:28:47,720
We can talk a lot about this.

365
00:28:47,720 --> 00:28:51,720
This is also partly a very technical problem, actually, how to make machines understand our

366
00:28:51,720 --> 00:28:54,880
goals, learn our goals, retain their goals.

367
00:28:54,880 --> 00:28:59,320
Also a problem, of course, of making sure that we give the right incentives to the corporations

368
00:28:59,320 --> 00:29:05,640
and governments that, in turn, have power over the machines.

369
00:29:05,640 --> 00:29:15,240
My research that my group at MIT focuses on is very much dedicated to technical artificial

370
00:29:15,240 --> 00:29:23,560
intelligence research, and we don't have time for me to go into it in any great detail

371
00:29:23,680 --> 00:29:27,080
at all, but I'll just give you a little bit of a flavor.

372
00:29:27,080 --> 00:29:34,480
So today, the vast majority of the really high-powered AI systems, like the language

373
00:29:34,480 --> 00:29:39,600
models that we looked at a little bit here, and you'll hear more about today, like Google

374
00:29:39,600 --> 00:29:43,400
Palm and GPT-3 and so on, are gigantic black boxes.

375
00:29:43,400 --> 00:29:48,600
They do very intelligent things, but we really don't understand how they work, which limits

376
00:29:48,600 --> 00:29:51,720
the extent to which you can trust them.

377
00:29:51,760 --> 00:29:54,680
We are very focused on how can we open up the black box?

378
00:29:54,680 --> 00:30:03,400
How can we use the training of a giant black box AI model as the first step, rather than

379
00:30:03,400 --> 00:30:05,560
the last step?

380
00:30:05,560 --> 00:30:11,800
So our vision is, first, you create something like this that learns something very smart,

381
00:30:11,800 --> 00:30:17,120
and then, instead of just selling it at that point, you do a second step and try to get

382
00:30:17,120 --> 00:30:19,680
the knowledge out of it.

383
00:30:19,680 --> 00:30:23,200
So you can put the knowledge into something which you can trust more.

384
00:30:23,200 --> 00:30:27,800
We've had some success with very basic things, like if you have a big table of numbers, for

385
00:30:27,800 --> 00:30:32,080
example, and you want to predict the last column from the previous ones, this is called

386
00:30:32,080 --> 00:30:35,040
symbolic regression.

387
00:30:35,040 --> 00:30:40,920
It's the nonlinear NP-hard version of what's known as linear regression, which is very

388
00:30:40,920 --> 00:30:41,920
easy.

389
00:30:41,920 --> 00:30:46,960
This normally takes longer than the age of the universe to discover simple formulas,

390
00:30:46,960 --> 00:30:48,640
in general.

391
00:30:48,640 --> 00:30:51,960
But we were able to get state-of-the-art performance on this.

392
00:30:51,960 --> 00:30:56,760
We first train a black box neural network, and then we use all sorts of techniques by

393
00:30:56,760 --> 00:31:02,680
looking at the gradients and so on to figure out how we can be broken apart into modules.

394
00:31:02,680 --> 00:31:07,720
And then we do this in a recursive way that I can tell you about over coffee if you're

395
00:31:07,720 --> 00:31:09,200
interested.

396
00:31:09,200 --> 00:31:13,920
And eventually, the thing works so well that we were able to put in the 100 most famous

397
00:31:13,920 --> 00:31:18,800
physics equations from the Feynman Lecturers and from other books.

398
00:31:18,800 --> 00:31:24,280
And by just showing data, it was able to not just fit them accurately with a sort of black

399
00:31:24,280 --> 00:31:28,840
box system that you don't understand, but actually discover what the formulas were so

400
00:31:28,840 --> 00:31:31,120
you could extract out the knowledge.

401
00:31:31,120 --> 00:31:36,840
This is very inspired, for me, I was very inspired to do this project by an Italian,

402
00:31:36,840 --> 00:31:39,440
Galileo Galilei.

403
00:31:39,440 --> 00:31:45,560
Because when he was four years old, if his mother threw him an apple, of course he could

404
00:31:45,560 --> 00:31:47,360
catch it.

405
00:31:47,360 --> 00:31:50,720
Because his neural network that he had trained during his childhood in his brain was very

406
00:31:50,720 --> 00:31:57,000
good at predicting the shape in which apples moved under the influence of gravity.

407
00:31:57,000 --> 00:32:00,080
And so you could do the same when you were four years old.

408
00:32:00,080 --> 00:32:04,520
But when Galileo got older, he did something more.

409
00:32:04,520 --> 00:32:10,000
He said, hmm, can I take this intuitive knowledge that I have that I don't know how it's represented

410
00:32:10,000 --> 00:32:14,040
and somehow get it out of my brain?

411
00:32:14,040 --> 00:32:20,040
And he said, well, this curve, it's a parabola, regardless of what kind of apple it is and

412
00:32:20,040 --> 00:32:21,280
how fast it's thrown.

413
00:32:21,280 --> 00:32:23,840
And there's an equation for it, y equals x squared.

414
00:32:23,840 --> 00:32:28,840
And he was able to tell his friends about this and publish papers.

415
00:32:28,840 --> 00:32:38,480
So this ability to transform poorly understood intuitive knowledge into symbolic representations

416
00:32:38,480 --> 00:32:41,380
is really at the heart of human intelligence.

417
00:32:41,380 --> 00:32:44,640
When you speak Italian with your friends, you're doing exactly the same thing.

418
00:32:44,640 --> 00:32:48,840
You're taking some knowledge that you have in your brain and you're verbalizing it, putting

419
00:32:48,840 --> 00:32:53,560
it in a symbolic representation where it can be communicated and explained.

420
00:32:53,560 --> 00:32:58,680
So if we can do it, I believe that we can make machines that can also do it.

421
00:32:58,680 --> 00:33:03,160
And they will be much more safe and trustworthy that way because the hard part is usually

422
00:33:03,160 --> 00:33:04,680
discovering the knowledge.

423
00:33:04,680 --> 00:33:09,520
Once you have the knowledge, you can implement it in something maybe which is not the standard

424
00:33:09,520 --> 00:33:12,720
neural network, but something that you really, really understand.

425
00:33:12,720 --> 00:33:18,800
So this was just one little hopeful example of how I think that we can do much better

426
00:33:18,800 --> 00:33:25,320
in terms of trustworthiness of our AI systems going forward than we should.

427
00:33:25,680 --> 00:33:30,160
And in my final two minutes, let's talk just a little bit about the destination also.

428
00:33:30,160 --> 00:33:34,200
So we're making this ever more powerful tech.

429
00:33:34,200 --> 00:33:39,440
I mentioned various ideas for how we can steer it better, control it better, trust it better.

430
00:33:39,440 --> 00:33:43,640
But what do we ultimately want to do with it?

431
00:33:43,640 --> 00:33:48,720
How can we ensure an inspiring future with artificial intelligence, both for businesses

432
00:33:48,720 --> 00:33:52,720
like Bending Spoons and for our entire civilization?

433
00:33:53,560 --> 00:34:00,400
I've already alluded to one important strategy, which is just to think about all the uses

434
00:34:00,400 --> 00:34:08,280
of AI and then draw a very clear red line between what we consider acceptable uses and

435
00:34:08,280 --> 00:34:12,000
unacceptable uses.

436
00:34:12,000 --> 00:34:16,240
I would encourage you to keep this very ethical framework in the back of your mind in your

437
00:34:16,240 --> 00:34:21,240
future career also whenever you build something, ask yourself also what the social impact

438
00:34:21,240 --> 00:34:23,800
of it will be.

439
00:34:23,800 --> 00:34:31,400
And the second strategy is we really need to articulate positive visions for what we want

440
00:34:31,400 --> 00:34:38,520
to accomplish with this because shared positive visions are the fundamental driver of collaboration

441
00:34:38,520 --> 00:34:43,920
in companies, in relationships, and in the world.

442
00:34:43,920 --> 00:34:50,920
And it's easy to say, well, we'll never find any goals for the future that the US and China

443
00:34:51,120 --> 00:34:53,960
and Italy are all going to agree on.

444
00:34:53,960 --> 00:34:55,720
But that's obviously not true.

445
00:34:55,720 --> 00:35:01,080
The United Nations Sustainable Development Goals, for example, are quite ambitious visions

446
00:35:01,080 --> 00:35:03,600
and basically every country on the planet agrees with it.

447
00:35:03,600 --> 00:35:07,760
We even wrote this paper here in Nature Commons recently about how AI can help us accomplish

448
00:35:07,760 --> 00:35:10,720
the Sustainable Development Goals faster.

449
00:35:10,720 --> 00:35:16,880
And if we get ever more advanced AI, we can go much more ambitious than that also and

450
00:35:16,880 --> 00:35:23,880
say, well, let's not just try to reduce a few problems a little bit, but really actually

451
00:35:24,680 --> 00:35:28,440
solve them and just do dramatically better.

452
00:35:28,440 --> 00:35:31,760
The sky is the limit.

453
00:35:31,760 --> 00:35:38,760
So in summary, I feel that artificial general intelligence is the ultimate game-changing

454
00:35:38,880 --> 00:35:45,080
technology for our human species, and it is coming.

455
00:35:45,080 --> 00:35:49,680
And what does that mean, exactly?

456
00:35:49,680 --> 00:35:56,680
First of all, it means we're not just some tiny little life forms here on a tiny little

457
00:35:56,680 --> 00:36:02,320
planet that's completely irrelevant because we're so small.

458
00:36:02,320 --> 00:36:08,320
What's happening on this planet in your lifetime is probably the most significant thing ever

459
00:36:08,400 --> 00:36:14,640
to happen anywhere in our universe so far, and could easily affect the entire future

460
00:36:14,640 --> 00:36:21,640
of much of this gorgeous and beautiful universe that you're looking at in the background.

461
00:36:21,640 --> 00:36:28,640
So my charge to you folks is be proactive and think about how can you steer this technology

462
00:36:30,160 --> 00:36:32,440
to a good place?

463
00:36:32,440 --> 00:36:38,680
Let's be the masters of our own destiny by envisioning it the way we want it to be and

464
00:36:38,680 --> 00:36:40,840
by actually building it.

465
00:36:40,840 --> 00:36:41,840
Thank you.

466
00:36:41,840 --> 00:36:42,840
Thank you.

467
00:36:42,840 --> 00:36:43,840
That was great.

468
00:36:43,840 --> 00:36:44,840
Thank you very much.

469
00:36:44,840 --> 00:36:45,840
Stay there for a minute.

470
00:36:45,840 --> 00:36:46,840
Thank you very much, Max.

471
00:36:46,840 --> 00:36:57,840
That was absolutely awesome.

472
00:36:57,840 --> 00:37:01,800
I want to move things along, but we can't let Max go without asking him a few questions.

473
00:37:01,800 --> 00:37:03,440
This is your opportunity.

474
00:37:03,440 --> 00:37:05,920
Who would like to ask a question?

475
00:37:05,920 --> 00:37:06,920
Who will be brave and ask that?

476
00:37:06,920 --> 00:37:08,680
Do you want me to ask the first question?

477
00:37:08,680 --> 00:37:09,680
Okay.

478
00:37:09,680 --> 00:37:10,760
All right then.

479
00:37:10,760 --> 00:37:16,040
You talked about, I've got about five questions I want to ask you.

480
00:37:16,040 --> 00:37:17,920
We're not going to have time for all of these.

481
00:37:17,920 --> 00:37:23,000
Are we better at seeing how AI works?

482
00:37:23,000 --> 00:37:30,000
I'm reminded of the example of Microsoft's Tay when it was your friend in your pocket

483
00:37:30,000 --> 00:37:33,760
that you would have a conversation with and it randomly started to be racist and insult

484
00:37:33,760 --> 00:37:38,280
people and they turned it off because they didn't know why it was doing it.

485
00:37:38,280 --> 00:37:45,040
Are we better now at looking, opening up the brain and looking inside it and seeing why

486
00:37:45,040 --> 00:37:50,120
it is doing things as opposed to looking at the outcome?

487
00:37:50,120 --> 00:37:51,120
Very good question.

488
00:37:51,120 --> 00:37:55,520
We're a little bit better, but in the meantime the technology has gotten dramatically more

489
00:37:55,520 --> 00:37:56,520
powerful.

490
00:37:56,520 --> 00:37:58,880
So we really need to up our game in this area.

491
00:37:58,880 --> 00:38:03,760
I really do believe that this is one of the most valuable things we can do for a good

492
00:38:03,760 --> 00:38:10,300
future to win this wisdom race between the growing power of the tech and the growing

493
00:38:10,300 --> 00:38:12,920
wisdom with which we understand it.

494
00:38:12,920 --> 00:38:16,200
I hope there are some of the examples I mentioned that suggest that it's not as hopeless at

495
00:38:16,200 --> 00:38:18,880
all as people think.

496
00:38:18,880 --> 00:38:25,400
After all, we humans are able to extract our own insights in a fashion we can explain to

497
00:38:25,400 --> 00:38:31,960
others and we really should be able to get the AIs to do the same for us.

498
00:38:31,960 --> 00:38:32,960
Any questions?

499
00:38:32,960 --> 00:38:33,960
Yes, sir.

500
00:38:33,960 --> 00:38:34,960
There we go.

501
00:38:34,960 --> 00:38:35,960
This works.

502
00:38:35,960 --> 00:38:36,960
It does.

503
00:38:36,960 --> 00:38:37,960
Bonjour, everybody.

504
00:38:37,960 --> 00:38:45,680
So I was thinking about the super intelligence.

505
00:38:45,680 --> 00:38:50,000
We could see it as our next big step in evolution.

506
00:38:50,000 --> 00:38:51,640
We're already moving so far.

507
00:38:51,640 --> 00:38:58,720
The older generations are having really troubles in understanding AI, the power, the steering

508
00:38:58,720 --> 00:39:00,520
and the direction of it.

509
00:39:00,520 --> 00:39:08,240
So I wanted to hear your take on do you have a feeling of when the overtaking, the super

510
00:39:08,240 --> 00:39:18,320
intelligence might happen and how can we gracefully go in that direction, make it gracefully

511
00:39:18,320 --> 00:39:22,040
modern within the society, let's say.

512
00:39:22,040 --> 00:39:23,040
Great.

513
00:39:23,040 --> 00:39:31,080
So as to the question of when, you know, the most reliable polls of AI researchers suggest

514
00:39:31,080 --> 00:39:38,520
maybe a 30 years, but it could be a lot sooner, it could be later, but I think it's very likely

515
00:39:38,520 --> 00:39:43,720
to be within your lifetime, which is really the number one thing to take away from that.

516
00:39:43,720 --> 00:39:49,560
In terms of how to make it graceful and make it something good, I think we have both a

517
00:39:49,560 --> 00:39:53,320
bunch of these technical challenges, again, how can we make AI systems that we actually

518
00:39:53,320 --> 00:40:00,720
trust to do what we want them to do, and how can we make sure that we improve our democracy

519
00:40:00,720 --> 00:40:06,320
so that the control over this ever greater power actually lies in the hands of all of

520
00:40:06,320 --> 00:40:07,320
us.

521
00:40:07,320 --> 00:40:12,760
I personally believe that the only way in which you can guarantee that things actually can

522
00:40:12,760 --> 00:40:16,760
get better for everybody is if everybody has a say in how it's used.

523
00:40:16,760 --> 00:40:22,000
I think right now, especially in America where I live, things are going pretty rapidly in

524
00:40:22,000 --> 00:40:28,320
the opposite direction, where ever more power gets concentrated into ever fewer hands and

525
00:40:28,320 --> 00:40:35,560
you even have large social media companies starting to get almost a monopoly of the truth,

526
00:40:35,560 --> 00:40:38,280
which gives them even more power.

527
00:40:38,280 --> 00:40:44,840
And as a scientist, I much prefer the democratic idea that everybody should be able to challenge

528
00:40:44,840 --> 00:40:48,760
everything and to make it.

529
00:40:48,760 --> 00:40:54,960
So Europe is, I think, actually totally key in this.

530
00:40:54,960 --> 00:40:59,200
We have this tradition in Europe of trying to build a society that ultimately really

531
00:40:59,200 --> 00:41:00,680
works for everybody.

532
00:41:00,680 --> 00:41:07,920
That's why we have free healthcare and free universities in Italy, not in the United States.

533
00:41:07,920 --> 00:41:15,760
And I would encourage you all to sort of envision how can we reinvent, reimagine sort of the

534
00:41:15,760 --> 00:41:23,040
welfare state 3.0, which is even more awesome because it has all this technology and remains

535
00:41:23,040 --> 00:41:30,600
very firmly aligned with what's actually good for people, all people, not just some

536
00:41:30,600 --> 00:41:31,600
tech nerds.

537
00:41:31,600 --> 00:41:32,600
Thanks.

538
00:41:32,600 --> 00:41:36,600
Does anyone else have a question?

539
00:41:36,600 --> 00:41:38,600
Yes, please.

540
00:41:38,600 --> 00:41:43,640
Oh, okay.

541
00:41:43,640 --> 00:41:53,840
So my question is, do you think, so let's say we are able to develop AGI, but we cannot

542
00:41:53,840 --> 00:42:01,680
impose like, so if we have the ability to develop AGI, and if this means that we cannot

543
00:42:01,680 --> 00:42:07,200
impose like, go alignment limits on it, should we do it?

544
00:42:07,200 --> 00:42:12,880
To have something intelligent as humans, it should have also like the bad qualities of

545
00:42:12,880 --> 00:42:13,880
humans.

546
00:42:13,880 --> 00:42:24,360
So if we can't control it, should we still build it?

547
00:42:24,360 --> 00:42:26,480
On one hand, of course not.

548
00:42:26,480 --> 00:42:32,680
On the other hand, the way the economy works, I think it's not realistic to say, let's

549
00:42:32,680 --> 00:42:38,760
just press pause on technology until we got our act together, because there's just so

550
00:42:38,760 --> 00:42:41,720
much money in it and so much power in it.

551
00:42:41,720 --> 00:42:47,800
I think what's the more realistic game plan is rather than trying to go, stop, stop, stop.

552
00:42:47,800 --> 00:42:52,360
To think of this as a race between the growing power of the technology and the growing wisdom

553
00:42:52,360 --> 00:42:56,240
with which we manage it, solve all of these problems.

554
00:42:56,240 --> 00:43:01,640
If we can't slow down the growth of power, what we can do is accelerate the growth of

555
00:43:01,640 --> 00:43:03,160
the wisdom.

556
00:43:03,160 --> 00:43:07,080
That's why it's so wonderful that Luca and his colleagues are organizing conversations

557
00:43:07,080 --> 00:43:09,520
about precisely this.

558
00:43:09,520 --> 00:43:11,440
How can we grow the wisdom?

559
00:43:11,440 --> 00:43:14,440
How can we invest more in AI safety research?

560
00:43:14,440 --> 00:43:21,720
How can we have more conversations in society about how we want to use this, how it should

561
00:43:21,720 --> 00:43:25,040
be regulated, who should be in charge, and so on?

562
00:43:25,040 --> 00:43:31,440
That I think is our best shot that we have.

563
00:43:31,440 --> 00:43:37,880
In Europe right now, the European Union is actually developing the EU AI Act, which is

564
00:43:37,880 --> 00:43:43,160
the first time in the West that there will be actually a law trying to steer things in

565
00:43:43,160 --> 00:43:44,160
the right direction.

566
00:43:44,160 --> 00:43:46,000
I personally think this is very exciting.

567
00:43:46,000 --> 00:43:50,160
In America, there's no meaningful attempts to regulate AI at all, because lobbyists are

568
00:43:50,160 --> 00:43:53,840
too powerful there.

569
00:43:53,840 --> 00:43:59,160
Sure enough, big social media companies from America have now sent more lobbyists to Brussels

570
00:43:59,160 --> 00:44:05,560
to fight this than the oil companies ever sent to Brussels.

571
00:44:05,560 --> 00:44:08,680
As Europeans, be mindful of this.

572
00:44:08,680 --> 00:44:12,800
This might be the first battle you can actually win, where you start laying down the ground

573
00:44:12,800 --> 00:44:19,240
rules in such a way that you score up a big victory for the wisdom.

574
00:44:19,240 --> 00:44:24,000
Then right here, I have a question.

575
00:44:24,000 --> 00:44:25,000
Good morning.

576
00:44:25,000 --> 00:44:27,000
Thank you for your presentation.

577
00:44:27,000 --> 00:44:29,840
My question is about your program.

578
00:44:29,840 --> 00:44:36,120
You said that you are a physicist, so I want to know how was your transition to AI and

579
00:44:36,120 --> 00:44:38,240
what was your challenges?

580
00:44:38,240 --> 00:44:39,240
Thank you.

581
00:44:39,240 --> 00:44:40,240
Thank you.

582
00:44:40,240 --> 00:44:42,520
Yeah, that's right.

583
00:44:42,520 --> 00:44:47,960
When I was a teenager, I used to lie in my hammock between two apple trees and think about and

584
00:44:47,960 --> 00:44:51,120
realize that I was just really excited by big questions.

585
00:44:51,120 --> 00:44:55,960
The two biggest was our universe out there, which is also what my first book was about,

586
00:44:55,960 --> 00:45:01,640
and then the second one was our universe in here, intelligence and the mind.

587
00:45:01,640 --> 00:45:07,320
Seven years ago, roughly, after spending my career on physics, I decided to dramatically

588
00:45:07,320 --> 00:45:11,580
change careers and start doing AI research instead.

589
00:45:11,580 --> 00:45:19,700
I could get away with it in terms of my job because they can't fire me, an MIT, whatever

590
00:45:19,700 --> 00:45:22,900
I do, a tenure.

591
00:45:22,900 --> 00:45:24,300
Your question is very good.

592
00:45:24,300 --> 00:45:25,420
How hard was it?

593
00:45:25,420 --> 00:45:31,340
I would say it certainly was quite hard to learn so many things in a new field, but it

594
00:45:31,340 --> 00:45:34,380
was also really fun.

595
00:45:34,380 --> 00:45:35,900
It's such a fascinating topic.

596
00:45:35,900 --> 00:45:36,900
You know that.

597
00:45:36,900 --> 00:45:37,900
That's why you're here.

598
00:45:38,220 --> 00:45:42,860
And I was actually surprised also by how much I could make use of things I knew from my

599
00:45:42,860 --> 00:45:44,380
past life.

600
00:45:44,380 --> 00:45:50,620
So in physics, I had worked a lot on dealing with large data sets and information theory.

601
00:45:50,620 --> 00:45:55,380
I love computers and apps, which is why as we heard in the intro, I wrote games when

602
00:45:55,380 --> 00:45:59,780
I was a teenager and so on.

603
00:45:59,780 --> 00:46:03,820
The message I have for all of you is if you're ever thinking about a career change in the

604
00:46:03,820 --> 00:46:09,940
future and you're really, really excited about it and thought it through a little bit, you

605
00:46:09,940 --> 00:46:11,780
go for it.

606
00:46:11,780 --> 00:46:17,340
You get one shot to live on this planet, so make it count.

607
00:46:17,340 --> 00:46:20,780
Yeah, another question.

608
00:46:20,780 --> 00:46:26,940
This is a very broad question, but do you think AI has to be understandable in order to be

609
00:46:26,940 --> 00:46:28,940
ethical or trustworthy?

610
00:46:28,940 --> 00:46:31,020
Oh, that's good.

611
00:46:31,020 --> 00:46:38,300
Do we need it to be understanding in order to be ethical or what was the last word?

612
00:46:38,300 --> 00:46:42,580
For trustworthy, yes.

613
00:46:42,580 --> 00:46:49,540
For trustworthy, for sure, I think we should trust things, machines, not because some sales

614
00:46:49,540 --> 00:46:51,660
representative says, oh, trust this.

615
00:46:51,660 --> 00:46:55,900
It's great that it has a little sticker on it saying AI, but rather because we can understand

616
00:46:55,900 --> 00:46:59,740
how it works.

617
00:47:00,700 --> 00:47:05,420
On the other hand, just because it does what it's supposed to does not in any way guarantee

618
00:47:05,420 --> 00:47:07,060
it's ethical.

619
00:47:07,060 --> 00:47:14,820
If some terrorist has built this slaughter bot, then it's very trustworthy and he programs

620
00:47:14,820 --> 00:47:21,100
it to go kill all people with a certain skin color, for example, it's completely trustworthy.

621
00:47:21,100 --> 00:47:27,020
It's going to obey its owner even if it's by maybe my standards completely unethical.

622
00:47:27,100 --> 00:47:29,100
Those are two separate things.

623
00:47:29,100 --> 00:47:34,380
What that means simply is that, yes, of course, we have to first of all make things trustworthy,

624
00:47:34,380 --> 00:47:39,660
otherwise we don't even have the luxury of talking about ethics, but it's not enough.

625
00:47:39,660 --> 00:47:46,660
We also have to have a very serious conversation in our society about how to make sure that

626
00:47:46,660 --> 00:47:52,020
we align the goals of people and companies and governments who have this technology to

627
00:47:52,020 --> 00:47:56,620
do what's good for society as a whole.

628
00:47:56,620 --> 00:48:02,060
I think we're not doing so great there either today.

629
00:48:02,060 --> 00:48:08,460
If you have a company that decides to chop down the rainforest or whatever, maybe their

630
00:48:08,460 --> 00:48:16,380
technology they use to do it is very trustworthy for them, but companies are also a kind of

631
00:48:16,380 --> 00:48:22,140
artificial intelligence, even though it's built out of people, not out of machines.

632
00:48:22,140 --> 00:48:27,140
With the same alignment challenge we have of making sure that the machines we build

633
00:48:27,140 --> 00:48:34,380
do what's good for humanity, we also have to apply that same approach to companies and

634
00:48:34,380 --> 00:48:37,540
other entities that control machines.

635
00:48:37,540 --> 00:48:41,260
Thank you very much.

636
00:48:41,260 --> 00:48:44,540
One more question here, another one over there as well.

637
00:48:44,540 --> 00:48:46,260
We are running out of time.

638
00:48:46,260 --> 00:48:50,540
If you have questions actually for Max, we're going to be doing a panel later on, so come

639
00:48:50,540 --> 00:48:57,860
and find me and we can ask those questions later on as well.

640
00:48:57,860 --> 00:48:59,860
Over here.

641
00:48:59,860 --> 00:49:05,460
One question about the James Webb Telescope, it's been like 10 days that it's operative,

642
00:49:05,460 --> 00:49:11,540
so I wanted to ask you how is AI going to impact the research on these infrared images

643
00:49:11,540 --> 00:49:18,500
that we now have about the deep universe, because we've never been able to look so deep.

644
00:49:18,500 --> 00:49:25,140
How is going to be the AI impact on that, and do we have some future projects on it?

645
00:49:25,140 --> 00:49:32,660
I'm very excited about the opportunity of using AI for science more broadly, and astronomy

646
00:49:32,660 --> 00:49:37,060
and extragalactic astrophysics is a great example of this, because we're getting such

647
00:49:37,060 --> 00:49:41,780
enormous amounts of data, that you just cannot do what you would do in the past, it's like

648
00:49:41,780 --> 00:49:46,300
give it to a grad student to look at it all and come back and tell you what they found.

649
00:49:46,580 --> 00:49:50,100
It'll go away for 100 years.

650
00:49:50,100 --> 00:49:56,220
We have already been quite successful using AI to analyze enormous amounts of astronomical

651
00:49:56,220 --> 00:50:01,620
data to figure out, to find all the stars in the galaxies in there, to figure out what's

652
00:50:01,620 --> 00:50:06,780
there, what's different, what's surprising.

653
00:50:06,780 --> 00:50:15,020
I think 10 years from now it'll be almost as difficult to find a physicist or astrophysicist

654
00:50:15,020 --> 00:50:20,580
who does not use any kind of machine learning, as it is today to find someone in those fields

655
00:50:20,580 --> 00:50:22,340
who says, I don't use mathematics.

