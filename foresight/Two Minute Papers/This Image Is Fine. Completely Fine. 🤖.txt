Dear Fellow Scholars, this is Two Minute Papers with Dr. Károly Zsolnai-Fehér.
Today, we are going to find out whether a machine can really think like we humans think.
The answer is yes and no.
Let me try to explain.
Obviously, there are many differences between how we think, but first, let's try to argue
for the similarities.
First, this neural network looks at an image and tries to decide whether it depicts a dog
or not.
What this does is that it slices up the image into small pieces and keeps a score on what
it had seen in these snippets.
Floppy ears, black snout, fur, okay, we're good.
We can conclude that we have a dog over here.
We humans would also have a hard time identifying a dog without these landmarks, plus one for
thinking the same way.
Second, this is DeepMind's deep reinforcement learning algorithm.
It looks at the screen much like a human would and tries to learn what the controls do and
what the game is about as the game is running.
And much like a human, first, it has no idea what is going on and loses all of its lives
almost immediately, but over time, it gets a bit of the feel of the game.
Improvements, good, but if we wait for longer, it sharpens its skillset so much that, look,
it found out that the best way to beat the game is to dig a tunnel through the blocks
and just kick back and enjoy the show.
Excellent.
Again, plus one for thinking the same way.
Now, let's look at this new permutation invariant neural network and see what the name means,
what it can do, and how it relates to our thinking.
Experiment number one.
Permutations.
A permutation means shuffling things around.
And we will add shuffling into this cart pole balancing experiment.
Here the learning algorithm does not look at the pixels of the game, but instead takes
a look at numbers, for instance, angles, velocity, and position.
And as you see, with those, it learned to balance the pole super quickly.
The permutation part means that we shuffle this information every now and then.
That will surely confuse it, so let's try that.
Here we go, and nice, didn't even lose it.
Shuffle again.
It lost it due to a sudden change in the incoming data, but look, it recovered rapidly.
And can it keep it upright?
Yes, it can.
So, is this a plus one or a minus one?
Is this human thinking or robot thinking?
Well, over time, humans can get used to input information switching around too.
But not this quickly.
So, this one is debatable.
However, I guarantee that the next experiment will not be debatable at all.
Now experiment number two, reshuffling on steroids.
We already learned that some amount of reshuffling is okay.
So now, let's have our little AI play Pong, but with a twist, because this time, the reshuffling
is getting real.
Yes, we now broke up the screen into small little blocks and have reshuffled it to the
point that it is impossible to read.
But, you know what, let's make it even worse.
Instead of just reshuffling, we will reshuffle the reshuffling.
What does that mean?
We can rearrange these tiles every few seconds.
A true nightmare situation for even an established algorithm, and especially when we are learning
the game.
Okay, this is nonsense, right?
There is no way anyone can meaningfully play the game from this noise, right?
And now, hold on to your papers, because the learning algorithm still works fine.
Not only on Pong, but on a racing game too.
Whoa!
A big minus one for human thinking.
But, if it works fine, you know exactly what needs to be done.
Yes, let's make it even harder.
Experiment number three, stolen blocks.
Yes, let's keep reshuffling, change the reshuffling over time.
And also steal 70% of the data.
And wow, it is still fine.
It only sees 30% of the game all jumbled up, and it still plays just fine.
I cannot believe what I am seeing here.
Another minus one, this does not seem to think like a human would.
So all that is absolutely amazing.
But what is it looking at?
Aha, see the white blocks?
It is looking at the sides of the road, likely to know what the curvature is, and how to
drive it.
And look, only occasionally, it peeps at the green patches too.
So does this mean what I think it means?
Experiment number four, if you have been holding onto your papers so far, now, squeeze
that paper, shuffling, and let's shovel in some additional useless complexity, which
will take the form of this background.
And my goodness, it still works just fine.
And the minus ones just keep on coming.
So this was quite a ride.
But what is the conclusion here?
Well, learning algorithms show some ways in which they think like we think, but the answer
is no, do not think of a neural network or a reinforcement learner as a digital copy
of the brain.
Not even close.
Now, even better, this is not just a fantastic thought experiment, all this has utility.
For instance, in his lecture, one of the authors, David Ha, notes that humans can also get
upside down goggles or bicycles where the left and right directions are flipped.
And if they do, it takes a great deal of time for the human to adapt.
For the neural network, no issues whatsoever.
What a time to be alive!
This episode has been supported by Lambda GPU Cloud.
If you're looking for inexpensive cloud GPUs for AI, check out Lambda GPU Cloud.
They've recently launched Quadro RTX 6000, RTX 8000, and V100 instances, and hold on
to your papers because Lambda GPU Cloud can cost less than half of AWS and Azure.
Plus, they are the only cloud service with 48GB RTX 8000.
Join researchers at organizations like Apple, MIT, and Caltech in using Lambda Cloud instances,
workstations, or servers.
Make sure to go to lambdalabs.com slash papers to sign up for one of their amazing GPU instances
today.
Our thanks to Lambda for their long-standing support and for helping us make better videos
for you.
Thanks for watching and for your generous support, and I'll see you next time!
