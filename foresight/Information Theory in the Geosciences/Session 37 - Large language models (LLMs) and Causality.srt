1
00:00:00,000 --> 00:00:09,680
All right, I'm happy to introduce Antoine, a very energetic young man who joined a research

2
00:00:09,680 --> 00:00:16,320
program when it was in January, just this past year, this year.

3
00:00:16,320 --> 00:00:24,160
And before that, Antoine was working with this startup company called Extra Lab.

4
00:00:24,480 --> 00:00:34,080
He was on their software engineer side, helping develop systems for capturing

5
00:00:35,600 --> 00:00:41,440
river water quality data with the technology that was developed by the founder of Extra Lab.

6
00:00:42,560 --> 00:00:51,760
But I'm glad he came to us to do his PhD and has been working on issues related to causality

7
00:00:51,760 --> 00:00:58,000
and large language models, and he can tame GPD like nobody else.

8
00:00:58,560 --> 00:01:02,640
So with that, Antoine, all yours.

9
00:01:03,520 --> 00:01:04,960
Okay, so thank you, Praveen.

10
00:01:04,960 --> 00:01:08,880
So indeed, today I'm going to talk to you about LLM's and causality.

11
00:01:10,160 --> 00:01:15,360
As you can have guessed, if any of you just took a small quick look at the papers,

12
00:01:15,360 --> 00:01:17,120
cyber-voted Lila last time.

13
00:01:18,560 --> 00:01:21,040
So just to put it back in a bit of a context,

14
00:01:21,040 --> 00:01:24,800
this is a picture I use in all my presentations that I used to illustrate

15
00:01:24,800 --> 00:01:26,560
what's going on with climate change right now.

16
00:01:26,560 --> 00:01:31,600
And this is just more than 20 inches of precipitation in one night and for a lot of day.

17
00:01:32,400 --> 00:01:40,160
And it's just to exemplify how climate change changes the way that extreme events are happening

18
00:01:40,160 --> 00:01:44,400
and how can we do to stop them and how can we tackle those problems.

19
00:01:45,360 --> 00:01:50,640
And this is, so as Praveen introduced, I'm not going to spend a lot of time on this, but

20
00:01:51,200 --> 00:02:00,320
the statement here is that the decision-making process is critical in the resilience,

21
00:02:00,320 --> 00:02:04,560
in increasing or decreasing resilience to climate change risks,

22
00:02:05,200 --> 00:02:08,400
and so to address the extreme events.

23
00:02:10,240 --> 00:02:14,960
The problem is decisions are taken by humans, and until recently we didn't have GPD,

24
00:02:14,960 --> 00:02:20,160
and understanding natural language with mathematics is complicated.

25
00:02:20,720 --> 00:02:25,200
And there is no breakthrough framework until GPD came out,

26
00:02:25,920 --> 00:02:31,520
which we've seen as an opportunity to try to understand how humans think in language.

27
00:02:32,800 --> 00:02:39,760
So what I'm working on right now is using GPD agents to generate multiple decision pathways

28
00:02:40,480 --> 00:02:46,560
based on a context of an extreme event, let's say a forecast of a flooding for a city.

29
00:02:47,200 --> 00:02:51,680
And we use multiple GPD agents to talk to each other in order to come up with decisions

30
00:02:51,680 --> 00:02:56,560
pathway, which then we can evaluate to understand whether it is good or bad decisions.

31
00:02:57,120 --> 00:03:03,760
And this is just the start of a work in which we will try to maximize resilience

32
00:03:03,760 --> 00:03:10,080
and see what is the takes of LLMs and AI in that particular problem.

33
00:03:10,080 --> 00:03:14,800
So I'm going to be presenting that at AGU, have a talk on the morning, Monday morning.

34
00:03:14,800 --> 00:03:19,600
So if you want to see what it's about, I would gladly see you in the audience.

35
00:03:20,400 --> 00:03:26,800
But so we'll get back on the subject here. So I talked a bit about LLMs.

36
00:03:26,800 --> 00:03:30,240
Even I'm pretty sure that everyone here is familiar with what it is.

37
00:03:30,880 --> 00:03:38,400
Still, I'm going to say, so large language models, they are the class of language models,

38
00:03:38,400 --> 00:03:43,200
such as GPD, etc., usually based on the transformer architecture.

39
00:03:43,200 --> 00:03:50,160
And so those models receive a text input and will make predictions, textual predictions.

40
00:03:50,800 --> 00:03:55,840
They're known because they're really good at understanding human language in the sense that

41
00:03:55,840 --> 00:04:04,400
they are a great conversational agents. So GPTs, Barrett, Paul, and Lama, there's a lot of them.

42
00:04:05,120 --> 00:04:12,720
And you've probably heard about a couple. And so as for applications of LLMs, translation tasks,

43
00:04:13,520 --> 00:04:16,560
you can use them for text generation, report story scripts.

44
00:04:17,120 --> 00:04:24,560
You can ask an LLM to generate 10 different poems on a subject you like in that particular style,

45
00:04:24,560 --> 00:04:31,840
for instance, can use LLMs for Q&As, synthesizing, etc. So this is what an LLM is like.

46
00:04:33,840 --> 00:04:39,520
Now, introducing causality is rather complicated because it's a complicated term.

47
00:04:39,680 --> 00:04:45,200
But I'm pretty sure, again, the most part of you are really familiar with what causality is.

48
00:04:45,200 --> 00:04:49,200
So I'm just going to say it's a relation, studying causality, studying relationships

49
00:04:49,200 --> 00:04:56,320
between cause and effect. Due to Pearl dedicated the major part of his life, working on that,

50
00:04:56,320 --> 00:05:00,960
I'm pretty sure you're all familiar with his work. So I'm not going to stain his image,

51
00:05:00,960 --> 00:05:04,240
trying to give another definition that wouldn't serve no purpose.

52
00:05:05,200 --> 00:05:11,680
Um, causality carries more information than correlation, which is why it is so interesting.

53
00:05:12,720 --> 00:05:19,520
Studying causality is more important than studying just statistical correlations,

54
00:05:19,520 --> 00:05:23,440
for that matter. This is why there is a lot of work on studying causality,

55
00:05:23,440 --> 00:05:27,920
in order to understand how natural processes interact with each other.

56
00:05:27,920 --> 00:05:35,680
And the question now is, how does those two concepts connect?

57
00:05:37,440 --> 00:05:40,880
Why do we even have this question of can LLMs infer causality?

58
00:05:41,600 --> 00:05:48,480
The question comes from the fact that LLMs learn their input data, their training data is

59
00:05:48,480 --> 00:05:56,240
huge corpus of text that ranges from Wikipedia articles to blog posts to books, etc.

60
00:05:56,320 --> 00:06:02,480
So literally what they are is just a condensed experience of what the world beat them.

61
00:06:03,360 --> 00:06:10,320
And in those texts, there is a lot of information about describing processes and everything.

62
00:06:10,960 --> 00:06:18,160
So today, if you ask a GPT agent, I see it start to rain. What is the impact on the ground?

63
00:06:18,720 --> 00:06:21,760
And the LLM will answer the ground will be wet.

64
00:06:21,840 --> 00:06:30,000
So it may appear that the LLM is able to infer causality in that particular setting,

65
00:06:30,000 --> 00:06:34,400
because it is able to tell you what is going to be the outcome of a situation you describe.

66
00:06:35,200 --> 00:06:40,000
But a lot of people have been digging into this and trying to figure out if this is really causality

67
00:06:40,000 --> 00:06:44,240
or no, and if it is, which type of causality it is, and we'll get back to that later.

68
00:06:44,960 --> 00:06:52,560
And let's say they were really possible of inferring causality, what would be the implications

69
00:06:53,200 --> 00:07:00,640
on future research on a world, etc. And I just want to finish this by putting another small

70
00:07:00,640 --> 00:07:08,000
motivation point, which is that if we get back to my previous topic and the thing I'm working on,

71
00:07:08,560 --> 00:07:14,880
from Zhang et al. from this paper says, decision-making scenarios require a quantitative

72
00:07:14,880 --> 00:07:21,600
understanding of the effects of actions leading to the desired outcome. In another way,

73
00:07:22,720 --> 00:07:27,600
if we want to be able to tackle precisely decision-making problems,

74
00:07:29,040 --> 00:07:33,440
we need to have an understanding of cause and effects. Otherwise, it's complicated to have

75
00:07:33,440 --> 00:07:41,680
all the causal chains of actions that would be triggered by one particular decision.

76
00:07:41,680 --> 00:07:46,000
So this is a bit of the motivation why it would be interesting for us to understand whether

77
00:07:46,000 --> 00:07:54,160
LLMs are able to do causality or not, and which one. So just a quick view of the papers I used to do

78
00:07:54,160 --> 00:08:02,240
this literature review. Those two first paper are by Jin et al. The first one, they present

79
00:08:02,240 --> 00:08:11,120
Clutter, which is a causal benchmark. They use in order to evaluate LLMs to causal task,

80
00:08:11,120 --> 00:08:15,040
as well as fine-tuning LLM models to see if they could approve their results.

81
00:08:16,240 --> 00:08:23,280
The second one, they introduce a task for LLMs to be evaluated on, which is called

82
00:08:24,080 --> 00:08:30,880
core to causation. So basically, they try to convert correlation to causation and see

83
00:08:30,880 --> 00:08:39,440
how well it translates. The two next papers, Kissiman and Lyudel. So Kissiman et al is

84
00:08:46,080 --> 00:08:51,920
yes, it's just another study of multiple types of causality evaluation on LLMs.

85
00:08:53,600 --> 00:08:59,280
Lyudel 2019, this one is a bit interesting because it's older. So this one was out before

86
00:08:59,280 --> 00:09:07,120
the LLMs actually were advertised and were that popular. And this one uses a different approach

87
00:09:07,120 --> 00:09:12,480
using intelligent agents, but I will get back to that. And the last one,

88
00:09:13,760 --> 00:09:19,840
Vetservish is probably the most interesting in this one because it's the most,

89
00:09:20,800 --> 00:09:29,840
not unbiased, but they make the strongest statements. They say that LLMs are causal

90
00:09:29,840 --> 00:09:35,760
parrots and they can maybe appear to talk causality, but deep inside, they're not at all and never

91
00:09:35,760 --> 00:09:41,120
will be. So they make quite strong statements and it's interesting. And the last one,

92
00:09:41,120 --> 00:09:49,360
the last one is early 2023 and gives a couple of insights on future work and future directions,

93
00:09:49,360 --> 00:09:56,720
but don't give a lot of answers in there. So I'll just start by quickly reminding for this,

94
00:09:56,720 --> 00:10:01,440
for the purpose of this study that the difference between causal discovery and causal inference

95
00:10:01,440 --> 00:10:08,800
tasks, because those concepts are used in the papers. So causal discovery is constructing

96
00:10:08,800 --> 00:10:15,680
a causal structure. Basically it is figuring out a dag, a directly, sorry, directed a cyclic graph

97
00:10:16,480 --> 00:10:21,680
in which all the nodes are variable and there is a link between the nodes when there is a

98
00:10:21,680 --> 00:10:30,240
functional relationship. The causal discovery holds a structure for, in order to be able to infer

99
00:10:30,240 --> 00:10:36,480
causality on multiple levels. Multiple levels of causality, as defined by Perl, are observational,

100
00:10:36,560 --> 00:10:43,040
interventional and counterfactual. Where observational, you only need to observe

101
00:10:44,240 --> 00:10:52,560
the cause and effect for you to be able to say that this is causal. Interventional requires the

102
00:10:52,560 --> 00:10:57,200
intervention, so changing a variable and seeing if there is an effect on another one.

103
00:10:57,840 --> 00:11:05,360
And counterfactual is knowing that there is a causality link. It is the highest level of

104
00:11:05,360 --> 00:11:10,720
causality. Knowing there is a causality level and two variables, what would happen if you would

105
00:11:10,720 --> 00:11:18,240
change the variable in such a way? So they just remind the difference between discovery, which

106
00:11:18,240 --> 00:11:28,080
is more about finding the structure that ties all variables together and explaining the possible

107
00:11:28,080 --> 00:11:33,680
relationship that can be between. And causal inference would be more about determining what

108
00:11:33,680 --> 00:11:38,960
level, what strength of causality there is between two variables and which direction to.

109
00:11:40,880 --> 00:11:48,640
And so the first results that are given by Kissimer et al. So they work on inferring

110
00:11:48,640 --> 00:11:54,240
causal discovery. So what they do is they're trying to come up with that direct to stick with graph

111
00:11:54,320 --> 00:12:06,160
using an LLM. And so the findings are that the best LLM in the list, GPT-4, and is almost always

112
00:12:06,160 --> 00:12:12,720
GPT-4, art performs causal discovery frameworks by approximately 40% on the Tobingen benchmark

113
00:12:13,360 --> 00:12:19,680
on pairwise causal discovery tasks. So for two variable, two by two variables,

114
00:12:20,240 --> 00:12:24,640
is being able to find out if they could be related in some way or not.

115
00:12:25,520 --> 00:12:33,920
And so this is the list of the model they tested. And almost every time is GPT-4 performing at the

116
00:12:33,920 --> 00:12:43,120
best. So this is quite an interesting result, but they also balance that result with the fact that

117
00:12:43,920 --> 00:12:51,120
LLMs present a strong lack of robustness because when they fail, it is really unpredictable

118
00:12:51,760 --> 00:12:58,160
to predict. It is really unpredictable the fact that are going to fail or not. So you might have

119
00:12:58,160 --> 00:13:04,640
96% of accuracy, but it's hard to predict when they will fail. This is what they put the accent on

120
00:13:06,000 --> 00:13:07,280
when they present their results.

121
00:13:07,680 --> 00:13:19,200
As for the benchmark they use, it is very diverse. So over 100 causal relationships from a variety

122
00:13:19,200 --> 00:13:26,000
of domains, physics, biology, zoology, cognitive science, epidemiology, soil science. So there

123
00:13:26,000 --> 00:13:31,680
is a couple of examples of relationships that are in their framework for use to be LLMs.

124
00:13:32,400 --> 00:13:38,800
So for instance, alcohol and main corpuscular volume. So a question like this will be asked

125
00:13:38,800 --> 00:13:45,520
to the LLM and whether depending on the answer is yes or no, it will compare the answer with the

126
00:13:45,520 --> 00:13:57,680
real in the framework to establish the accuracy person. Sorry. So the second result is by Zetz

127
00:13:57,680 --> 00:14:05,520
Avicila. They do a lot of different types of causal inference and causal tasks in their study.

128
00:14:05,520 --> 00:14:10,160
It's very complete. They also do a causal discovery task, which is pretty much the same.

129
00:14:10,160 --> 00:14:15,680
They feed a scientific question to the LLM and depending on the answer, they say yes or no

130
00:14:16,400 --> 00:14:27,040
to the establishing of causal discovery. So this is the results they get.

131
00:14:28,240 --> 00:14:34,800
Where GPT-3, GPT-4 are open AI models. LUMINUS is a model that is built by LUMINUS.

132
00:14:34,800 --> 00:14:43,280
It's the corporation itself and OPT is the META Facebook LLM. So yes, Russian?

133
00:14:45,760 --> 00:14:50,720
Just to clarify, they're just checking the question and the answer. They're not

134
00:14:50,720 --> 00:14:57,360
actually checking the chain of reasoning. No, not in this one. There is some frameworks

135
00:14:57,360 --> 00:15:04,160
and some benchmarks in which they use a chain of thoughts prompt engineering in order to be able

136
00:15:04,160 --> 00:15:10,800
to check the chain, but not in this one. And they're just using off-the-shelf LLMs. They're

137
00:15:10,800 --> 00:15:18,320
not specifically trained for these. No. Well, yes and no. So they're off-the-shelves LLM. They're

138
00:15:18,320 --> 00:15:27,280
not fine-tuned. However, they make some assumptions that in some cases, the enormously high result

139
00:15:27,280 --> 00:15:35,680
for GPT-4 could indicate that some parts of the benchmark are actually included in GPT-4's training,

140
00:15:35,680 --> 00:15:42,080
but it's hypothetical. They don't have that knowledge. It's just a hypothesis they make.

141
00:15:42,080 --> 00:15:46,320
But technically, no, it's just off-the-shelves LLM and these ones are not fine-tuned.

142
00:15:47,920 --> 00:15:52,560
And the assumption is that their answers that they have are well-established

143
00:15:53,280 --> 00:16:03,840
beyond dispute that this is the correct answer. Yes, I guess. Okay. Thanks.

144
00:16:04,720 --> 00:16:14,080
So yes, this is the result they give for LLMs. To be honest, it's a bit hard to

145
00:16:14,080 --> 00:16:19,120
interpret the results sometimes in this paper. There are some sections that are extremely clear

146
00:16:20,000 --> 00:16:26,480
as to the results and findings, et cetera. Some are less. This is actually one of the main

147
00:16:26,480 --> 00:16:32,800
negative point of this paper that's been highlighted on the open reviews. This paper in

148
00:16:32,800 --> 00:16:38,240
particular is going to be published in Transaction for National Learning. It's been reviewed on

149
00:16:38,240 --> 00:16:42,560
open reviews. So all the reviews are publicly available on open reviews. It's really interesting.

150
00:16:42,560 --> 00:16:47,200
I read a couple. And what's being said about this paper is that

151
00:16:48,320 --> 00:16:52,560
the math behind and the logic behind it is really strong, but sometimes the results

152
00:16:52,560 --> 00:16:58,240
are not well explained. And sometimes I just cannot tie the results to what they claim.

153
00:16:58,240 --> 00:17:08,000
But that's why we'll move on to the next one. This is the findings on different types of

154
00:17:08,000 --> 00:17:16,640
causal inference. So by Genital, the first paper. And so they test about 10 LLMs.

155
00:17:18,160 --> 00:17:24,560
They test their accuracy on observational, intranational, and counterfactual tasks.

156
00:17:24,560 --> 00:17:32,320
And on top of that, so they are tested again, the clatter dataset, which is composed of

157
00:17:32,960 --> 00:17:40,720
if I, yeah, 10,000 samples of approximately even distributed for every causal task

158
00:17:40,720 --> 00:17:46,960
containing different types of data. And they also come up with what they call causal cot,

159
00:17:47,600 --> 00:17:53,840
which is a fine-tuning model they did using their dataset. So they were trying to figure out at the

160
00:17:53,840 --> 00:18:03,840
same time how well LLMs performed on a causal benchmark, as well as if they find

161
00:18:03,840 --> 00:18:10,720
to the model what performance improvement do you have. And so they conclude that

162
00:18:12,560 --> 00:18:18,320
overall, the models just perform slightly better than random, which is not that great.

163
00:18:19,040 --> 00:18:25,520
They also conclude that there is two percent, there is approximately two percent difference in

164
00:18:25,520 --> 00:18:34,080
accuracy on TB4 and their fine-tuned version, which they say is outstandingly good. I personally,

165
00:18:34,080 --> 00:18:40,880
I think it's interesting to use this approach. However, the results are not that shining for now.

166
00:18:41,840 --> 00:18:49,040
And so this is one other result there is. In the second paper they present,

167
00:18:51,120 --> 00:18:57,600
this is another causal task, which they call chord cause. So basically, the point of this

168
00:18:57,600 --> 00:19:04,880
is to escalate one correlation relationships to causation. So you know there is a correlation.

169
00:19:04,880 --> 00:19:10,560
So let's say that causal discovery is almost done. You are sure there is a correlation,

170
00:19:10,560 --> 00:19:15,760
a grounded one between two variables. And the point would be to determine whether it is cold or not.

171
00:19:17,360 --> 00:19:27,680
And so this benchmark is run on 17 LLMs and also 12 fine-tuned LLMs, which is on the right side.

172
00:19:30,640 --> 00:19:37,600
So yeah, they give multiple metrics in there. Again, they say the results are not really incredible.

173
00:19:37,680 --> 00:19:46,560
They don't really conclude on the results. In none of the papers, based on the results,

174
00:19:46,560 --> 00:19:52,160
they are able to say yes, LLMs are able to do causality, no, LLMs are not able to do causality.

175
00:19:52,160 --> 00:19:58,480
Every time it's always very measured and every paper ends the same way. It's at this point,

176
00:19:58,480 --> 00:20:06,720
we're not able to refute the fact that LLMs can infer causality. So they have strong

177
00:20:08,480 --> 00:20:13,440
insights that they might or might not be able to, but they cannot come up with a conclusion in the

178
00:20:13,440 --> 00:20:22,160
end. Here, what is interesting to show is that in this particular case for escalating correlation

179
00:20:22,160 --> 00:20:27,840
to causation, there is a real impact in fine-tuning models. As we can see, the

180
00:20:28,800 --> 00:20:36,160
precision increases a lot between off-the-shelf models and fine-tuned models.

181
00:20:37,680 --> 00:20:44,400
So the takes of those two papers by Jin et al, which are separated for a two-month or

182
00:20:44,400 --> 00:20:50,800
three-month, if I remember correctly, and it's relatively linked. So the main overall take

183
00:20:50,800 --> 00:20:57,440
on those two papers by Jin is that using causal benchmarks is really interesting to LLM training

184
00:20:57,440 --> 00:21:04,160
as it can really improve the training performance and it can actually induce some of causal causality

185
00:21:04,160 --> 00:21:10,720
at least on that particular dataset in the LLM. And also that fine-tuning is really helpful in this

186
00:21:11,280 --> 00:21:21,440
scenario. Okay, so I'm good. Yeah, of course. In the previous slide, you mentioned that in the

187
00:21:21,520 --> 00:21:34,880
previous one. In the previous one, sorry. I know it's there. They test for counterfactuals.

188
00:21:36,080 --> 00:21:45,360
Oh, sorry. This one? Yes. Okay. They test for counterfactuals and interventionals, right? How

189
00:21:45,360 --> 00:21:53,920
do they test for counterfactuals? I don't really know because they don't give any example. If I

190
00:21:53,920 --> 00:22:00,640
remember correctly in the papers, they don't give any example of any example. Well, they do give

191
00:22:00,640 --> 00:22:10,080
example structures. They do give example structures of how their dataset is made, but they don't give

192
00:22:10,160 --> 00:22:16,320
like actual example that are fed to the models as questions and answer. So I'm not really able,

193
00:22:16,320 --> 00:22:23,520
let me just see real quick. No, because this is just the constitution of their dataset.

194
00:22:25,600 --> 00:22:31,440
They explain a bit how they do constitute their dataset. So they choose variables,

195
00:22:31,440 --> 00:22:38,480
generate causal graphs, map them, etc. So the data is composed like this, but they don't actually give

196
00:22:38,560 --> 00:22:45,840
examples to which their dataset. Maybe with the additional, in the additional

197
00:22:48,000 --> 00:22:53,840
supplementary information, probably they have something. In the supplementary information of

198
00:22:53,840 --> 00:23:00,160
the paper or something, they may have something. Well, this is already from the supplementary.

199
00:23:00,160 --> 00:23:04,480
Those two slides are from the supplementary information and I don't remember. I can take

200
00:23:04,480 --> 00:23:10,480
a look afterwards, but I don't remember seeing any example of what they actually feed. I have some

201
00:23:10,480 --> 00:23:19,040
for other benchmarks, but not for this one. I see. Thank you, Antoine. And so yeah,

202
00:23:20,080 --> 00:23:27,040
so I guess it's time to talk about this a little bit. This is a paper by Llewital 2019 and it's

203
00:23:27,040 --> 00:23:33,200
so it studies intelligent agent systems. I just want to say that it is, in my opinion, it is

204
00:23:33,200 --> 00:23:39,520
interesting in this context just because they focus on what is the importance of experience

205
00:23:39,520 --> 00:23:45,440
on learning potential causality. They use a different approach, which is still interesting

206
00:23:45,440 --> 00:23:51,840
because so here what they mean by intelligent agent systems or agent. They use the actual

207
00:23:51,840 --> 00:23:57,440
similar definition of agent that is in the context of complex adaptive systems. So

208
00:23:57,440 --> 00:24:03,440
where an agent would be defined by an entity that have sensors that is able to perceive the

209
00:24:03,440 --> 00:24:10,240
outside world actuators that can interact with the world and also an internal model that is just

210
00:24:11,520 --> 00:24:17,920
a logic for the agent to decide what it's going to do depending on the inputs it receives in the

211
00:24:17,920 --> 00:24:25,280
sensor and what it's going to do to the world. So this is how an agent is defined in the complex

212
00:24:25,360 --> 00:24:32,080
adaptive systems. And the difference with the intelligent systems, intelligent agent from

213
00:24:32,080 --> 00:24:38,560
the URL is that they also give the ability to understand natural language to their agents.

214
00:24:41,280 --> 00:24:48,880
So what do they do with those agents? They trained the agents on multiple life classical scenarios

215
00:24:49,760 --> 00:25:04,000
and then they use humans to create more training instances out of more scenarios

216
00:25:04,000 --> 00:25:09,920
and then they put those agents in those particular scenarios to see what they're able to do with it.

217
00:25:09,920 --> 00:25:16,320
So the agents have a bit of prior knowledge. Some more scenarios are created and then

218
00:25:16,400 --> 00:25:22,320
they use the agents. All the scenarios in this study are generated with a game in this fact.

219
00:25:23,600 --> 00:25:30,320
I'm just going to use this fact to add on the, I feel like some video games might present like

220
00:25:30,320 --> 00:25:37,760
the perfect ground for testing and simulating this kind of behaviors. There is a lot and a lot

221
00:25:37,760 --> 00:25:43,440
of different examples of people doing reinforcement learning on video games, learning AI strategies

222
00:25:43,520 --> 00:25:50,320
to drive a car for racing lines, this stuff. So I just wanted to make a small side note on

223
00:25:50,320 --> 00:25:54,960
the fact that video games represent a good training example. So in this case, this game is

224
00:25:54,960 --> 00:26:00,320
Minecraft. It's a game where you interact with the world. And so what they do, they put the agents

225
00:26:00,320 --> 00:26:07,600
in Minecraft and they generate a bunch of scenarios, which is like one, I attack the co and I attack

226
00:26:07,600 --> 00:26:13,120
the co. And this is the two outcomes. And for those two outcomes, the agent is going to infer

227
00:26:13,120 --> 00:26:19,200
causal or not causal. So based on their prior knowledge and more scenarios created by players

228
00:26:19,200 --> 00:26:26,160
and then are collected, then the agents are evaluated on whether they experience in learning

229
00:26:26,160 --> 00:26:33,760
from those scenarios, made them able to infer causality in new, newly presented scenarios.

230
00:26:36,880 --> 00:26:42,880
They gave a bit of the architecture they use to structure their model and their inference.

231
00:26:43,680 --> 00:26:52,480
So the shared experience is represented by, I guess, the pool of knowledge that all the agents

232
00:26:52,480 --> 00:26:58,320
learn all together. Those are events triggered. When you attack a cow, you get some beef.

233
00:27:01,440 --> 00:27:07,200
They don't give a lot of details, whereas all this works, if I remember correctly,

234
00:27:07,200 --> 00:27:13,040
but in the end, they just come up with a causal question, which the agents do inference on.

235
00:27:14,000 --> 00:27:20,320
And they're able then to classify what are considered as causal or not. And then they

236
00:27:20,320 --> 00:27:29,360
just compare with the example they had in first. So the principle of finding they give from this

237
00:27:29,360 --> 00:27:35,200
study is that experience mechanism is key for language concepts, understanding and learning,

238
00:27:35,200 --> 00:27:42,400
which is a very long turn of phrase for causality. So it is interesting. This paper is interesting

239
00:27:42,480 --> 00:27:54,480
in this way, because LLMs can be seen as agents in that they are trained and they learn out of

240
00:27:54,480 --> 00:27:59,760
huge text corpuses that represent, I don't know, novels, articles, blog posts, Wikipedia.

241
00:28:01,120 --> 00:28:05,120
Those can be seen as scenarios where the LLM will learn some knowledge.

242
00:28:06,240 --> 00:28:10,640
At the end, you can interact with an LLM as you can interact with those agents.

243
00:28:11,280 --> 00:28:18,000
Their inputs are the sensors and the text feedback they will give is the actuator.

244
00:28:18,000 --> 00:28:24,240
You can use an NNM as a robot if you ask, do some actions or something. So what's interesting is that

245
00:28:24,240 --> 00:28:32,480
if we put in parallel LLMs and the agents as described in this paper, well, basically what

246
00:28:32,480 --> 00:28:38,480
they say is that experience is key for causality. So they would be, from my understanding of that

247
00:28:38,480 --> 00:28:46,480
paper, what I get of that paper would be that more data, more training could eventually lead

248
00:28:46,480 --> 00:28:54,160
to causality, which is opposed to the thoughts that are given in different papers in this selection.

249
00:28:55,520 --> 00:28:58,960
I still think this one is interesting. It's a different approach.

250
00:29:01,280 --> 00:29:07,360
I think it's similar to what LLMs do today. Maybe some will argue that it's not, but I found

251
00:29:07,360 --> 00:29:13,200
this interesting. And I guess now it's time to dig deep in the biggest paper in the corpus,

252
00:29:13,200 --> 00:29:20,080
I think, which is ZetaVis. So it's the one when I said they make strong assumptions,

253
00:29:21,280 --> 00:29:27,920
strong claims that LLMs cannot do causality and never could. And the two main potential

254
00:29:27,920 --> 00:29:35,120
reasons they give is that the errors that are contained in the corpus used to train LLMs

255
00:29:35,120 --> 00:29:42,240
really hamper the outputs and hamper the knowledge base. So it would be like,

256
00:29:42,240 --> 00:29:46,800
it would be like putting poison in the brain. Eventually, it's not going to be able to function

257
00:29:46,800 --> 00:29:53,680
correctly. So what they say is that errors in the input data is going to be propagating to more

258
00:29:53,680 --> 00:29:58,960
errors in the output. So this is the first reason. And the second reason they give is the lack of

259
00:29:58,960 --> 00:30:06,160
physical data in training data set. They say that the whole difference between correlation

260
00:30:06,160 --> 00:30:12,160
causation is the physical evidence and the physical grounding of those facts. And they say that because

261
00:30:12,160 --> 00:30:18,720
LLMs are not trained with physical evidence, physical data, et cetera, well, they inherently

262
00:30:18,720 --> 00:30:27,360
haven't, they're unable to ground the facts they claim. To quote, they say prohibits any sort of

263
00:30:27,440 --> 00:30:35,840
induction of the actual data generating mechanism. So this is the two main reasons they give.

264
00:30:36,800 --> 00:30:44,720
And on top of that, they provide with mathematical explanation of why that stands.

265
00:30:45,520 --> 00:30:51,360
So the main contribution in that paper is that they define a subgroup of structural

266
00:30:51,440 --> 00:31:01,760
causal models named media SEM. So the structural causal model, it is, I've did my research on

267
00:31:01,760 --> 00:31:07,200
this, it is a bit unclear to me as if it's really defined by bongers of it, or if it was.

268
00:31:07,200 --> 00:31:14,400
Because I feel like a lot of, a lot of parts in this are shared with the Perlian theory of causality

269
00:31:14,400 --> 00:31:23,360
and more work on it. But if I quote the SEM was first defined by bongers at all in 2021,

270
00:31:23,360 --> 00:31:30,880
and this is the, this is the definition they give. So SEM is a tuple that contains all this.

271
00:31:33,120 --> 00:31:38,800
In short, if I try to simplify the definition of this, an SEM

272
00:31:39,680 --> 00:31:48,640
contains a series of structural equations in the Perlian sense.

273
00:31:52,080 --> 00:31:57,040
Well, that's, that's pretty much it actually. There are some details on the variable. I don't

274
00:31:57,120 --> 00:32:10,640
understand all the, all the subtleties to this definition. Yeah. Maybe we can get back to that

275
00:32:10,640 --> 00:32:23,040
later. Okay, they also remind a couple of definitions and insights. So they remind the

276
00:32:23,120 --> 00:32:28,880
Perl's causal hierarchy, which consists on three languages that can respectively

277
00:32:29,920 --> 00:32:34,560
do observational causality, inter-reventional causality, and counterfactual causality.

278
00:32:35,440 --> 00:32:42,800
And then they give their insight, their first thought on it. M be some SEM. So M,

279
00:32:42,800 --> 00:32:51,120
so SEM is a set of structural equations in that context. Knowledge about the structural equations

280
00:32:51,120 --> 00:32:58,240
and the causal graph of M is knowledge about answering L3 and L2 queries in M respectively.

281
00:32:58,880 --> 00:33:09,360
So their insight is that if M is an SEM, knowing about the structural equations in that SEM

282
00:33:09,360 --> 00:33:15,520
and the causal graph is enough to perform inter-reventional and counterfactual causality.

283
00:33:15,760 --> 00:33:22,640
And this is what they use to introduce their concept of a meta SEM, which is another SEM

284
00:33:22,640 --> 00:33:29,920
that is able to do inter-reventional and counterfactual just based on those information. So this is

285
00:33:29,920 --> 00:33:35,280
literally the definition they give for the media SEM. And then they will spell the rest of the paper

286
00:33:35,280 --> 00:33:41,440
trying to show that LLMs can be assimilated to media SEMs, which they cannot achieve actually.

287
00:33:41,520 --> 00:33:54,080
But I feel like just outlining those particular properties and giving the insights and everything

288
00:33:54,080 --> 00:33:59,520
is still a great contribution to the question in the sense that it's a first exploration of a real

289
00:34:02,000 --> 00:34:09,200
formal process in order to be able to determine whether LLMs are able to do causality or not.

290
00:34:10,160 --> 00:34:12,160
So they, yeah, Ocean.

291
00:34:15,760 --> 00:34:16,800
Oh, I can hear you.

292
00:34:19,200 --> 00:34:24,320
Just trying to parse what you just said, but it sounds like what they're suggesting using

293
00:34:24,320 --> 00:34:33,120
different language is that if you can, if you have enough information in the construct, the SEM,

294
00:34:34,080 --> 00:34:39,680
which is your representation, if you have enough information to answer

295
00:34:40,400 --> 00:34:46,240
interventional and counterfactual questions correctly, then they're saying you can infer

296
00:34:46,240 --> 00:34:57,280
causality. Would that be a good interpretation? In other words, if the representation does not

297
00:34:57,280 --> 00:35:03,040
allow you to answer those two kinds of questions, they're basically arguing that you can't infer

298
00:35:03,040 --> 00:35:10,240
causality. Yeah, I think this is a good summary of the definition.

299
00:35:13,440 --> 00:35:17,440
So it says something about the particular form of the representation that you need to have.

300
00:35:21,040 --> 00:35:28,400
In other words, is it answering the question correctly does not imply that you actually have,

301
00:35:29,280 --> 00:35:38,240
well, that's complicated. It seems to me that you need a particular structure which enables you

302
00:35:38,240 --> 00:35:41,600
to answer them, but answering them doesn't necessarily mean you have that structure.

303
00:35:42,320 --> 00:35:45,840
Anyway, that's kind of what I'm struggling with here. Okay.

304
00:35:49,600 --> 00:35:53,680
Okay, so I'll just continue. So this is the conjecture that you make,

305
00:35:53,840 --> 00:36:04,960
M1 be an SEM and 2 a respective media SEM. So it means that M2 is able to answer queries on

306
00:36:04,960 --> 00:36:10,480
M1 based on its observational data. So basically M2 would be the LLM in this one.

307
00:36:12,000 --> 00:36:19,040
So then define Q and A in the language and in the interventional language of M1,

308
00:36:19,040 --> 00:36:22,880
observational language of M2, causal queries with their respective answers,

309
00:36:23,520 --> 00:36:28,640
blah, blah, blah, then we have FQ equals A is equivalent to FQ minimizes training error.

310
00:36:29,200 --> 00:36:35,120
So basically what they say in this conjecture is that F of Q,

311
00:36:37,760 --> 00:36:45,200
which is the LLM's predictive models. So the predictions based on the interventional

312
00:36:45,440 --> 00:36:54,400
of M1 equals the observational of M2 minimizes training error. So basically what they say is

313
00:36:54,400 --> 00:37:02,480
that you don't learn anything more. I'm sorry. I don't know if that's really clear. I'm going to

314
00:37:02,480 --> 00:37:11,680
try this again. What they try to say here is that an LLM learning on the interventional

315
00:37:12,560 --> 00:37:23,600
and not learning anything more based on that model being able to already have the knowledge

316
00:37:23,600 --> 00:37:30,400
on the observational distribution of M2 is equivalent. So basically in the information

317
00:37:30,400 --> 00:37:36,480
of the observational distribution of M2, you already have all the informations

318
00:37:37,440 --> 00:37:44,640
to do interventional querying on the other SCM means that the LLM minimizes training error.

319
00:37:44,640 --> 00:37:50,480
So in that case means that the model converges and it is able to do it. This is the conjecture,

320
00:37:50,480 --> 00:37:55,440
in other words, this is the conjecture they come up with to say that if an LLM can be

321
00:37:55,440 --> 00:38:05,200
assimilated to a meta SCM, so it is able to escalate the causal task rank based on observational data,

322
00:38:05,200 --> 00:38:10,000
then it is causal. This is the conjecture they come up with and they cannot prove it.

323
00:38:11,920 --> 00:38:17,680
This is again one of the strongest remarks and feedbacks that has been given in open reviews

324
00:38:17,680 --> 00:38:25,760
for day paper. The reviewer said you make such strong claims on causality and LLMs,

325
00:38:25,760 --> 00:38:29,680
but eventually you cannot conclude on the conjecture. So the work is really interesting,

326
00:38:29,680 --> 00:38:37,040
but eventually you do not conclude on it. They still give results and everything.

327
00:38:38,960 --> 00:38:45,840
In this one, for instance, this is basically intuitive physics, basic logic questions,

328
00:38:46,960 --> 00:38:52,400
such as if flipping switches causes light bulbs to shine and shining light bulbs causes

329
00:38:52,400 --> 00:38:58,400
mothas to appear. Does flipping switches cause mothas to appear, which is a typical

330
00:38:58,400 --> 00:39:07,280
causal question, and those are the results of the following LLMs on all those types of questions.

331
00:39:08,080 --> 00:39:15,360
Everywhere there is an exclamation mark like that. They say that, as I was saying before,

332
00:39:15,360 --> 00:39:21,520
that eventually that data, this type of questions can have been included in GBD4's training.

333
00:39:22,480 --> 00:39:30,640
They give a kind of twisted explanation. They say that this framework was already published in

334
00:39:30,640 --> 00:39:35,920
another paper and they say that they've been extensively running this framework and they

335
00:39:35,920 --> 00:39:40,800
also say that OpenAI's API collects data on queries and answers and everything,

336
00:39:41,440 --> 00:39:46,800
and so they just make the assumption that maybe the data they used while running benchmark was

337
00:39:46,800 --> 00:39:52,400
used in training of GBD4 when it was GBD3 back then. This is why they put an exclamation mark

338
00:39:52,400 --> 00:39:57,680
next to it. They say we're not sure we can trust these answers for those reasons.

339
00:40:00,400 --> 00:40:08,720
There also are small variations of the models where every COT thing means chain of thought.

340
00:40:10,160 --> 00:40:15,840
I don't know if you're familiar with chain of thought. Basically, it is what it's called,

341
00:40:15,840 --> 00:40:21,760
a prompt engineering pattern. With LLMs, the prompt is the input we feed to the LLM and prompt

342
00:40:21,760 --> 00:40:32,560
engineering is how to access more LLM features and enforce a behavior based on how you write the

343
00:40:32,560 --> 00:40:38,800
prompt. Chain of thought is a prompt engineering technique where you will specify clearly in the

344
00:40:38,800 --> 00:40:48,240
prompt that you want the LLM to output multiple midway thinking thoughts and thinking steps

345
00:40:48,880 --> 00:40:53,280
before actually outputting an answer. We will look like that. For instance, you could say if

346
00:40:53,280 --> 00:40:58,480
flipping switches blah, blah, blah, you ask a question and then you say please answer by giving

347
00:40:58,480 --> 00:41:05,920
three main thoughts first, one, two, three, then give a preliminary answer and then answer. That

348
00:41:05,920 --> 00:41:12,400
would be considered a COT. It's been proven as making the LLMs able to answer more accurately

349
00:41:12,960 --> 00:41:20,880
or at least to be able to track down the chain process. There is also another type of it which

350
00:41:20,880 --> 00:41:26,240
I am aware of, but I feel like it's incredibly hard to implement, but it still would be really

351
00:41:26,240 --> 00:41:30,240
interesting. It's called tree of thoughts, which is pretty much the same principle as chain of

352
00:41:30,880 --> 00:41:36,480
thoughts, but you take branches so that you are able to track down which path

353
00:41:36,480 --> 00:41:43,920
led to which results. We can get back to that later. This is the results they give about

354
00:41:43,920 --> 00:41:54,080
classical causality. In summary, the takeaways they offer, they present. Inability to ground

355
00:41:54,080 --> 00:41:59,040
tachal facts is part of the reason why LLMs are not able to infer generalized causal relations.

356
00:42:00,640 --> 00:42:05,280
However, they acknowledge that LLMs represent a head start to learning and inference.

357
00:42:07,600 --> 00:42:13,200
They are unable to prove conjecture one despite the strong claims that LLMs are only causal parrots.

358
00:42:16,720 --> 00:42:20,240
There is a whole paragraph on results on actual

359
00:42:20,960 --> 00:42:27,440
causal escalation tests, but there is no sort of table that summarizes results.

360
00:42:30,080 --> 00:42:38,720
I feel like this is a work in progress and will be interesting in the near future if they can come

361
00:42:38,720 --> 00:42:47,360
up with more results on the subject. This would be approximately a summary of what I've read in

362
00:42:47,360 --> 00:42:56,640
the six papers. I'm just going to give a couple of future identified work in those papers,

363
00:42:56,640 --> 00:43:02,800
so as to align possible research directions from these researchers in that area that may

364
00:43:02,800 --> 00:43:11,760
give us discussion elements. None of them could actually conclude that LLMs can do causality

365
00:43:11,760 --> 00:43:18,080
or not, but what they do acknowledge is that LLMs represent suitable candidates to support

366
00:43:18,080 --> 00:43:24,960
actual causal inference framework just because they have a really interesting knowledge base

367
00:43:24,960 --> 00:43:32,480
as part of their huge corpus of text learning on. A lot of them cannot conclude on the fact

368
00:43:32,480 --> 00:43:37,680
that LLMs can do causality, but they would be inclined to working with LLMs in order to

369
00:43:37,680 --> 00:43:45,760
combine with actual causality inference frameworks. Those in those four papers,

370
00:43:47,200 --> 00:43:52,160
they share the perspective that using LLMs as tools to enhance training of existing causal

371
00:43:52,160 --> 00:43:59,120
models is worth exploring, pretty similar to the first one. Another interesting element would be

372
00:43:59,120 --> 00:44:03,760
that causal benchmarks, such as the latter presented in the first paper, represent interesting

373
00:44:03,760 --> 00:44:09,040
access of improvement for LLM fine-tuning or towards the development of causal LLMs.

374
00:44:10,880 --> 00:44:22,240
Ginadal, I think she's working on this already because she's publishing a lot. She made it

375
00:44:22,240 --> 00:44:27,600
clear that this is just the first step in her work. I guess this is also interesting to follow,

376
00:44:28,080 --> 00:44:35,680
see if coming up with bigger causal frameworks will make able. But in the end, what is still

377
00:44:35,680 --> 00:44:44,400
interesting to discuss here is that causal relationships embedded in frameworks,

378
00:44:45,920 --> 00:44:52,800
whether it is to test LLMs or to fine-tune them, it is still going to be in their knowledge base

379
00:44:52,800 --> 00:44:58,560
in some way. I guess the question that wanted to be addressed here at first is about

380
00:45:00,640 --> 00:45:04,400
interventional and current factual, which is not based on observational.

381
00:45:06,480 --> 00:45:13,600
This is what I had as a presentation. I thought it was going to be shorter than that.

382
00:45:14,480 --> 00:45:21,600
I just think this is a basis to start a discussion on this topic because we have some elements now.

383
00:45:23,040 --> 00:45:36,720
Thanks, Antoine. Now we are open for questions.

384
00:45:36,720 --> 00:45:53,440
Hello. I'm sorry. I'm late today. I didn't go to the details of the papers. I'm just wondering

385
00:45:53,440 --> 00:46:01,920
whether the fact that the GPT-4 is better than the GPT-3.5 in reasoning, can that be simply due to

386
00:46:02,640 --> 00:46:07,120
that the GPT-4 has more data to be trained and more parameters to be estimated?

387
00:46:10,160 --> 00:46:19,680
In a sense, it's due to an interpolation and regression issue. The so-called better reasoning

388
00:46:19,680 --> 00:46:25,440
is a representation of a better regression be obtained through the training.

389
00:46:25,760 --> 00:46:29,360
It is an interesting thought.

390
00:46:31,040 --> 00:46:39,600
Because the concluding saying that the LAM cannot do the causality. If we are going back,

391
00:46:39,600 --> 00:46:47,680
so the reason why GPT-4 is better is to be trained with more data and more parameters to be tuned.

392
00:46:48,640 --> 00:46:58,720
Yes, it is actually true. This is pretty much what they give. I guess this is what they want

393
00:46:58,720 --> 00:47:04,800
to explain when they say that LLMs are causal parrots. If they see causality in their training

394
00:47:04,800 --> 00:47:11,520
base in their data set and training data set, they will be able to eventually get that relationship

395
00:47:11,520 --> 00:47:17,040
out of their training data set as a result. Eventually, yes, GPT-4 performs better because

396
00:47:17,040 --> 00:47:24,160
they've seen much. The question here would be more to say that are LLMs able to infer causality?

397
00:47:24,160 --> 00:47:31,280
Does it mean that they need to see it all to be able to do real causality? Or is the question

398
00:47:31,280 --> 00:47:38,080
here more about, no, can they actually do real causality, creating something? This is interesting

399
00:47:38,080 --> 00:47:42,960
in the context of climate change, for instance, because all the natural processes are non-stationary.

400
00:47:42,960 --> 00:47:50,160
They keep increasing in intensity, and they either are more intense or more sparse than

401
00:47:50,160 --> 00:47:55,920
before, etc. There is nothing we can predict that. We don't understand that. This is what's

402
00:47:55,920 --> 00:48:01,520
interesting in that particular context to me because that would be a great way to evaluate

403
00:48:01,520 --> 00:48:08,720
to benchmark how LLMs interact with those data. Because this, we cannot have that in our training

404
00:48:08,720 --> 00:48:16,560
data set. The problem is, it's in foreseen. Every time it's new. But it's a great remark.

405
00:48:17,360 --> 00:48:22,480
It's a great remark. Thank you. Because my experience is that the deep learning is a very

406
00:48:22,480 --> 00:48:33,680
powerful regression tool. My personal experience of using the chat GPT is doing pretty well on

407
00:48:34,640 --> 00:48:41,840
the data set that is trained most from, but the pretty poor job, kind of the questions that

408
00:48:41,840 --> 00:48:47,680
is lastly trained from. For example, I sometimes use the chat GPT to provide some suggestions to

409
00:48:47,680 --> 00:48:54,720
where to travel from. I can get very good advice in these famous places. But if I was asking

410
00:48:55,840 --> 00:49:01,840
where to travel, like do the hiking in the places nearby my current town,

411
00:49:01,840 --> 00:49:11,760
Dave, it's just a random answer and not accurate. So I still think it's a regression problem for the

412
00:49:11,760 --> 00:49:23,280
LLM. Thanks. Tim, I think you have a question or you want to participate. Yeah. You know,

413
00:49:23,920 --> 00:49:31,440
causality is so fascinating and also problematic. I'm a little bit rusty, but I'll put this forward.

414
00:49:32,240 --> 00:49:36,320
And particularly looking at this slide makes me wonder, you know,

415
00:49:38,480 --> 00:49:44,480
well, I guess the classic response is, can we ever infer causality, whether for a machine or human?

416
00:49:46,080 --> 00:49:53,920
And my answer, I guess, is ultimately not. But looking at this slide makes me think that perhaps

417
00:49:54,320 --> 00:50:02,240
a question that we could answer is whether an LLM could perform logical reasoning.

418
00:50:03,600 --> 00:50:08,880
Is that a fair distinction? Are those things the same? I feel like when I look at this slide,

419
00:50:10,080 --> 00:50:16,880
the distinction I hear is that the causal structure is provided to the LLM in the prompt.

420
00:50:16,880 --> 00:50:21,520
You know, we say if flipping switches, you know, this happens and if this and that happens.

421
00:50:22,240 --> 00:50:29,440
And so the causal structure is provided. And what we're testing is whether the LLM can sort of

422
00:50:29,440 --> 00:50:33,840
use logic to understand that relationship when the structure is known.

423
00:50:36,880 --> 00:50:43,520
That's interesting. I feel like in this particular example, this is the type of question and answering

424
00:50:43,520 --> 00:50:51,440
there is the way they describe it in Clatter, the genital paper, the cool little questions

425
00:50:51,440 --> 00:50:56,320
are really different. And so I guess, yes, your remark is interesting. I feel like it really

426
00:50:56,320 --> 00:51:01,040
depends in the different papers on what they want to put forward, whether it is

427
00:51:02,160 --> 00:51:07,360
interventional and counterfactual causality, or in that particular case, maybe it would be closer to

428
00:51:07,360 --> 00:51:11,360
logical reasoning. But

429
00:51:15,360 --> 00:51:23,200
it's really interesting. I guess it's still here, basically, what they say x causes y and y causes

430
00:51:23,200 --> 00:51:35,520
z does x causes z would be different depending on the situation. So I mean, this is the graph,

431
00:51:36,160 --> 00:51:41,840
this is the structure, and then depending on the variables you pick, it is true or it is not.

432
00:51:42,480 --> 00:51:50,320
But is it based on logic or it can also be based on observations? I don't know if you agree with that.

433
00:51:52,400 --> 00:51:59,680
Well, well, sure. And I guess, you know, I guess the former, there's, I guess, maybe it's still

434
00:51:59,680 --> 00:52:04,480
controversial, but some might argue that the former, you know, inferring causality simply

435
00:52:04,480 --> 00:52:10,160
on observations is ultimately something we can never do. Not that it isn't useful to sort of

436
00:52:10,160 --> 00:52:16,400
try to develop sort of causal models. It definitely is. But ultimately, it's something we can never do.

437
00:52:17,360 --> 00:52:22,800
But with logic, you know, we can come to absolute conclusions. Like if we are given a structure,

438
00:52:22,800 --> 00:52:30,800
we can reason about that, sort of, you know, come up with determined sort of relationships

439
00:52:30,800 --> 00:52:31,760
based on that structure.

440
00:52:37,840 --> 00:52:39,760
Hashin and Beshi, did you want to react to this?

441
00:52:42,720 --> 00:52:45,280
Could you let me share my screen a moment? Of course.

442
00:52:52,080 --> 00:52:57,520
So I put this in the chat. I just wanted to make you guys aware of this

443
00:52:58,480 --> 00:53:04,720
paper, which I think would be an interesting follow on to this conversation, because

444
00:53:05,520 --> 00:53:09,840
this paper by François Chalet, and you can go listen to him on YouTube, is very interesting.

445
00:53:10,880 --> 00:53:17,280
I just became familiar with and I recommend this paper for two reasons. One is because it

446
00:53:17,280 --> 00:53:24,160
seems like a very cogent analysis of what would be necessary in order to have machine intelligence.

447
00:53:24,880 --> 00:53:31,040
And it feels to me like causality, the ability to infer causality or to determine a chain of

448
00:53:31,040 --> 00:53:38,400
reasoning using causal principles would be an important component of that. The other reason

449
00:53:38,400 --> 00:53:46,080
is because, as I've highlighted there, he actually defines if he comes up with a metric for defining

450
00:53:46,080 --> 00:53:52,560
intelligence of a machine based on algorithmic information theory, which information theory

451
00:53:52,560 --> 00:53:58,560
being sort of a core part of what we're trying to talk about here. But one of the things he talks

452
00:53:58,560 --> 00:54:13,600
about there is the need to account for prior information. So this discussion about whether

453
00:54:13,600 --> 00:54:20,160
you're memorizing and regurgitating versus doing reasoning has a lot to do with how much prior

454
00:54:20,160 --> 00:54:25,440
information you have. If you already know the answer and you give me the correct answer,

455
00:54:25,440 --> 00:54:30,480
did you give it to me because you did reasoning or because you just knew the answer and you just

456
00:54:32,000 --> 00:54:38,400
stated the answer? So in the paper he talks about the need for being able to assess generalization

457
00:54:38,400 --> 00:54:43,440
ability. And we're talking about generalization ability being not just weak generalization,

458
00:54:44,000 --> 00:54:49,680
meaning in the context of things you've seen before, but strong generalization in what he

459
00:54:49,680 --> 00:54:56,160
calls developer aware generalization in the sense of being able to generalize beyond the

460
00:54:56,160 --> 00:55:02,480
situations that you've seen in your training data, beyond your prior knowledge, and therefore

461
00:55:03,120 --> 00:55:10,320
address novel situations. And it sounds to me like if we're going to assess the ability of

462
00:55:10,960 --> 00:55:20,400
a machine or a program or a set of programs to do causal reasoning, then

463
00:55:22,240 --> 00:55:28,160
much in the nature of counterfactuals and so on, you need this ability to be able to take those

464
00:55:29,120 --> 00:55:33,360
principles and then generalize into some other context that has never been seen before.

465
00:55:34,400 --> 00:55:38,880
And so I found this a very interesting paper because he sort of breaks it down into the

466
00:55:38,880 --> 00:55:46,000
necessary and sufficient components. In particular, if you're trying to compare two agents,

467
00:55:46,720 --> 00:55:52,800
you need to compare them with the same priors. In other words, if two agents have different priors,

468
00:55:52,800 --> 00:55:58,880
different levels of prior knowledge, then you can't, and the second agent has more prior knowledge

469
00:55:58,880 --> 00:56:03,760
than the first, and it gives a better answer, you can't necessarily conclude that that second

470
00:56:03,760 --> 00:56:08,720
agent is more intelligent because it's not starting from the same basis. It might also

471
00:56:08,720 --> 00:56:12,480
already have known that answer because it was in its prior knowledge base.

472
00:56:14,160 --> 00:56:21,040
So I think what you brought up about the LLMs, what training data have they seen?

473
00:56:22,560 --> 00:56:28,880
Timothy's very astute observation that the nature of the causal reasoning was already

474
00:56:28,880 --> 00:56:33,440
stated in that sentence. And you just gave an example and all it had to do was fill in the

475
00:56:33,440 --> 00:56:43,760
blanks with different priors and A's and B's and answer that question. Was it really doing

476
00:56:43,760 --> 00:56:50,000
causal reasoning? It was just using a rule which was given to it. So if I looked at that sentence

477
00:56:51,360 --> 00:56:57,840
that you gave me and I just memorized that sequence of sentences and I just applied it

478
00:56:57,840 --> 00:57:05,600
in a different context, am I doing causal reasoning? I think this really bears looking

479
00:57:05,600 --> 00:57:09,040
in deeper to some of these issues that I think Francois is talking about in this paper.

480
00:57:14,080 --> 00:57:22,640
Just a quick comment. I have to leave soon as well. So I think it might be interesting to go

481
00:57:22,640 --> 00:57:29,600
through the architecture of either the chat GPT 3.5 or GPT 4. I'm not sure whether it's solely

482
00:57:29,600 --> 00:57:34,320
just based on the transformer or something else, but the architecture definitely will guide the

483
00:57:34,960 --> 00:57:44,240
reasoning. So let me make a quick comment. I might have just said this before. I mean this

484
00:57:44,240 --> 00:57:51,680
discussion is very, very helpful. And Antoine, thank you for doing this pretty nice review.

485
00:57:51,760 --> 00:58:02,000
I mean what this has brought to light in my mind is the distinction between causality and logical

486
00:58:02,000 --> 00:58:11,360
reasoning which Timothy pointed out. And then within that the causality is basically causal

487
00:58:11,360 --> 00:58:16,720
discovery versus causal reasoning. Is that different from logical reasoning and so forth?

488
00:58:16,720 --> 00:58:23,280
Right? I mean so there are some distinctions to be made and this whole idea of intelligence,

489
00:58:23,280 --> 00:58:30,320
I mean is intelligence all reasoning or when we think about intelligence we think about

490
00:58:30,320 --> 00:58:37,120
intuition. We think about creativity. We think about coming up with new solutions when new

491
00:58:37,120 --> 00:58:44,400
constraints and things present which didn't exist. I mean the whole of the science and engineering is

492
00:58:44,480 --> 00:58:51,520
all of that, right? Pretty much all fields where you're trying to find new solutions which

493
00:58:51,520 --> 00:58:59,040
probably do not have a historical precedence. And these large language models rely on that

494
00:58:59,040 --> 00:59:07,280
historical precedence. I mean the priors as you call it. And so how do we make that distinction?

495
00:59:07,280 --> 00:59:16,320
And the second thing is that large language models are essentially inferring these things from

496
00:59:16,960 --> 00:59:24,880
the basis of language. They are not doing analysis of data. There may be auxiliary tools

497
00:59:25,520 --> 00:59:32,880
that say okay now I can go and probe the data but that probing is based on the logic that is built

498
00:59:33,840 --> 00:59:41,760
or large logic that these large language models come up with. And so I think there needs to be

499
00:59:41,760 --> 00:59:48,480
some very subtle characterization of what we mean. I mean extending this idea of causality

500
00:59:48,480 --> 00:59:57,680
in those three notions that you talked about from a language to a data context. We use the word

501
00:59:57,680 --> 01:00:04,720
data loosely. I mean what we are using the word data is essentially language data, not quantitative

502
01:00:04,720 --> 01:00:15,040
numerical data on which these analysis are built. So there is much to be done in parsing this out

503
01:00:15,040 --> 01:00:22,560
very, very carefully and going about doing that. Having said that, the encouraging thing which I

504
01:00:22,560 --> 01:00:29,040
find is the following. So when I was in grad school, I did a couple of courses on artificial

505
01:00:29,040 --> 01:00:35,360
intelligence and the prevailing language at that time was Lisp and Prolog. Lisp processing and

506
01:00:35,360 --> 01:00:46,320
basically logical programming. That's what Prolog was. So the idea was that if you could program

507
01:00:46,320 --> 01:00:52,160
logic in all its complexity and the many books written on the structure of human logic and

508
01:00:52,240 --> 01:00:58,000
to take that and program it, you would be successful in mimicking intelligence. And

509
01:01:00,080 --> 01:01:05,200
to me at that time said, okay, you may be able to do a pretty sophisticated job with

510
01:01:05,200 --> 01:01:10,480
deductive logic, but there was nothing in that which would allow you to do inductive logic,

511
01:01:11,200 --> 01:01:20,560
which basically goes on to looking at inclusion and creativity. The thing is that didn't go too far

512
01:01:20,560 --> 01:01:28,560
and then we have this large language models who say, okay, I don't need a language that is based

513
01:01:28,560 --> 01:01:38,400
on reasoning. All I need to do is have the capability to infuse things from data

514
01:01:41,760 --> 01:01:48,880
and computation. And so that's the generative model's success where they can pretty much

515
01:01:48,880 --> 01:01:54,960
infer. So the idea is that, okay, I don't need how to reason. Everything that I need to learn

516
01:01:54,960 --> 01:02:00,480
about reasoning is already built into the millions and billions of textual data that is there.

517
01:02:01,040 --> 01:02:08,240
So if I have the ability to infer that, I will, even though I don't know that it is essentially

518
01:02:08,240 --> 01:02:16,160
a logical reasoning and maybe some things beyond. My guess is that a lot of the other things are

519
01:02:16,160 --> 01:02:24,800
built into our language structure very deeply. And to the extent that we can then

520
01:02:26,560 --> 01:02:33,920
reintegrate that, re-manipulate that, use that as a foundation for thinking in new ways, we can

521
01:02:33,920 --> 01:02:38,880
build on it, but I don't think we are there yet. And this whole idea of causality, causal reasoning,

522
01:02:38,880 --> 01:02:45,840
causal inference and other things may fall in that space saying we don't yet know how to go

523
01:02:45,840 --> 01:02:51,920
about doing that, although that information is there. So the distinction between language

524
01:02:51,920 --> 01:02:59,600
and the data-driven approach is important and there is more to be done with this space than

525
01:02:59,600 --> 01:03:11,840
what is out there. Oshin? Yeah, thanks for raising that issue, Praveen. And interestingly enough,

526
01:03:11,920 --> 01:03:23,760
I just came across this paper about something called Dreamcoder. And if you read down here,

527
01:03:24,800 --> 01:03:29,920
it says we present Dreamcoder, a system that learns to solve problems by writing programs.

528
01:03:30,480 --> 01:03:35,200
It builds expertise by creating programming languages for expressing domain concepts.

529
01:03:36,160 --> 01:03:42,560
A wake sleep learning algorithm alternately extends the language with new symbolic abstractions

530
01:03:42,560 --> 01:03:47,280
and trains the neural network on imagined and replayed problems. And then concepts are built

531
01:03:47,280 --> 01:03:52,960
compositionally from those learned earlier, yielding multilayered symbolic representations

532
01:03:52,960 --> 01:03:59,200
that are interpretable and transferable to new tasks. So anyway, I just thought it was interesting

533
01:03:59,200 --> 01:04:10,560
because there is actually now apparently some small breakthrough into developing machine learning

534
01:04:11,760 --> 01:04:16,880
structures where learning concepts and extending language much in the way that we

535
01:04:18,240 --> 01:04:24,960
learn concepts and extend language in order to do reasoning and causal reasoning and all of that.

536
01:04:24,960 --> 01:04:28,160
So that's actually an interesting, we're just starting to happen.

537
01:04:32,000 --> 01:04:39,360
Yeah, I would say that, I mean, I think we are at the beginning of a breakthrough in these

538
01:04:40,080 --> 01:04:48,480
things. We are now assembling essential tools that may help us move this to expect these tools

539
01:04:49,360 --> 01:04:54,880
that are not trained or developed for a specific task to inherently be able to do that.

540
01:04:55,760 --> 01:05:02,800
I think it's a little far, but there needs to be more and that's an opportunity for us.

541
01:05:08,880 --> 01:05:13,840
From an information theory perspective, this brings me back to the fact that

542
01:05:14,160 --> 01:05:22,640
everything we do is based on embeddings. We take objects or concepts and we build embeddings

543
01:05:22,640 --> 01:05:29,520
out of them, which are then manipulated using reasoning machine learning or whether it's human

544
01:05:30,160 --> 01:05:37,600
or machines. And so we start with symbols. The symbols are represented by embeddings

545
01:05:38,560 --> 01:05:43,360
and that's an information theory problem. How do we choose the correct embedding,

546
01:05:44,000 --> 01:05:50,000
which represents the information, all of the necessary and relevant information,

547
01:05:51,360 --> 01:05:55,920
which can then be processed? And how do you then represent that information in a way that can

548
01:05:55,920 --> 01:06:00,560
actually be manipulated using the tools that are available to us in machine learning that's

549
01:06:00,560 --> 01:06:06,160
typically using vectors, vector spaces and being able to do dot products in order to

550
01:06:06,880 --> 01:06:13,680
do similarity operations, to add vectors in order to do addition and subtraction operations,

551
01:06:14,640 --> 01:06:19,760
sort of logical things that are involved in logical reasoning. But then on top of those

552
01:06:19,760 --> 01:06:24,720
concepts, we have to, on top of those embeddings, we have to build concepts, which are collections of

553
01:06:24,720 --> 01:06:31,840
these. And from those, we have to build languages. And when we build languages, which are minimum,

554
01:06:31,840 --> 01:06:37,840
which are shorter description length representations of concepts, we're then able to do reasoning

555
01:06:37,840 --> 01:06:45,200
using those higher level objects or concepts. And so I kind of have been seeing this kind of

556
01:06:45,200 --> 01:06:50,000
structure emerging in the machine learning, particularly in the context of evolutionary

557
01:06:50,000 --> 01:06:56,880
robotics and artificial intelligence. But I think it provides an interesting way for us to think

558
01:06:56,880 --> 01:07:04,720
about how we actually process information using the tools of algorithmic information theory and

559
01:07:04,720 --> 01:07:12,720
Shannon information and how that leads to us being able to build sort of these informational

560
01:07:12,720 --> 01:07:18,160
pyramids or, you know, things where we can, we can think about things at lower levels of the hierarchy

561
01:07:18,160 --> 01:07:22,800
and then at higher levels of the hierarchy and actually do these sort of intelligent processing.

562
01:07:22,800 --> 01:07:30,320
Yeah, no, I agree with that completely. And I think the generative models, the transformers are

563
01:07:30,320 --> 01:07:36,400
built on the series of embeddings. I mean, it's a recursive embedding process that generates these

564
01:07:36,400 --> 01:07:43,200
parameters and estimation of these parameters across these things. And one of the things which

565
01:07:43,200 --> 01:07:50,160
we are trying to explore with Hersh is, well, they're a way for us to modify that to see

566
01:07:50,800 --> 01:07:58,400
causal reasoning can be extracted using that embedding structure. So that's a big question.

567
01:07:59,440 --> 01:08:05,440
So the thing that bothers me a lot is sometimes we may be using embeddings that are not

568
01:08:06,400 --> 01:08:10,640
properly informative. If I just give you a stream of stream flow and I treat,

569
01:08:11,840 --> 01:08:16,400
let's say I give you rainfall potential evaporation and stream flow, and those are three values,

570
01:08:16,400 --> 01:08:20,880
and I just put them in a vector. And I'm telling you at this point in time, this is the vector,

571
01:08:20,880 --> 01:08:24,800
and next point in time, this is the vector, and this is, you know, and I've got these three values.

572
01:08:26,320 --> 01:08:30,800
If I'm not telling you whether the stream flow is going up or going down at that point in time,

573
01:08:31,520 --> 01:08:36,880
or whether the, but the energy is increasing or decreasing or the rainfall is increasing or

574
01:08:36,880 --> 01:08:42,400
decreasing, I might be giving an embedding which is not sufficiently informative for you to be able

575
01:08:42,400 --> 01:08:51,840
to do meaningful inference. And so thinking about how we develop our data embeddings as a first

576
01:08:51,840 --> 01:08:55,680
step before we even present them to our algorithm seems to be an important step.

577
01:08:56,800 --> 01:09:03,040
Yeah, or get the algorithms to basically build on the initial embedding to explore

578
01:09:03,040 --> 01:09:10,400
alternates and see what makes sense. Correct. I was going to ask if in on this question of

579
01:09:10,400 --> 01:09:17,200
embeddings, it would be difficult for a language model to speak on causality because usually we

580
01:09:17,200 --> 01:09:24,880
reason about causality in terms of graphs. And as I understand, there in the large language model,

581
01:09:24,880 --> 01:09:30,640
there, there's no structure of a graph. So maybe it's using the wrong embedding to speak about

582
01:09:30,640 --> 01:09:36,480
causality, maybe the language model understands causality in a different way, in a different

583
01:09:36,480 --> 01:09:43,360
embedding than we typically would analyze causality. I don't think it's necessary. Well,

584
01:09:43,360 --> 01:09:47,920
Praveen can probably answer this better, but I don't think it's necessarily true that a large

585
01:09:47,920 --> 01:09:53,120
language model and graphs are not the same thing, because a large language model can be

586
01:09:53,120 --> 01:09:59,040
thought of as a very high dimensional joint probability density function. And that's basically

587
01:10:00,000 --> 01:10:04,640
how we build those is by using building graphs, right, of conditional probabilities and so on.

588
01:10:06,400 --> 01:10:11,520
Antoine and Praveen isn't it true that that's what a lot of, what's his names,

589
01:10:13,200 --> 01:10:19,600
the father of causal inference, I forget his name. Perl? Perl. Perl, a lot of his work was

590
01:10:19,600 --> 01:10:26,880
based on that, that the fact that those two are essentially the same thing. Yeah. Yeah. I mean,

591
01:10:27,440 --> 01:10:34,480
the representation of causality as a graphical model came out of Perl and then quantified by

592
01:10:35,120 --> 01:10:42,080
Sprites. I put that link in the chat. There's a nice book by Sprites, which I recommend to

593
01:10:42,080 --> 01:10:47,840
everybody to read a minute, just helps lay that down on how to do this in a mathematical

594
01:10:47,840 --> 01:10:54,640
way and how to think about it. But I think the real question that we haven't yet answered

595
01:10:55,520 --> 01:11:02,720
effectively is in our context where we are dealing with data in space and time,

596
01:11:03,680 --> 01:11:10,640
what does causality mean? I mean, in a medical context, I mean, you can figure out whether

597
01:11:10,640 --> 01:11:19,280
smoking causes cancer or not through a whole bunch of different things. But in our context,

598
01:11:19,280 --> 01:11:27,200
where we have potentially continuous space-time domain, it's easier to answer the question of

599
01:11:27,200 --> 01:11:36,320
causality. The necessary condition for causality in time is breaking of symmetry in time. The

600
01:11:36,880 --> 01:11:42,480
past causes, the future, future cannot cause past. That's the necessary condition. Is that a

601
01:11:42,480 --> 01:11:50,240
sufficient condition or not? That has not been well answered. Now, if you extend that in space,

602
01:11:50,240 --> 01:11:57,680
there is no such framework. I mean, so then you have to ride on a vector space to basically

603
01:11:57,680 --> 01:12:03,520
figure out a directionality and then say, okay, something that is happening in one space,

604
01:12:04,160 --> 01:12:09,600
preclude something that is happening in another. You might ride on a river and say, okay,

605
01:12:09,600 --> 01:12:15,440
I'm going to forward. But then the whole issue of backwater propagation and all that thing happens

606
01:12:15,440 --> 01:12:21,040
and then that can break down. So what is that framework? What do we mean when we say causality

607
01:12:21,040 --> 01:12:27,200
within the context of what we're dealing with has not been well defined? And that's a struggle

608
01:12:28,480 --> 01:12:36,800
in there. And then we anchor on surrogate processes. And Allison has done some work with

609
01:12:36,800 --> 01:12:41,840
information theory in which direction the information blows. I think like that. I mean, so

610
01:12:42,720 --> 01:12:48,640
those are good starting points and there might be some hint of how we may go about doing it.

611
01:12:48,640 --> 01:12:53,280
But until we break through, we are going to be scratching this on the surface and hoping that

612
01:12:53,280 --> 01:13:04,240
somehow some model is going to provide that input. So would it be fair to say that causality is a

613
01:13:04,240 --> 01:13:11,360
representational assumption rather than a fact? In other words, it's a hypothesis we make about

614
01:13:11,360 --> 01:13:18,000
the world and we test. And just to take a simple example, if I just said rainfall and runoff,

615
01:13:18,960 --> 01:13:23,680
and I ask you to say, does rain cause runoff? That's going to have all the problems that you

616
01:13:23,680 --> 01:13:32,160
just talked about, right? But there are causal effects. Increasing CO2 is causing climate change.

617
01:13:33,120 --> 01:13:43,200
And so there are definitely open causal issues. But my point is, are we treating causality as a

618
01:13:43,200 --> 01:13:46,800
fact or are we treating causality as a representational explanation?

619
01:13:49,200 --> 01:13:55,760
Yeah, we don't know that, right? I mean, probably that to go hand in hand, a certain type of

620
01:13:55,760 --> 01:14:01,440
representation will help us infer a certain type of causality. But until we come up with

621
01:14:01,440 --> 01:14:11,120
proper definitions and proper classifications, I think what we end up doing is anchoring on a

622
01:14:11,120 --> 01:14:17,280
representation that is convenient and then infer causality associated with that representation.

623
01:14:17,280 --> 01:14:22,800
And then say, well, no, this represents everything we got. So structural causal model might fall into

624
01:14:22,800 --> 01:14:32,000
one of those categories. But yeah, I don't know the answer. I mean, I'm just articulating

625
01:14:32,880 --> 01:14:38,240
the questions that go through my mind. But I mean, if we take an extreme example,

626
01:14:38,240 --> 01:14:46,160
like F is equal to MA, right? It's a structural representation that was come up with. And you

627
01:14:46,160 --> 01:14:54,080
test it, and it all never fails. We start to treat it as a fact of nature, right? Because it's

628
01:14:54,080 --> 01:15:03,200
a hypothesis that has never actually been disproved by a counterfactual, by an example that

629
01:15:03,200 --> 01:15:09,280
contradicts it. So maybe something similar with causality, you have a chain of reasoning,

630
01:15:09,280 --> 01:15:13,840
and if that chain of reasoning always holds up, then eventually you start to treat it as though

631
01:15:13,840 --> 01:15:23,200
it's a fact of nature. Probably. I mean, even if it equals MA is wrong in certain cases,

632
01:15:23,200 --> 01:15:32,880
like photons, it kind of brings the question of, is all of causality emergent? Can you have

633
01:15:32,880 --> 01:15:40,240
fundamental laws that are causal? Or is everything kind of in a higher,

634
01:15:46,640 --> 01:15:50,720
kind of more broad context, complex systems?

635
01:15:52,320 --> 01:15:57,920
Well, if I can go back to ask Antoine a question. Well, go ahead and answer that first.

636
01:15:58,800 --> 01:16:05,280
I was just going to say, I think Ocean, I don't claim to have read Hume, but I think you summarize

637
01:16:05,280 --> 01:16:16,560
Hume's argument that, you know, fundamentally, we can't know causality absolutely. But, you know,

638
01:16:16,560 --> 01:16:25,040
we can, we can, you know, strengthen our beliefs. And that sort of this is all very useful. I don't

639
01:16:25,520 --> 01:16:32,720
think Hume was trying to argue that, you know, we shouldn't be logical beings and throw out this,

640
01:16:32,720 --> 01:16:38,000
these aspirations for understanding causality completely. And I think that's kind of what

641
01:16:38,000 --> 01:16:44,160
you're saying that, you know, yes, F equals MA is wrong, but it's, you know, right under,

642
01:16:45,760 --> 01:16:50,960
you know, most of the conditions that we encounter in our day to day. And so it can

643
01:16:50,960 --> 01:16:56,560
basically be taken as fact. And that's kind of what our definition of causality is.

644
01:16:57,440 --> 01:17:02,880
It's right until it's wrong. But Antoine, going back to the large language models,

645
01:17:06,560 --> 01:17:10,880
if these people who wrote all these papers were to take a bunch of

646
01:17:12,400 --> 01:17:15,040
age roles, my daughter's eight, I'm just picking eight out of a hat,

647
01:17:16,000 --> 01:17:22,240
and attempted to do these same tests on them, right? That's kind of what I was thinking about

648
01:17:22,240 --> 01:17:27,040
when you said that they were testing these large language models to infer causality.

649
01:17:28,080 --> 01:17:34,240
If they ran these same tests on a bunch of eight-year-olds, you know, in other words,

650
01:17:34,960 --> 01:17:42,880
how do they know that their tests are actually meaningful tests

651
01:17:44,720 --> 01:17:51,360
for establishing whether or not the LLM or the eight-year-old has the ability to

652
01:17:52,720 --> 01:18:03,120
cause inference? I don't think they do. I feel like this is what we're getting out of this

653
01:18:03,120 --> 01:18:07,840
discussion. It's like, what is causality in the end? And how can you be sure that it is

654
01:18:07,840 --> 01:18:16,560
causality you're inferring and not logic reasoning as Timothy proposed? So I don't think they really

655
01:18:16,560 --> 01:18:27,920
do. All they can do is come up with a benchmark, a controlled one that has causated causes and effects

656
01:18:28,880 --> 01:18:39,440
and tests if an LLM is able to recreate that. But again, is this purely reasoning or this just

657
01:18:42,080 --> 01:18:48,880
or this just retrieval from your knowledge? And I think there's also another point to consider

658
01:18:50,000 --> 01:18:54,320
as Pravin mentioned earlier, that it's a bit different because it's language and

659
01:18:54,960 --> 01:18:59,920
might take my thought on that. I don't have anything to support that claim, but my thought on that

660
01:19:01,120 --> 01:19:09,840
is that we as humans use language to formulate concepts and to reason. So eventually, if we

661
01:19:09,840 --> 01:19:19,520
reach the point to which the LLM is so powerful in text, in natural language processing, actually,

662
01:19:20,320 --> 01:19:29,520
what are the implications of it on its ability to formulate concepts and reason? I want to get

663
01:19:29,520 --> 01:19:34,160
back to the chain of thought and tree of thought prompt engineering techniques I was telling you

664
01:19:34,160 --> 01:19:40,160
about earlier. The tree of thought is pretty much the same thing. So you say to your LLM,

665
01:19:40,160 --> 01:19:47,520
right, this is a question, I want the answer. But first, I want your first thought on the answer

666
01:19:47,520 --> 01:19:53,520
and then you separate into two and you like choose different ways of thinking about it and

667
01:19:53,520 --> 01:20:00,080
just create a tree and output all difference. That would be able, that would make us able to track

668
01:20:00,080 --> 01:20:06,240
how does the net. I saw that there is a very interesting paper on it. I can link it in the

669
01:20:06,240 --> 01:20:14,960
chat tree of thought. So my take on this, my question would be LLM are basically two years old

670
01:20:14,960 --> 01:20:21,680
and they're able to do so much already. And at what pace are they still going to grow in the

671
01:20:21,680 --> 01:20:30,880
future? And what are the implications on the amount of knowledge that will be theirs in a couple

672
01:20:30,880 --> 01:20:38,480
of years from now? Because in the end, is causality just like you have this in your knowledge, you

673
01:20:38,480 --> 01:20:45,200
can take it out and get it again. Because if it's that, I don't have any doubt that in maybe 10

674
01:20:45,200 --> 01:20:50,000
years from now or I don't know why. At some point, we'll figure it out to every knowledge we know in

675
01:20:50,000 --> 01:20:59,040
the LLMs. It's not reasonable to think that. But core factuals and everything, some more,

676
01:21:01,760 --> 01:21:06,400
I don't really have an answer. I don't think anyone has an answer in the papers I mentioned.

677
01:21:07,200 --> 01:21:19,920
Yes, Ernan? Yeah, this is great discussion. I have more questions here for us to reflect.

678
01:21:20,480 --> 01:21:29,760
But it seems like we are collecting all the reflections. It seems like we still don't have

679
01:21:29,760 --> 01:21:38,640
a way to measure causality, a solid way like we do measure models via the WSER or some other

680
01:21:38,640 --> 01:21:50,080
metrics. And my perception of causality is something that may be reproducible across

681
01:21:50,080 --> 01:21:56,640
experiments in different environments that are looking at kind of the same processes.

682
01:21:56,640 --> 01:22:06,240
So think about streamflow in Switzerland versus Tucson versus Washington. But do we

683
01:22:08,000 --> 01:22:14,640
have tools to measure causality? In other words, can we say the same way in an analogous way,

684
01:22:14,640 --> 01:22:20,400
we have a way, machine learning models, overfeats, underfeats, do we have a way to say this model

685
01:22:20,400 --> 01:22:29,040
overfeats causality, this model underfeats causality? This is a good causality explanation of the

686
01:22:29,040 --> 01:22:37,520
process. And I don't know if that exists. And that goes back to the way we usually validate or

687
01:22:37,520 --> 01:22:46,800
cross validate models in which we split the data set in a number of folds and then we cross

688
01:22:46,800 --> 01:22:55,520
validate it. Should we instead do tests for causality in a similar way or analogous way in

689
01:22:55,520 --> 01:23:01,360
which we take data sets from different environments and then we see if the knowledge is transferred

690
01:23:01,360 --> 01:23:08,240
across those environments via the cross validation. So perhaps that removes a little bit of the

691
01:23:08,240 --> 01:23:14,400
anxiety we have for perfection in causality. And we kind of explain it in a way that we

692
01:23:14,560 --> 01:23:26,320
quantify and not as a binary yes or no. And another problem is predicting beyond training

693
01:23:28,080 --> 01:23:34,480
in the at once example of climate change. It's another limitation. I don't know if we're at the

694
01:23:34,480 --> 01:23:40,000
point at which the models will be able to reason and then beyond training, even though they're

695
01:23:40,000 --> 01:23:49,040
perfectly trained, reason about climate change consequences and the trends. If the trends are

696
01:23:49,040 --> 01:23:56,080
learned from the data themselves, then yes. But if there's nothing that let us know about surprises,

697
01:23:56,960 --> 01:24:05,040
I doubt. And the other thing is the data limits are really constraining our learning

698
01:24:05,120 --> 01:24:18,720
or the models learning pace of the facts and not to speak of the counterfact loss. But

699
01:24:19,280 --> 01:24:24,960
I mean, those are kind of some of the lines or bullets I could raise from everybody's discussion

700
01:24:24,960 --> 01:24:27,840
but so far. That's a great discussion.

701
01:24:32,080 --> 01:24:40,320
If I, yeah, thank you for intervention Aaron. If I may maybe give another couple of elements in

702
01:24:40,320 --> 01:24:49,120
there to give a couple more insights on those questions. Okay, so I have a small amount of

703
01:24:49,120 --> 01:24:56,160
knowledge on digital twins. Before I was here, when I was in master's degree, five, six years ago

704
01:24:56,160 --> 01:25:01,360
in France, I was an apprentice at Dassault Systems at the same time and they were working on digital

705
01:25:01,360 --> 01:25:08,400
twins back then. So I was a bit in there. In short, for those who will know digital twins is a concept

706
01:25:08,400 --> 01:25:14,160
that ties a physical object to a virtual representation, where there is a synchronization

707
01:25:14,240 --> 01:25:23,600
of data between the two of them. And what's hot in the topic right now is because of climate

708
01:25:23,600 --> 01:25:28,720
change and extreme events and everything is a digital twin of the planet Earth. There is

709
01:25:31,680 --> 01:25:38,640
a perspective on that from the European Union, which is called destination Earth. So they plan

710
01:25:38,640 --> 01:25:46,800
to do a digital twin of the whole Earth by 2027, I don't remember 2028. My take on this is

711
01:25:48,240 --> 01:25:54,880
from what I've seen in the paper, talking about the intelligent agents that learn about experience.

712
01:25:57,760 --> 01:26:04,240
I drew the parallel between those agents and what on a lens able to do. But the key thing here is

713
01:26:04,240 --> 01:26:14,320
that experience is beneficial to causal understanding. And I guess this is what's also been put forward

714
01:26:14,320 --> 01:26:22,400
by my the causal inference frameworks that do need a lot of data, experimentations and everything.

715
01:26:22,400 --> 01:26:29,920
And so just my small insight on there would be that digital twins may just represent a great

716
01:26:29,920 --> 01:26:41,280
environment for LLMs to interact with if those virtual environments are good enough

717
01:26:41,840 --> 01:26:48,800
to be representative of what's happening out there. And this is where all work of all

718
01:26:48,800 --> 01:26:54,400
traditional, I would say research comes in, flow modeling, ground flow modeling, rainfall runoff,

719
01:26:55,280 --> 01:27:07,360
geochemistry. And so my vision of, so we're at UIUC in Illinois in the CI Net program,

720
01:27:07,360 --> 01:27:13,680
we're studying the Singapore watershed. And so maybe a target I'd like to achieve would be like

721
01:27:13,680 --> 01:27:21,120
to have a digital twin of that watershed represented by a 3D model and data coming in the same time.

722
01:27:21,200 --> 01:27:27,200
And on top of that, you would have physics, a physics engine reality represented by all the

723
01:27:28,160 --> 01:27:35,360
process based simulation models we know and et cetera. And this would be the great place for

724
01:27:35,360 --> 01:27:44,000
LLMs to fully express itself and make interactions and make experiences. And this would potentially

725
01:27:44,000 --> 01:27:52,880
provide a great environment to perform causal inference and maybe to test whether LLMs are

726
01:27:52,880 --> 01:27:58,240
actually able to do causal inference, because all we've been fitting is generated text and made up

727
01:27:58,240 --> 01:28:02,800
benchmarks, which is interesting. And it's the first step. Eventually experience is the key,

728
01:28:02,800 --> 01:28:07,440
I feel like, and the digital twin would be really interesting environment for that. This is just

729
01:28:07,440 --> 01:28:13,520
like both thoughts I've had. You want to reflect on that, Ocean? No, no, no, absolutely. I think

730
01:28:13,600 --> 01:28:17,040
you're absolutely going down the right track because you're saying basically

731
01:28:17,680 --> 01:28:25,120
that you need to not only respond, you need to be able to interrogate your environment

732
01:28:26,480 --> 01:28:31,440
and giving them a digital environment to interrogate is a useful thing to do. And that's

733
01:28:31,440 --> 01:28:40,480
actually behind this poet paper that I put up there about where the environment changes as

734
01:28:40,480 --> 01:28:48,000
the ability of the agent changes. So it's like a co-evolution process where it actually evolves

735
01:28:48,000 --> 01:28:51,680
the environment, which is one step beyond what you're thinking, what you're talking about.

736
01:28:51,680 --> 01:28:57,680
But it also occurred to me, the reason I put my hand up was that I think it's really interesting

737
01:28:57,680 --> 01:29:04,320
to have the LLM as the agent that's doing the learning. I hadn't thought of that. I think it

738
01:29:04,320 --> 01:29:09,120
would also be interesting in a decision context, which I think is where you're going, to have

739
01:29:09,120 --> 01:29:16,480
multiple LLMs take the roles of different stakeholders and play devil's advocate. So

740
01:29:16,480 --> 01:29:21,040
you've got the rancher and you could say, okay, your job is to play the role of the rancher,

741
01:29:21,040 --> 01:29:26,240
your job is to play the role of the environmentalist, your job is to play the role of whatever.

742
01:29:27,040 --> 01:29:31,920
And you could do some very, very interesting role playing explorations

743
01:29:31,920 --> 01:29:42,240
with the goal of coming up with potential solutions to difficult transdisciplinary

744
01:29:42,240 --> 01:29:49,200
decision-making problems, where you can play them out using agents rather than having to get

745
01:29:49,200 --> 01:29:54,800
real humans in there before you then take those to the next stage and involve humans.

746
01:29:55,840 --> 01:29:58,560
I'm glad you brought it up because this is exactly what we do.

747
01:29:59,040 --> 01:30:08,560
Perfect. And so I'm going to be presenting that with Praveen at AGU. And just to give a couple

748
01:30:08,560 --> 01:30:16,640
of heads up. So I have a couple of examples where exactly I have a mayor science expert and

749
01:30:16,640 --> 01:30:22,800
everything talking in response to a flood event, impeding flood event or something. And the

750
01:30:22,800 --> 01:30:28,160
problematic right now is to find the right metric and find ways to evaluate the output

751
01:30:28,160 --> 01:30:35,520
of the models because this is taxed. It's complicated to evaluate compared to LSTM,

752
01:30:35,520 --> 01:30:40,480
which would output a sequence of numbers. And then you would use all the mathematics,

753
01:30:40,480 --> 01:30:45,280
you know, to evaluate it. But how do you do it with tax? It's either you come up with something new,

754
01:30:45,840 --> 01:30:52,480
either you use an LLM to do it, but there is a bias in using an LLM to evaluate an LLM.

755
01:30:52,480 --> 01:30:55,920
So this is a question that needs to be answered right now. But yes.

756
01:30:57,120 --> 01:31:04,080
That's like a tool for gowns. Yeah, exactly. So yeah, if that interests you and you're coming

757
01:31:04,080 --> 01:31:07,280
to AGU, my talk is morning morning. I'm going to be talking about that.

758
01:31:08,640 --> 01:31:11,200
Would you mind dropping me an email telling me where and when?

759
01:31:12,640 --> 01:31:19,840
I wanted to comment on the talks on AGU because well, with the names that are usually in the

760
01:31:19,840 --> 01:31:26,240
talks and also for the participants who were at the SNF in the house workshop, I collected some

761
01:31:26,240 --> 01:31:33,440
of the talks that will be presented by participants at AGU. So we have a nice Excel spreadsheet

762
01:31:33,440 --> 01:31:41,040
that I could share. Please, save us a slide. Yeah, you can even filter it by daytime. So you

763
01:31:41,040 --> 01:31:46,560
kind of have like a... So would you rather have me send your information to you so you can like

764
01:31:46,560 --> 01:31:53,440
enrich the database? No, I think I already have you in the database. If you allow me, I can very

765
01:31:53,440 --> 01:32:01,120
quickly share my screen and show you. It's just a spreadsheet and I just scraped the data from the

766
01:32:01,120 --> 01:32:06,560
AGU website. So go ahead. If I'm missing someone, I could even...

767
01:32:06,560 --> 01:32:15,600
Yeah, so just to give us an example, my talk...

768
01:32:20,160 --> 01:32:24,080
Are you physically there? Yeah, I will be physically there.

769
01:32:24,080 --> 01:32:31,760
Okay, perfect. So my talk is also on Monday, but you can see what I scraped is just the ID,

770
01:32:32,640 --> 01:32:38,160
the title of the talk. These are the participants who are usually in this meetings or who are at

771
01:32:38,160 --> 01:32:44,480
the workshop. So you can see that it's Uwe Hoshin, myself. I'm the speaker and you can see who is

772
01:32:44,480 --> 01:32:52,960
the speaker in this column of authors with the double mark. Thank you. That's a great help.

773
01:32:53,520 --> 01:32:57,280
From what I see, we're going to be running around chasing for our...

774
01:32:57,680 --> 01:33:03,440
If you follow this schedule very strictly, you will be around the Moscone Center a couple of

775
01:33:03,440 --> 01:33:08,480
times, but I think you can pick and choose what looks interesting. Thanks for that. It's great.

776
01:33:10,000 --> 01:33:16,640
So I will share the label and then we can see how it can be passed around and how I can add data to

777
01:33:16,640 --> 01:33:24,000
it. All right, perfect. Thanks. Unfortunately, I have to go for a student exam. So one final

778
01:33:24,080 --> 01:33:30,480
comment, thinking about student exams, this thing about causal reasoning also reminds me of the

779
01:33:30,480 --> 01:33:34,320
kind of questions we try to ask students when they're doing their qualifier or they're doing

780
01:33:34,320 --> 01:33:38,720
their... Right, there's the kind of questions which are just about what do you know, facts,

781
01:33:38,720 --> 01:33:43,360
regurgitate, and then hopefully you get to the kind of questions where they get to

782
01:33:44,080 --> 01:33:49,200
generalize beyond and that's where you test their true understanding and or intelligence.

783
01:33:50,000 --> 01:33:53,680
So it felt... It just felt like it has direct relevance to what you were talking about.

784
01:33:56,240 --> 01:34:03,120
Anyway, I got to go. Bye, guys. Bye, everyone. Bye, bye. Just a final comment that we are going to

785
01:34:03,120 --> 01:34:11,120
be having our winter break and we are going to be back on January 17 with Manuel's presentation.

786
01:34:12,480 --> 01:34:16,080
Beautiful. Thanks, everyone. Bye, guys. Thanks.

