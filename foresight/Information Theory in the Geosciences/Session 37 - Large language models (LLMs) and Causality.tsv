start	end	text
0	9680	All right, I'm happy to introduce Antoine, a very energetic young man who joined a research
9680	16320	program when it was in January, just this past year, this year.
16320	24160	And before that, Antoine was working with this startup company called Extra Lab.
24480	34080	He was on their software engineer side, helping develop systems for capturing
35600	41440	river water quality data with the technology that was developed by the founder of Extra Lab.
42560	51760	But I'm glad he came to us to do his PhD and has been working on issues related to causality
51760	58000	and large language models, and he can tame GPD like nobody else.
58560	62640	So with that, Antoine, all yours.
63520	64960	Okay, so thank you, Praveen.
64960	68880	So indeed, today I'm going to talk to you about LLM's and causality.
70160	75360	As you can have guessed, if any of you just took a small quick look at the papers,
75360	77120	cyber-voted Lila last time.
78560	81040	So just to put it back in a bit of a context,
81040	84800	this is a picture I use in all my presentations that I used to illustrate
84800	86560	what's going on with climate change right now.
86560	91600	And this is just more than 20 inches of precipitation in one night and for a lot of day.
92400	100160	And it's just to exemplify how climate change changes the way that extreme events are happening
100160	104400	and how can we do to stop them and how can we tackle those problems.
105360	110640	And this is, so as Praveen introduced, I'm not going to spend a lot of time on this, but
111200	120320	the statement here is that the decision-making process is critical in the resilience,
120320	124560	in increasing or decreasing resilience to climate change risks,
125200	128400	and so to address the extreme events.
130240	134960	The problem is decisions are taken by humans, and until recently we didn't have GPD,
134960	140160	and understanding natural language with mathematics is complicated.
140720	145200	And there is no breakthrough framework until GPD came out,
145920	151520	which we've seen as an opportunity to try to understand how humans think in language.
152800	159760	So what I'm working on right now is using GPD agents to generate multiple decision pathways
160480	166560	based on a context of an extreme event, let's say a forecast of a flooding for a city.
167200	171680	And we use multiple GPD agents to talk to each other in order to come up with decisions
171680	176560	pathway, which then we can evaluate to understand whether it is good or bad decisions.
177120	183760	And this is just the start of a work in which we will try to maximize resilience
183760	190080	and see what is the takes of LLMs and AI in that particular problem.
190080	194800	So I'm going to be presenting that at AGU, have a talk on the morning, Monday morning.
194800	199600	So if you want to see what it's about, I would gladly see you in the audience.
200400	206800	But so we'll get back on the subject here. So I talked a bit about LLMs.
206800	210240	Even I'm pretty sure that everyone here is familiar with what it is.
210880	218400	Still, I'm going to say, so large language models, they are the class of language models,
218400	223200	such as GPD, etc., usually based on the transformer architecture.
223200	230160	And so those models receive a text input and will make predictions, textual predictions.
230800	235840	They're known because they're really good at understanding human language in the sense that
235840	244400	they are a great conversational agents. So GPTs, Barrett, Paul, and Lama, there's a lot of them.
245120	252720	And you've probably heard about a couple. And so as for applications of LLMs, translation tasks,
253520	256560	you can use them for text generation, report story scripts.
257120	264560	You can ask an LLM to generate 10 different poems on a subject you like in that particular style,
264560	271840	for instance, can use LLMs for Q&As, synthesizing, etc. So this is what an LLM is like.
273840	279520	Now, introducing causality is rather complicated because it's a complicated term.
279680	285200	But I'm pretty sure, again, the most part of you are really familiar with what causality is.
285200	289200	So I'm just going to say it's a relation, studying causality, studying relationships
289200	296320	between cause and effect. Due to Pearl dedicated the major part of his life, working on that,
296320	300960	I'm pretty sure you're all familiar with his work. So I'm not going to stain his image,
300960	304240	trying to give another definition that wouldn't serve no purpose.
305200	311680	Um, causality carries more information than correlation, which is why it is so interesting.
312720	319520	Studying causality is more important than studying just statistical correlations,
319520	323440	for that matter. This is why there is a lot of work on studying causality,
323440	327920	in order to understand how natural processes interact with each other.
327920	335680	And the question now is, how does those two concepts connect?
337440	340880	Why do we even have this question of can LLMs infer causality?
341600	348480	The question comes from the fact that LLMs learn their input data, their training data is
348480	356240	huge corpus of text that ranges from Wikipedia articles to blog posts to books, etc.
356320	362480	So literally what they are is just a condensed experience of what the world beat them.
363360	370320	And in those texts, there is a lot of information about describing processes and everything.
370960	378160	So today, if you ask a GPT agent, I see it start to rain. What is the impact on the ground?
378720	381760	And the LLM will answer the ground will be wet.
381840	390000	So it may appear that the LLM is able to infer causality in that particular setting,
390000	394400	because it is able to tell you what is going to be the outcome of a situation you describe.
395200	400000	But a lot of people have been digging into this and trying to figure out if this is really causality
400000	404240	or no, and if it is, which type of causality it is, and we'll get back to that later.
404960	412560	And let's say they were really possible of inferring causality, what would be the implications
413200	420640	on future research on a world, etc. And I just want to finish this by putting another small
420640	428000	motivation point, which is that if we get back to my previous topic and the thing I'm working on,
428560	434880	from Zhang et al. from this paper says, decision-making scenarios require a quantitative
434880	441600	understanding of the effects of actions leading to the desired outcome. In another way,
442720	447600	if we want to be able to tackle precisely decision-making problems,
449040	453440	we need to have an understanding of cause and effects. Otherwise, it's complicated to have
453440	461680	all the causal chains of actions that would be triggered by one particular decision.
461680	466000	So this is a bit of the motivation why it would be interesting for us to understand whether
466000	474160	LLMs are able to do causality or not, and which one. So just a quick view of the papers I used to do
474160	482240	this literature review. Those two first paper are by Jin et al. The first one, they present
482240	491120	Clutter, which is a causal benchmark. They use in order to evaluate LLMs to causal task,
491120	495040	as well as fine-tuning LLM models to see if they could approve their results.
496240	503280	The second one, they introduce a task for LLMs to be evaluated on, which is called
504080	510880	core to causation. So basically, they try to convert correlation to causation and see
510880	519440	how well it translates. The two next papers, Kissiman and Lyudel. So Kissiman et al is
526080	531920	yes, it's just another study of multiple types of causality evaluation on LLMs.
533600	539280	Lyudel 2019, this one is a bit interesting because it's older. So this one was out before
539280	547120	the LLMs actually were advertised and were that popular. And this one uses a different approach
547120	552480	using intelligent agents, but I will get back to that. And the last one,
553760	559840	Vetservish is probably the most interesting in this one because it's the most,
560800	569840	not unbiased, but they make the strongest statements. They say that LLMs are causal
569840	575760	parrots and they can maybe appear to talk causality, but deep inside, they're not at all and never
575760	581120	will be. So they make quite strong statements and it's interesting. And the last one,
581120	589360	the last one is early 2023 and gives a couple of insights on future work and future directions,
589360	596720	but don't give a lot of answers in there. So I'll just start by quickly reminding for this,
596720	601440	for the purpose of this study that the difference between causal discovery and causal inference
601440	608800	tasks, because those concepts are used in the papers. So causal discovery is constructing
608800	615680	a causal structure. Basically it is figuring out a dag, a directly, sorry, directed a cyclic graph
616480	621680	in which all the nodes are variable and there is a link between the nodes when there is a
621680	630240	functional relationship. The causal discovery holds a structure for, in order to be able to infer
630240	636480	causality on multiple levels. Multiple levels of causality, as defined by Perl, are observational,
636560	643040	interventional and counterfactual. Where observational, you only need to observe
644240	652560	the cause and effect for you to be able to say that this is causal. Interventional requires the
652560	657200	intervention, so changing a variable and seeing if there is an effect on another one.
657840	665360	And counterfactual is knowing that there is a causality link. It is the highest level of
665360	670720	causality. Knowing there is a causality level and two variables, what would happen if you would
670720	678240	change the variable in such a way? So they just remind the difference between discovery, which
678240	688080	is more about finding the structure that ties all variables together and explaining the possible
688080	693680	relationship that can be between. And causal inference would be more about determining what
693680	698960	level, what strength of causality there is between two variables and which direction to.
700880	708640	And so the first results that are given by Kissimer et al. So they work on inferring
708640	714240	causal discovery. So what they do is they're trying to come up with that direct to stick with graph
714320	726160	using an LLM. And so the findings are that the best LLM in the list, GPT-4, and is almost always
726160	732720	GPT-4, art performs causal discovery frameworks by approximately 40% on the Tobingen benchmark
733360	739680	on pairwise causal discovery tasks. So for two variable, two by two variables,
740240	744640	is being able to find out if they could be related in some way or not.
745520	753920	And so this is the list of the model they tested. And almost every time is GPT-4 performing at the
753920	763120	best. So this is quite an interesting result, but they also balance that result with the fact that
763920	771120	LLMs present a strong lack of robustness because when they fail, it is really unpredictable
771760	778160	to predict. It is really unpredictable the fact that are going to fail or not. So you might have
778160	784640	96% of accuracy, but it's hard to predict when they will fail. This is what they put the accent on
786000	787280	when they present their results.
787680	799200	As for the benchmark they use, it is very diverse. So over 100 causal relationships from a variety
799200	806000	of domains, physics, biology, zoology, cognitive science, epidemiology, soil science. So there
806000	811680	is a couple of examples of relationships that are in their framework for use to be LLMs.
812400	818800	So for instance, alcohol and main corpuscular volume. So a question like this will be asked
818800	825520	to the LLM and whether depending on the answer is yes or no, it will compare the answer with the
825520	837680	real in the framework to establish the accuracy person. Sorry. So the second result is by Zetz
837680	845520	Avicila. They do a lot of different types of causal inference and causal tasks in their study.
845520	850160	It's very complete. They also do a causal discovery task, which is pretty much the same.
850160	855680	They feed a scientific question to the LLM and depending on the answer, they say yes or no
856400	867040	to the establishing of causal discovery. So this is the results they get.
868240	874800	Where GPT-3, GPT-4 are open AI models. LUMINUS is a model that is built by LUMINUS.
874800	883280	It's the corporation itself and OPT is the META Facebook LLM. So yes, Russian?
885760	890720	Just to clarify, they're just checking the question and the answer. They're not
890720	897360	actually checking the chain of reasoning. No, not in this one. There is some frameworks
897360	904160	and some benchmarks in which they use a chain of thoughts prompt engineering in order to be able
904160	910800	to check the chain, but not in this one. And they're just using off-the-shelf LLMs. They're
910800	918320	not specifically trained for these. No. Well, yes and no. So they're off-the-shelves LLM. They're
918320	927280	not fine-tuned. However, they make some assumptions that in some cases, the enormously high result
927280	935680	for GPT-4 could indicate that some parts of the benchmark are actually included in GPT-4's training,
935680	942080	but it's hypothetical. They don't have that knowledge. It's just a hypothesis they make.
942080	946320	But technically, no, it's just off-the-shelves LLM and these ones are not fine-tuned.
947920	952560	And the assumption is that their answers that they have are well-established
953280	963840	beyond dispute that this is the correct answer. Yes, I guess. Okay. Thanks.
964720	974080	So yes, this is the result they give for LLMs. To be honest, it's a bit hard to
974080	979120	interpret the results sometimes in this paper. There are some sections that are extremely clear
980000	986480	as to the results and findings, et cetera. Some are less. This is actually one of the main
986480	992800	negative point of this paper that's been highlighted on the open reviews. This paper in
992800	998240	particular is going to be published in Transaction for National Learning. It's been reviewed on
998240	1002560	open reviews. So all the reviews are publicly available on open reviews. It's really interesting.
1002560	1007200	I read a couple. And what's being said about this paper is that
1008320	1012560	the math behind and the logic behind it is really strong, but sometimes the results
1012560	1018240	are not well explained. And sometimes I just cannot tie the results to what they claim.
1018240	1028000	But that's why we'll move on to the next one. This is the findings on different types of
1028000	1036640	causal inference. So by Genital, the first paper. And so they test about 10 LLMs.
1038160	1044560	They test their accuracy on observational, intranational, and counterfactual tasks.
1044560	1052320	And on top of that, so they are tested again, the clatter dataset, which is composed of
1052960	1060720	if I, yeah, 10,000 samples of approximately even distributed for every causal task
1060720	1066960	containing different types of data. And they also come up with what they call causal cot,
1067600	1073840	which is a fine-tuning model they did using their dataset. So they were trying to figure out at the
1073840	1083840	same time how well LLMs performed on a causal benchmark, as well as if they find
1083840	1090720	to the model what performance improvement do you have. And so they conclude that
1092560	1098320	overall, the models just perform slightly better than random, which is not that great.
1099040	1105520	They also conclude that there is two percent, there is approximately two percent difference in
1105520	1114080	accuracy on TB4 and their fine-tuned version, which they say is outstandingly good. I personally,
1114080	1120880	I think it's interesting to use this approach. However, the results are not that shining for now.
1121840	1129040	And so this is one other result there is. In the second paper they present,
1131120	1137600	this is another causal task, which they call chord cause. So basically, the point of this
1137600	1144880	is to escalate one correlation relationships to causation. So you know there is a correlation.
1144880	1150560	So let's say that causal discovery is almost done. You are sure there is a correlation,
1150560	1155760	a grounded one between two variables. And the point would be to determine whether it is cold or not.
1157360	1167680	And so this benchmark is run on 17 LLMs and also 12 fine-tuned LLMs, which is on the right side.
1170640	1177600	So yeah, they give multiple metrics in there. Again, they say the results are not really incredible.
1177680	1186560	They don't really conclude on the results. In none of the papers, based on the results,
1186560	1192160	they are able to say yes, LLMs are able to do causality, no, LLMs are not able to do causality.
1192160	1198480	Every time it's always very measured and every paper ends the same way. It's at this point,
1198480	1206720	we're not able to refute the fact that LLMs can infer causality. So they have strong
1208480	1213440	insights that they might or might not be able to, but they cannot come up with a conclusion in the
1213440	1222160	end. Here, what is interesting to show is that in this particular case for escalating correlation
1222160	1227840	to causation, there is a real impact in fine-tuning models. As we can see, the
1228800	1236160	precision increases a lot between off-the-shelf models and fine-tuned models.
1237680	1244400	So the takes of those two papers by Jin et al, which are separated for a two-month or
1244400	1250800	three-month, if I remember correctly, and it's relatively linked. So the main overall take
1250800	1257440	on those two papers by Jin is that using causal benchmarks is really interesting to LLM training
1257440	1264160	as it can really improve the training performance and it can actually induce some of causal causality
1264160	1270720	at least on that particular dataset in the LLM. And also that fine-tuning is really helpful in this
1271280	1281440	scenario. Okay, so I'm good. Yeah, of course. In the previous slide, you mentioned that in the
1281520	1294880	previous one. In the previous one, sorry. I know it's there. They test for counterfactuals.
1296080	1305360	Oh, sorry. This one? Yes. Okay. They test for counterfactuals and interventionals, right? How
1305360	1313920	do they test for counterfactuals? I don't really know because they don't give any example. If I
1313920	1320640	remember correctly in the papers, they don't give any example of any example. Well, they do give
1320640	1330080	example structures. They do give example structures of how their dataset is made, but they don't give
1330160	1336320	like actual example that are fed to the models as questions and answer. So I'm not really able,
1336320	1343520	let me just see real quick. No, because this is just the constitution of their dataset.
1345600	1351440	They explain a bit how they do constitute their dataset. So they choose variables,
1351440	1358480	generate causal graphs, map them, etc. So the data is composed like this, but they don't actually give
1358560	1365840	examples to which their dataset. Maybe with the additional, in the additional
1368000	1373840	supplementary information, probably they have something. In the supplementary information of
1373840	1380160	the paper or something, they may have something. Well, this is already from the supplementary.
1380160	1384480	Those two slides are from the supplementary information and I don't remember. I can take
1384480	1390480	a look afterwards, but I don't remember seeing any example of what they actually feed. I have some
1390480	1399040	for other benchmarks, but not for this one. I see. Thank you, Antoine. And so yeah,
1400080	1407040	so I guess it's time to talk about this a little bit. This is a paper by Llewital 2019 and it's
1407040	1413200	so it studies intelligent agent systems. I just want to say that it is, in my opinion, it is
1413200	1419520	interesting in this context just because they focus on what is the importance of experience
1419520	1425440	on learning potential causality. They use a different approach, which is still interesting
1425440	1431840	because so here what they mean by intelligent agent systems or agent. They use the actual
1431840	1437440	similar definition of agent that is in the context of complex adaptive systems. So
1437440	1443440	where an agent would be defined by an entity that have sensors that is able to perceive the
1443440	1450240	outside world actuators that can interact with the world and also an internal model that is just
1451520	1457920	a logic for the agent to decide what it's going to do depending on the inputs it receives in the
1457920	1465280	sensor and what it's going to do to the world. So this is how an agent is defined in the complex
1465360	1472080	adaptive systems. And the difference with the intelligent systems, intelligent agent from
1472080	1478560	the URL is that they also give the ability to understand natural language to their agents.
1481280	1488880	So what do they do with those agents? They trained the agents on multiple life classical scenarios
1489760	1504000	and then they use humans to create more training instances out of more scenarios
1504000	1509920	and then they put those agents in those particular scenarios to see what they're able to do with it.
1509920	1516320	So the agents have a bit of prior knowledge. Some more scenarios are created and then
1516400	1522320	they use the agents. All the scenarios in this study are generated with a game in this fact.
1523600	1530320	I'm just going to use this fact to add on the, I feel like some video games might present like
1530320	1537760	the perfect ground for testing and simulating this kind of behaviors. There is a lot and a lot
1537760	1543440	of different examples of people doing reinforcement learning on video games, learning AI strategies
1543520	1550320	to drive a car for racing lines, this stuff. So I just wanted to make a small side note on
1550320	1554960	the fact that video games represent a good training example. So in this case, this game is
1554960	1560320	Minecraft. It's a game where you interact with the world. And so what they do, they put the agents
1560320	1567600	in Minecraft and they generate a bunch of scenarios, which is like one, I attack the co and I attack
1567600	1573120	the co. And this is the two outcomes. And for those two outcomes, the agent is going to infer
1573120	1579200	causal or not causal. So based on their prior knowledge and more scenarios created by players
1579200	1586160	and then are collected, then the agents are evaluated on whether they experience in learning
1586160	1593760	from those scenarios, made them able to infer causality in new, newly presented scenarios.
1596880	1602880	They gave a bit of the architecture they use to structure their model and their inference.
1603680	1612480	So the shared experience is represented by, I guess, the pool of knowledge that all the agents
1612480	1618320	learn all together. Those are events triggered. When you attack a cow, you get some beef.
1621440	1627200	They don't give a lot of details, whereas all this works, if I remember correctly,
1627200	1633040	but in the end, they just come up with a causal question, which the agents do inference on.
1634000	1640320	And they're able then to classify what are considered as causal or not. And then they
1640320	1649360	just compare with the example they had in first. So the principle of finding they give from this
1649360	1655200	study is that experience mechanism is key for language concepts, understanding and learning,
1655200	1662400	which is a very long turn of phrase for causality. So it is interesting. This paper is interesting
1662480	1674480	in this way, because LLMs can be seen as agents in that they are trained and they learn out of
1674480	1679760	huge text corpuses that represent, I don't know, novels, articles, blog posts, Wikipedia.
1681120	1685120	Those can be seen as scenarios where the LLM will learn some knowledge.
1686240	1690640	At the end, you can interact with an LLM as you can interact with those agents.
1691280	1698000	Their inputs are the sensors and the text feedback they will give is the actuator.
1698000	1704240	You can use an NNM as a robot if you ask, do some actions or something. So what's interesting is that
1704240	1712480	if we put in parallel LLMs and the agents as described in this paper, well, basically what
1712480	1718480	they say is that experience is key for causality. So they would be, from my understanding of that
1718480	1726480	paper, what I get of that paper would be that more data, more training could eventually lead
1726480	1734160	to causality, which is opposed to the thoughts that are given in different papers in this selection.
1735520	1738960	I still think this one is interesting. It's a different approach.
1741280	1747360	I think it's similar to what LLMs do today. Maybe some will argue that it's not, but I found
1747360	1753200	this interesting. And I guess now it's time to dig deep in the biggest paper in the corpus,
1753200	1760080	I think, which is ZetaVis. So it's the one when I said they make strong assumptions,
1761280	1767920	strong claims that LLMs cannot do causality and never could. And the two main potential
1767920	1775120	reasons they give is that the errors that are contained in the corpus used to train LLMs
1775120	1782240	really hamper the outputs and hamper the knowledge base. So it would be like,
1782240	1786800	it would be like putting poison in the brain. Eventually, it's not going to be able to function
1786800	1793680	correctly. So what they say is that errors in the input data is going to be propagating to more
1793680	1798960	errors in the output. So this is the first reason. And the second reason they give is the lack of
1798960	1806160	physical data in training data set. They say that the whole difference between correlation
1806160	1812160	causation is the physical evidence and the physical grounding of those facts. And they say that because
1812160	1818720	LLMs are not trained with physical evidence, physical data, et cetera, well, they inherently
1818720	1827360	haven't, they're unable to ground the facts they claim. To quote, they say prohibits any sort of
1827440	1835840	induction of the actual data generating mechanism. So this is the two main reasons they give.
1836800	1844720	And on top of that, they provide with mathematical explanation of why that stands.
1845520	1851360	So the main contribution in that paper is that they define a subgroup of structural
1851440	1861760	causal models named media SEM. So the structural causal model, it is, I've did my research on
1861760	1867200	this, it is a bit unclear to me as if it's really defined by bongers of it, or if it was.
1867200	1874400	Because I feel like a lot of, a lot of parts in this are shared with the Perlian theory of causality
1874400	1883360	and more work on it. But if I quote the SEM was first defined by bongers at all in 2021,
1883360	1890880	and this is the, this is the definition they give. So SEM is a tuple that contains all this.
1893120	1898800	In short, if I try to simplify the definition of this, an SEM
1899680	1908640	contains a series of structural equations in the Perlian sense.
1912080	1917040	Well, that's, that's pretty much it actually. There are some details on the variable. I don't
1917120	1930640	understand all the, all the subtleties to this definition. Yeah. Maybe we can get back to that
1930640	1943040	later. Okay, they also remind a couple of definitions and insights. So they remind the
1943120	1948880	Perl's causal hierarchy, which consists on three languages that can respectively
1949920	1954560	do observational causality, inter-reventional causality, and counterfactual causality.
1955440	1962800	And then they give their insight, their first thought on it. M be some SEM. So M,
1962800	1971120	so SEM is a set of structural equations in that context. Knowledge about the structural equations
1971120	1978240	and the causal graph of M is knowledge about answering L3 and L2 queries in M respectively.
1978880	1989360	So their insight is that if M is an SEM, knowing about the structural equations in that SEM
1989360	1995520	and the causal graph is enough to perform inter-reventional and counterfactual causality.
1995760	2002640	And this is what they use to introduce their concept of a meta SEM, which is another SEM
2002640	2009920	that is able to do inter-reventional and counterfactual just based on those information. So this is
2009920	2015280	literally the definition they give for the media SEM. And then they will spell the rest of the paper
2015280	2021440	trying to show that LLMs can be assimilated to media SEMs, which they cannot achieve actually.
2021520	2034080	But I feel like just outlining those particular properties and giving the insights and everything
2034080	2039520	is still a great contribution to the question in the sense that it's a first exploration of a real
2042000	2049200	formal process in order to be able to determine whether LLMs are able to do causality or not.
2050160	2052160	So they, yeah, Ocean.
2055760	2056800	Oh, I can hear you.
2059200	2064320	Just trying to parse what you just said, but it sounds like what they're suggesting using
2064320	2073120	different language is that if you can, if you have enough information in the construct, the SEM,
2074080	2079680	which is your representation, if you have enough information to answer
2080400	2086240	interventional and counterfactual questions correctly, then they're saying you can infer
2086240	2097280	causality. Would that be a good interpretation? In other words, if the representation does not
2097280	2103040	allow you to answer those two kinds of questions, they're basically arguing that you can't infer
2103040	2110240	causality. Yeah, I think this is a good summary of the definition.
2113440	2117440	So it says something about the particular form of the representation that you need to have.
2121040	2128400	In other words, is it answering the question correctly does not imply that you actually have,
2129280	2138240	well, that's complicated. It seems to me that you need a particular structure which enables you
2138240	2141600	to answer them, but answering them doesn't necessarily mean you have that structure.
2142320	2145840	Anyway, that's kind of what I'm struggling with here. Okay.
2149600	2153680	Okay, so I'll just continue. So this is the conjecture that you make,
2153840	2164960	M1 be an SEM and 2 a respective media SEM. So it means that M2 is able to answer queries on
2164960	2170480	M1 based on its observational data. So basically M2 would be the LLM in this one.
2172000	2179040	So then define Q and A in the language and in the interventional language of M1,
2179040	2182880	observational language of M2, causal queries with their respective answers,
2183520	2188640	blah, blah, blah, then we have FQ equals A is equivalent to FQ minimizes training error.
2189200	2195120	So basically what they say in this conjecture is that F of Q,
2197760	2205200	which is the LLM's predictive models. So the predictions based on the interventional
2205440	2214400	of M1 equals the observational of M2 minimizes training error. So basically what they say is
2214400	2222480	that you don't learn anything more. I'm sorry. I don't know if that's really clear. I'm going to
2222480	2231680	try this again. What they try to say here is that an LLM learning on the interventional
2232560	2243600	and not learning anything more based on that model being able to already have the knowledge
2243600	2250400	on the observational distribution of M2 is equivalent. So basically in the information
2250400	2256480	of the observational distribution of M2, you already have all the informations
2257440	2264640	to do interventional querying on the other SCM means that the LLM minimizes training error.
2264640	2270480	So in that case means that the model converges and it is able to do it. This is the conjecture,
2270480	2275440	in other words, this is the conjecture they come up with to say that if an LLM can be
2275440	2285200	assimilated to a meta SCM, so it is able to escalate the causal task rank based on observational data,
2285200	2290000	then it is causal. This is the conjecture they come up with and they cannot prove it.
2291920	2297680	This is again one of the strongest remarks and feedbacks that has been given in open reviews
2297680	2305760	for day paper. The reviewer said you make such strong claims on causality and LLMs,
2305760	2309680	but eventually you cannot conclude on the conjecture. So the work is really interesting,
2309680	2317040	but eventually you do not conclude on it. They still give results and everything.
2318960	2325840	In this one, for instance, this is basically intuitive physics, basic logic questions,
2326960	2332400	such as if flipping switches causes light bulbs to shine and shining light bulbs causes
2332400	2338400	mothas to appear. Does flipping switches cause mothas to appear, which is a typical
2338400	2347280	causal question, and those are the results of the following LLMs on all those types of questions.
2348080	2355360	Everywhere there is an exclamation mark like that. They say that, as I was saying before,
2355360	2361520	that eventually that data, this type of questions can have been included in GBD4's training.
2362480	2370640	They give a kind of twisted explanation. They say that this framework was already published in
2370640	2375920	another paper and they say that they've been extensively running this framework and they
2375920	2380800	also say that OpenAI's API collects data on queries and answers and everything,
2381440	2386800	and so they just make the assumption that maybe the data they used while running benchmark was
2386800	2392400	used in training of GBD4 when it was GBD3 back then. This is why they put an exclamation mark
2392400	2397680	next to it. They say we're not sure we can trust these answers for those reasons.
2400400	2408720	There also are small variations of the models where every COT thing means chain of thought.
2410160	2415840	I don't know if you're familiar with chain of thought. Basically, it is what it's called,
2415840	2421760	a prompt engineering pattern. With LLMs, the prompt is the input we feed to the LLM and prompt
2421760	2432560	engineering is how to access more LLM features and enforce a behavior based on how you write the
2432560	2438800	prompt. Chain of thought is a prompt engineering technique where you will specify clearly in the
2438800	2448240	prompt that you want the LLM to output multiple midway thinking thoughts and thinking steps
2448880	2453280	before actually outputting an answer. We will look like that. For instance, you could say if
2453280	2458480	flipping switches blah, blah, blah, you ask a question and then you say please answer by giving
2458480	2465920	three main thoughts first, one, two, three, then give a preliminary answer and then answer. That
2465920	2472400	would be considered a COT. It's been proven as making the LLMs able to answer more accurately
2472960	2480880	or at least to be able to track down the chain process. There is also another type of it which
2480880	2486240	I am aware of, but I feel like it's incredibly hard to implement, but it still would be really
2486240	2490240	interesting. It's called tree of thoughts, which is pretty much the same principle as chain of
2490880	2496480	thoughts, but you take branches so that you are able to track down which path
2496480	2503920	led to which results. We can get back to that later. This is the results they give about
2503920	2514080	classical causality. In summary, the takeaways they offer, they present. Inability to ground
2514080	2519040	tachal facts is part of the reason why LLMs are not able to infer generalized causal relations.
2520640	2525280	However, they acknowledge that LLMs represent a head start to learning and inference.
2527600	2533200	They are unable to prove conjecture one despite the strong claims that LLMs are only causal parrots.
2536720	2540240	There is a whole paragraph on results on actual
2540960	2547440	causal escalation tests, but there is no sort of table that summarizes results.
2550080	2558720	I feel like this is a work in progress and will be interesting in the near future if they can come
2558720	2567360	up with more results on the subject. This would be approximately a summary of what I've read in
2567360	2576640	the six papers. I'm just going to give a couple of future identified work in those papers,
2576640	2582800	so as to align possible research directions from these researchers in that area that may
2582800	2591760	give us discussion elements. None of them could actually conclude that LLMs can do causality
2591760	2598080	or not, but what they do acknowledge is that LLMs represent suitable candidates to support
2598080	2604960	actual causal inference framework just because they have a really interesting knowledge base
2604960	2612480	as part of their huge corpus of text learning on. A lot of them cannot conclude on the fact
2612480	2617680	that LLMs can do causality, but they would be inclined to working with LLMs in order to
2617680	2625760	combine with actual causality inference frameworks. Those in those four papers,
2627200	2632160	they share the perspective that using LLMs as tools to enhance training of existing causal
2632160	2639120	models is worth exploring, pretty similar to the first one. Another interesting element would be
2639120	2643760	that causal benchmarks, such as the latter presented in the first paper, represent interesting
2643760	2649040	access of improvement for LLM fine-tuning or towards the development of causal LLMs.
2650880	2662240	Ginadal, I think she's working on this already because she's publishing a lot. She made it
2662240	2667600	clear that this is just the first step in her work. I guess this is also interesting to follow,
2668080	2675680	see if coming up with bigger causal frameworks will make able. But in the end, what is still
2675680	2684400	interesting to discuss here is that causal relationships embedded in frameworks,
2685920	2692800	whether it is to test LLMs or to fine-tune them, it is still going to be in their knowledge base
2692800	2698560	in some way. I guess the question that wanted to be addressed here at first is about
2700640	2704400	interventional and current factual, which is not based on observational.
2706480	2713600	This is what I had as a presentation. I thought it was going to be shorter than that.
2714480	2721600	I just think this is a basis to start a discussion on this topic because we have some elements now.
2723040	2736720	Thanks, Antoine. Now we are open for questions.
2736720	2753440	Hello. I'm sorry. I'm late today. I didn't go to the details of the papers. I'm just wondering
2753440	2761920	whether the fact that the GPT-4 is better than the GPT-3.5 in reasoning, can that be simply due to
2762640	2767120	that the GPT-4 has more data to be trained and more parameters to be estimated?
2770160	2779680	In a sense, it's due to an interpolation and regression issue. The so-called better reasoning
2779680	2785440	is a representation of a better regression be obtained through the training.
2785760	2789360	It is an interesting thought.
2791040	2799600	Because the concluding saying that the LAM cannot do the causality. If we are going back,
2799600	2807680	so the reason why GPT-4 is better is to be trained with more data and more parameters to be tuned.
2808640	2818720	Yes, it is actually true. This is pretty much what they give. I guess this is what they want
2818720	2824800	to explain when they say that LLMs are causal parrots. If they see causality in their training
2824800	2831520	base in their data set and training data set, they will be able to eventually get that relationship
2831520	2837040	out of their training data set as a result. Eventually, yes, GPT-4 performs better because
2837040	2844160	they've seen much. The question here would be more to say that are LLMs able to infer causality?
2844160	2851280	Does it mean that they need to see it all to be able to do real causality? Or is the question
2851280	2858080	here more about, no, can they actually do real causality, creating something? This is interesting
2858080	2862960	in the context of climate change, for instance, because all the natural processes are non-stationary.
2862960	2870160	They keep increasing in intensity, and they either are more intense or more sparse than
2870160	2875920	before, etc. There is nothing we can predict that. We don't understand that. This is what's
2875920	2881520	interesting in that particular context to me because that would be a great way to evaluate
2881520	2888720	to benchmark how LLMs interact with those data. Because this, we cannot have that in our training
2888720	2896560	data set. The problem is, it's in foreseen. Every time it's new. But it's a great remark.
2897360	2902480	It's a great remark. Thank you. Because my experience is that the deep learning is a very
2902480	2913680	powerful regression tool. My personal experience of using the chat GPT is doing pretty well on
2914640	2921840	the data set that is trained most from, but the pretty poor job, kind of the questions that
2921840	2927680	is lastly trained from. For example, I sometimes use the chat GPT to provide some suggestions to
2927680	2934720	where to travel from. I can get very good advice in these famous places. But if I was asking
2935840	2941840	where to travel, like do the hiking in the places nearby my current town,
2941840	2951760	Dave, it's just a random answer and not accurate. So I still think it's a regression problem for the
2951760	2963280	LLM. Thanks. Tim, I think you have a question or you want to participate. Yeah. You know,
2963920	2971440	causality is so fascinating and also problematic. I'm a little bit rusty, but I'll put this forward.
2972240	2976320	And particularly looking at this slide makes me wonder, you know,
2978480	2984480	well, I guess the classic response is, can we ever infer causality, whether for a machine or human?
2986080	2993920	And my answer, I guess, is ultimately not. But looking at this slide makes me think that perhaps
2994320	3002240	a question that we could answer is whether an LLM could perform logical reasoning.
3003600	3008880	Is that a fair distinction? Are those things the same? I feel like when I look at this slide,
3010080	3016880	the distinction I hear is that the causal structure is provided to the LLM in the prompt.
3016880	3021520	You know, we say if flipping switches, you know, this happens and if this and that happens.
3022240	3029440	And so the causal structure is provided. And what we're testing is whether the LLM can sort of
3029440	3033840	use logic to understand that relationship when the structure is known.
3036880	3043520	That's interesting. I feel like in this particular example, this is the type of question and answering
3043520	3051440	there is the way they describe it in Clatter, the genital paper, the cool little questions
3051440	3056320	are really different. And so I guess, yes, your remark is interesting. I feel like it really
3056320	3061040	depends in the different papers on what they want to put forward, whether it is
3062160	3067360	interventional and counterfactual causality, or in that particular case, maybe it would be closer to
3067360	3071360	logical reasoning. But
3075360	3083200	it's really interesting. I guess it's still here, basically, what they say x causes y and y causes
3083200	3095520	z does x causes z would be different depending on the situation. So I mean, this is the graph,
3096160	3101840	this is the structure, and then depending on the variables you pick, it is true or it is not.
3102480	3110320	But is it based on logic or it can also be based on observations? I don't know if you agree with that.
3112400	3119680	Well, well, sure. And I guess, you know, I guess the former, there's, I guess, maybe it's still
3119680	3124480	controversial, but some might argue that the former, you know, inferring causality simply
3124480	3130160	on observations is ultimately something we can never do. Not that it isn't useful to sort of
3130160	3136400	try to develop sort of causal models. It definitely is. But ultimately, it's something we can never do.
3137360	3142800	But with logic, you know, we can come to absolute conclusions. Like if we are given a structure,
3142800	3150800	we can reason about that, sort of, you know, come up with determined sort of relationships
3150800	3151760	based on that structure.
3157840	3159760	Hashin and Beshi, did you want to react to this?
3162720	3165280	Could you let me share my screen a moment? Of course.
3172080	3177520	So I put this in the chat. I just wanted to make you guys aware of this
3178480	3184720	paper, which I think would be an interesting follow on to this conversation, because
3185520	3189840	this paper by François Chalet, and you can go listen to him on YouTube, is very interesting.
3190880	3197280	I just became familiar with and I recommend this paper for two reasons. One is because it
3197280	3204160	seems like a very cogent analysis of what would be necessary in order to have machine intelligence.
3204880	3211040	And it feels to me like causality, the ability to infer causality or to determine a chain of
3211040	3218400	reasoning using causal principles would be an important component of that. The other reason
3218400	3226080	is because, as I've highlighted there, he actually defines if he comes up with a metric for defining
3226080	3232560	intelligence of a machine based on algorithmic information theory, which information theory
3232560	3238560	being sort of a core part of what we're trying to talk about here. But one of the things he talks
3238560	3253600	about there is the need to account for prior information. So this discussion about whether
3253600	3260160	you're memorizing and regurgitating versus doing reasoning has a lot to do with how much prior
3260160	3265440	information you have. If you already know the answer and you give me the correct answer,
3265440	3270480	did you give it to me because you did reasoning or because you just knew the answer and you just
3272000	3278400	stated the answer? So in the paper he talks about the need for being able to assess generalization
3278400	3283440	ability. And we're talking about generalization ability being not just weak generalization,
3284000	3289680	meaning in the context of things you've seen before, but strong generalization in what he
3289680	3296160	calls developer aware generalization in the sense of being able to generalize beyond the
3296160	3302480	situations that you've seen in your training data, beyond your prior knowledge, and therefore
3303120	3310320	address novel situations. And it sounds to me like if we're going to assess the ability of
3310960	3320400	a machine or a program or a set of programs to do causal reasoning, then
3322240	3328160	much in the nature of counterfactuals and so on, you need this ability to be able to take those
3329120	3333360	principles and then generalize into some other context that has never been seen before.
3334400	3338880	And so I found this a very interesting paper because he sort of breaks it down into the
3338880	3346000	necessary and sufficient components. In particular, if you're trying to compare two agents,
3346720	3352800	you need to compare them with the same priors. In other words, if two agents have different priors,
3352800	3358880	different levels of prior knowledge, then you can't, and the second agent has more prior knowledge
3358880	3363760	than the first, and it gives a better answer, you can't necessarily conclude that that second
3363760	3368720	agent is more intelligent because it's not starting from the same basis. It might also
3368720	3372480	already have known that answer because it was in its prior knowledge base.
3374160	3381040	So I think what you brought up about the LLMs, what training data have they seen?
3382560	3388880	Timothy's very astute observation that the nature of the causal reasoning was already
3388880	3393440	stated in that sentence. And you just gave an example and all it had to do was fill in the
3393440	3403760	blanks with different priors and A's and B's and answer that question. Was it really doing
3403760	3410000	causal reasoning? It was just using a rule which was given to it. So if I looked at that sentence
3411360	3417840	that you gave me and I just memorized that sequence of sentences and I just applied it
3417840	3425600	in a different context, am I doing causal reasoning? I think this really bears looking
3425600	3429040	in deeper to some of these issues that I think Francois is talking about in this paper.
3434080	3442640	Just a quick comment. I have to leave soon as well. So I think it might be interesting to go
3442640	3449600	through the architecture of either the chat GPT 3.5 or GPT 4. I'm not sure whether it's solely
3449600	3454320	just based on the transformer or something else, but the architecture definitely will guide the
3454960	3464240	reasoning. So let me make a quick comment. I might have just said this before. I mean this
3464240	3471680	discussion is very, very helpful. And Antoine, thank you for doing this pretty nice review.
3471760	3482000	I mean what this has brought to light in my mind is the distinction between causality and logical
3482000	3491360	reasoning which Timothy pointed out. And then within that the causality is basically causal
3491360	3496720	discovery versus causal reasoning. Is that different from logical reasoning and so forth?
3496720	3503280	Right? I mean so there are some distinctions to be made and this whole idea of intelligence,
3503280	3510320	I mean is intelligence all reasoning or when we think about intelligence we think about
3510320	3517120	intuition. We think about creativity. We think about coming up with new solutions when new
3517120	3524400	constraints and things present which didn't exist. I mean the whole of the science and engineering is
3524480	3531520	all of that, right? Pretty much all fields where you're trying to find new solutions which
3531520	3539040	probably do not have a historical precedence. And these large language models rely on that
3539040	3547280	historical precedence. I mean the priors as you call it. And so how do we make that distinction?
3547280	3556320	And the second thing is that large language models are essentially inferring these things from
3556960	3564880	the basis of language. They are not doing analysis of data. There may be auxiliary tools
3565520	3572880	that say okay now I can go and probe the data but that probing is based on the logic that is built
3573840	3581760	or large logic that these large language models come up with. And so I think there needs to be
3581760	3588480	some very subtle characterization of what we mean. I mean extending this idea of causality
3588480	3597680	in those three notions that you talked about from a language to a data context. We use the word
3597680	3604720	data loosely. I mean what we are using the word data is essentially language data, not quantitative
3604720	3615040	numerical data on which these analysis are built. So there is much to be done in parsing this out
3615040	3622560	very, very carefully and going about doing that. Having said that, the encouraging thing which I
3622560	3629040	find is the following. So when I was in grad school, I did a couple of courses on artificial
3629040	3635360	intelligence and the prevailing language at that time was Lisp and Prolog. Lisp processing and
3635360	3646320	basically logical programming. That's what Prolog was. So the idea was that if you could program
3646320	3652160	logic in all its complexity and the many books written on the structure of human logic and
3652240	3658000	to take that and program it, you would be successful in mimicking intelligence. And
3660080	3665200	to me at that time said, okay, you may be able to do a pretty sophisticated job with
3665200	3670480	deductive logic, but there was nothing in that which would allow you to do inductive logic,
3671200	3680560	which basically goes on to looking at inclusion and creativity. The thing is that didn't go too far
3680560	3688560	and then we have this large language models who say, okay, I don't need a language that is based
3688560	3698400	on reasoning. All I need to do is have the capability to infuse things from data
3701760	3708880	and computation. And so that's the generative model's success where they can pretty much
3708880	3714960	infer. So the idea is that, okay, I don't need how to reason. Everything that I need to learn
3714960	3720480	about reasoning is already built into the millions and billions of textual data that is there.
3721040	3728240	So if I have the ability to infer that, I will, even though I don't know that it is essentially
3728240	3736160	a logical reasoning and maybe some things beyond. My guess is that a lot of the other things are
3736160	3744800	built into our language structure very deeply. And to the extent that we can then
3746560	3753920	reintegrate that, re-manipulate that, use that as a foundation for thinking in new ways, we can
3753920	3758880	build on it, but I don't think we are there yet. And this whole idea of causality, causal reasoning,
3758880	3765840	causal inference and other things may fall in that space saying we don't yet know how to go
3765840	3771920	about doing that, although that information is there. So the distinction between language
3771920	3779600	and the data-driven approach is important and there is more to be done with this space than
3779600	3791840	what is out there. Oshin? Yeah, thanks for raising that issue, Praveen. And interestingly enough,
3791920	3803760	I just came across this paper about something called Dreamcoder. And if you read down here,
3804800	3809920	it says we present Dreamcoder, a system that learns to solve problems by writing programs.
3810480	3815200	It builds expertise by creating programming languages for expressing domain concepts.
3816160	3822560	A wake sleep learning algorithm alternately extends the language with new symbolic abstractions
3822560	3827280	and trains the neural network on imagined and replayed problems. And then concepts are built
3827280	3832960	compositionally from those learned earlier, yielding multilayered symbolic representations
3832960	3839200	that are interpretable and transferable to new tasks. So anyway, I just thought it was interesting
3839200	3850560	because there is actually now apparently some small breakthrough into developing machine learning
3851760	3856880	structures where learning concepts and extending language much in the way that we
3858240	3864960	learn concepts and extend language in order to do reasoning and causal reasoning and all of that.
3864960	3868160	So that's actually an interesting, we're just starting to happen.
3872000	3879360	Yeah, I would say that, I mean, I think we are at the beginning of a breakthrough in these
3880080	3888480	things. We are now assembling essential tools that may help us move this to expect these tools
3889360	3894880	that are not trained or developed for a specific task to inherently be able to do that.
3895760	3902800	I think it's a little far, but there needs to be more and that's an opportunity for us.
3908880	3913840	From an information theory perspective, this brings me back to the fact that
3914160	3922640	everything we do is based on embeddings. We take objects or concepts and we build embeddings
3922640	3929520	out of them, which are then manipulated using reasoning machine learning or whether it's human
3930160	3937600	or machines. And so we start with symbols. The symbols are represented by embeddings
3938560	3943360	and that's an information theory problem. How do we choose the correct embedding,
3944000	3950000	which represents the information, all of the necessary and relevant information,
3951360	3955920	which can then be processed? And how do you then represent that information in a way that can
3955920	3960560	actually be manipulated using the tools that are available to us in machine learning that's
3960560	3966160	typically using vectors, vector spaces and being able to do dot products in order to
3966880	3973680	do similarity operations, to add vectors in order to do addition and subtraction operations,
3974640	3979760	sort of logical things that are involved in logical reasoning. But then on top of those
3979760	3984720	concepts, we have to, on top of those embeddings, we have to build concepts, which are collections of
3984720	3991840	these. And from those, we have to build languages. And when we build languages, which are minimum,
3991840	3997840	which are shorter description length representations of concepts, we're then able to do reasoning
3997840	4005200	using those higher level objects or concepts. And so I kind of have been seeing this kind of
4005200	4010000	structure emerging in the machine learning, particularly in the context of evolutionary
4010000	4016880	robotics and artificial intelligence. But I think it provides an interesting way for us to think
4016880	4024720	about how we actually process information using the tools of algorithmic information theory and
4024720	4032720	Shannon information and how that leads to us being able to build sort of these informational
4032720	4038160	pyramids or, you know, things where we can, we can think about things at lower levels of the hierarchy
4038160	4042800	and then at higher levels of the hierarchy and actually do these sort of intelligent processing.
4042800	4050320	Yeah, no, I agree with that completely. And I think the generative models, the transformers are
4050320	4056400	built on the series of embeddings. I mean, it's a recursive embedding process that generates these
4056400	4063200	parameters and estimation of these parameters across these things. And one of the things which
4063200	4070160	we are trying to explore with Hersh is, well, they're a way for us to modify that to see
4070800	4078400	causal reasoning can be extracted using that embedding structure. So that's a big question.
4079440	4085440	So the thing that bothers me a lot is sometimes we may be using embeddings that are not
4086400	4090640	properly informative. If I just give you a stream of stream flow and I treat,
4091840	4096400	let's say I give you rainfall potential evaporation and stream flow, and those are three values,
4096400	4100880	and I just put them in a vector. And I'm telling you at this point in time, this is the vector,
4100880	4104800	and next point in time, this is the vector, and this is, you know, and I've got these three values.
4106320	4110800	If I'm not telling you whether the stream flow is going up or going down at that point in time,
4111520	4116880	or whether the, but the energy is increasing or decreasing or the rainfall is increasing or
4116880	4122400	decreasing, I might be giving an embedding which is not sufficiently informative for you to be able
4122400	4131840	to do meaningful inference. And so thinking about how we develop our data embeddings as a first
4131840	4135680	step before we even present them to our algorithm seems to be an important step.
4136800	4143040	Yeah, or get the algorithms to basically build on the initial embedding to explore
4143040	4150400	alternates and see what makes sense. Correct. I was going to ask if in on this question of
4150400	4157200	embeddings, it would be difficult for a language model to speak on causality because usually we
4157200	4164880	reason about causality in terms of graphs. And as I understand, there in the large language model,
4164880	4170640	there, there's no structure of a graph. So maybe it's using the wrong embedding to speak about
4170640	4176480	causality, maybe the language model understands causality in a different way, in a different
4176480	4183360	embedding than we typically would analyze causality. I don't think it's necessary. Well,
4183360	4187920	Praveen can probably answer this better, but I don't think it's necessarily true that a large
4187920	4193120	language model and graphs are not the same thing, because a large language model can be
4193120	4199040	thought of as a very high dimensional joint probability density function. And that's basically
4200000	4204640	how we build those is by using building graphs, right, of conditional probabilities and so on.
4206400	4211520	Antoine and Praveen isn't it true that that's what a lot of, what's his names,
4213200	4219600	the father of causal inference, I forget his name. Perl? Perl. Perl, a lot of his work was
4219600	4226880	based on that, that the fact that those two are essentially the same thing. Yeah. Yeah. I mean,
4227440	4234480	the representation of causality as a graphical model came out of Perl and then quantified by
4235120	4242080	Sprites. I put that link in the chat. There's a nice book by Sprites, which I recommend to
4242080	4247840	everybody to read a minute, just helps lay that down on how to do this in a mathematical
4247840	4254640	way and how to think about it. But I think the real question that we haven't yet answered
4255520	4262720	effectively is in our context where we are dealing with data in space and time,
4263680	4270640	what does causality mean? I mean, in a medical context, I mean, you can figure out whether
4270640	4279280	smoking causes cancer or not through a whole bunch of different things. But in our context,
4279280	4287200	where we have potentially continuous space-time domain, it's easier to answer the question of
4287200	4296320	causality. The necessary condition for causality in time is breaking of symmetry in time. The
4296880	4302480	past causes, the future, future cannot cause past. That's the necessary condition. Is that a
4302480	4310240	sufficient condition or not? That has not been well answered. Now, if you extend that in space,
4310240	4317680	there is no such framework. I mean, so then you have to ride on a vector space to basically
4317680	4323520	figure out a directionality and then say, okay, something that is happening in one space,
4324160	4329600	preclude something that is happening in another. You might ride on a river and say, okay,
4329600	4335440	I'm going to forward. But then the whole issue of backwater propagation and all that thing happens
4335440	4341040	and then that can break down. So what is that framework? What do we mean when we say causality
4341040	4347200	within the context of what we're dealing with has not been well defined? And that's a struggle
4348480	4356800	in there. And then we anchor on surrogate processes. And Allison has done some work with
4356800	4361840	information theory in which direction the information blows. I think like that. I mean, so
4362720	4368640	those are good starting points and there might be some hint of how we may go about doing it.
4368640	4373280	But until we break through, we are going to be scratching this on the surface and hoping that
4373280	4384240	somehow some model is going to provide that input. So would it be fair to say that causality is a
4384240	4391360	representational assumption rather than a fact? In other words, it's a hypothesis we make about
4391360	4398000	the world and we test. And just to take a simple example, if I just said rainfall and runoff,
4398960	4403680	and I ask you to say, does rain cause runoff? That's going to have all the problems that you
4403680	4412160	just talked about, right? But there are causal effects. Increasing CO2 is causing climate change.
4413120	4423200	And so there are definitely open causal issues. But my point is, are we treating causality as a
4423200	4426800	fact or are we treating causality as a representational explanation?
4429200	4435760	Yeah, we don't know that, right? I mean, probably that to go hand in hand, a certain type of
4435760	4441440	representation will help us infer a certain type of causality. But until we come up with
4441440	4451120	proper definitions and proper classifications, I think what we end up doing is anchoring on a
4451120	4457280	representation that is convenient and then infer causality associated with that representation.
4457280	4462800	And then say, well, no, this represents everything we got. So structural causal model might fall into
4462800	4472000	one of those categories. But yeah, I don't know the answer. I mean, I'm just articulating
4472880	4478240	the questions that go through my mind. But I mean, if we take an extreme example,
4478240	4486160	like F is equal to MA, right? It's a structural representation that was come up with. And you
4486160	4494080	test it, and it all never fails. We start to treat it as a fact of nature, right? Because it's
4494080	4503200	a hypothesis that has never actually been disproved by a counterfactual, by an example that
4503200	4509280	contradicts it. So maybe something similar with causality, you have a chain of reasoning,
4509280	4513840	and if that chain of reasoning always holds up, then eventually you start to treat it as though
4513840	4523200	it's a fact of nature. Probably. I mean, even if it equals MA is wrong in certain cases,
4523200	4532880	like photons, it kind of brings the question of, is all of causality emergent? Can you have
4532880	4540240	fundamental laws that are causal? Or is everything kind of in a higher,
4546640	4550720	kind of more broad context, complex systems?
4552320	4557920	Well, if I can go back to ask Antoine a question. Well, go ahead and answer that first.
4558800	4565280	I was just going to say, I think Ocean, I don't claim to have read Hume, but I think you summarize
4565280	4576560	Hume's argument that, you know, fundamentally, we can't know causality absolutely. But, you know,
4576560	4585040	we can, we can, you know, strengthen our beliefs. And that sort of this is all very useful. I don't
4585520	4592720	think Hume was trying to argue that, you know, we shouldn't be logical beings and throw out this,
4592720	4598000	these aspirations for understanding causality completely. And I think that's kind of what
4598000	4604160	you're saying that, you know, yes, F equals MA is wrong, but it's, you know, right under,
4605760	4610960	you know, most of the conditions that we encounter in our day to day. And so it can
4610960	4616560	basically be taken as fact. And that's kind of what our definition of causality is.
4617440	4622880	It's right until it's wrong. But Antoine, going back to the large language models,
4626560	4630880	if these people who wrote all these papers were to take a bunch of
4632400	4635040	age roles, my daughter's eight, I'm just picking eight out of a hat,
4636000	4642240	and attempted to do these same tests on them, right? That's kind of what I was thinking about
4642240	4647040	when you said that they were testing these large language models to infer causality.
4648080	4654240	If they ran these same tests on a bunch of eight-year-olds, you know, in other words,
4654960	4662880	how do they know that their tests are actually meaningful tests
4664720	4671360	for establishing whether or not the LLM or the eight-year-old has the ability to
4672720	4683120	cause inference? I don't think they do. I feel like this is what we're getting out of this
4683120	4687840	discussion. It's like, what is causality in the end? And how can you be sure that it is
4687840	4696560	causality you're inferring and not logic reasoning as Timothy proposed? So I don't think they really
4696560	4707920	do. All they can do is come up with a benchmark, a controlled one that has causated causes and effects
4708880	4719440	and tests if an LLM is able to recreate that. But again, is this purely reasoning or this just
4722080	4728880	or this just retrieval from your knowledge? And I think there's also another point to consider
4730000	4734320	as Pravin mentioned earlier, that it's a bit different because it's language and
4734960	4739920	might take my thought on that. I don't have anything to support that claim, but my thought on that
4741120	4749840	is that we as humans use language to formulate concepts and to reason. So eventually, if we
4749840	4759520	reach the point to which the LLM is so powerful in text, in natural language processing, actually,
4760320	4769520	what are the implications of it on its ability to formulate concepts and reason? I want to get
4769520	4774160	back to the chain of thought and tree of thought prompt engineering techniques I was telling you
4774160	4780160	about earlier. The tree of thought is pretty much the same thing. So you say to your LLM,
4780160	4787520	right, this is a question, I want the answer. But first, I want your first thought on the answer
4787520	4793520	and then you separate into two and you like choose different ways of thinking about it and
4793520	4800080	just create a tree and output all difference. That would be able, that would make us able to track
4800080	4806240	how does the net. I saw that there is a very interesting paper on it. I can link it in the
4806240	4814960	chat tree of thought. So my take on this, my question would be LLM are basically two years old
4814960	4821680	and they're able to do so much already. And at what pace are they still going to grow in the
4821680	4830880	future? And what are the implications on the amount of knowledge that will be theirs in a couple
4830880	4838480	of years from now? Because in the end, is causality just like you have this in your knowledge, you
4838480	4845200	can take it out and get it again. Because if it's that, I don't have any doubt that in maybe 10
4845200	4850000	years from now or I don't know why. At some point, we'll figure it out to every knowledge we know in
4850000	4859040	the LLMs. It's not reasonable to think that. But core factuals and everything, some more,
4861760	4866400	I don't really have an answer. I don't think anyone has an answer in the papers I mentioned.
4867200	4879920	Yes, Ernan? Yeah, this is great discussion. I have more questions here for us to reflect.
4880480	4889760	But it seems like we are collecting all the reflections. It seems like we still don't have
4889760	4898640	a way to measure causality, a solid way like we do measure models via the WSER or some other
4898640	4910080	metrics. And my perception of causality is something that may be reproducible across
4910080	4916640	experiments in different environments that are looking at kind of the same processes.
4916640	4926240	So think about streamflow in Switzerland versus Tucson versus Washington. But do we
4928000	4934640	have tools to measure causality? In other words, can we say the same way in an analogous way,
4934640	4940400	we have a way, machine learning models, overfeats, underfeats, do we have a way to say this model
4940400	4949040	overfeats causality, this model underfeats causality? This is a good causality explanation of the
4949040	4957520	process. And I don't know if that exists. And that goes back to the way we usually validate or
4957520	4966800	cross validate models in which we split the data set in a number of folds and then we cross
4966800	4975520	validate it. Should we instead do tests for causality in a similar way or analogous way in
4975520	4981360	which we take data sets from different environments and then we see if the knowledge is transferred
4981360	4988240	across those environments via the cross validation. So perhaps that removes a little bit of the
4988240	4994400	anxiety we have for perfection in causality. And we kind of explain it in a way that we
4994560	5006320	quantify and not as a binary yes or no. And another problem is predicting beyond training
5008080	5014480	in the at once example of climate change. It's another limitation. I don't know if we're at the
5014480	5020000	point at which the models will be able to reason and then beyond training, even though they're
5020000	5029040	perfectly trained, reason about climate change consequences and the trends. If the trends are
5029040	5036080	learned from the data themselves, then yes. But if there's nothing that let us know about surprises,
5036960	5045040	I doubt. And the other thing is the data limits are really constraining our learning
5045120	5058720	or the models learning pace of the facts and not to speak of the counterfact loss. But
5059280	5064960	I mean, those are kind of some of the lines or bullets I could raise from everybody's discussion
5064960	5067840	but so far. That's a great discussion.
5072080	5080320	If I, yeah, thank you for intervention Aaron. If I may maybe give another couple of elements in
5080320	5089120	there to give a couple more insights on those questions. Okay, so I have a small amount of
5089120	5096160	knowledge on digital twins. Before I was here, when I was in master's degree, five, six years ago
5096160	5101360	in France, I was an apprentice at Dassault Systems at the same time and they were working on digital
5101360	5108400	twins back then. So I was a bit in there. In short, for those who will know digital twins is a concept
5108400	5114160	that ties a physical object to a virtual representation, where there is a synchronization
5114240	5123600	of data between the two of them. And what's hot in the topic right now is because of climate
5123600	5128720	change and extreme events and everything is a digital twin of the planet Earth. There is
5131680	5138640	a perspective on that from the European Union, which is called destination Earth. So they plan
5138640	5146800	to do a digital twin of the whole Earth by 2027, I don't remember 2028. My take on this is
5148240	5154880	from what I've seen in the paper, talking about the intelligent agents that learn about experience.
5157760	5164240	I drew the parallel between those agents and what on a lens able to do. But the key thing here is
5164240	5174320	that experience is beneficial to causal understanding. And I guess this is what's also been put forward
5174320	5182400	by my the causal inference frameworks that do need a lot of data, experimentations and everything.
5182400	5189920	And so just my small insight on there would be that digital twins may just represent a great
5189920	5201280	environment for LLMs to interact with if those virtual environments are good enough
5201840	5208800	to be representative of what's happening out there. And this is where all work of all
5208800	5214400	traditional, I would say research comes in, flow modeling, ground flow modeling, rainfall runoff,
5215280	5227360	geochemistry. And so my vision of, so we're at UIUC in Illinois in the CI Net program,
5227360	5233680	we're studying the Singapore watershed. And so maybe a target I'd like to achieve would be like
5233680	5241120	to have a digital twin of that watershed represented by a 3D model and data coming in the same time.
5241200	5247200	And on top of that, you would have physics, a physics engine reality represented by all the
5248160	5255360	process based simulation models we know and et cetera. And this would be the great place for
5255360	5264000	LLMs to fully express itself and make interactions and make experiences. And this would potentially
5264000	5272880	provide a great environment to perform causal inference and maybe to test whether LLMs are
5272880	5278240	actually able to do causal inference, because all we've been fitting is generated text and made up
5278240	5282800	benchmarks, which is interesting. And it's the first step. Eventually experience is the key,
5282800	5287440	I feel like, and the digital twin would be really interesting environment for that. This is just
5287440	5293520	like both thoughts I've had. You want to reflect on that, Ocean? No, no, no, absolutely. I think
5293600	5297040	you're absolutely going down the right track because you're saying basically
5297680	5305120	that you need to not only respond, you need to be able to interrogate your environment
5306480	5311440	and giving them a digital environment to interrogate is a useful thing to do. And that's
5311440	5320480	actually behind this poet paper that I put up there about where the environment changes as
5320480	5328000	the ability of the agent changes. So it's like a co-evolution process where it actually evolves
5328000	5331680	the environment, which is one step beyond what you're thinking, what you're talking about.
5331680	5337680	But it also occurred to me, the reason I put my hand up was that I think it's really interesting
5337680	5344320	to have the LLM as the agent that's doing the learning. I hadn't thought of that. I think it
5344320	5349120	would also be interesting in a decision context, which I think is where you're going, to have
5349120	5356480	multiple LLMs take the roles of different stakeholders and play devil's advocate. So
5356480	5361040	you've got the rancher and you could say, okay, your job is to play the role of the rancher,
5361040	5366240	your job is to play the role of the environmentalist, your job is to play the role of whatever.
5367040	5371920	And you could do some very, very interesting role playing explorations
5371920	5382240	with the goal of coming up with potential solutions to difficult transdisciplinary
5382240	5389200	decision-making problems, where you can play them out using agents rather than having to get
5389200	5394800	real humans in there before you then take those to the next stage and involve humans.
5395840	5398560	I'm glad you brought it up because this is exactly what we do.
5399040	5408560	Perfect. And so I'm going to be presenting that with Praveen at AGU. And just to give a couple
5408560	5416640	of heads up. So I have a couple of examples where exactly I have a mayor science expert and
5416640	5422800	everything talking in response to a flood event, impeding flood event or something. And the
5422800	5428160	problematic right now is to find the right metric and find ways to evaluate the output
5428160	5435520	of the models because this is taxed. It's complicated to evaluate compared to LSTM,
5435520	5440480	which would output a sequence of numbers. And then you would use all the mathematics,
5440480	5445280	you know, to evaluate it. But how do you do it with tax? It's either you come up with something new,
5445840	5452480	either you use an LLM to do it, but there is a bias in using an LLM to evaluate an LLM.
5452480	5455920	So this is a question that needs to be answered right now. But yes.
5457120	5464080	That's like a tool for gowns. Yeah, exactly. So yeah, if that interests you and you're coming
5464080	5467280	to AGU, my talk is morning morning. I'm going to be talking about that.
5468640	5471200	Would you mind dropping me an email telling me where and when?
5472640	5479840	I wanted to comment on the talks on AGU because well, with the names that are usually in the
5479840	5486240	talks and also for the participants who were at the SNF in the house workshop, I collected some
5486240	5493440	of the talks that will be presented by participants at AGU. So we have a nice Excel spreadsheet
5493440	5501040	that I could share. Please, save us a slide. Yeah, you can even filter it by daytime. So you
5501040	5506560	kind of have like a... So would you rather have me send your information to you so you can like
5506560	5513440	enrich the database? No, I think I already have you in the database. If you allow me, I can very
5513440	5521120	quickly share my screen and show you. It's just a spreadsheet and I just scraped the data from the
5521120	5526560	AGU website. So go ahead. If I'm missing someone, I could even...
5526560	5535600	Yeah, so just to give us an example, my talk...
5540160	5544080	Are you physically there? Yeah, I will be physically there.
5544080	5551760	Okay, perfect. So my talk is also on Monday, but you can see what I scraped is just the ID,
5552640	5558160	the title of the talk. These are the participants who are usually in this meetings or who are at
5558160	5564480	the workshop. So you can see that it's Uwe Hoshin, myself. I'm the speaker and you can see who is
5564480	5572960	the speaker in this column of authors with the double mark. Thank you. That's a great help.
5573520	5577280	From what I see, we're going to be running around chasing for our...
5577680	5583440	If you follow this schedule very strictly, you will be around the Moscone Center a couple of
5583440	5588480	times, but I think you can pick and choose what looks interesting. Thanks for that. It's great.
5590000	5596640	So I will share the label and then we can see how it can be passed around and how I can add data to
5596640	5604000	it. All right, perfect. Thanks. Unfortunately, I have to go for a student exam. So one final
5604080	5610480	comment, thinking about student exams, this thing about causal reasoning also reminds me of the
5610480	5614320	kind of questions we try to ask students when they're doing their qualifier or they're doing
5614320	5618720	their... Right, there's the kind of questions which are just about what do you know, facts,
5618720	5623360	regurgitate, and then hopefully you get to the kind of questions where they get to
5624080	5629200	generalize beyond and that's where you test their true understanding and or intelligence.
5630000	5633680	So it felt... It just felt like it has direct relevance to what you were talking about.
5636240	5643120	Anyway, I got to go. Bye, guys. Bye, everyone. Bye, bye. Just a final comment that we are going to
5643120	5651120	be having our winter break and we are going to be back on January 17 with Manuel's presentation.
5652480	5656080	Beautiful. Thanks, everyone. Bye, guys. Thanks.
