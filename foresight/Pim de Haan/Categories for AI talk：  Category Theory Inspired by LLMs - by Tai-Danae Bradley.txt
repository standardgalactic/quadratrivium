Oh, sorry, yes. Is this recording?
Good to go.
Okay, so hello everyone and welcome to the third guest lecture of the cats for AI series.
As you might have seen on the on the email list.
We've finished with the main lectures and even two guest lectures and we're now transitioning in a different mode.
So we're going to be having an irregular schedule of guest talks.
And which are going to from now on be open to the public so if you've got a speaker recommendations please do not hesitate to reach out.
And first in this one that this open kind of scheduling is a talk from tight and a Bradley, she's going to be telling us about category theory inspired by large language models, I'm very excited about the talk so tight please take it away.
All right, thank you Bruno and Petra and all of the organizers for the invitation.
And I just want to say I think it's really exciting to see that there's so much interest in category theory outside of the field of pure mathematics.
And in particular in machine learning so I think that's very inspiring, I have a lot to learn from you all and so I hope that maybe I can give back something interesting in today's talk.
So yes, as the title suggests, I want to share with you some category theory that's come from comes from work with some collaborators and I which I'll share in a second.
That's inspired by large language models so I was asked to give this talk I think several months ago, and I thought it was very convenient that open AI release chat GPT, you know, like less than two weeks ago.
So maybe I'm sure all of you have heard about that by now and I think it's kind of nice that, you know, language model or maybe fresh and everyone's mind.
So, I couldn't resist. I know that you have all probably heard about chat GPT and you probably see in your own examples.
I can resist just sharing one that I did. So I would say, don't really pay attention to what's on this slide because it's not really in the theme of the talk.
I was just jumping into chat GPT, and I had a question about some physics thing that I was interested in. And so I, I gave it this, this thing called a quantum state just think of it as a vector, and I asked GPT chat GPT does this vector satisfy a certain
like does it satisfy a property. And I thought it was really interesting that it got the answer wrong, but it knew what math steps to take and what pieces of that math to analyze in order to reason about a yes or no answer does this thing satisfy a property.
So, it kind of like knew what linear algebra things to do you start with this vector and then you get a matrix and then you can put something on that matrix and then you apply this formula, and then at the end you reason about it.
And so I thought that was like, oh, pretty neat, even though overall it kind of got stuff wrong, and at least kind of knew what to do without being told to reason about, you know, how would one even arrive at the answer.
At this point, of course, there's lots of chatter and lots of buzz, you know, is this a GI, or is it garbage or whatever. And I don't really want to get too much into that.
I do want to kind of draw one idea to the forefront of our minds. And that's this fact that the training data for large language models is really interesting. It's just raw text.
And so, from a mathematician's perspective, that prompts a question like what what structure is in unstructured text.
That allows you to like form concepts and reason about things so that you get actual coherent text. So whether or not, you know, chat GPT got the answer right or wrong.
Okay, fine. But what's interesting is that there's no like textual garbage on this page right like all of these words go together and it sounds like coherent English it is coherent English, but you're just starting with, you know, raw text, or, you know, probability distributions on text
configurations. And you can ask the question, what mathematical framework, can we, you know, write down that describes that passage from probability distributions on text continuations to some space of meaning or to some like
semantic information. Like what's the math there. And, and you might ask this like, can we look inside of a transformer and like point to things and say here. Okay, so that's one thing you could do. That's not quite what I want to do.
But maybe what I can describe in this talk is kind of an overall category theoretical framework that maybe can provide for you some tools if you wanted to dig further.
So that's why the title of this talk is category theory, inspired by a large language models, as opposed to the category theory of large language models I'm not making a grandiose claim like that.
But it is really interesting to ask, like how far can you get with just probability distributions on texts, or text continuations. And I think that category theory provides some some good and very natural tools to kind of
exploring this question. So let me go to the next slide. So here's the outline for the remaining few minutes. First, I want to share with you a category of language and some advantages of that.
We'll also see some disadvantages and then I will describe for you an enriched category of language so you've, we've heard a lot about category theory in this series.
But maybe a new idea might be something called enriched category theory so I won't assume you have heard of that already. So I'll kind of explain a little bit of what that means, and then tie it back into this question that we're trying to answer in the context of language.
So the next thing I'm sharing today is based on a paper together with John Torella and Janice Losopoulos. It came out earlier this year, earlier in the spring and enriched category theory of language.
So, since this is an introductory series on category theory, what I thought I would do is just kind of give you a bird's eye view, or the highlights of some of the ideas that that we're working on.
And then if you're interested, I would say, okay, for more details, you know, we can chat after you can take a look at the paper.
Now, before all that, I want to motivate those ideas by sharing with you an analogy.
And there's a really curious analogy between linear algebra and category theory is simple. I think it's really interesting. And maybe it might appeal, you know, to the machine learning community since we deal with vectors and major sees a lot.
And I like this analogy a lot, it'll come up later but I also think it really nicely motivates the ideas that is kind of the main theme of the paper.
Speaking of themes, this analogy that I will share with you kind of centers around a very common theme in mathematics. And I have to credit John Torella for really emphasizing this idea because I think it just illustrates in a crystal clear way the advantages of thinking from a category theoretical
perspective. Okay, so before I share with you this analogy, let me inch towards it by telling you a common theme in mathematics.
So, it's very well known in math that if blah, something has nice structure, and I'll say that means in a second, then functions from a fixed set into that thing have nice structure to.
Okay, what do I mean.
Think about a set x there it's on the screen. Okay, a set is just like a bag of marbles or something, it has no structure.
So, if I have elements in a set x, I can't add them, I can't multiply them I can't combine them in any way to do anything meaningful it's just this set that has no structure.
There is another set that has a lot of nice structure, kind of readily available and that's the reels like, for example, there's other things you could think of but let me just think about the reels.
In, in our I can add things, we can multiply them.
We can do all sorts of things so it has nice structure.
Well, what is a function from a set x into the reels.
I think for a second, for simplicity, just suppose that x has three elements, that's a three element set.
So a function from a three element set into the reels is a choice of three numbers.
So, if I look at all functions from x into the reels, I have the collection of all triples of numbers, and that is familiar to us that's just three dimensional space.
In other words, what I'm getting at here is that the collection of all functions from let's say a three element set into our has nice structure.
I can actually organize those elements, you know the image of this function as this array suggestively, because if I have two of these functions now or two of these arrays, I can add them.
You just add functions point wise, which corresponds to sort of adding the entries of these vectors element wise.
So you can multiply a function by a scalar and get another function that just corresponds to multiplying the entries of this vector component wise, you can multiply actually function together.
So I'm hinting at here, which maybe you can see is that functions from x into our actually form of vector space. We usually denote that vector space by our to the three, our three.
I could write this as our x our race to the power of x. In fact, we've seen this notation earlier, when Petter talked about exponential objects. So this is the set of all functions from a set into our is not just this that turns out to be a vector space.
And that's because those functions inherit inherit structure from this sort of ground field. Okay, so vector spaces or linear algebra are an example, really nice example of where, if something has nice structure, then functions into that thing also inherit nice structure.
And here's the key is that a very similar story holds in category theory.
Here's kind of the analogy I like to have in mind.
On the one hand, we just started with a set, and we, we said, okay, you can't really do much with the set there's no structure, it's just a collection of things.
Now, if someone just hands you an arbitrary category, what do you know about it. Okay, you know I have some objects and some morphisms between those objects and composition and identity morphisms but that's it.
You don't know if C is just an arbitrary category, if you can combine objects to get a new object in your category.
You have other structure looking looking about maybe in specific categories you do but in general just the definition doesn't give you that.
So, what you may like to do, just like in the case of sets.
There, we thought of a specific set, like the reels that had a lot of structure, and we looked at functions into that.
And obviously, it can be very fruitful to think about a category that has a lot of structure that's really nice, and look at functors into that category.
So what category is that well it turns out that the category of sets is a really good choice. So analogous to looking from functions at functions from a set into the reels.
Now, it turns out that looking at functors from an arbitrary category C into the category of sets is there's a lot of rich mathematics there.
So why is that. Let me just say just like in the reels you can add numbers you can multiply them you can combine numbers to get new numbers in the category of set you can do this also.
So, at the beginning of this series, Petter talked about an over fresher memory, you know you've heard of products or the Cartesian product of sets. He also mentioned the co product of sets, or you know I said the exponential object.
So set has this nice property that you can take a bunch of sets and combine them in some way to get a new set. Now let me just kind of pull back the curtain and speak plainly for a minute.
So the idea that I'm trying to mention right now are these constructions and category theory called limits and co limits.
What I'm saying is that set has all limits and set has all co limits. So here's kind of what that means I think some of you are have probably heard these words or you're familiar with them already.
But in mathematics, quite often what you want to do, you have a whole bunch of mathematical objects, maybe their sets, or maybe their groups, or maybe their topological spaces or maybe their vector spaces, and you ask,
is there a way that I can take this collection of objects and combine them in such a way as to get a new object in my category.
Now, there are sort of two typical ways that you can do this and by typical I mean these constructions are found all across the mathematical landscape.
And it turns out that when you kind of strip the constructions of their deep of their details and just look at the pattern. These constructions turn out to fall into two categories for lack of better word.
I mean that in like the English sense not the math sense. So on the one hand, you can form what's called the limit of those objects, and depending on how that's constructed these have familiar names like the intersection of two sets, or the Cartesian product of two sets,
or the Cartesian product of two topological spaces, direct sums of vector spaces.
Meets, if you have a lattice, and you can ask for the meat or, you know, sort of the minimum element, minimum element in the collection your lattice that's that's actually an example of a limit turns out greatest common divisors are also examples of these things.
If you have a group homomorphism and you ask for its kernel, all of these constructions on the left hand side are subsumed by one single idea and category theory they are instantiation of one single idea that's called a limit.
And dual to that, you know and category theory will always like stick the word co in front of something you kind of reverse the arrows and so you get dual operations.
On the other hand, on the right hand side, you have things like unions co products direct sums again joins in a lattice dual to greatest common divisors least common multiple and co kernels.
So these are all examples of things called limits and common limits, we won't worry about the actual definition, like they satisfy some universal property, but I mentioned that to say that in the category of set.
It has this wonderful property that you can take any collection of sets, and then asked for the limit, or asked for the co limit, and it turns out to exist. So it's another set in that category, and it satisfies universal properties.
You cannot do that with any category, any arbitrary category may or may not have that ability. It turns out the category sets does.
So that's just to suggest you that just like the real numbers was a nice set to look at functions into analogously the category of sets is a nice category to look at functors into it.
These functors because this is such a nice choice they have a name. So a functor from an arbitrary category C into the category sets. It's called a co pre sheaf.
Sometimes, you may want to look at contra variant functors from C into set. So it's just a functor where kind of flips the arrows. So in that case, folks will put a little OP above C.
They see up into set. Those are called pre sheaves. So pre sheaves. Let's just say functors.
In any case, this is a nice thing to look at it turns out that functors from C into set itself form a category so just like functions from x into our form a set with structure.
Functors from C into set form a category. It's a functor category and in fact, I think Pam talked about functor categories in his talk.
There the notation might have been square bracket from C to set, but this exponential notation is this notation for that same concept.
So it turns out objects or functors morphisms are natural transformations, but that's just kind of me drawing the analogy.
Now let me give you one more analogy.
And then we'll move on to the language.
There are very special vectors on the left hand side.
In particular, for each element in the set X, there's a very particular vector associated to the element and the analogous thing holds on the right hand side, and that vector is a one hot encoding or a basis vector.
So for every element in the set X on the left hand side, there is a particular function.
That sort of sends any other elements in the set to one, if it's the same as the element I chose or to zero otherwise, or if I write this out as an array it's just a vector with all zeros, except for one in the appropriate spot.
Analogously, there are functors from a category C into the category set, one of these code precheves that is very special.
And these are called representable functors.
So for each objects little C in my category.
There is a functor from the category into sets.
There's another object in the category D.
And it sends that object to the set of all morphisms from your given object C into D.
I've seen this notation already in this series. Sometimes this set is denoted harm from C to D. Some other people, instead of writing harm, they'll write just the name of the category C so it kind of reminds you oh morphisms in what category C.
I like to think of, these are called representable functors, these harm functors, representable functors, these are a particular kind of code precheve, or if I were to kind of move the argument to the left, it would be a precheve.
I'll try later, not now, but I like to think of representable functors as kind of analogous to these one hot encodings, because just like any other vector is built up from one hot encodings.
Similarly, any other code precheve is built up from these representables, but we'll say that later. So I like this analogy.
The reason I mentioned this is because these representable functors will play an important role in just a second. Okay, so that's my analogy for linear algebra maps into law, inherit structure from blah.
So in particular functions into the category set inherit really nice structure. Now let's see how this theme or this analogy plays out in the context of language.
So let me describe for you a category of language.
We'll see that it's nice, but like a set, you can't really do much with it. So we're going to look at functors out of that category. So let me describe that for you now is very simple.
So here's a category, consider all strings from some finite set of atomic symbols.
So if your finite set is the set of all words in English. Okay, it's a very large set but it's fine. You can just think of the take the free modeling on that set right all strings.
The sub string containment defines a preorder on this set. In other words, you know, we can make sense of when one string is contained in another string x is contained in y, and I can denote that by less than or equal to.
So I think Penn mentioned in his talk, the concept of a preorder.
So in my way of reminder, a preorder is a binary relation denoted by less than or equal to that's reflexive.
So every substring is every string is contained in itself. So X is less than X.
And that's transitive.
Right if blue on my screen is contained in small blue and small blue is contained in small blue marble that I know blue is contained in small blue marble. So that's transitivity.
And as Penn mentioned, every preordered set is a category reflexivity is exactly an identity morphism transitivity is exactly composition.
So I just defined for you a preordered set, aka a category.
And it just has at most one morphism between any two objects in that category. So it's very simple.
There's an arrow from X to Y whenever X is a substring of wine. Let's call this category L.
Fine, it just kind of tells you what goes with what does this expression go with this expression. If the answer is yes there's an arrow. If the answer is no there's no arrow.
It just tells you what goes with what. Okay, it's nice but that's very limited. In particular, I cannot, I cannot make sense of, like what's a concept in this category, or what's the context of something or can I combine ideas to get a new idea I don't have any of that
structure here.
So this is a cue from this theme that we just spent several minutes thinking about. And what we'd like to do is now consider functors from that category into the category of set.
So this is just repeating that theme and representable functors these hum functors are particularly nice in this case. And I like to think of them as like a first approximation to the meaning of of an expression.
Let's do that. Okay, pick, pick an expression in the category like the word blue.
And let's look at the hum functor hum blue blank. Okay, where if I put in an expression X and the blank on the middle here, I get a set.
This set just tells me, is there an arrow from blue to X, or not. If there is an arrow from blue tax, then home of blue comma X is just the one point set representing that one arrow.
There's no arrow from blue tax. If blue is not contained in X, then I get the empty set. Okay, now I like this, because this representable functor sort of its pre image is the collection of all expressions that contain the word blue.
And this reminds me of the onata lemma. So the onata limit is this. I'm sure you've heard of it this famous theorem and category theory, which essentially says that a mathematical object is completely determined by the network of relationships that has with other objects in the category.
So you want to understand something important or all all important things about an object, you can look at morphisms out of that object or morphisms into it.
And here, this representable functor picks out exactly the network of ways that the word you've chosen, like blue, or whatever fits into all expressions that contain it in the category in your language.
So, you know, if you think about language a lot, then I'm sure you've heard, you know, this famous quote by John first, you show no word by the company it keeps.
I mean, this is how word embeddings work to something of the meaning of a word is sort of captured in its context. So this representable functor captures precisely that context.
So you might think of this as like capturing something of meaning but you know I put that in quotes, and I say it's a first approximation, because as you can already guess this isn't everything.
You know, there's nothing about the distributional information here. This is just kind of yes or no bare bones, but it's a good start.
So this is kind of like the native perspective. When I see representable functors, I kind of think, ah, the network of ways that that word fits into its environment so capture something of that.
So here's, you know, I'm just kind of belaboring the point. But when I see this functor, another image that I have in mind is like, it's kind of like a vector.
And I put sort of above the equal sign because this is not proper category theory. But when I think of the representable functor, I think of the vector of zeros and ones, or empty sets and one point sets right indexed by all of the expressions in the language, where
an empty set in that slot, if blue is not contained in that expression like deep red being cherries, or a one point set in that slot if blue is contained in that, in that expression, that's indexing that spot like small blue marble, beautiful blue
ocean, etc. So when I think of representable functor, I think of like a vector whose entries are empty, or, or one point set indexed by expressions in the language.
Okay, this is just like something that I have in my mind.
Now, why did we do this to make our lives more complicated? No. So as we said, when you look at functors into a category with rich structure, like set, you can do things with it.
So I was kind of hinting that these representable functors behave like building blocks.
We can actually use representable functors, or these co pre sheaves to construct new co pre sheaves, and we can do that by using the structure that is in the base category set.
So I mentioned set has all limits, all co limits. It's also Cartesian close. What that means it has something like an internal home, which if you unwind all of that.
What it suggests is that we have some kind of notion of conjunction. That's kind of like what a limit is disjunction. That's kind of like what a co limit is enclosure Cartesian closure is kind of something like implication.
So, I want to give you a concrete example how am I making these connections like why is a co limit like disjunction or or.
A concrete example of what that looks like.
So suppose I have, you know, a functor representing red.
That's like the network of ways red fits into the language.
Suppose I also have the functor representing blue. Hey, the one we just looked at.
So I'm claiming that if I have two functors, I can take what's called their co product. That's a kind of co limit, which I claim is analogous to to disjunction.
I claim it's analogous to the concept quote unquote of red or blue. So here's why I make that claim. When you when you write down the definition. Here's what you find.
So a co product means I can take two functors harm red comma blank co product together with harm blue comma blank.
That defines a new functor that sends an expression X to the union of the two sets, harm of red index and harm of blue index.
So here's what that means.
I get a new functor, a new co pre chief. And again, if I envision it is like a vector whose entries are empty set or, or something else.
What you find is that this functor is sort of supported on all texts that either contain red or contain blue or contain both of them.
It's non empty on the union of all of those sets. So, for example, deep red being cherries that contains the word red so this that I get is the one point set small blue marble that contains the word blue.
So I get the one point set.
Did you put the kettle on contains neither of those words. So I get the empty set red and blue fireworks contains both.
So I actually get the union of two one point sets or two points that. Okay, so it's not empty this functor this co product of red and blue is not empty on all sets that either contain red, sorry, all expressions that either contain red, or blue or both.
And that pairs well with this idea of union as like or when dealing with sets. So that's kind of why we think of co products is kind of like disjunction.
Okay, so there are other things you can do, you can take the product, you can do this thing called internal home which is like implication.
Writing that down gets a little bit more complicated, but there are other limits and co limits. So the point is, so here's kind of a summary of what we've done so far.
So we started with a very bare bones category L. It's like a preorder, it just tells you what goes with what. So, in that sense, you can kind of think of it as like syntax, maybe.
What goes with what. Okay.
So we took, we passed from that category to the set of the category of co pre sheaves on it. So that's what I'm kind of thinking as a synantics. So that's kind of where the meaning is in this United types sense.
So every expression on the left, who responds to a representable functor on the right so that functor just picks out the kind of context of that word or all expressions that contain that word.
This passage from left to right, this assignment, blue goes to harm blue, comma blank.
That turns out to be a functor that functor is called the United embedding. So I mentioned that in case you've heard that word.
The op that you see on the left, it just is saying that this United embedding is contra variant so versus arrows but that's kind of like bookkeeping.
So, so this is just a summary of what we've done so far. We started with like a kind of in quotation marks think of it as like a syntax category very bare bones just tells you what goes with what, not a lot of structure, like no structure.
And then if you pass to co pre sheaves or functors from that category into set, you have the ability to capture something of meaning of a word in the sense of john first, in the sense of the native lemma.
And then you actually have structure in that category, and you can start to combine things in a way that kind of feels like logic or maybe like reasoning.
I like to think of pictures so here it is, you know I start with a word and then I send it to like kind of this network of ways that it fits into the category.
This is nice, but as you can already guess, it's very limited.
It's just kind of like binary yes or no does this fit in kind of discrete. It has nothing. It knows nothing about the distributional information of language.
So what you'd really like to do what would be better is if you have like you know version 2.0, where if I'm, you know, if you can give me the fact that blue is contained in small blue, but also what's the probability of seeing that.
If I see the word blue what's the probability that you know it'll be completed by small blue marble, or, you know, whatever that probability is it's going to be higher than like I woke up and had a blue idea, just to borrow, you know, Chomsky or something.
So you'd like to really wait the arrows in your category with conditional probabilities of continuing an expression with a larger expression.
And then if you include this distributional information, then you can ask, okay, can I combine concepts there and is that kind of capturing the sort of framework of getting something like logic or reasoning from, you know, just knowing what goes with what together with the probabilities.
Now the nice thing is that category theory provides a way to do this so this is exactly what we find an enriched category theory.
So let me let me give a quick introduction to that what is an enriched category theory.
So in category theory, if you have two arrows x to y.
I'm going to give you a sum of x to y or what I'm denoting by C x to y that's the set of all morphisms from x to y. Okay, you asked that that be a set.
The point is that in enriched category theory, that may not just be a set.
It could be a set with extra structure or could not be a set.
It could be a set with extra structure will think about it. If x and y are vector spaces.
Then the set of all linear transformations from x to y is also a vector space. It's not just a set you can add linear transformations you can scale or multiply them.
The HOM set is a set with additional structure it's actually a vector space. So in that case, one says the category of vector spaces is enriched over the category of vector spaces.
So whatever your HOM objects are you say your category is enriched over that.
I just said that on the upper left hand side. If this HOM set is a vector space you say oh my category is enriched over the category of vector spaces.
If your HOM set is actually a group, you say your category is enriched over the category of groups. If it's a topological space you're enriched over topological spaces.
If it turns out on the lower left, if this turns out to be a truth value like a zero or one, your categories enriched over truth values, those turn out to be exactly pre-orders.
So those things that Pam was talking about earlier, these are actually doing enriched category theory.
What if your HOM set is just a set, then your category is enriched over a set, and you're just doing ordinary category theory. So ordinary category theories like a special case of enriched category theory.
That's something that we're most interested in for the remaining few minutes.
If this is a conditional probability in a way that I'll explain in a second, then your category is enriched over the unit interval and that's kind of what I want to focus on just quickly.
So what's the unit interval? The unit interval, think of it as a category whose objects are numbers between zero and one, and where there's an arrow from little a to little b if a is less than or equal to b.
So it's a pre-order.
Now, the unit interval viewed as a category have a lot of the same properties as the category of sets.
So what I'm kind of hinting at is that we're now going to want to look at functors into the unit interval, which will be analogous to what we just did by looking at functors into set.
So in what ways is the unit interval like the category of sets? So on the slide, I showed you, okay, on the one hand, your objects are sets or numbers.
Your morphisms are functions or this pre-order. But what else do these two categories have in common?
Well, they have a manoidal product. In other words, in the category of set, you can take the Cartesian product of two sets, you can multiply sets, and there's a unit for that multiplication, namely the one point set.
So what that means is like, you know, a set x times the one point set is just isomorphic to x. So it's a unit with respect to this multiplication.
Well, the unit interval also has a multiplication, of course, multiplication, and it has a unit with respect to that, namely the number one.
So this turns out to be a manoidal product. So both categories are manoidal categories. But wait, there's more.
We mentioned that the category of sets has, you can construct co-limits or limits.
Turns out the same thing is true in the unit interval. So when you unwind the definition, what's the limit, what's the co-limit? It turns out just to be minimum.
Even if I have a bunch of numbers in the unit interval, I can take their meat or their limit. It turns out just to be the minimum of the numbers in that set.
And then finally, co-limits turn out just to be maximum.
So, okay, both categories have all limits and co-limits, and then, you know, there's more.
You have closure. So that kind of means you have this internal harm, or like Petra was saying earlier in this series, you have these exponential objects, which are kind of defined with respect to this multiplication.
I don't want to touch into that, but I just want to kind of suggest in your mind that the unit interval can now play a similar role that the category of sets did.
So how does this play out?
Well, it turns out that we can discuss a category enriched over the unit interval.
Well, just like a category enriched over sets is an ordinary category, we have an analogous idea where we switch out sets with the unit interval.
So what is a category enriched over the unit interval, also called a zero one category? Well, just like with a normal category, you start with some objects.
Similarly, a zero one category consists of some objects.
What else do you have? Well, in category theory, for every pair of objects, X, Y, you ask for a set, hum, X, Y, or C of X, Y.
Now I'm replacing set with the unit interval. So for every pair of objects on the right hand side, little X, little Y, I asked for a hum object, which is just an element in the unit interval.
So for every pair of objects, there's a number associated with them. And I think of that as a hum number or hum object.
Okay, what else is a zero one category consists of? Well, in normal category theory, we ask for composition, aka, if I have a way to get from X to Y, and why did Z, then I can compose them and get, you know, a morphism from X to Z.
So now looked at the Cartesian product on the left hand side, we said that that's analogous to multiplication of numbers. So that's what you see on the right hand side.
And just like morphisms are now this preorder, what I asked now for to have a zero one category is that there's a morphism from this hum object Y to Z times the hum object X to Y, that there's a morphism from that product to the hum object
Oh, that should say X to Z on the right hand side. That's a typo.
Okay, and then, you know, there's more, you can also ask for identities and identity morphism on an object X is just really a function from the one point set into the calm set from X to itself.
So a morphism from a one point set into a set just picks out an element in that set. So similarly, if I kind of look at the analog on the right hand side, instead of a one point set I have the number one, which plays the role of unit.
And instead of an arrow I have this less than or equal to.
And so this is the data of the zero one category and of course you ask it to satisfy some axioms.
You can also make sense once you have the notion of a, of a category enriched over the unit interval, you can discuss functors between two such categories but really it's, it's not a category in the traditional sense it's an enriched category.
So you can make sense of enriched functors. So I'll just kind of go over this briefly but you can imagine on the right hand side. It's a function from the objects of your first enriched category to the objects of the second, that satisfies some inequality that's very reminiscent
of what you might have an ordinary category theory.
For the sake of time let's let's not think too deeply about this, because the real punchline is that what happens when D, the category you're mapping into is a really nice category like the category of sets previously that gave us co pre sheaves and we saw that had a
really nice nature. So in this new kind of iteration, I want to replace set with probabilities, the zero one. And I can look at enriched co pre sheaves into the unit interval. That turns out as you may guess to form its own enriched category and you can talk about enriched natural
transformations and make sense of all this.
So let's not, you won't worry about them too much but that's kind of the idea. I just want to repeat the same story that we just did, but now I replaced the category of sets with the unit interval.
So, I'll just kind of summarize and give you the punchline here's what happens when you do that. So here's now, you know, semantics 2.0 or syntax 2.0 or language as an enriched category.
Like earlier I had a category with an arrow. If one expression is contained in the other. I'm going to now have the exact same thing and do exactly what I said I wish we could do, namely, decorate it with a conditional probability.
The punchline is that fits exactly into this framework of enriched category theory. So language is a zero one category. Its objects are strings of symbols from some atomic set of symbols like before.
The ham objects, the ham object between an expression X and Y will say it's the conditional probability that why this larger string is an extension of X or contains X.
You know if X is it contained in it or it's zero otherwise.
So you can check this actually defines an enriched category over zero one, you know, this sort of reflexivity or identity. Yeah, what's the probability that blue is contained in blue is one.
And then when you write down what this means it turns out that conditional probabilities multiply in exactly what you need to get this composition.
So all I'm saying here is that if you want to decorate these arrows with these conditional probabilities of continuation.
It's like your dreams are fulfilled. Oh, wow, enriched category theory says that this is, you know, something you can actually do formally.
As you may guess this is nice but it's limited as before. I can't combine anything, you know, there's no notion of concepts or combining things and having some kind of logical reasoning in it.
So again, you want to then pass from from that category to zero one filters on that category.
And so when you do this, you will find that representable functors like before are sort of have the right support. So as an example on this slide, you can look at the enriched representable functor of the word blue.
And I like to think of it as like this vector where they're which again is indexed by expressions in the language where there is a zero.
If that expression does not contain the word, the chosen word like blue, or it has this conditional probability, you know the conditional probability of seeing small blue marble given that you've seen blue maybe that's like point to two.
So it has the same support as before but now it has this distributional information.
And you can keep on going, you know you can combine these things. So it turns out that an enriched category theory, the appropriate notion of co products products, or more generally limits and co limits they're called weighted co limits.
So that definition gets a little bit technical. But when you take, when you unwind that definition, you can sort of ask, hey, what's this enriched notion of red or blue, the sort of concept, if I take the co product of these two co pre sheaves.
And if you choose the, these weights in the right way you basically get like that the point wise maximum of the two values individually.
So I think that's interesting and you can kind of try to think about what that what does that mean, like if you were to interpret that.
I'll let you partner that and since I'm running short on time. Let me just say there's a lot more that you can do here.
So you may have already thought about the fact that you know you can get from the unit interval into the non negative extended reels using negative log, you know there's there's an isomorphism actually between them.
And what that means is that your category which was enriched over the unit interval can now be enriched over non negative extended reels.
And that has a name and category theory. So a category enriched over that that's a pre order so it's a category. Those are called generalized metric spaces.
And that is that you can now think about the distances between these sort of concepts or distances between these representable and rich founders. And so what happens is that in this generalized metric space, you know, expressions that are likely
are close together, like blue goes to blue marble so they're kind of close in this generalized metric. But then those which are not likely extensions are kind of infinitely far away.
So maybe, you know, a sweet blue scent is not a thing that people say, or it has a low probability, because colors don't smell, or have smells. And so that kind of pairs well with your intuition that like hey that should be way far away.
You can think about distances now, you have these, you know, abilities to can to combine concepts we just looked at co products but there's so much more that you can do.
It turns out that this has nice connections to even tropical geometry. And so you on a spliceopolis might one of our co authors has really nice ideas about that, and lots more.
So all that to say, kind of wrapping up.
There's a lot that you can do by repeating this theme of starting with something that doesn't seem to have a lot of structure, like on the left hand side, and then looking at maps or functors into another category that has a lot of structure.
And there you you have the ability to form concepts to talk about distances to think about tropical geometry, which you cannot do on the left hand side.
So this all has an enriched story.
And maybe I will just kind of leave you with this teaser.
Everything I've described today, kind of rested on this analogy right between linear algebra and category theory.
So we said that this co appreciate categories like a vector space. I hinted that these representable functors are like one hot encodings.
And I told you I tell you why I think about that to think about it that way. So, just like every vector is a linear combination of these one hot encodings right you take your combination there like a basis.
So it turns out in category theory there's a theorem.
If you were to look so Emily real has a beautiful book on category theory, category theory in context, I think she calls this the density theorem.
And it turns out that every co pre chief is a co limit of these representable functors. So just like every vector is built up from basis vectors, every functor from your category in this set is built up from these home functors.
Okay, so they're kind of like a basis in that sense.
And then the analogy goes on and I will just end here because I think this is really fascinating. And I don't think it's more than an analogy which is kind of puzzling.
In linear algebra you have matrices in category theory you have pro functors. And it's basically, you know, if you write it down it's like the same thing as a matrix for every pair of objects and two categories you get a set a matrix for every two elements and a pair of, you know, a product of
matrixes can be multiplied and you know the formula for that. So in category theory pro functors can be composed, and the formula for that which is a kind of co limit looks a lot like matrix multiplication.
Interesting. Well, you know, in linear algebra, every matrix you can compute its SBD and get singular vectors, it turns out in category theory, every pro functor has something like singular vectors which is called the nuclear pro functor and in fact the way that you construct it is like line by line
is analogous to how you compute the SBD. I think what's really interesting is that as far as I know this is just an analogy.
And not like linear algebra is not a special case of category theory in this sense, but it's really curious that you can do things in linear algebra that we know and love.
So why do I say that because I think that that analogy sort of is encouraging that category theory can be a very natural and beneficial environment in which to kind of understand maybe what's going on with large language models, when all they have to work on our, you know,
distribution on text or probability distributions on text. And as we've seen, you can get quite far, at least from a mathematical perspective.
So, thank you for your attention. And if you're interested in learning more than the paper is available online. So thanks everyone.
Thank you so much, Tai. This was a wonderful talk that I think I'm going to have to keep digesting. I absolutely love the analogies.
I'm just going to open up the questions right now. So if you have a question, feel free to post it in the Q&A, and we can read them out. Alternatively, you can also raise your hand and we can just unmute you.
So while we collect the questions, it sounds like Petar has a question and I don't know if I should read Petar's question or Petardy, maybe you want to say it yourself.
Yeah, I'm happy to post it myself. First of all, thanks so much for such a wonderful talk. I really enjoyed all of the connections and it really brought, I guess, a brand new set of goggles with which I can view all these large language models in a bit cleaner way.
I had a question which it might be maybe trivially contained in what you already described, but, you know, while I think this theory is a nice way of explaining, you know, which word follows the next word and the similarity of sentences and stuff like that, which is what language models do in principle.
It also seems like today you get a large difference between success and failure with these language models, depending on how much you hack the prompts. Like, there was this one paper that said you can just add let's think step by step and it suddenly improves your reasoning capabilities by a whole lot.
I'm curious if you think your theory has an answer to why this happens or could have an answer for why this happens in the future. Yeah, I'm just really curious about that. Thank you.
Yeah, that's such a great question. So I don't know yet.
I don't know. But I think, I think it would take more investigating I think the kind of punchline that I wanted to convey is that maybe this could be a good direction to look in.
If you can kind of have a mathematical framework where you can start to see how concepts combine, where you can start to see something like logical structure emerging, then, and if you kind of explore that a little bit more, then maybe you can start to
pin down some tools that will then allow you to ask these kind of like interpretability questions are like, why is it that if I say, hey, can, you know, GBT and step by step, can you. Okay, but maybe now that there are tools that can allow you to do that and maybe it has to do something with
the sort of structure, you know, one thing that I didn't say, but this co pre chief category set. See, that's an example of a topos, and a topos is known in mathematics as a good place to do logic.
So that's encouraging from that perspective. I think it's much too early for me to give you like a definitive yes or no answer. But I think from a mathematical perspective it's promising because you have a lot of tools to kind of reason it exactly about these things that that are really good questions.
So it's too early to say but maybe I could say it's helpful.
Yeah, no worries, I just wanted to, I wanted to prompt you and see what happens. So, yeah.
Thank you. Thank you so much.
So there's been a, there's been a question during your one hot encoding slide from Jules about.
So, if you go back to the, it's there. Right. It was the in general. So I think it was the slide after actually in general the star could be any set right, not just an integer.
I think this, this might have been, I think this might have been remarking on the fact that one hot encoding.
Well, maybe Jules wants to wants to elaborate on the question themselves.
Okay, so, so Jules is saying this was answered later.
So, so that's good.
So I might ask a question myself so I absolutely love sort of the idea of thinking about structure in this way category theory has a lot of structure and when we think about unstructured text.
It doesn't, it's just sort of strings and sort of connecting in this way is absolutely fascinating.
What I'm curious about is, have you thought about connecting what you just said in this talk with with this other ways of structuring thanks text namely parts of speech, and sort of the kinds of parsing where I get a tree like structure of a sentence could
they perhaps see from the network of relationship that some things are adjectives and others are perhaps nouns.
Yeah, so that's a great question. Yes, you will notice I mentioned nothing of parts of speech.
On the one hand that was done intentionally since you know, GPT is training data is just raw text and one doesn't have to tag parts of speech.
Yeah, once you have this framework, you can ask, like, can you know, chat GPT give me some examples of adjectives.
Give me some examples of nouns I haven't tried that yet actually someone should try it or maybe I'll try it after this and see if it does it correctly. So yeah, does it learn things like that.
So I think, like my answer to Petter, it's hopeful. In fact, there's a philosopher that also we're working with Juan Luis Gastaldi who has some very good ideas in this direction. So I would say, stay tuned.
And let's see right now it's again, kind of too early but these are things that we're definitely thinking about and looking to go in that direction and some some folks even in our research circle have ideas but but still work in progress.
But yeah, that's a really great question.
Thank you. So we have a question from Tali. The question is, is there an analogy between matrix algebra and profanctors. Sorry, if there if there is an analogy between matrix algebra and profanctors.
What would be the categorical analog of higher order arrays or tensor networks.
Yeah, that's a fantastic question. So just like so a higher, so higher order array. So matrix is a function function. Let me just blow a matrix is a function from a product of two sets.
So a tensor of order three would be a function from a product of three sets, a tensor of order 10 would be a function from a product of forces.
So just like you can take, take the product of more than two sets. You can also look at functors from a product of more than two categories. So profanctors also have higher, you know, higher order array analogs.
And that's easy to write down. So, yes, so these things called tensor, you know, higher order tensors they have analogs and category and they're just kind of straightforward generalization.
So we have raised hand from Pym.
Hi, thanks for the very interesting talk I have a very pedestrian question sorry for that.
At some point you talked about the composition of like in the 01 category of the syntax.
Could you go to that slide perhaps, because I kind of.
Yeah, here. So here we have like why given X and Z given Y is Z given X.
Normally, I would guess you have some sort of a sum over why there. How does that work.
Yeah, so here there's no sum. I'm so when I look on the right side my mouse for some reason is not appearing on my slide but when I look at the arrow from blue to small blue, think of point to two is the probability of small blue conditioned on blue.
So the probability of seeing small blue, given that I have just seek for blue.
And there's only one way of getting there. So, okay, okay, I see.
Good. Thank you for asking for clarification. Good.
Okay, and this equality because typically I guess for this category wouldn't actually need equality here, right. You would. Yeah, so we just have an inequality would suffice.
Yeah, okay. Yeah, and any quality. Actually, both of those equalities like you just need any quality, but we happen to get equality here.
Okay, yeah, okay, thanks.
Yeah, thank you.
So, by the way, here in Glasgow we have a big watch party watching your talk ties so there's Mateo capuchy in the room with me who's going to come here and he is interested in asking a question.
Oh, fantastic. I didn't know there was a watch party. Hi.
I had just like, so I know about this co cat, which is another categorical framework for doing natural language processing and I'm wondering what's the relation.
So, um, disco cat I think is inherently looking at connections with quantum physics.
So I have not mentioned anything about quantum here. And also, the question that I'm trying to answer is a little bit different. So here I'm trying to see how can I start with probability distributions on texts.
And how can I pass from that into something that feels like meaning or semantic or something that has something about knowledge or reasoning, you know, inspired by large language models.
So the question that I'm starting with is a little bit different.
And the tools or the sort of assumptions that I'm making like I haven't said anything about quantum physics or
math. I'm also here. I mean, I could do this but I haven't. I get asked for a representation of all of this information. So I'm just working with categories right but you know if I look at a transformer there's no like category written down in the paper, you know, attention is all you need.
So I could ask, Okay, now that I have all of this mathematical structure, can I represent it when your altruba is a nice way to represent things. So can I, you know, represent them by vector spaces.
Could those vector spaces then be tagged with parts of speech which is kind of like what's happening in disco cat. Can I then like making a comparison with similar structure that appears in quantum. So I'm not doing anything like that.
I could want to. I mean, I could, and I have a paper on this with the honest that came out I think last year. So you could ask for a representation of this kind of information this category theoretical information in terms of linear algebra.
We think that to actually piggyback on someone else's question tensor networks are are very good choice for that. But even then sort of our premise for choosing that is a little bit different. So it turns out that in, you know, condensed matter physics and quantum
physics, they have very nice tools that happen to kind of match with the statistics of language.
But those tools, you know, can be used outside of the physics context, even though historically they've been used there. So even then in that work that we're doing we're not really saying anything like languages quantum or you know entanglement means this nothing, nothing like that.
I think that disco cat is quite different from this, even though we both happen to be, you know, thinking about language in terms of category but the questions we're answering are for different the tools are different.
The sort of premise or the reason why using those tools are different. Yeah, that's kind of my high level answer to that.
Thank you very much.
Thank you for the question.
I think this might be a good, good place to stop.
We're a bit over time, but this was absolutely fantastic. So I'll just say my thanks one more time.
As now we're basically since we're done with the main part of the course really what we're now in cats for AI as I've mentioned very old, the things we're going to be having our guest lectures.
So this is going to be on a regular schedule and if there's any recommendations you have from people who would love to talk or who you think could contribute meaningfully here would love to hear about it.
So far we have two future lectures scheduled, which are which are going to be sometime in March and we're going to update you on it as we learn more. I don't know if there's anybody else from the organizing team that has to say anything.
Other than one of those talks will be by David Spivak, so you should definitely not miss it.
Okay, so that's it. Thank you very much.
And see you next time.
Thank you all for the invitation for your time. Appreciate it.
Thanks so much for coming time. Really enjoyed it.
