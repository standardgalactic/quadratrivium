start	end	text
0	6080	Hello, everyone. I would like to welcome you to our course categories for AI.
9040	15600	In the following weeks, we'll discuss the exciting and rising role that category theory has in deep
15600	21040	learning. It's something incredibly interesting, and we're happy to be able to share this with you.
24000	28160	My name is Brona Gavranovic. I'm a PhD student at the University of Strathclyde,
28160	32320	and I'm bringing this course to you together with the rest of the organizing team,
32880	42560	Andrew, Joao, Pim and Petar. We are all in some way affiliated with deep learning category theory,
42560	48880	or both, and interested in talking about technologies that will benefit us all,
48880	56560	and we'll all want to use. So the first question we want to answer is, why categories for AI?
58800	65360	The reason is simple. We believe in a future where all deep learning experts will use some
65360	77040	aspects of category theory in their work. Now, deep learning is a new field, and as it's often
77040	84080	the case with new fields, with new scientific fields, they start in an ad hoc manner, and then
84080	90160	later these fields are understood differently than they were by their early practitioners.
91840	97680	So for example, in taxonomy, people have been grouping plants and animals for thousands of years,
97680	102800	but the way we understood what we were doing changed a lot in light of evolution and molecular
102800	109200	biology. In chemistry, we have explored chemical reactions for a long time, but our understanding
109200	116880	changed a lot with the discovery of the atom. And then in programming, people started to program
116880	123120	by tinkering with transistors and logic gates, but most of what we call programming is heavily
123120	135040	abstracted from that. And then now we have deep learning. Deep learning, despite its remarkable
135040	142480	success, is a field permeated by ad hoc design choices. So as we all know, neural networks,
142480	149600	neural network architectures have all these knobs and tweaks that we can't formally justify just yet.
150720	159360	We keep being surprised by new architectures such as GPT-3 or stable diffusion, and there is no
159440	164960	unifying framework for deep learning. There is no unifying framework that would explain the
164960	170480	probabilistic perspective, the neuroscience perspective, and merely just the gradient-based
170480	176640	iterative updating perspective. In fact, in the future, we might look at deep learning very differently.
178000	182800	And our claim is that category theory will become the unifying deep learning framework.
183680	189040	As a matter of fact, it might become a general theory of neural network architecture,
189040	193200	architectures, and such an essential tool for deep learning practitioners.
195280	200800	Now, this is certainly a bold claim, but it is the one we hope to substantiate in this course.
203280	209520	So what is category theory? If I had to describe category theory in one sentence,
209520	216560	it would be this one. Category theory takes a bird's-eye view of mathematics. From high in the
216640	222320	sky, details become invisible, but we can spot patterns that were impossible to detect from
222320	229520	ground level. The famous quote accurately describes what category theory has done to mathematics.
230640	236800	As we'll see, it's not taking a bird's-eye view of just mathematics, but it started to do that
236800	242000	to all of science in the form of a new wave that is being called applied category theory.
242800	247760	So before we do anything, I just want to give you a one-slide summary of what applied category
247760	255440	theory is and what we hope to teach you in this course. Applied category theory is a particular
255440	260320	way of structuring your knowledge. It's grounded in the idea of compositionality.
262240	266720	It originated in pure mathematics and has since spread to numerous fields.
266720	275680	This is a formal language that is not just a part of these many fields, but it's being used to build
275680	283840	bridges between these fields that were previously unknown. So other than in pure mathematics where
283840	291280	it's, which it permeates, it has been emerging across the sciences. So it's been found in physics,
291360	297680	it's been found in chemistry, it's been found in systems theory. It is all over computer science
297680	304560	and it is the theoretical foundation of functional programming. It has been found in game theory,
304560	310560	in information theory, control theory, probability theory, cryptography, and many others.
313040	319280	For us, the most relevant is deep learning where there has been a number of recent papers
319280	327600	in the last two or three years. Now, many people haven't heard of category theory and that might be
327600	332240	because it started being applied across the sciences only in the last five years or so.
333360	337920	So here we see a graph of the intersection of papers in category theory and machine learning.
338960	347600	As you can see, we are in very early stages. We have just had our fifth applied category theory
347600	353600	conference, the only one, and the only existing applied category theory journal just published
353600	361680	its fourth volume. So this recency of category theory might be one of the reasons it is difficult
361680	369200	to teach. For all of its expressive power, it's an notoriously difficult subject to learn.
370080	374240	As it originated in abstract mathematics, most introductory material
374240	381440	is not aimed at the general audience of programmers or scientists in general. There are a few exceptions.
383360	389760	So definitions in category theory are extremely information dense. For example, a monad,
389760	395360	the concept of a monad shown on this slide has a number of components satisfying a number of rules.
396320	399840	And then each of these components is often information dense as well,
399840	406160	meaning category theory requires a lot of initial investment to start appreciating.
409200	415120	But once you start learning category theory and start appreciating the definitions,
415120	420560	you see that the definitions form an incredibly cohesive theory, really not found anywhere else
420560	427360	in science. These interplay together at such a remarkable level, which is hard to appreciate
427360	432960	when you're just starting to learn it. It often looks very difficult. And on a graph,
434160	440960	the learning process looks something like this. So compared to traditional methods,
440960	445680	the structural method of category theory takes a long time to get started. But once you do,
445680	454080	the theory scales much better. So in some sense, category theory is a theory of how to scale up
454160	461600	our systems. And this is something we want to teach you in this course. We want to teach you
461600	467280	how to approach category theory, motivated with some practical examples from deep learning,
467840	471760	and really give you a sense of the philosophy behind category theory.
474480	480640	So this is week one. And after today's lecture, which is going to be an introductory
480720	489440	lecture to the general thoughts of category theory, we're going to start going into the details.
491200	496080	In week two, we will be studying essential building blocks of category theory, categories
496080	502160	and functors. In week three, we will study how category theory can be used to describe back
502160	507440	propagation using monoidal categories, admitting a visual and intuitive graphical language.
508400	512880	In week four, we'll study how geometric deep learning and naturality
514240	522080	and have their foundations in category theory. We're going to study natural graph networks.
524160	529760	And lastly, in week four, we will see how monoids, monads, and various algebraic structures
529760	533360	can be connected to recurrent neural networks and LSTMs.
537440	541360	After our five weeks of lectures, we have a series of talks by people who are
541360	546400	in the industry doing category theory deep learning or both. And we are very excited about those.
551440	558400	But yes, in today's lecture, we're going to start by studying what is compositionality,
559360	565280	what is category theory really, what you need to start learning and taking advantage of it,
565280	571520	perhaps the most important thing. And we're going to take a look at what category theory
571520	581760	has done and can do for deep learning. So yeah, let's get started. So what is compositionality?
581760	587600	This is a concept I've mentioned, which is central to category theory. And it's a concept
587600	593760	that's often misunderstood. It's often presented as an ability to build systems together by
593760	601600	composing them out of smaller subsystems. Now, this is certainly a component of compositionality,
601600	609360	but this is not all. Compositionality includes the ability to build the systems, but we don't
609360	615280	just want to build a very complex system that we can't reason about. It includes the ability to
615280	621360	reason about the resulting system recursively in terms of its components. And really,
621360	627680	compositionality is more of a property of the model of our system than the system itself.
628960	633120	So we're going to see what that means. Compositionality requires both of these things.
633680	639680	And for many models of our systems that are found throughout nature, we have the first
639680	643760	property that we can build the system, but not the second one that we can reason about it.
644560	652400	So for example, when we study behaviors of markets, of organizations, of economy,
652400	661120	of neural networks, and many other concepts, these things aren't compositional. Now, what does
661120	667840	that mean? These are very fuzzily defined concepts, but they're precisely very fuzzily defined only
667840	672400	because we don't know how to reason about them from outside. We don't know what are the fundamental
672400	676560	building blocks, and we don't know how to reason about the behavior of, say, the global economy
677360	682160	by studying behavior of economies of individual countries. There's many emergent effects that
682160	687520	are happening and that are hard to track. On the other hand, there are many systems,
688080	694000	many models of our systems that have both one and two. For instance, and these are often very
694000	698560	simple systems. We think of them as simple because we understand them. So for example, if we have two
698560	703520	differentiable functions, we can put them together and get a composite differentiable
703520	710160	functions by using the chain rule. If we have a compiler from a language A to language B,
710160	715120	and a compiler from language B to language C, we get a joint compiler that compiles all the way
715120	722400	through. We can compose various things like Markov kernels. We can compose merely polynomials and so
722560	732480	on. There's many kinds of systems. Now, this slide, if you're seeing this, this might
734160	739520	ring some alarm bells. It might seem like I've said something inconsistent here. It might seem like
739520	749520	we are saying this. So it certainly looks like I said differentiable functions like
749520	754240	compositional. And I said neural networks are made out of differentiable functions. Therefore,
754240	759440	neural networks are compositional, but I said they're not. So this meme might look like what
759440	764000	we're saying, but really what we're saying is the following. Compositional with respect to what.
765280	770000	So I said compositionality is a property of our models, and we have to specify what
770800	776240	our model is, what it is that we're studying. It's important to specify the property of the
776240	781600	system we want to model. So if our model treats neural networks as differentiable functions,
781600	787120	then, indeed, neural networks are compositional. We plug together a bunch of differentiable
787120	793360	functions, and using automatic differentiation, we can compute, or chain rule, really, we can
793360	801440	compute the derivative of the composite. But if our model treats neural networks as
801440	805360	generative or discriminative models, then this is not compositional.
808160	812960	It's important to specify what property we're studying and what exactly are we modeling.
815200	825360	Really, compositionality is about interfaces. If you think about a system composed of many
825360	833920	smaller subsystems, and the simplest case here is that of a function. When we compose these
833920	839440	systems, we want to ensure that all the data we need is available at the exposed interfaces.
843520	849920	So really, if we have three functions like this, the famous property of associativity
849920	855920	tells us that if we compose f and g, and then with h, that should be the same as composing g
855920	863280	with h and then with f. So this famous property posits that function composition should be
863280	869440	associated. But why is this the property that you might want to have? Because it allows us to treat
869440	878080	functions extensionally, only by looking at their interfaces. And if these composites are the same,
878080	883200	then function composition is associative. And we can look at only the input and output of the
883200	888320	system to know how it behaves. And we don't need to know anything about its internals or how it was
888320	898880	built. But if this property isn't satisfied, then we might be in trouble. Non-compositionality implies
898880	905920	that we need extra data to reason about the system, data that isn't available through the interfaces.
906880	911280	But if our method of interaction is only through the interfaces, which is the intended way of
911280	915600	interacting, then this means that we have uncertainty about how the system will behave.
916800	923600	And then composing many such systems together, our uncertainty can only grow.
926480	932320	Really, what we want to is minimize this uncertainty by imposing some invariances of how we
932960	937920	put systems together. So that's when we create a bigger system, we know something about how it
937920	944400	behaves. So really, compositionality is a very delicate property. And I'm a fan of this quote
945040	951920	on the bottom of this slide, which tells us that it is so powerful that it is worth going to extreme
951920	960080	lengths to achieve. So this is what category theory is. It's the study of compositionality,
960160	965520	of how we can put systems together. And to my surprise, when I started to learn it,
965520	971520	it's not really just about functions. Functions were an example here that is simple. But category
971520	977280	theory studies all sorts of complex systems, from trees on the top left, to networks on the top
977280	982640	right, to circuits on the bottom left, to bi-directional transformations on the bottom right.
983440	991840	So with this in mind, we can start describing what category theory is. Even though we're talking
991840	999280	about all sorts of abstract systems and gadgets, category theory is still a precise mathematical
999280	1006960	language to talk about it. It's the kind of language that emphasizes relationships between
1006960	1013120	concepts as opposed to concepts themselves. And we're going to see what that means shortly.
1016240	1022160	And we're going to see that particular structures in category theory have
1022160	1030320	visual representations that aren't just doodles or sketches. They are former representations
1030320	1036240	that can be manipulated, even in a computer. So to give you a sense of what this really is,
1036240	1042800	I'm going to have a very short demo where I'm going to just draw a few things. So I'm going to
1044400	1048480	switch to
1054560	1064240	my iPad and just give you a sense of how these things work. So I said category theory is a
1064240	1072960	unifying language for mathematics. And to appreciate what this means, we can perhaps focus
1072960	1081200	on some subfield of mathematics, like say group theory. So in group theory, we might want to
1081200	1088160	study a particular group. And a central concept in group theory is that of a group homomorphism.
1088160	1095040	If you have two groups, we can have a structure preserving a mapping between them. And then we
1095040	1099200	can have many mappings and they can go between groups themselves and so on. And there could be
1099200	1103520	many groups that we want to study. Maybe some of them are not connected and so on.
1110000	1114720	And we can study about various properties of these groups and study how they behave. Now,
1114720	1123680	separately, in set theory, we might have some set and we might study functions between set one
1123680	1129520	and set two. These are really honest to God functions that we can compose. And maybe there's
1130160	1137040	S3, S4. And there's various kinds of structures that we can study between them. It's a very rich
1137040	1141680	and intricate field. And I'm going to write here set and I'm going to write here group.
1142640	1149840	Then again, if we're studying, for instance, we might study vector spaces or similar things. So
1149840	1154320	for instance, I have a vector space one and a vector space two. I can still study some sort
1154320	1159280	of structure preserving mapping between these, between these other structures. And for instance,
1159280	1166880	this could be just merely vector spaces in a field R. Now, what category theory does is
1167680	1173040	gives us this bird eye view of these things. All of these structures are categories. So here
1173040	1178080	we have a category of groups. Here we have a category of sets. And here we have a category
1178080	1188800	of vector spaces. And all of these structures are special examples of category teams. The same way
1188800	1194720	we have studied structure preserving maps between groups, group homomorphisms,
1195600	1200160	by formulating all the groups as a category. Now we can study structure preserving maps
1200800	1208400	of these categories, which are called functors. So for instance, we might have a functor from
1208400	1213280	the category of groups to the category of sets, which takes a group and gives us the underlying
1213280	1219120	set. Or we might have a functor that takes a set and gives us the free vector space on a set and so
1219120	1227840	on. The idea being that once we have lifted ourselves to this abstract level, a lot of
1227840	1233680	new things open up to us. So what we end up studying is a lot of interesting things like
1233680	1240880	the category of, let's say, let's call them systems with a particular interface, input and
1240880	1246080	output. And we can have a system with internal state S1 and S2. And we can say the ways these
1246080	1250720	are related. And then we can study categories of systems with a different interface and map
1250720	1257600	between them and so on. And these systems can be actual processes, computations that do something
1257600	1262960	very interesting. That isn't just sort of what you might think of as strictly mathematical.
1265360	1270480	And lastly, we might study categories. These categories might have a lot of interesting
1270480	1277040	structure. For example, I've drawn here a monoidal category where we can sort of put
1277040	1287280	two objects in parallel, as opposed to just composing maps sequentially. And then in category
1287280	1292320	theory where it turns out that these monoidal categories, that's what they're called, where
1292320	1297360	we can put things in parallel, have a formal visual representation. So the story is a bit more
1297360	1302960	intricate, but you can think of it as taking this category and mapping it into a visual space
1302960	1309280	where each morphism has a specific shape. And you can draw it as boxes. So now these boxes
1309280	1313280	aren't just sketches and doodles, but they're formal mathematical objects.
1327440	1343520	So let me just switch to my other screen. Right, so this was merely aimed to paint a picture of how
1343520	1349760	these things look. And finishing with the monoidal categories in this demo,
1350160	1360880	these allow us to have a very natural visual language for talking about systems and processes.
1360880	1365920	And these are really, this is an example of what might, of a particular process of making a pie.
1365920	1372560	And we've all seen these things. So this is not something new. But it allows us, and it gives
1372560	1379360	us a very different perspective on these things. So when you're doing category theory, you might
1379360	1384160	be studying various sorts of systems and processes that do various sorts of things. And
1384160	1388880	in category theory, there's concepts discovered or invented by various people for studying these
1388880	1394480	things. So if we're studying resources or processes in this way, we might use monoidal categories.
1396960	1402560	If we want to study probability, for instance, we will use Markov categories. So for instance,
1402560	1407040	on the left side, here we see a process which takes an input, applies a function to it,
1407520	1414240	and then copies the result. On the right, we first copy the result and then apply f
1414240	1421120	individually on inputs. Now, if f is merely a function, then these two processes are equal.
1421120	1426880	And in many other cases, they're equal. But not all of them. If f is a stochastic function,
1427680	1432080	then these are not equal because rolling a dice and then copying the result doesn't give us the
1432080	1437840	same result as rolling two dice. So Markov categories allow us to describe such processes
1437840	1443760	at the needed level of generality. Then we might, if you want to study how local systems,
1443760	1448880	how behavior of local systems gives rise to the behavior of a global system,
1448880	1454720	you might want to use sheeps. Then again, if you want to study bi-directional transformations,
1454800	1462800	such as extracting a row from a database, then updating back, so forward and backward.
1462800	1467360	Or if you want to study neural networks, a forward pass and a backward pass,
1468160	1471840	we might want to use optics and lenses, which are bi-directional data structures.
1474480	1479040	Then again, if you want to study contextual computation or computation with side effects,
1479040	1482960	we might want to use things called monads or their Kleisler categories.
1484640	1491200	And of course, in computer science, the concept permeating it is recursion. And then we might
1491200	1497600	want to use things called final co-algebras or initial algebras to study recursive processes.
1500800	1505840	And once we study all of these things, we might find ourselves in this situation
1506480	1512880	where we study a bunch of different kinds of systems with different kinds of categories and
1512880	1517600	concepts. And that ends up just creating more categories and more concepts. And the theory
1517600	1523600	that studies category theory is called two-category theory. So something I love about the way you
1523600	1531200	approach CT, that it is very meta and recursive in nature, allowing us to describe category theory
1531200	1540640	using category theory. And then really, once you start going with this, you might have heard about
1540640	1546640	classical mathematical structures such as topological spaces or rings or fields. And once
1546640	1553120	you start going into category theory, it's hard to appreciate how deep and rich the field is,
1553120	1559680	because the water keeps getting deeper and deeper and the rabbit hole hides so many concepts that
1559680	1566080	I personally wasn't aware of that have a very expressive nature and a very compositional nature
1566080	1571600	in helping us understand all of the systems. This is certainly not the end. There's many more
1571600	1577600	that every category theorist has a certain understanding of a part of these concepts.
1580160	1588160	So this was very abstract in a sense, but I hope it conveyed some sense of what category theories
1589120	1595920	are. But now I would like to make this a bit more concrete and study what is the relationship
1595920	1604160	between category theory and deep learning? What has been done? How much are these concepts
1604160	1611680	actually connected to each other? So the thing I want to establish first is that the deep learning
1611680	1617760	community seems to be very well aware of the concept of compositionality. I have found numerous
1617840	1624240	workshops, programs, blog posts, lectures saying compositionality is important.
1625520	1628400	And I've linked a few of them here. I'm sure there's many more.
1630080	1635600	Joshua Bengio in his Turing lecture actually explicitly states that we need to build
1635600	1644320	compositionality into our machine learning models. But having studied category theory,
1644960	1650640	I'm still noticing that compositionality is done in ad hoc ways and that there is a large gap
1650640	1656160	between how compositionality is thought of in deep learning and the theory of compositionality
1656160	1663440	that category theory offers. Nonetheless, there have been some recent strides.
1666240	1671360	So I'd like to give a brief overview of what has been happening on the intersection of
1671360	1679200	category theory and machine learning. So the papers I am showing here are two papers that
1679200	1685520	study neural networks in the very abstract, in the form of something called a parametric lens.
1686320	1690480	We're going to see exactly what that is later in the course, but I just want to give you
1690480	1696000	an idea of how these things work. So it's something that has a following shape.
1696000	1704080	And it abstractly models the information flow we see in a neural network. So on the left side,
1704080	1708960	we see inputs and their gradients. On the right side, we see outputs of a neural network and
1708960	1716560	their gradients. And here we take the two-dimensional notation seriously, and therefore we have weights
1716560	1723520	coming from top, weights and their gradients. And this abstractly models how the information flows.
1723520	1728720	So we can see that from left and top, we get inputs and parameters, compute and output,
1729280	1735360	and then having received the gradients of the loss with respect to that output, we can back
1735360	1742000	propagate the gradients with respect to the input to the left and back propagate the
1743520	1746640	gradients of the loss with respect to weights to the top.
1747600	1754320	Now, the question you might ask is what is the benefit of this formulation?
1754320	1759440	Why would we model it like this? Well, when these constructions were originally discovered
1759440	1766960	a few years ago, something fascinating happened. Independently, game theorists were modeling
1766960	1774000	economic agents in game theory using these categorical models. And to the big surprise,
1774000	1778720	these models ended up having the same categorical form as the machine learning ones.
1780320	1786400	So if you're studying economic agents that take some inputs, produce some outputs,
1786400	1790880	receive some payoff for their things and have their trying to maximize some strategy,
1791440	1797680	you end up with a model that has the same shape. So independent formalizations by different people
1797680	1803760	of game theory and machine learning converge to the same mathematical form. This elucidated
1803760	1809520	a number of connections and really sparked a whole study of cybernetic systems or reinforcement-like
1810640	1815200	agent systems, the study of agents interacting with the environment using minimal assumptions
1815200	1819760	about what agents or the environment is. And really, this is something that can't be done
1819760	1826240	without the level of generality that category theory provides. So I found this quite fascinating
1826240	1833520	as it was being discovered. And it's one of the reasons why I, it allows me to study machine
1833520	1842320	learning in game theory through a unified, unified lens. What it also turned out that why would we
1842320	1846640	want to use these parametric lenses that we can also model optimizers of neural networks.
1848400	1853440	So, and what it ended up turning out that optimizers of neural networks have the same shape
1853440	1858800	as neural networks themselves. So on the top here, the idea is, so on the left and right,
1858960	1864480	we have parameters, but on top we have the state saved by these optimizers, the stateful ones like
1864480	1871440	momentum or add-on. And this gives us hints that optimizers are some kind of hardwired
1871440	1876320	meta learners and just as some papers have shown. So there's lots of interesting research opening
1876320	1881120	up to this. And an interesting thing from a personal standpoint happened with regards to
1881120	1886080	optimizers. We started formalizing gradient descent using lenses, this concept that I
1886080	1891520	mentioned that models by directionality. And for a long time actually had suspicions about
1891520	1898240	this formalization. The problem was that there was a part of a lens that really wasn't used at all
1898240	1904560	in gradient descent, casting reasonable doubt on whether lenses are the right abstraction.
1905920	1911360	Maybe they are really an overgeneralization. But then as we went on to formalize more
1911360	1917600	complicated optimizers, such as the one that used Nestor of momentum, we found that the bit
1917600	1923040	that computes the look ahead in Nestor of momentum, the one that's always lost over in explanations
1923040	1928160	and when you implement these things, it requires rethinking about how you structured the stuff.
1928160	1932000	It had a natural place in our formulation. It was the thing that we didn't use before.
1933360	1939120	So really, from a personal standpoint, it clarified how a lot of these things fit that we
1939120	1949920	started using this categorical formulas. And lastly, we're not the only ones using them.
1949920	1957920	So very recently, about a week ago, I found a deep learning professor at New York University,
1957920	1964560	if I'm not mistaken, independently starting to use the same sort of graphical notation
1964800	1974000	as the formal one we have been using. And I'm quite amazed that we were sort of having formal
1974000	1982400	justification that these two notations have converged to the same representation.
1984160	1988000	So yeah, this shows one line of work that has been done with machine learning,
1988000	1993760	with category theory. Other people have studied different things. People have studied recurrent
1993760	2000160	neural networks. And it has a very strange title here, but if you open the paper, you'll see that
2000160	2005840	it does study recurrent neural networks. And in recurrent neural networks, we have the
2005840	2010960	idea of back propagation through time, which is done by unrolling the recurrent neural network.
2010960	2018560	Now, what this paper showed is an interesting thing that this process of back propagation
2018560	2023600	through time doesn't merely use differentiation, but it is differentiation
2024720	2029520	in a very sophisticated way. And it uses the formalism of something called double categories,
2029520	2035200	which is yet an even more advanced but very visual concept. So this short picture shows you
2035200	2044640	how you can think of each time step of recurrent neural network as a vertical kind of morphism,
2044640	2047600	and you can think of layers of neural network as horizontal.
2053280	2060080	And then Petar and Andrew, who are co-organizers of this course, have done a lot of work on
2060080	2064400	geometric deep learning and graph neural networks. And they've established a connection between
2064400	2069440	graph neural networks and dynamic programming, which is quite fascinating because this might
2069440	2073520	be such very different things, thinking about how graph neural networks work and how
2074240	2079920	the algorithm of, say, Bellman-Ford works. Yet again, using the abstractions of category theory,
2079920	2090160	we can say something more precise about this. And generally about, yeah,
2093360	2097280	in generally about all of these neural networks, this is some of the stuff that has been done.
2097600	2104160	But really, there is so much more work to be done and so many more things to explore. I've just
2104160	2110240	given you a glimpse of some of the interesting things, but really, transformers haven't been
2110240	2114720	studied, have been studied to some small degree, but there's so much work really to do on them,
2114720	2119600	through category theory. Geometric deep learning is being studied now, actually,
2119600	2125680	through category theory, but then again, this is such a vast field. And the same holds for
2126320	2132400	generative adversarial networks, outer regressive models, NLP, these are all things,
2132400	2138000	all fields, which could benefit from having more and more eyes apply categorical methods.
2138000	2144160	I think we're at a point where there's so much ideas and thoughts and we're trying to scale up
2144160	2151440	these processes of studying things categorically. And the only thing we're lacking is resources
2151440	2161680	and people knowing about this. So this brings us towards the end of today's lecture.
2163760	2172160	We have seen how ACT is a rising field, how we can study a number of different scientific fields
2173920	2177280	and really a number of subfields of machine learning through the same lens.
2178160	2184640	It is based upon compositionality and we've seen how it gives us this uniform description of
2184640	2192320	processes and concepts. Now, in this lecture, I didn't really go into any of the details. And as
2192320	2200480	I've said, this is contrary to how category theory is usually defined and introduced. It is very
2200480	2206320	verbose and mathematical, but once we start going into it, it becomes very hard to gain
2206320	2214080	the intuition. So what we're trying to aim for in this course is to start slow and to motivate
2214080	2220160	these examples. So if you want to learn more about how all of these concepts and systems
2220800	2230000	can be thought of in this uniform way, we invite you to check out the next week's lectures
2230000	2235440	and to gain a really more precise and concrete understanding of the things I've been saying
2235440	2247280	about here. And really, to end with this, it's something that we want to communicate that really
2247280	2253280	applying this category theory to deep learning is one part of what category theory does, but
2253280	2260320	once you start thinking about this through the categorical lens, things, all things start
2260320	2267120	looking like category theory. So this is something we hope to communicate. I've put up a number of
2267120	2271920	references and reading lists here for people to, this slides are going to be put up online,
2271920	2276080	so you'll be able to click on them and have a look at some of the interesting conceptual things,
2276080	2280640	some of these are books, some of these are lectures that might be beneficial in getting a sense
2280640	2288640	of how this works. But yeah, for now, this is the point where I will end, and I would like to
2288640	2293600	invite you. So we've had a lot of problems actually with the Google groups and people signing up.
2294320	2299440	If you still want to sign up, please do because we have opened the Zoom link of this lecture
2300320	2305760	to everyone because not many people, as I said, could join. But if you want to look at the next
2305760	2314240	lectures, please sign up on the course website and do check out Zulip, which is also linked to
2314240	2318640	course participants where you can chat about category theory, chat about all of these lectures,
2318640	2324320	ask questions. This Zulip is a part of the wider applied category theory community,
2324320	2329520	so you'll be able to see how actual categories chat and the concepts we talk about and how these
2329520	2336880	things work. Although I would warn you to thread carefully there, some of these things are very
2336880	2345520	foreign and scary looking, but hopefully by the end of this course they will be less so. So yeah,
2345520	2351600	thank you very much. Wonderful. Thank you so much, Bruno, for the great talk and the great
2351600	2358320	introduction to this vast landscape of categories, which as you mentioned, in the coming weeks we
2358320	2365040	will dive into more, starting with my own lecture next week where we'll talk about the basic objects
2365040	2371680	in category theory in a more rigorous way. There's a lot of very interesting questions in the Zoom and
2372240	2376080	I'll be acting as a sort of moderator, reading them out to you and we can do a bit of back and
2376080	2383520	forth. So the top rated question comes from Matej Zetrovic and it comes with this motivation of
2383520	2388320	you have probabilistic circuits like some product networks that are compositional with respect to
2388320	2392880	both conditions that you presented. However, they're not as good as neural networks in terms of
2392880	2398800	expressivity. So a generative neural network can make better images than the ones from a
2398800	2405120	generative some product network. So then the question is, should we invest more time into
2405120	2410480	studying a non-compositional model like neural networks and make them compositional or see how
2410480	2414720	you can scale something that's inherently compositional like a some product network? What are your thoughts
2414720	2421760	on this? Yeah, that's an interesting question and as I've mentioned in the beginning,
2421840	2429360	compositionality is a property of the model of our systems. So to me it seems like the model
2429920	2435760	that we have of these non-compositional systems might be non-compositional in nature,
2435760	2441200	which doesn't necessarily mean that the actual model is non-compositional. It might be the fact
2441200	2446960	that we don't know what the essential composable building blocks are. So I'm not sure if I can
2446960	2454560	provide a good answer to which of the things we should study. I'm very biased in towards taking
2454560	2459120	out a system that doesn't seem compositional but feels like it is and then trying to make it so,
2459120	2466800	trying to understand how information flows and what are the compositional building blocks. So
2466800	2475040	that would be my answer. All right, the second question comes from Siavash and it's a very quick
2475040	2481360	one. So what, if anything, is the difference between compositionality versus composability?
2482640	2488320	Yeah, so these are, this might be a very loosely defined term sometimes composability,
2488320	2492880	for instance. Some people use composability to mean literally that we can plug together
2492880	2500080	systems, processes, functions, but then when we plug these things, composable things together,
2500320	2506480	they, people don't often mean that we can study the behavior of the composite in terms of the
2506480	2511600	smaller constituents. But then again, I think I found people using, at least in category theory,
2511600	2518160	the concept of composable only when we can study them, when we can study the
2519760	2524240	resulting system recursively. So I think you might find like different usages in different
2524240	2529280	communities. In category theory, this is taken very seriously. So you would call things composable
2529280	2532320	only if you have some sort of guarantees like this.
2534800	2541920	Okay, with this top rated from Grigori asks, is there a particular book you would recommend for
2541920	2545520	beginners? I have a particular book I would recommend, but maybe you have one or two?
2546320	2550400	Well, maybe you can start with yours. I mean, let's start with seven sketches for sure.
2553040	2558240	Yeah, I would say that it often depends on where you're coming from. I would say seven
2558240	2563760	sketches in composition, compositionality linked here is a really good resources for
2563760	2568480	scientists, engineers, and programmers in general. If you're coming from programming,
2568480	2574800	specifically, you might want to look at category theory for programmers by Bartosz Mielewski,
2574800	2581200	that which is a great research as research aims specifically at programmers. I will actually
2581200	2588000	link in the Zulib. I specifically keep a list of best introductory category theory resources
2588000	2593520	on my GitHub. So I don't have it here, but I will add it and I will link it in the Zulib,
2593520	2598320	which includes a number of many more like blog posts, videos and books.
2599920	2605120	Wonderful. Yeah, that will be very useful, I think, for the attendees. So there is a question
2605120	2610240	from Kylan, which asks, could you explain a bit more in detail, what does this graphical
2610240	2614640	representation of your network actually bring us? For example, the diagram you showed from
2614640	2620000	Alfredo Canziani, you said it seems to be just a diagram. What does it actually help us understand?
2621840	2630880	Yeah, so this is a really good question. So I will go back to this slide. So the short answer is
2630880	2637440	it restricts how we're thinking about this and it restricts the things we can do to this diagram.
2637440	2642800	So this might sound a little bit counterintuitive because we have a language and then it's very
2642800	2651280	restrictive, but this is in fact a very useful design tool is to constrain ourselves. So I actually
2651280	2658400	had a quick chat on Twitter with Alfredo and I noticed some of the ways, some of the things,
2658400	2663040	some of the ways he uses the diagrams to wire up some things were very non-compositional because
2663040	2668000	when you plug systems together, you wouldn't get a thing of the same kind, but through category
2668080	2674800	we have found another way to plug these things together to make it compositional. So the answer is
2674800	2680640	it helps us reason about the systems and really one day I hope we can implement these things.
2682720	2686240	Because as you'll find when you think about deep learning, there's the whole theory, but the way
2686240	2692240	implement sometimes has tricks and tweaks and there's not a uniform translation of the theory
2692240	2701920	into implementation and I think my sort of very long term goal is to have a completely formal and
2701920	2707280	uniform description of these processes at a high level, but also exactly at a low level
2707280	2715680	and I think these diagrams help us show this. So maybe to emphasize when we draw these diagrams,
2715680	2721120	this is exactly how we would implement them. There's not a secret thing going on that you
2721120	2725760	have to be careful, like you can take these diagrams very, very seriously, which is not something that
2725760	2730160	can be done with informal notation. I hope that answers the question.
2731920	2737920	All right, yeah, thanks for that. If the person asking the question wants to follow up, please
2737920	2743440	feel free to. The current top rated questions and thank you for all these questions, they're really,
2743440	2746960	really interesting and they keep pouring in, which is great to get this kind of engagement.
2747600	2751920	We're obviously all happy to keep discussing even after the lecture on Zulip and otherwise.
2751920	2756960	The top question right now is from Siva and it asks, since categories and geometry have a rich
2756960	2761040	interaction, can we use category theory to understand the geometry of neural networks,
2761040	2765520	such as the geometry of its parameter space or the symmetry of space?
2767040	2772640	Well, yeah, this is another great question and I think Petar might be in a much better position
2772640	2778240	than me to answer this. I think we both believe the answer is yes, but maybe I'll leave it to Petar
2778240	2785440	here. Yeah, I mean, I'll just add that so I know that many people who are signed up to attend this
2785440	2789440	course have already some working knowledge of geometric deep learning where we use geometry
2789440	2795440	to understand neural network architectures as a covariant functions. And one thing you will see
2795440	2802560	in this course, especially in PIMS lecture, which is in week four, is that actually you can observe
2802560	2810480	equivalents as a special case of a more broader category theory concept called naturality, which
2810480	2818000	effectively makes the conditions far more relaxed. For example, you don't need all of your functions
2818000	2822000	to be symmetries to analyze such a system. They don't need to compose with each other,
2822000	2827040	inverses don't have to exist. So you can be in a way resistant even to functions that destroy
2827040	2831520	some of your data rather than just leave it exactly the same. So in a way, it's something that
2831520	2837280	encompasses equivalent functions, but then allows you to model way more interesting things than that.
2837840	2844800	And I believe this also answers the question that Freddie Minow posted on his applied category
2844800	2848880	theory, a super set of geometric deep learning. The short answer that PIMM already wrote is that
2848880	2855280	we definitely think so. And the lecture from PIMM should elaborate this connection a lot more.
2855680	2865040	Okay. So then we have an operator question from Nitin that asks, can we quantify or understand
2865040	2870480	the causality or counterfactual nature of systems if they have compositionality? Does it add some
2870480	2874880	explainability nature to the system as a whole instead of looking at the subset of components
2874880	2882160	as independent modules that are not interdependent? Oh, these are all really, really good questions.
2882240	2887760	I actually don't know what the answer to this is. So there's been some work. Well, there's been
2889040	2894640	a number of papers studying category theory and causality, but I'm actually not sure what is
2894640	2901440	the state of the art with regards to counterfactual reasoning. There is certainly lots of papers
2901440	2906480	doing this, but I'm afraid I can't give a good answer to this. We certainly hope so that
2907280	2911600	something comes out, but it's not something that I think we can substantiate just yet.
2912800	2918080	Yeah, I think maybe I'll use this opportunity to plug one of our guest lectures from Taco Cohen,
2918080	2922160	who has worked quite a bit recently on trying to use category theory to formalize
2922720	2927200	causal reasoning in machine learning models, who has this very nice position paper on it
2927200	2932320	that came on the archive recently. And he will be giving us a guest lecture on this exact topic.
2932320	2937680	So please do stick around if you're interested in applications of category theory for causality.
2937680	2942880	I had a chance to speak to Taco on this on a few occasions, and he seems quite convinced that we
2942880	2948640	need category theory to reason about causality in the right way. So it'll be very interesting to
2948640	2955040	hear his thoughts on this. The current top rated question comes from Flavio, and it says,
2955040	2959920	category theory tutorials might be easy to find. Can we get more info on the specific relation
2959920	2965760	with deep learning? I think the answer to this question is really that those connections will
2965760	2970480	happen in the coming lectures, unless Bruno, you want to add anything else on top of that.
2972240	2977360	No, no, you're right. I would add that I also have a GitHub sort of listing all the papers
2977360	2983680	in category theory in machine learning. This is precisely the GitHub that hosts the data used
2983680	2990560	to generate this graphic. So maybe I'll also link that in the zealot. So that might be a good thing
2990560	2993760	to have a look before the next week's lecture to see what has been done.
2994720	3001760	Yeah, sounds good. Yes, definitely do share that if you have a chance. Andrew, I think you have a
3001760	3007360	raised hand. Yes, I just wanted to emphasize, especially with the previous or earlier question,
3008160	3012640	that at the moment there is no book on this subject, on the connection between category
3012640	3020640	theory and deep learning. So we're going to do our best in these lectures, but it's not
3020720	3029120	so easy to give reference materials besides these papers. This is a gap we're trying to
3029120	3033840	fill with these lectures. Yeah, I think that's also a good point to have. Although the seven
3033840	3039440	sketches, if you don't count machine learning per se, is a good starting point to just understand
3039440	3044800	what all these string diagrams are like and how they connect different areas. Yeah, for programming
3044880	3055280	and CS, plenty of resources, but yeah. Okay, so let's see, what else do we have?
3056240	3062000	So there's a lot of questions floating around. I'm just trying to pick which one. Okay, yeah,
3062000	3066160	this one is an interesting one. It's a bit philosophical, so I'm curious to hear Andrew,
3066160	3070880	sorry, Bruno, what you think about it. It comes from Lucino Prince. Do you think that
3070880	3076480	composability is a true constituent of nature, or is just the limit of how much we can understand?
3078080	3087200	Oh, I love these questions. I wish I could provide a really coherent answer to this. Certainly,
3087200	3095040	it seems like everywhere we look, things are compositional. But this might be like the story
3095040	3099440	of trying to find your car keys under the lamp post because that's where the light is.
3100400	3106640	We are certainly not doing compositionality. There's so many things that seem so foreign to us,
3108000	3111680	and we just don't look there. We look at the things which are compositional.
3113440	3118240	Oh, god, yeah, I'm not sure how to answer this question. So this is a very
3120720	3126800	conservative answer. That's what I'll say. I don't know if anybody else from the team has a response to this.
3129600	3138160	I think the question caught me a bit off guard. I'll have to think about it a bit more. But
3138880	3142800	yeah, Pym, Andrew, do you guys have some thought on this question on your side?
3142800	3158960	I mean, I think there's probably evidence somewhere that we tend to at least learn
3158960	3165280	things in a compositional way. I mean, I guess whether there's some underlying
3166240	3175680	nature is a big question. It's surely a very nice way to organize existing information,
3175680	3183200	let's say, if you think about it in a compositional manner. That just allows you to reason about it
3183200	3193920	a lot more easily. Okay, so let's see. There is a lot of new questions. The current top question,
3194000	3199760	which I think hasn't already been answered. And I guess it comes from some of our more
3199760	3204800	mathematically oriented audience. Stanislav specifically asks, if we're going to talk about
3204800	3209520	two categories, should we actually then consider enriched and end categories in principle?
3211040	3217840	Yeah, so this is certainly the next step. So a lot of the things I did not mention in this
3217840	3226160	very brief lecture is enriched categories or pre categories or higher category theory. There is
3226160	3235440	certainly an abundance of theory and thoughts and expressivity in all of these more nuanced areas.
3236640	3241840	So yeah, these are certainly things to study and give us a particular flavor of category theory.
3242160	3248800	And yeah, I would invite you if you know, but I would consider these to be advanced topics for now.
3250960	3256160	Okay, yeah, I definitely agree. Let's learn how to crawl before we learn how to run. And
3256160	3260960	there is a reason why some people might perceive the content so far to be a bit slow starting.
3260960	3265520	We have a very diverse audience coming from all sorts of backgrounds and we're trying to accommodate
3265520	3272080	for all of those backgrounds appropriately. So we have one very fast-rising question from
3272080	3276640	Ewan, which asks, I'm aware there's been some research on category theory to motivate the
3276640	3282240	graph neural network design. Has any work or much work been done to use these ideas to construct
3282240	3287840	modular and composable neural networks or more interpretable representations in the spirit of
3287840	3291120	say what Chris Ola has been doing with representations as types?
3293840	3303520	Yeah, so that's something I would love to think about and work on. To my knowledge, the answer is no.
3305200	3310000	Yeah, to my knowledge, the answer is no. But it seems like there is really no obstacle to doing
3310000	3319440	so, at least no major obstacle. So yeah, yeah. All right, then another top-rated question.
3319440	3324240	Yeva asks, which kinds of categories, broadly speaking, are we going to focus on most in this
3324240	3328720	course? So what do we see to be the most interesting categories for deep learning at this time?
3330720	3338080	Right, so well, in this next week's lecture that Petter says is going to give, we're going to talk
3338080	3344000	about categories in general. But the one after this, we're going to talk about, so categories
3345120	3351280	allow us to compose processes in a sequence, which is useful, but in a way limited, because often
3351280	3357840	in nature, we compose processes that is in parallel. So in third lecture, we're going to be studying
3357840	3364640	things called monoidal categories, where you can put processes in parallel. And very interestingly,
3364720	3369440	these are not processes where you can necessarily copy information, delete information, some
3369440	3375600	information. So we're going to add extra layers of new ones by studying things called Cartesian
3375600	3382960	categories where you can start to do all of these things. And then later, yeah, so we're going to
3382960	3391440	study, yeah, I'm not sure how to best describe it right now without going into depth, how to start
3391440	3400240	talking about sort of things used in equity variants. It's sort of the things that might
3400240	3407280	not be easy to explain right now before unpacking the lectures. But we're going to study part of
3407280	3411120	the things we're studying here is not just categories, but the ways these are categories
3411120	3416080	are related and concepts build on top of them. So we're going to study functors between categories,
3416080	3420800	monads on these categories, and various these algebraic structures that allow us to describe
3420800	3426640	this wiring of processes or some structure preserving maps between them. Yeah.
3428480	3433280	All right. Thank you for that. The current talk question from Jeffrey asks,
3433280	3438160	it's potentially also a philosophical question. How do you find or how do you decide what are
3438160	3443840	the essential composable blocks of what you want to study? Yeah, I mean, I think this is
3443840	3447520	really a question that's not really specific to category theory. It's sort of
3448480	3453680	generally to science, we're trying to find building blocks and trying to find the basic
3453680	3463520	concepts. And I think this is really an art at this point. There's not, we cannot really formalize
3463520	3474880	and systemize this, the process of science yet. Okay, so I think our one hour block that you
3474880	3483280	had allocated for this lecture has just expired. And also at the same time, I actually had to
3484640	3489920	reset my Zoom, so I actually don't see many of the questions that are still in the Q&A.
3490640	3498800	So perhaps if Andrew or someone can see if there's any other big salient questions left,
3498800	3502640	otherwise I think it's okay if we continue the discussion on Zulip. We already covered
3503360	3509760	a lot of grounds. The top one is just asking if we can put up slides in advance, which I think
3509760	3516560	we can try to do, but I would depend on the speaker. Yeah, I'll definitely put my slides up
3516560	3523040	before the lecture and maybe we can start doing it more going forward. I think for the first lecture,
3523040	3528720	we're just trying to make sure everybody gets access to this Zoom properly. But going forward,
3528720	3532400	when we start to get more technical, we will definitely aim to share the slides in advance.
3532880	3549680	Okay, yeah, so I think we will leave it at that. Thank you so much for coming to our first lecture,
3549680	3555040	and we hope you enjoyed. Bruno, thank you so much also for delivering a great motivational
3556480	3561440	entrance to everything that will come next. And we hope you enjoyed it. We hope to keep
3561440	3567760	the discussion going. So if you want to join us on Zulip in the coming days and weeks and discuss
3567760	3574800	the various aspects of the course with us as we go along and materials and so on, that would be
3574800	3579520	really great. And if you have any feedback on how things have gone today and how you would like them
3579520	3586240	to go forward, please do interact with us. However, you prefer to leave us that feedback
3586240	3591040	directly or anonymously. We very much welcome any comments you have. It's a course we're actively
3591040	3597840	building together with all of you. So on that note, let's thank Bruno one more time. And yeah,
3597840	3602880	hope you enjoyed and hope you'll have a great rest of your week. I will see you in a week's time
3602880	3606960	for a discussion of fundamentals of category theory.
