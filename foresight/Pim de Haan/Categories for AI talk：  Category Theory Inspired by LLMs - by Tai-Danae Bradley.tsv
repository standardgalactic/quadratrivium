start	end	text
0	3000	Oh, sorry, yes. Is this recording?
10000	11000	Good to go.
13000	19000	Okay, so hello everyone and welcome to the third guest lecture of the cats for AI series.
19000	23000	As you might have seen on the on the email list.
24000	31000	We've finished with the main lectures and even two guest lectures and we're now transitioning in a different mode.
31000	36000	So we're going to be having an irregular schedule of guest talks.
36000	45000	And which are going to from now on be open to the public so if you've got a speaker recommendations please do not hesitate to reach out.
45000	63000	And first in this one that this open kind of scheduling is a talk from tight and a Bradley, she's going to be telling us about category theory inspired by large language models, I'm very excited about the talk so tight please take it away.
63000	69000	All right, thank you Bruno and Petra and all of the organizers for the invitation.
69000	78000	And I just want to say I think it's really exciting to see that there's so much interest in category theory outside of the field of pure mathematics.
78000	90000	And in particular in machine learning so I think that's very inspiring, I have a lot to learn from you all and so I hope that maybe I can give back something interesting in today's talk.
90000	101000	So yes, as the title suggests, I want to share with you some category theory that's come from comes from work with some collaborators and I which I'll share in a second.
101000	115000	That's inspired by large language models so I was asked to give this talk I think several months ago, and I thought it was very convenient that open AI release chat GPT, you know, like less than two weeks ago.
115000	124000	So maybe I'm sure all of you have heard about that by now and I think it's kind of nice that, you know, language model or maybe fresh and everyone's mind.
124000	132000	So, I couldn't resist. I know that you have all probably heard about chat GPT and you probably see in your own examples.
132000	144000	I can resist just sharing one that I did. So I would say, don't really pay attention to what's on this slide because it's not really in the theme of the talk.
144000	161000	I was just jumping into chat GPT, and I had a question about some physics thing that I was interested in. And so I, I gave it this, this thing called a quantum state just think of it as a vector, and I asked GPT chat GPT does this vector satisfy a certain
162000	187000	like does it satisfy a property. And I thought it was really interesting that it got the answer wrong, but it knew what math steps to take and what pieces of that math to analyze in order to reason about a yes or no answer does this thing satisfy a property.
187000	199000	So, it kind of like knew what linear algebra things to do you start with this vector and then you get a matrix and then you can put something on that matrix and then you apply this formula, and then at the end you reason about it.
199000	213000	And so I thought that was like, oh, pretty neat, even though overall it kind of got stuff wrong, and at least kind of knew what to do without being told to reason about, you know, how would one even arrive at the answer.
213000	224000	At this point, of course, there's lots of chatter and lots of buzz, you know, is this a GI, or is it garbage or whatever. And I don't really want to get too much into that.
224000	242000	I do want to kind of draw one idea to the forefront of our minds. And that's this fact that the training data for large language models is really interesting. It's just raw text.
242000	254000	And so, from a mathematician's perspective, that prompts a question like what what structure is in unstructured text.
254000	266000	That allows you to like form concepts and reason about things so that you get actual coherent text. So whether or not, you know, chat GPT got the answer right or wrong.
266000	285000	Okay, fine. But what's interesting is that there's no like textual garbage on this page right like all of these words go together and it sounds like coherent English it is coherent English, but you're just starting with, you know, raw text, or, you know, probability distributions on text
285000	307000	configurations. And you can ask the question, what mathematical framework, can we, you know, write down that describes that passage from probability distributions on text continuations to some space of meaning or to some like
308000	320000	semantic information. Like what's the math there. And, and you might ask this like, can we look inside of a transformer and like point to things and say here. Okay, so that's one thing you could do. That's not quite what I want to do.
320000	331000	But maybe what I can describe in this talk is kind of an overall category theoretical framework that maybe can provide for you some tools if you wanted to dig further.
331000	345000	So that's why the title of this talk is category theory, inspired by a large language models, as opposed to the category theory of large language models I'm not making a grandiose claim like that.
345000	359000	But it is really interesting to ask, like how far can you get with just probability distributions on texts, or text continuations. And I think that category theory provides some some good and very natural tools to kind of
359000	375000	exploring this question. So let me go to the next slide. So here's the outline for the remaining few minutes. First, I want to share with you a category of language and some advantages of that.
375000	386000	We'll also see some disadvantages and then I will describe for you an enriched category of language so you've, we've heard a lot about category theory in this series.
386000	400000	But maybe a new idea might be something called enriched category theory so I won't assume you have heard of that already. So I'll kind of explain a little bit of what that means, and then tie it back into this question that we're trying to answer in the context of language.
400000	415000	So the next thing I'm sharing today is based on a paper together with John Torella and Janice Losopoulos. It came out earlier this year, earlier in the spring and enriched category theory of language.
415000	428000	So, since this is an introductory series on category theory, what I thought I would do is just kind of give you a bird's eye view, or the highlights of some of the ideas that that we're working on.
428000	435000	And then if you're interested, I would say, okay, for more details, you know, we can chat after you can take a look at the paper.
435000	444000	Now, before all that, I want to motivate those ideas by sharing with you an analogy.
444000	461000	And there's a really curious analogy between linear algebra and category theory is simple. I think it's really interesting. And maybe it might appeal, you know, to the machine learning community since we deal with vectors and major sees a lot.
461000	470000	And I like this analogy a lot, it'll come up later but I also think it really nicely motivates the ideas that is kind of the main theme of the paper.
470000	491000	Speaking of themes, this analogy that I will share with you kind of centers around a very common theme in mathematics. And I have to credit John Torella for really emphasizing this idea because I think it just illustrates in a crystal clear way the advantages of thinking from a category theoretical
491000	500000	perspective. Okay, so before I share with you this analogy, let me inch towards it by telling you a common theme in mathematics.
500000	518000	So, it's very well known in math that if blah, something has nice structure, and I'll say that means in a second, then functions from a fixed set into that thing have nice structure to.
518000	520000	Okay, what do I mean.
520000	529000	Think about a set x there it's on the screen. Okay, a set is just like a bag of marbles or something, it has no structure.
529000	541000	So, if I have elements in a set x, I can't add them, I can't multiply them I can't combine them in any way to do anything meaningful it's just this set that has no structure.
541000	552000	There is another set that has a lot of nice structure, kind of readily available and that's the reels like, for example, there's other things you could think of but let me just think about the reels.
552000	557000	In, in our I can add things, we can multiply them.
557000	561000	We can do all sorts of things so it has nice structure.
561000	568000	Well, what is a function from a set x into the reels.
568000	577000	I think for a second, for simplicity, just suppose that x has three elements, that's a three element set.
577000	586000	So a function from a three element set into the reels is a choice of three numbers.
587000	599000	So, if I look at all functions from x into the reels, I have the collection of all triples of numbers, and that is familiar to us that's just three dimensional space.
599000	609000	In other words, what I'm getting at here is that the collection of all functions from let's say a three element set into our has nice structure.
609000	621000	I can actually organize those elements, you know the image of this function as this array suggestively, because if I have two of these functions now or two of these arrays, I can add them.
621000	629000	You just add functions point wise, which corresponds to sort of adding the entries of these vectors element wise.
629000	643000	So you can multiply a function by a scalar and get another function that just corresponds to multiplying the entries of this vector component wise, you can multiply actually function together.
643000	656000	So I'm hinting at here, which maybe you can see is that functions from x into our actually form of vector space. We usually denote that vector space by our to the three, our three.
657000	675000	I could write this as our x our race to the power of x. In fact, we've seen this notation earlier, when Petter talked about exponential objects. So this is the set of all functions from a set into our is not just this that turns out to be a vector space.
675000	694000	And that's because those functions inherit inherit structure from this sort of ground field. Okay, so vector spaces or linear algebra are an example, really nice example of where, if something has nice structure, then functions into that thing also inherit nice structure.
694000	701000	And here's the key is that a very similar story holds in category theory.
702000	705000	Here's kind of the analogy I like to have in mind.
705000	715000	On the one hand, we just started with a set, and we, we said, okay, you can't really do much with the set there's no structure, it's just a collection of things.
715000	730000	Now, if someone just hands you an arbitrary category, what do you know about it. Okay, you know I have some objects and some morphisms between those objects and composition and identity morphisms but that's it.
730000	742000	You don't know if C is just an arbitrary category, if you can combine objects to get a new object in your category.
742000	750000	You have other structure looking looking about maybe in specific categories you do but in general just the definition doesn't give you that.
750000	756000	So, what you may like to do, just like in the case of sets.
756000	767000	There, we thought of a specific set, like the reels that had a lot of structure, and we looked at functions into that.
767000	783000	And obviously, it can be very fruitful to think about a category that has a lot of structure that's really nice, and look at functors into that category.
783000	796000	So what category is that well it turns out that the category of sets is a really good choice. So analogous to looking from functions at functions from a set into the reels.
796000	807000	Now, it turns out that looking at functors from an arbitrary category C into the category of sets is there's a lot of rich mathematics there.
807000	818000	So why is that. Let me just say just like in the reels you can add numbers you can multiply them you can combine numbers to get new numbers in the category of set you can do this also.
818000	835000	So, at the beginning of this series, Petter talked about an over fresher memory, you know you've heard of products or the Cartesian product of sets. He also mentioned the co product of sets, or you know I said the exponential object.
835000	850000	So set has this nice property that you can take a bunch of sets and combine them in some way to get a new set. Now let me just kind of pull back the curtain and speak plainly for a minute.
850000	857000	So the idea that I'm trying to mention right now are these constructions and category theory called limits and co limits.
857000	869000	What I'm saying is that set has all limits and set has all co limits. So here's kind of what that means I think some of you are have probably heard these words or you're familiar with them already.
869000	882000	But in mathematics, quite often what you want to do, you have a whole bunch of mathematical objects, maybe their sets, or maybe their groups, or maybe their topological spaces or maybe their vector spaces, and you ask,
882000	891000	is there a way that I can take this collection of objects and combine them in such a way as to get a new object in my category.
891000	903000	Now, there are sort of two typical ways that you can do this and by typical I mean these constructions are found all across the mathematical landscape.
903000	915000	And it turns out that when you kind of strip the constructions of their deep of their details and just look at the pattern. These constructions turn out to fall into two categories for lack of better word.
915000	932000	I mean that in like the English sense not the math sense. So on the one hand, you can form what's called the limit of those objects, and depending on how that's constructed these have familiar names like the intersection of two sets, or the Cartesian product of two sets,
932000	939000	or the Cartesian product of two topological spaces, direct sums of vector spaces.
939000	956000	Meets, if you have a lattice, and you can ask for the meat or, you know, sort of the minimum element, minimum element in the collection your lattice that's that's actually an example of a limit turns out greatest common divisors are also examples of these things.
956000	972000	If you have a group homomorphism and you ask for its kernel, all of these constructions on the left hand side are subsumed by one single idea and category theory they are instantiation of one single idea that's called a limit.
972000	982000	And dual to that, you know and category theory will always like stick the word co in front of something you kind of reverse the arrows and so you get dual operations.
982000	995000	On the other hand, on the right hand side, you have things like unions co products direct sums again joins in a lattice dual to greatest common divisors least common multiple and co kernels.
995000	1009000	So these are all examples of things called limits and common limits, we won't worry about the actual definition, like they satisfy some universal property, but I mentioned that to say that in the category of set.
1009000	1024000	It has this wonderful property that you can take any collection of sets, and then asked for the limit, or asked for the co limit, and it turns out to exist. So it's another set in that category, and it satisfies universal properties.
1024000	1033000	You cannot do that with any category, any arbitrary category may or may not have that ability. It turns out the category sets does.
1033000	1048000	So that's just to suggest you that just like the real numbers was a nice set to look at functions into analogously the category of sets is a nice category to look at functors into it.
1048000	1061000	These functors because this is such a nice choice they have a name. So a functor from an arbitrary category C into the category sets. It's called a co pre sheaf.
1061000	1077000	Sometimes, you may want to look at contra variant functors from C into set. So it's just a functor where kind of flips the arrows. So in that case, folks will put a little OP above C.
1077000	1087000	They see up into set. Those are called pre sheaves. So pre sheaves. Let's just say functors.
1087000	1103000	In any case, this is a nice thing to look at it turns out that functors from C into set itself form a category so just like functions from x into our form a set with structure.
1103000	1113000	Functors from C into set form a category. It's a functor category and in fact, I think Pam talked about functor categories in his talk.
1113000	1122000	There the notation might have been square bracket from C to set, but this exponential notation is this notation for that same concept.
1122000	1130000	So it turns out objects or functors morphisms are natural transformations, but that's just kind of me drawing the analogy.
1130000	1133000	Now let me give you one more analogy.
1133000	1137000	And then we'll move on to the language.
1137000	1143000	There are very special vectors on the left hand side.
1143000	1164000	In particular, for each element in the set X, there's a very particular vector associated to the element and the analogous thing holds on the right hand side, and that vector is a one hot encoding or a basis vector.
1164000	1173000	So for every element in the set X on the left hand side, there is a particular function.
1173000	1188000	That sort of sends any other elements in the set to one, if it's the same as the element I chose or to zero otherwise, or if I write this out as an array it's just a vector with all zeros, except for one in the appropriate spot.
1188000	1203000	Analogously, there are functors from a category C into the category set, one of these code precheves that is very special.
1203000	1206000	And these are called representable functors.
1206000	1211000	So for each objects little C in my category.
1211000	1217000	There is a functor from the category into sets.
1217000	1221000	There's another object in the category D.
1221000	1232000	And it sends that object to the set of all morphisms from your given object C into D.
1232000	1250000	I've seen this notation already in this series. Sometimes this set is denoted harm from C to D. Some other people, instead of writing harm, they'll write just the name of the category C so it kind of reminds you oh morphisms in what category C.
1250000	1266000	I like to think of, these are called representable functors, these harm functors, representable functors, these are a particular kind of code precheve, or if I were to kind of move the argument to the left, it would be a precheve.
1266000	1281000	I'll try later, not now, but I like to think of representable functors as kind of analogous to these one hot encodings, because just like any other vector is built up from one hot encodings.
1281000	1289000	Similarly, any other code precheve is built up from these representables, but we'll say that later. So I like this analogy.
1289000	1306000	The reason I mentioned this is because these representable functors will play an important role in just a second. Okay, so that's my analogy for linear algebra maps into law, inherit structure from blah.
1306000	1318000	So in particular functions into the category set inherit really nice structure. Now let's see how this theme or this analogy plays out in the context of language.
1318000	1323000	So let me describe for you a category of language.
1323000	1335000	We'll see that it's nice, but like a set, you can't really do much with it. So we're going to look at functors out of that category. So let me describe that for you now is very simple.
1335000	1343000	So here's a category, consider all strings from some finite set of atomic symbols.
1343000	1356000	So if your finite set is the set of all words in English. Okay, it's a very large set but it's fine. You can just think of the take the free modeling on that set right all strings.
1356000	1371000	The sub string containment defines a preorder on this set. In other words, you know, we can make sense of when one string is contained in another string x is contained in y, and I can denote that by less than or equal to.
1371000	1375000	So I think Penn mentioned in his talk, the concept of a preorder.
1375000	1383000	So in my way of reminder, a preorder is a binary relation denoted by less than or equal to that's reflexive.
1383000	1388000	So every substring is every string is contained in itself. So X is less than X.
1388000	1390000	And that's transitive.
1390000	1403000	Right if blue on my screen is contained in small blue and small blue is contained in small blue marble that I know blue is contained in small blue marble. So that's transitivity.
1403000	1414000	And as Penn mentioned, every preordered set is a category reflexivity is exactly an identity morphism transitivity is exactly composition.
1414000	1419000	So I just defined for you a preordered set, aka a category.
1419000	1426000	And it just has at most one morphism between any two objects in that category. So it's very simple.
1426000	1435000	There's an arrow from X to Y whenever X is a substring of wine. Let's call this category L.
1435000	1445000	Fine, it just kind of tells you what goes with what does this expression go with this expression. If the answer is yes there's an arrow. If the answer is no there's no arrow.
1445000	1462000	It just tells you what goes with what. Okay, it's nice but that's very limited. In particular, I cannot, I cannot make sense of, like what's a concept in this category, or what's the context of something or can I combine ideas to get a new idea I don't have any of that
1462000	1464000	structure here.
1464000	1477000	So this is a cue from this theme that we just spent several minutes thinking about. And what we'd like to do is now consider functors from that category into the category of set.
1477000	1491000	So this is just repeating that theme and representable functors these hum functors are particularly nice in this case. And I like to think of them as like a first approximation to the meaning of of an expression.
1491000	1497000	Let's do that. Okay, pick, pick an expression in the category like the word blue.
1497000	1512000	And let's look at the hum functor hum blue blank. Okay, where if I put in an expression X and the blank on the middle here, I get a set.
1512000	1529000	This set just tells me, is there an arrow from blue to X, or not. If there is an arrow from blue tax, then home of blue comma X is just the one point set representing that one arrow.
1529000	1550000	There's no arrow from blue tax. If blue is not contained in X, then I get the empty set. Okay, now I like this, because this representable functor sort of its pre image is the collection of all expressions that contain the word blue.
1550000	1568000	And this reminds me of the onata lemma. So the onata limit is this. I'm sure you've heard of it this famous theorem and category theory, which essentially says that a mathematical object is completely determined by the network of relationships that has with other objects in the category.
1568000	1578000	So you want to understand something important or all all important things about an object, you can look at morphisms out of that object or morphisms into it.
1578000	1594000	And here, this representable functor picks out exactly the network of ways that the word you've chosen, like blue, or whatever fits into all expressions that contain it in the category in your language.
1594000	1604000	So, you know, if you think about language a lot, then I'm sure you've heard, you know, this famous quote by John first, you show no word by the company it keeps.
1604000	1615000	I mean, this is how word embeddings work to something of the meaning of a word is sort of captured in its context. So this representable functor captures precisely that context.
1615000	1626000	So you might think of this as like capturing something of meaning but you know I put that in quotes, and I say it's a first approximation, because as you can already guess this isn't everything.
1626000	1635000	You know, there's nothing about the distributional information here. This is just kind of yes or no bare bones, but it's a good start.
1635000	1647000	So this is kind of like the native perspective. When I see representable functors, I kind of think, ah, the network of ways that that word fits into its environment so capture something of that.
1647000	1657000	So here's, you know, I'm just kind of belaboring the point. But when I see this functor, another image that I have in mind is like, it's kind of like a vector.
1657000	1675000	And I put sort of above the equal sign because this is not proper category theory. But when I think of the representable functor, I think of the vector of zeros and ones, or empty sets and one point sets right indexed by all of the expressions in the language, where
1675000	1694000	an empty set in that slot, if blue is not contained in that expression like deep red being cherries, or a one point set in that slot if blue is contained in that, in that expression, that's indexing that spot like small blue marble, beautiful blue
1694000	1706000	ocean, etc. So when I think of representable functor, I think of like a vector whose entries are empty, or, or one point set indexed by expressions in the language.
1706000	1711000	Okay, this is just like something that I have in my mind.
1711000	1724000	Now, why did we do this to make our lives more complicated? No. So as we said, when you look at functors into a category with rich structure, like set, you can do things with it.
1724000	1730000	So I was kind of hinting that these representable functors behave like building blocks.
1730000	1742000	We can actually use representable functors, or these co pre sheaves to construct new co pre sheaves, and we can do that by using the structure that is in the base category set.
1742000	1754000	So I mentioned set has all limits, all co limits. It's also Cartesian close. What that means it has something like an internal home, which if you unwind all of that.
1754000	1769000	What it suggests is that we have some kind of notion of conjunction. That's kind of like what a limit is disjunction. That's kind of like what a co limit is enclosure Cartesian closure is kind of something like implication.
1769000	1778000	So, I want to give you a concrete example how am I making these connections like why is a co limit like disjunction or or.
1778000	1781000	A concrete example of what that looks like.
1781000	1787000	So suppose I have, you know, a functor representing red.
1787000	1791000	That's like the network of ways red fits into the language.
1791000	1797000	Suppose I also have the functor representing blue. Hey, the one we just looked at.
1797000	1809000	So I'm claiming that if I have two functors, I can take what's called their co product. That's a kind of co limit, which I claim is analogous to to disjunction.
1809000	1820000	I claim it's analogous to the concept quote unquote of red or blue. So here's why I make that claim. When you when you write down the definition. Here's what you find.
1820000	1831000	So a co product means I can take two functors harm red comma blank co product together with harm blue comma blank.
1831000	1844000	That defines a new functor that sends an expression X to the union of the two sets, harm of red index and harm of blue index.
1844000	1846000	So here's what that means.
1846000	1858000	I get a new functor, a new co pre chief. And again, if I envision it is like a vector whose entries are empty set or, or something else.
1858000	1873000	What you find is that this functor is sort of supported on all texts that either contain red or contain blue or contain both of them.
1873000	1887000	It's non empty on the union of all of those sets. So, for example, deep red being cherries that contains the word red so this that I get is the one point set small blue marble that contains the word blue.
1887000	1890000	So I get the one point set.
1890000	1898000	Did you put the kettle on contains neither of those words. So I get the empty set red and blue fireworks contains both.
1898000	1916000	So I actually get the union of two one point sets or two points that. Okay, so it's not empty this functor this co product of red and blue is not empty on all sets that either contain red, sorry, all expressions that either contain red, or blue or both.
1916000	1927000	And that pairs well with this idea of union as like or when dealing with sets. So that's kind of why we think of co products is kind of like disjunction.
1927000	1935000	Okay, so there are other things you can do, you can take the product, you can do this thing called internal home which is like implication.
1935000	1944000	Writing that down gets a little bit more complicated, but there are other limits and co limits. So the point is, so here's kind of a summary of what we've done so far.
1944000	1956000	So we started with a very bare bones category L. It's like a preorder, it just tells you what goes with what. So, in that sense, you can kind of think of it as like syntax, maybe.
1956000	1958000	What goes with what. Okay.
1958000	1973000	So we took, we passed from that category to the set of the category of co pre sheaves on it. So that's what I'm kind of thinking as a synantics. So that's kind of where the meaning is in this United types sense.
1973000	1987000	So every expression on the left, who responds to a representable functor on the right so that functor just picks out the kind of context of that word or all expressions that contain that word.
1988000	1995000	This passage from left to right, this assignment, blue goes to harm blue, comma blank.
1995000	2002000	That turns out to be a functor that functor is called the United embedding. So I mentioned that in case you've heard that word.
2002000	2011000	The op that you see on the left, it just is saying that this United embedding is contra variant so versus arrows but that's kind of like bookkeeping.
2012000	2026000	So, so this is just a summary of what we've done so far. We started with like a kind of in quotation marks think of it as like a syntax category very bare bones just tells you what goes with what, not a lot of structure, like no structure.
2026000	2039000	And then if you pass to co pre sheaves or functors from that category into set, you have the ability to capture something of meaning of a word in the sense of john first, in the sense of the native lemma.
2039000	2048000	And then you actually have structure in that category, and you can start to combine things in a way that kind of feels like logic or maybe like reasoning.
2048000	2058000	I like to think of pictures so here it is, you know I start with a word and then I send it to like kind of this network of ways that it fits into the category.
2058000	2064000	This is nice, but as you can already guess, it's very limited.
2064000	2075000	It's just kind of like binary yes or no does this fit in kind of discrete. It has nothing. It knows nothing about the distributional information of language.
2075000	2091000	So what you'd really like to do what would be better is if you have like you know version 2.0, where if I'm, you know, if you can give me the fact that blue is contained in small blue, but also what's the probability of seeing that.
2091000	2107000	If I see the word blue what's the probability that you know it'll be completed by small blue marble, or, you know, whatever that probability is it's going to be higher than like I woke up and had a blue idea, just to borrow, you know, Chomsky or something.
2107000	2121000	So you'd like to really wait the arrows in your category with conditional probabilities of continuing an expression with a larger expression.
2122000	2139000	And then if you include this distributional information, then you can ask, okay, can I combine concepts there and is that kind of capturing the sort of framework of getting something like logic or reasoning from, you know, just knowing what goes with what together with the probabilities.
2139000	2150000	Now the nice thing is that category theory provides a way to do this so this is exactly what we find an enriched category theory.
2150000	2155000	So let me let me give a quick introduction to that what is an enriched category theory.
2155000	2160000	So in category theory, if you have two arrows x to y.
2160000	2170000	I'm going to give you a sum of x to y or what I'm denoting by C x to y that's the set of all morphisms from x to y. Okay, you asked that that be a set.
2170000	2177000	The point is that in enriched category theory, that may not just be a set.
2177000	2184000	It could be a set with extra structure or could not be a set.
2184000	2191000	It could be a set with extra structure will think about it. If x and y are vector spaces.
2191000	2202000	Then the set of all linear transformations from x to y is also a vector space. It's not just a set you can add linear transformations you can scale or multiply them.
2202000	2218000	The HOM set is a set with additional structure it's actually a vector space. So in that case, one says the category of vector spaces is enriched over the category of vector spaces.
2218000	2224000	So whatever your HOM objects are you say your category is enriched over that.
2224000	2234000	I just said that on the upper left hand side. If this HOM set is a vector space you say oh my category is enriched over the category of vector spaces.
2234000	2246000	If your HOM set is actually a group, you say your category is enriched over the category of groups. If it's a topological space you're enriched over topological spaces.
2246000	2257000	If it turns out on the lower left, if this turns out to be a truth value like a zero or one, your categories enriched over truth values, those turn out to be exactly pre-orders.
2257000	2262000	So those things that Pam was talking about earlier, these are actually doing enriched category theory.
2262000	2275000	What if your HOM set is just a set, then your category is enriched over a set, and you're just doing ordinary category theory. So ordinary category theories like a special case of enriched category theory.
2275000	2280000	That's something that we're most interested in for the remaining few minutes.
2280000	2292000	If this is a conditional probability in a way that I'll explain in a second, then your category is enriched over the unit interval and that's kind of what I want to focus on just quickly.
2292000	2308000	So what's the unit interval? The unit interval, think of it as a category whose objects are numbers between zero and one, and where there's an arrow from little a to little b if a is less than or equal to b.
2308000	2310000	So it's a pre-order.
2310000	2321000	Now, the unit interval viewed as a category have a lot of the same properties as the category of sets.
2321000	2332000	So what I'm kind of hinting at is that we're now going to want to look at functors into the unit interval, which will be analogous to what we just did by looking at functors into set.
2332000	2345000	So in what ways is the unit interval like the category of sets? So on the slide, I showed you, okay, on the one hand, your objects are sets or numbers.
2345000	2353000	Your morphisms are functions or this pre-order. But what else do these two categories have in common?
2353000	2368000	Well, they have a manoidal product. In other words, in the category of set, you can take the Cartesian product of two sets, you can multiply sets, and there's a unit for that multiplication, namely the one point set.
2368000	2378000	So what that means is like, you know, a set x times the one point set is just isomorphic to x. So it's a unit with respect to this multiplication.
2378000	2387000	Well, the unit interval also has a multiplication, of course, multiplication, and it has a unit with respect to that, namely the number one.
2387000	2393000	So this turns out to be a manoidal product. So both categories are manoidal categories. But wait, there's more.
2394000	2402000	We mentioned that the category of sets has, you can construct co-limits or limits.
2402000	2410000	Turns out the same thing is true in the unit interval. So when you unwind the definition, what's the limit, what's the co-limit? It turns out just to be minimum.
2410000	2419000	Even if I have a bunch of numbers in the unit interval, I can take their meat or their limit. It turns out just to be the minimum of the numbers in that set.
2419000	2423000	And then finally, co-limits turn out just to be maximum.
2423000	2429000	So, okay, both categories have all limits and co-limits, and then, you know, there's more.
2429000	2441000	You have closure. So that kind of means you have this internal harm, or like Petra was saying earlier in this series, you have these exponential objects, which are kind of defined with respect to this multiplication.
2441000	2451000	I don't want to touch into that, but I just want to kind of suggest in your mind that the unit interval can now play a similar role that the category of sets did.
2451000	2454000	So how does this play out?
2454000	2461000	Well, it turns out that we can discuss a category enriched over the unit interval.
2461000	2472000	Well, just like a category enriched over sets is an ordinary category, we have an analogous idea where we switch out sets with the unit interval.
2472000	2483000	So what is a category enriched over the unit interval, also called a zero one category? Well, just like with a normal category, you start with some objects.
2483000	2489000	Similarly, a zero one category consists of some objects.
2489000	2502000	What else do you have? Well, in category theory, for every pair of objects, X, Y, you ask for a set, hum, X, Y, or C of X, Y.
2502000	2517000	Now I'm replacing set with the unit interval. So for every pair of objects on the right hand side, little X, little Y, I asked for a hum object, which is just an element in the unit interval.
2517000	2527000	So for every pair of objects, there's a number associated with them. And I think of that as a hum number or hum object.
2527000	2543000	Okay, what else is a zero one category consists of? Well, in normal category theory, we ask for composition, aka, if I have a way to get from X to Y, and why did Z, then I can compose them and get, you know, a morphism from X to Z.
2543000	2553000	So now looked at the Cartesian product on the left hand side, we said that that's analogous to multiplication of numbers. So that's what you see on the right hand side.
2553000	2571000	And just like morphisms are now this preorder, what I asked now for to have a zero one category is that there's a morphism from this hum object Y to Z times the hum object X to Y, that there's a morphism from that product to the hum object
2571000	2576000	Oh, that should say X to Z on the right hand side. That's a typo.
2576000	2593000	Okay, and then, you know, there's more, you can also ask for identities and identity morphism on an object X is just really a function from the one point set into the calm set from X to itself.
2593000	2608000	So a morphism from a one point set into a set just picks out an element in that set. So similarly, if I kind of look at the analog on the right hand side, instead of a one point set I have the number one, which plays the role of unit.
2608000	2612000	And instead of an arrow I have this less than or equal to.
2612000	2620000	And so this is the data of the zero one category and of course you ask it to satisfy some axioms.
2621000	2638000	You can also make sense once you have the notion of a, of a category enriched over the unit interval, you can discuss functors between two such categories but really it's, it's not a category in the traditional sense it's an enriched category.
2638000	2656000	So you can make sense of enriched functors. So I'll just kind of go over this briefly but you can imagine on the right hand side. It's a function from the objects of your first enriched category to the objects of the second, that satisfies some inequality that's very reminiscent
2656000	2659000	of what you might have an ordinary category theory.
2659000	2678000	For the sake of time let's let's not think too deeply about this, because the real punchline is that what happens when D, the category you're mapping into is a really nice category like the category of sets previously that gave us co pre sheaves and we saw that had a
2678000	2698000	really nice nature. So in this new kind of iteration, I want to replace set with probabilities, the zero one. And I can look at enriched co pre sheaves into the unit interval. That turns out as you may guess to form its own enriched category and you can talk about enriched natural
2698000	2701000	transformations and make sense of all this.
2701000	2713000	So let's not, you won't worry about them too much but that's kind of the idea. I just want to repeat the same story that we just did, but now I replaced the category of sets with the unit interval.
2713000	2726000	So, I'll just kind of summarize and give you the punchline here's what happens when you do that. So here's now, you know, semantics 2.0 or syntax 2.0 or language as an enriched category.
2726000	2740000	Like earlier I had a category with an arrow. If one expression is contained in the other. I'm going to now have the exact same thing and do exactly what I said I wish we could do, namely, decorate it with a conditional probability.
2740000	2755000	The punchline is that fits exactly into this framework of enriched category theory. So language is a zero one category. Its objects are strings of symbols from some atomic set of symbols like before.
2755000	2771000	The ham objects, the ham object between an expression X and Y will say it's the conditional probability that why this larger string is an extension of X or contains X.
2771000	2775000	You know if X is it contained in it or it's zero otherwise.
2775000	2789000	So you can check this actually defines an enriched category over zero one, you know, this sort of reflexivity or identity. Yeah, what's the probability that blue is contained in blue is one.
2789000	2796000	And then when you write down what this means it turns out that conditional probabilities multiply in exactly what you need to get this composition.
2796000	2804000	So all I'm saying here is that if you want to decorate these arrows with these conditional probabilities of continuation.
2804000	2812000	It's like your dreams are fulfilled. Oh, wow, enriched category theory says that this is, you know, something you can actually do formally.
2812000	2824000	As you may guess this is nice but it's limited as before. I can't combine anything, you know, there's no notion of concepts or combining things and having some kind of logical reasoning in it.
2824000	2833000	So again, you want to then pass from from that category to zero one filters on that category.
2833000	2849000	And so when you do this, you will find that representable functors like before are sort of have the right support. So as an example on this slide, you can look at the enriched representable functor of the word blue.
2849000	2858000	And I like to think of it as like this vector where they're which again is indexed by expressions in the language where there is a zero.
2858000	2873000	If that expression does not contain the word, the chosen word like blue, or it has this conditional probability, you know the conditional probability of seeing small blue marble given that you've seen blue maybe that's like point to two.
2873000	2878000	So it has the same support as before but now it has this distributional information.
2878000	2894000	And you can keep on going, you know you can combine these things. So it turns out that an enriched category theory, the appropriate notion of co products products, or more generally limits and co limits they're called weighted co limits.
2894000	2909000	So that definition gets a little bit technical. But when you take, when you unwind that definition, you can sort of ask, hey, what's this enriched notion of red or blue, the sort of concept, if I take the co product of these two co pre sheaves.
2909000	2918000	And if you choose the, these weights in the right way you basically get like that the point wise maximum of the two values individually.
2918000	2926000	So I think that's interesting and you can kind of try to think about what that what does that mean, like if you were to interpret that.
2926000	2933000	I'll let you partner that and since I'm running short on time. Let me just say there's a lot more that you can do here.
2933000	2946000	So you may have already thought about the fact that you know you can get from the unit interval into the non negative extended reels using negative log, you know there's there's an isomorphism actually between them.
2946000	2963000	And what that means is that your category which was enriched over the unit interval can now be enriched over non negative extended reels.
2963000	2973000	And that has a name and category theory. So a category enriched over that that's a pre order so it's a category. Those are called generalized metric spaces.
2973000	2991000	And that is that you can now think about the distances between these sort of concepts or distances between these representable and rich founders. And so what happens is that in this generalized metric space, you know, expressions that are likely
2991000	3002000	are close together, like blue goes to blue marble so they're kind of close in this generalized metric. But then those which are not likely extensions are kind of infinitely far away.
3002000	3019000	So maybe, you know, a sweet blue scent is not a thing that people say, or it has a low probability, because colors don't smell, or have smells. And so that kind of pairs well with your intuition that like hey that should be way far away.
3019000	3028000	You can think about distances now, you have these, you know, abilities to can to combine concepts we just looked at co products but there's so much more that you can do.
3028000	3039000	It turns out that this has nice connections to even tropical geometry. And so you on a spliceopolis might one of our co authors has really nice ideas about that, and lots more.
3039000	3044000	So all that to say, kind of wrapping up.
3044000	3061000	There's a lot that you can do by repeating this theme of starting with something that doesn't seem to have a lot of structure, like on the left hand side, and then looking at maps or functors into another category that has a lot of structure.
3061000	3070000	And there you you have the ability to form concepts to talk about distances to think about tropical geometry, which you cannot do on the left hand side.
3070000	3073000	So this all has an enriched story.
3073000	3079000	And maybe I will just kind of leave you with this teaser.
3079000	3086000	Everything I've described today, kind of rested on this analogy right between linear algebra and category theory.
3086000	3097000	So we said that this co appreciate categories like a vector space. I hinted that these representable functors are like one hot encodings.
3097000	3111000	And I told you I tell you why I think about that to think about it that way. So, just like every vector is a linear combination of these one hot encodings right you take your combination there like a basis.
3111000	3115000	So it turns out in category theory there's a theorem.
3115000	3123000	If you were to look so Emily real has a beautiful book on category theory, category theory in context, I think she calls this the density theorem.
3123000	3139000	And it turns out that every co pre chief is a co limit of these representable functors. So just like every vector is built up from basis vectors, every functor from your category in this set is built up from these home functors.
3139000	3142000	Okay, so they're kind of like a basis in that sense.
3142000	3151000	And then the analogy goes on and I will just end here because I think this is really fascinating. And I don't think it's more than an analogy which is kind of puzzling.
3151000	3169000	In linear algebra you have matrices in category theory you have pro functors. And it's basically, you know, if you write it down it's like the same thing as a matrix for every pair of objects and two categories you get a set a matrix for every two elements and a pair of, you know, a product of
3170000	3185000	matrixes can be multiplied and you know the formula for that. So in category theory pro functors can be composed, and the formula for that which is a kind of co limit looks a lot like matrix multiplication.
3186000	3206000	Interesting. Well, you know, in linear algebra, every matrix you can compute its SBD and get singular vectors, it turns out in category theory, every pro functor has something like singular vectors which is called the nuclear pro functor and in fact the way that you construct it is like line by line
3206000	3214000	is analogous to how you compute the SBD. I think what's really interesting is that as far as I know this is just an analogy.
3214000	3224000	And not like linear algebra is not a special case of category theory in this sense, but it's really curious that you can do things in linear algebra that we know and love.
3225000	3250000	So why do I say that because I think that that analogy sort of is encouraging that category theory can be a very natural and beneficial environment in which to kind of understand maybe what's going on with large language models, when all they have to work on our, you know,
3251000	3258000	distribution on text or probability distributions on text. And as we've seen, you can get quite far, at least from a mathematical perspective.
3258000	3269000	So, thank you for your attention. And if you're interested in learning more than the paper is available online. So thanks everyone.
3269000	3279000	Thank you so much, Tai. This was a wonderful talk that I think I'm going to have to keep digesting. I absolutely love the analogies.
3279000	3294000	I'm just going to open up the questions right now. So if you have a question, feel free to post it in the Q&A, and we can read them out. Alternatively, you can also raise your hand and we can just unmute you.
3294000	3305000	So while we collect the questions, it sounds like Petar has a question and I don't know if I should read Petar's question or Petardy, maybe you want to say it yourself.
3305000	3322000	Yeah, I'm happy to post it myself. First of all, thanks so much for such a wonderful talk. I really enjoyed all of the connections and it really brought, I guess, a brand new set of goggles with which I can view all these large language models in a bit cleaner way.
3322000	3337000	I had a question which it might be maybe trivially contained in what you already described, but, you know, while I think this theory is a nice way of explaining, you know, which word follows the next word and the similarity of sentences and stuff like that, which is what language models do in principle.
3337000	3352000	It also seems like today you get a large difference between success and failure with these language models, depending on how much you hack the prompts. Like, there was this one paper that said you can just add let's think step by step and it suddenly improves your reasoning capabilities by a whole lot.
3352000	3361000	I'm curious if you think your theory has an answer to why this happens or could have an answer for why this happens in the future. Yeah, I'm just really curious about that. Thank you.
3361000	3366000	Yeah, that's such a great question. So I don't know yet.
3366000	3380000	I don't know. But I think, I think it would take more investigating I think the kind of punchline that I wanted to convey is that maybe this could be a good direction to look in.
3381000	3397000	If you can kind of have a mathematical framework where you can start to see how concepts combine, where you can start to see something like logical structure emerging, then, and if you kind of explore that a little bit more, then maybe you can start to
3397000	3412000	pin down some tools that will then allow you to ask these kind of like interpretability questions are like, why is it that if I say, hey, can, you know, GBT and step by step, can you. Okay, but maybe now that there are tools that can allow you to do that and maybe it has to do something with
3412000	3427000	the sort of structure, you know, one thing that I didn't say, but this co pre chief category set. See, that's an example of a topos, and a topos is known in mathematics as a good place to do logic.
3427000	3442000	So that's encouraging from that perspective. I think it's much too early for me to give you like a definitive yes or no answer. But I think from a mathematical perspective it's promising because you have a lot of tools to kind of reason it exactly about these things that that are really good questions.
3442000	3446000	So it's too early to say but maybe I could say it's helpful.
3446000	3454000	Yeah, no worries, I just wanted to, I wanted to prompt you and see what happens. So, yeah.
3455000	3463000	Thank you. Thank you so much.
3463000	3473000	So there's been a, there's been a question during your one hot encoding slide from Jules about.
3473000	3485000	So, if you go back to the, it's there. Right. It was the in general. So I think it was the slide after actually in general the star could be any set right, not just an integer.
3485000	3492000	I think this, this might have been, I think this might have been remarking on the fact that one hot encoding.
3492000	3498000	Well, maybe Jules wants to wants to elaborate on the question themselves.
3499000	3504000	Okay, so, so Jules is saying this was answered later.
3504000	3507000	So, so that's good.
3507000	3520000	So I might ask a question myself so I absolutely love sort of the idea of thinking about structure in this way category theory has a lot of structure and when we think about unstructured text.
3520000	3526000	It doesn't, it's just sort of strings and sort of connecting in this way is absolutely fascinating.
3526000	3544000	What I'm curious about is, have you thought about connecting what you just said in this talk with with this other ways of structuring thanks text namely parts of speech, and sort of the kinds of parsing where I get a tree like structure of a sentence could
3544000	3553000	they perhaps see from the network of relationship that some things are adjectives and others are perhaps nouns.
3553000	3559000	Yeah, so that's a great question. Yes, you will notice I mentioned nothing of parts of speech.
3559000	3569000	On the one hand that was done intentionally since you know, GPT is training data is just raw text and one doesn't have to tag parts of speech.
3569000	3579000	Yeah, once you have this framework, you can ask, like, can you know, chat GPT give me some examples of adjectives.
3579000	3587000	Give me some examples of nouns I haven't tried that yet actually someone should try it or maybe I'll try it after this and see if it does it correctly. So yeah, does it learn things like that.
3587000	3604000	So I think, like my answer to Petter, it's hopeful. In fact, there's a philosopher that also we're working with Juan Luis Gastaldi who has some very good ideas in this direction. So I would say, stay tuned.
3604000	3618000	And let's see right now it's again, kind of too early but these are things that we're definitely thinking about and looking to go in that direction and some some folks even in our research circle have ideas but but still work in progress.
3618000	3620000	But yeah, that's a really great question.
3620000	3633000	Thank you. So we have a question from Tali. The question is, is there an analogy between matrix algebra and profanctors. Sorry, if there if there is an analogy between matrix algebra and profanctors.
3633000	3638000	What would be the categorical analog of higher order arrays or tensor networks.
3638000	3653000	Yeah, that's a fantastic question. So just like so a higher, so higher order array. So matrix is a function function. Let me just blow a matrix is a function from a product of two sets.
3653000	3663000	So a tensor of order three would be a function from a product of three sets, a tensor of order 10 would be a function from a product of forces.
3663000	3678000	So just like you can take, take the product of more than two sets. You can also look at functors from a product of more than two categories. So profanctors also have higher, you know, higher order array analogs.
3678000	3690000	And that's easy to write down. So, yes, so these things called tensor, you know, higher order tensors they have analogs and category and they're just kind of straightforward generalization.
3691000	3698000	So we have raised hand from Pym.
3698000	3704000	Hi, thanks for the very interesting talk I have a very pedestrian question sorry for that.
3704000	3712000	At some point you talked about the composition of like in the 01 category of the syntax.
3712000	3717000	Could you go to that slide perhaps, because I kind of.
3717000	3727000	Yeah, here. So here we have like why given X and Z given Y is Z given X.
3727000	3737000	Normally, I would guess you have some sort of a sum over why there. How does that work.
3737000	3758000	Yeah, so here there's no sum. I'm so when I look on the right side my mouse for some reason is not appearing on my slide but when I look at the arrow from blue to small blue, think of point to two is the probability of small blue conditioned on blue.
3758000	3764000	So the probability of seeing small blue, given that I have just seek for blue.
3764000	3769000	And there's only one way of getting there. So, okay, okay, I see.
3769000	3772000	Good. Thank you for asking for clarification. Good.
3772000	3782000	Okay, and this equality because typically I guess for this category wouldn't actually need equality here, right. You would. Yeah, so we just have an inequality would suffice.
3782000	3791000	Yeah, okay. Yeah, and any quality. Actually, both of those equalities like you just need any quality, but we happen to get equality here.
3791000	3793000	Okay, yeah, okay, thanks.
3793000	3798000	Yeah, thank you.
3798000	3812000	So, by the way, here in Glasgow we have a big watch party watching your talk ties so there's Mateo capuchy in the room with me who's going to come here and he is interested in asking a question.
3812000	3817000	Oh, fantastic. I didn't know there was a watch party. Hi.
3817000	3827000	I had just like, so I know about this co cat, which is another categorical framework for doing natural language processing and I'm wondering what's the relation.
3827000	3835000	So, um, disco cat I think is inherently looking at connections with quantum physics.
3835000	3850000	So I have not mentioned anything about quantum here. And also, the question that I'm trying to answer is a little bit different. So here I'm trying to see how can I start with probability distributions on texts.
3850000	3863000	And how can I pass from that into something that feels like meaning or semantic or something that has something about knowledge or reasoning, you know, inspired by large language models.
3863000	3868000	So the question that I'm starting with is a little bit different.
3868000	3875000	And the tools or the sort of assumptions that I'm making like I haven't said anything about quantum physics or
3875000	3894000	math. I'm also here. I mean, I could do this but I haven't. I get asked for a representation of all of this information. So I'm just working with categories right but you know if I look at a transformer there's no like category written down in the paper, you know, attention is all you need.
3894000	3907000	So I could ask, Okay, now that I have all of this mathematical structure, can I represent it when your altruba is a nice way to represent things. So can I, you know, represent them by vector spaces.
3907000	3920000	Could those vector spaces then be tagged with parts of speech which is kind of like what's happening in disco cat. Can I then like making a comparison with similar structure that appears in quantum. So I'm not doing anything like that.
3920000	3937000	I could want to. I mean, I could, and I have a paper on this with the honest that came out I think last year. So you could ask for a representation of this kind of information this category theoretical information in terms of linear algebra.
3937000	3956000	We think that to actually piggyback on someone else's question tensor networks are are very good choice for that. But even then sort of our premise for choosing that is a little bit different. So it turns out that in, you know, condensed matter physics and quantum
3956000	3965000	physics, they have very nice tools that happen to kind of match with the statistics of language.
3965000	3980000	But those tools, you know, can be used outside of the physics context, even though historically they've been used there. So even then in that work that we're doing we're not really saying anything like languages quantum or you know entanglement means this nothing, nothing like that.
3980000	3992000	I think that disco cat is quite different from this, even though we both happen to be, you know, thinking about language in terms of category but the questions we're answering are for different the tools are different.
3992000	3999000	The sort of premise or the reason why using those tools are different. Yeah, that's kind of my high level answer to that.
3999000	4001000	Thank you very much.
4001000	4005000	Thank you for the question.
4005000	4011000	I think this might be a good, good place to stop.
4011000	4019000	We're a bit over time, but this was absolutely fantastic. So I'll just say my thanks one more time.
4019000	4030000	As now we're basically since we're done with the main part of the course really what we're now in cats for AI as I've mentioned very old, the things we're going to be having our guest lectures.
4030000	4042000	So this is going to be on a regular schedule and if there's any recommendations you have from people who would love to talk or who you think could contribute meaningfully here would love to hear about it.
4042000	4058000	So far we have two future lectures scheduled, which are which are going to be sometime in March and we're going to update you on it as we learn more. I don't know if there's anybody else from the organizing team that has to say anything.
4058000	4068000	Other than one of those talks will be by David Spivak, so you should definitely not miss it.
4068000	4071000	Okay, so that's it. Thank you very much.
4071000	4074000	And see you next time.
4074000	4077000	Thank you all for the invitation for your time. Appreciate it.
4077000	4080000	Thanks so much for coming time. Really enjoyed it.
