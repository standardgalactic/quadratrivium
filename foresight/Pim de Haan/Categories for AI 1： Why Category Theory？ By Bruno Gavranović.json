{"text": " Hello, everyone. I would like to welcome you to our course categories for AI. In the following weeks, we'll discuss the exciting and rising role that category theory has in deep learning. It's something incredibly interesting, and we're happy to be able to share this with you. My name is Brona Gavranovic. I'm a PhD student at the University of Strathclyde, and I'm bringing this course to you together with the rest of the organizing team, Andrew, Joao, Pim and Petar. We are all in some way affiliated with deep learning category theory, or both, and interested in talking about technologies that will benefit us all, and we'll all want to use. So the first question we want to answer is, why categories for AI? The reason is simple. We believe in a future where all deep learning experts will use some aspects of category theory in their work. Now, deep learning is a new field, and as it's often the case with new fields, with new scientific fields, they start in an ad hoc manner, and then later these fields are understood differently than they were by their early practitioners. So for example, in taxonomy, people have been grouping plants and animals for thousands of years, but the way we understood what we were doing changed a lot in light of evolution and molecular biology. In chemistry, we have explored chemical reactions for a long time, but our understanding changed a lot with the discovery of the atom. And then in programming, people started to program by tinkering with transistors and logic gates, but most of what we call programming is heavily abstracted from that. And then now we have deep learning. Deep learning, despite its remarkable success, is a field permeated by ad hoc design choices. So as we all know, neural networks, neural network architectures have all these knobs and tweaks that we can't formally justify just yet. We keep being surprised by new architectures such as GPT-3 or stable diffusion, and there is no unifying framework for deep learning. There is no unifying framework that would explain the probabilistic perspective, the neuroscience perspective, and merely just the gradient-based iterative updating perspective. In fact, in the future, we might look at deep learning very differently. And our claim is that category theory will become the unifying deep learning framework. As a matter of fact, it might become a general theory of neural network architecture, architectures, and such an essential tool for deep learning practitioners. Now, this is certainly a bold claim, but it is the one we hope to substantiate in this course. So what is category theory? If I had to describe category theory in one sentence, it would be this one. Category theory takes a bird's-eye view of mathematics. From high in the sky, details become invisible, but we can spot patterns that were impossible to detect from ground level. The famous quote accurately describes what category theory has done to mathematics. As we'll see, it's not taking a bird's-eye view of just mathematics, but it started to do that to all of science in the form of a new wave that is being called applied category theory. So before we do anything, I just want to give you a one-slide summary of what applied category theory is and what we hope to teach you in this course. Applied category theory is a particular way of structuring your knowledge. It's grounded in the idea of compositionality. It originated in pure mathematics and has since spread to numerous fields. This is a formal language that is not just a part of these many fields, but it's being used to build bridges between these fields that were previously unknown. So other than in pure mathematics where it's, which it permeates, it has been emerging across the sciences. So it's been found in physics, it's been found in chemistry, it's been found in systems theory. It is all over computer science and it is the theoretical foundation of functional programming. It has been found in game theory, in information theory, control theory, probability theory, cryptography, and many others. For us, the most relevant is deep learning where there has been a number of recent papers in the last two or three years. Now, many people haven't heard of category theory and that might be because it started being applied across the sciences only in the last five years or so. So here we see a graph of the intersection of papers in category theory and machine learning. As you can see, we are in very early stages. We have just had our fifth applied category theory conference, the only one, and the only existing applied category theory journal just published its fourth volume. So this recency of category theory might be one of the reasons it is difficult to teach. For all of its expressive power, it's an notoriously difficult subject to learn. As it originated in abstract mathematics, most introductory material is not aimed at the general audience of programmers or scientists in general. There are a few exceptions. So definitions in category theory are extremely information dense. For example, a monad, the concept of a monad shown on this slide has a number of components satisfying a number of rules. And then each of these components is often information dense as well, meaning category theory requires a lot of initial investment to start appreciating. But once you start learning category theory and start appreciating the definitions, you see that the definitions form an incredibly cohesive theory, really not found anywhere else in science. These interplay together at such a remarkable level, which is hard to appreciate when you're just starting to learn it. It often looks very difficult. And on a graph, the learning process looks something like this. So compared to traditional methods, the structural method of category theory takes a long time to get started. But once you do, the theory scales much better. So in some sense, category theory is a theory of how to scale up our systems. And this is something we want to teach you in this course. We want to teach you how to approach category theory, motivated with some practical examples from deep learning, and really give you a sense of the philosophy behind category theory. So this is week one. And after today's lecture, which is going to be an introductory lecture to the general thoughts of category theory, we're going to start going into the details. In week two, we will be studying essential building blocks of category theory, categories and functors. In week three, we will study how category theory can be used to describe back propagation using monoidal categories, admitting a visual and intuitive graphical language. In week four, we'll study how geometric deep learning and naturality and have their foundations in category theory. We're going to study natural graph networks. And lastly, in week four, we will see how monoids, monads, and various algebraic structures can be connected to recurrent neural networks and LSTMs. After our five weeks of lectures, we have a series of talks by people who are in the industry doing category theory deep learning or both. And we are very excited about those. But yes, in today's lecture, we're going to start by studying what is compositionality, what is category theory really, what you need to start learning and taking advantage of it, perhaps the most important thing. And we're going to take a look at what category theory has done and can do for deep learning. So yeah, let's get started. So what is compositionality? This is a concept I've mentioned, which is central to category theory. And it's a concept that's often misunderstood. It's often presented as an ability to build systems together by composing them out of smaller subsystems. Now, this is certainly a component of compositionality, but this is not all. Compositionality includes the ability to build the systems, but we don't just want to build a very complex system that we can't reason about. It includes the ability to reason about the resulting system recursively in terms of its components. And really, compositionality is more of a property of the model of our system than the system itself. So we're going to see what that means. Compositionality requires both of these things. And for many models of our systems that are found throughout nature, we have the first property that we can build the system, but not the second one that we can reason about it. So for example, when we study behaviors of markets, of organizations, of economy, of neural networks, and many other concepts, these things aren't compositional. Now, what does that mean? These are very fuzzily defined concepts, but they're precisely very fuzzily defined only because we don't know how to reason about them from outside. We don't know what are the fundamental building blocks, and we don't know how to reason about the behavior of, say, the global economy by studying behavior of economies of individual countries. There's many emergent effects that are happening and that are hard to track. On the other hand, there are many systems, many models of our systems that have both one and two. For instance, and these are often very simple systems. We think of them as simple because we understand them. So for example, if we have two differentiable functions, we can put them together and get a composite differentiable functions by using the chain rule. If we have a compiler from a language A to language B, and a compiler from language B to language C, we get a joint compiler that compiles all the way through. We can compose various things like Markov kernels. We can compose merely polynomials and so on. There's many kinds of systems. Now, this slide, if you're seeing this, this might ring some alarm bells. It might seem like I've said something inconsistent here. It might seem like we are saying this. So it certainly looks like I said differentiable functions like compositional. And I said neural networks are made out of differentiable functions. Therefore, neural networks are compositional, but I said they're not. So this meme might look like what we're saying, but really what we're saying is the following. Compositional with respect to what. So I said compositionality is a property of our models, and we have to specify what our model is, what it is that we're studying. It's important to specify the property of the system we want to model. So if our model treats neural networks as differentiable functions, then, indeed, neural networks are compositional. We plug together a bunch of differentiable functions, and using automatic differentiation, we can compute, or chain rule, really, we can compute the derivative of the composite. But if our model treats neural networks as generative or discriminative models, then this is not compositional. It's important to specify what property we're studying and what exactly are we modeling. Really, compositionality is about interfaces. If you think about a system composed of many smaller subsystems, and the simplest case here is that of a function. When we compose these systems, we want to ensure that all the data we need is available at the exposed interfaces. So really, if we have three functions like this, the famous property of associativity tells us that if we compose f and g, and then with h, that should be the same as composing g with h and then with f. So this famous property posits that function composition should be associated. But why is this the property that you might want to have? Because it allows us to treat functions extensionally, only by looking at their interfaces. And if these composites are the same, then function composition is associative. And we can look at only the input and output of the system to know how it behaves. And we don't need to know anything about its internals or how it was built. But if this property isn't satisfied, then we might be in trouble. Non-compositionality implies that we need extra data to reason about the system, data that isn't available through the interfaces. But if our method of interaction is only through the interfaces, which is the intended way of interacting, then this means that we have uncertainty about how the system will behave. And then composing many such systems together, our uncertainty can only grow. Really, what we want to is minimize this uncertainty by imposing some invariances of how we put systems together. So that's when we create a bigger system, we know something about how it behaves. So really, compositionality is a very delicate property. And I'm a fan of this quote on the bottom of this slide, which tells us that it is so powerful that it is worth going to extreme lengths to achieve. So this is what category theory is. It's the study of compositionality, of how we can put systems together. And to my surprise, when I started to learn it, it's not really just about functions. Functions were an example here that is simple. But category theory studies all sorts of complex systems, from trees on the top left, to networks on the top right, to circuits on the bottom left, to bi-directional transformations on the bottom right. So with this in mind, we can start describing what category theory is. Even though we're talking about all sorts of abstract systems and gadgets, category theory is still a precise mathematical language to talk about it. It's the kind of language that emphasizes relationships between concepts as opposed to concepts themselves. And we're going to see what that means shortly. And we're going to see that particular structures in category theory have visual representations that aren't just doodles or sketches. They are former representations that can be manipulated, even in a computer. So to give you a sense of what this really is, I'm going to have a very short demo where I'm going to just draw a few things. So I'm going to switch to my iPad and just give you a sense of how these things work. So I said category theory is a unifying language for mathematics. And to appreciate what this means, we can perhaps focus on some subfield of mathematics, like say group theory. So in group theory, we might want to study a particular group. And a central concept in group theory is that of a group homomorphism. If you have two groups, we can have a structure preserving a mapping between them. And then we can have many mappings and they can go between groups themselves and so on. And there could be many groups that we want to study. Maybe some of them are not connected and so on. And we can study about various properties of these groups and study how they behave. Now, separately, in set theory, we might have some set and we might study functions between set one and set two. These are really honest to God functions that we can compose. And maybe there's S3, S4. And there's various kinds of structures that we can study between them. It's a very rich and intricate field. And I'm going to write here set and I'm going to write here group. Then again, if we're studying, for instance, we might study vector spaces or similar things. So for instance, I have a vector space one and a vector space two. I can still study some sort of structure preserving mapping between these, between these other structures. And for instance, this could be just merely vector spaces in a field R. Now, what category theory does is gives us this bird eye view of these things. All of these structures are categories. So here we have a category of groups. Here we have a category of sets. And here we have a category of vector spaces. And all of these structures are special examples of category teams. The same way we have studied structure preserving maps between groups, group homomorphisms, by formulating all the groups as a category. Now we can study structure preserving maps of these categories, which are called functors. So for instance, we might have a functor from the category of groups to the category of sets, which takes a group and gives us the underlying set. Or we might have a functor that takes a set and gives us the free vector space on a set and so on. The idea being that once we have lifted ourselves to this abstract level, a lot of new things open up to us. So what we end up studying is a lot of interesting things like the category of, let's say, let's call them systems with a particular interface, input and output. And we can have a system with internal state S1 and S2. And we can say the ways these are related. And then we can study categories of systems with a different interface and map between them and so on. And these systems can be actual processes, computations that do something very interesting. That isn't just sort of what you might think of as strictly mathematical. And lastly, we might study categories. These categories might have a lot of interesting structure. For example, I've drawn here a monoidal category where we can sort of put two objects in parallel, as opposed to just composing maps sequentially. And then in category theory where it turns out that these monoidal categories, that's what they're called, where we can put things in parallel, have a formal visual representation. So the story is a bit more intricate, but you can think of it as taking this category and mapping it into a visual space where each morphism has a specific shape. And you can draw it as boxes. So now these boxes aren't just sketches and doodles, but they're formal mathematical objects. So let me just switch to my other screen. Right, so this was merely aimed to paint a picture of how these things look. And finishing with the monoidal categories in this demo, these allow us to have a very natural visual language for talking about systems and processes. And these are really, this is an example of what might, of a particular process of making a pie. And we've all seen these things. So this is not something new. But it allows us, and it gives us a very different perspective on these things. So when you're doing category theory, you might be studying various sorts of systems and processes that do various sorts of things. And in category theory, there's concepts discovered or invented by various people for studying these things. So if we're studying resources or processes in this way, we might use monoidal categories. If we want to study probability, for instance, we will use Markov categories. So for instance, on the left side, here we see a process which takes an input, applies a function to it, and then copies the result. On the right, we first copy the result and then apply f individually on inputs. Now, if f is merely a function, then these two processes are equal. And in many other cases, they're equal. But not all of them. If f is a stochastic function, then these are not equal because rolling a dice and then copying the result doesn't give us the same result as rolling two dice. So Markov categories allow us to describe such processes at the needed level of generality. Then we might, if you want to study how local systems, how behavior of local systems gives rise to the behavior of a global system, you might want to use sheeps. Then again, if you want to study bi-directional transformations, such as extracting a row from a database, then updating back, so forward and backward. Or if you want to study neural networks, a forward pass and a backward pass, we might want to use optics and lenses, which are bi-directional data structures. Then again, if you want to study contextual computation or computation with side effects, we might want to use things called monads or their Kleisler categories. And of course, in computer science, the concept permeating it is recursion. And then we might want to use things called final co-algebras or initial algebras to study recursive processes. And once we study all of these things, we might find ourselves in this situation where we study a bunch of different kinds of systems with different kinds of categories and concepts. And that ends up just creating more categories and more concepts. And the theory that studies category theory is called two-category theory. So something I love about the way you approach CT, that it is very meta and recursive in nature, allowing us to describe category theory using category theory. And then really, once you start going with this, you might have heard about classical mathematical structures such as topological spaces or rings or fields. And once you start going into category theory, it's hard to appreciate how deep and rich the field is, because the water keeps getting deeper and deeper and the rabbit hole hides so many concepts that I personally wasn't aware of that have a very expressive nature and a very compositional nature in helping us understand all of the systems. This is certainly not the end. There's many more that every category theorist has a certain understanding of a part of these concepts. So this was very abstract in a sense, but I hope it conveyed some sense of what category theories are. But now I would like to make this a bit more concrete and study what is the relationship between category theory and deep learning? What has been done? How much are these concepts actually connected to each other? So the thing I want to establish first is that the deep learning community seems to be very well aware of the concept of compositionality. I have found numerous workshops, programs, blog posts, lectures saying compositionality is important. And I've linked a few of them here. I'm sure there's many more. Joshua Bengio in his Turing lecture actually explicitly states that we need to build compositionality into our machine learning models. But having studied category theory, I'm still noticing that compositionality is done in ad hoc ways and that there is a large gap between how compositionality is thought of in deep learning and the theory of compositionality that category theory offers. Nonetheless, there have been some recent strides. So I'd like to give a brief overview of what has been happening on the intersection of category theory and machine learning. So the papers I am showing here are two papers that study neural networks in the very abstract, in the form of something called a parametric lens. We're going to see exactly what that is later in the course, but I just want to give you an idea of how these things work. So it's something that has a following shape. And it abstractly models the information flow we see in a neural network. So on the left side, we see inputs and their gradients. On the right side, we see outputs of a neural network and their gradients. And here we take the two-dimensional notation seriously, and therefore we have weights coming from top, weights and their gradients. And this abstractly models how the information flows. So we can see that from left and top, we get inputs and parameters, compute and output, and then having received the gradients of the loss with respect to that output, we can back propagate the gradients with respect to the input to the left and back propagate the gradients of the loss with respect to weights to the top. Now, the question you might ask is what is the benefit of this formulation? Why would we model it like this? Well, when these constructions were originally discovered a few years ago, something fascinating happened. Independently, game theorists were modeling economic agents in game theory using these categorical models. And to the big surprise, these models ended up having the same categorical form as the machine learning ones. So if you're studying economic agents that take some inputs, produce some outputs, receive some payoff for their things and have their trying to maximize some strategy, you end up with a model that has the same shape. So independent formalizations by different people of game theory and machine learning converge to the same mathematical form. This elucidated a number of connections and really sparked a whole study of cybernetic systems or reinforcement-like agent systems, the study of agents interacting with the environment using minimal assumptions about what agents or the environment is. And really, this is something that can't be done without the level of generality that category theory provides. So I found this quite fascinating as it was being discovered. And it's one of the reasons why I, it allows me to study machine learning in game theory through a unified, unified lens. What it also turned out that why would we want to use these parametric lenses that we can also model optimizers of neural networks. So, and what it ended up turning out that optimizers of neural networks have the same shape as neural networks themselves. So on the top here, the idea is, so on the left and right, we have parameters, but on top we have the state saved by these optimizers, the stateful ones like momentum or add-on. And this gives us hints that optimizers are some kind of hardwired meta learners and just as some papers have shown. So there's lots of interesting research opening up to this. And an interesting thing from a personal standpoint happened with regards to optimizers. We started formalizing gradient descent using lenses, this concept that I mentioned that models by directionality. And for a long time actually had suspicions about this formalization. The problem was that there was a part of a lens that really wasn't used at all in gradient descent, casting reasonable doubt on whether lenses are the right abstraction. Maybe they are really an overgeneralization. But then as we went on to formalize more complicated optimizers, such as the one that used Nestor of momentum, we found that the bit that computes the look ahead in Nestor of momentum, the one that's always lost over in explanations and when you implement these things, it requires rethinking about how you structured the stuff. It had a natural place in our formulation. It was the thing that we didn't use before. So really, from a personal standpoint, it clarified how a lot of these things fit that we started using this categorical formulas. And lastly, we're not the only ones using them. So very recently, about a week ago, I found a deep learning professor at New York University, if I'm not mistaken, independently starting to use the same sort of graphical notation as the formal one we have been using. And I'm quite amazed that we were sort of having formal justification that these two notations have converged to the same representation. So yeah, this shows one line of work that has been done with machine learning, with category theory. Other people have studied different things. People have studied recurrent neural networks. And it has a very strange title here, but if you open the paper, you'll see that it does study recurrent neural networks. And in recurrent neural networks, we have the idea of back propagation through time, which is done by unrolling the recurrent neural network. Now, what this paper showed is an interesting thing that this process of back propagation through time doesn't merely use differentiation, but it is differentiation in a very sophisticated way. And it uses the formalism of something called double categories, which is yet an even more advanced but very visual concept. So this short picture shows you how you can think of each time step of recurrent neural network as a vertical kind of morphism, and you can think of layers of neural network as horizontal. And then Petar and Andrew, who are co-organizers of this course, have done a lot of work on geometric deep learning and graph neural networks. And they've established a connection between graph neural networks and dynamic programming, which is quite fascinating because this might be such very different things, thinking about how graph neural networks work and how the algorithm of, say, Bellman-Ford works. Yet again, using the abstractions of category theory, we can say something more precise about this. And generally about, yeah, in generally about all of these neural networks, this is some of the stuff that has been done. But really, there is so much more work to be done and so many more things to explore. I've just given you a glimpse of some of the interesting things, but really, transformers haven't been studied, have been studied to some small degree, but there's so much work really to do on them, through category theory. Geometric deep learning is being studied now, actually, through category theory, but then again, this is such a vast field. And the same holds for generative adversarial networks, outer regressive models, NLP, these are all things, all fields, which could benefit from having more and more eyes apply categorical methods. I think we're at a point where there's so much ideas and thoughts and we're trying to scale up these processes of studying things categorically. And the only thing we're lacking is resources and people knowing about this. So this brings us towards the end of today's lecture. We have seen how ACT is a rising field, how we can study a number of different scientific fields and really a number of subfields of machine learning through the same lens. It is based upon compositionality and we've seen how it gives us this uniform description of processes and concepts. Now, in this lecture, I didn't really go into any of the details. And as I've said, this is contrary to how category theory is usually defined and introduced. It is very verbose and mathematical, but once we start going into it, it becomes very hard to gain the intuition. So what we're trying to aim for in this course is to start slow and to motivate these examples. So if you want to learn more about how all of these concepts and systems can be thought of in this uniform way, we invite you to check out the next week's lectures and to gain a really more precise and concrete understanding of the things I've been saying about here. And really, to end with this, it's something that we want to communicate that really applying this category theory to deep learning is one part of what category theory does, but once you start thinking about this through the categorical lens, things, all things start looking like category theory. So this is something we hope to communicate. I've put up a number of references and reading lists here for people to, this slides are going to be put up online, so you'll be able to click on them and have a look at some of the interesting conceptual things, some of these are books, some of these are lectures that might be beneficial in getting a sense of how this works. But yeah, for now, this is the point where I will end, and I would like to invite you. So we've had a lot of problems actually with the Google groups and people signing up. If you still want to sign up, please do because we have opened the Zoom link of this lecture to everyone because not many people, as I said, could join. But if you want to look at the next lectures, please sign up on the course website and do check out Zulip, which is also linked to course participants where you can chat about category theory, chat about all of these lectures, ask questions. This Zulip is a part of the wider applied category theory community, so you'll be able to see how actual categories chat and the concepts we talk about and how these things work. Although I would warn you to thread carefully there, some of these things are very foreign and scary looking, but hopefully by the end of this course they will be less so. So yeah, thank you very much. Wonderful. Thank you so much, Bruno, for the great talk and the great introduction to this vast landscape of categories, which as you mentioned, in the coming weeks we will dive into more, starting with my own lecture next week where we'll talk about the basic objects in category theory in a more rigorous way. There's a lot of very interesting questions in the Zoom and I'll be acting as a sort of moderator, reading them out to you and we can do a bit of back and forth. So the top rated question comes from Matej Zetrovic and it comes with this motivation of you have probabilistic circuits like some product networks that are compositional with respect to both conditions that you presented. However, they're not as good as neural networks in terms of expressivity. So a generative neural network can make better images than the ones from a generative some product network. So then the question is, should we invest more time into studying a non-compositional model like neural networks and make them compositional or see how you can scale something that's inherently compositional like a some product network? What are your thoughts on this? Yeah, that's an interesting question and as I've mentioned in the beginning, compositionality is a property of the model of our systems. So to me it seems like the model that we have of these non-compositional systems might be non-compositional in nature, which doesn't necessarily mean that the actual model is non-compositional. It might be the fact that we don't know what the essential composable building blocks are. So I'm not sure if I can provide a good answer to which of the things we should study. I'm very biased in towards taking out a system that doesn't seem compositional but feels like it is and then trying to make it so, trying to understand how information flows and what are the compositional building blocks. So that would be my answer. All right, the second question comes from Siavash and it's a very quick one. So what, if anything, is the difference between compositionality versus composability? Yeah, so these are, this might be a very loosely defined term sometimes composability, for instance. Some people use composability to mean literally that we can plug together systems, processes, functions, but then when we plug these things, composable things together, they, people don't often mean that we can study the behavior of the composite in terms of the smaller constituents. But then again, I think I found people using, at least in category theory, the concept of composable only when we can study them, when we can study the resulting system recursively. So I think you might find like different usages in different communities. In category theory, this is taken very seriously. So you would call things composable only if you have some sort of guarantees like this. Okay, with this top rated from Grigori asks, is there a particular book you would recommend for beginners? I have a particular book I would recommend, but maybe you have one or two? Well, maybe you can start with yours. I mean, let's start with seven sketches for sure. Yeah, I would say that it often depends on where you're coming from. I would say seven sketches in composition, compositionality linked here is a really good resources for scientists, engineers, and programmers in general. If you're coming from programming, specifically, you might want to look at category theory for programmers by Bartosz Mielewski, that which is a great research as research aims specifically at programmers. I will actually link in the Zulib. I specifically keep a list of best introductory category theory resources on my GitHub. So I don't have it here, but I will add it and I will link it in the Zulib, which includes a number of many more like blog posts, videos and books. Wonderful. Yeah, that will be very useful, I think, for the attendees. So there is a question from Kylan, which asks, could you explain a bit more in detail, what does this graphical representation of your network actually bring us? For example, the diagram you showed from Alfredo Canziani, you said it seems to be just a diagram. What does it actually help us understand? Yeah, so this is a really good question. So I will go back to this slide. So the short answer is it restricts how we're thinking about this and it restricts the things we can do to this diagram. So this might sound a little bit counterintuitive because we have a language and then it's very restrictive, but this is in fact a very useful design tool is to constrain ourselves. So I actually had a quick chat on Twitter with Alfredo and I noticed some of the ways, some of the things, some of the ways he uses the diagrams to wire up some things were very non-compositional because when you plug systems together, you wouldn't get a thing of the same kind, but through category we have found another way to plug these things together to make it compositional. So the answer is it helps us reason about the systems and really one day I hope we can implement these things. Because as you'll find when you think about deep learning, there's the whole theory, but the way implement sometimes has tricks and tweaks and there's not a uniform translation of the theory into implementation and I think my sort of very long term goal is to have a completely formal and uniform description of these processes at a high level, but also exactly at a low level and I think these diagrams help us show this. So maybe to emphasize when we draw these diagrams, this is exactly how we would implement them. There's not a secret thing going on that you have to be careful, like you can take these diagrams very, very seriously, which is not something that can be done with informal notation. I hope that answers the question. All right, yeah, thanks for that. If the person asking the question wants to follow up, please feel free to. The current top rated questions and thank you for all these questions, they're really, really interesting and they keep pouring in, which is great to get this kind of engagement. We're obviously all happy to keep discussing even after the lecture on Zulip and otherwise. The top question right now is from Siva and it asks, since categories and geometry have a rich interaction, can we use category theory to understand the geometry of neural networks, such as the geometry of its parameter space or the symmetry of space? Well, yeah, this is another great question and I think Petar might be in a much better position than me to answer this. I think we both believe the answer is yes, but maybe I'll leave it to Petar here. Yeah, I mean, I'll just add that so I know that many people who are signed up to attend this course have already some working knowledge of geometric deep learning where we use geometry to understand neural network architectures as a covariant functions. And one thing you will see in this course, especially in PIMS lecture, which is in week four, is that actually you can observe equivalents as a special case of a more broader category theory concept called naturality, which effectively makes the conditions far more relaxed. For example, you don't need all of your functions to be symmetries to analyze such a system. They don't need to compose with each other, inverses don't have to exist. So you can be in a way resistant even to functions that destroy some of your data rather than just leave it exactly the same. So in a way, it's something that encompasses equivalent functions, but then allows you to model way more interesting things than that. And I believe this also answers the question that Freddie Minow posted on his applied category theory, a super set of geometric deep learning. The short answer that PIMM already wrote is that we definitely think so. And the lecture from PIMM should elaborate this connection a lot more. Okay. So then we have an operator question from Nitin that asks, can we quantify or understand the causality or counterfactual nature of systems if they have compositionality? Does it add some explainability nature to the system as a whole instead of looking at the subset of components as independent modules that are not interdependent? Oh, these are all really, really good questions. I actually don't know what the answer to this is. So there's been some work. Well, there's been a number of papers studying category theory and causality, but I'm actually not sure what is the state of the art with regards to counterfactual reasoning. There is certainly lots of papers doing this, but I'm afraid I can't give a good answer to this. We certainly hope so that something comes out, but it's not something that I think we can substantiate just yet. Yeah, I think maybe I'll use this opportunity to plug one of our guest lectures from Taco Cohen, who has worked quite a bit recently on trying to use category theory to formalize causal reasoning in machine learning models, who has this very nice position paper on it that came on the archive recently. And he will be giving us a guest lecture on this exact topic. So please do stick around if you're interested in applications of category theory for causality. I had a chance to speak to Taco on this on a few occasions, and he seems quite convinced that we need category theory to reason about causality in the right way. So it'll be very interesting to hear his thoughts on this. The current top rated question comes from Flavio, and it says, category theory tutorials might be easy to find. Can we get more info on the specific relation with deep learning? I think the answer to this question is really that those connections will happen in the coming lectures, unless Bruno, you want to add anything else on top of that. No, no, you're right. I would add that I also have a GitHub sort of listing all the papers in category theory in machine learning. This is precisely the GitHub that hosts the data used to generate this graphic. So maybe I'll also link that in the zealot. So that might be a good thing to have a look before the next week's lecture to see what has been done. Yeah, sounds good. Yes, definitely do share that if you have a chance. Andrew, I think you have a raised hand. Yes, I just wanted to emphasize, especially with the previous or earlier question, that at the moment there is no book on this subject, on the connection between category theory and deep learning. So we're going to do our best in these lectures, but it's not so easy to give reference materials besides these papers. This is a gap we're trying to fill with these lectures. Yeah, I think that's also a good point to have. Although the seven sketches, if you don't count machine learning per se, is a good starting point to just understand what all these string diagrams are like and how they connect different areas. Yeah, for programming and CS, plenty of resources, but yeah. Okay, so let's see, what else do we have? So there's a lot of questions floating around. I'm just trying to pick which one. Okay, yeah, this one is an interesting one. It's a bit philosophical, so I'm curious to hear Andrew, sorry, Bruno, what you think about it. It comes from Lucino Prince. Do you think that composability is a true constituent of nature, or is just the limit of how much we can understand? Oh, I love these questions. I wish I could provide a really coherent answer to this. Certainly, it seems like everywhere we look, things are compositional. But this might be like the story of trying to find your car keys under the lamp post because that's where the light is. We are certainly not doing compositionality. There's so many things that seem so foreign to us, and we just don't look there. We look at the things which are compositional. Oh, god, yeah, I'm not sure how to answer this question. So this is a very conservative answer. That's what I'll say. I don't know if anybody else from the team has a response to this. I think the question caught me a bit off guard. I'll have to think about it a bit more. But yeah, Pym, Andrew, do you guys have some thought on this question on your side? I mean, I think there's probably evidence somewhere that we tend to at least learn things in a compositional way. I mean, I guess whether there's some underlying nature is a big question. It's surely a very nice way to organize existing information, let's say, if you think about it in a compositional manner. That just allows you to reason about it a lot more easily. Okay, so let's see. There is a lot of new questions. The current top question, which I think hasn't already been answered. And I guess it comes from some of our more mathematically oriented audience. Stanislav specifically asks, if we're going to talk about two categories, should we actually then consider enriched and end categories in principle? Yeah, so this is certainly the next step. So a lot of the things I did not mention in this very brief lecture is enriched categories or pre categories or higher category theory. There is certainly an abundance of theory and thoughts and expressivity in all of these more nuanced areas. So yeah, these are certainly things to study and give us a particular flavor of category theory. And yeah, I would invite you if you know, but I would consider these to be advanced topics for now. Okay, yeah, I definitely agree. Let's learn how to crawl before we learn how to run. And there is a reason why some people might perceive the content so far to be a bit slow starting. We have a very diverse audience coming from all sorts of backgrounds and we're trying to accommodate for all of those backgrounds appropriately. So we have one very fast-rising question from Ewan, which asks, I'm aware there's been some research on category theory to motivate the graph neural network design. Has any work or much work been done to use these ideas to construct modular and composable neural networks or more interpretable representations in the spirit of say what Chris Ola has been doing with representations as types? Yeah, so that's something I would love to think about and work on. To my knowledge, the answer is no. Yeah, to my knowledge, the answer is no. But it seems like there is really no obstacle to doing so, at least no major obstacle. So yeah, yeah. All right, then another top-rated question. Yeva asks, which kinds of categories, broadly speaking, are we going to focus on most in this course? So what do we see to be the most interesting categories for deep learning at this time? Right, so well, in this next week's lecture that Petter says is going to give, we're going to talk about categories in general. But the one after this, we're going to talk about, so categories allow us to compose processes in a sequence, which is useful, but in a way limited, because often in nature, we compose processes that is in parallel. So in third lecture, we're going to be studying things called monoidal categories, where you can put processes in parallel. And very interestingly, these are not processes where you can necessarily copy information, delete information, some information. So we're going to add extra layers of new ones by studying things called Cartesian categories where you can start to do all of these things. And then later, yeah, so we're going to study, yeah, I'm not sure how to best describe it right now without going into depth, how to start talking about sort of things used in equity variants. It's sort of the things that might not be easy to explain right now before unpacking the lectures. But we're going to study part of the things we're studying here is not just categories, but the ways these are categories are related and concepts build on top of them. So we're going to study functors between categories, monads on these categories, and various these algebraic structures that allow us to describe this wiring of processes or some structure preserving maps between them. Yeah. All right. Thank you for that. The current talk question from Jeffrey asks, it's potentially also a philosophical question. How do you find or how do you decide what are the essential composable blocks of what you want to study? Yeah, I mean, I think this is really a question that's not really specific to category theory. It's sort of generally to science, we're trying to find building blocks and trying to find the basic concepts. And I think this is really an art at this point. There's not, we cannot really formalize and systemize this, the process of science yet. Okay, so I think our one hour block that you had allocated for this lecture has just expired. And also at the same time, I actually had to reset my Zoom, so I actually don't see many of the questions that are still in the Q&A. So perhaps if Andrew or someone can see if there's any other big salient questions left, otherwise I think it's okay if we continue the discussion on Zulip. We already covered a lot of grounds. The top one is just asking if we can put up slides in advance, which I think we can try to do, but I would depend on the speaker. Yeah, I'll definitely put my slides up before the lecture and maybe we can start doing it more going forward. I think for the first lecture, we're just trying to make sure everybody gets access to this Zoom properly. But going forward, when we start to get more technical, we will definitely aim to share the slides in advance. Okay, yeah, so I think we will leave it at that. Thank you so much for coming to our first lecture, and we hope you enjoyed. Bruno, thank you so much also for delivering a great motivational entrance to everything that will come next. And we hope you enjoyed it. We hope to keep the discussion going. So if you want to join us on Zulip in the coming days and weeks and discuss the various aspects of the course with us as we go along and materials and so on, that would be really great. And if you have any feedback on how things have gone today and how you would like them to go forward, please do interact with us. However, you prefer to leave us that feedback directly or anonymously. We very much welcome any comments you have. It's a course we're actively building together with all of you. So on that note, let's thank Bruno one more time. And yeah, hope you enjoyed and hope you'll have a great rest of your week. I will see you in a week's time for a discussion of fundamentals of category theory.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 6.08, "text": " Hello, everyone. I would like to welcome you to our course categories for AI.", "tokens": [50364, 2425, 11, 1518, 13, 286, 576, 411, 281, 2928, 291, 281, 527, 1164, 10479, 337, 7318, 13, 50668], "temperature": 0.0, "avg_logprob": -0.15474879357122606, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.03806906193494797}, {"id": 1, "seek": 0, "start": 9.040000000000001, "end": 15.6, "text": " In the following weeks, we'll discuss the exciting and rising role that category theory has in deep", "tokens": [50816, 682, 264, 3480, 3259, 11, 321, 603, 2248, 264, 4670, 293, 11636, 3090, 300, 7719, 5261, 575, 294, 2452, 51144], "temperature": 0.0, "avg_logprob": -0.15474879357122606, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.03806906193494797}, {"id": 2, "seek": 0, "start": 15.6, "end": 21.04, "text": " learning. It's something incredibly interesting, and we're happy to be able to share this with you.", "tokens": [51144, 2539, 13, 467, 311, 746, 6252, 1880, 11, 293, 321, 434, 2055, 281, 312, 1075, 281, 2073, 341, 365, 291, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15474879357122606, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.03806906193494797}, {"id": 3, "seek": 0, "start": 24.0, "end": 28.16, "text": " My name is Brona Gavranovic. I'm a PhD student at the University of Strathclyde,", "tokens": [51564, 1222, 1315, 307, 1603, 4037, 460, 706, 81, 3730, 25537, 13, 286, 478, 257, 14476, 3107, 412, 264, 3535, 295, 8251, 998, 66, 356, 1479, 11, 51772], "temperature": 0.0, "avg_logprob": -0.15474879357122606, "compression_ratio": 1.4916666666666667, "no_speech_prob": 0.03806906193494797}, {"id": 4, "seek": 2816, "start": 28.16, "end": 32.32, "text": " and I'm bringing this course to you together with the rest of the organizing team,", "tokens": [50364, 293, 286, 478, 5062, 341, 1164, 281, 291, 1214, 365, 264, 1472, 295, 264, 17608, 1469, 11, 50572], "temperature": 0.0, "avg_logprob": -0.11816666342995384, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.0031400148291140795}, {"id": 5, "seek": 2816, "start": 32.88, "end": 42.56, "text": " Andrew, Joao, Pim and Petar. We are all in some way affiliated with deep learning category theory,", "tokens": [50600, 10110, 11, 3139, 25548, 11, 430, 332, 293, 10472, 289, 13, 492, 366, 439, 294, 512, 636, 42174, 365, 2452, 2539, 7719, 5261, 11, 51084], "temperature": 0.0, "avg_logprob": -0.11816666342995384, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.0031400148291140795}, {"id": 6, "seek": 2816, "start": 42.56, "end": 48.879999999999995, "text": " or both, and interested in talking about technologies that will benefit us all,", "tokens": [51084, 420, 1293, 11, 293, 3102, 294, 1417, 466, 7943, 300, 486, 5121, 505, 439, 11, 51400], "temperature": 0.0, "avg_logprob": -0.11816666342995384, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.0031400148291140795}, {"id": 7, "seek": 2816, "start": 48.879999999999995, "end": 56.56, "text": " and we'll all want to use. So the first question we want to answer is, why categories for AI?", "tokens": [51400, 293, 321, 603, 439, 528, 281, 764, 13, 407, 264, 700, 1168, 321, 528, 281, 1867, 307, 11, 983, 10479, 337, 7318, 30, 51784], "temperature": 0.0, "avg_logprob": -0.11816666342995384, "compression_ratio": 1.5236051502145922, "no_speech_prob": 0.0031400148291140795}, {"id": 8, "seek": 5816, "start": 58.8, "end": 65.36, "text": " The reason is simple. We believe in a future where all deep learning experts will use some", "tokens": [50396, 440, 1778, 307, 2199, 13, 492, 1697, 294, 257, 2027, 689, 439, 2452, 2539, 8572, 486, 764, 512, 50724], "temperature": 0.0, "avg_logprob": -0.0846415879069895, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0035065331030637026}, {"id": 9, "seek": 5816, "start": 65.36, "end": 77.03999999999999, "text": " aspects of category theory in their work. Now, deep learning is a new field, and as it's often", "tokens": [50724, 7270, 295, 7719, 5261, 294, 641, 589, 13, 823, 11, 2452, 2539, 307, 257, 777, 2519, 11, 293, 382, 309, 311, 2049, 51308], "temperature": 0.0, "avg_logprob": -0.0846415879069895, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0035065331030637026}, {"id": 10, "seek": 5816, "start": 77.03999999999999, "end": 84.08, "text": " the case with new fields, with new scientific fields, they start in an ad hoc manner, and then", "tokens": [51308, 264, 1389, 365, 777, 7909, 11, 365, 777, 8134, 7909, 11, 436, 722, 294, 364, 614, 16708, 9060, 11, 293, 550, 51660], "temperature": 0.0, "avg_logprob": -0.0846415879069895, "compression_ratio": 1.5730337078651686, "no_speech_prob": 0.0035065331030637026}, {"id": 11, "seek": 8408, "start": 84.08, "end": 90.16, "text": " later these fields are understood differently than they were by their early practitioners.", "tokens": [50364, 1780, 613, 7909, 366, 7320, 7614, 813, 436, 645, 538, 641, 2440, 25742, 13, 50668], "temperature": 0.0, "avg_logprob": -0.04650515913963318, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0013721635332331061}, {"id": 12, "seek": 8408, "start": 91.84, "end": 97.67999999999999, "text": " So for example, in taxonomy, people have been grouping plants and animals for thousands of years,", "tokens": [50752, 407, 337, 1365, 11, 294, 3366, 23423, 11, 561, 362, 668, 40149, 5972, 293, 4882, 337, 5383, 295, 924, 11, 51044], "temperature": 0.0, "avg_logprob": -0.04650515913963318, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0013721635332331061}, {"id": 13, "seek": 8408, "start": 97.67999999999999, "end": 102.8, "text": " but the way we understood what we were doing changed a lot in light of evolution and molecular", "tokens": [51044, 457, 264, 636, 321, 7320, 437, 321, 645, 884, 3105, 257, 688, 294, 1442, 295, 9303, 293, 19046, 51300], "temperature": 0.0, "avg_logprob": -0.04650515913963318, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0013721635332331061}, {"id": 14, "seek": 8408, "start": 102.8, "end": 109.2, "text": " biology. In chemistry, we have explored chemical reactions for a long time, but our understanding", "tokens": [51300, 14956, 13, 682, 12558, 11, 321, 362, 24016, 7313, 12215, 337, 257, 938, 565, 11, 457, 527, 3701, 51620], "temperature": 0.0, "avg_logprob": -0.04650515913963318, "compression_ratio": 1.6351931330472103, "no_speech_prob": 0.0013721635332331061}, {"id": 15, "seek": 10920, "start": 109.2, "end": 116.88, "text": " changed a lot with the discovery of the atom. And then in programming, people started to program", "tokens": [50364, 3105, 257, 688, 365, 264, 12114, 295, 264, 12018, 13, 400, 550, 294, 9410, 11, 561, 1409, 281, 1461, 50748], "temperature": 0.0, "avg_logprob": -0.05291321783354788, "compression_ratio": 1.5944444444444446, "no_speech_prob": 0.0013209364842623472}, {"id": 16, "seek": 10920, "start": 116.88, "end": 123.12, "text": " by tinkering with transistors and logic gates, but most of what we call programming is heavily", "tokens": [50748, 538, 256, 475, 1794, 365, 1145, 46976, 293, 9952, 19792, 11, 457, 881, 295, 437, 321, 818, 9410, 307, 10950, 51060], "temperature": 0.0, "avg_logprob": -0.05291321783354788, "compression_ratio": 1.5944444444444446, "no_speech_prob": 0.0013209364842623472}, {"id": 17, "seek": 10920, "start": 123.12, "end": 135.04, "text": " abstracted from that. And then now we have deep learning. Deep learning, despite its remarkable", "tokens": [51060, 12649, 292, 490, 300, 13, 400, 550, 586, 321, 362, 2452, 2539, 13, 14895, 2539, 11, 7228, 1080, 12802, 51656], "temperature": 0.0, "avg_logprob": -0.05291321783354788, "compression_ratio": 1.5944444444444446, "no_speech_prob": 0.0013209364842623472}, {"id": 18, "seek": 13504, "start": 135.04, "end": 142.48, "text": " success, is a field permeated by ad hoc design choices. So as we all know, neural networks,", "tokens": [50364, 2245, 11, 307, 257, 2519, 30287, 770, 538, 614, 16708, 1715, 7994, 13, 407, 382, 321, 439, 458, 11, 18161, 9590, 11, 50736], "temperature": 0.0, "avg_logprob": -0.09810181067023478, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.002961066784337163}, {"id": 19, "seek": 13504, "start": 142.48, "end": 149.6, "text": " neural network architectures have all these knobs and tweaks that we can't formally justify just yet.", "tokens": [50736, 18161, 3209, 6331, 1303, 362, 439, 613, 46999, 293, 46664, 300, 321, 393, 380, 25983, 20833, 445, 1939, 13, 51092], "temperature": 0.0, "avg_logprob": -0.09810181067023478, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.002961066784337163}, {"id": 20, "seek": 13504, "start": 150.72, "end": 159.35999999999999, "text": " We keep being surprised by new architectures such as GPT-3 or stable diffusion, and there is no", "tokens": [51148, 492, 1066, 885, 6100, 538, 777, 6331, 1303, 1270, 382, 26039, 51, 12, 18, 420, 8351, 25242, 11, 293, 456, 307, 572, 51580], "temperature": 0.0, "avg_logprob": -0.09810181067023478, "compression_ratio": 1.529100529100529, "no_speech_prob": 0.002961066784337163}, {"id": 21, "seek": 15936, "start": 159.44000000000003, "end": 164.96, "text": " unifying framework for deep learning. There is no unifying framework that would explain the", "tokens": [50368, 517, 5489, 8388, 337, 2452, 2539, 13, 821, 307, 572, 517, 5489, 8388, 300, 576, 2903, 264, 50644], "temperature": 0.0, "avg_logprob": -0.08332339111639529, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.002655420219525695}, {"id": 22, "seek": 15936, "start": 164.96, "end": 170.48000000000002, "text": " probabilistic perspective, the neuroscience perspective, and merely just the gradient-based", "tokens": [50644, 31959, 3142, 4585, 11, 264, 42762, 4585, 11, 293, 17003, 445, 264, 16235, 12, 6032, 50920], "temperature": 0.0, "avg_logprob": -0.08332339111639529, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.002655420219525695}, {"id": 23, "seek": 15936, "start": 170.48000000000002, "end": 176.64000000000001, "text": " iterative updating perspective. In fact, in the future, we might look at deep learning very differently.", "tokens": [50920, 17138, 1166, 25113, 4585, 13, 682, 1186, 11, 294, 264, 2027, 11, 321, 1062, 574, 412, 2452, 2539, 588, 7614, 13, 51228], "temperature": 0.0, "avg_logprob": -0.08332339111639529, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.002655420219525695}, {"id": 24, "seek": 15936, "start": 178.0, "end": 182.8, "text": " And our claim is that category theory will become the unifying deep learning framework.", "tokens": [51296, 400, 527, 3932, 307, 300, 7719, 5261, 486, 1813, 264, 517, 5489, 2452, 2539, 8388, 13, 51536], "temperature": 0.0, "avg_logprob": -0.08332339111639529, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.002655420219525695}, {"id": 25, "seek": 15936, "start": 183.68, "end": 189.04000000000002, "text": " As a matter of fact, it might become a general theory of neural network architecture,", "tokens": [51580, 1018, 257, 1871, 295, 1186, 11, 309, 1062, 1813, 257, 2674, 5261, 295, 18161, 3209, 9482, 11, 51848], "temperature": 0.0, "avg_logprob": -0.08332339111639529, "compression_ratio": 1.826086956521739, "no_speech_prob": 0.002655420219525695}, {"id": 26, "seek": 18904, "start": 189.04, "end": 193.2, "text": " architectures, and such an essential tool for deep learning practitioners.", "tokens": [50364, 6331, 1303, 11, 293, 1270, 364, 7115, 2290, 337, 2452, 2539, 25742, 13, 50572], "temperature": 0.0, "avg_logprob": -0.07708459612966954, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.00033466413151472807}, {"id": 27, "seek": 18904, "start": 195.28, "end": 200.79999999999998, "text": " Now, this is certainly a bold claim, but it is the one we hope to substantiate in this course.", "tokens": [50676, 823, 11, 341, 307, 3297, 257, 11928, 3932, 11, 457, 309, 307, 264, 472, 321, 1454, 281, 4594, 11520, 473, 294, 341, 1164, 13, 50952], "temperature": 0.0, "avg_logprob": -0.07708459612966954, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.00033466413151472807}, {"id": 28, "seek": 18904, "start": 203.28, "end": 209.51999999999998, "text": " So what is category theory? If I had to describe category theory in one sentence,", "tokens": [51076, 407, 437, 307, 7719, 5261, 30, 759, 286, 632, 281, 6786, 7719, 5261, 294, 472, 8174, 11, 51388], "temperature": 0.0, "avg_logprob": -0.07708459612966954, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.00033466413151472807}, {"id": 29, "seek": 18904, "start": 209.51999999999998, "end": 216.56, "text": " it would be this one. Category theory takes a bird's-eye view of mathematics. From high in the", "tokens": [51388, 309, 576, 312, 341, 472, 13, 383, 48701, 5261, 2516, 257, 5255, 311, 12, 25488, 1910, 295, 18666, 13, 3358, 1090, 294, 264, 51740], "temperature": 0.0, "avg_logprob": -0.07708459612966954, "compression_ratio": 1.5871559633027523, "no_speech_prob": 0.00033466413151472807}, {"id": 30, "seek": 21656, "start": 216.64000000000001, "end": 222.32, "text": " sky, details become invisible, but we can spot patterns that were impossible to detect from", "tokens": [50368, 5443, 11, 4365, 1813, 14603, 11, 457, 321, 393, 4008, 8294, 300, 645, 6243, 281, 5531, 490, 50652], "temperature": 0.0, "avg_logprob": -0.063520064299134, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0016170649323612452}, {"id": 31, "seek": 21656, "start": 222.32, "end": 229.52, "text": " ground level. The famous quote accurately describes what category theory has done to mathematics.", "tokens": [50652, 2727, 1496, 13, 440, 4618, 6513, 20095, 15626, 437, 7719, 5261, 575, 1096, 281, 18666, 13, 51012], "temperature": 0.0, "avg_logprob": -0.063520064299134, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0016170649323612452}, {"id": 32, "seek": 21656, "start": 230.64000000000001, "end": 236.8, "text": " As we'll see, it's not taking a bird's-eye view of just mathematics, but it started to do that", "tokens": [51068, 1018, 321, 603, 536, 11, 309, 311, 406, 1940, 257, 5255, 311, 12, 25488, 1910, 295, 445, 18666, 11, 457, 309, 1409, 281, 360, 300, 51376], "temperature": 0.0, "avg_logprob": -0.063520064299134, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0016170649323612452}, {"id": 33, "seek": 21656, "start": 236.8, "end": 242.0, "text": " to all of science in the form of a new wave that is being called applied category theory.", "tokens": [51376, 281, 439, 295, 3497, 294, 264, 1254, 295, 257, 777, 5772, 300, 307, 885, 1219, 6456, 7719, 5261, 13, 51636], "temperature": 0.0, "avg_logprob": -0.063520064299134, "compression_ratio": 1.626086956521739, "no_speech_prob": 0.0016170649323612452}, {"id": 34, "seek": 24200, "start": 242.8, "end": 247.76, "text": " So before we do anything, I just want to give you a one-slide summary of what applied category", "tokens": [50404, 407, 949, 321, 360, 1340, 11, 286, 445, 528, 281, 976, 291, 257, 472, 12, 10418, 482, 12691, 295, 437, 6456, 7719, 50652], "temperature": 0.0, "avg_logprob": -0.06070037585933034, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.0007542470702901483}, {"id": 35, "seek": 24200, "start": 247.76, "end": 255.44, "text": " theory is and what we hope to teach you in this course. Applied category theory is a particular", "tokens": [50652, 5261, 307, 293, 437, 321, 1454, 281, 2924, 291, 294, 341, 1164, 13, 3132, 39459, 7719, 5261, 307, 257, 1729, 51036], "temperature": 0.0, "avg_logprob": -0.06070037585933034, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.0007542470702901483}, {"id": 36, "seek": 24200, "start": 255.44, "end": 260.32, "text": " way of structuring your knowledge. It's grounded in the idea of compositionality.", "tokens": [51036, 636, 295, 6594, 1345, 428, 3601, 13, 467, 311, 23535, 294, 264, 1558, 295, 12686, 1860, 13, 51280], "temperature": 0.0, "avg_logprob": -0.06070037585933034, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.0007542470702901483}, {"id": 37, "seek": 24200, "start": 262.24, "end": 266.72, "text": " It originated in pure mathematics and has since spread to numerous fields.", "tokens": [51376, 467, 31129, 294, 6075, 18666, 293, 575, 1670, 3974, 281, 12546, 7909, 13, 51600], "temperature": 0.0, "avg_logprob": -0.06070037585933034, "compression_ratio": 1.6064814814814814, "no_speech_prob": 0.0007542470702901483}, {"id": 38, "seek": 26672, "start": 266.72, "end": 275.68, "text": " This is a formal language that is not just a part of these many fields, but it's being used to build", "tokens": [50364, 639, 307, 257, 9860, 2856, 300, 307, 406, 445, 257, 644, 295, 613, 867, 7909, 11, 457, 309, 311, 885, 1143, 281, 1322, 50812], "temperature": 0.0, "avg_logprob": -0.15828524844747194, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.0014279833994805813}, {"id": 39, "seek": 26672, "start": 275.68, "end": 283.84000000000003, "text": " bridges between these fields that were previously unknown. So other than in pure mathematics where", "tokens": [50812, 21114, 1296, 613, 7909, 300, 645, 8046, 9841, 13, 407, 661, 813, 294, 6075, 18666, 689, 51220], "temperature": 0.0, "avg_logprob": -0.15828524844747194, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.0014279833994805813}, {"id": 40, "seek": 26672, "start": 283.84000000000003, "end": 291.28000000000003, "text": " it's, which it permeates, it has been emerging across the sciences. So it's been found in physics,", "tokens": [51220, 309, 311, 11, 597, 309, 30287, 1024, 11, 309, 575, 668, 14989, 2108, 264, 17677, 13, 407, 309, 311, 668, 1352, 294, 10649, 11, 51592], "temperature": 0.0, "avg_logprob": -0.15828524844747194, "compression_ratio": 1.5520833333333333, "no_speech_prob": 0.0014279833994805813}, {"id": 41, "seek": 29128, "start": 291.35999999999996, "end": 297.67999999999995, "text": " it's been found in chemistry, it's been found in systems theory. It is all over computer science", "tokens": [50368, 309, 311, 668, 1352, 294, 12558, 11, 309, 311, 668, 1352, 294, 3652, 5261, 13, 467, 307, 439, 670, 3820, 3497, 50684], "temperature": 0.0, "avg_logprob": -0.06135884920756022, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0026898502837866545}, {"id": 42, "seek": 29128, "start": 297.67999999999995, "end": 304.55999999999995, "text": " and it is the theoretical foundation of functional programming. It has been found in game theory,", "tokens": [50684, 293, 309, 307, 264, 20864, 7030, 295, 11745, 9410, 13, 467, 575, 668, 1352, 294, 1216, 5261, 11, 51028], "temperature": 0.0, "avg_logprob": -0.06135884920756022, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0026898502837866545}, {"id": 43, "seek": 29128, "start": 304.55999999999995, "end": 310.55999999999995, "text": " in information theory, control theory, probability theory, cryptography, and many others.", "tokens": [51028, 294, 1589, 5261, 11, 1969, 5261, 11, 8482, 5261, 11, 9844, 5820, 11, 293, 867, 2357, 13, 51328], "temperature": 0.0, "avg_logprob": -0.06135884920756022, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0026898502837866545}, {"id": 44, "seek": 29128, "start": 313.03999999999996, "end": 319.28, "text": " For us, the most relevant is deep learning where there has been a number of recent papers", "tokens": [51452, 1171, 505, 11, 264, 881, 7340, 307, 2452, 2539, 689, 456, 575, 668, 257, 1230, 295, 5162, 10577, 51764], "temperature": 0.0, "avg_logprob": -0.06135884920756022, "compression_ratio": 1.7641509433962264, "no_speech_prob": 0.0026898502837866545}, {"id": 45, "seek": 31928, "start": 319.28, "end": 327.59999999999997, "text": " in the last two or three years. Now, many people haven't heard of category theory and that might be", "tokens": [50364, 294, 264, 1036, 732, 420, 1045, 924, 13, 823, 11, 867, 561, 2378, 380, 2198, 295, 7719, 5261, 293, 300, 1062, 312, 50780], "temperature": 0.0, "avg_logprob": -0.06336557865142822, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.000933550822082907}, {"id": 46, "seek": 31928, "start": 327.59999999999997, "end": 332.23999999999995, "text": " because it started being applied across the sciences only in the last five years or so.", "tokens": [50780, 570, 309, 1409, 885, 6456, 2108, 264, 17677, 787, 294, 264, 1036, 1732, 924, 420, 370, 13, 51012], "temperature": 0.0, "avg_logprob": -0.06336557865142822, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.000933550822082907}, {"id": 47, "seek": 31928, "start": 333.35999999999996, "end": 337.91999999999996, "text": " So here we see a graph of the intersection of papers in category theory and machine learning.", "tokens": [51068, 407, 510, 321, 536, 257, 4295, 295, 264, 15236, 295, 10577, 294, 7719, 5261, 293, 3479, 2539, 13, 51296], "temperature": 0.0, "avg_logprob": -0.06336557865142822, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.000933550822082907}, {"id": 48, "seek": 31928, "start": 338.96, "end": 347.59999999999997, "text": " As you can see, we are in very early stages. We have just had our fifth applied category theory", "tokens": [51348, 1018, 291, 393, 536, 11, 321, 366, 294, 588, 2440, 10232, 13, 492, 362, 445, 632, 527, 9266, 6456, 7719, 5261, 51780], "temperature": 0.0, "avg_logprob": -0.06336557865142822, "compression_ratio": 1.668141592920354, "no_speech_prob": 0.000933550822082907}, {"id": 49, "seek": 34760, "start": 347.6, "end": 353.6, "text": " conference, the only one, and the only existing applied category theory journal just published", "tokens": [50364, 7586, 11, 264, 787, 472, 11, 293, 264, 787, 6741, 6456, 7719, 5261, 6708, 445, 6572, 50664], "temperature": 0.0, "avg_logprob": -0.08250346936677631, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.001514917821623385}, {"id": 50, "seek": 34760, "start": 353.6, "end": 361.68, "text": " its fourth volume. So this recency of category theory might be one of the reasons it is difficult", "tokens": [50664, 1080, 6409, 5523, 13, 407, 341, 850, 3020, 295, 7719, 5261, 1062, 312, 472, 295, 264, 4112, 309, 307, 2252, 51068], "temperature": 0.0, "avg_logprob": -0.08250346936677631, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.001514917821623385}, {"id": 51, "seek": 34760, "start": 361.68, "end": 369.20000000000005, "text": " to teach. For all of its expressive power, it's an notoriously difficult subject to learn.", "tokens": [51068, 281, 2924, 13, 1171, 439, 295, 1080, 40189, 1347, 11, 309, 311, 364, 46772, 8994, 2252, 3983, 281, 1466, 13, 51444], "temperature": 0.0, "avg_logprob": -0.08250346936677631, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.001514917821623385}, {"id": 52, "seek": 34760, "start": 370.08000000000004, "end": 374.24, "text": " As it originated in abstract mathematics, most introductory material", "tokens": [51488, 1018, 309, 31129, 294, 12649, 18666, 11, 881, 39048, 2527, 51696], "temperature": 0.0, "avg_logprob": -0.08250346936677631, "compression_ratio": 1.614678899082569, "no_speech_prob": 0.001514917821623385}, {"id": 53, "seek": 37424, "start": 374.24, "end": 381.44, "text": " is not aimed at the general audience of programmers or scientists in general. There are a few exceptions.", "tokens": [50364, 307, 406, 20540, 412, 264, 2674, 4034, 295, 41504, 420, 7708, 294, 2674, 13, 821, 366, 257, 1326, 22847, 13, 50724], "temperature": 0.0, "avg_logprob": -0.0974297876711245, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0014081585686653852}, {"id": 54, "seek": 37424, "start": 383.36, "end": 389.76, "text": " So definitions in category theory are extremely information dense. For example, a monad,", "tokens": [50820, 407, 21988, 294, 7719, 5261, 366, 4664, 1589, 18011, 13, 1171, 1365, 11, 257, 1108, 345, 11, 51140], "temperature": 0.0, "avg_logprob": -0.0974297876711245, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0014081585686653852}, {"id": 55, "seek": 37424, "start": 389.76, "end": 395.36, "text": " the concept of a monad shown on this slide has a number of components satisfying a number of rules.", "tokens": [51140, 264, 3410, 295, 257, 1108, 345, 4898, 322, 341, 4137, 575, 257, 1230, 295, 6677, 18348, 257, 1230, 295, 4474, 13, 51420], "temperature": 0.0, "avg_logprob": -0.0974297876711245, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0014081585686653852}, {"id": 56, "seek": 37424, "start": 396.32, "end": 399.84000000000003, "text": " And then each of these components is often information dense as well,", "tokens": [51468, 400, 550, 1184, 295, 613, 6677, 307, 2049, 1589, 18011, 382, 731, 11, 51644], "temperature": 0.0, "avg_logprob": -0.0974297876711245, "compression_ratio": 1.6774193548387097, "no_speech_prob": 0.0014081585686653852}, {"id": 57, "seek": 39984, "start": 399.84, "end": 406.15999999999997, "text": " meaning category theory requires a lot of initial investment to start appreciating.", "tokens": [50364, 3620, 7719, 5261, 7029, 257, 688, 295, 5883, 6078, 281, 722, 3616, 990, 13, 50680], "temperature": 0.0, "avg_logprob": -0.07213239537345038, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.000586760404985398}, {"id": 58, "seek": 39984, "start": 409.2, "end": 415.12, "text": " But once you start learning category theory and start appreciating the definitions,", "tokens": [50832, 583, 1564, 291, 722, 2539, 7719, 5261, 293, 722, 3616, 990, 264, 21988, 11, 51128], "temperature": 0.0, "avg_logprob": -0.07213239537345038, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.000586760404985398}, {"id": 59, "seek": 39984, "start": 415.12, "end": 420.55999999999995, "text": " you see that the definitions form an incredibly cohesive theory, really not found anywhere else", "tokens": [51128, 291, 536, 300, 264, 21988, 1254, 364, 6252, 43025, 5261, 11, 534, 406, 1352, 4992, 1646, 51400], "temperature": 0.0, "avg_logprob": -0.07213239537345038, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.000586760404985398}, {"id": 60, "seek": 39984, "start": 420.55999999999995, "end": 427.35999999999996, "text": " in science. These interplay together at such a remarkable level, which is hard to appreciate", "tokens": [51400, 294, 3497, 13, 1981, 728, 2858, 1214, 412, 1270, 257, 12802, 1496, 11, 597, 307, 1152, 281, 4449, 51740], "temperature": 0.0, "avg_logprob": -0.07213239537345038, "compression_ratio": 1.7450980392156863, "no_speech_prob": 0.000586760404985398}, {"id": 61, "seek": 42736, "start": 427.36, "end": 432.96000000000004, "text": " when you're just starting to learn it. It often looks very difficult. And on a graph,", "tokens": [50364, 562, 291, 434, 445, 2891, 281, 1466, 309, 13, 467, 2049, 1542, 588, 2252, 13, 400, 322, 257, 4295, 11, 50644], "temperature": 0.0, "avg_logprob": -0.08535863104320708, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0006149903638288379}, {"id": 62, "seek": 42736, "start": 434.16, "end": 440.96000000000004, "text": " the learning process looks something like this. So compared to traditional methods,", "tokens": [50704, 264, 2539, 1399, 1542, 746, 411, 341, 13, 407, 5347, 281, 5164, 7150, 11, 51044], "temperature": 0.0, "avg_logprob": -0.08535863104320708, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0006149903638288379}, {"id": 63, "seek": 42736, "start": 440.96000000000004, "end": 445.68, "text": " the structural method of category theory takes a long time to get started. But once you do,", "tokens": [51044, 264, 15067, 3170, 295, 7719, 5261, 2516, 257, 938, 565, 281, 483, 1409, 13, 583, 1564, 291, 360, 11, 51280], "temperature": 0.0, "avg_logprob": -0.08535863104320708, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0006149903638288379}, {"id": 64, "seek": 42736, "start": 445.68, "end": 454.08000000000004, "text": " the theory scales much better. So in some sense, category theory is a theory of how to scale up", "tokens": [51280, 264, 5261, 17408, 709, 1101, 13, 407, 294, 512, 2020, 11, 7719, 5261, 307, 257, 5261, 295, 577, 281, 4373, 493, 51700], "temperature": 0.0, "avg_logprob": -0.08535863104320708, "compression_ratio": 1.6604651162790698, "no_speech_prob": 0.0006149903638288379}, {"id": 65, "seek": 45408, "start": 454.15999999999997, "end": 461.59999999999997, "text": " our systems. And this is something we want to teach you in this course. We want to teach you", "tokens": [50368, 527, 3652, 13, 400, 341, 307, 746, 321, 528, 281, 2924, 291, 294, 341, 1164, 13, 492, 528, 281, 2924, 291, 50740], "temperature": 0.0, "avg_logprob": -0.07555795938540728, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0019801591988652945}, {"id": 66, "seek": 45408, "start": 461.59999999999997, "end": 467.28, "text": " how to approach category theory, motivated with some practical examples from deep learning,", "tokens": [50740, 577, 281, 3109, 7719, 5261, 11, 14515, 365, 512, 8496, 5110, 490, 2452, 2539, 11, 51024], "temperature": 0.0, "avg_logprob": -0.07555795938540728, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0019801591988652945}, {"id": 67, "seek": 45408, "start": 467.84, "end": 471.76, "text": " and really give you a sense of the philosophy behind category theory.", "tokens": [51052, 293, 534, 976, 291, 257, 2020, 295, 264, 10675, 2261, 7719, 5261, 13, 51248], "temperature": 0.0, "avg_logprob": -0.07555795938540728, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0019801591988652945}, {"id": 68, "seek": 45408, "start": 474.47999999999996, "end": 480.64, "text": " So this is week one. And after today's lecture, which is going to be an introductory", "tokens": [51384, 407, 341, 307, 1243, 472, 13, 400, 934, 965, 311, 7991, 11, 597, 307, 516, 281, 312, 364, 39048, 51692], "temperature": 0.0, "avg_logprob": -0.07555795938540728, "compression_ratio": 1.599056603773585, "no_speech_prob": 0.0019801591988652945}, {"id": 69, "seek": 48064, "start": 480.71999999999997, "end": 489.44, "text": " lecture to the general thoughts of category theory, we're going to start going into the details.", "tokens": [50368, 7991, 281, 264, 2674, 4598, 295, 7719, 5261, 11, 321, 434, 516, 281, 722, 516, 666, 264, 4365, 13, 50804], "temperature": 0.0, "avg_logprob": -0.10901006460189819, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.0010134736075997353}, {"id": 70, "seek": 48064, "start": 491.2, "end": 496.08, "text": " In week two, we will be studying essential building blocks of category theory, categories", "tokens": [50892, 682, 1243, 732, 11, 321, 486, 312, 7601, 7115, 2390, 8474, 295, 7719, 5261, 11, 10479, 51136], "temperature": 0.0, "avg_logprob": -0.10901006460189819, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.0010134736075997353}, {"id": 71, "seek": 48064, "start": 496.08, "end": 502.15999999999997, "text": " and functors. In week three, we will study how category theory can be used to describe back", "tokens": [51136, 293, 1019, 5547, 13, 682, 1243, 1045, 11, 321, 486, 2979, 577, 7719, 5261, 393, 312, 1143, 281, 6786, 646, 51440], "temperature": 0.0, "avg_logprob": -0.10901006460189819, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.0010134736075997353}, {"id": 72, "seek": 48064, "start": 502.15999999999997, "end": 507.44, "text": " propagation using monoidal categories, admitting a visual and intuitive graphical language.", "tokens": [51440, 38377, 1228, 1108, 17079, 304, 10479, 11, 44056, 257, 5056, 293, 21769, 35942, 2856, 13, 51704], "temperature": 0.0, "avg_logprob": -0.10901006460189819, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.0010134736075997353}, {"id": 73, "seek": 50744, "start": 508.4, "end": 512.88, "text": " In week four, we'll study how geometric deep learning and naturality", "tokens": [50412, 682, 1243, 1451, 11, 321, 603, 2979, 577, 33246, 2452, 2539, 293, 3303, 507, 50636], "temperature": 0.0, "avg_logprob": -0.1342264684041341, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0005180633161216974}, {"id": 74, "seek": 50744, "start": 514.24, "end": 522.08, "text": " and have their foundations in category theory. We're going to study natural graph networks.", "tokens": [50704, 293, 362, 641, 22467, 294, 7719, 5261, 13, 492, 434, 516, 281, 2979, 3303, 4295, 9590, 13, 51096], "temperature": 0.0, "avg_logprob": -0.1342264684041341, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0005180633161216974}, {"id": 75, "seek": 50744, "start": 524.16, "end": 529.76, "text": " And lastly, in week four, we will see how monoids, monads, and various algebraic structures", "tokens": [51200, 400, 16386, 11, 294, 1243, 1451, 11, 321, 486, 536, 577, 35624, 3742, 11, 1108, 5834, 11, 293, 3683, 21989, 299, 9227, 51480], "temperature": 0.0, "avg_logprob": -0.1342264684041341, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0005180633161216974}, {"id": 76, "seek": 50744, "start": 529.76, "end": 533.36, "text": " can be connected to recurrent neural networks and LSTMs.", "tokens": [51480, 393, 312, 4582, 281, 18680, 1753, 18161, 9590, 293, 441, 6840, 26386, 13, 51660], "temperature": 0.0, "avg_logprob": -0.1342264684041341, "compression_ratio": 1.537313432835821, "no_speech_prob": 0.0005180633161216974}, {"id": 77, "seek": 53744, "start": 537.44, "end": 541.36, "text": " After our five weeks of lectures, we have a series of talks by people who are", "tokens": [50364, 2381, 527, 1732, 3259, 295, 16564, 11, 321, 362, 257, 2638, 295, 6686, 538, 561, 567, 366, 50560], "temperature": 0.0, "avg_logprob": -0.09373018855140322, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.00020300746837165207}, {"id": 78, "seek": 53744, "start": 541.36, "end": 546.4000000000001, "text": " in the industry doing category theory deep learning or both. And we are very excited about those.", "tokens": [50560, 294, 264, 3518, 884, 7719, 5261, 2452, 2539, 420, 1293, 13, 400, 321, 366, 588, 2919, 466, 729, 13, 50812], "temperature": 0.0, "avg_logprob": -0.09373018855140322, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.00020300746837165207}, {"id": 79, "seek": 53744, "start": 551.44, "end": 558.4000000000001, "text": " But yes, in today's lecture, we're going to start by studying what is compositionality,", "tokens": [51064, 583, 2086, 11, 294, 965, 311, 7991, 11, 321, 434, 516, 281, 722, 538, 7601, 437, 307, 12686, 1860, 11, 51412], "temperature": 0.0, "avg_logprob": -0.09373018855140322, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.00020300746837165207}, {"id": 80, "seek": 53744, "start": 559.36, "end": 565.2800000000001, "text": " what is category theory really, what you need to start learning and taking advantage of it,", "tokens": [51460, 437, 307, 7719, 5261, 534, 11, 437, 291, 643, 281, 722, 2539, 293, 1940, 5002, 295, 309, 11, 51756], "temperature": 0.0, "avg_logprob": -0.09373018855140322, "compression_ratio": 1.6824644549763033, "no_speech_prob": 0.00020300746837165207}, {"id": 81, "seek": 56528, "start": 565.28, "end": 571.52, "text": " perhaps the most important thing. And we're going to take a look at what category theory", "tokens": [50364, 4317, 264, 881, 1021, 551, 13, 400, 321, 434, 516, 281, 747, 257, 574, 412, 437, 7719, 5261, 50676], "temperature": 0.0, "avg_logprob": -0.06566623124209317, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.0009657609625719488}, {"id": 82, "seek": 56528, "start": 571.52, "end": 581.76, "text": " has done and can do for deep learning. So yeah, let's get started. So what is compositionality?", "tokens": [50676, 575, 1096, 293, 393, 360, 337, 2452, 2539, 13, 407, 1338, 11, 718, 311, 483, 1409, 13, 407, 437, 307, 12686, 1860, 30, 51188], "temperature": 0.0, "avg_logprob": -0.06566623124209317, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.0009657609625719488}, {"id": 83, "seek": 56528, "start": 581.76, "end": 587.6, "text": " This is a concept I've mentioned, which is central to category theory. And it's a concept", "tokens": [51188, 639, 307, 257, 3410, 286, 600, 2835, 11, 597, 307, 5777, 281, 7719, 5261, 13, 400, 309, 311, 257, 3410, 51480], "temperature": 0.0, "avg_logprob": -0.06566623124209317, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.0009657609625719488}, {"id": 84, "seek": 56528, "start": 587.6, "end": 593.76, "text": " that's often misunderstood. It's often presented as an ability to build systems together by", "tokens": [51480, 300, 311, 2049, 33870, 13, 467, 311, 2049, 8212, 382, 364, 3485, 281, 1322, 3652, 1214, 538, 51788], "temperature": 0.0, "avg_logprob": -0.06566623124209317, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.0009657609625719488}, {"id": 85, "seek": 59376, "start": 593.76, "end": 601.6, "text": " composing them out of smaller subsystems. Now, this is certainly a component of compositionality,", "tokens": [50364, 715, 6110, 552, 484, 295, 4356, 2090, 9321, 82, 13, 823, 11, 341, 307, 3297, 257, 6542, 295, 12686, 1860, 11, 50756], "temperature": 0.0, "avg_logprob": -0.06503597172823819, "compression_ratio": 1.8019323671497585, "no_speech_prob": 0.0010408367961645126}, {"id": 86, "seek": 59376, "start": 601.6, "end": 609.36, "text": " but this is not all. Compositionality includes the ability to build the systems, but we don't", "tokens": [50756, 457, 341, 307, 406, 439, 13, 6620, 5830, 1860, 5974, 264, 3485, 281, 1322, 264, 3652, 11, 457, 321, 500, 380, 51144], "temperature": 0.0, "avg_logprob": -0.06503597172823819, "compression_ratio": 1.8019323671497585, "no_speech_prob": 0.0010408367961645126}, {"id": 87, "seek": 59376, "start": 609.36, "end": 615.28, "text": " just want to build a very complex system that we can't reason about. It includes the ability to", "tokens": [51144, 445, 528, 281, 1322, 257, 588, 3997, 1185, 300, 321, 393, 380, 1778, 466, 13, 467, 5974, 264, 3485, 281, 51440], "temperature": 0.0, "avg_logprob": -0.06503597172823819, "compression_ratio": 1.8019323671497585, "no_speech_prob": 0.0010408367961645126}, {"id": 88, "seek": 59376, "start": 615.28, "end": 621.36, "text": " reason about the resulting system recursively in terms of its components. And really,", "tokens": [51440, 1778, 466, 264, 16505, 1185, 20560, 3413, 294, 2115, 295, 1080, 6677, 13, 400, 534, 11, 51744], "temperature": 0.0, "avg_logprob": -0.06503597172823819, "compression_ratio": 1.8019323671497585, "no_speech_prob": 0.0010408367961645126}, {"id": 89, "seek": 62136, "start": 621.36, "end": 627.6800000000001, "text": " compositionality is more of a property of the model of our system than the system itself.", "tokens": [50364, 12686, 1860, 307, 544, 295, 257, 4707, 295, 264, 2316, 295, 527, 1185, 813, 264, 1185, 2564, 13, 50680], "temperature": 0.0, "avg_logprob": -0.06268550100780669, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0014467714354395866}, {"id": 90, "seek": 62136, "start": 628.96, "end": 633.12, "text": " So we're going to see what that means. Compositionality requires both of these things.", "tokens": [50744, 407, 321, 434, 516, 281, 536, 437, 300, 1355, 13, 6620, 5830, 1860, 7029, 1293, 295, 613, 721, 13, 50952], "temperature": 0.0, "avg_logprob": -0.06268550100780669, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0014467714354395866}, {"id": 91, "seek": 62136, "start": 633.6800000000001, "end": 639.6800000000001, "text": " And for many models of our systems that are found throughout nature, we have the first", "tokens": [50980, 400, 337, 867, 5245, 295, 527, 3652, 300, 366, 1352, 3710, 3687, 11, 321, 362, 264, 700, 51280], "temperature": 0.0, "avg_logprob": -0.06268550100780669, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0014467714354395866}, {"id": 92, "seek": 62136, "start": 639.6800000000001, "end": 643.76, "text": " property that we can build the system, but not the second one that we can reason about it.", "tokens": [51280, 4707, 300, 321, 393, 1322, 264, 1185, 11, 457, 406, 264, 1150, 472, 300, 321, 393, 1778, 466, 309, 13, 51484], "temperature": 0.0, "avg_logprob": -0.06268550100780669, "compression_ratio": 1.7611940298507462, "no_speech_prob": 0.0014467714354395866}, {"id": 93, "seek": 64376, "start": 644.56, "end": 652.4, "text": " So for example, when we study behaviors of markets, of organizations, of economy,", "tokens": [50404, 407, 337, 1365, 11, 562, 321, 2979, 15501, 295, 8383, 11, 295, 6150, 11, 295, 5010, 11, 50796], "temperature": 0.0, "avg_logprob": -0.08975729412502713, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.001863039331510663}, {"id": 94, "seek": 64376, "start": 652.4, "end": 661.12, "text": " of neural networks, and many other concepts, these things aren't compositional. Now, what does", "tokens": [50796, 295, 18161, 9590, 11, 293, 867, 661, 10392, 11, 613, 721, 3212, 380, 10199, 2628, 13, 823, 11, 437, 775, 51232], "temperature": 0.0, "avg_logprob": -0.08975729412502713, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.001863039331510663}, {"id": 95, "seek": 64376, "start": 661.12, "end": 667.84, "text": " that mean? These are very fuzzily defined concepts, but they're precisely very fuzzily defined only", "tokens": [51232, 300, 914, 30, 1981, 366, 588, 283, 16740, 953, 7642, 10392, 11, 457, 436, 434, 13402, 588, 283, 16740, 953, 7642, 787, 51568], "temperature": 0.0, "avg_logprob": -0.08975729412502713, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.001863039331510663}, {"id": 96, "seek": 64376, "start": 667.84, "end": 672.4, "text": " because we don't know how to reason about them from outside. We don't know what are the fundamental", "tokens": [51568, 570, 321, 500, 380, 458, 577, 281, 1778, 466, 552, 490, 2380, 13, 492, 500, 380, 458, 437, 366, 264, 8088, 51796], "temperature": 0.0, "avg_logprob": -0.08975729412502713, "compression_ratio": 1.6277056277056277, "no_speech_prob": 0.001863039331510663}, {"id": 97, "seek": 67240, "start": 672.4, "end": 676.56, "text": " building blocks, and we don't know how to reason about the behavior of, say, the global economy", "tokens": [50364, 2390, 8474, 11, 293, 321, 500, 380, 458, 577, 281, 1778, 466, 264, 5223, 295, 11, 584, 11, 264, 4338, 5010, 50572], "temperature": 0.0, "avg_logprob": -0.09479575590653853, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.002318082842975855}, {"id": 98, "seek": 67240, "start": 677.36, "end": 682.16, "text": " by studying behavior of economies of individual countries. There's many emergent effects that", "tokens": [50612, 538, 7601, 5223, 295, 23158, 295, 2609, 3517, 13, 821, 311, 867, 4345, 6930, 5065, 300, 50852], "temperature": 0.0, "avg_logprob": -0.09479575590653853, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.002318082842975855}, {"id": 99, "seek": 67240, "start": 682.16, "end": 687.52, "text": " are happening and that are hard to track. On the other hand, there are many systems,", "tokens": [50852, 366, 2737, 293, 300, 366, 1152, 281, 2837, 13, 1282, 264, 661, 1011, 11, 456, 366, 867, 3652, 11, 51120], "temperature": 0.0, "avg_logprob": -0.09479575590653853, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.002318082842975855}, {"id": 100, "seek": 67240, "start": 688.0799999999999, "end": 694.0, "text": " many models of our systems that have both one and two. For instance, and these are often very", "tokens": [51148, 867, 5245, 295, 527, 3652, 300, 362, 1293, 472, 293, 732, 13, 1171, 5197, 11, 293, 613, 366, 2049, 588, 51444], "temperature": 0.0, "avg_logprob": -0.09479575590653853, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.002318082842975855}, {"id": 101, "seek": 67240, "start": 694.0, "end": 698.56, "text": " simple systems. We think of them as simple because we understand them. So for example, if we have two", "tokens": [51444, 2199, 3652, 13, 492, 519, 295, 552, 382, 2199, 570, 321, 1223, 552, 13, 407, 337, 1365, 11, 498, 321, 362, 732, 51672], "temperature": 0.0, "avg_logprob": -0.09479575590653853, "compression_ratio": 1.6967509025270757, "no_speech_prob": 0.002318082842975855}, {"id": 102, "seek": 69856, "start": 698.56, "end": 703.52, "text": " differentiable functions, we can put them together and get a composite differentiable", "tokens": [50364, 819, 9364, 6828, 11, 321, 393, 829, 552, 1214, 293, 483, 257, 25557, 819, 9364, 50612], "temperature": 0.0, "avg_logprob": -0.09832113638691518, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.0028363389428704977}, {"id": 103, "seek": 69856, "start": 703.52, "end": 710.16, "text": " functions by using the chain rule. If we have a compiler from a language A to language B,", "tokens": [50612, 6828, 538, 1228, 264, 5021, 4978, 13, 759, 321, 362, 257, 31958, 490, 257, 2856, 316, 281, 2856, 363, 11, 50944], "temperature": 0.0, "avg_logprob": -0.09832113638691518, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.0028363389428704977}, {"id": 104, "seek": 69856, "start": 710.16, "end": 715.1199999999999, "text": " and a compiler from language B to language C, we get a joint compiler that compiles all the way", "tokens": [50944, 293, 257, 31958, 490, 2856, 363, 281, 2856, 383, 11, 321, 483, 257, 7225, 31958, 300, 715, 4680, 439, 264, 636, 51192], "temperature": 0.0, "avg_logprob": -0.09832113638691518, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.0028363389428704977}, {"id": 105, "seek": 69856, "start": 715.1199999999999, "end": 722.4, "text": " through. We can compose various things like Markov kernels. We can compose merely polynomials and so", "tokens": [51192, 807, 13, 492, 393, 35925, 3683, 721, 411, 3934, 5179, 23434, 1625, 13, 492, 393, 35925, 17003, 22560, 12356, 293, 370, 51556], "temperature": 0.0, "avg_logprob": -0.09832113638691518, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.0028363389428704977}, {"id": 106, "seek": 72240, "start": 722.56, "end": 732.48, "text": " on. There's many kinds of systems. Now, this slide, if you're seeing this, this might", "tokens": [50372, 322, 13, 821, 311, 867, 3685, 295, 3652, 13, 823, 11, 341, 4137, 11, 498, 291, 434, 2577, 341, 11, 341, 1062, 50868], "temperature": 0.0, "avg_logprob": -0.14171110499988904, "compression_ratio": 1.5917159763313609, "no_speech_prob": 0.005374158266931772}, {"id": 107, "seek": 72240, "start": 734.16, "end": 739.52, "text": " ring some alarm bells. It might seem like I've said something inconsistent here. It might seem like", "tokens": [50952, 4875, 512, 14183, 25474, 13, 467, 1062, 1643, 411, 286, 600, 848, 746, 36891, 510, 13, 467, 1062, 1643, 411, 51220], "temperature": 0.0, "avg_logprob": -0.14171110499988904, "compression_ratio": 1.5917159763313609, "no_speech_prob": 0.005374158266931772}, {"id": 108, "seek": 72240, "start": 739.52, "end": 749.52, "text": " we are saying this. So it certainly looks like I said differentiable functions like", "tokens": [51220, 321, 366, 1566, 341, 13, 407, 309, 3297, 1542, 411, 286, 848, 819, 9364, 6828, 411, 51720], "temperature": 0.0, "avg_logprob": -0.14171110499988904, "compression_ratio": 1.5917159763313609, "no_speech_prob": 0.005374158266931772}, {"id": 109, "seek": 74952, "start": 749.52, "end": 754.24, "text": " compositional. And I said neural networks are made out of differentiable functions. Therefore,", "tokens": [50364, 10199, 2628, 13, 400, 286, 848, 18161, 9590, 366, 1027, 484, 295, 819, 9364, 6828, 13, 7504, 11, 50600], "temperature": 0.0, "avg_logprob": -0.09924003056117467, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.001384687377139926}, {"id": 110, "seek": 74952, "start": 754.24, "end": 759.4399999999999, "text": " neural networks are compositional, but I said they're not. So this meme might look like what", "tokens": [50600, 18161, 9590, 366, 10199, 2628, 11, 457, 286, 848, 436, 434, 406, 13, 407, 341, 21701, 1062, 574, 411, 437, 50860], "temperature": 0.0, "avg_logprob": -0.09924003056117467, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.001384687377139926}, {"id": 111, "seek": 74952, "start": 759.4399999999999, "end": 764.0, "text": " we're saying, but really what we're saying is the following. Compositional with respect to what.", "tokens": [50860, 321, 434, 1566, 11, 457, 534, 437, 321, 434, 1566, 307, 264, 3480, 13, 6620, 329, 2628, 365, 3104, 281, 437, 13, 51088], "temperature": 0.0, "avg_logprob": -0.09924003056117467, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.001384687377139926}, {"id": 112, "seek": 74952, "start": 765.28, "end": 770.0, "text": " So I said compositionality is a property of our models, and we have to specify what", "tokens": [51152, 407, 286, 848, 10199, 2628, 507, 307, 257, 4707, 295, 527, 5245, 11, 293, 321, 362, 281, 16500, 437, 51388], "temperature": 0.0, "avg_logprob": -0.09924003056117467, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.001384687377139926}, {"id": 113, "seek": 74952, "start": 770.8, "end": 776.24, "text": " our model is, what it is that we're studying. It's important to specify the property of the", "tokens": [51428, 527, 2316, 307, 11, 437, 309, 307, 300, 321, 434, 7601, 13, 467, 311, 1021, 281, 16500, 264, 4707, 295, 264, 51700], "temperature": 0.0, "avg_logprob": -0.09924003056117467, "compression_ratio": 1.8930041152263375, "no_speech_prob": 0.001384687377139926}, {"id": 114, "seek": 77624, "start": 776.24, "end": 781.6, "text": " system we want to model. So if our model treats neural networks as differentiable functions,", "tokens": [50364, 1185, 321, 528, 281, 2316, 13, 407, 498, 527, 2316, 19566, 18161, 9590, 382, 819, 9364, 6828, 11, 50632], "temperature": 0.0, "avg_logprob": -0.1122784972190857, "compression_ratio": 1.925531914893617, "no_speech_prob": 0.0009678879869170487}, {"id": 115, "seek": 77624, "start": 781.6, "end": 787.12, "text": " then, indeed, neural networks are compositional. We plug together a bunch of differentiable", "tokens": [50632, 550, 11, 6451, 11, 18161, 9590, 366, 10199, 2628, 13, 492, 5452, 1214, 257, 3840, 295, 819, 9364, 50908], "temperature": 0.0, "avg_logprob": -0.1122784972190857, "compression_ratio": 1.925531914893617, "no_speech_prob": 0.0009678879869170487}, {"id": 116, "seek": 77624, "start": 787.12, "end": 793.36, "text": " functions, and using automatic differentiation, we can compute, or chain rule, really, we can", "tokens": [50908, 6828, 11, 293, 1228, 12509, 38902, 11, 321, 393, 14722, 11, 420, 5021, 4978, 11, 534, 11, 321, 393, 51220], "temperature": 0.0, "avg_logprob": -0.1122784972190857, "compression_ratio": 1.925531914893617, "no_speech_prob": 0.0009678879869170487}, {"id": 117, "seek": 77624, "start": 793.36, "end": 801.44, "text": " compute the derivative of the composite. But if our model treats neural networks as", "tokens": [51220, 14722, 264, 13760, 295, 264, 25557, 13, 583, 498, 527, 2316, 19566, 18161, 9590, 382, 51624], "temperature": 0.0, "avg_logprob": -0.1122784972190857, "compression_ratio": 1.925531914893617, "no_speech_prob": 0.0009678879869170487}, {"id": 118, "seek": 80144, "start": 801.44, "end": 805.36, "text": " generative or discriminative models, then this is not compositional.", "tokens": [50364, 1337, 1166, 420, 20828, 1166, 5245, 11, 550, 341, 307, 406, 10199, 2628, 13, 50560], "temperature": 0.0, "avg_logprob": -0.09366695504439504, "compression_ratio": 1.5308641975308641, "no_speech_prob": 0.0008527365862391889}, {"id": 119, "seek": 80144, "start": 808.1600000000001, "end": 812.96, "text": " It's important to specify what property we're studying and what exactly are we modeling.", "tokens": [50700, 467, 311, 1021, 281, 16500, 437, 4707, 321, 434, 7601, 293, 437, 2293, 366, 321, 15983, 13, 50940], "temperature": 0.0, "avg_logprob": -0.09366695504439504, "compression_ratio": 1.5308641975308641, "no_speech_prob": 0.0008527365862391889}, {"id": 120, "seek": 80144, "start": 815.2, "end": 825.36, "text": " Really, compositionality is about interfaces. If you think about a system composed of many", "tokens": [51052, 4083, 11, 10199, 2628, 507, 307, 466, 28416, 13, 759, 291, 519, 466, 257, 1185, 18204, 295, 867, 51560], "temperature": 0.0, "avg_logprob": -0.09366695504439504, "compression_ratio": 1.5308641975308641, "no_speech_prob": 0.0008527365862391889}, {"id": 121, "seek": 82536, "start": 825.36, "end": 833.92, "text": " smaller subsystems, and the simplest case here is that of a function. When we compose these", "tokens": [50364, 4356, 2090, 9321, 82, 11, 293, 264, 22811, 1389, 510, 307, 300, 295, 257, 2445, 13, 1133, 321, 35925, 613, 50792], "temperature": 0.0, "avg_logprob": -0.10285431146621704, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0012021362781524658}, {"id": 122, "seek": 82536, "start": 833.92, "end": 839.44, "text": " systems, we want to ensure that all the data we need is available at the exposed interfaces.", "tokens": [50792, 3652, 11, 321, 528, 281, 5586, 300, 439, 264, 1412, 321, 643, 307, 2435, 412, 264, 9495, 28416, 13, 51068], "temperature": 0.0, "avg_logprob": -0.10285431146621704, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0012021362781524658}, {"id": 123, "seek": 82536, "start": 843.52, "end": 849.9200000000001, "text": " So really, if we have three functions like this, the famous property of associativity", "tokens": [51272, 407, 534, 11, 498, 321, 362, 1045, 6828, 411, 341, 11, 264, 4618, 4707, 295, 4180, 30142, 51592], "temperature": 0.0, "avg_logprob": -0.10285431146621704, "compression_ratio": 1.5517241379310345, "no_speech_prob": 0.0012021362781524658}, {"id": 124, "seek": 84992, "start": 849.92, "end": 855.92, "text": " tells us that if we compose f and g, and then with h, that should be the same as composing g", "tokens": [50364, 5112, 505, 300, 498, 321, 35925, 283, 293, 290, 11, 293, 550, 365, 276, 11, 300, 820, 312, 264, 912, 382, 715, 6110, 290, 50664], "temperature": 0.0, "avg_logprob": -0.0905230942592826, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.000790174410212785}, {"id": 125, "seek": 84992, "start": 855.92, "end": 863.28, "text": " with h and then with f. So this famous property posits that function composition should be", "tokens": [50664, 365, 276, 293, 550, 365, 283, 13, 407, 341, 4618, 4707, 1366, 1208, 300, 2445, 12686, 820, 312, 51032], "temperature": 0.0, "avg_logprob": -0.0905230942592826, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.000790174410212785}, {"id": 126, "seek": 84992, "start": 863.28, "end": 869.4399999999999, "text": " associated. But why is this the property that you might want to have? Because it allows us to treat", "tokens": [51032, 6615, 13, 583, 983, 307, 341, 264, 4707, 300, 291, 1062, 528, 281, 362, 30, 1436, 309, 4045, 505, 281, 2387, 51340], "temperature": 0.0, "avg_logprob": -0.0905230942592826, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.000790174410212785}, {"id": 127, "seek": 84992, "start": 869.4399999999999, "end": 878.0799999999999, "text": " functions extensionally, only by looking at their interfaces. And if these composites are the same,", "tokens": [51340, 6828, 10320, 379, 11, 787, 538, 1237, 412, 641, 28416, 13, 400, 498, 613, 10199, 3324, 366, 264, 912, 11, 51772], "temperature": 0.0, "avg_logprob": -0.0905230942592826, "compression_ratio": 1.7649769585253456, "no_speech_prob": 0.000790174410212785}, {"id": 128, "seek": 87808, "start": 878.08, "end": 883.2, "text": " then function composition is associative. And we can look at only the input and output of the", "tokens": [50364, 550, 2445, 12686, 307, 4180, 1166, 13, 400, 321, 393, 574, 412, 787, 264, 4846, 293, 5598, 295, 264, 50620], "temperature": 0.0, "avg_logprob": -0.07813410460948944, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.0014249299420043826}, {"id": 129, "seek": 87808, "start": 883.2, "end": 888.32, "text": " system to know how it behaves. And we don't need to know anything about its internals or how it was", "tokens": [50620, 1185, 281, 458, 577, 309, 36896, 13, 400, 321, 500, 380, 643, 281, 458, 1340, 466, 1080, 2154, 1124, 420, 577, 309, 390, 50876], "temperature": 0.0, "avg_logprob": -0.07813410460948944, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.0014249299420043826}, {"id": 130, "seek": 87808, "start": 888.32, "end": 898.88, "text": " built. But if this property isn't satisfied, then we might be in trouble. Non-compositionality implies", "tokens": [50876, 3094, 13, 583, 498, 341, 4707, 1943, 380, 11239, 11, 550, 321, 1062, 312, 294, 5253, 13, 8774, 12, 21541, 329, 2628, 507, 18779, 51404], "temperature": 0.0, "avg_logprob": -0.07813410460948944, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.0014249299420043826}, {"id": 131, "seek": 87808, "start": 898.88, "end": 905.9200000000001, "text": " that we need extra data to reason about the system, data that isn't available through the interfaces.", "tokens": [51404, 300, 321, 643, 2857, 1412, 281, 1778, 466, 264, 1185, 11, 1412, 300, 1943, 380, 2435, 807, 264, 28416, 13, 51756], "temperature": 0.0, "avg_logprob": -0.07813410460948944, "compression_ratio": 1.6864406779661016, "no_speech_prob": 0.0014249299420043826}, {"id": 132, "seek": 90592, "start": 906.88, "end": 911.28, "text": " But if our method of interaction is only through the interfaces, which is the intended way of", "tokens": [50412, 583, 498, 527, 3170, 295, 9285, 307, 787, 807, 264, 28416, 11, 597, 307, 264, 10226, 636, 295, 50632], "temperature": 0.0, "avg_logprob": -0.19107985806155514, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0007895230082795024}, {"id": 133, "seek": 90592, "start": 911.28, "end": 915.5999999999999, "text": " interacting, then this means that we have uncertainty about how the system will behave.", "tokens": [50632, 18017, 11, 550, 341, 1355, 300, 321, 362, 15697, 466, 577, 264, 1185, 486, 15158, 13, 50848], "temperature": 0.0, "avg_logprob": -0.19107985806155514, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0007895230082795024}, {"id": 134, "seek": 90592, "start": 916.8, "end": 923.5999999999999, "text": " And then composing many such systems together, our uncertainty can only grow.", "tokens": [50908, 400, 550, 715, 6110, 867, 1270, 3652, 1214, 11, 527, 15697, 393, 787, 1852, 13, 51248], "temperature": 0.0, "avg_logprob": -0.19107985806155514, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0007895230082795024}, {"id": 135, "seek": 90592, "start": 926.4799999999999, "end": 932.3199999999999, "text": " Really, what we want to is minimize this uncertainty by imposing some invariances of how we", "tokens": [51392, 4083, 11, 437, 321, 528, 281, 307, 17522, 341, 15697, 538, 40288, 512, 33270, 2676, 295, 577, 321, 51684], "temperature": 0.0, "avg_logprob": -0.19107985806155514, "compression_ratio": 1.703883495145631, "no_speech_prob": 0.0007895230082795024}, {"id": 136, "seek": 93232, "start": 932.96, "end": 937.9200000000001, "text": " put systems together. So that's when we create a bigger system, we know something about how it", "tokens": [50396, 829, 3652, 1214, 13, 407, 300, 311, 562, 321, 1884, 257, 3801, 1185, 11, 321, 458, 746, 466, 577, 309, 50644], "temperature": 0.0, "avg_logprob": -0.12910075897866108, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0028327261097729206}, {"id": 137, "seek": 93232, "start": 937.9200000000001, "end": 944.4000000000001, "text": " behaves. So really, compositionality is a very delicate property. And I'm a fan of this quote", "tokens": [50644, 36896, 13, 407, 534, 11, 10199, 2628, 507, 307, 257, 588, 21417, 4707, 13, 400, 286, 478, 257, 3429, 295, 341, 6513, 50968], "temperature": 0.0, "avg_logprob": -0.12910075897866108, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0028327261097729206}, {"id": 138, "seek": 93232, "start": 945.0400000000001, "end": 951.9200000000001, "text": " on the bottom of this slide, which tells us that it is so powerful that it is worth going to extreme", "tokens": [51000, 322, 264, 2767, 295, 341, 4137, 11, 597, 5112, 505, 300, 309, 307, 370, 4005, 300, 309, 307, 3163, 516, 281, 8084, 51344], "temperature": 0.0, "avg_logprob": -0.12910075897866108, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0028327261097729206}, {"id": 139, "seek": 93232, "start": 951.9200000000001, "end": 960.08, "text": " lengths to achieve. So this is what category theory is. It's the study of compositionality,", "tokens": [51344, 26329, 281, 4584, 13, 407, 341, 307, 437, 7719, 5261, 307, 13, 467, 311, 264, 2979, 295, 12686, 1860, 11, 51752], "temperature": 0.0, "avg_logprob": -0.12910075897866108, "compression_ratio": 1.6212765957446809, "no_speech_prob": 0.0028327261097729206}, {"id": 140, "seek": 96008, "start": 960.1600000000001, "end": 965.5200000000001, "text": " of how we can put systems together. And to my surprise, when I started to learn it,", "tokens": [50368, 295, 577, 321, 393, 829, 3652, 1214, 13, 400, 281, 452, 6365, 11, 562, 286, 1409, 281, 1466, 309, 11, 50636], "temperature": 0.0, "avg_logprob": -0.08543233341640896, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.0025446200743317604}, {"id": 141, "seek": 96008, "start": 965.5200000000001, "end": 971.5200000000001, "text": " it's not really just about functions. Functions were an example here that is simple. But category", "tokens": [50636, 309, 311, 406, 534, 445, 466, 6828, 13, 11166, 3916, 645, 364, 1365, 510, 300, 307, 2199, 13, 583, 7719, 50936], "temperature": 0.0, "avg_logprob": -0.08543233341640896, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.0025446200743317604}, {"id": 142, "seek": 96008, "start": 971.5200000000001, "end": 977.2800000000001, "text": " theory studies all sorts of complex systems, from trees on the top left, to networks on the top", "tokens": [50936, 5261, 5313, 439, 7527, 295, 3997, 3652, 11, 490, 5852, 322, 264, 1192, 1411, 11, 281, 9590, 322, 264, 1192, 51224], "temperature": 0.0, "avg_logprob": -0.08543233341640896, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.0025446200743317604}, {"id": 143, "seek": 96008, "start": 977.2800000000001, "end": 982.64, "text": " right, to circuits on the bottom left, to bi-directional transformations on the bottom right.", "tokens": [51224, 558, 11, 281, 26354, 322, 264, 2767, 1411, 11, 281, 3228, 12, 18267, 41048, 34852, 322, 264, 2767, 558, 13, 51492], "temperature": 0.0, "avg_logprob": -0.08543233341640896, "compression_ratio": 1.6787330316742082, "no_speech_prob": 0.0025446200743317604}, {"id": 144, "seek": 98264, "start": 983.4399999999999, "end": 991.84, "text": " So with this in mind, we can start describing what category theory is. Even though we're talking", "tokens": [50404, 407, 365, 341, 294, 1575, 11, 321, 393, 722, 16141, 437, 7719, 5261, 307, 13, 2754, 1673, 321, 434, 1417, 50824], "temperature": 0.0, "avg_logprob": -0.0909849484761556, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.001473752548918128}, {"id": 145, "seek": 98264, "start": 991.84, "end": 999.28, "text": " about all sorts of abstract systems and gadgets, category theory is still a precise mathematical", "tokens": [50824, 466, 439, 7527, 295, 12649, 3652, 293, 37635, 11, 7719, 5261, 307, 920, 257, 13600, 18894, 51196], "temperature": 0.0, "avg_logprob": -0.0909849484761556, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.001473752548918128}, {"id": 146, "seek": 98264, "start": 999.28, "end": 1006.96, "text": " language to talk about it. It's the kind of language that emphasizes relationships between", "tokens": [51196, 2856, 281, 751, 466, 309, 13, 467, 311, 264, 733, 295, 2856, 300, 48856, 6159, 1296, 51580], "temperature": 0.0, "avg_logprob": -0.0909849484761556, "compression_ratio": 1.5351351351351352, "no_speech_prob": 0.001473752548918128}, {"id": 147, "seek": 100696, "start": 1006.96, "end": 1013.12, "text": " concepts as opposed to concepts themselves. And we're going to see what that means shortly.", "tokens": [50364, 10392, 382, 8851, 281, 10392, 2969, 13, 400, 321, 434, 516, 281, 536, 437, 300, 1355, 13392, 13, 50672], "temperature": 0.0, "avg_logprob": -0.05526362321315668, "compression_ratio": 1.674641148325359, "no_speech_prob": 0.001948887249454856}, {"id": 148, "seek": 100696, "start": 1016.24, "end": 1022.1600000000001, "text": " And we're going to see that particular structures in category theory have", "tokens": [50828, 400, 321, 434, 516, 281, 536, 300, 1729, 9227, 294, 7719, 5261, 362, 51124], "temperature": 0.0, "avg_logprob": -0.05526362321315668, "compression_ratio": 1.674641148325359, "no_speech_prob": 0.001948887249454856}, {"id": 149, "seek": 100696, "start": 1022.1600000000001, "end": 1030.32, "text": " visual representations that aren't just doodles or sketches. They are former representations", "tokens": [51124, 5056, 33358, 300, 3212, 380, 445, 360, 35192, 420, 34547, 13, 814, 366, 5819, 33358, 51532], "temperature": 0.0, "avg_logprob": -0.05526362321315668, "compression_ratio": 1.674641148325359, "no_speech_prob": 0.001948887249454856}, {"id": 150, "seek": 100696, "start": 1030.32, "end": 1036.24, "text": " that can be manipulated, even in a computer. So to give you a sense of what this really is,", "tokens": [51532, 300, 393, 312, 37161, 11, 754, 294, 257, 3820, 13, 407, 281, 976, 291, 257, 2020, 295, 437, 341, 534, 307, 11, 51828], "temperature": 0.0, "avg_logprob": -0.05526362321315668, "compression_ratio": 1.674641148325359, "no_speech_prob": 0.001948887249454856}, {"id": 151, "seek": 103624, "start": 1036.24, "end": 1042.8, "text": " I'm going to have a very short demo where I'm going to just draw a few things. So I'm going to", "tokens": [50364, 286, 478, 516, 281, 362, 257, 588, 2099, 10723, 689, 286, 478, 516, 281, 445, 2642, 257, 1326, 721, 13, 407, 286, 478, 516, 281, 50692], "temperature": 0.0, "avg_logprob": -0.13065167835780553, "compression_ratio": 1.4885496183206106, "no_speech_prob": 0.0024564580526202917}, {"id": 152, "seek": 103624, "start": 1044.4, "end": 1048.48, "text": " switch to", "tokens": [50772, 3679, 281, 50976], "temperature": 0.0, "avg_logprob": -0.13065167835780553, "compression_ratio": 1.4885496183206106, "no_speech_prob": 0.0024564580526202917}, {"id": 153, "seek": 103624, "start": 1054.56, "end": 1064.24, "text": " my iPad and just give you a sense of how these things work. So I said category theory is a", "tokens": [51280, 452, 12945, 293, 445, 976, 291, 257, 2020, 295, 577, 613, 721, 589, 13, 407, 286, 848, 7719, 5261, 307, 257, 51764], "temperature": 0.0, "avg_logprob": -0.13065167835780553, "compression_ratio": 1.4885496183206106, "no_speech_prob": 0.0024564580526202917}, {"id": 154, "seek": 106424, "start": 1064.24, "end": 1072.96, "text": " unifying language for mathematics. And to appreciate what this means, we can perhaps focus", "tokens": [50364, 517, 5489, 2856, 337, 18666, 13, 400, 281, 4449, 437, 341, 1355, 11, 321, 393, 4317, 1879, 50800], "temperature": 0.0, "avg_logprob": -0.10684913663721796, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0011093801585957408}, {"id": 155, "seek": 106424, "start": 1072.96, "end": 1081.2, "text": " on some subfield of mathematics, like say group theory. So in group theory, we might want to", "tokens": [50800, 322, 512, 1422, 7610, 295, 18666, 11, 411, 584, 1594, 5261, 13, 407, 294, 1594, 5261, 11, 321, 1062, 528, 281, 51212], "temperature": 0.0, "avg_logprob": -0.10684913663721796, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0011093801585957408}, {"id": 156, "seek": 106424, "start": 1081.2, "end": 1088.16, "text": " study a particular group. And a central concept in group theory is that of a group homomorphism.", "tokens": [51212, 2979, 257, 1729, 1594, 13, 400, 257, 5777, 3410, 294, 1594, 5261, 307, 300, 295, 257, 1594, 3655, 32702, 1434, 13, 51560], "temperature": 0.0, "avg_logprob": -0.10684913663721796, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.0011093801585957408}, {"id": 157, "seek": 108816, "start": 1088.16, "end": 1095.0400000000002, "text": " If you have two groups, we can have a structure preserving a mapping between them. And then we", "tokens": [50364, 759, 291, 362, 732, 3935, 11, 321, 393, 362, 257, 3877, 33173, 257, 18350, 1296, 552, 13, 400, 550, 321, 50708], "temperature": 0.0, "avg_logprob": -0.10705518174445493, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.0015461778966709971}, {"id": 158, "seek": 108816, "start": 1095.0400000000002, "end": 1099.2, "text": " can have many mappings and they can go between groups themselves and so on. And there could be", "tokens": [50708, 393, 362, 867, 463, 28968, 293, 436, 393, 352, 1296, 3935, 2969, 293, 370, 322, 13, 400, 456, 727, 312, 50916], "temperature": 0.0, "avg_logprob": -0.10705518174445493, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.0015461778966709971}, {"id": 159, "seek": 108816, "start": 1099.2, "end": 1103.52, "text": " many groups that we want to study. Maybe some of them are not connected and so on.", "tokens": [50916, 867, 3935, 300, 321, 528, 281, 2979, 13, 2704, 512, 295, 552, 366, 406, 4582, 293, 370, 322, 13, 51132], "temperature": 0.0, "avg_logprob": -0.10705518174445493, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.0015461778966709971}, {"id": 160, "seek": 108816, "start": 1110.0, "end": 1114.72, "text": " And we can study about various properties of these groups and study how they behave. Now,", "tokens": [51456, 400, 321, 393, 2979, 466, 3683, 7221, 295, 613, 3935, 293, 2979, 577, 436, 15158, 13, 823, 11, 51692], "temperature": 0.0, "avg_logprob": -0.10705518174445493, "compression_ratio": 1.8190954773869348, "no_speech_prob": 0.0015461778966709971}, {"id": 161, "seek": 111472, "start": 1114.72, "end": 1123.68, "text": " separately, in set theory, we might have some set and we might study functions between set one", "tokens": [50364, 14759, 11, 294, 992, 5261, 11, 321, 1062, 362, 512, 992, 293, 321, 1062, 2979, 6828, 1296, 992, 472, 50812], "temperature": 0.0, "avg_logprob": -0.11939614948473479, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0005021077231504023}, {"id": 162, "seek": 111472, "start": 1123.68, "end": 1129.52, "text": " and set two. These are really honest to God functions that we can compose. And maybe there's", "tokens": [50812, 293, 992, 732, 13, 1981, 366, 534, 3245, 281, 1265, 6828, 300, 321, 393, 35925, 13, 400, 1310, 456, 311, 51104], "temperature": 0.0, "avg_logprob": -0.11939614948473479, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0005021077231504023}, {"id": 163, "seek": 111472, "start": 1130.16, "end": 1137.04, "text": " S3, S4. And there's various kinds of structures that we can study between them. It's a very rich", "tokens": [51136, 318, 18, 11, 318, 19, 13, 400, 456, 311, 3683, 3685, 295, 9227, 300, 321, 393, 2979, 1296, 552, 13, 467, 311, 257, 588, 4593, 51480], "temperature": 0.0, "avg_logprob": -0.11939614948473479, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0005021077231504023}, {"id": 164, "seek": 111472, "start": 1137.04, "end": 1141.68, "text": " and intricate field. And I'm going to write here set and I'm going to write here group.", "tokens": [51480, 293, 38015, 2519, 13, 400, 286, 478, 516, 281, 2464, 510, 992, 293, 286, 478, 516, 281, 2464, 510, 1594, 13, 51712], "temperature": 0.0, "avg_logprob": -0.11939614948473479, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.0005021077231504023}, {"id": 165, "seek": 114168, "start": 1142.64, "end": 1149.8400000000001, "text": " Then again, if we're studying, for instance, we might study vector spaces or similar things. So", "tokens": [50412, 1396, 797, 11, 498, 321, 434, 7601, 11, 337, 5197, 11, 321, 1062, 2979, 8062, 7673, 420, 2531, 721, 13, 407, 50772], "temperature": 0.0, "avg_logprob": -0.13546864739779768, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0007908117840997875}, {"id": 166, "seek": 114168, "start": 1149.8400000000001, "end": 1154.3200000000002, "text": " for instance, I have a vector space one and a vector space two. I can still study some sort", "tokens": [50772, 337, 5197, 11, 286, 362, 257, 8062, 1901, 472, 293, 257, 8062, 1901, 732, 13, 286, 393, 920, 2979, 512, 1333, 50996], "temperature": 0.0, "avg_logprob": -0.13546864739779768, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0007908117840997875}, {"id": 167, "seek": 114168, "start": 1154.3200000000002, "end": 1159.28, "text": " of structure preserving mapping between these, between these other structures. And for instance,", "tokens": [50996, 295, 3877, 33173, 18350, 1296, 613, 11, 1296, 613, 661, 9227, 13, 400, 337, 5197, 11, 51244], "temperature": 0.0, "avg_logprob": -0.13546864739779768, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0007908117840997875}, {"id": 168, "seek": 114168, "start": 1159.28, "end": 1166.88, "text": " this could be just merely vector spaces in a field R. Now, what category theory does is", "tokens": [51244, 341, 727, 312, 445, 17003, 8062, 7673, 294, 257, 2519, 497, 13, 823, 11, 437, 7719, 5261, 775, 307, 51624], "temperature": 0.0, "avg_logprob": -0.13546864739779768, "compression_ratio": 1.7799043062200957, "no_speech_prob": 0.0007908117840997875}, {"id": 169, "seek": 116688, "start": 1167.68, "end": 1173.0400000000002, "text": " gives us this bird eye view of these things. All of these structures are categories. So here", "tokens": [50404, 2709, 505, 341, 5255, 3313, 1910, 295, 613, 721, 13, 1057, 295, 613, 9227, 366, 10479, 13, 407, 510, 50672], "temperature": 0.0, "avg_logprob": -0.12019052850194724, "compression_ratio": 1.9619565217391304, "no_speech_prob": 0.0012632685247808695}, {"id": 170, "seek": 116688, "start": 1173.0400000000002, "end": 1178.0800000000002, "text": " we have a category of groups. Here we have a category of sets. And here we have a category", "tokens": [50672, 321, 362, 257, 7719, 295, 3935, 13, 1692, 321, 362, 257, 7719, 295, 6352, 13, 400, 510, 321, 362, 257, 7719, 50924], "temperature": 0.0, "avg_logprob": -0.12019052850194724, "compression_ratio": 1.9619565217391304, "no_speech_prob": 0.0012632685247808695}, {"id": 171, "seek": 116688, "start": 1178.0800000000002, "end": 1188.8000000000002, "text": " of vector spaces. And all of these structures are special examples of category teams. The same way", "tokens": [50924, 295, 8062, 7673, 13, 400, 439, 295, 613, 9227, 366, 2121, 5110, 295, 7719, 5491, 13, 440, 912, 636, 51460], "temperature": 0.0, "avg_logprob": -0.12019052850194724, "compression_ratio": 1.9619565217391304, "no_speech_prob": 0.0012632685247808695}, {"id": 172, "seek": 116688, "start": 1188.8000000000002, "end": 1194.72, "text": " we have studied structure preserving maps between groups, group homomorphisms,", "tokens": [51460, 321, 362, 9454, 3877, 33173, 11317, 1296, 3935, 11, 1594, 3655, 32702, 13539, 11, 51756], "temperature": 0.0, "avg_logprob": -0.12019052850194724, "compression_ratio": 1.9619565217391304, "no_speech_prob": 0.0012632685247808695}, {"id": 173, "seek": 119472, "start": 1195.6000000000001, "end": 1200.16, "text": " by formulating all the groups as a category. Now we can study structure preserving maps", "tokens": [50408, 538, 1254, 12162, 439, 264, 3935, 382, 257, 7719, 13, 823, 321, 393, 2979, 3877, 33173, 11317, 50636], "temperature": 0.0, "avg_logprob": -0.09577921162480893, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.00036218363675288856}, {"id": 174, "seek": 119472, "start": 1200.8, "end": 1208.4, "text": " of these categories, which are called functors. So for instance, we might have a functor from", "tokens": [50668, 295, 613, 10479, 11, 597, 366, 1219, 1019, 5547, 13, 407, 337, 5197, 11, 321, 1062, 362, 257, 1019, 1672, 490, 51048], "temperature": 0.0, "avg_logprob": -0.09577921162480893, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.00036218363675288856}, {"id": 175, "seek": 119472, "start": 1208.4, "end": 1213.28, "text": " the category of groups to the category of sets, which takes a group and gives us the underlying", "tokens": [51048, 264, 7719, 295, 3935, 281, 264, 7719, 295, 6352, 11, 597, 2516, 257, 1594, 293, 2709, 505, 264, 14217, 51292], "temperature": 0.0, "avg_logprob": -0.09577921162480893, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.00036218363675288856}, {"id": 176, "seek": 119472, "start": 1213.28, "end": 1219.1200000000001, "text": " set. Or we might have a functor that takes a set and gives us the free vector space on a set and so", "tokens": [51292, 992, 13, 1610, 321, 1062, 362, 257, 1019, 1672, 300, 2516, 257, 992, 293, 2709, 505, 264, 1737, 8062, 1901, 322, 257, 992, 293, 370, 51584], "temperature": 0.0, "avg_logprob": -0.09577921162480893, "compression_ratio": 1.8663366336633664, "no_speech_prob": 0.00036218363675288856}, {"id": 177, "seek": 121912, "start": 1219.12, "end": 1227.84, "text": " on. The idea being that once we have lifted ourselves to this abstract level, a lot of", "tokens": [50364, 322, 13, 440, 1558, 885, 300, 1564, 321, 362, 17854, 4175, 281, 341, 12649, 1496, 11, 257, 688, 295, 50800], "temperature": 0.0, "avg_logprob": -0.1276516863094863, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.003319589653983712}, {"id": 178, "seek": 121912, "start": 1227.84, "end": 1233.6799999999998, "text": " new things open up to us. So what we end up studying is a lot of interesting things like", "tokens": [50800, 777, 721, 1269, 493, 281, 505, 13, 407, 437, 321, 917, 493, 7601, 307, 257, 688, 295, 1880, 721, 411, 51092], "temperature": 0.0, "avg_logprob": -0.1276516863094863, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.003319589653983712}, {"id": 179, "seek": 121912, "start": 1233.6799999999998, "end": 1240.8799999999999, "text": " the category of, let's say, let's call them systems with a particular interface, input and", "tokens": [51092, 264, 7719, 295, 11, 718, 311, 584, 11, 718, 311, 818, 552, 3652, 365, 257, 1729, 9226, 11, 4846, 293, 51452], "temperature": 0.0, "avg_logprob": -0.1276516863094863, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.003319589653983712}, {"id": 180, "seek": 121912, "start": 1240.8799999999999, "end": 1246.08, "text": " output. And we can have a system with internal state S1 and S2. And we can say the ways these", "tokens": [51452, 5598, 13, 400, 321, 393, 362, 257, 1185, 365, 6920, 1785, 318, 16, 293, 318, 17, 13, 400, 321, 393, 584, 264, 2098, 613, 51712], "temperature": 0.0, "avg_logprob": -0.1276516863094863, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.003319589653983712}, {"id": 181, "seek": 124608, "start": 1246.08, "end": 1250.72, "text": " are related. And then we can study categories of systems with a different interface and map", "tokens": [50364, 366, 4077, 13, 400, 550, 321, 393, 2979, 10479, 295, 3652, 365, 257, 819, 9226, 293, 4471, 50596], "temperature": 0.0, "avg_logprob": -0.06262969374656677, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0009086841600947082}, {"id": 182, "seek": 124608, "start": 1250.72, "end": 1257.6, "text": " between them and so on. And these systems can be actual processes, computations that do something", "tokens": [50596, 1296, 552, 293, 370, 322, 13, 400, 613, 3652, 393, 312, 3539, 7555, 11, 2807, 763, 300, 360, 746, 50940], "temperature": 0.0, "avg_logprob": -0.06262969374656677, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0009086841600947082}, {"id": 183, "seek": 124608, "start": 1257.6, "end": 1262.96, "text": " very interesting. That isn't just sort of what you might think of as strictly mathematical.", "tokens": [50940, 588, 1880, 13, 663, 1943, 380, 445, 1333, 295, 437, 291, 1062, 519, 295, 382, 20792, 18894, 13, 51208], "temperature": 0.0, "avg_logprob": -0.06262969374656677, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0009086841600947082}, {"id": 184, "seek": 124608, "start": 1265.36, "end": 1270.48, "text": " And lastly, we might study categories. These categories might have a lot of interesting", "tokens": [51328, 400, 16386, 11, 321, 1062, 2979, 10479, 13, 1981, 10479, 1062, 362, 257, 688, 295, 1880, 51584], "temperature": 0.0, "avg_logprob": -0.06262969374656677, "compression_ratio": 1.716279069767442, "no_speech_prob": 0.0009086841600947082}, {"id": 185, "seek": 127048, "start": 1270.48, "end": 1277.04, "text": " structure. For example, I've drawn here a monoidal category where we can sort of put", "tokens": [50364, 3877, 13, 1171, 1365, 11, 286, 600, 10117, 510, 257, 1108, 17079, 304, 7719, 689, 321, 393, 1333, 295, 829, 50692], "temperature": 0.0, "avg_logprob": -0.09839790858579485, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0028725932352244854}, {"id": 186, "seek": 127048, "start": 1277.04, "end": 1287.28, "text": " two objects in parallel, as opposed to just composing maps sequentially. And then in category", "tokens": [50692, 732, 6565, 294, 8952, 11, 382, 8851, 281, 445, 715, 6110, 11317, 5123, 3137, 13, 400, 550, 294, 7719, 51204], "temperature": 0.0, "avg_logprob": -0.09839790858579485, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0028725932352244854}, {"id": 187, "seek": 127048, "start": 1287.28, "end": 1292.32, "text": " theory where it turns out that these monoidal categories, that's what they're called, where", "tokens": [51204, 5261, 689, 309, 4523, 484, 300, 613, 1108, 17079, 304, 10479, 11, 300, 311, 437, 436, 434, 1219, 11, 689, 51456], "temperature": 0.0, "avg_logprob": -0.09839790858579485, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0028725932352244854}, {"id": 188, "seek": 127048, "start": 1292.32, "end": 1297.3600000000001, "text": " we can put things in parallel, have a formal visual representation. So the story is a bit more", "tokens": [51456, 321, 393, 829, 721, 294, 8952, 11, 362, 257, 9860, 5056, 10290, 13, 407, 264, 1657, 307, 257, 857, 544, 51708], "temperature": 0.0, "avg_logprob": -0.09839790858579485, "compression_ratio": 1.6515837104072397, "no_speech_prob": 0.0028725932352244854}, {"id": 189, "seek": 129736, "start": 1297.36, "end": 1302.9599999999998, "text": " intricate, but you can think of it as taking this category and mapping it into a visual space", "tokens": [50364, 38015, 11, 457, 291, 393, 519, 295, 309, 382, 1940, 341, 7719, 293, 18350, 309, 666, 257, 5056, 1901, 50644], "temperature": 0.0, "avg_logprob": -0.10437144771698982, "compression_ratio": 1.4885057471264367, "no_speech_prob": 0.007043441757559776}, {"id": 190, "seek": 129736, "start": 1302.9599999999998, "end": 1309.28, "text": " where each morphism has a specific shape. And you can draw it as boxes. So now these boxes", "tokens": [50644, 689, 1184, 25778, 1434, 575, 257, 2685, 3909, 13, 400, 291, 393, 2642, 309, 382, 9002, 13, 407, 586, 613, 9002, 50960], "temperature": 0.0, "avg_logprob": -0.10437144771698982, "compression_ratio": 1.4885057471264367, "no_speech_prob": 0.007043441757559776}, {"id": 191, "seek": 129736, "start": 1309.28, "end": 1313.28, "text": " aren't just sketches and doodles, but they're formal mathematical objects.", "tokens": [50960, 3212, 380, 445, 34547, 293, 360, 35192, 11, 457, 436, 434, 9860, 18894, 6565, 13, 51160], "temperature": 0.0, "avg_logprob": -0.10437144771698982, "compression_ratio": 1.4885057471264367, "no_speech_prob": 0.007043441757559776}, {"id": 192, "seek": 132736, "start": 1327.4399999999998, "end": 1343.52, "text": " So let me just switch to my other screen. Right, so this was merely aimed to paint a picture of how", "tokens": [50368, 407, 718, 385, 445, 3679, 281, 452, 661, 2568, 13, 1779, 11, 370, 341, 390, 17003, 20540, 281, 4225, 257, 3036, 295, 577, 51172], "temperature": 0.0, "avg_logprob": -0.15284611384073893, "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.0015228315023705363}, {"id": 193, "seek": 132736, "start": 1343.52, "end": 1349.76, "text": " these things look. And finishing with the monoidal categories in this demo,", "tokens": [51172, 613, 721, 574, 13, 400, 12693, 365, 264, 1108, 17079, 304, 10479, 294, 341, 10723, 11, 51484], "temperature": 0.0, "avg_logprob": -0.15284611384073893, "compression_ratio": 1.3461538461538463, "no_speech_prob": 0.0015228315023705363}, {"id": 194, "seek": 134976, "start": 1350.16, "end": 1360.8799999999999, "text": " these allow us to have a very natural visual language for talking about systems and processes.", "tokens": [50384, 613, 2089, 505, 281, 362, 257, 588, 3303, 5056, 2856, 337, 1417, 466, 3652, 293, 7555, 13, 50920], "temperature": 0.0, "avg_logprob": -0.13887786865234375, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0014087636955082417}, {"id": 195, "seek": 134976, "start": 1360.8799999999999, "end": 1365.92, "text": " And these are really, this is an example of what might, of a particular process of making a pie.", "tokens": [50920, 400, 613, 366, 534, 11, 341, 307, 364, 1365, 295, 437, 1062, 11, 295, 257, 1729, 1399, 295, 1455, 257, 1730, 13, 51172], "temperature": 0.0, "avg_logprob": -0.13887786865234375, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0014087636955082417}, {"id": 196, "seek": 134976, "start": 1365.92, "end": 1372.56, "text": " And we've all seen these things. So this is not something new. But it allows us, and it gives", "tokens": [51172, 400, 321, 600, 439, 1612, 613, 721, 13, 407, 341, 307, 406, 746, 777, 13, 583, 309, 4045, 505, 11, 293, 309, 2709, 51504], "temperature": 0.0, "avg_logprob": -0.13887786865234375, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0014087636955082417}, {"id": 197, "seek": 134976, "start": 1372.56, "end": 1379.36, "text": " us a very different perspective on these things. So when you're doing category theory, you might", "tokens": [51504, 505, 257, 588, 819, 4585, 322, 613, 721, 13, 407, 562, 291, 434, 884, 7719, 5261, 11, 291, 1062, 51844], "temperature": 0.0, "avg_logprob": -0.13887786865234375, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0014087636955082417}, {"id": 198, "seek": 137936, "start": 1379.36, "end": 1384.1599999999999, "text": " be studying various sorts of systems and processes that do various sorts of things. And", "tokens": [50364, 312, 7601, 3683, 7527, 295, 3652, 293, 7555, 300, 360, 3683, 7527, 295, 721, 13, 400, 50604], "temperature": 0.0, "avg_logprob": -0.06214867697821723, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0016144083347171545}, {"id": 199, "seek": 137936, "start": 1384.1599999999999, "end": 1388.8799999999999, "text": " in category theory, there's concepts discovered or invented by various people for studying these", "tokens": [50604, 294, 7719, 5261, 11, 456, 311, 10392, 6941, 420, 14479, 538, 3683, 561, 337, 7601, 613, 50840], "temperature": 0.0, "avg_logprob": -0.06214867697821723, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0016144083347171545}, {"id": 200, "seek": 137936, "start": 1388.8799999999999, "end": 1394.4799999999998, "text": " things. So if we're studying resources or processes in this way, we might use monoidal categories.", "tokens": [50840, 721, 13, 407, 498, 321, 434, 7601, 3593, 420, 7555, 294, 341, 636, 11, 321, 1062, 764, 1108, 17079, 304, 10479, 13, 51120], "temperature": 0.0, "avg_logprob": -0.06214867697821723, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0016144083347171545}, {"id": 201, "seek": 137936, "start": 1396.9599999999998, "end": 1402.56, "text": " If we want to study probability, for instance, we will use Markov categories. So for instance,", "tokens": [51244, 759, 321, 528, 281, 2979, 8482, 11, 337, 5197, 11, 321, 486, 764, 3934, 5179, 10479, 13, 407, 337, 5197, 11, 51524], "temperature": 0.0, "avg_logprob": -0.06214867697821723, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0016144083347171545}, {"id": 202, "seek": 137936, "start": 1402.56, "end": 1407.04, "text": " on the left side, here we see a process which takes an input, applies a function to it,", "tokens": [51524, 322, 264, 1411, 1252, 11, 510, 321, 536, 257, 1399, 597, 2516, 364, 4846, 11, 13165, 257, 2445, 281, 309, 11, 51748], "temperature": 0.0, "avg_logprob": -0.06214867697821723, "compression_ratio": 1.8132295719844358, "no_speech_prob": 0.0016144083347171545}, {"id": 203, "seek": 140704, "start": 1407.52, "end": 1414.24, "text": " and then copies the result. On the right, we first copy the result and then apply f", "tokens": [50388, 293, 550, 14341, 264, 1874, 13, 1282, 264, 558, 11, 321, 700, 5055, 264, 1874, 293, 550, 3079, 283, 50724], "temperature": 0.0, "avg_logprob": -0.08164820107080603, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0007307538762688637}, {"id": 204, "seek": 140704, "start": 1414.24, "end": 1421.12, "text": " individually on inputs. Now, if f is merely a function, then these two processes are equal.", "tokens": [50724, 16652, 322, 15743, 13, 823, 11, 498, 283, 307, 17003, 257, 2445, 11, 550, 613, 732, 7555, 366, 2681, 13, 51068], "temperature": 0.0, "avg_logprob": -0.08164820107080603, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0007307538762688637}, {"id": 205, "seek": 140704, "start": 1421.12, "end": 1426.8799999999999, "text": " And in many other cases, they're equal. But not all of them. If f is a stochastic function,", "tokens": [51068, 400, 294, 867, 661, 3331, 11, 436, 434, 2681, 13, 583, 406, 439, 295, 552, 13, 759, 283, 307, 257, 342, 8997, 2750, 2445, 11, 51356], "temperature": 0.0, "avg_logprob": -0.08164820107080603, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0007307538762688637}, {"id": 206, "seek": 140704, "start": 1427.68, "end": 1432.08, "text": " then these are not equal because rolling a dice and then copying the result doesn't give us the", "tokens": [51396, 550, 613, 366, 406, 2681, 570, 9439, 257, 10313, 293, 550, 27976, 264, 1874, 1177, 380, 976, 505, 264, 51616], "temperature": 0.0, "avg_logprob": -0.08164820107080603, "compression_ratio": 1.7203791469194314, "no_speech_prob": 0.0007307538762688637}, {"id": 207, "seek": 143208, "start": 1432.08, "end": 1437.84, "text": " same result as rolling two dice. So Markov categories allow us to describe such processes", "tokens": [50364, 912, 1874, 382, 9439, 732, 10313, 13, 407, 3934, 5179, 10479, 2089, 505, 281, 6786, 1270, 7555, 50652], "temperature": 0.0, "avg_logprob": -0.09027834499583524, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.002203624462708831}, {"id": 208, "seek": 143208, "start": 1437.84, "end": 1443.76, "text": " at the needed level of generality. Then we might, if you want to study how local systems,", "tokens": [50652, 412, 264, 2978, 1496, 295, 1337, 1860, 13, 1396, 321, 1062, 11, 498, 291, 528, 281, 2979, 577, 2654, 3652, 11, 50948], "temperature": 0.0, "avg_logprob": -0.09027834499583524, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.002203624462708831}, {"id": 209, "seek": 143208, "start": 1443.76, "end": 1448.8799999999999, "text": " how behavior of local systems gives rise to the behavior of a global system,", "tokens": [50948, 577, 5223, 295, 2654, 3652, 2709, 6272, 281, 264, 5223, 295, 257, 4338, 1185, 11, 51204], "temperature": 0.0, "avg_logprob": -0.09027834499583524, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.002203624462708831}, {"id": 210, "seek": 143208, "start": 1448.8799999999999, "end": 1454.72, "text": " you might want to use sheeps. Then again, if you want to study bi-directional transformations,", "tokens": [51204, 291, 1062, 528, 281, 764, 750, 10653, 13, 1396, 797, 11, 498, 291, 528, 281, 2979, 3228, 12, 18267, 41048, 34852, 11, 51496], "temperature": 0.0, "avg_logprob": -0.09027834499583524, "compression_ratio": 1.7121951219512195, "no_speech_prob": 0.002203624462708831}, {"id": 211, "seek": 145472, "start": 1454.8, "end": 1462.8, "text": " such as extracting a row from a database, then updating back, so forward and backward.", "tokens": [50368, 1270, 382, 49844, 257, 5386, 490, 257, 8149, 11, 550, 25113, 646, 11, 370, 2128, 293, 23897, 13, 50768], "temperature": 0.0, "avg_logprob": -0.12246939502184905, "compression_ratio": 1.675, "no_speech_prob": 0.0008151671499945223}, {"id": 212, "seek": 145472, "start": 1462.8, "end": 1467.3600000000001, "text": " Or if you want to study neural networks, a forward pass and a backward pass,", "tokens": [50768, 1610, 498, 291, 528, 281, 2979, 18161, 9590, 11, 257, 2128, 1320, 293, 257, 23897, 1320, 11, 50996], "temperature": 0.0, "avg_logprob": -0.12246939502184905, "compression_ratio": 1.675, "no_speech_prob": 0.0008151671499945223}, {"id": 213, "seek": 145472, "start": 1468.16, "end": 1471.84, "text": " we might want to use optics and lenses, which are bi-directional data structures.", "tokens": [51036, 321, 1062, 528, 281, 764, 42599, 293, 18059, 11, 597, 366, 3228, 12, 18267, 41048, 1412, 9227, 13, 51220], "temperature": 0.0, "avg_logprob": -0.12246939502184905, "compression_ratio": 1.675, "no_speech_prob": 0.0008151671499945223}, {"id": 214, "seek": 145472, "start": 1474.48, "end": 1479.04, "text": " Then again, if you want to study contextual computation or computation with side effects,", "tokens": [51352, 1396, 797, 11, 498, 291, 528, 281, 2979, 35526, 24903, 420, 24903, 365, 1252, 5065, 11, 51580], "temperature": 0.0, "avg_logprob": -0.12246939502184905, "compression_ratio": 1.675, "no_speech_prob": 0.0008151671499945223}, {"id": 215, "seek": 147904, "start": 1479.04, "end": 1482.96, "text": " we might want to use things called monads or their Kleisler categories.", "tokens": [50364, 321, 1062, 528, 281, 764, 721, 1219, 1108, 5834, 420, 641, 17053, 271, 1918, 10479, 13, 50560], "temperature": 0.0, "avg_logprob": -0.12865258385153377, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.001979995984584093}, {"id": 216, "seek": 147904, "start": 1484.6399999999999, "end": 1491.2, "text": " And of course, in computer science, the concept permeating it is recursion. And then we might", "tokens": [50644, 400, 295, 1164, 11, 294, 3820, 3497, 11, 264, 3410, 30287, 990, 309, 307, 20560, 313, 13, 400, 550, 321, 1062, 50972], "temperature": 0.0, "avg_logprob": -0.12865258385153377, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.001979995984584093}, {"id": 217, "seek": 147904, "start": 1491.2, "end": 1497.6, "text": " want to use things called final co-algebras or initial algebras to study recursive processes.", "tokens": [50972, 528, 281, 764, 721, 1219, 2572, 598, 12, 304, 432, 38182, 420, 5883, 419, 432, 38182, 281, 2979, 20560, 488, 7555, 13, 51292], "temperature": 0.0, "avg_logprob": -0.12865258385153377, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.001979995984584093}, {"id": 218, "seek": 147904, "start": 1500.8, "end": 1505.84, "text": " And once we study all of these things, we might find ourselves in this situation", "tokens": [51452, 400, 1564, 321, 2979, 439, 295, 613, 721, 11, 321, 1062, 915, 4175, 294, 341, 2590, 51704], "temperature": 0.0, "avg_logprob": -0.12865258385153377, "compression_ratio": 1.8085106382978724, "no_speech_prob": 0.001979995984584093}, {"id": 219, "seek": 150584, "start": 1506.48, "end": 1512.8799999999999, "text": " where we study a bunch of different kinds of systems with different kinds of categories and", "tokens": [50396, 689, 321, 2979, 257, 3840, 295, 819, 3685, 295, 3652, 365, 819, 3685, 295, 10479, 293, 50716], "temperature": 0.0, "avg_logprob": -0.1200474658644343, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.0012601695489138365}, {"id": 220, "seek": 150584, "start": 1512.8799999999999, "end": 1517.6, "text": " concepts. And that ends up just creating more categories and more concepts. And the theory", "tokens": [50716, 10392, 13, 400, 300, 5314, 493, 445, 4084, 544, 10479, 293, 544, 10392, 13, 400, 264, 5261, 50952], "temperature": 0.0, "avg_logprob": -0.1200474658644343, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.0012601695489138365}, {"id": 221, "seek": 150584, "start": 1517.6, "end": 1523.6, "text": " that studies category theory is called two-category theory. So something I love about the way you", "tokens": [50952, 300, 5313, 7719, 5261, 307, 1219, 732, 12, 66, 48701, 5261, 13, 407, 746, 286, 959, 466, 264, 636, 291, 51252], "temperature": 0.0, "avg_logprob": -0.1200474658644343, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.0012601695489138365}, {"id": 222, "seek": 150584, "start": 1523.6, "end": 1531.1999999999998, "text": " approach CT, that it is very meta and recursive in nature, allowing us to describe category theory", "tokens": [51252, 3109, 19529, 11, 300, 309, 307, 588, 19616, 293, 20560, 488, 294, 3687, 11, 8293, 505, 281, 6786, 7719, 5261, 51632], "temperature": 0.0, "avg_logprob": -0.1200474658644343, "compression_ratio": 1.813397129186603, "no_speech_prob": 0.0012601695489138365}, {"id": 223, "seek": 153120, "start": 1531.2, "end": 1540.64, "text": " using category theory. And then really, once you start going with this, you might have heard about", "tokens": [50364, 1228, 7719, 5261, 13, 400, 550, 534, 11, 1564, 291, 722, 516, 365, 341, 11, 291, 1062, 362, 2198, 466, 50836], "temperature": 0.0, "avg_logprob": -0.11199045181274414, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001095436979085207}, {"id": 224, "seek": 153120, "start": 1540.64, "end": 1546.64, "text": " classical mathematical structures such as topological spaces or rings or fields. And once", "tokens": [50836, 13735, 18894, 9227, 1270, 382, 1192, 4383, 7673, 420, 11136, 420, 7909, 13, 400, 1564, 51136], "temperature": 0.0, "avg_logprob": -0.11199045181274414, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001095436979085207}, {"id": 225, "seek": 153120, "start": 1546.64, "end": 1553.1200000000001, "text": " you start going into category theory, it's hard to appreciate how deep and rich the field is,", "tokens": [51136, 291, 722, 516, 666, 7719, 5261, 11, 309, 311, 1152, 281, 4449, 577, 2452, 293, 4593, 264, 2519, 307, 11, 51460], "temperature": 0.0, "avg_logprob": -0.11199045181274414, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001095436979085207}, {"id": 226, "seek": 153120, "start": 1553.1200000000001, "end": 1559.68, "text": " because the water keeps getting deeper and deeper and the rabbit hole hides so many concepts that", "tokens": [51460, 570, 264, 1281, 5965, 1242, 7731, 293, 7731, 293, 264, 19509, 5458, 35953, 370, 867, 10392, 300, 51788], "temperature": 0.0, "avg_logprob": -0.11199045181274414, "compression_ratio": 1.7511520737327189, "no_speech_prob": 0.001095436979085207}, {"id": 227, "seek": 155968, "start": 1559.68, "end": 1566.0800000000002, "text": " I personally wasn't aware of that have a very expressive nature and a very compositional nature", "tokens": [50364, 286, 5665, 2067, 380, 3650, 295, 300, 362, 257, 588, 40189, 3687, 293, 257, 588, 10199, 2628, 3687, 50684], "temperature": 0.0, "avg_logprob": -0.08490508510952904, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.00021602862398140132}, {"id": 228, "seek": 155968, "start": 1566.0800000000002, "end": 1571.6000000000001, "text": " in helping us understand all of the systems. This is certainly not the end. There's many more", "tokens": [50684, 294, 4315, 505, 1223, 439, 295, 264, 3652, 13, 639, 307, 3297, 406, 264, 917, 13, 821, 311, 867, 544, 50960], "temperature": 0.0, "avg_logprob": -0.08490508510952904, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.00021602862398140132}, {"id": 229, "seek": 155968, "start": 1571.6000000000001, "end": 1577.6000000000001, "text": " that every category theorist has a certain understanding of a part of these concepts.", "tokens": [50960, 300, 633, 7719, 27423, 468, 575, 257, 1629, 3701, 295, 257, 644, 295, 613, 10392, 13, 51260], "temperature": 0.0, "avg_logprob": -0.08490508510952904, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.00021602862398140132}, {"id": 230, "seek": 155968, "start": 1580.16, "end": 1588.16, "text": " So this was very abstract in a sense, but I hope it conveyed some sense of what category theories", "tokens": [51388, 407, 341, 390, 588, 12649, 294, 257, 2020, 11, 457, 286, 1454, 309, 49340, 512, 2020, 295, 437, 7719, 13667, 51788], "temperature": 0.0, "avg_logprob": -0.08490508510952904, "compression_ratio": 1.6954545454545455, "no_speech_prob": 0.00021602862398140132}, {"id": 231, "seek": 158816, "start": 1589.1200000000001, "end": 1595.92, "text": " are. But now I would like to make this a bit more concrete and study what is the relationship", "tokens": [50412, 366, 13, 583, 586, 286, 576, 411, 281, 652, 341, 257, 857, 544, 9859, 293, 2979, 437, 307, 264, 2480, 50752], "temperature": 0.0, "avg_logprob": -0.09447690418788365, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.0007653907523490489}, {"id": 232, "seek": 158816, "start": 1595.92, "end": 1604.16, "text": " between category theory and deep learning? What has been done? How much are these concepts", "tokens": [50752, 1296, 7719, 5261, 293, 2452, 2539, 30, 708, 575, 668, 1096, 30, 1012, 709, 366, 613, 10392, 51164], "temperature": 0.0, "avg_logprob": -0.09447690418788365, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.0007653907523490489}, {"id": 233, "seek": 158816, "start": 1604.16, "end": 1611.68, "text": " actually connected to each other? So the thing I want to establish first is that the deep learning", "tokens": [51164, 767, 4582, 281, 1184, 661, 30, 407, 264, 551, 286, 528, 281, 8327, 700, 307, 300, 264, 2452, 2539, 51540], "temperature": 0.0, "avg_logprob": -0.09447690418788365, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.0007653907523490489}, {"id": 234, "seek": 158816, "start": 1611.68, "end": 1617.76, "text": " community seems to be very well aware of the concept of compositionality. I have found numerous", "tokens": [51540, 1768, 2544, 281, 312, 588, 731, 3650, 295, 264, 3410, 295, 12686, 1860, 13, 286, 362, 1352, 12546, 51844], "temperature": 0.0, "avg_logprob": -0.09447690418788365, "compression_ratio": 1.6059322033898304, "no_speech_prob": 0.0007653907523490489}, {"id": 235, "seek": 161776, "start": 1617.84, "end": 1624.24, "text": " workshops, programs, blog posts, lectures saying compositionality is important.", "tokens": [50368, 19162, 11, 4268, 11, 6968, 12300, 11, 16564, 1566, 12686, 1860, 307, 1021, 13, 50688], "temperature": 0.0, "avg_logprob": -0.13701249493492973, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0013572960160672665}, {"id": 236, "seek": 161776, "start": 1625.52, "end": 1628.4, "text": " And I've linked a few of them here. I'm sure there's many more.", "tokens": [50752, 400, 286, 600, 9408, 257, 1326, 295, 552, 510, 13, 286, 478, 988, 456, 311, 867, 544, 13, 50896], "temperature": 0.0, "avg_logprob": -0.13701249493492973, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0013572960160672665}, {"id": 237, "seek": 161776, "start": 1630.08, "end": 1635.6, "text": " Joshua Bengio in his Turing lecture actually explicitly states that we need to build", "tokens": [50980, 24005, 3964, 17862, 294, 702, 314, 1345, 7991, 767, 20803, 4368, 300, 321, 643, 281, 1322, 51256], "temperature": 0.0, "avg_logprob": -0.13701249493492973, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0013572960160672665}, {"id": 238, "seek": 161776, "start": 1635.6, "end": 1644.32, "text": " compositionality into our machine learning models. But having studied category theory,", "tokens": [51256, 12686, 1860, 666, 527, 3479, 2539, 5245, 13, 583, 1419, 9454, 7719, 5261, 11, 51692], "temperature": 0.0, "avg_logprob": -0.13701249493492973, "compression_ratio": 1.5217391304347827, "no_speech_prob": 0.0013572960160672665}, {"id": 239, "seek": 164432, "start": 1644.96, "end": 1650.6399999999999, "text": " I'm still noticing that compositionality is done in ad hoc ways and that there is a large gap", "tokens": [50396, 286, 478, 920, 21814, 300, 12686, 1860, 307, 1096, 294, 614, 16708, 2098, 293, 300, 456, 307, 257, 2416, 7417, 50680], "temperature": 0.0, "avg_logprob": -0.0688934326171875, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0008664040942676365}, {"id": 240, "seek": 164432, "start": 1650.6399999999999, "end": 1656.1599999999999, "text": " between how compositionality is thought of in deep learning and the theory of compositionality", "tokens": [50680, 1296, 577, 12686, 1860, 307, 1194, 295, 294, 2452, 2539, 293, 264, 5261, 295, 12686, 1860, 50956], "temperature": 0.0, "avg_logprob": -0.0688934326171875, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0008664040942676365}, {"id": 241, "seek": 164432, "start": 1656.1599999999999, "end": 1663.4399999999998, "text": " that category theory offers. Nonetheless, there have been some recent strides.", "tokens": [50956, 300, 7719, 5261, 7736, 13, 45437, 11, 456, 362, 668, 512, 5162, 1056, 1875, 13, 51320], "temperature": 0.0, "avg_logprob": -0.0688934326171875, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0008664040942676365}, {"id": 242, "seek": 164432, "start": 1666.24, "end": 1671.36, "text": " So I'd like to give a brief overview of what has been happening on the intersection of", "tokens": [51460, 407, 286, 1116, 411, 281, 976, 257, 5353, 12492, 295, 437, 575, 668, 2737, 322, 264, 15236, 295, 51716], "temperature": 0.0, "avg_logprob": -0.0688934326171875, "compression_ratio": 1.710144927536232, "no_speech_prob": 0.0008664040942676365}, {"id": 243, "seek": 167136, "start": 1671.36, "end": 1679.1999999999998, "text": " category theory and machine learning. So the papers I am showing here are two papers that", "tokens": [50364, 7719, 5261, 293, 3479, 2539, 13, 407, 264, 10577, 286, 669, 4099, 510, 366, 732, 10577, 300, 50756], "temperature": 0.0, "avg_logprob": -0.06749534045948702, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0008026321302168071}, {"id": 244, "seek": 167136, "start": 1679.1999999999998, "end": 1685.52, "text": " study neural networks in the very abstract, in the form of something called a parametric lens.", "tokens": [50756, 2979, 18161, 9590, 294, 264, 588, 12649, 11, 294, 264, 1254, 295, 746, 1219, 257, 6220, 17475, 6765, 13, 51072], "temperature": 0.0, "avg_logprob": -0.06749534045948702, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0008026321302168071}, {"id": 245, "seek": 167136, "start": 1686.32, "end": 1690.4799999999998, "text": " We're going to see exactly what that is later in the course, but I just want to give you", "tokens": [51112, 492, 434, 516, 281, 536, 2293, 437, 300, 307, 1780, 294, 264, 1164, 11, 457, 286, 445, 528, 281, 976, 291, 51320], "temperature": 0.0, "avg_logprob": -0.06749534045948702, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0008026321302168071}, {"id": 246, "seek": 167136, "start": 1690.4799999999998, "end": 1696.0, "text": " an idea of how these things work. So it's something that has a following shape.", "tokens": [51320, 364, 1558, 295, 577, 613, 721, 589, 13, 407, 309, 311, 746, 300, 575, 257, 3480, 3909, 13, 51596], "temperature": 0.0, "avg_logprob": -0.06749534045948702, "compression_ratio": 1.6118721461187215, "no_speech_prob": 0.0008026321302168071}, {"id": 247, "seek": 169600, "start": 1696.0, "end": 1704.08, "text": " And it abstractly models the information flow we see in a neural network. So on the left side,", "tokens": [50364, 400, 309, 12649, 356, 5245, 264, 1589, 3095, 321, 536, 294, 257, 18161, 3209, 13, 407, 322, 264, 1411, 1252, 11, 50768], "temperature": 0.0, "avg_logprob": -0.12144779122394064, "compression_ratio": 1.9948979591836735, "no_speech_prob": 0.0006660596118308604}, {"id": 248, "seek": 169600, "start": 1704.08, "end": 1708.96, "text": " we see inputs and their gradients. On the right side, we see outputs of a neural network and", "tokens": [50768, 321, 536, 15743, 293, 641, 2771, 2448, 13, 1282, 264, 558, 1252, 11, 321, 536, 23930, 295, 257, 18161, 3209, 293, 51012], "temperature": 0.0, "avg_logprob": -0.12144779122394064, "compression_ratio": 1.9948979591836735, "no_speech_prob": 0.0006660596118308604}, {"id": 249, "seek": 169600, "start": 1708.96, "end": 1716.56, "text": " their gradients. And here we take the two-dimensional notation seriously, and therefore we have weights", "tokens": [51012, 641, 2771, 2448, 13, 400, 510, 321, 747, 264, 732, 12, 18759, 24657, 6638, 11, 293, 4412, 321, 362, 17443, 51392], "temperature": 0.0, "avg_logprob": -0.12144779122394064, "compression_ratio": 1.9948979591836735, "no_speech_prob": 0.0006660596118308604}, {"id": 250, "seek": 169600, "start": 1716.56, "end": 1723.52, "text": " coming from top, weights and their gradients. And this abstractly models how the information flows.", "tokens": [51392, 1348, 490, 1192, 11, 17443, 293, 641, 2771, 2448, 13, 400, 341, 12649, 356, 5245, 577, 264, 1589, 12867, 13, 51740], "temperature": 0.0, "avg_logprob": -0.12144779122394064, "compression_ratio": 1.9948979591836735, "no_speech_prob": 0.0006660596118308604}, {"id": 251, "seek": 172352, "start": 1723.52, "end": 1728.72, "text": " So we can see that from left and top, we get inputs and parameters, compute and output,", "tokens": [50364, 407, 321, 393, 536, 300, 490, 1411, 293, 1192, 11, 321, 483, 15743, 293, 9834, 11, 14722, 293, 5598, 11, 50624], "temperature": 0.0, "avg_logprob": -0.11407689559154022, "compression_ratio": 2.0641025641025643, "no_speech_prob": 0.0014014517655596137}, {"id": 252, "seek": 172352, "start": 1729.28, "end": 1735.36, "text": " and then having received the gradients of the loss with respect to that output, we can back", "tokens": [50652, 293, 550, 1419, 4613, 264, 2771, 2448, 295, 264, 4470, 365, 3104, 281, 300, 5598, 11, 321, 393, 646, 50956], "temperature": 0.0, "avg_logprob": -0.11407689559154022, "compression_ratio": 2.0641025641025643, "no_speech_prob": 0.0014014517655596137}, {"id": 253, "seek": 172352, "start": 1735.36, "end": 1742.0, "text": " propagate the gradients with respect to the input to the left and back propagate the", "tokens": [50956, 48256, 264, 2771, 2448, 365, 3104, 281, 264, 4846, 281, 264, 1411, 293, 646, 48256, 264, 51288], "temperature": 0.0, "avg_logprob": -0.11407689559154022, "compression_ratio": 2.0641025641025643, "no_speech_prob": 0.0014014517655596137}, {"id": 254, "seek": 172352, "start": 1743.52, "end": 1746.6399999999999, "text": " gradients of the loss with respect to weights to the top.", "tokens": [51364, 2771, 2448, 295, 264, 4470, 365, 3104, 281, 17443, 281, 264, 1192, 13, 51520], "temperature": 0.0, "avg_logprob": -0.11407689559154022, "compression_ratio": 2.0641025641025643, "no_speech_prob": 0.0014014517655596137}, {"id": 255, "seek": 174664, "start": 1747.6000000000001, "end": 1754.3200000000002, "text": " Now, the question you might ask is what is the benefit of this formulation?", "tokens": [50412, 823, 11, 264, 1168, 291, 1062, 1029, 307, 437, 307, 264, 5121, 295, 341, 37642, 30, 50748], "temperature": 0.0, "avg_logprob": -0.12282044546944755, "compression_ratio": 1.5701357466063348, "no_speech_prob": 0.002357123652473092}, {"id": 256, "seek": 174664, "start": 1754.3200000000002, "end": 1759.44, "text": " Why would we model it like this? Well, when these constructions were originally discovered", "tokens": [50748, 1545, 576, 321, 2316, 309, 411, 341, 30, 1042, 11, 562, 613, 7690, 626, 645, 7993, 6941, 51004], "temperature": 0.0, "avg_logprob": -0.12282044546944755, "compression_ratio": 1.5701357466063348, "no_speech_prob": 0.002357123652473092}, {"id": 257, "seek": 174664, "start": 1759.44, "end": 1766.96, "text": " a few years ago, something fascinating happened. Independently, game theorists were modeling", "tokens": [51004, 257, 1326, 924, 2057, 11, 746, 10343, 2011, 13, 21809, 2276, 11, 1216, 27423, 1751, 645, 15983, 51380], "temperature": 0.0, "avg_logprob": -0.12282044546944755, "compression_ratio": 1.5701357466063348, "no_speech_prob": 0.002357123652473092}, {"id": 258, "seek": 174664, "start": 1766.96, "end": 1774.0, "text": " economic agents in game theory using these categorical models. And to the big surprise,", "tokens": [51380, 4836, 12554, 294, 1216, 5261, 1228, 613, 19250, 804, 5245, 13, 400, 281, 264, 955, 6365, 11, 51732], "temperature": 0.0, "avg_logprob": -0.12282044546944755, "compression_ratio": 1.5701357466063348, "no_speech_prob": 0.002357123652473092}, {"id": 259, "seek": 177400, "start": 1774.0, "end": 1778.72, "text": " these models ended up having the same categorical form as the machine learning ones.", "tokens": [50364, 613, 5245, 4590, 493, 1419, 264, 912, 19250, 804, 1254, 382, 264, 3479, 2539, 2306, 13, 50600], "temperature": 0.0, "avg_logprob": -0.09737574060757954, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0003402223519515246}, {"id": 260, "seek": 177400, "start": 1780.32, "end": 1786.4, "text": " So if you're studying economic agents that take some inputs, produce some outputs,", "tokens": [50680, 407, 498, 291, 434, 7601, 4836, 12554, 300, 747, 512, 15743, 11, 5258, 512, 23930, 11, 50984], "temperature": 0.0, "avg_logprob": -0.09737574060757954, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0003402223519515246}, {"id": 261, "seek": 177400, "start": 1786.4, "end": 1790.88, "text": " receive some payoff for their things and have their trying to maximize some strategy,", "tokens": [50984, 4774, 512, 46547, 337, 641, 721, 293, 362, 641, 1382, 281, 19874, 512, 5206, 11, 51208], "temperature": 0.0, "avg_logprob": -0.09737574060757954, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0003402223519515246}, {"id": 262, "seek": 177400, "start": 1791.44, "end": 1797.68, "text": " you end up with a model that has the same shape. So independent formalizations by different people", "tokens": [51236, 291, 917, 493, 365, 257, 2316, 300, 575, 264, 912, 3909, 13, 407, 6695, 9860, 14455, 538, 819, 561, 51548], "temperature": 0.0, "avg_logprob": -0.09737574060757954, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0003402223519515246}, {"id": 263, "seek": 177400, "start": 1797.68, "end": 1803.76, "text": " of game theory and machine learning converge to the same mathematical form. This elucidated", "tokens": [51548, 295, 1216, 5261, 293, 3479, 2539, 41881, 281, 264, 912, 18894, 1254, 13, 639, 806, 1311, 327, 770, 51852], "temperature": 0.0, "avg_logprob": -0.09737574060757954, "compression_ratio": 1.7619047619047619, "no_speech_prob": 0.0003402223519515246}, {"id": 264, "seek": 180376, "start": 1803.76, "end": 1809.52, "text": " a number of connections and really sparked a whole study of cybernetic systems or reinforcement-like", "tokens": [50364, 257, 1230, 295, 9271, 293, 534, 39653, 257, 1379, 2979, 295, 13411, 77, 3532, 3652, 420, 29280, 12, 4092, 50652], "temperature": 0.0, "avg_logprob": -0.1065713734302706, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0009077522554434836}, {"id": 265, "seek": 180376, "start": 1810.64, "end": 1815.2, "text": " agent systems, the study of agents interacting with the environment using minimal assumptions", "tokens": [50708, 9461, 3652, 11, 264, 2979, 295, 12554, 18017, 365, 264, 2823, 1228, 13206, 17695, 50936], "temperature": 0.0, "avg_logprob": -0.1065713734302706, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0009077522554434836}, {"id": 266, "seek": 180376, "start": 1815.2, "end": 1819.76, "text": " about what agents or the environment is. And really, this is something that can't be done", "tokens": [50936, 466, 437, 12554, 420, 264, 2823, 307, 13, 400, 534, 11, 341, 307, 746, 300, 393, 380, 312, 1096, 51164], "temperature": 0.0, "avg_logprob": -0.1065713734302706, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0009077522554434836}, {"id": 267, "seek": 180376, "start": 1819.76, "end": 1826.24, "text": " without the level of generality that category theory provides. So I found this quite fascinating", "tokens": [51164, 1553, 264, 1496, 295, 1337, 1860, 300, 7719, 5261, 6417, 13, 407, 286, 1352, 341, 1596, 10343, 51488], "temperature": 0.0, "avg_logprob": -0.1065713734302706, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0009077522554434836}, {"id": 268, "seek": 180376, "start": 1826.24, "end": 1833.52, "text": " as it was being discovered. And it's one of the reasons why I, it allows me to study machine", "tokens": [51488, 382, 309, 390, 885, 6941, 13, 400, 309, 311, 472, 295, 264, 4112, 983, 286, 11, 309, 4045, 385, 281, 2979, 3479, 51852], "temperature": 0.0, "avg_logprob": -0.1065713734302706, "compression_ratio": 1.7426470588235294, "no_speech_prob": 0.0009077522554434836}, {"id": 269, "seek": 183352, "start": 1833.52, "end": 1842.32, "text": " learning in game theory through a unified, unified lens. What it also turned out that why would we", "tokens": [50364, 2539, 294, 1216, 5261, 807, 257, 26787, 11, 26787, 6765, 13, 708, 309, 611, 3574, 484, 300, 983, 576, 321, 50804], "temperature": 0.0, "avg_logprob": -0.13472597786549773, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.0011661469470709562}, {"id": 270, "seek": 183352, "start": 1842.32, "end": 1846.6399999999999, "text": " want to use these parametric lenses that we can also model optimizers of neural networks.", "tokens": [50804, 528, 281, 764, 613, 6220, 17475, 18059, 300, 321, 393, 611, 2316, 5028, 22525, 295, 18161, 9590, 13, 51020], "temperature": 0.0, "avg_logprob": -0.13472597786549773, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.0011661469470709562}, {"id": 271, "seek": 183352, "start": 1848.4, "end": 1853.44, "text": " So, and what it ended up turning out that optimizers of neural networks have the same shape", "tokens": [51108, 407, 11, 293, 437, 309, 4590, 493, 6246, 484, 300, 5028, 22525, 295, 18161, 9590, 362, 264, 912, 3909, 51360], "temperature": 0.0, "avg_logprob": -0.13472597786549773, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.0011661469470709562}, {"id": 272, "seek": 183352, "start": 1853.44, "end": 1858.8, "text": " as neural networks themselves. So on the top here, the idea is, so on the left and right,", "tokens": [51360, 382, 18161, 9590, 2969, 13, 407, 322, 264, 1192, 510, 11, 264, 1558, 307, 11, 370, 322, 264, 1411, 293, 558, 11, 51628], "temperature": 0.0, "avg_logprob": -0.13472597786549773, "compression_ratio": 1.796116504854369, "no_speech_prob": 0.0011661469470709562}, {"id": 273, "seek": 185880, "start": 1858.96, "end": 1864.48, "text": " we have parameters, but on top we have the state saved by these optimizers, the stateful ones like", "tokens": [50372, 321, 362, 9834, 11, 457, 322, 1192, 321, 362, 264, 1785, 6624, 538, 613, 5028, 22525, 11, 264, 1785, 906, 2306, 411, 50648], "temperature": 0.0, "avg_logprob": -0.13591210501534598, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0016973211895674467}, {"id": 274, "seek": 185880, "start": 1864.48, "end": 1871.44, "text": " momentum or add-on. And this gives us hints that optimizers are some kind of hardwired", "tokens": [50648, 11244, 420, 909, 12, 266, 13, 400, 341, 2709, 505, 27271, 300, 5028, 22525, 366, 512, 733, 295, 1152, 86, 1824, 50996], "temperature": 0.0, "avg_logprob": -0.13591210501534598, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0016973211895674467}, {"id": 275, "seek": 185880, "start": 1871.44, "end": 1876.32, "text": " meta learners and just as some papers have shown. So there's lots of interesting research opening", "tokens": [50996, 19616, 23655, 293, 445, 382, 512, 10577, 362, 4898, 13, 407, 456, 311, 3195, 295, 1880, 2132, 5193, 51240], "temperature": 0.0, "avg_logprob": -0.13591210501534598, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0016973211895674467}, {"id": 276, "seek": 185880, "start": 1876.32, "end": 1881.12, "text": " up to this. And an interesting thing from a personal standpoint happened with regards to", "tokens": [51240, 493, 281, 341, 13, 400, 364, 1880, 551, 490, 257, 2973, 15827, 2011, 365, 14258, 281, 51480], "temperature": 0.0, "avg_logprob": -0.13591210501534598, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0016973211895674467}, {"id": 277, "seek": 185880, "start": 1881.12, "end": 1886.08, "text": " optimizers. We started formalizing gradient descent using lenses, this concept that I", "tokens": [51480, 5028, 22525, 13, 492, 1409, 9860, 3319, 16235, 23475, 1228, 18059, 11, 341, 3410, 300, 286, 51728], "temperature": 0.0, "avg_logprob": -0.13591210501534598, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0016973211895674467}, {"id": 278, "seek": 188608, "start": 1886.08, "end": 1891.52, "text": " mentioned that models by directionality. And for a long time actually had suspicions about", "tokens": [50364, 2835, 300, 5245, 538, 3513, 1860, 13, 400, 337, 257, 938, 565, 767, 632, 6535, 299, 626, 466, 50636], "temperature": 0.0, "avg_logprob": -0.09362490036908318, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.001449666335247457}, {"id": 279, "seek": 188608, "start": 1891.52, "end": 1898.24, "text": " this formalization. The problem was that there was a part of a lens that really wasn't used at all", "tokens": [50636, 341, 9860, 2144, 13, 440, 1154, 390, 300, 456, 390, 257, 644, 295, 257, 6765, 300, 534, 2067, 380, 1143, 412, 439, 50972], "temperature": 0.0, "avg_logprob": -0.09362490036908318, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.001449666335247457}, {"id": 280, "seek": 188608, "start": 1898.24, "end": 1904.56, "text": " in gradient descent, casting reasonable doubt on whether lenses are the right abstraction.", "tokens": [50972, 294, 16235, 23475, 11, 17301, 10585, 6385, 322, 1968, 18059, 366, 264, 558, 37765, 13, 51288], "temperature": 0.0, "avg_logprob": -0.09362490036908318, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.001449666335247457}, {"id": 281, "seek": 188608, "start": 1905.9199999999998, "end": 1911.36, "text": " Maybe they are really an overgeneralization. But then as we went on to formalize more", "tokens": [51356, 2704, 436, 366, 534, 364, 670, 1766, 2790, 2144, 13, 583, 550, 382, 321, 1437, 322, 281, 9860, 1125, 544, 51628], "temperature": 0.0, "avg_logprob": -0.09362490036908318, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.001449666335247457}, {"id": 282, "seek": 191136, "start": 1911.36, "end": 1917.6, "text": " complicated optimizers, such as the one that used Nestor of momentum, we found that the bit", "tokens": [50364, 6179, 5028, 22525, 11, 1270, 382, 264, 472, 300, 1143, 31581, 284, 295, 11244, 11, 321, 1352, 300, 264, 857, 50676], "temperature": 0.0, "avg_logprob": -0.13937970074740322, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0031148025300353765}, {"id": 283, "seek": 191136, "start": 1917.6, "end": 1923.04, "text": " that computes the look ahead in Nestor of momentum, the one that's always lost over in explanations", "tokens": [50676, 300, 715, 1819, 264, 574, 2286, 294, 31581, 284, 295, 11244, 11, 264, 472, 300, 311, 1009, 2731, 670, 294, 28708, 50948], "temperature": 0.0, "avg_logprob": -0.13937970074740322, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0031148025300353765}, {"id": 284, "seek": 191136, "start": 1923.04, "end": 1928.1599999999999, "text": " and when you implement these things, it requires rethinking about how you structured the stuff.", "tokens": [50948, 293, 562, 291, 4445, 613, 721, 11, 309, 7029, 319, 39873, 466, 577, 291, 18519, 264, 1507, 13, 51204], "temperature": 0.0, "avg_logprob": -0.13937970074740322, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0031148025300353765}, {"id": 285, "seek": 191136, "start": 1928.1599999999999, "end": 1932.0, "text": " It had a natural place in our formulation. It was the thing that we didn't use before.", "tokens": [51204, 467, 632, 257, 3303, 1081, 294, 527, 37642, 13, 467, 390, 264, 551, 300, 321, 994, 380, 764, 949, 13, 51396], "temperature": 0.0, "avg_logprob": -0.13937970074740322, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0031148025300353765}, {"id": 286, "seek": 191136, "start": 1933.36, "end": 1939.12, "text": " So really, from a personal standpoint, it clarified how a lot of these things fit that we", "tokens": [51464, 407, 534, 11, 490, 257, 2973, 15827, 11, 309, 47605, 577, 257, 688, 295, 613, 721, 3318, 300, 321, 51752], "temperature": 0.0, "avg_logprob": -0.13937970074740322, "compression_ratio": 1.724907063197026, "no_speech_prob": 0.0031148025300353765}, {"id": 287, "seek": 193912, "start": 1939.12, "end": 1949.9199999999998, "text": " started using this categorical formulas. And lastly, we're not the only ones using them.", "tokens": [50364, 1409, 1228, 341, 19250, 804, 30546, 13, 400, 16386, 11, 321, 434, 406, 264, 787, 2306, 1228, 552, 13, 50904], "temperature": 0.0, "avg_logprob": -0.10710518700735909, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.0012228627456352115}, {"id": 288, "seek": 193912, "start": 1949.9199999999998, "end": 1957.9199999999998, "text": " So very recently, about a week ago, I found a deep learning professor at New York University,", "tokens": [50904, 407, 588, 3938, 11, 466, 257, 1243, 2057, 11, 286, 1352, 257, 2452, 2539, 8304, 412, 1873, 3609, 3535, 11, 51304], "temperature": 0.0, "avg_logprob": -0.10710518700735909, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.0012228627456352115}, {"id": 289, "seek": 193912, "start": 1957.9199999999998, "end": 1964.56, "text": " if I'm not mistaken, independently starting to use the same sort of graphical notation", "tokens": [51304, 498, 286, 478, 406, 21333, 11, 21761, 2891, 281, 764, 264, 912, 1333, 295, 35942, 24657, 51636], "temperature": 0.0, "avg_logprob": -0.10710518700735909, "compression_ratio": 1.446236559139785, "no_speech_prob": 0.0012228627456352115}, {"id": 290, "seek": 196456, "start": 1964.8, "end": 1974.0, "text": " as the formal one we have been using. And I'm quite amazed that we were sort of having formal", "tokens": [50376, 382, 264, 9860, 472, 321, 362, 668, 1228, 13, 400, 286, 478, 1596, 20507, 300, 321, 645, 1333, 295, 1419, 9860, 50836], "temperature": 0.0, "avg_logprob": -0.16298633966690454, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.0008812273154035211}, {"id": 291, "seek": 196456, "start": 1974.0, "end": 1982.3999999999999, "text": " justification that these two notations have converged to the same representation.", "tokens": [50836, 31591, 300, 613, 732, 406, 763, 362, 9652, 3004, 281, 264, 912, 10290, 13, 51256], "temperature": 0.0, "avg_logprob": -0.16298633966690454, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.0008812273154035211}, {"id": 292, "seek": 196456, "start": 1984.1599999999999, "end": 1988.0, "text": " So yeah, this shows one line of work that has been done with machine learning,", "tokens": [51344, 407, 1338, 11, 341, 3110, 472, 1622, 295, 589, 300, 575, 668, 1096, 365, 3479, 2539, 11, 51536], "temperature": 0.0, "avg_logprob": -0.16298633966690454, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.0008812273154035211}, {"id": 293, "seek": 196456, "start": 1988.0, "end": 1993.76, "text": " with category theory. Other people have studied different things. People have studied recurrent", "tokens": [51536, 365, 7719, 5261, 13, 5358, 561, 362, 9454, 819, 721, 13, 3432, 362, 9454, 18680, 1753, 51824], "temperature": 0.0, "avg_logprob": -0.16298633966690454, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.0008812273154035211}, {"id": 294, "seek": 199376, "start": 1993.76, "end": 2000.16, "text": " neural networks. And it has a very strange title here, but if you open the paper, you'll see that", "tokens": [50364, 18161, 9590, 13, 400, 309, 575, 257, 588, 5861, 4876, 510, 11, 457, 498, 291, 1269, 264, 3035, 11, 291, 603, 536, 300, 50684], "temperature": 0.0, "avg_logprob": -0.07711177648499955, "compression_ratio": 1.85, "no_speech_prob": 0.0007544749532826245}, {"id": 295, "seek": 199376, "start": 2000.16, "end": 2005.84, "text": " it does study recurrent neural networks. And in recurrent neural networks, we have the", "tokens": [50684, 309, 775, 2979, 18680, 1753, 18161, 9590, 13, 400, 294, 18680, 1753, 18161, 9590, 11, 321, 362, 264, 50968], "temperature": 0.0, "avg_logprob": -0.07711177648499955, "compression_ratio": 1.85, "no_speech_prob": 0.0007544749532826245}, {"id": 296, "seek": 199376, "start": 2005.84, "end": 2010.96, "text": " idea of back propagation through time, which is done by unrolling the recurrent neural network.", "tokens": [50968, 1558, 295, 646, 38377, 807, 565, 11, 597, 307, 1096, 538, 517, 18688, 264, 18680, 1753, 18161, 3209, 13, 51224], "temperature": 0.0, "avg_logprob": -0.07711177648499955, "compression_ratio": 1.85, "no_speech_prob": 0.0007544749532826245}, {"id": 297, "seek": 199376, "start": 2010.96, "end": 2018.56, "text": " Now, what this paper showed is an interesting thing that this process of back propagation", "tokens": [51224, 823, 11, 437, 341, 3035, 4712, 307, 364, 1880, 551, 300, 341, 1399, 295, 646, 38377, 51604], "temperature": 0.0, "avg_logprob": -0.07711177648499955, "compression_ratio": 1.85, "no_speech_prob": 0.0007544749532826245}, {"id": 298, "seek": 201856, "start": 2018.56, "end": 2023.6, "text": " through time doesn't merely use differentiation, but it is differentiation", "tokens": [50364, 807, 565, 1177, 380, 17003, 764, 38902, 11, 457, 309, 307, 38902, 50616], "temperature": 0.0, "avg_logprob": -0.10433500024336803, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0025843188632279634}, {"id": 299, "seek": 201856, "start": 2024.72, "end": 2029.52, "text": " in a very sophisticated way. And it uses the formalism of something called double categories,", "tokens": [50672, 294, 257, 588, 16950, 636, 13, 400, 309, 4960, 264, 9860, 1434, 295, 746, 1219, 3834, 10479, 11, 50912], "temperature": 0.0, "avg_logprob": -0.10433500024336803, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0025843188632279634}, {"id": 300, "seek": 201856, "start": 2029.52, "end": 2035.2, "text": " which is yet an even more advanced but very visual concept. So this short picture shows you", "tokens": [50912, 597, 307, 1939, 364, 754, 544, 7339, 457, 588, 5056, 3410, 13, 407, 341, 2099, 3036, 3110, 291, 51196], "temperature": 0.0, "avg_logprob": -0.10433500024336803, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0025843188632279634}, {"id": 301, "seek": 201856, "start": 2035.2, "end": 2044.6399999999999, "text": " how you can think of each time step of recurrent neural network as a vertical kind of morphism,", "tokens": [51196, 577, 291, 393, 519, 295, 1184, 565, 1823, 295, 18680, 1753, 18161, 3209, 382, 257, 9429, 733, 295, 25778, 1434, 11, 51668], "temperature": 0.0, "avg_logprob": -0.10433500024336803, "compression_ratio": 1.5892857142857142, "no_speech_prob": 0.0025843188632279634}, {"id": 302, "seek": 204464, "start": 2044.64, "end": 2047.6000000000001, "text": " and you can think of layers of neural network as horizontal.", "tokens": [50364, 293, 291, 393, 519, 295, 7914, 295, 18161, 3209, 382, 12750, 13, 50512], "temperature": 0.0, "avg_logprob": -0.11170410073321799, "compression_ratio": 1.7387755102040816, "no_speech_prob": 0.0007882507052272558}, {"id": 303, "seek": 204464, "start": 2053.28, "end": 2060.08, "text": " And then Petar and Andrew, who are co-organizers of this course, have done a lot of work on", "tokens": [50796, 400, 550, 10472, 289, 293, 10110, 11, 567, 366, 598, 12, 12372, 22525, 295, 341, 1164, 11, 362, 1096, 257, 688, 295, 589, 322, 51136], "temperature": 0.0, "avg_logprob": -0.11170410073321799, "compression_ratio": 1.7387755102040816, "no_speech_prob": 0.0007882507052272558}, {"id": 304, "seek": 204464, "start": 2060.08, "end": 2064.4, "text": " geometric deep learning and graph neural networks. And they've established a connection between", "tokens": [51136, 33246, 2452, 2539, 293, 4295, 18161, 9590, 13, 400, 436, 600, 7545, 257, 4984, 1296, 51352], "temperature": 0.0, "avg_logprob": -0.11170410073321799, "compression_ratio": 1.7387755102040816, "no_speech_prob": 0.0007882507052272558}, {"id": 305, "seek": 204464, "start": 2064.4, "end": 2069.44, "text": " graph neural networks and dynamic programming, which is quite fascinating because this might", "tokens": [51352, 4295, 18161, 9590, 293, 8546, 9410, 11, 597, 307, 1596, 10343, 570, 341, 1062, 51604], "temperature": 0.0, "avg_logprob": -0.11170410073321799, "compression_ratio": 1.7387755102040816, "no_speech_prob": 0.0007882507052272558}, {"id": 306, "seek": 204464, "start": 2069.44, "end": 2073.52, "text": " be such very different things, thinking about how graph neural networks work and how", "tokens": [51604, 312, 1270, 588, 819, 721, 11, 1953, 466, 577, 4295, 18161, 9590, 589, 293, 577, 51808], "temperature": 0.0, "avg_logprob": -0.11170410073321799, "compression_ratio": 1.7387755102040816, "no_speech_prob": 0.0007882507052272558}, {"id": 307, "seek": 207352, "start": 2074.24, "end": 2079.92, "text": " the algorithm of, say, Bellman-Ford works. Yet again, using the abstractions of category theory,", "tokens": [50400, 264, 9284, 295, 11, 584, 11, 11485, 1601, 12, 37, 765, 1985, 13, 10890, 797, 11, 1228, 264, 12649, 626, 295, 7719, 5261, 11, 50684], "temperature": 0.0, "avg_logprob": -0.14355322140366283, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.001301788492128253}, {"id": 308, "seek": 207352, "start": 2079.92, "end": 2090.16, "text": " we can say something more precise about this. And generally about, yeah,", "tokens": [50684, 321, 393, 584, 746, 544, 13600, 466, 341, 13, 400, 5101, 466, 11, 1338, 11, 51196], "temperature": 0.0, "avg_logprob": -0.14355322140366283, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.001301788492128253}, {"id": 309, "seek": 207352, "start": 2093.36, "end": 2097.28, "text": " in generally about all of these neural networks, this is some of the stuff that has been done.", "tokens": [51356, 294, 5101, 466, 439, 295, 613, 18161, 9590, 11, 341, 307, 512, 295, 264, 1507, 300, 575, 668, 1096, 13, 51552], "temperature": 0.0, "avg_logprob": -0.14355322140366283, "compression_ratio": 1.5348837209302326, "no_speech_prob": 0.001301788492128253}, {"id": 310, "seek": 209728, "start": 2097.6000000000004, "end": 2104.1600000000003, "text": " But really, there is so much more work to be done and so many more things to explore. I've just", "tokens": [50380, 583, 534, 11, 456, 307, 370, 709, 544, 589, 281, 312, 1096, 293, 370, 867, 544, 721, 281, 6839, 13, 286, 600, 445, 50708], "temperature": 0.0, "avg_logprob": -0.17939361366065773, "compression_ratio": 1.78125, "no_speech_prob": 0.00074297736864537}, {"id": 311, "seek": 209728, "start": 2104.1600000000003, "end": 2110.2400000000002, "text": " given you a glimpse of some of the interesting things, but really, transformers haven't been", "tokens": [50708, 2212, 291, 257, 25838, 295, 512, 295, 264, 1880, 721, 11, 457, 534, 11, 4088, 433, 2378, 380, 668, 51012], "temperature": 0.0, "avg_logprob": -0.17939361366065773, "compression_ratio": 1.78125, "no_speech_prob": 0.00074297736864537}, {"id": 312, "seek": 209728, "start": 2110.2400000000002, "end": 2114.7200000000003, "text": " studied, have been studied to some small degree, but there's so much work really to do on them,", "tokens": [51012, 9454, 11, 362, 668, 9454, 281, 512, 1359, 4314, 11, 457, 456, 311, 370, 709, 589, 534, 281, 360, 322, 552, 11, 51236], "temperature": 0.0, "avg_logprob": -0.17939361366065773, "compression_ratio": 1.78125, "no_speech_prob": 0.00074297736864537}, {"id": 313, "seek": 209728, "start": 2114.7200000000003, "end": 2119.6000000000004, "text": " through category theory. Geometric deep learning is being studied now, actually,", "tokens": [51236, 807, 7719, 5261, 13, 2876, 29470, 2452, 2539, 307, 885, 9454, 586, 11, 767, 11, 51480], "temperature": 0.0, "avg_logprob": -0.17939361366065773, "compression_ratio": 1.78125, "no_speech_prob": 0.00074297736864537}, {"id": 314, "seek": 209728, "start": 2119.6000000000004, "end": 2125.6800000000003, "text": " through category theory, but then again, this is such a vast field. And the same holds for", "tokens": [51480, 807, 7719, 5261, 11, 457, 550, 797, 11, 341, 307, 1270, 257, 8369, 2519, 13, 400, 264, 912, 9190, 337, 51784], "temperature": 0.0, "avg_logprob": -0.17939361366065773, "compression_ratio": 1.78125, "no_speech_prob": 0.00074297736864537}, {"id": 315, "seek": 212568, "start": 2126.3199999999997, "end": 2132.3999999999996, "text": " generative adversarial networks, outer regressive models, NLP, these are all things,", "tokens": [50396, 1337, 1166, 17641, 44745, 9590, 11, 10847, 1121, 22733, 5245, 11, 426, 45196, 11, 613, 366, 439, 721, 11, 50700], "temperature": 0.0, "avg_logprob": -0.11450953764073989, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0010925172828137875}, {"id": 316, "seek": 212568, "start": 2132.3999999999996, "end": 2138.0, "text": " all fields, which could benefit from having more and more eyes apply categorical methods.", "tokens": [50700, 439, 7909, 11, 597, 727, 5121, 490, 1419, 544, 293, 544, 2575, 3079, 19250, 804, 7150, 13, 50980], "temperature": 0.0, "avg_logprob": -0.11450953764073989, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0010925172828137875}, {"id": 317, "seek": 212568, "start": 2138.0, "end": 2144.16, "text": " I think we're at a point where there's so much ideas and thoughts and we're trying to scale up", "tokens": [50980, 286, 519, 321, 434, 412, 257, 935, 689, 456, 311, 370, 709, 3487, 293, 4598, 293, 321, 434, 1382, 281, 4373, 493, 51288], "temperature": 0.0, "avg_logprob": -0.11450953764073989, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0010925172828137875}, {"id": 318, "seek": 212568, "start": 2144.16, "end": 2151.44, "text": " these processes of studying things categorically. And the only thing we're lacking is resources", "tokens": [51288, 613, 7555, 295, 7601, 721, 19250, 984, 13, 400, 264, 787, 551, 321, 434, 20889, 307, 3593, 51652], "temperature": 0.0, "avg_logprob": -0.11450953764073989, "compression_ratio": 1.6150442477876106, "no_speech_prob": 0.0010925172828137875}, {"id": 319, "seek": 215144, "start": 2151.44, "end": 2161.68, "text": " and people knowing about this. So this brings us towards the end of today's lecture.", "tokens": [50364, 293, 561, 5276, 466, 341, 13, 407, 341, 5607, 505, 3030, 264, 917, 295, 965, 311, 7991, 13, 50876], "temperature": 0.0, "avg_logprob": -0.0986189765314902, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0009345557191409171}, {"id": 320, "seek": 215144, "start": 2163.76, "end": 2172.16, "text": " We have seen how ACT is a rising field, how we can study a number of different scientific fields", "tokens": [50980, 492, 362, 1612, 577, 40341, 307, 257, 11636, 2519, 11, 577, 321, 393, 2979, 257, 1230, 295, 819, 8134, 7909, 51400], "temperature": 0.0, "avg_logprob": -0.0986189765314902, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0009345557191409171}, {"id": 321, "seek": 215144, "start": 2173.92, "end": 2177.28, "text": " and really a number of subfields of machine learning through the same lens.", "tokens": [51488, 293, 534, 257, 1230, 295, 1422, 7610, 82, 295, 3479, 2539, 807, 264, 912, 6765, 13, 51656], "temperature": 0.0, "avg_logprob": -0.0986189765314902, "compression_ratio": 1.4855491329479769, "no_speech_prob": 0.0009345557191409171}, {"id": 322, "seek": 217728, "start": 2178.1600000000003, "end": 2184.6400000000003, "text": " It is based upon compositionality and we've seen how it gives us this uniform description of", "tokens": [50408, 467, 307, 2361, 3564, 12686, 1860, 293, 321, 600, 1612, 577, 309, 2709, 505, 341, 9452, 3855, 295, 50732], "temperature": 0.0, "avg_logprob": -0.08352231449551052, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0016165596898645163}, {"id": 323, "seek": 217728, "start": 2184.6400000000003, "end": 2192.32, "text": " processes and concepts. Now, in this lecture, I didn't really go into any of the details. And as", "tokens": [50732, 7555, 293, 10392, 13, 823, 11, 294, 341, 7991, 11, 286, 994, 380, 534, 352, 666, 604, 295, 264, 4365, 13, 400, 382, 51116], "temperature": 0.0, "avg_logprob": -0.08352231449551052, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0016165596898645163}, {"id": 324, "seek": 217728, "start": 2192.32, "end": 2200.48, "text": " I've said, this is contrary to how category theory is usually defined and introduced. It is very", "tokens": [51116, 286, 600, 848, 11, 341, 307, 19506, 281, 577, 7719, 5261, 307, 2673, 7642, 293, 7268, 13, 467, 307, 588, 51524], "temperature": 0.0, "avg_logprob": -0.08352231449551052, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0016165596898645163}, {"id": 325, "seek": 217728, "start": 2200.48, "end": 2206.32, "text": " verbose and mathematical, but once we start going into it, it becomes very hard to gain", "tokens": [51524, 9595, 541, 293, 18894, 11, 457, 1564, 321, 722, 516, 666, 309, 11, 309, 3643, 588, 1152, 281, 6052, 51816], "temperature": 0.0, "avg_logprob": -0.08352231449551052, "compression_ratio": 1.5780590717299579, "no_speech_prob": 0.0016165596898645163}, {"id": 326, "seek": 220632, "start": 2206.32, "end": 2214.0800000000004, "text": " the intuition. So what we're trying to aim for in this course is to start slow and to motivate", "tokens": [50364, 264, 24002, 13, 407, 437, 321, 434, 1382, 281, 5939, 337, 294, 341, 1164, 307, 281, 722, 2964, 293, 281, 28497, 50752], "temperature": 0.0, "avg_logprob": -0.07724334472833677, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0022503482177853584}, {"id": 327, "seek": 220632, "start": 2214.0800000000004, "end": 2220.1600000000003, "text": " these examples. So if you want to learn more about how all of these concepts and systems", "tokens": [50752, 613, 5110, 13, 407, 498, 291, 528, 281, 1466, 544, 466, 577, 439, 295, 613, 10392, 293, 3652, 51056], "temperature": 0.0, "avg_logprob": -0.07724334472833677, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0022503482177853584}, {"id": 328, "seek": 220632, "start": 2220.8, "end": 2230.0, "text": " can be thought of in this uniform way, we invite you to check out the next week's lectures", "tokens": [51088, 393, 312, 1194, 295, 294, 341, 9452, 636, 11, 321, 7980, 291, 281, 1520, 484, 264, 958, 1243, 311, 16564, 51548], "temperature": 0.0, "avg_logprob": -0.07724334472833677, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0022503482177853584}, {"id": 329, "seek": 220632, "start": 2230.0, "end": 2235.44, "text": " and to gain a really more precise and concrete understanding of the things I've been saying", "tokens": [51548, 293, 281, 6052, 257, 534, 544, 13600, 293, 9859, 3701, 295, 264, 721, 286, 600, 668, 1566, 51820], "temperature": 0.0, "avg_logprob": -0.07724334472833677, "compression_ratio": 1.6194690265486726, "no_speech_prob": 0.0022503482177853584}, {"id": 330, "seek": 223544, "start": 2235.44, "end": 2247.28, "text": " about here. And really, to end with this, it's something that we want to communicate that really", "tokens": [50364, 466, 510, 13, 400, 534, 11, 281, 917, 365, 341, 11, 309, 311, 746, 300, 321, 528, 281, 7890, 300, 534, 50956], "temperature": 0.0, "avg_logprob": -0.07736356674678742, "compression_ratio": 1.680722891566265, "no_speech_prob": 0.0005520323175005615}, {"id": 331, "seek": 223544, "start": 2247.28, "end": 2253.28, "text": " applying this category theory to deep learning is one part of what category theory does, but", "tokens": [50956, 9275, 341, 7719, 5261, 281, 2452, 2539, 307, 472, 644, 295, 437, 7719, 5261, 775, 11, 457, 51256], "temperature": 0.0, "avg_logprob": -0.07736356674678742, "compression_ratio": 1.680722891566265, "no_speech_prob": 0.0005520323175005615}, {"id": 332, "seek": 223544, "start": 2253.28, "end": 2260.32, "text": " once you start thinking about this through the categorical lens, things, all things start", "tokens": [51256, 1564, 291, 722, 1953, 466, 341, 807, 264, 19250, 804, 6765, 11, 721, 11, 439, 721, 722, 51608], "temperature": 0.0, "avg_logprob": -0.07736356674678742, "compression_ratio": 1.680722891566265, "no_speech_prob": 0.0005520323175005615}, {"id": 333, "seek": 226032, "start": 2260.32, "end": 2267.1200000000003, "text": " looking like category theory. So this is something we hope to communicate. I've put up a number of", "tokens": [50364, 1237, 411, 7719, 5261, 13, 407, 341, 307, 746, 321, 1454, 281, 7890, 13, 286, 600, 829, 493, 257, 1230, 295, 50704], "temperature": 0.0, "avg_logprob": -0.09043194697453426, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.007215258199721575}, {"id": 334, "seek": 226032, "start": 2267.1200000000003, "end": 2271.92, "text": " references and reading lists here for people to, this slides are going to be put up online,", "tokens": [50704, 15400, 293, 3760, 14511, 510, 337, 561, 281, 11, 341, 9788, 366, 516, 281, 312, 829, 493, 2950, 11, 50944], "temperature": 0.0, "avg_logprob": -0.09043194697453426, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.007215258199721575}, {"id": 335, "seek": 226032, "start": 2271.92, "end": 2276.0800000000004, "text": " so you'll be able to click on them and have a look at some of the interesting conceptual things,", "tokens": [50944, 370, 291, 603, 312, 1075, 281, 2052, 322, 552, 293, 362, 257, 574, 412, 512, 295, 264, 1880, 24106, 721, 11, 51152], "temperature": 0.0, "avg_logprob": -0.09043194697453426, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.007215258199721575}, {"id": 336, "seek": 226032, "start": 2276.0800000000004, "end": 2280.6400000000003, "text": " some of these are books, some of these are lectures that might be beneficial in getting a sense", "tokens": [51152, 512, 295, 613, 366, 3642, 11, 512, 295, 613, 366, 16564, 300, 1062, 312, 14072, 294, 1242, 257, 2020, 51380], "temperature": 0.0, "avg_logprob": -0.09043194697453426, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.007215258199721575}, {"id": 337, "seek": 226032, "start": 2280.6400000000003, "end": 2288.6400000000003, "text": " of how this works. But yeah, for now, this is the point where I will end, and I would like to", "tokens": [51380, 295, 577, 341, 1985, 13, 583, 1338, 11, 337, 586, 11, 341, 307, 264, 935, 689, 286, 486, 917, 11, 293, 286, 576, 411, 281, 51780], "temperature": 0.0, "avg_logprob": -0.09043194697453426, "compression_ratio": 1.7408759124087592, "no_speech_prob": 0.007215258199721575}, {"id": 338, "seek": 228864, "start": 2288.64, "end": 2293.6, "text": " invite you. So we've had a lot of problems actually with the Google groups and people signing up.", "tokens": [50364, 7980, 291, 13, 407, 321, 600, 632, 257, 688, 295, 2740, 767, 365, 264, 3329, 3935, 293, 561, 13393, 493, 13, 50612], "temperature": 0.0, "avg_logprob": -0.11567813158035278, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.008660921826958656}, {"id": 339, "seek": 228864, "start": 2294.3199999999997, "end": 2299.44, "text": " If you still want to sign up, please do because we have opened the Zoom link of this lecture", "tokens": [50648, 759, 291, 920, 528, 281, 1465, 493, 11, 1767, 360, 570, 321, 362, 5625, 264, 13453, 2113, 295, 341, 7991, 50904], "temperature": 0.0, "avg_logprob": -0.11567813158035278, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.008660921826958656}, {"id": 340, "seek": 228864, "start": 2300.3199999999997, "end": 2305.7599999999998, "text": " to everyone because not many people, as I said, could join. But if you want to look at the next", "tokens": [50948, 281, 1518, 570, 406, 867, 561, 11, 382, 286, 848, 11, 727, 3917, 13, 583, 498, 291, 528, 281, 574, 412, 264, 958, 51220], "temperature": 0.0, "avg_logprob": -0.11567813158035278, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.008660921826958656}, {"id": 341, "seek": 228864, "start": 2305.7599999999998, "end": 2314.24, "text": " lectures, please sign up on the course website and do check out Zulip, which is also linked to", "tokens": [51220, 16564, 11, 1767, 1465, 493, 322, 264, 1164, 3144, 293, 360, 1520, 484, 1176, 425, 647, 11, 597, 307, 611, 9408, 281, 51644], "temperature": 0.0, "avg_logprob": -0.11567813158035278, "compression_ratio": 1.6144067796610169, "no_speech_prob": 0.008660921826958656}, {"id": 342, "seek": 231424, "start": 2314.24, "end": 2318.64, "text": " course participants where you can chat about category theory, chat about all of these lectures,", "tokens": [50364, 1164, 10503, 689, 291, 393, 5081, 466, 7719, 5261, 11, 5081, 466, 439, 295, 613, 16564, 11, 50584], "temperature": 0.0, "avg_logprob": -0.08732593059539795, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.006876679137349129}, {"id": 343, "seek": 231424, "start": 2318.64, "end": 2324.3199999999997, "text": " ask questions. This Zulip is a part of the wider applied category theory community,", "tokens": [50584, 1029, 1651, 13, 639, 1176, 425, 647, 307, 257, 644, 295, 264, 11842, 6456, 7719, 5261, 1768, 11, 50868], "temperature": 0.0, "avg_logprob": -0.08732593059539795, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.006876679137349129}, {"id": 344, "seek": 231424, "start": 2324.3199999999997, "end": 2329.52, "text": " so you'll be able to see how actual categories chat and the concepts we talk about and how these", "tokens": [50868, 370, 291, 603, 312, 1075, 281, 536, 577, 3539, 10479, 5081, 293, 264, 10392, 321, 751, 466, 293, 577, 613, 51128], "temperature": 0.0, "avg_logprob": -0.08732593059539795, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.006876679137349129}, {"id": 345, "seek": 231424, "start": 2329.52, "end": 2336.8799999999997, "text": " things work. Although I would warn you to thread carefully there, some of these things are very", "tokens": [51128, 721, 589, 13, 5780, 286, 576, 12286, 291, 281, 7207, 7500, 456, 11, 512, 295, 613, 721, 366, 588, 51496], "temperature": 0.0, "avg_logprob": -0.08732593059539795, "compression_ratio": 1.6756756756756757, "no_speech_prob": 0.006876679137349129}, {"id": 346, "seek": 233688, "start": 2336.88, "end": 2345.52, "text": " foreign and scary looking, but hopefully by the end of this course they will be less so. So yeah,", "tokens": [50364, 5329, 293, 6958, 1237, 11, 457, 4696, 538, 264, 917, 295, 341, 1164, 436, 486, 312, 1570, 370, 13, 407, 1338, 11, 50796], "temperature": 0.0, "avg_logprob": -0.11932500902113023, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.01733488030731678}, {"id": 347, "seek": 233688, "start": 2345.52, "end": 2351.6, "text": " thank you very much. Wonderful. Thank you so much, Bruno, for the great talk and the great", "tokens": [50796, 1309, 291, 588, 709, 13, 22768, 13, 1044, 291, 370, 709, 11, 23046, 11, 337, 264, 869, 751, 293, 264, 869, 51100], "temperature": 0.0, "avg_logprob": -0.11932500902113023, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.01733488030731678}, {"id": 348, "seek": 233688, "start": 2351.6, "end": 2358.32, "text": " introduction to this vast landscape of categories, which as you mentioned, in the coming weeks we", "tokens": [51100, 9339, 281, 341, 8369, 9661, 295, 10479, 11, 597, 382, 291, 2835, 11, 294, 264, 1348, 3259, 321, 51436], "temperature": 0.0, "avg_logprob": -0.11932500902113023, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.01733488030731678}, {"id": 349, "seek": 233688, "start": 2358.32, "end": 2365.04, "text": " will dive into more, starting with my own lecture next week where we'll talk about the basic objects", "tokens": [51436, 486, 9192, 666, 544, 11, 2891, 365, 452, 1065, 7991, 958, 1243, 689, 321, 603, 751, 466, 264, 3875, 6565, 51772], "temperature": 0.0, "avg_logprob": -0.11932500902113023, "compression_ratio": 1.5991735537190082, "no_speech_prob": 0.01733488030731678}, {"id": 350, "seek": 236504, "start": 2365.04, "end": 2371.68, "text": " in category theory in a more rigorous way. There's a lot of very interesting questions in the Zoom and", "tokens": [50364, 294, 7719, 5261, 294, 257, 544, 29882, 636, 13, 821, 311, 257, 688, 295, 588, 1880, 1651, 294, 264, 13453, 293, 50696], "temperature": 0.0, "avg_logprob": -0.11837438951458848, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0032665615435689688}, {"id": 351, "seek": 236504, "start": 2372.24, "end": 2376.08, "text": " I'll be acting as a sort of moderator, reading them out to you and we can do a bit of back and", "tokens": [50724, 286, 603, 312, 6577, 382, 257, 1333, 295, 37778, 11, 3760, 552, 484, 281, 291, 293, 321, 393, 360, 257, 857, 295, 646, 293, 50916], "temperature": 0.0, "avg_logprob": -0.11837438951458848, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0032665615435689688}, {"id": 352, "seek": 236504, "start": 2376.08, "end": 2383.52, "text": " forth. So the top rated question comes from Matej Zetrovic and it comes with this motivation of", "tokens": [50916, 5220, 13, 407, 264, 1192, 22103, 1168, 1487, 490, 27594, 73, 1176, 302, 24088, 299, 293, 309, 1487, 365, 341, 12335, 295, 51288], "temperature": 0.0, "avg_logprob": -0.11837438951458848, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0032665615435689688}, {"id": 353, "seek": 236504, "start": 2383.52, "end": 2388.32, "text": " you have probabilistic circuits like some product networks that are compositional with respect to", "tokens": [51288, 291, 362, 31959, 3142, 26354, 411, 512, 1674, 9590, 300, 366, 10199, 2628, 365, 3104, 281, 51528], "temperature": 0.0, "avg_logprob": -0.11837438951458848, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0032665615435689688}, {"id": 354, "seek": 236504, "start": 2388.32, "end": 2392.88, "text": " both conditions that you presented. However, they're not as good as neural networks in terms of", "tokens": [51528, 1293, 4487, 300, 291, 8212, 13, 2908, 11, 436, 434, 406, 382, 665, 382, 18161, 9590, 294, 2115, 295, 51756], "temperature": 0.0, "avg_logprob": -0.11837438951458848, "compression_ratio": 1.6621160409556315, "no_speech_prob": 0.0032665615435689688}, {"id": 355, "seek": 239288, "start": 2392.88, "end": 2398.8, "text": " expressivity. So a generative neural network can make better images than the ones from a", "tokens": [50364, 5109, 4253, 13, 407, 257, 1337, 1166, 18161, 3209, 393, 652, 1101, 5267, 813, 264, 2306, 490, 257, 50660], "temperature": 0.0, "avg_logprob": -0.10348324548630487, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0013617362128570676}, {"id": 356, "seek": 239288, "start": 2398.8, "end": 2405.12, "text": " generative some product network. So then the question is, should we invest more time into", "tokens": [50660, 1337, 1166, 512, 1674, 3209, 13, 407, 550, 264, 1168, 307, 11, 820, 321, 1963, 544, 565, 666, 50976], "temperature": 0.0, "avg_logprob": -0.10348324548630487, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0013617362128570676}, {"id": 357, "seek": 239288, "start": 2405.12, "end": 2410.48, "text": " studying a non-compositional model like neural networks and make them compositional or see how", "tokens": [50976, 7601, 257, 2107, 12, 21541, 329, 2628, 2316, 411, 18161, 9590, 293, 652, 552, 10199, 2628, 420, 536, 577, 51244], "temperature": 0.0, "avg_logprob": -0.10348324548630487, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0013617362128570676}, {"id": 358, "seek": 239288, "start": 2410.48, "end": 2414.7200000000003, "text": " you can scale something that's inherently compositional like a some product network? What are your thoughts", "tokens": [51244, 291, 393, 4373, 746, 300, 311, 27993, 10199, 2628, 411, 257, 512, 1674, 3209, 30, 708, 366, 428, 4598, 51456], "temperature": 0.0, "avg_logprob": -0.10348324548630487, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0013617362128570676}, {"id": 359, "seek": 239288, "start": 2414.7200000000003, "end": 2421.76, "text": " on this? Yeah, that's an interesting question and as I've mentioned in the beginning,", "tokens": [51456, 322, 341, 30, 865, 11, 300, 311, 364, 1880, 1168, 293, 382, 286, 600, 2835, 294, 264, 2863, 11, 51808], "temperature": 0.0, "avg_logprob": -0.10348324548630487, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.0013617362128570676}, {"id": 360, "seek": 242176, "start": 2421.84, "end": 2429.36, "text": " compositionality is a property of the model of our systems. So to me it seems like the model", "tokens": [50368, 12686, 1860, 307, 257, 4707, 295, 264, 2316, 295, 527, 3652, 13, 407, 281, 385, 309, 2544, 411, 264, 2316, 50744], "temperature": 0.0, "avg_logprob": -0.05801822784099173, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.0014938669046387076}, {"id": 361, "seek": 242176, "start": 2429.92, "end": 2435.76, "text": " that we have of these non-compositional systems might be non-compositional in nature,", "tokens": [50772, 300, 321, 362, 295, 613, 2107, 12, 21541, 329, 2628, 3652, 1062, 312, 2107, 12, 21541, 329, 2628, 294, 3687, 11, 51064], "temperature": 0.0, "avg_logprob": -0.05801822784099173, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.0014938669046387076}, {"id": 362, "seek": 242176, "start": 2435.76, "end": 2441.2000000000003, "text": " which doesn't necessarily mean that the actual model is non-compositional. It might be the fact", "tokens": [51064, 597, 1177, 380, 4725, 914, 300, 264, 3539, 2316, 307, 2107, 12, 21541, 329, 2628, 13, 467, 1062, 312, 264, 1186, 51336], "temperature": 0.0, "avg_logprob": -0.05801822784099173, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.0014938669046387076}, {"id": 363, "seek": 242176, "start": 2441.2000000000003, "end": 2446.96, "text": " that we don't know what the essential composable building blocks are. So I'm not sure if I can", "tokens": [51336, 300, 321, 500, 380, 458, 437, 264, 7115, 10199, 712, 2390, 8474, 366, 13, 407, 286, 478, 406, 988, 498, 286, 393, 51624], "temperature": 0.0, "avg_logprob": -0.05801822784099173, "compression_ratio": 1.7912621359223302, "no_speech_prob": 0.0014938669046387076}, {"id": 364, "seek": 244696, "start": 2446.96, "end": 2454.56, "text": " provide a good answer to which of the things we should study. I'm very biased in towards taking", "tokens": [50364, 2893, 257, 665, 1867, 281, 597, 295, 264, 721, 321, 820, 2979, 13, 286, 478, 588, 28035, 294, 3030, 1940, 50744], "temperature": 0.0, "avg_logprob": -0.12048272772149725, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0036413916386663914}, {"id": 365, "seek": 244696, "start": 2454.56, "end": 2459.12, "text": " out a system that doesn't seem compositional but feels like it is and then trying to make it so,", "tokens": [50744, 484, 257, 1185, 300, 1177, 380, 1643, 10199, 2628, 457, 3417, 411, 309, 307, 293, 550, 1382, 281, 652, 309, 370, 11, 50972], "temperature": 0.0, "avg_logprob": -0.12048272772149725, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0036413916386663914}, {"id": 366, "seek": 244696, "start": 2459.12, "end": 2466.8, "text": " trying to understand how information flows and what are the compositional building blocks. So", "tokens": [50972, 1382, 281, 1223, 577, 1589, 12867, 293, 437, 366, 264, 10199, 2628, 2390, 8474, 13, 407, 51356], "temperature": 0.0, "avg_logprob": -0.12048272772149725, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0036413916386663914}, {"id": 367, "seek": 244696, "start": 2466.8, "end": 2475.04, "text": " that would be my answer. All right, the second question comes from Siavash and it's a very quick", "tokens": [51356, 300, 576, 312, 452, 1867, 13, 1057, 558, 11, 264, 1150, 1168, 1487, 490, 4909, 706, 1299, 293, 309, 311, 257, 588, 1702, 51768], "temperature": 0.0, "avg_logprob": -0.12048272772149725, "compression_ratio": 1.6228813559322033, "no_speech_prob": 0.0036413916386663914}, {"id": 368, "seek": 247504, "start": 2475.04, "end": 2481.36, "text": " one. So what, if anything, is the difference between compositionality versus composability?", "tokens": [50364, 472, 13, 407, 437, 11, 498, 1340, 11, 307, 264, 2649, 1296, 12686, 1860, 5717, 10199, 2310, 30, 50680], "temperature": 0.0, "avg_logprob": -0.14839997703646435, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.003986263647675514}, {"id": 369, "seek": 247504, "start": 2482.64, "end": 2488.32, "text": " Yeah, so these are, this might be a very loosely defined term sometimes composability,", "tokens": [50744, 865, 11, 370, 613, 366, 11, 341, 1062, 312, 257, 588, 37966, 7642, 1433, 2171, 10199, 2310, 11, 51028], "temperature": 0.0, "avg_logprob": -0.14839997703646435, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.003986263647675514}, {"id": 370, "seek": 247504, "start": 2488.32, "end": 2492.88, "text": " for instance. Some people use composability to mean literally that we can plug together", "tokens": [51028, 337, 5197, 13, 2188, 561, 764, 10199, 2310, 281, 914, 3736, 300, 321, 393, 5452, 1214, 51256], "temperature": 0.0, "avg_logprob": -0.14839997703646435, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.003986263647675514}, {"id": 371, "seek": 247504, "start": 2492.88, "end": 2500.08, "text": " systems, processes, functions, but then when we plug these things, composable things together,", "tokens": [51256, 3652, 11, 7555, 11, 6828, 11, 457, 550, 562, 321, 5452, 613, 721, 11, 10199, 712, 721, 1214, 11, 51616], "temperature": 0.0, "avg_logprob": -0.14839997703646435, "compression_ratio": 1.6869158878504673, "no_speech_prob": 0.003986263647675514}, {"id": 372, "seek": 250008, "start": 2500.3199999999997, "end": 2506.48, "text": " they, people don't often mean that we can study the behavior of the composite in terms of the", "tokens": [50376, 436, 11, 561, 500, 380, 2049, 914, 300, 321, 393, 2979, 264, 5223, 295, 264, 25557, 294, 2115, 295, 264, 50684], "temperature": 0.0, "avg_logprob": -0.1309011005219959, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.0014521615812554955}, {"id": 373, "seek": 250008, "start": 2506.48, "end": 2511.6, "text": " smaller constituents. But then again, I think I found people using, at least in category theory,", "tokens": [50684, 4356, 30847, 13, 583, 550, 797, 11, 286, 519, 286, 1352, 561, 1228, 11, 412, 1935, 294, 7719, 5261, 11, 50940], "temperature": 0.0, "avg_logprob": -0.1309011005219959, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.0014521615812554955}, {"id": 374, "seek": 250008, "start": 2511.6, "end": 2518.16, "text": " the concept of composable only when we can study them, when we can study the", "tokens": [50940, 264, 3410, 295, 10199, 712, 787, 562, 321, 393, 2979, 552, 11, 562, 321, 393, 2979, 264, 51268], "temperature": 0.0, "avg_logprob": -0.1309011005219959, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.0014521615812554955}, {"id": 375, "seek": 250008, "start": 2519.7599999999998, "end": 2524.24, "text": " resulting system recursively. So I think you might find like different usages in different", "tokens": [51348, 16505, 1185, 20560, 3413, 13, 407, 286, 519, 291, 1062, 915, 411, 819, 505, 1660, 294, 819, 51572], "temperature": 0.0, "avg_logprob": -0.1309011005219959, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.0014521615812554955}, {"id": 376, "seek": 250008, "start": 2524.24, "end": 2529.2799999999997, "text": " communities. In category theory, this is taken very seriously. So you would call things composable", "tokens": [51572, 4456, 13, 682, 7719, 5261, 11, 341, 307, 2726, 588, 6638, 13, 407, 291, 576, 818, 721, 10199, 712, 51824], "temperature": 0.0, "avg_logprob": -0.1309011005219959, "compression_ratio": 1.8353413654618473, "no_speech_prob": 0.0014521615812554955}, {"id": 377, "seek": 252928, "start": 2529.28, "end": 2532.32, "text": " only if you have some sort of guarantees like this.", "tokens": [50364, 787, 498, 291, 362, 512, 1333, 295, 32567, 411, 341, 13, 50516], "temperature": 0.0, "avg_logprob": -0.16745195021996132, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.001893883803859353}, {"id": 378, "seek": 252928, "start": 2534.8, "end": 2541.92, "text": " Okay, with this top rated from Grigori asks, is there a particular book you would recommend for", "tokens": [50640, 1033, 11, 365, 341, 1192, 22103, 490, 2606, 328, 7386, 8962, 11, 307, 456, 257, 1729, 1446, 291, 576, 2748, 337, 50996], "temperature": 0.0, "avg_logprob": -0.16745195021996132, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.001893883803859353}, {"id": 379, "seek": 252928, "start": 2541.92, "end": 2545.52, "text": " beginners? I have a particular book I would recommend, but maybe you have one or two?", "tokens": [50996, 26992, 30, 286, 362, 257, 1729, 1446, 286, 576, 2748, 11, 457, 1310, 291, 362, 472, 420, 732, 30, 51176], "temperature": 0.0, "avg_logprob": -0.16745195021996132, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.001893883803859353}, {"id": 380, "seek": 252928, "start": 2546.32, "end": 2550.4, "text": " Well, maybe you can start with yours. I mean, let's start with seven sketches for sure.", "tokens": [51216, 1042, 11, 1310, 291, 393, 722, 365, 6342, 13, 286, 914, 11, 718, 311, 722, 365, 3407, 34547, 337, 988, 13, 51420], "temperature": 0.0, "avg_logprob": -0.16745195021996132, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.001893883803859353}, {"id": 381, "seek": 252928, "start": 2553.0400000000004, "end": 2558.2400000000002, "text": " Yeah, I would say that it often depends on where you're coming from. I would say seven", "tokens": [51552, 865, 11, 286, 576, 584, 300, 309, 2049, 5946, 322, 689, 291, 434, 1348, 490, 13, 286, 576, 584, 3407, 51812], "temperature": 0.0, "avg_logprob": -0.16745195021996132, "compression_ratio": 1.7361702127659575, "no_speech_prob": 0.001893883803859353}, {"id": 382, "seek": 255824, "start": 2558.24, "end": 2563.7599999999998, "text": " sketches in composition, compositionality linked here is a really good resources for", "tokens": [50364, 34547, 294, 12686, 11, 12686, 1860, 9408, 510, 307, 257, 534, 665, 3593, 337, 50640], "temperature": 0.0, "avg_logprob": -0.17819742812323816, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.0026644545141607523}, {"id": 383, "seek": 255824, "start": 2563.7599999999998, "end": 2568.4799999999996, "text": " scientists, engineers, and programmers in general. If you're coming from programming,", "tokens": [50640, 7708, 11, 11955, 11, 293, 41504, 294, 2674, 13, 759, 291, 434, 1348, 490, 9410, 11, 50876], "temperature": 0.0, "avg_logprob": -0.17819742812323816, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.0026644545141607523}, {"id": 384, "seek": 255824, "start": 2568.4799999999996, "end": 2574.7999999999997, "text": " specifically, you might want to look at category theory for programmers by Bartosz Mielewski,", "tokens": [50876, 4682, 11, 291, 1062, 528, 281, 574, 412, 7719, 5261, 337, 41504, 538, 22338, 329, 89, 376, 15949, 14358, 2984, 11, 51192], "temperature": 0.0, "avg_logprob": -0.17819742812323816, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.0026644545141607523}, {"id": 385, "seek": 255824, "start": 2574.7999999999997, "end": 2581.2, "text": " that which is a great research as research aims specifically at programmers. I will actually", "tokens": [51192, 300, 597, 307, 257, 869, 2132, 382, 2132, 24683, 4682, 412, 41504, 13, 286, 486, 767, 51512], "temperature": 0.0, "avg_logprob": -0.17819742812323816, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.0026644545141607523}, {"id": 386, "seek": 255824, "start": 2581.2, "end": 2588.0, "text": " link in the Zulib. I specifically keep a list of best introductory category theory resources", "tokens": [51512, 2113, 294, 264, 1176, 425, 897, 13, 286, 4682, 1066, 257, 1329, 295, 1151, 39048, 7719, 5261, 3593, 51852], "temperature": 0.0, "avg_logprob": -0.17819742812323816, "compression_ratio": 1.792828685258964, "no_speech_prob": 0.0026644545141607523}, {"id": 387, "seek": 258800, "start": 2588.0, "end": 2593.52, "text": " on my GitHub. So I don't have it here, but I will add it and I will link it in the Zulib,", "tokens": [50364, 322, 452, 23331, 13, 407, 286, 500, 380, 362, 309, 510, 11, 457, 286, 486, 909, 309, 293, 286, 486, 2113, 309, 294, 264, 1176, 425, 897, 11, 50640], "temperature": 0.0, "avg_logprob": -0.12320647619466867, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.002791627077385783}, {"id": 388, "seek": 258800, "start": 2593.52, "end": 2598.32, "text": " which includes a number of many more like blog posts, videos and books.", "tokens": [50640, 597, 5974, 257, 1230, 295, 867, 544, 411, 6968, 12300, 11, 2145, 293, 3642, 13, 50880], "temperature": 0.0, "avg_logprob": -0.12320647619466867, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.002791627077385783}, {"id": 389, "seek": 258800, "start": 2599.92, "end": 2605.12, "text": " Wonderful. Yeah, that will be very useful, I think, for the attendees. So there is a question", "tokens": [50960, 22768, 13, 865, 11, 300, 486, 312, 588, 4420, 11, 286, 519, 11, 337, 264, 34826, 13, 407, 456, 307, 257, 1168, 51220], "temperature": 0.0, "avg_logprob": -0.12320647619466867, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.002791627077385783}, {"id": 390, "seek": 258800, "start": 2605.12, "end": 2610.24, "text": " from Kylan, which asks, could you explain a bit more in detail, what does this graphical", "tokens": [51220, 490, 591, 24691, 11, 597, 8962, 11, 727, 291, 2903, 257, 857, 544, 294, 2607, 11, 437, 775, 341, 35942, 51476], "temperature": 0.0, "avg_logprob": -0.12320647619466867, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.002791627077385783}, {"id": 391, "seek": 258800, "start": 2610.24, "end": 2614.64, "text": " representation of your network actually bring us? For example, the diagram you showed from", "tokens": [51476, 10290, 295, 428, 3209, 767, 1565, 505, 30, 1171, 1365, 11, 264, 10686, 291, 4712, 490, 51696], "temperature": 0.0, "avg_logprob": -0.12320647619466867, "compression_ratio": 1.5535714285714286, "no_speech_prob": 0.002791627077385783}, {"id": 392, "seek": 261464, "start": 2614.64, "end": 2620.0, "text": " Alfredo Canziani, you said it seems to be just a diagram. What does it actually help us understand?", "tokens": [50364, 28327, 78, 383, 3910, 21309, 11, 291, 848, 309, 2544, 281, 312, 445, 257, 10686, 13, 708, 775, 309, 767, 854, 505, 1223, 30, 50632], "temperature": 0.0, "avg_logprob": -0.09987123489379883, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.001258124248124659}, {"id": 393, "seek": 261464, "start": 2621.8399999999997, "end": 2630.8799999999997, "text": " Yeah, so this is a really good question. So I will go back to this slide. So the short answer is", "tokens": [50724, 865, 11, 370, 341, 307, 257, 534, 665, 1168, 13, 407, 286, 486, 352, 646, 281, 341, 4137, 13, 407, 264, 2099, 1867, 307, 51176], "temperature": 0.0, "avg_logprob": -0.09987123489379883, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.001258124248124659}, {"id": 394, "seek": 261464, "start": 2630.8799999999997, "end": 2637.44, "text": " it restricts how we're thinking about this and it restricts the things we can do to this diagram.", "tokens": [51176, 309, 7694, 82, 577, 321, 434, 1953, 466, 341, 293, 309, 7694, 82, 264, 721, 321, 393, 360, 281, 341, 10686, 13, 51504], "temperature": 0.0, "avg_logprob": -0.09987123489379883, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.001258124248124659}, {"id": 395, "seek": 261464, "start": 2637.44, "end": 2642.7999999999997, "text": " So this might sound a little bit counterintuitive because we have a language and then it's very", "tokens": [51504, 407, 341, 1062, 1626, 257, 707, 857, 5682, 686, 48314, 570, 321, 362, 257, 2856, 293, 550, 309, 311, 588, 51772], "temperature": 0.0, "avg_logprob": -0.09987123489379883, "compression_ratio": 1.5918367346938775, "no_speech_prob": 0.001258124248124659}, {"id": 396, "seek": 264280, "start": 2642.8, "end": 2651.28, "text": " restrictive, but this is in fact a very useful design tool is to constrain ourselves. So I actually", "tokens": [50364, 43220, 11, 457, 341, 307, 294, 1186, 257, 588, 4420, 1715, 2290, 307, 281, 1817, 7146, 4175, 13, 407, 286, 767, 50788], "temperature": 0.0, "avg_logprob": -0.10095515149704953, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0024578399024903774}, {"id": 397, "seek": 264280, "start": 2651.28, "end": 2658.4, "text": " had a quick chat on Twitter with Alfredo and I noticed some of the ways, some of the things,", "tokens": [50788, 632, 257, 1702, 5081, 322, 5794, 365, 28327, 78, 293, 286, 5694, 512, 295, 264, 2098, 11, 512, 295, 264, 721, 11, 51144], "temperature": 0.0, "avg_logprob": -0.10095515149704953, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0024578399024903774}, {"id": 398, "seek": 264280, "start": 2658.4, "end": 2663.04, "text": " some of the ways he uses the diagrams to wire up some things were very non-compositional because", "tokens": [51144, 512, 295, 264, 2098, 415, 4960, 264, 36709, 281, 6234, 493, 512, 721, 645, 588, 2107, 12, 21541, 329, 2628, 570, 51376], "temperature": 0.0, "avg_logprob": -0.10095515149704953, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0024578399024903774}, {"id": 399, "seek": 264280, "start": 2663.04, "end": 2668.0, "text": " when you plug systems together, you wouldn't get a thing of the same kind, but through category", "tokens": [51376, 562, 291, 5452, 3652, 1214, 11, 291, 2759, 380, 483, 257, 551, 295, 264, 912, 733, 11, 457, 807, 7719, 51624], "temperature": 0.0, "avg_logprob": -0.10095515149704953, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.0024578399024903774}, {"id": 400, "seek": 266800, "start": 2668.08, "end": 2674.8, "text": " we have found another way to plug these things together to make it compositional. So the answer is", "tokens": [50368, 321, 362, 1352, 1071, 636, 281, 5452, 613, 721, 1214, 281, 652, 309, 10199, 2628, 13, 407, 264, 1867, 307, 50704], "temperature": 0.0, "avg_logprob": -0.12422386435575264, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.0035555323120206594}, {"id": 401, "seek": 266800, "start": 2674.8, "end": 2680.64, "text": " it helps us reason about the systems and really one day I hope we can implement these things.", "tokens": [50704, 309, 3665, 505, 1778, 466, 264, 3652, 293, 534, 472, 786, 286, 1454, 321, 393, 4445, 613, 721, 13, 50996], "temperature": 0.0, "avg_logprob": -0.12422386435575264, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.0035555323120206594}, {"id": 402, "seek": 266800, "start": 2682.72, "end": 2686.24, "text": " Because as you'll find when you think about deep learning, there's the whole theory, but the way", "tokens": [51100, 1436, 382, 291, 603, 915, 562, 291, 519, 466, 2452, 2539, 11, 456, 311, 264, 1379, 5261, 11, 457, 264, 636, 51276], "temperature": 0.0, "avg_logprob": -0.12422386435575264, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.0035555323120206594}, {"id": 403, "seek": 266800, "start": 2686.24, "end": 2692.24, "text": " implement sometimes has tricks and tweaks and there's not a uniform translation of the theory", "tokens": [51276, 4445, 2171, 575, 11733, 293, 46664, 293, 456, 311, 406, 257, 9452, 12853, 295, 264, 5261, 51576], "temperature": 0.0, "avg_logprob": -0.12422386435575264, "compression_ratio": 1.658008658008658, "no_speech_prob": 0.0035555323120206594}, {"id": 404, "seek": 269224, "start": 2692.24, "end": 2701.9199999999996, "text": " into implementation and I think my sort of very long term goal is to have a completely formal and", "tokens": [50364, 666, 11420, 293, 286, 519, 452, 1333, 295, 588, 938, 1433, 3387, 307, 281, 362, 257, 2584, 9860, 293, 50848], "temperature": 0.0, "avg_logprob": -0.0807259110843434, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.004445058759301901}, {"id": 405, "seek": 269224, "start": 2701.9199999999996, "end": 2707.2799999999997, "text": " uniform description of these processes at a high level, but also exactly at a low level", "tokens": [50848, 9452, 3855, 295, 613, 7555, 412, 257, 1090, 1496, 11, 457, 611, 2293, 412, 257, 2295, 1496, 51116], "temperature": 0.0, "avg_logprob": -0.0807259110843434, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.004445058759301901}, {"id": 406, "seek": 269224, "start": 2707.2799999999997, "end": 2715.68, "text": " and I think these diagrams help us show this. So maybe to emphasize when we draw these diagrams,", "tokens": [51116, 293, 286, 519, 613, 36709, 854, 505, 855, 341, 13, 407, 1310, 281, 16078, 562, 321, 2642, 613, 36709, 11, 51536], "temperature": 0.0, "avg_logprob": -0.0807259110843434, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.004445058759301901}, {"id": 407, "seek": 269224, "start": 2715.68, "end": 2721.12, "text": " this is exactly how we would implement them. There's not a secret thing going on that you", "tokens": [51536, 341, 307, 2293, 577, 321, 576, 4445, 552, 13, 821, 311, 406, 257, 4054, 551, 516, 322, 300, 291, 51808], "temperature": 0.0, "avg_logprob": -0.0807259110843434, "compression_ratio": 1.6460176991150441, "no_speech_prob": 0.004445058759301901}, {"id": 408, "seek": 272112, "start": 2721.12, "end": 2725.7599999999998, "text": " have to be careful, like you can take these diagrams very, very seriously, which is not something that", "tokens": [50364, 362, 281, 312, 5026, 11, 411, 291, 393, 747, 613, 36709, 588, 11, 588, 6638, 11, 597, 307, 406, 746, 300, 50596], "temperature": 0.0, "avg_logprob": -0.14977239448333454, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003746381029486656}, {"id": 409, "seek": 272112, "start": 2725.7599999999998, "end": 2730.16, "text": " can be done with informal notation. I hope that answers the question.", "tokens": [50596, 393, 312, 1096, 365, 24342, 24657, 13, 286, 1454, 300, 6338, 264, 1168, 13, 50816], "temperature": 0.0, "avg_logprob": -0.14977239448333454, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003746381029486656}, {"id": 410, "seek": 272112, "start": 2731.92, "end": 2737.92, "text": " All right, yeah, thanks for that. If the person asking the question wants to follow up, please", "tokens": [50904, 1057, 558, 11, 1338, 11, 3231, 337, 300, 13, 759, 264, 954, 3365, 264, 1168, 2738, 281, 1524, 493, 11, 1767, 51204], "temperature": 0.0, "avg_logprob": -0.14977239448333454, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003746381029486656}, {"id": 411, "seek": 272112, "start": 2737.92, "end": 2743.44, "text": " feel free to. The current top rated questions and thank you for all these questions, they're really,", "tokens": [51204, 841, 1737, 281, 13, 440, 2190, 1192, 22103, 1651, 293, 1309, 291, 337, 439, 613, 1651, 11, 436, 434, 534, 11, 51480], "temperature": 0.0, "avg_logprob": -0.14977239448333454, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003746381029486656}, {"id": 412, "seek": 272112, "start": 2743.44, "end": 2746.96, "text": " really interesting and they keep pouring in, which is great to get this kind of engagement.", "tokens": [51480, 534, 1880, 293, 436, 1066, 20450, 294, 11, 597, 307, 869, 281, 483, 341, 733, 295, 8742, 13, 51656], "temperature": 0.0, "avg_logprob": -0.14977239448333454, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.003746381029486656}, {"id": 413, "seek": 274696, "start": 2747.6, "end": 2751.92, "text": " We're obviously all happy to keep discussing even after the lecture on Zulip and otherwise.", "tokens": [50396, 492, 434, 2745, 439, 2055, 281, 1066, 10850, 754, 934, 264, 7991, 322, 1176, 425, 647, 293, 5911, 13, 50612], "temperature": 0.0, "avg_logprob": -0.14177198693303777, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0070923189632594585}, {"id": 414, "seek": 274696, "start": 2751.92, "end": 2756.96, "text": " The top question right now is from Siva and it asks, since categories and geometry have a rich", "tokens": [50612, 440, 1192, 1168, 558, 586, 307, 490, 318, 5931, 293, 309, 8962, 11, 1670, 10479, 293, 18426, 362, 257, 4593, 50864], "temperature": 0.0, "avg_logprob": -0.14177198693303777, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0070923189632594585}, {"id": 415, "seek": 274696, "start": 2756.96, "end": 2761.04, "text": " interaction, can we use category theory to understand the geometry of neural networks,", "tokens": [50864, 9285, 11, 393, 321, 764, 7719, 5261, 281, 1223, 264, 18426, 295, 18161, 9590, 11, 51068], "temperature": 0.0, "avg_logprob": -0.14177198693303777, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0070923189632594585}, {"id": 416, "seek": 274696, "start": 2761.04, "end": 2765.52, "text": " such as the geometry of its parameter space or the symmetry of space?", "tokens": [51068, 1270, 382, 264, 18426, 295, 1080, 13075, 1901, 420, 264, 25440, 295, 1901, 30, 51292], "temperature": 0.0, "avg_logprob": -0.14177198693303777, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0070923189632594585}, {"id": 417, "seek": 274696, "start": 2767.04, "end": 2772.64, "text": " Well, yeah, this is another great question and I think Petar might be in a much better position", "tokens": [51368, 1042, 11, 1338, 11, 341, 307, 1071, 869, 1168, 293, 286, 519, 10472, 289, 1062, 312, 294, 257, 709, 1101, 2535, 51648], "temperature": 0.0, "avg_logprob": -0.14177198693303777, "compression_ratio": 1.6692015209125475, "no_speech_prob": 0.0070923189632594585}, {"id": 418, "seek": 277264, "start": 2772.64, "end": 2778.24, "text": " than me to answer this. I think we both believe the answer is yes, but maybe I'll leave it to Petar", "tokens": [50364, 813, 385, 281, 1867, 341, 13, 286, 519, 321, 1293, 1697, 264, 1867, 307, 2086, 11, 457, 1310, 286, 603, 1856, 309, 281, 10472, 289, 50644], "temperature": 0.0, "avg_logprob": -0.09530651059925047, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.007418347056955099}, {"id": 419, "seek": 277264, "start": 2778.24, "end": 2785.44, "text": " here. Yeah, I mean, I'll just add that so I know that many people who are signed up to attend this", "tokens": [50644, 510, 13, 865, 11, 286, 914, 11, 286, 603, 445, 909, 300, 370, 286, 458, 300, 867, 561, 567, 366, 8175, 493, 281, 6888, 341, 51004], "temperature": 0.0, "avg_logprob": -0.09530651059925047, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.007418347056955099}, {"id": 420, "seek": 277264, "start": 2785.44, "end": 2789.44, "text": " course have already some working knowledge of geometric deep learning where we use geometry", "tokens": [51004, 1164, 362, 1217, 512, 1364, 3601, 295, 33246, 2452, 2539, 689, 321, 764, 18426, 51204], "temperature": 0.0, "avg_logprob": -0.09530651059925047, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.007418347056955099}, {"id": 421, "seek": 277264, "start": 2789.44, "end": 2795.44, "text": " to understand neural network architectures as a covariant functions. And one thing you will see", "tokens": [51204, 281, 1223, 18161, 3209, 6331, 1303, 382, 257, 49851, 394, 6828, 13, 400, 472, 551, 291, 486, 536, 51504], "temperature": 0.0, "avg_logprob": -0.09530651059925047, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.007418347056955099}, {"id": 422, "seek": 277264, "start": 2795.44, "end": 2802.56, "text": " in this course, especially in PIMS lecture, which is in week four, is that actually you can observe", "tokens": [51504, 294, 341, 1164, 11, 2318, 294, 430, 6324, 50, 7991, 11, 597, 307, 294, 1243, 1451, 11, 307, 300, 767, 291, 393, 11441, 51860], "temperature": 0.0, "avg_logprob": -0.09530651059925047, "compression_ratio": 1.625418060200669, "no_speech_prob": 0.007418347056955099}, {"id": 423, "seek": 280256, "start": 2802.56, "end": 2810.48, "text": " equivalents as a special case of a more broader category theory concept called naturality, which", "tokens": [50364, 9052, 791, 382, 257, 2121, 1389, 295, 257, 544, 13227, 7719, 5261, 3410, 1219, 3303, 507, 11, 597, 50760], "temperature": 0.0, "avg_logprob": -0.08962209245799917, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.00042374705662950873}, {"id": 424, "seek": 280256, "start": 2810.48, "end": 2818.0, "text": " effectively makes the conditions far more relaxed. For example, you don't need all of your functions", "tokens": [50760, 8659, 1669, 264, 4487, 1400, 544, 14628, 13, 1171, 1365, 11, 291, 500, 380, 643, 439, 295, 428, 6828, 51136], "temperature": 0.0, "avg_logprob": -0.08962209245799917, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.00042374705662950873}, {"id": 425, "seek": 280256, "start": 2818.0, "end": 2822.0, "text": " to be symmetries to analyze such a system. They don't need to compose with each other,", "tokens": [51136, 281, 312, 14232, 302, 2244, 281, 12477, 1270, 257, 1185, 13, 814, 500, 380, 643, 281, 35925, 365, 1184, 661, 11, 51336], "temperature": 0.0, "avg_logprob": -0.08962209245799917, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.00042374705662950873}, {"id": 426, "seek": 280256, "start": 2822.0, "end": 2827.04, "text": " inverses don't have to exist. So you can be in a way resistant even to functions that destroy", "tokens": [51336, 21378, 279, 500, 380, 362, 281, 2514, 13, 407, 291, 393, 312, 294, 257, 636, 20383, 754, 281, 6828, 300, 5293, 51588], "temperature": 0.0, "avg_logprob": -0.08962209245799917, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.00042374705662950873}, {"id": 427, "seek": 280256, "start": 2827.04, "end": 2831.52, "text": " some of your data rather than just leave it exactly the same. So in a way, it's something that", "tokens": [51588, 512, 295, 428, 1412, 2831, 813, 445, 1856, 309, 2293, 264, 912, 13, 407, 294, 257, 636, 11, 309, 311, 746, 300, 51812], "temperature": 0.0, "avg_logprob": -0.08962209245799917, "compression_ratio": 1.677304964539007, "no_speech_prob": 0.00042374705662950873}, {"id": 428, "seek": 283152, "start": 2831.52, "end": 2837.28, "text": " encompasses equivalent functions, but then allows you to model way more interesting things than that.", "tokens": [50364, 49866, 10344, 6828, 11, 457, 550, 4045, 291, 281, 2316, 636, 544, 1880, 721, 813, 300, 13, 50652], "temperature": 0.0, "avg_logprob": -0.13910440557143267, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.00031986666726879776}, {"id": 429, "seek": 283152, "start": 2837.84, "end": 2844.8, "text": " And I believe this also answers the question that Freddie Minow posted on his applied category", "tokens": [50680, 400, 286, 1697, 341, 611, 6338, 264, 1168, 300, 41264, 2829, 305, 9437, 322, 702, 6456, 7719, 51028], "temperature": 0.0, "avg_logprob": -0.13910440557143267, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.00031986666726879776}, {"id": 430, "seek": 283152, "start": 2844.8, "end": 2848.88, "text": " theory, a super set of geometric deep learning. The short answer that PIMM already wrote is that", "tokens": [51028, 5261, 11, 257, 1687, 992, 295, 33246, 2452, 2539, 13, 440, 2099, 1867, 300, 430, 6324, 44, 1217, 4114, 307, 300, 51232], "temperature": 0.0, "avg_logprob": -0.13910440557143267, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.00031986666726879776}, {"id": 431, "seek": 283152, "start": 2848.88, "end": 2855.28, "text": " we definitely think so. And the lecture from PIMM should elaborate this connection a lot more.", "tokens": [51232, 321, 2138, 519, 370, 13, 400, 264, 7991, 490, 430, 6324, 44, 820, 20945, 341, 4984, 257, 688, 544, 13, 51552], "temperature": 0.0, "avg_logprob": -0.13910440557143267, "compression_ratio": 1.583673469387755, "no_speech_prob": 0.00031986666726879776}, {"id": 432, "seek": 285528, "start": 2855.6800000000003, "end": 2865.0400000000004, "text": " Okay. So then we have an operator question from Nitin that asks, can we quantify or understand", "tokens": [50384, 1033, 13, 407, 550, 321, 362, 364, 12973, 1168, 490, 37942, 259, 300, 8962, 11, 393, 321, 40421, 420, 1223, 50852], "temperature": 0.0, "avg_logprob": -0.13311017643321643, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.0067939055152237415}, {"id": 433, "seek": 285528, "start": 2865.0400000000004, "end": 2870.48, "text": " the causality or counterfactual nature of systems if they have compositionality? Does it add some", "tokens": [50852, 264, 3302, 1860, 420, 5682, 44919, 901, 3687, 295, 3652, 498, 436, 362, 12686, 1860, 30, 4402, 309, 909, 512, 51124], "temperature": 0.0, "avg_logprob": -0.13311017643321643, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.0067939055152237415}, {"id": 434, "seek": 285528, "start": 2870.48, "end": 2874.88, "text": " explainability nature to the system as a whole instead of looking at the subset of components", "tokens": [51124, 2903, 2310, 3687, 281, 264, 1185, 382, 257, 1379, 2602, 295, 1237, 412, 264, 25993, 295, 6677, 51344], "temperature": 0.0, "avg_logprob": -0.13311017643321643, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.0067939055152237415}, {"id": 435, "seek": 285528, "start": 2874.88, "end": 2882.1600000000003, "text": " as independent modules that are not interdependent? Oh, these are all really, really good questions.", "tokens": [51344, 382, 6695, 16679, 300, 366, 406, 728, 36763, 317, 30, 876, 11, 613, 366, 439, 534, 11, 534, 665, 1651, 13, 51708], "temperature": 0.0, "avg_logprob": -0.13311017643321643, "compression_ratio": 1.6398305084745763, "no_speech_prob": 0.0067939055152237415}, {"id": 436, "seek": 288216, "start": 2882.24, "end": 2887.7599999999998, "text": " I actually don't know what the answer to this is. So there's been some work. Well, there's been", "tokens": [50368, 286, 767, 500, 380, 458, 437, 264, 1867, 281, 341, 307, 13, 407, 456, 311, 668, 512, 589, 13, 1042, 11, 456, 311, 668, 50644], "temperature": 0.0, "avg_logprob": -0.08687513600225034, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0022848350927233696}, {"id": 437, "seek": 288216, "start": 2889.04, "end": 2894.64, "text": " a number of papers studying category theory and causality, but I'm actually not sure what is", "tokens": [50708, 257, 1230, 295, 10577, 7601, 7719, 5261, 293, 3302, 1860, 11, 457, 286, 478, 767, 406, 988, 437, 307, 50988], "temperature": 0.0, "avg_logprob": -0.08687513600225034, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0022848350927233696}, {"id": 438, "seek": 288216, "start": 2894.64, "end": 2901.44, "text": " the state of the art with regards to counterfactual reasoning. There is certainly lots of papers", "tokens": [50988, 264, 1785, 295, 264, 1523, 365, 14258, 281, 5682, 44919, 901, 21577, 13, 821, 307, 3297, 3195, 295, 10577, 51328], "temperature": 0.0, "avg_logprob": -0.08687513600225034, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0022848350927233696}, {"id": 439, "seek": 288216, "start": 2901.44, "end": 2906.48, "text": " doing this, but I'm afraid I can't give a good answer to this. We certainly hope so that", "tokens": [51328, 884, 341, 11, 457, 286, 478, 4638, 286, 393, 380, 976, 257, 665, 1867, 281, 341, 13, 492, 3297, 1454, 370, 300, 51580], "temperature": 0.0, "avg_logprob": -0.08687513600225034, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0022848350927233696}, {"id": 440, "seek": 288216, "start": 2907.2799999999997, "end": 2911.6, "text": " something comes out, but it's not something that I think we can substantiate just yet.", "tokens": [51620, 746, 1487, 484, 11, 457, 309, 311, 406, 746, 300, 286, 519, 321, 393, 4594, 11520, 473, 445, 1939, 13, 51836], "temperature": 0.0, "avg_logprob": -0.08687513600225034, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0022848350927233696}, {"id": 441, "seek": 291216, "start": 2912.7999999999997, "end": 2918.08, "text": " Yeah, I think maybe I'll use this opportunity to plug one of our guest lectures from Taco Cohen,", "tokens": [50396, 865, 11, 286, 519, 1310, 286, 603, 764, 341, 2650, 281, 5452, 472, 295, 527, 8341, 16564, 490, 37992, 32968, 11, 50660], "temperature": 0.0, "avg_logprob": -0.10054114231696495, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.0005525380838662386}, {"id": 442, "seek": 291216, "start": 2918.08, "end": 2922.16, "text": " who has worked quite a bit recently on trying to use category theory to formalize", "tokens": [50660, 567, 575, 2732, 1596, 257, 857, 3938, 322, 1382, 281, 764, 7719, 5261, 281, 9860, 1125, 50864], "temperature": 0.0, "avg_logprob": -0.10054114231696495, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.0005525380838662386}, {"id": 443, "seek": 291216, "start": 2922.72, "end": 2927.2, "text": " causal reasoning in machine learning models, who has this very nice position paper on it", "tokens": [50892, 38755, 21577, 294, 3479, 2539, 5245, 11, 567, 575, 341, 588, 1481, 2535, 3035, 322, 309, 51116], "temperature": 0.0, "avg_logprob": -0.10054114231696495, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.0005525380838662386}, {"id": 444, "seek": 291216, "start": 2927.2, "end": 2932.3199999999997, "text": " that came on the archive recently. And he will be giving us a guest lecture on this exact topic.", "tokens": [51116, 300, 1361, 322, 264, 23507, 3938, 13, 400, 415, 486, 312, 2902, 505, 257, 8341, 7991, 322, 341, 1900, 4829, 13, 51372], "temperature": 0.0, "avg_logprob": -0.10054114231696495, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.0005525380838662386}, {"id": 445, "seek": 291216, "start": 2932.3199999999997, "end": 2937.68, "text": " So please do stick around if you're interested in applications of category theory for causality.", "tokens": [51372, 407, 1767, 360, 2897, 926, 498, 291, 434, 3102, 294, 5821, 295, 7719, 5261, 337, 3302, 1860, 13, 51640], "temperature": 0.0, "avg_logprob": -0.10054114231696495, "compression_ratio": 1.6405693950177935, "no_speech_prob": 0.0005525380838662386}, {"id": 446, "seek": 293768, "start": 2937.68, "end": 2942.8799999999997, "text": " I had a chance to speak to Taco on this on a few occasions, and he seems quite convinced that we", "tokens": [50364, 286, 632, 257, 2931, 281, 1710, 281, 37992, 322, 341, 322, 257, 1326, 20641, 11, 293, 415, 2544, 1596, 12561, 300, 321, 50624], "temperature": 0.0, "avg_logprob": -0.08884504491632635, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0008827690617181361}, {"id": 447, "seek": 293768, "start": 2942.8799999999997, "end": 2948.64, "text": " need category theory to reason about causality in the right way. So it'll be very interesting to", "tokens": [50624, 643, 7719, 5261, 281, 1778, 466, 3302, 1860, 294, 264, 558, 636, 13, 407, 309, 603, 312, 588, 1880, 281, 50912], "temperature": 0.0, "avg_logprob": -0.08884504491632635, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0008827690617181361}, {"id": 448, "seek": 293768, "start": 2948.64, "end": 2955.04, "text": " hear his thoughts on this. The current top rated question comes from Flavio, and it says,", "tokens": [50912, 1568, 702, 4598, 322, 341, 13, 440, 2190, 1192, 22103, 1168, 1487, 490, 3235, 706, 1004, 11, 293, 309, 1619, 11, 51232], "temperature": 0.0, "avg_logprob": -0.08884504491632635, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0008827690617181361}, {"id": 449, "seek": 293768, "start": 2955.04, "end": 2959.9199999999996, "text": " category theory tutorials might be easy to find. Can we get more info on the specific relation", "tokens": [51232, 7719, 5261, 17616, 1062, 312, 1858, 281, 915, 13, 1664, 321, 483, 544, 13614, 322, 264, 2685, 9721, 51476], "temperature": 0.0, "avg_logprob": -0.08884504491632635, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0008827690617181361}, {"id": 450, "seek": 293768, "start": 2959.9199999999996, "end": 2965.7599999999998, "text": " with deep learning? I think the answer to this question is really that those connections will", "tokens": [51476, 365, 2452, 2539, 30, 286, 519, 264, 1867, 281, 341, 1168, 307, 534, 300, 729, 9271, 486, 51768], "temperature": 0.0, "avg_logprob": -0.08884504491632635, "compression_ratio": 1.6619718309859155, "no_speech_prob": 0.0008827690617181361}, {"id": 451, "seek": 296576, "start": 2965.76, "end": 2970.48, "text": " happen in the coming lectures, unless Bruno, you want to add anything else on top of that.", "tokens": [50364, 1051, 294, 264, 1348, 16564, 11, 5969, 23046, 11, 291, 528, 281, 909, 1340, 1646, 322, 1192, 295, 300, 13, 50600], "temperature": 0.0, "avg_logprob": -0.11849543922825863, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0023563681170344353}, {"id": 452, "seek": 296576, "start": 2972.2400000000002, "end": 2977.36, "text": " No, no, you're right. I would add that I also have a GitHub sort of listing all the papers", "tokens": [50688, 883, 11, 572, 11, 291, 434, 558, 13, 286, 576, 909, 300, 286, 611, 362, 257, 23331, 1333, 295, 22161, 439, 264, 10577, 50944], "temperature": 0.0, "avg_logprob": -0.11849543922825863, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0023563681170344353}, {"id": 453, "seek": 296576, "start": 2977.36, "end": 2983.6800000000003, "text": " in category theory in machine learning. This is precisely the GitHub that hosts the data used", "tokens": [50944, 294, 7719, 5261, 294, 3479, 2539, 13, 639, 307, 13402, 264, 23331, 300, 21573, 264, 1412, 1143, 51260], "temperature": 0.0, "avg_logprob": -0.11849543922825863, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0023563681170344353}, {"id": 454, "seek": 296576, "start": 2983.6800000000003, "end": 2990.5600000000004, "text": " to generate this graphic. So maybe I'll also link that in the zealot. So that might be a good thing", "tokens": [51260, 281, 8460, 341, 14089, 13, 407, 1310, 286, 603, 611, 2113, 300, 294, 264, 5277, 304, 310, 13, 407, 300, 1062, 312, 257, 665, 551, 51604], "temperature": 0.0, "avg_logprob": -0.11849543922825863, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0023563681170344353}, {"id": 455, "seek": 296576, "start": 2990.5600000000004, "end": 2993.76, "text": " to have a look before the next week's lecture to see what has been done.", "tokens": [51604, 281, 362, 257, 574, 949, 264, 958, 1243, 311, 7991, 281, 536, 437, 575, 668, 1096, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11849543922825863, "compression_ratio": 1.6592592592592592, "no_speech_prob": 0.0023563681170344353}, {"id": 456, "seek": 299376, "start": 2994.7200000000003, "end": 3001.76, "text": " Yeah, sounds good. Yes, definitely do share that if you have a chance. Andrew, I think you have a", "tokens": [50412, 865, 11, 3263, 665, 13, 1079, 11, 2138, 360, 2073, 300, 498, 291, 362, 257, 2931, 13, 10110, 11, 286, 519, 291, 362, 257, 50764], "temperature": 0.0, "avg_logprob": -0.1561062047769735, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.00042376795317977667}, {"id": 457, "seek": 299376, "start": 3001.76, "end": 3007.36, "text": " raised hand. Yes, I just wanted to emphasize, especially with the previous or earlier question,", "tokens": [50764, 6005, 1011, 13, 1079, 11, 286, 445, 1415, 281, 16078, 11, 2318, 365, 264, 3894, 420, 3071, 1168, 11, 51044], "temperature": 0.0, "avg_logprob": -0.1561062047769735, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.00042376795317977667}, {"id": 458, "seek": 299376, "start": 3008.1600000000003, "end": 3012.6400000000003, "text": " that at the moment there is no book on this subject, on the connection between category", "tokens": [51084, 300, 412, 264, 1623, 456, 307, 572, 1446, 322, 341, 3983, 11, 322, 264, 4984, 1296, 7719, 51308], "temperature": 0.0, "avg_logprob": -0.1561062047769735, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.00042376795317977667}, {"id": 459, "seek": 299376, "start": 3012.6400000000003, "end": 3020.6400000000003, "text": " theory and deep learning. So we're going to do our best in these lectures, but it's not", "tokens": [51308, 5261, 293, 2452, 2539, 13, 407, 321, 434, 516, 281, 360, 527, 1151, 294, 613, 16564, 11, 457, 309, 311, 406, 51708], "temperature": 0.0, "avg_logprob": -0.1561062047769735, "compression_ratio": 1.5439330543933054, "no_speech_prob": 0.00042376795317977667}, {"id": 460, "seek": 302064, "start": 3020.72, "end": 3029.12, "text": " so easy to give reference materials besides these papers. This is a gap we're trying to", "tokens": [50368, 370, 1858, 281, 976, 6408, 5319, 11868, 613, 10577, 13, 639, 307, 257, 7417, 321, 434, 1382, 281, 50788], "temperature": 0.0, "avg_logprob": -0.121020525351338, "compression_ratio": 1.5949367088607596, "no_speech_prob": 0.0015726670389994979}, {"id": 461, "seek": 302064, "start": 3029.12, "end": 3033.8399999999997, "text": " fill with these lectures. Yeah, I think that's also a good point to have. Although the seven", "tokens": [50788, 2836, 365, 613, 16564, 13, 865, 11, 286, 519, 300, 311, 611, 257, 665, 935, 281, 362, 13, 5780, 264, 3407, 51024], "temperature": 0.0, "avg_logprob": -0.121020525351338, "compression_ratio": 1.5949367088607596, "no_speech_prob": 0.0015726670389994979}, {"id": 462, "seek": 302064, "start": 3033.8399999999997, "end": 3039.44, "text": " sketches, if you don't count machine learning per se, is a good starting point to just understand", "tokens": [51024, 34547, 11, 498, 291, 500, 380, 1207, 3479, 2539, 680, 369, 11, 307, 257, 665, 2891, 935, 281, 445, 1223, 51304], "temperature": 0.0, "avg_logprob": -0.121020525351338, "compression_ratio": 1.5949367088607596, "no_speech_prob": 0.0015726670389994979}, {"id": 463, "seek": 302064, "start": 3039.44, "end": 3044.7999999999997, "text": " what all these string diagrams are like and how they connect different areas. Yeah, for programming", "tokens": [51304, 437, 439, 613, 6798, 36709, 366, 411, 293, 577, 436, 1745, 819, 3179, 13, 865, 11, 337, 9410, 51572], "temperature": 0.0, "avg_logprob": -0.121020525351338, "compression_ratio": 1.5949367088607596, "no_speech_prob": 0.0015726670389994979}, {"id": 464, "seek": 304480, "start": 3044.88, "end": 3055.28, "text": " and CS, plenty of resources, but yeah. Okay, so let's see, what else do we have?", "tokens": [50368, 293, 9460, 11, 7140, 295, 3593, 11, 457, 1338, 13, 1033, 11, 370, 718, 311, 536, 11, 437, 1646, 360, 321, 362, 30, 50888], "temperature": 0.0, "avg_logprob": -0.1830708056080098, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.009404289536178112}, {"id": 465, "seek": 304480, "start": 3056.2400000000002, "end": 3062.0, "text": " So there's a lot of questions floating around. I'm just trying to pick which one. Okay, yeah,", "tokens": [50936, 407, 456, 311, 257, 688, 295, 1651, 12607, 926, 13, 286, 478, 445, 1382, 281, 1888, 597, 472, 13, 1033, 11, 1338, 11, 51224], "temperature": 0.0, "avg_logprob": -0.1830708056080098, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.009404289536178112}, {"id": 466, "seek": 304480, "start": 3062.0, "end": 3066.1600000000003, "text": " this one is an interesting one. It's a bit philosophical, so I'm curious to hear Andrew,", "tokens": [51224, 341, 472, 307, 364, 1880, 472, 13, 467, 311, 257, 857, 25066, 11, 370, 286, 478, 6369, 281, 1568, 10110, 11, 51432], "temperature": 0.0, "avg_logprob": -0.1830708056080098, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.009404289536178112}, {"id": 467, "seek": 304480, "start": 3066.1600000000003, "end": 3070.88, "text": " sorry, Bruno, what you think about it. It comes from Lucino Prince. Do you think that", "tokens": [51432, 2597, 11, 23046, 11, 437, 291, 519, 466, 309, 13, 467, 1487, 490, 9593, 2982, 9821, 13, 1144, 291, 519, 300, 51668], "temperature": 0.0, "avg_logprob": -0.1830708056080098, "compression_ratio": 1.5108225108225108, "no_speech_prob": 0.009404289536178112}, {"id": 468, "seek": 307088, "start": 3070.88, "end": 3076.48, "text": " composability is a true constituent of nature, or is just the limit of how much we can understand?", "tokens": [50364, 10199, 2310, 307, 257, 2074, 16085, 317, 295, 3687, 11, 420, 307, 445, 264, 4948, 295, 577, 709, 321, 393, 1223, 30, 50644], "temperature": 0.0, "avg_logprob": -0.11043588407747038, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.002016819780692458}, {"id": 469, "seek": 307088, "start": 3078.08, "end": 3087.2000000000003, "text": " Oh, I love these questions. I wish I could provide a really coherent answer to this. Certainly,", "tokens": [50724, 876, 11, 286, 959, 613, 1651, 13, 286, 3172, 286, 727, 2893, 257, 534, 36239, 1867, 281, 341, 13, 16628, 11, 51180], "temperature": 0.0, "avg_logprob": -0.11043588407747038, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.002016819780692458}, {"id": 470, "seek": 307088, "start": 3087.2000000000003, "end": 3095.04, "text": " it seems like everywhere we look, things are compositional. But this might be like the story", "tokens": [51180, 309, 2544, 411, 5315, 321, 574, 11, 721, 366, 10199, 2628, 13, 583, 341, 1062, 312, 411, 264, 1657, 51572], "temperature": 0.0, "avg_logprob": -0.11043588407747038, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.002016819780692458}, {"id": 471, "seek": 307088, "start": 3095.04, "end": 3099.44, "text": " of trying to find your car keys under the lamp post because that's where the light is.", "tokens": [51572, 295, 1382, 281, 915, 428, 1032, 9317, 833, 264, 12684, 2183, 570, 300, 311, 689, 264, 1442, 307, 13, 51792], "temperature": 0.0, "avg_logprob": -0.11043588407747038, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.002016819780692458}, {"id": 472, "seek": 309944, "start": 3100.4, "end": 3106.64, "text": " We are certainly not doing compositionality. There's so many things that seem so foreign to us,", "tokens": [50412, 492, 366, 3297, 406, 884, 10199, 2628, 507, 13, 821, 311, 370, 867, 721, 300, 1643, 370, 5329, 281, 505, 11, 50724], "temperature": 0.0, "avg_logprob": -0.14051965412340667, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0008815155015327036}, {"id": 473, "seek": 309944, "start": 3108.0, "end": 3111.68, "text": " and we just don't look there. We look at the things which are compositional.", "tokens": [50792, 293, 321, 445, 500, 380, 574, 456, 13, 492, 574, 412, 264, 721, 597, 366, 10199, 2628, 13, 50976], "temperature": 0.0, "avg_logprob": -0.14051965412340667, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0008815155015327036}, {"id": 474, "seek": 309944, "start": 3113.44, "end": 3118.2400000000002, "text": " Oh, god, yeah, I'm not sure how to answer this question. So this is a very", "tokens": [51064, 876, 11, 3044, 11, 1338, 11, 286, 478, 406, 988, 577, 281, 1867, 341, 1168, 13, 407, 341, 307, 257, 588, 51304], "temperature": 0.0, "avg_logprob": -0.14051965412340667, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0008815155015327036}, {"id": 475, "seek": 309944, "start": 3120.7200000000003, "end": 3126.8, "text": " conservative answer. That's what I'll say. I don't know if anybody else from the team has a response to this.", "tokens": [51428, 13780, 1867, 13, 663, 311, 437, 286, 603, 584, 13, 286, 500, 380, 458, 498, 4472, 1646, 490, 264, 1469, 575, 257, 4134, 281, 341, 13, 51732], "temperature": 0.0, "avg_logprob": -0.14051965412340667, "compression_ratio": 1.5866666666666667, "no_speech_prob": 0.0008815155015327036}, {"id": 476, "seek": 312944, "start": 3129.6, "end": 3138.16, "text": " I think the question caught me a bit off guard. I'll have to think about it a bit more. But", "tokens": [50372, 286, 519, 264, 1168, 5415, 385, 257, 857, 766, 6290, 13, 286, 603, 362, 281, 519, 466, 309, 257, 857, 544, 13, 583, 50800], "temperature": 0.0, "avg_logprob": -0.29368285743557676, "compression_ratio": 1.3153846153846154, "no_speech_prob": 0.0030667202081531286}, {"id": 477, "seek": 312944, "start": 3138.88, "end": 3142.8, "text": " yeah, Pym, Andrew, do you guys have some thought on this question on your side?", "tokens": [50836, 1338, 11, 430, 4199, 11, 10110, 11, 360, 291, 1074, 362, 512, 1194, 322, 341, 1168, 322, 428, 1252, 30, 51032], "temperature": 0.0, "avg_logprob": -0.29368285743557676, "compression_ratio": 1.3153846153846154, "no_speech_prob": 0.0030667202081531286}, {"id": 478, "seek": 314280, "start": 3142.8, "end": 3158.96, "text": " I mean, I think there's probably evidence somewhere that we tend to at least learn", "tokens": [50364, 286, 914, 11, 286, 519, 456, 311, 1391, 4467, 4079, 300, 321, 3928, 281, 412, 1935, 1466, 51172], "temperature": 0.0, "avg_logprob": -0.15233558416366577, "compression_ratio": 1.3644067796610169, "no_speech_prob": 0.010814092122018337}, {"id": 479, "seek": 314280, "start": 3158.96, "end": 3165.28, "text": " things in a compositional way. I mean, I guess whether there's some underlying", "tokens": [51172, 721, 294, 257, 10199, 2628, 636, 13, 286, 914, 11, 286, 2041, 1968, 456, 311, 512, 14217, 51488], "temperature": 0.0, "avg_logprob": -0.15233558416366577, "compression_ratio": 1.3644067796610169, "no_speech_prob": 0.010814092122018337}, {"id": 480, "seek": 316528, "start": 3166.2400000000002, "end": 3175.6800000000003, "text": " nature is a big question. It's surely a very nice way to organize existing information,", "tokens": [50412, 3687, 307, 257, 955, 1168, 13, 467, 311, 11468, 257, 588, 1481, 636, 281, 13859, 6741, 1589, 11, 50884], "temperature": 0.0, "avg_logprob": -0.17103581815152555, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.006790559738874435}, {"id": 481, "seek": 316528, "start": 3175.6800000000003, "end": 3183.2000000000003, "text": " let's say, if you think about it in a compositional manner. That just allows you to reason about it", "tokens": [50884, 718, 311, 584, 11, 498, 291, 519, 466, 309, 294, 257, 10199, 2628, 9060, 13, 663, 445, 4045, 291, 281, 1778, 466, 309, 51260], "temperature": 0.0, "avg_logprob": -0.17103581815152555, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.006790559738874435}, {"id": 482, "seek": 316528, "start": 3183.2000000000003, "end": 3193.92, "text": " a lot more easily. Okay, so let's see. There is a lot of new questions. The current top question,", "tokens": [51260, 257, 688, 544, 3612, 13, 1033, 11, 370, 718, 311, 536, 13, 821, 307, 257, 688, 295, 777, 1651, 13, 440, 2190, 1192, 1168, 11, 51796], "temperature": 0.0, "avg_logprob": -0.17103581815152555, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.006790559738874435}, {"id": 483, "seek": 319392, "start": 3194.0, "end": 3199.76, "text": " which I think hasn't already been answered. And I guess it comes from some of our more", "tokens": [50368, 597, 286, 519, 6132, 380, 1217, 668, 10103, 13, 400, 286, 2041, 309, 1487, 490, 512, 295, 527, 544, 50656], "temperature": 0.0, "avg_logprob": -0.11197805979165686, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.01182317640632391}, {"id": 484, "seek": 319392, "start": 3199.76, "end": 3204.8, "text": " mathematically oriented audience. Stanislav specifically asks, if we're going to talk about", "tokens": [50656, 44003, 21841, 4034, 13, 10061, 5788, 706, 4682, 8962, 11, 498, 321, 434, 516, 281, 751, 466, 50908], "temperature": 0.0, "avg_logprob": -0.11197805979165686, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.01182317640632391}, {"id": 485, "seek": 319392, "start": 3204.8, "end": 3209.52, "text": " two categories, should we actually then consider enriched and end categories in principle?", "tokens": [50908, 732, 10479, 11, 820, 321, 767, 550, 1949, 48624, 293, 917, 10479, 294, 8665, 30, 51144], "temperature": 0.0, "avg_logprob": -0.11197805979165686, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.01182317640632391}, {"id": 486, "seek": 319392, "start": 3211.04, "end": 3217.84, "text": " Yeah, so this is certainly the next step. So a lot of the things I did not mention in this", "tokens": [51220, 865, 11, 370, 341, 307, 3297, 264, 958, 1823, 13, 407, 257, 688, 295, 264, 721, 286, 630, 406, 2152, 294, 341, 51560], "temperature": 0.0, "avg_logprob": -0.11197805979165686, "compression_ratio": 1.5384615384615385, "no_speech_prob": 0.01182317640632391}, {"id": 487, "seek": 321784, "start": 3217.84, "end": 3226.1600000000003, "text": " very brief lecture is enriched categories or pre categories or higher category theory. There is", "tokens": [50364, 588, 5353, 7991, 307, 48624, 10479, 420, 659, 10479, 420, 2946, 7719, 5261, 13, 821, 307, 50780], "temperature": 0.0, "avg_logprob": -0.1229020962949659, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.004526949487626553}, {"id": 488, "seek": 321784, "start": 3226.1600000000003, "end": 3235.44, "text": " certainly an abundance of theory and thoughts and expressivity in all of these more nuanced areas.", "tokens": [50780, 3297, 364, 23391, 295, 5261, 293, 4598, 293, 5109, 4253, 294, 439, 295, 613, 544, 45115, 3179, 13, 51244], "temperature": 0.0, "avg_logprob": -0.1229020962949659, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.004526949487626553}, {"id": 489, "seek": 321784, "start": 3236.6400000000003, "end": 3241.84, "text": " So yeah, these are certainly things to study and give us a particular flavor of category theory.", "tokens": [51304, 407, 1338, 11, 613, 366, 3297, 721, 281, 2979, 293, 976, 505, 257, 1729, 6813, 295, 7719, 5261, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1229020962949659, "compression_ratio": 1.6820809248554913, "no_speech_prob": 0.004526949487626553}, {"id": 490, "seek": 324184, "start": 3242.1600000000003, "end": 3248.8, "text": " And yeah, I would invite you if you know, but I would consider these to be advanced topics for now.", "tokens": [50380, 400, 1338, 11, 286, 576, 7980, 291, 498, 291, 458, 11, 457, 286, 576, 1949, 613, 281, 312, 7339, 8378, 337, 586, 13, 50712], "temperature": 0.0, "avg_logprob": -0.13353486214914628, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0035839069169014692}, {"id": 491, "seek": 324184, "start": 3250.96, "end": 3256.1600000000003, "text": " Okay, yeah, I definitely agree. Let's learn how to crawl before we learn how to run. And", "tokens": [50820, 1033, 11, 1338, 11, 286, 2138, 3986, 13, 961, 311, 1466, 577, 281, 24767, 949, 321, 1466, 577, 281, 1190, 13, 400, 51080], "temperature": 0.0, "avg_logprob": -0.13353486214914628, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0035839069169014692}, {"id": 492, "seek": 324184, "start": 3256.1600000000003, "end": 3260.96, "text": " there is a reason why some people might perceive the content so far to be a bit slow starting.", "tokens": [51080, 456, 307, 257, 1778, 983, 512, 561, 1062, 20281, 264, 2701, 370, 1400, 281, 312, 257, 857, 2964, 2891, 13, 51320], "temperature": 0.0, "avg_logprob": -0.13353486214914628, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0035839069169014692}, {"id": 493, "seek": 324184, "start": 3260.96, "end": 3265.52, "text": " We have a very diverse audience coming from all sorts of backgrounds and we're trying to accommodate", "tokens": [51320, 492, 362, 257, 588, 9521, 4034, 1348, 490, 439, 7527, 295, 17336, 293, 321, 434, 1382, 281, 21410, 51548], "temperature": 0.0, "avg_logprob": -0.13353486214914628, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0035839069169014692}, {"id": 494, "seek": 326552, "start": 3265.52, "end": 3272.08, "text": " for all of those backgrounds appropriately. So we have one very fast-rising question from", "tokens": [50364, 337, 439, 295, 729, 17336, 23505, 13, 407, 321, 362, 472, 588, 2370, 12, 42125, 1168, 490, 50692], "temperature": 0.0, "avg_logprob": -0.1357066543013961, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.022958818823099136}, {"id": 495, "seek": 326552, "start": 3272.08, "end": 3276.64, "text": " Ewan, which asks, I'm aware there's been some research on category theory to motivate the", "tokens": [50692, 462, 7916, 11, 597, 8962, 11, 286, 478, 3650, 456, 311, 668, 512, 2132, 322, 7719, 5261, 281, 28497, 264, 50920], "temperature": 0.0, "avg_logprob": -0.1357066543013961, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.022958818823099136}, {"id": 496, "seek": 326552, "start": 3276.64, "end": 3282.24, "text": " graph neural network design. Has any work or much work been done to use these ideas to construct", "tokens": [50920, 4295, 18161, 3209, 1715, 13, 8646, 604, 589, 420, 709, 589, 668, 1096, 281, 764, 613, 3487, 281, 7690, 51200], "temperature": 0.0, "avg_logprob": -0.1357066543013961, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.022958818823099136}, {"id": 497, "seek": 326552, "start": 3282.24, "end": 3287.84, "text": " modular and composable neural networks or more interpretable representations in the spirit of", "tokens": [51200, 31111, 293, 10199, 712, 18161, 9590, 420, 544, 7302, 712, 33358, 294, 264, 3797, 295, 51480], "temperature": 0.0, "avg_logprob": -0.1357066543013961, "compression_ratio": 1.574468085106383, "no_speech_prob": 0.022958818823099136}, {"id": 498, "seek": 328784, "start": 3287.84, "end": 3291.1200000000003, "text": " say what Chris Ola has been doing with representations as types?", "tokens": [50364, 584, 437, 6688, 422, 875, 575, 668, 884, 365, 33358, 382, 3467, 30, 50528], "temperature": 0.0, "avg_logprob": -0.12774484297808478, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.005904562771320343}, {"id": 499, "seek": 328784, "start": 3293.84, "end": 3303.52, "text": " Yeah, so that's something I would love to think about and work on. To my knowledge, the answer is no.", "tokens": [50664, 865, 11, 370, 300, 311, 746, 286, 576, 959, 281, 519, 466, 293, 589, 322, 13, 1407, 452, 3601, 11, 264, 1867, 307, 572, 13, 51148], "temperature": 0.0, "avg_logprob": -0.12774484297808478, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.005904562771320343}, {"id": 500, "seek": 328784, "start": 3305.2000000000003, "end": 3310.0, "text": " Yeah, to my knowledge, the answer is no. But it seems like there is really no obstacle to doing", "tokens": [51232, 865, 11, 281, 452, 3601, 11, 264, 1867, 307, 572, 13, 583, 309, 2544, 411, 456, 307, 534, 572, 23112, 281, 884, 51472], "temperature": 0.0, "avg_logprob": -0.12774484297808478, "compression_ratio": 1.5411764705882354, "no_speech_prob": 0.005904562771320343}, {"id": 501, "seek": 331000, "start": 3310.0, "end": 3319.44, "text": " so, at least no major obstacle. So yeah, yeah. All right, then another top-rated question.", "tokens": [50364, 370, 11, 412, 1935, 572, 2563, 23112, 13, 407, 1338, 11, 1338, 13, 1057, 558, 11, 550, 1071, 1192, 12, 5468, 1168, 13, 50836], "temperature": 0.0, "avg_logprob": -0.19309078563343396, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.0018586966907605529}, {"id": 502, "seek": 331000, "start": 3319.44, "end": 3324.24, "text": " Yeva asks, which kinds of categories, broadly speaking, are we going to focus on most in this", "tokens": [50836, 835, 2757, 8962, 11, 597, 3685, 295, 10479, 11, 19511, 4124, 11, 366, 321, 516, 281, 1879, 322, 881, 294, 341, 51076], "temperature": 0.0, "avg_logprob": -0.19309078563343396, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.0018586966907605529}, {"id": 503, "seek": 331000, "start": 3324.24, "end": 3328.72, "text": " course? So what do we see to be the most interesting categories for deep learning at this time?", "tokens": [51076, 1164, 30, 407, 437, 360, 321, 536, 281, 312, 264, 881, 1880, 10479, 337, 2452, 2539, 412, 341, 565, 30, 51300], "temperature": 0.0, "avg_logprob": -0.19309078563343396, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.0018586966907605529}, {"id": 504, "seek": 331000, "start": 3330.72, "end": 3338.08, "text": " Right, so well, in this next week's lecture that Petter says is going to give, we're going to talk", "tokens": [51400, 1779, 11, 370, 731, 11, 294, 341, 958, 1243, 311, 7991, 300, 10472, 391, 1619, 307, 516, 281, 976, 11, 321, 434, 516, 281, 751, 51768], "temperature": 0.0, "avg_logprob": -0.19309078563343396, "compression_ratio": 1.5924369747899159, "no_speech_prob": 0.0018586966907605529}, {"id": 505, "seek": 333808, "start": 3338.08, "end": 3344.0, "text": " about categories in general. But the one after this, we're going to talk about, so categories", "tokens": [50364, 466, 10479, 294, 2674, 13, 583, 264, 472, 934, 341, 11, 321, 434, 516, 281, 751, 466, 11, 370, 10479, 50660], "temperature": 0.0, "avg_logprob": -0.13531232916790506, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.001569142914377153}, {"id": 506, "seek": 333808, "start": 3345.12, "end": 3351.2799999999997, "text": " allow us to compose processes in a sequence, which is useful, but in a way limited, because often", "tokens": [50716, 2089, 505, 281, 35925, 7555, 294, 257, 8310, 11, 597, 307, 4420, 11, 457, 294, 257, 636, 5567, 11, 570, 2049, 51024], "temperature": 0.0, "avg_logprob": -0.13531232916790506, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.001569142914377153}, {"id": 507, "seek": 333808, "start": 3351.2799999999997, "end": 3357.84, "text": " in nature, we compose processes that is in parallel. So in third lecture, we're going to be studying", "tokens": [51024, 294, 3687, 11, 321, 35925, 7555, 300, 307, 294, 8952, 13, 407, 294, 2636, 7991, 11, 321, 434, 516, 281, 312, 7601, 51352], "temperature": 0.0, "avg_logprob": -0.13531232916790506, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.001569142914377153}, {"id": 508, "seek": 333808, "start": 3357.84, "end": 3364.64, "text": " things called monoidal categories, where you can put processes in parallel. And very interestingly,", "tokens": [51352, 721, 1219, 1108, 17079, 304, 10479, 11, 689, 291, 393, 829, 7555, 294, 8952, 13, 400, 588, 25873, 11, 51692], "temperature": 0.0, "avg_logprob": -0.13531232916790506, "compression_ratio": 1.7657657657657657, "no_speech_prob": 0.001569142914377153}, {"id": 509, "seek": 336464, "start": 3364.72, "end": 3369.44, "text": " these are not processes where you can necessarily copy information, delete information, some", "tokens": [50368, 613, 366, 406, 7555, 689, 291, 393, 4725, 5055, 1589, 11, 12097, 1589, 11, 512, 50604], "temperature": 0.0, "avg_logprob": -0.0871522585550944, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.0006982365157455206}, {"id": 510, "seek": 336464, "start": 3369.44, "end": 3375.6, "text": " information. So we're going to add extra layers of new ones by studying things called Cartesian", "tokens": [50604, 1589, 13, 407, 321, 434, 516, 281, 909, 2857, 7914, 295, 777, 2306, 538, 7601, 721, 1219, 22478, 42434, 50912], "temperature": 0.0, "avg_logprob": -0.0871522585550944, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.0006982365157455206}, {"id": 511, "seek": 336464, "start": 3375.6, "end": 3382.96, "text": " categories where you can start to do all of these things. And then later, yeah, so we're going to", "tokens": [50912, 10479, 689, 291, 393, 722, 281, 360, 439, 295, 613, 721, 13, 400, 550, 1780, 11, 1338, 11, 370, 321, 434, 516, 281, 51280], "temperature": 0.0, "avg_logprob": -0.0871522585550944, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.0006982365157455206}, {"id": 512, "seek": 336464, "start": 3382.96, "end": 3391.44, "text": " study, yeah, I'm not sure how to best describe it right now without going into depth, how to start", "tokens": [51280, 2979, 11, 1338, 11, 286, 478, 406, 988, 577, 281, 1151, 6786, 309, 558, 586, 1553, 516, 666, 7161, 11, 577, 281, 722, 51704], "temperature": 0.0, "avg_logprob": -0.0871522585550944, "compression_ratio": 1.742081447963801, "no_speech_prob": 0.0006982365157455206}, {"id": 513, "seek": 339144, "start": 3391.44, "end": 3400.2400000000002, "text": " talking about sort of things used in equity variants. It's sort of the things that might", "tokens": [50364, 1417, 466, 1333, 295, 721, 1143, 294, 10769, 21669, 13, 467, 311, 1333, 295, 264, 721, 300, 1062, 50804], "temperature": 0.0, "avg_logprob": -0.12630781137718344, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.0015693055465817451}, {"id": 514, "seek": 339144, "start": 3400.2400000000002, "end": 3407.28, "text": " not be easy to explain right now before unpacking the lectures. But we're going to study part of", "tokens": [50804, 406, 312, 1858, 281, 2903, 558, 586, 949, 26699, 278, 264, 16564, 13, 583, 321, 434, 516, 281, 2979, 644, 295, 51156], "temperature": 0.0, "avg_logprob": -0.12630781137718344, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.0015693055465817451}, {"id": 515, "seek": 339144, "start": 3407.28, "end": 3411.12, "text": " the things we're studying here is not just categories, but the ways these are categories", "tokens": [51156, 264, 721, 321, 434, 7601, 510, 307, 406, 445, 10479, 11, 457, 264, 2098, 613, 366, 10479, 51348], "temperature": 0.0, "avg_logprob": -0.12630781137718344, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.0015693055465817451}, {"id": 516, "seek": 339144, "start": 3411.12, "end": 3416.08, "text": " are related and concepts build on top of them. So we're going to study functors between categories,", "tokens": [51348, 366, 4077, 293, 10392, 1322, 322, 1192, 295, 552, 13, 407, 321, 434, 516, 281, 2979, 1019, 5547, 1296, 10479, 11, 51596], "temperature": 0.0, "avg_logprob": -0.12630781137718344, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.0015693055465817451}, {"id": 517, "seek": 339144, "start": 3416.08, "end": 3420.8, "text": " monads on these categories, and various these algebraic structures that allow us to describe", "tokens": [51596, 1108, 5834, 322, 613, 10479, 11, 293, 3683, 613, 21989, 299, 9227, 300, 2089, 505, 281, 6786, 51832], "temperature": 0.0, "avg_logprob": -0.12630781137718344, "compression_ratio": 1.8830645161290323, "no_speech_prob": 0.0015693055465817451}, {"id": 518, "seek": 342080, "start": 3420.8, "end": 3426.6400000000003, "text": " this wiring of processes or some structure preserving maps between them. Yeah.", "tokens": [50364, 341, 27520, 295, 7555, 420, 512, 3877, 33173, 11317, 1296, 552, 13, 865, 13, 50656], "temperature": 0.0, "avg_logprob": -0.1398497990199498, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.0006449270877055824}, {"id": 519, "seek": 342080, "start": 3428.48, "end": 3433.28, "text": " All right. Thank you for that. The current talk question from Jeffrey asks,", "tokens": [50748, 1057, 558, 13, 1044, 291, 337, 300, 13, 440, 2190, 751, 1168, 490, 28721, 8962, 11, 50988], "temperature": 0.0, "avg_logprob": -0.1398497990199498, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.0006449270877055824}, {"id": 520, "seek": 342080, "start": 3433.28, "end": 3438.1600000000003, "text": " it's potentially also a philosophical question. How do you find or how do you decide what are", "tokens": [50988, 309, 311, 7263, 611, 257, 25066, 1168, 13, 1012, 360, 291, 915, 420, 577, 360, 291, 4536, 437, 366, 51232], "temperature": 0.0, "avg_logprob": -0.1398497990199498, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.0006449270877055824}, {"id": 521, "seek": 342080, "start": 3438.1600000000003, "end": 3443.84, "text": " the essential composable blocks of what you want to study? Yeah, I mean, I think this is", "tokens": [51232, 264, 7115, 10199, 712, 8474, 295, 437, 291, 528, 281, 2979, 30, 865, 11, 286, 914, 11, 286, 519, 341, 307, 51516], "temperature": 0.0, "avg_logprob": -0.1398497990199498, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.0006449270877055824}, {"id": 522, "seek": 342080, "start": 3443.84, "end": 3447.52, "text": " really a question that's not really specific to category theory. It's sort of", "tokens": [51516, 534, 257, 1168, 300, 311, 406, 534, 2685, 281, 7719, 5261, 13, 467, 311, 1333, 295, 51700], "temperature": 0.0, "avg_logprob": -0.1398497990199498, "compression_ratio": 1.6085271317829457, "no_speech_prob": 0.0006449270877055824}, {"id": 523, "seek": 344752, "start": 3448.48, "end": 3453.68, "text": " generally to science, we're trying to find building blocks and trying to find the basic", "tokens": [50412, 5101, 281, 3497, 11, 321, 434, 1382, 281, 915, 2390, 8474, 293, 1382, 281, 915, 264, 3875, 50672], "temperature": 0.0, "avg_logprob": -0.1499974659511021, "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.0008964678272604942}, {"id": 524, "seek": 344752, "start": 3453.68, "end": 3463.52, "text": " concepts. And I think this is really an art at this point. There's not, we cannot really formalize", "tokens": [50672, 10392, 13, 400, 286, 519, 341, 307, 534, 364, 1523, 412, 341, 935, 13, 821, 311, 406, 11, 321, 2644, 534, 9860, 1125, 51164], "temperature": 0.0, "avg_logprob": -0.1499974659511021, "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.0008964678272604942}, {"id": 525, "seek": 344752, "start": 3463.52, "end": 3474.88, "text": " and systemize this, the process of science yet. Okay, so I think our one hour block that you", "tokens": [51164, 293, 1185, 1125, 341, 11, 264, 1399, 295, 3497, 1939, 13, 1033, 11, 370, 286, 519, 527, 472, 1773, 3461, 300, 291, 51732], "temperature": 0.0, "avg_logprob": -0.1499974659511021, "compression_ratio": 1.5674157303370786, "no_speech_prob": 0.0008964678272604942}, {"id": 526, "seek": 347488, "start": 3474.88, "end": 3483.28, "text": " had allocated for this lecture has just expired. And also at the same time, I actually had to", "tokens": [50364, 632, 29772, 337, 341, 7991, 575, 445, 36587, 13, 400, 611, 412, 264, 912, 565, 11, 286, 767, 632, 281, 50784], "temperature": 0.0, "avg_logprob": -0.10913947388366028, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.0014538277173414826}, {"id": 527, "seek": 347488, "start": 3484.6400000000003, "end": 3489.92, "text": " reset my Zoom, so I actually don't see many of the questions that are still in the Q&A.", "tokens": [50852, 14322, 452, 13453, 11, 370, 286, 767, 500, 380, 536, 867, 295, 264, 1651, 300, 366, 920, 294, 264, 1249, 5, 32, 13, 51116], "temperature": 0.0, "avg_logprob": -0.10913947388366028, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.0014538277173414826}, {"id": 528, "seek": 347488, "start": 3490.6400000000003, "end": 3498.8, "text": " So perhaps if Andrew or someone can see if there's any other big salient questions left,", "tokens": [51152, 407, 4317, 498, 10110, 420, 1580, 393, 536, 498, 456, 311, 604, 661, 955, 1845, 1196, 1651, 1411, 11, 51560], "temperature": 0.0, "avg_logprob": -0.10913947388366028, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.0014538277173414826}, {"id": 529, "seek": 347488, "start": 3498.8, "end": 3502.6400000000003, "text": " otherwise I think it's okay if we continue the discussion on Zulip. We already covered", "tokens": [51560, 5911, 286, 519, 309, 311, 1392, 498, 321, 2354, 264, 5017, 322, 1176, 425, 647, 13, 492, 1217, 5343, 51752], "temperature": 0.0, "avg_logprob": -0.10913947388366028, "compression_ratio": 1.5521739130434782, "no_speech_prob": 0.0014538277173414826}, {"id": 530, "seek": 350264, "start": 3503.3599999999997, "end": 3509.7599999999998, "text": " a lot of grounds. The top one is just asking if we can put up slides in advance, which I think", "tokens": [50400, 257, 688, 295, 19196, 13, 440, 1192, 472, 307, 445, 3365, 498, 321, 393, 829, 493, 9788, 294, 7295, 11, 597, 286, 519, 50720], "temperature": 0.0, "avg_logprob": -0.08971590831361968, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0025895952712744474}, {"id": 531, "seek": 350264, "start": 3509.7599999999998, "end": 3516.56, "text": " we can try to do, but I would depend on the speaker. Yeah, I'll definitely put my slides up", "tokens": [50720, 321, 393, 853, 281, 360, 11, 457, 286, 576, 5672, 322, 264, 8145, 13, 865, 11, 286, 603, 2138, 829, 452, 9788, 493, 51060], "temperature": 0.0, "avg_logprob": -0.08971590831361968, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0025895952712744474}, {"id": 532, "seek": 350264, "start": 3516.56, "end": 3523.04, "text": " before the lecture and maybe we can start doing it more going forward. I think for the first lecture,", "tokens": [51060, 949, 264, 7991, 293, 1310, 321, 393, 722, 884, 309, 544, 516, 2128, 13, 286, 519, 337, 264, 700, 7991, 11, 51384], "temperature": 0.0, "avg_logprob": -0.08971590831361968, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0025895952712744474}, {"id": 533, "seek": 350264, "start": 3523.04, "end": 3528.72, "text": " we're just trying to make sure everybody gets access to this Zoom properly. But going forward,", "tokens": [51384, 321, 434, 445, 1382, 281, 652, 988, 2201, 2170, 2105, 281, 341, 13453, 6108, 13, 583, 516, 2128, 11, 51668], "temperature": 0.0, "avg_logprob": -0.08971590831361968, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0025895952712744474}, {"id": 534, "seek": 350264, "start": 3528.72, "end": 3532.4, "text": " when we start to get more technical, we will definitely aim to share the slides in advance.", "tokens": [51668, 562, 321, 722, 281, 483, 544, 6191, 11, 321, 486, 2138, 5939, 281, 2073, 264, 9788, 294, 7295, 13, 51852], "temperature": 0.0, "avg_logprob": -0.08971590831361968, "compression_ratio": 1.7592592592592593, "no_speech_prob": 0.0025895952712744474}, {"id": 535, "seek": 353264, "start": 3532.8799999999997, "end": 3549.68, "text": " Okay, yeah, so I think we will leave it at that. Thank you so much for coming to our first lecture,", "tokens": [50376, 1033, 11, 1338, 11, 370, 286, 519, 321, 486, 1856, 309, 412, 300, 13, 1044, 291, 370, 709, 337, 1348, 281, 527, 700, 7991, 11, 51216], "temperature": 0.0, "avg_logprob": -0.14344288962227958, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.001671650679782033}, {"id": 536, "seek": 353264, "start": 3549.68, "end": 3555.04, "text": " and we hope you enjoyed. Bruno, thank you so much also for delivering a great motivational", "tokens": [51216, 293, 321, 1454, 291, 4626, 13, 23046, 11, 1309, 291, 370, 709, 611, 337, 14666, 257, 869, 48186, 51484], "temperature": 0.0, "avg_logprob": -0.14344288962227958, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.001671650679782033}, {"id": 537, "seek": 353264, "start": 3556.48, "end": 3561.44, "text": " entrance to everything that will come next. And we hope you enjoyed it. We hope to keep", "tokens": [51556, 12014, 281, 1203, 300, 486, 808, 958, 13, 400, 321, 1454, 291, 4626, 309, 13, 492, 1454, 281, 1066, 51804], "temperature": 0.0, "avg_logprob": -0.14344288962227958, "compression_ratio": 1.5617977528089888, "no_speech_prob": 0.001671650679782033}, {"id": 538, "seek": 356144, "start": 3561.44, "end": 3567.76, "text": " the discussion going. So if you want to join us on Zulip in the coming days and weeks and discuss", "tokens": [50364, 264, 5017, 516, 13, 407, 498, 291, 528, 281, 3917, 505, 322, 1176, 425, 647, 294, 264, 1348, 1708, 293, 3259, 293, 2248, 50680], "temperature": 0.0, "avg_logprob": -0.05804619829878848, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.02027762494981289}, {"id": 539, "seek": 356144, "start": 3567.76, "end": 3574.8, "text": " the various aspects of the course with us as we go along and materials and so on, that would be", "tokens": [50680, 264, 3683, 7270, 295, 264, 1164, 365, 505, 382, 321, 352, 2051, 293, 5319, 293, 370, 322, 11, 300, 576, 312, 51032], "temperature": 0.0, "avg_logprob": -0.05804619829878848, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.02027762494981289}, {"id": 540, "seek": 356144, "start": 3574.8, "end": 3579.52, "text": " really great. And if you have any feedback on how things have gone today and how you would like them", "tokens": [51032, 534, 869, 13, 400, 498, 291, 362, 604, 5824, 322, 577, 721, 362, 2780, 965, 293, 577, 291, 576, 411, 552, 51268], "temperature": 0.0, "avg_logprob": -0.05804619829878848, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.02027762494981289}, {"id": 541, "seek": 356144, "start": 3579.52, "end": 3586.2400000000002, "text": " to go forward, please do interact with us. However, you prefer to leave us that feedback", "tokens": [51268, 281, 352, 2128, 11, 1767, 360, 4648, 365, 505, 13, 2908, 11, 291, 4382, 281, 1856, 505, 300, 5824, 51604], "temperature": 0.0, "avg_logprob": -0.05804619829878848, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.02027762494981289}, {"id": 542, "seek": 356144, "start": 3586.2400000000002, "end": 3591.04, "text": " directly or anonymously. We very much welcome any comments you have. It's a course we're actively", "tokens": [51604, 3838, 420, 37293, 5098, 13, 492, 588, 709, 2928, 604, 3053, 291, 362, 13, 467, 311, 257, 1164, 321, 434, 13022, 51844], "temperature": 0.0, "avg_logprob": -0.05804619829878848, "compression_ratio": 1.687719298245614, "no_speech_prob": 0.02027762494981289}, {"id": 543, "seek": 359104, "start": 3591.04, "end": 3597.84, "text": " building together with all of you. So on that note, let's thank Bruno one more time. And yeah,", "tokens": [50364, 2390, 1214, 365, 439, 295, 291, 13, 407, 322, 300, 3637, 11, 718, 311, 1309, 23046, 472, 544, 565, 13, 400, 1338, 11, 50704], "temperature": 0.0, "avg_logprob": -0.11133452824183873, "compression_ratio": 1.4437869822485208, "no_speech_prob": 0.0022832325194031}, {"id": 544, "seek": 359104, "start": 3597.84, "end": 3602.88, "text": " hope you enjoyed and hope you'll have a great rest of your week. I will see you in a week's time", "tokens": [50704, 1454, 291, 4626, 293, 1454, 291, 603, 362, 257, 869, 1472, 295, 428, 1243, 13, 286, 486, 536, 291, 294, 257, 1243, 311, 565, 50956], "temperature": 0.0, "avg_logprob": -0.11133452824183873, "compression_ratio": 1.4437869822485208, "no_speech_prob": 0.0022832325194031}, {"id": 545, "seek": 359104, "start": 3602.88, "end": 3606.96, "text": " for a discussion of fundamentals of category theory.", "tokens": [50956, 337, 257, 5017, 295, 29505, 295, 7719, 5261, 13, 51160], "temperature": 0.0, "avg_logprob": -0.11133452824183873, "compression_ratio": 1.4437869822485208, "no_speech_prob": 0.0022832325194031}], "language": "en"}