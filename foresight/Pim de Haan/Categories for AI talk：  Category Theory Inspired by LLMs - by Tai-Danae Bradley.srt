1
00:00:00,000 --> 00:00:03,000
Oh, sorry, yes. Is this recording?

2
00:00:10,000 --> 00:00:11,000
Good to go.

3
00:00:13,000 --> 00:00:19,000
Okay, so hello everyone and welcome to the third guest lecture of the cats for AI series.

4
00:00:19,000 --> 00:00:23,000
As you might have seen on the on the email list.

5
00:00:24,000 --> 00:00:31,000
We've finished with the main lectures and even two guest lectures and we're now transitioning in a different mode.

6
00:00:31,000 --> 00:00:36,000
So we're going to be having an irregular schedule of guest talks.

7
00:00:36,000 --> 00:00:45,000
And which are going to from now on be open to the public so if you've got a speaker recommendations please do not hesitate to reach out.

8
00:00:45,000 --> 00:01:03,000
And first in this one that this open kind of scheduling is a talk from tight and a Bradley, she's going to be telling us about category theory inspired by large language models, I'm very excited about the talk so tight please take it away.

9
00:01:03,000 --> 00:01:09,000
All right, thank you Bruno and Petra and all of the organizers for the invitation.

10
00:01:09,000 --> 00:01:18,000
And I just want to say I think it's really exciting to see that there's so much interest in category theory outside of the field of pure mathematics.

11
00:01:18,000 --> 00:01:30,000
And in particular in machine learning so I think that's very inspiring, I have a lot to learn from you all and so I hope that maybe I can give back something interesting in today's talk.

12
00:01:30,000 --> 00:01:41,000
So yes, as the title suggests, I want to share with you some category theory that's come from comes from work with some collaborators and I which I'll share in a second.

13
00:01:41,000 --> 00:01:55,000
That's inspired by large language models so I was asked to give this talk I think several months ago, and I thought it was very convenient that open AI release chat GPT, you know, like less than two weeks ago.

14
00:01:55,000 --> 00:02:04,000
So maybe I'm sure all of you have heard about that by now and I think it's kind of nice that, you know, language model or maybe fresh and everyone's mind.

15
00:02:04,000 --> 00:02:12,000
So, I couldn't resist. I know that you have all probably heard about chat GPT and you probably see in your own examples.

16
00:02:12,000 --> 00:02:24,000
I can resist just sharing one that I did. So I would say, don't really pay attention to what's on this slide because it's not really in the theme of the talk.

17
00:02:24,000 --> 00:02:41,000
I was just jumping into chat GPT, and I had a question about some physics thing that I was interested in. And so I, I gave it this, this thing called a quantum state just think of it as a vector, and I asked GPT chat GPT does this vector satisfy a certain

18
00:02:42,000 --> 00:03:07,000
like does it satisfy a property. And I thought it was really interesting that it got the answer wrong, but it knew what math steps to take and what pieces of that math to analyze in order to reason about a yes or no answer does this thing satisfy a property.

19
00:03:07,000 --> 00:03:19,000
So, it kind of like knew what linear algebra things to do you start with this vector and then you get a matrix and then you can put something on that matrix and then you apply this formula, and then at the end you reason about it.

20
00:03:19,000 --> 00:03:33,000
And so I thought that was like, oh, pretty neat, even though overall it kind of got stuff wrong, and at least kind of knew what to do without being told to reason about, you know, how would one even arrive at the answer.

21
00:03:33,000 --> 00:03:44,000
At this point, of course, there's lots of chatter and lots of buzz, you know, is this a GI, or is it garbage or whatever. And I don't really want to get too much into that.

22
00:03:44,000 --> 00:04:02,000
I do want to kind of draw one idea to the forefront of our minds. And that's this fact that the training data for large language models is really interesting. It's just raw text.

23
00:04:02,000 --> 00:04:14,000
And so, from a mathematician's perspective, that prompts a question like what what structure is in unstructured text.

24
00:04:14,000 --> 00:04:26,000
That allows you to like form concepts and reason about things so that you get actual coherent text. So whether or not, you know, chat GPT got the answer right or wrong.

25
00:04:26,000 --> 00:04:45,000
Okay, fine. But what's interesting is that there's no like textual garbage on this page right like all of these words go together and it sounds like coherent English it is coherent English, but you're just starting with, you know, raw text, or, you know, probability distributions on text

26
00:04:45,000 --> 00:05:07,000
configurations. And you can ask the question, what mathematical framework, can we, you know, write down that describes that passage from probability distributions on text continuations to some space of meaning or to some like

27
00:05:08,000 --> 00:05:20,000
semantic information. Like what's the math there. And, and you might ask this like, can we look inside of a transformer and like point to things and say here. Okay, so that's one thing you could do. That's not quite what I want to do.

28
00:05:20,000 --> 00:05:31,000
But maybe what I can describe in this talk is kind of an overall category theoretical framework that maybe can provide for you some tools if you wanted to dig further.

29
00:05:31,000 --> 00:05:45,000
So that's why the title of this talk is category theory, inspired by a large language models, as opposed to the category theory of large language models I'm not making a grandiose claim like that.

30
00:05:45,000 --> 00:05:59,000
But it is really interesting to ask, like how far can you get with just probability distributions on texts, or text continuations. And I think that category theory provides some some good and very natural tools to kind of

31
00:05:59,000 --> 00:06:15,000
exploring this question. So let me go to the next slide. So here's the outline for the remaining few minutes. First, I want to share with you a category of language and some advantages of that.

32
00:06:15,000 --> 00:06:26,000
We'll also see some disadvantages and then I will describe for you an enriched category of language so you've, we've heard a lot about category theory in this series.

33
00:06:26,000 --> 00:06:40,000
But maybe a new idea might be something called enriched category theory so I won't assume you have heard of that already. So I'll kind of explain a little bit of what that means, and then tie it back into this question that we're trying to answer in the context of language.

34
00:06:40,000 --> 00:06:55,000
So the next thing I'm sharing today is based on a paper together with John Torella and Janice Losopoulos. It came out earlier this year, earlier in the spring and enriched category theory of language.

35
00:06:55,000 --> 00:07:08,000
So, since this is an introductory series on category theory, what I thought I would do is just kind of give you a bird's eye view, or the highlights of some of the ideas that that we're working on.

36
00:07:08,000 --> 00:07:15,000
And then if you're interested, I would say, okay, for more details, you know, we can chat after you can take a look at the paper.

37
00:07:15,000 --> 00:07:24,000
Now, before all that, I want to motivate those ideas by sharing with you an analogy.

38
00:07:24,000 --> 00:07:41,000
And there's a really curious analogy between linear algebra and category theory is simple. I think it's really interesting. And maybe it might appeal, you know, to the machine learning community since we deal with vectors and major sees a lot.

39
00:07:41,000 --> 00:07:50,000
And I like this analogy a lot, it'll come up later but I also think it really nicely motivates the ideas that is kind of the main theme of the paper.

40
00:07:50,000 --> 00:08:11,000
Speaking of themes, this analogy that I will share with you kind of centers around a very common theme in mathematics. And I have to credit John Torella for really emphasizing this idea because I think it just illustrates in a crystal clear way the advantages of thinking from a category theoretical

41
00:08:11,000 --> 00:08:20,000
perspective. Okay, so before I share with you this analogy, let me inch towards it by telling you a common theme in mathematics.

42
00:08:20,000 --> 00:08:38,000
So, it's very well known in math that if blah, something has nice structure, and I'll say that means in a second, then functions from a fixed set into that thing have nice structure to.

43
00:08:38,000 --> 00:08:40,000
Okay, what do I mean.

44
00:08:40,000 --> 00:08:49,000
Think about a set x there it's on the screen. Okay, a set is just like a bag of marbles or something, it has no structure.

45
00:08:49,000 --> 00:09:01,000
So, if I have elements in a set x, I can't add them, I can't multiply them I can't combine them in any way to do anything meaningful it's just this set that has no structure.

46
00:09:01,000 --> 00:09:12,000
There is another set that has a lot of nice structure, kind of readily available and that's the reels like, for example, there's other things you could think of but let me just think about the reels.

47
00:09:12,000 --> 00:09:17,000
In, in our I can add things, we can multiply them.

48
00:09:17,000 --> 00:09:21,000
We can do all sorts of things so it has nice structure.

49
00:09:21,000 --> 00:09:28,000
Well, what is a function from a set x into the reels.

50
00:09:28,000 --> 00:09:37,000
I think for a second, for simplicity, just suppose that x has three elements, that's a three element set.

51
00:09:37,000 --> 00:09:46,000
So a function from a three element set into the reels is a choice of three numbers.

52
00:09:47,000 --> 00:09:59,000
So, if I look at all functions from x into the reels, I have the collection of all triples of numbers, and that is familiar to us that's just three dimensional space.

53
00:09:59,000 --> 00:10:09,000
In other words, what I'm getting at here is that the collection of all functions from let's say a three element set into our has nice structure.

54
00:10:09,000 --> 00:10:21,000
I can actually organize those elements, you know the image of this function as this array suggestively, because if I have two of these functions now or two of these arrays, I can add them.

55
00:10:21,000 --> 00:10:29,000
You just add functions point wise, which corresponds to sort of adding the entries of these vectors element wise.

56
00:10:29,000 --> 00:10:43,000
So you can multiply a function by a scalar and get another function that just corresponds to multiplying the entries of this vector component wise, you can multiply actually function together.

57
00:10:43,000 --> 00:10:56,000
So I'm hinting at here, which maybe you can see is that functions from x into our actually form of vector space. We usually denote that vector space by our to the three, our three.

58
00:10:57,000 --> 00:11:15,000
I could write this as our x our race to the power of x. In fact, we've seen this notation earlier, when Petter talked about exponential objects. So this is the set of all functions from a set into our is not just this that turns out to be a vector space.

59
00:11:15,000 --> 00:11:34,000
And that's because those functions inherit inherit structure from this sort of ground field. Okay, so vector spaces or linear algebra are an example, really nice example of where, if something has nice structure, then functions into that thing also inherit nice structure.

60
00:11:34,000 --> 00:11:41,000
And here's the key is that a very similar story holds in category theory.

61
00:11:42,000 --> 00:11:45,000
Here's kind of the analogy I like to have in mind.

62
00:11:45,000 --> 00:11:55,000
On the one hand, we just started with a set, and we, we said, okay, you can't really do much with the set there's no structure, it's just a collection of things.

63
00:11:55,000 --> 00:12:10,000
Now, if someone just hands you an arbitrary category, what do you know about it. Okay, you know I have some objects and some morphisms between those objects and composition and identity morphisms but that's it.

64
00:12:10,000 --> 00:12:22,000
You don't know if C is just an arbitrary category, if you can combine objects to get a new object in your category.

65
00:12:22,000 --> 00:12:30,000
You have other structure looking looking about maybe in specific categories you do but in general just the definition doesn't give you that.

66
00:12:30,000 --> 00:12:36,000
So, what you may like to do, just like in the case of sets.

67
00:12:36,000 --> 00:12:47,000
There, we thought of a specific set, like the reels that had a lot of structure, and we looked at functions into that.

68
00:12:47,000 --> 00:13:03,000
And obviously, it can be very fruitful to think about a category that has a lot of structure that's really nice, and look at functors into that category.

69
00:13:03,000 --> 00:13:16,000
So what category is that well it turns out that the category of sets is a really good choice. So analogous to looking from functions at functions from a set into the reels.

70
00:13:16,000 --> 00:13:27,000
Now, it turns out that looking at functors from an arbitrary category C into the category of sets is there's a lot of rich mathematics there.

71
00:13:27,000 --> 00:13:38,000
So why is that. Let me just say just like in the reels you can add numbers you can multiply them you can combine numbers to get new numbers in the category of set you can do this also.

72
00:13:38,000 --> 00:13:55,000
So, at the beginning of this series, Petter talked about an over fresher memory, you know you've heard of products or the Cartesian product of sets. He also mentioned the co product of sets, or you know I said the exponential object.

73
00:13:55,000 --> 00:14:10,000
So set has this nice property that you can take a bunch of sets and combine them in some way to get a new set. Now let me just kind of pull back the curtain and speak plainly for a minute.

74
00:14:10,000 --> 00:14:17,000
So the idea that I'm trying to mention right now are these constructions and category theory called limits and co limits.

75
00:14:17,000 --> 00:14:29,000
What I'm saying is that set has all limits and set has all co limits. So here's kind of what that means I think some of you are have probably heard these words or you're familiar with them already.

76
00:14:29,000 --> 00:14:42,000
But in mathematics, quite often what you want to do, you have a whole bunch of mathematical objects, maybe their sets, or maybe their groups, or maybe their topological spaces or maybe their vector spaces, and you ask,

77
00:14:42,000 --> 00:14:51,000
is there a way that I can take this collection of objects and combine them in such a way as to get a new object in my category.

78
00:14:51,000 --> 00:15:03,000
Now, there are sort of two typical ways that you can do this and by typical I mean these constructions are found all across the mathematical landscape.

79
00:15:03,000 --> 00:15:15,000
And it turns out that when you kind of strip the constructions of their deep of their details and just look at the pattern. These constructions turn out to fall into two categories for lack of better word.

80
00:15:15,000 --> 00:15:32,000
I mean that in like the English sense not the math sense. So on the one hand, you can form what's called the limit of those objects, and depending on how that's constructed these have familiar names like the intersection of two sets, or the Cartesian product of two sets,

81
00:15:32,000 --> 00:15:39,000
or the Cartesian product of two topological spaces, direct sums of vector spaces.

82
00:15:39,000 --> 00:15:56,000
Meets, if you have a lattice, and you can ask for the meat or, you know, sort of the minimum element, minimum element in the collection your lattice that's that's actually an example of a limit turns out greatest common divisors are also examples of these things.

83
00:15:56,000 --> 00:16:12,000
If you have a group homomorphism and you ask for its kernel, all of these constructions on the left hand side are subsumed by one single idea and category theory they are instantiation of one single idea that's called a limit.

84
00:16:12,000 --> 00:16:22,000
And dual to that, you know and category theory will always like stick the word co in front of something you kind of reverse the arrows and so you get dual operations.

85
00:16:22,000 --> 00:16:35,000
On the other hand, on the right hand side, you have things like unions co products direct sums again joins in a lattice dual to greatest common divisors least common multiple and co kernels.

86
00:16:35,000 --> 00:16:49,000
So these are all examples of things called limits and common limits, we won't worry about the actual definition, like they satisfy some universal property, but I mentioned that to say that in the category of set.

87
00:16:49,000 --> 00:17:04,000
It has this wonderful property that you can take any collection of sets, and then asked for the limit, or asked for the co limit, and it turns out to exist. So it's another set in that category, and it satisfies universal properties.

88
00:17:04,000 --> 00:17:13,000
You cannot do that with any category, any arbitrary category may or may not have that ability. It turns out the category sets does.

89
00:17:13,000 --> 00:17:28,000
So that's just to suggest you that just like the real numbers was a nice set to look at functions into analogously the category of sets is a nice category to look at functors into it.

90
00:17:28,000 --> 00:17:41,000
These functors because this is such a nice choice they have a name. So a functor from an arbitrary category C into the category sets. It's called a co pre sheaf.

91
00:17:41,000 --> 00:17:57,000
Sometimes, you may want to look at contra variant functors from C into set. So it's just a functor where kind of flips the arrows. So in that case, folks will put a little OP above C.

92
00:17:57,000 --> 00:18:07,000
They see up into set. Those are called pre sheaves. So pre sheaves. Let's just say functors.

93
00:18:07,000 --> 00:18:23,000
In any case, this is a nice thing to look at it turns out that functors from C into set itself form a category so just like functions from x into our form a set with structure.

94
00:18:23,000 --> 00:18:33,000
Functors from C into set form a category. It's a functor category and in fact, I think Pam talked about functor categories in his talk.

95
00:18:33,000 --> 00:18:42,000
There the notation might have been square bracket from C to set, but this exponential notation is this notation for that same concept.

96
00:18:42,000 --> 00:18:50,000
So it turns out objects or functors morphisms are natural transformations, but that's just kind of me drawing the analogy.

97
00:18:50,000 --> 00:18:53,000
Now let me give you one more analogy.

98
00:18:53,000 --> 00:18:57,000
And then we'll move on to the language.

99
00:18:57,000 --> 00:19:03,000
There are very special vectors on the left hand side.

100
00:19:03,000 --> 00:19:24,000
In particular, for each element in the set X, there's a very particular vector associated to the element and the analogous thing holds on the right hand side, and that vector is a one hot encoding or a basis vector.

101
00:19:24,000 --> 00:19:33,000
So for every element in the set X on the left hand side, there is a particular function.

102
00:19:33,000 --> 00:19:48,000
That sort of sends any other elements in the set to one, if it's the same as the element I chose or to zero otherwise, or if I write this out as an array it's just a vector with all zeros, except for one in the appropriate spot.

103
00:19:48,000 --> 00:20:03,000
Analogously, there are functors from a category C into the category set, one of these code precheves that is very special.

104
00:20:03,000 --> 00:20:06,000
And these are called representable functors.

105
00:20:06,000 --> 00:20:11,000
So for each objects little C in my category.

106
00:20:11,000 --> 00:20:17,000
There is a functor from the category into sets.

107
00:20:17,000 --> 00:20:21,000
There's another object in the category D.

108
00:20:21,000 --> 00:20:32,000
And it sends that object to the set of all morphisms from your given object C into D.

109
00:20:32,000 --> 00:20:50,000
I've seen this notation already in this series. Sometimes this set is denoted harm from C to D. Some other people, instead of writing harm, they'll write just the name of the category C so it kind of reminds you oh morphisms in what category C.

110
00:20:50,000 --> 00:21:06,000
I like to think of, these are called representable functors, these harm functors, representable functors, these are a particular kind of code precheve, or if I were to kind of move the argument to the left, it would be a precheve.

111
00:21:06,000 --> 00:21:21,000
I'll try later, not now, but I like to think of representable functors as kind of analogous to these one hot encodings, because just like any other vector is built up from one hot encodings.

112
00:21:21,000 --> 00:21:29,000
Similarly, any other code precheve is built up from these representables, but we'll say that later. So I like this analogy.

113
00:21:29,000 --> 00:21:46,000
The reason I mentioned this is because these representable functors will play an important role in just a second. Okay, so that's my analogy for linear algebra maps into law, inherit structure from blah.

114
00:21:46,000 --> 00:21:58,000
So in particular functions into the category set inherit really nice structure. Now let's see how this theme or this analogy plays out in the context of language.

115
00:21:58,000 --> 00:22:03,000
So let me describe for you a category of language.

116
00:22:03,000 --> 00:22:15,000
We'll see that it's nice, but like a set, you can't really do much with it. So we're going to look at functors out of that category. So let me describe that for you now is very simple.

117
00:22:15,000 --> 00:22:23,000
So here's a category, consider all strings from some finite set of atomic symbols.

118
00:22:23,000 --> 00:22:36,000
So if your finite set is the set of all words in English. Okay, it's a very large set but it's fine. You can just think of the take the free modeling on that set right all strings.

119
00:22:36,000 --> 00:22:51,000
The sub string containment defines a preorder on this set. In other words, you know, we can make sense of when one string is contained in another string x is contained in y, and I can denote that by less than or equal to.

120
00:22:51,000 --> 00:22:55,000
So I think Penn mentioned in his talk, the concept of a preorder.

121
00:22:55,000 --> 00:23:03,000
So in my way of reminder, a preorder is a binary relation denoted by less than or equal to that's reflexive.

122
00:23:03,000 --> 00:23:08,000
So every substring is every string is contained in itself. So X is less than X.

123
00:23:08,000 --> 00:23:10,000
And that's transitive.

124
00:23:10,000 --> 00:23:23,000
Right if blue on my screen is contained in small blue and small blue is contained in small blue marble that I know blue is contained in small blue marble. So that's transitivity.

125
00:23:23,000 --> 00:23:34,000
And as Penn mentioned, every preordered set is a category reflexivity is exactly an identity morphism transitivity is exactly composition.

126
00:23:34,000 --> 00:23:39,000
So I just defined for you a preordered set, aka a category.

127
00:23:39,000 --> 00:23:46,000
And it just has at most one morphism between any two objects in that category. So it's very simple.

128
00:23:46,000 --> 00:23:55,000
There's an arrow from X to Y whenever X is a substring of wine. Let's call this category L.

129
00:23:55,000 --> 00:24:05,000
Fine, it just kind of tells you what goes with what does this expression go with this expression. If the answer is yes there's an arrow. If the answer is no there's no arrow.

130
00:24:05,000 --> 00:24:22,000
It just tells you what goes with what. Okay, it's nice but that's very limited. In particular, I cannot, I cannot make sense of, like what's a concept in this category, or what's the context of something or can I combine ideas to get a new idea I don't have any of that

131
00:24:22,000 --> 00:24:24,000
structure here.

132
00:24:24,000 --> 00:24:37,000
So this is a cue from this theme that we just spent several minutes thinking about. And what we'd like to do is now consider functors from that category into the category of set.

133
00:24:37,000 --> 00:24:51,000
So this is just repeating that theme and representable functors these hum functors are particularly nice in this case. And I like to think of them as like a first approximation to the meaning of of an expression.

134
00:24:51,000 --> 00:24:57,000
Let's do that. Okay, pick, pick an expression in the category like the word blue.

135
00:24:57,000 --> 00:25:12,000
And let's look at the hum functor hum blue blank. Okay, where if I put in an expression X and the blank on the middle here, I get a set.

136
00:25:12,000 --> 00:25:29,000
This set just tells me, is there an arrow from blue to X, or not. If there is an arrow from blue tax, then home of blue comma X is just the one point set representing that one arrow.

137
00:25:29,000 --> 00:25:50,000
There's no arrow from blue tax. If blue is not contained in X, then I get the empty set. Okay, now I like this, because this representable functor sort of its pre image is the collection of all expressions that contain the word blue.

138
00:25:50,000 --> 00:26:08,000
And this reminds me of the onata lemma. So the onata limit is this. I'm sure you've heard of it this famous theorem and category theory, which essentially says that a mathematical object is completely determined by the network of relationships that has with other objects in the category.

139
00:26:08,000 --> 00:26:18,000
So you want to understand something important or all all important things about an object, you can look at morphisms out of that object or morphisms into it.

140
00:26:18,000 --> 00:26:34,000
And here, this representable functor picks out exactly the network of ways that the word you've chosen, like blue, or whatever fits into all expressions that contain it in the category in your language.

141
00:26:34,000 --> 00:26:44,000
So, you know, if you think about language a lot, then I'm sure you've heard, you know, this famous quote by John first, you show no word by the company it keeps.

142
00:26:44,000 --> 00:26:55,000
I mean, this is how word embeddings work to something of the meaning of a word is sort of captured in its context. So this representable functor captures precisely that context.

143
00:26:55,000 --> 00:27:06,000
So you might think of this as like capturing something of meaning but you know I put that in quotes, and I say it's a first approximation, because as you can already guess this isn't everything.

144
00:27:06,000 --> 00:27:15,000
You know, there's nothing about the distributional information here. This is just kind of yes or no bare bones, but it's a good start.

145
00:27:15,000 --> 00:27:27,000
So this is kind of like the native perspective. When I see representable functors, I kind of think, ah, the network of ways that that word fits into its environment so capture something of that.

146
00:27:27,000 --> 00:27:37,000
So here's, you know, I'm just kind of belaboring the point. But when I see this functor, another image that I have in mind is like, it's kind of like a vector.

147
00:27:37,000 --> 00:27:55,000
And I put sort of above the equal sign because this is not proper category theory. But when I think of the representable functor, I think of the vector of zeros and ones, or empty sets and one point sets right indexed by all of the expressions in the language, where

148
00:27:55,000 --> 00:28:14,000
an empty set in that slot, if blue is not contained in that expression like deep red being cherries, or a one point set in that slot if blue is contained in that, in that expression, that's indexing that spot like small blue marble, beautiful blue

149
00:28:14,000 --> 00:28:26,000
ocean, etc. So when I think of representable functor, I think of like a vector whose entries are empty, or, or one point set indexed by expressions in the language.

150
00:28:26,000 --> 00:28:31,000
Okay, this is just like something that I have in my mind.

151
00:28:31,000 --> 00:28:44,000
Now, why did we do this to make our lives more complicated? No. So as we said, when you look at functors into a category with rich structure, like set, you can do things with it.

152
00:28:44,000 --> 00:28:50,000
So I was kind of hinting that these representable functors behave like building blocks.

153
00:28:50,000 --> 00:29:02,000
We can actually use representable functors, or these co pre sheaves to construct new co pre sheaves, and we can do that by using the structure that is in the base category set.

154
00:29:02,000 --> 00:29:14,000
So I mentioned set has all limits, all co limits. It's also Cartesian close. What that means it has something like an internal home, which if you unwind all of that.

155
00:29:14,000 --> 00:29:29,000
What it suggests is that we have some kind of notion of conjunction. That's kind of like what a limit is disjunction. That's kind of like what a co limit is enclosure Cartesian closure is kind of something like implication.

156
00:29:29,000 --> 00:29:38,000
So, I want to give you a concrete example how am I making these connections like why is a co limit like disjunction or or.

157
00:29:38,000 --> 00:29:41,000
A concrete example of what that looks like.

158
00:29:41,000 --> 00:29:47,000
So suppose I have, you know, a functor representing red.

159
00:29:47,000 --> 00:29:51,000
That's like the network of ways red fits into the language.

160
00:29:51,000 --> 00:29:57,000
Suppose I also have the functor representing blue. Hey, the one we just looked at.

161
00:29:57,000 --> 00:30:09,000
So I'm claiming that if I have two functors, I can take what's called their co product. That's a kind of co limit, which I claim is analogous to to disjunction.

162
00:30:09,000 --> 00:30:20,000
I claim it's analogous to the concept quote unquote of red or blue. So here's why I make that claim. When you when you write down the definition. Here's what you find.

163
00:30:20,000 --> 00:30:31,000
So a co product means I can take two functors harm red comma blank co product together with harm blue comma blank.

164
00:30:31,000 --> 00:30:44,000
That defines a new functor that sends an expression X to the union of the two sets, harm of red index and harm of blue index.

165
00:30:44,000 --> 00:30:46,000
So here's what that means.

166
00:30:46,000 --> 00:30:58,000
I get a new functor, a new co pre chief. And again, if I envision it is like a vector whose entries are empty set or, or something else.

167
00:30:58,000 --> 00:31:13,000
What you find is that this functor is sort of supported on all texts that either contain red or contain blue or contain both of them.

168
00:31:13,000 --> 00:31:27,000
It's non empty on the union of all of those sets. So, for example, deep red being cherries that contains the word red so this that I get is the one point set small blue marble that contains the word blue.

169
00:31:27,000 --> 00:31:30,000
So I get the one point set.

170
00:31:30,000 --> 00:31:38,000
Did you put the kettle on contains neither of those words. So I get the empty set red and blue fireworks contains both.

171
00:31:38,000 --> 00:31:56,000
So I actually get the union of two one point sets or two points that. Okay, so it's not empty this functor this co product of red and blue is not empty on all sets that either contain red, sorry, all expressions that either contain red, or blue or both.

172
00:31:56,000 --> 00:32:07,000
And that pairs well with this idea of union as like or when dealing with sets. So that's kind of why we think of co products is kind of like disjunction.

173
00:32:07,000 --> 00:32:15,000
Okay, so there are other things you can do, you can take the product, you can do this thing called internal home which is like implication.

174
00:32:15,000 --> 00:32:24,000
Writing that down gets a little bit more complicated, but there are other limits and co limits. So the point is, so here's kind of a summary of what we've done so far.

175
00:32:24,000 --> 00:32:36,000
So we started with a very bare bones category L. It's like a preorder, it just tells you what goes with what. So, in that sense, you can kind of think of it as like syntax, maybe.

176
00:32:36,000 --> 00:32:38,000
What goes with what. Okay.

177
00:32:38,000 --> 00:32:53,000
So we took, we passed from that category to the set of the category of co pre sheaves on it. So that's what I'm kind of thinking as a synantics. So that's kind of where the meaning is in this United types sense.

178
00:32:53,000 --> 00:33:07,000
So every expression on the left, who responds to a representable functor on the right so that functor just picks out the kind of context of that word or all expressions that contain that word.

179
00:33:08,000 --> 00:33:15,000
This passage from left to right, this assignment, blue goes to harm blue, comma blank.

180
00:33:15,000 --> 00:33:22,000
That turns out to be a functor that functor is called the United embedding. So I mentioned that in case you've heard that word.

181
00:33:22,000 --> 00:33:31,000
The op that you see on the left, it just is saying that this United embedding is contra variant so versus arrows but that's kind of like bookkeeping.

182
00:33:32,000 --> 00:33:46,000
So, so this is just a summary of what we've done so far. We started with like a kind of in quotation marks think of it as like a syntax category very bare bones just tells you what goes with what, not a lot of structure, like no structure.

183
00:33:46,000 --> 00:33:59,000
And then if you pass to co pre sheaves or functors from that category into set, you have the ability to capture something of meaning of a word in the sense of john first, in the sense of the native lemma.

184
00:33:59,000 --> 00:34:08,000
And then you actually have structure in that category, and you can start to combine things in a way that kind of feels like logic or maybe like reasoning.

185
00:34:08,000 --> 00:34:18,000
I like to think of pictures so here it is, you know I start with a word and then I send it to like kind of this network of ways that it fits into the category.

186
00:34:18,000 --> 00:34:24,000
This is nice, but as you can already guess, it's very limited.

187
00:34:24,000 --> 00:34:35,000
It's just kind of like binary yes or no does this fit in kind of discrete. It has nothing. It knows nothing about the distributional information of language.

188
00:34:35,000 --> 00:34:51,000
So what you'd really like to do what would be better is if you have like you know version 2.0, where if I'm, you know, if you can give me the fact that blue is contained in small blue, but also what's the probability of seeing that.

189
00:34:51,000 --> 00:35:07,000
If I see the word blue what's the probability that you know it'll be completed by small blue marble, or, you know, whatever that probability is it's going to be higher than like I woke up and had a blue idea, just to borrow, you know, Chomsky or something.

190
00:35:07,000 --> 00:35:21,000
So you'd like to really wait the arrows in your category with conditional probabilities of continuing an expression with a larger expression.

191
00:35:22,000 --> 00:35:39,000
And then if you include this distributional information, then you can ask, okay, can I combine concepts there and is that kind of capturing the sort of framework of getting something like logic or reasoning from, you know, just knowing what goes with what together with the probabilities.

192
00:35:39,000 --> 00:35:50,000
Now the nice thing is that category theory provides a way to do this so this is exactly what we find an enriched category theory.

193
00:35:50,000 --> 00:35:55,000
So let me let me give a quick introduction to that what is an enriched category theory.

194
00:35:55,000 --> 00:36:00,000
So in category theory, if you have two arrows x to y.

195
00:36:00,000 --> 00:36:10,000
I'm going to give you a sum of x to y or what I'm denoting by C x to y that's the set of all morphisms from x to y. Okay, you asked that that be a set.

196
00:36:10,000 --> 00:36:17,000
The point is that in enriched category theory, that may not just be a set.

197
00:36:17,000 --> 00:36:24,000
It could be a set with extra structure or could not be a set.

198
00:36:24,000 --> 00:36:31,000
It could be a set with extra structure will think about it. If x and y are vector spaces.

199
00:36:31,000 --> 00:36:42,000
Then the set of all linear transformations from x to y is also a vector space. It's not just a set you can add linear transformations you can scale or multiply them.

200
00:36:42,000 --> 00:36:58,000
The HOM set is a set with additional structure it's actually a vector space. So in that case, one says the category of vector spaces is enriched over the category of vector spaces.

201
00:36:58,000 --> 00:37:04,000
So whatever your HOM objects are you say your category is enriched over that.

202
00:37:04,000 --> 00:37:14,000
I just said that on the upper left hand side. If this HOM set is a vector space you say oh my category is enriched over the category of vector spaces.

203
00:37:14,000 --> 00:37:26,000
If your HOM set is actually a group, you say your category is enriched over the category of groups. If it's a topological space you're enriched over topological spaces.

204
00:37:26,000 --> 00:37:37,000
If it turns out on the lower left, if this turns out to be a truth value like a zero or one, your categories enriched over truth values, those turn out to be exactly pre-orders.

205
00:37:37,000 --> 00:37:42,000
So those things that Pam was talking about earlier, these are actually doing enriched category theory.

206
00:37:42,000 --> 00:37:55,000
What if your HOM set is just a set, then your category is enriched over a set, and you're just doing ordinary category theory. So ordinary category theories like a special case of enriched category theory.

207
00:37:55,000 --> 00:38:00,000
That's something that we're most interested in for the remaining few minutes.

208
00:38:00,000 --> 00:38:12,000
If this is a conditional probability in a way that I'll explain in a second, then your category is enriched over the unit interval and that's kind of what I want to focus on just quickly.

209
00:38:12,000 --> 00:38:28,000
So what's the unit interval? The unit interval, think of it as a category whose objects are numbers between zero and one, and where there's an arrow from little a to little b if a is less than or equal to b.

210
00:38:28,000 --> 00:38:30,000
So it's a pre-order.

211
00:38:30,000 --> 00:38:41,000
Now, the unit interval viewed as a category have a lot of the same properties as the category of sets.

212
00:38:41,000 --> 00:38:52,000
So what I'm kind of hinting at is that we're now going to want to look at functors into the unit interval, which will be analogous to what we just did by looking at functors into set.

213
00:38:52,000 --> 00:39:05,000
So in what ways is the unit interval like the category of sets? So on the slide, I showed you, okay, on the one hand, your objects are sets or numbers.

214
00:39:05,000 --> 00:39:13,000
Your morphisms are functions or this pre-order. But what else do these two categories have in common?

215
00:39:13,000 --> 00:39:28,000
Well, they have a manoidal product. In other words, in the category of set, you can take the Cartesian product of two sets, you can multiply sets, and there's a unit for that multiplication, namely the one point set.

216
00:39:28,000 --> 00:39:38,000
So what that means is like, you know, a set x times the one point set is just isomorphic to x. So it's a unit with respect to this multiplication.

217
00:39:38,000 --> 00:39:47,000
Well, the unit interval also has a multiplication, of course, multiplication, and it has a unit with respect to that, namely the number one.

218
00:39:47,000 --> 00:39:53,000
So this turns out to be a manoidal product. So both categories are manoidal categories. But wait, there's more.

219
00:39:54,000 --> 00:40:02,000
We mentioned that the category of sets has, you can construct co-limits or limits.

220
00:40:02,000 --> 00:40:10,000
Turns out the same thing is true in the unit interval. So when you unwind the definition, what's the limit, what's the co-limit? It turns out just to be minimum.

221
00:40:10,000 --> 00:40:19,000
Even if I have a bunch of numbers in the unit interval, I can take their meat or their limit. It turns out just to be the minimum of the numbers in that set.

222
00:40:19,000 --> 00:40:23,000
And then finally, co-limits turn out just to be maximum.

223
00:40:23,000 --> 00:40:29,000
So, okay, both categories have all limits and co-limits, and then, you know, there's more.

224
00:40:29,000 --> 00:40:41,000
You have closure. So that kind of means you have this internal harm, or like Petra was saying earlier in this series, you have these exponential objects, which are kind of defined with respect to this multiplication.

225
00:40:41,000 --> 00:40:51,000
I don't want to touch into that, but I just want to kind of suggest in your mind that the unit interval can now play a similar role that the category of sets did.

226
00:40:51,000 --> 00:40:54,000
So how does this play out?

227
00:40:54,000 --> 00:41:01,000
Well, it turns out that we can discuss a category enriched over the unit interval.

228
00:41:01,000 --> 00:41:12,000
Well, just like a category enriched over sets is an ordinary category, we have an analogous idea where we switch out sets with the unit interval.

229
00:41:12,000 --> 00:41:23,000
So what is a category enriched over the unit interval, also called a zero one category? Well, just like with a normal category, you start with some objects.

230
00:41:23,000 --> 00:41:29,000
Similarly, a zero one category consists of some objects.

231
00:41:29,000 --> 00:41:42,000
What else do you have? Well, in category theory, for every pair of objects, X, Y, you ask for a set, hum, X, Y, or C of X, Y.

232
00:41:42,000 --> 00:41:57,000
Now I'm replacing set with the unit interval. So for every pair of objects on the right hand side, little X, little Y, I asked for a hum object, which is just an element in the unit interval.

233
00:41:57,000 --> 00:42:07,000
So for every pair of objects, there's a number associated with them. And I think of that as a hum number or hum object.

234
00:42:07,000 --> 00:42:23,000
Okay, what else is a zero one category consists of? Well, in normal category theory, we ask for composition, aka, if I have a way to get from X to Y, and why did Z, then I can compose them and get, you know, a morphism from X to Z.

235
00:42:23,000 --> 00:42:33,000
So now looked at the Cartesian product on the left hand side, we said that that's analogous to multiplication of numbers. So that's what you see on the right hand side.

236
00:42:33,000 --> 00:42:51,000
And just like morphisms are now this preorder, what I asked now for to have a zero one category is that there's a morphism from this hum object Y to Z times the hum object X to Y, that there's a morphism from that product to the hum object

237
00:42:51,000 --> 00:42:56,000
Oh, that should say X to Z on the right hand side. That's a typo.

238
00:42:56,000 --> 00:43:13,000
Okay, and then, you know, there's more, you can also ask for identities and identity morphism on an object X is just really a function from the one point set into the calm set from X to itself.

239
00:43:13,000 --> 00:43:28,000
So a morphism from a one point set into a set just picks out an element in that set. So similarly, if I kind of look at the analog on the right hand side, instead of a one point set I have the number one, which plays the role of unit.

240
00:43:28,000 --> 00:43:32,000
And instead of an arrow I have this less than or equal to.

241
00:43:32,000 --> 00:43:40,000
And so this is the data of the zero one category and of course you ask it to satisfy some axioms.

242
00:43:41,000 --> 00:43:58,000
You can also make sense once you have the notion of a, of a category enriched over the unit interval, you can discuss functors between two such categories but really it's, it's not a category in the traditional sense it's an enriched category.

243
00:43:58,000 --> 00:44:16,000
So you can make sense of enriched functors. So I'll just kind of go over this briefly but you can imagine on the right hand side. It's a function from the objects of your first enriched category to the objects of the second, that satisfies some inequality that's very reminiscent

244
00:44:16,000 --> 00:44:19,000
of what you might have an ordinary category theory.

245
00:44:19,000 --> 00:44:38,000
For the sake of time let's let's not think too deeply about this, because the real punchline is that what happens when D, the category you're mapping into is a really nice category like the category of sets previously that gave us co pre sheaves and we saw that had a

246
00:44:38,000 --> 00:44:58,000
really nice nature. So in this new kind of iteration, I want to replace set with probabilities, the zero one. And I can look at enriched co pre sheaves into the unit interval. That turns out as you may guess to form its own enriched category and you can talk about enriched natural

247
00:44:58,000 --> 00:45:01,000
transformations and make sense of all this.

248
00:45:01,000 --> 00:45:13,000
So let's not, you won't worry about them too much but that's kind of the idea. I just want to repeat the same story that we just did, but now I replaced the category of sets with the unit interval.

249
00:45:13,000 --> 00:45:26,000
So, I'll just kind of summarize and give you the punchline here's what happens when you do that. So here's now, you know, semantics 2.0 or syntax 2.0 or language as an enriched category.

250
00:45:26,000 --> 00:45:40,000
Like earlier I had a category with an arrow. If one expression is contained in the other. I'm going to now have the exact same thing and do exactly what I said I wish we could do, namely, decorate it with a conditional probability.

251
00:45:40,000 --> 00:45:55,000
The punchline is that fits exactly into this framework of enriched category theory. So language is a zero one category. Its objects are strings of symbols from some atomic set of symbols like before.

252
00:45:55,000 --> 00:46:11,000
The ham objects, the ham object between an expression X and Y will say it's the conditional probability that why this larger string is an extension of X or contains X.

253
00:46:11,000 --> 00:46:15,000
You know if X is it contained in it or it's zero otherwise.

254
00:46:15,000 --> 00:46:29,000
So you can check this actually defines an enriched category over zero one, you know, this sort of reflexivity or identity. Yeah, what's the probability that blue is contained in blue is one.

255
00:46:29,000 --> 00:46:36,000
And then when you write down what this means it turns out that conditional probabilities multiply in exactly what you need to get this composition.

256
00:46:36,000 --> 00:46:44,000
So all I'm saying here is that if you want to decorate these arrows with these conditional probabilities of continuation.

257
00:46:44,000 --> 00:46:52,000
It's like your dreams are fulfilled. Oh, wow, enriched category theory says that this is, you know, something you can actually do formally.

258
00:46:52,000 --> 00:47:04,000
As you may guess this is nice but it's limited as before. I can't combine anything, you know, there's no notion of concepts or combining things and having some kind of logical reasoning in it.

259
00:47:04,000 --> 00:47:13,000
So again, you want to then pass from from that category to zero one filters on that category.

260
00:47:13,000 --> 00:47:29,000
And so when you do this, you will find that representable functors like before are sort of have the right support. So as an example on this slide, you can look at the enriched representable functor of the word blue.

261
00:47:29,000 --> 00:47:38,000
And I like to think of it as like this vector where they're which again is indexed by expressions in the language where there is a zero.

262
00:47:38,000 --> 00:47:53,000
If that expression does not contain the word, the chosen word like blue, or it has this conditional probability, you know the conditional probability of seeing small blue marble given that you've seen blue maybe that's like point to two.

263
00:47:53,000 --> 00:47:58,000
So it has the same support as before but now it has this distributional information.

264
00:47:58,000 --> 00:48:14,000
And you can keep on going, you know you can combine these things. So it turns out that an enriched category theory, the appropriate notion of co products products, or more generally limits and co limits they're called weighted co limits.

265
00:48:14,000 --> 00:48:29,000
So that definition gets a little bit technical. But when you take, when you unwind that definition, you can sort of ask, hey, what's this enriched notion of red or blue, the sort of concept, if I take the co product of these two co pre sheaves.

266
00:48:29,000 --> 00:48:38,000
And if you choose the, these weights in the right way you basically get like that the point wise maximum of the two values individually.

267
00:48:38,000 --> 00:48:46,000
So I think that's interesting and you can kind of try to think about what that what does that mean, like if you were to interpret that.

268
00:48:46,000 --> 00:48:53,000
I'll let you partner that and since I'm running short on time. Let me just say there's a lot more that you can do here.

269
00:48:53,000 --> 00:49:06,000
So you may have already thought about the fact that you know you can get from the unit interval into the non negative extended reels using negative log, you know there's there's an isomorphism actually between them.

270
00:49:06,000 --> 00:49:23,000
And what that means is that your category which was enriched over the unit interval can now be enriched over non negative extended reels.

271
00:49:23,000 --> 00:49:33,000
And that has a name and category theory. So a category enriched over that that's a pre order so it's a category. Those are called generalized metric spaces.

272
00:49:33,000 --> 00:49:51,000
And that is that you can now think about the distances between these sort of concepts or distances between these representable and rich founders. And so what happens is that in this generalized metric space, you know, expressions that are likely

273
00:49:51,000 --> 00:50:02,000
are close together, like blue goes to blue marble so they're kind of close in this generalized metric. But then those which are not likely extensions are kind of infinitely far away.

274
00:50:02,000 --> 00:50:19,000
So maybe, you know, a sweet blue scent is not a thing that people say, or it has a low probability, because colors don't smell, or have smells. And so that kind of pairs well with your intuition that like hey that should be way far away.

275
00:50:19,000 --> 00:50:28,000
You can think about distances now, you have these, you know, abilities to can to combine concepts we just looked at co products but there's so much more that you can do.

276
00:50:28,000 --> 00:50:39,000
It turns out that this has nice connections to even tropical geometry. And so you on a spliceopolis might one of our co authors has really nice ideas about that, and lots more.

277
00:50:39,000 --> 00:50:44,000
So all that to say, kind of wrapping up.

278
00:50:44,000 --> 00:51:01,000
There's a lot that you can do by repeating this theme of starting with something that doesn't seem to have a lot of structure, like on the left hand side, and then looking at maps or functors into another category that has a lot of structure.

279
00:51:01,000 --> 00:51:10,000
And there you you have the ability to form concepts to talk about distances to think about tropical geometry, which you cannot do on the left hand side.

280
00:51:10,000 --> 00:51:13,000
So this all has an enriched story.

281
00:51:13,000 --> 00:51:19,000
And maybe I will just kind of leave you with this teaser.

282
00:51:19,000 --> 00:51:26,000
Everything I've described today, kind of rested on this analogy right between linear algebra and category theory.

283
00:51:26,000 --> 00:51:37,000
So we said that this co appreciate categories like a vector space. I hinted that these representable functors are like one hot encodings.

284
00:51:37,000 --> 00:51:51,000
And I told you I tell you why I think about that to think about it that way. So, just like every vector is a linear combination of these one hot encodings right you take your combination there like a basis.

285
00:51:51,000 --> 00:51:55,000
So it turns out in category theory there's a theorem.

286
00:51:55,000 --> 00:52:03,000
If you were to look so Emily real has a beautiful book on category theory, category theory in context, I think she calls this the density theorem.

287
00:52:03,000 --> 00:52:19,000
And it turns out that every co pre chief is a co limit of these representable functors. So just like every vector is built up from basis vectors, every functor from your category in this set is built up from these home functors.

288
00:52:19,000 --> 00:52:22,000
Okay, so they're kind of like a basis in that sense.

289
00:52:22,000 --> 00:52:31,000
And then the analogy goes on and I will just end here because I think this is really fascinating. And I don't think it's more than an analogy which is kind of puzzling.

290
00:52:31,000 --> 00:52:49,000
In linear algebra you have matrices in category theory you have pro functors. And it's basically, you know, if you write it down it's like the same thing as a matrix for every pair of objects and two categories you get a set a matrix for every two elements and a pair of, you know, a product of

291
00:52:50,000 --> 00:53:05,000
matrixes can be multiplied and you know the formula for that. So in category theory pro functors can be composed, and the formula for that which is a kind of co limit looks a lot like matrix multiplication.

292
00:53:06,000 --> 00:53:26,000
Interesting. Well, you know, in linear algebra, every matrix you can compute its SBD and get singular vectors, it turns out in category theory, every pro functor has something like singular vectors which is called the nuclear pro functor and in fact the way that you construct it is like line by line

293
00:53:26,000 --> 00:53:34,000
is analogous to how you compute the SBD. I think what's really interesting is that as far as I know this is just an analogy.

294
00:53:34,000 --> 00:53:44,000
And not like linear algebra is not a special case of category theory in this sense, but it's really curious that you can do things in linear algebra that we know and love.

295
00:53:45,000 --> 00:54:10,000
So why do I say that because I think that that analogy sort of is encouraging that category theory can be a very natural and beneficial environment in which to kind of understand maybe what's going on with large language models, when all they have to work on our, you know,

296
00:54:11,000 --> 00:54:18,000
distribution on text or probability distributions on text. And as we've seen, you can get quite far, at least from a mathematical perspective.

297
00:54:18,000 --> 00:54:29,000
So, thank you for your attention. And if you're interested in learning more than the paper is available online. So thanks everyone.

298
00:54:29,000 --> 00:54:39,000
Thank you so much, Tai. This was a wonderful talk that I think I'm going to have to keep digesting. I absolutely love the analogies.

299
00:54:39,000 --> 00:54:54,000
I'm just going to open up the questions right now. So if you have a question, feel free to post it in the Q&A, and we can read them out. Alternatively, you can also raise your hand and we can just unmute you.

300
00:54:54,000 --> 00:55:05,000
So while we collect the questions, it sounds like Petar has a question and I don't know if I should read Petar's question or Petardy, maybe you want to say it yourself.

301
00:55:05,000 --> 00:55:22,000
Yeah, I'm happy to post it myself. First of all, thanks so much for such a wonderful talk. I really enjoyed all of the connections and it really brought, I guess, a brand new set of goggles with which I can view all these large language models in a bit cleaner way.

302
00:55:22,000 --> 00:55:37,000
I had a question which it might be maybe trivially contained in what you already described, but, you know, while I think this theory is a nice way of explaining, you know, which word follows the next word and the similarity of sentences and stuff like that, which is what language models do in principle.

303
00:55:37,000 --> 00:55:52,000
It also seems like today you get a large difference between success and failure with these language models, depending on how much you hack the prompts. Like, there was this one paper that said you can just add let's think step by step and it suddenly improves your reasoning capabilities by a whole lot.

304
00:55:52,000 --> 00:56:01,000
I'm curious if you think your theory has an answer to why this happens or could have an answer for why this happens in the future. Yeah, I'm just really curious about that. Thank you.

305
00:56:01,000 --> 00:56:06,000
Yeah, that's such a great question. So I don't know yet.

306
00:56:06,000 --> 00:56:20,000
I don't know. But I think, I think it would take more investigating I think the kind of punchline that I wanted to convey is that maybe this could be a good direction to look in.

307
00:56:21,000 --> 00:56:37,000
If you can kind of have a mathematical framework where you can start to see how concepts combine, where you can start to see something like logical structure emerging, then, and if you kind of explore that a little bit more, then maybe you can start to

308
00:56:37,000 --> 00:56:52,000
pin down some tools that will then allow you to ask these kind of like interpretability questions are like, why is it that if I say, hey, can, you know, GBT and step by step, can you. Okay, but maybe now that there are tools that can allow you to do that and maybe it has to do something with

309
00:56:52,000 --> 00:57:07,000
the sort of structure, you know, one thing that I didn't say, but this co pre chief category set. See, that's an example of a topos, and a topos is known in mathematics as a good place to do logic.

310
00:57:07,000 --> 00:57:22,000
So that's encouraging from that perspective. I think it's much too early for me to give you like a definitive yes or no answer. But I think from a mathematical perspective it's promising because you have a lot of tools to kind of reason it exactly about these things that that are really good questions.

311
00:57:22,000 --> 00:57:26,000
So it's too early to say but maybe I could say it's helpful.

312
00:57:26,000 --> 00:57:34,000
Yeah, no worries, I just wanted to, I wanted to prompt you and see what happens. So, yeah.

313
00:57:35,000 --> 00:57:43,000
Thank you. Thank you so much.

314
00:57:43,000 --> 00:57:53,000
So there's been a, there's been a question during your one hot encoding slide from Jules about.

315
00:57:53,000 --> 00:58:05,000
So, if you go back to the, it's there. Right. It was the in general. So I think it was the slide after actually in general the star could be any set right, not just an integer.

316
00:58:05,000 --> 00:58:12,000
I think this, this might have been, I think this might have been remarking on the fact that one hot encoding.

317
00:58:12,000 --> 00:58:18,000
Well, maybe Jules wants to wants to elaborate on the question themselves.

318
00:58:19,000 --> 00:58:24,000
Okay, so, so Jules is saying this was answered later.

319
00:58:24,000 --> 00:58:27,000
So, so that's good.

320
00:58:27,000 --> 00:58:40,000
So I might ask a question myself so I absolutely love sort of the idea of thinking about structure in this way category theory has a lot of structure and when we think about unstructured text.

321
00:58:40,000 --> 00:58:46,000
It doesn't, it's just sort of strings and sort of connecting in this way is absolutely fascinating.

322
00:58:46,000 --> 00:59:04,000
What I'm curious about is, have you thought about connecting what you just said in this talk with with this other ways of structuring thanks text namely parts of speech, and sort of the kinds of parsing where I get a tree like structure of a sentence could

323
00:59:04,000 --> 00:59:13,000
they perhaps see from the network of relationship that some things are adjectives and others are perhaps nouns.

324
00:59:13,000 --> 00:59:19,000
Yeah, so that's a great question. Yes, you will notice I mentioned nothing of parts of speech.

325
00:59:19,000 --> 00:59:29,000
On the one hand that was done intentionally since you know, GPT is training data is just raw text and one doesn't have to tag parts of speech.

326
00:59:29,000 --> 00:59:39,000
Yeah, once you have this framework, you can ask, like, can you know, chat GPT give me some examples of adjectives.

327
00:59:39,000 --> 00:59:47,000
Give me some examples of nouns I haven't tried that yet actually someone should try it or maybe I'll try it after this and see if it does it correctly. So yeah, does it learn things like that.

328
00:59:47,000 --> 01:00:04,000
So I think, like my answer to Petter, it's hopeful. In fact, there's a philosopher that also we're working with Juan Luis Gastaldi who has some very good ideas in this direction. So I would say, stay tuned.

329
01:00:04,000 --> 01:00:18,000
And let's see right now it's again, kind of too early but these are things that we're definitely thinking about and looking to go in that direction and some some folks even in our research circle have ideas but but still work in progress.

330
01:00:18,000 --> 01:00:20,000
But yeah, that's a really great question.

331
01:00:20,000 --> 01:00:33,000
Thank you. So we have a question from Tali. The question is, is there an analogy between matrix algebra and profanctors. Sorry, if there if there is an analogy between matrix algebra and profanctors.

332
01:00:33,000 --> 01:00:38,000
What would be the categorical analog of higher order arrays or tensor networks.

333
01:00:38,000 --> 01:00:53,000
Yeah, that's a fantastic question. So just like so a higher, so higher order array. So matrix is a function function. Let me just blow a matrix is a function from a product of two sets.

334
01:00:53,000 --> 01:01:03,000
So a tensor of order three would be a function from a product of three sets, a tensor of order 10 would be a function from a product of forces.

335
01:01:03,000 --> 01:01:18,000
So just like you can take, take the product of more than two sets. You can also look at functors from a product of more than two categories. So profanctors also have higher, you know, higher order array analogs.

336
01:01:18,000 --> 01:01:30,000
And that's easy to write down. So, yes, so these things called tensor, you know, higher order tensors they have analogs and category and they're just kind of straightforward generalization.

337
01:01:31,000 --> 01:01:38,000
So we have raised hand from Pym.

338
01:01:38,000 --> 01:01:44,000
Hi, thanks for the very interesting talk I have a very pedestrian question sorry for that.

339
01:01:44,000 --> 01:01:52,000
At some point you talked about the composition of like in the 01 category of the syntax.

340
01:01:52,000 --> 01:01:57,000
Could you go to that slide perhaps, because I kind of.

341
01:01:57,000 --> 01:02:07,000
Yeah, here. So here we have like why given X and Z given Y is Z given X.

342
01:02:07,000 --> 01:02:17,000
Normally, I would guess you have some sort of a sum over why there. How does that work.

343
01:02:17,000 --> 01:02:38,000
Yeah, so here there's no sum. I'm so when I look on the right side my mouse for some reason is not appearing on my slide but when I look at the arrow from blue to small blue, think of point to two is the probability of small blue conditioned on blue.

344
01:02:38,000 --> 01:02:44,000
So the probability of seeing small blue, given that I have just seek for blue.

345
01:02:44,000 --> 01:02:49,000
And there's only one way of getting there. So, okay, okay, I see.

346
01:02:49,000 --> 01:02:52,000
Good. Thank you for asking for clarification. Good.

347
01:02:52,000 --> 01:03:02,000
Okay, and this equality because typically I guess for this category wouldn't actually need equality here, right. You would. Yeah, so we just have an inequality would suffice.

348
01:03:02,000 --> 01:03:11,000
Yeah, okay. Yeah, and any quality. Actually, both of those equalities like you just need any quality, but we happen to get equality here.

349
01:03:11,000 --> 01:03:13,000
Okay, yeah, okay, thanks.

350
01:03:13,000 --> 01:03:18,000
Yeah, thank you.

351
01:03:18,000 --> 01:03:32,000
So, by the way, here in Glasgow we have a big watch party watching your talk ties so there's Mateo capuchy in the room with me who's going to come here and he is interested in asking a question.

352
01:03:32,000 --> 01:03:37,000
Oh, fantastic. I didn't know there was a watch party. Hi.

353
01:03:37,000 --> 01:03:47,000
I had just like, so I know about this co cat, which is another categorical framework for doing natural language processing and I'm wondering what's the relation.

354
01:03:47,000 --> 01:03:55,000
So, um, disco cat I think is inherently looking at connections with quantum physics.

355
01:03:55,000 --> 01:04:10,000
So I have not mentioned anything about quantum here. And also, the question that I'm trying to answer is a little bit different. So here I'm trying to see how can I start with probability distributions on texts.

356
01:04:10,000 --> 01:04:23,000
And how can I pass from that into something that feels like meaning or semantic or something that has something about knowledge or reasoning, you know, inspired by large language models.

357
01:04:23,000 --> 01:04:28,000
So the question that I'm starting with is a little bit different.

358
01:04:28,000 --> 01:04:35,000
And the tools or the sort of assumptions that I'm making like I haven't said anything about quantum physics or

359
01:04:35,000 --> 01:04:54,000
math. I'm also here. I mean, I could do this but I haven't. I get asked for a representation of all of this information. So I'm just working with categories right but you know if I look at a transformer there's no like category written down in the paper, you know, attention is all you need.

360
01:04:54,000 --> 01:05:07,000
So I could ask, Okay, now that I have all of this mathematical structure, can I represent it when your altruba is a nice way to represent things. So can I, you know, represent them by vector spaces.

361
01:05:07,000 --> 01:05:20,000
Could those vector spaces then be tagged with parts of speech which is kind of like what's happening in disco cat. Can I then like making a comparison with similar structure that appears in quantum. So I'm not doing anything like that.

362
01:05:20,000 --> 01:05:37,000
I could want to. I mean, I could, and I have a paper on this with the honest that came out I think last year. So you could ask for a representation of this kind of information this category theoretical information in terms of linear algebra.

363
01:05:37,000 --> 01:05:56,000
We think that to actually piggyback on someone else's question tensor networks are are very good choice for that. But even then sort of our premise for choosing that is a little bit different. So it turns out that in, you know, condensed matter physics and quantum

364
01:05:56,000 --> 01:06:05,000
physics, they have very nice tools that happen to kind of match with the statistics of language.

365
01:06:05,000 --> 01:06:20,000
But those tools, you know, can be used outside of the physics context, even though historically they've been used there. So even then in that work that we're doing we're not really saying anything like languages quantum or you know entanglement means this nothing, nothing like that.

366
01:06:20,000 --> 01:06:32,000
I think that disco cat is quite different from this, even though we both happen to be, you know, thinking about language in terms of category but the questions we're answering are for different the tools are different.

367
01:06:32,000 --> 01:06:39,000
The sort of premise or the reason why using those tools are different. Yeah, that's kind of my high level answer to that.

368
01:06:39,000 --> 01:06:41,000
Thank you very much.

369
01:06:41,000 --> 01:06:45,000
Thank you for the question.

370
01:06:45,000 --> 01:06:51,000
I think this might be a good, good place to stop.

371
01:06:51,000 --> 01:06:59,000
We're a bit over time, but this was absolutely fantastic. So I'll just say my thanks one more time.

372
01:06:59,000 --> 01:07:10,000
As now we're basically since we're done with the main part of the course really what we're now in cats for AI as I've mentioned very old, the things we're going to be having our guest lectures.

373
01:07:10,000 --> 01:07:22,000
So this is going to be on a regular schedule and if there's any recommendations you have from people who would love to talk or who you think could contribute meaningfully here would love to hear about it.

374
01:07:22,000 --> 01:07:38,000
So far we have two future lectures scheduled, which are which are going to be sometime in March and we're going to update you on it as we learn more. I don't know if there's anybody else from the organizing team that has to say anything.

375
01:07:38,000 --> 01:07:48,000
Other than one of those talks will be by David Spivak, so you should definitely not miss it.

376
01:07:48,000 --> 01:07:51,000
Okay, so that's it. Thank you very much.

377
01:07:51,000 --> 01:07:54,000
And see you next time.

378
01:07:54,000 --> 01:07:57,000
Thank you all for the invitation for your time. Appreciate it.

379
01:07:57,000 --> 01:08:00,000
Thanks so much for coming time. Really enjoyed it.

