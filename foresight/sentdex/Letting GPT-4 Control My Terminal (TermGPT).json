{"text": " Giving GPT-4 full access to my terminal. For some of you, you're way too excited about the prospects of such a thing, and this probably scares the heck out of many others of you. The idea is actually pretty simple and safe. We're really just trying to make prototyping and R&D with GPT-4 and models like it much more fluid. As it stands right now, you might go to the chat GPT UI or some other chat UI and ask it for something like a matplotlib graphing example. You'll wait for the generation, then you'll copy and paste it into an editor, and then run it, and then maybe then you'll ask for some changes, more weight, more copy and pasta, and so on. What I want to do instead is eliminate a lot of the sort of waiting and going from prompt to result, and I really want to just make this as quickly happen as quickly as possible, as well as require as little effort as possible. While still reviewing, what the model actually wishes to do before allowing it to do that. We can even do the edit to dark theme too in this way. For something so simple as just a matplotlib example, these are fairly minimal time savings, although I think the effort is still pretty low comparatively. But you can also quickly see that we issued one prompt and then we waited for GPT-4 to generate all the commands at once, each of which would have been a separate kind of manual back and forth for us, especially if we hit some sort of error, or we're just waiting for something to happen. But instead we can review and run all of the commands in one go. As we continue to ask for more complicated things or even projects that might require multiple files or even installations of packages in setting up an environment, or even as we want maybe to modify those files, this sort of structure can wind up saving us a dawn of time and energy. So let's see some more fancy examples. Imagine that you found some sort of model on HuggingFace, like this code completion language model from Replit. That's very lightweight, and you'd like to make a demo with it by implementing the model into some sort of locally hosted web page with a text form input for the prompt and then some logic to display the output, you know, the continued text. Some very common use case for this particular model. So probably GPT-4 really first needs to know what model do we want to use, and then maybe do some light reading on the model's page to understand a bit about, you know, how to actually work with this model. Temp GPT doesn't have any web parsing ability built in, but it can program for us, and we can certainly parse with a program. So we can start off by asking Temp GPT to do just that for us. And it does this first by installing beautiful soup for Python, making the directory we asked for, and then writing some web scraping code to get the text from that page, and then saving it to the text file that we requested. We can then use Temp GPT to read that content.text file and create a web app for us that demos its abilities. We did hit an error along the way, which I just copy and paste to Temp GPT both times, and it is fixed. This is one thing I'm considering fully automating, and I'm trying to do that by allowing Temp GPT to read the terminal outputs to determine if more commands or changes need to be made, if an error is at least detected. The error and suggestion in the error text here could have been ingested by Temp GPT to easily implement and fix not really much different than us copying and pasting the error ourselves. Anyway, after reading the error, Temp GPT finds a way to remedy the problem and our program works. This has been extremely minimal with effort compared to what I would have done, even if I was either traditionally doing this by hand or even via the chat UI for chat GPT. And we can definitely do this in an even more like less clunky way. Speaking of which, how about a web parser idea? Can we use Temp GPT to modify a pre-existing project? Absolutely. Might we actually use Temp GPT to modify itself to add something like a web parser? Here, I ask Temp GPT to implement a web parsing feature similar to my file reading feature, and it modifies multiple areas of the Temp GPT script to do this. Testing this feature, we can ask Temp GPT to parse some web page and then save a summary of that text to file. For example, let's parse the readme page from GitHub for TensorFlow and see if GPT-4 can summarize it for us into one sentence about what TensorFlow is. In this case, though, it is plausible that GPT-4 could also just have used its own underlying knowledge of TensorFlow. For example, we could just outright ask GPT-4 about TensorFlow without any other context, and it could summarize into one sentence. Instead, let's ask about something newer, so we could ask about stable studio from Stability AI. Asking GPT-4 via the chat UI, and it looks like this time GPT-4 doesn't actually know, which makes sense. This is relatively new, and GPT-4's knowledge stops in late March or something like that. So then we can go back to our example, parse that readme, and get a good summary of it. Okay, so those are just some really quick examples of what's possible, and to be honest, this has all just been kind of my first attempts as I'm trying to kind of make this happen, as this is a common, at least personally workflow for me, and I really just wanted to stop, you know, highlighting or clicking copy code, paste, run, copy some error, go back, like all this like back and forth was getting very old, and I really just wanted to come up with a solution that would just automatically kind of do these things for me, execute the commands for me, all of that. So this is really my first run through, and there are for sure going to be better ways to do the things that I'm doing here, and I definitely there's lots of things that we could build on, and I'm going to be hosting this code. So if you have ideas or things you want to try, feel free to fork or submit a poll request or anything like that. So with that, let's go ahead and dive into the code and see how everything is working under the hood. All right, so here with the code, let's go ahead and run through how everything works. So first, we've just got a bunch of imports. Open AI for the API, of course, Colorama to make things pretty, time for a sleep context. This is how I'm setting the pre prompt. We will come to that in a moment, but for now, that's just that's it's the pre prompt sub process. This is for attempting to execute commands and get their output. I'm still struggling with this. Maybe someone can help me figure out what the heck why this doesn't work. I'm sure it's probably some super simple logic problem. But anyway, read for regular expressions, requests and beautiful soup for the web parsing capability that term GPT actually added for itself. Again, and this actually isn't really all that required since we term GPT can already write a web parser. But the real kicker here is, you know, it could also write code to open up a file. But what you want to be able to do is read that information into context. So doing that, I think still kind of requires these features, but I want to kind of clean this up and quite possibly make it a little more dynamic. So for example, like pulling from a website, do you really just want the paragraph text? Do we want to also handle for JavaScript that happens to populate things? You know, so like this is a really basic example, but this won't get you very far. So anyway, I'm still kind of thinking about how I really want to do that. But anyway, moving along, the open AI key, really all the open AI stuff, I've got a video on that. I'll try to remember to put a link in the description for the basics of working with that API. But anyway, that's where our key is debug. This is just for a few extra print statements, just to mostly for outputting the full context every time I query GPT for just because that'll tell you for sure what exactly you're sending and getting back. I find that to be the biggest amount of information that you don't always want to see. But if you're having a problem, you probably want to see that whole thing. So anyway, moving along, message history is that terminal commands. That's just like the from the context that initial stuff that comes in. Then we have the regular expression pattern for the sort of functionality of reading a file or reading a website. So this is just read a file. This is W for website. It's just going to follow that. Again, we really could use large language models to also parse this out and it wouldn't even need to be like super structured in any way. It could just read some text and then determine is there a file that should be read? If yes, read that file, add that in. So you don't necessarily have to do it this way. Again, more on that and the context and all that stuff in a little bit because eventually I would like to do this without using open AI. I'd like to use an open source model. So I just don't want to get too deep into prompt engineering with GPT for when I'd like to I'm kind of like holding out for a really solid open source model with a solid license. So no llama. Anyway, moving on GPT query. So this is pretty much nothing new since the GPT for tutorial. So again, if any of this is confusing to you other than maybe the color Rama stuff, this just sets a color sets a styling basically does the text whatever and then resets it back to normal. So that's just that's just typical color Rama stuff. But anyway, if the debug is true, we are sending the entire message history, which is the full context that we're sending off to GPT for. So if anything's not working the way we're expecting, that's the first thing that we're going to do. So for example, reading the console outputs like trying to get the return on in the console. It doesn't give the most recent command no matter how hard I try. So I don't know. At first I thought it was while I'm hitting errors and I was thinking the error is causing my problem. But I'm pretty sure I'm handling it such that you would still you should still capture the error. But for some reason I'm just not. So anyway, moving along. So this is just a query GPT for API extract paragraphs. This is the code that GPT for slash term GPT wrote and modified into our script for us. Just simply reads a website grabs all the paragraph text. So really, really basic. Again, I think this should be a little better. But moving along. So then we've just got some pretty colorama stuff. Welcome to term GPT. It just tells you kind of the point of term GPT is just to make prototyping faster. Not a whole bunch of other fancy stuff. It's just make prototyping and R&D really quick. But don't run commands without still being able to review them. So continuing along. So there's just some more color color stuff. You've seen what it outputs already. So then we get the input whatever the user wants to, you know, put in the command. And then we have these very simple ones like clear. This just clears out that whole contextual history and just resets us back to terminal commands. We'll talk about we'll show those momentarily output. This reads theoretically the output from the system basically in the terminal. What, you know, what was the response back to that terminal? What was printed out in that terminal? So if you did hit an error, it would be nice to be able to say output or, you know, call this command. And then it would read that rather than you needing to copy and paste in the error. But again, it's not really working. So I'm going to copy and paste the error still. Otherwise, what it's going to do is it's going to look for any matches for do we want to read any files and it can do any number of files. And then essentially what it's going to do is either for files or websites that text that it parses, it's just going to shove it in and just it's going to build that context. And so it's just going to populate that context with the actual text. So it just adds it in. So you'll say I have this file here and then you'll say you'll have like the read command and it will quite literally say the file name and then the content of the file. So again, no different than what you would say. Hey, I've got this Python file. Here's the code copy, paste you paste in the code. This is the same exact thing. It just builds that context for me. So once we have that user input, which is going to be, you know, please help me make a flash website or please help me do, you know, whatever it is, we have that user input, we're going to send that off to GPT. And we're essentially going to wait until GPT is done. So what does that mean? So now it's time to go to our context. So this is the pre-prompt that I'm using to force GPT-4 to behave in the way that I want. So essentially, this is all sent off and this is how the program begins. So we're starting off by just telling GPT-4. Let me add word wrap real quick, make this a little easier on us. We're saying, as if I had told it, you know, give in text input from a user looking to do programming, help me build a series of console commands that will achieve the goals of the prompt. Your responses will come one command at a time where each response is just the command and nothing else. When ready for the next command, the user will say next, all camps next. And you will respond with the next command. When we have reached the end of console commands, respond with done. And then you will do it again with some new input from the user. If you understand, say okay. And again, we're just forcing the assistant here, you know, to think in historically it said, okay. And then what I'm doing is a one shot kind of example. I did find GPT-4 to be pretty good at following this command, but very often it's still added commentary to the commands and all that. And yes, we could use literally a large language model to parse out the command still. I don't like that. So I wanted to do at least a one shot. I thought maybe it might take more, but a one shot gives us an opportunity to show it even more basic stuff. So like for example, in this case, we're using cat to write to these files. But there's lots of ways that this could have been done. But given all of the huge variability of stuff that can happen inside of a script, it would likely break many of the methods that might be used to populate a file. So this one is just the one that works. So it allows us to kind of still kind of show really what we're looking for. So for example, with such a basic thing as show me a basic flask web dev example with templates, right? And the first command, in this case, we're making this up. We're showing GPT-4 how we want it to behave. So we're saying if someone asked this, here's an example of the sequence of events that should occur. First, you're going to need to pip install flask. And then the user is going to say, next. And then GPT-4 is going to respond with, you're going to want to make a directory basic flask app. Next, so already we're trying to get it to kind of put this into specific project folders. Then what it's going to do is actually write and then into that directory and then app.py. It's going to add just a very basic app.py with some template examples. Thank you. And then, okay, that's what it says. And then the user, next. And then it wants to make a template stir, add that index.html. Next, blah, blah, blah. You get the idea all the way down to actually running that app.py. The user says, next. And actually we're done because we've run the app. So it's just a really simple example that we're showing GPT-4. This is how I want you to respond every time. And then if that way, if the user did happen to say clear or whatever, we just reset right back to this context and continue on. But I found that you can generally keep going pretty far and be totally fine. But if you are experiencing weird things, you would just clear it and start over. But anyway, coming over here, while not GPT-done. So GPT-done starts as, where the heck are you here? Because we haven't started a new input. So we're going to start as GPT-done equals false. So then what's going to happen is, as long as that's true, what we're going to do is we're going to wait for the next command from the user. And essentially, it's going to just inject that. So we don't need to say next. We're just going to automatically tell GPT-4 next in the script. And we're just waiting until we literally see reply-content.lower. The only reason I'm doing this, GPT-4 always did an all caps done. It was good. But chat GPT, for example, I was just trying between the two of them to see if we save some money. It would sometimes lowercase, sometimes add a period. So I was trying to handle for that. But again, GPT-4 really follows this very well. I'm sure I could come up with ways to structure things such that chat GPT or GPT-3.5 works. There's no reason in my head that this couldn't be done with chat GPT. But GPT-4 is just way better than chat GPT. So anyway, moving on. So if it has reached a done, then we set done to true. Cool. And we break out. And then essentially, all the way up to this point, we've been, every time we get a command, we've been adding that command to a list of commands right here. Once we have that list of commands, you could have been theoretically running them, getting the return, and plausibly adding this to the context or something just to see, did you hit an arrow along the way? I think that's a mistake because either you're going to have to sit there and keep clicking like next or saying next or whatever. Or you're going to be blindly running commands. And you're probably more likely to just kind of blindly just hit go, go, go. And I don't think that's a good idea. So we're going to build that whole list. And then we're essentially going to iterate through that whole list, printing it out in bold red, making it clear what the commands are that are going to be run. And then if we want to actually run those commands, then we are going to go ahead and iterate through all of them and run them. And this is where I'm attempting to get the actual output. This does work all the way up to, I believe, the last output. I need to look a little deeper into that and see if there's got to be something wrong with my logic or something. Like I feel like this should work, but for whatever reason, it's just not working. Anyway, if the user puts in anything other than Y here, it just doesn't run the commands. And that's fine. Sometimes still, it does suggest to you, it might still give you some text, especially I found that to be true with the console output. It'll just say, oh, I didn't find any errors or something like that. So it doesn't say done, like it should. But anyway, that's all there is to it. All of the power of this, this is all just a bunch of stupid logic, really around what is the pre-prompt. And what we can do with a pre-prompt is, I mean, there's essentially infinite pre-prompts that we could theoretically use. There are better examples plausibly that we could show it. And there's more capabilities that we could show it or teach it or even just kind of that might show up. So even given this really basic example, I have found term GPT using GPT-4 to be really impressive. It really hasn't let me down on any task yet. Certainly, there's really nothing that GPT-4 could do for me via the chat UI that I haven't been able to just do via term GPT. And it's just been so much easier. And again, I made this really for myself and I'm just sharing it with people. And it's for sure a work in progress. There are some relatively new open source large language models that I'd really like to see can they fill in this gap. Because like I said, even chat GPT is weak-ish at this. Again, I think I could come up with prompts that chat GPT or GPT-3.5 rather could be successful with. So I just, I don't want to put in a bunch of time making chat GPT work. I'd rather it be some sort of open source model with a permissive license. So anyway, stay tuned for that. And again, I will put everything up on GitHub. Feel free to make a fork or whatever or make pull requests and just kind of tinker with this. I think this is kind of cool. I like the idea of going back and forth like this. And just doing the R&D with GPT-4 but just removing all of that extra effort. It's been, it's been really fun using it. So anyway, hopefully you guys will enjoy it. If you've got questions, comments, concerns, whatever suggestions, feel free to leave them below. Otherwise, I will see you all in another video. If you've ever wondered and wanted to know more about how neural networks actually work, including the optimization and fitment, rather than just simply calling some method, then you might be interested in checking out the neural networks from scratch book by myself and Daniel Kukiawa. The book can be had in the form of an ebook PDF, softcover or hardcover. And we ship for free worldwide. Also, the physical books just come with an ebook copy. All copies are in full color, which helps because there's a lot of code syntax highlighting and lots of charts and diagrams to help convey the principles. Also, almost all of those charts and diagrams have QR codes and links that take you to animations to help further illustrate the concepts. This is truly a real neural networks from scratch, teaching everything from the forward pass calculation of laws, back propagation and optimization. The only math that you're expected to know coming in is basic algebra. The rest is taught by us in the book, step by step. Everything is shown and explained in the book first logically, then mathematically, then via raw Python code, no third party libraries. And then finally optimized via NumPy. And this is for every step of the way, building and training actual neural networks for a fully fundamental understanding of neural networks and how they work from scratch. If at any point you're lost or confused, all copies of the book also grant access to a Google Docs version of the book, where you can ask your questions in line with the actual text itself. This is an incredible project that I'm extremely proud of to share with you all. We've sold over 13,000 copies to students all over the world. If you're looking to take your knowledge of deep learning to the next level, or if you're just looking to start that journey, I can't imagine a better way. So to learn more and buy yourself a copy, you can head to nnfs.io.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.44, "text": " Giving GPT-4 full access to my terminal.", "tokens": [50364, 28983, 26039, 51, 12, 19, 1577, 2105, 281, 452, 14709, 13, 50536], "temperature": 0.0, "avg_logprob": -0.13217005133628845, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.03064398653805256}, {"id": 1, "seek": 0, "start": 3.44, "end": 7.04, "text": " For some of you, you're way too excited about the prospects of such a thing,", "tokens": [50536, 1171, 512, 295, 291, 11, 291, 434, 636, 886, 2919, 466, 264, 32933, 295, 1270, 257, 551, 11, 50716], "temperature": 0.0, "avg_logprob": -0.13217005133628845, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.03064398653805256}, {"id": 2, "seek": 0, "start": 7.04, "end": 9.68, "text": " and this probably scares the heck out of many others of you.", "tokens": [50716, 293, 341, 1391, 35721, 264, 12872, 484, 295, 867, 2357, 295, 291, 13, 50848], "temperature": 0.0, "avg_logprob": -0.13217005133628845, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.03064398653805256}, {"id": 3, "seek": 0, "start": 10.24, "end": 12.88, "text": " The idea is actually pretty simple and safe.", "tokens": [50876, 440, 1558, 307, 767, 1238, 2199, 293, 3273, 13, 51008], "temperature": 0.0, "avg_logprob": -0.13217005133628845, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.03064398653805256}, {"id": 4, "seek": 0, "start": 12.88, "end": 20.56, "text": " We're really just trying to make prototyping and R&D with GPT-4 and models like it much more fluid.", "tokens": [51008, 492, 434, 534, 445, 1382, 281, 652, 46219, 3381, 293, 497, 5, 35, 365, 26039, 51, 12, 19, 293, 5245, 411, 309, 709, 544, 9113, 13, 51392], "temperature": 0.0, "avg_logprob": -0.13217005133628845, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.03064398653805256}, {"id": 5, "seek": 0, "start": 20.56, "end": 25.12, "text": " As it stands right now, you might go to the chat GPT UI or some other chat UI", "tokens": [51392, 1018, 309, 7382, 558, 586, 11, 291, 1062, 352, 281, 264, 5081, 26039, 51, 15682, 420, 512, 661, 5081, 15682, 51620], "temperature": 0.0, "avg_logprob": -0.13217005133628845, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.03064398653805256}, {"id": 6, "seek": 0, "start": 25.68, "end": 29.52, "text": " and ask it for something like a matplotlib graphing example.", "tokens": [51648, 293, 1029, 309, 337, 746, 411, 257, 3803, 564, 310, 38270, 1295, 79, 571, 1365, 13, 51840], "temperature": 0.0, "avg_logprob": -0.13217005133628845, "compression_ratio": 1.6267605633802817, "no_speech_prob": 0.03064398653805256}, {"id": 7, "seek": 3000, "start": 30.0, "end": 33.52, "text": " You'll wait for the generation, then you'll copy and paste it into an editor,", "tokens": [50364, 509, 603, 1699, 337, 264, 5125, 11, 550, 291, 603, 5055, 293, 9163, 309, 666, 364, 9839, 11, 50540], "temperature": 0.0, "avg_logprob": -0.1088592564618146, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.0015486013144254684}, {"id": 8, "seek": 3000, "start": 33.52, "end": 36.16, "text": " and then run it, and then maybe then you'll ask for some changes,", "tokens": [50540, 293, 550, 1190, 309, 11, 293, 550, 1310, 550, 291, 603, 1029, 337, 512, 2962, 11, 50672], "temperature": 0.0, "avg_logprob": -0.1088592564618146, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.0015486013144254684}, {"id": 9, "seek": 3000, "start": 36.16, "end": 39.6, "text": " more weight, more copy and pasta, and so on.", "tokens": [50672, 544, 3364, 11, 544, 5055, 293, 13296, 11, 293, 370, 322, 13, 50844], "temperature": 0.0, "avg_logprob": -0.1088592564618146, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.0015486013144254684}, {"id": 10, "seek": 3000, "start": 39.6, "end": 43.84, "text": " What I want to do instead is eliminate a lot of the sort of waiting", "tokens": [50844, 708, 286, 528, 281, 360, 2602, 307, 13819, 257, 688, 295, 264, 1333, 295, 3806, 51056], "temperature": 0.0, "avg_logprob": -0.1088592564618146, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.0015486013144254684}, {"id": 11, "seek": 3000, "start": 43.84, "end": 49.6, "text": " and going from prompt to result, and I really want to just make this as quickly", "tokens": [51056, 293, 516, 490, 12391, 281, 1874, 11, 293, 286, 534, 528, 281, 445, 652, 341, 382, 2661, 51344], "temperature": 0.0, "avg_logprob": -0.1088592564618146, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.0015486013144254684}, {"id": 12, "seek": 3000, "start": 50.32, "end": 55.92, "text": " happen as quickly as possible, as well as require as little effort as possible.", "tokens": [51380, 1051, 382, 2661, 382, 1944, 11, 382, 731, 382, 3651, 382, 707, 4630, 382, 1944, 13, 51660], "temperature": 0.0, "avg_logprob": -0.1088592564618146, "compression_ratio": 1.7854077253218885, "no_speech_prob": 0.0015486013144254684}, {"id": 13, "seek": 5592, "start": 55.92, "end": 59.28, "text": " While still reviewing, what the model actually wishes to do", "tokens": [50364, 3987, 920, 19576, 11, 437, 264, 2316, 767, 15065, 281, 360, 50532], "temperature": 0.0, "avg_logprob": -0.07998227264921544, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.011331471614539623}, {"id": 14, "seek": 5592, "start": 59.28, "end": 61.6, "text": " before allowing it to do that.", "tokens": [50532, 949, 8293, 309, 281, 360, 300, 13, 50648], "temperature": 0.0, "avg_logprob": -0.07998227264921544, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.011331471614539623}, {"id": 15, "seek": 5592, "start": 61.6, "end": 65.12, "text": " We can even do the edit to dark theme too in this way.", "tokens": [50648, 492, 393, 754, 360, 264, 8129, 281, 2877, 6314, 886, 294, 341, 636, 13, 50824], "temperature": 0.0, "avg_logprob": -0.07998227264921544, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.011331471614539623}, {"id": 16, "seek": 5592, "start": 65.12, "end": 68.4, "text": " For something so simple as just a matplotlib example,", "tokens": [50824, 1171, 746, 370, 2199, 382, 445, 257, 3803, 564, 310, 38270, 1365, 11, 50988], "temperature": 0.0, "avg_logprob": -0.07998227264921544, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.011331471614539623}, {"id": 17, "seek": 5592, "start": 68.4, "end": 74.16, "text": " these are fairly minimal time savings, although I think the effort is still pretty low comparatively.", "tokens": [50988, 613, 366, 6457, 13206, 565, 13454, 11, 4878, 286, 519, 264, 4630, 307, 920, 1238, 2295, 6311, 19020, 13, 51276], "temperature": 0.0, "avg_logprob": -0.07998227264921544, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.011331471614539623}, {"id": 18, "seek": 5592, "start": 74.16, "end": 78.96000000000001, "text": " But you can also quickly see that we issued one prompt and then we waited for GPT-4", "tokens": [51276, 583, 291, 393, 611, 2661, 536, 300, 321, 14379, 472, 12391, 293, 550, 321, 15240, 337, 26039, 51, 12, 19, 51516], "temperature": 0.0, "avg_logprob": -0.07998227264921544, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.011331471614539623}, {"id": 19, "seek": 5592, "start": 78.96000000000001, "end": 83.52000000000001, "text": " to generate all the commands at once, each of which would have been a separate kind of", "tokens": [51516, 281, 8460, 439, 264, 16901, 412, 1564, 11, 1184, 295, 597, 576, 362, 668, 257, 4994, 733, 295, 51744], "temperature": 0.0, "avg_logprob": -0.07998227264921544, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.011331471614539623}, {"id": 20, "seek": 8352, "start": 83.52, "end": 86.88, "text": " manual back and forth for us, especially if we hit some sort of error,", "tokens": [50364, 9688, 646, 293, 5220, 337, 505, 11, 2318, 498, 321, 2045, 512, 1333, 295, 6713, 11, 50532], "temperature": 0.0, "avg_logprob": -0.07171799455370222, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.012820021249353886}, {"id": 21, "seek": 8352, "start": 86.88, "end": 89.11999999999999, "text": " or we're just waiting for something to happen.", "tokens": [50532, 420, 321, 434, 445, 3806, 337, 746, 281, 1051, 13, 50644], "temperature": 0.0, "avg_logprob": -0.07171799455370222, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.012820021249353886}, {"id": 22, "seek": 8352, "start": 89.11999999999999, "end": 94.39999999999999, "text": " But instead we can review and run all of the commands in one go.", "tokens": [50644, 583, 2602, 321, 393, 3131, 293, 1190, 439, 295, 264, 16901, 294, 472, 352, 13, 50908], "temperature": 0.0, "avg_logprob": -0.07171799455370222, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.012820021249353886}, {"id": 23, "seek": 8352, "start": 94.39999999999999, "end": 99.28, "text": " As we continue to ask for more complicated things or even projects that might require", "tokens": [50908, 1018, 321, 2354, 281, 1029, 337, 544, 6179, 721, 420, 754, 4455, 300, 1062, 3651, 51152], "temperature": 0.0, "avg_logprob": -0.07171799455370222, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.012820021249353886}, {"id": 24, "seek": 8352, "start": 99.28, "end": 103.75999999999999, "text": " multiple files or even installations of packages in setting up an environment,", "tokens": [51152, 3866, 7098, 420, 754, 41932, 295, 17401, 294, 3287, 493, 364, 2823, 11, 51376], "temperature": 0.0, "avg_logprob": -0.07171799455370222, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.012820021249353886}, {"id": 25, "seek": 8352, "start": 103.75999999999999, "end": 108.0, "text": " or even as we want maybe to modify those files,", "tokens": [51376, 420, 754, 382, 321, 528, 1310, 281, 16927, 729, 7098, 11, 51588], "temperature": 0.0, "avg_logprob": -0.07171799455370222, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.012820021249353886}, {"id": 26, "seek": 8352, "start": 108.0, "end": 112.47999999999999, "text": " this sort of structure can wind up saving us a dawn of time and energy.", "tokens": [51588, 341, 1333, 295, 3877, 393, 2468, 493, 6816, 505, 257, 18192, 295, 565, 293, 2281, 13, 51812], "temperature": 0.0, "avg_logprob": -0.07171799455370222, "compression_ratio": 1.6920289855072463, "no_speech_prob": 0.012820021249353886}, {"id": 27, "seek": 11248, "start": 112.48, "end": 115.28, "text": " So let's see some more fancy examples.", "tokens": [50364, 407, 718, 311, 536, 512, 544, 10247, 5110, 13, 50504], "temperature": 0.0, "avg_logprob": -0.10438300882066999, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0006878054700791836}, {"id": 28, "seek": 11248, "start": 115.28, "end": 118.24000000000001, "text": " Imagine that you found some sort of model on HuggingFace,", "tokens": [50504, 11739, 300, 291, 1352, 512, 1333, 295, 2316, 322, 46892, 3249, 37, 617, 11, 50652], "temperature": 0.0, "avg_logprob": -0.10438300882066999, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0006878054700791836}, {"id": 29, "seek": 11248, "start": 118.24000000000001, "end": 121.84, "text": " like this code completion language model from Replit.", "tokens": [50652, 411, 341, 3089, 19372, 2856, 2316, 490, 47762, 270, 13, 50832], "temperature": 0.0, "avg_logprob": -0.10438300882066999, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0006878054700791836}, {"id": 30, "seek": 11248, "start": 121.84, "end": 126.08, "text": " That's very lightweight, and you'd like to make a demo with it by implementing", "tokens": [50832, 663, 311, 588, 22052, 11, 293, 291, 1116, 411, 281, 652, 257, 10723, 365, 309, 538, 18114, 51044], "temperature": 0.0, "avg_logprob": -0.10438300882066999, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0006878054700791836}, {"id": 31, "seek": 11248, "start": 126.08, "end": 129.68, "text": " the model into some sort of locally hosted web page", "tokens": [51044, 264, 2316, 666, 512, 1333, 295, 16143, 19204, 3670, 3028, 51224], "temperature": 0.0, "avg_logprob": -0.10438300882066999, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0006878054700791836}, {"id": 32, "seek": 11248, "start": 129.68, "end": 135.68, "text": " with a text form input for the prompt and then some logic to display the output,", "tokens": [51224, 365, 257, 2487, 1254, 4846, 337, 264, 12391, 293, 550, 512, 9952, 281, 4674, 264, 5598, 11, 51524], "temperature": 0.0, "avg_logprob": -0.10438300882066999, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0006878054700791836}, {"id": 33, "seek": 11248, "start": 135.68, "end": 136.88, "text": " you know, the continued text.", "tokens": [51524, 291, 458, 11, 264, 7014, 2487, 13, 51584], "temperature": 0.0, "avg_logprob": -0.10438300882066999, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0006878054700791836}, {"id": 34, "seek": 11248, "start": 136.88, "end": 141.44, "text": " Some very common use case for this particular model.", "tokens": [51584, 2188, 588, 2689, 764, 1389, 337, 341, 1729, 2316, 13, 51812], "temperature": 0.0, "avg_logprob": -0.10438300882066999, "compression_ratio": 1.6604477611940298, "no_speech_prob": 0.0006878054700791836}, {"id": 35, "seek": 14144, "start": 142.24, "end": 147.84, "text": " So probably GPT-4 really first needs to know what model do we want to use,", "tokens": [50404, 407, 1391, 26039, 51, 12, 19, 534, 700, 2203, 281, 458, 437, 2316, 360, 321, 528, 281, 764, 11, 50684], "temperature": 0.0, "avg_logprob": -0.07992032197144655, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.0018673345912247896}, {"id": 36, "seek": 14144, "start": 147.84, "end": 152.64, "text": " and then maybe do some light reading on the model's page to understand a bit about,", "tokens": [50684, 293, 550, 1310, 360, 512, 1442, 3760, 322, 264, 2316, 311, 3028, 281, 1223, 257, 857, 466, 11, 50924], "temperature": 0.0, "avg_logprob": -0.07992032197144655, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.0018673345912247896}, {"id": 37, "seek": 14144, "start": 152.64, "end": 155.28, "text": " you know, how to actually work with this model.", "tokens": [50924, 291, 458, 11, 577, 281, 767, 589, 365, 341, 2316, 13, 51056], "temperature": 0.0, "avg_logprob": -0.07992032197144655, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.0018673345912247896}, {"id": 38, "seek": 14144, "start": 155.92, "end": 160.56, "text": " Temp GPT doesn't have any web parsing ability built in,", "tokens": [51088, 8095, 79, 26039, 51, 1177, 380, 362, 604, 3670, 21156, 278, 3485, 3094, 294, 11, 51320], "temperature": 0.0, "avg_logprob": -0.07992032197144655, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.0018673345912247896}, {"id": 39, "seek": 14144, "start": 160.56, "end": 165.04, "text": " but it can program for us, and we can certainly parse with a program.", "tokens": [51320, 457, 309, 393, 1461, 337, 505, 11, 293, 321, 393, 3297, 48377, 365, 257, 1461, 13, 51544], "temperature": 0.0, "avg_logprob": -0.07992032197144655, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.0018673345912247896}, {"id": 40, "seek": 14144, "start": 165.04, "end": 169.6, "text": " So we can start off by asking Temp GPT to do just that for us.", "tokens": [51544, 407, 321, 393, 722, 766, 538, 3365, 8095, 79, 26039, 51, 281, 360, 445, 300, 337, 505, 13, 51772], "temperature": 0.0, "avg_logprob": -0.07992032197144655, "compression_ratio": 1.6056910569105691, "no_speech_prob": 0.0018673345912247896}, {"id": 41, "seek": 16960, "start": 169.6, "end": 172.79999999999998, "text": " And it does this first by installing beautiful soup for Python,", "tokens": [50364, 400, 309, 775, 341, 700, 538, 20762, 2238, 7884, 337, 15329, 11, 50524], "temperature": 0.0, "avg_logprob": -0.11256625509669638, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.004198436159640551}, {"id": 42, "seek": 16960, "start": 173.44, "end": 178.72, "text": " making the directory we asked for, and then writing some web scraping code to get the text", "tokens": [50556, 1455, 264, 21120, 321, 2351, 337, 11, 293, 550, 3579, 512, 3670, 43738, 3089, 281, 483, 264, 2487, 50820], "temperature": 0.0, "avg_logprob": -0.11256625509669638, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.004198436159640551}, {"id": 43, "seek": 16960, "start": 178.72, "end": 182.72, "text": " from that page, and then saving it to the text file that we requested.", "tokens": [50820, 490, 300, 3028, 11, 293, 550, 6816, 309, 281, 264, 2487, 3991, 300, 321, 16436, 13, 51020], "temperature": 0.0, "avg_logprob": -0.11256625509669638, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.004198436159640551}, {"id": 44, "seek": 16960, "start": 182.72, "end": 188.64, "text": " We can then use Temp GPT to read that content.text file and create a web app for us that demos", "tokens": [51020, 492, 393, 550, 764, 8095, 79, 26039, 51, 281, 1401, 300, 2701, 13, 25111, 3991, 293, 1884, 257, 3670, 724, 337, 505, 300, 33788, 51316], "temperature": 0.0, "avg_logprob": -0.11256625509669638, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.004198436159640551}, {"id": 45, "seek": 16960, "start": 188.64, "end": 190.0, "text": " its abilities.", "tokens": [51316, 1080, 11582, 13, 51384], "temperature": 0.0, "avg_logprob": -0.11256625509669638, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.004198436159640551}, {"id": 46, "seek": 16960, "start": 190.0, "end": 195.04, "text": " We did hit an error along the way, which I just copy and paste to Temp GPT both times,", "tokens": [51384, 492, 630, 2045, 364, 6713, 2051, 264, 636, 11, 597, 286, 445, 5055, 293, 9163, 281, 8095, 79, 26039, 51, 1293, 1413, 11, 51636], "temperature": 0.0, "avg_logprob": -0.11256625509669638, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.004198436159640551}, {"id": 47, "seek": 16960, "start": 195.04, "end": 196.4, "text": " and it is fixed.", "tokens": [51636, 293, 309, 307, 6806, 13, 51704], "temperature": 0.0, "avg_logprob": -0.11256625509669638, "compression_ratio": 1.6755725190839694, "no_speech_prob": 0.004198436159640551}, {"id": 48, "seek": 19640, "start": 196.88, "end": 201.04, "text": " This is one thing I'm considering fully automating, and I'm trying to do that", "tokens": [50388, 639, 307, 472, 551, 286, 478, 8079, 4498, 3553, 990, 11, 293, 286, 478, 1382, 281, 360, 300, 50596], "temperature": 0.0, "avg_logprob": -0.07212240861193968, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.007344893179833889}, {"id": 49, "seek": 19640, "start": 201.6, "end": 208.56, "text": " by allowing Temp GPT to read the terminal outputs to determine if more commands or changes", "tokens": [50624, 538, 8293, 8095, 79, 26039, 51, 281, 1401, 264, 14709, 23930, 281, 6997, 498, 544, 16901, 420, 2962, 50972], "temperature": 0.0, "avg_logprob": -0.07212240861193968, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.007344893179833889}, {"id": 50, "seek": 19640, "start": 208.56, "end": 211.28, "text": " need to be made, if an error is at least detected.", "tokens": [50972, 643, 281, 312, 1027, 11, 498, 364, 6713, 307, 412, 1935, 21896, 13, 51108], "temperature": 0.0, "avg_logprob": -0.07212240861193968, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.007344893179833889}, {"id": 51, "seek": 19640, "start": 211.84, "end": 217.12, "text": " The error and suggestion in the error text here could have been ingested by Temp GPT", "tokens": [51136, 440, 6713, 293, 16541, 294, 264, 6713, 2487, 510, 727, 362, 668, 3957, 21885, 538, 8095, 79, 26039, 51, 51400], "temperature": 0.0, "avg_logprob": -0.07212240861193968, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.007344893179833889}, {"id": 52, "seek": 19640, "start": 217.12, "end": 222.4, "text": " to easily implement and fix not really much different than us copying and pasting the error", "tokens": [51400, 281, 3612, 4445, 293, 3191, 406, 534, 709, 819, 813, 505, 27976, 293, 1791, 278, 264, 6713, 51664], "temperature": 0.0, "avg_logprob": -0.07212240861193968, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.007344893179833889}, {"id": 53, "seek": 19640, "start": 222.4, "end": 222.96, "text": " ourselves.", "tokens": [51664, 4175, 13, 51692], "temperature": 0.0, "avg_logprob": -0.07212240861193968, "compression_ratio": 1.6477732793522266, "no_speech_prob": 0.007344893179833889}, {"id": 54, "seek": 22296, "start": 223.6, "end": 229.44, "text": " Anyway, after reading the error, Temp GPT finds a way to remedy the problem and our program", "tokens": [50396, 5684, 11, 934, 3760, 264, 6713, 11, 8095, 79, 26039, 51, 10704, 257, 636, 281, 31648, 264, 1154, 293, 527, 1461, 50688], "temperature": 0.0, "avg_logprob": -0.09178069086358098, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.004609272349625826}, {"id": 55, "seek": 22296, "start": 229.44, "end": 230.0, "text": " works.", "tokens": [50688, 1985, 13, 50716], "temperature": 0.0, "avg_logprob": -0.09178069086358098, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.004609272349625826}, {"id": 56, "seek": 22296, "start": 230.0, "end": 235.12, "text": " This has been extremely minimal with effort compared to what I would have done,", "tokens": [50716, 639, 575, 668, 4664, 13206, 365, 4630, 5347, 281, 437, 286, 576, 362, 1096, 11, 50972], "temperature": 0.0, "avg_logprob": -0.09178069086358098, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.004609272349625826}, {"id": 57, "seek": 22296, "start": 235.12, "end": 240.32, "text": " even if I was either traditionally doing this by hand or even via the chat UI for chat GPT.", "tokens": [50972, 754, 498, 286, 390, 2139, 19067, 884, 341, 538, 1011, 420, 754, 5766, 264, 5081, 15682, 337, 5081, 26039, 51, 13, 51232], "temperature": 0.0, "avg_logprob": -0.09178069086358098, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.004609272349625826}, {"id": 58, "seek": 22296, "start": 240.88, "end": 245.44, "text": " And we can definitely do this in an even more like less clunky way.", "tokens": [51260, 400, 321, 393, 2138, 360, 341, 294, 364, 754, 544, 411, 1570, 596, 25837, 636, 13, 51488], "temperature": 0.0, "avg_logprob": -0.09178069086358098, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.004609272349625826}, {"id": 59, "seek": 22296, "start": 245.44, "end": 247.76000000000002, "text": " Speaking of which, how about a web parser idea?", "tokens": [51488, 13069, 295, 597, 11, 577, 466, 257, 3670, 21156, 260, 1558, 30, 51604], "temperature": 0.0, "avg_logprob": -0.09178069086358098, "compression_ratio": 1.5317460317460319, "no_speech_prob": 0.004609272349625826}, {"id": 60, "seek": 24776, "start": 247.76, "end": 252.79999999999998, "text": " Can we use Temp GPT to modify a pre-existing project?", "tokens": [50364, 1664, 321, 764, 8095, 79, 26039, 51, 281, 16927, 257, 659, 12, 36447, 1716, 30, 50616], "temperature": 0.0, "avg_logprob": -0.07651791452359752, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.044672176241874695}, {"id": 61, "seek": 24776, "start": 252.79999999999998, "end": 253.76, "text": " Absolutely.", "tokens": [50616, 7021, 13, 50664], "temperature": 0.0, "avg_logprob": -0.07651791452359752, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.044672176241874695}, {"id": 62, "seek": 24776, "start": 253.76, "end": 259.52, "text": " Might we actually use Temp GPT to modify itself to add something like a web parser?", "tokens": [50664, 23964, 321, 767, 764, 8095, 79, 26039, 51, 281, 16927, 2564, 281, 909, 746, 411, 257, 3670, 21156, 260, 30, 50952], "temperature": 0.0, "avg_logprob": -0.07651791452359752, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.044672176241874695}, {"id": 63, "seek": 24776, "start": 260.15999999999997, "end": 266.08, "text": " Here, I ask Temp GPT to implement a web parsing feature similar to my file reading feature,", "tokens": [50984, 1692, 11, 286, 1029, 8095, 79, 26039, 51, 281, 4445, 257, 3670, 21156, 278, 4111, 2531, 281, 452, 3991, 3760, 4111, 11, 51280], "temperature": 0.0, "avg_logprob": -0.07651791452359752, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.044672176241874695}, {"id": 64, "seek": 24776, "start": 266.08, "end": 271.44, "text": " and it modifies multiple areas of the Temp GPT script to do this.", "tokens": [51280, 293, 309, 1072, 11221, 3866, 3179, 295, 264, 8095, 79, 26039, 51, 5755, 281, 360, 341, 13, 51548], "temperature": 0.0, "avg_logprob": -0.07651791452359752, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.044672176241874695}, {"id": 65, "seek": 24776, "start": 272.0, "end": 276.0, "text": " Testing this feature, we can ask Temp GPT to parse some web page and then save a summary", "tokens": [51576, 45517, 341, 4111, 11, 321, 393, 1029, 8095, 79, 26039, 51, 281, 48377, 512, 3670, 3028, 293, 550, 3155, 257, 12691, 51776], "temperature": 0.0, "avg_logprob": -0.07651791452359752, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.044672176241874695}, {"id": 66, "seek": 24776, "start": 276.0, "end": 277.12, "text": " of that text to file.", "tokens": [51776, 295, 300, 2487, 281, 3991, 13, 51832], "temperature": 0.0, "avg_logprob": -0.07651791452359752, "compression_ratio": 1.771186440677966, "no_speech_prob": 0.044672176241874695}, {"id": 67, "seek": 27712, "start": 277.12, "end": 282.0, "text": " For example, let's parse the readme page from GitHub for TensorFlow and see if GPT-4", "tokens": [50364, 1171, 1365, 11, 718, 311, 48377, 264, 1401, 1398, 3028, 490, 23331, 337, 37624, 293, 536, 498, 26039, 51, 12, 19, 50608], "temperature": 0.0, "avg_logprob": -0.07102419126151811, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.003075086511671543}, {"id": 68, "seek": 27712, "start": 282.0, "end": 285.6, "text": " can summarize it for us into one sentence about what TensorFlow is.", "tokens": [50608, 393, 20858, 309, 337, 505, 666, 472, 8174, 466, 437, 37624, 307, 13, 50788], "temperature": 0.0, "avg_logprob": -0.07102419126151811, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.003075086511671543}, {"id": 69, "seek": 27712, "start": 286.32, "end": 292.0, "text": " In this case, though, it is plausible that GPT-4 could also just have used its own underlying", "tokens": [50824, 682, 341, 1389, 11, 1673, 11, 309, 307, 39925, 300, 26039, 51, 12, 19, 727, 611, 445, 362, 1143, 1080, 1065, 14217, 51108], "temperature": 0.0, "avg_logprob": -0.07102419126151811, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.003075086511671543}, {"id": 70, "seek": 27712, "start": 292.0, "end": 293.44, "text": " knowledge of TensorFlow.", "tokens": [51108, 3601, 295, 37624, 13, 51180], "temperature": 0.0, "avg_logprob": -0.07102419126151811, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.003075086511671543}, {"id": 71, "seek": 27712, "start": 293.44, "end": 299.28000000000003, "text": " For example, we could just outright ask GPT-4 about TensorFlow without any other context,", "tokens": [51180, 1171, 1365, 11, 321, 727, 445, 35189, 1029, 26039, 51, 12, 19, 466, 37624, 1553, 604, 661, 4319, 11, 51472], "temperature": 0.0, "avg_logprob": -0.07102419126151811, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.003075086511671543}, {"id": 72, "seek": 27712, "start": 299.28000000000003, "end": 302.24, "text": " and it could summarize into one sentence.", "tokens": [51472, 293, 309, 727, 20858, 666, 472, 8174, 13, 51620], "temperature": 0.0, "avg_logprob": -0.07102419126151811, "compression_ratio": 1.7004219409282701, "no_speech_prob": 0.003075086511671543}, {"id": 73, "seek": 30224, "start": 302.24, "end": 308.0, "text": " Instead, let's ask about something newer, so we could ask about stable studio from Stability AI.", "tokens": [50364, 7156, 11, 718, 311, 1029, 466, 746, 17628, 11, 370, 321, 727, 1029, 466, 8351, 6811, 490, 745, 2310, 7318, 13, 50652], "temperature": 0.0, "avg_logprob": -0.10610707600911458, "compression_ratio": 1.6172413793103448, "no_speech_prob": 0.025174492970108986}, {"id": 74, "seek": 30224, "start": 308.0, "end": 313.6, "text": " Asking GPT-4 via the chat UI, and it looks like this time GPT-4 doesn't actually know,", "tokens": [50652, 1018, 5092, 26039, 51, 12, 19, 5766, 264, 5081, 15682, 11, 293, 309, 1542, 411, 341, 565, 26039, 51, 12, 19, 1177, 380, 767, 458, 11, 50932], "temperature": 0.0, "avg_logprob": -0.10610707600911458, "compression_ratio": 1.6172413793103448, "no_speech_prob": 0.025174492970108986}, {"id": 75, "seek": 30224, "start": 313.6, "end": 314.32, "text": " which makes sense.", "tokens": [50932, 597, 1669, 2020, 13, 50968], "temperature": 0.0, "avg_logprob": -0.10610707600911458, "compression_ratio": 1.6172413793103448, "no_speech_prob": 0.025174492970108986}, {"id": 76, "seek": 30224, "start": 314.32, "end": 320.32, "text": " This is relatively new, and GPT-4's knowledge stops in late March or something like that.", "tokens": [50968, 639, 307, 7226, 777, 11, 293, 26039, 51, 12, 19, 311, 3601, 10094, 294, 3469, 6129, 420, 746, 411, 300, 13, 51268], "temperature": 0.0, "avg_logprob": -0.10610707600911458, "compression_ratio": 1.6172413793103448, "no_speech_prob": 0.025174492970108986}, {"id": 77, "seek": 30224, "start": 320.32, "end": 323.84000000000003, "text": " So then we can go back to our example, parse that readme, and get a good summary of it.", "tokens": [51268, 407, 550, 321, 393, 352, 646, 281, 527, 1365, 11, 48377, 300, 1401, 1398, 11, 293, 483, 257, 665, 12691, 295, 309, 13, 51444], "temperature": 0.0, "avg_logprob": -0.10610707600911458, "compression_ratio": 1.6172413793103448, "no_speech_prob": 0.025174492970108986}, {"id": 78, "seek": 30224, "start": 323.84000000000003, "end": 328.72, "text": " Okay, so those are just some really quick examples of what's possible, and to be honest,", "tokens": [51444, 1033, 11, 370, 729, 366, 445, 512, 534, 1702, 5110, 295, 437, 311, 1944, 11, 293, 281, 312, 3245, 11, 51688], "temperature": 0.0, "avg_logprob": -0.10610707600911458, "compression_ratio": 1.6172413793103448, "no_speech_prob": 0.025174492970108986}, {"id": 79, "seek": 32872, "start": 328.72, "end": 334.0, "text": " this has all just been kind of my first attempts as I'm trying to kind of make this happen, as", "tokens": [50364, 341, 575, 439, 445, 668, 733, 295, 452, 700, 15257, 382, 286, 478, 1382, 281, 733, 295, 652, 341, 1051, 11, 382, 50628], "temperature": 0.0, "avg_logprob": -0.11287918563716667, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.20929697155952454}, {"id": 80, "seek": 32872, "start": 334.0, "end": 338.72, "text": " this is a common, at least personally workflow for me, and I really just wanted to stop,", "tokens": [50628, 341, 307, 257, 2689, 11, 412, 1935, 5665, 20993, 337, 385, 11, 293, 286, 534, 445, 1415, 281, 1590, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11287918563716667, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.20929697155952454}, {"id": 81, "seek": 32872, "start": 338.72, "end": 344.08000000000004, "text": " you know, highlighting or clicking copy code, paste, run, copy some error, go back, like all", "tokens": [50864, 291, 458, 11, 26551, 420, 9697, 5055, 3089, 11, 9163, 11, 1190, 11, 5055, 512, 6713, 11, 352, 646, 11, 411, 439, 51132], "temperature": 0.0, "avg_logprob": -0.11287918563716667, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.20929697155952454}, {"id": 82, "seek": 32872, "start": 344.08000000000004, "end": 348.96000000000004, "text": " this like back and forth was getting very old, and I really just wanted to come up with a solution", "tokens": [51132, 341, 411, 646, 293, 5220, 390, 1242, 588, 1331, 11, 293, 286, 534, 445, 1415, 281, 808, 493, 365, 257, 3827, 51376], "temperature": 0.0, "avg_logprob": -0.11287918563716667, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.20929697155952454}, {"id": 83, "seek": 32872, "start": 348.96000000000004, "end": 353.04, "text": " that would just automatically kind of do these things for me, execute the commands for me,", "tokens": [51376, 300, 576, 445, 6772, 733, 295, 360, 613, 721, 337, 385, 11, 14483, 264, 16901, 337, 385, 11, 51580], "temperature": 0.0, "avg_logprob": -0.11287918563716667, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.20929697155952454}, {"id": 84, "seek": 32872, "start": 353.04, "end": 353.6, "text": " all of that.", "tokens": [51580, 439, 295, 300, 13, 51608], "temperature": 0.0, "avg_logprob": -0.11287918563716667, "compression_ratio": 1.8143939393939394, "no_speech_prob": 0.20929697155952454}, {"id": 85, "seek": 35360, "start": 354.32000000000005, "end": 359.68, "text": " So this is really my first run through, and there are for sure going to be better ways to do", "tokens": [50400, 407, 341, 307, 534, 452, 700, 1190, 807, 11, 293, 456, 366, 337, 988, 516, 281, 312, 1101, 2098, 281, 360, 50668], "temperature": 0.0, "avg_logprob": -0.08867970194135394, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.11917157471179962}, {"id": 86, "seek": 35360, "start": 360.40000000000003, "end": 365.12, "text": " the things that I'm doing here, and I definitely there's lots of things that we could build on,", "tokens": [50704, 264, 721, 300, 286, 478, 884, 510, 11, 293, 286, 2138, 456, 311, 3195, 295, 721, 300, 321, 727, 1322, 322, 11, 50940], "temperature": 0.0, "avg_logprob": -0.08867970194135394, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.11917157471179962}, {"id": 87, "seek": 35360, "start": 365.12, "end": 366.72, "text": " and I'm going to be hosting this code.", "tokens": [50940, 293, 286, 478, 516, 281, 312, 16058, 341, 3089, 13, 51020], "temperature": 0.0, "avg_logprob": -0.08867970194135394, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.11917157471179962}, {"id": 88, "seek": 35360, "start": 366.72, "end": 370.96000000000004, "text": " So if you have ideas or things you want to try, feel free to fork or submit a poll request or", "tokens": [51020, 407, 498, 291, 362, 3487, 420, 721, 291, 528, 281, 853, 11, 841, 1737, 281, 17716, 420, 10315, 257, 6418, 5308, 420, 51232], "temperature": 0.0, "avg_logprob": -0.08867970194135394, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.11917157471179962}, {"id": 89, "seek": 35360, "start": 370.96000000000004, "end": 371.68, "text": " anything like that.", "tokens": [51232, 1340, 411, 300, 13, 51268], "temperature": 0.0, "avg_logprob": -0.08867970194135394, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.11917157471179962}, {"id": 90, "seek": 35360, "start": 371.68, "end": 375.28000000000003, "text": " So with that, let's go ahead and dive into the code and see how everything is working under", "tokens": [51268, 407, 365, 300, 11, 718, 311, 352, 2286, 293, 9192, 666, 264, 3089, 293, 536, 577, 1203, 307, 1364, 833, 51448], "temperature": 0.0, "avg_logprob": -0.08867970194135394, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.11917157471179962}, {"id": 91, "seek": 35360, "start": 375.28000000000003, "end": 375.76000000000005, "text": " the hood.", "tokens": [51448, 264, 13376, 13, 51472], "temperature": 0.0, "avg_logprob": -0.08867970194135394, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.11917157471179962}, {"id": 92, "seek": 35360, "start": 375.76000000000005, "end": 379.36, "text": " All right, so here with the code, let's go ahead and run through how everything works.", "tokens": [51472, 1057, 558, 11, 370, 510, 365, 264, 3089, 11, 718, 311, 352, 2286, 293, 1190, 807, 577, 1203, 1985, 13, 51652], "temperature": 0.0, "avg_logprob": -0.08867970194135394, "compression_ratio": 1.8928571428571428, "no_speech_prob": 0.11917157471179962}, {"id": 93, "seek": 37936, "start": 379.36, "end": 381.68, "text": " So first, we've just got a bunch of imports.", "tokens": [50364, 407, 700, 11, 321, 600, 445, 658, 257, 3840, 295, 41596, 13, 50480], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 94, "seek": 37936, "start": 381.68, "end": 386.48, "text": " Open AI for the API, of course, Colorama to make things pretty, time for a sleep", "tokens": [50480, 7238, 7318, 337, 264, 9362, 11, 295, 1164, 11, 10458, 2404, 281, 652, 721, 1238, 11, 565, 337, 257, 2817, 50720], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 95, "seek": 37936, "start": 387.12, "end": 387.76, "text": " context.", "tokens": [50752, 4319, 13, 50784], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 96, "seek": 37936, "start": 387.76, "end": 390.16, "text": " This is how I'm setting the pre prompt.", "tokens": [50784, 639, 307, 577, 286, 478, 3287, 264, 659, 12391, 13, 50904], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 97, "seek": 37936, "start": 390.16, "end": 394.24, "text": " We will come to that in a moment, but for now, that's just that's it's the pre prompt", "tokens": [50904, 492, 486, 808, 281, 300, 294, 257, 1623, 11, 457, 337, 586, 11, 300, 311, 445, 300, 311, 309, 311, 264, 659, 12391, 51108], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 98, "seek": 37936, "start": 395.2, "end": 396.16, "text": " sub process.", "tokens": [51156, 1422, 1399, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 99, "seek": 37936, "start": 396.16, "end": 400.24, "text": " This is for attempting to execute commands and get their output.", "tokens": [51204, 639, 307, 337, 22001, 281, 14483, 16901, 293, 483, 641, 5598, 13, 51408], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 100, "seek": 37936, "start": 400.24, "end": 401.52000000000004, "text": " I'm still struggling with this.", "tokens": [51408, 286, 478, 920, 9314, 365, 341, 13, 51472], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 101, "seek": 37936, "start": 401.52000000000004, "end": 404.72, "text": " Maybe someone can help me figure out what the heck why this doesn't work.", "tokens": [51472, 2704, 1580, 393, 854, 385, 2573, 484, 437, 264, 12872, 983, 341, 1177, 380, 589, 13, 51632], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 102, "seek": 37936, "start": 404.72, "end": 407.36, "text": " I'm sure it's probably some super simple logic problem.", "tokens": [51632, 286, 478, 988, 309, 311, 1391, 512, 1687, 2199, 9952, 1154, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1184840854123342, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.06006261333823204}, {"id": 103, "seek": 40736, "start": 407.36, "end": 413.92, "text": " But anyway, read for regular expressions, requests and beautiful soup for the web parsing", "tokens": [50364, 583, 4033, 11, 1401, 337, 3890, 15277, 11, 12475, 293, 2238, 7884, 337, 264, 3670, 21156, 278, 50692], "temperature": 0.0, "avg_logprob": -0.08919904764416148, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.0005032796179875731}, {"id": 104, "seek": 40736, "start": 413.92, "end": 416.96000000000004, "text": " capability that term GPT actually added for itself.", "tokens": [50692, 13759, 300, 1433, 26039, 51, 767, 3869, 337, 2564, 13, 50844], "temperature": 0.0, "avg_logprob": -0.08919904764416148, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.0005032796179875731}, {"id": 105, "seek": 40736, "start": 418.32, "end": 425.52000000000004, "text": " Again, and this actually isn't really all that required since we term GPT can already", "tokens": [50912, 3764, 11, 293, 341, 767, 1943, 380, 534, 439, 300, 4739, 1670, 321, 1433, 26039, 51, 393, 1217, 51272], "temperature": 0.0, "avg_logprob": -0.08919904764416148, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.0005032796179875731}, {"id": 106, "seek": 40736, "start": 425.52000000000004, "end": 426.8, "text": " write a web parser.", "tokens": [51272, 2464, 257, 3670, 21156, 260, 13, 51336], "temperature": 0.0, "avg_logprob": -0.08919904764416148, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.0005032796179875731}, {"id": 107, "seek": 40736, "start": 426.8, "end": 431.12, "text": " But the real kicker here is, you know, it could also write code to open up a file.", "tokens": [51336, 583, 264, 957, 4437, 260, 510, 307, 11, 291, 458, 11, 309, 727, 611, 2464, 3089, 281, 1269, 493, 257, 3991, 13, 51552], "temperature": 0.0, "avg_logprob": -0.08919904764416148, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.0005032796179875731}, {"id": 108, "seek": 40736, "start": 431.12, "end": 435.6, "text": " But what you want to be able to do is read that information into context.", "tokens": [51552, 583, 437, 291, 528, 281, 312, 1075, 281, 360, 307, 1401, 300, 1589, 666, 4319, 13, 51776], "temperature": 0.0, "avg_logprob": -0.08919904764416148, "compression_ratio": 1.6290322580645162, "no_speech_prob": 0.0005032796179875731}, {"id": 109, "seek": 43560, "start": 435.6, "end": 440.48, "text": " So doing that, I think still kind of requires these features, but I want to kind of clean", "tokens": [50364, 407, 884, 300, 11, 286, 519, 920, 733, 295, 7029, 613, 4122, 11, 457, 286, 528, 281, 733, 295, 2541, 50608], "temperature": 0.0, "avg_logprob": -0.08468457424279416, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0007553300238214433}, {"id": 110, "seek": 43560, "start": 440.48, "end": 444.48, "text": " this up and quite possibly make it a little more dynamic.", "tokens": [50608, 341, 493, 293, 1596, 6264, 652, 309, 257, 707, 544, 8546, 13, 50808], "temperature": 0.0, "avg_logprob": -0.08468457424279416, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0007553300238214433}, {"id": 111, "seek": 43560, "start": 444.48, "end": 450.40000000000003, "text": " So for example, like pulling from a website, do you really just want the paragraph text?", "tokens": [50808, 407, 337, 1365, 11, 411, 8407, 490, 257, 3144, 11, 360, 291, 534, 445, 528, 264, 18865, 2487, 30, 51104], "temperature": 0.0, "avg_logprob": -0.08468457424279416, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0007553300238214433}, {"id": 112, "seek": 43560, "start": 450.40000000000003, "end": 454.88, "text": " Do we want to also handle for JavaScript that happens to populate things?", "tokens": [51104, 1144, 321, 528, 281, 611, 4813, 337, 15778, 300, 2314, 281, 1665, 5256, 721, 30, 51328], "temperature": 0.0, "avg_logprob": -0.08468457424279416, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0007553300238214433}, {"id": 113, "seek": 43560, "start": 454.88, "end": 458.64000000000004, "text": " You know, so like this is a really basic example, but this won't get you very far.", "tokens": [51328, 509, 458, 11, 370, 411, 341, 307, 257, 534, 3875, 1365, 11, 457, 341, 1582, 380, 483, 291, 588, 1400, 13, 51516], "temperature": 0.0, "avg_logprob": -0.08468457424279416, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0007553300238214433}, {"id": 114, "seek": 43560, "start": 458.64000000000004, "end": 461.84000000000003, "text": " So anyway, I'm still kind of thinking about how I really want to do that.", "tokens": [51516, 407, 4033, 11, 286, 478, 920, 733, 295, 1953, 466, 577, 286, 534, 528, 281, 360, 300, 13, 51676], "temperature": 0.0, "avg_logprob": -0.08468457424279416, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0007553300238214433}, {"id": 115, "seek": 43560, "start": 461.84000000000003, "end": 464.48, "text": " But anyway, moving along, the open AI key,", "tokens": [51676, 583, 4033, 11, 2684, 2051, 11, 264, 1269, 7318, 2141, 11, 51808], "temperature": 0.0, "avg_logprob": -0.08468457424279416, "compression_ratio": 1.6776315789473684, "no_speech_prob": 0.0007553300238214433}, {"id": 116, "seek": 46448, "start": 465.28000000000003, "end": 467.6, "text": " really all the open AI stuff, I've got a video on that.", "tokens": [50404, 534, 439, 264, 1269, 7318, 1507, 11, 286, 600, 658, 257, 960, 322, 300, 13, 50520], "temperature": 0.0, "avg_logprob": -0.07317860485756233, "compression_ratio": 1.6948640483383686, "no_speech_prob": 0.005058865994215012}, {"id": 117, "seek": 46448, "start": 467.6, "end": 470.72, "text": " I'll try to remember to put a link in the description for the basics of working with", "tokens": [50520, 286, 603, 853, 281, 1604, 281, 829, 257, 2113, 294, 264, 3855, 337, 264, 14688, 295, 1364, 365, 50676], "temperature": 0.0, "avg_logprob": -0.07317860485756233, "compression_ratio": 1.6948640483383686, "no_speech_prob": 0.005058865994215012}, {"id": 118, "seek": 46448, "start": 470.72, "end": 471.44, "text": " that API.", "tokens": [50676, 300, 9362, 13, 50712], "temperature": 0.0, "avg_logprob": -0.07317860485756233, "compression_ratio": 1.6948640483383686, "no_speech_prob": 0.005058865994215012}, {"id": 119, "seek": 46448, "start": 472.40000000000003, "end": 474.8, "text": " But anyway, that's where our key is debug.", "tokens": [50760, 583, 4033, 11, 300, 311, 689, 527, 2141, 307, 24083, 13, 50880], "temperature": 0.0, "avg_logprob": -0.07317860485756233, "compression_ratio": 1.6948640483383686, "no_speech_prob": 0.005058865994215012}, {"id": 120, "seek": 46448, "start": 474.8, "end": 482.16, "text": " This is just for a few extra print statements, just to mostly for outputting the full context", "tokens": [50880, 639, 307, 445, 337, 257, 1326, 2857, 4482, 12363, 11, 445, 281, 5240, 337, 5598, 783, 264, 1577, 4319, 51248], "temperature": 0.0, "avg_logprob": -0.07317860485756233, "compression_ratio": 1.6948640483383686, "no_speech_prob": 0.005058865994215012}, {"id": 121, "seek": 46448, "start": 482.16, "end": 486.96000000000004, "text": " every time I query GPT for just because that'll tell you for sure what exactly you're sending", "tokens": [51248, 633, 565, 286, 14581, 26039, 51, 337, 445, 570, 300, 603, 980, 291, 337, 988, 437, 2293, 291, 434, 7750, 51488], "temperature": 0.0, "avg_logprob": -0.07317860485756233, "compression_ratio": 1.6948640483383686, "no_speech_prob": 0.005058865994215012}, {"id": 122, "seek": 46448, "start": 486.96000000000004, "end": 488.0, "text": " and getting back.", "tokens": [51488, 293, 1242, 646, 13, 51540], "temperature": 0.0, "avg_logprob": -0.07317860485756233, "compression_ratio": 1.6948640483383686, "no_speech_prob": 0.005058865994215012}, {"id": 123, "seek": 46448, "start": 488.0, "end": 491.68, "text": " I find that to be the biggest amount of information that you don't always want to see.", "tokens": [51540, 286, 915, 300, 281, 312, 264, 3880, 2372, 295, 1589, 300, 291, 500, 380, 1009, 528, 281, 536, 13, 51724], "temperature": 0.0, "avg_logprob": -0.07317860485756233, "compression_ratio": 1.6948640483383686, "no_speech_prob": 0.005058865994215012}, {"id": 124, "seek": 46448, "start": 491.68, "end": 494.08000000000004, "text": " But if you're having a problem, you probably want to see that whole thing.", "tokens": [51724, 583, 498, 291, 434, 1419, 257, 1154, 11, 291, 1391, 528, 281, 536, 300, 1379, 551, 13, 51844], "temperature": 0.0, "avg_logprob": -0.07317860485756233, "compression_ratio": 1.6948640483383686, "no_speech_prob": 0.005058865994215012}, {"id": 125, "seek": 49408, "start": 494.08, "end": 497.52, "text": " So anyway, moving along, message history is that terminal commands.", "tokens": [50364, 407, 4033, 11, 2684, 2051, 11, 3636, 2503, 307, 300, 14709, 16901, 13, 50536], "temperature": 0.0, "avg_logprob": -0.10470419732209678, "compression_ratio": 1.6732283464566928, "no_speech_prob": 0.0005883458652533591}, {"id": 126, "seek": 49408, "start": 497.52, "end": 500.56, "text": " That's just like the from the context that initial stuff that comes in.", "tokens": [50536, 663, 311, 445, 411, 264, 490, 264, 4319, 300, 5883, 1507, 300, 1487, 294, 13, 50688], "temperature": 0.0, "avg_logprob": -0.10470419732209678, "compression_ratio": 1.6732283464566928, "no_speech_prob": 0.0005883458652533591}, {"id": 127, "seek": 49408, "start": 502.4, "end": 507.76, "text": " Then we have the regular expression pattern for the sort of functionality of reading a file", "tokens": [50780, 1396, 321, 362, 264, 3890, 6114, 5102, 337, 264, 1333, 295, 14980, 295, 3760, 257, 3991, 51048], "temperature": 0.0, "avg_logprob": -0.10470419732209678, "compression_ratio": 1.6732283464566928, "no_speech_prob": 0.0005883458652533591}, {"id": 128, "seek": 49408, "start": 507.76, "end": 509.2, "text": " or reading a website.", "tokens": [51048, 420, 3760, 257, 3144, 13, 51120], "temperature": 0.0, "avg_logprob": -0.10470419732209678, "compression_ratio": 1.6732283464566928, "no_speech_prob": 0.0005883458652533591}, {"id": 129, "seek": 49408, "start": 509.2, "end": 510.47999999999996, "text": " So this is just read a file.", "tokens": [51120, 407, 341, 307, 445, 1401, 257, 3991, 13, 51184], "temperature": 0.0, "avg_logprob": -0.10470419732209678, "compression_ratio": 1.6732283464566928, "no_speech_prob": 0.0005883458652533591}, {"id": 130, "seek": 49408, "start": 510.47999999999996, "end": 511.91999999999996, "text": " This is W for website.", "tokens": [51184, 639, 307, 343, 337, 3144, 13, 51256], "temperature": 0.0, "avg_logprob": -0.10470419732209678, "compression_ratio": 1.6732283464566928, "no_speech_prob": 0.0005883458652533591}, {"id": 131, "seek": 49408, "start": 513.1999999999999, "end": 514.3199999999999, "text": " It's just going to follow that.", "tokens": [51320, 467, 311, 445, 516, 281, 1524, 300, 13, 51376], "temperature": 0.0, "avg_logprob": -0.10470419732209678, "compression_ratio": 1.6732283464566928, "no_speech_prob": 0.0005883458652533591}, {"id": 132, "seek": 49408, "start": 514.3199999999999, "end": 520.48, "text": " Again, we really could use large language models to also parse this out and it wouldn't", "tokens": [51376, 3764, 11, 321, 534, 727, 764, 2416, 2856, 5245, 281, 611, 48377, 341, 484, 293, 309, 2759, 380, 51684], "temperature": 0.0, "avg_logprob": -0.10470419732209678, "compression_ratio": 1.6732283464566928, "no_speech_prob": 0.0005883458652533591}, {"id": 133, "seek": 52048, "start": 520.48, "end": 522.8000000000001, "text": " even need to be like super structured in any way.", "tokens": [50364, 754, 643, 281, 312, 411, 1687, 18519, 294, 604, 636, 13, 50480], "temperature": 0.0, "avg_logprob": -0.1011575305734882, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.009706025011837482}, {"id": 134, "seek": 52048, "start": 522.8000000000001, "end": 527.76, "text": " It could just read some text and then determine is there a file that should be read?", "tokens": [50480, 467, 727, 445, 1401, 512, 2487, 293, 550, 6997, 307, 456, 257, 3991, 300, 820, 312, 1401, 30, 50728], "temperature": 0.0, "avg_logprob": -0.1011575305734882, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.009706025011837482}, {"id": 135, "seek": 52048, "start": 527.76, "end": 529.9200000000001, "text": " If yes, read that file, add that in.", "tokens": [50728, 759, 2086, 11, 1401, 300, 3991, 11, 909, 300, 294, 13, 50836], "temperature": 0.0, "avg_logprob": -0.1011575305734882, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.009706025011837482}, {"id": 136, "seek": 52048, "start": 529.9200000000001, "end": 532.4, "text": " So you don't necessarily have to do it this way.", "tokens": [50836, 407, 291, 500, 380, 4725, 362, 281, 360, 309, 341, 636, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1011575305734882, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.009706025011837482}, {"id": 137, "seek": 52048, "start": 532.4, "end": 537.44, "text": " Again, more on that and the context and all that stuff in a little bit because eventually", "tokens": [50960, 3764, 11, 544, 322, 300, 293, 264, 4319, 293, 439, 300, 1507, 294, 257, 707, 857, 570, 4728, 51212], "temperature": 0.0, "avg_logprob": -0.1011575305734882, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.009706025011837482}, {"id": 138, "seek": 52048, "start": 537.44, "end": 539.6, "text": " I would like to do this without using open AI.", "tokens": [51212, 286, 576, 411, 281, 360, 341, 1553, 1228, 1269, 7318, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1011575305734882, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.009706025011837482}, {"id": 139, "seek": 52048, "start": 539.6, "end": 540.96, "text": " I'd like to use an open source model.", "tokens": [51320, 286, 1116, 411, 281, 764, 364, 1269, 4009, 2316, 13, 51388], "temperature": 0.0, "avg_logprob": -0.1011575305734882, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.009706025011837482}, {"id": 140, "seek": 52048, "start": 540.96, "end": 547.2, "text": " So I just don't want to get too deep into prompt engineering with GPT for when I'd like to", "tokens": [51388, 407, 286, 445, 500, 380, 528, 281, 483, 886, 2452, 666, 12391, 7043, 365, 26039, 51, 337, 562, 286, 1116, 411, 281, 51700], "temperature": 0.0, "avg_logprob": -0.1011575305734882, "compression_ratio": 1.6993006993006994, "no_speech_prob": 0.009706025011837482}, {"id": 141, "seek": 54720, "start": 547.2, "end": 553.5200000000001, "text": " I'm kind of like holding out for a really solid open source model with a solid license.", "tokens": [50364, 286, 478, 733, 295, 411, 5061, 484, 337, 257, 534, 5100, 1269, 4009, 2316, 365, 257, 5100, 10476, 13, 50680], "temperature": 0.0, "avg_logprob": -0.138117346270331, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0022515698801726103}, {"id": 142, "seek": 54720, "start": 553.5200000000001, "end": 554.24, "text": " So no llama.", "tokens": [50680, 407, 572, 23272, 13, 50716], "temperature": 0.0, "avg_logprob": -0.138117346270331, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0022515698801726103}, {"id": 143, "seek": 54720, "start": 555.36, "end": 556.96, "text": " Anyway, moving on GPT query.", "tokens": [50772, 5684, 11, 2684, 322, 26039, 51, 14581, 13, 50852], "temperature": 0.0, "avg_logprob": -0.138117346270331, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0022515698801726103}, {"id": 144, "seek": 54720, "start": 556.96, "end": 562.88, "text": " So this is pretty much nothing new since the GPT for tutorial.", "tokens": [50852, 407, 341, 307, 1238, 709, 1825, 777, 1670, 264, 26039, 51, 337, 7073, 13, 51148], "temperature": 0.0, "avg_logprob": -0.138117346270331, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0022515698801726103}, {"id": 145, "seek": 54720, "start": 562.88, "end": 566.5600000000001, "text": " So again, if any of this is confusing to you other than maybe the color Rama stuff,", "tokens": [51148, 407, 797, 11, 498, 604, 295, 341, 307, 13181, 281, 291, 661, 813, 1310, 264, 2017, 39828, 1507, 11, 51332], "temperature": 0.0, "avg_logprob": -0.138117346270331, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0022515698801726103}, {"id": 146, "seek": 54720, "start": 566.5600000000001, "end": 572.5600000000001, "text": " this just sets a color sets a styling basically does the text whatever and then resets it back", "tokens": [51332, 341, 445, 6352, 257, 2017, 6352, 257, 27944, 1936, 775, 264, 2487, 2035, 293, 550, 725, 1385, 309, 646, 51632], "temperature": 0.0, "avg_logprob": -0.138117346270331, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0022515698801726103}, {"id": 147, "seek": 54720, "start": 572.5600000000001, "end": 572.96, "text": " to normal.", "tokens": [51632, 281, 2710, 13, 51652], "temperature": 0.0, "avg_logprob": -0.138117346270331, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0022515698801726103}, {"id": 148, "seek": 54720, "start": 572.96, "end": 575.6800000000001, "text": " So that's just that's just typical color Rama stuff.", "tokens": [51652, 407, 300, 311, 445, 300, 311, 445, 7476, 2017, 39828, 1507, 13, 51788], "temperature": 0.0, "avg_logprob": -0.138117346270331, "compression_ratio": 1.6730769230769231, "no_speech_prob": 0.0022515698801726103}, {"id": 149, "seek": 57568, "start": 576.4, "end": 581.3599999999999, "text": " But anyway, if the debug is true, we are sending the entire message history,", "tokens": [50400, 583, 4033, 11, 498, 264, 24083, 307, 2074, 11, 321, 366, 7750, 264, 2302, 3636, 2503, 11, 50648], "temperature": 0.0, "avg_logprob": -0.10272530147007533, "compression_ratio": 1.7534722222222223, "no_speech_prob": 0.0015007836045697331}, {"id": 150, "seek": 57568, "start": 581.3599999999999, "end": 584.8, "text": " which is the full context that we're sending off to GPT for.", "tokens": [50648, 597, 307, 264, 1577, 4319, 300, 321, 434, 7750, 766, 281, 26039, 51, 337, 13, 50820], "temperature": 0.0, "avg_logprob": -0.10272530147007533, "compression_ratio": 1.7534722222222223, "no_speech_prob": 0.0015007836045697331}, {"id": 151, "seek": 57568, "start": 584.8, "end": 588.16, "text": " So if anything's not working the way we're expecting, that's the first thing that we're", "tokens": [50820, 407, 498, 1340, 311, 406, 1364, 264, 636, 321, 434, 9650, 11, 300, 311, 264, 700, 551, 300, 321, 434, 50988], "temperature": 0.0, "avg_logprob": -0.10272530147007533, "compression_ratio": 1.7534722222222223, "no_speech_prob": 0.0015007836045697331}, {"id": 152, "seek": 57568, "start": 588.16, "end": 588.56, "text": " going to do.", "tokens": [50988, 516, 281, 360, 13, 51008], "temperature": 0.0, "avg_logprob": -0.10272530147007533, "compression_ratio": 1.7534722222222223, "no_speech_prob": 0.0015007836045697331}, {"id": 153, "seek": 57568, "start": 588.56, "end": 594.4, "text": " So for example, reading the console outputs like trying to get the return on in the console.", "tokens": [51008, 407, 337, 1365, 11, 3760, 264, 11076, 23930, 411, 1382, 281, 483, 264, 2736, 322, 294, 264, 11076, 13, 51300], "temperature": 0.0, "avg_logprob": -0.10272530147007533, "compression_ratio": 1.7534722222222223, "no_speech_prob": 0.0015007836045697331}, {"id": 154, "seek": 57568, "start": 595.8399999999999, "end": 599.92, "text": " It doesn't give the most recent command no matter how hard I try.", "tokens": [51372, 467, 1177, 380, 976, 264, 881, 5162, 5622, 572, 1871, 577, 1152, 286, 853, 13, 51576], "temperature": 0.0, "avg_logprob": -0.10272530147007533, "compression_ratio": 1.7534722222222223, "no_speech_prob": 0.0015007836045697331}, {"id": 155, "seek": 57568, "start": 599.92, "end": 601.52, "text": " So I don't know.", "tokens": [51576, 407, 286, 500, 380, 458, 13, 51656], "temperature": 0.0, "avg_logprob": -0.10272530147007533, "compression_ratio": 1.7534722222222223, "no_speech_prob": 0.0015007836045697331}, {"id": 156, "seek": 57568, "start": 601.52, "end": 605.3599999999999, "text": " At first I thought it was while I'm hitting errors and I was thinking the error is causing", "tokens": [51656, 1711, 700, 286, 1194, 309, 390, 1339, 286, 478, 8850, 13603, 293, 286, 390, 1953, 264, 6713, 307, 9853, 51848], "temperature": 0.0, "avg_logprob": -0.10272530147007533, "compression_ratio": 1.7534722222222223, "no_speech_prob": 0.0015007836045697331}, {"id": 157, "seek": 60536, "start": 605.44, "end": 606.32, "text": " my problem.", "tokens": [50368, 452, 1154, 13, 50412], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 158, "seek": 60536, "start": 606.32, "end": 609.76, "text": " But I'm pretty sure I'm handling it such that you would still you should still capture the", "tokens": [50412, 583, 286, 478, 1238, 988, 286, 478, 13175, 309, 1270, 300, 291, 576, 920, 291, 820, 920, 7983, 264, 50584], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 159, "seek": 60536, "start": 609.76, "end": 610.0, "text": " error.", "tokens": [50584, 6713, 13, 50596], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 160, "seek": 60536, "start": 610.0, "end": 611.36, "text": " But for some reason I'm just not.", "tokens": [50596, 583, 337, 512, 1778, 286, 478, 445, 406, 13, 50664], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 161, "seek": 60536, "start": 611.36, "end": 614.4, "text": " So anyway, moving along.", "tokens": [50664, 407, 4033, 11, 2684, 2051, 13, 50816], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 162, "seek": 60536, "start": 614.4, "end": 618.88, "text": " So this is just a query GPT for API extract paragraphs.", "tokens": [50816, 407, 341, 307, 445, 257, 14581, 26039, 51, 337, 9362, 8947, 48910, 13, 51040], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 163, "seek": 60536, "start": 618.88, "end": 624.24, "text": " This is the code that GPT for slash term GPT wrote and modified into our script for us.", "tokens": [51040, 639, 307, 264, 3089, 300, 26039, 51, 337, 17330, 1433, 26039, 51, 4114, 293, 15873, 666, 527, 5755, 337, 505, 13, 51308], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 164, "seek": 60536, "start": 625.12, "end": 627.76, "text": " Just simply reads a website grabs all the paragraph text.", "tokens": [51352, 1449, 2935, 15700, 257, 3144, 30028, 439, 264, 18865, 2487, 13, 51484], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 165, "seek": 60536, "start": 627.76, "end": 628.96, "text": " So really, really basic.", "tokens": [51484, 407, 534, 11, 534, 3875, 13, 51544], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 166, "seek": 60536, "start": 628.96, "end": 631.6, "text": " Again, I think this should be a little better.", "tokens": [51544, 3764, 11, 286, 519, 341, 820, 312, 257, 707, 1101, 13, 51676], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 167, "seek": 60536, "start": 633.04, "end": 634.24, "text": " But moving along.", "tokens": [51748, 583, 2684, 2051, 13, 51808], "temperature": 0.0, "avg_logprob": -0.11659761177476986, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0026308512315154076}, {"id": 168, "seek": 63424, "start": 634.24, "end": 636.16, "text": " So then we've just got some pretty colorama stuff.", "tokens": [50364, 407, 550, 321, 600, 445, 658, 512, 1238, 2017, 2404, 1507, 13, 50460], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 169, "seek": 63424, "start": 636.16, "end": 637.44, "text": " Welcome to term GPT.", "tokens": [50460, 4027, 281, 1433, 26039, 51, 13, 50524], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 170, "seek": 63424, "start": 638.24, "end": 643.2, "text": " It just tells you kind of the point of term GPT is just to make prototyping faster.", "tokens": [50564, 467, 445, 5112, 291, 733, 295, 264, 935, 295, 1433, 26039, 51, 307, 445, 281, 652, 46219, 3381, 4663, 13, 50812], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 171, "seek": 63424, "start": 643.2, "end": 644.72, "text": " Not a whole bunch of other fancy stuff.", "tokens": [50812, 1726, 257, 1379, 3840, 295, 661, 10247, 1507, 13, 50888], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 172, "seek": 63424, "start": 644.72, "end": 647.2, "text": " It's just make prototyping and R&D really quick.", "tokens": [50888, 467, 311, 445, 652, 46219, 3381, 293, 497, 5, 35, 534, 1702, 13, 51012], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 173, "seek": 63424, "start": 647.2, "end": 651.04, "text": " But don't run commands without still being able to review them.", "tokens": [51012, 583, 500, 380, 1190, 16901, 1553, 920, 885, 1075, 281, 3131, 552, 13, 51204], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 174, "seek": 63424, "start": 651.04, "end": 653.84, "text": " So continuing along.", "tokens": [51204, 407, 9289, 2051, 13, 51344], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 175, "seek": 63424, "start": 653.84, "end": 655.76, "text": " So there's just some more color color stuff.", "tokens": [51344, 407, 456, 311, 445, 512, 544, 2017, 2017, 1507, 13, 51440], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 176, "seek": 63424, "start": 655.76, "end": 657.12, "text": " You've seen what it outputs already.", "tokens": [51440, 509, 600, 1612, 437, 309, 23930, 1217, 13, 51508], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 177, "seek": 63424, "start": 658.08, "end": 662.16, "text": " So then we get the input whatever the user wants to, you know, put in the command.", "tokens": [51556, 407, 550, 321, 483, 264, 4846, 2035, 264, 4195, 2738, 281, 11, 291, 458, 11, 829, 294, 264, 5622, 13, 51760], "temperature": 0.0, "avg_logprob": -0.11867958959871835, "compression_ratio": 1.703448275862069, "no_speech_prob": 0.0007791286334395409}, {"id": 178, "seek": 66216, "start": 662.16, "end": 664.48, "text": " And then we have these very simple ones like clear.", "tokens": [50364, 400, 550, 321, 362, 613, 588, 2199, 2306, 411, 1850, 13, 50480], "temperature": 0.0, "avg_logprob": -0.09477607240068152, "compression_ratio": 1.8501628664495113, "no_speech_prob": 0.002550603821873665}, {"id": 179, "seek": 66216, "start": 664.48, "end": 669.52, "text": " This just clears out that whole contextual history and just resets us back to terminal commands.", "tokens": [50480, 639, 445, 47033, 484, 300, 1379, 35526, 2503, 293, 445, 725, 1385, 505, 646, 281, 14709, 16901, 13, 50732], "temperature": 0.0, "avg_logprob": -0.09477607240068152, "compression_ratio": 1.8501628664495113, "no_speech_prob": 0.002550603821873665}, {"id": 180, "seek": 66216, "start": 669.52, "end": 673.4399999999999, "text": " We'll talk about we'll show those momentarily output.", "tokens": [50732, 492, 603, 751, 466, 321, 603, 855, 729, 1623, 3289, 5598, 13, 50928], "temperature": 0.0, "avg_logprob": -0.09477607240068152, "compression_ratio": 1.8501628664495113, "no_speech_prob": 0.002550603821873665}, {"id": 181, "seek": 66216, "start": 673.4399999999999, "end": 677.68, "text": " This reads theoretically the output from the system basically in the terminal.", "tokens": [50928, 639, 15700, 29400, 264, 5598, 490, 264, 1185, 1936, 294, 264, 14709, 13, 51140], "temperature": 0.0, "avg_logprob": -0.09477607240068152, "compression_ratio": 1.8501628664495113, "no_speech_prob": 0.002550603821873665}, {"id": 182, "seek": 66216, "start": 677.68, "end": 680.0799999999999, "text": " What, you know, what was the response back to that terminal?", "tokens": [51140, 708, 11, 291, 458, 11, 437, 390, 264, 4134, 646, 281, 300, 14709, 30, 51260], "temperature": 0.0, "avg_logprob": -0.09477607240068152, "compression_ratio": 1.8501628664495113, "no_speech_prob": 0.002550603821873665}, {"id": 183, "seek": 66216, "start": 680.0799999999999, "end": 682.0, "text": " What was printed out in that terminal?", "tokens": [51260, 708, 390, 13567, 484, 294, 300, 14709, 30, 51356], "temperature": 0.0, "avg_logprob": -0.09477607240068152, "compression_ratio": 1.8501628664495113, "no_speech_prob": 0.002550603821873665}, {"id": 184, "seek": 66216, "start": 682.0, "end": 685.8399999999999, "text": " So if you did hit an error, it would be nice to be able to say output or, you know,", "tokens": [51356, 407, 498, 291, 630, 2045, 364, 6713, 11, 309, 576, 312, 1481, 281, 312, 1075, 281, 584, 5598, 420, 11, 291, 458, 11, 51548], "temperature": 0.0, "avg_logprob": -0.09477607240068152, "compression_ratio": 1.8501628664495113, "no_speech_prob": 0.002550603821873665}, {"id": 185, "seek": 66216, "start": 685.8399999999999, "end": 686.88, "text": " call this command.", "tokens": [51548, 818, 341, 5622, 13, 51600], "temperature": 0.0, "avg_logprob": -0.09477607240068152, "compression_ratio": 1.8501628664495113, "no_speech_prob": 0.002550603821873665}, {"id": 186, "seek": 66216, "start": 686.88, "end": 690.0, "text": " And then it would read that rather than you needing to copy and paste in the error.", "tokens": [51600, 400, 550, 309, 576, 1401, 300, 2831, 813, 291, 18006, 281, 5055, 293, 9163, 294, 264, 6713, 13, 51756], "temperature": 0.0, "avg_logprob": -0.09477607240068152, "compression_ratio": 1.8501628664495113, "no_speech_prob": 0.002550603821873665}, {"id": 187, "seek": 69000, "start": 690.0, "end": 691.68, "text": " But again, it's not really working.", "tokens": [50364, 583, 797, 11, 309, 311, 406, 534, 1364, 13, 50448], "temperature": 0.0, "avg_logprob": -0.108428113600787, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.001064841402694583}, {"id": 188, "seek": 69000, "start": 691.68, "end": 693.68, "text": " So I'm going to copy and paste the error still.", "tokens": [50448, 407, 286, 478, 516, 281, 5055, 293, 9163, 264, 6713, 920, 13, 50548], "temperature": 0.0, "avg_logprob": -0.108428113600787, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.001064841402694583}, {"id": 189, "seek": 69000, "start": 694.8, "end": 699.84, "text": " Otherwise, what it's going to do is it's going to look for any matches for do we want to read", "tokens": [50604, 10328, 11, 437, 309, 311, 516, 281, 360, 307, 309, 311, 516, 281, 574, 337, 604, 10676, 337, 360, 321, 528, 281, 1401, 50856], "temperature": 0.0, "avg_logprob": -0.108428113600787, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.001064841402694583}, {"id": 190, "seek": 69000, "start": 699.84, "end": 702.16, "text": " any files and it can do any number of files.", "tokens": [50856, 604, 7098, 293, 309, 393, 360, 604, 1230, 295, 7098, 13, 50972], "temperature": 0.0, "avg_logprob": -0.108428113600787, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.001064841402694583}, {"id": 191, "seek": 69000, "start": 702.16, "end": 707.44, "text": " And then essentially what it's going to do is either for files or websites that text that it", "tokens": [50972, 400, 550, 4476, 437, 309, 311, 516, 281, 360, 307, 2139, 337, 7098, 420, 12891, 300, 2487, 300, 309, 51236], "temperature": 0.0, "avg_logprob": -0.108428113600787, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.001064841402694583}, {"id": 192, "seek": 69000, "start": 707.44, "end": 711.04, "text": " parses, it's just going to shove it in and just it's going to build that context.", "tokens": [51236, 21156, 279, 11, 309, 311, 445, 516, 281, 35648, 309, 294, 293, 445, 309, 311, 516, 281, 1322, 300, 4319, 13, 51416], "temperature": 0.0, "avg_logprob": -0.108428113600787, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.001064841402694583}, {"id": 193, "seek": 69000, "start": 711.04, "end": 715.28, "text": " And so it's just going to populate that context with the actual text.", "tokens": [51416, 400, 370, 309, 311, 445, 516, 281, 1665, 5256, 300, 4319, 365, 264, 3539, 2487, 13, 51628], "temperature": 0.0, "avg_logprob": -0.108428113600787, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.001064841402694583}, {"id": 194, "seek": 69000, "start": 715.28, "end": 716.96, "text": " So it just adds it in.", "tokens": [51628, 407, 309, 445, 10860, 309, 294, 13, 51712], "temperature": 0.0, "avg_logprob": -0.108428113600787, "compression_ratio": 1.9367588932806323, "no_speech_prob": 0.001064841402694583}, {"id": 195, "seek": 71696, "start": 716.96, "end": 720.64, "text": " So you'll say I have this file here and then you'll say you'll have like the read command", "tokens": [50364, 407, 291, 603, 584, 286, 362, 341, 3991, 510, 293, 550, 291, 603, 584, 291, 603, 362, 411, 264, 1401, 5622, 50548], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 196, "seek": 71696, "start": 720.64, "end": 724.4000000000001, "text": " and it will quite literally say the file name and then the content of the file.", "tokens": [50548, 293, 309, 486, 1596, 3736, 584, 264, 3991, 1315, 293, 550, 264, 2701, 295, 264, 3991, 13, 50736], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 197, "seek": 71696, "start": 724.4000000000001, "end": 726.88, "text": " So again, no different than what you would say.", "tokens": [50736, 407, 797, 11, 572, 819, 813, 437, 291, 576, 584, 13, 50860], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 198, "seek": 71696, "start": 726.88, "end": 728.08, "text": " Hey, I've got this Python file.", "tokens": [50860, 1911, 11, 286, 600, 658, 341, 15329, 3991, 13, 50920], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 199, "seek": 71696, "start": 728.08, "end": 730.64, "text": " Here's the code copy, paste you paste in the code.", "tokens": [50920, 1692, 311, 264, 3089, 5055, 11, 9163, 291, 9163, 294, 264, 3089, 13, 51048], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 200, "seek": 71696, "start": 730.64, "end": 732.0, "text": " This is the same exact thing.", "tokens": [51048, 639, 307, 264, 912, 1900, 551, 13, 51116], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 201, "seek": 71696, "start": 732.0, "end": 734.32, "text": " It just builds that context for me.", "tokens": [51116, 467, 445, 15182, 300, 4319, 337, 385, 13, 51232], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 202, "seek": 71696, "start": 734.32, "end": 737.6800000000001, "text": " So once we have that user input, which is going to be, you know,", "tokens": [51232, 407, 1564, 321, 362, 300, 4195, 4846, 11, 597, 307, 516, 281, 312, 11, 291, 458, 11, 51400], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 203, "seek": 71696, "start": 737.6800000000001, "end": 742.0, "text": " please help me make a flash website or please help me do, you know, whatever it is,", "tokens": [51400, 1767, 854, 385, 652, 257, 7319, 3144, 420, 1767, 854, 385, 360, 11, 291, 458, 11, 2035, 309, 307, 11, 51616], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 204, "seek": 71696, "start": 742.0, "end": 745.0400000000001, "text": " we have that user input, we're going to send that off to GPT.", "tokens": [51616, 321, 362, 300, 4195, 4846, 11, 321, 434, 516, 281, 2845, 300, 766, 281, 26039, 51, 13, 51768], "temperature": 0.0, "avg_logprob": -0.09447629952136381, "compression_ratio": 1.8918032786885246, "no_speech_prob": 0.001032187370583415}, {"id": 205, "seek": 74504, "start": 745.5999999999999, "end": 748.7199999999999, "text": " And we're essentially going to wait until GPT is done.", "tokens": [50392, 400, 321, 434, 4476, 516, 281, 1699, 1826, 26039, 51, 307, 1096, 13, 50548], "temperature": 0.0, "avg_logprob": -0.08585588002608995, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.007575613912194967}, {"id": 206, "seek": 74504, "start": 748.7199999999999, "end": 750.16, "text": " So what does that mean?", "tokens": [50548, 407, 437, 775, 300, 914, 30, 50620], "temperature": 0.0, "avg_logprob": -0.08585588002608995, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.007575613912194967}, {"id": 207, "seek": 74504, "start": 750.16, "end": 752.24, "text": " So now it's time to go to our context.", "tokens": [50620, 407, 586, 309, 311, 565, 281, 352, 281, 527, 4319, 13, 50724], "temperature": 0.0, "avg_logprob": -0.08585588002608995, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.007575613912194967}, {"id": 208, "seek": 74504, "start": 752.24, "end": 759.36, "text": " So this is the pre-prompt that I'm using to force GPT-4 to behave in the way that I want.", "tokens": [50724, 407, 341, 307, 264, 659, 12, 28722, 662, 300, 286, 478, 1228, 281, 3464, 26039, 51, 12, 19, 281, 15158, 294, 264, 636, 300, 286, 528, 13, 51080], "temperature": 0.0, "avg_logprob": -0.08585588002608995, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.007575613912194967}, {"id": 209, "seek": 74504, "start": 759.92, "end": 766.16, "text": " So essentially, this is all sent off and this is how the program begins.", "tokens": [51108, 407, 4476, 11, 341, 307, 439, 2279, 766, 293, 341, 307, 577, 264, 1461, 7338, 13, 51420], "temperature": 0.0, "avg_logprob": -0.08585588002608995, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.007575613912194967}, {"id": 210, "seek": 74504, "start": 766.16, "end": 769.92, "text": " So we're starting off by just telling GPT-4.", "tokens": [51420, 407, 321, 434, 2891, 766, 538, 445, 3585, 26039, 51, 12, 19, 13, 51608], "temperature": 0.0, "avg_logprob": -0.08585588002608995, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.007575613912194967}, {"id": 211, "seek": 74504, "start": 769.92, "end": 772.8, "text": " Let me add word wrap real quick, make this a little easier on us.", "tokens": [51608, 961, 385, 909, 1349, 7019, 957, 1702, 11, 652, 341, 257, 707, 3571, 322, 505, 13, 51752], "temperature": 0.0, "avg_logprob": -0.08585588002608995, "compression_ratio": 1.6224066390041494, "no_speech_prob": 0.007575613912194967}, {"id": 212, "seek": 77280, "start": 773.3599999999999, "end": 776.0, "text": " We're saying, as if I had told it, you know,", "tokens": [50392, 492, 434, 1566, 11, 382, 498, 286, 632, 1907, 309, 11, 291, 458, 11, 50524], "temperature": 0.0, "avg_logprob": -0.10814290795444458, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.01065137330442667}, {"id": 213, "seek": 77280, "start": 776.0, "end": 778.4799999999999, "text": " give in text input from a user looking to do programming,", "tokens": [50524, 976, 294, 2487, 4846, 490, 257, 4195, 1237, 281, 360, 9410, 11, 50648], "temperature": 0.0, "avg_logprob": -0.10814290795444458, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.01065137330442667}, {"id": 214, "seek": 77280, "start": 778.4799999999999, "end": 782.7199999999999, "text": " help me build a series of console commands that will achieve the goals of the prompt.", "tokens": [50648, 854, 385, 1322, 257, 2638, 295, 11076, 16901, 300, 486, 4584, 264, 5493, 295, 264, 12391, 13, 50860], "temperature": 0.0, "avg_logprob": -0.10814290795444458, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.01065137330442667}, {"id": 215, "seek": 77280, "start": 782.7199999999999, "end": 789.04, "text": " Your responses will come one command at a time where each response is just the command", "tokens": [50860, 2260, 13019, 486, 808, 472, 5622, 412, 257, 565, 689, 1184, 4134, 307, 445, 264, 5622, 51176], "temperature": 0.0, "avg_logprob": -0.10814290795444458, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.01065137330442667}, {"id": 216, "seek": 77280, "start": 789.04, "end": 790.0, "text": " and nothing else.", "tokens": [51176, 293, 1825, 1646, 13, 51224], "temperature": 0.0, "avg_logprob": -0.10814290795444458, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.01065137330442667}, {"id": 217, "seek": 77280, "start": 790.0, "end": 794.0799999999999, "text": " When ready for the next command, the user will say next, all camps next.", "tokens": [51224, 1133, 1919, 337, 264, 958, 5622, 11, 264, 4195, 486, 584, 958, 11, 439, 16573, 958, 13, 51428], "temperature": 0.0, "avg_logprob": -0.10814290795444458, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.01065137330442667}, {"id": 218, "seek": 77280, "start": 795.04, "end": 797.3599999999999, "text": " And you will respond with the next command.", "tokens": [51476, 400, 291, 486, 4196, 365, 264, 958, 5622, 13, 51592], "temperature": 0.0, "avg_logprob": -0.10814290795444458, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.01065137330442667}, {"id": 219, "seek": 77280, "start": 797.3599999999999, "end": 801.3599999999999, "text": " When we have reached the end of console commands, respond with done.", "tokens": [51592, 1133, 321, 362, 6488, 264, 917, 295, 11076, 16901, 11, 4196, 365, 1096, 13, 51792], "temperature": 0.0, "avg_logprob": -0.10814290795444458, "compression_ratio": 1.8212927756653992, "no_speech_prob": 0.01065137330442667}, {"id": 220, "seek": 80136, "start": 801.92, "end": 805.84, "text": " And then you will do it again with some new input from the user.", "tokens": [50392, 400, 550, 291, 486, 360, 309, 797, 365, 512, 777, 4846, 490, 264, 4195, 13, 50588], "temperature": 0.0, "avg_logprob": -0.09133600832811042, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0011693715350702405}, {"id": 221, "seek": 80136, "start": 805.84, "end": 807.84, "text": " If you understand, say okay.", "tokens": [50588, 759, 291, 1223, 11, 584, 1392, 13, 50688], "temperature": 0.0, "avg_logprob": -0.09133600832811042, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0011693715350702405}, {"id": 222, "seek": 80136, "start": 807.84, "end": 811.12, "text": " And again, we're just forcing the assistant here, you know,", "tokens": [50688, 400, 797, 11, 321, 434, 445, 19030, 264, 10994, 510, 11, 291, 458, 11, 50852], "temperature": 0.0, "avg_logprob": -0.09133600832811042, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0011693715350702405}, {"id": 223, "seek": 80136, "start": 811.12, "end": 813.28, "text": " to think in historically it said, okay.", "tokens": [50852, 281, 519, 294, 16180, 309, 848, 11, 1392, 13, 50960], "temperature": 0.0, "avg_logprob": -0.09133600832811042, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0011693715350702405}, {"id": 224, "seek": 80136, "start": 813.92, "end": 816.72, "text": " And then what I'm doing is a one shot kind of example.", "tokens": [50992, 400, 550, 437, 286, 478, 884, 307, 257, 472, 3347, 733, 295, 1365, 13, 51132], "temperature": 0.0, "avg_logprob": -0.09133600832811042, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0011693715350702405}, {"id": 225, "seek": 80136, "start": 817.44, "end": 822.08, "text": " I did find GPT-4 to be pretty good at following this command,", "tokens": [51168, 286, 630, 915, 26039, 51, 12, 19, 281, 312, 1238, 665, 412, 3480, 341, 5622, 11, 51400], "temperature": 0.0, "avg_logprob": -0.09133600832811042, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0011693715350702405}, {"id": 226, "seek": 80136, "start": 822.08, "end": 825.6800000000001, "text": " but very often it's still added commentary to the commands and all that.", "tokens": [51400, 457, 588, 2049, 309, 311, 920, 3869, 23527, 281, 264, 16901, 293, 439, 300, 13, 51580], "temperature": 0.0, "avg_logprob": -0.09133600832811042, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0011693715350702405}, {"id": 227, "seek": 80136, "start": 825.6800000000001, "end": 829.6800000000001, "text": " And yes, we could use literally a large language model to parse out the command still.", "tokens": [51580, 400, 2086, 11, 321, 727, 764, 3736, 257, 2416, 2856, 2316, 281, 48377, 484, 264, 5622, 920, 13, 51780], "temperature": 0.0, "avg_logprob": -0.09133600832811042, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0011693715350702405}, {"id": 228, "seek": 80136, "start": 830.32, "end": 831.12, "text": " I don't like that.", "tokens": [51812, 286, 500, 380, 411, 300, 13, 51852], "temperature": 0.0, "avg_logprob": -0.09133600832811042, "compression_ratio": 1.663265306122449, "no_speech_prob": 0.0011693715350702405}, {"id": 229, "seek": 83112, "start": 831.12, "end": 833.04, "text": " So I wanted to do at least a one shot.", "tokens": [50364, 407, 286, 1415, 281, 360, 412, 1935, 257, 472, 3347, 13, 50460], "temperature": 0.0, "avg_logprob": -0.062530700977032, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005883298581466079}, {"id": 230, "seek": 83112, "start": 833.04, "end": 836.08, "text": " I thought maybe it might take more, but a one shot gives us an opportunity", "tokens": [50460, 286, 1194, 1310, 309, 1062, 747, 544, 11, 457, 257, 472, 3347, 2709, 505, 364, 2650, 50612], "temperature": 0.0, "avg_logprob": -0.062530700977032, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005883298581466079}, {"id": 231, "seek": 83112, "start": 836.88, "end": 838.96, "text": " to show it even more basic stuff.", "tokens": [50652, 281, 855, 309, 754, 544, 3875, 1507, 13, 50756], "temperature": 0.0, "avg_logprob": -0.062530700977032, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005883298581466079}, {"id": 232, "seek": 83112, "start": 838.96, "end": 842.64, "text": " So like for example, in this case, we're using cat to write to these files.", "tokens": [50756, 407, 411, 337, 1365, 11, 294, 341, 1389, 11, 321, 434, 1228, 3857, 281, 2464, 281, 613, 7098, 13, 50940], "temperature": 0.0, "avg_logprob": -0.062530700977032, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005883298581466079}, {"id": 233, "seek": 83112, "start": 842.64, "end": 845.2, "text": " But there's lots of ways that this could have been done.", "tokens": [50940, 583, 456, 311, 3195, 295, 2098, 300, 341, 727, 362, 668, 1096, 13, 51068], "temperature": 0.0, "avg_logprob": -0.062530700977032, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005883298581466079}, {"id": 234, "seek": 83112, "start": 845.2, "end": 851.2, "text": " But given all of the huge variability of stuff that can happen inside of a script,", "tokens": [51068, 583, 2212, 439, 295, 264, 2603, 35709, 295, 1507, 300, 393, 1051, 1854, 295, 257, 5755, 11, 51368], "temperature": 0.0, "avg_logprob": -0.062530700977032, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005883298581466079}, {"id": 235, "seek": 83112, "start": 851.2, "end": 856.96, "text": " it would likely break many of the methods that might be used to populate a file.", "tokens": [51368, 309, 576, 3700, 1821, 867, 295, 264, 7150, 300, 1062, 312, 1143, 281, 1665, 5256, 257, 3991, 13, 51656], "temperature": 0.0, "avg_logprob": -0.062530700977032, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005883298581466079}, {"id": 236, "seek": 83112, "start": 856.96, "end": 858.8, "text": " So this one is just the one that works.", "tokens": [51656, 407, 341, 472, 307, 445, 264, 472, 300, 1985, 13, 51748], "temperature": 0.0, "avg_logprob": -0.062530700977032, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0005883298581466079}, {"id": 237, "seek": 85880, "start": 858.8, "end": 863.1999999999999, "text": " So it allows us to kind of still kind of show really what we're looking for.", "tokens": [50364, 407, 309, 4045, 505, 281, 733, 295, 920, 733, 295, 855, 534, 437, 321, 434, 1237, 337, 13, 50584], "temperature": 0.0, "avg_logprob": -0.10467000802357991, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.00021653289149980992}, {"id": 238, "seek": 85880, "start": 863.1999999999999, "end": 869.12, "text": " So for example, with such a basic thing as show me a basic flask web dev example with templates,", "tokens": [50584, 407, 337, 1365, 11, 365, 1270, 257, 3875, 551, 382, 855, 385, 257, 3875, 932, 3863, 3670, 1905, 1365, 365, 21165, 11, 50880], "temperature": 0.0, "avg_logprob": -0.10467000802357991, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.00021653289149980992}, {"id": 239, "seek": 85880, "start": 869.12, "end": 874.88, "text": " right? And the first command, in this case, we're making this up.", "tokens": [50880, 558, 30, 400, 264, 700, 5622, 11, 294, 341, 1389, 11, 321, 434, 1455, 341, 493, 13, 51168], "temperature": 0.0, "avg_logprob": -0.10467000802357991, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.00021653289149980992}, {"id": 240, "seek": 85880, "start": 874.88, "end": 878.4799999999999, "text": " We're showing GPT-4 how we want it to behave.", "tokens": [51168, 492, 434, 4099, 26039, 51, 12, 19, 577, 321, 528, 309, 281, 15158, 13, 51348], "temperature": 0.0, "avg_logprob": -0.10467000802357991, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.00021653289149980992}, {"id": 241, "seek": 85880, "start": 878.4799999999999, "end": 884.0, "text": " So we're saying if someone asked this, here's an example of the sequence of events that should occur.", "tokens": [51348, 407, 321, 434, 1566, 498, 1580, 2351, 341, 11, 510, 311, 364, 1365, 295, 264, 8310, 295, 3931, 300, 820, 5160, 13, 51624], "temperature": 0.0, "avg_logprob": -0.10467000802357991, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.00021653289149980992}, {"id": 242, "seek": 85880, "start": 884.0, "end": 885.8399999999999, "text": " First, you're going to need to pip install flask.", "tokens": [51624, 2386, 11, 291, 434, 516, 281, 643, 281, 8489, 3625, 932, 3863, 13, 51716], "temperature": 0.0, "avg_logprob": -0.10467000802357991, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.00021653289149980992}, {"id": 243, "seek": 88584, "start": 886.48, "end": 888.64, "text": " And then the user is going to say, next.", "tokens": [50396, 400, 550, 264, 4195, 307, 516, 281, 584, 11, 958, 13, 50504], "temperature": 0.0, "avg_logprob": -0.11250284676239869, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.0013247958850115538}, {"id": 244, "seek": 88584, "start": 888.64, "end": 893.44, "text": " And then GPT-4 is going to respond with, you're going to want to make a directory basic flask app.", "tokens": [50504, 400, 550, 26039, 51, 12, 19, 307, 516, 281, 4196, 365, 11, 291, 434, 516, 281, 528, 281, 652, 257, 21120, 3875, 932, 3863, 724, 13, 50744], "temperature": 0.0, "avg_logprob": -0.11250284676239869, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.0013247958850115538}, {"id": 245, "seek": 88584, "start": 894.08, "end": 902.72, "text": " Next, so already we're trying to get it to kind of put this into specific project folders.", "tokens": [50776, 3087, 11, 370, 1217, 321, 434, 1382, 281, 483, 309, 281, 733, 295, 829, 341, 666, 2685, 1716, 31082, 13, 51208], "temperature": 0.0, "avg_logprob": -0.11250284676239869, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.0013247958850115538}, {"id": 246, "seek": 88584, "start": 903.44, "end": 908.72, "text": " Then what it's going to do is actually write and then into that directory and then app.py.", "tokens": [51244, 1396, 437, 309, 311, 516, 281, 360, 307, 767, 2464, 293, 550, 666, 300, 21120, 293, 550, 724, 13, 8200, 13, 51508], "temperature": 0.0, "avg_logprob": -0.11250284676239869, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.0013247958850115538}, {"id": 247, "seek": 88584, "start": 909.6800000000001, "end": 914.64, "text": " It's going to add just a very basic app.py with some template examples.", "tokens": [51556, 467, 311, 516, 281, 909, 445, 257, 588, 3875, 724, 13, 8200, 365, 512, 12379, 5110, 13, 51804], "temperature": 0.0, "avg_logprob": -0.11250284676239869, "compression_ratio": 1.6866952789699572, "no_speech_prob": 0.0013247958850115538}, {"id": 248, "seek": 91464, "start": 914.64, "end": 918.72, "text": " Thank you. And then, okay, that's what it says.", "tokens": [50364, 1044, 291, 13, 400, 550, 11, 1392, 11, 300, 311, 437, 309, 1619, 13, 50568], "temperature": 0.0, "avg_logprob": -0.10069070756435394, "compression_ratio": 1.6653061224489796, "no_speech_prob": 0.0018965627532452345}, {"id": 249, "seek": 91464, "start": 918.72, "end": 920.16, "text": " And then the user, next.", "tokens": [50568, 400, 550, 264, 4195, 11, 958, 13, 50640], "temperature": 0.0, "avg_logprob": -0.10069070756435394, "compression_ratio": 1.6653061224489796, "no_speech_prob": 0.0018965627532452345}, {"id": 250, "seek": 91464, "start": 920.16, "end": 924.48, "text": " And then it wants to make a template stir, add that index.html.", "tokens": [50640, 400, 550, 309, 2738, 281, 652, 257, 12379, 8946, 11, 909, 300, 8186, 13, 357, 15480, 13, 50856], "temperature": 0.0, "avg_logprob": -0.10069070756435394, "compression_ratio": 1.6653061224489796, "no_speech_prob": 0.0018965627532452345}, {"id": 251, "seek": 91464, "start": 924.48, "end": 925.28, "text": " Next, blah, blah, blah.", "tokens": [50856, 3087, 11, 12288, 11, 12288, 11, 12288, 13, 50896], "temperature": 0.0, "avg_logprob": -0.10069070756435394, "compression_ratio": 1.6653061224489796, "no_speech_prob": 0.0018965627532452345}, {"id": 252, "seek": 91464, "start": 925.28, "end": 930.16, "text": " You get the idea all the way down to actually running that app.py.", "tokens": [50896, 509, 483, 264, 1558, 439, 264, 636, 760, 281, 767, 2614, 300, 724, 13, 8200, 13, 51140], "temperature": 0.0, "avg_logprob": -0.10069070756435394, "compression_ratio": 1.6653061224489796, "no_speech_prob": 0.0018965627532452345}, {"id": 253, "seek": 91464, "start": 930.16, "end": 931.12, "text": " The user says, next.", "tokens": [51140, 440, 4195, 1619, 11, 958, 13, 51188], "temperature": 0.0, "avg_logprob": -0.10069070756435394, "compression_ratio": 1.6653061224489796, "no_speech_prob": 0.0018965627532452345}, {"id": 254, "seek": 91464, "start": 931.12, "end": 933.36, "text": " And actually we're done because we've run the app.", "tokens": [51188, 400, 767, 321, 434, 1096, 570, 321, 600, 1190, 264, 724, 13, 51300], "temperature": 0.0, "avg_logprob": -0.10069070756435394, "compression_ratio": 1.6653061224489796, "no_speech_prob": 0.0018965627532452345}, {"id": 255, "seek": 91464, "start": 933.36, "end": 937.4399999999999, "text": " So it's just a really simple example that we're showing GPT-4.", "tokens": [51300, 407, 309, 311, 445, 257, 534, 2199, 1365, 300, 321, 434, 4099, 26039, 51, 12, 19, 13, 51504], "temperature": 0.0, "avg_logprob": -0.10069070756435394, "compression_ratio": 1.6653061224489796, "no_speech_prob": 0.0018965627532452345}, {"id": 256, "seek": 91464, "start": 938.0, "end": 940.16, "text": " This is how I want you to respond every time.", "tokens": [51532, 639, 307, 577, 286, 528, 291, 281, 4196, 633, 565, 13, 51640], "temperature": 0.0, "avg_logprob": -0.10069070756435394, "compression_ratio": 1.6653061224489796, "no_speech_prob": 0.0018965627532452345}, {"id": 257, "seek": 94016, "start": 940.16, "end": 944.88, "text": " And then if that way, if the user did happen to say clear or whatever,", "tokens": [50364, 400, 550, 498, 300, 636, 11, 498, 264, 4195, 630, 1051, 281, 584, 1850, 420, 2035, 11, 50600], "temperature": 0.0, "avg_logprob": -0.08481407165527344, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0062888674437999725}, {"id": 258, "seek": 94016, "start": 944.88, "end": 947.92, "text": " we just reset right back to this context and continue on.", "tokens": [50600, 321, 445, 14322, 558, 646, 281, 341, 4319, 293, 2354, 322, 13, 50752], "temperature": 0.0, "avg_logprob": -0.08481407165527344, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0062888674437999725}, {"id": 259, "seek": 94016, "start": 947.92, "end": 952.8, "text": " But I found that you can generally keep going pretty far and be totally fine.", "tokens": [50752, 583, 286, 1352, 300, 291, 393, 5101, 1066, 516, 1238, 1400, 293, 312, 3879, 2489, 13, 50996], "temperature": 0.0, "avg_logprob": -0.08481407165527344, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0062888674437999725}, {"id": 260, "seek": 94016, "start": 952.8, "end": 955.8399999999999, "text": " But if you are experiencing weird things, you would just clear it and start over.", "tokens": [50996, 583, 498, 291, 366, 11139, 3657, 721, 11, 291, 576, 445, 1850, 309, 293, 722, 670, 13, 51148], "temperature": 0.0, "avg_logprob": -0.08481407165527344, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0062888674437999725}, {"id": 261, "seek": 94016, "start": 956.4, "end": 960.9599999999999, "text": " But anyway, coming over here, while not GPT-done.", "tokens": [51176, 583, 4033, 11, 1348, 670, 510, 11, 1339, 406, 26039, 51, 12, 45939, 13, 51404], "temperature": 0.0, "avg_logprob": -0.08481407165527344, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0062888674437999725}, {"id": 262, "seek": 94016, "start": 961.52, "end": 966.0799999999999, "text": " So GPT-done starts as, where the heck are you here?", "tokens": [51432, 407, 26039, 51, 12, 45939, 3719, 382, 11, 689, 264, 12872, 366, 291, 510, 30, 51660], "temperature": 0.0, "avg_logprob": -0.08481407165527344, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0062888674437999725}, {"id": 263, "seek": 94016, "start": 966.0799999999999, "end": 967.6, "text": " Because we haven't started a new input.", "tokens": [51660, 1436, 321, 2378, 380, 1409, 257, 777, 4846, 13, 51736], "temperature": 0.0, "avg_logprob": -0.08481407165527344, "compression_ratio": 1.6349809885931559, "no_speech_prob": 0.0062888674437999725}, {"id": 264, "seek": 96760, "start": 967.6, "end": 969.9200000000001, "text": " So we're going to start as GPT-done equals false.", "tokens": [50364, 407, 321, 434, 516, 281, 722, 382, 26039, 51, 12, 45939, 6915, 7908, 13, 50480], "temperature": 0.0, "avg_logprob": -0.09689585596537419, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.0019264925504103303}, {"id": 265, "seek": 96760, "start": 970.5600000000001, "end": 972.88, "text": " So then what's going to happen is, as long as that's true,", "tokens": [50512, 407, 550, 437, 311, 516, 281, 1051, 307, 11, 382, 938, 382, 300, 311, 2074, 11, 50628], "temperature": 0.0, "avg_logprob": -0.09689585596537419, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.0019264925504103303}, {"id": 266, "seek": 96760, "start": 972.88, "end": 976.08, "text": " what we're going to do is we're going to wait for the next command from the user.", "tokens": [50628, 437, 321, 434, 516, 281, 360, 307, 321, 434, 516, 281, 1699, 337, 264, 958, 5622, 490, 264, 4195, 13, 50788], "temperature": 0.0, "avg_logprob": -0.09689585596537419, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.0019264925504103303}, {"id": 267, "seek": 96760, "start": 977.0400000000001, "end": 979.84, "text": " And essentially, it's going to just inject that.", "tokens": [50836, 400, 4476, 11, 309, 311, 516, 281, 445, 10711, 300, 13, 50976], "temperature": 0.0, "avg_logprob": -0.09689585596537419, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.0019264925504103303}, {"id": 268, "seek": 96760, "start": 980.5600000000001, "end": 983.12, "text": " So we don't need to say next.", "tokens": [51012, 407, 321, 500, 380, 643, 281, 584, 958, 13, 51140], "temperature": 0.0, "avg_logprob": -0.09689585596537419, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.0019264925504103303}, {"id": 269, "seek": 96760, "start": 983.12, "end": 988.08, "text": " We're just going to automatically tell GPT-4 next in the script.", "tokens": [51140, 492, 434, 445, 516, 281, 6772, 980, 26039, 51, 12, 19, 958, 294, 264, 5755, 13, 51388], "temperature": 0.0, "avg_logprob": -0.09689585596537419, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.0019264925504103303}, {"id": 270, "seek": 96760, "start": 988.08, "end": 993.12, "text": " And we're just waiting until we literally see reply-content.lower.", "tokens": [51388, 400, 321, 434, 445, 3806, 1826, 321, 3736, 536, 16972, 12, 9000, 317, 13, 34598, 13, 51640], "temperature": 0.0, "avg_logprob": -0.09689585596537419, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.0019264925504103303}, {"id": 271, "seek": 96760, "start": 993.12, "end": 996.32, "text": " The only reason I'm doing this, GPT-4 always did an all caps done.", "tokens": [51640, 440, 787, 1778, 286, 478, 884, 341, 11, 26039, 51, 12, 19, 1009, 630, 364, 439, 13855, 1096, 13, 51800], "temperature": 0.0, "avg_logprob": -0.09689585596537419, "compression_ratio": 1.7593984962406015, "no_speech_prob": 0.0019264925504103303}, {"id": 272, "seek": 99632, "start": 996.32, "end": 997.12, "text": " It was good.", "tokens": [50364, 467, 390, 665, 13, 50404], "temperature": 0.0, "avg_logprob": -0.09665033465526143, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.014953898265957832}, {"id": 273, "seek": 99632, "start": 998.24, "end": 1002.6400000000001, "text": " But chat GPT, for example, I was just trying between the two of them to see if we save some money.", "tokens": [50460, 583, 5081, 26039, 51, 11, 337, 1365, 11, 286, 390, 445, 1382, 1296, 264, 732, 295, 552, 281, 536, 498, 321, 3155, 512, 1460, 13, 50680], "temperature": 0.0, "avg_logprob": -0.09665033465526143, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.014953898265957832}, {"id": 274, "seek": 99632, "start": 1004.32, "end": 1006.5600000000001, "text": " It would sometimes lowercase, sometimes add a period.", "tokens": [50764, 467, 576, 2171, 3126, 9765, 11, 2171, 909, 257, 2896, 13, 50876], "temperature": 0.0, "avg_logprob": -0.09665033465526143, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.014953898265957832}, {"id": 275, "seek": 99632, "start": 1006.5600000000001, "end": 1007.84, "text": " So I was trying to handle for that.", "tokens": [50876, 407, 286, 390, 1382, 281, 4813, 337, 300, 13, 50940], "temperature": 0.0, "avg_logprob": -0.09665033465526143, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.014953898265957832}, {"id": 276, "seek": 99632, "start": 1007.84, "end": 1012.0, "text": " But again, GPT-4 really follows this very well.", "tokens": [50940, 583, 797, 11, 26039, 51, 12, 19, 534, 10002, 341, 588, 731, 13, 51148], "temperature": 0.0, "avg_logprob": -0.09665033465526143, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.014953898265957832}, {"id": 277, "seek": 99632, "start": 1012.0, "end": 1020.08, "text": " I'm sure I could come up with ways to structure things such that chat GPT or GPT-3.5 works.", "tokens": [51148, 286, 478, 988, 286, 727, 808, 493, 365, 2098, 281, 3877, 721, 1270, 300, 5081, 26039, 51, 420, 26039, 51, 12, 18, 13, 20, 1985, 13, 51552], "temperature": 0.0, "avg_logprob": -0.09665033465526143, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.014953898265957832}, {"id": 278, "seek": 99632, "start": 1020.08, "end": 1023.6800000000001, "text": " There's no reason in my head that this couldn't be done with chat GPT.", "tokens": [51552, 821, 311, 572, 1778, 294, 452, 1378, 300, 341, 2809, 380, 312, 1096, 365, 5081, 26039, 51, 13, 51732], "temperature": 0.0, "avg_logprob": -0.09665033465526143, "compression_ratio": 1.6031128404669261, "no_speech_prob": 0.014953898265957832}, {"id": 279, "seek": 102368, "start": 1023.68, "end": 1026.3999999999999, "text": " But GPT-4 is just way better than chat GPT.", "tokens": [50364, 583, 26039, 51, 12, 19, 307, 445, 636, 1101, 813, 5081, 26039, 51, 13, 50500], "temperature": 0.0, "avg_logprob": -0.07298994493914081, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0013457126915454865}, {"id": 280, "seek": 102368, "start": 1026.3999999999999, "end": 1028.48, "text": " So anyway, moving on.", "tokens": [50500, 407, 4033, 11, 2684, 322, 13, 50604], "temperature": 0.0, "avg_logprob": -0.07298994493914081, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0013457126915454865}, {"id": 281, "seek": 102368, "start": 1030.3999999999999, "end": 1033.2, "text": " So if it has reached a done, then we set done to true.", "tokens": [50700, 407, 498, 309, 575, 6488, 257, 1096, 11, 550, 321, 992, 1096, 281, 2074, 13, 50840], "temperature": 0.0, "avg_logprob": -0.07298994493914081, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0013457126915454865}, {"id": 282, "seek": 102368, "start": 1033.2, "end": 1034.8799999999999, "text": " Cool. And we break out.", "tokens": [50840, 8561, 13, 400, 321, 1821, 484, 13, 50924], "temperature": 0.0, "avg_logprob": -0.07298994493914081, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0013457126915454865}, {"id": 283, "seek": 102368, "start": 1034.8799999999999, "end": 1039.04, "text": " And then essentially, all the way up to this point, we've been, every time we get a command,", "tokens": [50924, 400, 550, 4476, 11, 439, 264, 636, 493, 281, 341, 935, 11, 321, 600, 668, 11, 633, 565, 321, 483, 257, 5622, 11, 51132], "temperature": 0.0, "avg_logprob": -0.07298994493914081, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0013457126915454865}, {"id": 284, "seek": 102368, "start": 1039.04, "end": 1043.36, "text": " we've been adding that command to a list of commands right here.", "tokens": [51132, 321, 600, 668, 5127, 300, 5622, 281, 257, 1329, 295, 16901, 558, 510, 13, 51348], "temperature": 0.0, "avg_logprob": -0.07298994493914081, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0013457126915454865}, {"id": 285, "seek": 102368, "start": 1044.08, "end": 1047.76, "text": " Once we have that list of commands, you could have been theoretically running them,", "tokens": [51384, 3443, 321, 362, 300, 1329, 295, 16901, 11, 291, 727, 362, 668, 29400, 2614, 552, 11, 51568], "temperature": 0.0, "avg_logprob": -0.07298994493914081, "compression_ratio": 1.6425531914893616, "no_speech_prob": 0.0013457126915454865}, {"id": 286, "seek": 104776, "start": 1047.76, "end": 1054.08, "text": " getting the return, and plausibly adding this to the context or something just to see,", "tokens": [50364, 1242, 264, 2736, 11, 293, 34946, 3545, 5127, 341, 281, 264, 4319, 420, 746, 445, 281, 536, 11, 50680], "temperature": 0.0, "avg_logprob": -0.10697855552037557, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.17321720719337463}, {"id": 287, "seek": 104776, "start": 1054.08, "end": 1055.36, "text": " did you hit an arrow along the way?", "tokens": [50680, 630, 291, 2045, 364, 11610, 2051, 264, 636, 30, 50744], "temperature": 0.0, "avg_logprob": -0.10697855552037557, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.17321720719337463}, {"id": 288, "seek": 104776, "start": 1056.08, "end": 1059.36, "text": " I think that's a mistake because either you're going to have to sit there and keep clicking", "tokens": [50780, 286, 519, 300, 311, 257, 6146, 570, 2139, 291, 434, 516, 281, 362, 281, 1394, 456, 293, 1066, 9697, 50944], "temperature": 0.0, "avg_logprob": -0.10697855552037557, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.17321720719337463}, {"id": 289, "seek": 104776, "start": 1059.36, "end": 1061.2, "text": " like next or saying next or whatever.", "tokens": [50944, 411, 958, 420, 1566, 958, 420, 2035, 13, 51036], "temperature": 0.0, "avg_logprob": -0.10697855552037557, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.17321720719337463}, {"id": 290, "seek": 104776, "start": 1061.92, "end": 1064.16, "text": " Or you're going to be blindly running commands.", "tokens": [51072, 1610, 291, 434, 516, 281, 312, 47744, 2614, 16901, 13, 51184], "temperature": 0.0, "avg_logprob": -0.10697855552037557, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.17321720719337463}, {"id": 291, "seek": 104776, "start": 1064.16, "end": 1067.28, "text": " And you're probably more likely to just kind of blindly just hit go, go, go.", "tokens": [51184, 400, 291, 434, 1391, 544, 3700, 281, 445, 733, 295, 47744, 445, 2045, 352, 11, 352, 11, 352, 13, 51340], "temperature": 0.0, "avg_logprob": -0.10697855552037557, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.17321720719337463}, {"id": 292, "seek": 104776, "start": 1068.08, "end": 1069.28, "text": " And I don't think that's a good idea.", "tokens": [51380, 400, 286, 500, 380, 519, 300, 311, 257, 665, 1558, 13, 51440], "temperature": 0.0, "avg_logprob": -0.10697855552037557, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.17321720719337463}, {"id": 293, "seek": 104776, "start": 1069.28, "end": 1071.6, "text": " So we're going to build that whole list.", "tokens": [51440, 407, 321, 434, 516, 281, 1322, 300, 1379, 1329, 13, 51556], "temperature": 0.0, "avg_logprob": -0.10697855552037557, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.17321720719337463}, {"id": 294, "seek": 104776, "start": 1071.6, "end": 1076.8, "text": " And then we're essentially going to iterate through that whole list, printing it out in bold red,", "tokens": [51556, 400, 550, 321, 434, 4476, 516, 281, 44497, 807, 300, 1379, 1329, 11, 14699, 309, 484, 294, 11928, 2182, 11, 51816], "temperature": 0.0, "avg_logprob": -0.10697855552037557, "compression_ratio": 1.8590604026845639, "no_speech_prob": 0.17321720719337463}, {"id": 295, "seek": 107680, "start": 1077.52, "end": 1080.6399999999999, "text": " making it clear what the commands are that are going to be run.", "tokens": [50400, 1455, 309, 1850, 437, 264, 16901, 366, 300, 366, 516, 281, 312, 1190, 13, 50556], "temperature": 0.0, "avg_logprob": -0.07686683109828404, "compression_ratio": 1.7797202797202798, "no_speech_prob": 0.0007553366594947875}, {"id": 296, "seek": 107680, "start": 1081.52, "end": 1087.36, "text": " And then if we want to actually run those commands, then we are going to go ahead and", "tokens": [50600, 400, 550, 498, 321, 528, 281, 767, 1190, 729, 16901, 11, 550, 321, 366, 516, 281, 352, 2286, 293, 50892], "temperature": 0.0, "avg_logprob": -0.07686683109828404, "compression_ratio": 1.7797202797202798, "no_speech_prob": 0.0007553366594947875}, {"id": 297, "seek": 107680, "start": 1087.36, "end": 1088.8, "text": " iterate through all of them and run them.", "tokens": [50892, 44497, 807, 439, 295, 552, 293, 1190, 552, 13, 50964], "temperature": 0.0, "avg_logprob": -0.07686683109828404, "compression_ratio": 1.7797202797202798, "no_speech_prob": 0.0007553366594947875}, {"id": 298, "seek": 107680, "start": 1088.8, "end": 1091.9199999999998, "text": " And this is where I'm attempting to get the actual output.", "tokens": [50964, 400, 341, 307, 689, 286, 478, 22001, 281, 483, 264, 3539, 5598, 13, 51120], "temperature": 0.0, "avg_logprob": -0.07686683109828404, "compression_ratio": 1.7797202797202798, "no_speech_prob": 0.0007553366594947875}, {"id": 299, "seek": 107680, "start": 1091.9199999999998, "end": 1094.96, "text": " This does work all the way up to, I believe, the last output.", "tokens": [51120, 639, 775, 589, 439, 264, 636, 493, 281, 11, 286, 1697, 11, 264, 1036, 5598, 13, 51272], "temperature": 0.0, "avg_logprob": -0.07686683109828404, "compression_ratio": 1.7797202797202798, "no_speech_prob": 0.0007553366594947875}, {"id": 300, "seek": 107680, "start": 1094.96, "end": 1100.0, "text": " I need to look a little deeper into that and see if there's got to be something wrong with", "tokens": [51272, 286, 643, 281, 574, 257, 707, 7731, 666, 300, 293, 536, 498, 456, 311, 658, 281, 312, 746, 2085, 365, 51524], "temperature": 0.0, "avg_logprob": -0.07686683109828404, "compression_ratio": 1.7797202797202798, "no_speech_prob": 0.0007553366594947875}, {"id": 301, "seek": 107680, "start": 1100.0, "end": 1100.72, "text": " my logic or something.", "tokens": [51524, 452, 9952, 420, 746, 13, 51560], "temperature": 0.0, "avg_logprob": -0.07686683109828404, "compression_ratio": 1.7797202797202798, "no_speech_prob": 0.0007553366594947875}, {"id": 302, "seek": 107680, "start": 1100.72, "end": 1104.1599999999999, "text": " Like I feel like this should work, but for whatever reason, it's just not working.", "tokens": [51560, 1743, 286, 841, 411, 341, 820, 589, 11, 457, 337, 2035, 1778, 11, 309, 311, 445, 406, 1364, 13, 51732], "temperature": 0.0, "avg_logprob": -0.07686683109828404, "compression_ratio": 1.7797202797202798, "no_speech_prob": 0.0007553366594947875}, {"id": 303, "seek": 110416, "start": 1104.88, "end": 1111.6000000000001, "text": " Anyway, if the user puts in anything other than Y here, it just doesn't run the commands.", "tokens": [50400, 5684, 11, 498, 264, 4195, 8137, 294, 1340, 661, 813, 398, 510, 11, 309, 445, 1177, 380, 1190, 264, 16901, 13, 50736], "temperature": 0.0, "avg_logprob": -0.12325116671048678, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0025506627280265093}, {"id": 304, "seek": 110416, "start": 1111.6000000000001, "end": 1112.0800000000002, "text": " And that's fine.", "tokens": [50736, 400, 300, 311, 2489, 13, 50760], "temperature": 0.0, "avg_logprob": -0.12325116671048678, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0025506627280265093}, {"id": 305, "seek": 110416, "start": 1112.96, "end": 1118.5600000000002, "text": " Sometimes still, it does suggest to you, it might still give you some text,", "tokens": [50804, 4803, 920, 11, 309, 775, 3402, 281, 291, 11, 309, 1062, 920, 976, 291, 512, 2487, 11, 51084], "temperature": 0.0, "avg_logprob": -0.12325116671048678, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0025506627280265093}, {"id": 306, "seek": 110416, "start": 1118.5600000000002, "end": 1121.6000000000001, "text": " especially I found that to be true with the console output.", "tokens": [51084, 2318, 286, 1352, 300, 281, 312, 2074, 365, 264, 11076, 5598, 13, 51236], "temperature": 0.0, "avg_logprob": -0.12325116671048678, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0025506627280265093}, {"id": 307, "seek": 110416, "start": 1121.6000000000001, "end": 1124.24, "text": " It'll just say, oh, I didn't find any errors or something like that.", "tokens": [51236, 467, 603, 445, 584, 11, 1954, 11, 286, 994, 380, 915, 604, 13603, 420, 746, 411, 300, 13, 51368], "temperature": 0.0, "avg_logprob": -0.12325116671048678, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0025506627280265093}, {"id": 308, "seek": 110416, "start": 1124.24, "end": 1126.0800000000002, "text": " So it doesn't say done, like it should.", "tokens": [51368, 407, 309, 1177, 380, 584, 1096, 11, 411, 309, 820, 13, 51460], "temperature": 0.0, "avg_logprob": -0.12325116671048678, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0025506627280265093}, {"id": 309, "seek": 110416, "start": 1126.0800000000002, "end": 1129.2, "text": " But anyway, that's all there is to it.", "tokens": [51460, 583, 4033, 11, 300, 311, 439, 456, 307, 281, 309, 13, 51616], "temperature": 0.0, "avg_logprob": -0.12325116671048678, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0025506627280265093}, {"id": 310, "seek": 110416, "start": 1129.2, "end": 1133.28, "text": " All of the power of this, this is all just a bunch of stupid logic,", "tokens": [51616, 1057, 295, 264, 1347, 295, 341, 11, 341, 307, 439, 445, 257, 3840, 295, 6631, 9952, 11, 51820], "temperature": 0.0, "avg_logprob": -0.12325116671048678, "compression_ratio": 1.6838235294117647, "no_speech_prob": 0.0025506627280265093}, {"id": 311, "seek": 113328, "start": 1134.0, "end": 1136.3999999999999, "text": " really around what is the pre-prompt.", "tokens": [50400, 534, 926, 437, 307, 264, 659, 12, 28722, 662, 13, 50520], "temperature": 0.0, "avg_logprob": -0.09995221555902717, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.002549978671595454}, {"id": 312, "seek": 113328, "start": 1136.3999999999999, "end": 1142.32, "text": " And what we can do with a pre-prompt is, I mean, there's essentially infinite pre-prompts", "tokens": [50520, 400, 437, 321, 393, 360, 365, 257, 659, 12, 28722, 662, 307, 11, 286, 914, 11, 456, 311, 4476, 13785, 659, 12, 28722, 39280, 50816], "temperature": 0.0, "avg_logprob": -0.09995221555902717, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.002549978671595454}, {"id": 313, "seek": 113328, "start": 1142.32, "end": 1143.84, "text": " that we could theoretically use.", "tokens": [50816, 300, 321, 727, 29400, 764, 13, 50892], "temperature": 0.0, "avg_logprob": -0.09995221555902717, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.002549978671595454}, {"id": 314, "seek": 113328, "start": 1144.72, "end": 1148.24, "text": " There are better examples plausibly that we could show it.", "tokens": [50936, 821, 366, 1101, 5110, 34946, 3545, 300, 321, 727, 855, 309, 13, 51112], "temperature": 0.0, "avg_logprob": -0.09995221555902717, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.002549978671595454}, {"id": 315, "seek": 113328, "start": 1148.8, "end": 1155.28, "text": " And there's more capabilities that we could show it or teach it or even just kind of that might", "tokens": [51140, 400, 456, 311, 544, 10862, 300, 321, 727, 855, 309, 420, 2924, 309, 420, 754, 445, 733, 295, 300, 1062, 51464], "temperature": 0.0, "avg_logprob": -0.09995221555902717, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.002549978671595454}, {"id": 316, "seek": 113328, "start": 1155.28, "end": 1156.16, "text": " show up.", "tokens": [51464, 855, 493, 13, 51508], "temperature": 0.0, "avg_logprob": -0.09995221555902717, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.002549978671595454}, {"id": 317, "seek": 115616, "start": 1156.16, "end": 1162.88, "text": " So even given this really basic example, I have found term GPT using GPT-4 to be", "tokens": [50364, 407, 754, 2212, 341, 534, 3875, 1365, 11, 286, 362, 1352, 1433, 26039, 51, 1228, 26039, 51, 12, 19, 281, 312, 50700], "temperature": 0.0, "avg_logprob": -0.09168809652328491, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.2017211765050888}, {"id": 318, "seek": 115616, "start": 1164.5600000000002, "end": 1165.3600000000001, "text": " really impressive.", "tokens": [50784, 534, 8992, 13, 50824], "temperature": 0.0, "avg_logprob": -0.09168809652328491, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.2017211765050888}, {"id": 319, "seek": 115616, "start": 1165.3600000000001, "end": 1168.48, "text": " It really hasn't let me down on any task yet.", "tokens": [50824, 467, 534, 6132, 380, 718, 385, 760, 322, 604, 5633, 1939, 13, 50980], "temperature": 0.0, "avg_logprob": -0.09168809652328491, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.2017211765050888}, {"id": 320, "seek": 115616, "start": 1169.44, "end": 1174.96, "text": " Certainly, there's really nothing that GPT-4 could do for me via the chat UI that I haven't", "tokens": [51028, 16628, 11, 456, 311, 534, 1825, 300, 26039, 51, 12, 19, 727, 360, 337, 385, 5766, 264, 5081, 15682, 300, 286, 2378, 380, 51304], "temperature": 0.0, "avg_logprob": -0.09168809652328491, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.2017211765050888}, {"id": 321, "seek": 115616, "start": 1174.96, "end": 1177.2, "text": " been able to just do via term GPT.", "tokens": [51304, 668, 1075, 281, 445, 360, 5766, 1433, 26039, 51, 13, 51416], "temperature": 0.0, "avg_logprob": -0.09168809652328491, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.2017211765050888}, {"id": 322, "seek": 115616, "start": 1178.0, "end": 1180.0800000000002, "text": " And it's just been so much easier.", "tokens": [51456, 400, 309, 311, 445, 668, 370, 709, 3571, 13, 51560], "temperature": 0.0, "avg_logprob": -0.09168809652328491, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.2017211765050888}, {"id": 323, "seek": 115616, "start": 1180.0800000000002, "end": 1183.3600000000001, "text": " And again, I made this really for myself and I'm just sharing it with people.", "tokens": [51560, 400, 797, 11, 286, 1027, 341, 534, 337, 2059, 293, 286, 478, 445, 5414, 309, 365, 561, 13, 51724], "temperature": 0.0, "avg_logprob": -0.09168809652328491, "compression_ratio": 1.5909090909090908, "no_speech_prob": 0.2017211765050888}, {"id": 324, "seek": 118336, "start": 1184.32, "end": 1186.4799999999998, "text": " And it's for sure a work in progress.", "tokens": [50412, 400, 309, 311, 337, 988, 257, 589, 294, 4205, 13, 50520], "temperature": 0.0, "avg_logprob": -0.13253246886389597, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.002631107810884714}, {"id": 325, "seek": 118336, "start": 1187.1999999999998, "end": 1193.28, "text": " There are some relatively new open source large language models that I'd really like to see", "tokens": [50556, 821, 366, 512, 7226, 777, 1269, 4009, 2416, 2856, 5245, 300, 286, 1116, 534, 411, 281, 536, 50860], "temperature": 0.0, "avg_logprob": -0.13253246886389597, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.002631107810884714}, {"id": 326, "seek": 118336, "start": 1193.28, "end": 1194.9599999999998, "text": " can they fill in this gap.", "tokens": [50860, 393, 436, 2836, 294, 341, 7417, 13, 50944], "temperature": 0.0, "avg_logprob": -0.13253246886389597, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.002631107810884714}, {"id": 327, "seek": 118336, "start": 1194.9599999999998, "end": 1199.4399999999998, "text": " Because like I said, even chat GPT is weak-ish at this.", "tokens": [50944, 1436, 411, 286, 848, 11, 754, 5081, 26039, 51, 307, 5336, 12, 742, 412, 341, 13, 51168], "temperature": 0.0, "avg_logprob": -0.13253246886389597, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.002631107810884714}, {"id": 328, "seek": 118336, "start": 1199.4399999999998, "end": 1206.1599999999999, "text": " Again, I think I could come up with prompts that chat GPT or GPT-3.5 rather could be successful with.", "tokens": [51168, 3764, 11, 286, 519, 286, 727, 808, 493, 365, 41095, 300, 5081, 26039, 51, 420, 26039, 51, 12, 18, 13, 20, 2831, 727, 312, 4406, 365, 13, 51504], "temperature": 0.0, "avg_logprob": -0.13253246886389597, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.002631107810884714}, {"id": 329, "seek": 118336, "start": 1207.12, "end": 1210.7199999999998, "text": " So I just, I don't want to put in a bunch of time making chat GPT work.", "tokens": [51552, 407, 286, 445, 11, 286, 500, 380, 528, 281, 829, 294, 257, 3840, 295, 565, 1455, 5081, 26039, 51, 589, 13, 51732], "temperature": 0.0, "avg_logprob": -0.13253246886389597, "compression_ratio": 1.5256916996047432, "no_speech_prob": 0.002631107810884714}, {"id": 330, "seek": 121072, "start": 1210.8, "end": 1215.28, "text": " I'd rather it be some sort of open source model with a permissive license.", "tokens": [50368, 286, 1116, 2831, 309, 312, 512, 1333, 295, 1269, 4009, 2316, 365, 257, 4784, 891, 488, 10476, 13, 50592], "temperature": 0.0, "avg_logprob": -0.08645386658897695, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.022967856377363205}, {"id": 331, "seek": 121072, "start": 1215.28, "end": 1217.6000000000001, "text": " So anyway, stay tuned for that.", "tokens": [50592, 407, 4033, 11, 1754, 10870, 337, 300, 13, 50708], "temperature": 0.0, "avg_logprob": -0.08645386658897695, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.022967856377363205}, {"id": 332, "seek": 121072, "start": 1217.6000000000001, "end": 1219.68, "text": " And again, I will put everything up on GitHub.", "tokens": [50708, 400, 797, 11, 286, 486, 829, 1203, 493, 322, 23331, 13, 50812], "temperature": 0.0, "avg_logprob": -0.08645386658897695, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.022967856377363205}, {"id": 333, "seek": 121072, "start": 1220.4, "end": 1226.0, "text": " Feel free to make a fork or whatever or make pull requests and just kind of tinker with this.", "tokens": [50848, 14113, 1737, 281, 652, 257, 17716, 420, 2035, 420, 652, 2235, 12475, 293, 445, 733, 295, 256, 40467, 365, 341, 13, 51128], "temperature": 0.0, "avg_logprob": -0.08645386658897695, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.022967856377363205}, {"id": 334, "seek": 121072, "start": 1226.0, "end": 1227.1200000000001, "text": " I think this is kind of cool.", "tokens": [51128, 286, 519, 341, 307, 733, 295, 1627, 13, 51184], "temperature": 0.0, "avg_logprob": -0.08645386658897695, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.022967856377363205}, {"id": 335, "seek": 121072, "start": 1227.1200000000001, "end": 1229.44, "text": " I like the idea of going back and forth like this.", "tokens": [51184, 286, 411, 264, 1558, 295, 516, 646, 293, 5220, 411, 341, 13, 51300], "temperature": 0.0, "avg_logprob": -0.08645386658897695, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.022967856377363205}, {"id": 336, "seek": 121072, "start": 1229.44, "end": 1236.16, "text": " And just doing the R&D with GPT-4 but just removing all of that extra effort.", "tokens": [51300, 400, 445, 884, 264, 497, 5, 35, 365, 26039, 51, 12, 19, 457, 445, 12720, 439, 295, 300, 2857, 4630, 13, 51636], "temperature": 0.0, "avg_logprob": -0.08645386658897695, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.022967856377363205}, {"id": 337, "seek": 121072, "start": 1236.16, "end": 1238.08, "text": " It's been, it's been really fun using it.", "tokens": [51636, 467, 311, 668, 11, 309, 311, 668, 534, 1019, 1228, 309, 13, 51732], "temperature": 0.0, "avg_logprob": -0.08645386658897695, "compression_ratio": 1.5830388692579505, "no_speech_prob": 0.022967856377363205}, {"id": 338, "seek": 123808, "start": 1238.08, "end": 1240.56, "text": " So anyway, hopefully you guys will enjoy it.", "tokens": [50364, 407, 4033, 11, 4696, 291, 1074, 486, 2103, 309, 13, 50488], "temperature": 0.0, "avg_logprob": -0.10111758246350644, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.0071206591092050076}, {"id": 339, "seek": 123808, "start": 1240.56, "end": 1244.8799999999999, "text": " If you've got questions, comments, concerns, whatever suggestions, feel free to leave them below.", "tokens": [50488, 759, 291, 600, 658, 1651, 11, 3053, 11, 7389, 11, 2035, 13396, 11, 841, 1737, 281, 1856, 552, 2507, 13, 50704], "temperature": 0.0, "avg_logprob": -0.10111758246350644, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.0071206591092050076}, {"id": 340, "seek": 123808, "start": 1244.8799999999999, "end": 1248.08, "text": " Otherwise, I will see you all in another video.", "tokens": [50704, 10328, 11, 286, 486, 536, 291, 439, 294, 1071, 960, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10111758246350644, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.0071206591092050076}, {"id": 341, "seek": 123808, "start": 1248.08, "end": 1252.6399999999999, "text": " If you've ever wondered and wanted to know more about how neural networks actually work,", "tokens": [50864, 759, 291, 600, 1562, 17055, 293, 1415, 281, 458, 544, 466, 577, 18161, 9590, 767, 589, 11, 51092], "temperature": 0.0, "avg_logprob": -0.10111758246350644, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.0071206591092050076}, {"id": 342, "seek": 123808, "start": 1252.6399999999999, "end": 1257.6, "text": " including the optimization and fitment, rather than just simply calling some method,", "tokens": [51092, 3009, 264, 19618, 293, 3318, 518, 11, 2831, 813, 445, 2935, 5141, 512, 3170, 11, 51340], "temperature": 0.0, "avg_logprob": -0.10111758246350644, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.0071206591092050076}, {"id": 343, "seek": 123808, "start": 1257.6, "end": 1261.4399999999998, "text": " then you might be interested in checking out the neural networks from scratch book", "tokens": [51340, 550, 291, 1062, 312, 3102, 294, 8568, 484, 264, 18161, 9590, 490, 8459, 1446, 51532], "temperature": 0.0, "avg_logprob": -0.10111758246350644, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.0071206591092050076}, {"id": 344, "seek": 123808, "start": 1261.4399999999998, "end": 1263.28, "text": " by myself and Daniel Kukiawa.", "tokens": [51532, 538, 2059, 293, 8033, 591, 2034, 654, 4151, 13, 51624], "temperature": 0.0, "avg_logprob": -0.10111758246350644, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.0071206591092050076}, {"id": 345, "seek": 123808, "start": 1263.28, "end": 1267.6799999999998, "text": " The book can be had in the form of an ebook PDF, softcover or hardcover.", "tokens": [51624, 440, 1446, 393, 312, 632, 294, 264, 1254, 295, 364, 308, 2939, 17752, 11, 2787, 12516, 420, 1152, 12516, 13, 51844], "temperature": 0.0, "avg_logprob": -0.10111758246350644, "compression_ratio": 1.6566265060240963, "no_speech_prob": 0.0071206591092050076}, {"id": 346, "seek": 126768, "start": 1267.68, "end": 1270.0, "text": " And we ship for free worldwide.", "tokens": [50364, 400, 321, 5374, 337, 1737, 13485, 13, 50480], "temperature": 0.0, "avg_logprob": -0.05848331544913497, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0025505146477371454}, {"id": 347, "seek": 126768, "start": 1270.5600000000002, "end": 1273.8400000000001, "text": " Also, the physical books just come with an ebook copy.", "tokens": [50508, 2743, 11, 264, 4001, 3642, 445, 808, 365, 364, 308, 2939, 5055, 13, 50672], "temperature": 0.0, "avg_logprob": -0.05848331544913497, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0025505146477371454}, {"id": 348, "seek": 126768, "start": 1273.8400000000001, "end": 1278.5600000000002, "text": " All copies are in full color, which helps because there's a lot of code syntax highlighting", "tokens": [50672, 1057, 14341, 366, 294, 1577, 2017, 11, 597, 3665, 570, 456, 311, 257, 688, 295, 3089, 28431, 26551, 50908], "temperature": 0.0, "avg_logprob": -0.05848331544913497, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0025505146477371454}, {"id": 349, "seek": 126768, "start": 1278.5600000000002, "end": 1281.52, "text": " and lots of charts and diagrams to help convey the principles.", "tokens": [50908, 293, 3195, 295, 17767, 293, 36709, 281, 854, 16965, 264, 9156, 13, 51056], "temperature": 0.0, "avg_logprob": -0.05848331544913497, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0025505146477371454}, {"id": 350, "seek": 126768, "start": 1282.0800000000002, "end": 1286.64, "text": " Also, almost all of those charts and diagrams have QR codes and links that take you to", "tokens": [51084, 2743, 11, 1920, 439, 295, 729, 17767, 293, 36709, 362, 32784, 14211, 293, 6123, 300, 747, 291, 281, 51312], "temperature": 0.0, "avg_logprob": -0.05848331544913497, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0025505146477371454}, {"id": 351, "seek": 126768, "start": 1286.64, "end": 1289.2, "text": " animations to help further illustrate the concepts.", "tokens": [51312, 22868, 281, 854, 3052, 23221, 264, 10392, 13, 51440], "temperature": 0.0, "avg_logprob": -0.05848331544913497, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0025505146477371454}, {"id": 352, "seek": 126768, "start": 1290.0800000000002, "end": 1294.4, "text": " This is truly a real neural networks from scratch,", "tokens": [51484, 639, 307, 4908, 257, 957, 18161, 9590, 490, 8459, 11, 51700], "temperature": 0.0, "avg_logprob": -0.05848331544913497, "compression_ratio": 1.6450381679389312, "no_speech_prob": 0.0025505146477371454}, {"id": 353, "seek": 129440, "start": 1294.4, "end": 1299.1200000000001, "text": " teaching everything from the forward pass calculation of laws, back propagation and", "tokens": [50364, 4571, 1203, 490, 264, 2128, 1320, 17108, 295, 6064, 11, 646, 38377, 293, 50600], "temperature": 0.0, "avg_logprob": -0.10734243052346366, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.07583524286746979}, {"id": 354, "seek": 129440, "start": 1299.1200000000001, "end": 1299.8400000000001, "text": " optimization.", "tokens": [50600, 19618, 13, 50636], "temperature": 0.0, "avg_logprob": -0.10734243052346366, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.07583524286746979}, {"id": 355, "seek": 129440, "start": 1299.8400000000001, "end": 1303.68, "text": " The only math that you're expected to know coming in is basic algebra.", "tokens": [50636, 440, 787, 5221, 300, 291, 434, 5176, 281, 458, 1348, 294, 307, 3875, 21989, 13, 50828], "temperature": 0.0, "avg_logprob": -0.10734243052346366, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.07583524286746979}, {"id": 356, "seek": 129440, "start": 1303.68, "end": 1306.88, "text": " The rest is taught by us in the book, step by step.", "tokens": [50828, 440, 1472, 307, 5928, 538, 505, 294, 264, 1446, 11, 1823, 538, 1823, 13, 50988], "temperature": 0.0, "avg_logprob": -0.10734243052346366, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.07583524286746979}, {"id": 357, "seek": 129440, "start": 1306.88, "end": 1310.8000000000002, "text": " Everything is shown and explained in the book first logically, then mathematically,", "tokens": [50988, 5471, 307, 4898, 293, 8825, 294, 264, 1446, 700, 38887, 11, 550, 44003, 11, 51184], "temperature": 0.0, "avg_logprob": -0.10734243052346366, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.07583524286746979}, {"id": 358, "seek": 129440, "start": 1310.8000000000002, "end": 1314.4, "text": " then via raw Python code, no third party libraries.", "tokens": [51184, 550, 5766, 8936, 15329, 3089, 11, 572, 2636, 3595, 15148, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10734243052346366, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.07583524286746979}, {"id": 359, "seek": 129440, "start": 1314.4, "end": 1316.88, "text": " And then finally optimized via NumPy.", "tokens": [51364, 400, 550, 2721, 26941, 5766, 22592, 47, 88, 13, 51488], "temperature": 0.0, "avg_logprob": -0.10734243052346366, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.07583524286746979}, {"id": 360, "seek": 129440, "start": 1316.88, "end": 1320.96, "text": " And this is for every step of the way, building and training actual neural networks", "tokens": [51488, 400, 341, 307, 337, 633, 1823, 295, 264, 636, 11, 2390, 293, 3097, 3539, 18161, 9590, 51692], "temperature": 0.0, "avg_logprob": -0.10734243052346366, "compression_ratio": 1.6830985915492958, "no_speech_prob": 0.07583524286746979}, {"id": 361, "seek": 132096, "start": 1320.96, "end": 1326.4, "text": " for a fully fundamental understanding of neural networks and how they work from scratch.", "tokens": [50364, 337, 257, 4498, 8088, 3701, 295, 18161, 9590, 293, 577, 436, 589, 490, 8459, 13, 50636], "temperature": 0.0, "avg_logprob": -0.0556704817699785, "compression_ratio": 1.6221498371335505, "no_speech_prob": 0.007120658177882433}, {"id": 362, "seek": 132096, "start": 1326.4, "end": 1331.3600000000001, "text": " If at any point you're lost or confused, all copies of the book also grant access to a Google", "tokens": [50636, 759, 412, 604, 935, 291, 434, 2731, 420, 9019, 11, 439, 14341, 295, 264, 1446, 611, 6386, 2105, 281, 257, 3329, 50884], "temperature": 0.0, "avg_logprob": -0.0556704817699785, "compression_ratio": 1.6221498371335505, "no_speech_prob": 0.007120658177882433}, {"id": 363, "seek": 132096, "start": 1331.3600000000001, "end": 1337.6000000000001, "text": " Docs version of the book, where you can ask your questions in line with the actual text itself.", "tokens": [50884, 16024, 82, 3037, 295, 264, 1446, 11, 689, 291, 393, 1029, 428, 1651, 294, 1622, 365, 264, 3539, 2487, 2564, 13, 51196], "temperature": 0.0, "avg_logprob": -0.0556704817699785, "compression_ratio": 1.6221498371335505, "no_speech_prob": 0.007120658177882433}, {"id": 364, "seek": 132096, "start": 1337.6000000000001, "end": 1341.2, "text": " This is an incredible project that I'm extremely proud of to share with you all.", "tokens": [51196, 639, 307, 364, 4651, 1716, 300, 286, 478, 4664, 4570, 295, 281, 2073, 365, 291, 439, 13, 51376], "temperature": 0.0, "avg_logprob": -0.0556704817699785, "compression_ratio": 1.6221498371335505, "no_speech_prob": 0.007120658177882433}, {"id": 365, "seek": 132096, "start": 1341.2, "end": 1345.1200000000001, "text": " We've sold over 13,000 copies to students all over the world.", "tokens": [51376, 492, 600, 3718, 670, 3705, 11, 1360, 14341, 281, 1731, 439, 670, 264, 1002, 13, 51572], "temperature": 0.0, "avg_logprob": -0.0556704817699785, "compression_ratio": 1.6221498371335505, "no_speech_prob": 0.007120658177882433}, {"id": 366, "seek": 132096, "start": 1345.1200000000001, "end": 1347.68, "text": " If you're looking to take your knowledge of deep learning to the next level,", "tokens": [51572, 759, 291, 434, 1237, 281, 747, 428, 3601, 295, 2452, 2539, 281, 264, 958, 1496, 11, 51700], "temperature": 0.0, "avg_logprob": -0.0556704817699785, "compression_ratio": 1.6221498371335505, "no_speech_prob": 0.007120658177882433}, {"id": 367, "seek": 134768, "start": 1347.68, "end": 1352.4, "text": " or if you're just looking to start that journey, I can't imagine a better way.", "tokens": [50364, 420, 498, 291, 434, 445, 1237, 281, 722, 300, 4671, 11, 286, 393, 380, 3811, 257, 1101, 636, 13, 50600], "temperature": 0.0, "avg_logprob": -0.09806482358412309, "compression_ratio": 1.25, "no_speech_prob": 0.19670188426971436}, {"id": 368, "seek": 134768, "start": 1352.4, "end": 1358.72, "text": " So to learn more and buy yourself a copy, you can head to nnfs.io.", "tokens": [50600, 407, 281, 1466, 544, 293, 2256, 1803, 257, 5055, 11, 291, 393, 1378, 281, 297, 77, 16883, 13, 1004, 13, 50916], "temperature": 0.0, "avg_logprob": -0.09806482358412309, "compression_ratio": 1.25, "no_speech_prob": 0.19670188426971436}], "language": "en"}