{"text": " Llywbeth hon am wneud ym mwyn sicrhau hynny ac mae'r ydych\u6df7wyl sy'n cynnig mewn gw'r dym \u0442\u0430\u043a\u0443\u044e rhoi riannol? Mewn gwneud o'c flyny y plwydio yn rhai ddiddio'r cyfnodau, Beth lle mae am gwaeth hyn a chi v\u00e9liwr ja Prud Follow gwelwch. Dolwedd yn geist startoeth\uc624\u0435\u0447 arall yn i ballrain arall, lle arall yn gweithio'r Llywodraeth. rhoi\u56de\u4f86 i'w gyddi'w gwybodaeth honom i ... Almighty Mike, tewi ydy gyrygau\u2026 \u2026 dyma ar-dysgu! Fy enw i, i\ufffd\ufffd\ufffdch cicitYEalsoch\u2026 \u2026 yma amary teuluw... \u2026 i'r ffordd ydych Lady Worker... \u2026 Trunwau ystafelu'n bob gynyddio... \u2026 Mae'n eu clynyddu i'r ddelig, yn ddefnyddioSeePlayer... \u2026 Neil yma am ar-zell, amaeddrannu... Ac rydw i fyny mis o bwyfia'r amser, nad maen nhw gwybod gweithre vaccinations, rydw i'n amser ar gwell feed\u00eactig Sa\u600e\u4e48\u6837au? Rydw i'n mir enghreithio y cael meddwl ei m \u0441\u043b\u0435\u0434di habiloghau ar reside\u1ed9ld, ph\u756a dod yn gynghreithi a wyloach amoduslu ar draw. i'w ddweud i'w ddweud i'w ddweud. So, this is an overview of what we are going to go through. I am going to introduce the notion that you are the free energy principle, but from a, using a slightly heuristic approach in terms of action and the path of least resistance, highlighting the importance of having internal models or hypotheses that enable us to generate predictions, talking about active inference. And one key thing that I am going to focus on is translating the theory into a process theory that can be used to understand neuronal message passing in the brain and help us exactly constrain the sorts of experiments that Jim was talking about. So, that is going to be a big part of what I hope that we will be talking about, taking normative principles and seeing how they unpack in the service of understanding empirical measurements anatomy and physiology and how they can be used to nuance experimental design, showing the sorts of things that one can simulate and speaking to some empirical predictions of these sorts of schemes. And I have put, as an epilogue, more recent work, simulations of reading that introduce hierarchies into the particular forms of generative models that I want to survey for you. We won't have time to go without that, but I just want to show you the slides in case of something that catches your attention. So, I'm going to start with a question. Let's assume you're hungry, and let's assume you're an owl. So, what are you going to do? You're going to search for a mouse? And how are you going to do that? Don't cheat. You have to look at me, not at me. Absolutely. Perfect answer. So, in terms of optimal behaviour, the first thing you do is search. You scan. You confront the epistemics of reducing uncertainty about what you need to do in order to fulfil your goal. So, it's all about beliefs. So, in that answer is the basis of everything that I'm going to say. Your behaviour is always driven by beliefs, and that tells us something quite important. So, here's you scanning and searching, and you've found a little mouse that you might want to eat there. That's quite important because it speaks to two basic classes of ways of thinking about optimising behaviour. You can either imagine that there is some value function of the next state that will be brought about by some action, and optimise that action by selecting the action that maximises the value of the next state. That's the classical way of doing it, but that just doesn't work if the best next thing to do is to search and resolve uncertainty. Because uncertainty is an attribute of beliefs. Therefore, the function of a function that you need to optimise in terms of action you hear is a function of beliefs, which I'm deleting by Q, beliefs about the states of the world. That introduces a fundamental distinction between the sorts of schemes that you bring to bear in terms of understanding optimal behaviour. The other thing about the scanning and searching answer is that action depends upon beliefs about the world, states of the world, and subsequent actions. So, not only is it a function of beliefs about the world, but it's the order in which you interrogate that world. So, it makes a difference whether you search, then eat, as opposed to eat, then search. That means that we are in the game of optimising sequences or policies or actions. I'm going to call it a sequence of actions policy. So, what that means from the point of view technically of what sort of thing we have to optimise, it's a functional of a belief integrated over time, or summed over time, a path integral. If we call that an energy, then the integral, the path integral of an energy is called an action. So, what we've just said is that we've reduced the problem of good behaviour to Hamilton's principle of least action, where action is the path integral or the trajectory integral or the sum over an energy functional of beliefs. And that's the basic premise that I'm going to pursue. Just to highlight the distinction, if you subscribe to this way of thinking about how systems work, then you end up with optimal control theory, Bayesian decision theory, reinforcement learning and all that good stuff. Conversely, if you believe this is how biological systems work, then you end up essentially with Hamilton's principle of least action, the free energy principle, active inference, active learning and so on. And that's what we're going to focus on. And the energy function that I'm going to consider, we've already heard mentioned, is the variational free energy or the free energy, which we've already heard very roughly scores surprise. It approximates surprise or suprisal and is simplifying assumptions prediction error. So, what we are saying is that we're just in the game of minimising prediction error and more specifically prediction error over time over sequences of behaviour. I'm going to quickly go through this because there are lots of interesting connections with existing theories and formulations. This is a bit technical. These are both iconic and ironic equations. You'll hear more about those later on. In words, if it's the case that good agents, good people, minimise their free energy, their surprise, their average surprise and their uncertainty, then they must believe that the actions that they emit will minimise expected free energy. You can write that down very simply in terms of these belief functions here and rearrange them in a way that discloses important links with lots of established formal treatments of behaviour. I've written the expected free energy associated with any particular policy in terms of its expinsic value here and its epistemic value. Basically, these things store the surprise about what you predict will happen under a particular behaviour and what you think should happen. Your preference is like, I'm going to eat a mouse and I'm not going to be hungry. That's a surprise bit, explicit or expinsic surprise bit. There's another sort of average surprise or relative entropy which is called epistemic value. It's a reduction in uncertainty or the information gain, and that's the key bit. It's the epistemic which is missing from classic theories but is part of this formulation of Hamilton's principle of least action. That relates very closely to theories of visual salience, of Bayesian surprise. Technically, Bayesian surprise is the divergence of the difference between a prior belief and a posterior belief or a posterior belief to be informed by observations here. What we're saying is that we will choose to act in a way that reduces our uncertainty relative to prior beliefs, looking at data which gives us information that maximally reduces that uncertainty that has the greatest epistemic value or Bayesian surprise. In fact, mathematically, that's exactly the same as the mutual information between the causes, the hidden states of the world S and the consequences, the outcomes that we actually observe. So another way of saying this is that we are subscribing to the principle of maximum information, mutual information or minimum redundancy or maximum information efficiency of the sort articulated by Horace Barlow. Always of expressing one particular form of perspective on this underlying functional. Another way of thinking about this in the case, if there is no ambiguity, if we actually can observe the states directly, then we can discount this uncertainty term here. And what we're left with is something called KL control, which is the state of the art of what people would use in optimal control theory and dynamical systems control. In economics, it's called risk sensitive control. It's minimizing risk. So this is a surprise between what I think will happen and what I want to happen. And if what I think will happen is surprising relation to what I thought was going to happen, then I have a high degree of surprise, a high degree of risk, and I want to minimize that. And then finally, if there's no ambiguity or there's no risk, then we reduce to classical expected utility theory or all the sorts of theories that reinforcement depends upon this maximizing our preferred outcomes there. So, clearly, in order to be surprised, we have to have predictions against which we can match outcomes to score that surprise. And this brings us to generative models. And the departure that I promised you from what people currently understand in terms of predictive coding and what I'm going to talk about for the next few minutes is I'm going to formulate generative models not for continuous state space of the sorts used in predictive coding of, say, visual angles or content or acoustics, but generative models in which we can label the entire world in terms of a number of discrete states. So these are generative models for discrete state spaces, and they don't normally have the look and feel of predictive coding, but my story will be, is in fact they do. They are actually formally very, very similar to the sorts of schemes that we understand in terms of top-down predictions and bottom-up prediction errors in hierarchical message-passing allopredicative coding in the visual cortex. So in these models, all we have, this is not, ignore the equations, but it's focused on this graphical model here. What we're saying is that the world unfolds in one of many, many states, and the transitions from one state of the world to the next state of the world are encoded by probability transitions that themselves depend upon how we act. They depend upon the policies that we choose, and we have a certain confidence in those policies, denoted by their precision or inverse temperature beta here. So if we knew the probability transitions or the transitions from time to time of the states, we can generate a sequence or trajectory of states, and each state, at each point in time, generates an outcome through this likelihood of matrix A. And that's it. That's the generative model. The world has states, they unfold, and each state generates an outcome that's observable. And that's the basis of everything else that I'm going to say. If I'm now given a generative model, what I can do is I can evaluate the free energy of my beliefs under that generative model, and I can then minimize everything with respect to that proxy for surprise or uncertainty, namely the expected free energy. And I can write down equations or solutions that tell me how an optimal agent person would behave in a sort of Bayesian sense. And these are the solutions to the equations expressed in terms of the parameters of that model. So A was this mapping from states of the world to outcomes, and B was the mapping between subsequent hidden states. And despite the complicated nature of the equations on the previous slide, the actual updates, the solutions are incredibly simple. And furthermore, they look very much like the sorts of things that the brain does. So, for example, expected states of the world are a nonlinear sigmoid function of linear mixtures of expected states of the world and observations. So we're mixing together evidence from outcomes and our beliefs about the states of the world to update our beliefs about the current state of the world. Our beliefs about what we're going to do next, our policy pie here, is this a softmax function of the expected free energy weighted by an inverse temperature parameter that you will see we associate with dopamine, a classical softmax response rule. If you're not familiar with that, that's what people in economics and choice behaviour use for those people who deal more with perception. We also have a model of incentive salience. The confidence or the precision or the inverse temperature associated with our beliefs about action now becomes, as a Bayes optimal solution, that depends upon the goodness of a policy or the negative goodness, the expected free energy here. And the form of these equations speaks to a rough anatomy of computations in the brain, a computational anatomy. And it sort of goes like this, where we have these equations dictate what each update needs to know about the other updates. So, basically, it prescribes a connectome for the exchange of information or sufficient statistics that is implied by placing the Hamilton's principle of least action on the simplest sort of generating model that you can imagine. And that's the sort of anatomy we have here. Outcomes, expected states, expected policies, the goodness or the expected free energy of policies, the precision of policies, states in the future, which prescribe action. So, I won't go through that, but I'll just give you a more heuristic version of that one. So, what those equations tell us. So, this is like a very top-down argument. It's not, you know, let's think about how the brain works and come up with some hypotheses. This unfolds or unravels from, impacts from, just applying Hamilton's principle of least action to a very simple generating model. And what it tells us is that sensory input comes in, say, at the back of the brain. It informs and updates expectations about hidden states of the world, sometimes referred to as state estimation. They are associated with a free energy or a surprise that is combined with an evaluation of those states in relation to prior preferences and their potential reduction of uncertainty, their epistemic value. They are combined to give us beliefs about the policy that we are currently pursuing. We have a certain confidence in that policy. And then those policies are used to weight all the different states conditioned upon what we are currently doing to give us the best estimate of what's going to happen next, the next state of the world. And if we know that, then we can choose the action that brings about, that realises our expectations, our predictions about the next state of the world, that action solicits a new observation from the environment and the cycle begins again. So, we have a perception action cycle that falls out of the minimisation scheme that we've just been talking about. So, very briefly, I'm just going to show you how that sort of thing works with a series of examples, and then hopefully I'll turn it over to you to see what you want to talk about. The first example is just a very simple simulation of foraging in a two-arm maze. So, in this example, there's a little rat here, and there are rewards on the right and the left arms of the maze, but the rat doesn't know where the reward is. There's also an informative queue at the bottom of the maze here, and if it went to solicit that queue, it would then know where the reward was, and it could only make two moves. So, it can either take a chance and go to one of the other top arms, or it can be a bit more clever and resolve any uncertainty about the context it's currently operating in, which arm is baited, and go and retrieve the epistemic value of the informative queue and then make an informed decision. So, this is exactly the searching that you were talking about before, scaling your environment, knowing where you are, resolve your epistemic, solve the epistemic problem, and then turn to your prior preferences or your pragmatics. You can write this model down in very simple terms of these A and B matrices here. There's partial reinforcement here, and the C matrix here just denotes the preferences in terms of what sorts of states this rat thinks it should occupy, basically thinks it should be in the baited arm and not in the unbaited arm. That's all it's saying here, with minus threes and plus threes on the upper arms that are baited. And if we do that, and we just integrate those solutions that I told you before, we actually generate very realistic behaviour, summarised here in terms of the expected policy and the policies that this agent or this little animal can entertain. It stays there and then goes to one of the three arms, or it goes to one of the two arms, or it goes to the bottom and then goes to any of the three arms. So there are eight policies here. And what it does in the first instance is because it doesn't know where the reward is. It gets the cue and then obtains its reward. What we've done here is actually baited the left arm all the time. So slowly it accumulates evidence that, in fact, the reward's always on this side here. So as time goes on, it actually switches and learns, and it's probably better to avoid or dispense with the epistemic move and go directly to the reward. And it starts doing that after about 20 or 30 trials here, at which point its reaction times, and this is the actual floating point operations of the scheme, decrease. And because the goodness of a policy is this path integral, it's actually spent more time being rewarded. So if you like, the payoff also increases by going straight there. So this prescribes good policies, and it can be used to simulate nice behaviors of the sort you've seen experimentally. But what I want to do finally is just connect that to neurophysiology and neuroanatomy. But to do that, I have to have a process theory. I have to have a theory which says this particular neuroactivity or this particular connection strength corresponds to this quantity in the model. And I have to have a process in play that is neurarily plausible. And the way that we're going to do that is just take those update equations that we've seen before, and instead of just writing down the solutions mathematically, I'm going to recast the solutions in terms of a gradient descent or a hill climbing, or actually a hill descent here. So this is a standard way of optimizing something. If you've got a quantity you want to minimize, you just go downhill until it stops getting smaller. And if I do that, I can write down exactly the same scheme in terms of differential equations on expected states of the world. It's very similar form, but here that's a rate of change of activity, which is now a nonlinear function of linear mixtures of expectations about states of the world and the observations. And in doing that, I've created a dynamical system that now has as much closer to the look and feel of a neuronal system. And that now enables me to look at the dynamics that underlie the behavior. And these are the dynamics here, and we can basically break these into inference and state estimation in terms of the updates or the fluctuations in the states as new evidence comes along. Policy selection that we've already seen with our softmax response rule, and learning as we accumulate from trial to trial evidence about particular states or contingents of the world in this instance that the left hand arm of the maze was always baited. I illustrated those things here, a couple of interesting things to note. First of all, with every new move and every bit of new sensory information, there are lots of fluctuations in these states that look very much like an ERP. Furthermore, when we become a little bit more automatic or not habitual, but certainly going straight for our reward, there is an attenuation of these responses. The confidence, the precision in those responses also shows these phasic changes and progressive changes as we learn the context. So we actually get something which looks remarkably similar to transfer of dopamine responses as we become more familiar and more confident about the outcomes that we see. Let me just quickly show you a couple of those outcomes. This slide highlights just one trial, and it shows the representations of time over the different hidden states of the world. Just highlights a couple of things. First of all, it shows that as we accumulate evidence for our preferred policies or our preferred outcomes, the probability that we are in a state which we will ultimately choose increases whereas the probability of states that we don't decreases. This is formally identical to evidence accumulation or drift diffusion models, but now a consequence of a gradient descent on variational free energy or a bound for surprise. What we also see is an interesting dynamics in the sense that if information keeps coming in every, say, 250 milliseconds, like the frequency at which we go and sample the world with mechanic eye movements, that means that we have two timescales in play. One is a theta rhythm as we go and get information once, say, four times every second. But within each sampling there's this fast updating that's minimising and optimising our beliefs, and that faster updating has a temporal scale in the gamma range. So what we see is effectively, as we move along, fast updating that repeats itself every theta cycle, but as we accumulate more and more evidence we get more and more efficient and confident about the things that we are inferring. The dynamics mean that they accumulate evidence more quickly, more efficiently, and we get a phase procession of the sort seen in the hippocampus. I've already mentioned that as time goes on, by virtue of increasing our confidence as we assimilate this evidence, then that confidence is expressed in the confidence of our policies and we have a nice way of assimilating dopamine responses. We can look at the behaviour or the activity of these representations of different states of the world at different points in time during our policy, and if we plot their responses as a function of where the rat actually is, we can simulate place cell activity. There has many characteristics of the sort seen empirically. This just illustrates this theta-gamma coupling, which is an almost necessary consequence of this sort of solitary sampling of the world, and then updating bleeds quickly before the next sample comes along. Again, the sort of thing that one sees empirically. We can now do violation responses exactly as Jim was talking about. What I've shown here are the responses to two trials. They're identical in nature, but one is from the beginning of the trial where the rat was not familiar with its environment, and one is at the end of the trial where it becomes very familiar just before it starts going directly for the reward. Interesting, if we look at the representations of key states here, what we see is that they are much more efficient and therefore less exuberant updating of expectations of hidden states that if we subtract the standard familiar one from the odd ball or the unfamiliar one, we reproduce the temporal dynamics of things like the mismatch negativity in ERP research. We also demonstrate this transfer of confidence or simulated dopamine responses from the rewarded cue per se to this instructional condition stimulus here. I'm going from slightly negative to positive here. We can play similar games by introducing deliberate violations and illicit P300 responses. We can look at reinforcement learning by switching contingencies halfway through and look at the effects on dopamine-urgent responses and also electrophysiological responses. How long have I got? Oh, that's very good, isn't it? I've only been talking for 25 minutes. I can be true to my promise to finish in half an hour. This is the epilogue. That's the story so far. Most of that will be in the next few weeks in the published literature. You'll notice at the moment there's nothing really about hierarchies. Most people here, I'm sure, are more interested in the implications of this sort of theory for perceptual hierarchies and evidence of accumulation and purely perceptual domain. The more recent work that I wanted to, this is not published, to introduce you to, is now taking this formalism, which has a lot of constant validity in relation to choice behaviour and your economics, active vision, active sensing, and see what it has to say about the source of themes we're more interested in, which is the hierarchical message passing and the deep generative models that we assume. The brain is using to actually understand perceptual sequences, say. So this is the epilogue. Again, I'll just speed through this in five minutes. What we're going to do now is tell exactly the same story, but now we're going to put one of those discrete state-space models, they're known as Markov decision processes, on top of the first one and another one on top of that and another one on top of that. So in this construction, hidden states, at any one level in the model, don't generate outcomes, they generate the first or the initial hidden state of the level below. And then that cycles over a few iterations and then terminates like the rat-terminated when it entered the baited arms of the cues. And that process repeats hierarchically to any arbitrary depth. So what we have are deep temporal generative models. And they're really interesting because not only do they have a hierarchical structure in their form, but also in their time, because if the state at any high level is generating the initial state that must have subsequent states, then it means that the lower states unfold more quickly than the higher states. So one way of thinking about this is the generative model says that at this hour, at this minute and at this second, I am safe, I was reading, I'm on this page, on this paragraph and on this word. So if you think about the lower levels of us ticking over more quickly, like the second hand of a clock, and every revolution or every trajectory or every path they take, then the high level goes forward one step, and then it goes round again, it goes another step, another gain, sorry, again and then another step. And then that process is repeated. So as the minute hand is going round, once it goes round, then the hour hand goes round. So what we have here is a generative model that basically has in mind, literally, beliefs about the world that are much more protracted in time and are hierarchically nested. So if you could invert this sort of model, you would have a representation of the context and the context of context and the context of context. At each point, as you go deeper into the model, they are more temporally enduring. So you know that working from the top down, you'd know the story of the narrative, if you knew the story of the narrative, you'd be able to generate a particular sentence, if you could generate a particular sentence, if you could generate a particular word, if you could generate a particular word... .. you could generate a particular letter. All faster and faster and more elemental timescales. That's the sort of model now that people are starting to play with. It has exactly the same performance before\u2014 coping with policies in play that generate transitions among hidden states that generate outcomes, but now the first hidden state is generated by the same sort of model, formally identical, of a higher level. There are transitions over time here, but they are much slower. that by making these red lines upon red states here, a different colour from these because these, basically, are the same as these but they're re-used at a later point in time. And this sort of model now allows you to think more carefully about the hierarchical message passing and the implications of neuro-anatomy. So if we now turn straight to the process theory, these are the differential equations that fall out of that generatord model in the previous slide. ei wneud o unig lwydaeth pob ddechrau. Derbyn i ch risingdef o dystru i dystru i ddim yn cael ei ddif respir, egOC yn gyfrasch ychydig ar y totu org10-g \u062c\u0648\u0646 inequality. A dwi'n credu hynny bir maen i ddim yn ddigonol gyda L rug yn misoedd f hybridd stag yn y gwroad \u0442\u0440 s\u00edr. Til ond mythigr o'r ddim yn y ddigonor, a gan y doddoriaeth yn y tai nhw'r drefnio fan y plainachau. Diolch yn ddechrau am ddechrau coddyntau hynny a detch, ac rydw i addysgu'n chi fod rhywbeth rydyn ni'n ach yn perwhaith gweithio. Mae ydw mitant ar hufwn, felly dyma'n ddechrau ddiwrnod ni'n ddechrau Ond biggestonethol, er di particularly gw Snapod, yn ynghyd yn yr ymddangos lleethau hyn arall hynny, ddweud yma cwm declar o repliedol, ar m \uac8cisirol arall y dyfod, Golw llawer arall llawer mewn amser gan y cwmputatio nanallynol, y\u043a\u0438\u043c\u0438 yuyaeth yw Somehow carried a 84.5 Sauce Ndun, iawn i' fi bwyzaid eu droslawn f\u03c6m. I airportio pleid seg\u00fan fill. Er si GLORIA cyhoedd, Assembly Down yw dweud yn gyda hynny yng Nghymru a'i sefydlu'r rhywbeth gyda'r berth dweud, er mwyafMatthewch szeynerrym yn\u0442\u044b grandiaf minzysolEl Mae wedi'i converti analw!!!!!! A \ub418fnodd ar yr adegowadau mewn gindig adegowadauthat atbypaenter, a gennych amherei A ddau roi gwodd!. Fmarfin analw Ring ac mewn gwir y c\u0438\u0442\u0435ch, dw i ddim yn gwneud o wnes cy hunain Dasodd yr adegowadau i gwenallu cais grathio adegowadau Mae\u043b\u044e\u0447 ychydig iddo nhw a \uc120urdwy Rydyn ni gadael sy'n fioedd y gwaith ma wedi ar hyn o gwbl panallu, a hyffordda. Mae'n gwneud ond yn gennaledd, wedi a cofdown o'i gael niferion, llaoi i am\u03b9\u03bf overseas ychydig wedi ai'r hyn am misgfemi seisin ag llwy na'ch wneud ar hyn o hyd llwyddi, i hwnna, gael dd sel m sorcera iawn i'r Fyged growth Yw'r Oorferd Folaid theoriaeth mewn cyd y fry Jaime Meddor C ar y rueth Cenaiswn ni'n samp lun prosg uppu pho t Momo astad i'n ein bachMyw yma ar gyfnodd Rhaid Fygedr Diolch ar gyfer hwn i'r Ro Virus Fog \u0442\u043e\u043bwy... ...ynghyd gy ved Cynyd rel Fygedr honw i'r fnod i'n tufyniadhe iddopeth yma y gweithiau neu yn nar\u771f Curioroedd ar gyfer rhowch \u00e2 boffraedd y pwyntill ddoseud o effaith yi'r Llyfr yma rydym hanfodd d communism dweud yn ddeud democrat. Rydyn ni'n ei chylygu diwethaf y gallwllais liquids mae yw'r eff cheatuppau pleannam ar y liar mewn ilmenau Mae nid yn ein cael cyn deafnaetennu am y chylynadau fossemwjent hynny ac mae'n!] If you subscribe to this scheme anatomically and the theory being a metaphor for neuronal activity in the brain, what you'd end up claiming is that the goodness of a policy, is negative expected for energy, is encoded in the is encoded in the call date for high-level, more abstract representations in the deep-generative model, for more intermediate levels, sorry, the call date for intermediate levels for more abstract ones in the globus pallidum, and in the putamen for motor loops, whereas the policy expectations per se have here been assigned again to the globus pallidus internum. So just a way of getting from the mathematical anatomy to the biological or neuroanatomy in a purely top-down dispassionate mathematically dry way, is taking the equations and seeing what form do they imply for message-passing and what sorts of message-passing do we see in the real brain. Andre Bastos will, I think he may not, but he did a lot of work on this sort of intrinsic connectivity within a macro-column, clinical micro-circuits for predictive coding, exactly the same game can be played here for this discrete state-space model, and that's one key exception which Lars might like, because there's been a lot of debate in Scotland about whether the superficial parameter cells encode predictions or expectations or prediction errors. In this scheme, in this discrete state-space scheme, the superficial parameter cells code expectations, not prediction errors, and you may ask why. Well, the reason is, they do encode prediction errors, but they do it by physically in a very different way, and that follows from the form of this differential equation here, where we're expressing the states as a sigmoid function, a softmax operator, on V, which we associate with depolarisation, where the rate of change of depolarisation or voltage is proportional to the error. So, the error from the point of view of a neural mass model now becomes the conductance. So, the cells are encoding prediction error, but the error is in the conductance. So, when all the postsynaptic drives to the conductance postsynaptically are in balance and there is no further change or drive to the potential, that means prediction errors are zero. So, when the cell or a population has reached electrodynamically steady state, it's found its minimum prediction error, and then it fires, but it is the firing here that is associated with the expected states of the world here. So, that's an interesting thing which I thought might be useful for discussion in terms of reconciling a lot of paradoxes about what's been passed forward, and would you expect it to be expended away or would you expect it to be boosted, sharpened, all sorts of interesting issues here. Closing with an example of reading, a deep hierarchical model where we have beliefs about six sentences, each comprising four words. Each word here has iconic letters that can either be in an uppercase or a lowercase, a palindromic in the sense that it doesn't matter whether the cat has to flee from the cat, it doesn't matter whether we flip them in a horizontal way, it still means the same thing, but it does, this agent is surprised if we use a lowercase. Three words. Let me just skip through this because we want to spend more time in discussion if we can. So, that's just a generative model with two levels, semantics or sentence structure, word structure and outcomes generating particular, if you like, letters, but there are icons in this instance. And then with this scheme, we can simulate things like reading. So, here's a little four page story or sentence and that word is flee, that word is wait because there's nothing next to the bird, that word is feed because there are seeds that the bird can feed on, and that is wait. So, this is a sentence, flee, wait, feed, wait, and that's a happy sentence and it will categorise it as happy. But the problem that we're trying to address here is exactly what we started with. How do you scan? How do you search? Where do you go forage for information to resolve as much uncertainty as you can about which of these six sentences is in play? And when the system does this just by trying to minimise its expected free energy, it shows this very interesting behaviour where it jumps from one word or page to the next without really dwelling and wasting time resolving uncertainty that is already resolved. So, once it sees a cat, it already knows that this has to be a flea word and it doesn't need to see where the other letters in this word are actually doing. It already knows, there's no more epistemic value to be had, there's no more uncertainty to resolve. It'll now jump to the next page and resolves uncertainty after a couple of Sokelechi movements, then jump to the next page. And after this once a card in the final page, it knows exactly what this sentence was doing. And if I can, I'll just show a movie of it doing that. So, the red dots correspond to where it's looking at the present time and the images that are mixtures of the icons represent conditional expectations. And the main point to be taken from this is that it knows there's a bird there, but it never looked there. It has sufficient prior knowledge in its deep, depth temporal model. It doesn't need to actually go and see stuff. It knows stuff is there because it knows what caused that stuff. And with this sort of simulation, one can then do exactly what Jim was talking about, which was if it knows stuff and it has predictions, then it should be possible to disclose or reveal that knowledge, that predictability by introducing violations and elicit the sorts of classical responses that we see empirically. And what we've done here is because we've got a deep model, we can do local and global violations. We can make the final story, the final sentence, a very surprising one without changing any of the stimuli, at the same time with or without making the prior beliefs about the upper lower case, the sort of local featureal expectations. We can switch those around so we replay exactly the same stimuli and the same behaviors, but just by changing the prior beliefs of the agent, we can cause certain things to be surprising, and those things can either be at the local, the first level, or the higher, the second level. And if we do that, we get lots of behaviors that look again a little bit like delay period activity in the prefrontal cortex of a periscadic sort that you see prior to a saccade being selected and enacted. While at the same time the band pass filtered voltages that are being driven by the implicit prediction errors look very much like periscadic ERPs. And when you look at those periscadic ERPs under local versus global violations, what you actually see is something almost identical if it's a local violation to a mismatch negativity, whereas, for the global violation, you get the mismatches or the differences much later on in time, very much like a P300. I can see what I was going to show you. No, I can't. I can't. That was a very pretty slide, but I can't. Very clever. It's a quote from, well, you don't need to know that. And then the final slide, it's got a thank you. A lot of people were on this slide, but you'll never know who now, will you? So thank you very much. So the workshop is structured so that there's a lot of time for discussion after each talk. And so the floor is open now for people to ask questions or make observations. I was just informed about how they do this in philosophy conferences that involves raising your hand or your finger, but I haven't mastered that yet, so I don't understand it. So I think it's too complicated for this group. But not for philosophers. So who would like to start? I'll start. I was just wondering if you could say more. You mentioned that you get something when there's choice involved with the rat experiment simulation. Something that looks like a drift diffusion model. And I've always been puzzled at how you get something that looks really like choice or agency out of a predictive coding model. So maybe you can elaborate a little bit on that. So the question is, where does the choice come into predictive coding? I think that question. Let me just be a little more specific. It's not that I don't see how you get choice behavior in the sense that you can use predictive coding in order to evaluate some options. But the notion of agency seems, if what you're doing is just predicting what you will do, then it seems to kind of undermine the notion of agency. So the answer to that question is very simple. It's tribly simple. You put agency into these schemes through prior preferences that define the sort of agent that I am. So we were talking before about reducing surprise of all sorts, whether it's epistemic uncertainty. But the simplest sort of pragmatic surprise, if I have a cost function that I don't want to be very hungry, or I don't want to end up in an arm that has no rewards in it, then I'd simply have to have the pride belief that at the end of the day I will end up rewarded or sated or happy or complete. So that anything else that happens is surprising. And therefore I can then bring the whole machinery of predictive coding to bear upon the problem of suppressing prediction errors and surprises. So I'm putting it very simply in terms of predictive coding. If I, a priori, believe that I'm always going to be happy and complete and that I am built to always minimise my prediction errors in the future, then I will look as if I have agency, I will look as if I have purpose, because I will always choose my actions in a way that avoids the prediction errors that suggest that I am not happy and complete. So the answer is just to absorb cost functions into inference by making costly states surprising through prior preferences. And that comes out of things like planning as inference. There are lots of ways of articulating that from the point of view of the rhetoric that I was using. The expected free energy has two bits to it. It has this epistemic bit and this pragmatic bit, but very simply it's uncertainty and surprise. The epistemic bit is minimising uncertainty. The value, the purpose, the goal is a pragmatic bit defined through cost functions that are literally the surprise of a costly outcome. So, just to follow up a little bit, so is this sort of like a hyper-prior that's going to be, I mean it sounds like we all have to have this ultimate belief that it's all going to end well at the end of the day. So pessimists, none of us are really pessimists or something like that, right? By definition. You may be perverse in your optimism, but you are quintessentially optimistic. You're getting to some, you know, the deeper backstories behind the free energy principle. The only, if you like, assumption that this instance of Hamilton's principle of release action makes is that you exist. And if you exist, that means you behave as if you have beliefs that you exist. And by existing, that just means that you're not decaying or dying. All your states are within some bounds, be they physiological or homeostatic or pecunary in terms of being rich or in terms of interceptive inference. You're, you know, the hedonics on that happy. But it's all about keeping things in bounds. It's all about minimising entropy, minimising uncertainty. So it always looks as if agents that exist have prior beliefs that they exist. And when you unpack that, that simply means I have preferred states that I will expect myself to occupy. Literally they are attracting states, they are an attractor. So that rhetoric, which actually is a rhetoric from dynamical systems theory, applies identically to this sort of purposeful reinforcement learning, or sort of goal directed style of thinking about things. There are attracting states, they are simply the ones that you frequent, which means that you will appear to behave as if you have prior preferences for being in those states. And you will always choose actions to get to those states. It is those prior preferences that define the sort of agent you are. So in answer to your very first question, the agentfulness comes in by implication, or just through the sorts of priors that characterise that particular sort of agent. So if I was a virus, I would have very different preferences than if I was a person. But there are still both plausible and viable preferences in the sorts of agents. Hi, thanks for your talk, Carl. I was wondering now that the slides are back if we could go to the delay period activity that you had briefly mentioned. And I just wanted to see that a little bit unpacked and related to what you were just talking about, that is agents reducing their free energy. Is that principle then generate the delay period activity that we see in places like prefrontal cortex and in working memory and so forth? Thanks. Right, well, those sorts of phenomena which we all know and have and will try to explain and measure empirically. So let me just try and find it. Do you remember where it was? Right, thank you. There you go. All those sorts of nuts and bolts getting down and dirty in terms of what this scheme would do when you put dynamics on it through the gradient descent, depend upon the gerontid model. So actually very similar to the last, one key component of the gerontid model are the prior beliefs, the preferences, what gives it purpose, what are its goals. Your question, I think, has a very similar answer. Once you've written down the gerontid model, everything else is not up for discussion. The maths tells you exactly what has to happen once you've written down the gerontid model. And the delay period activity you're talking about simply follows from the fact you've got a deep gerontid model or a deep temporal model. So as soon as you write that deep structure into the model, it means that certain beliefs have to outlive or change on a slower temporal scale than other beliefs lower in the hierarchy. Which means that you have to have delay period activity whilst other stuff unfolds at the lower levels of the hierarchy. So in this particular example, what I've done here is show the beliefs about the six sentences over the four moves, giving us six times four moves or five moves. So it should be about 30 beliefs here. On the same time access as beliefs about the particular word that's currently being seen. These resets here indicate the onset of saccades and the acquisition of new information. You can see roughly every 250 milliseconds there's a saccade and new beliefs are updated about the current word. But at the high level, we're only considering beliefs about each letter in my part. We're only considering beliefs about the word. So beliefs about the word corresponding to what's on this page or what's in this word are invariant during the successive saccades as you sample the different letters. So these things change more quickly than these things. When these are completed, then there's a change here and then the cycle begins again. So these tick over faster than this and then this looks a little bit now like the rastles that you see prior to the emission of the saccade here. They're not from the same paper but a related paradigm. If I take the voltage causing this delay period activity and band pass filter it, you get these sorts of fluctuations out here. So when there's an increase in delay period activity, there's usually a positive deflection that looks a little bit like an ERP. And just to follow up, so is the presence of delay period activity, is that associated then with prediction error or with the build up of a prediction? No, I think the prediction error in this scheme and this is the maths that comes from the discrete aspect of the genetic models lies in the rate of change of neural firing. If you associate the bi-physical encoding of expected states of the world in terms of population firing rates, then if you subscribe to that, if you accept that, then the prediction error now becomes the conductances that drive the depolarisation that drive the firing. So these basically reflect the fact that as time goes on you can more and more confident that one particular sentence is in play and you can see that. This is beliefs about which sentence is in play at the first, second, third, fourth and fifth page or word and they are now internally consistent and at every point in time in the past, at the end, I now believe I was reading the first sentence. And that belief endures during the sampling of all the actual letters within each of the words. So these would now represent just basically numbers between nought and one, zero and 100% neural firing that score your expectation that this is the current state of the world. At the beginning, there's lots of ambiguity, not an enormous amount, but there is ambiguity. It's 50-50 because of the six sentences, only two of them begin with the word flee, which means that we resolve our uncertainty about four of them, but we're still ambiguous about having ambiguity about sentences one and four and that can only be resolved at the end because these sentences only differ in the words right at the end. So during this time, there's delay period activity which we've got these two explanations, hypotheses in play that are resolved epistemically, optimally, right at the end when we get to the last word here and it's a wait and that determines which of the letters it was. Thank you for the talk. I wanted to go back to this idea of this contrast between reinforcement learning and the kind of formulation that you're making here. So one of the things that I thought was interesting is this formulation in terms of external value plus you basically decompose your KL divergence to external value and epistemic value. So how do you get exploration in this model? So it seems to me that you're doing an ARMAX over actions to get some balance between immediate value and information gain. Is that the basic idea? Yes. I mean, we can look at the equation or we can look at this. That's absolutely right. So, well, the exploration is good that you brought that in because another perspective on this is the whole foraging ethological perspective on exploration versus exploitation. That rhetoric just maps very simply to the epistemic and the pragmatic. So, and there is no, again, there is no up for discussion or there's no ad hoc waiting between the two. The expected free energy can always be written down in terms of exploration plus exploitation in terms of the epistemic value and the pragmatic value. And what happens is in minimizing that one quantity, you get this scanning searching behaviour until the epistemic bit has been reduced, allowing then you to focus on the pragmatic bit. So for free, you get a base optimal exploration to the extent that it is sufficient to resolve uncertainty given the precision of your beliefs about your prior preferences that then allow you to pursue your goals. So this solves the exploration dilemma in a base optimal sense. It also suggests that the very carving of behaviour into these two complementary drives is actually probably a misdirection. So it's only you and me that have actually teased apart the two components of the expected free energy and called one an epistemic exploratory one or a novelty seeking one and the other bit a pragmatic cost function like rewarding preference goal directed like one. There are lots of different ways of rearranging those. You can also rearrange them in terms of risk and ambiguity, intrinsic and extrinsic value. There are lots of ways of carving them and getting different perspectives. When you see that and when you work with that, you start to realize that it's not necessarily the best thing just to have one particular religious perspective on it because it lends you to the false belief if you subscribe to this formalism that there has to be some other adjudicator. There has to be some other harmonculus that's decided, oh, I need to explore now. I've done my exploration and now I'm going to go and do a bit of pragmat and stop the scanning and then I'm going to go and exploit what I've discovered. It doesn't work like that. You should get that for free. If they're both part of the same cost function, then once you've sufficiently reduced your uncertainty, then just naturally you go into as illustrated here your exploratory behaviour. This is exploratory behaviour or novelty seeking in the sense that you don't know what the queue is going to tell you. It doesn't have any immediate rewarding aspect to it. There are no preferences associated with the condition stimulus, but it's interesting, uncertainty resolving, but after a time it becomes boring because you already know what it's going to tell you. Once it does that, then you get exploratory behaviour. I should have done that. I should have put exploration and exploitation. Have an argument with me because it's meant to be a discussion. Do you not like that? It's very interesting perspective on it. I'm surprised that it comes out that the simple deterministic policy works well and just kind of works out of the box. My inkling is that when you go and implement this stuff, it can be difficult for it to balance the exploration and exploitation. So there's no tuning parameters, there's no off policy estimation, you just throw it in there. It all works out. I know exactly what you're saying because this was a big selling point when we first realised that a couple of years ago and it will remain a bit like one of these five to ten year changing the direction of the ocean liner of the oil tanker. Two years later, all that we're saying is in fact people behave according to Hampton's principle of least action. That's all that we're saying. That implicitly, or it looks like, that behaviour has this dual aspect. It doesn't, if you formulate it as a variational principle of the sort you did at school when doing new turning mechanics and then Einstein did with general relativity. It sort of falls out of the mix in a way that does actually dismiss these separatist perspectives on, I can either do this or that and I've got to now optimise the exploration in relation to the exploitation. The key trick that puts you into this simple world of Hampton's principle of least action is the realisation you can't prescribe good behaviour unless the prescription is an optimisation of a function of beliefs. A really simple example would be an economics game. I've got a really high risk and a low risk option. But there may be a third option, which is if I don't know which is which, I should do nothing until I know more. In using words like I don't know, I am now saying that my behaviour now becomes a function of my knowledge or my uncertainty, so I now induce different options, different behaviours, different policies and actions that rest upon my degree of belief, which means you can't do it with value function optimisation or utility function optimisation or reinforcement learning. It can't be done with queue learning. It cannot be done with a Bellman optimisation scheme because what you've done is you've said that there are better behaviours when I don't know what to do, which I generally don't do anything or wait until more information comes about. And once you write that down, you make your objective function a function of a probability distribution which becomes an energy and then you integrate that over time and then you've got to Hampton's principle of least action. So the simplicity post hoc is evident for me anyway. Does that make any sense? Can I ask you a question about the general approach here, which is I see that you start saying that there is an urge, there is a force to reduce surprise. So this is like I'm making an analogy with physics because that's what you're doing. And there you start saying I can rewrite the whole thing instead of force in terms of a Fourier energy. And then you go on and explain what it implies. My fear is that there is a difference here between what we do in terms of behaviour. First of all, reducing behaviour to just minimizing surprise, there are other forces that we cannot measure. We cannot even measure forces, force in terms of reducing surprise in an individual. So even though you can write these equations and describe something general so you can prescribe what should be the brain doing, do you think you can actually make any prediction? I know you're doing it, but I'm asking how can you think that you can do it because there is no measurement to tell you that's actually the case. So in physics you could come up with any new evidence, you would write a new term into your equation and just keep adding it and then you're always safe because there are some conservation laws that basically will keep the Hamilton principle intact. Well, who said there is such a thing in terms of behaviour and I think the simplest thing to assume there is no such thing. So then by doing that, if you let's say you keep adding terms because you believe that what we are doing is just reducing surprise, this is the ultimate goal of behaviour, then just keep adding terms to your free energy. And then basically the whole thing becomes a tautology because you're assuming that you're adding terms and there is no proof or disprove for that. So that's my main question is that what is, I mean, how do you think it's going to work? Right, that's a very good, I mean I ended quite a bit weakly, but that was a very good question. So lots of really interesting issues there, so the tautology issue, the practical utility of this style of theorising, can it ever be falsified adding things too? So I think we could spend hours talking about any one of those. First of all, the whole point of my style of neuroscience is that we never add anything in. You're always obliged to, for every advance, you have to get rid of something which was a distraction or ad hoc or a heuristic. We've been talking about this magic parameter, the nuances, the balance between exploration and exploitation. So that just goes, there's only one thing that's being minimised here, about everything and that's variation free energy. That's it, there's nothing being added. However, of course, the free energy is a function of the gerontive model. So all the interesting, all the hard work or the heavy lifting understanding this biological system in this experimental context or this social context, that really calls upon you writing down a gerontive model. So that's, it doesn't mean there is no free lunch, there's still a lot of neurobiology and psychology and cognitive neuroscience and computational neuroscience to do. It's just all at the level of the gerontive model, not the normative principles or the variational principles behind it. The tautology, part of that drive for simplification is a drive to get to the ultimate explanation which has to be tautological and for me the free energy principle is tautological. It's as tautological as the natural selection, but beautifully so. Once it's completely tautological, I'll be happy. Part of that tautology comes along in a slightly technical guise called the complete class theorem and I'm bringing that to the discussion because it speaks to, I think, a very interesting point you were essentially making. Is there anything that you can not explain, any behaviour in a real biological system that cannot be explained by this? Because if there isn't, then what's the point? Now, if there is no behaviour that this cannot explain, so it is provably true that for any pair of cost functions and behaviours there are a set of prior beliefs, prior preferences that we're talking about that endow the behaviour with an agent, with agency, that render that behaviour based optimal and by definition therefore conforms to the free energy principle. So what that means is that there is no behaviour that this can't describe if you can find the right prize. So is that a weakness or a strength? Well, in the sense of falsifiability, it's a weakness. In the sense of actually using it practically, it's a real strength because what that means is any system, normal human being or psychiatric cohort, that you bring to me, if I can solve the problem of getting the most appropriate or a sufficiently good generative model that describes their behaviour, it means I can quantify exactly the sort of person they are by their prior beliefs. And we actually can write back down to the first question again, is what makes an agent an agent, it's their prior beliefs. It's that attracting set that describes the sorts of states that that sort of person occupies. So, yeah, there is a deep tautology, there's a fundamental difficulty for falsification, but there's also a glorious insight underneath that which means that everything can now be written down in terms of an agent's prior beliefs and that if you can get the right model, you can actually estimate these things. You can actually quantify them and this is one of the tenets of computational psychiatry. It's to be able to use games, ERPs, mismatch negativities, whatever in the service of say, what sort of person am I looking at, quantified in terms of the prior beliefs about the way they should behave. Carl, but aren't you making the assumption that there is just a unique set of beliefs that will match a data set? Because if you have several beliefs and I would think that when we minimize those type of system, that's equivalent to several solutions and which almost always we've got several solutions because we've got several minimas there. Because it's not convex, the story that we are minimizing there. So, therefore, we have a lot of minimas and I would say that if we're lucky that means that for any behavior that we can measure, we are now with a set and that could be thrilling to know which one, a set of prior beliefs. So, in other words, we cannot inverse that or am I wrong? No, no, no, you're absolutely right but you've taken us into metabasian land now. Okay, so just to, those people who may be getting a bit confused. So, the argument I think, let me just paraphrase it, if we're now saying well how do we practically use this style of thinking and I'm now in the job of quantifying this person with autistic spectrum disorder, given a bead's task, an earned task, in terms of the prior beliefs about the volatility of the environment and the need to please the experimenter by responding within a certain time frame. If that is the problem, then we're now in and observing the observer or using Bayesian inference to make inferences about a Bayesian machine, which is the autistic patient, which is hence the metabasian thing and you're absolutely right, that could be an ill-posed problem. So, there may be, the complete class there does not say that there is only one unique set of prior beliefs that will make, render the behaviour based on what it says that it exists, it doesn't have to be a unique solution. However, what will happen is that if you actually use Bayesian statistics to infer the prior beliefs of the Bayesian or ASD subject, you will then see if you've got the appropriate model that there are a number of equally plausible solutions and you will also see that you haven't got enough data to disambiguate between them. But you will also have an insight into which sorts of experiments you would need to do that disambiguation because you've got a generative model underneath of this. You can do simulations to see the sorts of data that you are, or the ways of looking at the responses that would enable you now to narrow down these competing but equally plausible sets of prior beliefs that characterize that subject. So, it's a very important issue, but I think it's a generic one about how we actually apply Bayesian statistics to understand the ways in which our data are being generated. I think that in that sense it's less profound than the complete class theorem in itself, which speaks to the sort of tautology of the game that we find ourselves in. I think we can have time for one more short question. I have two short ones. The first one is about the semantics and the second one is about the substance. So, regarding the semantics, I'm wondering, it seems as though there are these two aspects to the problem. One is inference, the other is control and there is the epistemic cost in the context of inference and there is the pragmatic costs, ultimately reproduction, the well-being and survival of the organism. It seems as though you are declaring the primacy of surprise, so you want to absorb the pragmatic costs under the epistemic costs. So, that's a bit of a semantic issue perhaps, but doesn't it make more sense to do it the other way around and say, so the ultimate goal is reproduction and epistemic benefits should be a sub-goal of that. So, that's the semantic question. The question on substances, is this something that is already going on in engineering in AI, for example? Can these principles be scaled up to real-world tasks such as visual object recognition? If so, has it already been done or is this an impending revolution for AI? And if not, is it just a reinterpretation of what already exists? I agree because they were very short questions. Very short. You couldn't get them shorter than that, could you? Do you want me to try to answer them shortly? The semantic one. No, I don't think so. I take your point that one could articulate a whole theory and absorb uncertainty into cost functions, but I think that misses the key point that I was trying to make at the beginning of the talk, that the quantity you have to optimise is a function of probability distributions or beliefs. That's the key thing, which means you can never use a Bellman optimality scheme to properly solve that. You do see heroic attempts, and those heroic attempts have been endemic for the past 40 years, they're known as partial observed mark-up decision processes or belief state machines, and all sorts of heroic attempts to try and recover or resolve the problem, or once you write down a discrete state space over continuous beliefs, you can't do it, therefore you have to parameterise those continuous beliefs in those sorts of glorious and clever ways, they don't work, they don't scale. So people have been aware of the problem, and hence the vast literature on partial observed mark-up decision problems. What I'm saying is that's the wrong approach, the right approach is Hampton's principle of least action, where the functional that you need to optimise is a function of the beliefs before you start, and by proof of principle I can demonstrate the veracity of that argument, because I can solve problems which people working with partial observed MDPs cannot solve. So I think it's much more than semantics, I think it's actually, people have got it wrong in the 20th century. I think the Bellman's optimality equations are very beautiful construct, completeness direction. We should have been looking at Hamilton's principle of least action, and that's the 21st century. Are people doing this? Yeah. So Google Deep Mind, for example, they've now, a small group within Google Deep Mind, have now started using variational free energy in their deep energy model, not the temporal models, but certainly sort of 10-led neural networks in the context of the sort of pattern recognition approach. So people have always rears, I think, that the variational approach was the right way to do this, and of course you remember most of deep learning started with Geoffrey Hinton's work, and he came from the variational formulation, so he was the first person to be down to propose the Helmholtz machine, but they've taken a sort of security stream back to those early work in the 1990s. Will it scale? I don't know, because I don't think they're quite on top of that, because what they do is they haven't quite got, well, this is insulting, but I hope, is anybody here from Google? Ah, well I won't say that then. Well anyway, they love amortisation, they love casting things in terms of learning, so what they do is instead of actually trying to optimise their beliefs, their expectations, they try and optimise beautifully constructed deep nets, convolution nets, that have parameters that would map from data to beliefs, or sufficient statistics or beliefs, and then they learn that because they're experts, and they are more than well experts, they are the experts in optimising the parameters. That's a slight problem because it denies any context sensitivity of the sort that we deal with the neuroscientists and I deal with them in my simulations. I think once they get beyond amortising their deep networks, then they'll be in this game, and then we'll find out whether it scales, you know, that you'll need lots of computer scientists, big computers. So I would imagine the next five to ten years this style of approach, and so the very actual free energy formulation will become increasingly dominant in people doing artificial intelligence. And I should quip, I mean, for some people, the new AI is actually active inference. It wasn't a coincidence that we chose that sort of rhetoric to promote it. Yes, so maybe I could just have a comment rather than a question then. Okay, okay. So that's a bit straight-laced approach to free energy reduction. It's based on surprise aversion, but I'm sure you have another talk on surprise-seeking, curiosity, creativity, playful managerial styles, which must have some adaptive purpose beyond our interest in horror movies and jokes. It must help us get out of dead-end problem space to problem spaces we can't even imagine. So the comment is, if you add surprise-seeking to free energy minimisation, I don't believe it remains tractable. So I'll leave it at that.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 11.64, "text": " Llywbeth hon am wneud ym mwyn sicrhau hynny ac mae'r ydych\u6df7wyl sy'n cynnig mewn gw'r dym \u0442\u0430\u043a\u0443\u044e rhoi riannol?", "tokens": [50364, 441, 356, 86, 65, 3293, 2157, 669, 261, 716, 532, 288, 76, 275, 28284, 33579, 48759, 1459, 2477, 77, 1634, 696, 43783, 6, 81, 288, 3173, 339, 48640, 86, 5088, 943, 6, 77, 28365, 77, 328, 385, 895, 29255, 6, 81, 274, 4199, 42456, 20293, 72, 367, 952, 77, 401, 30, 50946], "temperature": 1.0, "avg_logprob": -3.5339457876730287, "compression_ratio": 1.330708661417323, "no_speech_prob": 0.0870196595788002}, {"id": 1, "seek": 0, "start": 12.76, "end": 22.46, "text": " Mewn gwneud o'c flyny y plwydio yn rhai ddiddio'r cyfnodau, Beth lle mae am gwaeth hyn a chi v\u00e9liwr ja Prud Follow gwelwch.", "tokens": [51002, 1923, 895, 29255, 716, 532, 277, 6, 66, 3603, 1634, 288, 499, 86, 6655, 1004, 17861, 367, 18230, 274, 67, 14273, 1004, 6, 81, 3185, 69, 77, 378, 1459, 11, 14011, 12038, 43783, 669, 290, 4151, 3293, 2477, 77, 257, 13228, 19050, 2081, 7449, 2784, 2114, 532, 9876, 29255, 338, 86, 339, 13, 51487], "temperature": 1.0, "avg_logprob": -3.5339457876730287, "compression_ratio": 1.330708661417323, "no_speech_prob": 0.0870196595788002}, {"id": 2, "seek": 0, "start": 22.78, "end": 28.64, "text": " Dolwedd yn geist startoeth\uc624\u0435\u0447 arall yn i ballrain arall, lle arall yn gweithio'r Llywodraeth.", "tokens": [51503, 18786, 26896, 67, 17861, 1519, 468, 722, 78, 3293, 8920, 4310, 594, 336, 17861, 741, 2594, 7146, 594, 336, 11, 12038, 594, 336, 17861, 290, 826, 355, 1004, 6, 81, 441, 356, 86, 378, 424, 3293, 13, 51796], "temperature": 1.0, "avg_logprob": -3.5339457876730287, "compression_ratio": 1.330708661417323, "no_speech_prob": 0.0870196595788002}, {"id": 3, "seek": 2864, "start": 28.64, "end": 45.620000000000005, "text": " rhoi", "tokens": [50364, 20293, 72, 51213], "temperature": 1.0, "avg_logprob": -3.291631062825521, "compression_ratio": 0.3333333333333333, "no_speech_prob": 0.03482084721326828}, {"id": 4, "seek": 4562, "start": 45.62, "end": 71.74, "text": "\u56de\u4f86 i'w gyddi'w gwybodaeth honom i", "tokens": [50364, 46556, 741, 6, 86, 290, 6655, 4504, 6, 86, 290, 9726, 65, 13449, 3293, 2157, 298, 741, 51670], "temperature": 1.0, "avg_logprob": -3.234917776925223, "compression_ratio": 0.8809523809523809, "no_speech_prob": 0.07735646516084671}, {"id": 5, "seek": 7174, "start": 71.74, "end": 75.25999999999999, "text": " ... Almighty Mike, tewi ydy gyrygau\u2026", "tokens": [50364, 1097, 16849, 6602, 11, 256, 1023, 72, 288, 3173, 15823, 627, 70, 1459, 1260, 50540], "temperature": 1.0, "avg_logprob": -3.7828531608307103, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.06499694287776947}, {"id": 6, "seek": 7174, "start": 75.39999999999999, "end": 77.52, "text": " \u2026 dyma ar-dysgu!", "tokens": [50547, 5799, 14584, 1696, 594, 12, 67, 749, 2794, 0, 50653], "temperature": 1.0, "avg_logprob": -3.7828531608307103, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.06499694287776947}, {"id": 7, "seek": 7174, "start": 78.06, "end": 82.0, "text": " Fy enw i, i\ufffd\ufffd\ufffdch cicitYEalsoch\u2026", "tokens": [50680, 479, 88, 465, 86, 741, 11, 741, 7871, 339, 269, 8876, 18881, 41551, 339, 1260, 50877], "temperature": 1.0, "avg_logprob": -3.7828531608307103, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.06499694287776947}, {"id": 8, "seek": 7174, "start": 82.89999999999999, "end": 85.66, "text": " \u2026 yma amary teuluw...", "tokens": [50922, 5799, 288, 1696, 669, 822, 535, 12845, 86, 485, 51060], "temperature": 1.0, "avg_logprob": -3.7828531608307103, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.06499694287776947}, {"id": 9, "seek": 7174, "start": 85.8, "end": 89.6, "text": " \u2026 i'r ffordd ydych Lady Worker...", "tokens": [51067, 5799, 741, 6, 81, 283, 7404, 67, 288, 3173, 339, 11256, 6603, 260, 485, 51257], "temperature": 1.0, "avg_logprob": -3.7828531608307103, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.06499694287776947}, {"id": 10, "seek": 7174, "start": 89.6, "end": 94.1, "text": " \u2026 Trunwau ystafelu'n bob gynyddio...", "tokens": [51257, 5799, 1765, 409, 86, 1459, 288, 372, 2792, 34813, 6, 77, 27292, 15823, 1634, 24810, 1004, 485, 51482], "temperature": 1.0, "avg_logprob": -3.7828531608307103, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.06499694287776947}, {"id": 11, "seek": 7174, "start": 94.22, "end": 98.74, "text": " \u2026 Mae'n eu clynyddu i'r ddelig, yn ddefnyddioSeePlayer...", "tokens": [51488, 5799, 31055, 6, 77, 2228, 269, 356, 1634, 67, 769, 741, 6, 81, 274, 18105, 328, 11, 17861, 274, 20595, 1634, 24810, 1004, 26869, 24262, 260, 485, 51714], "temperature": 1.0, "avg_logprob": -3.7828531608307103, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.06499694287776947}, {"id": 12, "seek": 7174, "start": 98.91999999999999, "end": 101.5, "text": " \u2026 Neil yma am ar-zell, amaeddrannu...", "tokens": [51723, 5799, 18615, 288, 1696, 669, 594, 12, 89, 898, 11, 10889, 292, 16753, 969, 84, 485, 51852], "temperature": 1.0, "avg_logprob": -3.7828531608307103, "compression_ratio": 1.4095238095238096, "no_speech_prob": 0.06499694287776947}, {"id": 13, "seek": 10150, "start": 101.5, "end": 107.42, "text": " Ac rydw i fyny mis o bwyfia'r amser,", "tokens": [50364, 5097, 367, 6655, 86, 741, 38777, 1634, 3346, 277, 272, 9726, 22054, 6, 81, 669, 12484, 11, 50660], "temperature": 1.0, "avg_logprob": -4.09302908914131, "compression_ratio": 1.2571428571428571, "no_speech_prob": 0.03761399909853935}, {"id": 14, "seek": 10150, "start": 107.42, "end": 112.76, "text": " nad maen nhw gwybod gweithre vaccinations,", "tokens": [50660, 12617, 463, 268, 6245, 86, 290, 9726, 47466, 290, 826, 355, 265, 39333, 11, 50927], "temperature": 1.0, "avg_logprob": -4.09302908914131, "compression_ratio": 1.2571428571428571, "no_speech_prob": 0.03761399909853935}, {"id": 15, "seek": 10150, "start": 112.76, "end": 115.1, "text": " rydw i'n amser ar gwell feed\u00eactig Sa\u600e\u4e48\u6837au?", "tokens": [50927, 367, 6655, 86, 741, 6, 77, 669, 12484, 594, 290, 6326, 3154, 1307, 349, 328, 6299, 48200, 1459, 30, 51044], "temperature": 1.0, "avg_logprob": -4.09302908914131, "compression_ratio": 1.2571428571428571, "no_speech_prob": 0.03761399909853935}, {"id": 16, "seek": 10150, "start": 115.1, "end": 122.44, "text": " Rydw i'n mir enghreithio y cael meddwl ei m \u0441\u043b\u0435\u0434di habiloghau ar reside\u1ed9ld,", "tokens": [51044, 497, 6655, 86, 741, 6, 77, 3149, 1741, 71, 265, 355, 1004, 288, 1335, 338, 1205, 67, 39192, 14020, 275, 15363, 4504, 36565, 664, 71, 1459, 594, 725, 482, 8047, 348, 11, 51411], "temperature": 1.0, "avg_logprob": -4.09302908914131, "compression_ratio": 1.2571428571428571, "no_speech_prob": 0.03761399909853935}, {"id": 17, "seek": 10150, "start": 122.44, "end": 128.2, "text": " ph\u756a dod yn gynghreithi a wyloach amoduslu ar draw.", "tokens": [51411, 903, 33118, 13886, 17861, 15823, 872, 71, 265, 355, 72, 257, 4628, 752, 608, 669, 32419, 2781, 594, 2642, 13, 51699], "temperature": 1.0, "avg_logprob": -4.09302908914131, "compression_ratio": 1.2571428571428571, "no_speech_prob": 0.03761399909853935}, {"id": 18, "seek": 12820, "start": 128.2, "end": 130.2, "text": " i'w ddweud i'w ddweud i'w ddweud.", "tokens": [50364, 741, 6, 86, 274, 67, 826, 532, 741, 6, 86, 274, 67, 826, 532, 741, 6, 86, 274, 67, 826, 532, 13, 50464], "temperature": 0.0, "avg_logprob": -0.7904503631591797, "compression_ratio": 1.434782608695652, "no_speech_prob": 0.03330672159790993}, {"id": 19, "seek": 15820, "start": 158.2, "end": 187.2, "text": " So, this is an overview of what we are going to go through. I am going to introduce the notion that you are the free energy principle, but from a, using a slightly heuristic approach in terms of action and the path of least resistance,", "tokens": [50364, 407, 11, 341, 307, 364, 12492, 295, 437, 321, 366, 516, 281, 352, 807, 13, 286, 669, 516, 281, 5366, 264, 10710, 300, 291, 366, 264, 1737, 2281, 8665, 11, 457, 490, 257, 11, 1228, 257, 4748, 415, 374, 3142, 3109, 294, 2115, 295, 3069, 293, 264, 3100, 295, 1935, 7335, 11, 51814], "temperature": 0.4, "avg_logprob": -0.3759367806570871, "compression_ratio": 1.4779874213836477, "no_speech_prob": 0.08437570184469223}, {"id": 20, "seek": 18720, "start": 187.2, "end": 196.2, "text": " highlighting the importance of having internal models or hypotheses that enable us to generate predictions, talking about active inference.", "tokens": [50364, 26551, 264, 7379, 295, 1419, 6920, 5245, 420, 49969, 300, 9528, 505, 281, 8460, 21264, 11, 1417, 466, 4967, 38253, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1291344960530599, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.04150281473994255}, {"id": 21, "seek": 18720, "start": 196.2, "end": 212.2, "text": " And one key thing that I am going to focus on is translating the theory into a process theory that can be used to understand neuronal message passing in the brain and help us exactly constrain the sorts of experiments that Jim was talking about.", "tokens": [50814, 400, 472, 2141, 551, 300, 286, 669, 516, 281, 1879, 322, 307, 35030, 264, 5261, 666, 257, 1399, 5261, 300, 393, 312, 1143, 281, 1223, 12087, 21523, 3636, 8437, 294, 264, 3567, 293, 854, 505, 2293, 1817, 7146, 264, 7527, 295, 12050, 300, 6637, 390, 1417, 466, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1291344960530599, "compression_ratio": 1.6594827586206897, "no_speech_prob": 0.04150281473994255}, {"id": 22, "seek": 21220, "start": 212.2, "end": 229.2, "text": " So, that is going to be a big part of what I hope that we will be talking about, taking normative principles and seeing how they unpack in the service of understanding empirical measurements anatomy and physiology and how they can be used to nuance experimental design,", "tokens": [50364, 407, 11, 300, 307, 516, 281, 312, 257, 955, 644, 295, 437, 286, 1454, 300, 321, 486, 312, 1417, 466, 11, 1940, 2026, 1166, 9156, 293, 2577, 577, 436, 26699, 294, 264, 2643, 295, 3701, 31886, 15383, 31566, 293, 43585, 293, 577, 436, 393, 312, 1143, 281, 42625, 17069, 1715, 11, 51214], "temperature": 0.0, "avg_logprob": -0.1819883173162287, "compression_ratio": 1.5371428571428571, "no_speech_prob": 0.08881770074367523}, {"id": 23, "seek": 22920, "start": 230.2, "end": 237.2, "text": " showing the sorts of things that one can simulate and speaking to some empirical predictions of these sorts of schemes.", "tokens": [50414, 4099, 264, 7527, 295, 721, 300, 472, 393, 27817, 293, 4124, 281, 512, 31886, 21264, 295, 613, 7527, 295, 26954, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14678610751503393, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.6663849353790283}, {"id": 24, "seek": 22920, "start": 237.2, "end": 248.2, "text": " And I have put, as an epilogue, more recent work, simulations of reading that introduce hierarchies into the particular forms of generative models that I want to survey for you.", "tokens": [50764, 400, 286, 362, 829, 11, 382, 364, 2388, 388, 7213, 11, 544, 5162, 589, 11, 35138, 295, 3760, 300, 5366, 35250, 530, 666, 264, 1729, 6422, 295, 1337, 1166, 5245, 300, 286, 528, 281, 8984, 337, 291, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14678610751503393, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.6663849353790283}, {"id": 25, "seek": 22920, "start": 248.2, "end": 254.2, "text": " We won't have time to go without that, but I just want to show you the slides in case of something that catches your attention.", "tokens": [51314, 492, 1582, 380, 362, 565, 281, 352, 1553, 300, 11, 457, 286, 445, 528, 281, 855, 291, 264, 9788, 294, 1389, 295, 746, 300, 25496, 428, 3202, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14678610751503393, "compression_ratio": 1.6865079365079365, "no_speech_prob": 0.6663849353790283}, {"id": 26, "seek": 25420, "start": 254.2, "end": 263.2, "text": " So, I'm going to start with a question. Let's assume you're hungry, and let's assume you're an owl. So, what are you going to do?", "tokens": [50364, 407, 11, 286, 478, 516, 281, 722, 365, 257, 1168, 13, 961, 311, 6552, 291, 434, 8067, 11, 293, 718, 311, 6552, 291, 434, 364, 34488, 13, 407, 11, 437, 366, 291, 516, 281, 360, 30, 50814], "temperature": 0.0, "avg_logprob": -0.24595935004098074, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.014386150054633617}, {"id": 27, "seek": 25420, "start": 266.2, "end": 270.2, "text": " You're going to search for a mouse? And how are you going to do that?", "tokens": [50964, 509, 434, 516, 281, 3164, 337, 257, 9719, 30, 400, 577, 366, 291, 516, 281, 360, 300, 30, 51164], "temperature": 0.0, "avg_logprob": -0.24595935004098074, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.014386150054633617}, {"id": 28, "seek": 25420, "start": 272.2, "end": 275.2, "text": " Don't cheat. You have to look at me, not at me.", "tokens": [51264, 1468, 380, 17470, 13, 509, 362, 281, 574, 412, 385, 11, 406, 412, 385, 13, 51414], "temperature": 0.0, "avg_logprob": -0.24595935004098074, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.014386150054633617}, {"id": 29, "seek": 25420, "start": 277.2, "end": 279.2, "text": " Absolutely. Perfect answer.", "tokens": [51514, 7021, 13, 10246, 1867, 13, 51614], "temperature": 0.0, "avg_logprob": -0.24595935004098074, "compression_ratio": 1.5804597701149425, "no_speech_prob": 0.014386150054633617}, {"id": 30, "seek": 27920, "start": 279.2, "end": 286.2, "text": " So, in terms of optimal behaviour, the first thing you do is search. You scan.", "tokens": [50364, 407, 11, 294, 2115, 295, 16252, 17229, 11, 264, 700, 551, 291, 360, 307, 3164, 13, 509, 11049, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11126170987668245, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.005812964867800474}, {"id": 31, "seek": 27920, "start": 286.2, "end": 294.2, "text": " You confront the epistemics of reducing uncertainty about what you need to do in order to fulfil your goal.", "tokens": [50714, 509, 12422, 264, 2388, 468, 38014, 295, 12245, 15697, 466, 437, 291, 643, 281, 360, 294, 1668, 281, 41054, 428, 3387, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11126170987668245, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.005812964867800474}, {"id": 32, "seek": 27920, "start": 294.2, "end": 300.2, "text": " So, it's all about beliefs. So, in that answer is the basis of everything that I'm going to say.", "tokens": [51114, 407, 11, 309, 311, 439, 466, 13585, 13, 407, 11, 294, 300, 1867, 307, 264, 5143, 295, 1203, 300, 286, 478, 516, 281, 584, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11126170987668245, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.005812964867800474}, {"id": 33, "seek": 27920, "start": 300.2, "end": 307.2, "text": " Your behaviour is always driven by beliefs, and that tells us something quite important.", "tokens": [51414, 2260, 17229, 307, 1009, 9555, 538, 13585, 11, 293, 300, 5112, 505, 746, 1596, 1021, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11126170987668245, "compression_ratio": 1.6387665198237886, "no_speech_prob": 0.005812964867800474}, {"id": 34, "seek": 30720, "start": 307.2, "end": 314.2, "text": " So, here's you scanning and searching, and you've found a little mouse that you might want to eat there.", "tokens": [50364, 407, 11, 510, 311, 291, 27019, 293, 10808, 11, 293, 291, 600, 1352, 257, 707, 9719, 300, 291, 1062, 528, 281, 1862, 456, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10803840637207031, "compression_ratio": 1.5885167464114833, "no_speech_prob": 0.0005035530775785446}, {"id": 35, "seek": 30720, "start": 314.2, "end": 323.2, "text": " That's quite important because it speaks to two basic classes of ways of thinking about optimising behaviour.", "tokens": [50714, 663, 311, 1596, 1021, 570, 309, 10789, 281, 732, 3875, 5359, 295, 2098, 295, 1953, 466, 5028, 3436, 17229, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10803840637207031, "compression_ratio": 1.5885167464114833, "no_speech_prob": 0.0005035530775785446}, {"id": 36, "seek": 30720, "start": 323.2, "end": 332.2, "text": " You can either imagine that there is some value function of the next state that will be brought about by some action,", "tokens": [51164, 509, 393, 2139, 3811, 300, 456, 307, 512, 2158, 2445, 295, 264, 958, 1785, 300, 486, 312, 3038, 466, 538, 512, 3069, 11, 51614], "temperature": 0.0, "avg_logprob": -0.10803840637207031, "compression_ratio": 1.5885167464114833, "no_speech_prob": 0.0005035530775785446}, {"id": 37, "seek": 33220, "start": 332.2, "end": 339.2, "text": " and optimise that action by selecting the action that maximises the value of the next state.", "tokens": [50364, 293, 5028, 908, 300, 3069, 538, 18182, 264, 3069, 300, 5138, 3598, 264, 2158, 295, 264, 958, 1785, 13, 50714], "temperature": 0.0, "avg_logprob": -0.06907190382480621, "compression_ratio": 1.590643274853801, "no_speech_prob": 0.0005585675244219601}, {"id": 38, "seek": 33220, "start": 339.2, "end": 349.2, "text": " That's the classical way of doing it, but that just doesn't work if the best next thing to do is to search and resolve uncertainty.", "tokens": [50714, 663, 311, 264, 13735, 636, 295, 884, 309, 11, 457, 300, 445, 1177, 380, 589, 498, 264, 1151, 958, 551, 281, 360, 307, 281, 3164, 293, 14151, 15697, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06907190382480621, "compression_ratio": 1.590643274853801, "no_speech_prob": 0.0005585675244219601}, {"id": 39, "seek": 33220, "start": 349.2, "end": 352.2, "text": " Because uncertainty is an attribute of beliefs.", "tokens": [51214, 1436, 15697, 307, 364, 19667, 295, 13585, 13, 51364], "temperature": 0.0, "avg_logprob": -0.06907190382480621, "compression_ratio": 1.590643274853801, "no_speech_prob": 0.0005585675244219601}, {"id": 40, "seek": 35220, "start": 352.2, "end": 363.2, "text": " Therefore, the function of a function that you need to optimise in terms of action you hear is a function of beliefs,", "tokens": [50364, 7504, 11, 264, 2445, 295, 257, 2445, 300, 291, 643, 281, 5028, 908, 294, 2115, 295, 3069, 291, 1568, 307, 257, 2445, 295, 13585, 11, 50914], "temperature": 0.0, "avg_logprob": -0.11566753387451172, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.08022115379571915}, {"id": 41, "seek": 35220, "start": 363.2, "end": 367.2, "text": " which I'm deleting by Q, beliefs about the states of the world.", "tokens": [50914, 597, 286, 478, 48946, 538, 1249, 11, 13585, 466, 264, 4368, 295, 264, 1002, 13, 51114], "temperature": 0.0, "avg_logprob": -0.11566753387451172, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.08022115379571915}, {"id": 42, "seek": 35220, "start": 367.2, "end": 377.2, "text": " That introduces a fundamental distinction between the sorts of schemes that you bring to bear in terms of understanding optimal behaviour.", "tokens": [51114, 663, 31472, 257, 8088, 16844, 1296, 264, 7527, 295, 26954, 300, 291, 1565, 281, 6155, 294, 2115, 295, 3701, 16252, 17229, 13, 51614], "temperature": 0.0, "avg_logprob": -0.11566753387451172, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.08022115379571915}, {"id": 43, "seek": 37720, "start": 377.2, "end": 389.2, "text": " The other thing about the scanning and searching answer is that action depends upon beliefs about the world, states of the world, and subsequent actions.", "tokens": [50364, 440, 661, 551, 466, 264, 27019, 293, 10808, 1867, 307, 300, 3069, 5946, 3564, 13585, 466, 264, 1002, 11, 4368, 295, 264, 1002, 11, 293, 19962, 5909, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08703818207695371, "compression_ratio": 1.792929292929293, "no_speech_prob": 0.003557710675522685}, {"id": 44, "seek": 37720, "start": 389.2, "end": 396.2, "text": " So, not only is it a function of beliefs about the world, but it's the order in which you interrogate that world.", "tokens": [50964, 407, 11, 406, 787, 307, 309, 257, 2445, 295, 13585, 466, 264, 1002, 11, 457, 309, 311, 264, 1668, 294, 597, 291, 24871, 473, 300, 1002, 13, 51314], "temperature": 0.0, "avg_logprob": -0.08703818207695371, "compression_ratio": 1.792929292929293, "no_speech_prob": 0.003557710675522685}, {"id": 45, "seek": 37720, "start": 396.2, "end": 402.2, "text": " So, it makes a difference whether you search, then eat, as opposed to eat, then search.", "tokens": [51314, 407, 11, 309, 1669, 257, 2649, 1968, 291, 3164, 11, 550, 1862, 11, 382, 8851, 281, 1862, 11, 550, 3164, 13, 51614], "temperature": 0.0, "avg_logprob": -0.08703818207695371, "compression_ratio": 1.792929292929293, "no_speech_prob": 0.003557710675522685}, {"id": 46, "seek": 40220, "start": 402.2, "end": 409.2, "text": " That means that we are in the game of optimising sequences or policies or actions.", "tokens": [50364, 663, 1355, 300, 321, 366, 294, 264, 1216, 295, 5028, 3436, 22978, 420, 7657, 420, 5909, 13, 50714], "temperature": 0.0, "avg_logprob": -0.13498309942392203, "compression_ratio": 1.837719298245614, "no_speech_prob": 0.008097651414573193}, {"id": 47, "seek": 40220, "start": 409.2, "end": 411.2, "text": " I'm going to call it a sequence of actions policy.", "tokens": [50714, 286, 478, 516, 281, 818, 309, 257, 8310, 295, 5909, 3897, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13498309942392203, "compression_ratio": 1.837719298245614, "no_speech_prob": 0.008097651414573193}, {"id": 48, "seek": 40220, "start": 411.2, "end": 424.2, "text": " So, what that means from the point of view technically of what sort of thing we have to optimise, it's a functional of a belief integrated over time, or summed over time, a path integral.", "tokens": [50814, 407, 11, 437, 300, 1355, 490, 264, 935, 295, 1910, 12120, 295, 437, 1333, 295, 551, 321, 362, 281, 5028, 908, 11, 309, 311, 257, 11745, 295, 257, 7107, 10919, 670, 565, 11, 420, 2408, 1912, 670, 565, 11, 257, 3100, 11573, 13, 51464], "temperature": 0.0, "avg_logprob": -0.13498309942392203, "compression_ratio": 1.837719298245614, "no_speech_prob": 0.008097651414573193}, {"id": 49, "seek": 40220, "start": 424.2, "end": 429.2, "text": " If we call that an energy, then the integral, the path integral of an energy is called an action.", "tokens": [51464, 759, 321, 818, 300, 364, 2281, 11, 550, 264, 11573, 11, 264, 3100, 11573, 295, 364, 2281, 307, 1219, 364, 3069, 13, 51714], "temperature": 0.0, "avg_logprob": -0.13498309942392203, "compression_ratio": 1.837719298245614, "no_speech_prob": 0.008097651414573193}, {"id": 50, "seek": 42920, "start": 429.2, "end": 443.2, "text": " So, what we've just said is that we've reduced the problem of good behaviour to Hamilton's principle of least action, where action is the path integral or the trajectory integral or the sum over an energy functional of beliefs.", "tokens": [50364, 407, 11, 437, 321, 600, 445, 848, 307, 300, 321, 600, 9212, 264, 1154, 295, 665, 17229, 281, 18484, 311, 8665, 295, 1935, 3069, 11, 689, 3069, 307, 264, 3100, 11573, 420, 264, 21512, 11573, 420, 264, 2408, 670, 364, 2281, 11745, 295, 13585, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10645920783281326, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.008157114498317242}, {"id": 51, "seek": 42920, "start": 443.2, "end": 448.2, "text": " And that's the basic premise that I'm going to pursue.", "tokens": [51064, 400, 300, 311, 264, 3875, 22045, 300, 286, 478, 516, 281, 12392, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10645920783281326, "compression_ratio": 1.5326086956521738, "no_speech_prob": 0.008157114498317242}, {"id": 52, "seek": 44820, "start": 449.2, "end": 462.2, "text": " Just to highlight the distinction, if you subscribe to this way of thinking about how systems work, then you end up with optimal control theory, Bayesian decision theory, reinforcement learning and all that good stuff.", "tokens": [50414, 1449, 281, 5078, 264, 16844, 11, 498, 291, 3022, 281, 341, 636, 295, 1953, 466, 577, 3652, 589, 11, 550, 291, 917, 493, 365, 16252, 1969, 5261, 11, 7840, 42434, 3537, 5261, 11, 29280, 2539, 293, 439, 300, 665, 1507, 13, 51064], "temperature": 0.0, "avg_logprob": -0.113613895986272, "compression_ratio": 1.7418032786885247, "no_speech_prob": 0.1746596395969391}, {"id": 53, "seek": 44820, "start": 462.2, "end": 474.2, "text": " Conversely, if you believe this is how biological systems work, then you end up essentially with Hamilton's principle of least action, the free energy principle, active inference, active learning and so on.", "tokens": [51064, 33247, 736, 11, 498, 291, 1697, 341, 307, 577, 13910, 3652, 589, 11, 550, 291, 917, 493, 4476, 365, 18484, 311, 8665, 295, 1935, 3069, 11, 264, 1737, 2281, 8665, 11, 4967, 38253, 11, 4967, 2539, 293, 370, 322, 13, 51664], "temperature": 0.0, "avg_logprob": -0.113613895986272, "compression_ratio": 1.7418032786885247, "no_speech_prob": 0.1746596395969391}, {"id": 54, "seek": 47420, "start": 474.2, "end": 476.2, "text": " And that's what we're going to focus on.", "tokens": [50364, 400, 300, 311, 437, 321, 434, 516, 281, 1879, 322, 13, 50464], "temperature": 0.0, "avg_logprob": -0.21511483836818385, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.06188848242163658}, {"id": 55, "seek": 47420, "start": 476.2, "end": 489.2, "text": " And the energy function that I'm going to consider, we've already heard mentioned, is the variational free energy or the free energy, which we've already heard very roughly scores surprise.", "tokens": [50464, 400, 264, 2281, 2445, 300, 286, 478, 516, 281, 1949, 11, 321, 600, 1217, 2198, 2835, 11, 307, 264, 3034, 1478, 1737, 2281, 420, 264, 1737, 2281, 11, 597, 321, 600, 1217, 2198, 588, 9810, 13444, 6365, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21511483836818385, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.06188848242163658}, {"id": 56, "seek": 47420, "start": 489.2, "end": 495.2, "text": " It approximates surprise or suprisal and is simplifying assumptions prediction error.", "tokens": [51114, 467, 8542, 1024, 6365, 420, 459, 1424, 271, 304, 293, 307, 6883, 5489, 17695, 17630, 6713, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21511483836818385, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.06188848242163658}, {"id": 57, "seek": 49520, "start": 495.2, "end": 504.2, "text": " So, what we are saying is that we're just in the game of minimising prediction error and more specifically prediction error over time over sequences of behaviour.", "tokens": [50364, 407, 11, 437, 321, 366, 1566, 307, 300, 321, 434, 445, 294, 264, 1216, 295, 4464, 3436, 17630, 6713, 293, 544, 4682, 17630, 6713, 670, 565, 670, 22978, 295, 17229, 13, 50814], "temperature": 0.0, "avg_logprob": -0.13066498438517252, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.060108594596385956}, {"id": 58, "seek": 49520, "start": 504.2, "end": 513.2, "text": " I'm going to quickly go through this because there are lots of interesting connections with existing theories and formulations.", "tokens": [50814, 286, 478, 516, 281, 2661, 352, 807, 341, 570, 456, 366, 3195, 295, 1880, 9271, 365, 6741, 13667, 293, 1254, 4136, 13, 51264], "temperature": 0.0, "avg_logprob": -0.13066498438517252, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.060108594596385956}, {"id": 59, "seek": 49520, "start": 513.2, "end": 521.2, "text": " This is a bit technical. These are both iconic and ironic equations. You'll hear more about those later on.", "tokens": [51264, 639, 307, 257, 857, 6191, 13, 1981, 366, 1293, 15762, 293, 33719, 11787, 13, 509, 603, 1568, 544, 466, 729, 1780, 322, 13, 51664], "temperature": 0.0, "avg_logprob": -0.13066498438517252, "compression_ratio": 1.6583333333333334, "no_speech_prob": 0.060108594596385956}, {"id": 60, "seek": 52120, "start": 522.2, "end": 540.2, "text": " In words, if it's the case that good agents, good people, minimise their free energy, their surprise, their average surprise and their uncertainty, then they must believe that the actions that they emit will minimise expected free energy.", "tokens": [50414, 682, 2283, 11, 498, 309, 311, 264, 1389, 300, 665, 12554, 11, 665, 561, 11, 4464, 908, 641, 1737, 2281, 11, 641, 6365, 11, 641, 4274, 6365, 293, 641, 15697, 11, 550, 436, 1633, 1697, 300, 264, 5909, 300, 436, 32084, 486, 4464, 908, 5176, 1737, 2281, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09870874881744385, "compression_ratio": 1.6413793103448275, "no_speech_prob": 0.010791489854454994}, {"id": 61, "seek": 54020, "start": 540.2, "end": 554.2, "text": " You can write that down very simply in terms of these belief functions here and rearrange them in a way that discloses important links with lots of established formal treatments of behaviour.", "tokens": [50364, 509, 393, 2464, 300, 760, 588, 2935, 294, 2115, 295, 613, 7107, 6828, 510, 293, 39568, 552, 294, 257, 636, 300, 17092, 4201, 1021, 6123, 365, 3195, 295, 7545, 9860, 15795, 295, 17229, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07657265035729659, "compression_ratio": 1.4253731343283582, "no_speech_prob": 0.013377286493778229}, {"id": 62, "seek": 55420, "start": 554.2, "end": 565.2, "text": " I've written the expected free energy associated with any particular policy in terms of its expinsic value here and its epistemic value.", "tokens": [50364, 286, 600, 3720, 264, 5176, 1737, 2281, 6615, 365, 604, 1729, 3897, 294, 2115, 295, 1080, 1278, 1292, 299, 2158, 510, 293, 1080, 2388, 468, 3438, 2158, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11465302220097294, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.02983015961945057}, {"id": 63, "seek": 55420, "start": 565.2, "end": 574.2, "text": " Basically, these things store the surprise about what you predict will happen under a particular behaviour and what you think should happen.", "tokens": [50914, 8537, 11, 613, 721, 3531, 264, 6365, 466, 437, 291, 6069, 486, 1051, 833, 257, 1729, 17229, 293, 437, 291, 519, 820, 1051, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11465302220097294, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.02983015961945057}, {"id": 64, "seek": 55420, "start": 574.2, "end": 578.2, "text": " Your preference is like, I'm going to eat a mouse and I'm not going to be hungry.", "tokens": [51364, 2260, 17502, 307, 411, 11, 286, 478, 516, 281, 1862, 257, 9719, 293, 286, 478, 406, 516, 281, 312, 8067, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11465302220097294, "compression_ratio": 1.5955555555555556, "no_speech_prob": 0.02983015961945057}, {"id": 65, "seek": 57820, "start": 578.2, "end": 582.2, "text": " That's a surprise bit, explicit or expinsic surprise bit.", "tokens": [50364, 663, 311, 257, 6365, 857, 11, 13691, 420, 1278, 1292, 299, 6365, 857, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16636298423589663, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.003645181655883789}, {"id": 66, "seek": 57820, "start": 582.2, "end": 588.2, "text": " There's another sort of average surprise or relative entropy which is called epistemic value.", "tokens": [50564, 821, 311, 1071, 1333, 295, 4274, 6365, 420, 4972, 30867, 597, 307, 1219, 2388, 468, 3438, 2158, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16636298423589663, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.003645181655883789}, {"id": 67, "seek": 57820, "start": 588.2, "end": 593.2, "text": " It's a reduction in uncertainty or the information gain, and that's the key bit.", "tokens": [50864, 467, 311, 257, 11004, 294, 15697, 420, 264, 1589, 6052, 11, 293, 300, 311, 264, 2141, 857, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16636298423589663, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.003645181655883789}, {"id": 68, "seek": 57820, "start": 593.2, "end": 602.2, "text": " It's the epistemic which is missing from classic theories but is part of this formulation of Hamilton's principle of least action.", "tokens": [51114, 467, 311, 264, 2388, 468, 3438, 597, 307, 5361, 490, 7230, 13667, 457, 307, 644, 295, 341, 37642, 295, 18484, 311, 8665, 295, 1935, 3069, 13, 51564], "temperature": 0.0, "avg_logprob": -0.16636298423589663, "compression_ratio": 1.6805555555555556, "no_speech_prob": 0.003645181655883789}, {"id": 69, "seek": 60220, "start": 602.2, "end": 609.2, "text": " That relates very closely to theories of visual salience, of Bayesian surprise.", "tokens": [50364, 663, 16155, 588, 8185, 281, 13667, 295, 5056, 1845, 1182, 11, 295, 7840, 42434, 6365, 13, 50714], "temperature": 0.0, "avg_logprob": -0.15282181593088004, "compression_ratio": 1.62987012987013, "no_speech_prob": 0.00586326839402318}, {"id": 70, "seek": 60220, "start": 609.2, "end": 620.2, "text": " Technically, Bayesian surprise is the divergence of the difference between a prior belief and a posterior belief or a posterior belief to be informed by observations here.", "tokens": [50714, 42494, 11, 7840, 42434, 6365, 307, 264, 47387, 295, 264, 2649, 1296, 257, 4059, 7107, 293, 257, 33529, 7107, 420, 257, 33529, 7107, 281, 312, 11740, 538, 18163, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15282181593088004, "compression_ratio": 1.62987012987013, "no_speech_prob": 0.00586326839402318}, {"id": 71, "seek": 62020, "start": 620.2, "end": 639.2, "text": " What we're saying is that we will choose to act in a way that reduces our uncertainty relative to prior beliefs, looking at data which gives us information that maximally reduces that uncertainty that has the greatest epistemic value or Bayesian surprise.", "tokens": [50364, 708, 321, 434, 1566, 307, 300, 321, 486, 2826, 281, 605, 294, 257, 636, 300, 18081, 527, 15697, 4972, 281, 4059, 13585, 11, 1237, 412, 1412, 597, 2709, 505, 1589, 300, 5138, 379, 18081, 300, 15697, 300, 575, 264, 6636, 2388, 468, 3438, 2158, 420, 7840, 42434, 6365, 13, 51314], "temperature": 0.0, "avg_logprob": -0.11182689666748047, "compression_ratio": 1.5454545454545454, "no_speech_prob": 0.2229350209236145}, {"id": 72, "seek": 63920, "start": 639.2, "end": 651.2, "text": " In fact, mathematically, that's exactly the same as the mutual information between the causes, the hidden states of the world S and the consequences, the outcomes that we actually observe.", "tokens": [50364, 682, 1186, 11, 44003, 11, 300, 311, 2293, 264, 912, 382, 264, 16917, 1589, 1296, 264, 7700, 11, 264, 7633, 4368, 295, 264, 1002, 318, 293, 264, 10098, 11, 264, 10070, 300, 321, 767, 11441, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12058647871017455, "compression_ratio": 1.7445887445887447, "no_speech_prob": 0.1704232543706894}, {"id": 73, "seek": 63920, "start": 651.2, "end": 663.2, "text": " So another way of saying this is that we are subscribing to the principle of maximum information, mutual information or minimum redundancy or maximum information efficiency of the sort articulated by Horace Barlow.", "tokens": [50964, 407, 1071, 636, 295, 1566, 341, 307, 300, 321, 366, 19981, 281, 264, 8665, 295, 6674, 1589, 11, 16917, 1589, 420, 7285, 27830, 6717, 420, 6674, 1589, 10493, 295, 264, 1333, 43322, 538, 10691, 617, 4156, 14107, 13, 51564], "temperature": 0.0, "avg_logprob": -0.12058647871017455, "compression_ratio": 1.7445887445887447, "no_speech_prob": 0.1704232543706894}, {"id": 74, "seek": 66320, "start": 663.2, "end": 671.2, "text": " Always of expressing one particular form of perspective on this underlying functional.", "tokens": [50364, 11270, 295, 22171, 472, 1729, 1254, 295, 4585, 322, 341, 14217, 11745, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1283182018208054, "compression_ratio": 1.5481927710843373, "no_speech_prob": 0.18178397417068481}, {"id": 75, "seek": 66320, "start": 671.2, "end": 683.2, "text": " Another way of thinking about this in the case, if there is no ambiguity, if we actually can observe the states directly, then we can discount this uncertainty term here.", "tokens": [50764, 3996, 636, 295, 1953, 466, 341, 294, 264, 1389, 11, 498, 456, 307, 572, 46519, 11, 498, 321, 767, 393, 11441, 264, 4368, 3838, 11, 550, 321, 393, 11635, 341, 15697, 1433, 510, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1283182018208054, "compression_ratio": 1.5481927710843373, "no_speech_prob": 0.18178397417068481}, {"id": 76, "seek": 68320, "start": 683.2, "end": 693.2, "text": " And what we're left with is something called KL control, which is the state of the art of what people would use in optimal control theory and dynamical systems control.", "tokens": [50364, 400, 437, 321, 434, 1411, 365, 307, 746, 1219, 47991, 1969, 11, 597, 307, 264, 1785, 295, 264, 1523, 295, 437, 561, 576, 764, 294, 16252, 1969, 5261, 293, 5999, 804, 3652, 1969, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1142340359622485, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.5519578456878662}, {"id": 77, "seek": 68320, "start": 693.2, "end": 703.2, "text": " In economics, it's called risk sensitive control. It's minimizing risk. So this is a surprise between what I think will happen and what I want to happen.", "tokens": [50864, 682, 14564, 11, 309, 311, 1219, 3148, 9477, 1969, 13, 467, 311, 46608, 3148, 13, 407, 341, 307, 257, 6365, 1296, 437, 286, 519, 486, 1051, 293, 437, 286, 528, 281, 1051, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1142340359622485, "compression_ratio": 1.5940594059405941, "no_speech_prob": 0.5519578456878662}, {"id": 78, "seek": 70320, "start": 703.2, "end": 713.2, "text": " And if what I think will happen is surprising relation to what I thought was going to happen, then I have a high degree of surprise, a high degree of risk, and I want to minimize that.", "tokens": [50364, 400, 498, 437, 286, 519, 486, 1051, 307, 8830, 9721, 281, 437, 286, 1194, 390, 516, 281, 1051, 11, 550, 286, 362, 257, 1090, 4314, 295, 6365, 11, 257, 1090, 4314, 295, 3148, 11, 293, 286, 528, 281, 17522, 300, 13, 50864], "temperature": 0.0, "avg_logprob": -0.14555409821597012, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.2247699648141861}, {"id": 79, "seek": 70320, "start": 713.2, "end": 728.2, "text": " And then finally, if there's no ambiguity or there's no risk, then we reduce to classical expected utility theory or all the sorts of theories that reinforcement depends upon this maximizing our preferred outcomes there.", "tokens": [50864, 400, 550, 2721, 11, 498, 456, 311, 572, 46519, 420, 456, 311, 572, 3148, 11, 550, 321, 5407, 281, 13735, 5176, 14877, 5261, 420, 439, 264, 7527, 295, 13667, 300, 29280, 5946, 3564, 341, 5138, 3319, 527, 16494, 10070, 456, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14555409821597012, "compression_ratio": 1.6945606694560669, "no_speech_prob": 0.2247699648141861}, {"id": 80, "seek": 72820, "start": 728.2, "end": 738.2, "text": " So, clearly, in order to be surprised, we have to have predictions against which we can match outcomes to score that surprise.", "tokens": [50364, 407, 11, 4448, 11, 294, 1668, 281, 312, 6100, 11, 321, 362, 281, 362, 21264, 1970, 597, 321, 393, 2995, 10070, 281, 6175, 300, 6365, 13, 50864], "temperature": 0.0, "avg_logprob": -0.13387441074146944, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.010176731273531914}, {"id": 81, "seek": 72820, "start": 738.2, "end": 755.2, "text": " And this brings us to generative models. And the departure that I promised you from what people currently understand in terms of predictive coding and what I'm going to talk about for the next few minutes is I'm going to formulate generative models not for continuous state space", "tokens": [50864, 400, 341, 5607, 505, 281, 1337, 1166, 5245, 13, 400, 264, 25866, 300, 286, 10768, 291, 490, 437, 561, 4362, 1223, 294, 2115, 295, 35521, 17720, 293, 437, 286, 478, 516, 281, 751, 466, 337, 264, 958, 1326, 2077, 307, 286, 478, 516, 281, 47881, 1337, 1166, 5245, 406, 337, 10957, 1785, 1901, 51714], "temperature": 0.0, "avg_logprob": -0.13387441074146944, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.010176731273531914}, {"id": 82, "seek": 75520, "start": 755.2, "end": 767.2, "text": " of the sorts used in predictive coding of, say, visual angles or content or acoustics, but generative models in which we can label the entire world in terms of a number of discrete states.", "tokens": [50364, 295, 264, 7527, 1143, 294, 35521, 17720, 295, 11, 584, 11, 5056, 14708, 420, 2701, 420, 22740, 1167, 11, 457, 1337, 1166, 5245, 294, 597, 321, 393, 7645, 264, 2302, 1002, 294, 2115, 295, 257, 1230, 295, 27706, 4368, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10359815804355116, "compression_ratio": 1.708133971291866, "no_speech_prob": 0.04477665573358536}, {"id": 83, "seek": 75520, "start": 767.2, "end": 776.2, "text": " So these are generative models for discrete state spaces, and they don't normally have the look and feel of predictive coding, but my story will be, is in fact they do.", "tokens": [50964, 407, 613, 366, 1337, 1166, 5245, 337, 27706, 1785, 7673, 11, 293, 436, 500, 380, 5646, 362, 264, 574, 293, 841, 295, 35521, 17720, 11, 457, 452, 1657, 486, 312, 11, 307, 294, 1186, 436, 360, 13, 51414], "temperature": 0.0, "avg_logprob": -0.10359815804355116, "compression_ratio": 1.708133971291866, "no_speech_prob": 0.04477665573358536}, {"id": 84, "seek": 77620, "start": 776.2, "end": 789.2, "text": " They are actually formally very, very similar to the sorts of schemes that we understand in terms of top-down predictions and bottom-up prediction errors in hierarchical message-passing allopredicative coding in the visual cortex.", "tokens": [50364, 814, 366, 767, 25983, 588, 11, 588, 2531, 281, 264, 7527, 295, 26954, 300, 321, 1223, 294, 2115, 295, 1192, 12, 5093, 21264, 293, 2767, 12, 1010, 17630, 13603, 294, 35250, 804, 3636, 12, 9216, 278, 439, 404, 986, 299, 1166, 17720, 294, 264, 5056, 33312, 13, 51014], "temperature": 0.0, "avg_logprob": -0.16948382059733072, "compression_ratio": 1.4743589743589745, "no_speech_prob": 0.12045560777187347}, {"id": 85, "seek": 78920, "start": 790.2, "end": 797.2, "text": " So in these models, all we have, this is not, ignore the equations, but it's focused on this graphical model here.", "tokens": [50414, 407, 294, 613, 5245, 11, 439, 321, 362, 11, 341, 307, 406, 11, 11200, 264, 11787, 11, 457, 309, 311, 5178, 322, 341, 35942, 2316, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.14029373357325425, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.5046300292015076}, {"id": 86, "seek": 78920, "start": 797.2, "end": 811.2, "text": " What we're saying is that the world unfolds in one of many, many states, and the transitions from one state of the world to the next state of the world are encoded by probability transitions that themselves depend upon how we act.", "tokens": [50764, 708, 321, 434, 1566, 307, 300, 264, 1002, 17980, 82, 294, 472, 295, 867, 11, 867, 4368, 11, 293, 264, 23767, 490, 472, 1785, 295, 264, 1002, 281, 264, 958, 1785, 295, 264, 1002, 366, 2058, 12340, 538, 8482, 23767, 300, 2969, 5672, 3564, 577, 321, 605, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14029373357325425, "compression_ratio": 1.674757281553398, "no_speech_prob": 0.5046300292015076}, {"id": 87, "seek": 81120, "start": 811.2, "end": 820.2, "text": " They depend upon the policies that we choose, and we have a certain confidence in those policies, denoted by their precision or inverse temperature beta here.", "tokens": [50364, 814, 5672, 3564, 264, 7657, 300, 321, 2826, 11, 293, 321, 362, 257, 1629, 6687, 294, 729, 7657, 11, 1441, 23325, 538, 641, 18356, 420, 17340, 4292, 9861, 510, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10777579035077776, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.0281143207103014}, {"id": 88, "seek": 81120, "start": 820.2, "end": 837.2, "text": " So if we knew the probability transitions or the transitions from time to time of the states, we can generate a sequence or trajectory of states, and each state, at each point in time, generates an outcome through this likelihood of matrix A.", "tokens": [50814, 407, 498, 321, 2586, 264, 8482, 23767, 420, 264, 23767, 490, 565, 281, 565, 295, 264, 4368, 11, 321, 393, 8460, 257, 8310, 420, 21512, 295, 4368, 11, 293, 1184, 1785, 11, 412, 1184, 935, 294, 565, 11, 23815, 364, 9700, 807, 341, 22119, 295, 8141, 316, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10777579035077776, "compression_ratio": 1.6991525423728813, "no_speech_prob": 0.0281143207103014}, {"id": 89, "seek": 83720, "start": 837.2, "end": 847.2, "text": " And that's it. That's the generative model. The world has states, they unfold, and each state generates an outcome that's observable.", "tokens": [50364, 400, 300, 311, 309, 13, 663, 311, 264, 1337, 1166, 2316, 13, 440, 1002, 575, 4368, 11, 436, 17980, 11, 293, 1184, 1785, 23815, 364, 9700, 300, 311, 9951, 712, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16022291550269493, "compression_ratio": 1.451851851851852, "no_speech_prob": 0.060981765389442444}, {"id": 90, "seek": 83720, "start": 847.2, "end": 852.2, "text": " And that's the basis of everything else that I'm going to say.", "tokens": [50864, 400, 300, 311, 264, 5143, 295, 1203, 1646, 300, 286, 478, 516, 281, 584, 13, 51114], "temperature": 0.0, "avg_logprob": -0.16022291550269493, "compression_ratio": 1.451851851851852, "no_speech_prob": 0.060981765389442444}, {"id": 91, "seek": 85220, "start": 852.2, "end": 873.2, "text": " If I'm now given a generative model, what I can do is I can evaluate the free energy of my beliefs under that generative model, and I can then minimize everything with respect to that proxy for surprise or uncertainty, namely the expected free energy.", "tokens": [50364, 759, 286, 478, 586, 2212, 257, 1337, 1166, 2316, 11, 437, 286, 393, 360, 307, 286, 393, 13059, 264, 1737, 2281, 295, 452, 13585, 833, 300, 1337, 1166, 2316, 11, 293, 286, 393, 550, 17522, 1203, 365, 3104, 281, 300, 29690, 337, 6365, 420, 15697, 11, 20926, 264, 5176, 1737, 2281, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09171707289559501, "compression_ratio": 1.539877300613497, "no_speech_prob": 0.33353516459465027}, {"id": 92, "seek": 87320, "start": 873.2, "end": 885.2, "text": " And I can write down equations or solutions that tell me how an optimal agent person would behave in a sort of Bayesian sense.", "tokens": [50364, 400, 286, 393, 2464, 760, 11787, 420, 6547, 300, 980, 385, 577, 364, 16252, 9461, 954, 576, 15158, 294, 257, 1333, 295, 7840, 42434, 2020, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06689320882161458, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.5471611022949219}, {"id": 93, "seek": 87320, "start": 885.2, "end": 890.2, "text": " And these are the solutions to the equations expressed in terms of the parameters of that model.", "tokens": [50964, 400, 613, 366, 264, 6547, 281, 264, 11787, 12675, 294, 2115, 295, 264, 9834, 295, 300, 2316, 13, 51214], "temperature": 0.0, "avg_logprob": -0.06689320882161458, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.5471611022949219}, {"id": 94, "seek": 87320, "start": 890.2, "end": 898.2, "text": " So A was this mapping from states of the world to outcomes, and B was the mapping between subsequent hidden states.", "tokens": [51214, 407, 316, 390, 341, 18350, 490, 4368, 295, 264, 1002, 281, 10070, 11, 293, 363, 390, 264, 18350, 1296, 19962, 7633, 4368, 13, 51614], "temperature": 0.0, "avg_logprob": -0.06689320882161458, "compression_ratio": 1.645631067961165, "no_speech_prob": 0.5471611022949219}, {"id": 95, "seek": 89820, "start": 898.2, "end": 907.2, "text": " And despite the complicated nature of the equations on the previous slide, the actual updates, the solutions are incredibly simple.", "tokens": [50364, 400, 7228, 264, 6179, 3687, 295, 264, 11787, 322, 264, 3894, 4137, 11, 264, 3539, 9205, 11, 264, 6547, 366, 6252, 2199, 13, 50814], "temperature": 0.0, "avg_logprob": -0.07966664433479309, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.02927766926586628}, {"id": 96, "seek": 89820, "start": 907.2, "end": 912.2, "text": " And furthermore, they look very much like the sorts of things that the brain does.", "tokens": [50814, 400, 3052, 3138, 11, 436, 574, 588, 709, 411, 264, 7527, 295, 721, 300, 264, 3567, 775, 13, 51064], "temperature": 0.0, "avg_logprob": -0.07966664433479309, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.02927766926586628}, {"id": 97, "seek": 89820, "start": 912.2, "end": 923.2, "text": " So, for example, expected states of the world are a nonlinear sigmoid function of linear mixtures of expected states of the world and observations.", "tokens": [51064, 407, 11, 337, 1365, 11, 5176, 4368, 295, 264, 1002, 366, 257, 2107, 28263, 4556, 3280, 327, 2445, 295, 8213, 2752, 37610, 295, 5176, 4368, 295, 264, 1002, 293, 18163, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07966664433479309, "compression_ratio": 1.723809523809524, "no_speech_prob": 0.02927766926586628}, {"id": 98, "seek": 92320, "start": 923.2, "end": 932.2, "text": " So we're mixing together evidence from outcomes and our beliefs about the states of the world to update our beliefs about the current state of the world.", "tokens": [50364, 407, 321, 434, 11983, 1214, 4467, 490, 10070, 293, 527, 13585, 466, 264, 4368, 295, 264, 1002, 281, 5623, 527, 13585, 466, 264, 2190, 1785, 295, 264, 1002, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08921585764203753, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.029825815930962563}, {"id": 99, "seek": 92320, "start": 932.2, "end": 949.2, "text": " Our beliefs about what we're going to do next, our policy pie here, is this a softmax function of the expected free energy weighted by an inverse temperature parameter that you will see we associate with dopamine, a classical softmax response rule.", "tokens": [50814, 2621, 13585, 466, 437, 321, 434, 516, 281, 360, 958, 11, 527, 3897, 1730, 510, 11, 307, 341, 257, 2787, 41167, 2445, 295, 264, 5176, 1737, 2281, 32807, 538, 364, 17340, 4292, 13075, 300, 291, 486, 536, 321, 14644, 365, 37219, 11, 257, 13735, 2787, 41167, 4134, 4978, 13, 51664], "temperature": 0.0, "avg_logprob": -0.08921585764203753, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.029825815930962563}, {"id": 100, "seek": 94920, "start": 950.2, "end": 958.2, "text": " If you're not familiar with that, that's what people in economics and choice behaviour use for those people who deal more with perception.", "tokens": [50414, 759, 291, 434, 406, 4963, 365, 300, 11, 300, 311, 437, 561, 294, 14564, 293, 3922, 17229, 764, 337, 729, 561, 567, 2028, 544, 365, 12860, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10816109317472611, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.0036731399595737457}, {"id": 101, "seek": 94920, "start": 958.2, "end": 974.2, "text": " We also have a model of incentive salience. The confidence or the precision or the inverse temperature associated with our beliefs about action now becomes, as a Bayes optimal solution, that depends upon the goodness of a policy or the negative goodness, the expected free energy here.", "tokens": [50814, 492, 611, 362, 257, 2316, 295, 22346, 1845, 1182, 13, 440, 6687, 420, 264, 18356, 420, 264, 17340, 4292, 6615, 365, 527, 13585, 466, 3069, 586, 3643, 11, 382, 257, 7840, 279, 16252, 3827, 11, 300, 5946, 3564, 264, 8387, 295, 257, 3897, 420, 264, 3671, 8387, 11, 264, 5176, 1737, 2281, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10816109317472611, "compression_ratio": 1.6307692307692307, "no_speech_prob": 0.0036731399595737457}, {"id": 102, "seek": 97420, "start": 974.2, "end": 983.2, "text": " And the form of these equations speaks to a rough anatomy of computations in the brain, a computational anatomy.", "tokens": [50364, 400, 264, 1254, 295, 613, 11787, 10789, 281, 257, 5903, 31566, 295, 2807, 763, 294, 264, 3567, 11, 257, 28270, 31566, 13, 50814], "temperature": 0.0, "avg_logprob": -0.09333241660639925, "compression_ratio": 1.5906040268456376, "no_speech_prob": 0.004138259217143059}, {"id": 103, "seek": 97420, "start": 983.2, "end": 992.2, "text": " And it sort of goes like this, where we have these equations dictate what each update needs to know about the other updates.", "tokens": [50814, 400, 309, 1333, 295, 1709, 411, 341, 11, 689, 321, 362, 613, 11787, 36071, 437, 1184, 5623, 2203, 281, 458, 466, 264, 661, 9205, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09333241660639925, "compression_ratio": 1.5906040268456376, "no_speech_prob": 0.004138259217143059}, {"id": 104, "seek": 99220, "start": 992.2, "end": 1008.2, "text": " So, basically, it prescribes a connectome for the exchange of information or sufficient statistics that is implied by placing the Hamilton's principle of least action on the simplest sort of generating model that you can imagine.", "tokens": [50364, 407, 11, 1936, 11, 309, 1183, 1142, 6446, 257, 1745, 423, 337, 264, 7742, 295, 1589, 420, 11563, 12523, 300, 307, 32614, 538, 17221, 264, 18484, 311, 8665, 295, 1935, 3069, 322, 264, 22811, 1333, 295, 17746, 2316, 300, 291, 393, 3811, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1310824231898531, "compression_ratio": 1.4774193548387098, "no_speech_prob": 0.008557572960853577}, {"id": 105, "seek": 100820, "start": 1008.2, "end": 1022.2, "text": " And that's the sort of anatomy we have here. Outcomes, expected states, expected policies, the goodness or the expected free energy of policies, the precision of policies, states in the future, which prescribe action.", "tokens": [50364, 400, 300, 311, 264, 1333, 295, 31566, 321, 362, 510, 13, 5925, 9055, 11, 5176, 4368, 11, 5176, 7657, 11, 264, 8387, 420, 264, 5176, 1737, 2281, 295, 7657, 11, 264, 18356, 295, 7657, 11, 4368, 294, 264, 2027, 11, 597, 49292, 3069, 13, 51064], "temperature": 0.0, "avg_logprob": -0.09828180290130248, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.2246030569076538}, {"id": 106, "seek": 100820, "start": 1022.2, "end": 1028.2, "text": " So, I won't go through that, but I'll just give you a more heuristic version of that one. So, what those equations tell us.", "tokens": [51064, 407, 11, 286, 1582, 380, 352, 807, 300, 11, 457, 286, 603, 445, 976, 291, 257, 544, 415, 374, 3142, 3037, 295, 300, 472, 13, 407, 11, 437, 729, 11787, 980, 505, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09828180290130248, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.2246030569076538}, {"id": 107, "seek": 102820, "start": 1028.2, "end": 1035.2, "text": " So, this is like a very top-down argument. It's not, you know, let's think about how the brain works and come up with some hypotheses.", "tokens": [50364, 407, 11, 341, 307, 411, 257, 588, 1192, 12, 5093, 6770, 13, 467, 311, 406, 11, 291, 458, 11, 718, 311, 519, 466, 577, 264, 3567, 1985, 293, 808, 493, 365, 512, 49969, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11346483953071362, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.5172664523124695}, {"id": 108, "seek": 102820, "start": 1035.2, "end": 1044.2, "text": " This unfolds or unravels from, impacts from, just applying Hamilton's principle of least action to a very simple generating model.", "tokens": [50714, 639, 17980, 82, 420, 40507, 82, 490, 11, 11606, 490, 11, 445, 9275, 18484, 311, 8665, 295, 1935, 3069, 281, 257, 588, 2199, 17746, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11346483953071362, "compression_ratio": 1.424731182795699, "no_speech_prob": 0.5172664523124695}, {"id": 109, "seek": 104420, "start": 1044.2, "end": 1056.2, "text": " And what it tells us is that sensory input comes in, say, at the back of the brain. It informs and updates expectations about hidden states of the world, sometimes referred to as state estimation.", "tokens": [50364, 400, 437, 309, 5112, 505, 307, 300, 27233, 4846, 1487, 294, 11, 584, 11, 412, 264, 646, 295, 264, 3567, 13, 467, 45320, 293, 9205, 9843, 466, 7633, 4368, 295, 264, 1002, 11, 2171, 10839, 281, 382, 1785, 35701, 13, 50964], "temperature": 0.0, "avg_logprob": -0.08744888995067183, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.3857439160346985}, {"id": 110, "seek": 104420, "start": 1056.2, "end": 1068.2, "text": " They are associated with a free energy or a surprise that is combined with an evaluation of those states in relation to prior preferences and their potential reduction of uncertainty, their epistemic value.", "tokens": [50964, 814, 366, 6615, 365, 257, 1737, 2281, 420, 257, 6365, 300, 307, 9354, 365, 364, 13344, 295, 729, 4368, 294, 9721, 281, 4059, 21910, 293, 641, 3995, 11004, 295, 15697, 11, 641, 2388, 468, 3438, 2158, 13, 51564], "temperature": 0.0, "avg_logprob": -0.08744888995067183, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.3857439160346985}, {"id": 111, "seek": 106820, "start": 1068.2, "end": 1078.2, "text": " They are combined to give us beliefs about the policy that we are currently pursuing. We have a certain confidence in that policy.", "tokens": [50364, 814, 366, 9354, 281, 976, 505, 13585, 466, 264, 3897, 300, 321, 366, 4362, 20222, 13, 492, 362, 257, 1629, 6687, 294, 300, 3897, 13, 50864], "temperature": 0.0, "avg_logprob": -0.07082900866656236, "compression_ratio": 1.6733668341708543, "no_speech_prob": 0.0923294946551323}, {"id": 112, "seek": 106820, "start": 1078.2, "end": 1088.2, "text": " And then those policies are used to weight all the different states conditioned upon what we are currently doing to give us the best estimate of what's going to happen next, the next state of the world.", "tokens": [50864, 400, 550, 729, 7657, 366, 1143, 281, 3364, 439, 264, 819, 4368, 35833, 3564, 437, 321, 366, 4362, 884, 281, 976, 505, 264, 1151, 12539, 295, 437, 311, 516, 281, 1051, 958, 11, 264, 958, 1785, 295, 264, 1002, 13, 51364], "temperature": 0.0, "avg_logprob": -0.07082900866656236, "compression_ratio": 1.6733668341708543, "no_speech_prob": 0.0923294946551323}, {"id": 113, "seek": 108820, "start": 1088.2, "end": 1102.2, "text": " And if we know that, then we can choose the action that brings about, that realises our expectations, our predictions about the next state of the world, that action solicits a new observation from the environment and the cycle begins again.", "tokens": [50364, 400, 498, 321, 458, 300, 11, 550, 321, 393, 2826, 264, 3069, 300, 5607, 466, 11, 300, 957, 3598, 527, 9843, 11, 527, 21264, 466, 264, 958, 1785, 295, 264, 1002, 11, 300, 3069, 23665, 1208, 257, 777, 14816, 490, 264, 2823, 293, 264, 6586, 7338, 797, 13, 51064], "temperature": 0.0, "avg_logprob": -0.0699759630056528, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.25604966282844543}, {"id": 114, "seek": 108820, "start": 1102.2, "end": 1109.2, "text": " So, we have a perception action cycle that falls out of the minimisation scheme that we've just been talking about.", "tokens": [51064, 407, 11, 321, 362, 257, 12860, 3069, 6586, 300, 8804, 484, 295, 264, 4464, 7623, 12232, 300, 321, 600, 445, 668, 1417, 466, 13, 51414], "temperature": 0.0, "avg_logprob": -0.0699759630056528, "compression_ratio": 1.679245283018868, "no_speech_prob": 0.25604966282844543}, {"id": 115, "seek": 110920, "start": 1109.2, "end": 1120.2, "text": " So, very briefly, I'm just going to show you how that sort of thing works with a series of examples, and then hopefully I'll turn it over to you to see what you want to talk about.", "tokens": [50364, 407, 11, 588, 10515, 11, 286, 478, 445, 516, 281, 855, 291, 577, 300, 1333, 295, 551, 1985, 365, 257, 2638, 295, 5110, 11, 293, 550, 4696, 286, 603, 1261, 309, 670, 281, 291, 281, 536, 437, 291, 528, 281, 751, 466, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07741199029940311, "compression_ratio": 1.6812749003984064, "no_speech_prob": 0.28758421540260315}, {"id": 116, "seek": 110920, "start": 1120.2, "end": 1134.2, "text": " The first example is just a very simple simulation of foraging in a two-arm maze. So, in this example, there's a little rat here, and there are rewards on the right and the left arms of the maze, but the rat doesn't know where the reward is.", "tokens": [50914, 440, 700, 1365, 307, 445, 257, 588, 2199, 16575, 295, 337, 3568, 294, 257, 732, 12, 4452, 33032, 13, 407, 11, 294, 341, 1365, 11, 456, 311, 257, 707, 5937, 510, 11, 293, 456, 366, 17203, 322, 264, 558, 293, 264, 1411, 5812, 295, 264, 33032, 11, 457, 264, 5937, 1177, 380, 458, 689, 264, 7782, 307, 13, 51614], "temperature": 0.0, "avg_logprob": -0.07741199029940311, "compression_ratio": 1.6812749003984064, "no_speech_prob": 0.28758421540260315}, {"id": 117, "seek": 113420, "start": 1134.2, "end": 1144.2, "text": " There's also an informative queue at the bottom of the maze here, and if it went to solicit that queue, it would then know where the reward was, and it could only make two moves.", "tokens": [50364, 821, 311, 611, 364, 27759, 18639, 412, 264, 2767, 295, 264, 33032, 510, 11, 293, 498, 309, 1437, 281, 23665, 270, 300, 18639, 11, 309, 576, 550, 458, 689, 264, 7782, 390, 11, 293, 309, 727, 787, 652, 732, 6067, 13, 50864], "temperature": 0.0, "avg_logprob": -0.10210099390574864, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.04549684375524521}, {"id": 118, "seek": 113420, "start": 1144.2, "end": 1162.2, "text": " So, it can either take a chance and go to one of the other top arms, or it can be a bit more clever and resolve any uncertainty about the context it's currently operating in, which arm is baited, and go and retrieve the epistemic value of the informative queue and then make an informed decision.", "tokens": [50864, 407, 11, 309, 393, 2139, 747, 257, 2931, 293, 352, 281, 472, 295, 264, 661, 1192, 5812, 11, 420, 309, 393, 312, 257, 857, 544, 13494, 293, 14151, 604, 15697, 466, 264, 4319, 309, 311, 4362, 7447, 294, 11, 597, 3726, 307, 16865, 292, 11, 293, 352, 293, 30254, 264, 2388, 468, 3438, 2158, 295, 264, 27759, 18639, 293, 550, 652, 364, 11740, 3537, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10210099390574864, "compression_ratio": 1.7857142857142858, "no_speech_prob": 0.04549684375524521}, {"id": 119, "seek": 116220, "start": 1162.2, "end": 1176.2, "text": " So, this is exactly the searching that you were talking about before, scaling your environment, knowing where you are, resolve your epistemic, solve the epistemic problem, and then turn to your prior preferences or your pragmatics.", "tokens": [50364, 407, 11, 341, 307, 2293, 264, 10808, 300, 291, 645, 1417, 466, 949, 11, 21589, 428, 2823, 11, 5276, 689, 291, 366, 11, 14151, 428, 2388, 468, 3438, 11, 5039, 264, 2388, 468, 3438, 1154, 11, 293, 550, 1261, 281, 428, 4059, 21910, 420, 428, 33394, 15677, 1167, 13, 51064], "temperature": 0.0, "avg_logprob": -0.08667664257985241, "compression_ratio": 1.5503355704697988, "no_speech_prob": 0.016134781762957573}, {"id": 120, "seek": 117620, "start": 1176.2, "end": 1199.2, "text": " You can write this model down in very simple terms of these A and B matrices here. There's partial reinforcement here, and the C matrix here just denotes the preferences in terms of what sorts of states this rat thinks it should occupy, basically thinks it should be in the baited arm and not in the unbaited arm.", "tokens": [50364, 509, 393, 2464, 341, 2316, 760, 294, 588, 2199, 2115, 295, 613, 316, 293, 363, 32284, 510, 13, 821, 311, 14641, 29280, 510, 11, 293, 264, 383, 8141, 510, 445, 1441, 17251, 264, 21910, 294, 2115, 295, 437, 7527, 295, 4368, 341, 5937, 7309, 309, 820, 30645, 11, 1936, 7309, 309, 820, 312, 294, 264, 16865, 292, 3726, 293, 406, 294, 264, 517, 41274, 292, 3726, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13154075515102331, "compression_ratio": 1.6134020618556701, "no_speech_prob": 0.3998633027076721}, {"id": 121, "seek": 119920, "start": 1199.2, "end": 1225.2, "text": " That's all it's saying here, with minus threes and plus threes on the upper arms that are baited. And if we do that, and we just integrate those solutions that I told you before, we actually generate very realistic behaviour, summarised here in terms of the expected policy and the policies that this agent or this little animal can entertain.", "tokens": [50364, 663, 311, 439, 309, 311, 1566, 510, 11, 365, 3175, 258, 4856, 293, 1804, 258, 4856, 322, 264, 6597, 5812, 300, 366, 16865, 292, 13, 400, 498, 321, 360, 300, 11, 293, 321, 445, 13365, 729, 6547, 300, 286, 1907, 291, 949, 11, 321, 767, 8460, 588, 12465, 17229, 11, 14611, 2640, 510, 294, 2115, 295, 264, 5176, 3897, 293, 264, 7657, 300, 341, 9461, 420, 341, 707, 5496, 393, 7655, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14767092152645714, "compression_ratio": 1.602803738317757, "no_speech_prob": 0.14698491990566254}, {"id": 122, "seek": 122520, "start": 1225.2, "end": 1243.2, "text": " It stays there and then goes to one of the three arms, or it goes to one of the two arms, or it goes to the bottom and then goes to any of the three arms. So there are eight policies here.", "tokens": [50364, 467, 10834, 456, 293, 550, 1709, 281, 472, 295, 264, 1045, 5812, 11, 420, 309, 1709, 281, 472, 295, 264, 732, 5812, 11, 420, 309, 1709, 281, 264, 2767, 293, 550, 1709, 281, 604, 295, 264, 1045, 5812, 13, 407, 456, 366, 3180, 7657, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1371559715270996, "compression_ratio": 1.8613861386138615, "no_speech_prob": 0.08860069513320923}, {"id": 123, "seek": 124320, "start": 1243.2, "end": 1260.2, "text": " And what it does in the first instance is because it doesn't know where the reward is. It gets the cue and then obtains its reward. What we've done here is actually baited the left arm all the time.", "tokens": [50364, 400, 437, 309, 775, 294, 264, 700, 5197, 307, 570, 309, 1177, 380, 458, 689, 264, 7782, 307, 13, 467, 2170, 264, 22656, 293, 550, 7464, 2315, 1080, 7782, 13, 708, 321, 600, 1096, 510, 307, 767, 16865, 292, 264, 1411, 3726, 439, 264, 565, 13, 51214], "temperature": 0.0, "avg_logprob": -0.12146618843078613, "compression_ratio": 1.4558823529411764, "no_speech_prob": 0.03889434039592743}, {"id": 124, "seek": 126020, "start": 1260.2, "end": 1278.2, "text": " So slowly it accumulates evidence that, in fact, the reward's always on this side here. So as time goes on, it actually switches and learns, and it's probably better to avoid or dispense with the epistemic move and go directly to the reward.", "tokens": [50364, 407, 5692, 309, 12989, 26192, 4467, 300, 11, 294, 1186, 11, 264, 7782, 311, 1009, 322, 341, 1252, 510, 13, 407, 382, 565, 1709, 322, 11, 309, 767, 19458, 293, 27152, 11, 293, 309, 311, 1391, 1101, 281, 5042, 420, 4920, 1288, 365, 264, 2388, 468, 3438, 1286, 293, 352, 3838, 281, 264, 7782, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11378961498454465, "compression_ratio": 1.4695121951219512, "no_speech_prob": 0.01401115208864212}, {"id": 125, "seek": 127820, "start": 1278.2, "end": 1297.2, "text": " And it starts doing that after about 20 or 30 trials here, at which point its reaction times, and this is the actual floating point operations of the scheme, decrease. And because the goodness of a policy is this path integral, it's actually spent more time being rewarded.", "tokens": [50364, 400, 309, 3719, 884, 300, 934, 466, 945, 420, 2217, 12450, 510, 11, 412, 597, 935, 1080, 5480, 1413, 11, 293, 341, 307, 264, 3539, 12607, 935, 7705, 295, 264, 12232, 11, 11514, 13, 400, 570, 264, 8387, 295, 257, 3897, 307, 341, 3100, 11573, 11, 309, 311, 767, 4418, 544, 565, 885, 29105, 13, 51314], "temperature": 0.0, "avg_logprob": -0.12185287475585938, "compression_ratio": 1.5082872928176796, "no_speech_prob": 0.09200049936771393}, {"id": 126, "seek": 129720, "start": 1297.2, "end": 1311.2, "text": " So if you like, the payoff also increases by going straight there. So this prescribes good policies, and it can be used to simulate nice behaviors of the sort you've seen experimentally.", "tokens": [50364, 407, 498, 291, 411, 11, 264, 46547, 611, 8637, 538, 516, 2997, 456, 13, 407, 341, 1183, 1142, 6446, 665, 7657, 11, 293, 309, 393, 312, 1143, 281, 27817, 1481, 15501, 295, 264, 1333, 291, 600, 1612, 5120, 379, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1610773043199019, "compression_ratio": 1.3478260869565217, "no_speech_prob": 0.09236744046211243}, {"id": 127, "seek": 131120, "start": 1311.2, "end": 1329.2, "text": " But what I want to do finally is just connect that to neurophysiology and neuroanatomy. But to do that, I have to have a process theory. I have to have a theory which says this particular neuroactivity or this particular connection strength corresponds to this quantity in the model.", "tokens": [50364, 583, 437, 286, 528, 281, 360, 2721, 307, 445, 1745, 300, 281, 16499, 950, 749, 46457, 293, 16499, 282, 267, 8488, 13, 583, 281, 360, 300, 11, 286, 362, 281, 362, 257, 1399, 5261, 13, 286, 362, 281, 362, 257, 5261, 597, 1619, 341, 1729, 16499, 578, 4253, 420, 341, 1729, 4984, 3800, 23249, 281, 341, 11275, 294, 264, 2316, 13, 51264], "temperature": 0.0, "avg_logprob": -0.09160485634436974, "compression_ratio": 1.715151515151515, "no_speech_prob": 0.021090159192681313}, {"id": 128, "seek": 132920, "start": 1329.2, "end": 1354.2, "text": " And I have to have a process in play that is neurarily plausible. And the way that we're going to do that is just take those update equations that we've seen before, and instead of just writing down the solutions mathematically, I'm going to recast the solutions in terms of a gradient descent or a hill climbing, or actually a hill descent here.", "tokens": [50364, 400, 286, 362, 281, 362, 257, 1399, 294, 862, 300, 307, 12087, 3289, 39925, 13, 400, 264, 636, 300, 321, 434, 516, 281, 360, 300, 307, 445, 747, 729, 5623, 11787, 300, 321, 600, 1612, 949, 11, 293, 2602, 295, 445, 3579, 760, 264, 6547, 44003, 11, 286, 478, 516, 281, 850, 525, 264, 6547, 294, 2115, 295, 257, 16235, 23475, 420, 257, 10997, 14780, 11, 420, 767, 257, 10997, 23475, 510, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1495131455458604, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.12410957366228104}, {"id": 129, "seek": 135420, "start": 1354.2, "end": 1370.2, "text": " So this is a standard way of optimizing something. If you've got a quantity you want to minimize, you just go downhill until it stops getting smaller. And if I do that, I can write down exactly the same scheme in terms of differential equations on expected states of the world.", "tokens": [50364, 407, 341, 307, 257, 3832, 636, 295, 40425, 746, 13, 759, 291, 600, 658, 257, 11275, 291, 528, 281, 17522, 11, 291, 445, 352, 29929, 1826, 309, 10094, 1242, 4356, 13, 400, 498, 286, 360, 300, 11, 286, 393, 2464, 760, 2293, 264, 912, 12232, 294, 2115, 295, 15756, 11787, 322, 5176, 4368, 295, 264, 1002, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06911771023859743, "compression_ratio": 1.4656084656084656, "no_speech_prob": 0.030252555385231972}, {"id": 130, "seek": 137020, "start": 1370.2, "end": 1390.2, "text": " It's very similar form, but here that's a rate of change of activity, which is now a nonlinear function of linear mixtures of expectations about states of the world and the observations. And in doing that, I've created a dynamical system that now has as much closer to the look and feel of a neuronal system.", "tokens": [50364, 467, 311, 588, 2531, 1254, 11, 457, 510, 300, 311, 257, 3314, 295, 1319, 295, 5191, 11, 597, 307, 586, 257, 2107, 28263, 2445, 295, 8213, 2752, 37610, 295, 9843, 466, 4368, 295, 264, 1002, 293, 264, 18163, 13, 400, 294, 884, 300, 11, 286, 600, 2942, 257, 5999, 804, 1185, 300, 586, 575, 382, 709, 4966, 281, 264, 574, 293, 841, 295, 257, 12087, 21523, 1185, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1410969098409017, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.36273035407066345}, {"id": 131, "seek": 139020, "start": 1391.2, "end": 1409.2, "text": " And that now enables me to look at the dynamics that underlie the behavior. And these are the dynamics here, and we can basically break these into inference and state estimation in terms of the updates or the fluctuations in the states as new evidence comes along.", "tokens": [50414, 400, 300, 586, 17077, 385, 281, 574, 412, 264, 15679, 300, 833, 6302, 264, 5223, 13, 400, 613, 366, 264, 15679, 510, 11, 293, 321, 393, 1936, 1821, 613, 666, 38253, 293, 1785, 35701, 294, 2115, 295, 264, 9205, 420, 264, 45276, 294, 264, 4368, 382, 777, 4467, 1487, 2051, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10646528764204545, "compression_ratio": 1.6097560975609757, "no_speech_prob": 0.19118954241275787}, {"id": 132, "seek": 140920, "start": 1409.2, "end": 1425.2, "text": " Policy selection that we've already seen with our softmax response rule, and learning as we accumulate from trial to trial evidence about particular states or contingents of the world in this instance that the left hand arm of the maze was always baited.", "tokens": [50364, 21708, 9450, 300, 321, 600, 1217, 1612, 365, 527, 2787, 41167, 4134, 4978, 11, 293, 2539, 382, 321, 33384, 490, 7308, 281, 7308, 4467, 466, 1729, 4368, 420, 27820, 791, 295, 264, 1002, 294, 341, 5197, 300, 264, 1411, 1011, 3726, 295, 264, 33032, 390, 1009, 16865, 292, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1780654619324882, "compression_ratio": 1.4853801169590644, "no_speech_prob": 0.21461203694343567}, {"id": 133, "seek": 142520, "start": 1426.2, "end": 1438.2, "text": " I illustrated those things here, a couple of interesting things to note. First of all, with every new move and every bit of new sensory information, there are lots of fluctuations in these states that look very much like an ERP.", "tokens": [50414, 286, 33875, 729, 721, 510, 11, 257, 1916, 295, 1880, 721, 281, 3637, 13, 2386, 295, 439, 11, 365, 633, 777, 1286, 293, 633, 857, 295, 777, 27233, 1589, 11, 456, 366, 3195, 295, 45276, 294, 613, 4368, 300, 574, 588, 709, 411, 364, 14929, 47, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1561148587395163, "compression_ratio": 1.4522292993630572, "no_speech_prob": 0.2524459660053253}, {"id": 134, "seek": 143820, "start": 1438.2, "end": 1459.2, "text": " Furthermore, when we become a little bit more automatic or not habitual, but certainly going straight for our reward, there is an attenuation of these responses. The confidence, the precision in those responses also shows these phasic changes and progressive changes as we learn the context.", "tokens": [50364, 23999, 11, 562, 321, 1813, 257, 707, 857, 544, 12509, 420, 406, 46883, 11, 457, 3297, 516, 2997, 337, 527, 7782, 11, 456, 307, 364, 951, 268, 16073, 295, 613, 13019, 13, 440, 6687, 11, 264, 18356, 294, 729, 13019, 611, 3110, 613, 903, 296, 299, 2962, 293, 16131, 2962, 382, 321, 1466, 264, 4319, 13, 51414], "temperature": 0.0, "avg_logprob": -0.12900776863098146, "compression_ratio": 1.564516129032258, "no_speech_prob": 0.18136636912822723}, {"id": 135, "seek": 145920, "start": 1459.2, "end": 1470.2, "text": " So we actually get something which looks remarkably similar to transfer of dopamine responses as we become more familiar and more confident about the outcomes that we see.", "tokens": [50364, 407, 321, 767, 483, 746, 597, 1542, 37381, 2531, 281, 5003, 295, 37219, 13019, 382, 321, 1813, 544, 4963, 293, 544, 6679, 466, 264, 10070, 300, 321, 536, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11473067147391183, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.1634696125984192}, {"id": 136, "seek": 145920, "start": 1470.2, "end": 1485.2, "text": " Let me just quickly show you a couple of those outcomes. This slide highlights just one trial, and it shows the representations of time over the different hidden states of the world.", "tokens": [50914, 961, 385, 445, 2661, 855, 291, 257, 1916, 295, 729, 10070, 13, 639, 4137, 14254, 445, 472, 7308, 11, 293, 309, 3110, 264, 33358, 295, 565, 670, 264, 819, 7633, 4368, 295, 264, 1002, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11473067147391183, "compression_ratio": 1.5945945945945945, "no_speech_prob": 0.1634696125984192}, {"id": 137, "seek": 148520, "start": 1486.2, "end": 1505.2, "text": " Just highlights a couple of things. First of all, it shows that as we accumulate evidence for our preferred policies or our preferred outcomes, the probability that we are in a state which we will ultimately choose increases whereas the probability of states that we don't decreases.", "tokens": [50414, 1449, 14254, 257, 1916, 295, 721, 13, 2386, 295, 439, 11, 309, 3110, 300, 382, 321, 33384, 4467, 337, 527, 16494, 7657, 420, 527, 16494, 10070, 11, 264, 8482, 300, 321, 366, 294, 257, 1785, 597, 321, 486, 6284, 2826, 8637, 9735, 264, 8482, 295, 4368, 300, 321, 500, 380, 24108, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09974273613521031, "compression_ratio": 1.6171428571428572, "no_speech_prob": 0.011089743115007877}, {"id": 138, "seek": 150520, "start": 1506.2, "end": 1519.2, "text": " This is formally identical to evidence accumulation or drift diffusion models, but now a consequence of a gradient descent on variational free energy or a bound for surprise.", "tokens": [50414, 639, 307, 25983, 14800, 281, 4467, 35647, 420, 19699, 25242, 5245, 11, 457, 586, 257, 18326, 295, 257, 16235, 23475, 322, 3034, 1478, 1737, 2281, 420, 257, 5472, 337, 6365, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16953928811209543, "compression_ratio": 1.3700787401574803, "no_speech_prob": 0.161451518535614}, {"id": 139, "seek": 151920, "start": 1519.2, "end": 1537.2, "text": " What we also see is an interesting dynamics in the sense that if information keeps coming in every, say, 250 milliseconds, like the frequency at which we go and sample the world with mechanic eye movements, that means that we have two timescales in play.", "tokens": [50364, 708, 321, 611, 536, 307, 364, 1880, 15679, 294, 264, 2020, 300, 498, 1589, 5965, 1348, 294, 633, 11, 584, 11, 11650, 34184, 11, 411, 264, 7893, 412, 597, 321, 352, 293, 6889, 264, 1002, 365, 23860, 3313, 9981, 11, 300, 1355, 300, 321, 362, 732, 1413, 66, 4229, 294, 862, 13, 51264], "temperature": 0.0, "avg_logprob": -0.20085258143288748, "compression_ratio": 1.4597701149425288, "no_speech_prob": 0.30373117327690125}, {"id": 140, "seek": 153720, "start": 1537.2, "end": 1554.2, "text": " One is a theta rhythm as we go and get information once, say, four times every second. But within each sampling there's this fast updating that's minimising and optimising our beliefs, and that faster updating has a temporal scale in the gamma range.", "tokens": [50364, 1485, 307, 257, 9725, 11801, 382, 321, 352, 293, 483, 1589, 1564, 11, 584, 11, 1451, 1413, 633, 1150, 13, 583, 1951, 1184, 21179, 456, 311, 341, 2370, 25113, 300, 311, 4464, 3436, 293, 5028, 3436, 527, 13585, 11, 293, 300, 4663, 25113, 575, 257, 30881, 4373, 294, 264, 15546, 3613, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13719192573002406, "compression_ratio": 1.4970059880239521, "no_speech_prob": 0.2234642654657364}, {"id": 141, "seek": 155420, "start": 1554.2, "end": 1571.2, "text": " So what we see is effectively, as we move along, fast updating that repeats itself every theta cycle, but as we accumulate more and more evidence we get more and more efficient and confident about the things that we are inferring.", "tokens": [50364, 407, 437, 321, 536, 307, 8659, 11, 382, 321, 1286, 2051, 11, 2370, 25113, 300, 35038, 2564, 633, 9725, 6586, 11, 457, 382, 321, 33384, 544, 293, 544, 4467, 321, 483, 544, 293, 544, 7148, 293, 6679, 466, 264, 721, 300, 321, 366, 13596, 2937, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11944560050964355, "compression_ratio": 1.554054054054054, "no_speech_prob": 0.011866142973303795}, {"id": 142, "seek": 157120, "start": 1572.2, "end": 1580.2, "text": " The dynamics mean that they accumulate evidence more quickly, more efficiently, and we get a phase procession of the sort seen in the hippocampus.", "tokens": [50414, 440, 15679, 914, 300, 436, 33384, 4467, 544, 2661, 11, 544, 19621, 11, 293, 321, 483, 257, 5574, 1399, 313, 295, 264, 1333, 1612, 294, 264, 27745, 905, 1215, 301, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1487602506365095, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.02254360169172287}, {"id": 143, "seek": 157120, "start": 1581.2, "end": 1596.2, "text": " I've already mentioned that as time goes on, by virtue of increasing our confidence as we assimilate this evidence, then that confidence is expressed in the confidence of our policies and we have a nice way of assimilating dopamine responses.", "tokens": [50864, 286, 600, 1217, 2835, 300, 382, 565, 1709, 322, 11, 538, 20816, 295, 5662, 527, 6687, 382, 321, 8249, 48104, 341, 4467, 11, 550, 300, 6687, 307, 12675, 294, 264, 6687, 295, 527, 7657, 293, 321, 362, 257, 1481, 636, 295, 8249, 388, 990, 37219, 13019, 13, 51614], "temperature": 0.0, "avg_logprob": -0.1487602506365095, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.02254360169172287}, {"id": 144, "seek": 159620, "start": 1597.2, "end": 1613.2, "text": " We can look at the behaviour or the activity of these representations of different states of the world at different points in time during our policy, and if we plot their responses as a function of where the rat actually is, we can simulate place cell activity.", "tokens": [50414, 492, 393, 574, 412, 264, 17229, 420, 264, 5191, 295, 613, 33358, 295, 819, 4368, 295, 264, 1002, 412, 819, 2793, 294, 565, 1830, 527, 3897, 11, 293, 498, 321, 7542, 641, 13019, 382, 257, 2445, 295, 689, 264, 5937, 767, 307, 11, 321, 393, 27817, 1081, 2815, 5191, 13, 51214], "temperature": 0.0, "avg_logprob": -0.0908550156487359, "compression_ratio": 1.562874251497006, "no_speech_prob": 0.004208892583847046}, {"id": 145, "seek": 161320, "start": 1614.2, "end": 1631.2, "text": " There has many characteristics of the sort seen empirically. This just illustrates this theta-gamma coupling, which is an almost necessary consequence of this sort of solitary sampling of the world, and then updating bleeds quickly before the next sample comes along.", "tokens": [50414, 821, 575, 867, 10891, 295, 264, 1333, 1612, 25790, 984, 13, 639, 445, 41718, 341, 9725, 12, 33815, 1696, 37447, 11, 597, 307, 364, 1920, 4818, 18326, 295, 341, 1333, 295, 44155, 21179, 295, 264, 1002, 11, 293, 550, 25113, 5408, 5147, 2661, 949, 264, 958, 6889, 1487, 2051, 13, 51264], "temperature": 0.0, "avg_logprob": -0.18846857989275898, "compression_ratio": 1.5, "no_speech_prob": 0.3025479316711426}, {"id": 146, "seek": 163120, "start": 1631.2, "end": 1643.2, "text": " Again, the sort of thing that one sees empirically. We can now do violation responses exactly as Jim was talking about. What I've shown here are the responses to two trials.", "tokens": [50364, 3764, 11, 264, 1333, 295, 551, 300, 472, 8194, 25790, 984, 13, 492, 393, 586, 360, 22840, 13019, 2293, 382, 6637, 390, 1417, 466, 13, 708, 286, 600, 4898, 510, 366, 264, 13019, 281, 732, 12450, 13, 50964], "temperature": 0.0, "avg_logprob": -0.10640636734340501, "compression_ratio": 1.688259109311741, "no_speech_prob": 0.1567101627588272}, {"id": 147, "seek": 163120, "start": 1644.2, "end": 1655.2, "text": " They're identical in nature, but one is from the beginning of the trial where the rat was not familiar with its environment, and one is at the end of the trial where it becomes very familiar just before it starts going directly for the reward.", "tokens": [51014, 814, 434, 14800, 294, 3687, 11, 457, 472, 307, 490, 264, 2863, 295, 264, 7308, 689, 264, 5937, 390, 406, 4963, 365, 1080, 2823, 11, 293, 472, 307, 412, 264, 917, 295, 264, 7308, 689, 309, 3643, 588, 4963, 445, 949, 309, 3719, 516, 3838, 337, 264, 7782, 13, 51564], "temperature": 0.0, "avg_logprob": -0.10640636734340501, "compression_ratio": 1.688259109311741, "no_speech_prob": 0.1567101627588272}, {"id": 148, "seek": 165520, "start": 1656.2, "end": 1678.2, "text": " Interesting, if we look at the representations of key states here, what we see is that they are much more efficient and therefore less exuberant updating of expectations of hidden states that if we subtract the standard familiar one from the odd ball or the unfamiliar one, we reproduce the temporal dynamics of things like the mismatch negativity in ERP research.", "tokens": [50414, 14711, 11, 498, 321, 574, 412, 264, 33358, 295, 2141, 4368, 510, 11, 437, 321, 536, 307, 300, 436, 366, 709, 544, 7148, 293, 4412, 1570, 454, 10261, 394, 25113, 295, 9843, 295, 7633, 4368, 300, 498, 321, 16390, 264, 3832, 4963, 472, 490, 264, 7401, 2594, 420, 264, 29415, 472, 11, 321, 29501, 264, 30881, 15679, 295, 721, 411, 264, 23220, 852, 39297, 294, 14929, 47, 2132, 13, 51514], "temperature": 0.0, "avg_logprob": -0.14569767207315523, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.019366273656487465}, {"id": 149, "seek": 167820, "start": 1679.2, "end": 1692.2, "text": " We also demonstrate this transfer of confidence or simulated dopamine responses from the rewarded cue per se to this instructional condition stimulus here.", "tokens": [50414, 492, 611, 11698, 341, 5003, 295, 6687, 420, 41713, 37219, 13019, 490, 264, 29105, 22656, 680, 369, 281, 341, 35716, 4188, 21366, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.2849432911191668, "compression_ratio": 1.3839285714285714, "no_speech_prob": 0.045495446771383286}, {"id": 150, "seek": 169220, "start": 1692.2, "end": 1713.2, "text": " I'm going from slightly negative to positive here. We can play similar games by introducing deliberate violations and illicit P300 responses. We can look at reinforcement learning by switching contingencies halfway through and look at the effects on dopamine-urgent responses and also electrophysiological responses.", "tokens": [50364, 286, 478, 516, 490, 4748, 3671, 281, 3353, 510, 13, 492, 393, 862, 2531, 2813, 538, 15424, 30515, 30405, 293, 3171, 8876, 430, 12566, 13019, 13, 492, 393, 574, 412, 29280, 2539, 538, 16493, 27820, 6464, 15461, 807, 293, 574, 412, 264, 5065, 322, 37219, 12, 5476, 317, 13019, 293, 611, 2185, 11741, 749, 72, 4383, 13019, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1853111675807408, "compression_ratio": 1.538812785388128, "no_speech_prob": 0.2540741264820099}, {"id": 151, "seek": 169220, "start": 1716.2, "end": 1717.2, "text": " How long have I got?", "tokens": [51564, 1012, 938, 362, 286, 658, 30, 51614], "temperature": 0.0, "avg_logprob": -0.1853111675807408, "compression_ratio": 1.538812785388128, "no_speech_prob": 0.2540741264820099}, {"id": 152, "seek": 171720, "start": 1717.2, "end": 1741.2, "text": " Oh, that's very good, isn't it? I've only been talking for 25 minutes. I can be true to my promise to finish in half an hour. This is the epilogue. That's the story so far. Most of that will be in the next few weeks in the published literature.", "tokens": [50364, 876, 11, 300, 311, 588, 665, 11, 1943, 380, 309, 30, 286, 600, 787, 668, 1417, 337, 3552, 2077, 13, 286, 393, 312, 2074, 281, 452, 6228, 281, 2413, 294, 1922, 364, 1773, 13, 639, 307, 264, 2388, 388, 7213, 13, 663, 311, 264, 1657, 370, 1400, 13, 4534, 295, 300, 486, 312, 294, 264, 958, 1326, 3259, 294, 264, 6572, 10394, 13, 51564], "temperature": 0.0, "avg_logprob": -0.17935448262228895, "compression_ratio": 1.3785310734463276, "no_speech_prob": 0.010349315591156483}, {"id": 153, "seek": 174120, "start": 1742.2, "end": 1755.2, "text": " You'll notice at the moment there's nothing really about hierarchies. Most people here, I'm sure, are more interested in the implications of this sort of theory for perceptual hierarchies and evidence of accumulation and purely perceptual domain.", "tokens": [50414, 509, 603, 3449, 412, 264, 1623, 456, 311, 1825, 534, 466, 35250, 530, 13, 4534, 561, 510, 11, 286, 478, 988, 11, 366, 544, 3102, 294, 264, 16602, 295, 341, 1333, 295, 5261, 337, 43276, 901, 35250, 530, 293, 4467, 295, 35647, 293, 17491, 43276, 901, 9274, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1658799831683819, "compression_ratio": 1.5375, "no_speech_prob": 0.022184040397405624}, {"id": 154, "seek": 175520, "start": 1755.2, "end": 1784.2, "text": " The more recent work that I wanted to, this is not published, to introduce you to, is now taking this formalism, which has a lot of constant validity in relation to choice behaviour and your economics, active vision, active sensing, and see what it has to say about the source of themes we're more interested in, which is the hierarchical message passing and the deep generative models that we assume.", "tokens": [50414, 440, 544, 5162, 589, 300, 286, 1415, 281, 11, 341, 307, 406, 6572, 11, 281, 5366, 291, 281, 11, 307, 586, 1940, 341, 9860, 1434, 11, 597, 575, 257, 688, 295, 5754, 40943, 294, 9721, 281, 3922, 17229, 293, 428, 14564, 11, 4967, 5201, 11, 4967, 30654, 11, 293, 536, 437, 309, 575, 281, 584, 466, 264, 4009, 295, 13544, 321, 434, 544, 3102, 294, 11, 597, 307, 264, 35250, 804, 3636, 8437, 293, 264, 2452, 1337, 1166, 5245, 300, 321, 6552, 13, 51814], "temperature": 0.0, "avg_logprob": -0.20456566921500272, "compression_ratio": 1.636734693877551, "no_speech_prob": 0.0561085119843483}, {"id": 155, "seek": 178520, "start": 1785.2, "end": 1789.2, "text": " The brain is using to actually understand perceptual sequences, say.", "tokens": [50364, 440, 3567, 307, 1228, 281, 767, 1223, 43276, 901, 22978, 11, 584, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13258479339907867, "compression_ratio": 1.6875, "no_speech_prob": 0.004822096787393093}, {"id": 156, "seek": 178520, "start": 1791.2, "end": 1808.2, "text": " So this is the epilogue. Again, I'll just speed through this in five minutes. What we're going to do now is tell exactly the same story, but now we're going to put one of those discrete state-space models, they're known as Markov decision processes, on top of the first one and another one on top of that and another one on top of that.", "tokens": [50664, 407, 341, 307, 264, 2388, 388, 7213, 13, 3764, 11, 286, 603, 445, 3073, 807, 341, 294, 1732, 2077, 13, 708, 321, 434, 516, 281, 360, 586, 307, 980, 2293, 264, 912, 1657, 11, 457, 586, 321, 434, 516, 281, 829, 472, 295, 729, 27706, 1785, 12, 24824, 5245, 11, 436, 434, 2570, 382, 3934, 5179, 3537, 7555, 11, 322, 1192, 295, 264, 700, 472, 293, 1071, 472, 322, 1192, 295, 300, 293, 1071, 472, 322, 1192, 295, 300, 13, 51514], "temperature": 0.0, "avg_logprob": -0.13258479339907867, "compression_ratio": 1.6875, "no_speech_prob": 0.004822096787393093}, {"id": 157, "seek": 180820, "start": 1809.2, "end": 1820.2, "text": " So in this construction, hidden states, at any one level in the model, don't generate outcomes, they generate the first or the initial hidden state of the level below.", "tokens": [50414, 407, 294, 341, 6435, 11, 7633, 4368, 11, 412, 604, 472, 1496, 294, 264, 2316, 11, 500, 380, 8460, 10070, 11, 436, 8460, 264, 700, 420, 264, 5883, 7633, 1785, 295, 264, 1496, 2507, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16357025733360878, "compression_ratio": 1.4521739130434783, "no_speech_prob": 0.09792142361402512}, {"id": 158, "seek": 182020, "start": 1821.2, "end": 1833.2, "text": " And then that cycles over a few iterations and then terminates like the rat-terminated when it entered the baited arms of the cues.", "tokens": [50414, 400, 550, 300, 17796, 670, 257, 1326, 36540, 293, 550, 10761, 1024, 411, 264, 5937, 12, 29725, 770, 562, 309, 9065, 264, 16865, 292, 5812, 295, 264, 32192, 13, 51014], "temperature": 0.0, "avg_logprob": -0.2107551139697694, "compression_ratio": 1.5308641975308641, "no_speech_prob": 0.009496660903096199}, {"id": 159, "seek": 182020, "start": 1834.2, "end": 1843.2, "text": " And that process repeats hierarchically to any arbitrary depth. So what we have are deep temporal generative models.", "tokens": [51064, 400, 300, 1399, 35038, 35250, 984, 281, 604, 23211, 7161, 13, 407, 437, 321, 362, 366, 2452, 30881, 1337, 1166, 5245, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2107551139697694, "compression_ratio": 1.5308641975308641, "no_speech_prob": 0.009496660903096199}, {"id": 160, "seek": 184320, "start": 1844.2, "end": 1868.2, "text": " And they're really interesting because not only do they have a hierarchical structure in their form, but also in their time, because if the state at any high level is generating the initial state that must have subsequent states, then it means that the lower states unfold more quickly than the higher states.", "tokens": [50414, 400, 436, 434, 534, 1880, 570, 406, 787, 360, 436, 362, 257, 35250, 804, 3877, 294, 641, 1254, 11, 457, 611, 294, 641, 565, 11, 570, 498, 264, 1785, 412, 604, 1090, 1496, 307, 17746, 264, 5883, 1785, 300, 1633, 362, 19962, 4368, 11, 550, 309, 1355, 300, 264, 3126, 4368, 17980, 544, 2661, 813, 264, 2946, 4368, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09542322915697855, "compression_ratio": 1.6612903225806452, "no_speech_prob": 0.022892776876688004}, {"id": 161, "seek": 186820, "start": 1869.2, "end": 1884.2, "text": " So one way of thinking about this is the generative model says that at this hour, at this minute and at this second, I am safe, I was reading, I'm on this page, on this paragraph and on this word.", "tokens": [50414, 407, 472, 636, 295, 1953, 466, 341, 307, 264, 1337, 1166, 2316, 1619, 300, 412, 341, 1773, 11, 412, 341, 3456, 293, 412, 341, 1150, 11, 286, 669, 3273, 11, 286, 390, 3760, 11, 286, 478, 322, 341, 3028, 11, 322, 341, 18865, 293, 322, 341, 1349, 13, 51164], "temperature": 0.0, "avg_logprob": -0.08903423639444205, "compression_ratio": 1.4961832061068703, "no_speech_prob": 0.02625926397740841}, {"id": 162, "seek": 188420, "start": 1885.2, "end": 1904.2, "text": " So if you think about the lower levels of us ticking over more quickly, like the second hand of a clock, and every revolution or every trajectory or every path they take, then the high level goes forward one step, and then it goes round again, it goes another step, another gain, sorry, again and then another step.", "tokens": [50414, 407, 498, 291, 519, 466, 264, 3126, 4358, 295, 505, 33999, 670, 544, 2661, 11, 411, 264, 1150, 1011, 295, 257, 7830, 11, 293, 633, 8894, 420, 633, 21512, 420, 633, 3100, 436, 747, 11, 550, 264, 1090, 1496, 1709, 2128, 472, 1823, 11, 293, 550, 309, 1709, 3098, 797, 11, 309, 1709, 1071, 1823, 11, 1071, 6052, 11, 2597, 11, 797, 293, 550, 1071, 1823, 13, 51364], "temperature": 0.0, "avg_logprob": -0.184295869209397, "compression_ratio": 1.75, "no_speech_prob": 0.11770347505807877}, {"id": 163, "seek": 190420, "start": 1905.2, "end": 1912.2, "text": " And then that process is repeated. So as the minute hand is going round, once it goes round, then the hour hand goes round.", "tokens": [50414, 400, 550, 300, 1399, 307, 10477, 13, 407, 382, 264, 3456, 1011, 307, 516, 3098, 11, 1564, 309, 1709, 3098, 11, 550, 264, 1773, 1011, 1709, 3098, 13, 50764], "temperature": 0.0, "avg_logprob": -0.08282079299290974, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.027613606303930283}, {"id": 164, "seek": 190420, "start": 1912.2, "end": 1926.2, "text": " So what we have here is a generative model that basically has in mind, literally, beliefs about the world that are much more protracted in time and are hierarchically nested.", "tokens": [50764, 407, 437, 321, 362, 510, 307, 257, 1337, 1166, 2316, 300, 1936, 575, 294, 1575, 11, 3736, 11, 13585, 466, 264, 1002, 300, 366, 709, 544, 1742, 1897, 292, 294, 565, 293, 366, 35250, 984, 15646, 292, 13, 51464], "temperature": 0.0, "avg_logprob": -0.08282079299290974, "compression_ratio": 1.6108108108108108, "no_speech_prob": 0.027613606303930283}, {"id": 165, "seek": 192620, "start": 1926.2, "end": 1937.2, "text": " So if you could invert this sort of model, you would have a representation of the context and the context of context and the context of context.", "tokens": [50364, 407, 498, 291, 727, 33966, 341, 1333, 295, 2316, 11, 291, 576, 362, 257, 10290, 295, 264, 4319, 293, 264, 4319, 295, 4319, 293, 264, 4319, 295, 4319, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10789449126632125, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.016472483053803444}, {"id": 166, "seek": 192620, "start": 1937.2, "end": 1942.2, "text": " At each point, as you go deeper into the model, they are more temporally enduring.", "tokens": [50914, 1711, 1184, 935, 11, 382, 291, 352, 7731, 666, 264, 2316, 11, 436, 366, 544, 8219, 379, 36562, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10789449126632125, "compression_ratio": 1.644927536231884, "no_speech_prob": 0.016472483053803444}, {"id": 167, "seek": 194220, "start": 1942.2, "end": 1958.2, "text": " So you know that working from the top down, you'd know the story of the narrative, if you knew the story of the narrative, you'd be able to generate a particular sentence, if you could generate a particular sentence, if you could generate a particular word, if you could generate a particular word...", "tokens": [50364, 407, 291, 458, 300, 1364, 490, 264, 1192, 760, 11, 291, 1116, 458, 264, 1657, 295, 264, 9977, 11, 498, 291, 2586, 264, 1657, 295, 264, 9977, 11, 291, 1116, 312, 1075, 281, 8460, 257, 1729, 8174, 11, 498, 291, 727, 8460, 257, 1729, 8174, 11, 498, 291, 727, 8460, 257, 1729, 1349, 11, 498, 291, 727, 8460, 257, 1729, 1349, 485, 51164], "temperature": 1.0, "avg_logprob": -0.36947886149088544, "compression_ratio": 2.4146341463414633, "no_speech_prob": 0.2519417405128479}, {"id": 168, "seek": 194220, "start": 1958.2, "end": 1962.2, "text": " .. you could generate a particular letter. All faster and faster and more elemental timescales.", "tokens": [51164, 4386, 291, 727, 8460, 257, 1729, 5063, 13, 1057, 4663, 293, 4663, 293, 544, 39427, 1413, 66, 4229, 13, 51364], "temperature": 1.0, "avg_logprob": -0.36947886149088544, "compression_ratio": 2.4146341463414633, "no_speech_prob": 0.2519417405128479}, {"id": 169, "seek": 196220, "start": 1962.2, "end": 1969.02, "text": " That's the sort of model now that people are starting to play with.", "tokens": [50364, 663, 311, 264, 1333, 295, 2316, 586, 300, 561, 366, 2891, 281, 862, 365, 13, 50705], "temperature": 1.0, "avg_logprob": -0.9902376695112749, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.058005817234516144}, {"id": 170, "seek": 196220, "start": 1969.02, "end": 1972.8600000000001, "text": " It has exactly the same performance before\u2014 coping with policies in play", "tokens": [50705, 467, 575, 2293, 264, 912, 3389, 949, 2958, 32893, 365, 7657, 294, 862, 50897], "temperature": 1.0, "avg_logprob": -0.9902376695112749, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.058005817234516144}, {"id": 171, "seek": 196220, "start": 1972.8600000000001, "end": 1976.22, "text": " that generate transitions among hidden states that generate outcomes,", "tokens": [50897, 300, 8460, 23767, 3654, 7633, 4368, 300, 8460, 10070, 11, 51065], "temperature": 1.0, "avg_logprob": -0.9902376695112749, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.058005817234516144}, {"id": 172, "seek": 196220, "start": 1976.22, "end": 1979.14, "text": " but now the first hidden state is generated", "tokens": [51065, 457, 586, 264, 700, 7633, 1785, 307, 10833, 51211], "temperature": 1.0, "avg_logprob": -0.9902376695112749, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.058005817234516144}, {"id": 173, "seek": 196220, "start": 1979.14, "end": 1985.4, "text": " by the same sort of model, formally identical, of a higher level.", "tokens": [51211, 538, 264, 912, 1333, 295, 2316, 11, 25983, 14800, 11, 295, 257, 2946, 1496, 13, 51524], "temperature": 1.0, "avg_logprob": -0.9902376695112749, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.058005817234516144}, {"id": 174, "seek": 196220, "start": 1985.4, "end": 1989.3, "text": " There are transitions over time here, but they are much slower.", "tokens": [51524, 821, 366, 23767, 670, 565, 510, 11, 457, 436, 366, 709, 14009, 13, 51719], "temperature": 1.0, "avg_logprob": -0.9902376695112749, "compression_ratio": 1.7232142857142858, "no_speech_prob": 0.058005817234516144}, {"id": 175, "seek": 198930, "start": 1989.3, "end": 1992.82, "text": " that by making these red lines upon red states here,", "tokens": [50364, 300, 538, 1455, 613, 2182, 3876, 3564, 2182, 4368, 510, 11, 50540], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 176, "seek": 198930, "start": 1993.54, "end": 1995.7, "text": " a different colour from these because these, basically,", "tokens": [50576, 257, 819, 8267, 490, 613, 570, 613, 11, 1936, 11, 50684], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 177, "seek": 198930, "start": 1995.78, "end": 1997.8999999999999, "text": " are the same as these but they're re-used", "tokens": [50688, 366, 264, 912, 382, 613, 457, 436, 434, 319, 12, 4717, 50794], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 178, "seek": 198930, "start": 1997.98, "end": 1999.3, "text": " at a later point in time.", "tokens": [50798, 412, 257, 1780, 935, 294, 565, 13, 50864], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 179, "seek": 198930, "start": 2000.5, "end": 2004.5, "text": " And this sort of model now allows you to think more carefully", "tokens": [50924, 400, 341, 1333, 295, 2316, 586, 4045, 291, 281, 519, 544, 7500, 51124], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 180, "seek": 198930, "start": 2004.58, "end": 2006.7, "text": " about the hierarchical message passing", "tokens": [51128, 466, 264, 35250, 804, 3636, 8437, 51234], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 181, "seek": 198930, "start": 2006.94, "end": 2008.58, "text": " and the implications of neuro-anatomy.", "tokens": [51246, 293, 264, 16602, 295, 16499, 12, 282, 267, 8488, 13, 51328], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 182, "seek": 198930, "start": 2008.6599999999999, "end": 2010.7, "text": " So if we now turn straight to the process theory,", "tokens": [51332, 407, 498, 321, 586, 1261, 2997, 281, 264, 1399, 5261, 11, 51434], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 183, "seek": 198930, "start": 2011.3, "end": 2013.78, "text": " these are the differential equations", "tokens": [51464, 613, 366, 264, 15756, 11787, 51588], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 184, "seek": 198930, "start": 2014.86, "end": 2017.78, "text": " that fall out of that generatord model in the previous slide.", "tokens": [51642, 300, 2100, 484, 295, 300, 1337, 267, 765, 2316, 294, 264, 3894, 4137, 13, 51788], "temperature": 1.0, "avg_logprob": -0.9132660296784729, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.02722342498600483}, {"id": 185, "seek": 201778, "start": 2017.78, "end": 2022.34, "text": " ei wneud o unig lwydaeth pob ddechrau.", "tokens": [50364, 14020, 261, 716, 532, 277, 517, 328, 287, 9726, 2675, 3293, 714, 65, 274, 1479, 339, 48907, 13, 50592], "temperature": 1.0, "avg_logprob": -3.756629889738475, "compression_ratio": 1.3991228070175439, "no_speech_prob": 0.12477323412895203}, {"id": 186, "seek": 201778, "start": 2022.58, "end": 2027.26, "text": " Derbyn i ch risingdef o dystru i dystru i ddim yn cael ei ddif respir,", "tokens": [50604, 5618, 2322, 77, 741, 417, 11636, 20595, 277, 14584, 372, 894, 741, 14584, 372, 894, 741, 274, 13595, 17861, 1335, 338, 14020, 274, 67, 351, 18412, 11, 50838], "temperature": 1.0, "avg_logprob": -3.756629889738475, "compression_ratio": 1.3991228070175439, "no_speech_prob": 0.12477323412895203}, {"id": 187, "seek": 201778, "start": 2027.42, "end": 2032.66, "text": " egOC yn gyfrasch ychydig ar y totu org10-g \u062c\u0648\u0646 inequality.", "tokens": [50846, 24263, 30087, 17861, 15823, 69, 3906, 339, 288, 339, 6655, 328, 594, 288, 1993, 84, 14045, 3279, 12, 70, 10874, 11536, 16970, 13, 51108], "temperature": 1.0, "avg_logprob": -3.756629889738475, "compression_ratio": 1.3991228070175439, "no_speech_prob": 0.12477323412895203}, {"id": 188, "seek": 201778, "start": 2032.98, "end": 2037.58, "text": " A dwi'n credu hynny bir maen i ddim yn ddigonol gyda L rug", "tokens": [51124, 316, 274, 6253, 6, 77, 1197, 769, 2477, 77, 1634, 1904, 463, 268, 741, 274, 13595, 17861, 274, 25259, 266, 401, 15823, 2675, 441, 18329, 51354], "temperature": 1.0, "avg_logprob": -3.756629889738475, "compression_ratio": 1.3991228070175439, "no_speech_prob": 0.12477323412895203}, {"id": 189, "seek": 201778, "start": 2037.74, "end": 2043.34, "text": " yn misoedd f hybridd stag yn y gwroad \u0442\u0440 s\u00edr.", "tokens": [51362, 17861, 3346, 78, 38786, 283, 13051, 67, 342, 559, 17861, 288, 29255, 8417, 7550, 8600, 81, 13, 51642], "temperature": 1.0, "avg_logprob": -3.756629889738475, "compression_ratio": 1.3991228070175439, "no_speech_prob": 0.12477323412895203}, {"id": 190, "seek": 201778, "start": 2043.58, "end": 2047.7, "text": " Til ond mythigr o'r ddim yn y ddigonor,", "tokens": [51654, 45141, 322, 67, 9474, 328, 81, 277, 6, 81, 274, 13595, 17861, 288, 274, 25259, 266, 284, 11, 51860], "temperature": 1.0, "avg_logprob": -3.756629889738475, "compression_ratio": 1.3991228070175439, "no_speech_prob": 0.12477323412895203}, {"id": 191, "seek": 204778, "start": 2047.78, "end": 2052.06, "text": " a gan y doddoriaeth yn y tai nhw'r drefnio fan y plainachau.", "tokens": [50364, 257, 7574, 288, 13886, 67, 8172, 3293, 17861, 288, 20499, 6245, 86, 6, 81, 22540, 69, 77, 1004, 3429, 288, 11121, 608, 1459, 13, 50578], "temperature": 1.0, "avg_logprob": -2.994012939453125, "compression_ratio": 1.544378698224852, "no_speech_prob": 0.04353281110525131}, {"id": 192, "seek": 204778, "start": 2052.86, "end": 2057.94, "text": " Diolch yn ddechrau am ddechrau coddyntau hynny a detch,", "tokens": [50618, 8789, 401, 339, 17861, 274, 1479, 339, 48907, 669, 274, 1479, 339, 48907, 269, 378, 3173, 580, 1459, 2477, 77, 1634, 257, 274, 7858, 11, 50872], "temperature": 1.0, "avg_logprob": -2.994012939453125, "compression_ratio": 1.544378698224852, "no_speech_prob": 0.04353281110525131}, {"id": 193, "seek": 204778, "start": 2058.42, "end": 2067.02, "text": " ac rydw i addysgu'n chi fod rhywbeth rydyn ni'n ach yn perwhaith gweithio.", "tokens": [50896, 696, 367, 6655, 86, 741, 909, 749, 2794, 6, 77, 13228, 47698, 8740, 86, 65, 3293, 367, 6655, 2534, 3867, 6, 77, 2800, 17861, 680, 1363, 64, 355, 290, 826, 355, 1004, 13, 51326], "temperature": 1.0, "avg_logprob": -2.994012939453125, "compression_ratio": 1.544378698224852, "no_speech_prob": 0.04353281110525131}, {"id": 194, "seek": 204778, "start": 2067.14, "end": 2073.86, "text": " Mae ydw mitant ar hufwn, felly dyma'n ddechrau ddiwrnod ni'n ddechrau", "tokens": [51332, 31055, 288, 67, 86, 2194, 394, 594, 2137, 69, 895, 11, 5696, 88, 14584, 1696, 6, 77, 274, 1479, 339, 48907, 274, 4504, 7449, 77, 378, 3867, 6, 77, 274, 1479, 339, 48907, 51668], "temperature": 1.0, "avg_logprob": -2.994012939453125, "compression_ratio": 1.544378698224852, "no_speech_prob": 0.04353281110525131}, {"id": 195, "seek": 207386, "start": 2074.1, "end": 2080.52, "text": " Ond biggestonethol, er di particularly gw Snapod,", "tokens": [50376, 40091, 3880, 266, 3293, 401, 11, 1189, 1026, 4098, 29255, 18254, 378, 11, 50697], "temperature": 1.0, "avg_logprob": -3.790840802873884, "compression_ratio": 1.2575757575757576, "no_speech_prob": 0.008143451996147633}, {"id": 196, "seek": 207386, "start": 2080.52, "end": 2088.76, "text": " yn ynghyd yn yr ymddangos lleethau hyn arall hynny,", "tokens": [50697, 17861, 288, 872, 21591, 17861, 37739, 288, 76, 24810, 656, 329, 12038, 3293, 1459, 2477, 77, 594, 336, 2477, 77, 1634, 11, 51109], "temperature": 1.0, "avg_logprob": -3.790840802873884, "compression_ratio": 1.2575757575757576, "no_speech_prob": 0.008143451996147633}, {"id": 197, "seek": 207386, "start": 2088.76, "end": 2095.36, "text": " ddweud yma cwm declar o repliedol, ar m \uac8cisirol arall y dyfod,", "tokens": [51109, 274, 67, 826, 532, 288, 1696, 269, 86, 76, 16694, 277, 20345, 401, 11, 594, 275, 7845, 271, 347, 401, 594, 336, 288, 14584, 69, 378, 11, 51439], "temperature": 1.0, "avg_logprob": -3.790840802873884, "compression_ratio": 1.2575757575757576, "no_speech_prob": 0.008143451996147633}, {"id": 198, "seek": 209536, "start": 2095.36, "end": 2102.76, "text": " Golw llawer arall llawer mewn amser gan y cwmputatio nanallynol, y\u043a\u0438\u043c\u0438 yuyaeth", "tokens": [50364, 36319, 86, 220, 3505, 1554, 594, 336, 220, 3505, 1554, 385, 895, 669, 12484, 7574, 288, 269, 86, 2455, 325, 267, 1004, 14067, 379, 77, 401, 11, 288, 44130, 288, 37199, 3293, 50734], "temperature": 1.0, "avg_logprob": -4.009437216481855, "compression_ratio": 1.325925925925926, "no_speech_prob": 0.0034297960810363293}, {"id": 199, "seek": 209536, "start": 2102.76, "end": 2108.56, "text": " yw Somehow carried a 84.5 Sauce Ndun, iawn i' fi bwyzaid eu droslawn f\u03c6m.", "tokens": [50734, 288, 86, 28357, 9094, 257, 29018, 13, 20, 36720, 426, 67, 409, 11, 20721, 895, 741, 6, 15848, 272, 9726, 2394, 327, 2228, 1224, 329, 875, 895, 283, 12015, 76, 13, 51024], "temperature": 1.0, "avg_logprob": -4.009437216481855, "compression_ratio": 1.325925925925926, "no_speech_prob": 0.0034297960810363293}, {"id": 200, "seek": 209536, "start": 2109.1600000000003, "end": 2112.76, "text": " I airportio pleid seg\u00fan fill.", "tokens": [51054, 286, 10155, 1004, 3362, 327, 36570, 2836, 13, 51234], "temperature": 1.0, "avg_logprob": -4.009437216481855, "compression_ratio": 1.325925925925926, "no_speech_prob": 0.0034297960810363293}, {"id": 201, "seek": 209536, "start": 2112.76, "end": 2117.6400000000003, "text": " Er si GLORIA cyhoedd, Assembly Down yw dweud yn gyda hynny yng Nghymru a'i sefydlu'r", "tokens": [51234, 3300, 1511, 24074, 3185, 1289, 38786, 11, 20399, 9506, 288, 86, 274, 826, 532, 17861, 15823, 2675, 2477, 77, 1634, 288, 872, 21198, 3495, 76, 894, 257, 6, 72, 369, 69, 6655, 2781, 6, 81, 51478], "temperature": 1.0, "avg_logprob": -4.009437216481855, "compression_ratio": 1.325925925925926, "no_speech_prob": 0.0034297960810363293}, {"id": 202, "seek": 209536, "start": 2117.6400000000003, "end": 2122.96, "text": " rhywbeth gyda'r berth dweud, er mwyafMatthewch szeynerrym yn\u0442\u044b grandiaf minzysolEl", "tokens": [51478, 8740, 86, 65, 3293, 15823, 2675, 6, 81, 5948, 392, 274, 826, 532, 11, 1189, 275, 9726, 2792, 42325, 392, 1023, 339, 7870, 2030, 77, 5318, 76, 17861, 16759, 2697, 654, 69, 923, 89, 749, 401, 17356, 51744], "temperature": 1.0, "avg_logprob": -4.009437216481855, "compression_ratio": 1.325925925925926, "no_speech_prob": 0.0034297960810363293}, {"id": 203, "seek": 212296, "start": 2122.96, "end": 2125.66, "text": " Mae wedi'i converti analw!!!!!!", "tokens": [50364, 31055, 6393, 72, 6, 72, 7620, 72, 2624, 86, 50199, 50499], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 204, "seek": 212296, "start": 2125.66, "end": 2128.64, "text": " A \ub418fnodd ar yr adegowadau mewn gindig adegowadau", "tokens": [50499, 316, 5514, 69, 77, 378, 67, 594, 37739, 614, 1146, 305, 345, 1459, 385, 895, 290, 471, 328, 614, 1146, 305, 345, 1459, 50648], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 205, "seek": 212296, "start": 2128.64, "end": 2131.06, "text": "that atbypaenter, a gennych amherei", "tokens": [50648, 6780, 412, 2322, 4306, 268, 391, 11, 257, 1049, 9399, 669, 6703, 72, 50769], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 206, "seek": 212296, "start": 2131.06, "end": 2133.04, "text": " A ddau roi gwodd!.", "tokens": [50769, 316, 274, 67, 1459, 744, 72, 29255, 378, 67, 37817, 50868], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 207, "seek": 212296, "start": 2133.04, "end": 2135.04, "text": " Fmarfin analw Ring", "tokens": [50868, 479, 6209, 5194, 2624, 86, 19844, 50968], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 208, "seek": 212296, "start": 2135.04, "end": 2137.26, "text": " ac mewn gwir y c\u0438\u0442\u0435ch,", "tokens": [50968, 696, 385, 895, 29255, 347, 288, 269, 5878, 339, 11, 51079], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 209, "seek": 212296, "start": 2137.26, "end": 2139.16, "text": " dw i ddim yn gwneud o wnes cy hunain", "tokens": [51079, 27379, 741, 274, 13595, 17861, 29255, 716, 532, 277, 261, 4081, 3185, 7396, 491, 51174], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 210, "seek": 212296, "start": 2139.16, "end": 2142.36, "text": " Dasodd yr adegowadau i gwenallu", "tokens": [51174, 2846, 378, 67, 37739, 614, 1146, 305, 345, 1459, 741, 290, 15615, 336, 84, 51334], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 211, "seek": 212296, "start": 2142.36, "end": 2144.42, "text": " cais grathio adegowadau", "tokens": [51334, 1335, 271, 677, 998, 1004, 614, 1146, 305, 345, 1459, 51437], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 212, "seek": 212296, "start": 2144.42, "end": 2146.7200000000003, "text": " Mae\u043b\u044e\u0447 ychydig iddo nhw a \uc120urdwy", "tokens": [51437, 31055, 11833, 288, 339, 6655, 328, 4496, 2595, 6245, 86, 257, 11835, 10752, 9726, 51552], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 213, "seek": 212296, "start": 2146.7200000000003, "end": 2149.26, "text": " Rydyn ni gadael", "tokens": [51552, 497, 6655, 2534, 3867, 290, 1538, 338, 51679], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 214, "seek": 212296, "start": 2149.26, "end": 2151.6, "text": " sy'n fioedd y gwaith", "tokens": [51679, 943, 6, 77, 283, 1004, 38786, 288, 290, 4151, 355, 51796], "temperature": 1.0, "avg_logprob": -3.657364713931512, "compression_ratio": 1.4362139917695473, "no_speech_prob": 0.0500786118209362}, {"id": 215, "seek": 215160, "start": 2151.6, "end": 2160.2999999999997, "text": " ma wedi ar hyn o gwbl panallu, a hyffordda.", "tokens": [50364, 463, 6393, 72, 594, 2477, 77, 277, 29255, 5199, 2462, 336, 84, 11, 257, 2477, 602, 765, 2675, 13, 50799], "temperature": 1.0, "avg_logprob": -3.3765131632486978, "compression_ratio": 1.1868131868131868, "no_speech_prob": 0.03926174342632294}, {"id": 216, "seek": 215160, "start": 2160.38, "end": 2172.16, "text": " Mae'n gwneud ond yn gennaledd, wedi a cofdown o'i gael niferion,", "tokens": [50803, 31055, 6, 77, 29255, 716, 532, 322, 67, 17861, 1049, 77, 5573, 67, 11, 6393, 72, 257, 598, 69, 5093, 277, 6, 72, 290, 4300, 297, 9361, 313, 11, 51392], "temperature": 1.0, "avg_logprob": -3.3765131632486978, "compression_ratio": 1.1868131868131868, "no_speech_prob": 0.03926174342632294}, {"id": 217, "seek": 217216, "start": 2172.16, "end": 2177.2599999999998, "text": " llaoi i am\u03b9\u03bf overseas ychydig wedi ai'r hyn am misgfemi", "tokens": [50364, 220, 3505, 4869, 741, 669, 26330, 16274, 288, 339, 6655, 328, 6393, 72, 9783, 6, 81, 2477, 77, 669, 3346, 70, 69, 13372, 50619], "temperature": 1.0, "avg_logprob": -4.021936504657452, "compression_ratio": 1.2232142857142858, "no_speech_prob": 0.022591793909668922}, {"id": 218, "seek": 217216, "start": 2177.3399999999997, "end": 2192.92, "text": " seisin ag llwy", "tokens": [50623, 369, 33787, 623, 4849, 9726, 51402], "temperature": 1.0, "avg_logprob": -4.021936504657452, "compression_ratio": 1.2232142857142858, "no_speech_prob": 0.022591793909668922}, {"id": 219, "seek": 217216, "start": 2192.96, "end": 2199.12, "text": " na'ch wneud ar hyn o hyd llwyddi, i hwnna, gael dd sel m sorcera", "tokens": [51404, 1667, 6, 339, 261, 716, 532, 594, 2477, 77, 277, 5796, 4849, 86, 6655, 4504, 11, 741, 276, 895, 629, 11, 290, 4300, 274, 67, 5851, 275, 41349, 64, 51712], "temperature": 1.0, "avg_logprob": -4.021936504657452, "compression_ratio": 1.2232142857142858, "no_speech_prob": 0.022591793909668922}, {"id": 220, "seek": 219912, "start": 2199.12, "end": 2202.66, "text": " iawn i'r Fyged growth Yw'r Oorferd Folaid theoriaeth mewn cyd y fry Jaime", "tokens": [50364, 20721, 895, 741, 6, 81, 479, 88, 3004, 4599, 398, 86, 6, 81, 422, 284, 612, 67, 479, 4711, 327, 264, 8172, 3293, 385, 895, 3185, 67, 288, 13776, 46119, 50541], "temperature": 1.0, "avg_logprob": -3.964274783250762, "compression_ratio": 1.4673202614379084, "no_speech_prob": 0.021896956488490105}, {"id": 221, "seek": 219912, "start": 2202.66, "end": 2207.3199999999997, "text": " Meddor C ar y rueth Cenaiswn ni'n samp lun prosg uppu pho t Momo", "tokens": [50541, 3982, 67, 284, 383, 594, 288, 5420, 3293, 383, 268, 1527, 895, 3867, 6, 77, 34098, 19039, 6267, 70, 11775, 84, 903, 78, 256, 47984, 50774], "temperature": 1.0, "avg_logprob": -3.964274783250762, "compression_ratio": 1.4673202614379084, "no_speech_prob": 0.021896956488490105}, {"id": 222, "seek": 219912, "start": 2207.3199999999997, "end": 2210.96, "text": " astad i'n ein bachMyw yma ar gyfnodd Rhaid Fygedr", "tokens": [50774, 5357, 345, 741, 6, 77, 1343, 272, 608, 8506, 86, 288, 1696, 594, 15823, 69, 77, 378, 67, 497, 1641, 327, 479, 88, 3004, 81, 50956], "temperature": 1.0, "avg_logprob": -3.964274783250762, "compression_ratio": 1.4673202614379084, "no_speech_prob": 0.021896956488490105}, {"id": 223, "seek": 219912, "start": 2210.96, "end": 2214.08, "text": " Diolch ar gyfer hwn i'r Ro Virus Fog \u0442\u043e\u043bwy...", "tokens": [50956, 8789, 401, 339, 594, 15823, 612, 276, 895, 741, 6, 81, 3101, 39790, 479, 664, 36038, 9726, 485, 51112], "temperature": 1.0, "avg_logprob": -3.964274783250762, "compression_ratio": 1.4673202614379084, "no_speech_prob": 0.021896956488490105}, {"id": 224, "seek": 219912, "start": 2214.08, "end": 2218.2599999999998, "text": " ...ynghyd gy ved Cynyd rel Fygedr honw i'r fnod i'n tufyniadhe iddopeth yma y gweithiau", "tokens": [51112, 1097, 88, 872, 21591, 15823, 14267, 383, 2534, 6655, 1039, 479, 88, 3004, 81, 2157, 86, 741, 6, 81, 283, 77, 378, 741, 6, 77, 2604, 69, 2534, 38069, 675, 4496, 67, 404, 3293, 288, 1696, 288, 290, 826, 355, 45274, 51321], "temperature": 1.0, "avg_logprob": -3.964274783250762, "compression_ratio": 1.4673202614379084, "no_speech_prob": 0.021896956488490105}, {"id": 225, "seek": 219912, "start": 2218.2599999999998, "end": 2221.62, "text": " neu yn nar\u771f Curioroedd ar gyfer rhowch \u00e2 boffraedd y pwyntill", "tokens": [51321, 22510, 17861, 6714, 6303, 7907, 1004, 340, 38786, 594, 15823, 612, 367, 4286, 339, 20621, 748, 602, 424, 38786, 288, 280, 9726, 580, 373, 51489], "temperature": 1.0, "avg_logprob": -3.964274783250762, "compression_ratio": 1.4673202614379084, "no_speech_prob": 0.021896956488490105}, {"id": 226, "seek": 219912, "start": 2221.62, "end": 2225.88, "text": " ddoseud o effaith yi'r Llyfr yma rydym hanfodd d communism", "tokens": [51489, 274, 67, 541, 532, 277, 1244, 64, 355, 288, 72, 6, 81, 441, 356, 5779, 288, 1696, 20791, 3173, 76, 7276, 69, 378, 67, 274, 42160, 51702], "temperature": 1.0, "avg_logprob": -3.964274783250762, "compression_ratio": 1.4673202614379084, "no_speech_prob": 0.021896956488490105}, {"id": 227, "seek": 222588, "start": 2225.88, "end": 2234.2000000000003, "text": " dweud yn ddeud democrat. Rydyn ni'n ei chylygu diwethaf y gallwllais liquids mae yw'r eff cheatuppau pleannam ar y liar mewn ilmenau", "tokens": [50364, 274, 826, 532, 17861, 274, 1479, 532, 37221, 13, 497, 6655, 2534, 3867, 6, 77, 14020, 417, 88, 356, 2794, 1026, 86, 302, 1641, 69, 288, 8527, 86, 285, 1527, 38960, 43783, 288, 86, 6, 81, 1244, 17470, 10504, 1459, 3362, 969, 335, 594, 288, 27323, 385, 895, 1930, 2558, 1459, 50780], "temperature": 1.0, "avg_logprob": -3.9948360143082864, "compression_ratio": 1.3806451612903226, "no_speech_prob": 0.022384680807590485}, {"id": 228, "seek": 222588, "start": 2234.76, "end": 2255.58, "text": " Mae nid yn ein cael cyn deafnaetennu am y chylynadau fossemwjent hynny ac mae'n!]", "tokens": [50808, 31055, 297, 327, 17861, 1343, 1335, 338, 28365, 15559, 629, 302, 1857, 84, 669, 288, 417, 88, 356, 36567, 1459, 24528, 76, 86, 73, 317, 2477, 77, 1634, 696, 43783, 6, 77, 7003, 51849], "temperature": 1.0, "avg_logprob": -3.9948360143082864, "compression_ratio": 1.3806451612903226, "no_speech_prob": 0.022384680807590485}, {"id": 229, "seek": 225588, "start": 2255.88, "end": 2270.88, "text": " If you subscribe to this scheme anatomically and the theory being a metaphor for neuronal activity in the brain, what you'd end up claiming is that the goodness of a policy, is negative expected for energy, is encoded in the", "tokens": [50364, 759, 291, 3022, 281, 341, 12232, 21618, 298, 984, 293, 264, 5261, 885, 257, 19157, 337, 12087, 21523, 5191, 294, 264, 3567, 11, 437, 291, 1116, 917, 493, 19232, 307, 300, 264, 8387, 295, 257, 3897, 11, 307, 3671, 5176, 337, 2281, 11, 307, 2058, 12340, 294, 264, 51114], "temperature": 0.8, "avg_logprob": -0.5115813475388747, "compression_ratio": 1.4640522875816993, "no_speech_prob": 0.035277221351861954}, {"id": 230, "seek": 227088, "start": 2270.88, "end": 2279.88, "text": " is encoded in the call date for high-level, more abstract representations in the deep-generative model,", "tokens": [50364, 307, 2058, 12340, 294, 264, 818, 4002, 337, 1090, 12, 12418, 11, 544, 12649, 33358, 294, 264, 2452, 12, 21848, 1166, 2316, 11, 50814], "temperature": 0.0, "avg_logprob": -0.3804471051251447, "compression_ratio": 1.7716535433070866, "no_speech_prob": 0.044487692415714264}, {"id": 231, "seek": 227088, "start": 2279.88, "end": 2288.88, "text": " for more intermediate levels, sorry, the call date for intermediate levels for more abstract ones in the globus pallidum,", "tokens": [50814, 337, 544, 19376, 4358, 11, 2597, 11, 264, 818, 4002, 337, 19376, 4358, 337, 544, 12649, 2306, 294, 264, 16125, 301, 24075, 327, 449, 11, 51264], "temperature": 0.0, "avg_logprob": -0.3804471051251447, "compression_ratio": 1.7716535433070866, "no_speech_prob": 0.044487692415714264}, {"id": 232, "seek": 228888, "start": 2289.88, "end": 2304.88, "text": " and in the putamen for motor loops, whereas the policy expectations per se have here been assigned again to the globus pallidus internum.", "tokens": [50414, 293, 294, 264, 829, 22403, 337, 5932, 16121, 11, 9735, 264, 3897, 9843, 680, 369, 362, 510, 668, 13279, 797, 281, 264, 16125, 301, 24075, 327, 301, 2154, 449, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1998604185440961, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.09504015743732452}, {"id": 233, "seek": 228888, "start": 2304.88, "end": 2317.88, "text": " So just a way of getting from the mathematical anatomy to the biological or neuroanatomy in a purely top-down dispassionate mathematically dry way,", "tokens": [51164, 407, 445, 257, 636, 295, 1242, 490, 264, 18894, 31566, 281, 264, 13910, 420, 16499, 282, 267, 8488, 294, 257, 17491, 1192, 12, 5093, 4920, 640, 313, 473, 44003, 4016, 636, 11, 51814], "temperature": 0.0, "avg_logprob": -0.1998604185440961, "compression_ratio": 1.574585635359116, "no_speech_prob": 0.09504015743732452}, {"id": 234, "seek": 231788, "start": 2317.88, "end": 2325.88, "text": " is taking the equations and seeing what form do they imply for message-passing and what sorts of message-passing do we see in the real brain.", "tokens": [50364, 307, 1940, 264, 11787, 293, 2577, 437, 1254, 360, 436, 33616, 337, 3636, 12, 9216, 278, 293, 437, 7527, 295, 3636, 12, 9216, 278, 360, 321, 536, 294, 264, 957, 3567, 13, 50764], "temperature": 0.0, "avg_logprob": -0.20319826578356556, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0014604585012421012}, {"id": 235, "seek": 231788, "start": 2325.88, "end": 2334.88, "text": " Andre Bastos will, I think he may not, but he did a lot of work on this sort of intrinsic connectivity within a macro-column,", "tokens": [50764, 20667, 31915, 329, 486, 11, 286, 519, 415, 815, 406, 11, 457, 415, 630, 257, 688, 295, 589, 322, 341, 1333, 295, 35698, 21095, 1951, 257, 18887, 12, 8768, 16449, 11, 51214], "temperature": 0.0, "avg_logprob": -0.20319826578356556, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0014604585012421012}, {"id": 236, "seek": 231788, "start": 2334.88, "end": 2342.88, "text": " clinical micro-circuits for predictive coding, exactly the same game can be played here for this discrete state-space model,", "tokens": [51214, 9115, 4532, 12, 23568, 66, 7688, 337, 35521, 17720, 11, 2293, 264, 912, 1216, 393, 312, 3737, 510, 337, 341, 27706, 1785, 12, 24824, 2316, 11, 51614], "temperature": 0.0, "avg_logprob": -0.20319826578356556, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0014604585012421012}, {"id": 237, "seek": 234288, "start": 2342.88, "end": 2353.88, "text": " and that's one key exception which Lars might like, because there's been a lot of debate in Scotland about whether the superficial parameter cells encode predictions", "tokens": [50364, 293, 300, 311, 472, 2141, 11183, 597, 41563, 1062, 411, 11, 570, 456, 311, 668, 257, 688, 295, 7958, 294, 11180, 466, 1968, 264, 34622, 13075, 5438, 2058, 1429, 21264, 50914], "temperature": 0.0, "avg_logprob": -0.16625052232008714, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.02199142426252365}, {"id": 238, "seek": 234288, "start": 2353.88, "end": 2366.88, "text": " or expectations or prediction errors. In this scheme, in this discrete state-space scheme, the superficial parameter cells code expectations, not prediction errors,", "tokens": [50914, 420, 9843, 420, 17630, 13603, 13, 682, 341, 12232, 11, 294, 341, 27706, 1785, 12, 24824, 12232, 11, 264, 34622, 13075, 5438, 3089, 9843, 11, 406, 17630, 13603, 11, 51564], "temperature": 0.0, "avg_logprob": -0.16625052232008714, "compression_ratio": 1.7741935483870968, "no_speech_prob": 0.02199142426252365}, {"id": 239, "seek": 236688, "start": 2366.88, "end": 2373.88, "text": " and you may ask why. Well, the reason is, they do encode prediction errors, but they do it by physically in a very different way,", "tokens": [50364, 293, 291, 815, 1029, 983, 13, 1042, 11, 264, 1778, 307, 11, 436, 360, 2058, 1429, 17630, 13603, 11, 457, 436, 360, 309, 538, 9762, 294, 257, 588, 819, 636, 11, 50714], "temperature": 0.0, "avg_logprob": -0.11701138486567232, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.019345557317137718}, {"id": 240, "seek": 236688, "start": 2373.88, "end": 2383.88, "text": " and that follows from the form of this differential equation here, where we're expressing the states as a sigmoid function, a softmax operator,", "tokens": [50714, 293, 300, 10002, 490, 264, 1254, 295, 341, 15756, 5367, 510, 11, 689, 321, 434, 22171, 264, 4368, 382, 257, 4556, 3280, 327, 2445, 11, 257, 2787, 41167, 12973, 11, 51214], "temperature": 0.0, "avg_logprob": -0.11701138486567232, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.019345557317137718}, {"id": 241, "seek": 236688, "start": 2383.88, "end": 2390.88, "text": " on V, which we associate with depolarisation, where the rate of change of depolarisation or voltage is proportional to the error.", "tokens": [51214, 322, 691, 11, 597, 321, 14644, 365, 1367, 15276, 7623, 11, 689, 264, 3314, 295, 1319, 295, 1367, 15276, 7623, 420, 8352, 307, 24969, 281, 264, 6713, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11701138486567232, "compression_ratio": 1.6721991701244814, "no_speech_prob": 0.019345557317137718}, {"id": 242, "seek": 239088, "start": 2390.88, "end": 2399.88, "text": " So, the error from the point of view of a neural mass model now becomes the conductance. So, the cells are encoding prediction error, but the error is in the conductance.", "tokens": [50364, 407, 11, 264, 6713, 490, 264, 935, 295, 1910, 295, 257, 18161, 2758, 2316, 586, 3643, 264, 6018, 719, 13, 407, 11, 264, 5438, 366, 43430, 17630, 6713, 11, 457, 264, 6713, 307, 294, 264, 6018, 719, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11158811891233766, "compression_ratio": 1.7853107344632768, "no_speech_prob": 0.005077554378658533}, {"id": 243, "seek": 239088, "start": 2399.88, "end": 2410.88, "text": " So, when all the postsynaptic drives to the conductance postsynaptically are in balance and there is no further change or drive to the potential,", "tokens": [50814, 407, 11, 562, 439, 264, 12300, 2534, 2796, 299, 11754, 281, 264, 6018, 719, 12300, 2534, 2796, 984, 366, 294, 4772, 293, 456, 307, 572, 3052, 1319, 420, 3332, 281, 264, 3995, 11, 51364], "temperature": 0.0, "avg_logprob": -0.11158811891233766, "compression_ratio": 1.7853107344632768, "no_speech_prob": 0.005077554378658533}, {"id": 244, "seek": 241088, "start": 2410.88, "end": 2420.88, "text": " that means prediction errors are zero. So, when the cell or a population has reached electrodynamically steady state, it's found its minimum prediction error,", "tokens": [50364, 300, 1355, 17630, 13603, 366, 4018, 13, 407, 11, 562, 264, 2815, 420, 257, 4415, 575, 6488, 44216, 5216, 984, 13211, 1785, 11, 309, 311, 1352, 1080, 7285, 17630, 6713, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11027748710230777, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.2985118627548218}, {"id": 245, "seek": 241088, "start": 2420.88, "end": 2432.88, "text": " and then it fires, but it is the firing here that is associated with the expected states of the world here.", "tokens": [50864, 293, 550, 309, 15044, 11, 457, 309, 307, 264, 16045, 510, 300, 307, 6615, 365, 264, 5176, 4368, 295, 264, 1002, 510, 13, 51464], "temperature": 0.0, "avg_logprob": -0.11027748710230777, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.2985118627548218}, {"id": 246, "seek": 241088, "start": 2432.88, "end": 2439.88, "text": " So, that's an interesting thing which I thought might be useful for discussion in terms of reconciling a lot of paradoxes about what's been passed forward,", "tokens": [51464, 407, 11, 300, 311, 364, 1880, 551, 597, 286, 1194, 1062, 312, 4420, 337, 5017, 294, 2115, 295, 9993, 3208, 278, 257, 688, 295, 26221, 279, 466, 437, 311, 668, 4678, 2128, 11, 51814], "temperature": 0.0, "avg_logprob": -0.11027748710230777, "compression_ratio": 1.66798418972332, "no_speech_prob": 0.2985118627548218}, {"id": 247, "seek": 243988, "start": 2439.88, "end": 2447.88, "text": " and would you expect it to be expended away or would you expect it to be boosted, sharpened, all sorts of interesting issues here.", "tokens": [50364, 293, 576, 291, 2066, 309, 281, 312, 1278, 3502, 1314, 420, 576, 291, 2066, 309, 281, 312, 9194, 292, 11, 31570, 292, 11, 439, 7527, 295, 1880, 2663, 510, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1416645600245549, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.001526995562016964}, {"id": 248, "seek": 243988, "start": 2447.88, "end": 2456.88, "text": " Closing with an example of reading, a deep hierarchical model where we have beliefs about six sentences, each comprising four words.", "tokens": [50764, 2033, 6110, 365, 364, 1365, 295, 3760, 11, 257, 2452, 35250, 804, 2316, 689, 321, 362, 13585, 466, 2309, 16579, 11, 1184, 16802, 3436, 1451, 2283, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1416645600245549, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.001526995562016964}, {"id": 249, "seek": 243988, "start": 2456.88, "end": 2468.88, "text": " Each word here has iconic letters that can either be in an uppercase or a lowercase, a palindromic in the sense that it doesn't matter whether the cat has to flee", "tokens": [51214, 6947, 1349, 510, 575, 15762, 7825, 300, 393, 2139, 312, 294, 364, 11775, 2869, 651, 420, 257, 3126, 9765, 11, 257, 3984, 471, 4397, 299, 294, 264, 2020, 300, 309, 1177, 380, 1871, 1968, 264, 3857, 575, 281, 25146, 51814], "temperature": 0.0, "avg_logprob": -0.1416645600245549, "compression_ratio": 1.717741935483871, "no_speech_prob": 0.001526995562016964}, {"id": 250, "seek": 246888, "start": 2468.88, "end": 2480.88, "text": " from the cat, it doesn't matter whether we flip them in a horizontal way, it still means the same thing, but it does, this agent is surprised if we use a lowercase.", "tokens": [50364, 490, 264, 3857, 11, 309, 1177, 380, 1871, 1968, 321, 7929, 552, 294, 257, 12750, 636, 11, 309, 920, 1355, 264, 912, 551, 11, 457, 309, 775, 11, 341, 9461, 307, 6100, 498, 321, 764, 257, 3126, 9765, 13, 50964], "temperature": 0.0, "avg_logprob": -0.20065122842788696, "compression_ratio": 1.3615384615384616, "no_speech_prob": 0.007287951651960611}, {"id": 251, "seek": 246888, "start": 2480.88, "end": 2481.88, "text": " Three words.", "tokens": [50964, 6244, 2283, 13, 51014], "temperature": 0.0, "avg_logprob": -0.20065122842788696, "compression_ratio": 1.3615384615384616, "no_speech_prob": 0.007287951651960611}, {"id": 252, "seek": 248188, "start": 2481.88, "end": 2498.88, "text": " Let me just skip through this because we want to spend more time in discussion if we can.", "tokens": [50364, 961, 385, 445, 10023, 807, 341, 570, 321, 528, 281, 3496, 544, 565, 294, 5017, 498, 321, 393, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19794955620398888, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.17099027335643768}, {"id": 253, "seek": 248188, "start": 2498.88, "end": 2510.88, "text": " So, that's just a generative model with two levels, semantics or sentence structure, word structure and outcomes generating particular, if you like, letters, but there are icons in this instance.", "tokens": [51214, 407, 11, 300, 311, 445, 257, 1337, 1166, 2316, 365, 732, 4358, 11, 4361, 45298, 420, 8174, 3877, 11, 1349, 3877, 293, 10070, 17746, 1729, 11, 498, 291, 411, 11, 7825, 11, 457, 456, 366, 23308, 294, 341, 5197, 13, 51814], "temperature": 0.0, "avg_logprob": -0.19794955620398888, "compression_ratio": 1.507936507936508, "no_speech_prob": 0.17099027335643768}, {"id": 254, "seek": 251088, "start": 2510.88, "end": 2514.88, "text": " And then with this scheme, we can simulate things like reading.", "tokens": [50364, 400, 550, 365, 341, 12232, 11, 321, 393, 27817, 721, 411, 3760, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13700684462443435, "compression_ratio": 1.8985507246376812, "no_speech_prob": 0.006969338748604059}, {"id": 255, "seek": 251088, "start": 2514.88, "end": 2527.88, "text": " So, here's a little four page story or sentence and that word is flee, that word is wait because there's nothing next to the bird, that word is feed because there are seeds that the bird can feed on, and that is wait.", "tokens": [50564, 407, 11, 510, 311, 257, 707, 1451, 3028, 1657, 420, 8174, 293, 300, 1349, 307, 25146, 11, 300, 1349, 307, 1699, 570, 456, 311, 1825, 958, 281, 264, 5255, 11, 300, 1349, 307, 3154, 570, 456, 366, 9203, 300, 264, 5255, 393, 3154, 322, 11, 293, 300, 307, 1699, 13, 51214], "temperature": 0.0, "avg_logprob": -0.13700684462443435, "compression_ratio": 1.8985507246376812, "no_speech_prob": 0.006969338748604059}, {"id": 256, "seek": 251088, "start": 2527.88, "end": 2535.88, "text": " So, this is a sentence, flee, wait, feed, wait, and that's a happy sentence and it will categorise it as happy.", "tokens": [51214, 407, 11, 341, 307, 257, 8174, 11, 25146, 11, 1699, 11, 3154, 11, 1699, 11, 293, 300, 311, 257, 2055, 8174, 293, 309, 486, 19250, 908, 309, 382, 2055, 13, 51614], "temperature": 0.0, "avg_logprob": -0.13700684462443435, "compression_ratio": 1.8985507246376812, "no_speech_prob": 0.006969338748604059}, {"id": 257, "seek": 253588, "start": 2535.88, "end": 2541.88, "text": " But the problem that we're trying to address here is exactly what we started with.", "tokens": [50364, 583, 264, 1154, 300, 321, 434, 1382, 281, 2985, 510, 307, 2293, 437, 321, 1409, 365, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10130250830399362, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.04461073502898216}, {"id": 258, "seek": 253588, "start": 2541.88, "end": 2549.88, "text": " How do you scan? How do you search? Where do you go forage for information to resolve as much uncertainty as you can about which of these six sentences is in play?", "tokens": [50664, 1012, 360, 291, 11049, 30, 1012, 360, 291, 3164, 30, 2305, 360, 291, 352, 337, 609, 337, 1589, 281, 14151, 382, 709, 15697, 382, 291, 393, 466, 597, 295, 613, 2309, 16579, 307, 294, 862, 30, 51064], "temperature": 0.0, "avg_logprob": -0.10130250830399362, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.04461073502898216}, {"id": 259, "seek": 253588, "start": 2549.88, "end": 2563.88, "text": " And when the system does this just by trying to minimise its expected free energy, it shows this very interesting behaviour where it jumps from one word or page to the next", "tokens": [51064, 400, 562, 264, 1185, 775, 341, 445, 538, 1382, 281, 4464, 908, 1080, 5176, 1737, 2281, 11, 309, 3110, 341, 588, 1880, 17229, 689, 309, 16704, 490, 472, 1349, 420, 3028, 281, 264, 958, 51764], "temperature": 0.0, "avg_logprob": -0.10130250830399362, "compression_ratio": 1.6177606177606179, "no_speech_prob": 0.04461073502898216}, {"id": 260, "seek": 256388, "start": 2563.88, "end": 2569.88, "text": " without really dwelling and wasting time resolving uncertainty that is already resolved.", "tokens": [50364, 1553, 534, 41750, 293, 20457, 565, 49940, 15697, 300, 307, 1217, 20772, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1620959064416718, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.035036031156778336}, {"id": 261, "seek": 256388, "start": 2569.88, "end": 2579.88, "text": " So, once it sees a cat, it already knows that this has to be a flea word and it doesn't need to see where the other letters in this word are actually doing.", "tokens": [50664, 407, 11, 1564, 309, 8194, 257, 3857, 11, 309, 1217, 3255, 300, 341, 575, 281, 312, 257, 7025, 64, 1349, 293, 309, 1177, 380, 643, 281, 536, 689, 264, 661, 7825, 294, 341, 1349, 366, 767, 884, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1620959064416718, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.035036031156778336}, {"id": 262, "seek": 256388, "start": 2579.88, "end": 2584.88, "text": " It already knows, there's no more epistemic value to be had, there's no more uncertainty to resolve.", "tokens": [51164, 467, 1217, 3255, 11, 456, 311, 572, 544, 2388, 468, 3438, 2158, 281, 312, 632, 11, 456, 311, 572, 544, 15697, 281, 14151, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1620959064416718, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.035036031156778336}, {"id": 263, "seek": 256388, "start": 2584.88, "end": 2591.88, "text": " It'll now jump to the next page and resolves uncertainty after a couple of Sokelechi movements, then jump to the next page.", "tokens": [51414, 467, 603, 586, 3012, 281, 264, 958, 3028, 293, 7923, 977, 15697, 934, 257, 1916, 295, 407, 330, 306, 8036, 9981, 11, 550, 3012, 281, 264, 958, 3028, 13, 51764], "temperature": 0.0, "avg_logprob": -0.1620959064416718, "compression_ratio": 1.8875502008032128, "no_speech_prob": 0.035036031156778336}, {"id": 264, "seek": 259188, "start": 2591.88, "end": 2598.88, "text": " And after this once a card in the final page, it knows exactly what this sentence was doing.", "tokens": [50364, 400, 934, 341, 1564, 257, 2920, 294, 264, 2572, 3028, 11, 309, 3255, 2293, 437, 341, 8174, 390, 884, 13, 50714], "temperature": 0.0, "avg_logprob": -0.191721621013823, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.006876163650304079}, {"id": 265, "seek": 259188, "start": 2598.88, "end": 2610.88, "text": " And if I can, I'll just show a movie of it doing that.", "tokens": [50714, 400, 498, 286, 393, 11, 286, 603, 445, 855, 257, 3169, 295, 309, 884, 300, 13, 51314], "temperature": 0.0, "avg_logprob": -0.191721621013823, "compression_ratio": 1.2564102564102564, "no_speech_prob": 0.006876163650304079}, {"id": 266, "seek": 261088, "start": 2610.88, "end": 2622.88, "text": " So, the red dots correspond to where it's looking at the present time and the images that are mixtures of the icons represent conditional expectations.", "tokens": [50364, 407, 11, 264, 2182, 15026, 6805, 281, 689, 309, 311, 1237, 412, 264, 1974, 565, 293, 264, 5267, 300, 366, 2752, 37610, 295, 264, 23308, 2906, 27708, 9843, 13, 50964], "temperature": 0.0, "avg_logprob": -0.15217291954720374, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.13188600540161133}, {"id": 267, "seek": 261088, "start": 2622.88, "end": 2628.88, "text": " And the main point to be taken from this is that it knows there's a bird there, but it never looked there.", "tokens": [50964, 400, 264, 2135, 935, 281, 312, 2726, 490, 341, 307, 300, 309, 3255, 456, 311, 257, 5255, 456, 11, 457, 309, 1128, 2956, 456, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15217291954720374, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.13188600540161133}, {"id": 268, "seek": 261088, "start": 2628.88, "end": 2632.88, "text": " It has sufficient prior knowledge in its deep, depth temporal model.", "tokens": [51264, 467, 575, 11563, 4059, 3601, 294, 1080, 2452, 11, 7161, 30881, 2316, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15217291954720374, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.13188600540161133}, {"id": 269, "seek": 261088, "start": 2632.88, "end": 2639.88, "text": " It doesn't need to actually go and see stuff. It knows stuff is there because it knows what caused that stuff.", "tokens": [51464, 467, 1177, 380, 643, 281, 767, 352, 293, 536, 1507, 13, 467, 3255, 1507, 307, 456, 570, 309, 3255, 437, 7008, 300, 1507, 13, 51814], "temperature": 0.0, "avg_logprob": -0.15217291954720374, "compression_ratio": 1.7312252964426877, "no_speech_prob": 0.13188600540161133}, {"id": 270, "seek": 263988, "start": 2639.88, "end": 2647.88, "text": " And with this sort of simulation, one can then do exactly what Jim was talking about, which was if it knows stuff and it has predictions,", "tokens": [50364, 400, 365, 341, 1333, 295, 16575, 11, 472, 393, 550, 360, 2293, 437, 6637, 390, 1417, 466, 11, 597, 390, 498, 309, 3255, 1507, 293, 309, 575, 21264, 11, 50764], "temperature": 0.0, "avg_logprob": -0.08863447714542998, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.007105167955160141}, {"id": 271, "seek": 263988, "start": 2647.88, "end": 2655.88, "text": " then it should be possible to disclose or reveal that knowledge, that predictability by introducing violations", "tokens": [50764, 550, 309, 820, 312, 1944, 281, 36146, 420, 10658, 300, 3601, 11, 300, 6069, 2310, 538, 15424, 30405, 51164], "temperature": 0.0, "avg_logprob": -0.08863447714542998, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.007105167955160141}, {"id": 272, "seek": 263988, "start": 2655.88, "end": 2660.88, "text": " and elicit the sorts of classical responses that we see empirically.", "tokens": [51164, 293, 806, 8876, 264, 7527, 295, 13735, 13019, 300, 321, 536, 25790, 984, 13, 51414], "temperature": 0.0, "avg_logprob": -0.08863447714542998, "compression_ratio": 1.5771144278606966, "no_speech_prob": 0.007105167955160141}, {"id": 273, "seek": 266088, "start": 2660.88, "end": 2665.88, "text": " And what we've done here is because we've got a deep model, we can do local and global violations.", "tokens": [50364, 400, 437, 321, 600, 1096, 510, 307, 570, 321, 600, 658, 257, 2452, 2316, 11, 321, 393, 360, 2654, 293, 4338, 30405, 13, 50614], "temperature": 0.0, "avg_logprob": -0.15004621092806159, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.4972880482673645}, {"id": 274, "seek": 266088, "start": 2665.88, "end": 2672.88, "text": " We can make the final story, the final sentence, a very surprising one without changing any of the stimuli,", "tokens": [50614, 492, 393, 652, 264, 2572, 1657, 11, 264, 2572, 8174, 11, 257, 588, 8830, 472, 1553, 4473, 604, 295, 264, 47752, 11, 50964], "temperature": 0.0, "avg_logprob": -0.15004621092806159, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.4972880482673645}, {"id": 275, "seek": 266088, "start": 2672.88, "end": 2682.88, "text": " at the same time with or without making the prior beliefs about the upper lower case, the sort of local featureal expectations.", "tokens": [50964, 412, 264, 912, 565, 365, 420, 1553, 1455, 264, 4059, 13585, 466, 264, 6597, 3126, 1389, 11, 264, 1333, 295, 2654, 4111, 304, 9843, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15004621092806159, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.4972880482673645}, {"id": 276, "seek": 266088, "start": 2682.88, "end": 2688.88, "text": " We can switch those around so we replay exactly the same stimuli and the same behaviors,", "tokens": [51464, 492, 393, 3679, 729, 926, 370, 321, 23836, 2293, 264, 912, 47752, 293, 264, 912, 15501, 11, 51764], "temperature": 0.0, "avg_logprob": -0.15004621092806159, "compression_ratio": 1.7407407407407407, "no_speech_prob": 0.4972880482673645}, {"id": 277, "seek": 268888, "start": 2688.88, "end": 2694.88, "text": " but just by changing the prior beliefs of the agent, we can cause certain things to be surprising,", "tokens": [50364, 457, 445, 538, 4473, 264, 4059, 13585, 295, 264, 9461, 11, 321, 393, 3082, 1629, 721, 281, 312, 8830, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10602785616504903, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.003083608578890562}, {"id": 278, "seek": 268888, "start": 2694.88, "end": 2699.88, "text": " and those things can either be at the local, the first level, or the higher, the second level.", "tokens": [50664, 293, 729, 721, 393, 2139, 312, 412, 264, 2654, 11, 264, 700, 1496, 11, 420, 264, 2946, 11, 264, 1150, 1496, 13, 50914], "temperature": 0.0, "avg_logprob": -0.10602785616504903, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.003083608578890562}, {"id": 279, "seek": 268888, "start": 2699.88, "end": 2708.88, "text": " And if we do that, we get lots of behaviors that look again a little bit like delay period activity", "tokens": [50914, 400, 498, 321, 360, 300, 11, 321, 483, 3195, 295, 15501, 300, 574, 797, 257, 707, 857, 411, 8577, 2896, 5191, 51364], "temperature": 0.0, "avg_logprob": -0.10602785616504903, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.003083608578890562}, {"id": 280, "seek": 268888, "start": 2708.88, "end": 2716.88, "text": " in the prefrontal cortex of a periscadic sort that you see prior to a saccade being selected and enacted.", "tokens": [51364, 294, 264, 659, 11496, 304, 33312, 295, 257, 680, 5606, 43341, 1333, 300, 291, 536, 4059, 281, 257, 4899, 30340, 885, 8209, 293, 41313, 13, 51764], "temperature": 0.0, "avg_logprob": -0.10602785616504903, "compression_ratio": 1.705128205128205, "no_speech_prob": 0.003083608578890562}, {"id": 281, "seek": 271688, "start": 2716.88, "end": 2725.88, "text": " While at the same time the band pass filtered voltages that are being driven by the implicit prediction errors", "tokens": [50364, 3987, 412, 264, 912, 565, 264, 4116, 1320, 37111, 49614, 300, 366, 885, 9555, 538, 264, 26947, 17630, 13603, 50814], "temperature": 0.0, "avg_logprob": -0.12452097942954615, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.004572104662656784}, {"id": 282, "seek": 271688, "start": 2725.88, "end": 2729.88, "text": " look very much like periscadic ERPs.", "tokens": [50814, 574, 588, 709, 411, 680, 5606, 43341, 14929, 23043, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12452097942954615, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.004572104662656784}, {"id": 283, "seek": 271688, "start": 2729.88, "end": 2734.88, "text": " And when you look at those periscadic ERPs under local versus global violations,", "tokens": [51014, 400, 562, 291, 574, 412, 729, 680, 5606, 43341, 14929, 23043, 833, 2654, 5717, 4338, 30405, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12452097942954615, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.004572104662656784}, {"id": 284, "seek": 271688, "start": 2734.88, "end": 2740.88, "text": " what you actually see is something almost identical if it's a local violation to a mismatch negativity,", "tokens": [51264, 437, 291, 767, 536, 307, 746, 1920, 14800, 498, 309, 311, 257, 2654, 22840, 281, 257, 23220, 852, 39297, 11, 51564], "temperature": 0.0, "avg_logprob": -0.12452097942954615, "compression_ratio": 1.5734597156398105, "no_speech_prob": 0.004572104662656784}, {"id": 285, "seek": 274088, "start": 2740.88, "end": 2749.88, "text": " whereas, for the global violation, you get the mismatches or the differences much later on in time,", "tokens": [50364, 9735, 11, 337, 264, 4338, 22840, 11, 291, 483, 264, 23220, 852, 279, 420, 264, 7300, 709, 1780, 322, 294, 565, 11, 50814], "temperature": 0.0, "avg_logprob": -0.17804176576675906, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.054528363049030304}, {"id": 286, "seek": 274088, "start": 2749.88, "end": 2752.88, "text": " very much like a P300.", "tokens": [50814, 588, 709, 411, 257, 430, 12566, 13, 50964], "temperature": 0.0, "avg_logprob": -0.17804176576675906, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.054528363049030304}, {"id": 287, "seek": 274088, "start": 2752.88, "end": 2757.88, "text": " I can see what I was going to show you. No, I can't. I can't.", "tokens": [50964, 286, 393, 536, 437, 286, 390, 516, 281, 855, 291, 13, 883, 11, 286, 393, 380, 13, 286, 393, 380, 13, 51214], "temperature": 0.0, "avg_logprob": -0.17804176576675906, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.054528363049030304}, {"id": 288, "seek": 274088, "start": 2757.88, "end": 2761.88, "text": " That was a very pretty slide, but I can't.", "tokens": [51214, 663, 390, 257, 588, 1238, 4137, 11, 457, 286, 393, 380, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17804176576675906, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.054528363049030304}, {"id": 289, "seek": 274088, "start": 2761.88, "end": 2766.88, "text": " Very clever. It's a quote from, well, you don't need to know that.", "tokens": [51414, 4372, 13494, 13, 467, 311, 257, 6513, 490, 11, 731, 11, 291, 500, 380, 643, 281, 458, 300, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17804176576675906, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.054528363049030304}, {"id": 290, "seek": 276688, "start": 2766.88, "end": 2771.88, "text": " And then the final slide, it's got a thank you.", "tokens": [50364, 400, 550, 264, 2572, 4137, 11, 309, 311, 658, 257, 1309, 291, 13, 50614], "temperature": 0.0, "avg_logprob": -0.1064362876555499, "compression_ratio": 1.4723926380368098, "no_speech_prob": 0.0015804105205461383}, {"id": 291, "seek": 276688, "start": 2771.88, "end": 2776.88, "text": " A lot of people were on this slide, but you'll never know who now, will you?", "tokens": [50614, 316, 688, 295, 561, 645, 322, 341, 4137, 11, 457, 291, 603, 1128, 458, 567, 586, 11, 486, 291, 30, 50864], "temperature": 0.0, "avg_logprob": -0.1064362876555499, "compression_ratio": 1.4723926380368098, "no_speech_prob": 0.0015804105205461383}, {"id": 292, "seek": 276688, "start": 2776.88, "end": 2778.88, "text": " So thank you very much.", "tokens": [50864, 407, 1309, 291, 588, 709, 13, 50964], "temperature": 0.0, "avg_logprob": -0.1064362876555499, "compression_ratio": 1.4723926380368098, "no_speech_prob": 0.0015804105205461383}, {"id": 293, "seek": 276688, "start": 2787.88, "end": 2795.88, "text": " So the workshop is structured so that there's a lot of time for discussion after each talk.", "tokens": [51414, 407, 264, 13541, 307, 18519, 370, 300, 456, 311, 257, 688, 295, 565, 337, 5017, 934, 1184, 751, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1064362876555499, "compression_ratio": 1.4723926380368098, "no_speech_prob": 0.0015804105205461383}, {"id": 294, "seek": 279588, "start": 2795.88, "end": 2803.88, "text": " And so the floor is open now for people to ask questions or make observations.", "tokens": [50364, 400, 370, 264, 4123, 307, 1269, 586, 337, 561, 281, 1029, 1651, 420, 652, 18163, 13, 50764], "temperature": 0.0, "avg_logprob": -0.09610954702716984, "compression_ratio": 1.495049504950495, "no_speech_prob": 0.0025041387416422367}, {"id": 295, "seek": 279588, "start": 2803.88, "end": 2810.88, "text": " I was just informed about how they do this in philosophy conferences that involves raising your hand or your finger,", "tokens": [50764, 286, 390, 445, 11740, 466, 577, 436, 360, 341, 294, 10675, 22032, 300, 11626, 11225, 428, 1011, 420, 428, 5984, 11, 51114], "temperature": 0.0, "avg_logprob": -0.09610954702716984, "compression_ratio": 1.495049504950495, "no_speech_prob": 0.0025041387416422367}, {"id": 296, "seek": 279588, "start": 2810.88, "end": 2814.88, "text": " but I haven't mastered that yet, so I don't understand it.", "tokens": [51114, 457, 286, 2378, 380, 38686, 300, 1939, 11, 370, 286, 500, 380, 1223, 309, 13, 51314], "temperature": 0.0, "avg_logprob": -0.09610954702716984, "compression_ratio": 1.495049504950495, "no_speech_prob": 0.0025041387416422367}, {"id": 297, "seek": 279588, "start": 2814.88, "end": 2821.88, "text": " So I think it's too complicated for this group.", "tokens": [51314, 407, 286, 519, 309, 311, 886, 6179, 337, 341, 1594, 13, 51664], "temperature": 0.0, "avg_logprob": -0.09610954702716984, "compression_ratio": 1.495049504950495, "no_speech_prob": 0.0025041387416422367}, {"id": 298, "seek": 282188, "start": 2821.88, "end": 2825.88, "text": " But not for philosophers.", "tokens": [50364, 583, 406, 337, 36839, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14744056354869495, "compression_ratio": 1.3612903225806452, "no_speech_prob": 0.0016680480912327766}, {"id": 299, "seek": 282188, "start": 2825.88, "end": 2833.88, "text": " So who would like to start?", "tokens": [50564, 407, 567, 576, 411, 281, 722, 30, 50964], "temperature": 0.0, "avg_logprob": -0.14744056354869495, "compression_ratio": 1.3612903225806452, "no_speech_prob": 0.0016680480912327766}, {"id": 300, "seek": 282188, "start": 2833.88, "end": 2836.88, "text": " I'll start.", "tokens": [50964, 286, 603, 722, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14744056354869495, "compression_ratio": 1.3612903225806452, "no_speech_prob": 0.0016680480912327766}, {"id": 301, "seek": 282188, "start": 2836.88, "end": 2839.88, "text": " I was just wondering if you could say more.", "tokens": [51114, 286, 390, 445, 6359, 498, 291, 727, 584, 544, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14744056354869495, "compression_ratio": 1.3612903225806452, "no_speech_prob": 0.0016680480912327766}, {"id": 302, "seek": 282188, "start": 2839.88, "end": 2848.88, "text": " You mentioned that you get something when there's choice involved with the rat experiment simulation.", "tokens": [51264, 509, 2835, 300, 291, 483, 746, 562, 456, 311, 3922, 3288, 365, 264, 5937, 5120, 16575, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14744056354869495, "compression_ratio": 1.3612903225806452, "no_speech_prob": 0.0016680480912327766}, {"id": 303, "seek": 284888, "start": 2848.88, "end": 2851.88, "text": " Something that looks like a drift diffusion model.", "tokens": [50364, 6595, 300, 1542, 411, 257, 19699, 25242, 2316, 13, 50514], "temperature": 0.0, "avg_logprob": -0.07999482479962436, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.007987787015736103}, {"id": 304, "seek": 284888, "start": 2851.88, "end": 2859.88, "text": " And I've always been puzzled at how you get something that looks really like choice or agency out of a predictive coding model.", "tokens": [50514, 400, 286, 600, 1009, 668, 18741, 1493, 412, 577, 291, 483, 746, 300, 1542, 534, 411, 3922, 420, 7934, 484, 295, 257, 35521, 17720, 2316, 13, 50914], "temperature": 0.0, "avg_logprob": -0.07999482479962436, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.007987787015736103}, {"id": 305, "seek": 284888, "start": 2859.88, "end": 2863.88, "text": " So maybe you can elaborate a little bit on that.", "tokens": [50914, 407, 1310, 291, 393, 20945, 257, 707, 857, 322, 300, 13, 51114], "temperature": 0.0, "avg_logprob": -0.07999482479962436, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.007987787015736103}, {"id": 306, "seek": 284888, "start": 2863.88, "end": 2869.88, "text": " So the question is, where does the choice come into predictive coding?", "tokens": [51114, 407, 264, 1168, 307, 11, 689, 775, 264, 3922, 808, 666, 35521, 17720, 30, 51414], "temperature": 0.0, "avg_logprob": -0.07999482479962436, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.007987787015736103}, {"id": 307, "seek": 284888, "start": 2869.88, "end": 2871.88, "text": " I think that question.", "tokens": [51414, 286, 519, 300, 1168, 13, 51514], "temperature": 0.0, "avg_logprob": -0.07999482479962436, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.007987787015736103}, {"id": 308, "seek": 284888, "start": 2871.88, "end": 2874.88, "text": " Let me just be a little more specific.", "tokens": [51514, 961, 385, 445, 312, 257, 707, 544, 2685, 13, 51664], "temperature": 0.0, "avg_logprob": -0.07999482479962436, "compression_ratio": 1.651376146788991, "no_speech_prob": 0.007987787015736103}, {"id": 309, "seek": 287488, "start": 2874.88, "end": 2886.88, "text": " It's not that I don't see how you get choice behavior in the sense that you can use predictive coding in order to evaluate some options.", "tokens": [50364, 467, 311, 406, 300, 286, 500, 380, 536, 577, 291, 483, 3922, 5223, 294, 264, 2020, 300, 291, 393, 764, 35521, 17720, 294, 1668, 281, 13059, 512, 3956, 13, 50964], "temperature": 0.0, "avg_logprob": -0.06848363022306073, "compression_ratio": 1.6358381502890174, "no_speech_prob": 0.004724507685750723}, {"id": 310, "seek": 287488, "start": 2886.88, "end": 2903.88, "text": " But the notion of agency seems, if what you're doing is just predicting what you will do, then it seems to kind of undermine the notion of agency.", "tokens": [50964, 583, 264, 10710, 295, 7934, 2544, 11, 498, 437, 291, 434, 884, 307, 445, 32884, 437, 291, 486, 360, 11, 550, 309, 2544, 281, 733, 295, 39257, 264, 10710, 295, 7934, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06848363022306073, "compression_ratio": 1.6358381502890174, "no_speech_prob": 0.004724507685750723}, {"id": 311, "seek": 290388, "start": 2903.88, "end": 2907.88, "text": " So the answer to that question is very simple. It's tribly simple.", "tokens": [50364, 407, 264, 1867, 281, 300, 1168, 307, 588, 2199, 13, 467, 311, 1376, 25021, 2199, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14036387539981457, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.005412306170910597}, {"id": 312, "seek": 290388, "start": 2907.88, "end": 2914.88, "text": " You put agency into these schemes through prior preferences that define the sort of agent that I am.", "tokens": [50564, 509, 829, 7934, 666, 613, 26954, 807, 4059, 21910, 300, 6964, 264, 1333, 295, 9461, 300, 286, 669, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14036387539981457, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.005412306170910597}, {"id": 313, "seek": 290388, "start": 2914.88, "end": 2921.88, "text": " So we were talking before about reducing surprise of all sorts, whether it's epistemic uncertainty.", "tokens": [50914, 407, 321, 645, 1417, 949, 466, 12245, 6365, 295, 439, 7527, 11, 1968, 309, 311, 2388, 468, 3438, 15697, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14036387539981457, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.005412306170910597}, {"id": 314, "seek": 290388, "start": 2921.88, "end": 2929.88, "text": " But the simplest sort of pragmatic surprise, if I have a cost function that I don't want to be very hungry,", "tokens": [51264, 583, 264, 22811, 1333, 295, 46904, 6365, 11, 498, 286, 362, 257, 2063, 2445, 300, 286, 500, 380, 528, 281, 312, 588, 8067, 11, 51664], "temperature": 0.0, "avg_logprob": -0.14036387539981457, "compression_ratio": 1.6025641025641026, "no_speech_prob": 0.005412306170910597}, {"id": 315, "seek": 292988, "start": 2929.88, "end": 2933.88, "text": " or I don't want to end up in an arm that has no rewards in it,", "tokens": [50364, 420, 286, 500, 380, 528, 281, 917, 493, 294, 364, 3726, 300, 575, 572, 17203, 294, 309, 11, 50564], "temperature": 0.0, "avg_logprob": -0.09260547679403554, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0041487133130431175}, {"id": 316, "seek": 292988, "start": 2933.88, "end": 2941.88, "text": " then I'd simply have to have the pride belief that at the end of the day I will end up rewarded or sated or happy or complete.", "tokens": [50564, 550, 286, 1116, 2935, 362, 281, 362, 264, 10936, 7107, 300, 412, 264, 917, 295, 264, 786, 286, 486, 917, 493, 29105, 420, 262, 770, 420, 2055, 420, 3566, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09260547679403554, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0041487133130431175}, {"id": 317, "seek": 292988, "start": 2941.88, "end": 2944.88, "text": " So that anything else that happens is surprising.", "tokens": [50964, 407, 300, 1340, 1646, 300, 2314, 307, 8830, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09260547679403554, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0041487133130431175}, {"id": 318, "seek": 292988, "start": 2944.88, "end": 2952.88, "text": " And therefore I can then bring the whole machinery of predictive coding to bear upon the problem of suppressing prediction errors and surprises.", "tokens": [51114, 400, 4412, 286, 393, 550, 1565, 264, 1379, 27302, 295, 35521, 17720, 281, 6155, 3564, 264, 1154, 295, 1003, 18605, 17630, 13603, 293, 22655, 13, 51514], "temperature": 0.0, "avg_logprob": -0.09260547679403554, "compression_ratio": 1.6842105263157894, "no_speech_prob": 0.0041487133130431175}, {"id": 319, "seek": 295288, "start": 2952.88, "end": 2955.88, "text": " So I'm putting it very simply in terms of predictive coding.", "tokens": [50364, 407, 286, 478, 3372, 309, 588, 2935, 294, 2115, 295, 35521, 17720, 13, 50514], "temperature": 0.0, "avg_logprob": -0.10195528292188458, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.255362331867218}, {"id": 320, "seek": 295288, "start": 2955.88, "end": 2964.88, "text": " If I, a priori, believe that I'm always going to be happy and complete and that I am built to always minimise my prediction errors in the future,", "tokens": [50514, 759, 286, 11, 257, 4059, 72, 11, 1697, 300, 286, 478, 1009, 516, 281, 312, 2055, 293, 3566, 293, 300, 286, 669, 3094, 281, 1009, 4464, 908, 452, 17630, 13603, 294, 264, 2027, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10195528292188458, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.255362331867218}, {"id": 321, "seek": 295288, "start": 2964.88, "end": 2969.88, "text": " then I will look as if I have agency, I will look as if I have purpose,", "tokens": [50964, 550, 286, 486, 574, 382, 498, 286, 362, 7934, 11, 286, 486, 574, 382, 498, 286, 362, 4334, 11, 51214], "temperature": 0.0, "avg_logprob": -0.10195528292188458, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.255362331867218}, {"id": 322, "seek": 295288, "start": 2969.88, "end": 2977.88, "text": " because I will always choose my actions in a way that avoids the prediction errors that suggest that I am not happy and complete.", "tokens": [51214, 570, 286, 486, 1009, 2826, 452, 5909, 294, 257, 636, 300, 3641, 3742, 264, 17630, 13603, 300, 3402, 300, 286, 669, 406, 2055, 293, 3566, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10195528292188458, "compression_ratio": 1.8630136986301369, "no_speech_prob": 0.255362331867218}, {"id": 323, "seek": 297788, "start": 2977.88, "end": 2990.88, "text": " So the answer is just to absorb cost functions into inference by making costly states surprising through prior preferences.", "tokens": [50364, 407, 264, 1867, 307, 445, 281, 15631, 2063, 6828, 666, 38253, 538, 1455, 28328, 4368, 8830, 807, 4059, 21910, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1320286266139296, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.0012081877794116735}, {"id": 324, "seek": 297788, "start": 2990.88, "end": 2993.88, "text": " And that comes out of things like planning as inference.", "tokens": [51014, 400, 300, 1487, 484, 295, 721, 411, 5038, 382, 38253, 13, 51164], "temperature": 0.0, "avg_logprob": -0.1320286266139296, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.0012081877794116735}, {"id": 325, "seek": 297788, "start": 2997.88, "end": 3003.88, "text": " There are lots of ways of articulating that from the point of view of the rhetoric that I was using.", "tokens": [51364, 821, 366, 3195, 295, 2098, 295, 15228, 12162, 300, 490, 264, 935, 295, 1910, 295, 264, 29604, 300, 286, 390, 1228, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1320286266139296, "compression_ratio": 1.5524861878453038, "no_speech_prob": 0.0012081877794116735}, {"id": 326, "seek": 300388, "start": 3003.88, "end": 3005.88, "text": " The expected free energy has two bits to it.", "tokens": [50364, 440, 5176, 1737, 2281, 575, 732, 9239, 281, 309, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11651998990541929, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.006288938689976931}, {"id": 327, "seek": 300388, "start": 3005.88, "end": 3010.88, "text": " It has this epistemic bit and this pragmatic bit, but very simply it's uncertainty and surprise.", "tokens": [50464, 467, 575, 341, 2388, 468, 3438, 857, 293, 341, 46904, 857, 11, 457, 588, 2935, 309, 311, 15697, 293, 6365, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11651998990541929, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.006288938689976931}, {"id": 328, "seek": 300388, "start": 3010.88, "end": 3013.88, "text": " The epistemic bit is minimising uncertainty.", "tokens": [50714, 440, 2388, 468, 3438, 857, 307, 4464, 3436, 15697, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11651998990541929, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.006288938689976931}, {"id": 329, "seek": 300388, "start": 3013.88, "end": 3024.88, "text": " The value, the purpose, the goal is a pragmatic bit defined through cost functions that are literally the surprise of a costly outcome.", "tokens": [50864, 440, 2158, 11, 264, 4334, 11, 264, 3387, 307, 257, 46904, 857, 7642, 807, 2063, 6828, 300, 366, 3736, 264, 6365, 295, 257, 28328, 9700, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11651998990541929, "compression_ratio": 1.6683937823834196, "no_speech_prob": 0.006288938689976931}, {"id": 330, "seek": 302488, "start": 3025.88, "end": 3033.88, "text": " So, just to follow up a little bit, so is this sort of like a hyper-prior that's going to be,", "tokens": [50414, 407, 11, 445, 281, 1524, 493, 257, 707, 857, 11, 370, 307, 341, 1333, 295, 411, 257, 9848, 12, 36391, 284, 300, 311, 516, 281, 312, 11, 50814], "temperature": 0.0, "avg_logprob": -0.14850746080713365, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.10716138035058975}, {"id": 331, "seek": 302488, "start": 3033.88, "end": 3039.88, "text": " I mean it sounds like we all have to have this ultimate belief that it's all going to end well at the end of the day.", "tokens": [50814, 286, 914, 309, 3263, 411, 321, 439, 362, 281, 362, 341, 9705, 7107, 300, 309, 311, 439, 516, 281, 917, 731, 412, 264, 917, 295, 264, 786, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14850746080713365, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.10716138035058975}, {"id": 332, "seek": 302488, "start": 3039.88, "end": 3043.88, "text": " So pessimists, none of us are really pessimists or something like that, right?", "tokens": [51114, 407, 37399, 1751, 11, 6022, 295, 505, 366, 534, 37399, 1751, 420, 746, 411, 300, 11, 558, 30, 51314], "temperature": 0.0, "avg_logprob": -0.14850746080713365, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.10716138035058975}, {"id": 333, "seek": 302488, "start": 3043.88, "end": 3050.88, "text": " By definition. You may be perverse in your optimism, but you are quintessentially optimistic.", "tokens": [51314, 3146, 7123, 13, 509, 815, 312, 680, 4308, 294, 428, 31074, 11, 457, 291, 366, 40006, 442, 3137, 19397, 13, 51664], "temperature": 0.0, "avg_logprob": -0.14850746080713365, "compression_ratio": 1.620253164556962, "no_speech_prob": 0.10716138035058975}, {"id": 334, "seek": 305088, "start": 3051.88, "end": 3057.88, "text": " You're getting to some, you know, the deeper backstories behind the free energy principle.", "tokens": [50414, 509, 434, 1242, 281, 512, 11, 291, 458, 11, 264, 7731, 646, 372, 2083, 2261, 264, 1737, 2281, 8665, 13, 50714], "temperature": 0.0, "avg_logprob": -0.17089490890502929, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.0032426975667476654}, {"id": 335, "seek": 305088, "start": 3057.88, "end": 3066.88, "text": " The only, if you like, assumption that this instance of Hamilton's principle of release action makes is that you exist.", "tokens": [50714, 440, 787, 11, 498, 291, 411, 11, 15302, 300, 341, 5197, 295, 18484, 311, 8665, 295, 4374, 3069, 1669, 307, 300, 291, 2514, 13, 51164], "temperature": 0.0, "avg_logprob": -0.17089490890502929, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.0032426975667476654}, {"id": 336, "seek": 305088, "start": 3066.88, "end": 3073.88, "text": " And if you exist, that means you behave as if you have beliefs that you exist.", "tokens": [51164, 400, 498, 291, 2514, 11, 300, 1355, 291, 15158, 382, 498, 291, 362, 13585, 300, 291, 2514, 13, 51514], "temperature": 0.0, "avg_logprob": -0.17089490890502929, "compression_ratio": 1.5792349726775956, "no_speech_prob": 0.0032426975667476654}, {"id": 337, "seek": 307388, "start": 3073.88, "end": 3077.88, "text": " And by existing, that just means that you're not decaying or dying.", "tokens": [50364, 400, 538, 6741, 11, 300, 445, 1355, 300, 291, 434, 406, 21039, 278, 420, 8639, 13, 50564], "temperature": 0.0, "avg_logprob": -0.1846673437889586, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.005381311755627394}, {"id": 338, "seek": 307388, "start": 3077.88, "end": 3087.88, "text": " All your states are within some bounds, be they physiological or homeostatic or pecunary in terms of being rich or in terms of interceptive inference.", "tokens": [50564, 1057, 428, 4368, 366, 1951, 512, 29905, 11, 312, 436, 41234, 420, 1280, 555, 2399, 420, 42451, 409, 822, 294, 2115, 295, 885, 4593, 420, 294, 2115, 295, 24700, 488, 38253, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1846673437889586, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.005381311755627394}, {"id": 339, "seek": 307388, "start": 3087.88, "end": 3091.88, "text": " You're, you know, the hedonics on that happy.", "tokens": [51064, 509, 434, 11, 291, 458, 11, 264, 33653, 266, 1167, 322, 300, 2055, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1846673437889586, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.005381311755627394}, {"id": 340, "seek": 307388, "start": 3091.88, "end": 3099.88, "text": " But it's all about keeping things in bounds. It's all about minimising entropy, minimising uncertainty.", "tokens": [51264, 583, 309, 311, 439, 466, 5145, 721, 294, 29905, 13, 467, 311, 439, 466, 4464, 3436, 30867, 11, 4464, 3436, 15697, 13, 51664], "temperature": 0.0, "avg_logprob": -0.1846673437889586, "compression_ratio": 1.6428571428571428, "no_speech_prob": 0.005381311755627394}, {"id": 341, "seek": 309988, "start": 3099.88, "end": 3107.88, "text": " So it always looks as if agents that exist have prior beliefs that they exist.", "tokens": [50364, 407, 309, 1009, 1542, 382, 498, 12554, 300, 2514, 362, 4059, 13585, 300, 436, 2514, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1210268589488247, "compression_ratio": 1.5443037974683544, "no_speech_prob": 9.712530300021172e-05}, {"id": 342, "seek": 309988, "start": 3107.88, "end": 3117.88, "text": " And when you unpack that, that simply means I have preferred states that I will expect myself to occupy.", "tokens": [50764, 400, 562, 291, 26699, 300, 11, 300, 2935, 1355, 286, 362, 16494, 4368, 300, 286, 486, 2066, 2059, 281, 30645, 13, 51264], "temperature": 0.0, "avg_logprob": -0.1210268589488247, "compression_ratio": 1.5443037974683544, "no_speech_prob": 9.712530300021172e-05}, {"id": 343, "seek": 309988, "start": 3117.88, "end": 3121.88, "text": " Literally they are attracting states, they are an attractor.", "tokens": [51264, 23768, 436, 366, 36594, 4368, 11, 436, 366, 364, 5049, 284, 13, 51464], "temperature": 0.0, "avg_logprob": -0.1210268589488247, "compression_ratio": 1.5443037974683544, "no_speech_prob": 9.712530300021172e-05}, {"id": 344, "seek": 312188, "start": 3121.88, "end": 3130.88, "text": " So that rhetoric, which actually is a rhetoric from dynamical systems theory, applies identically to this sort of purposeful reinforcement learning,", "tokens": [50364, 407, 300, 29604, 11, 597, 767, 307, 257, 29604, 490, 5999, 804, 3652, 5261, 11, 13165, 2473, 984, 281, 341, 1333, 295, 4334, 906, 29280, 2539, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1372035538277975, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.037054989486932755}, {"id": 345, "seek": 312188, "start": 3130.88, "end": 3135.88, "text": " or sort of goal directed style of thinking about things.", "tokens": [50814, 420, 1333, 295, 3387, 12898, 3758, 295, 1953, 466, 721, 13, 51064], "temperature": 0.0, "avg_logprob": -0.1372035538277975, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.037054989486932755}, {"id": 346, "seek": 312188, "start": 3135.88, "end": 3141.88, "text": " There are attracting states, they are simply the ones that you frequent,", "tokens": [51064, 821, 366, 36594, 4368, 11, 436, 366, 2935, 264, 2306, 300, 291, 18004, 11, 51364], "temperature": 0.0, "avg_logprob": -0.1372035538277975, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.037054989486932755}, {"id": 347, "seek": 312188, "start": 3141.88, "end": 3148.88, "text": " which means that you will appear to behave as if you have prior preferences for being in those states.", "tokens": [51364, 597, 1355, 300, 291, 486, 4204, 281, 15158, 382, 498, 291, 362, 4059, 21910, 337, 885, 294, 729, 4368, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1372035538277975, "compression_ratio": 1.6637554585152838, "no_speech_prob": 0.037054989486932755}, {"id": 348, "seek": 314888, "start": 3148.88, "end": 3151.88, "text": " And you will always choose actions to get to those states.", "tokens": [50364, 400, 291, 486, 1009, 2826, 5909, 281, 483, 281, 729, 4368, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09325599670410156, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.003596120746806264}, {"id": 349, "seek": 314888, "start": 3151.88, "end": 3155.88, "text": " It is those prior preferences that define the sort of agent you are.", "tokens": [50514, 467, 307, 729, 4059, 21910, 300, 6964, 264, 1333, 295, 9461, 291, 366, 13, 50714], "temperature": 0.0, "avg_logprob": -0.09325599670410156, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.003596120746806264}, {"id": 350, "seek": 314888, "start": 3155.88, "end": 3162.88, "text": " So in answer to your very first question, the agentfulness comes in by implication,", "tokens": [50714, 407, 294, 1867, 281, 428, 588, 700, 1168, 11, 264, 9461, 26872, 1487, 294, 538, 37814, 11, 51064], "temperature": 0.0, "avg_logprob": -0.09325599670410156, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.003596120746806264}, {"id": 351, "seek": 314888, "start": 3162.88, "end": 3168.88, "text": " or just through the sorts of priors that characterise that particular sort of agent.", "tokens": [51064, 420, 445, 807, 264, 7527, 295, 1790, 830, 300, 2517, 908, 300, 1729, 1333, 295, 9461, 13, 51364], "temperature": 0.0, "avg_logprob": -0.09325599670410156, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.003596120746806264}, {"id": 352, "seek": 314888, "start": 3168.88, "end": 3173.88, "text": " So if I was a virus, I would have very different preferences than if I was a person.", "tokens": [51364, 407, 498, 286, 390, 257, 5752, 11, 286, 576, 362, 588, 819, 21910, 813, 498, 286, 390, 257, 954, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09325599670410156, "compression_ratio": 1.731818181818182, "no_speech_prob": 0.003596120746806264}, {"id": 353, "seek": 317388, "start": 3174.88, "end": 3180.88, "text": " But there are still both plausible and viable preferences in the sorts of agents.", "tokens": [50414, 583, 456, 366, 920, 1293, 39925, 293, 22024, 21910, 294, 264, 7527, 295, 12554, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10459218555026584, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.008418037556111813}, {"id": 354, "seek": 317388, "start": 3180.88, "end": 3182.88, "text": " Hi, thanks for your talk, Carl.", "tokens": [50714, 2421, 11, 3231, 337, 428, 751, 11, 14256, 13, 50814], "temperature": 0.0, "avg_logprob": -0.10459218555026584, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.008418037556111813}, {"id": 355, "seek": 317388, "start": 3182.88, "end": 3189.88, "text": " I was wondering now that the slides are back if we could go to the delay period activity that you had briefly mentioned.", "tokens": [50814, 286, 390, 6359, 586, 300, 264, 9788, 366, 646, 498, 321, 727, 352, 281, 264, 8577, 2896, 5191, 300, 291, 632, 10515, 2835, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10459218555026584, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.008418037556111813}, {"id": 356, "seek": 317388, "start": 3189.88, "end": 3195.88, "text": " And I just wanted to see that a little bit unpacked and related to what you were just talking about,", "tokens": [51164, 400, 286, 445, 1415, 281, 536, 300, 257, 707, 857, 26699, 292, 293, 4077, 281, 437, 291, 645, 445, 1417, 466, 11, 51464], "temperature": 0.0, "avg_logprob": -0.10459218555026584, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.008418037556111813}, {"id": 357, "seek": 317388, "start": 3195.88, "end": 3198.88, "text": " that is agents reducing their free energy.", "tokens": [51464, 300, 307, 12554, 12245, 641, 1737, 2281, 13, 51614], "temperature": 0.0, "avg_logprob": -0.10459218555026584, "compression_ratio": 1.6085106382978724, "no_speech_prob": 0.008418037556111813}, {"id": 358, "seek": 319888, "start": 3198.88, "end": 3207.88, "text": " Is that principle then generate the delay period activity that we see in places like prefrontal cortex and in working memory and so forth?", "tokens": [50364, 1119, 300, 8665, 550, 8460, 264, 8577, 2896, 5191, 300, 321, 536, 294, 3190, 411, 659, 11496, 304, 33312, 293, 294, 1364, 4675, 293, 370, 5220, 30, 50814], "temperature": 0.0, "avg_logprob": -0.16693973541259766, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.007155193481594324}, {"id": 359, "seek": 319888, "start": 3207.88, "end": 3208.88, "text": " Thanks.", "tokens": [50814, 2561, 13, 50864], "temperature": 0.0, "avg_logprob": -0.16693973541259766, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.007155193481594324}, {"id": 360, "seek": 319888, "start": 3208.88, "end": 3217.88, "text": " Right, well, those sorts of phenomena which we all know and have and will try to explain and measure empirically.", "tokens": [50864, 1779, 11, 731, 11, 729, 7527, 295, 22004, 597, 321, 439, 458, 293, 362, 293, 486, 853, 281, 2903, 293, 3481, 25790, 984, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16693973541259766, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.007155193481594324}, {"id": 361, "seek": 319888, "start": 3217.88, "end": 3219.88, "text": " So let me just try and find it.", "tokens": [51314, 407, 718, 385, 445, 853, 293, 915, 309, 13, 51414], "temperature": 0.0, "avg_logprob": -0.16693973541259766, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.007155193481594324}, {"id": 362, "seek": 319888, "start": 3219.88, "end": 3221.88, "text": " Do you remember where it was?", "tokens": [51414, 1144, 291, 1604, 689, 309, 390, 30, 51514], "temperature": 0.0, "avg_logprob": -0.16693973541259766, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.007155193481594324}, {"id": 363, "seek": 319888, "start": 3221.88, "end": 3223.88, "text": " Right, thank you.", "tokens": [51514, 1779, 11, 1309, 291, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16693973541259766, "compression_ratio": 1.5246636771300448, "no_speech_prob": 0.007155193481594324}, {"id": 364, "seek": 322388, "start": 3224.88, "end": 3226.88, "text": " There you go.", "tokens": [50414, 821, 291, 352, 13, 50514], "temperature": 0.0, "avg_logprob": -0.19022775404524095, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.0007459308835677803}, {"id": 365, "seek": 322388, "start": 3226.88, "end": 3235.88, "text": " All those sorts of nuts and bolts getting down and dirty in terms of what this scheme would do when you put dynamics on it through the gradient descent,", "tokens": [50514, 1057, 729, 7527, 295, 10483, 293, 18127, 1242, 760, 293, 9360, 294, 2115, 295, 437, 341, 12232, 576, 360, 562, 291, 829, 15679, 322, 309, 807, 264, 16235, 23475, 11, 50964], "temperature": 0.0, "avg_logprob": -0.19022775404524095, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.0007459308835677803}, {"id": 366, "seek": 322388, "start": 3235.88, "end": 3237.88, "text": " depend upon the gerontid model.", "tokens": [50964, 5672, 3564, 264, 5713, 896, 327, 2316, 13, 51064], "temperature": 0.0, "avg_logprob": -0.19022775404524095, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.0007459308835677803}, {"id": 367, "seek": 322388, "start": 3237.88, "end": 3245.88, "text": " So actually very similar to the last, one key component of the gerontid model are the prior beliefs, the preferences, what gives it purpose, what are its goals.", "tokens": [51064, 407, 767, 588, 2531, 281, 264, 1036, 11, 472, 2141, 6542, 295, 264, 5713, 896, 327, 2316, 366, 264, 4059, 13585, 11, 264, 21910, 11, 437, 2709, 309, 4334, 11, 437, 366, 1080, 5493, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19022775404524095, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.0007459308835677803}, {"id": 368, "seek": 322388, "start": 3245.88, "end": 3248.88, "text": " Your question, I think, has a very similar answer.", "tokens": [51464, 2260, 1168, 11, 286, 519, 11, 575, 257, 588, 2531, 1867, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19022775404524095, "compression_ratio": 1.653225806451613, "no_speech_prob": 0.0007459308835677803}, {"id": 369, "seek": 324888, "start": 3248.88, "end": 3254.88, "text": " Once you've written down the gerontid model, everything else is not up for discussion.", "tokens": [50364, 3443, 291, 600, 3720, 760, 264, 5713, 896, 327, 2316, 11, 1203, 1646, 307, 406, 493, 337, 5017, 13, 50664], "temperature": 0.0, "avg_logprob": -0.06802738023840862, "compression_ratio": 1.829090909090909, "no_speech_prob": 0.06498558074235916}, {"id": 370, "seek": 324888, "start": 3254.88, "end": 3258.88, "text": " The maths tells you exactly what has to happen once you've written down the gerontid model.", "tokens": [50664, 440, 36287, 5112, 291, 2293, 437, 575, 281, 1051, 1564, 291, 600, 3720, 760, 264, 5713, 896, 327, 2316, 13, 50864], "temperature": 0.0, "avg_logprob": -0.06802738023840862, "compression_ratio": 1.829090909090909, "no_speech_prob": 0.06498558074235916}, {"id": 371, "seek": 324888, "start": 3258.88, "end": 3264.88, "text": " And the delay period activity you're talking about simply follows from the fact you've got a deep gerontid model or a deep temporal model.", "tokens": [50864, 400, 264, 8577, 2896, 5191, 291, 434, 1417, 466, 2935, 10002, 490, 264, 1186, 291, 600, 658, 257, 2452, 5713, 896, 327, 2316, 420, 257, 2452, 30881, 2316, 13, 51164], "temperature": 0.0, "avg_logprob": -0.06802738023840862, "compression_ratio": 1.829090909090909, "no_speech_prob": 0.06498558074235916}, {"id": 372, "seek": 324888, "start": 3264.88, "end": 3277.88, "text": " So as soon as you write that deep structure into the model, it means that certain beliefs have to outlive or change on a slower temporal scale than other beliefs lower in the hierarchy.", "tokens": [51164, 407, 382, 2321, 382, 291, 2464, 300, 2452, 3877, 666, 264, 2316, 11, 309, 1355, 300, 1629, 13585, 362, 281, 484, 45273, 420, 1319, 322, 257, 14009, 30881, 4373, 813, 661, 13585, 3126, 294, 264, 22333, 13, 51814], "temperature": 0.0, "avg_logprob": -0.06802738023840862, "compression_ratio": 1.829090909090909, "no_speech_prob": 0.06498558074235916}, {"id": 373, "seek": 327788, "start": 3277.88, "end": 3285.88, "text": " Which means that you have to have delay period activity whilst other stuff unfolds at the lower levels of the hierarchy.", "tokens": [50364, 3013, 1355, 300, 291, 362, 281, 362, 8577, 2896, 5191, 18534, 661, 1507, 17980, 82, 412, 264, 3126, 4358, 295, 264, 22333, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12275685762104235, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.0010661999695003033}, {"id": 374, "seek": 327788, "start": 3285.88, "end": 3295.88, "text": " So in this particular example, what I've done here is show the beliefs about the six sentences over the four moves,", "tokens": [50764, 407, 294, 341, 1729, 1365, 11, 437, 286, 600, 1096, 510, 307, 855, 264, 13585, 466, 264, 2309, 16579, 670, 264, 1451, 6067, 11, 51264], "temperature": 0.0, "avg_logprob": -0.12275685762104235, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.0010661999695003033}, {"id": 375, "seek": 327788, "start": 3295.88, "end": 3299.88, "text": " giving us six times four moves or five moves.", "tokens": [51264, 2902, 505, 2309, 1413, 1451, 6067, 420, 1732, 6067, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12275685762104235, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.0010661999695003033}, {"id": 376, "seek": 327788, "start": 3299.88, "end": 3303.88, "text": " So it should be about 30 beliefs here.", "tokens": [51464, 407, 309, 820, 312, 466, 2217, 13585, 510, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12275685762104235, "compression_ratio": 1.5658536585365854, "no_speech_prob": 0.0010661999695003033}, {"id": 377, "seek": 330388, "start": 3303.88, "end": 3311.88, "text": " On the same time access as beliefs about the particular word that's currently being seen.", "tokens": [50364, 1282, 264, 912, 565, 2105, 382, 13585, 466, 264, 1729, 1349, 300, 311, 4362, 885, 1612, 13, 50764], "temperature": 0.0, "avg_logprob": -0.19719549325796273, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0007524812826886773}, {"id": 378, "seek": 330388, "start": 3311.88, "end": 3318.88, "text": " These resets here indicate the onset of saccades and the acquisition of new information.", "tokens": [50764, 1981, 725, 1385, 510, 13330, 264, 34948, 295, 4899, 66, 2977, 293, 264, 21668, 295, 777, 1589, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19719549325796273, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0007524812826886773}, {"id": 379, "seek": 330388, "start": 3318.88, "end": 3325.88, "text": " You can see roughly every 250 milliseconds there's a saccade and new beliefs are updated about the current word.", "tokens": [51114, 509, 393, 536, 9810, 633, 11650, 34184, 456, 311, 257, 4899, 30340, 293, 777, 13585, 366, 10588, 466, 264, 2190, 1349, 13, 51464], "temperature": 0.0, "avg_logprob": -0.19719549325796273, "compression_ratio": 1.5396825396825398, "no_speech_prob": 0.0007524812826886773}, {"id": 380, "seek": 332588, "start": 3325.88, "end": 3334.88, "text": " But at the high level, we're only considering beliefs about each letter in my part.", "tokens": [50364, 583, 412, 264, 1090, 1496, 11, 321, 434, 787, 8079, 13585, 466, 1184, 5063, 294, 452, 644, 13, 50814], "temperature": 0.0, "avg_logprob": -0.16700537308402683, "compression_ratio": 1.7426900584795322, "no_speech_prob": 0.028265850618481636}, {"id": 381, "seek": 332588, "start": 3334.88, "end": 3337.88, "text": " We're only considering beliefs about the word.", "tokens": [50814, 492, 434, 787, 8079, 13585, 466, 264, 1349, 13, 50964], "temperature": 0.0, "avg_logprob": -0.16700537308402683, "compression_ratio": 1.7426900584795322, "no_speech_prob": 0.028265850618481636}, {"id": 382, "seek": 332588, "start": 3337.88, "end": 3350.88, "text": " So beliefs about the word corresponding to what's on this page or what's in this word are invariant during the successive saccades as you sample the different letters.", "tokens": [50964, 407, 13585, 466, 264, 1349, 11760, 281, 437, 311, 322, 341, 3028, 420, 437, 311, 294, 341, 1349, 366, 33270, 394, 1830, 264, 48043, 4899, 66, 2977, 382, 291, 6889, 264, 819, 7825, 13, 51614], "temperature": 0.0, "avg_logprob": -0.16700537308402683, "compression_ratio": 1.7426900584795322, "no_speech_prob": 0.028265850618481636}, {"id": 383, "seek": 335088, "start": 3350.88, "end": 3354.88, "text": " So these things change more quickly than these things.", "tokens": [50364, 407, 613, 721, 1319, 544, 2661, 813, 613, 721, 13, 50564], "temperature": 0.0, "avg_logprob": -0.12240623994307084, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.007595003116875887}, {"id": 384, "seek": 335088, "start": 3354.88, "end": 3358.88, "text": " When these are completed, then there's a change here and then the cycle begins again.", "tokens": [50564, 1133, 613, 366, 7365, 11, 550, 456, 311, 257, 1319, 510, 293, 550, 264, 6586, 7338, 797, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12240623994307084, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.007595003116875887}, {"id": 385, "seek": 335088, "start": 3358.88, "end": 3368.88, "text": " So these tick over faster than this and then this looks a little bit now like the rastles that you see prior to the emission of the saccade here.", "tokens": [50764, 407, 613, 5204, 670, 4663, 813, 341, 293, 550, 341, 1542, 257, 707, 857, 586, 411, 264, 367, 525, 904, 300, 291, 536, 4059, 281, 264, 29513, 295, 264, 4899, 30340, 510, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12240623994307084, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.007595003116875887}, {"id": 386, "seek": 335088, "start": 3368.88, "end": 3372.88, "text": " They're not from the same paper but a related paradigm.", "tokens": [51264, 814, 434, 406, 490, 264, 912, 3035, 457, 257, 4077, 24709, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12240623994307084, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.007595003116875887}, {"id": 387, "seek": 335088, "start": 3372.88, "end": 3379.88, "text": " If I take the voltage causing this delay period activity and band pass filter it, you get these sorts of fluctuations out here.", "tokens": [51464, 759, 286, 747, 264, 8352, 9853, 341, 8577, 2896, 5191, 293, 4116, 1320, 6608, 309, 11, 291, 483, 613, 7527, 295, 45276, 484, 510, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12240623994307084, "compression_ratio": 1.7602996254681649, "no_speech_prob": 0.007595003116875887}, {"id": 388, "seek": 337988, "start": 3379.88, "end": 3386.88, "text": " So when there's an increase in delay period activity, there's usually a positive deflection that looks a little bit like an ERP.", "tokens": [50364, 407, 562, 456, 311, 364, 3488, 294, 8577, 2896, 5191, 11, 456, 311, 2673, 257, 3353, 1060, 5450, 300, 1542, 257, 707, 857, 411, 364, 14929, 47, 13, 50714], "temperature": 0.0, "avg_logprob": -0.1315647948021982, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0006489760708063841}, {"id": 389, "seek": 337988, "start": 3386.88, "end": 3394.88, "text": " And just to follow up, so is the presence of delay period activity, is that associated then with prediction error or with the build up of a prediction?", "tokens": [50714, 400, 445, 281, 1524, 493, 11, 370, 307, 264, 6814, 295, 8577, 2896, 5191, 11, 307, 300, 6615, 550, 365, 17630, 6713, 420, 365, 264, 1322, 493, 295, 257, 17630, 30, 51114], "temperature": 0.0, "avg_logprob": -0.1315647948021982, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0006489760708063841}, {"id": 390, "seek": 337988, "start": 3394.88, "end": 3406.88, "text": " No, I think the prediction error in this scheme and this is the maths that comes from the discrete aspect of the genetic models lies in the rate of change of neural firing.", "tokens": [51114, 883, 11, 286, 519, 264, 17630, 6713, 294, 341, 12232, 293, 341, 307, 264, 36287, 300, 1487, 490, 264, 27706, 4171, 295, 264, 12462, 5245, 9134, 294, 264, 3314, 295, 1319, 295, 18161, 16045, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1315647948021982, "compression_ratio": 1.735632183908046, "no_speech_prob": 0.0006489760708063841}, {"id": 391, "seek": 340688, "start": 3406.88, "end": 3423.88, "text": " If you associate the bi-physical encoding of expected states of the world in terms of population firing rates, then if you subscribe to that, if you accept that, then the prediction error now becomes the conductances that drive the depolarisation that drive the firing.", "tokens": [50364, 759, 291, 14644, 264, 3228, 12, 950, 36280, 43430, 295, 5176, 4368, 295, 264, 1002, 294, 2115, 295, 4415, 16045, 6846, 11, 550, 498, 291, 3022, 281, 300, 11, 498, 291, 3241, 300, 11, 550, 264, 17630, 6713, 586, 3643, 264, 6018, 2676, 300, 3332, 264, 1367, 15276, 7623, 300, 3332, 264, 16045, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1567808363172743, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.013287712819874287}, {"id": 392, "seek": 340688, "start": 3423.88, "end": 3433.88, "text": " So these basically reflect the fact that as time goes on you can more and more confident that one particular sentence is in play and you can see that.", "tokens": [51214, 407, 613, 1936, 5031, 264, 1186, 300, 382, 565, 1709, 322, 291, 393, 544, 293, 544, 6679, 300, 472, 1729, 8174, 307, 294, 862, 293, 291, 393, 536, 300, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1567808363172743, "compression_ratio": 1.7796610169491525, "no_speech_prob": 0.013287712819874287}, {"id": 393, "seek": 343388, "start": 3433.88, "end": 3453.88, "text": " This is beliefs about which sentence is in play at the first, second, third, fourth and fifth page or word and they are now internally consistent and at every point in time in the past, at the end, I now believe I was reading the first sentence.", "tokens": [50364, 639, 307, 13585, 466, 597, 8174, 307, 294, 862, 412, 264, 700, 11, 1150, 11, 2636, 11, 6409, 293, 9266, 3028, 420, 1349, 293, 436, 366, 586, 19501, 8398, 293, 412, 633, 935, 294, 565, 294, 264, 1791, 11, 412, 264, 917, 11, 286, 586, 1697, 286, 390, 3760, 264, 700, 8174, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14379538765436486, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.014105168171226978}, {"id": 394, "seek": 343388, "start": 3453.88, "end": 3461.88, "text": " And that belief endures during the sampling of all the actual letters within each of the words.", "tokens": [51364, 400, 300, 7107, 465, 769, 495, 1830, 264, 21179, 295, 439, 264, 3539, 7825, 1951, 1184, 295, 264, 2283, 13, 51764], "temperature": 0.0, "avg_logprob": -0.14379538765436486, "compression_ratio": 1.6553398058252426, "no_speech_prob": 0.014105168171226978}, {"id": 395, "seek": 346188, "start": 3461.88, "end": 3472.88, "text": " So these would now represent just basically numbers between nought and one, zero and 100% neural firing that score your expectation that this is the current state of the world.", "tokens": [50364, 407, 613, 576, 586, 2906, 445, 1936, 3547, 1296, 297, 930, 293, 472, 11, 4018, 293, 2319, 4, 18161, 16045, 300, 6175, 428, 14334, 300, 341, 307, 264, 2190, 1785, 295, 264, 1002, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14862014401343562, "compression_ratio": 1.5916030534351144, "no_speech_prob": 0.019781488925218582}, {"id": 396, "seek": 346188, "start": 3472.88, "end": 3488.88, "text": " At the beginning, there's lots of ambiguity, not an enormous amount, but there is ambiguity. It's 50-50 because of the six sentences, only two of them begin with the word flee, which means that we resolve our uncertainty about four of them,", "tokens": [50914, 1711, 264, 2863, 11, 456, 311, 3195, 295, 46519, 11, 406, 364, 11322, 2372, 11, 457, 456, 307, 46519, 13, 467, 311, 2625, 12, 2803, 570, 295, 264, 2309, 16579, 11, 787, 732, 295, 552, 1841, 365, 264, 1349, 25146, 11, 597, 1355, 300, 321, 14151, 527, 15697, 466, 1451, 295, 552, 11, 51714], "temperature": 0.0, "avg_logprob": -0.14862014401343562, "compression_ratio": 1.5916030534351144, "no_speech_prob": 0.019781488925218582}, {"id": 397, "seek": 348888, "start": 3488.88, "end": 3501.88, "text": " but we're still ambiguous about having ambiguity about sentences one and four and that can only be resolved at the end because these sentences only differ in the words right at the end.", "tokens": [50364, 457, 321, 434, 920, 39465, 466, 1419, 46519, 466, 16579, 472, 293, 1451, 293, 300, 393, 787, 312, 20772, 412, 264, 917, 570, 613, 16579, 787, 743, 294, 264, 2283, 558, 412, 264, 917, 13, 51014], "temperature": 0.0, "avg_logprob": -0.1053148905436198, "compression_ratio": 1.5163934426229508, "no_speech_prob": 0.01009537186473608}, {"id": 398, "seek": 350188, "start": 3501.88, "end": 3519.88, "text": " So during this time, there's delay period activity which we've got these two explanations, hypotheses in play that are resolved epistemically, optimally, right at the end when we get to the last word here and it's a wait and that determines which of the letters it was.", "tokens": [50364, 407, 1830, 341, 565, 11, 456, 311, 8577, 2896, 5191, 597, 321, 600, 658, 613, 732, 28708, 11, 49969, 294, 862, 300, 366, 20772, 2388, 43958, 984, 11, 5028, 379, 11, 558, 412, 264, 917, 562, 321, 483, 281, 264, 1036, 1349, 510, 293, 309, 311, 257, 1699, 293, 300, 24799, 597, 295, 264, 7825, 309, 390, 13, 51264], "temperature": 0.0, "avg_logprob": -0.16144451018302672, "compression_ratio": 1.5459770114942528, "no_speech_prob": 0.07765977084636688}, {"id": 399, "seek": 351988, "start": 3519.88, "end": 3533.88, "text": " Thank you for the talk. I wanted to go back to this idea of this contrast between reinforcement learning and the kind of formulation that you're making here.", "tokens": [50364, 1044, 291, 337, 264, 751, 13, 286, 1415, 281, 352, 646, 281, 341, 1558, 295, 341, 8712, 1296, 29280, 2539, 293, 264, 733, 295, 37642, 300, 291, 434, 1455, 510, 13, 51064], "temperature": 0.0, "avg_logprob": -0.123148070441352, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.40373706817626953}, {"id": 400, "seek": 351988, "start": 3533.88, "end": 3547.88, "text": " So one of the things that I thought was interesting is this formulation in terms of external value plus you basically decompose your KL divergence to external value and epistemic value.", "tokens": [51064, 407, 472, 295, 264, 721, 300, 286, 1194, 390, 1880, 307, 341, 37642, 294, 2115, 295, 8320, 2158, 1804, 291, 1936, 22867, 541, 428, 47991, 47387, 281, 8320, 2158, 293, 2388, 468, 3438, 2158, 13, 51764], "temperature": 0.0, "avg_logprob": -0.123148070441352, "compression_ratio": 1.6255924170616114, "no_speech_prob": 0.40373706817626953}, {"id": 401, "seek": 354788, "start": 3547.88, "end": 3564.88, "text": " So how do you get exploration in this model? So it seems to me that you're doing an ARMAX over actions to get some balance between immediate value and information gain.", "tokens": [50364, 407, 577, 360, 291, 483, 16197, 294, 341, 2316, 30, 407, 309, 2544, 281, 385, 300, 291, 434, 884, 364, 8943, 9998, 55, 670, 5909, 281, 483, 512, 4772, 1296, 11629, 2158, 293, 1589, 6052, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1527343491713206, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.03910614177584648}, {"id": 402, "seek": 354788, "start": 3564.88, "end": 3567.88, "text": " Is that the basic idea?", "tokens": [51214, 1119, 300, 264, 3875, 1558, 30, 51364], "temperature": 0.0, "avg_logprob": -0.1527343491713206, "compression_ratio": 1.3333333333333333, "no_speech_prob": 0.03910614177584648}, {"id": 403, "seek": 356788, "start": 3567.88, "end": 3581.88, "text": " Yes. I mean, we can look at the equation or we can look at this. That's absolutely right. So, well, the exploration is good that you brought that in because another perspective on this is the whole foraging ethological perspective on exploration versus exploitation.", "tokens": [50364, 1079, 13, 286, 914, 11, 321, 393, 574, 412, 264, 5367, 420, 321, 393, 574, 412, 341, 13, 663, 311, 3122, 558, 13, 407, 11, 731, 11, 264, 16197, 307, 665, 300, 291, 3038, 300, 294, 570, 1071, 4585, 322, 341, 307, 264, 1379, 337, 3568, 6468, 4383, 4585, 322, 16197, 5717, 33122, 13, 51064], "temperature": 0.0, "avg_logprob": -0.21497012828958445, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.18104897439479828}, {"id": 404, "seek": 358188, "start": 3581.88, "end": 3593.88, "text": " That rhetoric just maps very simply to the epistemic and the pragmatic. So, and there is no, again, there is no up for discussion or there's no ad hoc waiting between the two.", "tokens": [50364, 663, 29604, 445, 11317, 588, 2935, 281, 264, 2388, 468, 3438, 293, 264, 46904, 13, 407, 11, 293, 456, 307, 572, 11, 797, 11, 456, 307, 572, 493, 337, 5017, 420, 456, 311, 572, 614, 16708, 3806, 1296, 264, 732, 13, 50964], "temperature": 0.0, "avg_logprob": -0.09310767491658528, "compression_ratio": 1.7248677248677249, "no_speech_prob": 0.2896525263786316}, {"id": 405, "seek": 358188, "start": 3593.88, "end": 3602.88, "text": " The expected free energy can always be written down in terms of exploration plus exploitation in terms of the epistemic value and the pragmatic value.", "tokens": [50964, 440, 5176, 1737, 2281, 393, 1009, 312, 3720, 760, 294, 2115, 295, 16197, 1804, 33122, 294, 2115, 295, 264, 2388, 468, 3438, 2158, 293, 264, 46904, 2158, 13, 51414], "temperature": 0.0, "avg_logprob": -0.09310767491658528, "compression_ratio": 1.7248677248677249, "no_speech_prob": 0.2896525263786316}, {"id": 406, "seek": 360288, "start": 3602.88, "end": 3617.88, "text": " And what happens is in minimizing that one quantity, you get this scanning searching behaviour until the epistemic bit has been reduced, allowing then you to focus on the pragmatic bit.", "tokens": [50364, 400, 437, 2314, 307, 294, 46608, 300, 472, 11275, 11, 291, 483, 341, 27019, 10808, 17229, 1826, 264, 2388, 468, 3438, 857, 575, 668, 9212, 11, 8293, 550, 291, 281, 1879, 322, 264, 46904, 857, 13, 51114], "temperature": 0.0, "avg_logprob": -0.14894132260923032, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.46078750491142273}, {"id": 407, "seek": 360288, "start": 3617.88, "end": 3629.88, "text": " So for free, you get a base optimal exploration to the extent that it is sufficient to resolve uncertainty given the precision of your beliefs about your prior preferences that then allow you to pursue your goals.", "tokens": [51114, 407, 337, 1737, 11, 291, 483, 257, 3096, 16252, 16197, 281, 264, 8396, 300, 309, 307, 11563, 281, 14151, 15697, 2212, 264, 18356, 295, 428, 13585, 466, 428, 4059, 21910, 300, 550, 2089, 291, 281, 12392, 428, 5493, 13, 51714], "temperature": 0.0, "avg_logprob": -0.14894132260923032, "compression_ratio": 1.6906779661016949, "no_speech_prob": 0.46078750491142273}, {"id": 408, "seek": 362988, "start": 3629.88, "end": 3643.88, "text": " So this solves the exploration dilemma in a base optimal sense. It also suggests that the very carving of behaviour into these two complementary drives is actually probably a misdirection.", "tokens": [50364, 407, 341, 39890, 264, 16197, 34312, 294, 257, 3096, 16252, 2020, 13, 467, 611, 13409, 300, 264, 588, 31872, 295, 17229, 666, 613, 732, 40705, 11754, 307, 767, 1391, 257, 3346, 18267, 882, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10489281227714137, "compression_ratio": 1.3722627737226278, "no_speech_prob": 0.007722052745521069}, {"id": 409, "seek": 364388, "start": 3644.88, "end": 3661.88, "text": " So it's only you and me that have actually teased apart the two components of the expected free energy and called one an epistemic exploratory one or a novelty seeking one and the other bit a pragmatic cost function like rewarding preference goal directed like one.", "tokens": [50414, 407, 309, 311, 787, 291, 293, 385, 300, 362, 767, 535, 1937, 4936, 264, 732, 6677, 295, 264, 5176, 1737, 2281, 293, 1219, 472, 364, 2388, 468, 3438, 24765, 4745, 472, 420, 257, 44805, 11670, 472, 293, 264, 661, 857, 257, 46904, 2063, 2445, 411, 20063, 17502, 3387, 12898, 411, 472, 13, 51264], "temperature": 0.0, "avg_logprob": -0.12905137879507883, "compression_ratio": 1.5229885057471264, "no_speech_prob": 0.5197597742080688}, {"id": 410, "seek": 366188, "start": 3662.88, "end": 3669.88, "text": " There are lots of different ways of rearranging those. You can also rearrange them in terms of risk and ambiguity, intrinsic and extrinsic value.", "tokens": [50414, 821, 366, 3195, 295, 819, 2098, 295, 29875, 9741, 729, 13, 509, 393, 611, 39568, 552, 294, 2115, 295, 3148, 293, 46519, 11, 35698, 293, 16455, 1292, 299, 2158, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1123867623599959, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.4057186543941498}, {"id": 411, "seek": 366188, "start": 3669.88, "end": 3685.88, "text": " There are lots of ways of carving them and getting different perspectives. When you see that and when you work with that, you start to realize that it's not necessarily the best thing just to have one particular religious perspective on it", "tokens": [50764, 821, 366, 3195, 295, 2098, 295, 31872, 552, 293, 1242, 819, 16766, 13, 1133, 291, 536, 300, 293, 562, 291, 589, 365, 300, 11, 291, 722, 281, 4325, 300, 309, 311, 406, 4725, 264, 1151, 551, 445, 281, 362, 472, 1729, 7185, 4585, 322, 309, 51564], "temperature": 0.0, "avg_logprob": -0.1123867623599959, "compression_ratio": 1.7579908675799087, "no_speech_prob": 0.4057186543941498}, {"id": 412, "seek": 368588, "start": 3686.88, "end": 3700.88, "text": " because it lends you to the false belief if you subscribe to this formalism that there has to be some other adjudicator. There has to be some other harmonculus that's decided, oh, I need to explore now.", "tokens": [50414, 570, 309, 287, 2581, 291, 281, 264, 7908, 7107, 498, 291, 3022, 281, 341, 9860, 1434, 300, 456, 575, 281, 312, 512, 661, 614, 9218, 299, 1639, 13, 821, 575, 281, 312, 512, 661, 14750, 36002, 300, 311, 3047, 11, 1954, 11, 286, 643, 281, 6839, 586, 13, 51114], "temperature": 0.0, "avg_logprob": -0.21713431064899152, "compression_ratio": 1.5303030303030303, "no_speech_prob": 0.36462700366973877}, {"id": 413, "seek": 370088, "start": 3700.88, "end": 3710.88, "text": " I've done my exploration and now I'm going to go and do a bit of pragmat and stop the scanning and then I'm going to go and exploit what I've discovered. It doesn't work like that. You should get that for free.", "tokens": [50364, 286, 600, 1096, 452, 16197, 293, 586, 286, 478, 516, 281, 352, 293, 360, 257, 857, 295, 33394, 15677, 293, 1590, 264, 27019, 293, 550, 286, 478, 516, 281, 352, 293, 25924, 437, 286, 600, 6941, 13, 467, 1177, 380, 589, 411, 300, 13, 509, 820, 483, 300, 337, 1737, 13, 50864], "temperature": 0.0, "avg_logprob": -0.19723129272460938, "compression_ratio": 1.6694915254237288, "no_speech_prob": 0.2721005380153656}, {"id": 414, "seek": 370088, "start": 3710.88, "end": 3725.88, "text": " If they're both part of the same cost function, then once you've sufficiently reduced your uncertainty, then just naturally you go into as illustrated here your exploratory behaviour.", "tokens": [50864, 759, 436, 434, 1293, 644, 295, 264, 912, 2063, 2445, 11, 550, 1564, 291, 600, 31868, 9212, 428, 15697, 11, 550, 445, 8195, 291, 352, 666, 382, 33875, 510, 428, 24765, 4745, 17229, 13, 51614], "temperature": 0.0, "avg_logprob": -0.19723129272460938, "compression_ratio": 1.6694915254237288, "no_speech_prob": 0.2721005380153656}, {"id": 415, "seek": 372588, "start": 3725.88, "end": 3735.88, "text": " This is exploratory behaviour or novelty seeking in the sense that you don't know what the queue is going to tell you. It doesn't have any immediate rewarding aspect to it.", "tokens": [50364, 639, 307, 24765, 4745, 17229, 420, 44805, 11670, 294, 264, 2020, 300, 291, 500, 380, 458, 437, 264, 18639, 307, 516, 281, 980, 291, 13, 467, 1177, 380, 362, 604, 11629, 20063, 4171, 281, 309, 13, 50864], "temperature": 0.0, "avg_logprob": -0.1383529011207291, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.05361384153366089}, {"id": 416, "seek": 372588, "start": 3735.88, "end": 3746.88, "text": " There are no preferences associated with the condition stimulus, but it's interesting, uncertainty resolving, but after a time it becomes boring because you already know what it's going to tell you.", "tokens": [50864, 821, 366, 572, 21910, 6615, 365, 264, 4188, 21366, 11, 457, 309, 311, 1880, 11, 15697, 49940, 11, 457, 934, 257, 565, 309, 3643, 9989, 570, 291, 1217, 458, 437, 309, 311, 516, 281, 980, 291, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1383529011207291, "compression_ratio": 1.6271929824561404, "no_speech_prob": 0.05361384153366089}, {"id": 417, "seek": 374688, "start": 3746.88, "end": 3759.88, "text": " Once it does that, then you get exploratory behaviour. I should have done that. I should have put exploration and exploitation. Have an argument with me because it's meant to be a discussion. Do you not like that?", "tokens": [50364, 3443, 309, 775, 300, 11, 550, 291, 483, 24765, 4745, 17229, 13, 286, 820, 362, 1096, 300, 13, 286, 820, 362, 829, 16197, 293, 33122, 13, 3560, 364, 6770, 365, 385, 570, 309, 311, 4140, 281, 312, 257, 5017, 13, 1144, 291, 406, 411, 300, 30, 51014], "temperature": 0.0, "avg_logprob": -0.19294923782348633, "compression_ratio": 1.4689655172413794, "no_speech_prob": 0.10394611209630966}, {"id": 418, "seek": 375988, "start": 3760.88, "end": 3775.88, "text": " It's very interesting perspective on it. I'm surprised that it comes out that the simple deterministic policy works well and just kind of works out of the box.", "tokens": [50414, 467, 311, 588, 1880, 4585, 322, 309, 13, 286, 478, 6100, 300, 309, 1487, 484, 300, 264, 2199, 15957, 3142, 3897, 1985, 731, 293, 445, 733, 295, 1985, 484, 295, 264, 2424, 13, 51164], "temperature": 0.0, "avg_logprob": -0.13909686578286662, "compression_ratio": 1.325, "no_speech_prob": 0.6413034200668335}, {"id": 419, "seek": 377588, "start": 3775.88, "end": 3802.88, "text": " My inkling is that when you go and implement this stuff, it can be difficult for it to balance the exploration and exploitation. So there's no tuning parameters, there's no off policy estimation, you just throw it in there.", "tokens": [50364, 1222, 11276, 1688, 307, 300, 562, 291, 352, 293, 4445, 341, 1507, 11, 309, 393, 312, 2252, 337, 309, 281, 4772, 264, 16197, 293, 33122, 13, 407, 456, 311, 572, 15164, 9834, 11, 456, 311, 572, 766, 3897, 35701, 11, 291, 445, 3507, 309, 294, 456, 13, 51714], "temperature": 0.0, "avg_logprob": -0.277708184485342, "compression_ratio": 1.4768211920529801, "no_speech_prob": 0.2538068890571594}, {"id": 420, "seek": 380288, "start": 3802.88, "end": 3821.88, "text": " It all works out. I know exactly what you're saying because this was a big selling point when we first realised that a couple of years ago and it will remain a bit like one of these five to ten year changing the direction of the ocean liner of the oil tanker.", "tokens": [50364, 467, 439, 1985, 484, 13, 286, 458, 2293, 437, 291, 434, 1566, 570, 341, 390, 257, 955, 6511, 935, 562, 321, 700, 21337, 300, 257, 1916, 295, 924, 2057, 293, 309, 486, 6222, 257, 857, 411, 472, 295, 613, 1732, 281, 2064, 1064, 4473, 264, 3513, 295, 264, 7810, 24468, 295, 264, 3184, 5466, 260, 13, 51314], "temperature": 0.0, "avg_logprob": -0.16506117184956867, "compression_ratio": 1.5235294117647058, "no_speech_prob": 0.06441912055015564}, {"id": 421, "seek": 382188, "start": 3822.88, "end": 3833.88, "text": " Two years later, all that we're saying is in fact people behave according to Hampton's principle of least action. That's all that we're saying.", "tokens": [50414, 4453, 924, 1780, 11, 439, 300, 321, 434, 1566, 307, 294, 1186, 561, 15158, 4650, 281, 8234, 21987, 311, 8665, 295, 1935, 3069, 13, 663, 311, 439, 300, 321, 434, 1566, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2162919839223226, "compression_ratio": 1.3364485981308412, "no_speech_prob": 0.1515766680240631}, {"id": 422, "seek": 383388, "start": 3834.88, "end": 3851.88, "text": " That implicitly, or it looks like, that behaviour has this dual aspect. It doesn't, if you formulate it as a variational principle of the sort you did at school when doing new turning mechanics and then Einstein did with general relativity.", "tokens": [50414, 663, 26947, 356, 11, 420, 309, 1542, 411, 11, 300, 17229, 575, 341, 11848, 4171, 13, 467, 1177, 380, 11, 498, 291, 47881, 309, 382, 257, 3034, 1478, 8665, 295, 264, 1333, 291, 630, 412, 1395, 562, 884, 777, 6246, 12939, 293, 550, 23486, 630, 365, 2674, 45675, 13, 51264], "temperature": 0.0, "avg_logprob": -0.25379440019715505, "compression_ratio": 1.4035087719298245, "no_speech_prob": 0.27641138434410095}, {"id": 423, "seek": 385188, "start": 3852.88, "end": 3871.88, "text": " It sort of falls out of the mix in a way that does actually dismiss these separatist perspectives on, I can either do this or that and I've got to now optimise the exploration in relation to the exploitation.", "tokens": [50414, 467, 1333, 295, 8804, 484, 295, 264, 2890, 294, 257, 636, 300, 775, 767, 16974, 613, 3128, 267, 468, 16766, 322, 11, 286, 393, 2139, 360, 341, 420, 300, 293, 286, 600, 658, 281, 586, 5028, 908, 264, 16197, 294, 9721, 281, 264, 33122, 13, 51364], "temperature": 0.0, "avg_logprob": -0.11037342888968331, "compression_ratio": 1.4857142857142858, "no_speech_prob": 0.1373857706785202}, {"id": 424, "seek": 387188, "start": 3871.88, "end": 3886.88, "text": " The key trick that puts you into this simple world of Hampton's principle of least action is the realisation you can't prescribe good behaviour unless the prescription is an optimisation of a function of beliefs.", "tokens": [50364, 440, 2141, 4282, 300, 8137, 291, 666, 341, 2199, 1002, 295, 8234, 21987, 311, 8665, 295, 1935, 3069, 307, 264, 957, 7623, 291, 393, 380, 49292, 665, 17229, 5969, 264, 22456, 307, 364, 5028, 7623, 295, 257, 2445, 295, 13585, 13, 51114], "temperature": 0.0, "avg_logprob": -0.09009083641899956, "compression_ratio": 1.4929577464788732, "no_speech_prob": 0.04241234064102173}, {"id": 425, "seek": 388688, "start": 3886.88, "end": 3903.88, "text": " A really simple example would be an economics game. I've got a really high risk and a low risk option. But there may be a third option, which is if I don't know which is which, I should do nothing until I know more.", "tokens": [50364, 316, 534, 2199, 1365, 576, 312, 364, 14564, 1216, 13, 286, 600, 658, 257, 534, 1090, 3148, 293, 257, 2295, 3148, 3614, 13, 583, 456, 815, 312, 257, 2636, 3614, 11, 597, 307, 498, 286, 500, 380, 458, 597, 307, 597, 11, 286, 820, 360, 1825, 1826, 286, 458, 544, 13, 51214], "temperature": 0.0, "avg_logprob": -0.1776818708939986, "compression_ratio": 1.4625850340136055, "no_speech_prob": 0.07602758705615997}, {"id": 426, "seek": 390388, "start": 3904.88, "end": 3926.88, "text": " In using words like I don't know, I am now saying that my behaviour now becomes a function of my knowledge or my uncertainty, so I now induce different options, different behaviours, different policies and actions that rest upon my degree of belief, which means you can't do it with value function optimisation or utility function optimisation or reinforcement learning.", "tokens": [50414, 682, 1228, 2283, 411, 286, 500, 380, 458, 11, 286, 669, 586, 1566, 300, 452, 17229, 586, 3643, 257, 2445, 295, 452, 3601, 420, 452, 15697, 11, 370, 286, 586, 41263, 819, 3956, 11, 819, 15475, 5067, 11, 819, 7657, 293, 5909, 300, 1472, 3564, 452, 4314, 295, 7107, 11, 597, 1355, 291, 393, 380, 360, 309, 365, 2158, 2445, 5028, 7623, 420, 14877, 2445, 5028, 7623, 420, 29280, 2539, 13, 51514], "temperature": 0.0, "avg_logprob": -0.1123434575398763, "compression_ratio": 1.7535545023696681, "no_speech_prob": 0.20413601398468018}, {"id": 427, "seek": 392688, "start": 3926.88, "end": 3939.88, "text": " It can't be done with queue learning. It cannot be done with a Bellman optimisation scheme because what you've done is you've said that there are better behaviours when I don't know what to do, which I generally don't do anything or wait until more information comes about.", "tokens": [50364, 467, 393, 380, 312, 1096, 365, 18639, 2539, 13, 467, 2644, 312, 1096, 365, 257, 11485, 1601, 5028, 7623, 12232, 570, 437, 291, 600, 1096, 307, 291, 600, 848, 300, 456, 366, 1101, 15475, 5067, 562, 286, 500, 380, 458, 437, 281, 360, 11, 597, 286, 5101, 500, 380, 360, 1340, 420, 1699, 1826, 544, 1589, 1487, 466, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12405947402671531, "compression_ratio": 1.750877192982456, "no_speech_prob": 0.04265216365456581}, {"id": 428, "seek": 392688, "start": 3940.88, "end": 3952.88, "text": " And once you write that down, you make your objective function a function of a probability distribution which becomes an energy and then you integrate that over time and then you've got to Hampton's principle of least action.", "tokens": [51064, 400, 1564, 291, 2464, 300, 760, 11, 291, 652, 428, 10024, 2445, 257, 2445, 295, 257, 8482, 7316, 597, 3643, 364, 2281, 293, 550, 291, 13365, 300, 670, 565, 293, 550, 291, 600, 658, 281, 8234, 21987, 311, 8665, 295, 1935, 3069, 13, 51664], "temperature": 0.0, "avg_logprob": -0.12405947402671531, "compression_ratio": 1.750877192982456, "no_speech_prob": 0.04265216365456581}, {"id": 429, "seek": 395288, "start": 3952.88, "end": 3960.88, "text": " So the simplicity post hoc is evident for me anyway. Does that make any sense?", "tokens": [50364, 407, 264, 25632, 2183, 16708, 307, 16371, 337, 385, 4033, 13, 4402, 300, 652, 604, 2020, 30, 50764], "temperature": 0.0, "avg_logprob": -0.1847548568457888, "compression_ratio": 1.4472049689440993, "no_speech_prob": 0.005765238776803017}, {"id": 430, "seek": 395288, "start": 3961.88, "end": 3972.88, "text": " Can I ask you a question about the general approach here, which is I see that you start saying that there is an urge, there is a force to reduce surprise.", "tokens": [50814, 1664, 286, 1029, 291, 257, 1168, 466, 264, 2674, 3109, 510, 11, 597, 307, 286, 536, 300, 291, 722, 1566, 300, 456, 307, 364, 19029, 11, 456, 307, 257, 3464, 281, 5407, 6365, 13, 51364], "temperature": 0.0, "avg_logprob": -0.1847548568457888, "compression_ratio": 1.4472049689440993, "no_speech_prob": 0.005765238776803017}, {"id": 431, "seek": 397288, "start": 3972.88, "end": 3983.88, "text": " So this is like I'm making an analogy with physics because that's what you're doing. And there you start saying I can rewrite the whole thing instead of force in terms of a Fourier energy.", "tokens": [50364, 407, 341, 307, 411, 286, 478, 1455, 364, 21663, 365, 10649, 570, 300, 311, 437, 291, 434, 884, 13, 400, 456, 291, 722, 1566, 286, 393, 28132, 264, 1379, 551, 2602, 295, 3464, 294, 2115, 295, 257, 36810, 2281, 13, 50914], "temperature": 0.0, "avg_logprob": -0.14165031433105468, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.05150587111711502}, {"id": 432, "seek": 397288, "start": 3984.88, "end": 3991.88, "text": " And then you go on and explain what it implies. My fear is that there is a difference here between what we do in terms of behaviour.", "tokens": [50964, 400, 550, 291, 352, 322, 293, 2903, 437, 309, 18779, 13, 1222, 4240, 307, 300, 456, 307, 257, 2649, 510, 1296, 437, 321, 360, 294, 2115, 295, 17229, 13, 51314], "temperature": 0.0, "avg_logprob": -0.14165031433105468, "compression_ratio": 1.5735294117647058, "no_speech_prob": 0.05150587111711502}, {"id": 433, "seek": 399188, "start": 3991.88, "end": 4006.88, "text": " First of all, reducing behaviour to just minimizing surprise, there are other forces that we cannot measure. We cannot even measure forces, force in terms of reducing surprise in an individual.", "tokens": [50364, 2386, 295, 439, 11, 12245, 17229, 281, 445, 46608, 6365, 11, 456, 366, 661, 5874, 300, 321, 2644, 3481, 13, 492, 2644, 754, 3481, 5874, 11, 3464, 294, 2115, 295, 12245, 6365, 294, 364, 2609, 13, 51114], "temperature": 0.0, "avg_logprob": -0.10973831176757813, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.02780705876648426}, {"id": 434, "seek": 399188, "start": 4007.88, "end": 4017.88, "text": " So even though you can write these equations and describe something general so you can prescribe what should be the brain doing, do you think you can actually make any prediction?", "tokens": [51164, 407, 754, 1673, 291, 393, 2464, 613, 11787, 293, 6786, 746, 2674, 370, 291, 393, 49292, 437, 820, 312, 264, 3567, 884, 11, 360, 291, 519, 291, 393, 767, 652, 604, 17630, 30, 51664], "temperature": 0.0, "avg_logprob": -0.10973831176757813, "compression_ratio": 1.6877828054298643, "no_speech_prob": 0.02780705876648426}, {"id": 435, "seek": 401788, "start": 4017.88, "end": 4026.88, "text": " I know you're doing it, but I'm asking how can you think that you can do it because there is no measurement to tell you that's actually the case.", "tokens": [50364, 286, 458, 291, 434, 884, 309, 11, 457, 286, 478, 3365, 577, 393, 291, 519, 300, 291, 393, 360, 309, 570, 456, 307, 572, 13160, 281, 980, 291, 300, 311, 767, 264, 1389, 13, 50814], "temperature": 0.0, "avg_logprob": -0.08573801764126482, "compression_ratio": 1.6596638655462186, "no_speech_prob": 0.007306588813662529}, {"id": 436, "seek": 401788, "start": 4027.88, "end": 4040.88, "text": " So in physics you could come up with any new evidence, you would write a new term into your equation and just keep adding it and then you're always safe because there are some conservation laws that basically will keep the Hamilton principle intact.", "tokens": [50864, 407, 294, 10649, 291, 727, 808, 493, 365, 604, 777, 4467, 11, 291, 576, 2464, 257, 777, 1433, 666, 428, 5367, 293, 445, 1066, 5127, 309, 293, 550, 291, 434, 1009, 3273, 570, 456, 366, 512, 16185, 6064, 300, 1936, 486, 1066, 264, 18484, 8665, 23493, 13, 51514], "temperature": 0.0, "avg_logprob": -0.08573801764126482, "compression_ratio": 1.6596638655462186, "no_speech_prob": 0.007306588813662529}, {"id": 437, "seek": 404088, "start": 4040.88, "end": 4049.88, "text": " Well, who said there is such a thing in terms of behaviour and I think the simplest thing to assume there is no such thing.", "tokens": [50364, 1042, 11, 567, 848, 456, 307, 1270, 257, 551, 294, 2115, 295, 17229, 293, 286, 519, 264, 22811, 551, 281, 6552, 456, 307, 572, 1270, 551, 13, 50814], "temperature": 0.0, "avg_logprob": -0.15904540049878857, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.022649159654974937}, {"id": 438, "seek": 404088, "start": 4050.88, "end": 4062.88, "text": " So then by doing that, if you let's say you keep adding terms because you believe that what we are doing is just reducing surprise, this is the ultimate goal of behaviour, then just keep adding terms to your free energy.", "tokens": [50864, 407, 550, 538, 884, 300, 11, 498, 291, 718, 311, 584, 291, 1066, 5127, 2115, 570, 291, 1697, 300, 437, 321, 366, 884, 307, 445, 12245, 6365, 11, 341, 307, 264, 9705, 3387, 295, 17229, 11, 550, 445, 1066, 5127, 2115, 281, 428, 1737, 2281, 13, 51464], "temperature": 0.0, "avg_logprob": -0.15904540049878857, "compression_ratio": 1.702970297029703, "no_speech_prob": 0.022649159654974937}, {"id": 439, "seek": 406288, "start": 4062.88, "end": 4074.88, "text": " And then basically the whole thing becomes a tautology because you're assuming that you're adding terms and there is no proof or disprove for that.", "tokens": [50364, 400, 550, 1936, 264, 1379, 551, 3643, 257, 256, 1375, 1793, 570, 291, 434, 11926, 300, 291, 434, 5127, 2115, 293, 456, 307, 572, 8177, 420, 717, 46955, 337, 300, 13, 50964], "temperature": 0.0, "avg_logprob": -0.19015642230430346, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.033903948962688446}, {"id": 440, "seek": 406288, "start": 4075.88, "end": 4081.88, "text": " So that's my main question is that what is, I mean, how do you think it's going to work?", "tokens": [51014, 407, 300, 311, 452, 2135, 1168, 307, 300, 437, 307, 11, 286, 914, 11, 577, 360, 291, 519, 309, 311, 516, 281, 589, 30, 51314], "temperature": 0.0, "avg_logprob": -0.19015642230430346, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.033903948962688446}, {"id": 441, "seek": 406288, "start": 4082.88, "end": 4088.88, "text": " Right, that's a very good, I mean I ended quite a bit weakly, but that was a very good question.", "tokens": [51364, 1779, 11, 300, 311, 257, 588, 665, 11, 286, 914, 286, 4590, 1596, 257, 857, 5336, 356, 11, 457, 300, 390, 257, 588, 665, 1168, 13, 51664], "temperature": 0.0, "avg_logprob": -0.19015642230430346, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.033903948962688446}, {"id": 442, "seek": 408888, "start": 4088.88, "end": 4101.88, "text": " So lots of really interesting issues there, so the tautology issue, the practical utility of this style of theorising, can it ever be falsified adding things too?", "tokens": [50364, 407, 3195, 295, 534, 1880, 2663, 456, 11, 370, 264, 256, 1375, 1793, 2734, 11, 264, 8496, 14877, 295, 341, 3758, 295, 27423, 3436, 11, 393, 309, 1562, 312, 16720, 2587, 5127, 721, 886, 30, 51014], "temperature": 0.0, "avg_logprob": -0.17292589707808062, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0027973102405667305}, {"id": 443, "seek": 408888, "start": 4102.88, "end": 4107.88, "text": " So I think we could spend hours talking about any one of those.", "tokens": [51064, 407, 286, 519, 321, 727, 3496, 2496, 1417, 466, 604, 472, 295, 729, 13, 51314], "temperature": 0.0, "avg_logprob": -0.17292589707808062, "compression_ratio": 1.4675324675324675, "no_speech_prob": 0.0027973102405667305}, {"id": 444, "seek": 410788, "start": 4107.88, "end": 4115.88, "text": " First of all, the whole point of my style of neuroscience is that we never add anything in.", "tokens": [50364, 2386, 295, 439, 11, 264, 1379, 935, 295, 452, 3758, 295, 42762, 307, 300, 321, 1128, 909, 1340, 294, 13, 50764], "temperature": 0.0, "avg_logprob": -0.17386781601678758, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.19251097738742828}, {"id": 445, "seek": 410788, "start": 4116.88, "end": 4121.88, "text": " You're always obliged to, for every advance, you have to get rid of something which was a distraction or ad hoc or a heuristic.", "tokens": [50814, 509, 434, 1009, 47194, 281, 11, 337, 633, 7295, 11, 291, 362, 281, 483, 3973, 295, 746, 597, 390, 257, 30217, 420, 614, 16708, 420, 257, 415, 374, 3142, 13, 51064], "temperature": 0.0, "avg_logprob": -0.17386781601678758, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.19251097738742828}, {"id": 446, "seek": 410788, "start": 4122.88, "end": 4128.88, "text": " We've been talking about this magic parameter, the nuances, the balance between exploration and exploitation.", "tokens": [51114, 492, 600, 668, 1417, 466, 341, 5585, 13075, 11, 264, 38775, 11, 264, 4772, 1296, 16197, 293, 33122, 13, 51414], "temperature": 0.0, "avg_logprob": -0.17386781601678758, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.19251097738742828}, {"id": 447, "seek": 410788, "start": 4129.88, "end": 4134.88, "text": " So that just goes, there's only one thing that's being minimised here, about everything and that's variation free energy.", "tokens": [51464, 407, 300, 445, 1709, 11, 456, 311, 787, 472, 551, 300, 311, 885, 4464, 2640, 510, 11, 466, 1203, 293, 300, 311, 12990, 1737, 2281, 13, 51714], "temperature": 0.0, "avg_logprob": -0.17386781601678758, "compression_ratio": 1.6828358208955223, "no_speech_prob": 0.19251097738742828}, {"id": 448, "seek": 413488, "start": 4134.88, "end": 4140.88, "text": " That's it, there's nothing being added. However, of course, the free energy is a function of the gerontive model.", "tokens": [50364, 663, 311, 309, 11, 456, 311, 1825, 885, 3869, 13, 2908, 11, 295, 1164, 11, 264, 1737, 2281, 307, 257, 2445, 295, 264, 5713, 896, 488, 2316, 13, 50664], "temperature": 0.0, "avg_logprob": -0.15129850545060744, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0020264671184122562}, {"id": 449, "seek": 413488, "start": 4141.88, "end": 4152.88, "text": " So all the interesting, all the hard work or the heavy lifting understanding this biological system in this experimental context or this social context, that really calls upon you writing down a gerontive model.", "tokens": [50714, 407, 439, 264, 1880, 11, 439, 264, 1152, 589, 420, 264, 4676, 15798, 3701, 341, 13910, 1185, 294, 341, 17069, 4319, 420, 341, 2093, 4319, 11, 300, 534, 5498, 3564, 291, 3579, 760, 257, 5713, 896, 488, 2316, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15129850545060744, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0020264671184122562}, {"id": 450, "seek": 413488, "start": 4153.88, "end": 4160.88, "text": " So that's, it doesn't mean there is no free lunch, there's still a lot of neurobiology and psychology and cognitive neuroscience and computational neuroscience to do.", "tokens": [51314, 407, 300, 311, 11, 309, 1177, 380, 914, 456, 307, 572, 1737, 6349, 11, 456, 311, 920, 257, 688, 295, 16499, 5614, 1793, 293, 15105, 293, 15605, 42762, 293, 28270, 42762, 281, 360, 13, 51664], "temperature": 0.0, "avg_logprob": -0.15129850545060744, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0020264671184122562}, {"id": 451, "seek": 416088, "start": 4160.88, "end": 4167.88, "text": " It's just all at the level of the gerontive model, not the normative principles or the variational principles behind it.", "tokens": [50364, 467, 311, 445, 439, 412, 264, 1496, 295, 264, 5713, 896, 488, 2316, 11, 406, 264, 2026, 1166, 9156, 420, 264, 3034, 1478, 9156, 2261, 309, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10644584952048886, "compression_ratio": 1.7679324894514767, "no_speech_prob": 0.002591757569462061}, {"id": 452, "seek": 416088, "start": 4168.88, "end": 4179.88, "text": " The tautology, part of that drive for simplification is a drive to get to the ultimate explanation which has to be tautological and for me the free energy principle is tautological.", "tokens": [50764, 440, 256, 1375, 1793, 11, 644, 295, 300, 3332, 337, 6883, 3774, 307, 257, 3332, 281, 483, 281, 264, 9705, 10835, 597, 575, 281, 312, 256, 1375, 4383, 293, 337, 385, 264, 1737, 2281, 8665, 307, 256, 1375, 4383, 13, 51314], "temperature": 0.0, "avg_logprob": -0.10644584952048886, "compression_ratio": 1.7679324894514767, "no_speech_prob": 0.002591757569462061}, {"id": 453, "seek": 416088, "start": 4180.88, "end": 4186.88, "text": " It's as tautological as the natural selection, but beautifully so. Once it's completely tautological, I'll be happy.", "tokens": [51364, 467, 311, 382, 256, 1375, 4383, 382, 264, 3303, 9450, 11, 457, 16525, 370, 13, 3443, 309, 311, 2584, 256, 1375, 4383, 11, 286, 603, 312, 2055, 13, 51664], "temperature": 0.0, "avg_logprob": -0.10644584952048886, "compression_ratio": 1.7679324894514767, "no_speech_prob": 0.002591757569462061}, {"id": 454, "seek": 418688, "start": 4186.88, "end": 4200.88, "text": " Part of that tautology comes along in a slightly technical guise called the complete class theorem and I'm bringing that to the discussion because it speaks to, I think, a very interesting point you were essentially making.", "tokens": [50364, 4100, 295, 300, 256, 1375, 1793, 1487, 2051, 294, 257, 4748, 6191, 695, 908, 1219, 264, 3566, 1508, 20904, 293, 286, 478, 5062, 300, 281, 264, 5017, 570, 309, 10789, 281, 11, 286, 519, 11, 257, 588, 1880, 935, 291, 645, 4476, 1455, 13, 51064], "temperature": 0.0, "avg_logprob": -0.11795604392273785, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.001155807520262897}, {"id": 455, "seek": 418688, "start": 4201.88, "end": 4207.88, "text": " Is there anything that you can not explain, any behaviour in a real biological system that cannot be explained by this?", "tokens": [51114, 1119, 456, 1340, 300, 291, 393, 406, 2903, 11, 604, 17229, 294, 257, 957, 13910, 1185, 300, 2644, 312, 8825, 538, 341, 30, 51414], "temperature": 0.0, "avg_logprob": -0.11795604392273785, "compression_ratio": 1.573394495412844, "no_speech_prob": 0.001155807520262897}, {"id": 456, "seek": 420788, "start": 4208.88, "end": 4211.88, "text": " Because if there isn't, then what's the point?", "tokens": [50414, 1436, 498, 456, 1943, 380, 11, 550, 437, 311, 264, 935, 30, 50564], "temperature": 0.0, "avg_logprob": -0.15748982886745505, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.13074317574501038}, {"id": 457, "seek": 420788, "start": 4212.88, "end": 4235.88, "text": " Now, if there is no behaviour that this cannot explain, so it is provably true that for any pair of cost functions and behaviours there are a set of prior beliefs, prior preferences that we're talking about that endow the behaviour with an agent, with agency,", "tokens": [50614, 823, 11, 498, 456, 307, 572, 17229, 300, 341, 2644, 2903, 11, 370, 309, 307, 1439, 1188, 2074, 300, 337, 604, 6119, 295, 2063, 6828, 293, 15475, 5067, 456, 366, 257, 992, 295, 4059, 13585, 11, 4059, 21910, 300, 321, 434, 1417, 466, 300, 917, 305, 264, 17229, 365, 364, 9461, 11, 365, 7934, 11, 51764], "temperature": 0.0, "avg_logprob": -0.15748982886745505, "compression_ratio": 1.6363636363636365, "no_speech_prob": 0.13074317574501038}, {"id": 458, "seek": 423588, "start": 4235.88, "end": 4240.88, "text": " that render that behaviour based optimal and by definition therefore conforms to the free energy principle.", "tokens": [50364, 300, 15529, 300, 17229, 2361, 16252, 293, 538, 7123, 4412, 18975, 82, 281, 264, 1737, 2281, 8665, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11456114512223464, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.0024890294298529625}, {"id": 459, "seek": 423588, "start": 4241.88, "end": 4246.88, "text": " So what that means is that there is no behaviour that this can't describe if you can find the right prize.", "tokens": [50664, 407, 437, 300, 1355, 307, 300, 456, 307, 572, 17229, 300, 341, 393, 380, 6786, 498, 291, 393, 915, 264, 558, 12818, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11456114512223464, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.0024890294298529625}, {"id": 460, "seek": 423588, "start": 4247.88, "end": 4252.88, "text": " So is that a weakness or a strength? Well, in the sense of falsifiability, it's a weakness.", "tokens": [50964, 407, 307, 300, 257, 12772, 420, 257, 3800, 30, 1042, 11, 294, 264, 2020, 295, 16720, 17638, 2310, 11, 309, 311, 257, 12772, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11456114512223464, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.0024890294298529625}, {"id": 461, "seek": 423588, "start": 4253.88, "end": 4263.88, "text": " In the sense of actually using it practically, it's a real strength because what that means is any system, normal human being or psychiatric cohort,", "tokens": [51264, 682, 264, 2020, 295, 767, 1228, 309, 15667, 11, 309, 311, 257, 957, 3800, 570, 437, 300, 1355, 307, 604, 1185, 11, 2710, 1952, 885, 420, 40123, 28902, 11, 51764], "temperature": 0.0, "avg_logprob": -0.11456114512223464, "compression_ratio": 1.7567567567567568, "no_speech_prob": 0.0024890294298529625}, {"id": 462, "seek": 426388, "start": 4263.88, "end": 4271.88, "text": " that you bring to me, if I can solve the problem of getting the most appropriate or a sufficiently good generative model that describes their behaviour,", "tokens": [50364, 300, 291, 1565, 281, 385, 11, 498, 286, 393, 5039, 264, 1154, 295, 1242, 264, 881, 6854, 420, 257, 31868, 665, 1337, 1166, 2316, 300, 15626, 641, 17229, 11, 50764], "temperature": 0.0, "avg_logprob": -0.11792759801827225, "compression_ratio": 1.77734375, "no_speech_prob": 0.00026696259737946093}, {"id": 463, "seek": 426388, "start": 4272.88, "end": 4276.88, "text": " it means I can quantify exactly the sort of person they are by their prior beliefs.", "tokens": [50814, 309, 1355, 286, 393, 40421, 2293, 264, 1333, 295, 954, 436, 366, 538, 641, 4059, 13585, 13, 51014], "temperature": 0.0, "avg_logprob": -0.11792759801827225, "compression_ratio": 1.77734375, "no_speech_prob": 0.00026696259737946093}, {"id": 464, "seek": 426388, "start": 4277.88, "end": 4281.88, "text": " And we actually can write back down to the first question again, is what makes an agent an agent, it's their prior beliefs.", "tokens": [51064, 400, 321, 767, 393, 2464, 646, 760, 281, 264, 700, 1168, 797, 11, 307, 437, 1669, 364, 9461, 364, 9461, 11, 309, 311, 641, 4059, 13585, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11792759801827225, "compression_ratio": 1.77734375, "no_speech_prob": 0.00026696259737946093}, {"id": 465, "seek": 426388, "start": 4282.88, "end": 4287.88, "text": " It's that attracting set that describes the sorts of states that that sort of person occupies.", "tokens": [51314, 467, 311, 300, 36594, 992, 300, 15626, 264, 7527, 295, 4368, 300, 300, 1333, 295, 954, 8073, 530, 13, 51564], "temperature": 0.0, "avg_logprob": -0.11792759801827225, "compression_ratio": 1.77734375, "no_speech_prob": 0.00026696259737946093}, {"id": 466, "seek": 428788, "start": 4287.88, "end": 4294.88, "text": " So, yeah, there is a deep tautology, there's a fundamental difficulty for falsification,", "tokens": [50364, 407, 11, 1338, 11, 456, 307, 257, 2452, 256, 1375, 1793, 11, 456, 311, 257, 8088, 10360, 337, 16720, 3774, 11, 50714], "temperature": 0.0, "avg_logprob": -0.14530712625254755, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002661790233105421}, {"id": 467, "seek": 428788, "start": 4295.88, "end": 4300.88, "text": " but there's also a glorious insight underneath that which means that everything can now be written down in terms of an agent's prior beliefs", "tokens": [50764, 457, 456, 311, 611, 257, 24026, 11269, 7223, 300, 597, 1355, 300, 1203, 393, 586, 312, 3720, 760, 294, 2115, 295, 364, 9461, 311, 4059, 13585, 51014], "temperature": 0.0, "avg_logprob": -0.14530712625254755, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002661790233105421}, {"id": 468, "seek": 428788, "start": 4301.88, "end": 4304.88, "text": " and that if you can get the right model, you can actually estimate these things.", "tokens": [51064, 293, 300, 498, 291, 393, 483, 264, 558, 2316, 11, 291, 393, 767, 12539, 613, 721, 13, 51214], "temperature": 0.0, "avg_logprob": -0.14530712625254755, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002661790233105421}, {"id": 469, "seek": 428788, "start": 4305.88, "end": 4308.88, "text": " You can actually quantify them and this is one of the tenets of computational psychiatry.", "tokens": [51264, 509, 393, 767, 40421, 552, 293, 341, 307, 472, 295, 264, 2064, 1385, 295, 28270, 26347, 627, 13, 51414], "temperature": 0.0, "avg_logprob": -0.14530712625254755, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.002661790233105421}, {"id": 470, "seek": 430888, "start": 4308.88, "end": 4318.88, "text": " It's to be able to use games, ERPs, mismatch negativities, whatever in the service of say, what sort of person am I looking at,", "tokens": [50364, 467, 311, 281, 312, 1075, 281, 764, 2813, 11, 14929, 23043, 11, 23220, 852, 2485, 10662, 1088, 11, 2035, 294, 264, 2643, 295, 584, 11, 437, 1333, 295, 954, 669, 286, 1237, 412, 11, 50864], "temperature": 0.0, "avg_logprob": -0.16023500954232564, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.0017002831446006894}, {"id": 471, "seek": 430888, "start": 4319.88, "end": 4322.88, "text": " quantified in terms of the prior beliefs about the way they should behave.", "tokens": [50914, 4426, 2587, 294, 2115, 295, 264, 4059, 13585, 466, 264, 636, 436, 820, 15158, 13, 51064], "temperature": 0.0, "avg_logprob": -0.16023500954232564, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.0017002831446006894}, {"id": 472, "seek": 430888, "start": 4323.88, "end": 4331.88, "text": " Carl, but aren't you making the assumption that there is just a unique set of beliefs that will match a data set?", "tokens": [51114, 14256, 11, 457, 3212, 380, 291, 1455, 264, 15302, 300, 456, 307, 445, 257, 3845, 992, 295, 13585, 300, 486, 2995, 257, 1412, 992, 30, 51514], "temperature": 0.0, "avg_logprob": -0.16023500954232564, "compression_ratio": 1.5119617224880382, "no_speech_prob": 0.0017002831446006894}, {"id": 473, "seek": 433188, "start": 4331.88, "end": 4336.88, "text": " Because if you have several beliefs and I would think that when we minimize those type of system,", "tokens": [50364, 1436, 498, 291, 362, 2940, 13585, 293, 286, 576, 519, 300, 562, 321, 17522, 729, 2010, 295, 1185, 11, 50614], "temperature": 0.0, "avg_logprob": -0.19093359733114437, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.05755792558193207}, {"id": 474, "seek": 433188, "start": 4337.88, "end": 4344.88, "text": " that's equivalent to several solutions and which almost always we've got several solutions because we've got several minimas there.", "tokens": [50664, 300, 311, 10344, 281, 2940, 6547, 293, 597, 1920, 1009, 321, 600, 658, 2940, 6547, 570, 321, 600, 658, 2940, 4464, 296, 456, 13, 51014], "temperature": 0.0, "avg_logprob": -0.19093359733114437, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.05755792558193207}, {"id": 475, "seek": 433188, "start": 4345.88, "end": 4348.88, "text": " Because it's not convex, the story that we are minimizing there.", "tokens": [51064, 1436, 309, 311, 406, 42432, 11, 264, 1657, 300, 321, 366, 46608, 456, 13, 51214], "temperature": 0.0, "avg_logprob": -0.19093359733114437, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.05755792558193207}, {"id": 476, "seek": 433188, "start": 4349.88, "end": 4354.88, "text": " So, therefore, we have a lot of minimas and I would say that if we're lucky that means that for any behavior that we can measure,", "tokens": [51264, 407, 11, 4412, 11, 321, 362, 257, 688, 295, 4464, 296, 293, 286, 576, 584, 300, 498, 321, 434, 6356, 300, 1355, 300, 337, 604, 5223, 300, 321, 393, 3481, 11, 51514], "temperature": 0.0, "avg_logprob": -0.19093359733114437, "compression_ratio": 1.8354978354978355, "no_speech_prob": 0.05755792558193207}, {"id": 477, "seek": 435488, "start": 4354.88, "end": 4360.88, "text": " we are now with a set and that could be thrilling to know which one, a set of prior beliefs.", "tokens": [50364, 321, 366, 586, 365, 257, 992, 293, 300, 727, 312, 39347, 281, 458, 597, 472, 11, 257, 992, 295, 4059, 13585, 13, 50664], "temperature": 0.0, "avg_logprob": -0.19537769219814202, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.008103081956505775}, {"id": 478, "seek": 435488, "start": 4361.88, "end": 4364.88, "text": " So, in other words, we cannot inverse that or am I wrong?", "tokens": [50714, 407, 11, 294, 661, 2283, 11, 321, 2644, 17340, 300, 420, 669, 286, 2085, 30, 50864], "temperature": 0.0, "avg_logprob": -0.19537769219814202, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.008103081956505775}, {"id": 479, "seek": 435488, "start": 4365.88, "end": 4369.88, "text": " No, no, no, you're absolutely right but you've taken us into metabasian land now.", "tokens": [50914, 883, 11, 572, 11, 572, 11, 291, 434, 3122, 558, 457, 291, 600, 2726, 505, 666, 1131, 455, 296, 952, 2117, 586, 13, 51114], "temperature": 0.0, "avg_logprob": -0.19537769219814202, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.008103081956505775}, {"id": 480, "seek": 435488, "start": 4370.88, "end": 4375.88, "text": " Okay, so just to, those people who may be getting a bit confused.", "tokens": [51164, 1033, 11, 370, 445, 281, 11, 729, 561, 567, 815, 312, 1242, 257, 857, 9019, 13, 51414], "temperature": 0.0, "avg_logprob": -0.19537769219814202, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.008103081956505775}, {"id": 481, "seek": 435488, "start": 4376.88, "end": 4383.88, "text": " So, the argument I think, let me just paraphrase it, if we're now saying well how do we practically use this style of thinking", "tokens": [51464, 407, 11, 264, 6770, 286, 519, 11, 718, 385, 445, 36992, 1703, 651, 309, 11, 498, 321, 434, 586, 1566, 731, 577, 360, 321, 15667, 764, 341, 3758, 295, 1953, 51814], "temperature": 0.0, "avg_logprob": -0.19537769219814202, "compression_ratio": 1.579925650557621, "no_speech_prob": 0.008103081956505775}, {"id": 482, "seek": 438488, "start": 4384.88, "end": 4392.88, "text": " and I'm now in the job of quantifying this person with autistic spectrum disorder,", "tokens": [50364, 293, 286, 478, 586, 294, 264, 1691, 295, 4426, 5489, 341, 954, 365, 33272, 11143, 13399, 11, 50764], "temperature": 0.0, "avg_logprob": -0.14274291386679996, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.00016241017146967351}, {"id": 483, "seek": 438488, "start": 4393.88, "end": 4399.88, "text": " given a bead's task, an earned task, in terms of the prior beliefs about the volatility of the environment", "tokens": [50814, 2212, 257, 24117, 311, 5633, 11, 364, 12283, 5633, 11, 294, 2115, 295, 264, 4059, 13585, 466, 264, 25877, 295, 264, 2823, 51114], "temperature": 0.0, "avg_logprob": -0.14274291386679996, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.00016241017146967351}, {"id": 484, "seek": 438488, "start": 4400.88, "end": 4406.88, "text": " and the need to please the experimenter by responding within a certain time frame.", "tokens": [51164, 293, 264, 643, 281, 1767, 264, 5120, 260, 538, 16670, 1951, 257, 1629, 565, 3920, 13, 51464], "temperature": 0.0, "avg_logprob": -0.14274291386679996, "compression_ratio": 1.5280898876404494, "no_speech_prob": 0.00016241017146967351}, {"id": 485, "seek": 440688, "start": 4406.88, "end": 4414.88, "text": " If that is the problem, then we're now in and observing the observer or using Bayesian inference", "tokens": [50364, 759, 300, 307, 264, 1154, 11, 550, 321, 434, 586, 294, 293, 22107, 264, 27878, 420, 1228, 7840, 42434, 38253, 50764], "temperature": 0.0, "avg_logprob": -0.13638678002864757, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0006062959437258542}, {"id": 486, "seek": 440688, "start": 4415.88, "end": 4422.88, "text": " to make inferences about a Bayesian machine, which is the autistic patient, which is hence the metabasian thing", "tokens": [50814, 281, 652, 13596, 2667, 466, 257, 7840, 42434, 3479, 11, 597, 307, 264, 33272, 4537, 11, 597, 307, 16678, 264, 1131, 455, 296, 952, 551, 51164], "temperature": 0.0, "avg_logprob": -0.13638678002864757, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0006062959437258542}, {"id": 487, "seek": 440688, "start": 4423.88, "end": 4425.88, "text": " and you're absolutely right, that could be an ill-posed problem.", "tokens": [51214, 293, 291, 434, 3122, 558, 11, 300, 727, 312, 364, 3171, 12, 79, 1744, 1154, 13, 51314], "temperature": 0.0, "avg_logprob": -0.13638678002864757, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0006062959437258542}, {"id": 488, "seek": 440688, "start": 4426.88, "end": 4432.88, "text": " So, there may be, the complete class there does not say that there is only one unique set of prior beliefs", "tokens": [51364, 407, 11, 456, 815, 312, 11, 264, 3566, 1508, 456, 775, 406, 584, 300, 456, 307, 787, 472, 3845, 992, 295, 4059, 13585, 51664], "temperature": 0.0, "avg_logprob": -0.13638678002864757, "compression_ratio": 1.6814159292035398, "no_speech_prob": 0.0006062959437258542}, {"id": 489, "seek": 443288, "start": 4432.88, "end": 4436.88, "text": " that will make, render the behaviour based on what it says that it exists, it doesn't have to be a unique solution.", "tokens": [50364, 300, 486, 652, 11, 15529, 264, 17229, 2361, 322, 437, 309, 1619, 300, 309, 8198, 11, 309, 1177, 380, 362, 281, 312, 257, 3845, 3827, 13, 50564], "temperature": 0.0, "avg_logprob": -0.13303844745342547, "compression_ratio": 1.6934865900383143, "no_speech_prob": 0.0009120882023125887}, {"id": 490, "seek": 443288, "start": 4437.88, "end": 4445.88, "text": " However, what will happen is that if you actually use Bayesian statistics to infer the prior beliefs of the Bayesian or ASD subject,", "tokens": [50614, 2908, 11, 437, 486, 1051, 307, 300, 498, 291, 767, 764, 7840, 42434, 12523, 281, 13596, 264, 4059, 13585, 295, 264, 7840, 42434, 420, 7469, 35, 3983, 11, 51014], "temperature": 0.0, "avg_logprob": -0.13303844745342547, "compression_ratio": 1.6934865900383143, "no_speech_prob": 0.0009120882023125887}, {"id": 491, "seek": 443288, "start": 4446.88, "end": 4451.88, "text": " you will then see if you've got the appropriate model that there are a number of equally plausible solutions", "tokens": [51064, 291, 486, 550, 536, 498, 291, 600, 658, 264, 6854, 2316, 300, 456, 366, 257, 1230, 295, 12309, 39925, 6547, 51314], "temperature": 0.0, "avg_logprob": -0.13303844745342547, "compression_ratio": 1.6934865900383143, "no_speech_prob": 0.0009120882023125887}, {"id": 492, "seek": 443288, "start": 4452.88, "end": 4456.88, "text": " and you will also see that you haven't got enough data to disambiguate between them.", "tokens": [51364, 293, 291, 486, 611, 536, 300, 291, 2378, 380, 658, 1547, 1412, 281, 717, 2173, 328, 10107, 1296, 552, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13303844745342547, "compression_ratio": 1.6934865900383143, "no_speech_prob": 0.0009120882023125887}, {"id": 493, "seek": 445688, "start": 4456.88, "end": 4461.88, "text": " But you will also have an insight into which sorts of experiments you would need to do that disambiguation", "tokens": [50364, 583, 291, 486, 611, 362, 364, 11269, 666, 597, 7527, 295, 12050, 291, 576, 643, 281, 360, 300, 717, 2173, 328, 16073, 50614], "temperature": 0.0, "avg_logprob": -0.1093737320466475, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0152737470343709}, {"id": 494, "seek": 445688, "start": 4462.88, "end": 4464.88, "text": " because you've got a generative model underneath of this.", "tokens": [50664, 570, 291, 600, 658, 257, 1337, 1166, 2316, 7223, 295, 341, 13, 50764], "temperature": 0.0, "avg_logprob": -0.1093737320466475, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0152737470343709}, {"id": 495, "seek": 445688, "start": 4465.88, "end": 4469.88, "text": " You can do simulations to see the sorts of data that you are, or the ways of looking at the responses", "tokens": [50814, 509, 393, 360, 35138, 281, 536, 264, 7527, 295, 1412, 300, 291, 366, 11, 420, 264, 2098, 295, 1237, 412, 264, 13019, 51014], "temperature": 0.0, "avg_logprob": -0.1093737320466475, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0152737470343709}, {"id": 496, "seek": 445688, "start": 4470.88, "end": 4477.88, "text": " that would enable you now to narrow down these competing but equally plausible sets of prior beliefs that characterize that subject.", "tokens": [51064, 300, 576, 9528, 291, 586, 281, 9432, 760, 613, 15439, 457, 12309, 39925, 6352, 295, 4059, 13585, 300, 38463, 300, 3983, 13, 51414], "temperature": 0.0, "avg_logprob": -0.1093737320466475, "compression_ratio": 1.6419753086419753, "no_speech_prob": 0.0152737470343709}, {"id": 497, "seek": 447788, "start": 4478.88, "end": 4485.88, "text": " So, it's a very important issue, but I think it's a generic one about how we actually apply Bayesian statistics", "tokens": [50414, 407, 11, 309, 311, 257, 588, 1021, 2734, 11, 457, 286, 519, 309, 311, 257, 19577, 472, 466, 577, 321, 767, 3079, 7840, 42434, 12523, 50764], "temperature": 0.0, "avg_logprob": -0.11616161189128443, "compression_ratio": 1.6375, "no_speech_prob": 0.19966477155685425}, {"id": 498, "seek": 447788, "start": 4486.88, "end": 4489.88, "text": " to understand the ways in which our data are being generated.", "tokens": [50814, 281, 1223, 264, 2098, 294, 597, 527, 1412, 366, 885, 10833, 13, 50964], "temperature": 0.0, "avg_logprob": -0.11616161189128443, "compression_ratio": 1.6375, "no_speech_prob": 0.19966477155685425}, {"id": 499, "seek": 447788, "start": 4490.88, "end": 4495.88, "text": " I think that in that sense it's less profound than the complete class theorem in itself,", "tokens": [51014, 286, 519, 300, 294, 300, 2020, 309, 311, 1570, 14382, 813, 264, 3566, 1508, 20904, 294, 2564, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11616161189128443, "compression_ratio": 1.6375, "no_speech_prob": 0.19966477155685425}, {"id": 500, "seek": 447788, "start": 4496.88, "end": 4500.88, "text": " which speaks to the sort of tautology of the game that we find ourselves in.", "tokens": [51314, 597, 10789, 281, 264, 1333, 295, 256, 1375, 1793, 295, 264, 1216, 300, 321, 915, 4175, 294, 13, 51514], "temperature": 0.0, "avg_logprob": -0.11616161189128443, "compression_ratio": 1.6375, "no_speech_prob": 0.19966477155685425}, {"id": 501, "seek": 447788, "start": 4503.88, "end": 4505.88, "text": " I think we can have time for one more short question.", "tokens": [51664, 286, 519, 321, 393, 362, 565, 337, 472, 544, 2099, 1168, 13, 51764], "temperature": 0.0, "avg_logprob": -0.11616161189128443, "compression_ratio": 1.6375, "no_speech_prob": 0.19966477155685425}, {"id": 502, "seek": 450588, "start": 4505.88, "end": 4509.88, "text": " I have two short ones.", "tokens": [50364, 286, 362, 732, 2099, 2306, 13, 50564], "temperature": 0.0, "avg_logprob": -0.15552688256288186, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.006505026947706938}, {"id": 503, "seek": 450588, "start": 4510.88, "end": 4516.88, "text": " The first one is about the semantics and the second one is about the substance.", "tokens": [50614, 440, 700, 472, 307, 466, 264, 4361, 45298, 293, 264, 1150, 472, 307, 466, 264, 12961, 13, 50914], "temperature": 0.0, "avg_logprob": -0.15552688256288186, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.006505026947706938}, {"id": 504, "seek": 450588, "start": 4517.88, "end": 4523.88, "text": " So, regarding the semantics, I'm wondering, it seems as though there are these two aspects to the problem.", "tokens": [50964, 407, 11, 8595, 264, 4361, 45298, 11, 286, 478, 6359, 11, 309, 2544, 382, 1673, 456, 366, 613, 732, 7270, 281, 264, 1154, 13, 51264], "temperature": 0.0, "avg_logprob": -0.15552688256288186, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.006505026947706938}, {"id": 505, "seek": 450588, "start": 4524.88, "end": 4529.88, "text": " One is inference, the other is control and there is the epistemic cost in the context of inference", "tokens": [51314, 1485, 307, 38253, 11, 264, 661, 307, 1969, 293, 456, 307, 264, 2388, 468, 3438, 2063, 294, 264, 4319, 295, 38253, 51564], "temperature": 0.0, "avg_logprob": -0.15552688256288186, "compression_ratio": 1.664864864864865, "no_speech_prob": 0.006505026947706938}, {"id": 506, "seek": 452988, "start": 4529.88, "end": 4536.88, "text": " and there is the pragmatic costs, ultimately reproduction, the well-being and survival of the organism.", "tokens": [50364, 293, 456, 307, 264, 46904, 5497, 11, 6284, 33934, 11, 264, 731, 12, 13054, 293, 12559, 295, 264, 24128, 13, 50714], "temperature": 0.0, "avg_logprob": -0.11516723400209009, "compression_ratio": 1.6161137440758293, "no_speech_prob": 0.007397923152893782}, {"id": 507, "seek": 452988, "start": 4537.88, "end": 4546.88, "text": " It seems as though you are declaring the primacy of surprise, so you want to absorb the pragmatic costs under the epistemic costs.", "tokens": [50764, 467, 2544, 382, 1673, 291, 366, 40374, 264, 2886, 2551, 295, 6365, 11, 370, 291, 528, 281, 15631, 264, 46904, 5497, 833, 264, 2388, 468, 3438, 5497, 13, 51214], "temperature": 0.0, "avg_logprob": -0.11516723400209009, "compression_ratio": 1.6161137440758293, "no_speech_prob": 0.007397923152893782}, {"id": 508, "seek": 452988, "start": 4547.88, "end": 4552.88, "text": " So, that's a bit of a semantic issue perhaps, but doesn't it make more sense to do it the other way around", "tokens": [51264, 407, 11, 300, 311, 257, 857, 295, 257, 47982, 2734, 4317, 11, 457, 1177, 380, 309, 652, 544, 2020, 281, 360, 309, 264, 661, 636, 926, 51514], "temperature": 0.0, "avg_logprob": -0.11516723400209009, "compression_ratio": 1.6161137440758293, "no_speech_prob": 0.007397923152893782}, {"id": 509, "seek": 455288, "start": 4552.88, "end": 4560.88, "text": " and say, so the ultimate goal is reproduction and epistemic benefits should be a sub-goal of that.", "tokens": [50364, 293, 584, 11, 370, 264, 9705, 3387, 307, 33934, 293, 2388, 468, 3438, 5311, 820, 312, 257, 1422, 12, 1571, 304, 295, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.12466845934904075, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.013725308701395988}, {"id": 510, "seek": 455288, "start": 4561.88, "end": 4563.88, "text": " So, that's the semantic question.", "tokens": [50814, 407, 11, 300, 311, 264, 47982, 1168, 13, 50914], "temperature": 0.0, "avg_logprob": -0.12466845934904075, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.013725308701395988}, {"id": 511, "seek": 455288, "start": 4564.88, "end": 4574.88, "text": " The question on substances, is this something that is already going on in engineering in AI, for example?", "tokens": [50964, 440, 1168, 322, 25455, 11, 307, 341, 746, 300, 307, 1217, 516, 322, 294, 7043, 294, 7318, 11, 337, 1365, 30, 51464], "temperature": 0.0, "avg_logprob": -0.12466845934904075, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.013725308701395988}, {"id": 512, "seek": 455288, "start": 4575.88, "end": 4581.88, "text": " Can these principles be scaled up to real-world tasks such as visual object recognition?", "tokens": [51514, 1664, 613, 9156, 312, 36039, 493, 281, 957, 12, 13217, 9608, 1270, 382, 5056, 2657, 11150, 30, 51814], "temperature": 0.0, "avg_logprob": -0.12466845934904075, "compression_ratio": 1.5069124423963134, "no_speech_prob": 0.013725308701395988}, {"id": 513, "seek": 458288, "start": 4582.88, "end": 4589.88, "text": " If so, has it already been done or is this an impending revolution for AI?", "tokens": [50364, 759, 370, 11, 575, 309, 1217, 668, 1096, 420, 307, 341, 364, 704, 2029, 8894, 337, 7318, 30, 50714], "temperature": 0.0, "avg_logprob": -0.21182709178705325, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0017374507151544094}, {"id": 514, "seek": 458288, "start": 4590.88, "end": 4595.88, "text": " And if not, is it just a reinterpretation of what already exists?", "tokens": [50764, 400, 498, 406, 11, 307, 309, 445, 257, 319, 41935, 399, 295, 437, 1217, 8198, 30, 51014], "temperature": 0.0, "avg_logprob": -0.21182709178705325, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0017374507151544094}, {"id": 515, "seek": 458288, "start": 4596.88, "end": 4598.88, "text": " I agree because they were very short questions.", "tokens": [51064, 286, 3986, 570, 436, 645, 588, 2099, 1651, 13, 51164], "temperature": 0.0, "avg_logprob": -0.21182709178705325, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0017374507151544094}, {"id": 516, "seek": 458288, "start": 4599.88, "end": 4602.88, "text": " Very short. You couldn't get them shorter than that, could you?", "tokens": [51214, 4372, 2099, 13, 509, 2809, 380, 483, 552, 11639, 813, 300, 11, 727, 291, 30, 51364], "temperature": 0.0, "avg_logprob": -0.21182709178705325, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0017374507151544094}, {"id": 517, "seek": 458288, "start": 4603.88, "end": 4605.88, "text": " Do you want me to try to answer them shortly?", "tokens": [51414, 1144, 291, 528, 385, 281, 853, 281, 1867, 552, 13392, 30, 51514], "temperature": 0.0, "avg_logprob": -0.21182709178705325, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0017374507151544094}, {"id": 518, "seek": 458288, "start": 4606.88, "end": 4608.88, "text": " The semantic one.", "tokens": [51564, 440, 47982, 472, 13, 51664], "temperature": 0.0, "avg_logprob": -0.21182709178705325, "compression_ratio": 1.5047619047619047, "no_speech_prob": 0.0017374507151544094}, {"id": 519, "seek": 460888, "start": 4608.88, "end": 4611.88, "text": " No, I don't think so.", "tokens": [50364, 883, 11, 286, 500, 380, 519, 370, 13, 50514], "temperature": 0.0, "avg_logprob": -0.09444308925319363, "compression_ratio": 1.5678391959798994, "no_speech_prob": 0.001687905052676797}, {"id": 520, "seek": 460888, "start": 4612.88, "end": 4619.88, "text": " I take your point that one could articulate a whole theory and absorb uncertainty into cost functions,", "tokens": [50564, 286, 747, 428, 935, 300, 472, 727, 30305, 257, 1379, 5261, 293, 15631, 15697, 666, 2063, 6828, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09444308925319363, "compression_ratio": 1.5678391959798994, "no_speech_prob": 0.001687905052676797}, {"id": 521, "seek": 460888, "start": 4620.88, "end": 4627.88, "text": " but I think that misses the key point that I was trying to make at the beginning of the talk,", "tokens": [50964, 457, 286, 519, 300, 29394, 264, 2141, 935, 300, 286, 390, 1382, 281, 652, 412, 264, 2863, 295, 264, 751, 11, 51314], "temperature": 0.0, "avg_logprob": -0.09444308925319363, "compression_ratio": 1.5678391959798994, "no_speech_prob": 0.001687905052676797}, {"id": 522, "seek": 460888, "start": 4628.88, "end": 4633.88, "text": " that the quantity you have to optimise is a function of probability distributions or beliefs.", "tokens": [51364, 300, 264, 11275, 291, 362, 281, 5028, 908, 307, 257, 2445, 295, 8482, 37870, 420, 13585, 13, 51614], "temperature": 0.0, "avg_logprob": -0.09444308925319363, "compression_ratio": 1.5678391959798994, "no_speech_prob": 0.001687905052676797}, {"id": 523, "seek": 463388, "start": 4634.88, "end": 4641.88, "text": " That's the key thing, which means you can never use a Bellman optimality scheme to properly solve that.", "tokens": [50414, 663, 311, 264, 2141, 551, 11, 597, 1355, 291, 393, 1128, 764, 257, 11485, 1601, 5028, 1860, 12232, 281, 6108, 5039, 300, 13, 50764], "temperature": 0.0, "avg_logprob": -0.127451567422776, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.01074548251926899}, {"id": 524, "seek": 463388, "start": 4642.88, "end": 4647.88, "text": " You do see heroic attempts, and those heroic attempts have been endemic for the past 40 years,", "tokens": [50814, 509, 360, 536, 32915, 15257, 11, 293, 729, 32915, 15257, 362, 668, 917, 3438, 337, 264, 1791, 3356, 924, 11, 51064], "temperature": 0.0, "avg_logprob": -0.127451567422776, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.01074548251926899}, {"id": 525, "seek": 463388, "start": 4648.88, "end": 4652.88, "text": " they're known as partial observed mark-up decision processes or belief state machines,", "tokens": [51114, 436, 434, 2570, 382, 14641, 13095, 1491, 12, 1010, 3537, 7555, 420, 7107, 1785, 8379, 11, 51314], "temperature": 0.0, "avg_logprob": -0.127451567422776, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.01074548251926899}, {"id": 526, "seek": 463388, "start": 4653.88, "end": 4658.88, "text": " and all sorts of heroic attempts to try and recover or resolve the problem,", "tokens": [51364, 293, 439, 7527, 295, 32915, 15257, 281, 853, 293, 8114, 420, 14151, 264, 1154, 11, 51614], "temperature": 0.0, "avg_logprob": -0.127451567422776, "compression_ratio": 1.6116071428571428, "no_speech_prob": 0.01074548251926899}, {"id": 527, "seek": 465888, "start": 4658.88, "end": 4665.88, "text": " or once you write down a discrete state space over continuous beliefs, you can't do it,", "tokens": [50364, 420, 1564, 291, 2464, 760, 257, 27706, 1785, 1901, 670, 10957, 13585, 11, 291, 393, 380, 360, 309, 11, 50714], "temperature": 0.0, "avg_logprob": -0.10391995566231864, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0024481576401740313}, {"id": 528, "seek": 465888, "start": 4666.88, "end": 4670.88, "text": " therefore you have to parameterise those continuous beliefs in those sorts of glorious and clever ways,", "tokens": [50764, 4412, 291, 362, 281, 13075, 908, 729, 10957, 13585, 294, 729, 7527, 295, 24026, 293, 13494, 2098, 11, 50964], "temperature": 0.0, "avg_logprob": -0.10391995566231864, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0024481576401740313}, {"id": 529, "seek": 465888, "start": 4671.88, "end": 4672.88, "text": " they don't work, they don't scale.", "tokens": [51014, 436, 500, 380, 589, 11, 436, 500, 380, 4373, 13, 51064], "temperature": 0.0, "avg_logprob": -0.10391995566231864, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0024481576401740313}, {"id": 530, "seek": 465888, "start": 4673.88, "end": 4678.88, "text": " So people have been aware of the problem, and hence the vast literature on partial observed mark-up decision problems.", "tokens": [51114, 407, 561, 362, 668, 3650, 295, 264, 1154, 11, 293, 16678, 264, 8369, 10394, 322, 14641, 13095, 1491, 12, 1010, 3537, 2740, 13, 51364], "temperature": 0.0, "avg_logprob": -0.10391995566231864, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0024481576401740313}, {"id": 531, "seek": 465888, "start": 4679.88, "end": 4684.88, "text": " What I'm saying is that's the wrong approach, the right approach is Hampton's principle of least action,", "tokens": [51414, 708, 286, 478, 1566, 307, 300, 311, 264, 2085, 3109, 11, 264, 558, 3109, 307, 8234, 21987, 311, 8665, 295, 1935, 3069, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10391995566231864, "compression_ratio": 1.6981132075471699, "no_speech_prob": 0.0024481576401740313}, {"id": 532, "seek": 468488, "start": 4684.88, "end": 4689.88, "text": " where the functional that you need to optimise is a function of the beliefs before you start,", "tokens": [50364, 689, 264, 11745, 300, 291, 643, 281, 5028, 908, 307, 257, 2445, 295, 264, 13585, 949, 291, 722, 11, 50614], "temperature": 0.0, "avg_logprob": -0.12149067974965506, "compression_ratio": 1.632867132867133, "no_speech_prob": 0.0037016698624938726}, {"id": 533, "seek": 468488, "start": 4690.88, "end": 4693.88, "text": " and by proof of principle I can demonstrate the veracity of that argument,", "tokens": [50664, 293, 538, 8177, 295, 8665, 286, 393, 11698, 264, 1306, 19008, 295, 300, 6770, 11, 50814], "temperature": 0.0, "avg_logprob": -0.12149067974965506, "compression_ratio": 1.632867132867133, "no_speech_prob": 0.0037016698624938726}, {"id": 534, "seek": 468488, "start": 4694.88, "end": 4698.88, "text": " because I can solve problems which people working with partial observed MDPs cannot solve.", "tokens": [50864, 570, 286, 393, 5039, 2740, 597, 561, 1364, 365, 14641, 13095, 376, 11373, 82, 2644, 5039, 13, 51064], "temperature": 0.0, "avg_logprob": -0.12149067974965506, "compression_ratio": 1.632867132867133, "no_speech_prob": 0.0037016698624938726}, {"id": 535, "seek": 468488, "start": 4699.88, "end": 4706.88, "text": " So I think it's much more than semantics, I think it's actually, people have got it wrong in the 20th century.", "tokens": [51114, 407, 286, 519, 309, 311, 709, 544, 813, 4361, 45298, 11, 286, 519, 309, 311, 767, 11, 561, 362, 658, 309, 2085, 294, 264, 945, 392, 4901, 13, 51464], "temperature": 0.0, "avg_logprob": -0.12149067974965506, "compression_ratio": 1.632867132867133, "no_speech_prob": 0.0037016698624938726}, {"id": 536, "seek": 468488, "start": 4707.88, "end": 4711.88, "text": " I think the Bellman's optimality equations are very beautiful construct, completeness direction.", "tokens": [51514, 286, 519, 264, 11485, 1601, 311, 5028, 1860, 11787, 366, 588, 2238, 7690, 11, 1557, 15264, 3513, 13, 51714], "temperature": 0.0, "avg_logprob": -0.12149067974965506, "compression_ratio": 1.632867132867133, "no_speech_prob": 0.0037016698624938726}, {"id": 537, "seek": 471188, "start": 4711.88, "end": 4715.88, "text": " We should have been looking at Hamilton's principle of least action, and that's the 21st century.", "tokens": [50364, 492, 820, 362, 668, 1237, 412, 18484, 311, 8665, 295, 1935, 3069, 11, 293, 300, 311, 264, 5080, 372, 4901, 13, 50564], "temperature": 0.0, "avg_logprob": -0.16162804882935802, "compression_ratio": 1.5676691729323309, "no_speech_prob": 0.0017608375055715442}, {"id": 538, "seek": 471188, "start": 4716.88, "end": 4717.88, "text": " Are people doing this? Yeah.", "tokens": [50614, 2014, 561, 884, 341, 30, 865, 13, 50664], "temperature": 0.0, "avg_logprob": -0.16162804882935802, "compression_ratio": 1.5676691729323309, "no_speech_prob": 0.0017608375055715442}, {"id": 539, "seek": 471188, "start": 4718.88, "end": 4724.88, "text": " So Google Deep Mind, for example, they've now, a small group within Google Deep Mind,", "tokens": [50714, 407, 3329, 14895, 13719, 11, 337, 1365, 11, 436, 600, 586, 11, 257, 1359, 1594, 1951, 3329, 14895, 13719, 11, 51014], "temperature": 0.0, "avg_logprob": -0.16162804882935802, "compression_ratio": 1.5676691729323309, "no_speech_prob": 0.0017608375055715442}, {"id": 540, "seek": 471188, "start": 4725.88, "end": 4730.88, "text": " have now started using variational free energy in their deep energy model, not the temporal models,", "tokens": [51064, 362, 586, 1409, 1228, 3034, 1478, 1737, 2281, 294, 641, 2452, 2281, 2316, 11, 406, 264, 30881, 5245, 11, 51314], "temperature": 0.0, "avg_logprob": -0.16162804882935802, "compression_ratio": 1.5676691729323309, "no_speech_prob": 0.0017608375055715442}, {"id": 541, "seek": 471188, "start": 4731.88, "end": 4738.88, "text": " but certainly sort of 10-led neural networks in the context of the sort of pattern recognition approach.", "tokens": [51364, 457, 3297, 1333, 295, 1266, 12, 1493, 18161, 9590, 294, 264, 4319, 295, 264, 1333, 295, 5102, 11150, 3109, 13, 51714], "temperature": 0.0, "avg_logprob": -0.16162804882935802, "compression_ratio": 1.5676691729323309, "no_speech_prob": 0.0017608375055715442}, {"id": 542, "seek": 473888, "start": 4738.88, "end": 4745.88, "text": " So people have always rears, I think, that the variational approach was the right way to do this,", "tokens": [50364, 407, 561, 362, 1009, 319, 685, 11, 286, 519, 11, 300, 264, 3034, 1478, 3109, 390, 264, 558, 636, 281, 360, 341, 11, 50714], "temperature": 0.0, "avg_logprob": -0.17477769267802334, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.0009278664365410805}, {"id": 543, "seek": 473888, "start": 4746.88, "end": 4750.88, "text": " and of course you remember most of deep learning started with Geoffrey Hinton's work,", "tokens": [50764, 293, 295, 1164, 291, 1604, 881, 295, 2452, 2539, 1409, 365, 26119, 7950, 389, 12442, 311, 589, 11, 50964], "temperature": 0.0, "avg_logprob": -0.17477769267802334, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.0009278664365410805}, {"id": 544, "seek": 473888, "start": 4751.88, "end": 4759.88, "text": " and he came from the variational formulation, so he was the first person to be down to propose the Helmholtz machine,", "tokens": [51014, 293, 415, 1361, 490, 264, 3034, 1478, 37642, 11, 370, 415, 390, 264, 700, 954, 281, 312, 760, 281, 17421, 264, 6128, 76, 71, 4837, 89, 3479, 11, 51414], "temperature": 0.0, "avg_logprob": -0.17477769267802334, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.0009278664365410805}, {"id": 545, "seek": 473888, "start": 4760.88, "end": 4764.88, "text": " but they've taken a sort of security stream back to those early work in the 1990s.", "tokens": [51464, 457, 436, 600, 2726, 257, 1333, 295, 3825, 4309, 646, 281, 729, 2440, 589, 294, 264, 13384, 82, 13, 51664], "temperature": 0.0, "avg_logprob": -0.17477769267802334, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.0009278664365410805}, {"id": 546, "seek": 476488, "start": 4764.88, "end": 4768.88, "text": " Will it scale? I don't know, because I don't think they're quite on top of that,", "tokens": [50364, 3099, 309, 4373, 30, 286, 500, 380, 458, 11, 570, 286, 500, 380, 519, 436, 434, 1596, 322, 1192, 295, 300, 11, 50564], "temperature": 0.0, "avg_logprob": -0.16332514040938048, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.010036814026534557}, {"id": 547, "seek": 476488, "start": 4769.88, "end": 4776.88, "text": " because what they do is they haven't quite got, well, this is insulting, but I hope, is anybody here from Google?", "tokens": [50614, 570, 437, 436, 360, 307, 436, 2378, 380, 1596, 658, 11, 731, 11, 341, 307, 44463, 11, 457, 286, 1454, 11, 307, 4472, 510, 490, 3329, 30, 50964], "temperature": 0.0, "avg_logprob": -0.16332514040938048, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.010036814026534557}, {"id": 548, "seek": 476488, "start": 4777.88, "end": 4780.88, "text": " Ah, well I won't say that then.", "tokens": [51014, 2438, 11, 731, 286, 1582, 380, 584, 300, 550, 13, 51164], "temperature": 0.0, "avg_logprob": -0.16332514040938048, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.010036814026534557}, {"id": 549, "seek": 476488, "start": 4781.88, "end": 4786.88, "text": " Well anyway, they love amortisation, they love casting things in terms of learning,", "tokens": [51214, 1042, 4033, 11, 436, 959, 669, 477, 7623, 11, 436, 959, 17301, 721, 294, 2115, 295, 2539, 11, 51464], "temperature": 0.0, "avg_logprob": -0.16332514040938048, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.010036814026534557}, {"id": 550, "seek": 476488, "start": 4787.88, "end": 4791.88, "text": " so what they do is instead of actually trying to optimise their beliefs, their expectations,", "tokens": [51514, 370, 437, 436, 360, 307, 2602, 295, 767, 1382, 281, 5028, 908, 641, 13585, 11, 641, 9843, 11, 51714], "temperature": 0.0, "avg_logprob": -0.16332514040938048, "compression_ratio": 1.6932773109243697, "no_speech_prob": 0.010036814026534557}, {"id": 551, "seek": 479188, "start": 4791.88, "end": 4796.88, "text": " they try and optimise beautifully constructed deep nets, convolution nets,", "tokens": [50364, 436, 853, 293, 5028, 908, 16525, 17083, 2452, 36170, 11, 45216, 36170, 11, 50614], "temperature": 0.0, "avg_logprob": -0.18812068816154234, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.0005278797470964491}, {"id": 552, "seek": 479188, "start": 4797.88, "end": 4803.88, "text": " that have parameters that would map from data to beliefs, or sufficient statistics or beliefs,", "tokens": [50664, 300, 362, 9834, 300, 576, 4471, 490, 1412, 281, 13585, 11, 420, 11563, 12523, 420, 13585, 11, 50964], "temperature": 0.0, "avg_logprob": -0.18812068816154234, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.0005278797470964491}, {"id": 553, "seek": 479188, "start": 4804.88, "end": 4808.88, "text": " and then they learn that because they're experts, and they are more than well experts,", "tokens": [51014, 293, 550, 436, 1466, 300, 570, 436, 434, 8572, 11, 293, 436, 366, 544, 813, 731, 8572, 11, 51214], "temperature": 0.0, "avg_logprob": -0.18812068816154234, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.0005278797470964491}, {"id": 554, "seek": 479188, "start": 4809.88, "end": 4812.88, "text": " they are the experts in optimising the parameters.", "tokens": [51264, 436, 366, 264, 8572, 294, 5028, 3436, 264, 9834, 13, 51414], "temperature": 0.0, "avg_logprob": -0.18812068816154234, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.0005278797470964491}, {"id": 555, "seek": 479188, "start": 4813.88, "end": 4818.88, "text": " That's a slight problem because it denies any context sensitivity of the sort that we deal with the neuroscientists", "tokens": [51464, 663, 311, 257, 4036, 1154, 570, 309, 1441, 530, 604, 4319, 19392, 295, 264, 1333, 300, 321, 2028, 365, 264, 28813, 5412, 1751, 51714], "temperature": 0.0, "avg_logprob": -0.18812068816154234, "compression_ratio": 1.7848101265822784, "no_speech_prob": 0.0005278797470964491}, {"id": 556, "seek": 481888, "start": 4818.88, "end": 4820.88, "text": " and I deal with them in my simulations.", "tokens": [50364, 293, 286, 2028, 365, 552, 294, 452, 35138, 13, 50464], "temperature": 0.0, "avg_logprob": -0.14533711433410645, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.0021218168549239635}, {"id": 557, "seek": 481888, "start": 4821.88, "end": 4825.88, "text": " I think once they get beyond amortising their deep networks, then they'll be in this game,", "tokens": [50514, 286, 519, 1564, 436, 483, 4399, 669, 477, 3436, 641, 2452, 9590, 11, 550, 436, 603, 312, 294, 341, 1216, 11, 50714], "temperature": 0.0, "avg_logprob": -0.14533711433410645, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.0021218168549239635}, {"id": 558, "seek": 481888, "start": 4826.88, "end": 4831.88, "text": " and then we'll find out whether it scales, you know, that you'll need lots of computer scientists, big computers.", "tokens": [50764, 293, 550, 321, 603, 915, 484, 1968, 309, 17408, 11, 291, 458, 11, 300, 291, 603, 643, 3195, 295, 3820, 7708, 11, 955, 10807, 13, 51014], "temperature": 0.0, "avg_logprob": -0.14533711433410645, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.0021218168549239635}, {"id": 559, "seek": 481888, "start": 4832.88, "end": 4835.88, "text": " So I would imagine the next five to ten years this style of approach,", "tokens": [51064, 407, 286, 576, 3811, 264, 958, 1732, 281, 2064, 924, 341, 3758, 295, 3109, 11, 51214], "temperature": 0.0, "avg_logprob": -0.14533711433410645, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.0021218168549239635}, {"id": 560, "seek": 481888, "start": 4836.88, "end": 4843.88, "text": " and so the very actual free energy formulation will become increasingly dominant in people doing artificial intelligence.", "tokens": [51264, 293, 370, 264, 588, 3539, 1737, 2281, 37642, 486, 1813, 12980, 15657, 294, 561, 884, 11677, 7599, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14533711433410645, "compression_ratio": 1.6329588014981273, "no_speech_prob": 0.0021218168549239635}, {"id": 561, "seek": 484388, "start": 4843.88, "end": 4850.88, "text": " And I should quip, I mean, for some people, the new AI is actually active inference.", "tokens": [50364, 400, 286, 820, 421, 647, 11, 286, 914, 11, 337, 512, 561, 11, 264, 777, 7318, 307, 767, 4967, 38253, 13, 50714], "temperature": 0.0, "avg_logprob": -0.21530123225978162, "compression_ratio": 1.4024390243902438, "no_speech_prob": 0.0060128564946353436}, {"id": 562, "seek": 484388, "start": 4851.88, "end": 4860.88, "text": " It wasn't a coincidence that we chose that sort of rhetoric to promote it.", "tokens": [50764, 467, 2067, 380, 257, 22137, 300, 321, 5111, 300, 1333, 295, 29604, 281, 9773, 309, 13, 51214], "temperature": 0.0, "avg_logprob": -0.21530123225978162, "compression_ratio": 1.4024390243902438, "no_speech_prob": 0.0060128564946353436}, {"id": 563, "seek": 484388, "start": 4861.88, "end": 4864.88, "text": " Yes, so maybe I could just have a comment rather than a question then.", "tokens": [51264, 1079, 11, 370, 1310, 286, 727, 445, 362, 257, 2871, 2831, 813, 257, 1168, 550, 13, 51414], "temperature": 0.0, "avg_logprob": -0.21530123225978162, "compression_ratio": 1.4024390243902438, "no_speech_prob": 0.0060128564946353436}, {"id": 564, "seek": 486488, "start": 4865.88, "end": 4866.88, "text": " Okay, okay.", "tokens": [50414, 1033, 11, 1392, 13, 50464], "temperature": 0.0, "avg_logprob": -0.11008852178400214, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.04326832294464111}, {"id": 565, "seek": 486488, "start": 4867.88, "end": 4872.88, "text": " So that's a bit straight-laced approach to free energy reduction.", "tokens": [50514, 407, 300, 311, 257, 857, 2997, 12, 75, 3839, 3109, 281, 1737, 2281, 11004, 13, 50764], "temperature": 0.0, "avg_logprob": -0.11008852178400214, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.04326832294464111}, {"id": 566, "seek": 486488, "start": 4873.88, "end": 4878.88, "text": " It's based on surprise aversion, but I'm sure you have another talk on surprise-seeking,", "tokens": [50814, 467, 311, 2361, 322, 6365, 257, 29153, 11, 457, 286, 478, 988, 291, 362, 1071, 751, 322, 6365, 12, 405, 38437, 11, 51064], "temperature": 0.0, "avg_logprob": -0.11008852178400214, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.04326832294464111}, {"id": 567, "seek": 486488, "start": 4879.88, "end": 4882.88, "text": " curiosity, creativity, playful managerial styles,", "tokens": [51114, 18769, 11, 12915, 11, 30730, 6598, 831, 13273, 11, 51264], "temperature": 0.0, "avg_logprob": -0.11008852178400214, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.04326832294464111}, {"id": 568, "seek": 486488, "start": 4883.88, "end": 4890.88, "text": " which must have some adaptive purpose beyond our interest in horror movies and jokes.", "tokens": [51314, 597, 1633, 362, 512, 27912, 4334, 4399, 527, 1179, 294, 11501, 6233, 293, 14439, 13, 51664], "temperature": 0.0, "avg_logprob": -0.11008852178400214, "compression_ratio": 1.4660194174757282, "no_speech_prob": 0.04326832294464111}, {"id": 569, "seek": 489088, "start": 4890.88, "end": 4897.88, "text": " It must help us get out of dead-end problem space to problem spaces we can't even imagine.", "tokens": [50364, 467, 1633, 854, 505, 483, 484, 295, 3116, 12, 521, 1154, 1901, 281, 1154, 7673, 321, 393, 380, 754, 3811, 13, 50714], "temperature": 0.0, "avg_logprob": -0.10222336602589441, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.0032317570876330137}, {"id": 570, "seek": 489088, "start": 4898.88, "end": 4906.88, "text": " So the comment is, if you add surprise-seeking to free energy minimisation, I don't believe it remains tractable.", "tokens": [50764, 407, 264, 2871, 307, 11, 498, 291, 909, 6365, 12, 405, 38437, 281, 1737, 2281, 4464, 7623, 11, 286, 500, 380, 1697, 309, 7023, 24207, 712, 13, 51164], "temperature": 0.0, "avg_logprob": -0.10222336602589441, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.0032317570876330137}, {"id": 571, "seek": 489088, "start": 4907.88, "end": 4908.88, "text": " So I'll leave it at that.", "tokens": [51214, 407, 286, 603, 1856, 309, 412, 300, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10222336602589441, "compression_ratio": 1.4465408805031446, "no_speech_prob": 0.0032317570876330137}], "language": "cy"}