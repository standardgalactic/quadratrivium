WEBVTT

00:00.000 --> 00:11.640
Llywbeth hon am wneud ym mwyn sicrhau hynny ac mae'r ydych混wyl sy'n cynnig mewn gw'r dym такую rhoi riannol?

00:12.760 --> 00:22.460
Mewn gwneud o'c flyny y plwydio yn rhai ddiddio'r cyfnodau, Beth lle mae am gwaeth hyn a chi véliwr ja Prud Follow gwelwch.

00:22.780 --> 00:28.640
Dolwedd yn geist startoeth오еч arall yn i ballrain arall, lle arall yn gweithio'r Llywodraeth.

00:28.640 --> 00:45.620
rhoi

00:45.620 --> 01:11.740
回來 i'w gyddi'w gwybodaeth honom i

01:11.740 --> 01:15.260
... Almighty Mike, tewi ydy gyrygau…

01:15.400 --> 01:17.520
… dyma ar-dysgu!

01:18.060 --> 01:22.000
Fy enw i, i���ch cicitYEalsoch…

01:22.900 --> 01:25.660
… yma amary teuluw...

01:25.800 --> 01:29.600
… i'r ffordd ydych Lady Worker...

01:29.600 --> 01:34.100
… Trunwau ystafelu'n bob gynyddio...

01:34.220 --> 01:38.740
… Mae'n eu clynyddu i'r ddelig, yn ddefnyddioSeePlayer...

01:38.920 --> 01:41.500
… Neil yma am ar-zell, amaeddrannu...

01:41.500 --> 01:47.420
Ac rydw i fyny mis o bwyfia'r amser,

01:47.420 --> 01:52.760
nad maen nhw gwybod gweithre vaccinations,

01:52.760 --> 01:55.100
rydw i'n amser ar gwell feedêctig Sa怎么样au?

01:55.100 --> 02:02.440
Rydw i'n mir enghreithio y cael meddwl ei m следdi habiloghau ar resideộld,

02:02.440 --> 02:08.200
ph番 dod yn gynghreithi a wyloach amoduslu ar draw.

02:08.200 --> 02:10.200
i'w ddweud i'w ddweud i'w ddweud.

02:38.200 --> 03:07.200
So, this is an overview of what we are going to go through. I am going to introduce the notion that you are the free energy principle, but from a, using a slightly heuristic approach in terms of action and the path of least resistance,

03:07.200 --> 03:16.200
highlighting the importance of having internal models or hypotheses that enable us to generate predictions, talking about active inference.

03:16.200 --> 03:32.200
And one key thing that I am going to focus on is translating the theory into a process theory that can be used to understand neuronal message passing in the brain and help us exactly constrain the sorts of experiments that Jim was talking about.

03:32.200 --> 03:49.200
So, that is going to be a big part of what I hope that we will be talking about, taking normative principles and seeing how they unpack in the service of understanding empirical measurements anatomy and physiology and how they can be used to nuance experimental design,

03:50.200 --> 03:57.200
showing the sorts of things that one can simulate and speaking to some empirical predictions of these sorts of schemes.

03:57.200 --> 04:08.200
And I have put, as an epilogue, more recent work, simulations of reading that introduce hierarchies into the particular forms of generative models that I want to survey for you.

04:08.200 --> 04:14.200
We won't have time to go without that, but I just want to show you the slides in case of something that catches your attention.

04:14.200 --> 04:23.200
So, I'm going to start with a question. Let's assume you're hungry, and let's assume you're an owl. So, what are you going to do?

04:26.200 --> 04:30.200
You're going to search for a mouse? And how are you going to do that?

04:32.200 --> 04:35.200
Don't cheat. You have to look at me, not at me.

04:37.200 --> 04:39.200
Absolutely. Perfect answer.

04:39.200 --> 04:46.200
So, in terms of optimal behaviour, the first thing you do is search. You scan.

04:46.200 --> 04:54.200
You confront the epistemics of reducing uncertainty about what you need to do in order to fulfil your goal.

04:54.200 --> 05:00.200
So, it's all about beliefs. So, in that answer is the basis of everything that I'm going to say.

05:00.200 --> 05:07.200
Your behaviour is always driven by beliefs, and that tells us something quite important.

05:07.200 --> 05:14.200
So, here's you scanning and searching, and you've found a little mouse that you might want to eat there.

05:14.200 --> 05:23.200
That's quite important because it speaks to two basic classes of ways of thinking about optimising behaviour.

05:23.200 --> 05:32.200
You can either imagine that there is some value function of the next state that will be brought about by some action,

05:32.200 --> 05:39.200
and optimise that action by selecting the action that maximises the value of the next state.

05:39.200 --> 05:49.200
That's the classical way of doing it, but that just doesn't work if the best next thing to do is to search and resolve uncertainty.

05:49.200 --> 05:52.200
Because uncertainty is an attribute of beliefs.

05:52.200 --> 06:03.200
Therefore, the function of a function that you need to optimise in terms of action you hear is a function of beliefs,

06:03.200 --> 06:07.200
which I'm deleting by Q, beliefs about the states of the world.

06:07.200 --> 06:17.200
That introduces a fundamental distinction between the sorts of schemes that you bring to bear in terms of understanding optimal behaviour.

06:17.200 --> 06:29.200
The other thing about the scanning and searching answer is that action depends upon beliefs about the world, states of the world, and subsequent actions.

06:29.200 --> 06:36.200
So, not only is it a function of beliefs about the world, but it's the order in which you interrogate that world.

06:36.200 --> 06:42.200
So, it makes a difference whether you search, then eat, as opposed to eat, then search.

06:42.200 --> 06:49.200
That means that we are in the game of optimising sequences or policies or actions.

06:49.200 --> 06:51.200
I'm going to call it a sequence of actions policy.

06:51.200 --> 07:04.200
So, what that means from the point of view technically of what sort of thing we have to optimise, it's a functional of a belief integrated over time, or summed over time, a path integral.

07:04.200 --> 07:09.200
If we call that an energy, then the integral, the path integral of an energy is called an action.

07:09.200 --> 07:23.200
So, what we've just said is that we've reduced the problem of good behaviour to Hamilton's principle of least action, where action is the path integral or the trajectory integral or the sum over an energy functional of beliefs.

07:23.200 --> 07:28.200
And that's the basic premise that I'm going to pursue.

07:29.200 --> 07:42.200
Just to highlight the distinction, if you subscribe to this way of thinking about how systems work, then you end up with optimal control theory, Bayesian decision theory, reinforcement learning and all that good stuff.

07:42.200 --> 07:54.200
Conversely, if you believe this is how biological systems work, then you end up essentially with Hamilton's principle of least action, the free energy principle, active inference, active learning and so on.

07:54.200 --> 07:56.200
And that's what we're going to focus on.

07:56.200 --> 08:09.200
And the energy function that I'm going to consider, we've already heard mentioned, is the variational free energy or the free energy, which we've already heard very roughly scores surprise.

08:09.200 --> 08:15.200
It approximates surprise or suprisal and is simplifying assumptions prediction error.

08:15.200 --> 08:24.200
So, what we are saying is that we're just in the game of minimising prediction error and more specifically prediction error over time over sequences of behaviour.

08:24.200 --> 08:33.200
I'm going to quickly go through this because there are lots of interesting connections with existing theories and formulations.

08:33.200 --> 08:41.200
This is a bit technical. These are both iconic and ironic equations. You'll hear more about those later on.

08:42.200 --> 09:00.200
In words, if it's the case that good agents, good people, minimise their free energy, their surprise, their average surprise and their uncertainty, then they must believe that the actions that they emit will minimise expected free energy.

09:00.200 --> 09:14.200
You can write that down very simply in terms of these belief functions here and rearrange them in a way that discloses important links with lots of established formal treatments of behaviour.

09:14.200 --> 09:25.200
I've written the expected free energy associated with any particular policy in terms of its expinsic value here and its epistemic value.

09:25.200 --> 09:34.200
Basically, these things store the surprise about what you predict will happen under a particular behaviour and what you think should happen.

09:34.200 --> 09:38.200
Your preference is like, I'm going to eat a mouse and I'm not going to be hungry.

09:38.200 --> 09:42.200
That's a surprise bit, explicit or expinsic surprise bit.

09:42.200 --> 09:48.200
There's another sort of average surprise or relative entropy which is called epistemic value.

09:48.200 --> 09:53.200
It's a reduction in uncertainty or the information gain, and that's the key bit.

09:53.200 --> 10:02.200
It's the epistemic which is missing from classic theories but is part of this formulation of Hamilton's principle of least action.

10:02.200 --> 10:09.200
That relates very closely to theories of visual salience, of Bayesian surprise.

10:09.200 --> 10:20.200
Technically, Bayesian surprise is the divergence of the difference between a prior belief and a posterior belief or a posterior belief to be informed by observations here.

10:20.200 --> 10:39.200
What we're saying is that we will choose to act in a way that reduces our uncertainty relative to prior beliefs, looking at data which gives us information that maximally reduces that uncertainty that has the greatest epistemic value or Bayesian surprise.

10:39.200 --> 10:51.200
In fact, mathematically, that's exactly the same as the mutual information between the causes, the hidden states of the world S and the consequences, the outcomes that we actually observe.

10:51.200 --> 11:03.200
So another way of saying this is that we are subscribing to the principle of maximum information, mutual information or minimum redundancy or maximum information efficiency of the sort articulated by Horace Barlow.

11:03.200 --> 11:11.200
Always of expressing one particular form of perspective on this underlying functional.

11:11.200 --> 11:23.200
Another way of thinking about this in the case, if there is no ambiguity, if we actually can observe the states directly, then we can discount this uncertainty term here.

11:23.200 --> 11:33.200
And what we're left with is something called KL control, which is the state of the art of what people would use in optimal control theory and dynamical systems control.

11:33.200 --> 11:43.200
In economics, it's called risk sensitive control. It's minimizing risk. So this is a surprise between what I think will happen and what I want to happen.

11:43.200 --> 11:53.200
And if what I think will happen is surprising relation to what I thought was going to happen, then I have a high degree of surprise, a high degree of risk, and I want to minimize that.

11:53.200 --> 12:08.200
And then finally, if there's no ambiguity or there's no risk, then we reduce to classical expected utility theory or all the sorts of theories that reinforcement depends upon this maximizing our preferred outcomes there.

12:08.200 --> 12:18.200
So, clearly, in order to be surprised, we have to have predictions against which we can match outcomes to score that surprise.

12:18.200 --> 12:35.200
And this brings us to generative models. And the departure that I promised you from what people currently understand in terms of predictive coding and what I'm going to talk about for the next few minutes is I'm going to formulate generative models not for continuous state space

12:35.200 --> 12:47.200
of the sorts used in predictive coding of, say, visual angles or content or acoustics, but generative models in which we can label the entire world in terms of a number of discrete states.

12:47.200 --> 12:56.200
So these are generative models for discrete state spaces, and they don't normally have the look and feel of predictive coding, but my story will be, is in fact they do.

12:56.200 --> 13:09.200
They are actually formally very, very similar to the sorts of schemes that we understand in terms of top-down predictions and bottom-up prediction errors in hierarchical message-passing allopredicative coding in the visual cortex.

13:10.200 --> 13:17.200
So in these models, all we have, this is not, ignore the equations, but it's focused on this graphical model here.

13:17.200 --> 13:31.200
What we're saying is that the world unfolds in one of many, many states, and the transitions from one state of the world to the next state of the world are encoded by probability transitions that themselves depend upon how we act.

13:31.200 --> 13:40.200
They depend upon the policies that we choose, and we have a certain confidence in those policies, denoted by their precision or inverse temperature beta here.

13:40.200 --> 13:57.200
So if we knew the probability transitions or the transitions from time to time of the states, we can generate a sequence or trajectory of states, and each state, at each point in time, generates an outcome through this likelihood of matrix A.

13:57.200 --> 14:07.200
And that's it. That's the generative model. The world has states, they unfold, and each state generates an outcome that's observable.

14:07.200 --> 14:12.200
And that's the basis of everything else that I'm going to say.

14:12.200 --> 14:33.200
If I'm now given a generative model, what I can do is I can evaluate the free energy of my beliefs under that generative model, and I can then minimize everything with respect to that proxy for surprise or uncertainty, namely the expected free energy.

14:33.200 --> 14:45.200
And I can write down equations or solutions that tell me how an optimal agent person would behave in a sort of Bayesian sense.

14:45.200 --> 14:50.200
And these are the solutions to the equations expressed in terms of the parameters of that model.

14:50.200 --> 14:58.200
So A was this mapping from states of the world to outcomes, and B was the mapping between subsequent hidden states.

14:58.200 --> 15:07.200
And despite the complicated nature of the equations on the previous slide, the actual updates, the solutions are incredibly simple.

15:07.200 --> 15:12.200
And furthermore, they look very much like the sorts of things that the brain does.

15:12.200 --> 15:23.200
So, for example, expected states of the world are a nonlinear sigmoid function of linear mixtures of expected states of the world and observations.

15:23.200 --> 15:32.200
So we're mixing together evidence from outcomes and our beliefs about the states of the world to update our beliefs about the current state of the world.

15:32.200 --> 15:49.200
Our beliefs about what we're going to do next, our policy pie here, is this a softmax function of the expected free energy weighted by an inverse temperature parameter that you will see we associate with dopamine, a classical softmax response rule.

15:50.200 --> 15:58.200
If you're not familiar with that, that's what people in economics and choice behaviour use for those people who deal more with perception.

15:58.200 --> 16:14.200
We also have a model of incentive salience. The confidence or the precision or the inverse temperature associated with our beliefs about action now becomes, as a Bayes optimal solution, that depends upon the goodness of a policy or the negative goodness, the expected free energy here.

16:14.200 --> 16:23.200
And the form of these equations speaks to a rough anatomy of computations in the brain, a computational anatomy.

16:23.200 --> 16:32.200
And it sort of goes like this, where we have these equations dictate what each update needs to know about the other updates.

16:32.200 --> 16:48.200
So, basically, it prescribes a connectome for the exchange of information or sufficient statistics that is implied by placing the Hamilton's principle of least action on the simplest sort of generating model that you can imagine.

16:48.200 --> 17:02.200
And that's the sort of anatomy we have here. Outcomes, expected states, expected policies, the goodness or the expected free energy of policies, the precision of policies, states in the future, which prescribe action.

17:02.200 --> 17:08.200
So, I won't go through that, but I'll just give you a more heuristic version of that one. So, what those equations tell us.

17:08.200 --> 17:15.200
So, this is like a very top-down argument. It's not, you know, let's think about how the brain works and come up with some hypotheses.

17:15.200 --> 17:24.200
This unfolds or unravels from, impacts from, just applying Hamilton's principle of least action to a very simple generating model.

17:24.200 --> 17:36.200
And what it tells us is that sensory input comes in, say, at the back of the brain. It informs and updates expectations about hidden states of the world, sometimes referred to as state estimation.

17:36.200 --> 17:48.200
They are associated with a free energy or a surprise that is combined with an evaluation of those states in relation to prior preferences and their potential reduction of uncertainty, their epistemic value.

17:48.200 --> 17:58.200
They are combined to give us beliefs about the policy that we are currently pursuing. We have a certain confidence in that policy.

17:58.200 --> 18:08.200
And then those policies are used to weight all the different states conditioned upon what we are currently doing to give us the best estimate of what's going to happen next, the next state of the world.

18:08.200 --> 18:22.200
And if we know that, then we can choose the action that brings about, that realises our expectations, our predictions about the next state of the world, that action solicits a new observation from the environment and the cycle begins again.

18:22.200 --> 18:29.200
So, we have a perception action cycle that falls out of the minimisation scheme that we've just been talking about.

18:29.200 --> 18:40.200
So, very briefly, I'm just going to show you how that sort of thing works with a series of examples, and then hopefully I'll turn it over to you to see what you want to talk about.

18:40.200 --> 18:54.200
The first example is just a very simple simulation of foraging in a two-arm maze. So, in this example, there's a little rat here, and there are rewards on the right and the left arms of the maze, but the rat doesn't know where the reward is.

18:54.200 --> 19:04.200
There's also an informative queue at the bottom of the maze here, and if it went to solicit that queue, it would then know where the reward was, and it could only make two moves.

19:04.200 --> 19:22.200
So, it can either take a chance and go to one of the other top arms, or it can be a bit more clever and resolve any uncertainty about the context it's currently operating in, which arm is baited, and go and retrieve the epistemic value of the informative queue and then make an informed decision.

19:22.200 --> 19:36.200
So, this is exactly the searching that you were talking about before, scaling your environment, knowing where you are, resolve your epistemic, solve the epistemic problem, and then turn to your prior preferences or your pragmatics.

19:36.200 --> 19:59.200
You can write this model down in very simple terms of these A and B matrices here. There's partial reinforcement here, and the C matrix here just denotes the preferences in terms of what sorts of states this rat thinks it should occupy, basically thinks it should be in the baited arm and not in the unbaited arm.

19:59.200 --> 20:25.200
That's all it's saying here, with minus threes and plus threes on the upper arms that are baited. And if we do that, and we just integrate those solutions that I told you before, we actually generate very realistic behaviour, summarised here in terms of the expected policy and the policies that this agent or this little animal can entertain.

20:25.200 --> 20:43.200
It stays there and then goes to one of the three arms, or it goes to one of the two arms, or it goes to the bottom and then goes to any of the three arms. So there are eight policies here.

20:43.200 --> 21:00.200
And what it does in the first instance is because it doesn't know where the reward is. It gets the cue and then obtains its reward. What we've done here is actually baited the left arm all the time.

21:00.200 --> 21:18.200
So slowly it accumulates evidence that, in fact, the reward's always on this side here. So as time goes on, it actually switches and learns, and it's probably better to avoid or dispense with the epistemic move and go directly to the reward.

21:18.200 --> 21:37.200
And it starts doing that after about 20 or 30 trials here, at which point its reaction times, and this is the actual floating point operations of the scheme, decrease. And because the goodness of a policy is this path integral, it's actually spent more time being rewarded.

21:37.200 --> 21:51.200
So if you like, the payoff also increases by going straight there. So this prescribes good policies, and it can be used to simulate nice behaviors of the sort you've seen experimentally.

21:51.200 --> 22:09.200
But what I want to do finally is just connect that to neurophysiology and neuroanatomy. But to do that, I have to have a process theory. I have to have a theory which says this particular neuroactivity or this particular connection strength corresponds to this quantity in the model.

22:09.200 --> 22:34.200
And I have to have a process in play that is neurarily plausible. And the way that we're going to do that is just take those update equations that we've seen before, and instead of just writing down the solutions mathematically, I'm going to recast the solutions in terms of a gradient descent or a hill climbing, or actually a hill descent here.

22:34.200 --> 22:50.200
So this is a standard way of optimizing something. If you've got a quantity you want to minimize, you just go downhill until it stops getting smaller. And if I do that, I can write down exactly the same scheme in terms of differential equations on expected states of the world.

22:50.200 --> 23:10.200
It's very similar form, but here that's a rate of change of activity, which is now a nonlinear function of linear mixtures of expectations about states of the world and the observations. And in doing that, I've created a dynamical system that now has as much closer to the look and feel of a neuronal system.

23:11.200 --> 23:29.200
And that now enables me to look at the dynamics that underlie the behavior. And these are the dynamics here, and we can basically break these into inference and state estimation in terms of the updates or the fluctuations in the states as new evidence comes along.

23:29.200 --> 23:45.200
Policy selection that we've already seen with our softmax response rule, and learning as we accumulate from trial to trial evidence about particular states or contingents of the world in this instance that the left hand arm of the maze was always baited.

23:46.200 --> 23:58.200
I illustrated those things here, a couple of interesting things to note. First of all, with every new move and every bit of new sensory information, there are lots of fluctuations in these states that look very much like an ERP.

23:58.200 --> 24:19.200
Furthermore, when we become a little bit more automatic or not habitual, but certainly going straight for our reward, there is an attenuation of these responses. The confidence, the precision in those responses also shows these phasic changes and progressive changes as we learn the context.

24:19.200 --> 24:30.200
So we actually get something which looks remarkably similar to transfer of dopamine responses as we become more familiar and more confident about the outcomes that we see.

24:30.200 --> 24:45.200
Let me just quickly show you a couple of those outcomes. This slide highlights just one trial, and it shows the representations of time over the different hidden states of the world.

24:46.200 --> 25:05.200
Just highlights a couple of things. First of all, it shows that as we accumulate evidence for our preferred policies or our preferred outcomes, the probability that we are in a state which we will ultimately choose increases whereas the probability of states that we don't decreases.

25:06.200 --> 25:19.200
This is formally identical to evidence accumulation or drift diffusion models, but now a consequence of a gradient descent on variational free energy or a bound for surprise.

25:19.200 --> 25:37.200
What we also see is an interesting dynamics in the sense that if information keeps coming in every, say, 250 milliseconds, like the frequency at which we go and sample the world with mechanic eye movements, that means that we have two timescales in play.

25:37.200 --> 25:54.200
One is a theta rhythm as we go and get information once, say, four times every second. But within each sampling there's this fast updating that's minimising and optimising our beliefs, and that faster updating has a temporal scale in the gamma range.

25:54.200 --> 26:11.200
So what we see is effectively, as we move along, fast updating that repeats itself every theta cycle, but as we accumulate more and more evidence we get more and more efficient and confident about the things that we are inferring.

26:12.200 --> 26:20.200
The dynamics mean that they accumulate evidence more quickly, more efficiently, and we get a phase procession of the sort seen in the hippocampus.

26:21.200 --> 26:36.200
I've already mentioned that as time goes on, by virtue of increasing our confidence as we assimilate this evidence, then that confidence is expressed in the confidence of our policies and we have a nice way of assimilating dopamine responses.

26:37.200 --> 26:53.200
We can look at the behaviour or the activity of these representations of different states of the world at different points in time during our policy, and if we plot their responses as a function of where the rat actually is, we can simulate place cell activity.

26:54.200 --> 27:11.200
There has many characteristics of the sort seen empirically. This just illustrates this theta-gamma coupling, which is an almost necessary consequence of this sort of solitary sampling of the world, and then updating bleeds quickly before the next sample comes along.

27:11.200 --> 27:23.200
Again, the sort of thing that one sees empirically. We can now do violation responses exactly as Jim was talking about. What I've shown here are the responses to two trials.

27:24.200 --> 27:35.200
They're identical in nature, but one is from the beginning of the trial where the rat was not familiar with its environment, and one is at the end of the trial where it becomes very familiar just before it starts going directly for the reward.

27:36.200 --> 27:58.200
Interesting, if we look at the representations of key states here, what we see is that they are much more efficient and therefore less exuberant updating of expectations of hidden states that if we subtract the standard familiar one from the odd ball or the unfamiliar one, we reproduce the temporal dynamics of things like the mismatch negativity in ERP research.

27:59.200 --> 28:12.200
We also demonstrate this transfer of confidence or simulated dopamine responses from the rewarded cue per se to this instructional condition stimulus here.

28:12.200 --> 28:33.200
I'm going from slightly negative to positive here. We can play similar games by introducing deliberate violations and illicit P300 responses. We can look at reinforcement learning by switching contingencies halfway through and look at the effects on dopamine-urgent responses and also electrophysiological responses.

28:36.200 --> 28:37.200
How long have I got?

28:37.200 --> 29:01.200
Oh, that's very good, isn't it? I've only been talking for 25 minutes. I can be true to my promise to finish in half an hour. This is the epilogue. That's the story so far. Most of that will be in the next few weeks in the published literature.

29:02.200 --> 29:15.200
You'll notice at the moment there's nothing really about hierarchies. Most people here, I'm sure, are more interested in the implications of this sort of theory for perceptual hierarchies and evidence of accumulation and purely perceptual domain.

29:15.200 --> 29:44.200
The more recent work that I wanted to, this is not published, to introduce you to, is now taking this formalism, which has a lot of constant validity in relation to choice behaviour and your economics, active vision, active sensing, and see what it has to say about the source of themes we're more interested in, which is the hierarchical message passing and the deep generative models that we assume.

29:45.200 --> 29:49.200
The brain is using to actually understand perceptual sequences, say.

29:51.200 --> 30:08.200
So this is the epilogue. Again, I'll just speed through this in five minutes. What we're going to do now is tell exactly the same story, but now we're going to put one of those discrete state-space models, they're known as Markov decision processes, on top of the first one and another one on top of that and another one on top of that.

30:09.200 --> 30:20.200
So in this construction, hidden states, at any one level in the model, don't generate outcomes, they generate the first or the initial hidden state of the level below.

30:21.200 --> 30:33.200
And then that cycles over a few iterations and then terminates like the rat-terminated when it entered the baited arms of the cues.

30:34.200 --> 30:43.200
And that process repeats hierarchically to any arbitrary depth. So what we have are deep temporal generative models.

30:44.200 --> 31:08.200
And they're really interesting because not only do they have a hierarchical structure in their form, but also in their time, because if the state at any high level is generating the initial state that must have subsequent states, then it means that the lower states unfold more quickly than the higher states.

31:09.200 --> 31:24.200
So one way of thinking about this is the generative model says that at this hour, at this minute and at this second, I am safe, I was reading, I'm on this page, on this paragraph and on this word.

31:25.200 --> 31:44.200
So if you think about the lower levels of us ticking over more quickly, like the second hand of a clock, and every revolution or every trajectory or every path they take, then the high level goes forward one step, and then it goes round again, it goes another step, another gain, sorry, again and then another step.

31:45.200 --> 31:52.200
And then that process is repeated. So as the minute hand is going round, once it goes round, then the hour hand goes round.

31:52.200 --> 32:06.200
So what we have here is a generative model that basically has in mind, literally, beliefs about the world that are much more protracted in time and are hierarchically nested.

32:06.200 --> 32:17.200
So if you could invert this sort of model, you would have a representation of the context and the context of context and the context of context.

32:17.200 --> 32:22.200
At each point, as you go deeper into the model, they are more temporally enduring.

32:22.200 --> 32:38.200
So you know that working from the top down, you'd know the story of the narrative, if you knew the story of the narrative, you'd be able to generate a particular sentence, if you could generate a particular sentence, if you could generate a particular word, if you could generate a particular word...

32:38.200 --> 32:42.200
.. you could generate a particular letter. All faster and faster and more elemental timescales.

32:42.200 --> 32:49.020
That's the sort of model now that people are starting to play with.

32:49.020 --> 32:52.860
It has exactly the same performance before— coping with policies in play

32:52.860 --> 32:56.220
that generate transitions among hidden states that generate outcomes,

32:56.220 --> 32:59.140
but now the first hidden state is generated

32:59.140 --> 33:05.400
by the same sort of model, formally identical, of a higher level.

33:05.400 --> 33:09.300
There are transitions over time here, but they are much slower.

33:09.300 --> 33:12.820
that by making these red lines upon red states here,

33:13.540 --> 33:15.700
a different colour from these because these, basically,

33:15.780 --> 33:17.900
are the same as these but they're re-used

33:17.980 --> 33:19.300
at a later point in time.

33:20.500 --> 33:24.500
And this sort of model now allows you to think more carefully

33:24.580 --> 33:26.700
about the hierarchical message passing

33:26.940 --> 33:28.580
and the implications of neuro-anatomy.

33:28.660 --> 33:30.700
So if we now turn straight to the process theory,

33:31.300 --> 33:33.780
these are the differential equations

33:34.860 --> 33:37.780
that fall out of that generatord model in the previous slide.

33:37.780 --> 33:42.340
ei wneud o unig lwydaeth pob ddechrau.

33:42.580 --> 33:47.260
Derbyn i ch risingdef o dystru i dystru i ddim yn cael ei ddif respir,

33:47.420 --> 33:52.660
egOC yn gyfrasch ychydig ar y totu org10-g جون inequality.

33:52.980 --> 33:57.580
A dwi'n credu hynny bir maen i ddim yn ddigonol gyda L rug

33:57.740 --> 34:03.340
yn misoedd f hybridd stag yn y gwroad тр sír.

34:03.580 --> 34:07.700
Til ond mythigr o'r ddim yn y ddigonor,

34:07.780 --> 34:12.060
a gan y doddoriaeth yn y tai nhw'r drefnio fan y plainachau.

34:12.860 --> 34:17.940
Diolch yn ddechrau am ddechrau coddyntau hynny a detch,

34:18.420 --> 34:27.020
ac rydw i addysgu'n chi fod rhywbeth rydyn ni'n ach yn perwhaith gweithio.

34:27.140 --> 34:33.860
Mae ydw mitant ar hufwn, felly dyma'n ddechrau ddiwrnod ni'n ddechrau

34:34.100 --> 34:40.520
Ond biggestonethol, er di particularly gw Snapod,

34:40.520 --> 34:48.760
yn ynghyd yn yr ymddangos lleethau hyn arall hynny,

34:48.760 --> 34:55.360
ddweud yma cwm declar o repliedol, ar m 게isirol arall y dyfod,

34:55.360 --> 35:02.760
Golw llawer arall llawer mewn amser gan y cwmputatio nanallynol, yкими yuyaeth

35:02.760 --> 35:08.560
yw Somehow carried a 84.5 Sauce Ndun, iawn i' fi bwyzaid eu droslawn fφm.

35:09.160 --> 35:12.760
I airportio pleid según fill.

35:12.760 --> 35:17.640
Er si GLORIA cyhoedd, Assembly Down yw dweud yn gyda hynny yng Nghymru a'i sefydlu'r

35:17.640 --> 35:22.960
rhywbeth gyda'r berth dweud, er mwyafMatthewch szeynerrym ynты grandiaf minzysolEl

35:22.960 --> 35:25.660
Mae wedi'i converti analw!!!!!!

35:25.660 --> 35:28.640
A 되fnodd ar yr adegowadau mewn gindig adegowadau

35:28.640 --> 35:31.060
that atbypaenter, a gennych amherei

35:31.060 --> 35:33.040
A ddau roi gwodd!.

35:33.040 --> 35:35.040
Fmarfin analw Ring

35:35.040 --> 35:37.260
ac mewn gwir y cитеch,

35:37.260 --> 35:39.160
dw i ddim yn gwneud o wnes cy hunain

35:39.160 --> 35:42.360
Dasodd yr adegowadau i gwenallu

35:42.360 --> 35:44.420
cais grathio adegowadau

35:44.420 --> 35:46.720
Maeлюч ychydig iddo nhw a 선urdwy

35:46.720 --> 35:49.260
Rydyn ni gadael

35:49.260 --> 35:51.600
sy'n fioedd y gwaith

35:51.600 --> 36:00.300
ma wedi ar hyn o gwbl panallu, a hyffordda.

36:00.380 --> 36:12.160
Mae'n gwneud ond yn gennaledd, wedi a cofdown o'i gael niferion,

36:12.160 --> 36:17.260
llaoi i amιο overseas ychydig wedi ai'r hyn am misgfemi

36:17.340 --> 36:32.920
seisin ag llwy

36:32.960 --> 36:39.120
na'ch wneud ar hyn o hyd llwyddi, i hwnna, gael dd sel m sorcera

36:39.120 --> 36:42.660
iawn i'r Fyged growth Yw'r Oorferd Folaid theoriaeth mewn cyd y fry Jaime

36:42.660 --> 36:47.320
Meddor C ar y rueth Cenaiswn ni'n samp lun prosg uppu pho t Momo

36:47.320 --> 36:50.960
astad i'n ein bachMyw yma ar gyfnodd Rhaid Fygedr

36:50.960 --> 36:54.080
Diolch ar gyfer hwn i'r Ro Virus Fog толwy...

36:54.080 --> 36:58.260
...ynghyd gy ved Cynyd rel Fygedr honw i'r fnod i'n tufyniadhe iddopeth yma y gweithiau

36:58.260 --> 37:01.620
neu yn nar真 Curioroedd ar gyfer rhowch â boffraedd y pwyntill

37:01.620 --> 37:05.880
ddoseud o effaith yi'r Llyfr yma rydym hanfodd d communism

37:05.880 --> 37:14.200
dweud yn ddeud democrat. Rydyn ni'n ei chylygu diwethaf y gallwllais liquids mae yw'r eff cheatuppau pleannam ar y liar mewn ilmenau

37:14.760 --> 37:35.580
Mae nid yn ein cael cyn deafnaetennu am y chylynadau fossemwjent hynny ac mae'n!]

37:35.880 --> 37:50.880
If you subscribe to this scheme anatomically and the theory being a metaphor for neuronal activity in the brain, what you'd end up claiming is that the goodness of a policy, is negative expected for energy, is encoded in the

37:50.880 --> 37:59.880
is encoded in the call date for high-level, more abstract representations in the deep-generative model,

37:59.880 --> 38:08.880
for more intermediate levels, sorry, the call date for intermediate levels for more abstract ones in the globus pallidum,

38:09.880 --> 38:24.880
and in the putamen for motor loops, whereas the policy expectations per se have here been assigned again to the globus pallidus internum.

38:24.880 --> 38:37.880
So just a way of getting from the mathematical anatomy to the biological or neuroanatomy in a purely top-down dispassionate mathematically dry way,

38:37.880 --> 38:45.880
is taking the equations and seeing what form do they imply for message-passing and what sorts of message-passing do we see in the real brain.

38:45.880 --> 38:54.880
Andre Bastos will, I think he may not, but he did a lot of work on this sort of intrinsic connectivity within a macro-column,

38:54.880 --> 39:02.880
clinical micro-circuits for predictive coding, exactly the same game can be played here for this discrete state-space model,

39:02.880 --> 39:13.880
and that's one key exception which Lars might like, because there's been a lot of debate in Scotland about whether the superficial parameter cells encode predictions

39:13.880 --> 39:26.880
or expectations or prediction errors. In this scheme, in this discrete state-space scheme, the superficial parameter cells code expectations, not prediction errors,

39:26.880 --> 39:33.880
and you may ask why. Well, the reason is, they do encode prediction errors, but they do it by physically in a very different way,

39:33.880 --> 39:43.880
and that follows from the form of this differential equation here, where we're expressing the states as a sigmoid function, a softmax operator,

39:43.880 --> 39:50.880
on V, which we associate with depolarisation, where the rate of change of depolarisation or voltage is proportional to the error.

39:50.880 --> 39:59.880
So, the error from the point of view of a neural mass model now becomes the conductance. So, the cells are encoding prediction error, but the error is in the conductance.

39:59.880 --> 40:10.880
So, when all the postsynaptic drives to the conductance postsynaptically are in balance and there is no further change or drive to the potential,

40:10.880 --> 40:20.880
that means prediction errors are zero. So, when the cell or a population has reached electrodynamically steady state, it's found its minimum prediction error,

40:20.880 --> 40:32.880
and then it fires, but it is the firing here that is associated with the expected states of the world here.

40:32.880 --> 40:39.880
So, that's an interesting thing which I thought might be useful for discussion in terms of reconciling a lot of paradoxes about what's been passed forward,

40:39.880 --> 40:47.880
and would you expect it to be expended away or would you expect it to be boosted, sharpened, all sorts of interesting issues here.

40:47.880 --> 40:56.880
Closing with an example of reading, a deep hierarchical model where we have beliefs about six sentences, each comprising four words.

40:56.880 --> 41:08.880
Each word here has iconic letters that can either be in an uppercase or a lowercase, a palindromic in the sense that it doesn't matter whether the cat has to flee

41:08.880 --> 41:20.880
from the cat, it doesn't matter whether we flip them in a horizontal way, it still means the same thing, but it does, this agent is surprised if we use a lowercase.

41:20.880 --> 41:21.880
Three words.

41:21.880 --> 41:38.880
Let me just skip through this because we want to spend more time in discussion if we can.

41:38.880 --> 41:50.880
So, that's just a generative model with two levels, semantics or sentence structure, word structure and outcomes generating particular, if you like, letters, but there are icons in this instance.

41:50.880 --> 41:54.880
And then with this scheme, we can simulate things like reading.

41:54.880 --> 42:07.880
So, here's a little four page story or sentence and that word is flee, that word is wait because there's nothing next to the bird, that word is feed because there are seeds that the bird can feed on, and that is wait.

42:07.880 --> 42:15.880
So, this is a sentence, flee, wait, feed, wait, and that's a happy sentence and it will categorise it as happy.

42:15.880 --> 42:21.880
But the problem that we're trying to address here is exactly what we started with.

42:21.880 --> 42:29.880
How do you scan? How do you search? Where do you go forage for information to resolve as much uncertainty as you can about which of these six sentences is in play?

42:29.880 --> 42:43.880
And when the system does this just by trying to minimise its expected free energy, it shows this very interesting behaviour where it jumps from one word or page to the next

42:43.880 --> 42:49.880
without really dwelling and wasting time resolving uncertainty that is already resolved.

42:49.880 --> 42:59.880
So, once it sees a cat, it already knows that this has to be a flea word and it doesn't need to see where the other letters in this word are actually doing.

42:59.880 --> 43:04.880
It already knows, there's no more epistemic value to be had, there's no more uncertainty to resolve.

43:04.880 --> 43:11.880
It'll now jump to the next page and resolves uncertainty after a couple of Sokelechi movements, then jump to the next page.

43:11.880 --> 43:18.880
And after this once a card in the final page, it knows exactly what this sentence was doing.

43:18.880 --> 43:30.880
And if I can, I'll just show a movie of it doing that.

43:30.880 --> 43:42.880
So, the red dots correspond to where it's looking at the present time and the images that are mixtures of the icons represent conditional expectations.

43:42.880 --> 43:48.880
And the main point to be taken from this is that it knows there's a bird there, but it never looked there.

43:48.880 --> 43:52.880
It has sufficient prior knowledge in its deep, depth temporal model.

43:52.880 --> 43:59.880
It doesn't need to actually go and see stuff. It knows stuff is there because it knows what caused that stuff.

43:59.880 --> 44:07.880
And with this sort of simulation, one can then do exactly what Jim was talking about, which was if it knows stuff and it has predictions,

44:07.880 --> 44:15.880
then it should be possible to disclose or reveal that knowledge, that predictability by introducing violations

44:15.880 --> 44:20.880
and elicit the sorts of classical responses that we see empirically.

44:20.880 --> 44:25.880
And what we've done here is because we've got a deep model, we can do local and global violations.

44:25.880 --> 44:32.880
We can make the final story, the final sentence, a very surprising one without changing any of the stimuli,

44:32.880 --> 44:42.880
at the same time with or without making the prior beliefs about the upper lower case, the sort of local featureal expectations.

44:42.880 --> 44:48.880
We can switch those around so we replay exactly the same stimuli and the same behaviors,

44:48.880 --> 44:54.880
but just by changing the prior beliefs of the agent, we can cause certain things to be surprising,

44:54.880 --> 44:59.880
and those things can either be at the local, the first level, or the higher, the second level.

44:59.880 --> 45:08.880
And if we do that, we get lots of behaviors that look again a little bit like delay period activity

45:08.880 --> 45:16.880
in the prefrontal cortex of a periscadic sort that you see prior to a saccade being selected and enacted.

45:16.880 --> 45:25.880
While at the same time the band pass filtered voltages that are being driven by the implicit prediction errors

45:25.880 --> 45:29.880
look very much like periscadic ERPs.

45:29.880 --> 45:34.880
And when you look at those periscadic ERPs under local versus global violations,

45:34.880 --> 45:40.880
what you actually see is something almost identical if it's a local violation to a mismatch negativity,

45:40.880 --> 45:49.880
whereas, for the global violation, you get the mismatches or the differences much later on in time,

45:49.880 --> 45:52.880
very much like a P300.

45:52.880 --> 45:57.880
I can see what I was going to show you. No, I can't. I can't.

45:57.880 --> 46:01.880
That was a very pretty slide, but I can't.

46:01.880 --> 46:06.880
Very clever. It's a quote from, well, you don't need to know that.

46:06.880 --> 46:11.880
And then the final slide, it's got a thank you.

46:11.880 --> 46:16.880
A lot of people were on this slide, but you'll never know who now, will you?

46:16.880 --> 46:18.880
So thank you very much.

46:27.880 --> 46:35.880
So the workshop is structured so that there's a lot of time for discussion after each talk.

46:35.880 --> 46:43.880
And so the floor is open now for people to ask questions or make observations.

46:43.880 --> 46:50.880
I was just informed about how they do this in philosophy conferences that involves raising your hand or your finger,

46:50.880 --> 46:54.880
but I haven't mastered that yet, so I don't understand it.

46:54.880 --> 47:01.880
So I think it's too complicated for this group.

47:01.880 --> 47:05.880
But not for philosophers.

47:05.880 --> 47:13.880
So who would like to start?

47:13.880 --> 47:16.880
I'll start.

47:16.880 --> 47:19.880
I was just wondering if you could say more.

47:19.880 --> 47:28.880
You mentioned that you get something when there's choice involved with the rat experiment simulation.

47:28.880 --> 47:31.880
Something that looks like a drift diffusion model.

47:31.880 --> 47:39.880
And I've always been puzzled at how you get something that looks really like choice or agency out of a predictive coding model.

47:39.880 --> 47:43.880
So maybe you can elaborate a little bit on that.

47:43.880 --> 47:49.880
So the question is, where does the choice come into predictive coding?

47:49.880 --> 47:51.880
I think that question.

47:51.880 --> 47:54.880
Let me just be a little more specific.

47:54.880 --> 48:06.880
It's not that I don't see how you get choice behavior in the sense that you can use predictive coding in order to evaluate some options.

48:06.880 --> 48:23.880
But the notion of agency seems, if what you're doing is just predicting what you will do, then it seems to kind of undermine the notion of agency.

48:23.880 --> 48:27.880
So the answer to that question is very simple. It's tribly simple.

48:27.880 --> 48:34.880
You put agency into these schemes through prior preferences that define the sort of agent that I am.

48:34.880 --> 48:41.880
So we were talking before about reducing surprise of all sorts, whether it's epistemic uncertainty.

48:41.880 --> 48:49.880
But the simplest sort of pragmatic surprise, if I have a cost function that I don't want to be very hungry,

48:49.880 --> 48:53.880
or I don't want to end up in an arm that has no rewards in it,

48:53.880 --> 49:01.880
then I'd simply have to have the pride belief that at the end of the day I will end up rewarded or sated or happy or complete.

49:01.880 --> 49:04.880
So that anything else that happens is surprising.

49:04.880 --> 49:12.880
And therefore I can then bring the whole machinery of predictive coding to bear upon the problem of suppressing prediction errors and surprises.

49:12.880 --> 49:15.880
So I'm putting it very simply in terms of predictive coding.

49:15.880 --> 49:24.880
If I, a priori, believe that I'm always going to be happy and complete and that I am built to always minimise my prediction errors in the future,

49:24.880 --> 49:29.880
then I will look as if I have agency, I will look as if I have purpose,

49:29.880 --> 49:37.880
because I will always choose my actions in a way that avoids the prediction errors that suggest that I am not happy and complete.

49:37.880 --> 49:50.880
So the answer is just to absorb cost functions into inference by making costly states surprising through prior preferences.

49:50.880 --> 49:53.880
And that comes out of things like planning as inference.

49:57.880 --> 50:03.880
There are lots of ways of articulating that from the point of view of the rhetoric that I was using.

50:03.880 --> 50:05.880
The expected free energy has two bits to it.

50:05.880 --> 50:10.880
It has this epistemic bit and this pragmatic bit, but very simply it's uncertainty and surprise.

50:10.880 --> 50:13.880
The epistemic bit is minimising uncertainty.

50:13.880 --> 50:24.880
The value, the purpose, the goal is a pragmatic bit defined through cost functions that are literally the surprise of a costly outcome.

50:25.880 --> 50:33.880
So, just to follow up a little bit, so is this sort of like a hyper-prior that's going to be,

50:33.880 --> 50:39.880
I mean it sounds like we all have to have this ultimate belief that it's all going to end well at the end of the day.

50:39.880 --> 50:43.880
So pessimists, none of us are really pessimists or something like that, right?

50:43.880 --> 50:50.880
By definition. You may be perverse in your optimism, but you are quintessentially optimistic.

50:51.880 --> 50:57.880
You're getting to some, you know, the deeper backstories behind the free energy principle.

50:57.880 --> 51:06.880
The only, if you like, assumption that this instance of Hamilton's principle of release action makes is that you exist.

51:06.880 --> 51:13.880
And if you exist, that means you behave as if you have beliefs that you exist.

51:13.880 --> 51:17.880
And by existing, that just means that you're not decaying or dying.

51:17.880 --> 51:27.880
All your states are within some bounds, be they physiological or homeostatic or pecunary in terms of being rich or in terms of interceptive inference.

51:27.880 --> 51:31.880
You're, you know, the hedonics on that happy.

51:31.880 --> 51:39.880
But it's all about keeping things in bounds. It's all about minimising entropy, minimising uncertainty.

51:39.880 --> 51:47.880
So it always looks as if agents that exist have prior beliefs that they exist.

51:47.880 --> 51:57.880
And when you unpack that, that simply means I have preferred states that I will expect myself to occupy.

51:57.880 --> 52:01.880
Literally they are attracting states, they are an attractor.

52:01.880 --> 52:10.880
So that rhetoric, which actually is a rhetoric from dynamical systems theory, applies identically to this sort of purposeful reinforcement learning,

52:10.880 --> 52:15.880
or sort of goal directed style of thinking about things.

52:15.880 --> 52:21.880
There are attracting states, they are simply the ones that you frequent,

52:21.880 --> 52:28.880
which means that you will appear to behave as if you have prior preferences for being in those states.

52:28.880 --> 52:31.880
And you will always choose actions to get to those states.

52:31.880 --> 52:35.880
It is those prior preferences that define the sort of agent you are.

52:35.880 --> 52:42.880
So in answer to your very first question, the agentfulness comes in by implication,

52:42.880 --> 52:48.880
or just through the sorts of priors that characterise that particular sort of agent.

52:48.880 --> 52:53.880
So if I was a virus, I would have very different preferences than if I was a person.

52:54.880 --> 53:00.880
But there are still both plausible and viable preferences in the sorts of agents.

53:00.880 --> 53:02.880
Hi, thanks for your talk, Carl.

53:02.880 --> 53:09.880
I was wondering now that the slides are back if we could go to the delay period activity that you had briefly mentioned.

53:09.880 --> 53:15.880
And I just wanted to see that a little bit unpacked and related to what you were just talking about,

53:15.880 --> 53:18.880
that is agents reducing their free energy.

53:18.880 --> 53:27.880
Is that principle then generate the delay period activity that we see in places like prefrontal cortex and in working memory and so forth?

53:27.880 --> 53:28.880
Thanks.

53:28.880 --> 53:37.880
Right, well, those sorts of phenomena which we all know and have and will try to explain and measure empirically.

53:37.880 --> 53:39.880
So let me just try and find it.

53:39.880 --> 53:41.880
Do you remember where it was?

53:41.880 --> 53:43.880
Right, thank you.

53:44.880 --> 53:46.880
There you go.

53:46.880 --> 53:55.880
All those sorts of nuts and bolts getting down and dirty in terms of what this scheme would do when you put dynamics on it through the gradient descent,

53:55.880 --> 53:57.880
depend upon the gerontid model.

53:57.880 --> 54:05.880
So actually very similar to the last, one key component of the gerontid model are the prior beliefs, the preferences, what gives it purpose, what are its goals.

54:05.880 --> 54:08.880
Your question, I think, has a very similar answer.

54:08.880 --> 54:14.880
Once you've written down the gerontid model, everything else is not up for discussion.

54:14.880 --> 54:18.880
The maths tells you exactly what has to happen once you've written down the gerontid model.

54:18.880 --> 54:24.880
And the delay period activity you're talking about simply follows from the fact you've got a deep gerontid model or a deep temporal model.

54:24.880 --> 54:37.880
So as soon as you write that deep structure into the model, it means that certain beliefs have to outlive or change on a slower temporal scale than other beliefs lower in the hierarchy.

54:37.880 --> 54:45.880
Which means that you have to have delay period activity whilst other stuff unfolds at the lower levels of the hierarchy.

54:45.880 --> 54:55.880
So in this particular example, what I've done here is show the beliefs about the six sentences over the four moves,

54:55.880 --> 54:59.880
giving us six times four moves or five moves.

54:59.880 --> 55:03.880
So it should be about 30 beliefs here.

55:03.880 --> 55:11.880
On the same time access as beliefs about the particular word that's currently being seen.

55:11.880 --> 55:18.880
These resets here indicate the onset of saccades and the acquisition of new information.

55:18.880 --> 55:25.880
You can see roughly every 250 milliseconds there's a saccade and new beliefs are updated about the current word.

55:25.880 --> 55:34.880
But at the high level, we're only considering beliefs about each letter in my part.

55:34.880 --> 55:37.880
We're only considering beliefs about the word.

55:37.880 --> 55:50.880
So beliefs about the word corresponding to what's on this page or what's in this word are invariant during the successive saccades as you sample the different letters.

55:50.880 --> 55:54.880
So these things change more quickly than these things.

55:54.880 --> 55:58.880
When these are completed, then there's a change here and then the cycle begins again.

55:58.880 --> 56:08.880
So these tick over faster than this and then this looks a little bit now like the rastles that you see prior to the emission of the saccade here.

56:08.880 --> 56:12.880
They're not from the same paper but a related paradigm.

56:12.880 --> 56:19.880
If I take the voltage causing this delay period activity and band pass filter it, you get these sorts of fluctuations out here.

56:19.880 --> 56:26.880
So when there's an increase in delay period activity, there's usually a positive deflection that looks a little bit like an ERP.

56:26.880 --> 56:34.880
And just to follow up, so is the presence of delay period activity, is that associated then with prediction error or with the build up of a prediction?

56:34.880 --> 56:46.880
No, I think the prediction error in this scheme and this is the maths that comes from the discrete aspect of the genetic models lies in the rate of change of neural firing.

56:46.880 --> 57:03.880
If you associate the bi-physical encoding of expected states of the world in terms of population firing rates, then if you subscribe to that, if you accept that, then the prediction error now becomes the conductances that drive the depolarisation that drive the firing.

57:03.880 --> 57:13.880
So these basically reflect the fact that as time goes on you can more and more confident that one particular sentence is in play and you can see that.

57:13.880 --> 57:33.880
This is beliefs about which sentence is in play at the first, second, third, fourth and fifth page or word and they are now internally consistent and at every point in time in the past, at the end, I now believe I was reading the first sentence.

57:33.880 --> 57:41.880
And that belief endures during the sampling of all the actual letters within each of the words.

57:41.880 --> 57:52.880
So these would now represent just basically numbers between nought and one, zero and 100% neural firing that score your expectation that this is the current state of the world.

57:52.880 --> 58:08.880
At the beginning, there's lots of ambiguity, not an enormous amount, but there is ambiguity. It's 50-50 because of the six sentences, only two of them begin with the word flee, which means that we resolve our uncertainty about four of them,

58:08.880 --> 58:21.880
but we're still ambiguous about having ambiguity about sentences one and four and that can only be resolved at the end because these sentences only differ in the words right at the end.

58:21.880 --> 58:39.880
So during this time, there's delay period activity which we've got these two explanations, hypotheses in play that are resolved epistemically, optimally, right at the end when we get to the last word here and it's a wait and that determines which of the letters it was.

58:39.880 --> 58:53.880
Thank you for the talk. I wanted to go back to this idea of this contrast between reinforcement learning and the kind of formulation that you're making here.

58:53.880 --> 59:07.880
So one of the things that I thought was interesting is this formulation in terms of external value plus you basically decompose your KL divergence to external value and epistemic value.

59:07.880 --> 59:24.880
So how do you get exploration in this model? So it seems to me that you're doing an ARMAX over actions to get some balance between immediate value and information gain.

59:24.880 --> 59:27.880
Is that the basic idea?

59:27.880 --> 59:41.880
Yes. I mean, we can look at the equation or we can look at this. That's absolutely right. So, well, the exploration is good that you brought that in because another perspective on this is the whole foraging ethological perspective on exploration versus exploitation.

59:41.880 --> 59:53.880
That rhetoric just maps very simply to the epistemic and the pragmatic. So, and there is no, again, there is no up for discussion or there's no ad hoc waiting between the two.

59:53.880 --> 01:00:02.880
The expected free energy can always be written down in terms of exploration plus exploitation in terms of the epistemic value and the pragmatic value.

01:00:02.880 --> 01:00:17.880
And what happens is in minimizing that one quantity, you get this scanning searching behaviour until the epistemic bit has been reduced, allowing then you to focus on the pragmatic bit.

01:00:17.880 --> 01:00:29.880
So for free, you get a base optimal exploration to the extent that it is sufficient to resolve uncertainty given the precision of your beliefs about your prior preferences that then allow you to pursue your goals.

01:00:29.880 --> 01:00:43.880
So this solves the exploration dilemma in a base optimal sense. It also suggests that the very carving of behaviour into these two complementary drives is actually probably a misdirection.

01:00:44.880 --> 01:01:01.880
So it's only you and me that have actually teased apart the two components of the expected free energy and called one an epistemic exploratory one or a novelty seeking one and the other bit a pragmatic cost function like rewarding preference goal directed like one.

01:01:02.880 --> 01:01:09.880
There are lots of different ways of rearranging those. You can also rearrange them in terms of risk and ambiguity, intrinsic and extrinsic value.

01:01:09.880 --> 01:01:25.880
There are lots of ways of carving them and getting different perspectives. When you see that and when you work with that, you start to realize that it's not necessarily the best thing just to have one particular religious perspective on it

01:01:26.880 --> 01:01:40.880
because it lends you to the false belief if you subscribe to this formalism that there has to be some other adjudicator. There has to be some other harmonculus that's decided, oh, I need to explore now.

01:01:40.880 --> 01:01:50.880
I've done my exploration and now I'm going to go and do a bit of pragmat and stop the scanning and then I'm going to go and exploit what I've discovered. It doesn't work like that. You should get that for free.

01:01:50.880 --> 01:02:05.880
If they're both part of the same cost function, then once you've sufficiently reduced your uncertainty, then just naturally you go into as illustrated here your exploratory behaviour.

01:02:05.880 --> 01:02:15.880
This is exploratory behaviour or novelty seeking in the sense that you don't know what the queue is going to tell you. It doesn't have any immediate rewarding aspect to it.

01:02:15.880 --> 01:02:26.880
There are no preferences associated with the condition stimulus, but it's interesting, uncertainty resolving, but after a time it becomes boring because you already know what it's going to tell you.

01:02:26.880 --> 01:02:39.880
Once it does that, then you get exploratory behaviour. I should have done that. I should have put exploration and exploitation. Have an argument with me because it's meant to be a discussion. Do you not like that?

01:02:40.880 --> 01:02:55.880
It's very interesting perspective on it. I'm surprised that it comes out that the simple deterministic policy works well and just kind of works out of the box.

01:02:55.880 --> 01:03:22.880
My inkling is that when you go and implement this stuff, it can be difficult for it to balance the exploration and exploitation. So there's no tuning parameters, there's no off policy estimation, you just throw it in there.

01:03:22.880 --> 01:03:41.880
It all works out. I know exactly what you're saying because this was a big selling point when we first realised that a couple of years ago and it will remain a bit like one of these five to ten year changing the direction of the ocean liner of the oil tanker.

01:03:42.880 --> 01:03:53.880
Two years later, all that we're saying is in fact people behave according to Hampton's principle of least action. That's all that we're saying.

01:03:54.880 --> 01:04:11.880
That implicitly, or it looks like, that behaviour has this dual aspect. It doesn't, if you formulate it as a variational principle of the sort you did at school when doing new turning mechanics and then Einstein did with general relativity.

01:04:12.880 --> 01:04:31.880
It sort of falls out of the mix in a way that does actually dismiss these separatist perspectives on, I can either do this or that and I've got to now optimise the exploration in relation to the exploitation.

01:04:31.880 --> 01:04:46.880
The key trick that puts you into this simple world of Hampton's principle of least action is the realisation you can't prescribe good behaviour unless the prescription is an optimisation of a function of beliefs.

01:04:46.880 --> 01:05:03.880
A really simple example would be an economics game. I've got a really high risk and a low risk option. But there may be a third option, which is if I don't know which is which, I should do nothing until I know more.

01:05:04.880 --> 01:05:26.880
In using words like I don't know, I am now saying that my behaviour now becomes a function of my knowledge or my uncertainty, so I now induce different options, different behaviours, different policies and actions that rest upon my degree of belief, which means you can't do it with value function optimisation or utility function optimisation or reinforcement learning.

01:05:26.880 --> 01:05:39.880
It can't be done with queue learning. It cannot be done with a Bellman optimisation scheme because what you've done is you've said that there are better behaviours when I don't know what to do, which I generally don't do anything or wait until more information comes about.

01:05:40.880 --> 01:05:52.880
And once you write that down, you make your objective function a function of a probability distribution which becomes an energy and then you integrate that over time and then you've got to Hampton's principle of least action.

01:05:52.880 --> 01:06:00.880
So the simplicity post hoc is evident for me anyway. Does that make any sense?

01:06:01.880 --> 01:06:12.880
Can I ask you a question about the general approach here, which is I see that you start saying that there is an urge, there is a force to reduce surprise.

01:06:12.880 --> 01:06:23.880
So this is like I'm making an analogy with physics because that's what you're doing. And there you start saying I can rewrite the whole thing instead of force in terms of a Fourier energy.

01:06:24.880 --> 01:06:31.880
And then you go on and explain what it implies. My fear is that there is a difference here between what we do in terms of behaviour.

01:06:31.880 --> 01:06:46.880
First of all, reducing behaviour to just minimizing surprise, there are other forces that we cannot measure. We cannot even measure forces, force in terms of reducing surprise in an individual.

01:06:47.880 --> 01:06:57.880
So even though you can write these equations and describe something general so you can prescribe what should be the brain doing, do you think you can actually make any prediction?

01:06:57.880 --> 01:07:06.880
I know you're doing it, but I'm asking how can you think that you can do it because there is no measurement to tell you that's actually the case.

01:07:07.880 --> 01:07:20.880
So in physics you could come up with any new evidence, you would write a new term into your equation and just keep adding it and then you're always safe because there are some conservation laws that basically will keep the Hamilton principle intact.

01:07:20.880 --> 01:07:29.880
Well, who said there is such a thing in terms of behaviour and I think the simplest thing to assume there is no such thing.

01:07:30.880 --> 01:07:42.880
So then by doing that, if you let's say you keep adding terms because you believe that what we are doing is just reducing surprise, this is the ultimate goal of behaviour, then just keep adding terms to your free energy.

01:07:42.880 --> 01:07:54.880
And then basically the whole thing becomes a tautology because you're assuming that you're adding terms and there is no proof or disprove for that.

01:07:55.880 --> 01:08:01.880
So that's my main question is that what is, I mean, how do you think it's going to work?

01:08:02.880 --> 01:08:08.880
Right, that's a very good, I mean I ended quite a bit weakly, but that was a very good question.

01:08:08.880 --> 01:08:21.880
So lots of really interesting issues there, so the tautology issue, the practical utility of this style of theorising, can it ever be falsified adding things too?

01:08:22.880 --> 01:08:27.880
So I think we could spend hours talking about any one of those.

01:08:27.880 --> 01:08:35.880
First of all, the whole point of my style of neuroscience is that we never add anything in.

01:08:36.880 --> 01:08:41.880
You're always obliged to, for every advance, you have to get rid of something which was a distraction or ad hoc or a heuristic.

01:08:42.880 --> 01:08:48.880
We've been talking about this magic parameter, the nuances, the balance between exploration and exploitation.

01:08:49.880 --> 01:08:54.880
So that just goes, there's only one thing that's being minimised here, about everything and that's variation free energy.

01:08:54.880 --> 01:09:00.880
That's it, there's nothing being added. However, of course, the free energy is a function of the gerontive model.

01:09:01.880 --> 01:09:12.880
So all the interesting, all the hard work or the heavy lifting understanding this biological system in this experimental context or this social context, that really calls upon you writing down a gerontive model.

01:09:13.880 --> 01:09:20.880
So that's, it doesn't mean there is no free lunch, there's still a lot of neurobiology and psychology and cognitive neuroscience and computational neuroscience to do.

01:09:20.880 --> 01:09:27.880
It's just all at the level of the gerontive model, not the normative principles or the variational principles behind it.

01:09:28.880 --> 01:09:39.880
The tautology, part of that drive for simplification is a drive to get to the ultimate explanation which has to be tautological and for me the free energy principle is tautological.

01:09:40.880 --> 01:09:46.880
It's as tautological as the natural selection, but beautifully so. Once it's completely tautological, I'll be happy.

01:09:46.880 --> 01:10:00.880
Part of that tautology comes along in a slightly technical guise called the complete class theorem and I'm bringing that to the discussion because it speaks to, I think, a very interesting point you were essentially making.

01:10:01.880 --> 01:10:07.880
Is there anything that you can not explain, any behaviour in a real biological system that cannot be explained by this?

01:10:08.880 --> 01:10:11.880
Because if there isn't, then what's the point?

01:10:12.880 --> 01:10:35.880
Now, if there is no behaviour that this cannot explain, so it is provably true that for any pair of cost functions and behaviours there are a set of prior beliefs, prior preferences that we're talking about that endow the behaviour with an agent, with agency,

01:10:35.880 --> 01:10:40.880
that render that behaviour based optimal and by definition therefore conforms to the free energy principle.

01:10:41.880 --> 01:10:46.880
So what that means is that there is no behaviour that this can't describe if you can find the right prize.

01:10:47.880 --> 01:10:52.880
So is that a weakness or a strength? Well, in the sense of falsifiability, it's a weakness.

01:10:53.880 --> 01:11:03.880
In the sense of actually using it practically, it's a real strength because what that means is any system, normal human being or psychiatric cohort,

01:11:03.880 --> 01:11:11.880
that you bring to me, if I can solve the problem of getting the most appropriate or a sufficiently good generative model that describes their behaviour,

01:11:12.880 --> 01:11:16.880
it means I can quantify exactly the sort of person they are by their prior beliefs.

01:11:17.880 --> 01:11:21.880
And we actually can write back down to the first question again, is what makes an agent an agent, it's their prior beliefs.

01:11:22.880 --> 01:11:27.880
It's that attracting set that describes the sorts of states that that sort of person occupies.

01:11:27.880 --> 01:11:34.880
So, yeah, there is a deep tautology, there's a fundamental difficulty for falsification,

01:11:35.880 --> 01:11:40.880
but there's also a glorious insight underneath that which means that everything can now be written down in terms of an agent's prior beliefs

01:11:41.880 --> 01:11:44.880
and that if you can get the right model, you can actually estimate these things.

01:11:45.880 --> 01:11:48.880
You can actually quantify them and this is one of the tenets of computational psychiatry.

01:11:48.880 --> 01:11:58.880
It's to be able to use games, ERPs, mismatch negativities, whatever in the service of say, what sort of person am I looking at,

01:11:59.880 --> 01:12:02.880
quantified in terms of the prior beliefs about the way they should behave.

01:12:03.880 --> 01:12:11.880
Carl, but aren't you making the assumption that there is just a unique set of beliefs that will match a data set?

01:12:11.880 --> 01:12:16.880
Because if you have several beliefs and I would think that when we minimize those type of system,

01:12:17.880 --> 01:12:24.880
that's equivalent to several solutions and which almost always we've got several solutions because we've got several minimas there.

01:12:25.880 --> 01:12:28.880
Because it's not convex, the story that we are minimizing there.

01:12:29.880 --> 01:12:34.880
So, therefore, we have a lot of minimas and I would say that if we're lucky that means that for any behavior that we can measure,

01:12:34.880 --> 01:12:40.880
we are now with a set and that could be thrilling to know which one, a set of prior beliefs.

01:12:41.880 --> 01:12:44.880
So, in other words, we cannot inverse that or am I wrong?

01:12:45.880 --> 01:12:49.880
No, no, no, you're absolutely right but you've taken us into metabasian land now.

01:12:50.880 --> 01:12:55.880
Okay, so just to, those people who may be getting a bit confused.

01:12:56.880 --> 01:13:03.880
So, the argument I think, let me just paraphrase it, if we're now saying well how do we practically use this style of thinking

01:13:04.880 --> 01:13:12.880
and I'm now in the job of quantifying this person with autistic spectrum disorder,

01:13:13.880 --> 01:13:19.880
given a bead's task, an earned task, in terms of the prior beliefs about the volatility of the environment

01:13:20.880 --> 01:13:26.880
and the need to please the experimenter by responding within a certain time frame.

01:13:26.880 --> 01:13:34.880
If that is the problem, then we're now in and observing the observer or using Bayesian inference

01:13:35.880 --> 01:13:42.880
to make inferences about a Bayesian machine, which is the autistic patient, which is hence the metabasian thing

01:13:43.880 --> 01:13:45.880
and you're absolutely right, that could be an ill-posed problem.

01:13:46.880 --> 01:13:52.880
So, there may be, the complete class there does not say that there is only one unique set of prior beliefs

01:13:52.880 --> 01:13:56.880
that will make, render the behaviour based on what it says that it exists, it doesn't have to be a unique solution.

01:13:57.880 --> 01:14:05.880
However, what will happen is that if you actually use Bayesian statistics to infer the prior beliefs of the Bayesian or ASD subject,

01:14:06.880 --> 01:14:11.880
you will then see if you've got the appropriate model that there are a number of equally plausible solutions

01:14:12.880 --> 01:14:16.880
and you will also see that you haven't got enough data to disambiguate between them.

01:14:16.880 --> 01:14:21.880
But you will also have an insight into which sorts of experiments you would need to do that disambiguation

01:14:22.880 --> 01:14:24.880
because you've got a generative model underneath of this.

01:14:25.880 --> 01:14:29.880
You can do simulations to see the sorts of data that you are, or the ways of looking at the responses

01:14:30.880 --> 01:14:37.880
that would enable you now to narrow down these competing but equally plausible sets of prior beliefs that characterize that subject.

01:14:38.880 --> 01:14:45.880
So, it's a very important issue, but I think it's a generic one about how we actually apply Bayesian statistics

01:14:46.880 --> 01:14:49.880
to understand the ways in which our data are being generated.

01:14:50.880 --> 01:14:55.880
I think that in that sense it's less profound than the complete class theorem in itself,

01:14:56.880 --> 01:15:00.880
which speaks to the sort of tautology of the game that we find ourselves in.

01:15:03.880 --> 01:15:05.880
I think we can have time for one more short question.

01:15:05.880 --> 01:15:09.880
I have two short ones.

01:15:10.880 --> 01:15:16.880
The first one is about the semantics and the second one is about the substance.

01:15:17.880 --> 01:15:23.880
So, regarding the semantics, I'm wondering, it seems as though there are these two aspects to the problem.

01:15:24.880 --> 01:15:29.880
One is inference, the other is control and there is the epistemic cost in the context of inference

01:15:29.880 --> 01:15:36.880
and there is the pragmatic costs, ultimately reproduction, the well-being and survival of the organism.

01:15:37.880 --> 01:15:46.880
It seems as though you are declaring the primacy of surprise, so you want to absorb the pragmatic costs under the epistemic costs.

01:15:47.880 --> 01:15:52.880
So, that's a bit of a semantic issue perhaps, but doesn't it make more sense to do it the other way around

01:15:52.880 --> 01:16:00.880
and say, so the ultimate goal is reproduction and epistemic benefits should be a sub-goal of that.

01:16:01.880 --> 01:16:03.880
So, that's the semantic question.

01:16:04.880 --> 01:16:14.880
The question on substances, is this something that is already going on in engineering in AI, for example?

01:16:15.880 --> 01:16:21.880
Can these principles be scaled up to real-world tasks such as visual object recognition?

01:16:22.880 --> 01:16:29.880
If so, has it already been done or is this an impending revolution for AI?

01:16:30.880 --> 01:16:35.880
And if not, is it just a reinterpretation of what already exists?

01:16:36.880 --> 01:16:38.880
I agree because they were very short questions.

01:16:39.880 --> 01:16:42.880
Very short. You couldn't get them shorter than that, could you?

01:16:43.880 --> 01:16:45.880
Do you want me to try to answer them shortly?

01:16:46.880 --> 01:16:48.880
The semantic one.

01:16:48.880 --> 01:16:51.880
No, I don't think so.

01:16:52.880 --> 01:16:59.880
I take your point that one could articulate a whole theory and absorb uncertainty into cost functions,

01:17:00.880 --> 01:17:07.880
but I think that misses the key point that I was trying to make at the beginning of the talk,

01:17:08.880 --> 01:17:13.880
that the quantity you have to optimise is a function of probability distributions or beliefs.

01:17:14.880 --> 01:17:21.880
That's the key thing, which means you can never use a Bellman optimality scheme to properly solve that.

01:17:22.880 --> 01:17:27.880
You do see heroic attempts, and those heroic attempts have been endemic for the past 40 years,

01:17:28.880 --> 01:17:32.880
they're known as partial observed mark-up decision processes or belief state machines,

01:17:33.880 --> 01:17:38.880
and all sorts of heroic attempts to try and recover or resolve the problem,

01:17:38.880 --> 01:17:45.880
or once you write down a discrete state space over continuous beliefs, you can't do it,

01:17:46.880 --> 01:17:50.880
therefore you have to parameterise those continuous beliefs in those sorts of glorious and clever ways,

01:17:51.880 --> 01:17:52.880
they don't work, they don't scale.

01:17:53.880 --> 01:17:58.880
So people have been aware of the problem, and hence the vast literature on partial observed mark-up decision problems.

01:17:59.880 --> 01:18:04.880
What I'm saying is that's the wrong approach, the right approach is Hampton's principle of least action,

01:18:04.880 --> 01:18:09.880
where the functional that you need to optimise is a function of the beliefs before you start,

01:18:10.880 --> 01:18:13.880
and by proof of principle I can demonstrate the veracity of that argument,

01:18:14.880 --> 01:18:18.880
because I can solve problems which people working with partial observed MDPs cannot solve.

01:18:19.880 --> 01:18:26.880
So I think it's much more than semantics, I think it's actually, people have got it wrong in the 20th century.

01:18:27.880 --> 01:18:31.880
I think the Bellman's optimality equations are very beautiful construct, completeness direction.

01:18:31.880 --> 01:18:35.880
We should have been looking at Hamilton's principle of least action, and that's the 21st century.

01:18:36.880 --> 01:18:37.880
Are people doing this? Yeah.

01:18:38.880 --> 01:18:44.880
So Google Deep Mind, for example, they've now, a small group within Google Deep Mind,

01:18:45.880 --> 01:18:50.880
have now started using variational free energy in their deep energy model, not the temporal models,

01:18:51.880 --> 01:18:58.880
but certainly sort of 10-led neural networks in the context of the sort of pattern recognition approach.

01:18:58.880 --> 01:19:05.880
So people have always rears, I think, that the variational approach was the right way to do this,

01:19:06.880 --> 01:19:10.880
and of course you remember most of deep learning started with Geoffrey Hinton's work,

01:19:11.880 --> 01:19:19.880
and he came from the variational formulation, so he was the first person to be down to propose the Helmholtz machine,

01:19:20.880 --> 01:19:24.880
but they've taken a sort of security stream back to those early work in the 1990s.

01:19:24.880 --> 01:19:28.880
Will it scale? I don't know, because I don't think they're quite on top of that,

01:19:29.880 --> 01:19:36.880
because what they do is they haven't quite got, well, this is insulting, but I hope, is anybody here from Google?

01:19:37.880 --> 01:19:40.880
Ah, well I won't say that then.

01:19:41.880 --> 01:19:46.880
Well anyway, they love amortisation, they love casting things in terms of learning,

01:19:47.880 --> 01:19:51.880
so what they do is instead of actually trying to optimise their beliefs, their expectations,

01:19:51.880 --> 01:19:56.880
they try and optimise beautifully constructed deep nets, convolution nets,

01:19:57.880 --> 01:20:03.880
that have parameters that would map from data to beliefs, or sufficient statistics or beliefs,

01:20:04.880 --> 01:20:08.880
and then they learn that because they're experts, and they are more than well experts,

01:20:09.880 --> 01:20:12.880
they are the experts in optimising the parameters.

01:20:13.880 --> 01:20:18.880
That's a slight problem because it denies any context sensitivity of the sort that we deal with the neuroscientists

01:20:18.880 --> 01:20:20.880
and I deal with them in my simulations.

01:20:21.880 --> 01:20:25.880
I think once they get beyond amortising their deep networks, then they'll be in this game,

01:20:26.880 --> 01:20:31.880
and then we'll find out whether it scales, you know, that you'll need lots of computer scientists, big computers.

01:20:32.880 --> 01:20:35.880
So I would imagine the next five to ten years this style of approach,

01:20:36.880 --> 01:20:43.880
and so the very actual free energy formulation will become increasingly dominant in people doing artificial intelligence.

01:20:43.880 --> 01:20:50.880
And I should quip, I mean, for some people, the new AI is actually active inference.

01:20:51.880 --> 01:21:00.880
It wasn't a coincidence that we chose that sort of rhetoric to promote it.

01:21:01.880 --> 01:21:04.880
Yes, so maybe I could just have a comment rather than a question then.

01:21:05.880 --> 01:21:06.880
Okay, okay.

01:21:07.880 --> 01:21:12.880
So that's a bit straight-laced approach to free energy reduction.

01:21:13.880 --> 01:21:18.880
It's based on surprise aversion, but I'm sure you have another talk on surprise-seeking,

01:21:19.880 --> 01:21:22.880
curiosity, creativity, playful managerial styles,

01:21:23.880 --> 01:21:30.880
which must have some adaptive purpose beyond our interest in horror movies and jokes.

01:21:30.880 --> 01:21:37.880
It must help us get out of dead-end problem space to problem spaces we can't even imagine.

01:21:38.880 --> 01:21:46.880
So the comment is, if you add surprise-seeking to free energy minimisation, I don't believe it remains tractable.

01:21:47.880 --> 01:21:48.880
So I'll leave it at that.

