start	end	text
0	2480	The following content is provided under a Creative
2480	3800	Commons license.
3800	6120	Your support will help MIT OpenCourseWare
6120	10080	continue to offer high quality educational resources for free.
10080	12760	To make a donation or to view additional materials
12760	16680	from hundreds of MIT courses, visit MIT OpenCourseWare
16680	17840	at ocw.mit.edu.
21520	27240	So today we'll actually just do a brief chapter
27240	28640	on Bayesian statistics.
28640	31400	And there's entire courses on Bayesian statistics.
31400	33480	There's entire books on Bayesian statistics.
33480	36080	There's entire careers on Bayesian statistics.
36080	39280	So admittedly, I'm not going to be
39280	40960	able to do it justice and tell you
40960	42960	all the interesting things that are happening
42960	44080	in Bayesian statistics.
44080	47320	But I think it's important as a statistician
47320	49360	to know what it is, how it works,
49360	52520	because it's actually a weapon of choice
52520	54400	for many practitioners.
54400	58120	And because it allows them to incorporate their knowledge
58120	60720	about a problem in a fairly systematic manner.
60720	63640	So if you look at, like, say, the Bayesian statistics
63640	65520	literature, it's huge.
65520	69600	And so here I give you sort of a range
69600	72880	of what you can expect to see in Bayesian statistics
72880	78320	from your second edition of a traditional book, something
78320	81440	that involves computation, some things that involve
81440	82240	rethinking.
82240	84760	And there's a lot of Bayesian thinking.
84760	87400	There's a lot of things that talk about sort
87400	90240	of like philosophy of thinking Bayesian.
90240	92280	This book, for example, seems to be one of them.
92280	94720	This book is definitely one of them.
94720	98880	This one represents sort of a broad literature
98880	102240	on Bayesian statistics for applications, for example,
102240	103560	in social sciences.
103560	105280	But even in large-scale machine learning,
105280	107320	there's a lot of Bayesian statistics happening,
107320	110280	particularly using something called Bayesian parametrics
110280	113480	or hierarchical Bayesian modeling.
113480	119520	So we do have some experts at MIT in the C cell.
119520	123600	Tamara Broderick, for example, is a person who does quite
123600	126200	a bit of interesting work on Bayesian parametrics.
126200	128320	And if that's something you want to know more about,
128320	130920	I urge you to go and talk to her.
130920	134040	So before we go in those more advanced things,
134040	137200	we need to start with what is the Bayesian approach?
137200	138680	What do Bayesians do?
138680	142600	And how is it different from what we've been doing so far?
142640	146360	So to understand the difference between Bayesians
146360	148840	and what we've been doing so far is
148840	151360	we need to first put a name on what we've been doing so far.
151360	152960	It's called Frequentist Statistics.
152960	156480	So it's usually Bayesian versus Frequentist statistics.
156480	158800	I mean, by versus, I don't mean that there's naturally
158800	160400	an opposition to them.
160400	163360	Actually, often you will see the same method that
163360	165440	comes out of both approaches.
165440	166880	So let's see how we did it.
166880	168960	The first thing, we had data.
168960	170720	We observed some data.
170720	173000	And we assumed that this data was generated randomly.
173000	174840	The reason we did that is that because this
174840	177840	would allow us to leverage tools from probability.
177840	181000	So let's say by nature, measurements, you do a survey,
181000	183080	you get some data.
183080	186040	Then we made some assumptions on the data generating process.
186040	187400	For example, we assumed there were
187400	189480	IID that was one of the recurring things.
189480	191640	Sometimes we assumed it was Gaussian
191640	193440	if we wanted to use, say, t-test.
193440	195320	Maybe we did some non-parametric statistics.
195320	197680	So we assumed it was a smooth function
197680	200360	or maybe linear regression function.
200400	201560	So those are our modeling.
201560	204160	And this was basically a way to say,
204160	207960	well, we're not going to allow for any distributions
207960	211680	for the data that we have, but maybe a small set of distribution
211680	214800	that index by some small parameters, for example.
214800	218440	Or at least remove some of the possibilities.
218440	220800	Otherwise, there's nothing we could learn.
220800	225280	And so, for example, this was associated
225280	227000	to some parameter of interest, say,
227000	231080	data or beta in the regression model.
231080	233840	All right, then we had this unknown problem
233840	237600	and this unknown parameter, and we wanted to find it.
237600	239600	We wanted to either estimate it or test it
239600	242440	or maybe find a confidence interval for this object.
242440	246040	So far, I should not have said anything that's new.
246040	248200	But this last sentence is actually
248200	250640	what's going to be different from the Bayesian part.
250640	253000	In particular, this unknown but fixed thing
253000	256120	is what's going to be changing.
256120	258760	So in the Bayesian approach, we still
258760	262000	assume that we observe some random data.
262000	264120	But the generating process is slightly different.
264120	265720	It's sort of a two-layer process.
265720	267920	And there's one process that generates the parameter,
267920	269720	and then one process that, given this parameter,
269720	271400	generates the data.
271400	275240	So what the first layer does, I mean,
275240	276840	nobody really believes that there's
276840	280520	some random process that's happening about generating
280520	284920	what is going to be the true expected number of people who
284920	285960	turn their head to the right.
285960	287920	When they kiss, but this is actually
287920	292680	going to be something that brings us some easiness for us
292680	296120	to incorporate what we call prior belief.
296120	298680	So we'll see an example in a second.
298680	301480	But often, you actually have prior belief
301480	303000	of what this parameter should be.
303000	305120	When we did, let's say, these squares,
305120	309360	we looked over all of the vectors in all of r to the p,
309360	314120	including the ones that have coefficients equal to $50 million.
314120	317720	And so those are things that maybe we might be able to roll out.
317720	319320	And maybe we might be able to roll out
319320	321840	at a much smaller scale.
321840	323560	For example, well, I mean, I don't know.
323560	329200	I'm not an expert on turning your head to the right or to the left.
329200	330960	But maybe you can roll out the fact
330960	333240	that almost everybody is turning their head
333240	335360	in the same direction, or almost everybody's
335360	338040	turning their head to another direction.
338040	339880	So we have this prior belief.
339880	343200	And this prior belief is going to play, say,
343200	345160	hopefully, less and less important role
345160	347560	as we collect more and more data.
347560	349160	But if we have a smaller amount of data,
349160	352760	we might want to be able to use this information rather
352760	354720	than just shooting in the dark.
354720	358160	And so the idea is to have this prior belief.
358160	360480	And then we want to update this prior belief
360480	363040	into what's called a posterior belief
363040	364920	after we've seen some data.
364920	368240	Maybe I believe that there's something that
368240	369680	should be in some range.
369680	371400	But maybe after I see data, maybe it's
371400	372640	comforting me in my belief.
372640	375280	So I'm actually having maybe a belief that's more.
375280	378440	So a belief encompasses basically what you think
378440	379920	and how strongly you think about it.
379920	381400	That's what I call belief.
381400	384080	So for example, if I have a belief about some parameter
384080	387440	theta, maybe my belief is telling me where theta should be
387440	389960	and how strongly I believe in it in the sense
389960	394640	that I have a very narrow region where theta could be.
394640	397600	And so the posterior belief says, well, you see some data.
397600	400000	And maybe you're more confident or less confident about what
400000	400500	you've seen.
400500	402780	You've shifted your belief a little bit.
402780	404620	And so that's what we're going to try to see
404620	407940	and how to do this in a principal manner.
407940	410020	So of course, to understand this better,
410020	412140	there's nothing better than an example.
412140	416220	So let's talk about another stupid statistical question,
416220	418620	which is let's try to understand p.
418620	421300	Of course, I'm not going to talk about politics from now on.
421300	423940	So let's talk about p, the proportion of women
423940	424940	in the population.
431100	431700	OK?
431700	438660	And so what I could do is to collect some data, x1, xn,
438660	444660	and assume that they're Bernoulli with some parameter p
444660	445160	unknown.
445160	445660	Right?
445660	447660	So p is in 0, 1.
450060	450560	OK?
450560	452780	Let's assume that those guys are i, d.
452780	457700	So this is just an indicator for each of my collected data,
458220	461540	whether the person I randomly sample is a woman,
461540	464060	I get a 1, and if it's a man, I get a 0.
464060	464420	OK?
464420	469740	And so now the question is, I sample these people randomly.
469740	471580	I denote their gender.
471580	474780	And the frequentist approach was just saying, OK,
474780	478260	let's just estimate p hat being xn bar.
478260	481020	And then we could do some tests.
481020	481200	Right?
481200	482260	So here there's a test.
482260	485340	I want to test maybe if p is equal to 0.5 or not.
485340	489860	That sounds like a pretty reasonable thing to test.
489860	493140	But we want to also maybe estimate p.
493140	495140	But here this is a case where we definitely
495140	497140	have for our belief of what p should be.
497140	497700	Right?
497700	502060	We are pretty confident that p is not going to be 0.7.
502060	506700	We actually believe that p should be extremely close to 1.5.
506700	506940	OK?
506940	508460	But maybe not exactly.
508460	509340	Maybe I don't know.
509340	512700	Maybe this population is not the population in the world,
512700	515660	but maybe this is the population of, say, some college.
515660	519300	And we want to understand if this college has half women or not.
519300	519580	Right?
519580	522100	So maybe we know it's going to be close to 1.5,
522100	524700	but maybe we're not quite sure.
524700	529260	And so we're going to want to integrate that knowledge.
529260	529980	All right?
529980	531980	So I could integrate it in a blunt manner
531980	535420	by saying discard the data and say that p is equal to 1.5.
535420	537580	But maybe that's just a little too much.
537580	541380	So how do I do this trade-off between adding the data
541380	545060	and combining it with this prior knowledge?
545060	549100	In many ways, in many instances, essentially what's
549100	551020	going to happen is this 1.5 is going
551020	554260	to act like one new observation, essentially.
554260	557020	So if you have five observations,
557020	560140	this is just a six observation, which will play a role.
560140	561820	If you have a million observations,
561820	562860	you're going to have a million in one,
562860	564660	and it's not going to play so much a role.
564660	566700	That's basically how it goes.
566700	568620	That's basically how it goes.
568660	573580	But definitely not always, because we'll
573580	576980	see that if I take my prior to be a point minus 1.5 here,
576980	579140	it's basically as if I was discarding my data.
579140	581740	So essentially, there's also your ability
581740	585460	to encompass how strongly you believe in this prior.
585460	587820	And if you believe infinitely more in the prior
587820	589620	than you believe in the data you collected,
589620	591900	then, of course, it's not going to act like one more
591900	593340	observation.
593340	595540	All right, so the Bayesian approach
595540	599020	is a tool to, one, include mathematically our prior,
599020	602060	and our prior belief into statistical procedures.
602060	603940	So maybe I have this prior knowledge,
603940	606100	but if I'm a medical doctor, it's not clear to me
606100	609900	how I'm going to turn this into some principle way of building
609900	610420	estimators.
610420	612140	And of course, the second goal is
612140	613860	going to be to update this prior belief
613860	619780	into posterior belief by using the data, all right?
619780	623940	So how do I do this?
623940	625500	And at some point, I sort of suggested
625500	628660	that there's two layers.
628660	631700	One is where you draw the parameter at random.
631700	635780	And two, once you have the parameter, condition
635780	639340	this parameter, you draw your data.
639340	641100	Nobody believes this actually is happening,
641100	643620	that nature is just rolling dice for us
643620	645500	and choosing parameters at random.
645500	648260	But what's happening is that this idea
648260	651420	that the parameter comes from some random distribution
651420	655460	actually captures very well this idea that how you would
655460	656980	encompass your prior, right?
656980	659100	How would you say my belief is as follows?
659100	661900	Well, here's an example about p.
661900	668180	I'm 90% sure that p is between 0.4 and 0.6.
668180	674060	And I'm 95% sure that p is between 0.3 and 0.8.
674060	678500	OK, so essentially, I have this possible value of p.
678500	688540	And what I know is that there's 90% here between, what did I
688540	692420	say, 0.4 and 0.6.
695460	699260	And then I have 0.3 and 0.8.
699260	703860	And I know that I'm 95% sure that I'm in here.
703860	705340	And this, if you remember, this sort of
705340	707340	looks like the kind of pictures that I
707340	710140	made when I had some Gaussian, right, for example.
710140	714220	And I said, oh, here we have 90% of the observations.
714220	716660	And here we have 95% of the observations.
720460	724620	So in a way, if I were able to tell you
724620	727540	all those ranges for all possible values,
727540	730580	then I would essentially describe a probability
730580	733420	distribution for p.
733420	734780	And what I'm essentially saying is
734780	736620	that p is going to have this kind of shape.
736620	739060	So of course, if I tell you only twice this information
739060	742300	that there's 90% I'm here and I'm here between here and here
742300	744620	and 95% I'm between here and here,
744620	747220	then there's many ways I can accomplish that, right?
747220	751660	I could have something that looks like this, maybe, right?
751660	755580	I could be really, I mean, it could be like this.
755580	757820	I mean, there's many ways I can have this.
757820	759900	Some of them are definitely going to be mathematically
759900	762260	more convenient than others.
762260	764340	And hopefully, we're going to have things
764340	767260	that I can parameterize very well.
767260	769900	Because if I tell you this is this guy,
769900	776780	then there's basically 1, 2, 3, 4, 5, 6, 7 parameters.
776780	779020	So I probably don't want something that has 7 parameters,
779020	781180	but maybe I can say, oh, it's a Gaussian.
781180	782820	And all I have to do is to tell you
782820	787060	where it's centered and what the standard deviation is.
787060	791060	OK, so the idea of using this two-layer thing
791060	793540	where we think of the parameter p as being drawn
793540	795940	from some distribution is really just a way for us
795940	799260	to capture this information, our prior belief being,
799260	802820	well, there's this percentage of chances that it's there.
802820	805540	But the person who has a chance, I'm deliberately not
805540	806940	using probability here.
806940	808780	It's really, right?
808780	812140	So it's really a way to get close to this.
812140	814340	All right, so that's what I said.
814340	816180	The true parameter is not random,
816180	820420	but the Bayesian approach does as if it was random
820420	824500	and then just spits out a procedure out of this thought
824500	829060	process, this thought experiment.
829060	834100	So when you practice Bayesian statistics a lot,
834100	837100	you start getting automatisms.
837100	840580	So you start getting some things that you do
840580	843140	without really thinking about it, just like when you're
843140	844740	a statistician, the first thing you do
844740	847820	is can I think of this data as being Gaussian, for example?
847820	850340	When you're Bayesian, you're thinking about, OK,
850340	851620	I have a set of parameters, right?
851620	855060	So here, I can describe my parameter as being theta
855060	861540	in general in some big space parameter theta.
861540	864900	But what spaces did we encounter?
864900	867100	Well, we encountered the real line.
867100	870980	We encountered the interval 0, 1 for Bernoulli's.
870980	876340	And we encountered maybe some deposit of real line
876340	879340	for exponential distributions, et cetera.
879340	882060	And so what I'm going to need to do, if I want to model some,
882060	884580	if I want to put some prior on those spaces,
884580	888340	I'm going to have to have a usual set of tools for this guy,
888340	890340	usual set of tools for this guy, usual set of tools
890340	891220	for this guy.
891220	892540	And by usual set of tools, I mean,
892540	894660	I'm going to have to have a family of distributions
894660	896820	that's supported on this.
896820	900700	So in particular, this is the speech in which my parameter
900700	903900	that I usually denote by p for Bernoulli lives.
903900	906780	And so what I need is to find a distribution
906780	913540	on the interval 0, 1, just like this guy.
913540	915660	The problem with the Gaussian is that it's not
915660	917940	on the interval 0, 1.
917940	920260	It's going to spill out in the end.
920260	922780	And it's not going to be something that works for me.
922780	924780	And so the question is, I need to think
924780	927060	about distributions that are probably continuous.
927060	930100	Why would I restrict myself to discrete distributions that
930100	931500	are actually convenient?
931500	934060	And for Bernoulli, one that's actually
934060	936740	basically the main tool that everybody's using
936740	939700	is this so-called beta distribution.
939700	942220	So the beta distribution has two parameters.
950620	959100	So x follows a beta with parameters, say a and b,
959100	969060	if it has a density, f of x is equal to x to the a minus 1,
969060	978060	1 minus x to the b minus 1, if x is in the interval 0, 1,
978060	980300	and 0 for all other x's.
981260	988220	So why is that a good thing?
988220	991860	Well, it's a density that's on the interval 0, 1, for sure.
991860	993740	But now I have these two parameters.
993740	996060	And the set of shapes that I can get
996060	1000300	by tweaking those two parameters is incredible.
1000300	1004340	I mean, it's going to be a unimodal distribution.
1004340	1005700	It's still fairly nice.
1005700	1007740	It's not going to be something that goes like this and this,
1007740	1014060	because if you think about this, what would it mean if your prior
1014060	1019620	distribution on the interval 0, 1 had this shape?
1019620	1021940	It would mean that maybe you think that p is here,
1021940	1023340	or maybe you think that p is here,
1023340	1025300	or maybe you think that p is here,
1025300	1028780	which essentially mean that you think that p can come maybe
1028780	1030900	from three different phenomena.
1030900	1033260	And there's other models that are called mixtures for that
1033260	1035100	that directly account for the fact
1035140	1041100	that maybe there are several phenomena that are aggregated in your data set.
1041100	1043460	But if you think that your data set is sort of pure
1043460	1045700	and that everything comes from the same phenomenon,
1045700	1048900	you want something that looks like maybe like this,
1048900	1052900	or maybe looks like this, or maybe is sort of symmetric.
1052900	1054460	You want to get all this stuff, right?
1054460	1058580	Maybe you want something that says, well, if I'm talking about p
1058580	1064860	being the probability of the proportion of women in the whole world,
1064860	1068620	you want something that's probably really spiked around 1-1.5,
1068620	1070500	almost the point mass, because you know,
1070500	1075020	I mean, OK, let's agree that 0.5 is the actual number.
1075020	1078980	So you want something maybe that says, OK, maybe I'm wrong,
1078980	1081140	but I'm sure I'm not going to be really that way off.
1081140	1083340	And so you want something that's really pointy.
1083340	1086700	But if it's something you've never checked, right?
1086700	1089820	And again, I cannot make references at this point,
1089820	1092820	but something where you might have some uncertainty,
1092820	1094300	then that should be around 1-1.5.
1094300	1096700	Maybe you want something that's like a little more,
1096700	1099420	allows you to say, well, I think there's more around 1-1.5,
1099420	1102980	but there's still some fluctuations that are possible, OK?
1102980	1105180	And in particular here, I talk about p
1105180	1109340	where the two parameters, a and b, are actually the same.
1109340	1110900	I call them a.
1110900	1113540	One is called scale, the other one's called shape.
1113540	1115420	Oh, by the way, sorry, this is not a density,
1115420	1118780	so it actually has to be normalized, right?
1118780	1120180	When you integrate this guy, it's going
1120180	1122460	to be some function that depends on a and b, actually.
1122460	1125460	Depends on this function through the beta function, right?
1125460	1127220	Which is this combination of gamma function.
1127220	1129900	So that's why it's called beta distribution.
1129900	1133500	But well, that's the definition of the beta function
1133500	1135180	when you integrate this thing anyway.
1135180	1137020	So I mean, you just have to normalize it.
1137020	1139740	It's just a number that depends on the nb, OK?
1139740	1141780	So here, if you take a equal to b,
1141780	1143100	you have something that essentially is
1143100	1145340	symmetric around 1-1.5, right?
1145340	1147060	Because what does it look like?
1147060	1148020	Well, it's something.
1148020	1151020	So my density f of x is going to be what?
1151020	1159220	It's going to be my constant times x times 1 minus x
1159220	1161660	to the a minus 1, right?
1161660	1166100	And this function, x times 1 minus x looks like this.
1166100	1167740	We've drawn it before, right?
1167740	1169420	That was something that showed up
1169420	1176500	as being the variance of my Bernoulli.
1176500	1180980	So we know it's something that takes its maximum at 1-1.5,
1181980	1184220	and now I'm just taking the power of this guy.
1184220	1186020	So I'm really just distorting this thing
1186020	1193340	into some fairly symmetric manner, OK?
1193340	1200340	So this distribution that we actually take for p, right?
1200340	1202540	So here, I assume that p, the parameter, right?
1202540	1204260	I mean, notice that this is kind of weird.
1204260	1206060	First of all, this is probably the first time
1206060	1209580	in this entire course that we have this has this something
1209580	1212340	has a distribution when it's actually a lower case letter.
1212340	1213740	That's something you have to deal with,
1213740	1216940	because we've been using lower case letters for parameters,
1216940	1218700	and now we want them to have a distribution.
1218700	1220460	So that's what's going to happen, all right?
1220460	1223860	And this is called the prior distribution, OK?
1223860	1225300	So really, I should write something
1225300	1234060	like f of p is equal to a constant times p 1 minus p
1234060	1235220	to the a minus 1.
1235220	1238100	Well, no, actually, I should not, because then it's confusing.
1238100	1239980	OK, so let me not do this.
1239980	1242140	One thing in terms of notation that I'm going to write,
1242140	1243940	I'm going to write, when I have a constant here,
1243940	1245340	and I don't want to make it explicit,
1245340	1248500	and we'll see in a second why I don't need to make it explicit,
1248500	1259300	I'm going to write this as f of x is proportional to x 1
1259300	1264060	minus x to the a minus 1, OK?
1264060	1268780	So that's just to say equal to some constant that does not
1268780	1271540	depend on x times this thing, OK?
1276340	1280660	So if we continue with our experiment, now if p,
1280660	1282900	right, so that's the experiment where I'm trying to,
1282900	1286380	I'm drawing this data x 1 to x n, which is Bernoulli p,
1286380	1289900	if p has some distribution, it's not clear
1289900	1292700	what it means to have a Bernoulli with some random parameter.
1292700	1295020	So what I'm going to do is then I'm going to first draw my p.
1295020	1298700	Let's say I get a number 0.52, and then I'm
1298700	1301140	going to draw my data conditionally on p, all right?
1301140	1305820	So here comes the first and last flowchart of this class.
1305820	1309540	So I'm going to first, all right?
1309540	1312820	So nature first draws p, OK?
1312820	1318420	So p follows, say, some beta AA.
1318420	1326900	Then I condition on p, and then I draw x 1, x n, that are i,
1326900	1330780	i, d, Bernoulli p.
1330780	1334380	Everybody understand the process of generating this data, right?
1334380	1336260	So you first draw a parameter, and then you just
1336260	1341180	flip those independent bias coins with this particular p.
1341180	1345260	So there's this layered thing.
1345260	1348660	So now, conditionally on p, right?
1348660	1351980	So here, I have this prior about p, which was the thing.
1351980	1354140	So this is just the thought process again, right?
1354140	1356420	It's not anything that actually happens in practice.
1356420	1359940	This is my way of thinking about how the data was generated.
1359940	1363300	And from this, I'm going to try to come up with some procedure.
1363300	1367820	Just like if your estimator is the average of the data,
1367820	1369740	you don't have to understand probability
1369740	1372380	to say that my estimator is the average of the data, right?
1372380	1374220	I mean, anyone outside this room understand
1374260	1378540	that the average is a good estimator for some average behavior,
1378540	1381540	and they don't need to think of the data as being
1381540	1383020	a random variable, et cetera.
1383020	1385060	So same thing, basically.
1385060	1386460	Now, we will see.
1386460	1388060	I mean, actually, we won't.
1388060	1390780	But in this case, well, we will.
1390780	1393100	In this case, you can see that, essentially, the posterior
1393100	1396700	distribution is still a beta, all right?
1396700	1400100	So what it means is that I had this thing,
1400100	1403140	then I observed my data, and then I continue.
1403140	1411820	And here, I'm going to update my prior
1411820	1416700	into some posterior distribution, pi.
1416700	1420900	And here, this guy is actually also a beta, all right?
1420900	1425460	So pi now, P, my posterior distribution on P,
1425460	1428260	is also a beta distribution with the parameters that
1428260	1431700	are on this slide, and I'll have space to reproduce them.
1431700	1434500	So I start the beginning of this flow chart
1434500	1437140	as having P, which is a prior.
1437140	1438820	I'm going to get some observations,
1438820	1443900	and then I'm going to update what my posterior is, OK?
1443900	1446940	So this posterior is basically something
1446940	1450100	that's in Bayesian statistics was beautiful,
1450100	1453780	is as soon as you have the distribution,
1453780	1457060	it's essentially capturing all the information about the data
1457060	1460380	that you want for P. And it's not just a point, right?
1460380	1461500	It's not just an average.
1461500	1463660	It's actually an entire distribution
1463660	1467060	for the possible values of theta.
1467060	1470740	And it's not the same thing as saying, well, you know,
1470740	1474820	if theta hat is equal to x and bar in the Gaussian case,
1474820	1477180	I know that this is some mean mu,
1477180	1479700	and then maybe it has variance sigma square over n.
1479700	1482940	That's not what I mean by this is my posterior distribution,
1482940	1483580	right?
1483580	1486660	This is not what I mean.
1486660	1488700	This is going to come from this guy, right?
1488700	1491340	The Gaussian thing and the central limit theorem.
1491340	1492980	But what I mean is this guy.
1492980	1498180	And this came exclusively from the prior distribution.
1498180	1500860	If I had another prior, I would not necessarily
1500860	1503740	have a beta distribution on the output.
1503740	1507580	So when I have the same family of distributions
1507580	1511100	at the beginning and at the end of this flow chart,
1511100	1521220	I say that beta is a conjugate prior,
1521220	1523980	meaning I put in beta as a prior,
1523980	1527420	and I get betas at posterior.
1527420	1530900	And that's why betas are so popular.
1530900	1532300	Conjugate priors are really nice,
1532300	1535340	because you know that whatever you put in,
1535340	1537220	what you're going to get in the end is a beta.
1537220	1538820	So all you have to think about is the parameters.
1538820	1541060	You don't have to check again what the posterior is
1541060	1543300	going to look like, what the PDF of this guy is going to be.
1543300	1544500	You don't have to think about it.
1544500	1546660	You just have to check what the parameters are.
1546660	1548300	And there's families of conjugate priors.
1548300	1551180	Gaussian gives Gaussian, for example.
1551180	1552220	There's a bunch of them.
1552220	1556420	And this is what drives people into using specific priors
1556420	1558260	as opposed to other.
1558260	1560700	It has nice mathematical properties.
1560700	1564180	Nobody believes that the P distribution is really
1564180	1567460	distributed according to beta, but it's flexible enough
1567460	1570540	and super convenient mathematically.
1570540	1573900	All right, so now let's see for one second
1573900	1575780	before we actually go any further.
1575780	1577860	What I did, so A and B, I didn't mention it.
1578740	1584140	And here, A and B are positive numbers.
1584140	1587540	OK, they can be anything positive.
1587540	1591980	So here what I did is that I updated A into A plus the sum
1591980	1598420	of my data, and B into B plus and minus the sum of my data.
1598420	1602220	So that's essentially A becomes A plus the number of ones,
1602220	1607380	and B becomes B. Well, that's only when I have A and A, right?
1607420	1610220	So the first parameters become itself plus the number of ones,
1610220	1612660	and the second one becomes itself plus the number of zeros.
1615460	1619180	And so just as a sanity check, what does this mean?
1619180	1628980	If A goes to 0, what is the beta when A goes to 0?
1628980	1631700	We can actually read this from here, right?
1631700	1646140	So we had A, sorry, actually, let's take A goes to, no, actually,
1646140	1647340	sorry, let's just do this.
1657540	1658700	OK, let's not do this now.
1658700	1660860	I'll do it when we talk about non-informative prior,
1660900	1664220	because it's a little too messy here.
1664220	1667980	OK, so how do we do this?
1667980	1671380	How did I get this posterior distribution given the prior?
1671380	1673100	How do I update this?
1673100	1676060	Well, this is called Bayesian statistics,
1676060	1678820	and you've heard this word Bayes before,
1678820	1682020	and the way you've heard it is in the Bayes formula, right?
1682020	1683700	What was the Bayes formula?
1683700	1688060	The Bayes formula was telling you that the probability of A given
1688060	1692500	B was equal to something that depended on the probability
1692500	1693620	of B given A, right?
1693620	1694780	That's what it was.
1694780	1698660	And I mean, you can actually either remember the formula,
1698660	1700580	you can remember the definition, and this
1700580	1706420	is what P of A and B divided by P of B. So this
1706420	1715540	is P of B given A times P of A divided by P of B, right?
1715540	1720100	That's what Bayes formula is telling you, agree?
1720100	1722620	So now what I want is to have something
1722620	1728140	that's telling me how this is going to work, OK?
1728140	1734460	So what is going to play the role of those events, A and B?
1734460	1742020	Well, one is going to be the distribution of my parameter theta
1742020	1744500	given that I see the data, and this
1744500	1746540	is going to tell me what is the distribution of the data
1746540	1749260	given that I know what my parameter theta is.
1749260	1752660	But that part, if this is data and this is the parameter theta,
1752660	1755700	this is what we've been doing all along.
1755700	1758700	The distribution of the data given the parameter here
1758700	1763140	was n i i d Bernoulli P. I know that.
1763140	1767940	I know exactly what their joint probability mass function is.
1767940	1769300	Then that was what?
1769300	1772700	So we said that this is going to be my data,
1772700	1777260	and this is going to be my parameter, OK?
1777260	1780180	So that means that this is the probability of my data
1780180	1780940	given the parameter.
1780940	1782980	This is the probability given the parameter.
1782980	1785820	This is the probability of the parameter.
1785820	1786300	What is this?
1786300	1789340	What did we call this?
1789340	1790260	This is the prior.
1790260	1793580	It's just the distribution of my parameter.
1793580	1795900	Now, what is this?
1795900	1798860	Well, this is just the distribution of the data itself,
1798860	1799500	all right?
1799540	1808260	So this is essentially the distribution of this if this
1808260	1815100	was indeed not conditioned on P, right?
1815100	1818700	So if I don't condition on P, this data
1818700	1823260	is going to be a bunch of i i d Bernoulli
1823260	1825460	with some parameter, but the parameter is random, right?
1825460	1827820	So for different realization of this data set,
1827940	1830100	I'm going to get different parameters for the Bernoulli.
1830100	1834540	And so that leads to some sort of convolution.
1834540	1836140	I mean, it's not really a convolution in this case,
1836140	1838660	but it's some sort of composition of distributions, right?
1838660	1841260	I have the distribution, the randomness that comes from here,
1841260	1844340	and then the randomness that comes from realizing the Bernoulli.
1844340	1846260	So that's just the marginal distribution,
1846260	1848700	and it actually might be painful to understand what this is,
1848700	1849540	right?
1849540	1851140	I mean, in a way, it's sort of a mixture,
1851140	1852900	and it's not super nice.
1852900	1855820	But we'll see that this actually won't matter for us.
1855820	1857060	This is going to be some number.
1857060	1858540	It's going to be there, but it won't
1858540	1860940	matter for us what it is, because it actually
1860940	1862460	does not depend on the parameter,
1862460	1866220	and that's all that matters to us.
1866220	1870900	OK, so let's put some names on those things, right?
1870900	1873100	I mean, this was very informal, so let's
1873100	1879660	put some actual names on what we want to call what we call prior.
1879660	1882220	So what is the formal definition of a prior?
1882220	1884860	What is the formal definition of a posterior?
1884980	1887500	And what are the rules to update it, OK?
1887500	1890380	So I'm going to have my data, which is going to be x1, xn.
1893620	1898540	And so let's say they're IID, but they don't actually have to.
1898540	1901340	And so I'm going to have given theta.
1907460	1909340	And when I say given, it's either given
1909340	1911900	like I did in the first part of this course
1911900	1914420	in all previous chapters or conditionally on, right?
1914420	1917540	So if you're thinking like a Bayesian,
1917540	1919820	what I really mean is conditionally
1919820	1922180	on this random parameter, OK?
1922180	1925020	So it's like as if it was a fixed number.
1925020	1930140	Then they're going to have the distribution, x1, xn,
1930140	1931860	is going to have some distribution.
1931860	1939180	Let's say, let's assume for now it's pdf, pn of x1, xn, OK?
1939180	1942260	And I'm going to write theta like this.
1942260	1944060	So for example, what is this?
1944580	1947140	So let's say this is a pdf.
1947140	1948140	It could be a pmf.
1948140	1951260	Everything I say, I'm going to think of them as being pdfs.
1951260	1952940	I'm going to combine pdfs with pdf,
1952940	1955660	but I could combine pdf with pmfs, pmf with pdfs,
1955660	1957460	or pmf with pms, OK?
1957460	1961140	So everywhere you see a d, it could be an m.
1961140	1962660	All right, so now I have those things.
1962660	1963860	So what does that mean?
1963860	1973900	So here's some example, x1, xn are IID and theta1, right?
1973900	1977420	So now I know exactly what the joint pdf of this thing is.
1977420	1983740	So it means that pn of x1, xn given theta is equal to what?
1983740	1989380	Well, it's like 1 over sigma root, sorry, 1 root 2 pi
1989380	1993820	to the power n e to the minus sum from i
1993820	1998500	equal 1 to n of xi minus theta squared divided by 2, right?
1998500	2001900	So that's just the joint distribution of n iID and theta
2001900	2005060	1 random variables, OK?
2005060	2007220	So that's my pn given theta.
2007220	2011620	Now, this is what we denoted by sort of like f sub theta
2011620	2013340	before, right?
2013340	2015860	We had this subscript before, but now we just
2015860	2017420	put a bar in theta because we want
2017420	2020700	to remember that this is actually conditioned on theta, right?
2020700	2022140	But this is just notation.
2022140	2026060	You should just think of this as being just the usual thing
2026060	2030740	that you get from some statistical model, all right?
2030740	2034540	So now, that's going to be pn.
2034540	2040140	And here, I'm going to assume that theta is,
2040140	2041420	why do I put pi here?
2048780	2050820	OK.
2050820	2059540	So theta has prior distribution pi.
2060820	2068980	OK, so for example, so think of it as either PDF or PMF again.
2068980	2073860	So for example, pi of theta was what?
2073860	2080260	Well, it was some constant times theta to the a minus 1,
2080260	2083780	1 minus theta to the a minus 1, right?
2083780	2089060	So it has some prior distribution, and that's another PMF.
2089100	2092340	So now, I'm given the distribution of my x's given theta.
2092340	2093940	I'm given the distribution of my theta,
2093940	2097420	so I'm given this guy, right?
2097420	2100140	That's this guy.
2100140	2105420	I'm given that guy, which is my pi, right?
2105420	2111740	So that's my pn of x1, xn given theta.
2111740	2114020	That's my pi of theta.
2114020	2117340	And then I have here just, this is what?
2117340	2127260	Well, this is just the integral of pn x1, xn times pi of theta d
2127260	2128340	theta, right?
2128340	2129860	Overall possible sets of theta.
2129860	2133420	That's just when I integrate out my theta,
2133420	2135820	or I compute, say, the marginal distribution,
2135820	2137340	I get this by integrating, right?
2137340	2140500	That's just basic probability, conditional probabilities,
2140500	2141000	right?
2141000	2142620	Then if I had the PMF, I would just
2142620	2145860	sum over the values of theta's, OK?
2148340	2155220	So now, what I want is to find what's called,
2155220	2158940	so that's the prior distribution.
2158940	2161060	And I want to find the posterior distribution.
2161060	2178820	So it's called, it's pi of theta given x1, xn.
2181540	2183540	And so if I use Bayes' rule, I know
2183540	2194620	that this is pn of x1, xn given theta times pi of theta.
2194620	2197540	And then it's divided by the distribution
2197540	2201100	of those guys, which I will write as integral over theta
2201100	2208860	of pn x1, xn given theta times pi of theta d theta.
2214220	2217180	Everybody's with me still?
2217180	2219220	So if you're not comfortable with this,
2219220	2223020	it means that you probably need to go read your couple pages
2223020	2225820	on conditional densities and conditional PMFs
2225820	2227420	from your probability class.
2227420	2228980	There's really not much there.
2228980	2232220	It's just a matter of being able to define those quantities.
2232220	2235620	F density of x given y, this is just
2235620	2237260	what's called a conditional density.
2237260	2238820	You need to understand what this object is
2238820	2241940	and how it relates to the joint distribution of x and y,
2241940	2244820	or maybe the distribution of x or the distribution of y.
2247380	2248900	But it's the same rules.
2248900	2251420	I mean, one way to actually remember this
2251420	2253700	is this is exactly the same rules as this.
2253700	2255780	When you see a bar, it's the same thing
2255780	2257700	as the probability of this and this guy.
2257700	2262020	So for densities, it's just a comma divided by the second guy.
2262020	2265260	The probability of the second guy, that's it.
2265260	2266980	So if you remember this, you can just
2266980	2269980	do some pattern matching and see what I just wrote here.
2270740	2271740	OK?
2271740	2273020	OK.
2273020	2276940	So now I can compute every single one of these guys.
2276940	2283980	This is something I get from my modeling.
2283980	2285260	So I did not write this.
2285260	2289060	It's not written in the slides.
2289060	2294740	But I give a name to this guy that was my prior distribution.
2294740	2296500	And that was my posterior distribution.
2300980	2306980	In chapter 3, maybe, what did we call this guy?
2311980	2313580	Well, the one that does not have a name.
2313580	2319580	And that's in the box, this guy.
2319580	2320300	How did we call it?
2327300	2329220	It is the joint distribution of the x i's.
2330980	2334780	And we give you the name.
2334780	2336140	It's the likelihood, right?
2336140	2337660	This is exactly the likelihood.
2337660	2339140	This was the likelihood of theta.
2343940	2346380	And this is something that's very important to remember.
2346380	2350300	And that really reminds you that these things are really
2350300	2352940	not that different, maximum likelihood estimation
2352940	2354020	and Bayesian estimation.
2354020	2358580	Because your posterior is really just your likelihood
2358580	2362820	times something that's just putting some weights on the
2362820	2366340	Thetas, depending on where you think theta should be.
2366340	2368700	So if I had, say, a maximum likelihood estimator in my
2368700	2371140	likelihood and theta looked like this.
2371140	2373460	But my prior in theta looked like this.
2373460	2376980	I said, oh, I really want Thetas that are like this.
2376980	2379460	So what's going to happen is that I'm going to turn this
2379460	2381380	into some posterior that looks like this.
2384420	2387620	So I'm just really weighting this posterior.
2387620	2389940	This is a constant that does not depend on theta, right?
2389940	2390500	Agreed?
2390500	2393460	I integrated over Thetas, so Thetas go on.
2393460	2396140	So forget about this guy.
2396140	2399420	I have basically that the posterior distribution up to
2399420	2401860	scaling, because it has to be a probability density and not
2401860	2404660	just any function that's positive, is the product of
2404660	2405140	this guy.
2405140	2406980	It's a weighted version of my likelihood.
2406980	2407900	That's all it is.
2407900	2410580	I'm just weighting the likelihood using my
2410580	2413140	prior belief on theta.
2413140	2417220	And so given this guy, a natural estimator, if you
2417220	2421100	follow the maximum likelihood principle, would be the
2421100	2423220	maximum of this posterior.
2423220	2424620	Agreed?
2424620	2428620	That would basically be doing exactly what a maximum
2428620	2431660	likelihood estimation is telling you.
2431660	2433540	So it turns out that you can.
2433540	2435340	It's called maximum a posteriori.
2435340	2439300	And I won't talk much about this or map.
2439300	2444500	So that's maximum a posteriori.
2444500	2450500	So it's just the theta hat is the arg max of pi theta given
2450500	2451500	x1, xn.
2455180	2456220	It sounds like it's OK.
2456220	2459740	I give you a density and you say, OK, I have a density for
2459740	2460980	all values of my parameters.
2460980	2463340	You're asking me to summarize it into one number.
2463340	2466580	I'm just going to take the most likely number of those guys.
2466580	2468300	But you could summarize it otherwise.
2468300	2470780	You could take the average, right?
2470780	2472460	You could take the median.
2472460	2474380	You could take a bunch of numbers.
2474380	2476820	And the beauty of Bayesian statistics is that you don't
2476820	2479180	have to take any number in particular.
2479180	2481500	You have an entire posterior distribution.
2481500	2485380	This is not only telling you where theta is, but it's
2485380	2489980	actually telling you the difference if you actually
2489980	2493100	give as something, it gives you the posterior, right?
2493100	2496300	So now let's say the theta is a p between 0 and 1.
2496300	2500380	If my posterior distribution looks like this, or if my
2500380	2504020	posterior distribution looks like this, then those two
2504020	2507660	guys have one the same mode, right?
2507660	2509220	This is the same value.
2509220	2511700	And they're symmetric, so they also have the same mean.
2511700	2513580	So these two posterior distributions give me the
2513580	2515540	same summary into one number.
2515540	2518740	However, clearly one is much more confident than the other
2518740	2523540	one, so I might as well just speed that as a solution.
2523540	2525220	Some people can, you can do even better.
2525220	2529620	People actually do things such as drawing a random number
2529620	2530540	from this distribution.
2530540	2532780	So this is my number.
2532780	2534940	Well, that's kind of dangerous, but you can imagine you
2534940	2537060	could do this, right?
2537060	2539700	All right.
2539700	2542140	So this is what works.
2542140	2543580	That's what we went through.
2543580	2548300	So here, as you notice, I don't care so much about this
2548300	2550220	part here, right?
2550220	2551700	Because it does not depend on theta.
2551700	2555180	So I know that given the product of those two things,
2555180	2557740	this thing is only the constant that I need to divide so
2557740	2560140	that when I integrate this thing over theta, it
2560140	2561460	integrates to one.
2561460	2564780	Because this has to be a probability density on theta.
2564780	2568020	So I can write this and just forget about that part, and
2568020	2571940	that's what's right on the top of this slide.
2571940	2577860	Just this notation, this sort of weird alpha or, I don't know,
2577860	2580500	infinity sine crop to the right, whatever you want to call
2580500	2584420	this, this thing is actually just really emphasizing the
2584420	2586260	fact that I don't care.
2586260	2593500	I write it because I can, and you know what it is, but you
2593500	2598740	don't actually have to, well, in some instances, you have to
2598740	2600540	compute the integral, in some instances, you don't have to
2600540	2603300	compute the integral, and a lot of Bayesian computation is
2603300	2606700	about saying, OK, it's actually really hard to compute this
2606700	2608260	integral, so I'd rather not doing it.
2608260	2611540	So let me try to find some methods that allow me to
2611540	2614540	sample from the posterior distribution without having to
2614540	2617660	compute this, and that's what's called Monte Carlo Markov
2617660	2620420	chains, or MCMC, and that's exactly what they're doing.
2620420	2622860	They're just using only ratios of things like that for
2622860	2626140	different datas, and which means that if you take ratios, the
2626140	2628500	normalizing constant is gone, and you don't need to find this
2628500	2630780	integral.
2630780	2633140	So we won't go into those details at all.
2633140	2636140	That would be the purpose of an entire course on Bayesian
2636140	2636580	inference.
2636580	2640300	Actually, even Bayesian computations would be an
2640300	2642660	entire course on its own.
2642660	2644340	There's some very interesting things that are going on
2644340	2648100	there, the interface of stats and computation.
2648100	2652020	All right, so let's go back to our example and see if we
2652020	2653900	can actually compute any of those things, because it's very
2653900	2657780	nice to give you some data, some formulas, but let's see if
2657780	2659820	we can actually do it.
2659820	2664020	In particular, can I actually recover this claim that the
2664020	2671220	posterior associated to a beta prior with Bernoulli
2671260	2674980	likelihood is actually giving me a beta again.
2674980	2676700	All right, so what was my prior?
2681220	2685940	Well, it was beta, so P was following a beta AA, which
2685940	2696860	means that P, the density, so that was pi of theta, well, I
2696940	2702860	am going to write it as pi of P, was proportional to P to the
2702860	2708780	A minus 1 times 1 minus P to the A minus 1, right?
2708780	2710420	So that's the first ingredient I need to
2710420	2711380	compute my posterior.
2711380	2714300	I really need only two, if I wanted about up to constant.
2714300	2721860	The second one was P. Well, we've computed that many
2721860	2725540	times, and we had even a nice compact way of writing it,
2725540	2732540	which was that Pn of x1, xn, given a parameter P, right?
2732540	2735780	So the density, the joint density of my data given P,
2735780	2738580	that's my likelihood, the likelihood of P, was what?
2738580	2745380	Well, it was P to the sum of the xis, 1 minus P to the n
2745380	2747140	minus sum of the xis.
2750900	2754660	Anybody wants me to parse this more, or do you remember
2754660	2757060	seeing that from maximum likelihood estimation?
2757060	2757540	Yeah?
2757540	2760660	So when you condition on random variables, you really just
2760660	2763780	treat that random variable with something that's
2763780	2764300	in the middle.
2764300	2765260	That's what conditioning does.
2769660	2770460	OK?
2770460	2771140	Yeah?
2771140	2779700	On the previous slide, for the bottom there, it's d pi of T.
2779700	2783380	Can it be d pi of T, or is it?
2783460	2787180	So d pi of T is a measure theoretic notation,
2787180	2789900	which I use without thinking, and I should not,
2789900	2792420	because I can see it upsets you.
2792420	2795060	D pi of T is just a natural way to say
2795060	2798020	that I integrate against whatever
2798020	2803940	I'm given for the prior of theta.
2803940	2807860	In particular, if theta is just the mix of a PDF
2807860	2809620	and a point mass, right?
2809620	2814460	Maybe I say that my P takes value 0.5 with probability 0.5,
2814460	2817380	and then is uniform on the interval with probability
2817380	2818420	0.5.
2818420	2821980	OK, so for this, I neither have a PDF nor a PMF,
2821980	2824180	but I can still talk about integrating with respect
2824180	2824980	to this, right?
2824980	2830020	It's going to look like if I take a function f of T, d pi of T,
2830020	2834540	is going to be 1 half of f of 1 half, right?
2834540	2837620	That's the point mass with probability 1 half at 1 half,
2837620	2843140	plus 1 half of the integral between 0 and 1 of f of T, d T.
2843140	2846020	So this is just a notation, which is actually,
2846020	2852460	funnily enough, is interchangeable with pi of d T.
2852460	2857860	But if you have a density, it's really just the density pi
2857860	2861460	of T, d T, if pi is really a density.
2861460	2864660	But that's when pi is a measure in other density.
2864700	2869220	But so everybody else forget about this.
2869220	2872180	I mean, this is not something you should really worry about.
2872180	2874420	At this point, this is more graduate level probability
2874420	2875980	classes.
2875980	2877260	But yeah, it's called measure theory,
2877260	2879180	and that's when you think of pi as being a measure.
2879180	2880380	In an abstract fashion, you don't have
2880380	2882780	to worry whether it's a density or not,
2882780	2885700	or whether it has a density even.
2885700	2889580	OK, so everybody's OK with this?
2889900	2897380	All right, so now I need to compute my posterior.
2897380	2903140	And as I said, my posterior is really
2903140	2908300	just the product of the likelihood weighted by the prior.
2908300	2912980	So hopefully, at this stage of your education,
2912980	2915420	you can multiply two functions.
2915420	2918260	So what's happening is if I multiply this guy with this guy,
2918260	2922900	well, p gets this guy to the power of this guy plus this guy.
2933780	2940060	And then 1 minus p gets the power n minus sum of xi's.
2940060	2942900	So this is always from i equal 1 to n,
2942900	2944540	and then plus a minus 1 as well.
2949060	2951620	And this is, sorry, this is up to constant,
2951620	2955540	because I still need to solve this.
2955540	2958420	And I could try to do it, but I really don't have to,
2958420	2963900	because I know that if my density has this form,
2963900	2965580	then it's a beta distribution.
2965580	2967020	And then I can just go on Wikipedia
2967020	2969180	and see what should be the normalization factor.
2969180	2971060	But I know it's going to be a beta distribution.
2971060	2973900	It's actually the beta with parameter.
2973900	2977500	So this is really my beta with parameter
2977540	2983620	sum of xi i equal 1 to n plus a minus 1.
2983620	2990660	And then the second parameter is n minus sum of the xi's plus a minus 1.
2990660	2992300	OK?
2992300	2995060	Sorry.
2995060	2998580	I just wrote what was here.
2998580	3001620	Oh, what happened to my 1?
3001620	3002940	Oh, no, sorry, sorry, sorry.
3002940	3005660	Beta has the power minus 1, right?
3005700	3009140	So that's the parameter of the beta.
3009140	3011780	And this is the parameter of the beta, right?
3011780	3015060	So beta, well, I don't think it's anywhere.
3015060	3016260	Yeah, beta is over there, right?
3016260	3020300	So I just replace a by what I see.
3020300	3022340	a is just becoming this guy plus this guy,
3022340	3023700	and this guy plus this guy.
3026460	3028500	Everybody's comfortable with this computation?
3029020	3038180	All right, so we just agreed that beta priors for Bernoulli
3038180	3041500	observations are certainly convenient, right?
3041500	3043980	And because they're just conjugate,
3043980	3046300	and we know that's what's going to come out in the end,
3046300	3048300	that's going to be a beta as well.
3048300	3050180	So I mean, I just claim it was convenient.
3050180	3052340	It was certainly convenient to compute this, right?
3052340	3055820	I mean, there was certainly some compatibility
3055820	3057980	when I had to multiply this function by that function,
3057980	3061020	and you can imagine that things could go much more wrong
3061020	3063220	than just having p to some power and p to some power,
3063220	3066380	1 minus p to some power, 1 minus p to some power.
3066380	3069100	Things were nice.
3069100	3071860	Now, this is nice, but I can also question the following
3071860	3072340	things.
3072340	3074100	Why beta, for one?
3074100	3076820	I mean, the beta tells me something,
3076820	3080780	but that's convenient, but then how do I pick a?
3080780	3085420	I know that a should definitely capture
3085420	3090180	the fact that where I want to have my p most likely located,
3090180	3094540	but it also actually captures the variance of my beta.
3094540	3096740	And so choosing different a's is going
3096740	3097940	to have different functions.
3097940	3103260	If I have a and b, if I started with the beta with parameter
3103260	3106500	here, I started with a b here.
3106500	3109940	I would just pick up the b here, agreed?
3109940	3111300	And that would just be asymmetric,
3111300	3113820	but they're going to capture mean and variance of this thing.
3113820	3115740	And so how do I pick those guys?
3115740	3118860	I mean, if I'm a doctor and you're
3118860	3121540	asking me, what do you think the chances of this drug working
3121540	3123460	on this kind of patients is, and I
3123460	3126100	have to say to spit out the parameters of a beta for you,
3126100	3128700	it might be a bit of a complicated thing to do.
3128700	3130820	So how do you do this, especially for problems?
3130820	3136180	So by now, people have actually mastered the art of coming up
3136180	3139220	with how to formulate those numbers,
3139220	3141580	but in new problems that come up, how do you do this?
3141620	3143900	What happens if you want to use Bayesian methods,
3143900	3148220	but you actually do not know what you expect to see?
3148220	3151060	Maybe this is the first time you've, I mean, to be fair,
3151060	3153260	before we started this class, I hope all of you
3153260	3156460	had no idea whether people tended to bend their head
3156460	3158260	to the right or to the left before kissing,
3158260	3160580	because if you did, well, you have too much time on your hand
3160580	3162340	and I should double your homework.
3162340	3166220	And so in this case, you have to sort of,
3166220	3168860	maybe you still want to use the Bayesian machinery.
3168860	3171020	Maybe you just want to do something nice.
3171020	3171700	It's nice, right?
3171700	3173300	I mean, it worked out pretty well.
3173300	3174660	And so what if you want to do well,
3174660	3176340	you actually want to use some priors that
3176340	3179820	have no carry, no information that basically do not
3179820	3182660	prefer any theta to another theta.
3182660	3186660	Now, you could read this slide or you could look at this formula.
3190020	3194940	We just said that this pi here was just here
3194940	3197820	to weigh some thetas more than others,
3197820	3199900	depending on our prior belief.
3199900	3202420	If our prior belief does not want to put any preference
3202420	3205500	towards some thetas than to others, what do I do?
3208100	3209500	Yeah, remove it.
3209500	3211460	And the way to remove something we multiply by
3211460	3212660	is just replace it by 1.
3212660	3214820	That's really what we're doing.
3214820	3218460	So if this was a constant, then not depending on theta,
3218460	3221420	then that would mean that we're not preferring any theta.
3221420	3224380	And we're looking sort of at the likelihood,
3224380	3226580	but not as a function that we're trying to maximize,
3226580	3230220	but as a function that we normalize in such a way
3230220	3232500	that it's actually a distribution.
3232500	3234780	So if I have pi, which is not here,
3234780	3236820	this is really just taking the likelihood, which
3236820	3239180	is a positive function, mean not integrate to 1.
3239180	3242340	So I normalize it so that it integrates to 1.
3242340	3245140	And then I just say, well, this is my posterior distribution.
3245140	3246780	Now, I could just maximize this thing
3246780	3249100	and spit out my maximum likelihood estimator,
3249100	3250980	but now I can also integrate and find
3250980	3252380	what the expectation of this guy is.
3252380	3254220	I can find what the median of this guy is.
3254220	3256380	I can sample data from this guy.
3256380	3259460	I can understand what the variance of this guy is,
3259460	3261700	which is something we did not do when we just
3261700	3263660	did maximum likelihood estimation because,
3263660	3265620	given a function, all we cared about
3265620	3270020	was the arg max of this function.
3270020	3275860	So this priors are called uninformative.
3275860	3280100	So this is just replacing this number by 1.
3280100	3284020	And if I have a or by a constant, because it still
3284020	3285700	has to be a density.
3285700	3290420	And so if I have a bounded set, I'm
3290420	3293540	just looking for the uniform distribution on this bounded
3293540	3296620	set, the one that puts constant one
3296620	3299140	over the size of this thing.
3299140	3301660	But if I have an unbounded set, what
3301660	3303900	is the density that takes a constant value
3303900	3307580	on the entire real line, for example?
3307580	3308500	What is this density?
3316580	3318300	Doesn't exist.
3318300	3320860	I mean, it just doesn't exist.
3320860	3323500	The way you can think of it is a Gaussian with the variance
3323500	3326380	going to infinity, maybe, or something like this.
3326380	3327900	But you can think of it in many ways.
3327900	3332380	You can think of the limit of the uniform between minus t
3332380	3334220	and t with t going to infinity.
3334220	3336500	But this thing is actually 0.
3336500	3338340	There's nothing there.
3338340	3342020	And so you can actually still talk about this.
3342020	3344220	You could always talk about this thing
3344220	3346580	where you think of this guy as being a constant,
3346580	3348500	remove this thing from this equation,
3348500	3350700	and just say, well, my posterior is just
3350700	3354140	the likelihood divided by the integral of the likelihood
3354140	3354700	over theta.
3354700	3358660	And if theta is the entire real line, so be it.
3358660	3360540	As long as this integral converges,
3360540	3363700	you can still talk about this stuff.
3363700	3366340	And so this is what's called an improper prior.
3369140	3371780	An improper prior is just a non-negative function
3371780	3373700	defined on theta, but it does not
3373700	3379740	have to integrate neither to 1 nor to anything.
3379740	3380940	It does not have to write.
3380940	3382740	If I integrate the function equal to 1
3382740	3384380	on the entire real line, what do I get?
3387820	3389100	Infinity, right?
3389100	3392140	I mean, it's not a proper integral,
3392140	3394180	and so it's not a proper prior, and it's
3394180	3395980	called an improper prior.
3395980	3399380	And those improper priors are usually
3399380	3402860	what you see when you start to want non-informative priors
3402860	3404220	on infinite set status.
3404220	3406780	I mean, that's just the nature of it.
3406780	3410020	You should think of it as being the uniform distribution
3410020	3414620	on some infinite set if that thing were to exist.
3414620	3419180	So let's see some examples about non-informative priors.
3419180	3424340	So if I'm on the interval 0, 1, this is a finite set,
3424340	3428740	so I can talk about the uniform prior on the interval 0, 1
3428740	3431540	for a parameter p of a Bernoulli, OK?
3431540	3447540	And so if I want to talk about this,
3447540	3455100	then it means that my prior is p follows some uniform
3455100	3457580	on the interval 0, 1, OK?
3457580	3465980	So it means that the density is, well, f of x is 1
3465980	3468940	if x is in 0, 1 and 0.
3468940	3472020	Otherwise, there's actually not even a normalization.
3472020	3473900	This thing integrates to 1.
3473900	3476220	And so now, if I look at my likelihood,
3476220	3477340	it's still the same thing.
3477340	3484460	So my posterior becomes theta x1, xn.
3484460	3487180	So that's my posterior.
3487220	3489900	I don't write the likelihood again because we still have it.
3489900	3492500	Well, we don't have it there anymore.
3492500	3493620	Is it here?
3493620	3495340	Or did I just erase it?
3495340	3497740	Yeah, the likelihood is given here.
3497740	3500140	So copy paste over there.
3500140	3503060	And so the posterior is just this thing times 1.
3503060	3504300	So you will see it in a second.
3504300	3507980	So it's p times the sum to the power sum of the xi's,
3507980	3511980	1 minus p to the power n minus sum of the xi's.
3511980	3514700	And then it's multiplied by 1 and then
3514700	3522500	divided by this integral between 0 and 1 of p sum of the xi's,
3522500	3530940	1 minus p n minus sum of the xi's dp,
3530940	3532220	which does not depend on p.
3532220	3536220	And I really don't care what this thing actually is.
3536220	3543580	So now, sorry, that's prior posterior of p.
3543580	3546100	And now I can see, well, what is this?
3546100	3553420	Well, it's actually just the beta with parameters this guy
3553420	3561700	plus 1 and this guy plus 1.
3561700	3578300	So I didn't tell you what the expectation of a beta was.
3578300	3581180	We don't know what the expectation of a beta is.
3581180	3581900	Agreed?
3581900	3585900	I mean, if I wanted to find, say, the expectation of this thing,
3585900	3587660	that would be some good estimator,
3587660	3589820	we know that the maximum of this guy,
3589820	3591180	what is the maximum of this thing?
3594860	3596100	Well, it's just this thing, right?
3596100	3598300	I mean, it's the average of the xi's, right?
3598300	3600300	That's just the maximum likelihood estimator for Bernoulli.
3600300	3601820	We know it's the average.
3601820	3603980	Do you think if I take the expectation of this thing,
3603980	3605220	I'm going to get the average?
3614060	3615820	So actually, I'm not going to get the average.
3615820	3620340	I'm going to get this guy plus this guy divided by n plus 1.
3620340	3624900	So I'm going to do as if I had, oh, sorry.
3624900	3627340	OK, let me not say it like that.
3627340	3628900	Let's look at what this thing is doing.
3628900	3632900	It's looking at the number of 0's, the number of 1's,
3632900	3634540	and it's adding 1.
3634540	3636300	And this guy is looking at the number of 0's,
3636300	3639220	and it's adding 1, OK?
3639220	3641940	Why is it adding this 1?
3641940	3644300	What's going on here?
3644300	3648140	Well, what would happen if I had, so this actually
3648140	3652980	is going to matter mostly when the number of 1's is actually
3652980	3656100	0 or the number of 0's is 0?
3656100	3659980	Because what it does is just pushes the 0 from non-zero.
3659980	3663020	And why is that something that this Bayesian method actually
3663020	3665300	does for you automatically is because when
3665300	3669140	we put this non-informative prior on p, which
3669140	3672020	was uniform on the interval 0, 1, in particular,
3672020	3676700	we know that the probability that p is equal to 0 is 0,
3676700	3679220	and the probability that p is equal to 1 is 0.
3679220	3681220	And so the problem is that, essentially,
3681220	3684540	if I did not add this 1 with some positive probability,
3684540	3687060	I would be allowed to spit out something that actually
3687060	3689900	had p hat, which was equal to 0.
3689900	3693300	In the case, if by chance, let's say I have n is equal to 3,
3693300	3695940	and I get only 0, 0, 0, right?
3695940	3700340	That could happen with probability 1 over p cubed,
3701340	3703900	sorry, 1 minus p cubed.
3703900	3706380	Then this thing is just going to not,
3706380	3707700	that's not something that I want,
3707700	3709300	and I'm actually using my prior.
3709300	3711180	So my prior is not informative, but somehow it
3711180	3712940	captures the fact that I don't want to believe
3712940	3715780	that p is going to be either equal to 0 or 1.
3715780	3719500	OK, and so that's sort of taken care of here.
3719500	3725620	OK, so let's move away a little bit from the Bernoulli example,
3725620	3726100	shall we?
3726100	3728100	I mean, we think we've seen enough of it.
3728140	3730860	And so let's talk about the Gaussian model, right?
3730860	3737980	Let's say I want to do Gaussian inference in the,
3737980	3740020	I want to do inference in a Gaussian model using
3740020	3742020	vision methods.
3742020	3750220	OK, so I'm going to actually look at, so say, OK,
3750220	3764780	so what I want is that xi, x1, xn, or say, n, 0, 1, i, i, d,
3764780	3770260	sorry, theta 1, i, i, d, conditionally on theta.
3770260	3776340	OK, so that means that pn of x1, xn, given theta,
3776340	3778620	is equal to, well, exactly what I wrote before.
3778620	3784380	So 1 square root 2 pi to the n exponential minus 1
3784380	3789340	half sum of xi minus theta squared.
3789340	3791140	OK, so that's just the joint distribution
3791140	3793940	of my n-Gaussians with mean theta.
3793940	3796660	Another question is, what is the posterior distribution?
3796660	3802420	OK, well, here I said, let's use the uninformative prior,
3802420	3803860	which is an improper prior, right?
3803860	3805540	It puts weight 1 on everyone.
3805540	3809220	It has the so-called uniform on the entire real line,
3809220	3811180	so that's certainly not a density.
3811180	3814380	But I can still just use this, right?
3814380	3822220	So all I need to do is to get this divided by normalizing
3822220	3823180	this thing, right?
3823180	3824660	So that's what I need to do.
3824660	3826380	But if I look at this, right?
3826380	3829500	So essentially, I want to understand,
3829500	3832980	so this is proportional to exponential minus 1
3832980	3838860	half sum from i equal 1 to n of xi minus theta squared.
3838860	3842900	And now I want to see this thing as a density not on the xi's,
3842900	3846420	but on theta, right?
3846420	3850020	What I want is a density on theta.
3850020	3853660	So it looks like I have chances of getting something
3853660	3855540	that looks like a Gaussian.
3855540	3857540	But if I really need to have a Gaussian,
3857540	3859540	I would need to see minus 1 half,
3859540	3862700	and then I would need to see theta minus something here.
3862900	3865220	Not just the sum of something minus theta's.
3865220	3867380	So I need to work a little bit more
3867380	3871540	so I can see what this to expand the square here.
3871540	3872900	So this thing here is going to be
3872900	3877380	equal to exponential minus 1 half sum from i equal 1
3877380	3885300	to n of xi squared minus 2 xi theta plus theta squared.
3892700	3893200	Ah.
3906140	3911820	OK, and so now basically what I'm going to do
3911820	3915900	is everything, remember, is up to this little sign, right?
3915900	3919740	So every time I see a term that does not depend on theta,
3919740	3922260	I can just push it in there and just make it disappear.
3922260	3923900	Agreed?
3923900	3926820	OK, this term here, exponential minus 1
3926820	3931780	half sum of xi squared, does it depend on theta?
3931780	3933740	No, so I'm just pushing it here.
3933740	3935980	This guy, yes, and the other one, yes.
3935980	3940180	So this is proportional to exponential xi, sorry,
3940180	3945020	sum of the xi.
3945020	3946820	And then I'm going to pull out my theta.
3946820	3950140	The minus 1 half cancel with the minus 2.
3950140	3956900	And then I have minus 1 half sum from i equal 1
3956900	3958620	to n of theta squared, right?
3961620	3963460	Agreed?
3963460	3965380	So now what this thing looks like, well,
3965380	3969580	this looks very much like some theta minus something squared.
3969580	3976940	This thing here is really just n over 2 times theta.
3976980	3981780	So sorry, times theta squared.
3981780	3985180	So now what I need to do is to write this of the form theta
3985180	3988420	minus something, let's call it mu squared,
3988420	3991820	maybe divided by 2 sigma squared, right?
3991820	3994220	I want to turn this into that, maybe up to terms
3994220	3996540	that do not depend on theta.
3996540	3999140	That's what I'm going to try to do.
3999140	4000700	So that's called completing the square,
4000700	4001940	and that's some exercise you do.
4001940	4004140	You've done it probably already in the homework,
4004140	4006580	and that's something you do a lot when
4006580	4008820	you do Bayesian statistics in particular.
4008820	4009500	So let's do this.
4009500	4011340	Well, what is going to be the leading term?
4011340	4014180	Well, theta squared is going to be multiplied by this thing.
4014180	4017140	So I'm going to pull out my n over 2,
4017140	4024820	and then I'm going to write this as theta squared minus theta
4024820	4026220	minus something squared.
4026220	4028300	And this something is going to be 1
4028300	4030260	half of what I see in the cross product, right?
4030580	4032260	Well, I need to actually pull this thing out.
4032260	4036140	So let me write it like that first.
4036140	4039420	So that's theta squared.
4039420	4047940	And then I'm going to write it as minus 2 times 1 over n,
4047940	4054900	sum from i equal 1 to n of xi times theta, right?
4054900	4057900	That's exactly just the rewriting of what we had before.
4057900	4060100	That's the rewriting of what we had before.
4060100	4062660	And that should look much more familiar.
4062660	4068340	x squared minus a squared minus 2 blab a,
4068340	4069740	and then I missed something.
4069740	4074740	So this thing I'm going to be able to rewrite as theta minus xn
4074740	4077940	bar squared.
4077940	4080740	But then I need to remove the square of xn bar,
4080740	4081820	because it's not here.
4082420	4083540	OK?
4083540	4086020	So I just complete the square.
4086020	4088420	And then I actually really don't care what this thing actually
4088420	4090020	was, because it's going to go again
4090020	4092940	in the little alpha sign over there.
4092940	4094540	So this thing eventually is going
4094540	4102540	to be proportional to exponential of minus n over 2 times theta
4102540	4105620	minus xn bar squared.
4105620	4107460	And so we know that this is going to be
4107460	4111100	times theta minus xn bar squared.
4111100	4113380	And so we know that if this is a density that's
4113380	4122060	proportional to this guy, it has to be some n with mean
4122060	4124860	xn bar and variance.
4124860	4127540	Well, this is supposed to be 1 over sigma squared,
4127540	4129420	this guy over here, this n.
4129420	4133900	So that's really just 1 over n, OK?
4133940	4142500	So the posterior distribution is a Gaussian centered
4142500	4149740	at the average of my observations and with variance 1 over n.
4149740	4150240	OK?
4153580	4155260	Everybody's with me?
4155260	4157940	So just why I'm saying this, I mean,
4157940	4159700	this was the output of some computation,
4159700	4161460	but it sort of makes sense, right?
4161460	4163540	It's really telling me that the more observations
4163540	4166260	I have, the more concentrated this posterior is,
4166260	4167740	concentrated around what?
4167740	4170020	Well, around this xn bar.
4170020	4173140	So that looks like something we've sort of seen before,
4173140	4175420	but it does not have the same meaning somehow.
4175420	4177340	This is really just the posterior distribution,
4177340	4180500	and it's not really, I mean, it sort of says,
4180500	4183340	it's sort of a sanity check that I have this 1 over n when
4183340	4185420	I have xn bar, but it's not the same thing
4185420	4187500	as saying that the variance of xn bar was 1 over n
4187500	4189180	like we had before, OK?
4194340	4198300	So as an exercise, well, you probably will have it,
4198300	4201300	but I would recommend if you don't get it,
4201300	4218020	just try pi of theta to be equal to some n mu 1, OK?
4218020	4222380	So here, the prior that we use was completely non-informative.
4222380	4225620	What happens if I take my prior to be some Gaussian, which
4225620	4227540	is centered at mu, and it has the same variance
4227540	4230140	as the other guys, OK?
4230140	4233140	So what's going to happen here is that we're going to put a weight,
4233140	4234540	and everything that's away from mu
4234540	4237740	is going to actually get less weight, right?
4237740	4240860	And I want to know how I'm going to be updating this prior
4240860	4242100	into a posterior.
4242100	4245700	So that's right, so everybody sees what I'm saying here.
4245700	4247580	So pi of theta is just so that means
4247580	4250060	that pi of theta has the density proportional
4250060	4255700	to exponential minus 1 half theta minus mu squared, right?
4255700	4260500	So I need to multiply my posterior with this
4260500	4263420	and then see what I'd say actually going to be a Gaussian.
4263420	4264860	This is also a conjugate prior.
4264860	4266460	It's going to spit out another Gaussian.
4266460	4268420	You're going to have to complete a square again
4268420	4270820	and just check what it's actually giving you.
4270820	4272540	And so spoiler alert, it's going to look
4272540	4274420	like you get an extra observation, which
4274420	4277580	is actually equal to mu, OK?
4277620	4282300	So it's going to be the average of n plus 1 observations,
4282300	4287500	the first n ones being x1 to xn and the last one being mu.
4287500	4290420	And it sort of makes sense.
4290420	4292700	OK, so that's actually a fairly simple exercise.
4292700	4296740	But before, rather than going into more computation,
4296740	4298540	this is something you can definitely do
4298540	4301780	in the comfort of your room, I want
4301780	4303340	to talk about other types of priors, right?
4303340	4307340	So the first thing I said is, OK, there's this beta prior
4307340	4310380	that I just pulled out of my hat and that was just convenient.
4310380	4312900	Then there was this non-informative prior.
4312900	4313820	It was convenient, right?
4313820	4314780	It was non-informative.
4314780	4317340	So if you don't know anything else,
4317340	4318860	maybe that's what you want to do.
4318860	4321620	The question is, are there any other priors
4321620	4324500	that are sort of principled and generic in the sense
4324500	4328460	that the uninformative prior was generic, right?
4328460	4329500	I mean, it was equal to 1.
4329500	4331380	That's as generic as it gets.
4331380	4334140	And so is there anything that's generic as well?
4334140	4337220	Well, there's these priors that are called Jeffery's priors.
4337220	4339060	And Jeffery's prior is a prior which
4339060	4341500	is proportional to square root of the determinant
4341500	4345660	of the Fisher information of theta, OK?
4345660	4348620	And so this is actually kind of a weird thing to do, right?
4348620	4351420	It says, compute your, look at your model, right?
4351420	4354340	Your model is going to have a Fisher information.
4354340	4356180	Let's say it exists.
4356180	4359900	And because we know it does not always
4359900	4361580	exist, for example, in the multinomial model,
4361580	4363980	we didn't have a Fisher information.
4363980	4366140	And so the determinant of a matrix
4366140	4368660	is somehow measuring the size of a matrix, right?
4368660	4370540	And if you don't trust me, just think
4370540	4373860	about the matrix being of size one by one,
4373860	4376820	then the determinant is just the number that you have there.
4376820	4378660	And so this is really something that
4378660	4382500	looks like the Fisher information.
4382500	4384700	I mean, it's just basically the amount of information
4384700	4386340	is proportional to the amount of information
4386340	4389620	that you have at a certain point, OK?
4389620	4392220	And so what my prior is saying is saying, well,
4392220	4394300	I want to put more weights on those status that
4394300	4399860	are going to just extract more information from the data, OK?
4399860	4402260	So you can actually compute those things, right?
4402260	4406420	So in the first example, Jeffery's prior
4406420	4408100	is something that looks like this, right?
4408100	4410260	I mean, in one dimension, Fisher information
4410260	4413380	is essentially one over the variance, right?
4413380	4415620	So that's just one over the square root of the variance,
4415620	4417540	because I have the square root.
4417540	4421460	And when I have the uniform, sorry,
4421460	4426780	the Jeffery's prior, when I have the Gaussian case, right?
4426780	4428940	So this is the identity matrix that I
4428940	4430620	would have in the Gaussian case.
4430620	4432620	So the determinant of the identity is one,
4432620	4434900	so square root of one is one.
4434900	4436180	And so I would basically get one,
4436180	4439220	and that gives me my improper prior, my uninformative prior
4439220	4440820	that I had.
4440820	4443100	OK, so the uninformative prior one is fine.
4443100	4446780	I mean, clearly, all the Thetas carry the same information
4446780	4447980	in the Gaussian model, right?
4447980	4450220	I mean, whether I translate it here or here,
4450260	4451740	it's pretty clear that none of them
4451740	4453180	is actually better than the other.
4453180	4456540	But clearly, for the Bernoulli case,
4456540	4462260	the piece that are closer to the boundary
4462260	4463740	carry more information, right?
4463740	4466100	So I sort of like those guys because they just
4466100	4467700	like carry more information.
4467700	4470340	So what I do is that I take this function, so p1 minus p,
4470340	4474820	remember, is something that looks like this on the interval
4474820	4477980	0, 1, 0, and 1.
4478020	4481260	So this guy, 1 over square root of p1 minus p,
4481260	4487380	is something that looks like this, agreed?
4487380	4488820	And so what he's doing is sort of like
4488820	4491180	wants to push towards the piece that actually
4491180	4494380	carry more information.
4494380	4496980	I mean, whether you want to bias your data that way or not
4496980	4498620	is something you need to think about, right?
4498620	4500700	I mean, when you put a prior on your data,
4500700	4503180	on your parameter, you're sort of like biasing
4503180	4505060	towards this idea, your data.
4505060	4507740	And maybe that's maybe not such a good idea
4507740	4513100	when you have some p that's actually close to 1 half,
4513100	4513820	for example.
4513820	4514940	You're actually saying, no, I don't
4514940	4516620	want to see a p that's close to 1 half.
4516620	4518340	Just make a decision one way or another,
4518340	4519700	but just make a decision.
4519700	4522500	So it's sort of forcing you to do that.
4522500	4526100	OK, and so Jeffery's prior, so I'm running out of time,
4526100	4529140	so I don't want to go into too much details.
4529140	4532420	But we'll probably stop here, actually.
4532420	4547780	So Jeffery's priors have this very nice property
4547780	4551740	is that they actually do not care about the parametrization
4551740	4552980	of your space.
4552980	4556380	So if you actually have p, and you suddenly
4556380	4558820	decide that p is not the right parameter for Bernoulli,
4558820	4562260	but it's p squared, you could decide to parametrize this
4562260	4563180	by p squared.
4563180	4565420	Maybe your doctor is actually much more
4565420	4568860	able to formulate some prior assumption on p squared
4568860	4569820	rather than p.
4569820	4570980	You never know.
4570980	4574380	And so what happens is that Jeffery's priors
4574380	4575860	are invariant into this.
4575860	4578100	And the reason is because, well, the information carried
4578100	4580060	by p is the same as the information carried
4580060	4581460	by p squared somehow, right?
4581460	4588700	I mean, those are essentially the same, I mean, well,
4588700	4590780	yeah, they're essentially the same thing.
4590780	4594620	And so I mean, you need to have a one-to-one map, right?
4594620	4598060	Where you basically, for each parameter before you
4598060	4601100	have another parameter, so let's call eta the new parameters.
4601100	4610380	Then the PDF of the new prior indexed by eta this time
4610380	4612980	is actually also Jeffery's prior.
4612980	4615220	But this time, the new Fisher information
4615220	4617340	is not the Fisher information with respect to theta,
4617340	4620060	but it says in the Fisher information associated
4620100	4623140	to the statistical model indexed by eta.
4623140	4626740	So essentially, when you change Jeffery's prior,
4626740	4628780	when you change the parameterization of your model,
4628780	4632820	you still get Jeffery's prior for the new parameterization,
4632820	4636540	which is, in a way, a desirable property.
4636540	4640460	All right, so Jeffery's priors, just
4640460	4642460	like non-informative priors, are priors
4642460	4645100	you want to use when you want a systematic way
4645100	4648620	without really thinking about what to pick for your model.
4648660	4657060	OK, so, well, OK, I'll finish this next time.
4657060	4659940	And we'll talk about Bayesian confidence regions.
4659940	4661620	We'll talk about Bayesian estimation.
4661620	4664100	Once I have a posterior, what do I get?
4664100	4665540	And basically, the only message is
4665540	4666980	going to be that, well, you might
4666980	4668940	want to integrate against the posterior.
4668940	4672140	Find the expectation of your posterior distribution.
4672140	4674660	That's a good point estimator for theta.
4674660	4680580	And then we'll just do a couple of computation.
4680580	4681340	All right, so.
