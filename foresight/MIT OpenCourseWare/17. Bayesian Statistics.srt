1
00:00:00,000 --> 00:00:02,480
The following content is provided under a Creative

2
00:00:02,480 --> 00:00:03,800
Commons license.

3
00:00:03,800 --> 00:00:06,120
Your support will help MIT OpenCourseWare

4
00:00:06,120 --> 00:00:10,080
continue to offer high quality educational resources for free.

5
00:00:10,080 --> 00:00:12,760
To make a donation or to view additional materials

6
00:00:12,760 --> 00:00:16,680
from hundreds of MIT courses, visit MIT OpenCourseWare

7
00:00:16,680 --> 00:00:17,840
at ocw.mit.edu.

8
00:00:21,520 --> 00:00:27,240
So today we'll actually just do a brief chapter

9
00:00:27,240 --> 00:00:28,640
on Bayesian statistics.

10
00:00:28,640 --> 00:00:31,400
And there's entire courses on Bayesian statistics.

11
00:00:31,400 --> 00:00:33,480
There's entire books on Bayesian statistics.

12
00:00:33,480 --> 00:00:36,080
There's entire careers on Bayesian statistics.

13
00:00:36,080 --> 00:00:39,280
So admittedly, I'm not going to be

14
00:00:39,280 --> 00:00:40,960
able to do it justice and tell you

15
00:00:40,960 --> 00:00:42,960
all the interesting things that are happening

16
00:00:42,960 --> 00:00:44,080
in Bayesian statistics.

17
00:00:44,080 --> 00:00:47,320
But I think it's important as a statistician

18
00:00:47,320 --> 00:00:49,360
to know what it is, how it works,

19
00:00:49,360 --> 00:00:52,520
because it's actually a weapon of choice

20
00:00:52,520 --> 00:00:54,400
for many practitioners.

21
00:00:54,400 --> 00:00:58,120
And because it allows them to incorporate their knowledge

22
00:00:58,120 --> 00:01:00,720
about a problem in a fairly systematic manner.

23
00:01:00,720 --> 00:01:03,640
So if you look at, like, say, the Bayesian statistics

24
00:01:03,640 --> 00:01:05,520
literature, it's huge.

25
00:01:05,520 --> 00:01:09,600
And so here I give you sort of a range

26
00:01:09,600 --> 00:01:12,880
of what you can expect to see in Bayesian statistics

27
00:01:12,880 --> 00:01:18,320
from your second edition of a traditional book, something

28
00:01:18,320 --> 00:01:21,440
that involves computation, some things that involve

29
00:01:21,440 --> 00:01:22,240
rethinking.

30
00:01:22,240 --> 00:01:24,760
And there's a lot of Bayesian thinking.

31
00:01:24,760 --> 00:01:27,400
There's a lot of things that talk about sort

32
00:01:27,400 --> 00:01:30,240
of like philosophy of thinking Bayesian.

33
00:01:30,240 --> 00:01:32,280
This book, for example, seems to be one of them.

34
00:01:32,280 --> 00:01:34,720
This book is definitely one of them.

35
00:01:34,720 --> 00:01:38,880
This one represents sort of a broad literature

36
00:01:38,880 --> 00:01:42,240
on Bayesian statistics for applications, for example,

37
00:01:42,240 --> 00:01:43,560
in social sciences.

38
00:01:43,560 --> 00:01:45,280
But even in large-scale machine learning,

39
00:01:45,280 --> 00:01:47,320
there's a lot of Bayesian statistics happening,

40
00:01:47,320 --> 00:01:50,280
particularly using something called Bayesian parametrics

41
00:01:50,280 --> 00:01:53,480
or hierarchical Bayesian modeling.

42
00:01:53,480 --> 00:01:59,520
So we do have some experts at MIT in the C cell.

43
00:01:59,520 --> 00:02:03,600
Tamara Broderick, for example, is a person who does quite

44
00:02:03,600 --> 00:02:06,200
a bit of interesting work on Bayesian parametrics.

45
00:02:06,200 --> 00:02:08,320
And if that's something you want to know more about,

46
00:02:08,320 --> 00:02:10,920
I urge you to go and talk to her.

47
00:02:10,920 --> 00:02:14,040
So before we go in those more advanced things,

48
00:02:14,040 --> 00:02:17,200
we need to start with what is the Bayesian approach?

49
00:02:17,200 --> 00:02:18,680
What do Bayesians do?

50
00:02:18,680 --> 00:02:22,600
And how is it different from what we've been doing so far?

51
00:02:22,640 --> 00:02:26,360
So to understand the difference between Bayesians

52
00:02:26,360 --> 00:02:28,840
and what we've been doing so far is

53
00:02:28,840 --> 00:02:31,360
we need to first put a name on what we've been doing so far.

54
00:02:31,360 --> 00:02:32,960
It's called Frequentist Statistics.

55
00:02:32,960 --> 00:02:36,480
So it's usually Bayesian versus Frequentist statistics.

56
00:02:36,480 --> 00:02:38,800
I mean, by versus, I don't mean that there's naturally

57
00:02:38,800 --> 00:02:40,400
an opposition to them.

58
00:02:40,400 --> 00:02:43,360
Actually, often you will see the same method that

59
00:02:43,360 --> 00:02:45,440
comes out of both approaches.

60
00:02:45,440 --> 00:02:46,880
So let's see how we did it.

61
00:02:46,880 --> 00:02:48,960
The first thing, we had data.

62
00:02:48,960 --> 00:02:50,720
We observed some data.

63
00:02:50,720 --> 00:02:53,000
And we assumed that this data was generated randomly.

64
00:02:53,000 --> 00:02:54,840
The reason we did that is that because this

65
00:02:54,840 --> 00:02:57,840
would allow us to leverage tools from probability.

66
00:02:57,840 --> 00:03:01,000
So let's say by nature, measurements, you do a survey,

67
00:03:01,000 --> 00:03:03,080
you get some data.

68
00:03:03,080 --> 00:03:06,040
Then we made some assumptions on the data generating process.

69
00:03:06,040 --> 00:03:07,400
For example, we assumed there were

70
00:03:07,400 --> 00:03:09,480
IID that was one of the recurring things.

71
00:03:09,480 --> 00:03:11,640
Sometimes we assumed it was Gaussian

72
00:03:11,640 --> 00:03:13,440
if we wanted to use, say, t-test.

73
00:03:13,440 --> 00:03:15,320
Maybe we did some non-parametric statistics.

74
00:03:15,320 --> 00:03:17,680
So we assumed it was a smooth function

75
00:03:17,680 --> 00:03:20,360
or maybe linear regression function.

76
00:03:20,400 --> 00:03:21,560
So those are our modeling.

77
00:03:21,560 --> 00:03:24,160
And this was basically a way to say,

78
00:03:24,160 --> 00:03:27,960
well, we're not going to allow for any distributions

79
00:03:27,960 --> 00:03:31,680
for the data that we have, but maybe a small set of distribution

80
00:03:31,680 --> 00:03:34,800
that index by some small parameters, for example.

81
00:03:34,800 --> 00:03:38,440
Or at least remove some of the possibilities.

82
00:03:38,440 --> 00:03:40,800
Otherwise, there's nothing we could learn.

83
00:03:40,800 --> 00:03:45,280
And so, for example, this was associated

84
00:03:45,280 --> 00:03:47,000
to some parameter of interest, say,

85
00:03:47,000 --> 00:03:51,080
data or beta in the regression model.

86
00:03:51,080 --> 00:03:53,840
All right, then we had this unknown problem

87
00:03:53,840 --> 00:03:57,600
and this unknown parameter, and we wanted to find it.

88
00:03:57,600 --> 00:03:59,600
We wanted to either estimate it or test it

89
00:03:59,600 --> 00:04:02,440
or maybe find a confidence interval for this object.

90
00:04:02,440 --> 00:04:06,040
So far, I should not have said anything that's new.

91
00:04:06,040 --> 00:04:08,200
But this last sentence is actually

92
00:04:08,200 --> 00:04:10,640
what's going to be different from the Bayesian part.

93
00:04:10,640 --> 00:04:13,000
In particular, this unknown but fixed thing

94
00:04:13,000 --> 00:04:16,120
is what's going to be changing.

95
00:04:16,120 --> 00:04:18,760
So in the Bayesian approach, we still

96
00:04:18,760 --> 00:04:22,000
assume that we observe some random data.

97
00:04:22,000 --> 00:04:24,120
But the generating process is slightly different.

98
00:04:24,120 --> 00:04:25,720
It's sort of a two-layer process.

99
00:04:25,720 --> 00:04:27,920
And there's one process that generates the parameter,

100
00:04:27,920 --> 00:04:29,720
and then one process that, given this parameter,

101
00:04:29,720 --> 00:04:31,400
generates the data.

102
00:04:31,400 --> 00:04:35,240
So what the first layer does, I mean,

103
00:04:35,240 --> 00:04:36,840
nobody really believes that there's

104
00:04:36,840 --> 00:04:40,520
some random process that's happening about generating

105
00:04:40,520 --> 00:04:44,920
what is going to be the true expected number of people who

106
00:04:44,920 --> 00:04:45,960
turn their head to the right.

107
00:04:45,960 --> 00:04:47,920
When they kiss, but this is actually

108
00:04:47,920 --> 00:04:52,680
going to be something that brings us some easiness for us

109
00:04:52,680 --> 00:04:56,120
to incorporate what we call prior belief.

110
00:04:56,120 --> 00:04:58,680
So we'll see an example in a second.

111
00:04:58,680 --> 00:05:01,480
But often, you actually have prior belief

112
00:05:01,480 --> 00:05:03,000
of what this parameter should be.

113
00:05:03,000 --> 00:05:05,120
When we did, let's say, these squares,

114
00:05:05,120 --> 00:05:09,360
we looked over all of the vectors in all of r to the p,

115
00:05:09,360 --> 00:05:14,120
including the ones that have coefficients equal to $50 million.

116
00:05:14,120 --> 00:05:17,720
And so those are things that maybe we might be able to roll out.

117
00:05:17,720 --> 00:05:19,320
And maybe we might be able to roll out

118
00:05:19,320 --> 00:05:21,840
at a much smaller scale.

119
00:05:21,840 --> 00:05:23,560
For example, well, I mean, I don't know.

120
00:05:23,560 --> 00:05:29,200
I'm not an expert on turning your head to the right or to the left.

121
00:05:29,200 --> 00:05:30,960
But maybe you can roll out the fact

122
00:05:30,960 --> 00:05:33,240
that almost everybody is turning their head

123
00:05:33,240 --> 00:05:35,360
in the same direction, or almost everybody's

124
00:05:35,360 --> 00:05:38,040
turning their head to another direction.

125
00:05:38,040 --> 00:05:39,880
So we have this prior belief.

126
00:05:39,880 --> 00:05:43,200
And this prior belief is going to play, say,

127
00:05:43,200 --> 00:05:45,160
hopefully, less and less important role

128
00:05:45,160 --> 00:05:47,560
as we collect more and more data.

129
00:05:47,560 --> 00:05:49,160
But if we have a smaller amount of data,

130
00:05:49,160 --> 00:05:52,760
we might want to be able to use this information rather

131
00:05:52,760 --> 00:05:54,720
than just shooting in the dark.

132
00:05:54,720 --> 00:05:58,160
And so the idea is to have this prior belief.

133
00:05:58,160 --> 00:06:00,480
And then we want to update this prior belief

134
00:06:00,480 --> 00:06:03,040
into what's called a posterior belief

135
00:06:03,040 --> 00:06:04,920
after we've seen some data.

136
00:06:04,920 --> 00:06:08,240
Maybe I believe that there's something that

137
00:06:08,240 --> 00:06:09,680
should be in some range.

138
00:06:09,680 --> 00:06:11,400
But maybe after I see data, maybe it's

139
00:06:11,400 --> 00:06:12,640
comforting me in my belief.

140
00:06:12,640 --> 00:06:15,280
So I'm actually having maybe a belief that's more.

141
00:06:15,280 --> 00:06:18,440
So a belief encompasses basically what you think

142
00:06:18,440 --> 00:06:19,920
and how strongly you think about it.

143
00:06:19,920 --> 00:06:21,400
That's what I call belief.

144
00:06:21,400 --> 00:06:24,080
So for example, if I have a belief about some parameter

145
00:06:24,080 --> 00:06:27,440
theta, maybe my belief is telling me where theta should be

146
00:06:27,440 --> 00:06:29,960
and how strongly I believe in it in the sense

147
00:06:29,960 --> 00:06:34,640
that I have a very narrow region where theta could be.

148
00:06:34,640 --> 00:06:37,600
And so the posterior belief says, well, you see some data.

149
00:06:37,600 --> 00:06:40,000
And maybe you're more confident or less confident about what

150
00:06:40,000 --> 00:06:40,500
you've seen.

151
00:06:40,500 --> 00:06:42,780
You've shifted your belief a little bit.

152
00:06:42,780 --> 00:06:44,620
And so that's what we're going to try to see

153
00:06:44,620 --> 00:06:47,940
and how to do this in a principal manner.

154
00:06:47,940 --> 00:06:50,020
So of course, to understand this better,

155
00:06:50,020 --> 00:06:52,140
there's nothing better than an example.

156
00:06:52,140 --> 00:06:56,220
So let's talk about another stupid statistical question,

157
00:06:56,220 --> 00:06:58,620
which is let's try to understand p.

158
00:06:58,620 --> 00:07:01,300
Of course, I'm not going to talk about politics from now on.

159
00:07:01,300 --> 00:07:03,940
So let's talk about p, the proportion of women

160
00:07:03,940 --> 00:07:04,940
in the population.

161
00:07:11,100 --> 00:07:11,700
OK?

162
00:07:11,700 --> 00:07:18,660
And so what I could do is to collect some data, x1, xn,

163
00:07:18,660 --> 00:07:24,660
and assume that they're Bernoulli with some parameter p

164
00:07:24,660 --> 00:07:25,160
unknown.

165
00:07:25,160 --> 00:07:25,660
Right?

166
00:07:25,660 --> 00:07:27,660
So p is in 0, 1.

167
00:07:30,060 --> 00:07:30,560
OK?

168
00:07:30,560 --> 00:07:32,780
Let's assume that those guys are i, d.

169
00:07:32,780 --> 00:07:37,700
So this is just an indicator for each of my collected data,

170
00:07:38,220 --> 00:07:41,540
whether the person I randomly sample is a woman,

171
00:07:41,540 --> 00:07:44,060
I get a 1, and if it's a man, I get a 0.

172
00:07:44,060 --> 00:07:44,420
OK?

173
00:07:44,420 --> 00:07:49,740
And so now the question is, I sample these people randomly.

174
00:07:49,740 --> 00:07:51,580
I denote their gender.

175
00:07:51,580 --> 00:07:54,780
And the frequentist approach was just saying, OK,

176
00:07:54,780 --> 00:07:58,260
let's just estimate p hat being xn bar.

177
00:07:58,260 --> 00:08:01,020
And then we could do some tests.

178
00:08:01,020 --> 00:08:01,200
Right?

179
00:08:01,200 --> 00:08:02,260
So here there's a test.

180
00:08:02,260 --> 00:08:05,340
I want to test maybe if p is equal to 0.5 or not.

181
00:08:05,340 --> 00:08:09,860
That sounds like a pretty reasonable thing to test.

182
00:08:09,860 --> 00:08:13,140
But we want to also maybe estimate p.

183
00:08:13,140 --> 00:08:15,140
But here this is a case where we definitely

184
00:08:15,140 --> 00:08:17,140
have for our belief of what p should be.

185
00:08:17,140 --> 00:08:17,700
Right?

186
00:08:17,700 --> 00:08:22,060
We are pretty confident that p is not going to be 0.7.

187
00:08:22,060 --> 00:08:26,700
We actually believe that p should be extremely close to 1.5.

188
00:08:26,700 --> 00:08:26,940
OK?

189
00:08:26,940 --> 00:08:28,460
But maybe not exactly.

190
00:08:28,460 --> 00:08:29,340
Maybe I don't know.

191
00:08:29,340 --> 00:08:32,700
Maybe this population is not the population in the world,

192
00:08:32,700 --> 00:08:35,660
but maybe this is the population of, say, some college.

193
00:08:35,660 --> 00:08:39,300
And we want to understand if this college has half women or not.

194
00:08:39,300 --> 00:08:39,580
Right?

195
00:08:39,580 --> 00:08:42,100
So maybe we know it's going to be close to 1.5,

196
00:08:42,100 --> 00:08:44,700
but maybe we're not quite sure.

197
00:08:44,700 --> 00:08:49,260
And so we're going to want to integrate that knowledge.

198
00:08:49,260 --> 00:08:49,980
All right?

199
00:08:49,980 --> 00:08:51,980
So I could integrate it in a blunt manner

200
00:08:51,980 --> 00:08:55,420
by saying discard the data and say that p is equal to 1.5.

201
00:08:55,420 --> 00:08:57,580
But maybe that's just a little too much.

202
00:08:57,580 --> 00:09:01,380
So how do I do this trade-off between adding the data

203
00:09:01,380 --> 00:09:05,060
and combining it with this prior knowledge?

204
00:09:05,060 --> 00:09:09,100
In many ways, in many instances, essentially what's

205
00:09:09,100 --> 00:09:11,020
going to happen is this 1.5 is going

206
00:09:11,020 --> 00:09:14,260
to act like one new observation, essentially.

207
00:09:14,260 --> 00:09:17,020
So if you have five observations,

208
00:09:17,020 --> 00:09:20,140
this is just a six observation, which will play a role.

209
00:09:20,140 --> 00:09:21,820
If you have a million observations,

210
00:09:21,820 --> 00:09:22,860
you're going to have a million in one,

211
00:09:22,860 --> 00:09:24,660
and it's not going to play so much a role.

212
00:09:24,660 --> 00:09:26,700
That's basically how it goes.

213
00:09:26,700 --> 00:09:28,620
That's basically how it goes.

214
00:09:28,660 --> 00:09:33,580
But definitely not always, because we'll

215
00:09:33,580 --> 00:09:36,980
see that if I take my prior to be a point minus 1.5 here,

216
00:09:36,980 --> 00:09:39,140
it's basically as if I was discarding my data.

217
00:09:39,140 --> 00:09:41,740
So essentially, there's also your ability

218
00:09:41,740 --> 00:09:45,460
to encompass how strongly you believe in this prior.

219
00:09:45,460 --> 00:09:47,820
And if you believe infinitely more in the prior

220
00:09:47,820 --> 00:09:49,620
than you believe in the data you collected,

221
00:09:49,620 --> 00:09:51,900
then, of course, it's not going to act like one more

222
00:09:51,900 --> 00:09:53,340
observation.

223
00:09:53,340 --> 00:09:55,540
All right, so the Bayesian approach

224
00:09:55,540 --> 00:09:59,020
is a tool to, one, include mathematically our prior,

225
00:09:59,020 --> 00:10:02,060
and our prior belief into statistical procedures.

226
00:10:02,060 --> 00:10:03,940
So maybe I have this prior knowledge,

227
00:10:03,940 --> 00:10:06,100
but if I'm a medical doctor, it's not clear to me

228
00:10:06,100 --> 00:10:09,900
how I'm going to turn this into some principle way of building

229
00:10:09,900 --> 00:10:10,420
estimators.

230
00:10:10,420 --> 00:10:12,140
And of course, the second goal is

231
00:10:12,140 --> 00:10:13,860
going to be to update this prior belief

232
00:10:13,860 --> 00:10:19,780
into posterior belief by using the data, all right?

233
00:10:19,780 --> 00:10:23,940
So how do I do this?

234
00:10:23,940 --> 00:10:25,500
And at some point, I sort of suggested

235
00:10:25,500 --> 00:10:28,660
that there's two layers.

236
00:10:28,660 --> 00:10:31,700
One is where you draw the parameter at random.

237
00:10:31,700 --> 00:10:35,780
And two, once you have the parameter, condition

238
00:10:35,780 --> 00:10:39,340
this parameter, you draw your data.

239
00:10:39,340 --> 00:10:41,100
Nobody believes this actually is happening,

240
00:10:41,100 --> 00:10:43,620
that nature is just rolling dice for us

241
00:10:43,620 --> 00:10:45,500
and choosing parameters at random.

242
00:10:45,500 --> 00:10:48,260
But what's happening is that this idea

243
00:10:48,260 --> 00:10:51,420
that the parameter comes from some random distribution

244
00:10:51,420 --> 00:10:55,460
actually captures very well this idea that how you would

245
00:10:55,460 --> 00:10:56,980
encompass your prior, right?

246
00:10:56,980 --> 00:10:59,100
How would you say my belief is as follows?

247
00:10:59,100 --> 00:11:01,900
Well, here's an example about p.

248
00:11:01,900 --> 00:11:08,180
I'm 90% sure that p is between 0.4 and 0.6.

249
00:11:08,180 --> 00:11:14,060
And I'm 95% sure that p is between 0.3 and 0.8.

250
00:11:14,060 --> 00:11:18,500
OK, so essentially, I have this possible value of p.

251
00:11:18,500 --> 00:11:28,540
And what I know is that there's 90% here between, what did I

252
00:11:28,540 --> 00:11:32,420
say, 0.4 and 0.6.

253
00:11:35,460 --> 00:11:39,260
And then I have 0.3 and 0.8.

254
00:11:39,260 --> 00:11:43,860
And I know that I'm 95% sure that I'm in here.

255
00:11:43,860 --> 00:11:45,340
And this, if you remember, this sort of

256
00:11:45,340 --> 00:11:47,340
looks like the kind of pictures that I

257
00:11:47,340 --> 00:11:50,140
made when I had some Gaussian, right, for example.

258
00:11:50,140 --> 00:11:54,220
And I said, oh, here we have 90% of the observations.

259
00:11:54,220 --> 00:11:56,660
And here we have 95% of the observations.

260
00:12:00,460 --> 00:12:04,620
So in a way, if I were able to tell you

261
00:12:04,620 --> 00:12:07,540
all those ranges for all possible values,

262
00:12:07,540 --> 00:12:10,580
then I would essentially describe a probability

263
00:12:10,580 --> 00:12:13,420
distribution for p.

264
00:12:13,420 --> 00:12:14,780
And what I'm essentially saying is

265
00:12:14,780 --> 00:12:16,620
that p is going to have this kind of shape.

266
00:12:16,620 --> 00:12:19,060
So of course, if I tell you only twice this information

267
00:12:19,060 --> 00:12:22,300
that there's 90% I'm here and I'm here between here and here

268
00:12:22,300 --> 00:12:24,620
and 95% I'm between here and here,

269
00:12:24,620 --> 00:12:27,220
then there's many ways I can accomplish that, right?

270
00:12:27,220 --> 00:12:31,660
I could have something that looks like this, maybe, right?

271
00:12:31,660 --> 00:12:35,580
I could be really, I mean, it could be like this.

272
00:12:35,580 --> 00:12:37,820
I mean, there's many ways I can have this.

273
00:12:37,820 --> 00:12:39,900
Some of them are definitely going to be mathematically

274
00:12:39,900 --> 00:12:42,260
more convenient than others.

275
00:12:42,260 --> 00:12:44,340
And hopefully, we're going to have things

276
00:12:44,340 --> 00:12:47,260
that I can parameterize very well.

277
00:12:47,260 --> 00:12:49,900
Because if I tell you this is this guy,

278
00:12:49,900 --> 00:12:56,780
then there's basically 1, 2, 3, 4, 5, 6, 7 parameters.

279
00:12:56,780 --> 00:12:59,020
So I probably don't want something that has 7 parameters,

280
00:12:59,020 --> 00:13:01,180
but maybe I can say, oh, it's a Gaussian.

281
00:13:01,180 --> 00:13:02,820
And all I have to do is to tell you

282
00:13:02,820 --> 00:13:07,060
where it's centered and what the standard deviation is.

283
00:13:07,060 --> 00:13:11,060
OK, so the idea of using this two-layer thing

284
00:13:11,060 --> 00:13:13,540
where we think of the parameter p as being drawn

285
00:13:13,540 --> 00:13:15,940
from some distribution is really just a way for us

286
00:13:15,940 --> 00:13:19,260
to capture this information, our prior belief being,

287
00:13:19,260 --> 00:13:22,820
well, there's this percentage of chances that it's there.

288
00:13:22,820 --> 00:13:25,540
But the person who has a chance, I'm deliberately not

289
00:13:25,540 --> 00:13:26,940
using probability here.

290
00:13:26,940 --> 00:13:28,780
It's really, right?

291
00:13:28,780 --> 00:13:32,140
So it's really a way to get close to this.

292
00:13:32,140 --> 00:13:34,340
All right, so that's what I said.

293
00:13:34,340 --> 00:13:36,180
The true parameter is not random,

294
00:13:36,180 --> 00:13:40,420
but the Bayesian approach does as if it was random

295
00:13:40,420 --> 00:13:44,500
and then just spits out a procedure out of this thought

296
00:13:44,500 --> 00:13:49,060
process, this thought experiment.

297
00:13:49,060 --> 00:13:54,100
So when you practice Bayesian statistics a lot,

298
00:13:54,100 --> 00:13:57,100
you start getting automatisms.

299
00:13:57,100 --> 00:14:00,580
So you start getting some things that you do

300
00:14:00,580 --> 00:14:03,140
without really thinking about it, just like when you're

301
00:14:03,140 --> 00:14:04,740
a statistician, the first thing you do

302
00:14:04,740 --> 00:14:07,820
is can I think of this data as being Gaussian, for example?

303
00:14:07,820 --> 00:14:10,340
When you're Bayesian, you're thinking about, OK,

304
00:14:10,340 --> 00:14:11,620
I have a set of parameters, right?

305
00:14:11,620 --> 00:14:15,060
So here, I can describe my parameter as being theta

306
00:14:15,060 --> 00:14:21,540
in general in some big space parameter theta.

307
00:14:21,540 --> 00:14:24,900
But what spaces did we encounter?

308
00:14:24,900 --> 00:14:27,100
Well, we encountered the real line.

309
00:14:27,100 --> 00:14:30,980
We encountered the interval 0, 1 for Bernoulli's.

310
00:14:30,980 --> 00:14:36,340
And we encountered maybe some deposit of real line

311
00:14:36,340 --> 00:14:39,340
for exponential distributions, et cetera.

312
00:14:39,340 --> 00:14:42,060
And so what I'm going to need to do, if I want to model some,

313
00:14:42,060 --> 00:14:44,580
if I want to put some prior on those spaces,

314
00:14:44,580 --> 00:14:48,340
I'm going to have to have a usual set of tools for this guy,

315
00:14:48,340 --> 00:14:50,340
usual set of tools for this guy, usual set of tools

316
00:14:50,340 --> 00:14:51,220
for this guy.

317
00:14:51,220 --> 00:14:52,540
And by usual set of tools, I mean,

318
00:14:52,540 --> 00:14:54,660
I'm going to have to have a family of distributions

319
00:14:54,660 --> 00:14:56,820
that's supported on this.

320
00:14:56,820 --> 00:15:00,700
So in particular, this is the speech in which my parameter

321
00:15:00,700 --> 00:15:03,900
that I usually denote by p for Bernoulli lives.

322
00:15:03,900 --> 00:15:06,780
And so what I need is to find a distribution

323
00:15:06,780 --> 00:15:13,540
on the interval 0, 1, just like this guy.

324
00:15:13,540 --> 00:15:15,660
The problem with the Gaussian is that it's not

325
00:15:15,660 --> 00:15:17,940
on the interval 0, 1.

326
00:15:17,940 --> 00:15:20,260
It's going to spill out in the end.

327
00:15:20,260 --> 00:15:22,780
And it's not going to be something that works for me.

328
00:15:22,780 --> 00:15:24,780
And so the question is, I need to think

329
00:15:24,780 --> 00:15:27,060
about distributions that are probably continuous.

330
00:15:27,060 --> 00:15:30,100
Why would I restrict myself to discrete distributions that

331
00:15:30,100 --> 00:15:31,500
are actually convenient?

332
00:15:31,500 --> 00:15:34,060
And for Bernoulli, one that's actually

333
00:15:34,060 --> 00:15:36,740
basically the main tool that everybody's using

334
00:15:36,740 --> 00:15:39,700
is this so-called beta distribution.

335
00:15:39,700 --> 00:15:42,220
So the beta distribution has two parameters.

336
00:15:50,620 --> 00:15:59,100
So x follows a beta with parameters, say a and b,

337
00:15:59,100 --> 00:16:09,060
if it has a density, f of x is equal to x to the a minus 1,

338
00:16:09,060 --> 00:16:18,060
1 minus x to the b minus 1, if x is in the interval 0, 1,

339
00:16:18,060 --> 00:16:20,300
and 0 for all other x's.

340
00:16:21,260 --> 00:16:28,220
So why is that a good thing?

341
00:16:28,220 --> 00:16:31,860
Well, it's a density that's on the interval 0, 1, for sure.

342
00:16:31,860 --> 00:16:33,740
But now I have these two parameters.

343
00:16:33,740 --> 00:16:36,060
And the set of shapes that I can get

344
00:16:36,060 --> 00:16:40,300
by tweaking those two parameters is incredible.

345
00:16:40,300 --> 00:16:44,340
I mean, it's going to be a unimodal distribution.

346
00:16:44,340 --> 00:16:45,700
It's still fairly nice.

347
00:16:45,700 --> 00:16:47,740
It's not going to be something that goes like this and this,

348
00:16:47,740 --> 00:16:54,060
because if you think about this, what would it mean if your prior

349
00:16:54,060 --> 00:16:59,620
distribution on the interval 0, 1 had this shape?

350
00:16:59,620 --> 00:17:01,940
It would mean that maybe you think that p is here,

351
00:17:01,940 --> 00:17:03,340
or maybe you think that p is here,

352
00:17:03,340 --> 00:17:05,300
or maybe you think that p is here,

353
00:17:05,300 --> 00:17:08,780
which essentially mean that you think that p can come maybe

354
00:17:08,780 --> 00:17:10,900
from three different phenomena.

355
00:17:10,900 --> 00:17:13,260
And there's other models that are called mixtures for that

356
00:17:13,260 --> 00:17:15,100
that directly account for the fact

357
00:17:15,140 --> 00:17:21,100
that maybe there are several phenomena that are aggregated in your data set.

358
00:17:21,100 --> 00:17:23,460
But if you think that your data set is sort of pure

359
00:17:23,460 --> 00:17:25,700
and that everything comes from the same phenomenon,

360
00:17:25,700 --> 00:17:28,900
you want something that looks like maybe like this,

361
00:17:28,900 --> 00:17:32,900
or maybe looks like this, or maybe is sort of symmetric.

362
00:17:32,900 --> 00:17:34,460
You want to get all this stuff, right?

363
00:17:34,460 --> 00:17:38,580
Maybe you want something that says, well, if I'm talking about p

364
00:17:38,580 --> 00:17:44,860
being the probability of the proportion of women in the whole world,

365
00:17:44,860 --> 00:17:48,620
you want something that's probably really spiked around 1-1.5,

366
00:17:48,620 --> 00:17:50,500
almost the point mass, because you know,

367
00:17:50,500 --> 00:17:55,020
I mean, OK, let's agree that 0.5 is the actual number.

368
00:17:55,020 --> 00:17:58,980
So you want something maybe that says, OK, maybe I'm wrong,

369
00:17:58,980 --> 00:18:01,140
but I'm sure I'm not going to be really that way off.

370
00:18:01,140 --> 00:18:03,340
And so you want something that's really pointy.

371
00:18:03,340 --> 00:18:06,700
But if it's something you've never checked, right?

372
00:18:06,700 --> 00:18:09,820
And again, I cannot make references at this point,

373
00:18:09,820 --> 00:18:12,820
but something where you might have some uncertainty,

374
00:18:12,820 --> 00:18:14,300
then that should be around 1-1.5.

375
00:18:14,300 --> 00:18:16,700
Maybe you want something that's like a little more,

376
00:18:16,700 --> 00:18:19,420
allows you to say, well, I think there's more around 1-1.5,

377
00:18:19,420 --> 00:18:22,980
but there's still some fluctuations that are possible, OK?

378
00:18:22,980 --> 00:18:25,180
And in particular here, I talk about p

379
00:18:25,180 --> 00:18:29,340
where the two parameters, a and b, are actually the same.

380
00:18:29,340 --> 00:18:30,900
I call them a.

381
00:18:30,900 --> 00:18:33,540
One is called scale, the other one's called shape.

382
00:18:33,540 --> 00:18:35,420
Oh, by the way, sorry, this is not a density,

383
00:18:35,420 --> 00:18:38,780
so it actually has to be normalized, right?

384
00:18:38,780 --> 00:18:40,180
When you integrate this guy, it's going

385
00:18:40,180 --> 00:18:42,460
to be some function that depends on a and b, actually.

386
00:18:42,460 --> 00:18:45,460
Depends on this function through the beta function, right?

387
00:18:45,460 --> 00:18:47,220
Which is this combination of gamma function.

388
00:18:47,220 --> 00:18:49,900
So that's why it's called beta distribution.

389
00:18:49,900 --> 00:18:53,500
But well, that's the definition of the beta function

390
00:18:53,500 --> 00:18:55,180
when you integrate this thing anyway.

391
00:18:55,180 --> 00:18:57,020
So I mean, you just have to normalize it.

392
00:18:57,020 --> 00:18:59,740
It's just a number that depends on the nb, OK?

393
00:18:59,740 --> 00:19:01,780
So here, if you take a equal to b,

394
00:19:01,780 --> 00:19:03,100
you have something that essentially is

395
00:19:03,100 --> 00:19:05,340
symmetric around 1-1.5, right?

396
00:19:05,340 --> 00:19:07,060
Because what does it look like?

397
00:19:07,060 --> 00:19:08,020
Well, it's something.

398
00:19:08,020 --> 00:19:11,020
So my density f of x is going to be what?

399
00:19:11,020 --> 00:19:19,220
It's going to be my constant times x times 1 minus x

400
00:19:19,220 --> 00:19:21,660
to the a minus 1, right?

401
00:19:21,660 --> 00:19:26,100
And this function, x times 1 minus x looks like this.

402
00:19:26,100 --> 00:19:27,740
We've drawn it before, right?

403
00:19:27,740 --> 00:19:29,420
That was something that showed up

404
00:19:29,420 --> 00:19:36,500
as being the variance of my Bernoulli.

405
00:19:36,500 --> 00:19:40,980
So we know it's something that takes its maximum at 1-1.5,

406
00:19:41,980 --> 00:19:44,220
and now I'm just taking the power of this guy.

407
00:19:44,220 --> 00:19:46,020
So I'm really just distorting this thing

408
00:19:46,020 --> 00:19:53,340
into some fairly symmetric manner, OK?

409
00:19:53,340 --> 00:20:00,340
So this distribution that we actually take for p, right?

410
00:20:00,340 --> 00:20:02,540
So here, I assume that p, the parameter, right?

411
00:20:02,540 --> 00:20:04,260
I mean, notice that this is kind of weird.

412
00:20:04,260 --> 00:20:06,060
First of all, this is probably the first time

413
00:20:06,060 --> 00:20:09,580
in this entire course that we have this has this something

414
00:20:09,580 --> 00:20:12,340
has a distribution when it's actually a lower case letter.

415
00:20:12,340 --> 00:20:13,740
That's something you have to deal with,

416
00:20:13,740 --> 00:20:16,940
because we've been using lower case letters for parameters,

417
00:20:16,940 --> 00:20:18,700
and now we want them to have a distribution.

418
00:20:18,700 --> 00:20:20,460
So that's what's going to happen, all right?

419
00:20:20,460 --> 00:20:23,860
And this is called the prior distribution, OK?

420
00:20:23,860 --> 00:20:25,300
So really, I should write something

421
00:20:25,300 --> 00:20:34,060
like f of p is equal to a constant times p 1 minus p

422
00:20:34,060 --> 00:20:35,220
to the a minus 1.

423
00:20:35,220 --> 00:20:38,100
Well, no, actually, I should not, because then it's confusing.

424
00:20:38,100 --> 00:20:39,980
OK, so let me not do this.

425
00:20:39,980 --> 00:20:42,140
One thing in terms of notation that I'm going to write,

426
00:20:42,140 --> 00:20:43,940
I'm going to write, when I have a constant here,

427
00:20:43,940 --> 00:20:45,340
and I don't want to make it explicit,

428
00:20:45,340 --> 00:20:48,500
and we'll see in a second why I don't need to make it explicit,

429
00:20:48,500 --> 00:20:59,300
I'm going to write this as f of x is proportional to x 1

430
00:20:59,300 --> 00:21:04,060
minus x to the a minus 1, OK?

431
00:21:04,060 --> 00:21:08,780
So that's just to say equal to some constant that does not

432
00:21:08,780 --> 00:21:11,540
depend on x times this thing, OK?

433
00:21:16,340 --> 00:21:20,660
So if we continue with our experiment, now if p,

434
00:21:20,660 --> 00:21:22,900
right, so that's the experiment where I'm trying to,

435
00:21:22,900 --> 00:21:26,380
I'm drawing this data x 1 to x n, which is Bernoulli p,

436
00:21:26,380 --> 00:21:29,900
if p has some distribution, it's not clear

437
00:21:29,900 --> 00:21:32,700
what it means to have a Bernoulli with some random parameter.

438
00:21:32,700 --> 00:21:35,020
So what I'm going to do is then I'm going to first draw my p.

439
00:21:35,020 --> 00:21:38,700
Let's say I get a number 0.52, and then I'm

440
00:21:38,700 --> 00:21:41,140
going to draw my data conditionally on p, all right?

441
00:21:41,140 --> 00:21:45,820
So here comes the first and last flowchart of this class.

442
00:21:45,820 --> 00:21:49,540
So I'm going to first, all right?

443
00:21:49,540 --> 00:21:52,820
So nature first draws p, OK?

444
00:21:52,820 --> 00:21:58,420
So p follows, say, some beta AA.

445
00:21:58,420 --> 00:22:06,900
Then I condition on p, and then I draw x 1, x n, that are i,

446
00:22:06,900 --> 00:22:10,780
i, d, Bernoulli p.

447
00:22:10,780 --> 00:22:14,380
Everybody understand the process of generating this data, right?

448
00:22:14,380 --> 00:22:16,260
So you first draw a parameter, and then you just

449
00:22:16,260 --> 00:22:21,180
flip those independent bias coins with this particular p.

450
00:22:21,180 --> 00:22:25,260
So there's this layered thing.

451
00:22:25,260 --> 00:22:28,660
So now, conditionally on p, right?

452
00:22:28,660 --> 00:22:31,980
So here, I have this prior about p, which was the thing.

453
00:22:31,980 --> 00:22:34,140
So this is just the thought process again, right?

454
00:22:34,140 --> 00:22:36,420
It's not anything that actually happens in practice.

455
00:22:36,420 --> 00:22:39,940
This is my way of thinking about how the data was generated.

456
00:22:39,940 --> 00:22:43,300
And from this, I'm going to try to come up with some procedure.

457
00:22:43,300 --> 00:22:47,820
Just like if your estimator is the average of the data,

458
00:22:47,820 --> 00:22:49,740
you don't have to understand probability

459
00:22:49,740 --> 00:22:52,380
to say that my estimator is the average of the data, right?

460
00:22:52,380 --> 00:22:54,220
I mean, anyone outside this room understand

461
00:22:54,260 --> 00:22:58,540
that the average is a good estimator for some average behavior,

462
00:22:58,540 --> 00:23:01,540
and they don't need to think of the data as being

463
00:23:01,540 --> 00:23:03,020
a random variable, et cetera.

464
00:23:03,020 --> 00:23:05,060
So same thing, basically.

465
00:23:05,060 --> 00:23:06,460
Now, we will see.

466
00:23:06,460 --> 00:23:08,060
I mean, actually, we won't.

467
00:23:08,060 --> 00:23:10,780
But in this case, well, we will.

468
00:23:10,780 --> 00:23:13,100
In this case, you can see that, essentially, the posterior

469
00:23:13,100 --> 00:23:16,700
distribution is still a beta, all right?

470
00:23:16,700 --> 00:23:20,100
So what it means is that I had this thing,

471
00:23:20,100 --> 00:23:23,140
then I observed my data, and then I continue.

472
00:23:23,140 --> 00:23:31,820
And here, I'm going to update my prior

473
00:23:31,820 --> 00:23:36,700
into some posterior distribution, pi.

474
00:23:36,700 --> 00:23:40,900
And here, this guy is actually also a beta, all right?

475
00:23:40,900 --> 00:23:45,460
So pi now, P, my posterior distribution on P,

476
00:23:45,460 --> 00:23:48,260
is also a beta distribution with the parameters that

477
00:23:48,260 --> 00:23:51,700
are on this slide, and I'll have space to reproduce them.

478
00:23:51,700 --> 00:23:54,500
So I start the beginning of this flow chart

479
00:23:54,500 --> 00:23:57,140
as having P, which is a prior.

480
00:23:57,140 --> 00:23:58,820
I'm going to get some observations,

481
00:23:58,820 --> 00:24:03,900
and then I'm going to update what my posterior is, OK?

482
00:24:03,900 --> 00:24:06,940
So this posterior is basically something

483
00:24:06,940 --> 00:24:10,100
that's in Bayesian statistics was beautiful,

484
00:24:10,100 --> 00:24:13,780
is as soon as you have the distribution,

485
00:24:13,780 --> 00:24:17,060
it's essentially capturing all the information about the data

486
00:24:17,060 --> 00:24:20,380
that you want for P. And it's not just a point, right?

487
00:24:20,380 --> 00:24:21,500
It's not just an average.

488
00:24:21,500 --> 00:24:23,660
It's actually an entire distribution

489
00:24:23,660 --> 00:24:27,060
for the possible values of theta.

490
00:24:27,060 --> 00:24:30,740
And it's not the same thing as saying, well, you know,

491
00:24:30,740 --> 00:24:34,820
if theta hat is equal to x and bar in the Gaussian case,

492
00:24:34,820 --> 00:24:37,180
I know that this is some mean mu,

493
00:24:37,180 --> 00:24:39,700
and then maybe it has variance sigma square over n.

494
00:24:39,700 --> 00:24:42,940
That's not what I mean by this is my posterior distribution,

495
00:24:42,940 --> 00:24:43,580
right?

496
00:24:43,580 --> 00:24:46,660
This is not what I mean.

497
00:24:46,660 --> 00:24:48,700
This is going to come from this guy, right?

498
00:24:48,700 --> 00:24:51,340
The Gaussian thing and the central limit theorem.

499
00:24:51,340 --> 00:24:52,980
But what I mean is this guy.

500
00:24:52,980 --> 00:24:58,180
And this came exclusively from the prior distribution.

501
00:24:58,180 --> 00:25:00,860
If I had another prior, I would not necessarily

502
00:25:00,860 --> 00:25:03,740
have a beta distribution on the output.

503
00:25:03,740 --> 00:25:07,580
So when I have the same family of distributions

504
00:25:07,580 --> 00:25:11,100
at the beginning and at the end of this flow chart,

505
00:25:11,100 --> 00:25:21,220
I say that beta is a conjugate prior,

506
00:25:21,220 --> 00:25:23,980
meaning I put in beta as a prior,

507
00:25:23,980 --> 00:25:27,420
and I get betas at posterior.

508
00:25:27,420 --> 00:25:30,900
And that's why betas are so popular.

509
00:25:30,900 --> 00:25:32,300
Conjugate priors are really nice,

510
00:25:32,300 --> 00:25:35,340
because you know that whatever you put in,

511
00:25:35,340 --> 00:25:37,220
what you're going to get in the end is a beta.

512
00:25:37,220 --> 00:25:38,820
So all you have to think about is the parameters.

513
00:25:38,820 --> 00:25:41,060
You don't have to check again what the posterior is

514
00:25:41,060 --> 00:25:43,300
going to look like, what the PDF of this guy is going to be.

515
00:25:43,300 --> 00:25:44,500
You don't have to think about it.

516
00:25:44,500 --> 00:25:46,660
You just have to check what the parameters are.

517
00:25:46,660 --> 00:25:48,300
And there's families of conjugate priors.

518
00:25:48,300 --> 00:25:51,180
Gaussian gives Gaussian, for example.

519
00:25:51,180 --> 00:25:52,220
There's a bunch of them.

520
00:25:52,220 --> 00:25:56,420
And this is what drives people into using specific priors

521
00:25:56,420 --> 00:25:58,260
as opposed to other.

522
00:25:58,260 --> 00:26:00,700
It has nice mathematical properties.

523
00:26:00,700 --> 00:26:04,180
Nobody believes that the P distribution is really

524
00:26:04,180 --> 00:26:07,460
distributed according to beta, but it's flexible enough

525
00:26:07,460 --> 00:26:10,540
and super convenient mathematically.

526
00:26:10,540 --> 00:26:13,900
All right, so now let's see for one second

527
00:26:13,900 --> 00:26:15,780
before we actually go any further.

528
00:26:15,780 --> 00:26:17,860
What I did, so A and B, I didn't mention it.

529
00:26:18,740 --> 00:26:24,140
And here, A and B are positive numbers.

530
00:26:24,140 --> 00:26:27,540
OK, they can be anything positive.

531
00:26:27,540 --> 00:26:31,980
So here what I did is that I updated A into A plus the sum

532
00:26:31,980 --> 00:26:38,420
of my data, and B into B plus and minus the sum of my data.

533
00:26:38,420 --> 00:26:42,220
So that's essentially A becomes A plus the number of ones,

534
00:26:42,220 --> 00:26:47,380
and B becomes B. Well, that's only when I have A and A, right?

535
00:26:47,420 --> 00:26:50,220
So the first parameters become itself plus the number of ones,

536
00:26:50,220 --> 00:26:52,660
and the second one becomes itself plus the number of zeros.

537
00:26:55,460 --> 00:26:59,180
And so just as a sanity check, what does this mean?

538
00:26:59,180 --> 00:27:08,980
If A goes to 0, what is the beta when A goes to 0?

539
00:27:08,980 --> 00:27:11,700
We can actually read this from here, right?

540
00:27:11,700 --> 00:27:26,140
So we had A, sorry, actually, let's take A goes to, no, actually,

541
00:27:26,140 --> 00:27:27,340
sorry, let's just do this.

542
00:27:37,540 --> 00:27:38,700
OK, let's not do this now.

543
00:27:38,700 --> 00:27:40,860
I'll do it when we talk about non-informative prior,

544
00:27:40,900 --> 00:27:44,220
because it's a little too messy here.

545
00:27:44,220 --> 00:27:47,980
OK, so how do we do this?

546
00:27:47,980 --> 00:27:51,380
How did I get this posterior distribution given the prior?

547
00:27:51,380 --> 00:27:53,100
How do I update this?

548
00:27:53,100 --> 00:27:56,060
Well, this is called Bayesian statistics,

549
00:27:56,060 --> 00:27:58,820
and you've heard this word Bayes before,

550
00:27:58,820 --> 00:28:02,020
and the way you've heard it is in the Bayes formula, right?

551
00:28:02,020 --> 00:28:03,700
What was the Bayes formula?

552
00:28:03,700 --> 00:28:08,060
The Bayes formula was telling you that the probability of A given

553
00:28:08,060 --> 00:28:12,500
B was equal to something that depended on the probability

554
00:28:12,500 --> 00:28:13,620
of B given A, right?

555
00:28:13,620 --> 00:28:14,780
That's what it was.

556
00:28:14,780 --> 00:28:18,660
And I mean, you can actually either remember the formula,

557
00:28:18,660 --> 00:28:20,580
you can remember the definition, and this

558
00:28:20,580 --> 00:28:26,420
is what P of A and B divided by P of B. So this

559
00:28:26,420 --> 00:28:35,540
is P of B given A times P of A divided by P of B, right?

560
00:28:35,540 --> 00:28:40,100
That's what Bayes formula is telling you, agree?

561
00:28:40,100 --> 00:28:42,620
So now what I want is to have something

562
00:28:42,620 --> 00:28:48,140
that's telling me how this is going to work, OK?

563
00:28:48,140 --> 00:28:54,460
So what is going to play the role of those events, A and B?

564
00:28:54,460 --> 00:29:02,020
Well, one is going to be the distribution of my parameter theta

565
00:29:02,020 --> 00:29:04,500
given that I see the data, and this

566
00:29:04,500 --> 00:29:06,540
is going to tell me what is the distribution of the data

567
00:29:06,540 --> 00:29:09,260
given that I know what my parameter theta is.

568
00:29:09,260 --> 00:29:12,660
But that part, if this is data and this is the parameter theta,

569
00:29:12,660 --> 00:29:15,700
this is what we've been doing all along.

570
00:29:15,700 --> 00:29:18,700
The distribution of the data given the parameter here

571
00:29:18,700 --> 00:29:23,140
was n i i d Bernoulli P. I know that.

572
00:29:23,140 --> 00:29:27,940
I know exactly what their joint probability mass function is.

573
00:29:27,940 --> 00:29:29,300
Then that was what?

574
00:29:29,300 --> 00:29:32,700
So we said that this is going to be my data,

575
00:29:32,700 --> 00:29:37,260
and this is going to be my parameter, OK?

576
00:29:37,260 --> 00:29:40,180
So that means that this is the probability of my data

577
00:29:40,180 --> 00:29:40,940
given the parameter.

578
00:29:40,940 --> 00:29:42,980
This is the probability given the parameter.

579
00:29:42,980 --> 00:29:45,820
This is the probability of the parameter.

580
00:29:45,820 --> 00:29:46,300
What is this?

581
00:29:46,300 --> 00:29:49,340
What did we call this?

582
00:29:49,340 --> 00:29:50,260
This is the prior.

583
00:29:50,260 --> 00:29:53,580
It's just the distribution of my parameter.

584
00:29:53,580 --> 00:29:55,900
Now, what is this?

585
00:29:55,900 --> 00:29:58,860
Well, this is just the distribution of the data itself,

586
00:29:58,860 --> 00:29:59,500
all right?

587
00:29:59,540 --> 00:30:08,260
So this is essentially the distribution of this if this

588
00:30:08,260 --> 00:30:15,100
was indeed not conditioned on P, right?

589
00:30:15,100 --> 00:30:18,700
So if I don't condition on P, this data

590
00:30:18,700 --> 00:30:23,260
is going to be a bunch of i i d Bernoulli

591
00:30:23,260 --> 00:30:25,460
with some parameter, but the parameter is random, right?

592
00:30:25,460 --> 00:30:27,820
So for different realization of this data set,

593
00:30:27,940 --> 00:30:30,100
I'm going to get different parameters for the Bernoulli.

594
00:30:30,100 --> 00:30:34,540
And so that leads to some sort of convolution.

595
00:30:34,540 --> 00:30:36,140
I mean, it's not really a convolution in this case,

596
00:30:36,140 --> 00:30:38,660
but it's some sort of composition of distributions, right?

597
00:30:38,660 --> 00:30:41,260
I have the distribution, the randomness that comes from here,

598
00:30:41,260 --> 00:30:44,340
and then the randomness that comes from realizing the Bernoulli.

599
00:30:44,340 --> 00:30:46,260
So that's just the marginal distribution,

600
00:30:46,260 --> 00:30:48,700
and it actually might be painful to understand what this is,

601
00:30:48,700 --> 00:30:49,540
right?

602
00:30:49,540 --> 00:30:51,140
I mean, in a way, it's sort of a mixture,

603
00:30:51,140 --> 00:30:52,900
and it's not super nice.

604
00:30:52,900 --> 00:30:55,820
But we'll see that this actually won't matter for us.

605
00:30:55,820 --> 00:30:57,060
This is going to be some number.

606
00:30:57,060 --> 00:30:58,540
It's going to be there, but it won't

607
00:30:58,540 --> 00:31:00,940
matter for us what it is, because it actually

608
00:31:00,940 --> 00:31:02,460
does not depend on the parameter,

609
00:31:02,460 --> 00:31:06,220
and that's all that matters to us.

610
00:31:06,220 --> 00:31:10,900
OK, so let's put some names on those things, right?

611
00:31:10,900 --> 00:31:13,100
I mean, this was very informal, so let's

612
00:31:13,100 --> 00:31:19,660
put some actual names on what we want to call what we call prior.

613
00:31:19,660 --> 00:31:22,220
So what is the formal definition of a prior?

614
00:31:22,220 --> 00:31:24,860
What is the formal definition of a posterior?

615
00:31:24,980 --> 00:31:27,500
And what are the rules to update it, OK?

616
00:31:27,500 --> 00:31:30,380
So I'm going to have my data, which is going to be x1, xn.

617
00:31:33,620 --> 00:31:38,540
And so let's say they're IID, but they don't actually have to.

618
00:31:38,540 --> 00:31:41,340
And so I'm going to have given theta.

619
00:31:47,460 --> 00:31:49,340
And when I say given, it's either given

620
00:31:49,340 --> 00:31:51,900
like I did in the first part of this course

621
00:31:51,900 --> 00:31:54,420
in all previous chapters or conditionally on, right?

622
00:31:54,420 --> 00:31:57,540
So if you're thinking like a Bayesian,

623
00:31:57,540 --> 00:31:59,820
what I really mean is conditionally

624
00:31:59,820 --> 00:32:02,180
on this random parameter, OK?

625
00:32:02,180 --> 00:32:05,020
So it's like as if it was a fixed number.

626
00:32:05,020 --> 00:32:10,140
Then they're going to have the distribution, x1, xn,

627
00:32:10,140 --> 00:32:11,860
is going to have some distribution.

628
00:32:11,860 --> 00:32:19,180
Let's say, let's assume for now it's pdf, pn of x1, xn, OK?

629
00:32:19,180 --> 00:32:22,260
And I'm going to write theta like this.

630
00:32:22,260 --> 00:32:24,060
So for example, what is this?

631
00:32:24,580 --> 00:32:27,140
So let's say this is a pdf.

632
00:32:27,140 --> 00:32:28,140
It could be a pmf.

633
00:32:28,140 --> 00:32:31,260
Everything I say, I'm going to think of them as being pdfs.

634
00:32:31,260 --> 00:32:32,940
I'm going to combine pdfs with pdf,

635
00:32:32,940 --> 00:32:35,660
but I could combine pdf with pmfs, pmf with pdfs,

636
00:32:35,660 --> 00:32:37,460
or pmf with pms, OK?

637
00:32:37,460 --> 00:32:41,140
So everywhere you see a d, it could be an m.

638
00:32:41,140 --> 00:32:42,660
All right, so now I have those things.

639
00:32:42,660 --> 00:32:43,860
So what does that mean?

640
00:32:43,860 --> 00:32:53,900
So here's some example, x1, xn are IID and theta1, right?

641
00:32:53,900 --> 00:32:57,420
So now I know exactly what the joint pdf of this thing is.

642
00:32:57,420 --> 00:33:03,740
So it means that pn of x1, xn given theta is equal to what?

643
00:33:03,740 --> 00:33:09,380
Well, it's like 1 over sigma root, sorry, 1 root 2 pi

644
00:33:09,380 --> 00:33:13,820
to the power n e to the minus sum from i

645
00:33:13,820 --> 00:33:18,500
equal 1 to n of xi minus theta squared divided by 2, right?

646
00:33:18,500 --> 00:33:21,900
So that's just the joint distribution of n iID and theta

647
00:33:21,900 --> 00:33:25,060
1 random variables, OK?

648
00:33:25,060 --> 00:33:27,220
So that's my pn given theta.

649
00:33:27,220 --> 00:33:31,620
Now, this is what we denoted by sort of like f sub theta

650
00:33:31,620 --> 00:33:33,340
before, right?

651
00:33:33,340 --> 00:33:35,860
We had this subscript before, but now we just

652
00:33:35,860 --> 00:33:37,420
put a bar in theta because we want

653
00:33:37,420 --> 00:33:40,700
to remember that this is actually conditioned on theta, right?

654
00:33:40,700 --> 00:33:42,140
But this is just notation.

655
00:33:42,140 --> 00:33:46,060
You should just think of this as being just the usual thing

656
00:33:46,060 --> 00:33:50,740
that you get from some statistical model, all right?

657
00:33:50,740 --> 00:33:54,540
So now, that's going to be pn.

658
00:33:54,540 --> 00:34:00,140
And here, I'm going to assume that theta is,

659
00:34:00,140 --> 00:34:01,420
why do I put pi here?

660
00:34:08,780 --> 00:34:10,820
OK.

661
00:34:10,820 --> 00:34:19,540
So theta has prior distribution pi.

662
00:34:20,820 --> 00:34:28,980
OK, so for example, so think of it as either PDF or PMF again.

663
00:34:28,980 --> 00:34:33,860
So for example, pi of theta was what?

664
00:34:33,860 --> 00:34:40,260
Well, it was some constant times theta to the a minus 1,

665
00:34:40,260 --> 00:34:43,780
1 minus theta to the a minus 1, right?

666
00:34:43,780 --> 00:34:49,060
So it has some prior distribution, and that's another PMF.

667
00:34:49,100 --> 00:34:52,340
So now, I'm given the distribution of my x's given theta.

668
00:34:52,340 --> 00:34:53,940
I'm given the distribution of my theta,

669
00:34:53,940 --> 00:34:57,420
so I'm given this guy, right?

670
00:34:57,420 --> 00:35:00,140
That's this guy.

671
00:35:00,140 --> 00:35:05,420
I'm given that guy, which is my pi, right?

672
00:35:05,420 --> 00:35:11,740
So that's my pn of x1, xn given theta.

673
00:35:11,740 --> 00:35:14,020
That's my pi of theta.

674
00:35:14,020 --> 00:35:17,340
And then I have here just, this is what?

675
00:35:17,340 --> 00:35:27,260
Well, this is just the integral of pn x1, xn times pi of theta d

676
00:35:27,260 --> 00:35:28,340
theta, right?

677
00:35:28,340 --> 00:35:29,860
Overall possible sets of theta.

678
00:35:29,860 --> 00:35:33,420
That's just when I integrate out my theta,

679
00:35:33,420 --> 00:35:35,820
or I compute, say, the marginal distribution,

680
00:35:35,820 --> 00:35:37,340
I get this by integrating, right?

681
00:35:37,340 --> 00:35:40,500
That's just basic probability, conditional probabilities,

682
00:35:40,500 --> 00:35:41,000
right?

683
00:35:41,000 --> 00:35:42,620
Then if I had the PMF, I would just

684
00:35:42,620 --> 00:35:45,860
sum over the values of theta's, OK?

685
00:35:48,340 --> 00:35:55,220
So now, what I want is to find what's called,

686
00:35:55,220 --> 00:35:58,940
so that's the prior distribution.

687
00:35:58,940 --> 00:36:01,060
And I want to find the posterior distribution.

688
00:36:01,060 --> 00:36:18,820
So it's called, it's pi of theta given x1, xn.

689
00:36:21,540 --> 00:36:23,540
And so if I use Bayes' rule, I know

690
00:36:23,540 --> 00:36:34,620
that this is pn of x1, xn given theta times pi of theta.

691
00:36:34,620 --> 00:36:37,540
And then it's divided by the distribution

692
00:36:37,540 --> 00:36:41,100
of those guys, which I will write as integral over theta

693
00:36:41,100 --> 00:36:48,860
of pn x1, xn given theta times pi of theta d theta.

694
00:36:54,220 --> 00:36:57,180
Everybody's with me still?

695
00:36:57,180 --> 00:36:59,220
So if you're not comfortable with this,

696
00:36:59,220 --> 00:37:03,020
it means that you probably need to go read your couple pages

697
00:37:03,020 --> 00:37:05,820
on conditional densities and conditional PMFs

698
00:37:05,820 --> 00:37:07,420
from your probability class.

699
00:37:07,420 --> 00:37:08,980
There's really not much there.

700
00:37:08,980 --> 00:37:12,220
It's just a matter of being able to define those quantities.

701
00:37:12,220 --> 00:37:15,620
F density of x given y, this is just

702
00:37:15,620 --> 00:37:17,260
what's called a conditional density.

703
00:37:17,260 --> 00:37:18,820
You need to understand what this object is

704
00:37:18,820 --> 00:37:21,940
and how it relates to the joint distribution of x and y,

705
00:37:21,940 --> 00:37:24,820
or maybe the distribution of x or the distribution of y.

706
00:37:27,380 --> 00:37:28,900
But it's the same rules.

707
00:37:28,900 --> 00:37:31,420
I mean, one way to actually remember this

708
00:37:31,420 --> 00:37:33,700
is this is exactly the same rules as this.

709
00:37:33,700 --> 00:37:35,780
When you see a bar, it's the same thing

710
00:37:35,780 --> 00:37:37,700
as the probability of this and this guy.

711
00:37:37,700 --> 00:37:42,020
So for densities, it's just a comma divided by the second guy.

712
00:37:42,020 --> 00:37:45,260
The probability of the second guy, that's it.

713
00:37:45,260 --> 00:37:46,980
So if you remember this, you can just

714
00:37:46,980 --> 00:37:49,980
do some pattern matching and see what I just wrote here.

715
00:37:50,740 --> 00:37:51,740
OK?

716
00:37:51,740 --> 00:37:53,020
OK.

717
00:37:53,020 --> 00:37:56,940
So now I can compute every single one of these guys.

718
00:37:56,940 --> 00:38:03,980
This is something I get from my modeling.

719
00:38:03,980 --> 00:38:05,260
So I did not write this.

720
00:38:05,260 --> 00:38:09,060
It's not written in the slides.

721
00:38:09,060 --> 00:38:14,740
But I give a name to this guy that was my prior distribution.

722
00:38:14,740 --> 00:38:16,500
And that was my posterior distribution.

723
00:38:20,980 --> 00:38:26,980
In chapter 3, maybe, what did we call this guy?

724
00:38:31,980 --> 00:38:33,580
Well, the one that does not have a name.

725
00:38:33,580 --> 00:38:39,580
And that's in the box, this guy.

726
00:38:39,580 --> 00:38:40,300
How did we call it?

727
00:38:47,300 --> 00:38:49,220
It is the joint distribution of the x i's.

728
00:38:50,980 --> 00:38:54,780
And we give you the name.

729
00:38:54,780 --> 00:38:56,140
It's the likelihood, right?

730
00:38:56,140 --> 00:38:57,660
This is exactly the likelihood.

731
00:38:57,660 --> 00:38:59,140
This was the likelihood of theta.

732
00:39:03,940 --> 00:39:06,380
And this is something that's very important to remember.

733
00:39:06,380 --> 00:39:10,300
And that really reminds you that these things are really

734
00:39:10,300 --> 00:39:12,940
not that different, maximum likelihood estimation

735
00:39:12,940 --> 00:39:14,020
and Bayesian estimation.

736
00:39:14,020 --> 00:39:18,580
Because your posterior is really just your likelihood

737
00:39:18,580 --> 00:39:22,820
times something that's just putting some weights on the

738
00:39:22,820 --> 00:39:26,340
Thetas, depending on where you think theta should be.

739
00:39:26,340 --> 00:39:28,700
So if I had, say, a maximum likelihood estimator in my

740
00:39:28,700 --> 00:39:31,140
likelihood and theta looked like this.

741
00:39:31,140 --> 00:39:33,460
But my prior in theta looked like this.

742
00:39:33,460 --> 00:39:36,980
I said, oh, I really want Thetas that are like this.

743
00:39:36,980 --> 00:39:39,460
So what's going to happen is that I'm going to turn this

744
00:39:39,460 --> 00:39:41,380
into some posterior that looks like this.

745
00:39:44,420 --> 00:39:47,620
So I'm just really weighting this posterior.

746
00:39:47,620 --> 00:39:49,940
This is a constant that does not depend on theta, right?

747
00:39:49,940 --> 00:39:50,500
Agreed?

748
00:39:50,500 --> 00:39:53,460
I integrated over Thetas, so Thetas go on.

749
00:39:53,460 --> 00:39:56,140
So forget about this guy.

750
00:39:56,140 --> 00:39:59,420
I have basically that the posterior distribution up to

751
00:39:59,420 --> 00:40:01,860
scaling, because it has to be a probability density and not

752
00:40:01,860 --> 00:40:04,660
just any function that's positive, is the product of

753
00:40:04,660 --> 00:40:05,140
this guy.

754
00:40:05,140 --> 00:40:06,980
It's a weighted version of my likelihood.

755
00:40:06,980 --> 00:40:07,900
That's all it is.

756
00:40:07,900 --> 00:40:10,580
I'm just weighting the likelihood using my

757
00:40:10,580 --> 00:40:13,140
prior belief on theta.

758
00:40:13,140 --> 00:40:17,220
And so given this guy, a natural estimator, if you

759
00:40:17,220 --> 00:40:21,100
follow the maximum likelihood principle, would be the

760
00:40:21,100 --> 00:40:23,220
maximum of this posterior.

761
00:40:23,220 --> 00:40:24,620
Agreed?

762
00:40:24,620 --> 00:40:28,620
That would basically be doing exactly what a maximum

763
00:40:28,620 --> 00:40:31,660
likelihood estimation is telling you.

764
00:40:31,660 --> 00:40:33,540
So it turns out that you can.

765
00:40:33,540 --> 00:40:35,340
It's called maximum a posteriori.

766
00:40:35,340 --> 00:40:39,300
And I won't talk much about this or map.

767
00:40:39,300 --> 00:40:44,500
So that's maximum a posteriori.

768
00:40:44,500 --> 00:40:50,500
So it's just the theta hat is the arg max of pi theta given

769
00:40:50,500 --> 00:40:51,500
x1, xn.

770
00:40:55,180 --> 00:40:56,220
It sounds like it's OK.

771
00:40:56,220 --> 00:40:59,740
I give you a density and you say, OK, I have a density for

772
00:40:59,740 --> 00:41:00,980
all values of my parameters.

773
00:41:00,980 --> 00:41:03,340
You're asking me to summarize it into one number.

774
00:41:03,340 --> 00:41:06,580
I'm just going to take the most likely number of those guys.

775
00:41:06,580 --> 00:41:08,300
But you could summarize it otherwise.

776
00:41:08,300 --> 00:41:10,780
You could take the average, right?

777
00:41:10,780 --> 00:41:12,460
You could take the median.

778
00:41:12,460 --> 00:41:14,380
You could take a bunch of numbers.

779
00:41:14,380 --> 00:41:16,820
And the beauty of Bayesian statistics is that you don't

780
00:41:16,820 --> 00:41:19,180
have to take any number in particular.

781
00:41:19,180 --> 00:41:21,500
You have an entire posterior distribution.

782
00:41:21,500 --> 00:41:25,380
This is not only telling you where theta is, but it's

783
00:41:25,380 --> 00:41:29,980
actually telling you the difference if you actually

784
00:41:29,980 --> 00:41:33,100
give as something, it gives you the posterior, right?

785
00:41:33,100 --> 00:41:36,300
So now let's say the theta is a p between 0 and 1.

786
00:41:36,300 --> 00:41:40,380
If my posterior distribution looks like this, or if my

787
00:41:40,380 --> 00:41:44,020
posterior distribution looks like this, then those two

788
00:41:44,020 --> 00:41:47,660
guys have one the same mode, right?

789
00:41:47,660 --> 00:41:49,220
This is the same value.

790
00:41:49,220 --> 00:41:51,700
And they're symmetric, so they also have the same mean.

791
00:41:51,700 --> 00:41:53,580
So these two posterior distributions give me the

792
00:41:53,580 --> 00:41:55,540
same summary into one number.

793
00:41:55,540 --> 00:41:58,740
However, clearly one is much more confident than the other

794
00:41:58,740 --> 00:42:03,540
one, so I might as well just speed that as a solution.

795
00:42:03,540 --> 00:42:05,220
Some people can, you can do even better.

796
00:42:05,220 --> 00:42:09,620
People actually do things such as drawing a random number

797
00:42:09,620 --> 00:42:10,540
from this distribution.

798
00:42:10,540 --> 00:42:12,780
So this is my number.

799
00:42:12,780 --> 00:42:14,940
Well, that's kind of dangerous, but you can imagine you

800
00:42:14,940 --> 00:42:17,060
could do this, right?

801
00:42:17,060 --> 00:42:19,700
All right.

802
00:42:19,700 --> 00:42:22,140
So this is what works.

803
00:42:22,140 --> 00:42:23,580
That's what we went through.

804
00:42:23,580 --> 00:42:28,300
So here, as you notice, I don't care so much about this

805
00:42:28,300 --> 00:42:30,220
part here, right?

806
00:42:30,220 --> 00:42:31,700
Because it does not depend on theta.

807
00:42:31,700 --> 00:42:35,180
So I know that given the product of those two things,

808
00:42:35,180 --> 00:42:37,740
this thing is only the constant that I need to divide so

809
00:42:37,740 --> 00:42:40,140
that when I integrate this thing over theta, it

810
00:42:40,140 --> 00:42:41,460
integrates to one.

811
00:42:41,460 --> 00:42:44,780
Because this has to be a probability density on theta.

812
00:42:44,780 --> 00:42:48,020
So I can write this and just forget about that part, and

813
00:42:48,020 --> 00:42:51,940
that's what's right on the top of this slide.

814
00:42:51,940 --> 00:42:57,860
Just this notation, this sort of weird alpha or, I don't know,

815
00:42:57,860 --> 00:43:00,500
infinity sine crop to the right, whatever you want to call

816
00:43:00,500 --> 00:43:04,420
this, this thing is actually just really emphasizing the

817
00:43:04,420 --> 00:43:06,260
fact that I don't care.

818
00:43:06,260 --> 00:43:13,500
I write it because I can, and you know what it is, but you

819
00:43:13,500 --> 00:43:18,740
don't actually have to, well, in some instances, you have to

820
00:43:18,740 --> 00:43:20,540
compute the integral, in some instances, you don't have to

821
00:43:20,540 --> 00:43:23,300
compute the integral, and a lot of Bayesian computation is

822
00:43:23,300 --> 00:43:26,700
about saying, OK, it's actually really hard to compute this

823
00:43:26,700 --> 00:43:28,260
integral, so I'd rather not doing it.

824
00:43:28,260 --> 00:43:31,540
So let me try to find some methods that allow me to

825
00:43:31,540 --> 00:43:34,540
sample from the posterior distribution without having to

826
00:43:34,540 --> 00:43:37,660
compute this, and that's what's called Monte Carlo Markov

827
00:43:37,660 --> 00:43:40,420
chains, or MCMC, and that's exactly what they're doing.

828
00:43:40,420 --> 00:43:42,860
They're just using only ratios of things like that for

829
00:43:42,860 --> 00:43:46,140
different datas, and which means that if you take ratios, the

830
00:43:46,140 --> 00:43:48,500
normalizing constant is gone, and you don't need to find this

831
00:43:48,500 --> 00:43:50,780
integral.

832
00:43:50,780 --> 00:43:53,140
So we won't go into those details at all.

833
00:43:53,140 --> 00:43:56,140
That would be the purpose of an entire course on Bayesian

834
00:43:56,140 --> 00:43:56,580
inference.

835
00:43:56,580 --> 00:44:00,300
Actually, even Bayesian computations would be an

836
00:44:00,300 --> 00:44:02,660
entire course on its own.

837
00:44:02,660 --> 00:44:04,340
There's some very interesting things that are going on

838
00:44:04,340 --> 00:44:08,100
there, the interface of stats and computation.

839
00:44:08,100 --> 00:44:12,020
All right, so let's go back to our example and see if we

840
00:44:12,020 --> 00:44:13,900
can actually compute any of those things, because it's very

841
00:44:13,900 --> 00:44:17,780
nice to give you some data, some formulas, but let's see if

842
00:44:17,780 --> 00:44:19,820
we can actually do it.

843
00:44:19,820 --> 00:44:24,020
In particular, can I actually recover this claim that the

844
00:44:24,020 --> 00:44:31,220
posterior associated to a beta prior with Bernoulli

845
00:44:31,260 --> 00:44:34,980
likelihood is actually giving me a beta again.

846
00:44:34,980 --> 00:44:36,700
All right, so what was my prior?

847
00:44:41,220 --> 00:44:45,940
Well, it was beta, so P was following a beta AA, which

848
00:44:45,940 --> 00:44:56,860
means that P, the density, so that was pi of theta, well, I

849
00:44:56,940 --> 00:45:02,860
am going to write it as pi of P, was proportional to P to the

850
00:45:02,860 --> 00:45:08,780
A minus 1 times 1 minus P to the A minus 1, right?

851
00:45:08,780 --> 00:45:10,420
So that's the first ingredient I need to

852
00:45:10,420 --> 00:45:11,380
compute my posterior.

853
00:45:11,380 --> 00:45:14,300
I really need only two, if I wanted about up to constant.

854
00:45:14,300 --> 00:45:21,860
The second one was P. Well, we've computed that many

855
00:45:21,860 --> 00:45:25,540
times, and we had even a nice compact way of writing it,

856
00:45:25,540 --> 00:45:32,540
which was that Pn of x1, xn, given a parameter P, right?

857
00:45:32,540 --> 00:45:35,780
So the density, the joint density of my data given P,

858
00:45:35,780 --> 00:45:38,580
that's my likelihood, the likelihood of P, was what?

859
00:45:38,580 --> 00:45:45,380
Well, it was P to the sum of the xis, 1 minus P to the n

860
00:45:45,380 --> 00:45:47,140
minus sum of the xis.

861
00:45:50,900 --> 00:45:54,660
Anybody wants me to parse this more, or do you remember

862
00:45:54,660 --> 00:45:57,060
seeing that from maximum likelihood estimation?

863
00:45:57,060 --> 00:45:57,540
Yeah?

864
00:45:57,540 --> 00:46:00,660
So when you condition on random variables, you really just

865
00:46:00,660 --> 00:46:03,780
treat that random variable with something that's

866
00:46:03,780 --> 00:46:04,300
in the middle.

867
00:46:04,300 --> 00:46:05,260
That's what conditioning does.

868
00:46:09,660 --> 00:46:10,460
OK?

869
00:46:10,460 --> 00:46:11,140
Yeah?

870
00:46:11,140 --> 00:46:19,700
On the previous slide, for the bottom there, it's d pi of T.

871
00:46:19,700 --> 00:46:23,380
Can it be d pi of T, or is it?

872
00:46:23,460 --> 00:46:27,180
So d pi of T is a measure theoretic notation,

873
00:46:27,180 --> 00:46:29,900
which I use without thinking, and I should not,

874
00:46:29,900 --> 00:46:32,420
because I can see it upsets you.

875
00:46:32,420 --> 00:46:35,060
D pi of T is just a natural way to say

876
00:46:35,060 --> 00:46:38,020
that I integrate against whatever

877
00:46:38,020 --> 00:46:43,940
I'm given for the prior of theta.

878
00:46:43,940 --> 00:46:47,860
In particular, if theta is just the mix of a PDF

879
00:46:47,860 --> 00:46:49,620
and a point mass, right?

880
00:46:49,620 --> 00:46:54,460
Maybe I say that my P takes value 0.5 with probability 0.5,

881
00:46:54,460 --> 00:46:57,380
and then is uniform on the interval with probability

882
00:46:57,380 --> 00:46:58,420
0.5.

883
00:46:58,420 --> 00:47:01,980
OK, so for this, I neither have a PDF nor a PMF,

884
00:47:01,980 --> 00:47:04,180
but I can still talk about integrating with respect

885
00:47:04,180 --> 00:47:04,980
to this, right?

886
00:47:04,980 --> 00:47:10,020
It's going to look like if I take a function f of T, d pi of T,

887
00:47:10,020 --> 00:47:14,540
is going to be 1 half of f of 1 half, right?

888
00:47:14,540 --> 00:47:17,620
That's the point mass with probability 1 half at 1 half,

889
00:47:17,620 --> 00:47:23,140
plus 1 half of the integral between 0 and 1 of f of T, d T.

890
00:47:23,140 --> 00:47:26,020
So this is just a notation, which is actually,

891
00:47:26,020 --> 00:47:32,460
funnily enough, is interchangeable with pi of d T.

892
00:47:32,460 --> 00:47:37,860
But if you have a density, it's really just the density pi

893
00:47:37,860 --> 00:47:41,460
of T, d T, if pi is really a density.

894
00:47:41,460 --> 00:47:44,660
But that's when pi is a measure in other density.

895
00:47:44,700 --> 00:47:49,220
But so everybody else forget about this.

896
00:47:49,220 --> 00:47:52,180
I mean, this is not something you should really worry about.

897
00:47:52,180 --> 00:47:54,420
At this point, this is more graduate level probability

898
00:47:54,420 --> 00:47:55,980
classes.

899
00:47:55,980 --> 00:47:57,260
But yeah, it's called measure theory,

900
00:47:57,260 --> 00:47:59,180
and that's when you think of pi as being a measure.

901
00:47:59,180 --> 00:48:00,380
In an abstract fashion, you don't have

902
00:48:00,380 --> 00:48:02,780
to worry whether it's a density or not,

903
00:48:02,780 --> 00:48:05,700
or whether it has a density even.

904
00:48:05,700 --> 00:48:09,580
OK, so everybody's OK with this?

905
00:48:09,900 --> 00:48:17,380
All right, so now I need to compute my posterior.

906
00:48:17,380 --> 00:48:23,140
And as I said, my posterior is really

907
00:48:23,140 --> 00:48:28,300
just the product of the likelihood weighted by the prior.

908
00:48:28,300 --> 00:48:32,980
So hopefully, at this stage of your education,

909
00:48:32,980 --> 00:48:35,420
you can multiply two functions.

910
00:48:35,420 --> 00:48:38,260
So what's happening is if I multiply this guy with this guy,

911
00:48:38,260 --> 00:48:42,900
well, p gets this guy to the power of this guy plus this guy.

912
00:48:53,780 --> 00:49:00,060
And then 1 minus p gets the power n minus sum of xi's.

913
00:49:00,060 --> 00:49:02,900
So this is always from i equal 1 to n,

914
00:49:02,900 --> 00:49:04,540
and then plus a minus 1 as well.

915
00:49:09,060 --> 00:49:11,620
And this is, sorry, this is up to constant,

916
00:49:11,620 --> 00:49:15,540
because I still need to solve this.

917
00:49:15,540 --> 00:49:18,420
And I could try to do it, but I really don't have to,

918
00:49:18,420 --> 00:49:23,900
because I know that if my density has this form,

919
00:49:23,900 --> 00:49:25,580
then it's a beta distribution.

920
00:49:25,580 --> 00:49:27,020
And then I can just go on Wikipedia

921
00:49:27,020 --> 00:49:29,180
and see what should be the normalization factor.

922
00:49:29,180 --> 00:49:31,060
But I know it's going to be a beta distribution.

923
00:49:31,060 --> 00:49:33,900
It's actually the beta with parameter.

924
00:49:33,900 --> 00:49:37,500
So this is really my beta with parameter

925
00:49:37,540 --> 00:49:43,620
sum of xi i equal 1 to n plus a minus 1.

926
00:49:43,620 --> 00:49:50,660
And then the second parameter is n minus sum of the xi's plus a minus 1.

927
00:49:50,660 --> 00:49:52,300
OK?

928
00:49:52,300 --> 00:49:55,060
Sorry.

929
00:49:55,060 --> 00:49:58,580
I just wrote what was here.

930
00:49:58,580 --> 00:50:01,620
Oh, what happened to my 1?

931
00:50:01,620 --> 00:50:02,940
Oh, no, sorry, sorry, sorry.

932
00:50:02,940 --> 00:50:05,660
Beta has the power minus 1, right?

933
00:50:05,700 --> 00:50:09,140
So that's the parameter of the beta.

934
00:50:09,140 --> 00:50:11,780
And this is the parameter of the beta, right?

935
00:50:11,780 --> 00:50:15,060
So beta, well, I don't think it's anywhere.

936
00:50:15,060 --> 00:50:16,260
Yeah, beta is over there, right?

937
00:50:16,260 --> 00:50:20,300
So I just replace a by what I see.

938
00:50:20,300 --> 00:50:22,340
a is just becoming this guy plus this guy,

939
00:50:22,340 --> 00:50:23,700
and this guy plus this guy.

940
00:50:26,460 --> 00:50:28,500
Everybody's comfortable with this computation?

941
00:50:29,020 --> 00:50:38,180
All right, so we just agreed that beta priors for Bernoulli

942
00:50:38,180 --> 00:50:41,500
observations are certainly convenient, right?

943
00:50:41,500 --> 00:50:43,980
And because they're just conjugate,

944
00:50:43,980 --> 00:50:46,300
and we know that's what's going to come out in the end,

945
00:50:46,300 --> 00:50:48,300
that's going to be a beta as well.

946
00:50:48,300 --> 00:50:50,180
So I mean, I just claim it was convenient.

947
00:50:50,180 --> 00:50:52,340
It was certainly convenient to compute this, right?

948
00:50:52,340 --> 00:50:55,820
I mean, there was certainly some compatibility

949
00:50:55,820 --> 00:50:57,980
when I had to multiply this function by that function,

950
00:50:57,980 --> 00:51:01,020
and you can imagine that things could go much more wrong

951
00:51:01,020 --> 00:51:03,220
than just having p to some power and p to some power,

952
00:51:03,220 --> 00:51:06,380
1 minus p to some power, 1 minus p to some power.

953
00:51:06,380 --> 00:51:09,100
Things were nice.

954
00:51:09,100 --> 00:51:11,860
Now, this is nice, but I can also question the following

955
00:51:11,860 --> 00:51:12,340
things.

956
00:51:12,340 --> 00:51:14,100
Why beta, for one?

957
00:51:14,100 --> 00:51:16,820
I mean, the beta tells me something,

958
00:51:16,820 --> 00:51:20,780
but that's convenient, but then how do I pick a?

959
00:51:20,780 --> 00:51:25,420
I know that a should definitely capture

960
00:51:25,420 --> 00:51:30,180
the fact that where I want to have my p most likely located,

961
00:51:30,180 --> 00:51:34,540
but it also actually captures the variance of my beta.

962
00:51:34,540 --> 00:51:36,740
And so choosing different a's is going

963
00:51:36,740 --> 00:51:37,940
to have different functions.

964
00:51:37,940 --> 00:51:43,260
If I have a and b, if I started with the beta with parameter

965
00:51:43,260 --> 00:51:46,500
here, I started with a b here.

966
00:51:46,500 --> 00:51:49,940
I would just pick up the b here, agreed?

967
00:51:49,940 --> 00:51:51,300
And that would just be asymmetric,

968
00:51:51,300 --> 00:51:53,820
but they're going to capture mean and variance of this thing.

969
00:51:53,820 --> 00:51:55,740
And so how do I pick those guys?

970
00:51:55,740 --> 00:51:58,860
I mean, if I'm a doctor and you're

971
00:51:58,860 --> 00:52:01,540
asking me, what do you think the chances of this drug working

972
00:52:01,540 --> 00:52:03,460
on this kind of patients is, and I

973
00:52:03,460 --> 00:52:06,100
have to say to spit out the parameters of a beta for you,

974
00:52:06,100 --> 00:52:08,700
it might be a bit of a complicated thing to do.

975
00:52:08,700 --> 00:52:10,820
So how do you do this, especially for problems?

976
00:52:10,820 --> 00:52:16,180
So by now, people have actually mastered the art of coming up

977
00:52:16,180 --> 00:52:19,220
with how to formulate those numbers,

978
00:52:19,220 --> 00:52:21,580
but in new problems that come up, how do you do this?

979
00:52:21,620 --> 00:52:23,900
What happens if you want to use Bayesian methods,

980
00:52:23,900 --> 00:52:28,220
but you actually do not know what you expect to see?

981
00:52:28,220 --> 00:52:31,060
Maybe this is the first time you've, I mean, to be fair,

982
00:52:31,060 --> 00:52:33,260
before we started this class, I hope all of you

983
00:52:33,260 --> 00:52:36,460
had no idea whether people tended to bend their head

984
00:52:36,460 --> 00:52:38,260
to the right or to the left before kissing,

985
00:52:38,260 --> 00:52:40,580
because if you did, well, you have too much time on your hand

986
00:52:40,580 --> 00:52:42,340
and I should double your homework.

987
00:52:42,340 --> 00:52:46,220
And so in this case, you have to sort of,

988
00:52:46,220 --> 00:52:48,860
maybe you still want to use the Bayesian machinery.

989
00:52:48,860 --> 00:52:51,020
Maybe you just want to do something nice.

990
00:52:51,020 --> 00:52:51,700
It's nice, right?

991
00:52:51,700 --> 00:52:53,300
I mean, it worked out pretty well.

992
00:52:53,300 --> 00:52:54,660
And so what if you want to do well,

993
00:52:54,660 --> 00:52:56,340
you actually want to use some priors that

994
00:52:56,340 --> 00:52:59,820
have no carry, no information that basically do not

995
00:52:59,820 --> 00:53:02,660
prefer any theta to another theta.

996
00:53:02,660 --> 00:53:06,660
Now, you could read this slide or you could look at this formula.

997
00:53:10,020 --> 00:53:14,940
We just said that this pi here was just here

998
00:53:14,940 --> 00:53:17,820
to weigh some thetas more than others,

999
00:53:17,820 --> 00:53:19,900
depending on our prior belief.

1000
00:53:19,900 --> 00:53:22,420
If our prior belief does not want to put any preference

1001
00:53:22,420 --> 00:53:25,500
towards some thetas than to others, what do I do?

1002
00:53:28,100 --> 00:53:29,500
Yeah, remove it.

1003
00:53:29,500 --> 00:53:31,460
And the way to remove something we multiply by

1004
00:53:31,460 --> 00:53:32,660
is just replace it by 1.

1005
00:53:32,660 --> 00:53:34,820
That's really what we're doing.

1006
00:53:34,820 --> 00:53:38,460
So if this was a constant, then not depending on theta,

1007
00:53:38,460 --> 00:53:41,420
then that would mean that we're not preferring any theta.

1008
00:53:41,420 --> 00:53:44,380
And we're looking sort of at the likelihood,

1009
00:53:44,380 --> 00:53:46,580
but not as a function that we're trying to maximize,

1010
00:53:46,580 --> 00:53:50,220
but as a function that we normalize in such a way

1011
00:53:50,220 --> 00:53:52,500
that it's actually a distribution.

1012
00:53:52,500 --> 00:53:54,780
So if I have pi, which is not here,

1013
00:53:54,780 --> 00:53:56,820
this is really just taking the likelihood, which

1014
00:53:56,820 --> 00:53:59,180
is a positive function, mean not integrate to 1.

1015
00:53:59,180 --> 00:54:02,340
So I normalize it so that it integrates to 1.

1016
00:54:02,340 --> 00:54:05,140
And then I just say, well, this is my posterior distribution.

1017
00:54:05,140 --> 00:54:06,780
Now, I could just maximize this thing

1018
00:54:06,780 --> 00:54:09,100
and spit out my maximum likelihood estimator,

1019
00:54:09,100 --> 00:54:10,980
but now I can also integrate and find

1020
00:54:10,980 --> 00:54:12,380
what the expectation of this guy is.

1021
00:54:12,380 --> 00:54:14,220
I can find what the median of this guy is.

1022
00:54:14,220 --> 00:54:16,380
I can sample data from this guy.

1023
00:54:16,380 --> 00:54:19,460
I can understand what the variance of this guy is,

1024
00:54:19,460 --> 00:54:21,700
which is something we did not do when we just

1025
00:54:21,700 --> 00:54:23,660
did maximum likelihood estimation because,

1026
00:54:23,660 --> 00:54:25,620
given a function, all we cared about

1027
00:54:25,620 --> 00:54:30,020
was the arg max of this function.

1028
00:54:30,020 --> 00:54:35,860
So this priors are called uninformative.

1029
00:54:35,860 --> 00:54:40,100
So this is just replacing this number by 1.

1030
00:54:40,100 --> 00:54:44,020
And if I have a or by a constant, because it still

1031
00:54:44,020 --> 00:54:45,700
has to be a density.

1032
00:54:45,700 --> 00:54:50,420
And so if I have a bounded set, I'm

1033
00:54:50,420 --> 00:54:53,540
just looking for the uniform distribution on this bounded

1034
00:54:53,540 --> 00:54:56,620
set, the one that puts constant one

1035
00:54:56,620 --> 00:54:59,140
over the size of this thing.

1036
00:54:59,140 --> 00:55:01,660
But if I have an unbounded set, what

1037
00:55:01,660 --> 00:55:03,900
is the density that takes a constant value

1038
00:55:03,900 --> 00:55:07,580
on the entire real line, for example?

1039
00:55:07,580 --> 00:55:08,500
What is this density?

1040
00:55:16,580 --> 00:55:18,300
Doesn't exist.

1041
00:55:18,300 --> 00:55:20,860
I mean, it just doesn't exist.

1042
00:55:20,860 --> 00:55:23,500
The way you can think of it is a Gaussian with the variance

1043
00:55:23,500 --> 00:55:26,380
going to infinity, maybe, or something like this.

1044
00:55:26,380 --> 00:55:27,900
But you can think of it in many ways.

1045
00:55:27,900 --> 00:55:32,380
You can think of the limit of the uniform between minus t

1046
00:55:32,380 --> 00:55:34,220
and t with t going to infinity.

1047
00:55:34,220 --> 00:55:36,500
But this thing is actually 0.

1048
00:55:36,500 --> 00:55:38,340
There's nothing there.

1049
00:55:38,340 --> 00:55:42,020
And so you can actually still talk about this.

1050
00:55:42,020 --> 00:55:44,220
You could always talk about this thing

1051
00:55:44,220 --> 00:55:46,580
where you think of this guy as being a constant,

1052
00:55:46,580 --> 00:55:48,500
remove this thing from this equation,

1053
00:55:48,500 --> 00:55:50,700
and just say, well, my posterior is just

1054
00:55:50,700 --> 00:55:54,140
the likelihood divided by the integral of the likelihood

1055
00:55:54,140 --> 00:55:54,700
over theta.

1056
00:55:54,700 --> 00:55:58,660
And if theta is the entire real line, so be it.

1057
00:55:58,660 --> 00:56:00,540
As long as this integral converges,

1058
00:56:00,540 --> 00:56:03,700
you can still talk about this stuff.

1059
00:56:03,700 --> 00:56:06,340
And so this is what's called an improper prior.

1060
00:56:09,140 --> 00:56:11,780
An improper prior is just a non-negative function

1061
00:56:11,780 --> 00:56:13,700
defined on theta, but it does not

1062
00:56:13,700 --> 00:56:19,740
have to integrate neither to 1 nor to anything.

1063
00:56:19,740 --> 00:56:20,940
It does not have to write.

1064
00:56:20,940 --> 00:56:22,740
If I integrate the function equal to 1

1065
00:56:22,740 --> 00:56:24,380
on the entire real line, what do I get?

1066
00:56:27,820 --> 00:56:29,100
Infinity, right?

1067
00:56:29,100 --> 00:56:32,140
I mean, it's not a proper integral,

1068
00:56:32,140 --> 00:56:34,180
and so it's not a proper prior, and it's

1069
00:56:34,180 --> 00:56:35,980
called an improper prior.

1070
00:56:35,980 --> 00:56:39,380
And those improper priors are usually

1071
00:56:39,380 --> 00:56:42,860
what you see when you start to want non-informative priors

1072
00:56:42,860 --> 00:56:44,220
on infinite set status.

1073
00:56:44,220 --> 00:56:46,780
I mean, that's just the nature of it.

1074
00:56:46,780 --> 00:56:50,020
You should think of it as being the uniform distribution

1075
00:56:50,020 --> 00:56:54,620
on some infinite set if that thing were to exist.

1076
00:56:54,620 --> 00:56:59,180
So let's see some examples about non-informative priors.

1077
00:56:59,180 --> 00:57:04,340
So if I'm on the interval 0, 1, this is a finite set,

1078
00:57:04,340 --> 00:57:08,740
so I can talk about the uniform prior on the interval 0, 1

1079
00:57:08,740 --> 00:57:11,540
for a parameter p of a Bernoulli, OK?

1080
00:57:11,540 --> 00:57:27,540
And so if I want to talk about this,

1081
00:57:27,540 --> 00:57:35,100
then it means that my prior is p follows some uniform

1082
00:57:35,100 --> 00:57:37,580
on the interval 0, 1, OK?

1083
00:57:37,580 --> 00:57:45,980
So it means that the density is, well, f of x is 1

1084
00:57:45,980 --> 00:57:48,940
if x is in 0, 1 and 0.

1085
00:57:48,940 --> 00:57:52,020
Otherwise, there's actually not even a normalization.

1086
00:57:52,020 --> 00:57:53,900
This thing integrates to 1.

1087
00:57:53,900 --> 00:57:56,220
And so now, if I look at my likelihood,

1088
00:57:56,220 --> 00:57:57,340
it's still the same thing.

1089
00:57:57,340 --> 00:58:04,460
So my posterior becomes theta x1, xn.

1090
00:58:04,460 --> 00:58:07,180
So that's my posterior.

1091
00:58:07,220 --> 00:58:09,900
I don't write the likelihood again because we still have it.

1092
00:58:09,900 --> 00:58:12,500
Well, we don't have it there anymore.

1093
00:58:12,500 --> 00:58:13,620
Is it here?

1094
00:58:13,620 --> 00:58:15,340
Or did I just erase it?

1095
00:58:15,340 --> 00:58:17,740
Yeah, the likelihood is given here.

1096
00:58:17,740 --> 00:58:20,140
So copy paste over there.

1097
00:58:20,140 --> 00:58:23,060
And so the posterior is just this thing times 1.

1098
00:58:23,060 --> 00:58:24,300
So you will see it in a second.

1099
00:58:24,300 --> 00:58:27,980
So it's p times the sum to the power sum of the xi's,

1100
00:58:27,980 --> 00:58:31,980
1 minus p to the power n minus sum of the xi's.

1101
00:58:31,980 --> 00:58:34,700
And then it's multiplied by 1 and then

1102
00:58:34,700 --> 00:58:42,500
divided by this integral between 0 and 1 of p sum of the xi's,

1103
00:58:42,500 --> 00:58:50,940
1 minus p n minus sum of the xi's dp,

1104
00:58:50,940 --> 00:58:52,220
which does not depend on p.

1105
00:58:52,220 --> 00:58:56,220
And I really don't care what this thing actually is.

1106
00:58:56,220 --> 00:59:03,580
So now, sorry, that's prior posterior of p.

1107
00:59:03,580 --> 00:59:06,100
And now I can see, well, what is this?

1108
00:59:06,100 --> 00:59:13,420
Well, it's actually just the beta with parameters this guy

1109
00:59:13,420 --> 00:59:21,700
plus 1 and this guy plus 1.

1110
00:59:21,700 --> 00:59:38,300
So I didn't tell you what the expectation of a beta was.

1111
00:59:38,300 --> 00:59:41,180
We don't know what the expectation of a beta is.

1112
00:59:41,180 --> 00:59:41,900
Agreed?

1113
00:59:41,900 --> 00:59:45,900
I mean, if I wanted to find, say, the expectation of this thing,

1114
00:59:45,900 --> 00:59:47,660
that would be some good estimator,

1115
00:59:47,660 --> 00:59:49,820
we know that the maximum of this guy,

1116
00:59:49,820 --> 00:59:51,180
what is the maximum of this thing?

1117
00:59:54,860 --> 00:59:56,100
Well, it's just this thing, right?

1118
00:59:56,100 --> 00:59:58,300
I mean, it's the average of the xi's, right?

1119
00:59:58,300 --> 01:00:00,300
That's just the maximum likelihood estimator for Bernoulli.

1120
01:00:00,300 --> 01:00:01,820
We know it's the average.

1121
01:00:01,820 --> 01:00:03,980
Do you think if I take the expectation of this thing,

1122
01:00:03,980 --> 01:00:05,220
I'm going to get the average?

1123
01:00:14,060 --> 01:00:15,820
So actually, I'm not going to get the average.

1124
01:00:15,820 --> 01:00:20,340
I'm going to get this guy plus this guy divided by n plus 1.

1125
01:00:20,340 --> 01:00:24,900
So I'm going to do as if I had, oh, sorry.

1126
01:00:24,900 --> 01:00:27,340
OK, let me not say it like that.

1127
01:00:27,340 --> 01:00:28,900
Let's look at what this thing is doing.

1128
01:00:28,900 --> 01:00:32,900
It's looking at the number of 0's, the number of 1's,

1129
01:00:32,900 --> 01:00:34,540
and it's adding 1.

1130
01:00:34,540 --> 01:00:36,300
And this guy is looking at the number of 0's,

1131
01:00:36,300 --> 01:00:39,220
and it's adding 1, OK?

1132
01:00:39,220 --> 01:00:41,940
Why is it adding this 1?

1133
01:00:41,940 --> 01:00:44,300
What's going on here?

1134
01:00:44,300 --> 01:00:48,140
Well, what would happen if I had, so this actually

1135
01:00:48,140 --> 01:00:52,980
is going to matter mostly when the number of 1's is actually

1136
01:00:52,980 --> 01:00:56,100
0 or the number of 0's is 0?

1137
01:00:56,100 --> 01:00:59,980
Because what it does is just pushes the 0 from non-zero.

1138
01:00:59,980 --> 01:01:03,020
And why is that something that this Bayesian method actually

1139
01:01:03,020 --> 01:01:05,300
does for you automatically is because when

1140
01:01:05,300 --> 01:01:09,140
we put this non-informative prior on p, which

1141
01:01:09,140 --> 01:01:12,020
was uniform on the interval 0, 1, in particular,

1142
01:01:12,020 --> 01:01:16,700
we know that the probability that p is equal to 0 is 0,

1143
01:01:16,700 --> 01:01:19,220
and the probability that p is equal to 1 is 0.

1144
01:01:19,220 --> 01:01:21,220
And so the problem is that, essentially,

1145
01:01:21,220 --> 01:01:24,540
if I did not add this 1 with some positive probability,

1146
01:01:24,540 --> 01:01:27,060
I would be allowed to spit out something that actually

1147
01:01:27,060 --> 01:01:29,900
had p hat, which was equal to 0.

1148
01:01:29,900 --> 01:01:33,300
In the case, if by chance, let's say I have n is equal to 3,

1149
01:01:33,300 --> 01:01:35,940
and I get only 0, 0, 0, right?

1150
01:01:35,940 --> 01:01:40,340
That could happen with probability 1 over p cubed,

1151
01:01:41,340 --> 01:01:43,900
sorry, 1 minus p cubed.

1152
01:01:43,900 --> 01:01:46,380
Then this thing is just going to not,

1153
01:01:46,380 --> 01:01:47,700
that's not something that I want,

1154
01:01:47,700 --> 01:01:49,300
and I'm actually using my prior.

1155
01:01:49,300 --> 01:01:51,180
So my prior is not informative, but somehow it

1156
01:01:51,180 --> 01:01:52,940
captures the fact that I don't want to believe

1157
01:01:52,940 --> 01:01:55,780
that p is going to be either equal to 0 or 1.

1158
01:01:55,780 --> 01:01:59,500
OK, and so that's sort of taken care of here.

1159
01:01:59,500 --> 01:02:05,620
OK, so let's move away a little bit from the Bernoulli example,

1160
01:02:05,620 --> 01:02:06,100
shall we?

1161
01:02:06,100 --> 01:02:08,100
I mean, we think we've seen enough of it.

1162
01:02:08,140 --> 01:02:10,860
And so let's talk about the Gaussian model, right?

1163
01:02:10,860 --> 01:02:17,980
Let's say I want to do Gaussian inference in the,

1164
01:02:17,980 --> 01:02:20,020
I want to do inference in a Gaussian model using

1165
01:02:20,020 --> 01:02:22,020
vision methods.

1166
01:02:22,020 --> 01:02:30,220
OK, so I'm going to actually look at, so say, OK,

1167
01:02:30,220 --> 01:02:44,780
so what I want is that xi, x1, xn, or say, n, 0, 1, i, i, d,

1168
01:02:44,780 --> 01:02:50,260
sorry, theta 1, i, i, d, conditionally on theta.

1169
01:02:50,260 --> 01:02:56,340
OK, so that means that pn of x1, xn, given theta,

1170
01:02:56,340 --> 01:02:58,620
is equal to, well, exactly what I wrote before.

1171
01:02:58,620 --> 01:03:04,380
So 1 square root 2 pi to the n exponential minus 1

1172
01:03:04,380 --> 01:03:09,340
half sum of xi minus theta squared.

1173
01:03:09,340 --> 01:03:11,140
OK, so that's just the joint distribution

1174
01:03:11,140 --> 01:03:13,940
of my n-Gaussians with mean theta.

1175
01:03:13,940 --> 01:03:16,660
Another question is, what is the posterior distribution?

1176
01:03:16,660 --> 01:03:22,420
OK, well, here I said, let's use the uninformative prior,

1177
01:03:22,420 --> 01:03:23,860
which is an improper prior, right?

1178
01:03:23,860 --> 01:03:25,540
It puts weight 1 on everyone.

1179
01:03:25,540 --> 01:03:29,220
It has the so-called uniform on the entire real line,

1180
01:03:29,220 --> 01:03:31,180
so that's certainly not a density.

1181
01:03:31,180 --> 01:03:34,380
But I can still just use this, right?

1182
01:03:34,380 --> 01:03:42,220
So all I need to do is to get this divided by normalizing

1183
01:03:42,220 --> 01:03:43,180
this thing, right?

1184
01:03:43,180 --> 01:03:44,660
So that's what I need to do.

1185
01:03:44,660 --> 01:03:46,380
But if I look at this, right?

1186
01:03:46,380 --> 01:03:49,500
So essentially, I want to understand,

1187
01:03:49,500 --> 01:03:52,980
so this is proportional to exponential minus 1

1188
01:03:52,980 --> 01:03:58,860
half sum from i equal 1 to n of xi minus theta squared.

1189
01:03:58,860 --> 01:04:02,900
And now I want to see this thing as a density not on the xi's,

1190
01:04:02,900 --> 01:04:06,420
but on theta, right?

1191
01:04:06,420 --> 01:04:10,020
What I want is a density on theta.

1192
01:04:10,020 --> 01:04:13,660
So it looks like I have chances of getting something

1193
01:04:13,660 --> 01:04:15,540
that looks like a Gaussian.

1194
01:04:15,540 --> 01:04:17,540
But if I really need to have a Gaussian,

1195
01:04:17,540 --> 01:04:19,540
I would need to see minus 1 half,

1196
01:04:19,540 --> 01:04:22,700
and then I would need to see theta minus something here.

1197
01:04:22,900 --> 01:04:25,220
Not just the sum of something minus theta's.

1198
01:04:25,220 --> 01:04:27,380
So I need to work a little bit more

1199
01:04:27,380 --> 01:04:31,540
so I can see what this to expand the square here.

1200
01:04:31,540 --> 01:04:32,900
So this thing here is going to be

1201
01:04:32,900 --> 01:04:37,380
equal to exponential minus 1 half sum from i equal 1

1202
01:04:37,380 --> 01:04:45,300
to n of xi squared minus 2 xi theta plus theta squared.

1203
01:04:52,700 --> 01:04:53,200
Ah.

1204
01:05:06,140 --> 01:05:11,820
OK, and so now basically what I'm going to do

1205
01:05:11,820 --> 01:05:15,900
is everything, remember, is up to this little sign, right?

1206
01:05:15,900 --> 01:05:19,740
So every time I see a term that does not depend on theta,

1207
01:05:19,740 --> 01:05:22,260
I can just push it in there and just make it disappear.

1208
01:05:22,260 --> 01:05:23,900
Agreed?

1209
01:05:23,900 --> 01:05:26,820
OK, this term here, exponential minus 1

1210
01:05:26,820 --> 01:05:31,780
half sum of xi squared, does it depend on theta?

1211
01:05:31,780 --> 01:05:33,740
No, so I'm just pushing it here.

1212
01:05:33,740 --> 01:05:35,980
This guy, yes, and the other one, yes.

1213
01:05:35,980 --> 01:05:40,180
So this is proportional to exponential xi, sorry,

1214
01:05:40,180 --> 01:05:45,020
sum of the xi.

1215
01:05:45,020 --> 01:05:46,820
And then I'm going to pull out my theta.

1216
01:05:46,820 --> 01:05:50,140
The minus 1 half cancel with the minus 2.

1217
01:05:50,140 --> 01:05:56,900
And then I have minus 1 half sum from i equal 1

1218
01:05:56,900 --> 01:05:58,620
to n of theta squared, right?

1219
01:06:01,620 --> 01:06:03,460
Agreed?

1220
01:06:03,460 --> 01:06:05,380
So now what this thing looks like, well,

1221
01:06:05,380 --> 01:06:09,580
this looks very much like some theta minus something squared.

1222
01:06:09,580 --> 01:06:16,940
This thing here is really just n over 2 times theta.

1223
01:06:16,980 --> 01:06:21,780
So sorry, times theta squared.

1224
01:06:21,780 --> 01:06:25,180
So now what I need to do is to write this of the form theta

1225
01:06:25,180 --> 01:06:28,420
minus something, let's call it mu squared,

1226
01:06:28,420 --> 01:06:31,820
maybe divided by 2 sigma squared, right?

1227
01:06:31,820 --> 01:06:34,220
I want to turn this into that, maybe up to terms

1228
01:06:34,220 --> 01:06:36,540
that do not depend on theta.

1229
01:06:36,540 --> 01:06:39,140
That's what I'm going to try to do.

1230
01:06:39,140 --> 01:06:40,700
So that's called completing the square,

1231
01:06:40,700 --> 01:06:41,940
and that's some exercise you do.

1232
01:06:41,940 --> 01:06:44,140
You've done it probably already in the homework,

1233
01:06:44,140 --> 01:06:46,580
and that's something you do a lot when

1234
01:06:46,580 --> 01:06:48,820
you do Bayesian statistics in particular.

1235
01:06:48,820 --> 01:06:49,500
So let's do this.

1236
01:06:49,500 --> 01:06:51,340
Well, what is going to be the leading term?

1237
01:06:51,340 --> 01:06:54,180
Well, theta squared is going to be multiplied by this thing.

1238
01:06:54,180 --> 01:06:57,140
So I'm going to pull out my n over 2,

1239
01:06:57,140 --> 01:07:04,820
and then I'm going to write this as theta squared minus theta

1240
01:07:04,820 --> 01:07:06,220
minus something squared.

1241
01:07:06,220 --> 01:07:08,300
And this something is going to be 1

1242
01:07:08,300 --> 01:07:10,260
half of what I see in the cross product, right?

1243
01:07:10,580 --> 01:07:12,260
Well, I need to actually pull this thing out.

1244
01:07:12,260 --> 01:07:16,140
So let me write it like that first.

1245
01:07:16,140 --> 01:07:19,420
So that's theta squared.

1246
01:07:19,420 --> 01:07:27,940
And then I'm going to write it as minus 2 times 1 over n,

1247
01:07:27,940 --> 01:07:34,900
sum from i equal 1 to n of xi times theta, right?

1248
01:07:34,900 --> 01:07:37,900
That's exactly just the rewriting of what we had before.

1249
01:07:37,900 --> 01:07:40,100
That's the rewriting of what we had before.

1250
01:07:40,100 --> 01:07:42,660
And that should look much more familiar.

1251
01:07:42,660 --> 01:07:48,340
x squared minus a squared minus 2 blab a,

1252
01:07:48,340 --> 01:07:49,740
and then I missed something.

1253
01:07:49,740 --> 01:07:54,740
So this thing I'm going to be able to rewrite as theta minus xn

1254
01:07:54,740 --> 01:07:57,940
bar squared.

1255
01:07:57,940 --> 01:08:00,740
But then I need to remove the square of xn bar,

1256
01:08:00,740 --> 01:08:01,820
because it's not here.

1257
01:08:02,420 --> 01:08:03,540
OK?

1258
01:08:03,540 --> 01:08:06,020
So I just complete the square.

1259
01:08:06,020 --> 01:08:08,420
And then I actually really don't care what this thing actually

1260
01:08:08,420 --> 01:08:10,020
was, because it's going to go again

1261
01:08:10,020 --> 01:08:12,940
in the little alpha sign over there.

1262
01:08:12,940 --> 01:08:14,540
So this thing eventually is going

1263
01:08:14,540 --> 01:08:22,540
to be proportional to exponential of minus n over 2 times theta

1264
01:08:22,540 --> 01:08:25,620
minus xn bar squared.

1265
01:08:25,620 --> 01:08:27,460
And so we know that this is going to be

1266
01:08:27,460 --> 01:08:31,100
times theta minus xn bar squared.

1267
01:08:31,100 --> 01:08:33,380
And so we know that if this is a density that's

1268
01:08:33,380 --> 01:08:42,060
proportional to this guy, it has to be some n with mean

1269
01:08:42,060 --> 01:08:44,860
xn bar and variance.

1270
01:08:44,860 --> 01:08:47,540
Well, this is supposed to be 1 over sigma squared,

1271
01:08:47,540 --> 01:08:49,420
this guy over here, this n.

1272
01:08:49,420 --> 01:08:53,900
So that's really just 1 over n, OK?

1273
01:08:53,940 --> 01:09:02,500
So the posterior distribution is a Gaussian centered

1274
01:09:02,500 --> 01:09:09,740
at the average of my observations and with variance 1 over n.

1275
01:09:09,740 --> 01:09:10,240
OK?

1276
01:09:13,580 --> 01:09:15,260
Everybody's with me?

1277
01:09:15,260 --> 01:09:17,940
So just why I'm saying this, I mean,

1278
01:09:17,940 --> 01:09:19,700
this was the output of some computation,

1279
01:09:19,700 --> 01:09:21,460
but it sort of makes sense, right?

1280
01:09:21,460 --> 01:09:23,540
It's really telling me that the more observations

1281
01:09:23,540 --> 01:09:26,260
I have, the more concentrated this posterior is,

1282
01:09:26,260 --> 01:09:27,740
concentrated around what?

1283
01:09:27,740 --> 01:09:30,020
Well, around this xn bar.

1284
01:09:30,020 --> 01:09:33,140
So that looks like something we've sort of seen before,

1285
01:09:33,140 --> 01:09:35,420
but it does not have the same meaning somehow.

1286
01:09:35,420 --> 01:09:37,340
This is really just the posterior distribution,

1287
01:09:37,340 --> 01:09:40,500
and it's not really, I mean, it sort of says,

1288
01:09:40,500 --> 01:09:43,340
it's sort of a sanity check that I have this 1 over n when

1289
01:09:43,340 --> 01:09:45,420
I have xn bar, but it's not the same thing

1290
01:09:45,420 --> 01:09:47,500
as saying that the variance of xn bar was 1 over n

1291
01:09:47,500 --> 01:09:49,180
like we had before, OK?

1292
01:09:54,340 --> 01:09:58,300
So as an exercise, well, you probably will have it,

1293
01:09:58,300 --> 01:10:01,300
but I would recommend if you don't get it,

1294
01:10:01,300 --> 01:10:18,020
just try pi of theta to be equal to some n mu 1, OK?

1295
01:10:18,020 --> 01:10:22,380
So here, the prior that we use was completely non-informative.

1296
01:10:22,380 --> 01:10:25,620
What happens if I take my prior to be some Gaussian, which

1297
01:10:25,620 --> 01:10:27,540
is centered at mu, and it has the same variance

1298
01:10:27,540 --> 01:10:30,140
as the other guys, OK?

1299
01:10:30,140 --> 01:10:33,140
So what's going to happen here is that we're going to put a weight,

1300
01:10:33,140 --> 01:10:34,540
and everything that's away from mu

1301
01:10:34,540 --> 01:10:37,740
is going to actually get less weight, right?

1302
01:10:37,740 --> 01:10:40,860
And I want to know how I'm going to be updating this prior

1303
01:10:40,860 --> 01:10:42,100
into a posterior.

1304
01:10:42,100 --> 01:10:45,700
So that's right, so everybody sees what I'm saying here.

1305
01:10:45,700 --> 01:10:47,580
So pi of theta is just so that means

1306
01:10:47,580 --> 01:10:50,060
that pi of theta has the density proportional

1307
01:10:50,060 --> 01:10:55,700
to exponential minus 1 half theta minus mu squared, right?

1308
01:10:55,700 --> 01:11:00,500
So I need to multiply my posterior with this

1309
01:11:00,500 --> 01:11:03,420
and then see what I'd say actually going to be a Gaussian.

1310
01:11:03,420 --> 01:11:04,860
This is also a conjugate prior.

1311
01:11:04,860 --> 01:11:06,460
It's going to spit out another Gaussian.

1312
01:11:06,460 --> 01:11:08,420
You're going to have to complete a square again

1313
01:11:08,420 --> 01:11:10,820
and just check what it's actually giving you.

1314
01:11:10,820 --> 01:11:12,540
And so spoiler alert, it's going to look

1315
01:11:12,540 --> 01:11:14,420
like you get an extra observation, which

1316
01:11:14,420 --> 01:11:17,580
is actually equal to mu, OK?

1317
01:11:17,620 --> 01:11:22,300
So it's going to be the average of n plus 1 observations,

1318
01:11:22,300 --> 01:11:27,500
the first n ones being x1 to xn and the last one being mu.

1319
01:11:27,500 --> 01:11:30,420
And it sort of makes sense.

1320
01:11:30,420 --> 01:11:32,700
OK, so that's actually a fairly simple exercise.

1321
01:11:32,700 --> 01:11:36,740
But before, rather than going into more computation,

1322
01:11:36,740 --> 01:11:38,540
this is something you can definitely do

1323
01:11:38,540 --> 01:11:41,780
in the comfort of your room, I want

1324
01:11:41,780 --> 01:11:43,340
to talk about other types of priors, right?

1325
01:11:43,340 --> 01:11:47,340
So the first thing I said is, OK, there's this beta prior

1326
01:11:47,340 --> 01:11:50,380
that I just pulled out of my hat and that was just convenient.

1327
01:11:50,380 --> 01:11:52,900
Then there was this non-informative prior.

1328
01:11:52,900 --> 01:11:53,820
It was convenient, right?

1329
01:11:53,820 --> 01:11:54,780
It was non-informative.

1330
01:11:54,780 --> 01:11:57,340
So if you don't know anything else,

1331
01:11:57,340 --> 01:11:58,860
maybe that's what you want to do.

1332
01:11:58,860 --> 01:12:01,620
The question is, are there any other priors

1333
01:12:01,620 --> 01:12:04,500
that are sort of principled and generic in the sense

1334
01:12:04,500 --> 01:12:08,460
that the uninformative prior was generic, right?

1335
01:12:08,460 --> 01:12:09,500
I mean, it was equal to 1.

1336
01:12:09,500 --> 01:12:11,380
That's as generic as it gets.

1337
01:12:11,380 --> 01:12:14,140
And so is there anything that's generic as well?

1338
01:12:14,140 --> 01:12:17,220
Well, there's these priors that are called Jeffery's priors.

1339
01:12:17,220 --> 01:12:19,060
And Jeffery's prior is a prior which

1340
01:12:19,060 --> 01:12:21,500
is proportional to square root of the determinant

1341
01:12:21,500 --> 01:12:25,660
of the Fisher information of theta, OK?

1342
01:12:25,660 --> 01:12:28,620
And so this is actually kind of a weird thing to do, right?

1343
01:12:28,620 --> 01:12:31,420
It says, compute your, look at your model, right?

1344
01:12:31,420 --> 01:12:34,340
Your model is going to have a Fisher information.

1345
01:12:34,340 --> 01:12:36,180
Let's say it exists.

1346
01:12:36,180 --> 01:12:39,900
And because we know it does not always

1347
01:12:39,900 --> 01:12:41,580
exist, for example, in the multinomial model,

1348
01:12:41,580 --> 01:12:43,980
we didn't have a Fisher information.

1349
01:12:43,980 --> 01:12:46,140
And so the determinant of a matrix

1350
01:12:46,140 --> 01:12:48,660
is somehow measuring the size of a matrix, right?

1351
01:12:48,660 --> 01:12:50,540
And if you don't trust me, just think

1352
01:12:50,540 --> 01:12:53,860
about the matrix being of size one by one,

1353
01:12:53,860 --> 01:12:56,820
then the determinant is just the number that you have there.

1354
01:12:56,820 --> 01:12:58,660
And so this is really something that

1355
01:12:58,660 --> 01:13:02,500
looks like the Fisher information.

1356
01:13:02,500 --> 01:13:04,700
I mean, it's just basically the amount of information

1357
01:13:04,700 --> 01:13:06,340
is proportional to the amount of information

1358
01:13:06,340 --> 01:13:09,620
that you have at a certain point, OK?

1359
01:13:09,620 --> 01:13:12,220
And so what my prior is saying is saying, well,

1360
01:13:12,220 --> 01:13:14,300
I want to put more weights on those status that

1361
01:13:14,300 --> 01:13:19,860
are going to just extract more information from the data, OK?

1362
01:13:19,860 --> 01:13:22,260
So you can actually compute those things, right?

1363
01:13:22,260 --> 01:13:26,420
So in the first example, Jeffery's prior

1364
01:13:26,420 --> 01:13:28,100
is something that looks like this, right?

1365
01:13:28,100 --> 01:13:30,260
I mean, in one dimension, Fisher information

1366
01:13:30,260 --> 01:13:33,380
is essentially one over the variance, right?

1367
01:13:33,380 --> 01:13:35,620
So that's just one over the square root of the variance,

1368
01:13:35,620 --> 01:13:37,540
because I have the square root.

1369
01:13:37,540 --> 01:13:41,460
And when I have the uniform, sorry,

1370
01:13:41,460 --> 01:13:46,780
the Jeffery's prior, when I have the Gaussian case, right?

1371
01:13:46,780 --> 01:13:48,940
So this is the identity matrix that I

1372
01:13:48,940 --> 01:13:50,620
would have in the Gaussian case.

1373
01:13:50,620 --> 01:13:52,620
So the determinant of the identity is one,

1374
01:13:52,620 --> 01:13:54,900
so square root of one is one.

1375
01:13:54,900 --> 01:13:56,180
And so I would basically get one,

1376
01:13:56,180 --> 01:13:59,220
and that gives me my improper prior, my uninformative prior

1377
01:13:59,220 --> 01:14:00,820
that I had.

1378
01:14:00,820 --> 01:14:03,100
OK, so the uninformative prior one is fine.

1379
01:14:03,100 --> 01:14:06,780
I mean, clearly, all the Thetas carry the same information

1380
01:14:06,780 --> 01:14:07,980
in the Gaussian model, right?

1381
01:14:07,980 --> 01:14:10,220
I mean, whether I translate it here or here,

1382
01:14:10,260 --> 01:14:11,740
it's pretty clear that none of them

1383
01:14:11,740 --> 01:14:13,180
is actually better than the other.

1384
01:14:13,180 --> 01:14:16,540
But clearly, for the Bernoulli case,

1385
01:14:16,540 --> 01:14:22,260
the piece that are closer to the boundary

1386
01:14:22,260 --> 01:14:23,740
carry more information, right?

1387
01:14:23,740 --> 01:14:26,100
So I sort of like those guys because they just

1388
01:14:26,100 --> 01:14:27,700
like carry more information.

1389
01:14:27,700 --> 01:14:30,340
So what I do is that I take this function, so p1 minus p,

1390
01:14:30,340 --> 01:14:34,820
remember, is something that looks like this on the interval

1391
01:14:34,820 --> 01:14:37,980
0, 1, 0, and 1.

1392
01:14:38,020 --> 01:14:41,260
So this guy, 1 over square root of p1 minus p,

1393
01:14:41,260 --> 01:14:47,380
is something that looks like this, agreed?

1394
01:14:47,380 --> 01:14:48,820
And so what he's doing is sort of like

1395
01:14:48,820 --> 01:14:51,180
wants to push towards the piece that actually

1396
01:14:51,180 --> 01:14:54,380
carry more information.

1397
01:14:54,380 --> 01:14:56,980
I mean, whether you want to bias your data that way or not

1398
01:14:56,980 --> 01:14:58,620
is something you need to think about, right?

1399
01:14:58,620 --> 01:15:00,700
I mean, when you put a prior on your data,

1400
01:15:00,700 --> 01:15:03,180
on your parameter, you're sort of like biasing

1401
01:15:03,180 --> 01:15:05,060
towards this idea, your data.

1402
01:15:05,060 --> 01:15:07,740
And maybe that's maybe not such a good idea

1403
01:15:07,740 --> 01:15:13,100
when you have some p that's actually close to 1 half,

1404
01:15:13,100 --> 01:15:13,820
for example.

1405
01:15:13,820 --> 01:15:14,940
You're actually saying, no, I don't

1406
01:15:14,940 --> 01:15:16,620
want to see a p that's close to 1 half.

1407
01:15:16,620 --> 01:15:18,340
Just make a decision one way or another,

1408
01:15:18,340 --> 01:15:19,700
but just make a decision.

1409
01:15:19,700 --> 01:15:22,500
So it's sort of forcing you to do that.

1410
01:15:22,500 --> 01:15:26,100
OK, and so Jeffery's prior, so I'm running out of time,

1411
01:15:26,100 --> 01:15:29,140
so I don't want to go into too much details.

1412
01:15:29,140 --> 01:15:32,420
But we'll probably stop here, actually.

1413
01:15:32,420 --> 01:15:47,780
So Jeffery's priors have this very nice property

1414
01:15:47,780 --> 01:15:51,740
is that they actually do not care about the parametrization

1415
01:15:51,740 --> 01:15:52,980
of your space.

1416
01:15:52,980 --> 01:15:56,380
So if you actually have p, and you suddenly

1417
01:15:56,380 --> 01:15:58,820
decide that p is not the right parameter for Bernoulli,

1418
01:15:58,820 --> 01:16:02,260
but it's p squared, you could decide to parametrize this

1419
01:16:02,260 --> 01:16:03,180
by p squared.

1420
01:16:03,180 --> 01:16:05,420
Maybe your doctor is actually much more

1421
01:16:05,420 --> 01:16:08,860
able to formulate some prior assumption on p squared

1422
01:16:08,860 --> 01:16:09,820
rather than p.

1423
01:16:09,820 --> 01:16:10,980
You never know.

1424
01:16:10,980 --> 01:16:14,380
And so what happens is that Jeffery's priors

1425
01:16:14,380 --> 01:16:15,860
are invariant into this.

1426
01:16:15,860 --> 01:16:18,100
And the reason is because, well, the information carried

1427
01:16:18,100 --> 01:16:20,060
by p is the same as the information carried

1428
01:16:20,060 --> 01:16:21,460
by p squared somehow, right?

1429
01:16:21,460 --> 01:16:28,700
I mean, those are essentially the same, I mean, well,

1430
01:16:28,700 --> 01:16:30,780
yeah, they're essentially the same thing.

1431
01:16:30,780 --> 01:16:34,620
And so I mean, you need to have a one-to-one map, right?

1432
01:16:34,620 --> 01:16:38,060
Where you basically, for each parameter before you

1433
01:16:38,060 --> 01:16:41,100
have another parameter, so let's call eta the new parameters.

1434
01:16:41,100 --> 01:16:50,380
Then the PDF of the new prior indexed by eta this time

1435
01:16:50,380 --> 01:16:52,980
is actually also Jeffery's prior.

1436
01:16:52,980 --> 01:16:55,220
But this time, the new Fisher information

1437
01:16:55,220 --> 01:16:57,340
is not the Fisher information with respect to theta,

1438
01:16:57,340 --> 01:17:00,060
but it says in the Fisher information associated

1439
01:17:00,100 --> 01:17:03,140
to the statistical model indexed by eta.

1440
01:17:03,140 --> 01:17:06,740
So essentially, when you change Jeffery's prior,

1441
01:17:06,740 --> 01:17:08,780
when you change the parameterization of your model,

1442
01:17:08,780 --> 01:17:12,820
you still get Jeffery's prior for the new parameterization,

1443
01:17:12,820 --> 01:17:16,540
which is, in a way, a desirable property.

1444
01:17:16,540 --> 01:17:20,460
All right, so Jeffery's priors, just

1445
01:17:20,460 --> 01:17:22,460
like non-informative priors, are priors

1446
01:17:22,460 --> 01:17:25,100
you want to use when you want a systematic way

1447
01:17:25,100 --> 01:17:28,620
without really thinking about what to pick for your model.

1448
01:17:28,660 --> 01:17:37,060
OK, so, well, OK, I'll finish this next time.

1449
01:17:37,060 --> 01:17:39,940
And we'll talk about Bayesian confidence regions.

1450
01:17:39,940 --> 01:17:41,620
We'll talk about Bayesian estimation.

1451
01:17:41,620 --> 01:17:44,100
Once I have a posterior, what do I get?

1452
01:17:44,100 --> 01:17:45,540
And basically, the only message is

1453
01:17:45,540 --> 01:17:46,980
going to be that, well, you might

1454
01:17:46,980 --> 01:17:48,940
want to integrate against the posterior.

1455
01:17:48,940 --> 01:17:52,140
Find the expectation of your posterior distribution.

1456
01:17:52,140 --> 01:17:54,660
That's a good point estimator for theta.

1457
01:17:54,660 --> 01:18:00,580
And then we'll just do a couple of computation.

1458
01:18:00,580 --> 01:18:01,340
All right, so.

