WEBVTT

00:00.000 --> 00:02.480
The following content is provided under a Creative

00:02.480 --> 00:03.800
Commons license.

00:03.800 --> 00:06.120
Your support will help MIT OpenCourseWare

00:06.120 --> 00:10.080
continue to offer high quality educational resources for free.

00:10.080 --> 00:12.760
To make a donation or to view additional materials

00:12.760 --> 00:16.680
from hundreds of MIT courses, visit MIT OpenCourseWare

00:16.680 --> 00:17.840
at ocw.mit.edu.

00:21.520 --> 00:27.240
So today we'll actually just do a brief chapter

00:27.240 --> 00:28.640
on Bayesian statistics.

00:28.640 --> 00:31.400
And there's entire courses on Bayesian statistics.

00:31.400 --> 00:33.480
There's entire books on Bayesian statistics.

00:33.480 --> 00:36.080
There's entire careers on Bayesian statistics.

00:36.080 --> 00:39.280
So admittedly, I'm not going to be

00:39.280 --> 00:40.960
able to do it justice and tell you

00:40.960 --> 00:42.960
all the interesting things that are happening

00:42.960 --> 00:44.080
in Bayesian statistics.

00:44.080 --> 00:47.320
But I think it's important as a statistician

00:47.320 --> 00:49.360
to know what it is, how it works,

00:49.360 --> 00:52.520
because it's actually a weapon of choice

00:52.520 --> 00:54.400
for many practitioners.

00:54.400 --> 00:58.120
And because it allows them to incorporate their knowledge

00:58.120 --> 01:00.720
about a problem in a fairly systematic manner.

01:00.720 --> 01:03.640
So if you look at, like, say, the Bayesian statistics

01:03.640 --> 01:05.520
literature, it's huge.

01:05.520 --> 01:09.600
And so here I give you sort of a range

01:09.600 --> 01:12.880
of what you can expect to see in Bayesian statistics

01:12.880 --> 01:18.320
from your second edition of a traditional book, something

01:18.320 --> 01:21.440
that involves computation, some things that involve

01:21.440 --> 01:22.240
rethinking.

01:22.240 --> 01:24.760
And there's a lot of Bayesian thinking.

01:24.760 --> 01:27.400
There's a lot of things that talk about sort

01:27.400 --> 01:30.240
of like philosophy of thinking Bayesian.

01:30.240 --> 01:32.280
This book, for example, seems to be one of them.

01:32.280 --> 01:34.720
This book is definitely one of them.

01:34.720 --> 01:38.880
This one represents sort of a broad literature

01:38.880 --> 01:42.240
on Bayesian statistics for applications, for example,

01:42.240 --> 01:43.560
in social sciences.

01:43.560 --> 01:45.280
But even in large-scale machine learning,

01:45.280 --> 01:47.320
there's a lot of Bayesian statistics happening,

01:47.320 --> 01:50.280
particularly using something called Bayesian parametrics

01:50.280 --> 01:53.480
or hierarchical Bayesian modeling.

01:53.480 --> 01:59.520
So we do have some experts at MIT in the C cell.

01:59.520 --> 02:03.600
Tamara Broderick, for example, is a person who does quite

02:03.600 --> 02:06.200
a bit of interesting work on Bayesian parametrics.

02:06.200 --> 02:08.320
And if that's something you want to know more about,

02:08.320 --> 02:10.920
I urge you to go and talk to her.

02:10.920 --> 02:14.040
So before we go in those more advanced things,

02:14.040 --> 02:17.200
we need to start with what is the Bayesian approach?

02:17.200 --> 02:18.680
What do Bayesians do?

02:18.680 --> 02:22.600
And how is it different from what we've been doing so far?

02:22.640 --> 02:26.360
So to understand the difference between Bayesians

02:26.360 --> 02:28.840
and what we've been doing so far is

02:28.840 --> 02:31.360
we need to first put a name on what we've been doing so far.

02:31.360 --> 02:32.960
It's called Frequentist Statistics.

02:32.960 --> 02:36.480
So it's usually Bayesian versus Frequentist statistics.

02:36.480 --> 02:38.800
I mean, by versus, I don't mean that there's naturally

02:38.800 --> 02:40.400
an opposition to them.

02:40.400 --> 02:43.360
Actually, often you will see the same method that

02:43.360 --> 02:45.440
comes out of both approaches.

02:45.440 --> 02:46.880
So let's see how we did it.

02:46.880 --> 02:48.960
The first thing, we had data.

02:48.960 --> 02:50.720
We observed some data.

02:50.720 --> 02:53.000
And we assumed that this data was generated randomly.

02:53.000 --> 02:54.840
The reason we did that is that because this

02:54.840 --> 02:57.840
would allow us to leverage tools from probability.

02:57.840 --> 03:01.000
So let's say by nature, measurements, you do a survey,

03:01.000 --> 03:03.080
you get some data.

03:03.080 --> 03:06.040
Then we made some assumptions on the data generating process.

03:06.040 --> 03:07.400
For example, we assumed there were

03:07.400 --> 03:09.480
IID that was one of the recurring things.

03:09.480 --> 03:11.640
Sometimes we assumed it was Gaussian

03:11.640 --> 03:13.440
if we wanted to use, say, t-test.

03:13.440 --> 03:15.320
Maybe we did some non-parametric statistics.

03:15.320 --> 03:17.680
So we assumed it was a smooth function

03:17.680 --> 03:20.360
or maybe linear regression function.

03:20.400 --> 03:21.560
So those are our modeling.

03:21.560 --> 03:24.160
And this was basically a way to say,

03:24.160 --> 03:27.960
well, we're not going to allow for any distributions

03:27.960 --> 03:31.680
for the data that we have, but maybe a small set of distribution

03:31.680 --> 03:34.800
that index by some small parameters, for example.

03:34.800 --> 03:38.440
Or at least remove some of the possibilities.

03:38.440 --> 03:40.800
Otherwise, there's nothing we could learn.

03:40.800 --> 03:45.280
And so, for example, this was associated

03:45.280 --> 03:47.000
to some parameter of interest, say,

03:47.000 --> 03:51.080
data or beta in the regression model.

03:51.080 --> 03:53.840
All right, then we had this unknown problem

03:53.840 --> 03:57.600
and this unknown parameter, and we wanted to find it.

03:57.600 --> 03:59.600
We wanted to either estimate it or test it

03:59.600 --> 04:02.440
or maybe find a confidence interval for this object.

04:02.440 --> 04:06.040
So far, I should not have said anything that's new.

04:06.040 --> 04:08.200
But this last sentence is actually

04:08.200 --> 04:10.640
what's going to be different from the Bayesian part.

04:10.640 --> 04:13.000
In particular, this unknown but fixed thing

04:13.000 --> 04:16.120
is what's going to be changing.

04:16.120 --> 04:18.760
So in the Bayesian approach, we still

04:18.760 --> 04:22.000
assume that we observe some random data.

04:22.000 --> 04:24.120
But the generating process is slightly different.

04:24.120 --> 04:25.720
It's sort of a two-layer process.

04:25.720 --> 04:27.920
And there's one process that generates the parameter,

04:27.920 --> 04:29.720
and then one process that, given this parameter,

04:29.720 --> 04:31.400
generates the data.

04:31.400 --> 04:35.240
So what the first layer does, I mean,

04:35.240 --> 04:36.840
nobody really believes that there's

04:36.840 --> 04:40.520
some random process that's happening about generating

04:40.520 --> 04:44.920
what is going to be the true expected number of people who

04:44.920 --> 04:45.960
turn their head to the right.

04:45.960 --> 04:47.920
When they kiss, but this is actually

04:47.920 --> 04:52.680
going to be something that brings us some easiness for us

04:52.680 --> 04:56.120
to incorporate what we call prior belief.

04:56.120 --> 04:58.680
So we'll see an example in a second.

04:58.680 --> 05:01.480
But often, you actually have prior belief

05:01.480 --> 05:03.000
of what this parameter should be.

05:03.000 --> 05:05.120
When we did, let's say, these squares,

05:05.120 --> 05:09.360
we looked over all of the vectors in all of r to the p,

05:09.360 --> 05:14.120
including the ones that have coefficients equal to $50 million.

05:14.120 --> 05:17.720
And so those are things that maybe we might be able to roll out.

05:17.720 --> 05:19.320
And maybe we might be able to roll out

05:19.320 --> 05:21.840
at a much smaller scale.

05:21.840 --> 05:23.560
For example, well, I mean, I don't know.

05:23.560 --> 05:29.200
I'm not an expert on turning your head to the right or to the left.

05:29.200 --> 05:30.960
But maybe you can roll out the fact

05:30.960 --> 05:33.240
that almost everybody is turning their head

05:33.240 --> 05:35.360
in the same direction, or almost everybody's

05:35.360 --> 05:38.040
turning their head to another direction.

05:38.040 --> 05:39.880
So we have this prior belief.

05:39.880 --> 05:43.200
And this prior belief is going to play, say,

05:43.200 --> 05:45.160
hopefully, less and less important role

05:45.160 --> 05:47.560
as we collect more and more data.

05:47.560 --> 05:49.160
But if we have a smaller amount of data,

05:49.160 --> 05:52.760
we might want to be able to use this information rather

05:52.760 --> 05:54.720
than just shooting in the dark.

05:54.720 --> 05:58.160
And so the idea is to have this prior belief.

05:58.160 --> 06:00.480
And then we want to update this prior belief

06:00.480 --> 06:03.040
into what's called a posterior belief

06:03.040 --> 06:04.920
after we've seen some data.

06:04.920 --> 06:08.240
Maybe I believe that there's something that

06:08.240 --> 06:09.680
should be in some range.

06:09.680 --> 06:11.400
But maybe after I see data, maybe it's

06:11.400 --> 06:12.640
comforting me in my belief.

06:12.640 --> 06:15.280
So I'm actually having maybe a belief that's more.

06:15.280 --> 06:18.440
So a belief encompasses basically what you think

06:18.440 --> 06:19.920
and how strongly you think about it.

06:19.920 --> 06:21.400
That's what I call belief.

06:21.400 --> 06:24.080
So for example, if I have a belief about some parameter

06:24.080 --> 06:27.440
theta, maybe my belief is telling me where theta should be

06:27.440 --> 06:29.960
and how strongly I believe in it in the sense

06:29.960 --> 06:34.640
that I have a very narrow region where theta could be.

06:34.640 --> 06:37.600
And so the posterior belief says, well, you see some data.

06:37.600 --> 06:40.000
And maybe you're more confident or less confident about what

06:40.000 --> 06:40.500
you've seen.

06:40.500 --> 06:42.780
You've shifted your belief a little bit.

06:42.780 --> 06:44.620
And so that's what we're going to try to see

06:44.620 --> 06:47.940
and how to do this in a principal manner.

06:47.940 --> 06:50.020
So of course, to understand this better,

06:50.020 --> 06:52.140
there's nothing better than an example.

06:52.140 --> 06:56.220
So let's talk about another stupid statistical question,

06:56.220 --> 06:58.620
which is let's try to understand p.

06:58.620 --> 07:01.300
Of course, I'm not going to talk about politics from now on.

07:01.300 --> 07:03.940
So let's talk about p, the proportion of women

07:03.940 --> 07:04.940
in the population.

07:11.100 --> 07:11.700
OK?

07:11.700 --> 07:18.660
And so what I could do is to collect some data, x1, xn,

07:18.660 --> 07:24.660
and assume that they're Bernoulli with some parameter p

07:24.660 --> 07:25.160
unknown.

07:25.160 --> 07:25.660
Right?

07:25.660 --> 07:27.660
So p is in 0, 1.

07:30.060 --> 07:30.560
OK?

07:30.560 --> 07:32.780
Let's assume that those guys are i, d.

07:32.780 --> 07:37.700
So this is just an indicator for each of my collected data,

07:38.220 --> 07:41.540
whether the person I randomly sample is a woman,

07:41.540 --> 07:44.060
I get a 1, and if it's a man, I get a 0.

07:44.060 --> 07:44.420
OK?

07:44.420 --> 07:49.740
And so now the question is, I sample these people randomly.

07:49.740 --> 07:51.580
I denote their gender.

07:51.580 --> 07:54.780
And the frequentist approach was just saying, OK,

07:54.780 --> 07:58.260
let's just estimate p hat being xn bar.

07:58.260 --> 08:01.020
And then we could do some tests.

08:01.020 --> 08:01.200
Right?

08:01.200 --> 08:02.260
So here there's a test.

08:02.260 --> 08:05.340
I want to test maybe if p is equal to 0.5 or not.

08:05.340 --> 08:09.860
That sounds like a pretty reasonable thing to test.

08:09.860 --> 08:13.140
But we want to also maybe estimate p.

08:13.140 --> 08:15.140
But here this is a case where we definitely

08:15.140 --> 08:17.140
have for our belief of what p should be.

08:17.140 --> 08:17.700
Right?

08:17.700 --> 08:22.060
We are pretty confident that p is not going to be 0.7.

08:22.060 --> 08:26.700
We actually believe that p should be extremely close to 1.5.

08:26.700 --> 08:26.940
OK?

08:26.940 --> 08:28.460
But maybe not exactly.

08:28.460 --> 08:29.340
Maybe I don't know.

08:29.340 --> 08:32.700
Maybe this population is not the population in the world,

08:32.700 --> 08:35.660
but maybe this is the population of, say, some college.

08:35.660 --> 08:39.300
And we want to understand if this college has half women or not.

08:39.300 --> 08:39.580
Right?

08:39.580 --> 08:42.100
So maybe we know it's going to be close to 1.5,

08:42.100 --> 08:44.700
but maybe we're not quite sure.

08:44.700 --> 08:49.260
And so we're going to want to integrate that knowledge.

08:49.260 --> 08:49.980
All right?

08:49.980 --> 08:51.980
So I could integrate it in a blunt manner

08:51.980 --> 08:55.420
by saying discard the data and say that p is equal to 1.5.

08:55.420 --> 08:57.580
But maybe that's just a little too much.

08:57.580 --> 09:01.380
So how do I do this trade-off between adding the data

09:01.380 --> 09:05.060
and combining it with this prior knowledge?

09:05.060 --> 09:09.100
In many ways, in many instances, essentially what's

09:09.100 --> 09:11.020
going to happen is this 1.5 is going

09:11.020 --> 09:14.260
to act like one new observation, essentially.

09:14.260 --> 09:17.020
So if you have five observations,

09:17.020 --> 09:20.140
this is just a six observation, which will play a role.

09:20.140 --> 09:21.820
If you have a million observations,

09:21.820 --> 09:22.860
you're going to have a million in one,

09:22.860 --> 09:24.660
and it's not going to play so much a role.

09:24.660 --> 09:26.700
That's basically how it goes.

09:26.700 --> 09:28.620
That's basically how it goes.

09:28.660 --> 09:33.580
But definitely not always, because we'll

09:33.580 --> 09:36.980
see that if I take my prior to be a point minus 1.5 here,

09:36.980 --> 09:39.140
it's basically as if I was discarding my data.

09:39.140 --> 09:41.740
So essentially, there's also your ability

09:41.740 --> 09:45.460
to encompass how strongly you believe in this prior.

09:45.460 --> 09:47.820
And if you believe infinitely more in the prior

09:47.820 --> 09:49.620
than you believe in the data you collected,

09:49.620 --> 09:51.900
then, of course, it's not going to act like one more

09:51.900 --> 09:53.340
observation.

09:53.340 --> 09:55.540
All right, so the Bayesian approach

09:55.540 --> 09:59.020
is a tool to, one, include mathematically our prior,

09:59.020 --> 10:02.060
and our prior belief into statistical procedures.

10:02.060 --> 10:03.940
So maybe I have this prior knowledge,

10:03.940 --> 10:06.100
but if I'm a medical doctor, it's not clear to me

10:06.100 --> 10:09.900
how I'm going to turn this into some principle way of building

10:09.900 --> 10:10.420
estimators.

10:10.420 --> 10:12.140
And of course, the second goal is

10:12.140 --> 10:13.860
going to be to update this prior belief

10:13.860 --> 10:19.780
into posterior belief by using the data, all right?

10:19.780 --> 10:23.940
So how do I do this?

10:23.940 --> 10:25.500
And at some point, I sort of suggested

10:25.500 --> 10:28.660
that there's two layers.

10:28.660 --> 10:31.700
One is where you draw the parameter at random.

10:31.700 --> 10:35.780
And two, once you have the parameter, condition

10:35.780 --> 10:39.340
this parameter, you draw your data.

10:39.340 --> 10:41.100
Nobody believes this actually is happening,

10:41.100 --> 10:43.620
that nature is just rolling dice for us

10:43.620 --> 10:45.500
and choosing parameters at random.

10:45.500 --> 10:48.260
But what's happening is that this idea

10:48.260 --> 10:51.420
that the parameter comes from some random distribution

10:51.420 --> 10:55.460
actually captures very well this idea that how you would

10:55.460 --> 10:56.980
encompass your prior, right?

10:56.980 --> 10:59.100
How would you say my belief is as follows?

10:59.100 --> 11:01.900
Well, here's an example about p.

11:01.900 --> 11:08.180
I'm 90% sure that p is between 0.4 and 0.6.

11:08.180 --> 11:14.060
And I'm 95% sure that p is between 0.3 and 0.8.

11:14.060 --> 11:18.500
OK, so essentially, I have this possible value of p.

11:18.500 --> 11:28.540
And what I know is that there's 90% here between, what did I

11:28.540 --> 11:32.420
say, 0.4 and 0.6.

11:35.460 --> 11:39.260
And then I have 0.3 and 0.8.

11:39.260 --> 11:43.860
And I know that I'm 95% sure that I'm in here.

11:43.860 --> 11:45.340
And this, if you remember, this sort of

11:45.340 --> 11:47.340
looks like the kind of pictures that I

11:47.340 --> 11:50.140
made when I had some Gaussian, right, for example.

11:50.140 --> 11:54.220
And I said, oh, here we have 90% of the observations.

11:54.220 --> 11:56.660
And here we have 95% of the observations.

12:00.460 --> 12:04.620
So in a way, if I were able to tell you

12:04.620 --> 12:07.540
all those ranges for all possible values,

12:07.540 --> 12:10.580
then I would essentially describe a probability

12:10.580 --> 12:13.420
distribution for p.

12:13.420 --> 12:14.780
And what I'm essentially saying is

12:14.780 --> 12:16.620
that p is going to have this kind of shape.

12:16.620 --> 12:19.060
So of course, if I tell you only twice this information

12:19.060 --> 12:22.300
that there's 90% I'm here and I'm here between here and here

12:22.300 --> 12:24.620
and 95% I'm between here and here,

12:24.620 --> 12:27.220
then there's many ways I can accomplish that, right?

12:27.220 --> 12:31.660
I could have something that looks like this, maybe, right?

12:31.660 --> 12:35.580
I could be really, I mean, it could be like this.

12:35.580 --> 12:37.820
I mean, there's many ways I can have this.

12:37.820 --> 12:39.900
Some of them are definitely going to be mathematically

12:39.900 --> 12:42.260
more convenient than others.

12:42.260 --> 12:44.340
And hopefully, we're going to have things

12:44.340 --> 12:47.260
that I can parameterize very well.

12:47.260 --> 12:49.900
Because if I tell you this is this guy,

12:49.900 --> 12:56.780
then there's basically 1, 2, 3, 4, 5, 6, 7 parameters.

12:56.780 --> 12:59.020
So I probably don't want something that has 7 parameters,

12:59.020 --> 13:01.180
but maybe I can say, oh, it's a Gaussian.

13:01.180 --> 13:02.820
And all I have to do is to tell you

13:02.820 --> 13:07.060
where it's centered and what the standard deviation is.

13:07.060 --> 13:11.060
OK, so the idea of using this two-layer thing

13:11.060 --> 13:13.540
where we think of the parameter p as being drawn

13:13.540 --> 13:15.940
from some distribution is really just a way for us

13:15.940 --> 13:19.260
to capture this information, our prior belief being,

13:19.260 --> 13:22.820
well, there's this percentage of chances that it's there.

13:22.820 --> 13:25.540
But the person who has a chance, I'm deliberately not

13:25.540 --> 13:26.940
using probability here.

13:26.940 --> 13:28.780
It's really, right?

13:28.780 --> 13:32.140
So it's really a way to get close to this.

13:32.140 --> 13:34.340
All right, so that's what I said.

13:34.340 --> 13:36.180
The true parameter is not random,

13:36.180 --> 13:40.420
but the Bayesian approach does as if it was random

13:40.420 --> 13:44.500
and then just spits out a procedure out of this thought

13:44.500 --> 13:49.060
process, this thought experiment.

13:49.060 --> 13:54.100
So when you practice Bayesian statistics a lot,

13:54.100 --> 13:57.100
you start getting automatisms.

13:57.100 --> 14:00.580
So you start getting some things that you do

14:00.580 --> 14:03.140
without really thinking about it, just like when you're

14:03.140 --> 14:04.740
a statistician, the first thing you do

14:04.740 --> 14:07.820
is can I think of this data as being Gaussian, for example?

14:07.820 --> 14:10.340
When you're Bayesian, you're thinking about, OK,

14:10.340 --> 14:11.620
I have a set of parameters, right?

14:11.620 --> 14:15.060
So here, I can describe my parameter as being theta

14:15.060 --> 14:21.540
in general in some big space parameter theta.

14:21.540 --> 14:24.900
But what spaces did we encounter?

14:24.900 --> 14:27.100
Well, we encountered the real line.

14:27.100 --> 14:30.980
We encountered the interval 0, 1 for Bernoulli's.

14:30.980 --> 14:36.340
And we encountered maybe some deposit of real line

14:36.340 --> 14:39.340
for exponential distributions, et cetera.

14:39.340 --> 14:42.060
And so what I'm going to need to do, if I want to model some,

14:42.060 --> 14:44.580
if I want to put some prior on those spaces,

14:44.580 --> 14:48.340
I'm going to have to have a usual set of tools for this guy,

14:48.340 --> 14:50.340
usual set of tools for this guy, usual set of tools

14:50.340 --> 14:51.220
for this guy.

14:51.220 --> 14:52.540
And by usual set of tools, I mean,

14:52.540 --> 14:54.660
I'm going to have to have a family of distributions

14:54.660 --> 14:56.820
that's supported on this.

14:56.820 --> 15:00.700
So in particular, this is the speech in which my parameter

15:00.700 --> 15:03.900
that I usually denote by p for Bernoulli lives.

15:03.900 --> 15:06.780
And so what I need is to find a distribution

15:06.780 --> 15:13.540
on the interval 0, 1, just like this guy.

15:13.540 --> 15:15.660
The problem with the Gaussian is that it's not

15:15.660 --> 15:17.940
on the interval 0, 1.

15:17.940 --> 15:20.260
It's going to spill out in the end.

15:20.260 --> 15:22.780
And it's not going to be something that works for me.

15:22.780 --> 15:24.780
And so the question is, I need to think

15:24.780 --> 15:27.060
about distributions that are probably continuous.

15:27.060 --> 15:30.100
Why would I restrict myself to discrete distributions that

15:30.100 --> 15:31.500
are actually convenient?

15:31.500 --> 15:34.060
And for Bernoulli, one that's actually

15:34.060 --> 15:36.740
basically the main tool that everybody's using

15:36.740 --> 15:39.700
is this so-called beta distribution.

15:39.700 --> 15:42.220
So the beta distribution has two parameters.

15:50.620 --> 15:59.100
So x follows a beta with parameters, say a and b,

15:59.100 --> 16:09.060
if it has a density, f of x is equal to x to the a minus 1,

16:09.060 --> 16:18.060
1 minus x to the b minus 1, if x is in the interval 0, 1,

16:18.060 --> 16:20.300
and 0 for all other x's.

16:21.260 --> 16:28.220
So why is that a good thing?

16:28.220 --> 16:31.860
Well, it's a density that's on the interval 0, 1, for sure.

16:31.860 --> 16:33.740
But now I have these two parameters.

16:33.740 --> 16:36.060
And the set of shapes that I can get

16:36.060 --> 16:40.300
by tweaking those two parameters is incredible.

16:40.300 --> 16:44.340
I mean, it's going to be a unimodal distribution.

16:44.340 --> 16:45.700
It's still fairly nice.

16:45.700 --> 16:47.740
It's not going to be something that goes like this and this,

16:47.740 --> 16:54.060
because if you think about this, what would it mean if your prior

16:54.060 --> 16:59.620
distribution on the interval 0, 1 had this shape?

16:59.620 --> 17:01.940
It would mean that maybe you think that p is here,

17:01.940 --> 17:03.340
or maybe you think that p is here,

17:03.340 --> 17:05.300
or maybe you think that p is here,

17:05.300 --> 17:08.780
which essentially mean that you think that p can come maybe

17:08.780 --> 17:10.900
from three different phenomena.

17:10.900 --> 17:13.260
And there's other models that are called mixtures for that

17:13.260 --> 17:15.100
that directly account for the fact

17:15.140 --> 17:21.100
that maybe there are several phenomena that are aggregated in your data set.

17:21.100 --> 17:23.460
But if you think that your data set is sort of pure

17:23.460 --> 17:25.700
and that everything comes from the same phenomenon,

17:25.700 --> 17:28.900
you want something that looks like maybe like this,

17:28.900 --> 17:32.900
or maybe looks like this, or maybe is sort of symmetric.

17:32.900 --> 17:34.460
You want to get all this stuff, right?

17:34.460 --> 17:38.580
Maybe you want something that says, well, if I'm talking about p

17:38.580 --> 17:44.860
being the probability of the proportion of women in the whole world,

17:44.860 --> 17:48.620
you want something that's probably really spiked around 1-1.5,

17:48.620 --> 17:50.500
almost the point mass, because you know,

17:50.500 --> 17:55.020
I mean, OK, let's agree that 0.5 is the actual number.

17:55.020 --> 17:58.980
So you want something maybe that says, OK, maybe I'm wrong,

17:58.980 --> 18:01.140
but I'm sure I'm not going to be really that way off.

18:01.140 --> 18:03.340
And so you want something that's really pointy.

18:03.340 --> 18:06.700
But if it's something you've never checked, right?

18:06.700 --> 18:09.820
And again, I cannot make references at this point,

18:09.820 --> 18:12.820
but something where you might have some uncertainty,

18:12.820 --> 18:14.300
then that should be around 1-1.5.

18:14.300 --> 18:16.700
Maybe you want something that's like a little more,

18:16.700 --> 18:19.420
allows you to say, well, I think there's more around 1-1.5,

18:19.420 --> 18:22.980
but there's still some fluctuations that are possible, OK?

18:22.980 --> 18:25.180
And in particular here, I talk about p

18:25.180 --> 18:29.340
where the two parameters, a and b, are actually the same.

18:29.340 --> 18:30.900
I call them a.

18:30.900 --> 18:33.540
One is called scale, the other one's called shape.

18:33.540 --> 18:35.420
Oh, by the way, sorry, this is not a density,

18:35.420 --> 18:38.780
so it actually has to be normalized, right?

18:38.780 --> 18:40.180
When you integrate this guy, it's going

18:40.180 --> 18:42.460
to be some function that depends on a and b, actually.

18:42.460 --> 18:45.460
Depends on this function through the beta function, right?

18:45.460 --> 18:47.220
Which is this combination of gamma function.

18:47.220 --> 18:49.900
So that's why it's called beta distribution.

18:49.900 --> 18:53.500
But well, that's the definition of the beta function

18:53.500 --> 18:55.180
when you integrate this thing anyway.

18:55.180 --> 18:57.020
So I mean, you just have to normalize it.

18:57.020 --> 18:59.740
It's just a number that depends on the nb, OK?

18:59.740 --> 19:01.780
So here, if you take a equal to b,

19:01.780 --> 19:03.100
you have something that essentially is

19:03.100 --> 19:05.340
symmetric around 1-1.5, right?

19:05.340 --> 19:07.060
Because what does it look like?

19:07.060 --> 19:08.020
Well, it's something.

19:08.020 --> 19:11.020
So my density f of x is going to be what?

19:11.020 --> 19:19.220
It's going to be my constant times x times 1 minus x

19:19.220 --> 19:21.660
to the a minus 1, right?

19:21.660 --> 19:26.100
And this function, x times 1 minus x looks like this.

19:26.100 --> 19:27.740
We've drawn it before, right?

19:27.740 --> 19:29.420
That was something that showed up

19:29.420 --> 19:36.500
as being the variance of my Bernoulli.

19:36.500 --> 19:40.980
So we know it's something that takes its maximum at 1-1.5,

19:41.980 --> 19:44.220
and now I'm just taking the power of this guy.

19:44.220 --> 19:46.020
So I'm really just distorting this thing

19:46.020 --> 19:53.340
into some fairly symmetric manner, OK?

19:53.340 --> 20:00.340
So this distribution that we actually take for p, right?

20:00.340 --> 20:02.540
So here, I assume that p, the parameter, right?

20:02.540 --> 20:04.260
I mean, notice that this is kind of weird.

20:04.260 --> 20:06.060
First of all, this is probably the first time

20:06.060 --> 20:09.580
in this entire course that we have this has this something

20:09.580 --> 20:12.340
has a distribution when it's actually a lower case letter.

20:12.340 --> 20:13.740
That's something you have to deal with,

20:13.740 --> 20:16.940
because we've been using lower case letters for parameters,

20:16.940 --> 20:18.700
and now we want them to have a distribution.

20:18.700 --> 20:20.460
So that's what's going to happen, all right?

20:20.460 --> 20:23.860
And this is called the prior distribution, OK?

20:23.860 --> 20:25.300
So really, I should write something

20:25.300 --> 20:34.060
like f of p is equal to a constant times p 1 minus p

20:34.060 --> 20:35.220
to the a minus 1.

20:35.220 --> 20:38.100
Well, no, actually, I should not, because then it's confusing.

20:38.100 --> 20:39.980
OK, so let me not do this.

20:39.980 --> 20:42.140
One thing in terms of notation that I'm going to write,

20:42.140 --> 20:43.940
I'm going to write, when I have a constant here,

20:43.940 --> 20:45.340
and I don't want to make it explicit,

20:45.340 --> 20:48.500
and we'll see in a second why I don't need to make it explicit,

20:48.500 --> 20:59.300
I'm going to write this as f of x is proportional to x 1

20:59.300 --> 21:04.060
minus x to the a minus 1, OK?

21:04.060 --> 21:08.780
So that's just to say equal to some constant that does not

21:08.780 --> 21:11.540
depend on x times this thing, OK?

21:16.340 --> 21:20.660
So if we continue with our experiment, now if p,

21:20.660 --> 21:22.900
right, so that's the experiment where I'm trying to,

21:22.900 --> 21:26.380
I'm drawing this data x 1 to x n, which is Bernoulli p,

21:26.380 --> 21:29.900
if p has some distribution, it's not clear

21:29.900 --> 21:32.700
what it means to have a Bernoulli with some random parameter.

21:32.700 --> 21:35.020
So what I'm going to do is then I'm going to first draw my p.

21:35.020 --> 21:38.700
Let's say I get a number 0.52, and then I'm

21:38.700 --> 21:41.140
going to draw my data conditionally on p, all right?

21:41.140 --> 21:45.820
So here comes the first and last flowchart of this class.

21:45.820 --> 21:49.540
So I'm going to first, all right?

21:49.540 --> 21:52.820
So nature first draws p, OK?

21:52.820 --> 21:58.420
So p follows, say, some beta AA.

21:58.420 --> 22:06.900
Then I condition on p, and then I draw x 1, x n, that are i,

22:06.900 --> 22:10.780
i, d, Bernoulli p.

22:10.780 --> 22:14.380
Everybody understand the process of generating this data, right?

22:14.380 --> 22:16.260
So you first draw a parameter, and then you just

22:16.260 --> 22:21.180
flip those independent bias coins with this particular p.

22:21.180 --> 22:25.260
So there's this layered thing.

22:25.260 --> 22:28.660
So now, conditionally on p, right?

22:28.660 --> 22:31.980
So here, I have this prior about p, which was the thing.

22:31.980 --> 22:34.140
So this is just the thought process again, right?

22:34.140 --> 22:36.420
It's not anything that actually happens in practice.

22:36.420 --> 22:39.940
This is my way of thinking about how the data was generated.

22:39.940 --> 22:43.300
And from this, I'm going to try to come up with some procedure.

22:43.300 --> 22:47.820
Just like if your estimator is the average of the data,

22:47.820 --> 22:49.740
you don't have to understand probability

22:49.740 --> 22:52.380
to say that my estimator is the average of the data, right?

22:52.380 --> 22:54.220
I mean, anyone outside this room understand

22:54.260 --> 22:58.540
that the average is a good estimator for some average behavior,

22:58.540 --> 23:01.540
and they don't need to think of the data as being

23:01.540 --> 23:03.020
a random variable, et cetera.

23:03.020 --> 23:05.060
So same thing, basically.

23:05.060 --> 23:06.460
Now, we will see.

23:06.460 --> 23:08.060
I mean, actually, we won't.

23:08.060 --> 23:10.780
But in this case, well, we will.

23:10.780 --> 23:13.100
In this case, you can see that, essentially, the posterior

23:13.100 --> 23:16.700
distribution is still a beta, all right?

23:16.700 --> 23:20.100
So what it means is that I had this thing,

23:20.100 --> 23:23.140
then I observed my data, and then I continue.

23:23.140 --> 23:31.820
And here, I'm going to update my prior

23:31.820 --> 23:36.700
into some posterior distribution, pi.

23:36.700 --> 23:40.900
And here, this guy is actually also a beta, all right?

23:40.900 --> 23:45.460
So pi now, P, my posterior distribution on P,

23:45.460 --> 23:48.260
is also a beta distribution with the parameters that

23:48.260 --> 23:51.700
are on this slide, and I'll have space to reproduce them.

23:51.700 --> 23:54.500
So I start the beginning of this flow chart

23:54.500 --> 23:57.140
as having P, which is a prior.

23:57.140 --> 23:58.820
I'm going to get some observations,

23:58.820 --> 24:03.900
and then I'm going to update what my posterior is, OK?

24:03.900 --> 24:06.940
So this posterior is basically something

24:06.940 --> 24:10.100
that's in Bayesian statistics was beautiful,

24:10.100 --> 24:13.780
is as soon as you have the distribution,

24:13.780 --> 24:17.060
it's essentially capturing all the information about the data

24:17.060 --> 24:20.380
that you want for P. And it's not just a point, right?

24:20.380 --> 24:21.500
It's not just an average.

24:21.500 --> 24:23.660
It's actually an entire distribution

24:23.660 --> 24:27.060
for the possible values of theta.

24:27.060 --> 24:30.740
And it's not the same thing as saying, well, you know,

24:30.740 --> 24:34.820
if theta hat is equal to x and bar in the Gaussian case,

24:34.820 --> 24:37.180
I know that this is some mean mu,

24:37.180 --> 24:39.700
and then maybe it has variance sigma square over n.

24:39.700 --> 24:42.940
That's not what I mean by this is my posterior distribution,

24:42.940 --> 24:43.580
right?

24:43.580 --> 24:46.660
This is not what I mean.

24:46.660 --> 24:48.700
This is going to come from this guy, right?

24:48.700 --> 24:51.340
The Gaussian thing and the central limit theorem.

24:51.340 --> 24:52.980
But what I mean is this guy.

24:52.980 --> 24:58.180
And this came exclusively from the prior distribution.

24:58.180 --> 25:00.860
If I had another prior, I would not necessarily

25:00.860 --> 25:03.740
have a beta distribution on the output.

25:03.740 --> 25:07.580
So when I have the same family of distributions

25:07.580 --> 25:11.100
at the beginning and at the end of this flow chart,

25:11.100 --> 25:21.220
I say that beta is a conjugate prior,

25:21.220 --> 25:23.980
meaning I put in beta as a prior,

25:23.980 --> 25:27.420
and I get betas at posterior.

25:27.420 --> 25:30.900
And that's why betas are so popular.

25:30.900 --> 25:32.300
Conjugate priors are really nice,

25:32.300 --> 25:35.340
because you know that whatever you put in,

25:35.340 --> 25:37.220
what you're going to get in the end is a beta.

25:37.220 --> 25:38.820
So all you have to think about is the parameters.

25:38.820 --> 25:41.060
You don't have to check again what the posterior is

25:41.060 --> 25:43.300
going to look like, what the PDF of this guy is going to be.

25:43.300 --> 25:44.500
You don't have to think about it.

25:44.500 --> 25:46.660
You just have to check what the parameters are.

25:46.660 --> 25:48.300
And there's families of conjugate priors.

25:48.300 --> 25:51.180
Gaussian gives Gaussian, for example.

25:51.180 --> 25:52.220
There's a bunch of them.

25:52.220 --> 25:56.420
And this is what drives people into using specific priors

25:56.420 --> 25:58.260
as opposed to other.

25:58.260 --> 26:00.700
It has nice mathematical properties.

26:00.700 --> 26:04.180
Nobody believes that the P distribution is really

26:04.180 --> 26:07.460
distributed according to beta, but it's flexible enough

26:07.460 --> 26:10.540
and super convenient mathematically.

26:10.540 --> 26:13.900
All right, so now let's see for one second

26:13.900 --> 26:15.780
before we actually go any further.

26:15.780 --> 26:17.860
What I did, so A and B, I didn't mention it.

26:18.740 --> 26:24.140
And here, A and B are positive numbers.

26:24.140 --> 26:27.540
OK, they can be anything positive.

26:27.540 --> 26:31.980
So here what I did is that I updated A into A plus the sum

26:31.980 --> 26:38.420
of my data, and B into B plus and minus the sum of my data.

26:38.420 --> 26:42.220
So that's essentially A becomes A plus the number of ones,

26:42.220 --> 26:47.380
and B becomes B. Well, that's only when I have A and A, right?

26:47.420 --> 26:50.220
So the first parameters become itself plus the number of ones,

26:50.220 --> 26:52.660
and the second one becomes itself plus the number of zeros.

26:55.460 --> 26:59.180
And so just as a sanity check, what does this mean?

26:59.180 --> 27:08.980
If A goes to 0, what is the beta when A goes to 0?

27:08.980 --> 27:11.700
We can actually read this from here, right?

27:11.700 --> 27:26.140
So we had A, sorry, actually, let's take A goes to, no, actually,

27:26.140 --> 27:27.340
sorry, let's just do this.

27:37.540 --> 27:38.700
OK, let's not do this now.

27:38.700 --> 27:40.860
I'll do it when we talk about non-informative prior,

27:40.900 --> 27:44.220
because it's a little too messy here.

27:44.220 --> 27:47.980
OK, so how do we do this?

27:47.980 --> 27:51.380
How did I get this posterior distribution given the prior?

27:51.380 --> 27:53.100
How do I update this?

27:53.100 --> 27:56.060
Well, this is called Bayesian statistics,

27:56.060 --> 27:58.820
and you've heard this word Bayes before,

27:58.820 --> 28:02.020
and the way you've heard it is in the Bayes formula, right?

28:02.020 --> 28:03.700
What was the Bayes formula?

28:03.700 --> 28:08.060
The Bayes formula was telling you that the probability of A given

28:08.060 --> 28:12.500
B was equal to something that depended on the probability

28:12.500 --> 28:13.620
of B given A, right?

28:13.620 --> 28:14.780
That's what it was.

28:14.780 --> 28:18.660
And I mean, you can actually either remember the formula,

28:18.660 --> 28:20.580
you can remember the definition, and this

28:20.580 --> 28:26.420
is what P of A and B divided by P of B. So this

28:26.420 --> 28:35.540
is P of B given A times P of A divided by P of B, right?

28:35.540 --> 28:40.100
That's what Bayes formula is telling you, agree?

28:40.100 --> 28:42.620
So now what I want is to have something

28:42.620 --> 28:48.140
that's telling me how this is going to work, OK?

28:48.140 --> 28:54.460
So what is going to play the role of those events, A and B?

28:54.460 --> 29:02.020
Well, one is going to be the distribution of my parameter theta

29:02.020 --> 29:04.500
given that I see the data, and this

29:04.500 --> 29:06.540
is going to tell me what is the distribution of the data

29:06.540 --> 29:09.260
given that I know what my parameter theta is.

29:09.260 --> 29:12.660
But that part, if this is data and this is the parameter theta,

29:12.660 --> 29:15.700
this is what we've been doing all along.

29:15.700 --> 29:18.700
The distribution of the data given the parameter here

29:18.700 --> 29:23.140
was n i i d Bernoulli P. I know that.

29:23.140 --> 29:27.940
I know exactly what their joint probability mass function is.

29:27.940 --> 29:29.300
Then that was what?

29:29.300 --> 29:32.700
So we said that this is going to be my data,

29:32.700 --> 29:37.260
and this is going to be my parameter, OK?

29:37.260 --> 29:40.180
So that means that this is the probability of my data

29:40.180 --> 29:40.940
given the parameter.

29:40.940 --> 29:42.980
This is the probability given the parameter.

29:42.980 --> 29:45.820
This is the probability of the parameter.

29:45.820 --> 29:46.300
What is this?

29:46.300 --> 29:49.340
What did we call this?

29:49.340 --> 29:50.260
This is the prior.

29:50.260 --> 29:53.580
It's just the distribution of my parameter.

29:53.580 --> 29:55.900
Now, what is this?

29:55.900 --> 29:58.860
Well, this is just the distribution of the data itself,

29:58.860 --> 29:59.500
all right?

29:59.540 --> 30:08.260
So this is essentially the distribution of this if this

30:08.260 --> 30:15.100
was indeed not conditioned on P, right?

30:15.100 --> 30:18.700
So if I don't condition on P, this data

30:18.700 --> 30:23.260
is going to be a bunch of i i d Bernoulli

30:23.260 --> 30:25.460
with some parameter, but the parameter is random, right?

30:25.460 --> 30:27.820
So for different realization of this data set,

30:27.940 --> 30:30.100
I'm going to get different parameters for the Bernoulli.

30:30.100 --> 30:34.540
And so that leads to some sort of convolution.

30:34.540 --> 30:36.140
I mean, it's not really a convolution in this case,

30:36.140 --> 30:38.660
but it's some sort of composition of distributions, right?

30:38.660 --> 30:41.260
I have the distribution, the randomness that comes from here,

30:41.260 --> 30:44.340
and then the randomness that comes from realizing the Bernoulli.

30:44.340 --> 30:46.260
So that's just the marginal distribution,

30:46.260 --> 30:48.700
and it actually might be painful to understand what this is,

30:48.700 --> 30:49.540
right?

30:49.540 --> 30:51.140
I mean, in a way, it's sort of a mixture,

30:51.140 --> 30:52.900
and it's not super nice.

30:52.900 --> 30:55.820
But we'll see that this actually won't matter for us.

30:55.820 --> 30:57.060
This is going to be some number.

30:57.060 --> 30:58.540
It's going to be there, but it won't

30:58.540 --> 31:00.940
matter for us what it is, because it actually

31:00.940 --> 31:02.460
does not depend on the parameter,

31:02.460 --> 31:06.220
and that's all that matters to us.

31:06.220 --> 31:10.900
OK, so let's put some names on those things, right?

31:10.900 --> 31:13.100
I mean, this was very informal, so let's

31:13.100 --> 31:19.660
put some actual names on what we want to call what we call prior.

31:19.660 --> 31:22.220
So what is the formal definition of a prior?

31:22.220 --> 31:24.860
What is the formal definition of a posterior?

31:24.980 --> 31:27.500
And what are the rules to update it, OK?

31:27.500 --> 31:30.380
So I'm going to have my data, which is going to be x1, xn.

31:33.620 --> 31:38.540
And so let's say they're IID, but they don't actually have to.

31:38.540 --> 31:41.340
And so I'm going to have given theta.

31:47.460 --> 31:49.340
And when I say given, it's either given

31:49.340 --> 31:51.900
like I did in the first part of this course

31:51.900 --> 31:54.420
in all previous chapters or conditionally on, right?

31:54.420 --> 31:57.540
So if you're thinking like a Bayesian,

31:57.540 --> 31:59.820
what I really mean is conditionally

31:59.820 --> 32:02.180
on this random parameter, OK?

32:02.180 --> 32:05.020
So it's like as if it was a fixed number.

32:05.020 --> 32:10.140
Then they're going to have the distribution, x1, xn,

32:10.140 --> 32:11.860
is going to have some distribution.

32:11.860 --> 32:19.180
Let's say, let's assume for now it's pdf, pn of x1, xn, OK?

32:19.180 --> 32:22.260
And I'm going to write theta like this.

32:22.260 --> 32:24.060
So for example, what is this?

32:24.580 --> 32:27.140
So let's say this is a pdf.

32:27.140 --> 32:28.140
It could be a pmf.

32:28.140 --> 32:31.260
Everything I say, I'm going to think of them as being pdfs.

32:31.260 --> 32:32.940
I'm going to combine pdfs with pdf,

32:32.940 --> 32:35.660
but I could combine pdf with pmfs, pmf with pdfs,

32:35.660 --> 32:37.460
or pmf with pms, OK?

32:37.460 --> 32:41.140
So everywhere you see a d, it could be an m.

32:41.140 --> 32:42.660
All right, so now I have those things.

32:42.660 --> 32:43.860
So what does that mean?

32:43.860 --> 32:53.900
So here's some example, x1, xn are IID and theta1, right?

32:53.900 --> 32:57.420
So now I know exactly what the joint pdf of this thing is.

32:57.420 --> 33:03.740
So it means that pn of x1, xn given theta is equal to what?

33:03.740 --> 33:09.380
Well, it's like 1 over sigma root, sorry, 1 root 2 pi

33:09.380 --> 33:13.820
to the power n e to the minus sum from i

33:13.820 --> 33:18.500
equal 1 to n of xi minus theta squared divided by 2, right?

33:18.500 --> 33:21.900
So that's just the joint distribution of n iID and theta

33:21.900 --> 33:25.060
1 random variables, OK?

33:25.060 --> 33:27.220
So that's my pn given theta.

33:27.220 --> 33:31.620
Now, this is what we denoted by sort of like f sub theta

33:31.620 --> 33:33.340
before, right?

33:33.340 --> 33:35.860
We had this subscript before, but now we just

33:35.860 --> 33:37.420
put a bar in theta because we want

33:37.420 --> 33:40.700
to remember that this is actually conditioned on theta, right?

33:40.700 --> 33:42.140
But this is just notation.

33:42.140 --> 33:46.060
You should just think of this as being just the usual thing

33:46.060 --> 33:50.740
that you get from some statistical model, all right?

33:50.740 --> 33:54.540
So now, that's going to be pn.

33:54.540 --> 34:00.140
And here, I'm going to assume that theta is,

34:00.140 --> 34:01.420
why do I put pi here?

34:08.780 --> 34:10.820
OK.

34:10.820 --> 34:19.540
So theta has prior distribution pi.

34:20.820 --> 34:28.980
OK, so for example, so think of it as either PDF or PMF again.

34:28.980 --> 34:33.860
So for example, pi of theta was what?

34:33.860 --> 34:40.260
Well, it was some constant times theta to the a minus 1,

34:40.260 --> 34:43.780
1 minus theta to the a minus 1, right?

34:43.780 --> 34:49.060
So it has some prior distribution, and that's another PMF.

34:49.100 --> 34:52.340
So now, I'm given the distribution of my x's given theta.

34:52.340 --> 34:53.940
I'm given the distribution of my theta,

34:53.940 --> 34:57.420
so I'm given this guy, right?

34:57.420 --> 35:00.140
That's this guy.

35:00.140 --> 35:05.420
I'm given that guy, which is my pi, right?

35:05.420 --> 35:11.740
So that's my pn of x1, xn given theta.

35:11.740 --> 35:14.020
That's my pi of theta.

35:14.020 --> 35:17.340
And then I have here just, this is what?

35:17.340 --> 35:27.260
Well, this is just the integral of pn x1, xn times pi of theta d

35:27.260 --> 35:28.340
theta, right?

35:28.340 --> 35:29.860
Overall possible sets of theta.

35:29.860 --> 35:33.420
That's just when I integrate out my theta,

35:33.420 --> 35:35.820
or I compute, say, the marginal distribution,

35:35.820 --> 35:37.340
I get this by integrating, right?

35:37.340 --> 35:40.500
That's just basic probability, conditional probabilities,

35:40.500 --> 35:41.000
right?

35:41.000 --> 35:42.620
Then if I had the PMF, I would just

35:42.620 --> 35:45.860
sum over the values of theta's, OK?

35:48.340 --> 35:55.220
So now, what I want is to find what's called,

35:55.220 --> 35:58.940
so that's the prior distribution.

35:58.940 --> 36:01.060
And I want to find the posterior distribution.

36:01.060 --> 36:18.820
So it's called, it's pi of theta given x1, xn.

36:21.540 --> 36:23.540
And so if I use Bayes' rule, I know

36:23.540 --> 36:34.620
that this is pn of x1, xn given theta times pi of theta.

36:34.620 --> 36:37.540
And then it's divided by the distribution

36:37.540 --> 36:41.100
of those guys, which I will write as integral over theta

36:41.100 --> 36:48.860
of pn x1, xn given theta times pi of theta d theta.

36:54.220 --> 36:57.180
Everybody's with me still?

36:57.180 --> 36:59.220
So if you're not comfortable with this,

36:59.220 --> 37:03.020
it means that you probably need to go read your couple pages

37:03.020 --> 37:05.820
on conditional densities and conditional PMFs

37:05.820 --> 37:07.420
from your probability class.

37:07.420 --> 37:08.980
There's really not much there.

37:08.980 --> 37:12.220
It's just a matter of being able to define those quantities.

37:12.220 --> 37:15.620
F density of x given y, this is just

37:15.620 --> 37:17.260
what's called a conditional density.

37:17.260 --> 37:18.820
You need to understand what this object is

37:18.820 --> 37:21.940
and how it relates to the joint distribution of x and y,

37:21.940 --> 37:24.820
or maybe the distribution of x or the distribution of y.

37:27.380 --> 37:28.900
But it's the same rules.

37:28.900 --> 37:31.420
I mean, one way to actually remember this

37:31.420 --> 37:33.700
is this is exactly the same rules as this.

37:33.700 --> 37:35.780
When you see a bar, it's the same thing

37:35.780 --> 37:37.700
as the probability of this and this guy.

37:37.700 --> 37:42.020
So for densities, it's just a comma divided by the second guy.

37:42.020 --> 37:45.260
The probability of the second guy, that's it.

37:45.260 --> 37:46.980
So if you remember this, you can just

37:46.980 --> 37:49.980
do some pattern matching and see what I just wrote here.

37:50.740 --> 37:51.740
OK?

37:51.740 --> 37:53.020
OK.

37:53.020 --> 37:56.940
So now I can compute every single one of these guys.

37:56.940 --> 38:03.980
This is something I get from my modeling.

38:03.980 --> 38:05.260
So I did not write this.

38:05.260 --> 38:09.060
It's not written in the slides.

38:09.060 --> 38:14.740
But I give a name to this guy that was my prior distribution.

38:14.740 --> 38:16.500
And that was my posterior distribution.

38:20.980 --> 38:26.980
In chapter 3, maybe, what did we call this guy?

38:31.980 --> 38:33.580
Well, the one that does not have a name.

38:33.580 --> 38:39.580
And that's in the box, this guy.

38:39.580 --> 38:40.300
How did we call it?

38:47.300 --> 38:49.220
It is the joint distribution of the x i's.

38:50.980 --> 38:54.780
And we give you the name.

38:54.780 --> 38:56.140
It's the likelihood, right?

38:56.140 --> 38:57.660
This is exactly the likelihood.

38:57.660 --> 38:59.140
This was the likelihood of theta.

39:03.940 --> 39:06.380
And this is something that's very important to remember.

39:06.380 --> 39:10.300
And that really reminds you that these things are really

39:10.300 --> 39:12.940
not that different, maximum likelihood estimation

39:12.940 --> 39:14.020
and Bayesian estimation.

39:14.020 --> 39:18.580
Because your posterior is really just your likelihood

39:18.580 --> 39:22.820
times something that's just putting some weights on the

39:22.820 --> 39:26.340
Thetas, depending on where you think theta should be.

39:26.340 --> 39:28.700
So if I had, say, a maximum likelihood estimator in my

39:28.700 --> 39:31.140
likelihood and theta looked like this.

39:31.140 --> 39:33.460
But my prior in theta looked like this.

39:33.460 --> 39:36.980
I said, oh, I really want Thetas that are like this.

39:36.980 --> 39:39.460
So what's going to happen is that I'm going to turn this

39:39.460 --> 39:41.380
into some posterior that looks like this.

39:44.420 --> 39:47.620
So I'm just really weighting this posterior.

39:47.620 --> 39:49.940
This is a constant that does not depend on theta, right?

39:49.940 --> 39:50.500
Agreed?

39:50.500 --> 39:53.460
I integrated over Thetas, so Thetas go on.

39:53.460 --> 39:56.140
So forget about this guy.

39:56.140 --> 39:59.420
I have basically that the posterior distribution up to

39:59.420 --> 40:01.860
scaling, because it has to be a probability density and not

40:01.860 --> 40:04.660
just any function that's positive, is the product of

40:04.660 --> 40:05.140
this guy.

40:05.140 --> 40:06.980
It's a weighted version of my likelihood.

40:06.980 --> 40:07.900
That's all it is.

40:07.900 --> 40:10.580
I'm just weighting the likelihood using my

40:10.580 --> 40:13.140
prior belief on theta.

40:13.140 --> 40:17.220
And so given this guy, a natural estimator, if you

40:17.220 --> 40:21.100
follow the maximum likelihood principle, would be the

40:21.100 --> 40:23.220
maximum of this posterior.

40:23.220 --> 40:24.620
Agreed?

40:24.620 --> 40:28.620
That would basically be doing exactly what a maximum

40:28.620 --> 40:31.660
likelihood estimation is telling you.

40:31.660 --> 40:33.540
So it turns out that you can.

40:33.540 --> 40:35.340
It's called maximum a posteriori.

40:35.340 --> 40:39.300
And I won't talk much about this or map.

40:39.300 --> 40:44.500
So that's maximum a posteriori.

40:44.500 --> 40:50.500
So it's just the theta hat is the arg max of pi theta given

40:50.500 --> 40:51.500
x1, xn.

40:55.180 --> 40:56.220
It sounds like it's OK.

40:56.220 --> 40:59.740
I give you a density and you say, OK, I have a density for

40:59.740 --> 41:00.980
all values of my parameters.

41:00.980 --> 41:03.340
You're asking me to summarize it into one number.

41:03.340 --> 41:06.580
I'm just going to take the most likely number of those guys.

41:06.580 --> 41:08.300
But you could summarize it otherwise.

41:08.300 --> 41:10.780
You could take the average, right?

41:10.780 --> 41:12.460
You could take the median.

41:12.460 --> 41:14.380
You could take a bunch of numbers.

41:14.380 --> 41:16.820
And the beauty of Bayesian statistics is that you don't

41:16.820 --> 41:19.180
have to take any number in particular.

41:19.180 --> 41:21.500
You have an entire posterior distribution.

41:21.500 --> 41:25.380
This is not only telling you where theta is, but it's

41:25.380 --> 41:29.980
actually telling you the difference if you actually

41:29.980 --> 41:33.100
give as something, it gives you the posterior, right?

41:33.100 --> 41:36.300
So now let's say the theta is a p between 0 and 1.

41:36.300 --> 41:40.380
If my posterior distribution looks like this, or if my

41:40.380 --> 41:44.020
posterior distribution looks like this, then those two

41:44.020 --> 41:47.660
guys have one the same mode, right?

41:47.660 --> 41:49.220
This is the same value.

41:49.220 --> 41:51.700
And they're symmetric, so they also have the same mean.

41:51.700 --> 41:53.580
So these two posterior distributions give me the

41:53.580 --> 41:55.540
same summary into one number.

41:55.540 --> 41:58.740
However, clearly one is much more confident than the other

41:58.740 --> 42:03.540
one, so I might as well just speed that as a solution.

42:03.540 --> 42:05.220
Some people can, you can do even better.

42:05.220 --> 42:09.620
People actually do things such as drawing a random number

42:09.620 --> 42:10.540
from this distribution.

42:10.540 --> 42:12.780
So this is my number.

42:12.780 --> 42:14.940
Well, that's kind of dangerous, but you can imagine you

42:14.940 --> 42:17.060
could do this, right?

42:17.060 --> 42:19.700
All right.

42:19.700 --> 42:22.140
So this is what works.

42:22.140 --> 42:23.580
That's what we went through.

42:23.580 --> 42:28.300
So here, as you notice, I don't care so much about this

42:28.300 --> 42:30.220
part here, right?

42:30.220 --> 42:31.700
Because it does not depend on theta.

42:31.700 --> 42:35.180
So I know that given the product of those two things,

42:35.180 --> 42:37.740
this thing is only the constant that I need to divide so

42:37.740 --> 42:40.140
that when I integrate this thing over theta, it

42:40.140 --> 42:41.460
integrates to one.

42:41.460 --> 42:44.780
Because this has to be a probability density on theta.

42:44.780 --> 42:48.020
So I can write this and just forget about that part, and

42:48.020 --> 42:51.940
that's what's right on the top of this slide.

42:51.940 --> 42:57.860
Just this notation, this sort of weird alpha or, I don't know,

42:57.860 --> 43:00.500
infinity sine crop to the right, whatever you want to call

43:00.500 --> 43:04.420
this, this thing is actually just really emphasizing the

43:04.420 --> 43:06.260
fact that I don't care.

43:06.260 --> 43:13.500
I write it because I can, and you know what it is, but you

43:13.500 --> 43:18.740
don't actually have to, well, in some instances, you have to

43:18.740 --> 43:20.540
compute the integral, in some instances, you don't have to

43:20.540 --> 43:23.300
compute the integral, and a lot of Bayesian computation is

43:23.300 --> 43:26.700
about saying, OK, it's actually really hard to compute this

43:26.700 --> 43:28.260
integral, so I'd rather not doing it.

43:28.260 --> 43:31.540
So let me try to find some methods that allow me to

43:31.540 --> 43:34.540
sample from the posterior distribution without having to

43:34.540 --> 43:37.660
compute this, and that's what's called Monte Carlo Markov

43:37.660 --> 43:40.420
chains, or MCMC, and that's exactly what they're doing.

43:40.420 --> 43:42.860
They're just using only ratios of things like that for

43:42.860 --> 43:46.140
different datas, and which means that if you take ratios, the

43:46.140 --> 43:48.500
normalizing constant is gone, and you don't need to find this

43:48.500 --> 43:50.780
integral.

43:50.780 --> 43:53.140
So we won't go into those details at all.

43:53.140 --> 43:56.140
That would be the purpose of an entire course on Bayesian

43:56.140 --> 43:56.580
inference.

43:56.580 --> 44:00.300
Actually, even Bayesian computations would be an

44:00.300 --> 44:02.660
entire course on its own.

44:02.660 --> 44:04.340
There's some very interesting things that are going on

44:04.340 --> 44:08.100
there, the interface of stats and computation.

44:08.100 --> 44:12.020
All right, so let's go back to our example and see if we

44:12.020 --> 44:13.900
can actually compute any of those things, because it's very

44:13.900 --> 44:17.780
nice to give you some data, some formulas, but let's see if

44:17.780 --> 44:19.820
we can actually do it.

44:19.820 --> 44:24.020
In particular, can I actually recover this claim that the

44:24.020 --> 44:31.220
posterior associated to a beta prior with Bernoulli

44:31.260 --> 44:34.980
likelihood is actually giving me a beta again.

44:34.980 --> 44:36.700
All right, so what was my prior?

44:41.220 --> 44:45.940
Well, it was beta, so P was following a beta AA, which

44:45.940 --> 44:56.860
means that P, the density, so that was pi of theta, well, I

44:56.940 --> 45:02.860
am going to write it as pi of P, was proportional to P to the

45:02.860 --> 45:08.780
A minus 1 times 1 minus P to the A minus 1, right?

45:08.780 --> 45:10.420
So that's the first ingredient I need to

45:10.420 --> 45:11.380
compute my posterior.

45:11.380 --> 45:14.300
I really need only two, if I wanted about up to constant.

45:14.300 --> 45:21.860
The second one was P. Well, we've computed that many

45:21.860 --> 45:25.540
times, and we had even a nice compact way of writing it,

45:25.540 --> 45:32.540
which was that Pn of x1, xn, given a parameter P, right?

45:32.540 --> 45:35.780
So the density, the joint density of my data given P,

45:35.780 --> 45:38.580
that's my likelihood, the likelihood of P, was what?

45:38.580 --> 45:45.380
Well, it was P to the sum of the xis, 1 minus P to the n

45:45.380 --> 45:47.140
minus sum of the xis.

45:50.900 --> 45:54.660
Anybody wants me to parse this more, or do you remember

45:54.660 --> 45:57.060
seeing that from maximum likelihood estimation?

45:57.060 --> 45:57.540
Yeah?

45:57.540 --> 46:00.660
So when you condition on random variables, you really just

46:00.660 --> 46:03.780
treat that random variable with something that's

46:03.780 --> 46:04.300
in the middle.

46:04.300 --> 46:05.260
That's what conditioning does.

46:09.660 --> 46:10.460
OK?

46:10.460 --> 46:11.140
Yeah?

46:11.140 --> 46:19.700
On the previous slide, for the bottom there, it's d pi of T.

46:19.700 --> 46:23.380
Can it be d pi of T, or is it?

46:23.460 --> 46:27.180
So d pi of T is a measure theoretic notation,

46:27.180 --> 46:29.900
which I use without thinking, and I should not,

46:29.900 --> 46:32.420
because I can see it upsets you.

46:32.420 --> 46:35.060
D pi of T is just a natural way to say

46:35.060 --> 46:38.020
that I integrate against whatever

46:38.020 --> 46:43.940
I'm given for the prior of theta.

46:43.940 --> 46:47.860
In particular, if theta is just the mix of a PDF

46:47.860 --> 46:49.620
and a point mass, right?

46:49.620 --> 46:54.460
Maybe I say that my P takes value 0.5 with probability 0.5,

46:54.460 --> 46:57.380
and then is uniform on the interval with probability

46:57.380 --> 46:58.420
0.5.

46:58.420 --> 47:01.980
OK, so for this, I neither have a PDF nor a PMF,

47:01.980 --> 47:04.180
but I can still talk about integrating with respect

47:04.180 --> 47:04.980
to this, right?

47:04.980 --> 47:10.020
It's going to look like if I take a function f of T, d pi of T,

47:10.020 --> 47:14.540
is going to be 1 half of f of 1 half, right?

47:14.540 --> 47:17.620
That's the point mass with probability 1 half at 1 half,

47:17.620 --> 47:23.140
plus 1 half of the integral between 0 and 1 of f of T, d T.

47:23.140 --> 47:26.020
So this is just a notation, which is actually,

47:26.020 --> 47:32.460
funnily enough, is interchangeable with pi of d T.

47:32.460 --> 47:37.860
But if you have a density, it's really just the density pi

47:37.860 --> 47:41.460
of T, d T, if pi is really a density.

47:41.460 --> 47:44.660
But that's when pi is a measure in other density.

47:44.700 --> 47:49.220
But so everybody else forget about this.

47:49.220 --> 47:52.180
I mean, this is not something you should really worry about.

47:52.180 --> 47:54.420
At this point, this is more graduate level probability

47:54.420 --> 47:55.980
classes.

47:55.980 --> 47:57.260
But yeah, it's called measure theory,

47:57.260 --> 47:59.180
and that's when you think of pi as being a measure.

47:59.180 --> 48:00.380
In an abstract fashion, you don't have

48:00.380 --> 48:02.780
to worry whether it's a density or not,

48:02.780 --> 48:05.700
or whether it has a density even.

48:05.700 --> 48:09.580
OK, so everybody's OK with this?

48:09.900 --> 48:17.380
All right, so now I need to compute my posterior.

48:17.380 --> 48:23.140
And as I said, my posterior is really

48:23.140 --> 48:28.300
just the product of the likelihood weighted by the prior.

48:28.300 --> 48:32.980
So hopefully, at this stage of your education,

48:32.980 --> 48:35.420
you can multiply two functions.

48:35.420 --> 48:38.260
So what's happening is if I multiply this guy with this guy,

48:38.260 --> 48:42.900
well, p gets this guy to the power of this guy plus this guy.

48:53.780 --> 49:00.060
And then 1 minus p gets the power n minus sum of xi's.

49:00.060 --> 49:02.900
So this is always from i equal 1 to n,

49:02.900 --> 49:04.540
and then plus a minus 1 as well.

49:09.060 --> 49:11.620
And this is, sorry, this is up to constant,

49:11.620 --> 49:15.540
because I still need to solve this.

49:15.540 --> 49:18.420
And I could try to do it, but I really don't have to,

49:18.420 --> 49:23.900
because I know that if my density has this form,

49:23.900 --> 49:25.580
then it's a beta distribution.

49:25.580 --> 49:27.020
And then I can just go on Wikipedia

49:27.020 --> 49:29.180
and see what should be the normalization factor.

49:29.180 --> 49:31.060
But I know it's going to be a beta distribution.

49:31.060 --> 49:33.900
It's actually the beta with parameter.

49:33.900 --> 49:37.500
So this is really my beta with parameter

49:37.540 --> 49:43.620
sum of xi i equal 1 to n plus a minus 1.

49:43.620 --> 49:50.660
And then the second parameter is n minus sum of the xi's plus a minus 1.

49:50.660 --> 49:52.300
OK?

49:52.300 --> 49:55.060
Sorry.

49:55.060 --> 49:58.580
I just wrote what was here.

49:58.580 --> 50:01.620
Oh, what happened to my 1?

50:01.620 --> 50:02.940
Oh, no, sorry, sorry, sorry.

50:02.940 --> 50:05.660
Beta has the power minus 1, right?

50:05.700 --> 50:09.140
So that's the parameter of the beta.

50:09.140 --> 50:11.780
And this is the parameter of the beta, right?

50:11.780 --> 50:15.060
So beta, well, I don't think it's anywhere.

50:15.060 --> 50:16.260
Yeah, beta is over there, right?

50:16.260 --> 50:20.300
So I just replace a by what I see.

50:20.300 --> 50:22.340
a is just becoming this guy plus this guy,

50:22.340 --> 50:23.700
and this guy plus this guy.

50:26.460 --> 50:28.500
Everybody's comfortable with this computation?

50:29.020 --> 50:38.180
All right, so we just agreed that beta priors for Bernoulli

50:38.180 --> 50:41.500
observations are certainly convenient, right?

50:41.500 --> 50:43.980
And because they're just conjugate,

50:43.980 --> 50:46.300
and we know that's what's going to come out in the end,

50:46.300 --> 50:48.300
that's going to be a beta as well.

50:48.300 --> 50:50.180
So I mean, I just claim it was convenient.

50:50.180 --> 50:52.340
It was certainly convenient to compute this, right?

50:52.340 --> 50:55.820
I mean, there was certainly some compatibility

50:55.820 --> 50:57.980
when I had to multiply this function by that function,

50:57.980 --> 51:01.020
and you can imagine that things could go much more wrong

51:01.020 --> 51:03.220
than just having p to some power and p to some power,

51:03.220 --> 51:06.380
1 minus p to some power, 1 minus p to some power.

51:06.380 --> 51:09.100
Things were nice.

51:09.100 --> 51:11.860
Now, this is nice, but I can also question the following

51:11.860 --> 51:12.340
things.

51:12.340 --> 51:14.100
Why beta, for one?

51:14.100 --> 51:16.820
I mean, the beta tells me something,

51:16.820 --> 51:20.780
but that's convenient, but then how do I pick a?

51:20.780 --> 51:25.420
I know that a should definitely capture

51:25.420 --> 51:30.180
the fact that where I want to have my p most likely located,

51:30.180 --> 51:34.540
but it also actually captures the variance of my beta.

51:34.540 --> 51:36.740
And so choosing different a's is going

51:36.740 --> 51:37.940
to have different functions.

51:37.940 --> 51:43.260
If I have a and b, if I started with the beta with parameter

51:43.260 --> 51:46.500
here, I started with a b here.

51:46.500 --> 51:49.940
I would just pick up the b here, agreed?

51:49.940 --> 51:51.300
And that would just be asymmetric,

51:51.300 --> 51:53.820
but they're going to capture mean and variance of this thing.

51:53.820 --> 51:55.740
And so how do I pick those guys?

51:55.740 --> 51:58.860
I mean, if I'm a doctor and you're

51:58.860 --> 52:01.540
asking me, what do you think the chances of this drug working

52:01.540 --> 52:03.460
on this kind of patients is, and I

52:03.460 --> 52:06.100
have to say to spit out the parameters of a beta for you,

52:06.100 --> 52:08.700
it might be a bit of a complicated thing to do.

52:08.700 --> 52:10.820
So how do you do this, especially for problems?

52:10.820 --> 52:16.180
So by now, people have actually mastered the art of coming up

52:16.180 --> 52:19.220
with how to formulate those numbers,

52:19.220 --> 52:21.580
but in new problems that come up, how do you do this?

52:21.620 --> 52:23.900
What happens if you want to use Bayesian methods,

52:23.900 --> 52:28.220
but you actually do not know what you expect to see?

52:28.220 --> 52:31.060
Maybe this is the first time you've, I mean, to be fair,

52:31.060 --> 52:33.260
before we started this class, I hope all of you

52:33.260 --> 52:36.460
had no idea whether people tended to bend their head

52:36.460 --> 52:38.260
to the right or to the left before kissing,

52:38.260 --> 52:40.580
because if you did, well, you have too much time on your hand

52:40.580 --> 52:42.340
and I should double your homework.

52:42.340 --> 52:46.220
And so in this case, you have to sort of,

52:46.220 --> 52:48.860
maybe you still want to use the Bayesian machinery.

52:48.860 --> 52:51.020
Maybe you just want to do something nice.

52:51.020 --> 52:51.700
It's nice, right?

52:51.700 --> 52:53.300
I mean, it worked out pretty well.

52:53.300 --> 52:54.660
And so what if you want to do well,

52:54.660 --> 52:56.340
you actually want to use some priors that

52:56.340 --> 52:59.820
have no carry, no information that basically do not

52:59.820 --> 53:02.660
prefer any theta to another theta.

53:02.660 --> 53:06.660
Now, you could read this slide or you could look at this formula.

53:10.020 --> 53:14.940
We just said that this pi here was just here

53:14.940 --> 53:17.820
to weigh some thetas more than others,

53:17.820 --> 53:19.900
depending on our prior belief.

53:19.900 --> 53:22.420
If our prior belief does not want to put any preference

53:22.420 --> 53:25.500
towards some thetas than to others, what do I do?

53:28.100 --> 53:29.500
Yeah, remove it.

53:29.500 --> 53:31.460
And the way to remove something we multiply by

53:31.460 --> 53:32.660
is just replace it by 1.

53:32.660 --> 53:34.820
That's really what we're doing.

53:34.820 --> 53:38.460
So if this was a constant, then not depending on theta,

53:38.460 --> 53:41.420
then that would mean that we're not preferring any theta.

53:41.420 --> 53:44.380
And we're looking sort of at the likelihood,

53:44.380 --> 53:46.580
but not as a function that we're trying to maximize,

53:46.580 --> 53:50.220
but as a function that we normalize in such a way

53:50.220 --> 53:52.500
that it's actually a distribution.

53:52.500 --> 53:54.780
So if I have pi, which is not here,

53:54.780 --> 53:56.820
this is really just taking the likelihood, which

53:56.820 --> 53:59.180
is a positive function, mean not integrate to 1.

53:59.180 --> 54:02.340
So I normalize it so that it integrates to 1.

54:02.340 --> 54:05.140
And then I just say, well, this is my posterior distribution.

54:05.140 --> 54:06.780
Now, I could just maximize this thing

54:06.780 --> 54:09.100
and spit out my maximum likelihood estimator,

54:09.100 --> 54:10.980
but now I can also integrate and find

54:10.980 --> 54:12.380
what the expectation of this guy is.

54:12.380 --> 54:14.220
I can find what the median of this guy is.

54:14.220 --> 54:16.380
I can sample data from this guy.

54:16.380 --> 54:19.460
I can understand what the variance of this guy is,

54:19.460 --> 54:21.700
which is something we did not do when we just

54:21.700 --> 54:23.660
did maximum likelihood estimation because,

54:23.660 --> 54:25.620
given a function, all we cared about

54:25.620 --> 54:30.020
was the arg max of this function.

54:30.020 --> 54:35.860
So this priors are called uninformative.

54:35.860 --> 54:40.100
So this is just replacing this number by 1.

54:40.100 --> 54:44.020
And if I have a or by a constant, because it still

54:44.020 --> 54:45.700
has to be a density.

54:45.700 --> 54:50.420
And so if I have a bounded set, I'm

54:50.420 --> 54:53.540
just looking for the uniform distribution on this bounded

54:53.540 --> 54:56.620
set, the one that puts constant one

54:56.620 --> 54:59.140
over the size of this thing.

54:59.140 --> 55:01.660
But if I have an unbounded set, what

55:01.660 --> 55:03.900
is the density that takes a constant value

55:03.900 --> 55:07.580
on the entire real line, for example?

55:07.580 --> 55:08.500
What is this density?

55:16.580 --> 55:18.300
Doesn't exist.

55:18.300 --> 55:20.860
I mean, it just doesn't exist.

55:20.860 --> 55:23.500
The way you can think of it is a Gaussian with the variance

55:23.500 --> 55:26.380
going to infinity, maybe, or something like this.

55:26.380 --> 55:27.900
But you can think of it in many ways.

55:27.900 --> 55:32.380
You can think of the limit of the uniform between minus t

55:32.380 --> 55:34.220
and t with t going to infinity.

55:34.220 --> 55:36.500
But this thing is actually 0.

55:36.500 --> 55:38.340
There's nothing there.

55:38.340 --> 55:42.020
And so you can actually still talk about this.

55:42.020 --> 55:44.220
You could always talk about this thing

55:44.220 --> 55:46.580
where you think of this guy as being a constant,

55:46.580 --> 55:48.500
remove this thing from this equation,

55:48.500 --> 55:50.700
and just say, well, my posterior is just

55:50.700 --> 55:54.140
the likelihood divided by the integral of the likelihood

55:54.140 --> 55:54.700
over theta.

55:54.700 --> 55:58.660
And if theta is the entire real line, so be it.

55:58.660 --> 56:00.540
As long as this integral converges,

56:00.540 --> 56:03.700
you can still talk about this stuff.

56:03.700 --> 56:06.340
And so this is what's called an improper prior.

56:09.140 --> 56:11.780
An improper prior is just a non-negative function

56:11.780 --> 56:13.700
defined on theta, but it does not

56:13.700 --> 56:19.740
have to integrate neither to 1 nor to anything.

56:19.740 --> 56:20.940
It does not have to write.

56:20.940 --> 56:22.740
If I integrate the function equal to 1

56:22.740 --> 56:24.380
on the entire real line, what do I get?

56:27.820 --> 56:29.100
Infinity, right?

56:29.100 --> 56:32.140
I mean, it's not a proper integral,

56:32.140 --> 56:34.180
and so it's not a proper prior, and it's

56:34.180 --> 56:35.980
called an improper prior.

56:35.980 --> 56:39.380
And those improper priors are usually

56:39.380 --> 56:42.860
what you see when you start to want non-informative priors

56:42.860 --> 56:44.220
on infinite set status.

56:44.220 --> 56:46.780
I mean, that's just the nature of it.

56:46.780 --> 56:50.020
You should think of it as being the uniform distribution

56:50.020 --> 56:54.620
on some infinite set if that thing were to exist.

56:54.620 --> 56:59.180
So let's see some examples about non-informative priors.

56:59.180 --> 57:04.340
So if I'm on the interval 0, 1, this is a finite set,

57:04.340 --> 57:08.740
so I can talk about the uniform prior on the interval 0, 1

57:08.740 --> 57:11.540
for a parameter p of a Bernoulli, OK?

57:11.540 --> 57:27.540
And so if I want to talk about this,

57:27.540 --> 57:35.100
then it means that my prior is p follows some uniform

57:35.100 --> 57:37.580
on the interval 0, 1, OK?

57:37.580 --> 57:45.980
So it means that the density is, well, f of x is 1

57:45.980 --> 57:48.940
if x is in 0, 1 and 0.

57:48.940 --> 57:52.020
Otherwise, there's actually not even a normalization.

57:52.020 --> 57:53.900
This thing integrates to 1.

57:53.900 --> 57:56.220
And so now, if I look at my likelihood,

57:56.220 --> 57:57.340
it's still the same thing.

57:57.340 --> 58:04.460
So my posterior becomes theta x1, xn.

58:04.460 --> 58:07.180
So that's my posterior.

58:07.220 --> 58:09.900
I don't write the likelihood again because we still have it.

58:09.900 --> 58:12.500
Well, we don't have it there anymore.

58:12.500 --> 58:13.620
Is it here?

58:13.620 --> 58:15.340
Or did I just erase it?

58:15.340 --> 58:17.740
Yeah, the likelihood is given here.

58:17.740 --> 58:20.140
So copy paste over there.

58:20.140 --> 58:23.060
And so the posterior is just this thing times 1.

58:23.060 --> 58:24.300
So you will see it in a second.

58:24.300 --> 58:27.980
So it's p times the sum to the power sum of the xi's,

58:27.980 --> 58:31.980
1 minus p to the power n minus sum of the xi's.

58:31.980 --> 58:34.700
And then it's multiplied by 1 and then

58:34.700 --> 58:42.500
divided by this integral between 0 and 1 of p sum of the xi's,

58:42.500 --> 58:50.940
1 minus p n minus sum of the xi's dp,

58:50.940 --> 58:52.220
which does not depend on p.

58:52.220 --> 58:56.220
And I really don't care what this thing actually is.

58:56.220 --> 59:03.580
So now, sorry, that's prior posterior of p.

59:03.580 --> 59:06.100
And now I can see, well, what is this?

59:06.100 --> 59:13.420
Well, it's actually just the beta with parameters this guy

59:13.420 --> 59:21.700
plus 1 and this guy plus 1.

59:21.700 --> 59:38.300
So I didn't tell you what the expectation of a beta was.

59:38.300 --> 59:41.180
We don't know what the expectation of a beta is.

59:41.180 --> 59:41.900
Agreed?

59:41.900 --> 59:45.900
I mean, if I wanted to find, say, the expectation of this thing,

59:45.900 --> 59:47.660
that would be some good estimator,

59:47.660 --> 59:49.820
we know that the maximum of this guy,

59:49.820 --> 59:51.180
what is the maximum of this thing?

59:54.860 --> 59:56.100
Well, it's just this thing, right?

59:56.100 --> 59:58.300
I mean, it's the average of the xi's, right?

59:58.300 --> 01:00:00.300
That's just the maximum likelihood estimator for Bernoulli.

01:00:00.300 --> 01:00:01.820
We know it's the average.

01:00:01.820 --> 01:00:03.980
Do you think if I take the expectation of this thing,

01:00:03.980 --> 01:00:05.220
I'm going to get the average?

01:00:14.060 --> 01:00:15.820
So actually, I'm not going to get the average.

01:00:15.820 --> 01:00:20.340
I'm going to get this guy plus this guy divided by n plus 1.

01:00:20.340 --> 01:00:24.900
So I'm going to do as if I had, oh, sorry.

01:00:24.900 --> 01:00:27.340
OK, let me not say it like that.

01:00:27.340 --> 01:00:28.900
Let's look at what this thing is doing.

01:00:28.900 --> 01:00:32.900
It's looking at the number of 0's, the number of 1's,

01:00:32.900 --> 01:00:34.540
and it's adding 1.

01:00:34.540 --> 01:00:36.300
And this guy is looking at the number of 0's,

01:00:36.300 --> 01:00:39.220
and it's adding 1, OK?

01:00:39.220 --> 01:00:41.940
Why is it adding this 1?

01:00:41.940 --> 01:00:44.300
What's going on here?

01:00:44.300 --> 01:00:48.140
Well, what would happen if I had, so this actually

01:00:48.140 --> 01:00:52.980
is going to matter mostly when the number of 1's is actually

01:00:52.980 --> 01:00:56.100
0 or the number of 0's is 0?

01:00:56.100 --> 01:00:59.980
Because what it does is just pushes the 0 from non-zero.

01:00:59.980 --> 01:01:03.020
And why is that something that this Bayesian method actually

01:01:03.020 --> 01:01:05.300
does for you automatically is because when

01:01:05.300 --> 01:01:09.140
we put this non-informative prior on p, which

01:01:09.140 --> 01:01:12.020
was uniform on the interval 0, 1, in particular,

01:01:12.020 --> 01:01:16.700
we know that the probability that p is equal to 0 is 0,

01:01:16.700 --> 01:01:19.220
and the probability that p is equal to 1 is 0.

01:01:19.220 --> 01:01:21.220
And so the problem is that, essentially,

01:01:21.220 --> 01:01:24.540
if I did not add this 1 with some positive probability,

01:01:24.540 --> 01:01:27.060
I would be allowed to spit out something that actually

01:01:27.060 --> 01:01:29.900
had p hat, which was equal to 0.

01:01:29.900 --> 01:01:33.300
In the case, if by chance, let's say I have n is equal to 3,

01:01:33.300 --> 01:01:35.940
and I get only 0, 0, 0, right?

01:01:35.940 --> 01:01:40.340
That could happen with probability 1 over p cubed,

01:01:41.340 --> 01:01:43.900
sorry, 1 minus p cubed.

01:01:43.900 --> 01:01:46.380
Then this thing is just going to not,

01:01:46.380 --> 01:01:47.700
that's not something that I want,

01:01:47.700 --> 01:01:49.300
and I'm actually using my prior.

01:01:49.300 --> 01:01:51.180
So my prior is not informative, but somehow it

01:01:51.180 --> 01:01:52.940
captures the fact that I don't want to believe

01:01:52.940 --> 01:01:55.780
that p is going to be either equal to 0 or 1.

01:01:55.780 --> 01:01:59.500
OK, and so that's sort of taken care of here.

01:01:59.500 --> 01:02:05.620
OK, so let's move away a little bit from the Bernoulli example,

01:02:05.620 --> 01:02:06.100
shall we?

01:02:06.100 --> 01:02:08.100
I mean, we think we've seen enough of it.

01:02:08.140 --> 01:02:10.860
And so let's talk about the Gaussian model, right?

01:02:10.860 --> 01:02:17.980
Let's say I want to do Gaussian inference in the,

01:02:17.980 --> 01:02:20.020
I want to do inference in a Gaussian model using

01:02:20.020 --> 01:02:22.020
vision methods.

01:02:22.020 --> 01:02:30.220
OK, so I'm going to actually look at, so say, OK,

01:02:30.220 --> 01:02:44.780
so what I want is that xi, x1, xn, or say, n, 0, 1, i, i, d,

01:02:44.780 --> 01:02:50.260
sorry, theta 1, i, i, d, conditionally on theta.

01:02:50.260 --> 01:02:56.340
OK, so that means that pn of x1, xn, given theta,

01:02:56.340 --> 01:02:58.620
is equal to, well, exactly what I wrote before.

01:02:58.620 --> 01:03:04.380
So 1 square root 2 pi to the n exponential minus 1

01:03:04.380 --> 01:03:09.340
half sum of xi minus theta squared.

01:03:09.340 --> 01:03:11.140
OK, so that's just the joint distribution

01:03:11.140 --> 01:03:13.940
of my n-Gaussians with mean theta.

01:03:13.940 --> 01:03:16.660
Another question is, what is the posterior distribution?

01:03:16.660 --> 01:03:22.420
OK, well, here I said, let's use the uninformative prior,

01:03:22.420 --> 01:03:23.860
which is an improper prior, right?

01:03:23.860 --> 01:03:25.540
It puts weight 1 on everyone.

01:03:25.540 --> 01:03:29.220
It has the so-called uniform on the entire real line,

01:03:29.220 --> 01:03:31.180
so that's certainly not a density.

01:03:31.180 --> 01:03:34.380
But I can still just use this, right?

01:03:34.380 --> 01:03:42.220
So all I need to do is to get this divided by normalizing

01:03:42.220 --> 01:03:43.180
this thing, right?

01:03:43.180 --> 01:03:44.660
So that's what I need to do.

01:03:44.660 --> 01:03:46.380
But if I look at this, right?

01:03:46.380 --> 01:03:49.500
So essentially, I want to understand,

01:03:49.500 --> 01:03:52.980
so this is proportional to exponential minus 1

01:03:52.980 --> 01:03:58.860
half sum from i equal 1 to n of xi minus theta squared.

01:03:58.860 --> 01:04:02.900
And now I want to see this thing as a density not on the xi's,

01:04:02.900 --> 01:04:06.420
but on theta, right?

01:04:06.420 --> 01:04:10.020
What I want is a density on theta.

01:04:10.020 --> 01:04:13.660
So it looks like I have chances of getting something

01:04:13.660 --> 01:04:15.540
that looks like a Gaussian.

01:04:15.540 --> 01:04:17.540
But if I really need to have a Gaussian,

01:04:17.540 --> 01:04:19.540
I would need to see minus 1 half,

01:04:19.540 --> 01:04:22.700
and then I would need to see theta minus something here.

01:04:22.900 --> 01:04:25.220
Not just the sum of something minus theta's.

01:04:25.220 --> 01:04:27.380
So I need to work a little bit more

01:04:27.380 --> 01:04:31.540
so I can see what this to expand the square here.

01:04:31.540 --> 01:04:32.900
So this thing here is going to be

01:04:32.900 --> 01:04:37.380
equal to exponential minus 1 half sum from i equal 1

01:04:37.380 --> 01:04:45.300
to n of xi squared minus 2 xi theta plus theta squared.

01:04:52.700 --> 01:04:53.200
Ah.

01:05:06.140 --> 01:05:11.820
OK, and so now basically what I'm going to do

01:05:11.820 --> 01:05:15.900
is everything, remember, is up to this little sign, right?

01:05:15.900 --> 01:05:19.740
So every time I see a term that does not depend on theta,

01:05:19.740 --> 01:05:22.260
I can just push it in there and just make it disappear.

01:05:22.260 --> 01:05:23.900
Agreed?

01:05:23.900 --> 01:05:26.820
OK, this term here, exponential minus 1

01:05:26.820 --> 01:05:31.780
half sum of xi squared, does it depend on theta?

01:05:31.780 --> 01:05:33.740
No, so I'm just pushing it here.

01:05:33.740 --> 01:05:35.980
This guy, yes, and the other one, yes.

01:05:35.980 --> 01:05:40.180
So this is proportional to exponential xi, sorry,

01:05:40.180 --> 01:05:45.020
sum of the xi.

01:05:45.020 --> 01:05:46.820
And then I'm going to pull out my theta.

01:05:46.820 --> 01:05:50.140
The minus 1 half cancel with the minus 2.

01:05:50.140 --> 01:05:56.900
And then I have minus 1 half sum from i equal 1

01:05:56.900 --> 01:05:58.620
to n of theta squared, right?

01:06:01.620 --> 01:06:03.460
Agreed?

01:06:03.460 --> 01:06:05.380
So now what this thing looks like, well,

01:06:05.380 --> 01:06:09.580
this looks very much like some theta minus something squared.

01:06:09.580 --> 01:06:16.940
This thing here is really just n over 2 times theta.

01:06:16.980 --> 01:06:21.780
So sorry, times theta squared.

01:06:21.780 --> 01:06:25.180
So now what I need to do is to write this of the form theta

01:06:25.180 --> 01:06:28.420
minus something, let's call it mu squared,

01:06:28.420 --> 01:06:31.820
maybe divided by 2 sigma squared, right?

01:06:31.820 --> 01:06:34.220
I want to turn this into that, maybe up to terms

01:06:34.220 --> 01:06:36.540
that do not depend on theta.

01:06:36.540 --> 01:06:39.140
That's what I'm going to try to do.

01:06:39.140 --> 01:06:40.700
So that's called completing the square,

01:06:40.700 --> 01:06:41.940
and that's some exercise you do.

01:06:41.940 --> 01:06:44.140
You've done it probably already in the homework,

01:06:44.140 --> 01:06:46.580
and that's something you do a lot when

01:06:46.580 --> 01:06:48.820
you do Bayesian statistics in particular.

01:06:48.820 --> 01:06:49.500
So let's do this.

01:06:49.500 --> 01:06:51.340
Well, what is going to be the leading term?

01:06:51.340 --> 01:06:54.180
Well, theta squared is going to be multiplied by this thing.

01:06:54.180 --> 01:06:57.140
So I'm going to pull out my n over 2,

01:06:57.140 --> 01:07:04.820
and then I'm going to write this as theta squared minus theta

01:07:04.820 --> 01:07:06.220
minus something squared.

01:07:06.220 --> 01:07:08.300
And this something is going to be 1

01:07:08.300 --> 01:07:10.260
half of what I see in the cross product, right?

01:07:10.580 --> 01:07:12.260
Well, I need to actually pull this thing out.

01:07:12.260 --> 01:07:16.140
So let me write it like that first.

01:07:16.140 --> 01:07:19.420
So that's theta squared.

01:07:19.420 --> 01:07:27.940
And then I'm going to write it as minus 2 times 1 over n,

01:07:27.940 --> 01:07:34.900
sum from i equal 1 to n of xi times theta, right?

01:07:34.900 --> 01:07:37.900
That's exactly just the rewriting of what we had before.

01:07:37.900 --> 01:07:40.100
That's the rewriting of what we had before.

01:07:40.100 --> 01:07:42.660
And that should look much more familiar.

01:07:42.660 --> 01:07:48.340
x squared minus a squared minus 2 blab a,

01:07:48.340 --> 01:07:49.740
and then I missed something.

01:07:49.740 --> 01:07:54.740
So this thing I'm going to be able to rewrite as theta minus xn

01:07:54.740 --> 01:07:57.940
bar squared.

01:07:57.940 --> 01:08:00.740
But then I need to remove the square of xn bar,

01:08:00.740 --> 01:08:01.820
because it's not here.

01:08:02.420 --> 01:08:03.540
OK?

01:08:03.540 --> 01:08:06.020
So I just complete the square.

01:08:06.020 --> 01:08:08.420
And then I actually really don't care what this thing actually

01:08:08.420 --> 01:08:10.020
was, because it's going to go again

01:08:10.020 --> 01:08:12.940
in the little alpha sign over there.

01:08:12.940 --> 01:08:14.540
So this thing eventually is going

01:08:14.540 --> 01:08:22.540
to be proportional to exponential of minus n over 2 times theta

01:08:22.540 --> 01:08:25.620
minus xn bar squared.

01:08:25.620 --> 01:08:27.460
And so we know that this is going to be

01:08:27.460 --> 01:08:31.100
times theta minus xn bar squared.

01:08:31.100 --> 01:08:33.380
And so we know that if this is a density that's

01:08:33.380 --> 01:08:42.060
proportional to this guy, it has to be some n with mean

01:08:42.060 --> 01:08:44.860
xn bar and variance.

01:08:44.860 --> 01:08:47.540
Well, this is supposed to be 1 over sigma squared,

01:08:47.540 --> 01:08:49.420
this guy over here, this n.

01:08:49.420 --> 01:08:53.900
So that's really just 1 over n, OK?

01:08:53.940 --> 01:09:02.500
So the posterior distribution is a Gaussian centered

01:09:02.500 --> 01:09:09.740
at the average of my observations and with variance 1 over n.

01:09:09.740 --> 01:09:10.240
OK?

01:09:13.580 --> 01:09:15.260
Everybody's with me?

01:09:15.260 --> 01:09:17.940
So just why I'm saying this, I mean,

01:09:17.940 --> 01:09:19.700
this was the output of some computation,

01:09:19.700 --> 01:09:21.460
but it sort of makes sense, right?

01:09:21.460 --> 01:09:23.540
It's really telling me that the more observations

01:09:23.540 --> 01:09:26.260
I have, the more concentrated this posterior is,

01:09:26.260 --> 01:09:27.740
concentrated around what?

01:09:27.740 --> 01:09:30.020
Well, around this xn bar.

01:09:30.020 --> 01:09:33.140
So that looks like something we've sort of seen before,

01:09:33.140 --> 01:09:35.420
but it does not have the same meaning somehow.

01:09:35.420 --> 01:09:37.340
This is really just the posterior distribution,

01:09:37.340 --> 01:09:40.500
and it's not really, I mean, it sort of says,

01:09:40.500 --> 01:09:43.340
it's sort of a sanity check that I have this 1 over n when

01:09:43.340 --> 01:09:45.420
I have xn bar, but it's not the same thing

01:09:45.420 --> 01:09:47.500
as saying that the variance of xn bar was 1 over n

01:09:47.500 --> 01:09:49.180
like we had before, OK?

01:09:54.340 --> 01:09:58.300
So as an exercise, well, you probably will have it,

01:09:58.300 --> 01:10:01.300
but I would recommend if you don't get it,

01:10:01.300 --> 01:10:18.020
just try pi of theta to be equal to some n mu 1, OK?

01:10:18.020 --> 01:10:22.380
So here, the prior that we use was completely non-informative.

01:10:22.380 --> 01:10:25.620
What happens if I take my prior to be some Gaussian, which

01:10:25.620 --> 01:10:27.540
is centered at mu, and it has the same variance

01:10:27.540 --> 01:10:30.140
as the other guys, OK?

01:10:30.140 --> 01:10:33.140
So what's going to happen here is that we're going to put a weight,

01:10:33.140 --> 01:10:34.540
and everything that's away from mu

01:10:34.540 --> 01:10:37.740
is going to actually get less weight, right?

01:10:37.740 --> 01:10:40.860
And I want to know how I'm going to be updating this prior

01:10:40.860 --> 01:10:42.100
into a posterior.

01:10:42.100 --> 01:10:45.700
So that's right, so everybody sees what I'm saying here.

01:10:45.700 --> 01:10:47.580
So pi of theta is just so that means

01:10:47.580 --> 01:10:50.060
that pi of theta has the density proportional

01:10:50.060 --> 01:10:55.700
to exponential minus 1 half theta minus mu squared, right?

01:10:55.700 --> 01:11:00.500
So I need to multiply my posterior with this

01:11:00.500 --> 01:11:03.420
and then see what I'd say actually going to be a Gaussian.

01:11:03.420 --> 01:11:04.860
This is also a conjugate prior.

01:11:04.860 --> 01:11:06.460
It's going to spit out another Gaussian.

01:11:06.460 --> 01:11:08.420
You're going to have to complete a square again

01:11:08.420 --> 01:11:10.820
and just check what it's actually giving you.

01:11:10.820 --> 01:11:12.540
And so spoiler alert, it's going to look

01:11:12.540 --> 01:11:14.420
like you get an extra observation, which

01:11:14.420 --> 01:11:17.580
is actually equal to mu, OK?

01:11:17.620 --> 01:11:22.300
So it's going to be the average of n plus 1 observations,

01:11:22.300 --> 01:11:27.500
the first n ones being x1 to xn and the last one being mu.

01:11:27.500 --> 01:11:30.420
And it sort of makes sense.

01:11:30.420 --> 01:11:32.700
OK, so that's actually a fairly simple exercise.

01:11:32.700 --> 01:11:36.740
But before, rather than going into more computation,

01:11:36.740 --> 01:11:38.540
this is something you can definitely do

01:11:38.540 --> 01:11:41.780
in the comfort of your room, I want

01:11:41.780 --> 01:11:43.340
to talk about other types of priors, right?

01:11:43.340 --> 01:11:47.340
So the first thing I said is, OK, there's this beta prior

01:11:47.340 --> 01:11:50.380
that I just pulled out of my hat and that was just convenient.

01:11:50.380 --> 01:11:52.900
Then there was this non-informative prior.

01:11:52.900 --> 01:11:53.820
It was convenient, right?

01:11:53.820 --> 01:11:54.780
It was non-informative.

01:11:54.780 --> 01:11:57.340
So if you don't know anything else,

01:11:57.340 --> 01:11:58.860
maybe that's what you want to do.

01:11:58.860 --> 01:12:01.620
The question is, are there any other priors

01:12:01.620 --> 01:12:04.500
that are sort of principled and generic in the sense

01:12:04.500 --> 01:12:08.460
that the uninformative prior was generic, right?

01:12:08.460 --> 01:12:09.500
I mean, it was equal to 1.

01:12:09.500 --> 01:12:11.380
That's as generic as it gets.

01:12:11.380 --> 01:12:14.140
And so is there anything that's generic as well?

01:12:14.140 --> 01:12:17.220
Well, there's these priors that are called Jeffery's priors.

01:12:17.220 --> 01:12:19.060
And Jeffery's prior is a prior which

01:12:19.060 --> 01:12:21.500
is proportional to square root of the determinant

01:12:21.500 --> 01:12:25.660
of the Fisher information of theta, OK?

01:12:25.660 --> 01:12:28.620
And so this is actually kind of a weird thing to do, right?

01:12:28.620 --> 01:12:31.420
It says, compute your, look at your model, right?

01:12:31.420 --> 01:12:34.340
Your model is going to have a Fisher information.

01:12:34.340 --> 01:12:36.180
Let's say it exists.

01:12:36.180 --> 01:12:39.900
And because we know it does not always

01:12:39.900 --> 01:12:41.580
exist, for example, in the multinomial model,

01:12:41.580 --> 01:12:43.980
we didn't have a Fisher information.

01:12:43.980 --> 01:12:46.140
And so the determinant of a matrix

01:12:46.140 --> 01:12:48.660
is somehow measuring the size of a matrix, right?

01:12:48.660 --> 01:12:50.540
And if you don't trust me, just think

01:12:50.540 --> 01:12:53.860
about the matrix being of size one by one,

01:12:53.860 --> 01:12:56.820
then the determinant is just the number that you have there.

01:12:56.820 --> 01:12:58.660
And so this is really something that

01:12:58.660 --> 01:13:02.500
looks like the Fisher information.

01:13:02.500 --> 01:13:04.700
I mean, it's just basically the amount of information

01:13:04.700 --> 01:13:06.340
is proportional to the amount of information

01:13:06.340 --> 01:13:09.620
that you have at a certain point, OK?

01:13:09.620 --> 01:13:12.220
And so what my prior is saying is saying, well,

01:13:12.220 --> 01:13:14.300
I want to put more weights on those status that

01:13:14.300 --> 01:13:19.860
are going to just extract more information from the data, OK?

01:13:19.860 --> 01:13:22.260
So you can actually compute those things, right?

01:13:22.260 --> 01:13:26.420
So in the first example, Jeffery's prior

01:13:26.420 --> 01:13:28.100
is something that looks like this, right?

01:13:28.100 --> 01:13:30.260
I mean, in one dimension, Fisher information

01:13:30.260 --> 01:13:33.380
is essentially one over the variance, right?

01:13:33.380 --> 01:13:35.620
So that's just one over the square root of the variance,

01:13:35.620 --> 01:13:37.540
because I have the square root.

01:13:37.540 --> 01:13:41.460
And when I have the uniform, sorry,

01:13:41.460 --> 01:13:46.780
the Jeffery's prior, when I have the Gaussian case, right?

01:13:46.780 --> 01:13:48.940
So this is the identity matrix that I

01:13:48.940 --> 01:13:50.620
would have in the Gaussian case.

01:13:50.620 --> 01:13:52.620
So the determinant of the identity is one,

01:13:52.620 --> 01:13:54.900
so square root of one is one.

01:13:54.900 --> 01:13:56.180
And so I would basically get one,

01:13:56.180 --> 01:13:59.220
and that gives me my improper prior, my uninformative prior

01:13:59.220 --> 01:14:00.820
that I had.

01:14:00.820 --> 01:14:03.100
OK, so the uninformative prior one is fine.

01:14:03.100 --> 01:14:06.780
I mean, clearly, all the Thetas carry the same information

01:14:06.780 --> 01:14:07.980
in the Gaussian model, right?

01:14:07.980 --> 01:14:10.220
I mean, whether I translate it here or here,

01:14:10.260 --> 01:14:11.740
it's pretty clear that none of them

01:14:11.740 --> 01:14:13.180
is actually better than the other.

01:14:13.180 --> 01:14:16.540
But clearly, for the Bernoulli case,

01:14:16.540 --> 01:14:22.260
the piece that are closer to the boundary

01:14:22.260 --> 01:14:23.740
carry more information, right?

01:14:23.740 --> 01:14:26.100
So I sort of like those guys because they just

01:14:26.100 --> 01:14:27.700
like carry more information.

01:14:27.700 --> 01:14:30.340
So what I do is that I take this function, so p1 minus p,

01:14:30.340 --> 01:14:34.820
remember, is something that looks like this on the interval

01:14:34.820 --> 01:14:37.980
0, 1, 0, and 1.

01:14:38.020 --> 01:14:41.260
So this guy, 1 over square root of p1 minus p,

01:14:41.260 --> 01:14:47.380
is something that looks like this, agreed?

01:14:47.380 --> 01:14:48.820
And so what he's doing is sort of like

01:14:48.820 --> 01:14:51.180
wants to push towards the piece that actually

01:14:51.180 --> 01:14:54.380
carry more information.

01:14:54.380 --> 01:14:56.980
I mean, whether you want to bias your data that way or not

01:14:56.980 --> 01:14:58.620
is something you need to think about, right?

01:14:58.620 --> 01:15:00.700
I mean, when you put a prior on your data,

01:15:00.700 --> 01:15:03.180
on your parameter, you're sort of like biasing

01:15:03.180 --> 01:15:05.060
towards this idea, your data.

01:15:05.060 --> 01:15:07.740
And maybe that's maybe not such a good idea

01:15:07.740 --> 01:15:13.100
when you have some p that's actually close to 1 half,

01:15:13.100 --> 01:15:13.820
for example.

01:15:13.820 --> 01:15:14.940
You're actually saying, no, I don't

01:15:14.940 --> 01:15:16.620
want to see a p that's close to 1 half.

01:15:16.620 --> 01:15:18.340
Just make a decision one way or another,

01:15:18.340 --> 01:15:19.700
but just make a decision.

01:15:19.700 --> 01:15:22.500
So it's sort of forcing you to do that.

01:15:22.500 --> 01:15:26.100
OK, and so Jeffery's prior, so I'm running out of time,

01:15:26.100 --> 01:15:29.140
so I don't want to go into too much details.

01:15:29.140 --> 01:15:32.420
But we'll probably stop here, actually.

01:15:32.420 --> 01:15:47.780
So Jeffery's priors have this very nice property

01:15:47.780 --> 01:15:51.740
is that they actually do not care about the parametrization

01:15:51.740 --> 01:15:52.980
of your space.

01:15:52.980 --> 01:15:56.380
So if you actually have p, and you suddenly

01:15:56.380 --> 01:15:58.820
decide that p is not the right parameter for Bernoulli,

01:15:58.820 --> 01:16:02.260
but it's p squared, you could decide to parametrize this

01:16:02.260 --> 01:16:03.180
by p squared.

01:16:03.180 --> 01:16:05.420
Maybe your doctor is actually much more

01:16:05.420 --> 01:16:08.860
able to formulate some prior assumption on p squared

01:16:08.860 --> 01:16:09.820
rather than p.

01:16:09.820 --> 01:16:10.980
You never know.

01:16:10.980 --> 01:16:14.380
And so what happens is that Jeffery's priors

01:16:14.380 --> 01:16:15.860
are invariant into this.

01:16:15.860 --> 01:16:18.100
And the reason is because, well, the information carried

01:16:18.100 --> 01:16:20.060
by p is the same as the information carried

01:16:20.060 --> 01:16:21.460
by p squared somehow, right?

01:16:21.460 --> 01:16:28.700
I mean, those are essentially the same, I mean, well,

01:16:28.700 --> 01:16:30.780
yeah, they're essentially the same thing.

01:16:30.780 --> 01:16:34.620
And so I mean, you need to have a one-to-one map, right?

01:16:34.620 --> 01:16:38.060
Where you basically, for each parameter before you

01:16:38.060 --> 01:16:41.100
have another parameter, so let's call eta the new parameters.

01:16:41.100 --> 01:16:50.380
Then the PDF of the new prior indexed by eta this time

01:16:50.380 --> 01:16:52.980
is actually also Jeffery's prior.

01:16:52.980 --> 01:16:55.220
But this time, the new Fisher information

01:16:55.220 --> 01:16:57.340
is not the Fisher information with respect to theta,

01:16:57.340 --> 01:17:00.060
but it says in the Fisher information associated

01:17:00.100 --> 01:17:03.140
to the statistical model indexed by eta.

01:17:03.140 --> 01:17:06.740
So essentially, when you change Jeffery's prior,

01:17:06.740 --> 01:17:08.780
when you change the parameterization of your model,

01:17:08.780 --> 01:17:12.820
you still get Jeffery's prior for the new parameterization,

01:17:12.820 --> 01:17:16.540
which is, in a way, a desirable property.

01:17:16.540 --> 01:17:20.460
All right, so Jeffery's priors, just

01:17:20.460 --> 01:17:22.460
like non-informative priors, are priors

01:17:22.460 --> 01:17:25.100
you want to use when you want a systematic way

01:17:25.100 --> 01:17:28.620
without really thinking about what to pick for your model.

01:17:28.660 --> 01:17:37.060
OK, so, well, OK, I'll finish this next time.

01:17:37.060 --> 01:17:39.940
And we'll talk about Bayesian confidence regions.

01:17:39.940 --> 01:17:41.620
We'll talk about Bayesian estimation.

01:17:41.620 --> 01:17:44.100
Once I have a posterior, what do I get?

01:17:44.100 --> 01:17:45.540
And basically, the only message is

01:17:45.540 --> 01:17:46.980
going to be that, well, you might

01:17:46.980 --> 01:17:48.940
want to integrate against the posterior.

01:17:48.940 --> 01:17:52.140
Find the expectation of your posterior distribution.

01:17:52.140 --> 01:17:54.660
That's a good point estimator for theta.

01:17:54.660 --> 01:18:00.580
And then we'll just do a couple of computation.

01:18:00.580 --> 01:18:01.340
All right, so.

