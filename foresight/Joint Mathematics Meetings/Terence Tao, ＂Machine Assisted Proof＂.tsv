start	end	text
0	13920	Hello. Hello-hello. Good afternoon. I'm Bryna Kraw, President of the AMS, and it's my pleasure
13920	19560	to welcome you to the colloquium lectures. These are the oldest lectures at the meetings
19560	27840	of the AMS. The first meeting was held in 1895. The second in 1896 were the first colloquium
27840	32440	lectures actually took place, and those were at Northwestern University, and since that's
32440	38720	my home institution, I can't help but mention them. The list of speakers is a veritable
38720	48440	who's who in mathematics, including Birkhoff, Morse, von Neumann, Tarski, Czern, Milner,
48440	55120	Smell, Nirenberg, Tate, and the list of people I've left off is, well, equally prestigious
55120	61640	to those that I included, and amongst them was also our speaker today's advisor, Stein,
61640	71400	Ilya Stein. So with that, I'll turn to the introduction of Terry Tau. Terry may be somebody
71400	75360	who doesn't need introduction. After all, he's been in a crossword puzzle as a clue
75360	83160	in the New York Times crossword puzzle. I don't want to use up all his time by listing
83200	88080	the awards he's won, but I could. I'll give you just a short list of the highlights of
88080	94600	the Fields Medal in 2006. I'm a Carther Fellowship. He's a fellow of the Royal Society, the Australian
94600	98240	Academy of Sciences, the American Academy of Arts and Science, and a member of the National
98240	102800	Academy of Science, and of course he's a fellow of the AMS, most important of those
102800	111280	distinctions. He has over 350 publications, meaning he hasn't slept in a few years, and
111400	116720	this includes numerous highly influential texts, and he has more than 50 collaborators,
116720	120160	and maybe that's when I counted last week, so I don't know, maybe this week there's
120160	126640	more. He's also one of the broadest researchers in mathematics, covering interests from pure
126640	135760	to applied, and I won't list all of the subjects. But it's not only research, he also serves
135760	142600	the profession in numerous ways, and starting in 2021 took on a very substantial role. He
142600	147040	was appointed by President Biden as a member of the President's Council of Advisors on
147040	153840	Science and Technology. So he's a force. He's mentored over 20 PhD students, and I could
153840	162840	go on and on. So I've known Terry for a while, since about 2004 or 2005 when we first met,
163240	171240	but one of my fondest memories of him was at a party that I was a co-host for. In 2008,
171240	178160	it was the election night, and Terry was sitting in the corner November 2008 on his computer on
178160	184680	some website that was giving election returns, announcing the states before the TV was. It
184680	190440	was really impressive, and it's just one of my fondest memories. Anyway, I won't keep you any
191240	193320	longer. It's my pleasure to introduce Terry.
193320	210520	All right. Thank you very much, Bryna. I'm very happy to be here at this JMM to give this lecture.
210520	216600	It's always nice to be up in San Francisco. So are we talking about what I think is a really
216600	222760	exciting development in mathematics that's going to shape our future, which is really the
224520	229800	development over the last few years of lots of technologies to make machines and computers
231880	240120	help us do math much more effectively. Now, to some sense, this is not new. We have used
240120	245240	both machines and computers, and I use the terms slightly differently. We've used both,
245240	250520	actually, for centuries, really. Nowadays, computers and machines, when we talk about
250520	254840	machine-assisted mathematics and computer-assisted mathematics, they're sort of synonymous. Initially,
254840	259320	they weren't because computers used to be human, and then they were mechanical and then finally
259320	267400	electronic. So, for example, one of the earliest use of computers was to build tables. So, for
267400	272120	example, the large tables of Napier and so forth were basically built by lots and lots of human
272120	277480	computers, and those are the earliest examples of somehow computer-assisted mathematics,
278520	282440	and tables are still really important today. I mean, not so much the log tables anymore,
283560	290280	but a lot of what we call experimental mathematics is based on creating lots and lots of large
290280	295560	tables of interesting things, especially in number theory. So, for example, famously,
295560	301560	the Gendo and Gauss built tables of prime numbers, and they used that to conjecture the prime number
301560	307240	theorem, which is, of course, now a theorem. And similarly, in the 60s, Bertrand's Wittendeyer
308840	314840	created lots and lots of big tables of the decurs and the ranks and so forth, and this was a key
314840	321160	input in formalizing the famous BSD conjecture. And maybe the biggest table of all in mathematics
321160	326760	is the OEIS, online encyclopedia of mathematical sequences. So, there's hundreds of thousands
327160	332200	of integer sequences, and every day, I think mathematicians discover unexpected connections,
332840	340520	or maybe rediscover an existing connection. I myself use the OEIS. If there's a quantity,
340520	343960	which I know there's a formula for, but I can't remember it, I can just compute the first five
343960	351400	elements put in the OEIS, I can usually find it. And most recently, people are starting to use
351480	356520	large databases of mathematical objects as training data for neural networks, and so I'll
356520	365800	give you an example of that later. So, that's one very storied and antique way of using computers
367320	371880	in mathematics. The other big use, of course, is in numerics or scientific computing,
372520	379240	and that's also a very old subject. Arguably, the first big scientific computation was in the
379240	386280	20s when Lorenz was asked to model the fluid flow for the construction of a new dike in the
386280	392360	Netherlands, and so he assembled a team of human computers, basically, to model what would happen
392360	399080	to the water flow and so forth. It's notable for the introduction, he's almost the first place
399080	404040	where floating point arithmetic was introduced. And, of course, we use scientific computing
404040	411080	nowadays to model PDEs, to solve large systems of equations, and, of course, we use them for
411080	416920	computer algebra packages, you know, magma, maple, sage, and so forth. Yeah, you want to do a big,
417880	422440	you know, numerical integration or algebraic, you know, computer Gribner bases, whatever,
423640	430440	you know, we routinely do this now, or cross mathematics. Of course, the numerics are sometimes
430440	436920	inaccurate, you know, there are round off errors and other possible problems, but there are ways
436920	440760	to make the combination more rigorous. For example, instead of floating point arithmetic,
440760	445960	if you use interval arithmetic, if you represent numbers by error ranges, a lower and upper bound,
446840	452360	and you keep those bounds like rational numbers, like finite precision, like infinite precision,
453080	459320	then you can avoid errors, at least in principle, at the cost maybe of
460920	466600	making the runtime longer. More recently, there are more advanced
468680	473720	algebra packages than just sort of the standard things you get in sage or Mathematica.
475480	481160	They think of SAT solvers, satisfiability solvers, or satisfiably modulo theory solvers,
481160	488200	SMT solvers. So what they, so a SAT solver gives, you feed it a whole statement, a bunch of
488200	495000	propositions, P1, P2, P3, and so forth, and you see that P1 and P2, all P3 is true, P3 and P4,
495000	500120	and not P5, one of them is true, and so forth, and it will try to see if there's a solution or not,
500840	505480	and many problems can be phrased as a SAT problem. So if you have a general purpose SAT solver,
506360	512760	you can potentially just feed it into such a program and solve the problem for you,
513480	518120	then there are these more sophisticated variants, SMT solvers, where you also feed laws of algebra
518760	526680	so you have some variables, and you assume that there are certain laws, these variables
526680	530200	obey certain laws, and you ask, can you deduce a new law from the laws you already have?
531640	536360	So those are potentially very powerful, and unfortunately, SAT solverability is an NP
536360	542120	complete problem, and once you get hundreds and hundreds of these propositions, it becomes very
542120	548760	hard to actually solve these problems, but still they are very useful. Here's a typical
548760	556520	application of a SAT solver, so a few years ago, there was this famous problem in commentaries
556520	561640	called the Boolean Pythagorean Triples problem, and so the problem is this, you take natural numbers
561640	568440	and you color them into two color classes, red and blue, and you ask, is it always the case that
568440	573000	one of these color classes contains a Pythagorean triple, three numbers A, B and C such that A
573000	580680	squared plus B squared equals C squared, and it turns out to be, we don't have like a human proof
580680	585880	of the statement, but we know it's true now because of a SAT solver, so there was a massive
585880	594440	computation that says that if you only go up to 7,824, then you can't do this, there was a way
594440	599640	to partition the numbers from 1 to 7,824 into two classes, neither of which contain a Pythagorean
599640	606280	triple, but once you go up to 7,825, no matter how you do it, you must always get one of the
606280	611640	two color classes must have a Pythagorean triple. In principle, this is a finite computation
611640	618680	because there's only two to seven, two to five different ways to compute different partitions,
618680	623320	and so you just check each one, but that is computationally unfeasible, but with a SAT
623320	630760	solver, you can rephrase this problem as a free satisfiability problem, and it's not just a matter
630760	634280	of running the solver, you have to optimize it and so forth, but it is possible to actually solve
634280	640120	this problem, and it gives you a certificate, it gives you a proof, and actually this is,
640120	646040	at the time, it was actually the world's longest proof. The proof certificate, first of all,
646040	652120	it took four CPU years to generate, and it's a 200 terabyte proof, although it is compressible.
652840	655400	I think it is still the second largest proof ever generated.
657160	663240	Okay, so that's, but this I still consider is a more classical way of using computers,
665400	672440	but what I think is exciting is that there are a lot of new ways that we can use computers
674440	678600	to do mathematics. Of course, there's still the boring ways, you know, we use computers to do
678600	686520	emails and write latex and so forth, I don't mean that, but there are sort of three new modalities,
687160	693320	which individually, they still have somewhat niche applications, but what I find really
693320	697320	interesting is that they can potentially be combined together, and the combination of them,
697320	703160	it could be something in general purpose that actually a lot of us could use. So the three sort
703160	709960	of new things. So the first is machine learning algorithms, where you have a problem, and if
709960	714280	you have a lot of data for that problem, you can set some sort of specialized neural network to
714280	717800	train it on the data, and it can generate counter examples for you, try to generate connections,
718760	721720	and so people are beginning to use this in all kinds of fields of mathematics, I'll give you
721720	730600	some examples later. So that's one development. Maybe the most high-profile development is
730600	735080	large language models like chat GPT, these are very general purpose models that can understand
735080	741560	natural language. To date, they have not been directly used for so much mathematics, I'm sure
741560	746200	many of you have tried talking to GPT, asking it to solve your favorite math problem, and it will
746200	755080	give you some so plausible looking nonsense in general. But when used correctly, I think they
755080	761160	do offer a lot of potential. I mean, I have found occasionally that these models can be useful for
761160	768200	suggesting proof techniques that I wasn't initially thinking of, or to suggest related topics or
768200	775400	literature. They're actually most useful for sort of secondary tasks. Okay, so for actually doing
775400	779480	math research, they still haven't really proved themselves, but for doing things like writing
779480	786360	code or organizing a bibliography, like a lot of the other more routine tasks that we do actually,
786360	794760	these LLMs are very useful. But the third new technology, which has been around for two decades,
794760	799160	but has only now sort of becoming ready for primetime, are these formal proof assistants,
799960	806600	which are languages designed to verify, or to verify, or many of them are actually designed
806600	813160	to verify electronics, but they can also verify mathematical proofs. And crucially, they can
813160	818440	also verify the output of large language models, which they can complement, they can fix the biggest
818440	827800	defect in principle of the LLMs. And they allow new types of ways to do mathematics, in particular,
827800	831800	they can allow really large scale collaborations, which we really can't do without these formal
831800	837080	proof assistants. And they can also generate data, which can be used for the other two
838840	843160	the other two technologies. So I'll talk about each of these three things separately, but
846040	849560	but they haven't, there's beginning to be experiments to combine them together,
850760	856120	but they're still kind of prototypes right now. But I think the paradigm of using all of them,
856120	861080	and also combining with the computer algebra systems and the SAP solvers into one integrated
861080	867320	package, it could really be quite a powerful methodical assistant. Okay, so let's talk,
867320	876440	I think my first slides begin with proof assistants. So the computer-assisted proofs are not new,
877320	882760	famously the full color theorem in 1976 was was proven partly by computer. Although at the time,
882760	889000	it was by modern standards, we will not call it a fully formalized proof, the 1976 proof.
889400	895720	The proof was this long document with lots and lots of subclaims, which
897160	902040	a lot of them were verified by hand, and a lot of them were verified by both electronic computers
902040	906680	and human computers. And I think one of the author's daughter actually had to go through 500 graphs
906680	912920	and check that they all had this discharging property. And actually, it had a lot of mistakes too.
912920	920120	So there's a lot of minor errors in the proof. They're all correctable, but it really
920840	925800	will not meet the standards today of a computer-verified proof.
928200	934920	The first proof, okay, so it took 30 to 20 years for an alternative proof of full color theorem
934920	941720	to be verified, and this proof is closer to being completely formal. So it's about 10-15 pages
941720	947160	of human readable argument, and then it reduces to this very specific computation, which anyone
947160	953800	can just run a computer program in whatever language they like to verify it. So it was a
953800	959880	computer-verified proof, but it still wasn't a formal proof. It wasn't written in a formal
959880	966520	proof language, which was designed to only output correct proofs. And that had to wait until the
966520	972120	early 2000s when Werner and Gontier actually formalized the entire full color theorem in one
972120	980040	of the early proof assistant languages, COC, in this case. So now we know with 100% certainty
980040	985720	that the full color theorem is correct. Well, modulo, trusting the compiler of COC.
986520	996440	All right. Another famous machine assistant proof, well, actually initially human proof,
996440	1003080	but eventually computer-verified was the proof of the coupler conjecture. So the coupler conjecture
1003080	1007080	is a statement about how efficient that you can pack unit spheres in the plane,
1008200	1013240	and so there's a natural way to stack unit spheres, and it's the way that you see oranges stacked in
1013240	1017960	the grocery store. It's called the hexagonal closed packing, and there's also a dual packing
1017960	1022760	with the same density called the cubic closed packing. And they have a certain density, pi over
1022760	1029480	three over two, and this was conjectured to be the densest packing. So this is an annoyingly hard
1029480	1038520	statement to prove. So it's an optimization problem in infinitely many variables. So each
1039480	1042520	each sphere has a different location, and there's an infinite number of spheres.
1043080	1047320	So you're trying to prove an inequality involving an infinite number
1047320	1053880	of variables involving solving infinite number constraints. So it doesn't immediately
1053880	1060840	lend itself to computer verification. But even in the 50s, it was realized that possibly this
1060840	1066920	could be done by some sort of brute force computation. And so Toth proposed the following
1066920	1072040	paradigm. So every time you see a packing, it comes with what's called a Voronoi decomposition.
1072040	1077160	So every sphere comes with a Voronoi cell, which is this polytope of all the points that are closer
1077160	1081080	to the center of that of that sphere than to all the other spheres. And there's partitions
1081800	1087720	space into all these little polyhedron, these Voronoi cells. And there are various relationships
1087720	1092280	between the volumes of these different cells. There's only so many spheres that you can pack
1092280	1095960	next to one reference sphere, and this creates all these kind of constraints.
1097800	1104120	And so the hope was that if you could gather enough inequalities between adjacent Voronoi cells,
1104840	1109480	the volumes of adjacent Voronoi cells, maybe every such system inequalities, in principle,
1109480	1115320	gives you an upper bound on the density of the sphere packing. And in principle, if you get
1115320	1119720	enough of these inequalities, maybe you could actually get the optimal bound of three or two.
1120600	1127640	So people tried this approach for many, many years, including some false attempts.
1128280	1131480	But they were not able to actually make this approach work.
1134040	1139000	But Thomas Hales, and then later, with a collaborator, was able to adapt this approach
1139000	1144760	to make it work in a series of papers from 94-98. But they had to modify the strategy quite a lot.
1144840	1149880	So instead of using the Voronoi decomposition, there was a more complicated decomposition
1149880	1154040	that was used. And instead of using volume, they had to define this new score function
1154680	1158600	attached to each polyhedron. But basically, the strategy was the same.
1159720	1164280	And he was able to prove lots and lots of linear inequalities between the scores of adjacent
1164280	1170520	polyhedra. And then just using linear programming was able to then get a bound. And
1171320	1175240	with the right choice of score and the right choice of partition, it was the optimal bound.
1177720	1182040	It's a very flexible method, because you have lots of ways you can do the partition and lots
1182040	1187480	of ways that you can do the score. But the problem is that it was too flexible. So here's a quote
1187480	1192840	from Hales. It says that Simon Ferguson, who was Hales' collaborator, and I realized every time we
1192840	1195720	encounter difficulty solving the minimization problem, we get just a scoring function to
1195720	1199000	score the difficulty. The function became more complicated, but with each change,
1199000	1203880	we could cut months or years from our work. This incessant fiddling was unpopular with my
1203880	1207720	colleagues. Every time I presented my work in progress at a conference, I was minimizing a
1207720	1211960	different function. Even worse, the function was moderately incompatible with earlier papers,
1211960	1219240	and this required going back and patching the earlier papers. So the proof was a mess, basically.
1220280	1225720	They did eventually finish it in 98, and they were able to derive the Kepler conjecture from
1225720	1230840	a linear programming computation from a very complicated optimization program. Initially,
1230840	1235880	it was done by hand, but with the increased complexity, there was no choice but to make
1235880	1243000	it more and more computer-assisted. So when the proof was announced, it was a combination of
1243000	1249400	250 pages of notes and lots and lots of gigabytes of programs and data, and it was famously hard
1249400	1254120	to referee. It took four years for annals to referee the paper with a panel of pro referees,
1254200	1257640	and even then, the panel was only 99 percent certain of the correctness of the proof, and they
1257640	1265640	could certify the corrections, the calculations. Because, I mean, in principle, it was all doable,
1265640	1271160	but the referees have to implement all these different computer calculations themselves,
1271960	1278360	but it was eventually accepted. But clearly, there was this big asterisk. There was a lot of
1278360	1281800	controversy about whether this was really a valid proof, and so this was one of the
1282600	1289480	the first really high-profile uses of formal proof assistance, because this was a result in which
1289480	1297080	there was serious doubt about the correctness. So they created, so Hales in 2003 initiated a
1297080	1302840	project to write down this massive proof in a completely formalized way so that a standard
1302840	1307800	proof assistant could verify it. He estimated it would take 20 years to make this work,
1308520	1315000	and so he had, he gathered 21 collaborators. It actually only took 11 years,
1317240	1322200	but yeah, eventually what they did was that they first created a blueprint,
1322200	1325640	you know, so a human readable version of the proof breaking things up into very,
1325640	1330600	very small steps, and then they formalized each step by bit, and it was finally done,
1330600	1334680	and then there was a, yeah, so they published a paper about the formalization, and that only
1334680	1341000	appeared in 2017. So this was sort of the state-of-the-art of formalization, you know,
1341000	1345880	as of say 20 years ago, you know, like it was possible to formalize big complicated results,
1345880	1354120	but it took an enormous amount of effort, you know, not something which you would do routinely.
1356120	1359720	There was a more recent effort in a similar spirit by Peter Schultzer,
1359720	1365160	he called it the liquid tensor experiment. So Schultzer introduced this theory of condensed
1365160	1370440	mathematics, which is, all right, this is really far from my own area of expertise, but
1371800	1376840	basically there are certain problems with, so certain types of mathematics you want to work
1376840	1380360	in various categories, like categories of topological obedient groups and topological vector spaces,
1380920	1384680	and there's a problem that they're not obedient categories, that they don't, they don't have,
1384680	1387800	I have a good notion of kernel and cold kernel, and things don't work out properly.
1388520	1394280	So he proposed replacing all of these standard categories with a more fancy version called
1394280	1401160	a condensed category, which has better category theoretic properties, and so the hope is that
1401160	1406840	you could use a lot more high-powered algebra to attack, to handle things with topological
1406840	1410040	structure or analytical structure, like function spaces, for example, binoc spaces.
1412760	1417160	But in order for you to work, there's a certain vanishing theorem, which I've written there,
1417720	1423080	but I cannot explain to you. Okay, so there's a certain category,
1426760	1430760	condescending groups, and there's an x-group involving p-binoc spaces that has to vanish,
1432680	1437160	and this vanishing theorem is needed in order for all of the rest of the theory to actually be
1437160	1445560	useful, if you want to apply it to function analysis in particular. And so Schultzer,
1445560	1449560	what a blog post about this is that, you know, I spent much of 2019 obsessed with the proofless
1449560	1453560	theorem, almost getting crazy over it. In the end, we were able to get an argument down on
1453560	1457080	paper, but I think no one else has ever dared to look at the details of this, and so I still
1457080	1464360	have some lingering doubts. With this hope, with this theorem, the hope that the condensed formulas
1464360	1469320	can be fruitfully applied to function analysis stands a force, being 99% sure is not enough
1469400	1477240	because this theorem is of the most fundamental importance. He says, I think this may be my most
1477240	1481720	important theorem to date, which is a really big claim, actually. Better be sure it's correct.
1482520	1487720	So this was another case where there was a great desire to formalize the proof.
1489560	1495400	So he asked publicly on the internet for help to formalize this in a modern
1495400	1502280	proof of this in language called Lean. And so again, about 20 people, I think, joined this
1502280	1512040	project to help out. And it, so Lean is based on, it has this philosophy where it has this
1512040	1517240	single huge library of mathematical theorems, like the fundamental calculus or the classification
1517240	1521480	of finite building groups, like a lot of the basic theorems of mathematics are already formalized
1521480	1525640	in this big library. And the idea is to just keep building on this library and adding to it
1525640	1533480	with additional projects. But one of the problems with that short of the face was that a lot of the
1534360	1537320	tools that, basic tools that you needed, like homological algebra and chief theory and top
1537320	1541080	box theory, weren't actually in the library yet. So actually part of what they had to do was actually
1541080	1547160	set up the foundations of that theory and formalize it in the library first. But it basically was
1547160	1554200	done in about two years. In one year, they formalized a key sub theorem. And then the whole
1554200	1562760	thing was formalized about a year afterwards. It did have some, in addition to sort of making
1563320	1569720	reassuring Peter that it was, everything was correct, there was some other minor benefits.
1569720	1573080	So first of all, there were actually a few minor errors in the proof that were discovered
1573080	1578680	in the formalization progress. But they go refixed also some small simplifications.
1579320	1586200	There was one big simplification actually. So they, the original proof used something very
1586200	1591160	complicated, which I also don't understand, called the Breen-Deline resolution. But in the course
1591160	1595640	of formalizing it, it was too hard to actually formalize this theory. But they found that there
1595640	1599720	was a weaker version of this theory, which was good enough to formalize this application. But
1599720	1604520	this was actually a major discovery because this weaker theory could also potentially attack
1604520	1610440	some other problems that the Breen-Deline resolution is not well suited for. And now
1610440	1617080	the math library is much, much better in, it has a lot of support for homological
1617080	1621560	algebra and sheath theory and lots of other things, which helped other formalization projects
1621560	1627320	become easier. I got interested in this formalization a few months ago.
1627400	1636120	So, oh, hang on. I should say, like with the Kepler experiment, so you don't just directly
1636120	1644360	transfer a paper from, you know, the archive or something into Breen. What we found is that
1644360	1648680	it really helps to create first an intermediate document. So halfway between a human readable proof
1648680	1658120	and a formal proof, which we call a blueprint. So it looks like a human proof and is written in a
1658680	1665640	version of latex. But like each step in the proof is linked to a lemma in lean. And so it's very
1665640	1672280	tightly integrated. It has a very nice feature. It can create dependency graphs, which I'll show you
1672280	1676120	an example later. You can see which parts of the proof have been formalized, which ones are not,
1676120	1682520	and what depends on what. It actually, it gives a lot of structure to the process of writing a paper,
1682520	1686760	which, you know, right now we do by hand without sort of much assistance, really.
1689160	1692680	Yeah, Schultz has said that the process of writing the blueprint really helped him understand the
1692680	1701240	proof a lot better. And actually, people have been also going the other way. There's also software
1701240	1704200	that takes formal proofs, which are written in something that looks like computer code,
1704200	1711320	and turns them back into human readable proofs. Here is a prototype software. So there's a
1711320	1718040	there's a theorem in topology. Okay, you know, I think it's a if a function is continuous on a dense
1718040	1722840	set, and there's some extra extra properties, then it's continuous extends to a continuous
1722840	1728600	function globally. And the proof is written was written first in lean, but then it was
1728680	1733240	already converted into a human proof, human readable proof. But again, where every step
1733240	1737080	you can expand and contract, like if there's a step you want to explain more, you can double
1737080	1741240	click it, and it will expand out, give all the justification. And you can click at any given
1741240	1745880	point in the proof, and it will tell you what the hypotheses are currently, what you can approve,
1745880	1752520	and you can give a lot of structure. I think eventually textbooks, this is maybe a good format,
1752520	1756760	say for undergraduate textbooks to have, you know, proofs of say, you know, tricky
1756760	1761960	films and analysis or something written in a way where, you know, in a not in a static way,
1761960	1767320	but where you can really, you know, drill down all the way down to the basic axioms if you wanted to.
1770600	1779400	Okay. One thing I mean, one thing notable about these formalization projects is that they allow
1779400	1783960	massive collaborations. So, you know, in other sciences, people collaborate with 20 people,
1783960	1790600	500 people, you know, 5,000 people. But in mathematics, still, we don't really collaborate
1790600	1797160	in really large groups. You know, five is kind of the maximum, usually. And part of it is that,
1797160	1801160	you know, if you want to collaborate with 20 people, if you don't already know these 20 people,
1801160	1807960	and you don't completely trust that they're doing correct mathematics, you know, it's very difficult
1807960	1813160	because you have to, everyone has to check everyone else's contribution. But with a proof
1813160	1818120	assistant, the proof assistant provides a formal guarantee. And so, you can really collaborate
1818120	1823640	with 20, hundreds of people that you've never met before. And you don't need to trust them.
1824440	1831720	Because they upload code and the lean compiler verifies, yes, this actually solves what they
1831720	1838280	claimed. And then you can accept it or it doesn't. So, I experienced this myself because I got
1838360	1843240	interested in formalization a few months ago. So, I'd recently proven a combinatorial theorem
1843960	1848680	with Tim Gowes, Ben Green, and Freddie Manners. It solved something called the polynomial
1848680	1854280	primonrugia conjecture. The precise conjecture is not important for this talk. It's a conjecture
1854280	1859720	in combinatorics. You have a subset of a finite through vector space of small doubling. And you
1859720	1864680	want to show that it is all, it is very close to actually being a coset of a subgroup. This was a
1864680	1873000	statement that was in combinatorics that was highly sought after. So, we had a 33-page paper
1873880	1879320	recently proving this. Mostly self-contained, fortunately. So, I thought I was a good candidate
1879320	1886280	for formalization. So, I asked on an internet forum, specializing in lean, for help to formalize it.
1886280	1892520	And then, again, like 20, 30 people joined in and actually only took three weeks to formalize.
1892920	1896280	The time taken to formalize these projects is going down quite a bit.
1898600	1903560	So, in particular, the time taken to formalize this project was roughly about the same time
1903560	1908120	it took for us to write the paper in the first place. And by the time we submitted the paper,
1908120	1911880	we were able to put as a note that the proof is actually being formalized.
1913880	1918520	So, as I said, it uses a single blueprint which splits up the proof into lots and lots of little
1918520	1927000	pieces. And so, it creates this nice little dependency graph. So, this is a picture of
1927000	1931320	the graph at an early stage of the project. So, there's no down the bottom called PFR. Maybe
1931320	1934280	only the people in the front can see it. That's the final theorem that we're trying to prove.
1935160	1939000	At the time of the screenshot, this was white, which means that it had not been
1940040	1944280	proved, but not even been stated properly. So, in fact, even before you prove the theorem,
1944280	1948200	you have to first state it, and then you have to make definitions and so forth.
1949240	1953320	Blue are things that have been defined, but not yet proven, and green have been things that have
1953320	1961400	been proven. And so, at any point in time, some bubbles are white, some are blue, and some
1961400	1967560	results depend on some others. And so, the way the formalization proceeded was that different
1967560	1972760	people just grabbed a note that was ready to be formalized because maybe all the previous results
1973240	1977720	that it depended on were formalized, and they just formalized that one step independent of
1977720	1983880	everybody else. And you didn't really need to coordinate too much. I mean, we did coordinate
1985000	1990760	on an internet forum, but each little note is completely specified. There's a very precise
1990760	1998280	definition and a very precise thing to prove, and we just need the proof. And we really don't care
1998360	2004120	exactly what the proof is. I mean, it has to be not massively long. So, people just picked up
2004120	2008440	individual claims. Here's one very simple claim. There's a certain functional called
2008440	2014280	ruja distance d, and there's a very simple claim that it was non-negative. And this turns out to
2014280	2023480	have a three-line proof, assuming some previous facts that were also on the blueprint. And so,
2023480	2028760	people just sort of picked up these things separately, and over time, it just filled up.
2028760	2032840	This is what a typical step in the proof looks like. This is the proof that this ruja distance
2032840	2038520	is non-negative. This is the code in the lean. It looks kind of a little bit like mathematics,
2039160	2045320	but it is actually... Once you think of the syntax, it actually reads... It's like reading
2045320	2048280	latex. The first time you read latex, it looks like a whole bunch of mess of symbols. But
2049160	2056040	it's actually... Every line corresponds to a sentence in mathematical English. For example,
2056040	2058440	the first line, if you want to prove that this distance is positive,
2058440	2062120	it suffices to prove that tricidistance is positive. So, you work with tricidistance,
2062120	2067400	because it turns out there's another lemma that bounds tricidistance. So, yeah, every step,
2067400	2074600	actually, it corresponds reasonably well to the way we think about mathematical proofs.
2075560	2083640	Yeah. So, fortunately, the proof didn't reveal any major issues. There were some typos,
2083640	2089880	but very, very minor picked up. And we also... There were some things we needed... Again,
2089880	2094680	we needed to add to the math library. The math library... We used Shannon entropy,
2094680	2100520	and Shannon entropy was not in the math library. Now it is. One thing about formalization is that
2100520	2107400	it still takes longer to formalize proofs than to write them. But... And maybe about 10 times
2107400	2115320	longer, I would say. Which is unfortunate. If it was faster to formalize to write proofs formally
2115320	2120920	than to write them by hand, I think then there will be a tipping point. And then I think a lot
2120920	2127160	of us would switch over just because they guarantee a correctness. So, we're not there yet. It is
2127160	2131640	definitely getting a lot faster than it was before. But one thing that we noticed is that,
2131640	2136840	actually, while it still takes a long time to write a proof, if you want to modify a proof
2136840	2143160	to change some parameters and make it a little bit better, that can be done much quicker in the
2143160	2148840	formal setting than with a paper proof. Because with paper proof, if you change all your little
2148840	2153160	parameters and so forth, you'll likely make also mistakes and you go back. And it's a very annoying
2153160	2158920	process. But it's actually much easier to modify an existing formal proof than to create one from
2158920	2164120	scratch. For example, we were able... There's a constant 12 exponent that appeared in our proof.
2165560	2171880	A few days afterwards, someone posted an improvement in the argument, improved 12 to 11,
2171880	2175320	and in a few days we were able to adapt the formal proof to do that as well.
2175480	2185720	Yeah. And because you can collaborate, I think the process is scalable. There was just recently Kevin
2185720	2190120	Buzzard, for example, a five-year project. The aim is to formalize Fermat's last theory of the proof.
2191480	2197640	And that is quite a big goal because there are lots and lots of sub-portions that have had no
2197640	2202680	formal proof at all. So, that, I think, will be quite an ambitious project. But I think it's now
2202680	2210360	doable. It wasn't doable five years ago, but now I think it is. Okay. So, that's four more proofs.
2211160	2218440	Another... Okay. I might actually have to speed up a little bit. Okay. So,
2220280	2227160	all right. So, machine learning, using neural networks has become also more and more common
2227160	2230760	place in different areas of mathematics. I think I'll skip over this one. So,
2231560	2237320	one place where it is being used is in PDE to construct candidate approximate solutions for
2237320	2240840	various problems. So, like... So, there's a famous problem in fluid equations, you know,
2240840	2243800	do the Navier-Stokes equations before we find our time? Do the Euler equations before we find our
2243800	2248760	time? Navier-Stokes is still challenging, but Euler, there's been a lot of progress recently
2249480	2253080	that people have been starting to construct self-similar solutions to the Euler equations
2253080	2258840	with some asterisks, but the asterisks are slowly being removed. And one of the strategies of proof
2258840	2263720	is to first construct an approximate solution, approximately self-similar solution that looks
2263720	2268120	like it's going to blow up and then use some rigorous perturbation theory to perturb that to an
2268120	2275320	actually blowing up solution. And machine learning has turned out to be useful to actually generate
2275320	2281640	these approximate solutions, but I'm going to skip that for lack of time. I'll tell you,
2281640	2289480	my favorite application of machine learning to mathematics is in knot theory. So, this is a
2289480	2296200	recent work. So, knots are a very old subject in mathematics, and there's lots of... One of the
2296200	2300280	fundamental things you study in knots is knot invariance. So, there's various statistics you
2300280	2306360	can study. You can assign to a knot, which depends on the topology of the knot or the geometry of the
2306360	2311640	knot. So, there's something called the signature, which is a combinatorial invariant. There are
2311640	2315800	these famous polynomials, like the Jonas polynomial and Homfly polynomial. Then there are these things
2315800	2320520	called hyperbolic invariance. The complement of a knot is often a hyperbolic space, and then you
2320520	2324920	can talk about the volume of that space and some other geometric invariance. And so, there are
2324920	2330280	these massive databases of knots. You can generate millions of knots, and you can compute all these
2330280	2335880	invariance. But they come from very different areas of mathematics. So, some knot invariance
2335880	2340760	come from combinatorics, some come from operative algebras, some come from hyperbolic geometry,
2340760	2351080	and it's not obvious how they're related. But what these mathematicians did was that they trained
2351080	2354920	a neural network on this big database of knots, and they studied the hyperbolic invariance and
2354920	2359400	the signature, which is the combinatorial invariant. And they found that you could train the network
2359480	2363720	to predict the signature from the hyperbolic invariant with, like, 99 percent accuracy.
2365000	2369800	So, they used, like, half the data to train the set, and then half the data to test the set,
2370520	2377480	to test the neural network. And so, what this told them is that there must be some connection.
2377480	2382680	The signature of a knot must somehow be a function, or at least very closely related to a function,
2382680	2388360	of a hyperbolic invariance. But the problem with neural nets is that they give you a function,
2388440	2391160	a functional relationship, or at least a conjectural functional relationship. But
2391160	2396920	it's this massively complicated function. It composes hundreds and hundreds of nonlinear
2396920	2400680	functions together, and it doesn't give you any insight as to what the relationship is. It just
2400680	2407080	shows you that there is a connection. But it's possible to do some analysis. So, they actually
2407080	2411720	tried a very basic thing which happened to work. It's what's called a saliency analysis. So,
2411720	2417800	this neural network gave them this function. Basically, they fed in 20 hyperbolic invariances
2417960	2422760	as input, and the signature as output. So, it's basically a function from R to 20 to the integers,
2422760	2427000	I think. And what they decided to do is that, once they had this function, they just tested
2427000	2432440	how sensitive the function was to each of its inputs. So, they just varied one of the 20 variables,
2432440	2437640	and they saw what happened to the output. And what they found was that only three of the 20
2437640	2441960	variables were actually important, that only three of them had a significant influence on the function.
2441960	2447480	The other 17 were basically not relevant at all. And so, they focused on those three variables,
2447480	2454360	and they started plotting this function just restricted to those three variables. And that's
2454360	2457960	just low enough to mention that you can eyeball the relationships. So, they started plotting
2458600	2463160	the signature as a function of two or three of these variables and using color and so forth.
2463160	2468760	And they started seeing patterns, and they could use these patterns to conjecture various
2468760	2474280	inequalities and various relationships. It turns out that the first few inequalities they conjectured
2474360	2483720	were false, and they could use the new net and the data set to confirm this. But with a bit of
2483720	2489160	back and forth, they were able to eventually settle upon a correct conjecture relating these
2489160	2499800	invariants to the signature. And it wasn't the invariants that they expected. So, yeah, they
2499800	2503160	were expecting the hypervolume, for example, to be really important and not to be relevant at all.
2505000	2509000	But once they found the right variables and the right conjectured inequality,
2509640	2514120	it suggested the proof, and then they were able to actually find a rigorous proof of the inequality
2514120	2519240	that they conjectured. So, it was a very nice back and forth between using the machine learning
2520600	2527880	tool to suggest the way forward, but then going back to traditional mathematics to prove things.
2528840	2536280	Okay. So, of course, the most high-profile development these days has been large language
2536280	2543720	models like GPT. And sometimes they work really, really well. So, here's an example of GPT-4,
2543720	2548280	which is OpenAI's most advanced large language model, actually solving a problem from the
2548280	2552920	IMO, the International Mathematical Olympiad. And so, it's a question, you know, there's a function
2552920	2558440	that obeys a search function equation. Can you prove, can you solve for the function?
2558440	2564280	And it actually happens to give a completely correct proof. Now, this is an extremely cherry
2564280	2570920	picked example. I think they tried, from this paper, they tried all the recent IMO problems,
2570920	2575640	and they could solve like one percent of the problems of this method. You know, famously,
2575640	2581160	it's bad even at basic arithmetic. You know, there's a, you ask it to solve seven times four
2581160	2585400	plus eight times eight, and it'll give you the wrong answer. It gives you 120. And then it will
2585400	2590200	keep going and say, and I'll explain why. And during the course of the explanation, it will
2590200	2596520	actually make a mistake. And, yeah, and then you point out that they made a mistake and say,
2596520	2602680	I'm sorry, the previous answer is incorrect. I mean, these large language models, they remind
2602680	2607160	me a lot of, you know, if you have sort of a somewhat weak undergraduate student in office hours,
2607160	2612520	and you ask them to solve a problem at the blackboard with no, with no age, you know,
2612520	2617080	it will, you know, he or she will try to try their best, you know, and try to turn something
2617080	2622520	that looks like a proof. But, yeah, they don't really have a good way of correcting themselves.
2624360	2628200	So, you know, sometimes they work really well, but often they're very, very unreliable.
2630520	2636680	But there's lots of interesting recent experiments where you can couple these language models to
2636680	2641880	other much more reliable tools to do mathematics. So, for example, GPT now comes with plugins for
2641880	2649240	Wolfram Alpha. So, now, if you ask GPT to do an ethnic calculation, it knows better than to try
2649240	2657720	to do it itself. It will outsource it to Wolfram Alpha. Then there's more recent examples where
2657720	2663240	people are coupling these large language models to a proof error file like Lean. So, you know,
2663320	2668360	you ask it to prove as a statement, you know, prove that the union of two open sets is open.
2669400	2673400	If you ask a raw large language model, it will give you a statement that
2673400	2677640	a proof that looks like a proof. But there's lots of little logical errors in the proof, but
2678440	2683560	you can force it to output in Lean, get Lean to compile, and if there's a completion error,
2683560	2687080	it will send back the error to the large language model and have to correct it and
2687080	2691320	create a feedback loop. And it can actually be used to solve
2692440	2697560	roughly sort of undergraduate math homework level problems by this technique. But now,
2697560	2702520	with 100% guarantee of accuracy, if it works, I mean, of course, sometimes it will just get stuck
2702520	2708360	and give up because it can never get the Lean compiler to accept the argument. But
2708600	2718360	it is beginning to make some headway. As I said before, I do find it can be useful
2719480	2726360	as a muse, kind of like if you're just starting on a project, I recently was trying to
2729080	2734520	prove some commentary identity. And I was thinking of using, I had some ideas in mind,
2734600	2740120	I was going to use asymptotics, work with some special cases, but nothing was working. And I
2740120	2744040	asked GPT for some suggestions. And it gave me some suggestions I was already thinking of,
2744840	2751480	plus some suggestions that were either completely vacuous or wrong. But it did tell me that you
2751480	2756680	should probably use generating functions, which I should have known. But at the time,
2756920	2764600	it escaped me. And just with that hint, I was able to actually get them to work.
2766680	2772280	So, you know, I mean, as just kind of someone to double check your thought process,
2772840	2777240	it is sort of useful. Still not great, but it has some potential use.
2779560	2785800	There was another tool which I do like, and I use more and more now. It's integrated into
2785880	2789720	something called VS code, which is an editor to write code that can also be used for latex,
2790280	2795240	something called GitHub co-pilot. It's basically an AI-powered autocomplete,
2796040	2802040	and you type in your code or your latex, whatever. And based on all the text that you've
2802040	2808360	already written, it will suggest a possible new sentence to generate. And so it can be very useful
2808360	2812600	for code. You write down three lines of code, and it's just the fourth. And sometimes you'll get
2812600	2817720	exactly right. Sometimes it's not exactly right. Sometimes it's complete rubbish. But then you
2817720	2821240	can accept it, and it can save a lot of time, especially if you're doing some very menial
2821240	2826360	code where you're repeating something over and over again. It works for latex. I wrote
2827880	2832040	a latex blog post, actually, recently, where I was trying to estimate an integral, and I broke
2832040	2835720	up the integral into three pieces. I said, okay, the first piece I can estimate by this technique,
2835720	2839720	and I wrote down how to estimate this technique. And then the co-pilot just suggested how to
2840440	2845320	estimate the second term. And actually, a completely correct argument. It was a modification of what
2845320	2852760	I had already written. So it's very good at just modifying text that you've already appeared.
2854600	2862280	And it's slowly being integrated into prefixes like Lean. So to the point where one line proves,
2862280	2866440	two line proves, we can kind of get the AI to fill in for us. The kind of steps that that
2867400	2873160	this is obvious or clearly this is true in a paper proof. I mean, not all of them, but we can get
2873160	2877000	to the point where the AI can fill in a lot of them, and that will make proof formalization a lot
2877000	2885960	faster. So there's a lot of potential. A lot of these technologies are very, very close to prime
2885960	2892760	time, but not quite. It still took me a month to learn Lean and so forth. They're still not
2892760	2897800	completely usable out of the box, but they are beginning to be more and more useful. And in
2897800	2902360	surprising areas, you wouldn't have expected, say, not theory to benefit from these tools,
2902360	2908280	but they do. They can't solve math problems on their own, except maybe undergraduate level
2908280	2913800	homework questions, maybe, is the current level. But as an assistant, I think they can be very,
2913800	2919560	very useful. They can generate conjectures. They can uncover connections that you wouldn't
2919560	2929080	normally guess. Once proof automation becomes easier and scales better, we may be able to do
2929800	2936920	completely new types of mathematics that we don't do right now. Right now, we prove
2936920	2941240	theorems one at a time. I mean, we're still kind of craftsmen. We take one theorem and
2941240	2945080	we prove it, and we take another thing and we prove it. Eventually, you could automate
2945880	2952520	exploring the entire theorem space of millions of different statements, which ones are true,
2952520	2957160	which ones are obviously false, and you could explore the geometry of theorems themselves.
2958600	2964920	I think we're going to see a lot of different ways to do mathematics, and we're going to
2964920	2971800	see a lot of different ways to make connections in fields that we don't currently. And it'll be
2971800	2975320	a lot easier to collaborate and work in different areas of mathematics,
2978680	2983960	yeah, especially because you can use these tools to sort of compartmentalize all these tasks,
2983960	2988920	all these big, complicated projects into small pieces. And plus, also, these large language
2988920	2995240	models actually will become very good at getting humans up to speed on any number of advanced
2995240	3005640	mathematical topics. Okay, oops. Yeah, but it's still not quite there yet. I would say,
3006200	3010440	if, for example, we prove formalization, it still takes about 10 to 20 times longer to
3010440	3015880	formalize a proof than to do it by hand, but it's dropping. And I see no reason why this
3015880	3023400	ratio cannot drop below one. Eventually, it will become faster to, eventually, when we just explain
3023480	3028680	our proofs to GPT. And GPT will generate, you know, it will ask questions whenever we're unclear,
3028680	3033240	but then it will just generate the latex and the lean for us. And we, you know,
3033240	3040840	eventually, and, you know, and check our work at the same time. So I think this is all in the future.
3043080	3044440	All right, so thanks for listening.
3054040	3064040	Thank you. That was lovely. I think we have a couple minutes for a few short questions.
3074280	3076040	Is there a microphone?
3084360	3092120	Okay. There will also be a Q&A next door in 204 for a few minutes after when this is over.
3092920	3098440	Are we using these mics, or are we calling up? Yes. That's great. I can't see the mic from here.
3099320	3105240	So the person I called on can ask the question. Okay, sure. So one prediction that some people
3105240	3112680	have bandied about, about advances with AI-assisted theorem provers is that we might enter a period
3112680	3118440	where there's a proliferation of proofs for new theorems that are formally verifiable,
3118440	3121640	but which we don't yet have the technology to translate into forms that are easily
3121640	3127720	comprehensible by humans. Do you see this being an issue? Well, it already happens. You know,
3127720	3133720	for example, this Boolean Pythagon-Triple's theorem, no human will ever read that proof.
3134680	3142440	So, I mean, I think it's actually not that scary. I mean, you know, we rely on big numerical
3142440	3148680	computations already in a lot of your mathematics. Of course, we would still want to have a human
3148680	3155560	understandable proof, but as the not-example shows, you can take an incomprehensible computer
3155560	3161880	proof and still analyze it and extract out from it a human proof. So I think that would be one of
3161880	3167080	the ways you do mathematics in the future is to clarify computer-assisted proofs and make them
3167080	3174840	human-understandable. Thank you. Can I ask you over there to ask a question? My question is
3176920	3182200	kind of speculative, but I wanted to ask your opinion on the rule of human intuition going
3182200	3187640	forward with this, because what a lot of what we talked about is formalization of human intuition
3187640	3192360	into formal mathematics. I was wondering if you think that intuitive part of coming up with the
3192360	3200520	idea for the proof itself could be automated in the near future? Not in the near future. As I said,
3200520	3206600	these likely models can generate things that resemble intuition, but it has a lot of garbage.
3208520	3213960	At the very low level of proving like one or two steps in a proof, we can use these proof
3213960	3223000	assistants to sort of only pick out the good intuition and discard the bad one. But what
3223000	3229000	large numbers are terrible at right now is differentiating good ideas from bad ideas.
3229000	3234520	They just generate ideas. So unless there's another breakthrough in AI, I don't think this is going
3234520	3240200	to happen any time soon. We'll take one more question from this side. So I was curious about
3240200	3246760	the need for blueprints. Is it that the system doesn't know enough definitions yet or is the proof
3246760	3253240	space too big? Some combination thereof? No, it's more of an organization for the humans. If you
3253240	3260680	want to coordinate 20 people to work on the same project, many of the people who work on these
3260680	3265880	projects, they don't understand the whole proof. So you need some structure to split it up into
3265880	3271720	really small pieces, atomic pieces that are self-contained for individual people to work on.
3271720	3277720	So it's not for the computer. The computer can compile anything. It's for the humans to
3277720	3282920	break up a complicated problem into lots of easy pieces. Kind of like how divisional labor works
3282920	3288680	in like modern industries, like factories. Okay, I'm going to invite all the other people
3288680	3295960	waiting to ask questions to join us in room 204 briefly. And let's thank Terry for a lovely talk.
