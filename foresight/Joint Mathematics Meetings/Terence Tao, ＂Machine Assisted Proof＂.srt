1
00:00:00,000 --> 00:00:13,920
Hello. Hello-hello. Good afternoon. I'm Bryna Kraw, President of the AMS, and it's my pleasure

2
00:00:13,920 --> 00:00:19,560
to welcome you to the colloquium lectures. These are the oldest lectures at the meetings

3
00:00:19,560 --> 00:00:27,840
of the AMS. The first meeting was held in 1895. The second in 1896 were the first colloquium

4
00:00:27,840 --> 00:00:32,440
lectures actually took place, and those were at Northwestern University, and since that's

5
00:00:32,440 --> 00:00:38,720
my home institution, I can't help but mention them. The list of speakers is a veritable

6
00:00:38,720 --> 00:00:48,440
who's who in mathematics, including Birkhoff, Morse, von Neumann, Tarski, Czern, Milner,

7
00:00:48,440 --> 00:00:55,120
Smell, Nirenberg, Tate, and the list of people I've left off is, well, equally prestigious

8
00:00:55,120 --> 00:01:01,640
to those that I included, and amongst them was also our speaker today's advisor, Stein,

9
00:01:01,640 --> 00:01:11,400
Ilya Stein. So with that, I'll turn to the introduction of Terry Tau. Terry may be somebody

10
00:01:11,400 --> 00:01:15,360
who doesn't need introduction. After all, he's been in a crossword puzzle as a clue

11
00:01:15,360 --> 00:01:23,160
in the New York Times crossword puzzle. I don't want to use up all his time by listing

12
00:01:23,200 --> 00:01:28,080
the awards he's won, but I could. I'll give you just a short list of the highlights of

13
00:01:28,080 --> 00:01:34,600
the Fields Medal in 2006. I'm a Carther Fellowship. He's a fellow of the Royal Society, the Australian

14
00:01:34,600 --> 00:01:38,240
Academy of Sciences, the American Academy of Arts and Science, and a member of the National

15
00:01:38,240 --> 00:01:42,800
Academy of Science, and of course he's a fellow of the AMS, most important of those

16
00:01:42,800 --> 00:01:51,280
distinctions. He has over 350 publications, meaning he hasn't slept in a few years, and

17
00:01:51,400 --> 00:01:56,720
this includes numerous highly influential texts, and he has more than 50 collaborators,

18
00:01:56,720 --> 00:02:00,160
and maybe that's when I counted last week, so I don't know, maybe this week there's

19
00:02:00,160 --> 00:02:06,640
more. He's also one of the broadest researchers in mathematics, covering interests from pure

20
00:02:06,640 --> 00:02:15,760
to applied, and I won't list all of the subjects. But it's not only research, he also serves

21
00:02:15,760 --> 00:02:22,600
the profession in numerous ways, and starting in 2021 took on a very substantial role. He

22
00:02:22,600 --> 00:02:27,040
was appointed by President Biden as a member of the President's Council of Advisors on

23
00:02:27,040 --> 00:02:33,840
Science and Technology. So he's a force. He's mentored over 20 PhD students, and I could

24
00:02:33,840 --> 00:02:42,840
go on and on. So I've known Terry for a while, since about 2004 or 2005 when we first met,

25
00:02:43,240 --> 00:02:51,240
but one of my fondest memories of him was at a party that I was a co-host for. In 2008,

26
00:02:51,240 --> 00:02:58,160
it was the election night, and Terry was sitting in the corner November 2008 on his computer on

27
00:02:58,160 --> 00:03:04,680
some website that was giving election returns, announcing the states before the TV was. It

28
00:03:04,680 --> 00:03:10,440
was really impressive, and it's just one of my fondest memories. Anyway, I won't keep you any

29
00:03:11,240 --> 00:03:13,320
longer. It's my pleasure to introduce Terry.

30
00:03:13,320 --> 00:03:30,520
All right. Thank you very much, Bryna. I'm very happy to be here at this JMM to give this lecture.

31
00:03:30,520 --> 00:03:36,600
It's always nice to be up in San Francisco. So are we talking about what I think is a really

32
00:03:36,600 --> 00:03:42,760
exciting development in mathematics that's going to shape our future, which is really the

33
00:03:44,520 --> 00:03:49,800
development over the last few years of lots of technologies to make machines and computers

34
00:03:51,880 --> 00:04:00,120
help us do math much more effectively. Now, to some sense, this is not new. We have used

35
00:04:00,120 --> 00:04:05,240
both machines and computers, and I use the terms slightly differently. We've used both,

36
00:04:05,240 --> 00:04:10,520
actually, for centuries, really. Nowadays, computers and machines, when we talk about

37
00:04:10,520 --> 00:04:14,840
machine-assisted mathematics and computer-assisted mathematics, they're sort of synonymous. Initially,

38
00:04:14,840 --> 00:04:19,320
they weren't because computers used to be human, and then they were mechanical and then finally

39
00:04:19,320 --> 00:04:27,400
electronic. So, for example, one of the earliest use of computers was to build tables. So, for

40
00:04:27,400 --> 00:04:32,120
example, the large tables of Napier and so forth were basically built by lots and lots of human

41
00:04:32,120 --> 00:04:37,480
computers, and those are the earliest examples of somehow computer-assisted mathematics,

42
00:04:38,520 --> 00:04:42,440
and tables are still really important today. I mean, not so much the log tables anymore,

43
00:04:43,560 --> 00:04:50,280
but a lot of what we call experimental mathematics is based on creating lots and lots of large

44
00:04:50,280 --> 00:04:55,560
tables of interesting things, especially in number theory. So, for example, famously,

45
00:04:55,560 --> 00:05:01,560
the Gendo and Gauss built tables of prime numbers, and they used that to conjecture the prime number

46
00:05:01,560 --> 00:05:07,240
theorem, which is, of course, now a theorem. And similarly, in the 60s, Bertrand's Wittendeyer

47
00:05:08,840 --> 00:05:14,840
created lots and lots of big tables of the decurs and the ranks and so forth, and this was a key

48
00:05:14,840 --> 00:05:21,160
input in formalizing the famous BSD conjecture. And maybe the biggest table of all in mathematics

49
00:05:21,160 --> 00:05:26,760
is the OEIS, online encyclopedia of mathematical sequences. So, there's hundreds of thousands

50
00:05:27,160 --> 00:05:32,200
of integer sequences, and every day, I think mathematicians discover unexpected connections,

51
00:05:32,840 --> 00:05:40,520
or maybe rediscover an existing connection. I myself use the OEIS. If there's a quantity,

52
00:05:40,520 --> 00:05:43,960
which I know there's a formula for, but I can't remember it, I can just compute the first five

53
00:05:43,960 --> 00:05:51,400
elements put in the OEIS, I can usually find it. And most recently, people are starting to use

54
00:05:51,480 --> 00:05:56,520
large databases of mathematical objects as training data for neural networks, and so I'll

55
00:05:56,520 --> 00:06:05,800
give you an example of that later. So, that's one very storied and antique way of using computers

56
00:06:07,320 --> 00:06:11,880
in mathematics. The other big use, of course, is in numerics or scientific computing,

57
00:06:12,520 --> 00:06:19,240
and that's also a very old subject. Arguably, the first big scientific computation was in the

58
00:06:19,240 --> 00:06:26,280
20s when Lorenz was asked to model the fluid flow for the construction of a new dike in the

59
00:06:26,280 --> 00:06:32,360
Netherlands, and so he assembled a team of human computers, basically, to model what would happen

60
00:06:32,360 --> 00:06:39,080
to the water flow and so forth. It's notable for the introduction, he's almost the first place

61
00:06:39,080 --> 00:06:44,040
where floating point arithmetic was introduced. And, of course, we use scientific computing

62
00:06:44,040 --> 00:06:51,080
nowadays to model PDEs, to solve large systems of equations, and, of course, we use them for

63
00:06:51,080 --> 00:06:56,920
computer algebra packages, you know, magma, maple, sage, and so forth. Yeah, you want to do a big,

64
00:06:57,880 --> 00:07:02,440
you know, numerical integration or algebraic, you know, computer Gribner bases, whatever,

65
00:07:03,640 --> 00:07:10,440
you know, we routinely do this now, or cross mathematics. Of course, the numerics are sometimes

66
00:07:10,440 --> 00:07:16,920
inaccurate, you know, there are round off errors and other possible problems, but there are ways

67
00:07:16,920 --> 00:07:20,760
to make the combination more rigorous. For example, instead of floating point arithmetic,

68
00:07:20,760 --> 00:07:25,960
if you use interval arithmetic, if you represent numbers by error ranges, a lower and upper bound,

69
00:07:26,840 --> 00:07:32,360
and you keep those bounds like rational numbers, like finite precision, like infinite precision,

70
00:07:33,080 --> 00:07:39,320
then you can avoid errors, at least in principle, at the cost maybe of

71
00:07:40,920 --> 00:07:46,600
making the runtime longer. More recently, there are more advanced

72
00:07:48,680 --> 00:07:53,720
algebra packages than just sort of the standard things you get in sage or Mathematica.

73
00:07:55,480 --> 00:08:01,160
They think of SAT solvers, satisfiability solvers, or satisfiably modulo theory solvers,

74
00:08:01,160 --> 00:08:08,200
SMT solvers. So what they, so a SAT solver gives, you feed it a whole statement, a bunch of

75
00:08:08,200 --> 00:08:15,000
propositions, P1, P2, P3, and so forth, and you see that P1 and P2, all P3 is true, P3 and P4,

76
00:08:15,000 --> 00:08:20,120
and not P5, one of them is true, and so forth, and it will try to see if there's a solution or not,

77
00:08:20,840 --> 00:08:25,480
and many problems can be phrased as a SAT problem. So if you have a general purpose SAT solver,

78
00:08:26,360 --> 00:08:32,760
you can potentially just feed it into such a program and solve the problem for you,

79
00:08:33,480 --> 00:08:38,120
then there are these more sophisticated variants, SMT solvers, where you also feed laws of algebra

80
00:08:38,760 --> 00:08:46,680
so you have some variables, and you assume that there are certain laws, these variables

81
00:08:46,680 --> 00:08:50,200
obey certain laws, and you ask, can you deduce a new law from the laws you already have?

82
00:08:51,640 --> 00:08:56,360
So those are potentially very powerful, and unfortunately, SAT solverability is an NP

83
00:08:56,360 --> 00:09:02,120
complete problem, and once you get hundreds and hundreds of these propositions, it becomes very

84
00:09:02,120 --> 00:09:08,760
hard to actually solve these problems, but still they are very useful. Here's a typical

85
00:09:08,760 --> 00:09:16,520
application of a SAT solver, so a few years ago, there was this famous problem in commentaries

86
00:09:16,520 --> 00:09:21,640
called the Boolean Pythagorean Triples problem, and so the problem is this, you take natural numbers

87
00:09:21,640 --> 00:09:28,440
and you color them into two color classes, red and blue, and you ask, is it always the case that

88
00:09:28,440 --> 00:09:33,000
one of these color classes contains a Pythagorean triple, three numbers A, B and C such that A

89
00:09:33,000 --> 00:09:40,680
squared plus B squared equals C squared, and it turns out to be, we don't have like a human proof

90
00:09:40,680 --> 00:09:45,880
of the statement, but we know it's true now because of a SAT solver, so there was a massive

91
00:09:45,880 --> 00:09:54,440
computation that says that if you only go up to 7,824, then you can't do this, there was a way

92
00:09:54,440 --> 00:09:59,640
to partition the numbers from 1 to 7,824 into two classes, neither of which contain a Pythagorean

93
00:09:59,640 --> 00:10:06,280
triple, but once you go up to 7,825, no matter how you do it, you must always get one of the

94
00:10:06,280 --> 00:10:11,640
two color classes must have a Pythagorean triple. In principle, this is a finite computation

95
00:10:11,640 --> 00:10:18,680
because there's only two to seven, two to five different ways to compute different partitions,

96
00:10:18,680 --> 00:10:23,320
and so you just check each one, but that is computationally unfeasible, but with a SAT

97
00:10:23,320 --> 00:10:30,760
solver, you can rephrase this problem as a free satisfiability problem, and it's not just a matter

98
00:10:30,760 --> 00:10:34,280
of running the solver, you have to optimize it and so forth, but it is possible to actually solve

99
00:10:34,280 --> 00:10:40,120
this problem, and it gives you a certificate, it gives you a proof, and actually this is,

100
00:10:40,120 --> 00:10:46,040
at the time, it was actually the world's longest proof. The proof certificate, first of all,

101
00:10:46,040 --> 00:10:52,120
it took four CPU years to generate, and it's a 200 terabyte proof, although it is compressible.

102
00:10:52,840 --> 00:10:55,400
I think it is still the second largest proof ever generated.

103
00:10:57,160 --> 00:11:03,240
Okay, so that's, but this I still consider is a more classical way of using computers,

104
00:11:05,400 --> 00:11:12,440
but what I think is exciting is that there are a lot of new ways that we can use computers

105
00:11:14,440 --> 00:11:18,600
to do mathematics. Of course, there's still the boring ways, you know, we use computers to do

106
00:11:18,600 --> 00:11:26,520
emails and write latex and so forth, I don't mean that, but there are sort of three new modalities,

107
00:11:27,160 --> 00:11:33,320
which individually, they still have somewhat niche applications, but what I find really

108
00:11:33,320 --> 00:11:37,320
interesting is that they can potentially be combined together, and the combination of them,

109
00:11:37,320 --> 00:11:43,160
it could be something in general purpose that actually a lot of us could use. So the three sort

110
00:11:43,160 --> 00:11:49,960
of new things. So the first is machine learning algorithms, where you have a problem, and if

111
00:11:49,960 --> 00:11:54,280
you have a lot of data for that problem, you can set some sort of specialized neural network to

112
00:11:54,280 --> 00:11:57,800
train it on the data, and it can generate counter examples for you, try to generate connections,

113
00:11:58,760 --> 00:12:01,720
and so people are beginning to use this in all kinds of fields of mathematics, I'll give you

114
00:12:01,720 --> 00:12:10,600
some examples later. So that's one development. Maybe the most high-profile development is

115
00:12:10,600 --> 00:12:15,080
large language models like chat GPT, these are very general purpose models that can understand

116
00:12:15,080 --> 00:12:21,560
natural language. To date, they have not been directly used for so much mathematics, I'm sure

117
00:12:21,560 --> 00:12:26,200
many of you have tried talking to GPT, asking it to solve your favorite math problem, and it will

118
00:12:26,200 --> 00:12:35,080
give you some so plausible looking nonsense in general. But when used correctly, I think they

119
00:12:35,080 --> 00:12:41,160
do offer a lot of potential. I mean, I have found occasionally that these models can be useful for

120
00:12:41,160 --> 00:12:48,200
suggesting proof techniques that I wasn't initially thinking of, or to suggest related topics or

121
00:12:48,200 --> 00:12:55,400
literature. They're actually most useful for sort of secondary tasks. Okay, so for actually doing

122
00:12:55,400 --> 00:12:59,480
math research, they still haven't really proved themselves, but for doing things like writing

123
00:12:59,480 --> 00:13:06,360
code or organizing a bibliography, like a lot of the other more routine tasks that we do actually,

124
00:13:06,360 --> 00:13:14,760
these LLMs are very useful. But the third new technology, which has been around for two decades,

125
00:13:14,760 --> 00:13:19,160
but has only now sort of becoming ready for primetime, are these formal proof assistants,

126
00:13:19,960 --> 00:13:26,600
which are languages designed to verify, or to verify, or many of them are actually designed

127
00:13:26,600 --> 00:13:33,160
to verify electronics, but they can also verify mathematical proofs. And crucially, they can

128
00:13:33,160 --> 00:13:38,440
also verify the output of large language models, which they can complement, they can fix the biggest

129
00:13:38,440 --> 00:13:47,800
defect in principle of the LLMs. And they allow new types of ways to do mathematics, in particular,

130
00:13:47,800 --> 00:13:51,800
they can allow really large scale collaborations, which we really can't do without these formal

131
00:13:51,800 --> 00:13:57,080
proof assistants. And they can also generate data, which can be used for the other two

132
00:13:58,840 --> 00:14:03,160
the other two technologies. So I'll talk about each of these three things separately, but

133
00:14:06,040 --> 00:14:09,560
but they haven't, there's beginning to be experiments to combine them together,

134
00:14:10,760 --> 00:14:16,120
but they're still kind of prototypes right now. But I think the paradigm of using all of them,

135
00:14:16,120 --> 00:14:21,080
and also combining with the computer algebra systems and the SAP solvers into one integrated

136
00:14:21,080 --> 00:14:27,320
package, it could really be quite a powerful methodical assistant. Okay, so let's talk,

137
00:14:27,320 --> 00:14:36,440
I think my first slides begin with proof assistants. So the computer-assisted proofs are not new,

138
00:14:37,320 --> 00:14:42,760
famously the full color theorem in 1976 was was proven partly by computer. Although at the time,

139
00:14:42,760 --> 00:14:49,000
it was by modern standards, we will not call it a fully formalized proof, the 1976 proof.

140
00:14:49,400 --> 00:14:55,720
The proof was this long document with lots and lots of subclaims, which

141
00:14:57,160 --> 00:15:02,040
a lot of them were verified by hand, and a lot of them were verified by both electronic computers

142
00:15:02,040 --> 00:15:06,680
and human computers. And I think one of the author's daughter actually had to go through 500 graphs

143
00:15:06,680 --> 00:15:12,920
and check that they all had this discharging property. And actually, it had a lot of mistakes too.

144
00:15:12,920 --> 00:15:20,120
So there's a lot of minor errors in the proof. They're all correctable, but it really

145
00:15:20,840 --> 00:15:25,800
will not meet the standards today of a computer-verified proof.

146
00:15:28,200 --> 00:15:34,920
The first proof, okay, so it took 30 to 20 years for an alternative proof of full color theorem

147
00:15:34,920 --> 00:15:41,720
to be verified, and this proof is closer to being completely formal. So it's about 10-15 pages

148
00:15:41,720 --> 00:15:47,160
of human readable argument, and then it reduces to this very specific computation, which anyone

149
00:15:47,160 --> 00:15:53,800
can just run a computer program in whatever language they like to verify it. So it was a

150
00:15:53,800 --> 00:15:59,880
computer-verified proof, but it still wasn't a formal proof. It wasn't written in a formal

151
00:15:59,880 --> 00:16:06,520
proof language, which was designed to only output correct proofs. And that had to wait until the

152
00:16:06,520 --> 00:16:12,120
early 2000s when Werner and Gontier actually formalized the entire full color theorem in one

153
00:16:12,120 --> 00:16:20,040
of the early proof assistant languages, COC, in this case. So now we know with 100% certainty

154
00:16:20,040 --> 00:16:25,720
that the full color theorem is correct. Well, modulo, trusting the compiler of COC.

155
00:16:26,520 --> 00:16:36,440
All right. Another famous machine assistant proof, well, actually initially human proof,

156
00:16:36,440 --> 00:16:43,080
but eventually computer-verified was the proof of the coupler conjecture. So the coupler conjecture

157
00:16:43,080 --> 00:16:47,080
is a statement about how efficient that you can pack unit spheres in the plane,

158
00:16:48,200 --> 00:16:53,240
and so there's a natural way to stack unit spheres, and it's the way that you see oranges stacked in

159
00:16:53,240 --> 00:16:57,960
the grocery store. It's called the hexagonal closed packing, and there's also a dual packing

160
00:16:57,960 --> 00:17:02,760
with the same density called the cubic closed packing. And they have a certain density, pi over

161
00:17:02,760 --> 00:17:09,480
three over two, and this was conjectured to be the densest packing. So this is an annoyingly hard

162
00:17:09,480 --> 00:17:18,520
statement to prove. So it's an optimization problem in infinitely many variables. So each

163
00:17:19,480 --> 00:17:22,520
each sphere has a different location, and there's an infinite number of spheres.

164
00:17:23,080 --> 00:17:27,320
So you're trying to prove an inequality involving an infinite number

165
00:17:27,320 --> 00:17:33,880
of variables involving solving infinite number constraints. So it doesn't immediately

166
00:17:33,880 --> 00:17:40,840
lend itself to computer verification. But even in the 50s, it was realized that possibly this

167
00:17:40,840 --> 00:17:46,920
could be done by some sort of brute force computation. And so Toth proposed the following

168
00:17:46,920 --> 00:17:52,040
paradigm. So every time you see a packing, it comes with what's called a Voronoi decomposition.

169
00:17:52,040 --> 00:17:57,160
So every sphere comes with a Voronoi cell, which is this polytope of all the points that are closer

170
00:17:57,160 --> 00:18:01,080
to the center of that of that sphere than to all the other spheres. And there's partitions

171
00:18:01,800 --> 00:18:07,720
space into all these little polyhedron, these Voronoi cells. And there are various relationships

172
00:18:07,720 --> 00:18:12,280
between the volumes of these different cells. There's only so many spheres that you can pack

173
00:18:12,280 --> 00:18:15,960
next to one reference sphere, and this creates all these kind of constraints.

174
00:18:17,800 --> 00:18:24,120
And so the hope was that if you could gather enough inequalities between adjacent Voronoi cells,

175
00:18:24,840 --> 00:18:29,480
the volumes of adjacent Voronoi cells, maybe every such system inequalities, in principle,

176
00:18:29,480 --> 00:18:35,320
gives you an upper bound on the density of the sphere packing. And in principle, if you get

177
00:18:35,320 --> 00:18:39,720
enough of these inequalities, maybe you could actually get the optimal bound of three or two.

178
00:18:40,600 --> 00:18:47,640
So people tried this approach for many, many years, including some false attempts.

179
00:18:48,280 --> 00:18:51,480
But they were not able to actually make this approach work.

180
00:18:54,040 --> 00:18:59,000
But Thomas Hales, and then later, with a collaborator, was able to adapt this approach

181
00:18:59,000 --> 00:19:04,760
to make it work in a series of papers from 94-98. But they had to modify the strategy quite a lot.

182
00:19:04,840 --> 00:19:09,880
So instead of using the Voronoi decomposition, there was a more complicated decomposition

183
00:19:09,880 --> 00:19:14,040
that was used. And instead of using volume, they had to define this new score function

184
00:19:14,680 --> 00:19:18,600
attached to each polyhedron. But basically, the strategy was the same.

185
00:19:19,720 --> 00:19:24,280
And he was able to prove lots and lots of linear inequalities between the scores of adjacent

186
00:19:24,280 --> 00:19:30,520
polyhedra. And then just using linear programming was able to then get a bound. And

187
00:19:31,320 --> 00:19:35,240
with the right choice of score and the right choice of partition, it was the optimal bound.

188
00:19:37,720 --> 00:19:42,040
It's a very flexible method, because you have lots of ways you can do the partition and lots

189
00:19:42,040 --> 00:19:47,480
of ways that you can do the score. But the problem is that it was too flexible. So here's a quote

190
00:19:47,480 --> 00:19:52,840
from Hales. It says that Simon Ferguson, who was Hales' collaborator, and I realized every time we

191
00:19:52,840 --> 00:19:55,720
encounter difficulty solving the minimization problem, we get just a scoring function to

192
00:19:55,720 --> 00:19:59,000
score the difficulty. The function became more complicated, but with each change,

193
00:19:59,000 --> 00:20:03,880
we could cut months or years from our work. This incessant fiddling was unpopular with my

194
00:20:03,880 --> 00:20:07,720
colleagues. Every time I presented my work in progress at a conference, I was minimizing a

195
00:20:07,720 --> 00:20:11,960
different function. Even worse, the function was moderately incompatible with earlier papers,

196
00:20:11,960 --> 00:20:19,240
and this required going back and patching the earlier papers. So the proof was a mess, basically.

197
00:20:20,280 --> 00:20:25,720
They did eventually finish it in 98, and they were able to derive the Kepler conjecture from

198
00:20:25,720 --> 00:20:30,840
a linear programming computation from a very complicated optimization program. Initially,

199
00:20:30,840 --> 00:20:35,880
it was done by hand, but with the increased complexity, there was no choice but to make

200
00:20:35,880 --> 00:20:43,000
it more and more computer-assisted. So when the proof was announced, it was a combination of

201
00:20:43,000 --> 00:20:49,400
250 pages of notes and lots and lots of gigabytes of programs and data, and it was famously hard

202
00:20:49,400 --> 00:20:54,120
to referee. It took four years for annals to referee the paper with a panel of pro referees,

203
00:20:54,200 --> 00:20:57,640
and even then, the panel was only 99 percent certain of the correctness of the proof, and they

204
00:20:57,640 --> 00:21:05,640
could certify the corrections, the calculations. Because, I mean, in principle, it was all doable,

205
00:21:05,640 --> 00:21:11,160
but the referees have to implement all these different computer calculations themselves,

206
00:21:11,960 --> 00:21:18,360
but it was eventually accepted. But clearly, there was this big asterisk. There was a lot of

207
00:21:18,360 --> 00:21:21,800
controversy about whether this was really a valid proof, and so this was one of the

208
00:21:22,600 --> 00:21:29,480
the first really high-profile uses of formal proof assistance, because this was a result in which

209
00:21:29,480 --> 00:21:37,080
there was serious doubt about the correctness. So they created, so Hales in 2003 initiated a

210
00:21:37,080 --> 00:21:42,840
project to write down this massive proof in a completely formalized way so that a standard

211
00:21:42,840 --> 00:21:47,800
proof assistant could verify it. He estimated it would take 20 years to make this work,

212
00:21:48,520 --> 00:21:55,000
and so he had, he gathered 21 collaborators. It actually only took 11 years,

213
00:21:57,240 --> 00:22:02,200
but yeah, eventually what they did was that they first created a blueprint,

214
00:22:02,200 --> 00:22:05,640
you know, so a human readable version of the proof breaking things up into very,

215
00:22:05,640 --> 00:22:10,600
very small steps, and then they formalized each step by bit, and it was finally done,

216
00:22:10,600 --> 00:22:14,680
and then there was a, yeah, so they published a paper about the formalization, and that only

217
00:22:14,680 --> 00:22:21,000
appeared in 2017. So this was sort of the state-of-the-art of formalization, you know,

218
00:22:21,000 --> 00:22:25,880
as of say 20 years ago, you know, like it was possible to formalize big complicated results,

219
00:22:25,880 --> 00:22:34,120
but it took an enormous amount of effort, you know, not something which you would do routinely.

220
00:22:36,120 --> 00:22:39,720
There was a more recent effort in a similar spirit by Peter Schultzer,

221
00:22:39,720 --> 00:22:45,160
he called it the liquid tensor experiment. So Schultzer introduced this theory of condensed

222
00:22:45,160 --> 00:22:50,440
mathematics, which is, all right, this is really far from my own area of expertise, but

223
00:22:51,800 --> 00:22:56,840
basically there are certain problems with, so certain types of mathematics you want to work

224
00:22:56,840 --> 00:23:00,360
in various categories, like categories of topological obedient groups and topological vector spaces,

225
00:23:00,920 --> 00:23:04,680
and there's a problem that they're not obedient categories, that they don't, they don't have,

226
00:23:04,680 --> 00:23:07,800
I have a good notion of kernel and cold kernel, and things don't work out properly.

227
00:23:08,520 --> 00:23:14,280
So he proposed replacing all of these standard categories with a more fancy version called

228
00:23:14,280 --> 00:23:21,160
a condensed category, which has better category theoretic properties, and so the hope is that

229
00:23:21,160 --> 00:23:26,840
you could use a lot more high-powered algebra to attack, to handle things with topological

230
00:23:26,840 --> 00:23:30,040
structure or analytical structure, like function spaces, for example, binoc spaces.

231
00:23:32,760 --> 00:23:37,160
But in order for you to work, there's a certain vanishing theorem, which I've written there,

232
00:23:37,720 --> 00:23:43,080
but I cannot explain to you. Okay, so there's a certain category,

233
00:23:46,760 --> 00:23:50,760
condescending groups, and there's an x-group involving p-binoc spaces that has to vanish,

234
00:23:52,680 --> 00:23:57,160
and this vanishing theorem is needed in order for all of the rest of the theory to actually be

235
00:23:57,160 --> 00:24:05,560
useful, if you want to apply it to function analysis in particular. And so Schultzer,

236
00:24:05,560 --> 00:24:09,560
what a blog post about this is that, you know, I spent much of 2019 obsessed with the proofless

237
00:24:09,560 --> 00:24:13,560
theorem, almost getting crazy over it. In the end, we were able to get an argument down on

238
00:24:13,560 --> 00:24:17,080
paper, but I think no one else has ever dared to look at the details of this, and so I still

239
00:24:17,080 --> 00:24:24,360
have some lingering doubts. With this hope, with this theorem, the hope that the condensed formulas

240
00:24:24,360 --> 00:24:29,320
can be fruitfully applied to function analysis stands a force, being 99% sure is not enough

241
00:24:29,400 --> 00:24:37,240
because this theorem is of the most fundamental importance. He says, I think this may be my most

242
00:24:37,240 --> 00:24:41,720
important theorem to date, which is a really big claim, actually. Better be sure it's correct.

243
00:24:42,520 --> 00:24:47,720
So this was another case where there was a great desire to formalize the proof.

244
00:24:49,560 --> 00:24:55,400
So he asked publicly on the internet for help to formalize this in a modern

245
00:24:55,400 --> 00:25:02,280
proof of this in language called Lean. And so again, about 20 people, I think, joined this

246
00:25:02,280 --> 00:25:12,040
project to help out. And it, so Lean is based on, it has this philosophy where it has this

247
00:25:12,040 --> 00:25:17,240
single huge library of mathematical theorems, like the fundamental calculus or the classification

248
00:25:17,240 --> 00:25:21,480
of finite building groups, like a lot of the basic theorems of mathematics are already formalized

249
00:25:21,480 --> 00:25:25,640
in this big library. And the idea is to just keep building on this library and adding to it

250
00:25:25,640 --> 00:25:33,480
with additional projects. But one of the problems with that short of the face was that a lot of the

251
00:25:34,360 --> 00:25:37,320
tools that, basic tools that you needed, like homological algebra and chief theory and top

252
00:25:37,320 --> 00:25:41,080
box theory, weren't actually in the library yet. So actually part of what they had to do was actually

253
00:25:41,080 --> 00:25:47,160
set up the foundations of that theory and formalize it in the library first. But it basically was

254
00:25:47,160 --> 00:25:54,200
done in about two years. In one year, they formalized a key sub theorem. And then the whole

255
00:25:54,200 --> 00:26:02,760
thing was formalized about a year afterwards. It did have some, in addition to sort of making

256
00:26:03,320 --> 00:26:09,720
reassuring Peter that it was, everything was correct, there was some other minor benefits.

257
00:26:09,720 --> 00:26:13,080
So first of all, there were actually a few minor errors in the proof that were discovered

258
00:26:13,080 --> 00:26:18,680
in the formalization progress. But they go refixed also some small simplifications.

259
00:26:19,320 --> 00:26:26,200
There was one big simplification actually. So they, the original proof used something very

260
00:26:26,200 --> 00:26:31,160
complicated, which I also don't understand, called the Breen-Deline resolution. But in the course

261
00:26:31,160 --> 00:26:35,640
of formalizing it, it was too hard to actually formalize this theory. But they found that there

262
00:26:35,640 --> 00:26:39,720
was a weaker version of this theory, which was good enough to formalize this application. But

263
00:26:39,720 --> 00:26:44,520
this was actually a major discovery because this weaker theory could also potentially attack

264
00:26:44,520 --> 00:26:50,440
some other problems that the Breen-Deline resolution is not well suited for. And now

265
00:26:50,440 --> 00:26:57,080
the math library is much, much better in, it has a lot of support for homological

266
00:26:57,080 --> 00:27:01,560
algebra and sheath theory and lots of other things, which helped other formalization projects

267
00:27:01,560 --> 00:27:07,320
become easier. I got interested in this formalization a few months ago.

268
00:27:07,400 --> 00:27:16,120
So, oh, hang on. I should say, like with the Kepler experiment, so you don't just directly

269
00:27:16,120 --> 00:27:24,360
transfer a paper from, you know, the archive or something into Breen. What we found is that

270
00:27:24,360 --> 00:27:28,680
it really helps to create first an intermediate document. So halfway between a human readable proof

271
00:27:28,680 --> 00:27:38,120
and a formal proof, which we call a blueprint. So it looks like a human proof and is written in a

272
00:27:38,680 --> 00:27:45,640
version of latex. But like each step in the proof is linked to a lemma in lean. And so it's very

273
00:27:45,640 --> 00:27:52,280
tightly integrated. It has a very nice feature. It can create dependency graphs, which I'll show you

274
00:27:52,280 --> 00:27:56,120
an example later. You can see which parts of the proof have been formalized, which ones are not,

275
00:27:56,120 --> 00:28:02,520
and what depends on what. It actually, it gives a lot of structure to the process of writing a paper,

276
00:28:02,520 --> 00:28:06,760
which, you know, right now we do by hand without sort of much assistance, really.

277
00:28:09,160 --> 00:28:12,680
Yeah, Schultz has said that the process of writing the blueprint really helped him understand the

278
00:28:12,680 --> 00:28:21,240
proof a lot better. And actually, people have been also going the other way. There's also software

279
00:28:21,240 --> 00:28:24,200
that takes formal proofs, which are written in something that looks like computer code,

280
00:28:24,200 --> 00:28:31,320
and turns them back into human readable proofs. Here is a prototype software. So there's a

281
00:28:31,320 --> 00:28:38,040
there's a theorem in topology. Okay, you know, I think it's a if a function is continuous on a dense

282
00:28:38,040 --> 00:28:42,840
set, and there's some extra extra properties, then it's continuous extends to a continuous

283
00:28:42,840 --> 00:28:48,600
function globally. And the proof is written was written first in lean, but then it was

284
00:28:48,680 --> 00:28:53,240
already converted into a human proof, human readable proof. But again, where every step

285
00:28:53,240 --> 00:28:57,080
you can expand and contract, like if there's a step you want to explain more, you can double

286
00:28:57,080 --> 00:29:01,240
click it, and it will expand out, give all the justification. And you can click at any given

287
00:29:01,240 --> 00:29:05,880
point in the proof, and it will tell you what the hypotheses are currently, what you can approve,

288
00:29:05,880 --> 00:29:12,520
and you can give a lot of structure. I think eventually textbooks, this is maybe a good format,

289
00:29:12,520 --> 00:29:16,760
say for undergraduate textbooks to have, you know, proofs of say, you know, tricky

290
00:29:16,760 --> 00:29:21,960
films and analysis or something written in a way where, you know, in a not in a static way,

291
00:29:21,960 --> 00:29:27,320
but where you can really, you know, drill down all the way down to the basic axioms if you wanted to.

292
00:29:30,600 --> 00:29:39,400
Okay. One thing I mean, one thing notable about these formalization projects is that they allow

293
00:29:39,400 --> 00:29:43,960
massive collaborations. So, you know, in other sciences, people collaborate with 20 people,

294
00:29:43,960 --> 00:29:50,600
500 people, you know, 5,000 people. But in mathematics, still, we don't really collaborate

295
00:29:50,600 --> 00:29:57,160
in really large groups. You know, five is kind of the maximum, usually. And part of it is that,

296
00:29:57,160 --> 00:30:01,160
you know, if you want to collaborate with 20 people, if you don't already know these 20 people,

297
00:30:01,160 --> 00:30:07,960
and you don't completely trust that they're doing correct mathematics, you know, it's very difficult

298
00:30:07,960 --> 00:30:13,160
because you have to, everyone has to check everyone else's contribution. But with a proof

299
00:30:13,160 --> 00:30:18,120
assistant, the proof assistant provides a formal guarantee. And so, you can really collaborate

300
00:30:18,120 --> 00:30:23,640
with 20, hundreds of people that you've never met before. And you don't need to trust them.

301
00:30:24,440 --> 00:30:31,720
Because they upload code and the lean compiler verifies, yes, this actually solves what they

302
00:30:31,720 --> 00:30:38,280
claimed. And then you can accept it or it doesn't. So, I experienced this myself because I got

303
00:30:38,360 --> 00:30:43,240
interested in formalization a few months ago. So, I'd recently proven a combinatorial theorem

304
00:30:43,960 --> 00:30:48,680
with Tim Gowes, Ben Green, and Freddie Manners. It solved something called the polynomial

305
00:30:48,680 --> 00:30:54,280
primonrugia conjecture. The precise conjecture is not important for this talk. It's a conjecture

306
00:30:54,280 --> 00:30:59,720
in combinatorics. You have a subset of a finite through vector space of small doubling. And you

307
00:30:59,720 --> 00:31:04,680
want to show that it is all, it is very close to actually being a coset of a subgroup. This was a

308
00:31:04,680 --> 00:31:13,000
statement that was in combinatorics that was highly sought after. So, we had a 33-page paper

309
00:31:13,880 --> 00:31:19,320
recently proving this. Mostly self-contained, fortunately. So, I thought I was a good candidate

310
00:31:19,320 --> 00:31:26,280
for formalization. So, I asked on an internet forum, specializing in lean, for help to formalize it.

311
00:31:26,280 --> 00:31:32,520
And then, again, like 20, 30 people joined in and actually only took three weeks to formalize.

312
00:31:32,920 --> 00:31:36,280
The time taken to formalize these projects is going down quite a bit.

313
00:31:38,600 --> 00:31:43,560
So, in particular, the time taken to formalize this project was roughly about the same time

314
00:31:43,560 --> 00:31:48,120
it took for us to write the paper in the first place. And by the time we submitted the paper,

315
00:31:48,120 --> 00:31:51,880
we were able to put as a note that the proof is actually being formalized.

316
00:31:53,880 --> 00:31:58,520
So, as I said, it uses a single blueprint which splits up the proof into lots and lots of little

317
00:31:58,520 --> 00:32:07,000
pieces. And so, it creates this nice little dependency graph. So, this is a picture of

318
00:32:07,000 --> 00:32:11,320
the graph at an early stage of the project. So, there's no down the bottom called PFR. Maybe

319
00:32:11,320 --> 00:32:14,280
only the people in the front can see it. That's the final theorem that we're trying to prove.

320
00:32:15,160 --> 00:32:19,000
At the time of the screenshot, this was white, which means that it had not been

321
00:32:20,040 --> 00:32:24,280
proved, but not even been stated properly. So, in fact, even before you prove the theorem,

322
00:32:24,280 --> 00:32:28,200
you have to first state it, and then you have to make definitions and so forth.

323
00:32:29,240 --> 00:32:33,320
Blue are things that have been defined, but not yet proven, and green have been things that have

324
00:32:33,320 --> 00:32:41,400
been proven. And so, at any point in time, some bubbles are white, some are blue, and some

325
00:32:41,400 --> 00:32:47,560
results depend on some others. And so, the way the formalization proceeded was that different

326
00:32:47,560 --> 00:32:52,760
people just grabbed a note that was ready to be formalized because maybe all the previous results

327
00:32:53,240 --> 00:32:57,720
that it depended on were formalized, and they just formalized that one step independent of

328
00:32:57,720 --> 00:33:03,880
everybody else. And you didn't really need to coordinate too much. I mean, we did coordinate

329
00:33:05,000 --> 00:33:10,760
on an internet forum, but each little note is completely specified. There's a very precise

330
00:33:10,760 --> 00:33:18,280
definition and a very precise thing to prove, and we just need the proof. And we really don't care

331
00:33:18,360 --> 00:33:24,120
exactly what the proof is. I mean, it has to be not massively long. So, people just picked up

332
00:33:24,120 --> 00:33:28,440
individual claims. Here's one very simple claim. There's a certain functional called

333
00:33:28,440 --> 00:33:34,280
ruja distance d, and there's a very simple claim that it was non-negative. And this turns out to

334
00:33:34,280 --> 00:33:43,480
have a three-line proof, assuming some previous facts that were also on the blueprint. And so,

335
00:33:43,480 --> 00:33:48,760
people just sort of picked up these things separately, and over time, it just filled up.

336
00:33:48,760 --> 00:33:52,840
This is what a typical step in the proof looks like. This is the proof that this ruja distance

337
00:33:52,840 --> 00:33:58,520
is non-negative. This is the code in the lean. It looks kind of a little bit like mathematics,

338
00:33:59,160 --> 00:34:05,320
but it is actually... Once you think of the syntax, it actually reads... It's like reading

339
00:34:05,320 --> 00:34:08,280
latex. The first time you read latex, it looks like a whole bunch of mess of symbols. But

340
00:34:09,160 --> 00:34:16,040
it's actually... Every line corresponds to a sentence in mathematical English. For example,

341
00:34:16,040 --> 00:34:18,440
the first line, if you want to prove that this distance is positive,

342
00:34:18,440 --> 00:34:22,120
it suffices to prove that tricidistance is positive. So, you work with tricidistance,

343
00:34:22,120 --> 00:34:27,400
because it turns out there's another lemma that bounds tricidistance. So, yeah, every step,

344
00:34:27,400 --> 00:34:34,600
actually, it corresponds reasonably well to the way we think about mathematical proofs.

345
00:34:35,560 --> 00:34:43,640
Yeah. So, fortunately, the proof didn't reveal any major issues. There were some typos,

346
00:34:43,640 --> 00:34:49,880
but very, very minor picked up. And we also... There were some things we needed... Again,

347
00:34:49,880 --> 00:34:54,680
we needed to add to the math library. The math library... We used Shannon entropy,

348
00:34:54,680 --> 00:35:00,520
and Shannon entropy was not in the math library. Now it is. One thing about formalization is that

349
00:35:00,520 --> 00:35:07,400
it still takes longer to formalize proofs than to write them. But... And maybe about 10 times

350
00:35:07,400 --> 00:35:15,320
longer, I would say. Which is unfortunate. If it was faster to formalize to write proofs formally

351
00:35:15,320 --> 00:35:20,920
than to write them by hand, I think then there will be a tipping point. And then I think a lot

352
00:35:20,920 --> 00:35:27,160
of us would switch over just because they guarantee a correctness. So, we're not there yet. It is

353
00:35:27,160 --> 00:35:31,640
definitely getting a lot faster than it was before. But one thing that we noticed is that,

354
00:35:31,640 --> 00:35:36,840
actually, while it still takes a long time to write a proof, if you want to modify a proof

355
00:35:36,840 --> 00:35:43,160
to change some parameters and make it a little bit better, that can be done much quicker in the

356
00:35:43,160 --> 00:35:48,840
formal setting than with a paper proof. Because with paper proof, if you change all your little

357
00:35:48,840 --> 00:35:53,160
parameters and so forth, you'll likely make also mistakes and you go back. And it's a very annoying

358
00:35:53,160 --> 00:35:58,920
process. But it's actually much easier to modify an existing formal proof than to create one from

359
00:35:58,920 --> 00:36:04,120
scratch. For example, we were able... There's a constant 12 exponent that appeared in our proof.

360
00:36:05,560 --> 00:36:11,880
A few days afterwards, someone posted an improvement in the argument, improved 12 to 11,

361
00:36:11,880 --> 00:36:15,320
and in a few days we were able to adapt the formal proof to do that as well.

362
00:36:15,480 --> 00:36:25,720
Yeah. And because you can collaborate, I think the process is scalable. There was just recently Kevin

363
00:36:25,720 --> 00:36:30,120
Buzzard, for example, a five-year project. The aim is to formalize Fermat's last theory of the proof.

364
00:36:31,480 --> 00:36:37,640
And that is quite a big goal because there are lots and lots of sub-portions that have had no

365
00:36:37,640 --> 00:36:42,680
formal proof at all. So, that, I think, will be quite an ambitious project. But I think it's now

366
00:36:42,680 --> 00:36:50,360
doable. It wasn't doable five years ago, but now I think it is. Okay. So, that's four more proofs.

367
00:36:51,160 --> 00:36:58,440
Another... Okay. I might actually have to speed up a little bit. Okay. So,

368
00:37:00,280 --> 00:37:07,160
all right. So, machine learning, using neural networks has become also more and more common

369
00:37:07,160 --> 00:37:10,760
place in different areas of mathematics. I think I'll skip over this one. So,

370
00:37:11,560 --> 00:37:17,320
one place where it is being used is in PDE to construct candidate approximate solutions for

371
00:37:17,320 --> 00:37:20,840
various problems. So, like... So, there's a famous problem in fluid equations, you know,

372
00:37:20,840 --> 00:37:23,800
do the Navier-Stokes equations before we find our time? Do the Euler equations before we find our

373
00:37:23,800 --> 00:37:28,760
time? Navier-Stokes is still challenging, but Euler, there's been a lot of progress recently

374
00:37:29,480 --> 00:37:33,080
that people have been starting to construct self-similar solutions to the Euler equations

375
00:37:33,080 --> 00:37:38,840
with some asterisks, but the asterisks are slowly being removed. And one of the strategies of proof

376
00:37:38,840 --> 00:37:43,720
is to first construct an approximate solution, approximately self-similar solution that looks

377
00:37:43,720 --> 00:37:48,120
like it's going to blow up and then use some rigorous perturbation theory to perturb that to an

378
00:37:48,120 --> 00:37:55,320
actually blowing up solution. And machine learning has turned out to be useful to actually generate

379
00:37:55,320 --> 00:38:01,640
these approximate solutions, but I'm going to skip that for lack of time. I'll tell you,

380
00:38:01,640 --> 00:38:09,480
my favorite application of machine learning to mathematics is in knot theory. So, this is a

381
00:38:09,480 --> 00:38:16,200
recent work. So, knots are a very old subject in mathematics, and there's lots of... One of the

382
00:38:16,200 --> 00:38:20,280
fundamental things you study in knots is knot invariance. So, there's various statistics you

383
00:38:20,280 --> 00:38:26,360
can study. You can assign to a knot, which depends on the topology of the knot or the geometry of the

384
00:38:26,360 --> 00:38:31,640
knot. So, there's something called the signature, which is a combinatorial invariant. There are

385
00:38:31,640 --> 00:38:35,800
these famous polynomials, like the Jonas polynomial and Homfly polynomial. Then there are these things

386
00:38:35,800 --> 00:38:40,520
called hyperbolic invariance. The complement of a knot is often a hyperbolic space, and then you

387
00:38:40,520 --> 00:38:44,920
can talk about the volume of that space and some other geometric invariance. And so, there are

388
00:38:44,920 --> 00:38:50,280
these massive databases of knots. You can generate millions of knots, and you can compute all these

389
00:38:50,280 --> 00:38:55,880
invariance. But they come from very different areas of mathematics. So, some knot invariance

390
00:38:55,880 --> 00:39:00,760
come from combinatorics, some come from operative algebras, some come from hyperbolic geometry,

391
00:39:00,760 --> 00:39:11,080
and it's not obvious how they're related. But what these mathematicians did was that they trained

392
00:39:11,080 --> 00:39:14,920
a neural network on this big database of knots, and they studied the hyperbolic invariance and

393
00:39:14,920 --> 00:39:19,400
the signature, which is the combinatorial invariant. And they found that you could train the network

394
00:39:19,480 --> 00:39:23,720
to predict the signature from the hyperbolic invariant with, like, 99 percent accuracy.

395
00:39:25,000 --> 00:39:29,800
So, they used, like, half the data to train the set, and then half the data to test the set,

396
00:39:30,520 --> 00:39:37,480
to test the neural network. And so, what this told them is that there must be some connection.

397
00:39:37,480 --> 00:39:42,680
The signature of a knot must somehow be a function, or at least very closely related to a function,

398
00:39:42,680 --> 00:39:48,360
of a hyperbolic invariance. But the problem with neural nets is that they give you a function,

399
00:39:48,440 --> 00:39:51,160
a functional relationship, or at least a conjectural functional relationship. But

400
00:39:51,160 --> 00:39:56,920
it's this massively complicated function. It composes hundreds and hundreds of nonlinear

401
00:39:56,920 --> 00:40:00,680
functions together, and it doesn't give you any insight as to what the relationship is. It just

402
00:40:00,680 --> 00:40:07,080
shows you that there is a connection. But it's possible to do some analysis. So, they actually

403
00:40:07,080 --> 00:40:11,720
tried a very basic thing which happened to work. It's what's called a saliency analysis. So,

404
00:40:11,720 --> 00:40:17,800
this neural network gave them this function. Basically, they fed in 20 hyperbolic invariances

405
00:40:17,960 --> 00:40:22,760
as input, and the signature as output. So, it's basically a function from R to 20 to the integers,

406
00:40:22,760 --> 00:40:27,000
I think. And what they decided to do is that, once they had this function, they just tested

407
00:40:27,000 --> 00:40:32,440
how sensitive the function was to each of its inputs. So, they just varied one of the 20 variables,

408
00:40:32,440 --> 00:40:37,640
and they saw what happened to the output. And what they found was that only three of the 20

409
00:40:37,640 --> 00:40:41,960
variables were actually important, that only three of them had a significant influence on the function.

410
00:40:41,960 --> 00:40:47,480
The other 17 were basically not relevant at all. And so, they focused on those three variables,

411
00:40:47,480 --> 00:40:54,360
and they started plotting this function just restricted to those three variables. And that's

412
00:40:54,360 --> 00:40:57,960
just low enough to mention that you can eyeball the relationships. So, they started plotting

413
00:40:58,600 --> 00:41:03,160
the signature as a function of two or three of these variables and using color and so forth.

414
00:41:03,160 --> 00:41:08,760
And they started seeing patterns, and they could use these patterns to conjecture various

415
00:41:08,760 --> 00:41:14,280
inequalities and various relationships. It turns out that the first few inequalities they conjectured

416
00:41:14,360 --> 00:41:23,720
were false, and they could use the new net and the data set to confirm this. But with a bit of

417
00:41:23,720 --> 00:41:29,160
back and forth, they were able to eventually settle upon a correct conjecture relating these

418
00:41:29,160 --> 00:41:39,800
invariants to the signature. And it wasn't the invariants that they expected. So, yeah, they

419
00:41:39,800 --> 00:41:43,160
were expecting the hypervolume, for example, to be really important and not to be relevant at all.

420
00:41:45,000 --> 00:41:49,000
But once they found the right variables and the right conjectured inequality,

421
00:41:49,640 --> 00:41:54,120
it suggested the proof, and then they were able to actually find a rigorous proof of the inequality

422
00:41:54,120 --> 00:41:59,240
that they conjectured. So, it was a very nice back and forth between using the machine learning

423
00:42:00,600 --> 00:42:07,880
tool to suggest the way forward, but then going back to traditional mathematics to prove things.

424
00:42:08,840 --> 00:42:16,280
Okay. So, of course, the most high-profile development these days has been large language

425
00:42:16,280 --> 00:42:23,720
models like GPT. And sometimes they work really, really well. So, here's an example of GPT-4,

426
00:42:23,720 --> 00:42:28,280
which is OpenAI's most advanced large language model, actually solving a problem from the

427
00:42:28,280 --> 00:42:32,920
IMO, the International Mathematical Olympiad. And so, it's a question, you know, there's a function

428
00:42:32,920 --> 00:42:38,440
that obeys a search function equation. Can you prove, can you solve for the function?

429
00:42:38,440 --> 00:42:44,280
And it actually happens to give a completely correct proof. Now, this is an extremely cherry

430
00:42:44,280 --> 00:42:50,920
picked example. I think they tried, from this paper, they tried all the recent IMO problems,

431
00:42:50,920 --> 00:42:55,640
and they could solve like one percent of the problems of this method. You know, famously,

432
00:42:55,640 --> 00:43:01,160
it's bad even at basic arithmetic. You know, there's a, you ask it to solve seven times four

433
00:43:01,160 --> 00:43:05,400
plus eight times eight, and it'll give you the wrong answer. It gives you 120. And then it will

434
00:43:05,400 --> 00:43:10,200
keep going and say, and I'll explain why. And during the course of the explanation, it will

435
00:43:10,200 --> 00:43:16,520
actually make a mistake. And, yeah, and then you point out that they made a mistake and say,

436
00:43:16,520 --> 00:43:22,680
I'm sorry, the previous answer is incorrect. I mean, these large language models, they remind

437
00:43:22,680 --> 00:43:27,160
me a lot of, you know, if you have sort of a somewhat weak undergraduate student in office hours,

438
00:43:27,160 --> 00:43:32,520
and you ask them to solve a problem at the blackboard with no, with no age, you know,

439
00:43:32,520 --> 00:43:37,080
it will, you know, he or she will try to try their best, you know, and try to turn something

440
00:43:37,080 --> 00:43:42,520
that looks like a proof. But, yeah, they don't really have a good way of correcting themselves.

441
00:43:44,360 --> 00:43:48,200
So, you know, sometimes they work really well, but often they're very, very unreliable.

442
00:43:50,520 --> 00:43:56,680
But there's lots of interesting recent experiments where you can couple these language models to

443
00:43:56,680 --> 00:44:01,880
other much more reliable tools to do mathematics. So, for example, GPT now comes with plugins for

444
00:44:01,880 --> 00:44:09,240
Wolfram Alpha. So, now, if you ask GPT to do an ethnic calculation, it knows better than to try

445
00:44:09,240 --> 00:44:17,720
to do it itself. It will outsource it to Wolfram Alpha. Then there's more recent examples where

446
00:44:17,720 --> 00:44:23,240
people are coupling these large language models to a proof error file like Lean. So, you know,

447
00:44:23,320 --> 00:44:28,360
you ask it to prove as a statement, you know, prove that the union of two open sets is open.

448
00:44:29,400 --> 00:44:33,400
If you ask a raw large language model, it will give you a statement that

449
00:44:33,400 --> 00:44:37,640
a proof that looks like a proof. But there's lots of little logical errors in the proof, but

450
00:44:38,440 --> 00:44:43,560
you can force it to output in Lean, get Lean to compile, and if there's a completion error,

451
00:44:43,560 --> 00:44:47,080
it will send back the error to the large language model and have to correct it and

452
00:44:47,080 --> 00:44:51,320
create a feedback loop. And it can actually be used to solve

453
00:44:52,440 --> 00:44:57,560
roughly sort of undergraduate math homework level problems by this technique. But now,

454
00:44:57,560 --> 00:45:02,520
with 100% guarantee of accuracy, if it works, I mean, of course, sometimes it will just get stuck

455
00:45:02,520 --> 00:45:08,360
and give up because it can never get the Lean compiler to accept the argument. But

456
00:45:08,600 --> 00:45:18,360
it is beginning to make some headway. As I said before, I do find it can be useful

457
00:45:19,480 --> 00:45:26,360
as a muse, kind of like if you're just starting on a project, I recently was trying to

458
00:45:29,080 --> 00:45:34,520
prove some commentary identity. And I was thinking of using, I had some ideas in mind,

459
00:45:34,600 --> 00:45:40,120
I was going to use asymptotics, work with some special cases, but nothing was working. And I

460
00:45:40,120 --> 00:45:44,040
asked GPT for some suggestions. And it gave me some suggestions I was already thinking of,

461
00:45:44,840 --> 00:45:51,480
plus some suggestions that were either completely vacuous or wrong. But it did tell me that you

462
00:45:51,480 --> 00:45:56,680
should probably use generating functions, which I should have known. But at the time,

463
00:45:56,920 --> 00:46:04,600
it escaped me. And just with that hint, I was able to actually get them to work.

464
00:46:06,680 --> 00:46:12,280
So, you know, I mean, as just kind of someone to double check your thought process,

465
00:46:12,840 --> 00:46:17,240
it is sort of useful. Still not great, but it has some potential use.

466
00:46:19,560 --> 00:46:25,800
There was another tool which I do like, and I use more and more now. It's integrated into

467
00:46:25,880 --> 00:46:29,720
something called VS code, which is an editor to write code that can also be used for latex,

468
00:46:30,280 --> 00:46:35,240
something called GitHub co-pilot. It's basically an AI-powered autocomplete,

469
00:46:36,040 --> 00:46:42,040
and you type in your code or your latex, whatever. And based on all the text that you've

470
00:46:42,040 --> 00:46:48,360
already written, it will suggest a possible new sentence to generate. And so it can be very useful

471
00:46:48,360 --> 00:46:52,600
for code. You write down three lines of code, and it's just the fourth. And sometimes you'll get

472
00:46:52,600 --> 00:46:57,720
exactly right. Sometimes it's not exactly right. Sometimes it's complete rubbish. But then you

473
00:46:57,720 --> 00:47:01,240
can accept it, and it can save a lot of time, especially if you're doing some very menial

474
00:47:01,240 --> 00:47:06,360
code where you're repeating something over and over again. It works for latex. I wrote

475
00:47:07,880 --> 00:47:12,040
a latex blog post, actually, recently, where I was trying to estimate an integral, and I broke

476
00:47:12,040 --> 00:47:15,720
up the integral into three pieces. I said, okay, the first piece I can estimate by this technique,

477
00:47:15,720 --> 00:47:19,720
and I wrote down how to estimate this technique. And then the co-pilot just suggested how to

478
00:47:20,440 --> 00:47:25,320
estimate the second term. And actually, a completely correct argument. It was a modification of what

479
00:47:25,320 --> 00:47:32,760
I had already written. So it's very good at just modifying text that you've already appeared.

480
00:47:34,600 --> 00:47:42,280
And it's slowly being integrated into prefixes like Lean. So to the point where one line proves,

481
00:47:42,280 --> 00:47:46,440
two line proves, we can kind of get the AI to fill in for us. The kind of steps that that

482
00:47:47,400 --> 00:47:53,160
this is obvious or clearly this is true in a paper proof. I mean, not all of them, but we can get

483
00:47:53,160 --> 00:47:57,000
to the point where the AI can fill in a lot of them, and that will make proof formalization a lot

484
00:47:57,000 --> 00:48:05,960
faster. So there's a lot of potential. A lot of these technologies are very, very close to prime

485
00:48:05,960 --> 00:48:12,760
time, but not quite. It still took me a month to learn Lean and so forth. They're still not

486
00:48:12,760 --> 00:48:17,800
completely usable out of the box, but they are beginning to be more and more useful. And in

487
00:48:17,800 --> 00:48:22,360
surprising areas, you wouldn't have expected, say, not theory to benefit from these tools,

488
00:48:22,360 --> 00:48:28,280
but they do. They can't solve math problems on their own, except maybe undergraduate level

489
00:48:28,280 --> 00:48:33,800
homework questions, maybe, is the current level. But as an assistant, I think they can be very,

490
00:48:33,800 --> 00:48:39,560
very useful. They can generate conjectures. They can uncover connections that you wouldn't

491
00:48:39,560 --> 00:48:49,080
normally guess. Once proof automation becomes easier and scales better, we may be able to do

492
00:48:49,800 --> 00:48:56,920
completely new types of mathematics that we don't do right now. Right now, we prove

493
00:48:56,920 --> 00:49:01,240
theorems one at a time. I mean, we're still kind of craftsmen. We take one theorem and

494
00:49:01,240 --> 00:49:05,080
we prove it, and we take another thing and we prove it. Eventually, you could automate

495
00:49:05,880 --> 00:49:12,520
exploring the entire theorem space of millions of different statements, which ones are true,

496
00:49:12,520 --> 00:49:17,160
which ones are obviously false, and you could explore the geometry of theorems themselves.

497
00:49:18,600 --> 00:49:24,920
I think we're going to see a lot of different ways to do mathematics, and we're going to

498
00:49:24,920 --> 00:49:31,800
see a lot of different ways to make connections in fields that we don't currently. And it'll be

499
00:49:31,800 --> 00:49:35,320
a lot easier to collaborate and work in different areas of mathematics,

500
00:49:38,680 --> 00:49:43,960
yeah, especially because you can use these tools to sort of compartmentalize all these tasks,

501
00:49:43,960 --> 00:49:48,920
all these big, complicated projects into small pieces. And plus, also, these large language

502
00:49:48,920 --> 00:49:55,240
models actually will become very good at getting humans up to speed on any number of advanced

503
00:49:55,240 --> 00:50:05,640
mathematical topics. Okay, oops. Yeah, but it's still not quite there yet. I would say,

504
00:50:06,200 --> 00:50:10,440
if, for example, we prove formalization, it still takes about 10 to 20 times longer to

505
00:50:10,440 --> 00:50:15,880
formalize a proof than to do it by hand, but it's dropping. And I see no reason why this

506
00:50:15,880 --> 00:50:23,400
ratio cannot drop below one. Eventually, it will become faster to, eventually, when we just explain

507
00:50:23,480 --> 00:50:28,680
our proofs to GPT. And GPT will generate, you know, it will ask questions whenever we're unclear,

508
00:50:28,680 --> 00:50:33,240
but then it will just generate the latex and the lean for us. And we, you know,

509
00:50:33,240 --> 00:50:40,840
eventually, and, you know, and check our work at the same time. So I think this is all in the future.

510
00:50:43,080 --> 00:50:44,440
All right, so thanks for listening.

511
00:50:54,040 --> 00:51:04,040
Thank you. That was lovely. I think we have a couple minutes for a few short questions.

512
00:51:14,280 --> 00:51:16,040
Is there a microphone?

513
00:51:24,360 --> 00:51:32,120
Okay. There will also be a Q&A next door in 204 for a few minutes after when this is over.

514
00:51:32,920 --> 00:51:38,440
Are we using these mics, or are we calling up? Yes. That's great. I can't see the mic from here.

515
00:51:39,320 --> 00:51:45,240
So the person I called on can ask the question. Okay, sure. So one prediction that some people

516
00:51:45,240 --> 00:51:52,680
have bandied about, about advances with AI-assisted theorem provers is that we might enter a period

517
00:51:52,680 --> 00:51:58,440
where there's a proliferation of proofs for new theorems that are formally verifiable,

518
00:51:58,440 --> 00:52:01,640
but which we don't yet have the technology to translate into forms that are easily

519
00:52:01,640 --> 00:52:07,720
comprehensible by humans. Do you see this being an issue? Well, it already happens. You know,

520
00:52:07,720 --> 00:52:13,720
for example, this Boolean Pythagon-Triple's theorem, no human will ever read that proof.

521
00:52:14,680 --> 00:52:22,440
So, I mean, I think it's actually not that scary. I mean, you know, we rely on big numerical

522
00:52:22,440 --> 00:52:28,680
computations already in a lot of your mathematics. Of course, we would still want to have a human

523
00:52:28,680 --> 00:52:35,560
understandable proof, but as the not-example shows, you can take an incomprehensible computer

524
00:52:35,560 --> 00:52:41,880
proof and still analyze it and extract out from it a human proof. So I think that would be one of

525
00:52:41,880 --> 00:52:47,080
the ways you do mathematics in the future is to clarify computer-assisted proofs and make them

526
00:52:47,080 --> 00:52:54,840
human-understandable. Thank you. Can I ask you over there to ask a question? My question is

527
00:52:56,920 --> 00:53:02,200
kind of speculative, but I wanted to ask your opinion on the rule of human intuition going

528
00:53:02,200 --> 00:53:07,640
forward with this, because what a lot of what we talked about is formalization of human intuition

529
00:53:07,640 --> 00:53:12,360
into formal mathematics. I was wondering if you think that intuitive part of coming up with the

530
00:53:12,360 --> 00:53:20,520
idea for the proof itself could be automated in the near future? Not in the near future. As I said,

531
00:53:20,520 --> 00:53:26,600
these likely models can generate things that resemble intuition, but it has a lot of garbage.

532
00:53:28,520 --> 00:53:33,960
At the very low level of proving like one or two steps in a proof, we can use these proof

533
00:53:33,960 --> 00:53:43,000
assistants to sort of only pick out the good intuition and discard the bad one. But what

534
00:53:43,000 --> 00:53:49,000
large numbers are terrible at right now is differentiating good ideas from bad ideas.

535
00:53:49,000 --> 00:53:54,520
They just generate ideas. So unless there's another breakthrough in AI, I don't think this is going

536
00:53:54,520 --> 00:54:00,200
to happen any time soon. We'll take one more question from this side. So I was curious about

537
00:54:00,200 --> 00:54:06,760
the need for blueprints. Is it that the system doesn't know enough definitions yet or is the proof

538
00:54:06,760 --> 00:54:13,240
space too big? Some combination thereof? No, it's more of an organization for the humans. If you

539
00:54:13,240 --> 00:54:20,680
want to coordinate 20 people to work on the same project, many of the people who work on these

540
00:54:20,680 --> 00:54:25,880
projects, they don't understand the whole proof. So you need some structure to split it up into

541
00:54:25,880 --> 00:54:31,720
really small pieces, atomic pieces that are self-contained for individual people to work on.

542
00:54:31,720 --> 00:54:37,720
So it's not for the computer. The computer can compile anything. It's for the humans to

543
00:54:37,720 --> 00:54:42,920
break up a complicated problem into lots of easy pieces. Kind of like how divisional labor works

544
00:54:42,920 --> 00:54:48,680
in like modern industries, like factories. Okay, I'm going to invite all the other people

545
00:54:48,680 --> 00:54:55,960
waiting to ask questions to join us in room 204 briefly. And let's thank Terry for a lovely talk.

