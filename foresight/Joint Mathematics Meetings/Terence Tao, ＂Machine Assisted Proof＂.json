{"text": " Hello. Hello-hello. Good afternoon. I'm Bryna Kraw, President of the AMS, and it's my pleasure to welcome you to the colloquium lectures. These are the oldest lectures at the meetings of the AMS. The first meeting was held in 1895. The second in 1896 were the first colloquium lectures actually took place, and those were at Northwestern University, and since that's my home institution, I can't help but mention them. The list of speakers is a veritable who's who in mathematics, including Birkhoff, Morse, von Neumann, Tarski, Czern, Milner, Smell, Nirenberg, Tate, and the list of people I've left off is, well, equally prestigious to those that I included, and amongst them was also our speaker today's advisor, Stein, Ilya Stein. So with that, I'll turn to the introduction of Terry Tau. Terry may be somebody who doesn't need introduction. After all, he's been in a crossword puzzle as a clue in the New York Times crossword puzzle. I don't want to use up all his time by listing the awards he's won, but I could. I'll give you just a short list of the highlights of the Fields Medal in 2006. I'm a Carther Fellowship. He's a fellow of the Royal Society, the Australian Academy of Sciences, the American Academy of Arts and Science, and a member of the National Academy of Science, and of course he's a fellow of the AMS, most important of those distinctions. He has over 350 publications, meaning he hasn't slept in a few years, and this includes numerous highly influential texts, and he has more than 50 collaborators, and maybe that's when I counted last week, so I don't know, maybe this week there's more. He's also one of the broadest researchers in mathematics, covering interests from pure to applied, and I won't list all of the subjects. But it's not only research, he also serves the profession in numerous ways, and starting in 2021 took on a very substantial role. He was appointed by President Biden as a member of the President's Council of Advisors on Science and Technology. So he's a force. He's mentored over 20 PhD students, and I could go on and on. So I've known Terry for a while, since about 2004 or 2005 when we first met, but one of my fondest memories of him was at a party that I was a co-host for. In 2008, it was the election night, and Terry was sitting in the corner November 2008 on his computer on some website that was giving election returns, announcing the states before the TV was. It was really impressive, and it's just one of my fondest memories. Anyway, I won't keep you any longer. It's my pleasure to introduce Terry. All right. Thank you very much, Bryna. I'm very happy to be here at this JMM to give this lecture. It's always nice to be up in San Francisco. So are we talking about what I think is a really exciting development in mathematics that's going to shape our future, which is really the development over the last few years of lots of technologies to make machines and computers help us do math much more effectively. Now, to some sense, this is not new. We have used both machines and computers, and I use the terms slightly differently. We've used both, actually, for centuries, really. Nowadays, computers and machines, when we talk about machine-assisted mathematics and computer-assisted mathematics, they're sort of synonymous. Initially, they weren't because computers used to be human, and then they were mechanical and then finally electronic. So, for example, one of the earliest use of computers was to build tables. So, for example, the large tables of Napier and so forth were basically built by lots and lots of human computers, and those are the earliest examples of somehow computer-assisted mathematics, and tables are still really important today. I mean, not so much the log tables anymore, but a lot of what we call experimental mathematics is based on creating lots and lots of large tables of interesting things, especially in number theory. So, for example, famously, the Gendo and Gauss built tables of prime numbers, and they used that to conjecture the prime number theorem, which is, of course, now a theorem. And similarly, in the 60s, Bertrand's Wittendeyer created lots and lots of big tables of the decurs and the ranks and so forth, and this was a key input in formalizing the famous BSD conjecture. And maybe the biggest table of all in mathematics is the OEIS, online encyclopedia of mathematical sequences. So, there's hundreds of thousands of integer sequences, and every day, I think mathematicians discover unexpected connections, or maybe rediscover an existing connection. I myself use the OEIS. If there's a quantity, which I know there's a formula for, but I can't remember it, I can just compute the first five elements put in the OEIS, I can usually find it. And most recently, people are starting to use large databases of mathematical objects as training data for neural networks, and so I'll give you an example of that later. So, that's one very storied and antique way of using computers in mathematics. The other big use, of course, is in numerics or scientific computing, and that's also a very old subject. Arguably, the first big scientific computation was in the 20s when Lorenz was asked to model the fluid flow for the construction of a new dike in the Netherlands, and so he assembled a team of human computers, basically, to model what would happen to the water flow and so forth. It's notable for the introduction, he's almost the first place where floating point arithmetic was introduced. And, of course, we use scientific computing nowadays to model PDEs, to solve large systems of equations, and, of course, we use them for computer algebra packages, you know, magma, maple, sage, and so forth. Yeah, you want to do a big, you know, numerical integration or algebraic, you know, computer Gribner bases, whatever, you know, we routinely do this now, or cross mathematics. Of course, the numerics are sometimes inaccurate, you know, there are round off errors and other possible problems, but there are ways to make the combination more rigorous. For example, instead of floating point arithmetic, if you use interval arithmetic, if you represent numbers by error ranges, a lower and upper bound, and you keep those bounds like rational numbers, like finite precision, like infinite precision, then you can avoid errors, at least in principle, at the cost maybe of making the runtime longer. More recently, there are more advanced algebra packages than just sort of the standard things you get in sage or Mathematica. They think of SAT solvers, satisfiability solvers, or satisfiably modulo theory solvers, SMT solvers. So what they, so a SAT solver gives, you feed it a whole statement, a bunch of propositions, P1, P2, P3, and so forth, and you see that P1 and P2, all P3 is true, P3 and P4, and not P5, one of them is true, and so forth, and it will try to see if there's a solution or not, and many problems can be phrased as a SAT problem. So if you have a general purpose SAT solver, you can potentially just feed it into such a program and solve the problem for you, then there are these more sophisticated variants, SMT solvers, where you also feed laws of algebra so you have some variables, and you assume that there are certain laws, these variables obey certain laws, and you ask, can you deduce a new law from the laws you already have? So those are potentially very powerful, and unfortunately, SAT solverability is an NP complete problem, and once you get hundreds and hundreds of these propositions, it becomes very hard to actually solve these problems, but still they are very useful. Here's a typical application of a SAT solver, so a few years ago, there was this famous problem in commentaries called the Boolean Pythagorean Triples problem, and so the problem is this, you take natural numbers and you color them into two color classes, red and blue, and you ask, is it always the case that one of these color classes contains a Pythagorean triple, three numbers A, B and C such that A squared plus B squared equals C squared, and it turns out to be, we don't have like a human proof of the statement, but we know it's true now because of a SAT solver, so there was a massive computation that says that if you only go up to 7,824, then you can't do this, there was a way to partition the numbers from 1 to 7,824 into two classes, neither of which contain a Pythagorean triple, but once you go up to 7,825, no matter how you do it, you must always get one of the two color classes must have a Pythagorean triple. In principle, this is a finite computation because there's only two to seven, two to five different ways to compute different partitions, and so you just check each one, but that is computationally unfeasible, but with a SAT solver, you can rephrase this problem as a free satisfiability problem, and it's not just a matter of running the solver, you have to optimize it and so forth, but it is possible to actually solve this problem, and it gives you a certificate, it gives you a proof, and actually this is, at the time, it was actually the world's longest proof. The proof certificate, first of all, it took four CPU years to generate, and it's a 200 terabyte proof, although it is compressible. I think it is still the second largest proof ever generated. Okay, so that's, but this I still consider is a more classical way of using computers, but what I think is exciting is that there are a lot of new ways that we can use computers to do mathematics. Of course, there's still the boring ways, you know, we use computers to do emails and write latex and so forth, I don't mean that, but there are sort of three new modalities, which individually, they still have somewhat niche applications, but what I find really interesting is that they can potentially be combined together, and the combination of them, it could be something in general purpose that actually a lot of us could use. So the three sort of new things. So the first is machine learning algorithms, where you have a problem, and if you have a lot of data for that problem, you can set some sort of specialized neural network to train it on the data, and it can generate counter examples for you, try to generate connections, and so people are beginning to use this in all kinds of fields of mathematics, I'll give you some examples later. So that's one development. Maybe the most high-profile development is large language models like chat GPT, these are very general purpose models that can understand natural language. To date, they have not been directly used for so much mathematics, I'm sure many of you have tried talking to GPT, asking it to solve your favorite math problem, and it will give you some so plausible looking nonsense in general. But when used correctly, I think they do offer a lot of potential. I mean, I have found occasionally that these models can be useful for suggesting proof techniques that I wasn't initially thinking of, or to suggest related topics or literature. They're actually most useful for sort of secondary tasks. Okay, so for actually doing math research, they still haven't really proved themselves, but for doing things like writing code or organizing a bibliography, like a lot of the other more routine tasks that we do actually, these LLMs are very useful. But the third new technology, which has been around for two decades, but has only now sort of becoming ready for primetime, are these formal proof assistants, which are languages designed to verify, or to verify, or many of them are actually designed to verify electronics, but they can also verify mathematical proofs. And crucially, they can also verify the output of large language models, which they can complement, they can fix the biggest defect in principle of the LLMs. And they allow new types of ways to do mathematics, in particular, they can allow really large scale collaborations, which we really can't do without these formal proof assistants. And they can also generate data, which can be used for the other two the other two technologies. So I'll talk about each of these three things separately, but but they haven't, there's beginning to be experiments to combine them together, but they're still kind of prototypes right now. But I think the paradigm of using all of them, and also combining with the computer algebra systems and the SAP solvers into one integrated package, it could really be quite a powerful methodical assistant. Okay, so let's talk, I think my first slides begin with proof assistants. So the computer-assisted proofs are not new, famously the full color theorem in 1976 was was proven partly by computer. Although at the time, it was by modern standards, we will not call it a fully formalized proof, the 1976 proof. The proof was this long document with lots and lots of subclaims, which a lot of them were verified by hand, and a lot of them were verified by both electronic computers and human computers. And I think one of the author's daughter actually had to go through 500 graphs and check that they all had this discharging property. And actually, it had a lot of mistakes too. So there's a lot of minor errors in the proof. They're all correctable, but it really will not meet the standards today of a computer-verified proof. The first proof, okay, so it took 30 to 20 years for an alternative proof of full color theorem to be verified, and this proof is closer to being completely formal. So it's about 10-15 pages of human readable argument, and then it reduces to this very specific computation, which anyone can just run a computer program in whatever language they like to verify it. So it was a computer-verified proof, but it still wasn't a formal proof. It wasn't written in a formal proof language, which was designed to only output correct proofs. And that had to wait until the early 2000s when Werner and Gontier actually formalized the entire full color theorem in one of the early proof assistant languages, COC, in this case. So now we know with 100% certainty that the full color theorem is correct. Well, modulo, trusting the compiler of COC. All right. Another famous machine assistant proof, well, actually initially human proof, but eventually computer-verified was the proof of the coupler conjecture. So the coupler conjecture is a statement about how efficient that you can pack unit spheres in the plane, and so there's a natural way to stack unit spheres, and it's the way that you see oranges stacked in the grocery store. It's called the hexagonal closed packing, and there's also a dual packing with the same density called the cubic closed packing. And they have a certain density, pi over three over two, and this was conjectured to be the densest packing. So this is an annoyingly hard statement to prove. So it's an optimization problem in infinitely many variables. So each each sphere has a different location, and there's an infinite number of spheres. So you're trying to prove an inequality involving an infinite number of variables involving solving infinite number constraints. So it doesn't immediately lend itself to computer verification. But even in the 50s, it was realized that possibly this could be done by some sort of brute force computation. And so Toth proposed the following paradigm. So every time you see a packing, it comes with what's called a Voronoi decomposition. So every sphere comes with a Voronoi cell, which is this polytope of all the points that are closer to the center of that of that sphere than to all the other spheres. And there's partitions space into all these little polyhedron, these Voronoi cells. And there are various relationships between the volumes of these different cells. There's only so many spheres that you can pack next to one reference sphere, and this creates all these kind of constraints. And so the hope was that if you could gather enough inequalities between adjacent Voronoi cells, the volumes of adjacent Voronoi cells, maybe every such system inequalities, in principle, gives you an upper bound on the density of the sphere packing. And in principle, if you get enough of these inequalities, maybe you could actually get the optimal bound of three or two. So people tried this approach for many, many years, including some false attempts. But they were not able to actually make this approach work. But Thomas Hales, and then later, with a collaborator, was able to adapt this approach to make it work in a series of papers from 94-98. But they had to modify the strategy quite a lot. So instead of using the Voronoi decomposition, there was a more complicated decomposition that was used. And instead of using volume, they had to define this new score function attached to each polyhedron. But basically, the strategy was the same. And he was able to prove lots and lots of linear inequalities between the scores of adjacent polyhedra. And then just using linear programming was able to then get a bound. And with the right choice of score and the right choice of partition, it was the optimal bound. It's a very flexible method, because you have lots of ways you can do the partition and lots of ways that you can do the score. But the problem is that it was too flexible. So here's a quote from Hales. It says that Simon Ferguson, who was Hales' collaborator, and I realized every time we encounter difficulty solving the minimization problem, we get just a scoring function to score the difficulty. The function became more complicated, but with each change, we could cut months or years from our work. This incessant fiddling was unpopular with my colleagues. Every time I presented my work in progress at a conference, I was minimizing a different function. Even worse, the function was moderately incompatible with earlier papers, and this required going back and patching the earlier papers. So the proof was a mess, basically. They did eventually finish it in 98, and they were able to derive the Kepler conjecture from a linear programming computation from a very complicated optimization program. Initially, it was done by hand, but with the increased complexity, there was no choice but to make it more and more computer-assisted. So when the proof was announced, it was a combination of 250 pages of notes and lots and lots of gigabytes of programs and data, and it was famously hard to referee. It took four years for annals to referee the paper with a panel of pro referees, and even then, the panel was only 99 percent certain of the correctness of the proof, and they could certify the corrections, the calculations. Because, I mean, in principle, it was all doable, but the referees have to implement all these different computer calculations themselves, but it was eventually accepted. But clearly, there was this big asterisk. There was a lot of controversy about whether this was really a valid proof, and so this was one of the the first really high-profile uses of formal proof assistance, because this was a result in which there was serious doubt about the correctness. So they created, so Hales in 2003 initiated a project to write down this massive proof in a completely formalized way so that a standard proof assistant could verify it. He estimated it would take 20 years to make this work, and so he had, he gathered 21 collaborators. It actually only took 11 years, but yeah, eventually what they did was that they first created a blueprint, you know, so a human readable version of the proof breaking things up into very, very small steps, and then they formalized each step by bit, and it was finally done, and then there was a, yeah, so they published a paper about the formalization, and that only appeared in 2017. So this was sort of the state-of-the-art of formalization, you know, as of say 20 years ago, you know, like it was possible to formalize big complicated results, but it took an enormous amount of effort, you know, not something which you would do routinely. There was a more recent effort in a similar spirit by Peter Schultzer, he called it the liquid tensor experiment. So Schultzer introduced this theory of condensed mathematics, which is, all right, this is really far from my own area of expertise, but basically there are certain problems with, so certain types of mathematics you want to work in various categories, like categories of topological obedient groups and topological vector spaces, and there's a problem that they're not obedient categories, that they don't, they don't have, I have a good notion of kernel and cold kernel, and things don't work out properly. So he proposed replacing all of these standard categories with a more fancy version called a condensed category, which has better category theoretic properties, and so the hope is that you could use a lot more high-powered algebra to attack, to handle things with topological structure or analytical structure, like function spaces, for example, binoc spaces. But in order for you to work, there's a certain vanishing theorem, which I've written there, but I cannot explain to you. Okay, so there's a certain category, condescending groups, and there's an x-group involving p-binoc spaces that has to vanish, and this vanishing theorem is needed in order for all of the rest of the theory to actually be useful, if you want to apply it to function analysis in particular. And so Schultzer, what a blog post about this is that, you know, I spent much of 2019 obsessed with the proofless theorem, almost getting crazy over it. In the end, we were able to get an argument down on paper, but I think no one else has ever dared to look at the details of this, and so I still have some lingering doubts. With this hope, with this theorem, the hope that the condensed formulas can be fruitfully applied to function analysis stands a force, being 99% sure is not enough because this theorem is of the most fundamental importance. He says, I think this may be my most important theorem to date, which is a really big claim, actually. Better be sure it's correct. So this was another case where there was a great desire to formalize the proof. So he asked publicly on the internet for help to formalize this in a modern proof of this in language called Lean. And so again, about 20 people, I think, joined this project to help out. And it, so Lean is based on, it has this philosophy where it has this single huge library of mathematical theorems, like the fundamental calculus or the classification of finite building groups, like a lot of the basic theorems of mathematics are already formalized in this big library. And the idea is to just keep building on this library and adding to it with additional projects. But one of the problems with that short of the face was that a lot of the tools that, basic tools that you needed, like homological algebra and chief theory and top box theory, weren't actually in the library yet. So actually part of what they had to do was actually set up the foundations of that theory and formalize it in the library first. But it basically was done in about two years. In one year, they formalized a key sub theorem. And then the whole thing was formalized about a year afterwards. It did have some, in addition to sort of making reassuring Peter that it was, everything was correct, there was some other minor benefits. So first of all, there were actually a few minor errors in the proof that were discovered in the formalization progress. But they go refixed also some small simplifications. There was one big simplification actually. So they, the original proof used something very complicated, which I also don't understand, called the Breen-Deline resolution. But in the course of formalizing it, it was too hard to actually formalize this theory. But they found that there was a weaker version of this theory, which was good enough to formalize this application. But this was actually a major discovery because this weaker theory could also potentially attack some other problems that the Breen-Deline resolution is not well suited for. And now the math library is much, much better in, it has a lot of support for homological algebra and sheath theory and lots of other things, which helped other formalization projects become easier. I got interested in this formalization a few months ago. So, oh, hang on. I should say, like with the Kepler experiment, so you don't just directly transfer a paper from, you know, the archive or something into Breen. What we found is that it really helps to create first an intermediate document. So halfway between a human readable proof and a formal proof, which we call a blueprint. So it looks like a human proof and is written in a version of latex. But like each step in the proof is linked to a lemma in lean. And so it's very tightly integrated. It has a very nice feature. It can create dependency graphs, which I'll show you an example later. You can see which parts of the proof have been formalized, which ones are not, and what depends on what. It actually, it gives a lot of structure to the process of writing a paper, which, you know, right now we do by hand without sort of much assistance, really. Yeah, Schultz has said that the process of writing the blueprint really helped him understand the proof a lot better. And actually, people have been also going the other way. There's also software that takes formal proofs, which are written in something that looks like computer code, and turns them back into human readable proofs. Here is a prototype software. So there's a there's a theorem in topology. Okay, you know, I think it's a if a function is continuous on a dense set, and there's some extra extra properties, then it's continuous extends to a continuous function globally. And the proof is written was written first in lean, but then it was already converted into a human proof, human readable proof. But again, where every step you can expand and contract, like if there's a step you want to explain more, you can double click it, and it will expand out, give all the justification. And you can click at any given point in the proof, and it will tell you what the hypotheses are currently, what you can approve, and you can give a lot of structure. I think eventually textbooks, this is maybe a good format, say for undergraduate textbooks to have, you know, proofs of say, you know, tricky films and analysis or something written in a way where, you know, in a not in a static way, but where you can really, you know, drill down all the way down to the basic axioms if you wanted to. Okay. One thing I mean, one thing notable about these formalization projects is that they allow massive collaborations. So, you know, in other sciences, people collaborate with 20 people, 500 people, you know, 5,000 people. But in mathematics, still, we don't really collaborate in really large groups. You know, five is kind of the maximum, usually. And part of it is that, you know, if you want to collaborate with 20 people, if you don't already know these 20 people, and you don't completely trust that they're doing correct mathematics, you know, it's very difficult because you have to, everyone has to check everyone else's contribution. But with a proof assistant, the proof assistant provides a formal guarantee. And so, you can really collaborate with 20, hundreds of people that you've never met before. And you don't need to trust them. Because they upload code and the lean compiler verifies, yes, this actually solves what they claimed. And then you can accept it or it doesn't. So, I experienced this myself because I got interested in formalization a few months ago. So, I'd recently proven a combinatorial theorem with Tim Gowes, Ben Green, and Freddie Manners. It solved something called the polynomial primonrugia conjecture. The precise conjecture is not important for this talk. It's a conjecture in combinatorics. You have a subset of a finite through vector space of small doubling. And you want to show that it is all, it is very close to actually being a coset of a subgroup. This was a statement that was in combinatorics that was highly sought after. So, we had a 33-page paper recently proving this. Mostly self-contained, fortunately. So, I thought I was a good candidate for formalization. So, I asked on an internet forum, specializing in lean, for help to formalize it. And then, again, like 20, 30 people joined in and actually only took three weeks to formalize. The time taken to formalize these projects is going down quite a bit. So, in particular, the time taken to formalize this project was roughly about the same time it took for us to write the paper in the first place. And by the time we submitted the paper, we were able to put as a note that the proof is actually being formalized. So, as I said, it uses a single blueprint which splits up the proof into lots and lots of little pieces. And so, it creates this nice little dependency graph. So, this is a picture of the graph at an early stage of the project. So, there's no down the bottom called PFR. Maybe only the people in the front can see it. That's the final theorem that we're trying to prove. At the time of the screenshot, this was white, which means that it had not been proved, but not even been stated properly. So, in fact, even before you prove the theorem, you have to first state it, and then you have to make definitions and so forth. Blue are things that have been defined, but not yet proven, and green have been things that have been proven. And so, at any point in time, some bubbles are white, some are blue, and some results depend on some others. And so, the way the formalization proceeded was that different people just grabbed a note that was ready to be formalized because maybe all the previous results that it depended on were formalized, and they just formalized that one step independent of everybody else. And you didn't really need to coordinate too much. I mean, we did coordinate on an internet forum, but each little note is completely specified. There's a very precise definition and a very precise thing to prove, and we just need the proof. And we really don't care exactly what the proof is. I mean, it has to be not massively long. So, people just picked up individual claims. Here's one very simple claim. There's a certain functional called ruja distance d, and there's a very simple claim that it was non-negative. And this turns out to have a three-line proof, assuming some previous facts that were also on the blueprint. And so, people just sort of picked up these things separately, and over time, it just filled up. This is what a typical step in the proof looks like. This is the proof that this ruja distance is non-negative. This is the code in the lean. It looks kind of a little bit like mathematics, but it is actually... Once you think of the syntax, it actually reads... It's like reading latex. The first time you read latex, it looks like a whole bunch of mess of symbols. But it's actually... Every line corresponds to a sentence in mathematical English. For example, the first line, if you want to prove that this distance is positive, it suffices to prove that tricidistance is positive. So, you work with tricidistance, because it turns out there's another lemma that bounds tricidistance. So, yeah, every step, actually, it corresponds reasonably well to the way we think about mathematical proofs. Yeah. So, fortunately, the proof didn't reveal any major issues. There were some typos, but very, very minor picked up. And we also... There were some things we needed... Again, we needed to add to the math library. The math library... We used Shannon entropy, and Shannon entropy was not in the math library. Now it is. One thing about formalization is that it still takes longer to formalize proofs than to write them. But... And maybe about 10 times longer, I would say. Which is unfortunate. If it was faster to formalize to write proofs formally than to write them by hand, I think then there will be a tipping point. And then I think a lot of us would switch over just because they guarantee a correctness. So, we're not there yet. It is definitely getting a lot faster than it was before. But one thing that we noticed is that, actually, while it still takes a long time to write a proof, if you want to modify a proof to change some parameters and make it a little bit better, that can be done much quicker in the formal setting than with a paper proof. Because with paper proof, if you change all your little parameters and so forth, you'll likely make also mistakes and you go back. And it's a very annoying process. But it's actually much easier to modify an existing formal proof than to create one from scratch. For example, we were able... There's a constant 12 exponent that appeared in our proof. A few days afterwards, someone posted an improvement in the argument, improved 12 to 11, and in a few days we were able to adapt the formal proof to do that as well. Yeah. And because you can collaborate, I think the process is scalable. There was just recently Kevin Buzzard, for example, a five-year project. The aim is to formalize Fermat's last theory of the proof. And that is quite a big goal because there are lots and lots of sub-portions that have had no formal proof at all. So, that, I think, will be quite an ambitious project. But I think it's now doable. It wasn't doable five years ago, but now I think it is. Okay. So, that's four more proofs. Another... Okay. I might actually have to speed up a little bit. Okay. So, all right. So, machine learning, using neural networks has become also more and more common place in different areas of mathematics. I think I'll skip over this one. So, one place where it is being used is in PDE to construct candidate approximate solutions for various problems. So, like... So, there's a famous problem in fluid equations, you know, do the Navier-Stokes equations before we find our time? Do the Euler equations before we find our time? Navier-Stokes is still challenging, but Euler, there's been a lot of progress recently that people have been starting to construct self-similar solutions to the Euler equations with some asterisks, but the asterisks are slowly being removed. And one of the strategies of proof is to first construct an approximate solution, approximately self-similar solution that looks like it's going to blow up and then use some rigorous perturbation theory to perturb that to an actually blowing up solution. And machine learning has turned out to be useful to actually generate these approximate solutions, but I'm going to skip that for lack of time. I'll tell you, my favorite application of machine learning to mathematics is in knot theory. So, this is a recent work. So, knots are a very old subject in mathematics, and there's lots of... One of the fundamental things you study in knots is knot invariance. So, there's various statistics you can study. You can assign to a knot, which depends on the topology of the knot or the geometry of the knot. So, there's something called the signature, which is a combinatorial invariant. There are these famous polynomials, like the Jonas polynomial and Homfly polynomial. Then there are these things called hyperbolic invariance. The complement of a knot is often a hyperbolic space, and then you can talk about the volume of that space and some other geometric invariance. And so, there are these massive databases of knots. You can generate millions of knots, and you can compute all these invariance. But they come from very different areas of mathematics. So, some knot invariance come from combinatorics, some come from operative algebras, some come from hyperbolic geometry, and it's not obvious how they're related. But what these mathematicians did was that they trained a neural network on this big database of knots, and they studied the hyperbolic invariance and the signature, which is the combinatorial invariant. And they found that you could train the network to predict the signature from the hyperbolic invariant with, like, 99 percent accuracy. So, they used, like, half the data to train the set, and then half the data to test the set, to test the neural network. And so, what this told them is that there must be some connection. The signature of a knot must somehow be a function, or at least very closely related to a function, of a hyperbolic invariance. But the problem with neural nets is that they give you a function, a functional relationship, or at least a conjectural functional relationship. But it's this massively complicated function. It composes hundreds and hundreds of nonlinear functions together, and it doesn't give you any insight as to what the relationship is. It just shows you that there is a connection. But it's possible to do some analysis. So, they actually tried a very basic thing which happened to work. It's what's called a saliency analysis. So, this neural network gave them this function. Basically, they fed in 20 hyperbolic invariances as input, and the signature as output. So, it's basically a function from R to 20 to the integers, I think. And what they decided to do is that, once they had this function, they just tested how sensitive the function was to each of its inputs. So, they just varied one of the 20 variables, and they saw what happened to the output. And what they found was that only three of the 20 variables were actually important, that only three of them had a significant influence on the function. The other 17 were basically not relevant at all. And so, they focused on those three variables, and they started plotting this function just restricted to those three variables. And that's just low enough to mention that you can eyeball the relationships. So, they started plotting the signature as a function of two or three of these variables and using color and so forth. And they started seeing patterns, and they could use these patterns to conjecture various inequalities and various relationships. It turns out that the first few inequalities they conjectured were false, and they could use the new net and the data set to confirm this. But with a bit of back and forth, they were able to eventually settle upon a correct conjecture relating these invariants to the signature. And it wasn't the invariants that they expected. So, yeah, they were expecting the hypervolume, for example, to be really important and not to be relevant at all. But once they found the right variables and the right conjectured inequality, it suggested the proof, and then they were able to actually find a rigorous proof of the inequality that they conjectured. So, it was a very nice back and forth between using the machine learning tool to suggest the way forward, but then going back to traditional mathematics to prove things. Okay. So, of course, the most high-profile development these days has been large language models like GPT. And sometimes they work really, really well. So, here's an example of GPT-4, which is OpenAI's most advanced large language model, actually solving a problem from the IMO, the International Mathematical Olympiad. And so, it's a question, you know, there's a function that obeys a search function equation. Can you prove, can you solve for the function? And it actually happens to give a completely correct proof. Now, this is an extremely cherry picked example. I think they tried, from this paper, they tried all the recent IMO problems, and they could solve like one percent of the problems of this method. You know, famously, it's bad even at basic arithmetic. You know, there's a, you ask it to solve seven times four plus eight times eight, and it'll give you the wrong answer. It gives you 120. And then it will keep going and say, and I'll explain why. And during the course of the explanation, it will actually make a mistake. And, yeah, and then you point out that they made a mistake and say, I'm sorry, the previous answer is incorrect. I mean, these large language models, they remind me a lot of, you know, if you have sort of a somewhat weak undergraduate student in office hours, and you ask them to solve a problem at the blackboard with no, with no age, you know, it will, you know, he or she will try to try their best, you know, and try to turn something that looks like a proof. But, yeah, they don't really have a good way of correcting themselves. So, you know, sometimes they work really well, but often they're very, very unreliable. But there's lots of interesting recent experiments where you can couple these language models to other much more reliable tools to do mathematics. So, for example, GPT now comes with plugins for Wolfram Alpha. So, now, if you ask GPT to do an ethnic calculation, it knows better than to try to do it itself. It will outsource it to Wolfram Alpha. Then there's more recent examples where people are coupling these large language models to a proof error file like Lean. So, you know, you ask it to prove as a statement, you know, prove that the union of two open sets is open. If you ask a raw large language model, it will give you a statement that a proof that looks like a proof. But there's lots of little logical errors in the proof, but you can force it to output in Lean, get Lean to compile, and if there's a completion error, it will send back the error to the large language model and have to correct it and create a feedback loop. And it can actually be used to solve roughly sort of undergraduate math homework level problems by this technique. But now, with 100% guarantee of accuracy, if it works, I mean, of course, sometimes it will just get stuck and give up because it can never get the Lean compiler to accept the argument. But it is beginning to make some headway. As I said before, I do find it can be useful as a muse, kind of like if you're just starting on a project, I recently was trying to prove some commentary identity. And I was thinking of using, I had some ideas in mind, I was going to use asymptotics, work with some special cases, but nothing was working. And I asked GPT for some suggestions. And it gave me some suggestions I was already thinking of, plus some suggestions that were either completely vacuous or wrong. But it did tell me that you should probably use generating functions, which I should have known. But at the time, it escaped me. And just with that hint, I was able to actually get them to work. So, you know, I mean, as just kind of someone to double check your thought process, it is sort of useful. Still not great, but it has some potential use. There was another tool which I do like, and I use more and more now. It's integrated into something called VS code, which is an editor to write code that can also be used for latex, something called GitHub co-pilot. It's basically an AI-powered autocomplete, and you type in your code or your latex, whatever. And based on all the text that you've already written, it will suggest a possible new sentence to generate. And so it can be very useful for code. You write down three lines of code, and it's just the fourth. And sometimes you'll get exactly right. Sometimes it's not exactly right. Sometimes it's complete rubbish. But then you can accept it, and it can save a lot of time, especially if you're doing some very menial code where you're repeating something over and over again. It works for latex. I wrote a latex blog post, actually, recently, where I was trying to estimate an integral, and I broke up the integral into three pieces. I said, okay, the first piece I can estimate by this technique, and I wrote down how to estimate this technique. And then the co-pilot just suggested how to estimate the second term. And actually, a completely correct argument. It was a modification of what I had already written. So it's very good at just modifying text that you've already appeared. And it's slowly being integrated into prefixes like Lean. So to the point where one line proves, two line proves, we can kind of get the AI to fill in for us. The kind of steps that that this is obvious or clearly this is true in a paper proof. I mean, not all of them, but we can get to the point where the AI can fill in a lot of them, and that will make proof formalization a lot faster. So there's a lot of potential. A lot of these technologies are very, very close to prime time, but not quite. It still took me a month to learn Lean and so forth. They're still not completely usable out of the box, but they are beginning to be more and more useful. And in surprising areas, you wouldn't have expected, say, not theory to benefit from these tools, but they do. They can't solve math problems on their own, except maybe undergraduate level homework questions, maybe, is the current level. But as an assistant, I think they can be very, very useful. They can generate conjectures. They can uncover connections that you wouldn't normally guess. Once proof automation becomes easier and scales better, we may be able to do completely new types of mathematics that we don't do right now. Right now, we prove theorems one at a time. I mean, we're still kind of craftsmen. We take one theorem and we prove it, and we take another thing and we prove it. Eventually, you could automate exploring the entire theorem space of millions of different statements, which ones are true, which ones are obviously false, and you could explore the geometry of theorems themselves. I think we're going to see a lot of different ways to do mathematics, and we're going to see a lot of different ways to make connections in fields that we don't currently. And it'll be a lot easier to collaborate and work in different areas of mathematics, yeah, especially because you can use these tools to sort of compartmentalize all these tasks, all these big, complicated projects into small pieces. And plus, also, these large language models actually will become very good at getting humans up to speed on any number of advanced mathematical topics. Okay, oops. Yeah, but it's still not quite there yet. I would say, if, for example, we prove formalization, it still takes about 10 to 20 times longer to formalize a proof than to do it by hand, but it's dropping. And I see no reason why this ratio cannot drop below one. Eventually, it will become faster to, eventually, when we just explain our proofs to GPT. And GPT will generate, you know, it will ask questions whenever we're unclear, but then it will just generate the latex and the lean for us. And we, you know, eventually, and, you know, and check our work at the same time. So I think this is all in the future. All right, so thanks for listening. Thank you. That was lovely. I think we have a couple minutes for a few short questions. Is there a microphone? Okay. There will also be a Q&A next door in 204 for a few minutes after when this is over. Are we using these mics, or are we calling up? Yes. That's great. I can't see the mic from here. So the person I called on can ask the question. Okay, sure. So one prediction that some people have bandied about, about advances with AI-assisted theorem provers is that we might enter a period where there's a proliferation of proofs for new theorems that are formally verifiable, but which we don't yet have the technology to translate into forms that are easily comprehensible by humans. Do you see this being an issue? Well, it already happens. You know, for example, this Boolean Pythagon-Triple's theorem, no human will ever read that proof. So, I mean, I think it's actually not that scary. I mean, you know, we rely on big numerical computations already in a lot of your mathematics. Of course, we would still want to have a human understandable proof, but as the not-example shows, you can take an incomprehensible computer proof and still analyze it and extract out from it a human proof. So I think that would be one of the ways you do mathematics in the future is to clarify computer-assisted proofs and make them human-understandable. Thank you. Can I ask you over there to ask a question? My question is kind of speculative, but I wanted to ask your opinion on the rule of human intuition going forward with this, because what a lot of what we talked about is formalization of human intuition into formal mathematics. I was wondering if you think that intuitive part of coming up with the idea for the proof itself could be automated in the near future? Not in the near future. As I said, these likely models can generate things that resemble intuition, but it has a lot of garbage. At the very low level of proving like one or two steps in a proof, we can use these proof assistants to sort of only pick out the good intuition and discard the bad one. But what large numbers are terrible at right now is differentiating good ideas from bad ideas. They just generate ideas. So unless there's another breakthrough in AI, I don't think this is going to happen any time soon. We'll take one more question from this side. So I was curious about the need for blueprints. Is it that the system doesn't know enough definitions yet or is the proof space too big? Some combination thereof? No, it's more of an organization for the humans. If you want to coordinate 20 people to work on the same project, many of the people who work on these projects, they don't understand the whole proof. So you need some structure to split it up into really small pieces, atomic pieces that are self-contained for individual people to work on. So it's not for the computer. The computer can compile anything. It's for the humans to break up a complicated problem into lots of easy pieces. Kind of like how divisional labor works in like modern industries, like factories. Okay, I'm going to invite all the other people waiting to ask questions to join us in room 204 briefly. And let's thank Terry for a lovely talk.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 13.92, "text": " Hello. Hello-hello. Good afternoon. I'm Bryna Kraw, President of the AMS, and it's my pleasure", "tokens": [50364, 2425, 13, 2425, 12, 675, 1913, 13, 2205, 6499, 13, 286, 478, 12812, 629, 591, 5131, 11, 3117, 295, 264, 6475, 50, 11, 293, 309, 311, 452, 6834, 51060], "temperature": 0.0, "avg_logprob": -0.2220516687707056, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.04776516556739807}, {"id": 1, "seek": 0, "start": 13.92, "end": 19.56, "text": " to welcome you to the colloquium lectures. These are the oldest lectures at the meetings", "tokens": [51060, 281, 2928, 291, 281, 264, 1263, 29826, 2197, 16564, 13, 1981, 366, 264, 14026, 16564, 412, 264, 8410, 51342], "temperature": 0.0, "avg_logprob": -0.2220516687707056, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.04776516556739807}, {"id": 2, "seek": 0, "start": 19.56, "end": 27.84, "text": " of the AMS. The first meeting was held in 1895. The second in 1896 were the first colloquium", "tokens": [51342, 295, 264, 6475, 50, 13, 440, 700, 3440, 390, 5167, 294, 2443, 15718, 13, 440, 1150, 294, 2443, 22962, 645, 264, 700, 1263, 29826, 2197, 51756], "temperature": 0.0, "avg_logprob": -0.2220516687707056, "compression_ratio": 1.5164835164835164, "no_speech_prob": 0.04776516556739807}, {"id": 3, "seek": 2784, "start": 27.84, "end": 32.44, "text": " lectures actually took place, and those were at Northwestern University, and since that's", "tokens": [50364, 16564, 767, 1890, 1081, 11, 293, 729, 645, 412, 26068, 1248, 3535, 11, 293, 1670, 300, 311, 50594], "temperature": 0.0, "avg_logprob": -0.17886489571876896, "compression_ratio": 1.475206611570248, "no_speech_prob": 0.015601190738379955}, {"id": 4, "seek": 2784, "start": 32.44, "end": 38.72, "text": " my home institution, I can't help but mention them. The list of speakers is a veritable", "tokens": [50594, 452, 1280, 7818, 11, 286, 393, 380, 854, 457, 2152, 552, 13, 440, 1329, 295, 9518, 307, 257, 1306, 16772, 50908], "temperature": 0.0, "avg_logprob": -0.17886489571876896, "compression_ratio": 1.475206611570248, "no_speech_prob": 0.015601190738379955}, {"id": 5, "seek": 2784, "start": 38.72, "end": 48.44, "text": " who's who in mathematics, including Birkhoff, Morse, von Neumann, Tarski, Czern, Milner,", "tokens": [50908, 567, 311, 567, 294, 18666, 11, 3009, 7145, 74, 1289, 602, 11, 5146, 405, 11, 2957, 1734, 449, 969, 11, 314, 685, 2984, 11, 383, 89, 1248, 11, 7036, 1193, 11, 51394], "temperature": 0.0, "avg_logprob": -0.17886489571876896, "compression_ratio": 1.475206611570248, "no_speech_prob": 0.015601190738379955}, {"id": 6, "seek": 2784, "start": 48.44, "end": 55.120000000000005, "text": " Smell, Nirenberg, Tate, and the list of people I've left off is, well, equally prestigious", "tokens": [51394, 3915, 898, 11, 426, 18833, 6873, 11, 314, 473, 11, 293, 264, 1329, 295, 561, 286, 600, 1411, 766, 307, 11, 731, 11, 12309, 33510, 51728], "temperature": 0.0, "avg_logprob": -0.17886489571876896, "compression_ratio": 1.475206611570248, "no_speech_prob": 0.015601190738379955}, {"id": 7, "seek": 5512, "start": 55.12, "end": 61.64, "text": " to those that I included, and amongst them was also our speaker today's advisor, Stein,", "tokens": [50364, 281, 729, 300, 286, 5556, 11, 293, 12918, 552, 390, 611, 527, 8145, 965, 311, 19161, 11, 29453, 11, 50690], "temperature": 0.0, "avg_logprob": -0.17462949855353244, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.011226225644350052}, {"id": 8, "seek": 5512, "start": 61.64, "end": 71.4, "text": " Ilya Stein. So with that, I'll turn to the introduction of Terry Tau. Terry may be somebody", "tokens": [50690, 286, 45106, 29453, 13, 407, 365, 300, 11, 286, 603, 1261, 281, 264, 9339, 295, 21983, 314, 1459, 13, 21983, 815, 312, 2618, 51178], "temperature": 0.0, "avg_logprob": -0.17462949855353244, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.011226225644350052}, {"id": 9, "seek": 5512, "start": 71.4, "end": 75.36, "text": " who doesn't need introduction. After all, he's been in a crossword puzzle as a clue", "tokens": [51178, 567, 1177, 380, 643, 9339, 13, 2381, 439, 11, 415, 311, 668, 294, 257, 3278, 7462, 12805, 382, 257, 13602, 51376], "temperature": 0.0, "avg_logprob": -0.17462949855353244, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.011226225644350052}, {"id": 10, "seek": 5512, "start": 75.36, "end": 83.16, "text": " in the New York Times crossword puzzle. I don't want to use up all his time by listing", "tokens": [51376, 294, 264, 1873, 3609, 11366, 3278, 7462, 12805, 13, 286, 500, 380, 528, 281, 764, 493, 439, 702, 565, 538, 22161, 51766], "temperature": 0.0, "avg_logprob": -0.17462949855353244, "compression_ratio": 1.5486725663716814, "no_speech_prob": 0.011226225644350052}, {"id": 11, "seek": 8316, "start": 83.2, "end": 88.08, "text": " the awards he's won, but I could. I'll give you just a short list of the highlights of", "tokens": [50366, 264, 15193, 415, 311, 1582, 11, 457, 286, 727, 13, 286, 603, 976, 291, 445, 257, 2099, 1329, 295, 264, 14254, 295, 50610], "temperature": 0.0, "avg_logprob": -0.13736995468791732, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.4225292503833771}, {"id": 12, "seek": 8316, "start": 88.08, "end": 94.6, "text": " the Fields Medal in 2006. I'm a Carther Fellowship. He's a fellow of the Royal Society, the Australian", "tokens": [50610, 264, 48190, 42437, 294, 14062, 13, 286, 478, 257, 2741, 616, 40011, 1210, 13, 634, 311, 257, 7177, 295, 264, 12717, 13742, 11, 264, 13337, 50936], "temperature": 0.0, "avg_logprob": -0.13736995468791732, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.4225292503833771}, {"id": 13, "seek": 8316, "start": 94.6, "end": 98.24, "text": " Academy of Sciences, the American Academy of Arts and Science, and a member of the National", "tokens": [50936, 11735, 295, 21108, 11, 264, 2665, 11735, 295, 12407, 293, 8976, 11, 293, 257, 4006, 295, 264, 4862, 51118], "temperature": 0.0, "avg_logprob": -0.13736995468791732, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.4225292503833771}, {"id": 14, "seek": 8316, "start": 98.24, "end": 102.8, "text": " Academy of Science, and of course he's a fellow of the AMS, most important of those", "tokens": [51118, 11735, 295, 8976, 11, 293, 295, 1164, 415, 311, 257, 7177, 295, 264, 6475, 50, 11, 881, 1021, 295, 729, 51346], "temperature": 0.0, "avg_logprob": -0.13736995468791732, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.4225292503833771}, {"id": 15, "seek": 8316, "start": 102.8, "end": 111.28, "text": " distinctions. He has over 350 publications, meaning he hasn't slept in a few years, and", "tokens": [51346, 1483, 49798, 13, 634, 575, 670, 18065, 25618, 11, 3620, 415, 6132, 380, 17400, 294, 257, 1326, 924, 11, 293, 51770], "temperature": 0.0, "avg_logprob": -0.13736995468791732, "compression_ratio": 1.6715867158671587, "no_speech_prob": 0.4225292503833771}, {"id": 16, "seek": 11128, "start": 111.4, "end": 116.72, "text": " this includes numerous highly influential texts, and he has more than 50 collaborators,", "tokens": [50370, 341, 5974, 12546, 5405, 22215, 15765, 11, 293, 415, 575, 544, 813, 2625, 39789, 11, 50636], "temperature": 0.0, "avg_logprob": -0.1543649366532249, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.014556054957211018}, {"id": 17, "seek": 11128, "start": 116.72, "end": 120.16, "text": " and maybe that's when I counted last week, so I don't know, maybe this week there's", "tokens": [50636, 293, 1310, 300, 311, 562, 286, 20150, 1036, 1243, 11, 370, 286, 500, 380, 458, 11, 1310, 341, 1243, 456, 311, 50808], "temperature": 0.0, "avg_logprob": -0.1543649366532249, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.014556054957211018}, {"id": 18, "seek": 11128, "start": 120.16, "end": 126.64, "text": " more. He's also one of the broadest researchers in mathematics, covering interests from pure", "tokens": [50808, 544, 13, 634, 311, 611, 472, 295, 264, 4152, 377, 10309, 294, 18666, 11, 10322, 8847, 490, 6075, 51132], "temperature": 0.0, "avg_logprob": -0.1543649366532249, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.014556054957211018}, {"id": 19, "seek": 11128, "start": 126.64, "end": 135.76, "text": " to applied, and I won't list all of the subjects. But it's not only research, he also serves", "tokens": [51132, 281, 6456, 11, 293, 286, 1582, 380, 1329, 439, 295, 264, 13066, 13, 583, 309, 311, 406, 787, 2132, 11, 415, 611, 13451, 51588], "temperature": 0.0, "avg_logprob": -0.1543649366532249, "compression_ratio": 1.5657894736842106, "no_speech_prob": 0.014556054957211018}, {"id": 20, "seek": 13576, "start": 135.76, "end": 142.6, "text": " the profession in numerous ways, and starting in 2021 took on a very substantial role. He", "tokens": [50364, 264, 7032, 294, 12546, 2098, 11, 293, 2891, 294, 7201, 1890, 322, 257, 588, 16726, 3090, 13, 634, 50706], "temperature": 0.0, "avg_logprob": -0.16907098791101477, "compression_ratio": 1.4710743801652892, "no_speech_prob": 0.1793631911277771}, {"id": 21, "seek": 13576, "start": 142.6, "end": 147.04, "text": " was appointed by President Biden as a member of the President's Council of Advisors on", "tokens": [50706, 390, 17653, 538, 3117, 9877, 382, 257, 4006, 295, 264, 3117, 311, 7076, 295, 31407, 830, 322, 50928], "temperature": 0.0, "avg_logprob": -0.16907098791101477, "compression_ratio": 1.4710743801652892, "no_speech_prob": 0.1793631911277771}, {"id": 22, "seek": 13576, "start": 147.04, "end": 153.84, "text": " Science and Technology. So he's a force. He's mentored over 20 PhD students, and I could", "tokens": [50928, 8976, 293, 15037, 13, 407, 415, 311, 257, 3464, 13, 634, 311, 3074, 2769, 670, 945, 14476, 1731, 11, 293, 286, 727, 51268], "temperature": 0.0, "avg_logprob": -0.16907098791101477, "compression_ratio": 1.4710743801652892, "no_speech_prob": 0.1793631911277771}, {"id": 23, "seek": 13576, "start": 153.84, "end": 162.84, "text": " go on and on. So I've known Terry for a while, since about 2004 or 2005 when we first met,", "tokens": [51268, 352, 322, 293, 322, 13, 407, 286, 600, 2570, 21983, 337, 257, 1339, 11, 1670, 466, 15817, 420, 14394, 562, 321, 700, 1131, 11, 51718], "temperature": 0.0, "avg_logprob": -0.16907098791101477, "compression_ratio": 1.4710743801652892, "no_speech_prob": 0.1793631911277771}, {"id": 24, "seek": 16284, "start": 163.24, "end": 171.24, "text": " but one of my fondest memories of him was at a party that I was a co-host for. In 2008,", "tokens": [50384, 457, 472, 295, 452, 9557, 377, 8495, 295, 796, 390, 412, 257, 3595, 300, 286, 390, 257, 598, 12, 6037, 337, 13, 682, 10389, 11, 50784], "temperature": 0.0, "avg_logprob": -0.13696202825992665, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.004947606939822435}, {"id": 25, "seek": 16284, "start": 171.24, "end": 178.16, "text": " it was the election night, and Terry was sitting in the corner November 2008 on his computer on", "tokens": [50784, 309, 390, 264, 6618, 1818, 11, 293, 21983, 390, 3798, 294, 264, 4538, 7674, 10389, 322, 702, 3820, 322, 51130], "temperature": 0.0, "avg_logprob": -0.13696202825992665, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.004947606939822435}, {"id": 26, "seek": 16284, "start": 178.16, "end": 184.68, "text": " some website that was giving election returns, announcing the states before the TV was. It", "tokens": [51130, 512, 3144, 300, 390, 2902, 6618, 11247, 11, 28706, 264, 4368, 949, 264, 3558, 390, 13, 467, 51456], "temperature": 0.0, "avg_logprob": -0.13696202825992665, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.004947606939822435}, {"id": 27, "seek": 16284, "start": 184.68, "end": 190.44, "text": " was really impressive, and it's just one of my fondest memories. Anyway, I won't keep you any", "tokens": [51456, 390, 534, 8992, 11, 293, 309, 311, 445, 472, 295, 452, 9557, 377, 8495, 13, 5684, 11, 286, 1582, 380, 1066, 291, 604, 51744], "temperature": 0.0, "avg_logprob": -0.13696202825992665, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.004947606939822435}, {"id": 28, "seek": 19044, "start": 191.24, "end": 193.32, "text": " longer. It's my pleasure to introduce Terry.", "tokens": [50404, 2854, 13, 467, 311, 452, 6834, 281, 5366, 21983, 13, 50508], "temperature": 0.0, "avg_logprob": -0.2622327377547079, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.0009320269455201924}, {"id": 29, "seek": 19044, "start": 193.32, "end": 210.52, "text": " All right. Thank you very much, Bryna. I'm very happy to be here at this JMM to give this lecture.", "tokens": [50508, 1057, 558, 13, 1044, 291, 588, 709, 11, 12812, 629, 13, 286, 478, 588, 2055, 281, 312, 510, 412, 341, 508, 17365, 281, 976, 341, 7991, 13, 51368], "temperature": 0.0, "avg_logprob": -0.2622327377547079, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.0009320269455201924}, {"id": 30, "seek": 19044, "start": 210.52, "end": 216.6, "text": " It's always nice to be up in San Francisco. So are we talking about what I think is a really", "tokens": [51368, 467, 311, 1009, 1481, 281, 312, 493, 294, 5271, 12279, 13, 407, 366, 321, 1417, 466, 437, 286, 519, 307, 257, 534, 51672], "temperature": 0.0, "avg_logprob": -0.2622327377547079, "compression_ratio": 1.3801169590643274, "no_speech_prob": 0.0009320269455201924}, {"id": 31, "seek": 21660, "start": 216.6, "end": 222.76, "text": " exciting development in mathematics that's going to shape our future, which is really the", "tokens": [50364, 4670, 3250, 294, 18666, 300, 311, 516, 281, 3909, 527, 2027, 11, 597, 307, 534, 264, 50672], "temperature": 0.0, "avg_logprob": -0.12936211809699918, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0018715522019192576}, {"id": 32, "seek": 21660, "start": 224.51999999999998, "end": 229.79999999999998, "text": " development over the last few years of lots of technologies to make machines and computers", "tokens": [50760, 3250, 670, 264, 1036, 1326, 924, 295, 3195, 295, 7943, 281, 652, 8379, 293, 10807, 51024], "temperature": 0.0, "avg_logprob": -0.12936211809699918, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0018715522019192576}, {"id": 33, "seek": 21660, "start": 231.88, "end": 240.12, "text": " help us do math much more effectively. Now, to some sense, this is not new. We have used", "tokens": [51128, 854, 505, 360, 5221, 709, 544, 8659, 13, 823, 11, 281, 512, 2020, 11, 341, 307, 406, 777, 13, 492, 362, 1143, 51540], "temperature": 0.0, "avg_logprob": -0.12936211809699918, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0018715522019192576}, {"id": 34, "seek": 21660, "start": 240.12, "end": 245.24, "text": " both machines and computers, and I use the terms slightly differently. We've used both,", "tokens": [51540, 1293, 8379, 293, 10807, 11, 293, 286, 764, 264, 2115, 4748, 7614, 13, 492, 600, 1143, 1293, 11, 51796], "temperature": 0.0, "avg_logprob": -0.12936211809699918, "compression_ratio": 1.6451612903225807, "no_speech_prob": 0.0018715522019192576}, {"id": 35, "seek": 24524, "start": 245.24, "end": 250.52, "text": " actually, for centuries, really. Nowadays, computers and machines, when we talk about", "tokens": [50364, 767, 11, 337, 13926, 11, 534, 13, 28908, 11, 10807, 293, 8379, 11, 562, 321, 751, 466, 50628], "temperature": 0.0, "avg_logprob": -0.14094952670010655, "compression_ratio": 1.8700787401574803, "no_speech_prob": 0.0005537295364774764}, {"id": 36, "seek": 24524, "start": 250.52, "end": 254.84, "text": " machine-assisted mathematics and computer-assisted mathematics, they're sort of synonymous. Initially,", "tokens": [50628, 3479, 12, 640, 33250, 18666, 293, 3820, 12, 640, 33250, 18666, 11, 436, 434, 1333, 295, 5451, 18092, 13, 29446, 11, 50844], "temperature": 0.0, "avg_logprob": -0.14094952670010655, "compression_ratio": 1.8700787401574803, "no_speech_prob": 0.0005537295364774764}, {"id": 37, "seek": 24524, "start": 254.84, "end": 259.32, "text": " they weren't because computers used to be human, and then they were mechanical and then finally", "tokens": [50844, 436, 4999, 380, 570, 10807, 1143, 281, 312, 1952, 11, 293, 550, 436, 645, 12070, 293, 550, 2721, 51068], "temperature": 0.0, "avg_logprob": -0.14094952670010655, "compression_ratio": 1.8700787401574803, "no_speech_prob": 0.0005537295364774764}, {"id": 38, "seek": 24524, "start": 259.32, "end": 267.40000000000003, "text": " electronic. So, for example, one of the earliest use of computers was to build tables. So, for", "tokens": [51068, 10092, 13, 407, 11, 337, 1365, 11, 472, 295, 264, 20573, 764, 295, 10807, 390, 281, 1322, 8020, 13, 407, 11, 337, 51472], "temperature": 0.0, "avg_logprob": -0.14094952670010655, "compression_ratio": 1.8700787401574803, "no_speech_prob": 0.0005537295364774764}, {"id": 39, "seek": 24524, "start": 267.40000000000003, "end": 272.12, "text": " example, the large tables of Napier and so forth were basically built by lots and lots of human", "tokens": [51472, 1365, 11, 264, 2416, 8020, 295, 18287, 811, 293, 370, 5220, 645, 1936, 3094, 538, 3195, 293, 3195, 295, 1952, 51708], "temperature": 0.0, "avg_logprob": -0.14094952670010655, "compression_ratio": 1.8700787401574803, "no_speech_prob": 0.0005537295364774764}, {"id": 40, "seek": 27212, "start": 272.12, "end": 277.48, "text": " computers, and those are the earliest examples of somehow computer-assisted mathematics,", "tokens": [50364, 10807, 11, 293, 729, 366, 264, 20573, 5110, 295, 6063, 3820, 12, 640, 33250, 18666, 11, 50632], "temperature": 0.0, "avg_logprob": -0.16709120614188058, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00028960485360585153}, {"id": 41, "seek": 27212, "start": 278.52, "end": 282.44, "text": " and tables are still really important today. I mean, not so much the log tables anymore,", "tokens": [50684, 293, 8020, 366, 920, 534, 1021, 965, 13, 286, 914, 11, 406, 370, 709, 264, 3565, 8020, 3602, 11, 50880], "temperature": 0.0, "avg_logprob": -0.16709120614188058, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00028960485360585153}, {"id": 42, "seek": 27212, "start": 283.56, "end": 290.28000000000003, "text": " but a lot of what we call experimental mathematics is based on creating lots and lots of large", "tokens": [50936, 457, 257, 688, 295, 437, 321, 818, 17069, 18666, 307, 2361, 322, 4084, 3195, 293, 3195, 295, 2416, 51272], "temperature": 0.0, "avg_logprob": -0.16709120614188058, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00028960485360585153}, {"id": 43, "seek": 27212, "start": 290.28000000000003, "end": 295.56, "text": " tables of interesting things, especially in number theory. So, for example, famously,", "tokens": [51272, 8020, 295, 1880, 721, 11, 2318, 294, 1230, 5261, 13, 407, 11, 337, 1365, 11, 34360, 11, 51536], "temperature": 0.0, "avg_logprob": -0.16709120614188058, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00028960485360585153}, {"id": 44, "seek": 27212, "start": 295.56, "end": 301.56, "text": " the Gendo and Gauss built tables of prime numbers, and they used that to conjecture the prime number", "tokens": [51536, 264, 460, 3999, 293, 10384, 2023, 3094, 8020, 295, 5835, 3547, 11, 293, 436, 1143, 300, 281, 416, 1020, 540, 264, 5835, 1230, 51836], "temperature": 0.0, "avg_logprob": -0.16709120614188058, "compression_ratio": 1.7790697674418605, "no_speech_prob": 0.00028960485360585153}, {"id": 45, "seek": 30156, "start": 301.56, "end": 307.24, "text": " theorem, which is, of course, now a theorem. And similarly, in the 60s, Bertrand's Wittendeyer", "tokens": [50364, 20904, 11, 597, 307, 11, 295, 1164, 11, 586, 257, 20904, 13, 400, 14138, 11, 294, 264, 4060, 82, 11, 29594, 3699, 311, 343, 593, 521, 2030, 260, 50648], "temperature": 0.0, "avg_logprob": -0.21488850788005348, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.0008888854063116014}, {"id": 46, "seek": 30156, "start": 308.84, "end": 314.84000000000003, "text": " created lots and lots of big tables of the decurs and the ranks and so forth, and this was a key", "tokens": [50728, 2942, 3195, 293, 3195, 295, 955, 8020, 295, 264, 979, 2156, 293, 264, 21406, 293, 370, 5220, 11, 293, 341, 390, 257, 2141, 51028], "temperature": 0.0, "avg_logprob": -0.21488850788005348, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.0008888854063116014}, {"id": 47, "seek": 30156, "start": 314.84000000000003, "end": 321.16, "text": " input in formalizing the famous BSD conjecture. And maybe the biggest table of all in mathematics", "tokens": [51028, 4846, 294, 9860, 3319, 264, 4618, 363, 23969, 416, 1020, 540, 13, 400, 1310, 264, 3880, 3199, 295, 439, 294, 18666, 51344], "temperature": 0.0, "avg_logprob": -0.21488850788005348, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.0008888854063116014}, {"id": 48, "seek": 30156, "start": 321.16, "end": 326.76, "text": " is the OEIS, online encyclopedia of mathematical sequences. So, there's hundreds of thousands", "tokens": [51344, 307, 264, 422, 36, 2343, 11, 2950, 465, 34080, 47795, 295, 18894, 22978, 13, 407, 11, 456, 311, 6779, 295, 5383, 51624], "temperature": 0.0, "avg_logprob": -0.21488850788005348, "compression_ratio": 1.563265306122449, "no_speech_prob": 0.0008888854063116014}, {"id": 49, "seek": 32676, "start": 327.15999999999997, "end": 332.2, "text": " of integer sequences, and every day, I think mathematicians discover unexpected connections,", "tokens": [50384, 295, 24922, 22978, 11, 293, 633, 786, 11, 286, 519, 32811, 2567, 4411, 13106, 9271, 11, 50636], "temperature": 0.0, "avg_logprob": -0.14624934500836312, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012824732111766934}, {"id": 50, "seek": 32676, "start": 332.84, "end": 340.52, "text": " or maybe rediscover an existing connection. I myself use the OEIS. If there's a quantity,", "tokens": [50668, 420, 1310, 2182, 40080, 364, 6741, 4984, 13, 286, 2059, 764, 264, 422, 36, 2343, 13, 759, 456, 311, 257, 11275, 11, 51052], "temperature": 0.0, "avg_logprob": -0.14624934500836312, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012824732111766934}, {"id": 51, "seek": 32676, "start": 340.52, "end": 343.96, "text": " which I know there's a formula for, but I can't remember it, I can just compute the first five", "tokens": [51052, 597, 286, 458, 456, 311, 257, 8513, 337, 11, 457, 286, 393, 380, 1604, 309, 11, 286, 393, 445, 14722, 264, 700, 1732, 51224], "temperature": 0.0, "avg_logprob": -0.14624934500836312, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012824732111766934}, {"id": 52, "seek": 32676, "start": 343.96, "end": 351.4, "text": " elements put in the OEIS, I can usually find it. And most recently, people are starting to use", "tokens": [51224, 4959, 829, 294, 264, 422, 36, 2343, 11, 286, 393, 2673, 915, 309, 13, 400, 881, 3938, 11, 561, 366, 2891, 281, 764, 51596], "temperature": 0.0, "avg_logprob": -0.14624934500836312, "compression_ratio": 1.5897435897435896, "no_speech_prob": 0.0012824732111766934}, {"id": 53, "seek": 35140, "start": 351.47999999999996, "end": 356.52, "text": " large databases of mathematical objects as training data for neural networks, and so I'll", "tokens": [50368, 2416, 22380, 295, 18894, 6565, 382, 3097, 1412, 337, 18161, 9590, 11, 293, 370, 286, 603, 50620], "temperature": 0.0, "avg_logprob": -0.09262590298707458, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0007846418884582818}, {"id": 54, "seek": 35140, "start": 356.52, "end": 365.79999999999995, "text": " give you an example of that later. So, that's one very storied and antique way of using computers", "tokens": [50620, 976, 291, 364, 1365, 295, 300, 1780, 13, 407, 11, 300, 311, 472, 588, 5967, 1091, 293, 41220, 636, 295, 1228, 10807, 51084], "temperature": 0.0, "avg_logprob": -0.09262590298707458, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0007846418884582818}, {"id": 55, "seek": 35140, "start": 367.32, "end": 371.88, "text": " in mathematics. The other big use, of course, is in numerics or scientific computing,", "tokens": [51160, 294, 18666, 13, 440, 661, 955, 764, 11, 295, 1164, 11, 307, 294, 7866, 1167, 420, 8134, 15866, 11, 51388], "temperature": 0.0, "avg_logprob": -0.09262590298707458, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0007846418884582818}, {"id": 56, "seek": 35140, "start": 372.52, "end": 379.23999999999995, "text": " and that's also a very old subject. Arguably, the first big scientific computation was in the", "tokens": [51420, 293, 300, 311, 611, 257, 588, 1331, 3983, 13, 48560, 1188, 11, 264, 700, 955, 8134, 24903, 390, 294, 264, 51756], "temperature": 0.0, "avg_logprob": -0.09262590298707458, "compression_ratio": 1.6311111111111112, "no_speech_prob": 0.0007846418884582818}, {"id": 57, "seek": 37924, "start": 379.24, "end": 386.28000000000003, "text": " 20s when Lorenz was asked to model the fluid flow for the construction of a new dike in the", "tokens": [50364, 945, 82, 562, 37162, 89, 390, 2351, 281, 2316, 264, 9113, 3095, 337, 264, 6435, 295, 257, 777, 1026, 330, 294, 264, 50716], "temperature": 0.0, "avg_logprob": -0.1072755293412642, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00046215960173867643}, {"id": 58, "seek": 37924, "start": 386.28000000000003, "end": 392.36, "text": " Netherlands, and so he assembled a team of human computers, basically, to model what would happen", "tokens": [50716, 20873, 11, 293, 370, 415, 24204, 257, 1469, 295, 1952, 10807, 11, 1936, 11, 281, 2316, 437, 576, 1051, 51020], "temperature": 0.0, "avg_logprob": -0.1072755293412642, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00046215960173867643}, {"id": 59, "seek": 37924, "start": 392.36, "end": 399.08, "text": " to the water flow and so forth. It's notable for the introduction, he's almost the first place", "tokens": [51020, 281, 264, 1281, 3095, 293, 370, 5220, 13, 467, 311, 22556, 337, 264, 9339, 11, 415, 311, 1920, 264, 700, 1081, 51356], "temperature": 0.0, "avg_logprob": -0.1072755293412642, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00046215960173867643}, {"id": 60, "seek": 37924, "start": 399.08, "end": 404.04, "text": " where floating point arithmetic was introduced. And, of course, we use scientific computing", "tokens": [51356, 689, 12607, 935, 42973, 390, 7268, 13, 400, 11, 295, 1164, 11, 321, 764, 8134, 15866, 51604], "temperature": 0.0, "avg_logprob": -0.1072755293412642, "compression_ratio": 1.5864978902953586, "no_speech_prob": 0.00046215960173867643}, {"id": 61, "seek": 40404, "start": 404.04, "end": 411.08000000000004, "text": " nowadays to model PDEs, to solve large systems of equations, and, of course, we use them for", "tokens": [50364, 13434, 281, 2316, 10464, 20442, 11, 281, 5039, 2416, 3652, 295, 11787, 11, 293, 11, 295, 1164, 11, 321, 764, 552, 337, 50716], "temperature": 0.0, "avg_logprob": -0.1787714958190918, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.001918574795126915}, {"id": 62, "seek": 40404, "start": 411.08000000000004, "end": 416.92, "text": " computer algebra packages, you know, magma, maple, sage, and so forth. Yeah, you want to do a big,", "tokens": [50716, 3820, 21989, 17401, 11, 291, 458, 11, 2258, 1696, 11, 31191, 11, 19721, 11, 293, 370, 5220, 13, 865, 11, 291, 528, 281, 360, 257, 955, 11, 51008], "temperature": 0.0, "avg_logprob": -0.1787714958190918, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.001918574795126915}, {"id": 63, "seek": 40404, "start": 417.88, "end": 422.44, "text": " you know, numerical integration or algebraic, you know, computer Gribner bases, whatever,", "tokens": [51056, 291, 458, 11, 29054, 10980, 420, 21989, 299, 11, 291, 458, 11, 3820, 460, 2024, 1193, 17949, 11, 2035, 11, 51284], "temperature": 0.0, "avg_logprob": -0.1787714958190918, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.001918574795126915}, {"id": 64, "seek": 40404, "start": 423.64000000000004, "end": 430.44, "text": " you know, we routinely do this now, or cross mathematics. Of course, the numerics are sometimes", "tokens": [51344, 291, 458, 11, 321, 40443, 360, 341, 586, 11, 420, 3278, 18666, 13, 2720, 1164, 11, 264, 7866, 1167, 366, 2171, 51684], "temperature": 0.0, "avg_logprob": -0.1787714958190918, "compression_ratio": 1.6905829596412556, "no_speech_prob": 0.001918574795126915}, {"id": 65, "seek": 43044, "start": 430.44, "end": 436.92, "text": " inaccurate, you know, there are round off errors and other possible problems, but there are ways", "tokens": [50364, 46443, 11, 291, 458, 11, 456, 366, 3098, 766, 13603, 293, 661, 1944, 2740, 11, 457, 456, 366, 2098, 50688], "temperature": 0.0, "avg_logprob": -0.13744292596373894, "compression_ratio": 1.8047808764940239, "no_speech_prob": 0.011778072454035282}, {"id": 66, "seek": 43044, "start": 436.92, "end": 440.76, "text": " to make the combination more rigorous. For example, instead of floating point arithmetic,", "tokens": [50688, 281, 652, 264, 6562, 544, 29882, 13, 1171, 1365, 11, 2602, 295, 12607, 935, 42973, 11, 50880], "temperature": 0.0, "avg_logprob": -0.13744292596373894, "compression_ratio": 1.8047808764940239, "no_speech_prob": 0.011778072454035282}, {"id": 67, "seek": 43044, "start": 440.76, "end": 445.96, "text": " if you use interval arithmetic, if you represent numbers by error ranges, a lower and upper bound,", "tokens": [50880, 498, 291, 764, 15035, 42973, 11, 498, 291, 2906, 3547, 538, 6713, 22526, 11, 257, 3126, 293, 6597, 5472, 11, 51140], "temperature": 0.0, "avg_logprob": -0.13744292596373894, "compression_ratio": 1.8047808764940239, "no_speech_prob": 0.011778072454035282}, {"id": 68, "seek": 43044, "start": 446.84, "end": 452.36, "text": " and you keep those bounds like rational numbers, like finite precision, like infinite precision,", "tokens": [51184, 293, 291, 1066, 729, 29905, 411, 15090, 3547, 11, 411, 19362, 18356, 11, 411, 13785, 18356, 11, 51460], "temperature": 0.0, "avg_logprob": -0.13744292596373894, "compression_ratio": 1.8047808764940239, "no_speech_prob": 0.011778072454035282}, {"id": 69, "seek": 43044, "start": 453.08, "end": 459.32, "text": " then you can avoid errors, at least in principle, at the cost maybe of", "tokens": [51496, 550, 291, 393, 5042, 13603, 11, 412, 1935, 294, 8665, 11, 412, 264, 2063, 1310, 295, 51808], "temperature": 0.0, "avg_logprob": -0.13744292596373894, "compression_ratio": 1.8047808764940239, "no_speech_prob": 0.011778072454035282}, {"id": 70, "seek": 46044, "start": 460.92, "end": 466.6, "text": " making the runtime longer. More recently, there are more advanced", "tokens": [50388, 1455, 264, 34474, 2854, 13, 5048, 3938, 11, 456, 366, 544, 7339, 50672], "temperature": 0.0, "avg_logprob": -0.20082576623123682, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.0005163790774531662}, {"id": 71, "seek": 46044, "start": 468.68, "end": 473.72, "text": " algebra packages than just sort of the standard things you get in sage or Mathematica.", "tokens": [50776, 21989, 17401, 813, 445, 1333, 295, 264, 3832, 721, 291, 483, 294, 19721, 420, 15776, 8615, 2262, 13, 51028], "temperature": 0.0, "avg_logprob": -0.20082576623123682, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.0005163790774531662}, {"id": 72, "seek": 46044, "start": 475.48, "end": 481.15999999999997, "text": " They think of SAT solvers, satisfiability solvers, or satisfiably modulo theory solvers,", "tokens": [51116, 814, 519, 295, 31536, 1404, 840, 11, 5519, 72, 2310, 1404, 840, 11, 420, 5519, 72, 1188, 1072, 13455, 5261, 1404, 840, 11, 51400], "temperature": 0.0, "avg_logprob": -0.20082576623123682, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.0005163790774531662}, {"id": 73, "seek": 46044, "start": 481.15999999999997, "end": 488.2, "text": " SMT solvers. So what they, so a SAT solver gives, you feed it a whole statement, a bunch of", "tokens": [51400, 13115, 51, 1404, 840, 13, 407, 437, 436, 11, 370, 257, 31536, 1404, 331, 2709, 11, 291, 3154, 309, 257, 1379, 5629, 11, 257, 3840, 295, 51752], "temperature": 0.0, "avg_logprob": -0.20082576623123682, "compression_ratio": 1.5933014354066986, "no_speech_prob": 0.0005163790774531662}, {"id": 74, "seek": 48820, "start": 488.2, "end": 495.0, "text": " propositions, P1, P2, P3, and so forth, and you see that P1 and P2, all P3 is true, P3 and P4,", "tokens": [50364, 7532, 2451, 11, 430, 16, 11, 430, 17, 11, 430, 18, 11, 293, 370, 5220, 11, 293, 291, 536, 300, 430, 16, 293, 430, 17, 11, 439, 430, 18, 307, 2074, 11, 430, 18, 293, 430, 19, 11, 50704], "temperature": 0.0, "avg_logprob": -0.12215741932820931, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.004268631339073181}, {"id": 75, "seek": 48820, "start": 495.0, "end": 500.12, "text": " and not P5, one of them is true, and so forth, and it will try to see if there's a solution or not,", "tokens": [50704, 293, 406, 430, 20, 11, 472, 295, 552, 307, 2074, 11, 293, 370, 5220, 11, 293, 309, 486, 853, 281, 536, 498, 456, 311, 257, 3827, 420, 406, 11, 50960], "temperature": 0.0, "avg_logprob": -0.12215741932820931, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.004268631339073181}, {"id": 76, "seek": 48820, "start": 500.84, "end": 505.48, "text": " and many problems can be phrased as a SAT problem. So if you have a general purpose SAT solver,", "tokens": [50996, 293, 867, 2740, 393, 312, 7636, 1937, 382, 257, 31536, 1154, 13, 407, 498, 291, 362, 257, 2674, 4334, 31536, 1404, 331, 11, 51228], "temperature": 0.0, "avg_logprob": -0.12215741932820931, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.004268631339073181}, {"id": 77, "seek": 48820, "start": 506.36, "end": 512.76, "text": " you can potentially just feed it into such a program and solve the problem for you,", "tokens": [51272, 291, 393, 7263, 445, 3154, 309, 666, 1270, 257, 1461, 293, 5039, 264, 1154, 337, 291, 11, 51592], "temperature": 0.0, "avg_logprob": -0.12215741932820931, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.004268631339073181}, {"id": 78, "seek": 48820, "start": 513.48, "end": 518.12, "text": " then there are these more sophisticated variants, SMT solvers, where you also feed laws of algebra", "tokens": [51628, 550, 456, 366, 613, 544, 16950, 21669, 11, 13115, 51, 1404, 840, 11, 689, 291, 611, 3154, 6064, 295, 21989, 51860], "temperature": 0.0, "avg_logprob": -0.12215741932820931, "compression_ratio": 1.758364312267658, "no_speech_prob": 0.004268631339073181}, {"id": 79, "seek": 51812, "start": 518.76, "end": 526.68, "text": " so you have some variables, and you assume that there are certain laws, these variables", "tokens": [50396, 370, 291, 362, 512, 9102, 11, 293, 291, 6552, 300, 456, 366, 1629, 6064, 11, 613, 9102, 50792], "temperature": 0.0, "avg_logprob": -0.16932734988984607, "compression_ratio": 1.6728971962616823, "no_speech_prob": 6.938818114576861e-05}, {"id": 80, "seek": 51812, "start": 526.68, "end": 530.2, "text": " obey certain laws, and you ask, can you deduce a new law from the laws you already have?", "tokens": [50792, 19297, 1629, 6064, 11, 293, 291, 1029, 11, 393, 291, 4172, 4176, 257, 777, 2101, 490, 264, 6064, 291, 1217, 362, 30, 50968], "temperature": 0.0, "avg_logprob": -0.16932734988984607, "compression_ratio": 1.6728971962616823, "no_speech_prob": 6.938818114576861e-05}, {"id": 81, "seek": 51812, "start": 531.64, "end": 536.36, "text": " So those are potentially very powerful, and unfortunately, SAT solverability is an NP", "tokens": [51040, 407, 729, 366, 7263, 588, 4005, 11, 293, 7015, 11, 31536, 1404, 331, 2310, 307, 364, 38611, 51276], "temperature": 0.0, "avg_logprob": -0.16932734988984607, "compression_ratio": 1.6728971962616823, "no_speech_prob": 6.938818114576861e-05}, {"id": 82, "seek": 51812, "start": 536.36, "end": 542.12, "text": " complete problem, and once you get hundreds and hundreds of these propositions, it becomes very", "tokens": [51276, 3566, 1154, 11, 293, 1564, 291, 483, 6779, 293, 6779, 295, 613, 7532, 2451, 11, 309, 3643, 588, 51564], "temperature": 0.0, "avg_logprob": -0.16932734988984607, "compression_ratio": 1.6728971962616823, "no_speech_prob": 6.938818114576861e-05}, {"id": 83, "seek": 54212, "start": 542.12, "end": 548.76, "text": " hard to actually solve these problems, but still they are very useful. Here's a typical", "tokens": [50364, 1152, 281, 767, 5039, 613, 2740, 11, 457, 920, 436, 366, 588, 4420, 13, 1692, 311, 257, 7476, 50696], "temperature": 0.0, "avg_logprob": -0.12718617288689865, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.003319723531603813}, {"id": 84, "seek": 54212, "start": 548.76, "end": 556.52, "text": " application of a SAT solver, so a few years ago, there was this famous problem in commentaries", "tokens": [50696, 3861, 295, 257, 31536, 1404, 331, 11, 370, 257, 1326, 924, 2057, 11, 456, 390, 341, 4618, 1154, 294, 2871, 4889, 51084], "temperature": 0.0, "avg_logprob": -0.12718617288689865, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.003319723531603813}, {"id": 85, "seek": 54212, "start": 556.52, "end": 561.64, "text": " called the Boolean Pythagorean Triples problem, and so the problem is this, you take natural numbers", "tokens": [51084, 1219, 264, 23351, 28499, 9953, 392, 559, 25885, 10931, 2622, 1154, 11, 293, 370, 264, 1154, 307, 341, 11, 291, 747, 3303, 3547, 51340], "temperature": 0.0, "avg_logprob": -0.12718617288689865, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.003319723531603813}, {"id": 86, "seek": 54212, "start": 561.64, "end": 568.44, "text": " and you color them into two color classes, red and blue, and you ask, is it always the case that", "tokens": [51340, 293, 291, 2017, 552, 666, 732, 2017, 5359, 11, 2182, 293, 3344, 11, 293, 291, 1029, 11, 307, 309, 1009, 264, 1389, 300, 51680], "temperature": 0.0, "avg_logprob": -0.12718617288689865, "compression_ratio": 1.6309012875536482, "no_speech_prob": 0.003319723531603813}, {"id": 87, "seek": 56844, "start": 568.44, "end": 573.0, "text": " one of these color classes contains a Pythagorean triple, three numbers A, B and C such that A", "tokens": [50364, 472, 295, 613, 2017, 5359, 8306, 257, 9953, 392, 559, 25885, 15508, 11, 1045, 3547, 316, 11, 363, 293, 383, 1270, 300, 316, 50592], "temperature": 0.0, "avg_logprob": -0.1450616654895601, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.0006027619238011539}, {"id": 88, "seek": 56844, "start": 573.0, "end": 580.6800000000001, "text": " squared plus B squared equals C squared, and it turns out to be, we don't have like a human proof", "tokens": [50592, 8889, 1804, 363, 8889, 6915, 383, 8889, 11, 293, 309, 4523, 484, 281, 312, 11, 321, 500, 380, 362, 411, 257, 1952, 8177, 50976], "temperature": 0.0, "avg_logprob": -0.1450616654895601, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.0006027619238011539}, {"id": 89, "seek": 56844, "start": 580.6800000000001, "end": 585.8800000000001, "text": " of the statement, but we know it's true now because of a SAT solver, so there was a massive", "tokens": [50976, 295, 264, 5629, 11, 457, 321, 458, 309, 311, 2074, 586, 570, 295, 257, 31536, 1404, 331, 11, 370, 456, 390, 257, 5994, 51236], "temperature": 0.0, "avg_logprob": -0.1450616654895601, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.0006027619238011539}, {"id": 90, "seek": 56844, "start": 585.8800000000001, "end": 594.44, "text": " computation that says that if you only go up to 7,824, then you can't do this, there was a way", "tokens": [51236, 24903, 300, 1619, 300, 498, 291, 787, 352, 493, 281, 1614, 11, 23, 7911, 11, 550, 291, 393, 380, 360, 341, 11, 456, 390, 257, 636, 51664], "temperature": 0.0, "avg_logprob": -0.1450616654895601, "compression_ratio": 1.5532786885245902, "no_speech_prob": 0.0006027619238011539}, {"id": 91, "seek": 59444, "start": 594.44, "end": 599.6400000000001, "text": " to partition the numbers from 1 to 7,824 into two classes, neither of which contain a Pythagorean", "tokens": [50364, 281, 24808, 264, 3547, 490, 502, 281, 1614, 11, 23, 7911, 666, 732, 5359, 11, 9662, 295, 597, 5304, 257, 9953, 392, 559, 25885, 50624], "temperature": 0.0, "avg_logprob": -0.10715782257818407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0032480766531080008}, {"id": 92, "seek": 59444, "start": 599.6400000000001, "end": 606.2800000000001, "text": " triple, but once you go up to 7,825, no matter how you do it, you must always get one of the", "tokens": [50624, 15508, 11, 457, 1564, 291, 352, 493, 281, 1614, 11, 23, 6074, 11, 572, 1871, 577, 291, 360, 309, 11, 291, 1633, 1009, 483, 472, 295, 264, 50956], "temperature": 0.0, "avg_logprob": -0.10715782257818407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0032480766531080008}, {"id": 93, "seek": 59444, "start": 606.2800000000001, "end": 611.6400000000001, "text": " two color classes must have a Pythagorean triple. In principle, this is a finite computation", "tokens": [50956, 732, 2017, 5359, 1633, 362, 257, 9953, 392, 559, 25885, 15508, 13, 682, 8665, 11, 341, 307, 257, 19362, 24903, 51224], "temperature": 0.0, "avg_logprob": -0.10715782257818407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0032480766531080008}, {"id": 94, "seek": 59444, "start": 611.6400000000001, "end": 618.6800000000001, "text": " because there's only two to seven, two to five different ways to compute different partitions,", "tokens": [51224, 570, 456, 311, 787, 732, 281, 3407, 11, 732, 281, 1732, 819, 2098, 281, 14722, 819, 644, 2451, 11, 51576], "temperature": 0.0, "avg_logprob": -0.10715782257818407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0032480766531080008}, {"id": 95, "seek": 59444, "start": 618.6800000000001, "end": 623.32, "text": " and so you just check each one, but that is computationally unfeasible, but with a SAT", "tokens": [51576, 293, 370, 291, 445, 1520, 1184, 472, 11, 457, 300, 307, 24903, 379, 517, 2106, 296, 964, 11, 457, 365, 257, 31536, 51808], "temperature": 0.0, "avg_logprob": -0.10715782257818407, "compression_ratio": 1.7222222222222223, "no_speech_prob": 0.0032480766531080008}, {"id": 96, "seek": 62332, "start": 623.32, "end": 630.7600000000001, "text": " solver, you can rephrase this problem as a free satisfiability problem, and it's not just a matter", "tokens": [50364, 1404, 331, 11, 291, 393, 319, 44598, 651, 341, 1154, 382, 257, 1737, 5519, 72, 2310, 1154, 11, 293, 309, 311, 406, 445, 257, 1871, 50736], "temperature": 0.0, "avg_logprob": -0.0974872189183389, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0014212552923709154}, {"id": 97, "seek": 62332, "start": 630.7600000000001, "end": 634.2800000000001, "text": " of running the solver, you have to optimize it and so forth, but it is possible to actually solve", "tokens": [50736, 295, 2614, 264, 1404, 331, 11, 291, 362, 281, 19719, 309, 293, 370, 5220, 11, 457, 309, 307, 1944, 281, 767, 5039, 50912], "temperature": 0.0, "avg_logprob": -0.0974872189183389, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0014212552923709154}, {"id": 98, "seek": 62332, "start": 634.2800000000001, "end": 640.12, "text": " this problem, and it gives you a certificate, it gives you a proof, and actually this is,", "tokens": [50912, 341, 1154, 11, 293, 309, 2709, 291, 257, 15953, 11, 309, 2709, 291, 257, 8177, 11, 293, 767, 341, 307, 11, 51204], "temperature": 0.0, "avg_logprob": -0.0974872189183389, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0014212552923709154}, {"id": 99, "seek": 62332, "start": 640.12, "end": 646.0400000000001, "text": " at the time, it was actually the world's longest proof. The proof certificate, first of all,", "tokens": [51204, 412, 264, 565, 11, 309, 390, 767, 264, 1002, 311, 15438, 8177, 13, 440, 8177, 15953, 11, 700, 295, 439, 11, 51500], "temperature": 0.0, "avg_logprob": -0.0974872189183389, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0014212552923709154}, {"id": 100, "seek": 62332, "start": 646.0400000000001, "end": 652.12, "text": " it took four CPU years to generate, and it's a 200 terabyte proof, although it is compressible.", "tokens": [51500, 309, 1890, 1451, 13199, 924, 281, 8460, 11, 293, 309, 311, 257, 2331, 1796, 34529, 8177, 11, 4878, 309, 307, 14778, 964, 13, 51804], "temperature": 0.0, "avg_logprob": -0.0974872189183389, "compression_ratio": 1.8269230769230769, "no_speech_prob": 0.0014212552923709154}, {"id": 101, "seek": 65212, "start": 652.84, "end": 655.4, "text": " I think it is still the second largest proof ever generated.", "tokens": [50400, 286, 519, 309, 307, 920, 264, 1150, 6443, 8177, 1562, 10833, 13, 50528], "temperature": 0.0, "avg_logprob": -0.16013475826808385, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.00030814766068942845}, {"id": 102, "seek": 65212, "start": 657.16, "end": 663.24, "text": " Okay, so that's, but this I still consider is a more classical way of using computers,", "tokens": [50616, 1033, 11, 370, 300, 311, 11, 457, 341, 286, 920, 1949, 307, 257, 544, 13735, 636, 295, 1228, 10807, 11, 50920], "temperature": 0.0, "avg_logprob": -0.16013475826808385, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.00030814766068942845}, {"id": 103, "seek": 65212, "start": 665.4, "end": 672.44, "text": " but what I think is exciting is that there are a lot of new ways that we can use computers", "tokens": [51028, 457, 437, 286, 519, 307, 4670, 307, 300, 456, 366, 257, 688, 295, 777, 2098, 300, 321, 393, 764, 10807, 51380], "temperature": 0.0, "avg_logprob": -0.16013475826808385, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.00030814766068942845}, {"id": 104, "seek": 65212, "start": 674.44, "end": 678.6, "text": " to do mathematics. Of course, there's still the boring ways, you know, we use computers to do", "tokens": [51480, 281, 360, 18666, 13, 2720, 1164, 11, 456, 311, 920, 264, 9989, 2098, 11, 291, 458, 11, 321, 764, 10807, 281, 360, 51688], "temperature": 0.0, "avg_logprob": -0.16013475826808385, "compression_ratio": 1.6938775510204083, "no_speech_prob": 0.00030814766068942845}, {"id": 105, "seek": 67860, "start": 678.6, "end": 686.52, "text": " emails and write latex and so forth, I don't mean that, but there are sort of three new modalities,", "tokens": [50364, 12524, 293, 2464, 3469, 87, 293, 370, 5220, 11, 286, 500, 380, 914, 300, 11, 457, 456, 366, 1333, 295, 1045, 777, 1072, 16110, 11, 50760], "temperature": 0.0, "avg_logprob": -0.14303099831869437, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00044942411477677524}, {"id": 106, "seek": 67860, "start": 687.16, "end": 693.32, "text": " which individually, they still have somewhat niche applications, but what I find really", "tokens": [50792, 597, 16652, 11, 436, 920, 362, 8344, 19956, 5821, 11, 457, 437, 286, 915, 534, 51100], "temperature": 0.0, "avg_logprob": -0.14303099831869437, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00044942411477677524}, {"id": 107, "seek": 67860, "start": 693.32, "end": 697.32, "text": " interesting is that they can potentially be combined together, and the combination of them,", "tokens": [51100, 1880, 307, 300, 436, 393, 7263, 312, 9354, 1214, 11, 293, 264, 6562, 295, 552, 11, 51300], "temperature": 0.0, "avg_logprob": -0.14303099831869437, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00044942411477677524}, {"id": 108, "seek": 67860, "start": 697.32, "end": 703.1600000000001, "text": " it could be something in general purpose that actually a lot of us could use. So the three sort", "tokens": [51300, 309, 727, 312, 746, 294, 2674, 4334, 300, 767, 257, 688, 295, 505, 727, 764, 13, 407, 264, 1045, 1333, 51592], "temperature": 0.0, "avg_logprob": -0.14303099831869437, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00044942411477677524}, {"id": 109, "seek": 70316, "start": 703.16, "end": 709.9599999999999, "text": " of new things. So the first is machine learning algorithms, where you have a problem, and if", "tokens": [50364, 295, 777, 721, 13, 407, 264, 700, 307, 3479, 2539, 14642, 11, 689, 291, 362, 257, 1154, 11, 293, 498, 50704], "temperature": 0.0, "avg_logprob": -0.1293147404988607, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0028024115599691868}, {"id": 110, "seek": 70316, "start": 709.9599999999999, "end": 714.28, "text": " you have a lot of data for that problem, you can set some sort of specialized neural network to", "tokens": [50704, 291, 362, 257, 688, 295, 1412, 337, 300, 1154, 11, 291, 393, 992, 512, 1333, 295, 19813, 18161, 3209, 281, 50920], "temperature": 0.0, "avg_logprob": -0.1293147404988607, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0028024115599691868}, {"id": 111, "seek": 70316, "start": 714.28, "end": 717.8, "text": " train it on the data, and it can generate counter examples for you, try to generate connections,", "tokens": [50920, 3847, 309, 322, 264, 1412, 11, 293, 309, 393, 8460, 5682, 5110, 337, 291, 11, 853, 281, 8460, 9271, 11, 51096], "temperature": 0.0, "avg_logprob": -0.1293147404988607, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0028024115599691868}, {"id": 112, "seek": 70316, "start": 718.76, "end": 721.7199999999999, "text": " and so people are beginning to use this in all kinds of fields of mathematics, I'll give you", "tokens": [51144, 293, 370, 561, 366, 2863, 281, 764, 341, 294, 439, 3685, 295, 7909, 295, 18666, 11, 286, 603, 976, 291, 51292], "temperature": 0.0, "avg_logprob": -0.1293147404988607, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0028024115599691868}, {"id": 113, "seek": 70316, "start": 721.7199999999999, "end": 730.6, "text": " some examples later. So that's one development. Maybe the most high-profile development is", "tokens": [51292, 512, 5110, 1780, 13, 407, 300, 311, 472, 3250, 13, 2704, 264, 881, 1090, 12, 29175, 794, 3250, 307, 51736], "temperature": 0.0, "avg_logprob": -0.1293147404988607, "compression_ratio": 1.7179487179487178, "no_speech_prob": 0.0028024115599691868}, {"id": 114, "seek": 73060, "start": 730.6, "end": 735.08, "text": " large language models like chat GPT, these are very general purpose models that can understand", "tokens": [50364, 2416, 2856, 5245, 411, 5081, 26039, 51, 11, 613, 366, 588, 2674, 4334, 5245, 300, 393, 1223, 50588], "temperature": 0.0, "avg_logprob": -0.1250910484927824, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.002783827017992735}, {"id": 115, "seek": 73060, "start": 735.08, "end": 741.5600000000001, "text": " natural language. To date, they have not been directly used for so much mathematics, I'm sure", "tokens": [50588, 3303, 2856, 13, 1407, 4002, 11, 436, 362, 406, 668, 3838, 1143, 337, 370, 709, 18666, 11, 286, 478, 988, 50912], "temperature": 0.0, "avg_logprob": -0.1250910484927824, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.002783827017992735}, {"id": 116, "seek": 73060, "start": 741.5600000000001, "end": 746.2, "text": " many of you have tried talking to GPT, asking it to solve your favorite math problem, and it will", "tokens": [50912, 867, 295, 291, 362, 3031, 1417, 281, 26039, 51, 11, 3365, 309, 281, 5039, 428, 2954, 5221, 1154, 11, 293, 309, 486, 51144], "temperature": 0.0, "avg_logprob": -0.1250910484927824, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.002783827017992735}, {"id": 117, "seek": 73060, "start": 746.2, "end": 755.08, "text": " give you some so plausible looking nonsense in general. But when used correctly, I think they", "tokens": [51144, 976, 291, 512, 370, 39925, 1237, 14925, 294, 2674, 13, 583, 562, 1143, 8944, 11, 286, 519, 436, 51588], "temperature": 0.0, "avg_logprob": -0.1250910484927824, "compression_ratio": 1.596638655462185, "no_speech_prob": 0.002783827017992735}, {"id": 118, "seek": 75508, "start": 755.08, "end": 761.1600000000001, "text": " do offer a lot of potential. I mean, I have found occasionally that these models can be useful for", "tokens": [50364, 360, 2626, 257, 688, 295, 3995, 13, 286, 914, 11, 286, 362, 1352, 16895, 300, 613, 5245, 393, 312, 4420, 337, 50668], "temperature": 0.0, "avg_logprob": -0.1396045003618513, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.0011370859574526548}, {"id": 119, "seek": 75508, "start": 761.1600000000001, "end": 768.2, "text": " suggesting proof techniques that I wasn't initially thinking of, or to suggest related topics or", "tokens": [50668, 18094, 8177, 7512, 300, 286, 2067, 380, 9105, 1953, 295, 11, 420, 281, 3402, 4077, 8378, 420, 51020], "temperature": 0.0, "avg_logprob": -0.1396045003618513, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.0011370859574526548}, {"id": 120, "seek": 75508, "start": 768.2, "end": 775.4000000000001, "text": " literature. They're actually most useful for sort of secondary tasks. Okay, so for actually doing", "tokens": [51020, 10394, 13, 814, 434, 767, 881, 4420, 337, 1333, 295, 11396, 9608, 13, 1033, 11, 370, 337, 767, 884, 51380], "temperature": 0.0, "avg_logprob": -0.1396045003618513, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.0011370859574526548}, {"id": 121, "seek": 75508, "start": 775.4000000000001, "end": 779.48, "text": " math research, they still haven't really proved themselves, but for doing things like writing", "tokens": [51380, 5221, 2132, 11, 436, 920, 2378, 380, 534, 14617, 2969, 11, 457, 337, 884, 721, 411, 3579, 51584], "temperature": 0.0, "avg_logprob": -0.1396045003618513, "compression_ratio": 1.6192468619246863, "no_speech_prob": 0.0011370859574526548}, {"id": 122, "seek": 77948, "start": 779.48, "end": 786.36, "text": " code or organizing a bibliography, like a lot of the other more routine tasks that we do actually,", "tokens": [50364, 3089, 420, 17608, 257, 34344, 5820, 11, 411, 257, 688, 295, 264, 661, 544, 9927, 9608, 300, 321, 360, 767, 11, 50708], "temperature": 0.0, "avg_logprob": -0.1986286077606544, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0006621044012717903}, {"id": 123, "seek": 77948, "start": 786.36, "end": 794.76, "text": " these LLMs are very useful. But the third new technology, which has been around for two decades,", "tokens": [50708, 613, 441, 43, 26386, 366, 588, 4420, 13, 583, 264, 2636, 777, 2899, 11, 597, 575, 668, 926, 337, 732, 7878, 11, 51128], "temperature": 0.0, "avg_logprob": -0.1986286077606544, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0006621044012717903}, {"id": 124, "seek": 77948, "start": 794.76, "end": 799.16, "text": " but has only now sort of becoming ready for primetime, are these formal proof assistants,", "tokens": [51128, 457, 575, 787, 586, 1333, 295, 5617, 1919, 337, 2886, 9764, 11, 366, 613, 9860, 8177, 34949, 11, 51348], "temperature": 0.0, "avg_logprob": -0.1986286077606544, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0006621044012717903}, {"id": 125, "seek": 77948, "start": 799.96, "end": 806.6, "text": " which are languages designed to verify, or to verify, or many of them are actually designed", "tokens": [51388, 597, 366, 8650, 4761, 281, 16888, 11, 420, 281, 16888, 11, 420, 867, 295, 552, 366, 767, 4761, 51720], "temperature": 0.0, "avg_logprob": -0.1986286077606544, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0006621044012717903}, {"id": 126, "seek": 80660, "start": 806.6, "end": 813.16, "text": " to verify electronics, but they can also verify mathematical proofs. And crucially, they can", "tokens": [50364, 281, 16888, 20611, 11, 457, 436, 393, 611, 16888, 18894, 8177, 82, 13, 400, 5140, 1909, 11, 436, 393, 50692], "temperature": 0.0, "avg_logprob": -0.10446189750324596, "compression_ratio": 1.792626728110599, "no_speech_prob": 0.0008388388669118285}, {"id": 127, "seek": 80660, "start": 813.16, "end": 818.44, "text": " also verify the output of large language models, which they can complement, they can fix the biggest", "tokens": [50692, 611, 16888, 264, 5598, 295, 2416, 2856, 5245, 11, 597, 436, 393, 17103, 11, 436, 393, 3191, 264, 3880, 50956], "temperature": 0.0, "avg_logprob": -0.10446189750324596, "compression_ratio": 1.792626728110599, "no_speech_prob": 0.0008388388669118285}, {"id": 128, "seek": 80660, "start": 818.44, "end": 827.8000000000001, "text": " defect in principle of the LLMs. And they allow new types of ways to do mathematics, in particular,", "tokens": [50956, 16445, 294, 8665, 295, 264, 441, 43, 26386, 13, 400, 436, 2089, 777, 3467, 295, 2098, 281, 360, 18666, 11, 294, 1729, 11, 51424], "temperature": 0.0, "avg_logprob": -0.10446189750324596, "compression_ratio": 1.792626728110599, "no_speech_prob": 0.0008388388669118285}, {"id": 129, "seek": 80660, "start": 827.8000000000001, "end": 831.8000000000001, "text": " they can allow really large scale collaborations, which we really can't do without these formal", "tokens": [51424, 436, 393, 2089, 534, 2416, 4373, 36908, 11, 597, 321, 534, 393, 380, 360, 1553, 613, 9860, 51624], "temperature": 0.0, "avg_logprob": -0.10446189750324596, "compression_ratio": 1.792626728110599, "no_speech_prob": 0.0008388388669118285}, {"id": 130, "seek": 83180, "start": 831.8, "end": 837.0799999999999, "text": " proof assistants. And they can also generate data, which can be used for the other two", "tokens": [50364, 8177, 34949, 13, 400, 436, 393, 611, 8460, 1412, 11, 597, 393, 312, 1143, 337, 264, 661, 732, 50628], "temperature": 0.0, "avg_logprob": -0.17860678871079247, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0016382772009819746}, {"id": 131, "seek": 83180, "start": 838.8399999999999, "end": 843.16, "text": " the other two technologies. So I'll talk about each of these three things separately, but", "tokens": [50716, 264, 661, 732, 7943, 13, 407, 286, 603, 751, 466, 1184, 295, 613, 1045, 721, 14759, 11, 457, 50932], "temperature": 0.0, "avg_logprob": -0.17860678871079247, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0016382772009819746}, {"id": 132, "seek": 83180, "start": 846.04, "end": 849.56, "text": " but they haven't, there's beginning to be experiments to combine them together,", "tokens": [51076, 457, 436, 2378, 380, 11, 456, 311, 2863, 281, 312, 12050, 281, 10432, 552, 1214, 11, 51252], "temperature": 0.0, "avg_logprob": -0.17860678871079247, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0016382772009819746}, {"id": 133, "seek": 83180, "start": 850.76, "end": 856.12, "text": " but they're still kind of prototypes right now. But I think the paradigm of using all of them,", "tokens": [51312, 457, 436, 434, 920, 733, 295, 42197, 558, 586, 13, 583, 286, 519, 264, 24709, 295, 1228, 439, 295, 552, 11, 51580], "temperature": 0.0, "avg_logprob": -0.17860678871079247, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0016382772009819746}, {"id": 134, "seek": 83180, "start": 856.12, "end": 861.0799999999999, "text": " and also combining with the computer algebra systems and the SAP solvers into one integrated", "tokens": [51580, 293, 611, 21928, 365, 264, 3820, 21989, 3652, 293, 264, 27743, 1404, 840, 666, 472, 10919, 51828], "temperature": 0.0, "avg_logprob": -0.17860678871079247, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0016382772009819746}, {"id": 135, "seek": 86108, "start": 861.08, "end": 867.32, "text": " package, it could really be quite a powerful methodical assistant. Okay, so let's talk,", "tokens": [50364, 7372, 11, 309, 727, 534, 312, 1596, 257, 4005, 3170, 804, 10994, 13, 1033, 11, 370, 718, 311, 751, 11, 50676], "temperature": 0.0, "avg_logprob": -0.19427432184634003, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.0007581880781799555}, {"id": 136, "seek": 86108, "start": 867.32, "end": 876.44, "text": " I think my first slides begin with proof assistants. So the computer-assisted proofs are not new,", "tokens": [50676, 286, 519, 452, 700, 9788, 1841, 365, 8177, 34949, 13, 407, 264, 3820, 12, 640, 33250, 8177, 82, 366, 406, 777, 11, 51132], "temperature": 0.0, "avg_logprob": -0.19427432184634003, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.0007581880781799555}, {"id": 137, "seek": 86108, "start": 877.32, "end": 882.76, "text": " famously the full color theorem in 1976 was was proven partly by computer. Although at the time,", "tokens": [51176, 34360, 264, 1577, 2017, 20904, 294, 33978, 390, 390, 12785, 17031, 538, 3820, 13, 5780, 412, 264, 565, 11, 51448], "temperature": 0.0, "avg_logprob": -0.19427432184634003, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.0007581880781799555}, {"id": 138, "seek": 86108, "start": 882.76, "end": 889.0, "text": " it was by modern standards, we will not call it a fully formalized proof, the 1976 proof.", "tokens": [51448, 309, 390, 538, 4363, 7787, 11, 321, 486, 406, 818, 309, 257, 4498, 9860, 1602, 8177, 11, 264, 33978, 8177, 13, 51760], "temperature": 0.0, "avg_logprob": -0.19427432184634003, "compression_ratio": 1.5564853556485356, "no_speech_prob": 0.0007581880781799555}, {"id": 139, "seek": 88900, "start": 889.4, "end": 895.72, "text": " The proof was this long document with lots and lots of subclaims, which", "tokens": [50384, 440, 8177, 390, 341, 938, 4166, 365, 3195, 293, 3195, 295, 1422, 66, 10970, 82, 11, 597, 50700], "temperature": 0.0, "avg_logprob": -0.15532716115315756, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0006691623711958528}, {"id": 140, "seek": 88900, "start": 897.16, "end": 902.04, "text": " a lot of them were verified by hand, and a lot of them were verified by both electronic computers", "tokens": [50772, 257, 688, 295, 552, 645, 31197, 538, 1011, 11, 293, 257, 688, 295, 552, 645, 31197, 538, 1293, 10092, 10807, 51016], "temperature": 0.0, "avg_logprob": -0.15532716115315756, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0006691623711958528}, {"id": 141, "seek": 88900, "start": 902.04, "end": 906.68, "text": " and human computers. And I think one of the author's daughter actually had to go through 500 graphs", "tokens": [51016, 293, 1952, 10807, 13, 400, 286, 519, 472, 295, 264, 3793, 311, 4653, 767, 632, 281, 352, 807, 5923, 24877, 51248], "temperature": 0.0, "avg_logprob": -0.15532716115315756, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0006691623711958528}, {"id": 142, "seek": 88900, "start": 906.68, "end": 912.92, "text": " and check that they all had this discharging property. And actually, it had a lot of mistakes too.", "tokens": [51248, 293, 1520, 300, 436, 439, 632, 341, 717, 7374, 3249, 4707, 13, 400, 767, 11, 309, 632, 257, 688, 295, 8038, 886, 13, 51560], "temperature": 0.0, "avg_logprob": -0.15532716115315756, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0006691623711958528}, {"id": 143, "seek": 91292, "start": 912.92, "end": 920.12, "text": " So there's a lot of minor errors in the proof. They're all correctable, but it really", "tokens": [50364, 407, 456, 311, 257, 688, 295, 6696, 13603, 294, 264, 8177, 13, 814, 434, 439, 3006, 712, 11, 457, 309, 534, 50724], "temperature": 0.0, "avg_logprob": -0.17214715480804443, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0003169586998410523}, {"id": 144, "seek": 91292, "start": 920.8399999999999, "end": 925.8, "text": " will not meet the standards today of a computer-verified proof.", "tokens": [50760, 486, 406, 1677, 264, 7787, 965, 295, 257, 3820, 12, 331, 2587, 8177, 13, 51008], "temperature": 0.0, "avg_logprob": -0.17214715480804443, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0003169586998410523}, {"id": 145, "seek": 91292, "start": 928.1999999999999, "end": 934.92, "text": " The first proof, okay, so it took 30 to 20 years for an alternative proof of full color theorem", "tokens": [51128, 440, 700, 8177, 11, 1392, 11, 370, 309, 1890, 2217, 281, 945, 924, 337, 364, 8535, 8177, 295, 1577, 2017, 20904, 51464], "temperature": 0.0, "avg_logprob": -0.17214715480804443, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0003169586998410523}, {"id": 146, "seek": 91292, "start": 934.92, "end": 941.7199999999999, "text": " to be verified, and this proof is closer to being completely formal. So it's about 10-15 pages", "tokens": [51464, 281, 312, 31197, 11, 293, 341, 8177, 307, 4966, 281, 885, 2584, 9860, 13, 407, 309, 311, 466, 1266, 12, 5211, 7183, 51804], "temperature": 0.0, "avg_logprob": -0.17214715480804443, "compression_ratio": 1.552511415525114, "no_speech_prob": 0.0003169586998410523}, {"id": 147, "seek": 94172, "start": 941.72, "end": 947.1600000000001, "text": " of human readable argument, and then it reduces to this very specific computation, which anyone", "tokens": [50364, 295, 1952, 49857, 6770, 11, 293, 550, 309, 18081, 281, 341, 588, 2685, 24903, 11, 597, 2878, 50636], "temperature": 0.0, "avg_logprob": -0.13071022250435568, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00034015363780781627}, {"id": 148, "seek": 94172, "start": 947.1600000000001, "end": 953.8000000000001, "text": " can just run a computer program in whatever language they like to verify it. So it was a", "tokens": [50636, 393, 445, 1190, 257, 3820, 1461, 294, 2035, 2856, 436, 411, 281, 16888, 309, 13, 407, 309, 390, 257, 50968], "temperature": 0.0, "avg_logprob": -0.13071022250435568, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00034015363780781627}, {"id": 149, "seek": 94172, "start": 953.8000000000001, "end": 959.88, "text": " computer-verified proof, but it still wasn't a formal proof. It wasn't written in a formal", "tokens": [50968, 3820, 12, 331, 2587, 8177, 11, 457, 309, 920, 2067, 380, 257, 9860, 8177, 13, 467, 2067, 380, 3720, 294, 257, 9860, 51272], "temperature": 0.0, "avg_logprob": -0.13071022250435568, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00034015363780781627}, {"id": 150, "seek": 94172, "start": 959.88, "end": 966.52, "text": " proof language, which was designed to only output correct proofs. And that had to wait until the", "tokens": [51272, 8177, 2856, 11, 597, 390, 4761, 281, 787, 5598, 3006, 8177, 82, 13, 400, 300, 632, 281, 1699, 1826, 264, 51604], "temperature": 0.0, "avg_logprob": -0.13071022250435568, "compression_ratio": 1.6681614349775784, "no_speech_prob": 0.00034015363780781627}, {"id": 151, "seek": 96652, "start": 966.52, "end": 972.12, "text": " early 2000s when Werner and Gontier actually formalized the entire full color theorem in one", "tokens": [50364, 2440, 8132, 82, 562, 14255, 1193, 293, 460, 896, 811, 767, 9860, 1602, 264, 2302, 1577, 2017, 20904, 294, 472, 50644], "temperature": 0.0, "avg_logprob": -0.24244706290108817, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.0006745847640559077}, {"id": 152, "seek": 96652, "start": 972.12, "end": 980.04, "text": " of the early proof assistant languages, COC, in this case. So now we know with 100% certainty", "tokens": [50644, 295, 264, 2440, 8177, 10994, 8650, 11, 3002, 34, 11, 294, 341, 1389, 13, 407, 586, 321, 458, 365, 2319, 4, 27022, 51040], "temperature": 0.0, "avg_logprob": -0.24244706290108817, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.0006745847640559077}, {"id": 153, "seek": 96652, "start": 980.04, "end": 985.72, "text": " that the full color theorem is correct. Well, modulo, trusting the compiler of COC.", "tokens": [51040, 300, 264, 1577, 2017, 20904, 307, 3006, 13, 1042, 11, 1072, 13455, 11, 28235, 264, 31958, 295, 3002, 34, 13, 51324], "temperature": 0.0, "avg_logprob": -0.24244706290108817, "compression_ratio": 1.4673913043478262, "no_speech_prob": 0.0006745847640559077}, {"id": 154, "seek": 98572, "start": 986.52, "end": 996.44, "text": " All right. Another famous machine assistant proof, well, actually initially human proof,", "tokens": [50404, 1057, 558, 13, 3996, 4618, 3479, 10994, 8177, 11, 731, 11, 767, 9105, 1952, 8177, 11, 50900], "temperature": 0.0, "avg_logprob": -0.15510778748587276, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0007753881509415805}, {"id": 155, "seek": 98572, "start": 996.44, "end": 1003.08, "text": " but eventually computer-verified was the proof of the coupler conjecture. So the coupler conjecture", "tokens": [50900, 457, 4728, 3820, 12, 331, 2587, 390, 264, 8177, 295, 264, 1384, 22732, 416, 1020, 540, 13, 407, 264, 1384, 22732, 416, 1020, 540, 51232], "temperature": 0.0, "avg_logprob": -0.15510778748587276, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0007753881509415805}, {"id": 156, "seek": 98572, "start": 1003.08, "end": 1007.08, "text": " is a statement about how efficient that you can pack unit spheres in the plane,", "tokens": [51232, 307, 257, 5629, 466, 577, 7148, 300, 291, 393, 2844, 4985, 41225, 294, 264, 5720, 11, 51432], "temperature": 0.0, "avg_logprob": -0.15510778748587276, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0007753881509415805}, {"id": 157, "seek": 98572, "start": 1008.2, "end": 1013.24, "text": " and so there's a natural way to stack unit spheres, and it's the way that you see oranges stacked in", "tokens": [51488, 293, 370, 456, 311, 257, 3303, 636, 281, 8630, 4985, 41225, 11, 293, 309, 311, 264, 636, 300, 291, 536, 35474, 28867, 294, 51740], "temperature": 0.0, "avg_logprob": -0.15510778748587276, "compression_ratio": 1.7083333333333333, "no_speech_prob": 0.0007753881509415805}, {"id": 158, "seek": 101324, "start": 1013.24, "end": 1017.96, "text": " the grocery store. It's called the hexagonal closed packing, and there's also a dual packing", "tokens": [50364, 264, 14410, 3531, 13, 467, 311, 1219, 264, 23291, 6709, 304, 5395, 20815, 11, 293, 456, 311, 611, 257, 11848, 20815, 50600], "temperature": 0.0, "avg_logprob": -0.11529891045538934, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.000688881438691169}, {"id": 159, "seek": 101324, "start": 1017.96, "end": 1022.76, "text": " with the same density called the cubic closed packing. And they have a certain density, pi over", "tokens": [50600, 365, 264, 912, 10305, 1219, 264, 28733, 5395, 20815, 13, 400, 436, 362, 257, 1629, 10305, 11, 3895, 670, 50840], "temperature": 0.0, "avg_logprob": -0.11529891045538934, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.000688881438691169}, {"id": 160, "seek": 101324, "start": 1022.76, "end": 1029.48, "text": " three over two, and this was conjectured to be the densest packing. So this is an annoyingly hard", "tokens": [50840, 1045, 670, 732, 11, 293, 341, 390, 416, 1020, 3831, 281, 312, 264, 24505, 377, 20815, 13, 407, 341, 307, 364, 11304, 356, 1152, 51176], "temperature": 0.0, "avg_logprob": -0.11529891045538934, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.000688881438691169}, {"id": 161, "seek": 101324, "start": 1029.48, "end": 1038.52, "text": " statement to prove. So it's an optimization problem in infinitely many variables. So each", "tokens": [51176, 5629, 281, 7081, 13, 407, 309, 311, 364, 19618, 1154, 294, 36227, 867, 9102, 13, 407, 1184, 51628], "temperature": 0.0, "avg_logprob": -0.11529891045538934, "compression_ratio": 1.6936936936936937, "no_speech_prob": 0.000688881438691169}, {"id": 162, "seek": 103852, "start": 1039.48, "end": 1042.52, "text": " each sphere has a different location, and there's an infinite number of spheres.", "tokens": [50412, 1184, 16687, 575, 257, 819, 4914, 11, 293, 456, 311, 364, 13785, 1230, 295, 41225, 13, 50564], "temperature": 0.0, "avg_logprob": -0.14537074830796984, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.00029903787071816623}, {"id": 163, "seek": 103852, "start": 1043.08, "end": 1047.32, "text": " So you're trying to prove an inequality involving an infinite number", "tokens": [50592, 407, 291, 434, 1382, 281, 7081, 364, 16970, 17030, 364, 13785, 1230, 50804], "temperature": 0.0, "avg_logprob": -0.14537074830796984, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.00029903787071816623}, {"id": 164, "seek": 103852, "start": 1047.32, "end": 1053.8799999999999, "text": " of variables involving solving infinite number constraints. So it doesn't immediately", "tokens": [50804, 295, 9102, 17030, 12606, 13785, 1230, 18491, 13, 407, 309, 1177, 380, 4258, 51132], "temperature": 0.0, "avg_logprob": -0.14537074830796984, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.00029903787071816623}, {"id": 165, "seek": 103852, "start": 1053.8799999999999, "end": 1060.84, "text": " lend itself to computer verification. But even in the 50s, it was realized that possibly this", "tokens": [51132, 21774, 2564, 281, 3820, 30206, 13, 583, 754, 294, 264, 2625, 82, 11, 309, 390, 5334, 300, 6264, 341, 51480], "temperature": 0.0, "avg_logprob": -0.14537074830796984, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.00029903787071816623}, {"id": 166, "seek": 103852, "start": 1060.84, "end": 1066.92, "text": " could be done by some sort of brute force computation. And so Toth proposed the following", "tokens": [51480, 727, 312, 1096, 538, 512, 1333, 295, 47909, 3464, 24903, 13, 400, 370, 314, 900, 10348, 264, 3480, 51784], "temperature": 0.0, "avg_logprob": -0.14537074830796984, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.00029903787071816623}, {"id": 167, "seek": 106692, "start": 1066.92, "end": 1072.04, "text": " paradigm. So every time you see a packing, it comes with what's called a Voronoi decomposition.", "tokens": [50364, 24709, 13, 407, 633, 565, 291, 536, 257, 20815, 11, 309, 1487, 365, 437, 311, 1219, 257, 12231, 266, 4869, 48356, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12362013192012392, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.000495572981890291}, {"id": 168, "seek": 106692, "start": 1072.04, "end": 1077.16, "text": " So every sphere comes with a Voronoi cell, which is this polytope of all the points that are closer", "tokens": [50620, 407, 633, 16687, 1487, 365, 257, 12231, 266, 4869, 2815, 11, 597, 307, 341, 6754, 83, 1114, 295, 439, 264, 2793, 300, 366, 4966, 50876], "temperature": 0.0, "avg_logprob": -0.12362013192012392, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.000495572981890291}, {"id": 169, "seek": 106692, "start": 1077.16, "end": 1081.0800000000002, "text": " to the center of that of that sphere than to all the other spheres. And there's partitions", "tokens": [50876, 281, 264, 3056, 295, 300, 295, 300, 16687, 813, 281, 439, 264, 661, 41225, 13, 400, 456, 311, 644, 2451, 51072], "temperature": 0.0, "avg_logprob": -0.12362013192012392, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.000495572981890291}, {"id": 170, "seek": 106692, "start": 1081.8000000000002, "end": 1087.72, "text": " space into all these little polyhedron, these Voronoi cells. And there are various relationships", "tokens": [51108, 1901, 666, 439, 613, 707, 6754, 27096, 2044, 11, 613, 12231, 266, 4869, 5438, 13, 400, 456, 366, 3683, 6159, 51404], "temperature": 0.0, "avg_logprob": -0.12362013192012392, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.000495572981890291}, {"id": 171, "seek": 106692, "start": 1087.72, "end": 1092.28, "text": " between the volumes of these different cells. There's only so many spheres that you can pack", "tokens": [51404, 1296, 264, 22219, 295, 613, 819, 5438, 13, 821, 311, 787, 370, 867, 41225, 300, 291, 393, 2844, 51632], "temperature": 0.0, "avg_logprob": -0.12362013192012392, "compression_ratio": 1.8814229249011858, "no_speech_prob": 0.000495572981890291}, {"id": 172, "seek": 109228, "start": 1092.28, "end": 1095.96, "text": " next to one reference sphere, and this creates all these kind of constraints.", "tokens": [50364, 958, 281, 472, 6408, 16687, 11, 293, 341, 7829, 439, 613, 733, 295, 18491, 13, 50548], "temperature": 0.0, "avg_logprob": -0.11263724869372797, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008174200193025172}, {"id": 173, "seek": 109228, "start": 1097.8, "end": 1104.12, "text": " And so the hope was that if you could gather enough inequalities between adjacent Voronoi cells,", "tokens": [50640, 400, 370, 264, 1454, 390, 300, 498, 291, 727, 5448, 1547, 41874, 1296, 24441, 12231, 266, 4869, 5438, 11, 50956], "temperature": 0.0, "avg_logprob": -0.11263724869372797, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008174200193025172}, {"id": 174, "seek": 109228, "start": 1104.84, "end": 1109.48, "text": " the volumes of adjacent Voronoi cells, maybe every such system inequalities, in principle,", "tokens": [50992, 264, 22219, 295, 24441, 12231, 266, 4869, 5438, 11, 1310, 633, 1270, 1185, 41874, 11, 294, 8665, 11, 51224], "temperature": 0.0, "avg_logprob": -0.11263724869372797, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008174200193025172}, {"id": 175, "seek": 109228, "start": 1109.48, "end": 1115.32, "text": " gives you an upper bound on the density of the sphere packing. And in principle, if you get", "tokens": [51224, 2709, 291, 364, 6597, 5472, 322, 264, 10305, 295, 264, 16687, 20815, 13, 400, 294, 8665, 11, 498, 291, 483, 51516], "temperature": 0.0, "avg_logprob": -0.11263724869372797, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008174200193025172}, {"id": 176, "seek": 109228, "start": 1115.32, "end": 1119.72, "text": " enough of these inequalities, maybe you could actually get the optimal bound of three or two.", "tokens": [51516, 1547, 295, 613, 41874, 11, 1310, 291, 727, 767, 483, 264, 16252, 5472, 295, 1045, 420, 732, 13, 51736], "temperature": 0.0, "avg_logprob": -0.11263724869372797, "compression_ratio": 1.8333333333333333, "no_speech_prob": 0.0008174200193025172}, {"id": 177, "seek": 111972, "start": 1120.6000000000001, "end": 1127.64, "text": " So people tried this approach for many, many years, including some false attempts.", "tokens": [50408, 407, 561, 3031, 341, 3109, 337, 867, 11, 867, 924, 11, 3009, 512, 7908, 15257, 13, 50760], "temperature": 0.0, "avg_logprob": -0.14873691926519556, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.00010953030869131908}, {"id": 178, "seek": 111972, "start": 1128.28, "end": 1131.48, "text": " But they were not able to actually make this approach work.", "tokens": [50792, 583, 436, 645, 406, 1075, 281, 767, 652, 341, 3109, 589, 13, 50952], "temperature": 0.0, "avg_logprob": -0.14873691926519556, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.00010953030869131908}, {"id": 179, "seek": 111972, "start": 1134.04, "end": 1139.0, "text": " But Thomas Hales, and then later, with a collaborator, was able to adapt this approach", "tokens": [51080, 583, 8500, 389, 4229, 11, 293, 550, 1780, 11, 365, 257, 5091, 1639, 11, 390, 1075, 281, 6231, 341, 3109, 51328], "temperature": 0.0, "avg_logprob": -0.14873691926519556, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.00010953030869131908}, {"id": 180, "seek": 111972, "start": 1139.0, "end": 1144.76, "text": " to make it work in a series of papers from 94-98. But they had to modify the strategy quite a lot.", "tokens": [51328, 281, 652, 309, 589, 294, 257, 2638, 295, 10577, 490, 30849, 12, 22516, 13, 583, 436, 632, 281, 16927, 264, 5206, 1596, 257, 688, 13, 51616], "temperature": 0.0, "avg_logprob": -0.14873691926519556, "compression_ratio": 1.5922330097087378, "no_speech_prob": 0.00010953030869131908}, {"id": 181, "seek": 114476, "start": 1144.84, "end": 1149.8799999999999, "text": " So instead of using the Voronoi decomposition, there was a more complicated decomposition", "tokens": [50368, 407, 2602, 295, 1228, 264, 12231, 266, 4869, 48356, 11, 456, 390, 257, 544, 6179, 48356, 50620], "temperature": 0.0, "avg_logprob": -0.08007317659806232, "compression_ratio": 1.804255319148936, "no_speech_prob": 0.0028630224987864494}, {"id": 182, "seek": 114476, "start": 1149.8799999999999, "end": 1154.04, "text": " that was used. And instead of using volume, they had to define this new score function", "tokens": [50620, 300, 390, 1143, 13, 400, 2602, 295, 1228, 5523, 11, 436, 632, 281, 6964, 341, 777, 6175, 2445, 50828], "temperature": 0.0, "avg_logprob": -0.08007317659806232, "compression_ratio": 1.804255319148936, "no_speech_prob": 0.0028630224987864494}, {"id": 183, "seek": 114476, "start": 1154.68, "end": 1158.6, "text": " attached to each polyhedron. But basically, the strategy was the same.", "tokens": [50860, 8570, 281, 1184, 6754, 27096, 2044, 13, 583, 1936, 11, 264, 5206, 390, 264, 912, 13, 51056], "temperature": 0.0, "avg_logprob": -0.08007317659806232, "compression_ratio": 1.804255319148936, "no_speech_prob": 0.0028630224987864494}, {"id": 184, "seek": 114476, "start": 1159.72, "end": 1164.28, "text": " And he was able to prove lots and lots of linear inequalities between the scores of adjacent", "tokens": [51112, 400, 415, 390, 1075, 281, 7081, 3195, 293, 3195, 295, 8213, 41874, 1296, 264, 13444, 295, 24441, 51340], "temperature": 0.0, "avg_logprob": -0.08007317659806232, "compression_ratio": 1.804255319148936, "no_speech_prob": 0.0028630224987864494}, {"id": 185, "seek": 114476, "start": 1164.28, "end": 1170.52, "text": " polyhedra. And then just using linear programming was able to then get a bound. And", "tokens": [51340, 6754, 27096, 424, 13, 400, 550, 445, 1228, 8213, 9410, 390, 1075, 281, 550, 483, 257, 5472, 13, 400, 51652], "temperature": 0.0, "avg_logprob": -0.08007317659806232, "compression_ratio": 1.804255319148936, "no_speech_prob": 0.0028630224987864494}, {"id": 186, "seek": 117052, "start": 1171.32, "end": 1175.24, "text": " with the right choice of score and the right choice of partition, it was the optimal bound.", "tokens": [50404, 365, 264, 558, 3922, 295, 6175, 293, 264, 558, 3922, 295, 24808, 11, 309, 390, 264, 16252, 5472, 13, 50600], "temperature": 0.0, "avg_logprob": -0.15615502568601652, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.0005716843297705054}, {"id": 187, "seek": 117052, "start": 1177.72, "end": 1182.04, "text": " It's a very flexible method, because you have lots of ways you can do the partition and lots", "tokens": [50724, 467, 311, 257, 588, 11358, 3170, 11, 570, 291, 362, 3195, 295, 2098, 291, 393, 360, 264, 24808, 293, 3195, 50940], "temperature": 0.0, "avg_logprob": -0.15615502568601652, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.0005716843297705054}, {"id": 188, "seek": 117052, "start": 1182.04, "end": 1187.48, "text": " of ways that you can do the score. But the problem is that it was too flexible. So here's a quote", "tokens": [50940, 295, 2098, 300, 291, 393, 360, 264, 6175, 13, 583, 264, 1154, 307, 300, 309, 390, 886, 11358, 13, 407, 510, 311, 257, 6513, 51212], "temperature": 0.0, "avg_logprob": -0.15615502568601652, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.0005716843297705054}, {"id": 189, "seek": 117052, "start": 1187.48, "end": 1192.84, "text": " from Hales. It says that Simon Ferguson, who was Hales' collaborator, and I realized every time we", "tokens": [51212, 490, 389, 4229, 13, 467, 1619, 300, 13193, 40823, 11, 567, 390, 389, 4229, 6, 5091, 1639, 11, 293, 286, 5334, 633, 565, 321, 51480], "temperature": 0.0, "avg_logprob": -0.15615502568601652, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.0005716843297705054}, {"id": 190, "seek": 117052, "start": 1192.84, "end": 1195.72, "text": " encounter difficulty solving the minimization problem, we get just a scoring function to", "tokens": [51480, 8593, 10360, 12606, 264, 4464, 2144, 1154, 11, 321, 483, 445, 257, 22358, 2445, 281, 51624], "temperature": 0.0, "avg_logprob": -0.15615502568601652, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.0005716843297705054}, {"id": 191, "seek": 117052, "start": 1195.72, "end": 1199.0, "text": " score the difficulty. The function became more complicated, but with each change,", "tokens": [51624, 6175, 264, 10360, 13, 440, 2445, 3062, 544, 6179, 11, 457, 365, 1184, 1319, 11, 51788], "temperature": 0.0, "avg_logprob": -0.15615502568601652, "compression_ratio": 1.8338870431893688, "no_speech_prob": 0.0005716843297705054}, {"id": 192, "seek": 119900, "start": 1199.0, "end": 1203.88, "text": " we could cut months or years from our work. This incessant fiddling was unpopular with my", "tokens": [50364, 321, 727, 1723, 2493, 420, 924, 490, 527, 589, 13, 639, 294, 780, 394, 283, 14273, 1688, 390, 517, 42376, 365, 452, 50608], "temperature": 0.0, "avg_logprob": -0.1285821795463562, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.000865409558173269}, {"id": 193, "seek": 119900, "start": 1203.88, "end": 1207.72, "text": " colleagues. Every time I presented my work in progress at a conference, I was minimizing a", "tokens": [50608, 7734, 13, 2048, 565, 286, 8212, 452, 589, 294, 4205, 412, 257, 7586, 11, 286, 390, 46608, 257, 50800], "temperature": 0.0, "avg_logprob": -0.1285821795463562, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.000865409558173269}, {"id": 194, "seek": 119900, "start": 1207.72, "end": 1211.96, "text": " different function. Even worse, the function was moderately incompatible with earlier papers,", "tokens": [50800, 819, 2445, 13, 2754, 5324, 11, 264, 2445, 390, 10494, 1592, 40393, 267, 964, 365, 3071, 10577, 11, 51012], "temperature": 0.0, "avg_logprob": -0.1285821795463562, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.000865409558173269}, {"id": 195, "seek": 119900, "start": 1211.96, "end": 1219.24, "text": " and this required going back and patching the earlier papers. So the proof was a mess, basically.", "tokens": [51012, 293, 341, 4739, 516, 646, 293, 9972, 278, 264, 3071, 10577, 13, 407, 264, 8177, 390, 257, 2082, 11, 1936, 13, 51376], "temperature": 0.0, "avg_logprob": -0.1285821795463562, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.000865409558173269}, {"id": 196, "seek": 119900, "start": 1220.28, "end": 1225.72, "text": " They did eventually finish it in 98, and they were able to derive the Kepler conjecture from", "tokens": [51428, 814, 630, 4728, 2413, 309, 294, 20860, 11, 293, 436, 645, 1075, 281, 28446, 264, 3189, 22732, 416, 1020, 540, 490, 51700], "temperature": 0.0, "avg_logprob": -0.1285821795463562, "compression_ratio": 1.6431095406360423, "no_speech_prob": 0.000865409558173269}, {"id": 197, "seek": 122572, "start": 1225.72, "end": 1230.84, "text": " a linear programming computation from a very complicated optimization program. Initially,", "tokens": [50364, 257, 8213, 9410, 24903, 490, 257, 588, 6179, 19618, 1461, 13, 29446, 11, 50620], "temperature": 0.0, "avg_logprob": -0.12712045473473094, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0035690446384251118}, {"id": 198, "seek": 122572, "start": 1230.84, "end": 1235.88, "text": " it was done by hand, but with the increased complexity, there was no choice but to make", "tokens": [50620, 309, 390, 1096, 538, 1011, 11, 457, 365, 264, 6505, 14024, 11, 456, 390, 572, 3922, 457, 281, 652, 50872], "temperature": 0.0, "avg_logprob": -0.12712045473473094, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0035690446384251118}, {"id": 199, "seek": 122572, "start": 1235.88, "end": 1243.0, "text": " it more and more computer-assisted. So when the proof was announced, it was a combination of", "tokens": [50872, 309, 544, 293, 544, 3820, 12, 640, 33250, 13, 407, 562, 264, 8177, 390, 7548, 11, 309, 390, 257, 6562, 295, 51228], "temperature": 0.0, "avg_logprob": -0.12712045473473094, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0035690446384251118}, {"id": 200, "seek": 122572, "start": 1243.0, "end": 1249.4, "text": " 250 pages of notes and lots and lots of gigabytes of programs and data, and it was famously hard", "tokens": [51228, 11650, 7183, 295, 5570, 293, 3195, 293, 3195, 295, 42741, 295, 4268, 293, 1412, 11, 293, 309, 390, 34360, 1152, 51548], "temperature": 0.0, "avg_logprob": -0.12712045473473094, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0035690446384251118}, {"id": 201, "seek": 122572, "start": 1249.4, "end": 1254.1200000000001, "text": " to referee. It took four years for annals to referee the paper with a panel of pro referees,", "tokens": [51548, 281, 43096, 13, 467, 1890, 1451, 924, 337, 2324, 1124, 281, 43096, 264, 3035, 365, 257, 4831, 295, 447, 33048, 279, 11, 51784], "temperature": 0.0, "avg_logprob": -0.12712045473473094, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0035690446384251118}, {"id": 202, "seek": 125412, "start": 1254.1999999999998, "end": 1257.6399999999999, "text": " and even then, the panel was only 99 percent certain of the correctness of the proof, and they", "tokens": [50368, 293, 754, 550, 11, 264, 4831, 390, 787, 11803, 3043, 1629, 295, 264, 3006, 1287, 295, 264, 8177, 11, 293, 436, 50540], "temperature": 0.0, "avg_logprob": -0.15505880779690212, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.00039417072548530996}, {"id": 203, "seek": 125412, "start": 1257.6399999999999, "end": 1265.6399999999999, "text": " could certify the corrections, the calculations. Because, I mean, in principle, it was all doable,", "tokens": [50540, 727, 5351, 2505, 264, 36406, 11, 264, 20448, 13, 1436, 11, 286, 914, 11, 294, 8665, 11, 309, 390, 439, 41183, 11, 50940], "temperature": 0.0, "avg_logprob": -0.15505880779690212, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.00039417072548530996}, {"id": 204, "seek": 125412, "start": 1265.6399999999999, "end": 1271.1599999999999, "text": " but the referees have to implement all these different computer calculations themselves,", "tokens": [50940, 457, 264, 33048, 279, 362, 281, 4445, 439, 613, 819, 3820, 20448, 2969, 11, 51216], "temperature": 0.0, "avg_logprob": -0.15505880779690212, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.00039417072548530996}, {"id": 205, "seek": 125412, "start": 1271.9599999999998, "end": 1278.36, "text": " but it was eventually accepted. But clearly, there was this big asterisk. There was a lot of", "tokens": [51256, 457, 309, 390, 4728, 9035, 13, 583, 4448, 11, 456, 390, 341, 955, 257, 3120, 7797, 13, 821, 390, 257, 688, 295, 51576], "temperature": 0.0, "avg_logprob": -0.15505880779690212, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.00039417072548530996}, {"id": 206, "seek": 125412, "start": 1278.36, "end": 1281.8, "text": " controversy about whether this was really a valid proof, and so this was one of the", "tokens": [51576, 22976, 466, 1968, 341, 390, 534, 257, 7363, 8177, 11, 293, 370, 341, 390, 472, 295, 264, 51748], "temperature": 0.0, "avg_logprob": -0.15505880779690212, "compression_ratio": 1.7859922178988328, "no_speech_prob": 0.00039417072548530996}, {"id": 207, "seek": 128180, "start": 1282.6, "end": 1289.48, "text": " the first really high-profile uses of formal proof assistance, because this was a result in which", "tokens": [50404, 264, 700, 534, 1090, 12, 29175, 794, 4960, 295, 9860, 8177, 9683, 11, 570, 341, 390, 257, 1874, 294, 597, 50748], "temperature": 0.0, "avg_logprob": -0.14093654856962315, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.0009703275864012539}, {"id": 208, "seek": 128180, "start": 1289.48, "end": 1297.08, "text": " there was serious doubt about the correctness. So they created, so Hales in 2003 initiated a", "tokens": [50748, 456, 390, 3156, 6385, 466, 264, 3006, 1287, 13, 407, 436, 2942, 11, 370, 389, 4229, 294, 16416, 28578, 257, 51128], "temperature": 0.0, "avg_logprob": -0.14093654856962315, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.0009703275864012539}, {"id": 209, "seek": 128180, "start": 1297.08, "end": 1302.84, "text": " project to write down this massive proof in a completely formalized way so that a standard", "tokens": [51128, 1716, 281, 2464, 760, 341, 5994, 8177, 294, 257, 2584, 9860, 1602, 636, 370, 300, 257, 3832, 51416], "temperature": 0.0, "avg_logprob": -0.14093654856962315, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.0009703275864012539}, {"id": 210, "seek": 128180, "start": 1302.84, "end": 1307.8, "text": " proof assistant could verify it. He estimated it would take 20 years to make this work,", "tokens": [51416, 8177, 10994, 727, 16888, 309, 13, 634, 14109, 309, 576, 747, 945, 924, 281, 652, 341, 589, 11, 51664], "temperature": 0.0, "avg_logprob": -0.14093654856962315, "compression_ratio": 1.5905172413793103, "no_speech_prob": 0.0009703275864012539}, {"id": 211, "seek": 130780, "start": 1308.52, "end": 1315.0, "text": " and so he had, he gathered 21 collaborators. It actually only took 11 years,", "tokens": [50400, 293, 370, 415, 632, 11, 415, 13032, 5080, 39789, 13, 467, 767, 787, 1890, 2975, 924, 11, 50724], "temperature": 0.0, "avg_logprob": -0.18360439001345166, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0016043302603065968}, {"id": 212, "seek": 130780, "start": 1317.24, "end": 1322.2, "text": " but yeah, eventually what they did was that they first created a blueprint,", "tokens": [50836, 457, 1338, 11, 4728, 437, 436, 630, 390, 300, 436, 700, 2942, 257, 35868, 11, 51084], "temperature": 0.0, "avg_logprob": -0.18360439001345166, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0016043302603065968}, {"id": 213, "seek": 130780, "start": 1322.2, "end": 1325.6399999999999, "text": " you know, so a human readable version of the proof breaking things up into very,", "tokens": [51084, 291, 458, 11, 370, 257, 1952, 49857, 3037, 295, 264, 8177, 7697, 721, 493, 666, 588, 11, 51256], "temperature": 0.0, "avg_logprob": -0.18360439001345166, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0016043302603065968}, {"id": 214, "seek": 130780, "start": 1325.6399999999999, "end": 1330.6, "text": " very small steps, and then they formalized each step by bit, and it was finally done,", "tokens": [51256, 588, 1359, 4439, 11, 293, 550, 436, 9860, 1602, 1184, 1823, 538, 857, 11, 293, 309, 390, 2721, 1096, 11, 51504], "temperature": 0.0, "avg_logprob": -0.18360439001345166, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0016043302603065968}, {"id": 215, "seek": 130780, "start": 1330.6, "end": 1334.68, "text": " and then there was a, yeah, so they published a paper about the formalization, and that only", "tokens": [51504, 293, 550, 456, 390, 257, 11, 1338, 11, 370, 436, 6572, 257, 3035, 466, 264, 9860, 2144, 11, 293, 300, 787, 51708], "temperature": 0.0, "avg_logprob": -0.18360439001345166, "compression_ratio": 1.7095435684647302, "no_speech_prob": 0.0016043302603065968}, {"id": 216, "seek": 133468, "start": 1334.68, "end": 1341.0, "text": " appeared in 2017. So this was sort of the state-of-the-art of formalization, you know,", "tokens": [50364, 8516, 294, 6591, 13, 407, 341, 390, 1333, 295, 264, 1785, 12, 2670, 12, 3322, 12, 446, 295, 9860, 2144, 11, 291, 458, 11, 50680], "temperature": 0.0, "avg_logprob": -0.1466906935304076, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.0010712085058912635}, {"id": 217, "seek": 133468, "start": 1341.0, "end": 1345.88, "text": " as of say 20 years ago, you know, like it was possible to formalize big complicated results,", "tokens": [50680, 382, 295, 584, 945, 924, 2057, 11, 291, 458, 11, 411, 309, 390, 1944, 281, 9860, 1125, 955, 6179, 3542, 11, 50924], "temperature": 0.0, "avg_logprob": -0.1466906935304076, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.0010712085058912635}, {"id": 218, "seek": 133468, "start": 1345.88, "end": 1354.1200000000001, "text": " but it took an enormous amount of effort, you know, not something which you would do routinely.", "tokens": [50924, 457, 309, 1890, 364, 11322, 2372, 295, 4630, 11, 291, 458, 11, 406, 746, 597, 291, 576, 360, 40443, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1466906935304076, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.0010712085058912635}, {"id": 219, "seek": 133468, "start": 1356.1200000000001, "end": 1359.72, "text": " There was a more recent effort in a similar spirit by Peter Schultzer,", "tokens": [51436, 821, 390, 257, 544, 5162, 4630, 294, 257, 2531, 3797, 538, 6508, 2065, 723, 4527, 11, 51616], "temperature": 0.0, "avg_logprob": -0.1466906935304076, "compression_ratio": 1.5515695067264574, "no_speech_prob": 0.0010712085058912635}, {"id": 220, "seek": 135972, "start": 1359.72, "end": 1365.16, "text": " he called it the liquid tensor experiment. So Schultzer introduced this theory of condensed", "tokens": [50364, 415, 1219, 309, 264, 6553, 40863, 5120, 13, 407, 2065, 723, 4527, 7268, 341, 5261, 295, 36398, 50636], "temperature": 0.0, "avg_logprob": -0.1838948974609375, "compression_ratio": 1.8151815181518152, "no_speech_prob": 0.0040190028958022594}, {"id": 221, "seek": 135972, "start": 1365.16, "end": 1370.44, "text": " mathematics, which is, all right, this is really far from my own area of expertise, but", "tokens": [50636, 18666, 11, 597, 307, 11, 439, 558, 11, 341, 307, 534, 1400, 490, 452, 1065, 1859, 295, 11769, 11, 457, 50900], "temperature": 0.0, "avg_logprob": -0.1838948974609375, "compression_ratio": 1.8151815181518152, "no_speech_prob": 0.0040190028958022594}, {"id": 222, "seek": 135972, "start": 1371.8, "end": 1376.84, "text": " basically there are certain problems with, so certain types of mathematics you want to work", "tokens": [50968, 1936, 456, 366, 1629, 2740, 365, 11, 370, 1629, 3467, 295, 18666, 291, 528, 281, 589, 51220], "temperature": 0.0, "avg_logprob": -0.1838948974609375, "compression_ratio": 1.8151815181518152, "no_speech_prob": 0.0040190028958022594}, {"id": 223, "seek": 135972, "start": 1376.84, "end": 1380.3600000000001, "text": " in various categories, like categories of topological obedient groups and topological vector spaces,", "tokens": [51220, 294, 3683, 10479, 11, 411, 10479, 295, 1192, 4383, 42541, 3935, 293, 1192, 4383, 8062, 7673, 11, 51396], "temperature": 0.0, "avg_logprob": -0.1838948974609375, "compression_ratio": 1.8151815181518152, "no_speech_prob": 0.0040190028958022594}, {"id": 224, "seek": 135972, "start": 1380.92, "end": 1384.68, "text": " and there's a problem that they're not obedient categories, that they don't, they don't have,", "tokens": [51424, 293, 456, 311, 257, 1154, 300, 436, 434, 406, 42541, 10479, 11, 300, 436, 500, 380, 11, 436, 500, 380, 362, 11, 51612], "temperature": 0.0, "avg_logprob": -0.1838948974609375, "compression_ratio": 1.8151815181518152, "no_speech_prob": 0.0040190028958022594}, {"id": 225, "seek": 135972, "start": 1384.68, "end": 1387.8, "text": " I have a good notion of kernel and cold kernel, and things don't work out properly.", "tokens": [51612, 286, 362, 257, 665, 10710, 295, 28256, 293, 3554, 28256, 11, 293, 721, 500, 380, 589, 484, 6108, 13, 51768], "temperature": 0.0, "avg_logprob": -0.1838948974609375, "compression_ratio": 1.8151815181518152, "no_speech_prob": 0.0040190028958022594}, {"id": 226, "seek": 138780, "start": 1388.52, "end": 1394.28, "text": " So he proposed replacing all of these standard categories with a more fancy version called", "tokens": [50400, 407, 415, 10348, 19139, 439, 295, 613, 3832, 10479, 365, 257, 544, 10247, 3037, 1219, 50688], "temperature": 0.0, "avg_logprob": -0.18212213703230315, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00023887907445896417}, {"id": 227, "seek": 138780, "start": 1394.28, "end": 1401.1599999999999, "text": " a condensed category, which has better category theoretic properties, and so the hope is that", "tokens": [50688, 257, 36398, 7719, 11, 597, 575, 1101, 7719, 14308, 299, 7221, 11, 293, 370, 264, 1454, 307, 300, 51032], "temperature": 0.0, "avg_logprob": -0.18212213703230315, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00023887907445896417}, {"id": 228, "seek": 138780, "start": 1401.1599999999999, "end": 1406.84, "text": " you could use a lot more high-powered algebra to attack, to handle things with topological", "tokens": [51032, 291, 727, 764, 257, 688, 544, 1090, 12, 27178, 21989, 281, 2690, 11, 281, 4813, 721, 365, 1192, 4383, 51316], "temperature": 0.0, "avg_logprob": -0.18212213703230315, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00023887907445896417}, {"id": 229, "seek": 138780, "start": 1406.84, "end": 1410.04, "text": " structure or analytical structure, like function spaces, for example, binoc spaces.", "tokens": [51316, 3877, 420, 29579, 3877, 11, 411, 2445, 7673, 11, 337, 1365, 11, 5171, 905, 7673, 13, 51476], "temperature": 0.0, "avg_logprob": -0.18212213703230315, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00023887907445896417}, {"id": 230, "seek": 138780, "start": 1412.76, "end": 1417.1599999999999, "text": " But in order for you to work, there's a certain vanishing theorem, which I've written there,", "tokens": [51612, 583, 294, 1668, 337, 291, 281, 589, 11, 456, 311, 257, 1629, 3161, 3807, 20904, 11, 597, 286, 600, 3720, 456, 11, 51832], "temperature": 0.0, "avg_logprob": -0.18212213703230315, "compression_ratio": 1.6678966789667897, "no_speech_prob": 0.00023887907445896417}, {"id": 231, "seek": 141716, "start": 1417.72, "end": 1423.0800000000002, "text": " but I cannot explain to you. Okay, so there's a certain category,", "tokens": [50392, 457, 286, 2644, 2903, 281, 291, 13, 1033, 11, 370, 456, 311, 257, 1629, 7719, 11, 50660], "temperature": 0.0, "avg_logprob": -0.23017604699295557, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.00028592717717401683}, {"id": 232, "seek": 141716, "start": 1426.76, "end": 1430.76, "text": " condescending groups, and there's an x-group involving p-binoc spaces that has to vanish,", "tokens": [50844, 2224, 37890, 2029, 3935, 11, 293, 456, 311, 364, 2031, 12, 17377, 17030, 280, 12, 13496, 905, 7673, 300, 575, 281, 43584, 11, 51044], "temperature": 0.0, "avg_logprob": -0.23017604699295557, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.00028592717717401683}, {"id": 233, "seek": 141716, "start": 1432.68, "end": 1437.16, "text": " and this vanishing theorem is needed in order for all of the rest of the theory to actually be", "tokens": [51140, 293, 341, 3161, 3807, 20904, 307, 2978, 294, 1668, 337, 439, 295, 264, 1472, 295, 264, 5261, 281, 767, 312, 51364], "temperature": 0.0, "avg_logprob": -0.23017604699295557, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.00028592717717401683}, {"id": 234, "seek": 141716, "start": 1437.16, "end": 1445.5600000000002, "text": " useful, if you want to apply it to function analysis in particular. And so Schultzer,", "tokens": [51364, 4420, 11, 498, 291, 528, 281, 3079, 309, 281, 2445, 5215, 294, 1729, 13, 400, 370, 2065, 723, 4527, 11, 51784], "temperature": 0.0, "avg_logprob": -0.23017604699295557, "compression_ratio": 1.5412844036697249, "no_speech_prob": 0.00028592717717401683}, {"id": 235, "seek": 144556, "start": 1445.56, "end": 1449.56, "text": " what a blog post about this is that, you know, I spent much of 2019 obsessed with the proofless", "tokens": [50364, 437, 257, 6968, 2183, 466, 341, 307, 300, 11, 291, 458, 11, 286, 4418, 709, 295, 6071, 16923, 365, 264, 8177, 1832, 50564], "temperature": 0.0, "avg_logprob": -0.1493716075502593, "compression_ratio": 1.5912162162162162, "no_speech_prob": 0.0026500362437218428}, {"id": 236, "seek": 144556, "start": 1449.56, "end": 1453.56, "text": " theorem, almost getting crazy over it. In the end, we were able to get an argument down on", "tokens": [50564, 20904, 11, 1920, 1242, 3219, 670, 309, 13, 682, 264, 917, 11, 321, 645, 1075, 281, 483, 364, 6770, 760, 322, 50764], "temperature": 0.0, "avg_logprob": -0.1493716075502593, "compression_ratio": 1.5912162162162162, "no_speech_prob": 0.0026500362437218428}, {"id": 237, "seek": 144556, "start": 1453.56, "end": 1457.08, "text": " paper, but I think no one else has ever dared to look at the details of this, and so I still", "tokens": [50764, 3035, 11, 457, 286, 519, 572, 472, 1646, 575, 1562, 44564, 281, 574, 412, 264, 4365, 295, 341, 11, 293, 370, 286, 920, 50940], "temperature": 0.0, "avg_logprob": -0.1493716075502593, "compression_ratio": 1.5912162162162162, "no_speech_prob": 0.0026500362437218428}, {"id": 238, "seek": 144556, "start": 1457.08, "end": 1464.36, "text": " have some lingering doubts. With this hope, with this theorem, the hope that the condensed formulas", "tokens": [50940, 362, 512, 49542, 22618, 13, 2022, 341, 1454, 11, 365, 341, 20904, 11, 264, 1454, 300, 264, 36398, 30546, 51304], "temperature": 0.0, "avg_logprob": -0.1493716075502593, "compression_ratio": 1.5912162162162162, "no_speech_prob": 0.0026500362437218428}, {"id": 239, "seek": 144556, "start": 1464.36, "end": 1469.32, "text": " can be fruitfully applied to function analysis stands a force, being 99% sure is not enough", "tokens": [51304, 393, 312, 6773, 2277, 6456, 281, 2445, 5215, 7382, 257, 3464, 11, 885, 11803, 4, 988, 307, 406, 1547, 51552], "temperature": 0.0, "avg_logprob": -0.1493716075502593, "compression_ratio": 1.5912162162162162, "no_speech_prob": 0.0026500362437218428}, {"id": 240, "seek": 146932, "start": 1469.3999999999999, "end": 1477.24, "text": " because this theorem is of the most fundamental importance. He says, I think this may be my most", "tokens": [50368, 570, 341, 20904, 307, 295, 264, 881, 8088, 7379, 13, 634, 1619, 11, 286, 519, 341, 815, 312, 452, 881, 50760], "temperature": 0.0, "avg_logprob": -0.17203365053449357, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.001985257025808096}, {"id": 241, "seek": 146932, "start": 1477.24, "end": 1481.72, "text": " important theorem to date, which is a really big claim, actually. Better be sure it's correct.", "tokens": [50760, 1021, 20904, 281, 4002, 11, 597, 307, 257, 534, 955, 3932, 11, 767, 13, 15753, 312, 988, 309, 311, 3006, 13, 50984], "temperature": 0.0, "avg_logprob": -0.17203365053449357, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.001985257025808096}, {"id": 242, "seek": 146932, "start": 1482.52, "end": 1487.72, "text": " So this was another case where there was a great desire to formalize the proof.", "tokens": [51024, 407, 341, 390, 1071, 1389, 689, 456, 390, 257, 869, 7516, 281, 9860, 1125, 264, 8177, 13, 51284], "temperature": 0.0, "avg_logprob": -0.17203365053449357, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.001985257025808096}, {"id": 243, "seek": 146932, "start": 1489.56, "end": 1495.3999999999999, "text": " So he asked publicly on the internet for help to formalize this in a modern", "tokens": [51376, 407, 415, 2351, 14843, 322, 264, 4705, 337, 854, 281, 9860, 1125, 341, 294, 257, 4363, 51668], "temperature": 0.0, "avg_logprob": -0.17203365053449357, "compression_ratio": 1.6445497630331753, "no_speech_prob": 0.001985257025808096}, {"id": 244, "seek": 149540, "start": 1495.4, "end": 1502.2800000000002, "text": " proof of this in language called Lean. And so again, about 20 people, I think, joined this", "tokens": [50364, 8177, 295, 341, 294, 2856, 1219, 49303, 13, 400, 370, 797, 11, 466, 945, 561, 11, 286, 519, 11, 6869, 341, 50708], "temperature": 0.0, "avg_logprob": -0.22467435639480066, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0015753800980746746}, {"id": 245, "seek": 149540, "start": 1502.2800000000002, "end": 1512.0400000000002, "text": " project to help out. And it, so Lean is based on, it has this philosophy where it has this", "tokens": [50708, 1716, 281, 854, 484, 13, 400, 309, 11, 370, 49303, 307, 2361, 322, 11, 309, 575, 341, 10675, 689, 309, 575, 341, 51196], "temperature": 0.0, "avg_logprob": -0.22467435639480066, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0015753800980746746}, {"id": 246, "seek": 149540, "start": 1512.0400000000002, "end": 1517.24, "text": " single huge library of mathematical theorems, like the fundamental calculus or the classification", "tokens": [51196, 2167, 2603, 6405, 295, 18894, 10299, 2592, 11, 411, 264, 8088, 33400, 420, 264, 21538, 51456], "temperature": 0.0, "avg_logprob": -0.22467435639480066, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0015753800980746746}, {"id": 247, "seek": 149540, "start": 1517.24, "end": 1521.48, "text": " of finite building groups, like a lot of the basic theorems of mathematics are already formalized", "tokens": [51456, 295, 19362, 2390, 3935, 11, 411, 257, 688, 295, 264, 3875, 10299, 2592, 295, 18666, 366, 1217, 9860, 1602, 51668], "temperature": 0.0, "avg_logprob": -0.22467435639480066, "compression_ratio": 1.632034632034632, "no_speech_prob": 0.0015753800980746746}, {"id": 248, "seek": 152148, "start": 1521.48, "end": 1525.64, "text": " in this big library. And the idea is to just keep building on this library and adding to it", "tokens": [50364, 294, 341, 955, 6405, 13, 400, 264, 1558, 307, 281, 445, 1066, 2390, 322, 341, 6405, 293, 5127, 281, 309, 50572], "temperature": 0.0, "avg_logprob": -0.20039654622035744, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.001646555494517088}, {"id": 249, "seek": 152148, "start": 1525.64, "end": 1533.48, "text": " with additional projects. But one of the problems with that short of the face was that a lot of the", "tokens": [50572, 365, 4497, 4455, 13, 583, 472, 295, 264, 2740, 365, 300, 2099, 295, 264, 1851, 390, 300, 257, 688, 295, 264, 50964], "temperature": 0.0, "avg_logprob": -0.20039654622035744, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.001646555494517088}, {"id": 250, "seek": 152148, "start": 1534.3600000000001, "end": 1537.32, "text": " tools that, basic tools that you needed, like homological algebra and chief theory and top", "tokens": [51008, 3873, 300, 11, 3875, 3873, 300, 291, 2978, 11, 411, 3655, 4383, 21989, 293, 9588, 5261, 293, 1192, 51156], "temperature": 0.0, "avg_logprob": -0.20039654622035744, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.001646555494517088}, {"id": 251, "seek": 152148, "start": 1537.32, "end": 1541.08, "text": " box theory, weren't actually in the library yet. So actually part of what they had to do was actually", "tokens": [51156, 2424, 5261, 11, 4999, 380, 767, 294, 264, 6405, 1939, 13, 407, 767, 644, 295, 437, 436, 632, 281, 360, 390, 767, 51344], "temperature": 0.0, "avg_logprob": -0.20039654622035744, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.001646555494517088}, {"id": 252, "seek": 152148, "start": 1541.08, "end": 1547.16, "text": " set up the foundations of that theory and formalize it in the library first. But it basically was", "tokens": [51344, 992, 493, 264, 22467, 295, 300, 5261, 293, 9860, 1125, 309, 294, 264, 6405, 700, 13, 583, 309, 1936, 390, 51648], "temperature": 0.0, "avg_logprob": -0.20039654622035744, "compression_ratio": 1.8901960784313725, "no_speech_prob": 0.001646555494517088}, {"id": 253, "seek": 154716, "start": 1547.16, "end": 1554.2, "text": " done in about two years. In one year, they formalized a key sub theorem. And then the whole", "tokens": [50364, 1096, 294, 466, 732, 924, 13, 682, 472, 1064, 11, 436, 9860, 1602, 257, 2141, 1422, 20904, 13, 400, 550, 264, 1379, 50716], "temperature": 0.0, "avg_logprob": -0.12445402145385742, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.002115940907970071}, {"id": 254, "seek": 154716, "start": 1554.2, "end": 1562.76, "text": " thing was formalized about a year afterwards. It did have some, in addition to sort of making", "tokens": [50716, 551, 390, 9860, 1602, 466, 257, 1064, 10543, 13, 467, 630, 362, 512, 11, 294, 4500, 281, 1333, 295, 1455, 51144], "temperature": 0.0, "avg_logprob": -0.12445402145385742, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.002115940907970071}, {"id": 255, "seek": 154716, "start": 1563.3200000000002, "end": 1569.72, "text": " reassuring Peter that it was, everything was correct, there was some other minor benefits.", "tokens": [51172, 19486, 1345, 6508, 300, 309, 390, 11, 1203, 390, 3006, 11, 456, 390, 512, 661, 6696, 5311, 13, 51492], "temperature": 0.0, "avg_logprob": -0.12445402145385742, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.002115940907970071}, {"id": 256, "seek": 154716, "start": 1569.72, "end": 1573.0800000000002, "text": " So first of all, there were actually a few minor errors in the proof that were discovered", "tokens": [51492, 407, 700, 295, 439, 11, 456, 645, 767, 257, 1326, 6696, 13603, 294, 264, 8177, 300, 645, 6941, 51660], "temperature": 0.0, "avg_logprob": -0.12445402145385742, "compression_ratio": 1.6944444444444444, "no_speech_prob": 0.002115940907970071}, {"id": 257, "seek": 157308, "start": 1573.08, "end": 1578.6799999999998, "text": " in the formalization progress. But they go refixed also some small simplifications.", "tokens": [50364, 294, 264, 9860, 2144, 4205, 13, 583, 436, 352, 1895, 40303, 611, 512, 1359, 6883, 7833, 13, 50644], "temperature": 0.0, "avg_logprob": -0.16191431328102393, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0007192459888756275}, {"id": 258, "seek": 157308, "start": 1579.32, "end": 1586.1999999999998, "text": " There was one big simplification actually. So they, the original proof used something very", "tokens": [50676, 821, 390, 472, 955, 6883, 3774, 767, 13, 407, 436, 11, 264, 3380, 8177, 1143, 746, 588, 51020], "temperature": 0.0, "avg_logprob": -0.16191431328102393, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0007192459888756275}, {"id": 259, "seek": 157308, "start": 1586.1999999999998, "end": 1591.1599999999999, "text": " complicated, which I also don't understand, called the Breen-Deline resolution. But in the course", "tokens": [51020, 6179, 11, 597, 286, 611, 500, 380, 1223, 11, 1219, 264, 363, 1492, 12, 40848, 533, 8669, 13, 583, 294, 264, 1164, 51268], "temperature": 0.0, "avg_logprob": -0.16191431328102393, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0007192459888756275}, {"id": 260, "seek": 157308, "start": 1591.1599999999999, "end": 1595.6399999999999, "text": " of formalizing it, it was too hard to actually formalize this theory. But they found that there", "tokens": [51268, 295, 9860, 3319, 309, 11, 309, 390, 886, 1152, 281, 767, 9860, 1125, 341, 5261, 13, 583, 436, 1352, 300, 456, 51492], "temperature": 0.0, "avg_logprob": -0.16191431328102393, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0007192459888756275}, {"id": 261, "seek": 157308, "start": 1595.6399999999999, "end": 1599.72, "text": " was a weaker version of this theory, which was good enough to formalize this application. But", "tokens": [51492, 390, 257, 24286, 3037, 295, 341, 5261, 11, 597, 390, 665, 1547, 281, 9860, 1125, 341, 3861, 13, 583, 51696], "temperature": 0.0, "avg_logprob": -0.16191431328102393, "compression_ratio": 1.8188976377952757, "no_speech_prob": 0.0007192459888756275}, {"id": 262, "seek": 159972, "start": 1599.72, "end": 1604.52, "text": " this was actually a major discovery because this weaker theory could also potentially attack", "tokens": [50364, 341, 390, 767, 257, 2563, 12114, 570, 341, 24286, 5261, 727, 611, 7263, 2690, 50604], "temperature": 0.0, "avg_logprob": -0.1654817263285319, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.0002334797172807157}, {"id": 263, "seek": 159972, "start": 1604.52, "end": 1610.44, "text": " some other problems that the Breen-Deline resolution is not well suited for. And now", "tokens": [50604, 512, 661, 2740, 300, 264, 363, 1492, 12, 40848, 533, 8669, 307, 406, 731, 24736, 337, 13, 400, 586, 50900], "temperature": 0.0, "avg_logprob": -0.1654817263285319, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.0002334797172807157}, {"id": 264, "seek": 159972, "start": 1610.44, "end": 1617.08, "text": " the math library is much, much better in, it has a lot of support for homological", "tokens": [50900, 264, 5221, 6405, 307, 709, 11, 709, 1101, 294, 11, 309, 575, 257, 688, 295, 1406, 337, 3655, 4383, 51232], "temperature": 0.0, "avg_logprob": -0.1654817263285319, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.0002334797172807157}, {"id": 265, "seek": 159972, "start": 1617.08, "end": 1621.56, "text": " algebra and sheath theory and lots of other things, which helped other formalization projects", "tokens": [51232, 21989, 293, 750, 998, 5261, 293, 3195, 295, 661, 721, 11, 597, 4254, 661, 9860, 2144, 4455, 51456], "temperature": 0.0, "avg_logprob": -0.1654817263285319, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.0002334797172807157}, {"id": 266, "seek": 159972, "start": 1621.56, "end": 1627.32, "text": " become easier. I got interested in this formalization a few months ago.", "tokens": [51456, 1813, 3571, 13, 286, 658, 3102, 294, 341, 9860, 2144, 257, 1326, 2493, 2057, 13, 51744], "temperature": 0.0, "avg_logprob": -0.1654817263285319, "compression_ratio": 1.628352490421456, "no_speech_prob": 0.0002334797172807157}, {"id": 267, "seek": 162732, "start": 1627.3999999999999, "end": 1636.12, "text": " So, oh, hang on. I should say, like with the Kepler experiment, so you don't just directly", "tokens": [50368, 407, 11, 1954, 11, 3967, 322, 13, 286, 820, 584, 11, 411, 365, 264, 3189, 22732, 5120, 11, 370, 291, 500, 380, 445, 3838, 50804], "temperature": 0.0, "avg_logprob": -0.19381298337663924, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.0002223183837486431}, {"id": 268, "seek": 162732, "start": 1636.12, "end": 1644.36, "text": " transfer a paper from, you know, the archive or something into Breen. What we found is that", "tokens": [50804, 5003, 257, 3035, 490, 11, 291, 458, 11, 264, 23507, 420, 746, 666, 363, 1492, 13, 708, 321, 1352, 307, 300, 51216], "temperature": 0.0, "avg_logprob": -0.19381298337663924, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.0002223183837486431}, {"id": 269, "seek": 162732, "start": 1644.36, "end": 1648.6799999999998, "text": " it really helps to create first an intermediate document. So halfway between a human readable proof", "tokens": [51216, 309, 534, 3665, 281, 1884, 700, 364, 19376, 4166, 13, 407, 15461, 1296, 257, 1952, 49857, 8177, 51432], "temperature": 0.0, "avg_logprob": -0.19381298337663924, "compression_ratio": 1.4242424242424243, "no_speech_prob": 0.0002223183837486431}, {"id": 270, "seek": 164868, "start": 1648.68, "end": 1658.1200000000001, "text": " and a formal proof, which we call a blueprint. So it looks like a human proof and is written in a", "tokens": [50364, 293, 257, 9860, 8177, 11, 597, 321, 818, 257, 35868, 13, 407, 309, 1542, 411, 257, 1952, 8177, 293, 307, 3720, 294, 257, 50836], "temperature": 0.0, "avg_logprob": -0.12790668711942785, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0014900141395628452}, {"id": 271, "seek": 164868, "start": 1658.68, "end": 1665.64, "text": " version of latex. But like each step in the proof is linked to a lemma in lean. And so it's very", "tokens": [50864, 3037, 295, 3469, 87, 13, 583, 411, 1184, 1823, 294, 264, 8177, 307, 9408, 281, 257, 7495, 1696, 294, 11659, 13, 400, 370, 309, 311, 588, 51212], "temperature": 0.0, "avg_logprob": -0.12790668711942785, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0014900141395628452}, {"id": 272, "seek": 164868, "start": 1665.64, "end": 1672.28, "text": " tightly integrated. It has a very nice feature. It can create dependency graphs, which I'll show you", "tokens": [51212, 21952, 10919, 13, 467, 575, 257, 588, 1481, 4111, 13, 467, 393, 1884, 33621, 24877, 11, 597, 286, 603, 855, 291, 51544], "temperature": 0.0, "avg_logprob": -0.12790668711942785, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0014900141395628452}, {"id": 273, "seek": 164868, "start": 1672.28, "end": 1676.1200000000001, "text": " an example later. You can see which parts of the proof have been formalized, which ones are not,", "tokens": [51544, 364, 1365, 1780, 13, 509, 393, 536, 597, 3166, 295, 264, 8177, 362, 668, 9860, 1602, 11, 597, 2306, 366, 406, 11, 51736], "temperature": 0.0, "avg_logprob": -0.12790668711942785, "compression_ratio": 1.6333333333333333, "no_speech_prob": 0.0014900141395628452}, {"id": 274, "seek": 167612, "start": 1676.12, "end": 1682.52, "text": " and what depends on what. It actually, it gives a lot of structure to the process of writing a paper,", "tokens": [50364, 293, 437, 5946, 322, 437, 13, 467, 767, 11, 309, 2709, 257, 688, 295, 3877, 281, 264, 1399, 295, 3579, 257, 3035, 11, 50684], "temperature": 0.0, "avg_logprob": -0.14946791502806517, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0017114372458308935}, {"id": 275, "seek": 167612, "start": 1682.52, "end": 1686.76, "text": " which, you know, right now we do by hand without sort of much assistance, really.", "tokens": [50684, 597, 11, 291, 458, 11, 558, 586, 321, 360, 538, 1011, 1553, 1333, 295, 709, 9683, 11, 534, 13, 50896], "temperature": 0.0, "avg_logprob": -0.14946791502806517, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0017114372458308935}, {"id": 276, "seek": 167612, "start": 1689.1599999999999, "end": 1692.6799999999998, "text": " Yeah, Schultz has said that the process of writing the blueprint really helped him understand the", "tokens": [51016, 865, 11, 2065, 723, 89, 575, 848, 300, 264, 1399, 295, 3579, 264, 35868, 534, 4254, 796, 1223, 264, 51192], "temperature": 0.0, "avg_logprob": -0.14946791502806517, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0017114372458308935}, {"id": 277, "seek": 167612, "start": 1692.6799999999998, "end": 1701.2399999999998, "text": " proof a lot better. And actually, people have been also going the other way. There's also software", "tokens": [51192, 8177, 257, 688, 1101, 13, 400, 767, 11, 561, 362, 668, 611, 516, 264, 661, 636, 13, 821, 311, 611, 4722, 51620], "temperature": 0.0, "avg_logprob": -0.14946791502806517, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0017114372458308935}, {"id": 278, "seek": 167612, "start": 1701.2399999999998, "end": 1704.1999999999998, "text": " that takes formal proofs, which are written in something that looks like computer code,", "tokens": [51620, 300, 2516, 9860, 8177, 82, 11, 597, 366, 3720, 294, 746, 300, 1542, 411, 3820, 3089, 11, 51768], "temperature": 0.0, "avg_logprob": -0.14946791502806517, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0017114372458308935}, {"id": 279, "seek": 170420, "start": 1704.2, "end": 1711.32, "text": " and turns them back into human readable proofs. Here is a prototype software. So there's a", "tokens": [50364, 293, 4523, 552, 646, 666, 1952, 49857, 8177, 82, 13, 1692, 307, 257, 19475, 4722, 13, 407, 456, 311, 257, 50720], "temperature": 0.0, "avg_logprob": -0.25168800354003906, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0003198253980372101}, {"id": 280, "seek": 170420, "start": 1711.32, "end": 1718.04, "text": " there's a theorem in topology. Okay, you know, I think it's a if a function is continuous on a dense", "tokens": [50720, 456, 311, 257, 20904, 294, 1192, 1793, 13, 1033, 11, 291, 458, 11, 286, 519, 309, 311, 257, 498, 257, 2445, 307, 10957, 322, 257, 18011, 51056], "temperature": 0.0, "avg_logprob": -0.25168800354003906, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0003198253980372101}, {"id": 281, "seek": 170420, "start": 1718.04, "end": 1722.8400000000001, "text": " set, and there's some extra extra properties, then it's continuous extends to a continuous", "tokens": [51056, 992, 11, 293, 456, 311, 512, 2857, 2857, 7221, 11, 550, 309, 311, 10957, 26448, 281, 257, 10957, 51296], "temperature": 0.0, "avg_logprob": -0.25168800354003906, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0003198253980372101}, {"id": 282, "seek": 170420, "start": 1722.8400000000001, "end": 1728.6000000000001, "text": " function globally. And the proof is written was written first in lean, but then it was", "tokens": [51296, 2445, 18958, 13, 400, 264, 8177, 307, 3720, 390, 3720, 700, 294, 11659, 11, 457, 550, 309, 390, 51584], "temperature": 0.0, "avg_logprob": -0.25168800354003906, "compression_ratio": 1.6926605504587156, "no_speech_prob": 0.0003198253980372101}, {"id": 283, "seek": 172860, "start": 1728.6799999999998, "end": 1733.24, "text": " already converted into a human proof, human readable proof. But again, where every step", "tokens": [50368, 1217, 16424, 666, 257, 1952, 8177, 11, 1952, 49857, 8177, 13, 583, 797, 11, 689, 633, 1823, 50596], "temperature": 0.0, "avg_logprob": -0.15025386525623835, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.011674308218061924}, {"id": 284, "seek": 172860, "start": 1733.24, "end": 1737.08, "text": " you can expand and contract, like if there's a step you want to explain more, you can double", "tokens": [50596, 291, 393, 5268, 293, 4364, 11, 411, 498, 456, 311, 257, 1823, 291, 528, 281, 2903, 544, 11, 291, 393, 3834, 50788], "temperature": 0.0, "avg_logprob": -0.15025386525623835, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.011674308218061924}, {"id": 285, "seek": 172860, "start": 1737.08, "end": 1741.24, "text": " click it, and it will expand out, give all the justification. And you can click at any given", "tokens": [50788, 2052, 309, 11, 293, 309, 486, 5268, 484, 11, 976, 439, 264, 31591, 13, 400, 291, 393, 2052, 412, 604, 2212, 50996], "temperature": 0.0, "avg_logprob": -0.15025386525623835, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.011674308218061924}, {"id": 286, "seek": 172860, "start": 1741.24, "end": 1745.8799999999999, "text": " point in the proof, and it will tell you what the hypotheses are currently, what you can approve,", "tokens": [50996, 935, 294, 264, 8177, 11, 293, 309, 486, 980, 291, 437, 264, 49969, 366, 4362, 11, 437, 291, 393, 18827, 11, 51228], "temperature": 0.0, "avg_logprob": -0.15025386525623835, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.011674308218061924}, {"id": 287, "seek": 172860, "start": 1745.8799999999999, "end": 1752.52, "text": " and you can give a lot of structure. I think eventually textbooks, this is maybe a good format,", "tokens": [51228, 293, 291, 393, 976, 257, 688, 295, 3877, 13, 286, 519, 4728, 33587, 11, 341, 307, 1310, 257, 665, 7877, 11, 51560], "temperature": 0.0, "avg_logprob": -0.15025386525623835, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.011674308218061924}, {"id": 288, "seek": 172860, "start": 1752.52, "end": 1756.76, "text": " say for undergraduate textbooks to have, you know, proofs of say, you know, tricky", "tokens": [51560, 584, 337, 19113, 33587, 281, 362, 11, 291, 458, 11, 8177, 82, 295, 584, 11, 291, 458, 11, 12414, 51772], "temperature": 0.0, "avg_logprob": -0.15025386525623835, "compression_ratio": 1.8272425249169435, "no_speech_prob": 0.011674308218061924}, {"id": 289, "seek": 175676, "start": 1756.76, "end": 1761.96, "text": " films and analysis or something written in a way where, you know, in a not in a static way,", "tokens": [50364, 7796, 293, 5215, 420, 746, 3720, 294, 257, 636, 689, 11, 291, 458, 11, 294, 257, 406, 294, 257, 13437, 636, 11, 50624], "temperature": 0.0, "avg_logprob": -0.13202506617495888, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.00046597738401032984}, {"id": 290, "seek": 175676, "start": 1761.96, "end": 1767.32, "text": " but where you can really, you know, drill down all the way down to the basic axioms if you wanted to.", "tokens": [50624, 457, 689, 291, 393, 534, 11, 291, 458, 11, 11392, 760, 439, 264, 636, 760, 281, 264, 3875, 6360, 72, 4785, 498, 291, 1415, 281, 13, 50892], "temperature": 0.0, "avg_logprob": -0.13202506617495888, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.00046597738401032984}, {"id": 291, "seek": 175676, "start": 1770.6, "end": 1779.4, "text": " Okay. One thing I mean, one thing notable about these formalization projects is that they allow", "tokens": [51056, 1033, 13, 1485, 551, 286, 914, 11, 472, 551, 22556, 466, 613, 9860, 2144, 4455, 307, 300, 436, 2089, 51496], "temperature": 0.0, "avg_logprob": -0.13202506617495888, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.00046597738401032984}, {"id": 292, "seek": 175676, "start": 1779.4, "end": 1783.96, "text": " massive collaborations. So, you know, in other sciences, people collaborate with 20 people,", "tokens": [51496, 5994, 36908, 13, 407, 11, 291, 458, 11, 294, 661, 17677, 11, 561, 18338, 365, 945, 561, 11, 51724], "temperature": 0.0, "avg_logprob": -0.13202506617495888, "compression_ratio": 1.6933333333333334, "no_speech_prob": 0.00046597738401032984}, {"id": 293, "seek": 178396, "start": 1783.96, "end": 1790.6000000000001, "text": " 500 people, you know, 5,000 people. But in mathematics, still, we don't really collaborate", "tokens": [50364, 5923, 561, 11, 291, 458, 11, 1025, 11, 1360, 561, 13, 583, 294, 18666, 11, 920, 11, 321, 500, 380, 534, 18338, 50696], "temperature": 0.0, "avg_logprob": -0.10597487627449682, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.000683352816849947}, {"id": 294, "seek": 178396, "start": 1790.6000000000001, "end": 1797.16, "text": " in really large groups. You know, five is kind of the maximum, usually. And part of it is that,", "tokens": [50696, 294, 534, 2416, 3935, 13, 509, 458, 11, 1732, 307, 733, 295, 264, 6674, 11, 2673, 13, 400, 644, 295, 309, 307, 300, 11, 51024], "temperature": 0.0, "avg_logprob": -0.10597487627449682, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.000683352816849947}, {"id": 295, "seek": 178396, "start": 1797.16, "end": 1801.16, "text": " you know, if you want to collaborate with 20 people, if you don't already know these 20 people,", "tokens": [51024, 291, 458, 11, 498, 291, 528, 281, 18338, 365, 945, 561, 11, 498, 291, 500, 380, 1217, 458, 613, 945, 561, 11, 51224], "temperature": 0.0, "avg_logprob": -0.10597487627449682, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.000683352816849947}, {"id": 296, "seek": 178396, "start": 1801.16, "end": 1807.96, "text": " and you don't completely trust that they're doing correct mathematics, you know, it's very difficult", "tokens": [51224, 293, 291, 500, 380, 2584, 3361, 300, 436, 434, 884, 3006, 18666, 11, 291, 458, 11, 309, 311, 588, 2252, 51564], "temperature": 0.0, "avg_logprob": -0.10597487627449682, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.000683352816849947}, {"id": 297, "seek": 178396, "start": 1807.96, "end": 1813.16, "text": " because you have to, everyone has to check everyone else's contribution. But with a proof", "tokens": [51564, 570, 291, 362, 281, 11, 1518, 575, 281, 1520, 1518, 1646, 311, 13150, 13, 583, 365, 257, 8177, 51824], "temperature": 0.0, "avg_logprob": -0.10597487627449682, "compression_ratio": 1.8053435114503817, "no_speech_prob": 0.000683352816849947}, {"id": 298, "seek": 181316, "start": 1813.16, "end": 1818.1200000000001, "text": " assistant, the proof assistant provides a formal guarantee. And so, you can really collaborate", "tokens": [50364, 10994, 11, 264, 8177, 10994, 6417, 257, 9860, 10815, 13, 400, 370, 11, 291, 393, 534, 18338, 50612], "temperature": 0.0, "avg_logprob": -0.17090116993764812, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0008114323136396706}, {"id": 299, "seek": 181316, "start": 1818.1200000000001, "end": 1823.64, "text": " with 20, hundreds of people that you've never met before. And you don't need to trust them.", "tokens": [50612, 365, 945, 11, 6779, 295, 561, 300, 291, 600, 1128, 1131, 949, 13, 400, 291, 500, 380, 643, 281, 3361, 552, 13, 50888], "temperature": 0.0, "avg_logprob": -0.17090116993764812, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0008114323136396706}, {"id": 300, "seek": 181316, "start": 1824.44, "end": 1831.72, "text": " Because they upload code and the lean compiler verifies, yes, this actually solves what they", "tokens": [50928, 1436, 436, 6580, 3089, 293, 264, 11659, 31958, 1306, 11221, 11, 2086, 11, 341, 767, 39890, 437, 436, 51292], "temperature": 0.0, "avg_logprob": -0.17090116993764812, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0008114323136396706}, {"id": 301, "seek": 181316, "start": 1831.72, "end": 1838.28, "text": " claimed. And then you can accept it or it doesn't. So, I experienced this myself because I got", "tokens": [51292, 12941, 13, 400, 550, 291, 393, 3241, 309, 420, 309, 1177, 380, 13, 407, 11, 286, 6751, 341, 2059, 570, 286, 658, 51620], "temperature": 0.0, "avg_logprob": -0.17090116993764812, "compression_ratio": 1.5648535564853556, "no_speech_prob": 0.0008114323136396706}, {"id": 302, "seek": 183828, "start": 1838.36, "end": 1843.24, "text": " interested in formalization a few months ago. So, I'd recently proven a combinatorial theorem", "tokens": [50368, 3102, 294, 9860, 2144, 257, 1326, 2493, 2057, 13, 407, 11, 286, 1116, 3938, 12785, 257, 2512, 31927, 831, 20904, 50612], "temperature": 0.0, "avg_logprob": -0.20823157779754153, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.0028254424687474966}, {"id": 303, "seek": 183828, "start": 1843.96, "end": 1848.68, "text": " with Tim Gowes, Ben Green, and Freddie Manners. It solved something called the polynomial", "tokens": [50648, 365, 7172, 460, 305, 279, 11, 3964, 6969, 11, 293, 41264, 16892, 433, 13, 467, 13041, 746, 1219, 264, 26110, 50884], "temperature": 0.0, "avg_logprob": -0.20823157779754153, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.0028254424687474966}, {"id": 304, "seek": 183828, "start": 1848.68, "end": 1854.28, "text": " primonrugia conjecture. The precise conjecture is not important for this talk. It's a conjecture", "tokens": [50884, 2886, 266, 81, 697, 654, 416, 1020, 540, 13, 440, 13600, 416, 1020, 540, 307, 406, 1021, 337, 341, 751, 13, 467, 311, 257, 416, 1020, 540, 51164], "temperature": 0.0, "avg_logprob": -0.20823157779754153, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.0028254424687474966}, {"id": 305, "seek": 183828, "start": 1854.28, "end": 1859.72, "text": " in combinatorics. You have a subset of a finite through vector space of small doubling. And you", "tokens": [51164, 294, 2512, 31927, 1167, 13, 509, 362, 257, 25993, 295, 257, 19362, 807, 8062, 1901, 295, 1359, 33651, 13, 400, 291, 51436], "temperature": 0.0, "avg_logprob": -0.20823157779754153, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.0028254424687474966}, {"id": 306, "seek": 183828, "start": 1859.72, "end": 1864.68, "text": " want to show that it is all, it is very close to actually being a coset of a subgroup. This was a", "tokens": [51436, 528, 281, 855, 300, 309, 307, 439, 11, 309, 307, 588, 1998, 281, 767, 885, 257, 3792, 302, 295, 257, 1422, 17377, 13, 639, 390, 257, 51684], "temperature": 0.0, "avg_logprob": -0.20823157779754153, "compression_ratio": 1.6122448979591837, "no_speech_prob": 0.0028254424687474966}, {"id": 307, "seek": 186468, "start": 1864.68, "end": 1873.0, "text": " statement that was in combinatorics that was highly sought after. So, we had a 33-page paper", "tokens": [50364, 5629, 300, 390, 294, 2512, 31927, 1167, 300, 390, 5405, 17532, 934, 13, 407, 11, 321, 632, 257, 11816, 12, 15161, 3035, 50780], "temperature": 0.0, "avg_logprob": -0.18071327209472657, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0014643040485680103}, {"id": 308, "seek": 186468, "start": 1873.88, "end": 1879.3200000000002, "text": " recently proving this. Mostly self-contained, fortunately. So, I thought I was a good candidate", "tokens": [50824, 3938, 27221, 341, 13, 29035, 2698, 12, 9000, 3563, 11, 25511, 13, 407, 11, 286, 1194, 286, 390, 257, 665, 11532, 51096], "temperature": 0.0, "avg_logprob": -0.18071327209472657, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0014643040485680103}, {"id": 309, "seek": 186468, "start": 1879.3200000000002, "end": 1886.28, "text": " for formalization. So, I asked on an internet forum, specializing in lean, for help to formalize it.", "tokens": [51096, 337, 9860, 2144, 13, 407, 11, 286, 2351, 322, 364, 4705, 17542, 11, 2121, 3319, 294, 11659, 11, 337, 854, 281, 9860, 1125, 309, 13, 51444], "temperature": 0.0, "avg_logprob": -0.18071327209472657, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0014643040485680103}, {"id": 310, "seek": 186468, "start": 1886.28, "end": 1892.52, "text": " And then, again, like 20, 30 people joined in and actually only took three weeks to formalize.", "tokens": [51444, 400, 550, 11, 797, 11, 411, 945, 11, 2217, 561, 6869, 294, 293, 767, 787, 1890, 1045, 3259, 281, 9860, 1125, 13, 51756], "temperature": 0.0, "avg_logprob": -0.18071327209472657, "compression_ratio": 1.5673469387755101, "no_speech_prob": 0.0014643040485680103}, {"id": 311, "seek": 189252, "start": 1892.92, "end": 1896.28, "text": " The time taken to formalize these projects is going down quite a bit.", "tokens": [50384, 440, 565, 2726, 281, 9860, 1125, 613, 4455, 307, 516, 760, 1596, 257, 857, 13, 50552], "temperature": 0.0, "avg_logprob": -0.11748614266654041, "compression_ratio": 1.7644628099173554, "no_speech_prob": 0.0004084438842255622}, {"id": 312, "seek": 189252, "start": 1898.6, "end": 1903.56, "text": " So, in particular, the time taken to formalize this project was roughly about the same time", "tokens": [50668, 407, 11, 294, 1729, 11, 264, 565, 2726, 281, 9860, 1125, 341, 1716, 390, 9810, 466, 264, 912, 565, 50916], "temperature": 0.0, "avg_logprob": -0.11748614266654041, "compression_ratio": 1.7644628099173554, "no_speech_prob": 0.0004084438842255622}, {"id": 313, "seek": 189252, "start": 1903.56, "end": 1908.12, "text": " it took for us to write the paper in the first place. And by the time we submitted the paper,", "tokens": [50916, 309, 1890, 337, 505, 281, 2464, 264, 3035, 294, 264, 700, 1081, 13, 400, 538, 264, 565, 321, 14405, 264, 3035, 11, 51144], "temperature": 0.0, "avg_logprob": -0.11748614266654041, "compression_ratio": 1.7644628099173554, "no_speech_prob": 0.0004084438842255622}, {"id": 314, "seek": 189252, "start": 1908.12, "end": 1911.8799999999999, "text": " we were able to put as a note that the proof is actually being formalized.", "tokens": [51144, 321, 645, 1075, 281, 829, 382, 257, 3637, 300, 264, 8177, 307, 767, 885, 9860, 1602, 13, 51332], "temperature": 0.0, "avg_logprob": -0.11748614266654041, "compression_ratio": 1.7644628099173554, "no_speech_prob": 0.0004084438842255622}, {"id": 315, "seek": 189252, "start": 1913.8799999999999, "end": 1918.52, "text": " So, as I said, it uses a single blueprint which splits up the proof into lots and lots of little", "tokens": [51432, 407, 11, 382, 286, 848, 11, 309, 4960, 257, 2167, 35868, 597, 37741, 493, 264, 8177, 666, 3195, 293, 3195, 295, 707, 51664], "temperature": 0.0, "avg_logprob": -0.11748614266654041, "compression_ratio": 1.7644628099173554, "no_speech_prob": 0.0004084438842255622}, {"id": 316, "seek": 191852, "start": 1918.52, "end": 1927.0, "text": " pieces. And so, it creates this nice little dependency graph. So, this is a picture of", "tokens": [50364, 3755, 13, 400, 370, 11, 309, 7829, 341, 1481, 707, 33621, 4295, 13, 407, 11, 341, 307, 257, 3036, 295, 50788], "temperature": 0.0, "avg_logprob": -0.12067266168265507, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.0034077907912433147}, {"id": 317, "seek": 191852, "start": 1927.0, "end": 1931.32, "text": " the graph at an early stage of the project. So, there's no down the bottom called PFR. Maybe", "tokens": [50788, 264, 4295, 412, 364, 2440, 3233, 295, 264, 1716, 13, 407, 11, 456, 311, 572, 760, 264, 2767, 1219, 430, 34658, 13, 2704, 51004], "temperature": 0.0, "avg_logprob": -0.12067266168265507, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.0034077907912433147}, {"id": 318, "seek": 191852, "start": 1931.32, "end": 1934.28, "text": " only the people in the front can see it. That's the final theorem that we're trying to prove.", "tokens": [51004, 787, 264, 561, 294, 264, 1868, 393, 536, 309, 13, 663, 311, 264, 2572, 20904, 300, 321, 434, 1382, 281, 7081, 13, 51152], "temperature": 0.0, "avg_logprob": -0.12067266168265507, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.0034077907912433147}, {"id": 319, "seek": 191852, "start": 1935.16, "end": 1939.0, "text": " At the time of the screenshot, this was white, which means that it had not been", "tokens": [51196, 1711, 264, 565, 295, 264, 27712, 11, 341, 390, 2418, 11, 597, 1355, 300, 309, 632, 406, 668, 51388], "temperature": 0.0, "avg_logprob": -0.12067266168265507, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.0034077907912433147}, {"id": 320, "seek": 191852, "start": 1940.04, "end": 1944.28, "text": " proved, but not even been stated properly. So, in fact, even before you prove the theorem,", "tokens": [51440, 14617, 11, 457, 406, 754, 668, 11323, 6108, 13, 407, 11, 294, 1186, 11, 754, 949, 291, 7081, 264, 20904, 11, 51652], "temperature": 0.0, "avg_logprob": -0.12067266168265507, "compression_ratio": 1.6946564885496183, "no_speech_prob": 0.0034077907912433147}, {"id": 321, "seek": 194428, "start": 1944.28, "end": 1948.2, "text": " you have to first state it, and then you have to make definitions and so forth.", "tokens": [50364, 291, 362, 281, 700, 1785, 309, 11, 293, 550, 291, 362, 281, 652, 21988, 293, 370, 5220, 13, 50560], "temperature": 0.0, "avg_logprob": -0.1301616616205338, "compression_ratio": 1.8508064516129032, "no_speech_prob": 0.0066917771473526955}, {"id": 322, "seek": 194428, "start": 1949.24, "end": 1953.32, "text": " Blue are things that have been defined, but not yet proven, and green have been things that have", "tokens": [50612, 8510, 366, 721, 300, 362, 668, 7642, 11, 457, 406, 1939, 12785, 11, 293, 3092, 362, 668, 721, 300, 362, 50816], "temperature": 0.0, "avg_logprob": -0.1301616616205338, "compression_ratio": 1.8508064516129032, "no_speech_prob": 0.0066917771473526955}, {"id": 323, "seek": 194428, "start": 1953.32, "end": 1961.3999999999999, "text": " been proven. And so, at any point in time, some bubbles are white, some are blue, and some", "tokens": [50816, 668, 12785, 13, 400, 370, 11, 412, 604, 935, 294, 565, 11, 512, 16295, 366, 2418, 11, 512, 366, 3344, 11, 293, 512, 51220], "temperature": 0.0, "avg_logprob": -0.1301616616205338, "compression_ratio": 1.8508064516129032, "no_speech_prob": 0.0066917771473526955}, {"id": 324, "seek": 194428, "start": 1961.3999999999999, "end": 1967.56, "text": " results depend on some others. And so, the way the formalization proceeded was that different", "tokens": [51220, 3542, 5672, 322, 512, 2357, 13, 400, 370, 11, 264, 636, 264, 9860, 2144, 39053, 390, 300, 819, 51528], "temperature": 0.0, "avg_logprob": -0.1301616616205338, "compression_ratio": 1.8508064516129032, "no_speech_prob": 0.0066917771473526955}, {"id": 325, "seek": 194428, "start": 1967.56, "end": 1972.76, "text": " people just grabbed a note that was ready to be formalized because maybe all the previous results", "tokens": [51528, 561, 445, 18607, 257, 3637, 300, 390, 1919, 281, 312, 9860, 1602, 570, 1310, 439, 264, 3894, 3542, 51788], "temperature": 0.0, "avg_logprob": -0.1301616616205338, "compression_ratio": 1.8508064516129032, "no_speech_prob": 0.0066917771473526955}, {"id": 326, "seek": 197276, "start": 1973.24, "end": 1977.72, "text": " that it depended on were formalized, and they just formalized that one step independent of", "tokens": [50388, 300, 309, 1367, 3502, 322, 645, 9860, 1602, 11, 293, 436, 445, 9860, 1602, 300, 472, 1823, 6695, 295, 50612], "temperature": 0.0, "avg_logprob": -0.11951101495978537, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.002063677879050374}, {"id": 327, "seek": 197276, "start": 1977.72, "end": 1983.8799999999999, "text": " everybody else. And you didn't really need to coordinate too much. I mean, we did coordinate", "tokens": [50612, 2201, 1646, 13, 400, 291, 994, 380, 534, 643, 281, 15670, 886, 709, 13, 286, 914, 11, 321, 630, 15670, 50920], "temperature": 0.0, "avg_logprob": -0.11951101495978537, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.002063677879050374}, {"id": 328, "seek": 197276, "start": 1985.0, "end": 1990.76, "text": " on an internet forum, but each little note is completely specified. There's a very precise", "tokens": [50976, 322, 364, 4705, 17542, 11, 457, 1184, 707, 3637, 307, 2584, 22206, 13, 821, 311, 257, 588, 13600, 51264], "temperature": 0.0, "avg_logprob": -0.11951101495978537, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.002063677879050374}, {"id": 329, "seek": 197276, "start": 1990.76, "end": 1998.28, "text": " definition and a very precise thing to prove, and we just need the proof. And we really don't care", "tokens": [51264, 7123, 293, 257, 588, 13600, 551, 281, 7081, 11, 293, 321, 445, 643, 264, 8177, 13, 400, 321, 534, 500, 380, 1127, 51640], "temperature": 0.0, "avg_logprob": -0.11951101495978537, "compression_ratio": 1.6801801801801801, "no_speech_prob": 0.002063677879050374}, {"id": 330, "seek": 199828, "start": 1998.36, "end": 2004.12, "text": " exactly what the proof is. I mean, it has to be not massively long. So, people just picked up", "tokens": [50368, 2293, 437, 264, 8177, 307, 13, 286, 914, 11, 309, 575, 281, 312, 406, 29379, 938, 13, 407, 11, 561, 445, 6183, 493, 50656], "temperature": 0.0, "avg_logprob": -0.18696281031558387, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.002872902899980545}, {"id": 331, "seek": 199828, "start": 2004.12, "end": 2008.44, "text": " individual claims. Here's one very simple claim. There's a certain functional called", "tokens": [50656, 2609, 9441, 13, 1692, 311, 472, 588, 2199, 3932, 13, 821, 311, 257, 1629, 11745, 1219, 50872], "temperature": 0.0, "avg_logprob": -0.18696281031558387, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.002872902899980545}, {"id": 332, "seek": 199828, "start": 2008.44, "end": 2014.28, "text": " ruja distance d, and there's a very simple claim that it was non-negative. And this turns out to", "tokens": [50872, 5420, 2938, 4560, 274, 11, 293, 456, 311, 257, 588, 2199, 3932, 300, 309, 390, 2107, 12, 28561, 1166, 13, 400, 341, 4523, 484, 281, 51164], "temperature": 0.0, "avg_logprob": -0.18696281031558387, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.002872902899980545}, {"id": 333, "seek": 199828, "start": 2014.28, "end": 2023.48, "text": " have a three-line proof, assuming some previous facts that were also on the blueprint. And so,", "tokens": [51164, 362, 257, 1045, 12, 1889, 8177, 11, 11926, 512, 3894, 9130, 300, 645, 611, 322, 264, 35868, 13, 400, 370, 11, 51624], "temperature": 0.0, "avg_logprob": -0.18696281031558387, "compression_ratio": 1.5811965811965811, "no_speech_prob": 0.002872902899980545}, {"id": 334, "seek": 202348, "start": 2023.48, "end": 2028.76, "text": " people just sort of picked up these things separately, and over time, it just filled up.", "tokens": [50364, 561, 445, 1333, 295, 6183, 493, 613, 721, 14759, 11, 293, 670, 565, 11, 309, 445, 6412, 493, 13, 50628], "temperature": 0.0, "avg_logprob": -0.17535677677443048, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000731899228412658}, {"id": 335, "seek": 202348, "start": 2028.76, "end": 2032.84, "text": " This is what a typical step in the proof looks like. This is the proof that this ruja distance", "tokens": [50628, 639, 307, 437, 257, 7476, 1823, 294, 264, 8177, 1542, 411, 13, 639, 307, 264, 8177, 300, 341, 5420, 2938, 4560, 50832], "temperature": 0.0, "avg_logprob": -0.17535677677443048, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000731899228412658}, {"id": 336, "seek": 202348, "start": 2032.84, "end": 2038.52, "text": " is non-negative. This is the code in the lean. It looks kind of a little bit like mathematics,", "tokens": [50832, 307, 2107, 12, 28561, 1166, 13, 639, 307, 264, 3089, 294, 264, 11659, 13, 467, 1542, 733, 295, 257, 707, 857, 411, 18666, 11, 51116], "temperature": 0.0, "avg_logprob": -0.17535677677443048, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000731899228412658}, {"id": 337, "seek": 202348, "start": 2039.16, "end": 2045.32, "text": " but it is actually... Once you think of the syntax, it actually reads... It's like reading", "tokens": [51148, 457, 309, 307, 767, 485, 3443, 291, 519, 295, 264, 28431, 11, 309, 767, 15700, 485, 467, 311, 411, 3760, 51456], "temperature": 0.0, "avg_logprob": -0.17535677677443048, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000731899228412658}, {"id": 338, "seek": 202348, "start": 2045.32, "end": 2048.28, "text": " latex. The first time you read latex, it looks like a whole bunch of mess of symbols. But", "tokens": [51456, 3469, 87, 13, 440, 700, 565, 291, 1401, 3469, 87, 11, 309, 1542, 411, 257, 1379, 3840, 295, 2082, 295, 16944, 13, 583, 51604], "temperature": 0.0, "avg_logprob": -0.17535677677443048, "compression_ratio": 1.7320754716981133, "no_speech_prob": 0.000731899228412658}, {"id": 339, "seek": 204828, "start": 2049.1600000000003, "end": 2056.0400000000004, "text": " it's actually... Every line corresponds to a sentence in mathematical English. For example,", "tokens": [50408, 309, 311, 767, 485, 2048, 1622, 23249, 281, 257, 8174, 294, 18894, 3669, 13, 1171, 1365, 11, 50752], "temperature": 0.0, "avg_logprob": -0.15762464055475198, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0012746170395985246}, {"id": 340, "seek": 204828, "start": 2056.0400000000004, "end": 2058.44, "text": " the first line, if you want to prove that this distance is positive,", "tokens": [50752, 264, 700, 1622, 11, 498, 291, 528, 281, 7081, 300, 341, 4560, 307, 3353, 11, 50872], "temperature": 0.0, "avg_logprob": -0.15762464055475198, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0012746170395985246}, {"id": 341, "seek": 204828, "start": 2058.44, "end": 2062.1200000000003, "text": " it suffices to prove that tricidistance is positive. So, you work with tricidistance,", "tokens": [50872, 309, 3889, 1473, 281, 7081, 300, 504, 299, 327, 20829, 307, 3353, 13, 407, 11, 291, 589, 365, 504, 299, 327, 20829, 11, 51056], "temperature": 0.0, "avg_logprob": -0.15762464055475198, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0012746170395985246}, {"id": 342, "seek": 204828, "start": 2062.1200000000003, "end": 2067.4, "text": " because it turns out there's another lemma that bounds tricidistance. So, yeah, every step,", "tokens": [51056, 570, 309, 4523, 484, 456, 311, 1071, 7495, 1696, 300, 29905, 504, 299, 327, 20829, 13, 407, 11, 1338, 11, 633, 1823, 11, 51320], "temperature": 0.0, "avg_logprob": -0.15762464055475198, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0012746170395985246}, {"id": 343, "seek": 204828, "start": 2067.4, "end": 2074.6000000000004, "text": " actually, it corresponds reasonably well to the way we think about mathematical proofs.", "tokens": [51320, 767, 11, 309, 23249, 23551, 731, 281, 264, 636, 321, 519, 466, 18894, 8177, 82, 13, 51680], "temperature": 0.0, "avg_logprob": -0.15762464055475198, "compression_ratio": 1.8205128205128205, "no_speech_prob": 0.0012746170395985246}, {"id": 344, "seek": 207460, "start": 2075.56, "end": 2083.64, "text": " Yeah. So, fortunately, the proof didn't reveal any major issues. There were some typos,", "tokens": [50412, 865, 13, 407, 11, 25511, 11, 264, 8177, 994, 380, 10658, 604, 2563, 2663, 13, 821, 645, 512, 2125, 329, 11, 50816], "temperature": 0.0, "avg_logprob": -0.16906329563685826, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0001048899139277637}, {"id": 345, "seek": 207460, "start": 2083.64, "end": 2089.88, "text": " but very, very minor picked up. And we also... There were some things we needed... Again,", "tokens": [50816, 457, 588, 11, 588, 6696, 6183, 493, 13, 400, 321, 611, 485, 821, 645, 512, 721, 321, 2978, 485, 3764, 11, 51128], "temperature": 0.0, "avg_logprob": -0.16906329563685826, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0001048899139277637}, {"id": 346, "seek": 207460, "start": 2089.88, "end": 2094.68, "text": " we needed to add to the math library. The math library... We used Shannon entropy,", "tokens": [51128, 321, 2978, 281, 909, 281, 264, 5221, 6405, 13, 440, 5221, 6405, 485, 492, 1143, 28974, 30867, 11, 51368], "temperature": 0.0, "avg_logprob": -0.16906329563685826, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0001048899139277637}, {"id": 347, "seek": 207460, "start": 2094.68, "end": 2100.52, "text": " and Shannon entropy was not in the math library. Now it is. One thing about formalization is that", "tokens": [51368, 293, 28974, 30867, 390, 406, 294, 264, 5221, 6405, 13, 823, 309, 307, 13, 1485, 551, 466, 9860, 2144, 307, 300, 51660], "temperature": 0.0, "avg_logprob": -0.16906329563685826, "compression_ratio": 1.6807511737089202, "no_speech_prob": 0.0001048899139277637}, {"id": 348, "seek": 210052, "start": 2100.52, "end": 2107.4, "text": " it still takes longer to formalize proofs than to write them. But... And maybe about 10 times", "tokens": [50364, 309, 920, 2516, 2854, 281, 9860, 1125, 8177, 82, 813, 281, 2464, 552, 13, 583, 485, 400, 1310, 466, 1266, 1413, 50708], "temperature": 0.0, "avg_logprob": -0.14317400768549757, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.002557495143264532}, {"id": 349, "seek": 210052, "start": 2107.4, "end": 2115.32, "text": " longer, I would say. Which is unfortunate. If it was faster to formalize to write proofs formally", "tokens": [50708, 2854, 11, 286, 576, 584, 13, 3013, 307, 17843, 13, 759, 309, 390, 4663, 281, 9860, 1125, 281, 2464, 8177, 82, 25983, 51104], "temperature": 0.0, "avg_logprob": -0.14317400768549757, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.002557495143264532}, {"id": 350, "seek": 210052, "start": 2115.32, "end": 2120.92, "text": " than to write them by hand, I think then there will be a tipping point. And then I think a lot", "tokens": [51104, 813, 281, 2464, 552, 538, 1011, 11, 286, 519, 550, 456, 486, 312, 257, 41625, 935, 13, 400, 550, 286, 519, 257, 688, 51384], "temperature": 0.0, "avg_logprob": -0.14317400768549757, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.002557495143264532}, {"id": 351, "seek": 210052, "start": 2120.92, "end": 2127.16, "text": " of us would switch over just because they guarantee a correctness. So, we're not there yet. It is", "tokens": [51384, 295, 505, 576, 3679, 670, 445, 570, 436, 10815, 257, 3006, 1287, 13, 407, 11, 321, 434, 406, 456, 1939, 13, 467, 307, 51696], "temperature": 0.0, "avg_logprob": -0.14317400768549757, "compression_ratio": 1.6695652173913043, "no_speech_prob": 0.002557495143264532}, {"id": 352, "seek": 212716, "start": 2127.16, "end": 2131.64, "text": " definitely getting a lot faster than it was before. But one thing that we noticed is that,", "tokens": [50364, 2138, 1242, 257, 688, 4663, 813, 309, 390, 949, 13, 583, 472, 551, 300, 321, 5694, 307, 300, 11, 50588], "temperature": 0.0, "avg_logprob": -0.12062247453537663, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0034729510080069304}, {"id": 353, "seek": 212716, "start": 2131.64, "end": 2136.8399999999997, "text": " actually, while it still takes a long time to write a proof, if you want to modify a proof", "tokens": [50588, 767, 11, 1339, 309, 920, 2516, 257, 938, 565, 281, 2464, 257, 8177, 11, 498, 291, 528, 281, 16927, 257, 8177, 50848], "temperature": 0.0, "avg_logprob": -0.12062247453537663, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0034729510080069304}, {"id": 354, "seek": 212716, "start": 2136.8399999999997, "end": 2143.16, "text": " to change some parameters and make it a little bit better, that can be done much quicker in the", "tokens": [50848, 281, 1319, 512, 9834, 293, 652, 309, 257, 707, 857, 1101, 11, 300, 393, 312, 1096, 709, 16255, 294, 264, 51164], "temperature": 0.0, "avg_logprob": -0.12062247453537663, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0034729510080069304}, {"id": 355, "seek": 212716, "start": 2143.16, "end": 2148.8399999999997, "text": " formal setting than with a paper proof. Because with paper proof, if you change all your little", "tokens": [51164, 9860, 3287, 813, 365, 257, 3035, 8177, 13, 1436, 365, 3035, 8177, 11, 498, 291, 1319, 439, 428, 707, 51448], "temperature": 0.0, "avg_logprob": -0.12062247453537663, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0034729510080069304}, {"id": 356, "seek": 212716, "start": 2148.8399999999997, "end": 2153.16, "text": " parameters and so forth, you'll likely make also mistakes and you go back. And it's a very annoying", "tokens": [51448, 9834, 293, 370, 5220, 11, 291, 603, 3700, 652, 611, 8038, 293, 291, 352, 646, 13, 400, 309, 311, 257, 588, 11304, 51664], "temperature": 0.0, "avg_logprob": -0.12062247453537663, "compression_ratio": 1.7649253731343284, "no_speech_prob": 0.0034729510080069304}, {"id": 357, "seek": 215316, "start": 2153.16, "end": 2158.92, "text": " process. But it's actually much easier to modify an existing formal proof than to create one from", "tokens": [50364, 1399, 13, 583, 309, 311, 767, 709, 3571, 281, 16927, 364, 6741, 9860, 8177, 813, 281, 1884, 472, 490, 50652], "temperature": 0.0, "avg_logprob": -0.15942352119533496, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0009400169947184622}, {"id": 358, "seek": 215316, "start": 2158.92, "end": 2164.12, "text": " scratch. For example, we were able... There's a constant 12 exponent that appeared in our proof.", "tokens": [50652, 8459, 13, 1171, 1365, 11, 321, 645, 1075, 485, 821, 311, 257, 5754, 2272, 37871, 300, 8516, 294, 527, 8177, 13, 50912], "temperature": 0.0, "avg_logprob": -0.15942352119533496, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0009400169947184622}, {"id": 359, "seek": 215316, "start": 2165.56, "end": 2171.8799999999997, "text": " A few days afterwards, someone posted an improvement in the argument, improved 12 to 11,", "tokens": [50984, 316, 1326, 1708, 10543, 11, 1580, 9437, 364, 10444, 294, 264, 6770, 11, 9689, 2272, 281, 2975, 11, 51300], "temperature": 0.0, "avg_logprob": -0.15942352119533496, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0009400169947184622}, {"id": 360, "seek": 215316, "start": 2171.8799999999997, "end": 2175.3199999999997, "text": " and in a few days we were able to adapt the formal proof to do that as well.", "tokens": [51300, 293, 294, 257, 1326, 1708, 321, 645, 1075, 281, 6231, 264, 9860, 8177, 281, 360, 300, 382, 731, 13, 51472], "temperature": 0.0, "avg_logprob": -0.15942352119533496, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0009400169947184622}, {"id": 361, "seek": 217532, "start": 2175.48, "end": 2185.7200000000003, "text": " Yeah. And because you can collaborate, I think the process is scalable. There was just recently Kevin", "tokens": [50372, 865, 13, 400, 570, 291, 393, 18338, 11, 286, 519, 264, 1399, 307, 38481, 13, 821, 390, 445, 3938, 9954, 50884], "temperature": 0.0, "avg_logprob": -0.2010153089250837, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000508295779582113}, {"id": 362, "seek": 217532, "start": 2185.7200000000003, "end": 2190.1200000000003, "text": " Buzzard, for example, a five-year project. The aim is to formalize Fermat's last theory of the proof.", "tokens": [50884, 29209, 515, 11, 337, 1365, 11, 257, 1732, 12, 5294, 1716, 13, 440, 5939, 307, 281, 9860, 1125, 43261, 267, 311, 1036, 5261, 295, 264, 8177, 13, 51104], "temperature": 0.0, "avg_logprob": -0.2010153089250837, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000508295779582113}, {"id": 363, "seek": 217532, "start": 2191.48, "end": 2197.6400000000003, "text": " And that is quite a big goal because there are lots and lots of sub-portions that have had no", "tokens": [51172, 400, 300, 307, 1596, 257, 955, 3387, 570, 456, 366, 3195, 293, 3195, 295, 1422, 12, 79, 477, 626, 300, 362, 632, 572, 51480], "temperature": 0.0, "avg_logprob": -0.2010153089250837, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000508295779582113}, {"id": 364, "seek": 217532, "start": 2197.6400000000003, "end": 2202.6800000000003, "text": " formal proof at all. So, that, I think, will be quite an ambitious project. But I think it's now", "tokens": [51480, 9860, 8177, 412, 439, 13, 407, 11, 300, 11, 286, 519, 11, 486, 312, 1596, 364, 20239, 1716, 13, 583, 286, 519, 309, 311, 586, 51732], "temperature": 0.0, "avg_logprob": -0.2010153089250837, "compression_ratio": 1.6147540983606556, "no_speech_prob": 0.000508295779582113}, {"id": 365, "seek": 220268, "start": 2202.68, "end": 2210.3599999999997, "text": " doable. It wasn't doable five years ago, but now I think it is. Okay. So, that's four more proofs.", "tokens": [50364, 41183, 13, 467, 2067, 380, 41183, 1732, 924, 2057, 11, 457, 586, 286, 519, 309, 307, 13, 1033, 13, 407, 11, 300, 311, 1451, 544, 8177, 82, 13, 50748], "temperature": 0.0, "avg_logprob": -0.1675748722527617, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.0005725300870835781}, {"id": 366, "seek": 220268, "start": 2211.16, "end": 2218.44, "text": " Another... Okay. I might actually have to speed up a little bit. Okay. So,", "tokens": [50788, 3996, 485, 1033, 13, 286, 1062, 767, 362, 281, 3073, 493, 257, 707, 857, 13, 1033, 13, 407, 11, 51152], "temperature": 0.0, "avg_logprob": -0.1675748722527617, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.0005725300870835781}, {"id": 367, "seek": 220268, "start": 2220.2799999999997, "end": 2227.16, "text": " all right. So, machine learning, using neural networks has become also more and more common", "tokens": [51244, 439, 558, 13, 407, 11, 3479, 2539, 11, 1228, 18161, 9590, 575, 1813, 611, 544, 293, 544, 2689, 51588], "temperature": 0.0, "avg_logprob": -0.1675748722527617, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.0005725300870835781}, {"id": 368, "seek": 220268, "start": 2227.16, "end": 2230.7599999999998, "text": " place in different areas of mathematics. I think I'll skip over this one. So,", "tokens": [51588, 1081, 294, 819, 3179, 295, 18666, 13, 286, 519, 286, 603, 10023, 670, 341, 472, 13, 407, 11, 51768], "temperature": 0.0, "avg_logprob": -0.1675748722527617, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.0005725300870835781}, {"id": 369, "seek": 223076, "start": 2231.5600000000004, "end": 2237.32, "text": " one place where it is being used is in PDE to construct candidate approximate solutions for", "tokens": [50404, 472, 1081, 689, 309, 307, 885, 1143, 307, 294, 10464, 36, 281, 7690, 11532, 30874, 6547, 337, 50692], "temperature": 0.0, "avg_logprob": -0.12030255453927176, "compression_ratio": 1.8576158940397351, "no_speech_prob": 0.00012622885697055608}, {"id": 370, "seek": 223076, "start": 2237.32, "end": 2240.84, "text": " various problems. So, like... So, there's a famous problem in fluid equations, you know,", "tokens": [50692, 3683, 2740, 13, 407, 11, 411, 485, 407, 11, 456, 311, 257, 4618, 1154, 294, 9113, 11787, 11, 291, 458, 11, 50868], "temperature": 0.0, "avg_logprob": -0.12030255453927176, "compression_ratio": 1.8576158940397351, "no_speech_prob": 0.00012622885697055608}, {"id": 371, "seek": 223076, "start": 2240.84, "end": 2243.8, "text": " do the Navier-Stokes equations before we find our time? Do the Euler equations before we find our", "tokens": [50868, 360, 264, 9219, 811, 12, 4520, 8606, 11787, 949, 321, 915, 527, 565, 30, 1144, 264, 462, 26318, 11787, 949, 321, 915, 527, 51016], "temperature": 0.0, "avg_logprob": -0.12030255453927176, "compression_ratio": 1.8576158940397351, "no_speech_prob": 0.00012622885697055608}, {"id": 372, "seek": 223076, "start": 2243.8, "end": 2248.76, "text": " time? Navier-Stokes is still challenging, but Euler, there's been a lot of progress recently", "tokens": [51016, 565, 30, 9219, 811, 12, 4520, 8606, 307, 920, 7595, 11, 457, 462, 26318, 11, 456, 311, 668, 257, 688, 295, 4205, 3938, 51264], "temperature": 0.0, "avg_logprob": -0.12030255453927176, "compression_ratio": 1.8576158940397351, "no_speech_prob": 0.00012622885697055608}, {"id": 373, "seek": 223076, "start": 2249.48, "end": 2253.0800000000004, "text": " that people have been starting to construct self-similar solutions to the Euler equations", "tokens": [51300, 300, 561, 362, 668, 2891, 281, 7690, 2698, 12, 30937, 2202, 6547, 281, 264, 462, 26318, 11787, 51480], "temperature": 0.0, "avg_logprob": -0.12030255453927176, "compression_ratio": 1.8576158940397351, "no_speech_prob": 0.00012622885697055608}, {"id": 374, "seek": 223076, "start": 2253.0800000000004, "end": 2258.84, "text": " with some asterisks, but the asterisks are slowly being removed. And one of the strategies of proof", "tokens": [51480, 365, 512, 257, 3120, 271, 1694, 11, 457, 264, 257, 3120, 271, 1694, 366, 5692, 885, 7261, 13, 400, 472, 295, 264, 9029, 295, 8177, 51768], "temperature": 0.0, "avg_logprob": -0.12030255453927176, "compression_ratio": 1.8576158940397351, "no_speech_prob": 0.00012622885697055608}, {"id": 375, "seek": 225884, "start": 2258.84, "end": 2263.7200000000003, "text": " is to first construct an approximate solution, approximately self-similar solution that looks", "tokens": [50364, 307, 281, 700, 7690, 364, 30874, 3827, 11, 10447, 2698, 12, 30937, 2202, 3827, 300, 1542, 50608], "temperature": 0.0, "avg_logprob": -0.11475429080781482, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00022448076924774796}, {"id": 376, "seek": 225884, "start": 2263.7200000000003, "end": 2268.1200000000003, "text": " like it's going to blow up and then use some rigorous perturbation theory to perturb that to an", "tokens": [50608, 411, 309, 311, 516, 281, 6327, 493, 293, 550, 764, 512, 29882, 40468, 399, 5261, 281, 40468, 300, 281, 364, 50828], "temperature": 0.0, "avg_logprob": -0.11475429080781482, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00022448076924774796}, {"id": 377, "seek": 225884, "start": 2268.1200000000003, "end": 2275.32, "text": " actually blowing up solution. And machine learning has turned out to be useful to actually generate", "tokens": [50828, 767, 15068, 493, 3827, 13, 400, 3479, 2539, 575, 3574, 484, 281, 312, 4420, 281, 767, 8460, 51188], "temperature": 0.0, "avg_logprob": -0.11475429080781482, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00022448076924774796}, {"id": 378, "seek": 225884, "start": 2275.32, "end": 2281.6400000000003, "text": " these approximate solutions, but I'm going to skip that for lack of time. I'll tell you,", "tokens": [51188, 613, 30874, 6547, 11, 457, 286, 478, 516, 281, 10023, 300, 337, 5011, 295, 565, 13, 286, 603, 980, 291, 11, 51504], "temperature": 0.0, "avg_logprob": -0.11475429080781482, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00022448076924774796}, {"id": 379, "seek": 228164, "start": 2281.64, "end": 2289.48, "text": " my favorite application of machine learning to mathematics is in knot theory. So, this is a", "tokens": [50364, 452, 2954, 3861, 295, 3479, 2539, 281, 18666, 307, 294, 16966, 5261, 13, 407, 11, 341, 307, 257, 50756], "temperature": 0.0, "avg_logprob": -0.15153136304629747, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0004914762685075402}, {"id": 380, "seek": 228164, "start": 2289.48, "end": 2296.2, "text": " recent work. So, knots are a very old subject in mathematics, and there's lots of... One of the", "tokens": [50756, 5162, 589, 13, 407, 11, 27426, 366, 257, 588, 1331, 3983, 294, 18666, 11, 293, 456, 311, 3195, 295, 485, 1485, 295, 264, 51092], "temperature": 0.0, "avg_logprob": -0.15153136304629747, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0004914762685075402}, {"id": 381, "seek": 228164, "start": 2296.2, "end": 2300.2799999999997, "text": " fundamental things you study in knots is knot invariance. So, there's various statistics you", "tokens": [51092, 8088, 721, 291, 2979, 294, 27426, 307, 16966, 33270, 719, 13, 407, 11, 456, 311, 3683, 12523, 291, 51296], "temperature": 0.0, "avg_logprob": -0.15153136304629747, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0004914762685075402}, {"id": 382, "seek": 228164, "start": 2300.2799999999997, "end": 2306.3599999999997, "text": " can study. You can assign to a knot, which depends on the topology of the knot or the geometry of the", "tokens": [51296, 393, 2979, 13, 509, 393, 6269, 281, 257, 16966, 11, 597, 5946, 322, 264, 1192, 1793, 295, 264, 16966, 420, 264, 18426, 295, 264, 51600], "temperature": 0.0, "avg_logprob": -0.15153136304629747, "compression_ratio": 1.6902654867256637, "no_speech_prob": 0.0004914762685075402}, {"id": 383, "seek": 230636, "start": 2306.36, "end": 2311.6400000000003, "text": " knot. So, there's something called the signature, which is a combinatorial invariant. There are", "tokens": [50364, 16966, 13, 407, 11, 456, 311, 746, 1219, 264, 13397, 11, 597, 307, 257, 2512, 31927, 831, 33270, 394, 13, 821, 366, 50628], "temperature": 0.0, "avg_logprob": -0.15233983713037827, "compression_ratio": 1.93687707641196, "no_speech_prob": 0.0005316087044775486}, {"id": 384, "seek": 230636, "start": 2311.6400000000003, "end": 2315.8, "text": " these famous polynomials, like the Jonas polynomial and Homfly polynomial. Then there are these things", "tokens": [50628, 613, 4618, 22560, 12356, 11, 411, 264, 34630, 26110, 293, 20903, 14061, 26110, 13, 1396, 456, 366, 613, 721, 50836], "temperature": 0.0, "avg_logprob": -0.15233983713037827, "compression_ratio": 1.93687707641196, "no_speech_prob": 0.0005316087044775486}, {"id": 385, "seek": 230636, "start": 2315.8, "end": 2320.52, "text": " called hyperbolic invariance. The complement of a knot is often a hyperbolic space, and then you", "tokens": [50836, 1219, 9848, 65, 7940, 33270, 719, 13, 440, 17103, 295, 257, 16966, 307, 2049, 257, 9848, 65, 7940, 1901, 11, 293, 550, 291, 51072], "temperature": 0.0, "avg_logprob": -0.15233983713037827, "compression_ratio": 1.93687707641196, "no_speech_prob": 0.0005316087044775486}, {"id": 386, "seek": 230636, "start": 2320.52, "end": 2324.92, "text": " can talk about the volume of that space and some other geometric invariance. And so, there are", "tokens": [51072, 393, 751, 466, 264, 5523, 295, 300, 1901, 293, 512, 661, 33246, 33270, 719, 13, 400, 370, 11, 456, 366, 51292], "temperature": 0.0, "avg_logprob": -0.15233983713037827, "compression_ratio": 1.93687707641196, "no_speech_prob": 0.0005316087044775486}, {"id": 387, "seek": 230636, "start": 2324.92, "end": 2330.28, "text": " these massive databases of knots. You can generate millions of knots, and you can compute all these", "tokens": [51292, 613, 5994, 22380, 295, 27426, 13, 509, 393, 8460, 6803, 295, 27426, 11, 293, 291, 393, 14722, 439, 613, 51560], "temperature": 0.0, "avg_logprob": -0.15233983713037827, "compression_ratio": 1.93687707641196, "no_speech_prob": 0.0005316087044775486}, {"id": 388, "seek": 230636, "start": 2330.28, "end": 2335.88, "text": " invariance. But they come from very different areas of mathematics. So, some knot invariance", "tokens": [51560, 33270, 719, 13, 583, 436, 808, 490, 588, 819, 3179, 295, 18666, 13, 407, 11, 512, 16966, 33270, 719, 51840], "temperature": 0.0, "avg_logprob": -0.15233983713037827, "compression_ratio": 1.93687707641196, "no_speech_prob": 0.0005316087044775486}, {"id": 389, "seek": 233588, "start": 2335.88, "end": 2340.76, "text": " come from combinatorics, some come from operative algebras, some come from hyperbolic geometry,", "tokens": [50364, 808, 490, 2512, 31927, 1167, 11, 512, 808, 490, 2208, 1166, 419, 432, 38182, 11, 512, 808, 490, 9848, 65, 7940, 18426, 11, 50608], "temperature": 0.0, "avg_logprob": -0.12303363515975628, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.0001641560229472816}, {"id": 390, "seek": 233588, "start": 2340.76, "end": 2351.08, "text": " and it's not obvious how they're related. But what these mathematicians did was that they trained", "tokens": [50608, 293, 309, 311, 406, 6322, 577, 436, 434, 4077, 13, 583, 437, 613, 32811, 2567, 630, 390, 300, 436, 8895, 51124], "temperature": 0.0, "avg_logprob": -0.12303363515975628, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.0001641560229472816}, {"id": 391, "seek": 233588, "start": 2351.08, "end": 2354.92, "text": " a neural network on this big database of knots, and they studied the hyperbolic invariance and", "tokens": [51124, 257, 18161, 3209, 322, 341, 955, 8149, 295, 27426, 11, 293, 436, 9454, 264, 9848, 65, 7940, 33270, 719, 293, 51316], "temperature": 0.0, "avg_logprob": -0.12303363515975628, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.0001641560229472816}, {"id": 392, "seek": 233588, "start": 2354.92, "end": 2359.4, "text": " the signature, which is the combinatorial invariant. And they found that you could train the network", "tokens": [51316, 264, 13397, 11, 597, 307, 264, 2512, 31927, 831, 33270, 394, 13, 400, 436, 1352, 300, 291, 727, 3847, 264, 3209, 51540], "temperature": 0.0, "avg_logprob": -0.12303363515975628, "compression_ratio": 1.760180995475113, "no_speech_prob": 0.0001641560229472816}, {"id": 393, "seek": 235940, "start": 2359.48, "end": 2363.7200000000003, "text": " to predict the signature from the hyperbolic invariant with, like, 99 percent accuracy.", "tokens": [50368, 281, 6069, 264, 13397, 490, 264, 9848, 65, 7940, 33270, 394, 365, 11, 411, 11, 11803, 3043, 14170, 13, 50580], "temperature": 0.0, "avg_logprob": -0.09987876415252686, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005270739085972309}, {"id": 394, "seek": 235940, "start": 2365.0, "end": 2369.8, "text": " So, they used, like, half the data to train the set, and then half the data to test the set,", "tokens": [50644, 407, 11, 436, 1143, 11, 411, 11, 1922, 264, 1412, 281, 3847, 264, 992, 11, 293, 550, 1922, 264, 1412, 281, 1500, 264, 992, 11, 50884], "temperature": 0.0, "avg_logprob": -0.09987876415252686, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005270739085972309}, {"id": 395, "seek": 235940, "start": 2370.52, "end": 2377.48, "text": " to test the neural network. And so, what this told them is that there must be some connection.", "tokens": [50920, 281, 1500, 264, 18161, 3209, 13, 400, 370, 11, 437, 341, 1907, 552, 307, 300, 456, 1633, 312, 512, 4984, 13, 51268], "temperature": 0.0, "avg_logprob": -0.09987876415252686, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005270739085972309}, {"id": 396, "seek": 235940, "start": 2377.48, "end": 2382.6800000000003, "text": " The signature of a knot must somehow be a function, or at least very closely related to a function,", "tokens": [51268, 440, 13397, 295, 257, 16966, 1633, 6063, 312, 257, 2445, 11, 420, 412, 1935, 588, 8185, 4077, 281, 257, 2445, 11, 51528], "temperature": 0.0, "avg_logprob": -0.09987876415252686, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005270739085972309}, {"id": 397, "seek": 235940, "start": 2382.6800000000003, "end": 2388.36, "text": " of a hyperbolic invariance. But the problem with neural nets is that they give you a function,", "tokens": [51528, 295, 257, 9848, 65, 7940, 33270, 719, 13, 583, 264, 1154, 365, 18161, 36170, 307, 300, 436, 976, 291, 257, 2445, 11, 51812], "temperature": 0.0, "avg_logprob": -0.09987876415252686, "compression_ratio": 1.9183673469387754, "no_speech_prob": 0.005270739085972309}, {"id": 398, "seek": 238836, "start": 2388.44, "end": 2391.1600000000003, "text": " a functional relationship, or at least a conjectural functional relationship. But", "tokens": [50368, 257, 11745, 2480, 11, 420, 412, 1935, 257, 416, 1020, 1807, 11745, 2480, 13, 583, 50504], "temperature": 0.0, "avg_logprob": -0.14508091183159294, "compression_ratio": 1.8389261744966443, "no_speech_prob": 0.00036029182956553996}, {"id": 399, "seek": 238836, "start": 2391.1600000000003, "end": 2396.92, "text": " it's this massively complicated function. It composes hundreds and hundreds of nonlinear", "tokens": [50504, 309, 311, 341, 29379, 6179, 2445, 13, 467, 715, 4201, 6779, 293, 6779, 295, 2107, 28263, 50792], "temperature": 0.0, "avg_logprob": -0.14508091183159294, "compression_ratio": 1.8389261744966443, "no_speech_prob": 0.00036029182956553996}, {"id": 400, "seek": 238836, "start": 2396.92, "end": 2400.6800000000003, "text": " functions together, and it doesn't give you any insight as to what the relationship is. It just", "tokens": [50792, 6828, 1214, 11, 293, 309, 1177, 380, 976, 291, 604, 11269, 382, 281, 437, 264, 2480, 307, 13, 467, 445, 50980], "temperature": 0.0, "avg_logprob": -0.14508091183159294, "compression_ratio": 1.8389261744966443, "no_speech_prob": 0.00036029182956553996}, {"id": 401, "seek": 238836, "start": 2400.6800000000003, "end": 2407.08, "text": " shows you that there is a connection. But it's possible to do some analysis. So, they actually", "tokens": [50980, 3110, 291, 300, 456, 307, 257, 4984, 13, 583, 309, 311, 1944, 281, 360, 512, 5215, 13, 407, 11, 436, 767, 51300], "temperature": 0.0, "avg_logprob": -0.14508091183159294, "compression_ratio": 1.8389261744966443, "no_speech_prob": 0.00036029182956553996}, {"id": 402, "seek": 238836, "start": 2407.08, "end": 2411.7200000000003, "text": " tried a very basic thing which happened to work. It's what's called a saliency analysis. So,", "tokens": [51300, 3031, 257, 588, 3875, 551, 597, 2011, 281, 589, 13, 467, 311, 437, 311, 1219, 257, 1845, 7848, 5215, 13, 407, 11, 51532], "temperature": 0.0, "avg_logprob": -0.14508091183159294, "compression_ratio": 1.8389261744966443, "no_speech_prob": 0.00036029182956553996}, {"id": 403, "seek": 238836, "start": 2411.7200000000003, "end": 2417.8, "text": " this neural network gave them this function. Basically, they fed in 20 hyperbolic invariances", "tokens": [51532, 341, 18161, 3209, 2729, 552, 341, 2445, 13, 8537, 11, 436, 4636, 294, 945, 9848, 65, 7940, 33270, 2676, 51836], "temperature": 0.0, "avg_logprob": -0.14508091183159294, "compression_ratio": 1.8389261744966443, "no_speech_prob": 0.00036029182956553996}, {"id": 404, "seek": 241780, "start": 2417.96, "end": 2422.76, "text": " as input, and the signature as output. So, it's basically a function from R to 20 to the integers,", "tokens": [50372, 382, 4846, 11, 293, 264, 13397, 382, 5598, 13, 407, 11, 309, 311, 1936, 257, 2445, 490, 497, 281, 945, 281, 264, 41674, 11, 50612], "temperature": 0.0, "avg_logprob": -0.10535084429404719, "compression_ratio": 1.9595959595959596, "no_speech_prob": 0.0008248205413110554}, {"id": 405, "seek": 241780, "start": 2422.76, "end": 2427.0, "text": " I think. And what they decided to do is that, once they had this function, they just tested", "tokens": [50612, 286, 519, 13, 400, 437, 436, 3047, 281, 360, 307, 300, 11, 1564, 436, 632, 341, 2445, 11, 436, 445, 8246, 50824], "temperature": 0.0, "avg_logprob": -0.10535084429404719, "compression_ratio": 1.9595959595959596, "no_speech_prob": 0.0008248205413110554}, {"id": 406, "seek": 241780, "start": 2427.0, "end": 2432.44, "text": " how sensitive the function was to each of its inputs. So, they just varied one of the 20 variables,", "tokens": [50824, 577, 9477, 264, 2445, 390, 281, 1184, 295, 1080, 15743, 13, 407, 11, 436, 445, 22877, 472, 295, 264, 945, 9102, 11, 51096], "temperature": 0.0, "avg_logprob": -0.10535084429404719, "compression_ratio": 1.9595959595959596, "no_speech_prob": 0.0008248205413110554}, {"id": 407, "seek": 241780, "start": 2432.44, "end": 2437.6400000000003, "text": " and they saw what happened to the output. And what they found was that only three of the 20", "tokens": [51096, 293, 436, 1866, 437, 2011, 281, 264, 5598, 13, 400, 437, 436, 1352, 390, 300, 787, 1045, 295, 264, 945, 51356], "temperature": 0.0, "avg_logprob": -0.10535084429404719, "compression_ratio": 1.9595959595959596, "no_speech_prob": 0.0008248205413110554}, {"id": 408, "seek": 241780, "start": 2437.6400000000003, "end": 2441.96, "text": " variables were actually important, that only three of them had a significant influence on the function.", "tokens": [51356, 9102, 645, 767, 1021, 11, 300, 787, 1045, 295, 552, 632, 257, 4776, 6503, 322, 264, 2445, 13, 51572], "temperature": 0.0, "avg_logprob": -0.10535084429404719, "compression_ratio": 1.9595959595959596, "no_speech_prob": 0.0008248205413110554}, {"id": 409, "seek": 241780, "start": 2441.96, "end": 2447.48, "text": " The other 17 were basically not relevant at all. And so, they focused on those three variables,", "tokens": [51572, 440, 661, 3282, 645, 1936, 406, 7340, 412, 439, 13, 400, 370, 11, 436, 5178, 322, 729, 1045, 9102, 11, 51848], "temperature": 0.0, "avg_logprob": -0.10535084429404719, "compression_ratio": 1.9595959595959596, "no_speech_prob": 0.0008248205413110554}, {"id": 410, "seek": 244748, "start": 2447.48, "end": 2454.36, "text": " and they started plotting this function just restricted to those three variables. And that's", "tokens": [50364, 293, 436, 1409, 41178, 341, 2445, 445, 20608, 281, 729, 1045, 9102, 13, 400, 300, 311, 50708], "temperature": 0.0, "avg_logprob": -0.07554118487299705, "compression_ratio": 1.950207468879668, "no_speech_prob": 0.00018097888096235693}, {"id": 411, "seek": 244748, "start": 2454.36, "end": 2457.96, "text": " just low enough to mention that you can eyeball the relationships. So, they started plotting", "tokens": [50708, 445, 2295, 1547, 281, 2152, 300, 291, 393, 38868, 264, 6159, 13, 407, 11, 436, 1409, 41178, 50888], "temperature": 0.0, "avg_logprob": -0.07554118487299705, "compression_ratio": 1.950207468879668, "no_speech_prob": 0.00018097888096235693}, {"id": 412, "seek": 244748, "start": 2458.6, "end": 2463.16, "text": " the signature as a function of two or three of these variables and using color and so forth.", "tokens": [50920, 264, 13397, 382, 257, 2445, 295, 732, 420, 1045, 295, 613, 9102, 293, 1228, 2017, 293, 370, 5220, 13, 51148], "temperature": 0.0, "avg_logprob": -0.07554118487299705, "compression_ratio": 1.950207468879668, "no_speech_prob": 0.00018097888096235693}, {"id": 413, "seek": 244748, "start": 2463.16, "end": 2468.76, "text": " And they started seeing patterns, and they could use these patterns to conjecture various", "tokens": [51148, 400, 436, 1409, 2577, 8294, 11, 293, 436, 727, 764, 613, 8294, 281, 416, 1020, 540, 3683, 51428], "temperature": 0.0, "avg_logprob": -0.07554118487299705, "compression_ratio": 1.950207468879668, "no_speech_prob": 0.00018097888096235693}, {"id": 414, "seek": 244748, "start": 2468.76, "end": 2474.28, "text": " inequalities and various relationships. It turns out that the first few inequalities they conjectured", "tokens": [51428, 41874, 293, 3683, 6159, 13, 467, 4523, 484, 300, 264, 700, 1326, 41874, 436, 416, 1020, 3831, 51704], "temperature": 0.0, "avg_logprob": -0.07554118487299705, "compression_ratio": 1.950207468879668, "no_speech_prob": 0.00018097888096235693}, {"id": 415, "seek": 247428, "start": 2474.36, "end": 2483.7200000000003, "text": " were false, and they could use the new net and the data set to confirm this. But with a bit of", "tokens": [50368, 645, 7908, 11, 293, 436, 727, 764, 264, 777, 2533, 293, 264, 1412, 992, 281, 9064, 341, 13, 583, 365, 257, 857, 295, 50836], "temperature": 0.0, "avg_logprob": -0.1700503775414, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0002517545653972775}, {"id": 416, "seek": 247428, "start": 2483.7200000000003, "end": 2489.1600000000003, "text": " back and forth, they were able to eventually settle upon a correct conjecture relating these", "tokens": [50836, 646, 293, 5220, 11, 436, 645, 1075, 281, 4728, 11852, 3564, 257, 3006, 416, 1020, 540, 23968, 613, 51108], "temperature": 0.0, "avg_logprob": -0.1700503775414, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0002517545653972775}, {"id": 417, "seek": 247428, "start": 2489.1600000000003, "end": 2499.8, "text": " invariants to the signature. And it wasn't the invariants that they expected. So, yeah, they", "tokens": [51108, 33270, 1719, 281, 264, 13397, 13, 400, 309, 2067, 380, 264, 33270, 1719, 300, 436, 5176, 13, 407, 11, 1338, 11, 436, 51640], "temperature": 0.0, "avg_logprob": -0.1700503775414, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0002517545653972775}, {"id": 418, "seek": 247428, "start": 2499.8, "end": 2503.1600000000003, "text": " were expecting the hypervolume, for example, to be really important and not to be relevant at all.", "tokens": [51640, 645, 9650, 264, 9848, 9646, 2540, 11, 337, 1365, 11, 281, 312, 534, 1021, 293, 406, 281, 312, 7340, 412, 439, 13, 51808], "temperature": 0.0, "avg_logprob": -0.1700503775414, "compression_ratio": 1.676991150442478, "no_speech_prob": 0.0002517545653972775}, {"id": 419, "seek": 250428, "start": 2505.0, "end": 2509.0, "text": " But once they found the right variables and the right conjectured inequality,", "tokens": [50400, 583, 1564, 436, 1352, 264, 558, 9102, 293, 264, 558, 416, 1020, 3831, 16970, 11, 50600], "temperature": 0.0, "avg_logprob": -0.10876028221773815, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.00031598922214470804}, {"id": 420, "seek": 250428, "start": 2509.6400000000003, "end": 2514.1200000000003, "text": " it suggested the proof, and then they were able to actually find a rigorous proof of the inequality", "tokens": [50632, 309, 10945, 264, 8177, 11, 293, 550, 436, 645, 1075, 281, 767, 915, 257, 29882, 8177, 295, 264, 16970, 50856], "temperature": 0.0, "avg_logprob": -0.10876028221773815, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.00031598922214470804}, {"id": 421, "seek": 250428, "start": 2514.1200000000003, "end": 2519.2400000000002, "text": " that they conjectured. So, it was a very nice back and forth between using the machine learning", "tokens": [50856, 300, 436, 416, 1020, 3831, 13, 407, 11, 309, 390, 257, 588, 1481, 646, 293, 5220, 1296, 1228, 264, 3479, 2539, 51112], "temperature": 0.0, "avg_logprob": -0.10876028221773815, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.00031598922214470804}, {"id": 422, "seek": 250428, "start": 2520.6000000000004, "end": 2527.88, "text": " tool to suggest the way forward, but then going back to traditional mathematics to prove things.", "tokens": [51180, 2290, 281, 3402, 264, 636, 2128, 11, 457, 550, 516, 646, 281, 5164, 18666, 281, 7081, 721, 13, 51544], "temperature": 0.0, "avg_logprob": -0.10876028221773815, "compression_ratio": 1.7289719626168225, "no_speech_prob": 0.00031598922214470804}, {"id": 423, "seek": 252788, "start": 2528.84, "end": 2536.28, "text": " Okay. So, of course, the most high-profile development these days has been large language", "tokens": [50412, 1033, 13, 407, 11, 295, 1164, 11, 264, 881, 1090, 12, 29175, 794, 3250, 613, 1708, 575, 668, 2416, 2856, 50784], "temperature": 0.0, "avg_logprob": -0.20325024922688803, "compression_ratio": 1.528688524590164, "no_speech_prob": 0.00032821865170262754}, {"id": 424, "seek": 252788, "start": 2536.28, "end": 2543.7200000000003, "text": " models like GPT. And sometimes they work really, really well. So, here's an example of GPT-4,", "tokens": [50784, 5245, 411, 26039, 51, 13, 400, 2171, 436, 589, 534, 11, 534, 731, 13, 407, 11, 510, 311, 364, 1365, 295, 26039, 51, 12, 19, 11, 51156], "temperature": 0.0, "avg_logprob": -0.20325024922688803, "compression_ratio": 1.528688524590164, "no_speech_prob": 0.00032821865170262754}, {"id": 425, "seek": 252788, "start": 2543.7200000000003, "end": 2548.28, "text": " which is OpenAI's most advanced large language model, actually solving a problem from the", "tokens": [51156, 597, 307, 7238, 48698, 311, 881, 7339, 2416, 2856, 2316, 11, 767, 12606, 257, 1154, 490, 264, 51384], "temperature": 0.0, "avg_logprob": -0.20325024922688803, "compression_ratio": 1.528688524590164, "no_speech_prob": 0.00032821865170262754}, {"id": 426, "seek": 252788, "start": 2548.28, "end": 2552.92, "text": " IMO, the International Mathematical Olympiad. And so, it's a question, you know, there's a function", "tokens": [51384, 286, 18976, 11, 264, 9157, 15776, 8615, 804, 10395, 38069, 13, 400, 370, 11, 309, 311, 257, 1168, 11, 291, 458, 11, 456, 311, 257, 2445, 51616], "temperature": 0.0, "avg_logprob": -0.20325024922688803, "compression_ratio": 1.528688524590164, "no_speech_prob": 0.00032821865170262754}, {"id": 427, "seek": 255292, "start": 2552.92, "end": 2558.44, "text": " that obeys a search function equation. Can you prove, can you solve for the function?", "tokens": [50364, 300, 36346, 749, 257, 3164, 2445, 5367, 13, 1664, 291, 7081, 11, 393, 291, 5039, 337, 264, 2445, 30, 50640], "temperature": 0.0, "avg_logprob": -0.142816994871412, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.002702956320717931}, {"id": 428, "seek": 255292, "start": 2558.44, "end": 2564.28, "text": " And it actually happens to give a completely correct proof. Now, this is an extremely cherry", "tokens": [50640, 400, 309, 767, 2314, 281, 976, 257, 2584, 3006, 8177, 13, 823, 11, 341, 307, 364, 4664, 20164, 50932], "temperature": 0.0, "avg_logprob": -0.142816994871412, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.002702956320717931}, {"id": 429, "seek": 255292, "start": 2564.28, "end": 2570.92, "text": " picked example. I think they tried, from this paper, they tried all the recent IMO problems,", "tokens": [50932, 6183, 1365, 13, 286, 519, 436, 3031, 11, 490, 341, 3035, 11, 436, 3031, 439, 264, 5162, 286, 18976, 2740, 11, 51264], "temperature": 0.0, "avg_logprob": -0.142816994871412, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.002702956320717931}, {"id": 430, "seek": 255292, "start": 2570.92, "end": 2575.64, "text": " and they could solve like one percent of the problems of this method. You know, famously,", "tokens": [51264, 293, 436, 727, 5039, 411, 472, 3043, 295, 264, 2740, 295, 341, 3170, 13, 509, 458, 11, 34360, 11, 51500], "temperature": 0.0, "avg_logprob": -0.142816994871412, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.002702956320717931}, {"id": 431, "seek": 255292, "start": 2575.64, "end": 2581.16, "text": " it's bad even at basic arithmetic. You know, there's a, you ask it to solve seven times four", "tokens": [51500, 309, 311, 1578, 754, 412, 3875, 42973, 13, 509, 458, 11, 456, 311, 257, 11, 291, 1029, 309, 281, 5039, 3407, 1413, 1451, 51776], "temperature": 0.0, "avg_logprob": -0.142816994871412, "compression_ratio": 1.6569343065693432, "no_speech_prob": 0.002702956320717931}, {"id": 432, "seek": 258116, "start": 2581.16, "end": 2585.3999999999996, "text": " plus eight times eight, and it'll give you the wrong answer. It gives you 120. And then it will", "tokens": [50364, 1804, 3180, 1413, 3180, 11, 293, 309, 603, 976, 291, 264, 2085, 1867, 13, 467, 2709, 291, 10411, 13, 400, 550, 309, 486, 50576], "temperature": 0.0, "avg_logprob": -0.1594053109486898, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0008572324295528233}, {"id": 433, "seek": 258116, "start": 2585.3999999999996, "end": 2590.2, "text": " keep going and say, and I'll explain why. And during the course of the explanation, it will", "tokens": [50576, 1066, 516, 293, 584, 11, 293, 286, 603, 2903, 983, 13, 400, 1830, 264, 1164, 295, 264, 10835, 11, 309, 486, 50816], "temperature": 0.0, "avg_logprob": -0.1594053109486898, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0008572324295528233}, {"id": 434, "seek": 258116, "start": 2590.2, "end": 2596.52, "text": " actually make a mistake. And, yeah, and then you point out that they made a mistake and say,", "tokens": [50816, 767, 652, 257, 6146, 13, 400, 11, 1338, 11, 293, 550, 291, 935, 484, 300, 436, 1027, 257, 6146, 293, 584, 11, 51132], "temperature": 0.0, "avg_logprob": -0.1594053109486898, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0008572324295528233}, {"id": 435, "seek": 258116, "start": 2596.52, "end": 2602.68, "text": " I'm sorry, the previous answer is incorrect. I mean, these large language models, they remind", "tokens": [51132, 286, 478, 2597, 11, 264, 3894, 1867, 307, 18424, 13, 286, 914, 11, 613, 2416, 2856, 5245, 11, 436, 4160, 51440], "temperature": 0.0, "avg_logprob": -0.1594053109486898, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0008572324295528233}, {"id": 436, "seek": 258116, "start": 2602.68, "end": 2607.16, "text": " me a lot of, you know, if you have sort of a somewhat weak undergraduate student in office hours,", "tokens": [51440, 385, 257, 688, 295, 11, 291, 458, 11, 498, 291, 362, 1333, 295, 257, 8344, 5336, 19113, 3107, 294, 3398, 2496, 11, 51664], "temperature": 0.0, "avg_logprob": -0.1594053109486898, "compression_ratio": 1.7163636363636363, "no_speech_prob": 0.0008572324295528233}, {"id": 437, "seek": 260716, "start": 2607.16, "end": 2612.52, "text": " and you ask them to solve a problem at the blackboard with no, with no age, you know,", "tokens": [50364, 293, 291, 1029, 552, 281, 5039, 257, 1154, 412, 264, 2211, 3787, 365, 572, 11, 365, 572, 3205, 11, 291, 458, 11, 50632], "temperature": 0.0, "avg_logprob": -0.1721682467703092, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00033364593400619924}, {"id": 438, "seek": 260716, "start": 2612.52, "end": 2617.08, "text": " it will, you know, he or she will try to try their best, you know, and try to turn something", "tokens": [50632, 309, 486, 11, 291, 458, 11, 415, 420, 750, 486, 853, 281, 853, 641, 1151, 11, 291, 458, 11, 293, 853, 281, 1261, 746, 50860], "temperature": 0.0, "avg_logprob": -0.1721682467703092, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00033364593400619924}, {"id": 439, "seek": 260716, "start": 2617.08, "end": 2622.52, "text": " that looks like a proof. But, yeah, they don't really have a good way of correcting themselves.", "tokens": [50860, 300, 1542, 411, 257, 8177, 13, 583, 11, 1338, 11, 436, 500, 380, 534, 362, 257, 665, 636, 295, 47032, 2969, 13, 51132], "temperature": 0.0, "avg_logprob": -0.1721682467703092, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00033364593400619924}, {"id": 440, "seek": 260716, "start": 2624.3599999999997, "end": 2628.2, "text": " So, you know, sometimes they work really well, but often they're very, very unreliable.", "tokens": [51224, 407, 11, 291, 458, 11, 2171, 436, 589, 534, 731, 11, 457, 2049, 436, 434, 588, 11, 588, 20584, 2081, 712, 13, 51416], "temperature": 0.0, "avg_logprob": -0.1721682467703092, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00033364593400619924}, {"id": 441, "seek": 260716, "start": 2630.52, "end": 2636.68, "text": " But there's lots of interesting recent experiments where you can couple these language models to", "tokens": [51532, 583, 456, 311, 3195, 295, 1880, 5162, 12050, 689, 291, 393, 1916, 613, 2856, 5245, 281, 51840], "temperature": 0.0, "avg_logprob": -0.1721682467703092, "compression_ratio": 1.7722007722007722, "no_speech_prob": 0.00033364593400619924}, {"id": 442, "seek": 263668, "start": 2636.68, "end": 2641.8799999999997, "text": " other much more reliable tools to do mathematics. So, for example, GPT now comes with plugins for", "tokens": [50364, 661, 709, 544, 12924, 3873, 281, 360, 18666, 13, 407, 11, 337, 1365, 11, 26039, 51, 586, 1487, 365, 33759, 337, 50624], "temperature": 0.0, "avg_logprob": -0.16165202977705975, "compression_ratio": 1.6, "no_speech_prob": 0.00018467288464307785}, {"id": 443, "seek": 263668, "start": 2641.8799999999997, "end": 2649.24, "text": " Wolfram Alpha. So, now, if you ask GPT to do an ethnic calculation, it knows better than to try", "tokens": [50624, 16634, 2356, 20588, 13, 407, 11, 586, 11, 498, 291, 1029, 26039, 51, 281, 360, 364, 14363, 17108, 11, 309, 3255, 1101, 813, 281, 853, 50992], "temperature": 0.0, "avg_logprob": -0.16165202977705975, "compression_ratio": 1.6, "no_speech_prob": 0.00018467288464307785}, {"id": 444, "seek": 263668, "start": 2649.24, "end": 2657.72, "text": " to do it itself. It will outsource it to Wolfram Alpha. Then there's more recent examples where", "tokens": [50992, 281, 360, 309, 2564, 13, 467, 486, 14758, 2948, 309, 281, 16634, 2356, 20588, 13, 1396, 456, 311, 544, 5162, 5110, 689, 51416], "temperature": 0.0, "avg_logprob": -0.16165202977705975, "compression_ratio": 1.6, "no_speech_prob": 0.00018467288464307785}, {"id": 445, "seek": 263668, "start": 2657.72, "end": 2663.24, "text": " people are coupling these large language models to a proof error file like Lean. So, you know,", "tokens": [51416, 561, 366, 37447, 613, 2416, 2856, 5245, 281, 257, 8177, 6713, 3991, 411, 49303, 13, 407, 11, 291, 458, 11, 51692], "temperature": 0.0, "avg_logprob": -0.16165202977705975, "compression_ratio": 1.6, "no_speech_prob": 0.00018467288464307785}, {"id": 446, "seek": 266324, "start": 2663.3199999999997, "end": 2668.3599999999997, "text": " you ask it to prove as a statement, you know, prove that the union of two open sets is open.", "tokens": [50368, 291, 1029, 309, 281, 7081, 382, 257, 5629, 11, 291, 458, 11, 7081, 300, 264, 11671, 295, 732, 1269, 6352, 307, 1269, 13, 50620], "temperature": 0.0, "avg_logprob": -0.16386650704048775, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.004369492642581463}, {"id": 447, "seek": 266324, "start": 2669.3999999999996, "end": 2673.3999999999996, "text": " If you ask a raw large language model, it will give you a statement that", "tokens": [50672, 759, 291, 1029, 257, 8936, 2416, 2856, 2316, 11, 309, 486, 976, 291, 257, 5629, 300, 50872], "temperature": 0.0, "avg_logprob": -0.16386650704048775, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.004369492642581463}, {"id": 448, "seek": 266324, "start": 2673.3999999999996, "end": 2677.64, "text": " a proof that looks like a proof. But there's lots of little logical errors in the proof, but", "tokens": [50872, 257, 8177, 300, 1542, 411, 257, 8177, 13, 583, 456, 311, 3195, 295, 707, 14978, 13603, 294, 264, 8177, 11, 457, 51084], "temperature": 0.0, "avg_logprob": -0.16386650704048775, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.004369492642581463}, {"id": 449, "seek": 266324, "start": 2678.4399999999996, "end": 2683.56, "text": " you can force it to output in Lean, get Lean to compile, and if there's a completion error,", "tokens": [51124, 291, 393, 3464, 309, 281, 5598, 294, 49303, 11, 483, 49303, 281, 31413, 11, 293, 498, 456, 311, 257, 19372, 6713, 11, 51380], "temperature": 0.0, "avg_logprob": -0.16386650704048775, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.004369492642581463}, {"id": 450, "seek": 266324, "start": 2683.56, "end": 2687.08, "text": " it will send back the error to the large language model and have to correct it and", "tokens": [51380, 309, 486, 2845, 646, 264, 6713, 281, 264, 2416, 2856, 2316, 293, 362, 281, 3006, 309, 293, 51556], "temperature": 0.0, "avg_logprob": -0.16386650704048775, "compression_ratio": 1.8117154811715481, "no_speech_prob": 0.004369492642581463}, {"id": 451, "seek": 268708, "start": 2687.08, "end": 2691.3199999999997, "text": " create a feedback loop. And it can actually be used to solve", "tokens": [50364, 1884, 257, 5824, 6367, 13, 400, 309, 393, 767, 312, 1143, 281, 5039, 50576], "temperature": 0.0, "avg_logprob": -0.1426710348862868, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.008672408759593964}, {"id": 452, "seek": 268708, "start": 2692.44, "end": 2697.56, "text": " roughly sort of undergraduate math homework level problems by this technique. But now,", "tokens": [50632, 9810, 1333, 295, 19113, 5221, 14578, 1496, 2740, 538, 341, 6532, 13, 583, 586, 11, 50888], "temperature": 0.0, "avg_logprob": -0.1426710348862868, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.008672408759593964}, {"id": 453, "seek": 268708, "start": 2697.56, "end": 2702.52, "text": " with 100% guarantee of accuracy, if it works, I mean, of course, sometimes it will just get stuck", "tokens": [50888, 365, 2319, 4, 10815, 295, 14170, 11, 498, 309, 1985, 11, 286, 914, 11, 295, 1164, 11, 2171, 309, 486, 445, 483, 5541, 51136], "temperature": 0.0, "avg_logprob": -0.1426710348862868, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.008672408759593964}, {"id": 454, "seek": 268708, "start": 2702.52, "end": 2708.36, "text": " and give up because it can never get the Lean compiler to accept the argument. But", "tokens": [51136, 293, 976, 493, 570, 309, 393, 1128, 483, 264, 49303, 31958, 281, 3241, 264, 6770, 13, 583, 51428], "temperature": 0.0, "avg_logprob": -0.1426710348862868, "compression_ratio": 1.4841628959276019, "no_speech_prob": 0.008672408759593964}, {"id": 455, "seek": 270836, "start": 2708.6, "end": 2718.36, "text": " it is beginning to make some headway. As I said before, I do find it can be useful", "tokens": [50376, 309, 307, 2863, 281, 652, 512, 1378, 676, 13, 1018, 286, 848, 949, 11, 286, 360, 915, 309, 393, 312, 4420, 50864], "temperature": 0.0, "avg_logprob": -0.1986520463141842, "compression_ratio": 1.5058823529411764, "no_speech_prob": 0.0015563606284558773}, {"id": 456, "seek": 270836, "start": 2719.48, "end": 2726.36, "text": " as a muse, kind of like if you're just starting on a project, I recently was trying to", "tokens": [50920, 382, 257, 39138, 11, 733, 295, 411, 498, 291, 434, 445, 2891, 322, 257, 1716, 11, 286, 3938, 390, 1382, 281, 51264], "temperature": 0.0, "avg_logprob": -0.1986520463141842, "compression_ratio": 1.5058823529411764, "no_speech_prob": 0.0015563606284558773}, {"id": 457, "seek": 270836, "start": 2729.08, "end": 2734.52, "text": " prove some commentary identity. And I was thinking of using, I had some ideas in mind,", "tokens": [51400, 7081, 512, 23527, 6575, 13, 400, 286, 390, 1953, 295, 1228, 11, 286, 632, 512, 3487, 294, 1575, 11, 51672], "temperature": 0.0, "avg_logprob": -0.1986520463141842, "compression_ratio": 1.5058823529411764, "no_speech_prob": 0.0015563606284558773}, {"id": 458, "seek": 273452, "start": 2734.6, "end": 2740.12, "text": " I was going to use asymptotics, work with some special cases, but nothing was working. And I", "tokens": [50368, 286, 390, 516, 281, 764, 35114, 42131, 11, 589, 365, 512, 2121, 3331, 11, 457, 1825, 390, 1364, 13, 400, 286, 50644], "temperature": 0.0, "avg_logprob": -0.15417916275734125, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005929689854383469}, {"id": 459, "seek": 273452, "start": 2740.12, "end": 2744.04, "text": " asked GPT for some suggestions. And it gave me some suggestions I was already thinking of,", "tokens": [50644, 2351, 26039, 51, 337, 512, 13396, 13, 400, 309, 2729, 385, 512, 13396, 286, 390, 1217, 1953, 295, 11, 50840], "temperature": 0.0, "avg_logprob": -0.15417916275734125, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005929689854383469}, {"id": 460, "seek": 273452, "start": 2744.84, "end": 2751.48, "text": " plus some suggestions that were either completely vacuous or wrong. But it did tell me that you", "tokens": [50880, 1804, 512, 13396, 300, 645, 2139, 2584, 2842, 12549, 420, 2085, 13, 583, 309, 630, 980, 385, 300, 291, 51212], "temperature": 0.0, "avg_logprob": -0.15417916275734125, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005929689854383469}, {"id": 461, "seek": 273452, "start": 2751.48, "end": 2756.68, "text": " should probably use generating functions, which I should have known. But at the time,", "tokens": [51212, 820, 1391, 764, 17746, 6828, 11, 597, 286, 820, 362, 2570, 13, 583, 412, 264, 565, 11, 51472], "temperature": 0.0, "avg_logprob": -0.15417916275734125, "compression_ratio": 1.6367713004484306, "no_speech_prob": 0.005929689854383469}, {"id": 462, "seek": 275668, "start": 2756.9199999999996, "end": 2764.6, "text": " it escaped me. And just with that hint, I was able to actually get them to work.", "tokens": [50376, 309, 20397, 385, 13, 400, 445, 365, 300, 12075, 11, 286, 390, 1075, 281, 767, 483, 552, 281, 589, 13, 50760], "temperature": 0.0, "avg_logprob": -0.21547083421186966, "compression_ratio": 1.5211267605633803, "no_speech_prob": 0.0026326810475438833}, {"id": 463, "seek": 275668, "start": 2766.68, "end": 2772.2799999999997, "text": " So, you know, I mean, as just kind of someone to double check your thought process,", "tokens": [50864, 407, 11, 291, 458, 11, 286, 914, 11, 382, 445, 733, 295, 1580, 281, 3834, 1520, 428, 1194, 1399, 11, 51144], "temperature": 0.0, "avg_logprob": -0.21547083421186966, "compression_ratio": 1.5211267605633803, "no_speech_prob": 0.0026326810475438833}, {"id": 464, "seek": 275668, "start": 2772.8399999999997, "end": 2777.24, "text": " it is sort of useful. Still not great, but it has some potential use.", "tokens": [51172, 309, 307, 1333, 295, 4420, 13, 8291, 406, 869, 11, 457, 309, 575, 512, 3995, 764, 13, 51392], "temperature": 0.0, "avg_logprob": -0.21547083421186966, "compression_ratio": 1.5211267605633803, "no_speech_prob": 0.0026326810475438833}, {"id": 465, "seek": 275668, "start": 2779.56, "end": 2785.7999999999997, "text": " There was another tool which I do like, and I use more and more now. It's integrated into", "tokens": [51508, 821, 390, 1071, 2290, 597, 286, 360, 411, 11, 293, 286, 764, 544, 293, 544, 586, 13, 467, 311, 10919, 666, 51820], "temperature": 0.0, "avg_logprob": -0.21547083421186966, "compression_ratio": 1.5211267605633803, "no_speech_prob": 0.0026326810475438833}, {"id": 466, "seek": 278580, "start": 2785.88, "end": 2789.7200000000003, "text": " something called VS code, which is an editor to write code that can also be used for latex,", "tokens": [50368, 746, 1219, 25091, 3089, 11, 597, 307, 364, 9839, 281, 2464, 3089, 300, 393, 611, 312, 1143, 337, 3469, 87, 11, 50560], "temperature": 0.0, "avg_logprob": -0.24107770023182926, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.0018564187921583652}, {"id": 467, "seek": 278580, "start": 2790.28, "end": 2795.2400000000002, "text": " something called GitHub co-pilot. It's basically an AI-powered autocomplete,", "tokens": [50588, 746, 1219, 23331, 598, 12, 79, 31516, 13, 467, 311, 1936, 364, 7318, 12, 27178, 45833, 298, 17220, 11, 50836], "temperature": 0.0, "avg_logprob": -0.24107770023182926, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.0018564187921583652}, {"id": 468, "seek": 278580, "start": 2796.04, "end": 2802.04, "text": " and you type in your code or your latex, whatever. And based on all the text that you've", "tokens": [50876, 293, 291, 2010, 294, 428, 3089, 420, 428, 3469, 87, 11, 2035, 13, 400, 2361, 322, 439, 264, 2487, 300, 291, 600, 51176], "temperature": 0.0, "avg_logprob": -0.24107770023182926, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.0018564187921583652}, {"id": 469, "seek": 278580, "start": 2802.04, "end": 2808.36, "text": " already written, it will suggest a possible new sentence to generate. And so it can be very useful", "tokens": [51176, 1217, 3720, 11, 309, 486, 3402, 257, 1944, 777, 8174, 281, 8460, 13, 400, 370, 309, 393, 312, 588, 4420, 51492], "temperature": 0.0, "avg_logprob": -0.24107770023182926, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.0018564187921583652}, {"id": 470, "seek": 278580, "start": 2808.36, "end": 2812.6000000000004, "text": " for code. You write down three lines of code, and it's just the fourth. And sometimes you'll get", "tokens": [51492, 337, 3089, 13, 509, 2464, 760, 1045, 3876, 295, 3089, 11, 293, 309, 311, 445, 264, 6409, 13, 400, 2171, 291, 603, 483, 51704], "temperature": 0.0, "avg_logprob": -0.24107770023182926, "compression_ratio": 1.6472727272727272, "no_speech_prob": 0.0018564187921583652}, {"id": 471, "seek": 281260, "start": 2812.6, "end": 2817.72, "text": " exactly right. Sometimes it's not exactly right. Sometimes it's complete rubbish. But then you", "tokens": [50364, 2293, 558, 13, 4803, 309, 311, 406, 2293, 558, 13, 4803, 309, 311, 3566, 29978, 13, 583, 550, 291, 50620], "temperature": 0.0, "avg_logprob": -0.12626413304171116, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.002512942533940077}, {"id": 472, "seek": 281260, "start": 2817.72, "end": 2821.24, "text": " can accept it, and it can save a lot of time, especially if you're doing some very menial", "tokens": [50620, 393, 3241, 309, 11, 293, 309, 393, 3155, 257, 688, 295, 565, 11, 2318, 498, 291, 434, 884, 512, 588, 1706, 831, 50796], "temperature": 0.0, "avg_logprob": -0.12626413304171116, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.002512942533940077}, {"id": 473, "seek": 281260, "start": 2821.24, "end": 2826.36, "text": " code where you're repeating something over and over again. It works for latex. I wrote", "tokens": [50796, 3089, 689, 291, 434, 18617, 746, 670, 293, 670, 797, 13, 467, 1985, 337, 3469, 87, 13, 286, 4114, 51052], "temperature": 0.0, "avg_logprob": -0.12626413304171116, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.002512942533940077}, {"id": 474, "seek": 281260, "start": 2827.88, "end": 2832.04, "text": " a latex blog post, actually, recently, where I was trying to estimate an integral, and I broke", "tokens": [51128, 257, 3469, 87, 6968, 2183, 11, 767, 11, 3938, 11, 689, 286, 390, 1382, 281, 12539, 364, 11573, 11, 293, 286, 6902, 51336], "temperature": 0.0, "avg_logprob": -0.12626413304171116, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.002512942533940077}, {"id": 475, "seek": 281260, "start": 2832.04, "end": 2835.72, "text": " up the integral into three pieces. I said, okay, the first piece I can estimate by this technique,", "tokens": [51336, 493, 264, 11573, 666, 1045, 3755, 13, 286, 848, 11, 1392, 11, 264, 700, 2522, 286, 393, 12539, 538, 341, 6532, 11, 51520], "temperature": 0.0, "avg_logprob": -0.12626413304171116, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.002512942533940077}, {"id": 476, "seek": 281260, "start": 2835.72, "end": 2839.72, "text": " and I wrote down how to estimate this technique. And then the co-pilot just suggested how to", "tokens": [51520, 293, 286, 4114, 760, 577, 281, 12539, 341, 6532, 13, 400, 550, 264, 598, 12, 79, 31516, 445, 10945, 577, 281, 51720], "temperature": 0.0, "avg_logprob": -0.12626413304171116, "compression_ratio": 1.8415841584158417, "no_speech_prob": 0.002512942533940077}, {"id": 477, "seek": 283972, "start": 2840.4399999999996, "end": 2845.3199999999997, "text": " estimate the second term. And actually, a completely correct argument. It was a modification of what", "tokens": [50400, 12539, 264, 1150, 1433, 13, 400, 767, 11, 257, 2584, 3006, 6770, 13, 467, 390, 257, 26747, 295, 437, 50644], "temperature": 0.0, "avg_logprob": -0.16968749671854, "compression_ratio": 1.5875, "no_speech_prob": 0.0005911464104428887}, {"id": 478, "seek": 283972, "start": 2845.3199999999997, "end": 2852.7599999999998, "text": " I had already written. So it's very good at just modifying text that you've already appeared.", "tokens": [50644, 286, 632, 1217, 3720, 13, 407, 309, 311, 588, 665, 412, 445, 42626, 2487, 300, 291, 600, 1217, 8516, 13, 51016], "temperature": 0.0, "avg_logprob": -0.16968749671854, "compression_ratio": 1.5875, "no_speech_prob": 0.0005911464104428887}, {"id": 479, "seek": 283972, "start": 2854.6, "end": 2862.2799999999997, "text": " And it's slowly being integrated into prefixes like Lean. So to the point where one line proves,", "tokens": [51108, 400, 309, 311, 5692, 885, 10919, 666, 18417, 36005, 411, 49303, 13, 407, 281, 264, 935, 689, 472, 1622, 25019, 11, 51492], "temperature": 0.0, "avg_logprob": -0.16968749671854, "compression_ratio": 1.5875, "no_speech_prob": 0.0005911464104428887}, {"id": 480, "seek": 283972, "start": 2862.2799999999997, "end": 2866.4399999999996, "text": " two line proves, we can kind of get the AI to fill in for us. The kind of steps that that", "tokens": [51492, 732, 1622, 25019, 11, 321, 393, 733, 295, 483, 264, 7318, 281, 2836, 294, 337, 505, 13, 440, 733, 295, 4439, 300, 300, 51700], "temperature": 0.0, "avg_logprob": -0.16968749671854, "compression_ratio": 1.5875, "no_speech_prob": 0.0005911464104428887}, {"id": 481, "seek": 286644, "start": 2867.4, "end": 2873.16, "text": " this is obvious or clearly this is true in a paper proof. I mean, not all of them, but we can get", "tokens": [50412, 341, 307, 6322, 420, 4448, 341, 307, 2074, 294, 257, 3035, 8177, 13, 286, 914, 11, 406, 439, 295, 552, 11, 457, 321, 393, 483, 50700], "temperature": 0.0, "avg_logprob": -0.15560222366481152, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0008714321884326637}, {"id": 482, "seek": 286644, "start": 2873.16, "end": 2877.0, "text": " to the point where the AI can fill in a lot of them, and that will make proof formalization a lot", "tokens": [50700, 281, 264, 935, 689, 264, 7318, 393, 2836, 294, 257, 688, 295, 552, 11, 293, 300, 486, 652, 8177, 9860, 2144, 257, 688, 50892], "temperature": 0.0, "avg_logprob": -0.15560222366481152, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0008714321884326637}, {"id": 483, "seek": 286644, "start": 2877.0, "end": 2885.96, "text": " faster. So there's a lot of potential. A lot of these technologies are very, very close to prime", "tokens": [50892, 4663, 13, 407, 456, 311, 257, 688, 295, 3995, 13, 316, 688, 295, 613, 7943, 366, 588, 11, 588, 1998, 281, 5835, 51340], "temperature": 0.0, "avg_logprob": -0.15560222366481152, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0008714321884326637}, {"id": 484, "seek": 286644, "start": 2885.96, "end": 2892.76, "text": " time, but not quite. It still took me a month to learn Lean and so forth. They're still not", "tokens": [51340, 565, 11, 457, 406, 1596, 13, 467, 920, 1890, 385, 257, 1618, 281, 1466, 49303, 293, 370, 5220, 13, 814, 434, 920, 406, 51680], "temperature": 0.0, "avg_logprob": -0.15560222366481152, "compression_ratio": 1.6551724137931034, "no_speech_prob": 0.0008714321884326637}, {"id": 485, "seek": 289276, "start": 2892.76, "end": 2897.8, "text": " completely usable out of the box, but they are beginning to be more and more useful. And in", "tokens": [50364, 2584, 29975, 484, 295, 264, 2424, 11, 457, 436, 366, 2863, 281, 312, 544, 293, 544, 4420, 13, 400, 294, 50616], "temperature": 0.0, "avg_logprob": -0.12623772927380483, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.008139020763337612}, {"id": 486, "seek": 289276, "start": 2897.8, "end": 2902.36, "text": " surprising areas, you wouldn't have expected, say, not theory to benefit from these tools,", "tokens": [50616, 8830, 3179, 11, 291, 2759, 380, 362, 5176, 11, 584, 11, 406, 5261, 281, 5121, 490, 613, 3873, 11, 50844], "temperature": 0.0, "avg_logprob": -0.12623772927380483, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.008139020763337612}, {"id": 487, "seek": 289276, "start": 2902.36, "end": 2908.28, "text": " but they do. They can't solve math problems on their own, except maybe undergraduate level", "tokens": [50844, 457, 436, 360, 13, 814, 393, 380, 5039, 5221, 2740, 322, 641, 1065, 11, 3993, 1310, 19113, 1496, 51140], "temperature": 0.0, "avg_logprob": -0.12623772927380483, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.008139020763337612}, {"id": 488, "seek": 289276, "start": 2908.28, "end": 2913.8, "text": " homework questions, maybe, is the current level. But as an assistant, I think they can be very,", "tokens": [51140, 14578, 1651, 11, 1310, 11, 307, 264, 2190, 1496, 13, 583, 382, 364, 10994, 11, 286, 519, 436, 393, 312, 588, 11, 51416], "temperature": 0.0, "avg_logprob": -0.12623772927380483, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.008139020763337612}, {"id": 489, "seek": 289276, "start": 2913.8, "end": 2919.5600000000004, "text": " very useful. They can generate conjectures. They can uncover connections that you wouldn't", "tokens": [51416, 588, 4420, 13, 814, 393, 8460, 416, 1020, 1303, 13, 814, 393, 21694, 9271, 300, 291, 2759, 380, 51704], "temperature": 0.0, "avg_logprob": -0.12623772927380483, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.008139020763337612}, {"id": 490, "seek": 291956, "start": 2919.56, "end": 2929.08, "text": " normally guess. Once proof automation becomes easier and scales better, we may be able to do", "tokens": [50364, 5646, 2041, 13, 3443, 8177, 17769, 3643, 3571, 293, 17408, 1101, 11, 321, 815, 312, 1075, 281, 360, 50840], "temperature": 0.0, "avg_logprob": -0.17051543972708963, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.001215611002407968}, {"id": 491, "seek": 291956, "start": 2929.7999999999997, "end": 2936.92, "text": " completely new types of mathematics that we don't do right now. Right now, we prove", "tokens": [50876, 2584, 777, 3467, 295, 18666, 300, 321, 500, 380, 360, 558, 586, 13, 1779, 586, 11, 321, 7081, 51232], "temperature": 0.0, "avg_logprob": -0.17051543972708963, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.001215611002407968}, {"id": 492, "seek": 291956, "start": 2936.92, "end": 2941.24, "text": " theorems one at a time. I mean, we're still kind of craftsmen. We take one theorem and", "tokens": [51232, 10299, 2592, 472, 412, 257, 565, 13, 286, 914, 11, 321, 434, 920, 733, 295, 27831, 2558, 13, 492, 747, 472, 20904, 293, 51448], "temperature": 0.0, "avg_logprob": -0.17051543972708963, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.001215611002407968}, {"id": 493, "seek": 291956, "start": 2941.24, "end": 2945.08, "text": " we prove it, and we take another thing and we prove it. Eventually, you could automate", "tokens": [51448, 321, 7081, 309, 11, 293, 321, 747, 1071, 551, 293, 321, 7081, 309, 13, 17586, 11, 291, 727, 31605, 51640], "temperature": 0.0, "avg_logprob": -0.17051543972708963, "compression_ratio": 1.6203703703703705, "no_speech_prob": 0.001215611002407968}, {"id": 494, "seek": 294508, "start": 2945.88, "end": 2952.52, "text": " exploring the entire theorem space of millions of different statements, which ones are true,", "tokens": [50404, 12736, 264, 2302, 20904, 1901, 295, 6803, 295, 819, 12363, 11, 597, 2306, 366, 2074, 11, 50736], "temperature": 0.0, "avg_logprob": -0.13398880678064684, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.0012509850785136223}, {"id": 495, "seek": 294508, "start": 2952.52, "end": 2957.16, "text": " which ones are obviously false, and you could explore the geometry of theorems themselves.", "tokens": [50736, 597, 2306, 366, 2745, 7908, 11, 293, 291, 727, 6839, 264, 18426, 295, 10299, 2592, 2969, 13, 50968], "temperature": 0.0, "avg_logprob": -0.13398880678064684, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.0012509850785136223}, {"id": 496, "seek": 294508, "start": 2958.6, "end": 2964.92, "text": " I think we're going to see a lot of different ways to do mathematics, and we're going to", "tokens": [51040, 286, 519, 321, 434, 516, 281, 536, 257, 688, 295, 819, 2098, 281, 360, 18666, 11, 293, 321, 434, 516, 281, 51356], "temperature": 0.0, "avg_logprob": -0.13398880678064684, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.0012509850785136223}, {"id": 497, "seek": 294508, "start": 2964.92, "end": 2971.7999999999997, "text": " see a lot of different ways to make connections in fields that we don't currently. And it'll be", "tokens": [51356, 536, 257, 688, 295, 819, 2098, 281, 652, 9271, 294, 7909, 300, 321, 500, 380, 4362, 13, 400, 309, 603, 312, 51700], "temperature": 0.0, "avg_logprob": -0.13398880678064684, "compression_ratio": 1.8217821782178218, "no_speech_prob": 0.0012509850785136223}, {"id": 498, "seek": 297180, "start": 2971.8, "end": 2975.32, "text": " a lot easier to collaborate and work in different areas of mathematics,", "tokens": [50364, 257, 688, 3571, 281, 18338, 293, 589, 294, 819, 3179, 295, 18666, 11, 50540], "temperature": 0.0, "avg_logprob": -0.1937557195688223, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00013637845404446125}, {"id": 499, "seek": 297180, "start": 2978.6800000000003, "end": 2983.96, "text": " yeah, especially because you can use these tools to sort of compartmentalize all these tasks,", "tokens": [50708, 1338, 11, 2318, 570, 291, 393, 764, 613, 3873, 281, 1333, 295, 26505, 304, 1125, 439, 613, 9608, 11, 50972], "temperature": 0.0, "avg_logprob": -0.1937557195688223, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00013637845404446125}, {"id": 500, "seek": 297180, "start": 2983.96, "end": 2988.92, "text": " all these big, complicated projects into small pieces. And plus, also, these large language", "tokens": [50972, 439, 613, 955, 11, 6179, 4455, 666, 1359, 3755, 13, 400, 1804, 11, 611, 11, 613, 2416, 2856, 51220], "temperature": 0.0, "avg_logprob": -0.1937557195688223, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00013637845404446125}, {"id": 501, "seek": 297180, "start": 2988.92, "end": 2995.2400000000002, "text": " models actually will become very good at getting humans up to speed on any number of advanced", "tokens": [51220, 5245, 767, 486, 1813, 588, 665, 412, 1242, 6255, 493, 281, 3073, 322, 604, 1230, 295, 7339, 51536], "temperature": 0.0, "avg_logprob": -0.1937557195688223, "compression_ratio": 1.5810810810810811, "no_speech_prob": 0.00013637845404446125}, {"id": 502, "seek": 299524, "start": 2995.24, "end": 3005.64, "text": " mathematical topics. Okay, oops. Yeah, but it's still not quite there yet. I would say,", "tokens": [50364, 18894, 8378, 13, 1033, 11, 34166, 13, 865, 11, 457, 309, 311, 920, 406, 1596, 456, 1939, 13, 286, 576, 584, 11, 50884], "temperature": 0.0, "avg_logprob": -0.21036300659179688, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0010956022888422012}, {"id": 503, "seek": 299524, "start": 3006.2, "end": 3010.4399999999996, "text": " if, for example, we prove formalization, it still takes about 10 to 20 times longer to", "tokens": [50912, 498, 11, 337, 1365, 11, 321, 7081, 9860, 2144, 11, 309, 920, 2516, 466, 1266, 281, 945, 1413, 2854, 281, 51124], "temperature": 0.0, "avg_logprob": -0.21036300659179688, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0010956022888422012}, {"id": 504, "seek": 299524, "start": 3010.4399999999996, "end": 3015.8799999999997, "text": " formalize a proof than to do it by hand, but it's dropping. And I see no reason why this", "tokens": [51124, 9860, 1125, 257, 8177, 813, 281, 360, 309, 538, 1011, 11, 457, 309, 311, 13601, 13, 400, 286, 536, 572, 1778, 983, 341, 51396], "temperature": 0.0, "avg_logprob": -0.21036300659179688, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0010956022888422012}, {"id": 505, "seek": 299524, "start": 3015.8799999999997, "end": 3023.3999999999996, "text": " ratio cannot drop below one. Eventually, it will become faster to, eventually, when we just explain", "tokens": [51396, 8509, 2644, 3270, 2507, 472, 13, 17586, 11, 309, 486, 1813, 4663, 281, 11, 4728, 11, 562, 321, 445, 2903, 51772], "temperature": 0.0, "avg_logprob": -0.21036300659179688, "compression_ratio": 1.5252100840336134, "no_speech_prob": 0.0010956022888422012}, {"id": 506, "seek": 302340, "start": 3023.48, "end": 3028.6800000000003, "text": " our proofs to GPT. And GPT will generate, you know, it will ask questions whenever we're unclear,", "tokens": [50368, 527, 8177, 82, 281, 26039, 51, 13, 400, 26039, 51, 486, 8460, 11, 291, 458, 11, 309, 486, 1029, 1651, 5699, 321, 434, 25636, 11, 50628], "temperature": 0.0, "avg_logprob": -0.24527504738796962, "compression_ratio": 1.575, "no_speech_prob": 0.0008250983082689345}, {"id": 507, "seek": 302340, "start": 3028.6800000000003, "end": 3033.2400000000002, "text": " but then it will just generate the latex and the lean for us. And we, you know,", "tokens": [50628, 457, 550, 309, 486, 445, 8460, 264, 3469, 87, 293, 264, 11659, 337, 505, 13, 400, 321, 11, 291, 458, 11, 50856], "temperature": 0.0, "avg_logprob": -0.24527504738796962, "compression_ratio": 1.575, "no_speech_prob": 0.0008250983082689345}, {"id": 508, "seek": 302340, "start": 3033.2400000000002, "end": 3040.84, "text": " eventually, and, you know, and check our work at the same time. So I think this is all in the future.", "tokens": [50856, 4728, 11, 293, 11, 291, 458, 11, 293, 1520, 527, 589, 412, 264, 912, 565, 13, 407, 286, 519, 341, 307, 439, 294, 264, 2027, 13, 51236], "temperature": 0.0, "avg_logprob": -0.24527504738796962, "compression_ratio": 1.575, "no_speech_prob": 0.0008250983082689345}, {"id": 509, "seek": 302340, "start": 3043.08, "end": 3044.44, "text": " All right, so thanks for listening.", "tokens": [51348, 1057, 558, 11, 370, 3231, 337, 4764, 13, 51416], "temperature": 0.0, "avg_logprob": -0.24527504738796962, "compression_ratio": 1.575, "no_speech_prob": 0.0008250983082689345}, {"id": 510, "seek": 305340, "start": 3054.04, "end": 3064.04, "text": " Thank you. That was lovely. I think we have a couple minutes for a few short questions.", "tokens": [50396, 1044, 291, 13, 663, 390, 7496, 13, 286, 519, 321, 362, 257, 1916, 2077, 337, 257, 1326, 2099, 1651, 13, 50896], "temperature": 0.0, "avg_logprob": -0.30135720570882163, "compression_ratio": 1.1458333333333333, "no_speech_prob": 0.005756436847150326}, {"id": 511, "seek": 305340, "start": 3074.28, "end": 3076.04, "text": " Is there a microphone?", "tokens": [51408, 1119, 456, 257, 10952, 30, 51496], "temperature": 0.0, "avg_logprob": -0.30135720570882163, "compression_ratio": 1.1458333333333333, "no_speech_prob": 0.005756436847150326}, {"id": 512, "seek": 308340, "start": 3084.36, "end": 3092.12, "text": " Okay. There will also be a Q&A next door in 204 for a few minutes after when this is over.", "tokens": [50412, 1033, 13, 821, 486, 611, 312, 257, 1249, 5, 32, 958, 2853, 294, 945, 19, 337, 257, 1326, 2077, 934, 562, 341, 307, 670, 13, 50800], "temperature": 0.0, "avg_logprob": -0.17209866841634114, "compression_ratio": 1.5219123505976095, "no_speech_prob": 0.005022103898227215}, {"id": 513, "seek": 308340, "start": 3092.92, "end": 3098.44, "text": " Are we using these mics, or are we calling up? Yes. That's great. I can't see the mic from here.", "tokens": [50840, 2014, 321, 1228, 613, 45481, 11, 420, 366, 321, 5141, 493, 30, 1079, 13, 663, 311, 869, 13, 286, 393, 380, 536, 264, 3123, 490, 510, 13, 51116], "temperature": 0.0, "avg_logprob": -0.17209866841634114, "compression_ratio": 1.5219123505976095, "no_speech_prob": 0.005022103898227215}, {"id": 514, "seek": 308340, "start": 3099.32, "end": 3105.2400000000002, "text": " So the person I called on can ask the question. Okay, sure. So one prediction that some people", "tokens": [51160, 407, 264, 954, 286, 1219, 322, 393, 1029, 264, 1168, 13, 1033, 11, 988, 13, 407, 472, 17630, 300, 512, 561, 51456], "temperature": 0.0, "avg_logprob": -0.17209866841634114, "compression_ratio": 1.5219123505976095, "no_speech_prob": 0.005022103898227215}, {"id": 515, "seek": 308340, "start": 3105.2400000000002, "end": 3112.6800000000003, "text": " have bandied about, about advances with AI-assisted theorem provers is that we might enter a period", "tokens": [51456, 362, 4116, 1091, 466, 11, 466, 25297, 365, 7318, 12, 640, 33250, 20904, 447, 840, 307, 300, 321, 1062, 3242, 257, 2896, 51828], "temperature": 0.0, "avg_logprob": -0.17209866841634114, "compression_ratio": 1.5219123505976095, "no_speech_prob": 0.005022103898227215}, {"id": 516, "seek": 311268, "start": 3112.68, "end": 3118.44, "text": " where there's a proliferation of proofs for new theorems that are formally verifiable,", "tokens": [50364, 689, 456, 311, 257, 24398, 44987, 295, 8177, 82, 337, 777, 10299, 2592, 300, 366, 25983, 1306, 30876, 11, 50652], "temperature": 0.0, "avg_logprob": -0.15160652865534244, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.001769656897522509}, {"id": 517, "seek": 311268, "start": 3118.44, "end": 3121.64, "text": " but which we don't yet have the technology to translate into forms that are easily", "tokens": [50652, 457, 597, 321, 500, 380, 1939, 362, 264, 2899, 281, 13799, 666, 6422, 300, 366, 3612, 50812], "temperature": 0.0, "avg_logprob": -0.15160652865534244, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.001769656897522509}, {"id": 518, "seek": 311268, "start": 3121.64, "end": 3127.72, "text": " comprehensible by humans. Do you see this being an issue? Well, it already happens. You know,", "tokens": [50812, 10753, 30633, 538, 6255, 13, 1144, 291, 536, 341, 885, 364, 2734, 30, 1042, 11, 309, 1217, 2314, 13, 509, 458, 11, 51116], "temperature": 0.0, "avg_logprob": -0.15160652865534244, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.001769656897522509}, {"id": 519, "seek": 311268, "start": 3127.72, "end": 3133.72, "text": " for example, this Boolean Pythagon-Triple's theorem, no human will ever read that proof.", "tokens": [51116, 337, 1365, 11, 341, 23351, 28499, 9953, 392, 559, 266, 12, 51, 470, 781, 311, 20904, 11, 572, 1952, 486, 1562, 1401, 300, 8177, 13, 51416], "temperature": 0.0, "avg_logprob": -0.15160652865534244, "compression_ratio": 1.5172413793103448, "no_speech_prob": 0.001769656897522509}, {"id": 520, "seek": 313372, "start": 3134.68, "end": 3142.4399999999996, "text": " So, I mean, I think it's actually not that scary. I mean, you know, we rely on big numerical", "tokens": [50412, 407, 11, 286, 914, 11, 286, 519, 309, 311, 767, 406, 300, 6958, 13, 286, 914, 11, 291, 458, 11, 321, 10687, 322, 955, 29054, 50800], "temperature": 0.0, "avg_logprob": -0.1520283053619693, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0011931220069527626}, {"id": 521, "seek": 313372, "start": 3142.4399999999996, "end": 3148.68, "text": " computations already in a lot of your mathematics. Of course, we would still want to have a human", "tokens": [50800, 2807, 763, 1217, 294, 257, 688, 295, 428, 18666, 13, 2720, 1164, 11, 321, 576, 920, 528, 281, 362, 257, 1952, 51112], "temperature": 0.0, "avg_logprob": -0.1520283053619693, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0011931220069527626}, {"id": 522, "seek": 313372, "start": 3148.68, "end": 3155.56, "text": " understandable proof, but as the not-example shows, you can take an incomprehensible computer", "tokens": [51112, 25648, 8177, 11, 457, 382, 264, 406, 12, 3121, 335, 781, 3110, 11, 291, 393, 747, 364, 14036, 40128, 30633, 3820, 51456], "temperature": 0.0, "avg_logprob": -0.1520283053619693, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0011931220069527626}, {"id": 523, "seek": 313372, "start": 3155.56, "end": 3161.8799999999997, "text": " proof and still analyze it and extract out from it a human proof. So I think that would be one of", "tokens": [51456, 8177, 293, 920, 12477, 309, 293, 8947, 484, 490, 309, 257, 1952, 8177, 13, 407, 286, 519, 300, 576, 312, 472, 295, 51772], "temperature": 0.0, "avg_logprob": -0.1520283053619693, "compression_ratio": 1.6118143459915613, "no_speech_prob": 0.0011931220069527626}, {"id": 524, "seek": 316188, "start": 3161.88, "end": 3167.08, "text": " the ways you do mathematics in the future is to clarify computer-assisted proofs and make them", "tokens": [50364, 264, 2098, 291, 360, 18666, 294, 264, 2027, 307, 281, 17594, 3820, 12, 640, 33250, 8177, 82, 293, 652, 552, 50624], "temperature": 0.0, "avg_logprob": -0.10005806816948784, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.00048680941108614206}, {"id": 525, "seek": 316188, "start": 3167.08, "end": 3174.84, "text": " human-understandable. Thank you. Can I ask you over there to ask a question? My question is", "tokens": [50624, 1952, 12, 6617, 1115, 712, 13, 1044, 291, 13, 1664, 286, 1029, 291, 670, 456, 281, 1029, 257, 1168, 30, 1222, 1168, 307, 51012], "temperature": 0.0, "avg_logprob": -0.10005806816948784, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.00048680941108614206}, {"id": 526, "seek": 316188, "start": 3176.92, "end": 3182.2000000000003, "text": " kind of speculative, but I wanted to ask your opinion on the rule of human intuition going", "tokens": [51116, 733, 295, 49415, 11, 457, 286, 1415, 281, 1029, 428, 4800, 322, 264, 4978, 295, 1952, 24002, 516, 51380], "temperature": 0.0, "avg_logprob": -0.10005806816948784, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.00048680941108614206}, {"id": 527, "seek": 316188, "start": 3182.2000000000003, "end": 3187.6400000000003, "text": " forward with this, because what a lot of what we talked about is formalization of human intuition", "tokens": [51380, 2128, 365, 341, 11, 570, 437, 257, 688, 295, 437, 321, 2825, 466, 307, 9860, 2144, 295, 1952, 24002, 51652], "temperature": 0.0, "avg_logprob": -0.10005806816948784, "compression_ratio": 1.5889830508474576, "no_speech_prob": 0.00048680941108614206}, {"id": 528, "seek": 318764, "start": 3187.64, "end": 3192.3599999999997, "text": " into formal mathematics. I was wondering if you think that intuitive part of coming up with the", "tokens": [50364, 666, 9860, 18666, 13, 286, 390, 6359, 498, 291, 519, 300, 21769, 644, 295, 1348, 493, 365, 264, 50600], "temperature": 0.0, "avg_logprob": -0.1022921880086263, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0007638680399395525}, {"id": 529, "seek": 318764, "start": 3192.3599999999997, "end": 3200.52, "text": " idea for the proof itself could be automated in the near future? Not in the near future. As I said,", "tokens": [50600, 1558, 337, 264, 8177, 2564, 727, 312, 18473, 294, 264, 2651, 2027, 30, 1726, 294, 264, 2651, 2027, 13, 1018, 286, 848, 11, 51008], "temperature": 0.0, "avg_logprob": -0.1022921880086263, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0007638680399395525}, {"id": 530, "seek": 318764, "start": 3200.52, "end": 3206.6, "text": " these likely models can generate things that resemble intuition, but it has a lot of garbage.", "tokens": [51008, 613, 3700, 5245, 393, 8460, 721, 300, 36870, 24002, 11, 457, 309, 575, 257, 688, 295, 14150, 13, 51312], "temperature": 0.0, "avg_logprob": -0.1022921880086263, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0007638680399395525}, {"id": 531, "seek": 318764, "start": 3208.52, "end": 3213.96, "text": " At the very low level of proving like one or two steps in a proof, we can use these proof", "tokens": [51408, 1711, 264, 588, 2295, 1496, 295, 27221, 411, 472, 420, 732, 4439, 294, 257, 8177, 11, 321, 393, 764, 613, 8177, 51680], "temperature": 0.0, "avg_logprob": -0.1022921880086263, "compression_ratio": 1.6550218340611353, "no_speech_prob": 0.0007638680399395525}, {"id": 532, "seek": 321396, "start": 3213.96, "end": 3223.0, "text": " assistants to sort of only pick out the good intuition and discard the bad one. But what", "tokens": [50364, 34949, 281, 1333, 295, 787, 1888, 484, 264, 665, 24002, 293, 31597, 264, 1578, 472, 13, 583, 437, 50816], "temperature": 0.0, "avg_logprob": -0.1755926664485488, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.002139569725841284}, {"id": 533, "seek": 321396, "start": 3223.0, "end": 3229.0, "text": " large numbers are terrible at right now is differentiating good ideas from bad ideas.", "tokens": [50816, 2416, 3547, 366, 6237, 412, 558, 586, 307, 27372, 990, 665, 3487, 490, 1578, 3487, 13, 51116], "temperature": 0.0, "avg_logprob": -0.1755926664485488, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.002139569725841284}, {"id": 534, "seek": 321396, "start": 3229.0, "end": 3234.52, "text": " They just generate ideas. So unless there's another breakthrough in AI, I don't think this is going", "tokens": [51116, 814, 445, 8460, 3487, 13, 407, 5969, 456, 311, 1071, 22397, 294, 7318, 11, 286, 500, 380, 519, 341, 307, 516, 51392], "temperature": 0.0, "avg_logprob": -0.1755926664485488, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.002139569725841284}, {"id": 535, "seek": 321396, "start": 3234.52, "end": 3240.2, "text": " to happen any time soon. We'll take one more question from this side. So I was curious about", "tokens": [51392, 281, 1051, 604, 565, 2321, 13, 492, 603, 747, 472, 544, 1168, 490, 341, 1252, 13, 407, 286, 390, 6369, 466, 51676], "temperature": 0.0, "avg_logprob": -0.1755926664485488, "compression_ratio": 1.5291666666666666, "no_speech_prob": 0.002139569725841284}, {"id": 536, "seek": 324020, "start": 3240.2, "end": 3246.7599999999998, "text": " the need for blueprints. Is it that the system doesn't know enough definitions yet or is the proof", "tokens": [50364, 264, 643, 337, 888, 23547, 47523, 13, 1119, 309, 300, 264, 1185, 1177, 380, 458, 1547, 21988, 1939, 420, 307, 264, 8177, 50692], "temperature": 0.0, "avg_logprob": -0.09989930203086451, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.008758130483329296}, {"id": 537, "seek": 324020, "start": 3246.7599999999998, "end": 3253.24, "text": " space too big? Some combination thereof? No, it's more of an organization for the humans. If you", "tokens": [50692, 1901, 886, 955, 30, 2188, 6562, 456, 2670, 30, 883, 11, 309, 311, 544, 295, 364, 4475, 337, 264, 6255, 13, 759, 291, 51016], "temperature": 0.0, "avg_logprob": -0.09989930203086451, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.008758130483329296}, {"id": 538, "seek": 324020, "start": 3253.24, "end": 3260.68, "text": " want to coordinate 20 people to work on the same project, many of the people who work on these", "tokens": [51016, 528, 281, 15670, 945, 561, 281, 589, 322, 264, 912, 1716, 11, 867, 295, 264, 561, 567, 589, 322, 613, 51388], "temperature": 0.0, "avg_logprob": -0.09989930203086451, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.008758130483329296}, {"id": 539, "seek": 324020, "start": 3260.68, "end": 3265.8799999999997, "text": " projects, they don't understand the whole proof. So you need some structure to split it up into", "tokens": [51388, 4455, 11, 436, 500, 380, 1223, 264, 1379, 8177, 13, 407, 291, 643, 512, 3877, 281, 7472, 309, 493, 666, 51648], "temperature": 0.0, "avg_logprob": -0.09989930203086451, "compression_ratio": 1.6083333333333334, "no_speech_prob": 0.008758130483329296}, {"id": 540, "seek": 326588, "start": 3265.88, "end": 3271.7200000000003, "text": " really small pieces, atomic pieces that are self-contained for individual people to work on.", "tokens": [50364, 534, 1359, 3755, 11, 22275, 3755, 300, 366, 2698, 12, 9000, 3563, 337, 2609, 561, 281, 589, 322, 13, 50656], "temperature": 0.0, "avg_logprob": -0.14261179956896552, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.0012008583871647716}, {"id": 541, "seek": 326588, "start": 3271.7200000000003, "end": 3277.7200000000003, "text": " So it's not for the computer. The computer can compile anything. It's for the humans to", "tokens": [50656, 407, 309, 311, 406, 337, 264, 3820, 13, 440, 3820, 393, 31413, 1340, 13, 467, 311, 337, 264, 6255, 281, 50956], "temperature": 0.0, "avg_logprob": -0.14261179956896552, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.0012008583871647716}, {"id": 542, "seek": 326588, "start": 3277.7200000000003, "end": 3282.92, "text": " break up a complicated problem into lots of easy pieces. Kind of like how divisional labor works", "tokens": [50956, 1821, 493, 257, 6179, 1154, 666, 3195, 295, 1858, 3755, 13, 9242, 295, 411, 577, 25974, 1966, 5938, 1985, 51216], "temperature": 0.0, "avg_logprob": -0.14261179956896552, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.0012008583871647716}, {"id": 543, "seek": 326588, "start": 3282.92, "end": 3288.6800000000003, "text": " in like modern industries, like factories. Okay, I'm going to invite all the other people", "tokens": [51216, 294, 411, 4363, 13284, 11, 411, 24813, 13, 1033, 11, 286, 478, 516, 281, 7980, 439, 264, 661, 561, 51504], "temperature": 0.0, "avg_logprob": -0.14261179956896552, "compression_ratio": 1.5887445887445888, "no_speech_prob": 0.0012008583871647716}, {"id": 544, "seek": 328868, "start": 3288.68, "end": 3295.96, "text": " waiting to ask questions to join us in room 204 briefly. And let's thank Terry for a lovely talk.", "tokens": [50364, 3806, 281, 1029, 1651, 281, 3917, 505, 294, 1808, 945, 19, 10515, 13, 400, 718, 311, 1309, 21983, 337, 257, 7496, 751, 13, 50728], "temperature": 0.0, "avg_logprob": -0.22677705838130072, "compression_ratio": 1.0210526315789474, "no_speech_prob": 0.19561061263084412}], "language": "en"}