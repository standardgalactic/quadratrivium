Hello. Hello-hello. Good afternoon. I'm Bryna Kraw, President of the AMS, and it's my pleasure
to welcome you to the colloquium lectures. These are the oldest lectures at the meetings
of the AMS. The first meeting was held in 1895. The second in 1896 were the first colloquium
lectures actually took place, and those were at Northwestern University, and since that's
my home institution, I can't help but mention them. The list of speakers is a veritable
who's who in mathematics, including Birkhoff, Morse, von Neumann, Tarski, Czern, Milner,
Smell, Nirenberg, Tate, and the list of people I've left off is, well, equally prestigious
to those that I included, and amongst them was also our speaker today's advisor, Stein,
Ilya Stein. So with that, I'll turn to the introduction of Terry Tau. Terry may be somebody
who doesn't need introduction. After all, he's been in a crossword puzzle as a clue
in the New York Times crossword puzzle. I don't want to use up all his time by listing
the awards he's won, but I could. I'll give you just a short list of the highlights of
the Fields Medal in 2006. I'm a Carther Fellowship. He's a fellow of the Royal Society, the Australian
Academy of Sciences, the American Academy of Arts and Science, and a member of the National
Academy of Science, and of course he's a fellow of the AMS, most important of those
distinctions. He has over 350 publications, meaning he hasn't slept in a few years, and
this includes numerous highly influential texts, and he has more than 50 collaborators,
and maybe that's when I counted last week, so I don't know, maybe this week there's
more. He's also one of the broadest researchers in mathematics, covering interests from pure
to applied, and I won't list all of the subjects. But it's not only research, he also serves
the profession in numerous ways, and starting in 2021 took on a very substantial role. He
was appointed by President Biden as a member of the President's Council of Advisors on
Science and Technology. So he's a force. He's mentored over 20 PhD students, and I could
go on and on. So I've known Terry for a while, since about 2004 or 2005 when we first met,
but one of my fondest memories of him was at a party that I was a co-host for. In 2008,
it was the election night, and Terry was sitting in the corner November 2008 on his computer on
some website that was giving election returns, announcing the states before the TV was. It
was really impressive, and it's just one of my fondest memories. Anyway, I won't keep you any
longer. It's my pleasure to introduce Terry.
All right. Thank you very much, Bryna. I'm very happy to be here at this JMM to give this lecture.
It's always nice to be up in San Francisco. So are we talking about what I think is a really
exciting development in mathematics that's going to shape our future, which is really the
development over the last few years of lots of technologies to make machines and computers
help us do math much more effectively. Now, to some sense, this is not new. We have used
both machines and computers, and I use the terms slightly differently. We've used both,
actually, for centuries, really. Nowadays, computers and machines, when we talk about
machine-assisted mathematics and computer-assisted mathematics, they're sort of synonymous. Initially,
they weren't because computers used to be human, and then they were mechanical and then finally
electronic. So, for example, one of the earliest use of computers was to build tables. So, for
example, the large tables of Napier and so forth were basically built by lots and lots of human
computers, and those are the earliest examples of somehow computer-assisted mathematics,
and tables are still really important today. I mean, not so much the log tables anymore,
but a lot of what we call experimental mathematics is based on creating lots and lots of large
tables of interesting things, especially in number theory. So, for example, famously,
the Gendo and Gauss built tables of prime numbers, and they used that to conjecture the prime number
theorem, which is, of course, now a theorem. And similarly, in the 60s, Bertrand's Wittendeyer
created lots and lots of big tables of the decurs and the ranks and so forth, and this was a key
input in formalizing the famous BSD conjecture. And maybe the biggest table of all in mathematics
is the OEIS, online encyclopedia of mathematical sequences. So, there's hundreds of thousands
of integer sequences, and every day, I think mathematicians discover unexpected connections,
or maybe rediscover an existing connection. I myself use the OEIS. If there's a quantity,
which I know there's a formula for, but I can't remember it, I can just compute the first five
elements put in the OEIS, I can usually find it. And most recently, people are starting to use
large databases of mathematical objects as training data for neural networks, and so I'll
give you an example of that later. So, that's one very storied and antique way of using computers
in mathematics. The other big use, of course, is in numerics or scientific computing,
and that's also a very old subject. Arguably, the first big scientific computation was in the
20s when Lorenz was asked to model the fluid flow for the construction of a new dike in the
Netherlands, and so he assembled a team of human computers, basically, to model what would happen
to the water flow and so forth. It's notable for the introduction, he's almost the first place
where floating point arithmetic was introduced. And, of course, we use scientific computing
nowadays to model PDEs, to solve large systems of equations, and, of course, we use them for
computer algebra packages, you know, magma, maple, sage, and so forth. Yeah, you want to do a big,
you know, numerical integration or algebraic, you know, computer Gribner bases, whatever,
you know, we routinely do this now, or cross mathematics. Of course, the numerics are sometimes
inaccurate, you know, there are round off errors and other possible problems, but there are ways
to make the combination more rigorous. For example, instead of floating point arithmetic,
if you use interval arithmetic, if you represent numbers by error ranges, a lower and upper bound,
and you keep those bounds like rational numbers, like finite precision, like infinite precision,
then you can avoid errors, at least in principle, at the cost maybe of
making the runtime longer. More recently, there are more advanced
algebra packages than just sort of the standard things you get in sage or Mathematica.
They think of SAT solvers, satisfiability solvers, or satisfiably modulo theory solvers,
SMT solvers. So what they, so a SAT solver gives, you feed it a whole statement, a bunch of
propositions, P1, P2, P3, and so forth, and you see that P1 and P2, all P3 is true, P3 and P4,
and not P5, one of them is true, and so forth, and it will try to see if there's a solution or not,
and many problems can be phrased as a SAT problem. So if you have a general purpose SAT solver,
you can potentially just feed it into such a program and solve the problem for you,
then there are these more sophisticated variants, SMT solvers, where you also feed laws of algebra
so you have some variables, and you assume that there are certain laws, these variables
obey certain laws, and you ask, can you deduce a new law from the laws you already have?
So those are potentially very powerful, and unfortunately, SAT solverability is an NP
complete problem, and once you get hundreds and hundreds of these propositions, it becomes very
hard to actually solve these problems, but still they are very useful. Here's a typical
application of a SAT solver, so a few years ago, there was this famous problem in commentaries
called the Boolean Pythagorean Triples problem, and so the problem is this, you take natural numbers
and you color them into two color classes, red and blue, and you ask, is it always the case that
one of these color classes contains a Pythagorean triple, three numbers A, B and C such that A
squared plus B squared equals C squared, and it turns out to be, we don't have like a human proof
of the statement, but we know it's true now because of a SAT solver, so there was a massive
computation that says that if you only go up to 7,824, then you can't do this, there was a way
to partition the numbers from 1 to 7,824 into two classes, neither of which contain a Pythagorean
triple, but once you go up to 7,825, no matter how you do it, you must always get one of the
two color classes must have a Pythagorean triple. In principle, this is a finite computation
because there's only two to seven, two to five different ways to compute different partitions,
and so you just check each one, but that is computationally unfeasible, but with a SAT
solver, you can rephrase this problem as a free satisfiability problem, and it's not just a matter
of running the solver, you have to optimize it and so forth, but it is possible to actually solve
this problem, and it gives you a certificate, it gives you a proof, and actually this is,
at the time, it was actually the world's longest proof. The proof certificate, first of all,
it took four CPU years to generate, and it's a 200 terabyte proof, although it is compressible.
I think it is still the second largest proof ever generated.
Okay, so that's, but this I still consider is a more classical way of using computers,
but what I think is exciting is that there are a lot of new ways that we can use computers
to do mathematics. Of course, there's still the boring ways, you know, we use computers to do
emails and write latex and so forth, I don't mean that, but there are sort of three new modalities,
which individually, they still have somewhat niche applications, but what I find really
interesting is that they can potentially be combined together, and the combination of them,
it could be something in general purpose that actually a lot of us could use. So the three sort
of new things. So the first is machine learning algorithms, where you have a problem, and if
you have a lot of data for that problem, you can set some sort of specialized neural network to
train it on the data, and it can generate counter examples for you, try to generate connections,
and so people are beginning to use this in all kinds of fields of mathematics, I'll give you
some examples later. So that's one development. Maybe the most high-profile development is
large language models like chat GPT, these are very general purpose models that can understand
natural language. To date, they have not been directly used for so much mathematics, I'm sure
many of you have tried talking to GPT, asking it to solve your favorite math problem, and it will
give you some so plausible looking nonsense in general. But when used correctly, I think they
do offer a lot of potential. I mean, I have found occasionally that these models can be useful for
suggesting proof techniques that I wasn't initially thinking of, or to suggest related topics or
literature. They're actually most useful for sort of secondary tasks. Okay, so for actually doing
math research, they still haven't really proved themselves, but for doing things like writing
code or organizing a bibliography, like a lot of the other more routine tasks that we do actually,
these LLMs are very useful. But the third new technology, which has been around for two decades,
but has only now sort of becoming ready for primetime, are these formal proof assistants,
which are languages designed to verify, or to verify, or many of them are actually designed
to verify electronics, but they can also verify mathematical proofs. And crucially, they can
also verify the output of large language models, which they can complement, they can fix the biggest
defect in principle of the LLMs. And they allow new types of ways to do mathematics, in particular,
they can allow really large scale collaborations, which we really can't do without these formal
proof assistants. And they can also generate data, which can be used for the other two
the other two technologies. So I'll talk about each of these three things separately, but
but they haven't, there's beginning to be experiments to combine them together,
but they're still kind of prototypes right now. But I think the paradigm of using all of them,
and also combining with the computer algebra systems and the SAP solvers into one integrated
package, it could really be quite a powerful methodical assistant. Okay, so let's talk,
I think my first slides begin with proof assistants. So the computer-assisted proofs are not new,
famously the full color theorem in 1976 was was proven partly by computer. Although at the time,
it was by modern standards, we will not call it a fully formalized proof, the 1976 proof.
The proof was this long document with lots and lots of subclaims, which
a lot of them were verified by hand, and a lot of them were verified by both electronic computers
and human computers. And I think one of the author's daughter actually had to go through 500 graphs
and check that they all had this discharging property. And actually, it had a lot of mistakes too.
So there's a lot of minor errors in the proof. They're all correctable, but it really
will not meet the standards today of a computer-verified proof.
The first proof, okay, so it took 30 to 20 years for an alternative proof of full color theorem
to be verified, and this proof is closer to being completely formal. So it's about 10-15 pages
of human readable argument, and then it reduces to this very specific computation, which anyone
can just run a computer program in whatever language they like to verify it. So it was a
computer-verified proof, but it still wasn't a formal proof. It wasn't written in a formal
proof language, which was designed to only output correct proofs. And that had to wait until the
early 2000s when Werner and Gontier actually formalized the entire full color theorem in one
of the early proof assistant languages, COC, in this case. So now we know with 100% certainty
that the full color theorem is correct. Well, modulo, trusting the compiler of COC.
All right. Another famous machine assistant proof, well, actually initially human proof,
but eventually computer-verified was the proof of the coupler conjecture. So the coupler conjecture
is a statement about how efficient that you can pack unit spheres in the plane,
and so there's a natural way to stack unit spheres, and it's the way that you see oranges stacked in
the grocery store. It's called the hexagonal closed packing, and there's also a dual packing
with the same density called the cubic closed packing. And they have a certain density, pi over
three over two, and this was conjectured to be the densest packing. So this is an annoyingly hard
statement to prove. So it's an optimization problem in infinitely many variables. So each
each sphere has a different location, and there's an infinite number of spheres.
So you're trying to prove an inequality involving an infinite number
of variables involving solving infinite number constraints. So it doesn't immediately
lend itself to computer verification. But even in the 50s, it was realized that possibly this
could be done by some sort of brute force computation. And so Toth proposed the following
paradigm. So every time you see a packing, it comes with what's called a Voronoi decomposition.
So every sphere comes with a Voronoi cell, which is this polytope of all the points that are closer
to the center of that of that sphere than to all the other spheres. And there's partitions
space into all these little polyhedron, these Voronoi cells. And there are various relationships
between the volumes of these different cells. There's only so many spheres that you can pack
next to one reference sphere, and this creates all these kind of constraints.
And so the hope was that if you could gather enough inequalities between adjacent Voronoi cells,
the volumes of adjacent Voronoi cells, maybe every such system inequalities, in principle,
gives you an upper bound on the density of the sphere packing. And in principle, if you get
enough of these inequalities, maybe you could actually get the optimal bound of three or two.
So people tried this approach for many, many years, including some false attempts.
But they were not able to actually make this approach work.
But Thomas Hales, and then later, with a collaborator, was able to adapt this approach
to make it work in a series of papers from 94-98. But they had to modify the strategy quite a lot.
So instead of using the Voronoi decomposition, there was a more complicated decomposition
that was used. And instead of using volume, they had to define this new score function
attached to each polyhedron. But basically, the strategy was the same.
And he was able to prove lots and lots of linear inequalities between the scores of adjacent
polyhedra. And then just using linear programming was able to then get a bound. And
with the right choice of score and the right choice of partition, it was the optimal bound.
It's a very flexible method, because you have lots of ways you can do the partition and lots
of ways that you can do the score. But the problem is that it was too flexible. So here's a quote
from Hales. It says that Simon Ferguson, who was Hales' collaborator, and I realized every time we
encounter difficulty solving the minimization problem, we get just a scoring function to
score the difficulty. The function became more complicated, but with each change,
we could cut months or years from our work. This incessant fiddling was unpopular with my
colleagues. Every time I presented my work in progress at a conference, I was minimizing a
different function. Even worse, the function was moderately incompatible with earlier papers,
and this required going back and patching the earlier papers. So the proof was a mess, basically.
They did eventually finish it in 98, and they were able to derive the Kepler conjecture from
a linear programming computation from a very complicated optimization program. Initially,
it was done by hand, but with the increased complexity, there was no choice but to make
it more and more computer-assisted. So when the proof was announced, it was a combination of
250 pages of notes and lots and lots of gigabytes of programs and data, and it was famously hard
to referee. It took four years for annals to referee the paper with a panel of pro referees,
and even then, the panel was only 99 percent certain of the correctness of the proof, and they
could certify the corrections, the calculations. Because, I mean, in principle, it was all doable,
but the referees have to implement all these different computer calculations themselves,
but it was eventually accepted. But clearly, there was this big asterisk. There was a lot of
controversy about whether this was really a valid proof, and so this was one of the
the first really high-profile uses of formal proof assistance, because this was a result in which
there was serious doubt about the correctness. So they created, so Hales in 2003 initiated a
project to write down this massive proof in a completely formalized way so that a standard
proof assistant could verify it. He estimated it would take 20 years to make this work,
and so he had, he gathered 21 collaborators. It actually only took 11 years,
but yeah, eventually what they did was that they first created a blueprint,
you know, so a human readable version of the proof breaking things up into very,
very small steps, and then they formalized each step by bit, and it was finally done,
and then there was a, yeah, so they published a paper about the formalization, and that only
appeared in 2017. So this was sort of the state-of-the-art of formalization, you know,
as of say 20 years ago, you know, like it was possible to formalize big complicated results,
but it took an enormous amount of effort, you know, not something which you would do routinely.
There was a more recent effort in a similar spirit by Peter Schultzer,
he called it the liquid tensor experiment. So Schultzer introduced this theory of condensed
mathematics, which is, all right, this is really far from my own area of expertise, but
basically there are certain problems with, so certain types of mathematics you want to work
in various categories, like categories of topological obedient groups and topological vector spaces,
and there's a problem that they're not obedient categories, that they don't, they don't have,
I have a good notion of kernel and cold kernel, and things don't work out properly.
So he proposed replacing all of these standard categories with a more fancy version called
a condensed category, which has better category theoretic properties, and so the hope is that
you could use a lot more high-powered algebra to attack, to handle things with topological
structure or analytical structure, like function spaces, for example, binoc spaces.
But in order for you to work, there's a certain vanishing theorem, which I've written there,
but I cannot explain to you. Okay, so there's a certain category,
condescending groups, and there's an x-group involving p-binoc spaces that has to vanish,
and this vanishing theorem is needed in order for all of the rest of the theory to actually be
useful, if you want to apply it to function analysis in particular. And so Schultzer,
what a blog post about this is that, you know, I spent much of 2019 obsessed with the proofless
theorem, almost getting crazy over it. In the end, we were able to get an argument down on
paper, but I think no one else has ever dared to look at the details of this, and so I still
have some lingering doubts. With this hope, with this theorem, the hope that the condensed formulas
can be fruitfully applied to function analysis stands a force, being 99% sure is not enough
because this theorem is of the most fundamental importance. He says, I think this may be my most
important theorem to date, which is a really big claim, actually. Better be sure it's correct.
So this was another case where there was a great desire to formalize the proof.
So he asked publicly on the internet for help to formalize this in a modern
proof of this in language called Lean. And so again, about 20 people, I think, joined this
project to help out. And it, so Lean is based on, it has this philosophy where it has this
single huge library of mathematical theorems, like the fundamental calculus or the classification
of finite building groups, like a lot of the basic theorems of mathematics are already formalized
in this big library. And the idea is to just keep building on this library and adding to it
with additional projects. But one of the problems with that short of the face was that a lot of the
tools that, basic tools that you needed, like homological algebra and chief theory and top
box theory, weren't actually in the library yet. So actually part of what they had to do was actually
set up the foundations of that theory and formalize it in the library first. But it basically was
done in about two years. In one year, they formalized a key sub theorem. And then the whole
thing was formalized about a year afterwards. It did have some, in addition to sort of making
reassuring Peter that it was, everything was correct, there was some other minor benefits.
So first of all, there were actually a few minor errors in the proof that were discovered
in the formalization progress. But they go refixed also some small simplifications.
There was one big simplification actually. So they, the original proof used something very
complicated, which I also don't understand, called the Breen-Deline resolution. But in the course
of formalizing it, it was too hard to actually formalize this theory. But they found that there
was a weaker version of this theory, which was good enough to formalize this application. But
this was actually a major discovery because this weaker theory could also potentially attack
some other problems that the Breen-Deline resolution is not well suited for. And now
the math library is much, much better in, it has a lot of support for homological
algebra and sheath theory and lots of other things, which helped other formalization projects
become easier. I got interested in this formalization a few months ago.
So, oh, hang on. I should say, like with the Kepler experiment, so you don't just directly
transfer a paper from, you know, the archive or something into Breen. What we found is that
it really helps to create first an intermediate document. So halfway between a human readable proof
and a formal proof, which we call a blueprint. So it looks like a human proof and is written in a
version of latex. But like each step in the proof is linked to a lemma in lean. And so it's very
tightly integrated. It has a very nice feature. It can create dependency graphs, which I'll show you
an example later. You can see which parts of the proof have been formalized, which ones are not,
and what depends on what. It actually, it gives a lot of structure to the process of writing a paper,
which, you know, right now we do by hand without sort of much assistance, really.
Yeah, Schultz has said that the process of writing the blueprint really helped him understand the
proof a lot better. And actually, people have been also going the other way. There's also software
that takes formal proofs, which are written in something that looks like computer code,
and turns them back into human readable proofs. Here is a prototype software. So there's a
there's a theorem in topology. Okay, you know, I think it's a if a function is continuous on a dense
set, and there's some extra extra properties, then it's continuous extends to a continuous
function globally. And the proof is written was written first in lean, but then it was
already converted into a human proof, human readable proof. But again, where every step
you can expand and contract, like if there's a step you want to explain more, you can double
click it, and it will expand out, give all the justification. And you can click at any given
point in the proof, and it will tell you what the hypotheses are currently, what you can approve,
and you can give a lot of structure. I think eventually textbooks, this is maybe a good format,
say for undergraduate textbooks to have, you know, proofs of say, you know, tricky
films and analysis or something written in a way where, you know, in a not in a static way,
but where you can really, you know, drill down all the way down to the basic axioms if you wanted to.
Okay. One thing I mean, one thing notable about these formalization projects is that they allow
massive collaborations. So, you know, in other sciences, people collaborate with 20 people,
500 people, you know, 5,000 people. But in mathematics, still, we don't really collaborate
in really large groups. You know, five is kind of the maximum, usually. And part of it is that,
you know, if you want to collaborate with 20 people, if you don't already know these 20 people,
and you don't completely trust that they're doing correct mathematics, you know, it's very difficult
because you have to, everyone has to check everyone else's contribution. But with a proof
assistant, the proof assistant provides a formal guarantee. And so, you can really collaborate
with 20, hundreds of people that you've never met before. And you don't need to trust them.
Because they upload code and the lean compiler verifies, yes, this actually solves what they
claimed. And then you can accept it or it doesn't. So, I experienced this myself because I got
interested in formalization a few months ago. So, I'd recently proven a combinatorial theorem
with Tim Gowes, Ben Green, and Freddie Manners. It solved something called the polynomial
primonrugia conjecture. The precise conjecture is not important for this talk. It's a conjecture
in combinatorics. You have a subset of a finite through vector space of small doubling. And you
want to show that it is all, it is very close to actually being a coset of a subgroup. This was a
statement that was in combinatorics that was highly sought after. So, we had a 33-page paper
recently proving this. Mostly self-contained, fortunately. So, I thought I was a good candidate
for formalization. So, I asked on an internet forum, specializing in lean, for help to formalize it.
And then, again, like 20, 30 people joined in and actually only took three weeks to formalize.
The time taken to formalize these projects is going down quite a bit.
So, in particular, the time taken to formalize this project was roughly about the same time
it took for us to write the paper in the first place. And by the time we submitted the paper,
we were able to put as a note that the proof is actually being formalized.
So, as I said, it uses a single blueprint which splits up the proof into lots and lots of little
pieces. And so, it creates this nice little dependency graph. So, this is a picture of
the graph at an early stage of the project. So, there's no down the bottom called PFR. Maybe
only the people in the front can see it. That's the final theorem that we're trying to prove.
At the time of the screenshot, this was white, which means that it had not been
proved, but not even been stated properly. So, in fact, even before you prove the theorem,
you have to first state it, and then you have to make definitions and so forth.
Blue are things that have been defined, but not yet proven, and green have been things that have
been proven. And so, at any point in time, some bubbles are white, some are blue, and some
results depend on some others. And so, the way the formalization proceeded was that different
people just grabbed a note that was ready to be formalized because maybe all the previous results
that it depended on were formalized, and they just formalized that one step independent of
everybody else. And you didn't really need to coordinate too much. I mean, we did coordinate
on an internet forum, but each little note is completely specified. There's a very precise
definition and a very precise thing to prove, and we just need the proof. And we really don't care
exactly what the proof is. I mean, it has to be not massively long. So, people just picked up
individual claims. Here's one very simple claim. There's a certain functional called
ruja distance d, and there's a very simple claim that it was non-negative. And this turns out to
have a three-line proof, assuming some previous facts that were also on the blueprint. And so,
people just sort of picked up these things separately, and over time, it just filled up.
This is what a typical step in the proof looks like. This is the proof that this ruja distance
is non-negative. This is the code in the lean. It looks kind of a little bit like mathematics,
but it is actually... Once you think of the syntax, it actually reads... It's like reading
latex. The first time you read latex, it looks like a whole bunch of mess of symbols. But
it's actually... Every line corresponds to a sentence in mathematical English. For example,
the first line, if you want to prove that this distance is positive,
it suffices to prove that tricidistance is positive. So, you work with tricidistance,
because it turns out there's another lemma that bounds tricidistance. So, yeah, every step,
actually, it corresponds reasonably well to the way we think about mathematical proofs.
Yeah. So, fortunately, the proof didn't reveal any major issues. There were some typos,
but very, very minor picked up. And we also... There were some things we needed... Again,
we needed to add to the math library. The math library... We used Shannon entropy,
and Shannon entropy was not in the math library. Now it is. One thing about formalization is that
it still takes longer to formalize proofs than to write them. But... And maybe about 10 times
longer, I would say. Which is unfortunate. If it was faster to formalize to write proofs formally
than to write them by hand, I think then there will be a tipping point. And then I think a lot
of us would switch over just because they guarantee a correctness. So, we're not there yet. It is
definitely getting a lot faster than it was before. But one thing that we noticed is that,
actually, while it still takes a long time to write a proof, if you want to modify a proof
to change some parameters and make it a little bit better, that can be done much quicker in the
formal setting than with a paper proof. Because with paper proof, if you change all your little
parameters and so forth, you'll likely make also mistakes and you go back. And it's a very annoying
process. But it's actually much easier to modify an existing formal proof than to create one from
scratch. For example, we were able... There's a constant 12 exponent that appeared in our proof.
A few days afterwards, someone posted an improvement in the argument, improved 12 to 11,
and in a few days we were able to adapt the formal proof to do that as well.
Yeah. And because you can collaborate, I think the process is scalable. There was just recently Kevin
Buzzard, for example, a five-year project. The aim is to formalize Fermat's last theory of the proof.
And that is quite a big goal because there are lots and lots of sub-portions that have had no
formal proof at all. So, that, I think, will be quite an ambitious project. But I think it's now
doable. It wasn't doable five years ago, but now I think it is. Okay. So, that's four more proofs.
Another... Okay. I might actually have to speed up a little bit. Okay. So,
all right. So, machine learning, using neural networks has become also more and more common
place in different areas of mathematics. I think I'll skip over this one. So,
one place where it is being used is in PDE to construct candidate approximate solutions for
various problems. So, like... So, there's a famous problem in fluid equations, you know,
do the Navier-Stokes equations before we find our time? Do the Euler equations before we find our
time? Navier-Stokes is still challenging, but Euler, there's been a lot of progress recently
that people have been starting to construct self-similar solutions to the Euler equations
with some asterisks, but the asterisks are slowly being removed. And one of the strategies of proof
is to first construct an approximate solution, approximately self-similar solution that looks
like it's going to blow up and then use some rigorous perturbation theory to perturb that to an
actually blowing up solution. And machine learning has turned out to be useful to actually generate
these approximate solutions, but I'm going to skip that for lack of time. I'll tell you,
my favorite application of machine learning to mathematics is in knot theory. So, this is a
recent work. So, knots are a very old subject in mathematics, and there's lots of... One of the
fundamental things you study in knots is knot invariance. So, there's various statistics you
can study. You can assign to a knot, which depends on the topology of the knot or the geometry of the
knot. So, there's something called the signature, which is a combinatorial invariant. There are
these famous polynomials, like the Jonas polynomial and Homfly polynomial. Then there are these things
called hyperbolic invariance. The complement of a knot is often a hyperbolic space, and then you
can talk about the volume of that space and some other geometric invariance. And so, there are
these massive databases of knots. You can generate millions of knots, and you can compute all these
invariance. But they come from very different areas of mathematics. So, some knot invariance
come from combinatorics, some come from operative algebras, some come from hyperbolic geometry,
and it's not obvious how they're related. But what these mathematicians did was that they trained
a neural network on this big database of knots, and they studied the hyperbolic invariance and
the signature, which is the combinatorial invariant. And they found that you could train the network
to predict the signature from the hyperbolic invariant with, like, 99 percent accuracy.
So, they used, like, half the data to train the set, and then half the data to test the set,
to test the neural network. And so, what this told them is that there must be some connection.
The signature of a knot must somehow be a function, or at least very closely related to a function,
of a hyperbolic invariance. But the problem with neural nets is that they give you a function,
a functional relationship, or at least a conjectural functional relationship. But
it's this massively complicated function. It composes hundreds and hundreds of nonlinear
functions together, and it doesn't give you any insight as to what the relationship is. It just
shows you that there is a connection. But it's possible to do some analysis. So, they actually
tried a very basic thing which happened to work. It's what's called a saliency analysis. So,
this neural network gave them this function. Basically, they fed in 20 hyperbolic invariances
as input, and the signature as output. So, it's basically a function from R to 20 to the integers,
I think. And what they decided to do is that, once they had this function, they just tested
how sensitive the function was to each of its inputs. So, they just varied one of the 20 variables,
and they saw what happened to the output. And what they found was that only three of the 20
variables were actually important, that only three of them had a significant influence on the function.
The other 17 were basically not relevant at all. And so, they focused on those three variables,
and they started plotting this function just restricted to those three variables. And that's
just low enough to mention that you can eyeball the relationships. So, they started plotting
the signature as a function of two or three of these variables and using color and so forth.
And they started seeing patterns, and they could use these patterns to conjecture various
inequalities and various relationships. It turns out that the first few inequalities they conjectured
were false, and they could use the new net and the data set to confirm this. But with a bit of
back and forth, they were able to eventually settle upon a correct conjecture relating these
invariants to the signature. And it wasn't the invariants that they expected. So, yeah, they
were expecting the hypervolume, for example, to be really important and not to be relevant at all.
But once they found the right variables and the right conjectured inequality,
it suggested the proof, and then they were able to actually find a rigorous proof of the inequality
that they conjectured. So, it was a very nice back and forth between using the machine learning
tool to suggest the way forward, but then going back to traditional mathematics to prove things.
Okay. So, of course, the most high-profile development these days has been large language
models like GPT. And sometimes they work really, really well. So, here's an example of GPT-4,
which is OpenAI's most advanced large language model, actually solving a problem from the
IMO, the International Mathematical Olympiad. And so, it's a question, you know, there's a function
that obeys a search function equation. Can you prove, can you solve for the function?
And it actually happens to give a completely correct proof. Now, this is an extremely cherry
picked example. I think they tried, from this paper, they tried all the recent IMO problems,
and they could solve like one percent of the problems of this method. You know, famously,
it's bad even at basic arithmetic. You know, there's a, you ask it to solve seven times four
plus eight times eight, and it'll give you the wrong answer. It gives you 120. And then it will
keep going and say, and I'll explain why. And during the course of the explanation, it will
actually make a mistake. And, yeah, and then you point out that they made a mistake and say,
I'm sorry, the previous answer is incorrect. I mean, these large language models, they remind
me a lot of, you know, if you have sort of a somewhat weak undergraduate student in office hours,
and you ask them to solve a problem at the blackboard with no, with no age, you know,
it will, you know, he or she will try to try their best, you know, and try to turn something
that looks like a proof. But, yeah, they don't really have a good way of correcting themselves.
So, you know, sometimes they work really well, but often they're very, very unreliable.
But there's lots of interesting recent experiments where you can couple these language models to
other much more reliable tools to do mathematics. So, for example, GPT now comes with plugins for
Wolfram Alpha. So, now, if you ask GPT to do an ethnic calculation, it knows better than to try
to do it itself. It will outsource it to Wolfram Alpha. Then there's more recent examples where
people are coupling these large language models to a proof error file like Lean. So, you know,
you ask it to prove as a statement, you know, prove that the union of two open sets is open.
If you ask a raw large language model, it will give you a statement that
a proof that looks like a proof. But there's lots of little logical errors in the proof, but
you can force it to output in Lean, get Lean to compile, and if there's a completion error,
it will send back the error to the large language model and have to correct it and
create a feedback loop. And it can actually be used to solve
roughly sort of undergraduate math homework level problems by this technique. But now,
with 100% guarantee of accuracy, if it works, I mean, of course, sometimes it will just get stuck
and give up because it can never get the Lean compiler to accept the argument. But
it is beginning to make some headway. As I said before, I do find it can be useful
as a muse, kind of like if you're just starting on a project, I recently was trying to
prove some commentary identity. And I was thinking of using, I had some ideas in mind,
I was going to use asymptotics, work with some special cases, but nothing was working. And I
asked GPT for some suggestions. And it gave me some suggestions I was already thinking of,
plus some suggestions that were either completely vacuous or wrong. But it did tell me that you
should probably use generating functions, which I should have known. But at the time,
it escaped me. And just with that hint, I was able to actually get them to work.
So, you know, I mean, as just kind of someone to double check your thought process,
it is sort of useful. Still not great, but it has some potential use.
There was another tool which I do like, and I use more and more now. It's integrated into
something called VS code, which is an editor to write code that can also be used for latex,
something called GitHub co-pilot. It's basically an AI-powered autocomplete,
and you type in your code or your latex, whatever. And based on all the text that you've
already written, it will suggest a possible new sentence to generate. And so it can be very useful
for code. You write down three lines of code, and it's just the fourth. And sometimes you'll get
exactly right. Sometimes it's not exactly right. Sometimes it's complete rubbish. But then you
can accept it, and it can save a lot of time, especially if you're doing some very menial
code where you're repeating something over and over again. It works for latex. I wrote
a latex blog post, actually, recently, where I was trying to estimate an integral, and I broke
up the integral into three pieces. I said, okay, the first piece I can estimate by this technique,
and I wrote down how to estimate this technique. And then the co-pilot just suggested how to
estimate the second term. And actually, a completely correct argument. It was a modification of what
I had already written. So it's very good at just modifying text that you've already appeared.
And it's slowly being integrated into prefixes like Lean. So to the point where one line proves,
two line proves, we can kind of get the AI to fill in for us. The kind of steps that that
this is obvious or clearly this is true in a paper proof. I mean, not all of them, but we can get
to the point where the AI can fill in a lot of them, and that will make proof formalization a lot
faster. So there's a lot of potential. A lot of these technologies are very, very close to prime
time, but not quite. It still took me a month to learn Lean and so forth. They're still not
completely usable out of the box, but they are beginning to be more and more useful. And in
surprising areas, you wouldn't have expected, say, not theory to benefit from these tools,
but they do. They can't solve math problems on their own, except maybe undergraduate level
homework questions, maybe, is the current level. But as an assistant, I think they can be very,
very useful. They can generate conjectures. They can uncover connections that you wouldn't
normally guess. Once proof automation becomes easier and scales better, we may be able to do
completely new types of mathematics that we don't do right now. Right now, we prove
theorems one at a time. I mean, we're still kind of craftsmen. We take one theorem and
we prove it, and we take another thing and we prove it. Eventually, you could automate
exploring the entire theorem space of millions of different statements, which ones are true,
which ones are obviously false, and you could explore the geometry of theorems themselves.
I think we're going to see a lot of different ways to do mathematics, and we're going to
see a lot of different ways to make connections in fields that we don't currently. And it'll be
a lot easier to collaborate and work in different areas of mathematics,
yeah, especially because you can use these tools to sort of compartmentalize all these tasks,
all these big, complicated projects into small pieces. And plus, also, these large language
models actually will become very good at getting humans up to speed on any number of advanced
mathematical topics. Okay, oops. Yeah, but it's still not quite there yet. I would say,
if, for example, we prove formalization, it still takes about 10 to 20 times longer to
formalize a proof than to do it by hand, but it's dropping. And I see no reason why this
ratio cannot drop below one. Eventually, it will become faster to, eventually, when we just explain
our proofs to GPT. And GPT will generate, you know, it will ask questions whenever we're unclear,
but then it will just generate the latex and the lean for us. And we, you know,
eventually, and, you know, and check our work at the same time. So I think this is all in the future.
All right, so thanks for listening.
Thank you. That was lovely. I think we have a couple minutes for a few short questions.
Is there a microphone?
Okay. There will also be a Q&A next door in 204 for a few minutes after when this is over.
Are we using these mics, or are we calling up? Yes. That's great. I can't see the mic from here.
So the person I called on can ask the question. Okay, sure. So one prediction that some people
have bandied about, about advances with AI-assisted theorem provers is that we might enter a period
where there's a proliferation of proofs for new theorems that are formally verifiable,
but which we don't yet have the technology to translate into forms that are easily
comprehensible by humans. Do you see this being an issue? Well, it already happens. You know,
for example, this Boolean Pythagon-Triple's theorem, no human will ever read that proof.
So, I mean, I think it's actually not that scary. I mean, you know, we rely on big numerical
computations already in a lot of your mathematics. Of course, we would still want to have a human
understandable proof, but as the not-example shows, you can take an incomprehensible computer
proof and still analyze it and extract out from it a human proof. So I think that would be one of
the ways you do mathematics in the future is to clarify computer-assisted proofs and make them
human-understandable. Thank you. Can I ask you over there to ask a question? My question is
kind of speculative, but I wanted to ask your opinion on the rule of human intuition going
forward with this, because what a lot of what we talked about is formalization of human intuition
into formal mathematics. I was wondering if you think that intuitive part of coming up with the
idea for the proof itself could be automated in the near future? Not in the near future. As I said,
these likely models can generate things that resemble intuition, but it has a lot of garbage.
At the very low level of proving like one or two steps in a proof, we can use these proof
assistants to sort of only pick out the good intuition and discard the bad one. But what
large numbers are terrible at right now is differentiating good ideas from bad ideas.
They just generate ideas. So unless there's another breakthrough in AI, I don't think this is going
to happen any time soon. We'll take one more question from this side. So I was curious about
the need for blueprints. Is it that the system doesn't know enough definitions yet or is the proof
space too big? Some combination thereof? No, it's more of an organization for the humans. If you
want to coordinate 20 people to work on the same project, many of the people who work on these
projects, they don't understand the whole proof. So you need some structure to split it up into
really small pieces, atomic pieces that are self-contained for individual people to work on.
So it's not for the computer. The computer can compile anything. It's for the humans to
break up a complicated problem into lots of easy pieces. Kind of like how divisional labor works
in like modern industries, like factories. Okay, I'm going to invite all the other people
waiting to ask questions to join us in room 204 briefly. And let's thank Terry for a lovely talk.
