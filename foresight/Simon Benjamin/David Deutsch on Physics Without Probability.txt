Hello everyone. Thanks for turning out.
Those of you who are not in physics coming here in the rain,
is my pleasure to introduce Professor David Deutsch,
who is going to speak to us today.
Many of you will know David's name.
It's not controversial to say that he is one of the key individuals
who really kicked off the field that has become
the field of quantum information and quantum information technology,
which as you know Oxford now has a very large leading role in the UK and in the world.
David yw'llid da guarantee gyda'ch hun a rhaoster dim ond o bron y heavy
y ddim yn y spampio Germany.
Cwer o'n 85 hwn yn weithio unnetwch ag endangered
winnwt gan'r iawn ddisgrifennu 15 ar y reit societyenni,
yn Genesis Spearwood.
Mae gennyn nhw i'n mleidio'r cyfrwursed peuvent i farodes
mutta fel cam unedais il ymgyrch ond ac i'r ysgolus i ddangos
xesen.
Tyb Lastwiy yn defnyddio y folding y Viw Ym fadesch yn ein sp vivio.
Felly arniwch awrfod pan wahanol supersigon,
a i wneud rawr maen Llyfrgell C vin.
Dur i 2300, David ..
.. ddiwrnod o'r prolref iait yn unig cyfnod,
yng Nghymru, David, o Chiarra.
So, David, ond, ond...
Thank you.
Right. Well, the ideas I'm going to talk about today are part of Constructor Theory,
which I've been working on with Chiarra Marleto,
and among other things, Constructor Theory uses a new mode of explanation,
of physical reality, through which we hope to eliminate a lot more than just probability,
a whole load of explanatory dead weight and blind alleys from physics
and from other sciences and even beyond science.
What's a mode of explanation?
Well, at present, the prevailing mode, which is mistakenly thought to be the most fundamental,
is like this.
There are particles and there are fields in spacetime,
and they obey laws of motion and initial conditions,
and once you know those, you can predict the whole of the rest of their behaviour.
But there are other modes of explanation in common use, in physics and elsewhere,
and here are some of them.
No? Yes. Good.
Just for illustration, I'm mainly going to talk about the probabilistic mode
that I've put in red there,
but as I said, Constructor Theory seeks to unify and to underlie all those modes and others
with a single mode of explanation, the Constructor Theoretic mode,
which is explanation via the dichotomy between transformations that it's possible to bring about accurately
and ones that it's not possible to bring about accurately,
and those are possible and impossible tasks.
Now, the basic principle of Constructor Theory is that just that all the laws of physics
are expressible in that way.
Since every task is supposed to be either definitively possible or impossible to perform,
there can't be such a thing as a task being possible with a given probability, not 0 or 1,
and at the fundamental level, such things might arise as approximations,
but that's not a description of reality.
Therefore, there's no place for probability, according to Constructor Theory,
there's no place for probability or random processes at the foundations of physics.
However, I'm not going to make the case against probability from Constructor Theory today.
It's more like the other way around.
This is a motivation for Constructor Theory because independently of Constructor Theory,
the world just is not probabilistic, it's an illusion,
and the probabilistic mode of explanation has about the same status as the Flat Earth Theory.
Namely, you might find it useful to use the predictions of the Flat Earth Theory
when you're planning out your garden,
but when you're thinking about what the world is like,
and even more when you're thinking about what the laws of nature are,
it would be hopeless the theory of the Flat Earth would just be an impediment to understanding what's out there.
So, same is true of probability, and so let me explain.
Well, probability and the associated concepts such as a stochastic process
that produces a random sequence of outcomes,
these weren't originally invented for any purpose in physics or any fundamental purpose.
But basic mathematical theory of probability, namely certain numbers attached to certain elements of a set
which then obey axioms like they all add up to one and stuff,
was invented in the 16th century by people who only wanted to win at games of chance.
This is one of them, Cardano, and that started off game theory,
and game theorists use the idea of a random sequence as a mathematical model of physical acts
that are integral to games of chance, such as shuffling a pack of cards or throwing dice,
and probability, via the central concept of equally likely,
was a mathematical model for the intuitive idea of fair shuffling of cards,
a fair dice and a fair throw of the dice.
So, when I say that, which is also essential to games of chance,
when I say that this was merely a mathematical model, perhaps I should say more clearly what I mean,
I mean that they didn't assume that anything in the physical world,
shuffling cards and so on, was literally what we would today call a stochastic process,
that it literally had properties such as probability.
That's because, first of all, the properties that they actually needed from shuffling cards
were far weaker than full literal randomness.
They were basically that the outcomes of shuffling should be fair and equitable among the players
and unpredictable to them.
So, that meant that the outcomes were not related in any simple way to any sequence or algorithm
that they could call to mind or execute during playing the game.
So, most pseudo random sequences would have done just as well if they'd had computers then.
And secondly, more importantly, I don't think it would even have occurred to them
to connect their probability in their model with some real physical quantity
because classical physics, which was in its infancy at the time of Cardano,
but bit later when it was perfected, was deterministic
and therefore inconsistent with stochastic processes happening in nature.
So, why did those game theorists blithly use mathematical axioms,
a probability theory that were both far too strong and already ruled out
by what was known about fundamental physics?
Well, simple. By being too strong, that's harmless,
that meant that they certainly satisfied the conditions that were necessary for analysing games.
And the real properties that they really wanted, such as fairness and so on,
couldn't be expressed precisely mathematically.
By the way, they can be expressed precisely in constructive theory,
but that's another story.
Consequently, it didn't matter also that their model relied on impossible physics
because the conclusions about game playing, if there had been that impossible physics,
led to the same conclusions about strategy and tactics as for the real physics.
So, it didn't matter for them.
There's another reason why they might have been blas√© about the connection between their theory and physics.
Certainly this was a move made later in game theory and in probability theory up to the present day.
They might have regarded the probabilities not as attributes of the dice and the cards or the dealer,
but as attributes of the player receiving the cards.
That's the subjective interpretation of probability, and I'll come back to it in a moment,
but note already that whatever role the subjective state of mind of the player may play in this story,
fairness must depend on physical properties of the cards and dice and how they're treated during the game,
and therefore game theory must, in principle, be rooted in some sort of model of those objects and processes.
Anyway, from that theory, game theorists derived maxims for playing games of chance,
such as in poker, never draw to an inside straight,
and the important thing is that this was a narrow, parochial application of a mathematical trick, the theory of probability.
Indeed, the reason that game theory was possible at all was that its conclusions don't depend on the detailed physics of any game playing processes.
It didn't matter what the orbits of the planets are, it didn't matter what matter is made of,
or let alone anything deeper about the laws of physics.
The theory was directed specifically at modeling a particular human social behavior,
which even other animals don't do, let alone the physical world at large.
So it should be very surprising that this little mathematical trick, the theory of probability,
has found further applications in a very diverse fields in science and beyond,
and even that it seems to be fundamental to some of those fields.
So here are some of them, roughly in the historical order of their invention.
So we'd be surprised if, say, the seven of diamonds appeared in a law of physics.
But probability, which has the same provenance as the seven of diamonds,
seems to be central to physics in particular, and it seems to work.
So why would anyone want to expunge it from physics and from every other fundamental theory?
Well, of course, we don't need a reason. Physics likes to do without things.
As we discover more and more about the world, we sometimes find out that things that we thought existed don't exist,
such as celestial spheres and the force of gravity and trajectories of particles and so on.
They were all thought to exist, and then discovering that they don't exist was a great advance in understanding.
But before we can decide that something doesn't exist, we must explain what the world without that thing is like,
and also, preferably, why thinking in terms of that thing seemed to work
and why it was a useful fiction and why it may still be a useful fiction.
So compare these two statements.
The first statement specifies a factual, observable property of the world.
It specifies what will happen when poker is played and specified hands arise.
But the second one is not about physical facts.
It is consistent with any sequence of cards arising in a poker game.
In fact, it's consistent with any physical events, such as someone repeatedly drawing to an inside straight and repeatedly winning.
Not inconsistent. Yes, it's true that it was risky.
The second statement is true, all right, but it doesn't apparently refer to any physical events that actually happened in that case.
One sometimes hears a sort of desperate denial of this along the following lines.
The statement, it was too risky, is about the physical world.
It refers to all the other players who drew to an inside straight and lost,
and it refers to the fact that there are more of them. They outnumber the winners.
Well, first of all, it doesn't.
There have only been finitely many poker games in the world.
Ignore for the moment the fact that there are parallel universes,
and in those particular player, both wins and loses in different universes,
and quantum theory does in fact solve some of these problems, but not by counting the number of players.
So, for the moment, in a given universe or in classical physics,
the proportion of players who drew to an inside straight and lost doesn't exactly equal the probability of losing.
Just as repeated tosses of a fair coin, however fair it is,
don't result in equal numbers of heads and tails in general.
And anyway, since when do gamblers care about whether other gamblers lose or win?
If probability refers to other players losing money,
it doesn't refer to any physical fact about the game as it actually was in the case that I cited.
And the same holds for all probability statements.
So, that's meant to be, in the second column, that's meant to be all probability statements.
That's the dot, dot, dot.
The blatant fact that is generally overlooked is that no statement from the second column can ever imply any statement from the first column.
In other words, assertions about probabilities do not refer to the physical world.
They don't assert anything about the physical world.
Frequencies, like the fraction of winners over losers historically, are things that happen in real life,
and therefore they don't, in general, equal probabilities.
And it's no good saying that they equal them approximately, because they don't.
They only probably equal them approximately, and that's a statement from the second column.
Similarly, you can't say that probability statements are about what will happen in the long run.
No, they aren't. All you can deduce from them are statements about what will probably happen in the long run.
But frequencies in an infinite sequence of measurements, of experiments, do equal the probabilities exactly,
and this inspires yet another desperate denial to the fact that the finite sequences are approximations to infinite ones.
But nothing about a finite subsequence of an infinite sequence can possibly follow from a statement about relative frequencies in the infinite sequence,
unless the subsequence is a typical one, and a statement that is a typical one belongs firmly in the second column.
Or you may take that subjective route that I mentioned.
You could imagine that probability statements are not assertions about the world at all.
They are assertions about our minds, probability being a measure of ignorance, or of a degree of rational belief.
Those are two different subjective theories called credence, but that's no good for present purposes because then you still need something to connect statements about our minds to statements about physical reality.
So, to this end, the philosopher David Lewis proposed his principle principle, which just asserts as an axiom that rational agents have the same credences as the physical probabilities.
But note that that gives no explanation about why those physical, purportedly physical numbers should inform decisions in that way.
You may as well propose an axiom that rational people avoid black cats or ladders. That's not physics.
So, the upshot is that Cardano and the game theorists never did succeed, after all, in their purpose of finding ways of winning at games of chance, or of minimizing their losses.
They only found ways of probably winning.
And later, people added these purported philosophical principles that say a rational person would do this or that, but actually a rational gambler knows that having probably won, no matter how often one does it, won't pay the rent.
Physically, physically, it is most unlike actual winning.
So, in case there still appear to be any clothes on this emperor, because of the mass of cultural interpretation that's been loaded onto it, just replace all the probabilistic terms in the second column by magical terms.
Why? What physical reason is there to allow statements in the second column to inform decisions by fiat and not statements in the third, which we could equally well connect to reality by some fiat?
That's the magical column. Thanks. Thanks a lot.
So, what I have called these desperate denials of the firewall between the first column and the other two are more commonly known as interpretations of the probability calculus, subjective interpretation, frequency interpretation, ensemble interpretations, and all in many variants,
regarded as attempts to connect the second column with the first, they are all, like the examples I've given, either circular or meaningless, or conflict with the probability calculus, or just don't do what they say they do.
This discussion that I've just given about the defects in the theory of probability draws on the work of David Papineau, who's a philosopher of probability.
He has called this situation at the heart of his field a scandal. Probability concepts and language and the whole theory simply form a closed system of statements and ideas that just refer to each other and can never yield a statement about the physical world.
Now, if we go back to the applications of probability, we can see that there is a common feature running through all of them. They are all in a certain sense and slightly different senses.
They're all normative. That is to say, they are about, at root, how one should act if one were to believe that certain potential events have particular probabilities.
Now, how one should act is rather a strange thing for a scientific theory to talk about. You know, you can't get an ought from an is.
In fact, the firewall between factual statements and moral statements exists for a very similar reason to the one separating factual statements from probability ones.
So, another way of papering over the divide or ignoring it is by different kind of fiat. One simply puts a normative statement about probabilities into the physical theory.
That's called a stochastic physical theory. There are many ways of expressing that fiat. Here's one.
So, it contains a lot of hidden stuff. It pretends only to contain that first row, but actually there are some purportedly factual statements about numbers that purportedly have something to do with physics.
And then there has to be a principle to give them some normative psychological meaning, the second row.
And then the third row, actually, you need another axiom from decision theory to say how one should actually behave rather than how one should think.
And this is weird, but luckily it so happens that the only stochastic theory that has ever been proposed in the history of science as a fundamental description of the world is quantum theory in its mid-20th century state vector collapse form.
And in that form, its probabilistic part is called the Born Rule, which says that if and only if an observable is measured, then the probabilities of the various outcomes are the moduli squared of those coefficients and the bottom line there.
By the way, does anyone here actually believe that the state vector collapse occurs in physical reality? Show of hands? Okay, no, good.
Oh, so one person, so you have my sympathy, and I hope you'll see in what follows that help is at hand, closer than you think.
So because, well, help is at hand because ordinary unitary non-collapse quantum theory provides in large part the way out of that whole probability scandal.
It's called the decision theoretic approach, sometimes called the decision theoretic approach to the Born Rule or to probability, but those are both misnomers because neither the Born Rule nor probability nor collapse, of course, ever appear in the decision theoretic argument.
In its simplest form, it goes like this. We imagine an array of gaming machines, one-armed bandits in a casino, so there's a whole array of them, and you play by inserting one casino token, and then when you pull the handle, the machine prepares a quantum system inside itself.
In a state psi, and then measures an observable x of that system, it then displays the result, as shown in red on the middle panel, and that will therefore always be an eigenvalue of x.
And then the machine delivers a payoff of that number of casino tokens, which need not be a whole number, we're allowing fractional tokens if x has non-integer eigenvalues.
Different machines in this casino are identical except for the state psi, which is constant for each machine, but different for different machines.
But the psi is not a secret, it's written on the front of the machine, just like this, conveniently expressed as a superposition of eigenstates of the observable x.
Now, four machines whose psi is a single eigenvalue of x, with eigenvalue little x, then playing on that machine is not a game of chance, it's just a matter of putting in one token and receiving back little x tokens.
Let's call that a classical machine, because it could be implemented without quantum systems, without quantum technology.
So, other things being equal, a rational player would be willing to play on any classical machine that has little x greater than one, and unwilling to play when it has little x less than one.
What about cases when psi is not an eigenstate of x? What then is the dividing line between being worth playing and not being worth playing?
Well, let's call the dividing line for a machine operating with state psi, let's call that dividing line v of psi, and this is perhaps a little bit of a wordy definition, but it's just the maximum amount of money that the player would be willing to pay for the privilege of playing that machine.
So, for a classical machine, psi is an eigenstate of x, and v of psi is just the eigenvalue.
Well, some other facts about v of psi, more general psi, are obvious too. We don't need probability or the Born Rule or anything. For example, if the state is a superposition of eigenstates of x, all of whose eigenvalues are greater than one, then elementary rationality says that it's worth playing, because whatever the outcome, the machine will give you more than the one token that you put in.
Similarly, for a superposition with all eigenvalues less than one, it's not worth playing. That is, it's not worth playing in the token-winning sense of the game theorists. You might well play for fun, but then you're being paid partly in fun, so let's ignore that complication.
But now, what if the hard case, what if psi is a superposition of two eigenstates, let's say one with eigenvalue below one and one with eigenvalue above one?
Well, in collapse quantum theory, with the Born Rule and everything, that tells us that for general states psi, the probabilities of the respective outcomes are those coefficient squared, and then the principle principle tells us to adjust our credences to match those numbers, though it doesn't say why.
And probabilistic game theory tells us that a rational player should value playing such a machine the same as if it were guaranteed to produce that expectation value.
Again, it doesn't say why. In general, axioms of stochastic theories are not explanatory, which alone should disqualify them from being part of any scientific theory in fundamental science, but I digress.
Now, what about without collapse? Well, let's take a simple case of an equal amplitude superposition with eigenvalues x1 and x2, each with amplitude one over root two, so we're aiming to prove without collapse or anything that v of that equal amplitude state, sorry.
We'll be the average of x1 and x2. So that's what we want to prove.
So here's the proof, a quick and dirty version of the proof. The devil is in the detail, which I'm not going to talk about. The details are extensive, but they lead to the same conclusion.
So first of all, we need to note two implications of elementary rationality, not probabilistic in any way, namely equations one and two.
I'm stating them for a slightly more general case, but it doesn't matter that the theta is going to be pi by four in the end.
So equation two says that v of psi does not depend on the objective exchange value of casino tokens, so long as they're additive.
If the casino suddenly declares that it's going to redeem tokens in pounds instead of dollars, the relative order in which a rational player values playing on different machines will not change.
So that's one implication of rationality, and the other is expressed in equation one.
If two machines use superpositions where each eigenvalue in the expansion of one of them differs by constant k from that in the other, then the rational player's valuation of the two machines also differs by that same constant k.
And that's just because. In that case, all the first machine does is physically is the same as what the second one does plus additionally paying out k tokens.
Now we make those substitutions there on the slide, and it follows with a bit of algebra that for the equal amplitude state, v of psi is indeed the expectation value of x in that state, and we can prove the same for a general state and that's QED.
But look what we've done here in a deeper sense. We've proved the same conclusion about what rational players do as we would have from classical collapse quantum theory, but without assuming that collapse happens.
We've proved it without the born rule, without those axioms and all that stuff. So we've done this on the right hand side, whereas with collapse quantum theory we have to go through all that process on the left hand side to get to the same conclusion.
So on the right hand side, the only assumptions are elementary rationality and unitary quantum theory without collapse. From the perspective of this talk, that means that we have dispensed with probability, both in nature, in the quantum world and in our minds if we have any credences about the quantum world.
Because in nature, as described by quantum theory, there are no stochastic processes and no credences in the sense of beliefs with numerical measures that obey the probability calculus, no credences affect the decisions of any rational person making decisions about quantum systems.
That is even better than that. Those decisions now on the right hand side are not only derived, but unlike in the collapse case, they are explained because we don't have to introduce all those unexplained postulates.
That also explains why collapse theory, despite its false assumptions, was and is successful in a particular domain of application.
So now, let's look again at that list of fundamental applications of probability. We can strike out quantum theory from the list and we can put a tentative mark there about credences too since we now know that they're not needed in this particular application.
We've also eliminated probability from the theory of games of chance that use quantum processes where the chance element is generated by quantum indeterminacy.
So what about games of chance in general? Well recall what I said at the beginning, probability is a very large sledgehammer with which to crack the egg of modelling things like fair dice.
Now we see that the same job could be done by quantum theory as by the impossible physics that they assumed. Since a pseudo random sequence would have served the same purpose, so would a quantum generated sequence.
For the same reason, we can strike out the use of probabilities in general decision theory which is just game theory writ large and in actuarial science. I just put that in because it was historically a very early application of probability.
It measures, it prepares the system in a state sign and then it measures an observable x on that system. The result is the pale.
Just as in game theory, the theory of evolution doesn't depend in any way on true randomness in the mutations. All that matters is that the theory can explain how biological adaptations to an environment can evolve if the mutations aren't systematic, if they don't depend systematically on the environment.
Only the selection is supposed to be systematic, that's the essence of the theory of evolution and probability randomness is just a feature of the model convenience so we can strike that out too from the theory of evolution, strike out the theory of evolution.
By the way, Chiara has just published a paper about the constructor theory of biology which among other things describes evolution without evoking randomness in any way.
Of course, by all these strike outs, I don't mean that the mathematical formalism of quantum theory isn't sometimes useful. I'm saying that the quantities called probabilities in that formalism do not refer to any stochastic random processes in nature nor to anything in rational minds such as degrees of belief or credence,
nothing in physics or in minds thinking about physics. In information theory, probability was again originally used as a model for an even simpler thing, namely this could be any message of n bits.
Our communication system needs to be able to cope with any message of n bits and we don't know which it's going to be and this was translated in the model to all two to the n strings are equally likely.
This has caused all sorts of confusion such as people saying that a state contains maximum information when it has maximum entropy, which is nonsense and so on, but that's another story. Anyway, we can strike it out.
I'll just say in passing that constructor theoretic information theory is also something that Chiara and I have recently developed and it does fulfil all the hopes that we have for constructor theory in general including not being subject to the kind of confusion about information that I just mentioned and it unifies classical and quantum information.
That was published a while ago. So what's left? Quantum statistical mechanics at least doesn't need probabilities since it has entanglement and decoherence and therefore it can avail itself of what I've just described about quantum theory, the decision theoretic approach.
I should say why the universe is in such a state as to make the laws of thermodynamics hold, for example that it's uniform, that it's initially ordered and so on, is a substantive question, but it's not a probabilistic question.
So we can strike that out too. So I'll leave striking out classical statistical mechanics as an exercise for the audience.
Now, experimental error, that's an interesting case. I think it was historically the earliest application of probability after games of chance and it has some interesting misconceptions in it in addition to probability which one of them is as follows.
So it's connected with probabilities. Error processes in experiments are traditionally categorised as random and systematic, but both of those are misleading terms.
For simplicity, just imagine a measurement of a constant of nature such as the speed of light. And suppose that we are asked to give an estimate of the best error attainable with a given instrument like Fiso's wheel.
Processes that were random with known probabilities would not be sources of error at all in such a case since they could be reduced without limit just by repeating the experiment.
So the important errors are the ones that affect experiments through processes whose governing laws are unknown. That is, the laws may be known, but how they affect the experiment is unknown.
And those cause systematic errors, so ironically a systematic error is one that obeys known system.
So what does it mean to estimate an error caused by the unknown? Traditional to go into probability and subjective probability, but as I've said that's all nonsense, so what can it mean?
Well, suppose that a physical constant, let's say, call it chi, is to be measured and that the bound for a given technology is claimed to be epsilon.
So if little x is the average of measurements obtained with a particular instrument, well, the best result obtainable with a particular instrument, then the first line, x minus chi, modus of x minus chi is less than epsilon, we're saying.
And for simplicity, assume that the individual outcomes that we obtain when we do the experiment repeatedly with different copies of the apparatus and so on are x1 and x2, then we have both x1 and x2 both obey that inequality as well.
And then just from some algebra, it follows that the average of x1 and x2, just as in the, if they were random errors, has a smaller error than either x1 or x2 separately, which is a contradiction, because epsilon was supposed to be the best error obtainable with that apparatus.
So systematic experimental errors cannot be bounded by any known bound, therefore, among other things, they can't be described by probabilities, nor can our knowledge about them.
These unknown variables in science are counterintuitive and often misunderstood because we have been accustomed by Bayesianism and other subjective philosophies to replace real ignorance by fantasy probabilities.
But we've just seen that neither physical probabilities nor probability credences can enter the analysis of errors in a fundamental way.
So what do we mean when we estimate the error in an experiment?
Well, that turns out to be a big question with surprising answers that there's no time to go into here except I'll just briefly mention, I hope to complete a paper on that quite soon.
But I'll just briefly mention that what an error estimate really means is it's the error such that if the experimental result turned out to differ from the true value by more than that when the true value is later discovered by some other method,
then if it was more than that it would make the theory of the apparatus problematic.
So that's in short what an error means, but for present purposes we can just strike out error analysis there.
So Brownian motion, that doesn't actually purport to be a fundamental theory since although it has a stochastic law of motion, that's assumed to be an approximation to a more microscopic cause such as impacts from molecules that aren't explicitly treated by the theory of Brownian motion.
So that theory is also related to the theory of errors, but in the approximation that the so-called random errors are swamping the systematic errors.
Although we've just seen that that's something that cannot be known in a particular case, but if the unknown systematic errors are too large then it's not Brownian motion.
So we can strike that out.
And with it we can also strike out the applications from high finance that are directly analogous to the theory of Brownian motion.
Finally, since none of those other applications of probability now involve stochastic processes, they do not require credences either.
It doesn't matter what scientists think, what scientists believe about whether a theory is true or false, so long as they execute rationality.
And so Bayesianism and the other subjective interpretations of probability have no remaining scientific function and for these purposes they can be dropped too.
Now I hope I have shown you that probability doesn't make sense as a description or explanation of what really happens.
It can be a metaphor, it can be a technique for calculation or an approximation in a certain sense, but an approximation to make sense has to be an approximation to something.
So if probabilities are to inform decisions in some approximate way, there has to be an explanation rooted in a description of an actual physical world in which events and processes happen, not probably happen, and not just via some ad hoc axioms.
So I hope I've also persuaded you that it's right and proper to try to expunge every trace of probability and randomness from the laws of physics and from our conception of the world and from the methodology of science so that we may fully restore realism as well as rationality.
It's a simplification, a unification and an elimination of nonsense and it's true.
Now I bet there are hard headed instrumentalists in the audience who might be thinking, okay, so this simplification is all very nice and elegant,
but since the principal uses of the mathematics of probability are largely unaffected, what really is the benefit of eliminating it at the fundamental level either?
Well it's true, fundamental falsehoods don't always rear up and bite you.
You could believe in a flat earth as many people did for a long time and that falsehood may never have affected your life or your thinking.
On the other hand it might have destroyed the entire human species because belief in a flat earth theory as a description of reality is incompatible with developing, say, technology to avert asteroid strikes.
Similarly a belief in probability in quantum theory may not prevent you from developing quantum computers, quantum algorithms in practice,
but because probability and the born rule entail fundamental misconceptions about the physical world, they could very well prevent you from developing the successors of quantum theory.
And in particular, constructor theory is the framework in which I suspect successors to quantum theory will be developed.
Now as I said, constructor theory is incompatible with physical probabilities.
So that is my case. Thank you.
Many of the things that David alluded to, papers and so forth, easy to find.
Okay, I saw one hand go up, so please.
I'm sorry, but the slot machine, I'm afraid I'm so far walking out of here with no understanding of how to modify my way of thinking about a single machine that ended up coming in.
So if someone asked me, what are the funny numbers in the square roots mean in front of the eigen states in the slot machine?
I would say, well, what those funny numbers mean is if I pull the handle lots and lots of times, then on the limit that I do it forever, one third of the time I get this one result and the other two thirds of the time I get this other result.
And say, well, which result will you get next?
I don't know.
I didn't understand how to modify my way of thinking about that based on the stuff you talked about.
Well, the idea is that I'm not sure which version of quantum theory you're thinking in terms of possibly collapsed theory in which there is such a thing.
I don't know. I want to know how I should think about this.
The way you should think about it is not in terms of the outcomes. The outcomes are an emergent property in some complicated way.
What those coefficients are, they're just descriptors of the state.
The state that the thing is prepared in.
So, for example, if it was a spin system, it would be prepared by rotating it in some way with magnetic fields and those coefficients would depend on the angle through which it's been rotated.
That's what it actually means.
Now, what that causes in our subjective experience, do you need an interpretation of quantum theory for that?
Interestingly, all we need, and the right interpretation is the many universal interpretation, but we don't really need to draw on that for these results.
We just have to say that the motion is unitary.
That's all. That the state exists physically and its motion is unitary.
Other question?
Hello.
I'd like to find out more about what we think about.
I can go along with the idea that it's very weird to have them in a fundamental micro-physical theory.
If in the future we find some kind of theory which is a deterministic, we have to wrestle with what that means.
Or, you know, find out how we don't have them.
But as a high level concept, they seem to be useful in some kind of vision philosophy of science which doesn't.
It's not trying to talk about the fundamental physics itself.
So I'm an observer, people would say, has a credence maybe if they don't know exactly what the micro-state of the real world is.
And they have some idea that I think it's within this region, but I've got no reason to differentiate between actual micro-states.
So having no reason, so these are two slightly different ways of doing a subjective interpretation.
The first one, where you guessed what the probabilities are in some theory and it's going to be an approximation to some microscopic state that you don't know,
that's a legitimate way of approximating something.
You have guessed what the probabilities are and then you can test if theory says that there's something, you can test whether that's so.
When you say you don't know what the micro-states are, then you're trying to derive knowledge from ignorance.
And that is simply logically not valid and it's also inconsistent with the probability calculus.
It's only by sort of hand-waving ignoring that fact that one can apply Bayesian philosophy in words to things like we don't know what the state is.
If we don't know what something is, we don't know what it is.
So the standard refutation of that kind of subjective interpretation is if there are three possibilities A, B and C, you don't know anything about which state it's in,
then you might say, well bet on whether it's A or either B or C.
And then if you say that because you don't know which of those two possibilities is right, that you must give them equal probabilities,
then the same is true for the permuted versions and that's inconsistent.
It's logically inconsistent so it can't be used for that.
The situation is very analogous to the flat earth theory.
When you assume that your garden is a plain surface, you're not assuming the flat earth theory, you're just assuming the mathematics of the flat earth theory applied to this process.
But there's no way in which you are assuming that the true thing is an approximation to the flat earth. It isn't.
So with your decision theoristic argument, which is used to bypass the principle and to derive the Born Rule, you invoke this function V, this evaluation function.
However, you also asserted that it had some properties that it was linear in the eigenvalues of the function.
Yes.
So have you also asserted that the rational entity would‚Ä¶
No. We're assuming a rational entity who has linear valuations.
So this same issue arises in classical game theory.
As I said, a person, a real game player is not the same as the idealised game player of game theory.
A real game player has all sorts of other motivations.
Otherwise, a real game player would never play a game of chance such as you read where the expectation value of your winnings is negative.
So we assume this idealised game player in classical game theory and we assume exactly the same player in the decision theory approach to quantum theory.
So it's not because we want to say why people play games.
It's just this one particular aspect of game playing that has probability.
And it's that that is analogous between the two cases.
Okay, but you still made an assertion about how this player would behave, right?
So how is that any more elegant tool than the principle?
No. The assertion about how the player behaves is the same in both cases.
We're analysing how a player who believes a particular thing will behave.
We're not saying that any real player actually behaves like that. No real player does exactly.
We'll probably take another question.
I thought, yeah.
David, you were at the foundation of what you're saying is you're eliminating the distinction between epistemological chance and ontological chance.
I'm saying that epistemological chance, well, yes I am.
I'm saying that neither of them exist, but ontological chance can be a good approximation in some situations.
And epistemological chance really should be dispensed with all together.
Okay, so if you focus on the epistemological chance there can be two kinds.
One where we don't know enough about the system to be able to calculate the outcome.
Yes.
There's another kind in which, even if we did know everything, we would not be able to compute the outcome because that problem was not computable.
Yes.
Does that not create an effective ontological chance?
Well no, that's a particular situation again in which the probability calculus may be a good approximation.
Rather like, if we think about the distribution of the digits of pi, then it's a meaningful approximation to say, well they're going to be random.
They're not really random, but the mathematics of them has enough in common with the mathematics of truly random sequences for it to be a good approximation for some purposes, but obviously not for all purposes.
So the common feature is just that no matter what we call it, they're just unpredictable whether it's ontological or epistemological.
Yes, unpredictability can be modelled by randomness sometimes.
Okay, I think we've probably got time for one more question, and then I expect David's happy to hang around for a few minutes afterwards, so those of you who didn't get to ask a question, then he's just coming in front.
What have I asked? Do I see one here?
Yeah, I just wanted to come back to something that you seem to be wanting to expunge probability from any fundamental physics, and I think we all know that probability is very softly problematic.
But you seem to be wanting to do this by replacing it with the decision-theoretic governance in which are made with respect to some admittedly non-existent player or some game that is probably also not being played.
So I'm really struggling to see how that is any more desirable than any of the other approaches to the admittedly philosophically problematic probability.
Yes, so the main advantage is it doesn't assert anything false about reality.
Well, it's asserting that reality has something to do with this thing that we know doesn't exist.
No, no, it's just saying that if, so it's taking an idealised game with idealised processes happening both for randomising and for the players, and it's analysing how those players would behave if they believed some rationality, if they conformed to some elements of rationality that have nothing to do with probability.
But that entire argument is counterfactual.
Yes, it's counterfactual, but then you need an argument to say that a real situation resembles that idealised situation in particular worse.
So if I go into a casino to play roulette, obviously I'm not satisfying the axioms of game theory because I'm playing a game which has an expectation value that's negative of winning.
On the other hand, if I play poker and I'm good at it and then I wonder whether to draw or not or how much to bet, then I can use game theory because and only because I guess that my real situation resembles the idealised situation in the relevant ways.
But it's only as good as that. It's not a theory of reality.
So it's based, as you said, on a guess on the correspondence between what you might be doing in certain situations to something that is counterfactual.
Yes, exactly.
So inside me right now, I'm assuming that there are carbon-14 atoms decaying, that there is no-one doing that game of chance, that there's no sensible way in which you can prepare it to an idealised.
Making that game of chance, talking about if there's length of time, will take any given moment to decay.
I think we'll just let David reply to that.
OK, well, you can. If you want to work out the half-life, then you simply have to imagine the situation in which these game players bet on how long it will take and what the rational thing for them to do is, and you'll find that the rational thing for them to do is to bet on an exponential decay.
And that's the problem.
So, once more, if we could do that.
Thank you.
