ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ �
ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ
ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ �
ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ �
ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ �
ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ
ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ Spanish
ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლაˇʜ wears
Okay.
So I put the thing in the YouTube link.
ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ ლ �
�̢̡̹̼̼̺̺̺̱̜̥̠̗̖̟̜͉̀̄̊̓̈̔̽͠�
Really good.
ლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლ�
ლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლ�
ლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლ�
ლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლ�
ლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლ�
ლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლლ�
ლakes ლლლ ლლლლლლლლლლლლლ ლლლლლ
ancestri  бат æ
Okay, and depending on the time to also have a fun discussion on discrete versus continuous.
Is that was on me that had.
No, that was me I killed it.
And using my slides.
We see random walk up.
There's another way.
Sorry.
Hi everybody. Welcome back to another lecture in spring 2021 semester of computational thinking at MIT at 683 18S 191.
So as Alan said, today we're going to have the second lecture about random walks.
So we actually had a small technical problem with the video of the last lecture.
And so we have only just been able to actually upload that video to the YouTube channel.
So if you're watching this, you probably want to actually look at the previous video first, although they're reasonably independent that we start with.
Okay, so what are we going to see today?
Well, we can just more about random walks.
And what we're going to see is that in Julia.
Let me just zoom in.
We have an object we can actually define a way that we want to visualize you that object when we when we asked Julia, you know, to just display the value of the object we can actually customize how it does so and we'll see that in action using structured matrices.
So if you remember from a few lectures ago, we had various kinds of matrices with structure by which we mean that we know where there are zeros in the matrix.
And it turns out that in Julia, Julia has predefined types that represent such structure matrices like lower triangular, for example.
And when we display them, it will display them in a nice way.
And we'll see this function called CUME sum, which does cumulative sums.
And we'll see how we can do vectors which contain vectors inside them.
So each element of the sort of outside vector is itself a vector.
And we'll want to construct it a vector of vectors.
We often want to put them all together.
And that's called concatenating them.
And we do that with, for example, the hcat function, cat for concatenate.
And also look at two ways we can draw plots in three dimensions.
I think we've already mentioned these, which are heat map and surface from both from the plots.jl magic.
And then we'll start off with a nice diversion, apparently, which will turn out to actually be related to what we've been talking about.
So that's Pascal's triangle.
So probably you saw Pascal's triangle at high school or sometime previously.
And just to note that, of course, as usual, I think it's Stigler's law that says that the person after whom a concept is named is never the person that actually discovered it.
Pascal was not the first person to study this triangle.
It had already been studied centuries before in India, Persia, China, etc.
And here's a link to the Wikipedia page, which has links to documents that show how it was being studied in other countries much before Pascal.
But Pascal apparently did contribute a lot to the understanding of this triangle.
So what is Pascal's triangle?
In general, in Julia, I've written a little function that takes in a number capital N, and it generates this matrix that we're seeing here.
And the way it does that is by calling this function called binomial, which generates what are called binomial coefficients.
So what are the binomial coefficients that actually you can think of them as the kth entry on the nth row is the coefficient of x to the k when you expand out one plus x to the power n.
So we could actually do this symbolically in Julia, but maybe I don't have everything set up right now to do that.
But if you think about what is one plus x squared, I'm just going to expand out the brackets and I'm going to get one plus two x plus x squared.
And so if you look at the coefficients of each power of x, you get one multiplying x to the power of zero, which is one, two, and then one.
And that is exactly what we see on this row of the matrix one, two, one.
And so if we do one plus x cubed, we'll get one plus three x plus three x squared plus x cubed.
And so that's where these numbers come from. That's one place they come from.
And this is called binomial. It has two terms. That's what binomial means.
And so these coefficients that we get when we expand this product of all of these things, what are we actually calculating?
You can think of writing this out. I write that out as one plus x times one plus x.
What am I actually doing? I'm just choosing one of these two objects from each set of parentheses.
And so the coefficient is actually the number of ways that I can choose one one and one x.
The number of ways I can choose that is one from the first bracket and x from the second or an x from the first bracket and one from the second.
So there are two ways of doing that, and that's what this coefficient is.
And so there's this function binomial to calculate within Julia. What is that actually doing?
There's a formula for that in terms of factorials, and you can write that out.
And as usual, if we wanted to know exactly how Julia was calculating it, we could do sort of edit binomial of one, two, or whatever, and it would take us to the source code and then look at exactly how it's implemented, which we won't do right now.
Feel free to do it yourself.
Okay, so note that in this matrix, because of the way these binomial coefficients are defined, it does not actually make sense to talk about n binomial of nk, also called n choose k.
It doesn't make sense to talk about that, at least in this interpretation as coefficients for k bigger than n.
And so it just fills all of those elements with zero.
If you look at this matrix, it's actually kind of difficult to visually note, notice that all of those elements are zero.
So actually you would kind of want to, that's kind of, there's too much visual noise, those zeros are too, you know, it's difficult to pick out which are zeros and which are not zeros.
And so we might want a different representation of that matrix.
And so if you notice this matrix, the coefficients that we're interested in are actually on the lower diagonal, the lower triangle, the lower half of the matrix.
Basically they're on the diagonal and below the diagonal.
And so this matrix is actually what we call a lower triangular matrix.
And Julia has a type called lower triangular, which we can apply to this matrix that I generated, which is coming from this Pascal function I wrote.
So I'm passing in 10, that's the size of the matrix, 10 by 10.
And now I'm passing it into this lower triangular constructor.
So this is a new type called lower triangular, which therefore has a constructor, and I can pass in the data I want to put in that type, an object of that type.
And we see that when I do that, it displays it in a nicer way so that these zeros and that would just become dots.
And so this is an example of a type that defined in Julia where we actually can tell, you know, decide how to display objects of this type.
And so the way we would do that is actually, we do this by extending the show function from base.
There's a function called show in base Julia, and we will extend that by doing something right.
So let's make a new type called my type, which has an int inside.
And then we're going to do base dot show.
I need this IO object, which is input output, and an object X of my type, how am I going to display that.
I'm going to put something like I'm going to print to that output stream.
You know, this is my type with value.
I'm going to interpolate it into that string of X.
Let's not call it X, we'll call it obj.
Hello, let's just call it hello.
Hello dot X.
So the value of X inside the hello object.
Close the string.
And then if I make an object of that type, let's call that Y.
I'm going to make an object of that type by calling the constructor that Julia automatically provides.
My type of 10, and it should display Y equals, you know, this is my type with value 10.
So that's basically what they're doing.
So when you generate this lower triangular type, it prints out the size of the matrix, and it tells us that it's lower triangular, and these are the types of the objects inside.
So N64 means that the elements of this lower triangular matrix are of type N64.
And matrix N64, even though it's duplicating and repeating this N64, it just means that the underlying data that is being fed into this lower triangular object comes from a matrix with N64 inside.
Okay, so these are called type parameters, and we've seen that a bit before.
So yeah, so what's happening here is that, you know, because I'm telling it that Julia that this is a lower triangular matrix, it actually knows that these zeros are what are called structural zeros.
They are zeros that we know are in those positions in the matrix because we know somebody told us that there's matrix of lower triangular.
The lower triangular matrix you occur in various important algorithms in computer science, because of the way the algorithm works, for example, Gaussian elimination, you know that the result of this algorithm will be a matrix of this type.
And so you know that there are going to be zeros there, and then we can use that later on to, for example, solve a linear system with this matrix in a simple way.
Okay, so as a another structure matrix representation as a sparse matrices who already seen those in a previous lecture.
And when we display those in Julia 1.6, the display has actually changed.
If you remember before we saw that basically when you tried to print out this matrix, it would give you a list of positions i, j with their associated values.
But that has actually changed in Julia 1.6 to make it nicer to visualize.
Now you see them printed out in this way, just like the lower triangular matrix, with these dots representing structural zeros that we know are zero because of this.
I love that change because whenever I just saw that list of i, j and a, i, j, it never meant much to me.
Yeah, that was exactly that.
It was pretty difficult to understand the structure of the matrix like that.
But the point about sparse matrices is actually, you know, they really become interesting when the matrix is big.
And so let's for fun look at bigger matrices from Pascal's triangle.
And what we're going to look at is actually where the elements of Pascal's triangle are odd, odd numbers.
So if you look at this table, you'll see that there are some odd numbers like these fives and these 15s, but there are also a lot of even numbers.
So the question is just what is the pattern of odd and even, and that turns out to be actually rather interesting.
So let's do that.
So here I'm printing out the sparse matrix and I'm getting from Pascal with this, this, this size, right?
So here's a.
It's one more than the size.
Yeah, right.
Pascal's triangle starts at row zero and column zero.
And so here I've actually got.
Yeah, I'm numbering starting at zero.
So as I increase the size, well, we see that indeed the matrix grows, but at some particular point you see at 16 it decides, oh, that's getting too big.
And since I want to, since I have a sparse matrix that I want to visualize, it actually gives me a much more compressed representation.
So this is called the sparsity pattern of the matrix.
And now the dots actually are meaning the non zero elements.
So the zero elements are now just displayed as blank space.
And what you notice is that this pattern of odd elements is actually rather interesting.
And when I keep increasing the size of the matrix, you get this amazingly intricate pattern, which you might know is called subpinsk is triangle, or a sort of squash version of subpinsk, subpinsk is triangle.
And so why is this happening?
That's a much more complicated question that we're not going to go into, but it's just, you know, nice to observe that Julia is actually showing us this in a very nice way that's actually giving us much more information than if we just saw the numbers printed out with all those zeros.
I think there's also sort of a simple but big point that I think people sometimes miss, which is that when something gets very, very large, you really sometimes want to lose information to make it understandable.
You know, you go to something like Google Maps, right? When you go outward, you don't have the level, the street level detail anymore, right? It disappears. I mean, we've all seen that, right? And then the county level, the state level disappears as you go out further, right?
And then when you zoom in, you get more levels of detail.
So, you know, one might think that, oh, well, when you zoom out, you should just keep everything because who wants to lose information.
But it's always somehow better that like when you go outward, like less is more.
It's an obvious thing, but, you know, we don't always remember to sort of portray things that way.
Yeah, thanks, people.
Okay, so by the way, there is an alternative way to look at Pascal's Triangle with just to change the variables and then it actually rotates it so you now get Pascal's Triangle rose on the diagonal of this matrix and then some sort of looks more symmetrical in this representation maybe.
And yeah, thanks.
And I, exactly, those are the rows of Pascal's Triangle in the way that we are used to seeing them and it's basically symmetrical now with respect to the y equals x for the diagonal line.
And actually that matrix, this matrix, this new version can be produced from the previous version by doing this particular matrix multiplication of the original Pascal Triangle in the lower Triangle form.
And then you transpose that that's what this prime means it means transpose it's a flip rows and columns.
And this is an upper triangle triangular version of the Pascal same Pascal triangle and we do matrix multiplication between those.
We actually get back this same version.
And then you thought you everything about Pascal triangle new fun things come along right.
So, okay.
So, so how do we actually get Pascal what is Pascal triangle actually come from.
So, you know, we, we, we were using these binomial binomial coefficients, but you know, you probably didn't learn Pascal triangle like that when when you were in school, rather, you learn how to build up the rose.
You know this smaller version. Let's remember how do we build up this one to one row. Well, this where is this to come from it comes from adding this one that's just above it with this one that's the left of that one so it's sort of adding the north to the northwest component,
adding at each time. And that's true for each of these.
Thanks for each of these things I did. I was going to draw that out but I didn't decided not doing it but yeah it's easier to do just by sketching my hand.
And so, well, if we think about it. Sorry so you start with this initial row with just one one and all zeros and then you just follow this rule and you construct all of these previous, all these next rows, one by one you construct the rose.
And so, so if we think about how we would actually do that in a program, or would we do literally start with a vector, and then we would apply this rule to get the next vector the rule that I just said is adding the upstairs and left hand elements.
So what is that rule actually. Well, if you think about it, what we're doing is multiplying this number by one, and adding this number multiplied by one, adding those together to get the next one, and that is exactly a convolution.
So we've seen convolutions come up several times and here is another example of a convolution. Remember, let's remember that basically we're thinking of convolutions as some kind of local operation local meaning that it only deals with a neighborhood of your current point.
But we're doing the same operation at each on the neighborhood of each point right so we're doing exactly the same kind of thing to each, each element of this vector, and the output is the output of applying that little local operation to each element in turn.
Remember the pixelated Mario, which was a two dimensional convolution, and it was like that little three by three window that moved along the pixelated Mario well now we're kind of in one dimension.
And so, it's not like a, it's not a three by three, it's just, well in this case it's just a two, right it's the numbers one one which slide along any one row of this matrix and produce the next row of this matrix.
Yes, this is kind of like an image filter except we're applying it in a different way now, because we're now, you know, taking the result of the first operation and making that the new next column and then we're taking the result of that operation and applying the operation the convolution again.
So this is actually a new new application of convolutions where we repeatedly apply the same convolution operation on the output of what we just got.
And when we do that we build up literally the same matrix that we had before.
Okay, so what does this have to do with random walks, well, this is almost exactly what a random walk does so let's have to go around, so let's look at that.
Let's remind ourselves what a random walk is. So, you know, the, well, so what is a random walk it's just you start somewhere, and you move with a simple simple random walk in one dimension, you start at zero, for example, and at each step, each time click of a clock.
You move left to minus one, or you add minus one to your current position, you jump by my distance minus one with probability one half, you jump right with probability one half.
And then you keep doing that right just like you kept doing something to Pascal's triangle, just now we keep adding random random steps to wherever we just got to.
Okay, so that those random jumps and can think of as random variables as we discussed a couple of lectures ago.
Each step is a random variable, which has as I said, you know, the outcome minus one with probability one half and the outcome plus one with probability one half.
And so every time I generate a new copy of that random variable that that is very similar related to the newly random variable.
Every time I do that, I will get a different answer I'll either get plus one or minus one with randomly and when I, you know, do it some some probabilities of distribution with a histogram, I should see the approximately
half of the time I get plus one and a half of the time I get minus one.
So that's what we've already covered before but now random walks what do they do they take me that that that random variable, and now they do it a lot of times.
And then to each, you know, when when they have all those different random variables, we're now going to add up the outcomes of all those random variables to make the value of the random wall to position the random
walk to time and will add up and of these jumps to get the position and time and of the random walk or rather the displacement right so it could start somewhere that's not exactly at zero at the initial position.
And so the random walk is configured as actually the displacement of time.
The distance from the origin.
If you think about it often what we do is we have that each of these steps is actually independent of the previous step.
So if I take a random step at time 10, and then the next step at time 11 should not depend on which, you know, choice I made at time 10.
There are other kinds of random walks that are more complicated where that choice will depend on something, but we're not going to talk about those.
So basically what we have is that each step is it is has the same distribution the same chance of jumping left from right, and it's independent of all the others.
So these, this is the very common case involved with the theory, which is called the case of independent and identically distributed random variables.
So, you know, it says what it is.
I always thought that was a very long term for the term and that's why it's often abbreviated as I ID independent and identically just do the same thing without any link between them.
Yeah.
You're exactly repeat the same thing over and over again.
And they're all independent, but then for a random walk want to take all of those I ID independent identically distributed random variables and do something with them we're going to add them all up.
So what do we get we get something like this so if X I, we is the is the name that we're giving to the ice step, that's a random variable, and X one is out of the random variable course one to the first step with this particular jump probability
distribution X two at the second step etc and X and the end step, then the sum of all of those those is the position of the random walk and time and just the sum of those random variables, which you could write with this thing.
And so again, what we what we mean by the sum, we mean a new random variable, which whose outcomes are the sums of the outcomes of the random variables that we put in, and the probabilities behave in a particular way that we're going to cover in
Okay, so what we but but but there's a key point about a random walk which is that okay once we've we've got the result and time and we also want the result of time and plus one.
And how do we get that we get we take the sum of these x is all the way up to time and plus one.
And that is supposed to be actually the position and time and plus the end and the step at n plus that time and plus one.
And so you think about it, although the ex is independent, you know, a mutually independent or independent of each other of one another.
The s is the distance of the positions at time and I'm not the right so if the position, if you, if you happen to have a path and random walk path trajectory that goes very far in time 100.
The position will still be very far away, they're correlated.
So then no longer independent so that makes it actually harder to generate samples from this from this object is a more complicated object because actually what we, what we need is in need what's called the whole trajectory of this random walk.
And those are given by partial sums like this, right so the position at time one, or just, we're thinking of just random walks that starts at position zero.
Then the position at time one it's just given by the outcome of the first jump, but the position at time two is the outcome of the first jump the same outcome, plus the outcome of the second jump.
And then at the time three, it's the sum of the first those first two that we already recorded that's the point where basically sort of recording these in order.
And we are just the recorded value is going to be the previous value plus whatever the random outcome at the end step was.
So we're recording those in order and then we just sum them all up in this way.
And this process is called a cumulative sum.
So here's an example with numbers here are my random steps plus or minus one in the same way that we've been doing all along.
And now I want to add those up in this way right so the first first result is going to be minus one, then the second result is going to be minus one plus one which is zero, and the third one will be minus one plus one plus one which is one etc.
So that is what this Q sum function doesn't do.
So Q sum of this vector of steps indeed goes minus one zero one just like they did by hand and then it carries on and gets all the results into these partial sums.
And that's called a cumulative sum or prefix sum or scan only has some other names.
And you can do this with this Q sum function.
So here's another example 1234 the Q sum is 13610 which you might recognize as the triangle numbers.
I noticed the trajectory doesn't.
It doesn't have the zero steps so to speak.
Right so yeah exactly.
Yes okay that's true actually for that.
Yeah, so you can do that by.
We want to make a new vector where we start with zero, and then we're going to concatenate that zero.
That's what the semicolon does I'm making an array with these square brackets and then I'm concatenating that whatever is before the semicolon with whatever is after.
And then that gives me the whole random walk trajectory.
So it has one more entry in the vector than the number of steps.
So I got a bank account and every day I either added or withdrew $1 and I'm allowed to go negative, you know that, you know, which some bank accounts let you do.
And this is the graph of my balance on each and every day.
Yeah, that's a great analogy.
Yeah, so yes, here's just plotting that vector in time you can see that indeed it's doing the right thing.
And note that if you've come from some other languages, especially things like Python or MATLAB and you had simulated random walks there, you probably would have been told to use QM some, and that you should not do it.
For example, with a falling, but in Julia.
And that's because QM some in those languages is more efficient is more performative faster, much faster.
And so you might think that that's because QM some isn't an inherently a faster function than writing a poll, that is, that's not, that's not correct.
In Julia is just as efficient to write a for loop yourself.
And what and QM some is basically just another way, approximately, at least of writing a for loop yourself.
So what's happening is that in those other languages, it's not that QM some is fast, it's actually that for loop us what.
So basically, those languages are just preventing you from doing the natural thing, which would be to write a for loop, and then forcing you to use this other other other way of thinking about this about this, this process to use this this
function, because that function is actually implemented in C code is not implemented in Python or MATLAB at all.
Whereas in Julia is just all implemented in.
Yeah, in those languages, but in Julia, in those languages in Julia, that's just, just writing out a for loop, effectively, that you could write yourself right so I could have written a function my QM some of some data.
That's going to be a vector.
What do I need.
I need a new vector.
That's going to be, let's say the same length as the so I could do that by making sort of new be equals similar of the.
So that makes a new copy of a new vector with the same length and type of the, and then I'm going to do a for loop for iron one to the length of the new be number I is going to be where so the first one to stop
new be at one is just the same as as be at one, and then from to to the end to to the length of the I'm going to do new vi equals new be at I minus one plus be at I
and that will show that that should give me the same, the same thing.
So let's just check that this is all unprepared so
so I'm going to do QM some of steps and my QM some of steps
the same result they do that's great so my logic seems to be right.
You know, I did it just to the test of my logic.
Actually, by the way, there is a module in Julia called test for the capital T, which enables you to do these kind of unit tests.
So you can just write at test.
So you can actually sort of add these tests, your code to make sure that, you know, suppose I now modify this in some way, so I, you know, I start with 100 by mistake.
Well, when I rerun the test, it will now tell me that the test failed, there was an error and it will, and it should tell me actually where that test is failed.
In this case, it's not telling me that I think I should do with puto, but, you know, when I put it back, puto will rerun and now the test passes.
And so this is a great way to make sure that once you have some code and you want to refactor it or modify it in some way, you should write tests that tell you when you rerun the code with those tests, you know, is the code still working in the same way.
Okay, and so now what we need to do is test the performance side, I'm going to load the benchmark package.
And then we're going to do at the time of Q some of steps and you need this dollars.
Actually, I didn't mention that last time when we looked at the time.
And then we need with terminal do because we're inside puto to print out the result in here instead of in the terminal to display this this timing information in puto instead of in the terminal that I ran it from.
So Q some that built in Julia one takes 66 nanoseconds and now the moment of truth, what does my Q some take and let's see.
So it's almost the same speed right so I mean I'd be more interested if you did something a lot larger like 10,000.
Okay, so I say cool steps.
Oh, just just change steps here.
Yeah, well, let's just do steps to so steps to the random minus one one with 10,000 that's 10 to the power four elements.
And then let's do Julius built in Q some with steps to how long does that take.
And then how long does my my Q some take.
So the point is, you know, this is so easy to write in Julia this is exactly the code you know once you understood what Q some is doing this is exactly the code that you would write.
I just like looking at Q some I mean I know what it is it's sort of like a higher level of abstraction.
That's very important to have definitely.
So, yeah, so Julius Q some took this amount of time so what is these these two allocations is actually making the array by the similar command makes a new array.
And you can see that actually my Q some is much lower than Julius built in one and he was, you might think, Oh, that's because you were using a photo, and no it's actually because I need to annotate this line right so Julia.
What right so Julia by default has bounds checks on a race so when you do something like V square brackets I you're checking you're extracting this element number I from from V.
And the, but Julia will actually, you know if I do steps of 11, it will actually tell me that I out of bounds so it's doing a check every time I access this element.
Are you accessing an element that actually exists in your array or not.
And so I actually want to turn that off if I write a function like this, because I don't need it you know if I write the function correctly I can guarantee that I'm always in bounds.
Sorry, dangerous right let's let's give people warnings here.
No yeah this is dangerous.
Yeah, inbound is dangerous, because, you know, I'm having to guarantee myself that I'm not stepping outside bounds.
I don't want to be any more that's why I have to explicitly turn it off.
So this is turning off turns off bounds checks right so at inbound turns off bounds checks.
And this is dangerous.
Okay so when I do that and I rerun my, my benchmark, what happens is that it's actually now apparently.
Yeah, it's basically the same. It takes the same time.
Right so that is showing that a for loop in Julia as long as you include this bounds check to turning off the bounce check which is what QMSUM is doing internally.
Then you will actually get the same speed so that's a proof that your for loops and this higher level construct in Julia are equally fast.
Okay.
If you go and look at the code for QMSUM you probably will find that it's actually a bit complicated because it's trying to deal with cases where you have a higher, a higher dimension array like a matrix I mean, I don't think I actually want to do that right now.
Okay, so to finish today's lecture.
Let's look at a different way of thinking about these random walks so so far we've been growing trajectories of random walks.
And actually let's, I don't think you're drawing. Oh, you did draw one trajectory. Okay, I drew one trajectory so let's let's draw some more trajectories so you definitely didn't draw trajectories today.
So,
let's just
not sure why it's taking quite so long.
Oh, because it's rerunning these benchmarks every time.
Let's just comment these out.
Okay, so I'm going to just rerun this a few times and we'll see some different trajectories so let's actually add some limits on why going from minus 10 to 10 say
Not sure it's still taking a long time.
It's still rerunning these things so they didn't evaluate themselves. Okay, so now it should be fast so.
Okay, so there we have a sequence of different random walk trajectories.
Stop all starting in zero and you know some of them go off the end because I didn't put enough space on my axes.
By the way, I, I'm sort of, but we make this the circle smaller smaller, but I want to make a point.
Yeah, that's much better actually.
You know, the idea that a trajectory is a random object just like a number is a random object might be worth shouting out a little bit right that that the random object is no longer one number right it's an entire
I mean, it's practically a function right like I could look at the value of five or something exactly. Yeah exactly we are generating in some sense random functions.
And if I generate, you know, if I make it make the length a bit longer and expand by why limits again.
You can see that it actually starts looking much more like a sort of stock price like we were talking about last time. And let's let's go to 10,000.
And let's make these. Yeah.
Yeah, it's sort of literally you can think of that as a function, you know from zero to 10,000. And it's, it's sort of really a random continuous function.
And you really can't think of these, these, these things like that we're basically sampling from a set of random functions with certain properties, a certain probability distribution actually.
So what we really want to know is what is the probability distribution of these sort of random paths random functions or yeah, so if you mind blowing to realize that a, a trajectory is, is a random object.
Yeah, it's definitely mind blowing to me. Yeah. So for example, if we fix our attention that you know in this case time 2500. Just just look at this vertical line is grid line here.
Let's generate some paths. Okay, maybe 10,000 too many is too slow. Let's go back to 100. Okay, so let's let's fix our attention at a time 25, just regenerate us first not good idea.
Sorry, my limits are too.
Fix our attention at time 25 and ask, well, where is the random walk at time 25 you see that it dances around right that this is particular point let's actually maybe add that in.
So, we'll just do a scatter at 25 and steps 25.
Color that in red.
And I didn't quite get it right. It's steps 24.
It's cum sum of steps. It's cum sum of steps. Sorry. Yes.
There we go. Okay, so that's the one we're going to look at as I put like the title to be the value of cum steps of 24. So you can actually see the number.
So it's the position at time 25 equals. Yeah, okay, equals.
Okay, so now let's regenerate them again. And so you see that this point is dancing around. And now we want to ask, well, what is the probability distribution of that position.
So that red point is now a random number, right, taken from the random, it's a random number.
That is a random number at time 25. And so when we can ask what is the distribution of that random number, really an even number just just you can try it a few times but or you can think about why that's true.
Yeah, yeah, if you look at the value that's coming out, it's always even because it's the result of 24 plus ones or minus ones right 24 odd numbers.
So there's a parity argument going on. Yeah.
So we can ask what is the probability distribution of that thing. And so what is that thing, that value at height. It's, as Alan said, the sum of 24 plus ones or minus ones.
And so we actually saw what happens when you add random variables and many random variables together. If you like to the go, what we expect to happen is that we get something that is close to a Gaussian distribution and normal distribution.
And so we should expect the position at time 25 to be something looking something like a normal distribution.
Okay, now if I repeated that same experiment at time 50, while I would see a different distribution, because at time 50 I might expect to have gone a bit further away from the origin than at time 25.
So here I'm going to get a different distribution that also should look something like a Gaussian distribution or a normal distribution.
And I can do that actually at all of these times in between.
And I'll talk about that a second just before we do anything right. The mean of that distribution is sort of, I guess, obvious, I think.
Right. And I guess I would expect the, the, since at 50, I figure it can go, you know, it's wider right it can go further than 25.
I guess I expect the variance to go up in some way.
Good point. Yeah, so this is probably we're going to, you know, if you if you sort of try now to fix your attention on 15 when I generate some new samples.
Hello.
Maybe it's wiggling around more than to 25 is wiggling around.
Can we characterize, you know, can we quantify all of these, these, these qualitative arguments. That's what we're going to try and do.
So we could literally take, you know, literally generate a lot of paths and literally calculate these distributions.
We could do that, or we could use a different approach, which is what we're going to do. So what's the different approach.
We're actually going to think about, well, if I have the probability distribution at some time, how can I get from that probability distribution to the probability distribution at the next time.
So let's call P I P sub I P with a subscript I and a superscript T.
So I'm going to use T for the time so that it's easier to understand.
So I'm going to call that the probability that ST, which is the position of the random time T, the probability that that thing is equal to I.
So I is the position that we were just drawing and T is the horizontal time.
It was just 25. We were focusing on T equals 25.
Exactly.
So we're wondering, and I said that I has to be even or else the probability is zero.
Right.
Exactly.
So, exactly.
So S 25, we wanted to know, well, how likely is it that S 25 takes on the value 10, for example.
So for example, so how likely is it that it takes on the value nine?
It has probability zero because it's always even.
How likely is it to take on the value 10?
It's some number that we want to calculate.
And the sum of all of those numbers over all the values of I over all the possible endpoint positions at time 25 has to be equal to one.
It has to be somewhere.
So if I'm tossing 24 coins and getting plus one or minus one, what is the chance that I'm going to add up to 10?
Exactly.
Yeah.
So that's, you know, yeah, exactly.
So let's collect all of those values together, all of those probabilities for different values of I.
That should, as I said, sum up to one.
And those, that collection of probabilities for all the possible heights is a vector, actually.
So let's call it the vector bold face P at time T.
This is not a power.
It's just a label saying that it's on time T.
I could be negative.
Yeah.
Yeah, these things have a weird indexing.
They can go from sort of minus 10 to plus 10 or something.
Yeah.
The I, the value of I, thanks.
So what happens at the next time step T plus one?
Well, at the next time step, if we were to take the histograms, we would get a different probability distribution, which is called, you know, P vector T plus one.
So how are those two vectors actually related?
Well, if you think about it, how, so let's fix our attention on 25 on position and not 25.
Sorry.
Let's fix our attention on now on height 10 at time T plus one.
Right.
And the question is, that's not, that's not right.
Yeah.
Well, let's say that.
Yeah.
So how could I arrive at position 10 at time 25?
Well, I had to have been at position nine or position 11 at time, the previous time, 20 times 24.
And then I had to jump from those positions in the correct direction.
So basically I have, however much the probability was at that point, times the probability that it jumped in the correct direction.
So that I can write down in this very compact formula.
What does this say?
It just says in a formula, what I just said in words, the probability that I'm at position I at the next time step T plus one.
Well, I had to be at the previous time step T, I had to be a position I minus one, and then jump towards position I, which I'm due with probability a half, or plus is or I had to be at position I plus one and jump towards position I backwards.
So this is a formula that tells me if I know what the probabilities are, the vector of all the probabilities at position at time T, how I get the new vector of probabilities at time T plus one.
This isn't so it's telling me how these probability vectors evolve in time.
And this equation is often called, so it's a time evolution equation, discrete in discrete time and discrete space.
And it's often called the master equation, which is a really terrible name, but often what it's called, I think that dates from the 30s or something.
And it, yeah, as I said, it describes this time evolution of probabilities.
And so if you think about it, what is this doing? Well, it's actually rather similar to something we already saw at the beginning of the lecture, which is Pascal's triangle.
This is almost exactly the same as Pascal's triangle, except for this factor of one half.
But we're adding two things together from the previous row. Basically, if you think of these as rows, or I guess they were columns, if you think of them as columns, and you rotate your head now they're rows.
Basically, each column, you get from the previous column by applying the same kind of convolution that we applied in Pascal's triangle.
Except now, we also are missing the one at position I right so before we took the one just above and the one to the left of that now we're taking one to the left and one to the right and we're missing out the one in the middle.
So there's another convolution. And so these probabilities are actually evolving just like the rows of Pascal's triangle.
Okay, so we can implement that in Julia. So for example, let's write a function called evolve, which just does this calculation. So here's the calculation.
So again, we're taking in, we're supposing that we have a vector of probabilities at time, at some time, and we're going to make a new vector of probabilities with again this similar function.
And then we're going to loop through all of the whole system and do this rule that I just wrote down to the time you know the probability at position I the new time is a half of this sum is convolution we could write that as a convolution.
Just like we did in images, we have an issue, a problem, which is what to do with the boundaries.
So what happens at the boundaries if some probability wants to jump or if you're random walk, you know, thinking about the random walk if you're actually going to fix some boundaries of the system, why do we need to do that now, because we're, we're actually talking about the probability of being at each point in space, and we can only store a finite number of them.
So we actually have to impose some limit on the size of the system. And then we have to worry once we've imposed that limit, you know, with when we were simulating trajectories of a random walk we didn't have any such limit because it could just wander off as far as it went.
But now we're actually trying to store information at each of those points, we really need to stop and say, oh, you can only go up to this distance.
And so we need to decide what the random walk will do when it hits the boundaries. So it could do different things, it could bounce off and come back.
It could go round and sort of jump off one side of the system and come back on the other side as periodic boundary, when it when it bounces off, it's called a reflecting boundary.
And what we're doing here is called an absorbing boundary.
So if we're just setting the value there to zero, the value of the probability, and that actually corresponds to probability leaking out of the system.
So when you, any probability or random walk that hits the edge just disappears, just vanishes into sort of an infinite sea.
You can think of my system being in contact with the sea, and I'm thinking of this random walk as heat moving around and when it hits the sea it just sort of dissipates into the ocean.
The problem is your sum of probabilities will no longer be one.
Right, so this has a weird property that the sum is no longer one because some of your probability, a part of your probability leaks out of the system.
Right, and you can think of it as like the piece that, you know, the remainder that would bring you to one is now like the probability of being in the sea.
Yeah, it's the probability, exactly, you have some probability that by this time you have already escaped, and that a probability adds up to all the other probabilities to give you one.
So yeah, as I said, this is just a convolution now the kernel looks like this.
So, you could write it like that.
Okay, we've done the probability that boundary conditions now we have another thing which is the initial condition what is the initial distribution where do these random walks start or what is the probability that they start at different places.
So if they all start at site at position zero and time one, then the probability of being at position zero is one.
Everything is there certain with certainty, and the probability of them being anywhere else at any other site I is zero is impossible.
And so we just set up a an initial condition that looks like that.
So, you know, we can just call this function condition with 10 other say 11 positions and you see that there's a one in the middle and zeros everywhere else.
And then, so now we can just run the time evolution.
And so in order to do that, we, you know, here's a here's a function that does that.
So, what are we doing, we're passing in an initial vector p zero, which is the probability distribution at time zero.
And then I'm making a vector that contains that object.
So that is actually going to be a vector of vectors.
Right. So if I have sort of P zero is initial condition at with 11 cells.
I already have a P zero somewhere else here.
Sorry. Yeah, so there's P zero.
That's the initial condition. In this case, I put 101 cells.
And it's taking ages to run.
Maybe I'll just point out that your micro century is sort of coming to an end.
So if I put square brackets of P zero, I actually get a vector.
And inside that vector, it has just one element, which is itself a vector.
So that is what I went by a vector of vectors, the vector containing vectors.
And then, so let's run this time evolution.
And what I'm going to get back is a vector of vectors where the vector at time at the vector number element number four, for example, is the probability distribution of random
walks at time four.
Then I can visualize that in various ways.
I would jump to the 3D visual at this point, if you don't mind.
Okay. Yeah.
So here's just, this is the time evolution of the histogram.
Spreading out.
So this is a spreading process.
And then we can show this, this in three dimensions.
Here we have time on this axis.
So we're starting with everything in position one.
And then you see that it's spreading out over time as we go along.
And we can also see that with the random walk trajectory, which is this line on the bottom.
And every time I generate a new trajectory, you can see how it moves around.
And so these bars are actually telling me how likely it is for this blue dot to be in each of these positions.
Okay. So that's the, so, so you want to run a few more random walks,
just so people could just work.
So you think the final lines, it's kind of fun to watch, watch that.
If you fix your attention, for example, on this blue dot, you should see that it visits different sites with these
proportional to these probabilities.
So it's very unlikely to be kind of far away.
And it's very likely to be to end up and not very likely, but it's more likely to end up in the middle.
So hopefully that visualization makes it kind of clear in what way, what does it mean to have a probability distribution, right?
We're drawing each of these histograms at that time is a probability distribution for the position of the random walker at that time.
And averaged over all of these paths.
And then we have a probability distribution for each position.
And I hope people actually could see that there's Pascal's triangle is basically there in a sense.
You know that, I mean, if you want to think of this as one and one, one, literally divided by two, so it's half half and this is one, two, one, right, divided by four, right?
And this is one, three, three, one divided by eight, right?
And, you know, if you recognize numbers, continues down.
And so this is in some sense a kind of picture of Pascal's triangle, you know, normalized by two to the end and each row.
Right.
Yeah.
Okay, so, you know, random walks are, as we said last time, very important and very interesting and actually contain a lot of beautiful mathematics.
Pascal's triangle comes back to suddenly without us kind of expecting it.
And yeah, so there'll be some more questions about that in the homeworks.
Yeah, I guess we'll do this next time. I have some fun stuff to say, but yeah, we'll have to wait till Wednesday.
Yeah, okay, so see you all next time.
