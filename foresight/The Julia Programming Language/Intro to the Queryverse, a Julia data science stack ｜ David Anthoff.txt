2.5cm
3.5cm
4.5cm
5.5cm
6.5cm
5.5cm
6.5cm
7.5cm
8.5cm
9.5cm
9.5cm
10.5cm
11.5cm
12.5cm
13.5cm
5.5cm
6.5cm
7.5cm
8.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
9.5cm
Ole, పర్వస్, గినా ఆసి Buzz
chose
Yay
差 k highlighted
so you're probably better off if you just follow along on Julia Box.
Okay, so what I want to do next is I want to run through my end-to-end example
and show you that.
And for that I'm going to switch over to a Julia notebook.
The way I'm going to do this is that I will type things into a blank Jupyter notebook
and you can follow things along on the Jupyter notebook that's uploaded on Julia Box
that has all the code in it already.
I'm mostly typing things here because then it slows me down
and I just don't execute one cell after another
and I'm too fast for you to follow along.
So we'll start out by loading query-vers,
Vega data sets and index tables.
So those are the three packages that we're going to use today.
Query-vers itself is a meta package so it pulls in a whole bunch of other packages.
It just makes it easier to get started so that you don't have to load all of them manually yourself.
We're going to start out by looking at a data set called CARS
that we load from the Vega data sets package.
So the data set function here is a function that just gives you access to a whole bunch of data sets
that the Vega folks have put together.
So let's load that.
So the data here comes in tabular format.
So this is some data about CARS.
We have different columns so this is sort of a typical way data is structured
in a tabular data manipulation environment.
So if you want to analyze this data set and you just get started,
you just got this data set, you haven't really looked at it,
one great way to start out with this is to get a visual overview of what this data set looks like.
And so here we actually come to a feature that I haven't really widely announced previously
and it's also one of the features that won't work if you are on Julia Box.
So if you want to be patient now if you follow along on Julia Box,
you just watch the video and then if you try to do this locally on your machines,
you should be fine.
So I'm going to do the following.
I'm assigned the CARS data set to the local variable CARS.
And so now I'm starting out with CARS and I'm going to use the pipe operator here.
We're going to use that a lot in this tutorial.
And I'm going to pipe it into the Voyager function.
Voyager is a UI toolkit that was developed at the University of Washington
that I've wrapped as part of the query verse in a Julia package.
So what does that do if I execute that?
It opens up a new window, a graphical UI that you can use to explore tabular data sets.
The default that you get if you pipe a table into a Voyager window
is one plot for each column that summarizes the data in this specific column.
So the first plot here for example summarizes the cylinders column
and it shows us how many rows we have for each distinct value that appears in cylinders.
The tool automatically picks a plot type
that is appropriate for the data type and the data that is in a specific column.
So for example here we have a plot for the year column
and Voyager automatically recognizes oh this is time data
so it's automatically creating a time series plot here.
So as a first start this is sort of great because it gives you sort of at one glance
an overview of your data set.
I'm going to show you one more thing and then we'll move off from Voyager
but I encourage you to explore that tool because it's really really powerful
for exploratory data analysis.
So what we're going to do next is I'm going to pick on the left here in this column
you can see a section called wild card fields
and there's one wild card fields called quantitative fields.
So that you can think of is a proxy for all the columns in our data set
that are numbers and not for example years
or a time dimension or nominal types of data sets.
So I'm going to take that and I'm going to drag that onto the x axis of my plot.
So what it is now creating automatically is one plot
for each quantitative column in my data set
and then it's plotting that.
So now if I take the same quantitative fields and drag it onto the y encoding channel here
I'm automatically creating scatter plots of all the quantitative fields
against each other.
So I can very quickly use this tool to get a sense of
where do we see correlations in this data set,
what sort of the structure of this data set.
So I think this is a really powerful tool
especially at the exploratory when you start to explore data sets.
So I'm going to close this again now
and we're back to our Jupyter notebook.
And so anyone who had to pause at this stage
because you were on a Julia box you can rejoin this now.
So data exploration in a UI is great
but sometimes we also just want to manipulate a data set with code.
And for that the query verse provides lots of what I call query operators.
So these are verbs that manipulate input data in some form.
So I'm just going to show you in this sort of run through
and to end example one of those.
So we're going to take the cars data set
and then now I'm going to pipe it into the filter command.
So what does the filter command do?
It applies a filter or checks the condition for every row of the input data set.
And only if that condition turns out to be true
will it return that row in the output set.
So here I'm going to say filter.
Inside the filter I write my filter condition in the following way.
I use an underscore to reference the current row.
So this filter condition will be applied to each row sequentially
and in the condition I can refer to the current row by an underscore.
Then I write dot and now I can extract the column that I want to check something about.
So here I'm going to check that the origin column equals USA.
So if I run this I get a new table
and in this table now we filtered out a whole bunch of rows
and we were only left with the rows where the origin column is actually USA.
So this part of Queryverse essentially covers
the kind of data manipulation story that you would get with something like Deplier.
And there are many more query operators and we'll learn about them later in this tutorial.
I'm going to copy this code and paste it into a new cell
to briefly touch on another aspect of Queryverse.
So data manipulation is one thing, data visualization is another thing.
You also probably want to save your manipulated data into some file format.
So what I'm going to do here is I've piped the data from the cars data set into the filter condition
and now I'm going to pipe the filtered data into another operation
and here I'm going to pipe it into the save function
and I'm going to save it as cars.csv
So I'm going to save it as a CSV file, the manipulated data here.
So let's run that.
If I open, I'm in JupyterLab here,
if I open my file browser here
we can now see that in the folder I have a CSV file, US cars, I can open that,
look at that and you will see that there are only the US cars in this data set.
All right, so the Queryverse provides a very rich tabular file I.O. story
both for saving and for loading files.
So let's exercise it a little bit, let's load this CSV file again.
So I'm going to say load US cars.csv.
So you can see here that I'm using save and load functions that are very generic.
I'm not even specifying what file type I'm going to load here.
The way this works is that this hooks into a package called file.io
that provides a uniform file.io API for lots of different file formats.
So that provides a very uniform and nice interface
for interacting with different file formats.
So here I'm going to load this CSV file that I just saved
so we can see it looks the same way.
All right, so let's copy that.
So what's the final thing we might want to do with the data set here
and where Queryverse can actually help you?
So we've loaded a tabular data set here.
Maybe we want to plot it actually.
So this touches the new plotting features that I've added to Queryverse over the last year or so.
It really only came together last week, but it was almost a year of work.
So first of all, we pipe a data set into our plotting command.
The plotting command is a macro.
It's called Vigalite plot.
The first argument we pass here is the type of mark that we want to put onto this plot.
So Vigalite is a plotting package that's very much in the spirit of a grammar of graphics API.
So if you come from GG plot, it will probably look somewhat familiar.
And so the first thing we have to specify here is that we want to plot points.
And now we have to what the Vigalite folks call encode properties of these points
with the values from the data set that we have passed in here.
So here I'm going to say I want these points positioned so that the X position of each point
is taken from the miles per gallon column.
And the Y position for each point is taken from the weight in LBS column.
And then I want to split things up by color, and I want that to come from the cylinders column.
Now note that cylinders here in our data set is a numerical data type.
But I actually wanted to be shown as if it was a nominal or categorical data type in my plot.
So here I'm using what's a shorthand for this.
I'm specifying the name of the column as a string, and then in addition to the name of the column
I'm saying colon n, which is a syntax here for saying actually treat this as nominal data.
Okay, so let's plot this and run this.
And there we got a scatter plot with a legend for the cylinders that in this case here looks not perfect
because there's some scaling issue.
I'm running everything here on a highly scaled Chrome instance.
If you plot this with your normal default settings in your browser
it should all look very pretty.
In general the whole plotting story here should just work in lots of different clines.
So I believe that this is a very robust solution at this point.
Okay, so that was sort of my end-to-end example.
I didn't do much here, but I did touch on a whole bunch of features that the query verse provides.
So we looked at file.io, both loading and saving data.
We looked at manipulation of data with query itself.
We only used one query operator or a filter, but we'll touch on a lot more later on.
We looked at an exploratory data visualization tool, the Voyager tool,
and then finally I showed you how you can plot things with Vega Light.
So that kind of sort of covers the broad landscape of what the query verse provides in terms of features.
And now the rest of the tutorial I'll dive a lot deeper into individual aspects of that
and explain how you can use these individual features in a more rich way.
So we'll start out here with the file.io story
and see how that looks like.
Let me quickly switch over to my slides.
So file.io is the next topic we're going to cover.
So the file.io story in query verse is pretty broad.
So the next slide here shows you all the different file formats
that are integrated into this uniform load and save pattern
that I've showed you already with CSV files.
But this actually extends to many other file formats.
So currently you can load CSV, feather, Excel, both the new and the old formats,
data, SPSS, SAS, parquet and bedgraph files.
I actually don't know what bedgraph files are.
That is a contribution from the community, but that also works.
So all of those file formats sit under the same uniform load function interface
and interact with the whole query verse.
There's also support for saving files in CSV and feather files and bedgraph files.
The saving story is a little less complete at this point.
I indicated here that I have plans to support saving for Excel
and the various stats file formats.
And I'm also planning to add support for FST files.
What planned here means is that I know which packages I need to use
and they exist to implement that.
So it just requires a few days of my time to finish that off.
If there's a blank cell here, it means I currently don't have a plan
how to do this because it would require much more work
than I'm willing to put into that right now.
So parquet file writing would really require
that someone else I think steps up to the plate.
So that's what the query verse file.io story covers in terms of file formats.
The interface for loading files is very simple.
It's this load function from the file.io package,
but it gets re-exported if you load query verse.
So it's just available here.
So you just call load, you pass a file name as the first argument,
and then you can pass optional arguments and keyword arguments
depending on the type of file you're loading.
So if you're loading, for example, a CSV file,
you can specify what the limiter character should be
if you specify an Excel file,
you can control what sheet should be loaded and so on.
So there are the precise optional and keyword arguments
that you can pass in here really depend on the type of file that you're loading.
What you get from a load function is a table.
So all of these file formats are tabular files,
and so you get back a table.
I'm calling this a table.
It's actually not a data frame.
It's also not any other specific table type,
and we will later in the tutorial I will explain to you
what that actually means, what the type, the Julia type,
what Julia type is returned from the load function here.
For now, just in the abstract thing,
load, return, if you load a file that's a tabular file,
it returns a table in some magical Julia type
that we haven't discussed yet, what type is used for that.
So let me just scroll over here.
So what I'm going to do next is I'm going to just show you
a whole bunch of examples of how this works and that it works.
The way I will do this is, I'm opening my file browser here,
there's a subfolder called data here in the tutorial materials,
and it has the same data set that we've been looking at so far,
the CARS data set saved as a CSV file, as a DTA file.
Now, I always mix up which of the stats packages format that is,
so this is one of the, it's either SPSS, SAS, or STATA,
and I forgot which one it is.
I have it as a feather file, I have it here as another statistical format,
I have it as yet another statistical file as an Excel file,
from all of these different files here.
I just say load, the file path to where, to the file I want to load.
I get a nice preview here of the table that I've just loaded,
tells me how many more rows there are that I'm not seeing at this point.
And then for the other file format, it just works exactly the same way.
So I say load data cars.feather,
so now I'm actually going to load the feather file from disk.
Same thing, I get a nice preview of the table here,
it tells me how many more rows there are.
Now, Excel files are interesting, so I'm going to load that one next,
so it's data cars xlsx.
Now, an Excel file can have more than one table in it,
because it can have multiple sheets in there.
So when I want to load an Excel file,
I actually need to tell the loader which sheet I want to load,
because otherwise it doesn't know that.
So in this case here, this Excel file has one sheet that is called cars,
so I'll just pass that as the set algorithm
that automatically tries to detect where on an Excel sheet there is tabular data,
and then it automatically extracts the header and so on.
But sometimes you want to have a little bit more control,
you might want to exactly specify which cells you want to load,
so let's try that.
So here I'm going to say data cars xlsx,
and then here I can also pass in Excel range reference syntax.
I want to load data from the cars sheet,
but I only load the cells C1 to G41.
So I'm not loading all columns,
I'm skipping column A and B,
and then I'm skipping some columns at the end,
and I'm also not loading all rows,
I'm only loading row 1 to 41.
I can do that, and I get a table back
that is now a subset of the table that is actually stored in this Excel file.
Okay, so that was Excel files.
Let's look at the various statistical files that you can load from,
so here carsDTA works as you expect,
the same data again,
we can load data cars SAS7BDAT,
so that's another file format.
So at this point you might think this is getting a bit boring,
why is he showing us every single one of these file formats?
The reason is that it was a lot of work to do,
so I now want to show it off.
Okay, the last one here is the SAV file format,
and that also works.
So the vision here really is to have a very simple API for users
that ideally works with all types of file formats,
and as a user you don't really have to think
which package to have to use,
what's the precise syntax here.
For the file formats where there's one table per file,
you can essentially use this load function
without thinking much about it.
And then for other files that are a little bit more tricky
like Excel files or maybe CSV files with custom formats,
you'll have to look at some of the options,
but it's still a uniform API
which is nice for this kind of exercise.
So this was loading files,
so let me next show you
how the tabular file IO story works for saving files.
So I'm back to my slides here.
There are two ways in which you can save tabular data
into files in Queryverse.
In all cases you will use the SAV function.
The first format is that you call SAV
and you pass in as the first argument,
the file name that you want to save something as,
and as the second argument you pass in the table that you want to save.
Now again, I'm using table loosely here,
it doesn't have to be a data frame,
it just has to be something that looks and feels like a table,
and we will later on learn what that actually means.
And then you can also again pass in optional arguments
that control formatting aspects for some of the CSV files, for example.
So that's the first option,
so let's exercise that one,
so one second.
So I'm going to start out with the CARS variable again.
Remember that is the one we originally loaded from the data set function,
so it's a table here.
And then we can easily save that out,
we can say SAV CARS 1 CSV CARS.
So I'm passing in the file name as the first argument,
and then as the second argument I'm passing in the variable CARS
that holds a tabular data set.
So I do that, and I look at my files here.
Let's refresh this.
I need to go to this folder here.
I can open it and it did get saved as a CSV file here.
So that's syntax number one.
Syntax option number two is that you use the piping operator
that I've shown you previously already.
So in that case, you start out with something that returns a table.
So this could be just a variable that holds a table.
And then you pipe that table into the SAV function.
You pass in the file name as the first argument,
but you don't pass in the data again,
because it has been piped into this SAV function.
And then you could put in the optional arguments after that.
So let's try to see how that would look.
In that case, we would start out with the CARS variable,
and we would pipe things.
I'm going to use a new name here so that we don't overwrite things.
So here I'm piping the CARS table into the SAV function,
and I'm saving it as CARS2.csv.
And if we look at our files here and refresh this,
then yes, that also works.
It saved it as a CSV file.
Okay.
I already mentioned that for CSV files,
we can specify some options.
So I'm just going to show you one example of that.
So I can say CARS.
I pipe that into CARS3.csv.
But now let's say I want to use a different delimitered character,
so I can just say delim equals semicolon here.
So if I do that, it will actually save things out
into a CSV file but use a different delimitered character.
Let's see whether that worked.
So I'm opening my file browser here.
I'm opening CARS3.csv.
And as you can see, the default JupyterLab CSV viewer
tried to show this with a comma as a delimiterer.
That doesn't look good.
But if I change the delimiter here to a semicolon,
then things look as they should
because it actually used a semicolon
as the delimiting character between columns here.
Okay, so that shows you how you save things as a CSV file.
And then you can obviously also save things as a feather file.
So CARS5.feather saves everything into a feather file on disk.
I don't have a viewer for feather files,
so the only thing we can do here is load it from disk again.
But it should just all work.
You can, of course, directly combine load and save operations
if you want to convert some files.
So let me show you one example of that.
So I'm going to say load.
I'm going to load the CARS.sas7bdat file.
So this is one of these file formats
that is used by one of these statistic packages.
Then I'm going to save it out as a CARS5.csv file right away.
So I'm piping the table that was loaded from the statistical file format
into a CSV file.
And that, of course, works because the load function returns a table.
The only thing I have to pipe into save is something that returns a table.
And then everything works.
It's probably worth pointing out here
that in this example of loading and saving,
the data actually gets never stored in a data frame
or any of the other tabular data types that you might be familiar with.
So we'll learn later a little bit how that works under the hood.
But an example like this one here completely bypasses any intermediate table type.
It essentially streams data from the statistical file format
right into the CSV file format.
Okay, let me go back to my slides.
So that covers the file.io story.
I will post after this lecture a link to a write-up
where I describe all the optional options
that exist for the various file formats in one place.
So there's a documentation for the query verse that is sort of starting.
So right now the documentation story for a query verse
is that each individual package has actually quite decent documentation.
But I'm currently writing a unified documentation
that covers all aspects of all the packages.
And the chapter about file.io is finished.
So I'll post a link to the file.io chapter later on
where you can get one nice documentation
that covers all the different file formats and options that you have for that.
So the next topic I want to cover is the question,
what is actually a table?
So I was slightly vague when I talked about the load function.
I just said it returns something that is a table.
And then I said when you pipe something into the save function
I said it has to be something that is a table.
But what does that actually mean?
Is that sort of a specific Julia type that you have to use
or how does that end out?
So that's the next section that I'm going to cover.
I'm going to talk about the definition of tables in the query verse.
Okay, so what is a table?
So one of the core things that query verse brings to the table
is a very simple abstract interface for a table.
So it's a table interface that is defined in the package table trades.
So how do you think about this?
You think about this as a convention or a definition of an interface
that says if you are a type and you are a table type,
if you implement the following few functions on your type,
then you confirm with the table interface that's defined in table trades
and then all the other packages that also are based on this table trades interface
can easily interrupt with you and can consume you and so on.
So I think there has been discussion in Julia Land for a while
about an abstract table supertype.
The stuff defined in table trades kind of plays that role
in that it is sort of a very minimal interface
that different packages can use to talk about tables
and then interact with each other, but it's not a supertype actually.
So the way it is solved is I think a more general approach.
But long story short is table trades defines a very primitive
and basic interface for what a table is.
The interface has also, I've talked about that in the past as an iterable table,
so that's the name of the interface.
So those are really just the same thing here.
So then the question is which table types that exist in the Julia ecosystem
are actually integrated with this story and all of those packages.
So all of those packages in some form either have a type that is a table.
So data frames for example is a table or index tables is a table
or they can load tabular data.
Some of them are file.io things.
So all of them in some form have produce or consume tabular data
and all of them are integrated with this table interface
that's defined in table trades.
So what that means is that all of these packages can work with each other
even though the individual packages don't know anything about each other.
So you can for example plot an index table or a JuliaDB table with Gadfly
even though Gadfly knows absolutely nothing about JuliaDB or index tables.
Or you can use a data frame and visualize it with Voyager,
the UI toolkit that I've shown you.
Even though data frames knows nothing about Voyager
and Voyager knows nothing about data frames.
It also means that Vega like the plotting library
can consume any of the various tables that are defined in any of these packages.
So I think this is pretty comprehensive.
I think I counted it before here, before I started this.
It's like 26 packages right now.
I'm not aware of any other table types that are floating around right now in JuliaLand.
If there are, please get in touch with me.
It's very easy to add this integration.
And if you add that integration to your table type or your tabular interface
then you immediately sort of can interrupt with all the other types here,
other packages in here.
The way this is implemented is sort of interesting.
So I'm going to color this a little bit.
All the green packages have what I call native integration of this table trades interface.
So the package authors of these green packages decided to take a dependency on table trades.
For example, index tables by JuliaCompute.
The pandas package that is a wraparound if we have it in the package itself.
So that's about half the packages here have sort of a native integration of the table trades interface.
All the other packages for various reasons at this point do not depend on the table trades packages.
And so instead what I did is I have a package called iterable tables
that tax this interface onto the types in those packages.
So that means, for example, for data frames,
that I actually provide the integration of data frames with the table trades universe.
And that integration code lives in the iterable tables package and not in data frames itself.
For you as a user, this really doesn't matter because when you load query verse
or any of the other packages from my stack, the iterable tables packages automatically loaded
and everything will just interrupt.
Cluster all the different packages that interrupt here by the type of thing that they do.
So we have in green, we have various different table types that are integrated here.
So I think this is actually one of the great things about the Julia system
that we don't just have one data frame type.
We have different tabular data types that specialize and optimize for specific situations.
So index tables adds indexes indices to tabular data.
Temporal and time series have tabular data types that are specifically meant for time series work.
So they have features that make a lot of sense in a time series setting
but not in a generic tabular data setting.
Pandas is a wrapper around the pandas package in Python.
So I think that's actually a strength of the Julia data ecosystem
that we have a variety of different table types.
But of course then you need a common interface so that for example you can plot any of these tables
and that's what's provided here.
So all the yellow packages are IO packages.
So some of them are files, some of them are database IO packages.
The blue packages are visualization or UI packages.
So we integrate with GAT fly and stat plots for example.
Then there's Vega light, that's the sort of native package for plotting in queries.
But you can use GAT fly or stat plots if you prefer those.
Table view is a package that provides tabular viewing of tables in Juno.
So that also works with any of the tables that are defined here.
Okay, so let me briefly hint a little bit at how this works under the hood,
how this interface actually works.
So the first thing is there's no abstract super type.
So there's no abstract table that all these packages have to derive from.
I think that's a really key design point here because otherwise you will never get everyone to buy in
if you sort of come in and say oh if you want to comply with this generic tabular interface
the first thing is you have to add this type into your type hierarchy.
That's sort of too invasive, right? So that's actually not how it works.
The definition is actually the interface is very minimal.
It is really just if your type can iterate named tuples, then it's a table.
And there's a few minor twists to that but that's sort of the broad story.
If you have a type and it can iterate named tuples, so one named tuple for each row
and each field in the named tuple is a column, then you're a table.
So this is very simple but it's actually nice and fast and generic
and that's the definition of this interface.
There's actually a couple of optional interfaces on top of that
that can make things a little bit faster in certain situations but they're optional.
So if you're using two packages that happen to both implement one of these optional interfaces
things might get faster but you won't even notice other than things being faster, right?
As long as they implement the iterator of named tuples interface
everything will work with each other.
The interface is very stable. So it's very minimal and it's very stable.
So it hasn't changed I think in two years or something like that.
I have no changes planned. It's essentially done, I think.
So there will be a little bit of work to make it compatible with Julia 0.7
but that's going to be very minimal.
Other than that it's just, you know, it's there, it's being used
and it's not going to change.
I think that's sort of an important message here
because sometimes in Julia land things break every half year.
I try very hard to not do that in Queryverse.
I think that is actually one of the most difficult things right now with Julia
when you write code and half a year later all the packages have made changes
and broken everything so I've not done that with Queryverse at all
and I'm pretty, I'm very committed to maintaining backwards compatibility.
And if you want to have sort of a low level basic interface for a table like this one here
then it really needs to be stable.
The interface is fully documented. I can post a link there.
It describes exactly what you have to do if you want to plug into this ecosystem
but really from a user's point of view and maybe the audience for this tutorial
the details of how this is implemented probably don't matter.
The main thing is that all of these packages work with each other smoothly
and you don't have to worry about that.
Okay, so that was my description of what is a table
and how does that work in Queryverse?
Oh, okay, I wanted to show you a couple of examples for that.
So we're not moving on to the pipe.
I'm going to show you a couple of code examples of what that means in practice.
So let's actually load some data from a file again.
So I'm going to load the cars.csv file again
and then I'm going to pipe it into the type of function
to just get a sense of what Julia type is actually returned here.
Okay, so the Julia type that is returned here is a csv file type.
So this is not one of the table types that you're probably familiar with
but it turns out it is a type that implements this iterable tables interface.
So it can iterate name tuples, one for each row.
So that's enough to interrupt with everything else.
Let's look at what we get if I load a feather file.
Cars.feather.
What's the type that's returned here?
Well, it's a feather file type, right?
That's the Julia type.
Again, the feather file type implements this common table interface
so it can interact with everything.
Remember, originally we used the dataset function from the Vega dataset package.
Let's look at the type that's returned here.
Even that is a custom type.
It's a Vega dataset type.
It's not returning a data frame, right?
It's returning a very minimal type
that just happens to implement the common table abstraction here.
So then, what do you do with something
that implements this abstract table interface?
We've already seen that you can pipe any of these into the save function
and save things in various file formats.
But you can also just pass any of these types
into a constructor for one of the table types that we have in JuliaLand.
So for example, I can say data frame load data cars feather.
So what's happening here?
It will load everything as a data frame.
So this load call here is going to return an instance of type feather file.
The data frame constructor is able to consume any type
that implements the iterable tables interface
and initialize itself a new data frame from that.
So here we've loaded things from a feather file into a data frame.
We can also use the pipe for that.
We can say cars.feather
and then we can pipe it into the data frame type
and that will construct a new data frame
with the data loaded from this cars feather file.
But I don't have to pipe it into a data frame, of course.
I can pipe it into something else.
So what I'm going to do next is I'm going to pipe the feather contact
into an indexed table.
So table is the function from indexed table that creates an index table.
So index table is a table type that was created by Julia computing
that powers JuliaDB.
So that also works, right?
So I can load any of these things into an index table.
And this works with any of the tables that I've shown you in the picture previously.
I can pipe it into a table type, I can pipe it into one of all interops.
I can also do silly, I can say data feather file
and I'm piping it into an index table
and then I'm piping it into a data frame.
So how does this work?
At this stage here, we're loading a feather file.
We're converting it into an index table
and then we're passing the index table to the data frame table.
Now, because the index table also implements the abstract table interface,
we can construct a data frame from that, right?
Okay.
So I don't know why you would want to do this, but you can, right?
So here we convert first to an index table and then we convert to a data frame.
All right, let me see what...
It obviously works the other way around as well,
so I can say load data cars feather
and first convert it into a data frame,
then convert it into an index table.
You get the same story here.
So I can...
Let's just take this part here.
And so I'm just going to show you a couple more examples
of how this works sort of across the whole stack.
So here I'm going to...
I'm loading the data from the feather file.
I'm converting it into a data frame
and now I'm going to plot the data frame.
Okay, so I'm going to say plot, do a point plot, x equals
horse, power, y equals acceleration.
So in this case here, I'm actually piping a data frame into the plot command
and then of course it works because the data frame is a table
and our plot command doesn't care what type of table you pipe in.
Okay?
I can do the same thing.
I'm just copying this code here.
I can convert it into an index table first,
so a different table type
and then I can pipe the index table into our plotting function.
So all of these table types work well with each other.
Now in this case actually it's silly to even create
an intermediate table, a table type,
so we don't have to convert it either into a data frame
nor an index table.
We can just pipe the data directly from the feather file
into our plot command and that also works perfectly fine
because what we get returned from the load function
is something that is a table
and therefore we can pipe it into the plot command.
So the story here in Queryverse,
even if you're not interested in the query manipulation story itself,
you know, it's handy.
You can do something like I can load a statistic file
into an index table.
So I'm going to do that
and then here I'm going to convert the same table,
this index table into a data frame
and I'm storing that in a variable called df for data frame.
So now I have a variable it that is an indexed table
and I have a variable df that's a data frame.
Well, let's just go back to the example
of the interactive explorer tool, the Voyager tool.
So this will not work on Julia Box.
I can take the indexed table and pipe it into Voyager.
Okay, and it opens up here.
Or I can take the data frame
and I can pipe it into Voyager and it shows up here.
So this common table interface really enables us to
use whatever table type we want to use in a given situation
and it doesn't really matter that much anymore.
So which table type you use is in some situations
at least becomes almost a secondary consideration.
And then of course all the file saving functionality
works as well.
So you can, for example, save an indexed table,
so the type from Julia Computing as a feather file.
You can save the data frame as a feather file using this.
So it all just works with each other.
So let's go back to our slides
and talk about the next topic here.
And that's the pipe.
So we've covered that a little bit,
how you pipe things from one stage to another.
So I want to expand a little bit on that.
Let me just take a look at my notes
at what I wanted to show you here.
So the normal way you write up a pipe
is that you start out with something
that produces a sequence of elements.
Then you pipe it into a next step
that will transform and produce a next series of elements.
Then you can pipe it into the next step and so on.
And the story here is that the data starts out
from source, it gets transformed,
it gets piped into step one.
Step one does some transformation to it.
The output of step one gets then piped into step two.
So that's the standard story of the pipe operator.
So an example of that,
let me just pipe here.
An example of that we've seen.
So we might load something from disk,
data, cars, CSV,
we might filter it.
So we might say that the origin here has to be Europe.
We only want to see the cars from Europe
and then we pipe it into a data frame.
And then we get a data frame with only the rows
that happen to be in Europe.
Now again, note here that we're not constructing,
we're only constructing the data frame at the end.
As things get loaded from the CSV file,
we're applying the filter first
before we store things in a data frame.
So if you have a short pipeline,
then you can write it like this one here
where you write everything in one line.
I normally like to break things up
as soon as they become a little bit more tedious.
So if this is a multi-line pipe,
then I typically, you can put the piping operator
at the end of the line, start a new line,
write the next step in your pipe.
Add another piping operator at the end
and then write it like that.
So if you have multiple steps here
and you can easily, once you start to manipulate,
do more fancy data manipulations,
you can easily end up with four, five, six steps here.
Then breaking things up into different lines
in my mind makes things more readable.
Okay, so now let me actually copy this example here
and copy it into a new cell here.
So what I'm going to do here is,
so we're loading something from a CSV file,
we're filtering it,
and now I'm going to save it as cars 10 CSV.
Now the save function is interesting
because the save function,
so if I execute this, I actually see nothing.
So why don't I see anything?
Well, the save function actually doesn't return anything.
It is a, it writes something to disk,
namely the thing that you piped into it,
but then it stops.
There's no return value.
It doesn't show anything here in an interactive prompt
and you also could not continue piping here
because in some sense the save function eats the data
that comes in and then doesn't pipe anything out again.
So that can be inconvenient in some cases
because sometimes you might want to save something here
but then continue the pipe.
So how do we handle those cases?
There is a macro for that
and I actually hope that at some point
we might get another piping operator in Julia Bayes
but we'll see whether that will fly or not.
But for now you have to use a macro for that.
You can wrap your step in the T macro
and so it's called T
because I think Kenno pointed out
that that's how it is called in Unix shells
where it's like a T intersection, right?
So what's the story here?
So if you have a step in your pipe that uses the T macro,
then the data originally gets piped from source
into the second.
