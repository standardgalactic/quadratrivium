WEBVTT

00:00.000 --> 00:03.120
So as you know, we're going to talk about deep learning and we're going to jump right in.

00:04.480 --> 00:11.920
So much of practical applications of deep learning today, machine learning and AI in general,

00:13.120 --> 00:19.520
are used a paradigm called supervised learning, which I'm sure most of you have heard of before.

00:19.520 --> 00:24.720
So this is the paradigm by which you train a machine by showing it examples of inputs and outputs.

00:25.600 --> 00:29.680
You want to build a machine to distinguish images of cars from airplanes, you show it an image of

00:29.680 --> 00:34.560
a car. If the machine says car, you don't do anything. If it says something else, you adjust

00:34.560 --> 00:38.400
the internal parameters of the system so that the output gets closer to the one you want.

00:39.280 --> 00:45.840
So imagine the target output is some vector of activities on a set of outputs. You want the

00:45.840 --> 00:49.840
vector coming out of the machine to get closer to the vector that is the desired output.

00:51.440 --> 00:56.560
And this works really well. As long as you have lots of data, it works for speech recognition,

00:56.560 --> 01:02.240
image recognition, face recognition, generating captions, translation, all kinds of stuff.

01:03.280 --> 01:07.280
So this is, I would say, 95% of all applications of machine learning today.

01:08.160 --> 01:12.000
There are two other paradigms, one of which I will not talk about, one of which I will talk

01:12.000 --> 01:17.200
about a lot. So the two other paradigms are reinforcement learning, which I will not talk

01:17.200 --> 01:23.920
about. And there are other courses. There's a course by Larry Pinto about this that I encourage

01:24.000 --> 01:29.760
you to take. And a third paradigm is self-supervised learning or unsupervised learning. And we'll

01:29.760 --> 01:36.320
talk about this quite a lot in the following weeks. But for now, let's talk about supervised

01:36.320 --> 01:39.920
learning. Self-supervised learning, you could think of it as kind of a play on supervised learning.

01:41.840 --> 01:46.000
So the traditional model of pattern recognition machine learning and supervised learning,

01:46.000 --> 01:51.920
certainly going back to the late 50s or the 60s, is the idea by which you take a raw signal,

01:51.920 --> 01:56.640
let's say an image or an audio signal or a set of features representing an object,

01:56.640 --> 02:03.120
and then you turn it into a representation using a feature extractor, which in the past

02:03.120 --> 02:09.520
was engineered. And then you take that representation, which is generally in the form of a vector or

02:09.520 --> 02:13.520
a table of numbers or some kind of tensor, a multi-dimensional array. But sometimes,

02:14.240 --> 02:18.480
could be a different type of representation. And you feed that to a trainable classifier.

02:19.360 --> 02:23.360
So this is the learning where the learning takes part. This is the classical model,

02:24.080 --> 02:28.800
and it's still popular. It's still used a lot. But basically, what deep learning has done is

02:28.800 --> 02:35.920
replace this sort of manual hand engineering of the feature extractor by a stack of trainable

02:36.560 --> 02:40.640
modules, if you want. So in deep learning, the main idea of deep learning, and the only reason

02:40.640 --> 02:46.960
why it's called deep, is that we stack a bunch of modules, each of which transforms the input a

02:46.960 --> 02:53.520
little bit into something that's going to slightly higher level of abstraction, if you want.

02:54.240 --> 03:04.800
And then we train the entire system end to end. So I represented those sort of pinkish modules

03:04.800 --> 03:10.160
to indicate the ones that are trainable, and the blue modules are the fixed ones, the hand

03:10.160 --> 03:15.840
engineered ones. So that's why deep learning is called deep. We stack multiple layers of

03:15.840 --> 03:21.600
trainable things, and we train it end to end. The idea for this goes back a long time. The

03:21.600 --> 03:27.760
practical methods for this go back to the mid to late 80s, with the back propagation algorithm,

03:27.760 --> 03:34.480
which is going to be the main subject of today's lecture, actually. But it took a long time for

03:34.480 --> 03:41.680
this idea to actually percolate and become the main tool that people use to build machine learning

03:41.680 --> 03:48.000
system. It's only about 10 years old. Okay, so let's go through a few definitions. So we're going

03:48.000 --> 03:52.560
to deal with parameterized models, a parameterized model, or learning model, if you want, is a

03:52.560 --> 03:58.720
parameterized function, g of x and w, where x is the input, and w is a set of parameters.

04:00.240 --> 04:05.600
I'm representing this here on the right with a particular symbolism, where a function

04:06.560 --> 04:12.800
like this that produces a single output, think of the output as either a vector or matrix or a

04:12.800 --> 04:18.240
tensor, or perhaps even a scalar, but generally is multidimensional. It can actually be something

04:18.240 --> 04:24.240
else in a multidimensional array, but something that, you know, maybe like a sparse array representation

04:24.240 --> 04:29.280
or a graph with values on it. But for now, let's think of it just as a multidimensional array.

04:30.240 --> 04:35.600
So both the inputs and the outputs are multidimensional arrays, what people call tensors.

04:36.560 --> 04:40.480
It's not really kind of the appropriate definition of tensor, but it's okay.

04:42.080 --> 04:46.080
And that function is parameterized by a set of parameters w. Those are the knobs that we're

04:46.080 --> 04:52.080
going to adjust during training, and they basically determine the input-output relationship between

04:53.040 --> 05:01.520
you know, between the input x and the predicted output y bar. Okay, so I'm not explicitly

05:01.520 --> 05:08.000
representing the wire that comes in with w. Here, I kind of assume that w is somewhere inside of

05:09.200 --> 05:15.360
this module. Think of this as an object in object-oriented programming. So it's an instance

05:15.360 --> 05:21.360
of a class that you instantiated and it's got a slot in it that represents the parameters,

05:21.360 --> 05:28.160
and there is a forward function basically that takes as argument the input and returns the output.

05:28.160 --> 05:36.240
Okay, so a basic running machine will have a cost function and the cost function in supervised

05:36.240 --> 05:44.560
running, but also in some other settings will basically compute the discrepancy, distance,

05:45.120 --> 05:50.320
divergence, whatever you want to call it, between the desired output y, which is given to you from

05:50.320 --> 05:56.000
the training set, and the output produced by the system y bar. Okay, so an example of this,

05:56.000 --> 06:01.920
a very simple example of a setting like this is linear regression. In linear regression, x is a

06:01.920 --> 06:08.320
vector composed of components x i's, w is also a vector, and the output is a scalar that is simply

06:08.320 --> 06:16.160
the dot product of x with w. So y bar now is a scalar, and what you compute is the square

06:16.160 --> 06:22.480
distance, the square difference really between y and y bar. If w is a matrix, then now y is a vector,

06:22.480 --> 06:28.800
and you compute the square norm of the difference between y and y bar, and that's basically linear

06:28.800 --> 06:35.360
regression. So learning will consist in finding the set of w's that minimize this particular cost

06:35.360 --> 06:40.080
function average over a training set. I'll come to this in a minute, but I want you to think right

06:40.080 --> 06:45.920
now about the fact that this g function may not be something particularly simple to compute. So

06:46.240 --> 06:52.320
it may not be just multiplying a vector by a matrix. It may not be just carrying some

06:53.600 --> 06:58.800
sort of fixed computation with sort of a fixed number of steps. It could involve something

06:58.800 --> 07:03.840
complicated. It could involve minimizing a function with respect to some other variable that you

07:03.840 --> 07:10.880
don't know. It could involve a lot of iteration of some algorithm that converges towards a fixed

07:10.880 --> 07:19.920
point. So let's not restrict ourselves to g of x w that are simple things. It could be very

07:19.920 --> 07:26.160
complicated things, and we'll come to this in a few weeks. So this is just to explain the

07:26.160 --> 07:34.480
notations that I will use during the course of this class. So we have observed input and desired

07:34.480 --> 07:41.920
output variables. Those are kind of gray grayish bubbles. Other variables that are produced by

07:41.920 --> 07:48.800
the system or internal to the system are those kind of, you know, empty circle variables.

07:49.920 --> 07:53.840
We have determinacy functions or functions that are so they are indicated by this sort of

07:54.640 --> 07:58.480
rounded shape here. They can take multiple inputs have multiple outputs.

07:59.920 --> 08:04.400
And each of those can be tensors or scalars or whatever. And they have implicit parameters

08:04.480 --> 08:10.160
that are tunable by training. And then we have cost functions. So cost functions are basically

08:11.360 --> 08:17.840
functions that take one or multiple inputs and output a scalar. But I'm not representing the

08:17.840 --> 08:24.880
output is implicit. Okay, so if you have a red square, it has an implicit output. And it's a

08:24.880 --> 08:31.680
scalar and we interpret it as a cost or an energy function. So this symbolism is kind of similar

08:31.680 --> 08:37.760
to what people use in graphical models. If you if you've heard what a graphical model is,

08:37.760 --> 08:41.440
particularly the type of graphical model called a factor graph. So in a factor graph, you have

08:41.440 --> 08:46.640
those variable bubbles, and you have those factors, which are those square cost functions.

08:47.520 --> 08:50.720
You don't have this idea that you had deterministic functions in it, because

08:50.720 --> 08:54.240
graphical models don't care about the fact that you have functions in one direction or another.

08:54.240 --> 09:00.960
But here we care about it. So with this extra symbol. Okay, so machine learning consists in

09:00.960 --> 09:07.520
basically minimizing finding the set of parameters W that minimize the cost function averaged over

09:07.520 --> 09:16.640
a training set. So a training set is a set of pairs x, x, y indexed by an index P. Okay, so we have

09:16.640 --> 09:22.960
P training samples. And it'll be the index of the training set the training sample. And our overall

09:23.680 --> 09:26.160
last function that we're going to have to minimize

09:28.480 --> 09:34.480
is the, you know, is equal to the cost of the discrepancy between the y and the output of our

09:34.480 --> 09:43.120
model by bar g of x, w, as I said earlier. So L is a value, C is a C is a module and L is a

09:43.120 --> 09:49.280
is a is a way of writing C of y, g of x, w, you know, whether it depends explicitly on x, y and

09:49.280 --> 09:55.440
w. Okay, but it's the same thing really. The overall last function, which is this

09:57.040 --> 10:03.200
kind of curly L is the average of the per sample loss function over the entire training set. Okay,

10:03.840 --> 10:10.160
so compute L for the entire training set, divide by some all the terms divide by P, and that's the

10:10.160 --> 10:16.800
average. That's the loss. Okay, so now the name of the game is trying to find the minimum of that

10:16.800 --> 10:23.600
loss with respect to the parameters. This is an optimization problem. So symbolically, I can

10:23.600 --> 10:29.040
represent this entire graph as the thing on the right. This is rarely used in practice, but this

10:29.040 --> 10:36.000
is sort of a way to visualize this. So think about each training sample as a sort of identical copy

10:36.000 --> 10:42.960
of the replica, if you want, of the model and the cost function applied to a different training

10:42.960 --> 10:47.200
sample, and then there is an average operation that computes the loss, right? So everything you

10:48.000 --> 10:54.480
can write as a formula, you can probably write in terms of those graphs. This is going to be very

10:54.480 --> 11:01.440
useful as we're going to see later. Okay, so supervised machine learning and a lot of other

11:01.440 --> 11:09.440
machine learning patterns as well actually are can be viewed as function optimization and a very

11:09.440 --> 11:17.120
simple approach to optimizing a function, which means finding the set of parameters to a function

11:17.120 --> 11:23.680
that minimize its value, okay, is gradient descent or gradient based algorithms. So gradient based

11:23.680 --> 11:29.520
algorithm makes the assumption that the function is somewhat smooth and mostly differentiable,

11:29.520 --> 11:34.400
doesn't have to be everywhere differentiable, but has to be continuous, has to be almost everywhere

11:34.400 --> 11:40.960
differentiable. And it has to be somewhat smooth, otherwise, the local information of the slope

11:40.960 --> 11:46.240
doesn't tell you much about where the minimum is. Okay, so here's an example here depicted on the right.

11:48.160 --> 11:53.120
The lines that you see here, the pink lines are the lines of equal cost and this cost is quadratic,

11:53.760 --> 12:00.480
so it's basically a kind of paraboloid. And this is the trajectory of a method called stochastic

12:00.480 --> 12:04.880
gradient descent, which we'll talk about in a minute. So for stochastic gradient descent, the

12:04.880 --> 12:08.960
procedure is you show an example, you run you through the machine, you compute the objective

12:08.960 --> 12:15.840
for that particular sample, and then you figure out by how much and how to modify each of the knobs

12:15.840 --> 12:20.880
in the machine, the W parameters, so that the objective function goes down by a little bit,

12:20.880 --> 12:24.560
you make that change, and then you go to the next sample. Let's be a little more formal.

12:25.200 --> 12:32.320
So gradient descent is this very basic algorithm here, you replace the value of W by its previous

12:32.320 --> 12:39.520
value minus a step size, eta here, multiplied by the gradient of the objective function with respect

12:39.520 --> 12:47.760
to the parameters. So what is a gradient? A gradient is a vector of the same size as the

12:47.760 --> 12:53.280
parameter vector. And for each component of the parameter vector, it tells you by how much

12:54.080 --> 13:00.240
the the loss function L would increase if you increase the parameter by a tiny amount.

13:00.240 --> 13:05.280
Okay, it's a derivative, but it's a directional derivative, right? So let's say among all the

13:05.280 --> 13:13.200
directions, you only look at W34. And as you imagine that you tweak W34 by a tiny amount,

13:14.080 --> 13:19.920
the loss function curly L is going to increase by a tiny amount, you divide the tiny amount by

13:19.920 --> 13:27.120
which L increase by the tiny amount that you modified this W34. And what you get is the gradient

13:27.120 --> 13:33.840
of the loss with respect to W34. If you do this for every single weight, you get the gradient

13:33.840 --> 13:38.160
of the loss function with respect to all the weights. And it's a vector, which for each component

13:38.160 --> 13:45.680
of the weight gives you the parameter gives you that quantity. Okay, so, you know, since Newton

13:45.680 --> 13:51.280
and earlier, it's been written as, you know, dL over dW, because it indicates the fact that

13:51.280 --> 13:56.000
there is this little twiddle where you can twiddle W by little. And there's a resulting

13:56.880 --> 14:00.960
twiddling of L. And if you divide those two twiddles, and they are infinitely small,

14:00.960 --> 14:05.840
you get the derivative that's kind of standard notation in mathematics for a few hundred years.

14:05.840 --> 14:15.360
Okay, so now the gradient is going to be a vector. Okay. And as indicated here on the top, right,

14:15.440 --> 14:26.560
that vector is an arrow that points upwards along the line of larger slope. Okay, so if you are

14:26.560 --> 14:32.240
in a 2D surface, you have two W parameters. Okay, and the surface is represented here,

14:32.880 --> 14:38.880
some sort of quadratic ball here in this case. So it's a second degree polynomial in W1 and W0.

14:39.840 --> 14:46.320
Here on the right is the kind of a top-down view of this where the lines represent the lines of

14:46.320 --> 14:52.320
equal cost. The little arrow is here, represent the gradient at various locations. Okay,

14:52.960 --> 14:58.080
so you have a long arrow if the slope is steep, a short arrow if the slope is

14:59.760 --> 15:06.720
not steep, not large. At the bottom, it's zero. And it points towards the direction of

15:07.280 --> 15:12.480
higher slope. All right, so imagine you are in a landscape, a mountainous landscape,

15:13.280 --> 15:18.640
and you're in a fog and you want to go down the valley. You look around you and you can tell

15:19.920 --> 15:27.120
the local slope of the landscape. You can't tell where the minimum is because you're in a fog,

15:27.760 --> 15:34.400
but you can tell the local slope. So you can figure out what is the direction of larger slope

15:34.400 --> 15:39.600
and then take a step and it will take you upwards. Now you turn around 180 degrees,

15:39.600 --> 15:44.640
take a step in that direction, and that is going to take you downwards. If you keep doing this

15:44.640 --> 15:50.560
and the landscape is convex, which means it has only one local minimum, this will eventually

15:51.520 --> 15:57.520
take you down to the valley and presumably to the village. Right, so that's gradient-based

15:57.520 --> 16:07.840
algorithms. They all differ by how you compute the gradient first and by what this eta step-size

16:07.840 --> 16:15.120
parameter is. So in simple forms, eta is just a positive constant that sometimes is decreased as

16:15.120 --> 16:23.760
the system learns more, but most of the time not. But in more complex versions of gradient-based

16:23.760 --> 16:29.200
learning, eta is actually an entire matrix itself, generally a positive definite or semi-definite

16:29.200 --> 16:37.680
matrix. And so the direction adopted by those algorithms is not necessarily the steepest descent.

16:37.680 --> 16:43.360
It goes downwards, but it's not necessarily the steepest descent. And we can see why here. So

16:44.480 --> 16:49.440
in this diagram here that I'm showing, this is a trajectory that will be followed by gradient

16:49.440 --> 16:55.520
descent in this quadratic cost environment. And as you see, the trajectory is not straight.

16:56.160 --> 17:02.080
It's not straight because the system goes down by following the slope of steepest descent. And so

17:03.280 --> 17:06.640
it goes down the valley before finding the minimum of the valley, if you want.

17:07.280 --> 17:12.160
So if your cost function is a little squeezed in one direction, it will go down the ravine

17:12.160 --> 17:17.280
and then follow the ravine towards the bottom. In complex situations where you have

17:18.000 --> 17:24.400
things that are, the trajectory actually is being cut here. But when the

17:24.400 --> 17:30.720
weather function is highly irregular, this might even be more complicated. And then you might have

17:30.720 --> 17:35.440
to be smart about what you do here. Okay. So stochastic gradient descent is

17:36.480 --> 17:46.560
universally used in deep learning. And this is a slight modification of the gradient

17:47.440 --> 17:54.960
steepest descent algorithm where you don't compute the gradient of the entire objective function

17:54.960 --> 18:02.320
averaged over all the samples. But what you do is you take one sample and you compute the gradient

18:02.320 --> 18:07.600
of the objective function for that one sample with respect to the parameters and you take a step.

18:08.240 --> 18:14.160
Okay. And you keep doing this. You pick another sample, compute the gradient of the objective

18:14.160 --> 18:19.760
function for that sample with respect to the way it's making a date. Why is it called stochastic

18:19.760 --> 18:28.800
gradient? Stochastic is a fancy term for random, essentially. And it's called stochastic because

18:28.800 --> 18:35.120
the evaluation of the gradient you get on the basis of a single sample is a noisy estimate of

18:35.120 --> 18:40.240
the full gradient. The average of the gradients, because the gradient is a linear operation,

18:40.240 --> 18:44.800
the average of the gradients will be the gradient of the average. And so things work out. If you

18:44.800 --> 18:50.160
compute the gradient and you kind of keep going, overall, the average trajectory will be sort of

18:50.160 --> 18:56.720
the trajectory you would have followed by doing full gradient. Okay. But in fact,

18:57.600 --> 19:02.480
the reason we're doing this is because it's much more efficient in terms of speed of convergence. So

19:03.920 --> 19:07.520
although the trajectory followed by stochastic gradient is very noisy,

19:07.520 --> 19:11.760
things kind of bounce around a lot. As you can see in the trajectory here at the bottom,

19:13.280 --> 19:18.080
you know, things have, the trajectory is very erratic. But in fact, it goes to the bottom faster

19:18.960 --> 19:24.640
and has other advantages that people are still writing papers on. Okay. The reason for that is

19:24.640 --> 19:32.320
that stochastic gradient exploits the redundancy between the samples. So all the, you know, machine

19:32.320 --> 19:37.520
learning setting, the training samples have some similarities between them. If they don't, then

19:37.520 --> 19:42.560
basically the learning problem is impossible. So they necessarily do have some redundancy

19:42.560 --> 19:46.880
between them. And the faster you update the parameters, the more you, the more often you

19:46.880 --> 19:52.960
update them, the more you exploit this redundancy between those parameters. Now in practice, what

19:52.960 --> 19:59.200
people do is they use mini batches. So instead of computing the gradient on the basis of a single

19:59.200 --> 20:06.800
sample, you take a batch of samples, typically anywhere between let's say 30 and a few thousand.

20:08.080 --> 20:12.880
But smaller batches are better in most cases actually. And you compute the average of the

20:13.920 --> 20:21.200
gradient over those samples. Okay. So compute the average cost over those samples and compute the

20:21.200 --> 20:27.760
gradient of the average over those samples and then make an update. The reason for doing this

20:28.480 --> 20:35.600
is not intrinsically an algorithmic reason. It's because it's a simple way of parallelizing

20:36.560 --> 20:42.320
stochastic gradients on parallel hardware such as GPUs. Okay. So there's never, there's no good

20:42.320 --> 20:48.880
reason to do batching other than the fact that our hardware likes it. Okay. Question. Yeah. So

20:49.600 --> 20:54.880
for actually for, for real complex deep learning problems, does this objecting function have to

20:54.880 --> 21:00.880
be continuously differentiable? Well, it needs to be continuous mostly. If it's non continuous,

21:00.880 --> 21:08.960
you're going to get in trouble. It needs to be differentiable almost everywhere. But in fact,

21:10.320 --> 21:14.640
neural nets that most people use are actually not differentiable. And there's a lot of places

21:14.640 --> 21:18.400
where they're not differentiable. But they are continuous in the sense that there are functions

21:18.400 --> 21:23.360
that have kind of corners in them, if you want. They have kinks. And if you have a kink once in a

21:23.360 --> 21:34.880
while, it's not too much of a problem. But so in that case, those quantities should not be called

21:34.880 --> 21:39.840
gradients, they should be called subgradients. Okay. So a sub gradient is basically a generalization

21:39.840 --> 21:47.360
of the idea of derivative or gradient to functions that have kinks in them. So wherever you have a

21:47.360 --> 21:53.760
function that has a kink in it, any, any slope that is between the slope of one, one side and the

21:53.760 --> 22:02.400
slope of the other side is a, is a valid sub gradient. Okay. So when you write the kink,

22:02.400 --> 22:06.720
you decide, well, the derivative is this or it's that or it's going to somewhere in between. And

22:06.720 --> 22:15.440
you're fine. Most of the proof that applied to, you know, smooth functions, you know, in terms

22:15.440 --> 22:22.800
of minimization, often apply also to non-smooth function that basically are differentiable most

22:22.800 --> 22:30.000
of the way. So then how do we ensure strict convexity? We do not ensure strict convexity. The,

22:31.360 --> 22:37.040
in fact, in deep learning systems, most deep learning systems, the function that we are

22:37.040 --> 22:43.040
optimizing is non-convex, right? In fact, this is one reason why it took so long for deep learning

22:43.040 --> 22:48.000
to become prominent is because a lot of people, particularly theoreticians, people who sort of

22:48.000 --> 22:52.560
theoretically minded, were very scared of the idea that you had to minimize a non-convex

22:52.560 --> 22:56.560
objective and they say, this can't possibly work because we can't prove anything about it.

22:56.560 --> 23:00.240
It turns out it does work. You can't prove anything about it, but it does work. And so

23:01.040 --> 23:07.440
this is a situation, and it's an interesting thing to think about, a situation where the,

23:07.440 --> 23:11.600
the theoretical thinking basically limited what people could do in terms of engineering

23:12.480 --> 23:16.240
because they couldn't prove things about it. But that would be actually very powerful.

23:16.240 --> 23:19.040
Okay. Yeah. Like your colleague, you optimize non-convex functions.

23:19.840 --> 23:23.440
Like your colleague at the Bell Labs, who didn't like the non-mathy.

23:26.160 --> 23:31.520
Oh, it was a whole debate, you know, in the machine learning community that lasted 20 years,

23:31.520 --> 23:39.120
basically. All right. So what about how doesn't SGD get stuck in local minima once it reaches them?

23:39.120 --> 23:42.400
It does. Okay. So,

23:44.640 --> 23:52.080
so full gradient does get stuck in local minima, right? SGD gets like, you know,

23:52.080 --> 23:56.480
it's slightly less stuck in local minima because it's noisy. It allows it sometimes to escape

23:56.480 --> 24:05.440
local minima. But the real reason why we're going to optimize non-convex functions and local minima

24:05.440 --> 24:12.720
are not going to be such a huge problem is that there aren't that many local minima that are traps.

24:12.720 --> 24:19.600
Okay. So we're going to build neural nets, and those neural nets are, or deep running systems,

24:19.600 --> 24:24.720
and they're going to be built in such a way that the, the parameter space is such a, such a high

24:24.720 --> 24:29.120
dimension that is going to be very hard for the system to actually create local minima for us.

24:29.120 --> 24:36.240
Okay. So think about a picture where we have in one dimension a cost function that has one

24:36.240 --> 24:41.440
local minima and then a global minimum, right? Okay. So it's a function like this, right?

24:43.280 --> 24:47.040
And we start from here. If we optimize using gradient descent, we're going to get stuck in

24:47.040 --> 24:52.160
that local minimum. Now, let's imagine that we parameterize this function now with two parameters.

24:52.160 --> 24:55.840
Okay. So we're not a one dimensional, we're not looking at a one dimensional function anymore.

24:55.840 --> 24:58.560
We're looking at two dimensional function. We have an extra parameter.

24:59.680 --> 25:05.120
This extra parameter will allow us to go around the mountain and go towards the valley,

25:05.120 --> 25:10.320
perhaps without having to climb the little hill in the middle. Okay. So this is just an intuitive

25:10.320 --> 25:15.600
example to tell you that in very high dimensional spaces, you may not have as much of a local minimum

25:15.600 --> 25:21.040
problem as you have in the sort of intuitive picture of low dimensional spaces, right? So here

25:21.040 --> 25:25.120
that those pictures are in two dimensions. They are very misleading. We're going to be working

25:25.120 --> 25:34.080
with millions of dimensions and you know, some of the most recent deep learning systems have

25:34.080 --> 25:40.960
trillions of problems. Yeah. So local minima is not going to be that much of a problem. We're

25:40.960 --> 25:45.760
going to have other problems, but not that one. So there is like a trend in this hyper like

25:46.960 --> 25:52.080
over parameterization, right? Like it seems like that more neurons we have and the better

25:52.080 --> 25:56.400
these networks work somehow. That's right. So we're going to make those networks very large and

25:56.400 --> 25:59.520
they're going to be over parameterized, which means they're going to have way more adjustable

25:59.520 --> 26:03.200
parameters than we would actually need, which means they're going to be able to learn the

26:03.200 --> 26:07.360
training set almost perfectly. And the big question is how well are they going to work on

26:07.360 --> 26:12.560
a separate validation set or test set that is separate from the training set?

26:13.520 --> 26:19.040
Two more questions. They're going to work in a real situation where, you know, the distribution

26:19.040 --> 26:22.640
of samples may be different from what we trained it on. So that's the real question of machine

26:22.640 --> 26:28.320
learning, which I'm sure a lot of you are familiar with. Two more questions. Can we do?

26:28.320 --> 26:36.880
Yeah. So how do we escape instead of subtle points? Right. So there are tons and tons of

26:36.880 --> 26:41.920
subtle points in deep learning systems. A combinatorially large number of subtle points,

26:41.920 --> 26:48.160
as a matter of fact. I'll have a lecture on this. So I don't want to kind of spend too long

26:48.160 --> 26:54.240
answering. Okay. But yeah, there are subtle points. The trick with subtle points is you don't

26:54.240 --> 27:01.920
want to get too close to them, essentially. And stochastic gradient helps a little bit

27:01.920 --> 27:07.520
with subtle points. Some people are proposed sort of explicit methods to stay away from

27:07.520 --> 27:12.480
subtle points. But in practice, doesn't seem to be that much of a problem, actually.

27:12.720 --> 27:18.320
Finally, how do you pick samples for stochastic gradient in the center randomly?

27:19.680 --> 27:25.760
Okay. There is lots of different methods for that. Okay. Yeah. I mean, the basic thing you should do

27:25.760 --> 27:30.880
is you have your training set. You shuffle the samples in a random order. Okay. And then you

27:30.880 --> 27:39.360
just pick them one at a time. And then you cycle through them. An alternative is once you get to

27:39.360 --> 27:44.800
the end, you reshuffle them and then cycle through them again. An alternative is you pick a random

27:44.800 --> 27:50.800
sample using a random number. Every time you pick a new sample, you pick them randomly.

27:54.400 --> 28:00.800
If you do batching, a good idea is to put in a batch samples that are maximally different from

28:00.800 --> 28:04.800
each other. So things that are, for example, different categories if you do classification.

28:05.200 --> 28:09.600
But most people just do them, you know, just pick them randomly. But it's good to have samples

28:09.600 --> 28:15.520
that are maximally different that are nearby either in a batch or during the processor training.

28:15.520 --> 28:20.160
And then there are all kinds of tricks that people use to sort of emphasize difficult samples

28:21.120 --> 28:25.280
so that the boring, easy samples are not, you don't waste your time just, you know,

28:25.280 --> 28:30.800
seeing them over and over again. It's all kinds of tricks. All right. But, you know,

28:31.440 --> 28:36.000
the simpler one is, which most people use, you shuffle your samples and you run through them.

28:37.040 --> 28:41.360
Most people now use also data augmentation. So every sample is actually distorted by some

28:42.480 --> 28:46.880
process. For an image, you can distort the geometry a little bit, you change the colors,

28:46.880 --> 28:54.000
you add noise, et cetera. This is an artificial way of sort of adding more samples than you actually

28:54.000 --> 28:58.880
have. And people do this kind of randomly on the fly or they kind of precompute those

28:59.840 --> 29:05.760
those transformations. So lots of tricks there as well. Last question. How do you pick the batch

29:05.760 --> 29:12.720
size? The best. The batch, batch size. Oh, the batch size. That's determined by your hardware.

29:12.720 --> 29:19.600
So if you have a GPU, generally for, you know, reasonably sized networks, your batch size would

29:19.600 --> 29:24.560
be anywhere between 16 and 64 or something like that. For smaller networks, you might have to batch

29:24.640 --> 29:29.840
more to kind of exploit your, your hardware better to kind of have maximum usage of it.

29:30.560 --> 29:35.040
If you parallelize on multiple GPUs within a machine, you may have to, to have, you know,

29:35.040 --> 29:39.040
so let's say you have eight GPUs, then you'll be sort of eight times 32. So there's no

29:40.480 --> 29:46.160
256 or something. And then, you know, a lot of the big guys kind of parallelize that over

29:46.160 --> 29:50.640
multiple machines, each of which has eight GPUs. Some of them have TPUs, whatever. And then you

29:50.640 --> 29:54.880
might have to parallelize over thousands of examples. This diminishing return in doing this,

29:55.520 --> 30:01.600
when you increase the size of the batch, you actually reduce the, the, the speed of convergence.

30:01.600 --> 30:05.360
You accelerate the calculation, but you reduce the speed of convergence. So at some point,

30:05.360 --> 30:11.920
it's not worth increasing your batch size. So if we are doing a classification problem with k

30:11.920 --> 30:19.840
classes, what's going to be like our go to batch size? So there are papers that say if your batch

30:19.840 --> 30:24.880
size is significantly larger than the number of categories, or let's say twice the number of

30:24.880 --> 30:30.800
categories, then you're, you're probably wasting competition, essentially. Okay. I mean, throwing

30:30.800 --> 30:36.080
down convergence. So you're trying to train an image recognizer on ImageNet. If your batch size

30:36.080 --> 30:43.200
is larger than about a thousand, you're probably wasting time. Okay, that's it. Thanks. I mean,

30:43.200 --> 30:48.720
you're wasting competition. You're not wasting time. Okay. Okay. Okay. So let's talk about

30:48.720 --> 30:54.960
traditional neural net. So a traditional neural net is a, a, a model, a particular type of

30:54.960 --> 31:02.080
parameterized function, which is built by stacking linear and nonlinear operations. Right. So here

31:02.080 --> 31:05.680
is this kind of a depiction of a traditional neural net here in this case, with two layers, but I'm,

31:05.680 --> 31:10.640
you know, I'm not imagining there might be more layers here. So you have a bunch of inputs here

31:10.640 --> 31:19.120
on the left. Each input is multiplied by a weight, different weights, presumably. And those, the

31:19.120 --> 31:25.200
weighted sum of those inputs by those weights is, is computed here by what's called a unit or neuron.

31:26.160 --> 31:30.960
People don't like using the word neuron in that context, because there are incredibly simplified

31:30.960 --> 31:37.840
models of neurons in the brain, but, but that's the inspiration really. Okay. So one of those units

31:37.920 --> 31:42.480
just computes a weighted sum of its inputs, using those weights. Okay, this unit use,

31:42.480 --> 31:47.040
computes a different weighted sum of the same inputs with different weights and etc. So here

31:47.040 --> 31:50.480
we have three units here in the first layer. This is called a hidden layer, by the way,

31:51.360 --> 31:54.720
because it's neither an input nor an output, right? This is the input, and this is the output,

31:54.720 --> 31:59.200
and this is somewhere in the middle. So we compute those weighted sums, and then we pass those

31:59.200 --> 32:04.720
weighted sums individually through a, a nonlinear function. So here what I've shown is the value

32:04.720 --> 32:11.440
function. So this is called rectified linear unit. In the, this is the name that people

32:12.000 --> 32:18.000
have given it in the neural net lingual. In other contexts, this is called a half wave rectifier,

32:18.000 --> 32:24.640
if you're an engineer. It's called positive part, if you are a mathematician. Okay. Basically,

32:24.640 --> 32:29.600
it's a function that is equal to the identity when its argument is positive, and it's equal to zero

32:29.600 --> 32:38.080
if its argument is negative. Okay. So very simple graph. And then we stack a second layer of the

32:38.080 --> 32:42.480
same thing, the second stage, right? So again, a layer of linear operations where we compute

32:42.480 --> 32:47.200
weighted sums, and then we pass a result to nonlinearities. And we can stack many of those

32:47.200 --> 32:52.000
layers, and that's basically a traditional plain vanilla garden variety neural net.

32:53.200 --> 32:59.120
In this case, fully connected. So fully connected neural net means that every unit in one layer

32:59.120 --> 33:03.120
is connected to every unit in the next layer. And you have this sort of well organized layer,

33:04.640 --> 33:09.760
or architecture, if you want, right? Each of those weights are going to be the things that

33:09.760 --> 33:15.840
our learning algorithm is going to, is going to tune. And the big trick, the one trick really

33:15.840 --> 33:22.640
of deep learning is how we compute those gradients. Okay. So if you want, if you want to write this,

33:22.720 --> 33:29.200
you can say the weighted sum number i, so you can give a number to each of the units

33:29.920 --> 33:37.680
in the network. So this unit with number i, and the weighted sum s of i, is simply the sum

33:38.400 --> 33:43.840
where j goes over the upstream, the set of upstream units to i, which may be all the units in the

33:43.840 --> 33:51.120
previous layer or not could be just a subset. Okay. And then you compute the product of zj,

33:51.120 --> 33:57.600
which is the output of the unit number j times wij, which is the weight that links

33:57.600 --> 34:04.080
unit j to unit i. Okay. And then after that, you take this si, which is the weighted sum,

34:04.080 --> 34:08.400
you pass it through the activation function, this value, or whatever it is that you use,

34:08.400 --> 34:16.000
and that gives you zi, which is the activation for unit i. Okay. Super notation. By changing the

34:16.000 --> 34:20.240
set of upstream units of every unit, by building a graph of interconnection, you can basically

34:20.240 --> 34:27.360
build any kind of network arrangement that you want. There is one constraint that we can lift,

34:27.360 --> 34:33.120
that we will lift in the subsequent lecture, which is that the graph has to be

34:36.080 --> 34:40.320
ac-click in the sense that it can't have loops. Okay. If you have loops, that means you can't

34:40.320 --> 34:46.000
organize the units in layers. You can't sort of number them in a way that you can compute them

34:46.640 --> 34:51.680
so that every time you want to compute a unit, you already have the state of the previous units.

34:51.680 --> 34:56.720
If there are loops, then you can do that. Right? So for now, we're going to assume that

34:57.520 --> 35:00.800
the wij matrix, the w matrix, doesn't have loops,

35:04.000 --> 35:09.520
represents a graph that doesn't have loops. That's why I should say. Okay. So here's sort of an

35:09.520 --> 35:14.560
intuitive explanation of the back propagation algorithm. So the back propagation algorithm

35:14.560 --> 35:21.680
is the main technique that is used everywhere in deep learning to compute the gradient of

35:22.400 --> 35:28.240
a cost function, whatever it is, objective function, with respect to a variable inside

35:28.240 --> 35:32.960
of the network. This variable can be a state variable like a z or an s, or it could be a

35:32.960 --> 35:38.080
parameter variable like a w. Okay. And we're going to need to do both. Okay. So this is going to be

35:38.080 --> 35:41.600
an intuitive explanation. And then after that, there's going to be a more mathematical explanation,

35:41.600 --> 35:47.200
which is less intuitive, but perhaps actually easier to understand. But let me start with

35:47.200 --> 35:51.440
the intuition here. So let's say we have a big network. And inside of this big network, we have

35:51.440 --> 35:55.920
one of those little activation functions. Okay. In this case, it's a sigmoid function, but

35:55.920 --> 36:00.080
doesn't matter what it is for now. Okay. This function takes an s and produces a z.

36:01.040 --> 36:12.400
We call this function h of s, right? So when we wiggle z, the cost is going to wiggle by some

36:12.400 --> 36:19.120
quantity, right? And we divide the wiggling of z by the wiggling of c by the wiggling of z that

36:19.120 --> 36:25.040
causes it. That gives us the partial derivative of c with respect to z. So this one term is a gradient

36:25.040 --> 36:29.360
of c with respect to all the z's in the network. And there's one component of that gradient,

36:29.360 --> 36:36.240
which is the partial derivative of the cost with respect to that single variable z inside the

36:36.240 --> 36:43.040
network. Okay. And that really indicates how much c would wiggle if we wiggled z by some amount.

36:43.040 --> 36:46.960
We divide the wiggling of c by the wiggling of z and that gives us the partial derivative

36:46.960 --> 36:53.840
of c with respect to z. This is not how we're going to compute the gradient of c with respect to z,

36:53.840 --> 36:59.200
but this is a description of what it is conceptually. Okay. Or intuitively, rather.

36:59.920 --> 37:03.440
Okay. So let's assume that we know this quantity. So we know the partial derivative

37:04.000 --> 37:11.280
of c with respect to z. Okay. So c with respect to z is this quantity here, dc over dz. Okay.

37:12.720 --> 37:17.840
So think of dz as the wiggling of z and dc as the wiggling of c, divide one by the other,

37:17.840 --> 37:24.800
and you get the partial derivative of c with respect to z. What we have here is,

37:28.400 --> 37:34.000
what we have to apply is the chain rule, the rule that tells us how to compute the

37:34.560 --> 37:39.280
derivative of a function composed of two individual functions that we apply one after the other.

37:39.280 --> 37:43.600
Right. So remember, chain rule, if you have a function g, then you apply to another function h,

37:44.320 --> 37:47.200
which is function of parameter s, and you want the derivative of it.

37:48.560 --> 37:52.480
The derivative of that is equal to the derivative of g at point h of s,

37:52.480 --> 37:59.440
multiplied by the derivative of h at point s. Right. That's chain rule. You know that a few

37:59.440 --> 38:05.280
years ago, hopefully. Now, if I want to write this in terms of partial derivative, it's the same

38:05.280 --> 38:10.080
thing, right? Partial derivative is just a derivative just with respect to one single variable.

38:10.080 --> 38:15.840
So I would write this something like this, dc over ds. So c really is the result of applying

38:15.840 --> 38:22.560
this h function to s, and then applying some unknown g function to compute c, which is kind

38:22.560 --> 38:29.200
of the rest of the network plus the cost. But I'm just going to call the gradient. I'm going to

38:29.200 --> 38:38.080
assume that this dc over dz is known. Someone gave it to me. So this variable here on the right,

38:38.080 --> 38:45.840
dc over dz is given to me, and I want to compute dc over ds. So what I need to do is write this,

38:46.800 --> 38:54.480
dc over ds equal dc over dz times dz over ds. Right. And why is this identity true? It's because

38:54.480 --> 39:01.760
I can simplify by dz. It's as simple as this, right? So you have, you know, trivial algebra,

39:01.760 --> 39:06.080
you have dz at the denominator here, dz at the numerator here, simplify, you get dc over ds.

39:06.560 --> 39:11.520
It's a very trivial, simple identity, which is basically just generally applied to partial derivatives.

39:13.440 --> 39:20.240
Now, dz over ds, we know what it is. It's just h prime of s, just the derivative of the h function.

39:21.760 --> 39:27.040
So we have this formula, dc over ds equal dc over dz, which we assume is known, times h prime of s.

39:28.320 --> 39:33.040
What does that mean? That means that if we have this component of the gradient of the cost function

39:33.040 --> 39:40.640
with respect to z here, we multiply this by the derivative of the h function at point s,

39:40.640 --> 39:46.320
the same point s that we had here. And what we get now is the gradient of the cost function

39:46.320 --> 39:52.640
with respect to s. Now, here's the trick. If we had a chain of those h functions, we could keep

39:52.640 --> 39:56.720
propagating this gradient backwards by just multiplying by the derivative of all those h

39:56.720 --> 40:03.280
functions going backwards. And that's why it's called back propagation. So it's just a practical

40:03.280 --> 40:08.240
application of a chain rule. And if you want to convince yourself of this, you can run through

40:08.240 --> 40:16.160
this idea of perturbation. If I twiddle s by some value, it's going to twiddle z by some value equal

40:16.160 --> 40:26.160
to ds times h prime of s, basically the slope of s. So dz equals h prime of s times ds. And then

40:26.160 --> 40:34.640
I'm going to have to multiply this by dc over dz. And so I rearrange the terms and I get immediately

40:34.640 --> 40:43.920
that this formula dc over ds equals dc over dz times h prime of s. So we had another element in

40:43.920 --> 40:50.240
our multilayer net, which was the linear sum. And there, it's just a little bit more complicated,

40:50.240 --> 40:58.080
but not really. So one particular variable z here, we would like to compute the derivative,

40:58.080 --> 41:05.120
the partial derivative of our cost function with respect to that z. And we're going to assume that

41:05.120 --> 41:10.000
we know the partial derivative of s with respect to each of those s's, the weighted sums at the

41:10.000 --> 41:18.720
next layer that z is going into. So z only influences c through those s's. So presumably,

41:18.720 --> 41:26.640
by basically multiplying how each of those s's influence c and then multiplying by how z

41:27.600 --> 41:31.760
influences each of the s's and summing up, we're going to get the influence of z over c.

41:31.760 --> 41:35.120
Right? And that's the basic idea. Okay, so here's what we're going to do.

41:35.840 --> 41:50.240
Let's say we perturb z by dz. This is going to perturb s0 by dz times w0. Okay, we multiply z

41:50.240 --> 41:56.960
by w0. So the derivative of this linear operation is the coefficient itself. Right? So here,

41:57.600 --> 42:09.440
the perturbation is ds0 is equal to dz times w0. Okay? And now in turn, this is going to modify c,

42:10.160 --> 42:19.520
and we're going to multiply this quantity by dc over ds0 to get the dc, if you want. Okay?

42:20.640 --> 42:25.840
Now, whenever we perturb z, it's not going to perturb just s0, it's also going to perturb s1

42:25.840 --> 42:31.280
and s2. And to see the effect on c, we're going to have to sum up the effect of the perturbation

42:31.280 --> 42:37.120
on each of the s's and then sum them up to see the overall effect on c. So this is written here

42:37.120 --> 42:46.400
on the left. The perturbation of c is equal to the perturbation of s multiplied by the partial

42:46.400 --> 42:53.600
derivative of c with respect to s plus the perturbation of s1 multiplied by the partial

42:53.600 --> 43:00.160
derivative of dc with respect to s1 plus same thing for s2. Okay? So this is the fact that,

43:01.280 --> 43:06.880
you know, we need to take into account all the perturbations here that z may influence.

43:08.480 --> 43:13.280
And so I can just write down now a very simple thing, you know, because dc of 0 is equal to

43:13.280 --> 43:21.840
w0 times dz and, you know, ds of 2 is w2 times dz, I can plug this in there and just write dc

43:21.840 --> 43:28.480
over dz equal dc over ds0, which I assume is known, times w0 plus dc over ds1 times w1

43:28.480 --> 43:34.320
plus dc over ds2 times w2. Okay? If I want to represent this operation graphically,

43:35.200 --> 43:42.720
this is shown on the right here. I have dc over ds0, dc over ds1, dc over ds2, which I assume

43:42.720 --> 43:55.120
are known or given to me somehow. I compute dc over ds0 multiplied by w0 and multiply dc over ds1

43:55.120 --> 44:01.760
by w1, dc over ds2 by w2. I sum them up and that gives me dc over dz. Okay? It's just the formula

44:01.760 --> 44:08.800
here. Okay? So here's the cool trick about back propagation through a linear module that computes

44:08.800 --> 44:14.480
weighted sums. You take the same weights and you still compute weighted sum with those weights,

44:14.480 --> 44:20.480
but you use the weights backwards. Okay? So whenever you had the unit that was sending its output to

44:20.480 --> 44:27.200
multiple outputs to multiple units through a weight, you take the gradient of the cost with

44:27.200 --> 44:34.000
respect to all those weighted sums and you compute their weighted sum backwards using the weights

44:34.000 --> 44:41.840
backwards to get the gradient with respect to the state of the unit at the bottom. You can do

44:41.840 --> 44:48.720
this for all the units. Okay? So it's super simple. Now, if you were to write a program to do backprop

44:48.720 --> 44:54.880
for classical neural nets in Python, it would take like half a page. It's very, very simple.

44:57.520 --> 45:01.280
Is one function to compute weighted sums going forward in the right order?

45:01.360 --> 45:07.840
Another function and applying the nonlinearity is another function to compute weighted sums

45:07.840 --> 45:13.120
weighted sums going backward and multiplying by the derivative of the nonlinearity at every step.

45:14.160 --> 45:19.200
Right? It's incredibly simple. What's surprising is that it took so long for people to realize this

45:19.200 --> 45:26.480
was so useful, maybe because it was too simple. Okay? So it's useful to write this in matrix form.

45:26.480 --> 45:34.800
So really, the way you should think about a neural net of this type is each state inside the

45:34.800 --> 45:38.560
network, think of it as a vector. It could be a multidimensional array, but let's think of it

45:38.560 --> 45:44.000
just as a vector. A linear operation is just going to multiply this vector by matrix and each row of

45:44.000 --> 45:48.880
the matrix contains all the weights that are used to compute a particular weighted sum for a particular

45:48.880 --> 45:58.560
unit. Okay? So multiply this by this matrix. So this dimension has to be equal to that dimension,

45:58.560 --> 46:02.560
which is not really well depicted here, actually. One sec. From the previous slide,

46:02.560 --> 46:11.040
you wrote ds0. What is s, differentiated with respect to? So there is a ds. What is ds, basically?

46:11.600 --> 46:21.600
ds0, you mean? Yeah. Okay. ds0 is a perturbation of s0. Okay? An infinitely small perturbation of s0.

46:23.040 --> 46:28.880
Doesn't matter what it is. Okay? And what we're saying here is that if you have an infinitely

46:28.880 --> 46:34.960
small perturbation of s0, and you multiply this perturbation by the partial derivative of c with

46:34.960 --> 46:45.280
respect to s0, okay? You get the perturbation of c, except that that corresponds to this

46:45.280 --> 46:49.440
perturbation of s0, right? But we're not interested in just the perturbation of s0. We're

46:49.440 --> 46:54.080
also interested in the perturbation of s1 and s2. So the overall perturbation of c would be the sum

46:54.080 --> 47:00.160
of the perturbations of s0, s1, and s2 multiplied by the corresponding partial derivative of c

47:00.160 --> 47:06.960
with respect to each of them. Okay? You know, it's a virtual thing, right? It's not an existing

47:06.960 --> 47:12.080
thing you're going to manipulate. Just imagine that there is some perturbation of s0 here.

47:13.200 --> 47:16.880
Okay? This is going to perturb c by some value, and that value is going to be the perturbation

47:16.880 --> 47:22.640
of s0 multiplied by the partial derivative of c with respect to s0. Okay? And then if you perturb

47:23.360 --> 47:29.440
s1 simultaneously, you're also going to cause a perturbation of c. If you perturb s2 simultaneously,

47:29.440 --> 47:34.560
you're also going to cause a perturbation of c. The overall perturbation of c will be the sum

47:34.560 --> 47:40.960
of those perturbations, and that is given by this expression here. Now, those d, those infinitely

47:40.960 --> 47:47.600
small quantities, ds, dc, etc., think of them as, you know, numbers. You can do algebra with them.

47:47.600 --> 47:51.680
You can divide one by the other. You know, you can do stuff like that. So now you say, you know,

47:51.680 --> 48:06.080
what is ds0 equal to? If I tweak z by a quantity dz, it's going in turn to modify s0 by ds0.

48:06.080 --> 48:14.880
Okay? And what is the quantity by which s0 is going to be tweaked? If I tweak z by dz,

48:14.880 --> 48:22.400
because s is the result of computing the product of z by w0, then the perturbation is also going

48:22.400 --> 48:29.360
to be multiplied by w0, right? So the ds0 corresponding to a particular dz is going

48:29.360 --> 48:36.000
to be equal to dz times w0. And this is what's expressed here. Okay? ds0 equal w0 dz.

48:37.200 --> 48:42.160
Okay. Now, if I take this expression for ds0 and I insert it here in this formula,

48:43.040 --> 48:50.960
okay, I get dc equal w0 times dz times dc over ds0 plus same thing for 1 plus same thing for 2.

48:50.960 --> 48:57.360
And I'm going to take the dz and pass it to the other side. I'm going to divide both sides by dz.

48:57.360 --> 49:04.000
So now I get dc over dz equal, the dz doesn't appear anymore because it's been put underneath

49:04.000 --> 49:13.280
here. It's w0 times dc over ds0 plus w1 times dc over ds1, et cetera. Okay? It's just simple algebra.

49:15.360 --> 49:20.160
It's differential calculus, basically. Right. So it's better to write this in matrix form.

49:21.680 --> 49:30.240
So really, when you're computing, if I go back a few slides, when this is really kind of a matrix

49:30.240 --> 49:36.400
of all the weights that are kind of upstream of the zj's, so you can align the zj as a vector,

49:37.600 --> 49:45.360
maybe only the zj's that have nonzero terms in w, wij. And then you can write those w's

49:45.360 --> 49:51.120
as a matrix, and this is just a matrix vector product. Okay? So this is the way this would be

49:51.120 --> 49:55.520
written. You have a vector, you multiply by matrix, you get a new vector, pass that through

49:55.520 --> 50:01.840
nonlinearities, reuse, multiply that by matrix, et cetera. Right? So symbolically, you can write

50:02.400 --> 50:08.000
a simple neural net this way. We have linear blocks, okay, linear functional blocks, which

50:08.000 --> 50:15.200
basically take the previous state and multiply by matrix. Okay? So you have a state here, z1,

50:15.200 --> 50:21.760
multiply by matrix, you get w1, z1, and that gives you the vector of weighted sums, s2.

50:21.760 --> 50:29.440
Okay? Then you take that, pass it through the nonlinear functions, each component individually,

50:30.160 --> 50:36.800
and that gives you z2. Right? So that's a three-layer neural net. First weight matrix,

50:36.800 --> 50:41.520
nonlinearity, second weight matrix, nonlinearity, third weight matrix, and this is the output.

50:41.520 --> 50:47.920
There are two hidden layers, three layers of weights. Okay, the reason for writing it this way

50:47.920 --> 50:54.720
is that this is, like symbolically, the easiest way to understand really what kind of backprop

50:54.720 --> 51:01.120
does. And in fact, it corresponds also to the way we define neural nets and we run them on

51:03.120 --> 51:11.360
deep learning frameworks like PyTorch. So this is the sort of object-oriented version of

51:12.320 --> 51:19.440
defining a neural net in PyTorch. We're going to use predefined class, which are the linear class

51:20.480 --> 51:27.040
that basically multiplies a vector by matrix. It also has biases, but let's not talk about this

51:27.040 --> 51:32.320
just now. And another class, which is the value function, which takes a vector or a multi-dimensional

51:32.320 --> 51:38.800
array and applies the nonlinear function to every component separately. Okay, so this is

51:38.800 --> 51:46.000
a little piece of Python program that uses Torch. We import Torch. We make an image, which is, you

51:46.000 --> 51:51.520
know, 10 pixels by 20 pixels and three components for color. We compute the size of it and we're

51:51.520 --> 51:56.320
going to plug a neural net where the number of inputs is the number of components of our image.

51:56.320 --> 52:02.000
So in this case, that would be 600 or so. And we're going to define a class. The class is going

52:02.000 --> 52:06.720
to define a neural net and that's pretty much all we need to do here. So we define our network

52:06.720 --> 52:10.000
architecture. It's a subclass of neural net module, which is a pretty fine class.

52:11.520 --> 52:16.880
It's got a constructor here that will take the sizes of the internal layers that we want,

52:16.880 --> 52:23.360
the size of the input, the size of S1 and Z1, the size of S2 and Z2, and the size of S3.

52:25.040 --> 52:32.720
We call the parent class initializer. And then we just create three modules that are all linear

52:33.680 --> 52:37.040
modules. And we need to kind of store them somewhere because they have internal parameters.

52:37.040 --> 52:43.120
So we're going to have three slots in our object, N0, N1, N2, module 1, module 0, module 1, module 2.

52:44.000 --> 52:49.440
And each of them is going to be an instance of the class NN.linear with two sizes, the input size

52:49.440 --> 52:55.440
and the output size. Okay, so the first module has input size D0, output size D1, etc. And those

52:55.440 --> 53:00.880
classes are, since there is a capital L, means it's an object and inside there are parameters

53:00.880 --> 53:08.000
inside that item there. Right. So for example, the value doesn't have a capital because it doesn't

53:08.000 --> 53:12.960
have internal parameters. It's not kind of a trainable module. It's just a function. Whereas

53:12.960 --> 53:17.120
those things with capitals, they have sort of internal parameters, the weight matrices inside

53:17.120 --> 53:23.760
of them. So now we define a forward function, which basically computes the output from the input.

53:24.480 --> 53:30.800
And the first thing we do is we take the input thing, which may be a multidimensional array,

53:30.800 --> 53:38.480
and we flatten it. We flatten it using this idiomatic expression here in PyTorch.

53:39.840 --> 53:48.320
And then we apply the first module to X. We put the result in S1, which is a temporary variable,

53:48.960 --> 53:56.000
then we apply the value to S1, put the result in Z, then apply the second layer,

53:56.000 --> 54:01.840
put the result in S2, apply the value again, put the result in S3, and then the last linear

54:01.840 --> 54:06.960
layer, put the result in S3 and return S3. And there is a typo. So the second line should have

54:06.960 --> 54:20.320
been S1, it's the self.m0 of Z0, right? Z0 here, yes, correct. Yeah, this is something that

54:21.040 --> 54:27.520
is going to be fixed, right? Which I didn't fix. I know. This is Z0. Thanks for reminding me of this.

54:30.480 --> 54:34.960
Okay, but you'll see examples. I mean, I'll show you kind of actual examples of this,

54:34.960 --> 54:39.040
and you'll be able to run them yourself. That's all you need to do. You don't need to write

54:41.440 --> 54:46.240
how you compute the back prop, how you propagate the gradients. You could write it,

54:46.240 --> 54:49.600
and it would be as simple as forward. You could write a backward function, and it would basically

54:50.640 --> 54:54.160
multiply by the matrices going backwards. But you don't need to do this because

54:54.160 --> 54:58.720
PyTorch does this automatically for you. When you define the forward function, it knows what

54:59.520 --> 55:02.960
modules you've called in what order, what are the dependencies between the variables,

55:02.960 --> 55:07.280
and it will know how to generate the functions that compute the gradient

55:07.280 --> 55:12.400
backwards. So you don't need to worry about it. That's the magic of PyTorch, if you want.

55:12.400 --> 55:16.720
That's a bit the magic of deep learning, really. That's called automatic differentiation,

55:18.720 --> 55:23.120
and this is a particular form of automatic differentiation. There's another way to write

55:23.680 --> 55:28.000
functions in PyTorch that are kind of more functional. So you're not using modules

55:28.000 --> 55:31.840
with internal parameters. You're just coding functions one after the other. And PyTorch

55:31.840 --> 55:36.800
has a mechanism by which it can automatically compute the gradient of any function you define

55:36.800 --> 55:42.240
with respect to whatever parameters you want. Yeah, actually, these big guys with the capital L,

55:42.240 --> 55:48.720
like the nn.capital linear inside is going to have a lowercase linear, which is like the functional

55:48.720 --> 55:55.040
part, which is performing the matrix multiplication between the weights stored inside the object

55:55.040 --> 56:01.920
with the capital L and then the input. So every capital letter object will inside have

56:01.920 --> 56:07.680
the functional way. So one can decide to use either the functional form by default,

56:07.680 --> 56:13.840
or use this encapsulated version, which are more convenient to just use, right?

56:14.400 --> 56:19.920
Right. So at the end, you can create an instance of this class. You can create multiple instances,

56:19.920 --> 56:24.080
but you can create one here, just call my net and give it the sizes you want.

56:25.360 --> 56:31.360
And then to apply this to a particular image, you just do how to equal model of image. That's

56:31.360 --> 56:38.320
as simple as that. Okay, so this is your first neural net, and it does all the backup automatically.

56:39.520 --> 56:44.800
But you need to understand how that works, right? It's not because PyTorch does it for you,

56:45.360 --> 56:51.440
that you can sort of forget about how you actually compute the gradient of a function,

56:51.440 --> 56:54.320
because it's inevitable that at some point, you're going to want to

56:54.960 --> 56:58.240
actually assemble a neural net with a module that does not pre-exist, and you're going to have to

56:58.240 --> 57:04.560
write your own backup function. So to do this, you basically have, if you want to create a new module

57:04.560 --> 57:11.840
with some complex operation that does not pre-exist in PyTorch, then you do something like

57:11.840 --> 57:16.880
this. You define your class, but you write your own backward function, basically.

57:18.960 --> 57:28.560
Okay, so let's get one step up in terms of abstraction, and write this in sort of slightly more

57:31.120 --> 57:38.480
generic form, mathematical form, if you want. So let's say we have a cost function here,

57:39.040 --> 57:44.560
and we want to compute the gradient of this cost function with a stack to a particular

57:44.560 --> 57:48.880
vector in the system ZF. It could be a parameter, it could be a state, it doesn't matter. Okay,

57:49.680 --> 57:56.320
some states inside. And we have chain rule, and chain rule is nothing more than this,

57:56.320 --> 58:04.320
that I explained earlier. dC over dZF is equal to dC over dZG, dZG over dZF, as long as C

58:04.640 --> 58:13.760
is only influenced by ZF through ZG. There's no other way for ZF to influence C than to go

58:13.760 --> 58:20.160
through ZG, then this formula is correct. Okay? And of course, the identity is trivial,

58:20.160 --> 58:28.640
because it's just a simplification by this infinitesimal vector quantity dZG. Okay?

58:29.600 --> 58:36.080
So let's say ZG is a vector of size dG by one, so this means column vector. Okay?

58:36.800 --> 58:40.560
And ZF is a column vector of size dF.

58:45.600 --> 58:49.280
This is, if you want to write the correct dimensions of this,

58:51.200 --> 58:55.680
you know, we get something a little complicated. Okay, so first of all,

58:56.640 --> 59:03.440
this object here, dZG over dZF, well, let me start with this one. Okay,

59:03.440 --> 59:08.800
this one dC over dZG, that's a gradient vector. Okay? ZG is a vector, dC over dZG is a gradient

59:08.800 --> 59:16.320
vector. And it's the same size as dZG. But by convention, we actually write it as a line,

59:16.320 --> 59:23.440
as a row vector. Okay? So this thing here is going to be a row vector whose size is the same size

59:23.440 --> 59:27.440
as ZG, but it's going to be horizontal instead of vertical. Okay?

59:30.400 --> 59:33.600
This object here is something more complicated. It's actually a matrix.

59:34.640 --> 59:40.800
Why is it a matrix is because it's the derivative of a vector with respect to another vector.

59:40.800 --> 59:46.240
Okay? So let's look at this diagram here on the right. We have a function G, it takes ZF as an

59:46.240 --> 59:52.640
input, and it produces ZG as an output. And if we want to capture the information about the

59:52.640 --> 01:00:00.320
derivative of that module, which is this quantity here dZG over dZF, there's a lot of terms to

01:00:00.320 --> 01:00:06.160
capture because there's a lot of ways in which every single output, every component of ZG can be

01:00:06.160 --> 01:00:11.680
influenced by every component of ZF. Right? So if for every pair of components, ZG and ZF,

01:00:12.240 --> 01:00:19.040
there is a derivative term, which indicates by how much ZG would be perturbed if I perturbed ZF

01:00:19.040 --> 01:00:27.920
by a small infinitesimal quantity. Right? We have that for every pair of components of ZG and ZF.

01:00:27.920 --> 01:00:35.760
As a result, this is a matrix whose dimension is the number of rows is the size of ZG and the

01:00:35.760 --> 01:00:46.160
number of columns is the size of ZF. And each term in this matrix is one partial derivative term.

01:00:46.160 --> 01:00:53.760
So this whole matrix here, if I take the component ij, it's the partial derivative of the i-th

01:00:53.760 --> 01:01:02.560
output of that module, the i-th component of ZG, with respect to the j-th component of ZF.

01:01:04.000 --> 01:01:12.000
Okay? So what we get here is a row vector is equal to a row vector multiplied by a matrix,

01:01:12.000 --> 01:01:18.720
and the sizes kind of work out so that they're compatible with each other.

01:01:19.920 --> 01:01:23.280
Okay. So what is back propagation now? Back propagation is this formula.

01:01:24.720 --> 01:01:29.200
Okay? It says if you have the gradient of some cost function with respect to some variable,

01:01:29.200 --> 01:01:32.080
and you know the dependency of these variables with respect to another variable,

01:01:32.080 --> 01:01:36.960
you multiply this gradient vector by that Jacobian matrix, and you get the gradient

01:01:36.960 --> 01:01:43.120
vector with respect to that second variable. So graphically here on the right, if I have

01:01:43.920 --> 01:01:49.520
the gradient of the cost with respect to ZG, which is DC over DZG, and I want to compute

01:01:49.520 --> 01:01:56.080
the gradient of C with respect to ZF, which is DC over DZF, I only need to take that vector,

01:01:56.080 --> 01:02:03.040
which is a row vector, multiply it by the Jacobian matrix, DG over DZF, or DZG over DZF,

01:02:03.920 --> 01:02:11.280
and I get DC over DZF. Okay? It's this formula. Someone is objecting here. Isn't the summation

01:02:11.280 --> 01:02:19.840
missing here? Which summation? Summation of all the components of these partial multiplications.

01:02:19.840 --> 01:02:24.480
Here? Yeah. Well, this is a vector. This is a vector. This is a matrix. There is a lot of

01:02:24.480 --> 01:02:27.760
sums going on here because when you compute the product of this vector with its matrix,

01:02:27.760 --> 01:02:32.720
you're going to have a lot of sums, right? Yep. So it's hidden, right? Yeah. The sums are hidden.

01:02:33.040 --> 01:02:35.520
Okay. Inside of this vector matrix product.

01:02:40.080 --> 01:02:44.320
Like, you can take a specific example. Let's imagine that this G function is just a matrix

01:02:44.320 --> 01:02:52.160
multiplication. Okay? We just multiply by ZF by matrix W. So we have a linear operation. The derivative

01:02:52.160 --> 01:02:59.360
of the Jacobian matrix of the multiplication by matrix is the transpose of that matrix. So what

01:02:59.360 --> 01:03:04.000
we're going to do here is take this vector, multiply it by the transpose of the W matrix,

01:03:04.800 --> 01:03:13.200
and what we get is that vector. Okay? And it all makes sense, right? The sizes make sense.

01:03:13.200 --> 01:03:19.600
This matrix here is the transpose of the weight matrix, which of course had the reverse size.

01:03:20.640 --> 01:03:24.240
We multiply it. We pre-multiply it by the row vector of the gradient from the

01:03:25.200 --> 01:03:28.080
layer above, and we get the gradient with respect to the layer below.

01:03:30.640 --> 01:03:35.760
Okay? So backpropagating through a linear module just means multiplying the transpose

01:03:35.760 --> 01:03:42.000
of the matrix used by that module. And it's just a generalized form of what I explained earlier,

01:03:42.560 --> 01:03:48.000
you know, of propagating through the weights of a linear system. But it's less intuitive, right?

01:03:50.080 --> 01:03:53.840
Okay. So we're going to be able to do backpropagation by computing gradients all the way through,

01:03:53.840 --> 01:04:02.080
by propagating backwards. But this module really has two inputs. It has an input, which is ZF,

01:04:02.080 --> 01:04:11.040
and the other one is WG, the weight matrix, the parameter vector that is used inside of this

01:04:11.040 --> 01:04:17.760
module. So there is a second Jacobian matrix, which is the Jacobian matrix of ZG with respect to

01:04:18.560 --> 01:04:25.200
the terms of this weight parameter. Okay? And to compute the gradient of the cost function

01:04:25.200 --> 01:04:32.080
with respect to those weight parameters, I need to multiply this gradient vector by the Jacobian

01:04:32.080 --> 01:04:37.360
matrix of that block with respect to its weight. And it's not the same as the Jacobian matrix with

01:04:37.360 --> 01:04:44.480
respect to the input. It's a different Jacobian matrix. I'll come back to this in a second.

01:04:45.120 --> 01:04:55.200
So to do backprop, again, if we have a vector of gradients of some cost with respect to a state,

01:04:55.920 --> 01:05:00.960
and we have a function that is a function of one or several variables, we multiply this gradient by

01:05:00.960 --> 01:05:06.160
the Jacobian matrix of this block with respect to each of these inputs, and that gives us the

01:05:06.160 --> 01:05:11.520
gradient with respect to each of the inputs. And that's going to be expressed here. So this

01:05:11.520 --> 01:05:21.440
is the backpropagation of states in a layer-wise classical type neural net. DC over DZK, which is

01:05:21.440 --> 01:05:27.920
the state of layer K, is DC over ZK plus one, which is the gradient of the cost with respect to

01:05:27.920 --> 01:05:35.120
the layer above, times the Jacobian matrix of the state of layer K plus one with respect to the

01:05:35.200 --> 01:05:42.000
state of layer K. Now we assume DC over DZK plus one is known, and we just need to multiply

01:05:42.000 --> 01:05:46.720
with the Jacobian matrix of the function that links ZK to ZK plus one. The function is used to

01:05:46.720 --> 01:05:51.280
compute ZK plus one from ZK. And this may be a function also of some parameters inside.

01:05:51.280 --> 01:05:57.440
But here, that's the matrix of partial derivatives of F, which is with output to ZK plus one,

01:05:57.440 --> 01:06:04.800
with respect to each of the components of ZK. So that's the first rule of backpropagation,

01:06:04.800 --> 01:06:09.840
and it's a recursive rule. So you can start from the top. You start initially with DC over DC,

01:06:09.840 --> 01:06:16.640
which is one, which is why I have this one here on top. And then you just keep multiplying by

01:06:17.680 --> 01:06:23.360
the Jacobian matrix all the way down, and backpropagate gradients. And now you get gradients

01:06:23.360 --> 01:06:26.960
with respect to all the states. You also want the gradients with respect to the weights,

01:06:26.960 --> 01:06:32.240
because that's what you need to do learning. So what you can write is the same chain rule,

01:06:32.240 --> 01:06:38.160
DC over DWK is equal to DC over the ZK plus one, which we assume is known, times

01:06:38.160 --> 01:06:44.240
DZK plus one of DWK, right? And you can write this as DC over DK plus one. And the dependency

01:06:44.240 --> 01:06:50.560
between ZK plus one and WK is the function ZK applied to WK. So you can differentiate the

01:06:51.920 --> 01:06:55.280
function, the output of the function ZK with respect to WK, and that gives you another

01:06:55.280 --> 01:06:59.360
Jacobian matrix. And so those two formulas, you can do backpropagation just about anything.

01:07:00.080 --> 01:07:06.320
Really what goes on inside PyTorch and inside most of those frameworks, TensorFlow and

01:07:06.320 --> 01:07:12.000
Jackson, whatever. It's something like this where you have, so let's take a very simple

01:07:12.000 --> 01:07:16.960
diagram here where you have an input parameterized function that computes an output that goes to

01:07:16.960 --> 01:07:21.520
a cost function. And that cost function measures the discrepancy between the output of the system

01:07:21.600 --> 01:07:29.840
and the desired output. So you can write this function as C of G of W. I didn't put the X here,

01:07:29.840 --> 01:07:37.920
but just for charity. And the derivative of this is, again, you apply chain rule or you can write

01:07:38.000 --> 01:07:51.600
it with partial derivatives this way. And same for, you know, expand the dependency of the output

01:07:51.600 --> 01:07:58.160
with respect to the parameters as the Jacobian matrix of G with respect to W. If W is a scalar,

01:07:58.160 --> 01:08:06.240
then this is just a derivative, partial derivative. Okay, now you can express this as a compute graph.

01:08:06.240 --> 01:08:11.920
So you can say, like, how am I going to compute DC over DW? What I'm going to have to do is take

01:08:11.920 --> 01:08:15.920
the value one, which is the derivative of C with respect to itself, basically,

01:08:15.920 --> 01:08:21.200
the loss with respect to itself. I'm going to multiply this by the derivative of the cost with

01:08:21.200 --> 01:08:32.800
respect to Y bar. Okay, and that's going to give me DC over DY bar, obviously. Okay, this is the

01:08:32.880 --> 01:08:38.000
same as this because I'm just multiplied by one. Then multiply this by the Jacobian matrix of G

01:08:38.000 --> 01:08:43.840
with respect to W, which is a derivative if W is a scalar. That, of course, depends on X.

01:08:45.280 --> 01:08:52.000
And I get DC over DW. So this is a so-called compute graph, right? This is a way of organizing

01:08:52.000 --> 01:08:58.560
operations to compute the gradient. And there is essentially an automatic way of transforming

01:08:58.640 --> 01:09:05.200
a compute graph of this type into a compute graph of this type that computes the gradient

01:09:05.200 --> 01:09:11.920
automatically. And this is the magic that happens in the automatic differentiation inside PyTorch and

01:09:11.920 --> 01:09:18.000
TensorFlow and other systems. Some systems are pretty smart about this in a sense that

01:09:19.120 --> 01:09:24.640
those functions can be fairly complicated. They can involve themselves computing derivatives and

01:09:25.600 --> 01:09:30.400
they can involve dynamic computation, where the graph of computation depends on the data.

01:09:31.200 --> 01:09:35.920
And actually PyTorch does this properly. I'm not going to go through all the details of this,

01:09:35.920 --> 01:09:41.120
but this is kind of a way of reminding you what the dimensions of all those things are, right? So

01:09:41.920 --> 01:09:46.160
if Y is a column vector of size M, W is a column vector of size N,

01:09:47.600 --> 01:09:51.920
then this is a row vector of size N, this is a row vector of size M, and this is a

01:09:51.920 --> 01:09:57.600
geocomium matrix of size N by N. And all of this works out.

01:10:01.680 --> 01:10:06.480
Okay, so the way we're going to build neural nets, and I'll come back to this in a

01:10:06.480 --> 01:10:15.280
subsequent lecture, is that we are going to have at our disposal a large collection of basic modules

01:10:15.280 --> 01:10:21.200
which we're going to be able to arrange in more or less complex graphs

01:10:22.400 --> 01:10:30.320
as a way to build the architecture of a learning system. Okay, so either we're going to write a

01:10:30.320 --> 01:10:36.640
class or we're going to write a program that runs the forward pass, and this program is going to be

01:10:37.440 --> 01:10:44.160
composed of basic mathematical operations, addition, subtraction of tensors or multi-dimensional arrays,

01:10:45.840 --> 01:10:50.560
other types of scalar operations, or the application of one of the predefined

01:10:51.760 --> 01:10:57.760
complex parameterized functions, like a linear module, a value, or things like that.

01:11:00.320 --> 01:11:08.080
And we have at our disposal a large library of such modules, which are things that people have

01:11:08.080 --> 01:11:14.640
come up with over the years that are kind of basic modules that are used in a lot of applications.

01:11:15.280 --> 01:11:19.280
Right, so the basic things that we've seen so far I think is like values. There's other

01:11:19.280 --> 01:11:24.400
nonlinear functions like sigmoids and variations of this. There's a large collection of them.

01:11:25.280 --> 01:11:28.800
And then we have cost functions like square error, cross entropy, hinge loss, ranking loss,

01:11:28.800 --> 01:11:32.400
and blah, blah, blah, which I'm not going to go through now, but we'll talk about this later.

01:11:35.520 --> 01:11:42.880
The nice thing about this formalism is that, as I said before, you can sort of compute

01:11:43.440 --> 01:11:52.640
graphs. You can construct a deep learning system by assembling those modules in any kind

01:11:52.640 --> 01:11:59.360
of arrangement you want, as long as there is no loops in the connection graph. So as long as

01:11:59.920 --> 01:12:04.080
you can come up with a partial order in those modules that will ensure that they are

01:12:04.080 --> 01:12:11.040
computed in the proper way. But there is a way to handle loops, and that's called recurrent

01:12:11.040 --> 01:12:18.400
nets. We'll talk about this later. Okay, so here's a few practical tricks if you want to

01:12:18.400 --> 01:12:23.360
play with neural nets, and you're going to do that soon enough, perhaps even tomorrow.

01:12:28.800 --> 01:12:32.960
And these are kind of a bit of a black art of deep learning, which is sort of a lot of it is

01:12:32.960 --> 01:12:37.280
implemented already in things like PyTorch if you used under tools, but some of it is kind of more

01:12:37.280 --> 01:12:42.000
of the sort of oral culture if you want of the deep learning community. You can find this in

01:12:42.000 --> 01:12:49.760
papers, but it's a little difficult to find sometimes. So most neural nets use values as

01:12:49.760 --> 01:12:55.280
the main nonlinearity, so this sort of half wave rectifier. Hyperbole tangent, which is a

01:12:55.280 --> 01:13:00.160
similar function, and logistic function, which is also a similar function, are used, but not as

01:13:00.160 --> 01:13:04.880
much, not nearly as much. You need to initialize the ways properly. So if you have a neural net

01:13:04.880 --> 01:13:10.400
and you initialize the ways to zero, it never takes off. It will never learn. The gradients

01:13:10.400 --> 01:13:17.200
will always be zero all the time. And the reason is because when you back propagate the gradient,

01:13:17.200 --> 01:13:21.600
you multiply by the transpose of the weight matrix. If that weight matrix is zero, your gradient is

01:13:21.600 --> 01:13:27.360
zero. So if you start with all the weights equal to zero, you never take off. And someone asked

01:13:27.360 --> 01:13:34.640
the question about saddle points before. Zero is a saddle point. And so if you start at this

01:13:34.640 --> 01:13:40.000
saddle point, you never get out of it. So you have to break the symmetry in the system. You have to

01:13:40.000 --> 01:13:46.800
initialize the weights to small random values. They don't need to be random, but it works fine

01:13:46.800 --> 01:13:54.160
if they're random. And the way you initialize is actually quite important. So there's all kinds of

01:13:54.160 --> 01:13:59.040
tricks to initialize things properly. One of the tricks was invented by my friend,

01:13:59.920 --> 01:14:05.760
about 30 years ago, even more than that, actually, 34 years ago, almost. Unfortunately, now it's

01:14:05.760 --> 01:14:11.680
called differently. It's called the kaming trick, but it's the same. And it consists in

01:14:12.240 --> 01:14:17.280
initializing the weights to random values in such a way that if a unit has many inputs, the weights

01:14:17.280 --> 01:14:22.640
are smaller than if it has few inputs. And the reason for this is that you want the weighted

01:14:22.640 --> 01:14:29.680
sum to be roughly kind of have some reasonable value. If the input variables have some reasonable

01:14:29.680 --> 01:14:36.160
value, let's say variance one or something like this, and you're computing a weighted sum of them,

01:14:36.160 --> 01:14:41.840
the weighted sum, the size of the weighted sum is going to grow like the square root of the number

01:14:41.840 --> 01:14:46.880
of inputs. And so you want to set the weights to something like the inverse square root if you want

01:14:46.880 --> 01:14:54.160
the weighted sum to be kind of about the same size as each of the inputs. So that's built into

01:14:54.160 --> 01:15:00.160
PyTorch. You can call this, you know, initialization procedure. What's the exact name of it? I can't

01:15:00.160 --> 01:15:06.160
remember. The one that is coming, coming, coming here, then there is the Xavier and then there is

01:15:06.160 --> 01:15:10.800
also yours we have in PyTorch. Yeah, they're slightly different, but they kind of do the same

01:15:10.800 --> 01:15:19.120
more or less. Yeah, the Xavier Glow version, yeah. Yeah, this one divides by the Fennin and Fennin.

01:15:20.880 --> 01:15:25.520
There's various loss functions, so I haven't talked yet about what the cross-entropy loss is,

01:15:25.520 --> 01:15:30.160
but cross-entropy loss is a particular cost that's used for classification. I'll probably

01:15:30.160 --> 01:15:35.600
talk about this next week and I'll have some time at the end of this lecture. This is for

01:15:35.600 --> 01:15:41.760
classification. As I said, we use stochastic gradient descent on mini-batches and mini-batches

01:15:41.760 --> 01:15:46.560
only because the hardware that we have needs mini-batches to perform properly. If we had

01:15:46.560 --> 01:15:52.160
different hardware, we would use mini-batch size one. As I said before, we need to shuffle the

01:15:52.160 --> 01:15:58.960
training samples. So if someone gives you a training set and puts all the examples of category one,

01:15:58.960 --> 01:16:04.080
then all the example category two, all the example category three, etc. If you use stochastic gradient

01:16:04.080 --> 01:16:09.760
by keeping this order, it is not going to work. You have to shuffle the samples so that

01:16:11.520 --> 01:16:17.360
you basically get samples from all the categories within kind of a small subset, if you want.

01:16:19.680 --> 01:16:24.160
There is an objection here for the stochastic gradient. Isn't Adam better?

01:16:24.880 --> 01:16:35.920
All right. Okay. There is a lot of variants of stochastic gradient. There are all stochastic

01:16:35.920 --> 01:16:42.240
gradient methods. In fact, people in optimization said this should not be called stochastic gradient

01:16:42.240 --> 01:16:47.360
descent because it's not a descent algorithm because stochastic gradient sometimes goes uphill

01:16:47.360 --> 01:16:55.200
because of the noise. So people who want to really kind of be correct about this say it's

01:16:55.200 --> 01:16:58.400
stochastic gradient optimization, but not stochastic gradient descent. That's the first thing.

01:16:59.520 --> 01:17:05.280
Stochastic gradient optimization or stochastic gradient descent, SGD, is a special case of gradient

01:17:05.280 --> 01:17:13.280
based optimization. The specification of it says you have to have a step size eta,

01:17:14.000 --> 01:17:18.720
but nobody tells you how you set this step size eta and nobody tells you that this step size is

01:17:18.720 --> 01:17:26.320
a scalar or a diagonal matrix or a full matrix. Okay. So there are variations of SGD in which

01:17:27.440 --> 01:17:31.360
eta is changed all the time for every sample or every batch.

01:17:34.720 --> 01:17:39.280
In SGD, most of the time this eta is decreased according to a schedule and there are a bunch

01:17:39.280 --> 01:17:48.000
of standard schedule in PyTorch that are implemented. In techniques like Adam, the eta is actually a

01:17:48.000 --> 01:17:52.160
diagonal matrix and that diagonal matrix, the term in the diagonal matrix are changed all the

01:17:52.160 --> 01:17:58.640
time. They're computed based on some estimate of the curvature of the cost function. There's a lot

01:17:58.640 --> 01:18:06.720
of methods to do this. Okay. They're all SGD type methods. Okay. Adam is an SGD method with a special

01:18:06.720 --> 01:18:16.320
type of eta. So yeah, in the opt-in package in Torch, there's a whole bunch of those methods.

01:18:19.040 --> 01:18:23.280
There's going to be a whole lecture on this, so don't worry about it, about optimization.

01:18:25.520 --> 01:18:31.920
Normalize the input variables to zero mean and unit variance. So this is a very important point that

01:18:32.560 --> 01:18:40.240
this type of optimization method, gradient based optimization methods, when you have weighted

01:18:40.240 --> 01:18:46.400
sounds, kind of linear operations, tends to be very sensitive to how the data is prepared.

01:18:47.440 --> 01:18:50.400
So if you have two variables that have very widely different

01:18:51.360 --> 01:18:56.480
variances, one of them varies between, let's say, minus one and plus one. The other one

01:18:56.480 --> 01:19:02.880
varies between minus 100 and plus 100. The system will basically not pay attention to the one that

01:19:02.880 --> 01:19:08.240
varies between plus one and minus one. We'll only pay attention to the big one. And this may be good

01:19:08.240 --> 01:19:13.360
or this may be bad. Furthermore, the learning rate you're going to have to use the eta parameter,

01:19:13.360 --> 01:19:19.120
the step size, is going to have to be set to a relatively small value to prevent the weights

01:19:19.120 --> 01:19:27.520
that look at this highly variable input from diverging. The gradients are going to be very

01:19:27.520 --> 01:19:31.680
large because the gradients basically are proportional to the size of the input or even to

01:19:31.680 --> 01:19:36.000
the variance of the input. So if you don't want your system to diverge, you're going to have to

01:19:36.000 --> 01:19:43.520
tune down the learning rate if the input variance is large. If the input variables are all shifted,

01:19:43.600 --> 01:19:51.040
they're all between, let's say, 99 and 101 instead of minus one and one. Then again, it's very

01:19:51.040 --> 01:19:58.480
difficult for a gradient-based algorithm that use weighted sums to figure out those things.

01:19:58.480 --> 01:20:03.760
Again, I'll talk about this more formally later. Right now, just remember the trick

01:20:03.760 --> 01:20:07.840
that you need to normalize your input. So basically, take every variable of your input,

01:20:07.840 --> 01:20:12.320
subtract the mean, you compute the mean over the training set of each variable. So let's say your

01:20:12.320 --> 01:20:18.880
training set is a set of images. The images are, let's say, 100 by 100 pixels. Let's say they're

01:20:18.880 --> 01:20:23.760
grayscale, so you get 10,000 variables. And let's say you get a million samples, right? You're going

01:20:23.760 --> 01:20:32.080
to take each of those 10,000 variables, compute the mean of it over the training set, compute the

01:20:32.080 --> 01:20:38.000
standard deviation of it over the entire training set. And the samples you're going to show to your

01:20:38.000 --> 01:20:46.320
system are going to be a sample where you have subtracted the mean from each of the 10,000 pixels

01:20:46.320 --> 01:20:54.000
and divided the resulting values by the standard deviation that you computed.

01:20:55.360 --> 01:20:58.720
Okay? So now what you have is a bunch of variables that are all zero mean

01:20:59.520 --> 01:21:05.680
and all standard deviation equal to one. And that makes your neural net happy.

01:21:06.080 --> 01:21:08.880
That makes your optimization algorithm happy, actually.

01:21:08.880 --> 01:21:16.000
We have actually a question. So you keep repeating SGD type methods, gradient based methods,

01:21:16.000 --> 01:21:22.160
because there are other types of methods. Yes. Okay. So there is gradient free methods.

01:21:22.160 --> 01:21:29.040
So gradient free method is a method where you do not assume that the function you're trying to

01:21:29.040 --> 01:21:33.920
optimize is differentiable or even continuous with respect to the parameters.

01:21:34.640 --> 01:21:42.800
For several reasons, perhaps it's a function that looks like a golf course, right? It's flat and

01:21:42.800 --> 01:21:47.360
then maybe it's got steps and, you know, it's difficult to, like the local gradient information

01:21:47.360 --> 01:21:52.080
does not give you any information as to where you should go to find the minimum. Okay?

01:21:53.520 --> 01:22:00.080
It could be that the function is essentially discrete, right? It's not a function of continuous

01:22:00.080 --> 01:22:06.960
variables, function of discrete variables. So for example, am I going to win this chess game?

01:22:08.240 --> 01:22:11.760
The variable you can manipulate is the position on the board. That's a discrete variable.

01:22:13.280 --> 01:22:20.160
So you can't, you can compute a gradient of, you know, a score with respect to a position on

01:22:20.160 --> 01:22:29.120
the chess game. It's a discrete variable. Another example is the cost function is not

01:22:29.120 --> 01:22:33.360
something you can compute. You don't actually know the cost function. Okay? So for example,

01:22:34.720 --> 01:22:39.280
the only thing you can do is give an input to the cost function and it tells you the cost.

01:22:39.920 --> 01:22:43.600
But you can't, you don't know the function. It's not, right? It's not a program on a computer.

01:22:43.600 --> 01:22:48.720
You can't backprop a gradient to it. A good example of this is the real world. The real world,

01:22:49.840 --> 01:22:55.120
you can think of it as a cost function, right? You learn to ride a bike and you ride your bike

01:22:55.120 --> 01:23:04.160
and at some point you fall. The real world does not give you a gradient of that cost function,

01:23:04.160 --> 01:23:10.720
which is how much you hurt with respect to your actions. Okay? The only thing you can do is try

01:23:10.720 --> 01:23:16.080
something else and see if you get the same result or not. Okay? So what do you do in that case? So

01:23:16.080 --> 01:23:21.360
basically now your cost function is a black box. So now you cannot propagate gradient to this

01:23:21.360 --> 01:23:26.640
black box. What you have to do is estimate the gradient by perturbing the, what you see to

01:23:26.640 --> 01:23:34.080
that black box, right? So, you know, you try something, right? And that something would be a

01:23:34.080 --> 01:23:42.240
perturbation of your input to this black box and you see what resulting perturbation occurs

01:23:42.240 --> 01:23:47.760
on the black, on the output of the black box, the cost. And now you can estimate whether you,

01:23:48.480 --> 01:23:56.400
you know, this modification improved or made the result worse, right? So essentially,

01:23:56.400 --> 01:24:01.520
this is like this optimization problem I was telling you about earlier. The gradient based

01:24:01.520 --> 01:24:06.560
algorithm is like you are in the mountain, lost in the mountain in a fog, you can't see anything.

01:24:07.360 --> 01:24:11.920
But you can estimate the direction of steepest descent, right? You can just look around and you

01:24:11.920 --> 01:24:15.680
can tell which is the direction of steepest descent. You just take a step in that direction.

01:24:18.080 --> 01:24:24.160
What if you can't see, right? So basically to estimate in which direction the function goes

01:24:24.160 --> 01:24:30.720
down, you have to actually take a step, okay? So you take a step in one direction, then you

01:24:30.720 --> 01:24:33.920
come back, then you can take a step in the other direction, come back, and then maybe you get an

01:24:33.920 --> 01:24:38.480
estimate for where the steepest descent is. Now you can take a step for steepest descent. So this

01:24:38.480 --> 01:24:44.160
is estimating the gradient by perturbation instead of by analytic means of back propagating

01:24:44.160 --> 01:24:51.200
gradients, okay, computing Jacobians or whatever, partial derivatives. And then there is the second

01:24:51.200 --> 01:24:56.880
step of complexity. Let's imagine that the landscape you are in is basically flat everywhere,

01:24:56.880 --> 01:25:02.400
except, you know, once in a while there is a step, okay? So taking a small step in one direction

01:25:02.400 --> 01:25:06.480
will not give you any information about which direction you have to go to. So there you have

01:25:06.480 --> 01:25:12.400
to use other techniques, taking bigger steps, you know, working for a while and seeing if you

01:25:13.280 --> 01:25:19.120
fall down the step or not, or go up a step. You know, maybe you can multiply yourself in

01:25:19.120 --> 01:25:24.160
sort of 10,000 copies of yourself and then kind of explore the surroundings. And then whenever

01:25:24.160 --> 01:25:30.400
someone says, oh, I find a hole, calls everyone to kind of come there, okay? So all those methods

01:25:30.400 --> 01:25:36.800
are called gradient-free optimization algorithms. Sometimes they're called zero-th order method.

01:25:36.800 --> 01:25:40.320
Why zero-th order? Because first order is when you can compute the derivative.

01:25:40.320 --> 01:25:43.680
Zero-th order is when you cannot compute the derivative. You can only compute the function

01:25:43.680 --> 01:25:47.920
or get a value for the function. And then you have second order methods that compute not just

01:25:47.920 --> 01:25:52.640
the first derivative, but also the second derivative. And they're also gradient-based,

01:25:52.640 --> 01:25:56.640
okay, because they need the first derivative as well. But they can accelerate the process by

01:25:56.640 --> 01:26:02.640
also computing the second derivative. And Adam is a very simplified form of kind of, you know,

01:26:03.920 --> 01:26:08.720
second order method. It's not a second order method, but it has a hint of second order.

01:26:08.720 --> 01:26:11.680
Another hint of second order method is what's called conjugate gradient.

01:26:13.120 --> 01:26:16.480
It's another class of method called quasi-Newton methods, which are also kind of

01:26:17.520 --> 01:26:20.160
using kind of curvature information, if you want, to kind of accelerate.

01:26:21.280 --> 01:26:26.480
Many of those are not actually practical for neural net training, but there are some forms that are.

01:26:28.000 --> 01:26:33.760
If you're interested in zero-th order optimization, there is a library that is actually produced

01:26:34.720 --> 01:26:39.440
by, it's an open source library, which originated at Facebook Research in Paris

01:26:40.560 --> 01:26:44.320
by an author called Olivier Tito, but it's really a community effort. There's a lot of

01:26:44.320 --> 01:26:49.360
contributors to it. It's called Nevergrad. And it implements a very large number of different

01:26:49.360 --> 01:26:54.080
optimization algorithms that do not assume that you have access to the gradient. Okay.

01:26:55.200 --> 01:26:58.400
There are genetic algorithms or evolutionary methods. There are

01:26:59.360 --> 01:27:05.040
particle swarm optimization. There are perturbation methods. There is all kinds of tricks, right?

01:27:05.040 --> 01:27:09.360
I mean, there's a whole catalog of those things. And those sometimes it's unavoidable. You have

01:27:09.360 --> 01:27:14.720
to use them because you don't know the cost function. So a very common situation where you

01:27:14.720 --> 01:27:20.880
had to use those things is reinforcement learning. So reinforcement learning is basically a situation

01:27:20.880 --> 01:27:26.400
where you tell the system, you don't tell the system the correct answer. You only tell the

01:27:26.400 --> 01:27:30.960
system whether the answer was good or bad. It's because you give the value of the cost,

01:27:30.960 --> 01:27:34.000
but you don't tell the machine where the cost is. So the machine doesn't know where the cost

01:27:34.000 --> 01:27:40.400
function is. Okay. And so the machine cannot actually compute the gradient of the cost. And

01:27:40.400 --> 01:27:46.480
so it has to use something like a zero-th order method. So what you can do is you can compute

01:27:47.200 --> 01:27:50.720
a gradient with respect to the parameters of the overall cost function

01:27:51.520 --> 01:27:57.840
by perturbing the parameters. Or what you can do is compute the gradient of the cost function

01:27:57.840 --> 01:28:03.760
with respect to the output of your neural net. Okay. Using perturbation. And once you

01:28:03.760 --> 01:28:07.120
have this estimate, then you back propagate the gradient through your network using regular

01:28:07.120 --> 01:28:12.000
backprop. So that's a combination of estimating the gradient through perturbation for the cost

01:28:12.000 --> 01:28:16.720
function because you don't know it, and then backpropagating from there. This is basically

01:28:16.720 --> 01:28:22.160
the tactic that was used by the deep line people in sort of the first sort of deep

01:28:23.120 --> 01:28:30.400
queue learning type methods. Back to the normalization. Do we normalize the entire dataset

01:28:30.400 --> 01:28:41.200
or each batch? It's equivalent. So you normalize each sample, but the variable you're computing

01:28:41.200 --> 01:28:45.680
is on the entire training set, right? So you're computing the standard deviation

01:28:46.480 --> 01:28:51.280
and the mean over the entire training set. In fact, most of the time you don't even need to do it

01:28:51.280 --> 01:28:54.880
over the entire training set because mean and standard deviation converges pretty fast.

01:28:56.000 --> 01:29:02.320
So, but you do it over the entire training set, right? And what you get is a constant number,

01:29:02.320 --> 01:29:06.320
two constant numbers, a number that you subtract and a number that you should divide

01:29:06.320 --> 01:29:13.040
for each component of your input, okay? It's a fixed preprocessing. For a given training set,

01:29:13.040 --> 01:29:21.040
you'll have a fixed mean and standard deviation vector.

01:29:21.040 --> 01:29:27.760
But maybe we can connect to the other tool, right? The other module, the batch normalization, right?

01:29:27.760 --> 01:29:32.080
Okay, we haven't talked about that yet. Yeah, I'm saying that we can perhaps extend

01:29:32.720 --> 01:29:38.080
this normalization bit to the both sides, like the whole dataset and the batch itself.

01:29:38.080 --> 01:29:43.280
Okay, yes, yes. So, I mean, again, there's going to be a whole lecture on this. But

01:29:45.840 --> 01:29:52.080
for the same reason, it's good to have variables, the input that are zero mean and you need variants.

01:29:52.080 --> 01:29:56.080
It's also good for the state variables inside the network to basically have zero mean and

01:29:56.080 --> 01:30:00.480
you need variants. And so people have come up with various ways of doing normalization

01:30:01.360 --> 01:30:07.440
of the variables inside the network so that they approach zero mean and you need variants.

01:30:08.960 --> 01:30:14.560
But, and there are many ways to do this. They have two names like batch normalization, like

01:30:17.280 --> 01:30:20.560
layer normalization. And the idea goes back a very long time.

01:30:21.600 --> 01:30:28.080
Batch norm is kind of a more recent incarnation of it. Let's see, what was I scheduled to decrease

01:30:28.080 --> 01:30:34.480
the learning rate? Yeah, as it turns out, for reasons that are still not completely fully understood,

01:30:35.360 --> 01:30:41.600
you need to learn fast initially, you need a learning rate of a particular size.

01:30:42.720 --> 01:30:46.000
But to get good results in the end, you kind of need to decrease the learning rate to kind of

01:30:46.000 --> 01:30:53.040
let the system settle inside of minima. And that requires decreasing the learning rate.

01:30:54.000 --> 01:31:00.480
There's various semi-valid theoretical explanations for this, but experimentally,

01:31:00.480 --> 01:31:04.240
it's clear you need to do that. And again, there are schedules that are pre-programmed in PyTorch for

01:31:04.240 --> 01:31:12.800
this. Use a bit of L1 or L2 regularization on the weights or combination. Yeah, after you've

01:31:12.800 --> 01:31:18.160
trained your system for a few epochs, you might want to kind of prune it, eliminate the weights

01:31:18.160 --> 01:31:23.840
that are useless, make sure that the weights have their minimum size. And what you do is you add a

01:31:23.840 --> 01:31:30.640
term in the cost function that basically shrinks the weights at every iteration. You might know

01:31:30.640 --> 01:31:34.720
what L2 and L1 regularization means if you've taken a class in machine learning for large

01:31:34.720 --> 01:31:40.240
secret regression or stuff like that. It's very common. But L2 sometimes is called weight decay.

01:31:40.960 --> 01:31:49.200
This, again, are pre-programmed in PyTorch. A trick that a lot of people use for large neural

01:31:49.200 --> 01:31:56.560
nets is a trick called dropout. Dropout is implemented as kind of a layer in PyTorch. And

01:31:56.560 --> 01:32:04.800
what this layer does is that it takes the state of a layer and it randomly picks a certain proportion

01:32:04.800 --> 01:32:10.880
of the units and basically sets them to zero. So you can think of it as a mask,

01:32:12.160 --> 01:32:17.120
a layer that applies a mask to an input. And the mask is randomly picked at every sample.

01:32:18.480 --> 01:32:25.040
And some proportion of the value in the mask are set to zero. Some are set to one. And you

01:32:25.040 --> 01:32:30.560
multiply the input by the mask. So only a subset of the units are allowed to speak to the next

01:32:30.560 --> 01:32:36.080
layer, essentially. That's called dropout. And the reason for doing this is that it forces the

01:32:36.080 --> 01:32:43.520
unit to distribute the information about the input over multiple units instead of kind of

01:32:43.520 --> 01:32:49.680
squeezing everything into a small number. And it makes the system more robust. There's some

01:32:49.680 --> 01:32:55.680
theoretical arguments for why it does that. Experimentally, if you add this to a large

01:32:55.680 --> 01:33:02.160
network, you get better journalization error. You get better performance on the test set.

01:33:02.160 --> 01:33:08.960
It's not always necessary, but it helps. Okay, there's lots of tricks and I'll devote a lecture

01:33:08.960 --> 01:33:14.240
on this. So I'm not going to go through all of them right now. That requires explaining a bit

01:33:14.240 --> 01:33:19.040
more about optimizations. So really, what deep learning is about, like, I told you everything

01:33:19.040 --> 01:33:23.040
about deep learning, like the basics of deep learning. What I haven't told you is why we use

01:33:23.040 --> 01:33:28.720
deep learning. Okay, and that's basically what I'm going to tell you about now. The motivation

01:33:28.720 --> 01:33:33.360
for why is it that we need basically multi-layer neural nets or things of this type.

01:33:35.760 --> 01:33:42.960
Okay, so the traditional prototypical model of supervised learning for a very long time

01:33:42.960 --> 01:33:50.160
is basically a linear classifier. A linear classifier for a two-class problem is basically a

01:33:50.160 --> 01:33:54.800
single unit of the similar type that we talked about earlier. You compute a weighted sum of inputs

01:33:55.360 --> 01:34:02.000
at a bias, and you could think of the bias as just another trainable weight whose corresponding

01:34:02.000 --> 01:34:07.440
input is equal to one, if you want. And then you pass that through a threshold function,

01:34:07.440 --> 01:34:12.320
the sine function, that I put minus one if the weighted sum is below zero and plus one if it's

01:34:12.320 --> 01:34:19.920
above zero. Okay, so this basic linear classifier basically partitions the space, the input space

01:34:19.920 --> 01:34:27.120
of x's into two half spaces separated by hyperplane. Right, so the equation sum of i, w, i, x, i plus

01:34:27.120 --> 01:34:33.120
b equals zero is the surface that separates the category one that is going to produce y bar equal

01:34:33.120 --> 01:34:40.000
plus one from category two where y bar equals minus one. Why is it a, why does it divide the

01:34:40.000 --> 01:34:44.960
space into two halves? It's because you're computing the dot product of an input vector with a weight

01:34:44.960 --> 01:34:52.400
vector. If those two vectors are orthogonal, then the dot product is zero. Okay, b is just an offset.

01:34:53.440 --> 01:34:59.520
So the set of points in x space where this dot product is zero is the set of points that are

01:34:59.520 --> 01:35:07.760
orthogonal to the vector w. Okay, so in a n-dimensional space, your vector w is a vector,

01:35:08.480 --> 01:35:15.040
and the set of x whose dot product with w is zero is a hyperplane. Right, so it's a linear

01:35:15.040 --> 01:35:21.760
subspace of dimension n minus one. Okay, and that hyperplane divides the space of dimension n into

01:35:21.760 --> 01:35:28.320
halves. So here is the situation in two dimensions. You have two dimensions x1, x2. You have data

01:35:28.320 --> 01:35:34.240
points here, the red, the red category and the blue category. And there is a weight vector plus a bias

01:35:34.960 --> 01:35:40.400
where the, you know, the intercept here of this sort of green separating line

01:35:41.120 --> 01:35:46.800
with x1 is minus b times divided by w1. So that gives you an idea for what w should be.

01:35:47.520 --> 01:35:54.720
And the w vector is orthogonal to that separating surface. Okay, so changing b will

01:35:54.720 --> 01:35:58.320
change the position and then changing w will change the orientation basically.

01:35:58.880 --> 01:36:06.800
Now, what about situations like this where the points are, the red and blue points are not

01:36:06.800 --> 01:36:14.320
separable by a hyperplane? That's called a non-linearly separable case. So there you can't use a linear

01:36:14.320 --> 01:36:21.600
classifier to separate those. What are we going to do? In fact, there is a theorem that goes back

01:36:21.600 --> 01:36:29.040
to 1966 by Tom Kovar, who died recently actually. It was a Stanford that says the probability that

01:36:29.040 --> 01:36:36.080
a particular separation of p points is linearly separable in n dimension is close to one when p

01:36:36.080 --> 01:36:42.400
is smaller than n, but it's close to zero when p is larger than n. In other words, if you, if you

01:36:42.400 --> 01:36:47.520
take an n-dimensional space, you throw p random points in that n-dimensional space, data points,

01:36:47.520 --> 01:36:56.320
okay? And you randomly label them blue and red. You ask the question, what is the probability that

01:36:56.320 --> 01:37:01.840
that particular dichotomy is linearly separable? I can separate the blue points from the red points

01:37:01.840 --> 01:37:07.600
with a hyperplane. And the answer is, if p is less than n, you have a good chance that they

01:37:07.600 --> 01:37:12.000
will be separable. If p is larger than n, you basically have no chance that they will. Okay,

01:37:12.000 --> 01:37:19.200
so if you have an image classification problem, let's say, and you have tons of examples,

01:37:20.720 --> 01:37:25.680
way bigger. So let's say you do n-nist. So n-nist is a dataset of handwritten digits.

01:37:25.680 --> 01:37:30.320
The images are 28 by 28 pixels. In fact, the intrinsic dimension is smaller because some

01:37:30.320 --> 01:37:36.960
pixels are always zero. And you have 60,000 samples. The probability that those 60,000

01:37:36.960 --> 01:37:42.080
samples of, let's say, zeros from everything else or ones from everything else is nearly separable

01:37:42.880 --> 01:37:51.440
is basically nil. So, which is why people invented the classical model of pattern

01:37:51.440 --> 01:37:58.160
recognition. We consist in taking an input, engineering a feature extractor to produce a

01:37:58.160 --> 01:38:03.280
representation in such a way that in that space now, your problem becomes, let's say, linearly

01:38:03.280 --> 01:38:07.280
separable if you use a linear classifier or some other separability if you use another type of

01:38:07.280 --> 01:38:15.280
classifier. Okay? Now, necessarily, this feature extraction must be nonlinear itself. If the only

01:38:15.280 --> 01:38:19.600
thing it does is some affine transformation of the input, it's not going to make a nonlinearly

01:38:19.600 --> 01:38:26.480
separable problem into a linear separable one, right? So, necessarily, this feature extractor

01:38:26.480 --> 01:38:31.440
has to be nonlinear. This is very important to remember. Okay? A linear preprocessing doesn't

01:38:31.440 --> 01:38:36.880
do anything for you, essentially. So, people spend decades in computer vision, for example,

01:38:36.880 --> 01:38:42.240
as feature recognition, devising good feature extractors for particular problems. You know,

01:38:42.240 --> 01:38:46.720
what features are good to do face recognition, for example, right? Can I do things like detect the

01:38:46.720 --> 01:38:50.880
eyes and then measure the ratio between the separation of the eyes with the separation from

01:38:50.880 --> 01:38:55.680
the mouth and then, you know, computes a few features like this and then feed that to a classifier

01:38:55.680 --> 01:39:02.000
and figure out who the person is. So, most papers, you know, between, let's say, the 1960s

01:39:02.000 --> 01:39:10.240
or 70s and the late 2000s or early 2010s in computer vision were essentially about that,

01:39:10.240 --> 01:39:16.560
like how you represent images properly. Not all of them, okay? A lot of them for recognition.

01:39:18.160 --> 01:39:24.640
And a lot of people kind of devise very sort of generic ways of devising feature extractors.

01:39:25.760 --> 01:39:31.520
The basic idea is you just expand the dimension of the representation in a nonlinear way so that now

01:39:31.520 --> 01:39:35.440
your number of dimensions is larger than the number of samples. And now your problem has

01:39:35.440 --> 01:39:40.000
a chance of becoming linearly separable. So, the ideas that I'm not going to go through,

01:39:40.000 --> 01:39:44.000
like space styling, random projection. So, random projection basically is a very simple idea.

01:39:44.000 --> 01:39:53.520
You take your input vectors, you multiply them by random matrix, okay? And then you pass the result

01:39:53.520 --> 01:39:58.880
through some nonlinear operation, okay? That's called random projection.

01:40:00.240 --> 01:40:05.200
And it might make, if the dimension of the output is larger than the dimension of the input,

01:40:05.200 --> 01:40:10.480
it might make a nonlinearly separable problem linearly separable. It's very efficient because,

01:40:11.440 --> 01:40:16.080
you know, you might need a very large number of those, of this dimension to be able to kind of

01:40:16.080 --> 01:40:20.800
do a good job. But it works in certain cases and you don't have to train the first layer,

01:40:20.800 --> 01:40:24.880
you basically pick it randomly. And so, the only thing you need to train is a linear classifier

01:40:24.880 --> 01:40:29.520
on top. It's polynomial classifiers, which I'll talk about in a minute, in a minute,

01:40:29.520 --> 01:40:36.000
radio basis functions and kernel machines. So, those are basically techniques to turn

01:40:36.960 --> 01:40:45.040
an input into a representation that then will be essentially classifiable by a simple classifier

01:40:45.040 --> 01:40:51.680
like a linear classifier. So, what's a polynomial classifier? A polynomial classifier, basically,

01:40:51.680 --> 01:40:56.320
imagine that your input vector has two dimensions. The way you increase the dimensionality of the

01:40:56.320 --> 01:41:02.080
representation is that you take each of the input variables, but you also take every product of

01:41:02.080 --> 01:41:07.360
pairs of input variables, right? So, now you have a new feature vector, which is composed of x1,

01:41:07.360 --> 01:41:13.120
x2, you add one for the bias, and then also x1 times x2, x1 squared and x2 squared.

01:41:13.120 --> 01:41:19.840
So, when you do a linear classification in that space, what you're doing really is a quadratic

01:41:21.120 --> 01:41:25.760
classification in the original space, right? The surface, the separating surface in the

01:41:25.760 --> 01:41:34.080
original space now is a quadratic curve into dimension. In n dimension, it's a quadratic

01:41:34.080 --> 01:41:42.240
hypersurface, basically. So, it could be a parabola or ellipse or hyperbola, depending on the

01:41:42.240 --> 01:41:46.320
coefficients, right? Now, the problem with this is that it doesn't work very well in high

01:41:46.320 --> 01:41:51.040
dimension because the number of features grows with a square of the number of inputs. So, if you

01:41:51.040 --> 01:41:57.520
want to apply this to get an ImageNet type image, you know, the resolution is 256 by 256 by 3,

01:41:57.520 --> 01:42:02.720
because you have color channels. That's already a high dimension. If you take the cross product of

01:42:02.720 --> 01:42:07.200
all of those variables, that's way too large, okay?

01:42:08.000 --> 01:42:15.040
So, it's not really practical for high dimensional problems, but it's a trick. Now, here is,

01:42:15.920 --> 01:42:21.760
so, super vector machines are basically two-layer networks of kernel machines more generally,

01:42:22.400 --> 01:42:29.040
are two-layer systems in which the first layer has as many dimensions as you have training samples.

01:42:30.000 --> 01:42:34.640
Okay? So, for each training sample, you create a inner-on, a unit, if you want,

01:42:35.520 --> 01:42:40.960
and the role of this unit is to produce a large output if the input vector matches one of the

01:42:40.960 --> 01:42:47.120
training samples and a small output if it doesn't, or the other way around. A small output if it

01:42:47.120 --> 01:42:51.120
matches, a large output if it doesn't, okay? It doesn't really matter, but it has to be nonlinear.

01:42:51.760 --> 01:42:55.600
So, something like, you know, compute the dot product of the input by one of the training

01:42:55.600 --> 01:43:01.600
samples and passes through, you know, a negative exponential or a square or something like that.

01:43:01.920 --> 01:43:08.000
So, this gives you how much the input vector resembles one of the training samples, and you

01:43:08.000 --> 01:43:14.000
do this for every single training samples, okay? And then you train a linear classifier basically

01:43:14.000 --> 01:43:19.200
to use those inputs as, you know, as input to a linear classifier. You compute the weight so

01:43:19.200 --> 01:43:24.320
that linear classifier is basically as simple as that. There's some regularization involved, okay?

01:43:24.880 --> 01:43:30.320
So, essentially, it's kind of a lookup table, right? You have your entire training set as

01:43:31.040 --> 01:43:36.640
you know, points in your kind of nuance if you are, if you want for units in your first layer,

01:43:36.640 --> 01:43:42.080
and they each indicate how close the current input vector is to them. So, you get some picture

01:43:42.080 --> 01:43:46.640
of where the input vector is by basically having the relative position to all of the

01:43:46.640 --> 01:43:50.640
training samples, and then using a simple linear operation, you can figure out, like, what's the

01:43:50.640 --> 01:43:55.680
correct answer. This works really well for low dimensional problems, the small number of training

01:43:56.400 --> 01:44:02.880
samples, but you're not going to do computer vision with it, at least not without, not if X is our pixels,

01:44:04.480 --> 01:44:05.920
because it's basically template matching.

01:44:08.640 --> 01:44:15.040
Now, here is a very interesting fact. It's a fact that if you build a two-layer neural net

01:44:15.040 --> 01:44:19.920
on this model, okay? So, let's say a two-layer neural net, you have an input layer, a hidden layer,

01:44:20.880 --> 01:44:27.440
and not specifying the size, and a single output unit, and you ask, what functions can I approximate

01:44:27.440 --> 01:44:33.040
with an architecture of this type? The answer is, you can approximate pretty much any well-behaved

01:44:33.040 --> 01:44:38.240
function as close as you want, as long as you have enough of those units in the middle, okay?

01:44:38.240 --> 01:44:44.240
So, this is a theorem that says that two-layer neural nets are universal approximators. It

01:44:44.240 --> 01:44:49.120
doesn't really matter what nonlinear function you put in the middle. Any nonlinear function will

01:44:49.120 --> 01:44:57.040
do. A two-layer neural net is a universal approximator, and immediately you say, well,

01:44:57.040 --> 01:45:02.160
why do we need multiple layers, then, if we can approximate anything with two layers? And the

01:45:02.160 --> 01:45:07.200
answer is, it's very, very inefficient to try to approximate everything with only two layers,

01:45:07.200 --> 01:45:12.400
because many, many, many interesting functions we're interested in learning cannot be efficiently

01:45:12.400 --> 01:45:17.120
represented by a two-layer system. They can possibly be represented by a two-layer system,

01:45:17.120 --> 01:45:21.360
but the number of fielding units it would require would be so ridiculously large that it's completely

01:45:21.360 --> 01:45:31.120
impractical, okay? So, that's why we need layers. This very simple point is something that took

01:45:31.120 --> 01:45:37.680
about, you know, it took until the, basically, the 2010s for the machine learning and

01:45:37.680 --> 01:45:44.560
computer vision communities to understand, okay? If you understood what I just said,

01:45:44.560 --> 01:45:50.080
you just took a few seconds, so you beat them. There is a last question here before we

01:45:50.080 --> 01:45:55.760
finish class. So, does the depth of the network then have anything to do with generalization?

01:45:57.520 --> 01:46:03.840
Okay, so generalization is a different story, okay? Generalization is very difficult to predict.

01:46:03.840 --> 01:46:08.800
It depends on a lot of things. It depends on the appropriateness of the architecture to the

01:46:08.800 --> 01:46:13.680
problem at hand, okay? So, for example, people use convolutional nets for computer vision,

01:46:13.680 --> 01:46:18.400
they use transformers for text, you know, blah, blah, blah. So, there are certain architectures

01:46:18.400 --> 01:46:26.240
that work well for certain types of data. So, that's the main thing that will improve generalization.

01:46:29.600 --> 01:46:33.920
But generally, yes, multiple layers can improve generalization because

01:46:34.880 --> 01:46:40.240
for a particular function you're interested in learning, computing it with multiple layers

01:46:40.240 --> 01:46:44.000
will allow you to reduce the overall size of the system that will do a good job.

01:46:44.000 --> 01:46:48.240
And so, by reducing the size, you're basically making it easier for the system to find kind of

01:46:48.240 --> 01:46:52.160
good representation. But there is something else which has to do with compositionality.

01:46:52.160 --> 01:46:57.280
I'll come to this in a minute if I have time. Also, the minimum, the, like the,

01:46:57.280 --> 01:47:02.320
how do you call it, the well is like larger, right? If we have overparameterized networks.

01:47:02.320 --> 01:47:06.400
If you're overparameterized network, it's much easier to find a minimum to your objective function,

01:47:06.400 --> 01:47:12.480
right? Which is why neural nets are generally overparameterized. They generally have, like you,

01:47:12.480 --> 01:47:15.200
a much larger number of parameters than what you would think is necessary.

01:47:15.840 --> 01:47:18.960
And when you get them bigger, when you make them bigger, they work better usually.

01:47:18.960 --> 01:47:25.120
It's not always the case, but it's very curious phenomenon about this. We'll talk about this later.

01:47:25.120 --> 01:47:31.600
Okay, this is the one point I want to make. And it's the fact that the reason why we,

01:47:31.600 --> 01:47:37.920
why layers are good is that the world is compositional, the perceptual world in particular,

01:47:37.920 --> 01:47:41.600
but the world in general, the universe, if you want, is compositional. What does that mean? It

01:47:41.600 --> 01:47:47.680
means that, okay, at the level of the universe, right? We have elementary particles, they assemble

01:47:47.680 --> 01:47:52.720
to form less elementary particles, those assemble to form atoms, those assemble to form molecules,

01:47:52.720 --> 01:48:00.080
those assemble to form materials, those assemble to form, you know, structures, objects, etc.

01:48:00.080 --> 01:48:06.240
And, you know, environments, scenes, etc. You have the same kind of hierarchy for images,

01:48:06.240 --> 01:48:11.920
you have pixels, they assemble to form edges and textons and motifs, parts and objects.

01:48:12.640 --> 01:48:16.400
In text, you have characters that assemble to form words, word groups, clauses, sentences,

01:48:16.400 --> 01:48:23.520
stories. In speech, you have speech samples, assemble to form, you know, kind of elementary

01:48:23.520 --> 01:48:30.800
sounds, phones, phonemes, syllables, words, etc. So you have this kind of compositional hierarchy

01:48:30.800 --> 01:48:35.360
in a lot of natural signals. And this is what makes the world understandable, right? This is

01:48:35.360 --> 01:48:39.840
famous quote by Albert Einstein, the most incomprehensible thing about the world is that the

01:48:39.840 --> 01:48:44.080
world is comprehensible. And the reason why the world is comprehensible is because it's compositional,

01:48:44.080 --> 01:48:48.480
because small part assemble to form bigger part, and that allows you to have a description, an

01:48:48.480 --> 01:48:55.760
abstract description of the world in terms of parts from the level immediately below,

01:48:55.760 --> 01:49:00.480
in terms of level of abstraction. So to some extent, the layered architecture in a neural net

01:49:01.600 --> 01:49:08.160
reflects this idea that you have kind of a compositional hierarchy where simple things

01:49:08.160 --> 01:49:13.120
assemble to form slightly more complex things. So images, you have pixels formed to form edges

01:49:13.120 --> 01:49:17.440
that are kind of depicted here. These are actually feature detectors, the visualization of feature

01:49:17.440 --> 01:49:22.480
detectors by a particular convolutional net, which is a particular type of neural net, multilateral

01:49:22.480 --> 01:49:28.160
neural net. So at the low level, you have units that detect oriented edges, a couple layers up,

01:49:28.160 --> 01:49:34.000
you have things that detect simple motifs, circles, gratings, corners, etc. And then a few layers up,

01:49:34.000 --> 01:49:40.560
there are things like parts of objects and things like that. So I think personally that the magic

01:49:41.120 --> 01:49:48.240
of deep learning, the fact that multiple layers help is the fact that the perceptual world is

01:49:48.240 --> 01:49:53.600
basically a compositional hierarchy. And then this end-to-end learning in deep learning allows the

01:49:53.600 --> 01:50:00.720
system to learn hierarchical representations where each layer learns a representation that has a

01:50:00.720 --> 01:50:05.200
level of abstraction slightly higher than the previous one. So low level, you have individual

01:50:05.200 --> 01:50:10.080
pixels, then you have the presence or absence of an edge, then you have the presence or absence of

01:50:10.080 --> 01:50:14.560
a part of an object, and then you have the presence or absence of an object independently of

01:50:15.840 --> 01:50:19.920
the position of that object, the illumination, the color, the occlusions, the background,

01:50:19.920 --> 01:50:26.640
you know, things like that, right? So that's the motivation, the idea why deep learning is so

01:50:26.640 --> 01:50:32.880
successful and why it's basically taken over the world over the last 10 years or so. All right,

01:50:32.880 --> 01:50:37.200
thank you for your attention. That's great. So for tomorrow guys, don't forget to try to go over

01:50:37.200 --> 01:50:47.280
the 01 tutorial tensor, sorry, the 01 notebook that we have on the website such that we can get,

01:50:47.280 --> 01:50:51.840
like, you know, all on the same level for the ones that are not really familiar with NumPy stuff,

01:50:51.840 --> 01:51:04.480
okay? So otherwise, let's see you tomorrow morning and have a nice day. Take care everyone. Bye-bye.

