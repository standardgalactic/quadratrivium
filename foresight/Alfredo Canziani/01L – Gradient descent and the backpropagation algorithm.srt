1
00:00:00,000 --> 00:00:03,120
So as you know, we're going to talk about deep learning and we're going to jump right in.

2
00:00:04,480 --> 00:00:11,920
So much of practical applications of deep learning today, machine learning and AI in general,

3
00:00:13,120 --> 00:00:19,520
are used a paradigm called supervised learning, which I'm sure most of you have heard of before.

4
00:00:19,520 --> 00:00:24,720
So this is the paradigm by which you train a machine by showing it examples of inputs and outputs.

5
00:00:25,600 --> 00:00:29,680
You want to build a machine to distinguish images of cars from airplanes, you show it an image of

6
00:00:29,680 --> 00:00:34,560
a car. If the machine says car, you don't do anything. If it says something else, you adjust

7
00:00:34,560 --> 00:00:38,400
the internal parameters of the system so that the output gets closer to the one you want.

8
00:00:39,280 --> 00:00:45,840
So imagine the target output is some vector of activities on a set of outputs. You want the

9
00:00:45,840 --> 00:00:49,840
vector coming out of the machine to get closer to the vector that is the desired output.

10
00:00:51,440 --> 00:00:56,560
And this works really well. As long as you have lots of data, it works for speech recognition,

11
00:00:56,560 --> 00:01:02,240
image recognition, face recognition, generating captions, translation, all kinds of stuff.

12
00:01:03,280 --> 00:01:07,280
So this is, I would say, 95% of all applications of machine learning today.

13
00:01:08,160 --> 00:01:12,000
There are two other paradigms, one of which I will not talk about, one of which I will talk

14
00:01:12,000 --> 00:01:17,200
about a lot. So the two other paradigms are reinforcement learning, which I will not talk

15
00:01:17,200 --> 00:01:23,920
about. And there are other courses. There's a course by Larry Pinto about this that I encourage

16
00:01:24,000 --> 00:01:29,760
you to take. And a third paradigm is self-supervised learning or unsupervised learning. And we'll

17
00:01:29,760 --> 00:01:36,320
talk about this quite a lot in the following weeks. But for now, let's talk about supervised

18
00:01:36,320 --> 00:01:39,920
learning. Self-supervised learning, you could think of it as kind of a play on supervised learning.

19
00:01:41,840 --> 00:01:46,000
So the traditional model of pattern recognition machine learning and supervised learning,

20
00:01:46,000 --> 00:01:51,920
certainly going back to the late 50s or the 60s, is the idea by which you take a raw signal,

21
00:01:51,920 --> 00:01:56,640
let's say an image or an audio signal or a set of features representing an object,

22
00:01:56,640 --> 00:02:03,120
and then you turn it into a representation using a feature extractor, which in the past

23
00:02:03,120 --> 00:02:09,520
was engineered. And then you take that representation, which is generally in the form of a vector or

24
00:02:09,520 --> 00:02:13,520
a table of numbers or some kind of tensor, a multi-dimensional array. But sometimes,

25
00:02:14,240 --> 00:02:18,480
could be a different type of representation. And you feed that to a trainable classifier.

26
00:02:19,360 --> 00:02:23,360
So this is the learning where the learning takes part. This is the classical model,

27
00:02:24,080 --> 00:02:28,800
and it's still popular. It's still used a lot. But basically, what deep learning has done is

28
00:02:28,800 --> 00:02:35,920
replace this sort of manual hand engineering of the feature extractor by a stack of trainable

29
00:02:36,560 --> 00:02:40,640
modules, if you want. So in deep learning, the main idea of deep learning, and the only reason

30
00:02:40,640 --> 00:02:46,960
why it's called deep, is that we stack a bunch of modules, each of which transforms the input a

31
00:02:46,960 --> 00:02:53,520
little bit into something that's going to slightly higher level of abstraction, if you want.

32
00:02:54,240 --> 00:03:04,800
And then we train the entire system end to end. So I represented those sort of pinkish modules

33
00:03:04,800 --> 00:03:10,160
to indicate the ones that are trainable, and the blue modules are the fixed ones, the hand

34
00:03:10,160 --> 00:03:15,840
engineered ones. So that's why deep learning is called deep. We stack multiple layers of

35
00:03:15,840 --> 00:03:21,600
trainable things, and we train it end to end. The idea for this goes back a long time. The

36
00:03:21,600 --> 00:03:27,760
practical methods for this go back to the mid to late 80s, with the back propagation algorithm,

37
00:03:27,760 --> 00:03:34,480
which is going to be the main subject of today's lecture, actually. But it took a long time for

38
00:03:34,480 --> 00:03:41,680
this idea to actually percolate and become the main tool that people use to build machine learning

39
00:03:41,680 --> 00:03:48,000
system. It's only about 10 years old. Okay, so let's go through a few definitions. So we're going

40
00:03:48,000 --> 00:03:52,560
to deal with parameterized models, a parameterized model, or learning model, if you want, is a

41
00:03:52,560 --> 00:03:58,720
parameterized function, g of x and w, where x is the input, and w is a set of parameters.

42
00:04:00,240 --> 00:04:05,600
I'm representing this here on the right with a particular symbolism, where a function

43
00:04:06,560 --> 00:04:12,800
like this that produces a single output, think of the output as either a vector or matrix or a

44
00:04:12,800 --> 00:04:18,240
tensor, or perhaps even a scalar, but generally is multidimensional. It can actually be something

45
00:04:18,240 --> 00:04:24,240
else in a multidimensional array, but something that, you know, maybe like a sparse array representation

46
00:04:24,240 --> 00:04:29,280
or a graph with values on it. But for now, let's think of it just as a multidimensional array.

47
00:04:30,240 --> 00:04:35,600
So both the inputs and the outputs are multidimensional arrays, what people call tensors.

48
00:04:36,560 --> 00:04:40,480
It's not really kind of the appropriate definition of tensor, but it's okay.

49
00:04:42,080 --> 00:04:46,080
And that function is parameterized by a set of parameters w. Those are the knobs that we're

50
00:04:46,080 --> 00:04:52,080
going to adjust during training, and they basically determine the input-output relationship between

51
00:04:53,040 --> 00:05:01,520
you know, between the input x and the predicted output y bar. Okay, so I'm not explicitly

52
00:05:01,520 --> 00:05:08,000
representing the wire that comes in with w. Here, I kind of assume that w is somewhere inside of

53
00:05:09,200 --> 00:05:15,360
this module. Think of this as an object in object-oriented programming. So it's an instance

54
00:05:15,360 --> 00:05:21,360
of a class that you instantiated and it's got a slot in it that represents the parameters,

55
00:05:21,360 --> 00:05:28,160
and there is a forward function basically that takes as argument the input and returns the output.

56
00:05:28,160 --> 00:05:36,240
Okay, so a basic running machine will have a cost function and the cost function in supervised

57
00:05:36,240 --> 00:05:44,560
running, but also in some other settings will basically compute the discrepancy, distance,

58
00:05:45,120 --> 00:05:50,320
divergence, whatever you want to call it, between the desired output y, which is given to you from

59
00:05:50,320 --> 00:05:56,000
the training set, and the output produced by the system y bar. Okay, so an example of this,

60
00:05:56,000 --> 00:06:01,920
a very simple example of a setting like this is linear regression. In linear regression, x is a

61
00:06:01,920 --> 00:06:08,320
vector composed of components x i's, w is also a vector, and the output is a scalar that is simply

62
00:06:08,320 --> 00:06:16,160
the dot product of x with w. So y bar now is a scalar, and what you compute is the square

63
00:06:16,160 --> 00:06:22,480
distance, the square difference really between y and y bar. If w is a matrix, then now y is a vector,

64
00:06:22,480 --> 00:06:28,800
and you compute the square norm of the difference between y and y bar, and that's basically linear

65
00:06:28,800 --> 00:06:35,360
regression. So learning will consist in finding the set of w's that minimize this particular cost

66
00:06:35,360 --> 00:06:40,080
function average over a training set. I'll come to this in a minute, but I want you to think right

67
00:06:40,080 --> 00:06:45,920
now about the fact that this g function may not be something particularly simple to compute. So

68
00:06:46,240 --> 00:06:52,320
it may not be just multiplying a vector by a matrix. It may not be just carrying some

69
00:06:53,600 --> 00:06:58,800
sort of fixed computation with sort of a fixed number of steps. It could involve something

70
00:06:58,800 --> 00:07:03,840
complicated. It could involve minimizing a function with respect to some other variable that you

71
00:07:03,840 --> 00:07:10,880
don't know. It could involve a lot of iteration of some algorithm that converges towards a fixed

72
00:07:10,880 --> 00:07:19,920
point. So let's not restrict ourselves to g of x w that are simple things. It could be very

73
00:07:19,920 --> 00:07:26,160
complicated things, and we'll come to this in a few weeks. So this is just to explain the

74
00:07:26,160 --> 00:07:34,480
notations that I will use during the course of this class. So we have observed input and desired

75
00:07:34,480 --> 00:07:41,920
output variables. Those are kind of gray grayish bubbles. Other variables that are produced by

76
00:07:41,920 --> 00:07:48,800
the system or internal to the system are those kind of, you know, empty circle variables.

77
00:07:49,920 --> 00:07:53,840
We have determinacy functions or functions that are so they are indicated by this sort of

78
00:07:54,640 --> 00:07:58,480
rounded shape here. They can take multiple inputs have multiple outputs.

79
00:07:59,920 --> 00:08:04,400
And each of those can be tensors or scalars or whatever. And they have implicit parameters

80
00:08:04,480 --> 00:08:10,160
that are tunable by training. And then we have cost functions. So cost functions are basically

81
00:08:11,360 --> 00:08:17,840
functions that take one or multiple inputs and output a scalar. But I'm not representing the

82
00:08:17,840 --> 00:08:24,880
output is implicit. Okay, so if you have a red square, it has an implicit output. And it's a

83
00:08:24,880 --> 00:08:31,680
scalar and we interpret it as a cost or an energy function. So this symbolism is kind of similar

84
00:08:31,680 --> 00:08:37,760
to what people use in graphical models. If you if you've heard what a graphical model is,

85
00:08:37,760 --> 00:08:41,440
particularly the type of graphical model called a factor graph. So in a factor graph, you have

86
00:08:41,440 --> 00:08:46,640
those variable bubbles, and you have those factors, which are those square cost functions.

87
00:08:47,520 --> 00:08:50,720
You don't have this idea that you had deterministic functions in it, because

88
00:08:50,720 --> 00:08:54,240
graphical models don't care about the fact that you have functions in one direction or another.

89
00:08:54,240 --> 00:09:00,960
But here we care about it. So with this extra symbol. Okay, so machine learning consists in

90
00:09:00,960 --> 00:09:07,520
basically minimizing finding the set of parameters W that minimize the cost function averaged over

91
00:09:07,520 --> 00:09:16,640
a training set. So a training set is a set of pairs x, x, y indexed by an index P. Okay, so we have

92
00:09:16,640 --> 00:09:22,960
P training samples. And it'll be the index of the training set the training sample. And our overall

93
00:09:23,680 --> 00:09:26,160
last function that we're going to have to minimize

94
00:09:28,480 --> 00:09:34,480
is the, you know, is equal to the cost of the discrepancy between the y and the output of our

95
00:09:34,480 --> 00:09:43,120
model by bar g of x, w, as I said earlier. So L is a value, C is a C is a module and L is a

96
00:09:43,120 --> 00:09:49,280
is a is a way of writing C of y, g of x, w, you know, whether it depends explicitly on x, y and

97
00:09:49,280 --> 00:09:55,440
w. Okay, but it's the same thing really. The overall last function, which is this

98
00:09:57,040 --> 00:10:03,200
kind of curly L is the average of the per sample loss function over the entire training set. Okay,

99
00:10:03,840 --> 00:10:10,160
so compute L for the entire training set, divide by some all the terms divide by P, and that's the

100
00:10:10,160 --> 00:10:16,800
average. That's the loss. Okay, so now the name of the game is trying to find the minimum of that

101
00:10:16,800 --> 00:10:23,600
loss with respect to the parameters. This is an optimization problem. So symbolically, I can

102
00:10:23,600 --> 00:10:29,040
represent this entire graph as the thing on the right. This is rarely used in practice, but this

103
00:10:29,040 --> 00:10:36,000
is sort of a way to visualize this. So think about each training sample as a sort of identical copy

104
00:10:36,000 --> 00:10:42,960
of the replica, if you want, of the model and the cost function applied to a different training

105
00:10:42,960 --> 00:10:47,200
sample, and then there is an average operation that computes the loss, right? So everything you

106
00:10:48,000 --> 00:10:54,480
can write as a formula, you can probably write in terms of those graphs. This is going to be very

107
00:10:54,480 --> 00:11:01,440
useful as we're going to see later. Okay, so supervised machine learning and a lot of other

108
00:11:01,440 --> 00:11:09,440
machine learning patterns as well actually are can be viewed as function optimization and a very

109
00:11:09,440 --> 00:11:17,120
simple approach to optimizing a function, which means finding the set of parameters to a function

110
00:11:17,120 --> 00:11:23,680
that minimize its value, okay, is gradient descent or gradient based algorithms. So gradient based

111
00:11:23,680 --> 00:11:29,520
algorithm makes the assumption that the function is somewhat smooth and mostly differentiable,

112
00:11:29,520 --> 00:11:34,400
doesn't have to be everywhere differentiable, but has to be continuous, has to be almost everywhere

113
00:11:34,400 --> 00:11:40,960
differentiable. And it has to be somewhat smooth, otherwise, the local information of the slope

114
00:11:40,960 --> 00:11:46,240
doesn't tell you much about where the minimum is. Okay, so here's an example here depicted on the right.

115
00:11:48,160 --> 00:11:53,120
The lines that you see here, the pink lines are the lines of equal cost and this cost is quadratic,

116
00:11:53,760 --> 00:12:00,480
so it's basically a kind of paraboloid. And this is the trajectory of a method called stochastic

117
00:12:00,480 --> 00:12:04,880
gradient descent, which we'll talk about in a minute. So for stochastic gradient descent, the

118
00:12:04,880 --> 00:12:08,960
procedure is you show an example, you run you through the machine, you compute the objective

119
00:12:08,960 --> 00:12:15,840
for that particular sample, and then you figure out by how much and how to modify each of the knobs

120
00:12:15,840 --> 00:12:20,880
in the machine, the W parameters, so that the objective function goes down by a little bit,

121
00:12:20,880 --> 00:12:24,560
you make that change, and then you go to the next sample. Let's be a little more formal.

122
00:12:25,200 --> 00:12:32,320
So gradient descent is this very basic algorithm here, you replace the value of W by its previous

123
00:12:32,320 --> 00:12:39,520
value minus a step size, eta here, multiplied by the gradient of the objective function with respect

124
00:12:39,520 --> 00:12:47,760
to the parameters. So what is a gradient? A gradient is a vector of the same size as the

125
00:12:47,760 --> 00:12:53,280
parameter vector. And for each component of the parameter vector, it tells you by how much

126
00:12:54,080 --> 00:13:00,240
the the loss function L would increase if you increase the parameter by a tiny amount.

127
00:13:00,240 --> 00:13:05,280
Okay, it's a derivative, but it's a directional derivative, right? So let's say among all the

128
00:13:05,280 --> 00:13:13,200
directions, you only look at W34. And as you imagine that you tweak W34 by a tiny amount,

129
00:13:14,080 --> 00:13:19,920
the loss function curly L is going to increase by a tiny amount, you divide the tiny amount by

130
00:13:19,920 --> 00:13:27,120
which L increase by the tiny amount that you modified this W34. And what you get is the gradient

131
00:13:27,120 --> 00:13:33,840
of the loss with respect to W34. If you do this for every single weight, you get the gradient

132
00:13:33,840 --> 00:13:38,160
of the loss function with respect to all the weights. And it's a vector, which for each component

133
00:13:38,160 --> 00:13:45,680
of the weight gives you the parameter gives you that quantity. Okay, so, you know, since Newton

134
00:13:45,680 --> 00:13:51,280
and earlier, it's been written as, you know, dL over dW, because it indicates the fact that

135
00:13:51,280 --> 00:13:56,000
there is this little twiddle where you can twiddle W by little. And there's a resulting

136
00:13:56,880 --> 00:14:00,960
twiddling of L. And if you divide those two twiddles, and they are infinitely small,

137
00:14:00,960 --> 00:14:05,840
you get the derivative that's kind of standard notation in mathematics for a few hundred years.

138
00:14:05,840 --> 00:14:15,360
Okay, so now the gradient is going to be a vector. Okay. And as indicated here on the top, right,

139
00:14:15,440 --> 00:14:26,560
that vector is an arrow that points upwards along the line of larger slope. Okay, so if you are

140
00:14:26,560 --> 00:14:32,240
in a 2D surface, you have two W parameters. Okay, and the surface is represented here,

141
00:14:32,880 --> 00:14:38,880
some sort of quadratic ball here in this case. So it's a second degree polynomial in W1 and W0.

142
00:14:39,840 --> 00:14:46,320
Here on the right is the kind of a top-down view of this where the lines represent the lines of

143
00:14:46,320 --> 00:14:52,320
equal cost. The little arrow is here, represent the gradient at various locations. Okay,

144
00:14:52,960 --> 00:14:58,080
so you have a long arrow if the slope is steep, a short arrow if the slope is

145
00:14:59,760 --> 00:15:06,720
not steep, not large. At the bottom, it's zero. And it points towards the direction of

146
00:15:07,280 --> 00:15:12,480
higher slope. All right, so imagine you are in a landscape, a mountainous landscape,

147
00:15:13,280 --> 00:15:18,640
and you're in a fog and you want to go down the valley. You look around you and you can tell

148
00:15:19,920 --> 00:15:27,120
the local slope of the landscape. You can't tell where the minimum is because you're in a fog,

149
00:15:27,760 --> 00:15:34,400
but you can tell the local slope. So you can figure out what is the direction of larger slope

150
00:15:34,400 --> 00:15:39,600
and then take a step and it will take you upwards. Now you turn around 180 degrees,

151
00:15:39,600 --> 00:15:44,640
take a step in that direction, and that is going to take you downwards. If you keep doing this

152
00:15:44,640 --> 00:15:50,560
and the landscape is convex, which means it has only one local minimum, this will eventually

153
00:15:51,520 --> 00:15:57,520
take you down to the valley and presumably to the village. Right, so that's gradient-based

154
00:15:57,520 --> 00:16:07,840
algorithms. They all differ by how you compute the gradient first and by what this eta step-size

155
00:16:07,840 --> 00:16:15,120
parameter is. So in simple forms, eta is just a positive constant that sometimes is decreased as

156
00:16:15,120 --> 00:16:23,760
the system learns more, but most of the time not. But in more complex versions of gradient-based

157
00:16:23,760 --> 00:16:29,200
learning, eta is actually an entire matrix itself, generally a positive definite or semi-definite

158
00:16:29,200 --> 00:16:37,680
matrix. And so the direction adopted by those algorithms is not necessarily the steepest descent.

159
00:16:37,680 --> 00:16:43,360
It goes downwards, but it's not necessarily the steepest descent. And we can see why here. So

160
00:16:44,480 --> 00:16:49,440
in this diagram here that I'm showing, this is a trajectory that will be followed by gradient

161
00:16:49,440 --> 00:16:55,520
descent in this quadratic cost environment. And as you see, the trajectory is not straight.

162
00:16:56,160 --> 00:17:02,080
It's not straight because the system goes down by following the slope of steepest descent. And so

163
00:17:03,280 --> 00:17:06,640
it goes down the valley before finding the minimum of the valley, if you want.

164
00:17:07,280 --> 00:17:12,160
So if your cost function is a little squeezed in one direction, it will go down the ravine

165
00:17:12,160 --> 00:17:17,280
and then follow the ravine towards the bottom. In complex situations where you have

166
00:17:18,000 --> 00:17:24,400
things that are, the trajectory actually is being cut here. But when the

167
00:17:24,400 --> 00:17:30,720
weather function is highly irregular, this might even be more complicated. And then you might have

168
00:17:30,720 --> 00:17:35,440
to be smart about what you do here. Okay. So stochastic gradient descent is

169
00:17:36,480 --> 00:17:46,560
universally used in deep learning. And this is a slight modification of the gradient

170
00:17:47,440 --> 00:17:54,960
steepest descent algorithm where you don't compute the gradient of the entire objective function

171
00:17:54,960 --> 00:18:02,320
averaged over all the samples. But what you do is you take one sample and you compute the gradient

172
00:18:02,320 --> 00:18:07,600
of the objective function for that one sample with respect to the parameters and you take a step.

173
00:18:08,240 --> 00:18:14,160
Okay. And you keep doing this. You pick another sample, compute the gradient of the objective

174
00:18:14,160 --> 00:18:19,760
function for that sample with respect to the way it's making a date. Why is it called stochastic

175
00:18:19,760 --> 00:18:28,800
gradient? Stochastic is a fancy term for random, essentially. And it's called stochastic because

176
00:18:28,800 --> 00:18:35,120
the evaluation of the gradient you get on the basis of a single sample is a noisy estimate of

177
00:18:35,120 --> 00:18:40,240
the full gradient. The average of the gradients, because the gradient is a linear operation,

178
00:18:40,240 --> 00:18:44,800
the average of the gradients will be the gradient of the average. And so things work out. If you

179
00:18:44,800 --> 00:18:50,160
compute the gradient and you kind of keep going, overall, the average trajectory will be sort of

180
00:18:50,160 --> 00:18:56,720
the trajectory you would have followed by doing full gradient. Okay. But in fact,

181
00:18:57,600 --> 00:19:02,480
the reason we're doing this is because it's much more efficient in terms of speed of convergence. So

182
00:19:03,920 --> 00:19:07,520
although the trajectory followed by stochastic gradient is very noisy,

183
00:19:07,520 --> 00:19:11,760
things kind of bounce around a lot. As you can see in the trajectory here at the bottom,

184
00:19:13,280 --> 00:19:18,080
you know, things have, the trajectory is very erratic. But in fact, it goes to the bottom faster

185
00:19:18,960 --> 00:19:24,640
and has other advantages that people are still writing papers on. Okay. The reason for that is

186
00:19:24,640 --> 00:19:32,320
that stochastic gradient exploits the redundancy between the samples. So all the, you know, machine

187
00:19:32,320 --> 00:19:37,520
learning setting, the training samples have some similarities between them. If they don't, then

188
00:19:37,520 --> 00:19:42,560
basically the learning problem is impossible. So they necessarily do have some redundancy

189
00:19:42,560 --> 00:19:46,880
between them. And the faster you update the parameters, the more you, the more often you

190
00:19:46,880 --> 00:19:52,960
update them, the more you exploit this redundancy between those parameters. Now in practice, what

191
00:19:52,960 --> 00:19:59,200
people do is they use mini batches. So instead of computing the gradient on the basis of a single

192
00:19:59,200 --> 00:20:06,800
sample, you take a batch of samples, typically anywhere between let's say 30 and a few thousand.

193
00:20:08,080 --> 00:20:12,880
But smaller batches are better in most cases actually. And you compute the average of the

194
00:20:13,920 --> 00:20:21,200
gradient over those samples. Okay. So compute the average cost over those samples and compute the

195
00:20:21,200 --> 00:20:27,760
gradient of the average over those samples and then make an update. The reason for doing this

196
00:20:28,480 --> 00:20:35,600
is not intrinsically an algorithmic reason. It's because it's a simple way of parallelizing

197
00:20:36,560 --> 00:20:42,320
stochastic gradients on parallel hardware such as GPUs. Okay. So there's never, there's no good

198
00:20:42,320 --> 00:20:48,880
reason to do batching other than the fact that our hardware likes it. Okay. Question. Yeah. So

199
00:20:49,600 --> 00:20:54,880
for actually for, for real complex deep learning problems, does this objecting function have to

200
00:20:54,880 --> 00:21:00,880
be continuously differentiable? Well, it needs to be continuous mostly. If it's non continuous,

201
00:21:00,880 --> 00:21:08,960
you're going to get in trouble. It needs to be differentiable almost everywhere. But in fact,

202
00:21:10,320 --> 00:21:14,640
neural nets that most people use are actually not differentiable. And there's a lot of places

203
00:21:14,640 --> 00:21:18,400
where they're not differentiable. But they are continuous in the sense that there are functions

204
00:21:18,400 --> 00:21:23,360
that have kind of corners in them, if you want. They have kinks. And if you have a kink once in a

205
00:21:23,360 --> 00:21:34,880
while, it's not too much of a problem. But so in that case, those quantities should not be called

206
00:21:34,880 --> 00:21:39,840
gradients, they should be called subgradients. Okay. So a sub gradient is basically a generalization

207
00:21:39,840 --> 00:21:47,360
of the idea of derivative or gradient to functions that have kinks in them. So wherever you have a

208
00:21:47,360 --> 00:21:53,760
function that has a kink in it, any, any slope that is between the slope of one, one side and the

209
00:21:53,760 --> 00:22:02,400
slope of the other side is a, is a valid sub gradient. Okay. So when you write the kink,

210
00:22:02,400 --> 00:22:06,720
you decide, well, the derivative is this or it's that or it's going to somewhere in between. And

211
00:22:06,720 --> 00:22:15,440
you're fine. Most of the proof that applied to, you know, smooth functions, you know, in terms

212
00:22:15,440 --> 00:22:22,800
of minimization, often apply also to non-smooth function that basically are differentiable most

213
00:22:22,800 --> 00:22:30,000
of the way. So then how do we ensure strict convexity? We do not ensure strict convexity. The,

214
00:22:31,360 --> 00:22:37,040
in fact, in deep learning systems, most deep learning systems, the function that we are

215
00:22:37,040 --> 00:22:43,040
optimizing is non-convex, right? In fact, this is one reason why it took so long for deep learning

216
00:22:43,040 --> 00:22:48,000
to become prominent is because a lot of people, particularly theoreticians, people who sort of

217
00:22:48,000 --> 00:22:52,560
theoretically minded, were very scared of the idea that you had to minimize a non-convex

218
00:22:52,560 --> 00:22:56,560
objective and they say, this can't possibly work because we can't prove anything about it.

219
00:22:56,560 --> 00:23:00,240
It turns out it does work. You can't prove anything about it, but it does work. And so

220
00:23:01,040 --> 00:23:07,440
this is a situation, and it's an interesting thing to think about, a situation where the,

221
00:23:07,440 --> 00:23:11,600
the theoretical thinking basically limited what people could do in terms of engineering

222
00:23:12,480 --> 00:23:16,240
because they couldn't prove things about it. But that would be actually very powerful.

223
00:23:16,240 --> 00:23:19,040
Okay. Yeah. Like your colleague, you optimize non-convex functions.

224
00:23:19,840 --> 00:23:23,440
Like your colleague at the Bell Labs, who didn't like the non-mathy.

225
00:23:26,160 --> 00:23:31,520
Oh, it was a whole debate, you know, in the machine learning community that lasted 20 years,

226
00:23:31,520 --> 00:23:39,120
basically. All right. So what about how doesn't SGD get stuck in local minima once it reaches them?

227
00:23:39,120 --> 00:23:42,400
It does. Okay. So,

228
00:23:44,640 --> 00:23:52,080
so full gradient does get stuck in local minima, right? SGD gets like, you know,

229
00:23:52,080 --> 00:23:56,480
it's slightly less stuck in local minima because it's noisy. It allows it sometimes to escape

230
00:23:56,480 --> 00:24:05,440
local minima. But the real reason why we're going to optimize non-convex functions and local minima

231
00:24:05,440 --> 00:24:12,720
are not going to be such a huge problem is that there aren't that many local minima that are traps.

232
00:24:12,720 --> 00:24:19,600
Okay. So we're going to build neural nets, and those neural nets are, or deep running systems,

233
00:24:19,600 --> 00:24:24,720
and they're going to be built in such a way that the, the parameter space is such a, such a high

234
00:24:24,720 --> 00:24:29,120
dimension that is going to be very hard for the system to actually create local minima for us.

235
00:24:29,120 --> 00:24:36,240
Okay. So think about a picture where we have in one dimension a cost function that has one

236
00:24:36,240 --> 00:24:41,440
local minima and then a global minimum, right? Okay. So it's a function like this, right?

237
00:24:43,280 --> 00:24:47,040
And we start from here. If we optimize using gradient descent, we're going to get stuck in

238
00:24:47,040 --> 00:24:52,160
that local minimum. Now, let's imagine that we parameterize this function now with two parameters.

239
00:24:52,160 --> 00:24:55,840
Okay. So we're not a one dimensional, we're not looking at a one dimensional function anymore.

240
00:24:55,840 --> 00:24:58,560
We're looking at two dimensional function. We have an extra parameter.

241
00:24:59,680 --> 00:25:05,120
This extra parameter will allow us to go around the mountain and go towards the valley,

242
00:25:05,120 --> 00:25:10,320
perhaps without having to climb the little hill in the middle. Okay. So this is just an intuitive

243
00:25:10,320 --> 00:25:15,600
example to tell you that in very high dimensional spaces, you may not have as much of a local minimum

244
00:25:15,600 --> 00:25:21,040
problem as you have in the sort of intuitive picture of low dimensional spaces, right? So here

245
00:25:21,040 --> 00:25:25,120
that those pictures are in two dimensions. They are very misleading. We're going to be working

246
00:25:25,120 --> 00:25:34,080
with millions of dimensions and you know, some of the most recent deep learning systems have

247
00:25:34,080 --> 00:25:40,960
trillions of problems. Yeah. So local minima is not going to be that much of a problem. We're

248
00:25:40,960 --> 00:25:45,760
going to have other problems, but not that one. So there is like a trend in this hyper like

249
00:25:46,960 --> 00:25:52,080
over parameterization, right? Like it seems like that more neurons we have and the better

250
00:25:52,080 --> 00:25:56,400
these networks work somehow. That's right. So we're going to make those networks very large and

251
00:25:56,400 --> 00:25:59,520
they're going to be over parameterized, which means they're going to have way more adjustable

252
00:25:59,520 --> 00:26:03,200
parameters than we would actually need, which means they're going to be able to learn the

253
00:26:03,200 --> 00:26:07,360
training set almost perfectly. And the big question is how well are they going to work on

254
00:26:07,360 --> 00:26:12,560
a separate validation set or test set that is separate from the training set?

255
00:26:13,520 --> 00:26:19,040
Two more questions. They're going to work in a real situation where, you know, the distribution

256
00:26:19,040 --> 00:26:22,640
of samples may be different from what we trained it on. So that's the real question of machine

257
00:26:22,640 --> 00:26:28,320
learning, which I'm sure a lot of you are familiar with. Two more questions. Can we do?

258
00:26:28,320 --> 00:26:36,880
Yeah. So how do we escape instead of subtle points? Right. So there are tons and tons of

259
00:26:36,880 --> 00:26:41,920
subtle points in deep learning systems. A combinatorially large number of subtle points,

260
00:26:41,920 --> 00:26:48,160
as a matter of fact. I'll have a lecture on this. So I don't want to kind of spend too long

261
00:26:48,160 --> 00:26:54,240
answering. Okay. But yeah, there are subtle points. The trick with subtle points is you don't

262
00:26:54,240 --> 00:27:01,920
want to get too close to them, essentially. And stochastic gradient helps a little bit

263
00:27:01,920 --> 00:27:07,520
with subtle points. Some people are proposed sort of explicit methods to stay away from

264
00:27:07,520 --> 00:27:12,480
subtle points. But in practice, doesn't seem to be that much of a problem, actually.

265
00:27:12,720 --> 00:27:18,320
Finally, how do you pick samples for stochastic gradient in the center randomly?

266
00:27:19,680 --> 00:27:25,760
Okay. There is lots of different methods for that. Okay. Yeah. I mean, the basic thing you should do

267
00:27:25,760 --> 00:27:30,880
is you have your training set. You shuffle the samples in a random order. Okay. And then you

268
00:27:30,880 --> 00:27:39,360
just pick them one at a time. And then you cycle through them. An alternative is once you get to

269
00:27:39,360 --> 00:27:44,800
the end, you reshuffle them and then cycle through them again. An alternative is you pick a random

270
00:27:44,800 --> 00:27:50,800
sample using a random number. Every time you pick a new sample, you pick them randomly.

271
00:27:54,400 --> 00:28:00,800
If you do batching, a good idea is to put in a batch samples that are maximally different from

272
00:28:00,800 --> 00:28:04,800
each other. So things that are, for example, different categories if you do classification.

273
00:28:05,200 --> 00:28:09,600
But most people just do them, you know, just pick them randomly. But it's good to have samples

274
00:28:09,600 --> 00:28:15,520
that are maximally different that are nearby either in a batch or during the processor training.

275
00:28:15,520 --> 00:28:20,160
And then there are all kinds of tricks that people use to sort of emphasize difficult samples

276
00:28:21,120 --> 00:28:25,280
so that the boring, easy samples are not, you don't waste your time just, you know,

277
00:28:25,280 --> 00:28:30,800
seeing them over and over again. It's all kinds of tricks. All right. But, you know,

278
00:28:31,440 --> 00:28:36,000
the simpler one is, which most people use, you shuffle your samples and you run through them.

279
00:28:37,040 --> 00:28:41,360
Most people now use also data augmentation. So every sample is actually distorted by some

280
00:28:42,480 --> 00:28:46,880
process. For an image, you can distort the geometry a little bit, you change the colors,

281
00:28:46,880 --> 00:28:54,000
you add noise, et cetera. This is an artificial way of sort of adding more samples than you actually

282
00:28:54,000 --> 00:28:58,880
have. And people do this kind of randomly on the fly or they kind of precompute those

283
00:28:59,840 --> 00:29:05,760
those transformations. So lots of tricks there as well. Last question. How do you pick the batch

284
00:29:05,760 --> 00:29:12,720
size? The best. The batch, batch size. Oh, the batch size. That's determined by your hardware.

285
00:29:12,720 --> 00:29:19,600
So if you have a GPU, generally for, you know, reasonably sized networks, your batch size would

286
00:29:19,600 --> 00:29:24,560
be anywhere between 16 and 64 or something like that. For smaller networks, you might have to batch

287
00:29:24,640 --> 00:29:29,840
more to kind of exploit your, your hardware better to kind of have maximum usage of it.

288
00:29:30,560 --> 00:29:35,040
If you parallelize on multiple GPUs within a machine, you may have to, to have, you know,

289
00:29:35,040 --> 00:29:39,040
so let's say you have eight GPUs, then you'll be sort of eight times 32. So there's no

290
00:29:40,480 --> 00:29:46,160
256 or something. And then, you know, a lot of the big guys kind of parallelize that over

291
00:29:46,160 --> 00:29:50,640
multiple machines, each of which has eight GPUs. Some of them have TPUs, whatever. And then you

292
00:29:50,640 --> 00:29:54,880
might have to parallelize over thousands of examples. This diminishing return in doing this,

293
00:29:55,520 --> 00:30:01,600
when you increase the size of the batch, you actually reduce the, the, the speed of convergence.

294
00:30:01,600 --> 00:30:05,360
You accelerate the calculation, but you reduce the speed of convergence. So at some point,

295
00:30:05,360 --> 00:30:11,920
it's not worth increasing your batch size. So if we are doing a classification problem with k

296
00:30:11,920 --> 00:30:19,840
classes, what's going to be like our go to batch size? So there are papers that say if your batch

297
00:30:19,840 --> 00:30:24,880
size is significantly larger than the number of categories, or let's say twice the number of

298
00:30:24,880 --> 00:30:30,800
categories, then you're, you're probably wasting competition, essentially. Okay. I mean, throwing

299
00:30:30,800 --> 00:30:36,080
down convergence. So you're trying to train an image recognizer on ImageNet. If your batch size

300
00:30:36,080 --> 00:30:43,200
is larger than about a thousand, you're probably wasting time. Okay, that's it. Thanks. I mean,

301
00:30:43,200 --> 00:30:48,720
you're wasting competition. You're not wasting time. Okay. Okay. Okay. So let's talk about

302
00:30:48,720 --> 00:30:54,960
traditional neural net. So a traditional neural net is a, a, a model, a particular type of

303
00:30:54,960 --> 00:31:02,080
parameterized function, which is built by stacking linear and nonlinear operations. Right. So here

304
00:31:02,080 --> 00:31:05,680
is this kind of a depiction of a traditional neural net here in this case, with two layers, but I'm,

305
00:31:05,680 --> 00:31:10,640
you know, I'm not imagining there might be more layers here. So you have a bunch of inputs here

306
00:31:10,640 --> 00:31:19,120
on the left. Each input is multiplied by a weight, different weights, presumably. And those, the

307
00:31:19,120 --> 00:31:25,200
weighted sum of those inputs by those weights is, is computed here by what's called a unit or neuron.

308
00:31:26,160 --> 00:31:30,960
People don't like using the word neuron in that context, because there are incredibly simplified

309
00:31:30,960 --> 00:31:37,840
models of neurons in the brain, but, but that's the inspiration really. Okay. So one of those units

310
00:31:37,920 --> 00:31:42,480
just computes a weighted sum of its inputs, using those weights. Okay, this unit use,

311
00:31:42,480 --> 00:31:47,040
computes a different weighted sum of the same inputs with different weights and etc. So here

312
00:31:47,040 --> 00:31:50,480
we have three units here in the first layer. This is called a hidden layer, by the way,

313
00:31:51,360 --> 00:31:54,720
because it's neither an input nor an output, right? This is the input, and this is the output,

314
00:31:54,720 --> 00:31:59,200
and this is somewhere in the middle. So we compute those weighted sums, and then we pass those

315
00:31:59,200 --> 00:32:04,720
weighted sums individually through a, a nonlinear function. So here what I've shown is the value

316
00:32:04,720 --> 00:32:11,440
function. So this is called rectified linear unit. In the, this is the name that people

317
00:32:12,000 --> 00:32:18,000
have given it in the neural net lingual. In other contexts, this is called a half wave rectifier,

318
00:32:18,000 --> 00:32:24,640
if you're an engineer. It's called positive part, if you are a mathematician. Okay. Basically,

319
00:32:24,640 --> 00:32:29,600
it's a function that is equal to the identity when its argument is positive, and it's equal to zero

320
00:32:29,600 --> 00:32:38,080
if its argument is negative. Okay. So very simple graph. And then we stack a second layer of the

321
00:32:38,080 --> 00:32:42,480
same thing, the second stage, right? So again, a layer of linear operations where we compute

322
00:32:42,480 --> 00:32:47,200
weighted sums, and then we pass a result to nonlinearities. And we can stack many of those

323
00:32:47,200 --> 00:32:52,000
layers, and that's basically a traditional plain vanilla garden variety neural net.

324
00:32:53,200 --> 00:32:59,120
In this case, fully connected. So fully connected neural net means that every unit in one layer

325
00:32:59,120 --> 00:33:03,120
is connected to every unit in the next layer. And you have this sort of well organized layer,

326
00:33:04,640 --> 00:33:09,760
or architecture, if you want, right? Each of those weights are going to be the things that

327
00:33:09,760 --> 00:33:15,840
our learning algorithm is going to, is going to tune. And the big trick, the one trick really

328
00:33:15,840 --> 00:33:22,640
of deep learning is how we compute those gradients. Okay. So if you want, if you want to write this,

329
00:33:22,720 --> 00:33:29,200
you can say the weighted sum number i, so you can give a number to each of the units

330
00:33:29,920 --> 00:33:37,680
in the network. So this unit with number i, and the weighted sum s of i, is simply the sum

331
00:33:38,400 --> 00:33:43,840
where j goes over the upstream, the set of upstream units to i, which may be all the units in the

332
00:33:43,840 --> 00:33:51,120
previous layer or not could be just a subset. Okay. And then you compute the product of zj,

333
00:33:51,120 --> 00:33:57,600
which is the output of the unit number j times wij, which is the weight that links

334
00:33:57,600 --> 00:34:04,080
unit j to unit i. Okay. And then after that, you take this si, which is the weighted sum,

335
00:34:04,080 --> 00:34:08,400
you pass it through the activation function, this value, or whatever it is that you use,

336
00:34:08,400 --> 00:34:16,000
and that gives you zi, which is the activation for unit i. Okay. Super notation. By changing the

337
00:34:16,000 --> 00:34:20,240
set of upstream units of every unit, by building a graph of interconnection, you can basically

338
00:34:20,240 --> 00:34:27,360
build any kind of network arrangement that you want. There is one constraint that we can lift,

339
00:34:27,360 --> 00:34:33,120
that we will lift in the subsequent lecture, which is that the graph has to be

340
00:34:36,080 --> 00:34:40,320
ac-click in the sense that it can't have loops. Okay. If you have loops, that means you can't

341
00:34:40,320 --> 00:34:46,000
organize the units in layers. You can't sort of number them in a way that you can compute them

342
00:34:46,640 --> 00:34:51,680
so that every time you want to compute a unit, you already have the state of the previous units.

343
00:34:51,680 --> 00:34:56,720
If there are loops, then you can do that. Right? So for now, we're going to assume that

344
00:34:57,520 --> 00:35:00,800
the wij matrix, the w matrix, doesn't have loops,

345
00:35:04,000 --> 00:35:09,520
represents a graph that doesn't have loops. That's why I should say. Okay. So here's sort of an

346
00:35:09,520 --> 00:35:14,560
intuitive explanation of the back propagation algorithm. So the back propagation algorithm

347
00:35:14,560 --> 00:35:21,680
is the main technique that is used everywhere in deep learning to compute the gradient of

348
00:35:22,400 --> 00:35:28,240
a cost function, whatever it is, objective function, with respect to a variable inside

349
00:35:28,240 --> 00:35:32,960
of the network. This variable can be a state variable like a z or an s, or it could be a

350
00:35:32,960 --> 00:35:38,080
parameter variable like a w. Okay. And we're going to need to do both. Okay. So this is going to be

351
00:35:38,080 --> 00:35:41,600
an intuitive explanation. And then after that, there's going to be a more mathematical explanation,

352
00:35:41,600 --> 00:35:47,200
which is less intuitive, but perhaps actually easier to understand. But let me start with

353
00:35:47,200 --> 00:35:51,440
the intuition here. So let's say we have a big network. And inside of this big network, we have

354
00:35:51,440 --> 00:35:55,920
one of those little activation functions. Okay. In this case, it's a sigmoid function, but

355
00:35:55,920 --> 00:36:00,080
doesn't matter what it is for now. Okay. This function takes an s and produces a z.

356
00:36:01,040 --> 00:36:12,400
We call this function h of s, right? So when we wiggle z, the cost is going to wiggle by some

357
00:36:12,400 --> 00:36:19,120
quantity, right? And we divide the wiggling of z by the wiggling of c by the wiggling of z that

358
00:36:19,120 --> 00:36:25,040
causes it. That gives us the partial derivative of c with respect to z. So this one term is a gradient

359
00:36:25,040 --> 00:36:29,360
of c with respect to all the z's in the network. And there's one component of that gradient,

360
00:36:29,360 --> 00:36:36,240
which is the partial derivative of the cost with respect to that single variable z inside the

361
00:36:36,240 --> 00:36:43,040
network. Okay. And that really indicates how much c would wiggle if we wiggled z by some amount.

362
00:36:43,040 --> 00:36:46,960
We divide the wiggling of c by the wiggling of z and that gives us the partial derivative

363
00:36:46,960 --> 00:36:53,840
of c with respect to z. This is not how we're going to compute the gradient of c with respect to z,

364
00:36:53,840 --> 00:36:59,200
but this is a description of what it is conceptually. Okay. Or intuitively, rather.

365
00:36:59,920 --> 00:37:03,440
Okay. So let's assume that we know this quantity. So we know the partial derivative

366
00:37:04,000 --> 00:37:11,280
of c with respect to z. Okay. So c with respect to z is this quantity here, dc over dz. Okay.

367
00:37:12,720 --> 00:37:17,840
So think of dz as the wiggling of z and dc as the wiggling of c, divide one by the other,

368
00:37:17,840 --> 00:37:24,800
and you get the partial derivative of c with respect to z. What we have here is,

369
00:37:28,400 --> 00:37:34,000
what we have to apply is the chain rule, the rule that tells us how to compute the

370
00:37:34,560 --> 00:37:39,280
derivative of a function composed of two individual functions that we apply one after the other.

371
00:37:39,280 --> 00:37:43,600
Right. So remember, chain rule, if you have a function g, then you apply to another function h,

372
00:37:44,320 --> 00:37:47,200
which is function of parameter s, and you want the derivative of it.

373
00:37:48,560 --> 00:37:52,480
The derivative of that is equal to the derivative of g at point h of s,

374
00:37:52,480 --> 00:37:59,440
multiplied by the derivative of h at point s. Right. That's chain rule. You know that a few

375
00:37:59,440 --> 00:38:05,280
years ago, hopefully. Now, if I want to write this in terms of partial derivative, it's the same

376
00:38:05,280 --> 00:38:10,080
thing, right? Partial derivative is just a derivative just with respect to one single variable.

377
00:38:10,080 --> 00:38:15,840
So I would write this something like this, dc over ds. So c really is the result of applying

378
00:38:15,840 --> 00:38:22,560
this h function to s, and then applying some unknown g function to compute c, which is kind

379
00:38:22,560 --> 00:38:29,200
of the rest of the network plus the cost. But I'm just going to call the gradient. I'm going to

380
00:38:29,200 --> 00:38:38,080
assume that this dc over dz is known. Someone gave it to me. So this variable here on the right,

381
00:38:38,080 --> 00:38:45,840
dc over dz is given to me, and I want to compute dc over ds. So what I need to do is write this,

382
00:38:46,800 --> 00:38:54,480
dc over ds equal dc over dz times dz over ds. Right. And why is this identity true? It's because

383
00:38:54,480 --> 00:39:01,760
I can simplify by dz. It's as simple as this, right? So you have, you know, trivial algebra,

384
00:39:01,760 --> 00:39:06,080
you have dz at the denominator here, dz at the numerator here, simplify, you get dc over ds.

385
00:39:06,560 --> 00:39:11,520
It's a very trivial, simple identity, which is basically just generally applied to partial derivatives.

386
00:39:13,440 --> 00:39:20,240
Now, dz over ds, we know what it is. It's just h prime of s, just the derivative of the h function.

387
00:39:21,760 --> 00:39:27,040
So we have this formula, dc over ds equal dc over dz, which we assume is known, times h prime of s.

388
00:39:28,320 --> 00:39:33,040
What does that mean? That means that if we have this component of the gradient of the cost function

389
00:39:33,040 --> 00:39:40,640
with respect to z here, we multiply this by the derivative of the h function at point s,

390
00:39:40,640 --> 00:39:46,320
the same point s that we had here. And what we get now is the gradient of the cost function

391
00:39:46,320 --> 00:39:52,640
with respect to s. Now, here's the trick. If we had a chain of those h functions, we could keep

392
00:39:52,640 --> 00:39:56,720
propagating this gradient backwards by just multiplying by the derivative of all those h

393
00:39:56,720 --> 00:40:03,280
functions going backwards. And that's why it's called back propagation. So it's just a practical

394
00:40:03,280 --> 00:40:08,240
application of a chain rule. And if you want to convince yourself of this, you can run through

395
00:40:08,240 --> 00:40:16,160
this idea of perturbation. If I twiddle s by some value, it's going to twiddle z by some value equal

396
00:40:16,160 --> 00:40:26,160
to ds times h prime of s, basically the slope of s. So dz equals h prime of s times ds. And then

397
00:40:26,160 --> 00:40:34,640
I'm going to have to multiply this by dc over dz. And so I rearrange the terms and I get immediately

398
00:40:34,640 --> 00:40:43,920
that this formula dc over ds equals dc over dz times h prime of s. So we had another element in

399
00:40:43,920 --> 00:40:50,240
our multilayer net, which was the linear sum. And there, it's just a little bit more complicated,

400
00:40:50,240 --> 00:40:58,080
but not really. So one particular variable z here, we would like to compute the derivative,

401
00:40:58,080 --> 00:41:05,120
the partial derivative of our cost function with respect to that z. And we're going to assume that

402
00:41:05,120 --> 00:41:10,000
we know the partial derivative of s with respect to each of those s's, the weighted sums at the

403
00:41:10,000 --> 00:41:18,720
next layer that z is going into. So z only influences c through those s's. So presumably,

404
00:41:18,720 --> 00:41:26,640
by basically multiplying how each of those s's influence c and then multiplying by how z

405
00:41:27,600 --> 00:41:31,760
influences each of the s's and summing up, we're going to get the influence of z over c.

406
00:41:31,760 --> 00:41:35,120
Right? And that's the basic idea. Okay, so here's what we're going to do.

407
00:41:35,840 --> 00:41:50,240
Let's say we perturb z by dz. This is going to perturb s0 by dz times w0. Okay, we multiply z

408
00:41:50,240 --> 00:41:56,960
by w0. So the derivative of this linear operation is the coefficient itself. Right? So here,

409
00:41:57,600 --> 00:42:09,440
the perturbation is ds0 is equal to dz times w0. Okay? And now in turn, this is going to modify c,

410
00:42:10,160 --> 00:42:19,520
and we're going to multiply this quantity by dc over ds0 to get the dc, if you want. Okay?

411
00:42:20,640 --> 00:42:25,840
Now, whenever we perturb z, it's not going to perturb just s0, it's also going to perturb s1

412
00:42:25,840 --> 00:42:31,280
and s2. And to see the effect on c, we're going to have to sum up the effect of the perturbation

413
00:42:31,280 --> 00:42:37,120
on each of the s's and then sum them up to see the overall effect on c. So this is written here

414
00:42:37,120 --> 00:42:46,400
on the left. The perturbation of c is equal to the perturbation of s multiplied by the partial

415
00:42:46,400 --> 00:42:53,600
derivative of c with respect to s plus the perturbation of s1 multiplied by the partial

416
00:42:53,600 --> 00:43:00,160
derivative of dc with respect to s1 plus same thing for s2. Okay? So this is the fact that,

417
00:43:01,280 --> 00:43:06,880
you know, we need to take into account all the perturbations here that z may influence.

418
00:43:08,480 --> 00:43:13,280
And so I can just write down now a very simple thing, you know, because dc of 0 is equal to

419
00:43:13,280 --> 00:43:21,840
w0 times dz and, you know, ds of 2 is w2 times dz, I can plug this in there and just write dc

420
00:43:21,840 --> 00:43:28,480
over dz equal dc over ds0, which I assume is known, times w0 plus dc over ds1 times w1

421
00:43:28,480 --> 00:43:34,320
plus dc over ds2 times w2. Okay? If I want to represent this operation graphically,

422
00:43:35,200 --> 00:43:42,720
this is shown on the right here. I have dc over ds0, dc over ds1, dc over ds2, which I assume

423
00:43:42,720 --> 00:43:55,120
are known or given to me somehow. I compute dc over ds0 multiplied by w0 and multiply dc over ds1

424
00:43:55,120 --> 00:44:01,760
by w1, dc over ds2 by w2. I sum them up and that gives me dc over dz. Okay? It's just the formula

425
00:44:01,760 --> 00:44:08,800
here. Okay? So here's the cool trick about back propagation through a linear module that computes

426
00:44:08,800 --> 00:44:14,480
weighted sums. You take the same weights and you still compute weighted sum with those weights,

427
00:44:14,480 --> 00:44:20,480
but you use the weights backwards. Okay? So whenever you had the unit that was sending its output to

428
00:44:20,480 --> 00:44:27,200
multiple outputs to multiple units through a weight, you take the gradient of the cost with

429
00:44:27,200 --> 00:44:34,000
respect to all those weighted sums and you compute their weighted sum backwards using the weights

430
00:44:34,000 --> 00:44:41,840
backwards to get the gradient with respect to the state of the unit at the bottom. You can do

431
00:44:41,840 --> 00:44:48,720
this for all the units. Okay? So it's super simple. Now, if you were to write a program to do backprop

432
00:44:48,720 --> 00:44:54,880
for classical neural nets in Python, it would take like half a page. It's very, very simple.

433
00:44:57,520 --> 00:45:01,280
Is one function to compute weighted sums going forward in the right order?

434
00:45:01,360 --> 00:45:07,840
Another function and applying the nonlinearity is another function to compute weighted sums

435
00:45:07,840 --> 00:45:13,120
weighted sums going backward and multiplying by the derivative of the nonlinearity at every step.

436
00:45:14,160 --> 00:45:19,200
Right? It's incredibly simple. What's surprising is that it took so long for people to realize this

437
00:45:19,200 --> 00:45:26,480
was so useful, maybe because it was too simple. Okay? So it's useful to write this in matrix form.

438
00:45:26,480 --> 00:45:34,800
So really, the way you should think about a neural net of this type is each state inside the

439
00:45:34,800 --> 00:45:38,560
network, think of it as a vector. It could be a multidimensional array, but let's think of it

440
00:45:38,560 --> 00:45:44,000
just as a vector. A linear operation is just going to multiply this vector by matrix and each row of

441
00:45:44,000 --> 00:45:48,880
the matrix contains all the weights that are used to compute a particular weighted sum for a particular

442
00:45:48,880 --> 00:45:58,560
unit. Okay? So multiply this by this matrix. So this dimension has to be equal to that dimension,

443
00:45:58,560 --> 00:46:02,560
which is not really well depicted here, actually. One sec. From the previous slide,

444
00:46:02,560 --> 00:46:11,040
you wrote ds0. What is s, differentiated with respect to? So there is a ds. What is ds, basically?

445
00:46:11,600 --> 00:46:21,600
ds0, you mean? Yeah. Okay. ds0 is a perturbation of s0. Okay? An infinitely small perturbation of s0.

446
00:46:23,040 --> 00:46:28,880
Doesn't matter what it is. Okay? And what we're saying here is that if you have an infinitely

447
00:46:28,880 --> 00:46:34,960
small perturbation of s0, and you multiply this perturbation by the partial derivative of c with

448
00:46:34,960 --> 00:46:45,280
respect to s0, okay? You get the perturbation of c, except that that corresponds to this

449
00:46:45,280 --> 00:46:49,440
perturbation of s0, right? But we're not interested in just the perturbation of s0. We're

450
00:46:49,440 --> 00:46:54,080
also interested in the perturbation of s1 and s2. So the overall perturbation of c would be the sum

451
00:46:54,080 --> 00:47:00,160
of the perturbations of s0, s1, and s2 multiplied by the corresponding partial derivative of c

452
00:47:00,160 --> 00:47:06,960
with respect to each of them. Okay? You know, it's a virtual thing, right? It's not an existing

453
00:47:06,960 --> 00:47:12,080
thing you're going to manipulate. Just imagine that there is some perturbation of s0 here.

454
00:47:13,200 --> 00:47:16,880
Okay? This is going to perturb c by some value, and that value is going to be the perturbation

455
00:47:16,880 --> 00:47:22,640
of s0 multiplied by the partial derivative of c with respect to s0. Okay? And then if you perturb

456
00:47:23,360 --> 00:47:29,440
s1 simultaneously, you're also going to cause a perturbation of c. If you perturb s2 simultaneously,

457
00:47:29,440 --> 00:47:34,560
you're also going to cause a perturbation of c. The overall perturbation of c will be the sum

458
00:47:34,560 --> 00:47:40,960
of those perturbations, and that is given by this expression here. Now, those d, those infinitely

459
00:47:40,960 --> 00:47:47,600
small quantities, ds, dc, etc., think of them as, you know, numbers. You can do algebra with them.

460
00:47:47,600 --> 00:47:51,680
You can divide one by the other. You know, you can do stuff like that. So now you say, you know,

461
00:47:51,680 --> 00:48:06,080
what is ds0 equal to? If I tweak z by a quantity dz, it's going in turn to modify s0 by ds0.

462
00:48:06,080 --> 00:48:14,880
Okay? And what is the quantity by which s0 is going to be tweaked? If I tweak z by dz,

463
00:48:14,880 --> 00:48:22,400
because s is the result of computing the product of z by w0, then the perturbation is also going

464
00:48:22,400 --> 00:48:29,360
to be multiplied by w0, right? So the ds0 corresponding to a particular dz is going

465
00:48:29,360 --> 00:48:36,000
to be equal to dz times w0. And this is what's expressed here. Okay? ds0 equal w0 dz.

466
00:48:37,200 --> 00:48:42,160
Okay. Now, if I take this expression for ds0 and I insert it here in this formula,

467
00:48:43,040 --> 00:48:50,960
okay, I get dc equal w0 times dz times dc over ds0 plus same thing for 1 plus same thing for 2.

468
00:48:50,960 --> 00:48:57,360
And I'm going to take the dz and pass it to the other side. I'm going to divide both sides by dz.

469
00:48:57,360 --> 00:49:04,000
So now I get dc over dz equal, the dz doesn't appear anymore because it's been put underneath

470
00:49:04,000 --> 00:49:13,280
here. It's w0 times dc over ds0 plus w1 times dc over ds1, et cetera. Okay? It's just simple algebra.

471
00:49:15,360 --> 00:49:20,160
It's differential calculus, basically. Right. So it's better to write this in matrix form.

472
00:49:21,680 --> 00:49:30,240
So really, when you're computing, if I go back a few slides, when this is really kind of a matrix

473
00:49:30,240 --> 00:49:36,400
of all the weights that are kind of upstream of the zj's, so you can align the zj as a vector,

474
00:49:37,600 --> 00:49:45,360
maybe only the zj's that have nonzero terms in w, wij. And then you can write those w's

475
00:49:45,360 --> 00:49:51,120
as a matrix, and this is just a matrix vector product. Okay? So this is the way this would be

476
00:49:51,120 --> 00:49:55,520
written. You have a vector, you multiply by matrix, you get a new vector, pass that through

477
00:49:55,520 --> 00:50:01,840
nonlinearities, reuse, multiply that by matrix, et cetera. Right? So symbolically, you can write

478
00:50:02,400 --> 00:50:08,000
a simple neural net this way. We have linear blocks, okay, linear functional blocks, which

479
00:50:08,000 --> 00:50:15,200
basically take the previous state and multiply by matrix. Okay? So you have a state here, z1,

480
00:50:15,200 --> 00:50:21,760
multiply by matrix, you get w1, z1, and that gives you the vector of weighted sums, s2.

481
00:50:21,760 --> 00:50:29,440
Okay? Then you take that, pass it through the nonlinear functions, each component individually,

482
00:50:30,160 --> 00:50:36,800
and that gives you z2. Right? So that's a three-layer neural net. First weight matrix,

483
00:50:36,800 --> 00:50:41,520
nonlinearity, second weight matrix, nonlinearity, third weight matrix, and this is the output.

484
00:50:41,520 --> 00:50:47,920
There are two hidden layers, three layers of weights. Okay, the reason for writing it this way

485
00:50:47,920 --> 00:50:54,720
is that this is, like symbolically, the easiest way to understand really what kind of backprop

486
00:50:54,720 --> 00:51:01,120
does. And in fact, it corresponds also to the way we define neural nets and we run them on

487
00:51:03,120 --> 00:51:11,360
deep learning frameworks like PyTorch. So this is the sort of object-oriented version of

488
00:51:12,320 --> 00:51:19,440
defining a neural net in PyTorch. We're going to use predefined class, which are the linear class

489
00:51:20,480 --> 00:51:27,040
that basically multiplies a vector by matrix. It also has biases, but let's not talk about this

490
00:51:27,040 --> 00:51:32,320
just now. And another class, which is the value function, which takes a vector or a multi-dimensional

491
00:51:32,320 --> 00:51:38,800
array and applies the nonlinear function to every component separately. Okay, so this is

492
00:51:38,800 --> 00:51:46,000
a little piece of Python program that uses Torch. We import Torch. We make an image, which is, you

493
00:51:46,000 --> 00:51:51,520
know, 10 pixels by 20 pixels and three components for color. We compute the size of it and we're

494
00:51:51,520 --> 00:51:56,320
going to plug a neural net where the number of inputs is the number of components of our image.

495
00:51:56,320 --> 00:52:02,000
So in this case, that would be 600 or so. And we're going to define a class. The class is going

496
00:52:02,000 --> 00:52:06,720
to define a neural net and that's pretty much all we need to do here. So we define our network

497
00:52:06,720 --> 00:52:10,000
architecture. It's a subclass of neural net module, which is a pretty fine class.

498
00:52:11,520 --> 00:52:16,880
It's got a constructor here that will take the sizes of the internal layers that we want,

499
00:52:16,880 --> 00:52:23,360
the size of the input, the size of S1 and Z1, the size of S2 and Z2, and the size of S3.

500
00:52:25,040 --> 00:52:32,720
We call the parent class initializer. And then we just create three modules that are all linear

501
00:52:33,680 --> 00:52:37,040
modules. And we need to kind of store them somewhere because they have internal parameters.

502
00:52:37,040 --> 00:52:43,120
So we're going to have three slots in our object, N0, N1, N2, module 1, module 0, module 1, module 2.

503
00:52:44,000 --> 00:52:49,440
And each of them is going to be an instance of the class NN.linear with two sizes, the input size

504
00:52:49,440 --> 00:52:55,440
and the output size. Okay, so the first module has input size D0, output size D1, etc. And those

505
00:52:55,440 --> 00:53:00,880
classes are, since there is a capital L, means it's an object and inside there are parameters

506
00:53:00,880 --> 00:53:08,000
inside that item there. Right. So for example, the value doesn't have a capital because it doesn't

507
00:53:08,000 --> 00:53:12,960
have internal parameters. It's not kind of a trainable module. It's just a function. Whereas

508
00:53:12,960 --> 00:53:17,120
those things with capitals, they have sort of internal parameters, the weight matrices inside

509
00:53:17,120 --> 00:53:23,760
of them. So now we define a forward function, which basically computes the output from the input.

510
00:53:24,480 --> 00:53:30,800
And the first thing we do is we take the input thing, which may be a multidimensional array,

511
00:53:30,800 --> 00:53:38,480
and we flatten it. We flatten it using this idiomatic expression here in PyTorch.

512
00:53:39,840 --> 00:53:48,320
And then we apply the first module to X. We put the result in S1, which is a temporary variable,

513
00:53:48,960 --> 00:53:56,000
then we apply the value to S1, put the result in Z, then apply the second layer,

514
00:53:56,000 --> 00:54:01,840
put the result in S2, apply the value again, put the result in S3, and then the last linear

515
00:54:01,840 --> 00:54:06,960
layer, put the result in S3 and return S3. And there is a typo. So the second line should have

516
00:54:06,960 --> 00:54:20,320
been S1, it's the self.m0 of Z0, right? Z0 here, yes, correct. Yeah, this is something that

517
00:54:21,040 --> 00:54:27,520
is going to be fixed, right? Which I didn't fix. I know. This is Z0. Thanks for reminding me of this.

518
00:54:30,480 --> 00:54:34,960
Okay, but you'll see examples. I mean, I'll show you kind of actual examples of this,

519
00:54:34,960 --> 00:54:39,040
and you'll be able to run them yourself. That's all you need to do. You don't need to write

520
00:54:41,440 --> 00:54:46,240
how you compute the back prop, how you propagate the gradients. You could write it,

521
00:54:46,240 --> 00:54:49,600
and it would be as simple as forward. You could write a backward function, and it would basically

522
00:54:50,640 --> 00:54:54,160
multiply by the matrices going backwards. But you don't need to do this because

523
00:54:54,160 --> 00:54:58,720
PyTorch does this automatically for you. When you define the forward function, it knows what

524
00:54:59,520 --> 00:55:02,960
modules you've called in what order, what are the dependencies between the variables,

525
00:55:02,960 --> 00:55:07,280
and it will know how to generate the functions that compute the gradient

526
00:55:07,280 --> 00:55:12,400
backwards. So you don't need to worry about it. That's the magic of PyTorch, if you want.

527
00:55:12,400 --> 00:55:16,720
That's a bit the magic of deep learning, really. That's called automatic differentiation,

528
00:55:18,720 --> 00:55:23,120
and this is a particular form of automatic differentiation. There's another way to write

529
00:55:23,680 --> 00:55:28,000
functions in PyTorch that are kind of more functional. So you're not using modules

530
00:55:28,000 --> 00:55:31,840
with internal parameters. You're just coding functions one after the other. And PyTorch

531
00:55:31,840 --> 00:55:36,800
has a mechanism by which it can automatically compute the gradient of any function you define

532
00:55:36,800 --> 00:55:42,240
with respect to whatever parameters you want. Yeah, actually, these big guys with the capital L,

533
00:55:42,240 --> 00:55:48,720
like the nn.capital linear inside is going to have a lowercase linear, which is like the functional

534
00:55:48,720 --> 00:55:55,040
part, which is performing the matrix multiplication between the weights stored inside the object

535
00:55:55,040 --> 00:56:01,920
with the capital L and then the input. So every capital letter object will inside have

536
00:56:01,920 --> 00:56:07,680
the functional way. So one can decide to use either the functional form by default,

537
00:56:07,680 --> 00:56:13,840
or use this encapsulated version, which are more convenient to just use, right?

538
00:56:14,400 --> 00:56:19,920
Right. So at the end, you can create an instance of this class. You can create multiple instances,

539
00:56:19,920 --> 00:56:24,080
but you can create one here, just call my net and give it the sizes you want.

540
00:56:25,360 --> 00:56:31,360
And then to apply this to a particular image, you just do how to equal model of image. That's

541
00:56:31,360 --> 00:56:38,320
as simple as that. Okay, so this is your first neural net, and it does all the backup automatically.

542
00:56:39,520 --> 00:56:44,800
But you need to understand how that works, right? It's not because PyTorch does it for you,

543
00:56:45,360 --> 00:56:51,440
that you can sort of forget about how you actually compute the gradient of a function,

544
00:56:51,440 --> 00:56:54,320
because it's inevitable that at some point, you're going to want to

545
00:56:54,960 --> 00:56:58,240
actually assemble a neural net with a module that does not pre-exist, and you're going to have to

546
00:56:58,240 --> 00:57:04,560
write your own backup function. So to do this, you basically have, if you want to create a new module

547
00:57:04,560 --> 00:57:11,840
with some complex operation that does not pre-exist in PyTorch, then you do something like

548
00:57:11,840 --> 00:57:16,880
this. You define your class, but you write your own backward function, basically.

549
00:57:18,960 --> 00:57:28,560
Okay, so let's get one step up in terms of abstraction, and write this in sort of slightly more

550
00:57:31,120 --> 00:57:38,480
generic form, mathematical form, if you want. So let's say we have a cost function here,

551
00:57:39,040 --> 00:57:44,560
and we want to compute the gradient of this cost function with a stack to a particular

552
00:57:44,560 --> 00:57:48,880
vector in the system ZF. It could be a parameter, it could be a state, it doesn't matter. Okay,

553
00:57:49,680 --> 00:57:56,320
some states inside. And we have chain rule, and chain rule is nothing more than this,

554
00:57:56,320 --> 00:58:04,320
that I explained earlier. dC over dZF is equal to dC over dZG, dZG over dZF, as long as C

555
00:58:04,640 --> 00:58:13,760
is only influenced by ZF through ZG. There's no other way for ZF to influence C than to go

556
00:58:13,760 --> 00:58:20,160
through ZG, then this formula is correct. Okay? And of course, the identity is trivial,

557
00:58:20,160 --> 00:58:28,640
because it's just a simplification by this infinitesimal vector quantity dZG. Okay?

558
00:58:29,600 --> 00:58:36,080
So let's say ZG is a vector of size dG by one, so this means column vector. Okay?

559
00:58:36,800 --> 00:58:40,560
And ZF is a column vector of size dF.

560
00:58:45,600 --> 00:58:49,280
This is, if you want to write the correct dimensions of this,

561
00:58:51,200 --> 00:58:55,680
you know, we get something a little complicated. Okay, so first of all,

562
00:58:56,640 --> 00:59:03,440
this object here, dZG over dZF, well, let me start with this one. Okay,

563
00:59:03,440 --> 00:59:08,800
this one dC over dZG, that's a gradient vector. Okay? ZG is a vector, dC over dZG is a gradient

564
00:59:08,800 --> 00:59:16,320
vector. And it's the same size as dZG. But by convention, we actually write it as a line,

565
00:59:16,320 --> 00:59:23,440
as a row vector. Okay? So this thing here is going to be a row vector whose size is the same size

566
00:59:23,440 --> 00:59:27,440
as ZG, but it's going to be horizontal instead of vertical. Okay?

567
00:59:30,400 --> 00:59:33,600
This object here is something more complicated. It's actually a matrix.

568
00:59:34,640 --> 00:59:40,800
Why is it a matrix is because it's the derivative of a vector with respect to another vector.

569
00:59:40,800 --> 00:59:46,240
Okay? So let's look at this diagram here on the right. We have a function G, it takes ZF as an

570
00:59:46,240 --> 00:59:52,640
input, and it produces ZG as an output. And if we want to capture the information about the

571
00:59:52,640 --> 01:00:00,320
derivative of that module, which is this quantity here dZG over dZF, there's a lot of terms to

572
01:00:00,320 --> 01:00:06,160
capture because there's a lot of ways in which every single output, every component of ZG can be

573
01:00:06,160 --> 01:00:11,680
influenced by every component of ZF. Right? So if for every pair of components, ZG and ZF,

574
01:00:12,240 --> 01:00:19,040
there is a derivative term, which indicates by how much ZG would be perturbed if I perturbed ZF

575
01:00:19,040 --> 01:00:27,920
by a small infinitesimal quantity. Right? We have that for every pair of components of ZG and ZF.

576
01:00:27,920 --> 01:00:35,760
As a result, this is a matrix whose dimension is the number of rows is the size of ZG and the

577
01:00:35,760 --> 01:00:46,160
number of columns is the size of ZF. And each term in this matrix is one partial derivative term.

578
01:00:46,160 --> 01:00:53,760
So this whole matrix here, if I take the component ij, it's the partial derivative of the i-th

579
01:00:53,760 --> 01:01:02,560
output of that module, the i-th component of ZG, with respect to the j-th component of ZF.

580
01:01:04,000 --> 01:01:12,000
Okay? So what we get here is a row vector is equal to a row vector multiplied by a matrix,

581
01:01:12,000 --> 01:01:18,720
and the sizes kind of work out so that they're compatible with each other.

582
01:01:19,920 --> 01:01:23,280
Okay. So what is back propagation now? Back propagation is this formula.

583
01:01:24,720 --> 01:01:29,200
Okay? It says if you have the gradient of some cost function with respect to some variable,

584
01:01:29,200 --> 01:01:32,080
and you know the dependency of these variables with respect to another variable,

585
01:01:32,080 --> 01:01:36,960
you multiply this gradient vector by that Jacobian matrix, and you get the gradient

586
01:01:36,960 --> 01:01:43,120
vector with respect to that second variable. So graphically here on the right, if I have

587
01:01:43,920 --> 01:01:49,520
the gradient of the cost with respect to ZG, which is DC over DZG, and I want to compute

588
01:01:49,520 --> 01:01:56,080
the gradient of C with respect to ZF, which is DC over DZF, I only need to take that vector,

589
01:01:56,080 --> 01:02:03,040
which is a row vector, multiply it by the Jacobian matrix, DG over DZF, or DZG over DZF,

590
01:02:03,920 --> 01:02:11,280
and I get DC over DZF. Okay? It's this formula. Someone is objecting here. Isn't the summation

591
01:02:11,280 --> 01:02:19,840
missing here? Which summation? Summation of all the components of these partial multiplications.

592
01:02:19,840 --> 01:02:24,480
Here? Yeah. Well, this is a vector. This is a vector. This is a matrix. There is a lot of

593
01:02:24,480 --> 01:02:27,760
sums going on here because when you compute the product of this vector with its matrix,

594
01:02:27,760 --> 01:02:32,720
you're going to have a lot of sums, right? Yep. So it's hidden, right? Yeah. The sums are hidden.

595
01:02:33,040 --> 01:02:35,520
Okay. Inside of this vector matrix product.

596
01:02:40,080 --> 01:02:44,320
Like, you can take a specific example. Let's imagine that this G function is just a matrix

597
01:02:44,320 --> 01:02:52,160
multiplication. Okay? We just multiply by ZF by matrix W. So we have a linear operation. The derivative

598
01:02:52,160 --> 01:02:59,360
of the Jacobian matrix of the multiplication by matrix is the transpose of that matrix. So what

599
01:02:59,360 --> 01:03:04,000
we're going to do here is take this vector, multiply it by the transpose of the W matrix,

600
01:03:04,800 --> 01:03:13,200
and what we get is that vector. Okay? And it all makes sense, right? The sizes make sense.

601
01:03:13,200 --> 01:03:19,600
This matrix here is the transpose of the weight matrix, which of course had the reverse size.

602
01:03:20,640 --> 01:03:24,240
We multiply it. We pre-multiply it by the row vector of the gradient from the

603
01:03:25,200 --> 01:03:28,080
layer above, and we get the gradient with respect to the layer below.

604
01:03:30,640 --> 01:03:35,760
Okay? So backpropagating through a linear module just means multiplying the transpose

605
01:03:35,760 --> 01:03:42,000
of the matrix used by that module. And it's just a generalized form of what I explained earlier,

606
01:03:42,560 --> 01:03:48,000
you know, of propagating through the weights of a linear system. But it's less intuitive, right?

607
01:03:50,080 --> 01:03:53,840
Okay. So we're going to be able to do backpropagation by computing gradients all the way through,

608
01:03:53,840 --> 01:04:02,080
by propagating backwards. But this module really has two inputs. It has an input, which is ZF,

609
01:04:02,080 --> 01:04:11,040
and the other one is WG, the weight matrix, the parameter vector that is used inside of this

610
01:04:11,040 --> 01:04:17,760
module. So there is a second Jacobian matrix, which is the Jacobian matrix of ZG with respect to

611
01:04:18,560 --> 01:04:25,200
the terms of this weight parameter. Okay? And to compute the gradient of the cost function

612
01:04:25,200 --> 01:04:32,080
with respect to those weight parameters, I need to multiply this gradient vector by the Jacobian

613
01:04:32,080 --> 01:04:37,360
matrix of that block with respect to its weight. And it's not the same as the Jacobian matrix with

614
01:04:37,360 --> 01:04:44,480
respect to the input. It's a different Jacobian matrix. I'll come back to this in a second.

615
01:04:45,120 --> 01:04:55,200
So to do backprop, again, if we have a vector of gradients of some cost with respect to a state,

616
01:04:55,920 --> 01:05:00,960
and we have a function that is a function of one or several variables, we multiply this gradient by

617
01:05:00,960 --> 01:05:06,160
the Jacobian matrix of this block with respect to each of these inputs, and that gives us the

618
01:05:06,160 --> 01:05:11,520
gradient with respect to each of the inputs. And that's going to be expressed here. So this

619
01:05:11,520 --> 01:05:21,440
is the backpropagation of states in a layer-wise classical type neural net. DC over DZK, which is

620
01:05:21,440 --> 01:05:27,920
the state of layer K, is DC over ZK plus one, which is the gradient of the cost with respect to

621
01:05:27,920 --> 01:05:35,120
the layer above, times the Jacobian matrix of the state of layer K plus one with respect to the

622
01:05:35,200 --> 01:05:42,000
state of layer K. Now we assume DC over DZK plus one is known, and we just need to multiply

623
01:05:42,000 --> 01:05:46,720
with the Jacobian matrix of the function that links ZK to ZK plus one. The function is used to

624
01:05:46,720 --> 01:05:51,280
compute ZK plus one from ZK. And this may be a function also of some parameters inside.

625
01:05:51,280 --> 01:05:57,440
But here, that's the matrix of partial derivatives of F, which is with output to ZK plus one,

626
01:05:57,440 --> 01:06:04,800
with respect to each of the components of ZK. So that's the first rule of backpropagation,

627
01:06:04,800 --> 01:06:09,840
and it's a recursive rule. So you can start from the top. You start initially with DC over DC,

628
01:06:09,840 --> 01:06:16,640
which is one, which is why I have this one here on top. And then you just keep multiplying by

629
01:06:17,680 --> 01:06:23,360
the Jacobian matrix all the way down, and backpropagate gradients. And now you get gradients

630
01:06:23,360 --> 01:06:26,960
with respect to all the states. You also want the gradients with respect to the weights,

631
01:06:26,960 --> 01:06:32,240
because that's what you need to do learning. So what you can write is the same chain rule,

632
01:06:32,240 --> 01:06:38,160
DC over DWK is equal to DC over the ZK plus one, which we assume is known, times

633
01:06:38,160 --> 01:06:44,240
DZK plus one of DWK, right? And you can write this as DC over DK plus one. And the dependency

634
01:06:44,240 --> 01:06:50,560
between ZK plus one and WK is the function ZK applied to WK. So you can differentiate the

635
01:06:51,920 --> 01:06:55,280
function, the output of the function ZK with respect to WK, and that gives you another

636
01:06:55,280 --> 01:06:59,360
Jacobian matrix. And so those two formulas, you can do backpropagation just about anything.

637
01:07:00,080 --> 01:07:06,320
Really what goes on inside PyTorch and inside most of those frameworks, TensorFlow and

638
01:07:06,320 --> 01:07:12,000
Jackson, whatever. It's something like this where you have, so let's take a very simple

639
01:07:12,000 --> 01:07:16,960
diagram here where you have an input parameterized function that computes an output that goes to

640
01:07:16,960 --> 01:07:21,520
a cost function. And that cost function measures the discrepancy between the output of the system

641
01:07:21,600 --> 01:07:29,840
and the desired output. So you can write this function as C of G of W. I didn't put the X here,

642
01:07:29,840 --> 01:07:37,920
but just for charity. And the derivative of this is, again, you apply chain rule or you can write

643
01:07:38,000 --> 01:07:51,600
it with partial derivatives this way. And same for, you know, expand the dependency of the output

644
01:07:51,600 --> 01:07:58,160
with respect to the parameters as the Jacobian matrix of G with respect to W. If W is a scalar,

645
01:07:58,160 --> 01:08:06,240
then this is just a derivative, partial derivative. Okay, now you can express this as a compute graph.

646
01:08:06,240 --> 01:08:11,920
So you can say, like, how am I going to compute DC over DW? What I'm going to have to do is take

647
01:08:11,920 --> 01:08:15,920
the value one, which is the derivative of C with respect to itself, basically,

648
01:08:15,920 --> 01:08:21,200
the loss with respect to itself. I'm going to multiply this by the derivative of the cost with

649
01:08:21,200 --> 01:08:32,800
respect to Y bar. Okay, and that's going to give me DC over DY bar, obviously. Okay, this is the

650
01:08:32,880 --> 01:08:38,000
same as this because I'm just multiplied by one. Then multiply this by the Jacobian matrix of G

651
01:08:38,000 --> 01:08:43,840
with respect to W, which is a derivative if W is a scalar. That, of course, depends on X.

652
01:08:45,280 --> 01:08:52,000
And I get DC over DW. So this is a so-called compute graph, right? This is a way of organizing

653
01:08:52,000 --> 01:08:58,560
operations to compute the gradient. And there is essentially an automatic way of transforming

654
01:08:58,640 --> 01:09:05,200
a compute graph of this type into a compute graph of this type that computes the gradient

655
01:09:05,200 --> 01:09:11,920
automatically. And this is the magic that happens in the automatic differentiation inside PyTorch and

656
01:09:11,920 --> 01:09:18,000
TensorFlow and other systems. Some systems are pretty smart about this in a sense that

657
01:09:19,120 --> 01:09:24,640
those functions can be fairly complicated. They can involve themselves computing derivatives and

658
01:09:25,600 --> 01:09:30,400
they can involve dynamic computation, where the graph of computation depends on the data.

659
01:09:31,200 --> 01:09:35,920
And actually PyTorch does this properly. I'm not going to go through all the details of this,

660
01:09:35,920 --> 01:09:41,120
but this is kind of a way of reminding you what the dimensions of all those things are, right? So

661
01:09:41,920 --> 01:09:46,160
if Y is a column vector of size M, W is a column vector of size N,

662
01:09:47,600 --> 01:09:51,920
then this is a row vector of size N, this is a row vector of size M, and this is a

663
01:09:51,920 --> 01:09:57,600
geocomium matrix of size N by N. And all of this works out.

664
01:10:01,680 --> 01:10:06,480
Okay, so the way we're going to build neural nets, and I'll come back to this in a

665
01:10:06,480 --> 01:10:15,280
subsequent lecture, is that we are going to have at our disposal a large collection of basic modules

666
01:10:15,280 --> 01:10:21,200
which we're going to be able to arrange in more or less complex graphs

667
01:10:22,400 --> 01:10:30,320
as a way to build the architecture of a learning system. Okay, so either we're going to write a

668
01:10:30,320 --> 01:10:36,640
class or we're going to write a program that runs the forward pass, and this program is going to be

669
01:10:37,440 --> 01:10:44,160
composed of basic mathematical operations, addition, subtraction of tensors or multi-dimensional arrays,

670
01:10:45,840 --> 01:10:50,560
other types of scalar operations, or the application of one of the predefined

671
01:10:51,760 --> 01:10:57,760
complex parameterized functions, like a linear module, a value, or things like that.

672
01:11:00,320 --> 01:11:08,080
And we have at our disposal a large library of such modules, which are things that people have

673
01:11:08,080 --> 01:11:14,640
come up with over the years that are kind of basic modules that are used in a lot of applications.

674
01:11:15,280 --> 01:11:19,280
Right, so the basic things that we've seen so far I think is like values. There's other

675
01:11:19,280 --> 01:11:24,400
nonlinear functions like sigmoids and variations of this. There's a large collection of them.

676
01:11:25,280 --> 01:11:28,800
And then we have cost functions like square error, cross entropy, hinge loss, ranking loss,

677
01:11:28,800 --> 01:11:32,400
and blah, blah, blah, which I'm not going to go through now, but we'll talk about this later.

678
01:11:35,520 --> 01:11:42,880
The nice thing about this formalism is that, as I said before, you can sort of compute

679
01:11:43,440 --> 01:11:52,640
graphs. You can construct a deep learning system by assembling those modules in any kind

680
01:11:52,640 --> 01:11:59,360
of arrangement you want, as long as there is no loops in the connection graph. So as long as

681
01:11:59,920 --> 01:12:04,080
you can come up with a partial order in those modules that will ensure that they are

682
01:12:04,080 --> 01:12:11,040
computed in the proper way. But there is a way to handle loops, and that's called recurrent

683
01:12:11,040 --> 01:12:18,400
nets. We'll talk about this later. Okay, so here's a few practical tricks if you want to

684
01:12:18,400 --> 01:12:23,360
play with neural nets, and you're going to do that soon enough, perhaps even tomorrow.

685
01:12:28,800 --> 01:12:32,960
And these are kind of a bit of a black art of deep learning, which is sort of a lot of it is

686
01:12:32,960 --> 01:12:37,280
implemented already in things like PyTorch if you used under tools, but some of it is kind of more

687
01:12:37,280 --> 01:12:42,000
of the sort of oral culture if you want of the deep learning community. You can find this in

688
01:12:42,000 --> 01:12:49,760
papers, but it's a little difficult to find sometimes. So most neural nets use values as

689
01:12:49,760 --> 01:12:55,280
the main nonlinearity, so this sort of half wave rectifier. Hyperbole tangent, which is a

690
01:12:55,280 --> 01:13:00,160
similar function, and logistic function, which is also a similar function, are used, but not as

691
01:13:00,160 --> 01:13:04,880
much, not nearly as much. You need to initialize the ways properly. So if you have a neural net

692
01:13:04,880 --> 01:13:10,400
and you initialize the ways to zero, it never takes off. It will never learn. The gradients

693
01:13:10,400 --> 01:13:17,200
will always be zero all the time. And the reason is because when you back propagate the gradient,

694
01:13:17,200 --> 01:13:21,600
you multiply by the transpose of the weight matrix. If that weight matrix is zero, your gradient is

695
01:13:21,600 --> 01:13:27,360
zero. So if you start with all the weights equal to zero, you never take off. And someone asked

696
01:13:27,360 --> 01:13:34,640
the question about saddle points before. Zero is a saddle point. And so if you start at this

697
01:13:34,640 --> 01:13:40,000
saddle point, you never get out of it. So you have to break the symmetry in the system. You have to

698
01:13:40,000 --> 01:13:46,800
initialize the weights to small random values. They don't need to be random, but it works fine

699
01:13:46,800 --> 01:13:54,160
if they're random. And the way you initialize is actually quite important. So there's all kinds of

700
01:13:54,160 --> 01:13:59,040
tricks to initialize things properly. One of the tricks was invented by my friend,

701
01:13:59,920 --> 01:14:05,760
about 30 years ago, even more than that, actually, 34 years ago, almost. Unfortunately, now it's

702
01:14:05,760 --> 01:14:11,680
called differently. It's called the kaming trick, but it's the same. And it consists in

703
01:14:12,240 --> 01:14:17,280
initializing the weights to random values in such a way that if a unit has many inputs, the weights

704
01:14:17,280 --> 01:14:22,640
are smaller than if it has few inputs. And the reason for this is that you want the weighted

705
01:14:22,640 --> 01:14:29,680
sum to be roughly kind of have some reasonable value. If the input variables have some reasonable

706
01:14:29,680 --> 01:14:36,160
value, let's say variance one or something like this, and you're computing a weighted sum of them,

707
01:14:36,160 --> 01:14:41,840
the weighted sum, the size of the weighted sum is going to grow like the square root of the number

708
01:14:41,840 --> 01:14:46,880
of inputs. And so you want to set the weights to something like the inverse square root if you want

709
01:14:46,880 --> 01:14:54,160
the weighted sum to be kind of about the same size as each of the inputs. So that's built into

710
01:14:54,160 --> 01:15:00,160
PyTorch. You can call this, you know, initialization procedure. What's the exact name of it? I can't

711
01:15:00,160 --> 01:15:06,160
remember. The one that is coming, coming, coming here, then there is the Xavier and then there is

712
01:15:06,160 --> 01:15:10,800
also yours we have in PyTorch. Yeah, they're slightly different, but they kind of do the same

713
01:15:10,800 --> 01:15:19,120
more or less. Yeah, the Xavier Glow version, yeah. Yeah, this one divides by the Fennin and Fennin.

714
01:15:20,880 --> 01:15:25,520
There's various loss functions, so I haven't talked yet about what the cross-entropy loss is,

715
01:15:25,520 --> 01:15:30,160
but cross-entropy loss is a particular cost that's used for classification. I'll probably

716
01:15:30,160 --> 01:15:35,600
talk about this next week and I'll have some time at the end of this lecture. This is for

717
01:15:35,600 --> 01:15:41,760
classification. As I said, we use stochastic gradient descent on mini-batches and mini-batches

718
01:15:41,760 --> 01:15:46,560
only because the hardware that we have needs mini-batches to perform properly. If we had

719
01:15:46,560 --> 01:15:52,160
different hardware, we would use mini-batch size one. As I said before, we need to shuffle the

720
01:15:52,160 --> 01:15:58,960
training samples. So if someone gives you a training set and puts all the examples of category one,

721
01:15:58,960 --> 01:16:04,080
then all the example category two, all the example category three, etc. If you use stochastic gradient

722
01:16:04,080 --> 01:16:09,760
by keeping this order, it is not going to work. You have to shuffle the samples so that

723
01:16:11,520 --> 01:16:17,360
you basically get samples from all the categories within kind of a small subset, if you want.

724
01:16:19,680 --> 01:16:24,160
There is an objection here for the stochastic gradient. Isn't Adam better?

725
01:16:24,880 --> 01:16:35,920
All right. Okay. There is a lot of variants of stochastic gradient. There are all stochastic

726
01:16:35,920 --> 01:16:42,240
gradient methods. In fact, people in optimization said this should not be called stochastic gradient

727
01:16:42,240 --> 01:16:47,360
descent because it's not a descent algorithm because stochastic gradient sometimes goes uphill

728
01:16:47,360 --> 01:16:55,200
because of the noise. So people who want to really kind of be correct about this say it's

729
01:16:55,200 --> 01:16:58,400
stochastic gradient optimization, but not stochastic gradient descent. That's the first thing.

730
01:16:59,520 --> 01:17:05,280
Stochastic gradient optimization or stochastic gradient descent, SGD, is a special case of gradient

731
01:17:05,280 --> 01:17:13,280
based optimization. The specification of it says you have to have a step size eta,

732
01:17:14,000 --> 01:17:18,720
but nobody tells you how you set this step size eta and nobody tells you that this step size is

733
01:17:18,720 --> 01:17:26,320
a scalar or a diagonal matrix or a full matrix. Okay. So there are variations of SGD in which

734
01:17:27,440 --> 01:17:31,360
eta is changed all the time for every sample or every batch.

735
01:17:34,720 --> 01:17:39,280
In SGD, most of the time this eta is decreased according to a schedule and there are a bunch

736
01:17:39,280 --> 01:17:48,000
of standard schedule in PyTorch that are implemented. In techniques like Adam, the eta is actually a

737
01:17:48,000 --> 01:17:52,160
diagonal matrix and that diagonal matrix, the term in the diagonal matrix are changed all the

738
01:17:52,160 --> 01:17:58,640
time. They're computed based on some estimate of the curvature of the cost function. There's a lot

739
01:17:58,640 --> 01:18:06,720
of methods to do this. Okay. They're all SGD type methods. Okay. Adam is an SGD method with a special

740
01:18:06,720 --> 01:18:16,320
type of eta. So yeah, in the opt-in package in Torch, there's a whole bunch of those methods.

741
01:18:19,040 --> 01:18:23,280
There's going to be a whole lecture on this, so don't worry about it, about optimization.

742
01:18:25,520 --> 01:18:31,920
Normalize the input variables to zero mean and unit variance. So this is a very important point that

743
01:18:32,560 --> 01:18:40,240
this type of optimization method, gradient based optimization methods, when you have weighted

744
01:18:40,240 --> 01:18:46,400
sounds, kind of linear operations, tends to be very sensitive to how the data is prepared.

745
01:18:47,440 --> 01:18:50,400
So if you have two variables that have very widely different

746
01:18:51,360 --> 01:18:56,480
variances, one of them varies between, let's say, minus one and plus one. The other one

747
01:18:56,480 --> 01:19:02,880
varies between minus 100 and plus 100. The system will basically not pay attention to the one that

748
01:19:02,880 --> 01:19:08,240
varies between plus one and minus one. We'll only pay attention to the big one. And this may be good

749
01:19:08,240 --> 01:19:13,360
or this may be bad. Furthermore, the learning rate you're going to have to use the eta parameter,

750
01:19:13,360 --> 01:19:19,120
the step size, is going to have to be set to a relatively small value to prevent the weights

751
01:19:19,120 --> 01:19:27,520
that look at this highly variable input from diverging. The gradients are going to be very

752
01:19:27,520 --> 01:19:31,680
large because the gradients basically are proportional to the size of the input or even to

753
01:19:31,680 --> 01:19:36,000
the variance of the input. So if you don't want your system to diverge, you're going to have to

754
01:19:36,000 --> 01:19:43,520
tune down the learning rate if the input variance is large. If the input variables are all shifted,

755
01:19:43,600 --> 01:19:51,040
they're all between, let's say, 99 and 101 instead of minus one and one. Then again, it's very

756
01:19:51,040 --> 01:19:58,480
difficult for a gradient-based algorithm that use weighted sums to figure out those things.

757
01:19:58,480 --> 01:20:03,760
Again, I'll talk about this more formally later. Right now, just remember the trick

758
01:20:03,760 --> 01:20:07,840
that you need to normalize your input. So basically, take every variable of your input,

759
01:20:07,840 --> 01:20:12,320
subtract the mean, you compute the mean over the training set of each variable. So let's say your

760
01:20:12,320 --> 01:20:18,880
training set is a set of images. The images are, let's say, 100 by 100 pixels. Let's say they're

761
01:20:18,880 --> 01:20:23,760
grayscale, so you get 10,000 variables. And let's say you get a million samples, right? You're going

762
01:20:23,760 --> 01:20:32,080
to take each of those 10,000 variables, compute the mean of it over the training set, compute the

763
01:20:32,080 --> 01:20:38,000
standard deviation of it over the entire training set. And the samples you're going to show to your

764
01:20:38,000 --> 01:20:46,320
system are going to be a sample where you have subtracted the mean from each of the 10,000 pixels

765
01:20:46,320 --> 01:20:54,000
and divided the resulting values by the standard deviation that you computed.

766
01:20:55,360 --> 01:20:58,720
Okay? So now what you have is a bunch of variables that are all zero mean

767
01:20:59,520 --> 01:21:05,680
and all standard deviation equal to one. And that makes your neural net happy.

768
01:21:06,080 --> 01:21:08,880
That makes your optimization algorithm happy, actually.

769
01:21:08,880 --> 01:21:16,000
We have actually a question. So you keep repeating SGD type methods, gradient based methods,

770
01:21:16,000 --> 01:21:22,160
because there are other types of methods. Yes. Okay. So there is gradient free methods.

771
01:21:22,160 --> 01:21:29,040
So gradient free method is a method where you do not assume that the function you're trying to

772
01:21:29,040 --> 01:21:33,920
optimize is differentiable or even continuous with respect to the parameters.

773
01:21:34,640 --> 01:21:42,800
For several reasons, perhaps it's a function that looks like a golf course, right? It's flat and

774
01:21:42,800 --> 01:21:47,360
then maybe it's got steps and, you know, it's difficult to, like the local gradient information

775
01:21:47,360 --> 01:21:52,080
does not give you any information as to where you should go to find the minimum. Okay?

776
01:21:53,520 --> 01:22:00,080
It could be that the function is essentially discrete, right? It's not a function of continuous

777
01:22:00,080 --> 01:22:06,960
variables, function of discrete variables. So for example, am I going to win this chess game?

778
01:22:08,240 --> 01:22:11,760
The variable you can manipulate is the position on the board. That's a discrete variable.

779
01:22:13,280 --> 01:22:20,160
So you can't, you can compute a gradient of, you know, a score with respect to a position on

780
01:22:20,160 --> 01:22:29,120
the chess game. It's a discrete variable. Another example is the cost function is not

781
01:22:29,120 --> 01:22:33,360
something you can compute. You don't actually know the cost function. Okay? So for example,

782
01:22:34,720 --> 01:22:39,280
the only thing you can do is give an input to the cost function and it tells you the cost.

783
01:22:39,920 --> 01:22:43,600
But you can't, you don't know the function. It's not, right? It's not a program on a computer.

784
01:22:43,600 --> 01:22:48,720
You can't backprop a gradient to it. A good example of this is the real world. The real world,

785
01:22:49,840 --> 01:22:55,120
you can think of it as a cost function, right? You learn to ride a bike and you ride your bike

786
01:22:55,120 --> 01:23:04,160
and at some point you fall. The real world does not give you a gradient of that cost function,

787
01:23:04,160 --> 01:23:10,720
which is how much you hurt with respect to your actions. Okay? The only thing you can do is try

788
01:23:10,720 --> 01:23:16,080
something else and see if you get the same result or not. Okay? So what do you do in that case? So

789
01:23:16,080 --> 01:23:21,360
basically now your cost function is a black box. So now you cannot propagate gradient to this

790
01:23:21,360 --> 01:23:26,640
black box. What you have to do is estimate the gradient by perturbing the, what you see to

791
01:23:26,640 --> 01:23:34,080
that black box, right? So, you know, you try something, right? And that something would be a

792
01:23:34,080 --> 01:23:42,240
perturbation of your input to this black box and you see what resulting perturbation occurs

793
01:23:42,240 --> 01:23:47,760
on the black, on the output of the black box, the cost. And now you can estimate whether you,

794
01:23:48,480 --> 01:23:56,400
you know, this modification improved or made the result worse, right? So essentially,

795
01:23:56,400 --> 01:24:01,520
this is like this optimization problem I was telling you about earlier. The gradient based

796
01:24:01,520 --> 01:24:06,560
algorithm is like you are in the mountain, lost in the mountain in a fog, you can't see anything.

797
01:24:07,360 --> 01:24:11,920
But you can estimate the direction of steepest descent, right? You can just look around and you

798
01:24:11,920 --> 01:24:15,680
can tell which is the direction of steepest descent. You just take a step in that direction.

799
01:24:18,080 --> 01:24:24,160
What if you can't see, right? So basically to estimate in which direction the function goes

800
01:24:24,160 --> 01:24:30,720
down, you have to actually take a step, okay? So you take a step in one direction, then you

801
01:24:30,720 --> 01:24:33,920
come back, then you can take a step in the other direction, come back, and then maybe you get an

802
01:24:33,920 --> 01:24:38,480
estimate for where the steepest descent is. Now you can take a step for steepest descent. So this

803
01:24:38,480 --> 01:24:44,160
is estimating the gradient by perturbation instead of by analytic means of back propagating

804
01:24:44,160 --> 01:24:51,200
gradients, okay, computing Jacobians or whatever, partial derivatives. And then there is the second

805
01:24:51,200 --> 01:24:56,880
step of complexity. Let's imagine that the landscape you are in is basically flat everywhere,

806
01:24:56,880 --> 01:25:02,400
except, you know, once in a while there is a step, okay? So taking a small step in one direction

807
01:25:02,400 --> 01:25:06,480
will not give you any information about which direction you have to go to. So there you have

808
01:25:06,480 --> 01:25:12,400
to use other techniques, taking bigger steps, you know, working for a while and seeing if you

809
01:25:13,280 --> 01:25:19,120
fall down the step or not, or go up a step. You know, maybe you can multiply yourself in

810
01:25:19,120 --> 01:25:24,160
sort of 10,000 copies of yourself and then kind of explore the surroundings. And then whenever

811
01:25:24,160 --> 01:25:30,400
someone says, oh, I find a hole, calls everyone to kind of come there, okay? So all those methods

812
01:25:30,400 --> 01:25:36,800
are called gradient-free optimization algorithms. Sometimes they're called zero-th order method.

813
01:25:36,800 --> 01:25:40,320
Why zero-th order? Because first order is when you can compute the derivative.

814
01:25:40,320 --> 01:25:43,680
Zero-th order is when you cannot compute the derivative. You can only compute the function

815
01:25:43,680 --> 01:25:47,920
or get a value for the function. And then you have second order methods that compute not just

816
01:25:47,920 --> 01:25:52,640
the first derivative, but also the second derivative. And they're also gradient-based,

817
01:25:52,640 --> 01:25:56,640
okay, because they need the first derivative as well. But they can accelerate the process by

818
01:25:56,640 --> 01:26:02,640
also computing the second derivative. And Adam is a very simplified form of kind of, you know,

819
01:26:03,920 --> 01:26:08,720
second order method. It's not a second order method, but it has a hint of second order.

820
01:26:08,720 --> 01:26:11,680
Another hint of second order method is what's called conjugate gradient.

821
01:26:13,120 --> 01:26:16,480
It's another class of method called quasi-Newton methods, which are also kind of

822
01:26:17,520 --> 01:26:20,160
using kind of curvature information, if you want, to kind of accelerate.

823
01:26:21,280 --> 01:26:26,480
Many of those are not actually practical for neural net training, but there are some forms that are.

824
01:26:28,000 --> 01:26:33,760
If you're interested in zero-th order optimization, there is a library that is actually produced

825
01:26:34,720 --> 01:26:39,440
by, it's an open source library, which originated at Facebook Research in Paris

826
01:26:40,560 --> 01:26:44,320
by an author called Olivier Tito, but it's really a community effort. There's a lot of

827
01:26:44,320 --> 01:26:49,360
contributors to it. It's called Nevergrad. And it implements a very large number of different

828
01:26:49,360 --> 01:26:54,080
optimization algorithms that do not assume that you have access to the gradient. Okay.

829
01:26:55,200 --> 01:26:58,400
There are genetic algorithms or evolutionary methods. There are

830
01:26:59,360 --> 01:27:05,040
particle swarm optimization. There are perturbation methods. There is all kinds of tricks, right?

831
01:27:05,040 --> 01:27:09,360
I mean, there's a whole catalog of those things. And those sometimes it's unavoidable. You have

832
01:27:09,360 --> 01:27:14,720
to use them because you don't know the cost function. So a very common situation where you

833
01:27:14,720 --> 01:27:20,880
had to use those things is reinforcement learning. So reinforcement learning is basically a situation

834
01:27:20,880 --> 01:27:26,400
where you tell the system, you don't tell the system the correct answer. You only tell the

835
01:27:26,400 --> 01:27:30,960
system whether the answer was good or bad. It's because you give the value of the cost,

836
01:27:30,960 --> 01:27:34,000
but you don't tell the machine where the cost is. So the machine doesn't know where the cost

837
01:27:34,000 --> 01:27:40,400
function is. Okay. And so the machine cannot actually compute the gradient of the cost. And

838
01:27:40,400 --> 01:27:46,480
so it has to use something like a zero-th order method. So what you can do is you can compute

839
01:27:47,200 --> 01:27:50,720
a gradient with respect to the parameters of the overall cost function

840
01:27:51,520 --> 01:27:57,840
by perturbing the parameters. Or what you can do is compute the gradient of the cost function

841
01:27:57,840 --> 01:28:03,760
with respect to the output of your neural net. Okay. Using perturbation. And once you

842
01:28:03,760 --> 01:28:07,120
have this estimate, then you back propagate the gradient through your network using regular

843
01:28:07,120 --> 01:28:12,000
backprop. So that's a combination of estimating the gradient through perturbation for the cost

844
01:28:12,000 --> 01:28:16,720
function because you don't know it, and then backpropagating from there. This is basically

845
01:28:16,720 --> 01:28:22,160
the tactic that was used by the deep line people in sort of the first sort of deep

846
01:28:23,120 --> 01:28:30,400
queue learning type methods. Back to the normalization. Do we normalize the entire dataset

847
01:28:30,400 --> 01:28:41,200
or each batch? It's equivalent. So you normalize each sample, but the variable you're computing

848
01:28:41,200 --> 01:28:45,680
is on the entire training set, right? So you're computing the standard deviation

849
01:28:46,480 --> 01:28:51,280
and the mean over the entire training set. In fact, most of the time you don't even need to do it

850
01:28:51,280 --> 01:28:54,880
over the entire training set because mean and standard deviation converges pretty fast.

851
01:28:56,000 --> 01:29:02,320
So, but you do it over the entire training set, right? And what you get is a constant number,

852
01:29:02,320 --> 01:29:06,320
two constant numbers, a number that you subtract and a number that you should divide

853
01:29:06,320 --> 01:29:13,040
for each component of your input, okay? It's a fixed preprocessing. For a given training set,

854
01:29:13,040 --> 01:29:21,040
you'll have a fixed mean and standard deviation vector.

855
01:29:21,040 --> 01:29:27,760
But maybe we can connect to the other tool, right? The other module, the batch normalization, right?

856
01:29:27,760 --> 01:29:32,080
Okay, we haven't talked about that yet. Yeah, I'm saying that we can perhaps extend

857
01:29:32,720 --> 01:29:38,080
this normalization bit to the both sides, like the whole dataset and the batch itself.

858
01:29:38,080 --> 01:29:43,280
Okay, yes, yes. So, I mean, again, there's going to be a whole lecture on this. But

859
01:29:45,840 --> 01:29:52,080
for the same reason, it's good to have variables, the input that are zero mean and you need variants.

860
01:29:52,080 --> 01:29:56,080
It's also good for the state variables inside the network to basically have zero mean and

861
01:29:56,080 --> 01:30:00,480
you need variants. And so people have come up with various ways of doing normalization

862
01:30:01,360 --> 01:30:07,440
of the variables inside the network so that they approach zero mean and you need variants.

863
01:30:08,960 --> 01:30:14,560
But, and there are many ways to do this. They have two names like batch normalization, like

864
01:30:17,280 --> 01:30:20,560
layer normalization. And the idea goes back a very long time.

865
01:30:21,600 --> 01:30:28,080
Batch norm is kind of a more recent incarnation of it. Let's see, what was I scheduled to decrease

866
01:30:28,080 --> 01:30:34,480
the learning rate? Yeah, as it turns out, for reasons that are still not completely fully understood,

867
01:30:35,360 --> 01:30:41,600
you need to learn fast initially, you need a learning rate of a particular size.

868
01:30:42,720 --> 01:30:46,000
But to get good results in the end, you kind of need to decrease the learning rate to kind of

869
01:30:46,000 --> 01:30:53,040
let the system settle inside of minima. And that requires decreasing the learning rate.

870
01:30:54,000 --> 01:31:00,480
There's various semi-valid theoretical explanations for this, but experimentally,

871
01:31:00,480 --> 01:31:04,240
it's clear you need to do that. And again, there are schedules that are pre-programmed in PyTorch for

872
01:31:04,240 --> 01:31:12,800
this. Use a bit of L1 or L2 regularization on the weights or combination. Yeah, after you've

873
01:31:12,800 --> 01:31:18,160
trained your system for a few epochs, you might want to kind of prune it, eliminate the weights

874
01:31:18,160 --> 01:31:23,840
that are useless, make sure that the weights have their minimum size. And what you do is you add a

875
01:31:23,840 --> 01:31:30,640
term in the cost function that basically shrinks the weights at every iteration. You might know

876
01:31:30,640 --> 01:31:34,720
what L2 and L1 regularization means if you've taken a class in machine learning for large

877
01:31:34,720 --> 01:31:40,240
secret regression or stuff like that. It's very common. But L2 sometimes is called weight decay.

878
01:31:40,960 --> 01:31:49,200
This, again, are pre-programmed in PyTorch. A trick that a lot of people use for large neural

879
01:31:49,200 --> 01:31:56,560
nets is a trick called dropout. Dropout is implemented as kind of a layer in PyTorch. And

880
01:31:56,560 --> 01:32:04,800
what this layer does is that it takes the state of a layer and it randomly picks a certain proportion

881
01:32:04,800 --> 01:32:10,880
of the units and basically sets them to zero. So you can think of it as a mask,

882
01:32:12,160 --> 01:32:17,120
a layer that applies a mask to an input. And the mask is randomly picked at every sample.

883
01:32:18,480 --> 01:32:25,040
And some proportion of the value in the mask are set to zero. Some are set to one. And you

884
01:32:25,040 --> 01:32:30,560
multiply the input by the mask. So only a subset of the units are allowed to speak to the next

885
01:32:30,560 --> 01:32:36,080
layer, essentially. That's called dropout. And the reason for doing this is that it forces the

886
01:32:36,080 --> 01:32:43,520
unit to distribute the information about the input over multiple units instead of kind of

887
01:32:43,520 --> 01:32:49,680
squeezing everything into a small number. And it makes the system more robust. There's some

888
01:32:49,680 --> 01:32:55,680
theoretical arguments for why it does that. Experimentally, if you add this to a large

889
01:32:55,680 --> 01:33:02,160
network, you get better journalization error. You get better performance on the test set.

890
01:33:02,160 --> 01:33:08,960
It's not always necessary, but it helps. Okay, there's lots of tricks and I'll devote a lecture

891
01:33:08,960 --> 01:33:14,240
on this. So I'm not going to go through all of them right now. That requires explaining a bit

892
01:33:14,240 --> 01:33:19,040
more about optimizations. So really, what deep learning is about, like, I told you everything

893
01:33:19,040 --> 01:33:23,040
about deep learning, like the basics of deep learning. What I haven't told you is why we use

894
01:33:23,040 --> 01:33:28,720
deep learning. Okay, and that's basically what I'm going to tell you about now. The motivation

895
01:33:28,720 --> 01:33:33,360
for why is it that we need basically multi-layer neural nets or things of this type.

896
01:33:35,760 --> 01:33:42,960
Okay, so the traditional prototypical model of supervised learning for a very long time

897
01:33:42,960 --> 01:33:50,160
is basically a linear classifier. A linear classifier for a two-class problem is basically a

898
01:33:50,160 --> 01:33:54,800
single unit of the similar type that we talked about earlier. You compute a weighted sum of inputs

899
01:33:55,360 --> 01:34:02,000
at a bias, and you could think of the bias as just another trainable weight whose corresponding

900
01:34:02,000 --> 01:34:07,440
input is equal to one, if you want. And then you pass that through a threshold function,

901
01:34:07,440 --> 01:34:12,320
the sine function, that I put minus one if the weighted sum is below zero and plus one if it's

902
01:34:12,320 --> 01:34:19,920
above zero. Okay, so this basic linear classifier basically partitions the space, the input space

903
01:34:19,920 --> 01:34:27,120
of x's into two half spaces separated by hyperplane. Right, so the equation sum of i, w, i, x, i plus

904
01:34:27,120 --> 01:34:33,120
b equals zero is the surface that separates the category one that is going to produce y bar equal

905
01:34:33,120 --> 01:34:40,000
plus one from category two where y bar equals minus one. Why is it a, why does it divide the

906
01:34:40,000 --> 01:34:44,960
space into two halves? It's because you're computing the dot product of an input vector with a weight

907
01:34:44,960 --> 01:34:52,400
vector. If those two vectors are orthogonal, then the dot product is zero. Okay, b is just an offset.

908
01:34:53,440 --> 01:34:59,520
So the set of points in x space where this dot product is zero is the set of points that are

909
01:34:59,520 --> 01:35:07,760
orthogonal to the vector w. Okay, so in a n-dimensional space, your vector w is a vector,

910
01:35:08,480 --> 01:35:15,040
and the set of x whose dot product with w is zero is a hyperplane. Right, so it's a linear

911
01:35:15,040 --> 01:35:21,760
subspace of dimension n minus one. Okay, and that hyperplane divides the space of dimension n into

912
01:35:21,760 --> 01:35:28,320
halves. So here is the situation in two dimensions. You have two dimensions x1, x2. You have data

913
01:35:28,320 --> 01:35:34,240
points here, the red, the red category and the blue category. And there is a weight vector plus a bias

914
01:35:34,960 --> 01:35:40,400
where the, you know, the intercept here of this sort of green separating line

915
01:35:41,120 --> 01:35:46,800
with x1 is minus b times divided by w1. So that gives you an idea for what w should be.

916
01:35:47,520 --> 01:35:54,720
And the w vector is orthogonal to that separating surface. Okay, so changing b will

917
01:35:54,720 --> 01:35:58,320
change the position and then changing w will change the orientation basically.

918
01:35:58,880 --> 01:36:06,800
Now, what about situations like this where the points are, the red and blue points are not

919
01:36:06,800 --> 01:36:14,320
separable by a hyperplane? That's called a non-linearly separable case. So there you can't use a linear

920
01:36:14,320 --> 01:36:21,600
classifier to separate those. What are we going to do? In fact, there is a theorem that goes back

921
01:36:21,600 --> 01:36:29,040
to 1966 by Tom Kovar, who died recently actually. It was a Stanford that says the probability that

922
01:36:29,040 --> 01:36:36,080
a particular separation of p points is linearly separable in n dimension is close to one when p

923
01:36:36,080 --> 01:36:42,400
is smaller than n, but it's close to zero when p is larger than n. In other words, if you, if you

924
01:36:42,400 --> 01:36:47,520
take an n-dimensional space, you throw p random points in that n-dimensional space, data points,

925
01:36:47,520 --> 01:36:56,320
okay? And you randomly label them blue and red. You ask the question, what is the probability that

926
01:36:56,320 --> 01:37:01,840
that particular dichotomy is linearly separable? I can separate the blue points from the red points

927
01:37:01,840 --> 01:37:07,600
with a hyperplane. And the answer is, if p is less than n, you have a good chance that they

928
01:37:07,600 --> 01:37:12,000
will be separable. If p is larger than n, you basically have no chance that they will. Okay,

929
01:37:12,000 --> 01:37:19,200
so if you have an image classification problem, let's say, and you have tons of examples,

930
01:37:20,720 --> 01:37:25,680
way bigger. So let's say you do n-nist. So n-nist is a dataset of handwritten digits.

931
01:37:25,680 --> 01:37:30,320
The images are 28 by 28 pixels. In fact, the intrinsic dimension is smaller because some

932
01:37:30,320 --> 01:37:36,960
pixels are always zero. And you have 60,000 samples. The probability that those 60,000

933
01:37:36,960 --> 01:37:42,080
samples of, let's say, zeros from everything else or ones from everything else is nearly separable

934
01:37:42,880 --> 01:37:51,440
is basically nil. So, which is why people invented the classical model of pattern

935
01:37:51,440 --> 01:37:58,160
recognition. We consist in taking an input, engineering a feature extractor to produce a

936
01:37:58,160 --> 01:38:03,280
representation in such a way that in that space now, your problem becomes, let's say, linearly

937
01:38:03,280 --> 01:38:07,280
separable if you use a linear classifier or some other separability if you use another type of

938
01:38:07,280 --> 01:38:15,280
classifier. Okay? Now, necessarily, this feature extraction must be nonlinear itself. If the only

939
01:38:15,280 --> 01:38:19,600
thing it does is some affine transformation of the input, it's not going to make a nonlinearly

940
01:38:19,600 --> 01:38:26,480
separable problem into a linear separable one, right? So, necessarily, this feature extractor

941
01:38:26,480 --> 01:38:31,440
has to be nonlinear. This is very important to remember. Okay? A linear preprocessing doesn't

942
01:38:31,440 --> 01:38:36,880
do anything for you, essentially. So, people spend decades in computer vision, for example,

943
01:38:36,880 --> 01:38:42,240
as feature recognition, devising good feature extractors for particular problems. You know,

944
01:38:42,240 --> 01:38:46,720
what features are good to do face recognition, for example, right? Can I do things like detect the

945
01:38:46,720 --> 01:38:50,880
eyes and then measure the ratio between the separation of the eyes with the separation from

946
01:38:50,880 --> 01:38:55,680
the mouth and then, you know, computes a few features like this and then feed that to a classifier

947
01:38:55,680 --> 01:39:02,000
and figure out who the person is. So, most papers, you know, between, let's say, the 1960s

948
01:39:02,000 --> 01:39:10,240
or 70s and the late 2000s or early 2010s in computer vision were essentially about that,

949
01:39:10,240 --> 01:39:16,560
like how you represent images properly. Not all of them, okay? A lot of them for recognition.

950
01:39:18,160 --> 01:39:24,640
And a lot of people kind of devise very sort of generic ways of devising feature extractors.

951
01:39:25,760 --> 01:39:31,520
The basic idea is you just expand the dimension of the representation in a nonlinear way so that now

952
01:39:31,520 --> 01:39:35,440
your number of dimensions is larger than the number of samples. And now your problem has

953
01:39:35,440 --> 01:39:40,000
a chance of becoming linearly separable. So, the ideas that I'm not going to go through,

954
01:39:40,000 --> 01:39:44,000
like space styling, random projection. So, random projection basically is a very simple idea.

955
01:39:44,000 --> 01:39:53,520
You take your input vectors, you multiply them by random matrix, okay? And then you pass the result

956
01:39:53,520 --> 01:39:58,880
through some nonlinear operation, okay? That's called random projection.

957
01:40:00,240 --> 01:40:05,200
And it might make, if the dimension of the output is larger than the dimension of the input,

958
01:40:05,200 --> 01:40:10,480
it might make a nonlinearly separable problem linearly separable. It's very efficient because,

959
01:40:11,440 --> 01:40:16,080
you know, you might need a very large number of those, of this dimension to be able to kind of

960
01:40:16,080 --> 01:40:20,800
do a good job. But it works in certain cases and you don't have to train the first layer,

961
01:40:20,800 --> 01:40:24,880
you basically pick it randomly. And so, the only thing you need to train is a linear classifier

962
01:40:24,880 --> 01:40:29,520
on top. It's polynomial classifiers, which I'll talk about in a minute, in a minute,

963
01:40:29,520 --> 01:40:36,000
radio basis functions and kernel machines. So, those are basically techniques to turn

964
01:40:36,960 --> 01:40:45,040
an input into a representation that then will be essentially classifiable by a simple classifier

965
01:40:45,040 --> 01:40:51,680
like a linear classifier. So, what's a polynomial classifier? A polynomial classifier, basically,

966
01:40:51,680 --> 01:40:56,320
imagine that your input vector has two dimensions. The way you increase the dimensionality of the

967
01:40:56,320 --> 01:41:02,080
representation is that you take each of the input variables, but you also take every product of

968
01:41:02,080 --> 01:41:07,360
pairs of input variables, right? So, now you have a new feature vector, which is composed of x1,

969
01:41:07,360 --> 01:41:13,120
x2, you add one for the bias, and then also x1 times x2, x1 squared and x2 squared.

970
01:41:13,120 --> 01:41:19,840
So, when you do a linear classification in that space, what you're doing really is a quadratic

971
01:41:21,120 --> 01:41:25,760
classification in the original space, right? The surface, the separating surface in the

972
01:41:25,760 --> 01:41:34,080
original space now is a quadratic curve into dimension. In n dimension, it's a quadratic

973
01:41:34,080 --> 01:41:42,240
hypersurface, basically. So, it could be a parabola or ellipse or hyperbola, depending on the

974
01:41:42,240 --> 01:41:46,320
coefficients, right? Now, the problem with this is that it doesn't work very well in high

975
01:41:46,320 --> 01:41:51,040
dimension because the number of features grows with a square of the number of inputs. So, if you

976
01:41:51,040 --> 01:41:57,520
want to apply this to get an ImageNet type image, you know, the resolution is 256 by 256 by 3,

977
01:41:57,520 --> 01:42:02,720
because you have color channels. That's already a high dimension. If you take the cross product of

978
01:42:02,720 --> 01:42:07,200
all of those variables, that's way too large, okay?

979
01:42:08,000 --> 01:42:15,040
So, it's not really practical for high dimensional problems, but it's a trick. Now, here is,

980
01:42:15,920 --> 01:42:21,760
so, super vector machines are basically two-layer networks of kernel machines more generally,

981
01:42:22,400 --> 01:42:29,040
are two-layer systems in which the first layer has as many dimensions as you have training samples.

982
01:42:30,000 --> 01:42:34,640
Okay? So, for each training sample, you create a inner-on, a unit, if you want,

983
01:42:35,520 --> 01:42:40,960
and the role of this unit is to produce a large output if the input vector matches one of the

984
01:42:40,960 --> 01:42:47,120
training samples and a small output if it doesn't, or the other way around. A small output if it

985
01:42:47,120 --> 01:42:51,120
matches, a large output if it doesn't, okay? It doesn't really matter, but it has to be nonlinear.

986
01:42:51,760 --> 01:42:55,600
So, something like, you know, compute the dot product of the input by one of the training

987
01:42:55,600 --> 01:43:01,600
samples and passes through, you know, a negative exponential or a square or something like that.

988
01:43:01,920 --> 01:43:08,000
So, this gives you how much the input vector resembles one of the training samples, and you

989
01:43:08,000 --> 01:43:14,000
do this for every single training samples, okay? And then you train a linear classifier basically

990
01:43:14,000 --> 01:43:19,200
to use those inputs as, you know, as input to a linear classifier. You compute the weight so

991
01:43:19,200 --> 01:43:24,320
that linear classifier is basically as simple as that. There's some regularization involved, okay?

992
01:43:24,880 --> 01:43:30,320
So, essentially, it's kind of a lookup table, right? You have your entire training set as

993
01:43:31,040 --> 01:43:36,640
you know, points in your kind of nuance if you are, if you want for units in your first layer,

994
01:43:36,640 --> 01:43:42,080
and they each indicate how close the current input vector is to them. So, you get some picture

995
01:43:42,080 --> 01:43:46,640
of where the input vector is by basically having the relative position to all of the

996
01:43:46,640 --> 01:43:50,640
training samples, and then using a simple linear operation, you can figure out, like, what's the

997
01:43:50,640 --> 01:43:55,680
correct answer. This works really well for low dimensional problems, the small number of training

998
01:43:56,400 --> 01:44:02,880
samples, but you're not going to do computer vision with it, at least not without, not if X is our pixels,

999
01:44:04,480 --> 01:44:05,920
because it's basically template matching.

1000
01:44:08,640 --> 01:44:15,040
Now, here is a very interesting fact. It's a fact that if you build a two-layer neural net

1001
01:44:15,040 --> 01:44:19,920
on this model, okay? So, let's say a two-layer neural net, you have an input layer, a hidden layer,

1002
01:44:20,880 --> 01:44:27,440
and not specifying the size, and a single output unit, and you ask, what functions can I approximate

1003
01:44:27,440 --> 01:44:33,040
with an architecture of this type? The answer is, you can approximate pretty much any well-behaved

1004
01:44:33,040 --> 01:44:38,240
function as close as you want, as long as you have enough of those units in the middle, okay?

1005
01:44:38,240 --> 01:44:44,240
So, this is a theorem that says that two-layer neural nets are universal approximators. It

1006
01:44:44,240 --> 01:44:49,120
doesn't really matter what nonlinear function you put in the middle. Any nonlinear function will

1007
01:44:49,120 --> 01:44:57,040
do. A two-layer neural net is a universal approximator, and immediately you say, well,

1008
01:44:57,040 --> 01:45:02,160
why do we need multiple layers, then, if we can approximate anything with two layers? And the

1009
01:45:02,160 --> 01:45:07,200
answer is, it's very, very inefficient to try to approximate everything with only two layers,

1010
01:45:07,200 --> 01:45:12,400
because many, many, many interesting functions we're interested in learning cannot be efficiently

1011
01:45:12,400 --> 01:45:17,120
represented by a two-layer system. They can possibly be represented by a two-layer system,

1012
01:45:17,120 --> 01:45:21,360
but the number of fielding units it would require would be so ridiculously large that it's completely

1013
01:45:21,360 --> 01:45:31,120
impractical, okay? So, that's why we need layers. This very simple point is something that took

1014
01:45:31,120 --> 01:45:37,680
about, you know, it took until the, basically, the 2010s for the machine learning and

1015
01:45:37,680 --> 01:45:44,560
computer vision communities to understand, okay? If you understood what I just said,

1016
01:45:44,560 --> 01:45:50,080
you just took a few seconds, so you beat them. There is a last question here before we

1017
01:45:50,080 --> 01:45:55,760
finish class. So, does the depth of the network then have anything to do with generalization?

1018
01:45:57,520 --> 01:46:03,840
Okay, so generalization is a different story, okay? Generalization is very difficult to predict.

1019
01:46:03,840 --> 01:46:08,800
It depends on a lot of things. It depends on the appropriateness of the architecture to the

1020
01:46:08,800 --> 01:46:13,680
problem at hand, okay? So, for example, people use convolutional nets for computer vision,

1021
01:46:13,680 --> 01:46:18,400
they use transformers for text, you know, blah, blah, blah. So, there are certain architectures

1022
01:46:18,400 --> 01:46:26,240
that work well for certain types of data. So, that's the main thing that will improve generalization.

1023
01:46:29,600 --> 01:46:33,920
But generally, yes, multiple layers can improve generalization because

1024
01:46:34,880 --> 01:46:40,240
for a particular function you're interested in learning, computing it with multiple layers

1025
01:46:40,240 --> 01:46:44,000
will allow you to reduce the overall size of the system that will do a good job.

1026
01:46:44,000 --> 01:46:48,240
And so, by reducing the size, you're basically making it easier for the system to find kind of

1027
01:46:48,240 --> 01:46:52,160
good representation. But there is something else which has to do with compositionality.

1028
01:46:52,160 --> 01:46:57,280
I'll come to this in a minute if I have time. Also, the minimum, the, like the,

1029
01:46:57,280 --> 01:47:02,320
how do you call it, the well is like larger, right? If we have overparameterized networks.

1030
01:47:02,320 --> 01:47:06,400
If you're overparameterized network, it's much easier to find a minimum to your objective function,

1031
01:47:06,400 --> 01:47:12,480
right? Which is why neural nets are generally overparameterized. They generally have, like you,

1032
01:47:12,480 --> 01:47:15,200
a much larger number of parameters than what you would think is necessary.

1033
01:47:15,840 --> 01:47:18,960
And when you get them bigger, when you make them bigger, they work better usually.

1034
01:47:18,960 --> 01:47:25,120
It's not always the case, but it's very curious phenomenon about this. We'll talk about this later.

1035
01:47:25,120 --> 01:47:31,600
Okay, this is the one point I want to make. And it's the fact that the reason why we,

1036
01:47:31,600 --> 01:47:37,920
why layers are good is that the world is compositional, the perceptual world in particular,

1037
01:47:37,920 --> 01:47:41,600
but the world in general, the universe, if you want, is compositional. What does that mean? It

1038
01:47:41,600 --> 01:47:47,680
means that, okay, at the level of the universe, right? We have elementary particles, they assemble

1039
01:47:47,680 --> 01:47:52,720
to form less elementary particles, those assemble to form atoms, those assemble to form molecules,

1040
01:47:52,720 --> 01:48:00,080
those assemble to form materials, those assemble to form, you know, structures, objects, etc.

1041
01:48:00,080 --> 01:48:06,240
And, you know, environments, scenes, etc. You have the same kind of hierarchy for images,

1042
01:48:06,240 --> 01:48:11,920
you have pixels, they assemble to form edges and textons and motifs, parts and objects.

1043
01:48:12,640 --> 01:48:16,400
In text, you have characters that assemble to form words, word groups, clauses, sentences,

1044
01:48:16,400 --> 01:48:23,520
stories. In speech, you have speech samples, assemble to form, you know, kind of elementary

1045
01:48:23,520 --> 01:48:30,800
sounds, phones, phonemes, syllables, words, etc. So you have this kind of compositional hierarchy

1046
01:48:30,800 --> 01:48:35,360
in a lot of natural signals. And this is what makes the world understandable, right? This is

1047
01:48:35,360 --> 01:48:39,840
famous quote by Albert Einstein, the most incomprehensible thing about the world is that the

1048
01:48:39,840 --> 01:48:44,080
world is comprehensible. And the reason why the world is comprehensible is because it's compositional,

1049
01:48:44,080 --> 01:48:48,480
because small part assemble to form bigger part, and that allows you to have a description, an

1050
01:48:48,480 --> 01:48:55,760
abstract description of the world in terms of parts from the level immediately below,

1051
01:48:55,760 --> 01:49:00,480
in terms of level of abstraction. So to some extent, the layered architecture in a neural net

1052
01:49:01,600 --> 01:49:08,160
reflects this idea that you have kind of a compositional hierarchy where simple things

1053
01:49:08,160 --> 01:49:13,120
assemble to form slightly more complex things. So images, you have pixels formed to form edges

1054
01:49:13,120 --> 01:49:17,440
that are kind of depicted here. These are actually feature detectors, the visualization of feature

1055
01:49:17,440 --> 01:49:22,480
detectors by a particular convolutional net, which is a particular type of neural net, multilateral

1056
01:49:22,480 --> 01:49:28,160
neural net. So at the low level, you have units that detect oriented edges, a couple layers up,

1057
01:49:28,160 --> 01:49:34,000
you have things that detect simple motifs, circles, gratings, corners, etc. And then a few layers up,

1058
01:49:34,000 --> 01:49:40,560
there are things like parts of objects and things like that. So I think personally that the magic

1059
01:49:41,120 --> 01:49:48,240
of deep learning, the fact that multiple layers help is the fact that the perceptual world is

1060
01:49:48,240 --> 01:49:53,600
basically a compositional hierarchy. And then this end-to-end learning in deep learning allows the

1061
01:49:53,600 --> 01:50:00,720
system to learn hierarchical representations where each layer learns a representation that has a

1062
01:50:00,720 --> 01:50:05,200
level of abstraction slightly higher than the previous one. So low level, you have individual

1063
01:50:05,200 --> 01:50:10,080
pixels, then you have the presence or absence of an edge, then you have the presence or absence of

1064
01:50:10,080 --> 01:50:14,560
a part of an object, and then you have the presence or absence of an object independently of

1065
01:50:15,840 --> 01:50:19,920
the position of that object, the illumination, the color, the occlusions, the background,

1066
01:50:19,920 --> 01:50:26,640
you know, things like that, right? So that's the motivation, the idea why deep learning is so

1067
01:50:26,640 --> 01:50:32,880
successful and why it's basically taken over the world over the last 10 years or so. All right,

1068
01:50:32,880 --> 01:50:37,200
thank you for your attention. That's great. So for tomorrow guys, don't forget to try to go over

1069
01:50:37,200 --> 01:50:47,280
the 01 tutorial tensor, sorry, the 01 notebook that we have on the website such that we can get,

1070
01:50:47,280 --> 01:50:51,840
like, you know, all on the same level for the ones that are not really familiar with NumPy stuff,

1071
01:50:51,840 --> 01:51:04,480
okay? So otherwise, let's see you tomorrow morning and have a nice day. Take care everyone. Bye-bye.

