{"text": " So as you know, we're going to talk about deep learning and we're going to jump right in. So much of practical applications of deep learning today, machine learning and AI in general, are used a paradigm called supervised learning, which I'm sure most of you have heard of before. So this is the paradigm by which you train a machine by showing it examples of inputs and outputs. You want to build a machine to distinguish images of cars from airplanes, you show it an image of a car. If the machine says car, you don't do anything. If it says something else, you adjust the internal parameters of the system so that the output gets closer to the one you want. So imagine the target output is some vector of activities on a set of outputs. You want the vector coming out of the machine to get closer to the vector that is the desired output. And this works really well. As long as you have lots of data, it works for speech recognition, image recognition, face recognition, generating captions, translation, all kinds of stuff. So this is, I would say, 95% of all applications of machine learning today. There are two other paradigms, one of which I will not talk about, one of which I will talk about a lot. So the two other paradigms are reinforcement learning, which I will not talk about. And there are other courses. There's a course by Larry Pinto about this that I encourage you to take. And a third paradigm is self-supervised learning or unsupervised learning. And we'll talk about this quite a lot in the following weeks. But for now, let's talk about supervised learning. Self-supervised learning, you could think of it as kind of a play on supervised learning. So the traditional model of pattern recognition machine learning and supervised learning, certainly going back to the late 50s or the 60s, is the idea by which you take a raw signal, let's say an image or an audio signal or a set of features representing an object, and then you turn it into a representation using a feature extractor, which in the past was engineered. And then you take that representation, which is generally in the form of a vector or a table of numbers or some kind of tensor, a multi-dimensional array. But sometimes, could be a different type of representation. And you feed that to a trainable classifier. So this is the learning where the learning takes part. This is the classical model, and it's still popular. It's still used a lot. But basically, what deep learning has done is replace this sort of manual hand engineering of the feature extractor by a stack of trainable modules, if you want. So in deep learning, the main idea of deep learning, and the only reason why it's called deep, is that we stack a bunch of modules, each of which transforms the input a little bit into something that's going to slightly higher level of abstraction, if you want. And then we train the entire system end to end. So I represented those sort of pinkish modules to indicate the ones that are trainable, and the blue modules are the fixed ones, the hand engineered ones. So that's why deep learning is called deep. We stack multiple layers of trainable things, and we train it end to end. The idea for this goes back a long time. The practical methods for this go back to the mid to late 80s, with the back propagation algorithm, which is going to be the main subject of today's lecture, actually. But it took a long time for this idea to actually percolate and become the main tool that people use to build machine learning system. It's only about 10 years old. Okay, so let's go through a few definitions. So we're going to deal with parameterized models, a parameterized model, or learning model, if you want, is a parameterized function, g of x and w, where x is the input, and w is a set of parameters. I'm representing this here on the right with a particular symbolism, where a function like this that produces a single output, think of the output as either a vector or matrix or a tensor, or perhaps even a scalar, but generally is multidimensional. It can actually be something else in a multidimensional array, but something that, you know, maybe like a sparse array representation or a graph with values on it. But for now, let's think of it just as a multidimensional array. So both the inputs and the outputs are multidimensional arrays, what people call tensors. It's not really kind of the appropriate definition of tensor, but it's okay. And that function is parameterized by a set of parameters w. Those are the knobs that we're going to adjust during training, and they basically determine the input-output relationship between you know, between the input x and the predicted output y bar. Okay, so I'm not explicitly representing the wire that comes in with w. Here, I kind of assume that w is somewhere inside of this module. Think of this as an object in object-oriented programming. So it's an instance of a class that you instantiated and it's got a slot in it that represents the parameters, and there is a forward function basically that takes as argument the input and returns the output. Okay, so a basic running machine will have a cost function and the cost function in supervised running, but also in some other settings will basically compute the discrepancy, distance, divergence, whatever you want to call it, between the desired output y, which is given to you from the training set, and the output produced by the system y bar. Okay, so an example of this, a very simple example of a setting like this is linear regression. In linear regression, x is a vector composed of components x i's, w is also a vector, and the output is a scalar that is simply the dot product of x with w. So y bar now is a scalar, and what you compute is the square distance, the square difference really between y and y bar. If w is a matrix, then now y is a vector, and you compute the square norm of the difference between y and y bar, and that's basically linear regression. So learning will consist in finding the set of w's that minimize this particular cost function average over a training set. I'll come to this in a minute, but I want you to think right now about the fact that this g function may not be something particularly simple to compute. So it may not be just multiplying a vector by a matrix. It may not be just carrying some sort of fixed computation with sort of a fixed number of steps. It could involve something complicated. It could involve minimizing a function with respect to some other variable that you don't know. It could involve a lot of iteration of some algorithm that converges towards a fixed point. So let's not restrict ourselves to g of x w that are simple things. It could be very complicated things, and we'll come to this in a few weeks. So this is just to explain the notations that I will use during the course of this class. So we have observed input and desired output variables. Those are kind of gray grayish bubbles. Other variables that are produced by the system or internal to the system are those kind of, you know, empty circle variables. We have determinacy functions or functions that are so they are indicated by this sort of rounded shape here. They can take multiple inputs have multiple outputs. And each of those can be tensors or scalars or whatever. And they have implicit parameters that are tunable by training. And then we have cost functions. So cost functions are basically functions that take one or multiple inputs and output a scalar. But I'm not representing the output is implicit. Okay, so if you have a red square, it has an implicit output. And it's a scalar and we interpret it as a cost or an energy function. So this symbolism is kind of similar to what people use in graphical models. If you if you've heard what a graphical model is, particularly the type of graphical model called a factor graph. So in a factor graph, you have those variable bubbles, and you have those factors, which are those square cost functions. You don't have this idea that you had deterministic functions in it, because graphical models don't care about the fact that you have functions in one direction or another. But here we care about it. So with this extra symbol. Okay, so machine learning consists in basically minimizing finding the set of parameters W that minimize the cost function averaged over a training set. So a training set is a set of pairs x, x, y indexed by an index P. Okay, so we have P training samples. And it'll be the index of the training set the training sample. And our overall last function that we're going to have to minimize is the, you know, is equal to the cost of the discrepancy between the y and the output of our model by bar g of x, w, as I said earlier. So L is a value, C is a C is a module and L is a is a is a way of writing C of y, g of x, w, you know, whether it depends explicitly on x, y and w. Okay, but it's the same thing really. The overall last function, which is this kind of curly L is the average of the per sample loss function over the entire training set. Okay, so compute L for the entire training set, divide by some all the terms divide by P, and that's the average. That's the loss. Okay, so now the name of the game is trying to find the minimum of that loss with respect to the parameters. This is an optimization problem. So symbolically, I can represent this entire graph as the thing on the right. This is rarely used in practice, but this is sort of a way to visualize this. So think about each training sample as a sort of identical copy of the replica, if you want, of the model and the cost function applied to a different training sample, and then there is an average operation that computes the loss, right? So everything you can write as a formula, you can probably write in terms of those graphs. This is going to be very useful as we're going to see later. Okay, so supervised machine learning and a lot of other machine learning patterns as well actually are can be viewed as function optimization and a very simple approach to optimizing a function, which means finding the set of parameters to a function that minimize its value, okay, is gradient descent or gradient based algorithms. So gradient based algorithm makes the assumption that the function is somewhat smooth and mostly differentiable, doesn't have to be everywhere differentiable, but has to be continuous, has to be almost everywhere differentiable. And it has to be somewhat smooth, otherwise, the local information of the slope doesn't tell you much about where the minimum is. Okay, so here's an example here depicted on the right. The lines that you see here, the pink lines are the lines of equal cost and this cost is quadratic, so it's basically a kind of paraboloid. And this is the trajectory of a method called stochastic gradient descent, which we'll talk about in a minute. So for stochastic gradient descent, the procedure is you show an example, you run you through the machine, you compute the objective for that particular sample, and then you figure out by how much and how to modify each of the knobs in the machine, the W parameters, so that the objective function goes down by a little bit, you make that change, and then you go to the next sample. Let's be a little more formal. So gradient descent is this very basic algorithm here, you replace the value of W by its previous value minus a step size, eta here, multiplied by the gradient of the objective function with respect to the parameters. So what is a gradient? A gradient is a vector of the same size as the parameter vector. And for each component of the parameter vector, it tells you by how much the the loss function L would increase if you increase the parameter by a tiny amount. Okay, it's a derivative, but it's a directional derivative, right? So let's say among all the directions, you only look at W34. And as you imagine that you tweak W34 by a tiny amount, the loss function curly L is going to increase by a tiny amount, you divide the tiny amount by which L increase by the tiny amount that you modified this W34. And what you get is the gradient of the loss with respect to W34. If you do this for every single weight, you get the gradient of the loss function with respect to all the weights. And it's a vector, which for each component of the weight gives you the parameter gives you that quantity. Okay, so, you know, since Newton and earlier, it's been written as, you know, dL over dW, because it indicates the fact that there is this little twiddle where you can twiddle W by little. And there's a resulting twiddling of L. And if you divide those two twiddles, and they are infinitely small, you get the derivative that's kind of standard notation in mathematics for a few hundred years. Okay, so now the gradient is going to be a vector. Okay. And as indicated here on the top, right, that vector is an arrow that points upwards along the line of larger slope. Okay, so if you are in a 2D surface, you have two W parameters. Okay, and the surface is represented here, some sort of quadratic ball here in this case. So it's a second degree polynomial in W1 and W0. Here on the right is the kind of a top-down view of this where the lines represent the lines of equal cost. The little arrow is here, represent the gradient at various locations. Okay, so you have a long arrow if the slope is steep, a short arrow if the slope is not steep, not large. At the bottom, it's zero. And it points towards the direction of higher slope. All right, so imagine you are in a landscape, a mountainous landscape, and you're in a fog and you want to go down the valley. You look around you and you can tell the local slope of the landscape. You can't tell where the minimum is because you're in a fog, but you can tell the local slope. So you can figure out what is the direction of larger slope and then take a step and it will take you upwards. Now you turn around 180 degrees, take a step in that direction, and that is going to take you downwards. If you keep doing this and the landscape is convex, which means it has only one local minimum, this will eventually take you down to the valley and presumably to the village. Right, so that's gradient-based algorithms. They all differ by how you compute the gradient first and by what this eta step-size parameter is. So in simple forms, eta is just a positive constant that sometimes is decreased as the system learns more, but most of the time not. But in more complex versions of gradient-based learning, eta is actually an entire matrix itself, generally a positive definite or semi-definite matrix. And so the direction adopted by those algorithms is not necessarily the steepest descent. It goes downwards, but it's not necessarily the steepest descent. And we can see why here. So in this diagram here that I'm showing, this is a trajectory that will be followed by gradient descent in this quadratic cost environment. And as you see, the trajectory is not straight. It's not straight because the system goes down by following the slope of steepest descent. And so it goes down the valley before finding the minimum of the valley, if you want. So if your cost function is a little squeezed in one direction, it will go down the ravine and then follow the ravine towards the bottom. In complex situations where you have things that are, the trajectory actually is being cut here. But when the weather function is highly irregular, this might even be more complicated. And then you might have to be smart about what you do here. Okay. So stochastic gradient descent is universally used in deep learning. And this is a slight modification of the gradient steepest descent algorithm where you don't compute the gradient of the entire objective function averaged over all the samples. But what you do is you take one sample and you compute the gradient of the objective function for that one sample with respect to the parameters and you take a step. Okay. And you keep doing this. You pick another sample, compute the gradient of the objective function for that sample with respect to the way it's making a date. Why is it called stochastic gradient? Stochastic is a fancy term for random, essentially. And it's called stochastic because the evaluation of the gradient you get on the basis of a single sample is a noisy estimate of the full gradient. The average of the gradients, because the gradient is a linear operation, the average of the gradients will be the gradient of the average. And so things work out. If you compute the gradient and you kind of keep going, overall, the average trajectory will be sort of the trajectory you would have followed by doing full gradient. Okay. But in fact, the reason we're doing this is because it's much more efficient in terms of speed of convergence. So although the trajectory followed by stochastic gradient is very noisy, things kind of bounce around a lot. As you can see in the trajectory here at the bottom, you know, things have, the trajectory is very erratic. But in fact, it goes to the bottom faster and has other advantages that people are still writing papers on. Okay. The reason for that is that stochastic gradient exploits the redundancy between the samples. So all the, you know, machine learning setting, the training samples have some similarities between them. If they don't, then basically the learning problem is impossible. So they necessarily do have some redundancy between them. And the faster you update the parameters, the more you, the more often you update them, the more you exploit this redundancy between those parameters. Now in practice, what people do is they use mini batches. So instead of computing the gradient on the basis of a single sample, you take a batch of samples, typically anywhere between let's say 30 and a few thousand. But smaller batches are better in most cases actually. And you compute the average of the gradient over those samples. Okay. So compute the average cost over those samples and compute the gradient of the average over those samples and then make an update. The reason for doing this is not intrinsically an algorithmic reason. It's because it's a simple way of parallelizing stochastic gradients on parallel hardware such as GPUs. Okay. So there's never, there's no good reason to do batching other than the fact that our hardware likes it. Okay. Question. Yeah. So for actually for, for real complex deep learning problems, does this objecting function have to be continuously differentiable? Well, it needs to be continuous mostly. If it's non continuous, you're going to get in trouble. It needs to be differentiable almost everywhere. But in fact, neural nets that most people use are actually not differentiable. And there's a lot of places where they're not differentiable. But they are continuous in the sense that there are functions that have kind of corners in them, if you want. They have kinks. And if you have a kink once in a while, it's not too much of a problem. But so in that case, those quantities should not be called gradients, they should be called subgradients. Okay. So a sub gradient is basically a generalization of the idea of derivative or gradient to functions that have kinks in them. So wherever you have a function that has a kink in it, any, any slope that is between the slope of one, one side and the slope of the other side is a, is a valid sub gradient. Okay. So when you write the kink, you decide, well, the derivative is this or it's that or it's going to somewhere in between. And you're fine. Most of the proof that applied to, you know, smooth functions, you know, in terms of minimization, often apply also to non-smooth function that basically are differentiable most of the way. So then how do we ensure strict convexity? We do not ensure strict convexity. The, in fact, in deep learning systems, most deep learning systems, the function that we are optimizing is non-convex, right? In fact, this is one reason why it took so long for deep learning to become prominent is because a lot of people, particularly theoreticians, people who sort of theoretically minded, were very scared of the idea that you had to minimize a non-convex objective and they say, this can't possibly work because we can't prove anything about it. It turns out it does work. You can't prove anything about it, but it does work. And so this is a situation, and it's an interesting thing to think about, a situation where the, the theoretical thinking basically limited what people could do in terms of engineering because they couldn't prove things about it. But that would be actually very powerful. Okay. Yeah. Like your colleague, you optimize non-convex functions. Like your colleague at the Bell Labs, who didn't like the non-mathy. Oh, it was a whole debate, you know, in the machine learning community that lasted 20 years, basically. All right. So what about how doesn't SGD get stuck in local minima once it reaches them? It does. Okay. So, so full gradient does get stuck in local minima, right? SGD gets like, you know, it's slightly less stuck in local minima because it's noisy. It allows it sometimes to escape local minima. But the real reason why we're going to optimize non-convex functions and local minima are not going to be such a huge problem is that there aren't that many local minima that are traps. Okay. So we're going to build neural nets, and those neural nets are, or deep running systems, and they're going to be built in such a way that the, the parameter space is such a, such a high dimension that is going to be very hard for the system to actually create local minima for us. Okay. So think about a picture where we have in one dimension a cost function that has one local minima and then a global minimum, right? Okay. So it's a function like this, right? And we start from here. If we optimize using gradient descent, we're going to get stuck in that local minimum. Now, let's imagine that we parameterize this function now with two parameters. Okay. So we're not a one dimensional, we're not looking at a one dimensional function anymore. We're looking at two dimensional function. We have an extra parameter. This extra parameter will allow us to go around the mountain and go towards the valley, perhaps without having to climb the little hill in the middle. Okay. So this is just an intuitive example to tell you that in very high dimensional spaces, you may not have as much of a local minimum problem as you have in the sort of intuitive picture of low dimensional spaces, right? So here that those pictures are in two dimensions. They are very misleading. We're going to be working with millions of dimensions and you know, some of the most recent deep learning systems have trillions of problems. Yeah. So local minima is not going to be that much of a problem. We're going to have other problems, but not that one. So there is like a trend in this hyper like over parameterization, right? Like it seems like that more neurons we have and the better these networks work somehow. That's right. So we're going to make those networks very large and they're going to be over parameterized, which means they're going to have way more adjustable parameters than we would actually need, which means they're going to be able to learn the training set almost perfectly. And the big question is how well are they going to work on a separate validation set or test set that is separate from the training set? Two more questions. They're going to work in a real situation where, you know, the distribution of samples may be different from what we trained it on. So that's the real question of machine learning, which I'm sure a lot of you are familiar with. Two more questions. Can we do? Yeah. So how do we escape instead of subtle points? Right. So there are tons and tons of subtle points in deep learning systems. A combinatorially large number of subtle points, as a matter of fact. I'll have a lecture on this. So I don't want to kind of spend too long answering. Okay. But yeah, there are subtle points. The trick with subtle points is you don't want to get too close to them, essentially. And stochastic gradient helps a little bit with subtle points. Some people are proposed sort of explicit methods to stay away from subtle points. But in practice, doesn't seem to be that much of a problem, actually. Finally, how do you pick samples for stochastic gradient in the center randomly? Okay. There is lots of different methods for that. Okay. Yeah. I mean, the basic thing you should do is you have your training set. You shuffle the samples in a random order. Okay. And then you just pick them one at a time. And then you cycle through them. An alternative is once you get to the end, you reshuffle them and then cycle through them again. An alternative is you pick a random sample using a random number. Every time you pick a new sample, you pick them randomly. If you do batching, a good idea is to put in a batch samples that are maximally different from each other. So things that are, for example, different categories if you do classification. But most people just do them, you know, just pick them randomly. But it's good to have samples that are maximally different that are nearby either in a batch or during the processor training. And then there are all kinds of tricks that people use to sort of emphasize difficult samples so that the boring, easy samples are not, you don't waste your time just, you know, seeing them over and over again. It's all kinds of tricks. All right. But, you know, the simpler one is, which most people use, you shuffle your samples and you run through them. Most people now use also data augmentation. So every sample is actually distorted by some process. For an image, you can distort the geometry a little bit, you change the colors, you add noise, et cetera. This is an artificial way of sort of adding more samples than you actually have. And people do this kind of randomly on the fly or they kind of precompute those those transformations. So lots of tricks there as well. Last question. How do you pick the batch size? The best. The batch, batch size. Oh, the batch size. That's determined by your hardware. So if you have a GPU, generally for, you know, reasonably sized networks, your batch size would be anywhere between 16 and 64 or something like that. For smaller networks, you might have to batch more to kind of exploit your, your hardware better to kind of have maximum usage of it. If you parallelize on multiple GPUs within a machine, you may have to, to have, you know, so let's say you have eight GPUs, then you'll be sort of eight times 32. So there's no 256 or something. And then, you know, a lot of the big guys kind of parallelize that over multiple machines, each of which has eight GPUs. Some of them have TPUs, whatever. And then you might have to parallelize over thousands of examples. This diminishing return in doing this, when you increase the size of the batch, you actually reduce the, the, the speed of convergence. You accelerate the calculation, but you reduce the speed of convergence. So at some point, it's not worth increasing your batch size. So if we are doing a classification problem with k classes, what's going to be like our go to batch size? So there are papers that say if your batch size is significantly larger than the number of categories, or let's say twice the number of categories, then you're, you're probably wasting competition, essentially. Okay. I mean, throwing down convergence. So you're trying to train an image recognizer on ImageNet. If your batch size is larger than about a thousand, you're probably wasting time. Okay, that's it. Thanks. I mean, you're wasting competition. You're not wasting time. Okay. Okay. Okay. So let's talk about traditional neural net. So a traditional neural net is a, a, a model, a particular type of parameterized function, which is built by stacking linear and nonlinear operations. Right. So here is this kind of a depiction of a traditional neural net here in this case, with two layers, but I'm, you know, I'm not imagining there might be more layers here. So you have a bunch of inputs here on the left. Each input is multiplied by a weight, different weights, presumably. And those, the weighted sum of those inputs by those weights is, is computed here by what's called a unit or neuron. People don't like using the word neuron in that context, because there are incredibly simplified models of neurons in the brain, but, but that's the inspiration really. Okay. So one of those units just computes a weighted sum of its inputs, using those weights. Okay, this unit use, computes a different weighted sum of the same inputs with different weights and etc. So here we have three units here in the first layer. This is called a hidden layer, by the way, because it's neither an input nor an output, right? This is the input, and this is the output, and this is somewhere in the middle. So we compute those weighted sums, and then we pass those weighted sums individually through a, a nonlinear function. So here what I've shown is the value function. So this is called rectified linear unit. In the, this is the name that people have given it in the neural net lingual. In other contexts, this is called a half wave rectifier, if you're an engineer. It's called positive part, if you are a mathematician. Okay. Basically, it's a function that is equal to the identity when its argument is positive, and it's equal to zero if its argument is negative. Okay. So very simple graph. And then we stack a second layer of the same thing, the second stage, right? So again, a layer of linear operations where we compute weighted sums, and then we pass a result to nonlinearities. And we can stack many of those layers, and that's basically a traditional plain vanilla garden variety neural net. In this case, fully connected. So fully connected neural net means that every unit in one layer is connected to every unit in the next layer. And you have this sort of well organized layer, or architecture, if you want, right? Each of those weights are going to be the things that our learning algorithm is going to, is going to tune. And the big trick, the one trick really of deep learning is how we compute those gradients. Okay. So if you want, if you want to write this, you can say the weighted sum number i, so you can give a number to each of the units in the network. So this unit with number i, and the weighted sum s of i, is simply the sum where j goes over the upstream, the set of upstream units to i, which may be all the units in the previous layer or not could be just a subset. Okay. And then you compute the product of zj, which is the output of the unit number j times wij, which is the weight that links unit j to unit i. Okay. And then after that, you take this si, which is the weighted sum, you pass it through the activation function, this value, or whatever it is that you use, and that gives you zi, which is the activation for unit i. Okay. Super notation. By changing the set of upstream units of every unit, by building a graph of interconnection, you can basically build any kind of network arrangement that you want. There is one constraint that we can lift, that we will lift in the subsequent lecture, which is that the graph has to be ac-click in the sense that it can't have loops. Okay. If you have loops, that means you can't organize the units in layers. You can't sort of number them in a way that you can compute them so that every time you want to compute a unit, you already have the state of the previous units. If there are loops, then you can do that. Right? So for now, we're going to assume that the wij matrix, the w matrix, doesn't have loops, represents a graph that doesn't have loops. That's why I should say. Okay. So here's sort of an intuitive explanation of the back propagation algorithm. So the back propagation algorithm is the main technique that is used everywhere in deep learning to compute the gradient of a cost function, whatever it is, objective function, with respect to a variable inside of the network. This variable can be a state variable like a z or an s, or it could be a parameter variable like a w. Okay. And we're going to need to do both. Okay. So this is going to be an intuitive explanation. And then after that, there's going to be a more mathematical explanation, which is less intuitive, but perhaps actually easier to understand. But let me start with the intuition here. So let's say we have a big network. And inside of this big network, we have one of those little activation functions. Okay. In this case, it's a sigmoid function, but doesn't matter what it is for now. Okay. This function takes an s and produces a z. We call this function h of s, right? So when we wiggle z, the cost is going to wiggle by some quantity, right? And we divide the wiggling of z by the wiggling of c by the wiggling of z that causes it. That gives us the partial derivative of c with respect to z. So this one term is a gradient of c with respect to all the z's in the network. And there's one component of that gradient, which is the partial derivative of the cost with respect to that single variable z inside the network. Okay. And that really indicates how much c would wiggle if we wiggled z by some amount. We divide the wiggling of c by the wiggling of z and that gives us the partial derivative of c with respect to z. This is not how we're going to compute the gradient of c with respect to z, but this is a description of what it is conceptually. Okay. Or intuitively, rather. Okay. So let's assume that we know this quantity. So we know the partial derivative of c with respect to z. Okay. So c with respect to z is this quantity here, dc over dz. Okay. So think of dz as the wiggling of z and dc as the wiggling of c, divide one by the other, and you get the partial derivative of c with respect to z. What we have here is, what we have to apply is the chain rule, the rule that tells us how to compute the derivative of a function composed of two individual functions that we apply one after the other. Right. So remember, chain rule, if you have a function g, then you apply to another function h, which is function of parameter s, and you want the derivative of it. The derivative of that is equal to the derivative of g at point h of s, multiplied by the derivative of h at point s. Right. That's chain rule. You know that a few years ago, hopefully. Now, if I want to write this in terms of partial derivative, it's the same thing, right? Partial derivative is just a derivative just with respect to one single variable. So I would write this something like this, dc over ds. So c really is the result of applying this h function to s, and then applying some unknown g function to compute c, which is kind of the rest of the network plus the cost. But I'm just going to call the gradient. I'm going to assume that this dc over dz is known. Someone gave it to me. So this variable here on the right, dc over dz is given to me, and I want to compute dc over ds. So what I need to do is write this, dc over ds equal dc over dz times dz over ds. Right. And why is this identity true? It's because I can simplify by dz. It's as simple as this, right? So you have, you know, trivial algebra, you have dz at the denominator here, dz at the numerator here, simplify, you get dc over ds. It's a very trivial, simple identity, which is basically just generally applied to partial derivatives. Now, dz over ds, we know what it is. It's just h prime of s, just the derivative of the h function. So we have this formula, dc over ds equal dc over dz, which we assume is known, times h prime of s. What does that mean? That means that if we have this component of the gradient of the cost function with respect to z here, we multiply this by the derivative of the h function at point s, the same point s that we had here. And what we get now is the gradient of the cost function with respect to s. Now, here's the trick. If we had a chain of those h functions, we could keep propagating this gradient backwards by just multiplying by the derivative of all those h functions going backwards. And that's why it's called back propagation. So it's just a practical application of a chain rule. And if you want to convince yourself of this, you can run through this idea of perturbation. If I twiddle s by some value, it's going to twiddle z by some value equal to ds times h prime of s, basically the slope of s. So dz equals h prime of s times ds. And then I'm going to have to multiply this by dc over dz. And so I rearrange the terms and I get immediately that this formula dc over ds equals dc over dz times h prime of s. So we had another element in our multilayer net, which was the linear sum. And there, it's just a little bit more complicated, but not really. So one particular variable z here, we would like to compute the derivative, the partial derivative of our cost function with respect to that z. And we're going to assume that we know the partial derivative of s with respect to each of those s's, the weighted sums at the next layer that z is going into. So z only influences c through those s's. So presumably, by basically multiplying how each of those s's influence c and then multiplying by how z influences each of the s's and summing up, we're going to get the influence of z over c. Right? And that's the basic idea. Okay, so here's what we're going to do. Let's say we perturb z by dz. This is going to perturb s0 by dz times w0. Okay, we multiply z by w0. So the derivative of this linear operation is the coefficient itself. Right? So here, the perturbation is ds0 is equal to dz times w0. Okay? And now in turn, this is going to modify c, and we're going to multiply this quantity by dc over ds0 to get the dc, if you want. Okay? Now, whenever we perturb z, it's not going to perturb just s0, it's also going to perturb s1 and s2. And to see the effect on c, we're going to have to sum up the effect of the perturbation on each of the s's and then sum them up to see the overall effect on c. So this is written here on the left. The perturbation of c is equal to the perturbation of s multiplied by the partial derivative of c with respect to s plus the perturbation of s1 multiplied by the partial derivative of dc with respect to s1 plus same thing for s2. Okay? So this is the fact that, you know, we need to take into account all the perturbations here that z may influence. And so I can just write down now a very simple thing, you know, because dc of 0 is equal to w0 times dz and, you know, ds of 2 is w2 times dz, I can plug this in there and just write dc over dz equal dc over ds0, which I assume is known, times w0 plus dc over ds1 times w1 plus dc over ds2 times w2. Okay? If I want to represent this operation graphically, this is shown on the right here. I have dc over ds0, dc over ds1, dc over ds2, which I assume are known or given to me somehow. I compute dc over ds0 multiplied by w0 and multiply dc over ds1 by w1, dc over ds2 by w2. I sum them up and that gives me dc over dz. Okay? It's just the formula here. Okay? So here's the cool trick about back propagation through a linear module that computes weighted sums. You take the same weights and you still compute weighted sum with those weights, but you use the weights backwards. Okay? So whenever you had the unit that was sending its output to multiple outputs to multiple units through a weight, you take the gradient of the cost with respect to all those weighted sums and you compute their weighted sum backwards using the weights backwards to get the gradient with respect to the state of the unit at the bottom. You can do this for all the units. Okay? So it's super simple. Now, if you were to write a program to do backprop for classical neural nets in Python, it would take like half a page. It's very, very simple. Is one function to compute weighted sums going forward in the right order? Another function and applying the nonlinearity is another function to compute weighted sums weighted sums going backward and multiplying by the derivative of the nonlinearity at every step. Right? It's incredibly simple. What's surprising is that it took so long for people to realize this was so useful, maybe because it was too simple. Okay? So it's useful to write this in matrix form. So really, the way you should think about a neural net of this type is each state inside the network, think of it as a vector. It could be a multidimensional array, but let's think of it just as a vector. A linear operation is just going to multiply this vector by matrix and each row of the matrix contains all the weights that are used to compute a particular weighted sum for a particular unit. Okay? So multiply this by this matrix. So this dimension has to be equal to that dimension, which is not really well depicted here, actually. One sec. From the previous slide, you wrote ds0. What is s, differentiated with respect to? So there is a ds. What is ds, basically? ds0, you mean? Yeah. Okay. ds0 is a perturbation of s0. Okay? An infinitely small perturbation of s0. Doesn't matter what it is. Okay? And what we're saying here is that if you have an infinitely small perturbation of s0, and you multiply this perturbation by the partial derivative of c with respect to s0, okay? You get the perturbation of c, except that that corresponds to this perturbation of s0, right? But we're not interested in just the perturbation of s0. We're also interested in the perturbation of s1 and s2. So the overall perturbation of c would be the sum of the perturbations of s0, s1, and s2 multiplied by the corresponding partial derivative of c with respect to each of them. Okay? You know, it's a virtual thing, right? It's not an existing thing you're going to manipulate. Just imagine that there is some perturbation of s0 here. Okay? This is going to perturb c by some value, and that value is going to be the perturbation of s0 multiplied by the partial derivative of c with respect to s0. Okay? And then if you perturb s1 simultaneously, you're also going to cause a perturbation of c. If you perturb s2 simultaneously, you're also going to cause a perturbation of c. The overall perturbation of c will be the sum of those perturbations, and that is given by this expression here. Now, those d, those infinitely small quantities, ds, dc, etc., think of them as, you know, numbers. You can do algebra with them. You can divide one by the other. You know, you can do stuff like that. So now you say, you know, what is ds0 equal to? If I tweak z by a quantity dz, it's going in turn to modify s0 by ds0. Okay? And what is the quantity by which s0 is going to be tweaked? If I tweak z by dz, because s is the result of computing the product of z by w0, then the perturbation is also going to be multiplied by w0, right? So the ds0 corresponding to a particular dz is going to be equal to dz times w0. And this is what's expressed here. Okay? ds0 equal w0 dz. Okay. Now, if I take this expression for ds0 and I insert it here in this formula, okay, I get dc equal w0 times dz times dc over ds0 plus same thing for 1 plus same thing for 2. And I'm going to take the dz and pass it to the other side. I'm going to divide both sides by dz. So now I get dc over dz equal, the dz doesn't appear anymore because it's been put underneath here. It's w0 times dc over ds0 plus w1 times dc over ds1, et cetera. Okay? It's just simple algebra. It's differential calculus, basically. Right. So it's better to write this in matrix form. So really, when you're computing, if I go back a few slides, when this is really kind of a matrix of all the weights that are kind of upstream of the zj's, so you can align the zj as a vector, maybe only the zj's that have nonzero terms in w, wij. And then you can write those w's as a matrix, and this is just a matrix vector product. Okay? So this is the way this would be written. You have a vector, you multiply by matrix, you get a new vector, pass that through nonlinearities, reuse, multiply that by matrix, et cetera. Right? So symbolically, you can write a simple neural net this way. We have linear blocks, okay, linear functional blocks, which basically take the previous state and multiply by matrix. Okay? So you have a state here, z1, multiply by matrix, you get w1, z1, and that gives you the vector of weighted sums, s2. Okay? Then you take that, pass it through the nonlinear functions, each component individually, and that gives you z2. Right? So that's a three-layer neural net. First weight matrix, nonlinearity, second weight matrix, nonlinearity, third weight matrix, and this is the output. There are two hidden layers, three layers of weights. Okay, the reason for writing it this way is that this is, like symbolically, the easiest way to understand really what kind of backprop does. And in fact, it corresponds also to the way we define neural nets and we run them on deep learning frameworks like PyTorch. So this is the sort of object-oriented version of defining a neural net in PyTorch. We're going to use predefined class, which are the linear class that basically multiplies a vector by matrix. It also has biases, but let's not talk about this just now. And another class, which is the value function, which takes a vector or a multi-dimensional array and applies the nonlinear function to every component separately. Okay, so this is a little piece of Python program that uses Torch. We import Torch. We make an image, which is, you know, 10 pixels by 20 pixels and three components for color. We compute the size of it and we're going to plug a neural net where the number of inputs is the number of components of our image. So in this case, that would be 600 or so. And we're going to define a class. The class is going to define a neural net and that's pretty much all we need to do here. So we define our network architecture. It's a subclass of neural net module, which is a pretty fine class. It's got a constructor here that will take the sizes of the internal layers that we want, the size of the input, the size of S1 and Z1, the size of S2 and Z2, and the size of S3. We call the parent class initializer. And then we just create three modules that are all linear modules. And we need to kind of store them somewhere because they have internal parameters. So we're going to have three slots in our object, N0, N1, N2, module 1, module 0, module 1, module 2. And each of them is going to be an instance of the class NN.linear with two sizes, the input size and the output size. Okay, so the first module has input size D0, output size D1, etc. And those classes are, since there is a capital L, means it's an object and inside there are parameters inside that item there. Right. So for example, the value doesn't have a capital because it doesn't have internal parameters. It's not kind of a trainable module. It's just a function. Whereas those things with capitals, they have sort of internal parameters, the weight matrices inside of them. So now we define a forward function, which basically computes the output from the input. And the first thing we do is we take the input thing, which may be a multidimensional array, and we flatten it. We flatten it using this idiomatic expression here in PyTorch. And then we apply the first module to X. We put the result in S1, which is a temporary variable, then we apply the value to S1, put the result in Z, then apply the second layer, put the result in S2, apply the value again, put the result in S3, and then the last linear layer, put the result in S3 and return S3. And there is a typo. So the second line should have been S1, it's the self.m0 of Z0, right? Z0 here, yes, correct. Yeah, this is something that is going to be fixed, right? Which I didn't fix. I know. This is Z0. Thanks for reminding me of this. Okay, but you'll see examples. I mean, I'll show you kind of actual examples of this, and you'll be able to run them yourself. That's all you need to do. You don't need to write how you compute the back prop, how you propagate the gradients. You could write it, and it would be as simple as forward. You could write a backward function, and it would basically multiply by the matrices going backwards. But you don't need to do this because PyTorch does this automatically for you. When you define the forward function, it knows what modules you've called in what order, what are the dependencies between the variables, and it will know how to generate the functions that compute the gradient backwards. So you don't need to worry about it. That's the magic of PyTorch, if you want. That's a bit the magic of deep learning, really. That's called automatic differentiation, and this is a particular form of automatic differentiation. There's another way to write functions in PyTorch that are kind of more functional. So you're not using modules with internal parameters. You're just coding functions one after the other. And PyTorch has a mechanism by which it can automatically compute the gradient of any function you define with respect to whatever parameters you want. Yeah, actually, these big guys with the capital L, like the nn.capital linear inside is going to have a lowercase linear, which is like the functional part, which is performing the matrix multiplication between the weights stored inside the object with the capital L and then the input. So every capital letter object will inside have the functional way. So one can decide to use either the functional form by default, or use this encapsulated version, which are more convenient to just use, right? Right. So at the end, you can create an instance of this class. You can create multiple instances, but you can create one here, just call my net and give it the sizes you want. And then to apply this to a particular image, you just do how to equal model of image. That's as simple as that. Okay, so this is your first neural net, and it does all the backup automatically. But you need to understand how that works, right? It's not because PyTorch does it for you, that you can sort of forget about how you actually compute the gradient of a function, because it's inevitable that at some point, you're going to want to actually assemble a neural net with a module that does not pre-exist, and you're going to have to write your own backup function. So to do this, you basically have, if you want to create a new module with some complex operation that does not pre-exist in PyTorch, then you do something like this. You define your class, but you write your own backward function, basically. Okay, so let's get one step up in terms of abstraction, and write this in sort of slightly more generic form, mathematical form, if you want. So let's say we have a cost function here, and we want to compute the gradient of this cost function with a stack to a particular vector in the system ZF. It could be a parameter, it could be a state, it doesn't matter. Okay, some states inside. And we have chain rule, and chain rule is nothing more than this, that I explained earlier. dC over dZF is equal to dC over dZG, dZG over dZF, as long as C is only influenced by ZF through ZG. There's no other way for ZF to influence C than to go through ZG, then this formula is correct. Okay? And of course, the identity is trivial, because it's just a simplification by this infinitesimal vector quantity dZG. Okay? So let's say ZG is a vector of size dG by one, so this means column vector. Okay? And ZF is a column vector of size dF. This is, if you want to write the correct dimensions of this, you know, we get something a little complicated. Okay, so first of all, this object here, dZG over dZF, well, let me start with this one. Okay, this one dC over dZG, that's a gradient vector. Okay? ZG is a vector, dC over dZG is a gradient vector. And it's the same size as dZG. But by convention, we actually write it as a line, as a row vector. Okay? So this thing here is going to be a row vector whose size is the same size as ZG, but it's going to be horizontal instead of vertical. Okay? This object here is something more complicated. It's actually a matrix. Why is it a matrix is because it's the derivative of a vector with respect to another vector. Okay? So let's look at this diagram here on the right. We have a function G, it takes ZF as an input, and it produces ZG as an output. And if we want to capture the information about the derivative of that module, which is this quantity here dZG over dZF, there's a lot of terms to capture because there's a lot of ways in which every single output, every component of ZG can be influenced by every component of ZF. Right? So if for every pair of components, ZG and ZF, there is a derivative term, which indicates by how much ZG would be perturbed if I perturbed ZF by a small infinitesimal quantity. Right? We have that for every pair of components of ZG and ZF. As a result, this is a matrix whose dimension is the number of rows is the size of ZG and the number of columns is the size of ZF. And each term in this matrix is one partial derivative term. So this whole matrix here, if I take the component ij, it's the partial derivative of the i-th output of that module, the i-th component of ZG, with respect to the j-th component of ZF. Okay? So what we get here is a row vector is equal to a row vector multiplied by a matrix, and the sizes kind of work out so that they're compatible with each other. Okay. So what is back propagation now? Back propagation is this formula. Okay? It says if you have the gradient of some cost function with respect to some variable, and you know the dependency of these variables with respect to another variable, you multiply this gradient vector by that Jacobian matrix, and you get the gradient vector with respect to that second variable. So graphically here on the right, if I have the gradient of the cost with respect to ZG, which is DC over DZG, and I want to compute the gradient of C with respect to ZF, which is DC over DZF, I only need to take that vector, which is a row vector, multiply it by the Jacobian matrix, DG over DZF, or DZG over DZF, and I get DC over DZF. Okay? It's this formula. Someone is objecting here. Isn't the summation missing here? Which summation? Summation of all the components of these partial multiplications. Here? Yeah. Well, this is a vector. This is a vector. This is a matrix. There is a lot of sums going on here because when you compute the product of this vector with its matrix, you're going to have a lot of sums, right? Yep. So it's hidden, right? Yeah. The sums are hidden. Okay. Inside of this vector matrix product. Like, you can take a specific example. Let's imagine that this G function is just a matrix multiplication. Okay? We just multiply by ZF by matrix W. So we have a linear operation. The derivative of the Jacobian matrix of the multiplication by matrix is the transpose of that matrix. So what we're going to do here is take this vector, multiply it by the transpose of the W matrix, and what we get is that vector. Okay? And it all makes sense, right? The sizes make sense. This matrix here is the transpose of the weight matrix, which of course had the reverse size. We multiply it. We pre-multiply it by the row vector of the gradient from the layer above, and we get the gradient with respect to the layer below. Okay? So backpropagating through a linear module just means multiplying the transpose of the matrix used by that module. And it's just a generalized form of what I explained earlier, you know, of propagating through the weights of a linear system. But it's less intuitive, right? Okay. So we're going to be able to do backpropagation by computing gradients all the way through, by propagating backwards. But this module really has two inputs. It has an input, which is ZF, and the other one is WG, the weight matrix, the parameter vector that is used inside of this module. So there is a second Jacobian matrix, which is the Jacobian matrix of ZG with respect to the terms of this weight parameter. Okay? And to compute the gradient of the cost function with respect to those weight parameters, I need to multiply this gradient vector by the Jacobian matrix of that block with respect to its weight. And it's not the same as the Jacobian matrix with respect to the input. It's a different Jacobian matrix. I'll come back to this in a second. So to do backprop, again, if we have a vector of gradients of some cost with respect to a state, and we have a function that is a function of one or several variables, we multiply this gradient by the Jacobian matrix of this block with respect to each of these inputs, and that gives us the gradient with respect to each of the inputs. And that's going to be expressed here. So this is the backpropagation of states in a layer-wise classical type neural net. DC over DZK, which is the state of layer K, is DC over ZK plus one, which is the gradient of the cost with respect to the layer above, times the Jacobian matrix of the state of layer K plus one with respect to the state of layer K. Now we assume DC over DZK plus one is known, and we just need to multiply with the Jacobian matrix of the function that links ZK to ZK plus one. The function is used to compute ZK plus one from ZK. And this may be a function also of some parameters inside. But here, that's the matrix of partial derivatives of F, which is with output to ZK plus one, with respect to each of the components of ZK. So that's the first rule of backpropagation, and it's a recursive rule. So you can start from the top. You start initially with DC over DC, which is one, which is why I have this one here on top. And then you just keep multiplying by the Jacobian matrix all the way down, and backpropagate gradients. And now you get gradients with respect to all the states. You also want the gradients with respect to the weights, because that's what you need to do learning. So what you can write is the same chain rule, DC over DWK is equal to DC over the ZK plus one, which we assume is known, times DZK plus one of DWK, right? And you can write this as DC over DK plus one. And the dependency between ZK plus one and WK is the function ZK applied to WK. So you can differentiate the function, the output of the function ZK with respect to WK, and that gives you another Jacobian matrix. And so those two formulas, you can do backpropagation just about anything. Really what goes on inside PyTorch and inside most of those frameworks, TensorFlow and Jackson, whatever. It's something like this where you have, so let's take a very simple diagram here where you have an input parameterized function that computes an output that goes to a cost function. And that cost function measures the discrepancy between the output of the system and the desired output. So you can write this function as C of G of W. I didn't put the X here, but just for charity. And the derivative of this is, again, you apply chain rule or you can write it with partial derivatives this way. And same for, you know, expand the dependency of the output with respect to the parameters as the Jacobian matrix of G with respect to W. If W is a scalar, then this is just a derivative, partial derivative. Okay, now you can express this as a compute graph. So you can say, like, how am I going to compute DC over DW? What I'm going to have to do is take the value one, which is the derivative of C with respect to itself, basically, the loss with respect to itself. I'm going to multiply this by the derivative of the cost with respect to Y bar. Okay, and that's going to give me DC over DY bar, obviously. Okay, this is the same as this because I'm just multiplied by one. Then multiply this by the Jacobian matrix of G with respect to W, which is a derivative if W is a scalar. That, of course, depends on X. And I get DC over DW. So this is a so-called compute graph, right? This is a way of organizing operations to compute the gradient. And there is essentially an automatic way of transforming a compute graph of this type into a compute graph of this type that computes the gradient automatically. And this is the magic that happens in the automatic differentiation inside PyTorch and TensorFlow and other systems. Some systems are pretty smart about this in a sense that those functions can be fairly complicated. They can involve themselves computing derivatives and they can involve dynamic computation, where the graph of computation depends on the data. And actually PyTorch does this properly. I'm not going to go through all the details of this, but this is kind of a way of reminding you what the dimensions of all those things are, right? So if Y is a column vector of size M, W is a column vector of size N, then this is a row vector of size N, this is a row vector of size M, and this is a geocomium matrix of size N by N. And all of this works out. Okay, so the way we're going to build neural nets, and I'll come back to this in a subsequent lecture, is that we are going to have at our disposal a large collection of basic modules which we're going to be able to arrange in more or less complex graphs as a way to build the architecture of a learning system. Okay, so either we're going to write a class or we're going to write a program that runs the forward pass, and this program is going to be composed of basic mathematical operations, addition, subtraction of tensors or multi-dimensional arrays, other types of scalar operations, or the application of one of the predefined complex parameterized functions, like a linear module, a value, or things like that. And we have at our disposal a large library of such modules, which are things that people have come up with over the years that are kind of basic modules that are used in a lot of applications. Right, so the basic things that we've seen so far I think is like values. There's other nonlinear functions like sigmoids and variations of this. There's a large collection of them. And then we have cost functions like square error, cross entropy, hinge loss, ranking loss, and blah, blah, blah, which I'm not going to go through now, but we'll talk about this later. The nice thing about this formalism is that, as I said before, you can sort of compute graphs. You can construct a deep learning system by assembling those modules in any kind of arrangement you want, as long as there is no loops in the connection graph. So as long as you can come up with a partial order in those modules that will ensure that they are computed in the proper way. But there is a way to handle loops, and that's called recurrent nets. We'll talk about this later. Okay, so here's a few practical tricks if you want to play with neural nets, and you're going to do that soon enough, perhaps even tomorrow. And these are kind of a bit of a black art of deep learning, which is sort of a lot of it is implemented already in things like PyTorch if you used under tools, but some of it is kind of more of the sort of oral culture if you want of the deep learning community. You can find this in papers, but it's a little difficult to find sometimes. So most neural nets use values as the main nonlinearity, so this sort of half wave rectifier. Hyperbole tangent, which is a similar function, and logistic function, which is also a similar function, are used, but not as much, not nearly as much. You need to initialize the ways properly. So if you have a neural net and you initialize the ways to zero, it never takes off. It will never learn. The gradients will always be zero all the time. And the reason is because when you back propagate the gradient, you multiply by the transpose of the weight matrix. If that weight matrix is zero, your gradient is zero. So if you start with all the weights equal to zero, you never take off. And someone asked the question about saddle points before. Zero is a saddle point. And so if you start at this saddle point, you never get out of it. So you have to break the symmetry in the system. You have to initialize the weights to small random values. They don't need to be random, but it works fine if they're random. And the way you initialize is actually quite important. So there's all kinds of tricks to initialize things properly. One of the tricks was invented by my friend, about 30 years ago, even more than that, actually, 34 years ago, almost. Unfortunately, now it's called differently. It's called the kaming trick, but it's the same. And it consists in initializing the weights to random values in such a way that if a unit has many inputs, the weights are smaller than if it has few inputs. And the reason for this is that you want the weighted sum to be roughly kind of have some reasonable value. If the input variables have some reasonable value, let's say variance one or something like this, and you're computing a weighted sum of them, the weighted sum, the size of the weighted sum is going to grow like the square root of the number of inputs. And so you want to set the weights to something like the inverse square root if you want the weighted sum to be kind of about the same size as each of the inputs. So that's built into PyTorch. You can call this, you know, initialization procedure. What's the exact name of it? I can't remember. The one that is coming, coming, coming here, then there is the Xavier and then there is also yours we have in PyTorch. Yeah, they're slightly different, but they kind of do the same more or less. Yeah, the Xavier Glow version, yeah. Yeah, this one divides by the Fennin and Fennin. There's various loss functions, so I haven't talked yet about what the cross-entropy loss is, but cross-entropy loss is a particular cost that's used for classification. I'll probably talk about this next week and I'll have some time at the end of this lecture. This is for classification. As I said, we use stochastic gradient descent on mini-batches and mini-batches only because the hardware that we have needs mini-batches to perform properly. If we had different hardware, we would use mini-batch size one. As I said before, we need to shuffle the training samples. So if someone gives you a training set and puts all the examples of category one, then all the example category two, all the example category three, etc. If you use stochastic gradient by keeping this order, it is not going to work. You have to shuffle the samples so that you basically get samples from all the categories within kind of a small subset, if you want. There is an objection here for the stochastic gradient. Isn't Adam better? All right. Okay. There is a lot of variants of stochastic gradient. There are all stochastic gradient methods. In fact, people in optimization said this should not be called stochastic gradient descent because it's not a descent algorithm because stochastic gradient sometimes goes uphill because of the noise. So people who want to really kind of be correct about this say it's stochastic gradient optimization, but not stochastic gradient descent. That's the first thing. Stochastic gradient optimization or stochastic gradient descent, SGD, is a special case of gradient based optimization. The specification of it says you have to have a step size eta, but nobody tells you how you set this step size eta and nobody tells you that this step size is a scalar or a diagonal matrix or a full matrix. Okay. So there are variations of SGD in which eta is changed all the time for every sample or every batch. In SGD, most of the time this eta is decreased according to a schedule and there are a bunch of standard schedule in PyTorch that are implemented. In techniques like Adam, the eta is actually a diagonal matrix and that diagonal matrix, the term in the diagonal matrix are changed all the time. They're computed based on some estimate of the curvature of the cost function. There's a lot of methods to do this. Okay. They're all SGD type methods. Okay. Adam is an SGD method with a special type of eta. So yeah, in the opt-in package in Torch, there's a whole bunch of those methods. There's going to be a whole lecture on this, so don't worry about it, about optimization. Normalize the input variables to zero mean and unit variance. So this is a very important point that this type of optimization method, gradient based optimization methods, when you have weighted sounds, kind of linear operations, tends to be very sensitive to how the data is prepared. So if you have two variables that have very widely different variances, one of them varies between, let's say, minus one and plus one. The other one varies between minus 100 and plus 100. The system will basically not pay attention to the one that varies between plus one and minus one. We'll only pay attention to the big one. And this may be good or this may be bad. Furthermore, the learning rate you're going to have to use the eta parameter, the step size, is going to have to be set to a relatively small value to prevent the weights that look at this highly variable input from diverging. The gradients are going to be very large because the gradients basically are proportional to the size of the input or even to the variance of the input. So if you don't want your system to diverge, you're going to have to tune down the learning rate if the input variance is large. If the input variables are all shifted, they're all between, let's say, 99 and 101 instead of minus one and one. Then again, it's very difficult for a gradient-based algorithm that use weighted sums to figure out those things. Again, I'll talk about this more formally later. Right now, just remember the trick that you need to normalize your input. So basically, take every variable of your input, subtract the mean, you compute the mean over the training set of each variable. So let's say your training set is a set of images. The images are, let's say, 100 by 100 pixels. Let's say they're grayscale, so you get 10,000 variables. And let's say you get a million samples, right? You're going to take each of those 10,000 variables, compute the mean of it over the training set, compute the standard deviation of it over the entire training set. And the samples you're going to show to your system are going to be a sample where you have subtracted the mean from each of the 10,000 pixels and divided the resulting values by the standard deviation that you computed. Okay? So now what you have is a bunch of variables that are all zero mean and all standard deviation equal to one. And that makes your neural net happy. That makes your optimization algorithm happy, actually. We have actually a question. So you keep repeating SGD type methods, gradient based methods, because there are other types of methods. Yes. Okay. So there is gradient free methods. So gradient free method is a method where you do not assume that the function you're trying to optimize is differentiable or even continuous with respect to the parameters. For several reasons, perhaps it's a function that looks like a golf course, right? It's flat and then maybe it's got steps and, you know, it's difficult to, like the local gradient information does not give you any information as to where you should go to find the minimum. Okay? It could be that the function is essentially discrete, right? It's not a function of continuous variables, function of discrete variables. So for example, am I going to win this chess game? The variable you can manipulate is the position on the board. That's a discrete variable. So you can't, you can compute a gradient of, you know, a score with respect to a position on the chess game. It's a discrete variable. Another example is the cost function is not something you can compute. You don't actually know the cost function. Okay? So for example, the only thing you can do is give an input to the cost function and it tells you the cost. But you can't, you don't know the function. It's not, right? It's not a program on a computer. You can't backprop a gradient to it. A good example of this is the real world. The real world, you can think of it as a cost function, right? You learn to ride a bike and you ride your bike and at some point you fall. The real world does not give you a gradient of that cost function, which is how much you hurt with respect to your actions. Okay? The only thing you can do is try something else and see if you get the same result or not. Okay? So what do you do in that case? So basically now your cost function is a black box. So now you cannot propagate gradient to this black box. What you have to do is estimate the gradient by perturbing the, what you see to that black box, right? So, you know, you try something, right? And that something would be a perturbation of your input to this black box and you see what resulting perturbation occurs on the black, on the output of the black box, the cost. And now you can estimate whether you, you know, this modification improved or made the result worse, right? So essentially, this is like this optimization problem I was telling you about earlier. The gradient based algorithm is like you are in the mountain, lost in the mountain in a fog, you can't see anything. But you can estimate the direction of steepest descent, right? You can just look around and you can tell which is the direction of steepest descent. You just take a step in that direction. What if you can't see, right? So basically to estimate in which direction the function goes down, you have to actually take a step, okay? So you take a step in one direction, then you come back, then you can take a step in the other direction, come back, and then maybe you get an estimate for where the steepest descent is. Now you can take a step for steepest descent. So this is estimating the gradient by perturbation instead of by analytic means of back propagating gradients, okay, computing Jacobians or whatever, partial derivatives. And then there is the second step of complexity. Let's imagine that the landscape you are in is basically flat everywhere, except, you know, once in a while there is a step, okay? So taking a small step in one direction will not give you any information about which direction you have to go to. So there you have to use other techniques, taking bigger steps, you know, working for a while and seeing if you fall down the step or not, or go up a step. You know, maybe you can multiply yourself in sort of 10,000 copies of yourself and then kind of explore the surroundings. And then whenever someone says, oh, I find a hole, calls everyone to kind of come there, okay? So all those methods are called gradient-free optimization algorithms. Sometimes they're called zero-th order method. Why zero-th order? Because first order is when you can compute the derivative. Zero-th order is when you cannot compute the derivative. You can only compute the function or get a value for the function. And then you have second order methods that compute not just the first derivative, but also the second derivative. And they're also gradient-based, okay, because they need the first derivative as well. But they can accelerate the process by also computing the second derivative. And Adam is a very simplified form of kind of, you know, second order method. It's not a second order method, but it has a hint of second order. Another hint of second order method is what's called conjugate gradient. It's another class of method called quasi-Newton methods, which are also kind of using kind of curvature information, if you want, to kind of accelerate. Many of those are not actually practical for neural net training, but there are some forms that are. If you're interested in zero-th order optimization, there is a library that is actually produced by, it's an open source library, which originated at Facebook Research in Paris by an author called Olivier Tito, but it's really a community effort. There's a lot of contributors to it. It's called Nevergrad. And it implements a very large number of different optimization algorithms that do not assume that you have access to the gradient. Okay. There are genetic algorithms or evolutionary methods. There are particle swarm optimization. There are perturbation methods. There is all kinds of tricks, right? I mean, there's a whole catalog of those things. And those sometimes it's unavoidable. You have to use them because you don't know the cost function. So a very common situation where you had to use those things is reinforcement learning. So reinforcement learning is basically a situation where you tell the system, you don't tell the system the correct answer. You only tell the system whether the answer was good or bad. It's because you give the value of the cost, but you don't tell the machine where the cost is. So the machine doesn't know where the cost function is. Okay. And so the machine cannot actually compute the gradient of the cost. And so it has to use something like a zero-th order method. So what you can do is you can compute a gradient with respect to the parameters of the overall cost function by perturbing the parameters. Or what you can do is compute the gradient of the cost function with respect to the output of your neural net. Okay. Using perturbation. And once you have this estimate, then you back propagate the gradient through your network using regular backprop. So that's a combination of estimating the gradient through perturbation for the cost function because you don't know it, and then backpropagating from there. This is basically the tactic that was used by the deep line people in sort of the first sort of deep queue learning type methods. Back to the normalization. Do we normalize the entire dataset or each batch? It's equivalent. So you normalize each sample, but the variable you're computing is on the entire training set, right? So you're computing the standard deviation and the mean over the entire training set. In fact, most of the time you don't even need to do it over the entire training set because mean and standard deviation converges pretty fast. So, but you do it over the entire training set, right? And what you get is a constant number, two constant numbers, a number that you subtract and a number that you should divide for each component of your input, okay? It's a fixed preprocessing. For a given training set, you'll have a fixed mean and standard deviation vector. But maybe we can connect to the other tool, right? The other module, the batch normalization, right? Okay, we haven't talked about that yet. Yeah, I'm saying that we can perhaps extend this normalization bit to the both sides, like the whole dataset and the batch itself. Okay, yes, yes. So, I mean, again, there's going to be a whole lecture on this. But for the same reason, it's good to have variables, the input that are zero mean and you need variants. It's also good for the state variables inside the network to basically have zero mean and you need variants. And so people have come up with various ways of doing normalization of the variables inside the network so that they approach zero mean and you need variants. But, and there are many ways to do this. They have two names like batch normalization, like layer normalization. And the idea goes back a very long time. Batch norm is kind of a more recent incarnation of it. Let's see, what was I scheduled to decrease the learning rate? Yeah, as it turns out, for reasons that are still not completely fully understood, you need to learn fast initially, you need a learning rate of a particular size. But to get good results in the end, you kind of need to decrease the learning rate to kind of let the system settle inside of minima. And that requires decreasing the learning rate. There's various semi-valid theoretical explanations for this, but experimentally, it's clear you need to do that. And again, there are schedules that are pre-programmed in PyTorch for this. Use a bit of L1 or L2 regularization on the weights or combination. Yeah, after you've trained your system for a few epochs, you might want to kind of prune it, eliminate the weights that are useless, make sure that the weights have their minimum size. And what you do is you add a term in the cost function that basically shrinks the weights at every iteration. You might know what L2 and L1 regularization means if you've taken a class in machine learning for large secret regression or stuff like that. It's very common. But L2 sometimes is called weight decay. This, again, are pre-programmed in PyTorch. A trick that a lot of people use for large neural nets is a trick called dropout. Dropout is implemented as kind of a layer in PyTorch. And what this layer does is that it takes the state of a layer and it randomly picks a certain proportion of the units and basically sets them to zero. So you can think of it as a mask, a layer that applies a mask to an input. And the mask is randomly picked at every sample. And some proportion of the value in the mask are set to zero. Some are set to one. And you multiply the input by the mask. So only a subset of the units are allowed to speak to the next layer, essentially. That's called dropout. And the reason for doing this is that it forces the unit to distribute the information about the input over multiple units instead of kind of squeezing everything into a small number. And it makes the system more robust. There's some theoretical arguments for why it does that. Experimentally, if you add this to a large network, you get better journalization error. You get better performance on the test set. It's not always necessary, but it helps. Okay, there's lots of tricks and I'll devote a lecture on this. So I'm not going to go through all of them right now. That requires explaining a bit more about optimizations. So really, what deep learning is about, like, I told you everything about deep learning, like the basics of deep learning. What I haven't told you is why we use deep learning. Okay, and that's basically what I'm going to tell you about now. The motivation for why is it that we need basically multi-layer neural nets or things of this type. Okay, so the traditional prototypical model of supervised learning for a very long time is basically a linear classifier. A linear classifier for a two-class problem is basically a single unit of the similar type that we talked about earlier. You compute a weighted sum of inputs at a bias, and you could think of the bias as just another trainable weight whose corresponding input is equal to one, if you want. And then you pass that through a threshold function, the sine function, that I put minus one if the weighted sum is below zero and plus one if it's above zero. Okay, so this basic linear classifier basically partitions the space, the input space of x's into two half spaces separated by hyperplane. Right, so the equation sum of i, w, i, x, i plus b equals zero is the surface that separates the category one that is going to produce y bar equal plus one from category two where y bar equals minus one. Why is it a, why does it divide the space into two halves? It's because you're computing the dot product of an input vector with a weight vector. If those two vectors are orthogonal, then the dot product is zero. Okay, b is just an offset. So the set of points in x space where this dot product is zero is the set of points that are orthogonal to the vector w. Okay, so in a n-dimensional space, your vector w is a vector, and the set of x whose dot product with w is zero is a hyperplane. Right, so it's a linear subspace of dimension n minus one. Okay, and that hyperplane divides the space of dimension n into halves. So here is the situation in two dimensions. You have two dimensions x1, x2. You have data points here, the red, the red category and the blue category. And there is a weight vector plus a bias where the, you know, the intercept here of this sort of green separating line with x1 is minus b times divided by w1. So that gives you an idea for what w should be. And the w vector is orthogonal to that separating surface. Okay, so changing b will change the position and then changing w will change the orientation basically. Now, what about situations like this where the points are, the red and blue points are not separable by a hyperplane? That's called a non-linearly separable case. So there you can't use a linear classifier to separate those. What are we going to do? In fact, there is a theorem that goes back to 1966 by Tom Kovar, who died recently actually. It was a Stanford that says the probability that a particular separation of p points is linearly separable in n dimension is close to one when p is smaller than n, but it's close to zero when p is larger than n. In other words, if you, if you take an n-dimensional space, you throw p random points in that n-dimensional space, data points, okay? And you randomly label them blue and red. You ask the question, what is the probability that that particular dichotomy is linearly separable? I can separate the blue points from the red points with a hyperplane. And the answer is, if p is less than n, you have a good chance that they will be separable. If p is larger than n, you basically have no chance that they will. Okay, so if you have an image classification problem, let's say, and you have tons of examples, way bigger. So let's say you do n-nist. So n-nist is a dataset of handwritten digits. The images are 28 by 28 pixels. In fact, the intrinsic dimension is smaller because some pixels are always zero. And you have 60,000 samples. The probability that those 60,000 samples of, let's say, zeros from everything else or ones from everything else is nearly separable is basically nil. So, which is why people invented the classical model of pattern recognition. We consist in taking an input, engineering a feature extractor to produce a representation in such a way that in that space now, your problem becomes, let's say, linearly separable if you use a linear classifier or some other separability if you use another type of classifier. Okay? Now, necessarily, this feature extraction must be nonlinear itself. If the only thing it does is some affine transformation of the input, it's not going to make a nonlinearly separable problem into a linear separable one, right? So, necessarily, this feature extractor has to be nonlinear. This is very important to remember. Okay? A linear preprocessing doesn't do anything for you, essentially. So, people spend decades in computer vision, for example, as feature recognition, devising good feature extractors for particular problems. You know, what features are good to do face recognition, for example, right? Can I do things like detect the eyes and then measure the ratio between the separation of the eyes with the separation from the mouth and then, you know, computes a few features like this and then feed that to a classifier and figure out who the person is. So, most papers, you know, between, let's say, the 1960s or 70s and the late 2000s or early 2010s in computer vision were essentially about that, like how you represent images properly. Not all of them, okay? A lot of them for recognition. And a lot of people kind of devise very sort of generic ways of devising feature extractors. The basic idea is you just expand the dimension of the representation in a nonlinear way so that now your number of dimensions is larger than the number of samples. And now your problem has a chance of becoming linearly separable. So, the ideas that I'm not going to go through, like space styling, random projection. So, random projection basically is a very simple idea. You take your input vectors, you multiply them by random matrix, okay? And then you pass the result through some nonlinear operation, okay? That's called random projection. And it might make, if the dimension of the output is larger than the dimension of the input, it might make a nonlinearly separable problem linearly separable. It's very efficient because, you know, you might need a very large number of those, of this dimension to be able to kind of do a good job. But it works in certain cases and you don't have to train the first layer, you basically pick it randomly. And so, the only thing you need to train is a linear classifier on top. It's polynomial classifiers, which I'll talk about in a minute, in a minute, radio basis functions and kernel machines. So, those are basically techniques to turn an input into a representation that then will be essentially classifiable by a simple classifier like a linear classifier. So, what's a polynomial classifier? A polynomial classifier, basically, imagine that your input vector has two dimensions. The way you increase the dimensionality of the representation is that you take each of the input variables, but you also take every product of pairs of input variables, right? So, now you have a new feature vector, which is composed of x1, x2, you add one for the bias, and then also x1 times x2, x1 squared and x2 squared. So, when you do a linear classification in that space, what you're doing really is a quadratic classification in the original space, right? The surface, the separating surface in the original space now is a quadratic curve into dimension. In n dimension, it's a quadratic hypersurface, basically. So, it could be a parabola or ellipse or hyperbola, depending on the coefficients, right? Now, the problem with this is that it doesn't work very well in high dimension because the number of features grows with a square of the number of inputs. So, if you want to apply this to get an ImageNet type image, you know, the resolution is 256 by 256 by 3, because you have color channels. That's already a high dimension. If you take the cross product of all of those variables, that's way too large, okay? So, it's not really practical for high dimensional problems, but it's a trick. Now, here is, so, super vector machines are basically two-layer networks of kernel machines more generally, are two-layer systems in which the first layer has as many dimensions as you have training samples. Okay? So, for each training sample, you create a inner-on, a unit, if you want, and the role of this unit is to produce a large output if the input vector matches one of the training samples and a small output if it doesn't, or the other way around. A small output if it matches, a large output if it doesn't, okay? It doesn't really matter, but it has to be nonlinear. So, something like, you know, compute the dot product of the input by one of the training samples and passes through, you know, a negative exponential or a square or something like that. So, this gives you how much the input vector resembles one of the training samples, and you do this for every single training samples, okay? And then you train a linear classifier basically to use those inputs as, you know, as input to a linear classifier. You compute the weight so that linear classifier is basically as simple as that. There's some regularization involved, okay? So, essentially, it's kind of a lookup table, right? You have your entire training set as you know, points in your kind of nuance if you are, if you want for units in your first layer, and they each indicate how close the current input vector is to them. So, you get some picture of where the input vector is by basically having the relative position to all of the training samples, and then using a simple linear operation, you can figure out, like, what's the correct answer. This works really well for low dimensional problems, the small number of training samples, but you're not going to do computer vision with it, at least not without, not if X is our pixels, because it's basically template matching. Now, here is a very interesting fact. It's a fact that if you build a two-layer neural net on this model, okay? So, let's say a two-layer neural net, you have an input layer, a hidden layer, and not specifying the size, and a single output unit, and you ask, what functions can I approximate with an architecture of this type? The answer is, you can approximate pretty much any well-behaved function as close as you want, as long as you have enough of those units in the middle, okay? So, this is a theorem that says that two-layer neural nets are universal approximators. It doesn't really matter what nonlinear function you put in the middle. Any nonlinear function will do. A two-layer neural net is a universal approximator, and immediately you say, well, why do we need multiple layers, then, if we can approximate anything with two layers? And the answer is, it's very, very inefficient to try to approximate everything with only two layers, because many, many, many interesting functions we're interested in learning cannot be efficiently represented by a two-layer system. They can possibly be represented by a two-layer system, but the number of fielding units it would require would be so ridiculously large that it's completely impractical, okay? So, that's why we need layers. This very simple point is something that took about, you know, it took until the, basically, the 2010s for the machine learning and computer vision communities to understand, okay? If you understood what I just said, you just took a few seconds, so you beat them. There is a last question here before we finish class. So, does the depth of the network then have anything to do with generalization? Okay, so generalization is a different story, okay? Generalization is very difficult to predict. It depends on a lot of things. It depends on the appropriateness of the architecture to the problem at hand, okay? So, for example, people use convolutional nets for computer vision, they use transformers for text, you know, blah, blah, blah. So, there are certain architectures that work well for certain types of data. So, that's the main thing that will improve generalization. But generally, yes, multiple layers can improve generalization because for a particular function you're interested in learning, computing it with multiple layers will allow you to reduce the overall size of the system that will do a good job. And so, by reducing the size, you're basically making it easier for the system to find kind of good representation. But there is something else which has to do with compositionality. I'll come to this in a minute if I have time. Also, the minimum, the, like the, how do you call it, the well is like larger, right? If we have overparameterized networks. If you're overparameterized network, it's much easier to find a minimum to your objective function, right? Which is why neural nets are generally overparameterized. They generally have, like you, a much larger number of parameters than what you would think is necessary. And when you get them bigger, when you make them bigger, they work better usually. It's not always the case, but it's very curious phenomenon about this. We'll talk about this later. Okay, this is the one point I want to make. And it's the fact that the reason why we, why layers are good is that the world is compositional, the perceptual world in particular, but the world in general, the universe, if you want, is compositional. What does that mean? It means that, okay, at the level of the universe, right? We have elementary particles, they assemble to form less elementary particles, those assemble to form atoms, those assemble to form molecules, those assemble to form materials, those assemble to form, you know, structures, objects, etc. And, you know, environments, scenes, etc. You have the same kind of hierarchy for images, you have pixels, they assemble to form edges and textons and motifs, parts and objects. In text, you have characters that assemble to form words, word groups, clauses, sentences, stories. In speech, you have speech samples, assemble to form, you know, kind of elementary sounds, phones, phonemes, syllables, words, etc. So you have this kind of compositional hierarchy in a lot of natural signals. And this is what makes the world understandable, right? This is famous quote by Albert Einstein, the most incomprehensible thing about the world is that the world is comprehensible. And the reason why the world is comprehensible is because it's compositional, because small part assemble to form bigger part, and that allows you to have a description, an abstract description of the world in terms of parts from the level immediately below, in terms of level of abstraction. So to some extent, the layered architecture in a neural net reflects this idea that you have kind of a compositional hierarchy where simple things assemble to form slightly more complex things. So images, you have pixels formed to form edges that are kind of depicted here. These are actually feature detectors, the visualization of feature detectors by a particular convolutional net, which is a particular type of neural net, multilateral neural net. So at the low level, you have units that detect oriented edges, a couple layers up, you have things that detect simple motifs, circles, gratings, corners, etc. And then a few layers up, there are things like parts of objects and things like that. So I think personally that the magic of deep learning, the fact that multiple layers help is the fact that the perceptual world is basically a compositional hierarchy. And then this end-to-end learning in deep learning allows the system to learn hierarchical representations where each layer learns a representation that has a level of abstraction slightly higher than the previous one. So low level, you have individual pixels, then you have the presence or absence of an edge, then you have the presence or absence of a part of an object, and then you have the presence or absence of an object independently of the position of that object, the illumination, the color, the occlusions, the background, you know, things like that, right? So that's the motivation, the idea why deep learning is so successful and why it's basically taken over the world over the last 10 years or so. All right, thank you for your attention. That's great. So for tomorrow guys, don't forget to try to go over the 01 tutorial tensor, sorry, the 01 notebook that we have on the website such that we can get, like, you know, all on the same level for the ones that are not really familiar with NumPy stuff, okay? So otherwise, let's see you tomorrow morning and have a nice day. Take care everyone. Bye-bye.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 3.12, "text": " So as you know, we're going to talk about deep learning and we're going to jump right in.", "tokens": [50364, 407, 382, 291, 458, 11, 321, 434, 516, 281, 751, 466, 2452, 2539, 293, 321, 434, 516, 281, 3012, 558, 294, 13, 50520], "temperature": 0.0, "avg_logprob": -0.13290115734478375, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0035226354375481606}, {"id": 1, "seek": 0, "start": 4.48, "end": 11.92, "text": " So much of practical applications of deep learning today, machine learning and AI in general,", "tokens": [50588, 407, 709, 295, 8496, 5821, 295, 2452, 2539, 965, 11, 3479, 2539, 293, 7318, 294, 2674, 11, 50960], "temperature": 0.0, "avg_logprob": -0.13290115734478375, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0035226354375481606}, {"id": 2, "seek": 0, "start": 13.120000000000001, "end": 19.52, "text": " are used a paradigm called supervised learning, which I'm sure most of you have heard of before.", "tokens": [51020, 366, 1143, 257, 24709, 1219, 46533, 2539, 11, 597, 286, 478, 988, 881, 295, 291, 362, 2198, 295, 949, 13, 51340], "temperature": 0.0, "avg_logprob": -0.13290115734478375, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0035226354375481606}, {"id": 3, "seek": 0, "start": 19.52, "end": 24.72, "text": " So this is the paradigm by which you train a machine by showing it examples of inputs and outputs.", "tokens": [51340, 407, 341, 307, 264, 24709, 538, 597, 291, 3847, 257, 3479, 538, 4099, 309, 5110, 295, 15743, 293, 23930, 13, 51600], "temperature": 0.0, "avg_logprob": -0.13290115734478375, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0035226354375481606}, {"id": 4, "seek": 0, "start": 25.6, "end": 29.68, "text": " You want to build a machine to distinguish images of cars from airplanes, you show it an image of", "tokens": [51644, 509, 528, 281, 1322, 257, 3479, 281, 20206, 5267, 295, 5163, 490, 32947, 11, 291, 855, 309, 364, 3256, 295, 51848], "temperature": 0.0, "avg_logprob": -0.13290115734478375, "compression_ratio": 1.7732342007434945, "no_speech_prob": 0.0035226354375481606}, {"id": 5, "seek": 2968, "start": 29.68, "end": 34.56, "text": " a car. If the machine says car, you don't do anything. If it says something else, you adjust", "tokens": [50364, 257, 1032, 13, 759, 264, 3479, 1619, 1032, 11, 291, 500, 380, 360, 1340, 13, 759, 309, 1619, 746, 1646, 11, 291, 4369, 50608], "temperature": 0.0, "avg_logprob": -0.0981624104955175, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.0008658160222694278}, {"id": 6, "seek": 2968, "start": 34.56, "end": 38.4, "text": " the internal parameters of the system so that the output gets closer to the one you want.", "tokens": [50608, 264, 6920, 9834, 295, 264, 1185, 370, 300, 264, 5598, 2170, 4966, 281, 264, 472, 291, 528, 13, 50800], "temperature": 0.0, "avg_logprob": -0.0981624104955175, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.0008658160222694278}, {"id": 7, "seek": 2968, "start": 39.28, "end": 45.84, "text": " So imagine the target output is some vector of activities on a set of outputs. You want the", "tokens": [50844, 407, 3811, 264, 3779, 5598, 307, 512, 8062, 295, 5354, 322, 257, 992, 295, 23930, 13, 509, 528, 264, 51172], "temperature": 0.0, "avg_logprob": -0.0981624104955175, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.0008658160222694278}, {"id": 8, "seek": 2968, "start": 45.84, "end": 49.84, "text": " vector coming out of the machine to get closer to the vector that is the desired output.", "tokens": [51172, 8062, 1348, 484, 295, 264, 3479, 281, 483, 4966, 281, 264, 8062, 300, 307, 264, 14721, 5598, 13, 51372], "temperature": 0.0, "avg_logprob": -0.0981624104955175, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.0008658160222694278}, {"id": 9, "seek": 2968, "start": 51.44, "end": 56.56, "text": " And this works really well. As long as you have lots of data, it works for speech recognition,", "tokens": [51452, 400, 341, 1985, 534, 731, 13, 1018, 938, 382, 291, 362, 3195, 295, 1412, 11, 309, 1985, 337, 6218, 11150, 11, 51708], "temperature": 0.0, "avg_logprob": -0.0981624104955175, "compression_ratio": 1.8247011952191234, "no_speech_prob": 0.0008658160222694278}, {"id": 10, "seek": 5656, "start": 56.56, "end": 62.24, "text": " image recognition, face recognition, generating captions, translation, all kinds of stuff.", "tokens": [50364, 3256, 11150, 11, 1851, 11150, 11, 17746, 44832, 11, 12853, 11, 439, 3685, 295, 1507, 13, 50648], "temperature": 0.0, "avg_logprob": -0.09521075162020597, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.0011493630008772016}, {"id": 11, "seek": 5656, "start": 63.28, "end": 67.28, "text": " So this is, I would say, 95% of all applications of machine learning today.", "tokens": [50700, 407, 341, 307, 11, 286, 576, 584, 11, 13420, 4, 295, 439, 5821, 295, 3479, 2539, 965, 13, 50900], "temperature": 0.0, "avg_logprob": -0.09521075162020597, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.0011493630008772016}, {"id": 12, "seek": 5656, "start": 68.16, "end": 72.0, "text": " There are two other paradigms, one of which I will not talk about, one of which I will talk", "tokens": [50944, 821, 366, 732, 661, 13480, 328, 2592, 11, 472, 295, 597, 286, 486, 406, 751, 466, 11, 472, 295, 597, 286, 486, 751, 51136], "temperature": 0.0, "avg_logprob": -0.09521075162020597, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.0011493630008772016}, {"id": 13, "seek": 5656, "start": 72.0, "end": 77.2, "text": " about a lot. So the two other paradigms are reinforcement learning, which I will not talk", "tokens": [51136, 466, 257, 688, 13, 407, 264, 732, 661, 13480, 328, 2592, 366, 29280, 2539, 11, 597, 286, 486, 406, 751, 51396], "temperature": 0.0, "avg_logprob": -0.09521075162020597, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.0011493630008772016}, {"id": 14, "seek": 5656, "start": 77.2, "end": 83.92, "text": " about. And there are other courses. There's a course by Larry Pinto about this that I encourage", "tokens": [51396, 466, 13, 400, 456, 366, 661, 7712, 13, 821, 311, 257, 1164, 538, 18145, 430, 17246, 466, 341, 300, 286, 5373, 51732], "temperature": 0.0, "avg_logprob": -0.09521075162020597, "compression_ratio": 1.8734177215189873, "no_speech_prob": 0.0011493630008772016}, {"id": 15, "seek": 8392, "start": 84.0, "end": 89.76, "text": " you to take. And a third paradigm is self-supervised learning or unsupervised learning. And we'll", "tokens": [50368, 291, 281, 747, 13, 400, 257, 2636, 24709, 307, 2698, 12, 48172, 24420, 2539, 420, 2693, 12879, 24420, 2539, 13, 400, 321, 603, 50656], "temperature": 0.0, "avg_logprob": -0.07897162855717174, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.0012792986817657948}, {"id": 16, "seek": 8392, "start": 89.76, "end": 96.32000000000001, "text": " talk about this quite a lot in the following weeks. But for now, let's talk about supervised", "tokens": [50656, 751, 466, 341, 1596, 257, 688, 294, 264, 3480, 3259, 13, 583, 337, 586, 11, 718, 311, 751, 466, 46533, 50984], "temperature": 0.0, "avg_logprob": -0.07897162855717174, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.0012792986817657948}, {"id": 17, "seek": 8392, "start": 96.32000000000001, "end": 99.92, "text": " learning. Self-supervised learning, you could think of it as kind of a play on supervised learning.", "tokens": [50984, 2539, 13, 16348, 12, 48172, 24420, 2539, 11, 291, 727, 519, 295, 309, 382, 733, 295, 257, 862, 322, 46533, 2539, 13, 51164], "temperature": 0.0, "avg_logprob": -0.07897162855717174, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.0012792986817657948}, {"id": 18, "seek": 8392, "start": 101.84, "end": 106.0, "text": " So the traditional model of pattern recognition machine learning and supervised learning,", "tokens": [51260, 407, 264, 5164, 2316, 295, 5102, 11150, 3479, 2539, 293, 46533, 2539, 11, 51468], "temperature": 0.0, "avg_logprob": -0.07897162855717174, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.0012792986817657948}, {"id": 19, "seek": 8392, "start": 106.0, "end": 111.92, "text": " certainly going back to the late 50s or the 60s, is the idea by which you take a raw signal,", "tokens": [51468, 3297, 516, 646, 281, 264, 3469, 2625, 82, 420, 264, 4060, 82, 11, 307, 264, 1558, 538, 597, 291, 747, 257, 8936, 6358, 11, 51764], "temperature": 0.0, "avg_logprob": -0.07897162855717174, "compression_ratio": 1.8844621513944224, "no_speech_prob": 0.0012792986817657948}, {"id": 20, "seek": 11192, "start": 111.92, "end": 116.64, "text": " let's say an image or an audio signal or a set of features representing an object,", "tokens": [50364, 718, 311, 584, 364, 3256, 420, 364, 6278, 6358, 420, 257, 992, 295, 4122, 13460, 364, 2657, 11, 50600], "temperature": 0.0, "avg_logprob": -0.10946916184335384, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.000984448241069913}, {"id": 21, "seek": 11192, "start": 116.64, "end": 123.12, "text": " and then you turn it into a representation using a feature extractor, which in the past", "tokens": [50600, 293, 550, 291, 1261, 309, 666, 257, 10290, 1228, 257, 4111, 8947, 284, 11, 597, 294, 264, 1791, 50924], "temperature": 0.0, "avg_logprob": -0.10946916184335384, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.000984448241069913}, {"id": 22, "seek": 11192, "start": 123.12, "end": 129.52, "text": " was engineered. And then you take that representation, which is generally in the form of a vector or", "tokens": [50924, 390, 38648, 13, 400, 550, 291, 747, 300, 10290, 11, 597, 307, 5101, 294, 264, 1254, 295, 257, 8062, 420, 51244], "temperature": 0.0, "avg_logprob": -0.10946916184335384, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.000984448241069913}, {"id": 23, "seek": 11192, "start": 129.52, "end": 133.52, "text": " a table of numbers or some kind of tensor, a multi-dimensional array. But sometimes,", "tokens": [51244, 257, 3199, 295, 3547, 420, 512, 733, 295, 40863, 11, 257, 4825, 12, 18759, 10225, 13, 583, 2171, 11, 51444], "temperature": 0.0, "avg_logprob": -0.10946916184335384, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.000984448241069913}, {"id": 24, "seek": 11192, "start": 134.24, "end": 138.48000000000002, "text": " could be a different type of representation. And you feed that to a trainable classifier.", "tokens": [51480, 727, 312, 257, 819, 2010, 295, 10290, 13, 400, 291, 3154, 300, 281, 257, 3847, 712, 1508, 9902, 13, 51692], "temperature": 0.0, "avg_logprob": -0.10946916184335384, "compression_ratio": 1.7983870967741935, "no_speech_prob": 0.000984448241069913}, {"id": 25, "seek": 13848, "start": 139.35999999999999, "end": 143.35999999999999, "text": " So this is the learning where the learning takes part. This is the classical model,", "tokens": [50408, 407, 341, 307, 264, 2539, 689, 264, 2539, 2516, 644, 13, 639, 307, 264, 13735, 2316, 11, 50608], "temperature": 0.0, "avg_logprob": -0.10380513689159292, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0002490884799044579}, {"id": 26, "seek": 13848, "start": 144.07999999999998, "end": 148.79999999999998, "text": " and it's still popular. It's still used a lot. But basically, what deep learning has done is", "tokens": [50644, 293, 309, 311, 920, 3743, 13, 467, 311, 920, 1143, 257, 688, 13, 583, 1936, 11, 437, 2452, 2539, 575, 1096, 307, 50880], "temperature": 0.0, "avg_logprob": -0.10380513689159292, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0002490884799044579}, {"id": 27, "seek": 13848, "start": 148.79999999999998, "end": 155.92, "text": " replace this sort of manual hand engineering of the feature extractor by a stack of trainable", "tokens": [50880, 7406, 341, 1333, 295, 9688, 1011, 7043, 295, 264, 4111, 8947, 284, 538, 257, 8630, 295, 3847, 712, 51236], "temperature": 0.0, "avg_logprob": -0.10380513689159292, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0002490884799044579}, {"id": 28, "seek": 13848, "start": 156.56, "end": 160.64, "text": " modules, if you want. So in deep learning, the main idea of deep learning, and the only reason", "tokens": [51268, 16679, 11, 498, 291, 528, 13, 407, 294, 2452, 2539, 11, 264, 2135, 1558, 295, 2452, 2539, 11, 293, 264, 787, 1778, 51472], "temperature": 0.0, "avg_logprob": -0.10380513689159292, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0002490884799044579}, {"id": 29, "seek": 13848, "start": 160.64, "end": 166.95999999999998, "text": " why it's called deep, is that we stack a bunch of modules, each of which transforms the input a", "tokens": [51472, 983, 309, 311, 1219, 2452, 11, 307, 300, 321, 8630, 257, 3840, 295, 16679, 11, 1184, 295, 597, 35592, 264, 4846, 257, 51788], "temperature": 0.0, "avg_logprob": -0.10380513689159292, "compression_ratio": 1.773076923076923, "no_speech_prob": 0.0002490884799044579}, {"id": 30, "seek": 16696, "start": 166.96, "end": 173.52, "text": " little bit into something that's going to slightly higher level of abstraction, if you want.", "tokens": [50364, 707, 857, 666, 746, 300, 311, 516, 281, 4748, 2946, 1496, 295, 37765, 11, 498, 291, 528, 13, 50692], "temperature": 0.0, "avg_logprob": -0.10012018957803416, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.00026929861633107066}, {"id": 31, "seek": 16696, "start": 174.24, "end": 184.8, "text": " And then we train the entire system end to end. So I represented those sort of pinkish modules", "tokens": [50728, 400, 550, 321, 3847, 264, 2302, 1185, 917, 281, 917, 13, 407, 286, 10379, 729, 1333, 295, 7022, 742, 16679, 51256], "temperature": 0.0, "avg_logprob": -0.10012018957803416, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.00026929861633107066}, {"id": 32, "seek": 16696, "start": 184.8, "end": 190.16, "text": " to indicate the ones that are trainable, and the blue modules are the fixed ones, the hand", "tokens": [51256, 281, 13330, 264, 2306, 300, 366, 3847, 712, 11, 293, 264, 3344, 16679, 366, 264, 6806, 2306, 11, 264, 1011, 51524], "temperature": 0.0, "avg_logprob": -0.10012018957803416, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.00026929861633107066}, {"id": 33, "seek": 16696, "start": 190.16, "end": 195.84, "text": " engineered ones. So that's why deep learning is called deep. We stack multiple layers of", "tokens": [51524, 38648, 2306, 13, 407, 300, 311, 983, 2452, 2539, 307, 1219, 2452, 13, 492, 8630, 3866, 7914, 295, 51808], "temperature": 0.0, "avg_logprob": -0.10012018957803416, "compression_ratio": 1.6238938053097345, "no_speech_prob": 0.00026929861633107066}, {"id": 34, "seek": 19584, "start": 195.84, "end": 201.6, "text": " trainable things, and we train it end to end. The idea for this goes back a long time. The", "tokens": [50364, 3847, 712, 721, 11, 293, 321, 3847, 309, 917, 281, 917, 13, 440, 1558, 337, 341, 1709, 646, 257, 938, 565, 13, 440, 50652], "temperature": 0.0, "avg_logprob": -0.07737775051847418, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.0007203310960903764}, {"id": 35, "seek": 19584, "start": 201.6, "end": 207.76, "text": " practical methods for this go back to the mid to late 80s, with the back propagation algorithm,", "tokens": [50652, 8496, 7150, 337, 341, 352, 646, 281, 264, 2062, 281, 3469, 4688, 82, 11, 365, 264, 646, 38377, 9284, 11, 50960], "temperature": 0.0, "avg_logprob": -0.07737775051847418, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.0007203310960903764}, {"id": 36, "seek": 19584, "start": 207.76, "end": 214.48000000000002, "text": " which is going to be the main subject of today's lecture, actually. But it took a long time for", "tokens": [50960, 597, 307, 516, 281, 312, 264, 2135, 3983, 295, 965, 311, 7991, 11, 767, 13, 583, 309, 1890, 257, 938, 565, 337, 51296], "temperature": 0.0, "avg_logprob": -0.07737775051847418, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.0007203310960903764}, {"id": 37, "seek": 19584, "start": 214.48000000000002, "end": 221.68, "text": " this idea to actually percolate and become the main tool that people use to build machine learning", "tokens": [51296, 341, 1558, 281, 767, 680, 8768, 473, 293, 1813, 264, 2135, 2290, 300, 561, 764, 281, 1322, 3479, 2539, 51656], "temperature": 0.0, "avg_logprob": -0.07737775051847418, "compression_ratio": 1.7239819004524888, "no_speech_prob": 0.0007203310960903764}, {"id": 38, "seek": 22168, "start": 221.68, "end": 228.0, "text": " system. It's only about 10 years old. Okay, so let's go through a few definitions. So we're going", "tokens": [50364, 1185, 13, 467, 311, 787, 466, 1266, 924, 1331, 13, 1033, 11, 370, 718, 311, 352, 807, 257, 1326, 21988, 13, 407, 321, 434, 516, 50680], "temperature": 0.0, "avg_logprob": -0.123007381085268, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0016737176338210702}, {"id": 39, "seek": 22168, "start": 228.0, "end": 232.56, "text": " to deal with parameterized models, a parameterized model, or learning model, if you want, is a", "tokens": [50680, 281, 2028, 365, 13075, 1602, 5245, 11, 257, 13075, 1602, 2316, 11, 420, 2539, 2316, 11, 498, 291, 528, 11, 307, 257, 50908], "temperature": 0.0, "avg_logprob": -0.123007381085268, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0016737176338210702}, {"id": 40, "seek": 22168, "start": 232.56, "end": 238.72, "text": " parameterized function, g of x and w, where x is the input, and w is a set of parameters.", "tokens": [50908, 13075, 1602, 2445, 11, 290, 295, 2031, 293, 261, 11, 689, 2031, 307, 264, 4846, 11, 293, 261, 307, 257, 992, 295, 9834, 13, 51216], "temperature": 0.0, "avg_logprob": -0.123007381085268, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0016737176338210702}, {"id": 41, "seek": 22168, "start": 240.24, "end": 245.60000000000002, "text": " I'm representing this here on the right with a particular symbolism, where a function", "tokens": [51292, 286, 478, 13460, 341, 510, 322, 264, 558, 365, 257, 1729, 47061, 11, 689, 257, 2445, 51560], "temperature": 0.0, "avg_logprob": -0.123007381085268, "compression_ratio": 1.6727272727272726, "no_speech_prob": 0.0016737176338210702}, {"id": 42, "seek": 24560, "start": 246.56, "end": 252.79999999999998, "text": " like this that produces a single output, think of the output as either a vector or matrix or a", "tokens": [50412, 411, 341, 300, 14725, 257, 2167, 5598, 11, 519, 295, 264, 5598, 382, 2139, 257, 8062, 420, 8141, 420, 257, 50724], "temperature": 0.0, "avg_logprob": -0.1635133934020996, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.004466969054192305}, {"id": 43, "seek": 24560, "start": 252.79999999999998, "end": 258.24, "text": " tensor, or perhaps even a scalar, but generally is multidimensional. It can actually be something", "tokens": [50724, 40863, 11, 420, 4317, 754, 257, 39684, 11, 457, 5101, 307, 2120, 327, 332, 11075, 13, 467, 393, 767, 312, 746, 50996], "temperature": 0.0, "avg_logprob": -0.1635133934020996, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.004466969054192305}, {"id": 44, "seek": 24560, "start": 258.24, "end": 264.24, "text": " else in a multidimensional array, but something that, you know, maybe like a sparse array representation", "tokens": [50996, 1646, 294, 257, 2120, 327, 332, 11075, 10225, 11, 457, 746, 300, 11, 291, 458, 11, 1310, 411, 257, 637, 11668, 10225, 10290, 51296], "temperature": 0.0, "avg_logprob": -0.1635133934020996, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.004466969054192305}, {"id": 45, "seek": 24560, "start": 264.24, "end": 269.28, "text": " or a graph with values on it. But for now, let's think of it just as a multidimensional array.", "tokens": [51296, 420, 257, 4295, 365, 4190, 322, 309, 13, 583, 337, 586, 11, 718, 311, 519, 295, 309, 445, 382, 257, 2120, 327, 332, 11075, 10225, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1635133934020996, "compression_ratio": 1.7345132743362832, "no_speech_prob": 0.004466969054192305}, {"id": 46, "seek": 26928, "start": 270.23999999999995, "end": 275.59999999999997, "text": " So both the inputs and the outputs are multidimensional arrays, what people call tensors.", "tokens": [50412, 407, 1293, 264, 15743, 293, 264, 23930, 366, 2120, 327, 332, 11075, 41011, 11, 437, 561, 818, 10688, 830, 13, 50680], "temperature": 0.0, "avg_logprob": -0.16089490481785365, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.00025313510559499264}, {"id": 47, "seek": 26928, "start": 276.55999999999995, "end": 280.47999999999996, "text": " It's not really kind of the appropriate definition of tensor, but it's okay.", "tokens": [50728, 467, 311, 406, 534, 733, 295, 264, 6854, 7123, 295, 40863, 11, 457, 309, 311, 1392, 13, 50924], "temperature": 0.0, "avg_logprob": -0.16089490481785365, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.00025313510559499264}, {"id": 48, "seek": 26928, "start": 282.08, "end": 286.08, "text": " And that function is parameterized by a set of parameters w. Those are the knobs that we're", "tokens": [51004, 400, 300, 2445, 307, 13075, 1602, 538, 257, 992, 295, 9834, 261, 13, 3950, 366, 264, 46999, 300, 321, 434, 51204], "temperature": 0.0, "avg_logprob": -0.16089490481785365, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.00025313510559499264}, {"id": 49, "seek": 26928, "start": 286.08, "end": 292.08, "text": " going to adjust during training, and they basically determine the input-output relationship between", "tokens": [51204, 516, 281, 4369, 1830, 3097, 11, 293, 436, 1936, 6997, 264, 4846, 12, 346, 2582, 2480, 1296, 51504], "temperature": 0.0, "avg_logprob": -0.16089490481785365, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.00025313510559499264}, {"id": 50, "seek": 29208, "start": 293.03999999999996, "end": 301.52, "text": " you know, between the input x and the predicted output y bar. Okay, so I'm not explicitly", "tokens": [50412, 291, 458, 11, 1296, 264, 4846, 2031, 293, 264, 19147, 5598, 288, 2159, 13, 1033, 11, 370, 286, 478, 406, 20803, 50836], "temperature": 0.0, "avg_logprob": -0.1615020280243248, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0006985591608099639}, {"id": 51, "seek": 29208, "start": 301.52, "end": 308.0, "text": " representing the wire that comes in with w. Here, I kind of assume that w is somewhere inside of", "tokens": [50836, 13460, 264, 6234, 300, 1487, 294, 365, 261, 13, 1692, 11, 286, 733, 295, 6552, 300, 261, 307, 4079, 1854, 295, 51160], "temperature": 0.0, "avg_logprob": -0.1615020280243248, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0006985591608099639}, {"id": 52, "seek": 29208, "start": 309.2, "end": 315.36, "text": " this module. Think of this as an object in object-oriented programming. So it's an instance", "tokens": [51220, 341, 10088, 13, 6557, 295, 341, 382, 364, 2657, 294, 2657, 12, 27414, 9410, 13, 407, 309, 311, 364, 5197, 51528], "temperature": 0.0, "avg_logprob": -0.1615020280243248, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0006985591608099639}, {"id": 53, "seek": 29208, "start": 315.36, "end": 321.36, "text": " of a class that you instantiated and it's got a slot in it that represents the parameters,", "tokens": [51528, 295, 257, 1508, 300, 291, 9836, 72, 770, 293, 309, 311, 658, 257, 14747, 294, 309, 300, 8855, 264, 9834, 11, 51828], "temperature": 0.0, "avg_logprob": -0.1615020280243248, "compression_ratio": 1.611353711790393, "no_speech_prob": 0.0006985591608099639}, {"id": 54, "seek": 32136, "start": 321.36, "end": 328.16, "text": " and there is a forward function basically that takes as argument the input and returns the output.", "tokens": [50364, 293, 456, 307, 257, 2128, 2445, 1936, 300, 2516, 382, 6770, 264, 4846, 293, 11247, 264, 5598, 13, 50704], "temperature": 0.0, "avg_logprob": -0.11162333155787268, "compression_ratio": 1.7330316742081449, "no_speech_prob": 8.347874972969294e-05}, {"id": 55, "seek": 32136, "start": 328.16, "end": 336.24, "text": " Okay, so a basic running machine will have a cost function and the cost function in supervised", "tokens": [50704, 1033, 11, 370, 257, 3875, 2614, 3479, 486, 362, 257, 2063, 2445, 293, 264, 2063, 2445, 294, 46533, 51108], "temperature": 0.0, "avg_logprob": -0.11162333155787268, "compression_ratio": 1.7330316742081449, "no_speech_prob": 8.347874972969294e-05}, {"id": 56, "seek": 32136, "start": 336.24, "end": 344.56, "text": " running, but also in some other settings will basically compute the discrepancy, distance,", "tokens": [51108, 2614, 11, 457, 611, 294, 512, 661, 6257, 486, 1936, 14722, 264, 2983, 265, 6040, 1344, 11, 4560, 11, 51524], "temperature": 0.0, "avg_logprob": -0.11162333155787268, "compression_ratio": 1.7330316742081449, "no_speech_prob": 8.347874972969294e-05}, {"id": 57, "seek": 32136, "start": 345.12, "end": 350.32, "text": " divergence, whatever you want to call it, between the desired output y, which is given to you from", "tokens": [51552, 47387, 11, 2035, 291, 528, 281, 818, 309, 11, 1296, 264, 14721, 5598, 288, 11, 597, 307, 2212, 281, 291, 490, 51812], "temperature": 0.0, "avg_logprob": -0.11162333155787268, "compression_ratio": 1.7330316742081449, "no_speech_prob": 8.347874972969294e-05}, {"id": 58, "seek": 35032, "start": 350.32, "end": 356.0, "text": " the training set, and the output produced by the system y bar. Okay, so an example of this,", "tokens": [50364, 264, 3097, 992, 11, 293, 264, 5598, 7126, 538, 264, 1185, 288, 2159, 13, 1033, 11, 370, 364, 1365, 295, 341, 11, 50648], "temperature": 0.0, "avg_logprob": -0.10228809045285595, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.0002491990162525326}, {"id": 59, "seek": 35032, "start": 356.0, "end": 361.92, "text": " a very simple example of a setting like this is linear regression. In linear regression, x is a", "tokens": [50648, 257, 588, 2199, 1365, 295, 257, 3287, 411, 341, 307, 8213, 24590, 13, 682, 8213, 24590, 11, 2031, 307, 257, 50944], "temperature": 0.0, "avg_logprob": -0.10228809045285595, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.0002491990162525326}, {"id": 60, "seek": 35032, "start": 361.92, "end": 368.32, "text": " vector composed of components x i's, w is also a vector, and the output is a scalar that is simply", "tokens": [50944, 8062, 18204, 295, 6677, 2031, 741, 311, 11, 261, 307, 611, 257, 8062, 11, 293, 264, 5598, 307, 257, 39684, 300, 307, 2935, 51264], "temperature": 0.0, "avg_logprob": -0.10228809045285595, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.0002491990162525326}, {"id": 61, "seek": 35032, "start": 368.32, "end": 376.15999999999997, "text": " the dot product of x with w. So y bar now is a scalar, and what you compute is the square", "tokens": [51264, 264, 5893, 1674, 295, 2031, 365, 261, 13, 407, 288, 2159, 586, 307, 257, 39684, 11, 293, 437, 291, 14722, 307, 264, 3732, 51656], "temperature": 0.0, "avg_logprob": -0.10228809045285595, "compression_ratio": 1.7904761904761906, "no_speech_prob": 0.0002491990162525326}, {"id": 62, "seek": 37616, "start": 376.16, "end": 382.48, "text": " distance, the square difference really between y and y bar. If w is a matrix, then now y is a vector,", "tokens": [50364, 4560, 11, 264, 3732, 2649, 534, 1296, 288, 293, 288, 2159, 13, 759, 261, 307, 257, 8141, 11, 550, 586, 288, 307, 257, 8062, 11, 50680], "temperature": 0.0, "avg_logprob": -0.08986794015635614, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.00031993817538022995}, {"id": 63, "seek": 37616, "start": 382.48, "end": 388.8, "text": " and you compute the square norm of the difference between y and y bar, and that's basically linear", "tokens": [50680, 293, 291, 14722, 264, 3732, 2026, 295, 264, 2649, 1296, 288, 293, 288, 2159, 11, 293, 300, 311, 1936, 8213, 50996], "temperature": 0.0, "avg_logprob": -0.08986794015635614, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.00031993817538022995}, {"id": 64, "seek": 37616, "start": 388.8, "end": 395.36, "text": " regression. So learning will consist in finding the set of w's that minimize this particular cost", "tokens": [50996, 24590, 13, 407, 2539, 486, 4603, 294, 5006, 264, 992, 295, 261, 311, 300, 17522, 341, 1729, 2063, 51324], "temperature": 0.0, "avg_logprob": -0.08986794015635614, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.00031993817538022995}, {"id": 65, "seek": 37616, "start": 395.36, "end": 400.08000000000004, "text": " function average over a training set. I'll come to this in a minute, but I want you to think right", "tokens": [51324, 2445, 4274, 670, 257, 3097, 992, 13, 286, 603, 808, 281, 341, 294, 257, 3456, 11, 457, 286, 528, 291, 281, 519, 558, 51560], "temperature": 0.0, "avg_logprob": -0.08986794015635614, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.00031993817538022995}, {"id": 66, "seek": 37616, "start": 400.08000000000004, "end": 405.92, "text": " now about the fact that this g function may not be something particularly simple to compute. So", "tokens": [51560, 586, 466, 264, 1186, 300, 341, 290, 2445, 815, 406, 312, 746, 4098, 2199, 281, 14722, 13, 407, 51852], "temperature": 0.0, "avg_logprob": -0.08986794015635614, "compression_ratio": 1.8058608058608059, "no_speech_prob": 0.00031993817538022995}, {"id": 67, "seek": 40616, "start": 406.24, "end": 412.32000000000005, "text": " it may not be just multiplying a vector by a matrix. It may not be just carrying some", "tokens": [50368, 309, 815, 406, 312, 445, 30955, 257, 8062, 538, 257, 8141, 13, 467, 815, 406, 312, 445, 9792, 512, 50672], "temperature": 0.0, "avg_logprob": -0.10063695907592773, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.00029128033202141523}, {"id": 68, "seek": 40616, "start": 413.6, "end": 418.8, "text": " sort of fixed computation with sort of a fixed number of steps. It could involve something", "tokens": [50736, 1333, 295, 6806, 24903, 365, 1333, 295, 257, 6806, 1230, 295, 4439, 13, 467, 727, 9494, 746, 50996], "temperature": 0.0, "avg_logprob": -0.10063695907592773, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.00029128033202141523}, {"id": 69, "seek": 40616, "start": 418.8, "end": 423.84000000000003, "text": " complicated. It could involve minimizing a function with respect to some other variable that you", "tokens": [50996, 6179, 13, 467, 727, 9494, 46608, 257, 2445, 365, 3104, 281, 512, 661, 7006, 300, 291, 51248], "temperature": 0.0, "avg_logprob": -0.10063695907592773, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.00029128033202141523}, {"id": 70, "seek": 40616, "start": 423.84000000000003, "end": 430.88, "text": " don't know. It could involve a lot of iteration of some algorithm that converges towards a fixed", "tokens": [51248, 500, 380, 458, 13, 467, 727, 9494, 257, 688, 295, 24784, 295, 512, 9284, 300, 9652, 2880, 3030, 257, 6806, 51600], "temperature": 0.0, "avg_logprob": -0.10063695907592773, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.00029128033202141523}, {"id": 71, "seek": 43088, "start": 430.88, "end": 439.92, "text": " point. So let's not restrict ourselves to g of x w that are simple things. It could be very", "tokens": [50364, 935, 13, 407, 718, 311, 406, 7694, 4175, 281, 290, 295, 2031, 261, 300, 366, 2199, 721, 13, 467, 727, 312, 588, 50816], "temperature": 0.0, "avg_logprob": -0.12964683855083628, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.0018380273832008243}, {"id": 72, "seek": 43088, "start": 439.92, "end": 446.15999999999997, "text": " complicated things, and we'll come to this in a few weeks. So this is just to explain the", "tokens": [50816, 6179, 721, 11, 293, 321, 603, 808, 281, 341, 294, 257, 1326, 3259, 13, 407, 341, 307, 445, 281, 2903, 264, 51128], "temperature": 0.0, "avg_logprob": -0.12964683855083628, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.0018380273832008243}, {"id": 73, "seek": 43088, "start": 446.15999999999997, "end": 454.48, "text": " notations that I will use during the course of this class. So we have observed input and desired", "tokens": [51128, 406, 763, 300, 286, 486, 764, 1830, 264, 1164, 295, 341, 1508, 13, 407, 321, 362, 13095, 4846, 293, 14721, 51544], "temperature": 0.0, "avg_logprob": -0.12964683855083628, "compression_ratio": 1.553072625698324, "no_speech_prob": 0.0018380273832008243}, {"id": 74, "seek": 45448, "start": 454.48, "end": 461.92, "text": " output variables. Those are kind of gray grayish bubbles. Other variables that are produced by", "tokens": [50364, 5598, 9102, 13, 3950, 366, 733, 295, 10855, 10855, 742, 16295, 13, 5358, 9102, 300, 366, 7126, 538, 50736], "temperature": 0.0, "avg_logprob": -0.1344094227269753, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.004069463349878788}, {"id": 75, "seek": 45448, "start": 461.92, "end": 468.8, "text": " the system or internal to the system are those kind of, you know, empty circle variables.", "tokens": [50736, 264, 1185, 420, 6920, 281, 264, 1185, 366, 729, 733, 295, 11, 291, 458, 11, 6707, 6329, 9102, 13, 51080], "temperature": 0.0, "avg_logprob": -0.1344094227269753, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.004069463349878788}, {"id": 76, "seek": 45448, "start": 469.92, "end": 473.84000000000003, "text": " We have determinacy functions or functions that are so they are indicated by this sort of", "tokens": [51136, 492, 362, 15957, 2551, 6828, 420, 6828, 300, 366, 370, 436, 366, 16176, 538, 341, 1333, 295, 51332], "temperature": 0.0, "avg_logprob": -0.1344094227269753, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.004069463349878788}, {"id": 77, "seek": 45448, "start": 474.64000000000004, "end": 478.48, "text": " rounded shape here. They can take multiple inputs have multiple outputs.", "tokens": [51372, 23382, 3909, 510, 13, 814, 393, 747, 3866, 15743, 362, 3866, 23930, 13, 51564], "temperature": 0.0, "avg_logprob": -0.1344094227269753, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.004069463349878788}, {"id": 78, "seek": 45448, "start": 479.92, "end": 484.40000000000003, "text": " And each of those can be tensors or scalars or whatever. And they have implicit parameters", "tokens": [51636, 400, 1184, 295, 729, 393, 312, 10688, 830, 420, 15664, 685, 420, 2035, 13, 400, 436, 362, 26947, 9834, 51860], "temperature": 0.0, "avg_logprob": -0.1344094227269753, "compression_ratio": 1.7877551020408162, "no_speech_prob": 0.004069463349878788}, {"id": 79, "seek": 48448, "start": 484.48, "end": 490.16, "text": " that are tunable by training. And then we have cost functions. So cost functions are basically", "tokens": [50364, 300, 366, 4267, 712, 538, 3097, 13, 400, 550, 321, 362, 2063, 6828, 13, 407, 2063, 6828, 366, 1936, 50648], "temperature": 0.0, "avg_logprob": -0.12262872549203727, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.00024527558707632124}, {"id": 80, "seek": 48448, "start": 491.36, "end": 497.84000000000003, "text": " functions that take one or multiple inputs and output a scalar. But I'm not representing the", "tokens": [50708, 6828, 300, 747, 472, 420, 3866, 15743, 293, 5598, 257, 39684, 13, 583, 286, 478, 406, 13460, 264, 51032], "temperature": 0.0, "avg_logprob": -0.12262872549203727, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.00024527558707632124}, {"id": 81, "seek": 48448, "start": 497.84000000000003, "end": 504.88, "text": " output is implicit. Okay, so if you have a red square, it has an implicit output. And it's a", "tokens": [51032, 5598, 307, 26947, 13, 1033, 11, 370, 498, 291, 362, 257, 2182, 3732, 11, 309, 575, 364, 26947, 5598, 13, 400, 309, 311, 257, 51384], "temperature": 0.0, "avg_logprob": -0.12262872549203727, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.00024527558707632124}, {"id": 82, "seek": 48448, "start": 504.88, "end": 511.68, "text": " scalar and we interpret it as a cost or an energy function. So this symbolism is kind of similar", "tokens": [51384, 39684, 293, 321, 7302, 309, 382, 257, 2063, 420, 364, 2281, 2445, 13, 407, 341, 47061, 307, 733, 295, 2531, 51724], "temperature": 0.0, "avg_logprob": -0.12262872549203727, "compression_ratio": 1.6830357142857142, "no_speech_prob": 0.00024527558707632124}, {"id": 83, "seek": 51168, "start": 511.68, "end": 517.76, "text": " to what people use in graphical models. If you if you've heard what a graphical model is,", "tokens": [50364, 281, 437, 561, 764, 294, 35942, 5245, 13, 759, 291, 498, 291, 600, 2198, 437, 257, 35942, 2316, 307, 11, 50668], "temperature": 0.0, "avg_logprob": -0.12412589596163842, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.0006069216760806739}, {"id": 84, "seek": 51168, "start": 517.76, "end": 521.44, "text": " particularly the type of graphical model called a factor graph. So in a factor graph, you have", "tokens": [50668, 4098, 264, 2010, 295, 35942, 2316, 1219, 257, 5952, 4295, 13, 407, 294, 257, 5952, 4295, 11, 291, 362, 50852], "temperature": 0.0, "avg_logprob": -0.12412589596163842, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.0006069216760806739}, {"id": 85, "seek": 51168, "start": 521.44, "end": 526.64, "text": " those variable bubbles, and you have those factors, which are those square cost functions.", "tokens": [50852, 729, 7006, 16295, 11, 293, 291, 362, 729, 6771, 11, 597, 366, 729, 3732, 2063, 6828, 13, 51112], "temperature": 0.0, "avg_logprob": -0.12412589596163842, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.0006069216760806739}, {"id": 86, "seek": 51168, "start": 527.52, "end": 530.72, "text": " You don't have this idea that you had deterministic functions in it, because", "tokens": [51156, 509, 500, 380, 362, 341, 1558, 300, 291, 632, 15957, 3142, 6828, 294, 309, 11, 570, 51316], "temperature": 0.0, "avg_logprob": -0.12412589596163842, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.0006069216760806739}, {"id": 87, "seek": 51168, "start": 530.72, "end": 534.24, "text": " graphical models don't care about the fact that you have functions in one direction or another.", "tokens": [51316, 35942, 5245, 500, 380, 1127, 466, 264, 1186, 300, 291, 362, 6828, 294, 472, 3513, 420, 1071, 13, 51492], "temperature": 0.0, "avg_logprob": -0.12412589596163842, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.0006069216760806739}, {"id": 88, "seek": 51168, "start": 534.24, "end": 540.96, "text": " But here we care about it. So with this extra symbol. Okay, so machine learning consists in", "tokens": [51492, 583, 510, 321, 1127, 466, 309, 13, 407, 365, 341, 2857, 5986, 13, 1033, 11, 370, 3479, 2539, 14689, 294, 51828], "temperature": 0.0, "avg_logprob": -0.12412589596163842, "compression_ratio": 1.935483870967742, "no_speech_prob": 0.0006069216760806739}, {"id": 89, "seek": 54096, "start": 540.96, "end": 547.52, "text": " basically minimizing finding the set of parameters W that minimize the cost function averaged over", "tokens": [50364, 1936, 46608, 5006, 264, 992, 295, 9834, 343, 300, 17522, 264, 2063, 2445, 18247, 2980, 670, 50692], "temperature": 0.0, "avg_logprob": -0.20359645843505858, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0007551577873528004}, {"id": 90, "seek": 54096, "start": 547.52, "end": 556.64, "text": " a training set. So a training set is a set of pairs x, x, y indexed by an index P. Okay, so we have", "tokens": [50692, 257, 3097, 992, 13, 407, 257, 3097, 992, 307, 257, 992, 295, 15494, 2031, 11, 2031, 11, 288, 8186, 292, 538, 364, 8186, 430, 13, 1033, 11, 370, 321, 362, 51148], "temperature": 0.0, "avg_logprob": -0.20359645843505858, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0007551577873528004}, {"id": 91, "seek": 54096, "start": 556.64, "end": 562.96, "text": " P training samples. And it'll be the index of the training set the training sample. And our overall", "tokens": [51148, 430, 3097, 10938, 13, 400, 309, 603, 312, 264, 8186, 295, 264, 3097, 992, 264, 3097, 6889, 13, 400, 527, 4787, 51464], "temperature": 0.0, "avg_logprob": -0.20359645843505858, "compression_ratio": 1.6931818181818181, "no_speech_prob": 0.0007551577873528004}, {"id": 92, "seek": 56296, "start": 563.6800000000001, "end": 566.1600000000001, "text": " last function that we're going to have to minimize", "tokens": [50400, 1036, 2445, 300, 321, 434, 516, 281, 362, 281, 17522, 50524], "temperature": 0.0, "avg_logprob": -0.17463091895693825, "compression_ratio": 1.6195121951219513, "no_speech_prob": 0.004197641275823116}, {"id": 93, "seek": 56296, "start": 568.48, "end": 574.48, "text": " is the, you know, is equal to the cost of the discrepancy between the y and the output of our", "tokens": [50640, 307, 264, 11, 291, 458, 11, 307, 2681, 281, 264, 2063, 295, 264, 2983, 265, 6040, 1344, 1296, 264, 288, 293, 264, 5598, 295, 527, 50940], "temperature": 0.0, "avg_logprob": -0.17463091895693825, "compression_ratio": 1.6195121951219513, "no_speech_prob": 0.004197641275823116}, {"id": 94, "seek": 56296, "start": 574.48, "end": 583.12, "text": " model by bar g of x, w, as I said earlier. So L is a value, C is a C is a module and L is a", "tokens": [50940, 2316, 538, 2159, 290, 295, 2031, 11, 261, 11, 382, 286, 848, 3071, 13, 407, 441, 307, 257, 2158, 11, 383, 307, 257, 383, 307, 257, 10088, 293, 441, 307, 257, 51372], "temperature": 0.0, "avg_logprob": -0.17463091895693825, "compression_ratio": 1.6195121951219513, "no_speech_prob": 0.004197641275823116}, {"id": 95, "seek": 56296, "start": 583.12, "end": 589.2800000000001, "text": " is a is a way of writing C of y, g of x, w, you know, whether it depends explicitly on x, y and", "tokens": [51372, 307, 257, 307, 257, 636, 295, 3579, 383, 295, 288, 11, 290, 295, 2031, 11, 261, 11, 291, 458, 11, 1968, 309, 5946, 20803, 322, 2031, 11, 288, 293, 51680], "temperature": 0.0, "avg_logprob": -0.17463091895693825, "compression_ratio": 1.6195121951219513, "no_speech_prob": 0.004197641275823116}, {"id": 96, "seek": 58928, "start": 589.28, "end": 595.4399999999999, "text": " w. Okay, but it's the same thing really. The overall last function, which is this", "tokens": [50364, 261, 13, 1033, 11, 457, 309, 311, 264, 912, 551, 534, 13, 440, 4787, 1036, 2445, 11, 597, 307, 341, 50672], "temperature": 0.0, "avg_logprob": -0.13558678724327866, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.00089668930741027}, {"id": 97, "seek": 58928, "start": 597.04, "end": 603.1999999999999, "text": " kind of curly L is the average of the per sample loss function over the entire training set. Okay,", "tokens": [50752, 733, 295, 32066, 441, 307, 264, 4274, 295, 264, 680, 6889, 4470, 2445, 670, 264, 2302, 3097, 992, 13, 1033, 11, 51060], "temperature": 0.0, "avg_logprob": -0.13558678724327866, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.00089668930741027}, {"id": 98, "seek": 58928, "start": 603.8399999999999, "end": 610.16, "text": " so compute L for the entire training set, divide by some all the terms divide by P, and that's the", "tokens": [51092, 370, 14722, 441, 337, 264, 2302, 3097, 992, 11, 9845, 538, 512, 439, 264, 2115, 9845, 538, 430, 11, 293, 300, 311, 264, 51408], "temperature": 0.0, "avg_logprob": -0.13558678724327866, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.00089668930741027}, {"id": 99, "seek": 58928, "start": 610.16, "end": 616.8, "text": " average. That's the loss. Okay, so now the name of the game is trying to find the minimum of that", "tokens": [51408, 4274, 13, 663, 311, 264, 4470, 13, 1033, 11, 370, 586, 264, 1315, 295, 264, 1216, 307, 1382, 281, 915, 264, 7285, 295, 300, 51740], "temperature": 0.0, "avg_logprob": -0.13558678724327866, "compression_ratio": 1.8038277511961722, "no_speech_prob": 0.00089668930741027}, {"id": 100, "seek": 61680, "start": 616.8, "end": 623.5999999999999, "text": " loss with respect to the parameters. This is an optimization problem. So symbolically, I can", "tokens": [50364, 4470, 365, 3104, 281, 264, 9834, 13, 639, 307, 364, 19618, 1154, 13, 407, 5986, 984, 11, 286, 393, 50704], "temperature": 0.0, "avg_logprob": -0.0812365214029948, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.00031010195380076766}, {"id": 101, "seek": 61680, "start": 623.5999999999999, "end": 629.04, "text": " represent this entire graph as the thing on the right. This is rarely used in practice, but this", "tokens": [50704, 2906, 341, 2302, 4295, 382, 264, 551, 322, 264, 558, 13, 639, 307, 13752, 1143, 294, 3124, 11, 457, 341, 50976], "temperature": 0.0, "avg_logprob": -0.0812365214029948, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.00031010195380076766}, {"id": 102, "seek": 61680, "start": 629.04, "end": 636.0, "text": " is sort of a way to visualize this. So think about each training sample as a sort of identical copy", "tokens": [50976, 307, 1333, 295, 257, 636, 281, 23273, 341, 13, 407, 519, 466, 1184, 3097, 6889, 382, 257, 1333, 295, 14800, 5055, 51324], "temperature": 0.0, "avg_logprob": -0.0812365214029948, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.00031010195380076766}, {"id": 103, "seek": 61680, "start": 636.0, "end": 642.9599999999999, "text": " of the replica, if you want, of the model and the cost function applied to a different training", "tokens": [51324, 295, 264, 35456, 11, 498, 291, 528, 11, 295, 264, 2316, 293, 264, 2063, 2445, 6456, 281, 257, 819, 3097, 51672], "temperature": 0.0, "avg_logprob": -0.0812365214029948, "compression_ratio": 1.6382978723404256, "no_speech_prob": 0.00031010195380076766}, {"id": 104, "seek": 64296, "start": 642.96, "end": 647.2, "text": " sample, and then there is an average operation that computes the loss, right? So everything you", "tokens": [50364, 6889, 11, 293, 550, 456, 307, 364, 4274, 6916, 300, 715, 1819, 264, 4470, 11, 558, 30, 407, 1203, 291, 50576], "temperature": 0.0, "avg_logprob": -0.1639510609886863, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0014321878552436829}, {"id": 105, "seek": 64296, "start": 648.0, "end": 654.48, "text": " can write as a formula, you can probably write in terms of those graphs. This is going to be very", "tokens": [50616, 393, 2464, 382, 257, 8513, 11, 291, 393, 1391, 2464, 294, 2115, 295, 729, 24877, 13, 639, 307, 516, 281, 312, 588, 50940], "temperature": 0.0, "avg_logprob": -0.1639510609886863, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0014321878552436829}, {"id": 106, "seek": 64296, "start": 654.48, "end": 661.44, "text": " useful as we're going to see later. Okay, so supervised machine learning and a lot of other", "tokens": [50940, 4420, 382, 321, 434, 516, 281, 536, 1780, 13, 1033, 11, 370, 46533, 3479, 2539, 293, 257, 688, 295, 661, 51288], "temperature": 0.0, "avg_logprob": -0.1639510609886863, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0014321878552436829}, {"id": 107, "seek": 64296, "start": 661.44, "end": 669.44, "text": " machine learning patterns as well actually are can be viewed as function optimization and a very", "tokens": [51288, 3479, 2539, 8294, 382, 731, 767, 366, 393, 312, 19174, 382, 2445, 19618, 293, 257, 588, 51688], "temperature": 0.0, "avg_logprob": -0.1639510609886863, "compression_ratio": 1.6324786324786325, "no_speech_prob": 0.0014321878552436829}, {"id": 108, "seek": 66944, "start": 669.44, "end": 677.12, "text": " simple approach to optimizing a function, which means finding the set of parameters to a function", "tokens": [50364, 2199, 3109, 281, 40425, 257, 2445, 11, 597, 1355, 5006, 264, 992, 295, 9834, 281, 257, 2445, 50748], "temperature": 0.0, "avg_logprob": -0.0946755588054657, "compression_ratio": 1.835680751173709, "no_speech_prob": 0.0008036668295972049}, {"id": 109, "seek": 66944, "start": 677.12, "end": 683.6800000000001, "text": " that minimize its value, okay, is gradient descent or gradient based algorithms. So gradient based", "tokens": [50748, 300, 17522, 1080, 2158, 11, 1392, 11, 307, 16235, 23475, 420, 16235, 2361, 14642, 13, 407, 16235, 2361, 51076], "temperature": 0.0, "avg_logprob": -0.0946755588054657, "compression_ratio": 1.835680751173709, "no_speech_prob": 0.0008036668295972049}, {"id": 110, "seek": 66944, "start": 683.6800000000001, "end": 689.5200000000001, "text": " algorithm makes the assumption that the function is somewhat smooth and mostly differentiable,", "tokens": [51076, 9284, 1669, 264, 15302, 300, 264, 2445, 307, 8344, 5508, 293, 5240, 819, 9364, 11, 51368], "temperature": 0.0, "avg_logprob": -0.0946755588054657, "compression_ratio": 1.835680751173709, "no_speech_prob": 0.0008036668295972049}, {"id": 111, "seek": 66944, "start": 689.5200000000001, "end": 694.4000000000001, "text": " doesn't have to be everywhere differentiable, but has to be continuous, has to be almost everywhere", "tokens": [51368, 1177, 380, 362, 281, 312, 5315, 819, 9364, 11, 457, 575, 281, 312, 10957, 11, 575, 281, 312, 1920, 5315, 51612], "temperature": 0.0, "avg_logprob": -0.0946755588054657, "compression_ratio": 1.835680751173709, "no_speech_prob": 0.0008036668295972049}, {"id": 112, "seek": 69440, "start": 694.4, "end": 700.9599999999999, "text": " differentiable. And it has to be somewhat smooth, otherwise, the local information of the slope", "tokens": [50364, 819, 9364, 13, 400, 309, 575, 281, 312, 8344, 5508, 11, 5911, 11, 264, 2654, 1589, 295, 264, 13525, 50692], "temperature": 0.0, "avg_logprob": -0.12849527475785236, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0015471907099708915}, {"id": 113, "seek": 69440, "start": 700.9599999999999, "end": 706.24, "text": " doesn't tell you much about where the minimum is. Okay, so here's an example here depicted on the right.", "tokens": [50692, 1177, 380, 980, 291, 709, 466, 689, 264, 7285, 307, 13, 1033, 11, 370, 510, 311, 364, 1365, 510, 30207, 322, 264, 558, 13, 50956], "temperature": 0.0, "avg_logprob": -0.12849527475785236, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0015471907099708915}, {"id": 114, "seek": 69440, "start": 708.16, "end": 713.12, "text": " The lines that you see here, the pink lines are the lines of equal cost and this cost is quadratic,", "tokens": [51052, 440, 3876, 300, 291, 536, 510, 11, 264, 7022, 3876, 366, 264, 3876, 295, 2681, 2063, 293, 341, 2063, 307, 37262, 11, 51300], "temperature": 0.0, "avg_logprob": -0.12849527475785236, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0015471907099708915}, {"id": 115, "seek": 69440, "start": 713.76, "end": 720.48, "text": " so it's basically a kind of paraboloid. And this is the trajectory of a method called stochastic", "tokens": [51332, 370, 309, 311, 1936, 257, 733, 295, 45729, 7902, 327, 13, 400, 341, 307, 264, 21512, 295, 257, 3170, 1219, 342, 8997, 2750, 51668], "temperature": 0.0, "avg_logprob": -0.12849527475785236, "compression_ratio": 1.6337448559670782, "no_speech_prob": 0.0015471907099708915}, {"id": 116, "seek": 72048, "start": 720.48, "end": 724.88, "text": " gradient descent, which we'll talk about in a minute. So for stochastic gradient descent, the", "tokens": [50364, 16235, 23475, 11, 597, 321, 603, 751, 466, 294, 257, 3456, 13, 407, 337, 342, 8997, 2750, 16235, 23475, 11, 264, 50584], "temperature": 0.0, "avg_logprob": -0.08583954760902807, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.00138769275508821}, {"id": 117, "seek": 72048, "start": 724.88, "end": 728.96, "text": " procedure is you show an example, you run you through the machine, you compute the objective", "tokens": [50584, 10747, 307, 291, 855, 364, 1365, 11, 291, 1190, 291, 807, 264, 3479, 11, 291, 14722, 264, 10024, 50788], "temperature": 0.0, "avg_logprob": -0.08583954760902807, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.00138769275508821}, {"id": 118, "seek": 72048, "start": 728.96, "end": 735.84, "text": " for that particular sample, and then you figure out by how much and how to modify each of the knobs", "tokens": [50788, 337, 300, 1729, 6889, 11, 293, 550, 291, 2573, 484, 538, 577, 709, 293, 577, 281, 16927, 1184, 295, 264, 46999, 51132], "temperature": 0.0, "avg_logprob": -0.08583954760902807, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.00138769275508821}, {"id": 119, "seek": 72048, "start": 735.84, "end": 740.88, "text": " in the machine, the W parameters, so that the objective function goes down by a little bit,", "tokens": [51132, 294, 264, 3479, 11, 264, 343, 9834, 11, 370, 300, 264, 10024, 2445, 1709, 760, 538, 257, 707, 857, 11, 51384], "temperature": 0.0, "avg_logprob": -0.08583954760902807, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.00138769275508821}, {"id": 120, "seek": 72048, "start": 740.88, "end": 744.5600000000001, "text": " you make that change, and then you go to the next sample. Let's be a little more formal.", "tokens": [51384, 291, 652, 300, 1319, 11, 293, 550, 291, 352, 281, 264, 958, 6889, 13, 961, 311, 312, 257, 707, 544, 9860, 13, 51568], "temperature": 0.0, "avg_logprob": -0.08583954760902807, "compression_ratio": 1.8171206225680934, "no_speech_prob": 0.00138769275508821}, {"id": 121, "seek": 74456, "start": 745.1999999999999, "end": 752.3199999999999, "text": " So gradient descent is this very basic algorithm here, you replace the value of W by its previous", "tokens": [50396, 407, 16235, 23475, 307, 341, 588, 3875, 9284, 510, 11, 291, 7406, 264, 2158, 295, 343, 538, 1080, 3894, 50752], "temperature": 0.0, "avg_logprob": -0.08769661256636696, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0008553459192626178}, {"id": 122, "seek": 74456, "start": 752.3199999999999, "end": 759.52, "text": " value minus a step size, eta here, multiplied by the gradient of the objective function with respect", "tokens": [50752, 2158, 3175, 257, 1823, 2744, 11, 32415, 510, 11, 17207, 538, 264, 16235, 295, 264, 10024, 2445, 365, 3104, 51112], "temperature": 0.0, "avg_logprob": -0.08769661256636696, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0008553459192626178}, {"id": 123, "seek": 74456, "start": 759.52, "end": 767.76, "text": " to the parameters. So what is a gradient? A gradient is a vector of the same size as the", "tokens": [51112, 281, 264, 9834, 13, 407, 437, 307, 257, 16235, 30, 316, 16235, 307, 257, 8062, 295, 264, 912, 2744, 382, 264, 51524], "temperature": 0.0, "avg_logprob": -0.08769661256636696, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0008553459192626178}, {"id": 124, "seek": 74456, "start": 767.76, "end": 773.28, "text": " parameter vector. And for each component of the parameter vector, it tells you by how much", "tokens": [51524, 13075, 8062, 13, 400, 337, 1184, 6542, 295, 264, 13075, 8062, 11, 309, 5112, 291, 538, 577, 709, 51800], "temperature": 0.0, "avg_logprob": -0.08769661256636696, "compression_ratio": 1.7339449541284404, "no_speech_prob": 0.0008553459192626178}, {"id": 125, "seek": 77328, "start": 774.0799999999999, "end": 780.24, "text": " the the loss function L would increase if you increase the parameter by a tiny amount.", "tokens": [50404, 264, 264, 4470, 2445, 441, 576, 3488, 498, 291, 3488, 264, 13075, 538, 257, 5870, 2372, 13, 50712], "temperature": 0.0, "avg_logprob": -0.14962513216080203, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.000855594698805362}, {"id": 126, "seek": 77328, "start": 780.24, "end": 785.28, "text": " Okay, it's a derivative, but it's a directional derivative, right? So let's say among all the", "tokens": [50712, 1033, 11, 309, 311, 257, 13760, 11, 457, 309, 311, 257, 42242, 13760, 11, 558, 30, 407, 718, 311, 584, 3654, 439, 264, 50964], "temperature": 0.0, "avg_logprob": -0.14962513216080203, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.000855594698805362}, {"id": 127, "seek": 77328, "start": 785.28, "end": 793.1999999999999, "text": " directions, you only look at W34. And as you imagine that you tweak W34 by a tiny amount,", "tokens": [50964, 11095, 11, 291, 787, 574, 412, 343, 12249, 13, 400, 382, 291, 3811, 300, 291, 29879, 343, 12249, 538, 257, 5870, 2372, 11, 51360], "temperature": 0.0, "avg_logprob": -0.14962513216080203, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.000855594698805362}, {"id": 128, "seek": 77328, "start": 794.0799999999999, "end": 799.92, "text": " the loss function curly L is going to increase by a tiny amount, you divide the tiny amount by", "tokens": [51404, 264, 4470, 2445, 32066, 441, 307, 516, 281, 3488, 538, 257, 5870, 2372, 11, 291, 9845, 264, 5870, 2372, 538, 51696], "temperature": 0.0, "avg_logprob": -0.14962513216080203, "compression_ratio": 1.806930693069307, "no_speech_prob": 0.000855594698805362}, {"id": 129, "seek": 79992, "start": 799.92, "end": 807.12, "text": " which L increase by the tiny amount that you modified this W34. And what you get is the gradient", "tokens": [50364, 597, 441, 3488, 538, 264, 5870, 2372, 300, 291, 15873, 341, 343, 12249, 13, 400, 437, 291, 483, 307, 264, 16235, 50724], "temperature": 0.0, "avg_logprob": -0.10387585288599918, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0007790614035911858}, {"id": 130, "seek": 79992, "start": 807.12, "end": 813.8399999999999, "text": " of the loss with respect to W34. If you do this for every single weight, you get the gradient", "tokens": [50724, 295, 264, 4470, 365, 3104, 281, 343, 12249, 13, 759, 291, 360, 341, 337, 633, 2167, 3364, 11, 291, 483, 264, 16235, 51060], "temperature": 0.0, "avg_logprob": -0.10387585288599918, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0007790614035911858}, {"id": 131, "seek": 79992, "start": 813.8399999999999, "end": 818.16, "text": " of the loss function with respect to all the weights. And it's a vector, which for each component", "tokens": [51060, 295, 264, 4470, 2445, 365, 3104, 281, 439, 264, 17443, 13, 400, 309, 311, 257, 8062, 11, 597, 337, 1184, 6542, 51276], "temperature": 0.0, "avg_logprob": -0.10387585288599918, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0007790614035911858}, {"id": 132, "seek": 79992, "start": 818.16, "end": 825.68, "text": " of the weight gives you the parameter gives you that quantity. Okay, so, you know, since Newton", "tokens": [51276, 295, 264, 3364, 2709, 291, 264, 13075, 2709, 291, 300, 11275, 13, 1033, 11, 370, 11, 291, 458, 11, 1670, 19541, 51652], "temperature": 0.0, "avg_logprob": -0.10387585288599918, "compression_ratio": 1.7534246575342465, "no_speech_prob": 0.0007790614035911858}, {"id": 133, "seek": 82568, "start": 825.68, "end": 831.28, "text": " and earlier, it's been written as, you know, dL over dW, because it indicates the fact that", "tokens": [50364, 293, 3071, 11, 309, 311, 668, 3720, 382, 11, 291, 458, 11, 274, 43, 670, 274, 54, 11, 570, 309, 16203, 264, 1186, 300, 50644], "temperature": 0.0, "avg_logprob": -0.14807164572118744, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0008039411040954292}, {"id": 134, "seek": 82568, "start": 831.28, "end": 836.0, "text": " there is this little twiddle where you can twiddle W by little. And there's a resulting", "tokens": [50644, 456, 307, 341, 707, 683, 327, 2285, 689, 291, 393, 683, 327, 2285, 343, 538, 707, 13, 400, 456, 311, 257, 16505, 50880], "temperature": 0.0, "avg_logprob": -0.14807164572118744, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0008039411040954292}, {"id": 135, "seek": 82568, "start": 836.88, "end": 840.9599999999999, "text": " twiddling of L. And if you divide those two twiddles, and they are infinitely small,", "tokens": [50924, 683, 14273, 1688, 295, 441, 13, 400, 498, 291, 9845, 729, 732, 683, 14273, 904, 11, 293, 436, 366, 36227, 1359, 11, 51128], "temperature": 0.0, "avg_logprob": -0.14807164572118744, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0008039411040954292}, {"id": 136, "seek": 82568, "start": 840.9599999999999, "end": 845.8399999999999, "text": " you get the derivative that's kind of standard notation in mathematics for a few hundred years.", "tokens": [51128, 291, 483, 264, 13760, 300, 311, 733, 295, 3832, 24657, 294, 18666, 337, 257, 1326, 3262, 924, 13, 51372], "temperature": 0.0, "avg_logprob": -0.14807164572118744, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0008039411040954292}, {"id": 137, "seek": 82568, "start": 845.8399999999999, "end": 855.3599999999999, "text": " Okay, so now the gradient is going to be a vector. Okay. And as indicated here on the top, right,", "tokens": [51372, 1033, 11, 370, 586, 264, 16235, 307, 516, 281, 312, 257, 8062, 13, 1033, 13, 400, 382, 16176, 510, 322, 264, 1192, 11, 558, 11, 51848], "temperature": 0.0, "avg_logprob": -0.14807164572118744, "compression_ratio": 1.7153558052434457, "no_speech_prob": 0.0008039411040954292}, {"id": 138, "seek": 85536, "start": 855.44, "end": 866.5600000000001, "text": " that vector is an arrow that points upwards along the line of larger slope. Okay, so if you are", "tokens": [50368, 300, 8062, 307, 364, 11610, 300, 2793, 22167, 2051, 264, 1622, 295, 4833, 13525, 13, 1033, 11, 370, 498, 291, 366, 50924], "temperature": 0.0, "avg_logprob": -0.08944119633855047, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.00020979340479243547}, {"id": 139, "seek": 85536, "start": 866.5600000000001, "end": 872.24, "text": " in a 2D surface, you have two W parameters. Okay, and the surface is represented here,", "tokens": [50924, 294, 257, 568, 35, 3753, 11, 291, 362, 732, 343, 9834, 13, 1033, 11, 293, 264, 3753, 307, 10379, 510, 11, 51208], "temperature": 0.0, "avg_logprob": -0.08944119633855047, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.00020979340479243547}, {"id": 140, "seek": 85536, "start": 872.88, "end": 878.88, "text": " some sort of quadratic ball here in this case. So it's a second degree polynomial in W1 and W0.", "tokens": [51240, 512, 1333, 295, 37262, 2594, 510, 294, 341, 1389, 13, 407, 309, 311, 257, 1150, 4314, 26110, 294, 343, 16, 293, 343, 15, 13, 51540], "temperature": 0.0, "avg_logprob": -0.08944119633855047, "compression_ratio": 1.4631578947368422, "no_speech_prob": 0.00020979340479243547}, {"id": 141, "seek": 87888, "start": 879.84, "end": 886.32, "text": " Here on the right is the kind of a top-down view of this where the lines represent the lines of", "tokens": [50412, 1692, 322, 264, 558, 307, 264, 733, 295, 257, 1192, 12, 5093, 1910, 295, 341, 689, 264, 3876, 2906, 264, 3876, 295, 50736], "temperature": 0.0, "avg_logprob": -0.1679224756028917, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005111848586238921}, {"id": 142, "seek": 87888, "start": 886.32, "end": 892.32, "text": " equal cost. The little arrow is here, represent the gradient at various locations. Okay,", "tokens": [50736, 2681, 2063, 13, 440, 707, 11610, 307, 510, 11, 2906, 264, 16235, 412, 3683, 9253, 13, 1033, 11, 51036], "temperature": 0.0, "avg_logprob": -0.1679224756028917, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005111848586238921}, {"id": 143, "seek": 87888, "start": 892.96, "end": 898.08, "text": " so you have a long arrow if the slope is steep, a short arrow if the slope is", "tokens": [51068, 370, 291, 362, 257, 938, 11610, 498, 264, 13525, 307, 16841, 11, 257, 2099, 11610, 498, 264, 13525, 307, 51324], "temperature": 0.0, "avg_logprob": -0.1679224756028917, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005111848586238921}, {"id": 144, "seek": 87888, "start": 899.76, "end": 906.72, "text": " not steep, not large. At the bottom, it's zero. And it points towards the direction of", "tokens": [51408, 406, 16841, 11, 406, 2416, 13, 1711, 264, 2767, 11, 309, 311, 4018, 13, 400, 309, 2793, 3030, 264, 3513, 295, 51756], "temperature": 0.0, "avg_logprob": -0.1679224756028917, "compression_ratio": 1.6778846153846154, "no_speech_prob": 0.0005111848586238921}, {"id": 145, "seek": 90672, "start": 907.28, "end": 912.48, "text": " higher slope. All right, so imagine you are in a landscape, a mountainous landscape,", "tokens": [50392, 2946, 13525, 13, 1057, 558, 11, 370, 3811, 291, 366, 294, 257, 9661, 11, 257, 6937, 563, 9661, 11, 50652], "temperature": 0.0, "avg_logprob": -0.10791811029961769, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0011874315096065402}, {"id": 146, "seek": 90672, "start": 913.28, "end": 918.64, "text": " and you're in a fog and you want to go down the valley. You look around you and you can tell", "tokens": [50692, 293, 291, 434, 294, 257, 13648, 293, 291, 528, 281, 352, 760, 264, 17636, 13, 509, 574, 926, 291, 293, 291, 393, 980, 50960], "temperature": 0.0, "avg_logprob": -0.10791811029961769, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0011874315096065402}, {"id": 147, "seek": 90672, "start": 919.9200000000001, "end": 927.12, "text": " the local slope of the landscape. You can't tell where the minimum is because you're in a fog,", "tokens": [51024, 264, 2654, 13525, 295, 264, 9661, 13, 509, 393, 380, 980, 689, 264, 7285, 307, 570, 291, 434, 294, 257, 13648, 11, 51384], "temperature": 0.0, "avg_logprob": -0.10791811029961769, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0011874315096065402}, {"id": 148, "seek": 90672, "start": 927.76, "end": 934.4, "text": " but you can tell the local slope. So you can figure out what is the direction of larger slope", "tokens": [51416, 457, 291, 393, 980, 264, 2654, 13525, 13, 407, 291, 393, 2573, 484, 437, 307, 264, 3513, 295, 4833, 13525, 51748], "temperature": 0.0, "avg_logprob": -0.10791811029961769, "compression_ratio": 1.8484848484848484, "no_speech_prob": 0.0011874315096065402}, {"id": 149, "seek": 93440, "start": 934.4, "end": 939.6, "text": " and then take a step and it will take you upwards. Now you turn around 180 degrees,", "tokens": [50364, 293, 550, 747, 257, 1823, 293, 309, 486, 747, 291, 22167, 13, 823, 291, 1261, 926, 11971, 5310, 11, 50624], "temperature": 0.0, "avg_logprob": -0.11146046898581764, "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.0006460155709646642}, {"id": 150, "seek": 93440, "start": 939.6, "end": 944.64, "text": " take a step in that direction, and that is going to take you downwards. If you keep doing this", "tokens": [50624, 747, 257, 1823, 294, 300, 3513, 11, 293, 300, 307, 516, 281, 747, 291, 39880, 13, 759, 291, 1066, 884, 341, 50876], "temperature": 0.0, "avg_logprob": -0.11146046898581764, "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.0006460155709646642}, {"id": 151, "seek": 93440, "start": 944.64, "end": 950.56, "text": " and the landscape is convex, which means it has only one local minimum, this will eventually", "tokens": [50876, 293, 264, 9661, 307, 42432, 11, 597, 1355, 309, 575, 787, 472, 2654, 7285, 11, 341, 486, 4728, 51172], "temperature": 0.0, "avg_logprob": -0.11146046898581764, "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.0006460155709646642}, {"id": 152, "seek": 93440, "start": 951.52, "end": 957.52, "text": " take you down to the valley and presumably to the village. Right, so that's gradient-based", "tokens": [51220, 747, 291, 760, 281, 264, 17636, 293, 26742, 281, 264, 7288, 13, 1779, 11, 370, 300, 311, 16235, 12, 6032, 51520], "temperature": 0.0, "avg_logprob": -0.11146046898581764, "compression_ratio": 1.6454545454545455, "no_speech_prob": 0.0006460155709646642}, {"id": 153, "seek": 95752, "start": 957.52, "end": 967.84, "text": " algorithms. They all differ by how you compute the gradient first and by what this eta step-size", "tokens": [50364, 14642, 13, 814, 439, 743, 538, 577, 291, 14722, 264, 16235, 700, 293, 538, 437, 341, 32415, 1823, 12, 27553, 50880], "temperature": 0.0, "avg_logprob": -0.1209717918844784, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.00599928991869092}, {"id": 154, "seek": 95752, "start": 967.84, "end": 975.12, "text": " parameter is. So in simple forms, eta is just a positive constant that sometimes is decreased as", "tokens": [50880, 13075, 307, 13, 407, 294, 2199, 6422, 11, 32415, 307, 445, 257, 3353, 5754, 300, 2171, 307, 24436, 382, 51244], "temperature": 0.0, "avg_logprob": -0.1209717918844784, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.00599928991869092}, {"id": 155, "seek": 95752, "start": 975.12, "end": 983.76, "text": " the system learns more, but most of the time not. But in more complex versions of gradient-based", "tokens": [51244, 264, 1185, 27152, 544, 11, 457, 881, 295, 264, 565, 406, 13, 583, 294, 544, 3997, 9606, 295, 16235, 12, 6032, 51676], "temperature": 0.0, "avg_logprob": -0.1209717918844784, "compression_ratio": 1.5025906735751295, "no_speech_prob": 0.00599928991869092}, {"id": 156, "seek": 98376, "start": 983.76, "end": 989.2, "text": " learning, eta is actually an entire matrix itself, generally a positive definite or semi-definite", "tokens": [50364, 2539, 11, 32415, 307, 767, 364, 2302, 8141, 2564, 11, 5101, 257, 3353, 25131, 420, 12909, 12, 1479, 5194, 642, 50636], "temperature": 0.0, "avg_logprob": -0.097998833388425, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00041710148798301816}, {"id": 157, "seek": 98376, "start": 989.2, "end": 997.68, "text": " matrix. And so the direction adopted by those algorithms is not necessarily the steepest descent.", "tokens": [50636, 8141, 13, 400, 370, 264, 3513, 12175, 538, 729, 14642, 307, 406, 4725, 264, 16841, 377, 23475, 13, 51060], "temperature": 0.0, "avg_logprob": -0.097998833388425, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00041710148798301816}, {"id": 158, "seek": 98376, "start": 997.68, "end": 1003.36, "text": " It goes downwards, but it's not necessarily the steepest descent. And we can see why here. So", "tokens": [51060, 467, 1709, 39880, 11, 457, 309, 311, 406, 4725, 264, 16841, 377, 23475, 13, 400, 321, 393, 536, 983, 510, 13, 407, 51344], "temperature": 0.0, "avg_logprob": -0.097998833388425, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00041710148798301816}, {"id": 159, "seek": 98376, "start": 1004.48, "end": 1009.4399999999999, "text": " in this diagram here that I'm showing, this is a trajectory that will be followed by gradient", "tokens": [51400, 294, 341, 10686, 510, 300, 286, 478, 4099, 11, 341, 307, 257, 21512, 300, 486, 312, 6263, 538, 16235, 51648], "temperature": 0.0, "avg_logprob": -0.097998833388425, "compression_ratio": 1.7022222222222223, "no_speech_prob": 0.00041710148798301816}, {"id": 160, "seek": 100944, "start": 1009.44, "end": 1015.5200000000001, "text": " descent in this quadratic cost environment. And as you see, the trajectory is not straight.", "tokens": [50364, 23475, 294, 341, 37262, 2063, 2823, 13, 400, 382, 291, 536, 11, 264, 21512, 307, 406, 2997, 13, 50668], "temperature": 0.0, "avg_logprob": -0.0860103496070047, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.001244381070137024}, {"id": 161, "seek": 100944, "start": 1016.1600000000001, "end": 1022.08, "text": " It's not straight because the system goes down by following the slope of steepest descent. And so", "tokens": [50700, 467, 311, 406, 2997, 570, 264, 1185, 1709, 760, 538, 3480, 264, 13525, 295, 16841, 377, 23475, 13, 400, 370, 50996], "temperature": 0.0, "avg_logprob": -0.0860103496070047, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.001244381070137024}, {"id": 162, "seek": 100944, "start": 1023.2800000000001, "end": 1026.64, "text": " it goes down the valley before finding the minimum of the valley, if you want.", "tokens": [51056, 309, 1709, 760, 264, 17636, 949, 5006, 264, 7285, 295, 264, 17636, 11, 498, 291, 528, 13, 51224], "temperature": 0.0, "avg_logprob": -0.0860103496070047, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.001244381070137024}, {"id": 163, "seek": 100944, "start": 1027.28, "end": 1032.16, "text": " So if your cost function is a little squeezed in one direction, it will go down the ravine", "tokens": [51256, 407, 498, 428, 2063, 2445, 307, 257, 707, 39470, 294, 472, 3513, 11, 309, 486, 352, 760, 264, 32987, 533, 51500], "temperature": 0.0, "avg_logprob": -0.0860103496070047, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.001244381070137024}, {"id": 164, "seek": 100944, "start": 1032.16, "end": 1037.28, "text": " and then follow the ravine towards the bottom. In complex situations where you have", "tokens": [51500, 293, 550, 1524, 264, 32987, 533, 3030, 264, 2767, 13, 682, 3997, 6851, 689, 291, 362, 51756], "temperature": 0.0, "avg_logprob": -0.0860103496070047, "compression_ratio": 1.7509881422924902, "no_speech_prob": 0.001244381070137024}, {"id": 165, "seek": 103728, "start": 1038.0, "end": 1044.3999999999999, "text": " things that are, the trajectory actually is being cut here. But when the", "tokens": [50400, 721, 300, 366, 11, 264, 21512, 767, 307, 885, 1723, 510, 13, 583, 562, 264, 50720], "temperature": 0.0, "avg_logprob": -0.200404421488444, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.0011682686163112521}, {"id": 166, "seek": 103728, "start": 1044.3999999999999, "end": 1050.72, "text": " weather function is highly irregular, this might even be more complicated. And then you might have", "tokens": [50720, 5503, 2445, 307, 5405, 29349, 11, 341, 1062, 754, 312, 544, 6179, 13, 400, 550, 291, 1062, 362, 51036], "temperature": 0.0, "avg_logprob": -0.200404421488444, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.0011682686163112521}, {"id": 167, "seek": 103728, "start": 1050.72, "end": 1055.44, "text": " to be smart about what you do here. Okay. So stochastic gradient descent is", "tokens": [51036, 281, 312, 4069, 466, 437, 291, 360, 510, 13, 1033, 13, 407, 342, 8997, 2750, 16235, 23475, 307, 51272], "temperature": 0.0, "avg_logprob": -0.200404421488444, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.0011682686163112521}, {"id": 168, "seek": 103728, "start": 1056.48, "end": 1066.56, "text": " universally used in deep learning. And this is a slight modification of the gradient", "tokens": [51324, 43995, 1143, 294, 2452, 2539, 13, 400, 341, 307, 257, 4036, 26747, 295, 264, 16235, 51828], "temperature": 0.0, "avg_logprob": -0.200404421488444, "compression_ratio": 1.5809523809523809, "no_speech_prob": 0.0011682686163112521}, {"id": 169, "seek": 106728, "start": 1067.44, "end": 1074.96, "text": " steepest descent algorithm where you don't compute the gradient of the entire objective function", "tokens": [50372, 16841, 377, 23475, 9284, 689, 291, 500, 380, 14722, 264, 16235, 295, 264, 2302, 10024, 2445, 50748], "temperature": 0.0, "avg_logprob": -0.09034797444063074, "compression_ratio": 1.9447236180904524, "no_speech_prob": 0.0005440461682155728}, {"id": 170, "seek": 106728, "start": 1074.96, "end": 1082.32, "text": " averaged over all the samples. But what you do is you take one sample and you compute the gradient", "tokens": [50748, 18247, 2980, 670, 439, 264, 10938, 13, 583, 437, 291, 360, 307, 291, 747, 472, 6889, 293, 291, 14722, 264, 16235, 51116], "temperature": 0.0, "avg_logprob": -0.09034797444063074, "compression_ratio": 1.9447236180904524, "no_speech_prob": 0.0005440461682155728}, {"id": 171, "seek": 106728, "start": 1082.32, "end": 1087.6, "text": " of the objective function for that one sample with respect to the parameters and you take a step.", "tokens": [51116, 295, 264, 10024, 2445, 337, 300, 472, 6889, 365, 3104, 281, 264, 9834, 293, 291, 747, 257, 1823, 13, 51380], "temperature": 0.0, "avg_logprob": -0.09034797444063074, "compression_ratio": 1.9447236180904524, "no_speech_prob": 0.0005440461682155728}, {"id": 172, "seek": 106728, "start": 1088.24, "end": 1094.16, "text": " Okay. And you keep doing this. You pick another sample, compute the gradient of the objective", "tokens": [51412, 1033, 13, 400, 291, 1066, 884, 341, 13, 509, 1888, 1071, 6889, 11, 14722, 264, 16235, 295, 264, 10024, 51708], "temperature": 0.0, "avg_logprob": -0.09034797444063074, "compression_ratio": 1.9447236180904524, "no_speech_prob": 0.0005440461682155728}, {"id": 173, "seek": 109416, "start": 1094.16, "end": 1099.76, "text": " function for that sample with respect to the way it's making a date. Why is it called stochastic", "tokens": [50364, 2445, 337, 300, 6889, 365, 3104, 281, 264, 636, 309, 311, 1455, 257, 4002, 13, 1545, 307, 309, 1219, 342, 8997, 2750, 50644], "temperature": 0.0, "avg_logprob": -0.08488591857578444, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.0006460940348915756}, {"id": 174, "seek": 109416, "start": 1099.76, "end": 1108.8000000000002, "text": " gradient? Stochastic is a fancy term for random, essentially. And it's called stochastic because", "tokens": [50644, 16235, 30, 745, 8997, 2750, 307, 257, 10247, 1433, 337, 4974, 11, 4476, 13, 400, 309, 311, 1219, 342, 8997, 2750, 570, 51096], "temperature": 0.0, "avg_logprob": -0.08488591857578444, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.0006460940348915756}, {"id": 175, "seek": 109416, "start": 1108.8000000000002, "end": 1115.1200000000001, "text": " the evaluation of the gradient you get on the basis of a single sample is a noisy estimate of", "tokens": [51096, 264, 13344, 295, 264, 16235, 291, 483, 322, 264, 5143, 295, 257, 2167, 6889, 307, 257, 24518, 12539, 295, 51412], "temperature": 0.0, "avg_logprob": -0.08488591857578444, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.0006460940348915756}, {"id": 176, "seek": 109416, "start": 1115.1200000000001, "end": 1120.24, "text": " the full gradient. The average of the gradients, because the gradient is a linear operation,", "tokens": [51412, 264, 1577, 16235, 13, 440, 4274, 295, 264, 2771, 2448, 11, 570, 264, 16235, 307, 257, 8213, 6916, 11, 51668], "temperature": 0.0, "avg_logprob": -0.08488591857578444, "compression_ratio": 1.7757009345794392, "no_speech_prob": 0.0006460940348915756}, {"id": 177, "seek": 112024, "start": 1120.24, "end": 1124.8, "text": " the average of the gradients will be the gradient of the average. And so things work out. If you", "tokens": [50364, 264, 4274, 295, 264, 2771, 2448, 486, 312, 264, 16235, 295, 264, 4274, 13, 400, 370, 721, 589, 484, 13, 759, 291, 50592], "temperature": 0.0, "avg_logprob": -0.08449903818277213, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0010159939993172884}, {"id": 178, "seek": 112024, "start": 1124.8, "end": 1130.16, "text": " compute the gradient and you kind of keep going, overall, the average trajectory will be sort of", "tokens": [50592, 14722, 264, 16235, 293, 291, 733, 295, 1066, 516, 11, 4787, 11, 264, 4274, 21512, 486, 312, 1333, 295, 50860], "temperature": 0.0, "avg_logprob": -0.08449903818277213, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0010159939993172884}, {"id": 179, "seek": 112024, "start": 1130.16, "end": 1136.72, "text": " the trajectory you would have followed by doing full gradient. Okay. But in fact,", "tokens": [50860, 264, 21512, 291, 576, 362, 6263, 538, 884, 1577, 16235, 13, 1033, 13, 583, 294, 1186, 11, 51188], "temperature": 0.0, "avg_logprob": -0.08449903818277213, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0010159939993172884}, {"id": 180, "seek": 112024, "start": 1137.6, "end": 1142.48, "text": " the reason we're doing this is because it's much more efficient in terms of speed of convergence. So", "tokens": [51232, 264, 1778, 321, 434, 884, 341, 307, 570, 309, 311, 709, 544, 7148, 294, 2115, 295, 3073, 295, 32181, 13, 407, 51476], "temperature": 0.0, "avg_logprob": -0.08449903818277213, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0010159939993172884}, {"id": 181, "seek": 112024, "start": 1143.92, "end": 1147.52, "text": " although the trajectory followed by stochastic gradient is very noisy,", "tokens": [51548, 4878, 264, 21512, 6263, 538, 342, 8997, 2750, 16235, 307, 588, 24518, 11, 51728], "temperature": 0.0, "avg_logprob": -0.08449903818277213, "compression_ratio": 1.8170731707317074, "no_speech_prob": 0.0010159939993172884}, {"id": 182, "seek": 114752, "start": 1147.52, "end": 1151.76, "text": " things kind of bounce around a lot. As you can see in the trajectory here at the bottom,", "tokens": [50364, 721, 733, 295, 15894, 926, 257, 688, 13, 1018, 291, 393, 536, 294, 264, 21512, 510, 412, 264, 2767, 11, 50576], "temperature": 0.0, "avg_logprob": -0.10832333057484728, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0005611946107819676}, {"id": 183, "seek": 114752, "start": 1153.28, "end": 1158.08, "text": " you know, things have, the trajectory is very erratic. But in fact, it goes to the bottom faster", "tokens": [50652, 291, 458, 11, 721, 362, 11, 264, 21512, 307, 588, 1189, 25198, 13, 583, 294, 1186, 11, 309, 1709, 281, 264, 2767, 4663, 50892], "temperature": 0.0, "avg_logprob": -0.10832333057484728, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0005611946107819676}, {"id": 184, "seek": 114752, "start": 1158.96, "end": 1164.6399999999999, "text": " and has other advantages that people are still writing papers on. Okay. The reason for that is", "tokens": [50936, 293, 575, 661, 14906, 300, 561, 366, 920, 3579, 10577, 322, 13, 1033, 13, 440, 1778, 337, 300, 307, 51220], "temperature": 0.0, "avg_logprob": -0.10832333057484728, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0005611946107819676}, {"id": 185, "seek": 114752, "start": 1164.6399999999999, "end": 1172.32, "text": " that stochastic gradient exploits the redundancy between the samples. So all the, you know, machine", "tokens": [51220, 300, 342, 8997, 2750, 16235, 12382, 1208, 264, 27830, 6717, 1296, 264, 10938, 13, 407, 439, 264, 11, 291, 458, 11, 3479, 51604], "temperature": 0.0, "avg_logprob": -0.10832333057484728, "compression_ratio": 1.645021645021645, "no_speech_prob": 0.0005611946107819676}, {"id": 186, "seek": 117232, "start": 1172.32, "end": 1177.52, "text": " learning setting, the training samples have some similarities between them. If they don't, then", "tokens": [50364, 2539, 3287, 11, 264, 3097, 10938, 362, 512, 24197, 1296, 552, 13, 759, 436, 500, 380, 11, 550, 50624], "temperature": 0.0, "avg_logprob": -0.07702228656181923, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.0006770129548385739}, {"id": 187, "seek": 117232, "start": 1177.52, "end": 1182.56, "text": " basically the learning problem is impossible. So they necessarily do have some redundancy", "tokens": [50624, 1936, 264, 2539, 1154, 307, 6243, 13, 407, 436, 4725, 360, 362, 512, 27830, 6717, 50876], "temperature": 0.0, "avg_logprob": -0.07702228656181923, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.0006770129548385739}, {"id": 188, "seek": 117232, "start": 1182.56, "end": 1186.8799999999999, "text": " between them. And the faster you update the parameters, the more you, the more often you", "tokens": [50876, 1296, 552, 13, 400, 264, 4663, 291, 5623, 264, 9834, 11, 264, 544, 291, 11, 264, 544, 2049, 291, 51092], "temperature": 0.0, "avg_logprob": -0.07702228656181923, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.0006770129548385739}, {"id": 189, "seek": 117232, "start": 1186.8799999999999, "end": 1192.96, "text": " update them, the more you exploit this redundancy between those parameters. Now in practice, what", "tokens": [51092, 5623, 552, 11, 264, 544, 291, 25924, 341, 27830, 6717, 1296, 729, 9834, 13, 823, 294, 3124, 11, 437, 51396], "temperature": 0.0, "avg_logprob": -0.07702228656181923, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.0006770129548385739}, {"id": 190, "seek": 117232, "start": 1192.96, "end": 1199.2, "text": " people do is they use mini batches. So instead of computing the gradient on the basis of a single", "tokens": [51396, 561, 360, 307, 436, 764, 8382, 15245, 279, 13, 407, 2602, 295, 15866, 264, 16235, 322, 264, 5143, 295, 257, 2167, 51708], "temperature": 0.0, "avg_logprob": -0.07702228656181923, "compression_ratio": 1.8577075098814229, "no_speech_prob": 0.0006770129548385739}, {"id": 191, "seek": 119920, "start": 1199.2, "end": 1206.8, "text": " sample, you take a batch of samples, typically anywhere between let's say 30 and a few thousand.", "tokens": [50364, 6889, 11, 291, 747, 257, 15245, 295, 10938, 11, 5850, 4992, 1296, 718, 311, 584, 2217, 293, 257, 1326, 4714, 13, 50744], "temperature": 0.0, "avg_logprob": -0.18111260358025047, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.0010984480613842607}, {"id": 192, "seek": 119920, "start": 1208.0800000000002, "end": 1212.88, "text": " But smaller batches are better in most cases actually. And you compute the average of the", "tokens": [50808, 583, 4356, 15245, 279, 366, 1101, 294, 881, 3331, 767, 13, 400, 291, 14722, 264, 4274, 295, 264, 51048], "temperature": 0.0, "avg_logprob": -0.18111260358025047, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.0010984480613842607}, {"id": 193, "seek": 119920, "start": 1213.92, "end": 1221.2, "text": " gradient over those samples. Okay. So compute the average cost over those samples and compute the", "tokens": [51100, 16235, 670, 729, 10938, 13, 1033, 13, 407, 14722, 264, 4274, 2063, 670, 729, 10938, 293, 14722, 264, 51464], "temperature": 0.0, "avg_logprob": -0.18111260358025047, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.0010984480613842607}, {"id": 194, "seek": 119920, "start": 1221.2, "end": 1227.76, "text": " gradient of the average over those samples and then make an update. The reason for doing this", "tokens": [51464, 16235, 295, 264, 4274, 670, 729, 10938, 293, 550, 652, 364, 5623, 13, 440, 1778, 337, 884, 341, 51792], "temperature": 0.0, "avg_logprob": -0.18111260358025047, "compression_ratio": 1.8173076923076923, "no_speech_prob": 0.0010984480613842607}, {"id": 195, "seek": 122776, "start": 1228.48, "end": 1235.6, "text": " is not intrinsically an algorithmic reason. It's because it's a simple way of parallelizing", "tokens": [50400, 307, 406, 28621, 984, 364, 9284, 299, 1778, 13, 467, 311, 570, 309, 311, 257, 2199, 636, 295, 8952, 3319, 50756], "temperature": 0.0, "avg_logprob": -0.13466043675199468, "compression_ratio": 1.575, "no_speech_prob": 0.0007398563320748508}, {"id": 196, "seek": 122776, "start": 1236.56, "end": 1242.32, "text": " stochastic gradients on parallel hardware such as GPUs. Okay. So there's never, there's no good", "tokens": [50804, 342, 8997, 2750, 2771, 2448, 322, 8952, 8837, 1270, 382, 18407, 82, 13, 1033, 13, 407, 456, 311, 1128, 11, 456, 311, 572, 665, 51092], "temperature": 0.0, "avg_logprob": -0.13466043675199468, "compression_ratio": 1.575, "no_speech_prob": 0.0007398563320748508}, {"id": 197, "seek": 122776, "start": 1242.32, "end": 1248.8799999999999, "text": " reason to do batching other than the fact that our hardware likes it. Okay. Question. Yeah. So", "tokens": [51092, 1778, 281, 360, 15245, 278, 661, 813, 264, 1186, 300, 527, 8837, 5902, 309, 13, 1033, 13, 14464, 13, 865, 13, 407, 51420], "temperature": 0.0, "avg_logprob": -0.13466043675199468, "compression_ratio": 1.575, "no_speech_prob": 0.0007398563320748508}, {"id": 198, "seek": 122776, "start": 1249.6, "end": 1254.8799999999999, "text": " for actually for, for real complex deep learning problems, does this objecting function have to", "tokens": [51456, 337, 767, 337, 11, 337, 957, 3997, 2452, 2539, 2740, 11, 775, 341, 2657, 278, 2445, 362, 281, 51720], "temperature": 0.0, "avg_logprob": -0.13466043675199468, "compression_ratio": 1.575, "no_speech_prob": 0.0007398563320748508}, {"id": 199, "seek": 125488, "start": 1254.88, "end": 1260.88, "text": " be continuously differentiable? Well, it needs to be continuous mostly. If it's non continuous,", "tokens": [50364, 312, 15684, 819, 9364, 30, 1042, 11, 309, 2203, 281, 312, 10957, 5240, 13, 759, 309, 311, 2107, 10957, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10795674772344084, "compression_ratio": 1.9233870967741935, "no_speech_prob": 0.0005875564529560506}, {"id": 200, "seek": 125488, "start": 1260.88, "end": 1268.96, "text": " you're going to get in trouble. It needs to be differentiable almost everywhere. But in fact,", "tokens": [50664, 291, 434, 516, 281, 483, 294, 5253, 13, 467, 2203, 281, 312, 819, 9364, 1920, 5315, 13, 583, 294, 1186, 11, 51068], "temperature": 0.0, "avg_logprob": -0.10795674772344084, "compression_ratio": 1.9233870967741935, "no_speech_prob": 0.0005875564529560506}, {"id": 201, "seek": 125488, "start": 1270.3200000000002, "end": 1274.64, "text": " neural nets that most people use are actually not differentiable. And there's a lot of places", "tokens": [51136, 18161, 36170, 300, 881, 561, 764, 366, 767, 406, 819, 9364, 13, 400, 456, 311, 257, 688, 295, 3190, 51352], "temperature": 0.0, "avg_logprob": -0.10795674772344084, "compression_ratio": 1.9233870967741935, "no_speech_prob": 0.0005875564529560506}, {"id": 202, "seek": 125488, "start": 1274.64, "end": 1278.4, "text": " where they're not differentiable. But they are continuous in the sense that there are functions", "tokens": [51352, 689, 436, 434, 406, 819, 9364, 13, 583, 436, 366, 10957, 294, 264, 2020, 300, 456, 366, 6828, 51540], "temperature": 0.0, "avg_logprob": -0.10795674772344084, "compression_ratio": 1.9233870967741935, "no_speech_prob": 0.0005875564529560506}, {"id": 203, "seek": 125488, "start": 1278.4, "end": 1283.3600000000001, "text": " that have kind of corners in them, if you want. They have kinks. And if you have a kink once in a", "tokens": [51540, 300, 362, 733, 295, 12413, 294, 552, 11, 498, 291, 528, 13, 814, 362, 350, 16431, 13, 400, 498, 291, 362, 257, 350, 475, 1564, 294, 257, 51788], "temperature": 0.0, "avg_logprob": -0.10795674772344084, "compression_ratio": 1.9233870967741935, "no_speech_prob": 0.0005875564529560506}, {"id": 204, "seek": 128336, "start": 1283.36, "end": 1294.8799999999999, "text": " while, it's not too much of a problem. But so in that case, those quantities should not be called", "tokens": [50364, 1339, 11, 309, 311, 406, 886, 709, 295, 257, 1154, 13, 583, 370, 294, 300, 1389, 11, 729, 22927, 820, 406, 312, 1219, 50940], "temperature": 0.0, "avg_logprob": -0.10430993260564031, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.000295833102427423}, {"id": 205, "seek": 128336, "start": 1294.8799999999999, "end": 1299.84, "text": " gradients, they should be called subgradients. Okay. So a sub gradient is basically a generalization", "tokens": [50940, 2771, 2448, 11, 436, 820, 312, 1219, 1422, 7165, 2448, 13, 1033, 13, 407, 257, 1422, 16235, 307, 1936, 257, 2674, 2144, 51188], "temperature": 0.0, "avg_logprob": -0.10430993260564031, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.000295833102427423}, {"id": 206, "seek": 128336, "start": 1299.84, "end": 1307.36, "text": " of the idea of derivative or gradient to functions that have kinks in them. So wherever you have a", "tokens": [51188, 295, 264, 1558, 295, 13760, 420, 16235, 281, 6828, 300, 362, 350, 16431, 294, 552, 13, 407, 8660, 291, 362, 257, 51564], "temperature": 0.0, "avg_logprob": -0.10430993260564031, "compression_ratio": 1.596774193548387, "no_speech_prob": 0.000295833102427423}, {"id": 207, "seek": 130736, "start": 1307.36, "end": 1313.76, "text": " function that has a kink in it, any, any slope that is between the slope of one, one side and the", "tokens": [50364, 2445, 300, 575, 257, 350, 475, 294, 309, 11, 604, 11, 604, 13525, 300, 307, 1296, 264, 13525, 295, 472, 11, 472, 1252, 293, 264, 50684], "temperature": 0.0, "avg_logprob": -0.13904339926583426, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0009104972123168409}, {"id": 208, "seek": 130736, "start": 1313.76, "end": 1322.3999999999999, "text": " slope of the other side is a, is a valid sub gradient. Okay. So when you write the kink,", "tokens": [50684, 13525, 295, 264, 661, 1252, 307, 257, 11, 307, 257, 7363, 1422, 16235, 13, 1033, 13, 407, 562, 291, 2464, 264, 350, 475, 11, 51116], "temperature": 0.0, "avg_logprob": -0.13904339926583426, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0009104972123168409}, {"id": 209, "seek": 130736, "start": 1322.3999999999999, "end": 1326.7199999999998, "text": " you decide, well, the derivative is this or it's that or it's going to somewhere in between. And", "tokens": [51116, 291, 4536, 11, 731, 11, 264, 13760, 307, 341, 420, 309, 311, 300, 420, 309, 311, 516, 281, 4079, 294, 1296, 13, 400, 51332], "temperature": 0.0, "avg_logprob": -0.13904339926583426, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0009104972123168409}, {"id": 210, "seek": 130736, "start": 1326.7199999999998, "end": 1335.4399999999998, "text": " you're fine. Most of the proof that applied to, you know, smooth functions, you know, in terms", "tokens": [51332, 291, 434, 2489, 13, 4534, 295, 264, 8177, 300, 6456, 281, 11, 291, 458, 11, 5508, 6828, 11, 291, 458, 11, 294, 2115, 51768], "temperature": 0.0, "avg_logprob": -0.13904339926583426, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.0009104972123168409}, {"id": 211, "seek": 133544, "start": 1335.44, "end": 1342.8, "text": " of minimization, often apply also to non-smooth function that basically are differentiable most", "tokens": [50364, 295, 4464, 2144, 11, 2049, 3079, 611, 281, 2107, 12, 82, 30803, 2445, 300, 1936, 366, 819, 9364, 881, 50732], "temperature": 0.0, "avg_logprob": -0.10254370172818501, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0006057436694391072}, {"id": 212, "seek": 133544, "start": 1342.8, "end": 1350.0, "text": " of the way. So then how do we ensure strict convexity? We do not ensure strict convexity. The,", "tokens": [50732, 295, 264, 636, 13, 407, 550, 577, 360, 321, 5586, 10910, 42432, 507, 30, 492, 360, 406, 5586, 10910, 42432, 507, 13, 440, 11, 51092], "temperature": 0.0, "avg_logprob": -0.10254370172818501, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0006057436694391072}, {"id": 213, "seek": 133544, "start": 1351.3600000000001, "end": 1357.04, "text": " in fact, in deep learning systems, most deep learning systems, the function that we are", "tokens": [51160, 294, 1186, 11, 294, 2452, 2539, 3652, 11, 881, 2452, 2539, 3652, 11, 264, 2445, 300, 321, 366, 51444], "temperature": 0.0, "avg_logprob": -0.10254370172818501, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0006057436694391072}, {"id": 214, "seek": 133544, "start": 1357.04, "end": 1363.04, "text": " optimizing is non-convex, right? In fact, this is one reason why it took so long for deep learning", "tokens": [51444, 40425, 307, 2107, 12, 1671, 303, 87, 11, 558, 30, 682, 1186, 11, 341, 307, 472, 1778, 983, 309, 1890, 370, 938, 337, 2452, 2539, 51744], "temperature": 0.0, "avg_logprob": -0.10254370172818501, "compression_ratio": 1.7534883720930232, "no_speech_prob": 0.0006057436694391072}, {"id": 215, "seek": 136304, "start": 1363.04, "end": 1368.0, "text": " to become prominent is because a lot of people, particularly theoreticians, people who sort of", "tokens": [50364, 281, 1813, 17034, 307, 570, 257, 688, 295, 561, 11, 4098, 14308, 8455, 11, 561, 567, 1333, 295, 50612], "temperature": 0.0, "avg_logprob": -0.12179564672803121, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.002798074623569846}, {"id": 216, "seek": 136304, "start": 1368.0, "end": 1372.56, "text": " theoretically minded, were very scared of the idea that you had to minimize a non-convex", "tokens": [50612, 29400, 36707, 11, 645, 588, 5338, 295, 264, 1558, 300, 291, 632, 281, 17522, 257, 2107, 12, 1671, 303, 87, 50840], "temperature": 0.0, "avg_logprob": -0.12179564672803121, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.002798074623569846}, {"id": 217, "seek": 136304, "start": 1372.56, "end": 1376.56, "text": " objective and they say, this can't possibly work because we can't prove anything about it.", "tokens": [50840, 10024, 293, 436, 584, 11, 341, 393, 380, 6264, 589, 570, 321, 393, 380, 7081, 1340, 466, 309, 13, 51040], "temperature": 0.0, "avg_logprob": -0.12179564672803121, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.002798074623569846}, {"id": 218, "seek": 136304, "start": 1376.56, "end": 1380.24, "text": " It turns out it does work. You can't prove anything about it, but it does work. And so", "tokens": [51040, 467, 4523, 484, 309, 775, 589, 13, 509, 393, 380, 7081, 1340, 466, 309, 11, 457, 309, 775, 589, 13, 400, 370, 51224], "temperature": 0.0, "avg_logprob": -0.12179564672803121, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.002798074623569846}, {"id": 219, "seek": 136304, "start": 1381.04, "end": 1387.44, "text": " this is a situation, and it's an interesting thing to think about, a situation where the,", "tokens": [51264, 341, 307, 257, 2590, 11, 293, 309, 311, 364, 1880, 551, 281, 519, 466, 11, 257, 2590, 689, 264, 11, 51584], "temperature": 0.0, "avg_logprob": -0.12179564672803121, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.002798074623569846}, {"id": 220, "seek": 136304, "start": 1387.44, "end": 1391.6, "text": " the theoretical thinking basically limited what people could do in terms of engineering", "tokens": [51584, 264, 20864, 1953, 1936, 5567, 437, 561, 727, 360, 294, 2115, 295, 7043, 51792], "temperature": 0.0, "avg_logprob": -0.12179564672803121, "compression_ratio": 1.8586206896551725, "no_speech_prob": 0.002798074623569846}, {"id": 221, "seek": 139160, "start": 1392.48, "end": 1396.24, "text": " because they couldn't prove things about it. But that would be actually very powerful.", "tokens": [50408, 570, 436, 2809, 380, 7081, 721, 466, 309, 13, 583, 300, 576, 312, 767, 588, 4005, 13, 50596], "temperature": 0.0, "avg_logprob": -0.23278325632077837, "compression_ratio": 1.5350553505535056, "no_speech_prob": 0.004114788491278887}, {"id": 222, "seek": 139160, "start": 1396.24, "end": 1399.04, "text": " Okay. Yeah. Like your colleague, you optimize non-convex functions.", "tokens": [50596, 1033, 13, 865, 13, 1743, 428, 13532, 11, 291, 19719, 2107, 12, 1671, 303, 87, 6828, 13, 50736], "temperature": 0.0, "avg_logprob": -0.23278325632077837, "compression_ratio": 1.5350553505535056, "no_speech_prob": 0.004114788491278887}, {"id": 223, "seek": 139160, "start": 1399.84, "end": 1403.4399999999998, "text": " Like your colleague at the Bell Labs, who didn't like the non-mathy.", "tokens": [50776, 1743, 428, 13532, 412, 264, 11485, 40047, 11, 567, 994, 380, 411, 264, 2107, 12, 24761, 88, 13, 50956], "temperature": 0.0, "avg_logprob": -0.23278325632077837, "compression_ratio": 1.5350553505535056, "no_speech_prob": 0.004114788491278887}, {"id": 224, "seek": 139160, "start": 1406.1599999999999, "end": 1411.52, "text": " Oh, it was a whole debate, you know, in the machine learning community that lasted 20 years,", "tokens": [51092, 876, 11, 309, 390, 257, 1379, 7958, 11, 291, 458, 11, 294, 264, 3479, 2539, 1768, 300, 21116, 945, 924, 11, 51360], "temperature": 0.0, "avg_logprob": -0.23278325632077837, "compression_ratio": 1.5350553505535056, "no_speech_prob": 0.004114788491278887}, {"id": 225, "seek": 139160, "start": 1411.52, "end": 1419.12, "text": " basically. All right. So what about how doesn't SGD get stuck in local minima once it reaches them?", "tokens": [51360, 1936, 13, 1057, 558, 13, 407, 437, 466, 577, 1177, 380, 34520, 35, 483, 5541, 294, 2654, 4464, 64, 1564, 309, 14235, 552, 30, 51740], "temperature": 0.0, "avg_logprob": -0.23278325632077837, "compression_ratio": 1.5350553505535056, "no_speech_prob": 0.004114788491278887}, {"id": 226, "seek": 141912, "start": 1419.12, "end": 1422.3999999999999, "text": " It does. Okay. So,", "tokens": [50364, 467, 775, 13, 1033, 13, 407, 11, 50528], "temperature": 0.0, "avg_logprob": -0.15281144584097514, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0008947637397795916}, {"id": 227, "seek": 141912, "start": 1424.6399999999999, "end": 1432.08, "text": " so full gradient does get stuck in local minima, right? SGD gets like, you know,", "tokens": [50640, 370, 1577, 16235, 775, 483, 5541, 294, 2654, 4464, 64, 11, 558, 30, 34520, 35, 2170, 411, 11, 291, 458, 11, 51012], "temperature": 0.0, "avg_logprob": -0.15281144584097514, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0008947637397795916}, {"id": 228, "seek": 141912, "start": 1432.08, "end": 1436.4799999999998, "text": " it's slightly less stuck in local minima because it's noisy. It allows it sometimes to escape", "tokens": [51012, 309, 311, 4748, 1570, 5541, 294, 2654, 4464, 64, 570, 309, 311, 24518, 13, 467, 4045, 309, 2171, 281, 7615, 51232], "temperature": 0.0, "avg_logprob": -0.15281144584097514, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0008947637397795916}, {"id": 229, "seek": 141912, "start": 1436.4799999999998, "end": 1445.4399999999998, "text": " local minima. But the real reason why we're going to optimize non-convex functions and local minima", "tokens": [51232, 2654, 4464, 64, 13, 583, 264, 957, 1778, 983, 321, 434, 516, 281, 19719, 2107, 12, 1671, 303, 87, 6828, 293, 2654, 4464, 64, 51680], "temperature": 0.0, "avg_logprob": -0.15281144584097514, "compression_ratio": 1.5260416666666667, "no_speech_prob": 0.0008947637397795916}, {"id": 230, "seek": 144544, "start": 1445.44, "end": 1452.72, "text": " are not going to be such a huge problem is that there aren't that many local minima that are traps.", "tokens": [50364, 366, 406, 516, 281, 312, 1270, 257, 2603, 1154, 307, 300, 456, 3212, 380, 300, 867, 2654, 4464, 64, 300, 366, 24173, 13, 50728], "temperature": 0.0, "avg_logprob": -0.1157845969151969, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.0013245827285572886}, {"id": 231, "seek": 144544, "start": 1452.72, "end": 1459.6000000000001, "text": " Okay. So we're going to build neural nets, and those neural nets are, or deep running systems,", "tokens": [50728, 1033, 13, 407, 321, 434, 516, 281, 1322, 18161, 36170, 11, 293, 729, 18161, 36170, 366, 11, 420, 2452, 2614, 3652, 11, 51072], "temperature": 0.0, "avg_logprob": -0.1157845969151969, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.0013245827285572886}, {"id": 232, "seek": 144544, "start": 1459.6000000000001, "end": 1464.72, "text": " and they're going to be built in such a way that the, the parameter space is such a, such a high", "tokens": [51072, 293, 436, 434, 516, 281, 312, 3094, 294, 1270, 257, 636, 300, 264, 11, 264, 13075, 1901, 307, 1270, 257, 11, 1270, 257, 1090, 51328], "temperature": 0.0, "avg_logprob": -0.1157845969151969, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.0013245827285572886}, {"id": 233, "seek": 144544, "start": 1464.72, "end": 1469.1200000000001, "text": " dimension that is going to be very hard for the system to actually create local minima for us.", "tokens": [51328, 10139, 300, 307, 516, 281, 312, 588, 1152, 337, 264, 1185, 281, 767, 1884, 2654, 4464, 64, 337, 505, 13, 51548], "temperature": 0.0, "avg_logprob": -0.1157845969151969, "compression_ratio": 1.838095238095238, "no_speech_prob": 0.0013245827285572886}, {"id": 234, "seek": 146912, "start": 1469.12, "end": 1476.2399999999998, "text": " Okay. So think about a picture where we have in one dimension a cost function that has one", "tokens": [50364, 1033, 13, 407, 519, 466, 257, 3036, 689, 321, 362, 294, 472, 10139, 257, 2063, 2445, 300, 575, 472, 50720], "temperature": 0.0, "avg_logprob": -0.11495809140412704, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0004727913183160126}, {"id": 235, "seek": 146912, "start": 1476.2399999999998, "end": 1481.4399999999998, "text": " local minima and then a global minimum, right? Okay. So it's a function like this, right?", "tokens": [50720, 2654, 4464, 64, 293, 550, 257, 4338, 7285, 11, 558, 30, 1033, 13, 407, 309, 311, 257, 2445, 411, 341, 11, 558, 30, 50980], "temperature": 0.0, "avg_logprob": -0.11495809140412704, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0004727913183160126}, {"id": 236, "seek": 146912, "start": 1483.28, "end": 1487.04, "text": " And we start from here. If we optimize using gradient descent, we're going to get stuck in", "tokens": [51072, 400, 321, 722, 490, 510, 13, 759, 321, 19719, 1228, 16235, 23475, 11, 321, 434, 516, 281, 483, 5541, 294, 51260], "temperature": 0.0, "avg_logprob": -0.11495809140412704, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0004727913183160126}, {"id": 237, "seek": 146912, "start": 1487.04, "end": 1492.1599999999999, "text": " that local minimum. Now, let's imagine that we parameterize this function now with two parameters.", "tokens": [51260, 300, 2654, 7285, 13, 823, 11, 718, 311, 3811, 300, 321, 13075, 1125, 341, 2445, 586, 365, 732, 9834, 13, 51516], "temperature": 0.0, "avg_logprob": -0.11495809140412704, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0004727913183160126}, {"id": 238, "seek": 146912, "start": 1492.1599999999999, "end": 1495.84, "text": " Okay. So we're not a one dimensional, we're not looking at a one dimensional function anymore.", "tokens": [51516, 1033, 13, 407, 321, 434, 406, 257, 472, 18795, 11, 321, 434, 406, 1237, 412, 257, 472, 18795, 2445, 3602, 13, 51700], "temperature": 0.0, "avg_logprob": -0.11495809140412704, "compression_ratio": 1.830708661417323, "no_speech_prob": 0.0004727913183160126}, {"id": 239, "seek": 149584, "start": 1495.84, "end": 1498.56, "text": " We're looking at two dimensional function. We have an extra parameter.", "tokens": [50364, 492, 434, 1237, 412, 732, 18795, 2445, 13, 492, 362, 364, 2857, 13075, 13, 50500], "temperature": 0.0, "avg_logprob": -0.08368747465072139, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.00027367763686925173}, {"id": 240, "seek": 149584, "start": 1499.6799999999998, "end": 1505.12, "text": " This extra parameter will allow us to go around the mountain and go towards the valley,", "tokens": [50556, 639, 2857, 13075, 486, 2089, 505, 281, 352, 926, 264, 6937, 293, 352, 3030, 264, 17636, 11, 50828], "temperature": 0.0, "avg_logprob": -0.08368747465072139, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.00027367763686925173}, {"id": 241, "seek": 149584, "start": 1505.12, "end": 1510.32, "text": " perhaps without having to climb the little hill in the middle. Okay. So this is just an intuitive", "tokens": [50828, 4317, 1553, 1419, 281, 10724, 264, 707, 10997, 294, 264, 2808, 13, 1033, 13, 407, 341, 307, 445, 364, 21769, 51088], "temperature": 0.0, "avg_logprob": -0.08368747465072139, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.00027367763686925173}, {"id": 242, "seek": 149584, "start": 1510.32, "end": 1515.6, "text": " example to tell you that in very high dimensional spaces, you may not have as much of a local minimum", "tokens": [51088, 1365, 281, 980, 291, 300, 294, 588, 1090, 18795, 7673, 11, 291, 815, 406, 362, 382, 709, 295, 257, 2654, 7285, 51352], "temperature": 0.0, "avg_logprob": -0.08368747465072139, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.00027367763686925173}, {"id": 243, "seek": 149584, "start": 1515.6, "end": 1521.04, "text": " problem as you have in the sort of intuitive picture of low dimensional spaces, right? So here", "tokens": [51352, 1154, 382, 291, 362, 294, 264, 1333, 295, 21769, 3036, 295, 2295, 18795, 7673, 11, 558, 30, 407, 510, 51624], "temperature": 0.0, "avg_logprob": -0.08368747465072139, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.00027367763686925173}, {"id": 244, "seek": 149584, "start": 1521.04, "end": 1525.12, "text": " that those pictures are in two dimensions. They are very misleading. We're going to be working", "tokens": [51624, 300, 729, 5242, 366, 294, 732, 12819, 13, 814, 366, 588, 36429, 13, 492, 434, 516, 281, 312, 1364, 51828], "temperature": 0.0, "avg_logprob": -0.08368747465072139, "compression_ratio": 1.7792207792207793, "no_speech_prob": 0.00027367763686925173}, {"id": 245, "seek": 152512, "start": 1525.12, "end": 1534.08, "text": " with millions of dimensions and you know, some of the most recent deep learning systems have", "tokens": [50364, 365, 6803, 295, 12819, 293, 291, 458, 11, 512, 295, 264, 881, 5162, 2452, 2539, 3652, 362, 50812], "temperature": 0.0, "avg_logprob": -0.14050146738688152, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0022472813725471497}, {"id": 246, "seek": 152512, "start": 1534.08, "end": 1540.9599999999998, "text": " trillions of problems. Yeah. So local minima is not going to be that much of a problem. We're", "tokens": [50812, 504, 46279, 295, 2740, 13, 865, 13, 407, 2654, 4464, 64, 307, 406, 516, 281, 312, 300, 709, 295, 257, 1154, 13, 492, 434, 51156], "temperature": 0.0, "avg_logprob": -0.14050146738688152, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0022472813725471497}, {"id": 247, "seek": 152512, "start": 1540.9599999999998, "end": 1545.76, "text": " going to have other problems, but not that one. So there is like a trend in this hyper like", "tokens": [51156, 516, 281, 362, 661, 2740, 11, 457, 406, 300, 472, 13, 407, 456, 307, 411, 257, 6028, 294, 341, 9848, 411, 51396], "temperature": 0.0, "avg_logprob": -0.14050146738688152, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0022472813725471497}, {"id": 248, "seek": 152512, "start": 1546.9599999999998, "end": 1552.08, "text": " over parameterization, right? Like it seems like that more neurons we have and the better", "tokens": [51456, 670, 13075, 2144, 11, 558, 30, 1743, 309, 2544, 411, 300, 544, 22027, 321, 362, 293, 264, 1101, 51712], "temperature": 0.0, "avg_logprob": -0.14050146738688152, "compression_ratio": 1.6355555555555557, "no_speech_prob": 0.0022472813725471497}, {"id": 249, "seek": 155208, "start": 1552.08, "end": 1556.3999999999999, "text": " these networks work somehow. That's right. So we're going to make those networks very large and", "tokens": [50364, 613, 9590, 589, 6063, 13, 663, 311, 558, 13, 407, 321, 434, 516, 281, 652, 729, 9590, 588, 2416, 293, 50580], "temperature": 0.0, "avg_logprob": -0.12018611354212608, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0014544392470270395}, {"id": 250, "seek": 155208, "start": 1556.3999999999999, "end": 1559.52, "text": " they're going to be over parameterized, which means they're going to have way more adjustable", "tokens": [50580, 436, 434, 516, 281, 312, 670, 13075, 1602, 11, 597, 1355, 436, 434, 516, 281, 362, 636, 544, 27804, 50736], "temperature": 0.0, "avg_logprob": -0.12018611354212608, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0014544392470270395}, {"id": 251, "seek": 155208, "start": 1559.52, "end": 1563.1999999999998, "text": " parameters than we would actually need, which means they're going to be able to learn the", "tokens": [50736, 9834, 813, 321, 576, 767, 643, 11, 597, 1355, 436, 434, 516, 281, 312, 1075, 281, 1466, 264, 50920], "temperature": 0.0, "avg_logprob": -0.12018611354212608, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0014544392470270395}, {"id": 252, "seek": 155208, "start": 1563.1999999999998, "end": 1567.36, "text": " training set almost perfectly. And the big question is how well are they going to work on", "tokens": [50920, 3097, 992, 1920, 6239, 13, 400, 264, 955, 1168, 307, 577, 731, 366, 436, 516, 281, 589, 322, 51128], "temperature": 0.0, "avg_logprob": -0.12018611354212608, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0014544392470270395}, {"id": 253, "seek": 155208, "start": 1567.36, "end": 1572.56, "text": " a separate validation set or test set that is separate from the training set?", "tokens": [51128, 257, 4994, 24071, 992, 420, 1500, 992, 300, 307, 4994, 490, 264, 3097, 992, 30, 51388], "temperature": 0.0, "avg_logprob": -0.12018611354212608, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0014544392470270395}, {"id": 254, "seek": 155208, "start": 1573.52, "end": 1579.04, "text": " Two more questions. They're going to work in a real situation where, you know, the distribution", "tokens": [51436, 4453, 544, 1651, 13, 814, 434, 516, 281, 589, 294, 257, 957, 2590, 689, 11, 291, 458, 11, 264, 7316, 51712], "temperature": 0.0, "avg_logprob": -0.12018611354212608, "compression_ratio": 1.946236559139785, "no_speech_prob": 0.0014544392470270395}, {"id": 255, "seek": 157904, "start": 1579.04, "end": 1582.6399999999999, "text": " of samples may be different from what we trained it on. So that's the real question of machine", "tokens": [50364, 295, 10938, 815, 312, 819, 490, 437, 321, 8895, 309, 322, 13, 407, 300, 311, 264, 957, 1168, 295, 3479, 50544], "temperature": 0.0, "avg_logprob": -0.17140932979746762, "compression_ratio": 1.6496350364963503, "no_speech_prob": 0.0013239041436463594}, {"id": 256, "seek": 157904, "start": 1582.6399999999999, "end": 1588.32, "text": " learning, which I'm sure a lot of you are familiar with. Two more questions. Can we do?", "tokens": [50544, 2539, 11, 597, 286, 478, 988, 257, 688, 295, 291, 366, 4963, 365, 13, 4453, 544, 1651, 13, 1664, 321, 360, 30, 50828], "temperature": 0.0, "avg_logprob": -0.17140932979746762, "compression_ratio": 1.6496350364963503, "no_speech_prob": 0.0013239041436463594}, {"id": 257, "seek": 157904, "start": 1588.32, "end": 1596.8799999999999, "text": " Yeah. So how do we escape instead of subtle points? Right. So there are tons and tons of", "tokens": [50828, 865, 13, 407, 577, 360, 321, 7615, 2602, 295, 13743, 2793, 30, 1779, 13, 407, 456, 366, 9131, 293, 9131, 295, 51256], "temperature": 0.0, "avg_logprob": -0.17140932979746762, "compression_ratio": 1.6496350364963503, "no_speech_prob": 0.0013239041436463594}, {"id": 258, "seek": 157904, "start": 1596.8799999999999, "end": 1601.92, "text": " subtle points in deep learning systems. A combinatorially large number of subtle points,", "tokens": [51256, 13743, 2793, 294, 2452, 2539, 3652, 13, 316, 2512, 31927, 2270, 2416, 1230, 295, 13743, 2793, 11, 51508], "temperature": 0.0, "avg_logprob": -0.17140932979746762, "compression_ratio": 1.6496350364963503, "no_speech_prob": 0.0013239041436463594}, {"id": 259, "seek": 157904, "start": 1601.92, "end": 1608.1599999999999, "text": " as a matter of fact. I'll have a lecture on this. So I don't want to kind of spend too long", "tokens": [51508, 382, 257, 1871, 295, 1186, 13, 286, 603, 362, 257, 7991, 322, 341, 13, 407, 286, 500, 380, 528, 281, 733, 295, 3496, 886, 938, 51820], "temperature": 0.0, "avg_logprob": -0.17140932979746762, "compression_ratio": 1.6496350364963503, "no_speech_prob": 0.0013239041436463594}, {"id": 260, "seek": 160816, "start": 1608.16, "end": 1614.24, "text": " answering. Okay. But yeah, there are subtle points. The trick with subtle points is you don't", "tokens": [50364, 13430, 13, 1033, 13, 583, 1338, 11, 456, 366, 13743, 2793, 13, 440, 4282, 365, 13743, 2793, 307, 291, 500, 380, 50668], "temperature": 0.0, "avg_logprob": -0.14347710720328397, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.000518465822096914}, {"id": 261, "seek": 160816, "start": 1614.24, "end": 1621.92, "text": " want to get too close to them, essentially. And stochastic gradient helps a little bit", "tokens": [50668, 528, 281, 483, 886, 1998, 281, 552, 11, 4476, 13, 400, 342, 8997, 2750, 16235, 3665, 257, 707, 857, 51052], "temperature": 0.0, "avg_logprob": -0.14347710720328397, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.000518465822096914}, {"id": 262, "seek": 160816, "start": 1621.92, "end": 1627.52, "text": " with subtle points. Some people are proposed sort of explicit methods to stay away from", "tokens": [51052, 365, 13743, 2793, 13, 2188, 561, 366, 10348, 1333, 295, 13691, 7150, 281, 1754, 1314, 490, 51332], "temperature": 0.0, "avg_logprob": -0.14347710720328397, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.000518465822096914}, {"id": 263, "seek": 160816, "start": 1627.52, "end": 1632.48, "text": " subtle points. But in practice, doesn't seem to be that much of a problem, actually.", "tokens": [51332, 13743, 2793, 13, 583, 294, 3124, 11, 1177, 380, 1643, 281, 312, 300, 709, 295, 257, 1154, 11, 767, 13, 51580], "temperature": 0.0, "avg_logprob": -0.14347710720328397, "compression_ratio": 1.6192660550458715, "no_speech_prob": 0.000518465822096914}, {"id": 264, "seek": 163248, "start": 1632.72, "end": 1638.32, "text": " Finally, how do you pick samples for stochastic gradient in the center randomly?", "tokens": [50376, 6288, 11, 577, 360, 291, 1888, 10938, 337, 342, 8997, 2750, 16235, 294, 264, 3056, 16979, 30, 50656], "temperature": 0.0, "avg_logprob": -0.1651377982281624, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0004236422828398645}, {"id": 265, "seek": 163248, "start": 1639.68, "end": 1645.76, "text": " Okay. There is lots of different methods for that. Okay. Yeah. I mean, the basic thing you should do", "tokens": [50724, 1033, 13, 821, 307, 3195, 295, 819, 7150, 337, 300, 13, 1033, 13, 865, 13, 286, 914, 11, 264, 3875, 551, 291, 820, 360, 51028], "temperature": 0.0, "avg_logprob": -0.1651377982281624, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0004236422828398645}, {"id": 266, "seek": 163248, "start": 1645.76, "end": 1650.88, "text": " is you have your training set. You shuffle the samples in a random order. Okay. And then you", "tokens": [51028, 307, 291, 362, 428, 3097, 992, 13, 509, 39426, 264, 10938, 294, 257, 4974, 1668, 13, 1033, 13, 400, 550, 291, 51284], "temperature": 0.0, "avg_logprob": -0.1651377982281624, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0004236422828398645}, {"id": 267, "seek": 163248, "start": 1650.88, "end": 1659.3600000000001, "text": " just pick them one at a time. And then you cycle through them. An alternative is once you get to", "tokens": [51284, 445, 1888, 552, 472, 412, 257, 565, 13, 400, 550, 291, 6586, 807, 552, 13, 1107, 8535, 307, 1564, 291, 483, 281, 51708], "temperature": 0.0, "avg_logprob": -0.1651377982281624, "compression_ratio": 1.6343612334801763, "no_speech_prob": 0.0004236422828398645}, {"id": 268, "seek": 165936, "start": 1659.36, "end": 1664.8, "text": " the end, you reshuffle them and then cycle through them again. An alternative is you pick a random", "tokens": [50364, 264, 917, 11, 291, 725, 71, 21665, 552, 293, 550, 6586, 807, 552, 797, 13, 1107, 8535, 307, 291, 1888, 257, 4974, 50636], "temperature": 0.0, "avg_logprob": -0.07058643508743453, "compression_ratio": 1.7348837209302326, "no_speech_prob": 0.0005440735840238631}, {"id": 269, "seek": 165936, "start": 1664.8, "end": 1670.8, "text": " sample using a random number. Every time you pick a new sample, you pick them randomly.", "tokens": [50636, 6889, 1228, 257, 4974, 1230, 13, 2048, 565, 291, 1888, 257, 777, 6889, 11, 291, 1888, 552, 16979, 13, 50936], "temperature": 0.0, "avg_logprob": -0.07058643508743453, "compression_ratio": 1.7348837209302326, "no_speech_prob": 0.0005440735840238631}, {"id": 270, "seek": 165936, "start": 1674.3999999999999, "end": 1680.8, "text": " If you do batching, a good idea is to put in a batch samples that are maximally different from", "tokens": [51116, 759, 291, 360, 15245, 278, 11, 257, 665, 1558, 307, 281, 829, 294, 257, 15245, 10938, 300, 366, 5138, 379, 819, 490, 51436], "temperature": 0.0, "avg_logprob": -0.07058643508743453, "compression_ratio": 1.7348837209302326, "no_speech_prob": 0.0005440735840238631}, {"id": 271, "seek": 165936, "start": 1680.8, "end": 1684.8, "text": " each other. So things that are, for example, different categories if you do classification.", "tokens": [51436, 1184, 661, 13, 407, 721, 300, 366, 11, 337, 1365, 11, 819, 10479, 498, 291, 360, 21538, 13, 51636], "temperature": 0.0, "avg_logprob": -0.07058643508743453, "compression_ratio": 1.7348837209302326, "no_speech_prob": 0.0005440735840238631}, {"id": 272, "seek": 168480, "start": 1685.2, "end": 1689.6, "text": " But most people just do them, you know, just pick them randomly. But it's good to have samples", "tokens": [50384, 583, 881, 561, 445, 360, 552, 11, 291, 458, 11, 445, 1888, 552, 16979, 13, 583, 309, 311, 665, 281, 362, 10938, 50604], "temperature": 0.0, "avg_logprob": -0.18713160923549108, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.0003101138863712549}, {"id": 273, "seek": 168480, "start": 1689.6, "end": 1695.52, "text": " that are maximally different that are nearby either in a batch or during the processor training.", "tokens": [50604, 300, 366, 5138, 379, 819, 300, 366, 11184, 2139, 294, 257, 15245, 420, 1830, 264, 15321, 3097, 13, 50900], "temperature": 0.0, "avg_logprob": -0.18713160923549108, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.0003101138863712549}, {"id": 274, "seek": 168480, "start": 1695.52, "end": 1700.1599999999999, "text": " And then there are all kinds of tricks that people use to sort of emphasize difficult samples", "tokens": [50900, 400, 550, 456, 366, 439, 3685, 295, 11733, 300, 561, 764, 281, 1333, 295, 16078, 2252, 10938, 51132], "temperature": 0.0, "avg_logprob": -0.18713160923549108, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.0003101138863712549}, {"id": 275, "seek": 168480, "start": 1701.12, "end": 1705.28, "text": " so that the boring, easy samples are not, you don't waste your time just, you know,", "tokens": [51180, 370, 300, 264, 9989, 11, 1858, 10938, 366, 406, 11, 291, 500, 380, 5964, 428, 565, 445, 11, 291, 458, 11, 51388], "temperature": 0.0, "avg_logprob": -0.18713160923549108, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.0003101138863712549}, {"id": 276, "seek": 168480, "start": 1705.28, "end": 1710.8, "text": " seeing them over and over again. It's all kinds of tricks. All right. But, you know,", "tokens": [51388, 2577, 552, 670, 293, 670, 797, 13, 467, 311, 439, 3685, 295, 11733, 13, 1057, 558, 13, 583, 11, 291, 458, 11, 51664], "temperature": 0.0, "avg_logprob": -0.18713160923549108, "compression_ratio": 1.7944664031620554, "no_speech_prob": 0.0003101138863712549}, {"id": 277, "seek": 171080, "start": 1711.44, "end": 1716.0, "text": " the simpler one is, which most people use, you shuffle your samples and you run through them.", "tokens": [50396, 264, 18587, 472, 307, 11, 597, 881, 561, 764, 11, 291, 39426, 428, 10938, 293, 291, 1190, 807, 552, 13, 50624], "temperature": 0.0, "avg_logprob": -0.15305782664905895, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0005271958070807159}, {"id": 278, "seek": 171080, "start": 1717.04, "end": 1721.36, "text": " Most people now use also data augmentation. So every sample is actually distorted by some", "tokens": [50676, 4534, 561, 586, 764, 611, 1412, 14501, 19631, 13, 407, 633, 6889, 307, 767, 33431, 538, 512, 50892], "temperature": 0.0, "avg_logprob": -0.15305782664905895, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0005271958070807159}, {"id": 279, "seek": 171080, "start": 1722.48, "end": 1726.8799999999999, "text": " process. For an image, you can distort the geometry a little bit, you change the colors,", "tokens": [50948, 1399, 13, 1171, 364, 3256, 11, 291, 393, 37555, 264, 18426, 257, 707, 857, 11, 291, 1319, 264, 4577, 11, 51168], "temperature": 0.0, "avg_logprob": -0.15305782664905895, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0005271958070807159}, {"id": 280, "seek": 171080, "start": 1726.8799999999999, "end": 1734.0, "text": " you add noise, et cetera. This is an artificial way of sort of adding more samples than you actually", "tokens": [51168, 291, 909, 5658, 11, 1030, 11458, 13, 639, 307, 364, 11677, 636, 295, 1333, 295, 5127, 544, 10938, 813, 291, 767, 51524], "temperature": 0.0, "avg_logprob": -0.15305782664905895, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0005271958070807159}, {"id": 281, "seek": 171080, "start": 1734.0, "end": 1738.8799999999999, "text": " have. And people do this kind of randomly on the fly or they kind of precompute those", "tokens": [51524, 362, 13, 400, 561, 360, 341, 733, 295, 16979, 322, 264, 3603, 420, 436, 733, 295, 659, 21541, 1169, 729, 51768], "temperature": 0.0, "avg_logprob": -0.15305782664905895, "compression_ratio": 1.712686567164179, "no_speech_prob": 0.0005271958070807159}, {"id": 282, "seek": 173888, "start": 1739.8400000000001, "end": 1745.7600000000002, "text": " those transformations. So lots of tricks there as well. Last question. How do you pick the batch", "tokens": [50412, 729, 34852, 13, 407, 3195, 295, 11733, 456, 382, 731, 13, 5264, 1168, 13, 1012, 360, 291, 1888, 264, 15245, 50708], "temperature": 0.0, "avg_logprob": -0.11611519863730983, "compression_ratio": 1.6125, "no_speech_prob": 0.0007784198969602585}, {"id": 283, "seek": 173888, "start": 1745.7600000000002, "end": 1752.72, "text": " size? The best. The batch, batch size. Oh, the batch size. That's determined by your hardware.", "tokens": [50708, 2744, 30, 440, 1151, 13, 440, 15245, 11, 15245, 2744, 13, 876, 11, 264, 15245, 2744, 13, 663, 311, 9540, 538, 428, 8837, 13, 51056], "temperature": 0.0, "avg_logprob": -0.11611519863730983, "compression_ratio": 1.6125, "no_speech_prob": 0.0007784198969602585}, {"id": 284, "seek": 173888, "start": 1752.72, "end": 1759.6000000000001, "text": " So if you have a GPU, generally for, you know, reasonably sized networks, your batch size would", "tokens": [51056, 407, 498, 291, 362, 257, 18407, 11, 5101, 337, 11, 291, 458, 11, 23551, 20004, 9590, 11, 428, 15245, 2744, 576, 51400], "temperature": 0.0, "avg_logprob": -0.11611519863730983, "compression_ratio": 1.6125, "no_speech_prob": 0.0007784198969602585}, {"id": 285, "seek": 173888, "start": 1759.6000000000001, "end": 1764.5600000000002, "text": " be anywhere between 16 and 64 or something like that. For smaller networks, you might have to batch", "tokens": [51400, 312, 4992, 1296, 3165, 293, 12145, 420, 746, 411, 300, 13, 1171, 4356, 9590, 11, 291, 1062, 362, 281, 15245, 51648], "temperature": 0.0, "avg_logprob": -0.11611519863730983, "compression_ratio": 1.6125, "no_speech_prob": 0.0007784198969602585}, {"id": 286, "seek": 176456, "start": 1764.6399999999999, "end": 1769.84, "text": " more to kind of exploit your, your hardware better to kind of have maximum usage of it.", "tokens": [50368, 544, 281, 733, 295, 25924, 428, 11, 428, 8837, 1101, 281, 733, 295, 362, 6674, 14924, 295, 309, 13, 50628], "temperature": 0.0, "avg_logprob": -0.12510796516172348, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.002147944876924157}, {"id": 287, "seek": 176456, "start": 1770.56, "end": 1775.04, "text": " If you parallelize on multiple GPUs within a machine, you may have to, to have, you know,", "tokens": [50664, 759, 291, 8952, 1125, 322, 3866, 18407, 82, 1951, 257, 3479, 11, 291, 815, 362, 281, 11, 281, 362, 11, 291, 458, 11, 50888], "temperature": 0.0, "avg_logprob": -0.12510796516172348, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.002147944876924157}, {"id": 288, "seek": 176456, "start": 1775.04, "end": 1779.04, "text": " so let's say you have eight GPUs, then you'll be sort of eight times 32. So there's no", "tokens": [50888, 370, 718, 311, 584, 291, 362, 3180, 18407, 82, 11, 550, 291, 603, 312, 1333, 295, 3180, 1413, 8858, 13, 407, 456, 311, 572, 51088], "temperature": 0.0, "avg_logprob": -0.12510796516172348, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.002147944876924157}, {"id": 289, "seek": 176456, "start": 1780.48, "end": 1786.1599999999999, "text": " 256 or something. And then, you know, a lot of the big guys kind of parallelize that over", "tokens": [51160, 38882, 420, 746, 13, 400, 550, 11, 291, 458, 11, 257, 688, 295, 264, 955, 1074, 733, 295, 8952, 1125, 300, 670, 51444], "temperature": 0.0, "avg_logprob": -0.12510796516172348, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.002147944876924157}, {"id": 290, "seek": 176456, "start": 1786.1599999999999, "end": 1790.6399999999999, "text": " multiple machines, each of which has eight GPUs. Some of them have TPUs, whatever. And then you", "tokens": [51444, 3866, 8379, 11, 1184, 295, 597, 575, 3180, 18407, 82, 13, 2188, 295, 552, 362, 314, 8115, 82, 11, 2035, 13, 400, 550, 291, 51668], "temperature": 0.0, "avg_logprob": -0.12510796516172348, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.002147944876924157}, {"id": 291, "seek": 179064, "start": 1790.64, "end": 1794.88, "text": " might have to parallelize over thousands of examples. This diminishing return in doing this,", "tokens": [50364, 1062, 362, 281, 8952, 1125, 670, 5383, 295, 5110, 13, 639, 15739, 3807, 2736, 294, 884, 341, 11, 50576], "temperature": 0.0, "avg_logprob": -0.11113953805184579, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.001523938961327076}, {"id": 292, "seek": 179064, "start": 1795.5200000000002, "end": 1801.6000000000001, "text": " when you increase the size of the batch, you actually reduce the, the, the speed of convergence.", "tokens": [50608, 562, 291, 3488, 264, 2744, 295, 264, 15245, 11, 291, 767, 5407, 264, 11, 264, 11, 264, 3073, 295, 32181, 13, 50912], "temperature": 0.0, "avg_logprob": -0.11113953805184579, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.001523938961327076}, {"id": 293, "seek": 179064, "start": 1801.6000000000001, "end": 1805.3600000000001, "text": " You accelerate the calculation, but you reduce the speed of convergence. So at some point,", "tokens": [50912, 509, 21341, 264, 17108, 11, 457, 291, 5407, 264, 3073, 295, 32181, 13, 407, 412, 512, 935, 11, 51100], "temperature": 0.0, "avg_logprob": -0.11113953805184579, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.001523938961327076}, {"id": 294, "seek": 179064, "start": 1805.3600000000001, "end": 1811.92, "text": " it's not worth increasing your batch size. So if we are doing a classification problem with k", "tokens": [51100, 309, 311, 406, 3163, 5662, 428, 15245, 2744, 13, 407, 498, 321, 366, 884, 257, 21538, 1154, 365, 350, 51428], "temperature": 0.0, "avg_logprob": -0.11113953805184579, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.001523938961327076}, {"id": 295, "seek": 179064, "start": 1811.92, "end": 1819.8400000000001, "text": " classes, what's going to be like our go to batch size? So there are papers that say if your batch", "tokens": [51428, 5359, 11, 437, 311, 516, 281, 312, 411, 527, 352, 281, 15245, 2744, 30, 407, 456, 366, 10577, 300, 584, 498, 428, 15245, 51824], "temperature": 0.0, "avg_logprob": -0.11113953805184579, "compression_ratio": 1.7878787878787878, "no_speech_prob": 0.001523938961327076}, {"id": 296, "seek": 181984, "start": 1819.84, "end": 1824.8799999999999, "text": " size is significantly larger than the number of categories, or let's say twice the number of", "tokens": [50364, 2744, 307, 10591, 4833, 813, 264, 1230, 295, 10479, 11, 420, 718, 311, 584, 6091, 264, 1230, 295, 50616], "temperature": 0.0, "avg_logprob": -0.15070251725677752, "compression_ratio": 1.9465020576131686, "no_speech_prob": 0.0007786893402226269}, {"id": 297, "seek": 181984, "start": 1824.8799999999999, "end": 1830.8, "text": " categories, then you're, you're probably wasting competition, essentially. Okay. I mean, throwing", "tokens": [50616, 10479, 11, 550, 291, 434, 11, 291, 434, 1391, 20457, 6211, 11, 4476, 13, 1033, 13, 286, 914, 11, 10238, 50912], "temperature": 0.0, "avg_logprob": -0.15070251725677752, "compression_ratio": 1.9465020576131686, "no_speech_prob": 0.0007786893402226269}, {"id": 298, "seek": 181984, "start": 1830.8, "end": 1836.08, "text": " down convergence. So you're trying to train an image recognizer on ImageNet. If your batch size", "tokens": [50912, 760, 32181, 13, 407, 291, 434, 1382, 281, 3847, 364, 3256, 3068, 6545, 322, 29903, 31890, 13, 759, 428, 15245, 2744, 51176], "temperature": 0.0, "avg_logprob": -0.15070251725677752, "compression_ratio": 1.9465020576131686, "no_speech_prob": 0.0007786893402226269}, {"id": 299, "seek": 181984, "start": 1836.08, "end": 1843.1999999999998, "text": " is larger than about a thousand, you're probably wasting time. Okay, that's it. Thanks. I mean,", "tokens": [51176, 307, 4833, 813, 466, 257, 4714, 11, 291, 434, 1391, 20457, 565, 13, 1033, 11, 300, 311, 309, 13, 2561, 13, 286, 914, 11, 51532], "temperature": 0.0, "avg_logprob": -0.15070251725677752, "compression_ratio": 1.9465020576131686, "no_speech_prob": 0.0007786893402226269}, {"id": 300, "seek": 181984, "start": 1843.1999999999998, "end": 1848.72, "text": " you're wasting competition. You're not wasting time. Okay. Okay. Okay. So let's talk about", "tokens": [51532, 291, 434, 20457, 6211, 13, 509, 434, 406, 20457, 565, 13, 1033, 13, 1033, 13, 1033, 13, 407, 718, 311, 751, 466, 51808], "temperature": 0.0, "avg_logprob": -0.15070251725677752, "compression_ratio": 1.9465020576131686, "no_speech_prob": 0.0007786893402226269}, {"id": 301, "seek": 184872, "start": 1848.72, "end": 1854.96, "text": " traditional neural net. So a traditional neural net is a, a, a model, a particular type of", "tokens": [50364, 5164, 18161, 2533, 13, 407, 257, 5164, 18161, 2533, 307, 257, 11, 257, 11, 257, 2316, 11, 257, 1729, 2010, 295, 50676], "temperature": 0.0, "avg_logprob": -0.11884583036104839, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0011329904664307833}, {"id": 302, "seek": 184872, "start": 1854.96, "end": 1862.08, "text": " parameterized function, which is built by stacking linear and nonlinear operations. Right. So here", "tokens": [50676, 13075, 1602, 2445, 11, 597, 307, 3094, 538, 41376, 8213, 293, 2107, 28263, 7705, 13, 1779, 13, 407, 510, 51032], "temperature": 0.0, "avg_logprob": -0.11884583036104839, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0011329904664307833}, {"id": 303, "seek": 184872, "start": 1862.08, "end": 1865.68, "text": " is this kind of a depiction of a traditional neural net here in this case, with two layers, but I'm,", "tokens": [51032, 307, 341, 733, 295, 257, 47740, 295, 257, 5164, 18161, 2533, 510, 294, 341, 1389, 11, 365, 732, 7914, 11, 457, 286, 478, 11, 51212], "temperature": 0.0, "avg_logprob": -0.11884583036104839, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0011329904664307833}, {"id": 304, "seek": 184872, "start": 1865.68, "end": 1870.64, "text": " you know, I'm not imagining there might be more layers here. So you have a bunch of inputs here", "tokens": [51212, 291, 458, 11, 286, 478, 406, 27798, 456, 1062, 312, 544, 7914, 510, 13, 407, 291, 362, 257, 3840, 295, 15743, 510, 51460], "temperature": 0.0, "avg_logprob": -0.11884583036104839, "compression_ratio": 1.7788018433179724, "no_speech_prob": 0.0011329904664307833}, {"id": 305, "seek": 187064, "start": 1870.64, "end": 1879.1200000000001, "text": " on the left. Each input is multiplied by a weight, different weights, presumably. And those, the", "tokens": [50364, 322, 264, 1411, 13, 6947, 4846, 307, 17207, 538, 257, 3364, 11, 819, 17443, 11, 26742, 13, 400, 729, 11, 264, 50788], "temperature": 0.0, "avg_logprob": -0.09964018996043872, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.023319480940699577}, {"id": 306, "seek": 187064, "start": 1879.1200000000001, "end": 1885.2, "text": " weighted sum of those inputs by those weights is, is computed here by what's called a unit or neuron.", "tokens": [50788, 32807, 2408, 295, 729, 15743, 538, 729, 17443, 307, 11, 307, 40610, 510, 538, 437, 311, 1219, 257, 4985, 420, 34090, 13, 51092], "temperature": 0.0, "avg_logprob": -0.09964018996043872, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.023319480940699577}, {"id": 307, "seek": 187064, "start": 1886.16, "end": 1890.96, "text": " People don't like using the word neuron in that context, because there are incredibly simplified", "tokens": [51140, 3432, 500, 380, 411, 1228, 264, 1349, 34090, 294, 300, 4319, 11, 570, 456, 366, 6252, 26335, 51380], "temperature": 0.0, "avg_logprob": -0.09964018996043872, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.023319480940699577}, {"id": 308, "seek": 187064, "start": 1890.96, "end": 1897.8400000000001, "text": " models of neurons in the brain, but, but that's the inspiration really. Okay. So one of those units", "tokens": [51380, 5245, 295, 22027, 294, 264, 3567, 11, 457, 11, 457, 300, 311, 264, 10249, 534, 13, 1033, 13, 407, 472, 295, 729, 6815, 51724], "temperature": 0.0, "avg_logprob": -0.09964018996043872, "compression_ratio": 1.6458333333333333, "no_speech_prob": 0.023319480940699577}, {"id": 309, "seek": 189784, "start": 1897.9199999999998, "end": 1902.48, "text": " just computes a weighted sum of its inputs, using those weights. Okay, this unit use,", "tokens": [50368, 445, 715, 1819, 257, 32807, 2408, 295, 1080, 15743, 11, 1228, 729, 17443, 13, 1033, 11, 341, 4985, 764, 11, 50596], "temperature": 0.0, "avg_logprob": -0.11280997360453886, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.0009545044740661979}, {"id": 310, "seek": 189784, "start": 1902.48, "end": 1907.04, "text": " computes a different weighted sum of the same inputs with different weights and etc. So here", "tokens": [50596, 715, 1819, 257, 819, 32807, 2408, 295, 264, 912, 15743, 365, 819, 17443, 293, 5183, 13, 407, 510, 50824], "temperature": 0.0, "avg_logprob": -0.11280997360453886, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.0009545044740661979}, {"id": 311, "seek": 189784, "start": 1907.04, "end": 1910.48, "text": " we have three units here in the first layer. This is called a hidden layer, by the way,", "tokens": [50824, 321, 362, 1045, 6815, 510, 294, 264, 700, 4583, 13, 639, 307, 1219, 257, 7633, 4583, 11, 538, 264, 636, 11, 50996], "temperature": 0.0, "avg_logprob": -0.11280997360453886, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.0009545044740661979}, {"id": 312, "seek": 189784, "start": 1911.36, "end": 1914.72, "text": " because it's neither an input nor an output, right? This is the input, and this is the output,", "tokens": [51040, 570, 309, 311, 9662, 364, 4846, 6051, 364, 5598, 11, 558, 30, 639, 307, 264, 4846, 11, 293, 341, 307, 264, 5598, 11, 51208], "temperature": 0.0, "avg_logprob": -0.11280997360453886, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.0009545044740661979}, {"id": 313, "seek": 189784, "start": 1914.72, "end": 1919.1999999999998, "text": " and this is somewhere in the middle. So we compute those weighted sums, and then we pass those", "tokens": [51208, 293, 341, 307, 4079, 294, 264, 2808, 13, 407, 321, 14722, 729, 32807, 34499, 11, 293, 550, 321, 1320, 729, 51432], "temperature": 0.0, "avg_logprob": -0.11280997360453886, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.0009545044740661979}, {"id": 314, "seek": 189784, "start": 1919.1999999999998, "end": 1924.72, "text": " weighted sums individually through a, a nonlinear function. So here what I've shown is the value", "tokens": [51432, 32807, 34499, 16652, 807, 257, 11, 257, 2107, 28263, 2445, 13, 407, 510, 437, 286, 600, 4898, 307, 264, 2158, 51708], "temperature": 0.0, "avg_logprob": -0.11280997360453886, "compression_ratio": 1.9963898916967509, "no_speech_prob": 0.0009545044740661979}, {"id": 315, "seek": 192472, "start": 1924.72, "end": 1931.44, "text": " function. So this is called rectified linear unit. In the, this is the name that people", "tokens": [50364, 2445, 13, 407, 341, 307, 1219, 11048, 2587, 8213, 4985, 13, 682, 264, 11, 341, 307, 264, 1315, 300, 561, 50700], "temperature": 0.0, "avg_logprob": -0.13681853186223925, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0010643890127539635}, {"id": 316, "seek": 192472, "start": 1932.0, "end": 1938.0, "text": " have given it in the neural net lingual. In other contexts, this is called a half wave rectifier,", "tokens": [50728, 362, 2212, 309, 294, 264, 18161, 2533, 22949, 901, 13, 682, 661, 30628, 11, 341, 307, 1219, 257, 1922, 5772, 11048, 9902, 11, 51028], "temperature": 0.0, "avg_logprob": -0.13681853186223925, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0010643890127539635}, {"id": 317, "seek": 192472, "start": 1938.0, "end": 1944.64, "text": " if you're an engineer. It's called positive part, if you are a mathematician. Okay. Basically,", "tokens": [51028, 498, 291, 434, 364, 11403, 13, 467, 311, 1219, 3353, 644, 11, 498, 291, 366, 257, 48281, 13, 1033, 13, 8537, 11, 51360], "temperature": 0.0, "avg_logprob": -0.13681853186223925, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0010643890127539635}, {"id": 318, "seek": 192472, "start": 1944.64, "end": 1949.6000000000001, "text": " it's a function that is equal to the identity when its argument is positive, and it's equal to zero", "tokens": [51360, 309, 311, 257, 2445, 300, 307, 2681, 281, 264, 6575, 562, 1080, 6770, 307, 3353, 11, 293, 309, 311, 2681, 281, 4018, 51608], "temperature": 0.0, "avg_logprob": -0.13681853186223925, "compression_ratio": 1.7117117117117118, "no_speech_prob": 0.0010643890127539635}, {"id": 319, "seek": 194960, "start": 1949.6, "end": 1958.08, "text": " if its argument is negative. Okay. So very simple graph. And then we stack a second layer of the", "tokens": [50364, 498, 1080, 6770, 307, 3671, 13, 1033, 13, 407, 588, 2199, 4295, 13, 400, 550, 321, 8630, 257, 1150, 4583, 295, 264, 50788], "temperature": 0.0, "avg_logprob": -0.10163108580703036, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0002491963969077915}, {"id": 320, "seek": 194960, "start": 1958.08, "end": 1962.48, "text": " same thing, the second stage, right? So again, a layer of linear operations where we compute", "tokens": [50788, 912, 551, 11, 264, 1150, 3233, 11, 558, 30, 407, 797, 11, 257, 4583, 295, 8213, 7705, 689, 321, 14722, 51008], "temperature": 0.0, "avg_logprob": -0.10163108580703036, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0002491963969077915}, {"id": 321, "seek": 194960, "start": 1962.48, "end": 1967.1999999999998, "text": " weighted sums, and then we pass a result to nonlinearities. And we can stack many of those", "tokens": [51008, 32807, 34499, 11, 293, 550, 321, 1320, 257, 1874, 281, 2107, 28263, 1088, 13, 400, 321, 393, 8630, 867, 295, 729, 51244], "temperature": 0.0, "avg_logprob": -0.10163108580703036, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0002491963969077915}, {"id": 322, "seek": 194960, "start": 1967.1999999999998, "end": 1972.0, "text": " layers, and that's basically a traditional plain vanilla garden variety neural net.", "tokens": [51244, 7914, 11, 293, 300, 311, 1936, 257, 5164, 11121, 17528, 7431, 5673, 18161, 2533, 13, 51484], "temperature": 0.0, "avg_logprob": -0.10163108580703036, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0002491963969077915}, {"id": 323, "seek": 194960, "start": 1973.1999999999998, "end": 1979.12, "text": " In this case, fully connected. So fully connected neural net means that every unit in one layer", "tokens": [51544, 682, 341, 1389, 11, 4498, 4582, 13, 407, 4498, 4582, 18161, 2533, 1355, 300, 633, 4985, 294, 472, 4583, 51840], "temperature": 0.0, "avg_logprob": -0.10163108580703036, "compression_ratio": 1.7293233082706767, "no_speech_prob": 0.0002491963969077915}, {"id": 324, "seek": 197912, "start": 1979.12, "end": 1983.12, "text": " is connected to every unit in the next layer. And you have this sort of well organized layer,", "tokens": [50364, 307, 4582, 281, 633, 4985, 294, 264, 958, 4583, 13, 400, 291, 362, 341, 1333, 295, 731, 9983, 4583, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11486015717188518, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.00041052751475945115}, {"id": 325, "seek": 197912, "start": 1984.6399999999999, "end": 1989.76, "text": " or architecture, if you want, right? Each of those weights are going to be the things that", "tokens": [50640, 420, 9482, 11, 498, 291, 528, 11, 558, 30, 6947, 295, 729, 17443, 366, 516, 281, 312, 264, 721, 300, 50896], "temperature": 0.0, "avg_logprob": -0.11486015717188518, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.00041052751475945115}, {"id": 326, "seek": 197912, "start": 1989.76, "end": 1995.84, "text": " our learning algorithm is going to, is going to tune. And the big trick, the one trick really", "tokens": [50896, 527, 2539, 9284, 307, 516, 281, 11, 307, 516, 281, 10864, 13, 400, 264, 955, 4282, 11, 264, 472, 4282, 534, 51200], "temperature": 0.0, "avg_logprob": -0.11486015717188518, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.00041052751475945115}, {"id": 327, "seek": 197912, "start": 1995.84, "end": 2002.6399999999999, "text": " of deep learning is how we compute those gradients. Okay. So if you want, if you want to write this,", "tokens": [51200, 295, 2452, 2539, 307, 577, 321, 14722, 729, 2771, 2448, 13, 1033, 13, 407, 498, 291, 528, 11, 498, 291, 528, 281, 2464, 341, 11, 51540], "temperature": 0.0, "avg_logprob": -0.11486015717188518, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.00041052751475945115}, {"id": 328, "seek": 200264, "start": 2002.72, "end": 2009.2, "text": " you can say the weighted sum number i, so you can give a number to each of the units", "tokens": [50368, 291, 393, 584, 264, 32807, 2408, 1230, 741, 11, 370, 291, 393, 976, 257, 1230, 281, 1184, 295, 264, 6815, 50692], "temperature": 0.0, "avg_logprob": -0.15537636620657785, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.003763626329600811}, {"id": 329, "seek": 200264, "start": 2009.92, "end": 2017.68, "text": " in the network. So this unit with number i, and the weighted sum s of i, is simply the sum", "tokens": [50728, 294, 264, 3209, 13, 407, 341, 4985, 365, 1230, 741, 11, 293, 264, 32807, 2408, 262, 295, 741, 11, 307, 2935, 264, 2408, 51116], "temperature": 0.0, "avg_logprob": -0.15537636620657785, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.003763626329600811}, {"id": 330, "seek": 200264, "start": 2018.4, "end": 2023.8400000000001, "text": " where j goes over the upstream, the set of upstream units to i, which may be all the units in the", "tokens": [51152, 689, 361, 1709, 670, 264, 33915, 11, 264, 992, 295, 33915, 6815, 281, 741, 11, 597, 815, 312, 439, 264, 6815, 294, 264, 51424], "temperature": 0.0, "avg_logprob": -0.15537636620657785, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.003763626329600811}, {"id": 331, "seek": 200264, "start": 2023.8400000000001, "end": 2031.1200000000001, "text": " previous layer or not could be just a subset. Okay. And then you compute the product of zj,", "tokens": [51424, 3894, 4583, 420, 406, 727, 312, 445, 257, 25993, 13, 1033, 13, 400, 550, 291, 14722, 264, 1674, 295, 710, 73, 11, 51788], "temperature": 0.0, "avg_logprob": -0.15537636620657785, "compression_ratio": 1.7464114832535884, "no_speech_prob": 0.003763626329600811}, {"id": 332, "seek": 203112, "start": 2031.12, "end": 2037.6, "text": " which is the output of the unit number j times wij, which is the weight that links", "tokens": [50364, 597, 307, 264, 5598, 295, 264, 4985, 1230, 361, 1413, 261, 1718, 11, 597, 307, 264, 3364, 300, 6123, 50688], "temperature": 0.0, "avg_logprob": -0.11856403021976866, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.0003149969852529466}, {"id": 333, "seek": 203112, "start": 2037.6, "end": 2044.08, "text": " unit j to unit i. Okay. And then after that, you take this si, which is the weighted sum,", "tokens": [50688, 4985, 361, 281, 4985, 741, 13, 1033, 13, 400, 550, 934, 300, 11, 291, 747, 341, 1511, 11, 597, 307, 264, 32807, 2408, 11, 51012], "temperature": 0.0, "avg_logprob": -0.11856403021976866, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.0003149969852529466}, {"id": 334, "seek": 203112, "start": 2044.08, "end": 2048.4, "text": " you pass it through the activation function, this value, or whatever it is that you use,", "tokens": [51012, 291, 1320, 309, 807, 264, 24433, 2445, 11, 341, 2158, 11, 420, 2035, 309, 307, 300, 291, 764, 11, 51228], "temperature": 0.0, "avg_logprob": -0.11856403021976866, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.0003149969852529466}, {"id": 335, "seek": 203112, "start": 2048.4, "end": 2056.0, "text": " and that gives you zi, which is the activation for unit i. Okay. Super notation. By changing the", "tokens": [51228, 293, 300, 2709, 291, 710, 72, 11, 597, 307, 264, 24433, 337, 4985, 741, 13, 1033, 13, 4548, 24657, 13, 3146, 4473, 264, 51608], "temperature": 0.0, "avg_logprob": -0.11856403021976866, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.0003149969852529466}, {"id": 336, "seek": 203112, "start": 2056.0, "end": 2060.24, "text": " set of upstream units of every unit, by building a graph of interconnection, you can basically", "tokens": [51608, 992, 295, 33915, 6815, 295, 633, 4985, 11, 538, 2390, 257, 4295, 295, 26253, 313, 11, 291, 393, 1936, 51820], "temperature": 0.0, "avg_logprob": -0.11856403021976866, "compression_ratio": 1.834008097165992, "no_speech_prob": 0.0003149969852529466}, {"id": 337, "seek": 206024, "start": 2060.24, "end": 2067.3599999999997, "text": " build any kind of network arrangement that you want. There is one constraint that we can lift,", "tokens": [50364, 1322, 604, 733, 295, 3209, 17620, 300, 291, 528, 13, 821, 307, 472, 25534, 300, 321, 393, 5533, 11, 50720], "temperature": 0.0, "avg_logprob": -0.11829784642095151, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0009394516819156706}, {"id": 338, "seek": 206024, "start": 2067.3599999999997, "end": 2073.12, "text": " that we will lift in the subsequent lecture, which is that the graph has to be", "tokens": [50720, 300, 321, 486, 5533, 294, 264, 19962, 7991, 11, 597, 307, 300, 264, 4295, 575, 281, 312, 51008], "temperature": 0.0, "avg_logprob": -0.11829784642095151, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0009394516819156706}, {"id": 339, "seek": 206024, "start": 2076.08, "end": 2080.3199999999997, "text": " ac-click in the sense that it can't have loops. Okay. If you have loops, that means you can't", "tokens": [51156, 696, 12, 18548, 294, 264, 2020, 300, 309, 393, 380, 362, 16121, 13, 1033, 13, 759, 291, 362, 16121, 11, 300, 1355, 291, 393, 380, 51368], "temperature": 0.0, "avg_logprob": -0.11829784642095151, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0009394516819156706}, {"id": 340, "seek": 206024, "start": 2080.3199999999997, "end": 2086.0, "text": " organize the units in layers. You can't sort of number them in a way that you can compute them", "tokens": [51368, 13859, 264, 6815, 294, 7914, 13, 509, 393, 380, 1333, 295, 1230, 552, 294, 257, 636, 300, 291, 393, 14722, 552, 51652], "temperature": 0.0, "avg_logprob": -0.11829784642095151, "compression_ratio": 1.691588785046729, "no_speech_prob": 0.0009394516819156706}, {"id": 341, "seek": 208600, "start": 2086.64, "end": 2091.68, "text": " so that every time you want to compute a unit, you already have the state of the previous units.", "tokens": [50396, 370, 300, 633, 565, 291, 528, 281, 14722, 257, 4985, 11, 291, 1217, 362, 264, 1785, 295, 264, 3894, 6815, 13, 50648], "temperature": 0.0, "avg_logprob": -0.14816447284734138, "compression_ratio": 1.7396694214876034, "no_speech_prob": 0.00017672173271421343}, {"id": 342, "seek": 208600, "start": 2091.68, "end": 2096.72, "text": " If there are loops, then you can do that. Right? So for now, we're going to assume that", "tokens": [50648, 759, 456, 366, 16121, 11, 550, 291, 393, 360, 300, 13, 1779, 30, 407, 337, 586, 11, 321, 434, 516, 281, 6552, 300, 50900], "temperature": 0.0, "avg_logprob": -0.14816447284734138, "compression_ratio": 1.7396694214876034, "no_speech_prob": 0.00017672173271421343}, {"id": 343, "seek": 208600, "start": 2097.52, "end": 2100.8, "text": " the wij matrix, the w matrix, doesn't have loops,", "tokens": [50940, 264, 261, 1718, 8141, 11, 264, 261, 8141, 11, 1177, 380, 362, 16121, 11, 51104], "temperature": 0.0, "avg_logprob": -0.14816447284734138, "compression_ratio": 1.7396694214876034, "no_speech_prob": 0.00017672173271421343}, {"id": 344, "seek": 208600, "start": 2104.0, "end": 2109.52, "text": " represents a graph that doesn't have loops. That's why I should say. Okay. So here's sort of an", "tokens": [51264, 8855, 257, 4295, 300, 1177, 380, 362, 16121, 13, 663, 311, 983, 286, 820, 584, 13, 1033, 13, 407, 510, 311, 1333, 295, 364, 51540], "temperature": 0.0, "avg_logprob": -0.14816447284734138, "compression_ratio": 1.7396694214876034, "no_speech_prob": 0.00017672173271421343}, {"id": 345, "seek": 208600, "start": 2109.52, "end": 2114.56, "text": " intuitive explanation of the back propagation algorithm. So the back propagation algorithm", "tokens": [51540, 21769, 10835, 295, 264, 646, 38377, 9284, 13, 407, 264, 646, 38377, 9284, 51792], "temperature": 0.0, "avg_logprob": -0.14816447284734138, "compression_ratio": 1.7396694214876034, "no_speech_prob": 0.00017672173271421343}, {"id": 346, "seek": 211456, "start": 2114.56, "end": 2121.68, "text": " is the main technique that is used everywhere in deep learning to compute the gradient of", "tokens": [50364, 307, 264, 2135, 6532, 300, 307, 1143, 5315, 294, 2452, 2539, 281, 14722, 264, 16235, 295, 50720], "temperature": 0.0, "avg_logprob": -0.079269696125942, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00027798491646535695}, {"id": 347, "seek": 211456, "start": 2122.4, "end": 2128.24, "text": " a cost function, whatever it is, objective function, with respect to a variable inside", "tokens": [50756, 257, 2063, 2445, 11, 2035, 309, 307, 11, 10024, 2445, 11, 365, 3104, 281, 257, 7006, 1854, 51048], "temperature": 0.0, "avg_logprob": -0.079269696125942, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00027798491646535695}, {"id": 348, "seek": 211456, "start": 2128.24, "end": 2132.96, "text": " of the network. This variable can be a state variable like a z or an s, or it could be a", "tokens": [51048, 295, 264, 3209, 13, 639, 7006, 393, 312, 257, 1785, 7006, 411, 257, 710, 420, 364, 262, 11, 420, 309, 727, 312, 257, 51284], "temperature": 0.0, "avg_logprob": -0.079269696125942, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00027798491646535695}, {"id": 349, "seek": 211456, "start": 2132.96, "end": 2138.08, "text": " parameter variable like a w. Okay. And we're going to need to do both. Okay. So this is going to be", "tokens": [51284, 13075, 7006, 411, 257, 261, 13, 1033, 13, 400, 321, 434, 516, 281, 643, 281, 360, 1293, 13, 1033, 13, 407, 341, 307, 516, 281, 312, 51540], "temperature": 0.0, "avg_logprob": -0.079269696125942, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00027798491646535695}, {"id": 350, "seek": 211456, "start": 2138.08, "end": 2141.6, "text": " an intuitive explanation. And then after that, there's going to be a more mathematical explanation,", "tokens": [51540, 364, 21769, 10835, 13, 400, 550, 934, 300, 11, 456, 311, 516, 281, 312, 257, 544, 18894, 10835, 11, 51716], "temperature": 0.0, "avg_logprob": -0.079269696125942, "compression_ratio": 1.7816091954022988, "no_speech_prob": 0.00027798491646535695}, {"id": 351, "seek": 214160, "start": 2141.6, "end": 2147.2, "text": " which is less intuitive, but perhaps actually easier to understand. But let me start with", "tokens": [50364, 597, 307, 1570, 21769, 11, 457, 4317, 767, 3571, 281, 1223, 13, 583, 718, 385, 722, 365, 50644], "temperature": 0.0, "avg_logprob": -0.08864579149471816, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0010815089335665107}, {"id": 352, "seek": 214160, "start": 2147.2, "end": 2151.44, "text": " the intuition here. So let's say we have a big network. And inside of this big network, we have", "tokens": [50644, 264, 24002, 510, 13, 407, 718, 311, 584, 321, 362, 257, 955, 3209, 13, 400, 1854, 295, 341, 955, 3209, 11, 321, 362, 50856], "temperature": 0.0, "avg_logprob": -0.08864579149471816, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0010815089335665107}, {"id": 353, "seek": 214160, "start": 2151.44, "end": 2155.92, "text": " one of those little activation functions. Okay. In this case, it's a sigmoid function, but", "tokens": [50856, 472, 295, 729, 707, 24433, 6828, 13, 1033, 13, 682, 341, 1389, 11, 309, 311, 257, 4556, 3280, 327, 2445, 11, 457, 51080], "temperature": 0.0, "avg_logprob": -0.08864579149471816, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0010815089335665107}, {"id": 354, "seek": 214160, "start": 2155.92, "end": 2160.08, "text": " doesn't matter what it is for now. Okay. This function takes an s and produces a z.", "tokens": [51080, 1177, 380, 1871, 437, 309, 307, 337, 586, 13, 1033, 13, 639, 2445, 2516, 364, 262, 293, 14725, 257, 710, 13, 51288], "temperature": 0.0, "avg_logprob": -0.08864579149471816, "compression_ratio": 1.6071428571428572, "no_speech_prob": 0.0010815089335665107}, {"id": 355, "seek": 216008, "start": 2161.04, "end": 2172.4, "text": " We call this function h of s, right? So when we wiggle z, the cost is going to wiggle by some", "tokens": [50412, 492, 818, 341, 2445, 276, 295, 262, 11, 558, 30, 407, 562, 321, 33377, 710, 11, 264, 2063, 307, 516, 281, 33377, 538, 512, 50980], "temperature": 0.0, "avg_logprob": -0.11058695499713604, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0015486135380342603}, {"id": 356, "seek": 216008, "start": 2172.4, "end": 2179.12, "text": " quantity, right? And we divide the wiggling of z by the wiggling of c by the wiggling of z that", "tokens": [50980, 11275, 11, 558, 30, 400, 321, 9845, 264, 261, 24542, 295, 710, 538, 264, 261, 24542, 295, 269, 538, 264, 261, 24542, 295, 710, 300, 51316], "temperature": 0.0, "avg_logprob": -0.11058695499713604, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0015486135380342603}, {"id": 357, "seek": 216008, "start": 2179.12, "end": 2185.04, "text": " causes it. That gives us the partial derivative of c with respect to z. So this one term is a gradient", "tokens": [51316, 7700, 309, 13, 663, 2709, 505, 264, 14641, 13760, 295, 269, 365, 3104, 281, 710, 13, 407, 341, 472, 1433, 307, 257, 16235, 51612], "temperature": 0.0, "avg_logprob": -0.11058695499713604, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0015486135380342603}, {"id": 358, "seek": 216008, "start": 2185.04, "end": 2189.36, "text": " of c with respect to all the z's in the network. And there's one component of that gradient,", "tokens": [51612, 295, 269, 365, 3104, 281, 439, 264, 710, 311, 294, 264, 3209, 13, 400, 456, 311, 472, 6542, 295, 300, 16235, 11, 51828], "temperature": 0.0, "avg_logprob": -0.11058695499713604, "compression_ratio": 1.7824074074074074, "no_speech_prob": 0.0015486135380342603}, {"id": 359, "seek": 218936, "start": 2189.36, "end": 2196.2400000000002, "text": " which is the partial derivative of the cost with respect to that single variable z inside the", "tokens": [50364, 597, 307, 264, 14641, 13760, 295, 264, 2063, 365, 3104, 281, 300, 2167, 7006, 710, 1854, 264, 50708], "temperature": 0.0, "avg_logprob": -0.08201112747192382, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.00023409021378029138}, {"id": 360, "seek": 218936, "start": 2196.2400000000002, "end": 2203.04, "text": " network. Okay. And that really indicates how much c would wiggle if we wiggled z by some amount.", "tokens": [50708, 3209, 13, 1033, 13, 400, 300, 534, 16203, 577, 709, 269, 576, 33377, 498, 321, 261, 6249, 1493, 710, 538, 512, 2372, 13, 51048], "temperature": 0.0, "avg_logprob": -0.08201112747192382, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.00023409021378029138}, {"id": 361, "seek": 218936, "start": 2203.04, "end": 2206.96, "text": " We divide the wiggling of c by the wiggling of z and that gives us the partial derivative", "tokens": [51048, 492, 9845, 264, 261, 24542, 295, 269, 538, 264, 261, 24542, 295, 710, 293, 300, 2709, 505, 264, 14641, 13760, 51244], "temperature": 0.0, "avg_logprob": -0.08201112747192382, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.00023409021378029138}, {"id": 362, "seek": 218936, "start": 2206.96, "end": 2213.84, "text": " of c with respect to z. This is not how we're going to compute the gradient of c with respect to z,", "tokens": [51244, 295, 269, 365, 3104, 281, 710, 13, 639, 307, 406, 577, 321, 434, 516, 281, 14722, 264, 16235, 295, 269, 365, 3104, 281, 710, 11, 51588], "temperature": 0.0, "avg_logprob": -0.08201112747192382, "compression_ratio": 1.7924528301886793, "no_speech_prob": 0.00023409021378029138}, {"id": 363, "seek": 221384, "start": 2213.84, "end": 2219.2000000000003, "text": " but this is a description of what it is conceptually. Okay. Or intuitively, rather.", "tokens": [50364, 457, 341, 307, 257, 3855, 295, 437, 309, 307, 3410, 671, 13, 1033, 13, 1610, 46506, 11, 2831, 13, 50632], "temperature": 0.0, "avg_logprob": -0.12369452270807005, "compression_ratio": 1.781725888324873, "no_speech_prob": 0.0003005610196851194}, {"id": 364, "seek": 221384, "start": 2219.92, "end": 2223.44, "text": " Okay. So let's assume that we know this quantity. So we know the partial derivative", "tokens": [50668, 1033, 13, 407, 718, 311, 6552, 300, 321, 458, 341, 11275, 13, 407, 321, 458, 264, 14641, 13760, 50844], "temperature": 0.0, "avg_logprob": -0.12369452270807005, "compression_ratio": 1.781725888324873, "no_speech_prob": 0.0003005610196851194}, {"id": 365, "seek": 221384, "start": 2224.0, "end": 2231.28, "text": " of c with respect to z. Okay. So c with respect to z is this quantity here, dc over dz. Okay.", "tokens": [50872, 295, 269, 365, 3104, 281, 710, 13, 1033, 13, 407, 269, 365, 3104, 281, 710, 307, 341, 11275, 510, 11, 274, 66, 670, 274, 89, 13, 1033, 13, 51236], "temperature": 0.0, "avg_logprob": -0.12369452270807005, "compression_ratio": 1.781725888324873, "no_speech_prob": 0.0003005610196851194}, {"id": 366, "seek": 221384, "start": 2232.7200000000003, "end": 2237.84, "text": " So think of dz as the wiggling of z and dc as the wiggling of c, divide one by the other,", "tokens": [51308, 407, 519, 295, 274, 89, 382, 264, 261, 24542, 295, 710, 293, 274, 66, 382, 264, 261, 24542, 295, 269, 11, 9845, 472, 538, 264, 661, 11, 51564], "temperature": 0.0, "avg_logprob": -0.12369452270807005, "compression_ratio": 1.781725888324873, "no_speech_prob": 0.0003005610196851194}, {"id": 367, "seek": 223784, "start": 2237.84, "end": 2244.8, "text": " and you get the partial derivative of c with respect to z. What we have here is,", "tokens": [50364, 293, 291, 483, 264, 14641, 13760, 295, 269, 365, 3104, 281, 710, 13, 708, 321, 362, 510, 307, 11, 50712], "temperature": 0.0, "avg_logprob": -0.14010782675309616, "compression_ratio": 1.78, "no_speech_prob": 0.0006985760992392898}, {"id": 368, "seek": 223784, "start": 2248.4, "end": 2254.0, "text": " what we have to apply is the chain rule, the rule that tells us how to compute the", "tokens": [50892, 437, 321, 362, 281, 3079, 307, 264, 5021, 4978, 11, 264, 4978, 300, 5112, 505, 577, 281, 14722, 264, 51172], "temperature": 0.0, "avg_logprob": -0.14010782675309616, "compression_ratio": 1.78, "no_speech_prob": 0.0006985760992392898}, {"id": 369, "seek": 223784, "start": 2254.56, "end": 2259.28, "text": " derivative of a function composed of two individual functions that we apply one after the other.", "tokens": [51200, 13760, 295, 257, 2445, 18204, 295, 732, 2609, 6828, 300, 321, 3079, 472, 934, 264, 661, 13, 51436], "temperature": 0.0, "avg_logprob": -0.14010782675309616, "compression_ratio": 1.78, "no_speech_prob": 0.0006985760992392898}, {"id": 370, "seek": 223784, "start": 2259.28, "end": 2263.6000000000004, "text": " Right. So remember, chain rule, if you have a function g, then you apply to another function h,", "tokens": [51436, 1779, 13, 407, 1604, 11, 5021, 4978, 11, 498, 291, 362, 257, 2445, 290, 11, 550, 291, 3079, 281, 1071, 2445, 276, 11, 51652], "temperature": 0.0, "avg_logprob": -0.14010782675309616, "compression_ratio": 1.78, "no_speech_prob": 0.0006985760992392898}, {"id": 371, "seek": 226360, "start": 2264.3199999999997, "end": 2267.2, "text": " which is function of parameter s, and you want the derivative of it.", "tokens": [50400, 597, 307, 2445, 295, 13075, 262, 11, 293, 291, 528, 264, 13760, 295, 309, 13, 50544], "temperature": 0.0, "avg_logprob": -0.0931705015676993, "compression_ratio": 1.8008474576271187, "no_speech_prob": 0.0009545517968945205}, {"id": 372, "seek": 226360, "start": 2268.56, "end": 2272.48, "text": " The derivative of that is equal to the derivative of g at point h of s,", "tokens": [50612, 440, 13760, 295, 300, 307, 2681, 281, 264, 13760, 295, 290, 412, 935, 276, 295, 262, 11, 50808], "temperature": 0.0, "avg_logprob": -0.0931705015676993, "compression_ratio": 1.8008474576271187, "no_speech_prob": 0.0009545517968945205}, {"id": 373, "seek": 226360, "start": 2272.48, "end": 2279.44, "text": " multiplied by the derivative of h at point s. Right. That's chain rule. You know that a few", "tokens": [50808, 17207, 538, 264, 13760, 295, 276, 412, 935, 262, 13, 1779, 13, 663, 311, 5021, 4978, 13, 509, 458, 300, 257, 1326, 51156], "temperature": 0.0, "avg_logprob": -0.0931705015676993, "compression_ratio": 1.8008474576271187, "no_speech_prob": 0.0009545517968945205}, {"id": 374, "seek": 226360, "start": 2279.44, "end": 2285.2799999999997, "text": " years ago, hopefully. Now, if I want to write this in terms of partial derivative, it's the same", "tokens": [51156, 924, 2057, 11, 4696, 13, 823, 11, 498, 286, 528, 281, 2464, 341, 294, 2115, 295, 14641, 13760, 11, 309, 311, 264, 912, 51448], "temperature": 0.0, "avg_logprob": -0.0931705015676993, "compression_ratio": 1.8008474576271187, "no_speech_prob": 0.0009545517968945205}, {"id": 375, "seek": 226360, "start": 2285.2799999999997, "end": 2290.08, "text": " thing, right? Partial derivative is just a derivative just with respect to one single variable.", "tokens": [51448, 551, 11, 558, 30, 4100, 831, 13760, 307, 445, 257, 13760, 445, 365, 3104, 281, 472, 2167, 7006, 13, 51688], "temperature": 0.0, "avg_logprob": -0.0931705015676993, "compression_ratio": 1.8008474576271187, "no_speech_prob": 0.0009545517968945205}, {"id": 376, "seek": 229008, "start": 2290.08, "end": 2295.84, "text": " So I would write this something like this, dc over ds. So c really is the result of applying", "tokens": [50364, 407, 286, 576, 2464, 341, 746, 411, 341, 11, 274, 66, 670, 274, 82, 13, 407, 269, 534, 307, 264, 1874, 295, 9275, 50652], "temperature": 0.0, "avg_logprob": -0.1082758342518526, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.00035693193785846233}, {"id": 377, "seek": 229008, "start": 2295.84, "end": 2302.56, "text": " this h function to s, and then applying some unknown g function to compute c, which is kind", "tokens": [50652, 341, 276, 2445, 281, 262, 11, 293, 550, 9275, 512, 9841, 290, 2445, 281, 14722, 269, 11, 597, 307, 733, 50988], "temperature": 0.0, "avg_logprob": -0.1082758342518526, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.00035693193785846233}, {"id": 378, "seek": 229008, "start": 2302.56, "end": 2309.2, "text": " of the rest of the network plus the cost. But I'm just going to call the gradient. I'm going to", "tokens": [50988, 295, 264, 1472, 295, 264, 3209, 1804, 264, 2063, 13, 583, 286, 478, 445, 516, 281, 818, 264, 16235, 13, 286, 478, 516, 281, 51320], "temperature": 0.0, "avg_logprob": -0.1082758342518526, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.00035693193785846233}, {"id": 379, "seek": 229008, "start": 2309.2, "end": 2318.08, "text": " assume that this dc over dz is known. Someone gave it to me. So this variable here on the right,", "tokens": [51320, 6552, 300, 341, 274, 66, 670, 274, 89, 307, 2570, 13, 8734, 2729, 309, 281, 385, 13, 407, 341, 7006, 510, 322, 264, 558, 11, 51764], "temperature": 0.0, "avg_logprob": -0.1082758342518526, "compression_ratio": 1.6981981981981982, "no_speech_prob": 0.00035693193785846233}, {"id": 380, "seek": 231808, "start": 2318.08, "end": 2325.84, "text": " dc over dz is given to me, and I want to compute dc over ds. So what I need to do is write this,", "tokens": [50364, 274, 66, 670, 274, 89, 307, 2212, 281, 385, 11, 293, 286, 528, 281, 14722, 274, 66, 670, 274, 82, 13, 407, 437, 286, 643, 281, 360, 307, 2464, 341, 11, 50752], "temperature": 0.0, "avg_logprob": -0.104260878834298, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0005975959938950837}, {"id": 381, "seek": 231808, "start": 2326.7999999999997, "end": 2334.48, "text": " dc over ds equal dc over dz times dz over ds. Right. And why is this identity true? It's because", "tokens": [50800, 274, 66, 670, 274, 82, 2681, 274, 66, 670, 274, 89, 1413, 274, 89, 670, 274, 82, 13, 1779, 13, 400, 983, 307, 341, 6575, 2074, 30, 467, 311, 570, 51184], "temperature": 0.0, "avg_logprob": -0.104260878834298, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0005975959938950837}, {"id": 382, "seek": 231808, "start": 2334.48, "end": 2341.7599999999998, "text": " I can simplify by dz. It's as simple as this, right? So you have, you know, trivial algebra,", "tokens": [51184, 286, 393, 20460, 538, 274, 89, 13, 467, 311, 382, 2199, 382, 341, 11, 558, 30, 407, 291, 362, 11, 291, 458, 11, 26703, 21989, 11, 51548], "temperature": 0.0, "avg_logprob": -0.104260878834298, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0005975959938950837}, {"id": 383, "seek": 231808, "start": 2341.7599999999998, "end": 2346.08, "text": " you have dz at the denominator here, dz at the numerator here, simplify, you get dc over ds.", "tokens": [51548, 291, 362, 274, 89, 412, 264, 20687, 510, 11, 274, 89, 412, 264, 30380, 510, 11, 20460, 11, 291, 483, 274, 66, 670, 274, 82, 13, 51764], "temperature": 0.0, "avg_logprob": -0.104260878834298, "compression_ratio": 1.738532110091743, "no_speech_prob": 0.0005975959938950837}, {"id": 384, "seek": 234608, "start": 2346.56, "end": 2351.52, "text": " It's a very trivial, simple identity, which is basically just generally applied to partial derivatives.", "tokens": [50388, 467, 311, 257, 588, 26703, 11, 2199, 6575, 11, 597, 307, 1936, 445, 5101, 6456, 281, 14641, 33733, 13, 50636], "temperature": 0.0, "avg_logprob": -0.1344253732523787, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.00022689109027851373}, {"id": 385, "seek": 234608, "start": 2353.44, "end": 2360.24, "text": " Now, dz over ds, we know what it is. It's just h prime of s, just the derivative of the h function.", "tokens": [50732, 823, 11, 274, 89, 670, 274, 82, 11, 321, 458, 437, 309, 307, 13, 467, 311, 445, 276, 5835, 295, 262, 11, 445, 264, 13760, 295, 264, 276, 2445, 13, 51072], "temperature": 0.0, "avg_logprob": -0.1344253732523787, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.00022689109027851373}, {"id": 386, "seek": 234608, "start": 2361.7599999999998, "end": 2367.04, "text": " So we have this formula, dc over ds equal dc over dz, which we assume is known, times h prime of s.", "tokens": [51148, 407, 321, 362, 341, 8513, 11, 274, 66, 670, 274, 82, 2681, 274, 66, 670, 274, 89, 11, 597, 321, 6552, 307, 2570, 11, 1413, 276, 5835, 295, 262, 13, 51412], "temperature": 0.0, "avg_logprob": -0.1344253732523787, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.00022689109027851373}, {"id": 387, "seek": 234608, "start": 2368.3199999999997, "end": 2373.04, "text": " What does that mean? That means that if we have this component of the gradient of the cost function", "tokens": [51476, 708, 775, 300, 914, 30, 663, 1355, 300, 498, 321, 362, 341, 6542, 295, 264, 16235, 295, 264, 2063, 2445, 51712], "temperature": 0.0, "avg_logprob": -0.1344253732523787, "compression_ratio": 1.7296137339055795, "no_speech_prob": 0.00022689109027851373}, {"id": 388, "seek": 237304, "start": 2373.04, "end": 2380.64, "text": " with respect to z here, we multiply this by the derivative of the h function at point s,", "tokens": [50364, 365, 3104, 281, 710, 510, 11, 321, 12972, 341, 538, 264, 13760, 295, 264, 276, 2445, 412, 935, 262, 11, 50744], "temperature": 0.0, "avg_logprob": -0.06949501247196407, "compression_ratio": 1.825, "no_speech_prob": 0.0015486436896026134}, {"id": 389, "seek": 237304, "start": 2380.64, "end": 2386.32, "text": " the same point s that we had here. And what we get now is the gradient of the cost function", "tokens": [50744, 264, 912, 935, 262, 300, 321, 632, 510, 13, 400, 437, 321, 483, 586, 307, 264, 16235, 295, 264, 2063, 2445, 51028], "temperature": 0.0, "avg_logprob": -0.06949501247196407, "compression_ratio": 1.825, "no_speech_prob": 0.0015486436896026134}, {"id": 390, "seek": 237304, "start": 2386.32, "end": 2392.64, "text": " with respect to s. Now, here's the trick. If we had a chain of those h functions, we could keep", "tokens": [51028, 365, 3104, 281, 262, 13, 823, 11, 510, 311, 264, 4282, 13, 759, 321, 632, 257, 5021, 295, 729, 276, 6828, 11, 321, 727, 1066, 51344], "temperature": 0.0, "avg_logprob": -0.06949501247196407, "compression_ratio": 1.825, "no_speech_prob": 0.0015486436896026134}, {"id": 391, "seek": 237304, "start": 2392.64, "end": 2396.72, "text": " propagating this gradient backwards by just multiplying by the derivative of all those h", "tokens": [51344, 12425, 990, 341, 16235, 12204, 538, 445, 30955, 538, 264, 13760, 295, 439, 729, 276, 51548], "temperature": 0.0, "avg_logprob": -0.06949501247196407, "compression_ratio": 1.825, "no_speech_prob": 0.0015486436896026134}, {"id": 392, "seek": 239672, "start": 2396.72, "end": 2403.2799999999997, "text": " functions going backwards. And that's why it's called back propagation. So it's just a practical", "tokens": [50364, 6828, 516, 12204, 13, 400, 300, 311, 983, 309, 311, 1219, 646, 38377, 13, 407, 309, 311, 445, 257, 8496, 50692], "temperature": 0.0, "avg_logprob": -0.08147847541024751, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0014102744171395898}, {"id": 393, "seek": 239672, "start": 2403.2799999999997, "end": 2408.24, "text": " application of a chain rule. And if you want to convince yourself of this, you can run through", "tokens": [50692, 3861, 295, 257, 5021, 4978, 13, 400, 498, 291, 528, 281, 13447, 1803, 295, 341, 11, 291, 393, 1190, 807, 50940], "temperature": 0.0, "avg_logprob": -0.08147847541024751, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0014102744171395898}, {"id": 394, "seek": 239672, "start": 2408.24, "end": 2416.16, "text": " this idea of perturbation. If I twiddle s by some value, it's going to twiddle z by some value equal", "tokens": [50940, 341, 1558, 295, 40468, 399, 13, 759, 286, 683, 327, 2285, 262, 538, 512, 2158, 11, 309, 311, 516, 281, 683, 327, 2285, 710, 538, 512, 2158, 2681, 51336], "temperature": 0.0, "avg_logprob": -0.08147847541024751, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0014102744171395898}, {"id": 395, "seek": 239672, "start": 2416.16, "end": 2426.16, "text": " to ds times h prime of s, basically the slope of s. So dz equals h prime of s times ds. And then", "tokens": [51336, 281, 274, 82, 1413, 276, 5835, 295, 262, 11, 1936, 264, 13525, 295, 262, 13, 407, 274, 89, 6915, 276, 5835, 295, 262, 1413, 274, 82, 13, 400, 550, 51836], "temperature": 0.0, "avg_logprob": -0.08147847541024751, "compression_ratio": 1.7212389380530972, "no_speech_prob": 0.0014102744171395898}, {"id": 396, "seek": 242616, "start": 2426.16, "end": 2434.64, "text": " I'm going to have to multiply this by dc over dz. And so I rearrange the terms and I get immediately", "tokens": [50364, 286, 478, 516, 281, 362, 281, 12972, 341, 538, 274, 66, 670, 274, 89, 13, 400, 370, 286, 39568, 264, 2115, 293, 286, 483, 4258, 50788], "temperature": 0.0, "avg_logprob": -0.1151249782148614, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.0003739622188732028}, {"id": 397, "seek": 242616, "start": 2434.64, "end": 2443.92, "text": " that this formula dc over ds equals dc over dz times h prime of s. So we had another element in", "tokens": [50788, 300, 341, 8513, 274, 66, 670, 274, 82, 6915, 274, 66, 670, 274, 89, 1413, 276, 5835, 295, 262, 13, 407, 321, 632, 1071, 4478, 294, 51252], "temperature": 0.0, "avg_logprob": -0.1151249782148614, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.0003739622188732028}, {"id": 398, "seek": 242616, "start": 2443.92, "end": 2450.24, "text": " our multilayer net, which was the linear sum. And there, it's just a little bit more complicated,", "tokens": [51252, 527, 2120, 388, 11167, 2533, 11, 597, 390, 264, 8213, 2408, 13, 400, 456, 11, 309, 311, 445, 257, 707, 857, 544, 6179, 11, 51568], "temperature": 0.0, "avg_logprob": -0.1151249782148614, "compression_ratio": 1.5076923076923077, "no_speech_prob": 0.0003739622188732028}, {"id": 399, "seek": 245024, "start": 2450.24, "end": 2458.08, "text": " but not really. So one particular variable z here, we would like to compute the derivative,", "tokens": [50364, 457, 406, 534, 13, 407, 472, 1729, 7006, 710, 510, 11, 321, 576, 411, 281, 14722, 264, 13760, 11, 50756], "temperature": 0.0, "avg_logprob": -0.10841514252044343, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0016227455344051123}, {"id": 400, "seek": 245024, "start": 2458.08, "end": 2465.12, "text": " the partial derivative of our cost function with respect to that z. And we're going to assume that", "tokens": [50756, 264, 14641, 13760, 295, 527, 2063, 2445, 365, 3104, 281, 300, 710, 13, 400, 321, 434, 516, 281, 6552, 300, 51108], "temperature": 0.0, "avg_logprob": -0.10841514252044343, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0016227455344051123}, {"id": 401, "seek": 245024, "start": 2465.12, "end": 2470.0, "text": " we know the partial derivative of s with respect to each of those s's, the weighted sums at the", "tokens": [51108, 321, 458, 264, 14641, 13760, 295, 262, 365, 3104, 281, 1184, 295, 729, 262, 311, 11, 264, 32807, 34499, 412, 264, 51352], "temperature": 0.0, "avg_logprob": -0.10841514252044343, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0016227455344051123}, {"id": 402, "seek": 245024, "start": 2470.0, "end": 2478.72, "text": " next layer that z is going into. So z only influences c through those s's. So presumably,", "tokens": [51352, 958, 4583, 300, 710, 307, 516, 666, 13, 407, 710, 787, 21222, 269, 807, 729, 262, 311, 13, 407, 26742, 11, 51788], "temperature": 0.0, "avg_logprob": -0.10841514252044343, "compression_ratio": 1.709090909090909, "no_speech_prob": 0.0016227455344051123}, {"id": 403, "seek": 247872, "start": 2478.72, "end": 2486.64, "text": " by basically multiplying how each of those s's influence c and then multiplying by how z", "tokens": [50364, 538, 1936, 30955, 577, 1184, 295, 729, 262, 311, 6503, 269, 293, 550, 30955, 538, 577, 710, 50760], "temperature": 0.0, "avg_logprob": -0.11940418464550073, "compression_ratio": 1.6193548387096774, "no_speech_prob": 0.0006770737818442285}, {"id": 404, "seek": 247872, "start": 2487.6, "end": 2491.7599999999998, "text": " influences each of the s's and summing up, we're going to get the influence of z over c.", "tokens": [50808, 21222, 1184, 295, 264, 262, 311, 293, 2408, 2810, 493, 11, 321, 434, 516, 281, 483, 264, 6503, 295, 710, 670, 269, 13, 51016], "temperature": 0.0, "avg_logprob": -0.11940418464550073, "compression_ratio": 1.6193548387096774, "no_speech_prob": 0.0006770737818442285}, {"id": 405, "seek": 247872, "start": 2491.7599999999998, "end": 2495.12, "text": " Right? And that's the basic idea. Okay, so here's what we're going to do.", "tokens": [51016, 1779, 30, 400, 300, 311, 264, 3875, 1558, 13, 1033, 11, 370, 510, 311, 437, 321, 434, 516, 281, 360, 13, 51184], "temperature": 0.0, "avg_logprob": -0.11940418464550073, "compression_ratio": 1.6193548387096774, "no_speech_prob": 0.0006770737818442285}, {"id": 406, "seek": 249512, "start": 2495.8399999999997, "end": 2510.24, "text": " Let's say we perturb z by dz. This is going to perturb s0 by dz times w0. Okay, we multiply z", "tokens": [50400, 961, 311, 584, 321, 40468, 710, 538, 274, 89, 13, 639, 307, 516, 281, 40468, 262, 15, 538, 274, 89, 1413, 261, 15, 13, 1033, 11, 321, 12972, 710, 51120], "temperature": 0.0, "avg_logprob": -0.11975381204060145, "compression_ratio": 1.3098591549295775, "no_speech_prob": 0.0007436727173626423}, {"id": 407, "seek": 249512, "start": 2510.24, "end": 2516.96, "text": " by w0. So the derivative of this linear operation is the coefficient itself. Right? So here,", "tokens": [51120, 538, 261, 15, 13, 407, 264, 13760, 295, 341, 8213, 6916, 307, 264, 17619, 2564, 13, 1779, 30, 407, 510, 11, 51456], "temperature": 0.0, "avg_logprob": -0.11975381204060145, "compression_ratio": 1.3098591549295775, "no_speech_prob": 0.0007436727173626423}, {"id": 408, "seek": 251696, "start": 2517.6, "end": 2529.44, "text": " the perturbation is ds0 is equal to dz times w0. Okay? And now in turn, this is going to modify c,", "tokens": [50396, 264, 40468, 399, 307, 274, 82, 15, 307, 2681, 281, 274, 89, 1413, 261, 15, 13, 1033, 30, 400, 586, 294, 1261, 11, 341, 307, 516, 281, 16927, 269, 11, 50988], "temperature": 0.0, "avg_logprob": -0.1119423442416721, "compression_ratio": 1.6022727272727273, "no_speech_prob": 0.0011159146670252085}, {"id": 409, "seek": 251696, "start": 2530.16, "end": 2539.52, "text": " and we're going to multiply this quantity by dc over ds0 to get the dc, if you want. Okay?", "tokens": [51024, 293, 321, 434, 516, 281, 12972, 341, 11275, 538, 274, 66, 670, 274, 82, 15, 281, 483, 264, 274, 66, 11, 498, 291, 528, 13, 1033, 30, 51492], "temperature": 0.0, "avg_logprob": -0.1119423442416721, "compression_ratio": 1.6022727272727273, "no_speech_prob": 0.0011159146670252085}, {"id": 410, "seek": 251696, "start": 2540.64, "end": 2545.84, "text": " Now, whenever we perturb z, it's not going to perturb just s0, it's also going to perturb s1", "tokens": [51548, 823, 11, 5699, 321, 40468, 710, 11, 309, 311, 406, 516, 281, 40468, 445, 262, 15, 11, 309, 311, 611, 516, 281, 40468, 262, 16, 51808], "temperature": 0.0, "avg_logprob": -0.1119423442416721, "compression_ratio": 1.6022727272727273, "no_speech_prob": 0.0011159146670252085}, {"id": 411, "seek": 254584, "start": 2545.84, "end": 2551.28, "text": " and s2. And to see the effect on c, we're going to have to sum up the effect of the perturbation", "tokens": [50364, 293, 262, 17, 13, 400, 281, 536, 264, 1802, 322, 269, 11, 321, 434, 516, 281, 362, 281, 2408, 493, 264, 1802, 295, 264, 40468, 399, 50636], "temperature": 0.0, "avg_logprob": -0.04632135352703056, "compression_ratio": 1.953125, "no_speech_prob": 0.004005175083875656}, {"id": 412, "seek": 254584, "start": 2551.28, "end": 2557.1200000000003, "text": " on each of the s's and then sum them up to see the overall effect on c. So this is written here", "tokens": [50636, 322, 1184, 295, 264, 262, 311, 293, 550, 2408, 552, 493, 281, 536, 264, 4787, 1802, 322, 269, 13, 407, 341, 307, 3720, 510, 50928], "temperature": 0.0, "avg_logprob": -0.04632135352703056, "compression_ratio": 1.953125, "no_speech_prob": 0.004005175083875656}, {"id": 413, "seek": 254584, "start": 2557.1200000000003, "end": 2566.4, "text": " on the left. The perturbation of c is equal to the perturbation of s multiplied by the partial", "tokens": [50928, 322, 264, 1411, 13, 440, 40468, 399, 295, 269, 307, 2681, 281, 264, 40468, 399, 295, 262, 17207, 538, 264, 14641, 51392], "temperature": 0.0, "avg_logprob": -0.04632135352703056, "compression_ratio": 1.953125, "no_speech_prob": 0.004005175083875656}, {"id": 414, "seek": 254584, "start": 2566.4, "end": 2573.6000000000004, "text": " derivative of c with respect to s plus the perturbation of s1 multiplied by the partial", "tokens": [51392, 13760, 295, 269, 365, 3104, 281, 262, 1804, 264, 40468, 399, 295, 262, 16, 17207, 538, 264, 14641, 51752], "temperature": 0.0, "avg_logprob": -0.04632135352703056, "compression_ratio": 1.953125, "no_speech_prob": 0.004005175083875656}, {"id": 415, "seek": 257360, "start": 2573.6, "end": 2580.16, "text": " derivative of dc with respect to s1 plus same thing for s2. Okay? So this is the fact that,", "tokens": [50364, 13760, 295, 274, 66, 365, 3104, 281, 262, 16, 1804, 912, 551, 337, 262, 17, 13, 1033, 30, 407, 341, 307, 264, 1186, 300, 11, 50692], "temperature": 0.0, "avg_logprob": -0.11194405122236772, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0015481830341741443}, {"id": 416, "seek": 257360, "start": 2581.2799999999997, "end": 2586.88, "text": " you know, we need to take into account all the perturbations here that z may influence.", "tokens": [50748, 291, 458, 11, 321, 643, 281, 747, 666, 2696, 439, 264, 40468, 763, 510, 300, 710, 815, 6503, 13, 51028], "temperature": 0.0, "avg_logprob": -0.11194405122236772, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0015481830341741443}, {"id": 417, "seek": 257360, "start": 2588.48, "end": 2593.2799999999997, "text": " And so I can just write down now a very simple thing, you know, because dc of 0 is equal to", "tokens": [51108, 400, 370, 286, 393, 445, 2464, 760, 586, 257, 588, 2199, 551, 11, 291, 458, 11, 570, 274, 66, 295, 1958, 307, 2681, 281, 51348], "temperature": 0.0, "avg_logprob": -0.11194405122236772, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0015481830341741443}, {"id": 418, "seek": 257360, "start": 2593.2799999999997, "end": 2601.8399999999997, "text": " w0 times dz and, you know, ds of 2 is w2 times dz, I can plug this in there and just write dc", "tokens": [51348, 261, 15, 1413, 274, 89, 293, 11, 291, 458, 11, 274, 82, 295, 568, 307, 261, 17, 1413, 274, 89, 11, 286, 393, 5452, 341, 294, 456, 293, 445, 2464, 274, 66, 51776], "temperature": 0.0, "avg_logprob": -0.11194405122236772, "compression_ratio": 1.6008771929824561, "no_speech_prob": 0.0015481830341741443}, {"id": 419, "seek": 260184, "start": 2601.84, "end": 2608.48, "text": " over dz equal dc over ds0, which I assume is known, times w0 plus dc over ds1 times w1", "tokens": [50364, 670, 274, 89, 2681, 274, 66, 670, 274, 82, 15, 11, 597, 286, 6552, 307, 2570, 11, 1413, 261, 15, 1804, 274, 66, 670, 274, 82, 16, 1413, 261, 16, 50696], "temperature": 0.0, "avg_logprob": -0.07829995406301397, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.003480631858110428}, {"id": 420, "seek": 260184, "start": 2608.48, "end": 2614.32, "text": " plus dc over ds2 times w2. Okay? If I want to represent this operation graphically,", "tokens": [50696, 1804, 274, 66, 670, 274, 82, 17, 1413, 261, 17, 13, 1033, 30, 759, 286, 528, 281, 2906, 341, 6916, 4295, 984, 11, 50988], "temperature": 0.0, "avg_logprob": -0.07829995406301397, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.003480631858110428}, {"id": 421, "seek": 260184, "start": 2615.2000000000003, "end": 2622.7200000000003, "text": " this is shown on the right here. I have dc over ds0, dc over ds1, dc over ds2, which I assume", "tokens": [51032, 341, 307, 4898, 322, 264, 558, 510, 13, 286, 362, 274, 66, 670, 274, 82, 15, 11, 274, 66, 670, 274, 82, 16, 11, 274, 66, 670, 274, 82, 17, 11, 597, 286, 6552, 51408], "temperature": 0.0, "avg_logprob": -0.07829995406301397, "compression_ratio": 1.7142857142857142, "no_speech_prob": 0.003480631858110428}, {"id": 422, "seek": 262272, "start": 2622.72, "end": 2635.12, "text": " are known or given to me somehow. I compute dc over ds0 multiplied by w0 and multiply dc over ds1", "tokens": [50364, 366, 2570, 420, 2212, 281, 385, 6063, 13, 286, 14722, 274, 66, 670, 274, 82, 15, 17207, 538, 261, 15, 293, 12972, 274, 66, 670, 274, 82, 16, 50984], "temperature": 0.0, "avg_logprob": -0.093078246483436, "compression_ratio": 1.5585106382978724, "no_speech_prob": 0.005639530718326569}, {"id": 423, "seek": 262272, "start": 2635.12, "end": 2641.7599999999998, "text": " by w1, dc over ds2 by w2. I sum them up and that gives me dc over dz. Okay? It's just the formula", "tokens": [50984, 538, 261, 16, 11, 274, 66, 670, 274, 82, 17, 538, 261, 17, 13, 286, 2408, 552, 493, 293, 300, 2709, 385, 274, 66, 670, 274, 89, 13, 1033, 30, 467, 311, 445, 264, 8513, 51316], "temperature": 0.0, "avg_logprob": -0.093078246483436, "compression_ratio": 1.5585106382978724, "no_speech_prob": 0.005639530718326569}, {"id": 424, "seek": 262272, "start": 2641.7599999999998, "end": 2648.7999999999997, "text": " here. Okay? So here's the cool trick about back propagation through a linear module that computes", "tokens": [51316, 510, 13, 1033, 30, 407, 510, 311, 264, 1627, 4282, 466, 646, 38377, 807, 257, 8213, 10088, 300, 715, 1819, 51668], "temperature": 0.0, "avg_logprob": -0.093078246483436, "compression_ratio": 1.5585106382978724, "no_speech_prob": 0.005639530718326569}, {"id": 425, "seek": 264880, "start": 2648.8, "end": 2654.48, "text": " weighted sums. You take the same weights and you still compute weighted sum with those weights,", "tokens": [50364, 32807, 34499, 13, 509, 747, 264, 912, 17443, 293, 291, 920, 14722, 32807, 2408, 365, 729, 17443, 11, 50648], "temperature": 0.0, "avg_logprob": -0.10155605688327696, "compression_ratio": 1.9108910891089108, "no_speech_prob": 0.0025107471738010645}, {"id": 426, "seek": 264880, "start": 2654.48, "end": 2660.48, "text": " but you use the weights backwards. Okay? So whenever you had the unit that was sending its output to", "tokens": [50648, 457, 291, 764, 264, 17443, 12204, 13, 1033, 30, 407, 5699, 291, 632, 264, 4985, 300, 390, 7750, 1080, 5598, 281, 50948], "temperature": 0.0, "avg_logprob": -0.10155605688327696, "compression_ratio": 1.9108910891089108, "no_speech_prob": 0.0025107471738010645}, {"id": 427, "seek": 264880, "start": 2660.48, "end": 2667.2000000000003, "text": " multiple outputs to multiple units through a weight, you take the gradient of the cost with", "tokens": [50948, 3866, 23930, 281, 3866, 6815, 807, 257, 3364, 11, 291, 747, 264, 16235, 295, 264, 2063, 365, 51284], "temperature": 0.0, "avg_logprob": -0.10155605688327696, "compression_ratio": 1.9108910891089108, "no_speech_prob": 0.0025107471738010645}, {"id": 428, "seek": 264880, "start": 2667.2000000000003, "end": 2674.0, "text": " respect to all those weighted sums and you compute their weighted sum backwards using the weights", "tokens": [51284, 3104, 281, 439, 729, 32807, 34499, 293, 291, 14722, 641, 32807, 2408, 12204, 1228, 264, 17443, 51624], "temperature": 0.0, "avg_logprob": -0.10155605688327696, "compression_ratio": 1.9108910891089108, "no_speech_prob": 0.0025107471738010645}, {"id": 429, "seek": 267400, "start": 2674.0, "end": 2681.84, "text": " backwards to get the gradient with respect to the state of the unit at the bottom. You can do", "tokens": [50364, 12204, 281, 483, 264, 16235, 365, 3104, 281, 264, 1785, 295, 264, 4985, 412, 264, 2767, 13, 509, 393, 360, 50756], "temperature": 0.0, "avg_logprob": -0.10001630985990484, "compression_ratio": 1.5423728813559323, "no_speech_prob": 0.0006562324124388397}, {"id": 430, "seek": 267400, "start": 2681.84, "end": 2688.72, "text": " this for all the units. Okay? So it's super simple. Now, if you were to write a program to do backprop", "tokens": [50756, 341, 337, 439, 264, 6815, 13, 1033, 30, 407, 309, 311, 1687, 2199, 13, 823, 11, 498, 291, 645, 281, 2464, 257, 1461, 281, 360, 646, 79, 1513, 51100], "temperature": 0.0, "avg_logprob": -0.10001630985990484, "compression_ratio": 1.5423728813559323, "no_speech_prob": 0.0006562324124388397}, {"id": 431, "seek": 267400, "start": 2688.72, "end": 2694.88, "text": " for classical neural nets in Python, it would take like half a page. It's very, very simple.", "tokens": [51100, 337, 13735, 18161, 36170, 294, 15329, 11, 309, 576, 747, 411, 1922, 257, 3028, 13, 467, 311, 588, 11, 588, 2199, 13, 51408], "temperature": 0.0, "avg_logprob": -0.10001630985990484, "compression_ratio": 1.5423728813559323, "no_speech_prob": 0.0006562324124388397}, {"id": 432, "seek": 267400, "start": 2697.52, "end": 2701.28, "text": " Is one function to compute weighted sums going forward in the right order?", "tokens": [51540, 1119, 472, 2445, 281, 14722, 32807, 34499, 516, 2128, 294, 264, 558, 1668, 30, 51728], "temperature": 0.0, "avg_logprob": -0.10001630985990484, "compression_ratio": 1.5423728813559323, "no_speech_prob": 0.0006562324124388397}, {"id": 433, "seek": 270128, "start": 2701.36, "end": 2707.84, "text": " Another function and applying the nonlinearity is another function to compute weighted sums", "tokens": [50368, 3996, 2445, 293, 9275, 264, 2107, 1889, 17409, 307, 1071, 2445, 281, 14722, 32807, 34499, 50692], "temperature": 0.0, "avg_logprob": -0.11151190237565474, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.0003798629331868142}, {"id": 434, "seek": 270128, "start": 2707.84, "end": 2713.1200000000003, "text": " weighted sums going backward and multiplying by the derivative of the nonlinearity at every step.", "tokens": [50692, 32807, 34499, 516, 23897, 293, 30955, 538, 264, 13760, 295, 264, 2107, 1889, 17409, 412, 633, 1823, 13, 50956], "temperature": 0.0, "avg_logprob": -0.11151190237565474, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.0003798629331868142}, {"id": 435, "seek": 270128, "start": 2714.1600000000003, "end": 2719.2000000000003, "text": " Right? It's incredibly simple. What's surprising is that it took so long for people to realize this", "tokens": [51008, 1779, 30, 467, 311, 6252, 2199, 13, 708, 311, 8830, 307, 300, 309, 1890, 370, 938, 337, 561, 281, 4325, 341, 51260], "temperature": 0.0, "avg_logprob": -0.11151190237565474, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.0003798629331868142}, {"id": 436, "seek": 270128, "start": 2719.2000000000003, "end": 2726.48, "text": " was so useful, maybe because it was too simple. Okay? So it's useful to write this in matrix form.", "tokens": [51260, 390, 370, 4420, 11, 1310, 570, 309, 390, 886, 2199, 13, 1033, 30, 407, 309, 311, 4420, 281, 2464, 341, 294, 8141, 1254, 13, 51624], "temperature": 0.0, "avg_logprob": -0.11151190237565474, "compression_ratio": 1.6652360515021458, "no_speech_prob": 0.0003798629331868142}, {"id": 437, "seek": 272648, "start": 2726.48, "end": 2734.8, "text": " So really, the way you should think about a neural net of this type is each state inside the", "tokens": [50364, 407, 534, 11, 264, 636, 291, 820, 519, 466, 257, 18161, 2533, 295, 341, 2010, 307, 1184, 1785, 1854, 264, 50780], "temperature": 0.0, "avg_logprob": -0.0919970248607879, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0004440121992956847}, {"id": 438, "seek": 272648, "start": 2734.8, "end": 2738.56, "text": " network, think of it as a vector. It could be a multidimensional array, but let's think of it", "tokens": [50780, 3209, 11, 519, 295, 309, 382, 257, 8062, 13, 467, 727, 312, 257, 2120, 327, 332, 11075, 10225, 11, 457, 718, 311, 519, 295, 309, 50968], "temperature": 0.0, "avg_logprob": -0.0919970248607879, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0004440121992956847}, {"id": 439, "seek": 272648, "start": 2738.56, "end": 2744.0, "text": " just as a vector. A linear operation is just going to multiply this vector by matrix and each row of", "tokens": [50968, 445, 382, 257, 8062, 13, 316, 8213, 6916, 307, 445, 516, 281, 12972, 341, 8062, 538, 8141, 293, 1184, 5386, 295, 51240], "temperature": 0.0, "avg_logprob": -0.0919970248607879, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0004440121992956847}, {"id": 440, "seek": 272648, "start": 2744.0, "end": 2748.88, "text": " the matrix contains all the weights that are used to compute a particular weighted sum for a particular", "tokens": [51240, 264, 8141, 8306, 439, 264, 17443, 300, 366, 1143, 281, 14722, 257, 1729, 32807, 2408, 337, 257, 1729, 51484], "temperature": 0.0, "avg_logprob": -0.0919970248607879, "compression_ratio": 1.7149122807017543, "no_speech_prob": 0.0004440121992956847}, {"id": 441, "seek": 274888, "start": 2748.88, "end": 2758.56, "text": " unit. Okay? So multiply this by this matrix. So this dimension has to be equal to that dimension,", "tokens": [50364, 4985, 13, 1033, 30, 407, 12972, 341, 538, 341, 8141, 13, 407, 341, 10139, 575, 281, 312, 2681, 281, 300, 10139, 11, 50848], "temperature": 0.0, "avg_logprob": -0.20557868175017527, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0047941673547029495}, {"id": 442, "seek": 274888, "start": 2758.56, "end": 2762.56, "text": " which is not really well depicted here, actually. One sec. From the previous slide,", "tokens": [50848, 597, 307, 406, 534, 731, 30207, 510, 11, 767, 13, 1485, 907, 13, 3358, 264, 3894, 4137, 11, 51048], "temperature": 0.0, "avg_logprob": -0.20557868175017527, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0047941673547029495}, {"id": 443, "seek": 274888, "start": 2762.56, "end": 2771.04, "text": " you wrote ds0. What is s, differentiated with respect to? So there is a ds. What is ds, basically?", "tokens": [51048, 291, 4114, 274, 82, 15, 13, 708, 307, 262, 11, 27372, 770, 365, 3104, 281, 30, 407, 456, 307, 257, 274, 82, 13, 708, 307, 274, 82, 11, 1936, 30, 51472], "temperature": 0.0, "avg_logprob": -0.20557868175017527, "compression_ratio": 1.4814814814814814, "no_speech_prob": 0.0047941673547029495}, {"id": 444, "seek": 277104, "start": 2771.6, "end": 2781.6, "text": " ds0, you mean? Yeah. Okay. ds0 is a perturbation of s0. Okay? An infinitely small perturbation of s0.", "tokens": [50392, 274, 82, 15, 11, 291, 914, 30, 865, 13, 1033, 13, 274, 82, 15, 307, 257, 40468, 399, 295, 262, 15, 13, 1033, 30, 1107, 36227, 1359, 40468, 399, 295, 262, 15, 13, 50892], "temperature": 0.0, "avg_logprob": -0.10535198809152627, "compression_ratio": 1.727810650887574, "no_speech_prob": 0.0019566118717193604}, {"id": 445, "seek": 277104, "start": 2783.04, "end": 2788.88, "text": " Doesn't matter what it is. Okay? And what we're saying here is that if you have an infinitely", "tokens": [50964, 12955, 380, 1871, 437, 309, 307, 13, 1033, 30, 400, 437, 321, 434, 1566, 510, 307, 300, 498, 291, 362, 364, 36227, 51256], "temperature": 0.0, "avg_logprob": -0.10535198809152627, "compression_ratio": 1.727810650887574, "no_speech_prob": 0.0019566118717193604}, {"id": 446, "seek": 277104, "start": 2788.88, "end": 2794.96, "text": " small perturbation of s0, and you multiply this perturbation by the partial derivative of c with", "tokens": [51256, 1359, 40468, 399, 295, 262, 15, 11, 293, 291, 12972, 341, 40468, 399, 538, 264, 14641, 13760, 295, 269, 365, 51560], "temperature": 0.0, "avg_logprob": -0.10535198809152627, "compression_ratio": 1.727810650887574, "no_speech_prob": 0.0019566118717193604}, {"id": 447, "seek": 279496, "start": 2794.96, "end": 2805.28, "text": " respect to s0, okay? You get the perturbation of c, except that that corresponds to this", "tokens": [50364, 3104, 281, 262, 15, 11, 1392, 30, 509, 483, 264, 40468, 399, 295, 269, 11, 3993, 300, 300, 23249, 281, 341, 50880], "temperature": 0.0, "avg_logprob": -0.1027535636826317, "compression_ratio": 1.8838383838383839, "no_speech_prob": 0.0004802540352102369}, {"id": 448, "seek": 279496, "start": 2805.28, "end": 2809.44, "text": " perturbation of s0, right? But we're not interested in just the perturbation of s0. We're", "tokens": [50880, 40468, 399, 295, 262, 15, 11, 558, 30, 583, 321, 434, 406, 3102, 294, 445, 264, 40468, 399, 295, 262, 15, 13, 492, 434, 51088], "temperature": 0.0, "avg_logprob": -0.1027535636826317, "compression_ratio": 1.8838383838383839, "no_speech_prob": 0.0004802540352102369}, {"id": 449, "seek": 279496, "start": 2809.44, "end": 2814.08, "text": " also interested in the perturbation of s1 and s2. So the overall perturbation of c would be the sum", "tokens": [51088, 611, 3102, 294, 264, 40468, 399, 295, 262, 16, 293, 262, 17, 13, 407, 264, 4787, 40468, 399, 295, 269, 576, 312, 264, 2408, 51320], "temperature": 0.0, "avg_logprob": -0.1027535636826317, "compression_ratio": 1.8838383838383839, "no_speech_prob": 0.0004802540352102369}, {"id": 450, "seek": 279496, "start": 2814.08, "end": 2820.16, "text": " of the perturbations of s0, s1, and s2 multiplied by the corresponding partial derivative of c", "tokens": [51320, 295, 264, 40468, 763, 295, 262, 15, 11, 262, 16, 11, 293, 262, 17, 17207, 538, 264, 11760, 14641, 13760, 295, 269, 51624], "temperature": 0.0, "avg_logprob": -0.1027535636826317, "compression_ratio": 1.8838383838383839, "no_speech_prob": 0.0004802540352102369}, {"id": 451, "seek": 282016, "start": 2820.16, "end": 2826.96, "text": " with respect to each of them. Okay? You know, it's a virtual thing, right? It's not an existing", "tokens": [50364, 365, 3104, 281, 1184, 295, 552, 13, 1033, 30, 509, 458, 11, 309, 311, 257, 6374, 551, 11, 558, 30, 467, 311, 406, 364, 6741, 50704], "temperature": 0.0, "avg_logprob": -0.07313172912597657, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.0009398050024174154}, {"id": 452, "seek": 282016, "start": 2826.96, "end": 2832.08, "text": " thing you're going to manipulate. Just imagine that there is some perturbation of s0 here.", "tokens": [50704, 551, 291, 434, 516, 281, 20459, 13, 1449, 3811, 300, 456, 307, 512, 40468, 399, 295, 262, 15, 510, 13, 50960], "temperature": 0.0, "avg_logprob": -0.07313172912597657, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.0009398050024174154}, {"id": 453, "seek": 282016, "start": 2833.2, "end": 2836.8799999999997, "text": " Okay? This is going to perturb c by some value, and that value is going to be the perturbation", "tokens": [51016, 1033, 30, 639, 307, 516, 281, 40468, 269, 538, 512, 2158, 11, 293, 300, 2158, 307, 516, 281, 312, 264, 40468, 399, 51200], "temperature": 0.0, "avg_logprob": -0.07313172912597657, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.0009398050024174154}, {"id": 454, "seek": 282016, "start": 2836.8799999999997, "end": 2842.64, "text": " of s0 multiplied by the partial derivative of c with respect to s0. Okay? And then if you perturb", "tokens": [51200, 295, 262, 15, 17207, 538, 264, 14641, 13760, 295, 269, 365, 3104, 281, 262, 15, 13, 1033, 30, 400, 550, 498, 291, 40468, 51488], "temperature": 0.0, "avg_logprob": -0.07313172912597657, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.0009398050024174154}, {"id": 455, "seek": 282016, "start": 2843.3599999999997, "end": 2849.44, "text": " s1 simultaneously, you're also going to cause a perturbation of c. If you perturb s2 simultaneously,", "tokens": [51524, 262, 16, 16561, 11, 291, 434, 611, 516, 281, 3082, 257, 40468, 399, 295, 269, 13, 759, 291, 40468, 262, 17, 16561, 11, 51828], "temperature": 0.0, "avg_logprob": -0.07313172912597657, "compression_ratio": 1.8972332015810276, "no_speech_prob": 0.0009398050024174154}, {"id": 456, "seek": 284944, "start": 2849.44, "end": 2854.56, "text": " you're also going to cause a perturbation of c. The overall perturbation of c will be the sum", "tokens": [50364, 291, 434, 611, 516, 281, 3082, 257, 40468, 399, 295, 269, 13, 440, 4787, 40468, 399, 295, 269, 486, 312, 264, 2408, 50620], "temperature": 0.0, "avg_logprob": -0.0826709961222711, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.000527411000803113}, {"id": 457, "seek": 284944, "start": 2854.56, "end": 2860.96, "text": " of those perturbations, and that is given by this expression here. Now, those d, those infinitely", "tokens": [50620, 295, 729, 40468, 763, 11, 293, 300, 307, 2212, 538, 341, 6114, 510, 13, 823, 11, 729, 274, 11, 729, 36227, 50940], "temperature": 0.0, "avg_logprob": -0.0826709961222711, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.000527411000803113}, {"id": 458, "seek": 284944, "start": 2860.96, "end": 2867.6, "text": " small quantities, ds, dc, etc., think of them as, you know, numbers. You can do algebra with them.", "tokens": [50940, 1359, 22927, 11, 274, 82, 11, 274, 66, 11, 5183, 7933, 519, 295, 552, 382, 11, 291, 458, 11, 3547, 13, 509, 393, 360, 21989, 365, 552, 13, 51272], "temperature": 0.0, "avg_logprob": -0.0826709961222711, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.000527411000803113}, {"id": 459, "seek": 284944, "start": 2867.6, "end": 2871.68, "text": " You can divide one by the other. You know, you can do stuff like that. So now you say, you know,", "tokens": [51272, 509, 393, 9845, 472, 538, 264, 661, 13, 509, 458, 11, 291, 393, 360, 1507, 411, 300, 13, 407, 586, 291, 584, 11, 291, 458, 11, 51476], "temperature": 0.0, "avg_logprob": -0.0826709961222711, "compression_ratio": 1.7432432432432432, "no_speech_prob": 0.000527411000803113}, {"id": 460, "seek": 287168, "start": 2871.68, "end": 2886.08, "text": " what is ds0 equal to? If I tweak z by a quantity dz, it's going in turn to modify s0 by ds0.", "tokens": [50364, 437, 307, 274, 82, 15, 2681, 281, 30, 759, 286, 29879, 710, 538, 257, 11275, 274, 89, 11, 309, 311, 516, 294, 1261, 281, 16927, 262, 15, 538, 274, 82, 15, 13, 51084], "temperature": 0.0, "avg_logprob": -0.11980053037405014, "compression_ratio": 1.4206349206349207, "no_speech_prob": 0.0059057073667645454}, {"id": 461, "seek": 287168, "start": 2886.08, "end": 2894.8799999999997, "text": " Okay? And what is the quantity by which s0 is going to be tweaked? If I tweak z by dz,", "tokens": [51084, 1033, 30, 400, 437, 307, 264, 11275, 538, 597, 262, 15, 307, 516, 281, 312, 6986, 7301, 30, 759, 286, 29879, 710, 538, 274, 89, 11, 51524], "temperature": 0.0, "avg_logprob": -0.11980053037405014, "compression_ratio": 1.4206349206349207, "no_speech_prob": 0.0059057073667645454}, {"id": 462, "seek": 289488, "start": 2894.88, "end": 2902.4, "text": " because s is the result of computing the product of z by w0, then the perturbation is also going", "tokens": [50364, 570, 262, 307, 264, 1874, 295, 15866, 264, 1674, 295, 710, 538, 261, 15, 11, 550, 264, 40468, 399, 307, 611, 516, 50740], "temperature": 0.0, "avg_logprob": -0.1223459243774414, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.004006189294159412}, {"id": 463, "seek": 289488, "start": 2902.4, "end": 2909.36, "text": " to be multiplied by w0, right? So the ds0 corresponding to a particular dz is going", "tokens": [50740, 281, 312, 17207, 538, 261, 15, 11, 558, 30, 407, 264, 274, 82, 15, 11760, 281, 257, 1729, 274, 89, 307, 516, 51088], "temperature": 0.0, "avg_logprob": -0.1223459243774414, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.004006189294159412}, {"id": 464, "seek": 289488, "start": 2909.36, "end": 2916.0, "text": " to be equal to dz times w0. And this is what's expressed here. Okay? ds0 equal w0 dz.", "tokens": [51088, 281, 312, 2681, 281, 274, 89, 1413, 261, 15, 13, 400, 341, 307, 437, 311, 12675, 510, 13, 1033, 30, 274, 82, 15, 2681, 261, 15, 274, 89, 13, 51420], "temperature": 0.0, "avg_logprob": -0.1223459243774414, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.004006189294159412}, {"id": 465, "seek": 289488, "start": 2917.2000000000003, "end": 2922.1600000000003, "text": " Okay. Now, if I take this expression for ds0 and I insert it here in this formula,", "tokens": [51480, 1033, 13, 823, 11, 498, 286, 747, 341, 6114, 337, 274, 82, 15, 293, 286, 8969, 309, 510, 294, 341, 8513, 11, 51728], "temperature": 0.0, "avg_logprob": -0.1223459243774414, "compression_ratio": 1.608294930875576, "no_speech_prob": 0.004006189294159412}, {"id": 466, "seek": 292216, "start": 2923.04, "end": 2930.96, "text": " okay, I get dc equal w0 times dz times dc over ds0 plus same thing for 1 plus same thing for 2.", "tokens": [50408, 1392, 11, 286, 483, 274, 66, 2681, 261, 15, 1413, 274, 89, 1413, 274, 66, 670, 274, 82, 15, 1804, 912, 551, 337, 502, 1804, 912, 551, 337, 568, 13, 50804], "temperature": 0.0, "avg_logprob": -0.12147848341200086, "compression_ratio": 1.6123595505617978, "no_speech_prob": 0.0010647564195096493}, {"id": 467, "seek": 292216, "start": 2930.96, "end": 2937.3599999999997, "text": " And I'm going to take the dz and pass it to the other side. I'm going to divide both sides by dz.", "tokens": [50804, 400, 286, 478, 516, 281, 747, 264, 274, 89, 293, 1320, 309, 281, 264, 661, 1252, 13, 286, 478, 516, 281, 9845, 1293, 4881, 538, 274, 89, 13, 51124], "temperature": 0.0, "avg_logprob": -0.12147848341200086, "compression_ratio": 1.6123595505617978, "no_speech_prob": 0.0010647564195096493}, {"id": 468, "seek": 292216, "start": 2937.3599999999997, "end": 2944.0, "text": " So now I get dc over dz equal, the dz doesn't appear anymore because it's been put underneath", "tokens": [51124, 407, 586, 286, 483, 274, 66, 670, 274, 89, 2681, 11, 264, 274, 89, 1177, 380, 4204, 3602, 570, 309, 311, 668, 829, 7223, 51456], "temperature": 0.0, "avg_logprob": -0.12147848341200086, "compression_ratio": 1.6123595505617978, "no_speech_prob": 0.0010647564195096493}, {"id": 469, "seek": 294400, "start": 2944.0, "end": 2953.28, "text": " here. It's w0 times dc over ds0 plus w1 times dc over ds1, et cetera. Okay? It's just simple algebra.", "tokens": [50364, 510, 13, 467, 311, 261, 15, 1413, 274, 66, 670, 274, 82, 15, 1804, 261, 16, 1413, 274, 66, 670, 274, 82, 16, 11, 1030, 11458, 13, 1033, 30, 467, 311, 445, 2199, 21989, 13, 50828], "temperature": 0.0, "avg_logprob": -0.12499324754736889, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.002980659483000636}, {"id": 470, "seek": 294400, "start": 2955.36, "end": 2960.16, "text": " It's differential calculus, basically. Right. So it's better to write this in matrix form.", "tokens": [50932, 467, 311, 15756, 33400, 11, 1936, 13, 1779, 13, 407, 309, 311, 1101, 281, 2464, 341, 294, 8141, 1254, 13, 51172], "temperature": 0.0, "avg_logprob": -0.12499324754736889, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.002980659483000636}, {"id": 471, "seek": 294400, "start": 2961.68, "end": 2970.24, "text": " So really, when you're computing, if I go back a few slides, when this is really kind of a matrix", "tokens": [51248, 407, 534, 11, 562, 291, 434, 15866, 11, 498, 286, 352, 646, 257, 1326, 9788, 11, 562, 341, 307, 534, 733, 295, 257, 8141, 51676], "temperature": 0.0, "avg_logprob": -0.12499324754736889, "compression_ratio": 1.5104166666666667, "no_speech_prob": 0.002980659483000636}, {"id": 472, "seek": 297024, "start": 2970.24, "end": 2976.3999999999996, "text": " of all the weights that are kind of upstream of the zj's, so you can align the zj as a vector,", "tokens": [50364, 295, 439, 264, 17443, 300, 366, 733, 295, 33915, 295, 264, 710, 73, 311, 11, 370, 291, 393, 7975, 264, 710, 73, 382, 257, 8062, 11, 50672], "temperature": 0.0, "avg_logprob": -0.11157119387672061, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0006875252583995461}, {"id": 473, "seek": 297024, "start": 2977.6, "end": 2985.3599999999997, "text": " maybe only the zj's that have nonzero terms in w, wij. And then you can write those w's", "tokens": [50732, 1310, 787, 264, 710, 73, 311, 300, 362, 2107, 32226, 2115, 294, 261, 11, 261, 1718, 13, 400, 550, 291, 393, 2464, 729, 261, 311, 51120], "temperature": 0.0, "avg_logprob": -0.11157119387672061, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0006875252583995461}, {"id": 474, "seek": 297024, "start": 2985.3599999999997, "end": 2991.12, "text": " as a matrix, and this is just a matrix vector product. Okay? So this is the way this would be", "tokens": [51120, 382, 257, 8141, 11, 293, 341, 307, 445, 257, 8141, 8062, 1674, 13, 1033, 30, 407, 341, 307, 264, 636, 341, 576, 312, 51408], "temperature": 0.0, "avg_logprob": -0.11157119387672061, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0006875252583995461}, {"id": 475, "seek": 297024, "start": 2991.12, "end": 2995.52, "text": " written. You have a vector, you multiply by matrix, you get a new vector, pass that through", "tokens": [51408, 3720, 13, 509, 362, 257, 8062, 11, 291, 12972, 538, 8141, 11, 291, 483, 257, 777, 8062, 11, 1320, 300, 807, 51628], "temperature": 0.0, "avg_logprob": -0.11157119387672061, "compression_ratio": 1.688073394495413, "no_speech_prob": 0.0006875252583995461}, {"id": 476, "seek": 299552, "start": 2995.52, "end": 3001.84, "text": " nonlinearities, reuse, multiply that by matrix, et cetera. Right? So symbolically, you can write", "tokens": [50364, 2107, 28263, 1088, 11, 26225, 11, 12972, 300, 538, 8141, 11, 1030, 11458, 13, 1779, 30, 407, 5986, 984, 11, 291, 393, 2464, 50680], "temperature": 0.0, "avg_logprob": -0.11105281174784959, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.00039808248402550817}, {"id": 477, "seek": 299552, "start": 3002.4, "end": 3008.0, "text": " a simple neural net this way. We have linear blocks, okay, linear functional blocks, which", "tokens": [50708, 257, 2199, 18161, 2533, 341, 636, 13, 492, 362, 8213, 8474, 11, 1392, 11, 8213, 11745, 8474, 11, 597, 50988], "temperature": 0.0, "avg_logprob": -0.11105281174784959, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.00039808248402550817}, {"id": 478, "seek": 299552, "start": 3008.0, "end": 3015.2, "text": " basically take the previous state and multiply by matrix. Okay? So you have a state here, z1,", "tokens": [50988, 1936, 747, 264, 3894, 1785, 293, 12972, 538, 8141, 13, 1033, 30, 407, 291, 362, 257, 1785, 510, 11, 710, 16, 11, 51348], "temperature": 0.0, "avg_logprob": -0.11105281174784959, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.00039808248402550817}, {"id": 479, "seek": 299552, "start": 3015.2, "end": 3021.7599999999998, "text": " multiply by matrix, you get w1, z1, and that gives you the vector of weighted sums, s2.", "tokens": [51348, 12972, 538, 8141, 11, 291, 483, 261, 16, 11, 710, 16, 11, 293, 300, 2709, 291, 264, 8062, 295, 32807, 34499, 11, 262, 17, 13, 51676], "temperature": 0.0, "avg_logprob": -0.11105281174784959, "compression_ratio": 1.618421052631579, "no_speech_prob": 0.00039808248402550817}, {"id": 480, "seek": 302176, "start": 3021.76, "end": 3029.44, "text": " Okay? Then you take that, pass it through the nonlinear functions, each component individually,", "tokens": [50364, 1033, 30, 1396, 291, 747, 300, 11, 1320, 309, 807, 264, 2107, 28263, 6828, 11, 1184, 6542, 16652, 11, 50748], "temperature": 0.0, "avg_logprob": -0.12280183089406867, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0003005573234986514}, {"id": 481, "seek": 302176, "start": 3030.1600000000003, "end": 3036.8, "text": " and that gives you z2. Right? So that's a three-layer neural net. First weight matrix,", "tokens": [50784, 293, 300, 2709, 291, 710, 17, 13, 1779, 30, 407, 300, 311, 257, 1045, 12, 8376, 260, 18161, 2533, 13, 2386, 3364, 8141, 11, 51116], "temperature": 0.0, "avg_logprob": -0.12280183089406867, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0003005573234986514}, {"id": 482, "seek": 302176, "start": 3036.8, "end": 3041.5200000000004, "text": " nonlinearity, second weight matrix, nonlinearity, third weight matrix, and this is the output.", "tokens": [51116, 2107, 1889, 17409, 11, 1150, 3364, 8141, 11, 2107, 1889, 17409, 11, 2636, 3364, 8141, 11, 293, 341, 307, 264, 5598, 13, 51352], "temperature": 0.0, "avg_logprob": -0.12280183089406867, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0003005573234986514}, {"id": 483, "seek": 302176, "start": 3041.5200000000004, "end": 3047.92, "text": " There are two hidden layers, three layers of weights. Okay, the reason for writing it this way", "tokens": [51352, 821, 366, 732, 7633, 7914, 11, 1045, 7914, 295, 17443, 13, 1033, 11, 264, 1778, 337, 3579, 309, 341, 636, 51672], "temperature": 0.0, "avg_logprob": -0.12280183089406867, "compression_ratio": 1.6607142857142858, "no_speech_prob": 0.0003005573234986514}, {"id": 484, "seek": 304792, "start": 3047.92, "end": 3054.7200000000003, "text": " is that this is, like symbolically, the easiest way to understand really what kind of backprop", "tokens": [50364, 307, 300, 341, 307, 11, 411, 5986, 984, 11, 264, 12889, 636, 281, 1223, 534, 437, 733, 295, 646, 79, 1513, 50704], "temperature": 0.0, "avg_logprob": -0.14120328085763115, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0008159633725881577}, {"id": 485, "seek": 304792, "start": 3054.7200000000003, "end": 3061.12, "text": " does. And in fact, it corresponds also to the way we define neural nets and we run them on", "tokens": [50704, 775, 13, 400, 294, 1186, 11, 309, 23249, 611, 281, 264, 636, 321, 6964, 18161, 36170, 293, 321, 1190, 552, 322, 51024], "temperature": 0.0, "avg_logprob": -0.14120328085763115, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0008159633725881577}, {"id": 486, "seek": 304792, "start": 3063.12, "end": 3071.36, "text": " deep learning frameworks like PyTorch. So this is the sort of object-oriented version of", "tokens": [51124, 2452, 2539, 29834, 411, 9953, 51, 284, 339, 13, 407, 341, 307, 264, 1333, 295, 2657, 12, 27414, 3037, 295, 51536], "temperature": 0.0, "avg_logprob": -0.14120328085763115, "compression_ratio": 1.46524064171123, "no_speech_prob": 0.0008159633725881577}, {"id": 487, "seek": 307136, "start": 3072.32, "end": 3079.44, "text": " defining a neural net in PyTorch. We're going to use predefined class, which are the linear class", "tokens": [50412, 17827, 257, 18161, 2533, 294, 9953, 51, 284, 339, 13, 492, 434, 516, 281, 764, 659, 37716, 1508, 11, 597, 366, 264, 8213, 1508, 50768], "temperature": 0.0, "avg_logprob": -0.1126097618265355, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.0004172611515969038}, {"id": 488, "seek": 307136, "start": 3080.48, "end": 3087.04, "text": " that basically multiplies a vector by matrix. It also has biases, but let's not talk about this", "tokens": [50820, 300, 1936, 12788, 530, 257, 8062, 538, 8141, 13, 467, 611, 575, 32152, 11, 457, 718, 311, 406, 751, 466, 341, 51148], "temperature": 0.0, "avg_logprob": -0.1126097618265355, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.0004172611515969038}, {"id": 489, "seek": 307136, "start": 3087.04, "end": 3092.32, "text": " just now. And another class, which is the value function, which takes a vector or a multi-dimensional", "tokens": [51148, 445, 586, 13, 400, 1071, 1508, 11, 597, 307, 264, 2158, 2445, 11, 597, 2516, 257, 8062, 420, 257, 4825, 12, 18759, 51412], "temperature": 0.0, "avg_logprob": -0.1126097618265355, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.0004172611515969038}, {"id": 490, "seek": 307136, "start": 3092.32, "end": 3098.8, "text": " array and applies the nonlinear function to every component separately. Okay, so this is", "tokens": [51412, 10225, 293, 13165, 264, 2107, 28263, 2445, 281, 633, 6542, 14759, 13, 1033, 11, 370, 341, 307, 51736], "temperature": 0.0, "avg_logprob": -0.1126097618265355, "compression_ratio": 1.5933609958506223, "no_speech_prob": 0.0004172611515969038}, {"id": 491, "seek": 309880, "start": 3098.8, "end": 3106.0, "text": " a little piece of Python program that uses Torch. We import Torch. We make an image, which is, you", "tokens": [50364, 257, 707, 2522, 295, 15329, 1461, 300, 4960, 7160, 339, 13, 492, 974, 7160, 339, 13, 492, 652, 364, 3256, 11, 597, 307, 11, 291, 50724], "temperature": 0.0, "avg_logprob": -0.09373586139981709, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0010646533919498324}, {"id": 492, "seek": 309880, "start": 3106.0, "end": 3111.52, "text": " know, 10 pixels by 20 pixels and three components for color. We compute the size of it and we're", "tokens": [50724, 458, 11, 1266, 18668, 538, 945, 18668, 293, 1045, 6677, 337, 2017, 13, 492, 14722, 264, 2744, 295, 309, 293, 321, 434, 51000], "temperature": 0.0, "avg_logprob": -0.09373586139981709, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0010646533919498324}, {"id": 493, "seek": 309880, "start": 3111.52, "end": 3116.32, "text": " going to plug a neural net where the number of inputs is the number of components of our image.", "tokens": [51000, 516, 281, 5452, 257, 18161, 2533, 689, 264, 1230, 295, 15743, 307, 264, 1230, 295, 6677, 295, 527, 3256, 13, 51240], "temperature": 0.0, "avg_logprob": -0.09373586139981709, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0010646533919498324}, {"id": 494, "seek": 309880, "start": 3116.32, "end": 3122.0, "text": " So in this case, that would be 600 or so. And we're going to define a class. The class is going", "tokens": [51240, 407, 294, 341, 1389, 11, 300, 576, 312, 11849, 420, 370, 13, 400, 321, 434, 516, 281, 6964, 257, 1508, 13, 440, 1508, 307, 516, 51524], "temperature": 0.0, "avg_logprob": -0.09373586139981709, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0010646533919498324}, {"id": 495, "seek": 309880, "start": 3122.0, "end": 3126.7200000000003, "text": " to define a neural net and that's pretty much all we need to do here. So we define our network", "tokens": [51524, 281, 6964, 257, 18161, 2533, 293, 300, 311, 1238, 709, 439, 321, 643, 281, 360, 510, 13, 407, 321, 6964, 527, 3209, 51760], "temperature": 0.0, "avg_logprob": -0.09373586139981709, "compression_ratio": 1.7785977859778597, "no_speech_prob": 0.0010646533919498324}, {"id": 496, "seek": 312672, "start": 3126.72, "end": 3130.0, "text": " architecture. It's a subclass of neural net module, which is a pretty fine class.", "tokens": [50364, 9482, 13, 467, 311, 257, 1422, 11665, 295, 18161, 2533, 10088, 11, 597, 307, 257, 1238, 2489, 1508, 13, 50528], "temperature": 0.0, "avg_logprob": -0.10509272010958924, "compression_ratio": 1.7281553398058251, "no_speech_prob": 0.00015115596761461347}, {"id": 497, "seek": 312672, "start": 3131.52, "end": 3136.8799999999997, "text": " It's got a constructor here that will take the sizes of the internal layers that we want,", "tokens": [50604, 467, 311, 658, 257, 47479, 510, 300, 486, 747, 264, 11602, 295, 264, 6920, 7914, 300, 321, 528, 11, 50872], "temperature": 0.0, "avg_logprob": -0.10509272010958924, "compression_ratio": 1.7281553398058251, "no_speech_prob": 0.00015115596761461347}, {"id": 498, "seek": 312672, "start": 3136.8799999999997, "end": 3143.3599999999997, "text": " the size of the input, the size of S1 and Z1, the size of S2 and Z2, and the size of S3.", "tokens": [50872, 264, 2744, 295, 264, 4846, 11, 264, 2744, 295, 318, 16, 293, 1176, 16, 11, 264, 2744, 295, 318, 17, 293, 1176, 17, 11, 293, 264, 2744, 295, 318, 18, 13, 51196], "temperature": 0.0, "avg_logprob": -0.10509272010958924, "compression_ratio": 1.7281553398058251, "no_speech_prob": 0.00015115596761461347}, {"id": 499, "seek": 312672, "start": 3145.04, "end": 3152.72, "text": " We call the parent class initializer. And then we just create three modules that are all linear", "tokens": [51280, 492, 818, 264, 2596, 1508, 5883, 6545, 13, 400, 550, 321, 445, 1884, 1045, 16679, 300, 366, 439, 8213, 51664], "temperature": 0.0, "avg_logprob": -0.10509272010958924, "compression_ratio": 1.7281553398058251, "no_speech_prob": 0.00015115596761461347}, {"id": 500, "seek": 315272, "start": 3153.68, "end": 3157.04, "text": " modules. And we need to kind of store them somewhere because they have internal parameters.", "tokens": [50412, 16679, 13, 400, 321, 643, 281, 733, 295, 3531, 552, 4079, 570, 436, 362, 6920, 9834, 13, 50580], "temperature": 0.0, "avg_logprob": -0.17133032191883435, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0023913797922432423}, {"id": 501, "seek": 315272, "start": 3157.04, "end": 3163.12, "text": " So we're going to have three slots in our object, N0, N1, N2, module 1, module 0, module 1, module 2.", "tokens": [50580, 407, 321, 434, 516, 281, 362, 1045, 24266, 294, 527, 2657, 11, 426, 15, 11, 426, 16, 11, 426, 17, 11, 10088, 502, 11, 10088, 1958, 11, 10088, 502, 11, 10088, 568, 13, 50884], "temperature": 0.0, "avg_logprob": -0.17133032191883435, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0023913797922432423}, {"id": 502, "seek": 315272, "start": 3164.0, "end": 3169.4399999999996, "text": " And each of them is going to be an instance of the class NN.linear with two sizes, the input size", "tokens": [50928, 400, 1184, 295, 552, 307, 516, 281, 312, 364, 5197, 295, 264, 1508, 426, 45, 13, 28263, 365, 732, 11602, 11, 264, 4846, 2744, 51200], "temperature": 0.0, "avg_logprob": -0.17133032191883435, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0023913797922432423}, {"id": 503, "seek": 315272, "start": 3169.4399999999996, "end": 3175.4399999999996, "text": " and the output size. Okay, so the first module has input size D0, output size D1, etc. And those", "tokens": [51200, 293, 264, 5598, 2744, 13, 1033, 11, 370, 264, 700, 10088, 575, 4846, 2744, 413, 15, 11, 5598, 2744, 413, 16, 11, 5183, 13, 400, 729, 51500], "temperature": 0.0, "avg_logprob": -0.17133032191883435, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0023913797922432423}, {"id": 504, "seek": 315272, "start": 3175.4399999999996, "end": 3180.8799999999997, "text": " classes are, since there is a capital L, means it's an object and inside there are parameters", "tokens": [51500, 5359, 366, 11, 1670, 456, 307, 257, 4238, 441, 11, 1355, 309, 311, 364, 2657, 293, 1854, 456, 366, 9834, 51772], "temperature": 0.0, "avg_logprob": -0.17133032191883435, "compression_ratio": 1.7851851851851852, "no_speech_prob": 0.0023913797922432423}, {"id": 505, "seek": 318088, "start": 3180.88, "end": 3188.0, "text": " inside that item there. Right. So for example, the value doesn't have a capital because it doesn't", "tokens": [50364, 1854, 300, 3174, 456, 13, 1779, 13, 407, 337, 1365, 11, 264, 2158, 1177, 380, 362, 257, 4238, 570, 309, 1177, 380, 50720], "temperature": 0.0, "avg_logprob": -0.11223914597060654, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0002377533382968977}, {"id": 506, "seek": 318088, "start": 3188.0, "end": 3192.96, "text": " have internal parameters. It's not kind of a trainable module. It's just a function. Whereas", "tokens": [50720, 362, 6920, 9834, 13, 467, 311, 406, 733, 295, 257, 3847, 712, 10088, 13, 467, 311, 445, 257, 2445, 13, 13813, 50968], "temperature": 0.0, "avg_logprob": -0.11223914597060654, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0002377533382968977}, {"id": 507, "seek": 318088, "start": 3192.96, "end": 3197.12, "text": " those things with capitals, they have sort of internal parameters, the weight matrices inside", "tokens": [50968, 729, 721, 365, 1410, 11118, 11, 436, 362, 1333, 295, 6920, 9834, 11, 264, 3364, 32284, 1854, 51176], "temperature": 0.0, "avg_logprob": -0.11223914597060654, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0002377533382968977}, {"id": 508, "seek": 318088, "start": 3197.12, "end": 3203.76, "text": " of them. So now we define a forward function, which basically computes the output from the input.", "tokens": [51176, 295, 552, 13, 407, 586, 321, 6964, 257, 2128, 2445, 11, 597, 1936, 715, 1819, 264, 5598, 490, 264, 4846, 13, 51508], "temperature": 0.0, "avg_logprob": -0.11223914597060654, "compression_ratio": 1.6508620689655173, "no_speech_prob": 0.0002377533382968977}, {"id": 509, "seek": 320376, "start": 3204.48, "end": 3210.8, "text": " And the first thing we do is we take the input thing, which may be a multidimensional array,", "tokens": [50400, 400, 264, 700, 551, 321, 360, 307, 321, 747, 264, 4846, 551, 11, 597, 815, 312, 257, 2120, 327, 332, 11075, 10225, 11, 50716], "temperature": 0.0, "avg_logprob": -0.10715339660644531, "compression_ratio": 1.5310734463276836, "no_speech_prob": 0.0010482233483344316}, {"id": 510, "seek": 320376, "start": 3210.8, "end": 3218.48, "text": " and we flatten it. We flatten it using this idiomatic expression here in PyTorch.", "tokens": [50716, 293, 321, 24183, 309, 13, 492, 24183, 309, 1228, 341, 18014, 13143, 6114, 510, 294, 9953, 51, 284, 339, 13, 51100], "temperature": 0.0, "avg_logprob": -0.10715339660644531, "compression_ratio": 1.5310734463276836, "no_speech_prob": 0.0010482233483344316}, {"id": 511, "seek": 320376, "start": 3219.84, "end": 3228.32, "text": " And then we apply the first module to X. We put the result in S1, which is a temporary variable,", "tokens": [51168, 400, 550, 321, 3079, 264, 700, 10088, 281, 1783, 13, 492, 829, 264, 1874, 294, 318, 16, 11, 597, 307, 257, 13413, 7006, 11, 51592], "temperature": 0.0, "avg_logprob": -0.10715339660644531, "compression_ratio": 1.5310734463276836, "no_speech_prob": 0.0010482233483344316}, {"id": 512, "seek": 322832, "start": 3228.96, "end": 3236.0, "text": " then we apply the value to S1, put the result in Z, then apply the second layer,", "tokens": [50396, 550, 321, 3079, 264, 2158, 281, 318, 16, 11, 829, 264, 1874, 294, 1176, 11, 550, 3079, 264, 1150, 4583, 11, 50748], "temperature": 0.0, "avg_logprob": -0.11417404609390452, "compression_ratio": 1.920863309352518, "no_speech_prob": 0.0044774701818823814}, {"id": 513, "seek": 322832, "start": 3236.0, "end": 3241.84, "text": " put the result in S2, apply the value again, put the result in S3, and then the last linear", "tokens": [50748, 829, 264, 1874, 294, 318, 17, 11, 3079, 264, 2158, 797, 11, 829, 264, 1874, 294, 318, 18, 11, 293, 550, 264, 1036, 8213, 51040], "temperature": 0.0, "avg_logprob": -0.11417404609390452, "compression_ratio": 1.920863309352518, "no_speech_prob": 0.0044774701818823814}, {"id": 514, "seek": 322832, "start": 3241.84, "end": 3246.96, "text": " layer, put the result in S3 and return S3. And there is a typo. So the second line should have", "tokens": [51040, 4583, 11, 829, 264, 1874, 294, 318, 18, 293, 2736, 318, 18, 13, 400, 456, 307, 257, 2125, 78, 13, 407, 264, 1150, 1622, 820, 362, 51296], "temperature": 0.0, "avg_logprob": -0.11417404609390452, "compression_ratio": 1.920863309352518, "no_speech_prob": 0.0044774701818823814}, {"id": 515, "seek": 324696, "start": 3246.96, "end": 3260.32, "text": " been S1, it's the self.m0 of Z0, right? Z0 here, yes, correct. Yeah, this is something that", "tokens": [50364, 668, 318, 16, 11, 309, 311, 264, 2698, 13, 76, 15, 295, 1176, 15, 11, 558, 30, 1176, 15, 510, 11, 2086, 11, 3006, 13, 865, 11, 341, 307, 746, 300, 51032], "temperature": 0.0, "avg_logprob": -0.21802361806233725, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.04437869414687157}, {"id": 516, "seek": 324696, "start": 3261.04, "end": 3267.52, "text": " is going to be fixed, right? Which I didn't fix. I know. This is Z0. Thanks for reminding me of this.", "tokens": [51068, 307, 516, 281, 312, 6806, 11, 558, 30, 3013, 286, 994, 380, 3191, 13, 286, 458, 13, 639, 307, 1176, 15, 13, 2561, 337, 27639, 385, 295, 341, 13, 51392], "temperature": 0.0, "avg_logprob": -0.21802361806233725, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.04437869414687157}, {"id": 517, "seek": 324696, "start": 3270.48, "end": 3274.96, "text": " Okay, but you'll see examples. I mean, I'll show you kind of actual examples of this,", "tokens": [51540, 1033, 11, 457, 291, 603, 536, 5110, 13, 286, 914, 11, 286, 603, 855, 291, 733, 295, 3539, 5110, 295, 341, 11, 51764], "temperature": 0.0, "avg_logprob": -0.21802361806233725, "compression_ratio": 1.4455958549222798, "no_speech_prob": 0.04437869414687157}, {"id": 518, "seek": 327496, "start": 3274.96, "end": 3279.04, "text": " and you'll be able to run them yourself. That's all you need to do. You don't need to write", "tokens": [50364, 293, 291, 603, 312, 1075, 281, 1190, 552, 1803, 13, 663, 311, 439, 291, 643, 281, 360, 13, 509, 500, 380, 643, 281, 2464, 50568], "temperature": 0.0, "avg_logprob": -0.1045502507409384, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0020499154925346375}, {"id": 519, "seek": 327496, "start": 3281.44, "end": 3286.2400000000002, "text": " how you compute the back prop, how you propagate the gradients. You could write it,", "tokens": [50688, 577, 291, 14722, 264, 646, 2365, 11, 577, 291, 48256, 264, 2771, 2448, 13, 509, 727, 2464, 309, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1045502507409384, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0020499154925346375}, {"id": 520, "seek": 327496, "start": 3286.2400000000002, "end": 3289.6, "text": " and it would be as simple as forward. You could write a backward function, and it would basically", "tokens": [50928, 293, 309, 576, 312, 382, 2199, 382, 2128, 13, 509, 727, 2464, 257, 23897, 2445, 11, 293, 309, 576, 1936, 51096], "temperature": 0.0, "avg_logprob": -0.1045502507409384, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0020499154925346375}, {"id": 521, "seek": 327496, "start": 3290.64, "end": 3294.16, "text": " multiply by the matrices going backwards. But you don't need to do this because", "tokens": [51148, 12972, 538, 264, 32284, 516, 12204, 13, 583, 291, 500, 380, 643, 281, 360, 341, 570, 51324], "temperature": 0.0, "avg_logprob": -0.1045502507409384, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0020499154925346375}, {"id": 522, "seek": 327496, "start": 3294.16, "end": 3298.7200000000003, "text": " PyTorch does this automatically for you. When you define the forward function, it knows what", "tokens": [51324, 9953, 51, 284, 339, 775, 341, 6772, 337, 291, 13, 1133, 291, 6964, 264, 2128, 2445, 11, 309, 3255, 437, 51552], "temperature": 0.0, "avg_logprob": -0.1045502507409384, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0020499154925346375}, {"id": 523, "seek": 327496, "start": 3299.52, "end": 3302.96, "text": " modules you've called in what order, what are the dependencies between the variables,", "tokens": [51592, 16679, 291, 600, 1219, 294, 437, 1668, 11, 437, 366, 264, 36606, 1296, 264, 9102, 11, 51764], "temperature": 0.0, "avg_logprob": -0.1045502507409384, "compression_ratio": 1.8601398601398602, "no_speech_prob": 0.0020499154925346375}, {"id": 524, "seek": 330296, "start": 3302.96, "end": 3307.28, "text": " and it will know how to generate the functions that compute the gradient", "tokens": [50364, 293, 309, 486, 458, 577, 281, 8460, 264, 6828, 300, 14722, 264, 16235, 50580], "temperature": 0.0, "avg_logprob": -0.10283797454833984, "compression_ratio": 1.8220640569395017, "no_speech_prob": 0.000200256981770508}, {"id": 525, "seek": 330296, "start": 3307.28, "end": 3312.4, "text": " backwards. So you don't need to worry about it. That's the magic of PyTorch, if you want.", "tokens": [50580, 12204, 13, 407, 291, 500, 380, 643, 281, 3292, 466, 309, 13, 663, 311, 264, 5585, 295, 9953, 51, 284, 339, 11, 498, 291, 528, 13, 50836], "temperature": 0.0, "avg_logprob": -0.10283797454833984, "compression_ratio": 1.8220640569395017, "no_speech_prob": 0.000200256981770508}, {"id": 526, "seek": 330296, "start": 3312.4, "end": 3316.7200000000003, "text": " That's a bit the magic of deep learning, really. That's called automatic differentiation,", "tokens": [50836, 663, 311, 257, 857, 264, 5585, 295, 2452, 2539, 11, 534, 13, 663, 311, 1219, 12509, 38902, 11, 51052], "temperature": 0.0, "avg_logprob": -0.10283797454833984, "compression_ratio": 1.8220640569395017, "no_speech_prob": 0.000200256981770508}, {"id": 527, "seek": 330296, "start": 3318.7200000000003, "end": 3323.12, "text": " and this is a particular form of automatic differentiation. There's another way to write", "tokens": [51152, 293, 341, 307, 257, 1729, 1254, 295, 12509, 38902, 13, 821, 311, 1071, 636, 281, 2464, 51372], "temperature": 0.0, "avg_logprob": -0.10283797454833984, "compression_ratio": 1.8220640569395017, "no_speech_prob": 0.000200256981770508}, {"id": 528, "seek": 330296, "start": 3323.68, "end": 3328.0, "text": " functions in PyTorch that are kind of more functional. So you're not using modules", "tokens": [51400, 6828, 294, 9953, 51, 284, 339, 300, 366, 733, 295, 544, 11745, 13, 407, 291, 434, 406, 1228, 16679, 51616], "temperature": 0.0, "avg_logprob": -0.10283797454833984, "compression_ratio": 1.8220640569395017, "no_speech_prob": 0.000200256981770508}, {"id": 529, "seek": 330296, "start": 3328.0, "end": 3331.84, "text": " with internal parameters. You're just coding functions one after the other. And PyTorch", "tokens": [51616, 365, 6920, 9834, 13, 509, 434, 445, 17720, 6828, 472, 934, 264, 661, 13, 400, 9953, 51, 284, 339, 51808], "temperature": 0.0, "avg_logprob": -0.10283797454833984, "compression_ratio": 1.8220640569395017, "no_speech_prob": 0.000200256981770508}, {"id": 530, "seek": 333184, "start": 3331.84, "end": 3336.8, "text": " has a mechanism by which it can automatically compute the gradient of any function you define", "tokens": [50364, 575, 257, 7513, 538, 597, 309, 393, 6772, 14722, 264, 16235, 295, 604, 2445, 291, 6964, 50612], "temperature": 0.0, "avg_logprob": -0.13939612252371653, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0006527199875563383}, {"id": 531, "seek": 333184, "start": 3336.8, "end": 3342.2400000000002, "text": " with respect to whatever parameters you want. Yeah, actually, these big guys with the capital L,", "tokens": [50612, 365, 3104, 281, 2035, 9834, 291, 528, 13, 865, 11, 767, 11, 613, 955, 1074, 365, 264, 4238, 441, 11, 50884], "temperature": 0.0, "avg_logprob": -0.13939612252371653, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0006527199875563383}, {"id": 532, "seek": 333184, "start": 3342.2400000000002, "end": 3348.7200000000003, "text": " like the nn.capital linear inside is going to have a lowercase linear, which is like the functional", "tokens": [50884, 411, 264, 297, 77, 13, 9485, 1686, 8213, 1854, 307, 516, 281, 362, 257, 3126, 9765, 8213, 11, 597, 307, 411, 264, 11745, 51208], "temperature": 0.0, "avg_logprob": -0.13939612252371653, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0006527199875563383}, {"id": 533, "seek": 333184, "start": 3348.7200000000003, "end": 3355.04, "text": " part, which is performing the matrix multiplication between the weights stored inside the object", "tokens": [51208, 644, 11, 597, 307, 10205, 264, 8141, 27290, 1296, 264, 17443, 12187, 1854, 264, 2657, 51524], "temperature": 0.0, "avg_logprob": -0.13939612252371653, "compression_ratio": 1.6753246753246753, "no_speech_prob": 0.0006527199875563383}, {"id": 534, "seek": 335504, "start": 3355.04, "end": 3361.92, "text": " with the capital L and then the input. So every capital letter object will inside have", "tokens": [50364, 365, 264, 4238, 441, 293, 550, 264, 4846, 13, 407, 633, 4238, 5063, 2657, 486, 1854, 362, 50708], "temperature": 0.0, "avg_logprob": -0.1479517695415451, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001074605155736208}, {"id": 535, "seek": 335504, "start": 3361.92, "end": 3367.68, "text": " the functional way. So one can decide to use either the functional form by default,", "tokens": [50708, 264, 11745, 636, 13, 407, 472, 393, 4536, 281, 764, 2139, 264, 11745, 1254, 538, 7576, 11, 50996], "temperature": 0.0, "avg_logprob": -0.1479517695415451, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001074605155736208}, {"id": 536, "seek": 335504, "start": 3367.68, "end": 3373.84, "text": " or use this encapsulated version, which are more convenient to just use, right?", "tokens": [50996, 420, 764, 341, 38745, 6987, 3037, 11, 597, 366, 544, 10851, 281, 445, 764, 11, 558, 30, 51304], "temperature": 0.0, "avg_logprob": -0.1479517695415451, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001074605155736208}, {"id": 537, "seek": 335504, "start": 3374.4, "end": 3379.92, "text": " Right. So at the end, you can create an instance of this class. You can create multiple instances,", "tokens": [51332, 1779, 13, 407, 412, 264, 917, 11, 291, 393, 1884, 364, 5197, 295, 341, 1508, 13, 509, 393, 1884, 3866, 14519, 11, 51608], "temperature": 0.0, "avg_logprob": -0.1479517695415451, "compression_ratio": 1.6462264150943395, "no_speech_prob": 0.001074605155736208}, {"id": 538, "seek": 337992, "start": 3379.92, "end": 3384.08, "text": " but you can create one here, just call my net and give it the sizes you want.", "tokens": [50364, 457, 291, 393, 1884, 472, 510, 11, 445, 818, 452, 2533, 293, 976, 309, 264, 11602, 291, 528, 13, 50572], "temperature": 0.0, "avg_logprob": -0.13509693924261598, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.0008968253387138247}, {"id": 539, "seek": 337992, "start": 3385.36, "end": 3391.36, "text": " And then to apply this to a particular image, you just do how to equal model of image. That's", "tokens": [50636, 400, 550, 281, 3079, 341, 281, 257, 1729, 3256, 11, 291, 445, 360, 577, 281, 2681, 2316, 295, 3256, 13, 663, 311, 50936], "temperature": 0.0, "avg_logprob": -0.13509693924261598, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.0008968253387138247}, {"id": 540, "seek": 337992, "start": 3391.36, "end": 3398.32, "text": " as simple as that. Okay, so this is your first neural net, and it does all the backup automatically.", "tokens": [50936, 382, 2199, 382, 300, 13, 1033, 11, 370, 341, 307, 428, 700, 18161, 2533, 11, 293, 309, 775, 439, 264, 14807, 6772, 13, 51284], "temperature": 0.0, "avg_logprob": -0.13509693924261598, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.0008968253387138247}, {"id": 541, "seek": 337992, "start": 3399.52, "end": 3404.8, "text": " But you need to understand how that works, right? It's not because PyTorch does it for you,", "tokens": [51344, 583, 291, 643, 281, 1223, 577, 300, 1985, 11, 558, 30, 467, 311, 406, 570, 9953, 51, 284, 339, 775, 309, 337, 291, 11, 51608], "temperature": 0.0, "avg_logprob": -0.13509693924261598, "compression_ratio": 1.548936170212766, "no_speech_prob": 0.0008968253387138247}, {"id": 542, "seek": 340480, "start": 3405.36, "end": 3411.44, "text": " that you can sort of forget about how you actually compute the gradient of a function,", "tokens": [50392, 300, 291, 393, 1333, 295, 2870, 466, 577, 291, 767, 14722, 264, 16235, 295, 257, 2445, 11, 50696], "temperature": 0.0, "avg_logprob": -0.10003800825639204, "compression_ratio": 1.78, "no_speech_prob": 0.0004372609546408057}, {"id": 543, "seek": 340480, "start": 3411.44, "end": 3414.32, "text": " because it's inevitable that at some point, you're going to want to", "tokens": [50696, 570, 309, 311, 21451, 300, 412, 512, 935, 11, 291, 434, 516, 281, 528, 281, 50840], "temperature": 0.0, "avg_logprob": -0.10003800825639204, "compression_ratio": 1.78, "no_speech_prob": 0.0004372609546408057}, {"id": 544, "seek": 340480, "start": 3414.96, "end": 3418.2400000000002, "text": " actually assemble a neural net with a module that does not pre-exist, and you're going to have to", "tokens": [50872, 767, 22364, 257, 18161, 2533, 365, 257, 10088, 300, 775, 406, 659, 12, 18217, 11, 293, 291, 434, 516, 281, 362, 281, 51036], "temperature": 0.0, "avg_logprob": -0.10003800825639204, "compression_ratio": 1.78, "no_speech_prob": 0.0004372609546408057}, {"id": 545, "seek": 340480, "start": 3418.2400000000002, "end": 3424.5600000000004, "text": " write your own backup function. So to do this, you basically have, if you want to create a new module", "tokens": [51036, 2464, 428, 1065, 14807, 2445, 13, 407, 281, 360, 341, 11, 291, 1936, 362, 11, 498, 291, 528, 281, 1884, 257, 777, 10088, 51352], "temperature": 0.0, "avg_logprob": -0.10003800825639204, "compression_ratio": 1.78, "no_speech_prob": 0.0004372609546408057}, {"id": 546, "seek": 340480, "start": 3424.5600000000004, "end": 3431.84, "text": " with some complex operation that does not pre-exist in PyTorch, then you do something like", "tokens": [51352, 365, 512, 3997, 6916, 300, 775, 406, 659, 12, 18217, 294, 9953, 51, 284, 339, 11, 550, 291, 360, 746, 411, 51716], "temperature": 0.0, "avg_logprob": -0.10003800825639204, "compression_ratio": 1.78, "no_speech_prob": 0.0004372609546408057}, {"id": 547, "seek": 343184, "start": 3431.84, "end": 3436.88, "text": " this. You define your class, but you write your own backward function, basically.", "tokens": [50364, 341, 13, 509, 6964, 428, 1508, 11, 457, 291, 2464, 428, 1065, 23897, 2445, 11, 1936, 13, 50616], "temperature": 0.0, "avg_logprob": -0.15236135090098663, "compression_ratio": 1.52, "no_speech_prob": 0.00041073374450206757}, {"id": 548, "seek": 343184, "start": 3438.96, "end": 3448.56, "text": " Okay, so let's get one step up in terms of abstraction, and write this in sort of slightly more", "tokens": [50720, 1033, 11, 370, 718, 311, 483, 472, 1823, 493, 294, 2115, 295, 37765, 11, 293, 2464, 341, 294, 1333, 295, 4748, 544, 51200], "temperature": 0.0, "avg_logprob": -0.15236135090098663, "compression_ratio": 1.52, "no_speech_prob": 0.00041073374450206757}, {"id": 549, "seek": 343184, "start": 3451.1200000000003, "end": 3458.48, "text": " generic form, mathematical form, if you want. So let's say we have a cost function here,", "tokens": [51328, 19577, 1254, 11, 18894, 1254, 11, 498, 291, 528, 13, 407, 718, 311, 584, 321, 362, 257, 2063, 2445, 510, 11, 51696], "temperature": 0.0, "avg_logprob": -0.15236135090098663, "compression_ratio": 1.52, "no_speech_prob": 0.00041073374450206757}, {"id": 550, "seek": 345848, "start": 3459.04, "end": 3464.56, "text": " and we want to compute the gradient of this cost function with a stack to a particular", "tokens": [50392, 293, 321, 528, 281, 14722, 264, 16235, 295, 341, 2063, 2445, 365, 257, 8630, 281, 257, 1729, 50668], "temperature": 0.0, "avg_logprob": -0.17408256530761718, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.0010159112280234694}, {"id": 551, "seek": 345848, "start": 3464.56, "end": 3468.88, "text": " vector in the system ZF. It could be a parameter, it could be a state, it doesn't matter. Okay,", "tokens": [50668, 8062, 294, 264, 1185, 1176, 37, 13, 467, 727, 312, 257, 13075, 11, 309, 727, 312, 257, 1785, 11, 309, 1177, 380, 1871, 13, 1033, 11, 50884], "temperature": 0.0, "avg_logprob": -0.17408256530761718, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.0010159112280234694}, {"id": 552, "seek": 345848, "start": 3469.68, "end": 3476.32, "text": " some states inside. And we have chain rule, and chain rule is nothing more than this,", "tokens": [50924, 512, 4368, 1854, 13, 400, 321, 362, 5021, 4978, 11, 293, 5021, 4978, 307, 1825, 544, 813, 341, 11, 51256], "temperature": 0.0, "avg_logprob": -0.17408256530761718, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.0010159112280234694}, {"id": 553, "seek": 345848, "start": 3476.32, "end": 3484.32, "text": " that I explained earlier. dC over dZF is equal to dC over dZG, dZG over dZF, as long as C", "tokens": [51256, 300, 286, 8825, 3071, 13, 274, 34, 670, 274, 57, 37, 307, 2681, 281, 274, 34, 670, 274, 57, 38, 11, 274, 57, 38, 670, 274, 57, 37, 11, 382, 938, 382, 383, 51656], "temperature": 0.0, "avg_logprob": -0.17408256530761718, "compression_ratio": 1.6053811659192825, "no_speech_prob": 0.0010159112280234694}, {"id": 554, "seek": 348432, "start": 3484.6400000000003, "end": 3493.76, "text": " is only influenced by ZF through ZG. There's no other way for ZF to influence C than to go", "tokens": [50380, 307, 787, 15269, 538, 1176, 37, 807, 1176, 38, 13, 821, 311, 572, 661, 636, 337, 1176, 37, 281, 6503, 383, 813, 281, 352, 50836], "temperature": 0.0, "avg_logprob": -0.14315436637564882, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.0017537090461701155}, {"id": 555, "seek": 348432, "start": 3493.76, "end": 3500.1600000000003, "text": " through ZG, then this formula is correct. Okay? And of course, the identity is trivial,", "tokens": [50836, 807, 1176, 38, 11, 550, 341, 8513, 307, 3006, 13, 1033, 30, 400, 295, 1164, 11, 264, 6575, 307, 26703, 11, 51156], "temperature": 0.0, "avg_logprob": -0.14315436637564882, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.0017537090461701155}, {"id": 556, "seek": 348432, "start": 3500.1600000000003, "end": 3508.6400000000003, "text": " because it's just a simplification by this infinitesimal vector quantity dZG. Okay?", "tokens": [51156, 570, 309, 311, 445, 257, 6883, 3774, 538, 341, 7193, 3324, 10650, 8062, 11275, 274, 57, 38, 13, 1033, 30, 51580], "temperature": 0.0, "avg_logprob": -0.14315436637564882, "compression_ratio": 1.4555555555555555, "no_speech_prob": 0.0017537090461701155}, {"id": 557, "seek": 350864, "start": 3509.6, "end": 3516.08, "text": " So let's say ZG is a vector of size dG by one, so this means column vector. Okay?", "tokens": [50412, 407, 718, 311, 584, 1176, 38, 307, 257, 8062, 295, 2744, 274, 38, 538, 472, 11, 370, 341, 1355, 7738, 8062, 13, 1033, 30, 50736], "temperature": 0.0, "avg_logprob": -0.11697188290682706, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.000404344784328714}, {"id": 558, "seek": 350864, "start": 3516.7999999999997, "end": 3520.56, "text": " And ZF is a column vector of size dF.", "tokens": [50772, 400, 1176, 37, 307, 257, 7738, 8062, 295, 2744, 274, 37, 13, 50960], "temperature": 0.0, "avg_logprob": -0.11697188290682706, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.000404344784328714}, {"id": 559, "seek": 350864, "start": 3525.6, "end": 3529.2799999999997, "text": " This is, if you want to write the correct dimensions of this,", "tokens": [51212, 639, 307, 11, 498, 291, 528, 281, 2464, 264, 3006, 12819, 295, 341, 11, 51396], "temperature": 0.0, "avg_logprob": -0.11697188290682706, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.000404344784328714}, {"id": 560, "seek": 350864, "start": 3531.2, "end": 3535.68, "text": " you know, we get something a little complicated. Okay, so first of all,", "tokens": [51492, 291, 458, 11, 321, 483, 746, 257, 707, 6179, 13, 1033, 11, 370, 700, 295, 439, 11, 51716], "temperature": 0.0, "avg_logprob": -0.11697188290682706, "compression_ratio": 1.4795321637426901, "no_speech_prob": 0.000404344784328714}, {"id": 561, "seek": 353568, "start": 3536.64, "end": 3543.44, "text": " this object here, dZG over dZF, well, let me start with this one. Okay,", "tokens": [50412, 341, 2657, 510, 11, 274, 57, 38, 670, 274, 57, 37, 11, 731, 11, 718, 385, 722, 365, 341, 472, 13, 1033, 11, 50752], "temperature": 0.0, "avg_logprob": -0.1295562459711443, "compression_ratio": 1.7839195979899498, "no_speech_prob": 0.00021315283083822578}, {"id": 562, "seek": 353568, "start": 3543.44, "end": 3548.7999999999997, "text": " this one dC over dZG, that's a gradient vector. Okay? ZG is a vector, dC over dZG is a gradient", "tokens": [50752, 341, 472, 274, 34, 670, 274, 57, 38, 11, 300, 311, 257, 16235, 8062, 13, 1033, 30, 1176, 38, 307, 257, 8062, 11, 274, 34, 670, 274, 57, 38, 307, 257, 16235, 51020], "temperature": 0.0, "avg_logprob": -0.1295562459711443, "compression_ratio": 1.7839195979899498, "no_speech_prob": 0.00021315283083822578}, {"id": 563, "seek": 353568, "start": 3548.7999999999997, "end": 3556.3199999999997, "text": " vector. And it's the same size as dZG. But by convention, we actually write it as a line,", "tokens": [51020, 8062, 13, 400, 309, 311, 264, 912, 2744, 382, 274, 57, 38, 13, 583, 538, 10286, 11, 321, 767, 2464, 309, 382, 257, 1622, 11, 51396], "temperature": 0.0, "avg_logprob": -0.1295562459711443, "compression_ratio": 1.7839195979899498, "no_speech_prob": 0.00021315283083822578}, {"id": 564, "seek": 353568, "start": 3556.3199999999997, "end": 3563.44, "text": " as a row vector. Okay? So this thing here is going to be a row vector whose size is the same size", "tokens": [51396, 382, 257, 5386, 8062, 13, 1033, 30, 407, 341, 551, 510, 307, 516, 281, 312, 257, 5386, 8062, 6104, 2744, 307, 264, 912, 2744, 51752], "temperature": 0.0, "avg_logprob": -0.1295562459711443, "compression_ratio": 1.7839195979899498, "no_speech_prob": 0.00021315283083822578}, {"id": 565, "seek": 356344, "start": 3563.44, "end": 3567.44, "text": " as ZG, but it's going to be horizontal instead of vertical. Okay?", "tokens": [50364, 382, 1176, 38, 11, 457, 309, 311, 516, 281, 312, 12750, 2602, 295, 9429, 13, 1033, 30, 50564], "temperature": 0.0, "avg_logprob": -0.08105395056984642, "compression_ratio": 1.5655430711610487, "no_speech_prob": 9.313478221883997e-05}, {"id": 566, "seek": 356344, "start": 3570.4, "end": 3573.6, "text": " This object here is something more complicated. It's actually a matrix.", "tokens": [50712, 639, 2657, 510, 307, 746, 544, 6179, 13, 467, 311, 767, 257, 8141, 13, 50872], "temperature": 0.0, "avg_logprob": -0.08105395056984642, "compression_ratio": 1.5655430711610487, "no_speech_prob": 9.313478221883997e-05}, {"id": 567, "seek": 356344, "start": 3574.64, "end": 3580.8, "text": " Why is it a matrix is because it's the derivative of a vector with respect to another vector.", "tokens": [50924, 1545, 307, 309, 257, 8141, 307, 570, 309, 311, 264, 13760, 295, 257, 8062, 365, 3104, 281, 1071, 8062, 13, 51232], "temperature": 0.0, "avg_logprob": -0.08105395056984642, "compression_ratio": 1.5655430711610487, "no_speech_prob": 9.313478221883997e-05}, {"id": 568, "seek": 356344, "start": 3580.8, "end": 3586.2400000000002, "text": " Okay? So let's look at this diagram here on the right. We have a function G, it takes ZF as an", "tokens": [51232, 1033, 30, 407, 718, 311, 574, 412, 341, 10686, 510, 322, 264, 558, 13, 492, 362, 257, 2445, 460, 11, 309, 2516, 1176, 37, 382, 364, 51504], "temperature": 0.0, "avg_logprob": -0.08105395056984642, "compression_ratio": 1.5655430711610487, "no_speech_prob": 9.313478221883997e-05}, {"id": 569, "seek": 356344, "start": 3586.2400000000002, "end": 3592.64, "text": " input, and it produces ZG as an output. And if we want to capture the information about the", "tokens": [51504, 4846, 11, 293, 309, 14725, 1176, 38, 382, 364, 5598, 13, 400, 498, 321, 528, 281, 7983, 264, 1589, 466, 264, 51824], "temperature": 0.0, "avg_logprob": -0.08105395056984642, "compression_ratio": 1.5655430711610487, "no_speech_prob": 9.313478221883997e-05}, {"id": 570, "seek": 359264, "start": 3592.64, "end": 3600.3199999999997, "text": " derivative of that module, which is this quantity here dZG over dZF, there's a lot of terms to", "tokens": [50364, 13760, 295, 300, 10088, 11, 597, 307, 341, 11275, 510, 274, 57, 38, 670, 274, 57, 37, 11, 456, 311, 257, 688, 295, 2115, 281, 50748], "temperature": 0.0, "avg_logprob": -0.10443943568638393, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00021315923368092626}, {"id": 571, "seek": 359264, "start": 3600.3199999999997, "end": 3606.16, "text": " capture because there's a lot of ways in which every single output, every component of ZG can be", "tokens": [50748, 7983, 570, 456, 311, 257, 688, 295, 2098, 294, 597, 633, 2167, 5598, 11, 633, 6542, 295, 1176, 38, 393, 312, 51040], "temperature": 0.0, "avg_logprob": -0.10443943568638393, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00021315923368092626}, {"id": 572, "seek": 359264, "start": 3606.16, "end": 3611.68, "text": " influenced by every component of ZF. Right? So if for every pair of components, ZG and ZF,", "tokens": [51040, 15269, 538, 633, 6542, 295, 1176, 37, 13, 1779, 30, 407, 498, 337, 633, 6119, 295, 6677, 11, 1176, 38, 293, 1176, 37, 11, 51316], "temperature": 0.0, "avg_logprob": -0.10443943568638393, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00021315923368092626}, {"id": 573, "seek": 359264, "start": 3612.24, "end": 3619.04, "text": " there is a derivative term, which indicates by how much ZG would be perturbed if I perturbed ZF", "tokens": [51344, 456, 307, 257, 13760, 1433, 11, 597, 16203, 538, 577, 709, 1176, 38, 576, 312, 13269, 374, 2883, 498, 286, 13269, 374, 2883, 1176, 37, 51684], "temperature": 0.0, "avg_logprob": -0.10443943568638393, "compression_ratio": 1.7181818181818183, "no_speech_prob": 0.00021315923368092626}, {"id": 574, "seek": 361904, "start": 3619.04, "end": 3627.92, "text": " by a small infinitesimal quantity. Right? We have that for every pair of components of ZG and ZF.", "tokens": [50364, 538, 257, 1359, 7193, 3324, 10650, 11275, 13, 1779, 30, 492, 362, 300, 337, 633, 6119, 295, 6677, 295, 1176, 38, 293, 1176, 37, 13, 50808], "temperature": 0.0, "avg_logprob": -0.05579713674692007, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.0004953455645591021}, {"id": 575, "seek": 361904, "start": 3627.92, "end": 3635.7599999999998, "text": " As a result, this is a matrix whose dimension is the number of rows is the size of ZG and the", "tokens": [50808, 1018, 257, 1874, 11, 341, 307, 257, 8141, 6104, 10139, 307, 264, 1230, 295, 13241, 307, 264, 2744, 295, 1176, 38, 293, 264, 51200], "temperature": 0.0, "avg_logprob": -0.05579713674692007, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.0004953455645591021}, {"id": 576, "seek": 361904, "start": 3635.7599999999998, "end": 3646.16, "text": " number of columns is the size of ZF. And each term in this matrix is one partial derivative term.", "tokens": [51200, 1230, 295, 13766, 307, 264, 2744, 295, 1176, 37, 13, 400, 1184, 1433, 294, 341, 8141, 307, 472, 14641, 13760, 1433, 13, 51720], "temperature": 0.0, "avg_logprob": -0.05579713674692007, "compression_ratio": 1.5706521739130435, "no_speech_prob": 0.0004953455645591021}, {"id": 577, "seek": 364616, "start": 3646.16, "end": 3653.7599999999998, "text": " So this whole matrix here, if I take the component ij, it's the partial derivative of the i-th", "tokens": [50364, 407, 341, 1379, 8141, 510, 11, 498, 286, 747, 264, 6542, 741, 73, 11, 309, 311, 264, 14641, 13760, 295, 264, 741, 12, 392, 50744], "temperature": 0.0, "avg_logprob": -0.11783773899078369, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00041072844760492444}, {"id": 578, "seek": 364616, "start": 3653.7599999999998, "end": 3662.56, "text": " output of that module, the i-th component of ZG, with respect to the j-th component of ZF.", "tokens": [50744, 5598, 295, 300, 10088, 11, 264, 741, 12, 392, 6542, 295, 1176, 38, 11, 365, 3104, 281, 264, 361, 12, 392, 6542, 295, 1176, 37, 13, 51184], "temperature": 0.0, "avg_logprob": -0.11783773899078369, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00041072844760492444}, {"id": 579, "seek": 364616, "start": 3664.0, "end": 3672.0, "text": " Okay? So what we get here is a row vector is equal to a row vector multiplied by a matrix,", "tokens": [51256, 1033, 30, 407, 437, 321, 483, 510, 307, 257, 5386, 8062, 307, 2681, 281, 257, 5386, 8062, 17207, 538, 257, 8141, 11, 51656], "temperature": 0.0, "avg_logprob": -0.11783773899078369, "compression_ratio": 1.5862068965517242, "no_speech_prob": 0.00041072844760492444}, {"id": 580, "seek": 367200, "start": 3672.0, "end": 3678.72, "text": " and the sizes kind of work out so that they're compatible with each other.", "tokens": [50364, 293, 264, 11602, 733, 295, 589, 484, 370, 300, 436, 434, 18218, 365, 1184, 661, 13, 50700], "temperature": 0.0, "avg_logprob": -0.12006786595220151, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.00029591217753477395}, {"id": 581, "seek": 367200, "start": 3679.92, "end": 3683.28, "text": " Okay. So what is back propagation now? Back propagation is this formula.", "tokens": [50760, 1033, 13, 407, 437, 307, 646, 38377, 586, 30, 5833, 38377, 307, 341, 8513, 13, 50928], "temperature": 0.0, "avg_logprob": -0.12006786595220151, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.00029591217753477395}, {"id": 582, "seek": 367200, "start": 3684.72, "end": 3689.2, "text": " Okay? It says if you have the gradient of some cost function with respect to some variable,", "tokens": [51000, 1033, 30, 467, 1619, 498, 291, 362, 264, 16235, 295, 512, 2063, 2445, 365, 3104, 281, 512, 7006, 11, 51224], "temperature": 0.0, "avg_logprob": -0.12006786595220151, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.00029591217753477395}, {"id": 583, "seek": 367200, "start": 3689.2, "end": 3692.08, "text": " and you know the dependency of these variables with respect to another variable,", "tokens": [51224, 293, 291, 458, 264, 33621, 295, 613, 9102, 365, 3104, 281, 1071, 7006, 11, 51368], "temperature": 0.0, "avg_logprob": -0.12006786595220151, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.00029591217753477395}, {"id": 584, "seek": 367200, "start": 3692.08, "end": 3696.96, "text": " you multiply this gradient vector by that Jacobian matrix, and you get the gradient", "tokens": [51368, 291, 12972, 341, 16235, 8062, 538, 300, 14117, 952, 8141, 11, 293, 291, 483, 264, 16235, 51612], "temperature": 0.0, "avg_logprob": -0.12006786595220151, "compression_ratio": 1.7413793103448276, "no_speech_prob": 0.00029591217753477395}, {"id": 585, "seek": 369696, "start": 3696.96, "end": 3703.12, "text": " vector with respect to that second variable. So graphically here on the right, if I have", "tokens": [50364, 8062, 365, 3104, 281, 300, 1150, 7006, 13, 407, 4295, 984, 510, 322, 264, 558, 11, 498, 286, 362, 50672], "temperature": 0.0, "avg_logprob": -0.09647278391986812, "compression_ratio": 1.8697916666666667, "no_speech_prob": 0.0008966806344687939}, {"id": 586, "seek": 369696, "start": 3703.92, "end": 3709.52, "text": " the gradient of the cost with respect to ZG, which is DC over DZG, and I want to compute", "tokens": [50712, 264, 16235, 295, 264, 2063, 365, 3104, 281, 1176, 38, 11, 597, 307, 9114, 670, 413, 57, 38, 11, 293, 286, 528, 281, 14722, 50992], "temperature": 0.0, "avg_logprob": -0.09647278391986812, "compression_ratio": 1.8697916666666667, "no_speech_prob": 0.0008966806344687939}, {"id": 587, "seek": 369696, "start": 3709.52, "end": 3716.08, "text": " the gradient of C with respect to ZF, which is DC over DZF, I only need to take that vector,", "tokens": [50992, 264, 16235, 295, 383, 365, 3104, 281, 1176, 37, 11, 597, 307, 9114, 670, 413, 57, 37, 11, 286, 787, 643, 281, 747, 300, 8062, 11, 51320], "temperature": 0.0, "avg_logprob": -0.09647278391986812, "compression_ratio": 1.8697916666666667, "no_speech_prob": 0.0008966806344687939}, {"id": 588, "seek": 369696, "start": 3716.08, "end": 3723.04, "text": " which is a row vector, multiply it by the Jacobian matrix, DG over DZF, or DZG over DZF,", "tokens": [51320, 597, 307, 257, 5386, 8062, 11, 12972, 309, 538, 264, 14117, 952, 8141, 11, 413, 38, 670, 413, 57, 37, 11, 420, 413, 57, 38, 670, 413, 57, 37, 11, 51668], "temperature": 0.0, "avg_logprob": -0.09647278391986812, "compression_ratio": 1.8697916666666667, "no_speech_prob": 0.0008966806344687939}, {"id": 589, "seek": 372304, "start": 3723.92, "end": 3731.2799999999997, "text": " and I get DC over DZF. Okay? It's this formula. Someone is objecting here. Isn't the summation", "tokens": [50408, 293, 286, 483, 9114, 670, 413, 57, 37, 13, 1033, 30, 467, 311, 341, 8513, 13, 8734, 307, 2657, 278, 510, 13, 6998, 380, 264, 28811, 50776], "temperature": 0.0, "avg_logprob": -0.14598256208765226, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00128254946321249}, {"id": 590, "seek": 372304, "start": 3731.2799999999997, "end": 3739.84, "text": " missing here? Which summation? Summation of all the components of these partial multiplications.", "tokens": [50776, 5361, 510, 30, 3013, 28811, 30, 8626, 76, 399, 295, 439, 264, 6677, 295, 613, 14641, 17596, 763, 13, 51204], "temperature": 0.0, "avg_logprob": -0.14598256208765226, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00128254946321249}, {"id": 591, "seek": 372304, "start": 3739.84, "end": 3744.48, "text": " Here? Yeah. Well, this is a vector. This is a vector. This is a matrix. There is a lot of", "tokens": [51204, 1692, 30, 865, 13, 1042, 11, 341, 307, 257, 8062, 13, 639, 307, 257, 8062, 13, 639, 307, 257, 8141, 13, 821, 307, 257, 688, 295, 51436], "temperature": 0.0, "avg_logprob": -0.14598256208765226, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00128254946321249}, {"id": 592, "seek": 372304, "start": 3744.48, "end": 3747.7599999999998, "text": " sums going on here because when you compute the product of this vector with its matrix,", "tokens": [51436, 34499, 516, 322, 510, 570, 562, 291, 14722, 264, 1674, 295, 341, 8062, 365, 1080, 8141, 11, 51600], "temperature": 0.0, "avg_logprob": -0.14598256208765226, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00128254946321249}, {"id": 593, "seek": 372304, "start": 3747.7599999999998, "end": 3752.72, "text": " you're going to have a lot of sums, right? Yep. So it's hidden, right? Yeah. The sums are hidden.", "tokens": [51600, 291, 434, 516, 281, 362, 257, 688, 295, 34499, 11, 558, 30, 7010, 13, 407, 309, 311, 7633, 11, 558, 30, 865, 13, 440, 34499, 366, 7633, 13, 51848], "temperature": 0.0, "avg_logprob": -0.14598256208765226, "compression_ratio": 1.7360594795539033, "no_speech_prob": 0.00128254946321249}, {"id": 594, "seek": 375304, "start": 3753.04, "end": 3755.52, "text": " Okay. Inside of this vector matrix product.", "tokens": [50364, 1033, 13, 15123, 295, 341, 8062, 8141, 1674, 13, 50488], "temperature": 0.0, "avg_logprob": -0.1285559218606831, "compression_ratio": 1.6213592233009708, "no_speech_prob": 8.480304677505046e-05}, {"id": 595, "seek": 375304, "start": 3760.08, "end": 3764.32, "text": " Like, you can take a specific example. Let's imagine that this G function is just a matrix", "tokens": [50716, 1743, 11, 291, 393, 747, 257, 2685, 1365, 13, 961, 311, 3811, 300, 341, 460, 2445, 307, 445, 257, 8141, 50928], "temperature": 0.0, "avg_logprob": -0.1285559218606831, "compression_ratio": 1.6213592233009708, "no_speech_prob": 8.480304677505046e-05}, {"id": 596, "seek": 375304, "start": 3764.32, "end": 3772.16, "text": " multiplication. Okay? We just multiply by ZF by matrix W. So we have a linear operation. The derivative", "tokens": [50928, 27290, 13, 1033, 30, 492, 445, 12972, 538, 1176, 37, 538, 8141, 343, 13, 407, 321, 362, 257, 8213, 6916, 13, 440, 13760, 51320], "temperature": 0.0, "avg_logprob": -0.1285559218606831, "compression_ratio": 1.6213592233009708, "no_speech_prob": 8.480304677505046e-05}, {"id": 597, "seek": 375304, "start": 3772.16, "end": 3779.36, "text": " of the Jacobian matrix of the multiplication by matrix is the transpose of that matrix. So what", "tokens": [51320, 295, 264, 14117, 952, 8141, 295, 264, 27290, 538, 8141, 307, 264, 25167, 295, 300, 8141, 13, 407, 437, 51680], "temperature": 0.0, "avg_logprob": -0.1285559218606831, "compression_ratio": 1.6213592233009708, "no_speech_prob": 8.480304677505046e-05}, {"id": 598, "seek": 377936, "start": 3779.36, "end": 3784.0, "text": " we're going to do here is take this vector, multiply it by the transpose of the W matrix,", "tokens": [50364, 321, 434, 516, 281, 360, 510, 307, 747, 341, 8062, 11, 12972, 309, 538, 264, 25167, 295, 264, 343, 8141, 11, 50596], "temperature": 0.0, "avg_logprob": -0.11163112965035946, "compression_ratio": 1.76, "no_speech_prob": 0.00014200599980540574}, {"id": 599, "seek": 377936, "start": 3784.8, "end": 3793.2000000000003, "text": " and what we get is that vector. Okay? And it all makes sense, right? The sizes make sense.", "tokens": [50636, 293, 437, 321, 483, 307, 300, 8062, 13, 1033, 30, 400, 309, 439, 1669, 2020, 11, 558, 30, 440, 11602, 652, 2020, 13, 51056], "temperature": 0.0, "avg_logprob": -0.11163112965035946, "compression_ratio": 1.76, "no_speech_prob": 0.00014200599980540574}, {"id": 600, "seek": 377936, "start": 3793.2000000000003, "end": 3799.6, "text": " This matrix here is the transpose of the weight matrix, which of course had the reverse size.", "tokens": [51056, 639, 8141, 510, 307, 264, 25167, 295, 264, 3364, 8141, 11, 597, 295, 1164, 632, 264, 9943, 2744, 13, 51376], "temperature": 0.0, "avg_logprob": -0.11163112965035946, "compression_ratio": 1.76, "no_speech_prob": 0.00014200599980540574}, {"id": 601, "seek": 377936, "start": 3800.6400000000003, "end": 3804.2400000000002, "text": " We multiply it. We pre-multiply it by the row vector of the gradient from the", "tokens": [51428, 492, 12972, 309, 13, 492, 659, 12, 76, 723, 647, 356, 309, 538, 264, 5386, 8062, 295, 264, 16235, 490, 264, 51608], "temperature": 0.0, "avg_logprob": -0.11163112965035946, "compression_ratio": 1.76, "no_speech_prob": 0.00014200599980540574}, {"id": 602, "seek": 380424, "start": 3805.2, "end": 3808.08, "text": " layer above, and we get the gradient with respect to the layer below.", "tokens": [50412, 4583, 3673, 11, 293, 321, 483, 264, 16235, 365, 3104, 281, 264, 4583, 2507, 13, 50556], "temperature": 0.0, "avg_logprob": -0.1289084553718567, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00019108352717012167}, {"id": 603, "seek": 380424, "start": 3810.64, "end": 3815.7599999999998, "text": " Okay? So backpropagating through a linear module just means multiplying the transpose", "tokens": [50684, 1033, 30, 407, 646, 79, 1513, 559, 990, 807, 257, 8213, 10088, 445, 1355, 30955, 264, 25167, 50940], "temperature": 0.0, "avg_logprob": -0.1289084553718567, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00019108352717012167}, {"id": 604, "seek": 380424, "start": 3815.7599999999998, "end": 3822.0, "text": " of the matrix used by that module. And it's just a generalized form of what I explained earlier,", "tokens": [50940, 295, 264, 8141, 1143, 538, 300, 10088, 13, 400, 309, 311, 445, 257, 44498, 1254, 295, 437, 286, 8825, 3071, 11, 51252], "temperature": 0.0, "avg_logprob": -0.1289084553718567, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00019108352717012167}, {"id": 605, "seek": 380424, "start": 3822.56, "end": 3828.0, "text": " you know, of propagating through the weights of a linear system. But it's less intuitive, right?", "tokens": [51280, 291, 458, 11, 295, 12425, 990, 807, 264, 17443, 295, 257, 8213, 1185, 13, 583, 309, 311, 1570, 21769, 11, 558, 30, 51552], "temperature": 0.0, "avg_logprob": -0.1289084553718567, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00019108352717012167}, {"id": 606, "seek": 380424, "start": 3830.08, "end": 3833.8399999999997, "text": " Okay. So we're going to be able to do backpropagation by computing gradients all the way through,", "tokens": [51656, 1033, 13, 407, 321, 434, 516, 281, 312, 1075, 281, 360, 646, 79, 1513, 559, 399, 538, 15866, 2771, 2448, 439, 264, 636, 807, 11, 51844], "temperature": 0.0, "avg_logprob": -0.1289084553718567, "compression_ratio": 1.6741573033707866, "no_speech_prob": 0.00019108352717012167}, {"id": 607, "seek": 383384, "start": 3833.84, "end": 3842.08, "text": " by propagating backwards. But this module really has two inputs. It has an input, which is ZF,", "tokens": [50364, 538, 12425, 990, 12204, 13, 583, 341, 10088, 534, 575, 732, 15743, 13, 467, 575, 364, 4846, 11, 597, 307, 1176, 37, 11, 50776], "temperature": 0.0, "avg_logprob": -0.09359026590983073, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.00011590979556785896}, {"id": 608, "seek": 383384, "start": 3842.08, "end": 3851.04, "text": " and the other one is WG, the weight matrix, the parameter vector that is used inside of this", "tokens": [50776, 293, 264, 661, 472, 307, 343, 38, 11, 264, 3364, 8141, 11, 264, 13075, 8062, 300, 307, 1143, 1854, 295, 341, 51224], "temperature": 0.0, "avg_logprob": -0.09359026590983073, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.00011590979556785896}, {"id": 609, "seek": 383384, "start": 3851.04, "end": 3857.76, "text": " module. So there is a second Jacobian matrix, which is the Jacobian matrix of ZG with respect to", "tokens": [51224, 10088, 13, 407, 456, 307, 257, 1150, 14117, 952, 8141, 11, 597, 307, 264, 14117, 952, 8141, 295, 1176, 38, 365, 3104, 281, 51560], "temperature": 0.0, "avg_logprob": -0.09359026590983073, "compression_ratio": 1.5434782608695652, "no_speech_prob": 0.00011590979556785896}, {"id": 610, "seek": 385776, "start": 3858.5600000000004, "end": 3865.2000000000003, "text": " the terms of this weight parameter. Okay? And to compute the gradient of the cost function", "tokens": [50404, 264, 2115, 295, 341, 3364, 13075, 13, 1033, 30, 400, 281, 14722, 264, 16235, 295, 264, 2063, 2445, 50736], "temperature": 0.0, "avg_logprob": -0.06307616076626621, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0005112145445309579}, {"id": 611, "seek": 385776, "start": 3865.2000000000003, "end": 3872.0800000000004, "text": " with respect to those weight parameters, I need to multiply this gradient vector by the Jacobian", "tokens": [50736, 365, 3104, 281, 729, 3364, 9834, 11, 286, 643, 281, 12972, 341, 16235, 8062, 538, 264, 14117, 952, 51080], "temperature": 0.0, "avg_logprob": -0.06307616076626621, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0005112145445309579}, {"id": 612, "seek": 385776, "start": 3872.0800000000004, "end": 3877.36, "text": " matrix of that block with respect to its weight. And it's not the same as the Jacobian matrix with", "tokens": [51080, 8141, 295, 300, 3461, 365, 3104, 281, 1080, 3364, 13, 400, 309, 311, 406, 264, 912, 382, 264, 14117, 952, 8141, 365, 51344], "temperature": 0.0, "avg_logprob": -0.06307616076626621, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0005112145445309579}, {"id": 613, "seek": 385776, "start": 3877.36, "end": 3884.48, "text": " respect to the input. It's a different Jacobian matrix. I'll come back to this in a second.", "tokens": [51344, 3104, 281, 264, 4846, 13, 467, 311, 257, 819, 14117, 952, 8141, 13, 286, 603, 808, 646, 281, 341, 294, 257, 1150, 13, 51700], "temperature": 0.0, "avg_logprob": -0.06307616076626621, "compression_ratio": 1.8529411764705883, "no_speech_prob": 0.0005112145445309579}, {"id": 614, "seek": 388448, "start": 3885.12, "end": 3895.2, "text": " So to do backprop, again, if we have a vector of gradients of some cost with respect to a state,", "tokens": [50396, 407, 281, 360, 646, 79, 1513, 11, 797, 11, 498, 321, 362, 257, 8062, 295, 2771, 2448, 295, 512, 2063, 365, 3104, 281, 257, 1785, 11, 50900], "temperature": 0.0, "avg_logprob": -0.07509769002596538, "compression_ratio": 1.8104265402843602, "no_speech_prob": 0.00024153354752343148}, {"id": 615, "seek": 388448, "start": 3895.92, "end": 3900.96, "text": " and we have a function that is a function of one or several variables, we multiply this gradient by", "tokens": [50936, 293, 321, 362, 257, 2445, 300, 307, 257, 2445, 295, 472, 420, 2940, 9102, 11, 321, 12972, 341, 16235, 538, 51188], "temperature": 0.0, "avg_logprob": -0.07509769002596538, "compression_ratio": 1.8104265402843602, "no_speech_prob": 0.00024153354752343148}, {"id": 616, "seek": 388448, "start": 3900.96, "end": 3906.16, "text": " the Jacobian matrix of this block with respect to each of these inputs, and that gives us the", "tokens": [51188, 264, 14117, 952, 8141, 295, 341, 3461, 365, 3104, 281, 1184, 295, 613, 15743, 11, 293, 300, 2709, 505, 264, 51448], "temperature": 0.0, "avg_logprob": -0.07509769002596538, "compression_ratio": 1.8104265402843602, "no_speech_prob": 0.00024153354752343148}, {"id": 617, "seek": 388448, "start": 3906.16, "end": 3911.52, "text": " gradient with respect to each of the inputs. And that's going to be expressed here. So this", "tokens": [51448, 16235, 365, 3104, 281, 1184, 295, 264, 15743, 13, 400, 300, 311, 516, 281, 312, 12675, 510, 13, 407, 341, 51716], "temperature": 0.0, "avg_logprob": -0.07509769002596538, "compression_ratio": 1.8104265402843602, "no_speech_prob": 0.00024153354752343148}, {"id": 618, "seek": 391152, "start": 3911.52, "end": 3921.44, "text": " is the backpropagation of states in a layer-wise classical type neural net. DC over DZK, which is", "tokens": [50364, 307, 264, 646, 79, 1513, 559, 399, 295, 4368, 294, 257, 4583, 12, 3711, 13735, 2010, 18161, 2533, 13, 9114, 670, 413, 57, 42, 11, 597, 307, 50860], "temperature": 0.0, "avg_logprob": -0.12111797332763671, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.0005029752501286566}, {"id": 619, "seek": 391152, "start": 3921.44, "end": 3927.92, "text": " the state of layer K, is DC over ZK plus one, which is the gradient of the cost with respect to", "tokens": [50860, 264, 1785, 295, 4583, 591, 11, 307, 9114, 670, 1176, 42, 1804, 472, 11, 597, 307, 264, 16235, 295, 264, 2063, 365, 3104, 281, 51184], "temperature": 0.0, "avg_logprob": -0.12111797332763671, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.0005029752501286566}, {"id": 620, "seek": 391152, "start": 3927.92, "end": 3935.12, "text": " the layer above, times the Jacobian matrix of the state of layer K plus one with respect to the", "tokens": [51184, 264, 4583, 3673, 11, 1413, 264, 14117, 952, 8141, 295, 264, 1785, 295, 4583, 591, 1804, 472, 365, 3104, 281, 264, 51544], "temperature": 0.0, "avg_logprob": -0.12111797332763671, "compression_ratio": 1.7515151515151515, "no_speech_prob": 0.0005029752501286566}, {"id": 621, "seek": 393512, "start": 3935.2, "end": 3942.0, "text": " state of layer K. Now we assume DC over DZK plus one is known, and we just need to multiply", "tokens": [50368, 1785, 295, 4583, 591, 13, 823, 321, 6552, 9114, 670, 413, 57, 42, 1804, 472, 307, 2570, 11, 293, 321, 445, 643, 281, 12972, 50708], "temperature": 0.0, "avg_logprob": -0.10698742002952756, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0004172362096142024}, {"id": 622, "seek": 393512, "start": 3942.0, "end": 3946.72, "text": " with the Jacobian matrix of the function that links ZK to ZK plus one. The function is used to", "tokens": [50708, 365, 264, 14117, 952, 8141, 295, 264, 2445, 300, 6123, 1176, 42, 281, 1176, 42, 1804, 472, 13, 440, 2445, 307, 1143, 281, 50944], "temperature": 0.0, "avg_logprob": -0.10698742002952756, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0004172362096142024}, {"id": 623, "seek": 393512, "start": 3946.72, "end": 3951.2799999999997, "text": " compute ZK plus one from ZK. And this may be a function also of some parameters inside.", "tokens": [50944, 14722, 1176, 42, 1804, 472, 490, 1176, 42, 13, 400, 341, 815, 312, 257, 2445, 611, 295, 512, 9834, 1854, 13, 51172], "temperature": 0.0, "avg_logprob": -0.10698742002952756, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0004172362096142024}, {"id": 624, "seek": 393512, "start": 3951.2799999999997, "end": 3957.44, "text": " But here, that's the matrix of partial derivatives of F, which is with output to ZK plus one,", "tokens": [51172, 583, 510, 11, 300, 311, 264, 8141, 295, 14641, 33733, 295, 479, 11, 597, 307, 365, 5598, 281, 1176, 42, 1804, 472, 11, 51480], "temperature": 0.0, "avg_logprob": -0.10698742002952756, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0004172362096142024}, {"id": 625, "seek": 393512, "start": 3957.44, "end": 3964.7999999999997, "text": " with respect to each of the components of ZK. So that's the first rule of backpropagation,", "tokens": [51480, 365, 3104, 281, 1184, 295, 264, 6677, 295, 1176, 42, 13, 407, 300, 311, 264, 700, 4978, 295, 646, 79, 1513, 559, 399, 11, 51848], "temperature": 0.0, "avg_logprob": -0.10698742002952756, "compression_ratio": 1.7063197026022305, "no_speech_prob": 0.0004172362096142024}, {"id": 626, "seek": 396480, "start": 3964.8, "end": 3969.84, "text": " and it's a recursive rule. So you can start from the top. You start initially with DC over DC,", "tokens": [50364, 293, 309, 311, 257, 20560, 488, 4978, 13, 407, 291, 393, 722, 490, 264, 1192, 13, 509, 722, 9105, 365, 9114, 670, 9114, 11, 50616], "temperature": 0.0, "avg_logprob": -0.10092072408707416, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00041072317981161177}, {"id": 627, "seek": 396480, "start": 3969.84, "end": 3976.6400000000003, "text": " which is one, which is why I have this one here on top. And then you just keep multiplying by", "tokens": [50616, 597, 307, 472, 11, 597, 307, 983, 286, 362, 341, 472, 510, 322, 1192, 13, 400, 550, 291, 445, 1066, 30955, 538, 50956], "temperature": 0.0, "avg_logprob": -0.10092072408707416, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00041072317981161177}, {"id": 628, "seek": 396480, "start": 3977.6800000000003, "end": 3983.36, "text": " the Jacobian matrix all the way down, and backpropagate gradients. And now you get gradients", "tokens": [51008, 264, 14117, 952, 8141, 439, 264, 636, 760, 11, 293, 646, 79, 1513, 559, 473, 2771, 2448, 13, 400, 586, 291, 483, 2771, 2448, 51292], "temperature": 0.0, "avg_logprob": -0.10092072408707416, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00041072317981161177}, {"id": 629, "seek": 396480, "start": 3983.36, "end": 3986.96, "text": " with respect to all the states. You also want the gradients with respect to the weights,", "tokens": [51292, 365, 3104, 281, 439, 264, 4368, 13, 509, 611, 528, 264, 2771, 2448, 365, 3104, 281, 264, 17443, 11, 51472], "temperature": 0.0, "avg_logprob": -0.10092072408707416, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00041072317981161177}, {"id": 630, "seek": 396480, "start": 3986.96, "end": 3992.2400000000002, "text": " because that's what you need to do learning. So what you can write is the same chain rule,", "tokens": [51472, 570, 300, 311, 437, 291, 643, 281, 360, 2539, 13, 407, 437, 291, 393, 2464, 307, 264, 912, 5021, 4978, 11, 51736], "temperature": 0.0, "avg_logprob": -0.10092072408707416, "compression_ratio": 1.746212121212121, "no_speech_prob": 0.00041072317981161177}, {"id": 631, "seek": 399224, "start": 3992.24, "end": 3998.16, "text": " DC over DWK is equal to DC over the ZK plus one, which we assume is known, times", "tokens": [50364, 9114, 670, 45318, 42, 307, 2681, 281, 9114, 670, 264, 1176, 42, 1804, 472, 11, 597, 321, 6552, 307, 2570, 11, 1413, 50660], "temperature": 0.0, "avg_logprob": -0.14487950134277344, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.001115422579459846}, {"id": 632, "seek": 399224, "start": 3998.16, "end": 4004.24, "text": " DZK plus one of DWK, right? And you can write this as DC over DK plus one. And the dependency", "tokens": [50660, 413, 57, 42, 1804, 472, 295, 45318, 42, 11, 558, 30, 400, 291, 393, 2464, 341, 382, 9114, 670, 31934, 1804, 472, 13, 400, 264, 33621, 50964], "temperature": 0.0, "avg_logprob": -0.14487950134277344, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.001115422579459846}, {"id": 633, "seek": 399224, "start": 4004.24, "end": 4010.56, "text": " between ZK plus one and WK is the function ZK applied to WK. So you can differentiate the", "tokens": [50964, 1296, 1176, 42, 1804, 472, 293, 343, 42, 307, 264, 2445, 1176, 42, 6456, 281, 343, 42, 13, 407, 291, 393, 23203, 264, 51280], "temperature": 0.0, "avg_logprob": -0.14487950134277344, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.001115422579459846}, {"id": 634, "seek": 399224, "start": 4011.9199999999996, "end": 4015.2799999999997, "text": " function, the output of the function ZK with respect to WK, and that gives you another", "tokens": [51348, 2445, 11, 264, 5598, 295, 264, 2445, 1176, 42, 365, 3104, 281, 343, 42, 11, 293, 300, 2709, 291, 1071, 51516], "temperature": 0.0, "avg_logprob": -0.14487950134277344, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.001115422579459846}, {"id": 635, "seek": 399224, "start": 4015.2799999999997, "end": 4019.3599999999997, "text": " Jacobian matrix. And so those two formulas, you can do backpropagation just about anything.", "tokens": [51516, 14117, 952, 8141, 13, 400, 370, 729, 732, 30546, 11, 291, 393, 360, 646, 79, 1513, 559, 399, 445, 466, 1340, 13, 51720], "temperature": 0.0, "avg_logprob": -0.14487950134277344, "compression_ratio": 1.7237354085603114, "no_speech_prob": 0.001115422579459846}, {"id": 636, "seek": 401936, "start": 4020.08, "end": 4026.32, "text": " Really what goes on inside PyTorch and inside most of those frameworks, TensorFlow and", "tokens": [50400, 4083, 437, 1709, 322, 1854, 9953, 51, 284, 339, 293, 1854, 881, 295, 729, 29834, 11, 37624, 293, 50712], "temperature": 0.0, "avg_logprob": -0.1280878310979799, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022689839533995837}, {"id": 637, "seek": 401936, "start": 4026.32, "end": 4032.0, "text": " Jackson, whatever. It's something like this where you have, so let's take a very simple", "tokens": [50712, 10647, 11, 2035, 13, 467, 311, 746, 411, 341, 689, 291, 362, 11, 370, 718, 311, 747, 257, 588, 2199, 50996], "temperature": 0.0, "avg_logprob": -0.1280878310979799, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022689839533995837}, {"id": 638, "seek": 401936, "start": 4032.0, "end": 4036.96, "text": " diagram here where you have an input parameterized function that computes an output that goes to", "tokens": [50996, 10686, 510, 689, 291, 362, 364, 4846, 13075, 1602, 2445, 300, 715, 1819, 364, 5598, 300, 1709, 281, 51244], "temperature": 0.0, "avg_logprob": -0.1280878310979799, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022689839533995837}, {"id": 639, "seek": 401936, "start": 4036.96, "end": 4041.52, "text": " a cost function. And that cost function measures the discrepancy between the output of the system", "tokens": [51244, 257, 2063, 2445, 13, 400, 300, 2063, 2445, 8000, 264, 2983, 265, 6040, 1344, 1296, 264, 5598, 295, 264, 1185, 51472], "temperature": 0.0, "avg_logprob": -0.1280878310979799, "compression_ratio": 1.6327433628318584, "no_speech_prob": 0.00022689839533995837}, {"id": 640, "seek": 404152, "start": 4041.6, "end": 4049.84, "text": " and the desired output. So you can write this function as C of G of W. I didn't put the X here,", "tokens": [50368, 293, 264, 14721, 5598, 13, 407, 291, 393, 2464, 341, 2445, 382, 383, 295, 460, 295, 343, 13, 286, 994, 380, 829, 264, 1783, 510, 11, 50780], "temperature": 0.0, "avg_logprob": -0.14038449746591072, "compression_ratio": 1.3687943262411348, "no_speech_prob": 0.0010001808404922485}, {"id": 641, "seek": 404152, "start": 4049.84, "end": 4057.92, "text": " but just for charity. And the derivative of this is, again, you apply chain rule or you can write", "tokens": [50780, 457, 445, 337, 16863, 13, 400, 264, 13760, 295, 341, 307, 11, 797, 11, 291, 3079, 5021, 4978, 420, 291, 393, 2464, 51184], "temperature": 0.0, "avg_logprob": -0.14038449746591072, "compression_ratio": 1.3687943262411348, "no_speech_prob": 0.0010001808404922485}, {"id": 642, "seek": 405792, "start": 4058.0, "end": 4071.6, "text": " it with partial derivatives this way. And same for, you know, expand the dependency of the output", "tokens": [50368, 309, 365, 14641, 33733, 341, 636, 13, 400, 912, 337, 11, 291, 458, 11, 5268, 264, 33621, 295, 264, 5598, 51048], "temperature": 0.0, "avg_logprob": -0.11705334545814827, "compression_ratio": 1.6174863387978142, "no_speech_prob": 0.004006390925496817}, {"id": 643, "seek": 405792, "start": 4071.6, "end": 4078.16, "text": " with respect to the parameters as the Jacobian matrix of G with respect to W. If W is a scalar,", "tokens": [51048, 365, 3104, 281, 264, 9834, 382, 264, 14117, 952, 8141, 295, 460, 365, 3104, 281, 343, 13, 759, 343, 307, 257, 39684, 11, 51376], "temperature": 0.0, "avg_logprob": -0.11705334545814827, "compression_ratio": 1.6174863387978142, "no_speech_prob": 0.004006390925496817}, {"id": 644, "seek": 405792, "start": 4078.16, "end": 4086.2400000000002, "text": " then this is just a derivative, partial derivative. Okay, now you can express this as a compute graph.", "tokens": [51376, 550, 341, 307, 445, 257, 13760, 11, 14641, 13760, 13, 1033, 11, 586, 291, 393, 5109, 341, 382, 257, 14722, 4295, 13, 51780], "temperature": 0.0, "avg_logprob": -0.11705334545814827, "compression_ratio": 1.6174863387978142, "no_speech_prob": 0.004006390925496817}, {"id": 645, "seek": 408624, "start": 4086.24, "end": 4091.9199999999996, "text": " So you can say, like, how am I going to compute DC over DW? What I'm going to have to do is take", "tokens": [50364, 407, 291, 393, 584, 11, 411, 11, 577, 669, 286, 516, 281, 14722, 9114, 670, 45318, 30, 708, 286, 478, 516, 281, 362, 281, 360, 307, 747, 50648], "temperature": 0.0, "avg_logprob": -0.12538613319396974, "compression_ratio": 1.7476190476190476, "no_speech_prob": 0.0003101116744801402}, {"id": 646, "seek": 408624, "start": 4091.9199999999996, "end": 4095.9199999999996, "text": " the value one, which is the derivative of C with respect to itself, basically,", "tokens": [50648, 264, 2158, 472, 11, 597, 307, 264, 13760, 295, 383, 365, 3104, 281, 2564, 11, 1936, 11, 50848], "temperature": 0.0, "avg_logprob": -0.12538613319396974, "compression_ratio": 1.7476190476190476, "no_speech_prob": 0.0003101116744801402}, {"id": 647, "seek": 408624, "start": 4095.9199999999996, "end": 4101.2, "text": " the loss with respect to itself. I'm going to multiply this by the derivative of the cost with", "tokens": [50848, 264, 4470, 365, 3104, 281, 2564, 13, 286, 478, 516, 281, 12972, 341, 538, 264, 13760, 295, 264, 2063, 365, 51112], "temperature": 0.0, "avg_logprob": -0.12538613319396974, "compression_ratio": 1.7476190476190476, "no_speech_prob": 0.0003101116744801402}, {"id": 648, "seek": 408624, "start": 4101.2, "end": 4112.8, "text": " respect to Y bar. Okay, and that's going to give me DC over DY bar, obviously. Okay, this is the", "tokens": [51112, 3104, 281, 398, 2159, 13, 1033, 11, 293, 300, 311, 516, 281, 976, 385, 9114, 670, 48427, 2159, 11, 2745, 13, 1033, 11, 341, 307, 264, 51692], "temperature": 0.0, "avg_logprob": -0.12538613319396974, "compression_ratio": 1.7476190476190476, "no_speech_prob": 0.0003101116744801402}, {"id": 649, "seek": 411280, "start": 4112.88, "end": 4118.0, "text": " same as this because I'm just multiplied by one. Then multiply this by the Jacobian matrix of G", "tokens": [50368, 912, 382, 341, 570, 286, 478, 445, 17207, 538, 472, 13, 1396, 12972, 341, 538, 264, 14117, 952, 8141, 295, 460, 50624], "temperature": 0.0, "avg_logprob": -0.14110342326917147, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.0004581815155688673}, {"id": 650, "seek": 411280, "start": 4118.0, "end": 4123.84, "text": " with respect to W, which is a derivative if W is a scalar. That, of course, depends on X.", "tokens": [50624, 365, 3104, 281, 343, 11, 597, 307, 257, 13760, 498, 343, 307, 257, 39684, 13, 663, 11, 295, 1164, 11, 5946, 322, 1783, 13, 50916], "temperature": 0.0, "avg_logprob": -0.14110342326917147, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.0004581815155688673}, {"id": 651, "seek": 411280, "start": 4125.28, "end": 4132.0, "text": " And I get DC over DW. So this is a so-called compute graph, right? This is a way of organizing", "tokens": [50988, 400, 286, 483, 9114, 670, 45318, 13, 407, 341, 307, 257, 370, 12, 11880, 14722, 4295, 11, 558, 30, 639, 307, 257, 636, 295, 17608, 51324], "temperature": 0.0, "avg_logprob": -0.14110342326917147, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.0004581815155688673}, {"id": 652, "seek": 411280, "start": 4132.0, "end": 4138.56, "text": " operations to compute the gradient. And there is essentially an automatic way of transforming", "tokens": [51324, 7705, 281, 14722, 264, 16235, 13, 400, 456, 307, 4476, 364, 12509, 636, 295, 27210, 51652], "temperature": 0.0, "avg_logprob": -0.14110342326917147, "compression_ratio": 1.5390946502057614, "no_speech_prob": 0.0004581815155688673}, {"id": 653, "seek": 413856, "start": 4138.64, "end": 4145.200000000001, "text": " a compute graph of this type into a compute graph of this type that computes the gradient", "tokens": [50368, 257, 14722, 4295, 295, 341, 2010, 666, 257, 14722, 4295, 295, 341, 2010, 300, 715, 1819, 264, 16235, 50696], "temperature": 0.0, "avg_logprob": -0.10966962343686587, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.000451100553618744}, {"id": 654, "seek": 413856, "start": 4145.200000000001, "end": 4151.92, "text": " automatically. And this is the magic that happens in the automatic differentiation inside PyTorch and", "tokens": [50696, 6772, 13, 400, 341, 307, 264, 5585, 300, 2314, 294, 264, 12509, 38902, 1854, 9953, 51, 284, 339, 293, 51032], "temperature": 0.0, "avg_logprob": -0.10966962343686587, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.000451100553618744}, {"id": 655, "seek": 413856, "start": 4151.92, "end": 4158.0, "text": " TensorFlow and other systems. Some systems are pretty smart about this in a sense that", "tokens": [51032, 37624, 293, 661, 3652, 13, 2188, 3652, 366, 1238, 4069, 466, 341, 294, 257, 2020, 300, 51336], "temperature": 0.0, "avg_logprob": -0.10966962343686587, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.000451100553618744}, {"id": 656, "seek": 413856, "start": 4159.120000000001, "end": 4164.64, "text": " those functions can be fairly complicated. They can involve themselves computing derivatives and", "tokens": [51392, 729, 6828, 393, 312, 6457, 6179, 13, 814, 393, 9494, 2969, 15866, 33733, 293, 51668], "temperature": 0.0, "avg_logprob": -0.10966962343686587, "compression_ratio": 1.7523364485981308, "no_speech_prob": 0.000451100553618744}, {"id": 657, "seek": 416464, "start": 4165.6, "end": 4170.400000000001, "text": " they can involve dynamic computation, where the graph of computation depends on the data.", "tokens": [50412, 436, 393, 9494, 8546, 24903, 11, 689, 264, 4295, 295, 24903, 5946, 322, 264, 1412, 13, 50652], "temperature": 0.0, "avg_logprob": -0.1282914866571841, "compression_ratio": 1.8185654008438819, "no_speech_prob": 0.0005441377870738506}, {"id": 658, "seek": 416464, "start": 4171.200000000001, "end": 4175.92, "text": " And actually PyTorch does this properly. I'm not going to go through all the details of this,", "tokens": [50692, 400, 767, 9953, 51, 284, 339, 775, 341, 6108, 13, 286, 478, 406, 516, 281, 352, 807, 439, 264, 4365, 295, 341, 11, 50928], "temperature": 0.0, "avg_logprob": -0.1282914866571841, "compression_ratio": 1.8185654008438819, "no_speech_prob": 0.0005441377870738506}, {"id": 659, "seek": 416464, "start": 4175.92, "end": 4181.12, "text": " but this is kind of a way of reminding you what the dimensions of all those things are, right? So", "tokens": [50928, 457, 341, 307, 733, 295, 257, 636, 295, 27639, 291, 437, 264, 12819, 295, 439, 729, 721, 366, 11, 558, 30, 407, 51188], "temperature": 0.0, "avg_logprob": -0.1282914866571841, "compression_ratio": 1.8185654008438819, "no_speech_prob": 0.0005441377870738506}, {"id": 660, "seek": 416464, "start": 4181.92, "end": 4186.160000000001, "text": " if Y is a column vector of size M, W is a column vector of size N,", "tokens": [51228, 498, 398, 307, 257, 7738, 8062, 295, 2744, 376, 11, 343, 307, 257, 7738, 8062, 295, 2744, 426, 11, 51440], "temperature": 0.0, "avg_logprob": -0.1282914866571841, "compression_ratio": 1.8185654008438819, "no_speech_prob": 0.0005441377870738506}, {"id": 661, "seek": 416464, "start": 4187.6, "end": 4191.92, "text": " then this is a row vector of size N, this is a row vector of size M, and this is a", "tokens": [51512, 550, 341, 307, 257, 5386, 8062, 295, 2744, 426, 11, 341, 307, 257, 5386, 8062, 295, 2744, 376, 11, 293, 341, 307, 257, 51728], "temperature": 0.0, "avg_logprob": -0.1282914866571841, "compression_ratio": 1.8185654008438819, "no_speech_prob": 0.0005441377870738506}, {"id": 662, "seek": 419192, "start": 4191.92, "end": 4197.6, "text": " geocomium matrix of size N by N. And all of this works out.", "tokens": [50364, 1519, 905, 298, 2197, 8141, 295, 2744, 426, 538, 426, 13, 400, 439, 295, 341, 1985, 484, 13, 50648], "temperature": 0.0, "avg_logprob": -0.19698130194820576, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.0002452820772305131}, {"id": 663, "seek": 419192, "start": 4201.68, "end": 4206.4800000000005, "text": " Okay, so the way we're going to build neural nets, and I'll come back to this in a", "tokens": [50852, 1033, 11, 370, 264, 636, 321, 434, 516, 281, 1322, 18161, 36170, 11, 293, 286, 603, 808, 646, 281, 341, 294, 257, 51092], "temperature": 0.0, "avg_logprob": -0.19698130194820576, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.0002452820772305131}, {"id": 664, "seek": 419192, "start": 4206.4800000000005, "end": 4215.28, "text": " subsequent lecture, is that we are going to have at our disposal a large collection of basic modules", "tokens": [51092, 19962, 7991, 11, 307, 300, 321, 366, 516, 281, 362, 412, 527, 26400, 257, 2416, 5765, 295, 3875, 16679, 51532], "temperature": 0.0, "avg_logprob": -0.19698130194820576, "compression_ratio": 1.4210526315789473, "no_speech_prob": 0.0002452820772305131}, {"id": 665, "seek": 421528, "start": 4215.28, "end": 4221.2, "text": " which we're going to be able to arrange in more or less complex graphs", "tokens": [50364, 597, 321, 434, 516, 281, 312, 1075, 281, 9424, 294, 544, 420, 1570, 3997, 24877, 50660], "temperature": 0.0, "avg_logprob": -0.10460124618705662, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0004440366756170988}, {"id": 666, "seek": 421528, "start": 4222.4, "end": 4230.32, "text": " as a way to build the architecture of a learning system. Okay, so either we're going to write a", "tokens": [50720, 382, 257, 636, 281, 1322, 264, 9482, 295, 257, 2539, 1185, 13, 1033, 11, 370, 2139, 321, 434, 516, 281, 2464, 257, 51116], "temperature": 0.0, "avg_logprob": -0.10460124618705662, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0004440366756170988}, {"id": 667, "seek": 421528, "start": 4230.32, "end": 4236.639999999999, "text": " class or we're going to write a program that runs the forward pass, and this program is going to be", "tokens": [51116, 1508, 420, 321, 434, 516, 281, 2464, 257, 1461, 300, 6676, 264, 2128, 1320, 11, 293, 341, 1461, 307, 516, 281, 312, 51432], "temperature": 0.0, "avg_logprob": -0.10460124618705662, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0004440366756170988}, {"id": 668, "seek": 421528, "start": 4237.44, "end": 4244.16, "text": " composed of basic mathematical operations, addition, subtraction of tensors or multi-dimensional arrays,", "tokens": [51472, 18204, 295, 3875, 18894, 7705, 11, 4500, 11, 16390, 313, 295, 10688, 830, 420, 4825, 12, 18759, 41011, 11, 51808], "temperature": 0.0, "avg_logprob": -0.10460124618705662, "compression_ratio": 1.733644859813084, "no_speech_prob": 0.0004440366756170988}, {"id": 669, "seek": 424528, "start": 4245.84, "end": 4250.5599999999995, "text": " other types of scalar operations, or the application of one of the predefined", "tokens": [50392, 661, 3467, 295, 39684, 7705, 11, 420, 264, 3861, 295, 472, 295, 264, 659, 37716, 50628], "temperature": 0.0, "avg_logprob": -0.12248150895281536, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.00033522461308166385}, {"id": 670, "seek": 424528, "start": 4251.759999999999, "end": 4257.759999999999, "text": " complex parameterized functions, like a linear module, a value, or things like that.", "tokens": [50688, 3997, 13075, 1602, 6828, 11, 411, 257, 8213, 10088, 11, 257, 2158, 11, 420, 721, 411, 300, 13, 50988], "temperature": 0.0, "avg_logprob": -0.12248150895281536, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.00033522461308166385}, {"id": 671, "seek": 424528, "start": 4260.32, "end": 4268.08, "text": " And we have at our disposal a large library of such modules, which are things that people have", "tokens": [51116, 400, 321, 362, 412, 527, 26400, 257, 2416, 6405, 295, 1270, 16679, 11, 597, 366, 721, 300, 561, 362, 51504], "temperature": 0.0, "avg_logprob": -0.12248150895281536, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.00033522461308166385}, {"id": 672, "seek": 424528, "start": 4268.08, "end": 4274.639999999999, "text": " come up with over the years that are kind of basic modules that are used in a lot of applications.", "tokens": [51504, 808, 493, 365, 670, 264, 924, 300, 366, 733, 295, 3875, 16679, 300, 366, 1143, 294, 257, 688, 295, 5821, 13, 51832], "temperature": 0.0, "avg_logprob": -0.12248150895281536, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.00033522461308166385}, {"id": 673, "seek": 427528, "start": 4275.28, "end": 4279.28, "text": " Right, so the basic things that we've seen so far I think is like values. There's other", "tokens": [50364, 1779, 11, 370, 264, 3875, 721, 300, 321, 600, 1612, 370, 1400, 286, 519, 307, 411, 4190, 13, 821, 311, 661, 50564], "temperature": 0.0, "avg_logprob": -0.1068466300638313, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.00020978292741347104}, {"id": 674, "seek": 427528, "start": 4279.28, "end": 4284.4, "text": " nonlinear functions like sigmoids and variations of this. There's a large collection of them.", "tokens": [50564, 2107, 28263, 6828, 411, 4556, 3280, 3742, 293, 17840, 295, 341, 13, 821, 311, 257, 2416, 5765, 295, 552, 13, 50820], "temperature": 0.0, "avg_logprob": -0.1068466300638313, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.00020978292741347104}, {"id": 675, "seek": 427528, "start": 4285.28, "end": 4288.8, "text": " And then we have cost functions like square error, cross entropy, hinge loss, ranking loss,", "tokens": [50864, 400, 550, 321, 362, 2063, 6828, 411, 3732, 6713, 11, 3278, 30867, 11, 28822, 4470, 11, 17833, 4470, 11, 51040], "temperature": 0.0, "avg_logprob": -0.1068466300638313, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.00020978292741347104}, {"id": 676, "seek": 427528, "start": 4288.8, "end": 4292.4, "text": " and blah, blah, blah, which I'm not going to go through now, but we'll talk about this later.", "tokens": [51040, 293, 12288, 11, 12288, 11, 12288, 11, 597, 286, 478, 406, 516, 281, 352, 807, 586, 11, 457, 321, 603, 751, 466, 341, 1780, 13, 51220], "temperature": 0.0, "avg_logprob": -0.1068466300638313, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.00020978292741347104}, {"id": 677, "seek": 427528, "start": 4295.5199999999995, "end": 4302.88, "text": " The nice thing about this formalism is that, as I said before, you can sort of compute", "tokens": [51376, 440, 1481, 551, 466, 341, 9860, 1434, 307, 300, 11, 382, 286, 848, 949, 11, 291, 393, 1333, 295, 14722, 51744], "temperature": 0.0, "avg_logprob": -0.1068466300638313, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.00020978292741347104}, {"id": 678, "seek": 430288, "start": 4303.4400000000005, "end": 4312.64, "text": " graphs. You can construct a deep learning system by assembling those modules in any kind", "tokens": [50392, 24877, 13, 509, 393, 7690, 257, 2452, 2539, 1185, 538, 43867, 729, 16679, 294, 604, 733, 50852], "temperature": 0.0, "avg_logprob": -0.11998706144445083, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.00034054977004416287}, {"id": 679, "seek": 430288, "start": 4312.64, "end": 4319.36, "text": " of arrangement you want, as long as there is no loops in the connection graph. So as long as", "tokens": [50852, 295, 17620, 291, 528, 11, 382, 938, 382, 456, 307, 572, 16121, 294, 264, 4984, 4295, 13, 407, 382, 938, 382, 51188], "temperature": 0.0, "avg_logprob": -0.11998706144445083, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.00034054977004416287}, {"id": 680, "seek": 430288, "start": 4319.92, "end": 4324.08, "text": " you can come up with a partial order in those modules that will ensure that they are", "tokens": [51216, 291, 393, 808, 493, 365, 257, 14641, 1668, 294, 729, 16679, 300, 486, 5586, 300, 436, 366, 51424], "temperature": 0.0, "avg_logprob": -0.11998706144445083, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.00034054977004416287}, {"id": 681, "seek": 430288, "start": 4324.08, "end": 4331.04, "text": " computed in the proper way. But there is a way to handle loops, and that's called recurrent", "tokens": [51424, 40610, 294, 264, 2296, 636, 13, 583, 456, 307, 257, 636, 281, 4813, 16121, 11, 293, 300, 311, 1219, 18680, 1753, 51772], "temperature": 0.0, "avg_logprob": -0.11998706144445083, "compression_ratio": 1.6574074074074074, "no_speech_prob": 0.00034054977004416287}, {"id": 682, "seek": 433104, "start": 4331.04, "end": 4338.4, "text": " nets. We'll talk about this later. Okay, so here's a few practical tricks if you want to", "tokens": [50364, 36170, 13, 492, 603, 751, 466, 341, 1780, 13, 1033, 11, 370, 510, 311, 257, 1326, 8496, 11733, 498, 291, 528, 281, 50732], "temperature": 0.0, "avg_logprob": -0.1394995961870466, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0008283841307274997}, {"id": 683, "seek": 433104, "start": 4338.4, "end": 4343.36, "text": " play with neural nets, and you're going to do that soon enough, perhaps even tomorrow.", "tokens": [50732, 862, 365, 18161, 36170, 11, 293, 291, 434, 516, 281, 360, 300, 2321, 1547, 11, 4317, 754, 4153, 13, 50980], "temperature": 0.0, "avg_logprob": -0.1394995961870466, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0008283841307274997}, {"id": 684, "seek": 433104, "start": 4348.8, "end": 4352.96, "text": " And these are kind of a bit of a black art of deep learning, which is sort of a lot of it is", "tokens": [51252, 400, 613, 366, 733, 295, 257, 857, 295, 257, 2211, 1523, 295, 2452, 2539, 11, 597, 307, 1333, 295, 257, 688, 295, 309, 307, 51460], "temperature": 0.0, "avg_logprob": -0.1394995961870466, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0008283841307274997}, {"id": 685, "seek": 433104, "start": 4352.96, "end": 4357.28, "text": " implemented already in things like PyTorch if you used under tools, but some of it is kind of more", "tokens": [51460, 12270, 1217, 294, 721, 411, 9953, 51, 284, 339, 498, 291, 1143, 833, 3873, 11, 457, 512, 295, 309, 307, 733, 295, 544, 51676], "temperature": 0.0, "avg_logprob": -0.1394995961870466, "compression_ratio": 1.5617021276595744, "no_speech_prob": 0.0008283841307274997}, {"id": 686, "seek": 435728, "start": 4357.28, "end": 4362.0, "text": " of the sort of oral culture if you want of the deep learning community. You can find this in", "tokens": [50364, 295, 264, 1333, 295, 19338, 3713, 498, 291, 528, 295, 264, 2452, 2539, 1768, 13, 509, 393, 915, 341, 294, 50600], "temperature": 0.0, "avg_logprob": -0.14647839433055812, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0004494049644563347}, {"id": 687, "seek": 435728, "start": 4362.0, "end": 4369.759999999999, "text": " papers, but it's a little difficult to find sometimes. So most neural nets use values as", "tokens": [50600, 10577, 11, 457, 309, 311, 257, 707, 2252, 281, 915, 2171, 13, 407, 881, 18161, 36170, 764, 4190, 382, 50988], "temperature": 0.0, "avg_logprob": -0.14647839433055812, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0004494049644563347}, {"id": 688, "seek": 435728, "start": 4369.759999999999, "end": 4375.28, "text": " the main nonlinearity, so this sort of half wave rectifier. Hyperbole tangent, which is a", "tokens": [50988, 264, 2135, 2107, 1889, 17409, 11, 370, 341, 1333, 295, 1922, 5772, 11048, 9902, 13, 29592, 1763, 306, 27747, 11, 597, 307, 257, 51264], "temperature": 0.0, "avg_logprob": -0.14647839433055812, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0004494049644563347}, {"id": 689, "seek": 435728, "start": 4375.28, "end": 4380.16, "text": " similar function, and logistic function, which is also a similar function, are used, but not as", "tokens": [51264, 2531, 2445, 11, 293, 3565, 3142, 2445, 11, 597, 307, 611, 257, 2531, 2445, 11, 366, 1143, 11, 457, 406, 382, 51508], "temperature": 0.0, "avg_logprob": -0.14647839433055812, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0004494049644563347}, {"id": 690, "seek": 435728, "start": 4380.16, "end": 4384.88, "text": " much, not nearly as much. You need to initialize the ways properly. So if you have a neural net", "tokens": [51508, 709, 11, 406, 6217, 382, 709, 13, 509, 643, 281, 5883, 1125, 264, 2098, 6108, 13, 407, 498, 291, 362, 257, 18161, 2533, 51744], "temperature": 0.0, "avg_logprob": -0.14647839433055812, "compression_ratio": 1.753787878787879, "no_speech_prob": 0.0004494049644563347}, {"id": 691, "seek": 438488, "start": 4384.88, "end": 4390.400000000001, "text": " and you initialize the ways to zero, it never takes off. It will never learn. The gradients", "tokens": [50364, 293, 291, 5883, 1125, 264, 2098, 281, 4018, 11, 309, 1128, 2516, 766, 13, 467, 486, 1128, 1466, 13, 440, 2771, 2448, 50640], "temperature": 0.0, "avg_logprob": -0.10542854349663917, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0002304821537109092}, {"id": 692, "seek": 438488, "start": 4390.400000000001, "end": 4397.2, "text": " will always be zero all the time. And the reason is because when you back propagate the gradient,", "tokens": [50640, 486, 1009, 312, 4018, 439, 264, 565, 13, 400, 264, 1778, 307, 570, 562, 291, 646, 48256, 264, 16235, 11, 50980], "temperature": 0.0, "avg_logprob": -0.10542854349663917, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0002304821537109092}, {"id": 693, "seek": 438488, "start": 4397.2, "end": 4401.6, "text": " you multiply by the transpose of the weight matrix. If that weight matrix is zero, your gradient is", "tokens": [50980, 291, 12972, 538, 264, 25167, 295, 264, 3364, 8141, 13, 759, 300, 3364, 8141, 307, 4018, 11, 428, 16235, 307, 51200], "temperature": 0.0, "avg_logprob": -0.10542854349663917, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0002304821537109092}, {"id": 694, "seek": 438488, "start": 4401.6, "end": 4407.36, "text": " zero. So if you start with all the weights equal to zero, you never take off. And someone asked", "tokens": [51200, 4018, 13, 407, 498, 291, 722, 365, 439, 264, 17443, 2681, 281, 4018, 11, 291, 1128, 747, 766, 13, 400, 1580, 2351, 51488], "temperature": 0.0, "avg_logprob": -0.10542854349663917, "compression_ratio": 1.7906976744186047, "no_speech_prob": 0.0002304821537109092}, {"id": 695, "seek": 440736, "start": 4407.36, "end": 4414.639999999999, "text": " the question about saddle points before. Zero is a saddle point. And so if you start at this", "tokens": [50364, 264, 1168, 466, 30459, 2793, 949, 13, 17182, 307, 257, 30459, 935, 13, 400, 370, 498, 291, 722, 412, 341, 50728], "temperature": 0.0, "avg_logprob": -0.08755827441657942, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0006164542282931507}, {"id": 696, "seek": 440736, "start": 4414.639999999999, "end": 4420.0, "text": " saddle point, you never get out of it. So you have to break the symmetry in the system. You have to", "tokens": [50728, 30459, 935, 11, 291, 1128, 483, 484, 295, 309, 13, 407, 291, 362, 281, 1821, 264, 25440, 294, 264, 1185, 13, 509, 362, 281, 50996], "temperature": 0.0, "avg_logprob": -0.08755827441657942, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0006164542282931507}, {"id": 697, "seek": 440736, "start": 4420.0, "end": 4426.799999999999, "text": " initialize the weights to small random values. They don't need to be random, but it works fine", "tokens": [50996, 5883, 1125, 264, 17443, 281, 1359, 4974, 4190, 13, 814, 500, 380, 643, 281, 312, 4974, 11, 457, 309, 1985, 2489, 51336], "temperature": 0.0, "avg_logprob": -0.08755827441657942, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0006164542282931507}, {"id": 698, "seek": 440736, "start": 4426.799999999999, "end": 4434.16, "text": " if they're random. And the way you initialize is actually quite important. So there's all kinds of", "tokens": [51336, 498, 436, 434, 4974, 13, 400, 264, 636, 291, 5883, 1125, 307, 767, 1596, 1021, 13, 407, 456, 311, 439, 3685, 295, 51704], "temperature": 0.0, "avg_logprob": -0.08755827441657942, "compression_ratio": 1.685589519650655, "no_speech_prob": 0.0006164542282931507}, {"id": 699, "seek": 443416, "start": 4434.16, "end": 4439.04, "text": " tricks to initialize things properly. One of the tricks was invented by my friend,", "tokens": [50364, 11733, 281, 5883, 1125, 721, 6108, 13, 1485, 295, 264, 11733, 390, 14479, 538, 452, 1277, 11, 50608], "temperature": 0.0, "avg_logprob": -0.18916006255568119, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0009394208318553865}, {"id": 700, "seek": 443416, "start": 4439.92, "end": 4445.76, "text": " about 30 years ago, even more than that, actually, 34 years ago, almost. Unfortunately, now it's", "tokens": [50652, 466, 2217, 924, 2057, 11, 754, 544, 813, 300, 11, 767, 11, 12790, 924, 2057, 11, 1920, 13, 8590, 11, 586, 309, 311, 50944], "temperature": 0.0, "avg_logprob": -0.18916006255568119, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0009394208318553865}, {"id": 701, "seek": 443416, "start": 4445.76, "end": 4451.68, "text": " called differently. It's called the kaming trick, but it's the same. And it consists in", "tokens": [50944, 1219, 7614, 13, 467, 311, 1219, 264, 350, 5184, 4282, 11, 457, 309, 311, 264, 912, 13, 400, 309, 14689, 294, 51240], "temperature": 0.0, "avg_logprob": -0.18916006255568119, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0009394208318553865}, {"id": 702, "seek": 443416, "start": 4452.24, "end": 4457.28, "text": " initializing the weights to random values in such a way that if a unit has many inputs, the weights", "tokens": [51268, 5883, 3319, 264, 17443, 281, 4974, 4190, 294, 1270, 257, 636, 300, 498, 257, 4985, 575, 867, 15743, 11, 264, 17443, 51520], "temperature": 0.0, "avg_logprob": -0.18916006255568119, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0009394208318553865}, {"id": 703, "seek": 443416, "start": 4457.28, "end": 4462.639999999999, "text": " are smaller than if it has few inputs. And the reason for this is that you want the weighted", "tokens": [51520, 366, 4356, 813, 498, 309, 575, 1326, 15743, 13, 400, 264, 1778, 337, 341, 307, 300, 291, 528, 264, 32807, 51788], "temperature": 0.0, "avg_logprob": -0.18916006255568119, "compression_ratio": 1.7037037037037037, "no_speech_prob": 0.0009394208318553865}, {"id": 704, "seek": 446264, "start": 4462.64, "end": 4469.68, "text": " sum to be roughly kind of have some reasonable value. If the input variables have some reasonable", "tokens": [50364, 2408, 281, 312, 9810, 733, 295, 362, 512, 10585, 2158, 13, 759, 264, 4846, 9102, 362, 512, 10585, 50716], "temperature": 0.0, "avg_logprob": -0.09289498951124109, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.00130316405557096}, {"id": 705, "seek": 446264, "start": 4469.68, "end": 4476.160000000001, "text": " value, let's say variance one or something like this, and you're computing a weighted sum of them,", "tokens": [50716, 2158, 11, 718, 311, 584, 21977, 472, 420, 746, 411, 341, 11, 293, 291, 434, 15866, 257, 32807, 2408, 295, 552, 11, 51040], "temperature": 0.0, "avg_logprob": -0.09289498951124109, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.00130316405557096}, {"id": 706, "seek": 446264, "start": 4476.160000000001, "end": 4481.84, "text": " the weighted sum, the size of the weighted sum is going to grow like the square root of the number", "tokens": [51040, 264, 32807, 2408, 11, 264, 2744, 295, 264, 32807, 2408, 307, 516, 281, 1852, 411, 264, 3732, 5593, 295, 264, 1230, 51324], "temperature": 0.0, "avg_logprob": -0.09289498951124109, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.00130316405557096}, {"id": 707, "seek": 446264, "start": 4481.84, "end": 4486.88, "text": " of inputs. And so you want to set the weights to something like the inverse square root if you want", "tokens": [51324, 295, 15743, 13, 400, 370, 291, 528, 281, 992, 264, 17443, 281, 746, 411, 264, 17340, 3732, 5593, 498, 291, 528, 51576], "temperature": 0.0, "avg_logprob": -0.09289498951124109, "compression_ratio": 1.9268292682926829, "no_speech_prob": 0.00130316405557096}, {"id": 708, "seek": 448688, "start": 4486.88, "end": 4494.16, "text": " the weighted sum to be kind of about the same size as each of the inputs. So that's built into", "tokens": [50364, 264, 32807, 2408, 281, 312, 733, 295, 466, 264, 912, 2744, 382, 1184, 295, 264, 15743, 13, 407, 300, 311, 3094, 666, 50728], "temperature": 0.0, "avg_logprob": -0.18295671355049564, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0014083910500630736}, {"id": 709, "seek": 448688, "start": 4494.16, "end": 4500.16, "text": " PyTorch. You can call this, you know, initialization procedure. What's the exact name of it? I can't", "tokens": [50728, 9953, 51, 284, 339, 13, 509, 393, 818, 341, 11, 291, 458, 11, 5883, 2144, 10747, 13, 708, 311, 264, 1900, 1315, 295, 309, 30, 286, 393, 380, 51028], "temperature": 0.0, "avg_logprob": -0.18295671355049564, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0014083910500630736}, {"id": 710, "seek": 448688, "start": 4500.16, "end": 4506.16, "text": " remember. The one that is coming, coming, coming here, then there is the Xavier and then there is", "tokens": [51028, 1604, 13, 440, 472, 300, 307, 1348, 11, 1348, 11, 1348, 510, 11, 550, 456, 307, 264, 44653, 293, 550, 456, 307, 51328], "temperature": 0.0, "avg_logprob": -0.18295671355049564, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0014083910500630736}, {"id": 711, "seek": 448688, "start": 4506.16, "end": 4510.8, "text": " also yours we have in PyTorch. Yeah, they're slightly different, but they kind of do the same", "tokens": [51328, 611, 6342, 321, 362, 294, 9953, 51, 284, 339, 13, 865, 11, 436, 434, 4748, 819, 11, 457, 436, 733, 295, 360, 264, 912, 51560], "temperature": 0.0, "avg_logprob": -0.18295671355049564, "compression_ratio": 1.6260504201680672, "no_speech_prob": 0.0014083910500630736}, {"id": 712, "seek": 451080, "start": 4510.8, "end": 4519.12, "text": " more or less. Yeah, the Xavier Glow version, yeah. Yeah, this one divides by the Fennin and Fennin.", "tokens": [50364, 544, 420, 1570, 13, 865, 11, 264, 44653, 5209, 305, 3037, 11, 1338, 13, 865, 11, 341, 472, 41347, 538, 264, 479, 1857, 259, 293, 479, 1857, 259, 13, 50780], "temperature": 0.0, "avg_logprob": -0.17369540375058012, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.001597004127688706}, {"id": 713, "seek": 451080, "start": 4520.88, "end": 4525.52, "text": " There's various loss functions, so I haven't talked yet about what the cross-entropy loss is,", "tokens": [50868, 821, 311, 3683, 4470, 6828, 11, 370, 286, 2378, 380, 2825, 1939, 466, 437, 264, 3278, 12, 317, 27514, 4470, 307, 11, 51100], "temperature": 0.0, "avg_logprob": -0.17369540375058012, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.001597004127688706}, {"id": 714, "seek": 451080, "start": 4525.52, "end": 4530.16, "text": " but cross-entropy loss is a particular cost that's used for classification. I'll probably", "tokens": [51100, 457, 3278, 12, 317, 27514, 4470, 307, 257, 1729, 2063, 300, 311, 1143, 337, 21538, 13, 286, 603, 1391, 51332], "temperature": 0.0, "avg_logprob": -0.17369540375058012, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.001597004127688706}, {"id": 715, "seek": 451080, "start": 4530.16, "end": 4535.6, "text": " talk about this next week and I'll have some time at the end of this lecture. This is for", "tokens": [51332, 751, 466, 341, 958, 1243, 293, 286, 603, 362, 512, 565, 412, 264, 917, 295, 341, 7991, 13, 639, 307, 337, 51604], "temperature": 0.0, "avg_logprob": -0.17369540375058012, "compression_ratio": 1.6359649122807018, "no_speech_prob": 0.001597004127688706}, {"id": 716, "seek": 453560, "start": 4535.6, "end": 4541.76, "text": " classification. As I said, we use stochastic gradient descent on mini-batches and mini-batches", "tokens": [50364, 21538, 13, 1018, 286, 848, 11, 321, 764, 342, 8997, 2750, 16235, 23475, 322, 8382, 12, 65, 852, 279, 293, 8382, 12, 65, 852, 279, 50672], "temperature": 0.0, "avg_logprob": -0.10548326617381612, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.008706389926373959}, {"id": 717, "seek": 453560, "start": 4541.76, "end": 4546.56, "text": " only because the hardware that we have needs mini-batches to perform properly. If we had", "tokens": [50672, 787, 570, 264, 8837, 300, 321, 362, 2203, 8382, 12, 65, 852, 279, 281, 2042, 6108, 13, 759, 321, 632, 50912], "temperature": 0.0, "avg_logprob": -0.10548326617381612, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.008706389926373959}, {"id": 718, "seek": 453560, "start": 4546.56, "end": 4552.160000000001, "text": " different hardware, we would use mini-batch size one. As I said before, we need to shuffle the", "tokens": [50912, 819, 8837, 11, 321, 576, 764, 8382, 12, 65, 852, 2744, 472, 13, 1018, 286, 848, 949, 11, 321, 643, 281, 39426, 264, 51192], "temperature": 0.0, "avg_logprob": -0.10548326617381612, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.008706389926373959}, {"id": 719, "seek": 453560, "start": 4552.160000000001, "end": 4558.96, "text": " training samples. So if someone gives you a training set and puts all the examples of category one,", "tokens": [51192, 3097, 10938, 13, 407, 498, 1580, 2709, 291, 257, 3097, 992, 293, 8137, 439, 264, 5110, 295, 7719, 472, 11, 51532], "temperature": 0.0, "avg_logprob": -0.10548326617381612, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.008706389926373959}, {"id": 720, "seek": 453560, "start": 4558.96, "end": 4564.08, "text": " then all the example category two, all the example category three, etc. If you use stochastic gradient", "tokens": [51532, 550, 439, 264, 1365, 7719, 732, 11, 439, 264, 1365, 7719, 1045, 11, 5183, 13, 759, 291, 764, 342, 8997, 2750, 16235, 51788], "temperature": 0.0, "avg_logprob": -0.10548326617381612, "compression_ratio": 1.9552845528455285, "no_speech_prob": 0.008706389926373959}, {"id": 721, "seek": 456408, "start": 4564.08, "end": 4569.76, "text": " by keeping this order, it is not going to work. You have to shuffle the samples so that", "tokens": [50364, 538, 5145, 341, 1668, 11, 309, 307, 406, 516, 281, 589, 13, 509, 362, 281, 39426, 264, 10938, 370, 300, 50648], "temperature": 0.0, "avg_logprob": -0.1232357993721962, "compression_ratio": 1.4382022471910112, "no_speech_prob": 0.0021784529089927673}, {"id": 722, "seek": 456408, "start": 4571.5199999999995, "end": 4577.36, "text": " you basically get samples from all the categories within kind of a small subset, if you want.", "tokens": [50736, 291, 1936, 483, 10938, 490, 439, 264, 10479, 1951, 733, 295, 257, 1359, 25993, 11, 498, 291, 528, 13, 51028], "temperature": 0.0, "avg_logprob": -0.1232357993721962, "compression_ratio": 1.4382022471910112, "no_speech_prob": 0.0021784529089927673}, {"id": 723, "seek": 456408, "start": 4579.68, "end": 4584.16, "text": " There is an objection here for the stochastic gradient. Isn't Adam better?", "tokens": [51144, 821, 307, 364, 35756, 510, 337, 264, 342, 8997, 2750, 16235, 13, 6998, 380, 7938, 1101, 30, 51368], "temperature": 0.0, "avg_logprob": -0.1232357993721962, "compression_ratio": 1.4382022471910112, "no_speech_prob": 0.0021784529089927673}, {"id": 724, "seek": 458416, "start": 4584.88, "end": 4595.92, "text": " All right. Okay. There is a lot of variants of stochastic gradient. There are all stochastic", "tokens": [50400, 1057, 558, 13, 1033, 13, 821, 307, 257, 688, 295, 21669, 295, 342, 8997, 2750, 16235, 13, 821, 366, 439, 342, 8997, 2750, 50952], "temperature": 0.0, "avg_logprob": -0.15858541835438122, "compression_ratio": 1.7245508982035929, "no_speech_prob": 0.0023950159084051847}, {"id": 725, "seek": 458416, "start": 4595.92, "end": 4602.24, "text": " gradient methods. In fact, people in optimization said this should not be called stochastic gradient", "tokens": [50952, 16235, 7150, 13, 682, 1186, 11, 561, 294, 19618, 848, 341, 820, 406, 312, 1219, 342, 8997, 2750, 16235, 51268], "temperature": 0.0, "avg_logprob": -0.15858541835438122, "compression_ratio": 1.7245508982035929, "no_speech_prob": 0.0023950159084051847}, {"id": 726, "seek": 458416, "start": 4602.24, "end": 4607.36, "text": " descent because it's not a descent algorithm because stochastic gradient sometimes goes uphill", "tokens": [51268, 23475, 570, 309, 311, 406, 257, 23475, 9284, 570, 342, 8997, 2750, 16235, 2171, 1709, 39132, 51524], "temperature": 0.0, "avg_logprob": -0.15858541835438122, "compression_ratio": 1.7245508982035929, "no_speech_prob": 0.0023950159084051847}, {"id": 727, "seek": 460736, "start": 4607.36, "end": 4615.2, "text": " because of the noise. So people who want to really kind of be correct about this say it's", "tokens": [50364, 570, 295, 264, 5658, 13, 407, 561, 567, 528, 281, 534, 733, 295, 312, 3006, 466, 341, 584, 309, 311, 50756], "temperature": 0.0, "avg_logprob": -0.1496331041509455, "compression_ratio": 1.8442211055276383, "no_speech_prob": 0.003536225762218237}, {"id": 728, "seek": 460736, "start": 4615.2, "end": 4618.4, "text": " stochastic gradient optimization, but not stochastic gradient descent. That's the first thing.", "tokens": [50756, 342, 8997, 2750, 16235, 19618, 11, 457, 406, 342, 8997, 2750, 16235, 23475, 13, 663, 311, 264, 700, 551, 13, 50916], "temperature": 0.0, "avg_logprob": -0.1496331041509455, "compression_ratio": 1.8442211055276383, "no_speech_prob": 0.003536225762218237}, {"id": 729, "seek": 460736, "start": 4619.5199999999995, "end": 4625.28, "text": " Stochastic gradient optimization or stochastic gradient descent, SGD, is a special case of gradient", "tokens": [50972, 745, 8997, 2750, 16235, 19618, 420, 342, 8997, 2750, 16235, 23475, 11, 34520, 35, 11, 307, 257, 2121, 1389, 295, 16235, 51260], "temperature": 0.0, "avg_logprob": -0.1496331041509455, "compression_ratio": 1.8442211055276383, "no_speech_prob": 0.003536225762218237}, {"id": 730, "seek": 460736, "start": 4625.28, "end": 4633.28, "text": " based optimization. The specification of it says you have to have a step size eta,", "tokens": [51260, 2361, 19618, 13, 440, 31256, 295, 309, 1619, 291, 362, 281, 362, 257, 1823, 2744, 32415, 11, 51660], "temperature": 0.0, "avg_logprob": -0.1496331041509455, "compression_ratio": 1.8442211055276383, "no_speech_prob": 0.003536225762218237}, {"id": 731, "seek": 463328, "start": 4634.0, "end": 4638.719999999999, "text": " but nobody tells you how you set this step size eta and nobody tells you that this step size is", "tokens": [50400, 457, 5079, 5112, 291, 577, 291, 992, 341, 1823, 2744, 32415, 293, 5079, 5112, 291, 300, 341, 1823, 2744, 307, 50636], "temperature": 0.0, "avg_logprob": -0.10788655835528706, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0005032226908951998}, {"id": 732, "seek": 463328, "start": 4638.719999999999, "end": 4646.32, "text": " a scalar or a diagonal matrix or a full matrix. Okay. So there are variations of SGD in which", "tokens": [50636, 257, 39684, 420, 257, 21539, 8141, 420, 257, 1577, 8141, 13, 1033, 13, 407, 456, 366, 17840, 295, 34520, 35, 294, 597, 51016], "temperature": 0.0, "avg_logprob": -0.10788655835528706, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0005032226908951998}, {"id": 733, "seek": 463328, "start": 4647.44, "end": 4651.36, "text": " eta is changed all the time for every sample or every batch.", "tokens": [51072, 32415, 307, 3105, 439, 264, 565, 337, 633, 6889, 420, 633, 15245, 13, 51268], "temperature": 0.0, "avg_logprob": -0.10788655835528706, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0005032226908951998}, {"id": 734, "seek": 463328, "start": 4654.719999999999, "end": 4659.28, "text": " In SGD, most of the time this eta is decreased according to a schedule and there are a bunch", "tokens": [51436, 682, 34520, 35, 11, 881, 295, 264, 565, 341, 32415, 307, 24436, 4650, 281, 257, 7567, 293, 456, 366, 257, 3840, 51664], "temperature": 0.0, "avg_logprob": -0.10788655835528706, "compression_ratio": 1.6813725490196079, "no_speech_prob": 0.0005032226908951998}, {"id": 735, "seek": 465928, "start": 4659.28, "end": 4668.0, "text": " of standard schedule in PyTorch that are implemented. In techniques like Adam, the eta is actually a", "tokens": [50364, 295, 3832, 7567, 294, 9953, 51, 284, 339, 300, 366, 12270, 13, 682, 7512, 411, 7938, 11, 264, 32415, 307, 767, 257, 50800], "temperature": 0.0, "avg_logprob": -0.131785087978717, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.000755146611481905}, {"id": 736, "seek": 465928, "start": 4668.0, "end": 4672.16, "text": " diagonal matrix and that diagonal matrix, the term in the diagonal matrix are changed all the", "tokens": [50800, 21539, 8141, 293, 300, 21539, 8141, 11, 264, 1433, 294, 264, 21539, 8141, 366, 3105, 439, 264, 51008], "temperature": 0.0, "avg_logprob": -0.131785087978717, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.000755146611481905}, {"id": 737, "seek": 465928, "start": 4672.16, "end": 4678.639999999999, "text": " time. They're computed based on some estimate of the curvature of the cost function. There's a lot", "tokens": [51008, 565, 13, 814, 434, 40610, 2361, 322, 512, 12539, 295, 264, 37638, 295, 264, 2063, 2445, 13, 821, 311, 257, 688, 51332], "temperature": 0.0, "avg_logprob": -0.131785087978717, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.000755146611481905}, {"id": 738, "seek": 465928, "start": 4678.639999999999, "end": 4686.719999999999, "text": " of methods to do this. Okay. They're all SGD type methods. Okay. Adam is an SGD method with a special", "tokens": [51332, 295, 7150, 281, 360, 341, 13, 1033, 13, 814, 434, 439, 34520, 35, 2010, 7150, 13, 1033, 13, 7938, 307, 364, 34520, 35, 3170, 365, 257, 2121, 51736], "temperature": 0.0, "avg_logprob": -0.131785087978717, "compression_ratio": 1.6808510638297873, "no_speech_prob": 0.000755146611481905}, {"id": 739, "seek": 468672, "start": 4686.72, "end": 4696.320000000001, "text": " type of eta. So yeah, in the opt-in package in Torch, there's a whole bunch of those methods.", "tokens": [50364, 2010, 295, 32415, 13, 407, 1338, 11, 294, 264, 2427, 12, 259, 7372, 294, 7160, 339, 11, 456, 311, 257, 1379, 3840, 295, 729, 7150, 13, 50844], "temperature": 0.0, "avg_logprob": -0.1672726821899414, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.00046517656301148236}, {"id": 740, "seek": 468672, "start": 4699.04, "end": 4703.280000000001, "text": " There's going to be a whole lecture on this, so don't worry about it, about optimization.", "tokens": [50980, 821, 311, 516, 281, 312, 257, 1379, 7991, 322, 341, 11, 370, 500, 380, 3292, 466, 309, 11, 466, 19618, 13, 51192], "temperature": 0.0, "avg_logprob": -0.1672726821899414, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.00046517656301148236}, {"id": 741, "seek": 468672, "start": 4705.52, "end": 4711.92, "text": " Normalize the input variables to zero mean and unit variance. So this is a very important point that", "tokens": [51304, 21277, 1125, 264, 4846, 9102, 281, 4018, 914, 293, 4985, 21977, 13, 407, 341, 307, 257, 588, 1021, 935, 300, 51624], "temperature": 0.0, "avg_logprob": -0.1672726821899414, "compression_ratio": 1.5106382978723405, "no_speech_prob": 0.00046517656301148236}, {"id": 742, "seek": 471192, "start": 4712.56, "end": 4720.24, "text": " this type of optimization method, gradient based optimization methods, when you have weighted", "tokens": [50396, 341, 2010, 295, 19618, 3170, 11, 16235, 2361, 19618, 7150, 11, 562, 291, 362, 32807, 50780], "temperature": 0.0, "avg_logprob": -0.14595764952820617, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0001940874644787982}, {"id": 743, "seek": 471192, "start": 4720.24, "end": 4726.4, "text": " sounds, kind of linear operations, tends to be very sensitive to how the data is prepared.", "tokens": [50780, 3263, 11, 733, 295, 8213, 7705, 11, 12258, 281, 312, 588, 9477, 281, 577, 264, 1412, 307, 4927, 13, 51088], "temperature": 0.0, "avg_logprob": -0.14595764952820617, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0001940874644787982}, {"id": 744, "seek": 471192, "start": 4727.4400000000005, "end": 4730.4, "text": " So if you have two variables that have very widely different", "tokens": [51140, 407, 498, 291, 362, 732, 9102, 300, 362, 588, 13371, 819, 51288], "temperature": 0.0, "avg_logprob": -0.14595764952820617, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0001940874644787982}, {"id": 745, "seek": 471192, "start": 4731.36, "end": 4736.4800000000005, "text": " variances, one of them varies between, let's say, minus one and plus one. The other one", "tokens": [51336, 1374, 21518, 11, 472, 295, 552, 21716, 1296, 11, 718, 311, 584, 11, 3175, 472, 293, 1804, 472, 13, 440, 661, 472, 51592], "temperature": 0.0, "avg_logprob": -0.14595764952820617, "compression_ratio": 1.6323529411764706, "no_speech_prob": 0.0001940874644787982}, {"id": 746, "seek": 473648, "start": 4736.48, "end": 4742.879999999999, "text": " varies between minus 100 and plus 100. The system will basically not pay attention to the one that", "tokens": [50364, 21716, 1296, 3175, 2319, 293, 1804, 2319, 13, 440, 1185, 486, 1936, 406, 1689, 3202, 281, 264, 472, 300, 50684], "temperature": 0.0, "avg_logprob": -0.08554509345521318, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0008556564571335912}, {"id": 747, "seek": 473648, "start": 4742.879999999999, "end": 4748.24, "text": " varies between plus one and minus one. We'll only pay attention to the big one. And this may be good", "tokens": [50684, 21716, 1296, 1804, 472, 293, 3175, 472, 13, 492, 603, 787, 1689, 3202, 281, 264, 955, 472, 13, 400, 341, 815, 312, 665, 50952], "temperature": 0.0, "avg_logprob": -0.08554509345521318, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0008556564571335912}, {"id": 748, "seek": 473648, "start": 4748.24, "end": 4753.36, "text": " or this may be bad. Furthermore, the learning rate you're going to have to use the eta parameter,", "tokens": [50952, 420, 341, 815, 312, 1578, 13, 23999, 11, 264, 2539, 3314, 291, 434, 516, 281, 362, 281, 764, 264, 32415, 13075, 11, 51208], "temperature": 0.0, "avg_logprob": -0.08554509345521318, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0008556564571335912}, {"id": 749, "seek": 473648, "start": 4753.36, "end": 4759.12, "text": " the step size, is going to have to be set to a relatively small value to prevent the weights", "tokens": [51208, 264, 1823, 2744, 11, 307, 516, 281, 362, 281, 312, 992, 281, 257, 7226, 1359, 2158, 281, 4871, 264, 17443, 51496], "temperature": 0.0, "avg_logprob": -0.08554509345521318, "compression_ratio": 1.7410714285714286, "no_speech_prob": 0.0008556564571335912}, {"id": 750, "seek": 475912, "start": 4759.12, "end": 4767.5199999999995, "text": " that look at this highly variable input from diverging. The gradients are going to be very", "tokens": [50364, 300, 574, 412, 341, 5405, 7006, 4846, 490, 18558, 3249, 13, 440, 2771, 2448, 366, 516, 281, 312, 588, 50784], "temperature": 0.0, "avg_logprob": -0.07338750627305772, "compression_ratio": 1.778301886792453, "no_speech_prob": 0.00033006613375619054}, {"id": 751, "seek": 475912, "start": 4767.5199999999995, "end": 4771.68, "text": " large because the gradients basically are proportional to the size of the input or even to", "tokens": [50784, 2416, 570, 264, 2771, 2448, 1936, 366, 24969, 281, 264, 2744, 295, 264, 4846, 420, 754, 281, 50992], "temperature": 0.0, "avg_logprob": -0.07338750627305772, "compression_ratio": 1.778301886792453, "no_speech_prob": 0.00033006613375619054}, {"id": 752, "seek": 475912, "start": 4771.68, "end": 4776.0, "text": " the variance of the input. So if you don't want your system to diverge, you're going to have to", "tokens": [50992, 264, 21977, 295, 264, 4846, 13, 407, 498, 291, 500, 380, 528, 428, 1185, 281, 18558, 432, 11, 291, 434, 516, 281, 362, 281, 51208], "temperature": 0.0, "avg_logprob": -0.07338750627305772, "compression_ratio": 1.778301886792453, "no_speech_prob": 0.00033006613375619054}, {"id": 753, "seek": 475912, "start": 4776.0, "end": 4783.5199999999995, "text": " tune down the learning rate if the input variance is large. If the input variables are all shifted,", "tokens": [51208, 10864, 760, 264, 2539, 3314, 498, 264, 4846, 21977, 307, 2416, 13, 759, 264, 4846, 9102, 366, 439, 18892, 11, 51584], "temperature": 0.0, "avg_logprob": -0.07338750627305772, "compression_ratio": 1.778301886792453, "no_speech_prob": 0.00033006613375619054}, {"id": 754, "seek": 478352, "start": 4783.6, "end": 4791.040000000001, "text": " they're all between, let's say, 99 and 101 instead of minus one and one. Then again, it's very", "tokens": [50368, 436, 434, 439, 1296, 11, 718, 311, 584, 11, 11803, 293, 21055, 2602, 295, 3175, 472, 293, 472, 13, 1396, 797, 11, 309, 311, 588, 50740], "temperature": 0.0, "avg_logprob": -0.10007836137499128, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0007553016184829175}, {"id": 755, "seek": 478352, "start": 4791.040000000001, "end": 4798.4800000000005, "text": " difficult for a gradient-based algorithm that use weighted sums to figure out those things.", "tokens": [50740, 2252, 337, 257, 16235, 12, 6032, 9284, 300, 764, 32807, 34499, 281, 2573, 484, 729, 721, 13, 51112], "temperature": 0.0, "avg_logprob": -0.10007836137499128, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0007553016184829175}, {"id": 756, "seek": 478352, "start": 4798.4800000000005, "end": 4803.76, "text": " Again, I'll talk about this more formally later. Right now, just remember the trick", "tokens": [51112, 3764, 11, 286, 603, 751, 466, 341, 544, 25983, 1780, 13, 1779, 586, 11, 445, 1604, 264, 4282, 51376], "temperature": 0.0, "avg_logprob": -0.10007836137499128, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0007553016184829175}, {"id": 757, "seek": 478352, "start": 4803.76, "end": 4807.84, "text": " that you need to normalize your input. So basically, take every variable of your input,", "tokens": [51376, 300, 291, 643, 281, 2710, 1125, 428, 4846, 13, 407, 1936, 11, 747, 633, 7006, 295, 428, 4846, 11, 51580], "temperature": 0.0, "avg_logprob": -0.10007836137499128, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0007553016184829175}, {"id": 758, "seek": 478352, "start": 4807.84, "end": 4812.320000000001, "text": " subtract the mean, you compute the mean over the training set of each variable. So let's say your", "tokens": [51580, 16390, 264, 914, 11, 291, 14722, 264, 914, 670, 264, 3097, 992, 295, 1184, 7006, 13, 407, 718, 311, 584, 428, 51804], "temperature": 0.0, "avg_logprob": -0.10007836137499128, "compression_ratio": 1.6344086021505377, "no_speech_prob": 0.0007553016184829175}, {"id": 759, "seek": 481232, "start": 4812.32, "end": 4818.88, "text": " training set is a set of images. The images are, let's say, 100 by 100 pixels. Let's say they're", "tokens": [50364, 3097, 992, 307, 257, 992, 295, 5267, 13, 440, 5267, 366, 11, 718, 311, 584, 11, 2319, 538, 2319, 18668, 13, 961, 311, 584, 436, 434, 50692], "temperature": 0.0, "avg_logprob": -0.10241598401750837, "compression_ratio": 1.8544600938967135, "no_speech_prob": 0.00041724141919985414}, {"id": 760, "seek": 481232, "start": 4818.88, "end": 4823.759999999999, "text": " grayscale, so you get 10,000 variables. And let's say you get a million samples, right? You're going", "tokens": [50692, 677, 3772, 37088, 11, 370, 291, 483, 1266, 11, 1360, 9102, 13, 400, 718, 311, 584, 291, 483, 257, 2459, 10938, 11, 558, 30, 509, 434, 516, 50936], "temperature": 0.0, "avg_logprob": -0.10241598401750837, "compression_ratio": 1.8544600938967135, "no_speech_prob": 0.00041724141919985414}, {"id": 761, "seek": 481232, "start": 4823.759999999999, "end": 4832.08, "text": " to take each of those 10,000 variables, compute the mean of it over the training set, compute the", "tokens": [50936, 281, 747, 1184, 295, 729, 1266, 11, 1360, 9102, 11, 14722, 264, 914, 295, 309, 670, 264, 3097, 992, 11, 14722, 264, 51352], "temperature": 0.0, "avg_logprob": -0.10241598401750837, "compression_ratio": 1.8544600938967135, "no_speech_prob": 0.00041724141919985414}, {"id": 762, "seek": 481232, "start": 4832.08, "end": 4838.0, "text": " standard deviation of it over the entire training set. And the samples you're going to show to your", "tokens": [51352, 3832, 25163, 295, 309, 670, 264, 2302, 3097, 992, 13, 400, 264, 10938, 291, 434, 516, 281, 855, 281, 428, 51648], "temperature": 0.0, "avg_logprob": -0.10241598401750837, "compression_ratio": 1.8544600938967135, "no_speech_prob": 0.00041724141919985414}, {"id": 763, "seek": 483800, "start": 4838.0, "end": 4846.32, "text": " system are going to be a sample where you have subtracted the mean from each of the 10,000 pixels", "tokens": [50364, 1185, 366, 516, 281, 312, 257, 6889, 689, 291, 362, 16390, 292, 264, 914, 490, 1184, 295, 264, 1266, 11, 1360, 18668, 50780], "temperature": 0.0, "avg_logprob": -0.09477059657757099, "compression_ratio": 1.569377990430622, "no_speech_prob": 0.00020659896836150438}, {"id": 764, "seek": 483800, "start": 4846.32, "end": 4854.0, "text": " and divided the resulting values by the standard deviation that you computed.", "tokens": [50780, 293, 6666, 264, 16505, 4190, 538, 264, 3832, 25163, 300, 291, 40610, 13, 51164], "temperature": 0.0, "avg_logprob": -0.09477059657757099, "compression_ratio": 1.569377990430622, "no_speech_prob": 0.00020659896836150438}, {"id": 765, "seek": 483800, "start": 4855.36, "end": 4858.72, "text": " Okay? So now what you have is a bunch of variables that are all zero mean", "tokens": [51232, 1033, 30, 407, 586, 437, 291, 362, 307, 257, 3840, 295, 9102, 300, 366, 439, 4018, 914, 51400], "temperature": 0.0, "avg_logprob": -0.09477059657757099, "compression_ratio": 1.569377990430622, "no_speech_prob": 0.00020659896836150438}, {"id": 766, "seek": 483800, "start": 4859.52, "end": 4865.68, "text": " and all standard deviation equal to one. And that makes your neural net happy.", "tokens": [51440, 293, 439, 3832, 25163, 2681, 281, 472, 13, 400, 300, 1669, 428, 18161, 2533, 2055, 13, 51748], "temperature": 0.0, "avg_logprob": -0.09477059657757099, "compression_ratio": 1.569377990430622, "no_speech_prob": 0.00020659896836150438}, {"id": 767, "seek": 486568, "start": 4866.08, "end": 4868.88, "text": " That makes your optimization algorithm happy, actually.", "tokens": [50384, 663, 1669, 428, 19618, 9284, 2055, 11, 767, 13, 50524], "temperature": 0.0, "avg_logprob": -0.23397971771575593, "compression_ratio": 1.7041666666666666, "no_speech_prob": 0.0009961636969819665}, {"id": 768, "seek": 486568, "start": 4868.88, "end": 4876.0, "text": " We have actually a question. So you keep repeating SGD type methods, gradient based methods,", "tokens": [50524, 492, 362, 767, 257, 1168, 13, 407, 291, 1066, 18617, 34520, 35, 2010, 7150, 11, 16235, 2361, 7150, 11, 50880], "temperature": 0.0, "avg_logprob": -0.23397971771575593, "compression_ratio": 1.7041666666666666, "no_speech_prob": 0.0009961636969819665}, {"id": 769, "seek": 486568, "start": 4876.0, "end": 4882.16, "text": " because there are other types of methods. Yes. Okay. So there is gradient free methods.", "tokens": [50880, 570, 456, 366, 661, 3467, 295, 7150, 13, 1079, 13, 1033, 13, 407, 456, 307, 16235, 1737, 7150, 13, 51188], "temperature": 0.0, "avg_logprob": -0.23397971771575593, "compression_ratio": 1.7041666666666666, "no_speech_prob": 0.0009961636969819665}, {"id": 770, "seek": 486568, "start": 4882.16, "end": 4889.04, "text": " So gradient free method is a method where you do not assume that the function you're trying to", "tokens": [51188, 407, 16235, 1737, 3170, 307, 257, 3170, 689, 291, 360, 406, 6552, 300, 264, 2445, 291, 434, 1382, 281, 51532], "temperature": 0.0, "avg_logprob": -0.23397971771575593, "compression_ratio": 1.7041666666666666, "no_speech_prob": 0.0009961636969819665}, {"id": 771, "seek": 486568, "start": 4889.04, "end": 4893.92, "text": " optimize is differentiable or even continuous with respect to the parameters.", "tokens": [51532, 19719, 307, 819, 9364, 420, 754, 10957, 365, 3104, 281, 264, 9834, 13, 51776], "temperature": 0.0, "avg_logprob": -0.23397971771575593, "compression_ratio": 1.7041666666666666, "no_speech_prob": 0.0009961636969819665}, {"id": 772, "seek": 489392, "start": 4894.64, "end": 4902.8, "text": " For several reasons, perhaps it's a function that looks like a golf course, right? It's flat and", "tokens": [50400, 1171, 2940, 4112, 11, 4317, 309, 311, 257, 2445, 300, 1542, 411, 257, 12880, 1164, 11, 558, 30, 467, 311, 4962, 293, 50808], "temperature": 0.0, "avg_logprob": -0.13527603987809067, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00024151500838343054}, {"id": 773, "seek": 489392, "start": 4902.8, "end": 4907.36, "text": " then maybe it's got steps and, you know, it's difficult to, like the local gradient information", "tokens": [50808, 550, 1310, 309, 311, 658, 4439, 293, 11, 291, 458, 11, 309, 311, 2252, 281, 11, 411, 264, 2654, 16235, 1589, 51036], "temperature": 0.0, "avg_logprob": -0.13527603987809067, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00024151500838343054}, {"id": 774, "seek": 489392, "start": 4907.36, "end": 4912.08, "text": " does not give you any information as to where you should go to find the minimum. Okay?", "tokens": [51036, 775, 406, 976, 291, 604, 1589, 382, 281, 689, 291, 820, 352, 281, 915, 264, 7285, 13, 1033, 30, 51272], "temperature": 0.0, "avg_logprob": -0.13527603987809067, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00024151500838343054}, {"id": 775, "seek": 489392, "start": 4913.52, "end": 4920.08, "text": " It could be that the function is essentially discrete, right? It's not a function of continuous", "tokens": [51344, 467, 727, 312, 300, 264, 2445, 307, 4476, 27706, 11, 558, 30, 467, 311, 406, 257, 2445, 295, 10957, 51672], "temperature": 0.0, "avg_logprob": -0.13527603987809067, "compression_ratio": 1.6592920353982301, "no_speech_prob": 0.00024151500838343054}, {"id": 776, "seek": 492008, "start": 4920.08, "end": 4926.96, "text": " variables, function of discrete variables. So for example, am I going to win this chess game?", "tokens": [50364, 9102, 11, 2445, 295, 27706, 9102, 13, 407, 337, 1365, 11, 669, 286, 516, 281, 1942, 341, 24122, 1216, 30, 50708], "temperature": 0.0, "avg_logprob": -0.11642429266083107, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0005702388007193804}, {"id": 777, "seek": 492008, "start": 4928.24, "end": 4931.76, "text": " The variable you can manipulate is the position on the board. That's a discrete variable.", "tokens": [50772, 440, 7006, 291, 393, 20459, 307, 264, 2535, 322, 264, 3150, 13, 663, 311, 257, 27706, 7006, 13, 50948], "temperature": 0.0, "avg_logprob": -0.11642429266083107, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0005702388007193804}, {"id": 778, "seek": 492008, "start": 4933.28, "end": 4940.16, "text": " So you can't, you can compute a gradient of, you know, a score with respect to a position on", "tokens": [51024, 407, 291, 393, 380, 11, 291, 393, 14722, 257, 16235, 295, 11, 291, 458, 11, 257, 6175, 365, 3104, 281, 257, 2535, 322, 51368], "temperature": 0.0, "avg_logprob": -0.11642429266083107, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0005702388007193804}, {"id": 779, "seek": 492008, "start": 4940.16, "end": 4949.12, "text": " the chess game. It's a discrete variable. Another example is the cost function is not", "tokens": [51368, 264, 24122, 1216, 13, 467, 311, 257, 27706, 7006, 13, 3996, 1365, 307, 264, 2063, 2445, 307, 406, 51816], "temperature": 0.0, "avg_logprob": -0.11642429266083107, "compression_ratio": 1.846938775510204, "no_speech_prob": 0.0005702388007193804}, {"id": 780, "seek": 494912, "start": 4949.12, "end": 4953.36, "text": " something you can compute. You don't actually know the cost function. Okay? So for example,", "tokens": [50364, 746, 291, 393, 14722, 13, 509, 500, 380, 767, 458, 264, 2063, 2445, 13, 1033, 30, 407, 337, 1365, 11, 50576], "temperature": 0.0, "avg_logprob": -0.10073241591453552, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.00031997496262192726}, {"id": 781, "seek": 494912, "start": 4954.72, "end": 4959.28, "text": " the only thing you can do is give an input to the cost function and it tells you the cost.", "tokens": [50644, 264, 787, 551, 291, 393, 360, 307, 976, 364, 4846, 281, 264, 2063, 2445, 293, 309, 5112, 291, 264, 2063, 13, 50872], "temperature": 0.0, "avg_logprob": -0.10073241591453552, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.00031997496262192726}, {"id": 782, "seek": 494912, "start": 4959.92, "end": 4963.599999999999, "text": " But you can't, you don't know the function. It's not, right? It's not a program on a computer.", "tokens": [50904, 583, 291, 393, 380, 11, 291, 500, 380, 458, 264, 2445, 13, 467, 311, 406, 11, 558, 30, 467, 311, 406, 257, 1461, 322, 257, 3820, 13, 51088], "temperature": 0.0, "avg_logprob": -0.10073241591453552, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.00031997496262192726}, {"id": 783, "seek": 494912, "start": 4963.599999999999, "end": 4968.72, "text": " You can't backprop a gradient to it. A good example of this is the real world. The real world,", "tokens": [51088, 509, 393, 380, 646, 79, 1513, 257, 16235, 281, 309, 13, 316, 665, 1365, 295, 341, 307, 264, 957, 1002, 13, 440, 957, 1002, 11, 51344], "temperature": 0.0, "avg_logprob": -0.10073241591453552, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.00031997496262192726}, {"id": 784, "seek": 494912, "start": 4969.84, "end": 4975.12, "text": " you can think of it as a cost function, right? You learn to ride a bike and you ride your bike", "tokens": [51400, 291, 393, 519, 295, 309, 382, 257, 2063, 2445, 11, 558, 30, 509, 1466, 281, 5077, 257, 5656, 293, 291, 5077, 428, 5656, 51664], "temperature": 0.0, "avg_logprob": -0.10073241591453552, "compression_ratio": 1.8755020080321285, "no_speech_prob": 0.00031997496262192726}, {"id": 785, "seek": 497512, "start": 4975.12, "end": 4984.16, "text": " and at some point you fall. The real world does not give you a gradient of that cost function,", "tokens": [50364, 293, 412, 512, 935, 291, 2100, 13, 440, 957, 1002, 775, 406, 976, 291, 257, 16235, 295, 300, 2063, 2445, 11, 50816], "temperature": 0.0, "avg_logprob": -0.07008502384026845, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.00039197655860334635}, {"id": 786, "seek": 497512, "start": 4984.16, "end": 4990.72, "text": " which is how much you hurt with respect to your actions. Okay? The only thing you can do is try", "tokens": [50816, 597, 307, 577, 709, 291, 4607, 365, 3104, 281, 428, 5909, 13, 1033, 30, 440, 787, 551, 291, 393, 360, 307, 853, 51144], "temperature": 0.0, "avg_logprob": -0.07008502384026845, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.00039197655860334635}, {"id": 787, "seek": 497512, "start": 4990.72, "end": 4996.08, "text": " something else and see if you get the same result or not. Okay? So what do you do in that case? So", "tokens": [51144, 746, 1646, 293, 536, 498, 291, 483, 264, 912, 1874, 420, 406, 13, 1033, 30, 407, 437, 360, 291, 360, 294, 300, 1389, 30, 407, 51412], "temperature": 0.0, "avg_logprob": -0.07008502384026845, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.00039197655860334635}, {"id": 788, "seek": 497512, "start": 4996.08, "end": 5001.36, "text": " basically now your cost function is a black box. So now you cannot propagate gradient to this", "tokens": [51412, 1936, 586, 428, 2063, 2445, 307, 257, 2211, 2424, 13, 407, 586, 291, 2644, 48256, 16235, 281, 341, 51676], "temperature": 0.0, "avg_logprob": -0.07008502384026845, "compression_ratio": 1.6724890829694323, "no_speech_prob": 0.00039197655860334635}, {"id": 789, "seek": 500136, "start": 5001.36, "end": 5006.639999999999, "text": " black box. What you have to do is estimate the gradient by perturbing the, what you see to", "tokens": [50364, 2211, 2424, 13, 708, 291, 362, 281, 360, 307, 12539, 264, 16235, 538, 13269, 374, 4324, 264, 11, 437, 291, 536, 281, 50628], "temperature": 0.0, "avg_logprob": -0.11724845484683388, "compression_ratio": 1.845, "no_speech_prob": 0.0016218266682699323}, {"id": 790, "seek": 500136, "start": 5006.639999999999, "end": 5014.08, "text": " that black box, right? So, you know, you try something, right? And that something would be a", "tokens": [50628, 300, 2211, 2424, 11, 558, 30, 407, 11, 291, 458, 11, 291, 853, 746, 11, 558, 30, 400, 300, 746, 576, 312, 257, 51000], "temperature": 0.0, "avg_logprob": -0.11724845484683388, "compression_ratio": 1.845, "no_speech_prob": 0.0016218266682699323}, {"id": 791, "seek": 500136, "start": 5014.08, "end": 5022.24, "text": " perturbation of your input to this black box and you see what resulting perturbation occurs", "tokens": [51000, 40468, 399, 295, 428, 4846, 281, 341, 2211, 2424, 293, 291, 536, 437, 16505, 40468, 399, 11843, 51408], "temperature": 0.0, "avg_logprob": -0.11724845484683388, "compression_ratio": 1.845, "no_speech_prob": 0.0016218266682699323}, {"id": 792, "seek": 500136, "start": 5022.24, "end": 5027.759999999999, "text": " on the black, on the output of the black box, the cost. And now you can estimate whether you,", "tokens": [51408, 322, 264, 2211, 11, 322, 264, 5598, 295, 264, 2211, 2424, 11, 264, 2063, 13, 400, 586, 291, 393, 12539, 1968, 291, 11, 51684], "temperature": 0.0, "avg_logprob": -0.11724845484683388, "compression_ratio": 1.845, "no_speech_prob": 0.0016218266682699323}, {"id": 793, "seek": 502776, "start": 5028.4800000000005, "end": 5036.400000000001, "text": " you know, this modification improved or made the result worse, right? So essentially,", "tokens": [50400, 291, 458, 11, 341, 26747, 9689, 420, 1027, 264, 1874, 5324, 11, 558, 30, 407, 4476, 11, 50796], "temperature": 0.0, "avg_logprob": -0.11831405675299814, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.00026116217486560345}, {"id": 794, "seek": 502776, "start": 5036.400000000001, "end": 5041.52, "text": " this is like this optimization problem I was telling you about earlier. The gradient based", "tokens": [50796, 341, 307, 411, 341, 19618, 1154, 286, 390, 3585, 291, 466, 3071, 13, 440, 16235, 2361, 51052], "temperature": 0.0, "avg_logprob": -0.11831405675299814, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.00026116217486560345}, {"id": 795, "seek": 502776, "start": 5041.52, "end": 5046.56, "text": " algorithm is like you are in the mountain, lost in the mountain in a fog, you can't see anything.", "tokens": [51052, 9284, 307, 411, 291, 366, 294, 264, 6937, 11, 2731, 294, 264, 6937, 294, 257, 13648, 11, 291, 393, 380, 536, 1340, 13, 51304], "temperature": 0.0, "avg_logprob": -0.11831405675299814, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.00026116217486560345}, {"id": 796, "seek": 502776, "start": 5047.360000000001, "end": 5051.92, "text": " But you can estimate the direction of steepest descent, right? You can just look around and you", "tokens": [51344, 583, 291, 393, 12539, 264, 3513, 295, 16841, 377, 23475, 11, 558, 30, 509, 393, 445, 574, 926, 293, 291, 51572], "temperature": 0.0, "avg_logprob": -0.11831405675299814, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.00026116217486560345}, {"id": 797, "seek": 502776, "start": 5051.92, "end": 5055.68, "text": " can tell which is the direction of steepest descent. You just take a step in that direction.", "tokens": [51572, 393, 980, 597, 307, 264, 3513, 295, 16841, 377, 23475, 13, 509, 445, 747, 257, 1823, 294, 300, 3513, 13, 51760], "temperature": 0.0, "avg_logprob": -0.11831405675299814, "compression_ratio": 1.801556420233463, "no_speech_prob": 0.00026116217486560345}, {"id": 798, "seek": 505776, "start": 5058.08, "end": 5064.16, "text": " What if you can't see, right? So basically to estimate in which direction the function goes", "tokens": [50380, 708, 498, 291, 393, 380, 536, 11, 558, 30, 407, 1936, 281, 12539, 294, 597, 3513, 264, 2445, 1709, 50684], "temperature": 0.0, "avg_logprob": -0.09436137100745892, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.0001794856507331133}, {"id": 799, "seek": 505776, "start": 5064.16, "end": 5070.72, "text": " down, you have to actually take a step, okay? So you take a step in one direction, then you", "tokens": [50684, 760, 11, 291, 362, 281, 767, 747, 257, 1823, 11, 1392, 30, 407, 291, 747, 257, 1823, 294, 472, 3513, 11, 550, 291, 51012], "temperature": 0.0, "avg_logprob": -0.09436137100745892, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.0001794856507331133}, {"id": 800, "seek": 505776, "start": 5070.72, "end": 5073.92, "text": " come back, then you can take a step in the other direction, come back, and then maybe you get an", "tokens": [51012, 808, 646, 11, 550, 291, 393, 747, 257, 1823, 294, 264, 661, 3513, 11, 808, 646, 11, 293, 550, 1310, 291, 483, 364, 51172], "temperature": 0.0, "avg_logprob": -0.09436137100745892, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.0001794856507331133}, {"id": 801, "seek": 505776, "start": 5073.92, "end": 5078.4800000000005, "text": " estimate for where the steepest descent is. Now you can take a step for steepest descent. So this", "tokens": [51172, 12539, 337, 689, 264, 16841, 377, 23475, 307, 13, 823, 291, 393, 747, 257, 1823, 337, 16841, 377, 23475, 13, 407, 341, 51400], "temperature": 0.0, "avg_logprob": -0.09436137100745892, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.0001794856507331133}, {"id": 802, "seek": 505776, "start": 5078.4800000000005, "end": 5084.16, "text": " is estimating the gradient by perturbation instead of by analytic means of back propagating", "tokens": [51400, 307, 8017, 990, 264, 16235, 538, 40468, 399, 2602, 295, 538, 40358, 1355, 295, 646, 12425, 990, 51684], "temperature": 0.0, "avg_logprob": -0.09436137100745892, "compression_ratio": 1.9341563786008231, "no_speech_prob": 0.0001794856507331133}, {"id": 803, "seek": 508416, "start": 5084.16, "end": 5091.2, "text": " gradients, okay, computing Jacobians or whatever, partial derivatives. And then there is the second", "tokens": [50364, 2771, 2448, 11, 1392, 11, 15866, 14117, 2567, 420, 2035, 11, 14641, 33733, 13, 400, 550, 456, 307, 264, 1150, 50716], "temperature": 0.0, "avg_logprob": -0.130494733827304, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000984258484095335}, {"id": 804, "seek": 508416, "start": 5091.2, "end": 5096.88, "text": " step of complexity. Let's imagine that the landscape you are in is basically flat everywhere,", "tokens": [50716, 1823, 295, 14024, 13, 961, 311, 3811, 300, 264, 9661, 291, 366, 294, 307, 1936, 4962, 5315, 11, 51000], "temperature": 0.0, "avg_logprob": -0.130494733827304, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000984258484095335}, {"id": 805, "seek": 508416, "start": 5096.88, "end": 5102.4, "text": " except, you know, once in a while there is a step, okay? So taking a small step in one direction", "tokens": [51000, 3993, 11, 291, 458, 11, 1564, 294, 257, 1339, 456, 307, 257, 1823, 11, 1392, 30, 407, 1940, 257, 1359, 1823, 294, 472, 3513, 51276], "temperature": 0.0, "avg_logprob": -0.130494733827304, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000984258484095335}, {"id": 806, "seek": 508416, "start": 5102.4, "end": 5106.48, "text": " will not give you any information about which direction you have to go to. So there you have", "tokens": [51276, 486, 406, 976, 291, 604, 1589, 466, 597, 3513, 291, 362, 281, 352, 281, 13, 407, 456, 291, 362, 51480], "temperature": 0.0, "avg_logprob": -0.130494733827304, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000984258484095335}, {"id": 807, "seek": 508416, "start": 5106.48, "end": 5112.4, "text": " to use other techniques, taking bigger steps, you know, working for a while and seeing if you", "tokens": [51480, 281, 764, 661, 7512, 11, 1940, 3801, 4439, 11, 291, 458, 11, 1364, 337, 257, 1339, 293, 2577, 498, 291, 51776], "temperature": 0.0, "avg_logprob": -0.130494733827304, "compression_ratio": 1.7345454545454546, "no_speech_prob": 0.000984258484095335}, {"id": 808, "seek": 511240, "start": 5113.28, "end": 5119.12, "text": " fall down the step or not, or go up a step. You know, maybe you can multiply yourself in", "tokens": [50408, 2100, 760, 264, 1823, 420, 406, 11, 420, 352, 493, 257, 1823, 13, 509, 458, 11, 1310, 291, 393, 12972, 1803, 294, 50700], "temperature": 0.0, "avg_logprob": -0.11560085841587611, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.000626233231741935}, {"id": 809, "seek": 511240, "start": 5119.12, "end": 5124.16, "text": " sort of 10,000 copies of yourself and then kind of explore the surroundings. And then whenever", "tokens": [50700, 1333, 295, 1266, 11, 1360, 14341, 295, 1803, 293, 550, 733, 295, 6839, 264, 25314, 13, 400, 550, 5699, 50952], "temperature": 0.0, "avg_logprob": -0.11560085841587611, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.000626233231741935}, {"id": 810, "seek": 511240, "start": 5124.16, "end": 5130.4, "text": " someone says, oh, I find a hole, calls everyone to kind of come there, okay? So all those methods", "tokens": [50952, 1580, 1619, 11, 1954, 11, 286, 915, 257, 5458, 11, 5498, 1518, 281, 733, 295, 808, 456, 11, 1392, 30, 407, 439, 729, 7150, 51264], "temperature": 0.0, "avg_logprob": -0.11560085841587611, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.000626233231741935}, {"id": 811, "seek": 511240, "start": 5130.4, "end": 5136.799999999999, "text": " are called gradient-free optimization algorithms. Sometimes they're called zero-th order method.", "tokens": [51264, 366, 1219, 16235, 12, 10792, 19618, 14642, 13, 4803, 436, 434, 1219, 4018, 12, 392, 1668, 3170, 13, 51584], "temperature": 0.0, "avg_logprob": -0.11560085841587611, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.000626233231741935}, {"id": 812, "seek": 511240, "start": 5136.799999999999, "end": 5140.32, "text": " Why zero-th order? Because first order is when you can compute the derivative.", "tokens": [51584, 1545, 4018, 12, 392, 1668, 30, 1436, 700, 1668, 307, 562, 291, 393, 14722, 264, 13760, 13, 51760], "temperature": 0.0, "avg_logprob": -0.11560085841587611, "compression_ratio": 1.6379928315412187, "no_speech_prob": 0.000626233231741935}, {"id": 813, "seek": 514032, "start": 5140.32, "end": 5143.679999999999, "text": " Zero-th order is when you cannot compute the derivative. You can only compute the function", "tokens": [50364, 17182, 12, 392, 1668, 307, 562, 291, 2644, 14722, 264, 13760, 13, 509, 393, 787, 14722, 264, 2445, 50532], "temperature": 0.0, "avg_logprob": -0.08355657011270523, "compression_ratio": 2.041044776119403, "no_speech_prob": 0.0003300838579889387}, {"id": 814, "seek": 514032, "start": 5143.679999999999, "end": 5147.92, "text": " or get a value for the function. And then you have second order methods that compute not just", "tokens": [50532, 420, 483, 257, 2158, 337, 264, 2445, 13, 400, 550, 291, 362, 1150, 1668, 7150, 300, 14722, 406, 445, 50744], "temperature": 0.0, "avg_logprob": -0.08355657011270523, "compression_ratio": 2.041044776119403, "no_speech_prob": 0.0003300838579889387}, {"id": 815, "seek": 514032, "start": 5147.92, "end": 5152.639999999999, "text": " the first derivative, but also the second derivative. And they're also gradient-based,", "tokens": [50744, 264, 700, 13760, 11, 457, 611, 264, 1150, 13760, 13, 400, 436, 434, 611, 16235, 12, 6032, 11, 50980], "temperature": 0.0, "avg_logprob": -0.08355657011270523, "compression_ratio": 2.041044776119403, "no_speech_prob": 0.0003300838579889387}, {"id": 816, "seek": 514032, "start": 5152.639999999999, "end": 5156.639999999999, "text": " okay, because they need the first derivative as well. But they can accelerate the process by", "tokens": [50980, 1392, 11, 570, 436, 643, 264, 700, 13760, 382, 731, 13, 583, 436, 393, 21341, 264, 1399, 538, 51180], "temperature": 0.0, "avg_logprob": -0.08355657011270523, "compression_ratio": 2.041044776119403, "no_speech_prob": 0.0003300838579889387}, {"id": 817, "seek": 514032, "start": 5156.639999999999, "end": 5162.639999999999, "text": " also computing the second derivative. And Adam is a very simplified form of kind of, you know,", "tokens": [51180, 611, 15866, 264, 1150, 13760, 13, 400, 7938, 307, 257, 588, 26335, 1254, 295, 733, 295, 11, 291, 458, 11, 51480], "temperature": 0.0, "avg_logprob": -0.08355657011270523, "compression_ratio": 2.041044776119403, "no_speech_prob": 0.0003300838579889387}, {"id": 818, "seek": 514032, "start": 5163.92, "end": 5168.719999999999, "text": " second order method. It's not a second order method, but it has a hint of second order.", "tokens": [51544, 1150, 1668, 3170, 13, 467, 311, 406, 257, 1150, 1668, 3170, 11, 457, 309, 575, 257, 12075, 295, 1150, 1668, 13, 51784], "temperature": 0.0, "avg_logprob": -0.08355657011270523, "compression_ratio": 2.041044776119403, "no_speech_prob": 0.0003300838579889387}, {"id": 819, "seek": 516872, "start": 5168.72, "end": 5171.68, "text": " Another hint of second order method is what's called conjugate gradient.", "tokens": [50364, 3996, 12075, 295, 1150, 1668, 3170, 307, 437, 311, 1219, 45064, 16235, 13, 50512], "temperature": 0.0, "avg_logprob": -0.13351233964113846, "compression_ratio": 1.696, "no_speech_prob": 7.965725671965629e-05}, {"id": 820, "seek": 516872, "start": 5173.12, "end": 5176.4800000000005, "text": " It's another class of method called quasi-Newton methods, which are also kind of", "tokens": [50584, 467, 311, 1071, 1508, 295, 3170, 1219, 20954, 12, 18278, 1756, 7150, 11, 597, 366, 611, 733, 295, 50752], "temperature": 0.0, "avg_logprob": -0.13351233964113846, "compression_ratio": 1.696, "no_speech_prob": 7.965725671965629e-05}, {"id": 821, "seek": 516872, "start": 5177.52, "end": 5180.16, "text": " using kind of curvature information, if you want, to kind of accelerate.", "tokens": [50804, 1228, 733, 295, 37638, 1589, 11, 498, 291, 528, 11, 281, 733, 295, 21341, 13, 50936], "temperature": 0.0, "avg_logprob": -0.13351233964113846, "compression_ratio": 1.696, "no_speech_prob": 7.965725671965629e-05}, {"id": 822, "seek": 516872, "start": 5181.280000000001, "end": 5186.4800000000005, "text": " Many of those are not actually practical for neural net training, but there are some forms that are.", "tokens": [50992, 5126, 295, 729, 366, 406, 767, 8496, 337, 18161, 2533, 3097, 11, 457, 456, 366, 512, 6422, 300, 366, 13, 51252], "temperature": 0.0, "avg_logprob": -0.13351233964113846, "compression_ratio": 1.696, "no_speech_prob": 7.965725671965629e-05}, {"id": 823, "seek": 516872, "start": 5188.0, "end": 5193.76, "text": " If you're interested in zero-th order optimization, there is a library that is actually produced", "tokens": [51328, 759, 291, 434, 3102, 294, 4018, 12, 392, 1668, 19618, 11, 456, 307, 257, 6405, 300, 307, 767, 7126, 51616], "temperature": 0.0, "avg_logprob": -0.13351233964113846, "compression_ratio": 1.696, "no_speech_prob": 7.965725671965629e-05}, {"id": 824, "seek": 519376, "start": 5194.72, "end": 5199.4400000000005, "text": " by, it's an open source library, which originated at Facebook Research in Paris", "tokens": [50412, 538, 11, 309, 311, 364, 1269, 4009, 6405, 11, 597, 31129, 412, 4384, 10303, 294, 8380, 50648], "temperature": 0.0, "avg_logprob": -0.1457530373021176, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.00067681580549106}, {"id": 825, "seek": 519376, "start": 5200.56, "end": 5204.320000000001, "text": " by an author called Olivier Tito, but it's really a community effort. There's a lot of", "tokens": [50704, 538, 364, 3793, 1219, 48075, 314, 3528, 11, 457, 309, 311, 534, 257, 1768, 4630, 13, 821, 311, 257, 688, 295, 50892], "temperature": 0.0, "avg_logprob": -0.1457530373021176, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.00067681580549106}, {"id": 826, "seek": 519376, "start": 5204.320000000001, "end": 5209.360000000001, "text": " contributors to it. It's called Nevergrad. And it implements a very large number of different", "tokens": [50892, 45627, 281, 309, 13, 467, 311, 1219, 7344, 7165, 13, 400, 309, 704, 17988, 257, 588, 2416, 1230, 295, 819, 51144], "temperature": 0.0, "avg_logprob": -0.1457530373021176, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.00067681580549106}, {"id": 827, "seek": 519376, "start": 5209.360000000001, "end": 5214.08, "text": " optimization algorithms that do not assume that you have access to the gradient. Okay.", "tokens": [51144, 19618, 14642, 300, 360, 406, 6552, 300, 291, 362, 2105, 281, 264, 16235, 13, 1033, 13, 51380], "temperature": 0.0, "avg_logprob": -0.1457530373021176, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.00067681580549106}, {"id": 828, "seek": 519376, "start": 5215.2, "end": 5218.400000000001, "text": " There are genetic algorithms or evolutionary methods. There are", "tokens": [51436, 821, 366, 12462, 14642, 420, 27567, 7150, 13, 821, 366, 51596], "temperature": 0.0, "avg_logprob": -0.1457530373021176, "compression_ratio": 1.5627376425855513, "no_speech_prob": 0.00067681580549106}, {"id": 829, "seek": 521840, "start": 5219.36, "end": 5225.04, "text": " particle swarm optimization. There are perturbation methods. There is all kinds of tricks, right?", "tokens": [50412, 12359, 49839, 19618, 13, 821, 366, 40468, 399, 7150, 13, 821, 307, 439, 3685, 295, 11733, 11, 558, 30, 50696], "temperature": 0.0, "avg_logprob": -0.14020624073273544, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.0005441187531687319}, {"id": 830, "seek": 521840, "start": 5225.04, "end": 5229.36, "text": " I mean, there's a whole catalog of those things. And those sometimes it's unavoidable. You have", "tokens": [50696, 286, 914, 11, 456, 311, 257, 1379, 19746, 295, 729, 721, 13, 400, 729, 2171, 309, 311, 36541, 17079, 712, 13, 509, 362, 50912], "temperature": 0.0, "avg_logprob": -0.14020624073273544, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.0005441187531687319}, {"id": 831, "seek": 521840, "start": 5229.36, "end": 5234.719999999999, "text": " to use them because you don't know the cost function. So a very common situation where you", "tokens": [50912, 281, 764, 552, 570, 291, 500, 380, 458, 264, 2063, 2445, 13, 407, 257, 588, 2689, 2590, 689, 291, 51180], "temperature": 0.0, "avg_logprob": -0.14020624073273544, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.0005441187531687319}, {"id": 832, "seek": 521840, "start": 5234.719999999999, "end": 5240.879999999999, "text": " had to use those things is reinforcement learning. So reinforcement learning is basically a situation", "tokens": [51180, 632, 281, 764, 729, 721, 307, 29280, 2539, 13, 407, 29280, 2539, 307, 1936, 257, 2590, 51488], "temperature": 0.0, "avg_logprob": -0.14020624073273544, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.0005441187531687319}, {"id": 833, "seek": 521840, "start": 5240.879999999999, "end": 5246.4, "text": " where you tell the system, you don't tell the system the correct answer. You only tell the", "tokens": [51488, 689, 291, 980, 264, 1185, 11, 291, 500, 380, 980, 264, 1185, 264, 3006, 1867, 13, 509, 787, 980, 264, 51764], "temperature": 0.0, "avg_logprob": -0.14020624073273544, "compression_ratio": 1.8346153846153845, "no_speech_prob": 0.0005441187531687319}, {"id": 834, "seek": 524640, "start": 5246.4, "end": 5250.96, "text": " system whether the answer was good or bad. It's because you give the value of the cost,", "tokens": [50364, 1185, 1968, 264, 1867, 390, 665, 420, 1578, 13, 467, 311, 570, 291, 976, 264, 2158, 295, 264, 2063, 11, 50592], "temperature": 0.0, "avg_logprob": -0.11328408428441698, "compression_ratio": 1.8438818565400843, "no_speech_prob": 0.00024532611132599413}, {"id": 835, "seek": 524640, "start": 5250.96, "end": 5254.0, "text": " but you don't tell the machine where the cost is. So the machine doesn't know where the cost", "tokens": [50592, 457, 291, 500, 380, 980, 264, 3479, 689, 264, 2063, 307, 13, 407, 264, 3479, 1177, 380, 458, 689, 264, 2063, 50744], "temperature": 0.0, "avg_logprob": -0.11328408428441698, "compression_ratio": 1.8438818565400843, "no_speech_prob": 0.00024532611132599413}, {"id": 836, "seek": 524640, "start": 5254.0, "end": 5260.4, "text": " function is. Okay. And so the machine cannot actually compute the gradient of the cost. And", "tokens": [50744, 2445, 307, 13, 1033, 13, 400, 370, 264, 3479, 2644, 767, 14722, 264, 16235, 295, 264, 2063, 13, 400, 51064], "temperature": 0.0, "avg_logprob": -0.11328408428441698, "compression_ratio": 1.8438818565400843, "no_speech_prob": 0.00024532611132599413}, {"id": 837, "seek": 524640, "start": 5260.4, "end": 5266.48, "text": " so it has to use something like a zero-th order method. So what you can do is you can compute", "tokens": [51064, 370, 309, 575, 281, 764, 746, 411, 257, 4018, 12, 392, 1668, 3170, 13, 407, 437, 291, 393, 360, 307, 291, 393, 14722, 51368], "temperature": 0.0, "avg_logprob": -0.11328408428441698, "compression_ratio": 1.8438818565400843, "no_speech_prob": 0.00024532611132599413}, {"id": 838, "seek": 524640, "start": 5267.2, "end": 5270.719999999999, "text": " a gradient with respect to the parameters of the overall cost function", "tokens": [51404, 257, 16235, 365, 3104, 281, 264, 9834, 295, 264, 4787, 2063, 2445, 51580], "temperature": 0.0, "avg_logprob": -0.11328408428441698, "compression_ratio": 1.8438818565400843, "no_speech_prob": 0.00024532611132599413}, {"id": 839, "seek": 527072, "start": 5271.52, "end": 5277.84, "text": " by perturbing the parameters. Or what you can do is compute the gradient of the cost function", "tokens": [50404, 538, 13269, 374, 4324, 264, 9834, 13, 1610, 437, 291, 393, 360, 307, 14722, 264, 16235, 295, 264, 2063, 2445, 50720], "temperature": 0.0, "avg_logprob": -0.10173447332649588, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.0009847632609307766}, {"id": 840, "seek": 527072, "start": 5277.84, "end": 5283.76, "text": " with respect to the output of your neural net. Okay. Using perturbation. And once you", "tokens": [50720, 365, 3104, 281, 264, 5598, 295, 428, 18161, 2533, 13, 1033, 13, 11142, 40468, 399, 13, 400, 1564, 291, 51016], "temperature": 0.0, "avg_logprob": -0.10173447332649588, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.0009847632609307766}, {"id": 841, "seek": 527072, "start": 5283.76, "end": 5287.12, "text": " have this estimate, then you back propagate the gradient through your network using regular", "tokens": [51016, 362, 341, 12539, 11, 550, 291, 646, 48256, 264, 16235, 807, 428, 3209, 1228, 3890, 51184], "temperature": 0.0, "avg_logprob": -0.10173447332649588, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.0009847632609307766}, {"id": 842, "seek": 527072, "start": 5287.12, "end": 5292.0, "text": " backprop. So that's a combination of estimating the gradient through perturbation for the cost", "tokens": [51184, 646, 79, 1513, 13, 407, 300, 311, 257, 6562, 295, 8017, 990, 264, 16235, 807, 40468, 399, 337, 264, 2063, 51428], "temperature": 0.0, "avg_logprob": -0.10173447332649588, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.0009847632609307766}, {"id": 843, "seek": 527072, "start": 5292.0, "end": 5296.72, "text": " function because you don't know it, and then backpropagating from there. This is basically", "tokens": [51428, 2445, 570, 291, 500, 380, 458, 309, 11, 293, 550, 646, 79, 1513, 559, 990, 490, 456, 13, 639, 307, 1936, 51664], "temperature": 0.0, "avg_logprob": -0.10173447332649588, "compression_ratio": 1.8134920634920635, "no_speech_prob": 0.0009847632609307766}, {"id": 844, "seek": 529672, "start": 5296.72, "end": 5302.16, "text": " the tactic that was used by the deep line people in sort of the first sort of deep", "tokens": [50364, 264, 31012, 300, 390, 1143, 538, 264, 2452, 1622, 561, 294, 1333, 295, 264, 700, 1333, 295, 2452, 50636], "temperature": 0.0, "avg_logprob": -0.1706508049598107, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.0006161711644381285}, {"id": 845, "seek": 529672, "start": 5303.12, "end": 5310.400000000001, "text": " queue learning type methods. Back to the normalization. Do we normalize the entire dataset", "tokens": [50684, 18639, 2539, 2010, 7150, 13, 5833, 281, 264, 2710, 2144, 13, 1144, 321, 2710, 1125, 264, 2302, 28872, 51048], "temperature": 0.0, "avg_logprob": -0.1706508049598107, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.0006161711644381285}, {"id": 846, "seek": 529672, "start": 5310.400000000001, "end": 5321.2, "text": " or each batch? It's equivalent. So you normalize each sample, but the variable you're computing", "tokens": [51048, 420, 1184, 15245, 30, 467, 311, 10344, 13, 407, 291, 2710, 1125, 1184, 6889, 11, 457, 264, 7006, 291, 434, 15866, 51588], "temperature": 0.0, "avg_logprob": -0.1706508049598107, "compression_ratio": 1.4944444444444445, "no_speech_prob": 0.0006161711644381285}, {"id": 847, "seek": 532120, "start": 5321.2, "end": 5325.679999999999, "text": " is on the entire training set, right? So you're computing the standard deviation", "tokens": [50364, 307, 322, 264, 2302, 3097, 992, 11, 558, 30, 407, 291, 434, 15866, 264, 3832, 25163, 50588], "temperature": 0.0, "avg_logprob": -0.09727196423512585, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0006666219560429454}, {"id": 848, "seek": 532120, "start": 5326.48, "end": 5331.28, "text": " and the mean over the entire training set. In fact, most of the time you don't even need to do it", "tokens": [50628, 293, 264, 914, 670, 264, 2302, 3097, 992, 13, 682, 1186, 11, 881, 295, 264, 565, 291, 500, 380, 754, 643, 281, 360, 309, 50868], "temperature": 0.0, "avg_logprob": -0.09727196423512585, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0006666219560429454}, {"id": 849, "seek": 532120, "start": 5331.28, "end": 5334.88, "text": " over the entire training set because mean and standard deviation converges pretty fast.", "tokens": [50868, 670, 264, 2302, 3097, 992, 570, 914, 293, 3832, 25163, 9652, 2880, 1238, 2370, 13, 51048], "temperature": 0.0, "avg_logprob": -0.09727196423512585, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0006666219560429454}, {"id": 850, "seek": 532120, "start": 5336.0, "end": 5342.32, "text": " So, but you do it over the entire training set, right? And what you get is a constant number,", "tokens": [51104, 407, 11, 457, 291, 360, 309, 670, 264, 2302, 3097, 992, 11, 558, 30, 400, 437, 291, 483, 307, 257, 5754, 1230, 11, 51420], "temperature": 0.0, "avg_logprob": -0.09727196423512585, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0006666219560429454}, {"id": 851, "seek": 532120, "start": 5342.32, "end": 5346.32, "text": " two constant numbers, a number that you subtract and a number that you should divide", "tokens": [51420, 732, 5754, 3547, 11, 257, 1230, 300, 291, 16390, 293, 257, 1230, 300, 291, 820, 9845, 51620], "temperature": 0.0, "avg_logprob": -0.09727196423512585, "compression_ratio": 2.0892018779342725, "no_speech_prob": 0.0006666219560429454}, {"id": 852, "seek": 534632, "start": 5346.32, "end": 5353.04, "text": " for each component of your input, okay? It's a fixed preprocessing. For a given training set,", "tokens": [50364, 337, 1184, 6542, 295, 428, 4846, 11, 1392, 30, 467, 311, 257, 6806, 2666, 340, 780, 278, 13, 1171, 257, 2212, 3097, 992, 11, 50700], "temperature": 0.0, "avg_logprob": -0.1269924207167192, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.001165952766314149}, {"id": 853, "seek": 534632, "start": 5353.04, "end": 5361.04, "text": " you'll have a fixed mean and standard deviation vector.", "tokens": [50700, 291, 603, 362, 257, 6806, 914, 293, 3832, 25163, 8062, 13, 51100], "temperature": 0.0, "avg_logprob": -0.1269924207167192, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.001165952766314149}, {"id": 854, "seek": 534632, "start": 5361.04, "end": 5367.759999999999, "text": " But maybe we can connect to the other tool, right? The other module, the batch normalization, right?", "tokens": [51100, 583, 1310, 321, 393, 1745, 281, 264, 661, 2290, 11, 558, 30, 440, 661, 10088, 11, 264, 15245, 2710, 2144, 11, 558, 30, 51436], "temperature": 0.0, "avg_logprob": -0.1269924207167192, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.001165952766314149}, {"id": 855, "seek": 534632, "start": 5367.759999999999, "end": 5372.08, "text": " Okay, we haven't talked about that yet. Yeah, I'm saying that we can perhaps extend", "tokens": [51436, 1033, 11, 321, 2378, 380, 2825, 466, 300, 1939, 13, 865, 11, 286, 478, 1566, 300, 321, 393, 4317, 10101, 51652], "temperature": 0.0, "avg_logprob": -0.1269924207167192, "compression_ratio": 1.5045045045045045, "no_speech_prob": 0.001165952766314149}, {"id": 856, "seek": 537208, "start": 5372.72, "end": 5378.08, "text": " this normalization bit to the both sides, like the whole dataset and the batch itself.", "tokens": [50396, 341, 2710, 2144, 857, 281, 264, 1293, 4881, 11, 411, 264, 1379, 28872, 293, 264, 15245, 2564, 13, 50664], "temperature": 0.0, "avg_logprob": -0.14941694055284774, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0008939541294239461}, {"id": 857, "seek": 537208, "start": 5378.08, "end": 5383.28, "text": " Okay, yes, yes. So, I mean, again, there's going to be a whole lecture on this. But", "tokens": [50664, 1033, 11, 2086, 11, 2086, 13, 407, 11, 286, 914, 11, 797, 11, 456, 311, 516, 281, 312, 257, 1379, 7991, 322, 341, 13, 583, 50924], "temperature": 0.0, "avg_logprob": -0.14941694055284774, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0008939541294239461}, {"id": 858, "seek": 537208, "start": 5385.84, "end": 5392.08, "text": " for the same reason, it's good to have variables, the input that are zero mean and you need variants.", "tokens": [51052, 337, 264, 912, 1778, 11, 309, 311, 665, 281, 362, 9102, 11, 264, 4846, 300, 366, 4018, 914, 293, 291, 643, 21669, 13, 51364], "temperature": 0.0, "avg_logprob": -0.14941694055284774, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0008939541294239461}, {"id": 859, "seek": 537208, "start": 5392.08, "end": 5396.08, "text": " It's also good for the state variables inside the network to basically have zero mean and", "tokens": [51364, 467, 311, 611, 665, 337, 264, 1785, 9102, 1854, 264, 3209, 281, 1936, 362, 4018, 914, 293, 51564], "temperature": 0.0, "avg_logprob": -0.14941694055284774, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0008939541294239461}, {"id": 860, "seek": 537208, "start": 5396.08, "end": 5400.48, "text": " you need variants. And so people have come up with various ways of doing normalization", "tokens": [51564, 291, 643, 21669, 13, 400, 370, 561, 362, 808, 493, 365, 3683, 2098, 295, 884, 2710, 2144, 51784], "temperature": 0.0, "avg_logprob": -0.14941694055284774, "compression_ratio": 1.7677165354330708, "no_speech_prob": 0.0008939541294239461}, {"id": 861, "seek": 540048, "start": 5401.36, "end": 5407.44, "text": " of the variables inside the network so that they approach zero mean and you need variants.", "tokens": [50408, 295, 264, 9102, 1854, 264, 3209, 370, 300, 436, 3109, 4018, 914, 293, 291, 643, 21669, 13, 50712], "temperature": 0.0, "avg_logprob": -0.14595807019402, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.0003625258104875684}, {"id": 862, "seek": 540048, "start": 5408.959999999999, "end": 5414.5599999999995, "text": " But, and there are many ways to do this. They have two names like batch normalization, like", "tokens": [50788, 583, 11, 293, 456, 366, 867, 2098, 281, 360, 341, 13, 814, 362, 732, 5288, 411, 15245, 2710, 2144, 11, 411, 51068], "temperature": 0.0, "avg_logprob": -0.14595807019402, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.0003625258104875684}, {"id": 863, "seek": 540048, "start": 5417.28, "end": 5420.5599999999995, "text": " layer normalization. And the idea goes back a very long time.", "tokens": [51204, 4583, 2710, 2144, 13, 400, 264, 1558, 1709, 646, 257, 588, 938, 565, 13, 51368], "temperature": 0.0, "avg_logprob": -0.14595807019402, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.0003625258104875684}, {"id": 864, "seek": 540048, "start": 5421.599999999999, "end": 5428.08, "text": " Batch norm is kind of a more recent incarnation of it. Let's see, what was I scheduled to decrease", "tokens": [51420, 363, 852, 2026, 307, 733, 295, 257, 544, 5162, 49988, 295, 309, 13, 961, 311, 536, 11, 437, 390, 286, 15678, 281, 11514, 51744], "temperature": 0.0, "avg_logprob": -0.14595807019402, "compression_ratio": 1.545045045045045, "no_speech_prob": 0.0003625258104875684}, {"id": 865, "seek": 542808, "start": 5428.08, "end": 5434.48, "text": " the learning rate? Yeah, as it turns out, for reasons that are still not completely fully understood,", "tokens": [50364, 264, 2539, 3314, 30, 865, 11, 382, 309, 4523, 484, 11, 337, 4112, 300, 366, 920, 406, 2584, 4498, 7320, 11, 50684], "temperature": 0.0, "avg_logprob": -0.10663158949031386, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0010479131015017629}, {"id": 866, "seek": 542808, "start": 5435.36, "end": 5441.6, "text": " you need to learn fast initially, you need a learning rate of a particular size.", "tokens": [50728, 291, 643, 281, 1466, 2370, 9105, 11, 291, 643, 257, 2539, 3314, 295, 257, 1729, 2744, 13, 51040], "temperature": 0.0, "avg_logprob": -0.10663158949031386, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0010479131015017629}, {"id": 867, "seek": 542808, "start": 5442.72, "end": 5446.0, "text": " But to get good results in the end, you kind of need to decrease the learning rate to kind of", "tokens": [51096, 583, 281, 483, 665, 3542, 294, 264, 917, 11, 291, 733, 295, 643, 281, 11514, 264, 2539, 3314, 281, 733, 295, 51260], "temperature": 0.0, "avg_logprob": -0.10663158949031386, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0010479131015017629}, {"id": 868, "seek": 542808, "start": 5446.0, "end": 5453.04, "text": " let the system settle inside of minima. And that requires decreasing the learning rate.", "tokens": [51260, 718, 264, 1185, 11852, 1854, 295, 4464, 64, 13, 400, 300, 7029, 23223, 264, 2539, 3314, 13, 51612], "temperature": 0.0, "avg_logprob": -0.10663158949031386, "compression_ratio": 1.766990291262136, "no_speech_prob": 0.0010479131015017629}, {"id": 869, "seek": 545304, "start": 5454.0, "end": 5460.48, "text": " There's various semi-valid theoretical explanations for this, but experimentally,", "tokens": [50412, 821, 311, 3683, 12909, 12, 3337, 327, 20864, 28708, 337, 341, 11, 457, 5120, 379, 11, 50736], "temperature": 0.0, "avg_logprob": -0.18429049096926295, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0005700034671463072}, {"id": 870, "seek": 545304, "start": 5460.48, "end": 5464.24, "text": " it's clear you need to do that. And again, there are schedules that are pre-programmed in PyTorch for", "tokens": [50736, 309, 311, 1850, 291, 643, 281, 360, 300, 13, 400, 797, 11, 456, 366, 28078, 300, 366, 659, 12, 32726, 1912, 294, 9953, 51, 284, 339, 337, 50924], "temperature": 0.0, "avg_logprob": -0.18429049096926295, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0005700034671463072}, {"id": 871, "seek": 545304, "start": 5464.24, "end": 5472.8, "text": " this. Use a bit of L1 or L2 regularization on the weights or combination. Yeah, after you've", "tokens": [50924, 341, 13, 8278, 257, 857, 295, 441, 16, 420, 441, 17, 3890, 2144, 322, 264, 17443, 420, 6562, 13, 865, 11, 934, 291, 600, 51352], "temperature": 0.0, "avg_logprob": -0.18429049096926295, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0005700034671463072}, {"id": 872, "seek": 545304, "start": 5472.8, "end": 5478.16, "text": " trained your system for a few epochs, you might want to kind of prune it, eliminate the weights", "tokens": [51352, 8895, 428, 1185, 337, 257, 1326, 30992, 28346, 11, 291, 1062, 528, 281, 733, 295, 582, 2613, 309, 11, 13819, 264, 17443, 51620], "temperature": 0.0, "avg_logprob": -0.18429049096926295, "compression_ratio": 1.5121951219512195, "no_speech_prob": 0.0005700034671463072}, {"id": 873, "seek": 547816, "start": 5478.16, "end": 5483.84, "text": " that are useless, make sure that the weights have their minimum size. And what you do is you add a", "tokens": [50364, 300, 366, 14115, 11, 652, 988, 300, 264, 17443, 362, 641, 7285, 2744, 13, 400, 437, 291, 360, 307, 291, 909, 257, 50648], "temperature": 0.0, "avg_logprob": -0.15664558620243282, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.0005973887746222317}, {"id": 874, "seek": 547816, "start": 5483.84, "end": 5490.639999999999, "text": " term in the cost function that basically shrinks the weights at every iteration. You might know", "tokens": [50648, 1433, 294, 264, 2063, 2445, 300, 1936, 9884, 16431, 264, 17443, 412, 633, 24784, 13, 509, 1062, 458, 50988], "temperature": 0.0, "avg_logprob": -0.15664558620243282, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.0005973887746222317}, {"id": 875, "seek": 547816, "start": 5490.639999999999, "end": 5494.72, "text": " what L2 and L1 regularization means if you've taken a class in machine learning for large", "tokens": [50988, 437, 441, 17, 293, 441, 16, 3890, 2144, 1355, 498, 291, 600, 2726, 257, 1508, 294, 3479, 2539, 337, 2416, 51192], "temperature": 0.0, "avg_logprob": -0.15664558620243282, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.0005973887746222317}, {"id": 876, "seek": 547816, "start": 5494.72, "end": 5500.24, "text": " secret regression or stuff like that. It's very common. But L2 sometimes is called weight decay.", "tokens": [51192, 4054, 24590, 420, 1507, 411, 300, 13, 467, 311, 588, 2689, 13, 583, 441, 17, 2171, 307, 1219, 3364, 21039, 13, 51468], "temperature": 0.0, "avg_logprob": -0.15664558620243282, "compression_ratio": 1.5743801652892562, "no_speech_prob": 0.0005973887746222317}, {"id": 877, "seek": 550024, "start": 5500.96, "end": 5509.2, "text": " This, again, are pre-programmed in PyTorch. A trick that a lot of people use for large neural", "tokens": [50400, 639, 11, 797, 11, 366, 659, 12, 32726, 1912, 294, 9953, 51, 284, 339, 13, 316, 4282, 300, 257, 688, 295, 561, 764, 337, 2416, 18161, 50812], "temperature": 0.0, "avg_logprob": -0.11558368878486829, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.0004725978651549667}, {"id": 878, "seek": 550024, "start": 5509.2, "end": 5516.5599999999995, "text": " nets is a trick called dropout. Dropout is implemented as kind of a layer in PyTorch. And", "tokens": [50812, 36170, 307, 257, 4282, 1219, 3270, 346, 13, 17675, 346, 307, 12270, 382, 733, 295, 257, 4583, 294, 9953, 51, 284, 339, 13, 400, 51180], "temperature": 0.0, "avg_logprob": -0.11558368878486829, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.0004725978651549667}, {"id": 879, "seek": 550024, "start": 5516.5599999999995, "end": 5524.8, "text": " what this layer does is that it takes the state of a layer and it randomly picks a certain proportion", "tokens": [51180, 437, 341, 4583, 775, 307, 300, 309, 2516, 264, 1785, 295, 257, 4583, 293, 309, 16979, 16137, 257, 1629, 16068, 51592], "temperature": 0.0, "avg_logprob": -0.11558368878486829, "compression_ratio": 1.565934065934066, "no_speech_prob": 0.0004725978651549667}, {"id": 880, "seek": 552480, "start": 5524.8, "end": 5530.88, "text": " of the units and basically sets them to zero. So you can think of it as a mask,", "tokens": [50364, 295, 264, 6815, 293, 1936, 6352, 552, 281, 4018, 13, 407, 291, 393, 519, 295, 309, 382, 257, 6094, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11369436530656712, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.0010646204464137554}, {"id": 881, "seek": 552480, "start": 5532.16, "end": 5537.12, "text": " a layer that applies a mask to an input. And the mask is randomly picked at every sample.", "tokens": [50732, 257, 4583, 300, 13165, 257, 6094, 281, 364, 4846, 13, 400, 264, 6094, 307, 16979, 6183, 412, 633, 6889, 13, 50980], "temperature": 0.0, "avg_logprob": -0.11369436530656712, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.0010646204464137554}, {"id": 882, "seek": 552480, "start": 5538.4800000000005, "end": 5545.04, "text": " And some proportion of the value in the mask are set to zero. Some are set to one. And you", "tokens": [51048, 400, 512, 16068, 295, 264, 2158, 294, 264, 6094, 366, 992, 281, 4018, 13, 2188, 366, 992, 281, 472, 13, 400, 291, 51376], "temperature": 0.0, "avg_logprob": -0.11369436530656712, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.0010646204464137554}, {"id": 883, "seek": 552480, "start": 5545.04, "end": 5550.56, "text": " multiply the input by the mask. So only a subset of the units are allowed to speak to the next", "tokens": [51376, 12972, 264, 4846, 538, 264, 6094, 13, 407, 787, 257, 25993, 295, 264, 6815, 366, 4350, 281, 1710, 281, 264, 958, 51652], "temperature": 0.0, "avg_logprob": -0.11369436530656712, "compression_ratio": 1.7401960784313726, "no_speech_prob": 0.0010646204464137554}, {"id": 884, "seek": 555056, "start": 5550.56, "end": 5556.080000000001, "text": " layer, essentially. That's called dropout. And the reason for doing this is that it forces the", "tokens": [50364, 4583, 11, 4476, 13, 663, 311, 1219, 3270, 346, 13, 400, 264, 1778, 337, 884, 341, 307, 300, 309, 5874, 264, 50640], "temperature": 0.0, "avg_logprob": -0.0672468150534281, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.000511076592374593}, {"id": 885, "seek": 555056, "start": 5556.080000000001, "end": 5563.52, "text": " unit to distribute the information about the input over multiple units instead of kind of", "tokens": [50640, 4985, 281, 20594, 264, 1589, 466, 264, 4846, 670, 3866, 6815, 2602, 295, 733, 295, 51012], "temperature": 0.0, "avg_logprob": -0.0672468150534281, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.000511076592374593}, {"id": 886, "seek": 555056, "start": 5563.52, "end": 5569.68, "text": " squeezing everything into a small number. And it makes the system more robust. There's some", "tokens": [51012, 36645, 1203, 666, 257, 1359, 1230, 13, 400, 309, 1669, 264, 1185, 544, 13956, 13, 821, 311, 512, 51320], "temperature": 0.0, "avg_logprob": -0.0672468150534281, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.000511076592374593}, {"id": 887, "seek": 555056, "start": 5569.68, "end": 5575.68, "text": " theoretical arguments for why it does that. Experimentally, if you add this to a large", "tokens": [51320, 20864, 12869, 337, 983, 309, 775, 300, 13, 37933, 379, 11, 498, 291, 909, 341, 281, 257, 2416, 51620], "temperature": 0.0, "avg_logprob": -0.0672468150534281, "compression_ratio": 1.5921052631578947, "no_speech_prob": 0.000511076592374593}, {"id": 888, "seek": 557568, "start": 5575.68, "end": 5582.16, "text": " network, you get better journalization error. You get better performance on the test set.", "tokens": [50364, 3209, 11, 291, 483, 1101, 6708, 2144, 6713, 13, 509, 483, 1101, 3389, 322, 264, 1500, 992, 13, 50688], "temperature": 0.0, "avg_logprob": -0.11795421304373906, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.0011156166438013315}, {"id": 889, "seek": 557568, "start": 5582.16, "end": 5588.96, "text": " It's not always necessary, but it helps. Okay, there's lots of tricks and I'll devote a lecture", "tokens": [50688, 467, 311, 406, 1009, 4818, 11, 457, 309, 3665, 13, 1033, 11, 456, 311, 3195, 295, 11733, 293, 286, 603, 23184, 257, 7991, 51028], "temperature": 0.0, "avg_logprob": -0.11795421304373906, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.0011156166438013315}, {"id": 890, "seek": 557568, "start": 5588.96, "end": 5594.240000000001, "text": " on this. So I'm not going to go through all of them right now. That requires explaining a bit", "tokens": [51028, 322, 341, 13, 407, 286, 478, 406, 516, 281, 352, 807, 439, 295, 552, 558, 586, 13, 663, 7029, 13468, 257, 857, 51292], "temperature": 0.0, "avg_logprob": -0.11795421304373906, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.0011156166438013315}, {"id": 891, "seek": 557568, "start": 5594.240000000001, "end": 5599.04, "text": " more about optimizations. So really, what deep learning is about, like, I told you everything", "tokens": [51292, 544, 466, 5028, 14455, 13, 407, 534, 11, 437, 2452, 2539, 307, 466, 11, 411, 11, 286, 1907, 291, 1203, 51532], "temperature": 0.0, "avg_logprob": -0.11795421304373906, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.0011156166438013315}, {"id": 892, "seek": 557568, "start": 5599.04, "end": 5603.04, "text": " about deep learning, like the basics of deep learning. What I haven't told you is why we use", "tokens": [51532, 466, 2452, 2539, 11, 411, 264, 14688, 295, 2452, 2539, 13, 708, 286, 2378, 380, 1907, 291, 307, 983, 321, 764, 51732], "temperature": 0.0, "avg_logprob": -0.11795421304373906, "compression_ratio": 1.6945454545454546, "no_speech_prob": 0.0011156166438013315}, {"id": 893, "seek": 560304, "start": 5603.04, "end": 5608.72, "text": " deep learning. Okay, and that's basically what I'm going to tell you about now. The motivation", "tokens": [50364, 2452, 2539, 13, 1033, 11, 293, 300, 311, 1936, 437, 286, 478, 516, 281, 980, 291, 466, 586, 13, 440, 12335, 50648], "temperature": 0.0, "avg_logprob": -0.11505923874076755, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005190277006477118}, {"id": 894, "seek": 560304, "start": 5608.72, "end": 5613.36, "text": " for why is it that we need basically multi-layer neural nets or things of this type.", "tokens": [50648, 337, 983, 307, 309, 300, 321, 643, 1936, 4825, 12, 8376, 260, 18161, 36170, 420, 721, 295, 341, 2010, 13, 50880], "temperature": 0.0, "avg_logprob": -0.11505923874076755, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005190277006477118}, {"id": 895, "seek": 560304, "start": 5615.76, "end": 5622.96, "text": " Okay, so the traditional prototypical model of supervised learning for a very long time", "tokens": [51000, 1033, 11, 370, 264, 5164, 46219, 34061, 2316, 295, 46533, 2539, 337, 257, 588, 938, 565, 51360], "temperature": 0.0, "avg_logprob": -0.11505923874076755, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005190277006477118}, {"id": 896, "seek": 560304, "start": 5622.96, "end": 5630.16, "text": " is basically a linear classifier. A linear classifier for a two-class problem is basically a", "tokens": [51360, 307, 1936, 257, 8213, 1508, 9902, 13, 316, 8213, 1508, 9902, 337, 257, 732, 12, 11665, 1154, 307, 1936, 257, 51720], "temperature": 0.0, "avg_logprob": -0.11505923874076755, "compression_ratio": 1.6901408450704225, "no_speech_prob": 0.0005190277006477118}, {"id": 897, "seek": 563016, "start": 5630.16, "end": 5634.8, "text": " single unit of the similar type that we talked about earlier. You compute a weighted sum of inputs", "tokens": [50364, 2167, 4985, 295, 264, 2531, 2010, 300, 321, 2825, 466, 3071, 13, 509, 14722, 257, 32807, 2408, 295, 15743, 50596], "temperature": 0.0, "avg_logprob": -0.11135445322309222, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0005975427338853478}, {"id": 898, "seek": 563016, "start": 5635.36, "end": 5642.0, "text": " at a bias, and you could think of the bias as just another trainable weight whose corresponding", "tokens": [50624, 412, 257, 12577, 11, 293, 291, 727, 519, 295, 264, 12577, 382, 445, 1071, 3847, 712, 3364, 6104, 11760, 50956], "temperature": 0.0, "avg_logprob": -0.11135445322309222, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0005975427338853478}, {"id": 899, "seek": 563016, "start": 5642.0, "end": 5647.44, "text": " input is equal to one, if you want. And then you pass that through a threshold function,", "tokens": [50956, 4846, 307, 2681, 281, 472, 11, 498, 291, 528, 13, 400, 550, 291, 1320, 300, 807, 257, 14678, 2445, 11, 51228], "temperature": 0.0, "avg_logprob": -0.11135445322309222, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0005975427338853478}, {"id": 900, "seek": 563016, "start": 5647.44, "end": 5652.32, "text": " the sine function, that I put minus one if the weighted sum is below zero and plus one if it's", "tokens": [51228, 264, 18609, 2445, 11, 300, 286, 829, 3175, 472, 498, 264, 32807, 2408, 307, 2507, 4018, 293, 1804, 472, 498, 309, 311, 51472], "temperature": 0.0, "avg_logprob": -0.11135445322309222, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0005975427338853478}, {"id": 901, "seek": 563016, "start": 5652.32, "end": 5659.92, "text": " above zero. Okay, so this basic linear classifier basically partitions the space, the input space", "tokens": [51472, 3673, 4018, 13, 1033, 11, 370, 341, 3875, 8213, 1508, 9902, 1936, 644, 2451, 264, 1901, 11, 264, 4846, 1901, 51852], "temperature": 0.0, "avg_logprob": -0.11135445322309222, "compression_ratio": 1.7246376811594204, "no_speech_prob": 0.0005975427338853478}, {"id": 902, "seek": 565992, "start": 5659.92, "end": 5667.12, "text": " of x's into two half spaces separated by hyperplane. Right, so the equation sum of i, w, i, x, i plus", "tokens": [50364, 295, 2031, 311, 666, 732, 1922, 7673, 12005, 538, 9848, 36390, 13, 1779, 11, 370, 264, 5367, 2408, 295, 741, 11, 261, 11, 741, 11, 2031, 11, 741, 1804, 50724], "temperature": 0.0, "avg_logprob": -0.1333844307625648, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00031994792516343296}, {"id": 903, "seek": 565992, "start": 5667.12, "end": 5673.12, "text": " b equals zero is the surface that separates the category one that is going to produce y bar equal", "tokens": [50724, 272, 6915, 4018, 307, 264, 3753, 300, 34149, 264, 7719, 472, 300, 307, 516, 281, 5258, 288, 2159, 2681, 51024], "temperature": 0.0, "avg_logprob": -0.1333844307625648, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00031994792516343296}, {"id": 904, "seek": 565992, "start": 5673.12, "end": 5680.0, "text": " plus one from category two where y bar equals minus one. Why is it a, why does it divide the", "tokens": [51024, 1804, 472, 490, 7719, 732, 689, 288, 2159, 6915, 3175, 472, 13, 1545, 307, 309, 257, 11, 983, 775, 309, 9845, 264, 51368], "temperature": 0.0, "avg_logprob": -0.1333844307625648, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00031994792516343296}, {"id": 905, "seek": 565992, "start": 5680.0, "end": 5684.96, "text": " space into two halves? It's because you're computing the dot product of an input vector with a weight", "tokens": [51368, 1901, 666, 732, 38490, 30, 467, 311, 570, 291, 434, 15866, 264, 5893, 1674, 295, 364, 4846, 8062, 365, 257, 3364, 51616], "temperature": 0.0, "avg_logprob": -0.1333844307625648, "compression_ratio": 1.6909871244635193, "no_speech_prob": 0.00031994792516343296}, {"id": 906, "seek": 568496, "start": 5684.96, "end": 5692.4, "text": " vector. If those two vectors are orthogonal, then the dot product is zero. Okay, b is just an offset.", "tokens": [50364, 8062, 13, 759, 729, 732, 18875, 366, 41488, 11, 550, 264, 5893, 1674, 307, 4018, 13, 1033, 11, 272, 307, 445, 364, 18687, 13, 50736], "temperature": 0.0, "avg_logprob": -0.08817901736811588, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0006562997004948556}, {"id": 907, "seek": 568496, "start": 5693.44, "end": 5699.52, "text": " So the set of points in x space where this dot product is zero is the set of points that are", "tokens": [50788, 407, 264, 992, 295, 2793, 294, 2031, 1901, 689, 341, 5893, 1674, 307, 4018, 307, 264, 992, 295, 2793, 300, 366, 51092], "temperature": 0.0, "avg_logprob": -0.08817901736811588, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0006562997004948556}, {"id": 908, "seek": 568496, "start": 5699.52, "end": 5707.76, "text": " orthogonal to the vector w. Okay, so in a n-dimensional space, your vector w is a vector,", "tokens": [51092, 41488, 281, 264, 8062, 261, 13, 1033, 11, 370, 294, 257, 297, 12, 18759, 1901, 11, 428, 8062, 261, 307, 257, 8062, 11, 51504], "temperature": 0.0, "avg_logprob": -0.08817901736811588, "compression_ratio": 1.763975155279503, "no_speech_prob": 0.0006562997004948556}, {"id": 909, "seek": 570776, "start": 5708.4800000000005, "end": 5715.04, "text": " and the set of x whose dot product with w is zero is a hyperplane. Right, so it's a linear", "tokens": [50400, 293, 264, 992, 295, 2031, 6104, 5893, 1674, 365, 261, 307, 4018, 307, 257, 9848, 36390, 13, 1779, 11, 370, 309, 311, 257, 8213, 50728], "temperature": 0.0, "avg_logprob": -0.11476978601193895, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0008040148532018065}, {"id": 910, "seek": 570776, "start": 5715.04, "end": 5721.76, "text": " subspace of dimension n minus one. Okay, and that hyperplane divides the space of dimension n into", "tokens": [50728, 2090, 17940, 295, 10139, 297, 3175, 472, 13, 1033, 11, 293, 300, 9848, 36390, 41347, 264, 1901, 295, 10139, 297, 666, 51064], "temperature": 0.0, "avg_logprob": -0.11476978601193895, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0008040148532018065}, {"id": 911, "seek": 570776, "start": 5721.76, "end": 5728.320000000001, "text": " halves. So here is the situation in two dimensions. You have two dimensions x1, x2. You have data", "tokens": [51064, 38490, 13, 407, 510, 307, 264, 2590, 294, 732, 12819, 13, 509, 362, 732, 12819, 2031, 16, 11, 2031, 17, 13, 509, 362, 1412, 51392], "temperature": 0.0, "avg_logprob": -0.11476978601193895, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0008040148532018065}, {"id": 912, "seek": 570776, "start": 5728.320000000001, "end": 5734.24, "text": " points here, the red, the red category and the blue category. And there is a weight vector plus a bias", "tokens": [51392, 2793, 510, 11, 264, 2182, 11, 264, 2182, 7719, 293, 264, 3344, 7719, 13, 400, 456, 307, 257, 3364, 8062, 1804, 257, 12577, 51688], "temperature": 0.0, "avg_logprob": -0.11476978601193895, "compression_ratio": 1.748878923766816, "no_speech_prob": 0.0008040148532018065}, {"id": 913, "seek": 573424, "start": 5734.96, "end": 5740.4, "text": " where the, you know, the intercept here of this sort of green separating line", "tokens": [50400, 689, 264, 11, 291, 458, 11, 264, 24700, 510, 295, 341, 1333, 295, 3092, 29279, 1622, 50672], "temperature": 0.0, "avg_logprob": -0.1477615237236023, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.0002779849455691874}, {"id": 914, "seek": 573424, "start": 5741.12, "end": 5746.8, "text": " with x1 is minus b times divided by w1. So that gives you an idea for what w should be.", "tokens": [50708, 365, 2031, 16, 307, 3175, 272, 1413, 6666, 538, 261, 16, 13, 407, 300, 2709, 291, 364, 1558, 337, 437, 261, 820, 312, 13, 50992], "temperature": 0.0, "avg_logprob": -0.1477615237236023, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.0002779849455691874}, {"id": 915, "seek": 573424, "start": 5747.5199999999995, "end": 5754.719999999999, "text": " And the w vector is orthogonal to that separating surface. Okay, so changing b will", "tokens": [51028, 400, 264, 261, 8062, 307, 41488, 281, 300, 29279, 3753, 13, 1033, 11, 370, 4473, 272, 486, 51388], "temperature": 0.0, "avg_logprob": -0.1477615237236023, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.0002779849455691874}, {"id": 916, "seek": 573424, "start": 5754.719999999999, "end": 5758.32, "text": " change the position and then changing w will change the orientation basically.", "tokens": [51388, 1319, 264, 2535, 293, 550, 4473, 261, 486, 1319, 264, 14764, 1936, 13, 51568], "temperature": 0.0, "avg_logprob": -0.1477615237236023, "compression_ratio": 1.6157635467980296, "no_speech_prob": 0.0002779849455691874}, {"id": 917, "seek": 575832, "start": 5758.88, "end": 5766.799999999999, "text": " Now, what about situations like this where the points are, the red and blue points are not", "tokens": [50392, 823, 11, 437, 466, 6851, 411, 341, 689, 264, 2793, 366, 11, 264, 2182, 293, 3344, 2793, 366, 406, 50788], "temperature": 0.0, "avg_logprob": -0.14681928808038885, "compression_ratio": 1.5368421052631578, "no_speech_prob": 8.219582377932966e-05}, {"id": 918, "seek": 575832, "start": 5766.799999999999, "end": 5774.32, "text": " separable by a hyperplane? That's called a non-linearly separable case. So there you can't use a linear", "tokens": [50788, 3128, 712, 538, 257, 9848, 36390, 30, 663, 311, 1219, 257, 2107, 12, 28263, 356, 3128, 712, 1389, 13, 407, 456, 291, 393, 380, 764, 257, 8213, 51164], "temperature": 0.0, "avg_logprob": -0.14681928808038885, "compression_ratio": 1.5368421052631578, "no_speech_prob": 8.219582377932966e-05}, {"id": 919, "seek": 575832, "start": 5774.32, "end": 5781.599999999999, "text": " classifier to separate those. What are we going to do? In fact, there is a theorem that goes back", "tokens": [51164, 1508, 9902, 281, 4994, 729, 13, 708, 366, 321, 516, 281, 360, 30, 682, 1186, 11, 456, 307, 257, 20904, 300, 1709, 646, 51528], "temperature": 0.0, "avg_logprob": -0.14681928808038885, "compression_ratio": 1.5368421052631578, "no_speech_prob": 8.219582377932966e-05}, {"id": 920, "seek": 578160, "start": 5781.6, "end": 5789.04, "text": " to 1966 by Tom Kovar, who died recently actually. It was a Stanford that says the probability that", "tokens": [50364, 281, 39157, 538, 5041, 591, 5179, 289, 11, 567, 4539, 3938, 767, 13, 467, 390, 257, 20374, 300, 1619, 264, 8482, 300, 50736], "temperature": 0.0, "avg_logprob": -0.129879846572876, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0022512124851346016}, {"id": 921, "seek": 578160, "start": 5789.04, "end": 5796.08, "text": " a particular separation of p points is linearly separable in n dimension is close to one when p", "tokens": [50736, 257, 1729, 14634, 295, 280, 2793, 307, 43586, 3128, 712, 294, 297, 10139, 307, 1998, 281, 472, 562, 280, 51088], "temperature": 0.0, "avg_logprob": -0.129879846572876, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0022512124851346016}, {"id": 922, "seek": 578160, "start": 5796.08, "end": 5802.400000000001, "text": " is smaller than n, but it's close to zero when p is larger than n. In other words, if you, if you", "tokens": [51088, 307, 4356, 813, 297, 11, 457, 309, 311, 1998, 281, 4018, 562, 280, 307, 4833, 813, 297, 13, 682, 661, 2283, 11, 498, 291, 11, 498, 291, 51404], "temperature": 0.0, "avg_logprob": -0.129879846572876, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0022512124851346016}, {"id": 923, "seek": 578160, "start": 5802.400000000001, "end": 5807.52, "text": " take an n-dimensional space, you throw p random points in that n-dimensional space, data points,", "tokens": [51404, 747, 364, 297, 12, 18759, 1901, 11, 291, 3507, 280, 4974, 2793, 294, 300, 297, 12, 18759, 1901, 11, 1412, 2793, 11, 51660], "temperature": 0.0, "avg_logprob": -0.129879846572876, "compression_ratio": 1.6986899563318778, "no_speech_prob": 0.0022512124851346016}, {"id": 924, "seek": 580752, "start": 5807.52, "end": 5816.320000000001, "text": " okay? And you randomly label them blue and red. You ask the question, what is the probability that", "tokens": [50364, 1392, 30, 400, 291, 16979, 7645, 552, 3344, 293, 2182, 13, 509, 1029, 264, 1168, 11, 437, 307, 264, 8482, 300, 50804], "temperature": 0.0, "avg_logprob": -0.07620648948513732, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.00021990695677231997}, {"id": 925, "seek": 580752, "start": 5816.320000000001, "end": 5821.84, "text": " that particular dichotomy is linearly separable? I can separate the blue points from the red points", "tokens": [50804, 300, 1729, 10390, 310, 8488, 307, 43586, 3128, 712, 30, 286, 393, 4994, 264, 3344, 2793, 490, 264, 2182, 2793, 51080], "temperature": 0.0, "avg_logprob": -0.07620648948513732, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.00021990695677231997}, {"id": 926, "seek": 580752, "start": 5821.84, "end": 5827.6, "text": " with a hyperplane. And the answer is, if p is less than n, you have a good chance that they", "tokens": [51080, 365, 257, 9848, 36390, 13, 400, 264, 1867, 307, 11, 498, 280, 307, 1570, 813, 297, 11, 291, 362, 257, 665, 2931, 300, 436, 51368], "temperature": 0.0, "avg_logprob": -0.07620648948513732, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.00021990695677231997}, {"id": 927, "seek": 580752, "start": 5827.6, "end": 5832.0, "text": " will be separable. If p is larger than n, you basically have no chance that they will. Okay,", "tokens": [51368, 486, 312, 3128, 712, 13, 759, 280, 307, 4833, 813, 297, 11, 291, 1936, 362, 572, 2931, 300, 436, 486, 13, 1033, 11, 51588], "temperature": 0.0, "avg_logprob": -0.07620648948513732, "compression_ratio": 1.7098214285714286, "no_speech_prob": 0.00021990695677231997}, {"id": 928, "seek": 583200, "start": 5832.0, "end": 5839.2, "text": " so if you have an image classification problem, let's say, and you have tons of examples,", "tokens": [50364, 370, 498, 291, 362, 364, 3256, 21538, 1154, 11, 718, 311, 584, 11, 293, 291, 362, 9131, 295, 5110, 11, 50724], "temperature": 0.0, "avg_logprob": -0.12264462521201686, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0006562244961969554}, {"id": 929, "seek": 583200, "start": 5840.72, "end": 5845.68, "text": " way bigger. So let's say you do n-nist. So n-nist is a dataset of handwritten digits.", "tokens": [50800, 636, 3801, 13, 407, 718, 311, 584, 291, 360, 297, 12, 77, 468, 13, 407, 297, 12, 77, 468, 307, 257, 28872, 295, 1011, 26859, 27011, 13, 51048], "temperature": 0.0, "avg_logprob": -0.12264462521201686, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0006562244961969554}, {"id": 930, "seek": 583200, "start": 5845.68, "end": 5850.32, "text": " The images are 28 by 28 pixels. In fact, the intrinsic dimension is smaller because some", "tokens": [51048, 440, 5267, 366, 7562, 538, 7562, 18668, 13, 682, 1186, 11, 264, 35698, 10139, 307, 4356, 570, 512, 51280], "temperature": 0.0, "avg_logprob": -0.12264462521201686, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0006562244961969554}, {"id": 931, "seek": 583200, "start": 5850.32, "end": 5856.96, "text": " pixels are always zero. And you have 60,000 samples. The probability that those 60,000", "tokens": [51280, 18668, 366, 1009, 4018, 13, 400, 291, 362, 4060, 11, 1360, 10938, 13, 440, 8482, 300, 729, 4060, 11, 1360, 51612], "temperature": 0.0, "avg_logprob": -0.12264462521201686, "compression_ratio": 1.6027397260273972, "no_speech_prob": 0.0006562244961969554}, {"id": 932, "seek": 585696, "start": 5856.96, "end": 5862.08, "text": " samples of, let's say, zeros from everything else or ones from everything else is nearly separable", "tokens": [50364, 10938, 295, 11, 718, 311, 584, 11, 35193, 490, 1203, 1646, 420, 2306, 490, 1203, 1646, 307, 6217, 3128, 712, 50620], "temperature": 0.0, "avg_logprob": -0.1397116638365246, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.00016091937141027302}, {"id": 933, "seek": 585696, "start": 5862.88, "end": 5871.44, "text": " is basically nil. So, which is why people invented the classical model of pattern", "tokens": [50660, 307, 1936, 297, 388, 13, 407, 11, 597, 307, 983, 561, 14479, 264, 13735, 2316, 295, 5102, 51088], "temperature": 0.0, "avg_logprob": -0.1397116638365246, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.00016091937141027302}, {"id": 934, "seek": 585696, "start": 5871.44, "end": 5878.16, "text": " recognition. We consist in taking an input, engineering a feature extractor to produce a", "tokens": [51088, 11150, 13, 492, 4603, 294, 1940, 364, 4846, 11, 7043, 257, 4111, 8947, 284, 281, 5258, 257, 51424], "temperature": 0.0, "avg_logprob": -0.1397116638365246, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.00016091937141027302}, {"id": 935, "seek": 585696, "start": 5878.16, "end": 5883.28, "text": " representation in such a way that in that space now, your problem becomes, let's say, linearly", "tokens": [51424, 10290, 294, 1270, 257, 636, 300, 294, 300, 1901, 586, 11, 428, 1154, 3643, 11, 718, 311, 584, 11, 43586, 51680], "temperature": 0.0, "avg_logprob": -0.1397116638365246, "compression_ratio": 1.6322869955156951, "no_speech_prob": 0.00016091937141027302}, {"id": 936, "seek": 588328, "start": 5883.28, "end": 5887.28, "text": " separable if you use a linear classifier or some other separability if you use another type of", "tokens": [50364, 3128, 712, 498, 291, 764, 257, 8213, 1508, 9902, 420, 512, 661, 3128, 2310, 498, 291, 764, 1071, 2010, 295, 50564], "temperature": 0.0, "avg_logprob": -0.09364235603203208, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.000816541607491672}, {"id": 937, "seek": 588328, "start": 5887.28, "end": 5895.28, "text": " classifier. Okay? Now, necessarily, this feature extraction must be nonlinear itself. If the only", "tokens": [50564, 1508, 9902, 13, 1033, 30, 823, 11, 4725, 11, 341, 4111, 30197, 1633, 312, 2107, 28263, 2564, 13, 759, 264, 787, 50964], "temperature": 0.0, "avg_logprob": -0.09364235603203208, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.000816541607491672}, {"id": 938, "seek": 588328, "start": 5895.28, "end": 5899.599999999999, "text": " thing it does is some affine transformation of the input, it's not going to make a nonlinearly", "tokens": [50964, 551, 309, 775, 307, 512, 2096, 533, 9887, 295, 264, 4846, 11, 309, 311, 406, 516, 281, 652, 257, 2107, 28263, 356, 51180], "temperature": 0.0, "avg_logprob": -0.09364235603203208, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.000816541607491672}, {"id": 939, "seek": 588328, "start": 5899.599999999999, "end": 5906.48, "text": " separable problem into a linear separable one, right? So, necessarily, this feature extractor", "tokens": [51180, 3128, 712, 1154, 666, 257, 8213, 3128, 712, 472, 11, 558, 30, 407, 11, 4725, 11, 341, 4111, 8947, 284, 51524], "temperature": 0.0, "avg_logprob": -0.09364235603203208, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.000816541607491672}, {"id": 940, "seek": 588328, "start": 5906.48, "end": 5911.44, "text": " has to be nonlinear. This is very important to remember. Okay? A linear preprocessing doesn't", "tokens": [51524, 575, 281, 312, 2107, 28263, 13, 639, 307, 588, 1021, 281, 1604, 13, 1033, 30, 316, 8213, 2666, 340, 780, 278, 1177, 380, 51772], "temperature": 0.0, "avg_logprob": -0.09364235603203208, "compression_ratio": 1.8482490272373542, "no_speech_prob": 0.000816541607491672}, {"id": 941, "seek": 591144, "start": 5911.44, "end": 5916.879999999999, "text": " do anything for you, essentially. So, people spend decades in computer vision, for example,", "tokens": [50364, 360, 1340, 337, 291, 11, 4476, 13, 407, 11, 561, 3496, 7878, 294, 3820, 5201, 11, 337, 1365, 11, 50636], "temperature": 0.0, "avg_logprob": -0.11467117733425564, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0009249306167475879}, {"id": 942, "seek": 591144, "start": 5916.879999999999, "end": 5922.24, "text": " as feature recognition, devising good feature extractors for particular problems. You know,", "tokens": [50636, 382, 4111, 11150, 11, 1905, 3436, 665, 4111, 8947, 830, 337, 1729, 2740, 13, 509, 458, 11, 50904], "temperature": 0.0, "avg_logprob": -0.11467117733425564, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0009249306167475879}, {"id": 943, "seek": 591144, "start": 5922.24, "end": 5926.719999999999, "text": " what features are good to do face recognition, for example, right? Can I do things like detect the", "tokens": [50904, 437, 4122, 366, 665, 281, 360, 1851, 11150, 11, 337, 1365, 11, 558, 30, 1664, 286, 360, 721, 411, 5531, 264, 51128], "temperature": 0.0, "avg_logprob": -0.11467117733425564, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0009249306167475879}, {"id": 944, "seek": 591144, "start": 5926.719999999999, "end": 5930.879999999999, "text": " eyes and then measure the ratio between the separation of the eyes with the separation from", "tokens": [51128, 2575, 293, 550, 3481, 264, 8509, 1296, 264, 14634, 295, 264, 2575, 365, 264, 14634, 490, 51336], "temperature": 0.0, "avg_logprob": -0.11467117733425564, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0009249306167475879}, {"id": 945, "seek": 591144, "start": 5930.879999999999, "end": 5935.679999999999, "text": " the mouth and then, you know, computes a few features like this and then feed that to a classifier", "tokens": [51336, 264, 4525, 293, 550, 11, 291, 458, 11, 715, 1819, 257, 1326, 4122, 411, 341, 293, 550, 3154, 300, 281, 257, 1508, 9902, 51576], "temperature": 0.0, "avg_logprob": -0.11467117733425564, "compression_ratio": 1.8192307692307692, "no_speech_prob": 0.0009249306167475879}, {"id": 946, "seek": 593568, "start": 5935.68, "end": 5942.0, "text": " and figure out who the person is. So, most papers, you know, between, let's say, the 1960s", "tokens": [50364, 293, 2573, 484, 567, 264, 954, 307, 13, 407, 11, 881, 10577, 11, 291, 458, 11, 1296, 11, 718, 311, 584, 11, 264, 16157, 82, 50680], "temperature": 0.0, "avg_logprob": -0.11628357569376628, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.0013449122197926044}, {"id": 947, "seek": 593568, "start": 5942.0, "end": 5950.240000000001, "text": " or 70s and the late 2000s or early 2010s in computer vision were essentially about that,", "tokens": [50680, 420, 5285, 82, 293, 264, 3469, 8132, 82, 420, 2440, 9657, 82, 294, 3820, 5201, 645, 4476, 466, 300, 11, 51092], "temperature": 0.0, "avg_logprob": -0.11628357569376628, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.0013449122197926044}, {"id": 948, "seek": 593568, "start": 5950.240000000001, "end": 5956.56, "text": " like how you represent images properly. Not all of them, okay? A lot of them for recognition.", "tokens": [51092, 411, 577, 291, 2906, 5267, 6108, 13, 1726, 439, 295, 552, 11, 1392, 30, 316, 688, 295, 552, 337, 11150, 13, 51408], "temperature": 0.0, "avg_logprob": -0.11628357569376628, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.0013449122197926044}, {"id": 949, "seek": 593568, "start": 5958.16, "end": 5964.64, "text": " And a lot of people kind of devise very sort of generic ways of devising feature extractors.", "tokens": [51488, 400, 257, 688, 295, 561, 733, 295, 1905, 908, 588, 1333, 295, 19577, 2098, 295, 1905, 3436, 4111, 8947, 830, 13, 51812], "temperature": 0.0, "avg_logprob": -0.11628357569376628, "compression_ratio": 1.512396694214876, "no_speech_prob": 0.0013449122197926044}, {"id": 950, "seek": 596568, "start": 5965.76, "end": 5971.52, "text": " The basic idea is you just expand the dimension of the representation in a nonlinear way so that now", "tokens": [50368, 440, 3875, 1558, 307, 291, 445, 5268, 264, 10139, 295, 264, 10290, 294, 257, 2107, 28263, 636, 370, 300, 586, 50656], "temperature": 0.0, "avg_logprob": -0.13219620765896017, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0002453360939398408}, {"id": 951, "seek": 596568, "start": 5971.52, "end": 5975.4400000000005, "text": " your number of dimensions is larger than the number of samples. And now your problem has", "tokens": [50656, 428, 1230, 295, 12819, 307, 4833, 813, 264, 1230, 295, 10938, 13, 400, 586, 428, 1154, 575, 50852], "temperature": 0.0, "avg_logprob": -0.13219620765896017, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0002453360939398408}, {"id": 952, "seek": 596568, "start": 5975.4400000000005, "end": 5980.0, "text": " a chance of becoming linearly separable. So, the ideas that I'm not going to go through,", "tokens": [50852, 257, 2931, 295, 5617, 43586, 3128, 712, 13, 407, 11, 264, 3487, 300, 286, 478, 406, 516, 281, 352, 807, 11, 51080], "temperature": 0.0, "avg_logprob": -0.13219620765896017, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0002453360939398408}, {"id": 953, "seek": 596568, "start": 5980.0, "end": 5984.0, "text": " like space styling, random projection. So, random projection basically is a very simple idea.", "tokens": [51080, 411, 1901, 27944, 11, 4974, 22743, 13, 407, 11, 4974, 22743, 1936, 307, 257, 588, 2199, 1558, 13, 51280], "temperature": 0.0, "avg_logprob": -0.13219620765896017, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0002453360939398408}, {"id": 954, "seek": 596568, "start": 5984.0, "end": 5993.52, "text": " You take your input vectors, you multiply them by random matrix, okay? And then you pass the result", "tokens": [51280, 509, 747, 428, 4846, 18875, 11, 291, 12972, 552, 538, 4974, 8141, 11, 1392, 30, 400, 550, 291, 1320, 264, 1874, 51756], "temperature": 0.0, "avg_logprob": -0.13219620765896017, "compression_ratio": 1.728937728937729, "no_speech_prob": 0.0002453360939398408}, {"id": 955, "seek": 599352, "start": 5993.52, "end": 5998.88, "text": " through some nonlinear operation, okay? That's called random projection.", "tokens": [50364, 807, 512, 2107, 28263, 6916, 11, 1392, 30, 663, 311, 1219, 4974, 22743, 13, 50632], "temperature": 0.0, "avg_logprob": -0.08093435981056907, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0004727393388748169}, {"id": 956, "seek": 599352, "start": 6000.240000000001, "end": 6005.200000000001, "text": " And it might make, if the dimension of the output is larger than the dimension of the input,", "tokens": [50700, 400, 309, 1062, 652, 11, 498, 264, 10139, 295, 264, 5598, 307, 4833, 813, 264, 10139, 295, 264, 4846, 11, 50948], "temperature": 0.0, "avg_logprob": -0.08093435981056907, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0004727393388748169}, {"id": 957, "seek": 599352, "start": 6005.200000000001, "end": 6010.4800000000005, "text": " it might make a nonlinearly separable problem linearly separable. It's very efficient because,", "tokens": [50948, 309, 1062, 652, 257, 2107, 28263, 356, 3128, 712, 1154, 43586, 3128, 712, 13, 467, 311, 588, 7148, 570, 11, 51212], "temperature": 0.0, "avg_logprob": -0.08093435981056907, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0004727393388748169}, {"id": 958, "seek": 599352, "start": 6011.4400000000005, "end": 6016.080000000001, "text": " you know, you might need a very large number of those, of this dimension to be able to kind of", "tokens": [51260, 291, 458, 11, 291, 1062, 643, 257, 588, 2416, 1230, 295, 729, 11, 295, 341, 10139, 281, 312, 1075, 281, 733, 295, 51492], "temperature": 0.0, "avg_logprob": -0.08093435981056907, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0004727393388748169}, {"id": 959, "seek": 599352, "start": 6016.080000000001, "end": 6020.8, "text": " do a good job. But it works in certain cases and you don't have to train the first layer,", "tokens": [51492, 360, 257, 665, 1691, 13, 583, 309, 1985, 294, 1629, 3331, 293, 291, 500, 380, 362, 281, 3847, 264, 700, 4583, 11, 51728], "temperature": 0.0, "avg_logprob": -0.08093435981056907, "compression_ratio": 1.7658730158730158, "no_speech_prob": 0.0004727393388748169}, {"id": 960, "seek": 602080, "start": 6020.8, "end": 6024.88, "text": " you basically pick it randomly. And so, the only thing you need to train is a linear classifier", "tokens": [50364, 291, 1936, 1888, 309, 16979, 13, 400, 370, 11, 264, 787, 551, 291, 643, 281, 3847, 307, 257, 8213, 1508, 9902, 50568], "temperature": 0.0, "avg_logprob": -0.1405417610617245, "compression_ratio": 1.65, "no_speech_prob": 0.0007318303105421364}, {"id": 961, "seek": 602080, "start": 6024.88, "end": 6029.52, "text": " on top. It's polynomial classifiers, which I'll talk about in a minute, in a minute,", "tokens": [50568, 322, 1192, 13, 467, 311, 26110, 1508, 23463, 11, 597, 286, 603, 751, 466, 294, 257, 3456, 11, 294, 257, 3456, 11, 50800], "temperature": 0.0, "avg_logprob": -0.1405417610617245, "compression_ratio": 1.65, "no_speech_prob": 0.0007318303105421364}, {"id": 962, "seek": 602080, "start": 6029.52, "end": 6036.0, "text": " radio basis functions and kernel machines. So, those are basically techniques to turn", "tokens": [50800, 6477, 5143, 6828, 293, 28256, 8379, 13, 407, 11, 729, 366, 1936, 7512, 281, 1261, 51124], "temperature": 0.0, "avg_logprob": -0.1405417610617245, "compression_ratio": 1.65, "no_speech_prob": 0.0007318303105421364}, {"id": 963, "seek": 602080, "start": 6036.96, "end": 6045.04, "text": " an input into a representation that then will be essentially classifiable by a simple classifier", "tokens": [51172, 364, 4846, 666, 257, 10290, 300, 550, 486, 312, 4476, 1508, 30876, 538, 257, 2199, 1508, 9902, 51576], "temperature": 0.0, "avg_logprob": -0.1405417610617245, "compression_ratio": 1.65, "no_speech_prob": 0.0007318303105421364}, {"id": 964, "seek": 604504, "start": 6045.04, "end": 6051.68, "text": " like a linear classifier. So, what's a polynomial classifier? A polynomial classifier, basically,", "tokens": [50364, 411, 257, 8213, 1508, 9902, 13, 407, 11, 437, 311, 257, 26110, 1508, 9902, 30, 316, 26110, 1508, 9902, 11, 1936, 11, 50696], "temperature": 0.0, "avg_logprob": -0.0974560966176435, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0008038992527872324}, {"id": 965, "seek": 604504, "start": 6051.68, "end": 6056.32, "text": " imagine that your input vector has two dimensions. The way you increase the dimensionality of the", "tokens": [50696, 3811, 300, 428, 4846, 8062, 575, 732, 12819, 13, 440, 636, 291, 3488, 264, 10139, 1860, 295, 264, 50928], "temperature": 0.0, "avg_logprob": -0.0974560966176435, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0008038992527872324}, {"id": 966, "seek": 604504, "start": 6056.32, "end": 6062.08, "text": " representation is that you take each of the input variables, but you also take every product of", "tokens": [50928, 10290, 307, 300, 291, 747, 1184, 295, 264, 4846, 9102, 11, 457, 291, 611, 747, 633, 1674, 295, 51216], "temperature": 0.0, "avg_logprob": -0.0974560966176435, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0008038992527872324}, {"id": 967, "seek": 604504, "start": 6062.08, "end": 6067.36, "text": " pairs of input variables, right? So, now you have a new feature vector, which is composed of x1,", "tokens": [51216, 15494, 295, 4846, 9102, 11, 558, 30, 407, 11, 586, 291, 362, 257, 777, 4111, 8062, 11, 597, 307, 18204, 295, 2031, 16, 11, 51480], "temperature": 0.0, "avg_logprob": -0.0974560966176435, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0008038992527872324}, {"id": 968, "seek": 604504, "start": 6067.36, "end": 6073.12, "text": " x2, you add one for the bias, and then also x1 times x2, x1 squared and x2 squared.", "tokens": [51480, 2031, 17, 11, 291, 909, 472, 337, 264, 12577, 11, 293, 550, 611, 2031, 16, 1413, 2031, 17, 11, 2031, 16, 8889, 293, 2031, 17, 8889, 13, 51768], "temperature": 0.0, "avg_logprob": -0.0974560966176435, "compression_ratio": 1.7744360902255638, "no_speech_prob": 0.0008038992527872324}, {"id": 969, "seek": 607312, "start": 6073.12, "end": 6079.84, "text": " So, when you do a linear classification in that space, what you're doing really is a quadratic", "tokens": [50364, 407, 11, 562, 291, 360, 257, 8213, 21538, 294, 300, 1901, 11, 437, 291, 434, 884, 534, 307, 257, 37262, 50700], "temperature": 0.0, "avg_logprob": -0.10597402784559462, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.00018808452296070755}, {"id": 970, "seek": 607312, "start": 6081.12, "end": 6085.76, "text": " classification in the original space, right? The surface, the separating surface in the", "tokens": [50764, 21538, 294, 264, 3380, 1901, 11, 558, 30, 440, 3753, 11, 264, 29279, 3753, 294, 264, 50996], "temperature": 0.0, "avg_logprob": -0.10597402784559462, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.00018808452296070755}, {"id": 971, "seek": 607312, "start": 6085.76, "end": 6094.08, "text": " original space now is a quadratic curve into dimension. In n dimension, it's a quadratic", "tokens": [50996, 3380, 1901, 586, 307, 257, 37262, 7605, 666, 10139, 13, 682, 297, 10139, 11, 309, 311, 257, 37262, 51412], "temperature": 0.0, "avg_logprob": -0.10597402784559462, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.00018808452296070755}, {"id": 972, "seek": 607312, "start": 6094.08, "end": 6102.24, "text": " hypersurface, basically. So, it could be a parabola or ellipse or hyperbola, depending on the", "tokens": [51412, 7420, 433, 374, 2868, 11, 1936, 13, 407, 11, 309, 727, 312, 257, 45729, 4711, 420, 8284, 48041, 420, 9848, 65, 4711, 11, 5413, 322, 264, 51820], "temperature": 0.0, "avg_logprob": -0.10597402784559462, "compression_ratio": 1.8434343434343434, "no_speech_prob": 0.00018808452296070755}, {"id": 973, "seek": 610224, "start": 6102.24, "end": 6106.32, "text": " coefficients, right? Now, the problem with this is that it doesn't work very well in high", "tokens": [50364, 31994, 11, 558, 30, 823, 11, 264, 1154, 365, 341, 307, 300, 309, 1177, 380, 589, 588, 731, 294, 1090, 50568], "temperature": 0.0, "avg_logprob": -0.13039325574122437, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.0004653910582419485}, {"id": 974, "seek": 610224, "start": 6106.32, "end": 6111.04, "text": " dimension because the number of features grows with a square of the number of inputs. So, if you", "tokens": [50568, 10139, 570, 264, 1230, 295, 4122, 13156, 365, 257, 3732, 295, 264, 1230, 295, 15743, 13, 407, 11, 498, 291, 50804], "temperature": 0.0, "avg_logprob": -0.13039325574122437, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.0004653910582419485}, {"id": 975, "seek": 610224, "start": 6111.04, "end": 6117.5199999999995, "text": " want to apply this to get an ImageNet type image, you know, the resolution is 256 by 256 by 3,", "tokens": [50804, 528, 281, 3079, 341, 281, 483, 364, 29903, 31890, 2010, 3256, 11, 291, 458, 11, 264, 8669, 307, 38882, 538, 38882, 538, 805, 11, 51128], "temperature": 0.0, "avg_logprob": -0.13039325574122437, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.0004653910582419485}, {"id": 976, "seek": 610224, "start": 6117.5199999999995, "end": 6122.719999999999, "text": " because you have color channels. That's already a high dimension. If you take the cross product of", "tokens": [51128, 570, 291, 362, 2017, 9235, 13, 663, 311, 1217, 257, 1090, 10139, 13, 759, 291, 747, 264, 3278, 1674, 295, 51388], "temperature": 0.0, "avg_logprob": -0.13039325574122437, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.0004653910582419485}, {"id": 977, "seek": 610224, "start": 6122.719999999999, "end": 6127.2, "text": " all of those variables, that's way too large, okay?", "tokens": [51388, 439, 295, 729, 9102, 11, 300, 311, 636, 886, 2416, 11, 1392, 30, 51612], "temperature": 0.0, "avg_logprob": -0.13039325574122437, "compression_ratio": 1.6119402985074627, "no_speech_prob": 0.0004653910582419485}, {"id": 978, "seek": 612720, "start": 6128.0, "end": 6135.04, "text": " So, it's not really practical for high dimensional problems, but it's a trick. Now, here is,", "tokens": [50404, 407, 11, 309, 311, 406, 534, 8496, 337, 1090, 18795, 2740, 11, 457, 309, 311, 257, 4282, 13, 823, 11, 510, 307, 11, 50756], "temperature": 0.0, "avg_logprob": -0.1716173673930921, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.00019108463311567903}, {"id": 979, "seek": 612720, "start": 6135.92, "end": 6141.76, "text": " so, super vector machines are basically two-layer networks of kernel machines more generally,", "tokens": [50800, 370, 11, 1687, 8062, 8379, 366, 1936, 732, 12, 8376, 260, 9590, 295, 28256, 8379, 544, 5101, 11, 51092], "temperature": 0.0, "avg_logprob": -0.1716173673930921, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.00019108463311567903}, {"id": 980, "seek": 612720, "start": 6142.4, "end": 6149.04, "text": " are two-layer systems in which the first layer has as many dimensions as you have training samples.", "tokens": [51124, 366, 732, 12, 8376, 260, 3652, 294, 597, 264, 700, 4583, 575, 382, 867, 12819, 382, 291, 362, 3097, 10938, 13, 51456], "temperature": 0.0, "avg_logprob": -0.1716173673930921, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.00019108463311567903}, {"id": 981, "seek": 612720, "start": 6150.0, "end": 6154.639999999999, "text": " Okay? So, for each training sample, you create a inner-on, a unit, if you want,", "tokens": [51504, 1033, 30, 407, 11, 337, 1184, 3097, 6889, 11, 291, 1884, 257, 7284, 12, 266, 11, 257, 4985, 11, 498, 291, 528, 11, 51736], "temperature": 0.0, "avg_logprob": -0.1716173673930921, "compression_ratio": 1.6123348017621146, "no_speech_prob": 0.00019108463311567903}, {"id": 982, "seek": 615464, "start": 6155.52, "end": 6160.96, "text": " and the role of this unit is to produce a large output if the input vector matches one of the", "tokens": [50408, 293, 264, 3090, 295, 341, 4985, 307, 281, 5258, 257, 2416, 5598, 498, 264, 4846, 8062, 10676, 472, 295, 264, 50680], "temperature": 0.0, "avg_logprob": -0.09205960826713498, "compression_ratio": 1.934959349593496, "no_speech_prob": 0.00036255380837246776}, {"id": 983, "seek": 615464, "start": 6160.96, "end": 6167.12, "text": " training samples and a small output if it doesn't, or the other way around. A small output if it", "tokens": [50680, 3097, 10938, 293, 257, 1359, 5598, 498, 309, 1177, 380, 11, 420, 264, 661, 636, 926, 13, 316, 1359, 5598, 498, 309, 50988], "temperature": 0.0, "avg_logprob": -0.09205960826713498, "compression_ratio": 1.934959349593496, "no_speech_prob": 0.00036255380837246776}, {"id": 984, "seek": 615464, "start": 6167.12, "end": 6171.12, "text": " matches, a large output if it doesn't, okay? It doesn't really matter, but it has to be nonlinear.", "tokens": [50988, 10676, 11, 257, 2416, 5598, 498, 309, 1177, 380, 11, 1392, 30, 467, 1177, 380, 534, 1871, 11, 457, 309, 575, 281, 312, 2107, 28263, 13, 51188], "temperature": 0.0, "avg_logprob": -0.09205960826713498, "compression_ratio": 1.934959349593496, "no_speech_prob": 0.00036255380837246776}, {"id": 985, "seek": 615464, "start": 6171.76, "end": 6175.6, "text": " So, something like, you know, compute the dot product of the input by one of the training", "tokens": [51220, 407, 11, 746, 411, 11, 291, 458, 11, 14722, 264, 5893, 1674, 295, 264, 4846, 538, 472, 295, 264, 3097, 51412], "temperature": 0.0, "avg_logprob": -0.09205960826713498, "compression_ratio": 1.934959349593496, "no_speech_prob": 0.00036255380837246776}, {"id": 986, "seek": 615464, "start": 6175.6, "end": 6181.6, "text": " samples and passes through, you know, a negative exponential or a square or something like that.", "tokens": [51412, 10938, 293, 11335, 807, 11, 291, 458, 11, 257, 3671, 21510, 420, 257, 3732, 420, 746, 411, 300, 13, 51712], "temperature": 0.0, "avg_logprob": -0.09205960826713498, "compression_ratio": 1.934959349593496, "no_speech_prob": 0.00036255380837246776}, {"id": 987, "seek": 618160, "start": 6181.92, "end": 6188.0, "text": " So, this gives you how much the input vector resembles one of the training samples, and you", "tokens": [50380, 407, 11, 341, 2709, 291, 577, 709, 264, 4846, 8062, 34433, 472, 295, 264, 3097, 10938, 11, 293, 291, 50684], "temperature": 0.0, "avg_logprob": -0.2095907780162075, "compression_ratio": 1.8045977011494252, "no_speech_prob": 6.204572855494916e-05}, {"id": 988, "seek": 618160, "start": 6188.0, "end": 6194.0, "text": " do this for every single training samples, okay? And then you train a linear classifier basically", "tokens": [50684, 360, 341, 337, 633, 2167, 3097, 10938, 11, 1392, 30, 400, 550, 291, 3847, 257, 8213, 1508, 9902, 1936, 50984], "temperature": 0.0, "avg_logprob": -0.2095907780162075, "compression_ratio": 1.8045977011494252, "no_speech_prob": 6.204572855494916e-05}, {"id": 989, "seek": 618160, "start": 6194.0, "end": 6199.200000000001, "text": " to use those inputs as, you know, as input to a linear classifier. You compute the weight so", "tokens": [50984, 281, 764, 729, 15743, 382, 11, 291, 458, 11, 382, 4846, 281, 257, 8213, 1508, 9902, 13, 509, 14722, 264, 3364, 370, 51244], "temperature": 0.0, "avg_logprob": -0.2095907780162075, "compression_ratio": 1.8045977011494252, "no_speech_prob": 6.204572855494916e-05}, {"id": 990, "seek": 618160, "start": 6199.200000000001, "end": 6204.320000000001, "text": " that linear classifier is basically as simple as that. There's some regularization involved, okay?", "tokens": [51244, 300, 8213, 1508, 9902, 307, 1936, 382, 2199, 382, 300, 13, 821, 311, 512, 3890, 2144, 3288, 11, 1392, 30, 51500], "temperature": 0.0, "avg_logprob": -0.2095907780162075, "compression_ratio": 1.8045977011494252, "no_speech_prob": 6.204572855494916e-05}, {"id": 991, "seek": 618160, "start": 6204.88, "end": 6210.320000000001, "text": " So, essentially, it's kind of a lookup table, right? You have your entire training set as", "tokens": [51528, 407, 11, 4476, 11, 309, 311, 733, 295, 257, 574, 1010, 3199, 11, 558, 30, 509, 362, 428, 2302, 3097, 992, 382, 51800], "temperature": 0.0, "avg_logprob": -0.2095907780162075, "compression_ratio": 1.8045977011494252, "no_speech_prob": 6.204572855494916e-05}, {"id": 992, "seek": 621032, "start": 6211.04, "end": 6216.639999999999, "text": " you know, points in your kind of nuance if you are, if you want for units in your first layer,", "tokens": [50400, 291, 458, 11, 2793, 294, 428, 733, 295, 42625, 498, 291, 366, 11, 498, 291, 528, 337, 6815, 294, 428, 700, 4583, 11, 50680], "temperature": 0.0, "avg_logprob": -0.20845437706063646, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0003353157371748239}, {"id": 993, "seek": 621032, "start": 6216.639999999999, "end": 6222.08, "text": " and they each indicate how close the current input vector is to them. So, you get some picture", "tokens": [50680, 293, 436, 1184, 13330, 577, 1998, 264, 2190, 4846, 8062, 307, 281, 552, 13, 407, 11, 291, 483, 512, 3036, 50952], "temperature": 0.0, "avg_logprob": -0.20845437706063646, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0003353157371748239}, {"id": 994, "seek": 621032, "start": 6222.08, "end": 6226.639999999999, "text": " of where the input vector is by basically having the relative position to all of the", "tokens": [50952, 295, 689, 264, 4846, 8062, 307, 538, 1936, 1419, 264, 4972, 2535, 281, 439, 295, 264, 51180], "temperature": 0.0, "avg_logprob": -0.20845437706063646, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0003353157371748239}, {"id": 995, "seek": 621032, "start": 6226.639999999999, "end": 6230.639999999999, "text": " training samples, and then using a simple linear operation, you can figure out, like, what's the", "tokens": [51180, 3097, 10938, 11, 293, 550, 1228, 257, 2199, 8213, 6916, 11, 291, 393, 2573, 484, 11, 411, 11, 437, 311, 264, 51380], "temperature": 0.0, "avg_logprob": -0.20845437706063646, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0003353157371748239}, {"id": 996, "seek": 621032, "start": 6230.639999999999, "end": 6235.679999999999, "text": " correct answer. This works really well for low dimensional problems, the small number of training", "tokens": [51380, 3006, 1867, 13, 639, 1985, 534, 731, 337, 2295, 18795, 2740, 11, 264, 1359, 1230, 295, 3097, 51632], "temperature": 0.0, "avg_logprob": -0.20845437706063646, "compression_ratio": 1.724264705882353, "no_speech_prob": 0.0003353157371748239}, {"id": 997, "seek": 623568, "start": 6236.400000000001, "end": 6242.88, "text": " samples, but you're not going to do computer vision with it, at least not without, not if X is our pixels,", "tokens": [50400, 10938, 11, 457, 291, 434, 406, 516, 281, 360, 3820, 5201, 365, 309, 11, 412, 1935, 406, 1553, 11, 406, 498, 1783, 307, 527, 18668, 11, 50724], "temperature": 0.0, "avg_logprob": -0.1485390563805898, "compression_ratio": 1.5694444444444444, "no_speech_prob": 6.921110616531223e-05}, {"id": 998, "seek": 623568, "start": 6244.4800000000005, "end": 6245.92, "text": " because it's basically template matching.", "tokens": [50804, 570, 309, 311, 1936, 12379, 14324, 13, 50876], "temperature": 0.0, "avg_logprob": -0.1485390563805898, "compression_ratio": 1.5694444444444444, "no_speech_prob": 6.921110616531223e-05}, {"id": 999, "seek": 623568, "start": 6248.64, "end": 6255.04, "text": " Now, here is a very interesting fact. It's a fact that if you build a two-layer neural net", "tokens": [51012, 823, 11, 510, 307, 257, 588, 1880, 1186, 13, 467, 311, 257, 1186, 300, 498, 291, 1322, 257, 732, 12, 8376, 260, 18161, 2533, 51332], "temperature": 0.0, "avg_logprob": -0.1485390563805898, "compression_ratio": 1.5694444444444444, "no_speech_prob": 6.921110616531223e-05}, {"id": 1000, "seek": 623568, "start": 6255.04, "end": 6259.92, "text": " on this model, okay? So, let's say a two-layer neural net, you have an input layer, a hidden layer,", "tokens": [51332, 322, 341, 2316, 11, 1392, 30, 407, 11, 718, 311, 584, 257, 732, 12, 8376, 260, 18161, 2533, 11, 291, 362, 364, 4846, 4583, 11, 257, 7633, 4583, 11, 51576], "temperature": 0.0, "avg_logprob": -0.1485390563805898, "compression_ratio": 1.5694444444444444, "no_speech_prob": 6.921110616531223e-05}, {"id": 1001, "seek": 625992, "start": 6260.88, "end": 6267.4400000000005, "text": " and not specifying the size, and a single output unit, and you ask, what functions can I approximate", "tokens": [50412, 293, 406, 1608, 5489, 264, 2744, 11, 293, 257, 2167, 5598, 4985, 11, 293, 291, 1029, 11, 437, 6828, 393, 286, 30874, 50740], "temperature": 0.0, "avg_logprob": -0.07584018381232889, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007915550377219915}, {"id": 1002, "seek": 625992, "start": 6267.4400000000005, "end": 6273.04, "text": " with an architecture of this type? The answer is, you can approximate pretty much any well-behaved", "tokens": [50740, 365, 364, 9482, 295, 341, 2010, 30, 440, 1867, 307, 11, 291, 393, 30874, 1238, 709, 604, 731, 12, 29437, 12865, 51020], "temperature": 0.0, "avg_logprob": -0.07584018381232889, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007915550377219915}, {"id": 1003, "seek": 625992, "start": 6273.04, "end": 6278.24, "text": " function as close as you want, as long as you have enough of those units in the middle, okay?", "tokens": [51020, 2445, 382, 1998, 382, 291, 528, 11, 382, 938, 382, 291, 362, 1547, 295, 729, 6815, 294, 264, 2808, 11, 1392, 30, 51280], "temperature": 0.0, "avg_logprob": -0.07584018381232889, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007915550377219915}, {"id": 1004, "seek": 625992, "start": 6278.24, "end": 6284.24, "text": " So, this is a theorem that says that two-layer neural nets are universal approximators. It", "tokens": [51280, 407, 11, 341, 307, 257, 20904, 300, 1619, 300, 732, 12, 8376, 260, 18161, 36170, 366, 11455, 8542, 3391, 13, 467, 51580], "temperature": 0.0, "avg_logprob": -0.07584018381232889, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007915550377219915}, {"id": 1005, "seek": 625992, "start": 6284.24, "end": 6289.12, "text": " doesn't really matter what nonlinear function you put in the middle. Any nonlinear function will", "tokens": [51580, 1177, 380, 534, 1871, 437, 2107, 28263, 2445, 291, 829, 294, 264, 2808, 13, 2639, 2107, 28263, 2445, 486, 51824], "temperature": 0.0, "avg_logprob": -0.07584018381232889, "compression_ratio": 1.7302158273381294, "no_speech_prob": 0.0007915550377219915}, {"id": 1006, "seek": 628912, "start": 6289.12, "end": 6297.04, "text": " do. A two-layer neural net is a universal approximator, and immediately you say, well,", "tokens": [50364, 360, 13, 316, 732, 12, 8376, 260, 18161, 2533, 307, 257, 11455, 8542, 1639, 11, 293, 4258, 291, 584, 11, 731, 11, 50760], "temperature": 0.0, "avg_logprob": -0.07262272018570083, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.0005524359876289964}, {"id": 1007, "seek": 628912, "start": 6297.04, "end": 6302.16, "text": " why do we need multiple layers, then, if we can approximate anything with two layers? And the", "tokens": [50760, 983, 360, 321, 643, 3866, 7914, 11, 550, 11, 498, 321, 393, 30874, 1340, 365, 732, 7914, 30, 400, 264, 51016], "temperature": 0.0, "avg_logprob": -0.07262272018570083, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.0005524359876289964}, {"id": 1008, "seek": 628912, "start": 6302.16, "end": 6307.2, "text": " answer is, it's very, very inefficient to try to approximate everything with only two layers,", "tokens": [51016, 1867, 307, 11, 309, 311, 588, 11, 588, 43495, 281, 853, 281, 30874, 1203, 365, 787, 732, 7914, 11, 51268], "temperature": 0.0, "avg_logprob": -0.07262272018570083, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.0005524359876289964}, {"id": 1009, "seek": 628912, "start": 6307.2, "end": 6312.4, "text": " because many, many, many interesting functions we're interested in learning cannot be efficiently", "tokens": [51268, 570, 867, 11, 867, 11, 867, 1880, 6828, 321, 434, 3102, 294, 2539, 2644, 312, 19621, 51528], "temperature": 0.0, "avg_logprob": -0.07262272018570083, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.0005524359876289964}, {"id": 1010, "seek": 628912, "start": 6312.4, "end": 6317.12, "text": " represented by a two-layer system. They can possibly be represented by a two-layer system,", "tokens": [51528, 10379, 538, 257, 732, 12, 8376, 260, 1185, 13, 814, 393, 6264, 312, 10379, 538, 257, 732, 12, 8376, 260, 1185, 11, 51764], "temperature": 0.0, "avg_logprob": -0.07262272018570083, "compression_ratio": 1.8669354838709677, "no_speech_prob": 0.0005524359876289964}, {"id": 1011, "seek": 631712, "start": 6317.12, "end": 6321.36, "text": " but the number of fielding units it would require would be so ridiculously large that it's completely", "tokens": [50364, 457, 264, 1230, 295, 2519, 278, 6815, 309, 576, 3651, 576, 312, 370, 41358, 2416, 300, 309, 311, 2584, 50576], "temperature": 0.0, "avg_logprob": -0.11902364817532626, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.00034578348277136683}, {"id": 1012, "seek": 631712, "start": 6321.36, "end": 6331.12, "text": " impractical, okay? So, that's why we need layers. This very simple point is something that took", "tokens": [50576, 704, 1897, 804, 11, 1392, 30, 407, 11, 300, 311, 983, 321, 643, 7914, 13, 639, 588, 2199, 935, 307, 746, 300, 1890, 51064], "temperature": 0.0, "avg_logprob": -0.11902364817532626, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.00034578348277136683}, {"id": 1013, "seek": 631712, "start": 6331.12, "end": 6337.68, "text": " about, you know, it took until the, basically, the 2010s for the machine learning and", "tokens": [51064, 466, 11, 291, 458, 11, 309, 1890, 1826, 264, 11, 1936, 11, 264, 9657, 82, 337, 264, 3479, 2539, 293, 51392], "temperature": 0.0, "avg_logprob": -0.11902364817532626, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.00034578348277136683}, {"id": 1014, "seek": 631712, "start": 6337.68, "end": 6344.5599999999995, "text": " computer vision communities to understand, okay? If you understood what I just said,", "tokens": [51392, 3820, 5201, 4456, 281, 1223, 11, 1392, 30, 759, 291, 7320, 437, 286, 445, 848, 11, 51736], "temperature": 0.0, "avg_logprob": -0.11902364817532626, "compression_ratio": 1.5397489539748954, "no_speech_prob": 0.00034578348277136683}, {"id": 1015, "seek": 634456, "start": 6344.56, "end": 6350.080000000001, "text": " you just took a few seconds, so you beat them. There is a last question here before we", "tokens": [50364, 291, 445, 1890, 257, 1326, 3949, 11, 370, 291, 4224, 552, 13, 821, 307, 257, 1036, 1168, 510, 949, 321, 50640], "temperature": 0.0, "avg_logprob": -0.10089876822062901, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.002213014755398035}, {"id": 1016, "seek": 634456, "start": 6350.080000000001, "end": 6355.76, "text": " finish class. So, does the depth of the network then have anything to do with generalization?", "tokens": [50640, 2413, 1508, 13, 407, 11, 775, 264, 7161, 295, 264, 3209, 550, 362, 1340, 281, 360, 365, 2674, 2144, 30, 50924], "temperature": 0.0, "avg_logprob": -0.10089876822062901, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.002213014755398035}, {"id": 1017, "seek": 634456, "start": 6357.52, "end": 6363.84, "text": " Okay, so generalization is a different story, okay? Generalization is very difficult to predict.", "tokens": [51012, 1033, 11, 370, 2674, 2144, 307, 257, 819, 1657, 11, 1392, 30, 6996, 2144, 307, 588, 2252, 281, 6069, 13, 51328], "temperature": 0.0, "avg_logprob": -0.10089876822062901, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.002213014755398035}, {"id": 1018, "seek": 634456, "start": 6363.84, "end": 6368.8, "text": " It depends on a lot of things. It depends on the appropriateness of the architecture to the", "tokens": [51328, 467, 5946, 322, 257, 688, 295, 721, 13, 467, 5946, 322, 264, 5745, 7186, 442, 295, 264, 9482, 281, 264, 51576], "temperature": 0.0, "avg_logprob": -0.10089876822062901, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.002213014755398035}, {"id": 1019, "seek": 634456, "start": 6368.8, "end": 6373.68, "text": " problem at hand, okay? So, for example, people use convolutional nets for computer vision,", "tokens": [51576, 1154, 412, 1011, 11, 1392, 30, 407, 11, 337, 1365, 11, 561, 764, 45216, 304, 36170, 337, 3820, 5201, 11, 51820], "temperature": 0.0, "avg_logprob": -0.10089876822062901, "compression_ratio": 1.6911764705882353, "no_speech_prob": 0.002213014755398035}, {"id": 1020, "seek": 637368, "start": 6373.68, "end": 6378.400000000001, "text": " they use transformers for text, you know, blah, blah, blah. So, there are certain architectures", "tokens": [50364, 436, 764, 4088, 433, 337, 2487, 11, 291, 458, 11, 12288, 11, 12288, 11, 12288, 13, 407, 11, 456, 366, 1629, 6331, 1303, 50600], "temperature": 0.0, "avg_logprob": -0.08381300437741163, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00024916024995036423}, {"id": 1021, "seek": 637368, "start": 6378.400000000001, "end": 6386.240000000001, "text": " that work well for certain types of data. So, that's the main thing that will improve generalization.", "tokens": [50600, 300, 589, 731, 337, 1629, 3467, 295, 1412, 13, 407, 11, 300, 311, 264, 2135, 551, 300, 486, 3470, 2674, 2144, 13, 50992], "temperature": 0.0, "avg_logprob": -0.08381300437741163, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00024916024995036423}, {"id": 1022, "seek": 637368, "start": 6389.6, "end": 6393.92, "text": " But generally, yes, multiple layers can improve generalization because", "tokens": [51160, 583, 5101, 11, 2086, 11, 3866, 7914, 393, 3470, 2674, 2144, 570, 51376], "temperature": 0.0, "avg_logprob": -0.08381300437741163, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00024916024995036423}, {"id": 1023, "seek": 637368, "start": 6394.88, "end": 6400.240000000001, "text": " for a particular function you're interested in learning, computing it with multiple layers", "tokens": [51424, 337, 257, 1729, 2445, 291, 434, 3102, 294, 2539, 11, 15866, 309, 365, 3866, 7914, 51692], "temperature": 0.0, "avg_logprob": -0.08381300437741163, "compression_ratio": 1.7014218009478672, "no_speech_prob": 0.00024916024995036423}, {"id": 1024, "seek": 640024, "start": 6400.24, "end": 6404.0, "text": " will allow you to reduce the overall size of the system that will do a good job.", "tokens": [50364, 486, 2089, 291, 281, 5407, 264, 4787, 2744, 295, 264, 1185, 300, 486, 360, 257, 665, 1691, 13, 50552], "temperature": 0.0, "avg_logprob": -0.13332043105749775, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.001645069569349289}, {"id": 1025, "seek": 640024, "start": 6404.0, "end": 6408.24, "text": " And so, by reducing the size, you're basically making it easier for the system to find kind of", "tokens": [50552, 400, 370, 11, 538, 12245, 264, 2744, 11, 291, 434, 1936, 1455, 309, 3571, 337, 264, 1185, 281, 915, 733, 295, 50764], "temperature": 0.0, "avg_logprob": -0.13332043105749775, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.001645069569349289}, {"id": 1026, "seek": 640024, "start": 6408.24, "end": 6412.16, "text": " good representation. But there is something else which has to do with compositionality.", "tokens": [50764, 665, 10290, 13, 583, 456, 307, 746, 1646, 597, 575, 281, 360, 365, 12686, 1860, 13, 50960], "temperature": 0.0, "avg_logprob": -0.13332043105749775, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.001645069569349289}, {"id": 1027, "seek": 640024, "start": 6412.16, "end": 6417.28, "text": " I'll come to this in a minute if I have time. Also, the minimum, the, like the,", "tokens": [50960, 286, 603, 808, 281, 341, 294, 257, 3456, 498, 286, 362, 565, 13, 2743, 11, 264, 7285, 11, 264, 11, 411, 264, 11, 51216], "temperature": 0.0, "avg_logprob": -0.13332043105749775, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.001645069569349289}, {"id": 1028, "seek": 640024, "start": 6417.28, "end": 6422.32, "text": " how do you call it, the well is like larger, right? If we have overparameterized networks.", "tokens": [51216, 577, 360, 291, 818, 309, 11, 264, 731, 307, 411, 4833, 11, 558, 30, 759, 321, 362, 670, 2181, 335, 2398, 1602, 9590, 13, 51468], "temperature": 0.0, "avg_logprob": -0.13332043105749775, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.001645069569349289}, {"id": 1029, "seek": 640024, "start": 6422.32, "end": 6426.4, "text": " If you're overparameterized network, it's much easier to find a minimum to your objective function,", "tokens": [51468, 759, 291, 434, 670, 2181, 335, 2398, 1602, 3209, 11, 309, 311, 709, 3571, 281, 915, 257, 7285, 281, 428, 10024, 2445, 11, 51672], "temperature": 0.0, "avg_logprob": -0.13332043105749775, "compression_ratio": 1.7740863787375416, "no_speech_prob": 0.001645069569349289}, {"id": 1030, "seek": 642640, "start": 6426.4, "end": 6432.48, "text": " right? Which is why neural nets are generally overparameterized. They generally have, like you,", "tokens": [50364, 558, 30, 3013, 307, 983, 18161, 36170, 366, 5101, 670, 2181, 335, 2398, 1602, 13, 814, 5101, 362, 11, 411, 291, 11, 50668], "temperature": 0.0, "avg_logprob": -0.11584144830703735, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0011506607988849282}, {"id": 1031, "seek": 642640, "start": 6432.48, "end": 6435.2, "text": " a much larger number of parameters than what you would think is necessary.", "tokens": [50668, 257, 709, 4833, 1230, 295, 9834, 813, 437, 291, 576, 519, 307, 4818, 13, 50804], "temperature": 0.0, "avg_logprob": -0.11584144830703735, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0011506607988849282}, {"id": 1032, "seek": 642640, "start": 6435.839999999999, "end": 6438.96, "text": " And when you get them bigger, when you make them bigger, they work better usually.", "tokens": [50836, 400, 562, 291, 483, 552, 3801, 11, 562, 291, 652, 552, 3801, 11, 436, 589, 1101, 2673, 13, 50992], "temperature": 0.0, "avg_logprob": -0.11584144830703735, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0011506607988849282}, {"id": 1033, "seek": 642640, "start": 6438.96, "end": 6445.12, "text": " It's not always the case, but it's very curious phenomenon about this. We'll talk about this later.", "tokens": [50992, 467, 311, 406, 1009, 264, 1389, 11, 457, 309, 311, 588, 6369, 14029, 466, 341, 13, 492, 603, 751, 466, 341, 1780, 13, 51300], "temperature": 0.0, "avg_logprob": -0.11584144830703735, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0011506607988849282}, {"id": 1034, "seek": 642640, "start": 6445.12, "end": 6451.599999999999, "text": " Okay, this is the one point I want to make. And it's the fact that the reason why we,", "tokens": [51300, 1033, 11, 341, 307, 264, 472, 935, 286, 528, 281, 652, 13, 400, 309, 311, 264, 1186, 300, 264, 1778, 983, 321, 11, 51624], "temperature": 0.0, "avg_logprob": -0.11584144830703735, "compression_ratio": 1.6566037735849057, "no_speech_prob": 0.0011506607988849282}, {"id": 1035, "seek": 645160, "start": 6451.6, "end": 6457.92, "text": " why layers are good is that the world is compositional, the perceptual world in particular,", "tokens": [50364, 983, 7914, 366, 665, 307, 300, 264, 1002, 307, 10199, 2628, 11, 264, 43276, 901, 1002, 294, 1729, 11, 50680], "temperature": 0.0, "avg_logprob": -0.11467158899898022, "compression_ratio": 2.1531531531531534, "no_speech_prob": 0.0013876801822334528}, {"id": 1036, "seek": 645160, "start": 6457.92, "end": 6461.6, "text": " but the world in general, the universe, if you want, is compositional. What does that mean? It", "tokens": [50680, 457, 264, 1002, 294, 2674, 11, 264, 6445, 11, 498, 291, 528, 11, 307, 10199, 2628, 13, 708, 775, 300, 914, 30, 467, 50864], "temperature": 0.0, "avg_logprob": -0.11467158899898022, "compression_ratio": 2.1531531531531534, "no_speech_prob": 0.0013876801822334528}, {"id": 1037, "seek": 645160, "start": 6461.6, "end": 6467.68, "text": " means that, okay, at the level of the universe, right? We have elementary particles, they assemble", "tokens": [50864, 1355, 300, 11, 1392, 11, 412, 264, 1496, 295, 264, 6445, 11, 558, 30, 492, 362, 16429, 10007, 11, 436, 22364, 51168], "temperature": 0.0, "avg_logprob": -0.11467158899898022, "compression_ratio": 2.1531531531531534, "no_speech_prob": 0.0013876801822334528}, {"id": 1038, "seek": 645160, "start": 6467.68, "end": 6472.72, "text": " to form less elementary particles, those assemble to form atoms, those assemble to form molecules,", "tokens": [51168, 281, 1254, 1570, 16429, 10007, 11, 729, 22364, 281, 1254, 16871, 11, 729, 22364, 281, 1254, 13093, 11, 51420], "temperature": 0.0, "avg_logprob": -0.11467158899898022, "compression_ratio": 2.1531531531531534, "no_speech_prob": 0.0013876801822334528}, {"id": 1039, "seek": 645160, "start": 6472.72, "end": 6480.08, "text": " those assemble to form materials, those assemble to form, you know, structures, objects, etc.", "tokens": [51420, 729, 22364, 281, 1254, 5319, 11, 729, 22364, 281, 1254, 11, 291, 458, 11, 9227, 11, 6565, 11, 5183, 13, 51788], "temperature": 0.0, "avg_logprob": -0.11467158899898022, "compression_ratio": 2.1531531531531534, "no_speech_prob": 0.0013876801822334528}, {"id": 1040, "seek": 648008, "start": 6480.08, "end": 6486.24, "text": " And, you know, environments, scenes, etc. You have the same kind of hierarchy for images,", "tokens": [50364, 400, 11, 291, 458, 11, 12388, 11, 8026, 11, 5183, 13, 509, 362, 264, 912, 733, 295, 22333, 337, 5267, 11, 50672], "temperature": 0.0, "avg_logprob": -0.15410908593071831, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.0002570091746747494}, {"id": 1041, "seek": 648008, "start": 6486.24, "end": 6491.92, "text": " you have pixels, they assemble to form edges and textons and motifs, parts and objects.", "tokens": [50672, 291, 362, 18668, 11, 436, 22364, 281, 1254, 8819, 293, 2487, 892, 293, 2184, 18290, 11, 3166, 293, 6565, 13, 50956], "temperature": 0.0, "avg_logprob": -0.15410908593071831, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.0002570091746747494}, {"id": 1042, "seek": 648008, "start": 6492.64, "end": 6496.4, "text": " In text, you have characters that assemble to form words, word groups, clauses, sentences,", "tokens": [50992, 682, 2487, 11, 291, 362, 4342, 300, 22364, 281, 1254, 2283, 11, 1349, 3935, 11, 49072, 11, 16579, 11, 51180], "temperature": 0.0, "avg_logprob": -0.15410908593071831, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.0002570091746747494}, {"id": 1043, "seek": 648008, "start": 6496.4, "end": 6503.5199999999995, "text": " stories. In speech, you have speech samples, assemble to form, you know, kind of elementary", "tokens": [51180, 3676, 13, 682, 6218, 11, 291, 362, 6218, 10938, 11, 22364, 281, 1254, 11, 291, 458, 11, 733, 295, 16429, 51536], "temperature": 0.0, "avg_logprob": -0.15410908593071831, "compression_ratio": 1.7821782178217822, "no_speech_prob": 0.0002570091746747494}, {"id": 1044, "seek": 650352, "start": 6503.52, "end": 6510.8, "text": " sounds, phones, phonemes, syllables, words, etc. So you have this kind of compositional hierarchy", "tokens": [50364, 3263, 11, 10216, 11, 30754, 443, 279, 11, 45364, 11, 2283, 11, 5183, 13, 407, 291, 362, 341, 733, 295, 10199, 2628, 22333, 50728], "temperature": 0.0, "avg_logprob": -0.09322610365605988, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.005907227285206318}, {"id": 1045, "seek": 650352, "start": 6510.8, "end": 6515.360000000001, "text": " in a lot of natural signals. And this is what makes the world understandable, right? This is", "tokens": [50728, 294, 257, 688, 295, 3303, 12354, 13, 400, 341, 307, 437, 1669, 264, 1002, 25648, 11, 558, 30, 639, 307, 50956], "temperature": 0.0, "avg_logprob": -0.09322610365605988, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.005907227285206318}, {"id": 1046, "seek": 650352, "start": 6515.360000000001, "end": 6519.84, "text": " famous quote by Albert Einstein, the most incomprehensible thing about the world is that the", "tokens": [50956, 4618, 6513, 538, 20812, 23486, 11, 264, 881, 14036, 40128, 30633, 551, 466, 264, 1002, 307, 300, 264, 51180], "temperature": 0.0, "avg_logprob": -0.09322610365605988, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.005907227285206318}, {"id": 1047, "seek": 650352, "start": 6519.84, "end": 6524.080000000001, "text": " world is comprehensible. And the reason why the world is comprehensible is because it's compositional,", "tokens": [51180, 1002, 307, 10753, 30633, 13, 400, 264, 1778, 983, 264, 1002, 307, 10753, 30633, 307, 570, 309, 311, 10199, 2628, 11, 51392], "temperature": 0.0, "avg_logprob": -0.09322610365605988, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.005907227285206318}, {"id": 1048, "seek": 650352, "start": 6524.080000000001, "end": 6528.4800000000005, "text": " because small part assemble to form bigger part, and that allows you to have a description, an", "tokens": [51392, 570, 1359, 644, 22364, 281, 1254, 3801, 644, 11, 293, 300, 4045, 291, 281, 362, 257, 3855, 11, 364, 51612], "temperature": 0.0, "avg_logprob": -0.09322610365605988, "compression_ratio": 1.8288973384030418, "no_speech_prob": 0.005907227285206318}, {"id": 1049, "seek": 652848, "start": 6528.48, "end": 6535.759999999999, "text": " abstract description of the world in terms of parts from the level immediately below,", "tokens": [50364, 12649, 3855, 295, 264, 1002, 294, 2115, 295, 3166, 490, 264, 1496, 4258, 2507, 11, 50728], "temperature": 0.0, "avg_logprob": -0.12606065471967062, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0015239730710163713}, {"id": 1050, "seek": 652848, "start": 6535.759999999999, "end": 6540.48, "text": " in terms of level of abstraction. So to some extent, the layered architecture in a neural net", "tokens": [50728, 294, 2115, 295, 1496, 295, 37765, 13, 407, 281, 512, 8396, 11, 264, 34666, 9482, 294, 257, 18161, 2533, 50964], "temperature": 0.0, "avg_logprob": -0.12606065471967062, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0015239730710163713}, {"id": 1051, "seek": 652848, "start": 6541.599999999999, "end": 6548.16, "text": " reflects this idea that you have kind of a compositional hierarchy where simple things", "tokens": [51020, 18926, 341, 1558, 300, 291, 362, 733, 295, 257, 10199, 2628, 22333, 689, 2199, 721, 51348], "temperature": 0.0, "avg_logprob": -0.12606065471967062, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0015239730710163713}, {"id": 1052, "seek": 652848, "start": 6548.16, "end": 6553.12, "text": " assemble to form slightly more complex things. So images, you have pixels formed to form edges", "tokens": [51348, 22364, 281, 1254, 4748, 544, 3997, 721, 13, 407, 5267, 11, 291, 362, 18668, 8693, 281, 1254, 8819, 51596], "temperature": 0.0, "avg_logprob": -0.12606065471967062, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0015239730710163713}, {"id": 1053, "seek": 652848, "start": 6553.12, "end": 6557.44, "text": " that are kind of depicted here. These are actually feature detectors, the visualization of feature", "tokens": [51596, 300, 366, 733, 295, 30207, 510, 13, 1981, 366, 767, 4111, 46866, 11, 264, 25801, 295, 4111, 51812], "temperature": 0.0, "avg_logprob": -0.12606065471967062, "compression_ratio": 1.776061776061776, "no_speech_prob": 0.0015239730710163713}, {"id": 1054, "seek": 655744, "start": 6557.44, "end": 6562.48, "text": " detectors by a particular convolutional net, which is a particular type of neural net, multilateral", "tokens": [50364, 46866, 538, 257, 1729, 45216, 304, 2533, 11, 597, 307, 257, 1729, 2010, 295, 18161, 2533, 11, 2120, 37751, 50616], "temperature": 0.0, "avg_logprob": -0.11009180545806885, "compression_ratio": 1.7873303167420815, "no_speech_prob": 0.0009684605756774545}, {"id": 1055, "seek": 655744, "start": 6562.48, "end": 6568.16, "text": " neural net. So at the low level, you have units that detect oriented edges, a couple layers up,", "tokens": [50616, 18161, 2533, 13, 407, 412, 264, 2295, 1496, 11, 291, 362, 6815, 300, 5531, 21841, 8819, 11, 257, 1916, 7914, 493, 11, 50900], "temperature": 0.0, "avg_logprob": -0.11009180545806885, "compression_ratio": 1.7873303167420815, "no_speech_prob": 0.0009684605756774545}, {"id": 1056, "seek": 655744, "start": 6568.16, "end": 6574.0, "text": " you have things that detect simple motifs, circles, gratings, corners, etc. And then a few layers up,", "tokens": [50900, 291, 362, 721, 300, 5531, 2199, 2184, 18290, 11, 13040, 11, 677, 990, 82, 11, 12413, 11, 5183, 13, 400, 550, 257, 1326, 7914, 493, 11, 51192], "temperature": 0.0, "avg_logprob": -0.11009180545806885, "compression_ratio": 1.7873303167420815, "no_speech_prob": 0.0009684605756774545}, {"id": 1057, "seek": 655744, "start": 6574.0, "end": 6580.5599999999995, "text": " there are things like parts of objects and things like that. So I think personally that the magic", "tokens": [51192, 456, 366, 721, 411, 3166, 295, 6565, 293, 721, 411, 300, 13, 407, 286, 519, 5665, 300, 264, 5585, 51520], "temperature": 0.0, "avg_logprob": -0.11009180545806885, "compression_ratio": 1.7873303167420815, "no_speech_prob": 0.0009684605756774545}, {"id": 1058, "seek": 658056, "start": 6581.120000000001, "end": 6588.240000000001, "text": " of deep learning, the fact that multiple layers help is the fact that the perceptual world is", "tokens": [50392, 295, 2452, 2539, 11, 264, 1186, 300, 3866, 7914, 854, 307, 264, 1186, 300, 264, 43276, 901, 1002, 307, 50748], "temperature": 0.0, "avg_logprob": -0.10008898148169884, "compression_ratio": 1.9673469387755103, "no_speech_prob": 0.001726558548398316}, {"id": 1059, "seek": 658056, "start": 6588.240000000001, "end": 6593.6, "text": " basically a compositional hierarchy. And then this end-to-end learning in deep learning allows the", "tokens": [50748, 1936, 257, 10199, 2628, 22333, 13, 400, 550, 341, 917, 12, 1353, 12, 521, 2539, 294, 2452, 2539, 4045, 264, 51016], "temperature": 0.0, "avg_logprob": -0.10008898148169884, "compression_ratio": 1.9673469387755103, "no_speech_prob": 0.001726558548398316}, {"id": 1060, "seek": 658056, "start": 6593.6, "end": 6600.72, "text": " system to learn hierarchical representations where each layer learns a representation that has a", "tokens": [51016, 1185, 281, 1466, 35250, 804, 33358, 689, 1184, 4583, 27152, 257, 10290, 300, 575, 257, 51372], "temperature": 0.0, "avg_logprob": -0.10008898148169884, "compression_ratio": 1.9673469387755103, "no_speech_prob": 0.001726558548398316}, {"id": 1061, "seek": 658056, "start": 6600.72, "end": 6605.200000000001, "text": " level of abstraction slightly higher than the previous one. So low level, you have individual", "tokens": [51372, 1496, 295, 37765, 4748, 2946, 813, 264, 3894, 472, 13, 407, 2295, 1496, 11, 291, 362, 2609, 51596], "temperature": 0.0, "avg_logprob": -0.10008898148169884, "compression_ratio": 1.9673469387755103, "no_speech_prob": 0.001726558548398316}, {"id": 1062, "seek": 658056, "start": 6605.200000000001, "end": 6610.080000000001, "text": " pixels, then you have the presence or absence of an edge, then you have the presence or absence of", "tokens": [51596, 18668, 11, 550, 291, 362, 264, 6814, 420, 17145, 295, 364, 4691, 11, 550, 291, 362, 264, 6814, 420, 17145, 295, 51840], "temperature": 0.0, "avg_logprob": -0.10008898148169884, "compression_ratio": 1.9673469387755103, "no_speech_prob": 0.001726558548398316}, {"id": 1063, "seek": 661008, "start": 6610.08, "end": 6614.5599999999995, "text": " a part of an object, and then you have the presence or absence of an object independently of", "tokens": [50364, 257, 644, 295, 364, 2657, 11, 293, 550, 291, 362, 264, 6814, 420, 17145, 295, 364, 2657, 21761, 295, 50588], "temperature": 0.0, "avg_logprob": -0.1048380560794119, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0010294528910890222}, {"id": 1064, "seek": 661008, "start": 6615.84, "end": 6619.92, "text": " the position of that object, the illumination, the color, the occlusions, the background,", "tokens": [50652, 264, 2535, 295, 300, 2657, 11, 264, 30579, 2486, 11, 264, 2017, 11, 264, 2678, 3063, 626, 11, 264, 3678, 11, 50856], "temperature": 0.0, "avg_logprob": -0.1048380560794119, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0010294528910890222}, {"id": 1065, "seek": 661008, "start": 6619.92, "end": 6626.64, "text": " you know, things like that, right? So that's the motivation, the idea why deep learning is so", "tokens": [50856, 291, 458, 11, 721, 411, 300, 11, 558, 30, 407, 300, 311, 264, 12335, 11, 264, 1558, 983, 2452, 2539, 307, 370, 51192], "temperature": 0.0, "avg_logprob": -0.1048380560794119, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0010294528910890222}, {"id": 1066, "seek": 661008, "start": 6626.64, "end": 6632.88, "text": " successful and why it's basically taken over the world over the last 10 years or so. All right,", "tokens": [51192, 4406, 293, 983, 309, 311, 1936, 2726, 670, 264, 1002, 670, 264, 1036, 1266, 924, 420, 370, 13, 1057, 558, 11, 51504], "temperature": 0.0, "avg_logprob": -0.1048380560794119, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0010294528910890222}, {"id": 1067, "seek": 661008, "start": 6632.88, "end": 6637.2, "text": " thank you for your attention. That's great. So for tomorrow guys, don't forget to try to go over", "tokens": [51504, 1309, 291, 337, 428, 3202, 13, 663, 311, 869, 13, 407, 337, 4153, 1074, 11, 500, 380, 2870, 281, 853, 281, 352, 670, 51720], "temperature": 0.0, "avg_logprob": -0.1048380560794119, "compression_ratio": 1.737037037037037, "no_speech_prob": 0.0010294528910890222}, {"id": 1068, "seek": 663720, "start": 6637.2, "end": 6647.28, "text": " the 01 tutorial tensor, sorry, the 01 notebook that we have on the website such that we can get,", "tokens": [50364, 264, 23185, 7073, 40863, 11, 2597, 11, 264, 23185, 21060, 300, 321, 362, 322, 264, 3144, 1270, 300, 321, 393, 483, 11, 50868], "temperature": 0.0, "avg_logprob": -0.19939818749060997, "compression_ratio": 1.475, "no_speech_prob": 0.022316480055451393}, {"id": 1069, "seek": 663720, "start": 6647.28, "end": 6651.84, "text": " like, you know, all on the same level for the ones that are not really familiar with NumPy stuff,", "tokens": [50868, 411, 11, 291, 458, 11, 439, 322, 264, 912, 1496, 337, 264, 2306, 300, 366, 406, 534, 4963, 365, 22592, 47, 88, 1507, 11, 51096], "temperature": 0.0, "avg_logprob": -0.19939818749060997, "compression_ratio": 1.475, "no_speech_prob": 0.022316480055451393}, {"id": 1070, "seek": 663720, "start": 6651.84, "end": 6664.48, "text": " okay? So otherwise, let's see you tomorrow morning and have a nice day. Take care everyone. Bye-bye.", "tokens": [51096, 1392, 30, 407, 5911, 11, 718, 311, 536, 291, 4153, 2446, 293, 362, 257, 1481, 786, 13, 3664, 1127, 1518, 13, 4621, 12, 6650, 13, 51728], "temperature": 0.0, "avg_logprob": -0.19939818749060997, "compression_ratio": 1.475, "no_speech_prob": 0.022316480055451393}], "language": "en"}