start	end	text
0	3120	So as you know, we're going to talk about deep learning and we're going to jump right in.
4480	11920	So much of practical applications of deep learning today, machine learning and AI in general,
13120	19520	are used a paradigm called supervised learning, which I'm sure most of you have heard of before.
19520	24720	So this is the paradigm by which you train a machine by showing it examples of inputs and outputs.
25600	29680	You want to build a machine to distinguish images of cars from airplanes, you show it an image of
29680	34560	a car. If the machine says car, you don't do anything. If it says something else, you adjust
34560	38400	the internal parameters of the system so that the output gets closer to the one you want.
39280	45840	So imagine the target output is some vector of activities on a set of outputs. You want the
45840	49840	vector coming out of the machine to get closer to the vector that is the desired output.
51440	56560	And this works really well. As long as you have lots of data, it works for speech recognition,
56560	62240	image recognition, face recognition, generating captions, translation, all kinds of stuff.
63280	67280	So this is, I would say, 95% of all applications of machine learning today.
68160	72000	There are two other paradigms, one of which I will not talk about, one of which I will talk
72000	77200	about a lot. So the two other paradigms are reinforcement learning, which I will not talk
77200	83920	about. And there are other courses. There's a course by Larry Pinto about this that I encourage
84000	89760	you to take. And a third paradigm is self-supervised learning or unsupervised learning. And we'll
89760	96320	talk about this quite a lot in the following weeks. But for now, let's talk about supervised
96320	99920	learning. Self-supervised learning, you could think of it as kind of a play on supervised learning.
101840	106000	So the traditional model of pattern recognition machine learning and supervised learning,
106000	111920	certainly going back to the late 50s or the 60s, is the idea by which you take a raw signal,
111920	116640	let's say an image or an audio signal or a set of features representing an object,
116640	123120	and then you turn it into a representation using a feature extractor, which in the past
123120	129520	was engineered. And then you take that representation, which is generally in the form of a vector or
129520	133520	a table of numbers or some kind of tensor, a multi-dimensional array. But sometimes,
134240	138480	could be a different type of representation. And you feed that to a trainable classifier.
139360	143360	So this is the learning where the learning takes part. This is the classical model,
144080	148800	and it's still popular. It's still used a lot. But basically, what deep learning has done is
148800	155920	replace this sort of manual hand engineering of the feature extractor by a stack of trainable
156560	160640	modules, if you want. So in deep learning, the main idea of deep learning, and the only reason
160640	166960	why it's called deep, is that we stack a bunch of modules, each of which transforms the input a
166960	173520	little bit into something that's going to slightly higher level of abstraction, if you want.
174240	184800	And then we train the entire system end to end. So I represented those sort of pinkish modules
184800	190160	to indicate the ones that are trainable, and the blue modules are the fixed ones, the hand
190160	195840	engineered ones. So that's why deep learning is called deep. We stack multiple layers of
195840	201600	trainable things, and we train it end to end. The idea for this goes back a long time. The
201600	207760	practical methods for this go back to the mid to late 80s, with the back propagation algorithm,
207760	214480	which is going to be the main subject of today's lecture, actually. But it took a long time for
214480	221680	this idea to actually percolate and become the main tool that people use to build machine learning
221680	228000	system. It's only about 10 years old. Okay, so let's go through a few definitions. So we're going
228000	232560	to deal with parameterized models, a parameterized model, or learning model, if you want, is a
232560	238720	parameterized function, g of x and w, where x is the input, and w is a set of parameters.
240240	245600	I'm representing this here on the right with a particular symbolism, where a function
246560	252800	like this that produces a single output, think of the output as either a vector or matrix or a
252800	258240	tensor, or perhaps even a scalar, but generally is multidimensional. It can actually be something
258240	264240	else in a multidimensional array, but something that, you know, maybe like a sparse array representation
264240	269280	or a graph with values on it. But for now, let's think of it just as a multidimensional array.
270240	275600	So both the inputs and the outputs are multidimensional arrays, what people call tensors.
276560	280480	It's not really kind of the appropriate definition of tensor, but it's okay.
282080	286080	And that function is parameterized by a set of parameters w. Those are the knobs that we're
286080	292080	going to adjust during training, and they basically determine the input-output relationship between
293040	301520	you know, between the input x and the predicted output y bar. Okay, so I'm not explicitly
301520	308000	representing the wire that comes in with w. Here, I kind of assume that w is somewhere inside of
309200	315360	this module. Think of this as an object in object-oriented programming. So it's an instance
315360	321360	of a class that you instantiated and it's got a slot in it that represents the parameters,
321360	328160	and there is a forward function basically that takes as argument the input and returns the output.
328160	336240	Okay, so a basic running machine will have a cost function and the cost function in supervised
336240	344560	running, but also in some other settings will basically compute the discrepancy, distance,
345120	350320	divergence, whatever you want to call it, between the desired output y, which is given to you from
350320	356000	the training set, and the output produced by the system y bar. Okay, so an example of this,
356000	361920	a very simple example of a setting like this is linear regression. In linear regression, x is a
361920	368320	vector composed of components x i's, w is also a vector, and the output is a scalar that is simply
368320	376160	the dot product of x with w. So y bar now is a scalar, and what you compute is the square
376160	382480	distance, the square difference really between y and y bar. If w is a matrix, then now y is a vector,
382480	388800	and you compute the square norm of the difference between y and y bar, and that's basically linear
388800	395360	regression. So learning will consist in finding the set of w's that minimize this particular cost
395360	400080	function average over a training set. I'll come to this in a minute, but I want you to think right
400080	405920	now about the fact that this g function may not be something particularly simple to compute. So
406240	412320	it may not be just multiplying a vector by a matrix. It may not be just carrying some
413600	418800	sort of fixed computation with sort of a fixed number of steps. It could involve something
418800	423840	complicated. It could involve minimizing a function with respect to some other variable that you
423840	430880	don't know. It could involve a lot of iteration of some algorithm that converges towards a fixed
430880	439920	point. So let's not restrict ourselves to g of x w that are simple things. It could be very
439920	446160	complicated things, and we'll come to this in a few weeks. So this is just to explain the
446160	454480	notations that I will use during the course of this class. So we have observed input and desired
454480	461920	output variables. Those are kind of gray grayish bubbles. Other variables that are produced by
461920	468800	the system or internal to the system are those kind of, you know, empty circle variables.
469920	473840	We have determinacy functions or functions that are so they are indicated by this sort of
474640	478480	rounded shape here. They can take multiple inputs have multiple outputs.
479920	484400	And each of those can be tensors or scalars or whatever. And they have implicit parameters
484480	490160	that are tunable by training. And then we have cost functions. So cost functions are basically
491360	497840	functions that take one or multiple inputs and output a scalar. But I'm not representing the
497840	504880	output is implicit. Okay, so if you have a red square, it has an implicit output. And it's a
504880	511680	scalar and we interpret it as a cost or an energy function. So this symbolism is kind of similar
511680	517760	to what people use in graphical models. If you if you've heard what a graphical model is,
517760	521440	particularly the type of graphical model called a factor graph. So in a factor graph, you have
521440	526640	those variable bubbles, and you have those factors, which are those square cost functions.
527520	530720	You don't have this idea that you had deterministic functions in it, because
530720	534240	graphical models don't care about the fact that you have functions in one direction or another.
534240	540960	But here we care about it. So with this extra symbol. Okay, so machine learning consists in
540960	547520	basically minimizing finding the set of parameters W that minimize the cost function averaged over
547520	556640	a training set. So a training set is a set of pairs x, x, y indexed by an index P. Okay, so we have
556640	562960	P training samples. And it'll be the index of the training set the training sample. And our overall
563680	566160	last function that we're going to have to minimize
568480	574480	is the, you know, is equal to the cost of the discrepancy between the y and the output of our
574480	583120	model by bar g of x, w, as I said earlier. So L is a value, C is a C is a module and L is a
583120	589280	is a is a way of writing C of y, g of x, w, you know, whether it depends explicitly on x, y and
589280	595440	w. Okay, but it's the same thing really. The overall last function, which is this
597040	603200	kind of curly L is the average of the per sample loss function over the entire training set. Okay,
603840	610160	so compute L for the entire training set, divide by some all the terms divide by P, and that's the
610160	616800	average. That's the loss. Okay, so now the name of the game is trying to find the minimum of that
616800	623600	loss with respect to the parameters. This is an optimization problem. So symbolically, I can
623600	629040	represent this entire graph as the thing on the right. This is rarely used in practice, but this
629040	636000	is sort of a way to visualize this. So think about each training sample as a sort of identical copy
636000	642960	of the replica, if you want, of the model and the cost function applied to a different training
642960	647200	sample, and then there is an average operation that computes the loss, right? So everything you
648000	654480	can write as a formula, you can probably write in terms of those graphs. This is going to be very
654480	661440	useful as we're going to see later. Okay, so supervised machine learning and a lot of other
661440	669440	machine learning patterns as well actually are can be viewed as function optimization and a very
669440	677120	simple approach to optimizing a function, which means finding the set of parameters to a function
677120	683680	that minimize its value, okay, is gradient descent or gradient based algorithms. So gradient based
683680	689520	algorithm makes the assumption that the function is somewhat smooth and mostly differentiable,
689520	694400	doesn't have to be everywhere differentiable, but has to be continuous, has to be almost everywhere
694400	700960	differentiable. And it has to be somewhat smooth, otherwise, the local information of the slope
700960	706240	doesn't tell you much about where the minimum is. Okay, so here's an example here depicted on the right.
708160	713120	The lines that you see here, the pink lines are the lines of equal cost and this cost is quadratic,
713760	720480	so it's basically a kind of paraboloid. And this is the trajectory of a method called stochastic
720480	724880	gradient descent, which we'll talk about in a minute. So for stochastic gradient descent, the
724880	728960	procedure is you show an example, you run you through the machine, you compute the objective
728960	735840	for that particular sample, and then you figure out by how much and how to modify each of the knobs
735840	740880	in the machine, the W parameters, so that the objective function goes down by a little bit,
740880	744560	you make that change, and then you go to the next sample. Let's be a little more formal.
745200	752320	So gradient descent is this very basic algorithm here, you replace the value of W by its previous
752320	759520	value minus a step size, eta here, multiplied by the gradient of the objective function with respect
759520	767760	to the parameters. So what is a gradient? A gradient is a vector of the same size as the
767760	773280	parameter vector. And for each component of the parameter vector, it tells you by how much
774080	780240	the the loss function L would increase if you increase the parameter by a tiny amount.
780240	785280	Okay, it's a derivative, but it's a directional derivative, right? So let's say among all the
785280	793200	directions, you only look at W34. And as you imagine that you tweak W34 by a tiny amount,
794080	799920	the loss function curly L is going to increase by a tiny amount, you divide the tiny amount by
799920	807120	which L increase by the tiny amount that you modified this W34. And what you get is the gradient
807120	813840	of the loss with respect to W34. If you do this for every single weight, you get the gradient
813840	818160	of the loss function with respect to all the weights. And it's a vector, which for each component
818160	825680	of the weight gives you the parameter gives you that quantity. Okay, so, you know, since Newton
825680	831280	and earlier, it's been written as, you know, dL over dW, because it indicates the fact that
831280	836000	there is this little twiddle where you can twiddle W by little. And there's a resulting
836880	840960	twiddling of L. And if you divide those two twiddles, and they are infinitely small,
840960	845840	you get the derivative that's kind of standard notation in mathematics for a few hundred years.
845840	855360	Okay, so now the gradient is going to be a vector. Okay. And as indicated here on the top, right,
855440	866560	that vector is an arrow that points upwards along the line of larger slope. Okay, so if you are
866560	872240	in a 2D surface, you have two W parameters. Okay, and the surface is represented here,
872880	878880	some sort of quadratic ball here in this case. So it's a second degree polynomial in W1 and W0.
879840	886320	Here on the right is the kind of a top-down view of this where the lines represent the lines of
886320	892320	equal cost. The little arrow is here, represent the gradient at various locations. Okay,
892960	898080	so you have a long arrow if the slope is steep, a short arrow if the slope is
899760	906720	not steep, not large. At the bottom, it's zero. And it points towards the direction of
907280	912480	higher slope. All right, so imagine you are in a landscape, a mountainous landscape,
913280	918640	and you're in a fog and you want to go down the valley. You look around you and you can tell
919920	927120	the local slope of the landscape. You can't tell where the minimum is because you're in a fog,
927760	934400	but you can tell the local slope. So you can figure out what is the direction of larger slope
934400	939600	and then take a step and it will take you upwards. Now you turn around 180 degrees,
939600	944640	take a step in that direction, and that is going to take you downwards. If you keep doing this
944640	950560	and the landscape is convex, which means it has only one local minimum, this will eventually
951520	957520	take you down to the valley and presumably to the village. Right, so that's gradient-based
957520	967840	algorithms. They all differ by how you compute the gradient first and by what this eta step-size
967840	975120	parameter is. So in simple forms, eta is just a positive constant that sometimes is decreased as
975120	983760	the system learns more, but most of the time not. But in more complex versions of gradient-based
983760	989200	learning, eta is actually an entire matrix itself, generally a positive definite or semi-definite
989200	997680	matrix. And so the direction adopted by those algorithms is not necessarily the steepest descent.
997680	1003360	It goes downwards, but it's not necessarily the steepest descent. And we can see why here. So
1004480	1009440	in this diagram here that I'm showing, this is a trajectory that will be followed by gradient
1009440	1015520	descent in this quadratic cost environment. And as you see, the trajectory is not straight.
1016160	1022080	It's not straight because the system goes down by following the slope of steepest descent. And so
1023280	1026640	it goes down the valley before finding the minimum of the valley, if you want.
1027280	1032160	So if your cost function is a little squeezed in one direction, it will go down the ravine
1032160	1037280	and then follow the ravine towards the bottom. In complex situations where you have
1038000	1044400	things that are, the trajectory actually is being cut here. But when the
1044400	1050720	weather function is highly irregular, this might even be more complicated. And then you might have
1050720	1055440	to be smart about what you do here. Okay. So stochastic gradient descent is
1056480	1066560	universally used in deep learning. And this is a slight modification of the gradient
1067440	1074960	steepest descent algorithm where you don't compute the gradient of the entire objective function
1074960	1082320	averaged over all the samples. But what you do is you take one sample and you compute the gradient
1082320	1087600	of the objective function for that one sample with respect to the parameters and you take a step.
1088240	1094160	Okay. And you keep doing this. You pick another sample, compute the gradient of the objective
1094160	1099760	function for that sample with respect to the way it's making a date. Why is it called stochastic
1099760	1108800	gradient? Stochastic is a fancy term for random, essentially. And it's called stochastic because
1108800	1115120	the evaluation of the gradient you get on the basis of a single sample is a noisy estimate of
1115120	1120240	the full gradient. The average of the gradients, because the gradient is a linear operation,
1120240	1124800	the average of the gradients will be the gradient of the average. And so things work out. If you
1124800	1130160	compute the gradient and you kind of keep going, overall, the average trajectory will be sort of
1130160	1136720	the trajectory you would have followed by doing full gradient. Okay. But in fact,
1137600	1142480	the reason we're doing this is because it's much more efficient in terms of speed of convergence. So
1143920	1147520	although the trajectory followed by stochastic gradient is very noisy,
1147520	1151760	things kind of bounce around a lot. As you can see in the trajectory here at the bottom,
1153280	1158080	you know, things have, the trajectory is very erratic. But in fact, it goes to the bottom faster
1158960	1164640	and has other advantages that people are still writing papers on. Okay. The reason for that is
1164640	1172320	that stochastic gradient exploits the redundancy between the samples. So all the, you know, machine
1172320	1177520	learning setting, the training samples have some similarities between them. If they don't, then
1177520	1182560	basically the learning problem is impossible. So they necessarily do have some redundancy
1182560	1186880	between them. And the faster you update the parameters, the more you, the more often you
1186880	1192960	update them, the more you exploit this redundancy between those parameters. Now in practice, what
1192960	1199200	people do is they use mini batches. So instead of computing the gradient on the basis of a single
1199200	1206800	sample, you take a batch of samples, typically anywhere between let's say 30 and a few thousand.
1208080	1212880	But smaller batches are better in most cases actually. And you compute the average of the
1213920	1221200	gradient over those samples. Okay. So compute the average cost over those samples and compute the
1221200	1227760	gradient of the average over those samples and then make an update. The reason for doing this
1228480	1235600	is not intrinsically an algorithmic reason. It's because it's a simple way of parallelizing
1236560	1242320	stochastic gradients on parallel hardware such as GPUs. Okay. So there's never, there's no good
1242320	1248880	reason to do batching other than the fact that our hardware likes it. Okay. Question. Yeah. So
1249600	1254880	for actually for, for real complex deep learning problems, does this objecting function have to
1254880	1260880	be continuously differentiable? Well, it needs to be continuous mostly. If it's non continuous,
1260880	1268960	you're going to get in trouble. It needs to be differentiable almost everywhere. But in fact,
1270320	1274640	neural nets that most people use are actually not differentiable. And there's a lot of places
1274640	1278400	where they're not differentiable. But they are continuous in the sense that there are functions
1278400	1283360	that have kind of corners in them, if you want. They have kinks. And if you have a kink once in a
1283360	1294880	while, it's not too much of a problem. But so in that case, those quantities should not be called
1294880	1299840	gradients, they should be called subgradients. Okay. So a sub gradient is basically a generalization
1299840	1307360	of the idea of derivative or gradient to functions that have kinks in them. So wherever you have a
1307360	1313760	function that has a kink in it, any, any slope that is between the slope of one, one side and the
1313760	1322400	slope of the other side is a, is a valid sub gradient. Okay. So when you write the kink,
1322400	1326720	you decide, well, the derivative is this or it's that or it's going to somewhere in between. And
1326720	1335440	you're fine. Most of the proof that applied to, you know, smooth functions, you know, in terms
1335440	1342800	of minimization, often apply also to non-smooth function that basically are differentiable most
1342800	1350000	of the way. So then how do we ensure strict convexity? We do not ensure strict convexity. The,
1351360	1357040	in fact, in deep learning systems, most deep learning systems, the function that we are
1357040	1363040	optimizing is non-convex, right? In fact, this is one reason why it took so long for deep learning
1363040	1368000	to become prominent is because a lot of people, particularly theoreticians, people who sort of
1368000	1372560	theoretically minded, were very scared of the idea that you had to minimize a non-convex
1372560	1376560	objective and they say, this can't possibly work because we can't prove anything about it.
1376560	1380240	It turns out it does work. You can't prove anything about it, but it does work. And so
1381040	1387440	this is a situation, and it's an interesting thing to think about, a situation where the,
1387440	1391600	the theoretical thinking basically limited what people could do in terms of engineering
1392480	1396240	because they couldn't prove things about it. But that would be actually very powerful.
1396240	1399040	Okay. Yeah. Like your colleague, you optimize non-convex functions.
1399840	1403440	Like your colleague at the Bell Labs, who didn't like the non-mathy.
1406160	1411520	Oh, it was a whole debate, you know, in the machine learning community that lasted 20 years,
1411520	1419120	basically. All right. So what about how doesn't SGD get stuck in local minima once it reaches them?
1419120	1422400	It does. Okay. So,
1424640	1432080	so full gradient does get stuck in local minima, right? SGD gets like, you know,
1432080	1436480	it's slightly less stuck in local minima because it's noisy. It allows it sometimes to escape
1436480	1445440	local minima. But the real reason why we're going to optimize non-convex functions and local minima
1445440	1452720	are not going to be such a huge problem is that there aren't that many local minima that are traps.
1452720	1459600	Okay. So we're going to build neural nets, and those neural nets are, or deep running systems,
1459600	1464720	and they're going to be built in such a way that the, the parameter space is such a, such a high
1464720	1469120	dimension that is going to be very hard for the system to actually create local minima for us.
1469120	1476240	Okay. So think about a picture where we have in one dimension a cost function that has one
1476240	1481440	local minima and then a global minimum, right? Okay. So it's a function like this, right?
1483280	1487040	And we start from here. If we optimize using gradient descent, we're going to get stuck in
1487040	1492160	that local minimum. Now, let's imagine that we parameterize this function now with two parameters.
1492160	1495840	Okay. So we're not a one dimensional, we're not looking at a one dimensional function anymore.
1495840	1498560	We're looking at two dimensional function. We have an extra parameter.
1499680	1505120	This extra parameter will allow us to go around the mountain and go towards the valley,
1505120	1510320	perhaps without having to climb the little hill in the middle. Okay. So this is just an intuitive
1510320	1515600	example to tell you that in very high dimensional spaces, you may not have as much of a local minimum
1515600	1521040	problem as you have in the sort of intuitive picture of low dimensional spaces, right? So here
1521040	1525120	that those pictures are in two dimensions. They are very misleading. We're going to be working
1525120	1534080	with millions of dimensions and you know, some of the most recent deep learning systems have
1534080	1540960	trillions of problems. Yeah. So local minima is not going to be that much of a problem. We're
1540960	1545760	going to have other problems, but not that one. So there is like a trend in this hyper like
1546960	1552080	over parameterization, right? Like it seems like that more neurons we have and the better
1552080	1556400	these networks work somehow. That's right. So we're going to make those networks very large and
1556400	1559520	they're going to be over parameterized, which means they're going to have way more adjustable
1559520	1563200	parameters than we would actually need, which means they're going to be able to learn the
1563200	1567360	training set almost perfectly. And the big question is how well are they going to work on
1567360	1572560	a separate validation set or test set that is separate from the training set?
1573520	1579040	Two more questions. They're going to work in a real situation where, you know, the distribution
1579040	1582640	of samples may be different from what we trained it on. So that's the real question of machine
1582640	1588320	learning, which I'm sure a lot of you are familiar with. Two more questions. Can we do?
1588320	1596880	Yeah. So how do we escape instead of subtle points? Right. So there are tons and tons of
1596880	1601920	subtle points in deep learning systems. A combinatorially large number of subtle points,
1601920	1608160	as a matter of fact. I'll have a lecture on this. So I don't want to kind of spend too long
1608160	1614240	answering. Okay. But yeah, there are subtle points. The trick with subtle points is you don't
1614240	1621920	want to get too close to them, essentially. And stochastic gradient helps a little bit
1621920	1627520	with subtle points. Some people are proposed sort of explicit methods to stay away from
1627520	1632480	subtle points. But in practice, doesn't seem to be that much of a problem, actually.
1632720	1638320	Finally, how do you pick samples for stochastic gradient in the center randomly?
1639680	1645760	Okay. There is lots of different methods for that. Okay. Yeah. I mean, the basic thing you should do
1645760	1650880	is you have your training set. You shuffle the samples in a random order. Okay. And then you
1650880	1659360	just pick them one at a time. And then you cycle through them. An alternative is once you get to
1659360	1664800	the end, you reshuffle them and then cycle through them again. An alternative is you pick a random
1664800	1670800	sample using a random number. Every time you pick a new sample, you pick them randomly.
1674400	1680800	If you do batching, a good idea is to put in a batch samples that are maximally different from
1680800	1684800	each other. So things that are, for example, different categories if you do classification.
1685200	1689600	But most people just do them, you know, just pick them randomly. But it's good to have samples
1689600	1695520	that are maximally different that are nearby either in a batch or during the processor training.
1695520	1700160	And then there are all kinds of tricks that people use to sort of emphasize difficult samples
1701120	1705280	so that the boring, easy samples are not, you don't waste your time just, you know,
1705280	1710800	seeing them over and over again. It's all kinds of tricks. All right. But, you know,
1711440	1716000	the simpler one is, which most people use, you shuffle your samples and you run through them.
1717040	1721360	Most people now use also data augmentation. So every sample is actually distorted by some
1722480	1726880	process. For an image, you can distort the geometry a little bit, you change the colors,
1726880	1734000	you add noise, et cetera. This is an artificial way of sort of adding more samples than you actually
1734000	1738880	have. And people do this kind of randomly on the fly or they kind of precompute those
1739840	1745760	those transformations. So lots of tricks there as well. Last question. How do you pick the batch
1745760	1752720	size? The best. The batch, batch size. Oh, the batch size. That's determined by your hardware.
1752720	1759600	So if you have a GPU, generally for, you know, reasonably sized networks, your batch size would
1759600	1764560	be anywhere between 16 and 64 or something like that. For smaller networks, you might have to batch
1764640	1769840	more to kind of exploit your, your hardware better to kind of have maximum usage of it.
1770560	1775040	If you parallelize on multiple GPUs within a machine, you may have to, to have, you know,
1775040	1779040	so let's say you have eight GPUs, then you'll be sort of eight times 32. So there's no
1780480	1786160	256 or something. And then, you know, a lot of the big guys kind of parallelize that over
1786160	1790640	multiple machines, each of which has eight GPUs. Some of them have TPUs, whatever. And then you
1790640	1794880	might have to parallelize over thousands of examples. This diminishing return in doing this,
1795520	1801600	when you increase the size of the batch, you actually reduce the, the, the speed of convergence.
1801600	1805360	You accelerate the calculation, but you reduce the speed of convergence. So at some point,
1805360	1811920	it's not worth increasing your batch size. So if we are doing a classification problem with k
1811920	1819840	classes, what's going to be like our go to batch size? So there are papers that say if your batch
1819840	1824880	size is significantly larger than the number of categories, or let's say twice the number of
1824880	1830800	categories, then you're, you're probably wasting competition, essentially. Okay. I mean, throwing
1830800	1836080	down convergence. So you're trying to train an image recognizer on ImageNet. If your batch size
1836080	1843200	is larger than about a thousand, you're probably wasting time. Okay, that's it. Thanks. I mean,
1843200	1848720	you're wasting competition. You're not wasting time. Okay. Okay. Okay. So let's talk about
1848720	1854960	traditional neural net. So a traditional neural net is a, a, a model, a particular type of
1854960	1862080	parameterized function, which is built by stacking linear and nonlinear operations. Right. So here
1862080	1865680	is this kind of a depiction of a traditional neural net here in this case, with two layers, but I'm,
1865680	1870640	you know, I'm not imagining there might be more layers here. So you have a bunch of inputs here
1870640	1879120	on the left. Each input is multiplied by a weight, different weights, presumably. And those, the
1879120	1885200	weighted sum of those inputs by those weights is, is computed here by what's called a unit or neuron.
1886160	1890960	People don't like using the word neuron in that context, because there are incredibly simplified
1890960	1897840	models of neurons in the brain, but, but that's the inspiration really. Okay. So one of those units
1897920	1902480	just computes a weighted sum of its inputs, using those weights. Okay, this unit use,
1902480	1907040	computes a different weighted sum of the same inputs with different weights and etc. So here
1907040	1910480	we have three units here in the first layer. This is called a hidden layer, by the way,
1911360	1914720	because it's neither an input nor an output, right? This is the input, and this is the output,
1914720	1919200	and this is somewhere in the middle. So we compute those weighted sums, and then we pass those
1919200	1924720	weighted sums individually through a, a nonlinear function. So here what I've shown is the value
1924720	1931440	function. So this is called rectified linear unit. In the, this is the name that people
1932000	1938000	have given it in the neural net lingual. In other contexts, this is called a half wave rectifier,
1938000	1944640	if you're an engineer. It's called positive part, if you are a mathematician. Okay. Basically,
1944640	1949600	it's a function that is equal to the identity when its argument is positive, and it's equal to zero
1949600	1958080	if its argument is negative. Okay. So very simple graph. And then we stack a second layer of the
1958080	1962480	same thing, the second stage, right? So again, a layer of linear operations where we compute
1962480	1967200	weighted sums, and then we pass a result to nonlinearities. And we can stack many of those
1967200	1972000	layers, and that's basically a traditional plain vanilla garden variety neural net.
1973200	1979120	In this case, fully connected. So fully connected neural net means that every unit in one layer
1979120	1983120	is connected to every unit in the next layer. And you have this sort of well organized layer,
1984640	1989760	or architecture, if you want, right? Each of those weights are going to be the things that
1989760	1995840	our learning algorithm is going to, is going to tune. And the big trick, the one trick really
1995840	2002640	of deep learning is how we compute those gradients. Okay. So if you want, if you want to write this,
2002720	2009200	you can say the weighted sum number i, so you can give a number to each of the units
2009920	2017680	in the network. So this unit with number i, and the weighted sum s of i, is simply the sum
2018400	2023840	where j goes over the upstream, the set of upstream units to i, which may be all the units in the
2023840	2031120	previous layer or not could be just a subset. Okay. And then you compute the product of zj,
2031120	2037600	which is the output of the unit number j times wij, which is the weight that links
2037600	2044080	unit j to unit i. Okay. And then after that, you take this si, which is the weighted sum,
2044080	2048400	you pass it through the activation function, this value, or whatever it is that you use,
2048400	2056000	and that gives you zi, which is the activation for unit i. Okay. Super notation. By changing the
2056000	2060240	set of upstream units of every unit, by building a graph of interconnection, you can basically
2060240	2067360	build any kind of network arrangement that you want. There is one constraint that we can lift,
2067360	2073120	that we will lift in the subsequent lecture, which is that the graph has to be
2076080	2080320	ac-click in the sense that it can't have loops. Okay. If you have loops, that means you can't
2080320	2086000	organize the units in layers. You can't sort of number them in a way that you can compute them
2086640	2091680	so that every time you want to compute a unit, you already have the state of the previous units.
2091680	2096720	If there are loops, then you can do that. Right? So for now, we're going to assume that
2097520	2100800	the wij matrix, the w matrix, doesn't have loops,
2104000	2109520	represents a graph that doesn't have loops. That's why I should say. Okay. So here's sort of an
2109520	2114560	intuitive explanation of the back propagation algorithm. So the back propagation algorithm
2114560	2121680	is the main technique that is used everywhere in deep learning to compute the gradient of
2122400	2128240	a cost function, whatever it is, objective function, with respect to a variable inside
2128240	2132960	of the network. This variable can be a state variable like a z or an s, or it could be a
2132960	2138080	parameter variable like a w. Okay. And we're going to need to do both. Okay. So this is going to be
2138080	2141600	an intuitive explanation. And then after that, there's going to be a more mathematical explanation,
2141600	2147200	which is less intuitive, but perhaps actually easier to understand. But let me start with
2147200	2151440	the intuition here. So let's say we have a big network. And inside of this big network, we have
2151440	2155920	one of those little activation functions. Okay. In this case, it's a sigmoid function, but
2155920	2160080	doesn't matter what it is for now. Okay. This function takes an s and produces a z.
2161040	2172400	We call this function h of s, right? So when we wiggle z, the cost is going to wiggle by some
2172400	2179120	quantity, right? And we divide the wiggling of z by the wiggling of c by the wiggling of z that
2179120	2185040	causes it. That gives us the partial derivative of c with respect to z. So this one term is a gradient
2185040	2189360	of c with respect to all the z's in the network. And there's one component of that gradient,
2189360	2196240	which is the partial derivative of the cost with respect to that single variable z inside the
2196240	2203040	network. Okay. And that really indicates how much c would wiggle if we wiggled z by some amount.
2203040	2206960	We divide the wiggling of c by the wiggling of z and that gives us the partial derivative
2206960	2213840	of c with respect to z. This is not how we're going to compute the gradient of c with respect to z,
2213840	2219200	but this is a description of what it is conceptually. Okay. Or intuitively, rather.
2219920	2223440	Okay. So let's assume that we know this quantity. So we know the partial derivative
2224000	2231280	of c with respect to z. Okay. So c with respect to z is this quantity here, dc over dz. Okay.
2232720	2237840	So think of dz as the wiggling of z and dc as the wiggling of c, divide one by the other,
2237840	2244800	and you get the partial derivative of c with respect to z. What we have here is,
2248400	2254000	what we have to apply is the chain rule, the rule that tells us how to compute the
2254560	2259280	derivative of a function composed of two individual functions that we apply one after the other.
2259280	2263600	Right. So remember, chain rule, if you have a function g, then you apply to another function h,
2264320	2267200	which is function of parameter s, and you want the derivative of it.
2268560	2272480	The derivative of that is equal to the derivative of g at point h of s,
2272480	2279440	multiplied by the derivative of h at point s. Right. That's chain rule. You know that a few
2279440	2285280	years ago, hopefully. Now, if I want to write this in terms of partial derivative, it's the same
2285280	2290080	thing, right? Partial derivative is just a derivative just with respect to one single variable.
2290080	2295840	So I would write this something like this, dc over ds. So c really is the result of applying
2295840	2302560	this h function to s, and then applying some unknown g function to compute c, which is kind
2302560	2309200	of the rest of the network plus the cost. But I'm just going to call the gradient. I'm going to
2309200	2318080	assume that this dc over dz is known. Someone gave it to me. So this variable here on the right,
2318080	2325840	dc over dz is given to me, and I want to compute dc over ds. So what I need to do is write this,
2326800	2334480	dc over ds equal dc over dz times dz over ds. Right. And why is this identity true? It's because
2334480	2341760	I can simplify by dz. It's as simple as this, right? So you have, you know, trivial algebra,
2341760	2346080	you have dz at the denominator here, dz at the numerator here, simplify, you get dc over ds.
2346560	2351520	It's a very trivial, simple identity, which is basically just generally applied to partial derivatives.
2353440	2360240	Now, dz over ds, we know what it is. It's just h prime of s, just the derivative of the h function.
2361760	2367040	So we have this formula, dc over ds equal dc over dz, which we assume is known, times h prime of s.
2368320	2373040	What does that mean? That means that if we have this component of the gradient of the cost function
2373040	2380640	with respect to z here, we multiply this by the derivative of the h function at point s,
2380640	2386320	the same point s that we had here. And what we get now is the gradient of the cost function
2386320	2392640	with respect to s. Now, here's the trick. If we had a chain of those h functions, we could keep
2392640	2396720	propagating this gradient backwards by just multiplying by the derivative of all those h
2396720	2403280	functions going backwards. And that's why it's called back propagation. So it's just a practical
2403280	2408240	application of a chain rule. And if you want to convince yourself of this, you can run through
2408240	2416160	this idea of perturbation. If I twiddle s by some value, it's going to twiddle z by some value equal
2416160	2426160	to ds times h prime of s, basically the slope of s. So dz equals h prime of s times ds. And then
2426160	2434640	I'm going to have to multiply this by dc over dz. And so I rearrange the terms and I get immediately
2434640	2443920	that this formula dc over ds equals dc over dz times h prime of s. So we had another element in
2443920	2450240	our multilayer net, which was the linear sum. And there, it's just a little bit more complicated,
2450240	2458080	but not really. So one particular variable z here, we would like to compute the derivative,
2458080	2465120	the partial derivative of our cost function with respect to that z. And we're going to assume that
2465120	2470000	we know the partial derivative of s with respect to each of those s's, the weighted sums at the
2470000	2478720	next layer that z is going into. So z only influences c through those s's. So presumably,
2478720	2486640	by basically multiplying how each of those s's influence c and then multiplying by how z
2487600	2491760	influences each of the s's and summing up, we're going to get the influence of z over c.
2491760	2495120	Right? And that's the basic idea. Okay, so here's what we're going to do.
2495840	2510240	Let's say we perturb z by dz. This is going to perturb s0 by dz times w0. Okay, we multiply z
2510240	2516960	by w0. So the derivative of this linear operation is the coefficient itself. Right? So here,
2517600	2529440	the perturbation is ds0 is equal to dz times w0. Okay? And now in turn, this is going to modify c,
2530160	2539520	and we're going to multiply this quantity by dc over ds0 to get the dc, if you want. Okay?
2540640	2545840	Now, whenever we perturb z, it's not going to perturb just s0, it's also going to perturb s1
2545840	2551280	and s2. And to see the effect on c, we're going to have to sum up the effect of the perturbation
2551280	2557120	on each of the s's and then sum them up to see the overall effect on c. So this is written here
2557120	2566400	on the left. The perturbation of c is equal to the perturbation of s multiplied by the partial
2566400	2573600	derivative of c with respect to s plus the perturbation of s1 multiplied by the partial
2573600	2580160	derivative of dc with respect to s1 plus same thing for s2. Okay? So this is the fact that,
2581280	2586880	you know, we need to take into account all the perturbations here that z may influence.
2588480	2593280	And so I can just write down now a very simple thing, you know, because dc of 0 is equal to
2593280	2601840	w0 times dz and, you know, ds of 2 is w2 times dz, I can plug this in there and just write dc
2601840	2608480	over dz equal dc over ds0, which I assume is known, times w0 plus dc over ds1 times w1
2608480	2614320	plus dc over ds2 times w2. Okay? If I want to represent this operation graphically,
2615200	2622720	this is shown on the right here. I have dc over ds0, dc over ds1, dc over ds2, which I assume
2622720	2635120	are known or given to me somehow. I compute dc over ds0 multiplied by w0 and multiply dc over ds1
2635120	2641760	by w1, dc over ds2 by w2. I sum them up and that gives me dc over dz. Okay? It's just the formula
2641760	2648800	here. Okay? So here's the cool trick about back propagation through a linear module that computes
2648800	2654480	weighted sums. You take the same weights and you still compute weighted sum with those weights,
2654480	2660480	but you use the weights backwards. Okay? So whenever you had the unit that was sending its output to
2660480	2667200	multiple outputs to multiple units through a weight, you take the gradient of the cost with
2667200	2674000	respect to all those weighted sums and you compute their weighted sum backwards using the weights
2674000	2681840	backwards to get the gradient with respect to the state of the unit at the bottom. You can do
2681840	2688720	this for all the units. Okay? So it's super simple. Now, if you were to write a program to do backprop
2688720	2694880	for classical neural nets in Python, it would take like half a page. It's very, very simple.
2697520	2701280	Is one function to compute weighted sums going forward in the right order?
2701360	2707840	Another function and applying the nonlinearity is another function to compute weighted sums
2707840	2713120	weighted sums going backward and multiplying by the derivative of the nonlinearity at every step.
2714160	2719200	Right? It's incredibly simple. What's surprising is that it took so long for people to realize this
2719200	2726480	was so useful, maybe because it was too simple. Okay? So it's useful to write this in matrix form.
2726480	2734800	So really, the way you should think about a neural net of this type is each state inside the
2734800	2738560	network, think of it as a vector. It could be a multidimensional array, but let's think of it
2738560	2744000	just as a vector. A linear operation is just going to multiply this vector by matrix and each row of
2744000	2748880	the matrix contains all the weights that are used to compute a particular weighted sum for a particular
2748880	2758560	unit. Okay? So multiply this by this matrix. So this dimension has to be equal to that dimension,
2758560	2762560	which is not really well depicted here, actually. One sec. From the previous slide,
2762560	2771040	you wrote ds0. What is s, differentiated with respect to? So there is a ds. What is ds, basically?
2771600	2781600	ds0, you mean? Yeah. Okay. ds0 is a perturbation of s0. Okay? An infinitely small perturbation of s0.
2783040	2788880	Doesn't matter what it is. Okay? And what we're saying here is that if you have an infinitely
2788880	2794960	small perturbation of s0, and you multiply this perturbation by the partial derivative of c with
2794960	2805280	respect to s0, okay? You get the perturbation of c, except that that corresponds to this
2805280	2809440	perturbation of s0, right? But we're not interested in just the perturbation of s0. We're
2809440	2814080	also interested in the perturbation of s1 and s2. So the overall perturbation of c would be the sum
2814080	2820160	of the perturbations of s0, s1, and s2 multiplied by the corresponding partial derivative of c
2820160	2826960	with respect to each of them. Okay? You know, it's a virtual thing, right? It's not an existing
2826960	2832080	thing you're going to manipulate. Just imagine that there is some perturbation of s0 here.
2833200	2836880	Okay? This is going to perturb c by some value, and that value is going to be the perturbation
2836880	2842640	of s0 multiplied by the partial derivative of c with respect to s0. Okay? And then if you perturb
2843360	2849440	s1 simultaneously, you're also going to cause a perturbation of c. If you perturb s2 simultaneously,
2849440	2854560	you're also going to cause a perturbation of c. The overall perturbation of c will be the sum
2854560	2860960	of those perturbations, and that is given by this expression here. Now, those d, those infinitely
2860960	2867600	small quantities, ds, dc, etc., think of them as, you know, numbers. You can do algebra with them.
2867600	2871680	You can divide one by the other. You know, you can do stuff like that. So now you say, you know,
2871680	2886080	what is ds0 equal to? If I tweak z by a quantity dz, it's going in turn to modify s0 by ds0.
2886080	2894880	Okay? And what is the quantity by which s0 is going to be tweaked? If I tweak z by dz,
2894880	2902400	because s is the result of computing the product of z by w0, then the perturbation is also going
2902400	2909360	to be multiplied by w0, right? So the ds0 corresponding to a particular dz is going
2909360	2916000	to be equal to dz times w0. And this is what's expressed here. Okay? ds0 equal w0 dz.
2917200	2922160	Okay. Now, if I take this expression for ds0 and I insert it here in this formula,
2923040	2930960	okay, I get dc equal w0 times dz times dc over ds0 plus same thing for 1 plus same thing for 2.
2930960	2937360	And I'm going to take the dz and pass it to the other side. I'm going to divide both sides by dz.
2937360	2944000	So now I get dc over dz equal, the dz doesn't appear anymore because it's been put underneath
2944000	2953280	here. It's w0 times dc over ds0 plus w1 times dc over ds1, et cetera. Okay? It's just simple algebra.
2955360	2960160	It's differential calculus, basically. Right. So it's better to write this in matrix form.
2961680	2970240	So really, when you're computing, if I go back a few slides, when this is really kind of a matrix
2970240	2976400	of all the weights that are kind of upstream of the zj's, so you can align the zj as a vector,
2977600	2985360	maybe only the zj's that have nonzero terms in w, wij. And then you can write those w's
2985360	2991120	as a matrix, and this is just a matrix vector product. Okay? So this is the way this would be
2991120	2995520	written. You have a vector, you multiply by matrix, you get a new vector, pass that through
2995520	3001840	nonlinearities, reuse, multiply that by matrix, et cetera. Right? So symbolically, you can write
3002400	3008000	a simple neural net this way. We have linear blocks, okay, linear functional blocks, which
3008000	3015200	basically take the previous state and multiply by matrix. Okay? So you have a state here, z1,
3015200	3021760	multiply by matrix, you get w1, z1, and that gives you the vector of weighted sums, s2.
3021760	3029440	Okay? Then you take that, pass it through the nonlinear functions, each component individually,
3030160	3036800	and that gives you z2. Right? So that's a three-layer neural net. First weight matrix,
3036800	3041520	nonlinearity, second weight matrix, nonlinearity, third weight matrix, and this is the output.
3041520	3047920	There are two hidden layers, three layers of weights. Okay, the reason for writing it this way
3047920	3054720	is that this is, like symbolically, the easiest way to understand really what kind of backprop
3054720	3061120	does. And in fact, it corresponds also to the way we define neural nets and we run them on
3063120	3071360	deep learning frameworks like PyTorch. So this is the sort of object-oriented version of
3072320	3079440	defining a neural net in PyTorch. We're going to use predefined class, which are the linear class
3080480	3087040	that basically multiplies a vector by matrix. It also has biases, but let's not talk about this
3087040	3092320	just now. And another class, which is the value function, which takes a vector or a multi-dimensional
3092320	3098800	array and applies the nonlinear function to every component separately. Okay, so this is
3098800	3106000	a little piece of Python program that uses Torch. We import Torch. We make an image, which is, you
3106000	3111520	know, 10 pixels by 20 pixels and three components for color. We compute the size of it and we're
3111520	3116320	going to plug a neural net where the number of inputs is the number of components of our image.
3116320	3122000	So in this case, that would be 600 or so. And we're going to define a class. The class is going
3122000	3126720	to define a neural net and that's pretty much all we need to do here. So we define our network
3126720	3130000	architecture. It's a subclass of neural net module, which is a pretty fine class.
3131520	3136880	It's got a constructor here that will take the sizes of the internal layers that we want,
3136880	3143360	the size of the input, the size of S1 and Z1, the size of S2 and Z2, and the size of S3.
3145040	3152720	We call the parent class initializer. And then we just create three modules that are all linear
3153680	3157040	modules. And we need to kind of store them somewhere because they have internal parameters.
3157040	3163120	So we're going to have three slots in our object, N0, N1, N2, module 1, module 0, module 1, module 2.
3164000	3169440	And each of them is going to be an instance of the class NN.linear with two sizes, the input size
3169440	3175440	and the output size. Okay, so the first module has input size D0, output size D1, etc. And those
3175440	3180880	classes are, since there is a capital L, means it's an object and inside there are parameters
3180880	3188000	inside that item there. Right. So for example, the value doesn't have a capital because it doesn't
3188000	3192960	have internal parameters. It's not kind of a trainable module. It's just a function. Whereas
3192960	3197120	those things with capitals, they have sort of internal parameters, the weight matrices inside
3197120	3203760	of them. So now we define a forward function, which basically computes the output from the input.
3204480	3210800	And the first thing we do is we take the input thing, which may be a multidimensional array,
3210800	3218480	and we flatten it. We flatten it using this idiomatic expression here in PyTorch.
3219840	3228320	And then we apply the first module to X. We put the result in S1, which is a temporary variable,
3228960	3236000	then we apply the value to S1, put the result in Z, then apply the second layer,
3236000	3241840	put the result in S2, apply the value again, put the result in S3, and then the last linear
3241840	3246960	layer, put the result in S3 and return S3. And there is a typo. So the second line should have
3246960	3260320	been S1, it's the self.m0 of Z0, right? Z0 here, yes, correct. Yeah, this is something that
3261040	3267520	is going to be fixed, right? Which I didn't fix. I know. This is Z0. Thanks for reminding me of this.
3270480	3274960	Okay, but you'll see examples. I mean, I'll show you kind of actual examples of this,
3274960	3279040	and you'll be able to run them yourself. That's all you need to do. You don't need to write
3281440	3286240	how you compute the back prop, how you propagate the gradients. You could write it,
3286240	3289600	and it would be as simple as forward. You could write a backward function, and it would basically
3290640	3294160	multiply by the matrices going backwards. But you don't need to do this because
3294160	3298720	PyTorch does this automatically for you. When you define the forward function, it knows what
3299520	3302960	modules you've called in what order, what are the dependencies between the variables,
3302960	3307280	and it will know how to generate the functions that compute the gradient
3307280	3312400	backwards. So you don't need to worry about it. That's the magic of PyTorch, if you want.
3312400	3316720	That's a bit the magic of deep learning, really. That's called automatic differentiation,
3318720	3323120	and this is a particular form of automatic differentiation. There's another way to write
3323680	3328000	functions in PyTorch that are kind of more functional. So you're not using modules
3328000	3331840	with internal parameters. You're just coding functions one after the other. And PyTorch
3331840	3336800	has a mechanism by which it can automatically compute the gradient of any function you define
3336800	3342240	with respect to whatever parameters you want. Yeah, actually, these big guys with the capital L,
3342240	3348720	like the nn.capital linear inside is going to have a lowercase linear, which is like the functional
3348720	3355040	part, which is performing the matrix multiplication between the weights stored inside the object
3355040	3361920	with the capital L and then the input. So every capital letter object will inside have
3361920	3367680	the functional way. So one can decide to use either the functional form by default,
3367680	3373840	or use this encapsulated version, which are more convenient to just use, right?
3374400	3379920	Right. So at the end, you can create an instance of this class. You can create multiple instances,
3379920	3384080	but you can create one here, just call my net and give it the sizes you want.
3385360	3391360	And then to apply this to a particular image, you just do how to equal model of image. That's
3391360	3398320	as simple as that. Okay, so this is your first neural net, and it does all the backup automatically.
3399520	3404800	But you need to understand how that works, right? It's not because PyTorch does it for you,
3405360	3411440	that you can sort of forget about how you actually compute the gradient of a function,
3411440	3414320	because it's inevitable that at some point, you're going to want to
3414960	3418240	actually assemble a neural net with a module that does not pre-exist, and you're going to have to
3418240	3424560	write your own backup function. So to do this, you basically have, if you want to create a new module
3424560	3431840	with some complex operation that does not pre-exist in PyTorch, then you do something like
3431840	3436880	this. You define your class, but you write your own backward function, basically.
3438960	3448560	Okay, so let's get one step up in terms of abstraction, and write this in sort of slightly more
3451120	3458480	generic form, mathematical form, if you want. So let's say we have a cost function here,
3459040	3464560	and we want to compute the gradient of this cost function with a stack to a particular
3464560	3468880	vector in the system ZF. It could be a parameter, it could be a state, it doesn't matter. Okay,
3469680	3476320	some states inside. And we have chain rule, and chain rule is nothing more than this,
3476320	3484320	that I explained earlier. dC over dZF is equal to dC over dZG, dZG over dZF, as long as C
3484640	3493760	is only influenced by ZF through ZG. There's no other way for ZF to influence C than to go
3493760	3500160	through ZG, then this formula is correct. Okay? And of course, the identity is trivial,
3500160	3508640	because it's just a simplification by this infinitesimal vector quantity dZG. Okay?
3509600	3516080	So let's say ZG is a vector of size dG by one, so this means column vector. Okay?
3516800	3520560	And ZF is a column vector of size dF.
3525600	3529280	This is, if you want to write the correct dimensions of this,
3531200	3535680	you know, we get something a little complicated. Okay, so first of all,
3536640	3543440	this object here, dZG over dZF, well, let me start with this one. Okay,
3543440	3548800	this one dC over dZG, that's a gradient vector. Okay? ZG is a vector, dC over dZG is a gradient
3548800	3556320	vector. And it's the same size as dZG. But by convention, we actually write it as a line,
3556320	3563440	as a row vector. Okay? So this thing here is going to be a row vector whose size is the same size
3563440	3567440	as ZG, but it's going to be horizontal instead of vertical. Okay?
3570400	3573600	This object here is something more complicated. It's actually a matrix.
3574640	3580800	Why is it a matrix is because it's the derivative of a vector with respect to another vector.
3580800	3586240	Okay? So let's look at this diagram here on the right. We have a function G, it takes ZF as an
3586240	3592640	input, and it produces ZG as an output. And if we want to capture the information about the
3592640	3600320	derivative of that module, which is this quantity here dZG over dZF, there's a lot of terms to
3600320	3606160	capture because there's a lot of ways in which every single output, every component of ZG can be
3606160	3611680	influenced by every component of ZF. Right? So if for every pair of components, ZG and ZF,
3612240	3619040	there is a derivative term, which indicates by how much ZG would be perturbed if I perturbed ZF
3619040	3627920	by a small infinitesimal quantity. Right? We have that for every pair of components of ZG and ZF.
3627920	3635760	As a result, this is a matrix whose dimension is the number of rows is the size of ZG and the
3635760	3646160	number of columns is the size of ZF. And each term in this matrix is one partial derivative term.
3646160	3653760	So this whole matrix here, if I take the component ij, it's the partial derivative of the i-th
3653760	3662560	output of that module, the i-th component of ZG, with respect to the j-th component of ZF.
3664000	3672000	Okay? So what we get here is a row vector is equal to a row vector multiplied by a matrix,
3672000	3678720	and the sizes kind of work out so that they're compatible with each other.
3679920	3683280	Okay. So what is back propagation now? Back propagation is this formula.
3684720	3689200	Okay? It says if you have the gradient of some cost function with respect to some variable,
3689200	3692080	and you know the dependency of these variables with respect to another variable,
3692080	3696960	you multiply this gradient vector by that Jacobian matrix, and you get the gradient
3696960	3703120	vector with respect to that second variable. So graphically here on the right, if I have
3703920	3709520	the gradient of the cost with respect to ZG, which is DC over DZG, and I want to compute
3709520	3716080	the gradient of C with respect to ZF, which is DC over DZF, I only need to take that vector,
3716080	3723040	which is a row vector, multiply it by the Jacobian matrix, DG over DZF, or DZG over DZF,
3723920	3731280	and I get DC over DZF. Okay? It's this formula. Someone is objecting here. Isn't the summation
3731280	3739840	missing here? Which summation? Summation of all the components of these partial multiplications.
3739840	3744480	Here? Yeah. Well, this is a vector. This is a vector. This is a matrix. There is a lot of
3744480	3747760	sums going on here because when you compute the product of this vector with its matrix,
3747760	3752720	you're going to have a lot of sums, right? Yep. So it's hidden, right? Yeah. The sums are hidden.
3753040	3755520	Okay. Inside of this vector matrix product.
3760080	3764320	Like, you can take a specific example. Let's imagine that this G function is just a matrix
3764320	3772160	multiplication. Okay? We just multiply by ZF by matrix W. So we have a linear operation. The derivative
3772160	3779360	of the Jacobian matrix of the multiplication by matrix is the transpose of that matrix. So what
3779360	3784000	we're going to do here is take this vector, multiply it by the transpose of the W matrix,
3784800	3793200	and what we get is that vector. Okay? And it all makes sense, right? The sizes make sense.
3793200	3799600	This matrix here is the transpose of the weight matrix, which of course had the reverse size.
3800640	3804240	We multiply it. We pre-multiply it by the row vector of the gradient from the
3805200	3808080	layer above, and we get the gradient with respect to the layer below.
3810640	3815760	Okay? So backpropagating through a linear module just means multiplying the transpose
3815760	3822000	of the matrix used by that module. And it's just a generalized form of what I explained earlier,
3822560	3828000	you know, of propagating through the weights of a linear system. But it's less intuitive, right?
3830080	3833840	Okay. So we're going to be able to do backpropagation by computing gradients all the way through,
3833840	3842080	by propagating backwards. But this module really has two inputs. It has an input, which is ZF,
3842080	3851040	and the other one is WG, the weight matrix, the parameter vector that is used inside of this
3851040	3857760	module. So there is a second Jacobian matrix, which is the Jacobian matrix of ZG with respect to
3858560	3865200	the terms of this weight parameter. Okay? And to compute the gradient of the cost function
3865200	3872080	with respect to those weight parameters, I need to multiply this gradient vector by the Jacobian
3872080	3877360	matrix of that block with respect to its weight. And it's not the same as the Jacobian matrix with
3877360	3884480	respect to the input. It's a different Jacobian matrix. I'll come back to this in a second.
3885120	3895200	So to do backprop, again, if we have a vector of gradients of some cost with respect to a state,
3895920	3900960	and we have a function that is a function of one or several variables, we multiply this gradient by
3900960	3906160	the Jacobian matrix of this block with respect to each of these inputs, and that gives us the
3906160	3911520	gradient with respect to each of the inputs. And that's going to be expressed here. So this
3911520	3921440	is the backpropagation of states in a layer-wise classical type neural net. DC over DZK, which is
3921440	3927920	the state of layer K, is DC over ZK plus one, which is the gradient of the cost with respect to
3927920	3935120	the layer above, times the Jacobian matrix of the state of layer K plus one with respect to the
3935200	3942000	state of layer K. Now we assume DC over DZK plus one is known, and we just need to multiply
3942000	3946720	with the Jacobian matrix of the function that links ZK to ZK plus one. The function is used to
3946720	3951280	compute ZK plus one from ZK. And this may be a function also of some parameters inside.
3951280	3957440	But here, that's the matrix of partial derivatives of F, which is with output to ZK plus one,
3957440	3964800	with respect to each of the components of ZK. So that's the first rule of backpropagation,
3964800	3969840	and it's a recursive rule. So you can start from the top. You start initially with DC over DC,
3969840	3976640	which is one, which is why I have this one here on top. And then you just keep multiplying by
3977680	3983360	the Jacobian matrix all the way down, and backpropagate gradients. And now you get gradients
3983360	3986960	with respect to all the states. You also want the gradients with respect to the weights,
3986960	3992240	because that's what you need to do learning. So what you can write is the same chain rule,
3992240	3998160	DC over DWK is equal to DC over the ZK plus one, which we assume is known, times
3998160	4004240	DZK plus one of DWK, right? And you can write this as DC over DK plus one. And the dependency
4004240	4010560	between ZK plus one and WK is the function ZK applied to WK. So you can differentiate the
4011920	4015280	function, the output of the function ZK with respect to WK, and that gives you another
4015280	4019360	Jacobian matrix. And so those two formulas, you can do backpropagation just about anything.
4020080	4026320	Really what goes on inside PyTorch and inside most of those frameworks, TensorFlow and
4026320	4032000	Jackson, whatever. It's something like this where you have, so let's take a very simple
4032000	4036960	diagram here where you have an input parameterized function that computes an output that goes to
4036960	4041520	a cost function. And that cost function measures the discrepancy between the output of the system
4041600	4049840	and the desired output. So you can write this function as C of G of W. I didn't put the X here,
4049840	4057920	but just for charity. And the derivative of this is, again, you apply chain rule or you can write
4058000	4071600	it with partial derivatives this way. And same for, you know, expand the dependency of the output
4071600	4078160	with respect to the parameters as the Jacobian matrix of G with respect to W. If W is a scalar,
4078160	4086240	then this is just a derivative, partial derivative. Okay, now you can express this as a compute graph.
4086240	4091920	So you can say, like, how am I going to compute DC over DW? What I'm going to have to do is take
4091920	4095920	the value one, which is the derivative of C with respect to itself, basically,
4095920	4101200	the loss with respect to itself. I'm going to multiply this by the derivative of the cost with
4101200	4112800	respect to Y bar. Okay, and that's going to give me DC over DY bar, obviously. Okay, this is the
4112880	4118000	same as this because I'm just multiplied by one. Then multiply this by the Jacobian matrix of G
4118000	4123840	with respect to W, which is a derivative if W is a scalar. That, of course, depends on X.
4125280	4132000	And I get DC over DW. So this is a so-called compute graph, right? This is a way of organizing
4132000	4138560	operations to compute the gradient. And there is essentially an automatic way of transforming
4138640	4145200	a compute graph of this type into a compute graph of this type that computes the gradient
4145200	4151920	automatically. And this is the magic that happens in the automatic differentiation inside PyTorch and
4151920	4158000	TensorFlow and other systems. Some systems are pretty smart about this in a sense that
4159120	4164640	those functions can be fairly complicated. They can involve themselves computing derivatives and
4165600	4170400	they can involve dynamic computation, where the graph of computation depends on the data.
4171200	4175920	And actually PyTorch does this properly. I'm not going to go through all the details of this,
4175920	4181120	but this is kind of a way of reminding you what the dimensions of all those things are, right? So
4181920	4186160	if Y is a column vector of size M, W is a column vector of size N,
4187600	4191920	then this is a row vector of size N, this is a row vector of size M, and this is a
4191920	4197600	geocomium matrix of size N by N. And all of this works out.
4201680	4206480	Okay, so the way we're going to build neural nets, and I'll come back to this in a
4206480	4215280	subsequent lecture, is that we are going to have at our disposal a large collection of basic modules
4215280	4221200	which we're going to be able to arrange in more or less complex graphs
4222400	4230320	as a way to build the architecture of a learning system. Okay, so either we're going to write a
4230320	4236640	class or we're going to write a program that runs the forward pass, and this program is going to be
4237440	4244160	composed of basic mathematical operations, addition, subtraction of tensors or multi-dimensional arrays,
4245840	4250560	other types of scalar operations, or the application of one of the predefined
4251760	4257760	complex parameterized functions, like a linear module, a value, or things like that.
4260320	4268080	And we have at our disposal a large library of such modules, which are things that people have
4268080	4274640	come up with over the years that are kind of basic modules that are used in a lot of applications.
4275280	4279280	Right, so the basic things that we've seen so far I think is like values. There's other
4279280	4284400	nonlinear functions like sigmoids and variations of this. There's a large collection of them.
4285280	4288800	And then we have cost functions like square error, cross entropy, hinge loss, ranking loss,
4288800	4292400	and blah, blah, blah, which I'm not going to go through now, but we'll talk about this later.
4295520	4302880	The nice thing about this formalism is that, as I said before, you can sort of compute
4303440	4312640	graphs. You can construct a deep learning system by assembling those modules in any kind
4312640	4319360	of arrangement you want, as long as there is no loops in the connection graph. So as long as
4319920	4324080	you can come up with a partial order in those modules that will ensure that they are
4324080	4331040	computed in the proper way. But there is a way to handle loops, and that's called recurrent
4331040	4338400	nets. We'll talk about this later. Okay, so here's a few practical tricks if you want to
4338400	4343360	play with neural nets, and you're going to do that soon enough, perhaps even tomorrow.
4348800	4352960	And these are kind of a bit of a black art of deep learning, which is sort of a lot of it is
4352960	4357280	implemented already in things like PyTorch if you used under tools, but some of it is kind of more
4357280	4362000	of the sort of oral culture if you want of the deep learning community. You can find this in
4362000	4369760	papers, but it's a little difficult to find sometimes. So most neural nets use values as
4369760	4375280	the main nonlinearity, so this sort of half wave rectifier. Hyperbole tangent, which is a
4375280	4380160	similar function, and logistic function, which is also a similar function, are used, but not as
4380160	4384880	much, not nearly as much. You need to initialize the ways properly. So if you have a neural net
4384880	4390400	and you initialize the ways to zero, it never takes off. It will never learn. The gradients
4390400	4397200	will always be zero all the time. And the reason is because when you back propagate the gradient,
4397200	4401600	you multiply by the transpose of the weight matrix. If that weight matrix is zero, your gradient is
4401600	4407360	zero. So if you start with all the weights equal to zero, you never take off. And someone asked
4407360	4414640	the question about saddle points before. Zero is a saddle point. And so if you start at this
4414640	4420000	saddle point, you never get out of it. So you have to break the symmetry in the system. You have to
4420000	4426800	initialize the weights to small random values. They don't need to be random, but it works fine
4426800	4434160	if they're random. And the way you initialize is actually quite important. So there's all kinds of
4434160	4439040	tricks to initialize things properly. One of the tricks was invented by my friend,
4439920	4445760	about 30 years ago, even more than that, actually, 34 years ago, almost. Unfortunately, now it's
4445760	4451680	called differently. It's called the kaming trick, but it's the same. And it consists in
4452240	4457280	initializing the weights to random values in such a way that if a unit has many inputs, the weights
4457280	4462640	are smaller than if it has few inputs. And the reason for this is that you want the weighted
4462640	4469680	sum to be roughly kind of have some reasonable value. If the input variables have some reasonable
4469680	4476160	value, let's say variance one or something like this, and you're computing a weighted sum of them,
4476160	4481840	the weighted sum, the size of the weighted sum is going to grow like the square root of the number
4481840	4486880	of inputs. And so you want to set the weights to something like the inverse square root if you want
4486880	4494160	the weighted sum to be kind of about the same size as each of the inputs. So that's built into
4494160	4500160	PyTorch. You can call this, you know, initialization procedure. What's the exact name of it? I can't
4500160	4506160	remember. The one that is coming, coming, coming here, then there is the Xavier and then there is
4506160	4510800	also yours we have in PyTorch. Yeah, they're slightly different, but they kind of do the same
4510800	4519120	more or less. Yeah, the Xavier Glow version, yeah. Yeah, this one divides by the Fennin and Fennin.
4520880	4525520	There's various loss functions, so I haven't talked yet about what the cross-entropy loss is,
4525520	4530160	but cross-entropy loss is a particular cost that's used for classification. I'll probably
4530160	4535600	talk about this next week and I'll have some time at the end of this lecture. This is for
4535600	4541760	classification. As I said, we use stochastic gradient descent on mini-batches and mini-batches
4541760	4546560	only because the hardware that we have needs mini-batches to perform properly. If we had
4546560	4552160	different hardware, we would use mini-batch size one. As I said before, we need to shuffle the
4552160	4558960	training samples. So if someone gives you a training set and puts all the examples of category one,
4558960	4564080	then all the example category two, all the example category three, etc. If you use stochastic gradient
4564080	4569760	by keeping this order, it is not going to work. You have to shuffle the samples so that
4571520	4577360	you basically get samples from all the categories within kind of a small subset, if you want.
4579680	4584160	There is an objection here for the stochastic gradient. Isn't Adam better?
4584880	4595920	All right. Okay. There is a lot of variants of stochastic gradient. There are all stochastic
4595920	4602240	gradient methods. In fact, people in optimization said this should not be called stochastic gradient
4602240	4607360	descent because it's not a descent algorithm because stochastic gradient sometimes goes uphill
4607360	4615200	because of the noise. So people who want to really kind of be correct about this say it's
4615200	4618400	stochastic gradient optimization, but not stochastic gradient descent. That's the first thing.
4619520	4625280	Stochastic gradient optimization or stochastic gradient descent, SGD, is a special case of gradient
4625280	4633280	based optimization. The specification of it says you have to have a step size eta,
4634000	4638720	but nobody tells you how you set this step size eta and nobody tells you that this step size is
4638720	4646320	a scalar or a diagonal matrix or a full matrix. Okay. So there are variations of SGD in which
4647440	4651360	eta is changed all the time for every sample or every batch.
4654720	4659280	In SGD, most of the time this eta is decreased according to a schedule and there are a bunch
4659280	4668000	of standard schedule in PyTorch that are implemented. In techniques like Adam, the eta is actually a
4668000	4672160	diagonal matrix and that diagonal matrix, the term in the diagonal matrix are changed all the
4672160	4678640	time. They're computed based on some estimate of the curvature of the cost function. There's a lot
4678640	4686720	of methods to do this. Okay. They're all SGD type methods. Okay. Adam is an SGD method with a special
4686720	4696320	type of eta. So yeah, in the opt-in package in Torch, there's a whole bunch of those methods.
4699040	4703280	There's going to be a whole lecture on this, so don't worry about it, about optimization.
4705520	4711920	Normalize the input variables to zero mean and unit variance. So this is a very important point that
4712560	4720240	this type of optimization method, gradient based optimization methods, when you have weighted
4720240	4726400	sounds, kind of linear operations, tends to be very sensitive to how the data is prepared.
4727440	4730400	So if you have two variables that have very widely different
4731360	4736480	variances, one of them varies between, let's say, minus one and plus one. The other one
4736480	4742880	varies between minus 100 and plus 100. The system will basically not pay attention to the one that
4742880	4748240	varies between plus one and minus one. We'll only pay attention to the big one. And this may be good
4748240	4753360	or this may be bad. Furthermore, the learning rate you're going to have to use the eta parameter,
4753360	4759120	the step size, is going to have to be set to a relatively small value to prevent the weights
4759120	4767520	that look at this highly variable input from diverging. The gradients are going to be very
4767520	4771680	large because the gradients basically are proportional to the size of the input or even to
4771680	4776000	the variance of the input. So if you don't want your system to diverge, you're going to have to
4776000	4783520	tune down the learning rate if the input variance is large. If the input variables are all shifted,
4783600	4791040	they're all between, let's say, 99 and 101 instead of minus one and one. Then again, it's very
4791040	4798480	difficult for a gradient-based algorithm that use weighted sums to figure out those things.
4798480	4803760	Again, I'll talk about this more formally later. Right now, just remember the trick
4803760	4807840	that you need to normalize your input. So basically, take every variable of your input,
4807840	4812320	subtract the mean, you compute the mean over the training set of each variable. So let's say your
4812320	4818880	training set is a set of images. The images are, let's say, 100 by 100 pixels. Let's say they're
4818880	4823760	grayscale, so you get 10,000 variables. And let's say you get a million samples, right? You're going
4823760	4832080	to take each of those 10,000 variables, compute the mean of it over the training set, compute the
4832080	4838000	standard deviation of it over the entire training set. And the samples you're going to show to your
4838000	4846320	system are going to be a sample where you have subtracted the mean from each of the 10,000 pixels
4846320	4854000	and divided the resulting values by the standard deviation that you computed.
4855360	4858720	Okay? So now what you have is a bunch of variables that are all zero mean
4859520	4865680	and all standard deviation equal to one. And that makes your neural net happy.
4866080	4868880	That makes your optimization algorithm happy, actually.
4868880	4876000	We have actually a question. So you keep repeating SGD type methods, gradient based methods,
4876000	4882160	because there are other types of methods. Yes. Okay. So there is gradient free methods.
4882160	4889040	So gradient free method is a method where you do not assume that the function you're trying to
4889040	4893920	optimize is differentiable or even continuous with respect to the parameters.
4894640	4902800	For several reasons, perhaps it's a function that looks like a golf course, right? It's flat and
4902800	4907360	then maybe it's got steps and, you know, it's difficult to, like the local gradient information
4907360	4912080	does not give you any information as to where you should go to find the minimum. Okay?
4913520	4920080	It could be that the function is essentially discrete, right? It's not a function of continuous
4920080	4926960	variables, function of discrete variables. So for example, am I going to win this chess game?
4928240	4931760	The variable you can manipulate is the position on the board. That's a discrete variable.
4933280	4940160	So you can't, you can compute a gradient of, you know, a score with respect to a position on
4940160	4949120	the chess game. It's a discrete variable. Another example is the cost function is not
4949120	4953360	something you can compute. You don't actually know the cost function. Okay? So for example,
4954720	4959280	the only thing you can do is give an input to the cost function and it tells you the cost.
4959920	4963600	But you can't, you don't know the function. It's not, right? It's not a program on a computer.
4963600	4968720	You can't backprop a gradient to it. A good example of this is the real world. The real world,
4969840	4975120	you can think of it as a cost function, right? You learn to ride a bike and you ride your bike
4975120	4984160	and at some point you fall. The real world does not give you a gradient of that cost function,
4984160	4990720	which is how much you hurt with respect to your actions. Okay? The only thing you can do is try
4990720	4996080	something else and see if you get the same result or not. Okay? So what do you do in that case? So
4996080	5001360	basically now your cost function is a black box. So now you cannot propagate gradient to this
5001360	5006640	black box. What you have to do is estimate the gradient by perturbing the, what you see to
5006640	5014080	that black box, right? So, you know, you try something, right? And that something would be a
5014080	5022240	perturbation of your input to this black box and you see what resulting perturbation occurs
5022240	5027760	on the black, on the output of the black box, the cost. And now you can estimate whether you,
5028480	5036400	you know, this modification improved or made the result worse, right? So essentially,
5036400	5041520	this is like this optimization problem I was telling you about earlier. The gradient based
5041520	5046560	algorithm is like you are in the mountain, lost in the mountain in a fog, you can't see anything.
5047360	5051920	But you can estimate the direction of steepest descent, right? You can just look around and you
5051920	5055680	can tell which is the direction of steepest descent. You just take a step in that direction.
5058080	5064160	What if you can't see, right? So basically to estimate in which direction the function goes
5064160	5070720	down, you have to actually take a step, okay? So you take a step in one direction, then you
5070720	5073920	come back, then you can take a step in the other direction, come back, and then maybe you get an
5073920	5078480	estimate for where the steepest descent is. Now you can take a step for steepest descent. So this
5078480	5084160	is estimating the gradient by perturbation instead of by analytic means of back propagating
5084160	5091200	gradients, okay, computing Jacobians or whatever, partial derivatives. And then there is the second
5091200	5096880	step of complexity. Let's imagine that the landscape you are in is basically flat everywhere,
5096880	5102400	except, you know, once in a while there is a step, okay? So taking a small step in one direction
5102400	5106480	will not give you any information about which direction you have to go to. So there you have
5106480	5112400	to use other techniques, taking bigger steps, you know, working for a while and seeing if you
5113280	5119120	fall down the step or not, or go up a step. You know, maybe you can multiply yourself in
5119120	5124160	sort of 10,000 copies of yourself and then kind of explore the surroundings. And then whenever
5124160	5130400	someone says, oh, I find a hole, calls everyone to kind of come there, okay? So all those methods
5130400	5136800	are called gradient-free optimization algorithms. Sometimes they're called zero-th order method.
5136800	5140320	Why zero-th order? Because first order is when you can compute the derivative.
5140320	5143680	Zero-th order is when you cannot compute the derivative. You can only compute the function
5143680	5147920	or get a value for the function. And then you have second order methods that compute not just
5147920	5152640	the first derivative, but also the second derivative. And they're also gradient-based,
5152640	5156640	okay, because they need the first derivative as well. But they can accelerate the process by
5156640	5162640	also computing the second derivative. And Adam is a very simplified form of kind of, you know,
5163920	5168720	second order method. It's not a second order method, but it has a hint of second order.
5168720	5171680	Another hint of second order method is what's called conjugate gradient.
5173120	5176480	It's another class of method called quasi-Newton methods, which are also kind of
5177520	5180160	using kind of curvature information, if you want, to kind of accelerate.
5181280	5186480	Many of those are not actually practical for neural net training, but there are some forms that are.
5188000	5193760	If you're interested in zero-th order optimization, there is a library that is actually produced
5194720	5199440	by, it's an open source library, which originated at Facebook Research in Paris
5200560	5204320	by an author called Olivier Tito, but it's really a community effort. There's a lot of
5204320	5209360	contributors to it. It's called Nevergrad. And it implements a very large number of different
5209360	5214080	optimization algorithms that do not assume that you have access to the gradient. Okay.
5215200	5218400	There are genetic algorithms or evolutionary methods. There are
5219360	5225040	particle swarm optimization. There are perturbation methods. There is all kinds of tricks, right?
5225040	5229360	I mean, there's a whole catalog of those things. And those sometimes it's unavoidable. You have
5229360	5234720	to use them because you don't know the cost function. So a very common situation where you
5234720	5240880	had to use those things is reinforcement learning. So reinforcement learning is basically a situation
5240880	5246400	where you tell the system, you don't tell the system the correct answer. You only tell the
5246400	5250960	system whether the answer was good or bad. It's because you give the value of the cost,
5250960	5254000	but you don't tell the machine where the cost is. So the machine doesn't know where the cost
5254000	5260400	function is. Okay. And so the machine cannot actually compute the gradient of the cost. And
5260400	5266480	so it has to use something like a zero-th order method. So what you can do is you can compute
5267200	5270720	a gradient with respect to the parameters of the overall cost function
5271520	5277840	by perturbing the parameters. Or what you can do is compute the gradient of the cost function
5277840	5283760	with respect to the output of your neural net. Okay. Using perturbation. And once you
5283760	5287120	have this estimate, then you back propagate the gradient through your network using regular
5287120	5292000	backprop. So that's a combination of estimating the gradient through perturbation for the cost
5292000	5296720	function because you don't know it, and then backpropagating from there. This is basically
5296720	5302160	the tactic that was used by the deep line people in sort of the first sort of deep
5303120	5310400	queue learning type methods. Back to the normalization. Do we normalize the entire dataset
5310400	5321200	or each batch? It's equivalent. So you normalize each sample, but the variable you're computing
5321200	5325680	is on the entire training set, right? So you're computing the standard deviation
5326480	5331280	and the mean over the entire training set. In fact, most of the time you don't even need to do it
5331280	5334880	over the entire training set because mean and standard deviation converges pretty fast.
5336000	5342320	So, but you do it over the entire training set, right? And what you get is a constant number,
5342320	5346320	two constant numbers, a number that you subtract and a number that you should divide
5346320	5353040	for each component of your input, okay? It's a fixed preprocessing. For a given training set,
5353040	5361040	you'll have a fixed mean and standard deviation vector.
5361040	5367760	But maybe we can connect to the other tool, right? The other module, the batch normalization, right?
5367760	5372080	Okay, we haven't talked about that yet. Yeah, I'm saying that we can perhaps extend
5372720	5378080	this normalization bit to the both sides, like the whole dataset and the batch itself.
5378080	5383280	Okay, yes, yes. So, I mean, again, there's going to be a whole lecture on this. But
5385840	5392080	for the same reason, it's good to have variables, the input that are zero mean and you need variants.
5392080	5396080	It's also good for the state variables inside the network to basically have zero mean and
5396080	5400480	you need variants. And so people have come up with various ways of doing normalization
5401360	5407440	of the variables inside the network so that they approach zero mean and you need variants.
5408960	5414560	But, and there are many ways to do this. They have two names like batch normalization, like
5417280	5420560	layer normalization. And the idea goes back a very long time.
5421600	5428080	Batch norm is kind of a more recent incarnation of it. Let's see, what was I scheduled to decrease
5428080	5434480	the learning rate? Yeah, as it turns out, for reasons that are still not completely fully understood,
5435360	5441600	you need to learn fast initially, you need a learning rate of a particular size.
5442720	5446000	But to get good results in the end, you kind of need to decrease the learning rate to kind of
5446000	5453040	let the system settle inside of minima. And that requires decreasing the learning rate.
5454000	5460480	There's various semi-valid theoretical explanations for this, but experimentally,
5460480	5464240	it's clear you need to do that. And again, there are schedules that are pre-programmed in PyTorch for
5464240	5472800	this. Use a bit of L1 or L2 regularization on the weights or combination. Yeah, after you've
5472800	5478160	trained your system for a few epochs, you might want to kind of prune it, eliminate the weights
5478160	5483840	that are useless, make sure that the weights have their minimum size. And what you do is you add a
5483840	5490640	term in the cost function that basically shrinks the weights at every iteration. You might know
5490640	5494720	what L2 and L1 regularization means if you've taken a class in machine learning for large
5494720	5500240	secret regression or stuff like that. It's very common. But L2 sometimes is called weight decay.
5500960	5509200	This, again, are pre-programmed in PyTorch. A trick that a lot of people use for large neural
5509200	5516560	nets is a trick called dropout. Dropout is implemented as kind of a layer in PyTorch. And
5516560	5524800	what this layer does is that it takes the state of a layer and it randomly picks a certain proportion
5524800	5530880	of the units and basically sets them to zero. So you can think of it as a mask,
5532160	5537120	a layer that applies a mask to an input. And the mask is randomly picked at every sample.
5538480	5545040	And some proportion of the value in the mask are set to zero. Some are set to one. And you
5545040	5550560	multiply the input by the mask. So only a subset of the units are allowed to speak to the next
5550560	5556080	layer, essentially. That's called dropout. And the reason for doing this is that it forces the
5556080	5563520	unit to distribute the information about the input over multiple units instead of kind of
5563520	5569680	squeezing everything into a small number. And it makes the system more robust. There's some
5569680	5575680	theoretical arguments for why it does that. Experimentally, if you add this to a large
5575680	5582160	network, you get better journalization error. You get better performance on the test set.
5582160	5588960	It's not always necessary, but it helps. Okay, there's lots of tricks and I'll devote a lecture
5588960	5594240	on this. So I'm not going to go through all of them right now. That requires explaining a bit
5594240	5599040	more about optimizations. So really, what deep learning is about, like, I told you everything
5599040	5603040	about deep learning, like the basics of deep learning. What I haven't told you is why we use
5603040	5608720	deep learning. Okay, and that's basically what I'm going to tell you about now. The motivation
5608720	5613360	for why is it that we need basically multi-layer neural nets or things of this type.
5615760	5622960	Okay, so the traditional prototypical model of supervised learning for a very long time
5622960	5630160	is basically a linear classifier. A linear classifier for a two-class problem is basically a
5630160	5634800	single unit of the similar type that we talked about earlier. You compute a weighted sum of inputs
5635360	5642000	at a bias, and you could think of the bias as just another trainable weight whose corresponding
5642000	5647440	input is equal to one, if you want. And then you pass that through a threshold function,
5647440	5652320	the sine function, that I put minus one if the weighted sum is below zero and plus one if it's
5652320	5659920	above zero. Okay, so this basic linear classifier basically partitions the space, the input space
5659920	5667120	of x's into two half spaces separated by hyperplane. Right, so the equation sum of i, w, i, x, i plus
5667120	5673120	b equals zero is the surface that separates the category one that is going to produce y bar equal
5673120	5680000	plus one from category two where y bar equals minus one. Why is it a, why does it divide the
5680000	5684960	space into two halves? It's because you're computing the dot product of an input vector with a weight
5684960	5692400	vector. If those two vectors are orthogonal, then the dot product is zero. Okay, b is just an offset.
5693440	5699520	So the set of points in x space where this dot product is zero is the set of points that are
5699520	5707760	orthogonal to the vector w. Okay, so in a n-dimensional space, your vector w is a vector,
5708480	5715040	and the set of x whose dot product with w is zero is a hyperplane. Right, so it's a linear
5715040	5721760	subspace of dimension n minus one. Okay, and that hyperplane divides the space of dimension n into
5721760	5728320	halves. So here is the situation in two dimensions. You have two dimensions x1, x2. You have data
5728320	5734240	points here, the red, the red category and the blue category. And there is a weight vector plus a bias
5734960	5740400	where the, you know, the intercept here of this sort of green separating line
5741120	5746800	with x1 is minus b times divided by w1. So that gives you an idea for what w should be.
5747520	5754720	And the w vector is orthogonal to that separating surface. Okay, so changing b will
5754720	5758320	change the position and then changing w will change the orientation basically.
5758880	5766800	Now, what about situations like this where the points are, the red and blue points are not
5766800	5774320	separable by a hyperplane? That's called a non-linearly separable case. So there you can't use a linear
5774320	5781600	classifier to separate those. What are we going to do? In fact, there is a theorem that goes back
5781600	5789040	to 1966 by Tom Kovar, who died recently actually. It was a Stanford that says the probability that
5789040	5796080	a particular separation of p points is linearly separable in n dimension is close to one when p
5796080	5802400	is smaller than n, but it's close to zero when p is larger than n. In other words, if you, if you
5802400	5807520	take an n-dimensional space, you throw p random points in that n-dimensional space, data points,
5807520	5816320	okay? And you randomly label them blue and red. You ask the question, what is the probability that
5816320	5821840	that particular dichotomy is linearly separable? I can separate the blue points from the red points
5821840	5827600	with a hyperplane. And the answer is, if p is less than n, you have a good chance that they
5827600	5832000	will be separable. If p is larger than n, you basically have no chance that they will. Okay,
5832000	5839200	so if you have an image classification problem, let's say, and you have tons of examples,
5840720	5845680	way bigger. So let's say you do n-nist. So n-nist is a dataset of handwritten digits.
5845680	5850320	The images are 28 by 28 pixels. In fact, the intrinsic dimension is smaller because some
5850320	5856960	pixels are always zero. And you have 60,000 samples. The probability that those 60,000
5856960	5862080	samples of, let's say, zeros from everything else or ones from everything else is nearly separable
5862880	5871440	is basically nil. So, which is why people invented the classical model of pattern
5871440	5878160	recognition. We consist in taking an input, engineering a feature extractor to produce a
5878160	5883280	representation in such a way that in that space now, your problem becomes, let's say, linearly
5883280	5887280	separable if you use a linear classifier or some other separability if you use another type of
5887280	5895280	classifier. Okay? Now, necessarily, this feature extraction must be nonlinear itself. If the only
5895280	5899600	thing it does is some affine transformation of the input, it's not going to make a nonlinearly
5899600	5906480	separable problem into a linear separable one, right? So, necessarily, this feature extractor
5906480	5911440	has to be nonlinear. This is very important to remember. Okay? A linear preprocessing doesn't
5911440	5916880	do anything for you, essentially. So, people spend decades in computer vision, for example,
5916880	5922240	as feature recognition, devising good feature extractors for particular problems. You know,
5922240	5926720	what features are good to do face recognition, for example, right? Can I do things like detect the
5926720	5930880	eyes and then measure the ratio between the separation of the eyes with the separation from
5930880	5935680	the mouth and then, you know, computes a few features like this and then feed that to a classifier
5935680	5942000	and figure out who the person is. So, most papers, you know, between, let's say, the 1960s
5942000	5950240	or 70s and the late 2000s or early 2010s in computer vision were essentially about that,
5950240	5956560	like how you represent images properly. Not all of them, okay? A lot of them for recognition.
5958160	5964640	And a lot of people kind of devise very sort of generic ways of devising feature extractors.
5965760	5971520	The basic idea is you just expand the dimension of the representation in a nonlinear way so that now
5971520	5975440	your number of dimensions is larger than the number of samples. And now your problem has
5975440	5980000	a chance of becoming linearly separable. So, the ideas that I'm not going to go through,
5980000	5984000	like space styling, random projection. So, random projection basically is a very simple idea.
5984000	5993520	You take your input vectors, you multiply them by random matrix, okay? And then you pass the result
5993520	5998880	through some nonlinear operation, okay? That's called random projection.
6000240	6005200	And it might make, if the dimension of the output is larger than the dimension of the input,
6005200	6010480	it might make a nonlinearly separable problem linearly separable. It's very efficient because,
6011440	6016080	you know, you might need a very large number of those, of this dimension to be able to kind of
6016080	6020800	do a good job. But it works in certain cases and you don't have to train the first layer,
6020800	6024880	you basically pick it randomly. And so, the only thing you need to train is a linear classifier
6024880	6029520	on top. It's polynomial classifiers, which I'll talk about in a minute, in a minute,
6029520	6036000	radio basis functions and kernel machines. So, those are basically techniques to turn
6036960	6045040	an input into a representation that then will be essentially classifiable by a simple classifier
6045040	6051680	like a linear classifier. So, what's a polynomial classifier? A polynomial classifier, basically,
6051680	6056320	imagine that your input vector has two dimensions. The way you increase the dimensionality of the
6056320	6062080	representation is that you take each of the input variables, but you also take every product of
6062080	6067360	pairs of input variables, right? So, now you have a new feature vector, which is composed of x1,
6067360	6073120	x2, you add one for the bias, and then also x1 times x2, x1 squared and x2 squared.
6073120	6079840	So, when you do a linear classification in that space, what you're doing really is a quadratic
6081120	6085760	classification in the original space, right? The surface, the separating surface in the
6085760	6094080	original space now is a quadratic curve into dimension. In n dimension, it's a quadratic
6094080	6102240	hypersurface, basically. So, it could be a parabola or ellipse or hyperbola, depending on the
6102240	6106320	coefficients, right? Now, the problem with this is that it doesn't work very well in high
6106320	6111040	dimension because the number of features grows with a square of the number of inputs. So, if you
6111040	6117520	want to apply this to get an ImageNet type image, you know, the resolution is 256 by 256 by 3,
6117520	6122720	because you have color channels. That's already a high dimension. If you take the cross product of
6122720	6127200	all of those variables, that's way too large, okay?
6128000	6135040	So, it's not really practical for high dimensional problems, but it's a trick. Now, here is,
6135920	6141760	so, super vector machines are basically two-layer networks of kernel machines more generally,
6142400	6149040	are two-layer systems in which the first layer has as many dimensions as you have training samples.
6150000	6154640	Okay? So, for each training sample, you create a inner-on, a unit, if you want,
6155520	6160960	and the role of this unit is to produce a large output if the input vector matches one of the
6160960	6167120	training samples and a small output if it doesn't, or the other way around. A small output if it
6167120	6171120	matches, a large output if it doesn't, okay? It doesn't really matter, but it has to be nonlinear.
6171760	6175600	So, something like, you know, compute the dot product of the input by one of the training
6175600	6181600	samples and passes through, you know, a negative exponential or a square or something like that.
6181920	6188000	So, this gives you how much the input vector resembles one of the training samples, and you
6188000	6194000	do this for every single training samples, okay? And then you train a linear classifier basically
6194000	6199200	to use those inputs as, you know, as input to a linear classifier. You compute the weight so
6199200	6204320	that linear classifier is basically as simple as that. There's some regularization involved, okay?
6204880	6210320	So, essentially, it's kind of a lookup table, right? You have your entire training set as
6211040	6216640	you know, points in your kind of nuance if you are, if you want for units in your first layer,
6216640	6222080	and they each indicate how close the current input vector is to them. So, you get some picture
6222080	6226640	of where the input vector is by basically having the relative position to all of the
6226640	6230640	training samples, and then using a simple linear operation, you can figure out, like, what's the
6230640	6235680	correct answer. This works really well for low dimensional problems, the small number of training
6236400	6242880	samples, but you're not going to do computer vision with it, at least not without, not if X is our pixels,
6244480	6245920	because it's basically template matching.
6248640	6255040	Now, here is a very interesting fact. It's a fact that if you build a two-layer neural net
6255040	6259920	on this model, okay? So, let's say a two-layer neural net, you have an input layer, a hidden layer,
6260880	6267440	and not specifying the size, and a single output unit, and you ask, what functions can I approximate
6267440	6273040	with an architecture of this type? The answer is, you can approximate pretty much any well-behaved
6273040	6278240	function as close as you want, as long as you have enough of those units in the middle, okay?
6278240	6284240	So, this is a theorem that says that two-layer neural nets are universal approximators. It
6284240	6289120	doesn't really matter what nonlinear function you put in the middle. Any nonlinear function will
6289120	6297040	do. A two-layer neural net is a universal approximator, and immediately you say, well,
6297040	6302160	why do we need multiple layers, then, if we can approximate anything with two layers? And the
6302160	6307200	answer is, it's very, very inefficient to try to approximate everything with only two layers,
6307200	6312400	because many, many, many interesting functions we're interested in learning cannot be efficiently
6312400	6317120	represented by a two-layer system. They can possibly be represented by a two-layer system,
6317120	6321360	but the number of fielding units it would require would be so ridiculously large that it's completely
6321360	6331120	impractical, okay? So, that's why we need layers. This very simple point is something that took
6331120	6337680	about, you know, it took until the, basically, the 2010s for the machine learning and
6337680	6344560	computer vision communities to understand, okay? If you understood what I just said,
6344560	6350080	you just took a few seconds, so you beat them. There is a last question here before we
6350080	6355760	finish class. So, does the depth of the network then have anything to do with generalization?
6357520	6363840	Okay, so generalization is a different story, okay? Generalization is very difficult to predict.
6363840	6368800	It depends on a lot of things. It depends on the appropriateness of the architecture to the
6368800	6373680	problem at hand, okay? So, for example, people use convolutional nets for computer vision,
6373680	6378400	they use transformers for text, you know, blah, blah, blah. So, there are certain architectures
6378400	6386240	that work well for certain types of data. So, that's the main thing that will improve generalization.
6389600	6393920	But generally, yes, multiple layers can improve generalization because
6394880	6400240	for a particular function you're interested in learning, computing it with multiple layers
6400240	6404000	will allow you to reduce the overall size of the system that will do a good job.
6404000	6408240	And so, by reducing the size, you're basically making it easier for the system to find kind of
6408240	6412160	good representation. But there is something else which has to do with compositionality.
6412160	6417280	I'll come to this in a minute if I have time. Also, the minimum, the, like the,
6417280	6422320	how do you call it, the well is like larger, right? If we have overparameterized networks.
6422320	6426400	If you're overparameterized network, it's much easier to find a minimum to your objective function,
6426400	6432480	right? Which is why neural nets are generally overparameterized. They generally have, like you,
6432480	6435200	a much larger number of parameters than what you would think is necessary.
6435840	6438960	And when you get them bigger, when you make them bigger, they work better usually.
6438960	6445120	It's not always the case, but it's very curious phenomenon about this. We'll talk about this later.
6445120	6451600	Okay, this is the one point I want to make. And it's the fact that the reason why we,
6451600	6457920	why layers are good is that the world is compositional, the perceptual world in particular,
6457920	6461600	but the world in general, the universe, if you want, is compositional. What does that mean? It
6461600	6467680	means that, okay, at the level of the universe, right? We have elementary particles, they assemble
6467680	6472720	to form less elementary particles, those assemble to form atoms, those assemble to form molecules,
6472720	6480080	those assemble to form materials, those assemble to form, you know, structures, objects, etc.
6480080	6486240	And, you know, environments, scenes, etc. You have the same kind of hierarchy for images,
6486240	6491920	you have pixels, they assemble to form edges and textons and motifs, parts and objects.
6492640	6496400	In text, you have characters that assemble to form words, word groups, clauses, sentences,
6496400	6503520	stories. In speech, you have speech samples, assemble to form, you know, kind of elementary
6503520	6510800	sounds, phones, phonemes, syllables, words, etc. So you have this kind of compositional hierarchy
6510800	6515360	in a lot of natural signals. And this is what makes the world understandable, right? This is
6515360	6519840	famous quote by Albert Einstein, the most incomprehensible thing about the world is that the
6519840	6524080	world is comprehensible. And the reason why the world is comprehensible is because it's compositional,
6524080	6528480	because small part assemble to form bigger part, and that allows you to have a description, an
6528480	6535760	abstract description of the world in terms of parts from the level immediately below,
6535760	6540480	in terms of level of abstraction. So to some extent, the layered architecture in a neural net
6541600	6548160	reflects this idea that you have kind of a compositional hierarchy where simple things
6548160	6553120	assemble to form slightly more complex things. So images, you have pixels formed to form edges
6553120	6557440	that are kind of depicted here. These are actually feature detectors, the visualization of feature
6557440	6562480	detectors by a particular convolutional net, which is a particular type of neural net, multilateral
6562480	6568160	neural net. So at the low level, you have units that detect oriented edges, a couple layers up,
6568160	6574000	you have things that detect simple motifs, circles, gratings, corners, etc. And then a few layers up,
6574000	6580560	there are things like parts of objects and things like that. So I think personally that the magic
6581120	6588240	of deep learning, the fact that multiple layers help is the fact that the perceptual world is
6588240	6593600	basically a compositional hierarchy. And then this end-to-end learning in deep learning allows the
6593600	6600720	system to learn hierarchical representations where each layer learns a representation that has a
6600720	6605200	level of abstraction slightly higher than the previous one. So low level, you have individual
6605200	6610080	pixels, then you have the presence or absence of an edge, then you have the presence or absence of
6610080	6614560	a part of an object, and then you have the presence or absence of an object independently of
6615840	6619920	the position of that object, the illumination, the color, the occlusions, the background,
6619920	6626640	you know, things like that, right? So that's the motivation, the idea why deep learning is so
6626640	6632880	successful and why it's basically taken over the world over the last 10 years or so. All right,
6632880	6637200	thank you for your attention. That's great. So for tomorrow guys, don't forget to try to go over
6637200	6647280	the 01 tutorial tensor, sorry, the 01 notebook that we have on the website such that we can get,
6647280	6651840	like, you know, all on the same level for the ones that are not really familiar with NumPy stuff,
6651840	6664480	okay? So otherwise, let's see you tomorrow morning and have a nice day. Take care everyone. Bye-bye.
