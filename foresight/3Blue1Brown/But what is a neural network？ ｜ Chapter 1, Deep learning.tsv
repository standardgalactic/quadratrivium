start	end	text
0	6040	This is a 3.
6040	11200	It's sloppily written and rendered at an extremely low resolution of 28 by 28 pixels,
11200	14360	but your brain has no trouble recognizing it as a 3.
14360	18520	And I want you to take a moment to appreciate how crazy it is that brains can do this so
18520	19520	effortlessly.
19520	25440	I mean, this, this, and this are also recognizable as 3s, even though the specific values of each
25440	28880	pixel is very different from one image to the next.
28880	34200	The particular light-sensitive cells in your eye that are firing when you see this 3 are
34200	37640	very different from the ones firing when you see this 3.
37640	42920	But something in that crazy smart visual cortex of yours resolves these as representing
42920	49320	the same idea, while at the same time recognizing other images as their own distinct ideas.
49320	54880	But if I told you, hey, sit down and write for me a program that takes in a grid of 28
54880	60760	by 28 pixels like this, and outputs a single number between 0 and 10, telling you what
60760	67360	it thinks the digit is, well, the task goes from comically trivial to dauntingly difficult.
67360	70640	Unless you've been living under a rock, I think I hardly need to motivate the relevance
70640	75200	and importance of machine learning and neural networks to the present and to the future.
75200	79280	But what I want to do here is show you what a neural network actually is, assuming no
79280	82560	background, and to help visualize what it's doing.
82560	85040	Not as a buzzword, but as a piece of math.
85040	89280	My hope is just that you come away feeling like the structure itself is motivated, and
89280	93440	to feel like you know what it means when you read or you hear about a neural network quote
93440	95480	unquote learning.
95480	99040	This video is just going to be devoted to the structure component of that, and the following
99040	101040	one is going to tackle learning.
101040	104760	What we're going to do is put together a neural network that can learn to recognize
104760	109760	handwritten digits.
109760	113720	This is a somewhat classic example for introducing the topic, and I'm happy to stick with the
113720	117400	status quo here, because at the end of the two videos, I want to point you to a couple
117400	121200	good resources where you can learn more, and where you can download the code that does
121200	125240	this and play with it on your own computer.
125240	129720	There are many, many variants of neural networks, and in recent years there's been sort of
129720	132640	a boom in research towards these variants.
132640	136800	But in these two introductory videos, you and I are just going to look at the simplest
136800	139840	plain vanilla form with no added frills.
139840	143880	This is kind of a necessary prerequisite for understanding any of the more powerful modern
143880	149220	variants, and trust me, it still has plenty of complexity for us to wrap our minds around.
149220	153440	But even in this simplest form, it can learn to recognize handwritten digits, which is
153440	157640	a pretty cool thing for a computer to be able to do.
157640	161360	And at the same time, you'll see how it does fall short of a couple hopes that we might
161360	163720	have for it.
163720	167560	As the name suggests, neural networks are inspired by the brain.
167560	169060	But let's break that down.
169060	172640	What are the neurons, and in what sense are they linked together?
172640	178320	Right now, when I say neuron, all I want you to think about is a thing that holds a number,
178320	180960	specifically a number between 0 and 1.
180960	183960	It's really not more than that.
183960	188880	For example, the network starts with a bunch of neurons corresponding to each of the 28
188880	194760	times 28 pixels of the input image, which is 784 neurons in total.
194760	201080	Each one of these holds a number that represents the grayscale value of the corresponding pixel,
201080	205440	ranging from 0 for black pixels up to 1 for white pixels.
205440	210000	This number inside the neuron is called its activation, and the image you might have in
210000	216760	mind here is that each neuron is lit up when its activation is a high number.
216760	226480	So all of these 784 neurons make up the first layer of our network.
226480	232080	Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.
232080	237640	The activation in these neurons, again some number that's between 0 and 1, represents
237640	242560	how much the system thinks that a given image corresponds with a given digit.
243080	247320	There's also a couple layers in between, called the hidden layers, which for the time
247320	252160	being should just be a giant question mark for how on earth this process of recognizing
252160	254320	digits is going to be handled.
254320	259320	In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's
259320	261080	kind of an arbitrary choice.
261080	264400	To be honest, I chose two layers based on how I want to motivate this structure in just
264400	268800	a moment, and 16, well that was just a nice number to fit on the screen.
268800	273240	In practice there is a lot of room for experiment with a specific structure here.
273240	277880	The way the network operates, activations in one layer determine the activations of the
277880	278880	next layer.
278880	283520	And of course the heart of the network, as an information processing mechanism, comes
283520	288240	down to exactly how those activations from one layer bring about activations in the next
288240	289240	layer.
289240	294280	It's meant to be loosely analogous to how in biological networks of neurons, some groups
294280	298080	of neurons firing cause certain others to fire.
298080	302200	Now the network I'm showing here has already been trained to recognize digits, and let me
302200	303720	show you what I mean by that.
303720	310080	It means if you feed in an image, lighting up all 784 neurons of the input layer according
310080	315520	to the brightness of each pixel in the image, that pattern of activations causes some very
315520	319960	specific pattern in the next layer, which causes some pattern in the one after it, which
319960	322760	finally gives some pattern in the output layer.
322760	327760	And the brightest neuron of that output layer is the network's choice, so to speak, for
327760	332600	what digit this image represents.
332600	336840	And before jumping into the math for how one layer influences the next, or how training
336840	341480	works, let's just talk about why it's even reasonable to expect a layered structure like
341480	344280	this to behave intelligently.
344280	345520	What are we expecting here?
345520	348960	What is the best hope for what those middle layers might be doing?
349320	354160	Well, when you or I recognize digits, we piece together various components.
354160	357480	A 9 has a loop up top and a line on the right.
357480	362000	An 8 also has a loop up top, but it's paired with another loop down low.
362000	367040	A 4 basically breaks down into three specific lines, and things like that.
367040	373280	Now in a perfect world, we might hope that each neuron in the second to last layer corresponds
373280	378200	with one of these subcomponents, that anytime you feed in an image with, say, a loop up
378240	383280	top, like a 9 or an 8, there's some specific neuron whose activation is going to be close
383280	384280	to one.
384280	389000	And I don't mean this specific loop of pixels, the hope would be that any generally loopy
389000	392520	pattern towards the top sets off this neuron.
392520	397580	That way, going from the third layer to the last one, just requires learning which combination
397580	401040	of subcomponents corresponds to which digits.
401040	404440	Of course, that just kicks the problem down the road, because how would you recognize
404440	408320	these subcomponents, or even learn what the right subcomponents should be, and I still
408320	412280	haven't even talked about how one layer influences the next, but run with me on this
412280	414080	one for a moment.
414080	417400	Recognizing a loop can also break down into subproblems.
417400	422040	One reasonable way to do this would be to first recognize the various little edges that
422040	423320	make it up.
423320	428920	Similarly, a long line, like the kind you might see in the digits 1, or 4, or 7, well
428920	433420	that's really just a long edge, or maybe you think of it as a certain pattern of several
433420	435240	smaller edges.
435240	440780	So maybe, our hope is that each neuron in the second layer of the network corresponds
440780	443820	with the various relevant little edges.
443820	449220	Maybe when an image like this one comes in, it lights up all of the neurons associated
449220	454560	with around 8 to 10 specific little edges, which in turn lights up the neurons associated
454560	459300	with the upper loop and a long vertical line, and those light up the neuron associated with
459300	460860	a 9.
460860	464940	Whether or not this is what our final network actually does is another question, one that
464940	467860	I'll come back to once we see how to train the network.
467860	472940	But this is a hope that we might have, a sort of goal with the layered structure like this.
472940	477500	Moreover, you can imagine how being able to detect edges and patterns like this would
477500	481020	be really useful for other image recognition tasks.
481020	484420	And even beyond image recognition, there are all sorts of intelligent things you might
484420	487940	want to do that break down into layers of abstraction.
487940	493180	Parsing speech, for example, involves taking raw audio and picking out distinct sounds,
493180	497740	which combine to make certain syllables, which combine to form words, which combine to make
497740	501260	up phrases and more abstract thoughts, etc.
501260	505980	But getting back to how any of this actually works, picture yourself right now designing
505980	511140	how exactly the activations in one layer might determine the activations in the next.
511140	516540	The goal is to have some mechanism that could conceivably combine pixels into edges, or
516540	519460	edges into patterns, or patterns into digits.
519460	524460	And to zoom in on one very specific example, let's say the hope is for one particular
524460	530100	neuron in the second layer to pick up on whether or not the image has an edge in this region
530100	531460	here.
531460	535780	The question at hand is what parameters should the network have?
535780	540940	What dials and knobs should you be able to tweak so that it's expressive enough to potentially
540940	546140	capture this pattern, or any other pixel pattern, or the pattern that several edges can make
546140	548780	a loop and other such things?
548780	553860	Well what we'll do is assign a weight to each one of the connections between our neuron
553860	556340	and the neurons from the first layer.
556340	558740	These weights are just numbers.
558740	564300	Then take all of those activations from the first layer and compute their weighted sum
564300	567540	according to these weights.
567540	571460	I find it helpful to think of these weights as being organized into a little grid of their
571460	575940	own, and I'm going to use green pixels to indicate positive weights, and red pixels
575940	581220	to indicate negative weights, where the brightness of that pixel is some loose depiction of the
581220	582220	weight's value.
582220	586780	Now if we made the weights associated with almost all of the pixels zero, except for
586780	591500	some positive weights in this region that we care about, then taking the weighted sum
591500	596100	of all the pixel values really just amounts to adding up the values of the pixel just
596100	599220	in the region that we care about.
599220	603340	And if you really wanted to pick up on whether there's an edge here, what you might do is
603340	607580	have some negative weights associated with the surrounding pixels.
607580	611900	Then the sum is largest when those middle pixels are bright, but the surrounding pixels
611900	614780	are darker.
614780	619020	When you compute a weighted sum like this, you might come out with any number, but for
619020	624220	this network what we want is for activations to be some value between zero and one.
624220	628900	So a common thing to do is to pump this weighted sum into some function that squishes the
628900	633860	real number line into the range between zero and one, and a common function that does this
633860	638420	is called the sigmoid function, also known as a logistic curve.
638420	642980	Basically very negative inputs end up close to zero, very positive inputs end up close
642980	649540	to one, and it just steadily increases around the input zero.
649540	655460	So the activation of the neuron here is basically a measure of how positive the relevant weighted
655460	657940	sum is.
657940	661300	But maybe it's not that you want the neuron to light up when the weighted sum is bigger
661300	662620	than zero.
662620	667140	Maybe you only want it to be active when the sum is bigger than say ten.
667140	671340	That is, you want some bias for it to be inactive.
671340	675820	What we'll do then is just add in some other number, like negative ten, to this weighted
675820	680740	sum before plugging it through the sigmoid squishification function.
680740	683500	That additional number is called the bias.
683500	687740	So the weights tell you what pixel pattern this neuron in the second layer is picking
687740	693420	up on, and the bias tells you how high the weighted sum needs to be before the neuron
693420	696300	starts getting meaningfully active.
696300	698620	And that is just one neuron.
698620	704300	Every other neuron in this layer is going to be connected to all 784 pixel neurons from
704300	710300	the first layer, and each one of those 784 connections has its own weight associated
710300	711820	with it.
711820	716220	Also each one has some bias, some other number that you add on to the weighted sum before
716220	718420	squishing it with the sigmoid.
718420	720140	And that's a lot to think about.
720140	727060	With this hidden layer of 16 neurons, that's a total of 784 times 16 weights along with
727060	728940	16 biases.
728940	732580	And all of that is just the connections from the first layer to the second.
732580	736900	The connections between the other layers also have a bunch of weights and biases associated
736900	738460	with them.
738460	745260	All said and done, this network has almost exactly 13,000 total weights and biases, 13,000
745260	751060	knobs and dials that can be tweaked and turned to make this network behave in different ways.
751060	756100	So when we talk about learning, what that's referring to is getting the computer to find
756100	760380	a valid setting for all of these many, many numbers so that it'll actually solve the
760380	762740	problem at hand.
762740	767140	One thought experiment that is at once fun and kind of horrifying is to imagine sitting
767140	771420	down and setting all of these weights and biases by hand, purposefully tweaking the
771420	776220	numbers so that the second layer picks up on edges, the third layer picks up on patterns,
776220	777220	etc.
777220	781180	I personally find this satisfying rather than just treating the network as a total black
781180	786100	box because when the network doesn't perform the way you anticipate, if you've built
786100	790420	up a little bit of a relationship with what those weights and biases actually mean, you
790420	795100	have a starting place for experimenting with how to change the structure to improve.
795100	799020	Or when the network does work, but not for the reasons you might expect, digging into
799020	803060	what the weights and biases are doing is a good way to challenge your assumptions and
803060	806820	really expose the full space of possible solutions.
806820	810420	By the way, the actual function here is a little cumbersome to write down, don't you
810420	812860	think?
812860	817820	So let me show you a more notationally compact way that these connections are represented.
817820	821660	This is how you'd see it if you choose to read up more about neural networks.
821660	828020	Organize all of the activations from one layer into a column as a vector.
828020	833380	Then organize all of the weights as a matrix, where each row of that matrix corresponds
833380	838580	to the connections between one layer and a particular neuron in the next layer.
838580	842740	What that means is that taking the weighted sum of the activations in the first layer
842740	847460	according to these weights corresponds to one of the terms in the matrix vector product
847460	854020	of everything we have on the left here.
854020	857860	By the way, so much of machine learning just comes down to having a good grasp of linear
857860	862380	algebra, so for any of you who want a nice visual understanding for matrices and what
862380	867380	matrix vector multiplication means, take a look at the series I did on linear algebra,
867380	869380	especially Chapter 3.
869380	873100	Back to our expression, instead of talking about adding the bias to each one of these
873100	879140	values independently, we represent it by organizing all those biases into a vector and adding
879140	883460	the entire vector to the previous matrix vector product.
883460	888900	And as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed
888900	893060	to represent is that you're going to apply the sigmoid function to each specific component
893060	895980	of the resulting vector inside.
895980	900740	So once you write down this weight matrix and these vectors as their own symbols, you
900740	905940	can communicate the full transition of activations from one layer to the next in an extremely
905940	908380	tight and neat little expression.
908380	913420	And this makes the relevant code both a lot simpler and a lot faster, since many libraries
913420	917860	optimize the heck out of matrix multiplication.
917860	921820	Remember how earlier I said these neurons are simply things that hold numbers?
921820	928340	Well, of course, the specific numbers that they hold depends on the image you feed in.
928340	932660	So it's actually more accurate to think of each neuron as a function, one that takes
932660	938460	in the outputs of all the neurons in the previous layer and spits out a number between 0 and
938460	939460	1.
939460	944940	See, the entire network is just a function, one that takes in 784 numbers as an input
944940	947540	and spits out 10 numbers as an output.
947540	952580	It's an absurdly complicated function, one that involves 13,000 parameters in the forms
952580	956740	of these weights and biases that pick up on certain patterns, and which involves iterating
956740	961140	many matrix vector products and the sigmoid squishification function.
961140	963820	But it's just a function nonetheless.
963820	966740	And in a way, it's kind of reassuring that it looks complicated.
967220	971080	I mean, if it were any simpler, what hope would we have that it could take on the challenge
971080	972580	of recognizing digits?
972580	975140	And how does it take on that challenge?
975140	979740	How does this network learn the appropriate weights and biases just by looking at data?
979740	983540	Well, that's what I'll show in the next video, and I'll also dig a little more into
983540	987620	what this particular network we're seeing is really doing.
987620	991540	Now is the point I suppose I should say subscribe to stay notified about when that video or
991540	996340	any new videos come out, but realistically, most of you don't actually receive notifications
996340	998180	from YouTube, do you?
998180	1001780	Maybe more honestly, I should say subscribe so that the neural networks that underlie
1001780	1006260	YouTube's recommendation algorithm are primed to believe that you want to see content from
1006260	1008260	this channel get recommended to you.
1008260	1010900	Anyway, stay posted for more.
1010900	1013660	Thank you very much to everyone supporting these videos on Patreon.
1013660	1017780	I've been a little slow to progress in the probability series this summer, but I'm jumping
1017780	1023780	back into it after this project, so patrons, you can look out for updates there.
1023780	1027940	To close things off here, I have with me Lisha Lee, who did her PhD work on the theoretical
1027940	1032340	side of deep learning, and who currently works at a venture capital firm called Amplify Partners,
1032340	1035420	who kindly provided some of the funding for this video.
1035420	1039740	So Lisha, one thing I think we should quickly bring up is this sigmoid function.
1039740	1043500	As I understand it, early networks used this to squish the relevant weighted sum into that
1043500	1047860	interval between zero and one, you know, kind of motivated by this biological analogy of
1047860	1050660	neurons either being inactive or active.
1050820	1055620	Relatively few modern networks actually use sigmoid anymore, it's kind of old school, right?
1055620	1059260	Yeah, or rather, relu seems to be much easier to train.
1059260	1062540	And relu stands for rectified linear unit?
1062540	1068380	Yes, it's this kind of function where you're just taking a max of zero and a, where a is
1068380	1071140	given by what you were explaining in the video.
1071140	1077100	And what this was sort of motivated from, I think, was partially by a biological analogy
1077220	1081380	with how neurons would either be activated or not.
1081380	1085620	And so if it passes a certain threshold, it would be the identity function.
1085620	1089500	But if it did not, then it would just not be activated, so be zero.
1089500	1091020	So it's kind of a simplification.
1091020	1095620	Using sigmoids didn't help training, or it was very difficult to train at some point
1095620	1097820	and people just tried relu.
1097820	1104820	And it happened to work very well for these incredibly deep neural networks.
1104820	1105940	All right, thank you, Lisha.
