WEBVTT

00:00.000 --> 00:06.040
This is a 3.

00:06.040 --> 00:11.200
It's sloppily written and rendered at an extremely low resolution of 28 by 28 pixels,

00:11.200 --> 00:14.360
but your brain has no trouble recognizing it as a 3.

00:14.360 --> 00:18.520
And I want you to take a moment to appreciate how crazy it is that brains can do this so

00:18.520 --> 00:19.520
effortlessly.

00:19.520 --> 00:25.440
I mean, this, this, and this are also recognizable as 3s, even though the specific values of each

00:25.440 --> 00:28.880
pixel is very different from one image to the next.

00:28.880 --> 00:34.200
The particular light-sensitive cells in your eye that are firing when you see this 3 are

00:34.200 --> 00:37.640
very different from the ones firing when you see this 3.

00:37.640 --> 00:42.920
But something in that crazy smart visual cortex of yours resolves these as representing

00:42.920 --> 00:49.320
the same idea, while at the same time recognizing other images as their own distinct ideas.

00:49.320 --> 00:54.880
But if I told you, hey, sit down and write for me a program that takes in a grid of 28

00:54.880 --> 01:00.760
by 28 pixels like this, and outputs a single number between 0 and 10, telling you what

01:00.760 --> 01:07.360
it thinks the digit is, well, the task goes from comically trivial to dauntingly difficult.

01:07.360 --> 01:10.640
Unless you've been living under a rock, I think I hardly need to motivate the relevance

01:10.640 --> 01:15.200
and importance of machine learning and neural networks to the present and to the future.

01:15.200 --> 01:19.280
But what I want to do here is show you what a neural network actually is, assuming no

01:19.280 --> 01:22.560
background, and to help visualize what it's doing.

01:22.560 --> 01:25.040
Not as a buzzword, but as a piece of math.

01:25.040 --> 01:29.280
My hope is just that you come away feeling like the structure itself is motivated, and

01:29.280 --> 01:33.440
to feel like you know what it means when you read or you hear about a neural network quote

01:33.440 --> 01:35.480
unquote learning.

01:35.480 --> 01:39.040
This video is just going to be devoted to the structure component of that, and the following

01:39.040 --> 01:41.040
one is going to tackle learning.

01:41.040 --> 01:44.760
What we're going to do is put together a neural network that can learn to recognize

01:44.760 --> 01:49.760
handwritten digits.

01:49.760 --> 01:53.720
This is a somewhat classic example for introducing the topic, and I'm happy to stick with the

01:53.720 --> 01:57.400
status quo here, because at the end of the two videos, I want to point you to a couple

01:57.400 --> 02:01.200
good resources where you can learn more, and where you can download the code that does

02:01.200 --> 02:05.240
this and play with it on your own computer.

02:05.240 --> 02:09.720
There are many, many variants of neural networks, and in recent years there's been sort of

02:09.720 --> 02:12.640
a boom in research towards these variants.

02:12.640 --> 02:16.800
But in these two introductory videos, you and I are just going to look at the simplest

02:16.800 --> 02:19.840
plain vanilla form with no added frills.

02:19.840 --> 02:23.880
This is kind of a necessary prerequisite for understanding any of the more powerful modern

02:23.880 --> 02:29.220
variants, and trust me, it still has plenty of complexity for us to wrap our minds around.

02:29.220 --> 02:33.440
But even in this simplest form, it can learn to recognize handwritten digits, which is

02:33.440 --> 02:37.640
a pretty cool thing for a computer to be able to do.

02:37.640 --> 02:41.360
And at the same time, you'll see how it does fall short of a couple hopes that we might

02:41.360 --> 02:43.720
have for it.

02:43.720 --> 02:47.560
As the name suggests, neural networks are inspired by the brain.

02:47.560 --> 02:49.060
But let's break that down.

02:49.060 --> 02:52.640
What are the neurons, and in what sense are they linked together?

02:52.640 --> 02:58.320
Right now, when I say neuron, all I want you to think about is a thing that holds a number,

02:58.320 --> 03:00.960
specifically a number between 0 and 1.

03:00.960 --> 03:03.960
It's really not more than that.

03:03.960 --> 03:08.880
For example, the network starts with a bunch of neurons corresponding to each of the 28

03:08.880 --> 03:14.760
times 28 pixels of the input image, which is 784 neurons in total.

03:14.760 --> 03:21.080
Each one of these holds a number that represents the grayscale value of the corresponding pixel,

03:21.080 --> 03:25.440
ranging from 0 for black pixels up to 1 for white pixels.

03:25.440 --> 03:30.000
This number inside the neuron is called its activation, and the image you might have in

03:30.000 --> 03:36.760
mind here is that each neuron is lit up when its activation is a high number.

03:36.760 --> 03:46.480
So all of these 784 neurons make up the first layer of our network.

03:46.480 --> 03:52.080
Now jumping over to the last layer, this has 10 neurons, each representing one of the digits.

03:52.080 --> 03:57.640
The activation in these neurons, again some number that's between 0 and 1, represents

03:57.640 --> 04:02.560
how much the system thinks that a given image corresponds with a given digit.

04:03.080 --> 04:07.320
There's also a couple layers in between, called the hidden layers, which for the time

04:07.320 --> 04:12.160
being should just be a giant question mark for how on earth this process of recognizing

04:12.160 --> 04:14.320
digits is going to be handled.

04:14.320 --> 04:19.320
In this network I chose two hidden layers, each one with 16 neurons, and admittedly that's

04:19.320 --> 04:21.080
kind of an arbitrary choice.

04:21.080 --> 04:24.400
To be honest, I chose two layers based on how I want to motivate this structure in just

04:24.400 --> 04:28.800
a moment, and 16, well that was just a nice number to fit on the screen.

04:28.800 --> 04:33.240
In practice there is a lot of room for experiment with a specific structure here.

04:33.240 --> 04:37.880
The way the network operates, activations in one layer determine the activations of the

04:37.880 --> 04:38.880
next layer.

04:38.880 --> 04:43.520
And of course the heart of the network, as an information processing mechanism, comes

04:43.520 --> 04:48.240
down to exactly how those activations from one layer bring about activations in the next

04:48.240 --> 04:49.240
layer.

04:49.240 --> 04:54.280
It's meant to be loosely analogous to how in biological networks of neurons, some groups

04:54.280 --> 04:58.080
of neurons firing cause certain others to fire.

04:58.080 --> 05:02.200
Now the network I'm showing here has already been trained to recognize digits, and let me

05:02.200 --> 05:03.720
show you what I mean by that.

05:03.720 --> 05:10.080
It means if you feed in an image, lighting up all 784 neurons of the input layer according

05:10.080 --> 05:15.520
to the brightness of each pixel in the image, that pattern of activations causes some very

05:15.520 --> 05:19.960
specific pattern in the next layer, which causes some pattern in the one after it, which

05:19.960 --> 05:22.760
finally gives some pattern in the output layer.

05:22.760 --> 05:27.760
And the brightest neuron of that output layer is the network's choice, so to speak, for

05:27.760 --> 05:32.600
what digit this image represents.

05:32.600 --> 05:36.840
And before jumping into the math for how one layer influences the next, or how training

05:36.840 --> 05:41.480
works, let's just talk about why it's even reasonable to expect a layered structure like

05:41.480 --> 05:44.280
this to behave intelligently.

05:44.280 --> 05:45.520
What are we expecting here?

05:45.520 --> 05:48.960
What is the best hope for what those middle layers might be doing?

05:49.320 --> 05:54.160
Well, when you or I recognize digits, we piece together various components.

05:54.160 --> 05:57.480
A 9 has a loop up top and a line on the right.

05:57.480 --> 06:02.000
An 8 also has a loop up top, but it's paired with another loop down low.

06:02.000 --> 06:07.040
A 4 basically breaks down into three specific lines, and things like that.

06:07.040 --> 06:13.280
Now in a perfect world, we might hope that each neuron in the second to last layer corresponds

06:13.280 --> 06:18.200
with one of these subcomponents, that anytime you feed in an image with, say, a loop up

06:18.240 --> 06:23.280
top, like a 9 or an 8, there's some specific neuron whose activation is going to be close

06:23.280 --> 06:24.280
to one.

06:24.280 --> 06:29.000
And I don't mean this specific loop of pixels, the hope would be that any generally loopy

06:29.000 --> 06:32.520
pattern towards the top sets off this neuron.

06:32.520 --> 06:37.580
That way, going from the third layer to the last one, just requires learning which combination

06:37.580 --> 06:41.040
of subcomponents corresponds to which digits.

06:41.040 --> 06:44.440
Of course, that just kicks the problem down the road, because how would you recognize

06:44.440 --> 06:48.320
these subcomponents, or even learn what the right subcomponents should be, and I still

06:48.320 --> 06:52.280
haven't even talked about how one layer influences the next, but run with me on this

06:52.280 --> 06:54.080
one for a moment.

06:54.080 --> 06:57.400
Recognizing a loop can also break down into subproblems.

06:57.400 --> 07:02.040
One reasonable way to do this would be to first recognize the various little edges that

07:02.040 --> 07:03.320
make it up.

07:03.320 --> 07:08.920
Similarly, a long line, like the kind you might see in the digits 1, or 4, or 7, well

07:08.920 --> 07:13.420
that's really just a long edge, or maybe you think of it as a certain pattern of several

07:13.420 --> 07:15.240
smaller edges.

07:15.240 --> 07:20.780
So maybe, our hope is that each neuron in the second layer of the network corresponds

07:20.780 --> 07:23.820
with the various relevant little edges.

07:23.820 --> 07:29.220
Maybe when an image like this one comes in, it lights up all of the neurons associated

07:29.220 --> 07:34.560
with around 8 to 10 specific little edges, which in turn lights up the neurons associated

07:34.560 --> 07:39.300
with the upper loop and a long vertical line, and those light up the neuron associated with

07:39.300 --> 07:40.860
a 9.

07:40.860 --> 07:44.940
Whether or not this is what our final network actually does is another question, one that

07:44.940 --> 07:47.860
I'll come back to once we see how to train the network.

07:47.860 --> 07:52.940
But this is a hope that we might have, a sort of goal with the layered structure like this.

07:52.940 --> 07:57.500
Moreover, you can imagine how being able to detect edges and patterns like this would

07:57.500 --> 08:01.020
be really useful for other image recognition tasks.

08:01.020 --> 08:04.420
And even beyond image recognition, there are all sorts of intelligent things you might

08:04.420 --> 08:07.940
want to do that break down into layers of abstraction.

08:07.940 --> 08:13.180
Parsing speech, for example, involves taking raw audio and picking out distinct sounds,

08:13.180 --> 08:17.740
which combine to make certain syllables, which combine to form words, which combine to make

08:17.740 --> 08:21.260
up phrases and more abstract thoughts, etc.

08:21.260 --> 08:25.980
But getting back to how any of this actually works, picture yourself right now designing

08:25.980 --> 08:31.140
how exactly the activations in one layer might determine the activations in the next.

08:31.140 --> 08:36.540
The goal is to have some mechanism that could conceivably combine pixels into edges, or

08:36.540 --> 08:39.460
edges into patterns, or patterns into digits.

08:39.460 --> 08:44.460
And to zoom in on one very specific example, let's say the hope is for one particular

08:44.460 --> 08:50.100
neuron in the second layer to pick up on whether or not the image has an edge in this region

08:50.100 --> 08:51.460
here.

08:51.460 --> 08:55.780
The question at hand is what parameters should the network have?

08:55.780 --> 09:00.940
What dials and knobs should you be able to tweak so that it's expressive enough to potentially

09:00.940 --> 09:06.140
capture this pattern, or any other pixel pattern, or the pattern that several edges can make

09:06.140 --> 09:08.780
a loop and other such things?

09:08.780 --> 09:13.860
Well what we'll do is assign a weight to each one of the connections between our neuron

09:13.860 --> 09:16.340
and the neurons from the first layer.

09:16.340 --> 09:18.740
These weights are just numbers.

09:18.740 --> 09:24.300
Then take all of those activations from the first layer and compute their weighted sum

09:24.300 --> 09:27.540
according to these weights.

09:27.540 --> 09:31.460
I find it helpful to think of these weights as being organized into a little grid of their

09:31.460 --> 09:35.940
own, and I'm going to use green pixels to indicate positive weights, and red pixels

09:35.940 --> 09:41.220
to indicate negative weights, where the brightness of that pixel is some loose depiction of the

09:41.220 --> 09:42.220
weight's value.

09:42.220 --> 09:46.780
Now if we made the weights associated with almost all of the pixels zero, except for

09:46.780 --> 09:51.500
some positive weights in this region that we care about, then taking the weighted sum

09:51.500 --> 09:56.100
of all the pixel values really just amounts to adding up the values of the pixel just

09:56.100 --> 09:59.220
in the region that we care about.

09:59.220 --> 10:03.340
And if you really wanted to pick up on whether there's an edge here, what you might do is

10:03.340 --> 10:07.580
have some negative weights associated with the surrounding pixels.

10:07.580 --> 10:11.900
Then the sum is largest when those middle pixels are bright, but the surrounding pixels

10:11.900 --> 10:14.780
are darker.

10:14.780 --> 10:19.020
When you compute a weighted sum like this, you might come out with any number, but for

10:19.020 --> 10:24.220
this network what we want is for activations to be some value between zero and one.

10:24.220 --> 10:28.900
So a common thing to do is to pump this weighted sum into some function that squishes the

10:28.900 --> 10:33.860
real number line into the range between zero and one, and a common function that does this

10:33.860 --> 10:38.420
is called the sigmoid function, also known as a logistic curve.

10:38.420 --> 10:42.980
Basically very negative inputs end up close to zero, very positive inputs end up close

10:42.980 --> 10:49.540
to one, and it just steadily increases around the input zero.

10:49.540 --> 10:55.460
So the activation of the neuron here is basically a measure of how positive the relevant weighted

10:55.460 --> 10:57.940
sum is.

10:57.940 --> 11:01.300
But maybe it's not that you want the neuron to light up when the weighted sum is bigger

11:01.300 --> 11:02.620
than zero.

11:02.620 --> 11:07.140
Maybe you only want it to be active when the sum is bigger than say ten.

11:07.140 --> 11:11.340
That is, you want some bias for it to be inactive.

11:11.340 --> 11:15.820
What we'll do then is just add in some other number, like negative ten, to this weighted

11:15.820 --> 11:20.740
sum before plugging it through the sigmoid squishification function.

11:20.740 --> 11:23.500
That additional number is called the bias.

11:23.500 --> 11:27.740
So the weights tell you what pixel pattern this neuron in the second layer is picking

11:27.740 --> 11:33.420
up on, and the bias tells you how high the weighted sum needs to be before the neuron

11:33.420 --> 11:36.300
starts getting meaningfully active.

11:36.300 --> 11:38.620
And that is just one neuron.

11:38.620 --> 11:44.300
Every other neuron in this layer is going to be connected to all 784 pixel neurons from

11:44.300 --> 11:50.300
the first layer, and each one of those 784 connections has its own weight associated

11:50.300 --> 11:51.820
with it.

11:51.820 --> 11:56.220
Also each one has some bias, some other number that you add on to the weighted sum before

11:56.220 --> 11:58.420
squishing it with the sigmoid.

11:58.420 --> 12:00.140
And that's a lot to think about.

12:00.140 --> 12:07.060
With this hidden layer of 16 neurons, that's a total of 784 times 16 weights along with

12:07.060 --> 12:08.940
16 biases.

12:08.940 --> 12:12.580
And all of that is just the connections from the first layer to the second.

12:12.580 --> 12:16.900
The connections between the other layers also have a bunch of weights and biases associated

12:16.900 --> 12:18.460
with them.

12:18.460 --> 12:25.260
All said and done, this network has almost exactly 13,000 total weights and biases, 13,000

12:25.260 --> 12:31.060
knobs and dials that can be tweaked and turned to make this network behave in different ways.

12:31.060 --> 12:36.100
So when we talk about learning, what that's referring to is getting the computer to find

12:36.100 --> 12:40.380
a valid setting for all of these many, many numbers so that it'll actually solve the

12:40.380 --> 12:42.740
problem at hand.

12:42.740 --> 12:47.140
One thought experiment that is at once fun and kind of horrifying is to imagine sitting

12:47.140 --> 12:51.420
down and setting all of these weights and biases by hand, purposefully tweaking the

12:51.420 --> 12:56.220
numbers so that the second layer picks up on edges, the third layer picks up on patterns,

12:56.220 --> 12:57.220
etc.

12:57.220 --> 13:01.180
I personally find this satisfying rather than just treating the network as a total black

13:01.180 --> 13:06.100
box because when the network doesn't perform the way you anticipate, if you've built

13:06.100 --> 13:10.420
up a little bit of a relationship with what those weights and biases actually mean, you

13:10.420 --> 13:15.100
have a starting place for experimenting with how to change the structure to improve.

13:15.100 --> 13:19.020
Or when the network does work, but not for the reasons you might expect, digging into

13:19.020 --> 13:23.060
what the weights and biases are doing is a good way to challenge your assumptions and

13:23.060 --> 13:26.820
really expose the full space of possible solutions.

13:26.820 --> 13:30.420
By the way, the actual function here is a little cumbersome to write down, don't you

13:30.420 --> 13:32.860
think?

13:32.860 --> 13:37.820
So let me show you a more notationally compact way that these connections are represented.

13:37.820 --> 13:41.660
This is how you'd see it if you choose to read up more about neural networks.

13:41.660 --> 13:48.020
Organize all of the activations from one layer into a column as a vector.

13:48.020 --> 13:53.380
Then organize all of the weights as a matrix, where each row of that matrix corresponds

13:53.380 --> 13:58.580
to the connections between one layer and a particular neuron in the next layer.

13:58.580 --> 14:02.740
What that means is that taking the weighted sum of the activations in the first layer

14:02.740 --> 14:07.460
according to these weights corresponds to one of the terms in the matrix vector product

14:07.460 --> 14:14.020
of everything we have on the left here.

14:14.020 --> 14:17.860
By the way, so much of machine learning just comes down to having a good grasp of linear

14:17.860 --> 14:22.380
algebra, so for any of you who want a nice visual understanding for matrices and what

14:22.380 --> 14:27.380
matrix vector multiplication means, take a look at the series I did on linear algebra,

14:27.380 --> 14:29.380
especially Chapter 3.

14:29.380 --> 14:33.100
Back to our expression, instead of talking about adding the bias to each one of these

14:33.100 --> 14:39.140
values independently, we represent it by organizing all those biases into a vector and adding

14:39.140 --> 14:43.460
the entire vector to the previous matrix vector product.

14:43.460 --> 14:48.900
And as a final step, I'll wrap a sigmoid around the outside here, and what that's supposed

14:48.900 --> 14:53.060
to represent is that you're going to apply the sigmoid function to each specific component

14:53.060 --> 14:55.980
of the resulting vector inside.

14:55.980 --> 15:00.740
So once you write down this weight matrix and these vectors as their own symbols, you

15:00.740 --> 15:05.940
can communicate the full transition of activations from one layer to the next in an extremely

15:05.940 --> 15:08.380
tight and neat little expression.

15:08.380 --> 15:13.420
And this makes the relevant code both a lot simpler and a lot faster, since many libraries

15:13.420 --> 15:17.860
optimize the heck out of matrix multiplication.

15:17.860 --> 15:21.820
Remember how earlier I said these neurons are simply things that hold numbers?

15:21.820 --> 15:28.340
Well, of course, the specific numbers that they hold depends on the image you feed in.

15:28.340 --> 15:32.660
So it's actually more accurate to think of each neuron as a function, one that takes

15:32.660 --> 15:38.460
in the outputs of all the neurons in the previous layer and spits out a number between 0 and

15:38.460 --> 15:39.460
1.

15:39.460 --> 15:44.940
See, the entire network is just a function, one that takes in 784 numbers as an input

15:44.940 --> 15:47.540
and spits out 10 numbers as an output.

15:47.540 --> 15:52.580
It's an absurdly complicated function, one that involves 13,000 parameters in the forms

15:52.580 --> 15:56.740
of these weights and biases that pick up on certain patterns, and which involves iterating

15:56.740 --> 16:01.140
many matrix vector products and the sigmoid squishification function.

16:01.140 --> 16:03.820
But it's just a function nonetheless.

16:03.820 --> 16:06.740
And in a way, it's kind of reassuring that it looks complicated.

16:07.220 --> 16:11.080
I mean, if it were any simpler, what hope would we have that it could take on the challenge

16:11.080 --> 16:12.580
of recognizing digits?

16:12.580 --> 16:15.140
And how does it take on that challenge?

16:15.140 --> 16:19.740
How does this network learn the appropriate weights and biases just by looking at data?

16:19.740 --> 16:23.540
Well, that's what I'll show in the next video, and I'll also dig a little more into

16:23.540 --> 16:27.620
what this particular network we're seeing is really doing.

16:27.620 --> 16:31.540
Now is the point I suppose I should say subscribe to stay notified about when that video or

16:31.540 --> 16:36.340
any new videos come out, but realistically, most of you don't actually receive notifications

16:36.340 --> 16:38.180
from YouTube, do you?

16:38.180 --> 16:41.780
Maybe more honestly, I should say subscribe so that the neural networks that underlie

16:41.780 --> 16:46.260
YouTube's recommendation algorithm are primed to believe that you want to see content from

16:46.260 --> 16:48.260
this channel get recommended to you.

16:48.260 --> 16:50.900
Anyway, stay posted for more.

16:50.900 --> 16:53.660
Thank you very much to everyone supporting these videos on Patreon.

16:53.660 --> 16:57.780
I've been a little slow to progress in the probability series this summer, but I'm jumping

16:57.780 --> 17:03.780
back into it after this project, so patrons, you can look out for updates there.

17:03.780 --> 17:07.940
To close things off here, I have with me Lisha Lee, who did her PhD work on the theoretical

17:07.940 --> 17:12.340
side of deep learning, and who currently works at a venture capital firm called Amplify Partners,

17:12.340 --> 17:15.420
who kindly provided some of the funding for this video.

17:15.420 --> 17:19.740
So Lisha, one thing I think we should quickly bring up is this sigmoid function.

17:19.740 --> 17:23.500
As I understand it, early networks used this to squish the relevant weighted sum into that

17:23.500 --> 17:27.860
interval between zero and one, you know, kind of motivated by this biological analogy of

17:27.860 --> 17:30.660
neurons either being inactive or active.

17:30.820 --> 17:35.620
Relatively few modern networks actually use sigmoid anymore, it's kind of old school, right?

17:35.620 --> 17:39.260
Yeah, or rather, relu seems to be much easier to train.

17:39.260 --> 17:42.540
And relu stands for rectified linear unit?

17:42.540 --> 17:48.380
Yes, it's this kind of function where you're just taking a max of zero and a, where a is

17:48.380 --> 17:51.140
given by what you were explaining in the video.

17:51.140 --> 17:57.100
And what this was sort of motivated from, I think, was partially by a biological analogy

17:57.220 --> 18:01.380
with how neurons would either be activated or not.

18:01.380 --> 18:05.620
And so if it passes a certain threshold, it would be the identity function.

18:05.620 --> 18:09.500
But if it did not, then it would just not be activated, so be zero.

18:09.500 --> 18:11.020
So it's kind of a simplification.

18:11.020 --> 18:15.620
Using sigmoids didn't help training, or it was very difficult to train at some point

18:15.620 --> 18:17.820
and people just tried relu.

18:17.820 --> 18:24.820
And it happened to work very well for these incredibly deep neural networks.

18:24.820 --> 18:25.940
All right, thank you, Lisha.

