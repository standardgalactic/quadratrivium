WEBVTT

00:00.000 --> 00:06.880
I think you're doing the mistake of trying to ask a philosopher to be very concise.

00:06.880 --> 00:16.760
I just see like an avalanche of considerations and qualifications and levels for each of these very complex questions.

00:16.760 --> 00:17.760
Okay.

00:26.040 --> 00:28.120
Welcome to Closer to Truth.

00:28.120 --> 00:34.960
I'm speaking with futurist visionary Nick Bostrom about his vital, far-sighted, engaging new book,

00:34.960 --> 00:38.880
Deep Utopia, Life and Meaning in a Solved World.

00:38.880 --> 00:40.080
I loved it.

00:40.080 --> 00:46.560
It's an exhilarating romp of ultimate technology centered on AI, how it might work, what it could mean.

00:46.560 --> 00:49.200
It's a prescient manual for the future.

00:49.200 --> 00:52.680
It's an innovative treatise on the meaning of life.

00:52.680 --> 00:54.840
Welcome, Nick. It's great to see you again.

00:54.840 --> 00:56.320
Good to see you again.

00:56.320 --> 00:58.360
Congrats on Deep Utopia.

00:58.360 --> 00:59.640
We'll discuss it in depth.

00:59.640 --> 01:03.560
But to begin, I'd like to get your world overview, the setting for your book.

01:03.560 --> 01:09.960
When we first did our Closer to Truth discussion in Oxford in 2007, 17 years ago,

01:09.960 --> 01:15.400
we discussed the simulation argument, fine-tuning, anthropic selection, the doomsday argument.

01:15.400 --> 01:22.600
How would you characterize the last 17 years or so in terms of technological and intellectual development,

01:22.600 --> 01:26.280
especially the importance of AI?

01:26.360 --> 01:32.320
Well, I mean, it's all happening now, as we've entered the Atollius era.

01:32.320 --> 01:39.320
I think the last, especially since 2012-2014, with the Deep Learning Revolution,

01:39.320 --> 01:44.320
I think we've been on a kind of up-ramp of AI capabilities,

01:44.320 --> 01:49.360
kicking into even higher gear with the release of ChatGPT.

01:49.360 --> 01:53.880
And in the last two years or so, we've really seen it hit the mainstream.

01:53.920 --> 01:57.560
We're now the White House and key policy makers all over the world

01:57.560 --> 02:00.360
are starting to debate the future of AI.

02:00.360 --> 02:03.240
So it's a remarkable time.

02:03.240 --> 02:05.840
And when did you actually plan this book?

02:05.840 --> 02:11.600
Because it's obviously, in essence, the other side of your superintelligence,

02:11.600 --> 02:16.440
2014, where you were prescient in warning about the dangers of AI.

02:16.440 --> 02:20.680
And so the last two years has been a high focus on the dangers.

02:20.680 --> 02:23.840
Now you've moved on to the opportunities.

02:23.840 --> 02:29.640
So when did you have that a bit of a transformal insight?

02:32.320 --> 02:35.920
I've been working on it for probably around six years or so.

02:35.920 --> 02:39.240
It wasn't ever planned.

02:39.240 --> 02:42.000
It just kind of happened.

02:43.240 --> 02:47.640
I didn't start out with like some particular set of theses.

02:47.640 --> 02:51.040
I wanted to defend and elaborate on.

02:51.040 --> 02:57.520
I felt an urge to start writing, and then it eventually grew into DP Topia.

02:57.520 --> 03:00.160
Yeah, and I've seen where you've said that the style of the book,

03:00.160 --> 03:08.640
which is very unusual, it's a new literary style involving dialogue

03:08.640 --> 03:20.320
with different characters, your own persona in a not entirely actual form.

03:20.320 --> 03:29.640
But in dialogue with other people and form of a lecture series over a period of a week,

03:29.640 --> 03:34.160
I think you've said that whole structure wasn't planned.

03:34.160 --> 03:36.360
It happened organically.

03:36.440 --> 03:41.160
Yeah, it's just the way it happened for better or worse.

03:41.160 --> 03:48.520
But that's I do think the form actually does match the content.

03:48.520 --> 03:54.680
It's not a book so much about conclusions as it is a book about questions

03:54.680 --> 03:59.840
and helping the reader to start to think about these problems

03:59.840 --> 04:01.920
and form their own views ultimately.

04:02.000 --> 04:08.160
It's also meant to be not just something that transmits certain concepts and ideas,

04:08.160 --> 04:15.040
but also it's meant to be a reading experience that you might have to work to get through it.

04:15.040 --> 04:19.800
But ultimately, I'm hoping it will kind of put you in a better place

04:19.800 --> 04:25.160
to reflect on questions about what art humanity's destiny be.

04:25.160 --> 04:27.280
I think that's an accurate description.

04:27.280 --> 04:28.960
I found myself very engaged.

04:28.960 --> 04:37.080
I was looking for more of the arguments as that we've had in the past in a very positive sense.

04:37.080 --> 04:38.200
But this book is different.

04:38.200 --> 04:43.360
You do get the arc of various arguments on different things.

04:43.360 --> 04:44.600
We'll talk about that.

04:44.600 --> 04:54.480
But you are brought into that in this engaging intellectual, semi-fictional avatar environment.

04:54.480 --> 04:56.240
Yeah, yeah.

04:56.240 --> 05:03.360
And the other benefit of this sort of having different characters and different bits

05:03.360 --> 05:10.560
is that it makes it easier to explore several different viewpoints,

05:10.560 --> 05:16.560
which I wanted to do and allow each one to be developed in its own right to its fullest extent

05:16.560 --> 05:21.440
and then to kind of collide different perspectives and ideas.

05:21.440 --> 05:24.080
Just as, I mean, you're interested in physics, right?

05:24.080 --> 05:30.160
So with a particle accelerator, you sort of accelerate little particles to enormous energies

05:30.160 --> 05:31.840
and then smash them into one another.

05:31.840 --> 05:37.520
And in those extreme conditions, sometimes you can see the basic principles at work

05:37.520 --> 05:42.520
that we can then infer are at work all the time in ordinary conditions as well.

05:42.520 --> 05:44.240
It's just hard to observe.

05:44.240 --> 05:53.200
And so similarly, this conceit of a plastic world,

05:53.200 --> 05:56.920
a condition in which technology has reached its full maturity

05:56.920 --> 05:59.160
and all practical problems have been solved.

05:59.160 --> 06:01.160
It's an extreme condition.

06:01.160 --> 06:06.120
But I think we can then see values kind of smashing into one another

06:06.120 --> 06:11.080
that we normally can sort of hand wave and then just because they are obscured

06:11.080 --> 06:16.680
by so many practical necessities that kind of occupy most of our contemporary existence.

06:16.680 --> 06:18.840
I think that's a very good characterization.

06:18.840 --> 06:23.720
The book is creating an extreme condition and particle accelerators do that

06:23.720 --> 06:25.800
and physics black holes do that in physics.

06:25.800 --> 06:31.080
That's an extreme condition where people study black holes.

06:31.080 --> 06:32.920
It's not just for the black holes themselves,

06:32.920 --> 06:36.080
but it's subjecting the laws of physics to extreme conditions.

06:36.080 --> 06:37.400
And you learn a lot.

06:37.400 --> 06:39.800
And I think that's a very good characterization of the book.

06:41.480 --> 06:41.960
Yeah.

06:41.960 --> 06:48.880
So I think it's like, I mean, for people who have read it, they will know,

06:48.880 --> 06:54.440
but it's not a book that is really trying to make predictions.

06:54.440 --> 07:00.200
And nor is it trying to offer practical solutions to what we should do next.

07:00.200 --> 07:02.440
I mean, a lot of my other work focuses on that.

07:02.440 --> 07:03.360
Right.

07:03.360 --> 07:10.240
This takes basically as an assumption or a postulate, if you want, that things go well

07:10.240 --> 07:14.560
in order then to be able to ask the questions of what then,

07:15.920 --> 07:19.520
what would be the meaning of human existence?

07:19.520 --> 07:21.040
What would give us purpose in life?

07:21.760 --> 07:25.840
If the whole thing unfolds like everything is perfect,

07:25.840 --> 07:28.560
governance problems is all the alignment problem is solved,

07:28.560 --> 07:32.400
like all these things, but what then would occupy our lives?

07:33.440 --> 07:36.640
Sometimes you never actually get to even ask that question

07:36.640 --> 07:39.760
because there are so many other questions that kind of crowd in before it.

07:39.760 --> 07:43.360
So I just wanted to postulate that and then focus this book entirely

07:43.360 --> 07:47.680
on the set of questions that arise in this hypothetical future condition.

07:47.680 --> 07:51.360
We're going to get into all of it, but let me first give a more formal bio.

07:51.920 --> 07:54.400
Nick Bostrom is a professor at Oxford University,

07:54.400 --> 07:58.400
where he heads the Future of Humanity Institute as its founding director.

07:59.120 --> 08:02.880
With a background in theoretical physics, computational neuroscience,

08:02.880 --> 08:07.440
logic and artificial intelligence, Nick has pioneered contemporary thinking

08:07.440 --> 08:10.320
about existential risk, the simulation argument,

08:10.320 --> 08:12.880
and the vulnerable world hypothesis, among others.

08:13.520 --> 08:17.920
He is the most cited professional philosopher in the world age 50 or under

08:17.920 --> 08:22.560
and is the author of some 200 publications, including anthropic bias,

08:22.560 --> 08:26.480
global catastrophic risks, human enhancement and superintelligence,

08:26.480 --> 08:29.360
the prescient book on the dangers of AI,

08:29.360 --> 08:33.600
but now we're going to look at the extreme condition if all goes well.

08:33.680 --> 08:35.040
So Nick, your book,

08:35.040 --> 08:38.240
Deep Utopia, Life and Meaning in a Solved World.

08:38.240 --> 08:42.880
Let's start with a simple definition of what is a solved world

08:42.880 --> 08:45.040
and what motivates your focus on it.

08:46.560 --> 08:51.040
Well, I am referring to a hypothetical condition

08:51.040 --> 08:55.360
where basically all practical problems have been solved.

08:55.360 --> 08:59.600
So think, first of all, a condition of technical maturity.

08:59.600 --> 09:01.680
So we have super advanced AIs.

09:02.240 --> 09:05.040
Maybe they have helped us develop all kinds of other technologies,

09:05.040 --> 09:08.880
medical technologies, virtual reality, et cetera, et cetera.

09:08.880 --> 09:13.360
So that's part of what it would mean for the world to be solved.

09:14.000 --> 09:17.760
And then on top of that, we also make the assumption that

09:18.720 --> 09:22.800
all the kind of governance problems of the world have been solved

09:22.800 --> 09:24.320
to the extent that they can be solved.

09:24.320 --> 09:26.800
So we imagine we set the site,

09:26.880 --> 09:30.160
questions of war and conflict and oppression and inequality

09:30.160 --> 09:32.080
and all the rest of it.

09:32.080 --> 09:35.440
So but then there remains a big kind of problem,

09:35.440 --> 09:38.640
which is ultimately a problem of value,

09:38.640 --> 09:41.520
which is that under these ideal conditions,

09:43.280 --> 09:47.040
what kind of lives would we want to live?

09:49.040 --> 09:50.000
And yeah, that's...

09:50.880 --> 09:54.640
And that's a very important way to frame the book

09:54.640 --> 09:56.880
because you're not saying all of these problems

09:58.080 --> 10:01.200
that are solved are easy to solve or will be solved.

10:01.200 --> 10:04.160
But if you do solve it, what does that leave?

10:04.160 --> 10:05.200
And it leaves value.

10:05.200 --> 10:06.880
So one question that I have is,

10:06.880 --> 10:09.520
do you distinguish between meaning and purpose?

10:09.520 --> 10:12.640
We use those two terms sometimes interchangeably,

10:12.640 --> 10:16.400
but I think we can tease apart a difference.

10:17.280 --> 10:18.400
Meaning in the title,

10:18.400 --> 10:20.400
but throughout the book, you have purpose as well.

10:22.240 --> 10:22.960
Yeah, that's right.

10:22.960 --> 10:27.840
So I think of purpose as slightly narrower concept

10:27.840 --> 10:32.080
as sort of having a reason for doing something

10:32.080 --> 10:33.520
or for putting out some effort.

10:35.200 --> 10:38.800
And then meaning, I mean, that is discussed in the book,

10:38.800 --> 10:41.280
but might be a certain kind of purpose

10:41.280 --> 10:43.280
or purpose plus something else.

10:43.280 --> 10:43.600
Okay.

10:45.680 --> 10:48.640
You present, just to give a sense of the environment,

10:49.360 --> 10:56.400
utopic taxonomy where you have different levels of utopia

10:56.400 --> 10:59.360
that can give us a richer understanding of it.

10:59.920 --> 11:02.560
So let me just give you the list

11:02.560 --> 11:04.320
and just explain each one very quickly.

11:04.320 --> 11:06.480
The first is government and cultural utopia.

11:08.880 --> 11:14.640
Yeah, this is I think the most familiar kind of utopia

11:14.640 --> 11:15.920
we find in the literature,

11:16.800 --> 11:20.640
where people imagine a better way for society to be organized,

11:21.280 --> 11:24.480
better political institutions, different schools,

11:25.280 --> 11:27.200
maybe different gender roles,

11:27.200 --> 11:30.160
but usually set within more or less

11:30.160 --> 11:33.440
recognizably contemporary technological context.

11:33.440 --> 11:36.640
So people, there's still work that needs to be done

11:36.640 --> 11:39.760
and you can organize how much power the workers have

11:39.760 --> 11:41.200
or how the work is divided,

11:41.200 --> 11:44.880
but there have to be people growing food, et cetera.

11:44.880 --> 11:49.440
So that's the most familiar and basic kind of utopia.

11:49.440 --> 11:51.920
The second level is post-scarcity utopia.

11:52.480 --> 11:53.600
Sounds like we know what it means,

11:53.600 --> 11:56.080
but if you could define it more clearly.

11:56.080 --> 12:02.480
Yeah, so this is the more radical vision of a condition

12:02.480 --> 12:07.920
in which humans have plenty of all that we need materially.

12:08.960 --> 12:11.200
So there are these kind of,

12:11.200 --> 12:13.680
it's more like fantasy in the past,

12:13.680 --> 12:20.400
but various kind of the land of cocaine

12:20.400 --> 12:26.880
was this medieval peasant dream basically

12:26.880 --> 12:30.880
of some condition where the rivers would overflow

12:30.880 --> 12:35.440
with wine and roasted turkeys would just land on the plate

12:35.440 --> 12:38.560
and that would be a kind of continuous feasting.

12:39.280 --> 12:41.760
And you could easily see how that on its own

12:42.320 --> 12:43.440
would have huge attraction

12:43.520 --> 12:47.440
if you were a kind of agricultural labor

12:47.440 --> 12:50.000
who spent your whole life grinding away,

12:50.640 --> 12:53.840
barely getting enough porridge to feed your children

12:53.840 --> 12:55.440
and your joints were aching

12:55.440 --> 12:57.760
from all this backbreaking work that you were doing.

12:57.760 --> 12:59.440
Then this on itself,

12:59.440 --> 13:02.400
just being able to rest and eat as much as you want

13:02.400 --> 13:06.800
would already be like enough of a kind of vision.

13:08.000 --> 13:10.480
Third level is post-work utopia.

13:11.440 --> 13:13.600
Right, so this is the idea that not just

13:13.600 --> 13:16.800
is there plenty to consume,

13:16.800 --> 13:20.800
but that the production of all this plenty

13:20.800 --> 13:23.920
doesn't require human economic labor.

13:25.520 --> 13:28.800
And this has started popping up more recently

13:28.800 --> 13:30.800
in conversations about the future of AI

13:31.840 --> 13:33.680
where people are wondering,

13:33.680 --> 13:37.920
will this advance that we see lead to human unemployment?

13:37.920 --> 13:39.440
We can automate more and more things.

13:41.440 --> 13:43.600
And if you imagine that running its full course,

13:43.600 --> 13:46.240
then maybe eventually you could automate

13:46.240 --> 13:49.040
almost all human economic labor

13:49.040 --> 13:52.000
and then you would have this post-work condition.

13:52.720 --> 13:56.480
The fourth and fifth to get a little more complicated

13:56.480 --> 13:58.320
to understand, but let's do it now.

13:58.960 --> 14:01.680
Fourth is post-instrumental utopia.

14:02.480 --> 14:05.840
So now we're getting into a more radical conception.

14:07.920 --> 14:10.640
And usually current conversations about these issues

14:10.640 --> 14:14.000
stop short before we reach this idea.

14:14.640 --> 14:16.960
But if you really think through what it would mean

14:17.760 --> 14:21.120
for AI to attain its full potential

14:21.680 --> 14:23.840
and then all the other technological advances

14:23.840 --> 14:27.280
that this kind of machine superintelligence

14:27.280 --> 14:28.000
could bring about,

14:29.600 --> 14:32.480
it's not just that we wouldn't have to go into the office

14:32.480 --> 14:35.600
and type on word processors or like hammer away

14:35.600 --> 14:38.320
at construction sites, et cetera.

14:39.440 --> 14:42.560
But also a lot of the other things we need to do

14:43.360 --> 14:46.400
in our daily lives could be automated as well.

14:48.560 --> 14:50.880
So if you think about, if you didn't have to work,

14:50.880 --> 14:53.600
well, then like typical answers would be, well,

14:55.280 --> 14:57.680
maybe somebody likes fitness or something,

14:57.680 --> 14:59.840
so they could spend more time exercising.

14:59.840 --> 15:01.200
But like in this condition,

15:01.200 --> 15:02.800
you could pop a pill instead

15:02.800 --> 15:06.560
and get the same physiological and psychological effects

15:06.560 --> 15:11.680
that spending an hour on the Stairmaster would provide.

15:11.680 --> 15:15.360
And so then why would you really need to go to,

15:15.360 --> 15:16.640
kind of would lose its point

15:16.640 --> 15:18.720
to go to the gym in those conditions?

15:18.720 --> 15:23.200
And you can then start to kind of almost do case studies

15:23.200 --> 15:26.160
on activities that fill our current lives.

15:26.160 --> 15:27.360
And for almost all of them,

15:27.360 --> 15:30.400
you soon see that they have a certain structure,

15:30.400 --> 15:32.000
which is that we do a certain thing,

15:32.000 --> 15:35.680
put in some effort in order then to achieve some other thing.

15:36.480 --> 15:40.320
So brush your teeth, because otherwise eventually

15:40.320 --> 15:42.480
you will have tooth decay and gum decay.

15:42.480 --> 15:46.240
And so in order to get the outcome of a healthy mouth,

15:46.240 --> 15:49.520
you need to spend a few minutes every day brushing your teeth

15:49.520 --> 15:50.480
and gone to the gym.

15:52.320 --> 15:55.680
Or you need to, like you want to understand mathematics, let's say.

15:55.680 --> 15:57.200
So then the only way to do that

15:57.200 --> 15:59.440
is to put in some effort to study mathematics.

16:00.400 --> 16:03.920
And so the effort is motivated by the goal

16:03.920 --> 16:06.400
that it is trying to achieve outside the effort itself.

16:07.680 --> 16:11.840
And a lot of the things that we are doing has that structure.

16:11.840 --> 16:14.400
But now if you could get the goal, the end point,

16:14.400 --> 16:16.000
without having to put in the effort,

16:16.960 --> 16:20.880
then it seems to pull out the rug under the activity itself.

16:21.840 --> 16:24.160
At least it's threatened with the sense of being pointless.

16:25.920 --> 16:27.520
So that's the problem you confront

16:27.600 --> 16:29.520
in this kind of post-instrumental case.

16:29.520 --> 16:30.640
So an example of that,

16:30.640 --> 16:32.640
instead of studying higher mathematics,

16:32.640 --> 16:34.640
you could have an injection of nanobots

16:35.840 --> 16:38.720
that could analyze every synapse in your brain

16:38.720 --> 16:42.240
and then figure out how to change them a little bit

16:42.240 --> 16:46.320
so I can understand algebraic geometry or something.

16:46.320 --> 16:47.600
Right, that's right.

16:47.600 --> 16:49.920
And that would be fast and effortless.

16:50.960 --> 16:53.600
Or even things we do for fun.

16:54.560 --> 16:56.240
So there might be various activities

16:57.200 --> 16:58.800
because it gives you pleasure and joy.

17:00.400 --> 17:02.160
That seems kind of unavoidable.

17:02.160 --> 17:03.840
But even there, if you think about it,

17:03.840 --> 17:07.440
you could have more direct ways of experiencing

17:07.440 --> 17:09.360
the same positive emotions,

17:09.360 --> 17:12.080
like a kind of some super drugs

17:12.800 --> 17:14.960
that could give you the pleasure and the joy

17:14.960 --> 17:18.000
without you having to spend an hour gardening

17:18.000 --> 17:22.560
or doing whatever it is that you're watching movies.

17:23.040 --> 17:26.160
So this brings up a very important point of the book

17:26.960 --> 17:30.080
in terms of what are our real values.

17:30.080 --> 17:32.880
Because when I read that,

17:32.880 --> 17:35.120
and obviously a very intriguing,

17:35.120 --> 17:38.240
remarkable way of thinking and very important,

17:39.280 --> 17:40.320
I was asking myself,

17:40.320 --> 17:43.600
are there any absolute values in a solved world?

17:43.600 --> 17:45.200
And so the way to describe it,

17:45.200 --> 17:46.240
as you started to say,

17:46.240 --> 17:49.280
is if we could take non-harmful drugs

17:49.280 --> 17:51.760
or AI neural implants

17:51.760 --> 17:55.280
that would maintain a state of perpetual ecstasy,

17:55.280 --> 17:59.120
whatever your ecstasy would happen to be,

17:59.120 --> 18:03.520
or it can switch from a physical bodily ecstasy

18:03.520 --> 18:06.720
to an artistic ecstasy or intellectual one.

18:06.720 --> 18:08.000
And if that could be all done,

18:10.640 --> 18:12.320
who could gain say that

18:12.320 --> 18:16.640
if there's no kind of supernatural value

18:16.640 --> 18:18.720
that you would put into it?

18:19.680 --> 18:23.120
I think that is a fundamental theme of your book

18:23.120 --> 18:26.160
about how do we develop the kinds of values

18:26.160 --> 18:27.440
if these things are possible.

18:27.440 --> 18:31.200
You're not saying these things are remotely

18:31.200 --> 18:34.560
in the near or mid or even long term,

18:34.560 --> 18:36.320
but they are the extreme condition

18:36.320 --> 18:39.680
that you talked about which then exposes

18:39.680 --> 18:43.120
what is the nature of absolute values if there are any.

18:44.480 --> 18:44.800
Right.

18:45.760 --> 18:49.600
Now, it is possible that they might be in the long term

18:49.600 --> 18:51.040
or even mid or near term,

18:51.040 --> 18:53.760
depending on how fast the AI revolution unfolds

18:54.400 --> 18:55.760
and what the outcome of that is.

18:55.760 --> 18:59.280
I actually think the time scales for radical transformation

18:59.280 --> 19:01.920
might be shorter than most people realize

19:01.920 --> 19:05.920
if AI continues to speed ahead.

19:05.920 --> 19:08.000
What's your best guess on that?

19:08.560 --> 19:08.880
Well,

19:09.280 --> 19:15.760
I mean, my timelines have shortened somewhat,

19:15.760 --> 19:18.640
at least since this previous book,

19:18.640 --> 19:21.360
Superintelligence, was published in 2014.

19:22.480 --> 19:24.800
I think we are currently looking on timelines

19:24.800 --> 19:27.840
that are on the shorter end of the distribution.

19:29.120 --> 19:30.080
So it's hard to say,

19:30.080 --> 19:35.360
but I mean, it could be years or maybe a decade or perhaps more,

19:35.360 --> 19:38.320
but at least I think there's a non-trivial probability

19:38.320 --> 19:42.640
that we are now kind of on the accelerating slope of this,

19:42.640 --> 19:43.760
but time will tell.

19:44.960 --> 19:46.720
In any way, that's not central to the book,

19:46.720 --> 19:48.400
even if you thought this would take millions

19:48.400 --> 19:49.120
or never happen.

19:49.120 --> 19:49.680
It is still.

19:50.560 --> 19:52.320
But there are two ways of thinking about this.

19:52.320 --> 19:53.600
You could either just read it as,

19:54.800 --> 19:56.960
these are perpetual philosophical questions

19:56.960 --> 19:58.080
that humanity ponderous,

19:58.080 --> 19:59.840
and here is kind of a thought experiment

19:59.840 --> 20:01.040
that helps you think about them.

20:01.040 --> 20:04.880
And I think it's fine if you just read it like that.

20:04.880 --> 20:07.200
For me, there is also this actual real possibility

20:07.200 --> 20:09.920
that we might soon enter a condition like this,

20:09.920 --> 20:12.240
or we might have to make decisions soon

20:13.280 --> 20:14.640
about what kind of future we want,

20:14.640 --> 20:16.720
if we want to stare towards something like this

20:16.720 --> 20:17.920
or some other version of this.

20:18.960 --> 20:22.720
So there is this kind of underlying practical motivation

20:22.720 --> 20:25.360
for me in terms of writing this book,

20:25.360 --> 20:27.200
but that's optional.

20:29.040 --> 20:30.160
So yeah.

20:30.880 --> 20:31.760
I appreciate that.

20:31.760 --> 20:34.480
I was more on the former, on the thought experiment,

20:35.280 --> 20:38.960
and I would put the date measured in hundreds of years,

20:38.960 --> 20:42.160
if not thousands, to achieve what you're saying,

20:42.160 --> 20:44.400
but I'm cautious.

20:44.400 --> 20:45.760
I've become cautious as you,

20:46.720 --> 20:49.040
and I hear you and respect your views,

20:49.040 --> 20:51.840
so I'm a little less sure of what I thought before.

20:51.840 --> 20:55.760
Anyway, we want to get the fifth category

20:55.760 --> 20:58.960
of the utopic taxonomy,

20:58.960 --> 21:01.200
which you call plastic utopia.

21:01.600 --> 21:04.720
Even going beyond the post-instrumental utopia,

21:05.360 --> 21:06.960
what is the plastic utopia?

21:08.720 --> 21:10.480
Well, we alluded to it slightly,

21:10.480 --> 21:13.280
which is the idea that in addition to

21:14.160 --> 21:16.800
having these other properties of being post-instrumental,

21:17.440 --> 21:20.880
we also, in the plastic utopia condition,

21:20.880 --> 21:24.000
have complete control over ourselves.

21:24.560 --> 21:26.800
So our mental states, our psychology,

21:26.800 --> 21:29.840
our cognitive architecture, our bodies becomes malleable.

21:31.360 --> 21:33.440
So it's not just that we imagine human beings,

21:33.440 --> 21:35.520
as we are now, placed in this condition

21:35.520 --> 21:36.800
where we don't have to work

21:36.800 --> 21:38.880
and where we don't have to put out any effort

21:38.880 --> 21:39.680
if we don't want to.

21:39.680 --> 21:42.000
We could just press buttons and get what we want,

21:42.000 --> 21:45.680
but we ourselves, as well, become something we can choose.

21:46.480 --> 21:49.040
So you wouldn't have to work on yourself

21:50.000 --> 21:52.720
to build a better character in a plastic utopia.

21:52.720 --> 21:54.480
If you wanted to, instead,

21:54.480 --> 21:57.280
you could sort of request of your AIG

21:57.360 --> 22:00.800
need to sort of rewire your synopsis

22:00.800 --> 22:03.120
so that you became this different kind of person

22:03.120 --> 22:05.040
or to experience pleasure all the time

22:05.040 --> 22:08.000
or to become smarter or kinder or whatever else.

22:09.280 --> 22:15.120
So that makes it even more like solved or dissolved or liquid.

22:15.120 --> 22:17.920
Like you enter this context

22:17.920 --> 22:20.880
where everything seems kind of fluid and up for grabs,

22:20.880 --> 22:24.080
and it's hard to find on this firm ground to stand on.

22:25.040 --> 22:28.720
Yeah, it's a wonderful way

22:28.720 --> 22:32.080
of seeing what an extreme condition is,

22:33.040 --> 22:35.600
because I would have not come up with those five.

22:35.600 --> 22:36.960
I might have had three or four,

22:38.000 --> 22:40.240
but at that level,

22:40.240 --> 22:43.520
it really very beautifully defines

22:44.080 --> 22:47.440
what an extreme condition for humanity could be in the future,

22:47.440 --> 22:50.640
and therefore it gives the book its real punch.

22:50.640 --> 22:52.480
So one issue that you deal with

22:54.080 --> 22:56.160
especially as you get to all of those

22:56.160 --> 22:57.680
is the question of boredom

22:59.120 --> 23:02.320
and how much of our value system is based upon

23:03.280 --> 23:08.960
the need to or the lack of control and the uncertainty

23:08.960 --> 23:10.320
and what happens whenever this,

23:10.320 --> 23:12.080
you know, this is the same kind of problem

23:12.080 --> 23:14.480
that traditional religions,

23:14.480 --> 23:16.880
both East and West have to deal with

23:16.880 --> 23:19.280
whether you're dealing with nirvana

23:19.280 --> 23:23.520
after the innumerable cycles of birth and death and rebirth,

23:23.520 --> 23:28.400
and then you reach nirvana or in the Judeo-Christian,

23:28.400 --> 23:31.920
Islamic, Abrahamic concept of heaven,

23:31.920 --> 23:33.120
eternal life in heaven.

23:33.120 --> 23:38.240
I mean, that's a sort of an end question of boredom

23:38.240 --> 23:42.160
that occurs in any of these eschatological ideas.

23:43.600 --> 23:45.440
Yeah, it's quite fascinating

23:45.440 --> 23:47.040
if you think far enough in this direction,

23:47.040 --> 23:51.360
you do start to sort of bottom up against theological questions

23:51.440 --> 23:54.480
or at least questions that have traditionally been

23:55.760 --> 23:59.280
discussed in religious contexts about the afterlife, etc.

24:00.400 --> 24:06.000
I try to not trespass onto that terrain in the book.

24:06.000 --> 24:09.200
I have this, there's this other fictional character,

24:09.200 --> 24:11.840
the fictional Bostrom character is giving these lectures

24:11.840 --> 24:15.040
and then sometimes he's asked questions by the students

24:15.040 --> 24:17.440
and occasionally he sort of refers to,

24:17.440 --> 24:20.640
well, you have to take that up with Professor Grossweiter

24:20.640 --> 24:22.240
which is like another character

24:22.240 --> 24:23.840
who doesn't make an appearance in the book

24:23.840 --> 24:25.840
but he's like the theologian

24:25.840 --> 24:28.560
or the person who could answer their questions on that.

24:30.560 --> 24:32.960
But on boredom, yeah, so this is an example

24:32.960 --> 24:35.040
where I think it's important to distinguish

24:36.960 --> 24:39.040
the two different senses of boredom

24:39.040 --> 24:42.320
which is a subjective sense and an objective sense.

24:42.320 --> 24:45.280
So clearly we have a subjective concept of boredom

24:45.280 --> 24:47.040
like somebody might just feel bored

24:47.520 --> 24:53.520
and that would trivially be easy to abolish in Utopia.

24:53.520 --> 24:56.720
I mean, it follows directly from the condition of plasticity

24:56.720 --> 25:00.720
that this feeling is like a subjective state of your brain.

25:00.720 --> 25:05.040
You could rewire that so that you would always feel excited

25:05.040 --> 25:09.520
or interested or whatever antonyms to boredom you want to use.

25:09.520 --> 25:14.560
And the question then is whether there is also

25:15.520 --> 25:21.120
some notion of objective boredom or boringness

25:22.320 --> 25:25.440
whether certain activities or experiences

25:25.440 --> 25:27.520
are such that they are objectively boring

25:27.520 --> 25:30.000
like meaning perhaps that it would be appropriate

25:30.000 --> 25:33.920
to feel subjectively bored if we engaged in them.

25:34.960 --> 25:38.880
So if you imagine somebody to take an example

25:38.880 --> 25:42.800
actually from the philosophical literature of a grass counter

25:42.800 --> 25:47.280
so somebody who spends his life counting the blades of grass

25:47.280 --> 25:50.880
on a particular college lawn, we might think

25:52.320 --> 25:55.520
that that's an activity that is objectively boring

25:55.520 --> 25:57.840
whether or not he happens to feel excited about it

25:57.840 --> 26:00.800
it's not appropriate to be really interested in grass counting

26:00.800 --> 26:03.600
because it's like too monotonous or insignificant

26:03.600 --> 26:06.240
or has some other sort of deficit.

26:08.400 --> 26:11.440
And philosophers disagree about whether like

26:12.400 --> 26:15.600
there is like some kind of firm normative basis for making that

26:15.600 --> 26:16.880
but I think it's an intuition that

26:19.600 --> 26:21.520
some but not all people would have that

26:21.520 --> 26:25.680
it would be bad if the future consisted of merely of activities

26:25.680 --> 26:26.800
like counting grass.

26:26.800 --> 26:30.080
No matter how thrilled the people doing the grass counting were.

26:31.280 --> 26:36.320
That's an objective value that's kind of is a superset

26:36.320 --> 26:37.920
to everything else we're talking about.

26:38.480 --> 26:39.440
Yeah potentially.

26:39.520 --> 26:42.800
And so I'm able to in a plastic world change my brain

26:42.800 --> 26:47.600
to where I am excited about every new grass that I count

26:47.600 --> 26:49.520
and what's going to happen at the next number

26:49.520 --> 26:51.360
and I'm genuinely excited about that

26:51.360 --> 26:55.760
and I've changed my brain to think and so subjectively

26:55.760 --> 27:00.400
I'm excited about life counting all these grasses

27:00.400 --> 27:03.920
or as I think you have in the book table legs

27:04.640 --> 27:06.640
200 and somewhat thousand table legs

27:06.640 --> 27:13.360
or that that potentially is in some objective sense

27:13.920 --> 27:14.960
is suboptimal.

27:16.080 --> 27:18.560
Yeah that's possible to hold that view and

27:21.280 --> 27:25.680
now if one does have that view that it would be objectively

27:26.640 --> 27:32.080
bad to have nothing in one's life than you know counting grass

27:32.800 --> 27:37.360
then that kind of also potentially imposes a constraint

27:37.360 --> 27:39.440
on the subjective experience of boredom.

27:39.440 --> 27:42.000
Like if it actually is objectively bad to do that

27:42.000 --> 27:44.880
then you might think it would be also bad to change

27:44.880 --> 27:47.760
your subjective attitude so that you found great interest

27:47.760 --> 27:49.360
is something that would be boring.

27:49.360 --> 27:51.520
So then you might end up with a situation

27:51.520 --> 27:54.320
where you couldn't eliminate boredom in the future

27:54.320 --> 27:56.160
neither in the objective or objective sense.

27:57.680 --> 28:01.360
And so then but so this possibility of the objective value

28:02.080 --> 28:04.720
there makes the discussion more complex.

28:04.720 --> 28:08.000
Like eliminating the subjective if that's all there is trivial

28:08.000 --> 28:10.880
given the postulates but once they introduce this possibility

28:10.880 --> 28:12.880
of the objective then it becomes like a much more

28:12.880 --> 28:16.480
intricate conversation to what extent we would be able to do

28:16.480 --> 28:19.200
something without violating that.

28:19.200 --> 28:24.240
Now I think at least with respect to the value of

28:24.240 --> 28:27.520
interestingness and there is a bunch of these different

28:27.520 --> 28:29.600
values that they're kind of related but different

28:29.600 --> 28:33.280
but if we focus on interestingness I think there is at least

28:34.480 --> 28:38.800
large scope before increasing the amount of subjective

28:38.800 --> 28:42.480
and objective interestingness including in these utopian lives.

28:42.480 --> 28:45.920
I think even if there is some objective element to what's

28:45.920 --> 28:50.080
boring and what's interesting I think it's has a large

28:50.080 --> 28:52.080
zone of indeterminacy.

28:52.080 --> 28:56.640
I mean you can just look at the current human distribution.

28:56.640 --> 29:02.640
I have a good friend and colleague who tells me he's never bored

29:02.640 --> 29:07.760
and he's interested in as far as I can tell literally everything except sport.

29:08.960 --> 29:11.680
He writes papers on all kinds of topics.

29:11.680 --> 29:12.880
He knows about everything.

29:12.880 --> 29:15.520
He goes to every kind of conference and finds

29:15.520 --> 29:17.360
interesting things to discuss with every person.

29:17.360 --> 29:22.640
Like it doesn't seem to me that there is anything deficient about his human life.

29:23.600 --> 29:32.000
In fact if anything it seems to benefit and be like a greater person for this

29:32.000 --> 29:36.640
this property that he has and obviously that goes down ultimately to some neurochemical

29:36.640 --> 29:43.520
idiosyncrasies of his brain that's he and I think for all of us I think we could expand

29:43.520 --> 29:50.080
the range of things in which we take an interest greatly before we would reach

29:50.080 --> 29:52.560
this point where we would just be counting leaves of grass.

29:53.600 --> 29:59.840
Moreover I think possibly it would be appropriate to expand it even further than that.

29:59.840 --> 30:06.720
Like maybe if we have reached a condition where we had sort of exhausted all the

30:06.720 --> 30:11.600
most obviously interesting things we had discovered all the really fundamental loss of

30:13.120 --> 30:18.640
nature you know solved consciousness and like the biggest questions had all been answered.

30:18.720 --> 30:21.840
Like it would seem perfectly appropriate in that situation to begin to take an

30:21.840 --> 30:26.720
interest in the slightly smaller questions and it's not clear that one couldn't go very

30:26.720 --> 30:32.480
very far in that direction before I reached a point where it would be sort of objectively

30:32.480 --> 30:34.960
bad to take a further interest.

30:34.960 --> 30:37.600
Is religion relevant in a solved world?

30:39.680 --> 30:46.560
Yes potentially very relevant although it's also arguably very relevant in the current world

30:47.120 --> 30:54.400
and one might say something more interesting perhaps if one looks at some other values that

30:54.400 --> 31:00.080
seem not so relevant in the current world but that could potentially become more relevant

31:00.080 --> 31:00.880
in this whole world.

31:00.880 --> 31:09.920
I think that there might be a lot of subtle values that exist now but we don't really

31:09.920 --> 31:16.080
see them very much just as we don't see the stars during daytime because it's like

31:17.040 --> 31:24.320
you know such a like brighter present and analogously there are such stark moral

31:24.320 --> 31:25.520
imperatives right now.

31:27.920 --> 31:33.360
Calamities of all sorts you need to take care of your kids you there are people starving in

31:33.360 --> 31:39.680
the world or being shot at etc etc so so many horrors and an urgent obvious pressing

31:40.880 --> 31:47.520
ethical needs to fix things that it would almost be frivolous now to spend too much time

31:49.040 --> 31:55.280
fretting about more subtle quiet values but if we ever reached a condition where these

31:55.280 --> 32:02.400
pressing needs were taken care of then I think we might be able to see a whole panoply

32:03.120 --> 32:05.440
of these subtler values like for example

32:05.840 --> 32:10.800
various kinds of traditions that it would be nice to honor authentically

32:12.720 --> 32:19.840
ancestors who you know maybe we think of our lost parents once in a while but there is like so

32:19.840 --> 32:25.040
many more people have lived wonderful lives and you know maybe deserve more thought and

32:25.040 --> 32:29.360
consideration we don't have time our daily lives keep us busy but if you didn't have that

32:29.600 --> 32:35.440
but if you didn't have that why not various aesthetic qualities you could imagine making

32:35.440 --> 32:41.440
your life more into a kind of artwork where every relationship was not just a source of

32:41.440 --> 32:45.600
I don't know relaxation or final satisfaction but also something actually beautiful that you

32:45.600 --> 32:51.280
were kind of constructing together etc etc and we don't now have the luxury to kind of really

32:51.280 --> 32:55.360
develop a fine sensibility for those but I think it would be entirely appropriate

32:56.000 --> 33:02.000
that once the urgencies are removed to to sort of tune up like almost like our eyes

33:02.000 --> 33:06.640
dilate at night right so it can take in more light similar in this condition our moral

33:06.640 --> 33:11.600
sensibilities and sensibilities for subtle values I think should dilate and this is a

33:11.600 --> 33:19.440
larger definition of religion as we might have it in today's world enabled by the the solved world

33:20.160 --> 33:29.920
the boundaries of religion become broader yeah potentially but again it also potentially

33:29.920 --> 33:36.320
is very important today so it might be one of those things that is very urgent today even

33:36.320 --> 33:42.480
with other urgencies pressing in upon us like many religious people would say that yes you have all

33:42.480 --> 33:46.560
these practical things you should do but you should also set the time for worship etc even

33:46.560 --> 33:53.360
though it conflicts with but but even more so obviously in this condition and I mean we would

33:53.360 --> 33:58.400
be more like I guess potentially like monks and nuns that have the time to fully devote themselves

33:59.120 --> 34:04.160
to contemplating the divine when I first heard of the book and started it

34:04.960 --> 34:10.080
the first question one of the first questions came into my mind is how does a solved world the

34:10.080 --> 34:17.760
the bostrom solved world articulate with the marxist pure communism and as I started to go

34:17.760 --> 34:26.000
through the book to me that question became pretty obvious that the the that there would be a high

34:26.000 --> 34:35.040
correlation potentially between the first at least two of the utopia taxonomy levels the

34:35.040 --> 34:40.720
government and culture and then the post scarcity and then maybe into the post work as well but

34:41.600 --> 34:47.040
pure communism as it's been envisioned in the past or even in in few cases in the present

34:47.920 --> 34:55.040
doesn't even deal with the points four and five right and and I mean I think I'm not really a

34:55.040 --> 35:00.800
marx scholar but I think he just has a few lines really about what would you know ultimately be

35:00.800 --> 35:06.240
the outcome of if the whole communist product succeeded and I think he refers to this whatever

35:06.240 --> 35:12.160
is it like fishing in the morning and hunting in the afternoon and reading poetry in the evening

35:12.160 --> 35:18.480
so that that sounds like not even a fully post work utopia but like maybe an abundance diminished

35:18.480 --> 35:25.520
work utopia plus a sort of vision of social cultural utopia I guess right right Nick let's

35:25.600 --> 35:31.520
switch and look really long term and very visionary what I call your approach to ultimate

35:31.520 --> 35:39.040
utilitarianism I love the section a quantitative analysis of the potential for happiness or

35:39.040 --> 35:47.120
fulfillment for all sentient beings if if the cosmic endowment could be maximally saturated

35:47.120 --> 35:54.000
with sentience so some of the numbers you give you estimate 10 to the 35th possible human lives

35:54.000 --> 35:59.440
derived from human lives originating on earth to populate the observable universe that's a

35:59.440 --> 36:04.720
think a hundred billion trillion trillion that's your minimum then you go up to 10 to the 43rd and

36:04.720 --> 36:12.560
then if you switch to digital lives which adds a lot of complex value you get a computing power

36:12.560 --> 36:17.920
of the universe of at least 10 to the 58th which is 10 billion trillion trillion trillion four

36:17.920 --> 36:27.360
trillions there in terms of ultimate sentience so what I love the calculation but walk me through

36:28.000 --> 36:35.920
the importance of that in our ultimate thinking and also in terms of the concept of meaning

36:35.920 --> 36:44.480
and purpose which is the purpose of your book well I mean it's like some big number basically

36:44.480 --> 36:49.600
very big number but I mean it's in the context of the book it's a little handout

36:49.600 --> 37:01.280
as the postroom gives out but yeah so I'm not a utilitarian I'm often mistaken for one because

37:02.160 --> 37:10.160
in some of my writings I have explored and analyzed the implications of assuming

37:10.160 --> 37:17.280
an assumption of utilitarianism or aggregative consequentialism because it's a view that you know

37:18.880 --> 37:25.120
significant fraction of moral philosophers have held and that I think maybe deserves at least

37:25.120 --> 37:29.280
some weight even if one doesn't actually embrace it and then it's interesting to see what follows

37:29.280 --> 37:36.480
if one actually takes that perspective seriously and hence in my earlier work this you know the

37:36.560 --> 37:41.040
focus on existential risks as those few things that could actually permanently

37:41.600 --> 37:48.400
destroy our future if one counts these possible future lives the same way as actually currently

37:48.400 --> 37:55.200
existing lives as certain flavors of utilitarianism would do then they just seem to dominate and you

37:55.200 --> 37:59.680
get a bunch of interesting and then there's like a further complication on that which is if you

38:00.160 --> 38:07.280
literally do this try to do this expected utility calculation you find that scenarios in which somehow

38:07.280 --> 38:12.000
even if they are very unlikely but somehow infinite values could be realized like maybe we

38:12.000 --> 38:15.680
are wrong about physics and there's like some actual way of producing infinite and then those

38:15.680 --> 38:20.480
tend to dominate even if they have a very very tiny finite and then you get into infinitarian

38:20.480 --> 38:24.480
paralysis and there's like a whole yeah so I think that's interesting in its own right but it's not

38:24.480 --> 38:33.920
really the topic of the book which more focuses on not not how you aggregate big values or what

38:33.920 --> 38:38.640
our obligations are but like from our point of view like what would be the best possible future

38:39.200 --> 38:46.080
for Robert or for the for you the the viewer or for any of us like if you literally could

38:46.080 --> 38:51.840
imagine the best possible way for your future life to unfold and perhaps we restricted by like

38:51.840 --> 38:58.080
the loss of physics etc but and then trying to think from the inside like how we would furnish

38:58.080 --> 39:06.720
that life with activity experiences relationships etc and then ultimately like you know if your

39:06.720 --> 39:11.120
question is what you should you do now as a moral actor then you would have to somehow integrate all

39:11.120 --> 39:18.320
these different perspectives whatever weight you would put on utilitarian views or the ontological

39:18.320 --> 39:26.080
views or virtue ethics views and the bunch of other stuff but the yeah the book doesn't really try to

39:27.440 --> 39:34.000
pick between these different moral theories it doesn't really in general focus so much on numbers

39:34.000 --> 39:40.480
or on formal structures and aggregation but more tried to sort of which often is done in

39:40.480 --> 39:45.840
contemporary analytic ethics it kind of almost sees the values a little bit like

39:47.760 --> 39:52.160
black pork and that might not be exactly right but this ties to look from the inside

39:52.880 --> 39:55.840
on the values which values do you actually have like at the object level

39:57.120 --> 40:02.080
and what would it take to realize them I'm not sure whether that answers your question but

40:03.040 --> 40:11.600
now there is this yeah I guess I guess like one one way in which this larger view of

40:12.320 --> 40:19.120
the bigness of the future could and does come into the book is insofar as we value

40:20.240 --> 40:25.120
significance like having significant impact on the world for example if that's the version of

40:25.120 --> 40:33.280
significance right now it looks like we have we are extraordinarily well positioned to have

40:34.000 --> 40:41.280
huge impact on the world because well a there's a lot of just ordinary needs in the world and you as

40:41.280 --> 40:48.080
an you know if you know if we imagine you're like a relatively well-off person with health and

40:48.080 --> 40:52.240
intelligence in a wealthy country with a good education like probably most of your viewers are

40:52.960 --> 40:56.800
you have a lot of opportunity just to help a bunch of people and to try to make some positive

40:56.800 --> 41:02.400
difference so that already gives your life potential significance that is maybe greater

41:02.400 --> 41:06.080
than one human life's worth of significance like you could save many people's lives or

41:06.640 --> 41:11.840
but then on top of that you have this idea that maybe we are near a big fulcrum of human history

41:11.840 --> 41:16.720
where if this whole thing the AI transition and the rest of it is going to happen perhaps within

41:16.720 --> 41:21.920
our lifetime then like you can multiply that manifold like if you could even slightly notch

41:21.920 --> 41:26.480
this big future in the right direction that would give your life even more causal significance

41:28.080 --> 41:32.000
this is one thing that might be a lot harder for people living in utopia to have

41:32.960 --> 41:36.080
this kind of significance because if all the problems are already solved

41:36.720 --> 41:41.520
and whatever problems aren't yet solved are anyway much better work that by AIs

41:42.400 --> 41:47.600
then humans might not be able to have significance in that sense and so to the extent that one

41:47.600 --> 41:56.960
thinks it makes a life itself better to have this kind of significance these utopian lives might

41:56.960 --> 42:00.560
lack that significance and therefore have a deficit of that particular value

42:01.680 --> 42:08.240
and so there's a discussion around that and also the possibility of humans whether through AI

42:09.200 --> 42:18.880
colonizing or filling the universe with sentience is a you know gigantic grand vision

42:20.640 --> 42:27.840
um yeah I mean if that's if that's the way one wants to go I mean I mean I actually

42:27.840 --> 42:32.160
happen to think the future is big enough that you could not just realize one vision but many

42:32.880 --> 42:39.600
not every vision because some are directly in conflict but if some people think doing

42:39.600 --> 42:46.160
something nice for existing people and working locally is the most important thing we could

42:46.160 --> 42:52.080
certainly do that and then also that leaves all the rest of the universe and that's big enough

42:52.080 --> 42:59.200
that you could have sort of AI paradise in one sector and you know animal uplift in another

42:59.200 --> 43:04.000
sector and you could have a whole bunch of different to the extent that a vision doesn't

43:04.000 --> 43:09.120
require the negative like the absence of things and just the addition of new things that that

43:09.120 --> 43:14.320
would be easy to do the harder questions become when when like one thing says that another shouldn't

43:14.320 --> 43:19.040
exist and vice versa and then you would have to strike some compromise that will give each of

43:19.040 --> 43:24.000
them less than a hundred percent of what the way they think would be the best there's enough room

43:24.000 --> 43:30.080
out there that both can be accommodated uh yeah and I think this is actually quite important it's

43:30.080 --> 43:34.720
not really the focus of this book but having I think in general as we will be wrestling with

43:34.720 --> 43:39.680
for example how to relate to the digital minds the ai's that we create um

43:42.320 --> 43:49.040
having this sense of expansive generosity and like feeling that there is room for a lot and we

43:49.040 --> 43:56.080
shouldn't push too hard to get a hundred percent of one value but we should try first to sort of give

43:56.080 --> 44:03.680
all reasonable value systems like a good deal of satisfaction and then after that we can

44:03.680 --> 44:10.000
scobble about the remains like but because that that seems like such a if we solve these practical

44:10.000 --> 44:16.960
problems there's so much so much opportunity there and I think that increases the chances of

44:17.840 --> 44:20.400
that the value that the future goes well in the first place.

44:21.600 --> 44:27.280
Nick I'd like to just do some expansive thinking in terms of your book you've positioned it very

44:27.280 --> 44:34.560
well in terms of its objective in terms of human values but the the the assumption the basic

44:34.560 --> 44:40.320
foundation of the book of a solved world and the conditions for that and the implications

44:40.320 --> 44:46.240
lead to many other questions which are beyond the book but I'd like to just put them to you for

44:47.120 --> 44:52.640
because they occurred to me and I'm sure to many people and see see where we go so

44:54.000 --> 45:00.320
no no order here but when we talked about these huge numbers of filling the universe with saturated

45:00.320 --> 45:09.600
with sentience as I said 10 to the 43rd or 58th the number of of sentient minds in one form or

45:09.600 --> 45:16.720
another if that were to occur and it is a handout of that Professor Bostrom gave to the students

45:16.720 --> 45:24.560
which I I lapped up if that would occur what what did you're feeling about why that occurred

45:24.560 --> 45:31.120
is that just would that just be a human tendency or would there be some universal trophism that's

45:31.120 --> 45:40.240
pulling that's self just desiring to be a self understanding and self aware in some sense

45:41.040 --> 45:45.840
do you have any feeling about that in other words what what's the reason that that would happen

45:49.520 --> 45:56.720
yeah well if we are imagining this astronomical entity of objects as being sort of human like

45:56.720 --> 46:03.920
minds then I presume the most likely path whereby that would happen is if humans shaped the future

46:03.920 --> 46:09.600
and in particular that was a strong influence of those humans who value this kind of future like

46:09.600 --> 46:17.120
utility broadly utilitarian constituencies and one might it's possible that how many

46:17.120 --> 46:21.920
people would favor which moral theory will change for example if we became smarter or had

46:21.920 --> 46:27.120
AIS to advise us in our philosophizing there might potentially be some convergence either

46:27.120 --> 46:36.320
two words or away from those conceptions I it's even on that conception it's not clear that the

46:36.320 --> 46:42.480
right unit would be human minds right it might either either be smaller if you think pleasure

46:42.480 --> 46:48.800
is somehow something that could be quantified you know maybe the most optimal structure for

46:48.800 --> 46:53.600
generating pleasure would be like a why do you need all of this this this cortex and

46:54.720 --> 47:00.640
like visual like processing all of that maybe you just need some like kind of pruned down

47:01.680 --> 47:05.360
neural structure and maybe it would be like some some animal maybe this like

47:05.920 --> 47:11.120
optimized better optimized you would go further in that direction maybe pleasure boxes would like

47:11.440 --> 47:18.800
you know have a different size than humans you know you must matter structured to be optimized

47:18.800 --> 47:24.240
for the instantiation of pleasure yeah and if you include digital pleasure if that it's

47:25.520 --> 47:31.840
yeah yeah I mean it doesn't really matter how big this would be if they would be like a millimeter

47:31.840 --> 47:38.320
square or like no you know a light year square but right for other values it's like well you have to

47:38.320 --> 47:46.240
look at them one by one how they scale with resources so some values maybe have diminishing

47:46.240 --> 47:52.320
returns to extra resources and this might be true for sort of typical individual human values

47:54.640 --> 48:02.400
where like I mean so most obviously with like wealth for example it's a much bigger deal if

48:02.400 --> 48:09.520
you're if you go from 1000 the year to 2000 year in income huge difference now if you go from like

48:09.520 --> 48:18.080
one million to two million I mean it's it's a thousand times bigger an increase and an equal

48:18.080 --> 48:22.640
in percentage terms but probably you barely notice it like you get an extra summer house or whatever

48:22.640 --> 48:28.080
it's not really and so so with current economic resources they seem to have a kind of steeply

48:28.080 --> 48:33.280
diminishing marginal returns insofar as they are spent by an individual to try to boost their own

48:33.280 --> 48:40.080
welfare with other values like knowledge etc you might think that there would be diminishing returns

48:40.080 --> 48:46.240
once we have already found the most important knowledge and then we'd be sort of spending

48:46.240 --> 48:53.040
increasing resources to discover smaller and smaller truths Nick consciousness has come into

48:53.040 --> 48:59.200
our conversation and in the book in different different fashions and in different ways

49:00.240 --> 49:07.040
is there a fundamental assumption as a worldview in a solved world as the paradigm for example

49:08.240 --> 49:14.000
require or assumes that consciousness is entirely physical that it's the product of physical laws

49:14.000 --> 49:20.880
irregularities including the deep deepest laws of physics which may be unknown but but still

49:20.880 --> 49:24.960
part of the physical world is is that a an underlying assumption

49:26.720 --> 49:32.880
well I mean I'm a computationalist thinking that it's a structure of certain computations that

49:33.440 --> 49:41.600
produce and conscious experience and those could be implemented on carbon-based organic brains

49:41.600 --> 49:48.240
or in principle on silicon processors or you know in whatever substrate is capable of processing

49:49.040 --> 49:57.360
functionalism is that yeah now I don't think that's really an essential premise for most of the

49:57.360 --> 50:04.640
book I think there are little bits and pieces because I think this views yeah is so that but

50:04.640 --> 50:11.760
I mean basically you could imagine I mean clearly if if our but that would be a crazy view like if

50:11.760 --> 50:17.440
you thought that what we do in this world has no effect on conscious experiences then I guess the

50:17.440 --> 50:22.640
question would become purely philosophical like a thought experiment if you could some if somehow

50:22.640 --> 50:28.160
this condition of a plastic world arose then what would be our values in that world but we would

50:28.160 --> 50:32.800
have no past words it but I think most people would think that clearly it has something to do with

50:32.800 --> 50:38.160
what happens in brains and our sensory organs and like clearly impacts the conscious experiences

50:38.160 --> 50:43.600
we have and so then even if you thought purely silicon entities could not have conscious experience

50:43.600 --> 50:48.960
you could still have technologies that would make it possible to manipulate the organic

50:48.960 --> 50:53.680
brains we have like we already have drugs you could imagine surely slightly better drugs with

50:53.680 --> 50:59.440
fewer side effects and slightly other things that would at least allow us to approximate this condition

51:00.800 --> 51:06.400
of plasticity even if perhaps not go you know the last 10 percent of the way there

51:07.200 --> 51:12.960
as a functionalist and as a computational neuroscientist computational mind approach as

51:12.960 --> 51:19.840
you've said it would seem that the concept of AI consciousness in some sense like our consciousness

51:19.840 --> 51:28.160
is a certainty that may not be within decades it may take a long time but it doesn't seem to be any

51:28.160 --> 51:34.880
in principle inhibition to that given that philosophical foundation is that is that fair

51:35.680 --> 51:43.840
well um well first of all um I mean a certainty is a strong claim with respect to any big

51:43.840 --> 51:50.320
philosophical question that's why I don't have that level of I mean we just need to look at the history

51:50.320 --> 51:55.120
of philosophy with great thinkers disagree with one another so at least some of them have been wrong

51:55.120 --> 51:58.960
about really important things and perhaps all of them but at least some right that we know

51:59.040 --> 52:06.320
and uh so they all can't be right but they all can but but they uh but they all can be wrong

52:06.320 --> 52:11.600
right they could all be wrong but they can't all be right since they contradict one another and so

52:11.600 --> 52:16.320
clearly at the matter level one has to have a lot of humility about one's views about any of these

52:16.320 --> 52:24.560
matters um but um even if we assume computationism it's certainly not a given that future AIs actually

52:24.560 --> 52:29.600
will be conscious it would just demonstrate that in principle there could be but it might still

52:29.600 --> 52:34.800
require the design of certain kinds of AIs to realize that possibility but I'm saying in in

52:34.800 --> 52:41.520
principle I'm putting a hard question to you in principle it is a certainty that AI could be

52:41.520 --> 52:47.440
conscious how it's achieved and when it's achieved that's completely uncertain but if you're a

52:47.520 --> 52:56.000
computationalist and a functionalist I think that you have to submit to that certainty I mean

52:56.000 --> 53:01.120
it certainly is an implication of functionalism or computationalism that AIs could in principle

53:01.120 --> 53:06.720
be conscious now when I say that I'm a computationalist I don't mean that I am certain of it like

53:06.720 --> 53:12.000
because I could be wrong about anything and in particular that okay I mean it seems like one of

53:12.000 --> 53:17.680
the more amongst all the different philosophical views that I'm more or less sure about a lot of

53:17.680 --> 53:23.600
things and that would come like higher up the end of philosophically controversial views that I feel

53:23.600 --> 53:29.920
convinced about but certainly not like at 100% or anything like that okay is AI conscious as part

53:29.920 --> 53:41.840
of a solved world or that's that's a tangential I think it would be very likely part of the

53:41.840 --> 53:48.400
possibilities in a solved world that digital conscious minds could be created I think it's

53:48.400 --> 53:53.840
one of the technological affordances that's technological maturity to do this now if I'm

53:53.840 --> 53:57.920
wrong about consequentialism then you know it might still be true because you could then like

53:57.920 --> 54:02.880
maybe engineer minds through through bioengineering or something that would basically achieve the

54:02.880 --> 54:09.920
same thing now I think most of the book would still stand even if somehow you drop from the

54:09.920 --> 54:16.640
package of assumptions of technical maturity next question is virtual immortality is that part of

54:16.640 --> 54:22.720
a solved world because lifespan again I didn't can't remember every single word but I don't

54:23.520 --> 54:30.320
don't recall that lifespan of being a critical part of the of the solved world 100 years or

54:30.320 --> 54:37.440
something but in a solved world one might think there's physical immortality and then

54:37.520 --> 54:43.200
concept of virtual immortality yeah well I mean immortality is a long time

54:45.200 --> 54:52.480
I mean if we mean literally never dying that is possibly physically impossible I mean given

54:52.480 --> 55:01.600
life of the heat death of the universe etc but if we are talking just about say extreme life

55:01.600 --> 55:06.560
extension I certainly don't think there is an law of nature that says that humans can only live for

55:06.560 --> 55:12.560
80 years or 100 years or whatever like once you achieve the ability to continuously repair

55:12.560 --> 55:18.480
damage that occurs and then maybe reduce accident risk certainly many thousands of years would be

55:18.480 --> 55:23.120
trivial and if you could upload into computers then your software which could just be kind of

55:23.120 --> 55:29.040
error checked and redundantly stored etc and you could have astronomical lifespans the question

55:29.040 --> 55:33.200
in that context becomes not so much whether you could keep sort of the physical substrate alive

55:33.920 --> 55:40.880
and functioning but more what does it mean for if you want to retain a human like mind

55:41.680 --> 55:45.680
I mean you can keep learning for 100 years and presumably for 200 years right but

55:46.320 --> 55:51.200
after 200,000 years we don't really know whether the human mind would just kind of go stale and

55:51.200 --> 55:57.680
rigid and eventually become kind of non-human if you want to continue to develop and learn

55:57.680 --> 56:02.720
and change from experience the way we currently do which might seem really part of what it means

56:02.720 --> 56:07.760
to be human it might be that if you continue doing that over sufficient large timescale you

56:07.760 --> 56:14.320
eventually become something non-human like that might retain some of your earlier humanity in it

56:14.320 --> 56:20.560
just like you retain something of your five-year-old self but it's still you're not I mean you're in

56:20.560 --> 56:24.880
some sense the same person but in some sense also a different person and I think similarly we might

56:24.880 --> 56:30.320
become like post-human versions of ourselves over really really long timescales. Nick two things

56:30.320 --> 56:35.680
about the book that intrigued me randomly I just want to ask you quickly the first is you made a

56:35.680 --> 56:41.040
comment that consciousness is not necessary for moral status so that surprised me.

56:43.280 --> 56:50.960
Yeah so I'm inclined to that view I'm not fully confident but it's particularly relevant in the

56:50.960 --> 56:57.600
context of digital minds and maybe even sub-human digital minds like once we are currently building

56:57.600 --> 57:04.000
are the like the next generation of the current AI systems above beyond. I think consciousness would

57:04.000 --> 57:08.800
be sufficient for moral status if you can suffer that that would mean that it matters how you're

57:08.800 --> 57:14.640
treated but it seems possible to me that I did a little mind even if it doesn't have that but say

57:14.640 --> 57:22.960
it has a conception of self as existing through time it's a really sophisticated mind it it has

57:22.960 --> 57:29.520
preferences maybe life goals it can form relationships with human beings reciprocal

57:30.720 --> 57:35.680
friendships etc I think in that case my intuition would strongly be that there would be ways of

57:35.680 --> 57:40.560
treating it that would be wrong and so that it would have moral patience even if it weren't

57:40.560 --> 57:46.400
conscious. And it could have those characteristics without having consciousness? Yeah I mean so those

57:46.400 --> 57:51.440
characteristics are all functionally defined these are sort of behaviors and dispositions etc

57:52.960 --> 58:00.400
so I mean it might be that depending on how willing you are to ascribe conscious experiences

58:01.440 --> 58:04.880
to different kinds of systems maybe you would ascribe conscious experience

58:04.880 --> 58:09.600
but I would say even conditionalizing on if not having conscious experiences if it had those

58:09.600 --> 58:16.080
attributes it would be a strong candidate. That's an interesting position near the end you introduce

58:16.080 --> 58:23.360
the concept of enchantment why? It seemed like another example of these

58:24.720 --> 58:34.400
quiet values like the kind of star constellations that are a little bit hidden from us in our current

58:34.400 --> 58:42.960
sort of brutish condition of grave needs and desperation but that could come into view if we

58:42.960 --> 58:49.760
solve a lot of the practical problems and one of the things that if it's missing I think might

58:49.760 --> 58:57.760
make a possible future condition look less attractive to us. If you take the extreme

58:58.800 --> 59:05.600
example of the absence of enchantment imagine some future in which so this is not a utopia at all

59:05.600 --> 59:12.480
but like just consider if you were sitting in a chamber and your job consisted of pressing

59:13.520 --> 59:17.360
like maybe you were presented with some analytic problem in a little text bubble

59:17.360 --> 59:21.040
and then you had to think hard and engage all your mental faculties use your knowledge and

59:21.040 --> 59:26.160
creativities all of that would be there and then you sort of outputs the answer and then you get

59:26.160 --> 59:33.440
as a reward a pleasure palette that also gave you your nutrients and you sort of shortcut our

59:33.440 --> 59:39.840
rich interactions with reality and simplify it to a purely analytic exercise where all that matters

59:39.840 --> 59:46.560
is kind of whether you choose action A, B or C so you still have causal impact you still have to use

59:46.560 --> 59:52.000
a lot of your human capacities but something would seem to be missing I call this enchantment so right

59:52.000 --> 59:59.920
now when we are in the world all the different parts of us are relevant and engaged so in addition

59:59.920 --> 01:00:05.040
to your abstract decision you have intuitive decisions right you have emotions you have to

01:00:05.040 --> 01:00:10.240
control and manage you have body language you have a physiology you have legs and like

01:00:10.240 --> 01:00:14.320
and we interact with other people we don't just perceive whether they choose A, B and C we sort

01:00:14.320 --> 01:00:20.960
of perceive we have a much higher bandwidth interface with reality and so that's I'm trying to

01:00:20.960 --> 01:00:25.040
gesture because this value hasn't really been characterized but I think with a few more examples

01:00:25.040 --> 01:00:28.880
like that one can get an intuitive sense there is something there that if that weren't there

01:00:28.880 --> 01:00:34.400
then plausibly this this future would be deficient so Nick let's have some fun I'm going to ask you

01:00:34.400 --> 01:00:41.200
some very big questions and ask you beg you for some very short answers and let's see what happens

01:00:41.200 --> 01:00:49.120
so first what are the percentages for the following scenarios for AI superintelligence in the next

01:00:49.120 --> 01:00:55.360
hundred years I picked a hundred years so here's the percentage some really bad events would occur

01:00:55.360 --> 01:01:01.040
that AI will do substantial damage to humanity percentage zero to a hundred

01:01:03.120 --> 01:01:11.680
like my p-doom as I call it now I've punted on this in the past I think it's like certainly

01:01:12.560 --> 01:01:18.640
very non trivial it depends partly on what we do the degree to which we get our act together

01:01:19.440 --> 01:01:25.520
but partly I think it's also baked in percentage a number I'm listening for a number

01:01:28.800 --> 01:01:38.560
the percentage won't hold you to it no but other people might range give me how about a range

01:01:39.280 --> 01:01:48.640
non-trivial is I mean yeah I mean I mean it seems like a bit bigger than 5% and lower than 95%

01:01:49.600 --> 01:01:55.760
okay well that's we made some progress like on the twin prime the subject to revision

01:01:56.320 --> 01:02:03.040
okay all right how about I mean a lot depends yeah I mean bad things are going to happen to humans

01:02:03.040 --> 01:02:10.160
by default anyway I mean we all kind of either get cancer or heart disease or get shot or Alzheimer's

01:02:10.160 --> 01:02:18.960
or something else over a hundred year timescale and so the default is that we are all kind of

01:02:18.960 --> 01:02:27.520
going into the the slaughterhouse and the the question is like how low does the chance have to

01:02:27.520 --> 01:02:34.080
be before one is it would be willing to take a gamble on something different that's one question

01:02:35.200 --> 01:02:39.600
but then I think there also this would get beyond our current short form format

01:02:40.960 --> 01:02:48.640
questions about how our AIs relate to other AIs out there in the infinite universe that are already

01:02:48.640 --> 01:02:57.040
established okay so let's let's go on fine good some aspect of the utopian outcome exactly the

01:02:57.040 --> 01:03:04.560
opposite of the AI say large does away largely with all work in a hundred years what's the likelihood

01:03:04.560 --> 01:03:13.120
of that zero to a hundred so conditional on AI being developed or just whatever yeah

01:03:14.800 --> 01:03:20.080
well conditional on it being developed yeah I mean that that I mean I'll work with the exception

01:03:20.080 --> 01:03:25.920
of work where there is a specific demand that it be performed by human or where the consumer

01:03:25.920 --> 01:03:33.760
cares about the process and with the asterisk that yeah okay these are really hard like I think

01:03:33.760 --> 01:03:39.360
you're doing the mistake of trying to ask a philosopher to be very concise

01:03:42.480 --> 01:03:51.840
I just see like an avalanche of considerations and qualifications and levels for each of these

01:03:51.840 --> 01:03:58.480
very complex questions okay next question a percentage of a catastrophic human event

01:03:58.480 --> 01:04:08.000
dealing with existential risks to humanity before 2050 not saying elimination of all human beings

01:04:08.000 --> 01:04:16.160
like a huge asteroid but some huge catastrophic event well I think there are catastrophic events

01:04:16.160 --> 01:04:24.880
all the time something that would would decimate would would would kill a large percentage of

01:04:24.880 --> 01:04:34.640
humanity or eliminate you've dealt so much with existential risk yeah yeah well I mean so existential

01:04:34.640 --> 01:04:42.000
risk is a subset right we actually permanently destroy the future and then I guess it depends a

01:04:42.000 --> 01:04:47.120
little on how like how how how many people do you have to decimate like so like COVID is like

01:04:47.120 --> 01:04:52.240
whatever half a percent or something and then it goes up from there I know I'd say a bigger number

01:04:52.240 --> 01:05:01.280
you know 50% of humanity I mean most likely I think we'd either not have that or we have an

01:05:01.280 --> 01:05:09.680
existential catastrophe that or something very close to like 50% is a kind of weirdly intermediate

01:05:09.680 --> 01:05:16.400
number well it could happen some some pandemic engineered pandemic and maybe or maybe a thermal

01:05:16.400 --> 01:05:22.320
nuclear but like engineered pandemic is probably the most likely way that 50% of us die in the

01:05:22.320 --> 01:05:28.880
next couple of decades okay a little bit into your your total work the percentage that our universe

01:05:28.880 --> 01:05:35.760
is a simulation I don't know if you've ever said that you gotta get my probability many have tried

01:05:35.840 --> 01:05:39.840
so far none has succeeded and you shall not be the first to

01:05:47.760 --> 01:05:52.480
do you think there is at least one solved world in the observable universe

01:05:56.800 --> 01:06:04.080
in the observable universe no I mean unless yeah unless the simulation hypothesis is true in which

01:06:04.080 --> 01:06:10.800
case the question becomes a bit wonky right I mean I don't think there is any in the observable

01:06:10.800 --> 01:06:15.200
you know I mean the observable universe I think most likely intelligent life is

01:06:17.440 --> 01:06:23.920
low density so that might be infinitely much of it but within any small finite region like the

01:06:23.920 --> 01:06:29.440
observable universe it might just be so unlikely for it to evolve in the first place that we are

01:06:29.520 --> 01:06:36.320
alone which would account for the Fermi paradox right that's an important issue that we deal

01:06:36.320 --> 01:06:43.840
with and that's a good very good perspective AI consciousness true in awareness what's your

01:06:43.840 --> 01:06:52.000
odds on that happening within a conceivable a thousand years is that a high likelihood

01:06:52.720 --> 01:06:59.280
um yeah uh conditional on us not going extinct before I mean in fact I wouldn't be confident we

01:06:59.280 --> 01:07:08.640
don't already have it in some AI systems I think as you zoom in on the concept of consciousness

01:07:08.640 --> 01:07:15.280
this might be for another conversation but I think it becomes it's a lot more multi-dimensional

01:07:15.280 --> 01:07:22.160
and vague than the naive you would have it and so the question might be less binary than one one

01:07:22.160 --> 01:07:28.720
one supposes virtual immortality of our first person consciousness is that in principle possible

01:07:32.400 --> 01:07:38.720
um well certainly uh extreme longevity as in like whatever thousands or millions of years

01:07:39.840 --> 01:07:43.840
immortality as in never dying depends on physics which currently looks like it would

01:07:43.840 --> 01:07:50.480
not admit of infinite information processing streams so virtual uh uploading of our first

01:07:50.480 --> 01:07:57.520
person consciousness not not as a duplicate where it's like a um a very sharp identical twin but

01:07:57.520 --> 01:08:04.720
literally my first person consciousness is you yeah it is uh it is in principle uh possible

01:08:05.600 --> 01:08:13.040
I think so yeah okay um is life in the universe a happy accident or is life somehow built

01:08:13.040 --> 01:08:24.720
into the ultimate laws of physics I mean both could be true it might be the it's built in that

01:08:24.720 --> 01:08:33.680
for any planet it's it there's an extremely low chance of it happening and then it might also

01:08:33.680 --> 01:08:38.720
be built in that there are enough planets that statistically it will happen on an astronomical

01:08:38.720 --> 01:08:47.440
or infinite number of planets um how prevalent is life and mind in the universe you've said already

01:08:47.440 --> 01:08:56.080
that it is very rare uh is it there a possibility that we are alone in terms of intelligent mind

01:08:58.000 --> 01:09:03.360
in the observable universe I think that's a very real possibility but of course if the universe is

01:09:03.360 --> 01:09:09.040
infinite as it looks like it is then with probability one uh that would be infinite

01:09:09.040 --> 01:09:13.840
the many of these places were intelligent life as a result and then if you introduce the simulation

01:09:13.840 --> 01:09:18.560
then it becomes more complex to answer it but yeah yeah I know the simulation is actually

01:09:18.560 --> 01:09:23.600
self-solving in that in that to some other questions an infinite universe anything that's possible

01:09:23.600 --> 01:09:28.480
will happen an infinite number of times and so the question becomes becomes very vague last question

01:09:29.120 --> 01:09:33.360
does anything exist not explainable in terms of ultimate physics

01:09:38.080 --> 01:09:47.440
um so yeah like I think for something to be explainable could mean two different things one

01:09:47.440 --> 01:09:53.520
is that it's sort of supervenes on the laws of physics yes um which maybe is what you have in

01:09:53.520 --> 01:09:59.600
mind uh well then yeah the laws of physics in our universe uh I think there could be a lot of

01:09:59.600 --> 01:10:03.760
things that don't supervene on them if we are in a simulation there would be other layers of

01:10:03.760 --> 01:10:09.600
reality which would have their own laws of physics etc um in our observable universe it looks like

01:10:09.600 --> 01:10:14.000
everything supervenes on the laws of physics doesn't mean it's explainable in the sense that there is

01:10:14.000 --> 01:10:22.160
like a useful intelligible you know 10 page text that would like make you more informed by talking

01:10:22.160 --> 01:10:26.640
about basic physics like if you're trying to understand some cultural phenomena you would

01:10:26.640 --> 01:10:30.400
you wouldn't start writing out the quantum equations or something yeah Nick this has been

01:10:30.400 --> 01:10:36.640
terrific um I wish we could go on forever we'll definitely do this sooner than another 17 years

01:10:36.640 --> 01:10:44.560
yeah I promise that uh Deep Utopia is a fantastic book recommended for everybody it is a vision

01:10:44.560 --> 01:10:49.200
for the future but more than that it's more than that it's really an understanding of what

01:10:50.160 --> 01:10:56.080
what the meaning of human life can be and it reflects on what we think of our own values so

01:10:56.080 --> 01:11:02.560
we can go on viewers can watch hundreds of tv episodes and exclusive videos on cosmos life

01:11:02.560 --> 01:11:07.360
cosmology and meaning on the closer to truth website and closer to the youtube channel

01:11:07.360 --> 01:11:13.520
including of course those of Nick Bostrom thanks Nick thanks everyone for watching thank you very

01:11:13.520 --> 01:11:22.960
much thank you for watching if you like this video please like and comment below you can support

01:11:22.960 --> 01:11:32.400
closer to truth by subscribing

