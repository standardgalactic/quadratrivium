1
00:00:00,000 --> 00:00:06,880
I think you're doing the mistake of trying to ask a philosopher to be very concise.

2
00:00:06,880 --> 00:00:16,760
I just see like an avalanche of considerations and qualifications and levels for each of these very complex questions.

3
00:00:16,760 --> 00:00:17,760
Okay.

4
00:00:26,040 --> 00:00:28,120
Welcome to Closer to Truth.

5
00:00:28,120 --> 00:00:34,960
I'm speaking with futurist visionary Nick Bostrom about his vital, far-sighted, engaging new book,

6
00:00:34,960 --> 00:00:38,880
Deep Utopia, Life and Meaning in a Solved World.

7
00:00:38,880 --> 00:00:40,080
I loved it.

8
00:00:40,080 --> 00:00:46,560
It's an exhilarating romp of ultimate technology centered on AI, how it might work, what it could mean.

9
00:00:46,560 --> 00:00:49,200
It's a prescient manual for the future.

10
00:00:49,200 --> 00:00:52,680
It's an innovative treatise on the meaning of life.

11
00:00:52,680 --> 00:00:54,840
Welcome, Nick. It's great to see you again.

12
00:00:54,840 --> 00:00:56,320
Good to see you again.

13
00:00:56,320 --> 00:00:58,360
Congrats on Deep Utopia.

14
00:00:58,360 --> 00:00:59,640
We'll discuss it in depth.

15
00:00:59,640 --> 00:01:03,560
But to begin, I'd like to get your world overview, the setting for your book.

16
00:01:03,560 --> 00:01:09,960
When we first did our Closer to Truth discussion in Oxford in 2007, 17 years ago,

17
00:01:09,960 --> 00:01:15,400
we discussed the simulation argument, fine-tuning, anthropic selection, the doomsday argument.

18
00:01:15,400 --> 00:01:22,600
How would you characterize the last 17 years or so in terms of technological and intellectual development,

19
00:01:22,600 --> 00:01:26,280
especially the importance of AI?

20
00:01:26,360 --> 00:01:32,320
Well, I mean, it's all happening now, as we've entered the Atollius era.

21
00:01:32,320 --> 00:01:39,320
I think the last, especially since 2012-2014, with the Deep Learning Revolution,

22
00:01:39,320 --> 00:01:44,320
I think we've been on a kind of up-ramp of AI capabilities,

23
00:01:44,320 --> 00:01:49,360
kicking into even higher gear with the release of ChatGPT.

24
00:01:49,360 --> 00:01:53,880
And in the last two years or so, we've really seen it hit the mainstream.

25
00:01:53,920 --> 00:01:57,560
We're now the White House and key policy makers all over the world

26
00:01:57,560 --> 00:02:00,360
are starting to debate the future of AI.

27
00:02:00,360 --> 00:02:03,240
So it's a remarkable time.

28
00:02:03,240 --> 00:02:05,840
And when did you actually plan this book?

29
00:02:05,840 --> 00:02:11,600
Because it's obviously, in essence, the other side of your superintelligence,

30
00:02:11,600 --> 00:02:16,440
2014, where you were prescient in warning about the dangers of AI.

31
00:02:16,440 --> 00:02:20,680
And so the last two years has been a high focus on the dangers.

32
00:02:20,680 --> 00:02:23,840
Now you've moved on to the opportunities.

33
00:02:23,840 --> 00:02:29,640
So when did you have that a bit of a transformal insight?

34
00:02:32,320 --> 00:02:35,920
I've been working on it for probably around six years or so.

35
00:02:35,920 --> 00:02:39,240
It wasn't ever planned.

36
00:02:39,240 --> 00:02:42,000
It just kind of happened.

37
00:02:43,240 --> 00:02:47,640
I didn't start out with like some particular set of theses.

38
00:02:47,640 --> 00:02:51,040
I wanted to defend and elaborate on.

39
00:02:51,040 --> 00:02:57,520
I felt an urge to start writing, and then it eventually grew into DP Topia.

40
00:02:57,520 --> 00:03:00,160
Yeah, and I've seen where you've said that the style of the book,

41
00:03:00,160 --> 00:03:08,640
which is very unusual, it's a new literary style involving dialogue

42
00:03:08,640 --> 00:03:20,320
with different characters, your own persona in a not entirely actual form.

43
00:03:20,320 --> 00:03:29,640
But in dialogue with other people and form of a lecture series over a period of a week,

44
00:03:29,640 --> 00:03:34,160
I think you've said that whole structure wasn't planned.

45
00:03:34,160 --> 00:03:36,360
It happened organically.

46
00:03:36,440 --> 00:03:41,160
Yeah, it's just the way it happened for better or worse.

47
00:03:41,160 --> 00:03:48,520
But that's I do think the form actually does match the content.

48
00:03:48,520 --> 00:03:54,680
It's not a book so much about conclusions as it is a book about questions

49
00:03:54,680 --> 00:03:59,840
and helping the reader to start to think about these problems

50
00:03:59,840 --> 00:04:01,920
and form their own views ultimately.

51
00:04:02,000 --> 00:04:08,160
It's also meant to be not just something that transmits certain concepts and ideas,

52
00:04:08,160 --> 00:04:15,040
but also it's meant to be a reading experience that you might have to work to get through it.

53
00:04:15,040 --> 00:04:19,800
But ultimately, I'm hoping it will kind of put you in a better place

54
00:04:19,800 --> 00:04:25,160
to reflect on questions about what art humanity's destiny be.

55
00:04:25,160 --> 00:04:27,280
I think that's an accurate description.

56
00:04:27,280 --> 00:04:28,960
I found myself very engaged.

57
00:04:28,960 --> 00:04:37,080
I was looking for more of the arguments as that we've had in the past in a very positive sense.

58
00:04:37,080 --> 00:04:38,200
But this book is different.

59
00:04:38,200 --> 00:04:43,360
You do get the arc of various arguments on different things.

60
00:04:43,360 --> 00:04:44,600
We'll talk about that.

61
00:04:44,600 --> 00:04:54,480
But you are brought into that in this engaging intellectual, semi-fictional avatar environment.

62
00:04:54,480 --> 00:04:56,240
Yeah, yeah.

63
00:04:56,240 --> 00:05:03,360
And the other benefit of this sort of having different characters and different bits

64
00:05:03,360 --> 00:05:10,560
is that it makes it easier to explore several different viewpoints,

65
00:05:10,560 --> 00:05:16,560
which I wanted to do and allow each one to be developed in its own right to its fullest extent

66
00:05:16,560 --> 00:05:21,440
and then to kind of collide different perspectives and ideas.

67
00:05:21,440 --> 00:05:24,080
Just as, I mean, you're interested in physics, right?

68
00:05:24,080 --> 00:05:30,160
So with a particle accelerator, you sort of accelerate little particles to enormous energies

69
00:05:30,160 --> 00:05:31,840
and then smash them into one another.

70
00:05:31,840 --> 00:05:37,520
And in those extreme conditions, sometimes you can see the basic principles at work

71
00:05:37,520 --> 00:05:42,520
that we can then infer are at work all the time in ordinary conditions as well.

72
00:05:42,520 --> 00:05:44,240
It's just hard to observe.

73
00:05:44,240 --> 00:05:53,200
And so similarly, this conceit of a plastic world,

74
00:05:53,200 --> 00:05:56,920
a condition in which technology has reached its full maturity

75
00:05:56,920 --> 00:05:59,160
and all practical problems have been solved.

76
00:05:59,160 --> 00:06:01,160
It's an extreme condition.

77
00:06:01,160 --> 00:06:06,120
But I think we can then see values kind of smashing into one another

78
00:06:06,120 --> 00:06:11,080
that we normally can sort of hand wave and then just because they are obscured

79
00:06:11,080 --> 00:06:16,680
by so many practical necessities that kind of occupy most of our contemporary existence.

80
00:06:16,680 --> 00:06:18,840
I think that's a very good characterization.

81
00:06:18,840 --> 00:06:23,720
The book is creating an extreme condition and particle accelerators do that

82
00:06:23,720 --> 00:06:25,800
and physics black holes do that in physics.

83
00:06:25,800 --> 00:06:31,080
That's an extreme condition where people study black holes.

84
00:06:31,080 --> 00:06:32,920
It's not just for the black holes themselves,

85
00:06:32,920 --> 00:06:36,080
but it's subjecting the laws of physics to extreme conditions.

86
00:06:36,080 --> 00:06:37,400
And you learn a lot.

87
00:06:37,400 --> 00:06:39,800
And I think that's a very good characterization of the book.

88
00:06:41,480 --> 00:06:41,960
Yeah.

89
00:06:41,960 --> 00:06:48,880
So I think it's like, I mean, for people who have read it, they will know,

90
00:06:48,880 --> 00:06:54,440
but it's not a book that is really trying to make predictions.

91
00:06:54,440 --> 00:07:00,200
And nor is it trying to offer practical solutions to what we should do next.

92
00:07:00,200 --> 00:07:02,440
I mean, a lot of my other work focuses on that.

93
00:07:02,440 --> 00:07:03,360
Right.

94
00:07:03,360 --> 00:07:10,240
This takes basically as an assumption or a postulate, if you want, that things go well

95
00:07:10,240 --> 00:07:14,560
in order then to be able to ask the questions of what then,

96
00:07:15,920 --> 00:07:19,520
what would be the meaning of human existence?

97
00:07:19,520 --> 00:07:21,040
What would give us purpose in life?

98
00:07:21,760 --> 00:07:25,840
If the whole thing unfolds like everything is perfect,

99
00:07:25,840 --> 00:07:28,560
governance problems is all the alignment problem is solved,

100
00:07:28,560 --> 00:07:32,400
like all these things, but what then would occupy our lives?

101
00:07:33,440 --> 00:07:36,640
Sometimes you never actually get to even ask that question

102
00:07:36,640 --> 00:07:39,760
because there are so many other questions that kind of crowd in before it.

103
00:07:39,760 --> 00:07:43,360
So I just wanted to postulate that and then focus this book entirely

104
00:07:43,360 --> 00:07:47,680
on the set of questions that arise in this hypothetical future condition.

105
00:07:47,680 --> 00:07:51,360
We're going to get into all of it, but let me first give a more formal bio.

106
00:07:51,920 --> 00:07:54,400
Nick Bostrom is a professor at Oxford University,

107
00:07:54,400 --> 00:07:58,400
where he heads the Future of Humanity Institute as its founding director.

108
00:07:59,120 --> 00:08:02,880
With a background in theoretical physics, computational neuroscience,

109
00:08:02,880 --> 00:08:07,440
logic and artificial intelligence, Nick has pioneered contemporary thinking

110
00:08:07,440 --> 00:08:10,320
about existential risk, the simulation argument,

111
00:08:10,320 --> 00:08:12,880
and the vulnerable world hypothesis, among others.

112
00:08:13,520 --> 00:08:17,920
He is the most cited professional philosopher in the world age 50 or under

113
00:08:17,920 --> 00:08:22,560
and is the author of some 200 publications, including anthropic bias,

114
00:08:22,560 --> 00:08:26,480
global catastrophic risks, human enhancement and superintelligence,

115
00:08:26,480 --> 00:08:29,360
the prescient book on the dangers of AI,

116
00:08:29,360 --> 00:08:33,600
but now we're going to look at the extreme condition if all goes well.

117
00:08:33,680 --> 00:08:35,040
So Nick, your book,

118
00:08:35,040 --> 00:08:38,240
Deep Utopia, Life and Meaning in a Solved World.

119
00:08:38,240 --> 00:08:42,880
Let's start with a simple definition of what is a solved world

120
00:08:42,880 --> 00:08:45,040
and what motivates your focus on it.

121
00:08:46,560 --> 00:08:51,040
Well, I am referring to a hypothetical condition

122
00:08:51,040 --> 00:08:55,360
where basically all practical problems have been solved.

123
00:08:55,360 --> 00:08:59,600
So think, first of all, a condition of technical maturity.

124
00:08:59,600 --> 00:09:01,680
So we have super advanced AIs.

125
00:09:02,240 --> 00:09:05,040
Maybe they have helped us develop all kinds of other technologies,

126
00:09:05,040 --> 00:09:08,880
medical technologies, virtual reality, et cetera, et cetera.

127
00:09:08,880 --> 00:09:13,360
So that's part of what it would mean for the world to be solved.

128
00:09:14,000 --> 00:09:17,760
And then on top of that, we also make the assumption that

129
00:09:18,720 --> 00:09:22,800
all the kind of governance problems of the world have been solved

130
00:09:22,800 --> 00:09:24,320
to the extent that they can be solved.

131
00:09:24,320 --> 00:09:26,800
So we imagine we set the site,

132
00:09:26,880 --> 00:09:30,160
questions of war and conflict and oppression and inequality

133
00:09:30,160 --> 00:09:32,080
and all the rest of it.

134
00:09:32,080 --> 00:09:35,440
So but then there remains a big kind of problem,

135
00:09:35,440 --> 00:09:38,640
which is ultimately a problem of value,

136
00:09:38,640 --> 00:09:41,520
which is that under these ideal conditions,

137
00:09:43,280 --> 00:09:47,040
what kind of lives would we want to live?

138
00:09:49,040 --> 00:09:50,000
And yeah, that's...

139
00:09:50,880 --> 00:09:54,640
And that's a very important way to frame the book

140
00:09:54,640 --> 00:09:56,880
because you're not saying all of these problems

141
00:09:58,080 --> 00:10:01,200
that are solved are easy to solve or will be solved.

142
00:10:01,200 --> 00:10:04,160
But if you do solve it, what does that leave?

143
00:10:04,160 --> 00:10:05,200
And it leaves value.

144
00:10:05,200 --> 00:10:06,880
So one question that I have is,

145
00:10:06,880 --> 00:10:09,520
do you distinguish between meaning and purpose?

146
00:10:09,520 --> 00:10:12,640
We use those two terms sometimes interchangeably,

147
00:10:12,640 --> 00:10:16,400
but I think we can tease apart a difference.

148
00:10:17,280 --> 00:10:18,400
Meaning in the title,

149
00:10:18,400 --> 00:10:20,400
but throughout the book, you have purpose as well.

150
00:10:22,240 --> 00:10:22,960
Yeah, that's right.

151
00:10:22,960 --> 00:10:27,840
So I think of purpose as slightly narrower concept

152
00:10:27,840 --> 00:10:32,080
as sort of having a reason for doing something

153
00:10:32,080 --> 00:10:33,520
or for putting out some effort.

154
00:10:35,200 --> 00:10:38,800
And then meaning, I mean, that is discussed in the book,

155
00:10:38,800 --> 00:10:41,280
but might be a certain kind of purpose

156
00:10:41,280 --> 00:10:43,280
or purpose plus something else.

157
00:10:43,280 --> 00:10:43,600
Okay.

158
00:10:45,680 --> 00:10:48,640
You present, just to give a sense of the environment,

159
00:10:49,360 --> 00:10:56,400
utopic taxonomy where you have different levels of utopia

160
00:10:56,400 --> 00:10:59,360
that can give us a richer understanding of it.

161
00:10:59,920 --> 00:11:02,560
So let me just give you the list

162
00:11:02,560 --> 00:11:04,320
and just explain each one very quickly.

163
00:11:04,320 --> 00:11:06,480
The first is government and cultural utopia.

164
00:11:08,880 --> 00:11:14,640
Yeah, this is I think the most familiar kind of utopia

165
00:11:14,640 --> 00:11:15,920
we find in the literature,

166
00:11:16,800 --> 00:11:20,640
where people imagine a better way for society to be organized,

167
00:11:21,280 --> 00:11:24,480
better political institutions, different schools,

168
00:11:25,280 --> 00:11:27,200
maybe different gender roles,

169
00:11:27,200 --> 00:11:30,160
but usually set within more or less

170
00:11:30,160 --> 00:11:33,440
recognizably contemporary technological context.

171
00:11:33,440 --> 00:11:36,640
So people, there's still work that needs to be done

172
00:11:36,640 --> 00:11:39,760
and you can organize how much power the workers have

173
00:11:39,760 --> 00:11:41,200
or how the work is divided,

174
00:11:41,200 --> 00:11:44,880
but there have to be people growing food, et cetera.

175
00:11:44,880 --> 00:11:49,440
So that's the most familiar and basic kind of utopia.

176
00:11:49,440 --> 00:11:51,920
The second level is post-scarcity utopia.

177
00:11:52,480 --> 00:11:53,600
Sounds like we know what it means,

178
00:11:53,600 --> 00:11:56,080
but if you could define it more clearly.

179
00:11:56,080 --> 00:12:02,480
Yeah, so this is the more radical vision of a condition

180
00:12:02,480 --> 00:12:07,920
in which humans have plenty of all that we need materially.

181
00:12:08,960 --> 00:12:11,200
So there are these kind of,

182
00:12:11,200 --> 00:12:13,680
it's more like fantasy in the past,

183
00:12:13,680 --> 00:12:20,400
but various kind of the land of cocaine

184
00:12:20,400 --> 00:12:26,880
was this medieval peasant dream basically

185
00:12:26,880 --> 00:12:30,880
of some condition where the rivers would overflow

186
00:12:30,880 --> 00:12:35,440
with wine and roasted turkeys would just land on the plate

187
00:12:35,440 --> 00:12:38,560
and that would be a kind of continuous feasting.

188
00:12:39,280 --> 00:12:41,760
And you could easily see how that on its own

189
00:12:42,320 --> 00:12:43,440
would have huge attraction

190
00:12:43,520 --> 00:12:47,440
if you were a kind of agricultural labor

191
00:12:47,440 --> 00:12:50,000
who spent your whole life grinding away,

192
00:12:50,640 --> 00:12:53,840
barely getting enough porridge to feed your children

193
00:12:53,840 --> 00:12:55,440
and your joints were aching

194
00:12:55,440 --> 00:12:57,760
from all this backbreaking work that you were doing.

195
00:12:57,760 --> 00:12:59,440
Then this on itself,

196
00:12:59,440 --> 00:13:02,400
just being able to rest and eat as much as you want

197
00:13:02,400 --> 00:13:06,800
would already be like enough of a kind of vision.

198
00:13:08,000 --> 00:13:10,480
Third level is post-work utopia.

199
00:13:11,440 --> 00:13:13,600
Right, so this is the idea that not just

200
00:13:13,600 --> 00:13:16,800
is there plenty to consume,

201
00:13:16,800 --> 00:13:20,800
but that the production of all this plenty

202
00:13:20,800 --> 00:13:23,920
doesn't require human economic labor.

203
00:13:25,520 --> 00:13:28,800
And this has started popping up more recently

204
00:13:28,800 --> 00:13:30,800
in conversations about the future of AI

205
00:13:31,840 --> 00:13:33,680
where people are wondering,

206
00:13:33,680 --> 00:13:37,920
will this advance that we see lead to human unemployment?

207
00:13:37,920 --> 00:13:39,440
We can automate more and more things.

208
00:13:41,440 --> 00:13:43,600
And if you imagine that running its full course,

209
00:13:43,600 --> 00:13:46,240
then maybe eventually you could automate

210
00:13:46,240 --> 00:13:49,040
almost all human economic labor

211
00:13:49,040 --> 00:13:52,000
and then you would have this post-work condition.

212
00:13:52,720 --> 00:13:56,480
The fourth and fifth to get a little more complicated

213
00:13:56,480 --> 00:13:58,320
to understand, but let's do it now.

214
00:13:58,960 --> 00:14:01,680
Fourth is post-instrumental utopia.

215
00:14:02,480 --> 00:14:05,840
So now we're getting into a more radical conception.

216
00:14:07,920 --> 00:14:10,640
And usually current conversations about these issues

217
00:14:10,640 --> 00:14:14,000
stop short before we reach this idea.

218
00:14:14,640 --> 00:14:16,960
But if you really think through what it would mean

219
00:14:17,760 --> 00:14:21,120
for AI to attain its full potential

220
00:14:21,680 --> 00:14:23,840
and then all the other technological advances

221
00:14:23,840 --> 00:14:27,280
that this kind of machine superintelligence

222
00:14:27,280 --> 00:14:28,000
could bring about,

223
00:14:29,600 --> 00:14:32,480
it's not just that we wouldn't have to go into the office

224
00:14:32,480 --> 00:14:35,600
and type on word processors or like hammer away

225
00:14:35,600 --> 00:14:38,320
at construction sites, et cetera.

226
00:14:39,440 --> 00:14:42,560
But also a lot of the other things we need to do

227
00:14:43,360 --> 00:14:46,400
in our daily lives could be automated as well.

228
00:14:48,560 --> 00:14:50,880
So if you think about, if you didn't have to work,

229
00:14:50,880 --> 00:14:53,600
well, then like typical answers would be, well,

230
00:14:55,280 --> 00:14:57,680
maybe somebody likes fitness or something,

231
00:14:57,680 --> 00:14:59,840
so they could spend more time exercising.

232
00:14:59,840 --> 00:15:01,200
But like in this condition,

233
00:15:01,200 --> 00:15:02,800
you could pop a pill instead

234
00:15:02,800 --> 00:15:06,560
and get the same physiological and psychological effects

235
00:15:06,560 --> 00:15:11,680
that spending an hour on the Stairmaster would provide.

236
00:15:11,680 --> 00:15:15,360
And so then why would you really need to go to,

237
00:15:15,360 --> 00:15:16,640
kind of would lose its point

238
00:15:16,640 --> 00:15:18,720
to go to the gym in those conditions?

239
00:15:18,720 --> 00:15:23,200
And you can then start to kind of almost do case studies

240
00:15:23,200 --> 00:15:26,160
on activities that fill our current lives.

241
00:15:26,160 --> 00:15:27,360
And for almost all of them,

242
00:15:27,360 --> 00:15:30,400
you soon see that they have a certain structure,

243
00:15:30,400 --> 00:15:32,000
which is that we do a certain thing,

244
00:15:32,000 --> 00:15:35,680
put in some effort in order then to achieve some other thing.

245
00:15:36,480 --> 00:15:40,320
So brush your teeth, because otherwise eventually

246
00:15:40,320 --> 00:15:42,480
you will have tooth decay and gum decay.

247
00:15:42,480 --> 00:15:46,240
And so in order to get the outcome of a healthy mouth,

248
00:15:46,240 --> 00:15:49,520
you need to spend a few minutes every day brushing your teeth

249
00:15:49,520 --> 00:15:50,480
and gone to the gym.

250
00:15:52,320 --> 00:15:55,680
Or you need to, like you want to understand mathematics, let's say.

251
00:15:55,680 --> 00:15:57,200
So then the only way to do that

252
00:15:57,200 --> 00:15:59,440
is to put in some effort to study mathematics.

253
00:16:00,400 --> 00:16:03,920
And so the effort is motivated by the goal

254
00:16:03,920 --> 00:16:06,400
that it is trying to achieve outside the effort itself.

255
00:16:07,680 --> 00:16:11,840
And a lot of the things that we are doing has that structure.

256
00:16:11,840 --> 00:16:14,400
But now if you could get the goal, the end point,

257
00:16:14,400 --> 00:16:16,000
without having to put in the effort,

258
00:16:16,960 --> 00:16:20,880
then it seems to pull out the rug under the activity itself.

259
00:16:21,840 --> 00:16:24,160
At least it's threatened with the sense of being pointless.

260
00:16:25,920 --> 00:16:27,520
So that's the problem you confront

261
00:16:27,600 --> 00:16:29,520
in this kind of post-instrumental case.

262
00:16:29,520 --> 00:16:30,640
So an example of that,

263
00:16:30,640 --> 00:16:32,640
instead of studying higher mathematics,

264
00:16:32,640 --> 00:16:34,640
you could have an injection of nanobots

265
00:16:35,840 --> 00:16:38,720
that could analyze every synapse in your brain

266
00:16:38,720 --> 00:16:42,240
and then figure out how to change them a little bit

267
00:16:42,240 --> 00:16:46,320
so I can understand algebraic geometry or something.

268
00:16:46,320 --> 00:16:47,600
Right, that's right.

269
00:16:47,600 --> 00:16:49,920
And that would be fast and effortless.

270
00:16:50,960 --> 00:16:53,600
Or even things we do for fun.

271
00:16:54,560 --> 00:16:56,240
So there might be various activities

272
00:16:57,200 --> 00:16:58,800
because it gives you pleasure and joy.

273
00:17:00,400 --> 00:17:02,160
That seems kind of unavoidable.

274
00:17:02,160 --> 00:17:03,840
But even there, if you think about it,

275
00:17:03,840 --> 00:17:07,440
you could have more direct ways of experiencing

276
00:17:07,440 --> 00:17:09,360
the same positive emotions,

277
00:17:09,360 --> 00:17:12,080
like a kind of some super drugs

278
00:17:12,800 --> 00:17:14,960
that could give you the pleasure and the joy

279
00:17:14,960 --> 00:17:18,000
without you having to spend an hour gardening

280
00:17:18,000 --> 00:17:22,560
or doing whatever it is that you're watching movies.

281
00:17:23,040 --> 00:17:26,160
So this brings up a very important point of the book

282
00:17:26,960 --> 00:17:30,080
in terms of what are our real values.

283
00:17:30,080 --> 00:17:32,880
Because when I read that,

284
00:17:32,880 --> 00:17:35,120
and obviously a very intriguing,

285
00:17:35,120 --> 00:17:38,240
remarkable way of thinking and very important,

286
00:17:39,280 --> 00:17:40,320
I was asking myself,

287
00:17:40,320 --> 00:17:43,600
are there any absolute values in a solved world?

288
00:17:43,600 --> 00:17:45,200
And so the way to describe it,

289
00:17:45,200 --> 00:17:46,240
as you started to say,

290
00:17:46,240 --> 00:17:49,280
is if we could take non-harmful drugs

291
00:17:49,280 --> 00:17:51,760
or AI neural implants

292
00:17:51,760 --> 00:17:55,280
that would maintain a state of perpetual ecstasy,

293
00:17:55,280 --> 00:17:59,120
whatever your ecstasy would happen to be,

294
00:17:59,120 --> 00:18:03,520
or it can switch from a physical bodily ecstasy

295
00:18:03,520 --> 00:18:06,720
to an artistic ecstasy or intellectual one.

296
00:18:06,720 --> 00:18:08,000
And if that could be all done,

297
00:18:10,640 --> 00:18:12,320
who could gain say that

298
00:18:12,320 --> 00:18:16,640
if there's no kind of supernatural value

299
00:18:16,640 --> 00:18:18,720
that you would put into it?

300
00:18:19,680 --> 00:18:23,120
I think that is a fundamental theme of your book

301
00:18:23,120 --> 00:18:26,160
about how do we develop the kinds of values

302
00:18:26,160 --> 00:18:27,440
if these things are possible.

303
00:18:27,440 --> 00:18:31,200
You're not saying these things are remotely

304
00:18:31,200 --> 00:18:34,560
in the near or mid or even long term,

305
00:18:34,560 --> 00:18:36,320
but they are the extreme condition

306
00:18:36,320 --> 00:18:39,680
that you talked about which then exposes

307
00:18:39,680 --> 00:18:43,120
what is the nature of absolute values if there are any.

308
00:18:44,480 --> 00:18:44,800
Right.

309
00:18:45,760 --> 00:18:49,600
Now, it is possible that they might be in the long term

310
00:18:49,600 --> 00:18:51,040
or even mid or near term,

311
00:18:51,040 --> 00:18:53,760
depending on how fast the AI revolution unfolds

312
00:18:54,400 --> 00:18:55,760
and what the outcome of that is.

313
00:18:55,760 --> 00:18:59,280
I actually think the time scales for radical transformation

314
00:18:59,280 --> 00:19:01,920
might be shorter than most people realize

315
00:19:01,920 --> 00:19:05,920
if AI continues to speed ahead.

316
00:19:05,920 --> 00:19:08,000
What's your best guess on that?

317
00:19:08,560 --> 00:19:08,880
Well,

318
00:19:09,280 --> 00:19:15,760
I mean, my timelines have shortened somewhat,

319
00:19:15,760 --> 00:19:18,640
at least since this previous book,

320
00:19:18,640 --> 00:19:21,360
Superintelligence, was published in 2014.

321
00:19:22,480 --> 00:19:24,800
I think we are currently looking on timelines

322
00:19:24,800 --> 00:19:27,840
that are on the shorter end of the distribution.

323
00:19:29,120 --> 00:19:30,080
So it's hard to say,

324
00:19:30,080 --> 00:19:35,360
but I mean, it could be years or maybe a decade or perhaps more,

325
00:19:35,360 --> 00:19:38,320
but at least I think there's a non-trivial probability

326
00:19:38,320 --> 00:19:42,640
that we are now kind of on the accelerating slope of this,

327
00:19:42,640 --> 00:19:43,760
but time will tell.

328
00:19:44,960 --> 00:19:46,720
In any way, that's not central to the book,

329
00:19:46,720 --> 00:19:48,400
even if you thought this would take millions

330
00:19:48,400 --> 00:19:49,120
or never happen.

331
00:19:49,120 --> 00:19:49,680
It is still.

332
00:19:50,560 --> 00:19:52,320
But there are two ways of thinking about this.

333
00:19:52,320 --> 00:19:53,600
You could either just read it as,

334
00:19:54,800 --> 00:19:56,960
these are perpetual philosophical questions

335
00:19:56,960 --> 00:19:58,080
that humanity ponderous,

336
00:19:58,080 --> 00:19:59,840
and here is kind of a thought experiment

337
00:19:59,840 --> 00:20:01,040
that helps you think about them.

338
00:20:01,040 --> 00:20:04,880
And I think it's fine if you just read it like that.

339
00:20:04,880 --> 00:20:07,200
For me, there is also this actual real possibility

340
00:20:07,200 --> 00:20:09,920
that we might soon enter a condition like this,

341
00:20:09,920 --> 00:20:12,240
or we might have to make decisions soon

342
00:20:13,280 --> 00:20:14,640
about what kind of future we want,

343
00:20:14,640 --> 00:20:16,720
if we want to stare towards something like this

344
00:20:16,720 --> 00:20:17,920
or some other version of this.

345
00:20:18,960 --> 00:20:22,720
So there is this kind of underlying practical motivation

346
00:20:22,720 --> 00:20:25,360
for me in terms of writing this book,

347
00:20:25,360 --> 00:20:27,200
but that's optional.

348
00:20:29,040 --> 00:20:30,160
So yeah.

349
00:20:30,880 --> 00:20:31,760
I appreciate that.

350
00:20:31,760 --> 00:20:34,480
I was more on the former, on the thought experiment,

351
00:20:35,280 --> 00:20:38,960
and I would put the date measured in hundreds of years,

352
00:20:38,960 --> 00:20:42,160
if not thousands, to achieve what you're saying,

353
00:20:42,160 --> 00:20:44,400
but I'm cautious.

354
00:20:44,400 --> 00:20:45,760
I've become cautious as you,

355
00:20:46,720 --> 00:20:49,040
and I hear you and respect your views,

356
00:20:49,040 --> 00:20:51,840
so I'm a little less sure of what I thought before.

357
00:20:51,840 --> 00:20:55,760
Anyway, we want to get the fifth category

358
00:20:55,760 --> 00:20:58,960
of the utopic taxonomy,

359
00:20:58,960 --> 00:21:01,200
which you call plastic utopia.

360
00:21:01,600 --> 00:21:04,720
Even going beyond the post-instrumental utopia,

361
00:21:05,360 --> 00:21:06,960
what is the plastic utopia?

362
00:21:08,720 --> 00:21:10,480
Well, we alluded to it slightly,

363
00:21:10,480 --> 00:21:13,280
which is the idea that in addition to

364
00:21:14,160 --> 00:21:16,800
having these other properties of being post-instrumental,

365
00:21:17,440 --> 00:21:20,880
we also, in the plastic utopia condition,

366
00:21:20,880 --> 00:21:24,000
have complete control over ourselves.

367
00:21:24,560 --> 00:21:26,800
So our mental states, our psychology,

368
00:21:26,800 --> 00:21:29,840
our cognitive architecture, our bodies becomes malleable.

369
00:21:31,360 --> 00:21:33,440
So it's not just that we imagine human beings,

370
00:21:33,440 --> 00:21:35,520
as we are now, placed in this condition

371
00:21:35,520 --> 00:21:36,800
where we don't have to work

372
00:21:36,800 --> 00:21:38,880
and where we don't have to put out any effort

373
00:21:38,880 --> 00:21:39,680
if we don't want to.

374
00:21:39,680 --> 00:21:42,000
We could just press buttons and get what we want,

375
00:21:42,000 --> 00:21:45,680
but we ourselves, as well, become something we can choose.

376
00:21:46,480 --> 00:21:49,040
So you wouldn't have to work on yourself

377
00:21:50,000 --> 00:21:52,720
to build a better character in a plastic utopia.

378
00:21:52,720 --> 00:21:54,480
If you wanted to, instead,

379
00:21:54,480 --> 00:21:57,280
you could sort of request of your AIG

380
00:21:57,360 --> 00:22:00,800
need to sort of rewire your synopsis

381
00:22:00,800 --> 00:22:03,120
so that you became this different kind of person

382
00:22:03,120 --> 00:22:05,040
or to experience pleasure all the time

383
00:22:05,040 --> 00:22:08,000
or to become smarter or kinder or whatever else.

384
00:22:09,280 --> 00:22:15,120
So that makes it even more like solved or dissolved or liquid.

385
00:22:15,120 --> 00:22:17,920
Like you enter this context

386
00:22:17,920 --> 00:22:20,880
where everything seems kind of fluid and up for grabs,

387
00:22:20,880 --> 00:22:24,080
and it's hard to find on this firm ground to stand on.

388
00:22:25,040 --> 00:22:28,720
Yeah, it's a wonderful way

389
00:22:28,720 --> 00:22:32,080
of seeing what an extreme condition is,

390
00:22:33,040 --> 00:22:35,600
because I would have not come up with those five.

391
00:22:35,600 --> 00:22:36,960
I might have had three or four,

392
00:22:38,000 --> 00:22:40,240
but at that level,

393
00:22:40,240 --> 00:22:43,520
it really very beautifully defines

394
00:22:44,080 --> 00:22:47,440
what an extreme condition for humanity could be in the future,

395
00:22:47,440 --> 00:22:50,640
and therefore it gives the book its real punch.

396
00:22:50,640 --> 00:22:52,480
So one issue that you deal with

397
00:22:54,080 --> 00:22:56,160
especially as you get to all of those

398
00:22:56,160 --> 00:22:57,680
is the question of boredom

399
00:22:59,120 --> 00:23:02,320
and how much of our value system is based upon

400
00:23:03,280 --> 00:23:08,960
the need to or the lack of control and the uncertainty

401
00:23:08,960 --> 00:23:10,320
and what happens whenever this,

402
00:23:10,320 --> 00:23:12,080
you know, this is the same kind of problem

403
00:23:12,080 --> 00:23:14,480
that traditional religions,

404
00:23:14,480 --> 00:23:16,880
both East and West have to deal with

405
00:23:16,880 --> 00:23:19,280
whether you're dealing with nirvana

406
00:23:19,280 --> 00:23:23,520
after the innumerable cycles of birth and death and rebirth,

407
00:23:23,520 --> 00:23:28,400
and then you reach nirvana or in the Judeo-Christian,

408
00:23:28,400 --> 00:23:31,920
Islamic, Abrahamic concept of heaven,

409
00:23:31,920 --> 00:23:33,120
eternal life in heaven.

410
00:23:33,120 --> 00:23:38,240
I mean, that's a sort of an end question of boredom

411
00:23:38,240 --> 00:23:42,160
that occurs in any of these eschatological ideas.

412
00:23:43,600 --> 00:23:45,440
Yeah, it's quite fascinating

413
00:23:45,440 --> 00:23:47,040
if you think far enough in this direction,

414
00:23:47,040 --> 00:23:51,360
you do start to sort of bottom up against theological questions

415
00:23:51,440 --> 00:23:54,480
or at least questions that have traditionally been

416
00:23:55,760 --> 00:23:59,280
discussed in religious contexts about the afterlife, etc.

417
00:24:00,400 --> 00:24:06,000
I try to not trespass onto that terrain in the book.

418
00:24:06,000 --> 00:24:09,200
I have this, there's this other fictional character,

419
00:24:09,200 --> 00:24:11,840
the fictional Bostrom character is giving these lectures

420
00:24:11,840 --> 00:24:15,040
and then sometimes he's asked questions by the students

421
00:24:15,040 --> 00:24:17,440
and occasionally he sort of refers to,

422
00:24:17,440 --> 00:24:20,640
well, you have to take that up with Professor Grossweiter

423
00:24:20,640 --> 00:24:22,240
which is like another character

424
00:24:22,240 --> 00:24:23,840
who doesn't make an appearance in the book

425
00:24:23,840 --> 00:24:25,840
but he's like the theologian

426
00:24:25,840 --> 00:24:28,560
or the person who could answer their questions on that.

427
00:24:30,560 --> 00:24:32,960
But on boredom, yeah, so this is an example

428
00:24:32,960 --> 00:24:35,040
where I think it's important to distinguish

429
00:24:36,960 --> 00:24:39,040
the two different senses of boredom

430
00:24:39,040 --> 00:24:42,320
which is a subjective sense and an objective sense.

431
00:24:42,320 --> 00:24:45,280
So clearly we have a subjective concept of boredom

432
00:24:45,280 --> 00:24:47,040
like somebody might just feel bored

433
00:24:47,520 --> 00:24:53,520
and that would trivially be easy to abolish in Utopia.

434
00:24:53,520 --> 00:24:56,720
I mean, it follows directly from the condition of plasticity

435
00:24:56,720 --> 00:25:00,720
that this feeling is like a subjective state of your brain.

436
00:25:00,720 --> 00:25:05,040
You could rewire that so that you would always feel excited

437
00:25:05,040 --> 00:25:09,520
or interested or whatever antonyms to boredom you want to use.

438
00:25:09,520 --> 00:25:14,560
And the question then is whether there is also

439
00:25:15,520 --> 00:25:21,120
some notion of objective boredom or boringness

440
00:25:22,320 --> 00:25:25,440
whether certain activities or experiences

441
00:25:25,440 --> 00:25:27,520
are such that they are objectively boring

442
00:25:27,520 --> 00:25:30,000
like meaning perhaps that it would be appropriate

443
00:25:30,000 --> 00:25:33,920
to feel subjectively bored if we engaged in them.

444
00:25:34,960 --> 00:25:38,880
So if you imagine somebody to take an example

445
00:25:38,880 --> 00:25:42,800
actually from the philosophical literature of a grass counter

446
00:25:42,800 --> 00:25:47,280
so somebody who spends his life counting the blades of grass

447
00:25:47,280 --> 00:25:50,880
on a particular college lawn, we might think

448
00:25:52,320 --> 00:25:55,520
that that's an activity that is objectively boring

449
00:25:55,520 --> 00:25:57,840
whether or not he happens to feel excited about it

450
00:25:57,840 --> 00:26:00,800
it's not appropriate to be really interested in grass counting

451
00:26:00,800 --> 00:26:03,600
because it's like too monotonous or insignificant

452
00:26:03,600 --> 00:26:06,240
or has some other sort of deficit.

453
00:26:08,400 --> 00:26:11,440
And philosophers disagree about whether like

454
00:26:12,400 --> 00:26:15,600
there is like some kind of firm normative basis for making that

455
00:26:15,600 --> 00:26:16,880
but I think it's an intuition that

456
00:26:19,600 --> 00:26:21,520
some but not all people would have that

457
00:26:21,520 --> 00:26:25,680
it would be bad if the future consisted of merely of activities

458
00:26:25,680 --> 00:26:26,800
like counting grass.

459
00:26:26,800 --> 00:26:30,080
No matter how thrilled the people doing the grass counting were.

460
00:26:31,280 --> 00:26:36,320
That's an objective value that's kind of is a superset

461
00:26:36,320 --> 00:26:37,920
to everything else we're talking about.

462
00:26:38,480 --> 00:26:39,440
Yeah potentially.

463
00:26:39,520 --> 00:26:42,800
And so I'm able to in a plastic world change my brain

464
00:26:42,800 --> 00:26:47,600
to where I am excited about every new grass that I count

465
00:26:47,600 --> 00:26:49,520
and what's going to happen at the next number

466
00:26:49,520 --> 00:26:51,360
and I'm genuinely excited about that

467
00:26:51,360 --> 00:26:55,760
and I've changed my brain to think and so subjectively

468
00:26:55,760 --> 00:27:00,400
I'm excited about life counting all these grasses

469
00:27:00,400 --> 00:27:03,920
or as I think you have in the book table legs

470
00:27:04,640 --> 00:27:06,640
200 and somewhat thousand table legs

471
00:27:06,640 --> 00:27:13,360
or that that potentially is in some objective sense

472
00:27:13,920 --> 00:27:14,960
is suboptimal.

473
00:27:16,080 --> 00:27:18,560
Yeah that's possible to hold that view and

474
00:27:21,280 --> 00:27:25,680
now if one does have that view that it would be objectively

475
00:27:26,640 --> 00:27:32,080
bad to have nothing in one's life than you know counting grass

476
00:27:32,800 --> 00:27:37,360
then that kind of also potentially imposes a constraint

477
00:27:37,360 --> 00:27:39,440
on the subjective experience of boredom.

478
00:27:39,440 --> 00:27:42,000
Like if it actually is objectively bad to do that

479
00:27:42,000 --> 00:27:44,880
then you might think it would be also bad to change

480
00:27:44,880 --> 00:27:47,760
your subjective attitude so that you found great interest

481
00:27:47,760 --> 00:27:49,360
is something that would be boring.

482
00:27:49,360 --> 00:27:51,520
So then you might end up with a situation

483
00:27:51,520 --> 00:27:54,320
where you couldn't eliminate boredom in the future

484
00:27:54,320 --> 00:27:56,160
neither in the objective or objective sense.

485
00:27:57,680 --> 00:28:01,360
And so then but so this possibility of the objective value

486
00:28:02,080 --> 00:28:04,720
there makes the discussion more complex.

487
00:28:04,720 --> 00:28:08,000
Like eliminating the subjective if that's all there is trivial

488
00:28:08,000 --> 00:28:10,880
given the postulates but once they introduce this possibility

489
00:28:10,880 --> 00:28:12,880
of the objective then it becomes like a much more

490
00:28:12,880 --> 00:28:16,480
intricate conversation to what extent we would be able to do

491
00:28:16,480 --> 00:28:19,200
something without violating that.

492
00:28:19,200 --> 00:28:24,240
Now I think at least with respect to the value of

493
00:28:24,240 --> 00:28:27,520
interestingness and there is a bunch of these different

494
00:28:27,520 --> 00:28:29,600
values that they're kind of related but different

495
00:28:29,600 --> 00:28:33,280
but if we focus on interestingness I think there is at least

496
00:28:34,480 --> 00:28:38,800
large scope before increasing the amount of subjective

497
00:28:38,800 --> 00:28:42,480
and objective interestingness including in these utopian lives.

498
00:28:42,480 --> 00:28:45,920
I think even if there is some objective element to what's

499
00:28:45,920 --> 00:28:50,080
boring and what's interesting I think it's has a large

500
00:28:50,080 --> 00:28:52,080
zone of indeterminacy.

501
00:28:52,080 --> 00:28:56,640
I mean you can just look at the current human distribution.

502
00:28:56,640 --> 00:29:02,640
I have a good friend and colleague who tells me he's never bored

503
00:29:02,640 --> 00:29:07,760
and he's interested in as far as I can tell literally everything except sport.

504
00:29:08,960 --> 00:29:11,680
He writes papers on all kinds of topics.

505
00:29:11,680 --> 00:29:12,880
He knows about everything.

506
00:29:12,880 --> 00:29:15,520
He goes to every kind of conference and finds

507
00:29:15,520 --> 00:29:17,360
interesting things to discuss with every person.

508
00:29:17,360 --> 00:29:22,640
Like it doesn't seem to me that there is anything deficient about his human life.

509
00:29:23,600 --> 00:29:32,000
In fact if anything it seems to benefit and be like a greater person for this

510
00:29:32,000 --> 00:29:36,640
this property that he has and obviously that goes down ultimately to some neurochemical

511
00:29:36,640 --> 00:29:43,520
idiosyncrasies of his brain that's he and I think for all of us I think we could expand

512
00:29:43,520 --> 00:29:50,080
the range of things in which we take an interest greatly before we would reach

513
00:29:50,080 --> 00:29:52,560
this point where we would just be counting leaves of grass.

514
00:29:53,600 --> 00:29:59,840
Moreover I think possibly it would be appropriate to expand it even further than that.

515
00:29:59,840 --> 00:30:06,720
Like maybe if we have reached a condition where we had sort of exhausted all the

516
00:30:06,720 --> 00:30:11,600
most obviously interesting things we had discovered all the really fundamental loss of

517
00:30:13,120 --> 00:30:18,640
nature you know solved consciousness and like the biggest questions had all been answered.

518
00:30:18,720 --> 00:30:21,840
Like it would seem perfectly appropriate in that situation to begin to take an

519
00:30:21,840 --> 00:30:26,720
interest in the slightly smaller questions and it's not clear that one couldn't go very

520
00:30:26,720 --> 00:30:32,480
very far in that direction before I reached a point where it would be sort of objectively

521
00:30:32,480 --> 00:30:34,960
bad to take a further interest.

522
00:30:34,960 --> 00:30:37,600
Is religion relevant in a solved world?

523
00:30:39,680 --> 00:30:46,560
Yes potentially very relevant although it's also arguably very relevant in the current world

524
00:30:47,120 --> 00:30:54,400
and one might say something more interesting perhaps if one looks at some other values that

525
00:30:54,400 --> 00:31:00,080
seem not so relevant in the current world but that could potentially become more relevant

526
00:31:00,080 --> 00:31:00,880
in this whole world.

527
00:31:00,880 --> 00:31:09,920
I think that there might be a lot of subtle values that exist now but we don't really

528
00:31:09,920 --> 00:31:16,080
see them very much just as we don't see the stars during daytime because it's like

529
00:31:17,040 --> 00:31:24,320
you know such a like brighter present and analogously there are such stark moral

530
00:31:24,320 --> 00:31:25,520
imperatives right now.

531
00:31:27,920 --> 00:31:33,360
Calamities of all sorts you need to take care of your kids you there are people starving in

532
00:31:33,360 --> 00:31:39,680
the world or being shot at etc etc so so many horrors and an urgent obvious pressing

533
00:31:40,880 --> 00:31:47,520
ethical needs to fix things that it would almost be frivolous now to spend too much time

534
00:31:49,040 --> 00:31:55,280
fretting about more subtle quiet values but if we ever reached a condition where these

535
00:31:55,280 --> 00:32:02,400
pressing needs were taken care of then I think we might be able to see a whole panoply

536
00:32:03,120 --> 00:32:05,440
of these subtler values like for example

537
00:32:05,840 --> 00:32:10,800
various kinds of traditions that it would be nice to honor authentically

538
00:32:12,720 --> 00:32:19,840
ancestors who you know maybe we think of our lost parents once in a while but there is like so

539
00:32:19,840 --> 00:32:25,040
many more people have lived wonderful lives and you know maybe deserve more thought and

540
00:32:25,040 --> 00:32:29,360
consideration we don't have time our daily lives keep us busy but if you didn't have that

541
00:32:29,600 --> 00:32:35,440
but if you didn't have that why not various aesthetic qualities you could imagine making

542
00:32:35,440 --> 00:32:41,440
your life more into a kind of artwork where every relationship was not just a source of

543
00:32:41,440 --> 00:32:45,600
I don't know relaxation or final satisfaction but also something actually beautiful that you

544
00:32:45,600 --> 00:32:51,280
were kind of constructing together etc etc and we don't now have the luxury to kind of really

545
00:32:51,280 --> 00:32:55,360
develop a fine sensibility for those but I think it would be entirely appropriate

546
00:32:56,000 --> 00:33:02,000
that once the urgencies are removed to to sort of tune up like almost like our eyes

547
00:33:02,000 --> 00:33:06,640
dilate at night right so it can take in more light similar in this condition our moral

548
00:33:06,640 --> 00:33:11,600
sensibilities and sensibilities for subtle values I think should dilate and this is a

549
00:33:11,600 --> 00:33:19,440
larger definition of religion as we might have it in today's world enabled by the the solved world

550
00:33:20,160 --> 00:33:29,920
the boundaries of religion become broader yeah potentially but again it also potentially

551
00:33:29,920 --> 00:33:36,320
is very important today so it might be one of those things that is very urgent today even

552
00:33:36,320 --> 00:33:42,480
with other urgencies pressing in upon us like many religious people would say that yes you have all

553
00:33:42,480 --> 00:33:46,560
these practical things you should do but you should also set the time for worship etc even

554
00:33:46,560 --> 00:33:53,360
though it conflicts with but but even more so obviously in this condition and I mean we would

555
00:33:53,360 --> 00:33:58,400
be more like I guess potentially like monks and nuns that have the time to fully devote themselves

556
00:33:59,120 --> 00:34:04,160
to contemplating the divine when I first heard of the book and started it

557
00:34:04,960 --> 00:34:10,080
the first question one of the first questions came into my mind is how does a solved world the

558
00:34:10,080 --> 00:34:17,760
the bostrom solved world articulate with the marxist pure communism and as I started to go

559
00:34:17,760 --> 00:34:26,000
through the book to me that question became pretty obvious that the the that there would be a high

560
00:34:26,000 --> 00:34:35,040
correlation potentially between the first at least two of the utopia taxonomy levels the

561
00:34:35,040 --> 00:34:40,720
government and culture and then the post scarcity and then maybe into the post work as well but

562
00:34:41,600 --> 00:34:47,040
pure communism as it's been envisioned in the past or even in in few cases in the present

563
00:34:47,920 --> 00:34:55,040
doesn't even deal with the points four and five right and and I mean I think I'm not really a

564
00:34:55,040 --> 00:35:00,800
marx scholar but I think he just has a few lines really about what would you know ultimately be

565
00:35:00,800 --> 00:35:06,240
the outcome of if the whole communist product succeeded and I think he refers to this whatever

566
00:35:06,240 --> 00:35:12,160
is it like fishing in the morning and hunting in the afternoon and reading poetry in the evening

567
00:35:12,160 --> 00:35:18,480
so that that sounds like not even a fully post work utopia but like maybe an abundance diminished

568
00:35:18,480 --> 00:35:25,520
work utopia plus a sort of vision of social cultural utopia I guess right right Nick let's

569
00:35:25,600 --> 00:35:31,520
switch and look really long term and very visionary what I call your approach to ultimate

570
00:35:31,520 --> 00:35:39,040
utilitarianism I love the section a quantitative analysis of the potential for happiness or

571
00:35:39,040 --> 00:35:47,120
fulfillment for all sentient beings if if the cosmic endowment could be maximally saturated

572
00:35:47,120 --> 00:35:54,000
with sentience so some of the numbers you give you estimate 10 to the 35th possible human lives

573
00:35:54,000 --> 00:35:59,440
derived from human lives originating on earth to populate the observable universe that's a

574
00:35:59,440 --> 00:36:04,720
think a hundred billion trillion trillion that's your minimum then you go up to 10 to the 43rd and

575
00:36:04,720 --> 00:36:12,560
then if you switch to digital lives which adds a lot of complex value you get a computing power

576
00:36:12,560 --> 00:36:17,920
of the universe of at least 10 to the 58th which is 10 billion trillion trillion trillion four

577
00:36:17,920 --> 00:36:27,360
trillions there in terms of ultimate sentience so what I love the calculation but walk me through

578
00:36:28,000 --> 00:36:35,920
the importance of that in our ultimate thinking and also in terms of the concept of meaning

579
00:36:35,920 --> 00:36:44,480
and purpose which is the purpose of your book well I mean it's like some big number basically

580
00:36:44,480 --> 00:36:49,600
very big number but I mean it's in the context of the book it's a little handout

581
00:36:49,600 --> 00:37:01,280
as the postroom gives out but yeah so I'm not a utilitarian I'm often mistaken for one because

582
00:37:02,160 --> 00:37:10,160
in some of my writings I have explored and analyzed the implications of assuming

583
00:37:10,160 --> 00:37:17,280
an assumption of utilitarianism or aggregative consequentialism because it's a view that you know

584
00:37:18,880 --> 00:37:25,120
significant fraction of moral philosophers have held and that I think maybe deserves at least

585
00:37:25,120 --> 00:37:29,280
some weight even if one doesn't actually embrace it and then it's interesting to see what follows

586
00:37:29,280 --> 00:37:36,480
if one actually takes that perspective seriously and hence in my earlier work this you know the

587
00:37:36,560 --> 00:37:41,040
focus on existential risks as those few things that could actually permanently

588
00:37:41,600 --> 00:37:48,400
destroy our future if one counts these possible future lives the same way as actually currently

589
00:37:48,400 --> 00:37:55,200
existing lives as certain flavors of utilitarianism would do then they just seem to dominate and you

590
00:37:55,200 --> 00:37:59,680
get a bunch of interesting and then there's like a further complication on that which is if you

591
00:38:00,160 --> 00:38:07,280
literally do this try to do this expected utility calculation you find that scenarios in which somehow

592
00:38:07,280 --> 00:38:12,000
even if they are very unlikely but somehow infinite values could be realized like maybe we

593
00:38:12,000 --> 00:38:15,680
are wrong about physics and there's like some actual way of producing infinite and then those

594
00:38:15,680 --> 00:38:20,480
tend to dominate even if they have a very very tiny finite and then you get into infinitarian

595
00:38:20,480 --> 00:38:24,480
paralysis and there's like a whole yeah so I think that's interesting in its own right but it's not

596
00:38:24,480 --> 00:38:33,920
really the topic of the book which more focuses on not not how you aggregate big values or what

597
00:38:33,920 --> 00:38:38,640
our obligations are but like from our point of view like what would be the best possible future

598
00:38:39,200 --> 00:38:46,080
for Robert or for the for you the the viewer or for any of us like if you literally could

599
00:38:46,080 --> 00:38:51,840
imagine the best possible way for your future life to unfold and perhaps we restricted by like

600
00:38:51,840 --> 00:38:58,080
the loss of physics etc but and then trying to think from the inside like how we would furnish

601
00:38:58,080 --> 00:39:06,720
that life with activity experiences relationships etc and then ultimately like you know if your

602
00:39:06,720 --> 00:39:11,120
question is what you should you do now as a moral actor then you would have to somehow integrate all

603
00:39:11,120 --> 00:39:18,320
these different perspectives whatever weight you would put on utilitarian views or the ontological

604
00:39:18,320 --> 00:39:26,080
views or virtue ethics views and the bunch of other stuff but the yeah the book doesn't really try to

605
00:39:27,440 --> 00:39:34,000
pick between these different moral theories it doesn't really in general focus so much on numbers

606
00:39:34,000 --> 00:39:40,480
or on formal structures and aggregation but more tried to sort of which often is done in

607
00:39:40,480 --> 00:39:45,840
contemporary analytic ethics it kind of almost sees the values a little bit like

608
00:39:47,760 --> 00:39:52,160
black pork and that might not be exactly right but this ties to look from the inside

609
00:39:52,880 --> 00:39:55,840
on the values which values do you actually have like at the object level

610
00:39:57,120 --> 00:40:02,080
and what would it take to realize them I'm not sure whether that answers your question but

611
00:40:03,040 --> 00:40:11,600
now there is this yeah I guess I guess like one one way in which this larger view of

612
00:40:12,320 --> 00:40:19,120
the bigness of the future could and does come into the book is insofar as we value

613
00:40:20,240 --> 00:40:25,120
significance like having significant impact on the world for example if that's the version of

614
00:40:25,120 --> 00:40:33,280
significance right now it looks like we have we are extraordinarily well positioned to have

615
00:40:34,000 --> 00:40:41,280
huge impact on the world because well a there's a lot of just ordinary needs in the world and you as

616
00:40:41,280 --> 00:40:48,080
an you know if you know if we imagine you're like a relatively well-off person with health and

617
00:40:48,080 --> 00:40:52,240
intelligence in a wealthy country with a good education like probably most of your viewers are

618
00:40:52,960 --> 00:40:56,800
you have a lot of opportunity just to help a bunch of people and to try to make some positive

619
00:40:56,800 --> 00:41:02,400
difference so that already gives your life potential significance that is maybe greater

620
00:41:02,400 --> 00:41:06,080
than one human life's worth of significance like you could save many people's lives or

621
00:41:06,640 --> 00:41:11,840
but then on top of that you have this idea that maybe we are near a big fulcrum of human history

622
00:41:11,840 --> 00:41:16,720
where if this whole thing the AI transition and the rest of it is going to happen perhaps within

623
00:41:16,720 --> 00:41:21,920
our lifetime then like you can multiply that manifold like if you could even slightly notch

624
00:41:21,920 --> 00:41:26,480
this big future in the right direction that would give your life even more causal significance

625
00:41:28,080 --> 00:41:32,000
this is one thing that might be a lot harder for people living in utopia to have

626
00:41:32,960 --> 00:41:36,080
this kind of significance because if all the problems are already solved

627
00:41:36,720 --> 00:41:41,520
and whatever problems aren't yet solved are anyway much better work that by AIs

628
00:41:42,400 --> 00:41:47,600
then humans might not be able to have significance in that sense and so to the extent that one

629
00:41:47,600 --> 00:41:56,960
thinks it makes a life itself better to have this kind of significance these utopian lives might

630
00:41:56,960 --> 00:42:00,560
lack that significance and therefore have a deficit of that particular value

631
00:42:01,680 --> 00:42:08,240
and so there's a discussion around that and also the possibility of humans whether through AI

632
00:42:09,200 --> 00:42:18,880
colonizing or filling the universe with sentience is a you know gigantic grand vision

633
00:42:20,640 --> 00:42:27,840
um yeah I mean if that's if that's the way one wants to go I mean I mean I actually

634
00:42:27,840 --> 00:42:32,160
happen to think the future is big enough that you could not just realize one vision but many

635
00:42:32,880 --> 00:42:39,600
not every vision because some are directly in conflict but if some people think doing

636
00:42:39,600 --> 00:42:46,160
something nice for existing people and working locally is the most important thing we could

637
00:42:46,160 --> 00:42:52,080
certainly do that and then also that leaves all the rest of the universe and that's big enough

638
00:42:52,080 --> 00:42:59,200
that you could have sort of AI paradise in one sector and you know animal uplift in another

639
00:42:59,200 --> 00:43:04,000
sector and you could have a whole bunch of different to the extent that a vision doesn't

640
00:43:04,000 --> 00:43:09,120
require the negative like the absence of things and just the addition of new things that that

641
00:43:09,120 --> 00:43:14,320
would be easy to do the harder questions become when when like one thing says that another shouldn't

642
00:43:14,320 --> 00:43:19,040
exist and vice versa and then you would have to strike some compromise that will give each of

643
00:43:19,040 --> 00:43:24,000
them less than a hundred percent of what the way they think would be the best there's enough room

644
00:43:24,000 --> 00:43:30,080
out there that both can be accommodated uh yeah and I think this is actually quite important it's

645
00:43:30,080 --> 00:43:34,720
not really the focus of this book but having I think in general as we will be wrestling with

646
00:43:34,720 --> 00:43:39,680
for example how to relate to the digital minds the ai's that we create um

647
00:43:42,320 --> 00:43:49,040
having this sense of expansive generosity and like feeling that there is room for a lot and we

648
00:43:49,040 --> 00:43:56,080
shouldn't push too hard to get a hundred percent of one value but we should try first to sort of give

649
00:43:56,080 --> 00:44:03,680
all reasonable value systems like a good deal of satisfaction and then after that we can

650
00:44:03,680 --> 00:44:10,000
scobble about the remains like but because that that seems like such a if we solve these practical

651
00:44:10,000 --> 00:44:16,960
problems there's so much so much opportunity there and I think that increases the chances of

652
00:44:17,840 --> 00:44:20,400
that the value that the future goes well in the first place.

653
00:44:21,600 --> 00:44:27,280
Nick I'd like to just do some expansive thinking in terms of your book you've positioned it very

654
00:44:27,280 --> 00:44:34,560
well in terms of its objective in terms of human values but the the the assumption the basic

655
00:44:34,560 --> 00:44:40,320
foundation of the book of a solved world and the conditions for that and the implications

656
00:44:40,320 --> 00:44:46,240
lead to many other questions which are beyond the book but I'd like to just put them to you for

657
00:44:47,120 --> 00:44:52,640
because they occurred to me and I'm sure to many people and see see where we go so

658
00:44:54,000 --> 00:45:00,320
no no order here but when we talked about these huge numbers of filling the universe with saturated

659
00:45:00,320 --> 00:45:09,600
with sentience as I said 10 to the 43rd or 58th the number of of sentient minds in one form or

660
00:45:09,600 --> 00:45:16,720
another if that were to occur and it is a handout of that Professor Bostrom gave to the students

661
00:45:16,720 --> 00:45:24,560
which I I lapped up if that would occur what what did you're feeling about why that occurred

662
00:45:24,560 --> 00:45:31,120
is that just would that just be a human tendency or would there be some universal trophism that's

663
00:45:31,120 --> 00:45:40,240
pulling that's self just desiring to be a self understanding and self aware in some sense

664
00:45:41,040 --> 00:45:45,840
do you have any feeling about that in other words what what's the reason that that would happen

665
00:45:49,520 --> 00:45:56,720
yeah well if we are imagining this astronomical entity of objects as being sort of human like

666
00:45:56,720 --> 00:46:03,920
minds then I presume the most likely path whereby that would happen is if humans shaped the future

667
00:46:03,920 --> 00:46:09,600
and in particular that was a strong influence of those humans who value this kind of future like

668
00:46:09,600 --> 00:46:17,120
utility broadly utilitarian constituencies and one might it's possible that how many

669
00:46:17,120 --> 00:46:21,920
people would favor which moral theory will change for example if we became smarter or had

670
00:46:21,920 --> 00:46:27,120
AIS to advise us in our philosophizing there might potentially be some convergence either

671
00:46:27,120 --> 00:46:36,320
two words or away from those conceptions I it's even on that conception it's not clear that the

672
00:46:36,320 --> 00:46:42,480
right unit would be human minds right it might either either be smaller if you think pleasure

673
00:46:42,480 --> 00:46:48,800
is somehow something that could be quantified you know maybe the most optimal structure for

674
00:46:48,800 --> 00:46:53,600
generating pleasure would be like a why do you need all of this this this cortex and

675
00:46:54,720 --> 00:47:00,640
like visual like processing all of that maybe you just need some like kind of pruned down

676
00:47:01,680 --> 00:47:05,360
neural structure and maybe it would be like some some animal maybe this like

677
00:47:05,920 --> 00:47:11,120
optimized better optimized you would go further in that direction maybe pleasure boxes would like

678
00:47:11,440 --> 00:47:18,800
you know have a different size than humans you know you must matter structured to be optimized

679
00:47:18,800 --> 00:47:24,240
for the instantiation of pleasure yeah and if you include digital pleasure if that it's

680
00:47:25,520 --> 00:47:31,840
yeah yeah I mean it doesn't really matter how big this would be if they would be like a millimeter

681
00:47:31,840 --> 00:47:38,320
square or like no you know a light year square but right for other values it's like well you have to

682
00:47:38,320 --> 00:47:46,240
look at them one by one how they scale with resources so some values maybe have diminishing

683
00:47:46,240 --> 00:47:52,320
returns to extra resources and this might be true for sort of typical individual human values

684
00:47:54,640 --> 00:48:02,400
where like I mean so most obviously with like wealth for example it's a much bigger deal if

685
00:48:02,400 --> 00:48:09,520
you're if you go from 1000 the year to 2000 year in income huge difference now if you go from like

686
00:48:09,520 --> 00:48:18,080
one million to two million I mean it's it's a thousand times bigger an increase and an equal

687
00:48:18,080 --> 00:48:22,640
in percentage terms but probably you barely notice it like you get an extra summer house or whatever

688
00:48:22,640 --> 00:48:28,080
it's not really and so so with current economic resources they seem to have a kind of steeply

689
00:48:28,080 --> 00:48:33,280
diminishing marginal returns insofar as they are spent by an individual to try to boost their own

690
00:48:33,280 --> 00:48:40,080
welfare with other values like knowledge etc you might think that there would be diminishing returns

691
00:48:40,080 --> 00:48:46,240
once we have already found the most important knowledge and then we'd be sort of spending

692
00:48:46,240 --> 00:48:53,040
increasing resources to discover smaller and smaller truths Nick consciousness has come into

693
00:48:53,040 --> 00:48:59,200
our conversation and in the book in different different fashions and in different ways

694
00:49:00,240 --> 00:49:07,040
is there a fundamental assumption as a worldview in a solved world as the paradigm for example

695
00:49:08,240 --> 00:49:14,000
require or assumes that consciousness is entirely physical that it's the product of physical laws

696
00:49:14,000 --> 00:49:20,880
irregularities including the deep deepest laws of physics which may be unknown but but still

697
00:49:20,880 --> 00:49:24,960
part of the physical world is is that a an underlying assumption

698
00:49:26,720 --> 00:49:32,880
well I mean I'm a computationalist thinking that it's a structure of certain computations that

699
00:49:33,440 --> 00:49:41,600
produce and conscious experience and those could be implemented on carbon-based organic brains

700
00:49:41,600 --> 00:49:48,240
or in principle on silicon processors or you know in whatever substrate is capable of processing

701
00:49:49,040 --> 00:49:57,360
functionalism is that yeah now I don't think that's really an essential premise for most of the

702
00:49:57,360 --> 00:50:04,640
book I think there are little bits and pieces because I think this views yeah is so that but

703
00:50:04,640 --> 00:50:11,760
I mean basically you could imagine I mean clearly if if our but that would be a crazy view like if

704
00:50:11,760 --> 00:50:17,440
you thought that what we do in this world has no effect on conscious experiences then I guess the

705
00:50:17,440 --> 00:50:22,640
question would become purely philosophical like a thought experiment if you could some if somehow

706
00:50:22,640 --> 00:50:28,160
this condition of a plastic world arose then what would be our values in that world but we would

707
00:50:28,160 --> 00:50:32,800
have no past words it but I think most people would think that clearly it has something to do with

708
00:50:32,800 --> 00:50:38,160
what happens in brains and our sensory organs and like clearly impacts the conscious experiences

709
00:50:38,160 --> 00:50:43,600
we have and so then even if you thought purely silicon entities could not have conscious experience

710
00:50:43,600 --> 00:50:48,960
you could still have technologies that would make it possible to manipulate the organic

711
00:50:48,960 --> 00:50:53,680
brains we have like we already have drugs you could imagine surely slightly better drugs with

712
00:50:53,680 --> 00:50:59,440
fewer side effects and slightly other things that would at least allow us to approximate this condition

713
00:51:00,800 --> 00:51:06,400
of plasticity even if perhaps not go you know the last 10 percent of the way there

714
00:51:07,200 --> 00:51:12,960
as a functionalist and as a computational neuroscientist computational mind approach as

715
00:51:12,960 --> 00:51:19,840
you've said it would seem that the concept of AI consciousness in some sense like our consciousness

716
00:51:19,840 --> 00:51:28,160
is a certainty that may not be within decades it may take a long time but it doesn't seem to be any

717
00:51:28,160 --> 00:51:34,880
in principle inhibition to that given that philosophical foundation is that is that fair

718
00:51:35,680 --> 00:51:43,840
well um well first of all um I mean a certainty is a strong claim with respect to any big

719
00:51:43,840 --> 00:51:50,320
philosophical question that's why I don't have that level of I mean we just need to look at the history

720
00:51:50,320 --> 00:51:55,120
of philosophy with great thinkers disagree with one another so at least some of them have been wrong

721
00:51:55,120 --> 00:51:58,960
about really important things and perhaps all of them but at least some right that we know

722
00:51:59,040 --> 00:52:06,320
and uh so they all can't be right but they all can but but they uh but they all can be wrong

723
00:52:06,320 --> 00:52:11,600
right they could all be wrong but they can't all be right since they contradict one another and so

724
00:52:11,600 --> 00:52:16,320
clearly at the matter level one has to have a lot of humility about one's views about any of these

725
00:52:16,320 --> 00:52:24,560
matters um but um even if we assume computationism it's certainly not a given that future AIs actually

726
00:52:24,560 --> 00:52:29,600
will be conscious it would just demonstrate that in principle there could be but it might still

727
00:52:29,600 --> 00:52:34,800
require the design of certain kinds of AIs to realize that possibility but I'm saying in in

728
00:52:34,800 --> 00:52:41,520
principle I'm putting a hard question to you in principle it is a certainty that AI could be

729
00:52:41,520 --> 00:52:47,440
conscious how it's achieved and when it's achieved that's completely uncertain but if you're a

730
00:52:47,520 --> 00:52:56,000
computationalist and a functionalist I think that you have to submit to that certainty I mean

731
00:52:56,000 --> 00:53:01,120
it certainly is an implication of functionalism or computationalism that AIs could in principle

732
00:53:01,120 --> 00:53:06,720
be conscious now when I say that I'm a computationalist I don't mean that I am certain of it like

733
00:53:06,720 --> 00:53:12,000
because I could be wrong about anything and in particular that okay I mean it seems like one of

734
00:53:12,000 --> 00:53:17,680
the more amongst all the different philosophical views that I'm more or less sure about a lot of

735
00:53:17,680 --> 00:53:23,600
things and that would come like higher up the end of philosophically controversial views that I feel

736
00:53:23,600 --> 00:53:29,920
convinced about but certainly not like at 100% or anything like that okay is AI conscious as part

737
00:53:29,920 --> 00:53:41,840
of a solved world or that's that's a tangential I think it would be very likely part of the

738
00:53:41,840 --> 00:53:48,400
possibilities in a solved world that digital conscious minds could be created I think it's

739
00:53:48,400 --> 00:53:53,840
one of the technological affordances that's technological maturity to do this now if I'm

740
00:53:53,840 --> 00:53:57,920
wrong about consequentialism then you know it might still be true because you could then like

741
00:53:57,920 --> 00:54:02,880
maybe engineer minds through through bioengineering or something that would basically achieve the

742
00:54:02,880 --> 00:54:09,920
same thing now I think most of the book would still stand even if somehow you drop from the

743
00:54:09,920 --> 00:54:16,640
package of assumptions of technical maturity next question is virtual immortality is that part of

744
00:54:16,640 --> 00:54:22,720
a solved world because lifespan again I didn't can't remember every single word but I don't

745
00:54:23,520 --> 00:54:30,320
don't recall that lifespan of being a critical part of the of the solved world 100 years or

746
00:54:30,320 --> 00:54:37,440
something but in a solved world one might think there's physical immortality and then

747
00:54:37,520 --> 00:54:43,200
concept of virtual immortality yeah well I mean immortality is a long time

748
00:54:45,200 --> 00:54:52,480
I mean if we mean literally never dying that is possibly physically impossible I mean given

749
00:54:52,480 --> 00:55:01,600
life of the heat death of the universe etc but if we are talking just about say extreme life

750
00:55:01,600 --> 00:55:06,560
extension I certainly don't think there is an law of nature that says that humans can only live for

751
00:55:06,560 --> 00:55:12,560
80 years or 100 years or whatever like once you achieve the ability to continuously repair

752
00:55:12,560 --> 00:55:18,480
damage that occurs and then maybe reduce accident risk certainly many thousands of years would be

753
00:55:18,480 --> 00:55:23,120
trivial and if you could upload into computers then your software which could just be kind of

754
00:55:23,120 --> 00:55:29,040
error checked and redundantly stored etc and you could have astronomical lifespans the question

755
00:55:29,040 --> 00:55:33,200
in that context becomes not so much whether you could keep sort of the physical substrate alive

756
00:55:33,920 --> 00:55:40,880
and functioning but more what does it mean for if you want to retain a human like mind

757
00:55:41,680 --> 00:55:45,680
I mean you can keep learning for 100 years and presumably for 200 years right but

758
00:55:46,320 --> 00:55:51,200
after 200,000 years we don't really know whether the human mind would just kind of go stale and

759
00:55:51,200 --> 00:55:57,680
rigid and eventually become kind of non-human if you want to continue to develop and learn

760
00:55:57,680 --> 00:56:02,720
and change from experience the way we currently do which might seem really part of what it means

761
00:56:02,720 --> 00:56:07,760
to be human it might be that if you continue doing that over sufficient large timescale you

762
00:56:07,760 --> 00:56:14,320
eventually become something non-human like that might retain some of your earlier humanity in it

763
00:56:14,320 --> 00:56:20,560
just like you retain something of your five-year-old self but it's still you're not I mean you're in

764
00:56:20,560 --> 00:56:24,880
some sense the same person but in some sense also a different person and I think similarly we might

765
00:56:24,880 --> 00:56:30,320
become like post-human versions of ourselves over really really long timescales. Nick two things

766
00:56:30,320 --> 00:56:35,680
about the book that intrigued me randomly I just want to ask you quickly the first is you made a

767
00:56:35,680 --> 00:56:41,040
comment that consciousness is not necessary for moral status so that surprised me.

768
00:56:43,280 --> 00:56:50,960
Yeah so I'm inclined to that view I'm not fully confident but it's particularly relevant in the

769
00:56:50,960 --> 00:56:57,600
context of digital minds and maybe even sub-human digital minds like once we are currently building

770
00:56:57,600 --> 00:57:04,000
are the like the next generation of the current AI systems above beyond. I think consciousness would

771
00:57:04,000 --> 00:57:08,800
be sufficient for moral status if you can suffer that that would mean that it matters how you're

772
00:57:08,800 --> 00:57:14,640
treated but it seems possible to me that I did a little mind even if it doesn't have that but say

773
00:57:14,640 --> 00:57:22,960
it has a conception of self as existing through time it's a really sophisticated mind it it has

774
00:57:22,960 --> 00:57:29,520
preferences maybe life goals it can form relationships with human beings reciprocal

775
00:57:30,720 --> 00:57:35,680
friendships etc I think in that case my intuition would strongly be that there would be ways of

776
00:57:35,680 --> 00:57:40,560
treating it that would be wrong and so that it would have moral patience even if it weren't

777
00:57:40,560 --> 00:57:46,400
conscious. And it could have those characteristics without having consciousness? Yeah I mean so those

778
00:57:46,400 --> 00:57:51,440
characteristics are all functionally defined these are sort of behaviors and dispositions etc

779
00:57:52,960 --> 00:58:00,400
so I mean it might be that depending on how willing you are to ascribe conscious experiences

780
00:58:01,440 --> 00:58:04,880
to different kinds of systems maybe you would ascribe conscious experience

781
00:58:04,880 --> 00:58:09,600
but I would say even conditionalizing on if not having conscious experiences if it had those

782
00:58:09,600 --> 00:58:16,080
attributes it would be a strong candidate. That's an interesting position near the end you introduce

783
00:58:16,080 --> 00:58:23,360
the concept of enchantment why? It seemed like another example of these

784
00:58:24,720 --> 00:58:34,400
quiet values like the kind of star constellations that are a little bit hidden from us in our current

785
00:58:34,400 --> 00:58:42,960
sort of brutish condition of grave needs and desperation but that could come into view if we

786
00:58:42,960 --> 00:58:49,760
solve a lot of the practical problems and one of the things that if it's missing I think might

787
00:58:49,760 --> 00:58:57,760
make a possible future condition look less attractive to us. If you take the extreme

788
00:58:58,800 --> 00:59:05,600
example of the absence of enchantment imagine some future in which so this is not a utopia at all

789
00:59:05,600 --> 00:59:12,480
but like just consider if you were sitting in a chamber and your job consisted of pressing

790
00:59:13,520 --> 00:59:17,360
like maybe you were presented with some analytic problem in a little text bubble

791
00:59:17,360 --> 00:59:21,040
and then you had to think hard and engage all your mental faculties use your knowledge and

792
00:59:21,040 --> 00:59:26,160
creativities all of that would be there and then you sort of outputs the answer and then you get

793
00:59:26,160 --> 00:59:33,440
as a reward a pleasure palette that also gave you your nutrients and you sort of shortcut our

794
00:59:33,440 --> 00:59:39,840
rich interactions with reality and simplify it to a purely analytic exercise where all that matters

795
00:59:39,840 --> 00:59:46,560
is kind of whether you choose action A, B or C so you still have causal impact you still have to use

796
00:59:46,560 --> 00:59:52,000
a lot of your human capacities but something would seem to be missing I call this enchantment so right

797
00:59:52,000 --> 00:59:59,920
now when we are in the world all the different parts of us are relevant and engaged so in addition

798
00:59:59,920 --> 01:00:05,040
to your abstract decision you have intuitive decisions right you have emotions you have to

799
01:00:05,040 --> 01:00:10,240
control and manage you have body language you have a physiology you have legs and like

800
01:00:10,240 --> 01:00:14,320
and we interact with other people we don't just perceive whether they choose A, B and C we sort

801
01:00:14,320 --> 01:00:20,960
of perceive we have a much higher bandwidth interface with reality and so that's I'm trying to

802
01:00:20,960 --> 01:00:25,040
gesture because this value hasn't really been characterized but I think with a few more examples

803
01:00:25,040 --> 01:00:28,880
like that one can get an intuitive sense there is something there that if that weren't there

804
01:00:28,880 --> 01:00:34,400
then plausibly this this future would be deficient so Nick let's have some fun I'm going to ask you

805
01:00:34,400 --> 01:00:41,200
some very big questions and ask you beg you for some very short answers and let's see what happens

806
01:00:41,200 --> 01:00:49,120
so first what are the percentages for the following scenarios for AI superintelligence in the next

807
01:00:49,120 --> 01:00:55,360
hundred years I picked a hundred years so here's the percentage some really bad events would occur

808
01:00:55,360 --> 01:01:01,040
that AI will do substantial damage to humanity percentage zero to a hundred

809
01:01:03,120 --> 01:01:11,680
like my p-doom as I call it now I've punted on this in the past I think it's like certainly

810
01:01:12,560 --> 01:01:18,640
very non trivial it depends partly on what we do the degree to which we get our act together

811
01:01:19,440 --> 01:01:25,520
but partly I think it's also baked in percentage a number I'm listening for a number

812
01:01:28,800 --> 01:01:38,560
the percentage won't hold you to it no but other people might range give me how about a range

813
01:01:39,280 --> 01:01:48,640
non-trivial is I mean yeah I mean I mean it seems like a bit bigger than 5% and lower than 95%

814
01:01:49,600 --> 01:01:55,760
okay well that's we made some progress like on the twin prime the subject to revision

815
01:01:56,320 --> 01:02:03,040
okay all right how about I mean a lot depends yeah I mean bad things are going to happen to humans

816
01:02:03,040 --> 01:02:10,160
by default anyway I mean we all kind of either get cancer or heart disease or get shot or Alzheimer's

817
01:02:10,160 --> 01:02:18,960
or something else over a hundred year timescale and so the default is that we are all kind of

818
01:02:18,960 --> 01:02:27,520
going into the the slaughterhouse and the the question is like how low does the chance have to

819
01:02:27,520 --> 01:02:34,080
be before one is it would be willing to take a gamble on something different that's one question

820
01:02:35,200 --> 01:02:39,600
but then I think there also this would get beyond our current short form format

821
01:02:40,960 --> 01:02:48,640
questions about how our AIs relate to other AIs out there in the infinite universe that are already

822
01:02:48,640 --> 01:02:57,040
established okay so let's let's go on fine good some aspect of the utopian outcome exactly the

823
01:02:57,040 --> 01:03:04,560
opposite of the AI say large does away largely with all work in a hundred years what's the likelihood

824
01:03:04,560 --> 01:03:13,120
of that zero to a hundred so conditional on AI being developed or just whatever yeah

825
01:03:14,800 --> 01:03:20,080
well conditional on it being developed yeah I mean that that I mean I'll work with the exception

826
01:03:20,080 --> 01:03:25,920
of work where there is a specific demand that it be performed by human or where the consumer

827
01:03:25,920 --> 01:03:33,760
cares about the process and with the asterisk that yeah okay these are really hard like I think

828
01:03:33,760 --> 01:03:39,360
you're doing the mistake of trying to ask a philosopher to be very concise

829
01:03:42,480 --> 01:03:51,840
I just see like an avalanche of considerations and qualifications and levels for each of these

830
01:03:51,840 --> 01:03:58,480
very complex questions okay next question a percentage of a catastrophic human event

831
01:03:58,480 --> 01:04:08,000
dealing with existential risks to humanity before 2050 not saying elimination of all human beings

832
01:04:08,000 --> 01:04:16,160
like a huge asteroid but some huge catastrophic event well I think there are catastrophic events

833
01:04:16,160 --> 01:04:24,880
all the time something that would would decimate would would would kill a large percentage of

834
01:04:24,880 --> 01:04:34,640
humanity or eliminate you've dealt so much with existential risk yeah yeah well I mean so existential

835
01:04:34,640 --> 01:04:42,000
risk is a subset right we actually permanently destroy the future and then I guess it depends a

836
01:04:42,000 --> 01:04:47,120
little on how like how how how many people do you have to decimate like so like COVID is like

837
01:04:47,120 --> 01:04:52,240
whatever half a percent or something and then it goes up from there I know I'd say a bigger number

838
01:04:52,240 --> 01:05:01,280
you know 50% of humanity I mean most likely I think we'd either not have that or we have an

839
01:05:01,280 --> 01:05:09,680
existential catastrophe that or something very close to like 50% is a kind of weirdly intermediate

840
01:05:09,680 --> 01:05:16,400
number well it could happen some some pandemic engineered pandemic and maybe or maybe a thermal

841
01:05:16,400 --> 01:05:22,320
nuclear but like engineered pandemic is probably the most likely way that 50% of us die in the

842
01:05:22,320 --> 01:05:28,880
next couple of decades okay a little bit into your your total work the percentage that our universe

843
01:05:28,880 --> 01:05:35,760
is a simulation I don't know if you've ever said that you gotta get my probability many have tried

844
01:05:35,840 --> 01:05:39,840
so far none has succeeded and you shall not be the first to

845
01:05:47,760 --> 01:05:52,480
do you think there is at least one solved world in the observable universe

846
01:05:56,800 --> 01:06:04,080
in the observable universe no I mean unless yeah unless the simulation hypothesis is true in which

847
01:06:04,080 --> 01:06:10,800
case the question becomes a bit wonky right I mean I don't think there is any in the observable

848
01:06:10,800 --> 01:06:15,200
you know I mean the observable universe I think most likely intelligent life is

849
01:06:17,440 --> 01:06:23,920
low density so that might be infinitely much of it but within any small finite region like the

850
01:06:23,920 --> 01:06:29,440
observable universe it might just be so unlikely for it to evolve in the first place that we are

851
01:06:29,520 --> 01:06:36,320
alone which would account for the Fermi paradox right that's an important issue that we deal

852
01:06:36,320 --> 01:06:43,840
with and that's a good very good perspective AI consciousness true in awareness what's your

853
01:06:43,840 --> 01:06:52,000
odds on that happening within a conceivable a thousand years is that a high likelihood

854
01:06:52,720 --> 01:06:59,280
um yeah uh conditional on us not going extinct before I mean in fact I wouldn't be confident we

855
01:06:59,280 --> 01:07:08,640
don't already have it in some AI systems I think as you zoom in on the concept of consciousness

856
01:07:08,640 --> 01:07:15,280
this might be for another conversation but I think it becomes it's a lot more multi-dimensional

857
01:07:15,280 --> 01:07:22,160
and vague than the naive you would have it and so the question might be less binary than one one

858
01:07:22,160 --> 01:07:28,720
one supposes virtual immortality of our first person consciousness is that in principle possible

859
01:07:32,400 --> 01:07:38,720
um well certainly uh extreme longevity as in like whatever thousands or millions of years

860
01:07:39,840 --> 01:07:43,840
immortality as in never dying depends on physics which currently looks like it would

861
01:07:43,840 --> 01:07:50,480
not admit of infinite information processing streams so virtual uh uploading of our first

862
01:07:50,480 --> 01:07:57,520
person consciousness not not as a duplicate where it's like a um a very sharp identical twin but

863
01:07:57,520 --> 01:08:04,720
literally my first person consciousness is you yeah it is uh it is in principle uh possible

864
01:08:05,600 --> 01:08:13,040
I think so yeah okay um is life in the universe a happy accident or is life somehow built

865
01:08:13,040 --> 01:08:24,720
into the ultimate laws of physics I mean both could be true it might be the it's built in that

866
01:08:24,720 --> 01:08:33,680
for any planet it's it there's an extremely low chance of it happening and then it might also

867
01:08:33,680 --> 01:08:38,720
be built in that there are enough planets that statistically it will happen on an astronomical

868
01:08:38,720 --> 01:08:47,440
or infinite number of planets um how prevalent is life and mind in the universe you've said already

869
01:08:47,440 --> 01:08:56,080
that it is very rare uh is it there a possibility that we are alone in terms of intelligent mind

870
01:08:58,000 --> 01:09:03,360
in the observable universe I think that's a very real possibility but of course if the universe is

871
01:09:03,360 --> 01:09:09,040
infinite as it looks like it is then with probability one uh that would be infinite

872
01:09:09,040 --> 01:09:13,840
the many of these places were intelligent life as a result and then if you introduce the simulation

873
01:09:13,840 --> 01:09:18,560
then it becomes more complex to answer it but yeah yeah I know the simulation is actually

874
01:09:18,560 --> 01:09:23,600
self-solving in that in that to some other questions an infinite universe anything that's possible

875
01:09:23,600 --> 01:09:28,480
will happen an infinite number of times and so the question becomes becomes very vague last question

876
01:09:29,120 --> 01:09:33,360
does anything exist not explainable in terms of ultimate physics

877
01:09:38,080 --> 01:09:47,440
um so yeah like I think for something to be explainable could mean two different things one

878
01:09:47,440 --> 01:09:53,520
is that it's sort of supervenes on the laws of physics yes um which maybe is what you have in

879
01:09:53,520 --> 01:09:59,600
mind uh well then yeah the laws of physics in our universe uh I think there could be a lot of

880
01:09:59,600 --> 01:10:03,760
things that don't supervene on them if we are in a simulation there would be other layers of

881
01:10:03,760 --> 01:10:09,600
reality which would have their own laws of physics etc um in our observable universe it looks like

882
01:10:09,600 --> 01:10:14,000
everything supervenes on the laws of physics doesn't mean it's explainable in the sense that there is

883
01:10:14,000 --> 01:10:22,160
like a useful intelligible you know 10 page text that would like make you more informed by talking

884
01:10:22,160 --> 01:10:26,640
about basic physics like if you're trying to understand some cultural phenomena you would

885
01:10:26,640 --> 01:10:30,400
you wouldn't start writing out the quantum equations or something yeah Nick this has been

886
01:10:30,400 --> 01:10:36,640
terrific um I wish we could go on forever we'll definitely do this sooner than another 17 years

887
01:10:36,640 --> 01:10:44,560
yeah I promise that uh Deep Utopia is a fantastic book recommended for everybody it is a vision

888
01:10:44,560 --> 01:10:49,200
for the future but more than that it's more than that it's really an understanding of what

889
01:10:50,160 --> 01:10:56,080
what the meaning of human life can be and it reflects on what we think of our own values so

890
01:10:56,080 --> 01:11:02,560
we can go on viewers can watch hundreds of tv episodes and exclusive videos on cosmos life

891
01:11:02,560 --> 01:11:07,360
cosmology and meaning on the closer to truth website and closer to the youtube channel

892
01:11:07,360 --> 01:11:13,520
including of course those of Nick Bostrom thanks Nick thanks everyone for watching thank you very

893
01:11:13,520 --> 01:11:22,960
much thank you for watching if you like this video please like and comment below you can support

894
01:11:22,960 --> 01:11:32,400
closer to truth by subscribing

