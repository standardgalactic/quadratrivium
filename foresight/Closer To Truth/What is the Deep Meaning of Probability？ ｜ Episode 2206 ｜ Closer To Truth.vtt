WEBVTT

00:00.000 --> 00:29.000
I want to know how the world works.

00:30.000 --> 00:39.000
I scan the hierarchy of being from fundamental physics to physical chemistry, biochemistry, biology, psychology, sociology.

00:39.000 --> 00:49.000
Bottom to top, I see mathematics. And I wonder, is the math really there at the foundations making it all happen?

00:49.000 --> 01:00.000
Or is the math merely our way of describing the data, curve fitting, approximating relationships, or hand waving, making simple models?

01:00.000 --> 01:12.000
This distinction between math as intrinsic and fundamental versus math as extrinsic and descriptive seems especially relevant for probability.

01:12.000 --> 01:18.000
Does probability have a split personality in explicating science?

01:18.000 --> 01:29.000
A potential duality between probability as fundamental to the driving essence and probability as descriptive of the observational data?

01:29.000 --> 01:39.000
It's this potential duality, these two pillars of probability, that I'll call the deep meaning of probability.

01:39.000 --> 01:43.000
What is the deep meaning of probability?

01:43.000 --> 01:47.000
I'm Robert Lawrence Kuhn and Closer to Truth is my journey to find out.

01:47.000 --> 02:03.000
I'm going to follow probability's potential two pillars, pillar one.

02:03.000 --> 02:12.000
Probability as the intrinsic beating heart of quantum physics, also resonating in biology from neural networks to population genetics.

02:12.000 --> 02:24.000
Pillar two, applied probability, predicting the likelihood of future events and its offspring statistics, analyzing the frequency of past events.

02:24.000 --> 02:34.000
But to begin, I get the basics. I go to a mathematician who specializes in probability, Columbia's Ivan Corwin.

02:35.000 --> 02:43.000
Ivan, when I think about probability theory, I'm torn between two different visions of it.

02:43.000 --> 02:52.000
One is as a descriptor of the world. On the other hand, probability is baked into the fundamentals of reality through quantum theory.

02:52.000 --> 02:58.000
Give me your overview of what the field is and how it was founded.

02:58.000 --> 03:05.000
Probability as a field was only really axiomatized in the 30s in the work of Komar Gorov in Russia.

03:05.000 --> 03:13.000
Part of the reason why probability hadn't really lifted off before then was because it was seen as an offense to the gods.

03:13.000 --> 03:22.000
The notion that you would try to predict the outcome of something, that wasn't very favorable because the gods were the ones who were dictating what the outcomes would be.

03:22.000 --> 03:30.000
Now luckily, gambling came into the picture and some people, including Cardano, who was one of the real founders of probability in the 15th century,

03:30.000 --> 03:34.000
he introduced some of the sort of fundamental ideas of probability.

03:34.000 --> 03:39.000
For instance, the idea of enumerating a state space and assigning probabilities to different events.

03:39.000 --> 03:43.000
So if you roll a die, there are six outcomes and you assign probabilities to each of these.

03:43.000 --> 03:50.000
And so people would play games and because they didn't really have the notions of probability, they didn't know how to compute probabilities of outcomes.

03:50.000 --> 03:58.000
So once somebody kind of thought about it and was willing to really enumerate and then assign probabilities, they were able to make a lot of money.

03:58.000 --> 04:01.000
Let me tell you the biggest developments of probability.

04:01.000 --> 04:08.000
So the first was in 1600s, 1630s or so, and it was the law of large numbers.

04:08.000 --> 04:17.000
And this is the result, you've probably heard it, you flip a coin two times, you can have a head-a-head, a head-a-tail, a tail-a-tail, a tail-a-tail.

04:17.000 --> 04:25.000
Not each one's a quarter, but now if you flip it a thousand times or ten thousand times and you look at the number of heads versus the number of tails,

04:25.000 --> 04:28.000
you'll find that the ratio will converge to a half.

04:28.000 --> 04:36.000
Now the fact that that ratio really converged to the sort of expected probabilities is what's called the law of large numbers.

04:36.000 --> 04:42.000
And that result, in the case of fair coin flips, was proved about 300 years ago.

04:42.000 --> 04:47.000
And then it took actually quite a while for people to show that it wasn't just for coin flips,

04:47.000 --> 04:52.000
that there were kind of other types of systems that would describe this sort of scaling limit,

04:52.000 --> 04:57.000
that if you do it a lot, you'll converge to some deterministic limit.

04:57.000 --> 05:03.000
The next level that one goes to in probability, it's what's called the central limit theorem.

05:03.000 --> 05:10.000
And the idea there is if you actually flip a coin a thousand times, you don't get 500 heads and 500 tails.

05:10.000 --> 05:15.000
But you usually get within, say, plus or minus 50.

05:15.000 --> 05:18.000
And where's that plus or minus 50 coming from?

05:18.000 --> 05:25.000
It says that in the scale of the square root of the system size, you will see a bell curve emerge.

05:25.000 --> 05:29.000
You know, you've heard of bell curves, you've heard of Gaussian distribution.

05:29.000 --> 05:34.000
And it's not because it's the answer to how many coin flips you, you know, how many heads do you get in tails.

05:34.000 --> 05:39.000
It's because it comes up all over the place in mathematics and in science.

05:39.000 --> 05:44.000
And the third is came from insurance and it's called large deviation theory.

05:44.000 --> 05:48.000
There are certain situations where you don't care about the average behavior.

05:48.000 --> 05:51.000
You care about aberrant behaviors.

05:51.000 --> 05:54.000
The one in a million who does something sensationally good or sensationally bad.

05:54.000 --> 05:59.000
Or if you're insurance, you care about that one out of a million chance that the building burns down.

05:59.000 --> 06:07.000
And so the challenge there was to understand how do you estimate the probability of extremely unlikely events occurring.

06:07.000 --> 06:10.000
And you might think, you know, who cares about large deviations?

06:10.000 --> 06:16.000
But every time you turn your car on, something needs to happen and you want that thing happens

06:16.000 --> 06:22.000
and the probability of something bad happens is exponentially small compared to the number of times you actually turn the car on.

06:22.000 --> 06:28.000
So this is a little bit of the sort of history of thematically what probability thinks about.

06:29.000 --> 06:37.000
Probability's three foundational themes provide good grounding to discern probability's potential two pillars

06:37.000 --> 06:41.000
and thus to probe probability's deep meaning.

06:41.000 --> 06:45.000
The law of large numbers, which forces convergence.

06:45.000 --> 06:50.000
The central limit theorem, which generates normal distributions.

06:50.000 --> 06:55.000
The large deviation theory, which quantifies rare events.

06:56.000 --> 07:03.000
To seek probability's deep meaning, I'm now prepared to observe probability in the wild,

07:03.000 --> 07:09.000
how probability works in the real world of science, and I go straight to the wildest.

07:11.000 --> 07:14.000
Cosmology, our vast universe.

07:16.000 --> 07:22.000
I seek an astrophysicist who develops statistical tools to analyze cosmological data,

07:22.000 --> 07:28.000
including large-scale galactic structure and the cosmic microwave background.

07:31.000 --> 07:35.000
You have to understand that when you have some data to analyze,

07:35.000 --> 07:38.000
you first start using probability as a tool.

07:38.000 --> 07:46.000
And cosmology is a particular interesting example because there are several types of probability that are involved.

07:46.000 --> 07:50.000
There is the probability that simply describes the measurement's errors.

07:50.000 --> 07:53.000
When you make a measurement, you always make a little bit of a mistake,

07:53.000 --> 07:57.000
but there is a theory of probability that tells you what the mistake you make,

07:57.000 --> 08:00.000
and therefore what's the most likely correct value.

08:00.000 --> 08:04.000
And more measurements you make, the smaller is the error bars,

08:04.000 --> 08:08.000
and in the limit you can arrive to basically zero error if you make infinite measurement.

08:08.000 --> 08:13.000
When we talk about cosmology, we are dealing with a deeper sense of probability,

08:13.000 --> 08:18.000
and here it's one step of abstraction.

08:18.000 --> 08:23.000
It's the probability of the model that describes the universe.

08:23.000 --> 08:28.000
So instead of treating the probability like saying there is a true model,

08:28.000 --> 08:34.000
and then I do the experiment and I check what the probability of the data is,

08:34.000 --> 08:39.000
given the model, I want to invert that, and I want to assign a probability to the model

08:39.000 --> 08:43.000
because I want to know what is the correct model that describes the universe,

08:43.000 --> 08:47.000
and all I have are the observations and not the model.

08:47.000 --> 08:49.000
Right, so give me some examples.

08:49.000 --> 08:52.000
The microwave background is probably the simplest example.

08:52.000 --> 08:55.000
In a measurement of the cosmic microwave background,

08:55.000 --> 09:01.000
the experiment wants to measure the temperature or the polarization of the sky in a particular direction,

09:01.000 --> 09:03.000
which in this picture will become a pixel,

09:03.000 --> 09:07.000
and then in that pixel there will be an error, a measurement error,

09:07.000 --> 09:09.000
that has got to do with the noise that is in your instrument,

09:09.000 --> 09:11.000
and how well you can do that measurement.

09:11.000 --> 09:15.000
But then there is another error associated to that,

09:15.000 --> 09:22.000
because the universe we see is one possible realization of all the possible universes

09:22.000 --> 09:25.000
that your model could have generated,

09:25.000 --> 09:28.000
and maybe other model, other singular model could have generated

09:28.000 --> 09:32.000
that are still consistent with the picture of the universe we have.

09:32.000 --> 09:38.000
And what we want to infer is what is the probability of the model

09:38.000 --> 09:41.000
that has generated this data that we observe.

09:41.000 --> 09:46.000
And we also have to put into account the fact that we see the universe we see,

09:46.000 --> 09:49.000
we don't see all the universe which is much bigger.

09:49.000 --> 09:53.000
So you have to put all that into your error bars

09:53.000 --> 09:56.000
and state what you are saying and what the meaning of it is,

09:56.000 --> 09:58.000
I measure something.

09:58.000 --> 10:01.000
The very interesting things that come in is when you ask,

10:01.000 --> 10:05.000
well, if this is the primary universe, this is the baby universe,

10:05.000 --> 10:08.000
and there are already in homogeneity in there,

10:08.000 --> 10:10.000
who or what put them there?

10:10.000 --> 10:15.000
Yeah, people have very theoretical models about quantum mechanics,

10:15.000 --> 10:16.000
they're so small.

10:16.000 --> 10:20.000
Exactly, but then you will know that in quantum mechanics,

10:20.000 --> 10:22.000
there's probability everywhere.

10:22.000 --> 10:26.000
So we go back to randomness and probability.

10:26.000 --> 10:31.000
So in some sense, we are a product of uncertainty and probability,

10:31.000 --> 10:35.000
because it's the quantum randomness that creates those perturbations

10:35.000 --> 10:40.000
and out of those perturbations, gravity worked on them for some 14 billion years,

10:40.000 --> 10:43.000
and here we are having this interesting discussion.

10:43.000 --> 10:47.000
So in essence, you're talking about three kinds of probability radically,

10:47.000 --> 10:49.000
each one radically different from each other.

10:49.000 --> 10:53.000
Yes, and it's the same theory of probability,

10:53.000 --> 10:56.000
which you can write down with the same kind of equation

10:56.000 --> 10:58.000
and the same kind of machinery

10:58.000 --> 11:03.000
that allows you to describe these three different types of probability,

11:03.000 --> 11:06.000
and they all get rolled up into an error bar

11:06.000 --> 11:09.000
about what we think the universe is made of, say.

11:12.000 --> 11:16.000
Those error bars encoding probabilities

11:16.000 --> 11:19.000
help reveal the composition of the cosmos.

11:19.000 --> 11:24.000
Leysche distinguishes three kinds of probability in cosmology.

11:24.000 --> 11:29.000
The first is how tightly measurements cluster around particular values,

11:29.000 --> 11:32.000
which is a test of confidence in those values.

11:32.000 --> 11:35.000
This is probabilities pillar one.

11:35.000 --> 11:39.000
The second kind of probability is how the measured values

11:39.000 --> 11:42.000
support a given model that claims to describe the universe.

11:42.000 --> 11:45.000
This applies probabilities pillar one.

11:48.000 --> 11:52.000
The third kind of probability is the inherent uncertainty

11:52.000 --> 11:55.000
of quantum mechanics in the very early universe,

11:55.000 --> 12:00.000
and gravity's astonishing amplification of those miniscule fluctuations

12:00.000 --> 12:03.000
to construct, over aeons of time,

12:03.000 --> 12:07.000
the vast galaxies and stars we see today.

12:07.000 --> 12:10.000
This is probabilities pillar two.

12:13.000 --> 12:18.000
But the deep meaning of probability in physics and cosmology is debated.

12:18.000 --> 12:21.000
I speak with the author of Existential Physics,

12:21.000 --> 12:25.000
a physicist who relishes challenging current belief,

12:25.000 --> 12:28.000
Sabine Hasenfelder.

12:28.000 --> 12:33.000
The reason we are not making much progress on the foundations of physics

12:33.000 --> 12:38.000
is that on a really fundamental level, we do not understand probability.

12:38.000 --> 12:42.000
So probability appears prominently, of course, in quantum mechanics,

12:42.000 --> 12:47.000
but it also appears in the discussion about the multiverse,

12:47.000 --> 12:50.000
the question of why are the constants of nature,

12:50.000 --> 12:53.000
these particular constants that we observe,

12:53.000 --> 12:57.000
and also in the argument that the Large Hadron Collider

12:57.000 --> 13:00.000
should have seen new particles besides the Higgs boson,

13:00.000 --> 13:02.000
which has not happened.

13:02.000 --> 13:05.000
The whole issue with quantum mechanics is that the wave function

13:05.000 --> 13:07.000
is not the probability distribution,

13:07.000 --> 13:12.000
but you calculate the probability distribution from the wave function,

13:12.000 --> 13:16.000
and the probability distribution is the only thing that we can observe,

13:16.000 --> 13:20.000
whereas the wave function itself is not observable.

13:20.000 --> 13:24.000
So the problem with the multiverse is that in the multiverse,

13:24.000 --> 13:27.000
you have this infinite number of universes,

13:27.000 --> 13:29.000
which brings up questions of the type,

13:29.000 --> 13:33.000
why do we find ourselves in this particular universe

13:33.000 --> 13:39.000
with these particular values of the constants of nature that we have measured,

13:39.000 --> 13:44.000
and there are some anthropic arguments that you have to take into account here,

13:44.000 --> 13:49.000
that we just cannot live in certain kinds of universes with certain constants,

13:49.000 --> 13:55.000
but once you have that, you still have a distribution over universes

13:55.000 --> 13:58.000
in which we could find ourselves,

13:58.000 --> 14:03.000
and what you then want to argue is that in this multiverse,

14:03.000 --> 14:10.000
we would be likely to find ourselves in something that looks like what we actually see.

14:10.000 --> 14:15.000
The reason you want that is to argue that the multiverse actually explains something.

14:15.000 --> 14:21.000
Now, the problem with that is that if you have an infinite number of universes,

14:21.000 --> 14:27.000
it is very difficult to properly define some notion of probability on that.

14:27.000 --> 14:32.000
So you always end up comparing infinities to infinities,

14:32.000 --> 14:36.000
and that's not a mathematically well-defined procedure.

14:36.000 --> 14:42.000
You have to use additional assumptions to fix that problem.

14:42.000 --> 14:45.000
Then that goes into the next category that you mentioned.

14:45.000 --> 14:50.000
If you look at all the constants that are in the standard model,

14:50.000 --> 14:55.000
then they all look good, they all look reasonably probable,

14:55.000 --> 15:00.000
except for one, which is the mass of the Higgs boson.

15:01.000 --> 15:06.000
Now, physicists were arguing before the Large Hadron Collider turned on

15:06.000 --> 15:15.000
that this particular constant is so improbable that the standard model cannot be the last word.

15:15.000 --> 15:19.000
Instead, there has to be more to particle physics,

15:19.000 --> 15:24.000
which would explain why this constant is what it is.

15:24.000 --> 15:29.000
So the goal is that you amend the standard model

15:29.000 --> 15:33.000
so that this constant eventually turns out to be probable.

15:33.000 --> 15:36.000
This goes under the name naturalness argument,

15:36.000 --> 15:40.000
and these naturalness arguments were the key reason

15:40.000 --> 15:44.000
why so many theoretical physicists believed

15:44.000 --> 15:51.000
that the Large Hadron Collider should see some new physics besides the Higgs boson.

15:51.000 --> 15:58.000
This naturalness argument is also sometimes called an argument from fine-tuning.

15:58.000 --> 16:05.000
It basically says that there are certain cancellations between numbers

16:05.000 --> 16:08.000
that have to work out very, very precisely.

16:08.000 --> 16:11.000
This is a notion of fine-tuning,

16:11.000 --> 16:18.000
but you can also see it as an unnatural coincidence.

16:18.000 --> 16:22.000
So this is where this unnaturalness comes from.

16:22.000 --> 16:26.000
So if you have what looks like fine-tuning on its surface,

16:26.000 --> 16:29.000
you have to search for something else to make it natural,

16:29.000 --> 16:33.000
or you have to have an unnatural explanation for the fine-tuning,

16:33.000 --> 16:36.000
which gives the physicists hives.

16:36.000 --> 16:39.000
Yes, exactly. It's just that on a fundamental level,

16:39.000 --> 16:44.000
you can very well just accept that this constant is whatever it is.

16:44.000 --> 16:49.000
Ultimately, this argument goes back to a specific assumption

16:49.000 --> 16:58.000
about the probability distribution parameters in some space which we cannot observe,

16:58.000 --> 17:02.000
because the only thing we can observe is our universe,

17:02.000 --> 17:06.000
with this particular selection of the constants of nature.

17:06.000 --> 17:12.000
So making any kind of assumption about the probability of the constants of nature

17:12.000 --> 17:16.000
in a space that we cannot really observe

17:16.000 --> 17:21.000
is for what I'm concerned not proper science.

17:21.000 --> 17:28.000
To Sabine, probability is a lens through which physics and cosmology can be viewed.

17:28.000 --> 17:33.000
I'm intrigued by the naturalness argument.

17:33.000 --> 17:37.000
It seems so fine-tuned as to be unnatural,

17:37.000 --> 17:42.000
but a natural explanation is required and must be found.

17:42.000 --> 17:47.000
But when that natural explanation is a multiverse,

17:47.000 --> 17:51.000
Sabine says that is not science.

17:51.000 --> 17:56.000
But probability is not limited, of course, to physics and cosmology.

17:56.000 --> 18:02.000
Probability pervades science, biology, psychology, sociology, medicine.

18:02.000 --> 18:05.000
Everywhere there is data.

18:05.000 --> 18:10.000
This burdened data science and complex systems are in class A.

18:10.000 --> 18:14.000
Probability is a way of wrapping up things we don't understand,

18:14.000 --> 18:16.000
a variability of randomness.

18:16.000 --> 18:21.000
The idea of randomness being that if I were to rerun the tape of the world a second time,

18:21.000 --> 18:23.000
slightly different things might happen,

18:23.000 --> 18:27.000
because maybe I don't know the initial conditions of the system perfectly well.

18:27.000 --> 18:32.000
And so as a result, the systems diverge slightly in these two different runs of the simulation, so to speak.

18:32.000 --> 18:37.000
And in order to capture sort of the underlying mechanisms that are driving the whole system,

18:37.000 --> 18:40.000
I need to be able to capture that variability.

18:40.000 --> 18:46.000
And so in practice what we do with probabilistic modeling is we stuff that variability into an error term.

18:46.000 --> 18:51.000
And we say that there is a set of deterministic rules that govern the way things work on average,

18:51.000 --> 18:57.000
and then there's some randomness that we include to capture the variability that we're not covering with.

18:57.000 --> 19:03.000
The size of error bars, how big it is at any point, is a very important descriptor of the system.

19:03.000 --> 19:07.000
So if a system is very unpredictable, the error bars will be enormous,

19:07.000 --> 19:13.000
because you are not able to capture enough of the underlying mechanisms to explain that variability.

19:13.000 --> 19:18.000
So the role of taking these large data sets and trying to boil them down into scientific insights

19:18.000 --> 19:24.000
is partly about starting with a model that is very poor, that puts most of the variability into the error term,

19:24.000 --> 19:28.000
and then slowly picking apart what are the threads of causality,

19:28.000 --> 19:32.000
and then pulling those pieces out of the error model, out of the probabilistic part,

19:32.000 --> 19:35.000
so that you can capture that structure more readily.

19:35.000 --> 19:39.000
And that leaves all the stuff we don't understand sort of captured by the probability.

19:39.000 --> 19:41.000
Looking at the large data sets that we have available today,

19:41.000 --> 19:46.000
these probabilistic models that use probability to capture the variation of things,

19:46.000 --> 19:52.000
this is the only way that we can extract insight from these data sets about complex systems.

19:52.000 --> 19:57.000
Because in complex systems, the ways things can interact with each other can be so complicated.

19:57.000 --> 20:02.000
When you're thinking about the behavior of a cell, the genome is incredibly complicated,

20:02.000 --> 20:07.000
and the environment has a role, and so in order to get a good model,

20:07.000 --> 20:10.000
you have to be able to throw out some of the factors,

20:10.000 --> 20:14.000
and doing that means you have to put them into the probability part.

20:14.000 --> 20:21.000
I stand humble before the power and ubiquity of probability.

20:21.000 --> 20:25.000
I like probability as a way of wrapping up things we don't understand,

20:25.000 --> 20:34.000
using this information, these variables we capture to tease out underlying mechanisms driving whole systems.

20:34.000 --> 20:38.000
What's the forefront of modern f...

20:38.000 --> 20:43.000
I return to probability expert Ivan Corwin.

20:43.000 --> 20:49.000
There are kind of two themes that are really coming up a lot in probability research these days.

20:49.000 --> 20:53.000
So the first is universality, and the second is integrability.

20:53.000 --> 20:58.000
So universality refers to the question, or the phenomena,

20:58.000 --> 21:02.000
that despite different microscopic natures of systems,

21:02.000 --> 21:06.000
a lot of different systems look the same when you zoom out,

21:06.000 --> 21:10.000
or when you look at them over a long period of time in the right scale.

21:10.000 --> 21:15.000
And integrability deals with the question of what do they look like?

21:15.000 --> 21:18.000
It's just an example of how this works.

21:18.000 --> 21:25.000
So universality, so you could imagine an example of particles moving on the line,

21:25.000 --> 21:27.000
so your traffic.

21:27.000 --> 21:31.000
So you have cars lined up in a row, and they try to move to the right,

21:31.000 --> 21:34.000
and they do so after some random amount of time.

21:34.000 --> 21:40.000
And so you can ask how many cars will have crossed a given location over a long period of time.

21:40.000 --> 21:43.000
You can compute, you know, the average number of cars.

21:43.000 --> 21:45.000
That's like a law of large numbers.

21:45.000 --> 21:49.000
And then you can ask about the fluctuations around that.

21:49.000 --> 21:54.000
And under this particular model, you can show that the fluctuation will grow in scale,

21:54.000 --> 21:57.000
like the one-third power of time.

21:57.000 --> 22:01.000
Universality holds that it's actually not just kind of within one class,

22:01.000 --> 22:05.000
but between classes you have oftentimes the exact same distributions,

22:05.000 --> 22:07.000
the exact same statistics arising.

22:07.000 --> 22:11.000
So let's take the example of bacterial growth on a petri dish.

22:11.000 --> 22:16.000
So you inculcate a little bacterial colony in the middle of a petri dish,

22:16.000 --> 22:19.000
and you watch it grow outward, and you look at the boundary.

22:19.000 --> 22:23.000
And you see that the boundary is roughly growing spherically or circularly,

22:23.000 --> 22:26.000
but there are fluctuations.

22:26.000 --> 22:29.000
And you can ask how do the fluctuations grow as a function of time?

22:29.000 --> 22:33.000
And you do this on a thousand petri dishes, and you measure over time,

22:33.000 --> 22:39.000
and you see that the fluctuations, it's supposed to be also the one-third power of time,

22:39.000 --> 22:42.000
or in the sense of the radius, and the exact same distribution

22:42.000 --> 22:45.000
that I mentioned in the context of traffic flow.

22:45.000 --> 22:49.000
So there's something very universal about this distribution coming up.

22:49.000 --> 22:51.000
Okay, let's go on to integrability.

22:51.000 --> 22:55.000
Okay, so the first example of integrability is coin flipping.

22:55.000 --> 22:59.000
You flip a coin a thousand times, you ask how many heads or tails there are.

22:59.000 --> 23:02.000
Now you can enumerate the outcomes, and that's very complicated,

23:02.000 --> 23:05.000
or you can use what's called the binomial formula,

23:05.000 --> 23:08.000
which tells you that you can write it in terms of factorials.

23:08.000 --> 23:13.000
Now once you have a formula, you can start to take asymptotics of factorials.

23:13.000 --> 23:17.000
So you go from a microscopic formula, you perform asymptotics,

23:17.000 --> 23:22.000
you show that kind of the formula admits large-scale limits,

23:22.000 --> 23:26.000
and that gives you this statistic, in this case the bell curve.

23:26.000 --> 23:29.000
There turn out to be a certain number of special systems,

23:29.000 --> 23:32.000
systems that have some enhanced mathematical structure

23:32.000 --> 23:35.000
that allow you to actually compute formulas,

23:35.000 --> 23:38.000
albeit a little bit more complicated than factorials,

23:38.000 --> 23:43.000
but formulas that don't grow in complexity as the system size grows.

23:43.000 --> 23:47.000
This notion of integrability that informs what is universal,

23:47.000 --> 23:51.000
and universality gives steam to integrability, it gives it power.

23:51.000 --> 23:54.000
A lot of the world is large, and a lot of the world is too complicated

23:54.000 --> 23:58.000
to really deterministically understand, so it's effectively random.

23:58.000 --> 24:04.000
And the ubiquity of probability is kind of a necessary thing.

24:04.000 --> 24:07.000
Probability gives you this sort of dimensional reduction

24:07.000 --> 24:11.000
from this very complicated deterministic world

24:11.000 --> 24:17.000
to a much more tangible, but random world.

24:17.000 --> 24:19.000
So there's a little bit of a cost in that,

24:19.000 --> 24:23.000
but you still gain a lot in terms of tractability.

24:23.000 --> 24:28.000
To probe the deep meaning of probability,

24:28.000 --> 24:32.000
I begin with the profound power of probability,

24:32.000 --> 24:37.000
refining data, assessing theories, touching ultimate reality.

24:37.000 --> 24:41.000
There are two basic kinds of probability,

24:41.000 --> 24:44.000
inherent randomness of quantum systems,

24:44.000 --> 24:48.000
a way of describing non-random systems.

24:48.000 --> 24:52.000
Probability in cosmology quantifies confidence

24:52.000 --> 24:55.000
in measurements, adjudicates competing models,

24:55.000 --> 25:00.000
reveals how quantum fluctuations become galactic structures.

25:00.000 --> 25:04.000
But perhaps at the foundations of physics,

25:04.000 --> 25:07.000
we do not understand probability.

25:07.000 --> 25:09.000
Contrarians should be appreciated

25:09.000 --> 25:12.000
for keeping us open-minded and humble.

25:12.000 --> 25:15.000
Probability is a way of capturing things we do not understand,

25:15.000 --> 25:19.000
or as variable, or as random.

25:19.000 --> 25:23.000
The deep meaning of probability reflects its duality.

25:23.000 --> 25:28.000
The two pillars of probability split personality in science.

25:28.000 --> 25:33.000
One, a basic operating principle of deepest quantum physics.

25:33.000 --> 25:37.000
Two, an analytical tool that parses past events

25:37.000 --> 25:39.000
and predicts future events

25:39.000 --> 25:43.000
to discern how things work at deepest levels.

25:43.000 --> 25:47.000
To understand the elements that compose our world,

25:47.000 --> 25:52.000
and perhaps its ultimate essence, probability is key.

25:52.000 --> 25:59.000
So, what's the probability we are closer to truth?

26:18.000 --> 26:22.000
Copyright © 2020 All Rights Reserved

26:22.000 --> 26:26.000
No part of this recording may be reproduced

26:26.000 --> 26:28.000
without any permission.

26:28.000 --> 26:30.000
All rights reserved.

26:30.000 --> 26:32.000
All rights reserved.

26:32.000 --> 26:35.000
All rights reserved.

26:47.000 --> 26:52.000
© 2020 All Rights Reserved

