I think you're doing the mistake of trying to ask a philosopher to be very concise.
I just see like an avalanche of considerations and qualifications and levels for each of these very complex questions.
Okay.
Welcome to Closer to Truth.
I'm speaking with futurist visionary Nick Bostrom about his vital, far-sighted, engaging new book,
Deep Utopia, Life and Meaning in a Solved World.
I loved it.
It's an exhilarating romp of ultimate technology centered on AI, how it might work, what it could mean.
It's a prescient manual for the future.
It's an innovative treatise on the meaning of life.
Welcome, Nick. It's great to see you again.
Good to see you again.
Congrats on Deep Utopia.
We'll discuss it in depth.
But to begin, I'd like to get your world overview, the setting for your book.
When we first did our Closer to Truth discussion in Oxford in 2007, 17 years ago,
we discussed the simulation argument, fine-tuning, anthropic selection, the doomsday argument.
How would you characterize the last 17 years or so in terms of technological and intellectual development,
especially the importance of AI?
Well, I mean, it's all happening now, as we've entered the Atollius era.
I think the last, especially since 2012-2014, with the Deep Learning Revolution,
I think we've been on a kind of up-ramp of AI capabilities,
kicking into even higher gear with the release of ChatGPT.
And in the last two years or so, we've really seen it hit the mainstream.
We're now the White House and key policy makers all over the world
are starting to debate the future of AI.
So it's a remarkable time.
And when did you actually plan this book?
Because it's obviously, in essence, the other side of your superintelligence,
2014, where you were prescient in warning about the dangers of AI.
And so the last two years has been a high focus on the dangers.
Now you've moved on to the opportunities.
So when did you have that a bit of a transformal insight?
I've been working on it for probably around six years or so.
It wasn't ever planned.
It just kind of happened.
I didn't start out with like some particular set of theses.
I wanted to defend and elaborate on.
I felt an urge to start writing, and then it eventually grew into DP Topia.
Yeah, and I've seen where you've said that the style of the book,
which is very unusual, it's a new literary style involving dialogue
with different characters, your own persona in a not entirely actual form.
But in dialogue with other people and form of a lecture series over a period of a week,
I think you've said that whole structure wasn't planned.
It happened organically.
Yeah, it's just the way it happened for better or worse.
But that's I do think the form actually does match the content.
It's not a book so much about conclusions as it is a book about questions
and helping the reader to start to think about these problems
and form their own views ultimately.
It's also meant to be not just something that transmits certain concepts and ideas,
but also it's meant to be a reading experience that you might have to work to get through it.
But ultimately, I'm hoping it will kind of put you in a better place
to reflect on questions about what art humanity's destiny be.
I think that's an accurate description.
I found myself very engaged.
I was looking for more of the arguments as that we've had in the past in a very positive sense.
But this book is different.
You do get the arc of various arguments on different things.
We'll talk about that.
But you are brought into that in this engaging intellectual, semi-fictional avatar environment.
Yeah, yeah.
And the other benefit of this sort of having different characters and different bits
is that it makes it easier to explore several different viewpoints,
which I wanted to do and allow each one to be developed in its own right to its fullest extent
and then to kind of collide different perspectives and ideas.
Just as, I mean, you're interested in physics, right?
So with a particle accelerator, you sort of accelerate little particles to enormous energies
and then smash them into one another.
And in those extreme conditions, sometimes you can see the basic principles at work
that we can then infer are at work all the time in ordinary conditions as well.
It's just hard to observe.
And so similarly, this conceit of a plastic world,
a condition in which technology has reached its full maturity
and all practical problems have been solved.
It's an extreme condition.
But I think we can then see values kind of smashing into one another
that we normally can sort of hand wave and then just because they are obscured
by so many practical necessities that kind of occupy most of our contemporary existence.
I think that's a very good characterization.
The book is creating an extreme condition and particle accelerators do that
and physics black holes do that in physics.
That's an extreme condition where people study black holes.
It's not just for the black holes themselves,
but it's subjecting the laws of physics to extreme conditions.
And you learn a lot.
And I think that's a very good characterization of the book.
Yeah.
So I think it's like, I mean, for people who have read it, they will know,
but it's not a book that is really trying to make predictions.
And nor is it trying to offer practical solutions to what we should do next.
I mean, a lot of my other work focuses on that.
Right.
This takes basically as an assumption or a postulate, if you want, that things go well
in order then to be able to ask the questions of what then,
what would be the meaning of human existence?
What would give us purpose in life?
If the whole thing unfolds like everything is perfect,
governance problems is all the alignment problem is solved,
like all these things, but what then would occupy our lives?
Sometimes you never actually get to even ask that question
because there are so many other questions that kind of crowd in before it.
So I just wanted to postulate that and then focus this book entirely
on the set of questions that arise in this hypothetical future condition.
We're going to get into all of it, but let me first give a more formal bio.
Nick Bostrom is a professor at Oxford University,
where he heads the Future of Humanity Institute as its founding director.
With a background in theoretical physics, computational neuroscience,
logic and artificial intelligence, Nick has pioneered contemporary thinking
about existential risk, the simulation argument,
and the vulnerable world hypothesis, among others.
He is the most cited professional philosopher in the world age 50 or under
and is the author of some 200 publications, including anthropic bias,
global catastrophic risks, human enhancement and superintelligence,
the prescient book on the dangers of AI,
but now we're going to look at the extreme condition if all goes well.
So Nick, your book,
Deep Utopia, Life and Meaning in a Solved World.
Let's start with a simple definition of what is a solved world
and what motivates your focus on it.
Well, I am referring to a hypothetical condition
where basically all practical problems have been solved.
So think, first of all, a condition of technical maturity.
So we have super advanced AIs.
Maybe they have helped us develop all kinds of other technologies,
medical technologies, virtual reality, et cetera, et cetera.
So that's part of what it would mean for the world to be solved.
And then on top of that, we also make the assumption that
all the kind of governance problems of the world have been solved
to the extent that they can be solved.
So we imagine we set the site,
questions of war and conflict and oppression and inequality
and all the rest of it.
So but then there remains a big kind of problem,
which is ultimately a problem of value,
which is that under these ideal conditions,
what kind of lives would we want to live?
And yeah, that's...
And that's a very important way to frame the book
because you're not saying all of these problems
that are solved are easy to solve or will be solved.
But if you do solve it, what does that leave?
And it leaves value.
So one question that I have is,
do you distinguish between meaning and purpose?
We use those two terms sometimes interchangeably,
but I think we can tease apart a difference.
Meaning in the title,
but throughout the book, you have purpose as well.
Yeah, that's right.
So I think of purpose as slightly narrower concept
as sort of having a reason for doing something
or for putting out some effort.
And then meaning, I mean, that is discussed in the book,
but might be a certain kind of purpose
or purpose plus something else.
Okay.
You present, just to give a sense of the environment,
utopic taxonomy where you have different levels of utopia
that can give us a richer understanding of it.
So let me just give you the list
and just explain each one very quickly.
The first is government and cultural utopia.
Yeah, this is I think the most familiar kind of utopia
we find in the literature,
where people imagine a better way for society to be organized,
better political institutions, different schools,
maybe different gender roles,
but usually set within more or less
recognizably contemporary technological context.
So people, there's still work that needs to be done
and you can organize how much power the workers have
or how the work is divided,
but there have to be people growing food, et cetera.
So that's the most familiar and basic kind of utopia.
The second level is post-scarcity utopia.
Sounds like we know what it means,
but if you could define it more clearly.
Yeah, so this is the more radical vision of a condition
in which humans have plenty of all that we need materially.
So there are these kind of,
it's more like fantasy in the past,
but various kind of the land of cocaine
was this medieval peasant dream basically
of some condition where the rivers would overflow
with wine and roasted turkeys would just land on the plate
and that would be a kind of continuous feasting.
And you could easily see how that on its own
would have huge attraction
if you were a kind of agricultural labor
who spent your whole life grinding away,
barely getting enough porridge to feed your children
and your joints were aching
from all this backbreaking work that you were doing.
Then this on itself,
just being able to rest and eat as much as you want
would already be like enough of a kind of vision.
Third level is post-work utopia.
Right, so this is the idea that not just
is there plenty to consume,
but that the production of all this plenty
doesn't require human economic labor.
And this has started popping up more recently
in conversations about the future of AI
where people are wondering,
will this advance that we see lead to human unemployment?
We can automate more and more things.
And if you imagine that running its full course,
then maybe eventually you could automate
almost all human economic labor
and then you would have this post-work condition.
The fourth and fifth to get a little more complicated
to understand, but let's do it now.
Fourth is post-instrumental utopia.
So now we're getting into a more radical conception.
And usually current conversations about these issues
stop short before we reach this idea.
But if you really think through what it would mean
for AI to attain its full potential
and then all the other technological advances
that this kind of machine superintelligence
could bring about,
it's not just that we wouldn't have to go into the office
and type on word processors or like hammer away
at construction sites, et cetera.
But also a lot of the other things we need to do
in our daily lives could be automated as well.
So if you think about, if you didn't have to work,
well, then like typical answers would be, well,
maybe somebody likes fitness or something,
so they could spend more time exercising.
But like in this condition,
you could pop a pill instead
and get the same physiological and psychological effects
that spending an hour on the Stairmaster would provide.
And so then why would you really need to go to,
kind of would lose its point
to go to the gym in those conditions?
And you can then start to kind of almost do case studies
on activities that fill our current lives.
And for almost all of them,
you soon see that they have a certain structure,
which is that we do a certain thing,
put in some effort in order then to achieve some other thing.
So brush your teeth, because otherwise eventually
you will have tooth decay and gum decay.
And so in order to get the outcome of a healthy mouth,
you need to spend a few minutes every day brushing your teeth
and gone to the gym.
Or you need to, like you want to understand mathematics, let's say.
So then the only way to do that
is to put in some effort to study mathematics.
And so the effort is motivated by the goal
that it is trying to achieve outside the effort itself.
And a lot of the things that we are doing has that structure.
But now if you could get the goal, the end point,
without having to put in the effort,
then it seems to pull out the rug under the activity itself.
At least it's threatened with the sense of being pointless.
So that's the problem you confront
in this kind of post-instrumental case.
So an example of that,
instead of studying higher mathematics,
you could have an injection of nanobots
that could analyze every synapse in your brain
and then figure out how to change them a little bit
so I can understand algebraic geometry or something.
Right, that's right.
And that would be fast and effortless.
Or even things we do for fun.
So there might be various activities
because it gives you pleasure and joy.
That seems kind of unavoidable.
But even there, if you think about it,
you could have more direct ways of experiencing
the same positive emotions,
like a kind of some super drugs
that could give you the pleasure and the joy
without you having to spend an hour gardening
or doing whatever it is that you're watching movies.
So this brings up a very important point of the book
in terms of what are our real values.
Because when I read that,
and obviously a very intriguing,
remarkable way of thinking and very important,
I was asking myself,
are there any absolute values in a solved world?
And so the way to describe it,
as you started to say,
is if we could take non-harmful drugs
or AI neural implants
that would maintain a state of perpetual ecstasy,
whatever your ecstasy would happen to be,
or it can switch from a physical bodily ecstasy
to an artistic ecstasy or intellectual one.
And if that could be all done,
who could gain say that
if there's no kind of supernatural value
that you would put into it?
I think that is a fundamental theme of your book
about how do we develop the kinds of values
if these things are possible.
You're not saying these things are remotely
in the near or mid or even long term,
but they are the extreme condition
that you talked about which then exposes
what is the nature of absolute values if there are any.
Right.
Now, it is possible that they might be in the long term
or even mid or near term,
depending on how fast the AI revolution unfolds
and what the outcome of that is.
I actually think the time scales for radical transformation
might be shorter than most people realize
if AI continues to speed ahead.
What's your best guess on that?
Well,
I mean, my timelines have shortened somewhat,
at least since this previous book,
Superintelligence, was published in 2014.
I think we are currently looking on timelines
that are on the shorter end of the distribution.
So it's hard to say,
but I mean, it could be years or maybe a decade or perhaps more,
but at least I think there's a non-trivial probability
that we are now kind of on the accelerating slope of this,
but time will tell.
In any way, that's not central to the book,
even if you thought this would take millions
or never happen.
It is still.
But there are two ways of thinking about this.
You could either just read it as,
these are perpetual philosophical questions
that humanity ponderous,
and here is kind of a thought experiment
that helps you think about them.
And I think it's fine if you just read it like that.
For me, there is also this actual real possibility
that we might soon enter a condition like this,
or we might have to make decisions soon
about what kind of future we want,
if we want to stare towards something like this
or some other version of this.
So there is this kind of underlying practical motivation
for me in terms of writing this book,
but that's optional.
So yeah.
I appreciate that.
I was more on the former, on the thought experiment,
and I would put the date measured in hundreds of years,
if not thousands, to achieve what you're saying,
but I'm cautious.
I've become cautious as you,
and I hear you and respect your views,
so I'm a little less sure of what I thought before.
Anyway, we want to get the fifth category
of the utopic taxonomy,
which you call plastic utopia.
Even going beyond the post-instrumental utopia,
what is the plastic utopia?
Well, we alluded to it slightly,
which is the idea that in addition to
having these other properties of being post-instrumental,
we also, in the plastic utopia condition,
have complete control over ourselves.
So our mental states, our psychology,
our cognitive architecture, our bodies becomes malleable.
So it's not just that we imagine human beings,
as we are now, placed in this condition
where we don't have to work
and where we don't have to put out any effort
if we don't want to.
We could just press buttons and get what we want,
but we ourselves, as well, become something we can choose.
So you wouldn't have to work on yourself
to build a better character in a plastic utopia.
If you wanted to, instead,
you could sort of request of your AIG
need to sort of rewire your synopsis
so that you became this different kind of person
or to experience pleasure all the time
or to become smarter or kinder or whatever else.
So that makes it even more like solved or dissolved or liquid.
Like you enter this context
where everything seems kind of fluid and up for grabs,
and it's hard to find on this firm ground to stand on.
Yeah, it's a wonderful way
of seeing what an extreme condition is,
because I would have not come up with those five.
I might have had three or four,
but at that level,
it really very beautifully defines
what an extreme condition for humanity could be in the future,
and therefore it gives the book its real punch.
So one issue that you deal with
especially as you get to all of those
is the question of boredom
and how much of our value system is based upon
the need to or the lack of control and the uncertainty
and what happens whenever this,
you know, this is the same kind of problem
that traditional religions,
both East and West have to deal with
whether you're dealing with nirvana
after the innumerable cycles of birth and death and rebirth,
and then you reach nirvana or in the Judeo-Christian,
Islamic, Abrahamic concept of heaven,
eternal life in heaven.
I mean, that's a sort of an end question of boredom
that occurs in any of these eschatological ideas.
Yeah, it's quite fascinating
if you think far enough in this direction,
you do start to sort of bottom up against theological questions
or at least questions that have traditionally been
discussed in religious contexts about the afterlife, etc.
I try to not trespass onto that terrain in the book.
I have this, there's this other fictional character,
the fictional Bostrom character is giving these lectures
and then sometimes he's asked questions by the students
and occasionally he sort of refers to,
well, you have to take that up with Professor Grossweiter
which is like another character
who doesn't make an appearance in the book
but he's like the theologian
or the person who could answer their questions on that.
But on boredom, yeah, so this is an example
where I think it's important to distinguish
the two different senses of boredom
which is a subjective sense and an objective sense.
So clearly we have a subjective concept of boredom
like somebody might just feel bored
and that would trivially be easy to abolish in Utopia.
I mean, it follows directly from the condition of plasticity
that this feeling is like a subjective state of your brain.
You could rewire that so that you would always feel excited
or interested or whatever antonyms to boredom you want to use.
And the question then is whether there is also
some notion of objective boredom or boringness
whether certain activities or experiences
are such that they are objectively boring
like meaning perhaps that it would be appropriate
to feel subjectively bored if we engaged in them.
So if you imagine somebody to take an example
actually from the philosophical literature of a grass counter
so somebody who spends his life counting the blades of grass
on a particular college lawn, we might think
that that's an activity that is objectively boring
whether or not he happens to feel excited about it
it's not appropriate to be really interested in grass counting
because it's like too monotonous or insignificant
or has some other sort of deficit.
And philosophers disagree about whether like
there is like some kind of firm normative basis for making that
but I think it's an intuition that
some but not all people would have that
it would be bad if the future consisted of merely of activities
like counting grass.
No matter how thrilled the people doing the grass counting were.
That's an objective value that's kind of is a superset
to everything else we're talking about.
Yeah potentially.
And so I'm able to in a plastic world change my brain
to where I am excited about every new grass that I count
and what's going to happen at the next number
and I'm genuinely excited about that
and I've changed my brain to think and so subjectively
I'm excited about life counting all these grasses
or as I think you have in the book table legs
200 and somewhat thousand table legs
or that that potentially is in some objective sense
is suboptimal.
Yeah that's possible to hold that view and
now if one does have that view that it would be objectively
bad to have nothing in one's life than you know counting grass
then that kind of also potentially imposes a constraint
on the subjective experience of boredom.
Like if it actually is objectively bad to do that
then you might think it would be also bad to change
your subjective attitude so that you found great interest
is something that would be boring.
So then you might end up with a situation
where you couldn't eliminate boredom in the future
neither in the objective or objective sense.
And so then but so this possibility of the objective value
there makes the discussion more complex.
Like eliminating the subjective if that's all there is trivial
given the postulates but once they introduce this possibility
of the objective then it becomes like a much more
intricate conversation to what extent we would be able to do
something without violating that.
Now I think at least with respect to the value of
interestingness and there is a bunch of these different
values that they're kind of related but different
but if we focus on interestingness I think there is at least
large scope before increasing the amount of subjective
and objective interestingness including in these utopian lives.
I think even if there is some objective element to what's
boring and what's interesting I think it's has a large
zone of indeterminacy.
I mean you can just look at the current human distribution.
I have a good friend and colleague who tells me he's never bored
and he's interested in as far as I can tell literally everything except sport.
He writes papers on all kinds of topics.
He knows about everything.
He goes to every kind of conference and finds
interesting things to discuss with every person.
Like it doesn't seem to me that there is anything deficient about his human life.
In fact if anything it seems to benefit and be like a greater person for this
this property that he has and obviously that goes down ultimately to some neurochemical
idiosyncrasies of his brain that's he and I think for all of us I think we could expand
the range of things in which we take an interest greatly before we would reach
this point where we would just be counting leaves of grass.
Moreover I think possibly it would be appropriate to expand it even further than that.
Like maybe if we have reached a condition where we had sort of exhausted all the
most obviously interesting things we had discovered all the really fundamental loss of
nature you know solved consciousness and like the biggest questions had all been answered.
Like it would seem perfectly appropriate in that situation to begin to take an
interest in the slightly smaller questions and it's not clear that one couldn't go very
very far in that direction before I reached a point where it would be sort of objectively
bad to take a further interest.
Is religion relevant in a solved world?
Yes potentially very relevant although it's also arguably very relevant in the current world
and one might say something more interesting perhaps if one looks at some other values that
seem not so relevant in the current world but that could potentially become more relevant
in this whole world.
I think that there might be a lot of subtle values that exist now but we don't really
see them very much just as we don't see the stars during daytime because it's like
you know such a like brighter present and analogously there are such stark moral
imperatives right now.
Calamities of all sorts you need to take care of your kids you there are people starving in
the world or being shot at etc etc so so many horrors and an urgent obvious pressing
ethical needs to fix things that it would almost be frivolous now to spend too much time
fretting about more subtle quiet values but if we ever reached a condition where these
pressing needs were taken care of then I think we might be able to see a whole panoply
of these subtler values like for example
various kinds of traditions that it would be nice to honor authentically
ancestors who you know maybe we think of our lost parents once in a while but there is like so
many more people have lived wonderful lives and you know maybe deserve more thought and
consideration we don't have time our daily lives keep us busy but if you didn't have that
but if you didn't have that why not various aesthetic qualities you could imagine making
your life more into a kind of artwork where every relationship was not just a source of
I don't know relaxation or final satisfaction but also something actually beautiful that you
were kind of constructing together etc etc and we don't now have the luxury to kind of really
develop a fine sensibility for those but I think it would be entirely appropriate
that once the urgencies are removed to to sort of tune up like almost like our eyes
dilate at night right so it can take in more light similar in this condition our moral
sensibilities and sensibilities for subtle values I think should dilate and this is a
larger definition of religion as we might have it in today's world enabled by the the solved world
the boundaries of religion become broader yeah potentially but again it also potentially
is very important today so it might be one of those things that is very urgent today even
with other urgencies pressing in upon us like many religious people would say that yes you have all
these practical things you should do but you should also set the time for worship etc even
though it conflicts with but but even more so obviously in this condition and I mean we would
be more like I guess potentially like monks and nuns that have the time to fully devote themselves
to contemplating the divine when I first heard of the book and started it
the first question one of the first questions came into my mind is how does a solved world the
the bostrom solved world articulate with the marxist pure communism and as I started to go
through the book to me that question became pretty obvious that the the that there would be a high
correlation potentially between the first at least two of the utopia taxonomy levels the
government and culture and then the post scarcity and then maybe into the post work as well but
pure communism as it's been envisioned in the past or even in in few cases in the present
doesn't even deal with the points four and five right and and I mean I think I'm not really a
marx scholar but I think he just has a few lines really about what would you know ultimately be
the outcome of if the whole communist product succeeded and I think he refers to this whatever
is it like fishing in the morning and hunting in the afternoon and reading poetry in the evening
so that that sounds like not even a fully post work utopia but like maybe an abundance diminished
work utopia plus a sort of vision of social cultural utopia I guess right right Nick let's
switch and look really long term and very visionary what I call your approach to ultimate
utilitarianism I love the section a quantitative analysis of the potential for happiness or
fulfillment for all sentient beings if if the cosmic endowment could be maximally saturated
with sentience so some of the numbers you give you estimate 10 to the 35th possible human lives
derived from human lives originating on earth to populate the observable universe that's a
think a hundred billion trillion trillion that's your minimum then you go up to 10 to the 43rd and
then if you switch to digital lives which adds a lot of complex value you get a computing power
of the universe of at least 10 to the 58th which is 10 billion trillion trillion trillion four
trillions there in terms of ultimate sentience so what I love the calculation but walk me through
the importance of that in our ultimate thinking and also in terms of the concept of meaning
and purpose which is the purpose of your book well I mean it's like some big number basically
very big number but I mean it's in the context of the book it's a little handout
as the postroom gives out but yeah so I'm not a utilitarian I'm often mistaken for one because
in some of my writings I have explored and analyzed the implications of assuming
an assumption of utilitarianism or aggregative consequentialism because it's a view that you know
significant fraction of moral philosophers have held and that I think maybe deserves at least
some weight even if one doesn't actually embrace it and then it's interesting to see what follows
if one actually takes that perspective seriously and hence in my earlier work this you know the
focus on existential risks as those few things that could actually permanently
destroy our future if one counts these possible future lives the same way as actually currently
existing lives as certain flavors of utilitarianism would do then they just seem to dominate and you
get a bunch of interesting and then there's like a further complication on that which is if you
literally do this try to do this expected utility calculation you find that scenarios in which somehow
even if they are very unlikely but somehow infinite values could be realized like maybe we
are wrong about physics and there's like some actual way of producing infinite and then those
tend to dominate even if they have a very very tiny finite and then you get into infinitarian
paralysis and there's like a whole yeah so I think that's interesting in its own right but it's not
really the topic of the book which more focuses on not not how you aggregate big values or what
our obligations are but like from our point of view like what would be the best possible future
for Robert or for the for you the the viewer or for any of us like if you literally could
imagine the best possible way for your future life to unfold and perhaps we restricted by like
the loss of physics etc but and then trying to think from the inside like how we would furnish
that life with activity experiences relationships etc and then ultimately like you know if your
question is what you should you do now as a moral actor then you would have to somehow integrate all
these different perspectives whatever weight you would put on utilitarian views or the ontological
views or virtue ethics views and the bunch of other stuff but the yeah the book doesn't really try to
pick between these different moral theories it doesn't really in general focus so much on numbers
or on formal structures and aggregation but more tried to sort of which often is done in
contemporary analytic ethics it kind of almost sees the values a little bit like
black pork and that might not be exactly right but this ties to look from the inside
on the values which values do you actually have like at the object level
and what would it take to realize them I'm not sure whether that answers your question but
now there is this yeah I guess I guess like one one way in which this larger view of
the bigness of the future could and does come into the book is insofar as we value
significance like having significant impact on the world for example if that's the version of
significance right now it looks like we have we are extraordinarily well positioned to have
huge impact on the world because well a there's a lot of just ordinary needs in the world and you as
an you know if you know if we imagine you're like a relatively well-off person with health and
intelligence in a wealthy country with a good education like probably most of your viewers are
you have a lot of opportunity just to help a bunch of people and to try to make some positive
difference so that already gives your life potential significance that is maybe greater
than one human life's worth of significance like you could save many people's lives or
but then on top of that you have this idea that maybe we are near a big fulcrum of human history
where if this whole thing the AI transition and the rest of it is going to happen perhaps within
our lifetime then like you can multiply that manifold like if you could even slightly notch
this big future in the right direction that would give your life even more causal significance
this is one thing that might be a lot harder for people living in utopia to have
this kind of significance because if all the problems are already solved
and whatever problems aren't yet solved are anyway much better work that by AIs
then humans might not be able to have significance in that sense and so to the extent that one
thinks it makes a life itself better to have this kind of significance these utopian lives might
lack that significance and therefore have a deficit of that particular value
and so there's a discussion around that and also the possibility of humans whether through AI
colonizing or filling the universe with sentience is a you know gigantic grand vision
um yeah I mean if that's if that's the way one wants to go I mean I mean I actually
happen to think the future is big enough that you could not just realize one vision but many
not every vision because some are directly in conflict but if some people think doing
something nice for existing people and working locally is the most important thing we could
certainly do that and then also that leaves all the rest of the universe and that's big enough
that you could have sort of AI paradise in one sector and you know animal uplift in another
sector and you could have a whole bunch of different to the extent that a vision doesn't
require the negative like the absence of things and just the addition of new things that that
would be easy to do the harder questions become when when like one thing says that another shouldn't
exist and vice versa and then you would have to strike some compromise that will give each of
them less than a hundred percent of what the way they think would be the best there's enough room
out there that both can be accommodated uh yeah and I think this is actually quite important it's
not really the focus of this book but having I think in general as we will be wrestling with
for example how to relate to the digital minds the ai's that we create um
having this sense of expansive generosity and like feeling that there is room for a lot and we
shouldn't push too hard to get a hundred percent of one value but we should try first to sort of give
all reasonable value systems like a good deal of satisfaction and then after that we can
scobble about the remains like but because that that seems like such a if we solve these practical
problems there's so much so much opportunity there and I think that increases the chances of
that the value that the future goes well in the first place.
Nick I'd like to just do some expansive thinking in terms of your book you've positioned it very
well in terms of its objective in terms of human values but the the the assumption the basic
foundation of the book of a solved world and the conditions for that and the implications
lead to many other questions which are beyond the book but I'd like to just put them to you for
because they occurred to me and I'm sure to many people and see see where we go so
no no order here but when we talked about these huge numbers of filling the universe with saturated
with sentience as I said 10 to the 43rd or 58th the number of of sentient minds in one form or
another if that were to occur and it is a handout of that Professor Bostrom gave to the students
which I I lapped up if that would occur what what did you're feeling about why that occurred
is that just would that just be a human tendency or would there be some universal trophism that's
pulling that's self just desiring to be a self understanding and self aware in some sense
do you have any feeling about that in other words what what's the reason that that would happen
yeah well if we are imagining this astronomical entity of objects as being sort of human like
minds then I presume the most likely path whereby that would happen is if humans shaped the future
and in particular that was a strong influence of those humans who value this kind of future like
utility broadly utilitarian constituencies and one might it's possible that how many
people would favor which moral theory will change for example if we became smarter or had
AIS to advise us in our philosophizing there might potentially be some convergence either
two words or away from those conceptions I it's even on that conception it's not clear that the
right unit would be human minds right it might either either be smaller if you think pleasure
is somehow something that could be quantified you know maybe the most optimal structure for
generating pleasure would be like a why do you need all of this this this cortex and
like visual like processing all of that maybe you just need some like kind of pruned down
neural structure and maybe it would be like some some animal maybe this like
optimized better optimized you would go further in that direction maybe pleasure boxes would like
you know have a different size than humans you know you must matter structured to be optimized
for the instantiation of pleasure yeah and if you include digital pleasure if that it's
yeah yeah I mean it doesn't really matter how big this would be if they would be like a millimeter
square or like no you know a light year square but right for other values it's like well you have to
look at them one by one how they scale with resources so some values maybe have diminishing
returns to extra resources and this might be true for sort of typical individual human values
where like I mean so most obviously with like wealth for example it's a much bigger deal if
you're if you go from 1000 the year to 2000 year in income huge difference now if you go from like
one million to two million I mean it's it's a thousand times bigger an increase and an equal
in percentage terms but probably you barely notice it like you get an extra summer house or whatever
it's not really and so so with current economic resources they seem to have a kind of steeply
diminishing marginal returns insofar as they are spent by an individual to try to boost their own
welfare with other values like knowledge etc you might think that there would be diminishing returns
once we have already found the most important knowledge and then we'd be sort of spending
increasing resources to discover smaller and smaller truths Nick consciousness has come into
our conversation and in the book in different different fashions and in different ways
is there a fundamental assumption as a worldview in a solved world as the paradigm for example
require or assumes that consciousness is entirely physical that it's the product of physical laws
irregularities including the deep deepest laws of physics which may be unknown but but still
part of the physical world is is that a an underlying assumption
well I mean I'm a computationalist thinking that it's a structure of certain computations that
produce and conscious experience and those could be implemented on carbon-based organic brains
or in principle on silicon processors or you know in whatever substrate is capable of processing
functionalism is that yeah now I don't think that's really an essential premise for most of the
book I think there are little bits and pieces because I think this views yeah is so that but
I mean basically you could imagine I mean clearly if if our but that would be a crazy view like if
you thought that what we do in this world has no effect on conscious experiences then I guess the
question would become purely philosophical like a thought experiment if you could some if somehow
this condition of a plastic world arose then what would be our values in that world but we would
have no past words it but I think most people would think that clearly it has something to do with
what happens in brains and our sensory organs and like clearly impacts the conscious experiences
we have and so then even if you thought purely silicon entities could not have conscious experience
you could still have technologies that would make it possible to manipulate the organic
brains we have like we already have drugs you could imagine surely slightly better drugs with
fewer side effects and slightly other things that would at least allow us to approximate this condition
of plasticity even if perhaps not go you know the last 10 percent of the way there
as a functionalist and as a computational neuroscientist computational mind approach as
you've said it would seem that the concept of AI consciousness in some sense like our consciousness
is a certainty that may not be within decades it may take a long time but it doesn't seem to be any
in principle inhibition to that given that philosophical foundation is that is that fair
well um well first of all um I mean a certainty is a strong claim with respect to any big
philosophical question that's why I don't have that level of I mean we just need to look at the history
of philosophy with great thinkers disagree with one another so at least some of them have been wrong
about really important things and perhaps all of them but at least some right that we know
and uh so they all can't be right but they all can but but they uh but they all can be wrong
right they could all be wrong but they can't all be right since they contradict one another and so
clearly at the matter level one has to have a lot of humility about one's views about any of these
matters um but um even if we assume computationism it's certainly not a given that future AIs actually
will be conscious it would just demonstrate that in principle there could be but it might still
require the design of certain kinds of AIs to realize that possibility but I'm saying in in
principle I'm putting a hard question to you in principle it is a certainty that AI could be
conscious how it's achieved and when it's achieved that's completely uncertain but if you're a
computationalist and a functionalist I think that you have to submit to that certainty I mean
it certainly is an implication of functionalism or computationalism that AIs could in principle
be conscious now when I say that I'm a computationalist I don't mean that I am certain of it like
because I could be wrong about anything and in particular that okay I mean it seems like one of
the more amongst all the different philosophical views that I'm more or less sure about a lot of
things and that would come like higher up the end of philosophically controversial views that I feel
convinced about but certainly not like at 100% or anything like that okay is AI conscious as part
of a solved world or that's that's a tangential I think it would be very likely part of the
possibilities in a solved world that digital conscious minds could be created I think it's
one of the technological affordances that's technological maturity to do this now if I'm
wrong about consequentialism then you know it might still be true because you could then like
maybe engineer minds through through bioengineering or something that would basically achieve the
same thing now I think most of the book would still stand even if somehow you drop from the
package of assumptions of technical maturity next question is virtual immortality is that part of
a solved world because lifespan again I didn't can't remember every single word but I don't
don't recall that lifespan of being a critical part of the of the solved world 100 years or
something but in a solved world one might think there's physical immortality and then
concept of virtual immortality yeah well I mean immortality is a long time
I mean if we mean literally never dying that is possibly physically impossible I mean given
life of the heat death of the universe etc but if we are talking just about say extreme life
extension I certainly don't think there is an law of nature that says that humans can only live for
80 years or 100 years or whatever like once you achieve the ability to continuously repair
damage that occurs and then maybe reduce accident risk certainly many thousands of years would be
trivial and if you could upload into computers then your software which could just be kind of
error checked and redundantly stored etc and you could have astronomical lifespans the question
in that context becomes not so much whether you could keep sort of the physical substrate alive
and functioning but more what does it mean for if you want to retain a human like mind
I mean you can keep learning for 100 years and presumably for 200 years right but
after 200,000 years we don't really know whether the human mind would just kind of go stale and
rigid and eventually become kind of non-human if you want to continue to develop and learn
and change from experience the way we currently do which might seem really part of what it means
to be human it might be that if you continue doing that over sufficient large timescale you
eventually become something non-human like that might retain some of your earlier humanity in it
just like you retain something of your five-year-old self but it's still you're not I mean you're in
some sense the same person but in some sense also a different person and I think similarly we might
become like post-human versions of ourselves over really really long timescales. Nick two things
about the book that intrigued me randomly I just want to ask you quickly the first is you made a
comment that consciousness is not necessary for moral status so that surprised me.
Yeah so I'm inclined to that view I'm not fully confident but it's particularly relevant in the
context of digital minds and maybe even sub-human digital minds like once we are currently building
are the like the next generation of the current AI systems above beyond. I think consciousness would
be sufficient for moral status if you can suffer that that would mean that it matters how you're
treated but it seems possible to me that I did a little mind even if it doesn't have that but say
it has a conception of self as existing through time it's a really sophisticated mind it it has
preferences maybe life goals it can form relationships with human beings reciprocal
friendships etc I think in that case my intuition would strongly be that there would be ways of
treating it that would be wrong and so that it would have moral patience even if it weren't
conscious. And it could have those characteristics without having consciousness? Yeah I mean so those
characteristics are all functionally defined these are sort of behaviors and dispositions etc
so I mean it might be that depending on how willing you are to ascribe conscious experiences
to different kinds of systems maybe you would ascribe conscious experience
but I would say even conditionalizing on if not having conscious experiences if it had those
attributes it would be a strong candidate. That's an interesting position near the end you introduce
the concept of enchantment why? It seemed like another example of these
quiet values like the kind of star constellations that are a little bit hidden from us in our current
sort of brutish condition of grave needs and desperation but that could come into view if we
solve a lot of the practical problems and one of the things that if it's missing I think might
make a possible future condition look less attractive to us. If you take the extreme
example of the absence of enchantment imagine some future in which so this is not a utopia at all
but like just consider if you were sitting in a chamber and your job consisted of pressing
like maybe you were presented with some analytic problem in a little text bubble
and then you had to think hard and engage all your mental faculties use your knowledge and
creativities all of that would be there and then you sort of outputs the answer and then you get
as a reward a pleasure palette that also gave you your nutrients and you sort of shortcut our
rich interactions with reality and simplify it to a purely analytic exercise where all that matters
is kind of whether you choose action A, B or C so you still have causal impact you still have to use
a lot of your human capacities but something would seem to be missing I call this enchantment so right
now when we are in the world all the different parts of us are relevant and engaged so in addition
to your abstract decision you have intuitive decisions right you have emotions you have to
control and manage you have body language you have a physiology you have legs and like
and we interact with other people we don't just perceive whether they choose A, B and C we sort
of perceive we have a much higher bandwidth interface with reality and so that's I'm trying to
gesture because this value hasn't really been characterized but I think with a few more examples
like that one can get an intuitive sense there is something there that if that weren't there
then plausibly this this future would be deficient so Nick let's have some fun I'm going to ask you
some very big questions and ask you beg you for some very short answers and let's see what happens
so first what are the percentages for the following scenarios for AI superintelligence in the next
hundred years I picked a hundred years so here's the percentage some really bad events would occur
that AI will do substantial damage to humanity percentage zero to a hundred
like my p-doom as I call it now I've punted on this in the past I think it's like certainly
very non trivial it depends partly on what we do the degree to which we get our act together
but partly I think it's also baked in percentage a number I'm listening for a number
the percentage won't hold you to it no but other people might range give me how about a range
non-trivial is I mean yeah I mean I mean it seems like a bit bigger than 5% and lower than 95%
okay well that's we made some progress like on the twin prime the subject to revision
okay all right how about I mean a lot depends yeah I mean bad things are going to happen to humans
by default anyway I mean we all kind of either get cancer or heart disease or get shot or Alzheimer's
or something else over a hundred year timescale and so the default is that we are all kind of
going into the the slaughterhouse and the the question is like how low does the chance have to
be before one is it would be willing to take a gamble on something different that's one question
but then I think there also this would get beyond our current short form format
questions about how our AIs relate to other AIs out there in the infinite universe that are already
established okay so let's let's go on fine good some aspect of the utopian outcome exactly the
opposite of the AI say large does away largely with all work in a hundred years what's the likelihood
of that zero to a hundred so conditional on AI being developed or just whatever yeah
well conditional on it being developed yeah I mean that that I mean I'll work with the exception
of work where there is a specific demand that it be performed by human or where the consumer
cares about the process and with the asterisk that yeah okay these are really hard like I think
you're doing the mistake of trying to ask a philosopher to be very concise
I just see like an avalanche of considerations and qualifications and levels for each of these
very complex questions okay next question a percentage of a catastrophic human event
dealing with existential risks to humanity before 2050 not saying elimination of all human beings
like a huge asteroid but some huge catastrophic event well I think there are catastrophic events
all the time something that would would decimate would would would kill a large percentage of
humanity or eliminate you've dealt so much with existential risk yeah yeah well I mean so existential
risk is a subset right we actually permanently destroy the future and then I guess it depends a
little on how like how how how many people do you have to decimate like so like COVID is like
whatever half a percent or something and then it goes up from there I know I'd say a bigger number
you know 50% of humanity I mean most likely I think we'd either not have that or we have an
existential catastrophe that or something very close to like 50% is a kind of weirdly intermediate
number well it could happen some some pandemic engineered pandemic and maybe or maybe a thermal
nuclear but like engineered pandemic is probably the most likely way that 50% of us die in the
next couple of decades okay a little bit into your your total work the percentage that our universe
is a simulation I don't know if you've ever said that you gotta get my probability many have tried
so far none has succeeded and you shall not be the first to
do you think there is at least one solved world in the observable universe
in the observable universe no I mean unless yeah unless the simulation hypothesis is true in which
case the question becomes a bit wonky right I mean I don't think there is any in the observable
you know I mean the observable universe I think most likely intelligent life is
low density so that might be infinitely much of it but within any small finite region like the
observable universe it might just be so unlikely for it to evolve in the first place that we are
alone which would account for the Fermi paradox right that's an important issue that we deal
with and that's a good very good perspective AI consciousness true in awareness what's your
odds on that happening within a conceivable a thousand years is that a high likelihood
um yeah uh conditional on us not going extinct before I mean in fact I wouldn't be confident we
don't already have it in some AI systems I think as you zoom in on the concept of consciousness
this might be for another conversation but I think it becomes it's a lot more multi-dimensional
and vague than the naive you would have it and so the question might be less binary than one one
one supposes virtual immortality of our first person consciousness is that in principle possible
um well certainly uh extreme longevity as in like whatever thousands or millions of years
immortality as in never dying depends on physics which currently looks like it would
not admit of infinite information processing streams so virtual uh uploading of our first
person consciousness not not as a duplicate where it's like a um a very sharp identical twin but
literally my first person consciousness is you yeah it is uh it is in principle uh possible
I think so yeah okay um is life in the universe a happy accident or is life somehow built
into the ultimate laws of physics I mean both could be true it might be the it's built in that
for any planet it's it there's an extremely low chance of it happening and then it might also
be built in that there are enough planets that statistically it will happen on an astronomical
or infinite number of planets um how prevalent is life and mind in the universe you've said already
that it is very rare uh is it there a possibility that we are alone in terms of intelligent mind
in the observable universe I think that's a very real possibility but of course if the universe is
infinite as it looks like it is then with probability one uh that would be infinite
the many of these places were intelligent life as a result and then if you introduce the simulation
then it becomes more complex to answer it but yeah yeah I know the simulation is actually
self-solving in that in that to some other questions an infinite universe anything that's possible
will happen an infinite number of times and so the question becomes becomes very vague last question
does anything exist not explainable in terms of ultimate physics
um so yeah like I think for something to be explainable could mean two different things one
is that it's sort of supervenes on the laws of physics yes um which maybe is what you have in
mind uh well then yeah the laws of physics in our universe uh I think there could be a lot of
things that don't supervene on them if we are in a simulation there would be other layers of
reality which would have their own laws of physics etc um in our observable universe it looks like
everything supervenes on the laws of physics doesn't mean it's explainable in the sense that there is
like a useful intelligible you know 10 page text that would like make you more informed by talking
about basic physics like if you're trying to understand some cultural phenomena you would
you wouldn't start writing out the quantum equations or something yeah Nick this has been
terrific um I wish we could go on forever we'll definitely do this sooner than another 17 years
yeah I promise that uh Deep Utopia is a fantastic book recommended for everybody it is a vision
for the future but more than that it's more than that it's really an understanding of what
what the meaning of human life can be and it reflects on what we think of our own values so
we can go on viewers can watch hundreds of tv episodes and exclusive videos on cosmos life
cosmology and meaning on the closer to truth website and closer to the youtube channel
including of course those of Nick Bostrom thanks Nick thanks everyone for watching thank you very
much thank you for watching if you like this video please like and comment below you can support
closer to truth by subscribing
