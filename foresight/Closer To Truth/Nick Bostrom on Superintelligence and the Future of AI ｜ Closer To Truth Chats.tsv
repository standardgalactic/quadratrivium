start	end	text
0	6880	I think you're doing the mistake of trying to ask a philosopher to be very concise.
6880	16760	I just see like an avalanche of considerations and qualifications and levels for each of these very complex questions.
16760	17760	Okay.
26040	28120	Welcome to Closer to Truth.
28120	34960	I'm speaking with futurist visionary Nick Bostrom about his vital, far-sighted, engaging new book,
34960	38880	Deep Utopia, Life and Meaning in a Solved World.
38880	40080	I loved it.
40080	46560	It's an exhilarating romp of ultimate technology centered on AI, how it might work, what it could mean.
46560	49200	It's a prescient manual for the future.
49200	52680	It's an innovative treatise on the meaning of life.
52680	54840	Welcome, Nick. It's great to see you again.
54840	56320	Good to see you again.
56320	58360	Congrats on Deep Utopia.
58360	59640	We'll discuss it in depth.
59640	63560	But to begin, I'd like to get your world overview, the setting for your book.
63560	69960	When we first did our Closer to Truth discussion in Oxford in 2007, 17 years ago,
69960	75400	we discussed the simulation argument, fine-tuning, anthropic selection, the doomsday argument.
75400	82600	How would you characterize the last 17 years or so in terms of technological and intellectual development,
82600	86280	especially the importance of AI?
86360	92320	Well, I mean, it's all happening now, as we've entered the Atollius era.
92320	99320	I think the last, especially since 2012-2014, with the Deep Learning Revolution,
99320	104320	I think we've been on a kind of up-ramp of AI capabilities,
104320	109360	kicking into even higher gear with the release of ChatGPT.
109360	113880	And in the last two years or so, we've really seen it hit the mainstream.
113920	117560	We're now the White House and key policy makers all over the world
117560	120360	are starting to debate the future of AI.
120360	123240	So it's a remarkable time.
123240	125840	And when did you actually plan this book?
125840	131600	Because it's obviously, in essence, the other side of your superintelligence,
131600	136440	2014, where you were prescient in warning about the dangers of AI.
136440	140680	And so the last two years has been a high focus on the dangers.
140680	143840	Now you've moved on to the opportunities.
143840	149640	So when did you have that a bit of a transformal insight?
152320	155920	I've been working on it for probably around six years or so.
155920	159240	It wasn't ever planned.
159240	162000	It just kind of happened.
163240	167640	I didn't start out with like some particular set of theses.
167640	171040	I wanted to defend and elaborate on.
171040	177520	I felt an urge to start writing, and then it eventually grew into DP Topia.
177520	180160	Yeah, and I've seen where you've said that the style of the book,
180160	188640	which is very unusual, it's a new literary style involving dialogue
188640	200320	with different characters, your own persona in a not entirely actual form.
200320	209640	But in dialogue with other people and form of a lecture series over a period of a week,
209640	214160	I think you've said that whole structure wasn't planned.
214160	216360	It happened organically.
216440	221160	Yeah, it's just the way it happened for better or worse.
221160	228520	But that's I do think the form actually does match the content.
228520	234680	It's not a book so much about conclusions as it is a book about questions
234680	239840	and helping the reader to start to think about these problems
239840	241920	and form their own views ultimately.
242000	248160	It's also meant to be not just something that transmits certain concepts and ideas,
248160	255040	but also it's meant to be a reading experience that you might have to work to get through it.
255040	259800	But ultimately, I'm hoping it will kind of put you in a better place
259800	265160	to reflect on questions about what art humanity's destiny be.
265160	267280	I think that's an accurate description.
267280	268960	I found myself very engaged.
268960	277080	I was looking for more of the arguments as that we've had in the past in a very positive sense.
277080	278200	But this book is different.
278200	283360	You do get the arc of various arguments on different things.
283360	284600	We'll talk about that.
284600	294480	But you are brought into that in this engaging intellectual, semi-fictional avatar environment.
294480	296240	Yeah, yeah.
296240	303360	And the other benefit of this sort of having different characters and different bits
303360	310560	is that it makes it easier to explore several different viewpoints,
310560	316560	which I wanted to do and allow each one to be developed in its own right to its fullest extent
316560	321440	and then to kind of collide different perspectives and ideas.
321440	324080	Just as, I mean, you're interested in physics, right?
324080	330160	So with a particle accelerator, you sort of accelerate little particles to enormous energies
330160	331840	and then smash them into one another.
331840	337520	And in those extreme conditions, sometimes you can see the basic principles at work
337520	342520	that we can then infer are at work all the time in ordinary conditions as well.
342520	344240	It's just hard to observe.
344240	353200	And so similarly, this conceit of a plastic world,
353200	356920	a condition in which technology has reached its full maturity
356920	359160	and all practical problems have been solved.
359160	361160	It's an extreme condition.
361160	366120	But I think we can then see values kind of smashing into one another
366120	371080	that we normally can sort of hand wave and then just because they are obscured
371080	376680	by so many practical necessities that kind of occupy most of our contemporary existence.
376680	378840	I think that's a very good characterization.
378840	383720	The book is creating an extreme condition and particle accelerators do that
383720	385800	and physics black holes do that in physics.
385800	391080	That's an extreme condition where people study black holes.
391080	392920	It's not just for the black holes themselves,
392920	396080	but it's subjecting the laws of physics to extreme conditions.
396080	397400	And you learn a lot.
397400	399800	And I think that's a very good characterization of the book.
401480	401960	Yeah.
401960	408880	So I think it's like, I mean, for people who have read it, they will know,
408880	414440	but it's not a book that is really trying to make predictions.
414440	420200	And nor is it trying to offer practical solutions to what we should do next.
420200	422440	I mean, a lot of my other work focuses on that.
422440	423360	Right.
423360	430240	This takes basically as an assumption or a postulate, if you want, that things go well
430240	434560	in order then to be able to ask the questions of what then,
435920	439520	what would be the meaning of human existence?
439520	441040	What would give us purpose in life?
441760	445840	If the whole thing unfolds like everything is perfect,
445840	448560	governance problems is all the alignment problem is solved,
448560	452400	like all these things, but what then would occupy our lives?
453440	456640	Sometimes you never actually get to even ask that question
456640	459760	because there are so many other questions that kind of crowd in before it.
459760	463360	So I just wanted to postulate that and then focus this book entirely
463360	467680	on the set of questions that arise in this hypothetical future condition.
467680	471360	We're going to get into all of it, but let me first give a more formal bio.
471920	474400	Nick Bostrom is a professor at Oxford University,
474400	478400	where he heads the Future of Humanity Institute as its founding director.
479120	482880	With a background in theoretical physics, computational neuroscience,
482880	487440	logic and artificial intelligence, Nick has pioneered contemporary thinking
487440	490320	about existential risk, the simulation argument,
490320	492880	and the vulnerable world hypothesis, among others.
493520	497920	He is the most cited professional philosopher in the world age 50 or under
497920	502560	and is the author of some 200 publications, including anthropic bias,
502560	506480	global catastrophic risks, human enhancement and superintelligence,
506480	509360	the prescient book on the dangers of AI,
509360	513600	but now we're going to look at the extreme condition if all goes well.
513680	515040	So Nick, your book,
515040	518240	Deep Utopia, Life and Meaning in a Solved World.
518240	522880	Let's start with a simple definition of what is a solved world
522880	525040	and what motivates your focus on it.
526560	531040	Well, I am referring to a hypothetical condition
531040	535360	where basically all practical problems have been solved.
535360	539600	So think, first of all, a condition of technical maturity.
539600	541680	So we have super advanced AIs.
542240	545040	Maybe they have helped us develop all kinds of other technologies,
545040	548880	medical technologies, virtual reality, et cetera, et cetera.
548880	553360	So that's part of what it would mean for the world to be solved.
554000	557760	And then on top of that, we also make the assumption that
558720	562800	all the kind of governance problems of the world have been solved
562800	564320	to the extent that they can be solved.
564320	566800	So we imagine we set the site,
566880	570160	questions of war and conflict and oppression and inequality
570160	572080	and all the rest of it.
572080	575440	So but then there remains a big kind of problem,
575440	578640	which is ultimately a problem of value,
578640	581520	which is that under these ideal conditions,
583280	587040	what kind of lives would we want to live?
589040	590000	And yeah, that's...
590880	594640	And that's a very important way to frame the book
594640	596880	because you're not saying all of these problems
598080	601200	that are solved are easy to solve or will be solved.
601200	604160	But if you do solve it, what does that leave?
604160	605200	And it leaves value.
605200	606880	So one question that I have is,
606880	609520	do you distinguish between meaning and purpose?
609520	612640	We use those two terms sometimes interchangeably,
612640	616400	but I think we can tease apart a difference.
617280	618400	Meaning in the title,
618400	620400	but throughout the book, you have purpose as well.
622240	622960	Yeah, that's right.
622960	627840	So I think of purpose as slightly narrower concept
627840	632080	as sort of having a reason for doing something
632080	633520	or for putting out some effort.
635200	638800	And then meaning, I mean, that is discussed in the book,
638800	641280	but might be a certain kind of purpose
641280	643280	or purpose plus something else.
643280	643600	Okay.
645680	648640	You present, just to give a sense of the environment,
649360	656400	utopic taxonomy where you have different levels of utopia
656400	659360	that can give us a richer understanding of it.
659920	662560	So let me just give you the list
662560	664320	and just explain each one very quickly.
664320	666480	The first is government and cultural utopia.
668880	674640	Yeah, this is I think the most familiar kind of utopia
674640	675920	we find in the literature,
676800	680640	where people imagine a better way for society to be organized,
681280	684480	better political institutions, different schools,
685280	687200	maybe different gender roles,
687200	690160	but usually set within more or less
690160	693440	recognizably contemporary technological context.
693440	696640	So people, there's still work that needs to be done
696640	699760	and you can organize how much power the workers have
699760	701200	or how the work is divided,
701200	704880	but there have to be people growing food, et cetera.
704880	709440	So that's the most familiar and basic kind of utopia.
709440	711920	The second level is post-scarcity utopia.
712480	713600	Sounds like we know what it means,
713600	716080	but if you could define it more clearly.
716080	722480	Yeah, so this is the more radical vision of a condition
722480	727920	in which humans have plenty of all that we need materially.
728960	731200	So there are these kind of,
731200	733680	it's more like fantasy in the past,
733680	740400	but various kind of the land of cocaine
740400	746880	was this medieval peasant dream basically
746880	750880	of some condition where the rivers would overflow
750880	755440	with wine and roasted turkeys would just land on the plate
755440	758560	and that would be a kind of continuous feasting.
759280	761760	And you could easily see how that on its own
762320	763440	would have huge attraction
763520	767440	if you were a kind of agricultural labor
767440	770000	who spent your whole life grinding away,
770640	773840	barely getting enough porridge to feed your children
773840	775440	and your joints were aching
775440	777760	from all this backbreaking work that you were doing.
777760	779440	Then this on itself,
779440	782400	just being able to rest and eat as much as you want
782400	786800	would already be like enough of a kind of vision.
788000	790480	Third level is post-work utopia.
791440	793600	Right, so this is the idea that not just
793600	796800	is there plenty to consume,
796800	800800	but that the production of all this plenty
800800	803920	doesn't require human economic labor.
805520	808800	And this has started popping up more recently
808800	810800	in conversations about the future of AI
811840	813680	where people are wondering,
813680	817920	will this advance that we see lead to human unemployment?
817920	819440	We can automate more and more things.
821440	823600	And if you imagine that running its full course,
823600	826240	then maybe eventually you could automate
826240	829040	almost all human economic labor
829040	832000	and then you would have this post-work condition.
832720	836480	The fourth and fifth to get a little more complicated
836480	838320	to understand, but let's do it now.
838960	841680	Fourth is post-instrumental utopia.
842480	845840	So now we're getting into a more radical conception.
847920	850640	And usually current conversations about these issues
850640	854000	stop short before we reach this idea.
854640	856960	But if you really think through what it would mean
857760	861120	for AI to attain its full potential
861680	863840	and then all the other technological advances
863840	867280	that this kind of machine superintelligence
867280	868000	could bring about,
869600	872480	it's not just that we wouldn't have to go into the office
872480	875600	and type on word processors or like hammer away
875600	878320	at construction sites, et cetera.
879440	882560	But also a lot of the other things we need to do
883360	886400	in our daily lives could be automated as well.
888560	890880	So if you think about, if you didn't have to work,
890880	893600	well, then like typical answers would be, well,
895280	897680	maybe somebody likes fitness or something,
897680	899840	so they could spend more time exercising.
899840	901200	But like in this condition,
901200	902800	you could pop a pill instead
902800	906560	and get the same physiological and psychological effects
906560	911680	that spending an hour on the Stairmaster would provide.
911680	915360	And so then why would you really need to go to,
915360	916640	kind of would lose its point
916640	918720	to go to the gym in those conditions?
918720	923200	And you can then start to kind of almost do case studies
923200	926160	on activities that fill our current lives.
926160	927360	And for almost all of them,
927360	930400	you soon see that they have a certain structure,
930400	932000	which is that we do a certain thing,
932000	935680	put in some effort in order then to achieve some other thing.
936480	940320	So brush your teeth, because otherwise eventually
940320	942480	you will have tooth decay and gum decay.
942480	946240	And so in order to get the outcome of a healthy mouth,
946240	949520	you need to spend a few minutes every day brushing your teeth
949520	950480	and gone to the gym.
952320	955680	Or you need to, like you want to understand mathematics, let's say.
955680	957200	So then the only way to do that
957200	959440	is to put in some effort to study mathematics.
960400	963920	And so the effort is motivated by the goal
963920	966400	that it is trying to achieve outside the effort itself.
967680	971840	And a lot of the things that we are doing has that structure.
971840	974400	But now if you could get the goal, the end point,
974400	976000	without having to put in the effort,
976960	980880	then it seems to pull out the rug under the activity itself.
981840	984160	At least it's threatened with the sense of being pointless.
985920	987520	So that's the problem you confront
987600	989520	in this kind of post-instrumental case.
989520	990640	So an example of that,
990640	992640	instead of studying higher mathematics,
992640	994640	you could have an injection of nanobots
995840	998720	that could analyze every synapse in your brain
998720	1002240	and then figure out how to change them a little bit
1002240	1006320	so I can understand algebraic geometry or something.
1006320	1007600	Right, that's right.
1007600	1009920	And that would be fast and effortless.
1010960	1013600	Or even things we do for fun.
1014560	1016240	So there might be various activities
1017200	1018800	because it gives you pleasure and joy.
1020400	1022160	That seems kind of unavoidable.
1022160	1023840	But even there, if you think about it,
1023840	1027440	you could have more direct ways of experiencing
1027440	1029360	the same positive emotions,
1029360	1032080	like a kind of some super drugs
1032800	1034960	that could give you the pleasure and the joy
1034960	1038000	without you having to spend an hour gardening
1038000	1042560	or doing whatever it is that you're watching movies.
1043040	1046160	So this brings up a very important point of the book
1046960	1050080	in terms of what are our real values.
1050080	1052880	Because when I read that,
1052880	1055120	and obviously a very intriguing,
1055120	1058240	remarkable way of thinking and very important,
1059280	1060320	I was asking myself,
1060320	1063600	are there any absolute values in a solved world?
1063600	1065200	And so the way to describe it,
1065200	1066240	as you started to say,
1066240	1069280	is if we could take non-harmful drugs
1069280	1071760	or AI neural implants
1071760	1075280	that would maintain a state of perpetual ecstasy,
1075280	1079120	whatever your ecstasy would happen to be,
1079120	1083520	or it can switch from a physical bodily ecstasy
1083520	1086720	to an artistic ecstasy or intellectual one.
1086720	1088000	And if that could be all done,
1090640	1092320	who could gain say that
1092320	1096640	if there's no kind of supernatural value
1096640	1098720	that you would put into it?
1099680	1103120	I think that is a fundamental theme of your book
1103120	1106160	about how do we develop the kinds of values
1106160	1107440	if these things are possible.
1107440	1111200	You're not saying these things are remotely
1111200	1114560	in the near or mid or even long term,
1114560	1116320	but they are the extreme condition
1116320	1119680	that you talked about which then exposes
1119680	1123120	what is the nature of absolute values if there are any.
1124480	1124800	Right.
1125760	1129600	Now, it is possible that they might be in the long term
1129600	1131040	or even mid or near term,
1131040	1133760	depending on how fast the AI revolution unfolds
1134400	1135760	and what the outcome of that is.
1135760	1139280	I actually think the time scales for radical transformation
1139280	1141920	might be shorter than most people realize
1141920	1145920	if AI continues to speed ahead.
1145920	1148000	What's your best guess on that?
1148560	1148880	Well,
1149280	1155760	I mean, my timelines have shortened somewhat,
1155760	1158640	at least since this previous book,
1158640	1161360	Superintelligence, was published in 2014.
1162480	1164800	I think we are currently looking on timelines
1164800	1167840	that are on the shorter end of the distribution.
1169120	1170080	So it's hard to say,
1170080	1175360	but I mean, it could be years or maybe a decade or perhaps more,
1175360	1178320	but at least I think there's a non-trivial probability
1178320	1182640	that we are now kind of on the accelerating slope of this,
1182640	1183760	but time will tell.
1184960	1186720	In any way, that's not central to the book,
1186720	1188400	even if you thought this would take millions
1188400	1189120	or never happen.
1189120	1189680	It is still.
1190560	1192320	But there are two ways of thinking about this.
1192320	1193600	You could either just read it as,
1194800	1196960	these are perpetual philosophical questions
1196960	1198080	that humanity ponderous,
1198080	1199840	and here is kind of a thought experiment
1199840	1201040	that helps you think about them.
1201040	1204880	And I think it's fine if you just read it like that.
1204880	1207200	For me, there is also this actual real possibility
1207200	1209920	that we might soon enter a condition like this,
1209920	1212240	or we might have to make decisions soon
1213280	1214640	about what kind of future we want,
1214640	1216720	if we want to stare towards something like this
1216720	1217920	or some other version of this.
1218960	1222720	So there is this kind of underlying practical motivation
1222720	1225360	for me in terms of writing this book,
1225360	1227200	but that's optional.
1229040	1230160	So yeah.
1230880	1231760	I appreciate that.
1231760	1234480	I was more on the former, on the thought experiment,
1235280	1238960	and I would put the date measured in hundreds of years,
1238960	1242160	if not thousands, to achieve what you're saying,
1242160	1244400	but I'm cautious.
1244400	1245760	I've become cautious as you,
1246720	1249040	and I hear you and respect your views,
1249040	1251840	so I'm a little less sure of what I thought before.
1251840	1255760	Anyway, we want to get the fifth category
1255760	1258960	of the utopic taxonomy,
1258960	1261200	which you call plastic utopia.
1261600	1264720	Even going beyond the post-instrumental utopia,
1265360	1266960	what is the plastic utopia?
1268720	1270480	Well, we alluded to it slightly,
1270480	1273280	which is the idea that in addition to
1274160	1276800	having these other properties of being post-instrumental,
1277440	1280880	we also, in the plastic utopia condition,
1280880	1284000	have complete control over ourselves.
1284560	1286800	So our mental states, our psychology,
1286800	1289840	our cognitive architecture, our bodies becomes malleable.
1291360	1293440	So it's not just that we imagine human beings,
1293440	1295520	as we are now, placed in this condition
1295520	1296800	where we don't have to work
1296800	1298880	and where we don't have to put out any effort
1298880	1299680	if we don't want to.
1299680	1302000	We could just press buttons and get what we want,
1302000	1305680	but we ourselves, as well, become something we can choose.
1306480	1309040	So you wouldn't have to work on yourself
1310000	1312720	to build a better character in a plastic utopia.
1312720	1314480	If you wanted to, instead,
1314480	1317280	you could sort of request of your AIG
1317360	1320800	need to sort of rewire your synopsis
1320800	1323120	so that you became this different kind of person
1323120	1325040	or to experience pleasure all the time
1325040	1328000	or to become smarter or kinder or whatever else.
1329280	1335120	So that makes it even more like solved or dissolved or liquid.
1335120	1337920	Like you enter this context
1337920	1340880	where everything seems kind of fluid and up for grabs,
1340880	1344080	and it's hard to find on this firm ground to stand on.
1345040	1348720	Yeah, it's a wonderful way
1348720	1352080	of seeing what an extreme condition is,
1353040	1355600	because I would have not come up with those five.
1355600	1356960	I might have had three or four,
1358000	1360240	but at that level,
1360240	1363520	it really very beautifully defines
1364080	1367440	what an extreme condition for humanity could be in the future,
1367440	1370640	and therefore it gives the book its real punch.
1370640	1372480	So one issue that you deal with
1374080	1376160	especially as you get to all of those
1376160	1377680	is the question of boredom
1379120	1382320	and how much of our value system is based upon
1383280	1388960	the need to or the lack of control and the uncertainty
1388960	1390320	and what happens whenever this,
1390320	1392080	you know, this is the same kind of problem
1392080	1394480	that traditional religions,
1394480	1396880	both East and West have to deal with
1396880	1399280	whether you're dealing with nirvana
1399280	1403520	after the innumerable cycles of birth and death and rebirth,
1403520	1408400	and then you reach nirvana or in the Judeo-Christian,
1408400	1411920	Islamic, Abrahamic concept of heaven,
1411920	1413120	eternal life in heaven.
1413120	1418240	I mean, that's a sort of an end question of boredom
1418240	1422160	that occurs in any of these eschatological ideas.
1423600	1425440	Yeah, it's quite fascinating
1425440	1427040	if you think far enough in this direction,
1427040	1431360	you do start to sort of bottom up against theological questions
1431440	1434480	or at least questions that have traditionally been
1435760	1439280	discussed in religious contexts about the afterlife, etc.
1440400	1446000	I try to not trespass onto that terrain in the book.
1446000	1449200	I have this, there's this other fictional character,
1449200	1451840	the fictional Bostrom character is giving these lectures
1451840	1455040	and then sometimes he's asked questions by the students
1455040	1457440	and occasionally he sort of refers to,
1457440	1460640	well, you have to take that up with Professor Grossweiter
1460640	1462240	which is like another character
1462240	1463840	who doesn't make an appearance in the book
1463840	1465840	but he's like the theologian
1465840	1468560	or the person who could answer their questions on that.
1470560	1472960	But on boredom, yeah, so this is an example
1472960	1475040	where I think it's important to distinguish
1476960	1479040	the two different senses of boredom
1479040	1482320	which is a subjective sense and an objective sense.
1482320	1485280	So clearly we have a subjective concept of boredom
1485280	1487040	like somebody might just feel bored
1487520	1493520	and that would trivially be easy to abolish in Utopia.
1493520	1496720	I mean, it follows directly from the condition of plasticity
1496720	1500720	that this feeling is like a subjective state of your brain.
1500720	1505040	You could rewire that so that you would always feel excited
1505040	1509520	or interested or whatever antonyms to boredom you want to use.
1509520	1514560	And the question then is whether there is also
1515520	1521120	some notion of objective boredom or boringness
1522320	1525440	whether certain activities or experiences
1525440	1527520	are such that they are objectively boring
1527520	1530000	like meaning perhaps that it would be appropriate
1530000	1533920	to feel subjectively bored if we engaged in them.
1534960	1538880	So if you imagine somebody to take an example
1538880	1542800	actually from the philosophical literature of a grass counter
1542800	1547280	so somebody who spends his life counting the blades of grass
1547280	1550880	on a particular college lawn, we might think
1552320	1555520	that that's an activity that is objectively boring
1555520	1557840	whether or not he happens to feel excited about it
1557840	1560800	it's not appropriate to be really interested in grass counting
1560800	1563600	because it's like too monotonous or insignificant
1563600	1566240	or has some other sort of deficit.
1568400	1571440	And philosophers disagree about whether like
1572400	1575600	there is like some kind of firm normative basis for making that
1575600	1576880	but I think it's an intuition that
1579600	1581520	some but not all people would have that
1581520	1585680	it would be bad if the future consisted of merely of activities
1585680	1586800	like counting grass.
1586800	1590080	No matter how thrilled the people doing the grass counting were.
1591280	1596320	That's an objective value that's kind of is a superset
1596320	1597920	to everything else we're talking about.
1598480	1599440	Yeah potentially.
1599520	1602800	And so I'm able to in a plastic world change my brain
1602800	1607600	to where I am excited about every new grass that I count
1607600	1609520	and what's going to happen at the next number
1609520	1611360	and I'm genuinely excited about that
1611360	1615760	and I've changed my brain to think and so subjectively
1615760	1620400	I'm excited about life counting all these grasses
1620400	1623920	or as I think you have in the book table legs
1624640	1626640	200 and somewhat thousand table legs
1626640	1633360	or that that potentially is in some objective sense
1633920	1634960	is suboptimal.
1636080	1638560	Yeah that's possible to hold that view and
1641280	1645680	now if one does have that view that it would be objectively
1646640	1652080	bad to have nothing in one's life than you know counting grass
1652800	1657360	then that kind of also potentially imposes a constraint
1657360	1659440	on the subjective experience of boredom.
1659440	1662000	Like if it actually is objectively bad to do that
1662000	1664880	then you might think it would be also bad to change
1664880	1667760	your subjective attitude so that you found great interest
1667760	1669360	is something that would be boring.
1669360	1671520	So then you might end up with a situation
1671520	1674320	where you couldn't eliminate boredom in the future
1674320	1676160	neither in the objective or objective sense.
1677680	1681360	And so then but so this possibility of the objective value
1682080	1684720	there makes the discussion more complex.
1684720	1688000	Like eliminating the subjective if that's all there is trivial
1688000	1690880	given the postulates but once they introduce this possibility
1690880	1692880	of the objective then it becomes like a much more
1692880	1696480	intricate conversation to what extent we would be able to do
1696480	1699200	something without violating that.
1699200	1704240	Now I think at least with respect to the value of
1704240	1707520	interestingness and there is a bunch of these different
1707520	1709600	values that they're kind of related but different
1709600	1713280	but if we focus on interestingness I think there is at least
1714480	1718800	large scope before increasing the amount of subjective
1718800	1722480	and objective interestingness including in these utopian lives.
1722480	1725920	I think even if there is some objective element to what's
1725920	1730080	boring and what's interesting I think it's has a large
1730080	1732080	zone of indeterminacy.
1732080	1736640	I mean you can just look at the current human distribution.
1736640	1742640	I have a good friend and colleague who tells me he's never bored
1742640	1747760	and he's interested in as far as I can tell literally everything except sport.
1748960	1751680	He writes papers on all kinds of topics.
1751680	1752880	He knows about everything.
1752880	1755520	He goes to every kind of conference and finds
1755520	1757360	interesting things to discuss with every person.
1757360	1762640	Like it doesn't seem to me that there is anything deficient about his human life.
1763600	1772000	In fact if anything it seems to benefit and be like a greater person for this
1772000	1776640	this property that he has and obviously that goes down ultimately to some neurochemical
1776640	1783520	idiosyncrasies of his brain that's he and I think for all of us I think we could expand
1783520	1790080	the range of things in which we take an interest greatly before we would reach
1790080	1792560	this point where we would just be counting leaves of grass.
1793600	1799840	Moreover I think possibly it would be appropriate to expand it even further than that.
1799840	1806720	Like maybe if we have reached a condition where we had sort of exhausted all the
1806720	1811600	most obviously interesting things we had discovered all the really fundamental loss of
1813120	1818640	nature you know solved consciousness and like the biggest questions had all been answered.
1818720	1821840	Like it would seem perfectly appropriate in that situation to begin to take an
1821840	1826720	interest in the slightly smaller questions and it's not clear that one couldn't go very
1826720	1832480	very far in that direction before I reached a point where it would be sort of objectively
1832480	1834960	bad to take a further interest.
1834960	1837600	Is religion relevant in a solved world?
1839680	1846560	Yes potentially very relevant although it's also arguably very relevant in the current world
1847120	1854400	and one might say something more interesting perhaps if one looks at some other values that
1854400	1860080	seem not so relevant in the current world but that could potentially become more relevant
1860080	1860880	in this whole world.
1860880	1869920	I think that there might be a lot of subtle values that exist now but we don't really
1869920	1876080	see them very much just as we don't see the stars during daytime because it's like
1877040	1884320	you know such a like brighter present and analogously there are such stark moral
1884320	1885520	imperatives right now.
1887920	1893360	Calamities of all sorts you need to take care of your kids you there are people starving in
1893360	1899680	the world or being shot at etc etc so so many horrors and an urgent obvious pressing
1900880	1907520	ethical needs to fix things that it would almost be frivolous now to spend too much time
1909040	1915280	fretting about more subtle quiet values but if we ever reached a condition where these
1915280	1922400	pressing needs were taken care of then I think we might be able to see a whole panoply
1923120	1925440	of these subtler values like for example
1925840	1930800	various kinds of traditions that it would be nice to honor authentically
1932720	1939840	ancestors who you know maybe we think of our lost parents once in a while but there is like so
1939840	1945040	many more people have lived wonderful lives and you know maybe deserve more thought and
1945040	1949360	consideration we don't have time our daily lives keep us busy but if you didn't have that
1949600	1955440	but if you didn't have that why not various aesthetic qualities you could imagine making
1955440	1961440	your life more into a kind of artwork where every relationship was not just a source of
1961440	1965600	I don't know relaxation or final satisfaction but also something actually beautiful that you
1965600	1971280	were kind of constructing together etc etc and we don't now have the luxury to kind of really
1971280	1975360	develop a fine sensibility for those but I think it would be entirely appropriate
1976000	1982000	that once the urgencies are removed to to sort of tune up like almost like our eyes
1982000	1986640	dilate at night right so it can take in more light similar in this condition our moral
1986640	1991600	sensibilities and sensibilities for subtle values I think should dilate and this is a
1991600	1999440	larger definition of religion as we might have it in today's world enabled by the the solved world
2000160	2009920	the boundaries of religion become broader yeah potentially but again it also potentially
2009920	2016320	is very important today so it might be one of those things that is very urgent today even
2016320	2022480	with other urgencies pressing in upon us like many religious people would say that yes you have all
2022480	2026560	these practical things you should do but you should also set the time for worship etc even
2026560	2033360	though it conflicts with but but even more so obviously in this condition and I mean we would
2033360	2038400	be more like I guess potentially like monks and nuns that have the time to fully devote themselves
2039120	2044160	to contemplating the divine when I first heard of the book and started it
2044960	2050080	the first question one of the first questions came into my mind is how does a solved world the
2050080	2057760	the bostrom solved world articulate with the marxist pure communism and as I started to go
2057760	2066000	through the book to me that question became pretty obvious that the the that there would be a high
2066000	2075040	correlation potentially between the first at least two of the utopia taxonomy levels the
2075040	2080720	government and culture and then the post scarcity and then maybe into the post work as well but
2081600	2087040	pure communism as it's been envisioned in the past or even in in few cases in the present
2087920	2095040	doesn't even deal with the points four and five right and and I mean I think I'm not really a
2095040	2100800	marx scholar but I think he just has a few lines really about what would you know ultimately be
2100800	2106240	the outcome of if the whole communist product succeeded and I think he refers to this whatever
2106240	2112160	is it like fishing in the morning and hunting in the afternoon and reading poetry in the evening
2112160	2118480	so that that sounds like not even a fully post work utopia but like maybe an abundance diminished
2118480	2125520	work utopia plus a sort of vision of social cultural utopia I guess right right Nick let's
2125600	2131520	switch and look really long term and very visionary what I call your approach to ultimate
2131520	2139040	utilitarianism I love the section a quantitative analysis of the potential for happiness or
2139040	2147120	fulfillment for all sentient beings if if the cosmic endowment could be maximally saturated
2147120	2154000	with sentience so some of the numbers you give you estimate 10 to the 35th possible human lives
2154000	2159440	derived from human lives originating on earth to populate the observable universe that's a
2159440	2164720	think a hundred billion trillion trillion that's your minimum then you go up to 10 to the 43rd and
2164720	2172560	then if you switch to digital lives which adds a lot of complex value you get a computing power
2172560	2177920	of the universe of at least 10 to the 58th which is 10 billion trillion trillion trillion four
2177920	2187360	trillions there in terms of ultimate sentience so what I love the calculation but walk me through
2188000	2195920	the importance of that in our ultimate thinking and also in terms of the concept of meaning
2195920	2204480	and purpose which is the purpose of your book well I mean it's like some big number basically
2204480	2209600	very big number but I mean it's in the context of the book it's a little handout
2209600	2221280	as the postroom gives out but yeah so I'm not a utilitarian I'm often mistaken for one because
2222160	2230160	in some of my writings I have explored and analyzed the implications of assuming
2230160	2237280	an assumption of utilitarianism or aggregative consequentialism because it's a view that you know
2238880	2245120	significant fraction of moral philosophers have held and that I think maybe deserves at least
2245120	2249280	some weight even if one doesn't actually embrace it and then it's interesting to see what follows
2249280	2256480	if one actually takes that perspective seriously and hence in my earlier work this you know the
2256560	2261040	focus on existential risks as those few things that could actually permanently
2261600	2268400	destroy our future if one counts these possible future lives the same way as actually currently
2268400	2275200	existing lives as certain flavors of utilitarianism would do then they just seem to dominate and you
2275200	2279680	get a bunch of interesting and then there's like a further complication on that which is if you
2280160	2287280	literally do this try to do this expected utility calculation you find that scenarios in which somehow
2287280	2292000	even if they are very unlikely but somehow infinite values could be realized like maybe we
2292000	2295680	are wrong about physics and there's like some actual way of producing infinite and then those
2295680	2300480	tend to dominate even if they have a very very tiny finite and then you get into infinitarian
2300480	2304480	paralysis and there's like a whole yeah so I think that's interesting in its own right but it's not
2304480	2313920	really the topic of the book which more focuses on not not how you aggregate big values or what
2313920	2318640	our obligations are but like from our point of view like what would be the best possible future
2319200	2326080	for Robert or for the for you the the viewer or for any of us like if you literally could
2326080	2331840	imagine the best possible way for your future life to unfold and perhaps we restricted by like
2331840	2338080	the loss of physics etc but and then trying to think from the inside like how we would furnish
2338080	2346720	that life with activity experiences relationships etc and then ultimately like you know if your
2346720	2351120	question is what you should you do now as a moral actor then you would have to somehow integrate all
2351120	2358320	these different perspectives whatever weight you would put on utilitarian views or the ontological
2358320	2366080	views or virtue ethics views and the bunch of other stuff but the yeah the book doesn't really try to
2367440	2374000	pick between these different moral theories it doesn't really in general focus so much on numbers
2374000	2380480	or on formal structures and aggregation but more tried to sort of which often is done in
2380480	2385840	contemporary analytic ethics it kind of almost sees the values a little bit like
2387760	2392160	black pork and that might not be exactly right but this ties to look from the inside
2392880	2395840	on the values which values do you actually have like at the object level
2397120	2402080	and what would it take to realize them I'm not sure whether that answers your question but
2403040	2411600	now there is this yeah I guess I guess like one one way in which this larger view of
2412320	2419120	the bigness of the future could and does come into the book is insofar as we value
2420240	2425120	significance like having significant impact on the world for example if that's the version of
2425120	2433280	significance right now it looks like we have we are extraordinarily well positioned to have
2434000	2441280	huge impact on the world because well a there's a lot of just ordinary needs in the world and you as
2441280	2448080	an you know if you know if we imagine you're like a relatively well-off person with health and
2448080	2452240	intelligence in a wealthy country with a good education like probably most of your viewers are
2452960	2456800	you have a lot of opportunity just to help a bunch of people and to try to make some positive
2456800	2462400	difference so that already gives your life potential significance that is maybe greater
2462400	2466080	than one human life's worth of significance like you could save many people's lives or
2466640	2471840	but then on top of that you have this idea that maybe we are near a big fulcrum of human history
2471840	2476720	where if this whole thing the AI transition and the rest of it is going to happen perhaps within
2476720	2481920	our lifetime then like you can multiply that manifold like if you could even slightly notch
2481920	2486480	this big future in the right direction that would give your life even more causal significance
2488080	2492000	this is one thing that might be a lot harder for people living in utopia to have
2492960	2496080	this kind of significance because if all the problems are already solved
2496720	2501520	and whatever problems aren't yet solved are anyway much better work that by AIs
2502400	2507600	then humans might not be able to have significance in that sense and so to the extent that one
2507600	2516960	thinks it makes a life itself better to have this kind of significance these utopian lives might
2516960	2520560	lack that significance and therefore have a deficit of that particular value
2521680	2528240	and so there's a discussion around that and also the possibility of humans whether through AI
2529200	2538880	colonizing or filling the universe with sentience is a you know gigantic grand vision
2540640	2547840	um yeah I mean if that's if that's the way one wants to go I mean I mean I actually
2547840	2552160	happen to think the future is big enough that you could not just realize one vision but many
2552880	2559600	not every vision because some are directly in conflict but if some people think doing
2559600	2566160	something nice for existing people and working locally is the most important thing we could
2566160	2572080	certainly do that and then also that leaves all the rest of the universe and that's big enough
2572080	2579200	that you could have sort of AI paradise in one sector and you know animal uplift in another
2579200	2584000	sector and you could have a whole bunch of different to the extent that a vision doesn't
2584000	2589120	require the negative like the absence of things and just the addition of new things that that
2589120	2594320	would be easy to do the harder questions become when when like one thing says that another shouldn't
2594320	2599040	exist and vice versa and then you would have to strike some compromise that will give each of
2599040	2604000	them less than a hundred percent of what the way they think would be the best there's enough room
2604000	2610080	out there that both can be accommodated uh yeah and I think this is actually quite important it's
2610080	2614720	not really the focus of this book but having I think in general as we will be wrestling with
2614720	2619680	for example how to relate to the digital minds the ai's that we create um
2622320	2629040	having this sense of expansive generosity and like feeling that there is room for a lot and we
2629040	2636080	shouldn't push too hard to get a hundred percent of one value but we should try first to sort of give
2636080	2643680	all reasonable value systems like a good deal of satisfaction and then after that we can
2643680	2650000	scobble about the remains like but because that that seems like such a if we solve these practical
2650000	2656960	problems there's so much so much opportunity there and I think that increases the chances of
2657840	2660400	that the value that the future goes well in the first place.
2661600	2667280	Nick I'd like to just do some expansive thinking in terms of your book you've positioned it very
2667280	2674560	well in terms of its objective in terms of human values but the the the assumption the basic
2674560	2680320	foundation of the book of a solved world and the conditions for that and the implications
2680320	2686240	lead to many other questions which are beyond the book but I'd like to just put them to you for
2687120	2692640	because they occurred to me and I'm sure to many people and see see where we go so
2694000	2700320	no no order here but when we talked about these huge numbers of filling the universe with saturated
2700320	2709600	with sentience as I said 10 to the 43rd or 58th the number of of sentient minds in one form or
2709600	2716720	another if that were to occur and it is a handout of that Professor Bostrom gave to the students
2716720	2724560	which I I lapped up if that would occur what what did you're feeling about why that occurred
2724560	2731120	is that just would that just be a human tendency or would there be some universal trophism that's
2731120	2740240	pulling that's self just desiring to be a self understanding and self aware in some sense
2741040	2745840	do you have any feeling about that in other words what what's the reason that that would happen
2749520	2756720	yeah well if we are imagining this astronomical entity of objects as being sort of human like
2756720	2763920	minds then I presume the most likely path whereby that would happen is if humans shaped the future
2763920	2769600	and in particular that was a strong influence of those humans who value this kind of future like
2769600	2777120	utility broadly utilitarian constituencies and one might it's possible that how many
2777120	2781920	people would favor which moral theory will change for example if we became smarter or had
2781920	2787120	AIS to advise us in our philosophizing there might potentially be some convergence either
2787120	2796320	two words or away from those conceptions I it's even on that conception it's not clear that the
2796320	2802480	right unit would be human minds right it might either either be smaller if you think pleasure
2802480	2808800	is somehow something that could be quantified you know maybe the most optimal structure for
2808800	2813600	generating pleasure would be like a why do you need all of this this this cortex and
2814720	2820640	like visual like processing all of that maybe you just need some like kind of pruned down
2821680	2825360	neural structure and maybe it would be like some some animal maybe this like
2825920	2831120	optimized better optimized you would go further in that direction maybe pleasure boxes would like
2831440	2838800	you know have a different size than humans you know you must matter structured to be optimized
2838800	2844240	for the instantiation of pleasure yeah and if you include digital pleasure if that it's
2845520	2851840	yeah yeah I mean it doesn't really matter how big this would be if they would be like a millimeter
2851840	2858320	square or like no you know a light year square but right for other values it's like well you have to
2858320	2866240	look at them one by one how they scale with resources so some values maybe have diminishing
2866240	2872320	returns to extra resources and this might be true for sort of typical individual human values
2874640	2882400	where like I mean so most obviously with like wealth for example it's a much bigger deal if
2882400	2889520	you're if you go from 1000 the year to 2000 year in income huge difference now if you go from like
2889520	2898080	one million to two million I mean it's it's a thousand times bigger an increase and an equal
2898080	2902640	in percentage terms but probably you barely notice it like you get an extra summer house or whatever
2902640	2908080	it's not really and so so with current economic resources they seem to have a kind of steeply
2908080	2913280	diminishing marginal returns insofar as they are spent by an individual to try to boost their own
2913280	2920080	welfare with other values like knowledge etc you might think that there would be diminishing returns
2920080	2926240	once we have already found the most important knowledge and then we'd be sort of spending
2926240	2933040	increasing resources to discover smaller and smaller truths Nick consciousness has come into
2933040	2939200	our conversation and in the book in different different fashions and in different ways
2940240	2947040	is there a fundamental assumption as a worldview in a solved world as the paradigm for example
2948240	2954000	require or assumes that consciousness is entirely physical that it's the product of physical laws
2954000	2960880	irregularities including the deep deepest laws of physics which may be unknown but but still
2960880	2964960	part of the physical world is is that a an underlying assumption
2966720	2972880	well I mean I'm a computationalist thinking that it's a structure of certain computations that
2973440	2981600	produce and conscious experience and those could be implemented on carbon-based organic brains
2981600	2988240	or in principle on silicon processors or you know in whatever substrate is capable of processing
2989040	2997360	functionalism is that yeah now I don't think that's really an essential premise for most of the
2997360	3004640	book I think there are little bits and pieces because I think this views yeah is so that but
3004640	3011760	I mean basically you could imagine I mean clearly if if our but that would be a crazy view like if
3011760	3017440	you thought that what we do in this world has no effect on conscious experiences then I guess the
3017440	3022640	question would become purely philosophical like a thought experiment if you could some if somehow
3022640	3028160	this condition of a plastic world arose then what would be our values in that world but we would
3028160	3032800	have no past words it but I think most people would think that clearly it has something to do with
3032800	3038160	what happens in brains and our sensory organs and like clearly impacts the conscious experiences
3038160	3043600	we have and so then even if you thought purely silicon entities could not have conscious experience
3043600	3048960	you could still have technologies that would make it possible to manipulate the organic
3048960	3053680	brains we have like we already have drugs you could imagine surely slightly better drugs with
3053680	3059440	fewer side effects and slightly other things that would at least allow us to approximate this condition
3060800	3066400	of plasticity even if perhaps not go you know the last 10 percent of the way there
3067200	3072960	as a functionalist and as a computational neuroscientist computational mind approach as
3072960	3079840	you've said it would seem that the concept of AI consciousness in some sense like our consciousness
3079840	3088160	is a certainty that may not be within decades it may take a long time but it doesn't seem to be any
3088160	3094880	in principle inhibition to that given that philosophical foundation is that is that fair
3095680	3103840	well um well first of all um I mean a certainty is a strong claim with respect to any big
3103840	3110320	philosophical question that's why I don't have that level of I mean we just need to look at the history
3110320	3115120	of philosophy with great thinkers disagree with one another so at least some of them have been wrong
3115120	3118960	about really important things and perhaps all of them but at least some right that we know
3119040	3126320	and uh so they all can't be right but they all can but but they uh but they all can be wrong
3126320	3131600	right they could all be wrong but they can't all be right since they contradict one another and so
3131600	3136320	clearly at the matter level one has to have a lot of humility about one's views about any of these
3136320	3144560	matters um but um even if we assume computationism it's certainly not a given that future AIs actually
3144560	3149600	will be conscious it would just demonstrate that in principle there could be but it might still
3149600	3154800	require the design of certain kinds of AIs to realize that possibility but I'm saying in in
3154800	3161520	principle I'm putting a hard question to you in principle it is a certainty that AI could be
3161520	3167440	conscious how it's achieved and when it's achieved that's completely uncertain but if you're a
3167520	3176000	computationalist and a functionalist I think that you have to submit to that certainty I mean
3176000	3181120	it certainly is an implication of functionalism or computationalism that AIs could in principle
3181120	3186720	be conscious now when I say that I'm a computationalist I don't mean that I am certain of it like
3186720	3192000	because I could be wrong about anything and in particular that okay I mean it seems like one of
3192000	3197680	the more amongst all the different philosophical views that I'm more or less sure about a lot of
3197680	3203600	things and that would come like higher up the end of philosophically controversial views that I feel
3203600	3209920	convinced about but certainly not like at 100% or anything like that okay is AI conscious as part
3209920	3221840	of a solved world or that's that's a tangential I think it would be very likely part of the
3221840	3228400	possibilities in a solved world that digital conscious minds could be created I think it's
3228400	3233840	one of the technological affordances that's technological maturity to do this now if I'm
3233840	3237920	wrong about consequentialism then you know it might still be true because you could then like
3237920	3242880	maybe engineer minds through through bioengineering or something that would basically achieve the
3242880	3249920	same thing now I think most of the book would still stand even if somehow you drop from the
3249920	3256640	package of assumptions of technical maturity next question is virtual immortality is that part of
3256640	3262720	a solved world because lifespan again I didn't can't remember every single word but I don't
3263520	3270320	don't recall that lifespan of being a critical part of the of the solved world 100 years or
3270320	3277440	something but in a solved world one might think there's physical immortality and then
3277520	3283200	concept of virtual immortality yeah well I mean immortality is a long time
3285200	3292480	I mean if we mean literally never dying that is possibly physically impossible I mean given
3292480	3301600	life of the heat death of the universe etc but if we are talking just about say extreme life
3301600	3306560	extension I certainly don't think there is an law of nature that says that humans can only live for
3306560	3312560	80 years or 100 years or whatever like once you achieve the ability to continuously repair
3312560	3318480	damage that occurs and then maybe reduce accident risk certainly many thousands of years would be
3318480	3323120	trivial and if you could upload into computers then your software which could just be kind of
3323120	3329040	error checked and redundantly stored etc and you could have astronomical lifespans the question
3329040	3333200	in that context becomes not so much whether you could keep sort of the physical substrate alive
3333920	3340880	and functioning but more what does it mean for if you want to retain a human like mind
3341680	3345680	I mean you can keep learning for 100 years and presumably for 200 years right but
3346320	3351200	after 200,000 years we don't really know whether the human mind would just kind of go stale and
3351200	3357680	rigid and eventually become kind of non-human if you want to continue to develop and learn
3357680	3362720	and change from experience the way we currently do which might seem really part of what it means
3362720	3367760	to be human it might be that if you continue doing that over sufficient large timescale you
3367760	3374320	eventually become something non-human like that might retain some of your earlier humanity in it
3374320	3380560	just like you retain something of your five-year-old self but it's still you're not I mean you're in
3380560	3384880	some sense the same person but in some sense also a different person and I think similarly we might
3384880	3390320	become like post-human versions of ourselves over really really long timescales. Nick two things
3390320	3395680	about the book that intrigued me randomly I just want to ask you quickly the first is you made a
3395680	3401040	comment that consciousness is not necessary for moral status so that surprised me.
3403280	3410960	Yeah so I'm inclined to that view I'm not fully confident but it's particularly relevant in the
3410960	3417600	context of digital minds and maybe even sub-human digital minds like once we are currently building
3417600	3424000	are the like the next generation of the current AI systems above beyond. I think consciousness would
3424000	3428800	be sufficient for moral status if you can suffer that that would mean that it matters how you're
3428800	3434640	treated but it seems possible to me that I did a little mind even if it doesn't have that but say
3434640	3442960	it has a conception of self as existing through time it's a really sophisticated mind it it has
3442960	3449520	preferences maybe life goals it can form relationships with human beings reciprocal
3450720	3455680	friendships etc I think in that case my intuition would strongly be that there would be ways of
3455680	3460560	treating it that would be wrong and so that it would have moral patience even if it weren't
3460560	3466400	conscious. And it could have those characteristics without having consciousness? Yeah I mean so those
3466400	3471440	characteristics are all functionally defined these are sort of behaviors and dispositions etc
3472960	3480400	so I mean it might be that depending on how willing you are to ascribe conscious experiences
3481440	3484880	to different kinds of systems maybe you would ascribe conscious experience
3484880	3489600	but I would say even conditionalizing on if not having conscious experiences if it had those
3489600	3496080	attributes it would be a strong candidate. That's an interesting position near the end you introduce
3496080	3503360	the concept of enchantment why? It seemed like another example of these
3504720	3514400	quiet values like the kind of star constellations that are a little bit hidden from us in our current
3514400	3522960	sort of brutish condition of grave needs and desperation but that could come into view if we
3522960	3529760	solve a lot of the practical problems and one of the things that if it's missing I think might
3529760	3537760	make a possible future condition look less attractive to us. If you take the extreme
3538800	3545600	example of the absence of enchantment imagine some future in which so this is not a utopia at all
3545600	3552480	but like just consider if you were sitting in a chamber and your job consisted of pressing
3553520	3557360	like maybe you were presented with some analytic problem in a little text bubble
3557360	3561040	and then you had to think hard and engage all your mental faculties use your knowledge and
3561040	3566160	creativities all of that would be there and then you sort of outputs the answer and then you get
3566160	3573440	as a reward a pleasure palette that also gave you your nutrients and you sort of shortcut our
3573440	3579840	rich interactions with reality and simplify it to a purely analytic exercise where all that matters
3579840	3586560	is kind of whether you choose action A, B or C so you still have causal impact you still have to use
3586560	3592000	a lot of your human capacities but something would seem to be missing I call this enchantment so right
3592000	3599920	now when we are in the world all the different parts of us are relevant and engaged so in addition
3599920	3605040	to your abstract decision you have intuitive decisions right you have emotions you have to
3605040	3610240	control and manage you have body language you have a physiology you have legs and like
3610240	3614320	and we interact with other people we don't just perceive whether they choose A, B and C we sort
3614320	3620960	of perceive we have a much higher bandwidth interface with reality and so that's I'm trying to
3620960	3625040	gesture because this value hasn't really been characterized but I think with a few more examples
3625040	3628880	like that one can get an intuitive sense there is something there that if that weren't there
3628880	3634400	then plausibly this this future would be deficient so Nick let's have some fun I'm going to ask you
3634400	3641200	some very big questions and ask you beg you for some very short answers and let's see what happens
3641200	3649120	so first what are the percentages for the following scenarios for AI superintelligence in the next
3649120	3655360	hundred years I picked a hundred years so here's the percentage some really bad events would occur
3655360	3661040	that AI will do substantial damage to humanity percentage zero to a hundred
3663120	3671680	like my p-doom as I call it now I've punted on this in the past I think it's like certainly
3672560	3678640	very non trivial it depends partly on what we do the degree to which we get our act together
3679440	3685520	but partly I think it's also baked in percentage a number I'm listening for a number
3688800	3698560	the percentage won't hold you to it no but other people might range give me how about a range
3699280	3708640	non-trivial is I mean yeah I mean I mean it seems like a bit bigger than 5% and lower than 95%
3709600	3715760	okay well that's we made some progress like on the twin prime the subject to revision
3716320	3723040	okay all right how about I mean a lot depends yeah I mean bad things are going to happen to humans
3723040	3730160	by default anyway I mean we all kind of either get cancer or heart disease or get shot or Alzheimer's
3730160	3738960	or something else over a hundred year timescale and so the default is that we are all kind of
3738960	3747520	going into the the slaughterhouse and the the question is like how low does the chance have to
3747520	3754080	be before one is it would be willing to take a gamble on something different that's one question
3755200	3759600	but then I think there also this would get beyond our current short form format
3760960	3768640	questions about how our AIs relate to other AIs out there in the infinite universe that are already
3768640	3777040	established okay so let's let's go on fine good some aspect of the utopian outcome exactly the
3777040	3784560	opposite of the AI say large does away largely with all work in a hundred years what's the likelihood
3784560	3793120	of that zero to a hundred so conditional on AI being developed or just whatever yeah
3794800	3800080	well conditional on it being developed yeah I mean that that I mean I'll work with the exception
3800080	3805920	of work where there is a specific demand that it be performed by human or where the consumer
3805920	3813760	cares about the process and with the asterisk that yeah okay these are really hard like I think
3813760	3819360	you're doing the mistake of trying to ask a philosopher to be very concise
3822480	3831840	I just see like an avalanche of considerations and qualifications and levels for each of these
3831840	3838480	very complex questions okay next question a percentage of a catastrophic human event
3838480	3848000	dealing with existential risks to humanity before 2050 not saying elimination of all human beings
3848000	3856160	like a huge asteroid but some huge catastrophic event well I think there are catastrophic events
3856160	3864880	all the time something that would would decimate would would would kill a large percentage of
3864880	3874640	humanity or eliminate you've dealt so much with existential risk yeah yeah well I mean so existential
3874640	3882000	risk is a subset right we actually permanently destroy the future and then I guess it depends a
3882000	3887120	little on how like how how how many people do you have to decimate like so like COVID is like
3887120	3892240	whatever half a percent or something and then it goes up from there I know I'd say a bigger number
3892240	3901280	you know 50% of humanity I mean most likely I think we'd either not have that or we have an
3901280	3909680	existential catastrophe that or something very close to like 50% is a kind of weirdly intermediate
3909680	3916400	number well it could happen some some pandemic engineered pandemic and maybe or maybe a thermal
3916400	3922320	nuclear but like engineered pandemic is probably the most likely way that 50% of us die in the
3922320	3928880	next couple of decades okay a little bit into your your total work the percentage that our universe
3928880	3935760	is a simulation I don't know if you've ever said that you gotta get my probability many have tried
3935840	3939840	so far none has succeeded and you shall not be the first to
3947760	3952480	do you think there is at least one solved world in the observable universe
3956800	3964080	in the observable universe no I mean unless yeah unless the simulation hypothesis is true in which
3964080	3970800	case the question becomes a bit wonky right I mean I don't think there is any in the observable
3970800	3975200	you know I mean the observable universe I think most likely intelligent life is
3977440	3983920	low density so that might be infinitely much of it but within any small finite region like the
3983920	3989440	observable universe it might just be so unlikely for it to evolve in the first place that we are
3989520	3996320	alone which would account for the Fermi paradox right that's an important issue that we deal
3996320	4003840	with and that's a good very good perspective AI consciousness true in awareness what's your
4003840	4012000	odds on that happening within a conceivable a thousand years is that a high likelihood
4012720	4019280	um yeah uh conditional on us not going extinct before I mean in fact I wouldn't be confident we
4019280	4028640	don't already have it in some AI systems I think as you zoom in on the concept of consciousness
4028640	4035280	this might be for another conversation but I think it becomes it's a lot more multi-dimensional
4035280	4042160	and vague than the naive you would have it and so the question might be less binary than one one
4042160	4048720	one supposes virtual immortality of our first person consciousness is that in principle possible
4052400	4058720	um well certainly uh extreme longevity as in like whatever thousands or millions of years
4059840	4063840	immortality as in never dying depends on physics which currently looks like it would
4063840	4070480	not admit of infinite information processing streams so virtual uh uploading of our first
4070480	4077520	person consciousness not not as a duplicate where it's like a um a very sharp identical twin but
4077520	4084720	literally my first person consciousness is you yeah it is uh it is in principle uh possible
4085600	4093040	I think so yeah okay um is life in the universe a happy accident or is life somehow built
4093040	4104720	into the ultimate laws of physics I mean both could be true it might be the it's built in that
4104720	4113680	for any planet it's it there's an extremely low chance of it happening and then it might also
4113680	4118720	be built in that there are enough planets that statistically it will happen on an astronomical
4118720	4127440	or infinite number of planets um how prevalent is life and mind in the universe you've said already
4127440	4136080	that it is very rare uh is it there a possibility that we are alone in terms of intelligent mind
4138000	4143360	in the observable universe I think that's a very real possibility but of course if the universe is
4143360	4149040	infinite as it looks like it is then with probability one uh that would be infinite
4149040	4153840	the many of these places were intelligent life as a result and then if you introduce the simulation
4153840	4158560	then it becomes more complex to answer it but yeah yeah I know the simulation is actually
4158560	4163600	self-solving in that in that to some other questions an infinite universe anything that's possible
4163600	4168480	will happen an infinite number of times and so the question becomes becomes very vague last question
4169120	4173360	does anything exist not explainable in terms of ultimate physics
4178080	4187440	um so yeah like I think for something to be explainable could mean two different things one
4187440	4193520	is that it's sort of supervenes on the laws of physics yes um which maybe is what you have in
4193520	4199600	mind uh well then yeah the laws of physics in our universe uh I think there could be a lot of
4199600	4203760	things that don't supervene on them if we are in a simulation there would be other layers of
4203760	4209600	reality which would have their own laws of physics etc um in our observable universe it looks like
4209600	4214000	everything supervenes on the laws of physics doesn't mean it's explainable in the sense that there is
4214000	4222160	like a useful intelligible you know 10 page text that would like make you more informed by talking
4222160	4226640	about basic physics like if you're trying to understand some cultural phenomena you would
4226640	4230400	you wouldn't start writing out the quantum equations or something yeah Nick this has been
4230400	4236640	terrific um I wish we could go on forever we'll definitely do this sooner than another 17 years
4236640	4244560	yeah I promise that uh Deep Utopia is a fantastic book recommended for everybody it is a vision
4244560	4249200	for the future but more than that it's more than that it's really an understanding of what
4250160	4256080	what the meaning of human life can be and it reflects on what we think of our own values so
4256080	4262560	we can go on viewers can watch hundreds of tv episodes and exclusive videos on cosmos life
4262560	4267360	cosmology and meaning on the closer to truth website and closer to the youtube channel
4267360	4273520	including of course those of Nick Bostrom thanks Nick thanks everyone for watching thank you very
4273520	4282960	much thank you for watching if you like this video please like and comment below you can support
4282960	4292400	closer to truth by subscribing
