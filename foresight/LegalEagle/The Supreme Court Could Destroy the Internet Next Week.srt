1
00:00:00,000 --> 00:00:03,600
This case has the potential to end the internet as we know it.

2
00:00:03,600 --> 00:00:07,600
The internet flourishes on user-generated content. It's the backbone of the internet.

3
00:00:07,600 --> 00:00:12,240
And if you write or say something that is illegal, you can be held liable for what you said.

4
00:00:12,240 --> 00:00:15,920
But usually the websites that host your content are not liable.

5
00:00:15,920 --> 00:00:19,600
And when you hear about Section 230, this is usually what we're talking about.

6
00:00:19,600 --> 00:00:23,280
But if websites were also liable for what users say,

7
00:00:23,280 --> 00:00:27,760
then websites would either shut down almost everything or police things so tightly

8
00:00:27,840 --> 00:00:31,680
that there would be basically nothing even remotely objectionable.

9
00:00:31,680 --> 00:00:33,440
Because if there's one thing that websites hate,

10
00:00:33,440 --> 00:00:38,320
it's being on the hook for billions and billions of dollars of potential liability and attorney's fees.

11
00:00:38,320 --> 00:00:40,720
And this isn't just some potential hypothetical.

12
00:00:40,720 --> 00:00:44,720
In the past, when laws made websites liable for user content,

13
00:00:44,720 --> 00:00:47,120
they just shut down the content and never came back.

14
00:00:47,680 --> 00:00:52,240
And one case before the Supreme Court this term seeks to do exactly that,

15
00:00:52,240 --> 00:00:54,160
but for all of the internet.

16
00:00:54,160 --> 00:00:58,480
No joke, this has the potential to end the internet as we know it today.

17
00:00:59,120 --> 00:01:00,400
So how did we get here?

18
00:01:02,960 --> 00:01:04,320
Well, it started with a tragedy.

19
00:01:04,320 --> 00:01:07,760
Noemi Gonzalez was a design major at California State University Long Beach,

20
00:01:07,760 --> 00:01:10,080
who was spending a semester studying in Paris.

21
00:01:10,080 --> 00:01:12,640
On the evening of November 15, 2015,

22
00:01:12,640 --> 00:01:15,840
she was dining with friends at a cafe on the Champs-Elysees,

23
00:01:15,840 --> 00:01:18,400
when an ISIS terrorist with a gun opened fire.

24
00:01:18,400 --> 00:01:21,360
Gonzalez was killed along with 130 other people

25
00:01:21,440 --> 00:01:24,720
in coordinated attacks at six locations across the city.

26
00:01:24,720 --> 00:01:26,640
ISIS claimed responsibility for the attacks,

27
00:01:26,640 --> 00:01:30,240
and eventually 20 men were convicted of crimes related to the massacre.

28
00:01:30,240 --> 00:01:33,360
The lone surviving gunman was sentenced to life without parole.

29
00:01:33,360 --> 00:01:35,280
So was justice served?

30
00:01:35,280 --> 00:01:37,680
Well, the Gonzalez family contends that it was not,

31
00:01:37,680 --> 00:01:40,320
because there was another perpetrator still at large.

32
00:01:40,320 --> 00:01:41,120
YouTube.

33
00:01:41,120 --> 00:01:43,280
The family sued Google, which owns YouTube,

34
00:01:43,280 --> 00:01:47,040
alleging that YouTube's algorithms amplify violent videos and hateful content,

35
00:01:47,040 --> 00:01:50,400
despite the company's efforts to ban violent accounts and limit their reach.

36
00:01:50,400 --> 00:01:52,720
The Gonzalez family says that YouTube's recommendations

37
00:01:52,720 --> 00:01:55,280
expose people to hateful content, radicalize viewers,

38
00:01:55,280 --> 00:01:57,840
and encourage them to make terrorist attacks of their own.

39
00:01:57,840 --> 00:02:02,000
Google argued that because ISIS, not YouTube, made and uploaded those videos,

40
00:02:02,000 --> 00:02:04,480
YouTube was not the provider or developer of those videos,

41
00:02:04,480 --> 00:02:08,800
and therefore it was immune from liability under 47 USC Section 230,

42
00:02:08,800 --> 00:02:12,320
otherwise known as Section 230 of the Communications Decency Act.

43
00:02:12,320 --> 00:02:15,120
Section 230 of the CDA immunizes internet services

44
00:02:15,120 --> 00:02:17,680
for the content that its users upload,

45
00:02:17,680 --> 00:02:22,240
and often allows websites to remove content without taking on liability.

46
00:02:22,240 --> 00:02:24,880
But the Gonzalez plaintiffs contend that YouTube's recommendations

47
00:02:24,880 --> 00:02:29,040
should not be covered by Section 230 because the company is acting like a content creator

48
00:02:29,040 --> 00:02:30,480
rather than a publisher.

49
00:02:30,480 --> 00:02:33,120
The Ninth Circuit Court of Appeals held that the Gonzalez claims

50
00:02:33,120 --> 00:02:36,400
fell within Section 230 and Google was immune from suit.

51
00:02:36,400 --> 00:02:39,280
The Gonzalez parties appealed and the Supreme Court granted cert

52
00:02:39,280 --> 00:02:41,760
to answer the question of whether Section 230

53
00:02:41,760 --> 00:02:46,960
immunizes an interactive computer service from liability for recommending other party content.

54
00:02:46,960 --> 00:02:48,960
Now in support of their claims, plaintiffs alleged the quote

55
00:02:48,960 --> 00:02:51,840
two of the 12 ISIS terrorists who carried out the attacks

56
00:02:51,840 --> 00:02:56,560
used online social media platforms to post links to ISIS recruitment YouTube videos

57
00:02:56,560 --> 00:02:58,560
and quote jihadi YouTube videos.

58
00:02:59,280 --> 00:03:04,000
One of the men who fired at Gonzalez had appeared in an ISIS recruiting video in 2014.

59
00:03:04,560 --> 00:03:09,280
Now the plaintiffs argue that the defendants violated the Anti-Terrorism Act, the ATA,

60
00:03:09,280 --> 00:03:14,880
by allowing ISIS to post videos on YouTube under Section 2333 of the ATA,

61
00:03:14,880 --> 00:03:18,000
as amended by the Justice Against Sponsors of Terrorism Act.

62
00:03:18,000 --> 00:03:21,760
Americans who are injured by quote an act of international terrorism

63
00:03:21,760 --> 00:03:25,200
that is committed, planned, or authorized by a terrorist organization

64
00:03:25,200 --> 00:03:30,320
may sue any person who quote aides and abets by knowingly providing substantial assistance

65
00:03:30,320 --> 00:03:34,320
or who conspires with a person who committed such an act of international terrorism

66
00:03:34,320 --> 00:03:36,000
and recover trouble damages.

67
00:03:36,000 --> 00:03:39,600
So the legal question in the lower courts was whether a social media platform

68
00:03:39,600 --> 00:03:43,680
that was not used to commit a specific quote active international terrorism

69
00:03:43,680 --> 00:03:47,120
may still be liable for aiding and abetting under Section 2333.

70
00:03:47,120 --> 00:03:48,560
The plaintiffs say that the answer is yes,

71
00:03:48,560 --> 00:03:51,440
that Google aided and abetted an act of international terrorism

72
00:03:51,440 --> 00:03:54,560
conspired with the perpetrator of an act of international terrorism

73
00:03:54,560 --> 00:03:58,480
and provided material support to ISIS by allowing ISIS to use YouTube.

74
00:03:58,480 --> 00:03:59,600
And of course the defendants say no,

75
00:03:59,600 --> 00:04:03,040
the plaintiff's claims are barred by Section 230 of the Communications Decency Act

76
00:04:03,040 --> 00:04:07,600
which often immunizes interactive computer services, aka websites,

77
00:04:07,600 --> 00:04:09,680
even when they make targeted recommendations.

78
00:04:09,680 --> 00:04:11,920
The trial court and the Ninth Circuit Court of Appeals

79
00:04:11,920 --> 00:04:15,840
sided with Google, ruling that the claims fell within Section 230 Section C

80
00:04:16,560 --> 00:04:20,240
because ISIS, not YouTube, created or developed the relevant content.

81
00:04:20,240 --> 00:04:22,720
The court said that YouTube quote selects the particular content

82
00:04:22,720 --> 00:04:25,280
provided to a user based on that user's inputs.

83
00:04:25,280 --> 00:04:30,640
The display of recommended content results from algorithms that are merely quote tools

84
00:04:30,640 --> 00:04:33,600
meant to facilitate the communication and content of others

85
00:04:33,600 --> 00:04:36,240
and not content in and of themselves.

86
00:04:36,240 --> 00:04:37,520
As the lower courts noted,

87
00:04:37,520 --> 00:04:41,520
the recommendations are based on the user's preferences, not YouTube's.

88
00:04:41,600 --> 00:04:45,440
The plaintiffs argue that Section 230C does not apply to quote

89
00:04:45,440 --> 00:04:47,840
activities that promote or recommend content.

90
00:04:47,840 --> 00:04:50,560
So let's dig deeper into what all of this means.

91
00:04:50,560 --> 00:04:53,840
First of all, the plaintiffs are not simply arguing that platforms are liable

92
00:04:53,840 --> 00:04:56,080
because terrorists have used their services.

93
00:04:56,080 --> 00:04:58,960
They argue that the powerful algorithms that recommend content

94
00:04:58,960 --> 00:05:03,440
are not covered by the CDA because the recommendations are content in and of themselves.

95
00:05:03,440 --> 00:05:04,400
And as the Ninth Circuit said,

96
00:05:04,400 --> 00:05:06,960
the complaint alleges Google uses computer algorithms

97
00:05:06,960 --> 00:05:10,400
to match and suggest content to users based upon their viewing history.

98
00:05:10,400 --> 00:05:12,240
The Gonzalez plaintiffs alleged that in this way,

99
00:05:12,240 --> 00:05:17,840
Google has recommended ISIS videos to users and enabled users to locate other videos and accounts

100
00:05:17,840 --> 00:05:18,880
related to ISIS.

101
00:05:18,880 --> 00:05:21,840
And that by doing so, Google assists ISIS in spreading its message.

102
00:05:21,840 --> 00:05:23,840
So the plaintiffs claim that YouTube facilitates

103
00:05:23,840 --> 00:05:26,320
communications between ISIS and people who watch their videos,

104
00:05:26,320 --> 00:05:28,640
thereby aiding ISIS in recruiting new members.

105
00:05:28,640 --> 00:05:32,320
The plaintiffs acknowledge that YouTube removed ISIS videos and suspended

106
00:05:32,320 --> 00:05:34,560
or blocked ISIS users at various times.

107
00:05:34,560 --> 00:05:38,080
However, the plaintiffs claim that YouTube should have been able to stop ISIS from

108
00:05:38,080 --> 00:05:40,400
reestablishing accounts using new identities.

109
00:05:40,400 --> 00:05:43,120
And the plaintiffs alleged that Google left some of the ISIS videos up

110
00:05:43,120 --> 00:05:46,000
because they did not contain content violating the site's policies.

111
00:05:46,000 --> 00:05:48,320
At other times, Google removed the offending content,

112
00:05:48,320 --> 00:05:50,480
but did not suspend or ban the accounts.

113
00:05:50,480 --> 00:05:54,880
Now, we've talked about Section 230 of the CDA many times on this channel.

114
00:05:54,880 --> 00:05:59,120
Section 230 was created back in 1996 at the dawn of the Internet.

115
00:05:59,120 --> 00:06:00,960
It has two key provisions.

116
00:06:00,960 --> 00:06:03,600
Section 230c1 says, quote,

117
00:06:03,600 --> 00:06:06,640
No provider or user of an interactive computer service

118
00:06:06,640 --> 00:06:10,400
shall be treated as the publisher or speaker of any information provided by

119
00:06:10,400 --> 00:06:12,400
another information content provider.

120
00:06:13,040 --> 00:06:16,640
This sentence is often called the 26 words that created the Internet.

121
00:06:16,640 --> 00:06:20,480
This section stipulates that providing access to third-party content

122
00:06:20,480 --> 00:06:24,800
does not make an online provider the publisher or speaker of that content.

123
00:06:25,360 --> 00:06:29,040
Now, the second major part of the law is Section 230c2,

124
00:06:29,040 --> 00:06:31,200
the so-called Good Samaritan provision,

125
00:06:31,200 --> 00:06:32,080
which states, quote,

126
00:06:32,080 --> 00:06:35,360
No provider or user of an interactive computer service

127
00:06:35,360 --> 00:06:39,520
shall be held liable on account of any action voluntarily taken in good faith

128
00:06:39,520 --> 00:06:42,320
to restrict access to or availability of material

129
00:06:42,320 --> 00:06:46,080
that the provider or user considers to be obscene, lewd, lascivious,

130
00:06:46,080 --> 00:06:49,520
filthy, excessively violent, harassing, or otherwise objectionable,

131
00:06:49,520 --> 00:06:52,880
whether or not such material is constitutionally protected.

132
00:06:52,880 --> 00:06:56,720
So this provision gives online platforms broad immunity from liability

133
00:06:56,720 --> 00:06:59,280
if they moderate content in good faith.

134
00:06:59,280 --> 00:07:02,960
This provision was meant to allow websites to police itself

135
00:07:02,960 --> 00:07:06,320
without incurring new liability because it was an issue of knowledge.

136
00:07:06,320 --> 00:07:09,600
A website might not know what speech was hosted on that particular website,

137
00:07:09,600 --> 00:07:11,680
in which case they wouldn't be liable for it,

138
00:07:11,680 --> 00:07:15,520
but if they knew that illegal speech or defamatory speech

139
00:07:15,520 --> 00:07:17,440
or some other kind of speech was hosted there

140
00:07:17,440 --> 00:07:21,040
and then took steps to remove it from the website itself,

141
00:07:21,040 --> 00:07:24,160
then they actively had knowledge of that particular speech

142
00:07:24,160 --> 00:07:27,360
and thus might become liable for hosting it in the future.

143
00:07:27,440 --> 00:07:29,760
So websites found themselves on the horns of a dilemma

144
00:07:29,760 --> 00:07:31,840
until section 230 of the CDA was passed

145
00:07:31,840 --> 00:07:34,720
to allow them to engage in good faith moderation

146
00:07:34,720 --> 00:07:36,960
because there was generally a knowledge requirement

147
00:07:36,960 --> 00:07:39,920
to be liable for things that were hosted on your website.

148
00:07:39,920 --> 00:07:42,400
If you had no knowledge of what kind of speech

149
00:07:42,400 --> 00:07:44,320
was on your website or platform,

150
00:07:44,320 --> 00:07:47,280
then you generally weren't liable for it.

151
00:07:47,280 --> 00:07:50,640
But the argument went that if you started to moderate your website,

152
00:07:50,640 --> 00:07:52,880
then that was evidence that you knew what was on there

153
00:07:52,880 --> 00:07:55,680
and therefore you became liable for all the stuff

154
00:07:55,680 --> 00:07:58,000
that you were trying to remove from your website.

155
00:07:58,000 --> 00:08:00,640
And this applied to spam, to pornography,

156
00:08:00,640 --> 00:08:04,800
to pirated media, to defamation, you name it.

157
00:08:04,800 --> 00:08:07,360
So the CDA was enacted in 1996

158
00:08:07,360 --> 00:08:09,440
when the internet was still in its infancy.

159
00:08:09,440 --> 00:08:11,360
At the time, companies like America Online,

160
00:08:11,360 --> 00:08:13,200
CoffeeServe and Prodigy were the portals

161
00:08:13,200 --> 00:08:15,120
that allowed people to access the internet.

162
00:08:15,120 --> 00:08:17,280
People could share files, exchange messages,

163
00:08:17,280 --> 00:08:19,040
and chat with each other in real time.

164
00:08:19,040 --> 00:08:20,560
But that posed a dilemma.

165
00:08:20,560 --> 00:08:23,920
Who would be liable if a user posted something defamatory

166
00:08:23,920 --> 00:08:26,640
or shared something else that was illegal?

167
00:08:26,640 --> 00:08:28,400
Would liability go beyond the person

168
00:08:28,400 --> 00:08:30,240
that was responsible for the post?

169
00:08:30,240 --> 00:08:31,760
Or would the internet companies

170
00:08:31,760 --> 00:08:34,160
who hosted the content also be on the hook?

171
00:08:34,160 --> 00:08:35,280
And that brings us to the distinction

172
00:08:35,280 --> 00:08:37,520
between content publishers, distributors, and platforms,

173
00:08:37,520 --> 00:08:40,000
which is often completely misunderstood.

174
00:08:40,000 --> 00:08:41,360
In the pre-internet age,

175
00:08:41,360 --> 00:08:43,040
publishers, distributors, and platforms

176
00:08:43,040 --> 00:08:44,640
were treated differently under the law.

177
00:08:44,640 --> 00:08:48,240
Publishers were newspapers, magazines, and broadcast stations.

178
00:08:48,240 --> 00:08:50,400
They were generally liable for republishing material

179
00:08:50,400 --> 00:08:51,520
from third parties

180
00:08:51,520 --> 00:08:52,880
since they solicited the things

181
00:08:52,880 --> 00:08:55,680
that they published and they could vet those materials.

182
00:08:55,680 --> 00:08:58,000
Distributors were businesses like bookstores,

183
00:08:58,000 --> 00:08:59,120
newsstands, and libraries,

184
00:08:59,120 --> 00:09:01,680
which distributed material that was printed by others.

185
00:09:01,680 --> 00:09:03,840
And distributors were not required

186
00:09:03,840 --> 00:09:05,600
to assess every book that they sold.

187
00:09:05,600 --> 00:09:06,800
For example, in the 1950s,

188
00:09:06,800 --> 00:09:09,440
the Supreme Court overturned a Los Angeles ordinance that said,

189
00:09:09,440 --> 00:09:12,240
if you have obscene material in your bookstore,

190
00:09:12,240 --> 00:09:13,920
you can be held criminally responsible.

191
00:09:13,920 --> 00:09:15,200
And in that case, a bookstore owner

192
00:09:15,200 --> 00:09:17,040
was convicted of violating the ordinance

193
00:09:17,040 --> 00:09:19,280
by selling a novel called Sweeter Than Life,

194
00:09:19,280 --> 00:09:20,960
which tells the story of a, quote,

195
00:09:21,040 --> 00:09:23,200
ruthless lesbian businesswoman.

196
00:09:23,200 --> 00:09:24,640
The Supreme Court said that the bookstore

197
00:09:24,640 --> 00:09:25,680
couldn't be responsible

198
00:09:25,680 --> 00:09:27,200
for reviewing every single thing

199
00:09:27,200 --> 00:09:29,360
in the book or magazine that it sells.

200
00:09:29,360 --> 00:09:30,800
And distributors only had liability

201
00:09:30,800 --> 00:09:31,840
if someone notified them

202
00:09:31,840 --> 00:09:34,160
that something they carried was illegal.

203
00:09:34,160 --> 00:09:36,560
That's sort of the origin of the knowledge requirement.

204
00:09:36,560 --> 00:09:37,840
And platforms were common carriers

205
00:09:37,840 --> 00:09:40,160
like phone companies and television broadcasters.

206
00:09:40,160 --> 00:09:42,240
Platforms weren't liable for third-party content

207
00:09:42,240 --> 00:09:42,800
that they carried.

208
00:09:42,800 --> 00:09:44,720
So, for example, let's say AT&T found out

209
00:09:44,720 --> 00:09:46,240
that someone's answering machine

210
00:09:46,240 --> 00:09:48,080
had a libelous outgoing message.

211
00:09:48,080 --> 00:09:50,160
Was AT&T required to act

212
00:09:50,160 --> 00:09:52,720
by canceling the owner's telephone service?

213
00:09:52,720 --> 00:09:53,680
Well, the court said no.

214
00:09:53,680 --> 00:09:55,200
AT&T couldn't be sued for libel

215
00:09:55,200 --> 00:09:56,640
simply because the owner used its service

216
00:09:56,640 --> 00:09:58,000
to say something libelous.

217
00:09:58,000 --> 00:09:59,760
And similarly, a broadcaster wasn't liable

218
00:09:59,760 --> 00:10:01,280
for running a political candidate

219
00:10:01,280 --> 00:10:03,680
that falsely accused someone of being a communist.

220
00:10:04,320 --> 00:10:06,400
But then the internet came around,

221
00:10:06,400 --> 00:10:08,320
and the internet era posed a new challenge

222
00:10:08,320 --> 00:10:09,440
for this case law.

223
00:10:09,440 --> 00:10:11,120
Two of the first internet service providers,

224
00:10:11,120 --> 00:10:12,320
CompuServe and Prodigy,

225
00:10:12,320 --> 00:10:14,160
were sued for hosting forums

226
00:10:14,160 --> 00:10:16,320
where users posted defamatory content.

227
00:10:16,320 --> 00:10:17,520
Now, Prodigy billed itself

228
00:10:17,520 --> 00:10:19,520
as a family-friendly version of the internet.

229
00:10:19,520 --> 00:10:21,280
Did you get a computer recently?

230
00:10:21,280 --> 00:10:22,880
Well, congratulations.

231
00:10:22,880 --> 00:10:24,880
You can now join hundreds of thousands

232
00:10:24,880 --> 00:10:26,320
of discovered Prodigy.

233
00:10:26,320 --> 00:10:27,840
It moderated comments,

234
00:10:27,840 --> 00:10:29,760
removing things it thought were bad.

235
00:10:29,760 --> 00:10:32,000
Now, CompuServe didn't moderate anything.

236
00:10:32,000 --> 00:10:34,640
CompuServe combines the power of your computer

237
00:10:34,640 --> 00:10:36,400
with the convenience of your telephone.

238
00:10:36,400 --> 00:10:37,920
And both of those companies were sued

239
00:10:37,920 --> 00:10:39,360
for defamatory statements

240
00:10:39,360 --> 00:10:41,920
that their users posted to the service.

241
00:10:41,920 --> 00:10:44,240
Now, the case against CompuServe was dismissed

242
00:10:44,240 --> 00:10:46,560
because the court considered it a distributor of content

243
00:10:46,560 --> 00:10:48,160
rather than a publisher.

244
00:10:48,160 --> 00:10:51,120
CompuServe could only be held liable for defamation

245
00:10:51,120 --> 00:10:53,200
if it knew or had reason to know

246
00:10:53,200 --> 00:10:55,600
of the defamatory nature of the content.

247
00:10:55,600 --> 00:10:56,800
And since they didn't do anything

248
00:10:56,800 --> 00:10:58,160
to moderate their forums,

249
00:10:58,160 --> 00:10:59,520
the company couldn't possibly know,

250
00:10:59,520 --> 00:11:01,280
or at least argued it couldn't possibly know,

251
00:11:01,280 --> 00:11:03,520
about what the denizens of Rumorville

252
00:11:03,520 --> 00:11:05,200
were posting about each other.

253
00:11:05,200 --> 00:11:07,520
But Prodigy was found liable

254
00:11:07,520 --> 00:11:09,600
for failing to moderate enough.

255
00:11:09,600 --> 00:11:10,640
Someone went on a forum

256
00:11:10,640 --> 00:11:12,800
and claimed that Jordan Belfort's investment firm,

257
00:11:12,800 --> 00:11:13,920
Stratton Oakmont,

258
00:11:13,920 --> 00:11:16,480
committed fraud in connection with a stock IPO.

259
00:11:16,560 --> 00:11:18,240
The firm sued Prodigy

260
00:11:18,240 --> 00:11:20,560
and the anonymous poster for defamation.

261
00:11:20,560 --> 00:11:22,640
And a court held that Prodigy was liable

262
00:11:22,640 --> 00:11:24,480
as the publisher of the content

263
00:11:24,480 --> 00:11:26,080
created by its users

264
00:11:26,080 --> 00:11:28,320
since it exercised editorial control

265
00:11:28,320 --> 00:11:30,160
over the messages on its bulletin board.

266
00:11:30,160 --> 00:11:31,520
Now, if the names Jordan Belfort

267
00:11:31,520 --> 00:11:33,520
and Stratton Oakmont ring a bell,

268
00:11:33,520 --> 00:11:36,000
that's because they were literally scamming people

269
00:11:36,000 --> 00:11:37,120
out of millions of dollars

270
00:11:37,120 --> 00:11:39,200
and have gone down as some of the biggest fraudsters

271
00:11:39,200 --> 00:11:40,960
in all of Wall Street history.

272
00:11:40,960 --> 00:11:42,880
The movie The Wolf of Wall Street

273
00:11:42,880 --> 00:11:44,320
is based on the fraud

274
00:11:44,320 --> 00:11:45,520
that they were perpetrating

275
00:11:45,520 --> 00:11:47,120
with respect to the IPOs.

276
00:11:47,120 --> 00:11:48,560
And it's one of the biggest ironies

277
00:11:48,560 --> 00:11:50,320
in all of First Amendment law

278
00:11:50,320 --> 00:11:52,160
that they won this lawsuit

279
00:11:52,160 --> 00:11:54,080
against the people that were hosting

280
00:11:54,080 --> 00:11:55,600
presumably accurate information

281
00:11:55,600 --> 00:11:57,600
about how they were scamming people out of money.

282
00:11:57,600 --> 00:11:58,560
That's part of the problem

283
00:11:58,560 --> 00:12:01,120
when websites are liable for user content

284
00:12:01,120 --> 00:12:04,000
is that people can file lawsuits to stifle speech.

285
00:12:04,000 --> 00:12:06,560
That's what a slap lawsuit is all about.

286
00:12:06,560 --> 00:12:09,200
And because lawsuits are incredibly expensive,

287
00:12:09,200 --> 00:12:12,160
sometimes even if you scammed people out of money,

288
00:12:12,160 --> 00:12:13,840
as long as you file a lawsuit,

289
00:12:13,840 --> 00:12:15,760
you can stifle that kind of speech.

290
00:12:15,760 --> 00:12:16,960
But I digress.

291
00:12:16,960 --> 00:12:18,320
As a result of these cases,

292
00:12:18,320 --> 00:12:19,600
internet service providers

293
00:12:19,600 --> 00:12:22,400
basically either decided to not moderate at all

294
00:12:22,400 --> 00:12:25,280
and leave absolutely everything up

295
00:12:25,280 --> 00:12:27,200
or basically moderate everything

296
00:12:27,200 --> 00:12:29,120
and not allow anything to go up.

297
00:12:29,120 --> 00:12:30,880
And basically no one was happy

298
00:12:30,880 --> 00:12:31,840
with this state of affairs.

299
00:12:31,840 --> 00:12:34,480
So Congress enacted the Communications Decency Act.

300
00:12:34,480 --> 00:12:36,720
And then as now, lawmakers claim

301
00:12:36,720 --> 00:12:37,920
to be especially worried

302
00:12:37,920 --> 00:12:40,080
about things like harassment and obscene material.

303
00:12:40,080 --> 00:12:42,160
So they gave online providers immunity

304
00:12:42,160 --> 00:12:44,160
from lawsuits if they moderated content.

305
00:12:44,160 --> 00:12:45,440
And again, the irony here

306
00:12:45,440 --> 00:12:46,960
is that this legislation came

307
00:12:46,960 --> 00:12:50,320
from the sort of family values type politicians,

308
00:12:50,320 --> 00:12:53,280
the people who always want to think about the children.

309
00:12:53,280 --> 00:12:57,360
Oh, won't somebody please think of the children?

310
00:12:57,360 --> 00:12:58,960
They wanted to allow websites

311
00:12:58,960 --> 00:13:02,160
to remove violent and sexual conducts

312
00:13:02,160 --> 00:13:04,960
so that the internet could be even cleaner.

313
00:13:04,960 --> 00:13:06,480
The same politicians who wanted

314
00:13:06,480 --> 00:13:08,880
and got the little parental advisory

315
00:13:08,880 --> 00:13:11,520
explicit content warnings put on rap albums.

316
00:13:11,600 --> 00:13:13,040
But I digress again.

317
00:13:13,600 --> 00:13:16,560
Section 230 was intended as a way

318
00:13:16,560 --> 00:13:18,240
for internet companies to create

319
00:13:18,240 --> 00:13:21,840
and enforce basic standards to run their websites.

320
00:13:21,840 --> 00:13:24,720
And when the CompuServe and Prodigy cases were decided,

321
00:13:24,720 --> 00:13:26,560
the courts only considered whether the ISPs

322
00:13:26,560 --> 00:13:28,000
were publishers or distributors.

323
00:13:28,000 --> 00:13:29,680
But an internet service like a website

324
00:13:29,680 --> 00:13:31,360
didn't fit neatly into those categories.

325
00:13:31,360 --> 00:13:33,680
A traditional publisher has total control

326
00:13:33,680 --> 00:13:35,280
over whether to publish content.

327
00:13:35,280 --> 00:13:37,360
A distributor can control what it buys,

328
00:13:37,360 --> 00:13:39,120
but it wouldn't have been practical

329
00:13:39,120 --> 00:13:41,840
for a human to fully screen every item.

330
00:13:41,840 --> 00:13:44,400
Modern social media companies host more material

331
00:13:44,400 --> 00:13:46,800
than any library or bookstore on earth,

332
00:13:46,800 --> 00:13:49,760
which makes screening all of that info daunting,

333
00:13:49,760 --> 00:13:51,120
if not impossible.

334
00:13:51,120 --> 00:13:52,240
And without Section 230,

335
00:13:52,240 --> 00:13:53,680
social media companies would be subject

336
00:13:53,680 --> 00:13:56,080
to strict liability for every message and post

337
00:13:56,080 --> 00:13:57,360
made on their services.

338
00:13:57,360 --> 00:14:00,080
So Congress chose to give those platforms and websites

339
00:14:00,080 --> 00:14:01,280
immunity as an incentive

340
00:14:01,280 --> 00:14:03,680
to get them to remove offending speech.

341
00:14:03,680 --> 00:14:05,440
Congress also chose not to dictate

342
00:14:05,440 --> 00:14:07,760
to those companies which speech they had to remove.

343
00:14:07,760 --> 00:14:10,400
So that brings us back to the original question in this case.

344
00:14:10,400 --> 00:14:12,960
How should courts read Section 230?

345
00:14:12,960 --> 00:14:14,880
The lower courts follow the tests set forth

346
00:14:14,880 --> 00:14:16,960
in the Ninth Circuit case called Barnes vs. Yahoo.

347
00:14:16,960 --> 00:14:19,840
And the Barnes test hues close to the text of Section 230.

348
00:14:19,840 --> 00:14:21,920
The law specifies that only one,

349
00:14:21,920 --> 00:14:23,840
no interactive computer service,

350
00:14:23,840 --> 00:14:26,480
shall be two, treated as a publisher or speaker,

351
00:14:26,480 --> 00:14:29,200
of three, content provided by another information

352
00:14:29,200 --> 00:14:31,360
content provider, aka users.

353
00:14:31,360 --> 00:14:32,480
Now the plaintiffs argue their claims

354
00:14:32,480 --> 00:14:34,000
do not treat Google as a publisher,

355
00:14:34,000 --> 00:14:37,600
but instead assert a simple duty not to support terrorists.

356
00:14:37,600 --> 00:14:40,480
They said that the ATA prohibits Walmart from supplying

357
00:14:40,480 --> 00:14:43,040
fertilizer, knives, or other material to ISIS.

358
00:14:43,040 --> 00:14:44,960
And in the same way, the ATA bars Google

359
00:14:44,960 --> 00:14:47,360
from giving ISIS a platform to communicate.

360
00:14:47,360 --> 00:14:49,360
The Ninth Circuit found this analogy specious.

361
00:14:49,360 --> 00:14:51,200
The idea of a duty not to support terrorists,

362
00:14:51,200 --> 00:14:54,000
quote, overlooks that publication itself

363
00:14:54,000 --> 00:14:57,280
is the form of support Google allegedly provided to ISIS.

364
00:14:57,280 --> 00:14:59,520
And publication of third party content

365
00:14:59,520 --> 00:15:02,400
is what Section 230 gives to internet services.

366
00:15:02,400 --> 00:15:05,280
According to lower courts, publishing encompasses,

367
00:15:05,280 --> 00:15:07,040
quote, any activity that can be boiled

368
00:15:07,040 --> 00:15:08,880
down to deciding whether to exclude material

369
00:15:08,880 --> 00:15:11,200
that third parties seek to post online.

370
00:15:11,200 --> 00:15:13,120
Google says that the CDA uses the words

371
00:15:13,120 --> 00:15:15,840
publisher or speaker in an ordinary sense,

372
00:15:15,840 --> 00:15:17,840
quote, based on an ordinary meaning,

373
00:15:17,840 --> 00:15:20,800
a publisher or speaker is one that publishes or speaks.

374
00:15:20,800 --> 00:15:22,720
Google contends that when YouTube's algorithms

375
00:15:22,720 --> 00:15:25,600
make recommendations, this activity is protected by Section 230,

376
00:15:25,600 --> 00:15:27,680
quote, Congress underscored that publishing

377
00:15:27,680 --> 00:15:30,560
for purposes of Section 230 includes sorting content

378
00:15:30,560 --> 00:15:33,840
via algorithms by defining interactive computer service

379
00:15:33,840 --> 00:15:37,040
to include tools that pick, choose, filter, search,

380
00:15:37,040 --> 00:15:39,600
subset, organize, or reorganize content.

381
00:15:39,600 --> 00:15:42,400
Congress intended to provide protection for these functions

382
00:15:42,400 --> 00:15:44,560
not for simply hosting third party content.

383
00:15:44,560 --> 00:15:45,600
And the plaintiffs disagree.

384
00:15:45,600 --> 00:15:48,320
Their interpretation of the Section 230C defense

385
00:15:48,320 --> 00:15:50,480
is that it requires a narrower interpretation

386
00:15:50,480 --> 00:15:51,440
of the word publisher.

387
00:15:51,440 --> 00:15:54,320
The Ninth Circuit previously held that Section 230C1

388
00:15:54,320 --> 00:15:56,160
uses publisher in its everyday sense

389
00:15:56,160 --> 00:15:57,360
and the Second Circuit agreed.

390
00:15:57,360 --> 00:15:59,360
But the plaintiffs contend that the word publisher

391
00:15:59,360 --> 00:16:03,040
is used in Section 230C1 with a narrower and distinct meaning,

392
00:16:03,120 --> 00:16:04,960
which that term has in defamation law.

393
00:16:04,960 --> 00:16:06,560
Basically, the plaintiffs are claiming that

394
00:16:06,560 --> 00:16:09,760
anytime YouTube recommends another video,

395
00:16:09,760 --> 00:16:12,560
it therefore becomes the speaker of that video

396
00:16:12,560 --> 00:16:15,520
as if YouTube had produced that video itself.

397
00:16:15,520 --> 00:16:17,920
Now, defendants say that this interpretation doesn't make sense,

398
00:16:17,920 --> 00:16:20,320
quote, watch the World Series of Poker on YouTube

399
00:16:20,320 --> 00:16:23,280
and YouTube's algorithms might display Texas Hold'em tutorials.

400
00:16:23,280 --> 00:16:25,440
That does not mean that YouTube endorses gambling

401
00:16:25,440 --> 00:16:29,040
any more than spell check endorses a suggested substitute word.

402
00:16:29,040 --> 00:16:31,680
Westlaw endorses higher listed cases

403
00:16:31,680 --> 00:16:34,720
or a chat room endorses posts organized by topic.

404
00:16:34,720 --> 00:16:35,920
As the Ninth Circuit noted,

405
00:16:35,920 --> 00:16:38,800
YouTube applies the same algorithms to all content.

406
00:16:38,800 --> 00:16:40,800
And it's here that I think people often misunderstand

407
00:16:40,800 --> 00:16:43,520
the nature of the internet and the nature of social media.

408
00:16:44,080 --> 00:16:45,760
With the case of YouTube,

409
00:16:45,760 --> 00:16:48,560
people upload millions of videos a day.

410
00:16:48,560 --> 00:16:53,680
If there wasn't an algorithm to sort through this deluge of videos,

411
00:16:53,680 --> 00:16:56,400
the only way to sort through them would be a fire hose

412
00:16:56,400 --> 00:17:00,560
of viewing every video chronologically, which nobody wants.

413
00:17:00,560 --> 00:17:04,480
It would be that or doing a pointed search to try and find content.

414
00:17:04,480 --> 00:17:06,560
But then you'd never be exposed to content

415
00:17:06,560 --> 00:17:07,920
that you actually do want to watch,

416
00:17:07,920 --> 00:17:10,000
but didn't know that you wanted to seek it out.

417
00:17:10,000 --> 00:17:11,760
And the plaintiffs have tried to differentiate

418
00:17:11,760 --> 00:17:15,760
between search results and other algorithmically generated results.

419
00:17:15,760 --> 00:17:18,160
But there's really no principled way to distinguish between

420
00:17:18,160 --> 00:17:21,600
results that are sorted when you actively search for something

421
00:17:21,600 --> 00:17:23,840
versus results that are presented to you

422
00:17:23,840 --> 00:17:25,600
because of a recommendation algorithm.

423
00:17:25,600 --> 00:17:28,960
And people love to hate algorithms, myself included.

424
00:17:28,960 --> 00:17:30,160
But the truth is,

425
00:17:30,160 --> 00:17:32,640
if there weren't algorithms to help us sort through

426
00:17:32,640 --> 00:17:35,280
all of the content on social media platforms,

427
00:17:35,280 --> 00:17:37,120
it would be impossible to wade through it.

428
00:17:37,120 --> 00:17:37,760
And then additionally,

429
00:17:37,760 --> 00:17:39,840
there is the issue of the material support

430
00:17:39,840 --> 00:17:42,400
related for a claim under the Anti-Terrorism Act.

431
00:17:42,400 --> 00:17:45,520
Now, the Department of Justice's brief in support of the plaintiffs

432
00:17:46,160 --> 00:17:48,080
argues that algorithmic recommendations

433
00:17:48,080 --> 00:17:49,840
convey the website's own implicit message

434
00:17:49,840 --> 00:17:52,000
that users will find the information relevant,

435
00:17:52,000 --> 00:17:53,360
and that makes the website liable

436
00:17:53,360 --> 00:17:56,560
because it's acting just like ISIS itself.

437
00:17:56,560 --> 00:17:58,880
Now, plaintiffs didn't claim this in their original complaint,

438
00:17:58,880 --> 00:18:01,440
instead they focused on how YouTube's recommendations

439
00:18:01,440 --> 00:18:02,800
amplify the ISIS speech,

440
00:18:02,800 --> 00:18:04,800
making it more visible to more users.

441
00:18:04,800 --> 00:18:06,480
But the US government urged the Supreme Court

442
00:18:06,480 --> 00:18:09,200
to adopt a narrow reading of Section 230

443
00:18:09,200 --> 00:18:10,800
that would permit a finding that Google

444
00:18:10,800 --> 00:18:12,240
materially contributed to the content

445
00:18:12,240 --> 00:18:14,400
by matching it to user preferences.

446
00:18:14,400 --> 00:18:16,720
Again, basically, the Department of Justice,

447
00:18:16,720 --> 00:18:17,760
as well as the plaintiffs,

448
00:18:17,760 --> 00:18:22,480
are arguing for a standard that if any website promotes content,

449
00:18:22,480 --> 00:18:25,680
uses an algorithm, or recommends content to users,

450
00:18:25,680 --> 00:18:27,600
that that website then becomes liable

451
00:18:27,680 --> 00:18:31,040
for all of the speech contained within that recommendation.

452
00:18:31,040 --> 00:18:33,440
What happens when websites are liable?

453
00:18:33,440 --> 00:18:34,720
Well, remember I mentioned

454
00:18:34,720 --> 00:18:37,280
that this has actually happened before?

455
00:18:37,280 --> 00:18:38,240
Well, it has.

456
00:18:38,240 --> 00:18:40,720
In 2018, then President Trump signed into law

457
00:18:40,720 --> 00:18:43,920
two bills making it easier to fight sex trafficking online.

458
00:18:43,920 --> 00:18:46,240
That was the Fight Online Sex Trafficking Act

459
00:18:46,240 --> 00:18:49,040
and the Stop Enabling Sex Traffickers Act,

460
00:18:49,040 --> 00:18:50,800
otherwise known as FOSTA and SESTA.

461
00:18:50,800 --> 00:18:52,560
And those two laws created an exception

462
00:18:52,560 --> 00:18:54,320
to Section 230's Safe Harbor Rule

463
00:18:54,320 --> 00:18:57,520
by stating that websites could be held legally responsible

464
00:18:57,760 --> 00:19:00,320
if third parties posted ads for prostitution

465
00:19:00,320 --> 00:19:01,360
on their platforms.

466
00:19:01,360 --> 00:19:02,720
The law penalizes websites

467
00:19:02,720 --> 00:19:05,680
which quote, promote or facilitate prostitution.

468
00:19:05,680 --> 00:19:08,080
And while this includes sites that promote illegal sex work,

469
00:19:08,080 --> 00:19:11,040
it also allows the police to investigate any site

470
00:19:11,040 --> 00:19:13,440
that is knowingly assisting, facilitating,

471
00:19:13,440 --> 00:19:15,040
or supporting sex trafficking.

472
00:19:15,040 --> 00:19:16,880
This language can be read to include sites

473
00:19:16,880 --> 00:19:19,680
with content that is legal, such as escort services,

474
00:19:19,680 --> 00:19:21,520
personal ads, and pornography.

475
00:19:21,520 --> 00:19:23,920
Now, critics of FOSTA and SESTA warned

476
00:19:23,920 --> 00:19:25,520
that the new laws were too broad

477
00:19:25,600 --> 00:19:27,600
and they would end up killing internet content

478
00:19:27,600 --> 00:19:29,920
that has nothing to do with sex trafficking.

479
00:19:29,920 --> 00:19:31,680
And they turned out to be correct.

480
00:19:31,680 --> 00:19:33,520
If a third party posts content

481
00:19:33,520 --> 00:19:35,840
that is covered by FOSTA or SESTA,

482
00:19:35,840 --> 00:19:38,000
and the website fails to prevent that content

483
00:19:38,000 --> 00:19:40,720
from being posted, that website can be sued.

484
00:19:40,720 --> 00:19:43,760
And that was simply too much legal risk for several websites.

485
00:19:43,760 --> 00:19:46,320
You should have gone for the head.

486
00:19:48,160 --> 00:19:51,520
Craigslist shut down its entire personal section.

487
00:19:51,520 --> 00:19:53,760
Tumblr banned all adult content

488
00:19:53,760 --> 00:19:55,600
and Reddit removed several subreddits

489
00:19:55,600 --> 00:19:58,480
and other websites just simply shut down entirely.

490
00:19:58,480 --> 00:20:00,560
So you might ask, did any of this assist

491
00:20:00,560 --> 00:20:02,880
with the prosecution of sex trafficking?

492
00:20:02,880 --> 00:20:06,000
Well, in 2021, the U.S. Government Accountability Office

493
00:20:06,000 --> 00:20:09,280
released a report on the first three years of FOSTA and SESTA

494
00:20:09,280 --> 00:20:11,040
finding that quote, over three years,

495
00:20:11,040 --> 00:20:13,520
the Department of Justice filed just one case

496
00:20:13,520 --> 00:20:15,040
under its rules against promoting

497
00:20:15,040 --> 00:20:17,200
or recklessly disregarding sex trafficking.

498
00:20:17,200 --> 00:20:19,120
So ironically, this provided essentially

499
00:20:19,120 --> 00:20:21,920
the perfect natural experiment to find out what happens

500
00:20:22,000 --> 00:20:25,520
when websites are liable for the speech content of its users.

501
00:20:25,520 --> 00:20:26,720
One of two things will happen.

502
00:20:26,720 --> 00:20:29,120
Either the website shuts down that section

503
00:20:29,120 --> 00:20:30,720
or shuts down the entire department,

504
00:20:31,360 --> 00:20:34,160
or the website employs an army of moderators

505
00:20:34,160 --> 00:20:36,960
to remove anything even remotely offensive.

506
00:20:36,960 --> 00:20:38,560
You basically either live in a world

507
00:20:38,560 --> 00:20:42,320
where a website allows everything, scams, spam,

508
00:20:42,320 --> 00:20:44,800
pornography, ultraviolence,

509
00:20:44,800 --> 00:20:46,800
just everything that you can possibly imagine

510
00:20:46,800 --> 00:20:49,840
that you probably don't want to see in your everyday life,

511
00:20:49,840 --> 00:20:53,040
or you live in a world where they employed

512
00:20:53,040 --> 00:20:55,200
extremely expensive moderators

513
00:20:55,200 --> 00:20:58,000
who prevent anything that might possibly be offensive.

514
00:20:58,000 --> 00:20:59,280
And that of course assumes that a website

515
00:20:59,280 --> 00:21:01,760
can even afford to employ those moderators.

516
00:21:01,760 --> 00:21:03,520
So there are those who want to see

517
00:21:03,520 --> 00:21:05,760
section 230 modified or scrapped altogether

518
00:21:05,760 --> 00:21:07,600
and have argued that these online services

519
00:21:07,600 --> 00:21:09,440
are acting like content developers.

520
00:21:09,440 --> 00:21:11,200
Now oral argument in front of the Supreme Court

521
00:21:11,200 --> 00:21:13,120
is next week, and I'm putting my money

522
00:21:13,120 --> 00:21:14,480
where my mouth is on this one.

523
00:21:14,480 --> 00:21:16,000
I filed a brief in the Supreme Court,

524
00:21:16,000 --> 00:21:18,000
along with Dr. Mike, Mythical Entertainment,

525
00:21:18,080 --> 00:21:20,720
Chubby Emu, Tim Schmoyer, and The Author's Alliance,

526
00:21:20,720 --> 00:21:22,400
and a bunch of other creators.

527
00:21:22,400 --> 00:21:24,160
We filed an amicus brief explaining

528
00:21:24,160 --> 00:21:25,840
the potential dire consequences

529
00:21:25,840 --> 00:21:27,120
of a really bad ruling on this.

530
00:21:27,120 --> 00:21:28,480
So we'll see how it goes.

531
00:21:28,480 --> 00:21:29,280
But in the meantime,

532
00:21:29,280 --> 00:21:30,960
I'll probably just be stress eating

533
00:21:30,960 --> 00:21:32,160
at the end of the internet.

534
00:21:32,160 --> 00:21:34,400
Unless I use today's sponsor, Factor 75,

535
00:21:34,400 --> 00:21:36,560
because Factor makes meeting your nutrition goals

536
00:21:36,560 --> 00:21:38,080
easier than ever by delivering fresh,

537
00:21:38,080 --> 00:21:40,000
never-frozen, dietitian-approved meals

538
00:21:40,000 --> 00:21:40,880
right to your doorstep,

539
00:21:40,880 --> 00:21:42,880
and the meals are completely ready to eat.

540
00:21:42,880 --> 00:21:44,560
Another team of gourmet chefs creates

541
00:21:44,560 --> 00:21:46,400
each meal using ingredients with integrity

542
00:21:46,400 --> 00:21:48,080
to help you feel your best all day long.

543
00:21:48,080 --> 00:21:49,920
And I can tell you from experience,

544
00:21:49,920 --> 00:21:51,920
they really are extremely delicious.

545
00:21:51,920 --> 00:21:53,920
Factor supports wholesome eating, made simple.

546
00:21:53,920 --> 00:21:55,280
Their menus are updated weekly

547
00:21:55,280 --> 00:21:56,960
with 34 different options.

548
00:21:56,960 --> 00:21:58,160
Choose your favorite meals,

549
00:21:58,160 --> 00:21:59,680
or let Factor craft the order

550
00:21:59,680 --> 00:22:01,840
based on your taste preferences in meal history.

551
00:22:01,840 --> 00:22:03,680
Factor takes the guesswork out of grocery shopping

552
00:22:03,680 --> 00:22:04,240
and meal prep,

553
00:22:04,240 --> 00:22:06,240
saving you time and energy for other things.

554
00:22:06,240 --> 00:22:07,920
There are no hassle-prepared foods.

555
00:22:07,920 --> 00:22:10,720
Make sure you always have something nutritious on hand

556
00:22:10,720 --> 00:22:12,960
when you don't have time to think about making a meal.

557
00:22:12,960 --> 00:22:15,280
And sometimes I just want a good, healthy meal

558
00:22:15,280 --> 00:22:16,560
without having to cook,

559
00:22:16,560 --> 00:22:18,720
or obviously shop for the ingredients.

560
00:22:18,720 --> 00:22:20,400
And Factor's meals arrive preprepared

561
00:22:20,400 --> 00:22:21,920
and ready to eat in just two minutes.

562
00:22:21,920 --> 00:22:23,120
And I can always scale up my meals

563
00:22:23,120 --> 00:22:24,800
if I need the extra energy

564
00:22:24,800 --> 00:22:26,320
after a long day of lawyering.

565
00:22:26,320 --> 00:22:28,720
So give Factor a try by heading to Factor75.com

566
00:22:28,720 --> 00:22:30,560
and use the code LegalEagle50

567
00:22:30,560 --> 00:22:33,040
to get 50% off your first Factor box.

568
00:22:33,040 --> 00:22:35,200
Or just click on the link that's on screen right now

569
00:22:35,200 --> 00:22:36,240
or down in the description

570
00:22:36,240 --> 00:22:37,920
and use the code LegalEagle50

571
00:22:37,920 --> 00:22:40,240
to get 50% off your first Factor box.

572
00:22:40,240 --> 00:22:42,400
And clicking that link really helps out this channel.

573
00:22:42,400 --> 00:22:43,840
And after that, click on this box over here

574
00:22:43,840 --> 00:22:44,800
for more LegalEagle,

575
00:22:44,800 --> 00:22:46,480
or I'll see you in court.

