WEBVTT

00:00.000 --> 00:10.280
Thanks so much for inviting me.

00:10.280 --> 00:15.440
This is a great, this is a great class and a great program and I'm really excited to

00:15.440 --> 00:20.000
see deep learning front and center as part of IAP.

00:20.000 --> 00:24.040
I understand you've covered kind of the basics of deep learning and I'm going to tell you

00:24.040 --> 00:30.080
today about something that's a little bit of a mashup on top of sort of standard deep

00:30.080 --> 00:33.080
learning kind of going beyond deep learning.

00:33.080 --> 00:36.960
But before I start, I just want to say something about this word artificial intelligence because

00:36.960 --> 00:39.480
you know I had artificial intelligence in the last slide.

00:39.480 --> 00:42.320
If you look at my business card, it actually says artificial intelligence in two separate

00:42.320 --> 00:45.520
places on that card and I'll have a confession.

00:45.520 --> 00:50.480
I'm a recovering academic so I was a professor at Harvard for ten years.

00:50.480 --> 00:52.560
I just joined IBM about two years ago.

00:52.560 --> 00:53.760
This is my first real job.

00:53.760 --> 00:58.280
My mom's very proud of me that I finally got out of school.

00:58.280 --> 01:04.320
But I will say as an academic researcher working on AI, we hated this term.

01:04.320 --> 01:09.360
2017 and before, we would do anything we could not to say these words.

01:09.360 --> 01:13.440
We'd say machine learning or we'd say deep learning, be more specific.

01:13.440 --> 01:17.520
But 2018 and beyond, for whatever reason, we've all given up and we're all calling it

01:17.520 --> 01:18.520
AI.

01:18.520 --> 01:22.840
We're calling it AI, Google's calling it AI, academics are calling it AI.

01:22.840 --> 01:27.400
But when I got to IBM, they had done something that I really appreciated and it helps kind

01:27.400 --> 01:28.400
of frame the discussion.

01:28.400 --> 01:31.320
It will frame the discussion about what I'm going to tell you about research-wise in a

01:31.320 --> 01:32.320
minute.

01:32.320 --> 01:33.320
And that's just to do something very simple.

01:33.320 --> 01:36.760
This is part of something that IBM does called the global technology outlook, which is like

01:36.760 --> 01:41.440
an annual process where we envision for the company, for the corporation what the future

01:41.440 --> 01:43.480
lies holds ahead.

01:43.480 --> 01:47.160
And they did something very simple just to put adjectives in front of AI, just to distinguish

01:47.160 --> 01:49.400
what we're talking about when we're talking about different things.

01:49.400 --> 01:54.040
So this will be relevant to where we want to push relative to where we are today with

01:54.040 --> 01:56.400
deep learning to where we want to go.

01:56.400 --> 01:59.960
And that's to distinguish what we have today as narrow AI.

01:59.960 --> 02:04.600
So it's not to say it's not powerful or disruptive, but just to say that it's limited in important

02:04.600 --> 02:05.800
ways.

02:05.800 --> 02:09.720
And also to kind of distinguish it from general AI, which is the stuff that the public and

02:09.720 --> 02:11.400
the press likes to talk about sometimes.

02:11.400 --> 02:15.720
When IBM Research, which if you don't know, we're a 4,300-person global research organization,

02:15.720 --> 02:21.200
we have six Nobel Prizes, we've been around for 75 years, when IBM Research tried to decide

02:21.200 --> 02:25.080
when this was going to happen, when general AI was going to happen, we said 2050 and beyond.

02:25.080 --> 02:28.840
And basically when you ask scientists something and they tell you 2050 and beyond when it's

02:28.840 --> 02:32.400
coming, that means we have no idea, but it's no time soon.

02:32.400 --> 02:34.120
But in the middle is this notion of broad AI.

02:34.120 --> 02:39.760
And that's really what we're here today about and what the lab I run is about.

02:39.760 --> 02:45.920
And just to unpack this one level deeper, on one hand, we have general AI.

02:45.920 --> 02:51.880
This is this idea of broadly autonomous systems that can decide what they do on their own.

02:51.880 --> 02:55.840
This is the kind of thing that Elon Musk described as summoning the demon.

02:55.840 --> 02:57.360
So congratulations, everyone.

02:57.360 --> 03:02.160
You're helping to summon the demon, according to Elon Musk, or slightly more level-headed

03:02.160 --> 03:06.280
people like Stephen Hawking, warning that artificial intelligence could end mankind.

03:06.320 --> 03:09.840
This is kind of, you know, maybe we need to worry about this in the future, but actually

03:09.840 --> 03:13.360
what I'll argue in just a minute is that we're actually in quite a bit more limited space

03:13.360 --> 03:14.360
right now.

03:14.360 --> 03:19.120
And really it's this broad AI that we really need to focus on.

03:19.120 --> 03:22.680
So how do we build systems that are multitasking, multi-domain, that can take knowledge from

03:22.680 --> 03:27.000
one place, supply it in another, that can incorporate lots of different kinds of data,

03:27.000 --> 03:31.800
not just images or video, but images, video, text, structure data, unstructured data, it's

03:31.800 --> 03:35.480
distributed, it runs in the cloud, it also runs in edge devices, and it's explainable.

03:35.480 --> 03:37.440
So we can understand what these systems do.

03:37.440 --> 03:41.320
And this is basically then the roadmap for everything that my lab does.

03:41.320 --> 03:46.640
So we're asking, what are the barriers we need to break down to bring in this era where

03:46.640 --> 03:49.720
we can apply AI to all the different kinds of problems that we need to apply to?

03:49.720 --> 03:51.920
So things like explainability.

03:51.920 --> 03:55.360
We need to have systems that aren't just black boxes, but we can look inside and understand

03:55.360 --> 03:57.640
why they make decisions, when they make a right decision.

03:57.640 --> 04:00.880
We know why it made that decision, when they make a wrong decision.

04:00.880 --> 04:05.000
We have the ability to reach in and figure out how we would debug that system.

04:05.000 --> 04:10.400
One interesting thing about the AI revolution, back in the day, people said that software

04:10.400 --> 04:12.200
was going to eat the world.

04:12.200 --> 04:18.560
And these days, Jensen Wang, the CEO of NVIDIA, is on record saying that AI is going to eat

04:18.560 --> 04:19.560
software.

04:19.560 --> 04:21.040
And I think increasingly that's true.

04:21.040 --> 04:26.720
We're going to have data-driven software systems that are based on technology like deep learning.

04:26.720 --> 04:29.040
But the terrifying thing about that is we don't really yet have debuggers.

04:29.040 --> 04:32.640
And it's very hard in many cases to figure out why systems aren't working.

04:32.640 --> 04:36.320
So this is something that's really holding back AI today.

04:36.320 --> 04:39.720
Security, I'll tell you a little bit about the kind of weird world of security we live

04:39.720 --> 04:43.120
in now with AI, where AI systems can be hacked in interesting ways.

04:43.120 --> 04:47.480
We can close those gaps to be able to really realize the full potential of AI.

04:47.480 --> 04:50.440
Systems need to be fair and unbiased.

04:50.440 --> 04:53.720
That's both the thing that's good for the world, but it's also the case that in many

04:53.720 --> 04:59.160
regulated industries, like the kinds of companies that IBM serves, like banks, they're regulated

04:59.160 --> 05:02.440
such that the government insists that their systems be provably fair.

05:02.440 --> 05:06.400
We need to be able to look inside, see, and understand that the decisions the system will

05:06.400 --> 05:07.920
make will be fair and unbiased.

05:07.920 --> 05:13.400
And then on a practical level, I think the real battleground going forward for deep learning

05:13.400 --> 05:18.600
and for AI in general, as much as people talk about big data, actually the most interesting

05:18.600 --> 05:23.400
battlegrounds that we see across many different industries all have to do with small data.

05:23.400 --> 05:27.040
So how do we work with very small amounts of data?

05:27.040 --> 05:31.320
It turns out if you look across all the businesses that make the world run, heavy industries,

05:31.320 --> 05:37.360
healthcare, financial services, most of the problems that those companies faced and that

05:37.360 --> 05:42.520
we face in the world in general don't have enormous, annotated, carefully curated data

05:42.520 --> 05:43.960
sets to go with them.

05:43.960 --> 05:49.280
So if we're going to be able to use AI broadly and tackle all of these hard problems that

05:49.280 --> 05:53.160
we want to solve, we need to be able to learn how to do more with less data.

05:53.160 --> 05:57.320
So part of that has to do with things like transfer learning, learning to transfer from

05:57.320 --> 06:01.960
one domain to another, so learning one domain and then use that knowledge somewhere else.

06:01.960 --> 06:05.320
But increasingly, and this is what I'm going to tell you about today, there's this notion

06:05.320 --> 06:06.640
of reasoning.

06:06.640 --> 06:10.880
So how do we not only extract the structure of the data we're looking at, the data domain

06:10.880 --> 06:16.360
we're interested in, but then also be able to logically and fluidly reason about that

06:16.360 --> 06:17.860
data?

06:17.860 --> 06:20.560
And then finally, just to close it out, just to give you a little bit of a pitch about

06:20.560 --> 06:24.600
what the lab is and what we do, there's also a piece about infrastructure that we think

06:24.600 --> 06:25.760
is really important.

06:25.760 --> 06:32.200
So if you track energy usage from computing year over year, by some estimates by the year

06:32.200 --> 06:37.640
2040, if we keep increasing our energy usage due to computing, we'll exceed the power budget

06:37.640 --> 06:39.360
of the planet Earth.

06:39.360 --> 06:42.760
There won't be enough solar radiation from the sun, not enough stuff we can dig up out

06:42.760 --> 06:46.120
of the Earth and burn to fuel our computing habit.

06:46.120 --> 06:51.960
And AI isn't helping, deep learning is not helping, so many models that we train will

06:51.960 --> 06:58.120
take the equivalent energy of running a whole city for several days, just for one model.

06:58.120 --> 07:01.040
And that's obviously not going to last for a long time.

07:01.040 --> 07:06.320
So we also do work both at the algorithmic level, some of which I'll tell you about today,

07:06.320 --> 07:10.320
but also at the physics level to ask, can we build different kinds of computers?

07:10.320 --> 07:15.360
So this is a diagram of a memristive device, this is an analog computer, which we think

07:15.360 --> 07:19.720
we can get power consumption for deep learning workloads down by maybe a factor of 100 or

07:19.720 --> 07:20.720
even a thousand.

07:21.360 --> 07:22.920
And we're also working in quantum computing.

07:22.920 --> 07:25.320
IBM, as you may know, is one of the leaders in quantum.

07:25.320 --> 07:28.640
We have some of the biggest quantum computers that are available today, and we're asking

07:28.640 --> 07:31.440
how that all interacts with AI.

07:31.440 --> 07:37.160
So when IBM, when we set out to do this challenge of how do we make AI broadly applicable to

07:37.160 --> 07:41.960
all the kinds of problems that we'd like to apply AI to, just as a small plug for the

07:41.960 --> 07:46.080
lab since we're here, we decided we didn't want to do it alone, and we chose a partner.

07:46.080 --> 07:48.640
And in particular, we chose MIT.

07:48.640 --> 07:54.560
And actually, the idea being that this is one of the last standing industrial research

07:54.560 --> 07:59.160
labs of the Bell Lab era, IBM Research, together with MIT, which obviously needs no introduction

07:59.160 --> 08:01.920
because we're here right now, and we're partnering around AI.

08:01.920 --> 08:05.600
And just to give you a little bit of historical context, it actually turns out that IBM and

08:05.600 --> 08:09.960
MIT have been together since the beginning of AI, literally since the term artificial

08:09.960 --> 08:16.200
intelligence was coined, way back in 1956, so right when the very first computers were

08:16.200 --> 08:17.200
being developed.

08:17.200 --> 08:23.120
Nathaniel Rochester, who's the gentleman right there, who developed the IBM 701, which is

08:23.120 --> 08:29.080
one of the first practical computers, got together with MIT professors like John McCarthy,

08:29.080 --> 08:31.760
and dreamed up this future of AI.

08:31.760 --> 08:32.760
And it's actually really fascinating.

08:32.760 --> 08:38.640
I encourage you all to go and find the proposal for this workshop, because a lot of the language,

08:38.640 --> 08:43.400
including neural network language, is all here, like they got a lot of the words right.

08:43.400 --> 08:49.800
They were just a little bit off on the time scale, maybe like seven decades off.

08:49.800 --> 08:51.280
But really interesting.

08:51.280 --> 08:55.600
And the partnership here, the idea here is that we're combining the long horizon, time

08:55.600 --> 09:02.360
horizon that MIT brings to the creation of knowledge, maybe 100-year time horizons, departments

09:02.360 --> 09:05.960
of everything in chemistry, biology, economics, and physics, together with IBM, where we have

09:05.960 --> 09:10.080
a lot of those same departments because we're such a big research organization.

09:10.080 --> 09:13.800
But to bring those together with industry problems, to bring data to the table, so we

09:13.800 --> 09:17.680
can do the kind of research we want to do, and to bring the compute resources along as

09:17.680 --> 09:18.680
well.

09:18.680 --> 09:21.520
So this is what the lab is, and what we do.

09:21.520 --> 09:26.480
We were founded with a quarter-billion-dollar investment over 10 years from IBM.

09:26.480 --> 09:32.960
And we have 50 projects currently, more than 50 projects currently running, over 150 researchers

09:32.960 --> 09:38.600
across MIT and IBM, and their opportunities for undergraduates, for graduate students

09:38.800 --> 09:40.200
to be involved in these projects.

09:40.200 --> 09:45.080
So if you're interested in the things I show you today, we'd love to have you join our

09:45.080 --> 09:48.520
team, either on the MIT side or on the IBM side.

09:48.520 --> 09:52.440
And we're basically drawing from all of the different departments of MIT, and even though

09:52.440 --> 09:56.840
we've only been running for about a year and a half, we have over 100 publications in

09:56.840 --> 09:58.760
top academic conferences and journals.

09:58.760 --> 10:04.560
We had 17 papers in NeurIPS just a few months ago, just to give you a sense of that everything's

10:04.560 --> 10:06.280
up and running.

10:06.280 --> 10:11.200
So this is the evolution, this is where we're going.

10:11.200 --> 10:14.880
So why do I say that today's AI is narrow?

10:14.880 --> 10:15.880
Why would I say that?

10:15.880 --> 10:26.880
Because clearly, AI is powerful, in particular, in 2015 Forbes said that deep learning and

10:26.880 --> 10:30.080
machine intelligence would eat the world.

10:30.080 --> 10:37.720
And I think it's safe to say that progress since 2012 or so has been incredibly rapid.

10:37.720 --> 10:44.240
So this was a paper that really, for me, as a researcher who was working in computer vision,

10:44.240 --> 10:47.560
really convinced me that something dramatic was happening.

10:47.560 --> 10:53.040
So this was a paper from Andre Carpathi, who now leads Tesla's AI program, together with

10:53.040 --> 10:55.760
Fei-Fei Li, who created the ImageNet dataset.

10:55.760 --> 10:59.800
And they built a system which you probably have studied a little bit in this course,

10:59.800 --> 11:04.640
where they can take an image and produce a beautiful natural language caption.

11:04.640 --> 11:09.200
So it takes an input like this, and it produces a caption like, a man in a black shirt is

11:09.200 --> 11:10.800
playing a guitar.

11:10.800 --> 11:15.760
Or you take in this image and you get a construction worker in an orange safety vest is working

11:15.760 --> 11:17.720
on the road.

11:17.720 --> 11:22.720
When I started studying computer vision and AI and machine learning, I wasn't sure we

11:22.720 --> 11:28.280
were going to actually achieve this even in my career or perhaps even in my lifetime.

11:28.280 --> 11:31.640
It seems like such science fiction, it's so commonplace now that we have systems that

11:31.640 --> 11:32.720
can do that.

11:32.720 --> 11:40.960
So it's hard to overstate how important deep learning has been in the progress of AI and

11:40.960 --> 11:41.960
machine learning.

11:41.960 --> 11:47.600
You know, meanwhile, there are very few games left that humans are better than machines

11:47.600 --> 11:48.600
at.

11:48.600 --> 11:55.800
Everything from Jeopardy, which IBM did way back in 2011, to Go with AlphaGo from DeepMind.

11:55.800 --> 12:01.560
I think Carnegie Mellon created a system that could beat the world champion in poker.

12:01.560 --> 12:05.560
And recently, my own company created a system called Project Debater that can actually carry

12:05.560 --> 12:10.320
on a pretty credible natural language debate with a debate champion.

12:10.320 --> 12:15.720
So if you like your computers to argue with you, we can do that for you now.

12:15.720 --> 12:20.440
And even domains like art, which we would have thought maybe would have been privileged

12:20.440 --> 12:25.360
domains for humanity, like surely machines can't create art, right?

12:26.000 --> 12:27.000
That's not the case.

12:27.000 --> 12:32.120
So even way back in 2015, which now feels like a long time ago, Matias Bekka's group

12:32.120 --> 12:36.840
at the Max Planck in Tubingen created the system with Style Transfer, where you could

12:36.840 --> 12:41.880
go from a photograph of your own and then re-render it in the style of any artist you

12:41.880 --> 12:42.880
like.

12:42.880 --> 12:47.280
So this is a very simple Style Transfer model that leveraged the internal representation

12:47.280 --> 12:52.880
of a convolutional neural network up to what we have today, which is, again, just astonishing

12:52.880 --> 12:54.960
how fast progress is moving.

12:55.560 --> 13:00.240
These are the outputs from a system called BigGAN, which came from DeepMind.

13:00.240 --> 13:05.040
And all four of these images are all of things that don't exist in the real world.

13:05.040 --> 13:07.280
So this dog, not a real dog.

13:07.280 --> 13:13.800
You put it in a random vector into the bigGAN, and it generates full cloth, this beautiful

13:13.800 --> 13:17.440
high-resolution dog, or this bubble, or this cup.

13:17.440 --> 13:20.640
And this is actually going to be a problem now, because now we have this notion of deepfakes.

13:20.640 --> 13:25.920
We're getting so good at creating fake images that now we're having to come up with actual

13:25.920 --> 13:26.920
countermeasures.

13:26.920 --> 13:31.880
And that's actually one thing we're working on in the laboratory I run at IBM, where we're

13:31.880 --> 13:37.280
trying to find ganttodotes, like, antidotes, countermeasures against GANs as we move forward.

13:37.280 --> 13:41.760
So clearly, the progress is impressive.

13:41.760 --> 13:46.320
So why am I saying that AI is still narrow today?

13:47.280 --> 13:50.360
Well, does anyone know what this is?

13:50.360 --> 13:52.440
Anyone have any ideas?

13:52.440 --> 13:53.440
Yeah.

13:53.440 --> 13:56.440
Good job, but you're wrong.

13:56.440 --> 13:58.560
It turns out it's a teddy bear.

13:58.560 --> 14:05.600
So if you ask a state-of-the-art ImageNet-trained CNN, and you often see these CNNs described

14:05.600 --> 14:08.880
as being superhuman in their accuracy, has anyone heard that before?

14:08.880 --> 14:12.720
Like, they'll say, object recognition is a solved problem.

14:12.720 --> 14:15.200
These ImageNet-trained CNNs can do better than humans.

14:16.080 --> 14:19.720
And if you've ever actually looked at the ImageNet carefully, the reason that's true

14:19.720 --> 14:24.800
is because ImageNet has huge numbers of categories of dogs, so you basically need to be a dog

14:24.800 --> 14:28.480
show judge to be able to outperform a human at ImageNet.

14:28.480 --> 14:32.160
But this is starting to illustrate a problem.

14:32.160 --> 14:33.160
This image is real.

14:33.160 --> 14:38.600
So this is a piece of art in the Museum of Modern Art in New York by Merit Oppenheim,

14:38.600 --> 14:46.800
called Le Dijonnet en Fereur, a luncheon in Fereur, a little bit unsettling image.

14:46.800 --> 14:50.360
But we, like, who thought it was a teddy bear?

14:50.360 --> 14:51.360
Right?

14:51.360 --> 14:58.680
Like, the most unteddy bear-like image ever, right?

14:58.680 --> 15:02.360
Why did the CNN think this was a teddy bear?

15:02.360 --> 15:04.840
Soft and fluffy.

15:04.840 --> 15:05.840
Soft and fluffy.

15:05.840 --> 15:06.840
It's round.

15:06.840 --> 15:07.840
It's got fur.

15:07.840 --> 15:11.120
So one of the things in the training set would be round and furry teddy bears.

15:11.120 --> 15:14.040
You know, it's a little bit of a garbage-in-garbage-out kind of scenario.

15:14.040 --> 15:18.800
This is, in many ways, you know, people talk about corner cases or edge cases, those rare

15:18.800 --> 15:24.560
things that happen, but are different from the distribution you've trained on previously.

15:24.560 --> 15:27.560
And this is a great example of such a thing.

15:27.560 --> 15:31.400
So this is starting to show that even though deep learning systems we have today are amazing,

15:31.400 --> 15:35.720
and they are amazing, there's, you know, there's room for improvement, there's something missing

15:35.720 --> 15:36.720
here.

15:36.880 --> 15:41.000
Actually, if we dig a little bit deeper, which, you know, a variety of researchers have done,

15:41.000 --> 15:48.120
so this is from Alan Yule's group at Johns Hopkins, even in cases where, you know, the

15:48.120 --> 15:52.440
objects are the standard objects that the system knows how to detect its supposedly

15:52.440 --> 15:57.160
superhuman, you know, sort of levels, if we take this guitar and we put it on top of

15:57.160 --> 16:01.320
this monkey in the jungle, a couple of funny things happen.

16:01.320 --> 16:03.720
One is it thinks the guitar is a bird.

16:04.320 --> 16:06.240
Anyone have an idea why that is?

16:06.240 --> 16:09.080
I hear pieces of the answer all around.

16:09.080 --> 16:11.040
So it's colorful, right?

16:11.040 --> 16:13.080
It's a color that you would expect a tropical bird.

16:13.080 --> 16:17.720
Things that are in the jungle in distribution would tend to be colorful tropical birds.

16:17.720 --> 16:20.960
Interestingly, because you put the guitar in front of the monkey, now the monkey's a

16:20.960 --> 16:21.960
person.

16:21.960 --> 16:28.120
And, you know, again, you know, monkeys don't play guitars in the training set, and that's

16:28.120 --> 16:29.440
clearly messing with the results.

16:29.440 --> 16:36.920
So even though we have no trouble at all telling that these objects are a guitar and a monkey,

16:36.920 --> 16:40.080
the state-of-the-art systems are falling down.

16:40.080 --> 16:45.840
And then even this captioning example, which I highlighted as being, you know, an amazing

16:45.840 --> 16:50.120
success for deep learning, and it is an amazing success for deep learning, when you poke a

16:50.120 --> 16:55.560
little bit harder, which, you know, Josh Tenenbaum and Sam Gershman and Brendan Lake and Tomer

16:55.560 --> 16:57.960
Oman did, you find things like this.

16:57.960 --> 17:04.840
So this image is captioned as a man riding a motorcycle on a beach.

17:04.840 --> 17:11.160
This next one is an airplane is parked on the tarmac at an airport.

17:11.160 --> 17:17.320
And this one, next one, is a group of people standing on top of a beach, which is correct.

17:17.320 --> 17:21.200
So score one for the AI.

17:21.200 --> 17:27.360
But what you can see is there's a strong sense in which the system doesn't really understand

17:27.360 --> 17:32.760
what it's looking at, and that leads to mistakes, and that leads to sort of, you know, missing

17:32.760 --> 17:35.760
the point, you know, in many cases.

17:35.760 --> 17:41.560
And again, this has to do with the fact that the systems are trained on the data, and largely

17:41.560 --> 17:44.680
they're constrained by what data they've seen before, and things that are out of sample,

17:44.680 --> 17:49.320
these edge cases, these corner cases, tend to perform poorly.

17:49.320 --> 17:54.120
Now the success of deep learning, you know, I think it's safe to say it's, you know, two

17:54.120 --> 17:55.120
things happened.

17:55.120 --> 18:00.360
So deep learning, as you already know, is a rebrand of a technology called artificial

18:00.360 --> 18:01.360
neural networks.

18:01.360 --> 18:05.600
It dates all the way back to that Dartmouth conference, at least to the 80s.

18:05.600 --> 18:11.360
You know, a lot of the fundamental map of backprop was worked out in the 80s.

18:11.360 --> 18:16.200
We went through decades of time where it was disreputable to study neural networks, and

18:16.200 --> 18:19.920
I lived through that era where you would try and publish a paper about neural networks,

18:19.920 --> 18:23.080
and people would tell you that everyone knows that neural networks don't work.

18:23.080 --> 18:29.320
So what happened was the amount of data that was available grew enormously, so we digitalized

18:29.320 --> 18:30.320
the world.

18:30.320 --> 18:31.320
We got digital cameras.

18:31.320 --> 18:34.800
Now we're all carrying, I'm carrying like four cameras on me right now.

18:34.800 --> 18:36.240
And we took a lot of images.

18:36.240 --> 18:40.040
And then the compute caught up as well, and particularly graphics processing units, graphics

18:40.040 --> 18:43.360
processing units, GPUs became available, and it turned out they were even better for doing

18:43.360 --> 18:45.720
deep learning than they were for doing graphics.

18:45.720 --> 18:50.200
And really the seminal moment that really flipped the switch and made deep learning take off

18:50.200 --> 18:54.760
was the collection of this data set called ImageNet, which Fei-Fei Li collected, and

18:54.760 --> 19:01.840
it's basically millions of carefully curated images with categories associated with them.

19:01.840 --> 19:06.560
Now you need to have data sets of this scale to make deep learning work.

19:06.560 --> 19:10.880
So if you're working on projects now and you're training neural networks, you'll know that

19:10.880 --> 19:15.800
you need to have thousands to millions of images to be able to train a network and have

19:15.800 --> 19:17.800
it perform well.

19:17.800 --> 19:20.200
That's in stark contrast to how we work.

19:20.200 --> 19:22.720
So does anyone know what this object is?

19:22.720 --> 19:24.560
Just a quick raise of your hands.

19:24.560 --> 19:27.560
Okay, a few people, not so many.

19:27.560 --> 19:31.200
But even though you've never seen this object before, a single training example, you're

19:31.200 --> 19:33.440
now all experts in this object.

19:33.440 --> 19:34.880
Just one training example.

19:34.880 --> 19:39.240
So I can show you to you and ask, is that object present in this image?

19:39.240 --> 19:41.240
I think we all agree, yes.

19:41.240 --> 19:45.440
I can ask you questions like how many are in this image, and I think we'd all agree there

19:45.440 --> 19:47.400
are two.

19:47.400 --> 19:50.520
And I can even ask you, is it present in this image?

19:50.520 --> 19:56.660
And I think you'd all agree, yeah, but it's weird, right?

19:56.660 --> 20:02.880
So not only can you recognize the object from a single training example, not thousands,

20:02.880 --> 20:08.400
not millions, one, you can reason about it now in context where it's just weird, right?

20:08.400 --> 20:12.440
And that's why you can tell that it's a fur-covered saucer cup and spoon and not a teddy bear

20:12.440 --> 20:14.440
because you have this ability to reason out a sample.

20:14.440 --> 20:19.560
And that's really a remarkable ability that we'd love to have because when you get past

20:19.560 --> 20:24.680
imagery, you get past digital images, there are very few data sets that have this kind

20:24.680 --> 20:28.340
of scale that ImageNet has.

20:28.340 --> 20:33.040
But even ImageNet turns out, you know, there's something else wrong with it.

20:33.040 --> 20:37.520
So does anyone notice anything about these chairs in the image, these were all taken

20:37.520 --> 20:39.720
from ImageNet, from the chairs category?

20:39.720 --> 20:42.520
Does anyone notice anything unusual about these?

20:42.880 --> 20:46.640
They're all facing the camera, they're in canonical views, they're all more or less centered.

20:46.640 --> 20:50.040
In the case where there's multiple chairs, they're kind of like almost like a texture

20:50.040 --> 20:51.200
of chairs, right?

20:51.200 --> 20:54.280
So these are very unusual images, actually.

20:54.280 --> 20:57.480
I mean, we look at them and we think these are normal images of chairs, but these are

20:57.480 --> 21:01.720
actually very carefully posed and crafted images.

21:01.720 --> 21:06.480
And one of the projects that we've been working on together with MIT across the MIT IBM Lab

21:06.480 --> 21:12.480
was, this is a project that was led by Boris Katz and Andre Barbou together with our own

21:12.600 --> 21:17.800
Dan Gutfreund, they asked, okay, well, what if we collected a data set where that wasn't

21:17.800 --> 21:22.400
true, so where we didn't have carefully, perfectly centered objects?

21:22.400 --> 21:26.080
And what they did is they enlisted a whole bunch of mechanical turkers on Amazon Mechanical

21:26.080 --> 21:31.880
Turk and they told them, take a hammer, take it into your bedroom, put it on your bed,

21:31.880 --> 21:36.360
and here's a smartphone app and please put it in this bounding box.

21:36.360 --> 21:40.960
So basically you'd have to go and the people get instructions, you know, take your chair,

21:40.960 --> 21:44.280
we want you to put it in the living room, and we want you to put it on its side and

21:44.280 --> 21:45.840
put it in that bounding box.

21:45.840 --> 21:49.360
Or we want you to take a knife out of your kitchen, put it in the bathroom, and make

21:49.360 --> 21:52.800
it fit in that bounding box, or, you know, take that bottle and put it on a chair in

21:52.800 --> 21:53.800
this orientation.

21:53.800 --> 21:57.400
So they went through and they just collected a huge amount of this data, so they collected

21:57.400 --> 22:02.720
50,000 of these images from 300 object classes that overlap with ImageNet, and then they

22:02.720 --> 22:06.960
asked the mechanical turkers to go into four different rooms with those things.

22:06.960 --> 22:12.840
So remember, everyone talks about how ImageNet is state-of-the-art in object categorization

22:12.840 --> 22:17.120
and that's a solved problem, but it turns out when you take these images of these objects

22:17.120 --> 22:22.520
that are not in the right place, humans can perform it well over 95 percent accuracy on

22:22.520 --> 22:28.280
this task, but the AIs, the CNNs that were previously performing, you know, at state-of-the-art

22:28.280 --> 22:33.560
levels, drop all the way down 40 to 45 percent down in their performance.

22:33.560 --> 22:38.240
So there's a very real sense in which, as amazing as deep learning is, and I'm going

22:38.240 --> 22:45.760
to keep saying this, deep learning is amazing, but some of the gains in the, you know, the

22:45.760 --> 22:49.920
sort of declarations of victory are a little bit overstated, and they all circle around

22:49.920 --> 22:56.680
this idea of small data, of corner cases, edge cases, and being able to reason about

22:56.680 --> 22:59.760
situations that are a little bit out of the ordinary.

22:59.760 --> 23:01.680
All right?

23:01.680 --> 23:07.320
And of course, the last piece, you know, that's concerning for anyone who's trying to deploy

23:07.320 --> 23:12.560
neural networks in the real world is that they're weirdly vulnerable to hacking.

23:12.560 --> 23:18.800
So I don't know if you guys covered adversarial examples in this class yet, but here's an

23:18.800 --> 23:25.400
example targeting that same captioning system, so where, you know, the captioning system

23:25.400 --> 23:29.120
can see this picture of a stop sign and produce this beautiful natural language caption, a

23:29.120 --> 23:32.000
red stop sign sitting on the side of a road.

23:32.000 --> 23:37.160
Our own Pinyu Chen, who's an expert in this area at IBM, created this image, which is

23:37.160 --> 23:41.800
a very subtle perturbation of the original, and you can get that to now say a brown teddy

23:41.800 --> 23:44.960
bear lying on top of the bed with high confidence.

23:44.960 --> 23:50.360
So this is a case, again, where there's something divergent between how we perceive images and

23:50.360 --> 23:56.280
understand what the content of an image is and how these end-to-end trained neural networks

23:56.280 --> 23:57.720
do the same.

23:57.720 --> 24:03.120
Like, you know, this kind of, you know, this image, the perturbations of the pixels in

24:03.120 --> 24:06.800
this image were done in such a way that they'd be small so that you couldn't see them, so

24:06.800 --> 24:10.400
they're specifically hidden from us.

24:10.400 --> 24:14.360
But it turns out that you don't have to actually have access to the digital image.

24:14.360 --> 24:18.480
You can also do real-world in-the-wild adversarial attacks.

24:18.480 --> 24:25.000
And this is one that was kind of fun, some folks in my lab and my group decided it'd

24:25.000 --> 24:27.760
be fun to have a t-shirt that was adversarial.

24:27.760 --> 24:33.520
So you took a person detector, and so this is like, you know, it'll detect a person.

24:33.520 --> 24:36.160
You can imagine, like, an AI-powered surveillance system.

24:36.160 --> 24:38.520
If you were to intrude in the building, you might want to have a person detector that could

24:38.520 --> 24:43.880
detect a person and warn somebody, hey, there's a person in your building that doesn't belong.

24:43.880 --> 24:46.720
But what they did is they created this shirt.

24:46.720 --> 24:48.360
So this shirt's very carefully crafted.

24:48.360 --> 24:51.560
It's a very loud, ugly shirt.

24:51.560 --> 24:52.560
We have it in the lab.

24:52.560 --> 24:56.320
If you want to come over any time and try it on, you're welcome to.

24:56.320 --> 24:58.960
But this shirt basically makes you invisible to AI.

24:58.960 --> 25:04.560
So this is Sija, who's our wizard adversarial example.

25:04.560 --> 25:07.520
He's not wearing the shirt, so you can see the person detector is detecting him just

25:07.520 --> 25:08.520
fine.

25:08.520 --> 25:09.880
Tron Fu is wearing the shirt.

25:09.880 --> 25:10.880
Therefore he is invisible.

25:10.880 --> 25:11.880
He is camouflaged.

25:11.880 --> 25:17.480
And you can see, even as you walk around, even if the shirt is folded and bent and wrinkled,

25:17.480 --> 25:19.360
it makes you invisible.

25:19.360 --> 25:23.040
So weird, right?

25:23.040 --> 25:27.880
If anything for us, this ugly-looking shirt makes you even more visible.

25:27.880 --> 25:32.560
So there's something weird about how deep learning seems to work relative to how weird

25:32.560 --> 25:34.400
we work.

25:34.400 --> 25:40.680
But there are also problems where, even under the best of conditions, no adversarial perturbation,

25:40.680 --> 25:45.000
you can have as much training data as you like, where deep learning still struggles.

25:45.000 --> 25:49.080
And these are really interesting for us, because these are cases where, no matter how much

25:49.080 --> 25:53.320
data you have, deep learning just doesn't cut it for some reason.

25:53.320 --> 25:54.320
And we want to know why.

25:54.320 --> 25:59.560
So problems like this, if you ask the question, so I give you a picture, and I ask the question,

25:59.560 --> 26:02.960
how many blocks are on the right of the three-level tower?

26:02.960 --> 26:06.240
Or will the block tower fall if the top block is removed?

26:06.240 --> 26:08.440
Or are there more animals than trees?

26:08.440 --> 26:12.520
Or what is the shape of the object closest to the large cylinder?

26:12.520 --> 26:16.440
These are all questions that even a child could answer, I mean, provided they understand

26:16.440 --> 26:22.880
language and can read and stuff, it's very easy for us to work on these things.

26:22.880 --> 26:25.840
But it turns out that deep learning systems, irrespective of how much training data you

26:25.840 --> 26:28.200
give, struggle.

26:28.200 --> 26:33.000
So that's a case where they're smoking, you kind of want to know where's the fire.

26:33.000 --> 26:36.920
Actually the answer we think, or one of the things we're exploring, is the idea that maybe

26:36.920 --> 26:39.880
the answer lies all the way back in 1956.

26:39.880 --> 26:46.320
So this is a picture of that Dartmouth workshop back in 1956.

26:46.320 --> 26:51.840
And back in this time period, neural networks were already, you know, had already been sort

26:51.840 --> 26:52.840
of born.

26:52.840 --> 26:56.400
We were thinking about neural networks, but we were also thinking about another kind of

26:56.400 --> 26:57.400
AI back then.

26:57.400 --> 27:00.680
And that kind of AI is interesting and different.

27:00.680 --> 27:04.560
It hasn't really enjoyed a resurgence the way that neural networks have.

27:04.560 --> 27:07.680
But just to step back for a moment, this is what you've been studying.

27:07.680 --> 27:10.600
Neural network basically is a nonlinear function approximator.

27:10.600 --> 27:15.960
You take an input in, and you get some output that you want out, and it learns the weights

27:15.960 --> 27:18.440
of the network through training with data.

27:18.440 --> 27:21.400
So this is what an apple looks like to a neural network.

27:21.400 --> 27:26.560
You know, you put an apple picture in, and you light up, you know, a unit that says there's

27:26.560 --> 27:29.360
probably an apple in that scene.

27:29.360 --> 27:33.960
There's another kind of AI that's been around since the beginning called symbolic AI.

27:33.960 --> 27:39.800
And this is from a book by Marvin Minsky in 1991 that was created here.

27:39.800 --> 27:42.800
And this is what an apple looks like to symbolic AI.

27:42.800 --> 27:44.760
So we know things about an apple.

27:44.760 --> 27:46.800
We know that an apple has an origin.

27:46.800 --> 27:47.880
It comes from an apple tree.

27:47.880 --> 27:50.120
We know that an apple is a kind of fruit.

27:50.120 --> 27:51.120
You know, that apple has parts.

27:51.120 --> 27:52.800
It has a body, and it has a stem.

27:52.800 --> 27:54.360
The body has a shape.

27:54.360 --> 27:55.360
It's round.

27:55.360 --> 27:56.360
It has a size.

27:56.360 --> 27:57.360
It can fit in your hand.

27:57.360 --> 27:58.360
It's got a color.

27:58.360 --> 27:59.720
It could be red or green.

27:59.720 --> 28:03.480
We know lots of knowledge about what an apple is.

28:03.480 --> 28:07.880
And that's a very different take on AI.

28:07.880 --> 28:12.920
And you know, basically this field of what we call good old fashioned AI or symbolic

28:12.920 --> 28:19.280
AI has been around since the very beginning, and it just hasn't yet enjoyed that resurgence

28:19.280 --> 28:21.280
that neural networks did.

28:21.280 --> 28:25.320
And one of the central theses that we're exploring is that just the same way that neural

28:25.320 --> 28:29.960
networks have been waiting, they were waiting for compute and data to come along to make

28:29.960 --> 28:31.480
them really work.

28:31.480 --> 28:37.200
We think that symbolic AI has also been waiting, but what it's been waiting for is neural networks.

28:37.200 --> 28:42.280
So now that neural networks work, can we go back to some of these ideas from symbolic

28:42.280 --> 28:45.960
AI and do something different?

28:45.960 --> 28:51.400
And the work I'm going to tell you about is a collaboration as part of the MIT IBM Lab.

28:51.400 --> 28:55.440
Chuangon is one of the researchers in my group together with Josh Tenenbaum, in particular

28:55.440 --> 29:00.560
Jojen Wu, who's now an assistant professor at Stanford, along with some others.

29:00.560 --> 29:05.280
And what they're asking is, can we mix together the ideas of neural networks together with

29:05.280 --> 29:10.080
the ideas from symbolic AI and do something that's more than the sum of its parts?

29:10.080 --> 29:14.920
And this picture that I showed you here, I showed you this earlier, this is actually

29:14.920 --> 29:21.080
a data set called Clever that was basically created to illustrate this problem, where

29:21.080 --> 29:24.720
irrespective of how much training data you have, this very simple kind of question answering

29:24.720 --> 29:27.400
task where you have to answer questions like, are there an equal number of large things

29:27.400 --> 29:30.280
in metal spheres seems to be hard.

29:30.280 --> 29:32.520
So the data set was created to illustrate the problem.

29:32.520 --> 29:37.760
And if you tackle this the way you're supposed to with neural networks and deep learning,

29:37.760 --> 29:43.280
and perhaps you've learned over the course of this IEP course that the best way to train

29:43.280 --> 29:45.480
a system is end to end.

29:45.480 --> 29:50.080
The best way to get what you want is to start from what you have and end with what you need

29:50.080 --> 29:54.280
and don't get in the way in the middle, just let the neural network do its thing.

29:54.280 --> 29:58.080
The problem is that when you build end to end neural networks and try and train them

29:58.080 --> 30:04.280
to go from these inputs to these outputs, for data sets like this it just doesn't work

30:04.280 --> 30:05.400
well at all.

30:05.400 --> 30:08.880
And the reason for that is that the concepts, things like colors and shapes and objects and

30:08.880 --> 30:13.320
things like that, and then the portions of reasoning, like counting the number of objects

30:13.320 --> 30:17.640
or reasoning about the relationships between objects are fundamentally entangled inside

30:17.640 --> 30:20.120
the representation of the neural network.

30:20.120 --> 30:24.040
And then not only does that cause problems where it's very difficult to cover the entire

30:24.040 --> 30:28.760
distribution and not get caught by corner cases, but it also means it's hard to transfer

30:28.760 --> 30:32.960
to other kinds of tasks like image captioning or instance retrieval or other kinds of things.

30:32.960 --> 30:37.080
So fundamentally this end to end approach just doesn't seem to work very well.

30:37.080 --> 30:39.680
So I'm already telling you something that's sort of probably against the advice you've

30:39.680 --> 30:42.360
gotten thus far.

30:42.360 --> 30:46.000
But when you step back and look at well how do we solve this problem of visual reasoning,

30:46.000 --> 30:49.760
you have a question like this, are there an equal number of large things in metal spheres?

30:50.400 --> 30:54.320
When we tackle this problem, well first we read the question and we see there's something

30:54.320 --> 30:55.840
about large things.

30:55.840 --> 30:59.960
We use our visual system to sort of find the large things.

30:59.960 --> 31:03.000
Then we read the question, we see there's something about metal spheres and we use our

31:03.000 --> 31:06.000
visual system to find the metal spheres.

31:06.000 --> 31:11.480
And then critically we do an operation, a symbolic operation, an equality operation

31:11.480 --> 31:15.200
where we decide are these an equal number and we say yes.

31:15.200 --> 31:16.200
So that's what we do.

31:16.240 --> 31:21.080
And if you unpack that, yeah it's got visual perception and CNNs are a great candidate

31:21.080 --> 31:22.400
for doing that.

31:22.400 --> 31:25.960
And yeah it's got question understanding, natural language processing and yeah RNNs are

31:25.960 --> 31:28.280
a great tool for doing that.

31:28.280 --> 31:33.720
But critically it also has this component of logical reasoning where you can very flexibly

31:33.720 --> 31:37.920
apply operations in a compositional way.

31:37.920 --> 31:43.920
So what the team did then was to basically this is kind of like the neurosymbolic hello

31:43.920 --> 31:44.920
world.

31:45.160 --> 31:48.160
It's the first program you write in most programming languages.

31:48.160 --> 31:52.200
This is kind of the simplest example that we could tackle.

31:52.200 --> 31:55.120
And this is the sort of diagram of the flow system and don't worry I'm going to unpack

31:55.120 --> 32:01.680
all this where because neural networks are good at vision we use a CNN to do the vision

32:01.680 --> 32:09.000
part but instead of going straight to an answer it's used to basically de-render the scene.

32:09.000 --> 32:13.280
So rendering goes from a symbolic representation to an image, de-rendering goes from the image

32:13.320 --> 32:17.560
back to some kind of symbolic structure representation so we're going to take apart the image using

32:17.560 --> 32:19.040
the neural network.

32:19.040 --> 32:24.600
And then the question, you'd be crazy not to use something like an LSTM to parse the

32:24.600 --> 32:28.880
language but instead of going from the language straight to an answer or going from the language

32:28.880 --> 32:34.640
to a label or something, the language is going to be parsed into a program, into a symbolic

32:34.640 --> 32:38.560
program which is then going to be executed on the structure representation.

32:38.560 --> 32:42.920
So just to walk you through that, we have a vision part, we have a language part.

32:42.920 --> 32:48.240
You parse the scene using a CNN so you turn, you know, you find the objects in the scene

32:48.240 --> 32:51.920
and you basically create a table that says what are the objects, what are their properties

32:51.920 --> 32:56.920
and where are they and then you do semantic parsing on the language part and again the

32:56.920 --> 33:01.400
goal here is to go from natural language with all of its, you know, sort of vagaries and

33:01.400 --> 33:08.520
messiness to a program, a program that we're going to run in a minute and so you need to

33:08.520 --> 33:13.680
learn how to take this language and turn it into a series of symbolic operations and

33:13.680 --> 33:19.840
then you're going to run that symbolic program on the structured symbolic information and

33:19.840 --> 33:21.960
get an answer.

33:21.960 --> 33:25.640
So you would start by filtering and saying I want to look, the question is asking about

33:25.640 --> 33:29.080
something red so I need to filter on red and then I need to query the shape of that object

33:29.080 --> 33:30.560
that I've just filtered.

33:30.560 --> 33:35.680
So this is basic, you know, sort of program execution.

33:35.680 --> 33:41.600
And critically the system is trained jointly with reinforcement learning so the neural

33:41.600 --> 33:46.400
network that does vision and the neural network that translates from language to a program

33:46.400 --> 33:50.480
fundamentally learns something different by virtue of being part of a hybrid symbolic

33:50.480 --> 33:52.000
system, right?

33:52.000 --> 33:56.280
So it gets reward, it doesn't get reward and you propagate gradients and all that based

33:56.280 --> 33:59.640
on whether or not the symbolic system got the right answer and we use reinforcement

33:59.640 --> 34:05.440
learning of course because you can't differentiate through the symbolic part but, you know, fundamentally

34:05.480 --> 34:09.900
this isn't just a matter of bolting neural networks onto a symbolic reasoner but rather

34:09.900 --> 34:14.240
training them jointly so that the symbolic, so the neural networks learn, extract the

34:14.240 --> 34:19.160
symbols, learn to give the right symbolic representations through experience and learning

34:19.160 --> 34:20.160
on the data.

34:20.160 --> 34:23.240
So this does a couple of really interesting things.

34:23.240 --> 34:26.800
So one of the first things you'll notice, this data set was created, this clever data

34:26.800 --> 34:31.000
set was created because it illustrated a problem with end-to-end learning but it turns out

34:31.000 --> 34:36.800
that with just a dash of symbolic execution now you can be effectively perfect on the

34:36.800 --> 34:41.120
clever data set so clever is now solved and this was actually an oral spotlight paper

34:41.120 --> 34:47.240
at NURBS because this is a big deal, previously unsolved problem now solved, that's good.

34:47.240 --> 34:50.960
But interestingly, more than that, remember I said the biggest problem with deploying

34:50.960 --> 34:54.920
neural networks in the real world is that we rarely have big data, we usually have pretty

34:54.920 --> 34:55.920
small data.

34:55.920 --> 35:01.400
So anything that reduces the sample, improves the sample efficiency of these methods is

35:01.400 --> 35:07.280
really valuable and so here's the number of training examples that the system is given,

35:07.280 --> 35:12.880
this is the accuracy of the system, the neural symbolic system is up here in blue, I'll just

35:12.880 --> 35:19.280
point out several things, one is it's always better and but if you look at, you know, down

35:19.280 --> 35:26.440
here the end-to-end train systems require close to a million examples, they're kind

35:26.440 --> 35:32.040
of, you know, okay results, not perfect but okay, the neural symbolic system again with

35:32.040 --> 35:36.920
just a dash of symbolic mixed in with just one percent of the data can do better than

35:36.920 --> 35:40.200
most of the end-to-end train systems and with just ten percent of the data, one tenth of

35:40.200 --> 35:44.040
the data can perform at effectively perfect performance.

35:44.040 --> 35:51.280
So drastically lower, you know, requirement for data, drastically higher sample efficiency.

35:51.280 --> 35:54.880
And then the last piece, remember I said explainability was super important, you know, people are actually

35:54.880 --> 35:58.600
going to really use AI systems, they need to be able to look inside and understand why

35:58.600 --> 36:03.400
the decision is made, otherwise they won't trust the AI system, they won't use it.

36:03.400 --> 36:08.240
Because the system has a symbolic choke point in the middle where you parse the question

36:08.240 --> 36:13.600
into a series of symbolic operations, we can debug the system the same way you would debug

36:13.600 --> 36:18.640
a traditional coded system, so you can see, okay well what did it do, it filtered on cyan,

36:18.640 --> 36:22.360
it filtered on metal, was that the right thing, you can just, you can understand why it made

36:22.360 --> 36:25.960
the decision it made, and if it made the wrong decision now you have some guidance on what

36:25.960 --> 36:27.720
you'd want to do next.

36:27.720 --> 36:34.160
So that was a paper from this team in 2018, Neurosymbolic BQA, actually since then there's

36:34.160 --> 36:39.960
basically been a parade of papers that have made this more and more sophisticated.

36:39.960 --> 36:45.880
So there was a paper in iClear in 2019 called the Neurosymbolic Concept Learner that relaxed

36:45.880 --> 36:50.360
the requirement that the concepts be pre-coded, there's another paper that just came out in

36:50.360 --> 36:54.400
Europe just a few months ago or last month called the Neurosymbolic Meta Concept Learner

36:54.400 --> 36:59.760
that autonomously learns new concepts, it can sort of use meta concepts to do better,

36:59.760 --> 37:03.960
and we're even now getting this to work in not just these toy images but also in real

37:03.960 --> 37:06.840
world images, which is obviously important.

37:06.840 --> 37:09.680
And we think this is actually a really interesting and profitable direction to go forward.

37:09.680 --> 37:13.880
So here's the Neurosymbolic Concept Learning, basically what's happened is we're relaxing

37:13.880 --> 37:20.480
now these concepts into concept embeddings, so when you look at an object with a CNN you

37:20.480 --> 37:27.000
can now embed into a space of color and then compare that color to stored concept embeddings,

37:27.000 --> 37:32.840
which means you can now learn new concepts dynamically and you can learn them from context.

37:32.840 --> 37:36.560
So you don't need to know that green is a color, you can figure that out and learn that,

37:36.560 --> 37:42.120
this is important because the world's full of new concepts that we're constantly encountering,

37:42.120 --> 37:45.120
so that was the sort of next innovation on the system.

37:45.120 --> 37:49.080
Also remember I said that one of the things that's magical about symbolic AI is that we

37:49.080 --> 37:54.280
can leverage lots of different kinds of knowledge, and in fact we can leverage these sort of

37:54.280 --> 38:01.000
meta relationships between different concepts, we know there are synonyms for instance, which

38:01.000 --> 38:05.800
led to this paper which was just presented last month called the Neurosymbolic Meta Concept

38:05.800 --> 38:11.920
Learner, where you can have a notion of is red the same kind of concept as green or is

38:11.920 --> 38:19.760
cube a synonym of block, which then of course lets you do things like if I go through in

38:19.760 --> 38:26.120
the regular mode here and I'm creating a representation of the object and I'm creating a symbolic program,

38:26.120 --> 38:31.920
I can do the regular thing, but then also critically I can now use relationships I know

38:31.920 --> 38:38.120
about synonyms and concept equivalencies to meta verify these things, and then I can

38:38.120 --> 38:41.480
take advantage of the fact that if I know that there's an airplane then I also know

38:41.480 --> 38:44.920
there's a plane because a plane and an airplane are synonyms.

38:44.920 --> 38:49.000
I can know that if there's any kind of kid and the answer is yes, that is there any kind

38:49.000 --> 38:53.000
of child, I know that's also yes because child and kid are synonyms.

38:53.000 --> 38:57.280
So we can start to see how we can get more and more complex and more and more sophisticated

38:57.280 --> 39:00.680
with our symbolic reasoning and do more and more and more.

39:00.680 --> 39:02.760
Of course it works well.

39:02.760 --> 39:07.520
We're also now extending since clever is now beaten, we're now looking also at, we're

39:07.520 --> 39:13.400
releasing a new data set called video clever, it's called cleverer, which is a very tortured

39:13.400 --> 39:18.720
acronym, looking at the relationships between objects and counterfactuals, what would happen

39:18.720 --> 39:22.560
if this block weren't there, so you can see that we can kind of expand to more and more

39:22.560 --> 39:25.560
sophisticated environments as we go.

39:25.640 --> 39:32.960
I'll just also say that this notion of symbolic program execution isn't the only idea from

39:32.960 --> 39:35.960
symbolic AI that we can bring together with neural networks.

39:35.960 --> 39:40.040
We're also looking at the field of planning, so there's a field of symbolic AI called planning

39:40.040 --> 39:44.680
where you try and start from an initial state and then use an action plan to arrive at some

39:44.680 --> 39:48.960
target state, which is really good for solving problems like the tower of Hanoi which you

39:48.960 --> 39:52.680
may have encountered or these kinds of slider puzzles where you need to produce a series

39:52.720 --> 39:58.600
of operations to achieve a certain end state, like make the picture into the right shape.

39:58.600 --> 40:02.960
Another area of projects that we're working on is mixing these together with neural networks

40:02.960 --> 40:06.920
so that we don't just have to rely on sort of static symbolic representations, but we

40:06.920 --> 40:10.040
can actually work in the latent space of an autoencoder.

40:10.040 --> 40:14.800
We have binary discrete autoencoders and we can actually plan in the latent space of an

40:14.800 --> 40:15.800
autoencoder.

40:15.800 --> 40:20.240
Obviously these are topics that would be a whole talk unto themselves, but I just want

40:20.280 --> 40:24.400
to give you a little bit of a flavor that this idea of mashing up neural networks and

40:24.400 --> 40:32.480
symbolic AI has a lot of range and there's a lot of room to grow and explore and lots

40:32.480 --> 40:36.520
of ideas in symbolic AI now that we can bring together and every time we do we seem to find

40:36.520 --> 40:39.520
that good things happen.

40:39.520 --> 40:45.960
So with that I'll stop just to give you one picture in your mind, these sort of two venerable

40:46.000 --> 40:47.000
traditions of AI.

40:47.000 --> 40:53.160
I think we're coming to a place where we can bring the symbolic stuff out of the closet

40:53.160 --> 40:58.400
dusted off and in many ways the power of neural networks solves many of the problems and they

40:58.400 --> 41:01.760
complement each other's strengths and weaknesses in really important and useful ways.

41:01.760 --> 41:05.520
So with that I'll stop and thank you all for your attention and if you have any questions

41:05.520 --> 41:06.520
I'm very happy to answer them.

41:06.520 --> 41:07.520
Thank you.

41:07.520 --> 41:08.520
Thank you.

41:08.520 --> 41:09.520
Thank you.

41:09.520 --> 41:10.520
Thank you.

41:10.520 --> 41:11.520
Thank you.

41:11.520 --> 41:12.520
Thank you.

41:12.520 --> 41:13.520
Thank you.

41:13.520 --> 41:14.520
Thank you.

41:14.520 --> 41:15.520
Thank you.

