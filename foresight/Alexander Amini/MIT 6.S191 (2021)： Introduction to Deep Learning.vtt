WEBVTT

00:00.000 --> 00:17.160
Good afternoon, everyone, and welcome to MIT 6S191, Introduction to Deep Learning.

00:17.160 --> 00:21.320
My name is Alexander Amini and I'm so excited to be your instructor this year along with

00:21.320 --> 00:24.640
Aviso Imani in this new virtual format.

00:24.640 --> 00:29.960
6S191 is a two-week boot camp on everything deep learning and will cover a ton of material

00:30.240 --> 00:31.560
in only two weeks.

00:31.560 --> 00:35.160
So I think it's really important for us to dive right in with these lectures.

00:35.160 --> 00:39.600
But before we do that, I do want to motivate exactly why I think this is such an awesome

00:39.600 --> 00:41.360
field to study.

00:41.360 --> 00:45.880
And when we taught this class last year, I decided to try introducing the class very

00:45.880 --> 00:46.880
differently.

00:46.880 --> 00:53.400
And instead of me telling the class how great 6S191 is, I wanted to let someone else do

00:53.400 --> 00:54.480
that instead.

00:54.480 --> 01:00.640
So actually, I want to start this year by showing you how we introduced 6S191 last year.

01:00.640 --> 01:11.440
Hi, everybody, and welcome to MIT 6S191, the official introductory course on deep learning

01:11.440 --> 01:14.440
taught here at MIT.

01:14.440 --> 01:22.720
Deep learning is revolutionizing so many fields from robotics, medicine, and everything

01:22.720 --> 01:23.720
in between.

01:23.720 --> 01:31.520
You'll learn from the medals of this field and how you can build some of these incredible

01:31.520 --> 01:32.520
algorithms.

01:32.520 --> 01:42.720
In fact, this entire speech and video are not real and were created using deep learning

01:42.720 --> 01:44.720
and artificial intelligence.

01:44.720 --> 01:48.720
And in this class, you'll learn how.

01:49.720 --> 01:59.720
It has been an honor to speak with you today, and I hope you enjoyed the course.

01:59.720 --> 02:03.720
So in case you couldn't tell, that was actually not a real video or audio.

02:03.720 --> 02:08.720
And the audio you actually heard was purposely degraded, a bit more to even make it more

02:08.720 --> 02:12.720
obvious that this was not real and avoid some potential misuse.

02:13.720 --> 02:18.720
Even with the purposely degraded audio, that intro went somewhat viral last year after

02:18.720 --> 02:22.420
the course and we got some really great and interesting feedback.

02:22.420 --> 02:27.080
And to be honest, after last year and when we did this, I thought it was going to be

02:27.080 --> 02:31.000
really hard for us to top it this year.

02:31.000 --> 02:34.720
But actually, I was wrong because the one thing I love about this field is that it's

02:34.720 --> 02:40.720
moving so incredibly fast that even within the past year, the state of the art has significantly

02:40.720 --> 02:41.720
advanced.

02:41.720 --> 02:47.480
And the video you saw that we used last year used deep learning, but it was not a particularly

02:47.480 --> 02:49.520
easy video to create.

02:49.520 --> 02:54.680
It required a full video of Obama speaking, and it used this to intelligently stitch

02:54.680 --> 02:59.440
together parts of the scene to make it look and appear like he was mouthing the words

02:59.440 --> 03:00.440
that I said.

03:00.440 --> 03:05.720
And to see the behind the scenes here, now you can see the same video with my voice.

03:05.720 --> 03:15.520
Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning

03:15.520 --> 03:18.960
taught here at MIT.

03:18.960 --> 03:24.380
Now it's actually possible to use just a single static image, not the full video to

03:24.380 --> 03:27.160
achieve the exact same thing.

03:27.160 --> 03:34.800
And now you can actually see eight more examples of Obama now just created using just a single

03:34.800 --> 03:41.000
static image, no more full dynamic videos, but we can achieve the same incredible realism

03:41.000 --> 03:43.440
and result using deep learning.

03:43.440 --> 03:49.200
Now of course, there's nothing restricting us to one person.

03:49.200 --> 03:54.200
This method generalizes to different faces, and there's nothing restricting us even to

03:54.200 --> 04:00.160
humans anymore or individuals that the algorithm has ever seen before.

04:00.160 --> 04:09.440
Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning

04:09.440 --> 04:13.960
taught here at MIT.

04:13.960 --> 04:20.080
The ability to generate these types of dynamic moving videos from only a single image is

04:20.080 --> 04:24.680
remarkable to me, and it's a testament to the true power of deep learning.

04:24.680 --> 04:30.640
In this class, you're going to actually not only learn about the technical basis of this

04:30.640 --> 04:39.800
technology, but also some of the very important and very important ethical and societal implications

04:39.800 --> 04:42.080
of this work as well.

04:42.080 --> 04:47.720
Now I hope this was a really great way to get you excited about this course and 6S191,

04:47.720 --> 04:50.240
and with that let's get started.

04:50.240 --> 04:54.240
We can actually start by taking a step back and asking ourselves what is deep learning,

04:54.320 --> 04:58.000
deep learning, and the context of intelligence.

04:58.000 --> 05:04.680
Intelligence is actually the ability to process information such that it can be used to inform

05:04.680 --> 05:06.680
a future decision.

05:06.680 --> 05:12.040
Now the field of artificial intelligence, or AI, is a science that actually focuses

05:12.040 --> 05:17.360
on building algorithms to do exactly this, to build algorithms to process information

05:17.360 --> 05:21.640
such that they can inform future predictions.

05:21.640 --> 05:27.440
Now machine learning, you can think of this as just a subset of AI that actually focuses

05:27.440 --> 05:33.160
on teaching an algorithm to learn from experiences without being explicitly programmed.

05:33.160 --> 05:37.480
Now deep learning takes this idea even further, and it's a subset of machine learning that

05:37.480 --> 05:44.120
focuses on using neural networks to automatically extract useful patterns in raw data, and then

05:44.120 --> 05:49.520
using these patterns or features to learn to perform that task.

05:49.520 --> 05:51.800
And that's exactly what this class is about.

05:51.800 --> 05:57.200
This class is about teaching algorithms how to learn a task directly from raw data.

05:57.200 --> 06:03.000
And we want to provide you with a solid foundation both technically and practically for you to

06:03.000 --> 06:11.080
understand under the hood how these algorithms are built and how they can learn.

06:11.080 --> 06:16.480
So this course is split between technical lectures as well as project software labs.

06:16.480 --> 06:20.600
We'll cover the foundation starting today with neural networks, which are really the

06:20.600 --> 06:24.960
building blocks of everything that we'll see in this course.

06:24.960 --> 06:30.880
And this year we also have two brand new really exciting hot topic lectures, focusing on uncertainty

06:30.880 --> 06:36.800
and probabilistic deep learning, as well as algorithmic bias and fairness.

06:36.800 --> 06:41.520
Finally we'll conclude with some really exciting guest lectures and student project presentations

06:41.520 --> 06:46.800
as part of a final project competition that all of you will be eligible to win some really

06:46.800 --> 06:49.440
exciting prizes.

06:49.440 --> 06:53.560
Now a bit of logistics before we dive into the technical side of the lecture.

06:53.560 --> 06:58.240
For those of you taking this course for credit, you will have two options to fulfill your

06:58.240 --> 06:59.800
credit requirement.

06:59.800 --> 07:07.600
The first option will be to actually work in teams of up to four or individually to develop

07:07.600 --> 07:10.880
a cool new deep learning idea.

07:10.920 --> 07:14.960
Now doing so will make you eligible to win some of the prizes that you can see on the

07:14.960 --> 07:17.000
right hand side.

07:17.000 --> 07:21.360
And we realize that in the context of this class, which is only two weeks, that's an

07:21.360 --> 07:25.840
extremely short amount of time to come up with an impressive project or research idea.

07:25.840 --> 07:30.840
So we're not going to be judging you on the novelty of that idea, but rather we're not

07:30.840 --> 07:35.360
going to be judging you on the results of that idea, but rather the novelty of the idea,

07:35.360 --> 07:39.880
your thinking process and how impactful this idea can be.

07:39.880 --> 07:43.560
But not on the results themselves.

07:43.560 --> 07:48.080
On the last day of class, you will actually give a three minute presentation to a group

07:48.080 --> 07:52.000
of judges who will then award the winners and the prizes.

07:52.000 --> 07:57.320
Now again, three minutes is extremely short to actually present your ideas and present

07:57.320 --> 08:03.360
your project, but I do believe that there's an art to presenting and conveying your ideas

08:03.360 --> 08:06.720
concisely and clearly in such a short amount of time.

08:06.720 --> 08:12.720
So we will be holding you strictly to that strict deadline.

08:12.720 --> 08:18.120
The second option to fulfill your grade requirement is to write a one page review on a deep learning

08:18.120 --> 08:19.640
paper.

08:19.640 --> 08:23.040
Here the grade is based more on the clarity of the writing and the technical communication

08:23.040 --> 08:25.000
of the main ideas.

08:25.000 --> 08:29.400
This will be due on Thursday, the last Thursday of the class, and you can pick whatever deep

08:29.400 --> 08:30.880
learning paper you would like.

08:30.880 --> 08:36.520
If you would like some pointers, we have provided some guide papers that can help you get started

08:36.520 --> 08:40.720
if you would just like to use one of those for your review.

08:40.720 --> 08:48.240
In addition to the final project prizes, we'll also be awarding this year three lab prizes,

08:48.240 --> 08:51.880
one associated to each of the software labs that students will complete.

08:51.880 --> 08:56.840
Again, completion of the software labs is not required for grade of this course, but

08:56.840 --> 08:59.640
it will make you eligible for some of these cool prizes.

08:59.640 --> 09:05.840
So please, we encourage everyone to compete for these prizes and get the opportunity to

09:05.840 --> 09:08.920
win them all.

09:08.920 --> 09:12.080
Please post to Piazza if you have any questions.

09:12.080 --> 09:17.200
Visit the course website for announcements and digital recordings of the lectures, etc.

09:17.200 --> 09:20.840
And please email us if you have any questions.

09:20.840 --> 09:26.040
Also there are software labs and office hours right after each of these technical lectures

09:26.040 --> 09:28.080
held in Gather Town.

09:28.080 --> 09:33.040
So please drop by in Gather Town to ask any questions about the software labs, specifically

09:33.040 --> 09:37.320
on those, or more generally about past software labs or about the lecture that occurred that

09:37.320 --> 09:40.520
day.

09:40.520 --> 09:48.200
Now this course has an incredible group of TAs and teaching assistants that you can reach

09:48.200 --> 09:54.040
out to at any time in case you have any issues or questions about the material that you're

09:54.040 --> 09:56.040
learning.

09:56.040 --> 10:00.200
And finally, we want to give a huge thanks to all of our sponsors who, without their

10:00.240 --> 10:03.320
help, this class would not be possible.

10:03.320 --> 10:07.880
This is the fourth year that we're teaching this class, and each year it just keeps getting

10:07.880 --> 10:12.080
bigger and bigger and bigger, and we really give a huge shout out to our sponsors for

10:12.080 --> 10:15.400
helping us make this happen each year.

10:15.400 --> 10:19.600
And especially this year in light of the virtual format.

10:19.600 --> 10:22.080
So now let's start with the fun stuff.

10:22.080 --> 10:27.800
Let's start by asking ourselves a question about why do we all care about deep learning?

10:27.800 --> 10:32.120
And specifically, why do we care right now?

10:32.120 --> 10:38.280
To understand that, it's important to actually understand first why is deep learning or how

10:38.280 --> 10:42.000
is deep learning different from traditional machine learning?

10:42.000 --> 10:47.880
Now traditionally, machine learning algorithms define a set of features in their data.

10:47.880 --> 10:53.440
Usually these are features that are handcrafted or hand engineered, and as a result they tend

10:53.440 --> 10:56.920
to be pretty brittle in practice when they're deployed.

10:56.920 --> 11:02.440
The key idea of deep learning is to learn these features directly from data in a hierarchical

11:02.440 --> 11:03.760
manner.

11:03.760 --> 11:08.640
That is, can we learn, if we want to learn how to detect a face, for example, can we

11:08.640 --> 11:14.880
learn to first start by detecting edges in the image, composing these edges together

11:14.880 --> 11:20.760
to detect mid-level features such as a eye or a nose or a mouth, and then going deeper

11:20.760 --> 11:26.680
and composing these features into structural, facial features, so that we can recognize

11:26.680 --> 11:29.440
this face.

11:29.440 --> 11:34.280
This hierarchical way of thinking is really core to deep learning as core to everything

11:34.280 --> 11:37.880
that we're going to learn in this class.

11:37.880 --> 11:43.480
Actually the fundamental building blocks though of deep learning and neural networks have

11:43.480 --> 11:45.480
actually existed for decades.

11:45.480 --> 11:49.920
So one interesting thing to consider is why are we studying this now?

11:49.920 --> 11:55.760
Now is an incredibly amazing time to study these algorithms, and for one reason is because

11:55.760 --> 11:58.800
data has become much more pervasive.

11:58.800 --> 12:04.640
These models are extremely hungry for data, and at the moment we're living in an era

12:04.640 --> 12:07.480
where we have more data than ever before.

12:07.480 --> 12:13.160
Secondly, these algorithms are massively parallelizable, so they can benefit tremendously

12:13.160 --> 12:19.320
from modern GPU hardware that simply did not exist when these algorithms were developed.

12:19.320 --> 12:24.360
And finally, due to open source toolboxes like TensorFlow, building and deploying these

12:24.360 --> 12:30.200
models has become extremely streamlined.

12:30.200 --> 12:35.180
So let's start actually with the fundamental building block of deep learning and of every

12:35.180 --> 12:36.620
neural network.

12:36.620 --> 12:40.760
That is just a single neuron, also known as a perceptron.

12:40.760 --> 12:45.520
So we're going to walk through exactly what is a perceptron, how it's defined, and we're

12:45.520 --> 12:49.920
going to build our way up to deeper neural networks all the way from there.

12:49.920 --> 12:54.400
So let's start really at the basic building block.

12:54.400 --> 12:59.000
The idea of a perceptron or a single neuron is actually very simple.

12:59.000 --> 13:03.720
So I think it's really important for all of you to understand this at its core.

13:03.720 --> 13:08.200
Let's start by actually talking about the forward propagation of information through

13:08.200 --> 13:10.280
this single neuron.

13:10.280 --> 13:16.840
We can define a set of inputs xi through xm, which you can see on the left-hand side,

13:16.840 --> 13:21.840
and each of these inputs or each of these numbers are multiplied by their corresponding

13:21.840 --> 13:25.040
weight and then added together.

13:25.040 --> 13:30.240
We take this single number, the result of that addition, and pass it through what's

13:30.240 --> 13:37.680
called a nonlinear activation function to produce our final output y.

13:37.680 --> 13:42.400
We can actually, actually this is not entirely correct because one thing I forgot to mention

13:42.400 --> 13:47.200
is that we also have what's called a bias term in here, which allows you to shift your

13:47.200 --> 13:50.160
activation function left or right.

13:50.160 --> 13:55.560
Now on the right-hand side of this diagram, you can actually see this concept illustrated

13:55.560 --> 13:59.240
or written out mathematically as a single equation.

13:59.240 --> 14:04.760
You can actually rewrite this in terms of linear algebra matrix multiplications and

14:04.760 --> 14:10.840
dot products to represent this a bit more concisely.

14:10.840 --> 14:13.120
So let's do that.

14:13.120 --> 14:19.200
Let's now do that with x, capital X, which is a vector of our inputs, x1 through xm,

14:19.200 --> 14:24.040
and capital W, which is a vector of our weights, w1 through wm.

14:24.040 --> 14:30.280
So each of these are vectors of length m, and the output is very simply obtained by taking

14:30.280 --> 14:39.280
their dot product, adding a bias, which in this case is w0, and then applying a nonlinearity,

14:39.280 --> 14:41.880
g.

14:41.880 --> 14:47.080
One thing is that I haven't, I've been mentioning it a couple of times, this nonlinearity, g.

14:47.080 --> 14:49.280
What exactly is it?

14:49.280 --> 14:53.560
Because I've mentioned it now a couple of times, well, it is a nonlinear function.

14:53.560 --> 15:00.160
One common example of this nonlinear activation function is what is known as the sigmoid function.

15:00.240 --> 15:02.760
Defined here on the right.

15:02.760 --> 15:06.120
In fact, there are many types of nonlinear functions.

15:06.120 --> 15:10.560
You can see three more examples here, including the sigmoid function.

15:10.560 --> 15:15.600
And throughout this presentation, you'll actually see these TensorFlow code blocks,

15:15.600 --> 15:20.200
which will actually illustrate how we can take some of the topics that we're learning

15:20.200 --> 15:27.080
in this class and actually practically use them using the TensorFlow software library.

15:27.520 --> 15:31.920
Now the sigmoid activation function, which I presented on the previous slide, is very

15:31.920 --> 15:34.400
popular since it's a function that gives outputs.

15:34.400 --> 15:40.880
It takes as input any real number, any activation value, and it outputs a number always between

15:40.880 --> 15:42.640
zero and one.

15:42.640 --> 15:47.000
So this makes it really, really suitable for problems and probability, because probabilities

15:47.000 --> 15:49.160
also have to be between zero and one.

15:49.160 --> 15:52.920
So this makes them very well suited for those types of problems.

15:52.920 --> 15:57.400
In modern deep neural networks, the ReLU activation function, which you can see on the

15:57.400 --> 16:01.440
right, is also extremely popular because of its simplicity.

16:01.440 --> 16:04.280
In this case, it's a piecewise linear function.

16:04.280 --> 16:11.160
It is zero before when it's in the negative regime, and it is strictly the identity function

16:11.160 --> 16:13.600
in the positive regime.

16:13.600 --> 16:20.280
But one really important question that I hope that you're asking yourselves right now is

16:20.320 --> 16:23.960
why do we even need activation functions?

16:23.960 --> 16:29.440
I think actually throughout this course, I do want to say that no matter what I say in

16:29.440 --> 16:34.640
the course, I hope that always you're questioning why this is a necessary step and why do we

16:34.640 --> 16:38.240
need each of these steps, because often these are the questions that can lead to really

16:38.240 --> 16:40.720
amazing research breakthroughs.

16:40.720 --> 16:43.480
So why do we need activation functions?

16:43.480 --> 16:48.040
Now the point of an activation function is to actually introduce non-linearities into

16:48.080 --> 16:51.680
our network, because these are non-linear functions.

16:51.680 --> 16:55.920
And it allows us to actually deal with non-linear data.

16:55.920 --> 17:01.120
This is extremely important in real life, especially because in the real world, data

17:01.120 --> 17:03.840
is almost always non-linear.

17:03.840 --> 17:07.640
Imagine I told you to separate here the green points from the red points, but all you could

17:07.640 --> 17:10.280
use is a single straight line.

17:10.280 --> 17:15.480
You might think this is easy with multiple lines or curved lines, but you can only use

17:15.480 --> 17:18.000
a single straight line.

17:18.000 --> 17:22.040
And that's what using a neural network with a linear activation function would be like.

17:22.040 --> 17:26.840
That makes the problem really hard, because no matter how deep the neural network is,

17:26.840 --> 17:32.360
you'll only be able to produce a single line decision boundary, and you're only able to

17:32.360 --> 17:36.080
separate your space with one line.

17:36.080 --> 17:42.080
Now using non-linear activation functions allows your neural network to approximate arbitrarily

17:42.080 --> 17:43.840
complex functions.

17:43.840 --> 17:49.200
And that's what makes neural networks extraordinarily powerful.

17:49.200 --> 17:53.520
Let's understand this with a simple example so that we can build up our intuition even

17:53.520 --> 17:55.120
further.

17:55.120 --> 18:01.120
Imagine I give you this trained network, now with weights on the left hand side, 3 and

18:01.120 --> 18:03.120
negative 2.

18:03.120 --> 18:07.040
This network only has two inputs, x1 and x2.

18:07.040 --> 18:11.920
If we want to get the output of it, we simply do the same story as I said before.

18:12.280 --> 18:18.400
First take a dot product of our inputs with our weights, add the bias, and apply a non-linearity.

18:18.400 --> 18:23.760
But let's take a look at what's inside of that non-linearity.

18:23.760 --> 18:31.480
It's simply a weighted combination of our inputs in the form of a two dimensional line,

18:31.480 --> 18:34.680
because in this case we only have two inputs.

18:34.680 --> 18:39.360
So if we want to compute this output, it's the same stories before, we take a dot product

18:39.360 --> 18:43.640
of x and w, we add our bias, and apply our non-linearity.

18:43.640 --> 18:47.120
What about what's inside of this non-linearity g?

18:47.120 --> 18:51.040
Well, this is just a 2D line.

18:51.040 --> 18:55.560
In fact, since it's just a two dimensional line, we can even plot it in two dimensional

18:55.560 --> 18:56.560
space.

18:56.560 --> 18:58.400
This is called the feature space, the input space.

18:58.400 --> 19:04.200
In this case, the feature space and the input space are equal because we only have one neuron.

19:04.200 --> 19:07.160
So in this plot, let me describe what you're seeing.

19:07.160 --> 19:10.440
So on the two axes, you're seeing our two inputs.

19:10.440 --> 19:15.640
So on one axis is x1, one of the inputs, on the other axis is x2, our other input.

19:15.640 --> 19:20.120
And we can plot the line here, our decision boundary of this trained neural network that

19:20.120 --> 19:23.920
I gave you, as a line in this space.

19:23.920 --> 19:28.080
Now this line corresponds to actually all of the decisions that this neural network

19:28.080 --> 19:29.960
can make.

19:29.960 --> 19:35.600
Because if I give you a new data point, for example here I'm giving you negative 1, 2,

19:35.600 --> 19:41.080
this point lies somewhere in the space, specifically at x1 equal to negative 1 and x2 equal to

19:41.080 --> 19:42.080
2.

19:42.080 --> 19:44.280
That's just a point in the space.

19:44.280 --> 19:51.600
I want you to compute its weighted combination and I can actually follow the perceptron equation

19:51.600 --> 19:53.560
to get the answer.

19:53.560 --> 19:59.560
So here we can see that if we plug it into the perceptron equation, we get 1 plus minus

19:59.560 --> 20:02.240
3 minus 4.

20:02.240 --> 20:04.560
And the result would be minus 6.

20:04.560 --> 20:13.400
We plug that into our nonlinear activation function g and we get a final output of 0.002.

20:13.400 --> 20:19.520
Now in fact, remember that the sigmoid function actually divides this space into two parts

20:19.520 --> 20:23.520
of either because it outputs everything between 0 and 1.

20:23.520 --> 20:31.760
It's dividing it between 0.5 and greater than 0.5 and less than 0.5.

20:31.760 --> 20:39.720
When the input is less than 0 and greater than 0.5, that's when the input is positive.

20:39.720 --> 20:43.840
We can illustrate this space actually, but this feature space, when we're dealing with

20:43.840 --> 20:49.400
a small dimensional data, like in this case we only have two dimensions.

20:49.400 --> 20:53.880
But soon we'll start to talk about problems where we have thousands or millions or in

20:53.880 --> 20:58.840
some cases even billions of weights in our neural network.

20:58.840 --> 21:04.880
And then drawing these types of plots becomes extremely challenging and not really possible

21:04.880 --> 21:05.880
anymore.

21:05.880 --> 21:09.920
But at least when we're in this regime of small number of inputs and small number of

21:09.920 --> 21:13.960
weights, we can make these plots to really understand the entire space.

21:13.960 --> 21:19.520
And for any new input that we obtain, for example an input right here, we can see exactly

21:19.520 --> 21:27.040
that this point is going to be having an activation function less than 0 and its output will be

21:27.040 --> 21:28.720
less than 0.5.

21:28.720 --> 21:33.320
The magnitude of that actually is computed by plugging it into the perceptron equation.

21:33.320 --> 21:37.800
So we can't avoid that, but we can immediately get an answer on the decision boundary, depending

21:37.800 --> 21:44.480
on which side of this hyperplane that we lie on when we plug it in.

21:44.480 --> 21:49.440
So now that we have an idea of how to build a perceptron, let's start by building neural

21:49.440 --> 21:53.960
networks and seeing how they all come together.

21:53.960 --> 21:58.180
So let's revisit that diagram of the perceptron that I showed you before.

21:58.180 --> 22:02.740
If there's only a few things that you get from this class, I really want everyone to

22:02.740 --> 22:05.540
take away how a perceptron works.

22:05.540 --> 22:07.900
And there's three steps, remember them always.

22:07.900 --> 22:12.980
The dot product, you take a dot product of your inputs and your weights.

22:12.980 --> 22:16.700
You add a bias and you apply your non-linearity.

22:16.700 --> 22:19.660
There's three steps.

22:19.660 --> 22:21.940
Let's simplify this diagram a little bit.

22:21.940 --> 22:25.060
Let's clean up some of the arrows and remove the bias.

22:25.060 --> 22:30.500
And we can actually see now that every line here has its own associated weight to it.

22:30.500 --> 22:34.100
And I'll remove the bias term, like I said, for simplicity.

22:34.100 --> 22:40.740
Note that z here is the result of that dot product plus bias, before we apply the activation

22:40.740 --> 22:43.140
function, though, g.

22:43.140 --> 22:48.660
The final output, though, is simply y, which is equal to the activation function of z, which

22:48.660 --> 22:51.580
is our activation value.

22:52.580 --> 22:58.180
Now, if we want to define a multi-output neural network, we can simply add another

22:58.180 --> 23:00.020
perceptron to this picture.

23:00.020 --> 23:04.540
So instead of having one perceptron, now we have two perceptrons and two outputs.

23:04.540 --> 23:09.860
Each one is a normal perceptron, exactly like we saw before, taking its inputs from each

23:09.860 --> 23:17.060
of the x1's through xm's, taking the dot product, adding a bias, and that's it.

23:17.060 --> 23:18.260
Now we have two outputs.

23:18.260 --> 23:21.940
Each of those perceptrons, though, will have a different set of weights.

23:21.940 --> 23:22.940
Remember that.

23:22.940 --> 23:25.660
We'll get back to that.

23:25.660 --> 23:30.980
If we want, so actually one thing to keep in mind here is because all the inputs are

23:30.980 --> 23:37.580
densely connected, every input has a connection to the weights of every perceptron.

23:37.580 --> 23:42.140
These are often called dense layers, or sometimes fully connected layers.

23:42.140 --> 23:47.540
Now, through this class, you're going to get a lot of experience actually coding up

23:47.580 --> 23:53.540
and practically creating some of these algorithms using a software toolbox called TensorFlow.

23:53.540 --> 23:59.020
So now that we have the understanding of how a single perceptron works and how a dense

23:59.020 --> 24:04.860
layer works, this is a stack of perceptrons, let's try and see how we can actually build

24:04.860 --> 24:09.340
up a dense layer like this all the way from scratch.

24:09.340 --> 24:14.580
To do that, we can actually start by initializing the two components of our dense layer, which

24:14.620 --> 24:17.620
are the weights and the biases.

24:17.620 --> 24:22.500
Now that we have these two parameters of our neural network, of our dense layer, we can

24:22.500 --> 24:26.940
actually define the forward propagation of information, just like we saw it and learned

24:26.940 --> 24:28.500
about already.

24:28.500 --> 24:33.540
That forward propagation of information is simply the dot product or the matrix multiplication

24:33.540 --> 24:40.540
of our inputs with our weights, add a bias, that gives us our activation function here,

24:41.380 --> 24:46.380
and then we apply this nonlinearity to compute the output.

24:46.380 --> 24:52.380
Now, TensorFlow has actually implemented this dense layer for us.

24:52.380 --> 24:57.140
So we don't need to do that from scratch, instead we can just call it like shown here.

24:57.140 --> 25:04.140
So to create a dense layer with two outputs, we can specify this units equal to two.

25:04.140 --> 25:08.380
Now let's take a look at what's called a single layered neural network.

25:08.380 --> 25:13.860
This is one we have a single hidden layer between our inputs and our outputs.

25:13.860 --> 25:19.140
This layer is called the hidden layer, because unlike an input layer and an output layer,

25:19.140 --> 25:24.260
the states of this hidden layer are typically unobserved, they're hidden to some extent,

25:24.260 --> 25:26.820
they're not strictly enforced either.

25:26.820 --> 25:31.060
And since we have this transformation now from the input layer to the hidden layer and

25:31.060 --> 25:36.660
from the hidden layer to the output layer, each of these layers are going to have their

25:36.740 --> 25:39.700
own specified weight matrices.

25:39.700 --> 25:45.660
We'll call w1 the weight matrices for the first layer and w2 the weight matrix for the

25:45.660 --> 25:48.580
second layer.

25:48.580 --> 25:55.580
If we take a zoomed in look at one of the neurons in this hidden layer, let's take

25:55.580 --> 26:01.540
for example z2 for example, this is the exact same perceptron that we saw before.

26:01.540 --> 26:07.700
We can compute its output, again using the exact same story, taking all of its inputs

26:07.700 --> 26:12.820
x1 through xm, applying a dot product with the weights, adding a bias and that gives

26:12.820 --> 26:14.860
us z2.

26:14.860 --> 26:20.340
If we look at a different neuron, let's suppose z3, we'll get a different value here because

26:20.340 --> 26:26.100
the weights leading to z3 are probably different than those leading to z2.

26:26.100 --> 26:32.860
Now this picture looks a bit messy, so let's try and clean things up a bit more.

26:32.860 --> 26:38.020
From now on, I'll just use this symbol here to denote what we call this dense layer or

26:38.020 --> 26:40.340
fully connected layers.

26:40.340 --> 26:46.820
And here you can actually see an example of how we can create this exact neural network

26:46.820 --> 26:51.220
again using TensorFlow with the predefined dense layer notation.

26:51.220 --> 26:55.980
Here we're creating a sequential model where we can stack layers on top of each other.

26:55.980 --> 27:04.300
This layer with n neurons and the second layer with 2 neurons, the output layer.

27:04.300 --> 27:09.020
And if we want to create a deep neural network, all we have to do is keep stacking these layers

27:09.020 --> 27:14.940
to create more and more hierarchical models, ones where the final output is computed by

27:14.940 --> 27:18.620
going deeper and deeper into the network.

27:18.620 --> 27:24.060
And to implement this in TensorFlow again, it's very similar as we saw before, again

27:24.060 --> 27:30.540
using the TFKARIS sequential call, we can stack each of these dense layers on top of

27:30.540 --> 27:37.260
each other, each one specified by the number of neurons in that dense layer, n1 and 2,

27:37.260 --> 27:42.940
but with the last output layer fixed to 2 outputs, if that's how many outputs we have.

27:42.940 --> 27:45.620
Okay, so that's awesome.

27:45.620 --> 27:52.100
Now we have an idea of not only how to build up a neural network directly from a perceptron,

27:52.100 --> 27:56.660
but how to compose them together to form complex deep neural networks.

27:56.660 --> 28:02.180
Let's take a look at how we can actually apply them to a very real problem that I believe

28:02.180 --> 28:07.140
all of you should care very deeply about.

28:07.140 --> 28:12.060
Here's a problem that we want to build an AI system to learn to answer.

28:12.060 --> 28:14.300
Will I pass this class?

28:14.300 --> 28:17.860
And we can start with a simple two feature model.

28:17.860 --> 28:21.780
One feature, let's say, is the number of lectures that you attend as part of this class.

28:21.780 --> 28:27.060
And the second feature is the number of hours that you spend working on your final project.

28:27.060 --> 28:32.860
You do have some training data from all of the past participants of Success191.

28:32.860 --> 28:36.460
And we can plot this data on this feature space like this.

28:36.460 --> 28:41.980
The green points here actually indicate students, so each point is one student that has passed

28:41.980 --> 28:47.060
the class, and the red points are students that have failed the class.

28:47.060 --> 28:52.260
You can see where they are in this feature space depends on the actual number of hours

28:52.260 --> 28:55.700
that they attended the lecture, the number of lectures they attended, and the number

28:55.700 --> 28:59.260
of hours they spent on the final project.

28:59.260 --> 29:00.340
And then there's you.

29:00.340 --> 29:06.340
You have attended four lectures, and you have spent five hours on your final project.

29:06.340 --> 29:14.460
And you want to understand how can you build a neural network given everyone else in this

29:14.460 --> 29:15.740
class?

29:15.740 --> 29:22.140
Will you pass or fail this class based on the training data that you see?

29:22.140 --> 29:23.140
So let's do it.

29:23.140 --> 29:27.300
We have now all of the requirements to do this now.

29:27.300 --> 29:33.060
So let's build a neural network with two inputs, x1 and x2, with x1 being the number

29:33.060 --> 29:38.340
of lectures that we attend, x2 is the number of hours you spend on your final project.

29:38.340 --> 29:44.500
We'll have one hidden layer with three units, and we'll feed those into a final probability

29:44.500 --> 29:51.980
output by passing this class, and we can see that the probability that we pass is 0.1,

29:51.980 --> 29:53.740
or 10%.

29:53.740 --> 30:00.940
That's not great, but the reason is because that this model was never actually trained.

30:00.940 --> 30:04.420
It's basically just a baby.

30:04.420 --> 30:06.220
It's never seen any data.

30:06.220 --> 30:08.900
Even though you have seen the data, it hasn't seen any data.

30:08.900 --> 30:14.260
And more importantly, you haven't told the model how to interpret this data.

30:14.260 --> 30:16.420
It needs to learn about this problem first.

30:16.420 --> 30:22.060
It knows nothing about this class or final projects or any of that.

30:22.060 --> 30:26.140
So one of the most important things to do this is actually you have to tell the model

30:26.140 --> 30:32.700
when it is making bad predictions in order for it to be able to correct itself.

30:32.700 --> 30:35.820
Now the loss of a neural network actually defines exactly this.

30:35.820 --> 30:41.140
It defines how wrong a prediction was.

30:41.140 --> 30:45.580
So it takes as input the predicted outputs and the ground truth outputs.

30:45.580 --> 30:50.140
Now if those two things are very far apart from each other, then the loss will be very

30:50.140 --> 30:51.540
large.

30:51.540 --> 30:56.300
On the other hand, the closer these two things are from each other, the smaller the loss,

30:56.300 --> 30:59.500
and the more accurate the loss the model will be.

30:59.500 --> 31:01.580
So we always want to minimize the loss.

31:01.580 --> 31:06.940
We want to incur, we want to predict something that's as close as possible to the ground

31:06.940 --> 31:09.740
truth.

31:09.740 --> 31:14.900
Now let's assume we have not just the data from one student, but as we have in this

31:14.900 --> 31:17.260
case the data from many students.

31:17.260 --> 31:23.100
We now care about not just how the model did on predicting just one prediction, but how

31:23.100 --> 31:26.580
it did on average across all of these students.

31:26.580 --> 31:29.260
This is what we call the empirical loss.

31:29.260 --> 31:34.820
And it's simply just the mean or the average of every loss from each individual example

31:34.820 --> 31:38.460
or each individual student.

31:38.460 --> 31:43.340
When training a neural network, we want to find a network that minimizes the empirical

31:43.340 --> 31:50.580
loss between our predictions and the true outputs.

31:50.580 --> 31:56.660
Now if we look at the problem of binary classification, where the neural network like we want to do

31:56.660 --> 32:02.180
in this case, is supposed to answer either yes or no, one or zero.

32:02.180 --> 32:06.740
We can use what is called a softmax cross entropy loss.

32:06.740 --> 32:15.500
Now the softmax cross entropy loss is actually written out here and it's defined by what's

32:15.500 --> 32:18.980
called the cross entropy between two probability distributions.

32:18.980 --> 32:25.060
It measures how far apart the ground truth probability distribution is from the predicted

32:25.060 --> 32:28.980
probability distribution.

32:28.980 --> 32:33.900
Let's suppose instead of predicting binary outputs, will I pass this class or will I

32:33.900 --> 32:35.980
not pass this class?

32:35.980 --> 32:42.380
Instead you want to predict the final grade as a real number, not a probability or as

32:42.380 --> 32:49.700
a percentage, we want the grade that you will get in this class.

32:49.700 --> 32:54.540
Now in this case, because the type of the output is different, we also need to use a

32:54.540 --> 32:59.220
different loss here, because our outputs are no longer 0, 1, but they can be any real

32:59.220 --> 33:00.220
number.

33:00.220 --> 33:05.300
They're just the grade that you're going to get on the final class.

33:05.300 --> 33:11.100
So for example, here since this is a continuous variable, the grade, we want to use what's

33:11.100 --> 33:12.700
called the mean squared error.

33:12.700 --> 33:18.660
This measures just the squared error, the squared difference between our ground truth

33:18.660 --> 33:25.180
and our predictions, again averaged over the entire data set.

33:25.180 --> 33:31.340
Okay great, so now we've seen two loss functions, one for classification, binary outputs, as

33:31.340 --> 33:38.100
well as regression, continuous outputs, and the problem now I think that we need to start

33:38.100 --> 33:40.820
asking ourselves is how can we take that loss function?

33:40.820 --> 33:44.340
We've seen our loss function, we've seen our network, now we have to actually understand

33:44.340 --> 33:46.420
how can we put those two things together?

33:46.420 --> 33:51.100
How can we use our loss function to train the weights of our neural network such that

33:51.100 --> 33:53.700
it can actually learn that problem?

33:53.700 --> 33:58.980
Well, what we want to do is actually find the weights of the neural network that will

33:58.980 --> 34:01.580
minimize the loss of our data set.

34:01.580 --> 34:08.380
That essentially means that we want to find the W's in our neural network that minimize

34:08.380 --> 34:09.380
J of W.

34:09.380 --> 34:14.900
J of W's are empirical cost function that we saw in the previous slides that average loss

34:14.900 --> 34:18.660
over each data point in the data set.

34:18.660 --> 34:25.500
Now, remember that W, capital W, is simply a collection of all of the weights in our

34:25.500 --> 34:28.940
neural network, not just from one layer, but from every single layer.

34:28.940 --> 34:33.540
So that's W0 from the zeroth layer to the first layer to the second layer, all concatenate

34:33.540 --> 34:34.540
into one.

34:34.540 --> 34:41.220
In this optimization problem we want to optimize all of the W's to minimize this empirical loss.

34:42.220 --> 34:47.260
Now, remember our loss function is just a simple function of our weights.

34:47.260 --> 34:53.220
If we have only two weights, we can actually plot this entire loss landscape over this

34:53.220 --> 34:54.220
grid of weights.

34:54.220 --> 34:58.580
So on the one axis on the bottom you can see weight number one and the other one you can

34:58.580 --> 35:01.420
see weight zero.

35:01.420 --> 35:04.300
There's only two weights in this neural network, very simple neural network.

35:04.300 --> 35:10.180
So we can actually plot for every W0 and W1, what is the loss?

35:10.220 --> 35:16.940
What is the error that we'd expect to see and obtain from this neural network?

35:16.940 --> 35:22.620
Now the whole process of training a neural network, optimizing it, is to find the lowest

35:22.620 --> 35:29.580
point in this loss landscape that will tell us our optimal W0 and W1.

35:29.580 --> 35:30.740
Now how can we do that?

35:30.740 --> 35:34.540
The first thing we have to do is pick a point.

35:34.540 --> 35:37.900
So let's pick any W0, W1.

35:37.940 --> 35:44.340
Starting from this point we can compute the gradient of the landscape at that point.

35:44.340 --> 35:51.540
Now the gradient tells us the direction of highest or steepest ascent.

35:51.540 --> 35:53.780
So that tells us which way is up.

35:53.780 --> 35:58.220
If we compute the gradient of our loss with respect to our weights, that's the derivative

35:58.220 --> 36:02.140
of our gradient for loss with respect to the weights, that tells us the direction of which

36:02.140 --> 36:07.580
way is up on that loss landscape from where we stand right now.

36:07.580 --> 36:10.740
Instead of going up though, we want to find the lowest loss.

36:10.740 --> 36:18.220
So let's take the negative of our gradient and take a small step in that direction.

36:18.220 --> 36:21.660
This will move us a little bit closer to the lowest point.

36:21.660 --> 36:22.900
And we just keep repeating this.

36:22.900 --> 36:28.420
Now we compute the gradient at this point and repeat the process until we converge.

36:28.420 --> 36:31.460
And we will converge to a local minimum.

36:31.460 --> 36:35.700
We don't know if it will converge to a global minimum, but at least we know that it should

36:35.700 --> 36:38.500
in theory converge to a local minimum.

36:38.500 --> 36:42.460
Now we can summarize this algorithm as follows.

36:42.460 --> 36:45.820
This algorithm is also known as gradient descent.

36:45.820 --> 36:52.140
So we start by initializing all of our weights randomly and we loop until convergence.

36:52.140 --> 36:55.980
We start from one of those weights, our initial point.

36:55.980 --> 36:57.420
We compute the gradient.

36:57.420 --> 36:59.180
That tells us which way is up.

36:59.180 --> 37:01.300
So we take a step in the opposite direction.

37:01.300 --> 37:03.740
We take a small step here.

37:03.740 --> 37:10.020
All is computed by multiplying our gradient by this factor eta.

37:10.020 --> 37:12.100
And we'll learn more about this factor later.

37:12.100 --> 37:14.060
This factor is called the learning rate.

37:14.060 --> 37:15.620
We'll learn more about that later.

37:15.620 --> 37:21.580
Now again, in TensorFlow, we can actually see this pseudocode of gradient descent algorithm

37:21.580 --> 37:23.580
written out in code.

37:23.580 --> 37:26.620
We can randomize all of our weights.

37:26.620 --> 37:31.060
That basically initializes our search, our optimization process at some point in space.

37:31.460 --> 37:33.620
Then we keep looping over and over and over again.

37:33.620 --> 37:38.100
We compute the loss, we compute the gradient, and we take a small step of our weights in

37:38.100 --> 37:41.500
the direction of that gradient.

37:41.500 --> 37:44.260
But now let's take a look at this term here.

37:44.260 --> 37:48.260
This is how we actually compute the gradient.

37:48.260 --> 37:53.220
This explains how the loss is changing with respect to the weight.

37:53.220 --> 37:56.740
But I never actually told you how we compute this.

37:56.740 --> 38:02.100
So let's talk about this process, which is actually extremely important in training neural

38:02.100 --> 38:03.100
networks.

38:03.100 --> 38:06.900
It's known as back propagation.

38:06.900 --> 38:08.340
So how does back propagation work?

38:08.340 --> 38:10.420
How do we compute this gradient?

38:10.420 --> 38:12.380
Let's start with a very simple neural network.

38:12.380 --> 38:15.700
This is probably the simplest neural network in existence.

38:15.700 --> 38:21.380
It only has one input, one hidden neuron, and one output.

38:21.380 --> 38:26.900
Taking the gradient of our loss, j of w, with respect to one of the weights, in this

38:26.900 --> 38:36.660
case just w2, for example, tells us how much a small change in w2 is going to affect our

38:36.660 --> 38:38.180
loss, j.

38:38.180 --> 38:44.660
So if we move around j, infinitesimally small, how will that affect our loss?

38:44.660 --> 38:50.100
That's what the gradient is going to tell us, derivative of j of w2.

38:50.100 --> 38:55.860
So if we write out this derivative, we can actually apply the chain rule to actually

38:55.860 --> 38:57.020
compute it.

38:57.020 --> 38:58.700
So what does that look like?

38:58.700 --> 39:11.980
Specifically, we can decompose that derivative into the derivative of j dw over dy multiplied

39:11.980 --> 39:17.500
by derivative of our output with respect to w2.

39:17.500 --> 39:20.820
Now the question here is with the second part.

39:20.820 --> 39:26.380
If we want to compute now not the derivative of our loss with respect to w2, but now the

39:26.380 --> 39:30.580
loss with respect to w1, we can do the same story as before.

39:30.580 --> 39:35.260
We can apply the chain rule now recursively.

39:35.260 --> 39:39.540
So now we have to apply the chain rule again to the second part.

39:39.540 --> 39:42.780
Now the second part is expanded even further.

39:42.780 --> 39:48.620
So the derivative of our output with respect to z1, which is the activation function of

39:48.620 --> 39:50.660
this first hidden unit.

39:50.660 --> 39:52.700
And we can back propagate this information now.

39:52.700 --> 39:59.140
You can see starting from our loss all the way through w2 and then recursively applying

39:59.140 --> 40:02.580
this chain rule again to get to w1.

40:02.580 --> 40:08.340
And this allows us to see both the gradient at both w2 and w1.

40:08.340 --> 40:15.860
So in this case, just to reiterate once again, this is telling us this dj dw1 is telling

40:15.860 --> 40:21.120
us how a small change in our weight is going to affect our loss.

40:21.120 --> 40:25.420
So we can see if we increase our weight a small amount, it will increase our loss.

40:25.420 --> 40:29.380
That means we will want to decrease the weight to decrease our loss.

40:29.380 --> 40:30.820
That's what the gradient tells us.

40:30.820 --> 40:38.180
Which direction we need to step in order to decrease or increase our loss function.

40:38.180 --> 40:41.660
Now we showed this here for just two weights in our neural network because we only have

40:41.660 --> 40:42.660
two weights.

40:42.660 --> 40:45.580
But imagine we have a very deep neural network.

40:45.580 --> 40:51.380
One with more than just two layers of or one layer rather of hidden units.

40:51.380 --> 40:56.980
We can just repeat this process of applying, recursively applying the chain rule to determine

40:56.980 --> 41:02.540
how every single way in the model needs to change to impact that loss.

41:02.540 --> 41:07.420
But really all this boils down to just recursively applying this chain rule formulation that

41:07.420 --> 41:10.620
you can see here.

41:10.620 --> 41:12.860
And that's the back propagation algorithm.

41:12.860 --> 41:15.220
In theory it sounds very simple.

41:15.220 --> 41:22.500
It's just a very basic extension on derivatives and the chain rule.

41:22.500 --> 41:27.900
But now let's actually touch on some insights from training these networks in practice that

41:27.900 --> 41:32.340
make this process much more complicated in practice.

41:32.340 --> 41:37.540
And why using back propagation as we saw there is not always so easy.

41:37.540 --> 41:42.860
Now in practice training neural networks and optimization of networks can be extremely

41:42.860 --> 41:47.420
difficult and it's actually extremely computationally intensive.

41:47.420 --> 41:54.420
Here's the visualization of what a loss landscape of a real neural network can look like visualized

41:54.420 --> 41:56.900
on just two dimensions.

41:56.900 --> 42:03.380
Now you can see here that the loss is extremely non-convex meaning that it has many, many

42:03.380 --> 42:05.500
local minimum.

42:05.500 --> 42:11.020
That can make using an algorithm like gradient descent very, very challenging because gradient

42:11.020 --> 42:15.620
descent is always going to step closest to the first local minimum but it can always

42:15.620 --> 42:16.780
get stuck there.

42:16.780 --> 42:22.140
So finding how to get to the global minima or a really good solution for your neural

42:22.140 --> 42:28.860
network can often be very sensitive to your hyper parameter such as where the optimizer

42:28.860 --> 42:31.140
starts in this loss landscape.

42:31.140 --> 42:36.060
If it starts in a potentially bad part of the landscape it can very easily get stuck

42:36.060 --> 42:39.820
in one of these local minimum.

42:39.820 --> 42:44.580
Now recall the equation that we talked about for gradient descent.

42:44.580 --> 42:45.940
This was the equation I showed you.

42:45.940 --> 42:52.220
Our next weight update is going to be your current weights minus a small amount called

42:52.220 --> 42:54.500
the learning rate multiplied by the gradient.

42:54.500 --> 43:00.220
So we have this minus sign because we want to step in the opposite direction and we multiply

43:00.220 --> 43:04.860
it by the gradient or we multiply it by the small number called here called eta which

43:04.860 --> 43:08.380
is what we call the learning rate.

43:08.380 --> 43:11.180
How fast do we want to do the learning?

43:11.180 --> 43:15.140
Now it determines actually not just how fast to do the learning that's maybe not the best

43:15.180 --> 43:22.300
way to say it but it tells us how large should each step we take in practice be with regards

43:22.300 --> 43:23.620
to that gradient.

43:23.620 --> 43:27.940
So the gradient tells us the direction but it doesn't necessarily tell us the magnitude

43:27.940 --> 43:29.300
of the direction.

43:29.300 --> 43:36.180
So eta can tell us actually a scale of how much we want to trust that gradient and step

43:36.180 --> 43:38.300
in the direction of that gradient.

43:38.300 --> 43:43.540
In practice setting even eta, this one parameter, this one number can be extremely difficult

43:43.540 --> 43:47.180
and I want to give you a quick example of why.

43:47.180 --> 43:54.700
So if you have a very non-convex or loss landscape where you have local minima, if you set the

43:54.700 --> 43:59.580
learning rate too low then the model can get stuck in these local minima.

43:59.580 --> 44:04.620
It can never escape them because it actually does optimize itself but it optimizes it to

44:04.620 --> 44:12.300
a very non-optimal minima and it can converge very slowly as well.

44:12.300 --> 44:17.660
On the other hand if we increase our learning rate too much then we can actually overshoot

44:17.660 --> 44:25.580
our minima and actually diverge and lose control and basically explode the training process

44:25.580 --> 44:26.780
completely.

44:26.780 --> 44:33.580
One of the challenges is actually how to use stable learning rates that are large enough

44:33.580 --> 44:41.780
to avoid the local minima but small enough so that they don't diverge completely.

44:42.420 --> 44:49.940
So they're small enough to actually converge to that global spot once they reach it.

44:49.940 --> 44:51.940
So how can we actually set this learning rate?

44:51.940 --> 44:57.700
Well one option which is actually somewhat popular in practice is to actually just try

44:57.700 --> 45:01.620
a lot of different learning rates and that actually works.

45:01.620 --> 45:06.540
It is a feasible approach but let's see if we can do something a little bit smarter than

45:06.540 --> 45:08.300
that, more intelligent.

45:08.340 --> 45:14.020
What if we could say instead how can we build an adaptive learning rate that actually looks

45:14.020 --> 45:20.060
at its lost landscape and adapts itself to account for what it sees in the landscape.

45:20.060 --> 45:24.020
There are actually many types of optimizers that do exactly this.

45:24.020 --> 45:26.980
This means that the learning rates are no longer fixed.

45:26.980 --> 45:32.140
They can increase or decrease depending on how large the gradient is in that location

45:32.260 --> 45:39.260
and how fast we want and how fast we're actually learning and many other options.

45:39.260 --> 45:44.260
They could be also with regards to the size of the weights at that point, the magnitudes, etc.

45:46.260 --> 45:52.580
In fact these have been widely explored and published as part of TensorFlow as well and

45:52.580 --> 45:56.420
during your labs we encourage each of you to really try out each of these different types

45:56.420 --> 46:01.940
of optimizers and experiment with their performance in different types of problems so that you

46:01.940 --> 46:07.900
can gain very important intuition about when to use different types of optimizers or what

46:07.900 --> 46:12.900
their advantages are and disadvantages in certain applications as well.

46:14.900 --> 46:18.100
Let's try and put all of this together.

46:18.100 --> 46:25.500
Here we can see a full loop of using TensorFlow to define your model on the first line, define

46:25.500 --> 46:26.740
your optimizer.

46:26.740 --> 46:30.060
Here you can replace this with any optimizer that you want.

46:30.180 --> 46:34.940
Here I'm just using stochastic gradient descent like we saw before.

46:34.940 --> 46:37.260
Feeding it through the model we loop forever.

46:37.260 --> 46:39.340
We're doing this forward prediction.

46:39.340 --> 46:40.780
We predict using our model.

46:40.780 --> 46:43.460
We compute the loss with our prediction.

46:43.460 --> 46:49.100
This is exactly the loss is telling us again how incorrect our prediction is with respect

46:49.100 --> 46:51.700
to the ground truth why.

46:51.700 --> 46:58.500
We compute the gradient of our loss with respect to each of the weights in our neural network.

46:58.540 --> 47:06.020
Then finally we apply those gradients using our optimizer to step and update our weights.

47:06.020 --> 47:10.660
This is really taking everything that we've learned in the class and lecture so far and

47:10.660 --> 47:18.820
applying it into one whole piece of code written in TensorFlow.

47:18.820 --> 47:24.460
So I want to continue this talk and really talk about tips for training these networks

47:24.500 --> 47:31.380
in practice now that we can focus on this very powerful idea of batching your data into

47:31.380 --> 47:33.580
mini batches.

47:33.580 --> 47:39.340
So before we saw it with gradient descent that we have the following algorithm.

47:39.340 --> 47:45.100
This gradient that we saw to compute using back propagation can be actually very intensive

47:45.100 --> 47:50.540
to compute especially if it's computed over your entire training set.

47:50.620 --> 47:54.980
This is a summation over every single data point in the entire data set and most real

47:54.980 --> 47:56.820
life applications.

47:56.820 --> 48:01.660
It is simply not feasible to compute this on every single iteration in your optimization

48:01.660 --> 48:03.780
loop.

48:03.780 --> 48:08.780
Alternatively let's consider a different variant of this algorithm called stochastic gradient

48:08.780 --> 48:09.780
descent.

48:09.780 --> 48:14.380
So instead of computing the gradient over our entire data set let's just pick a single

48:14.380 --> 48:19.980
point compute the gradient of that single point with respect to the weights and then

48:19.980 --> 48:23.820
update all of our weights based on that gradient.

48:23.820 --> 48:28.660
So this has some advantages this is very easy to compute because it's only using one data

48:28.660 --> 48:34.940
point now it's very fast but it's also very noisy because it's only from one data point.

48:34.940 --> 48:40.820
Instead there's a middle ground instead of computing this noisy gradient of a single

48:40.820 --> 48:48.740
point let's get a better estimate of our gradient by using a batch of b data points.

48:48.740 --> 48:54.740
So now let's pick a batch of b data points and we'll compute the gradient estimate simply

48:54.740 --> 48:57.700
as the average over this batch.

48:57.700 --> 49:02.780
So since b here is usually not that large on the order of tens or hundreds of samples

49:02.780 --> 49:08.820
this is much much faster to compute than regular gradient descent and it's also much much more

49:08.820 --> 49:16.940
accurate than purely stochastic gradient descent that only uses a single example.

49:16.940 --> 49:22.380
Now this increases the gradient accuracy estimation which also allows us to converge much more

49:22.380 --> 49:27.660
smoothly it also means that we can trust our gradient more than in stochastic gradient

49:27.660 --> 49:34.940
descent so that we can actually increase our learning rate a bit more as well.

49:34.940 --> 49:40.940
Mini batching also leads to massively parallelizable computation we can split up the batches on

49:40.940 --> 49:46.380
separate workers and separate machines and thus achieve even more parallelization and

49:46.380 --> 49:50.140
speed increases on our GPUs.

49:50.140 --> 49:54.420
Now the last topic I want to talk about is that of overfitting this is also known as

49:54.420 --> 50:01.020
the problem of generalization and is one of the most fundamental problems in all of machine

50:01.020 --> 50:05.660
learning and not just deep learning.

50:05.660 --> 50:11.940
Now overfitting like I said is critical to understand so I really want to make sure that

50:11.940 --> 50:17.780
this is a clear concept in everyone's mind ideally in machine learning we want to learn

50:17.780 --> 50:23.420
a model that accurately describes our test data not the training data even though we're

50:23.420 --> 50:28.140
optimizing this model based on the training data what we really want is for it to perform

50:28.140 --> 50:31.460
well on the test data.

50:31.460 --> 50:38.140
So said differently we want to build representations that can learn from our training data but

50:38.140 --> 50:42.300
still generalize well to unseen test data.

50:42.300 --> 50:49.380
Now assume you want to build a line to describe these points underfitting means that the model

50:49.380 --> 50:56.140
does simply not have enough capacity to represent these points so no matter how good we try

50:56.140 --> 51:01.340
to fit this model it simply does not have the capacity to represent this type of data.

51:01.340 --> 51:05.420
On the far right hand side we can see the extreme other extreme where here the model

51:05.420 --> 51:12.620
is too complex it has too many parameters and it does not generalize well to new data.

51:12.620 --> 51:16.740
In the middle though we can see what's called the ideal fit it's not overfitting it's not

51:16.740 --> 51:23.100
underfitting but it has a medium number of parameters and it's able to fit in a generalizable

51:23.100 --> 51:29.300
way to the output and is able to generalize well to brand new data when it sees it at

51:29.300 --> 51:32.020
test time.

51:32.020 --> 51:37.780
Now to address this problem let's talk about regularization how can we make sure that our

51:37.780 --> 51:43.020
models do not end up overfit because neural networks do have a ton of parameters how can

51:43.020 --> 51:47.540
we enforce some form of regularization to them.

51:47.540 --> 51:49.540
Now what is regularization?

51:49.540 --> 51:53.580
Regularization is a technique that constrains our optimization problem such that we can

51:53.580 --> 51:59.820
discourage these complex models from actually being learned and overfit.

51:59.820 --> 52:01.420
So again why do we need it?

52:01.420 --> 52:06.620
We need it so that our model can generalize to this unseen data set and in neural networks

52:06.620 --> 52:13.020
we have many techniques for actually imposing regularization onto the model.

52:13.020 --> 52:17.100
One very common technique and very simple to understand is called dropout.

52:17.100 --> 52:22.620
This is one of the most popular forms of regularization in deep learning and it's very simple.

52:22.620 --> 52:25.660
Let's revisit this picture of a neural network.

52:25.660 --> 52:32.300
This is a two-layered neural network, two hidden layers and in dropout during training

52:32.300 --> 52:39.220
all we simply do is randomly set some of the activations here to zero with some probability.

52:39.220 --> 52:47.060
So what we can do is let's say we pick our probability to be 50% or 0.5 we can drop randomly

52:47.060 --> 52:51.900
for each of the activations 50% of those neurons.

52:51.900 --> 52:56.340
This is extremely powerful as it lowers the capacity of our neural network so that they

52:56.340 --> 53:02.580
have to learn to perform better on test sets because sometimes on training sets it just

53:02.580 --> 53:05.220
simply cannot rely on some of those parameters.

53:05.220 --> 53:09.900
So it has to be able to be resilient to that kind of dropout.

53:09.900 --> 53:17.100
It also means that they're easier to train because at least on every forward pass of

53:17.100 --> 53:22.100
iterations we're training only 50% of the weights and only 50% of the gradients.

53:22.100 --> 53:27.900
So that also cuts our gradient computation time down by a factor of two.

53:27.900 --> 53:33.740
So because now we only have to compute half the number of neuron gradients.

53:33.740 --> 53:38.220
Now on every iteration we dropped out on the previous iteration 50% of neurons but on the

53:38.220 --> 53:45.700
next iteration we're going to drop out a different set of neurons.

53:45.700 --> 53:50.500
And this gives the network, it basically forces the network to learn how to take different

53:50.500 --> 53:57.180
pathways to get to its answer and it can't rely on any one pathway too strongly and overfit

53:57.180 --> 53:58.180
to that pathway.

53:58.180 --> 54:02.760
This is a way to really force it to generalize to this new data.

54:02.760 --> 54:08.900
The second regularization technique that we'll talk about is this notion of early stopping.

54:08.900 --> 54:12.180
And again here the idea is very basic.

54:12.180 --> 54:18.100
It's basically let's stop training once we realize that our loss is increasing on a

54:18.100 --> 54:22.820
held out validation or let's call it a test set.

54:22.820 --> 54:27.980
So when we start training we all know the definition of overfitting is when our model

54:27.980 --> 54:30.340
starts to perform worse on the test set.

54:30.340 --> 54:36.380
So if we set aside some of this training data to be quote unquote test data we can monitor

54:36.380 --> 54:41.380
how our network is learning on this data and simply just stop before it has a chance

54:41.380 --> 54:42.900
to overfit.

54:42.900 --> 54:46.420
So on the x-axis you can see the number of training iterations and on the y-axis you

54:46.420 --> 54:51.740
can see the loss that we get after training that number of iterations.

54:51.740 --> 54:55.500
So as we continue to train in the beginning both lines continue to decrease.

54:55.500 --> 55:01.220
This is as we'd expect and this is excellent since it means our model is getting stronger.

55:01.220 --> 55:06.540
Eventually though the network's testing loss plateaus and starts to increase.

55:06.540 --> 55:11.260
Note that the training accuracy will always continue to go down as long as the network

55:11.260 --> 55:17.420
has the capacity to memorize the data and this pattern continues for the rest of training.

55:17.420 --> 55:21.020
So it's important here to actually focus on this point here.

55:21.020 --> 55:25.220
This is the point where we need to stop training and after this point assuming that our test

55:25.220 --> 55:30.940
set is a valid representation of the true test set the accuracy of the model will only

55:30.940 --> 55:31.940
get worse.

55:31.940 --> 55:36.180
So we can stop training here take this model and this should be the model that we actually

55:36.180 --> 55:40.500
use when we deploy into the real world.

55:40.500 --> 55:44.220
Anything any model taken from the left hand side is going to be underfit is not going

55:44.220 --> 55:48.660
to be utilizing the full capacity of the network and anything taken from the right hand side

55:48.660 --> 55:56.300
is overfit and actually performing worse than it needs to on that held out test set.

55:56.300 --> 56:02.420
So I'll conclude this lecture by summarizing three key points that we've covered so far.

56:02.420 --> 56:07.340
We started about the fundamental building blocks of neural networks the perceptron.

56:07.340 --> 56:12.780
We learned about stacking and composing these perceptrons together to form complex hierarchical

56:12.780 --> 56:19.100
neural networks and how to mathematically optimize these models with back propagation.

56:19.100 --> 56:23.940
And finally we address the practical side of these models that you'll find useful for

56:23.940 --> 56:29.660
the labs today including adaptive learning rates, batching and regularization.

56:29.660 --> 56:34.180
So thank you for attending the first lecture in 6S191.

56:34.180 --> 56:34.860
Thank you very much.

