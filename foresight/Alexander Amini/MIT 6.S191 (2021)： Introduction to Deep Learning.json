{"text": " Good afternoon, everyone, and welcome to MIT 6S191, Introduction to Deep Learning. My name is Alexander Amini and I'm so excited to be your instructor this year along with Aviso Imani in this new virtual format. 6S191 is a two-week boot camp on everything deep learning and will cover a ton of material in only two weeks. So I think it's really important for us to dive right in with these lectures. But before we do that, I do want to motivate exactly why I think this is such an awesome field to study. And when we taught this class last year, I decided to try introducing the class very differently. And instead of me telling the class how great 6S191 is, I wanted to let someone else do that instead. So actually, I want to start this year by showing you how we introduced 6S191 last year. Hi, everybody, and welcome to MIT 6S191, the official introductory course on deep learning taught here at MIT. Deep learning is revolutionizing so many fields from robotics, medicine, and everything in between. You'll learn from the medals of this field and how you can build some of these incredible algorithms. In fact, this entire speech and video are not real and were created using deep learning and artificial intelligence. And in this class, you'll learn how. It has been an honor to speak with you today, and I hope you enjoyed the course. So in case you couldn't tell, that was actually not a real video or audio. And the audio you actually heard was purposely degraded, a bit more to even make it more obvious that this was not real and avoid some potential misuse. Even with the purposely degraded audio, that intro went somewhat viral last year after the course and we got some really great and interesting feedback. And to be honest, after last year and when we did this, I thought it was going to be really hard for us to top it this year. But actually, I was wrong because the one thing I love about this field is that it's moving so incredibly fast that even within the past year, the state of the art has significantly advanced. And the video you saw that we used last year used deep learning, but it was not a particularly easy video to create. It required a full video of Obama speaking, and it used this to intelligently stitch together parts of the scene to make it look and appear like he was mouthing the words that I said. And to see the behind the scenes here, now you can see the same video with my voice. Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning taught here at MIT. Now it's actually possible to use just a single static image, not the full video to achieve the exact same thing. And now you can actually see eight more examples of Obama now just created using just a single static image, no more full dynamic videos, but we can achieve the same incredible realism and result using deep learning. Now of course, there's nothing restricting us to one person. This method generalizes to different faces, and there's nothing restricting us even to humans anymore or individuals that the algorithm has ever seen before. Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning taught here at MIT. The ability to generate these types of dynamic moving videos from only a single image is remarkable to me, and it's a testament to the true power of deep learning. In this class, you're going to actually not only learn about the technical basis of this technology, but also some of the very important and very important ethical and societal implications of this work as well. Now I hope this was a really great way to get you excited about this course and 6S191, and with that let's get started. We can actually start by taking a step back and asking ourselves what is deep learning, deep learning, and the context of intelligence. Intelligence is actually the ability to process information such that it can be used to inform a future decision. Now the field of artificial intelligence, or AI, is a science that actually focuses on building algorithms to do exactly this, to build algorithms to process information such that they can inform future predictions. Now machine learning, you can think of this as just a subset of AI that actually focuses on teaching an algorithm to learn from experiences without being explicitly programmed. Now deep learning takes this idea even further, and it's a subset of machine learning that focuses on using neural networks to automatically extract useful patterns in raw data, and then using these patterns or features to learn to perform that task. And that's exactly what this class is about. This class is about teaching algorithms how to learn a task directly from raw data. And we want to provide you with a solid foundation both technically and practically for you to understand under the hood how these algorithms are built and how they can learn. So this course is split between technical lectures as well as project software labs. We'll cover the foundation starting today with neural networks, which are really the building blocks of everything that we'll see in this course. And this year we also have two brand new really exciting hot topic lectures, focusing on uncertainty and probabilistic deep learning, as well as algorithmic bias and fairness. Finally we'll conclude with some really exciting guest lectures and student project presentations as part of a final project competition that all of you will be eligible to win some really exciting prizes. Now a bit of logistics before we dive into the technical side of the lecture. For those of you taking this course for credit, you will have two options to fulfill your credit requirement. The first option will be to actually work in teams of up to four or individually to develop a cool new deep learning idea. Now doing so will make you eligible to win some of the prizes that you can see on the right hand side. And we realize that in the context of this class, which is only two weeks, that's an extremely short amount of time to come up with an impressive project or research idea. So we're not going to be judging you on the novelty of that idea, but rather we're not going to be judging you on the results of that idea, but rather the novelty of the idea, your thinking process and how impactful this idea can be. But not on the results themselves. On the last day of class, you will actually give a three minute presentation to a group of judges who will then award the winners and the prizes. Now again, three minutes is extremely short to actually present your ideas and present your project, but I do believe that there's an art to presenting and conveying your ideas concisely and clearly in such a short amount of time. So we will be holding you strictly to that strict deadline. The second option to fulfill your grade requirement is to write a one page review on a deep learning paper. Here the grade is based more on the clarity of the writing and the technical communication of the main ideas. This will be due on Thursday, the last Thursday of the class, and you can pick whatever deep learning paper you would like. If you would like some pointers, we have provided some guide papers that can help you get started if you would just like to use one of those for your review. In addition to the final project prizes, we'll also be awarding this year three lab prizes, one associated to each of the software labs that students will complete. Again, completion of the software labs is not required for grade of this course, but it will make you eligible for some of these cool prizes. So please, we encourage everyone to compete for these prizes and get the opportunity to win them all. Please post to Piazza if you have any questions. Visit the course website for announcements and digital recordings of the lectures, etc. And please email us if you have any questions. Also there are software labs and office hours right after each of these technical lectures held in Gather Town. So please drop by in Gather Town to ask any questions about the software labs, specifically on those, or more generally about past software labs or about the lecture that occurred that day. Now this course has an incredible group of TAs and teaching assistants that you can reach out to at any time in case you have any issues or questions about the material that you're learning. And finally, we want to give a huge thanks to all of our sponsors who, without their help, this class would not be possible. This is the fourth year that we're teaching this class, and each year it just keeps getting bigger and bigger and bigger, and we really give a huge shout out to our sponsors for helping us make this happen each year. And especially this year in light of the virtual format. So now let's start with the fun stuff. Let's start by asking ourselves a question about why do we all care about deep learning? And specifically, why do we care right now? To understand that, it's important to actually understand first why is deep learning or how is deep learning different from traditional machine learning? Now traditionally, machine learning algorithms define a set of features in their data. Usually these are features that are handcrafted or hand engineered, and as a result they tend to be pretty brittle in practice when they're deployed. The key idea of deep learning is to learn these features directly from data in a hierarchical manner. That is, can we learn, if we want to learn how to detect a face, for example, can we learn to first start by detecting edges in the image, composing these edges together to detect mid-level features such as a eye or a nose or a mouth, and then going deeper and composing these features into structural, facial features, so that we can recognize this face. This hierarchical way of thinking is really core to deep learning as core to everything that we're going to learn in this class. Actually the fundamental building blocks though of deep learning and neural networks have actually existed for decades. So one interesting thing to consider is why are we studying this now? Now is an incredibly amazing time to study these algorithms, and for one reason is because data has become much more pervasive. These models are extremely hungry for data, and at the moment we're living in an era where we have more data than ever before. Secondly, these algorithms are massively parallelizable, so they can benefit tremendously from modern GPU hardware that simply did not exist when these algorithms were developed. And finally, due to open source toolboxes like TensorFlow, building and deploying these models has become extremely streamlined. So let's start actually with the fundamental building block of deep learning and of every neural network. That is just a single neuron, also known as a perceptron. So we're going to walk through exactly what is a perceptron, how it's defined, and we're going to build our way up to deeper neural networks all the way from there. So let's start really at the basic building block. The idea of a perceptron or a single neuron is actually very simple. So I think it's really important for all of you to understand this at its core. Let's start by actually talking about the forward propagation of information through this single neuron. We can define a set of inputs xi through xm, which you can see on the left-hand side, and each of these inputs or each of these numbers are multiplied by their corresponding weight and then added together. We take this single number, the result of that addition, and pass it through what's called a nonlinear activation function to produce our final output y. We can actually, actually this is not entirely correct because one thing I forgot to mention is that we also have what's called a bias term in here, which allows you to shift your activation function left or right. Now on the right-hand side of this diagram, you can actually see this concept illustrated or written out mathematically as a single equation. You can actually rewrite this in terms of linear algebra matrix multiplications and dot products to represent this a bit more concisely. So let's do that. Let's now do that with x, capital X, which is a vector of our inputs, x1 through xm, and capital W, which is a vector of our weights, w1 through wm. So each of these are vectors of length m, and the output is very simply obtained by taking their dot product, adding a bias, which in this case is w0, and then applying a nonlinearity, g. One thing is that I haven't, I've been mentioning it a couple of times, this nonlinearity, g. What exactly is it? Because I've mentioned it now a couple of times, well, it is a nonlinear function. One common example of this nonlinear activation function is what is known as the sigmoid function. Defined here on the right. In fact, there are many types of nonlinear functions. You can see three more examples here, including the sigmoid function. And throughout this presentation, you'll actually see these TensorFlow code blocks, which will actually illustrate how we can take some of the topics that we're learning in this class and actually practically use them using the TensorFlow software library. Now the sigmoid activation function, which I presented on the previous slide, is very popular since it's a function that gives outputs. It takes as input any real number, any activation value, and it outputs a number always between zero and one. So this makes it really, really suitable for problems and probability, because probabilities also have to be between zero and one. So this makes them very well suited for those types of problems. In modern deep neural networks, the ReLU activation function, which you can see on the right, is also extremely popular because of its simplicity. In this case, it's a piecewise linear function. It is zero before when it's in the negative regime, and it is strictly the identity function in the positive regime. But one really important question that I hope that you're asking yourselves right now is why do we even need activation functions? I think actually throughout this course, I do want to say that no matter what I say in the course, I hope that always you're questioning why this is a necessary step and why do we need each of these steps, because often these are the questions that can lead to really amazing research breakthroughs. So why do we need activation functions? Now the point of an activation function is to actually introduce non-linearities into our network, because these are non-linear functions. And it allows us to actually deal with non-linear data. This is extremely important in real life, especially because in the real world, data is almost always non-linear. Imagine I told you to separate here the green points from the red points, but all you could use is a single straight line. You might think this is easy with multiple lines or curved lines, but you can only use a single straight line. And that's what using a neural network with a linear activation function would be like. That makes the problem really hard, because no matter how deep the neural network is, you'll only be able to produce a single line decision boundary, and you're only able to separate your space with one line. Now using non-linear activation functions allows your neural network to approximate arbitrarily complex functions. And that's what makes neural networks extraordinarily powerful. Let's understand this with a simple example so that we can build up our intuition even further. Imagine I give you this trained network, now with weights on the left hand side, 3 and negative 2. This network only has two inputs, x1 and x2. If we want to get the output of it, we simply do the same story as I said before. First take a dot product of our inputs with our weights, add the bias, and apply a non-linearity. But let's take a look at what's inside of that non-linearity. It's simply a weighted combination of our inputs in the form of a two dimensional line, because in this case we only have two inputs. So if we want to compute this output, it's the same stories before, we take a dot product of x and w, we add our bias, and apply our non-linearity. What about what's inside of this non-linearity g? Well, this is just a 2D line. In fact, since it's just a two dimensional line, we can even plot it in two dimensional space. This is called the feature space, the input space. In this case, the feature space and the input space are equal because we only have one neuron. So in this plot, let me describe what you're seeing. So on the two axes, you're seeing our two inputs. So on one axis is x1, one of the inputs, on the other axis is x2, our other input. And we can plot the line here, our decision boundary of this trained neural network that I gave you, as a line in this space. Now this line corresponds to actually all of the decisions that this neural network can make. Because if I give you a new data point, for example here I'm giving you negative 1, 2, this point lies somewhere in the space, specifically at x1 equal to negative 1 and x2 equal to 2. That's just a point in the space. I want you to compute its weighted combination and I can actually follow the perceptron equation to get the answer. So here we can see that if we plug it into the perceptron equation, we get 1 plus minus 3 minus 4. And the result would be minus 6. We plug that into our nonlinear activation function g and we get a final output of 0.002. Now in fact, remember that the sigmoid function actually divides this space into two parts of either because it outputs everything between 0 and 1. It's dividing it between 0.5 and greater than 0.5 and less than 0.5. When the input is less than 0 and greater than 0.5, that's when the input is positive. We can illustrate this space actually, but this feature space, when we're dealing with a small dimensional data, like in this case we only have two dimensions. But soon we'll start to talk about problems where we have thousands or millions or in some cases even billions of weights in our neural network. And then drawing these types of plots becomes extremely challenging and not really possible anymore. But at least when we're in this regime of small number of inputs and small number of weights, we can make these plots to really understand the entire space. And for any new input that we obtain, for example an input right here, we can see exactly that this point is going to be having an activation function less than 0 and its output will be less than 0.5. The magnitude of that actually is computed by plugging it into the perceptron equation. So we can't avoid that, but we can immediately get an answer on the decision boundary, depending on which side of this hyperplane that we lie on when we plug it in. So now that we have an idea of how to build a perceptron, let's start by building neural networks and seeing how they all come together. So let's revisit that diagram of the perceptron that I showed you before. If there's only a few things that you get from this class, I really want everyone to take away how a perceptron works. And there's three steps, remember them always. The dot product, you take a dot product of your inputs and your weights. You add a bias and you apply your non-linearity. There's three steps. Let's simplify this diagram a little bit. Let's clean up some of the arrows and remove the bias. And we can actually see now that every line here has its own associated weight to it. And I'll remove the bias term, like I said, for simplicity. Note that z here is the result of that dot product plus bias, before we apply the activation function, though, g. The final output, though, is simply y, which is equal to the activation function of z, which is our activation value. Now, if we want to define a multi-output neural network, we can simply add another perceptron to this picture. So instead of having one perceptron, now we have two perceptrons and two outputs. Each one is a normal perceptron, exactly like we saw before, taking its inputs from each of the x1's through xm's, taking the dot product, adding a bias, and that's it. Now we have two outputs. Each of those perceptrons, though, will have a different set of weights. Remember that. We'll get back to that. If we want, so actually one thing to keep in mind here is because all the inputs are densely connected, every input has a connection to the weights of every perceptron. These are often called dense layers, or sometimes fully connected layers. Now, through this class, you're going to get a lot of experience actually coding up and practically creating some of these algorithms using a software toolbox called TensorFlow. So now that we have the understanding of how a single perceptron works and how a dense layer works, this is a stack of perceptrons, let's try and see how we can actually build up a dense layer like this all the way from scratch. To do that, we can actually start by initializing the two components of our dense layer, which are the weights and the biases. Now that we have these two parameters of our neural network, of our dense layer, we can actually define the forward propagation of information, just like we saw it and learned about already. That forward propagation of information is simply the dot product or the matrix multiplication of our inputs with our weights, add a bias, that gives us our activation function here, and then we apply this nonlinearity to compute the output. Now, TensorFlow has actually implemented this dense layer for us. So we don't need to do that from scratch, instead we can just call it like shown here. So to create a dense layer with two outputs, we can specify this units equal to two. Now let's take a look at what's called a single layered neural network. This is one we have a single hidden layer between our inputs and our outputs. This layer is called the hidden layer, because unlike an input layer and an output layer, the states of this hidden layer are typically unobserved, they're hidden to some extent, they're not strictly enforced either. And since we have this transformation now from the input layer to the hidden layer and from the hidden layer to the output layer, each of these layers are going to have their own specified weight matrices. We'll call w1 the weight matrices for the first layer and w2 the weight matrix for the second layer. If we take a zoomed in look at one of the neurons in this hidden layer, let's take for example z2 for example, this is the exact same perceptron that we saw before. We can compute its output, again using the exact same story, taking all of its inputs x1 through xm, applying a dot product with the weights, adding a bias and that gives us z2. If we look at a different neuron, let's suppose z3, we'll get a different value here because the weights leading to z3 are probably different than those leading to z2. Now this picture looks a bit messy, so let's try and clean things up a bit more. From now on, I'll just use this symbol here to denote what we call this dense layer or fully connected layers. And here you can actually see an example of how we can create this exact neural network again using TensorFlow with the predefined dense layer notation. Here we're creating a sequential model where we can stack layers on top of each other. This layer with n neurons and the second layer with 2 neurons, the output layer. And if we want to create a deep neural network, all we have to do is keep stacking these layers to create more and more hierarchical models, ones where the final output is computed by going deeper and deeper into the network. And to implement this in TensorFlow again, it's very similar as we saw before, again using the TFKARIS sequential call, we can stack each of these dense layers on top of each other, each one specified by the number of neurons in that dense layer, n1 and 2, but with the last output layer fixed to 2 outputs, if that's how many outputs we have. Okay, so that's awesome. Now we have an idea of not only how to build up a neural network directly from a perceptron, but how to compose them together to form complex deep neural networks. Let's take a look at how we can actually apply them to a very real problem that I believe all of you should care very deeply about. Here's a problem that we want to build an AI system to learn to answer. Will I pass this class? And we can start with a simple two feature model. One feature, let's say, is the number of lectures that you attend as part of this class. And the second feature is the number of hours that you spend working on your final project. You do have some training data from all of the past participants of Success191. And we can plot this data on this feature space like this. The green points here actually indicate students, so each point is one student that has passed the class, and the red points are students that have failed the class. You can see where they are in this feature space depends on the actual number of hours that they attended the lecture, the number of lectures they attended, and the number of hours they spent on the final project. And then there's you. You have attended four lectures, and you have spent five hours on your final project. And you want to understand how can you build a neural network given everyone else in this class? Will you pass or fail this class based on the training data that you see? So let's do it. We have now all of the requirements to do this now. So let's build a neural network with two inputs, x1 and x2, with x1 being the number of lectures that we attend, x2 is the number of hours you spend on your final project. We'll have one hidden layer with three units, and we'll feed those into a final probability output by passing this class, and we can see that the probability that we pass is 0.1, or 10%. That's not great, but the reason is because that this model was never actually trained. It's basically just a baby. It's never seen any data. Even though you have seen the data, it hasn't seen any data. And more importantly, you haven't told the model how to interpret this data. It needs to learn about this problem first. It knows nothing about this class or final projects or any of that. So one of the most important things to do this is actually you have to tell the model when it is making bad predictions in order for it to be able to correct itself. Now the loss of a neural network actually defines exactly this. It defines how wrong a prediction was. So it takes as input the predicted outputs and the ground truth outputs. Now if those two things are very far apart from each other, then the loss will be very large. On the other hand, the closer these two things are from each other, the smaller the loss, and the more accurate the loss the model will be. So we always want to minimize the loss. We want to incur, we want to predict something that's as close as possible to the ground truth. Now let's assume we have not just the data from one student, but as we have in this case the data from many students. We now care about not just how the model did on predicting just one prediction, but how it did on average across all of these students. This is what we call the empirical loss. And it's simply just the mean or the average of every loss from each individual example or each individual student. When training a neural network, we want to find a network that minimizes the empirical loss between our predictions and the true outputs. Now if we look at the problem of binary classification, where the neural network like we want to do in this case, is supposed to answer either yes or no, one or zero. We can use what is called a softmax cross entropy loss. Now the softmax cross entropy loss is actually written out here and it's defined by what's called the cross entropy between two probability distributions. It measures how far apart the ground truth probability distribution is from the predicted probability distribution. Let's suppose instead of predicting binary outputs, will I pass this class or will I not pass this class? Instead you want to predict the final grade as a real number, not a probability or as a percentage, we want the grade that you will get in this class. Now in this case, because the type of the output is different, we also need to use a different loss here, because our outputs are no longer 0, 1, but they can be any real number. They're just the grade that you're going to get on the final class. So for example, here since this is a continuous variable, the grade, we want to use what's called the mean squared error. This measures just the squared error, the squared difference between our ground truth and our predictions, again averaged over the entire data set. Okay great, so now we've seen two loss functions, one for classification, binary outputs, as well as regression, continuous outputs, and the problem now I think that we need to start asking ourselves is how can we take that loss function? We've seen our loss function, we've seen our network, now we have to actually understand how can we put those two things together? How can we use our loss function to train the weights of our neural network such that it can actually learn that problem? Well, what we want to do is actually find the weights of the neural network that will minimize the loss of our data set. That essentially means that we want to find the W's in our neural network that minimize J of W. J of W's are empirical cost function that we saw in the previous slides that average loss over each data point in the data set. Now, remember that W, capital W, is simply a collection of all of the weights in our neural network, not just from one layer, but from every single layer. So that's W0 from the zeroth layer to the first layer to the second layer, all concatenate into one. In this optimization problem we want to optimize all of the W's to minimize this empirical loss. Now, remember our loss function is just a simple function of our weights. If we have only two weights, we can actually plot this entire loss landscape over this grid of weights. So on the one axis on the bottom you can see weight number one and the other one you can see weight zero. There's only two weights in this neural network, very simple neural network. So we can actually plot for every W0 and W1, what is the loss? What is the error that we'd expect to see and obtain from this neural network? Now the whole process of training a neural network, optimizing it, is to find the lowest point in this loss landscape that will tell us our optimal W0 and W1. Now how can we do that? The first thing we have to do is pick a point. So let's pick any W0, W1. Starting from this point we can compute the gradient of the landscape at that point. Now the gradient tells us the direction of highest or steepest ascent. So that tells us which way is up. If we compute the gradient of our loss with respect to our weights, that's the derivative of our gradient for loss with respect to the weights, that tells us the direction of which way is up on that loss landscape from where we stand right now. Instead of going up though, we want to find the lowest loss. So let's take the negative of our gradient and take a small step in that direction. This will move us a little bit closer to the lowest point. And we just keep repeating this. Now we compute the gradient at this point and repeat the process until we converge. And we will converge to a local minimum. We don't know if it will converge to a global minimum, but at least we know that it should in theory converge to a local minimum. Now we can summarize this algorithm as follows. This algorithm is also known as gradient descent. So we start by initializing all of our weights randomly and we loop until convergence. We start from one of those weights, our initial point. We compute the gradient. That tells us which way is up. So we take a step in the opposite direction. We take a small step here. All is computed by multiplying our gradient by this factor eta. And we'll learn more about this factor later. This factor is called the learning rate. We'll learn more about that later. Now again, in TensorFlow, we can actually see this pseudocode of gradient descent algorithm written out in code. We can randomize all of our weights. That basically initializes our search, our optimization process at some point in space. Then we keep looping over and over and over again. We compute the loss, we compute the gradient, and we take a small step of our weights in the direction of that gradient. But now let's take a look at this term here. This is how we actually compute the gradient. This explains how the loss is changing with respect to the weight. But I never actually told you how we compute this. So let's talk about this process, which is actually extremely important in training neural networks. It's known as back propagation. So how does back propagation work? How do we compute this gradient? Let's start with a very simple neural network. This is probably the simplest neural network in existence. It only has one input, one hidden neuron, and one output. Taking the gradient of our loss, j of w, with respect to one of the weights, in this case just w2, for example, tells us how much a small change in w2 is going to affect our loss, j. So if we move around j, infinitesimally small, how will that affect our loss? That's what the gradient is going to tell us, derivative of j of w2. So if we write out this derivative, we can actually apply the chain rule to actually compute it. So what does that look like? Specifically, we can decompose that derivative into the derivative of j dw over dy multiplied by derivative of our output with respect to w2. Now the question here is with the second part. If we want to compute now not the derivative of our loss with respect to w2, but now the loss with respect to w1, we can do the same story as before. We can apply the chain rule now recursively. So now we have to apply the chain rule again to the second part. Now the second part is expanded even further. So the derivative of our output with respect to z1, which is the activation function of this first hidden unit. And we can back propagate this information now. You can see starting from our loss all the way through w2 and then recursively applying this chain rule again to get to w1. And this allows us to see both the gradient at both w2 and w1. So in this case, just to reiterate once again, this is telling us this dj dw1 is telling us how a small change in our weight is going to affect our loss. So we can see if we increase our weight a small amount, it will increase our loss. That means we will want to decrease the weight to decrease our loss. That's what the gradient tells us. Which direction we need to step in order to decrease or increase our loss function. Now we showed this here for just two weights in our neural network because we only have two weights. But imagine we have a very deep neural network. One with more than just two layers of or one layer rather of hidden units. We can just repeat this process of applying, recursively applying the chain rule to determine how every single way in the model needs to change to impact that loss. But really all this boils down to just recursively applying this chain rule formulation that you can see here. And that's the back propagation algorithm. In theory it sounds very simple. It's just a very basic extension on derivatives and the chain rule. But now let's actually touch on some insights from training these networks in practice that make this process much more complicated in practice. And why using back propagation as we saw there is not always so easy. Now in practice training neural networks and optimization of networks can be extremely difficult and it's actually extremely computationally intensive. Here's the visualization of what a loss landscape of a real neural network can look like visualized on just two dimensions. Now you can see here that the loss is extremely non-convex meaning that it has many, many local minimum. That can make using an algorithm like gradient descent very, very challenging because gradient descent is always going to step closest to the first local minimum but it can always get stuck there. So finding how to get to the global minima or a really good solution for your neural network can often be very sensitive to your hyper parameter such as where the optimizer starts in this loss landscape. If it starts in a potentially bad part of the landscape it can very easily get stuck in one of these local minimum. Now recall the equation that we talked about for gradient descent. This was the equation I showed you. Our next weight update is going to be your current weights minus a small amount called the learning rate multiplied by the gradient. So we have this minus sign because we want to step in the opposite direction and we multiply it by the gradient or we multiply it by the small number called here called eta which is what we call the learning rate. How fast do we want to do the learning? Now it determines actually not just how fast to do the learning that's maybe not the best way to say it but it tells us how large should each step we take in practice be with regards to that gradient. So the gradient tells us the direction but it doesn't necessarily tell us the magnitude of the direction. So eta can tell us actually a scale of how much we want to trust that gradient and step in the direction of that gradient. In practice setting even eta, this one parameter, this one number can be extremely difficult and I want to give you a quick example of why. So if you have a very non-convex or loss landscape where you have local minima, if you set the learning rate too low then the model can get stuck in these local minima. It can never escape them because it actually does optimize itself but it optimizes it to a very non-optimal minima and it can converge very slowly as well. On the other hand if we increase our learning rate too much then we can actually overshoot our minima and actually diverge and lose control and basically explode the training process completely. One of the challenges is actually how to use stable learning rates that are large enough to avoid the local minima but small enough so that they don't diverge completely. So they're small enough to actually converge to that global spot once they reach it. So how can we actually set this learning rate? Well one option which is actually somewhat popular in practice is to actually just try a lot of different learning rates and that actually works. It is a feasible approach but let's see if we can do something a little bit smarter than that, more intelligent. What if we could say instead how can we build an adaptive learning rate that actually looks at its lost landscape and adapts itself to account for what it sees in the landscape. There are actually many types of optimizers that do exactly this. This means that the learning rates are no longer fixed. They can increase or decrease depending on how large the gradient is in that location and how fast we want and how fast we're actually learning and many other options. They could be also with regards to the size of the weights at that point, the magnitudes, etc. In fact these have been widely explored and published as part of TensorFlow as well and during your labs we encourage each of you to really try out each of these different types of optimizers and experiment with their performance in different types of problems so that you can gain very important intuition about when to use different types of optimizers or what their advantages are and disadvantages in certain applications as well. Let's try and put all of this together. Here we can see a full loop of using TensorFlow to define your model on the first line, define your optimizer. Here you can replace this with any optimizer that you want. Here I'm just using stochastic gradient descent like we saw before. Feeding it through the model we loop forever. We're doing this forward prediction. We predict using our model. We compute the loss with our prediction. This is exactly the loss is telling us again how incorrect our prediction is with respect to the ground truth why. We compute the gradient of our loss with respect to each of the weights in our neural network. Then finally we apply those gradients using our optimizer to step and update our weights. This is really taking everything that we've learned in the class and lecture so far and applying it into one whole piece of code written in TensorFlow. So I want to continue this talk and really talk about tips for training these networks in practice now that we can focus on this very powerful idea of batching your data into mini batches. So before we saw it with gradient descent that we have the following algorithm. This gradient that we saw to compute using back propagation can be actually very intensive to compute especially if it's computed over your entire training set. This is a summation over every single data point in the entire data set and most real life applications. It is simply not feasible to compute this on every single iteration in your optimization loop. Alternatively let's consider a different variant of this algorithm called stochastic gradient descent. So instead of computing the gradient over our entire data set let's just pick a single point compute the gradient of that single point with respect to the weights and then update all of our weights based on that gradient. So this has some advantages this is very easy to compute because it's only using one data point now it's very fast but it's also very noisy because it's only from one data point. Instead there's a middle ground instead of computing this noisy gradient of a single point let's get a better estimate of our gradient by using a batch of b data points. So now let's pick a batch of b data points and we'll compute the gradient estimate simply as the average over this batch. So since b here is usually not that large on the order of tens or hundreds of samples this is much much faster to compute than regular gradient descent and it's also much much more accurate than purely stochastic gradient descent that only uses a single example. Now this increases the gradient accuracy estimation which also allows us to converge much more smoothly it also means that we can trust our gradient more than in stochastic gradient descent so that we can actually increase our learning rate a bit more as well. Mini batching also leads to massively parallelizable computation we can split up the batches on separate workers and separate machines and thus achieve even more parallelization and speed increases on our GPUs. Now the last topic I want to talk about is that of overfitting this is also known as the problem of generalization and is one of the most fundamental problems in all of machine learning and not just deep learning. Now overfitting like I said is critical to understand so I really want to make sure that this is a clear concept in everyone's mind ideally in machine learning we want to learn a model that accurately describes our test data not the training data even though we're optimizing this model based on the training data what we really want is for it to perform well on the test data. So said differently we want to build representations that can learn from our training data but still generalize well to unseen test data. Now assume you want to build a line to describe these points underfitting means that the model does simply not have enough capacity to represent these points so no matter how good we try to fit this model it simply does not have the capacity to represent this type of data. On the far right hand side we can see the extreme other extreme where here the model is too complex it has too many parameters and it does not generalize well to new data. In the middle though we can see what's called the ideal fit it's not overfitting it's not underfitting but it has a medium number of parameters and it's able to fit in a generalizable way to the output and is able to generalize well to brand new data when it sees it at test time. Now to address this problem let's talk about regularization how can we make sure that our models do not end up overfit because neural networks do have a ton of parameters how can we enforce some form of regularization to them. Now what is regularization? Regularization is a technique that constrains our optimization problem such that we can discourage these complex models from actually being learned and overfit. So again why do we need it? We need it so that our model can generalize to this unseen data set and in neural networks we have many techniques for actually imposing regularization onto the model. One very common technique and very simple to understand is called dropout. This is one of the most popular forms of regularization in deep learning and it's very simple. Let's revisit this picture of a neural network. This is a two-layered neural network, two hidden layers and in dropout during training all we simply do is randomly set some of the activations here to zero with some probability. So what we can do is let's say we pick our probability to be 50% or 0.5 we can drop randomly for each of the activations 50% of those neurons. This is extremely powerful as it lowers the capacity of our neural network so that they have to learn to perform better on test sets because sometimes on training sets it just simply cannot rely on some of those parameters. So it has to be able to be resilient to that kind of dropout. It also means that they're easier to train because at least on every forward pass of iterations we're training only 50% of the weights and only 50% of the gradients. So that also cuts our gradient computation time down by a factor of two. So because now we only have to compute half the number of neuron gradients. Now on every iteration we dropped out on the previous iteration 50% of neurons but on the next iteration we're going to drop out a different set of neurons. And this gives the network, it basically forces the network to learn how to take different pathways to get to its answer and it can't rely on any one pathway too strongly and overfit to that pathway. This is a way to really force it to generalize to this new data. The second regularization technique that we'll talk about is this notion of early stopping. And again here the idea is very basic. It's basically let's stop training once we realize that our loss is increasing on a held out validation or let's call it a test set. So when we start training we all know the definition of overfitting is when our model starts to perform worse on the test set. So if we set aside some of this training data to be quote unquote test data we can monitor how our network is learning on this data and simply just stop before it has a chance to overfit. So on the x-axis you can see the number of training iterations and on the y-axis you can see the loss that we get after training that number of iterations. So as we continue to train in the beginning both lines continue to decrease. This is as we'd expect and this is excellent since it means our model is getting stronger. Eventually though the network's testing loss plateaus and starts to increase. Note that the training accuracy will always continue to go down as long as the network has the capacity to memorize the data and this pattern continues for the rest of training. So it's important here to actually focus on this point here. This is the point where we need to stop training and after this point assuming that our test set is a valid representation of the true test set the accuracy of the model will only get worse. So we can stop training here take this model and this should be the model that we actually use when we deploy into the real world. Anything any model taken from the left hand side is going to be underfit is not going to be utilizing the full capacity of the network and anything taken from the right hand side is overfit and actually performing worse than it needs to on that held out test set. So I'll conclude this lecture by summarizing three key points that we've covered so far. We started about the fundamental building blocks of neural networks the perceptron. We learned about stacking and composing these perceptrons together to form complex hierarchical neural networks and how to mathematically optimize these models with back propagation. And finally we address the practical side of these models that you'll find useful for the labs today including adaptive learning rates, batching and regularization. So thank you for attending the first lecture in 6S191. Thank you very much.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 17.16, "text": " Good afternoon, everyone, and welcome to MIT 6S191, Introduction to Deep Learning.", "tokens": [50364, 2205, 6499, 11, 1518, 11, 293, 2928, 281, 13100, 1386, 50, 3405, 16, 11, 27193, 882, 281, 14895, 15205, 13, 51222], "temperature": 0.0, "avg_logprob": -0.24838733673095703, "compression_ratio": 1.438095238095238, "no_speech_prob": 0.046553269028663635}, {"id": 1, "seek": 0, "start": 17.16, "end": 21.32, "text": " My name is Alexander Amini and I'm so excited to be your instructor this year along with", "tokens": [51222, 1222, 1315, 307, 14845, 2012, 3812, 293, 286, 478, 370, 2919, 281, 312, 428, 18499, 341, 1064, 2051, 365, 51430], "temperature": 0.0, "avg_logprob": -0.24838733673095703, "compression_ratio": 1.438095238095238, "no_speech_prob": 0.046553269028663635}, {"id": 2, "seek": 0, "start": 21.32, "end": 24.64, "text": " Aviso Imani in this new virtual format.", "tokens": [51430, 316, 4938, 78, 286, 43717, 294, 341, 777, 6374, 7877, 13, 51596], "temperature": 0.0, "avg_logprob": -0.24838733673095703, "compression_ratio": 1.438095238095238, "no_speech_prob": 0.046553269028663635}, {"id": 3, "seek": 0, "start": 24.64, "end": 29.96, "text": " 6S191 is a two-week boot camp on everything deep learning and will cover a ton of material", "tokens": [51596, 1386, 50, 3405, 16, 307, 257, 732, 12, 23188, 11450, 2255, 322, 1203, 2452, 2539, 293, 486, 2060, 257, 2952, 295, 2527, 51862], "temperature": 0.0, "avg_logprob": -0.24838733673095703, "compression_ratio": 1.438095238095238, "no_speech_prob": 0.046553269028663635}, {"id": 4, "seek": 2996, "start": 30.240000000000002, "end": 31.560000000000002, "text": " in only two weeks.", "tokens": [50378, 294, 787, 732, 3259, 13, 50444], "temperature": 0.0, "avg_logprob": -0.13493392202589247, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.12558366358280182}, {"id": 5, "seek": 2996, "start": 31.560000000000002, "end": 35.160000000000004, "text": " So I think it's really important for us to dive right in with these lectures.", "tokens": [50444, 407, 286, 519, 309, 311, 534, 1021, 337, 505, 281, 9192, 558, 294, 365, 613, 16564, 13, 50624], "temperature": 0.0, "avg_logprob": -0.13493392202589247, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.12558366358280182}, {"id": 6, "seek": 2996, "start": 35.160000000000004, "end": 39.6, "text": " But before we do that, I do want to motivate exactly why I think this is such an awesome", "tokens": [50624, 583, 949, 321, 360, 300, 11, 286, 360, 528, 281, 28497, 2293, 983, 286, 519, 341, 307, 1270, 364, 3476, 50846], "temperature": 0.0, "avg_logprob": -0.13493392202589247, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.12558366358280182}, {"id": 7, "seek": 2996, "start": 39.6, "end": 41.36, "text": " field to study.", "tokens": [50846, 2519, 281, 2979, 13, 50934], "temperature": 0.0, "avg_logprob": -0.13493392202589247, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.12558366358280182}, {"id": 8, "seek": 2996, "start": 41.36, "end": 45.88, "text": " And when we taught this class last year, I decided to try introducing the class very", "tokens": [50934, 400, 562, 321, 5928, 341, 1508, 1036, 1064, 11, 286, 3047, 281, 853, 15424, 264, 1508, 588, 51160], "temperature": 0.0, "avg_logprob": -0.13493392202589247, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.12558366358280182}, {"id": 9, "seek": 2996, "start": 45.88, "end": 46.88, "text": " differently.", "tokens": [51160, 7614, 13, 51210], "temperature": 0.0, "avg_logprob": -0.13493392202589247, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.12558366358280182}, {"id": 10, "seek": 2996, "start": 46.88, "end": 53.400000000000006, "text": " And instead of me telling the class how great 6S191 is, I wanted to let someone else do", "tokens": [51210, 400, 2602, 295, 385, 3585, 264, 1508, 577, 869, 1386, 50, 3405, 16, 307, 11, 286, 1415, 281, 718, 1580, 1646, 360, 51536], "temperature": 0.0, "avg_logprob": -0.13493392202589247, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.12558366358280182}, {"id": 11, "seek": 2996, "start": 53.400000000000006, "end": 54.480000000000004, "text": " that instead.", "tokens": [51536, 300, 2602, 13, 51590], "temperature": 0.0, "avg_logprob": -0.13493392202589247, "compression_ratio": 1.6169354838709677, "no_speech_prob": 0.12558366358280182}, {"id": 12, "seek": 5448, "start": 54.48, "end": 60.64, "text": " So actually, I want to start this year by showing you how we introduced 6S191 last year.", "tokens": [50364, 407, 767, 11, 286, 528, 281, 722, 341, 1064, 538, 4099, 291, 577, 321, 7268, 1386, 50, 3405, 16, 1036, 1064, 13, 50672], "temperature": 0.0, "avg_logprob": -0.28506946563720703, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0012054317630827427}, {"id": 13, "seek": 5448, "start": 60.64, "end": 71.44, "text": " Hi, everybody, and welcome to MIT 6S191, the official introductory course on deep learning", "tokens": [50672, 2421, 11, 2201, 11, 293, 2928, 281, 13100, 1386, 50, 3405, 16, 11, 264, 4783, 39048, 1164, 322, 2452, 2539, 51212], "temperature": 0.0, "avg_logprob": -0.28506946563720703, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0012054317630827427}, {"id": 14, "seek": 5448, "start": 71.44, "end": 74.44, "text": " taught here at MIT.", "tokens": [51212, 5928, 510, 412, 13100, 13, 51362], "temperature": 0.0, "avg_logprob": -0.28506946563720703, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0012054317630827427}, {"id": 15, "seek": 5448, "start": 74.44, "end": 82.72, "text": " Deep learning is revolutionizing so many fields from robotics, medicine, and everything", "tokens": [51362, 14895, 2539, 307, 8894, 3319, 370, 867, 7909, 490, 34145, 11, 7195, 11, 293, 1203, 51776], "temperature": 0.0, "avg_logprob": -0.28506946563720703, "compression_ratio": 1.4494949494949494, "no_speech_prob": 0.0012054317630827427}, {"id": 16, "seek": 8272, "start": 82.72, "end": 83.72, "text": " in between.", "tokens": [50364, 294, 1296, 13, 50414], "temperature": 0.0, "avg_logprob": -0.2903451078078326, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.03864037245512009}, {"id": 17, "seek": 8272, "start": 83.72, "end": 91.52, "text": " You'll learn from the medals of this field and how you can build some of these incredible", "tokens": [50414, 509, 603, 1466, 490, 264, 38647, 295, 341, 2519, 293, 577, 291, 393, 1322, 512, 295, 613, 4651, 50804], "temperature": 0.0, "avg_logprob": -0.2903451078078326, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.03864037245512009}, {"id": 18, "seek": 8272, "start": 91.52, "end": 92.52, "text": " algorithms.", "tokens": [50804, 14642, 13, 50854], "temperature": 0.0, "avg_logprob": -0.2903451078078326, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.03864037245512009}, {"id": 19, "seek": 8272, "start": 92.52, "end": 102.72, "text": " In fact, this entire speech and video are not real and were created using deep learning", "tokens": [50854, 682, 1186, 11, 341, 2302, 6218, 293, 960, 366, 406, 957, 293, 645, 2942, 1228, 2452, 2539, 51364], "temperature": 0.0, "avg_logprob": -0.2903451078078326, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.03864037245512009}, {"id": 20, "seek": 8272, "start": 102.72, "end": 104.72, "text": " and artificial intelligence.", "tokens": [51364, 293, 11677, 7599, 13, 51464], "temperature": 0.0, "avg_logprob": -0.2903451078078326, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.03864037245512009}, {"id": 21, "seek": 8272, "start": 104.72, "end": 108.72, "text": " And in this class, you'll learn how.", "tokens": [51464, 400, 294, 341, 1508, 11, 291, 603, 1466, 577, 13, 51664], "temperature": 0.0, "avg_logprob": -0.2903451078078326, "compression_ratio": 1.5084745762711864, "no_speech_prob": 0.03864037245512009}, {"id": 22, "seek": 10872, "start": 109.72, "end": 119.72, "text": " It has been an honor to speak with you today, and I hope you enjoyed the course.", "tokens": [50414, 467, 575, 668, 364, 5968, 281, 1710, 365, 291, 965, 11, 293, 286, 1454, 291, 4626, 264, 1164, 13, 50914], "temperature": 0.0, "avg_logprob": -0.13607852066619486, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.18662042915821075}, {"id": 23, "seek": 10872, "start": 119.72, "end": 123.72, "text": " So in case you couldn't tell, that was actually not a real video or audio.", "tokens": [50914, 407, 294, 1389, 291, 2809, 380, 980, 11, 300, 390, 767, 406, 257, 957, 960, 420, 6278, 13, 51114], "temperature": 0.0, "avg_logprob": -0.13607852066619486, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.18662042915821075}, {"id": 24, "seek": 10872, "start": 123.72, "end": 128.72, "text": " And the audio you actually heard was purposely degraded, a bit more to even make it more", "tokens": [51114, 400, 264, 6278, 291, 767, 2198, 390, 41840, 24740, 292, 11, 257, 857, 544, 281, 754, 652, 309, 544, 51364], "temperature": 0.0, "avg_logprob": -0.13607852066619486, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.18662042915821075}, {"id": 25, "seek": 10872, "start": 128.72, "end": 132.72, "text": " obvious that this was not real and avoid some potential misuse.", "tokens": [51364, 6322, 300, 341, 390, 406, 957, 293, 5042, 512, 3995, 3346, 438, 13, 51564], "temperature": 0.0, "avg_logprob": -0.13607852066619486, "compression_ratio": 1.5714285714285714, "no_speech_prob": 0.18662042915821075}, {"id": 26, "seek": 13272, "start": 133.72, "end": 138.72, "text": " Even with the purposely degraded audio, that intro went somewhat viral last year after", "tokens": [50414, 2754, 365, 264, 41840, 24740, 292, 6278, 11, 300, 12897, 1437, 8344, 16132, 1036, 1064, 934, 50664], "temperature": 0.0, "avg_logprob": -0.10577917098999023, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.2223425954580307}, {"id": 27, "seek": 13272, "start": 138.72, "end": 142.42, "text": " the course and we got some really great and interesting feedback.", "tokens": [50664, 264, 1164, 293, 321, 658, 512, 534, 869, 293, 1880, 5824, 13, 50849], "temperature": 0.0, "avg_logprob": -0.10577917098999023, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.2223425954580307}, {"id": 28, "seek": 13272, "start": 142.42, "end": 147.07999999999998, "text": " And to be honest, after last year and when we did this, I thought it was going to be", "tokens": [50849, 400, 281, 312, 3245, 11, 934, 1036, 1064, 293, 562, 321, 630, 341, 11, 286, 1194, 309, 390, 516, 281, 312, 51082], "temperature": 0.0, "avg_logprob": -0.10577917098999023, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.2223425954580307}, {"id": 29, "seek": 13272, "start": 147.07999999999998, "end": 151.0, "text": " really hard for us to top it this year.", "tokens": [51082, 534, 1152, 337, 505, 281, 1192, 309, 341, 1064, 13, 51278], "temperature": 0.0, "avg_logprob": -0.10577917098999023, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.2223425954580307}, {"id": 30, "seek": 13272, "start": 151.0, "end": 154.72, "text": " But actually, I was wrong because the one thing I love about this field is that it's", "tokens": [51278, 583, 767, 11, 286, 390, 2085, 570, 264, 472, 551, 286, 959, 466, 341, 2519, 307, 300, 309, 311, 51464], "temperature": 0.0, "avg_logprob": -0.10577917098999023, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.2223425954580307}, {"id": 31, "seek": 13272, "start": 154.72, "end": 160.72, "text": " moving so incredibly fast that even within the past year, the state of the art has significantly", "tokens": [51464, 2684, 370, 6252, 2370, 300, 754, 1951, 264, 1791, 1064, 11, 264, 1785, 295, 264, 1523, 575, 10591, 51764], "temperature": 0.0, "avg_logprob": -0.10577917098999023, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.2223425954580307}, {"id": 32, "seek": 13272, "start": 160.72, "end": 161.72, "text": " advanced.", "tokens": [51764, 7339, 13, 51814], "temperature": 0.0, "avg_logprob": -0.10577917098999023, "compression_ratio": 1.7116788321167884, "no_speech_prob": 0.2223425954580307}, {"id": 33, "seek": 16172, "start": 161.72, "end": 167.48, "text": " And the video you saw that we used last year used deep learning, but it was not a particularly", "tokens": [50364, 400, 264, 960, 291, 1866, 300, 321, 1143, 1036, 1064, 1143, 2452, 2539, 11, 457, 309, 390, 406, 257, 4098, 50652], "temperature": 0.0, "avg_logprob": -0.14192463653256196, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.03106258064508438}, {"id": 34, "seek": 16172, "start": 167.48, "end": 169.52, "text": " easy video to create.", "tokens": [50652, 1858, 960, 281, 1884, 13, 50754], "temperature": 0.0, "avg_logprob": -0.14192463653256196, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.03106258064508438}, {"id": 35, "seek": 16172, "start": 169.52, "end": 174.68, "text": " It required a full video of Obama speaking, and it used this to intelligently stitch", "tokens": [50754, 467, 4739, 257, 1577, 960, 295, 9560, 4124, 11, 293, 309, 1143, 341, 281, 5613, 2276, 5635, 51012], "temperature": 0.0, "avg_logprob": -0.14192463653256196, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.03106258064508438}, {"id": 36, "seek": 16172, "start": 174.68, "end": 179.44, "text": " together parts of the scene to make it look and appear like he was mouthing the words", "tokens": [51012, 1214, 3166, 295, 264, 4145, 281, 652, 309, 574, 293, 4204, 411, 415, 390, 275, 346, 571, 264, 2283, 51250], "temperature": 0.0, "avg_logprob": -0.14192463653256196, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.03106258064508438}, {"id": 37, "seek": 16172, "start": 179.44, "end": 180.44, "text": " that I said.", "tokens": [51250, 300, 286, 848, 13, 51300], "temperature": 0.0, "avg_logprob": -0.14192463653256196, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.03106258064508438}, {"id": 38, "seek": 16172, "start": 180.44, "end": 185.72, "text": " And to see the behind the scenes here, now you can see the same video with my voice.", "tokens": [51300, 400, 281, 536, 264, 2261, 264, 8026, 510, 11, 586, 291, 393, 536, 264, 912, 960, 365, 452, 3177, 13, 51564], "temperature": 0.0, "avg_logprob": -0.14192463653256196, "compression_ratio": 1.6523605150214593, "no_speech_prob": 0.03106258064508438}, {"id": 39, "seek": 18572, "start": 185.72, "end": 195.52, "text": " Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning", "tokens": [50364, 2421, 2201, 11, 293, 2928, 281, 13100, 1386, 50, 3405, 16, 11, 264, 4783, 39048, 1164, 322, 2452, 2539, 50854], "temperature": 0.0, "avg_logprob": -0.1525252171051808, "compression_ratio": 1.485981308411215, "no_speech_prob": 0.06274924427270889}, {"id": 40, "seek": 18572, "start": 195.52, "end": 198.96, "text": " taught here at MIT.", "tokens": [50854, 5928, 510, 412, 13100, 13, 51026], "temperature": 0.0, "avg_logprob": -0.1525252171051808, "compression_ratio": 1.485981308411215, "no_speech_prob": 0.06274924427270889}, {"id": 41, "seek": 18572, "start": 198.96, "end": 204.38, "text": " Now it's actually possible to use just a single static image, not the full video to", "tokens": [51026, 823, 309, 311, 767, 1944, 281, 764, 445, 257, 2167, 13437, 3256, 11, 406, 264, 1577, 960, 281, 51297], "temperature": 0.0, "avg_logprob": -0.1525252171051808, "compression_ratio": 1.485981308411215, "no_speech_prob": 0.06274924427270889}, {"id": 42, "seek": 18572, "start": 204.38, "end": 207.16, "text": " achieve the exact same thing.", "tokens": [51297, 4584, 264, 1900, 912, 551, 13, 51436], "temperature": 0.0, "avg_logprob": -0.1525252171051808, "compression_ratio": 1.485981308411215, "no_speech_prob": 0.06274924427270889}, {"id": 43, "seek": 18572, "start": 207.16, "end": 214.8, "text": " And now you can actually see eight more examples of Obama now just created using just a single", "tokens": [51436, 400, 586, 291, 393, 767, 536, 3180, 544, 5110, 295, 9560, 586, 445, 2942, 1228, 445, 257, 2167, 51818], "temperature": 0.0, "avg_logprob": -0.1525252171051808, "compression_ratio": 1.485981308411215, "no_speech_prob": 0.06274924427270889}, {"id": 44, "seek": 21480, "start": 214.8, "end": 221.0, "text": " static image, no more full dynamic videos, but we can achieve the same incredible realism", "tokens": [50364, 13437, 3256, 11, 572, 544, 1577, 8546, 2145, 11, 457, 321, 393, 4584, 264, 912, 4651, 38484, 50674], "temperature": 0.0, "avg_logprob": -0.15740035130427435, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.13635116815567017}, {"id": 45, "seek": 21480, "start": 221.0, "end": 223.44, "text": " and result using deep learning.", "tokens": [50674, 293, 1874, 1228, 2452, 2539, 13, 50796], "temperature": 0.0, "avg_logprob": -0.15740035130427435, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.13635116815567017}, {"id": 46, "seek": 21480, "start": 223.44, "end": 229.20000000000002, "text": " Now of course, there's nothing restricting us to one person.", "tokens": [50796, 823, 295, 1164, 11, 456, 311, 1825, 1472, 37714, 505, 281, 472, 954, 13, 51084], "temperature": 0.0, "avg_logprob": -0.15740035130427435, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.13635116815567017}, {"id": 47, "seek": 21480, "start": 229.20000000000002, "end": 234.20000000000002, "text": " This method generalizes to different faces, and there's nothing restricting us even to", "tokens": [51084, 639, 3170, 2674, 5660, 281, 819, 8475, 11, 293, 456, 311, 1825, 1472, 37714, 505, 754, 281, 51334], "temperature": 0.0, "avg_logprob": -0.15740035130427435, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.13635116815567017}, {"id": 48, "seek": 21480, "start": 234.20000000000002, "end": 240.16000000000003, "text": " humans anymore or individuals that the algorithm has ever seen before.", "tokens": [51334, 6255, 3602, 420, 5346, 300, 264, 9284, 575, 1562, 1612, 949, 13, 51632], "temperature": 0.0, "avg_logprob": -0.15740035130427435, "compression_ratio": 1.619047619047619, "no_speech_prob": 0.13635116815567017}, {"id": 49, "seek": 24016, "start": 240.16, "end": 249.44, "text": " Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning", "tokens": [50364, 2421, 2201, 11, 293, 2928, 281, 13100, 1386, 50, 3405, 16, 11, 264, 4783, 39048, 1164, 322, 2452, 2539, 50828], "temperature": 0.0, "avg_logprob": -0.09519568486000175, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.06948531419038773}, {"id": 50, "seek": 24016, "start": 249.44, "end": 253.96, "text": " taught here at MIT.", "tokens": [50828, 5928, 510, 412, 13100, 13, 51054], "temperature": 0.0, "avg_logprob": -0.09519568486000175, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.06948531419038773}, {"id": 51, "seek": 24016, "start": 253.96, "end": 260.08, "text": " The ability to generate these types of dynamic moving videos from only a single image is", "tokens": [51054, 440, 3485, 281, 8460, 613, 3467, 295, 8546, 2684, 2145, 490, 787, 257, 2167, 3256, 307, 51360], "temperature": 0.0, "avg_logprob": -0.09519568486000175, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.06948531419038773}, {"id": 52, "seek": 24016, "start": 260.08, "end": 264.68, "text": " remarkable to me, and it's a testament to the true power of deep learning.", "tokens": [51360, 12802, 281, 385, 11, 293, 309, 311, 257, 35499, 281, 264, 2074, 1347, 295, 2452, 2539, 13, 51590], "temperature": 0.0, "avg_logprob": -0.09519568486000175, "compression_ratio": 1.4145077720207253, "no_speech_prob": 0.06948531419038773}, {"id": 53, "seek": 26468, "start": 264.68, "end": 270.64, "text": " In this class, you're going to actually not only learn about the technical basis of this", "tokens": [50364, 682, 341, 1508, 11, 291, 434, 516, 281, 767, 406, 787, 1466, 466, 264, 6191, 5143, 295, 341, 50662], "temperature": 0.0, "avg_logprob": -0.13498170305006574, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.04335249587893486}, {"id": 54, "seek": 26468, "start": 270.64, "end": 279.8, "text": " technology, but also some of the very important and very important ethical and societal implications", "tokens": [50662, 2899, 11, 457, 611, 512, 295, 264, 588, 1021, 293, 588, 1021, 18890, 293, 33472, 16602, 51120], "temperature": 0.0, "avg_logprob": -0.13498170305006574, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.04335249587893486}, {"id": 55, "seek": 26468, "start": 279.8, "end": 282.08, "text": " of this work as well.", "tokens": [51120, 295, 341, 589, 382, 731, 13, 51234], "temperature": 0.0, "avg_logprob": -0.13498170305006574, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.04335249587893486}, {"id": 56, "seek": 26468, "start": 282.08, "end": 287.72, "text": " Now I hope this was a really great way to get you excited about this course and 6S191,", "tokens": [51234, 823, 286, 1454, 341, 390, 257, 534, 869, 636, 281, 483, 291, 2919, 466, 341, 1164, 293, 1386, 50, 3405, 16, 11, 51516], "temperature": 0.0, "avg_logprob": -0.13498170305006574, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.04335249587893486}, {"id": 57, "seek": 26468, "start": 287.72, "end": 290.24, "text": " and with that let's get started.", "tokens": [51516, 293, 365, 300, 718, 311, 483, 1409, 13, 51642], "temperature": 0.0, "avg_logprob": -0.13498170305006574, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.04335249587893486}, {"id": 58, "seek": 26468, "start": 290.24, "end": 294.24, "text": " We can actually start by taking a step back and asking ourselves what is deep learning,", "tokens": [51642, 492, 393, 767, 722, 538, 1940, 257, 1823, 646, 293, 3365, 4175, 437, 307, 2452, 2539, 11, 51842], "temperature": 0.0, "avg_logprob": -0.13498170305006574, "compression_ratio": 1.6963562753036436, "no_speech_prob": 0.04335249587893486}, {"id": 59, "seek": 29424, "start": 294.32, "end": 298.0, "text": " deep learning, and the context of intelligence.", "tokens": [50368, 2452, 2539, 11, 293, 264, 4319, 295, 7599, 13, 50552], "temperature": 0.0, "avg_logprob": -0.16529392901762033, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.04738660901784897}, {"id": 60, "seek": 29424, "start": 298.0, "end": 304.68, "text": " Intelligence is actually the ability to process information such that it can be used to inform", "tokens": [50552, 27274, 307, 767, 264, 3485, 281, 1399, 1589, 1270, 300, 309, 393, 312, 1143, 281, 1356, 50886], "temperature": 0.0, "avg_logprob": -0.16529392901762033, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.04738660901784897}, {"id": 61, "seek": 29424, "start": 304.68, "end": 306.68, "text": " a future decision.", "tokens": [50886, 257, 2027, 3537, 13, 50986], "temperature": 0.0, "avg_logprob": -0.16529392901762033, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.04738660901784897}, {"id": 62, "seek": 29424, "start": 306.68, "end": 312.04, "text": " Now the field of artificial intelligence, or AI, is a science that actually focuses", "tokens": [50986, 823, 264, 2519, 295, 11677, 7599, 11, 420, 7318, 11, 307, 257, 3497, 300, 767, 16109, 51254], "temperature": 0.0, "avg_logprob": -0.16529392901762033, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.04738660901784897}, {"id": 63, "seek": 29424, "start": 312.04, "end": 317.36, "text": " on building algorithms to do exactly this, to build algorithms to process information", "tokens": [51254, 322, 2390, 14642, 281, 360, 2293, 341, 11, 281, 1322, 14642, 281, 1399, 1589, 51520], "temperature": 0.0, "avg_logprob": -0.16529392901762033, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.04738660901784897}, {"id": 64, "seek": 29424, "start": 317.36, "end": 321.64, "text": " such that they can inform future predictions.", "tokens": [51520, 1270, 300, 436, 393, 1356, 2027, 21264, 13, 51734], "temperature": 0.0, "avg_logprob": -0.16529392901762033, "compression_ratio": 1.8300970873786409, "no_speech_prob": 0.04738660901784897}, {"id": 65, "seek": 32164, "start": 321.64, "end": 327.44, "text": " Now machine learning, you can think of this as just a subset of AI that actually focuses", "tokens": [50364, 823, 3479, 2539, 11, 291, 393, 519, 295, 341, 382, 445, 257, 25993, 295, 7318, 300, 767, 16109, 50654], "temperature": 0.0, "avg_logprob": -0.13149351484320138, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.02032395824790001}, {"id": 66, "seek": 32164, "start": 327.44, "end": 333.15999999999997, "text": " on teaching an algorithm to learn from experiences without being explicitly programmed.", "tokens": [50654, 322, 4571, 364, 9284, 281, 1466, 490, 5235, 1553, 885, 20803, 31092, 13, 50940], "temperature": 0.0, "avg_logprob": -0.13149351484320138, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.02032395824790001}, {"id": 67, "seek": 32164, "start": 333.15999999999997, "end": 337.47999999999996, "text": " Now deep learning takes this idea even further, and it's a subset of machine learning that", "tokens": [50940, 823, 2452, 2539, 2516, 341, 1558, 754, 3052, 11, 293, 309, 311, 257, 25993, 295, 3479, 2539, 300, 51156], "temperature": 0.0, "avg_logprob": -0.13149351484320138, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.02032395824790001}, {"id": 68, "seek": 32164, "start": 337.47999999999996, "end": 344.12, "text": " focuses on using neural networks to automatically extract useful patterns in raw data, and then", "tokens": [51156, 16109, 322, 1228, 18161, 9590, 281, 6772, 8947, 4420, 8294, 294, 8936, 1412, 11, 293, 550, 51488], "temperature": 0.0, "avg_logprob": -0.13149351484320138, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.02032395824790001}, {"id": 69, "seek": 32164, "start": 344.12, "end": 349.52, "text": " using these patterns or features to learn to perform that task.", "tokens": [51488, 1228, 613, 8294, 420, 4122, 281, 1466, 281, 2042, 300, 5633, 13, 51758], "temperature": 0.0, "avg_logprob": -0.13149351484320138, "compression_ratio": 1.7428571428571429, "no_speech_prob": 0.02032395824790001}, {"id": 70, "seek": 34952, "start": 349.52, "end": 351.79999999999995, "text": " And that's exactly what this class is about.", "tokens": [50364, 400, 300, 311, 2293, 437, 341, 1508, 307, 466, 13, 50478], "temperature": 0.0, "avg_logprob": -0.14156286576214958, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.013632736168801785}, {"id": 71, "seek": 34952, "start": 351.79999999999995, "end": 357.2, "text": " This class is about teaching algorithms how to learn a task directly from raw data.", "tokens": [50478, 639, 1508, 307, 466, 4571, 14642, 577, 281, 1466, 257, 5633, 3838, 490, 8936, 1412, 13, 50748], "temperature": 0.0, "avg_logprob": -0.14156286576214958, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.013632736168801785}, {"id": 72, "seek": 34952, "start": 357.2, "end": 363.0, "text": " And we want to provide you with a solid foundation both technically and practically for you to", "tokens": [50748, 400, 321, 528, 281, 2893, 291, 365, 257, 5100, 7030, 1293, 12120, 293, 15667, 337, 291, 281, 51038], "temperature": 0.0, "avg_logprob": -0.14156286576214958, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.013632736168801785}, {"id": 73, "seek": 34952, "start": 363.0, "end": 371.08, "text": " understand under the hood how these algorithms are built and how they can learn.", "tokens": [51038, 1223, 833, 264, 13376, 577, 613, 14642, 366, 3094, 293, 577, 436, 393, 1466, 13, 51442], "temperature": 0.0, "avg_logprob": -0.14156286576214958, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.013632736168801785}, {"id": 74, "seek": 34952, "start": 371.08, "end": 376.47999999999996, "text": " So this course is split between technical lectures as well as project software labs.", "tokens": [51442, 407, 341, 1164, 307, 7472, 1296, 6191, 16564, 382, 731, 382, 1716, 4722, 20339, 13, 51712], "temperature": 0.0, "avg_logprob": -0.14156286576214958, "compression_ratio": 1.691304347826087, "no_speech_prob": 0.013632736168801785}, {"id": 75, "seek": 37648, "start": 376.48, "end": 380.6, "text": " We'll cover the foundation starting today with neural networks, which are really the", "tokens": [50364, 492, 603, 2060, 264, 7030, 2891, 965, 365, 18161, 9590, 11, 597, 366, 534, 264, 50570], "temperature": 0.0, "avg_logprob": -0.13419294357299805, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005729351658374071}, {"id": 76, "seek": 37648, "start": 380.6, "end": 384.96000000000004, "text": " building blocks of everything that we'll see in this course.", "tokens": [50570, 2390, 8474, 295, 1203, 300, 321, 603, 536, 294, 341, 1164, 13, 50788], "temperature": 0.0, "avg_logprob": -0.13419294357299805, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005729351658374071}, {"id": 77, "seek": 37648, "start": 384.96000000000004, "end": 390.88, "text": " And this year we also have two brand new really exciting hot topic lectures, focusing on uncertainty", "tokens": [50788, 400, 341, 1064, 321, 611, 362, 732, 3360, 777, 534, 4670, 2368, 4829, 16564, 11, 8416, 322, 15697, 51084], "temperature": 0.0, "avg_logprob": -0.13419294357299805, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005729351658374071}, {"id": 78, "seek": 37648, "start": 390.88, "end": 396.8, "text": " and probabilistic deep learning, as well as algorithmic bias and fairness.", "tokens": [51084, 293, 31959, 3142, 2452, 2539, 11, 382, 731, 382, 9284, 299, 12577, 293, 29765, 13, 51380], "temperature": 0.0, "avg_logprob": -0.13419294357299805, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005729351658374071}, {"id": 79, "seek": 37648, "start": 396.8, "end": 401.52000000000004, "text": " Finally we'll conclude with some really exciting guest lectures and student project presentations", "tokens": [51380, 6288, 321, 603, 16886, 365, 512, 534, 4670, 8341, 16564, 293, 3107, 1716, 18964, 51616], "temperature": 0.0, "avg_logprob": -0.13419294357299805, "compression_ratio": 1.6561264822134387, "no_speech_prob": 0.005729351658374071}, {"id": 80, "seek": 40152, "start": 401.52, "end": 406.79999999999995, "text": " as part of a final project competition that all of you will be eligible to win some really", "tokens": [50364, 382, 644, 295, 257, 2572, 1716, 6211, 300, 439, 295, 291, 486, 312, 14728, 281, 1942, 512, 534, 50628], "temperature": 0.0, "avg_logprob": -0.1118313567806976, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.0032727369107306004}, {"id": 81, "seek": 40152, "start": 406.79999999999995, "end": 409.44, "text": " exciting prizes.", "tokens": [50628, 4670, 27350, 13, 50760], "temperature": 0.0, "avg_logprob": -0.1118313567806976, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.0032727369107306004}, {"id": 82, "seek": 40152, "start": 409.44, "end": 413.56, "text": " Now a bit of logistics before we dive into the technical side of the lecture.", "tokens": [50760, 823, 257, 857, 295, 27420, 949, 321, 9192, 666, 264, 6191, 1252, 295, 264, 7991, 13, 50966], "temperature": 0.0, "avg_logprob": -0.1118313567806976, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.0032727369107306004}, {"id": 83, "seek": 40152, "start": 413.56, "end": 418.24, "text": " For those of you taking this course for credit, you will have two options to fulfill your", "tokens": [50966, 1171, 729, 295, 291, 1940, 341, 1164, 337, 5397, 11, 291, 486, 362, 732, 3956, 281, 13875, 428, 51200], "temperature": 0.0, "avg_logprob": -0.1118313567806976, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.0032727369107306004}, {"id": 84, "seek": 40152, "start": 418.24, "end": 419.79999999999995, "text": " credit requirement.", "tokens": [51200, 5397, 11695, 13, 51278], "temperature": 0.0, "avg_logprob": -0.1118313567806976, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.0032727369107306004}, {"id": 85, "seek": 40152, "start": 419.79999999999995, "end": 427.59999999999997, "text": " The first option will be to actually work in teams of up to four or individually to develop", "tokens": [51278, 440, 700, 3614, 486, 312, 281, 767, 589, 294, 5491, 295, 493, 281, 1451, 420, 16652, 281, 1499, 51668], "temperature": 0.0, "avg_logprob": -0.1118313567806976, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.0032727369107306004}, {"id": 86, "seek": 40152, "start": 427.59999999999997, "end": 430.88, "text": " a cool new deep learning idea.", "tokens": [51668, 257, 1627, 777, 2452, 2539, 1558, 13, 51832], "temperature": 0.0, "avg_logprob": -0.1118313567806976, "compression_ratio": 1.6456692913385826, "no_speech_prob": 0.0032727369107306004}, {"id": 87, "seek": 43088, "start": 430.92, "end": 434.96, "text": " Now doing so will make you eligible to win some of the prizes that you can see on the", "tokens": [50366, 823, 884, 370, 486, 652, 291, 14728, 281, 1942, 512, 295, 264, 27350, 300, 291, 393, 536, 322, 264, 50568], "temperature": 0.0, "avg_logprob": -0.12445611277903158, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.001754170167259872}, {"id": 88, "seek": 43088, "start": 434.96, "end": 437.0, "text": " right hand side.", "tokens": [50568, 558, 1011, 1252, 13, 50670], "temperature": 0.0, "avg_logprob": -0.12445611277903158, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.001754170167259872}, {"id": 89, "seek": 43088, "start": 437.0, "end": 441.36, "text": " And we realize that in the context of this class, which is only two weeks, that's an", "tokens": [50670, 400, 321, 4325, 300, 294, 264, 4319, 295, 341, 1508, 11, 597, 307, 787, 732, 3259, 11, 300, 311, 364, 50888], "temperature": 0.0, "avg_logprob": -0.12445611277903158, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.001754170167259872}, {"id": 90, "seek": 43088, "start": 441.36, "end": 445.84, "text": " extremely short amount of time to come up with an impressive project or research idea.", "tokens": [50888, 4664, 2099, 2372, 295, 565, 281, 808, 493, 365, 364, 8992, 1716, 420, 2132, 1558, 13, 51112], "temperature": 0.0, "avg_logprob": -0.12445611277903158, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.001754170167259872}, {"id": 91, "seek": 43088, "start": 445.84, "end": 450.84, "text": " So we're not going to be judging you on the novelty of that idea, but rather we're not", "tokens": [51112, 407, 321, 434, 406, 516, 281, 312, 23587, 291, 322, 264, 44805, 295, 300, 1558, 11, 457, 2831, 321, 434, 406, 51362], "temperature": 0.0, "avg_logprob": -0.12445611277903158, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.001754170167259872}, {"id": 92, "seek": 43088, "start": 450.84, "end": 455.36, "text": " going to be judging you on the results of that idea, but rather the novelty of the idea,", "tokens": [51362, 516, 281, 312, 23587, 291, 322, 264, 3542, 295, 300, 1558, 11, 457, 2831, 264, 44805, 295, 264, 1558, 11, 51588], "temperature": 0.0, "avg_logprob": -0.12445611277903158, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.001754170167259872}, {"id": 93, "seek": 43088, "start": 455.36, "end": 459.88, "text": " your thinking process and how impactful this idea can be.", "tokens": [51588, 428, 1953, 1399, 293, 577, 30842, 341, 1558, 393, 312, 13, 51814], "temperature": 0.0, "avg_logprob": -0.12445611277903158, "compression_ratio": 1.902621722846442, "no_speech_prob": 0.001754170167259872}, {"id": 94, "seek": 45988, "start": 459.88, "end": 463.56, "text": " But not on the results themselves.", "tokens": [50364, 583, 406, 322, 264, 3542, 2969, 13, 50548], "temperature": 0.0, "avg_logprob": -0.10693300986776547, "compression_ratio": 1.7196652719665273, "no_speech_prob": 0.003272377885878086}, {"id": 95, "seek": 45988, "start": 463.56, "end": 468.08, "text": " On the last day of class, you will actually give a three minute presentation to a group", "tokens": [50548, 1282, 264, 1036, 786, 295, 1508, 11, 291, 486, 767, 976, 257, 1045, 3456, 5860, 281, 257, 1594, 50774], "temperature": 0.0, "avg_logprob": -0.10693300986776547, "compression_ratio": 1.7196652719665273, "no_speech_prob": 0.003272377885878086}, {"id": 96, "seek": 45988, "start": 468.08, "end": 472.0, "text": " of judges who will then award the winners and the prizes.", "tokens": [50774, 295, 14449, 567, 486, 550, 7130, 264, 17193, 293, 264, 27350, 13, 50970], "temperature": 0.0, "avg_logprob": -0.10693300986776547, "compression_ratio": 1.7196652719665273, "no_speech_prob": 0.003272377885878086}, {"id": 97, "seek": 45988, "start": 472.0, "end": 477.32, "text": " Now again, three minutes is extremely short to actually present your ideas and present", "tokens": [50970, 823, 797, 11, 1045, 2077, 307, 4664, 2099, 281, 767, 1974, 428, 3487, 293, 1974, 51236], "temperature": 0.0, "avg_logprob": -0.10693300986776547, "compression_ratio": 1.7196652719665273, "no_speech_prob": 0.003272377885878086}, {"id": 98, "seek": 45988, "start": 477.32, "end": 483.36, "text": " your project, but I do believe that there's an art to presenting and conveying your ideas", "tokens": [51236, 428, 1716, 11, 457, 286, 360, 1697, 300, 456, 311, 364, 1523, 281, 15578, 293, 18053, 1840, 428, 3487, 51538], "temperature": 0.0, "avg_logprob": -0.10693300986776547, "compression_ratio": 1.7196652719665273, "no_speech_prob": 0.003272377885878086}, {"id": 99, "seek": 45988, "start": 483.36, "end": 486.71999999999997, "text": " concisely and clearly in such a short amount of time.", "tokens": [51538, 1588, 271, 736, 293, 4448, 294, 1270, 257, 2099, 2372, 295, 565, 13, 51706], "temperature": 0.0, "avg_logprob": -0.10693300986776547, "compression_ratio": 1.7196652719665273, "no_speech_prob": 0.003272377885878086}, {"id": 100, "seek": 48672, "start": 486.72, "end": 492.72, "text": " So we will be holding you strictly to that strict deadline.", "tokens": [50364, 407, 321, 486, 312, 5061, 291, 20792, 281, 300, 10910, 20615, 13, 50664], "temperature": 0.0, "avg_logprob": -0.1278452180389665, "compression_ratio": 1.7949640287769784, "no_speech_prob": 0.0037068380042910576}, {"id": 101, "seek": 48672, "start": 492.72, "end": 498.12, "text": " The second option to fulfill your grade requirement is to write a one page review on a deep learning", "tokens": [50664, 440, 1150, 3614, 281, 13875, 428, 7204, 11695, 307, 281, 2464, 257, 472, 3028, 3131, 322, 257, 2452, 2539, 50934], "temperature": 0.0, "avg_logprob": -0.1278452180389665, "compression_ratio": 1.7949640287769784, "no_speech_prob": 0.0037068380042910576}, {"id": 102, "seek": 48672, "start": 498.12, "end": 499.64000000000004, "text": " paper.", "tokens": [50934, 3035, 13, 51010], "temperature": 0.0, "avg_logprob": -0.1278452180389665, "compression_ratio": 1.7949640287769784, "no_speech_prob": 0.0037068380042910576}, {"id": 103, "seek": 48672, "start": 499.64000000000004, "end": 503.04, "text": " Here the grade is based more on the clarity of the writing and the technical communication", "tokens": [51010, 1692, 264, 7204, 307, 2361, 544, 322, 264, 16992, 295, 264, 3579, 293, 264, 6191, 6101, 51180], "temperature": 0.0, "avg_logprob": -0.1278452180389665, "compression_ratio": 1.7949640287769784, "no_speech_prob": 0.0037068380042910576}, {"id": 104, "seek": 48672, "start": 503.04, "end": 505.0, "text": " of the main ideas.", "tokens": [51180, 295, 264, 2135, 3487, 13, 51278], "temperature": 0.0, "avg_logprob": -0.1278452180389665, "compression_ratio": 1.7949640287769784, "no_speech_prob": 0.0037068380042910576}, {"id": 105, "seek": 48672, "start": 505.0, "end": 509.40000000000003, "text": " This will be due on Thursday, the last Thursday of the class, and you can pick whatever deep", "tokens": [51278, 639, 486, 312, 3462, 322, 10383, 11, 264, 1036, 10383, 295, 264, 1508, 11, 293, 291, 393, 1888, 2035, 2452, 51498], "temperature": 0.0, "avg_logprob": -0.1278452180389665, "compression_ratio": 1.7949640287769784, "no_speech_prob": 0.0037068380042910576}, {"id": 106, "seek": 48672, "start": 509.40000000000003, "end": 510.88000000000005, "text": " learning paper you would like.", "tokens": [51498, 2539, 3035, 291, 576, 411, 13, 51572], "temperature": 0.0, "avg_logprob": -0.1278452180389665, "compression_ratio": 1.7949640287769784, "no_speech_prob": 0.0037068380042910576}, {"id": 107, "seek": 48672, "start": 510.88000000000005, "end": 516.52, "text": " If you would like some pointers, we have provided some guide papers that can help you get started", "tokens": [51572, 759, 291, 576, 411, 512, 44548, 11, 321, 362, 5649, 512, 5934, 10577, 300, 393, 854, 291, 483, 1409, 51854], "temperature": 0.0, "avg_logprob": -0.1278452180389665, "compression_ratio": 1.7949640287769784, "no_speech_prob": 0.0037068380042910576}, {"id": 108, "seek": 51652, "start": 516.52, "end": 520.72, "text": " if you would just like to use one of those for your review.", "tokens": [50364, 498, 291, 576, 445, 411, 281, 764, 472, 295, 729, 337, 428, 3131, 13, 50574], "temperature": 0.0, "avg_logprob": -0.11687373215297484, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.003272377885878086}, {"id": 109, "seek": 51652, "start": 520.72, "end": 528.24, "text": " In addition to the final project prizes, we'll also be awarding this year three lab prizes,", "tokens": [50574, 682, 4500, 281, 264, 2572, 1716, 27350, 11, 321, 603, 611, 312, 7130, 278, 341, 1064, 1045, 2715, 27350, 11, 50950], "temperature": 0.0, "avg_logprob": -0.11687373215297484, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.003272377885878086}, {"id": 110, "seek": 51652, "start": 528.24, "end": 531.88, "text": " one associated to each of the software labs that students will complete.", "tokens": [50950, 472, 6615, 281, 1184, 295, 264, 4722, 20339, 300, 1731, 486, 3566, 13, 51132], "temperature": 0.0, "avg_logprob": -0.11687373215297484, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.003272377885878086}, {"id": 111, "seek": 51652, "start": 531.88, "end": 536.84, "text": " Again, completion of the software labs is not required for grade of this course, but", "tokens": [51132, 3764, 11, 19372, 295, 264, 4722, 20339, 307, 406, 4739, 337, 7204, 295, 341, 1164, 11, 457, 51380], "temperature": 0.0, "avg_logprob": -0.11687373215297484, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.003272377885878086}, {"id": 112, "seek": 51652, "start": 536.84, "end": 539.64, "text": " it will make you eligible for some of these cool prizes.", "tokens": [51380, 309, 486, 652, 291, 14728, 337, 512, 295, 613, 1627, 27350, 13, 51520], "temperature": 0.0, "avg_logprob": -0.11687373215297484, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.003272377885878086}, {"id": 113, "seek": 51652, "start": 539.64, "end": 545.84, "text": " So please, we encourage everyone to compete for these prizes and get the opportunity to", "tokens": [51520, 407, 1767, 11, 321, 5373, 1518, 281, 11831, 337, 613, 27350, 293, 483, 264, 2650, 281, 51830], "temperature": 0.0, "avg_logprob": -0.11687373215297484, "compression_ratio": 1.726235741444867, "no_speech_prob": 0.003272377885878086}, {"id": 114, "seek": 54584, "start": 545.84, "end": 548.9200000000001, "text": " win them all.", "tokens": [50364, 1942, 552, 439, 13, 50518], "temperature": 0.0, "avg_logprob": -0.18106034461488116, "compression_ratio": 1.7359307359307359, "no_speech_prob": 0.012814215384423733}, {"id": 115, "seek": 54584, "start": 548.9200000000001, "end": 552.08, "text": " Please post to Piazza if you have any questions.", "tokens": [50518, 2555, 2183, 281, 430, 654, 26786, 498, 291, 362, 604, 1651, 13, 50676], "temperature": 0.0, "avg_logprob": -0.18106034461488116, "compression_ratio": 1.7359307359307359, "no_speech_prob": 0.012814215384423733}, {"id": 116, "seek": 54584, "start": 552.08, "end": 557.2, "text": " Visit the course website for announcements and digital recordings of the lectures, etc.", "tokens": [50676, 24548, 264, 1164, 3144, 337, 23785, 293, 4562, 25162, 295, 264, 16564, 11, 5183, 13, 50932], "temperature": 0.0, "avg_logprob": -0.18106034461488116, "compression_ratio": 1.7359307359307359, "no_speech_prob": 0.012814215384423733}, {"id": 117, "seek": 54584, "start": 557.2, "end": 560.84, "text": " And please email us if you have any questions.", "tokens": [50932, 400, 1767, 3796, 505, 498, 291, 362, 604, 1651, 13, 51114], "temperature": 0.0, "avg_logprob": -0.18106034461488116, "compression_ratio": 1.7359307359307359, "no_speech_prob": 0.012814215384423733}, {"id": 118, "seek": 54584, "start": 560.84, "end": 566.0400000000001, "text": " Also there are software labs and office hours right after each of these technical lectures", "tokens": [51114, 2743, 456, 366, 4722, 20339, 293, 3398, 2496, 558, 934, 1184, 295, 613, 6191, 16564, 51374], "temperature": 0.0, "avg_logprob": -0.18106034461488116, "compression_ratio": 1.7359307359307359, "no_speech_prob": 0.012814215384423733}, {"id": 119, "seek": 54584, "start": 566.0400000000001, "end": 568.08, "text": " held in Gather Town.", "tokens": [51374, 5167, 294, 39841, 15954, 13, 51476], "temperature": 0.0, "avg_logprob": -0.18106034461488116, "compression_ratio": 1.7359307359307359, "no_speech_prob": 0.012814215384423733}, {"id": 120, "seek": 54584, "start": 568.08, "end": 573.0400000000001, "text": " So please drop by in Gather Town to ask any questions about the software labs, specifically", "tokens": [51476, 407, 1767, 3270, 538, 294, 39841, 15954, 281, 1029, 604, 1651, 466, 264, 4722, 20339, 11, 4682, 51724], "temperature": 0.0, "avg_logprob": -0.18106034461488116, "compression_ratio": 1.7359307359307359, "no_speech_prob": 0.012814215384423733}, {"id": 121, "seek": 57304, "start": 573.04, "end": 577.3199999999999, "text": " on those, or more generally about past software labs or about the lecture that occurred that", "tokens": [50364, 322, 729, 11, 420, 544, 5101, 466, 1791, 4722, 20339, 420, 466, 264, 7991, 300, 11068, 300, 50578], "temperature": 0.0, "avg_logprob": -0.2088037066989475, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.020317263901233673}, {"id": 122, "seek": 57304, "start": 577.3199999999999, "end": 580.52, "text": " day.", "tokens": [50578, 786, 13, 50738], "temperature": 0.0, "avg_logprob": -0.2088037066989475, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.020317263901233673}, {"id": 123, "seek": 57304, "start": 580.52, "end": 588.1999999999999, "text": " Now this course has an incredible group of TAs and teaching assistants that you can reach", "tokens": [50738, 823, 341, 1164, 575, 364, 4651, 1594, 295, 314, 10884, 293, 4571, 34949, 300, 291, 393, 2524, 51122], "temperature": 0.0, "avg_logprob": -0.2088037066989475, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.020317263901233673}, {"id": 124, "seek": 57304, "start": 588.1999999999999, "end": 594.04, "text": " out to at any time in case you have any issues or questions about the material that you're", "tokens": [51122, 484, 281, 412, 604, 565, 294, 1389, 291, 362, 604, 2663, 420, 1651, 466, 264, 2527, 300, 291, 434, 51414], "temperature": 0.0, "avg_logprob": -0.2088037066989475, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.020317263901233673}, {"id": 125, "seek": 57304, "start": 594.04, "end": 596.04, "text": " learning.", "tokens": [51414, 2539, 13, 51514], "temperature": 0.0, "avg_logprob": -0.2088037066989475, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.020317263901233673}, {"id": 126, "seek": 57304, "start": 596.04, "end": 600.1999999999999, "text": " And finally, we want to give a huge thanks to all of our sponsors who, without their", "tokens": [51514, 400, 2721, 11, 321, 528, 281, 976, 257, 2603, 3231, 281, 439, 295, 527, 22593, 567, 11, 1553, 641, 51722], "temperature": 0.0, "avg_logprob": -0.2088037066989475, "compression_ratio": 1.6288209606986899, "no_speech_prob": 0.020317263901233673}, {"id": 127, "seek": 60020, "start": 600.24, "end": 603.32, "text": " help, this class would not be possible.", "tokens": [50366, 854, 11, 341, 1508, 576, 406, 312, 1944, 13, 50520], "temperature": 0.0, "avg_logprob": -0.14068668260486847, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.06000638008117676}, {"id": 128, "seek": 60020, "start": 603.32, "end": 607.88, "text": " This is the fourth year that we're teaching this class, and each year it just keeps getting", "tokens": [50520, 639, 307, 264, 6409, 1064, 300, 321, 434, 4571, 341, 1508, 11, 293, 1184, 1064, 309, 445, 5965, 1242, 50748], "temperature": 0.0, "avg_logprob": -0.14068668260486847, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.06000638008117676}, {"id": 129, "seek": 60020, "start": 607.88, "end": 612.08, "text": " bigger and bigger and bigger, and we really give a huge shout out to our sponsors for", "tokens": [50748, 3801, 293, 3801, 293, 3801, 11, 293, 321, 534, 976, 257, 2603, 8043, 484, 281, 527, 22593, 337, 50958], "temperature": 0.0, "avg_logprob": -0.14068668260486847, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.06000638008117676}, {"id": 130, "seek": 60020, "start": 612.08, "end": 615.4000000000001, "text": " helping us make this happen each year.", "tokens": [50958, 4315, 505, 652, 341, 1051, 1184, 1064, 13, 51124], "temperature": 0.0, "avg_logprob": -0.14068668260486847, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.06000638008117676}, {"id": 131, "seek": 60020, "start": 615.4000000000001, "end": 619.6, "text": " And especially this year in light of the virtual format.", "tokens": [51124, 400, 2318, 341, 1064, 294, 1442, 295, 264, 6374, 7877, 13, 51334], "temperature": 0.0, "avg_logprob": -0.14068668260486847, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.06000638008117676}, {"id": 132, "seek": 60020, "start": 619.6, "end": 622.08, "text": " So now let's start with the fun stuff.", "tokens": [51334, 407, 586, 718, 311, 722, 365, 264, 1019, 1507, 13, 51458], "temperature": 0.0, "avg_logprob": -0.14068668260486847, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.06000638008117676}, {"id": 133, "seek": 60020, "start": 622.08, "end": 627.8000000000001, "text": " Let's start by asking ourselves a question about why do we all care about deep learning?", "tokens": [51458, 961, 311, 722, 538, 3365, 4175, 257, 1168, 466, 983, 360, 321, 439, 1127, 466, 2452, 2539, 30, 51744], "temperature": 0.0, "avg_logprob": -0.14068668260486847, "compression_ratio": 1.6896551724137931, "no_speech_prob": 0.06000638008117676}, {"id": 134, "seek": 62780, "start": 627.8, "end": 632.12, "text": " And specifically, why do we care right now?", "tokens": [50364, 400, 4682, 11, 983, 360, 321, 1127, 558, 586, 30, 50580], "temperature": 0.0, "avg_logprob": -0.14926115337171053, "compression_ratio": 1.736, "no_speech_prob": 0.00100039376411587}, {"id": 135, "seek": 62780, "start": 632.12, "end": 638.28, "text": " To understand that, it's important to actually understand first why is deep learning or how", "tokens": [50580, 1407, 1223, 300, 11, 309, 311, 1021, 281, 767, 1223, 700, 983, 307, 2452, 2539, 420, 577, 50888], "temperature": 0.0, "avg_logprob": -0.14926115337171053, "compression_ratio": 1.736, "no_speech_prob": 0.00100039376411587}, {"id": 136, "seek": 62780, "start": 638.28, "end": 642.0, "text": " is deep learning different from traditional machine learning?", "tokens": [50888, 307, 2452, 2539, 819, 490, 5164, 3479, 2539, 30, 51074], "temperature": 0.0, "avg_logprob": -0.14926115337171053, "compression_ratio": 1.736, "no_speech_prob": 0.00100039376411587}, {"id": 137, "seek": 62780, "start": 642.0, "end": 647.88, "text": " Now traditionally, machine learning algorithms define a set of features in their data.", "tokens": [51074, 823, 19067, 11, 3479, 2539, 14642, 6964, 257, 992, 295, 4122, 294, 641, 1412, 13, 51368], "temperature": 0.0, "avg_logprob": -0.14926115337171053, "compression_ratio": 1.736, "no_speech_prob": 0.00100039376411587}, {"id": 138, "seek": 62780, "start": 647.88, "end": 653.4399999999999, "text": " Usually these are features that are handcrafted or hand engineered, and as a result they tend", "tokens": [51368, 11419, 613, 366, 4122, 300, 366, 1011, 5611, 292, 420, 1011, 38648, 11, 293, 382, 257, 1874, 436, 3928, 51646], "temperature": 0.0, "avg_logprob": -0.14926115337171053, "compression_ratio": 1.736, "no_speech_prob": 0.00100039376411587}, {"id": 139, "seek": 62780, "start": 653.4399999999999, "end": 656.92, "text": " to be pretty brittle in practice when they're deployed.", "tokens": [51646, 281, 312, 1238, 49325, 294, 3124, 562, 436, 434, 17826, 13, 51820], "temperature": 0.0, "avg_logprob": -0.14926115337171053, "compression_ratio": 1.736, "no_speech_prob": 0.00100039376411587}, {"id": 140, "seek": 65692, "start": 656.92, "end": 662.4399999999999, "text": " The key idea of deep learning is to learn these features directly from data in a hierarchical", "tokens": [50364, 440, 2141, 1558, 295, 2452, 2539, 307, 281, 1466, 613, 4122, 3838, 490, 1412, 294, 257, 35250, 804, 50640], "temperature": 0.0, "avg_logprob": -0.1376895697220512, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0006878007552586496}, {"id": 141, "seek": 65692, "start": 662.4399999999999, "end": 663.76, "text": " manner.", "tokens": [50640, 9060, 13, 50706], "temperature": 0.0, "avg_logprob": -0.1376895697220512, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0006878007552586496}, {"id": 142, "seek": 65692, "start": 663.76, "end": 668.64, "text": " That is, can we learn, if we want to learn how to detect a face, for example, can we", "tokens": [50706, 663, 307, 11, 393, 321, 1466, 11, 498, 321, 528, 281, 1466, 577, 281, 5531, 257, 1851, 11, 337, 1365, 11, 393, 321, 50950], "temperature": 0.0, "avg_logprob": -0.1376895697220512, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0006878007552586496}, {"id": 143, "seek": 65692, "start": 668.64, "end": 674.88, "text": " learn to first start by detecting edges in the image, composing these edges together", "tokens": [50950, 1466, 281, 700, 722, 538, 40237, 8819, 294, 264, 3256, 11, 715, 6110, 613, 8819, 1214, 51262], "temperature": 0.0, "avg_logprob": -0.1376895697220512, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0006878007552586496}, {"id": 144, "seek": 65692, "start": 674.88, "end": 680.76, "text": " to detect mid-level features such as a eye or a nose or a mouth, and then going deeper", "tokens": [51262, 281, 5531, 2062, 12, 12418, 4122, 1270, 382, 257, 3313, 420, 257, 6690, 420, 257, 4525, 11, 293, 550, 516, 7731, 51556], "temperature": 0.0, "avg_logprob": -0.1376895697220512, "compression_ratio": 1.7294685990338163, "no_speech_prob": 0.0006878007552586496}, {"id": 145, "seek": 68076, "start": 680.76, "end": 686.68, "text": " and composing these features into structural, facial features, so that we can recognize", "tokens": [50364, 293, 715, 6110, 613, 4122, 666, 15067, 11, 15642, 4122, 11, 370, 300, 321, 393, 5521, 50660], "temperature": 0.0, "avg_logprob": -0.16963974122078188, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007120635360479355}, {"id": 146, "seek": 68076, "start": 686.68, "end": 689.4399999999999, "text": " this face.", "tokens": [50660, 341, 1851, 13, 50798], "temperature": 0.0, "avg_logprob": -0.16963974122078188, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007120635360479355}, {"id": 147, "seek": 68076, "start": 689.4399999999999, "end": 694.28, "text": " This hierarchical way of thinking is really core to deep learning as core to everything", "tokens": [50798, 639, 35250, 804, 636, 295, 1953, 307, 534, 4965, 281, 2452, 2539, 382, 4965, 281, 1203, 51040], "temperature": 0.0, "avg_logprob": -0.16963974122078188, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007120635360479355}, {"id": 148, "seek": 68076, "start": 694.28, "end": 697.88, "text": " that we're going to learn in this class.", "tokens": [51040, 300, 321, 434, 516, 281, 1466, 294, 341, 1508, 13, 51220], "temperature": 0.0, "avg_logprob": -0.16963974122078188, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007120635360479355}, {"id": 149, "seek": 68076, "start": 697.88, "end": 703.48, "text": " Actually the fundamental building blocks though of deep learning and neural networks have", "tokens": [51220, 5135, 264, 8088, 2390, 8474, 1673, 295, 2452, 2539, 293, 18161, 9590, 362, 51500], "temperature": 0.0, "avg_logprob": -0.16963974122078188, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007120635360479355}, {"id": 150, "seek": 68076, "start": 703.48, "end": 705.48, "text": " actually existed for decades.", "tokens": [51500, 767, 13135, 337, 7878, 13, 51600], "temperature": 0.0, "avg_logprob": -0.16963974122078188, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007120635360479355}, {"id": 151, "seek": 68076, "start": 705.48, "end": 709.92, "text": " So one interesting thing to consider is why are we studying this now?", "tokens": [51600, 407, 472, 1880, 551, 281, 1949, 307, 983, 366, 321, 7601, 341, 586, 30, 51822], "temperature": 0.0, "avg_logprob": -0.16963974122078188, "compression_ratio": 1.6951219512195121, "no_speech_prob": 0.007120635360479355}, {"id": 152, "seek": 70992, "start": 709.92, "end": 715.76, "text": " Now is an incredibly amazing time to study these algorithms, and for one reason is because", "tokens": [50364, 823, 307, 364, 6252, 2243, 565, 281, 2979, 613, 14642, 11, 293, 337, 472, 1778, 307, 570, 50656], "temperature": 0.0, "avg_logprob": -0.1439976742393092, "compression_ratio": 1.6718146718146718, "no_speech_prob": 0.004330884665250778}, {"id": 153, "seek": 70992, "start": 715.76, "end": 718.8, "text": " data has become much more pervasive.", "tokens": [50656, 1412, 575, 1813, 709, 544, 680, 39211, 13, 50808], "temperature": 0.0, "avg_logprob": -0.1439976742393092, "compression_ratio": 1.6718146718146718, "no_speech_prob": 0.004330884665250778}, {"id": 154, "seek": 70992, "start": 718.8, "end": 724.64, "text": " These models are extremely hungry for data, and at the moment we're living in an era", "tokens": [50808, 1981, 5245, 366, 4664, 8067, 337, 1412, 11, 293, 412, 264, 1623, 321, 434, 2647, 294, 364, 4249, 51100], "temperature": 0.0, "avg_logprob": -0.1439976742393092, "compression_ratio": 1.6718146718146718, "no_speech_prob": 0.004330884665250778}, {"id": 155, "seek": 70992, "start": 724.64, "end": 727.4799999999999, "text": " where we have more data than ever before.", "tokens": [51100, 689, 321, 362, 544, 1412, 813, 1562, 949, 13, 51242], "temperature": 0.0, "avg_logprob": -0.1439976742393092, "compression_ratio": 1.6718146718146718, "no_speech_prob": 0.004330884665250778}, {"id": 156, "seek": 70992, "start": 727.4799999999999, "end": 733.16, "text": " Secondly, these algorithms are massively parallelizable, so they can benefit tremendously", "tokens": [51242, 19483, 11, 613, 14642, 366, 29379, 8952, 22395, 11, 370, 436, 393, 5121, 27985, 51526], "temperature": 0.0, "avg_logprob": -0.1439976742393092, "compression_ratio": 1.6718146718146718, "no_speech_prob": 0.004330884665250778}, {"id": 157, "seek": 70992, "start": 733.16, "end": 739.3199999999999, "text": " from modern GPU hardware that simply did not exist when these algorithms were developed.", "tokens": [51526, 490, 4363, 18407, 8837, 300, 2935, 630, 406, 2514, 562, 613, 14642, 645, 4743, 13, 51834], "temperature": 0.0, "avg_logprob": -0.1439976742393092, "compression_ratio": 1.6718146718146718, "no_speech_prob": 0.004330884665250778}, {"id": 158, "seek": 73932, "start": 739.32, "end": 744.36, "text": " And finally, due to open source toolboxes like TensorFlow, building and deploying these", "tokens": [50364, 400, 2721, 11, 3462, 281, 1269, 4009, 44593, 279, 411, 37624, 11, 2390, 293, 34198, 613, 50616], "temperature": 0.0, "avg_logprob": -0.13325016315166766, "compression_ratio": 1.5875, "no_speech_prob": 0.0018100370652973652}, {"id": 159, "seek": 73932, "start": 744.36, "end": 750.2, "text": " models has become extremely streamlined.", "tokens": [50616, 5245, 575, 1813, 4664, 48155, 13, 50908], "temperature": 0.0, "avg_logprob": -0.13325016315166766, "compression_ratio": 1.5875, "no_speech_prob": 0.0018100370652973652}, {"id": 160, "seek": 73932, "start": 750.2, "end": 755.1800000000001, "text": " So let's start actually with the fundamental building block of deep learning and of every", "tokens": [50908, 407, 718, 311, 722, 767, 365, 264, 8088, 2390, 3461, 295, 2452, 2539, 293, 295, 633, 51157], "temperature": 0.0, "avg_logprob": -0.13325016315166766, "compression_ratio": 1.5875, "no_speech_prob": 0.0018100370652973652}, {"id": 161, "seek": 73932, "start": 755.1800000000001, "end": 756.62, "text": " neural network.", "tokens": [51157, 18161, 3209, 13, 51229], "temperature": 0.0, "avg_logprob": -0.13325016315166766, "compression_ratio": 1.5875, "no_speech_prob": 0.0018100370652973652}, {"id": 162, "seek": 73932, "start": 756.62, "end": 760.7600000000001, "text": " That is just a single neuron, also known as a perceptron.", "tokens": [51229, 663, 307, 445, 257, 2167, 34090, 11, 611, 2570, 382, 257, 43276, 2044, 13, 51436], "temperature": 0.0, "avg_logprob": -0.13325016315166766, "compression_ratio": 1.5875, "no_speech_prob": 0.0018100370652973652}, {"id": 163, "seek": 73932, "start": 760.7600000000001, "end": 765.5200000000001, "text": " So we're going to walk through exactly what is a perceptron, how it's defined, and we're", "tokens": [51436, 407, 321, 434, 516, 281, 1792, 807, 2293, 437, 307, 257, 43276, 2044, 11, 577, 309, 311, 7642, 11, 293, 321, 434, 51674], "temperature": 0.0, "avg_logprob": -0.13325016315166766, "compression_ratio": 1.5875, "no_speech_prob": 0.0018100370652973652}, {"id": 164, "seek": 76552, "start": 765.52, "end": 769.92, "text": " going to build our way up to deeper neural networks all the way from there.", "tokens": [50364, 516, 281, 1322, 527, 636, 493, 281, 7731, 18161, 9590, 439, 264, 636, 490, 456, 13, 50584], "temperature": 0.0, "avg_logprob": -0.12362868969257061, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.01032636221498251}, {"id": 165, "seek": 76552, "start": 769.92, "end": 774.4, "text": " So let's start really at the basic building block.", "tokens": [50584, 407, 718, 311, 722, 534, 412, 264, 3875, 2390, 3461, 13, 50808], "temperature": 0.0, "avg_logprob": -0.12362868969257061, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.01032636221498251}, {"id": 166, "seek": 76552, "start": 774.4, "end": 779.0, "text": " The idea of a perceptron or a single neuron is actually very simple.", "tokens": [50808, 440, 1558, 295, 257, 43276, 2044, 420, 257, 2167, 34090, 307, 767, 588, 2199, 13, 51038], "temperature": 0.0, "avg_logprob": -0.12362868969257061, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.01032636221498251}, {"id": 167, "seek": 76552, "start": 779.0, "end": 783.72, "text": " So I think it's really important for all of you to understand this at its core.", "tokens": [51038, 407, 286, 519, 309, 311, 534, 1021, 337, 439, 295, 291, 281, 1223, 341, 412, 1080, 4965, 13, 51274], "temperature": 0.0, "avg_logprob": -0.12362868969257061, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.01032636221498251}, {"id": 168, "seek": 76552, "start": 783.72, "end": 788.1999999999999, "text": " Let's start by actually talking about the forward propagation of information through", "tokens": [51274, 961, 311, 722, 538, 767, 1417, 466, 264, 2128, 38377, 295, 1589, 807, 51498], "temperature": 0.0, "avg_logprob": -0.12362868969257061, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.01032636221498251}, {"id": 169, "seek": 76552, "start": 788.1999999999999, "end": 790.28, "text": " this single neuron.", "tokens": [51498, 341, 2167, 34090, 13, 51602], "temperature": 0.0, "avg_logprob": -0.12362868969257061, "compression_ratio": 1.704035874439462, "no_speech_prob": 0.01032636221498251}, {"id": 170, "seek": 79028, "start": 790.28, "end": 796.8399999999999, "text": " We can define a set of inputs xi through xm, which you can see on the left-hand side,", "tokens": [50364, 492, 393, 6964, 257, 992, 295, 15743, 36800, 807, 2031, 76, 11, 597, 291, 393, 536, 322, 264, 1411, 12, 5543, 1252, 11, 50692], "temperature": 0.0, "avg_logprob": -0.1361015802142264, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0021826280280947685}, {"id": 171, "seek": 79028, "start": 796.8399999999999, "end": 801.8399999999999, "text": " and each of these inputs or each of these numbers are multiplied by their corresponding", "tokens": [50692, 293, 1184, 295, 613, 15743, 420, 1184, 295, 613, 3547, 366, 17207, 538, 641, 11760, 50942], "temperature": 0.0, "avg_logprob": -0.1361015802142264, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0021826280280947685}, {"id": 172, "seek": 79028, "start": 801.8399999999999, "end": 805.04, "text": " weight and then added together.", "tokens": [50942, 3364, 293, 550, 3869, 1214, 13, 51102], "temperature": 0.0, "avg_logprob": -0.1361015802142264, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0021826280280947685}, {"id": 173, "seek": 79028, "start": 805.04, "end": 810.24, "text": " We take this single number, the result of that addition, and pass it through what's", "tokens": [51102, 492, 747, 341, 2167, 1230, 11, 264, 1874, 295, 300, 4500, 11, 293, 1320, 309, 807, 437, 311, 51362], "temperature": 0.0, "avg_logprob": -0.1361015802142264, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0021826280280947685}, {"id": 174, "seek": 79028, "start": 810.24, "end": 817.68, "text": " called a nonlinear activation function to produce our final output y.", "tokens": [51362, 1219, 257, 2107, 28263, 24433, 2445, 281, 5258, 527, 2572, 5598, 288, 13, 51734], "temperature": 0.0, "avg_logprob": -0.1361015802142264, "compression_ratio": 1.646788990825688, "no_speech_prob": 0.0021826280280947685}, {"id": 175, "seek": 81768, "start": 817.68, "end": 822.4, "text": " We can actually, actually this is not entirely correct because one thing I forgot to mention", "tokens": [50364, 492, 393, 767, 11, 767, 341, 307, 406, 7696, 3006, 570, 472, 551, 286, 5298, 281, 2152, 50600], "temperature": 0.0, "avg_logprob": -0.1328811061625578, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.015422413125634193}, {"id": 176, "seek": 81768, "start": 822.4, "end": 827.1999999999999, "text": " is that we also have what's called a bias term in here, which allows you to shift your", "tokens": [50600, 307, 300, 321, 611, 362, 437, 311, 1219, 257, 12577, 1433, 294, 510, 11, 597, 4045, 291, 281, 5513, 428, 50840], "temperature": 0.0, "avg_logprob": -0.1328811061625578, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.015422413125634193}, {"id": 177, "seek": 81768, "start": 827.1999999999999, "end": 830.16, "text": " activation function left or right.", "tokens": [50840, 24433, 2445, 1411, 420, 558, 13, 50988], "temperature": 0.0, "avg_logprob": -0.1328811061625578, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.015422413125634193}, {"id": 178, "seek": 81768, "start": 830.16, "end": 835.56, "text": " Now on the right-hand side of this diagram, you can actually see this concept illustrated", "tokens": [50988, 823, 322, 264, 558, 12, 5543, 1252, 295, 341, 10686, 11, 291, 393, 767, 536, 341, 3410, 33875, 51258], "temperature": 0.0, "avg_logprob": -0.1328811061625578, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.015422413125634193}, {"id": 179, "seek": 81768, "start": 835.56, "end": 839.2399999999999, "text": " or written out mathematically as a single equation.", "tokens": [51258, 420, 3720, 484, 44003, 382, 257, 2167, 5367, 13, 51442], "temperature": 0.0, "avg_logprob": -0.1328811061625578, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.015422413125634193}, {"id": 180, "seek": 81768, "start": 839.2399999999999, "end": 844.76, "text": " You can actually rewrite this in terms of linear algebra matrix multiplications and", "tokens": [51442, 509, 393, 767, 28132, 341, 294, 2115, 295, 8213, 21989, 8141, 17596, 763, 293, 51718], "temperature": 0.0, "avg_logprob": -0.1328811061625578, "compression_ratio": 1.685823754789272, "no_speech_prob": 0.015422413125634193}, {"id": 181, "seek": 84476, "start": 844.76, "end": 850.84, "text": " dot products to represent this a bit more concisely.", "tokens": [50364, 5893, 3383, 281, 2906, 341, 257, 857, 544, 1588, 271, 736, 13, 50668], "temperature": 0.0, "avg_logprob": -0.13071433357570483, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.014501879923045635}, {"id": 182, "seek": 84476, "start": 850.84, "end": 853.12, "text": " So let's do that.", "tokens": [50668, 407, 718, 311, 360, 300, 13, 50782], "temperature": 0.0, "avg_logprob": -0.13071433357570483, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.014501879923045635}, {"id": 183, "seek": 84476, "start": 853.12, "end": 859.2, "text": " Let's now do that with x, capital X, which is a vector of our inputs, x1 through xm,", "tokens": [50782, 961, 311, 586, 360, 300, 365, 2031, 11, 4238, 1783, 11, 597, 307, 257, 8062, 295, 527, 15743, 11, 2031, 16, 807, 2031, 76, 11, 51086], "temperature": 0.0, "avg_logprob": -0.13071433357570483, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.014501879923045635}, {"id": 184, "seek": 84476, "start": 859.2, "end": 864.04, "text": " and capital W, which is a vector of our weights, w1 through wm.", "tokens": [51086, 293, 4238, 343, 11, 597, 307, 257, 8062, 295, 527, 17443, 11, 261, 16, 807, 261, 76, 13, 51328], "temperature": 0.0, "avg_logprob": -0.13071433357570483, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.014501879923045635}, {"id": 185, "seek": 84476, "start": 864.04, "end": 870.28, "text": " So each of these are vectors of length m, and the output is very simply obtained by taking", "tokens": [51328, 407, 1184, 295, 613, 366, 18875, 295, 4641, 275, 11, 293, 264, 5598, 307, 588, 2935, 14879, 538, 1940, 51640], "temperature": 0.0, "avg_logprob": -0.13071433357570483, "compression_ratio": 1.6230366492146597, "no_speech_prob": 0.014501879923045635}, {"id": 186, "seek": 87028, "start": 870.28, "end": 879.28, "text": " their dot product, adding a bias, which in this case is w0, and then applying a nonlinearity,", "tokens": [50364, 641, 5893, 1674, 11, 5127, 257, 12577, 11, 597, 294, 341, 1389, 307, 261, 15, 11, 293, 550, 9275, 257, 2107, 1889, 17409, 11, 50814], "temperature": 0.0, "avg_logprob": -0.1714371767911044, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.010327034629881382}, {"id": 187, "seek": 87028, "start": 879.28, "end": 881.88, "text": " g.", "tokens": [50814, 290, 13, 50944], "temperature": 0.0, "avg_logprob": -0.1714371767911044, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.010327034629881382}, {"id": 188, "seek": 87028, "start": 881.88, "end": 887.0799999999999, "text": " One thing is that I haven't, I've been mentioning it a couple of times, this nonlinearity, g.", "tokens": [50944, 1485, 551, 307, 300, 286, 2378, 380, 11, 286, 600, 668, 18315, 309, 257, 1916, 295, 1413, 11, 341, 2107, 1889, 17409, 11, 290, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1714371767911044, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.010327034629881382}, {"id": 189, "seek": 87028, "start": 887.0799999999999, "end": 889.28, "text": " What exactly is it?", "tokens": [51204, 708, 2293, 307, 309, 30, 51314], "temperature": 0.0, "avg_logprob": -0.1714371767911044, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.010327034629881382}, {"id": 190, "seek": 87028, "start": 889.28, "end": 893.56, "text": " Because I've mentioned it now a couple of times, well, it is a nonlinear function.", "tokens": [51314, 1436, 286, 600, 2835, 309, 586, 257, 1916, 295, 1413, 11, 731, 11, 309, 307, 257, 2107, 28263, 2445, 13, 51528], "temperature": 0.0, "avg_logprob": -0.1714371767911044, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.010327034629881382}, {"id": 191, "seek": 87028, "start": 893.56, "end": 900.16, "text": " One common example of this nonlinear activation function is what is known as the sigmoid function.", "tokens": [51528, 1485, 2689, 1365, 295, 341, 2107, 28263, 24433, 2445, 307, 437, 307, 2570, 382, 264, 4556, 3280, 327, 2445, 13, 51858], "temperature": 0.0, "avg_logprob": -0.1714371767911044, "compression_ratio": 1.757847533632287, "no_speech_prob": 0.010327034629881382}, {"id": 192, "seek": 90016, "start": 900.24, "end": 902.76, "text": " Defined here on the right.", "tokens": [50368, 9548, 2001, 510, 322, 264, 558, 13, 50494], "temperature": 0.0, "avg_logprob": -0.1670155317887016, "compression_ratio": 1.628, "no_speech_prob": 0.0046088118106126785}, {"id": 193, "seek": 90016, "start": 902.76, "end": 906.12, "text": " In fact, there are many types of nonlinear functions.", "tokens": [50494, 682, 1186, 11, 456, 366, 867, 3467, 295, 2107, 28263, 6828, 13, 50662], "temperature": 0.0, "avg_logprob": -0.1670155317887016, "compression_ratio": 1.628, "no_speech_prob": 0.0046088118106126785}, {"id": 194, "seek": 90016, "start": 906.12, "end": 910.56, "text": " You can see three more examples here, including the sigmoid function.", "tokens": [50662, 509, 393, 536, 1045, 544, 5110, 510, 11, 3009, 264, 4556, 3280, 327, 2445, 13, 50884], "temperature": 0.0, "avg_logprob": -0.1670155317887016, "compression_ratio": 1.628, "no_speech_prob": 0.0046088118106126785}, {"id": 195, "seek": 90016, "start": 910.56, "end": 915.6, "text": " And throughout this presentation, you'll actually see these TensorFlow code blocks,", "tokens": [50884, 400, 3710, 341, 5860, 11, 291, 603, 767, 536, 613, 37624, 3089, 8474, 11, 51136], "temperature": 0.0, "avg_logprob": -0.1670155317887016, "compression_ratio": 1.628, "no_speech_prob": 0.0046088118106126785}, {"id": 196, "seek": 90016, "start": 915.6, "end": 920.1999999999999, "text": " which will actually illustrate how we can take some of the topics that we're learning", "tokens": [51136, 597, 486, 767, 23221, 577, 321, 393, 747, 512, 295, 264, 8378, 300, 321, 434, 2539, 51366], "temperature": 0.0, "avg_logprob": -0.1670155317887016, "compression_ratio": 1.628, "no_speech_prob": 0.0046088118106126785}, {"id": 197, "seek": 90016, "start": 920.1999999999999, "end": 927.0799999999999, "text": " in this class and actually practically use them using the TensorFlow software library.", "tokens": [51366, 294, 341, 1508, 293, 767, 15667, 764, 552, 1228, 264, 37624, 4722, 6405, 13, 51710], "temperature": 0.0, "avg_logprob": -0.1670155317887016, "compression_ratio": 1.628, "no_speech_prob": 0.0046088118106126785}, {"id": 198, "seek": 92708, "start": 927.5200000000001, "end": 931.9200000000001, "text": " Now the sigmoid activation function, which I presented on the previous slide, is very", "tokens": [50386, 823, 264, 4556, 3280, 327, 24433, 2445, 11, 597, 286, 8212, 322, 264, 3894, 4137, 11, 307, 588, 50606], "temperature": 0.0, "avg_logprob": -0.14132847235752985, "compression_ratio": 1.8073770491803278, "no_speech_prob": 0.00034597920603118837}, {"id": 199, "seek": 92708, "start": 931.9200000000001, "end": 934.4000000000001, "text": " popular since it's a function that gives outputs.", "tokens": [50606, 3743, 1670, 309, 311, 257, 2445, 300, 2709, 23930, 13, 50730], "temperature": 0.0, "avg_logprob": -0.14132847235752985, "compression_ratio": 1.8073770491803278, "no_speech_prob": 0.00034597920603118837}, {"id": 200, "seek": 92708, "start": 934.4000000000001, "end": 940.88, "text": " It takes as input any real number, any activation value, and it outputs a number always between", "tokens": [50730, 467, 2516, 382, 4846, 604, 957, 1230, 11, 604, 24433, 2158, 11, 293, 309, 23930, 257, 1230, 1009, 1296, 51054], "temperature": 0.0, "avg_logprob": -0.14132847235752985, "compression_ratio": 1.8073770491803278, "no_speech_prob": 0.00034597920603118837}, {"id": 201, "seek": 92708, "start": 940.88, "end": 942.64, "text": " zero and one.", "tokens": [51054, 4018, 293, 472, 13, 51142], "temperature": 0.0, "avg_logprob": -0.14132847235752985, "compression_ratio": 1.8073770491803278, "no_speech_prob": 0.00034597920603118837}, {"id": 202, "seek": 92708, "start": 942.64, "end": 947.0, "text": " So this makes it really, really suitable for problems and probability, because probabilities", "tokens": [51142, 407, 341, 1669, 309, 534, 11, 534, 12873, 337, 2740, 293, 8482, 11, 570, 33783, 51360], "temperature": 0.0, "avg_logprob": -0.14132847235752985, "compression_ratio": 1.8073770491803278, "no_speech_prob": 0.00034597920603118837}, {"id": 203, "seek": 92708, "start": 947.0, "end": 949.1600000000001, "text": " also have to be between zero and one.", "tokens": [51360, 611, 362, 281, 312, 1296, 4018, 293, 472, 13, 51468], "temperature": 0.0, "avg_logprob": -0.14132847235752985, "compression_ratio": 1.8073770491803278, "no_speech_prob": 0.00034597920603118837}, {"id": 204, "seek": 92708, "start": 949.1600000000001, "end": 952.9200000000001, "text": " So this makes them very well suited for those types of problems.", "tokens": [51468, 407, 341, 1669, 552, 588, 731, 24736, 337, 729, 3467, 295, 2740, 13, 51656], "temperature": 0.0, "avg_logprob": -0.14132847235752985, "compression_ratio": 1.8073770491803278, "no_speech_prob": 0.00034597920603118837}, {"id": 205, "seek": 95292, "start": 952.92, "end": 957.4, "text": " In modern deep neural networks, the ReLU activation function, which you can see on the", "tokens": [50364, 682, 4363, 2452, 18161, 9590, 11, 264, 1300, 43, 52, 24433, 2445, 11, 597, 291, 393, 536, 322, 264, 50588], "temperature": 0.0, "avg_logprob": -0.14967875679334006, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.00045828131260350347}, {"id": 206, "seek": 95292, "start": 957.4, "end": 961.4399999999999, "text": " right, is also extremely popular because of its simplicity.", "tokens": [50588, 558, 11, 307, 611, 4664, 3743, 570, 295, 1080, 25632, 13, 50790], "temperature": 0.0, "avg_logprob": -0.14967875679334006, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.00045828131260350347}, {"id": 207, "seek": 95292, "start": 961.4399999999999, "end": 964.28, "text": " In this case, it's a piecewise linear function.", "tokens": [50790, 682, 341, 1389, 11, 309, 311, 257, 2522, 3711, 8213, 2445, 13, 50932], "temperature": 0.0, "avg_logprob": -0.14967875679334006, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.00045828131260350347}, {"id": 208, "seek": 95292, "start": 964.28, "end": 971.16, "text": " It is zero before when it's in the negative regime, and it is strictly the identity function", "tokens": [50932, 467, 307, 4018, 949, 562, 309, 311, 294, 264, 3671, 13120, 11, 293, 309, 307, 20792, 264, 6575, 2445, 51276], "temperature": 0.0, "avg_logprob": -0.14967875679334006, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.00045828131260350347}, {"id": 209, "seek": 95292, "start": 971.16, "end": 973.5999999999999, "text": " in the positive regime.", "tokens": [51276, 294, 264, 3353, 13120, 13, 51398], "temperature": 0.0, "avg_logprob": -0.14967875679334006, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.00045828131260350347}, {"id": 210, "seek": 95292, "start": 973.5999999999999, "end": 980.28, "text": " But one really important question that I hope that you're asking yourselves right now is", "tokens": [51398, 583, 472, 534, 1021, 1168, 300, 286, 1454, 300, 291, 434, 3365, 14791, 558, 586, 307, 51732], "temperature": 0.0, "avg_logprob": -0.14967875679334006, "compression_ratio": 1.606425702811245, "no_speech_prob": 0.00045828131260350347}, {"id": 211, "seek": 98028, "start": 980.3199999999999, "end": 983.9599999999999, "text": " why do we even need activation functions?", "tokens": [50366, 983, 360, 321, 754, 643, 24433, 6828, 30, 50548], "temperature": 0.0, "avg_logprob": -0.1419827959953098, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.002889270894229412}, {"id": 212, "seek": 98028, "start": 983.9599999999999, "end": 989.4399999999999, "text": " I think actually throughout this course, I do want to say that no matter what I say in", "tokens": [50548, 286, 519, 767, 3710, 341, 1164, 11, 286, 360, 528, 281, 584, 300, 572, 1871, 437, 286, 584, 294, 50822], "temperature": 0.0, "avg_logprob": -0.1419827959953098, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.002889270894229412}, {"id": 213, "seek": 98028, "start": 989.4399999999999, "end": 994.64, "text": " the course, I hope that always you're questioning why this is a necessary step and why do we", "tokens": [50822, 264, 1164, 11, 286, 1454, 300, 1009, 291, 434, 21257, 983, 341, 307, 257, 4818, 1823, 293, 983, 360, 321, 51082], "temperature": 0.0, "avg_logprob": -0.1419827959953098, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.002889270894229412}, {"id": 214, "seek": 98028, "start": 994.64, "end": 998.24, "text": " need each of these steps, because often these are the questions that can lead to really", "tokens": [51082, 643, 1184, 295, 613, 4439, 11, 570, 2049, 613, 366, 264, 1651, 300, 393, 1477, 281, 534, 51262], "temperature": 0.0, "avg_logprob": -0.1419827959953098, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.002889270894229412}, {"id": 215, "seek": 98028, "start": 998.24, "end": 1000.72, "text": " amazing research breakthroughs.", "tokens": [51262, 2243, 2132, 22397, 82, 13, 51386], "temperature": 0.0, "avg_logprob": -0.1419827959953098, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.002889270894229412}, {"id": 216, "seek": 98028, "start": 1000.72, "end": 1003.48, "text": " So why do we need activation functions?", "tokens": [51386, 407, 983, 360, 321, 643, 24433, 6828, 30, 51524], "temperature": 0.0, "avg_logprob": -0.1419827959953098, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.002889270894229412}, {"id": 217, "seek": 98028, "start": 1003.48, "end": 1008.04, "text": " Now the point of an activation function is to actually introduce non-linearities into", "tokens": [51524, 823, 264, 935, 295, 364, 24433, 2445, 307, 281, 767, 5366, 2107, 12, 28263, 1088, 666, 51752], "temperature": 0.0, "avg_logprob": -0.1419827959953098, "compression_ratio": 1.8531746031746033, "no_speech_prob": 0.002889270894229412}, {"id": 218, "seek": 100804, "start": 1008.0799999999999, "end": 1011.68, "text": " our network, because these are non-linear functions.", "tokens": [50366, 527, 3209, 11, 570, 613, 366, 2107, 12, 28263, 6828, 13, 50546], "temperature": 0.0, "avg_logprob": -0.10816295124660029, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.003823808627203107}, {"id": 219, "seek": 100804, "start": 1011.68, "end": 1015.92, "text": " And it allows us to actually deal with non-linear data.", "tokens": [50546, 400, 309, 4045, 505, 281, 767, 2028, 365, 2107, 12, 28263, 1412, 13, 50758], "temperature": 0.0, "avg_logprob": -0.10816295124660029, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.003823808627203107}, {"id": 220, "seek": 100804, "start": 1015.92, "end": 1021.12, "text": " This is extremely important in real life, especially because in the real world, data", "tokens": [50758, 639, 307, 4664, 1021, 294, 957, 993, 11, 2318, 570, 294, 264, 957, 1002, 11, 1412, 51018], "temperature": 0.0, "avg_logprob": -0.10816295124660029, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.003823808627203107}, {"id": 221, "seek": 100804, "start": 1021.12, "end": 1023.8399999999999, "text": " is almost always non-linear.", "tokens": [51018, 307, 1920, 1009, 2107, 12, 28263, 13, 51154], "temperature": 0.0, "avg_logprob": -0.10816295124660029, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.003823808627203107}, {"id": 222, "seek": 100804, "start": 1023.8399999999999, "end": 1027.6399999999999, "text": " Imagine I told you to separate here the green points from the red points, but all you could", "tokens": [51154, 11739, 286, 1907, 291, 281, 4994, 510, 264, 3092, 2793, 490, 264, 2182, 2793, 11, 457, 439, 291, 727, 51344], "temperature": 0.0, "avg_logprob": -0.10816295124660029, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.003823808627203107}, {"id": 223, "seek": 100804, "start": 1027.6399999999999, "end": 1030.28, "text": " use is a single straight line.", "tokens": [51344, 764, 307, 257, 2167, 2997, 1622, 13, 51476], "temperature": 0.0, "avg_logprob": -0.10816295124660029, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.003823808627203107}, {"id": 224, "seek": 100804, "start": 1030.28, "end": 1035.48, "text": " You might think this is easy with multiple lines or curved lines, but you can only use", "tokens": [51476, 509, 1062, 519, 341, 307, 1858, 365, 3866, 3876, 420, 24991, 3876, 11, 457, 291, 393, 787, 764, 51736], "temperature": 0.0, "avg_logprob": -0.10816295124660029, "compression_ratio": 1.7075098814229248, "no_speech_prob": 0.003823808627203107}, {"id": 225, "seek": 103548, "start": 1035.48, "end": 1038.0, "text": " a single straight line.", "tokens": [50364, 257, 2167, 2997, 1622, 13, 50490], "temperature": 0.0, "avg_logprob": -0.12000871908785117, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0019265147857367992}, {"id": 226, "seek": 103548, "start": 1038.0, "end": 1042.04, "text": " And that's what using a neural network with a linear activation function would be like.", "tokens": [50490, 400, 300, 311, 437, 1228, 257, 18161, 3209, 365, 257, 8213, 24433, 2445, 576, 312, 411, 13, 50692], "temperature": 0.0, "avg_logprob": -0.12000871908785117, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0019265147857367992}, {"id": 227, "seek": 103548, "start": 1042.04, "end": 1046.84, "text": " That makes the problem really hard, because no matter how deep the neural network is,", "tokens": [50692, 663, 1669, 264, 1154, 534, 1152, 11, 570, 572, 1871, 577, 2452, 264, 18161, 3209, 307, 11, 50932], "temperature": 0.0, "avg_logprob": -0.12000871908785117, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0019265147857367992}, {"id": 228, "seek": 103548, "start": 1046.84, "end": 1052.3600000000001, "text": " you'll only be able to produce a single line decision boundary, and you're only able to", "tokens": [50932, 291, 603, 787, 312, 1075, 281, 5258, 257, 2167, 1622, 3537, 12866, 11, 293, 291, 434, 787, 1075, 281, 51208], "temperature": 0.0, "avg_logprob": -0.12000871908785117, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0019265147857367992}, {"id": 229, "seek": 103548, "start": 1052.3600000000001, "end": 1056.08, "text": " separate your space with one line.", "tokens": [51208, 4994, 428, 1901, 365, 472, 1622, 13, 51394], "temperature": 0.0, "avg_logprob": -0.12000871908785117, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0019265147857367992}, {"id": 230, "seek": 103548, "start": 1056.08, "end": 1062.08, "text": " Now using non-linear activation functions allows your neural network to approximate arbitrarily", "tokens": [51394, 823, 1228, 2107, 12, 28263, 24433, 6828, 4045, 428, 18161, 3209, 281, 30874, 19071, 3289, 51694], "temperature": 0.0, "avg_logprob": -0.12000871908785117, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0019265147857367992}, {"id": 231, "seek": 103548, "start": 1062.08, "end": 1063.84, "text": " complex functions.", "tokens": [51694, 3997, 6828, 13, 51782], "temperature": 0.0, "avg_logprob": -0.12000871908785117, "compression_ratio": 1.7682926829268293, "no_speech_prob": 0.0019265147857367992}, {"id": 232, "seek": 106384, "start": 1063.84, "end": 1069.1999999999998, "text": " And that's what makes neural networks extraordinarily powerful.", "tokens": [50364, 400, 300, 311, 437, 1669, 18161, 9590, 34557, 4005, 13, 50632], "temperature": 0.0, "avg_logprob": -0.15606039821511447, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0003053372201975435}, {"id": 233, "seek": 106384, "start": 1069.1999999999998, "end": 1073.52, "text": " Let's understand this with a simple example so that we can build up our intuition even", "tokens": [50632, 961, 311, 1223, 341, 365, 257, 2199, 1365, 370, 300, 321, 393, 1322, 493, 527, 24002, 754, 50848], "temperature": 0.0, "avg_logprob": -0.15606039821511447, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0003053372201975435}, {"id": 234, "seek": 106384, "start": 1073.52, "end": 1075.12, "text": " further.", "tokens": [50848, 3052, 13, 50928], "temperature": 0.0, "avg_logprob": -0.15606039821511447, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0003053372201975435}, {"id": 235, "seek": 106384, "start": 1075.12, "end": 1081.12, "text": " Imagine I give you this trained network, now with weights on the left hand side, 3 and", "tokens": [50928, 11739, 286, 976, 291, 341, 8895, 3209, 11, 586, 365, 17443, 322, 264, 1411, 1011, 1252, 11, 805, 293, 51228], "temperature": 0.0, "avg_logprob": -0.15606039821511447, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0003053372201975435}, {"id": 236, "seek": 106384, "start": 1081.12, "end": 1083.12, "text": " negative 2.", "tokens": [51228, 3671, 568, 13, 51328], "temperature": 0.0, "avg_logprob": -0.15606039821511447, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0003053372201975435}, {"id": 237, "seek": 106384, "start": 1083.12, "end": 1087.04, "text": " This network only has two inputs, x1 and x2.", "tokens": [51328, 639, 3209, 787, 575, 732, 15743, 11, 2031, 16, 293, 2031, 17, 13, 51524], "temperature": 0.0, "avg_logprob": -0.15606039821511447, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0003053372201975435}, {"id": 238, "seek": 106384, "start": 1087.04, "end": 1091.9199999999998, "text": " If we want to get the output of it, we simply do the same story as I said before.", "tokens": [51524, 759, 321, 528, 281, 483, 264, 5598, 295, 309, 11, 321, 2935, 360, 264, 912, 1657, 382, 286, 848, 949, 13, 51768], "temperature": 0.0, "avg_logprob": -0.15606039821511447, "compression_ratio": 1.565040650406504, "no_speech_prob": 0.0003053372201975435}, {"id": 239, "seek": 109192, "start": 1092.28, "end": 1098.4, "text": " First take a dot product of our inputs with our weights, add the bias, and apply a non-linearity.", "tokens": [50382, 2386, 747, 257, 5893, 1674, 295, 527, 15743, 365, 527, 17443, 11, 909, 264, 12577, 11, 293, 3079, 257, 2107, 12, 1889, 17409, 13, 50688], "temperature": 0.0, "avg_logprob": -0.15841039021809897, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.003706961637362838}, {"id": 240, "seek": 109192, "start": 1098.4, "end": 1103.76, "text": " But let's take a look at what's inside of that non-linearity.", "tokens": [50688, 583, 718, 311, 747, 257, 574, 412, 437, 311, 1854, 295, 300, 2107, 12, 1889, 17409, 13, 50956], "temperature": 0.0, "avg_logprob": -0.15841039021809897, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.003706961637362838}, {"id": 241, "seek": 109192, "start": 1103.76, "end": 1111.48, "text": " It's simply a weighted combination of our inputs in the form of a two dimensional line,", "tokens": [50956, 467, 311, 2935, 257, 32807, 6562, 295, 527, 15743, 294, 264, 1254, 295, 257, 732, 18795, 1622, 11, 51342], "temperature": 0.0, "avg_logprob": -0.15841039021809897, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.003706961637362838}, {"id": 242, "seek": 109192, "start": 1111.48, "end": 1114.68, "text": " because in this case we only have two inputs.", "tokens": [51342, 570, 294, 341, 1389, 321, 787, 362, 732, 15743, 13, 51502], "temperature": 0.0, "avg_logprob": -0.15841039021809897, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.003706961637362838}, {"id": 243, "seek": 109192, "start": 1114.68, "end": 1119.3600000000001, "text": " So if we want to compute this output, it's the same stories before, we take a dot product", "tokens": [51502, 407, 498, 321, 528, 281, 14722, 341, 5598, 11, 309, 311, 264, 912, 3676, 949, 11, 321, 747, 257, 5893, 1674, 51736], "temperature": 0.0, "avg_logprob": -0.15841039021809897, "compression_ratio": 1.7330316742081449, "no_speech_prob": 0.003706961637362838}, {"id": 244, "seek": 111936, "start": 1119.36, "end": 1123.6399999999999, "text": " of x and w, we add our bias, and apply our non-linearity.", "tokens": [50364, 295, 2031, 293, 261, 11, 321, 909, 527, 12577, 11, 293, 3079, 527, 2107, 12, 1889, 17409, 13, 50578], "temperature": 0.0, "avg_logprob": -0.16485972595214843, "compression_ratio": 1.7591836734693878, "no_speech_prob": 0.0015977504663169384}, {"id": 245, "seek": 111936, "start": 1123.6399999999999, "end": 1127.12, "text": " What about what's inside of this non-linearity g?", "tokens": [50578, 708, 466, 437, 311, 1854, 295, 341, 2107, 12, 1889, 17409, 290, 30, 50752], "temperature": 0.0, "avg_logprob": -0.16485972595214843, "compression_ratio": 1.7591836734693878, "no_speech_prob": 0.0015977504663169384}, {"id": 246, "seek": 111936, "start": 1127.12, "end": 1131.04, "text": " Well, this is just a 2D line.", "tokens": [50752, 1042, 11, 341, 307, 445, 257, 568, 35, 1622, 13, 50948], "temperature": 0.0, "avg_logprob": -0.16485972595214843, "compression_ratio": 1.7591836734693878, "no_speech_prob": 0.0015977504663169384}, {"id": 247, "seek": 111936, "start": 1131.04, "end": 1135.56, "text": " In fact, since it's just a two dimensional line, we can even plot it in two dimensional", "tokens": [50948, 682, 1186, 11, 1670, 309, 311, 445, 257, 732, 18795, 1622, 11, 321, 393, 754, 7542, 309, 294, 732, 18795, 51174], "temperature": 0.0, "avg_logprob": -0.16485972595214843, "compression_ratio": 1.7591836734693878, "no_speech_prob": 0.0015977504663169384}, {"id": 248, "seek": 111936, "start": 1135.56, "end": 1136.56, "text": " space.", "tokens": [51174, 1901, 13, 51224], "temperature": 0.0, "avg_logprob": -0.16485972595214843, "compression_ratio": 1.7591836734693878, "no_speech_prob": 0.0015977504663169384}, {"id": 249, "seek": 111936, "start": 1136.56, "end": 1138.3999999999999, "text": " This is called the feature space, the input space.", "tokens": [51224, 639, 307, 1219, 264, 4111, 1901, 11, 264, 4846, 1901, 13, 51316], "temperature": 0.0, "avg_logprob": -0.16485972595214843, "compression_ratio": 1.7591836734693878, "no_speech_prob": 0.0015977504663169384}, {"id": 250, "seek": 111936, "start": 1138.3999999999999, "end": 1144.1999999999998, "text": " In this case, the feature space and the input space are equal because we only have one neuron.", "tokens": [51316, 682, 341, 1389, 11, 264, 4111, 1901, 293, 264, 4846, 1901, 366, 2681, 570, 321, 787, 362, 472, 34090, 13, 51606], "temperature": 0.0, "avg_logprob": -0.16485972595214843, "compression_ratio": 1.7591836734693878, "no_speech_prob": 0.0015977504663169384}, {"id": 251, "seek": 111936, "start": 1144.1999999999998, "end": 1147.1599999999999, "text": " So in this plot, let me describe what you're seeing.", "tokens": [51606, 407, 294, 341, 7542, 11, 718, 385, 6786, 437, 291, 434, 2577, 13, 51754], "temperature": 0.0, "avg_logprob": -0.16485972595214843, "compression_ratio": 1.7591836734693878, "no_speech_prob": 0.0015977504663169384}, {"id": 252, "seek": 114716, "start": 1147.16, "end": 1150.44, "text": " So on the two axes, you're seeing our two inputs.", "tokens": [50364, 407, 322, 264, 732, 35387, 11, 291, 434, 2577, 527, 732, 15743, 13, 50528], "temperature": 0.0, "avg_logprob": -0.14644406846732147, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0035932916216552258}, {"id": 253, "seek": 114716, "start": 1150.44, "end": 1155.64, "text": " So on one axis is x1, one of the inputs, on the other axis is x2, our other input.", "tokens": [50528, 407, 322, 472, 10298, 307, 2031, 16, 11, 472, 295, 264, 15743, 11, 322, 264, 661, 10298, 307, 2031, 17, 11, 527, 661, 4846, 13, 50788], "temperature": 0.0, "avg_logprob": -0.14644406846732147, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0035932916216552258}, {"id": 254, "seek": 114716, "start": 1155.64, "end": 1160.1200000000001, "text": " And we can plot the line here, our decision boundary of this trained neural network that", "tokens": [50788, 400, 321, 393, 7542, 264, 1622, 510, 11, 527, 3537, 12866, 295, 341, 8895, 18161, 3209, 300, 51012], "temperature": 0.0, "avg_logprob": -0.14644406846732147, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0035932916216552258}, {"id": 255, "seek": 114716, "start": 1160.1200000000001, "end": 1163.92, "text": " I gave you, as a line in this space.", "tokens": [51012, 286, 2729, 291, 11, 382, 257, 1622, 294, 341, 1901, 13, 51202], "temperature": 0.0, "avg_logprob": -0.14644406846732147, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0035932916216552258}, {"id": 256, "seek": 114716, "start": 1163.92, "end": 1168.0800000000002, "text": " Now this line corresponds to actually all of the decisions that this neural network", "tokens": [51202, 823, 341, 1622, 23249, 281, 767, 439, 295, 264, 5327, 300, 341, 18161, 3209, 51410], "temperature": 0.0, "avg_logprob": -0.14644406846732147, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0035932916216552258}, {"id": 257, "seek": 114716, "start": 1168.0800000000002, "end": 1169.96, "text": " can make.", "tokens": [51410, 393, 652, 13, 51504], "temperature": 0.0, "avg_logprob": -0.14644406846732147, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0035932916216552258}, {"id": 258, "seek": 114716, "start": 1169.96, "end": 1175.6000000000001, "text": " Because if I give you a new data point, for example here I'm giving you negative 1, 2,", "tokens": [51504, 1436, 498, 286, 976, 291, 257, 777, 1412, 935, 11, 337, 1365, 510, 286, 478, 2902, 291, 3671, 502, 11, 568, 11, 51786], "temperature": 0.0, "avg_logprob": -0.14644406846732147, "compression_ratio": 1.742063492063492, "no_speech_prob": 0.0035932916216552258}, {"id": 259, "seek": 117560, "start": 1175.6, "end": 1181.08, "text": " this point lies somewhere in the space, specifically at x1 equal to negative 1 and x2 equal to", "tokens": [50364, 341, 935, 9134, 4079, 294, 264, 1901, 11, 4682, 412, 2031, 16, 2681, 281, 3671, 502, 293, 2031, 17, 2681, 281, 50638], "temperature": 0.0, "avg_logprob": -0.15857427687872025, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.01032672543078661}, {"id": 260, "seek": 117560, "start": 1181.08, "end": 1182.08, "text": " 2.", "tokens": [50638, 568, 13, 50688], "temperature": 0.0, "avg_logprob": -0.15857427687872025, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.01032672543078661}, {"id": 261, "seek": 117560, "start": 1182.08, "end": 1184.28, "text": " That's just a point in the space.", "tokens": [50688, 663, 311, 445, 257, 935, 294, 264, 1901, 13, 50798], "temperature": 0.0, "avg_logprob": -0.15857427687872025, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.01032672543078661}, {"id": 262, "seek": 117560, "start": 1184.28, "end": 1191.6, "text": " I want you to compute its weighted combination and I can actually follow the perceptron equation", "tokens": [50798, 286, 528, 291, 281, 14722, 1080, 32807, 6562, 293, 286, 393, 767, 1524, 264, 43276, 2044, 5367, 51164], "temperature": 0.0, "avg_logprob": -0.15857427687872025, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.01032672543078661}, {"id": 263, "seek": 117560, "start": 1191.6, "end": 1193.56, "text": " to get the answer.", "tokens": [51164, 281, 483, 264, 1867, 13, 51262], "temperature": 0.0, "avg_logprob": -0.15857427687872025, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.01032672543078661}, {"id": 264, "seek": 117560, "start": 1193.56, "end": 1199.56, "text": " So here we can see that if we plug it into the perceptron equation, we get 1 plus minus", "tokens": [51262, 407, 510, 321, 393, 536, 300, 498, 321, 5452, 309, 666, 264, 43276, 2044, 5367, 11, 321, 483, 502, 1804, 3175, 51562], "temperature": 0.0, "avg_logprob": -0.15857427687872025, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.01032672543078661}, {"id": 265, "seek": 117560, "start": 1199.56, "end": 1202.24, "text": " 3 minus 4.", "tokens": [51562, 805, 3175, 1017, 13, 51696], "temperature": 0.0, "avg_logprob": -0.15857427687872025, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.01032672543078661}, {"id": 266, "seek": 117560, "start": 1202.24, "end": 1204.56, "text": " And the result would be minus 6.", "tokens": [51696, 400, 264, 1874, 576, 312, 3175, 1386, 13, 51812], "temperature": 0.0, "avg_logprob": -0.15857427687872025, "compression_ratio": 1.669603524229075, "no_speech_prob": 0.01032672543078661}, {"id": 267, "seek": 120456, "start": 1204.56, "end": 1213.3999999999999, "text": " We plug that into our nonlinear activation function g and we get a final output of 0.002.", "tokens": [50364, 492, 5452, 300, 666, 527, 2107, 28263, 24433, 2445, 290, 293, 321, 483, 257, 2572, 5598, 295, 1958, 13, 628, 17, 13, 50806], "temperature": 0.0, "avg_logprob": -0.15473135506234517, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0005883779376745224}, {"id": 268, "seek": 120456, "start": 1213.3999999999999, "end": 1219.52, "text": " Now in fact, remember that the sigmoid function actually divides this space into two parts", "tokens": [50806, 823, 294, 1186, 11, 1604, 300, 264, 4556, 3280, 327, 2445, 767, 41347, 341, 1901, 666, 732, 3166, 51112], "temperature": 0.0, "avg_logprob": -0.15473135506234517, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0005883779376745224}, {"id": 269, "seek": 120456, "start": 1219.52, "end": 1223.52, "text": " of either because it outputs everything between 0 and 1.", "tokens": [51112, 295, 2139, 570, 309, 23930, 1203, 1296, 1958, 293, 502, 13, 51312], "temperature": 0.0, "avg_logprob": -0.15473135506234517, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0005883779376745224}, {"id": 270, "seek": 120456, "start": 1223.52, "end": 1231.76, "text": " It's dividing it between 0.5 and greater than 0.5 and less than 0.5.", "tokens": [51312, 467, 311, 26764, 309, 1296, 1958, 13, 20, 293, 5044, 813, 1958, 13, 20, 293, 1570, 813, 1958, 13, 20, 13, 51724], "temperature": 0.0, "avg_logprob": -0.15473135506234517, "compression_ratio": 1.5854922279792747, "no_speech_prob": 0.0005883779376745224}, {"id": 271, "seek": 123176, "start": 1231.76, "end": 1239.72, "text": " When the input is less than 0 and greater than 0.5, that's when the input is positive.", "tokens": [50364, 1133, 264, 4846, 307, 1570, 813, 1958, 293, 5044, 813, 1958, 13, 20, 11, 300, 311, 562, 264, 4846, 307, 3353, 13, 50762], "temperature": 0.0, "avg_logprob": -0.12516434649203687, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.00317243835888803}, {"id": 272, "seek": 123176, "start": 1239.72, "end": 1243.84, "text": " We can illustrate this space actually, but this feature space, when we're dealing with", "tokens": [50762, 492, 393, 23221, 341, 1901, 767, 11, 457, 341, 4111, 1901, 11, 562, 321, 434, 6260, 365, 50968], "temperature": 0.0, "avg_logprob": -0.12516434649203687, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.00317243835888803}, {"id": 273, "seek": 123176, "start": 1243.84, "end": 1249.4, "text": " a small dimensional data, like in this case we only have two dimensions.", "tokens": [50968, 257, 1359, 18795, 1412, 11, 411, 294, 341, 1389, 321, 787, 362, 732, 12819, 13, 51246], "temperature": 0.0, "avg_logprob": -0.12516434649203687, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.00317243835888803}, {"id": 274, "seek": 123176, "start": 1249.4, "end": 1253.8799999999999, "text": " But soon we'll start to talk about problems where we have thousands or millions or in", "tokens": [51246, 583, 2321, 321, 603, 722, 281, 751, 466, 2740, 689, 321, 362, 5383, 420, 6803, 420, 294, 51470], "temperature": 0.0, "avg_logprob": -0.12516434649203687, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.00317243835888803}, {"id": 275, "seek": 123176, "start": 1253.8799999999999, "end": 1258.84, "text": " some cases even billions of weights in our neural network.", "tokens": [51470, 512, 3331, 754, 17375, 295, 17443, 294, 527, 18161, 3209, 13, 51718], "temperature": 0.0, "avg_logprob": -0.12516434649203687, "compression_ratio": 1.6853448275862069, "no_speech_prob": 0.00317243835888803}, {"id": 276, "seek": 125884, "start": 1258.84, "end": 1264.8799999999999, "text": " And then drawing these types of plots becomes extremely challenging and not really possible", "tokens": [50364, 400, 550, 6316, 613, 3467, 295, 28609, 3643, 4664, 7595, 293, 406, 534, 1944, 50666], "temperature": 0.0, "avg_logprob": -0.12316960151042414, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0100121283903718}, {"id": 277, "seek": 125884, "start": 1264.8799999999999, "end": 1265.8799999999999, "text": " anymore.", "tokens": [50666, 3602, 13, 50716], "temperature": 0.0, "avg_logprob": -0.12316960151042414, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0100121283903718}, {"id": 278, "seek": 125884, "start": 1265.8799999999999, "end": 1269.9199999999998, "text": " But at least when we're in this regime of small number of inputs and small number of", "tokens": [50716, 583, 412, 1935, 562, 321, 434, 294, 341, 13120, 295, 1359, 1230, 295, 15743, 293, 1359, 1230, 295, 50918], "temperature": 0.0, "avg_logprob": -0.12316960151042414, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0100121283903718}, {"id": 279, "seek": 125884, "start": 1269.9199999999998, "end": 1273.9599999999998, "text": " weights, we can make these plots to really understand the entire space.", "tokens": [50918, 17443, 11, 321, 393, 652, 613, 28609, 281, 534, 1223, 264, 2302, 1901, 13, 51120], "temperature": 0.0, "avg_logprob": -0.12316960151042414, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0100121283903718}, {"id": 280, "seek": 125884, "start": 1273.9599999999998, "end": 1279.52, "text": " And for any new input that we obtain, for example an input right here, we can see exactly", "tokens": [51120, 400, 337, 604, 777, 4846, 300, 321, 12701, 11, 337, 1365, 364, 4846, 558, 510, 11, 321, 393, 536, 2293, 51398], "temperature": 0.0, "avg_logprob": -0.12316960151042414, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0100121283903718}, {"id": 281, "seek": 125884, "start": 1279.52, "end": 1287.04, "text": " that this point is going to be having an activation function less than 0 and its output will be", "tokens": [51398, 300, 341, 935, 307, 516, 281, 312, 1419, 364, 24433, 2445, 1570, 813, 1958, 293, 1080, 5598, 486, 312, 51774], "temperature": 0.0, "avg_logprob": -0.12316960151042414, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0100121283903718}, {"id": 282, "seek": 125884, "start": 1287.04, "end": 1288.72, "text": " less than 0.5.", "tokens": [51774, 1570, 813, 1958, 13, 20, 13, 51858], "temperature": 0.0, "avg_logprob": -0.12316960151042414, "compression_ratio": 1.741444866920152, "no_speech_prob": 0.0100121283903718}, {"id": 283, "seek": 128872, "start": 1288.72, "end": 1293.32, "text": " The magnitude of that actually is computed by plugging it into the perceptron equation.", "tokens": [50364, 440, 15668, 295, 300, 767, 307, 40610, 538, 42975, 309, 666, 264, 43276, 2044, 5367, 13, 50594], "temperature": 0.0, "avg_logprob": -0.10200665060397798, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0005192785756662488}, {"id": 284, "seek": 128872, "start": 1293.32, "end": 1297.8, "text": " So we can't avoid that, but we can immediately get an answer on the decision boundary, depending", "tokens": [50594, 407, 321, 393, 380, 5042, 300, 11, 457, 321, 393, 4258, 483, 364, 1867, 322, 264, 3537, 12866, 11, 5413, 50818], "temperature": 0.0, "avg_logprob": -0.10200665060397798, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0005192785756662488}, {"id": 285, "seek": 128872, "start": 1297.8, "end": 1304.48, "text": " on which side of this hyperplane that we lie on when we plug it in.", "tokens": [50818, 322, 597, 1252, 295, 341, 9848, 36390, 300, 321, 4544, 322, 562, 321, 5452, 309, 294, 13, 51152], "temperature": 0.0, "avg_logprob": -0.10200665060397798, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0005192785756662488}, {"id": 286, "seek": 128872, "start": 1304.48, "end": 1309.44, "text": " So now that we have an idea of how to build a perceptron, let's start by building neural", "tokens": [51152, 407, 586, 300, 321, 362, 364, 1558, 295, 577, 281, 1322, 257, 43276, 2044, 11, 718, 311, 722, 538, 2390, 18161, 51400], "temperature": 0.0, "avg_logprob": -0.10200665060397798, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0005192785756662488}, {"id": 287, "seek": 128872, "start": 1309.44, "end": 1313.96, "text": " networks and seeing how they all come together.", "tokens": [51400, 9590, 293, 2577, 577, 436, 439, 808, 1214, 13, 51626], "temperature": 0.0, "avg_logprob": -0.10200665060397798, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0005192785756662488}, {"id": 288, "seek": 128872, "start": 1313.96, "end": 1318.18, "text": " So let's revisit that diagram of the perceptron that I showed you before.", "tokens": [51626, 407, 718, 311, 32676, 300, 10686, 295, 264, 43276, 2044, 300, 286, 4712, 291, 949, 13, 51837], "temperature": 0.0, "avg_logprob": -0.10200665060397798, "compression_ratio": 1.7148148148148148, "no_speech_prob": 0.0005192785756662488}, {"id": 289, "seek": 131818, "start": 1318.18, "end": 1322.74, "text": " If there's only a few things that you get from this class, I really want everyone to", "tokens": [50364, 759, 456, 311, 787, 257, 1326, 721, 300, 291, 483, 490, 341, 1508, 11, 286, 534, 528, 1518, 281, 50592], "temperature": 0.0, "avg_logprob": -0.15739636807828336, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004305430338717997}, {"id": 290, "seek": 131818, "start": 1322.74, "end": 1325.54, "text": " take away how a perceptron works.", "tokens": [50592, 747, 1314, 577, 257, 43276, 2044, 1985, 13, 50732], "temperature": 0.0, "avg_logprob": -0.15739636807828336, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004305430338717997}, {"id": 291, "seek": 131818, "start": 1325.54, "end": 1327.9, "text": " And there's three steps, remember them always.", "tokens": [50732, 400, 456, 311, 1045, 4439, 11, 1604, 552, 1009, 13, 50850], "temperature": 0.0, "avg_logprob": -0.15739636807828336, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004305430338717997}, {"id": 292, "seek": 131818, "start": 1327.9, "end": 1332.98, "text": " The dot product, you take a dot product of your inputs and your weights.", "tokens": [50850, 440, 5893, 1674, 11, 291, 747, 257, 5893, 1674, 295, 428, 15743, 293, 428, 17443, 13, 51104], "temperature": 0.0, "avg_logprob": -0.15739636807828336, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004305430338717997}, {"id": 293, "seek": 131818, "start": 1332.98, "end": 1336.7, "text": " You add a bias and you apply your non-linearity.", "tokens": [51104, 509, 909, 257, 12577, 293, 291, 3079, 428, 2107, 12, 1889, 17409, 13, 51290], "temperature": 0.0, "avg_logprob": -0.15739636807828336, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004305430338717997}, {"id": 294, "seek": 131818, "start": 1336.7, "end": 1339.66, "text": " There's three steps.", "tokens": [51290, 821, 311, 1045, 4439, 13, 51438], "temperature": 0.0, "avg_logprob": -0.15739636807828336, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004305430338717997}, {"id": 295, "seek": 131818, "start": 1339.66, "end": 1341.94, "text": " Let's simplify this diagram a little bit.", "tokens": [51438, 961, 311, 20460, 341, 10686, 257, 707, 857, 13, 51552], "temperature": 0.0, "avg_logprob": -0.15739636807828336, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004305430338717997}, {"id": 296, "seek": 131818, "start": 1341.94, "end": 1345.0600000000002, "text": " Let's clean up some of the arrows and remove the bias.", "tokens": [51552, 961, 311, 2541, 493, 512, 295, 264, 19669, 293, 4159, 264, 12577, 13, 51708], "temperature": 0.0, "avg_logprob": -0.15739636807828336, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.0004305430338717997}, {"id": 297, "seek": 134506, "start": 1345.06, "end": 1350.5, "text": " And we can actually see now that every line here has its own associated weight to it.", "tokens": [50364, 400, 321, 393, 767, 536, 586, 300, 633, 1622, 510, 575, 1080, 1065, 6615, 3364, 281, 309, 13, 50636], "temperature": 0.0, "avg_logprob": -0.15625443750498247, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0004442016943357885}, {"id": 298, "seek": 134506, "start": 1350.5, "end": 1354.1, "text": " And I'll remove the bias term, like I said, for simplicity.", "tokens": [50636, 400, 286, 603, 4159, 264, 12577, 1433, 11, 411, 286, 848, 11, 337, 25632, 13, 50816], "temperature": 0.0, "avg_logprob": -0.15625443750498247, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0004442016943357885}, {"id": 299, "seek": 134506, "start": 1354.1, "end": 1360.74, "text": " Note that z here is the result of that dot product plus bias, before we apply the activation", "tokens": [50816, 11633, 300, 710, 510, 307, 264, 1874, 295, 300, 5893, 1674, 1804, 12577, 11, 949, 321, 3079, 264, 24433, 51148], "temperature": 0.0, "avg_logprob": -0.15625443750498247, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0004442016943357885}, {"id": 300, "seek": 134506, "start": 1360.74, "end": 1363.1399999999999, "text": " function, though, g.", "tokens": [51148, 2445, 11, 1673, 11, 290, 13, 51268], "temperature": 0.0, "avg_logprob": -0.15625443750498247, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0004442016943357885}, {"id": 301, "seek": 134506, "start": 1363.1399999999999, "end": 1368.6599999999999, "text": " The final output, though, is simply y, which is equal to the activation function of z, which", "tokens": [51268, 440, 2572, 5598, 11, 1673, 11, 307, 2935, 288, 11, 597, 307, 2681, 281, 264, 24433, 2445, 295, 710, 11, 597, 51544], "temperature": 0.0, "avg_logprob": -0.15625443750498247, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0004442016943357885}, {"id": 302, "seek": 134506, "start": 1368.6599999999999, "end": 1371.58, "text": " is our activation value.", "tokens": [51544, 307, 527, 24433, 2158, 13, 51690], "temperature": 0.0, "avg_logprob": -0.15625443750498247, "compression_ratio": 1.6535087719298245, "no_speech_prob": 0.0004442016943357885}, {"id": 303, "seek": 137158, "start": 1372.58, "end": 1378.1799999999998, "text": " Now, if we want to define a multi-output neural network, we can simply add another", "tokens": [50414, 823, 11, 498, 321, 528, 281, 6964, 257, 4825, 12, 346, 2582, 18161, 3209, 11, 321, 393, 2935, 909, 1071, 50694], "temperature": 0.0, "avg_logprob": -0.11986598790248978, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0004173003835603595}, {"id": 304, "seek": 137158, "start": 1378.1799999999998, "end": 1380.02, "text": " perceptron to this picture.", "tokens": [50694, 43276, 2044, 281, 341, 3036, 13, 50786], "temperature": 0.0, "avg_logprob": -0.11986598790248978, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0004173003835603595}, {"id": 305, "seek": 137158, "start": 1380.02, "end": 1384.54, "text": " So instead of having one perceptron, now we have two perceptrons and two outputs.", "tokens": [50786, 407, 2602, 295, 1419, 472, 43276, 2044, 11, 586, 321, 362, 732, 43276, 13270, 293, 732, 23930, 13, 51012], "temperature": 0.0, "avg_logprob": -0.11986598790248978, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0004173003835603595}, {"id": 306, "seek": 137158, "start": 1384.54, "end": 1389.86, "text": " Each one is a normal perceptron, exactly like we saw before, taking its inputs from each", "tokens": [51012, 6947, 472, 307, 257, 2710, 43276, 2044, 11, 2293, 411, 321, 1866, 949, 11, 1940, 1080, 15743, 490, 1184, 51278], "temperature": 0.0, "avg_logprob": -0.11986598790248978, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0004173003835603595}, {"id": 307, "seek": 137158, "start": 1389.86, "end": 1397.06, "text": " of the x1's through xm's, taking the dot product, adding a bias, and that's it.", "tokens": [51278, 295, 264, 2031, 16, 311, 807, 2031, 76, 311, 11, 1940, 264, 5893, 1674, 11, 5127, 257, 12577, 11, 293, 300, 311, 309, 13, 51638], "temperature": 0.0, "avg_logprob": -0.11986598790248978, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0004173003835603595}, {"id": 308, "seek": 137158, "start": 1397.06, "end": 1398.26, "text": " Now we have two outputs.", "tokens": [51638, 823, 321, 362, 732, 23930, 13, 51698], "temperature": 0.0, "avg_logprob": -0.11986598790248978, "compression_ratio": 1.670995670995671, "no_speech_prob": 0.0004173003835603595}, {"id": 309, "seek": 139826, "start": 1398.26, "end": 1401.94, "text": " Each of those perceptrons, though, will have a different set of weights.", "tokens": [50364, 6947, 295, 729, 43276, 13270, 11, 1673, 11, 486, 362, 257, 819, 992, 295, 17443, 13, 50548], "temperature": 0.0, "avg_logprob": -0.15506968366990395, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0011694554705172777}, {"id": 310, "seek": 139826, "start": 1401.94, "end": 1402.94, "text": " Remember that.", "tokens": [50548, 5459, 300, 13, 50598], "temperature": 0.0, "avg_logprob": -0.15506968366990395, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0011694554705172777}, {"id": 311, "seek": 139826, "start": 1402.94, "end": 1405.66, "text": " We'll get back to that.", "tokens": [50598, 492, 603, 483, 646, 281, 300, 13, 50734], "temperature": 0.0, "avg_logprob": -0.15506968366990395, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0011694554705172777}, {"id": 312, "seek": 139826, "start": 1405.66, "end": 1410.98, "text": " If we want, so actually one thing to keep in mind here is because all the inputs are", "tokens": [50734, 759, 321, 528, 11, 370, 767, 472, 551, 281, 1066, 294, 1575, 510, 307, 570, 439, 264, 15743, 366, 51000], "temperature": 0.0, "avg_logprob": -0.15506968366990395, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0011694554705172777}, {"id": 313, "seek": 139826, "start": 1410.98, "end": 1417.58, "text": " densely connected, every input has a connection to the weights of every perceptron.", "tokens": [51000, 24505, 736, 4582, 11, 633, 4846, 575, 257, 4984, 281, 264, 17443, 295, 633, 43276, 2044, 13, 51330], "temperature": 0.0, "avg_logprob": -0.15506968366990395, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0011694554705172777}, {"id": 314, "seek": 139826, "start": 1417.58, "end": 1422.14, "text": " These are often called dense layers, or sometimes fully connected layers.", "tokens": [51330, 1981, 366, 2049, 1219, 18011, 7914, 11, 420, 2171, 4498, 4582, 7914, 13, 51558], "temperature": 0.0, "avg_logprob": -0.15506968366990395, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0011694554705172777}, {"id": 315, "seek": 139826, "start": 1422.14, "end": 1427.54, "text": " Now, through this class, you're going to get a lot of experience actually coding up", "tokens": [51558, 823, 11, 807, 341, 1508, 11, 291, 434, 516, 281, 483, 257, 688, 295, 1752, 767, 17720, 493, 51828], "temperature": 0.0, "avg_logprob": -0.15506968366990395, "compression_ratio": 1.691119691119691, "no_speech_prob": 0.0011694554705172777}, {"id": 316, "seek": 142754, "start": 1427.58, "end": 1433.54, "text": " and practically creating some of these algorithms using a software toolbox called TensorFlow.", "tokens": [50366, 293, 15667, 4084, 512, 295, 613, 14642, 1228, 257, 4722, 44593, 1219, 37624, 13, 50664], "temperature": 0.0, "avg_logprob": -0.10435393421920304, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010986654087901115}, {"id": 317, "seek": 142754, "start": 1433.54, "end": 1439.02, "text": " So now that we have the understanding of how a single perceptron works and how a dense", "tokens": [50664, 407, 586, 300, 321, 362, 264, 3701, 295, 577, 257, 2167, 43276, 2044, 1985, 293, 577, 257, 18011, 50938], "temperature": 0.0, "avg_logprob": -0.10435393421920304, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010986654087901115}, {"id": 318, "seek": 142754, "start": 1439.02, "end": 1444.86, "text": " layer works, this is a stack of perceptrons, let's try and see how we can actually build", "tokens": [50938, 4583, 1985, 11, 341, 307, 257, 8630, 295, 43276, 13270, 11, 718, 311, 853, 293, 536, 577, 321, 393, 767, 1322, 51230], "temperature": 0.0, "avg_logprob": -0.10435393421920304, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010986654087901115}, {"id": 319, "seek": 142754, "start": 1444.86, "end": 1449.34, "text": " up a dense layer like this all the way from scratch.", "tokens": [51230, 493, 257, 18011, 4583, 411, 341, 439, 264, 636, 490, 8459, 13, 51454], "temperature": 0.0, "avg_logprob": -0.10435393421920304, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010986654087901115}, {"id": 320, "seek": 142754, "start": 1449.34, "end": 1454.58, "text": " To do that, we can actually start by initializing the two components of our dense layer, which", "tokens": [51454, 1407, 360, 300, 11, 321, 393, 767, 722, 538, 5883, 3319, 264, 732, 6677, 295, 527, 18011, 4583, 11, 597, 51716], "temperature": 0.0, "avg_logprob": -0.10435393421920304, "compression_ratio": 1.7160493827160495, "no_speech_prob": 0.0010986654087901115}, {"id": 321, "seek": 145458, "start": 1454.62, "end": 1457.62, "text": " are the weights and the biases.", "tokens": [50366, 366, 264, 17443, 293, 264, 32152, 13, 50516], "temperature": 0.0, "avg_logprob": -0.1115566400381235, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00047283709864132106}, {"id": 322, "seek": 145458, "start": 1457.62, "end": 1462.5, "text": " Now that we have these two parameters of our neural network, of our dense layer, we can", "tokens": [50516, 823, 300, 321, 362, 613, 732, 9834, 295, 527, 18161, 3209, 11, 295, 527, 18011, 4583, 11, 321, 393, 50760], "temperature": 0.0, "avg_logprob": -0.1115566400381235, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00047283709864132106}, {"id": 323, "seek": 145458, "start": 1462.5, "end": 1466.9399999999998, "text": " actually define the forward propagation of information, just like we saw it and learned", "tokens": [50760, 767, 6964, 264, 2128, 38377, 295, 1589, 11, 445, 411, 321, 1866, 309, 293, 3264, 50982], "temperature": 0.0, "avg_logprob": -0.1115566400381235, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00047283709864132106}, {"id": 324, "seek": 145458, "start": 1466.9399999999998, "end": 1468.5, "text": " about already.", "tokens": [50982, 466, 1217, 13, 51060], "temperature": 0.0, "avg_logprob": -0.1115566400381235, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00047283709864132106}, {"id": 325, "seek": 145458, "start": 1468.5, "end": 1473.54, "text": " That forward propagation of information is simply the dot product or the matrix multiplication", "tokens": [51060, 663, 2128, 38377, 295, 1589, 307, 2935, 264, 5893, 1674, 420, 264, 8141, 27290, 51312], "temperature": 0.0, "avg_logprob": -0.1115566400381235, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00047283709864132106}, {"id": 326, "seek": 145458, "start": 1473.54, "end": 1480.54, "text": " of our inputs with our weights, add a bias, that gives us our activation function here,", "tokens": [51312, 295, 527, 15743, 365, 527, 17443, 11, 909, 257, 12577, 11, 300, 2709, 505, 527, 24433, 2445, 510, 11, 51662], "temperature": 0.0, "avg_logprob": -0.1115566400381235, "compression_ratio": 1.7608695652173914, "no_speech_prob": 0.00047283709864132106}, {"id": 327, "seek": 148054, "start": 1481.3799999999999, "end": 1486.3799999999999, "text": " and then we apply this nonlinearity to compute the output.", "tokens": [50406, 293, 550, 321, 3079, 341, 2107, 1889, 17409, 281, 14722, 264, 5598, 13, 50656], "temperature": 0.0, "avg_logprob": -0.14997553056286228, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0011335172457620502}, {"id": 328, "seek": 148054, "start": 1486.3799999999999, "end": 1492.3799999999999, "text": " Now, TensorFlow has actually implemented this dense layer for us.", "tokens": [50656, 823, 11, 37624, 575, 767, 12270, 341, 18011, 4583, 337, 505, 13, 50956], "temperature": 0.0, "avg_logprob": -0.14997553056286228, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0011335172457620502}, {"id": 329, "seek": 148054, "start": 1492.3799999999999, "end": 1497.1399999999999, "text": " So we don't need to do that from scratch, instead we can just call it like shown here.", "tokens": [50956, 407, 321, 500, 380, 643, 281, 360, 300, 490, 8459, 11, 2602, 321, 393, 445, 818, 309, 411, 4898, 510, 13, 51194], "temperature": 0.0, "avg_logprob": -0.14997553056286228, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0011335172457620502}, {"id": 330, "seek": 148054, "start": 1497.1399999999999, "end": 1504.1399999999999, "text": " So to create a dense layer with two outputs, we can specify this units equal to two.", "tokens": [51194, 407, 281, 1884, 257, 18011, 4583, 365, 732, 23930, 11, 321, 393, 16500, 341, 6815, 2681, 281, 732, 13, 51544], "temperature": 0.0, "avg_logprob": -0.14997553056286228, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0011335172457620502}, {"id": 331, "seek": 148054, "start": 1504.1399999999999, "end": 1508.3799999999999, "text": " Now let's take a look at what's called a single layered neural network.", "tokens": [51544, 823, 718, 311, 747, 257, 574, 412, 437, 311, 1219, 257, 2167, 34666, 18161, 3209, 13, 51756], "temperature": 0.0, "avg_logprob": -0.14997553056286228, "compression_ratio": 1.6211453744493391, "no_speech_prob": 0.0011335172457620502}, {"id": 332, "seek": 150838, "start": 1508.38, "end": 1513.8600000000001, "text": " This is one we have a single hidden layer between our inputs and our outputs.", "tokens": [50364, 639, 307, 472, 321, 362, 257, 2167, 7633, 4583, 1296, 527, 15743, 293, 527, 23930, 13, 50638], "temperature": 0.0, "avg_logprob": -0.12275022930569118, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0008829996222630143}, {"id": 333, "seek": 150838, "start": 1513.8600000000001, "end": 1519.14, "text": " This layer is called the hidden layer, because unlike an input layer and an output layer,", "tokens": [50638, 639, 4583, 307, 1219, 264, 7633, 4583, 11, 570, 8343, 364, 4846, 4583, 293, 364, 5598, 4583, 11, 50902], "temperature": 0.0, "avg_logprob": -0.12275022930569118, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0008829996222630143}, {"id": 334, "seek": 150838, "start": 1519.14, "end": 1524.2600000000002, "text": " the states of this hidden layer are typically unobserved, they're hidden to some extent,", "tokens": [50902, 264, 4368, 295, 341, 7633, 4583, 366, 5850, 8526, 929, 6913, 11, 436, 434, 7633, 281, 512, 8396, 11, 51158], "temperature": 0.0, "avg_logprob": -0.12275022930569118, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0008829996222630143}, {"id": 335, "seek": 150838, "start": 1524.2600000000002, "end": 1526.8200000000002, "text": " they're not strictly enforced either.", "tokens": [51158, 436, 434, 406, 20792, 40953, 2139, 13, 51286], "temperature": 0.0, "avg_logprob": -0.12275022930569118, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0008829996222630143}, {"id": 336, "seek": 150838, "start": 1526.8200000000002, "end": 1531.0600000000002, "text": " And since we have this transformation now from the input layer to the hidden layer and", "tokens": [51286, 400, 1670, 321, 362, 341, 9887, 586, 490, 264, 4846, 4583, 281, 264, 7633, 4583, 293, 51498], "temperature": 0.0, "avg_logprob": -0.12275022930569118, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0008829996222630143}, {"id": 337, "seek": 150838, "start": 1531.0600000000002, "end": 1536.66, "text": " from the hidden layer to the output layer, each of these layers are going to have their", "tokens": [51498, 490, 264, 7633, 4583, 281, 264, 5598, 4583, 11, 1184, 295, 613, 7914, 366, 516, 281, 362, 641, 51778], "temperature": 0.0, "avg_logprob": -0.12275022930569118, "compression_ratio": 1.995744680851064, "no_speech_prob": 0.0008829996222630143}, {"id": 338, "seek": 153666, "start": 1536.74, "end": 1539.7, "text": " own specified weight matrices.", "tokens": [50368, 1065, 22206, 3364, 32284, 13, 50516], "temperature": 0.0, "avg_logprob": -0.1349523425102234, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0006461701705120504}, {"id": 339, "seek": 153666, "start": 1539.7, "end": 1545.66, "text": " We'll call w1 the weight matrices for the first layer and w2 the weight matrix for the", "tokens": [50516, 492, 603, 818, 261, 16, 264, 3364, 32284, 337, 264, 700, 4583, 293, 261, 17, 264, 3364, 8141, 337, 264, 50814], "temperature": 0.0, "avg_logprob": -0.1349523425102234, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0006461701705120504}, {"id": 340, "seek": 153666, "start": 1545.66, "end": 1548.5800000000002, "text": " second layer.", "tokens": [50814, 1150, 4583, 13, 50960], "temperature": 0.0, "avg_logprob": -0.1349523425102234, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0006461701705120504}, {"id": 341, "seek": 153666, "start": 1548.5800000000002, "end": 1555.5800000000002, "text": " If we take a zoomed in look at one of the neurons in this hidden layer, let's take", "tokens": [50960, 759, 321, 747, 257, 8863, 292, 294, 574, 412, 472, 295, 264, 22027, 294, 341, 7633, 4583, 11, 718, 311, 747, 51310], "temperature": 0.0, "avg_logprob": -0.1349523425102234, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0006461701705120504}, {"id": 342, "seek": 153666, "start": 1555.5800000000002, "end": 1561.5400000000002, "text": " for example z2 for example, this is the exact same perceptron that we saw before.", "tokens": [51310, 337, 1365, 710, 17, 337, 1365, 11, 341, 307, 264, 1900, 912, 43276, 2044, 300, 321, 1866, 949, 13, 51608], "temperature": 0.0, "avg_logprob": -0.1349523425102234, "compression_ratio": 1.6444444444444444, "no_speech_prob": 0.0006461701705120504}, {"id": 343, "seek": 156154, "start": 1561.54, "end": 1567.7, "text": " We can compute its output, again using the exact same story, taking all of its inputs", "tokens": [50364, 492, 393, 14722, 1080, 5598, 11, 797, 1228, 264, 1900, 912, 1657, 11, 1940, 439, 295, 1080, 15743, 50672], "temperature": 0.0, "avg_logprob": -0.11879930288895317, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.010985329747200012}, {"id": 344, "seek": 156154, "start": 1567.7, "end": 1572.82, "text": " x1 through xm, applying a dot product with the weights, adding a bias and that gives", "tokens": [50672, 2031, 16, 807, 2031, 76, 11, 9275, 257, 5893, 1674, 365, 264, 17443, 11, 5127, 257, 12577, 293, 300, 2709, 50928], "temperature": 0.0, "avg_logprob": -0.11879930288895317, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.010985329747200012}, {"id": 345, "seek": 156154, "start": 1572.82, "end": 1574.86, "text": " us z2.", "tokens": [50928, 505, 710, 17, 13, 51030], "temperature": 0.0, "avg_logprob": -0.11879930288895317, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.010985329747200012}, {"id": 346, "seek": 156154, "start": 1574.86, "end": 1580.34, "text": " If we look at a different neuron, let's suppose z3, we'll get a different value here because", "tokens": [51030, 759, 321, 574, 412, 257, 819, 34090, 11, 718, 311, 7297, 710, 18, 11, 321, 603, 483, 257, 819, 2158, 510, 570, 51304], "temperature": 0.0, "avg_logprob": -0.11879930288895317, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.010985329747200012}, {"id": 347, "seek": 156154, "start": 1580.34, "end": 1586.1, "text": " the weights leading to z3 are probably different than those leading to z2.", "tokens": [51304, 264, 17443, 5775, 281, 710, 18, 366, 1391, 819, 813, 729, 5775, 281, 710, 17, 13, 51592], "temperature": 0.0, "avg_logprob": -0.11879930288895317, "compression_ratio": 1.6121495327102804, "no_speech_prob": 0.010985329747200012}, {"id": 348, "seek": 158610, "start": 1586.1, "end": 1592.86, "text": " Now this picture looks a bit messy, so let's try and clean things up a bit more.", "tokens": [50364, 823, 341, 3036, 1542, 257, 857, 16191, 11, 370, 718, 311, 853, 293, 2541, 721, 493, 257, 857, 544, 13, 50702], "temperature": 0.0, "avg_logprob": -0.131751693212069, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0023230432998389006}, {"id": 349, "seek": 158610, "start": 1592.86, "end": 1598.02, "text": " From now on, I'll just use this symbol here to denote what we call this dense layer or", "tokens": [50702, 3358, 586, 322, 11, 286, 603, 445, 764, 341, 5986, 510, 281, 45708, 437, 321, 818, 341, 18011, 4583, 420, 50960], "temperature": 0.0, "avg_logprob": -0.131751693212069, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0023230432998389006}, {"id": 350, "seek": 158610, "start": 1598.02, "end": 1600.34, "text": " fully connected layers.", "tokens": [50960, 4498, 4582, 7914, 13, 51076], "temperature": 0.0, "avg_logprob": -0.131751693212069, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0023230432998389006}, {"id": 351, "seek": 158610, "start": 1600.34, "end": 1606.82, "text": " And here you can actually see an example of how we can create this exact neural network", "tokens": [51076, 400, 510, 291, 393, 767, 536, 364, 1365, 295, 577, 321, 393, 1884, 341, 1900, 18161, 3209, 51400], "temperature": 0.0, "avg_logprob": -0.131751693212069, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0023230432998389006}, {"id": 352, "seek": 158610, "start": 1606.82, "end": 1611.2199999999998, "text": " again using TensorFlow with the predefined dense layer notation.", "tokens": [51400, 797, 1228, 37624, 365, 264, 659, 37716, 18011, 4583, 24657, 13, 51620], "temperature": 0.0, "avg_logprob": -0.131751693212069, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0023230432998389006}, {"id": 353, "seek": 158610, "start": 1611.2199999999998, "end": 1615.98, "text": " Here we're creating a sequential model where we can stack layers on top of each other.", "tokens": [51620, 1692, 321, 434, 4084, 257, 42881, 2316, 689, 321, 393, 8630, 7914, 322, 1192, 295, 1184, 661, 13, 51858], "temperature": 0.0, "avg_logprob": -0.131751693212069, "compression_ratio": 1.6203007518796992, "no_speech_prob": 0.0023230432998389006}, {"id": 354, "seek": 161598, "start": 1615.98, "end": 1624.3, "text": " This layer with n neurons and the second layer with 2 neurons, the output layer.", "tokens": [50364, 639, 4583, 365, 297, 22027, 293, 264, 1150, 4583, 365, 568, 22027, 11, 264, 5598, 4583, 13, 50780], "temperature": 0.0, "avg_logprob": -0.12143484751383464, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00113344332203269}, {"id": 355, "seek": 161598, "start": 1624.3, "end": 1629.02, "text": " And if we want to create a deep neural network, all we have to do is keep stacking these layers", "tokens": [50780, 400, 498, 321, 528, 281, 1884, 257, 2452, 18161, 3209, 11, 439, 321, 362, 281, 360, 307, 1066, 41376, 613, 7914, 51016], "temperature": 0.0, "avg_logprob": -0.12143484751383464, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00113344332203269}, {"id": 356, "seek": 161598, "start": 1629.02, "end": 1634.94, "text": " to create more and more hierarchical models, ones where the final output is computed by", "tokens": [51016, 281, 1884, 544, 293, 544, 35250, 804, 5245, 11, 2306, 689, 264, 2572, 5598, 307, 40610, 538, 51312], "temperature": 0.0, "avg_logprob": -0.12143484751383464, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00113344332203269}, {"id": 357, "seek": 161598, "start": 1634.94, "end": 1638.6200000000001, "text": " going deeper and deeper into the network.", "tokens": [51312, 516, 7731, 293, 7731, 666, 264, 3209, 13, 51496], "temperature": 0.0, "avg_logprob": -0.12143484751383464, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00113344332203269}, {"id": 358, "seek": 161598, "start": 1638.6200000000001, "end": 1644.06, "text": " And to implement this in TensorFlow again, it's very similar as we saw before, again", "tokens": [51496, 400, 281, 4445, 341, 294, 37624, 797, 11, 309, 311, 588, 2531, 382, 321, 1866, 949, 11, 797, 51768], "temperature": 0.0, "avg_logprob": -0.12143484751383464, "compression_ratio": 1.6781115879828326, "no_speech_prob": 0.00113344332203269}, {"id": 359, "seek": 164406, "start": 1644.06, "end": 1650.54, "text": " using the TFKARIS sequential call, we can stack each of these dense layers on top of", "tokens": [50364, 1228, 264, 40964, 42, 1899, 2343, 42881, 818, 11, 321, 393, 8630, 1184, 295, 613, 18011, 7914, 322, 1192, 295, 50688], "temperature": 0.0, "avg_logprob": -0.18511082866404316, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.003706870600581169}, {"id": 360, "seek": 164406, "start": 1650.54, "end": 1657.26, "text": " each other, each one specified by the number of neurons in that dense layer, n1 and 2,", "tokens": [50688, 1184, 661, 11, 1184, 472, 22206, 538, 264, 1230, 295, 22027, 294, 300, 18011, 4583, 11, 297, 16, 293, 568, 11, 51024], "temperature": 0.0, "avg_logprob": -0.18511082866404316, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.003706870600581169}, {"id": 361, "seek": 164406, "start": 1657.26, "end": 1662.94, "text": " but with the last output layer fixed to 2 outputs, if that's how many outputs we have.", "tokens": [51024, 457, 365, 264, 1036, 5598, 4583, 6806, 281, 568, 23930, 11, 498, 300, 311, 577, 867, 23930, 321, 362, 13, 51308], "temperature": 0.0, "avg_logprob": -0.18511082866404316, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.003706870600581169}, {"id": 362, "seek": 164406, "start": 1662.94, "end": 1665.62, "text": " Okay, so that's awesome.", "tokens": [51308, 1033, 11, 370, 300, 311, 3476, 13, 51442], "temperature": 0.0, "avg_logprob": -0.18511082866404316, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.003706870600581169}, {"id": 363, "seek": 164406, "start": 1665.62, "end": 1672.1, "text": " Now we have an idea of not only how to build up a neural network directly from a perceptron,", "tokens": [51442, 823, 321, 362, 364, 1558, 295, 406, 787, 577, 281, 1322, 493, 257, 18161, 3209, 3838, 490, 257, 43276, 2044, 11, 51766], "temperature": 0.0, "avg_logprob": -0.18511082866404316, "compression_ratio": 1.5666666666666667, "no_speech_prob": 0.003706870600581169}, {"id": 364, "seek": 167210, "start": 1672.1, "end": 1676.6599999999999, "text": " but how to compose them together to form complex deep neural networks.", "tokens": [50364, 457, 577, 281, 35925, 552, 1214, 281, 1254, 3997, 2452, 18161, 9590, 13, 50592], "temperature": 0.0, "avg_logprob": -0.1464712598682505, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.025946125388145447}, {"id": 365, "seek": 167210, "start": 1676.6599999999999, "end": 1682.1799999999998, "text": " Let's take a look at how we can actually apply them to a very real problem that I believe", "tokens": [50592, 961, 311, 747, 257, 574, 412, 577, 321, 393, 767, 3079, 552, 281, 257, 588, 957, 1154, 300, 286, 1697, 50868], "temperature": 0.0, "avg_logprob": -0.1464712598682505, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.025946125388145447}, {"id": 366, "seek": 167210, "start": 1682.1799999999998, "end": 1687.1399999999999, "text": " all of you should care very deeply about.", "tokens": [50868, 439, 295, 291, 820, 1127, 588, 8760, 466, 13, 51116], "temperature": 0.0, "avg_logprob": -0.1464712598682505, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.025946125388145447}, {"id": 367, "seek": 167210, "start": 1687.1399999999999, "end": 1692.06, "text": " Here's a problem that we want to build an AI system to learn to answer.", "tokens": [51116, 1692, 311, 257, 1154, 300, 321, 528, 281, 1322, 364, 7318, 1185, 281, 1466, 281, 1867, 13, 51362], "temperature": 0.0, "avg_logprob": -0.1464712598682505, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.025946125388145447}, {"id": 368, "seek": 167210, "start": 1692.06, "end": 1694.3, "text": " Will I pass this class?", "tokens": [51362, 3099, 286, 1320, 341, 1508, 30, 51474], "temperature": 0.0, "avg_logprob": -0.1464712598682505, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.025946125388145447}, {"id": 369, "seek": 167210, "start": 1694.3, "end": 1697.86, "text": " And we can start with a simple two feature model.", "tokens": [51474, 400, 321, 393, 722, 365, 257, 2199, 732, 4111, 2316, 13, 51652], "temperature": 0.0, "avg_logprob": -0.1464712598682505, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.025946125388145447}, {"id": 370, "seek": 167210, "start": 1697.86, "end": 1701.78, "text": " One feature, let's say, is the number of lectures that you attend as part of this class.", "tokens": [51652, 1485, 4111, 11, 718, 311, 584, 11, 307, 264, 1230, 295, 16564, 300, 291, 6888, 382, 644, 295, 341, 1508, 13, 51848], "temperature": 0.0, "avg_logprob": -0.1464712598682505, "compression_ratio": 1.6367041198501873, "no_speech_prob": 0.025946125388145447}, {"id": 371, "seek": 170178, "start": 1701.78, "end": 1707.06, "text": " And the second feature is the number of hours that you spend working on your final project.", "tokens": [50364, 400, 264, 1150, 4111, 307, 264, 1230, 295, 2496, 300, 291, 3496, 1364, 322, 428, 2572, 1716, 13, 50628], "temperature": 0.0, "avg_logprob": -0.09400128281634787, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696430061012506}, {"id": 372, "seek": 170178, "start": 1707.06, "end": 1712.86, "text": " You do have some training data from all of the past participants of Success191.", "tokens": [50628, 509, 360, 362, 512, 3097, 1412, 490, 439, 295, 264, 1791, 10503, 295, 23669, 3405, 16, 13, 50918], "temperature": 0.0, "avg_logprob": -0.09400128281634787, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696430061012506}, {"id": 373, "seek": 170178, "start": 1712.86, "end": 1716.46, "text": " And we can plot this data on this feature space like this.", "tokens": [50918, 400, 321, 393, 7542, 341, 1412, 322, 341, 4111, 1901, 411, 341, 13, 51098], "temperature": 0.0, "avg_logprob": -0.09400128281634787, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696430061012506}, {"id": 374, "seek": 170178, "start": 1716.46, "end": 1721.98, "text": " The green points here actually indicate students, so each point is one student that has passed", "tokens": [51098, 440, 3092, 2793, 510, 767, 13330, 1731, 11, 370, 1184, 935, 307, 472, 3107, 300, 575, 4678, 51374], "temperature": 0.0, "avg_logprob": -0.09400128281634787, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696430061012506}, {"id": 375, "seek": 170178, "start": 1721.98, "end": 1727.06, "text": " the class, and the red points are students that have failed the class.", "tokens": [51374, 264, 1508, 11, 293, 264, 2182, 2793, 366, 1731, 300, 362, 7612, 264, 1508, 13, 51628], "temperature": 0.0, "avg_logprob": -0.09400128281634787, "compression_ratio": 1.6923076923076923, "no_speech_prob": 0.0009696430061012506}, {"id": 376, "seek": 172706, "start": 1727.06, "end": 1732.26, "text": " You can see where they are in this feature space depends on the actual number of hours", "tokens": [50364, 509, 393, 536, 689, 436, 366, 294, 341, 4111, 1901, 5946, 322, 264, 3539, 1230, 295, 2496, 50624], "temperature": 0.0, "avg_logprob": -0.11612655719121297, "compression_ratio": 1.9205607476635513, "no_speech_prob": 0.019121354445815086}, {"id": 377, "seek": 172706, "start": 1732.26, "end": 1735.7, "text": " that they attended the lecture, the number of lectures they attended, and the number", "tokens": [50624, 300, 436, 15990, 264, 7991, 11, 264, 1230, 295, 16564, 436, 15990, 11, 293, 264, 1230, 50796], "temperature": 0.0, "avg_logprob": -0.11612655719121297, "compression_ratio": 1.9205607476635513, "no_speech_prob": 0.019121354445815086}, {"id": 378, "seek": 172706, "start": 1735.7, "end": 1739.26, "text": " of hours they spent on the final project.", "tokens": [50796, 295, 2496, 436, 4418, 322, 264, 2572, 1716, 13, 50974], "temperature": 0.0, "avg_logprob": -0.11612655719121297, "compression_ratio": 1.9205607476635513, "no_speech_prob": 0.019121354445815086}, {"id": 379, "seek": 172706, "start": 1739.26, "end": 1740.34, "text": " And then there's you.", "tokens": [50974, 400, 550, 456, 311, 291, 13, 51028], "temperature": 0.0, "avg_logprob": -0.11612655719121297, "compression_ratio": 1.9205607476635513, "no_speech_prob": 0.019121354445815086}, {"id": 380, "seek": 172706, "start": 1740.34, "end": 1746.34, "text": " You have attended four lectures, and you have spent five hours on your final project.", "tokens": [51028, 509, 362, 15990, 1451, 16564, 11, 293, 291, 362, 4418, 1732, 2496, 322, 428, 2572, 1716, 13, 51328], "temperature": 0.0, "avg_logprob": -0.11612655719121297, "compression_ratio": 1.9205607476635513, "no_speech_prob": 0.019121354445815086}, {"id": 381, "seek": 172706, "start": 1746.34, "end": 1754.46, "text": " And you want to understand how can you build a neural network given everyone else in this", "tokens": [51328, 400, 291, 528, 281, 1223, 577, 393, 291, 1322, 257, 18161, 3209, 2212, 1518, 1646, 294, 341, 51734], "temperature": 0.0, "avg_logprob": -0.11612655719121297, "compression_ratio": 1.9205607476635513, "no_speech_prob": 0.019121354445815086}, {"id": 382, "seek": 175446, "start": 1754.46, "end": 1755.74, "text": " class?", "tokens": [50364, 1508, 30, 50428], "temperature": 0.0, "avg_logprob": -0.10242625205747542, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.042073991149663925}, {"id": 383, "seek": 175446, "start": 1755.74, "end": 1762.14, "text": " Will you pass or fail this class based on the training data that you see?", "tokens": [50428, 3099, 291, 1320, 420, 3061, 341, 1508, 2361, 322, 264, 3097, 1412, 300, 291, 536, 30, 50748], "temperature": 0.0, "avg_logprob": -0.10242625205747542, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.042073991149663925}, {"id": 384, "seek": 175446, "start": 1762.14, "end": 1763.14, "text": " So let's do it.", "tokens": [50748, 407, 718, 311, 360, 309, 13, 50798], "temperature": 0.0, "avg_logprob": -0.10242625205747542, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.042073991149663925}, {"id": 385, "seek": 175446, "start": 1763.14, "end": 1767.3, "text": " We have now all of the requirements to do this now.", "tokens": [50798, 492, 362, 586, 439, 295, 264, 7728, 281, 360, 341, 586, 13, 51006], "temperature": 0.0, "avg_logprob": -0.10242625205747542, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.042073991149663925}, {"id": 386, "seek": 175446, "start": 1767.3, "end": 1773.06, "text": " So let's build a neural network with two inputs, x1 and x2, with x1 being the number", "tokens": [51006, 407, 718, 311, 1322, 257, 18161, 3209, 365, 732, 15743, 11, 2031, 16, 293, 2031, 17, 11, 365, 2031, 16, 885, 264, 1230, 51294], "temperature": 0.0, "avg_logprob": -0.10242625205747542, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.042073991149663925}, {"id": 387, "seek": 175446, "start": 1773.06, "end": 1778.3400000000001, "text": " of lectures that we attend, x2 is the number of hours you spend on your final project.", "tokens": [51294, 295, 16564, 300, 321, 6888, 11, 2031, 17, 307, 264, 1230, 295, 2496, 291, 3496, 322, 428, 2572, 1716, 13, 51558], "temperature": 0.0, "avg_logprob": -0.10242625205747542, "compression_ratio": 1.5609756097560976, "no_speech_prob": 0.042073991149663925}, {"id": 388, "seek": 177834, "start": 1778.34, "end": 1784.5, "text": " We'll have one hidden layer with three units, and we'll feed those into a final probability", "tokens": [50364, 492, 603, 362, 472, 7633, 4583, 365, 1045, 6815, 11, 293, 321, 603, 3154, 729, 666, 257, 2572, 8482, 50672], "temperature": 0.0, "avg_logprob": -0.1293096435203981, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.4607357084751129}, {"id": 389, "seek": 177834, "start": 1784.5, "end": 1791.98, "text": " output by passing this class, and we can see that the probability that we pass is 0.1,", "tokens": [50672, 5598, 538, 8437, 341, 1508, 11, 293, 321, 393, 536, 300, 264, 8482, 300, 321, 1320, 307, 1958, 13, 16, 11, 51046], "temperature": 0.0, "avg_logprob": -0.1293096435203981, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.4607357084751129}, {"id": 390, "seek": 177834, "start": 1791.98, "end": 1793.74, "text": " or 10%.", "tokens": [51046, 420, 1266, 6856, 51134], "temperature": 0.0, "avg_logprob": -0.1293096435203981, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.4607357084751129}, {"id": 391, "seek": 177834, "start": 1793.74, "end": 1800.9399999999998, "text": " That's not great, but the reason is because that this model was never actually trained.", "tokens": [51134, 663, 311, 406, 869, 11, 457, 264, 1778, 307, 570, 300, 341, 2316, 390, 1128, 767, 8895, 13, 51494], "temperature": 0.0, "avg_logprob": -0.1293096435203981, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.4607357084751129}, {"id": 392, "seek": 177834, "start": 1800.9399999999998, "end": 1804.4199999999998, "text": " It's basically just a baby.", "tokens": [51494, 467, 311, 1936, 445, 257, 3186, 13, 51668], "temperature": 0.0, "avg_logprob": -0.1293096435203981, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.4607357084751129}, {"id": 393, "seek": 177834, "start": 1804.4199999999998, "end": 1806.22, "text": " It's never seen any data.", "tokens": [51668, 467, 311, 1128, 1612, 604, 1412, 13, 51758], "temperature": 0.0, "avg_logprob": -0.1293096435203981, "compression_ratio": 1.561904761904762, "no_speech_prob": 0.4607357084751129}, {"id": 394, "seek": 180622, "start": 1806.22, "end": 1808.9, "text": " Even though you have seen the data, it hasn't seen any data.", "tokens": [50364, 2754, 1673, 291, 362, 1612, 264, 1412, 11, 309, 6132, 380, 1612, 604, 1412, 13, 50498], "temperature": 0.0, "avg_logprob": -0.11059799357357188, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00394494878128171}, {"id": 395, "seek": 180622, "start": 1808.9, "end": 1814.26, "text": " And more importantly, you haven't told the model how to interpret this data.", "tokens": [50498, 400, 544, 8906, 11, 291, 2378, 380, 1907, 264, 2316, 577, 281, 7302, 341, 1412, 13, 50766], "temperature": 0.0, "avg_logprob": -0.11059799357357188, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00394494878128171}, {"id": 396, "seek": 180622, "start": 1814.26, "end": 1816.42, "text": " It needs to learn about this problem first.", "tokens": [50766, 467, 2203, 281, 1466, 466, 341, 1154, 700, 13, 50874], "temperature": 0.0, "avg_logprob": -0.11059799357357188, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00394494878128171}, {"id": 397, "seek": 180622, "start": 1816.42, "end": 1822.06, "text": " It knows nothing about this class or final projects or any of that.", "tokens": [50874, 467, 3255, 1825, 466, 341, 1508, 420, 2572, 4455, 420, 604, 295, 300, 13, 51156], "temperature": 0.0, "avg_logprob": -0.11059799357357188, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00394494878128171}, {"id": 398, "seek": 180622, "start": 1822.06, "end": 1826.14, "text": " So one of the most important things to do this is actually you have to tell the model", "tokens": [51156, 407, 472, 295, 264, 881, 1021, 721, 281, 360, 341, 307, 767, 291, 362, 281, 980, 264, 2316, 51360], "temperature": 0.0, "avg_logprob": -0.11059799357357188, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00394494878128171}, {"id": 399, "seek": 180622, "start": 1826.14, "end": 1832.7, "text": " when it is making bad predictions in order for it to be able to correct itself.", "tokens": [51360, 562, 309, 307, 1455, 1578, 21264, 294, 1668, 337, 309, 281, 312, 1075, 281, 3006, 2564, 13, 51688], "temperature": 0.0, "avg_logprob": -0.11059799357357188, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00394494878128171}, {"id": 400, "seek": 180622, "start": 1832.7, "end": 1835.82, "text": " Now the loss of a neural network actually defines exactly this.", "tokens": [51688, 823, 264, 4470, 295, 257, 18161, 3209, 767, 23122, 2293, 341, 13, 51844], "temperature": 0.0, "avg_logprob": -0.11059799357357188, "compression_ratio": 1.7675276752767528, "no_speech_prob": 0.00394494878128171}, {"id": 401, "seek": 183582, "start": 1835.82, "end": 1841.1399999999999, "text": " It defines how wrong a prediction was.", "tokens": [50364, 467, 23122, 577, 2085, 257, 17630, 390, 13, 50630], "temperature": 0.0, "avg_logprob": -0.11971500666454585, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.005059849936515093}, {"id": 402, "seek": 183582, "start": 1841.1399999999999, "end": 1845.58, "text": " So it takes as input the predicted outputs and the ground truth outputs.", "tokens": [50630, 407, 309, 2516, 382, 4846, 264, 19147, 23930, 293, 264, 2727, 3494, 23930, 13, 50852], "temperature": 0.0, "avg_logprob": -0.11971500666454585, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.005059849936515093}, {"id": 403, "seek": 183582, "start": 1845.58, "end": 1850.1399999999999, "text": " Now if those two things are very far apart from each other, then the loss will be very", "tokens": [50852, 823, 498, 729, 732, 721, 366, 588, 1400, 4936, 490, 1184, 661, 11, 550, 264, 4470, 486, 312, 588, 51080], "temperature": 0.0, "avg_logprob": -0.11971500666454585, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.005059849936515093}, {"id": 404, "seek": 183582, "start": 1850.1399999999999, "end": 1851.54, "text": " large.", "tokens": [51080, 2416, 13, 51150], "temperature": 0.0, "avg_logprob": -0.11971500666454585, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.005059849936515093}, {"id": 405, "seek": 183582, "start": 1851.54, "end": 1856.3, "text": " On the other hand, the closer these two things are from each other, the smaller the loss,", "tokens": [51150, 1282, 264, 661, 1011, 11, 264, 4966, 613, 732, 721, 366, 490, 1184, 661, 11, 264, 4356, 264, 4470, 11, 51388], "temperature": 0.0, "avg_logprob": -0.11971500666454585, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.005059849936515093}, {"id": 406, "seek": 183582, "start": 1856.3, "end": 1859.5, "text": " and the more accurate the loss the model will be.", "tokens": [51388, 293, 264, 544, 8559, 264, 4470, 264, 2316, 486, 312, 13, 51548], "temperature": 0.0, "avg_logprob": -0.11971500666454585, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.005059849936515093}, {"id": 407, "seek": 183582, "start": 1859.5, "end": 1861.58, "text": " So we always want to minimize the loss.", "tokens": [51548, 407, 321, 1009, 528, 281, 17522, 264, 4470, 13, 51652], "temperature": 0.0, "avg_logprob": -0.11971500666454585, "compression_ratio": 1.8246445497630333, "no_speech_prob": 0.005059849936515093}, {"id": 408, "seek": 186158, "start": 1861.58, "end": 1866.9399999999998, "text": " We want to incur, we want to predict something that's as close as possible to the ground", "tokens": [50364, 492, 528, 281, 35774, 11, 321, 528, 281, 6069, 746, 300, 311, 382, 1998, 382, 1944, 281, 264, 2727, 50632], "temperature": 0.0, "avg_logprob": -0.14092142275064298, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004754519090056419}, {"id": 409, "seek": 186158, "start": 1866.9399999999998, "end": 1869.74, "text": " truth.", "tokens": [50632, 3494, 13, 50772], "temperature": 0.0, "avg_logprob": -0.14092142275064298, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004754519090056419}, {"id": 410, "seek": 186158, "start": 1869.74, "end": 1874.8999999999999, "text": " Now let's assume we have not just the data from one student, but as we have in this", "tokens": [50772, 823, 718, 311, 6552, 321, 362, 406, 445, 264, 1412, 490, 472, 3107, 11, 457, 382, 321, 362, 294, 341, 51030], "temperature": 0.0, "avg_logprob": -0.14092142275064298, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004754519090056419}, {"id": 411, "seek": 186158, "start": 1874.8999999999999, "end": 1877.26, "text": " case the data from many students.", "tokens": [51030, 1389, 264, 1412, 490, 867, 1731, 13, 51148], "temperature": 0.0, "avg_logprob": -0.14092142275064298, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004754519090056419}, {"id": 412, "seek": 186158, "start": 1877.26, "end": 1883.1, "text": " We now care about not just how the model did on predicting just one prediction, but how", "tokens": [51148, 492, 586, 1127, 466, 406, 445, 577, 264, 2316, 630, 322, 32884, 445, 472, 17630, 11, 457, 577, 51440], "temperature": 0.0, "avg_logprob": -0.14092142275064298, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004754519090056419}, {"id": 413, "seek": 186158, "start": 1883.1, "end": 1886.58, "text": " it did on average across all of these students.", "tokens": [51440, 309, 630, 322, 4274, 2108, 439, 295, 613, 1731, 13, 51614], "temperature": 0.0, "avg_logprob": -0.14092142275064298, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004754519090056419}, {"id": 414, "seek": 186158, "start": 1886.58, "end": 1889.26, "text": " This is what we call the empirical loss.", "tokens": [51614, 639, 307, 437, 321, 818, 264, 31886, 4470, 13, 51748], "temperature": 0.0, "avg_logprob": -0.14092142275064298, "compression_ratio": 1.7647058823529411, "no_speech_prob": 0.004754519090056419}, {"id": 415, "seek": 188926, "start": 1889.26, "end": 1894.82, "text": " And it's simply just the mean or the average of every loss from each individual example", "tokens": [50364, 400, 309, 311, 2935, 445, 264, 914, 420, 264, 4274, 295, 633, 4470, 490, 1184, 2609, 1365, 50642], "temperature": 0.0, "avg_logprob": -0.10386630296707153, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00011591677321121097}, {"id": 416, "seek": 188926, "start": 1894.82, "end": 1898.46, "text": " or each individual student.", "tokens": [50642, 420, 1184, 2609, 3107, 13, 50824], "temperature": 0.0, "avg_logprob": -0.10386630296707153, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00011591677321121097}, {"id": 417, "seek": 188926, "start": 1898.46, "end": 1903.34, "text": " When training a neural network, we want to find a network that minimizes the empirical", "tokens": [50824, 1133, 3097, 257, 18161, 3209, 11, 321, 528, 281, 915, 257, 3209, 300, 4464, 5660, 264, 31886, 51068], "temperature": 0.0, "avg_logprob": -0.10386630296707153, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00011591677321121097}, {"id": 418, "seek": 188926, "start": 1903.34, "end": 1910.58, "text": " loss between our predictions and the true outputs.", "tokens": [51068, 4470, 1296, 527, 21264, 293, 264, 2074, 23930, 13, 51430], "temperature": 0.0, "avg_logprob": -0.10386630296707153, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00011591677321121097}, {"id": 419, "seek": 188926, "start": 1910.58, "end": 1916.66, "text": " Now if we look at the problem of binary classification, where the neural network like we want to do", "tokens": [51430, 823, 498, 321, 574, 412, 264, 1154, 295, 17434, 21538, 11, 689, 264, 18161, 3209, 411, 321, 528, 281, 360, 51734], "temperature": 0.0, "avg_logprob": -0.10386630296707153, "compression_ratio": 1.6418604651162791, "no_speech_prob": 0.00011591677321121097}, {"id": 420, "seek": 191666, "start": 1916.66, "end": 1922.18, "text": " in this case, is supposed to answer either yes or no, one or zero.", "tokens": [50364, 294, 341, 1389, 11, 307, 3442, 281, 1867, 2139, 2086, 420, 572, 11, 472, 420, 4018, 13, 50640], "temperature": 0.0, "avg_logprob": -0.13392317862737746, "compression_ratio": 1.755980861244019, "no_speech_prob": 0.12245340645313263}, {"id": 421, "seek": 191666, "start": 1922.18, "end": 1926.74, "text": " We can use what is called a softmax cross entropy loss.", "tokens": [50640, 492, 393, 764, 437, 307, 1219, 257, 2787, 41167, 3278, 30867, 4470, 13, 50868], "temperature": 0.0, "avg_logprob": -0.13392317862737746, "compression_ratio": 1.755980861244019, "no_speech_prob": 0.12245340645313263}, {"id": 422, "seek": 191666, "start": 1926.74, "end": 1935.5, "text": " Now the softmax cross entropy loss is actually written out here and it's defined by what's", "tokens": [50868, 823, 264, 2787, 41167, 3278, 30867, 4470, 307, 767, 3720, 484, 510, 293, 309, 311, 7642, 538, 437, 311, 51306], "temperature": 0.0, "avg_logprob": -0.13392317862737746, "compression_ratio": 1.755980861244019, "no_speech_prob": 0.12245340645313263}, {"id": 423, "seek": 191666, "start": 1935.5, "end": 1938.98, "text": " called the cross entropy between two probability distributions.", "tokens": [51306, 1219, 264, 3278, 30867, 1296, 732, 8482, 37870, 13, 51480], "temperature": 0.0, "avg_logprob": -0.13392317862737746, "compression_ratio": 1.755980861244019, "no_speech_prob": 0.12245340645313263}, {"id": 424, "seek": 191666, "start": 1938.98, "end": 1945.0600000000002, "text": " It measures how far apart the ground truth probability distribution is from the predicted", "tokens": [51480, 467, 8000, 577, 1400, 4936, 264, 2727, 3494, 8482, 7316, 307, 490, 264, 19147, 51784], "temperature": 0.0, "avg_logprob": -0.13392317862737746, "compression_ratio": 1.755980861244019, "no_speech_prob": 0.12245340645313263}, {"id": 425, "seek": 194506, "start": 1945.06, "end": 1948.98, "text": " probability distribution.", "tokens": [50364, 8482, 7316, 13, 50560], "temperature": 0.0, "avg_logprob": -0.13794377575749936, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.002323039574548602}, {"id": 426, "seek": 194506, "start": 1948.98, "end": 1953.8999999999999, "text": " Let's suppose instead of predicting binary outputs, will I pass this class or will I", "tokens": [50560, 961, 311, 7297, 2602, 295, 32884, 17434, 23930, 11, 486, 286, 1320, 341, 1508, 420, 486, 286, 50806], "temperature": 0.0, "avg_logprob": -0.13794377575749936, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.002323039574548602}, {"id": 427, "seek": 194506, "start": 1953.8999999999999, "end": 1955.98, "text": " not pass this class?", "tokens": [50806, 406, 1320, 341, 1508, 30, 50910], "temperature": 0.0, "avg_logprob": -0.13794377575749936, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.002323039574548602}, {"id": 428, "seek": 194506, "start": 1955.98, "end": 1962.3799999999999, "text": " Instead you want to predict the final grade as a real number, not a probability or as", "tokens": [50910, 7156, 291, 528, 281, 6069, 264, 2572, 7204, 382, 257, 957, 1230, 11, 406, 257, 8482, 420, 382, 51230], "temperature": 0.0, "avg_logprob": -0.13794377575749936, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.002323039574548602}, {"id": 429, "seek": 194506, "start": 1962.3799999999999, "end": 1969.7, "text": " a percentage, we want the grade that you will get in this class.", "tokens": [51230, 257, 9668, 11, 321, 528, 264, 7204, 300, 291, 486, 483, 294, 341, 1508, 13, 51596], "temperature": 0.0, "avg_logprob": -0.13794377575749936, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.002323039574548602}, {"id": 430, "seek": 194506, "start": 1969.7, "end": 1974.54, "text": " Now in this case, because the type of the output is different, we also need to use a", "tokens": [51596, 823, 294, 341, 1389, 11, 570, 264, 2010, 295, 264, 5598, 307, 819, 11, 321, 611, 643, 281, 764, 257, 51838], "temperature": 0.0, "avg_logprob": -0.13794377575749936, "compression_ratio": 1.7230046948356808, "no_speech_prob": 0.002323039574548602}, {"id": 431, "seek": 197454, "start": 1974.54, "end": 1979.22, "text": " different loss here, because our outputs are no longer 0, 1, but they can be any real", "tokens": [50364, 819, 4470, 510, 11, 570, 527, 23930, 366, 572, 2854, 1958, 11, 502, 11, 457, 436, 393, 312, 604, 957, 50598], "temperature": 0.0, "avg_logprob": -0.1531624537642284, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00781337358057499}, {"id": 432, "seek": 197454, "start": 1979.22, "end": 1980.22, "text": " number.", "tokens": [50598, 1230, 13, 50648], "temperature": 0.0, "avg_logprob": -0.1531624537642284, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00781337358057499}, {"id": 433, "seek": 197454, "start": 1980.22, "end": 1985.3, "text": " They're just the grade that you're going to get on the final class.", "tokens": [50648, 814, 434, 445, 264, 7204, 300, 291, 434, 516, 281, 483, 322, 264, 2572, 1508, 13, 50902], "temperature": 0.0, "avg_logprob": -0.1531624537642284, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00781337358057499}, {"id": 434, "seek": 197454, "start": 1985.3, "end": 1991.1, "text": " So for example, here since this is a continuous variable, the grade, we want to use what's", "tokens": [50902, 407, 337, 1365, 11, 510, 1670, 341, 307, 257, 10957, 7006, 11, 264, 7204, 11, 321, 528, 281, 764, 437, 311, 51192], "temperature": 0.0, "avg_logprob": -0.1531624537642284, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00781337358057499}, {"id": 435, "seek": 197454, "start": 1991.1, "end": 1992.7, "text": " called the mean squared error.", "tokens": [51192, 1219, 264, 914, 8889, 6713, 13, 51272], "temperature": 0.0, "avg_logprob": -0.1531624537642284, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00781337358057499}, {"id": 436, "seek": 197454, "start": 1992.7, "end": 1998.6599999999999, "text": " This measures just the squared error, the squared difference between our ground truth", "tokens": [51272, 639, 8000, 445, 264, 8889, 6713, 11, 264, 8889, 2649, 1296, 527, 2727, 3494, 51570], "temperature": 0.0, "avg_logprob": -0.1531624537642284, "compression_ratio": 1.6473214285714286, "no_speech_prob": 0.00781337358057499}, {"id": 437, "seek": 199866, "start": 1998.66, "end": 2005.18, "text": " and our predictions, again averaged over the entire data set.", "tokens": [50364, 293, 527, 21264, 11, 797, 18247, 2980, 670, 264, 2302, 1412, 992, 13, 50690], "temperature": 0.0, "avg_logprob": -0.19525619653555062, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.30050423741340637}, {"id": 438, "seek": 199866, "start": 2005.18, "end": 2011.3400000000001, "text": " Okay great, so now we've seen two loss functions, one for classification, binary outputs, as", "tokens": [50690, 1033, 869, 11, 370, 586, 321, 600, 1612, 732, 4470, 6828, 11, 472, 337, 21538, 11, 17434, 23930, 11, 382, 50998], "temperature": 0.0, "avg_logprob": -0.19525619653555062, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.30050423741340637}, {"id": 439, "seek": 199866, "start": 2011.3400000000001, "end": 2018.1000000000001, "text": " well as regression, continuous outputs, and the problem now I think that we need to start", "tokens": [50998, 731, 382, 24590, 11, 10957, 23930, 11, 293, 264, 1154, 586, 286, 519, 300, 321, 643, 281, 722, 51336], "temperature": 0.0, "avg_logprob": -0.19525619653555062, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.30050423741340637}, {"id": 440, "seek": 199866, "start": 2018.1000000000001, "end": 2020.8200000000002, "text": " asking ourselves is how can we take that loss function?", "tokens": [51336, 3365, 4175, 307, 577, 393, 321, 747, 300, 4470, 2445, 30, 51472], "temperature": 0.0, "avg_logprob": -0.19525619653555062, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.30050423741340637}, {"id": 441, "seek": 199866, "start": 2020.8200000000002, "end": 2024.3400000000001, "text": " We've seen our loss function, we've seen our network, now we have to actually understand", "tokens": [51472, 492, 600, 1612, 527, 4470, 2445, 11, 321, 600, 1612, 527, 3209, 11, 586, 321, 362, 281, 767, 1223, 51648], "temperature": 0.0, "avg_logprob": -0.19525619653555062, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.30050423741340637}, {"id": 442, "seek": 199866, "start": 2024.3400000000001, "end": 2026.42, "text": " how can we put those two things together?", "tokens": [51648, 577, 393, 321, 829, 729, 732, 721, 1214, 30, 51752], "temperature": 0.0, "avg_logprob": -0.19525619653555062, "compression_ratio": 1.7449392712550607, "no_speech_prob": 0.30050423741340637}, {"id": 443, "seek": 202642, "start": 2026.42, "end": 2031.1000000000001, "text": " How can we use our loss function to train the weights of our neural network such that", "tokens": [50364, 1012, 393, 321, 764, 527, 4470, 2445, 281, 3847, 264, 17443, 295, 527, 18161, 3209, 1270, 300, 50598], "temperature": 0.0, "avg_logprob": -0.1543451944986979, "compression_ratio": 1.9022222222222223, "no_speech_prob": 0.00239657424390316}, {"id": 444, "seek": 202642, "start": 2031.1000000000001, "end": 2033.7, "text": " it can actually learn that problem?", "tokens": [50598, 309, 393, 767, 1466, 300, 1154, 30, 50728], "temperature": 0.0, "avg_logprob": -0.1543451944986979, "compression_ratio": 1.9022222222222223, "no_speech_prob": 0.00239657424390316}, {"id": 445, "seek": 202642, "start": 2033.7, "end": 2038.98, "text": " Well, what we want to do is actually find the weights of the neural network that will", "tokens": [50728, 1042, 11, 437, 321, 528, 281, 360, 307, 767, 915, 264, 17443, 295, 264, 18161, 3209, 300, 486, 50992], "temperature": 0.0, "avg_logprob": -0.1543451944986979, "compression_ratio": 1.9022222222222223, "no_speech_prob": 0.00239657424390316}, {"id": 446, "seek": 202642, "start": 2038.98, "end": 2041.5800000000002, "text": " minimize the loss of our data set.", "tokens": [50992, 17522, 264, 4470, 295, 527, 1412, 992, 13, 51122], "temperature": 0.0, "avg_logprob": -0.1543451944986979, "compression_ratio": 1.9022222222222223, "no_speech_prob": 0.00239657424390316}, {"id": 447, "seek": 202642, "start": 2041.5800000000002, "end": 2048.38, "text": " That essentially means that we want to find the W's in our neural network that minimize", "tokens": [51122, 663, 4476, 1355, 300, 321, 528, 281, 915, 264, 343, 311, 294, 527, 18161, 3209, 300, 17522, 51462], "temperature": 0.0, "avg_logprob": -0.1543451944986979, "compression_ratio": 1.9022222222222223, "no_speech_prob": 0.00239657424390316}, {"id": 448, "seek": 202642, "start": 2048.38, "end": 2049.38, "text": " J of W.", "tokens": [51462, 508, 295, 343, 13, 51512], "temperature": 0.0, "avg_logprob": -0.1543451944986979, "compression_ratio": 1.9022222222222223, "no_speech_prob": 0.00239657424390316}, {"id": 449, "seek": 202642, "start": 2049.38, "end": 2054.9, "text": " J of W's are empirical cost function that we saw in the previous slides that average loss", "tokens": [51512, 508, 295, 343, 311, 366, 31886, 2063, 2445, 300, 321, 1866, 294, 264, 3894, 9788, 300, 4274, 4470, 51788], "temperature": 0.0, "avg_logprob": -0.1543451944986979, "compression_ratio": 1.9022222222222223, "no_speech_prob": 0.00239657424390316}, {"id": 450, "seek": 205490, "start": 2054.9, "end": 2058.6600000000003, "text": " over each data point in the data set.", "tokens": [50364, 670, 1184, 1412, 935, 294, 264, 1412, 992, 13, 50552], "temperature": 0.0, "avg_logprob": -0.17933341831836885, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.032092656940221786}, {"id": 451, "seek": 205490, "start": 2058.6600000000003, "end": 2065.5, "text": " Now, remember that W, capital W, is simply a collection of all of the weights in our", "tokens": [50552, 823, 11, 1604, 300, 343, 11, 4238, 343, 11, 307, 2935, 257, 5765, 295, 439, 295, 264, 17443, 294, 527, 50894], "temperature": 0.0, "avg_logprob": -0.17933341831836885, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.032092656940221786}, {"id": 452, "seek": 205490, "start": 2065.5, "end": 2068.94, "text": " neural network, not just from one layer, but from every single layer.", "tokens": [50894, 18161, 3209, 11, 406, 445, 490, 472, 4583, 11, 457, 490, 633, 2167, 4583, 13, 51066], "temperature": 0.0, "avg_logprob": -0.17933341831836885, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.032092656940221786}, {"id": 453, "seek": 205490, "start": 2068.94, "end": 2073.54, "text": " So that's W0 from the zeroth layer to the first layer to the second layer, all concatenate", "tokens": [51066, 407, 300, 311, 343, 15, 490, 264, 44746, 900, 4583, 281, 264, 700, 4583, 281, 264, 1150, 4583, 11, 439, 1588, 7186, 473, 51296], "temperature": 0.0, "avg_logprob": -0.17933341831836885, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.032092656940221786}, {"id": 454, "seek": 205490, "start": 2073.54, "end": 2074.54, "text": " into one.", "tokens": [51296, 666, 472, 13, 51346], "temperature": 0.0, "avg_logprob": -0.17933341831836885, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.032092656940221786}, {"id": 455, "seek": 205490, "start": 2074.54, "end": 2081.2200000000003, "text": " In this optimization problem we want to optimize all of the W's to minimize this empirical loss.", "tokens": [51346, 682, 341, 19618, 1154, 321, 528, 281, 19719, 439, 295, 264, 343, 311, 281, 17522, 341, 31886, 4470, 13, 51680], "temperature": 0.0, "avg_logprob": -0.17933341831836885, "compression_ratio": 1.652542372881356, "no_speech_prob": 0.032092656940221786}, {"id": 456, "seek": 208122, "start": 2082.22, "end": 2087.2599999999998, "text": " Now, remember our loss function is just a simple function of our weights.", "tokens": [50414, 823, 11, 1604, 527, 4470, 2445, 307, 445, 257, 2199, 2445, 295, 527, 17443, 13, 50666], "temperature": 0.0, "avg_logprob": -0.12822750511519407, "compression_ratio": 1.8634361233480177, "no_speech_prob": 0.009707022458314896}, {"id": 457, "seek": 208122, "start": 2087.2599999999998, "end": 2093.22, "text": " If we have only two weights, we can actually plot this entire loss landscape over this", "tokens": [50666, 759, 321, 362, 787, 732, 17443, 11, 321, 393, 767, 7542, 341, 2302, 4470, 9661, 670, 341, 50964], "temperature": 0.0, "avg_logprob": -0.12822750511519407, "compression_ratio": 1.8634361233480177, "no_speech_prob": 0.009707022458314896}, {"id": 458, "seek": 208122, "start": 2093.22, "end": 2094.22, "text": " grid of weights.", "tokens": [50964, 10748, 295, 17443, 13, 51014], "temperature": 0.0, "avg_logprob": -0.12822750511519407, "compression_ratio": 1.8634361233480177, "no_speech_prob": 0.009707022458314896}, {"id": 459, "seek": 208122, "start": 2094.22, "end": 2098.58, "text": " So on the one axis on the bottom you can see weight number one and the other one you can", "tokens": [51014, 407, 322, 264, 472, 10298, 322, 264, 2767, 291, 393, 536, 3364, 1230, 472, 293, 264, 661, 472, 291, 393, 51232], "temperature": 0.0, "avg_logprob": -0.12822750511519407, "compression_ratio": 1.8634361233480177, "no_speech_prob": 0.009707022458314896}, {"id": 460, "seek": 208122, "start": 2098.58, "end": 2101.4199999999996, "text": " see weight zero.", "tokens": [51232, 536, 3364, 4018, 13, 51374], "temperature": 0.0, "avg_logprob": -0.12822750511519407, "compression_ratio": 1.8634361233480177, "no_speech_prob": 0.009707022458314896}, {"id": 461, "seek": 208122, "start": 2101.4199999999996, "end": 2104.2999999999997, "text": " There's only two weights in this neural network, very simple neural network.", "tokens": [51374, 821, 311, 787, 732, 17443, 294, 341, 18161, 3209, 11, 588, 2199, 18161, 3209, 13, 51518], "temperature": 0.0, "avg_logprob": -0.12822750511519407, "compression_ratio": 1.8634361233480177, "no_speech_prob": 0.009707022458314896}, {"id": 462, "seek": 208122, "start": 2104.2999999999997, "end": 2110.18, "text": " So we can actually plot for every W0 and W1, what is the loss?", "tokens": [51518, 407, 321, 393, 767, 7542, 337, 633, 343, 15, 293, 343, 16, 11, 437, 307, 264, 4470, 30, 51812], "temperature": 0.0, "avg_logprob": -0.12822750511519407, "compression_ratio": 1.8634361233480177, "no_speech_prob": 0.009707022458314896}, {"id": 463, "seek": 211018, "start": 2110.22, "end": 2116.94, "text": " What is the error that we'd expect to see and obtain from this neural network?", "tokens": [50366, 708, 307, 264, 6713, 300, 321, 1116, 2066, 281, 536, 293, 12701, 490, 341, 18161, 3209, 30, 50702], "temperature": 0.0, "avg_logprob": -0.1323004961013794, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.000269467564066872}, {"id": 464, "seek": 211018, "start": 2116.94, "end": 2122.62, "text": " Now the whole process of training a neural network, optimizing it, is to find the lowest", "tokens": [50702, 823, 264, 1379, 1399, 295, 3097, 257, 18161, 3209, 11, 40425, 309, 11, 307, 281, 915, 264, 12437, 50986], "temperature": 0.0, "avg_logprob": -0.1323004961013794, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.000269467564066872}, {"id": 465, "seek": 211018, "start": 2122.62, "end": 2129.58, "text": " point in this loss landscape that will tell us our optimal W0 and W1.", "tokens": [50986, 935, 294, 341, 4470, 9661, 300, 486, 980, 505, 527, 16252, 343, 15, 293, 343, 16, 13, 51334], "temperature": 0.0, "avg_logprob": -0.1323004961013794, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.000269467564066872}, {"id": 466, "seek": 211018, "start": 2129.58, "end": 2130.74, "text": " Now how can we do that?", "tokens": [51334, 823, 577, 393, 321, 360, 300, 30, 51392], "temperature": 0.0, "avg_logprob": -0.1323004961013794, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.000269467564066872}, {"id": 467, "seek": 211018, "start": 2130.74, "end": 2134.54, "text": " The first thing we have to do is pick a point.", "tokens": [51392, 440, 700, 551, 321, 362, 281, 360, 307, 1888, 257, 935, 13, 51582], "temperature": 0.0, "avg_logprob": -0.1323004961013794, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.000269467564066872}, {"id": 468, "seek": 211018, "start": 2134.54, "end": 2137.8999999999996, "text": " So let's pick any W0, W1.", "tokens": [51582, 407, 718, 311, 1888, 604, 343, 15, 11, 343, 16, 13, 51750], "temperature": 0.0, "avg_logprob": -0.1323004961013794, "compression_ratio": 1.568075117370892, "no_speech_prob": 0.000269467564066872}, {"id": 469, "seek": 213790, "start": 2137.94, "end": 2144.34, "text": " Starting from this point we can compute the gradient of the landscape at that point.", "tokens": [50366, 16217, 490, 341, 935, 321, 393, 14722, 264, 16235, 295, 264, 9661, 412, 300, 935, 13, 50686], "temperature": 0.0, "avg_logprob": -0.11797804098862869, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.004198331385850906}, {"id": 470, "seek": 213790, "start": 2144.34, "end": 2151.54, "text": " Now the gradient tells us the direction of highest or steepest ascent.", "tokens": [50686, 823, 264, 16235, 5112, 505, 264, 3513, 295, 6343, 420, 16841, 377, 382, 2207, 13, 51046], "temperature": 0.0, "avg_logprob": -0.11797804098862869, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.004198331385850906}, {"id": 471, "seek": 213790, "start": 2151.54, "end": 2153.78, "text": " So that tells us which way is up.", "tokens": [51046, 407, 300, 5112, 505, 597, 636, 307, 493, 13, 51158], "temperature": 0.0, "avg_logprob": -0.11797804098862869, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.004198331385850906}, {"id": 472, "seek": 213790, "start": 2153.78, "end": 2158.2200000000003, "text": " If we compute the gradient of our loss with respect to our weights, that's the derivative", "tokens": [51158, 759, 321, 14722, 264, 16235, 295, 527, 4470, 365, 3104, 281, 527, 17443, 11, 300, 311, 264, 13760, 51380], "temperature": 0.0, "avg_logprob": -0.11797804098862869, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.004198331385850906}, {"id": 473, "seek": 213790, "start": 2158.2200000000003, "end": 2162.14, "text": " of our gradient for loss with respect to the weights, that tells us the direction of which", "tokens": [51380, 295, 527, 16235, 337, 4470, 365, 3104, 281, 264, 17443, 11, 300, 5112, 505, 264, 3513, 295, 597, 51576], "temperature": 0.0, "avg_logprob": -0.11797804098862869, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.004198331385850906}, {"id": 474, "seek": 213790, "start": 2162.14, "end": 2167.58, "text": " way is up on that loss landscape from where we stand right now.", "tokens": [51576, 636, 307, 493, 322, 300, 4470, 9661, 490, 689, 321, 1463, 558, 586, 13, 51848], "temperature": 0.0, "avg_logprob": -0.11797804098862869, "compression_ratio": 2.066666666666667, "no_speech_prob": 0.004198331385850906}, {"id": 475, "seek": 216758, "start": 2167.58, "end": 2170.74, "text": " Instead of going up though, we want to find the lowest loss.", "tokens": [50364, 7156, 295, 516, 493, 1673, 11, 321, 528, 281, 915, 264, 12437, 4470, 13, 50522], "temperature": 0.0, "avg_logprob": -0.10481293159618713, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.002714818110689521}, {"id": 476, "seek": 216758, "start": 2170.74, "end": 2178.22, "text": " So let's take the negative of our gradient and take a small step in that direction.", "tokens": [50522, 407, 718, 311, 747, 264, 3671, 295, 527, 16235, 293, 747, 257, 1359, 1823, 294, 300, 3513, 13, 50896], "temperature": 0.0, "avg_logprob": -0.10481293159618713, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.002714818110689521}, {"id": 477, "seek": 216758, "start": 2178.22, "end": 2181.66, "text": " This will move us a little bit closer to the lowest point.", "tokens": [50896, 639, 486, 1286, 505, 257, 707, 857, 4966, 281, 264, 12437, 935, 13, 51068], "temperature": 0.0, "avg_logprob": -0.10481293159618713, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.002714818110689521}, {"id": 478, "seek": 216758, "start": 2181.66, "end": 2182.9, "text": " And we just keep repeating this.", "tokens": [51068, 400, 321, 445, 1066, 18617, 341, 13, 51130], "temperature": 0.0, "avg_logprob": -0.10481293159618713, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.002714818110689521}, {"id": 479, "seek": 216758, "start": 2182.9, "end": 2188.42, "text": " Now we compute the gradient at this point and repeat the process until we converge.", "tokens": [51130, 823, 321, 14722, 264, 16235, 412, 341, 935, 293, 7149, 264, 1399, 1826, 321, 41881, 13, 51406], "temperature": 0.0, "avg_logprob": -0.10481293159618713, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.002714818110689521}, {"id": 480, "seek": 216758, "start": 2188.42, "end": 2191.46, "text": " And we will converge to a local minimum.", "tokens": [51406, 400, 321, 486, 41881, 281, 257, 2654, 7285, 13, 51558], "temperature": 0.0, "avg_logprob": -0.10481293159618713, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.002714818110689521}, {"id": 481, "seek": 216758, "start": 2191.46, "end": 2195.7, "text": " We don't know if it will converge to a global minimum, but at least we know that it should", "tokens": [51558, 492, 500, 380, 458, 498, 309, 486, 41881, 281, 257, 4338, 7285, 11, 457, 412, 1935, 321, 458, 300, 309, 820, 51770], "temperature": 0.0, "avg_logprob": -0.10481293159618713, "compression_ratio": 1.7451737451737452, "no_speech_prob": 0.002714818110689521}, {"id": 482, "seek": 219570, "start": 2195.7, "end": 2198.5, "text": " in theory converge to a local minimum.", "tokens": [50364, 294, 5261, 41881, 281, 257, 2654, 7285, 13, 50504], "temperature": 0.0, "avg_logprob": -0.14626967112223307, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.0007321525481529534}, {"id": 483, "seek": 219570, "start": 2198.5, "end": 2202.46, "text": " Now we can summarize this algorithm as follows.", "tokens": [50504, 823, 321, 393, 20858, 341, 9284, 382, 10002, 13, 50702], "temperature": 0.0, "avg_logprob": -0.14626967112223307, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.0007321525481529534}, {"id": 484, "seek": 219570, "start": 2202.46, "end": 2205.8199999999997, "text": " This algorithm is also known as gradient descent.", "tokens": [50702, 639, 9284, 307, 611, 2570, 382, 16235, 23475, 13, 50870], "temperature": 0.0, "avg_logprob": -0.14626967112223307, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.0007321525481529534}, {"id": 485, "seek": 219570, "start": 2205.8199999999997, "end": 2212.14, "text": " So we start by initializing all of our weights randomly and we loop until convergence.", "tokens": [50870, 407, 321, 722, 538, 5883, 3319, 439, 295, 527, 17443, 16979, 293, 321, 6367, 1826, 32181, 13, 51186], "temperature": 0.0, "avg_logprob": -0.14626967112223307, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.0007321525481529534}, {"id": 486, "seek": 219570, "start": 2212.14, "end": 2215.98, "text": " We start from one of those weights, our initial point.", "tokens": [51186, 492, 722, 490, 472, 295, 729, 17443, 11, 527, 5883, 935, 13, 51378], "temperature": 0.0, "avg_logprob": -0.14626967112223307, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.0007321525481529534}, {"id": 487, "seek": 219570, "start": 2215.98, "end": 2217.4199999999996, "text": " We compute the gradient.", "tokens": [51378, 492, 14722, 264, 16235, 13, 51450], "temperature": 0.0, "avg_logprob": -0.14626967112223307, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.0007321525481529534}, {"id": 488, "seek": 219570, "start": 2217.4199999999996, "end": 2219.18, "text": " That tells us which way is up.", "tokens": [51450, 663, 5112, 505, 597, 636, 307, 493, 13, 51538], "temperature": 0.0, "avg_logprob": -0.14626967112223307, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.0007321525481529534}, {"id": 489, "seek": 219570, "start": 2219.18, "end": 2221.2999999999997, "text": " So we take a step in the opposite direction.", "tokens": [51538, 407, 321, 747, 257, 1823, 294, 264, 6182, 3513, 13, 51644], "temperature": 0.0, "avg_logprob": -0.14626967112223307, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.0007321525481529534}, {"id": 490, "seek": 219570, "start": 2221.2999999999997, "end": 2223.74, "text": " We take a small step here.", "tokens": [51644, 492, 747, 257, 1359, 1823, 510, 13, 51766], "temperature": 0.0, "avg_logprob": -0.14626967112223307, "compression_ratio": 1.698744769874477, "no_speech_prob": 0.0007321525481529534}, {"id": 491, "seek": 222374, "start": 2223.74, "end": 2230.02, "text": " All is computed by multiplying our gradient by this factor eta.", "tokens": [50364, 1057, 307, 40610, 538, 30955, 527, 16235, 538, 341, 5952, 32415, 13, 50678], "temperature": 0.0, "avg_logprob": -0.16751251220703126, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.008845078758895397}, {"id": 492, "seek": 222374, "start": 2230.02, "end": 2232.1, "text": " And we'll learn more about this factor later.", "tokens": [50678, 400, 321, 603, 1466, 544, 466, 341, 5952, 1780, 13, 50782], "temperature": 0.0, "avg_logprob": -0.16751251220703126, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.008845078758895397}, {"id": 493, "seek": 222374, "start": 2232.1, "end": 2234.06, "text": " This factor is called the learning rate.", "tokens": [50782, 639, 5952, 307, 1219, 264, 2539, 3314, 13, 50880], "temperature": 0.0, "avg_logprob": -0.16751251220703126, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.008845078758895397}, {"id": 494, "seek": 222374, "start": 2234.06, "end": 2235.62, "text": " We'll learn more about that later.", "tokens": [50880, 492, 603, 1466, 544, 466, 300, 1780, 13, 50958], "temperature": 0.0, "avg_logprob": -0.16751251220703126, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.008845078758895397}, {"id": 495, "seek": 222374, "start": 2235.62, "end": 2241.58, "text": " Now again, in TensorFlow, we can actually see this pseudocode of gradient descent algorithm", "tokens": [50958, 823, 797, 11, 294, 37624, 11, 321, 393, 767, 536, 341, 25505, 532, 905, 1429, 295, 16235, 23475, 9284, 51256], "temperature": 0.0, "avg_logprob": -0.16751251220703126, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.008845078758895397}, {"id": 496, "seek": 222374, "start": 2241.58, "end": 2243.58, "text": " written out in code.", "tokens": [51256, 3720, 484, 294, 3089, 13, 51356], "temperature": 0.0, "avg_logprob": -0.16751251220703126, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.008845078758895397}, {"id": 497, "seek": 222374, "start": 2243.58, "end": 2246.62, "text": " We can randomize all of our weights.", "tokens": [51356, 492, 393, 4974, 1125, 439, 295, 527, 17443, 13, 51508], "temperature": 0.0, "avg_logprob": -0.16751251220703126, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.008845078758895397}, {"id": 498, "seek": 222374, "start": 2246.62, "end": 2251.06, "text": " That basically initializes our search, our optimization process at some point in space.", "tokens": [51508, 663, 1936, 5883, 5660, 527, 3164, 11, 527, 19618, 1399, 412, 512, 935, 294, 1901, 13, 51730], "temperature": 0.0, "avg_logprob": -0.16751251220703126, "compression_ratio": 1.7056451612903225, "no_speech_prob": 0.008845078758895397}, {"id": 499, "seek": 225106, "start": 2251.46, "end": 2253.62, "text": " Then we keep looping over and over and over again.", "tokens": [50384, 1396, 321, 1066, 6367, 278, 670, 293, 670, 293, 670, 797, 13, 50492], "temperature": 0.0, "avg_logprob": -0.13528922109892874, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.025947576388716698}, {"id": 500, "seek": 225106, "start": 2253.62, "end": 2258.1, "text": " We compute the loss, we compute the gradient, and we take a small step of our weights in", "tokens": [50492, 492, 14722, 264, 4470, 11, 321, 14722, 264, 16235, 11, 293, 321, 747, 257, 1359, 1823, 295, 527, 17443, 294, 50716], "temperature": 0.0, "avg_logprob": -0.13528922109892874, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.025947576388716698}, {"id": 501, "seek": 225106, "start": 2258.1, "end": 2261.5, "text": " the direction of that gradient.", "tokens": [50716, 264, 3513, 295, 300, 16235, 13, 50886], "temperature": 0.0, "avg_logprob": -0.13528922109892874, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.025947576388716698}, {"id": 502, "seek": 225106, "start": 2261.5, "end": 2264.2599999999998, "text": " But now let's take a look at this term here.", "tokens": [50886, 583, 586, 718, 311, 747, 257, 574, 412, 341, 1433, 510, 13, 51024], "temperature": 0.0, "avg_logprob": -0.13528922109892874, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.025947576388716698}, {"id": 503, "seek": 225106, "start": 2264.2599999999998, "end": 2268.2599999999998, "text": " This is how we actually compute the gradient.", "tokens": [51024, 639, 307, 577, 321, 767, 14722, 264, 16235, 13, 51224], "temperature": 0.0, "avg_logprob": -0.13528922109892874, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.025947576388716698}, {"id": 504, "seek": 225106, "start": 2268.2599999999998, "end": 2273.22, "text": " This explains how the loss is changing with respect to the weight.", "tokens": [51224, 639, 13948, 577, 264, 4470, 307, 4473, 365, 3104, 281, 264, 3364, 13, 51472], "temperature": 0.0, "avg_logprob": -0.13528922109892874, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.025947576388716698}, {"id": 505, "seek": 225106, "start": 2273.22, "end": 2276.74, "text": " But I never actually told you how we compute this.", "tokens": [51472, 583, 286, 1128, 767, 1907, 291, 577, 321, 14722, 341, 13, 51648], "temperature": 0.0, "avg_logprob": -0.13528922109892874, "compression_ratio": 1.8181818181818181, "no_speech_prob": 0.025947576388716698}, {"id": 506, "seek": 227674, "start": 2276.74, "end": 2282.1, "text": " So let's talk about this process, which is actually extremely important in training neural", "tokens": [50364, 407, 718, 311, 751, 466, 341, 1399, 11, 597, 307, 767, 4664, 1021, 294, 3097, 18161, 50632], "temperature": 0.0, "avg_logprob": -0.10373900486872746, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0005883780540898442}, {"id": 507, "seek": 227674, "start": 2282.1, "end": 2283.1, "text": " networks.", "tokens": [50632, 9590, 13, 50682], "temperature": 0.0, "avg_logprob": -0.10373900486872746, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0005883780540898442}, {"id": 508, "seek": 227674, "start": 2283.1, "end": 2286.8999999999996, "text": " It's known as back propagation.", "tokens": [50682, 467, 311, 2570, 382, 646, 38377, 13, 50872], "temperature": 0.0, "avg_logprob": -0.10373900486872746, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0005883780540898442}, {"id": 509, "seek": 227674, "start": 2286.8999999999996, "end": 2288.3399999999997, "text": " So how does back propagation work?", "tokens": [50872, 407, 577, 775, 646, 38377, 589, 30, 50944], "temperature": 0.0, "avg_logprob": -0.10373900486872746, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0005883780540898442}, {"id": 510, "seek": 227674, "start": 2288.3399999999997, "end": 2290.4199999999996, "text": " How do we compute this gradient?", "tokens": [50944, 1012, 360, 321, 14722, 341, 16235, 30, 51048], "temperature": 0.0, "avg_logprob": -0.10373900486872746, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0005883780540898442}, {"id": 511, "seek": 227674, "start": 2290.4199999999996, "end": 2292.3799999999997, "text": " Let's start with a very simple neural network.", "tokens": [51048, 961, 311, 722, 365, 257, 588, 2199, 18161, 3209, 13, 51146], "temperature": 0.0, "avg_logprob": -0.10373900486872746, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0005883780540898442}, {"id": 512, "seek": 227674, "start": 2292.3799999999997, "end": 2295.7, "text": " This is probably the simplest neural network in existence.", "tokens": [51146, 639, 307, 1391, 264, 22811, 18161, 3209, 294, 9123, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10373900486872746, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0005883780540898442}, {"id": 513, "seek": 227674, "start": 2295.7, "end": 2301.3799999999997, "text": " It only has one input, one hidden neuron, and one output.", "tokens": [51312, 467, 787, 575, 472, 4846, 11, 472, 7633, 34090, 11, 293, 472, 5598, 13, 51596], "temperature": 0.0, "avg_logprob": -0.10373900486872746, "compression_ratio": 1.6545454545454545, "no_speech_prob": 0.0005883780540898442}, {"id": 514, "seek": 230138, "start": 2301.38, "end": 2306.9, "text": " Taking the gradient of our loss, j of w, with respect to one of the weights, in this", "tokens": [50364, 17837, 264, 16235, 295, 527, 4470, 11, 361, 295, 261, 11, 365, 3104, 281, 472, 295, 264, 17443, 11, 294, 341, 50640], "temperature": 0.0, "avg_logprob": -0.1492380122749173, "compression_ratio": 1.645, "no_speech_prob": 0.15602301061153412}, {"id": 515, "seek": 230138, "start": 2306.9, "end": 2316.6600000000003, "text": " case just w2, for example, tells us how much a small change in w2 is going to affect our", "tokens": [50640, 1389, 445, 261, 17, 11, 337, 1365, 11, 5112, 505, 577, 709, 257, 1359, 1319, 294, 261, 17, 307, 516, 281, 3345, 527, 51128], "temperature": 0.0, "avg_logprob": -0.1492380122749173, "compression_ratio": 1.645, "no_speech_prob": 0.15602301061153412}, {"id": 516, "seek": 230138, "start": 2316.6600000000003, "end": 2318.1800000000003, "text": " loss, j.", "tokens": [51128, 4470, 11, 361, 13, 51204], "temperature": 0.0, "avg_logprob": -0.1492380122749173, "compression_ratio": 1.645, "no_speech_prob": 0.15602301061153412}, {"id": 517, "seek": 230138, "start": 2318.1800000000003, "end": 2324.6600000000003, "text": " So if we move around j, infinitesimally small, how will that affect our loss?", "tokens": [51204, 407, 498, 321, 1286, 926, 361, 11, 7193, 3324, 332, 379, 1359, 11, 577, 486, 300, 3345, 527, 4470, 30, 51528], "temperature": 0.0, "avg_logprob": -0.1492380122749173, "compression_ratio": 1.645, "no_speech_prob": 0.15602301061153412}, {"id": 518, "seek": 230138, "start": 2324.6600000000003, "end": 2330.1, "text": " That's what the gradient is going to tell us, derivative of j of w2.", "tokens": [51528, 663, 311, 437, 264, 16235, 307, 516, 281, 980, 505, 11, 13760, 295, 361, 295, 261, 17, 13, 51800], "temperature": 0.0, "avg_logprob": -0.1492380122749173, "compression_ratio": 1.645, "no_speech_prob": 0.15602301061153412}, {"id": 519, "seek": 233010, "start": 2330.1, "end": 2335.86, "text": " So if we write out this derivative, we can actually apply the chain rule to actually", "tokens": [50364, 407, 498, 321, 2464, 484, 341, 13760, 11, 321, 393, 767, 3079, 264, 5021, 4978, 281, 767, 50652], "temperature": 0.0, "avg_logprob": -0.16946955288157745, "compression_ratio": 1.598802395209581, "no_speech_prob": 0.006903067696839571}, {"id": 520, "seek": 233010, "start": 2335.86, "end": 2337.02, "text": " compute it.", "tokens": [50652, 14722, 309, 13, 50710], "temperature": 0.0, "avg_logprob": -0.16946955288157745, "compression_ratio": 1.598802395209581, "no_speech_prob": 0.006903067696839571}, {"id": 521, "seek": 233010, "start": 2337.02, "end": 2338.7, "text": " So what does that look like?", "tokens": [50710, 407, 437, 775, 300, 574, 411, 30, 50794], "temperature": 0.0, "avg_logprob": -0.16946955288157745, "compression_ratio": 1.598802395209581, "no_speech_prob": 0.006903067696839571}, {"id": 522, "seek": 233010, "start": 2338.7, "end": 2351.98, "text": " Specifically, we can decompose that derivative into the derivative of j dw over dy multiplied", "tokens": [50794, 26058, 11, 321, 393, 22867, 541, 300, 13760, 666, 264, 13760, 295, 361, 274, 86, 670, 14584, 17207, 51458], "temperature": 0.0, "avg_logprob": -0.16946955288157745, "compression_ratio": 1.598802395209581, "no_speech_prob": 0.006903067696839571}, {"id": 523, "seek": 233010, "start": 2351.98, "end": 2357.5, "text": " by derivative of our output with respect to w2.", "tokens": [51458, 538, 13760, 295, 527, 5598, 365, 3104, 281, 261, 17, 13, 51734], "temperature": 0.0, "avg_logprob": -0.16946955288157745, "compression_ratio": 1.598802395209581, "no_speech_prob": 0.006903067696839571}, {"id": 524, "seek": 235750, "start": 2357.5, "end": 2360.82, "text": " Now the question here is with the second part.", "tokens": [50364, 823, 264, 1168, 510, 307, 365, 264, 1150, 644, 13, 50530], "temperature": 0.0, "avg_logprob": -0.10446342669035259, "compression_ratio": 1.76, "no_speech_prob": 0.09004271030426025}, {"id": 525, "seek": 235750, "start": 2360.82, "end": 2366.38, "text": " If we want to compute now not the derivative of our loss with respect to w2, but now the", "tokens": [50530, 759, 321, 528, 281, 14722, 586, 406, 264, 13760, 295, 527, 4470, 365, 3104, 281, 261, 17, 11, 457, 586, 264, 50808], "temperature": 0.0, "avg_logprob": -0.10446342669035259, "compression_ratio": 1.76, "no_speech_prob": 0.09004271030426025}, {"id": 526, "seek": 235750, "start": 2366.38, "end": 2370.58, "text": " loss with respect to w1, we can do the same story as before.", "tokens": [50808, 4470, 365, 3104, 281, 261, 16, 11, 321, 393, 360, 264, 912, 1657, 382, 949, 13, 51018], "temperature": 0.0, "avg_logprob": -0.10446342669035259, "compression_ratio": 1.76, "no_speech_prob": 0.09004271030426025}, {"id": 527, "seek": 235750, "start": 2370.58, "end": 2375.26, "text": " We can apply the chain rule now recursively.", "tokens": [51018, 492, 393, 3079, 264, 5021, 4978, 586, 20560, 3413, 13, 51252], "temperature": 0.0, "avg_logprob": -0.10446342669035259, "compression_ratio": 1.76, "no_speech_prob": 0.09004271030426025}, {"id": 528, "seek": 235750, "start": 2375.26, "end": 2379.54, "text": " So now we have to apply the chain rule again to the second part.", "tokens": [51252, 407, 586, 321, 362, 281, 3079, 264, 5021, 4978, 797, 281, 264, 1150, 644, 13, 51466], "temperature": 0.0, "avg_logprob": -0.10446342669035259, "compression_ratio": 1.76, "no_speech_prob": 0.09004271030426025}, {"id": 529, "seek": 235750, "start": 2379.54, "end": 2382.78, "text": " Now the second part is expanded even further.", "tokens": [51466, 823, 264, 1150, 644, 307, 14342, 754, 3052, 13, 51628], "temperature": 0.0, "avg_logprob": -0.10446342669035259, "compression_ratio": 1.76, "no_speech_prob": 0.09004271030426025}, {"id": 530, "seek": 238278, "start": 2382.78, "end": 2388.6200000000003, "text": " So the derivative of our output with respect to z1, which is the activation function of", "tokens": [50364, 407, 264, 13760, 295, 527, 5598, 365, 3104, 281, 710, 16, 11, 597, 307, 264, 24433, 2445, 295, 50656], "temperature": 0.0, "avg_logprob": -0.1170364002605061, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.008061148226261139}, {"id": 531, "seek": 238278, "start": 2388.6200000000003, "end": 2390.6600000000003, "text": " this first hidden unit.", "tokens": [50656, 341, 700, 7633, 4985, 13, 50758], "temperature": 0.0, "avg_logprob": -0.1170364002605061, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.008061148226261139}, {"id": 532, "seek": 238278, "start": 2390.6600000000003, "end": 2392.7000000000003, "text": " And we can back propagate this information now.", "tokens": [50758, 400, 321, 393, 646, 48256, 341, 1589, 586, 13, 50860], "temperature": 0.0, "avg_logprob": -0.1170364002605061, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.008061148226261139}, {"id": 533, "seek": 238278, "start": 2392.7000000000003, "end": 2399.1400000000003, "text": " You can see starting from our loss all the way through w2 and then recursively applying", "tokens": [50860, 509, 393, 536, 2891, 490, 527, 4470, 439, 264, 636, 807, 261, 17, 293, 550, 20560, 3413, 9275, 51182], "temperature": 0.0, "avg_logprob": -0.1170364002605061, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.008061148226261139}, {"id": 534, "seek": 238278, "start": 2399.1400000000003, "end": 2402.5800000000004, "text": " this chain rule again to get to w1.", "tokens": [51182, 341, 5021, 4978, 797, 281, 483, 281, 261, 16, 13, 51354], "temperature": 0.0, "avg_logprob": -0.1170364002605061, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.008061148226261139}, {"id": 535, "seek": 238278, "start": 2402.5800000000004, "end": 2408.34, "text": " And this allows us to see both the gradient at both w2 and w1.", "tokens": [51354, 400, 341, 4045, 505, 281, 536, 1293, 264, 16235, 412, 1293, 261, 17, 293, 261, 16, 13, 51642], "temperature": 0.0, "avg_logprob": -0.1170364002605061, "compression_ratio": 1.5799086757990868, "no_speech_prob": 0.008061148226261139}, {"id": 536, "seek": 240834, "start": 2408.34, "end": 2415.86, "text": " So in this case, just to reiterate once again, this is telling us this dj dw1 is telling", "tokens": [50364, 407, 294, 341, 1389, 11, 445, 281, 33528, 1564, 797, 11, 341, 307, 3585, 505, 341, 274, 73, 274, 86, 16, 307, 3585, 50740], "temperature": 0.0, "avg_logprob": -0.09052233739730416, "compression_ratio": 1.9360730593607305, "no_speech_prob": 0.006903444416821003}, {"id": 537, "seek": 240834, "start": 2415.86, "end": 2421.1200000000003, "text": " us how a small change in our weight is going to affect our loss.", "tokens": [50740, 505, 577, 257, 1359, 1319, 294, 527, 3364, 307, 516, 281, 3345, 527, 4470, 13, 51003], "temperature": 0.0, "avg_logprob": -0.09052233739730416, "compression_ratio": 1.9360730593607305, "no_speech_prob": 0.006903444416821003}, {"id": 538, "seek": 240834, "start": 2421.1200000000003, "end": 2425.42, "text": " So we can see if we increase our weight a small amount, it will increase our loss.", "tokens": [51003, 407, 321, 393, 536, 498, 321, 3488, 527, 3364, 257, 1359, 2372, 11, 309, 486, 3488, 527, 4470, 13, 51218], "temperature": 0.0, "avg_logprob": -0.09052233739730416, "compression_ratio": 1.9360730593607305, "no_speech_prob": 0.006903444416821003}, {"id": 539, "seek": 240834, "start": 2425.42, "end": 2429.38, "text": " That means we will want to decrease the weight to decrease our loss.", "tokens": [51218, 663, 1355, 321, 486, 528, 281, 11514, 264, 3364, 281, 11514, 527, 4470, 13, 51416], "temperature": 0.0, "avg_logprob": -0.09052233739730416, "compression_ratio": 1.9360730593607305, "no_speech_prob": 0.006903444416821003}, {"id": 540, "seek": 240834, "start": 2429.38, "end": 2430.82, "text": " That's what the gradient tells us.", "tokens": [51416, 663, 311, 437, 264, 16235, 5112, 505, 13, 51488], "temperature": 0.0, "avg_logprob": -0.09052233739730416, "compression_ratio": 1.9360730593607305, "no_speech_prob": 0.006903444416821003}, {"id": 541, "seek": 240834, "start": 2430.82, "end": 2438.1800000000003, "text": " Which direction we need to step in order to decrease or increase our loss function.", "tokens": [51488, 3013, 3513, 321, 643, 281, 1823, 294, 1668, 281, 11514, 420, 3488, 527, 4470, 2445, 13, 51856], "temperature": 0.0, "avg_logprob": -0.09052233739730416, "compression_ratio": 1.9360730593607305, "no_speech_prob": 0.006903444416821003}, {"id": 542, "seek": 243818, "start": 2438.18, "end": 2441.66, "text": " Now we showed this here for just two weights in our neural network because we only have", "tokens": [50364, 823, 321, 4712, 341, 510, 337, 445, 732, 17443, 294, 527, 18161, 3209, 570, 321, 787, 362, 50538], "temperature": 0.0, "avg_logprob": -0.11555468819358132, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.020956864580512047}, {"id": 543, "seek": 243818, "start": 2441.66, "end": 2442.66, "text": " two weights.", "tokens": [50538, 732, 17443, 13, 50588], "temperature": 0.0, "avg_logprob": -0.11555468819358132, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.020956864580512047}, {"id": 544, "seek": 243818, "start": 2442.66, "end": 2445.58, "text": " But imagine we have a very deep neural network.", "tokens": [50588, 583, 3811, 321, 362, 257, 588, 2452, 18161, 3209, 13, 50734], "temperature": 0.0, "avg_logprob": -0.11555468819358132, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.020956864580512047}, {"id": 545, "seek": 243818, "start": 2445.58, "end": 2451.3799999999997, "text": " One with more than just two layers of or one layer rather of hidden units.", "tokens": [50734, 1485, 365, 544, 813, 445, 732, 7914, 295, 420, 472, 4583, 2831, 295, 7633, 6815, 13, 51024], "temperature": 0.0, "avg_logprob": -0.11555468819358132, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.020956864580512047}, {"id": 546, "seek": 243818, "start": 2451.3799999999997, "end": 2456.98, "text": " We can just repeat this process of applying, recursively applying the chain rule to determine", "tokens": [51024, 492, 393, 445, 7149, 341, 1399, 295, 9275, 11, 20560, 3413, 9275, 264, 5021, 4978, 281, 6997, 51304], "temperature": 0.0, "avg_logprob": -0.11555468819358132, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.020956864580512047}, {"id": 547, "seek": 243818, "start": 2456.98, "end": 2462.54, "text": " how every single way in the model needs to change to impact that loss.", "tokens": [51304, 577, 633, 2167, 636, 294, 264, 2316, 2203, 281, 1319, 281, 2712, 300, 4470, 13, 51582], "temperature": 0.0, "avg_logprob": -0.11555468819358132, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.020956864580512047}, {"id": 548, "seek": 243818, "start": 2462.54, "end": 2467.4199999999996, "text": " But really all this boils down to just recursively applying this chain rule formulation that", "tokens": [51582, 583, 534, 439, 341, 35049, 760, 281, 445, 20560, 3413, 9275, 341, 5021, 4978, 37642, 300, 51826], "temperature": 0.0, "avg_logprob": -0.11555468819358132, "compression_ratio": 1.821969696969697, "no_speech_prob": 0.020956864580512047}, {"id": 549, "seek": 246742, "start": 2467.42, "end": 2470.62, "text": " you can see here.", "tokens": [50364, 291, 393, 536, 510, 13, 50524], "temperature": 0.0, "avg_logprob": -0.15784001023802038, "compression_ratio": 1.53, "no_speech_prob": 0.01911715231835842}, {"id": 550, "seek": 246742, "start": 2470.62, "end": 2472.86, "text": " And that's the back propagation algorithm.", "tokens": [50524, 400, 300, 311, 264, 646, 38377, 9284, 13, 50636], "temperature": 0.0, "avg_logprob": -0.15784001023802038, "compression_ratio": 1.53, "no_speech_prob": 0.01911715231835842}, {"id": 551, "seek": 246742, "start": 2472.86, "end": 2475.2200000000003, "text": " In theory it sounds very simple.", "tokens": [50636, 682, 5261, 309, 3263, 588, 2199, 13, 50754], "temperature": 0.0, "avg_logprob": -0.15784001023802038, "compression_ratio": 1.53, "no_speech_prob": 0.01911715231835842}, {"id": 552, "seek": 246742, "start": 2475.2200000000003, "end": 2482.5, "text": " It's just a very basic extension on derivatives and the chain rule.", "tokens": [50754, 467, 311, 445, 257, 588, 3875, 10320, 322, 33733, 293, 264, 5021, 4978, 13, 51118], "temperature": 0.0, "avg_logprob": -0.15784001023802038, "compression_ratio": 1.53, "no_speech_prob": 0.01911715231835842}, {"id": 553, "seek": 246742, "start": 2482.5, "end": 2487.9, "text": " But now let's actually touch on some insights from training these networks in practice that", "tokens": [51118, 583, 586, 718, 311, 767, 2557, 322, 512, 14310, 490, 3097, 613, 9590, 294, 3124, 300, 51388], "temperature": 0.0, "avg_logprob": -0.15784001023802038, "compression_ratio": 1.53, "no_speech_prob": 0.01911715231835842}, {"id": 554, "seek": 246742, "start": 2487.9, "end": 2492.34, "text": " make this process much more complicated in practice.", "tokens": [51388, 652, 341, 1399, 709, 544, 6179, 294, 3124, 13, 51610], "temperature": 0.0, "avg_logprob": -0.15784001023802038, "compression_ratio": 1.53, "no_speech_prob": 0.01911715231835842}, {"id": 555, "seek": 249234, "start": 2492.34, "end": 2497.54, "text": " And why using back propagation as we saw there is not always so easy.", "tokens": [50364, 400, 983, 1228, 646, 38377, 382, 321, 1866, 456, 307, 406, 1009, 370, 1858, 13, 50624], "temperature": 0.0, "avg_logprob": -0.15041548496968038, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09005482494831085}, {"id": 556, "seek": 249234, "start": 2497.54, "end": 2502.86, "text": " Now in practice training neural networks and optimization of networks can be extremely", "tokens": [50624, 823, 294, 3124, 3097, 18161, 9590, 293, 19618, 295, 9590, 393, 312, 4664, 50890], "temperature": 0.0, "avg_logprob": -0.15041548496968038, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09005482494831085}, {"id": 557, "seek": 249234, "start": 2502.86, "end": 2507.42, "text": " difficult and it's actually extremely computationally intensive.", "tokens": [50890, 2252, 293, 309, 311, 767, 4664, 24903, 379, 18957, 13, 51118], "temperature": 0.0, "avg_logprob": -0.15041548496968038, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09005482494831085}, {"id": 558, "seek": 249234, "start": 2507.42, "end": 2514.42, "text": " Here's the visualization of what a loss landscape of a real neural network can look like visualized", "tokens": [51118, 1692, 311, 264, 25801, 295, 437, 257, 4470, 9661, 295, 257, 957, 18161, 3209, 393, 574, 411, 5056, 1602, 51468], "temperature": 0.0, "avg_logprob": -0.15041548496968038, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09005482494831085}, {"id": 559, "seek": 249234, "start": 2514.42, "end": 2516.9, "text": " on just two dimensions.", "tokens": [51468, 322, 445, 732, 12819, 13, 51592], "temperature": 0.0, "avg_logprob": -0.15041548496968038, "compression_ratio": 1.6666666666666667, "no_speech_prob": 0.09005482494831085}, {"id": 560, "seek": 251690, "start": 2516.9, "end": 2523.38, "text": " Now you can see here that the loss is extremely non-convex meaning that it has many, many", "tokens": [50364, 823, 291, 393, 536, 510, 300, 264, 4470, 307, 4664, 2107, 12, 1671, 303, 87, 3620, 300, 309, 575, 867, 11, 867, 50688], "temperature": 0.0, "avg_logprob": -0.12658709028492804, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.002182524651288986}, {"id": 561, "seek": 251690, "start": 2523.38, "end": 2525.5, "text": " local minimum.", "tokens": [50688, 2654, 7285, 13, 50794], "temperature": 0.0, "avg_logprob": -0.12658709028492804, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.002182524651288986}, {"id": 562, "seek": 251690, "start": 2525.5, "end": 2531.02, "text": " That can make using an algorithm like gradient descent very, very challenging because gradient", "tokens": [50794, 663, 393, 652, 1228, 364, 9284, 411, 16235, 23475, 588, 11, 588, 7595, 570, 16235, 51070], "temperature": 0.0, "avg_logprob": -0.12658709028492804, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.002182524651288986}, {"id": 563, "seek": 251690, "start": 2531.02, "end": 2535.62, "text": " descent is always going to step closest to the first local minimum but it can always", "tokens": [51070, 23475, 307, 1009, 516, 281, 1823, 13699, 281, 264, 700, 2654, 7285, 457, 309, 393, 1009, 51300], "temperature": 0.0, "avg_logprob": -0.12658709028492804, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.002182524651288986}, {"id": 564, "seek": 251690, "start": 2535.62, "end": 2536.78, "text": " get stuck there.", "tokens": [51300, 483, 5541, 456, 13, 51358], "temperature": 0.0, "avg_logprob": -0.12658709028492804, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.002182524651288986}, {"id": 565, "seek": 251690, "start": 2536.78, "end": 2542.14, "text": " So finding how to get to the global minima or a really good solution for your neural", "tokens": [51358, 407, 5006, 577, 281, 483, 281, 264, 4338, 4464, 64, 420, 257, 534, 665, 3827, 337, 428, 18161, 51626], "temperature": 0.0, "avg_logprob": -0.12658709028492804, "compression_ratio": 1.7004405286343611, "no_speech_prob": 0.002182524651288986}, {"id": 566, "seek": 254214, "start": 2542.14, "end": 2548.8599999999997, "text": " network can often be very sensitive to your hyper parameter such as where the optimizer", "tokens": [50364, 3209, 393, 2049, 312, 588, 9477, 281, 428, 9848, 13075, 1270, 382, 689, 264, 5028, 6545, 50700], "temperature": 0.0, "avg_logprob": -0.1222672700881958, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0038241965230554342}, {"id": 567, "seek": 254214, "start": 2548.8599999999997, "end": 2551.14, "text": " starts in this loss landscape.", "tokens": [50700, 3719, 294, 341, 4470, 9661, 13, 50814], "temperature": 0.0, "avg_logprob": -0.1222672700881958, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0038241965230554342}, {"id": 568, "seek": 254214, "start": 2551.14, "end": 2556.06, "text": " If it starts in a potentially bad part of the landscape it can very easily get stuck", "tokens": [50814, 759, 309, 3719, 294, 257, 7263, 1578, 644, 295, 264, 9661, 309, 393, 588, 3612, 483, 5541, 51060], "temperature": 0.0, "avg_logprob": -0.1222672700881958, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0038241965230554342}, {"id": 569, "seek": 254214, "start": 2556.06, "end": 2559.8199999999997, "text": " in one of these local minimum.", "tokens": [51060, 294, 472, 295, 613, 2654, 7285, 13, 51248], "temperature": 0.0, "avg_logprob": -0.1222672700881958, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0038241965230554342}, {"id": 570, "seek": 254214, "start": 2559.8199999999997, "end": 2564.58, "text": " Now recall the equation that we talked about for gradient descent.", "tokens": [51248, 823, 9901, 264, 5367, 300, 321, 2825, 466, 337, 16235, 23475, 13, 51486], "temperature": 0.0, "avg_logprob": -0.1222672700881958, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0038241965230554342}, {"id": 571, "seek": 254214, "start": 2564.58, "end": 2565.94, "text": " This was the equation I showed you.", "tokens": [51486, 639, 390, 264, 5367, 286, 4712, 291, 13, 51554], "temperature": 0.0, "avg_logprob": -0.1222672700881958, "compression_ratio": 1.5971563981042654, "no_speech_prob": 0.0038241965230554342}, {"id": 572, "seek": 256594, "start": 2565.94, "end": 2572.2200000000003, "text": " Our next weight update is going to be your current weights minus a small amount called", "tokens": [50364, 2621, 958, 3364, 5623, 307, 516, 281, 312, 428, 2190, 17443, 3175, 257, 1359, 2372, 1219, 50678], "temperature": 0.0, "avg_logprob": -0.11411062876383464, "compression_ratio": 1.9271255060728745, "no_speech_prob": 0.25663381814956665}, {"id": 573, "seek": 256594, "start": 2572.2200000000003, "end": 2574.5, "text": " the learning rate multiplied by the gradient.", "tokens": [50678, 264, 2539, 3314, 17207, 538, 264, 16235, 13, 50792], "temperature": 0.0, "avg_logprob": -0.11411062876383464, "compression_ratio": 1.9271255060728745, "no_speech_prob": 0.25663381814956665}, {"id": 574, "seek": 256594, "start": 2574.5, "end": 2580.2200000000003, "text": " So we have this minus sign because we want to step in the opposite direction and we multiply", "tokens": [50792, 407, 321, 362, 341, 3175, 1465, 570, 321, 528, 281, 1823, 294, 264, 6182, 3513, 293, 321, 12972, 51078], "temperature": 0.0, "avg_logprob": -0.11411062876383464, "compression_ratio": 1.9271255060728745, "no_speech_prob": 0.25663381814956665}, {"id": 575, "seek": 256594, "start": 2580.2200000000003, "end": 2584.86, "text": " it by the gradient or we multiply it by the small number called here called eta which", "tokens": [51078, 309, 538, 264, 16235, 420, 321, 12972, 309, 538, 264, 1359, 1230, 1219, 510, 1219, 32415, 597, 51310], "temperature": 0.0, "avg_logprob": -0.11411062876383464, "compression_ratio": 1.9271255060728745, "no_speech_prob": 0.25663381814956665}, {"id": 576, "seek": 256594, "start": 2584.86, "end": 2588.38, "text": " is what we call the learning rate.", "tokens": [51310, 307, 437, 321, 818, 264, 2539, 3314, 13, 51486], "temperature": 0.0, "avg_logprob": -0.11411062876383464, "compression_ratio": 1.9271255060728745, "no_speech_prob": 0.25663381814956665}, {"id": 577, "seek": 256594, "start": 2588.38, "end": 2591.18, "text": " How fast do we want to do the learning?", "tokens": [51486, 1012, 2370, 360, 321, 528, 281, 360, 264, 2539, 30, 51626], "temperature": 0.0, "avg_logprob": -0.11411062876383464, "compression_ratio": 1.9271255060728745, "no_speech_prob": 0.25663381814956665}, {"id": 578, "seek": 256594, "start": 2591.18, "end": 2595.14, "text": " Now it determines actually not just how fast to do the learning that's maybe not the best", "tokens": [51626, 823, 309, 24799, 767, 406, 445, 577, 2370, 281, 360, 264, 2539, 300, 311, 1310, 406, 264, 1151, 51824], "temperature": 0.0, "avg_logprob": -0.11411062876383464, "compression_ratio": 1.9271255060728745, "no_speech_prob": 0.25663381814956665}, {"id": 579, "seek": 259514, "start": 2595.18, "end": 2602.2999999999997, "text": " way to say it but it tells us how large should each step we take in practice be with regards", "tokens": [50366, 636, 281, 584, 309, 457, 309, 5112, 505, 577, 2416, 820, 1184, 1823, 321, 747, 294, 3124, 312, 365, 14258, 50722], "temperature": 0.0, "avg_logprob": -0.1056166343318606, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.0340883694589138}, {"id": 580, "seek": 259514, "start": 2602.2999999999997, "end": 2603.62, "text": " to that gradient.", "tokens": [50722, 281, 300, 16235, 13, 50788], "temperature": 0.0, "avg_logprob": -0.1056166343318606, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.0340883694589138}, {"id": 581, "seek": 259514, "start": 2603.62, "end": 2607.94, "text": " So the gradient tells us the direction but it doesn't necessarily tell us the magnitude", "tokens": [50788, 407, 264, 16235, 5112, 505, 264, 3513, 457, 309, 1177, 380, 4725, 980, 505, 264, 15668, 51004], "temperature": 0.0, "avg_logprob": -0.1056166343318606, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.0340883694589138}, {"id": 582, "seek": 259514, "start": 2607.94, "end": 2609.2999999999997, "text": " of the direction.", "tokens": [51004, 295, 264, 3513, 13, 51072], "temperature": 0.0, "avg_logprob": -0.1056166343318606, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.0340883694589138}, {"id": 583, "seek": 259514, "start": 2609.2999999999997, "end": 2616.18, "text": " So eta can tell us actually a scale of how much we want to trust that gradient and step", "tokens": [51072, 407, 32415, 393, 980, 505, 767, 257, 4373, 295, 577, 709, 321, 528, 281, 3361, 300, 16235, 293, 1823, 51416], "temperature": 0.0, "avg_logprob": -0.1056166343318606, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.0340883694589138}, {"id": 584, "seek": 259514, "start": 2616.18, "end": 2618.2999999999997, "text": " in the direction of that gradient.", "tokens": [51416, 294, 264, 3513, 295, 300, 16235, 13, 51522], "temperature": 0.0, "avg_logprob": -0.1056166343318606, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.0340883694589138}, {"id": 585, "seek": 259514, "start": 2618.2999999999997, "end": 2623.54, "text": " In practice setting even eta, this one parameter, this one number can be extremely difficult", "tokens": [51522, 682, 3124, 3287, 754, 32415, 11, 341, 472, 13075, 11, 341, 472, 1230, 393, 312, 4664, 2252, 51784], "temperature": 0.0, "avg_logprob": -0.1056166343318606, "compression_ratio": 1.8864628820960698, "no_speech_prob": 0.0340883694589138}, {"id": 586, "seek": 262354, "start": 2623.54, "end": 2627.18, "text": " and I want to give you a quick example of why.", "tokens": [50364, 293, 286, 528, 281, 976, 291, 257, 1702, 1365, 295, 983, 13, 50546], "temperature": 0.0, "avg_logprob": -0.1337637853140783, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004878394538536668}, {"id": 587, "seek": 262354, "start": 2627.18, "end": 2634.7, "text": " So if you have a very non-convex or loss landscape where you have local minima, if you set the", "tokens": [50546, 407, 498, 291, 362, 257, 588, 2107, 12, 1671, 303, 87, 420, 4470, 9661, 689, 291, 362, 2654, 4464, 64, 11, 498, 291, 992, 264, 50922], "temperature": 0.0, "avg_logprob": -0.1337637853140783, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004878394538536668}, {"id": 588, "seek": 262354, "start": 2634.7, "end": 2639.58, "text": " learning rate too low then the model can get stuck in these local minima.", "tokens": [50922, 2539, 3314, 886, 2295, 550, 264, 2316, 393, 483, 5541, 294, 613, 2654, 4464, 64, 13, 51166], "temperature": 0.0, "avg_logprob": -0.1337637853140783, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004878394538536668}, {"id": 589, "seek": 262354, "start": 2639.58, "end": 2644.62, "text": " It can never escape them because it actually does optimize itself but it optimizes it to", "tokens": [51166, 467, 393, 1128, 7615, 552, 570, 309, 767, 775, 19719, 2564, 457, 309, 5028, 5660, 309, 281, 51418], "temperature": 0.0, "avg_logprob": -0.1337637853140783, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004878394538536668}, {"id": 590, "seek": 262354, "start": 2644.62, "end": 2652.3, "text": " a very non-optimal minima and it can converge very slowly as well.", "tokens": [51418, 257, 588, 2107, 12, 5747, 10650, 4464, 64, 293, 309, 393, 41881, 588, 5692, 382, 731, 13, 51802], "temperature": 0.0, "avg_logprob": -0.1337637853140783, "compression_ratio": 1.7175925925925926, "no_speech_prob": 0.0004878394538536668}, {"id": 591, "seek": 265230, "start": 2652.3, "end": 2657.6600000000003, "text": " On the other hand if we increase our learning rate too much then we can actually overshoot", "tokens": [50364, 1282, 264, 661, 1011, 498, 321, 3488, 527, 2539, 3314, 886, 709, 550, 321, 393, 767, 15488, 24467, 50632], "temperature": 0.0, "avg_logprob": -0.16190369536237018, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.004331164062023163}, {"id": 592, "seek": 265230, "start": 2657.6600000000003, "end": 2665.5800000000004, "text": " our minima and actually diverge and lose control and basically explode the training process", "tokens": [50632, 527, 4464, 64, 293, 767, 18558, 432, 293, 3624, 1969, 293, 1936, 21411, 264, 3097, 1399, 51028], "temperature": 0.0, "avg_logprob": -0.16190369536237018, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.004331164062023163}, {"id": 593, "seek": 265230, "start": 2665.5800000000004, "end": 2666.78, "text": " completely.", "tokens": [51028, 2584, 13, 51088], "temperature": 0.0, "avg_logprob": -0.16190369536237018, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.004331164062023163}, {"id": 594, "seek": 265230, "start": 2666.78, "end": 2673.5800000000004, "text": " One of the challenges is actually how to use stable learning rates that are large enough", "tokens": [51088, 1485, 295, 264, 4759, 307, 767, 577, 281, 764, 8351, 2539, 6846, 300, 366, 2416, 1547, 51428], "temperature": 0.0, "avg_logprob": -0.16190369536237018, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.004331164062023163}, {"id": 595, "seek": 265230, "start": 2673.5800000000004, "end": 2681.78, "text": " to avoid the local minima but small enough so that they don't diverge completely.", "tokens": [51428, 281, 5042, 264, 2654, 4464, 64, 457, 1359, 1547, 370, 300, 436, 500, 380, 18558, 432, 2584, 13, 51838], "temperature": 0.0, "avg_logprob": -0.16190369536237018, "compression_ratio": 1.7718446601941749, "no_speech_prob": 0.004331164062023163}, {"id": 596, "seek": 268178, "start": 2682.42, "end": 2689.94, "text": " So they're small enough to actually converge to that global spot once they reach it.", "tokens": [50396, 407, 436, 434, 1359, 1547, 281, 767, 41881, 281, 300, 4338, 4008, 1564, 436, 2524, 309, 13, 50772], "temperature": 0.0, "avg_logprob": -0.14586966378348215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.009705842472612858}, {"id": 597, "seek": 268178, "start": 2689.94, "end": 2691.94, "text": " So how can we actually set this learning rate?", "tokens": [50772, 407, 577, 393, 321, 767, 992, 341, 2539, 3314, 30, 50872], "temperature": 0.0, "avg_logprob": -0.14586966378348215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.009705842472612858}, {"id": 598, "seek": 268178, "start": 2691.94, "end": 2697.7000000000003, "text": " Well one option which is actually somewhat popular in practice is to actually just try", "tokens": [50872, 1042, 472, 3614, 597, 307, 767, 8344, 3743, 294, 3124, 307, 281, 767, 445, 853, 51160], "temperature": 0.0, "avg_logprob": -0.14586966378348215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.009705842472612858}, {"id": 599, "seek": 268178, "start": 2697.7000000000003, "end": 2701.6200000000003, "text": " a lot of different learning rates and that actually works.", "tokens": [51160, 257, 688, 295, 819, 2539, 6846, 293, 300, 767, 1985, 13, 51356], "temperature": 0.0, "avg_logprob": -0.14586966378348215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.009705842472612858}, {"id": 600, "seek": 268178, "start": 2701.6200000000003, "end": 2706.5400000000004, "text": " It is a feasible approach but let's see if we can do something a little bit smarter than", "tokens": [51356, 467, 307, 257, 26648, 3109, 457, 718, 311, 536, 498, 321, 393, 360, 746, 257, 707, 857, 20294, 813, 51602], "temperature": 0.0, "avg_logprob": -0.14586966378348215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.009705842472612858}, {"id": 601, "seek": 268178, "start": 2706.5400000000004, "end": 2708.3, "text": " that, more intelligent.", "tokens": [51602, 300, 11, 544, 13232, 13, 51690], "temperature": 0.0, "avg_logprob": -0.14586966378348215, "compression_ratio": 1.6810344827586208, "no_speech_prob": 0.009705842472612858}, {"id": 602, "seek": 270830, "start": 2708.34, "end": 2714.02, "text": " What if we could say instead how can we build an adaptive learning rate that actually looks", "tokens": [50366, 708, 498, 321, 727, 584, 2602, 577, 393, 321, 1322, 364, 27912, 2539, 3314, 300, 767, 1542, 50650], "temperature": 0.0, "avg_logprob": -0.09265074619027071, "compression_ratio": 1.71875, "no_speech_prob": 0.00034597792546264827}, {"id": 603, "seek": 270830, "start": 2714.02, "end": 2720.0600000000004, "text": " at its lost landscape and adapts itself to account for what it sees in the landscape.", "tokens": [50650, 412, 1080, 2731, 9661, 293, 23169, 1373, 2564, 281, 2696, 337, 437, 309, 8194, 294, 264, 9661, 13, 50952], "temperature": 0.0, "avg_logprob": -0.09265074619027071, "compression_ratio": 1.71875, "no_speech_prob": 0.00034597792546264827}, {"id": 604, "seek": 270830, "start": 2720.0600000000004, "end": 2724.02, "text": " There are actually many types of optimizers that do exactly this.", "tokens": [50952, 821, 366, 767, 867, 3467, 295, 5028, 22525, 300, 360, 2293, 341, 13, 51150], "temperature": 0.0, "avg_logprob": -0.09265074619027071, "compression_ratio": 1.71875, "no_speech_prob": 0.00034597792546264827}, {"id": 605, "seek": 270830, "start": 2724.02, "end": 2726.98, "text": " This means that the learning rates are no longer fixed.", "tokens": [51150, 639, 1355, 300, 264, 2539, 6846, 366, 572, 2854, 6806, 13, 51298], "temperature": 0.0, "avg_logprob": -0.09265074619027071, "compression_ratio": 1.71875, "no_speech_prob": 0.00034597792546264827}, {"id": 606, "seek": 270830, "start": 2726.98, "end": 2732.1400000000003, "text": " They can increase or decrease depending on how large the gradient is in that location", "tokens": [51298, 814, 393, 3488, 420, 11514, 5413, 322, 577, 2416, 264, 16235, 307, 294, 300, 4914, 51556], "temperature": 0.0, "avg_logprob": -0.09265074619027071, "compression_ratio": 1.71875, "no_speech_prob": 0.00034597792546264827}, {"id": 607, "seek": 273214, "start": 2732.2599999999998, "end": 2739.2599999999998, "text": " and how fast we want and how fast we're actually learning and many other options.", "tokens": [50370, 293, 577, 2370, 321, 528, 293, 577, 2370, 321, 434, 767, 2539, 293, 867, 661, 3956, 13, 50720], "temperature": 0.0, "avg_logprob": -0.16796215057373046, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0007793150143697858}, {"id": 608, "seek": 273214, "start": 2739.2599999999998, "end": 2744.2599999999998, "text": " They could be also with regards to the size of the weights at that point, the magnitudes, etc.", "tokens": [50720, 814, 727, 312, 611, 365, 14258, 281, 264, 2744, 295, 264, 17443, 412, 300, 935, 11, 264, 4944, 16451, 11, 5183, 13, 50970], "temperature": 0.0, "avg_logprob": -0.16796215057373046, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0007793150143697858}, {"id": 609, "seek": 273214, "start": 2746.2599999999998, "end": 2752.58, "text": " In fact these have been widely explored and published as part of TensorFlow as well and", "tokens": [51070, 682, 1186, 613, 362, 668, 13371, 24016, 293, 6572, 382, 644, 295, 37624, 382, 731, 293, 51386], "temperature": 0.0, "avg_logprob": -0.16796215057373046, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0007793150143697858}, {"id": 610, "seek": 273214, "start": 2752.58, "end": 2756.42, "text": " during your labs we encourage each of you to really try out each of these different types", "tokens": [51386, 1830, 428, 20339, 321, 5373, 1184, 295, 291, 281, 534, 853, 484, 1184, 295, 613, 819, 3467, 51578], "temperature": 0.0, "avg_logprob": -0.16796215057373046, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0007793150143697858}, {"id": 611, "seek": 273214, "start": 2756.42, "end": 2761.94, "text": " of optimizers and experiment with their performance in different types of problems so that you", "tokens": [51578, 295, 5028, 22525, 293, 5120, 365, 641, 3389, 294, 819, 3467, 295, 2740, 370, 300, 291, 51854], "temperature": 0.0, "avg_logprob": -0.16796215057373046, "compression_ratio": 1.7007575757575757, "no_speech_prob": 0.0007793150143697858}, {"id": 612, "seek": 276194, "start": 2761.94, "end": 2767.9, "text": " can gain very important intuition about when to use different types of optimizers or what", "tokens": [50364, 393, 6052, 588, 1021, 24002, 466, 562, 281, 764, 819, 3467, 295, 5028, 22525, 420, 437, 50662], "temperature": 0.0, "avg_logprob": -0.15616791898554022, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0004305041511543095}, {"id": 613, "seek": 276194, "start": 2767.9, "end": 2772.9, "text": " their advantages are and disadvantages in certain applications as well.", "tokens": [50662, 641, 14906, 366, 293, 37431, 294, 1629, 5821, 382, 731, 13, 50912], "temperature": 0.0, "avg_logprob": -0.15616791898554022, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0004305041511543095}, {"id": 614, "seek": 276194, "start": 2774.9, "end": 2778.1, "text": " Let's try and put all of this together.", "tokens": [51012, 961, 311, 853, 293, 829, 439, 295, 341, 1214, 13, 51172], "temperature": 0.0, "avg_logprob": -0.15616791898554022, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0004305041511543095}, {"id": 615, "seek": 276194, "start": 2778.1, "end": 2785.5, "text": " Here we can see a full loop of using TensorFlow to define your model on the first line, define", "tokens": [51172, 1692, 321, 393, 536, 257, 1577, 6367, 295, 1228, 37624, 281, 6964, 428, 2316, 322, 264, 700, 1622, 11, 6964, 51542], "temperature": 0.0, "avg_logprob": -0.15616791898554022, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0004305041511543095}, {"id": 616, "seek": 276194, "start": 2785.5, "end": 2786.7400000000002, "text": " your optimizer.", "tokens": [51542, 428, 5028, 6545, 13, 51604], "temperature": 0.0, "avg_logprob": -0.15616791898554022, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0004305041511543095}, {"id": 617, "seek": 276194, "start": 2786.7400000000002, "end": 2790.06, "text": " Here you can replace this with any optimizer that you want.", "tokens": [51604, 1692, 291, 393, 7406, 341, 365, 604, 5028, 6545, 300, 291, 528, 13, 51770], "temperature": 0.0, "avg_logprob": -0.15616791898554022, "compression_ratio": 1.631578947368421, "no_speech_prob": 0.0004305041511543095}, {"id": 618, "seek": 279006, "start": 2790.18, "end": 2794.94, "text": " Here I'm just using stochastic gradient descent like we saw before.", "tokens": [50370, 1692, 286, 478, 445, 1228, 342, 8997, 2750, 16235, 23475, 411, 321, 1866, 949, 13, 50608], "temperature": 0.0, "avg_logprob": -0.19674017769949778, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0073442463763058186}, {"id": 619, "seek": 279006, "start": 2794.94, "end": 2797.2599999999998, "text": " Feeding it through the model we loop forever.", "tokens": [50608, 3697, 9794, 309, 807, 264, 2316, 321, 6367, 5680, 13, 50724], "temperature": 0.0, "avg_logprob": -0.19674017769949778, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0073442463763058186}, {"id": 620, "seek": 279006, "start": 2797.2599999999998, "end": 2799.34, "text": " We're doing this forward prediction.", "tokens": [50724, 492, 434, 884, 341, 2128, 17630, 13, 50828], "temperature": 0.0, "avg_logprob": -0.19674017769949778, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0073442463763058186}, {"id": 621, "seek": 279006, "start": 2799.34, "end": 2800.7799999999997, "text": " We predict using our model.", "tokens": [50828, 492, 6069, 1228, 527, 2316, 13, 50900], "temperature": 0.0, "avg_logprob": -0.19674017769949778, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0073442463763058186}, {"id": 622, "seek": 279006, "start": 2800.7799999999997, "end": 2803.46, "text": " We compute the loss with our prediction.", "tokens": [50900, 492, 14722, 264, 4470, 365, 527, 17630, 13, 51034], "temperature": 0.0, "avg_logprob": -0.19674017769949778, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0073442463763058186}, {"id": 623, "seek": 279006, "start": 2803.46, "end": 2809.1, "text": " This is exactly the loss is telling us again how incorrect our prediction is with respect", "tokens": [51034, 639, 307, 2293, 264, 4470, 307, 3585, 505, 797, 577, 18424, 527, 17630, 307, 365, 3104, 51316], "temperature": 0.0, "avg_logprob": -0.19674017769949778, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0073442463763058186}, {"id": 624, "seek": 279006, "start": 2809.1, "end": 2811.7, "text": " to the ground truth why.", "tokens": [51316, 281, 264, 2727, 3494, 983, 13, 51446], "temperature": 0.0, "avg_logprob": -0.19674017769949778, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0073442463763058186}, {"id": 625, "seek": 279006, "start": 2811.7, "end": 2818.5, "text": " We compute the gradient of our loss with respect to each of the weights in our neural network.", "tokens": [51446, 492, 14722, 264, 16235, 295, 527, 4470, 365, 3104, 281, 1184, 295, 264, 17443, 294, 527, 18161, 3209, 13, 51786], "temperature": 0.0, "avg_logprob": -0.19674017769949778, "compression_ratio": 1.825531914893617, "no_speech_prob": 0.0073442463763058186}, {"id": 626, "seek": 281850, "start": 2818.54, "end": 2826.02, "text": " Then finally we apply those gradients using our optimizer to step and update our weights.", "tokens": [50366, 1396, 2721, 321, 3079, 729, 2771, 2448, 1228, 527, 5028, 6545, 281, 1823, 293, 5623, 527, 17443, 13, 50740], "temperature": 0.0, "avg_logprob": -0.13738372227916978, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00034596689511090517}, {"id": 627, "seek": 281850, "start": 2826.02, "end": 2830.66, "text": " This is really taking everything that we've learned in the class and lecture so far and", "tokens": [50740, 639, 307, 534, 1940, 1203, 300, 321, 600, 3264, 294, 264, 1508, 293, 7991, 370, 1400, 293, 50972], "temperature": 0.0, "avg_logprob": -0.13738372227916978, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00034596689511090517}, {"id": 628, "seek": 281850, "start": 2830.66, "end": 2838.82, "text": " applying it into one whole piece of code written in TensorFlow.", "tokens": [50972, 9275, 309, 666, 472, 1379, 2522, 295, 3089, 3720, 294, 37624, 13, 51380], "temperature": 0.0, "avg_logprob": -0.13738372227916978, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00034596689511090517}, {"id": 629, "seek": 281850, "start": 2838.82, "end": 2844.46, "text": " So I want to continue this talk and really talk about tips for training these networks", "tokens": [51380, 407, 286, 528, 281, 2354, 341, 751, 293, 534, 751, 466, 6082, 337, 3097, 613, 9590, 51662], "temperature": 0.0, "avg_logprob": -0.13738372227916978, "compression_ratio": 1.5327102803738317, "no_speech_prob": 0.00034596689511090517}, {"id": 630, "seek": 284446, "start": 2844.5, "end": 2851.38, "text": " in practice now that we can focus on this very powerful idea of batching your data into", "tokens": [50366, 294, 3124, 586, 300, 321, 393, 1879, 322, 341, 588, 4005, 1558, 295, 15245, 278, 428, 1412, 666, 50710], "temperature": 0.0, "avg_logprob": -0.13886630070673955, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.012049885466694832}, {"id": 631, "seek": 284446, "start": 2851.38, "end": 2853.58, "text": " mini batches.", "tokens": [50710, 8382, 15245, 279, 13, 50820], "temperature": 0.0, "avg_logprob": -0.13886630070673955, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.012049885466694832}, {"id": 632, "seek": 284446, "start": 2853.58, "end": 2859.34, "text": " So before we saw it with gradient descent that we have the following algorithm.", "tokens": [50820, 407, 949, 321, 1866, 309, 365, 16235, 23475, 300, 321, 362, 264, 3480, 9284, 13, 51108], "temperature": 0.0, "avg_logprob": -0.13886630070673955, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.012049885466694832}, {"id": 633, "seek": 284446, "start": 2859.34, "end": 2865.1, "text": " This gradient that we saw to compute using back propagation can be actually very intensive", "tokens": [51108, 639, 16235, 300, 321, 1866, 281, 14722, 1228, 646, 38377, 393, 312, 767, 588, 18957, 51396], "temperature": 0.0, "avg_logprob": -0.13886630070673955, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.012049885466694832}, {"id": 634, "seek": 284446, "start": 2865.1, "end": 2870.54, "text": " to compute especially if it's computed over your entire training set.", "tokens": [51396, 281, 14722, 2318, 498, 309, 311, 40610, 670, 428, 2302, 3097, 992, 13, 51668], "temperature": 0.0, "avg_logprob": -0.13886630070673955, "compression_ratio": 1.6521739130434783, "no_speech_prob": 0.012049885466694832}, {"id": 635, "seek": 287054, "start": 2870.62, "end": 2874.98, "text": " This is a summation over every single data point in the entire data set and most real", "tokens": [50368, 639, 307, 257, 28811, 670, 633, 2167, 1412, 935, 294, 264, 2302, 1412, 992, 293, 881, 957, 50586], "temperature": 0.0, "avg_logprob": -0.1629782089820275, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014102718560025096}, {"id": 636, "seek": 287054, "start": 2874.98, "end": 2876.82, "text": " life applications.", "tokens": [50586, 993, 5821, 13, 50678], "temperature": 0.0, "avg_logprob": -0.1629782089820275, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014102718560025096}, {"id": 637, "seek": 287054, "start": 2876.82, "end": 2881.66, "text": " It is simply not feasible to compute this on every single iteration in your optimization", "tokens": [50678, 467, 307, 2935, 406, 26648, 281, 14722, 341, 322, 633, 2167, 24784, 294, 428, 19618, 50920], "temperature": 0.0, "avg_logprob": -0.1629782089820275, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014102718560025096}, {"id": 638, "seek": 287054, "start": 2881.66, "end": 2883.7799999999997, "text": " loop.", "tokens": [50920, 6367, 13, 51026], "temperature": 0.0, "avg_logprob": -0.1629782089820275, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014102718560025096}, {"id": 639, "seek": 287054, "start": 2883.7799999999997, "end": 2888.7799999999997, "text": " Alternatively let's consider a different variant of this algorithm called stochastic gradient", "tokens": [51026, 46167, 718, 311, 1949, 257, 819, 17501, 295, 341, 9284, 1219, 342, 8997, 2750, 16235, 51276], "temperature": 0.0, "avg_logprob": -0.1629782089820275, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014102718560025096}, {"id": 640, "seek": 287054, "start": 2888.7799999999997, "end": 2889.7799999999997, "text": " descent.", "tokens": [51276, 23475, 13, 51326], "temperature": 0.0, "avg_logprob": -0.1629782089820275, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014102718560025096}, {"id": 641, "seek": 287054, "start": 2889.7799999999997, "end": 2894.38, "text": " So instead of computing the gradient over our entire data set let's just pick a single", "tokens": [51326, 407, 2602, 295, 15866, 264, 16235, 670, 527, 2302, 1412, 992, 718, 311, 445, 1888, 257, 2167, 51556], "temperature": 0.0, "avg_logprob": -0.1629782089820275, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014102718560025096}, {"id": 642, "seek": 287054, "start": 2894.38, "end": 2899.98, "text": " point compute the gradient of that single point with respect to the weights and then", "tokens": [51556, 935, 14722, 264, 16235, 295, 300, 2167, 935, 365, 3104, 281, 264, 17443, 293, 550, 51836], "temperature": 0.0, "avg_logprob": -0.1629782089820275, "compression_ratio": 1.8022813688212929, "no_speech_prob": 0.0014102718560025096}, {"id": 643, "seek": 289998, "start": 2899.98, "end": 2903.82, "text": " update all of our weights based on that gradient.", "tokens": [50364, 5623, 439, 295, 527, 17443, 2361, 322, 300, 16235, 13, 50556], "temperature": 0.0, "avg_logprob": -0.12990727675588506, "compression_ratio": 1.817351598173516, "no_speech_prob": 0.004609047435224056}, {"id": 644, "seek": 289998, "start": 2903.82, "end": 2908.66, "text": " So this has some advantages this is very easy to compute because it's only using one data", "tokens": [50556, 407, 341, 575, 512, 14906, 341, 307, 588, 1858, 281, 14722, 570, 309, 311, 787, 1228, 472, 1412, 50798], "temperature": 0.0, "avg_logprob": -0.12990727675588506, "compression_ratio": 1.817351598173516, "no_speech_prob": 0.004609047435224056}, {"id": 645, "seek": 289998, "start": 2908.66, "end": 2914.94, "text": " point now it's very fast but it's also very noisy because it's only from one data point.", "tokens": [50798, 935, 586, 309, 311, 588, 2370, 457, 309, 311, 611, 588, 24518, 570, 309, 311, 787, 490, 472, 1412, 935, 13, 51112], "temperature": 0.0, "avg_logprob": -0.12990727675588506, "compression_ratio": 1.817351598173516, "no_speech_prob": 0.004609047435224056}, {"id": 646, "seek": 289998, "start": 2914.94, "end": 2920.82, "text": " Instead there's a middle ground instead of computing this noisy gradient of a single", "tokens": [51112, 7156, 456, 311, 257, 2808, 2727, 2602, 295, 15866, 341, 24518, 16235, 295, 257, 2167, 51406], "temperature": 0.0, "avg_logprob": -0.12990727675588506, "compression_ratio": 1.817351598173516, "no_speech_prob": 0.004609047435224056}, {"id": 647, "seek": 289998, "start": 2920.82, "end": 2928.7400000000002, "text": " point let's get a better estimate of our gradient by using a batch of b data points.", "tokens": [51406, 935, 718, 311, 483, 257, 1101, 12539, 295, 527, 16235, 538, 1228, 257, 15245, 295, 272, 1412, 2793, 13, 51802], "temperature": 0.0, "avg_logprob": -0.12990727675588506, "compression_ratio": 1.817351598173516, "no_speech_prob": 0.004609047435224056}, {"id": 648, "seek": 292874, "start": 2928.74, "end": 2934.74, "text": " So now let's pick a batch of b data points and we'll compute the gradient estimate simply", "tokens": [50364, 407, 586, 718, 311, 1888, 257, 15245, 295, 272, 1412, 2793, 293, 321, 603, 14722, 264, 16235, 12539, 2935, 50664], "temperature": 0.0, "avg_logprob": -0.09501060743010446, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.00781509093940258}, {"id": 649, "seek": 292874, "start": 2934.74, "end": 2937.7, "text": " as the average over this batch.", "tokens": [50664, 382, 264, 4274, 670, 341, 15245, 13, 50812], "temperature": 0.0, "avg_logprob": -0.09501060743010446, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.00781509093940258}, {"id": 650, "seek": 292874, "start": 2937.7, "end": 2942.7799999999997, "text": " So since b here is usually not that large on the order of tens or hundreds of samples", "tokens": [50812, 407, 1670, 272, 510, 307, 2673, 406, 300, 2416, 322, 264, 1668, 295, 10688, 420, 6779, 295, 10938, 51066], "temperature": 0.0, "avg_logprob": -0.09501060743010446, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.00781509093940258}, {"id": 651, "seek": 292874, "start": 2942.7799999999997, "end": 2948.8199999999997, "text": " this is much much faster to compute than regular gradient descent and it's also much much more", "tokens": [51066, 341, 307, 709, 709, 4663, 281, 14722, 813, 3890, 16235, 23475, 293, 309, 311, 611, 709, 709, 544, 51368], "temperature": 0.0, "avg_logprob": -0.09501060743010446, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.00781509093940258}, {"id": 652, "seek": 292874, "start": 2948.8199999999997, "end": 2956.9399999999996, "text": " accurate than purely stochastic gradient descent that only uses a single example.", "tokens": [51368, 8559, 813, 17491, 342, 8997, 2750, 16235, 23475, 300, 787, 4960, 257, 2167, 1365, 13, 51774], "temperature": 0.0, "avg_logprob": -0.09501060743010446, "compression_ratio": 1.7454545454545454, "no_speech_prob": 0.00781509093940258}, {"id": 653, "seek": 295694, "start": 2956.94, "end": 2962.38, "text": " Now this increases the gradient accuracy estimation which also allows us to converge much more", "tokens": [50364, 823, 341, 8637, 264, 16235, 14170, 35701, 597, 611, 4045, 505, 281, 41881, 709, 544, 50636], "temperature": 0.0, "avg_logprob": -0.14064773760343852, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.007575948256999254}, {"id": 654, "seek": 295694, "start": 2962.38, "end": 2967.66, "text": " smoothly it also means that we can trust our gradient more than in stochastic gradient", "tokens": [50636, 19565, 309, 611, 1355, 300, 321, 393, 3361, 527, 16235, 544, 813, 294, 342, 8997, 2750, 16235, 50900], "temperature": 0.0, "avg_logprob": -0.14064773760343852, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.007575948256999254}, {"id": 655, "seek": 295694, "start": 2967.66, "end": 2974.94, "text": " descent so that we can actually increase our learning rate a bit more as well.", "tokens": [50900, 23475, 370, 300, 321, 393, 767, 3488, 527, 2539, 3314, 257, 857, 544, 382, 731, 13, 51264], "temperature": 0.0, "avg_logprob": -0.14064773760343852, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.007575948256999254}, {"id": 656, "seek": 295694, "start": 2974.94, "end": 2980.94, "text": " Mini batching also leads to massively parallelizable computation we can split up the batches on", "tokens": [51264, 18239, 15245, 278, 611, 6689, 281, 29379, 8952, 22395, 24903, 321, 393, 7472, 493, 264, 15245, 279, 322, 51564], "temperature": 0.0, "avg_logprob": -0.14064773760343852, "compression_ratio": 1.7033492822966507, "no_speech_prob": 0.007575948256999254}, {"id": 657, "seek": 298094, "start": 2980.94, "end": 2986.38, "text": " separate workers and separate machines and thus achieve even more parallelization and", "tokens": [50364, 4994, 5600, 293, 4994, 8379, 293, 8807, 4584, 754, 544, 8952, 2144, 293, 50636], "temperature": 0.0, "avg_logprob": -0.10042648566396613, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.1096569076180458}, {"id": 658, "seek": 298094, "start": 2986.38, "end": 2990.14, "text": " speed increases on our GPUs.", "tokens": [50636, 3073, 8637, 322, 527, 18407, 82, 13, 50824], "temperature": 0.0, "avg_logprob": -0.10042648566396613, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.1096569076180458}, {"id": 659, "seek": 298094, "start": 2990.14, "end": 2994.42, "text": " Now the last topic I want to talk about is that of overfitting this is also known as", "tokens": [50824, 823, 264, 1036, 4829, 286, 528, 281, 751, 466, 307, 300, 295, 670, 69, 2414, 341, 307, 611, 2570, 382, 51038], "temperature": 0.0, "avg_logprob": -0.10042648566396613, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.1096569076180458}, {"id": 660, "seek": 298094, "start": 2994.42, "end": 3001.02, "text": " the problem of generalization and is one of the most fundamental problems in all of machine", "tokens": [51038, 264, 1154, 295, 2674, 2144, 293, 307, 472, 295, 264, 881, 8088, 2740, 294, 439, 295, 3479, 51368], "temperature": 0.0, "avg_logprob": -0.10042648566396613, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.1096569076180458}, {"id": 661, "seek": 298094, "start": 3001.02, "end": 3005.66, "text": " learning and not just deep learning.", "tokens": [51368, 2539, 293, 406, 445, 2452, 2539, 13, 51600], "temperature": 0.0, "avg_logprob": -0.10042648566396613, "compression_ratio": 1.6318407960199004, "no_speech_prob": 0.1096569076180458}, {"id": 662, "seek": 300566, "start": 3005.66, "end": 3011.94, "text": " Now overfitting like I said is critical to understand so I really want to make sure that", "tokens": [50364, 823, 670, 69, 2414, 411, 286, 848, 307, 4924, 281, 1223, 370, 286, 534, 528, 281, 652, 988, 300, 50678], "temperature": 0.0, "avg_logprob": -0.11212042008323231, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.03307824209332466}, {"id": 663, "seek": 300566, "start": 3011.94, "end": 3017.7799999999997, "text": " this is a clear concept in everyone's mind ideally in machine learning we want to learn", "tokens": [50678, 341, 307, 257, 1850, 3410, 294, 1518, 311, 1575, 22915, 294, 3479, 2539, 321, 528, 281, 1466, 50970], "temperature": 0.0, "avg_logprob": -0.11212042008323231, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.03307824209332466}, {"id": 664, "seek": 300566, "start": 3017.7799999999997, "end": 3023.42, "text": " a model that accurately describes our test data not the training data even though we're", "tokens": [50970, 257, 2316, 300, 20095, 15626, 527, 1500, 1412, 406, 264, 3097, 1412, 754, 1673, 321, 434, 51252], "temperature": 0.0, "avg_logprob": -0.11212042008323231, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.03307824209332466}, {"id": 665, "seek": 300566, "start": 3023.42, "end": 3028.14, "text": " optimizing this model based on the training data what we really want is for it to perform", "tokens": [51252, 40425, 341, 2316, 2361, 322, 264, 3097, 1412, 437, 321, 534, 528, 307, 337, 309, 281, 2042, 51488], "temperature": 0.0, "avg_logprob": -0.11212042008323231, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.03307824209332466}, {"id": 666, "seek": 300566, "start": 3028.14, "end": 3031.46, "text": " well on the test data.", "tokens": [51488, 731, 322, 264, 1500, 1412, 13, 51654], "temperature": 0.0, "avg_logprob": -0.11212042008323231, "compression_ratio": 1.7293577981651376, "no_speech_prob": 0.03307824209332466}, {"id": 667, "seek": 303146, "start": 3031.46, "end": 3038.14, "text": " So said differently we want to build representations that can learn from our training data but", "tokens": [50364, 407, 848, 7614, 321, 528, 281, 1322, 33358, 300, 393, 1466, 490, 527, 3097, 1412, 457, 50698], "temperature": 0.0, "avg_logprob": -0.09170406467311985, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.009411418810486794}, {"id": 668, "seek": 303146, "start": 3038.14, "end": 3042.3, "text": " still generalize well to unseen test data.", "tokens": [50698, 920, 2674, 1125, 731, 281, 40608, 1500, 1412, 13, 50906], "temperature": 0.0, "avg_logprob": -0.09170406467311985, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.009411418810486794}, {"id": 669, "seek": 303146, "start": 3042.3, "end": 3049.38, "text": " Now assume you want to build a line to describe these points underfitting means that the model", "tokens": [50906, 823, 6552, 291, 528, 281, 1322, 257, 1622, 281, 6786, 613, 2793, 833, 69, 2414, 1355, 300, 264, 2316, 51260], "temperature": 0.0, "avg_logprob": -0.09170406467311985, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.009411418810486794}, {"id": 670, "seek": 303146, "start": 3049.38, "end": 3056.14, "text": " does simply not have enough capacity to represent these points so no matter how good we try", "tokens": [51260, 775, 2935, 406, 362, 1547, 6042, 281, 2906, 613, 2793, 370, 572, 1871, 577, 665, 321, 853, 51598], "temperature": 0.0, "avg_logprob": -0.09170406467311985, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.009411418810486794}, {"id": 671, "seek": 303146, "start": 3056.14, "end": 3061.34, "text": " to fit this model it simply does not have the capacity to represent this type of data.", "tokens": [51598, 281, 3318, 341, 2316, 309, 2935, 775, 406, 362, 264, 6042, 281, 2906, 341, 2010, 295, 1412, 13, 51858], "temperature": 0.0, "avg_logprob": -0.09170406467311985, "compression_ratio": 1.8105726872246697, "no_speech_prob": 0.009411418810486794}, {"id": 672, "seek": 306134, "start": 3061.34, "end": 3065.42, "text": " On the far right hand side we can see the extreme other extreme where here the model", "tokens": [50364, 1282, 264, 1400, 558, 1011, 1252, 321, 393, 536, 264, 8084, 661, 8084, 689, 510, 264, 2316, 50568], "temperature": 0.0, "avg_logprob": -0.10340732711929458, "compression_ratio": 1.925764192139738, "no_speech_prob": 0.0069028218276798725}, {"id": 673, "seek": 306134, "start": 3065.42, "end": 3072.6200000000003, "text": " is too complex it has too many parameters and it does not generalize well to new data.", "tokens": [50568, 307, 886, 3997, 309, 575, 886, 867, 9834, 293, 309, 775, 406, 2674, 1125, 731, 281, 777, 1412, 13, 50928], "temperature": 0.0, "avg_logprob": -0.10340732711929458, "compression_ratio": 1.925764192139738, "no_speech_prob": 0.0069028218276798725}, {"id": 674, "seek": 306134, "start": 3072.6200000000003, "end": 3076.7400000000002, "text": " In the middle though we can see what's called the ideal fit it's not overfitting it's not", "tokens": [50928, 682, 264, 2808, 1673, 321, 393, 536, 437, 311, 1219, 264, 7157, 3318, 309, 311, 406, 670, 69, 2414, 309, 311, 406, 51134], "temperature": 0.0, "avg_logprob": -0.10340732711929458, "compression_ratio": 1.925764192139738, "no_speech_prob": 0.0069028218276798725}, {"id": 675, "seek": 306134, "start": 3076.7400000000002, "end": 3083.1000000000004, "text": " underfitting but it has a medium number of parameters and it's able to fit in a generalizable", "tokens": [51134, 833, 69, 2414, 457, 309, 575, 257, 6399, 1230, 295, 9834, 293, 309, 311, 1075, 281, 3318, 294, 257, 2674, 22395, 51452], "temperature": 0.0, "avg_logprob": -0.10340732711929458, "compression_ratio": 1.925764192139738, "no_speech_prob": 0.0069028218276798725}, {"id": 676, "seek": 306134, "start": 3083.1000000000004, "end": 3089.3, "text": " way to the output and is able to generalize well to brand new data when it sees it at", "tokens": [51452, 636, 281, 264, 5598, 293, 307, 1075, 281, 2674, 1125, 731, 281, 3360, 777, 1412, 562, 309, 8194, 309, 412, 51762], "temperature": 0.0, "avg_logprob": -0.10340732711929458, "compression_ratio": 1.925764192139738, "no_speech_prob": 0.0069028218276798725}, {"id": 677, "seek": 308930, "start": 3089.3, "end": 3092.02, "text": " test time.", "tokens": [50364, 1500, 565, 13, 50500], "temperature": 0.0, "avg_logprob": -0.13473022551763625, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0952921062707901}, {"id": 678, "seek": 308930, "start": 3092.02, "end": 3097.78, "text": " Now to address this problem let's talk about regularization how can we make sure that our", "tokens": [50500, 823, 281, 2985, 341, 1154, 718, 311, 751, 466, 3890, 2144, 577, 393, 321, 652, 988, 300, 527, 50788], "temperature": 0.0, "avg_logprob": -0.13473022551763625, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0952921062707901}, {"id": 679, "seek": 308930, "start": 3097.78, "end": 3103.02, "text": " models do not end up overfit because neural networks do have a ton of parameters how can", "tokens": [50788, 5245, 360, 406, 917, 493, 670, 6845, 570, 18161, 9590, 360, 362, 257, 2952, 295, 9834, 577, 393, 51050], "temperature": 0.0, "avg_logprob": -0.13473022551763625, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0952921062707901}, {"id": 680, "seek": 308930, "start": 3103.02, "end": 3107.54, "text": " we enforce some form of regularization to them.", "tokens": [51050, 321, 24825, 512, 1254, 295, 3890, 2144, 281, 552, 13, 51276], "temperature": 0.0, "avg_logprob": -0.13473022551763625, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0952921062707901}, {"id": 681, "seek": 308930, "start": 3107.54, "end": 3109.54, "text": " Now what is regularization?", "tokens": [51276, 823, 437, 307, 3890, 2144, 30, 51376], "temperature": 0.0, "avg_logprob": -0.13473022551763625, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0952921062707901}, {"id": 682, "seek": 308930, "start": 3109.54, "end": 3113.5800000000004, "text": " Regularization is a technique that constrains our optimization problem such that we can", "tokens": [51376, 45659, 2144, 307, 257, 6532, 300, 11525, 1292, 527, 19618, 1154, 1270, 300, 321, 393, 51578], "temperature": 0.0, "avg_logprob": -0.13473022551763625, "compression_ratio": 1.7053140096618358, "no_speech_prob": 0.0952921062707901}, {"id": 683, "seek": 311358, "start": 3113.58, "end": 3119.8199999999997, "text": " discourage these complex models from actually being learned and overfit.", "tokens": [50364, 21497, 609, 613, 3997, 5245, 490, 767, 885, 3264, 293, 670, 6845, 13, 50676], "temperature": 0.0, "avg_logprob": -0.12665860959798983, "compression_ratio": 1.7109375, "no_speech_prob": 0.0600702241063118}, {"id": 684, "seek": 311358, "start": 3119.8199999999997, "end": 3121.42, "text": " So again why do we need it?", "tokens": [50676, 407, 797, 983, 360, 321, 643, 309, 30, 50756], "temperature": 0.0, "avg_logprob": -0.12665860959798983, "compression_ratio": 1.7109375, "no_speech_prob": 0.0600702241063118}, {"id": 685, "seek": 311358, "start": 3121.42, "end": 3126.62, "text": " We need it so that our model can generalize to this unseen data set and in neural networks", "tokens": [50756, 492, 643, 309, 370, 300, 527, 2316, 393, 2674, 1125, 281, 341, 40608, 1412, 992, 293, 294, 18161, 9590, 51016], "temperature": 0.0, "avg_logprob": -0.12665860959798983, "compression_ratio": 1.7109375, "no_speech_prob": 0.0600702241063118}, {"id": 686, "seek": 311358, "start": 3126.62, "end": 3133.02, "text": " we have many techniques for actually imposing regularization onto the model.", "tokens": [51016, 321, 362, 867, 7512, 337, 767, 40288, 3890, 2144, 3911, 264, 2316, 13, 51336], "temperature": 0.0, "avg_logprob": -0.12665860959798983, "compression_ratio": 1.7109375, "no_speech_prob": 0.0600702241063118}, {"id": 687, "seek": 311358, "start": 3133.02, "end": 3137.1, "text": " One very common technique and very simple to understand is called dropout.", "tokens": [51336, 1485, 588, 2689, 6532, 293, 588, 2199, 281, 1223, 307, 1219, 3270, 346, 13, 51540], "temperature": 0.0, "avg_logprob": -0.12665860959798983, "compression_ratio": 1.7109375, "no_speech_prob": 0.0600702241063118}, {"id": 688, "seek": 311358, "start": 3137.1, "end": 3142.62, "text": " This is one of the most popular forms of regularization in deep learning and it's very simple.", "tokens": [51540, 639, 307, 472, 295, 264, 881, 3743, 6422, 295, 3890, 2144, 294, 2452, 2539, 293, 309, 311, 588, 2199, 13, 51816], "temperature": 0.0, "avg_logprob": -0.12665860959798983, "compression_ratio": 1.7109375, "no_speech_prob": 0.0600702241063118}, {"id": 689, "seek": 314262, "start": 3142.62, "end": 3145.66, "text": " Let's revisit this picture of a neural network.", "tokens": [50364, 961, 311, 32676, 341, 3036, 295, 257, 18161, 3209, 13, 50516], "temperature": 0.0, "avg_logprob": -0.12187274460940017, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.015902332961559296}, {"id": 690, "seek": 314262, "start": 3145.66, "end": 3152.2999999999997, "text": " This is a two-layered neural network, two hidden layers and in dropout during training", "tokens": [50516, 639, 307, 257, 732, 12, 8376, 4073, 18161, 3209, 11, 732, 7633, 7914, 293, 294, 3270, 346, 1830, 3097, 50848], "temperature": 0.0, "avg_logprob": -0.12187274460940017, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.015902332961559296}, {"id": 691, "seek": 314262, "start": 3152.2999999999997, "end": 3159.22, "text": " all we simply do is randomly set some of the activations here to zero with some probability.", "tokens": [50848, 439, 321, 2935, 360, 307, 16979, 992, 512, 295, 264, 2430, 763, 510, 281, 4018, 365, 512, 8482, 13, 51194], "temperature": 0.0, "avg_logprob": -0.12187274460940017, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.015902332961559296}, {"id": 692, "seek": 314262, "start": 3159.22, "end": 3167.06, "text": " So what we can do is let's say we pick our probability to be 50% or 0.5 we can drop randomly", "tokens": [51194, 407, 437, 321, 393, 360, 307, 718, 311, 584, 321, 1888, 527, 8482, 281, 312, 2625, 4, 420, 1958, 13, 20, 321, 393, 3270, 16979, 51586], "temperature": 0.0, "avg_logprob": -0.12187274460940017, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.015902332961559296}, {"id": 693, "seek": 314262, "start": 3167.06, "end": 3171.9, "text": " for each of the activations 50% of those neurons.", "tokens": [51586, 337, 1184, 295, 264, 2430, 763, 2625, 4, 295, 729, 22027, 13, 51828], "temperature": 0.0, "avg_logprob": -0.12187274460940017, "compression_ratio": 1.6972477064220184, "no_speech_prob": 0.015902332961559296}, {"id": 694, "seek": 317190, "start": 3171.9, "end": 3176.34, "text": " This is extremely powerful as it lowers the capacity of our neural network so that they", "tokens": [50364, 639, 307, 4664, 4005, 382, 309, 44936, 264, 6042, 295, 527, 18161, 3209, 370, 300, 436, 50586], "temperature": 0.0, "avg_logprob": -0.10695773501728856, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0011694313725456595}, {"id": 695, "seek": 317190, "start": 3176.34, "end": 3182.58, "text": " have to learn to perform better on test sets because sometimes on training sets it just", "tokens": [50586, 362, 281, 1466, 281, 2042, 1101, 322, 1500, 6352, 570, 2171, 322, 3097, 6352, 309, 445, 50898], "temperature": 0.0, "avg_logprob": -0.10695773501728856, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0011694313725456595}, {"id": 696, "seek": 317190, "start": 3182.58, "end": 3185.2200000000003, "text": " simply cannot rely on some of those parameters.", "tokens": [50898, 2935, 2644, 10687, 322, 512, 295, 729, 9834, 13, 51030], "temperature": 0.0, "avg_logprob": -0.10695773501728856, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0011694313725456595}, {"id": 697, "seek": 317190, "start": 3185.2200000000003, "end": 3189.9, "text": " So it has to be able to be resilient to that kind of dropout.", "tokens": [51030, 407, 309, 575, 281, 312, 1075, 281, 312, 23699, 281, 300, 733, 295, 3270, 346, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10695773501728856, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0011694313725456595}, {"id": 698, "seek": 317190, "start": 3189.9, "end": 3197.1, "text": " It also means that they're easier to train because at least on every forward pass of", "tokens": [51264, 467, 611, 1355, 300, 436, 434, 3571, 281, 3847, 570, 412, 1935, 322, 633, 2128, 1320, 295, 51624], "temperature": 0.0, "avg_logprob": -0.10695773501728856, "compression_ratio": 1.6299559471365639, "no_speech_prob": 0.0011694313725456595}, {"id": 699, "seek": 319710, "start": 3197.1, "end": 3202.1, "text": " iterations we're training only 50% of the weights and only 50% of the gradients.", "tokens": [50364, 36540, 321, 434, 3097, 787, 2625, 4, 295, 264, 17443, 293, 787, 2625, 4, 295, 264, 2771, 2448, 13, 50614], "temperature": 0.0, "avg_logprob": -0.13919987577073117, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.06752240657806396}, {"id": 700, "seek": 319710, "start": 3202.1, "end": 3207.9, "text": " So that also cuts our gradient computation time down by a factor of two.", "tokens": [50614, 407, 300, 611, 9992, 527, 16235, 24903, 565, 760, 538, 257, 5952, 295, 732, 13, 50904], "temperature": 0.0, "avg_logprob": -0.13919987577073117, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.06752240657806396}, {"id": 701, "seek": 319710, "start": 3207.9, "end": 3213.74, "text": " So because now we only have to compute half the number of neuron gradients.", "tokens": [50904, 407, 570, 586, 321, 787, 362, 281, 14722, 1922, 264, 1230, 295, 34090, 2771, 2448, 13, 51196], "temperature": 0.0, "avg_logprob": -0.13919987577073117, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.06752240657806396}, {"id": 702, "seek": 319710, "start": 3213.74, "end": 3218.22, "text": " Now on every iteration we dropped out on the previous iteration 50% of neurons but on the", "tokens": [51196, 823, 322, 633, 24784, 321, 8119, 484, 322, 264, 3894, 24784, 2625, 4, 295, 22027, 457, 322, 264, 51420], "temperature": 0.0, "avg_logprob": -0.13919987577073117, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.06752240657806396}, {"id": 703, "seek": 319710, "start": 3218.22, "end": 3225.7, "text": " next iteration we're going to drop out a different set of neurons.", "tokens": [51420, 958, 24784, 321, 434, 516, 281, 3270, 484, 257, 819, 992, 295, 22027, 13, 51794], "temperature": 0.0, "avg_logprob": -0.13919987577073117, "compression_ratio": 1.820754716981132, "no_speech_prob": 0.06752240657806396}, {"id": 704, "seek": 322570, "start": 3225.7, "end": 3230.5, "text": " And this gives the network, it basically forces the network to learn how to take different", "tokens": [50364, 400, 341, 2709, 264, 3209, 11, 309, 1936, 5874, 264, 3209, 281, 1466, 577, 281, 747, 819, 50604], "temperature": 0.0, "avg_logprob": -0.1350047317976804, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.21708954870700836}, {"id": 705, "seek": 322570, "start": 3230.5, "end": 3237.18, "text": " pathways to get to its answer and it can't rely on any one pathway too strongly and overfit", "tokens": [50604, 22988, 281, 483, 281, 1080, 1867, 293, 309, 393, 380, 10687, 322, 604, 472, 18590, 886, 10613, 293, 670, 6845, 50938], "temperature": 0.0, "avg_logprob": -0.1350047317976804, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.21708954870700836}, {"id": 706, "seek": 322570, "start": 3237.18, "end": 3238.18, "text": " to that pathway.", "tokens": [50938, 281, 300, 18590, 13, 50988], "temperature": 0.0, "avg_logprob": -0.1350047317976804, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.21708954870700836}, {"id": 707, "seek": 322570, "start": 3238.18, "end": 3242.7599999999998, "text": " This is a way to really force it to generalize to this new data.", "tokens": [50988, 639, 307, 257, 636, 281, 534, 3464, 309, 281, 2674, 1125, 281, 341, 777, 1412, 13, 51217], "temperature": 0.0, "avg_logprob": -0.1350047317976804, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.21708954870700836}, {"id": 708, "seek": 322570, "start": 3242.7599999999998, "end": 3248.8999999999996, "text": " The second regularization technique that we'll talk about is this notion of early stopping.", "tokens": [51217, 440, 1150, 3890, 2144, 6532, 300, 321, 603, 751, 466, 307, 341, 10710, 295, 2440, 12767, 13, 51524], "temperature": 0.0, "avg_logprob": -0.1350047317976804, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.21708954870700836}, {"id": 709, "seek": 322570, "start": 3248.8999999999996, "end": 3252.18, "text": " And again here the idea is very basic.", "tokens": [51524, 400, 797, 510, 264, 1558, 307, 588, 3875, 13, 51688], "temperature": 0.0, "avg_logprob": -0.1350047317976804, "compression_ratio": 1.6952789699570816, "no_speech_prob": 0.21708954870700836}, {"id": 710, "seek": 325218, "start": 3252.18, "end": 3258.1, "text": " It's basically let's stop training once we realize that our loss is increasing on a", "tokens": [50364, 467, 311, 1936, 718, 311, 1590, 3097, 1564, 321, 4325, 300, 527, 4470, 307, 5662, 322, 257, 50660], "temperature": 0.0, "avg_logprob": -0.10414036264959371, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.022970709949731827}, {"id": 711, "seek": 325218, "start": 3258.1, "end": 3262.8199999999997, "text": " held out validation or let's call it a test set.", "tokens": [50660, 5167, 484, 24071, 420, 718, 311, 818, 309, 257, 1500, 992, 13, 50896], "temperature": 0.0, "avg_logprob": -0.10414036264959371, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.022970709949731827}, {"id": 712, "seek": 325218, "start": 3262.8199999999997, "end": 3267.98, "text": " So when we start training we all know the definition of overfitting is when our model", "tokens": [50896, 407, 562, 321, 722, 3097, 321, 439, 458, 264, 7123, 295, 670, 69, 2414, 307, 562, 527, 2316, 51154], "temperature": 0.0, "avg_logprob": -0.10414036264959371, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.022970709949731827}, {"id": 713, "seek": 325218, "start": 3267.98, "end": 3270.3399999999997, "text": " starts to perform worse on the test set.", "tokens": [51154, 3719, 281, 2042, 5324, 322, 264, 1500, 992, 13, 51272], "temperature": 0.0, "avg_logprob": -0.10414036264959371, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.022970709949731827}, {"id": 714, "seek": 325218, "start": 3270.3399999999997, "end": 3276.3799999999997, "text": " So if we set aside some of this training data to be quote unquote test data we can monitor", "tokens": [51272, 407, 498, 321, 992, 7359, 512, 295, 341, 3097, 1412, 281, 312, 6513, 37557, 1500, 1412, 321, 393, 6002, 51574], "temperature": 0.0, "avg_logprob": -0.10414036264959371, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.022970709949731827}, {"id": 715, "seek": 325218, "start": 3276.3799999999997, "end": 3281.3799999999997, "text": " how our network is learning on this data and simply just stop before it has a chance", "tokens": [51574, 577, 527, 3209, 307, 2539, 322, 341, 1412, 293, 2935, 445, 1590, 949, 309, 575, 257, 2931, 51824], "temperature": 0.0, "avg_logprob": -0.10414036264959371, "compression_ratio": 1.7469879518072289, "no_speech_prob": 0.022970709949731827}, {"id": 716, "seek": 328138, "start": 3281.38, "end": 3282.9, "text": " to overfit.", "tokens": [50364, 281, 670, 6845, 13, 50440], "temperature": 0.0, "avg_logprob": -0.12255935508663915, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.02297022007405758}, {"id": 717, "seek": 328138, "start": 3282.9, "end": 3286.42, "text": " So on the x-axis you can see the number of training iterations and on the y-axis you", "tokens": [50440, 407, 322, 264, 2031, 12, 24633, 291, 393, 536, 264, 1230, 295, 3097, 36540, 293, 322, 264, 288, 12, 24633, 291, 50616], "temperature": 0.0, "avg_logprob": -0.12255935508663915, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.02297022007405758}, {"id": 718, "seek": 328138, "start": 3286.42, "end": 3291.7400000000002, "text": " can see the loss that we get after training that number of iterations.", "tokens": [50616, 393, 536, 264, 4470, 300, 321, 483, 934, 3097, 300, 1230, 295, 36540, 13, 50882], "temperature": 0.0, "avg_logprob": -0.12255935508663915, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.02297022007405758}, {"id": 719, "seek": 328138, "start": 3291.7400000000002, "end": 3295.5, "text": " So as we continue to train in the beginning both lines continue to decrease.", "tokens": [50882, 407, 382, 321, 2354, 281, 3847, 294, 264, 2863, 1293, 3876, 2354, 281, 11514, 13, 51070], "temperature": 0.0, "avg_logprob": -0.12255935508663915, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.02297022007405758}, {"id": 720, "seek": 328138, "start": 3295.5, "end": 3301.2200000000003, "text": " This is as we'd expect and this is excellent since it means our model is getting stronger.", "tokens": [51070, 639, 307, 382, 321, 1116, 2066, 293, 341, 307, 7103, 1670, 309, 1355, 527, 2316, 307, 1242, 7249, 13, 51356], "temperature": 0.0, "avg_logprob": -0.12255935508663915, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.02297022007405758}, {"id": 721, "seek": 328138, "start": 3301.2200000000003, "end": 3306.54, "text": " Eventually though the network's testing loss plateaus and starts to increase.", "tokens": [51356, 17586, 1673, 264, 3209, 311, 4997, 4470, 5924, 8463, 293, 3719, 281, 3488, 13, 51622], "temperature": 0.0, "avg_logprob": -0.12255935508663915, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.02297022007405758}, {"id": 722, "seek": 328138, "start": 3306.54, "end": 3311.26, "text": " Note that the training accuracy will always continue to go down as long as the network", "tokens": [51622, 11633, 300, 264, 3097, 14170, 486, 1009, 2354, 281, 352, 760, 382, 938, 382, 264, 3209, 51858], "temperature": 0.0, "avg_logprob": -0.12255935508663915, "compression_ratio": 1.9011406844106464, "no_speech_prob": 0.02297022007405758}, {"id": 723, "seek": 331126, "start": 3311.26, "end": 3317.42, "text": " has the capacity to memorize the data and this pattern continues for the rest of training.", "tokens": [50364, 575, 264, 6042, 281, 27478, 264, 1412, 293, 341, 5102, 6515, 337, 264, 1472, 295, 3097, 13, 50672], "temperature": 0.0, "avg_logprob": -0.09755760055404526, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.004608680959790945}, {"id": 724, "seek": 331126, "start": 3317.42, "end": 3321.0200000000004, "text": " So it's important here to actually focus on this point here.", "tokens": [50672, 407, 309, 311, 1021, 510, 281, 767, 1879, 322, 341, 935, 510, 13, 50852], "temperature": 0.0, "avg_logprob": -0.09755760055404526, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.004608680959790945}, {"id": 725, "seek": 331126, "start": 3321.0200000000004, "end": 3325.2200000000003, "text": " This is the point where we need to stop training and after this point assuming that our test", "tokens": [50852, 639, 307, 264, 935, 689, 321, 643, 281, 1590, 3097, 293, 934, 341, 935, 11926, 300, 527, 1500, 51062], "temperature": 0.0, "avg_logprob": -0.09755760055404526, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.004608680959790945}, {"id": 726, "seek": 331126, "start": 3325.2200000000003, "end": 3330.94, "text": " set is a valid representation of the true test set the accuracy of the model will only", "tokens": [51062, 992, 307, 257, 7363, 10290, 295, 264, 2074, 1500, 992, 264, 14170, 295, 264, 2316, 486, 787, 51348], "temperature": 0.0, "avg_logprob": -0.09755760055404526, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.004608680959790945}, {"id": 727, "seek": 331126, "start": 3330.94, "end": 3331.94, "text": " get worse.", "tokens": [51348, 483, 5324, 13, 51398], "temperature": 0.0, "avg_logprob": -0.09755760055404526, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.004608680959790945}, {"id": 728, "seek": 331126, "start": 3331.94, "end": 3336.1800000000003, "text": " So we can stop training here take this model and this should be the model that we actually", "tokens": [51398, 407, 321, 393, 1590, 3097, 510, 747, 341, 2316, 293, 341, 820, 312, 264, 2316, 300, 321, 767, 51610], "temperature": 0.0, "avg_logprob": -0.09755760055404526, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.004608680959790945}, {"id": 729, "seek": 331126, "start": 3336.1800000000003, "end": 3340.5, "text": " use when we deploy into the real world.", "tokens": [51610, 764, 562, 321, 7274, 666, 264, 957, 1002, 13, 51826], "temperature": 0.0, "avg_logprob": -0.09755760055404526, "compression_ratio": 1.876984126984127, "no_speech_prob": 0.004608680959790945}, {"id": 730, "seek": 334050, "start": 3340.5, "end": 3344.22, "text": " Anything any model taken from the left hand side is going to be underfit is not going", "tokens": [50364, 11998, 604, 2316, 2726, 490, 264, 1411, 1011, 1252, 307, 516, 281, 312, 833, 6845, 307, 406, 516, 50550], "temperature": 0.0, "avg_logprob": -0.10858698280490174, "compression_ratio": 1.744, "no_speech_prob": 0.060041557997465134}, {"id": 731, "seek": 334050, "start": 3344.22, "end": 3348.66, "text": " to be utilizing the full capacity of the network and anything taken from the right hand side", "tokens": [50550, 281, 312, 26775, 264, 1577, 6042, 295, 264, 3209, 293, 1340, 2726, 490, 264, 558, 1011, 1252, 50772], "temperature": 0.0, "avg_logprob": -0.10858698280490174, "compression_ratio": 1.744, "no_speech_prob": 0.060041557997465134}, {"id": 732, "seek": 334050, "start": 3348.66, "end": 3356.3, "text": " is overfit and actually performing worse than it needs to on that held out test set.", "tokens": [50772, 307, 670, 6845, 293, 767, 10205, 5324, 813, 309, 2203, 281, 322, 300, 5167, 484, 1500, 992, 13, 51154], "temperature": 0.0, "avg_logprob": -0.10858698280490174, "compression_ratio": 1.744, "no_speech_prob": 0.060041557997465134}, {"id": 733, "seek": 334050, "start": 3356.3, "end": 3362.42, "text": " So I'll conclude this lecture by summarizing three key points that we've covered so far.", "tokens": [51154, 407, 286, 603, 16886, 341, 7991, 538, 14611, 3319, 1045, 2141, 2793, 300, 321, 600, 5343, 370, 1400, 13, 51460], "temperature": 0.0, "avg_logprob": -0.10858698280490174, "compression_ratio": 1.744, "no_speech_prob": 0.060041557997465134}, {"id": 734, "seek": 334050, "start": 3362.42, "end": 3367.34, "text": " We started about the fundamental building blocks of neural networks the perceptron.", "tokens": [51460, 492, 1409, 466, 264, 8088, 2390, 8474, 295, 18161, 9590, 264, 43276, 2044, 13, 51706], "temperature": 0.0, "avg_logprob": -0.10858698280490174, "compression_ratio": 1.744, "no_speech_prob": 0.060041557997465134}, {"id": 735, "seek": 336734, "start": 3367.34, "end": 3372.78, "text": " We learned about stacking and composing these perceptrons together to form complex hierarchical", "tokens": [50364, 492, 3264, 466, 41376, 293, 715, 6110, 613, 43276, 13270, 1214, 281, 1254, 3997, 35250, 804, 50636], "temperature": 0.0, "avg_logprob": -0.12921340648944563, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.017437206581234932}, {"id": 736, "seek": 336734, "start": 3372.78, "end": 3379.1000000000004, "text": " neural networks and how to mathematically optimize these models with back propagation.", "tokens": [50636, 18161, 9590, 293, 577, 281, 44003, 19719, 613, 5245, 365, 646, 38377, 13, 50952], "temperature": 0.0, "avg_logprob": -0.12921340648944563, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.017437206581234932}, {"id": 737, "seek": 336734, "start": 3379.1000000000004, "end": 3383.94, "text": " And finally we address the practical side of these models that you'll find useful for", "tokens": [50952, 400, 2721, 321, 2985, 264, 8496, 1252, 295, 613, 5245, 300, 291, 603, 915, 4420, 337, 51194], "temperature": 0.0, "avg_logprob": -0.12921340648944563, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.017437206581234932}, {"id": 738, "seek": 336734, "start": 3383.94, "end": 3389.6600000000003, "text": " the labs today including adaptive learning rates, batching and regularization.", "tokens": [51194, 264, 20339, 965, 3009, 27912, 2539, 6846, 11, 15245, 278, 293, 3890, 2144, 13, 51480], "temperature": 0.0, "avg_logprob": -0.12921340648944563, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.017437206581234932}, {"id": 739, "seek": 336734, "start": 3389.6600000000003, "end": 3394.1800000000003, "text": " So thank you for attending the first lecture in 6S191.", "tokens": [51480, 407, 1309, 291, 337, 15862, 264, 700, 7991, 294, 1386, 50, 3405, 16, 13, 51706], "temperature": 0.0, "avg_logprob": -0.12921340648944563, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.017437206581234932}, {"id": 740, "seek": 336734, "start": 3394.1800000000003, "end": 3394.86, "text": " Thank you very much.", "tokens": [51706, 1044, 291, 588, 709, 13, 51740], "temperature": 0.0, "avg_logprob": -0.12921340648944563, "compression_ratio": 1.6395348837209303, "no_speech_prob": 0.017437206581234932}], "language": "en"}