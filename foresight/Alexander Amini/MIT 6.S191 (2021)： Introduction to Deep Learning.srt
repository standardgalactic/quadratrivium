1
00:00:00,000 --> 00:00:17,160
Good afternoon, everyone, and welcome to MIT 6S191, Introduction to Deep Learning.

2
00:00:17,160 --> 00:00:21,320
My name is Alexander Amini and I'm so excited to be your instructor this year along with

3
00:00:21,320 --> 00:00:24,640
Aviso Imani in this new virtual format.

4
00:00:24,640 --> 00:00:29,960
6S191 is a two-week boot camp on everything deep learning and will cover a ton of material

5
00:00:30,240 --> 00:00:31,560
in only two weeks.

6
00:00:31,560 --> 00:00:35,160
So I think it's really important for us to dive right in with these lectures.

7
00:00:35,160 --> 00:00:39,600
But before we do that, I do want to motivate exactly why I think this is such an awesome

8
00:00:39,600 --> 00:00:41,360
field to study.

9
00:00:41,360 --> 00:00:45,880
And when we taught this class last year, I decided to try introducing the class very

10
00:00:45,880 --> 00:00:46,880
differently.

11
00:00:46,880 --> 00:00:53,400
And instead of me telling the class how great 6S191 is, I wanted to let someone else do

12
00:00:53,400 --> 00:00:54,480
that instead.

13
00:00:54,480 --> 00:01:00,640
So actually, I want to start this year by showing you how we introduced 6S191 last year.

14
00:01:00,640 --> 00:01:11,440
Hi, everybody, and welcome to MIT 6S191, the official introductory course on deep learning

15
00:01:11,440 --> 00:01:14,440
taught here at MIT.

16
00:01:14,440 --> 00:01:22,720
Deep learning is revolutionizing so many fields from robotics, medicine, and everything

17
00:01:22,720 --> 00:01:23,720
in between.

18
00:01:23,720 --> 00:01:31,520
You'll learn from the medals of this field and how you can build some of these incredible

19
00:01:31,520 --> 00:01:32,520
algorithms.

20
00:01:32,520 --> 00:01:42,720
In fact, this entire speech and video are not real and were created using deep learning

21
00:01:42,720 --> 00:01:44,720
and artificial intelligence.

22
00:01:44,720 --> 00:01:48,720
And in this class, you'll learn how.

23
00:01:49,720 --> 00:01:59,720
It has been an honor to speak with you today, and I hope you enjoyed the course.

24
00:01:59,720 --> 00:02:03,720
So in case you couldn't tell, that was actually not a real video or audio.

25
00:02:03,720 --> 00:02:08,720
And the audio you actually heard was purposely degraded, a bit more to even make it more

26
00:02:08,720 --> 00:02:12,720
obvious that this was not real and avoid some potential misuse.

27
00:02:13,720 --> 00:02:18,720
Even with the purposely degraded audio, that intro went somewhat viral last year after

28
00:02:18,720 --> 00:02:22,420
the course and we got some really great and interesting feedback.

29
00:02:22,420 --> 00:02:27,080
And to be honest, after last year and when we did this, I thought it was going to be

30
00:02:27,080 --> 00:02:31,000
really hard for us to top it this year.

31
00:02:31,000 --> 00:02:34,720
But actually, I was wrong because the one thing I love about this field is that it's

32
00:02:34,720 --> 00:02:40,720
moving so incredibly fast that even within the past year, the state of the art has significantly

33
00:02:40,720 --> 00:02:41,720
advanced.

34
00:02:41,720 --> 00:02:47,480
And the video you saw that we used last year used deep learning, but it was not a particularly

35
00:02:47,480 --> 00:02:49,520
easy video to create.

36
00:02:49,520 --> 00:02:54,680
It required a full video of Obama speaking, and it used this to intelligently stitch

37
00:02:54,680 --> 00:02:59,440
together parts of the scene to make it look and appear like he was mouthing the words

38
00:02:59,440 --> 00:03:00,440
that I said.

39
00:03:00,440 --> 00:03:05,720
And to see the behind the scenes here, now you can see the same video with my voice.

40
00:03:05,720 --> 00:03:15,520
Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning

41
00:03:15,520 --> 00:03:18,960
taught here at MIT.

42
00:03:18,960 --> 00:03:24,380
Now it's actually possible to use just a single static image, not the full video to

43
00:03:24,380 --> 00:03:27,160
achieve the exact same thing.

44
00:03:27,160 --> 00:03:34,800
And now you can actually see eight more examples of Obama now just created using just a single

45
00:03:34,800 --> 00:03:41,000
static image, no more full dynamic videos, but we can achieve the same incredible realism

46
00:03:41,000 --> 00:03:43,440
and result using deep learning.

47
00:03:43,440 --> 00:03:49,200
Now of course, there's nothing restricting us to one person.

48
00:03:49,200 --> 00:03:54,200
This method generalizes to different faces, and there's nothing restricting us even to

49
00:03:54,200 --> 00:04:00,160
humans anymore or individuals that the algorithm has ever seen before.

50
00:04:00,160 --> 00:04:09,440
Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning

51
00:04:09,440 --> 00:04:13,960
taught here at MIT.

52
00:04:13,960 --> 00:04:20,080
The ability to generate these types of dynamic moving videos from only a single image is

53
00:04:20,080 --> 00:04:24,680
remarkable to me, and it's a testament to the true power of deep learning.

54
00:04:24,680 --> 00:04:30,640
In this class, you're going to actually not only learn about the technical basis of this

55
00:04:30,640 --> 00:04:39,800
technology, but also some of the very important and very important ethical and societal implications

56
00:04:39,800 --> 00:04:42,080
of this work as well.

57
00:04:42,080 --> 00:04:47,720
Now I hope this was a really great way to get you excited about this course and 6S191,

58
00:04:47,720 --> 00:04:50,240
and with that let's get started.

59
00:04:50,240 --> 00:04:54,240
We can actually start by taking a step back and asking ourselves what is deep learning,

60
00:04:54,320 --> 00:04:58,000
deep learning, and the context of intelligence.

61
00:04:58,000 --> 00:05:04,680
Intelligence is actually the ability to process information such that it can be used to inform

62
00:05:04,680 --> 00:05:06,680
a future decision.

63
00:05:06,680 --> 00:05:12,040
Now the field of artificial intelligence, or AI, is a science that actually focuses

64
00:05:12,040 --> 00:05:17,360
on building algorithms to do exactly this, to build algorithms to process information

65
00:05:17,360 --> 00:05:21,640
such that they can inform future predictions.

66
00:05:21,640 --> 00:05:27,440
Now machine learning, you can think of this as just a subset of AI that actually focuses

67
00:05:27,440 --> 00:05:33,160
on teaching an algorithm to learn from experiences without being explicitly programmed.

68
00:05:33,160 --> 00:05:37,480
Now deep learning takes this idea even further, and it's a subset of machine learning that

69
00:05:37,480 --> 00:05:44,120
focuses on using neural networks to automatically extract useful patterns in raw data, and then

70
00:05:44,120 --> 00:05:49,520
using these patterns or features to learn to perform that task.

71
00:05:49,520 --> 00:05:51,800
And that's exactly what this class is about.

72
00:05:51,800 --> 00:05:57,200
This class is about teaching algorithms how to learn a task directly from raw data.

73
00:05:57,200 --> 00:06:03,000
And we want to provide you with a solid foundation both technically and practically for you to

74
00:06:03,000 --> 00:06:11,080
understand under the hood how these algorithms are built and how they can learn.

75
00:06:11,080 --> 00:06:16,480
So this course is split between technical lectures as well as project software labs.

76
00:06:16,480 --> 00:06:20,600
We'll cover the foundation starting today with neural networks, which are really the

77
00:06:20,600 --> 00:06:24,960
building blocks of everything that we'll see in this course.

78
00:06:24,960 --> 00:06:30,880
And this year we also have two brand new really exciting hot topic lectures, focusing on uncertainty

79
00:06:30,880 --> 00:06:36,800
and probabilistic deep learning, as well as algorithmic bias and fairness.

80
00:06:36,800 --> 00:06:41,520
Finally we'll conclude with some really exciting guest lectures and student project presentations

81
00:06:41,520 --> 00:06:46,800
as part of a final project competition that all of you will be eligible to win some really

82
00:06:46,800 --> 00:06:49,440
exciting prizes.

83
00:06:49,440 --> 00:06:53,560
Now a bit of logistics before we dive into the technical side of the lecture.

84
00:06:53,560 --> 00:06:58,240
For those of you taking this course for credit, you will have two options to fulfill your

85
00:06:58,240 --> 00:06:59,800
credit requirement.

86
00:06:59,800 --> 00:07:07,600
The first option will be to actually work in teams of up to four or individually to develop

87
00:07:07,600 --> 00:07:10,880
a cool new deep learning idea.

88
00:07:10,920 --> 00:07:14,960
Now doing so will make you eligible to win some of the prizes that you can see on the

89
00:07:14,960 --> 00:07:17,000
right hand side.

90
00:07:17,000 --> 00:07:21,360
And we realize that in the context of this class, which is only two weeks, that's an

91
00:07:21,360 --> 00:07:25,840
extremely short amount of time to come up with an impressive project or research idea.

92
00:07:25,840 --> 00:07:30,840
So we're not going to be judging you on the novelty of that idea, but rather we're not

93
00:07:30,840 --> 00:07:35,360
going to be judging you on the results of that idea, but rather the novelty of the idea,

94
00:07:35,360 --> 00:07:39,880
your thinking process and how impactful this idea can be.

95
00:07:39,880 --> 00:07:43,560
But not on the results themselves.

96
00:07:43,560 --> 00:07:48,080
On the last day of class, you will actually give a three minute presentation to a group

97
00:07:48,080 --> 00:07:52,000
of judges who will then award the winners and the prizes.

98
00:07:52,000 --> 00:07:57,320
Now again, three minutes is extremely short to actually present your ideas and present

99
00:07:57,320 --> 00:08:03,360
your project, but I do believe that there's an art to presenting and conveying your ideas

100
00:08:03,360 --> 00:08:06,720
concisely and clearly in such a short amount of time.

101
00:08:06,720 --> 00:08:12,720
So we will be holding you strictly to that strict deadline.

102
00:08:12,720 --> 00:08:18,120
The second option to fulfill your grade requirement is to write a one page review on a deep learning

103
00:08:18,120 --> 00:08:19,640
paper.

104
00:08:19,640 --> 00:08:23,040
Here the grade is based more on the clarity of the writing and the technical communication

105
00:08:23,040 --> 00:08:25,000
of the main ideas.

106
00:08:25,000 --> 00:08:29,400
This will be due on Thursday, the last Thursday of the class, and you can pick whatever deep

107
00:08:29,400 --> 00:08:30,880
learning paper you would like.

108
00:08:30,880 --> 00:08:36,520
If you would like some pointers, we have provided some guide papers that can help you get started

109
00:08:36,520 --> 00:08:40,720
if you would just like to use one of those for your review.

110
00:08:40,720 --> 00:08:48,240
In addition to the final project prizes, we'll also be awarding this year three lab prizes,

111
00:08:48,240 --> 00:08:51,880
one associated to each of the software labs that students will complete.

112
00:08:51,880 --> 00:08:56,840
Again, completion of the software labs is not required for grade of this course, but

113
00:08:56,840 --> 00:08:59,640
it will make you eligible for some of these cool prizes.

114
00:08:59,640 --> 00:09:05,840
So please, we encourage everyone to compete for these prizes and get the opportunity to

115
00:09:05,840 --> 00:09:08,920
win them all.

116
00:09:08,920 --> 00:09:12,080
Please post to Piazza if you have any questions.

117
00:09:12,080 --> 00:09:17,200
Visit the course website for announcements and digital recordings of the lectures, etc.

118
00:09:17,200 --> 00:09:20,840
And please email us if you have any questions.

119
00:09:20,840 --> 00:09:26,040
Also there are software labs and office hours right after each of these technical lectures

120
00:09:26,040 --> 00:09:28,080
held in Gather Town.

121
00:09:28,080 --> 00:09:33,040
So please drop by in Gather Town to ask any questions about the software labs, specifically

122
00:09:33,040 --> 00:09:37,320
on those, or more generally about past software labs or about the lecture that occurred that

123
00:09:37,320 --> 00:09:40,520
day.

124
00:09:40,520 --> 00:09:48,200
Now this course has an incredible group of TAs and teaching assistants that you can reach

125
00:09:48,200 --> 00:09:54,040
out to at any time in case you have any issues or questions about the material that you're

126
00:09:54,040 --> 00:09:56,040
learning.

127
00:09:56,040 --> 00:10:00,200
And finally, we want to give a huge thanks to all of our sponsors who, without their

128
00:10:00,240 --> 00:10:03,320
help, this class would not be possible.

129
00:10:03,320 --> 00:10:07,880
This is the fourth year that we're teaching this class, and each year it just keeps getting

130
00:10:07,880 --> 00:10:12,080
bigger and bigger and bigger, and we really give a huge shout out to our sponsors for

131
00:10:12,080 --> 00:10:15,400
helping us make this happen each year.

132
00:10:15,400 --> 00:10:19,600
And especially this year in light of the virtual format.

133
00:10:19,600 --> 00:10:22,080
So now let's start with the fun stuff.

134
00:10:22,080 --> 00:10:27,800
Let's start by asking ourselves a question about why do we all care about deep learning?

135
00:10:27,800 --> 00:10:32,120
And specifically, why do we care right now?

136
00:10:32,120 --> 00:10:38,280
To understand that, it's important to actually understand first why is deep learning or how

137
00:10:38,280 --> 00:10:42,000
is deep learning different from traditional machine learning?

138
00:10:42,000 --> 00:10:47,880
Now traditionally, machine learning algorithms define a set of features in their data.

139
00:10:47,880 --> 00:10:53,440
Usually these are features that are handcrafted or hand engineered, and as a result they tend

140
00:10:53,440 --> 00:10:56,920
to be pretty brittle in practice when they're deployed.

141
00:10:56,920 --> 00:11:02,440
The key idea of deep learning is to learn these features directly from data in a hierarchical

142
00:11:02,440 --> 00:11:03,760
manner.

143
00:11:03,760 --> 00:11:08,640
That is, can we learn, if we want to learn how to detect a face, for example, can we

144
00:11:08,640 --> 00:11:14,880
learn to first start by detecting edges in the image, composing these edges together

145
00:11:14,880 --> 00:11:20,760
to detect mid-level features such as a eye or a nose or a mouth, and then going deeper

146
00:11:20,760 --> 00:11:26,680
and composing these features into structural, facial features, so that we can recognize

147
00:11:26,680 --> 00:11:29,440
this face.

148
00:11:29,440 --> 00:11:34,280
This hierarchical way of thinking is really core to deep learning as core to everything

149
00:11:34,280 --> 00:11:37,880
that we're going to learn in this class.

150
00:11:37,880 --> 00:11:43,480
Actually the fundamental building blocks though of deep learning and neural networks have

151
00:11:43,480 --> 00:11:45,480
actually existed for decades.

152
00:11:45,480 --> 00:11:49,920
So one interesting thing to consider is why are we studying this now?

153
00:11:49,920 --> 00:11:55,760
Now is an incredibly amazing time to study these algorithms, and for one reason is because

154
00:11:55,760 --> 00:11:58,800
data has become much more pervasive.

155
00:11:58,800 --> 00:12:04,640
These models are extremely hungry for data, and at the moment we're living in an era

156
00:12:04,640 --> 00:12:07,480
where we have more data than ever before.

157
00:12:07,480 --> 00:12:13,160
Secondly, these algorithms are massively parallelizable, so they can benefit tremendously

158
00:12:13,160 --> 00:12:19,320
from modern GPU hardware that simply did not exist when these algorithms were developed.

159
00:12:19,320 --> 00:12:24,360
And finally, due to open source toolboxes like TensorFlow, building and deploying these

160
00:12:24,360 --> 00:12:30,200
models has become extremely streamlined.

161
00:12:30,200 --> 00:12:35,180
So let's start actually with the fundamental building block of deep learning and of every

162
00:12:35,180 --> 00:12:36,620
neural network.

163
00:12:36,620 --> 00:12:40,760
That is just a single neuron, also known as a perceptron.

164
00:12:40,760 --> 00:12:45,520
So we're going to walk through exactly what is a perceptron, how it's defined, and we're

165
00:12:45,520 --> 00:12:49,920
going to build our way up to deeper neural networks all the way from there.

166
00:12:49,920 --> 00:12:54,400
So let's start really at the basic building block.

167
00:12:54,400 --> 00:12:59,000
The idea of a perceptron or a single neuron is actually very simple.

168
00:12:59,000 --> 00:13:03,720
So I think it's really important for all of you to understand this at its core.

169
00:13:03,720 --> 00:13:08,200
Let's start by actually talking about the forward propagation of information through

170
00:13:08,200 --> 00:13:10,280
this single neuron.

171
00:13:10,280 --> 00:13:16,840
We can define a set of inputs xi through xm, which you can see on the left-hand side,

172
00:13:16,840 --> 00:13:21,840
and each of these inputs or each of these numbers are multiplied by their corresponding

173
00:13:21,840 --> 00:13:25,040
weight and then added together.

174
00:13:25,040 --> 00:13:30,240
We take this single number, the result of that addition, and pass it through what's

175
00:13:30,240 --> 00:13:37,680
called a nonlinear activation function to produce our final output y.

176
00:13:37,680 --> 00:13:42,400
We can actually, actually this is not entirely correct because one thing I forgot to mention

177
00:13:42,400 --> 00:13:47,200
is that we also have what's called a bias term in here, which allows you to shift your

178
00:13:47,200 --> 00:13:50,160
activation function left or right.

179
00:13:50,160 --> 00:13:55,560
Now on the right-hand side of this diagram, you can actually see this concept illustrated

180
00:13:55,560 --> 00:13:59,240
or written out mathematically as a single equation.

181
00:13:59,240 --> 00:14:04,760
You can actually rewrite this in terms of linear algebra matrix multiplications and

182
00:14:04,760 --> 00:14:10,840
dot products to represent this a bit more concisely.

183
00:14:10,840 --> 00:14:13,120
So let's do that.

184
00:14:13,120 --> 00:14:19,200
Let's now do that with x, capital X, which is a vector of our inputs, x1 through xm,

185
00:14:19,200 --> 00:14:24,040
and capital W, which is a vector of our weights, w1 through wm.

186
00:14:24,040 --> 00:14:30,280
So each of these are vectors of length m, and the output is very simply obtained by taking

187
00:14:30,280 --> 00:14:39,280
their dot product, adding a bias, which in this case is w0, and then applying a nonlinearity,

188
00:14:39,280 --> 00:14:41,880
g.

189
00:14:41,880 --> 00:14:47,080
One thing is that I haven't, I've been mentioning it a couple of times, this nonlinearity, g.

190
00:14:47,080 --> 00:14:49,280
What exactly is it?

191
00:14:49,280 --> 00:14:53,560
Because I've mentioned it now a couple of times, well, it is a nonlinear function.

192
00:14:53,560 --> 00:15:00,160
One common example of this nonlinear activation function is what is known as the sigmoid function.

193
00:15:00,240 --> 00:15:02,760
Defined here on the right.

194
00:15:02,760 --> 00:15:06,120
In fact, there are many types of nonlinear functions.

195
00:15:06,120 --> 00:15:10,560
You can see three more examples here, including the sigmoid function.

196
00:15:10,560 --> 00:15:15,600
And throughout this presentation, you'll actually see these TensorFlow code blocks,

197
00:15:15,600 --> 00:15:20,200
which will actually illustrate how we can take some of the topics that we're learning

198
00:15:20,200 --> 00:15:27,080
in this class and actually practically use them using the TensorFlow software library.

199
00:15:27,520 --> 00:15:31,920
Now the sigmoid activation function, which I presented on the previous slide, is very

200
00:15:31,920 --> 00:15:34,400
popular since it's a function that gives outputs.

201
00:15:34,400 --> 00:15:40,880
It takes as input any real number, any activation value, and it outputs a number always between

202
00:15:40,880 --> 00:15:42,640
zero and one.

203
00:15:42,640 --> 00:15:47,000
So this makes it really, really suitable for problems and probability, because probabilities

204
00:15:47,000 --> 00:15:49,160
also have to be between zero and one.

205
00:15:49,160 --> 00:15:52,920
So this makes them very well suited for those types of problems.

206
00:15:52,920 --> 00:15:57,400
In modern deep neural networks, the ReLU activation function, which you can see on the

207
00:15:57,400 --> 00:16:01,440
right, is also extremely popular because of its simplicity.

208
00:16:01,440 --> 00:16:04,280
In this case, it's a piecewise linear function.

209
00:16:04,280 --> 00:16:11,160
It is zero before when it's in the negative regime, and it is strictly the identity function

210
00:16:11,160 --> 00:16:13,600
in the positive regime.

211
00:16:13,600 --> 00:16:20,280
But one really important question that I hope that you're asking yourselves right now is

212
00:16:20,320 --> 00:16:23,960
why do we even need activation functions?

213
00:16:23,960 --> 00:16:29,440
I think actually throughout this course, I do want to say that no matter what I say in

214
00:16:29,440 --> 00:16:34,640
the course, I hope that always you're questioning why this is a necessary step and why do we

215
00:16:34,640 --> 00:16:38,240
need each of these steps, because often these are the questions that can lead to really

216
00:16:38,240 --> 00:16:40,720
amazing research breakthroughs.

217
00:16:40,720 --> 00:16:43,480
So why do we need activation functions?

218
00:16:43,480 --> 00:16:48,040
Now the point of an activation function is to actually introduce non-linearities into

219
00:16:48,080 --> 00:16:51,680
our network, because these are non-linear functions.

220
00:16:51,680 --> 00:16:55,920
And it allows us to actually deal with non-linear data.

221
00:16:55,920 --> 00:17:01,120
This is extremely important in real life, especially because in the real world, data

222
00:17:01,120 --> 00:17:03,840
is almost always non-linear.

223
00:17:03,840 --> 00:17:07,640
Imagine I told you to separate here the green points from the red points, but all you could

224
00:17:07,640 --> 00:17:10,280
use is a single straight line.

225
00:17:10,280 --> 00:17:15,480
You might think this is easy with multiple lines or curved lines, but you can only use

226
00:17:15,480 --> 00:17:18,000
a single straight line.

227
00:17:18,000 --> 00:17:22,040
And that's what using a neural network with a linear activation function would be like.

228
00:17:22,040 --> 00:17:26,840
That makes the problem really hard, because no matter how deep the neural network is,

229
00:17:26,840 --> 00:17:32,360
you'll only be able to produce a single line decision boundary, and you're only able to

230
00:17:32,360 --> 00:17:36,080
separate your space with one line.

231
00:17:36,080 --> 00:17:42,080
Now using non-linear activation functions allows your neural network to approximate arbitrarily

232
00:17:42,080 --> 00:17:43,840
complex functions.

233
00:17:43,840 --> 00:17:49,200
And that's what makes neural networks extraordinarily powerful.

234
00:17:49,200 --> 00:17:53,520
Let's understand this with a simple example so that we can build up our intuition even

235
00:17:53,520 --> 00:17:55,120
further.

236
00:17:55,120 --> 00:18:01,120
Imagine I give you this trained network, now with weights on the left hand side, 3 and

237
00:18:01,120 --> 00:18:03,120
negative 2.

238
00:18:03,120 --> 00:18:07,040
This network only has two inputs, x1 and x2.

239
00:18:07,040 --> 00:18:11,920
If we want to get the output of it, we simply do the same story as I said before.

240
00:18:12,280 --> 00:18:18,400
First take a dot product of our inputs with our weights, add the bias, and apply a non-linearity.

241
00:18:18,400 --> 00:18:23,760
But let's take a look at what's inside of that non-linearity.

242
00:18:23,760 --> 00:18:31,480
It's simply a weighted combination of our inputs in the form of a two dimensional line,

243
00:18:31,480 --> 00:18:34,680
because in this case we only have two inputs.

244
00:18:34,680 --> 00:18:39,360
So if we want to compute this output, it's the same stories before, we take a dot product

245
00:18:39,360 --> 00:18:43,640
of x and w, we add our bias, and apply our non-linearity.

246
00:18:43,640 --> 00:18:47,120
What about what's inside of this non-linearity g?

247
00:18:47,120 --> 00:18:51,040
Well, this is just a 2D line.

248
00:18:51,040 --> 00:18:55,560
In fact, since it's just a two dimensional line, we can even plot it in two dimensional

249
00:18:55,560 --> 00:18:56,560
space.

250
00:18:56,560 --> 00:18:58,400
This is called the feature space, the input space.

251
00:18:58,400 --> 00:19:04,200
In this case, the feature space and the input space are equal because we only have one neuron.

252
00:19:04,200 --> 00:19:07,160
So in this plot, let me describe what you're seeing.

253
00:19:07,160 --> 00:19:10,440
So on the two axes, you're seeing our two inputs.

254
00:19:10,440 --> 00:19:15,640
So on one axis is x1, one of the inputs, on the other axis is x2, our other input.

255
00:19:15,640 --> 00:19:20,120
And we can plot the line here, our decision boundary of this trained neural network that

256
00:19:20,120 --> 00:19:23,920
I gave you, as a line in this space.

257
00:19:23,920 --> 00:19:28,080
Now this line corresponds to actually all of the decisions that this neural network

258
00:19:28,080 --> 00:19:29,960
can make.

259
00:19:29,960 --> 00:19:35,600
Because if I give you a new data point, for example here I'm giving you negative 1, 2,

260
00:19:35,600 --> 00:19:41,080
this point lies somewhere in the space, specifically at x1 equal to negative 1 and x2 equal to

261
00:19:41,080 --> 00:19:42,080
2.

262
00:19:42,080 --> 00:19:44,280
That's just a point in the space.

263
00:19:44,280 --> 00:19:51,600
I want you to compute its weighted combination and I can actually follow the perceptron equation

264
00:19:51,600 --> 00:19:53,560
to get the answer.

265
00:19:53,560 --> 00:19:59,560
So here we can see that if we plug it into the perceptron equation, we get 1 plus minus

266
00:19:59,560 --> 00:20:02,240
3 minus 4.

267
00:20:02,240 --> 00:20:04,560
And the result would be minus 6.

268
00:20:04,560 --> 00:20:13,400
We plug that into our nonlinear activation function g and we get a final output of 0.002.

269
00:20:13,400 --> 00:20:19,520
Now in fact, remember that the sigmoid function actually divides this space into two parts

270
00:20:19,520 --> 00:20:23,520
of either because it outputs everything between 0 and 1.

271
00:20:23,520 --> 00:20:31,760
It's dividing it between 0.5 and greater than 0.5 and less than 0.5.

272
00:20:31,760 --> 00:20:39,720
When the input is less than 0 and greater than 0.5, that's when the input is positive.

273
00:20:39,720 --> 00:20:43,840
We can illustrate this space actually, but this feature space, when we're dealing with

274
00:20:43,840 --> 00:20:49,400
a small dimensional data, like in this case we only have two dimensions.

275
00:20:49,400 --> 00:20:53,880
But soon we'll start to talk about problems where we have thousands or millions or in

276
00:20:53,880 --> 00:20:58,840
some cases even billions of weights in our neural network.

277
00:20:58,840 --> 00:21:04,880
And then drawing these types of plots becomes extremely challenging and not really possible

278
00:21:04,880 --> 00:21:05,880
anymore.

279
00:21:05,880 --> 00:21:09,920
But at least when we're in this regime of small number of inputs and small number of

280
00:21:09,920 --> 00:21:13,960
weights, we can make these plots to really understand the entire space.

281
00:21:13,960 --> 00:21:19,520
And for any new input that we obtain, for example an input right here, we can see exactly

282
00:21:19,520 --> 00:21:27,040
that this point is going to be having an activation function less than 0 and its output will be

283
00:21:27,040 --> 00:21:28,720
less than 0.5.

284
00:21:28,720 --> 00:21:33,320
The magnitude of that actually is computed by plugging it into the perceptron equation.

285
00:21:33,320 --> 00:21:37,800
So we can't avoid that, but we can immediately get an answer on the decision boundary, depending

286
00:21:37,800 --> 00:21:44,480
on which side of this hyperplane that we lie on when we plug it in.

287
00:21:44,480 --> 00:21:49,440
So now that we have an idea of how to build a perceptron, let's start by building neural

288
00:21:49,440 --> 00:21:53,960
networks and seeing how they all come together.

289
00:21:53,960 --> 00:21:58,180
So let's revisit that diagram of the perceptron that I showed you before.

290
00:21:58,180 --> 00:22:02,740
If there's only a few things that you get from this class, I really want everyone to

291
00:22:02,740 --> 00:22:05,540
take away how a perceptron works.

292
00:22:05,540 --> 00:22:07,900
And there's three steps, remember them always.

293
00:22:07,900 --> 00:22:12,980
The dot product, you take a dot product of your inputs and your weights.

294
00:22:12,980 --> 00:22:16,700
You add a bias and you apply your non-linearity.

295
00:22:16,700 --> 00:22:19,660
There's three steps.

296
00:22:19,660 --> 00:22:21,940
Let's simplify this diagram a little bit.

297
00:22:21,940 --> 00:22:25,060
Let's clean up some of the arrows and remove the bias.

298
00:22:25,060 --> 00:22:30,500
And we can actually see now that every line here has its own associated weight to it.

299
00:22:30,500 --> 00:22:34,100
And I'll remove the bias term, like I said, for simplicity.

300
00:22:34,100 --> 00:22:40,740
Note that z here is the result of that dot product plus bias, before we apply the activation

301
00:22:40,740 --> 00:22:43,140
function, though, g.

302
00:22:43,140 --> 00:22:48,660
The final output, though, is simply y, which is equal to the activation function of z, which

303
00:22:48,660 --> 00:22:51,580
is our activation value.

304
00:22:52,580 --> 00:22:58,180
Now, if we want to define a multi-output neural network, we can simply add another

305
00:22:58,180 --> 00:23:00,020
perceptron to this picture.

306
00:23:00,020 --> 00:23:04,540
So instead of having one perceptron, now we have two perceptrons and two outputs.

307
00:23:04,540 --> 00:23:09,860
Each one is a normal perceptron, exactly like we saw before, taking its inputs from each

308
00:23:09,860 --> 00:23:17,060
of the x1's through xm's, taking the dot product, adding a bias, and that's it.

309
00:23:17,060 --> 00:23:18,260
Now we have two outputs.

310
00:23:18,260 --> 00:23:21,940
Each of those perceptrons, though, will have a different set of weights.

311
00:23:21,940 --> 00:23:22,940
Remember that.

312
00:23:22,940 --> 00:23:25,660
We'll get back to that.

313
00:23:25,660 --> 00:23:30,980
If we want, so actually one thing to keep in mind here is because all the inputs are

314
00:23:30,980 --> 00:23:37,580
densely connected, every input has a connection to the weights of every perceptron.

315
00:23:37,580 --> 00:23:42,140
These are often called dense layers, or sometimes fully connected layers.

316
00:23:42,140 --> 00:23:47,540
Now, through this class, you're going to get a lot of experience actually coding up

317
00:23:47,580 --> 00:23:53,540
and practically creating some of these algorithms using a software toolbox called TensorFlow.

318
00:23:53,540 --> 00:23:59,020
So now that we have the understanding of how a single perceptron works and how a dense

319
00:23:59,020 --> 00:24:04,860
layer works, this is a stack of perceptrons, let's try and see how we can actually build

320
00:24:04,860 --> 00:24:09,340
up a dense layer like this all the way from scratch.

321
00:24:09,340 --> 00:24:14,580
To do that, we can actually start by initializing the two components of our dense layer, which

322
00:24:14,620 --> 00:24:17,620
are the weights and the biases.

323
00:24:17,620 --> 00:24:22,500
Now that we have these two parameters of our neural network, of our dense layer, we can

324
00:24:22,500 --> 00:24:26,940
actually define the forward propagation of information, just like we saw it and learned

325
00:24:26,940 --> 00:24:28,500
about already.

326
00:24:28,500 --> 00:24:33,540
That forward propagation of information is simply the dot product or the matrix multiplication

327
00:24:33,540 --> 00:24:40,540
of our inputs with our weights, add a bias, that gives us our activation function here,

328
00:24:41,380 --> 00:24:46,380
and then we apply this nonlinearity to compute the output.

329
00:24:46,380 --> 00:24:52,380
Now, TensorFlow has actually implemented this dense layer for us.

330
00:24:52,380 --> 00:24:57,140
So we don't need to do that from scratch, instead we can just call it like shown here.

331
00:24:57,140 --> 00:25:04,140
So to create a dense layer with two outputs, we can specify this units equal to two.

332
00:25:04,140 --> 00:25:08,380
Now let's take a look at what's called a single layered neural network.

333
00:25:08,380 --> 00:25:13,860
This is one we have a single hidden layer between our inputs and our outputs.

334
00:25:13,860 --> 00:25:19,140
This layer is called the hidden layer, because unlike an input layer and an output layer,

335
00:25:19,140 --> 00:25:24,260
the states of this hidden layer are typically unobserved, they're hidden to some extent,

336
00:25:24,260 --> 00:25:26,820
they're not strictly enforced either.

337
00:25:26,820 --> 00:25:31,060
And since we have this transformation now from the input layer to the hidden layer and

338
00:25:31,060 --> 00:25:36,660
from the hidden layer to the output layer, each of these layers are going to have their

339
00:25:36,740 --> 00:25:39,700
own specified weight matrices.

340
00:25:39,700 --> 00:25:45,660
We'll call w1 the weight matrices for the first layer and w2 the weight matrix for the

341
00:25:45,660 --> 00:25:48,580
second layer.

342
00:25:48,580 --> 00:25:55,580
If we take a zoomed in look at one of the neurons in this hidden layer, let's take

343
00:25:55,580 --> 00:26:01,540
for example z2 for example, this is the exact same perceptron that we saw before.

344
00:26:01,540 --> 00:26:07,700
We can compute its output, again using the exact same story, taking all of its inputs

345
00:26:07,700 --> 00:26:12,820
x1 through xm, applying a dot product with the weights, adding a bias and that gives

346
00:26:12,820 --> 00:26:14,860
us z2.

347
00:26:14,860 --> 00:26:20,340
If we look at a different neuron, let's suppose z3, we'll get a different value here because

348
00:26:20,340 --> 00:26:26,100
the weights leading to z3 are probably different than those leading to z2.

349
00:26:26,100 --> 00:26:32,860
Now this picture looks a bit messy, so let's try and clean things up a bit more.

350
00:26:32,860 --> 00:26:38,020
From now on, I'll just use this symbol here to denote what we call this dense layer or

351
00:26:38,020 --> 00:26:40,340
fully connected layers.

352
00:26:40,340 --> 00:26:46,820
And here you can actually see an example of how we can create this exact neural network

353
00:26:46,820 --> 00:26:51,220
again using TensorFlow with the predefined dense layer notation.

354
00:26:51,220 --> 00:26:55,980
Here we're creating a sequential model where we can stack layers on top of each other.

355
00:26:55,980 --> 00:27:04,300
This layer with n neurons and the second layer with 2 neurons, the output layer.

356
00:27:04,300 --> 00:27:09,020
And if we want to create a deep neural network, all we have to do is keep stacking these layers

357
00:27:09,020 --> 00:27:14,940
to create more and more hierarchical models, ones where the final output is computed by

358
00:27:14,940 --> 00:27:18,620
going deeper and deeper into the network.

359
00:27:18,620 --> 00:27:24,060
And to implement this in TensorFlow again, it's very similar as we saw before, again

360
00:27:24,060 --> 00:27:30,540
using the TFKARIS sequential call, we can stack each of these dense layers on top of

361
00:27:30,540 --> 00:27:37,260
each other, each one specified by the number of neurons in that dense layer, n1 and 2,

362
00:27:37,260 --> 00:27:42,940
but with the last output layer fixed to 2 outputs, if that's how many outputs we have.

363
00:27:42,940 --> 00:27:45,620
Okay, so that's awesome.

364
00:27:45,620 --> 00:27:52,100
Now we have an idea of not only how to build up a neural network directly from a perceptron,

365
00:27:52,100 --> 00:27:56,660
but how to compose them together to form complex deep neural networks.

366
00:27:56,660 --> 00:28:02,180
Let's take a look at how we can actually apply them to a very real problem that I believe

367
00:28:02,180 --> 00:28:07,140
all of you should care very deeply about.

368
00:28:07,140 --> 00:28:12,060
Here's a problem that we want to build an AI system to learn to answer.

369
00:28:12,060 --> 00:28:14,300
Will I pass this class?

370
00:28:14,300 --> 00:28:17,860
And we can start with a simple two feature model.

371
00:28:17,860 --> 00:28:21,780
One feature, let's say, is the number of lectures that you attend as part of this class.

372
00:28:21,780 --> 00:28:27,060
And the second feature is the number of hours that you spend working on your final project.

373
00:28:27,060 --> 00:28:32,860
You do have some training data from all of the past participants of Success191.

374
00:28:32,860 --> 00:28:36,460
And we can plot this data on this feature space like this.

375
00:28:36,460 --> 00:28:41,980
The green points here actually indicate students, so each point is one student that has passed

376
00:28:41,980 --> 00:28:47,060
the class, and the red points are students that have failed the class.

377
00:28:47,060 --> 00:28:52,260
You can see where they are in this feature space depends on the actual number of hours

378
00:28:52,260 --> 00:28:55,700
that they attended the lecture, the number of lectures they attended, and the number

379
00:28:55,700 --> 00:28:59,260
of hours they spent on the final project.

380
00:28:59,260 --> 00:29:00,340
And then there's you.

381
00:29:00,340 --> 00:29:06,340
You have attended four lectures, and you have spent five hours on your final project.

382
00:29:06,340 --> 00:29:14,460
And you want to understand how can you build a neural network given everyone else in this

383
00:29:14,460 --> 00:29:15,740
class?

384
00:29:15,740 --> 00:29:22,140
Will you pass or fail this class based on the training data that you see?

385
00:29:22,140 --> 00:29:23,140
So let's do it.

386
00:29:23,140 --> 00:29:27,300
We have now all of the requirements to do this now.

387
00:29:27,300 --> 00:29:33,060
So let's build a neural network with two inputs, x1 and x2, with x1 being the number

388
00:29:33,060 --> 00:29:38,340
of lectures that we attend, x2 is the number of hours you spend on your final project.

389
00:29:38,340 --> 00:29:44,500
We'll have one hidden layer with three units, and we'll feed those into a final probability

390
00:29:44,500 --> 00:29:51,980
output by passing this class, and we can see that the probability that we pass is 0.1,

391
00:29:51,980 --> 00:29:53,740
or 10%.

392
00:29:53,740 --> 00:30:00,940
That's not great, but the reason is because that this model was never actually trained.

393
00:30:00,940 --> 00:30:04,420
It's basically just a baby.

394
00:30:04,420 --> 00:30:06,220
It's never seen any data.

395
00:30:06,220 --> 00:30:08,900
Even though you have seen the data, it hasn't seen any data.

396
00:30:08,900 --> 00:30:14,260
And more importantly, you haven't told the model how to interpret this data.

397
00:30:14,260 --> 00:30:16,420
It needs to learn about this problem first.

398
00:30:16,420 --> 00:30:22,060
It knows nothing about this class or final projects or any of that.

399
00:30:22,060 --> 00:30:26,140
So one of the most important things to do this is actually you have to tell the model

400
00:30:26,140 --> 00:30:32,700
when it is making bad predictions in order for it to be able to correct itself.

401
00:30:32,700 --> 00:30:35,820
Now the loss of a neural network actually defines exactly this.

402
00:30:35,820 --> 00:30:41,140
It defines how wrong a prediction was.

403
00:30:41,140 --> 00:30:45,580
So it takes as input the predicted outputs and the ground truth outputs.

404
00:30:45,580 --> 00:30:50,140
Now if those two things are very far apart from each other, then the loss will be very

405
00:30:50,140 --> 00:30:51,540
large.

406
00:30:51,540 --> 00:30:56,300
On the other hand, the closer these two things are from each other, the smaller the loss,

407
00:30:56,300 --> 00:30:59,500
and the more accurate the loss the model will be.

408
00:30:59,500 --> 00:31:01,580
So we always want to minimize the loss.

409
00:31:01,580 --> 00:31:06,940
We want to incur, we want to predict something that's as close as possible to the ground

410
00:31:06,940 --> 00:31:09,740
truth.

411
00:31:09,740 --> 00:31:14,900
Now let's assume we have not just the data from one student, but as we have in this

412
00:31:14,900 --> 00:31:17,260
case the data from many students.

413
00:31:17,260 --> 00:31:23,100
We now care about not just how the model did on predicting just one prediction, but how

414
00:31:23,100 --> 00:31:26,580
it did on average across all of these students.

415
00:31:26,580 --> 00:31:29,260
This is what we call the empirical loss.

416
00:31:29,260 --> 00:31:34,820
And it's simply just the mean or the average of every loss from each individual example

417
00:31:34,820 --> 00:31:38,460
or each individual student.

418
00:31:38,460 --> 00:31:43,340
When training a neural network, we want to find a network that minimizes the empirical

419
00:31:43,340 --> 00:31:50,580
loss between our predictions and the true outputs.

420
00:31:50,580 --> 00:31:56,660
Now if we look at the problem of binary classification, where the neural network like we want to do

421
00:31:56,660 --> 00:32:02,180
in this case, is supposed to answer either yes or no, one or zero.

422
00:32:02,180 --> 00:32:06,740
We can use what is called a softmax cross entropy loss.

423
00:32:06,740 --> 00:32:15,500
Now the softmax cross entropy loss is actually written out here and it's defined by what's

424
00:32:15,500 --> 00:32:18,980
called the cross entropy between two probability distributions.

425
00:32:18,980 --> 00:32:25,060
It measures how far apart the ground truth probability distribution is from the predicted

426
00:32:25,060 --> 00:32:28,980
probability distribution.

427
00:32:28,980 --> 00:32:33,900
Let's suppose instead of predicting binary outputs, will I pass this class or will I

428
00:32:33,900 --> 00:32:35,980
not pass this class?

429
00:32:35,980 --> 00:32:42,380
Instead you want to predict the final grade as a real number, not a probability or as

430
00:32:42,380 --> 00:32:49,700
a percentage, we want the grade that you will get in this class.

431
00:32:49,700 --> 00:32:54,540
Now in this case, because the type of the output is different, we also need to use a

432
00:32:54,540 --> 00:32:59,220
different loss here, because our outputs are no longer 0, 1, but they can be any real

433
00:32:59,220 --> 00:33:00,220
number.

434
00:33:00,220 --> 00:33:05,300
They're just the grade that you're going to get on the final class.

435
00:33:05,300 --> 00:33:11,100
So for example, here since this is a continuous variable, the grade, we want to use what's

436
00:33:11,100 --> 00:33:12,700
called the mean squared error.

437
00:33:12,700 --> 00:33:18,660
This measures just the squared error, the squared difference between our ground truth

438
00:33:18,660 --> 00:33:25,180
and our predictions, again averaged over the entire data set.

439
00:33:25,180 --> 00:33:31,340
Okay great, so now we've seen two loss functions, one for classification, binary outputs, as

440
00:33:31,340 --> 00:33:38,100
well as regression, continuous outputs, and the problem now I think that we need to start

441
00:33:38,100 --> 00:33:40,820
asking ourselves is how can we take that loss function?

442
00:33:40,820 --> 00:33:44,340
We've seen our loss function, we've seen our network, now we have to actually understand

443
00:33:44,340 --> 00:33:46,420
how can we put those two things together?

444
00:33:46,420 --> 00:33:51,100
How can we use our loss function to train the weights of our neural network such that

445
00:33:51,100 --> 00:33:53,700
it can actually learn that problem?

446
00:33:53,700 --> 00:33:58,980
Well, what we want to do is actually find the weights of the neural network that will

447
00:33:58,980 --> 00:34:01,580
minimize the loss of our data set.

448
00:34:01,580 --> 00:34:08,380
That essentially means that we want to find the W's in our neural network that minimize

449
00:34:08,380 --> 00:34:09,380
J of W.

450
00:34:09,380 --> 00:34:14,900
J of W's are empirical cost function that we saw in the previous slides that average loss

451
00:34:14,900 --> 00:34:18,660
over each data point in the data set.

452
00:34:18,660 --> 00:34:25,500
Now, remember that W, capital W, is simply a collection of all of the weights in our

453
00:34:25,500 --> 00:34:28,940
neural network, not just from one layer, but from every single layer.

454
00:34:28,940 --> 00:34:33,540
So that's W0 from the zeroth layer to the first layer to the second layer, all concatenate

455
00:34:33,540 --> 00:34:34,540
into one.

456
00:34:34,540 --> 00:34:41,220
In this optimization problem we want to optimize all of the W's to minimize this empirical loss.

457
00:34:42,220 --> 00:34:47,260
Now, remember our loss function is just a simple function of our weights.

458
00:34:47,260 --> 00:34:53,220
If we have only two weights, we can actually plot this entire loss landscape over this

459
00:34:53,220 --> 00:34:54,220
grid of weights.

460
00:34:54,220 --> 00:34:58,580
So on the one axis on the bottom you can see weight number one and the other one you can

461
00:34:58,580 --> 00:35:01,420
see weight zero.

462
00:35:01,420 --> 00:35:04,300
There's only two weights in this neural network, very simple neural network.

463
00:35:04,300 --> 00:35:10,180
So we can actually plot for every W0 and W1, what is the loss?

464
00:35:10,220 --> 00:35:16,940
What is the error that we'd expect to see and obtain from this neural network?

465
00:35:16,940 --> 00:35:22,620
Now the whole process of training a neural network, optimizing it, is to find the lowest

466
00:35:22,620 --> 00:35:29,580
point in this loss landscape that will tell us our optimal W0 and W1.

467
00:35:29,580 --> 00:35:30,740
Now how can we do that?

468
00:35:30,740 --> 00:35:34,540
The first thing we have to do is pick a point.

469
00:35:34,540 --> 00:35:37,900
So let's pick any W0, W1.

470
00:35:37,940 --> 00:35:44,340
Starting from this point we can compute the gradient of the landscape at that point.

471
00:35:44,340 --> 00:35:51,540
Now the gradient tells us the direction of highest or steepest ascent.

472
00:35:51,540 --> 00:35:53,780
So that tells us which way is up.

473
00:35:53,780 --> 00:35:58,220
If we compute the gradient of our loss with respect to our weights, that's the derivative

474
00:35:58,220 --> 00:36:02,140
of our gradient for loss with respect to the weights, that tells us the direction of which

475
00:36:02,140 --> 00:36:07,580
way is up on that loss landscape from where we stand right now.

476
00:36:07,580 --> 00:36:10,740
Instead of going up though, we want to find the lowest loss.

477
00:36:10,740 --> 00:36:18,220
So let's take the negative of our gradient and take a small step in that direction.

478
00:36:18,220 --> 00:36:21,660
This will move us a little bit closer to the lowest point.

479
00:36:21,660 --> 00:36:22,900
And we just keep repeating this.

480
00:36:22,900 --> 00:36:28,420
Now we compute the gradient at this point and repeat the process until we converge.

481
00:36:28,420 --> 00:36:31,460
And we will converge to a local minimum.

482
00:36:31,460 --> 00:36:35,700
We don't know if it will converge to a global minimum, but at least we know that it should

483
00:36:35,700 --> 00:36:38,500
in theory converge to a local minimum.

484
00:36:38,500 --> 00:36:42,460
Now we can summarize this algorithm as follows.

485
00:36:42,460 --> 00:36:45,820
This algorithm is also known as gradient descent.

486
00:36:45,820 --> 00:36:52,140
So we start by initializing all of our weights randomly and we loop until convergence.

487
00:36:52,140 --> 00:36:55,980
We start from one of those weights, our initial point.

488
00:36:55,980 --> 00:36:57,420
We compute the gradient.

489
00:36:57,420 --> 00:36:59,180
That tells us which way is up.

490
00:36:59,180 --> 00:37:01,300
So we take a step in the opposite direction.

491
00:37:01,300 --> 00:37:03,740
We take a small step here.

492
00:37:03,740 --> 00:37:10,020
All is computed by multiplying our gradient by this factor eta.

493
00:37:10,020 --> 00:37:12,100
And we'll learn more about this factor later.

494
00:37:12,100 --> 00:37:14,060
This factor is called the learning rate.

495
00:37:14,060 --> 00:37:15,620
We'll learn more about that later.

496
00:37:15,620 --> 00:37:21,580
Now again, in TensorFlow, we can actually see this pseudocode of gradient descent algorithm

497
00:37:21,580 --> 00:37:23,580
written out in code.

498
00:37:23,580 --> 00:37:26,620
We can randomize all of our weights.

499
00:37:26,620 --> 00:37:31,060
That basically initializes our search, our optimization process at some point in space.

500
00:37:31,460 --> 00:37:33,620
Then we keep looping over and over and over again.

501
00:37:33,620 --> 00:37:38,100
We compute the loss, we compute the gradient, and we take a small step of our weights in

502
00:37:38,100 --> 00:37:41,500
the direction of that gradient.

503
00:37:41,500 --> 00:37:44,260
But now let's take a look at this term here.

504
00:37:44,260 --> 00:37:48,260
This is how we actually compute the gradient.

505
00:37:48,260 --> 00:37:53,220
This explains how the loss is changing with respect to the weight.

506
00:37:53,220 --> 00:37:56,740
But I never actually told you how we compute this.

507
00:37:56,740 --> 00:38:02,100
So let's talk about this process, which is actually extremely important in training neural

508
00:38:02,100 --> 00:38:03,100
networks.

509
00:38:03,100 --> 00:38:06,900
It's known as back propagation.

510
00:38:06,900 --> 00:38:08,340
So how does back propagation work?

511
00:38:08,340 --> 00:38:10,420
How do we compute this gradient?

512
00:38:10,420 --> 00:38:12,380
Let's start with a very simple neural network.

513
00:38:12,380 --> 00:38:15,700
This is probably the simplest neural network in existence.

514
00:38:15,700 --> 00:38:21,380
It only has one input, one hidden neuron, and one output.

515
00:38:21,380 --> 00:38:26,900
Taking the gradient of our loss, j of w, with respect to one of the weights, in this

516
00:38:26,900 --> 00:38:36,660
case just w2, for example, tells us how much a small change in w2 is going to affect our

517
00:38:36,660 --> 00:38:38,180
loss, j.

518
00:38:38,180 --> 00:38:44,660
So if we move around j, infinitesimally small, how will that affect our loss?

519
00:38:44,660 --> 00:38:50,100
That's what the gradient is going to tell us, derivative of j of w2.

520
00:38:50,100 --> 00:38:55,860
So if we write out this derivative, we can actually apply the chain rule to actually

521
00:38:55,860 --> 00:38:57,020
compute it.

522
00:38:57,020 --> 00:38:58,700
So what does that look like?

523
00:38:58,700 --> 00:39:11,980
Specifically, we can decompose that derivative into the derivative of j dw over dy multiplied

524
00:39:11,980 --> 00:39:17,500
by derivative of our output with respect to w2.

525
00:39:17,500 --> 00:39:20,820
Now the question here is with the second part.

526
00:39:20,820 --> 00:39:26,380
If we want to compute now not the derivative of our loss with respect to w2, but now the

527
00:39:26,380 --> 00:39:30,580
loss with respect to w1, we can do the same story as before.

528
00:39:30,580 --> 00:39:35,260
We can apply the chain rule now recursively.

529
00:39:35,260 --> 00:39:39,540
So now we have to apply the chain rule again to the second part.

530
00:39:39,540 --> 00:39:42,780
Now the second part is expanded even further.

531
00:39:42,780 --> 00:39:48,620
So the derivative of our output with respect to z1, which is the activation function of

532
00:39:48,620 --> 00:39:50,660
this first hidden unit.

533
00:39:50,660 --> 00:39:52,700
And we can back propagate this information now.

534
00:39:52,700 --> 00:39:59,140
You can see starting from our loss all the way through w2 and then recursively applying

535
00:39:59,140 --> 00:40:02,580
this chain rule again to get to w1.

536
00:40:02,580 --> 00:40:08,340
And this allows us to see both the gradient at both w2 and w1.

537
00:40:08,340 --> 00:40:15,860
So in this case, just to reiterate once again, this is telling us this dj dw1 is telling

538
00:40:15,860 --> 00:40:21,120
us how a small change in our weight is going to affect our loss.

539
00:40:21,120 --> 00:40:25,420
So we can see if we increase our weight a small amount, it will increase our loss.

540
00:40:25,420 --> 00:40:29,380
That means we will want to decrease the weight to decrease our loss.

541
00:40:29,380 --> 00:40:30,820
That's what the gradient tells us.

542
00:40:30,820 --> 00:40:38,180
Which direction we need to step in order to decrease or increase our loss function.

543
00:40:38,180 --> 00:40:41,660
Now we showed this here for just two weights in our neural network because we only have

544
00:40:41,660 --> 00:40:42,660
two weights.

545
00:40:42,660 --> 00:40:45,580
But imagine we have a very deep neural network.

546
00:40:45,580 --> 00:40:51,380
One with more than just two layers of or one layer rather of hidden units.

547
00:40:51,380 --> 00:40:56,980
We can just repeat this process of applying, recursively applying the chain rule to determine

548
00:40:56,980 --> 00:41:02,540
how every single way in the model needs to change to impact that loss.

549
00:41:02,540 --> 00:41:07,420
But really all this boils down to just recursively applying this chain rule formulation that

550
00:41:07,420 --> 00:41:10,620
you can see here.

551
00:41:10,620 --> 00:41:12,860
And that's the back propagation algorithm.

552
00:41:12,860 --> 00:41:15,220
In theory it sounds very simple.

553
00:41:15,220 --> 00:41:22,500
It's just a very basic extension on derivatives and the chain rule.

554
00:41:22,500 --> 00:41:27,900
But now let's actually touch on some insights from training these networks in practice that

555
00:41:27,900 --> 00:41:32,340
make this process much more complicated in practice.

556
00:41:32,340 --> 00:41:37,540
And why using back propagation as we saw there is not always so easy.

557
00:41:37,540 --> 00:41:42,860
Now in practice training neural networks and optimization of networks can be extremely

558
00:41:42,860 --> 00:41:47,420
difficult and it's actually extremely computationally intensive.

559
00:41:47,420 --> 00:41:54,420
Here's the visualization of what a loss landscape of a real neural network can look like visualized

560
00:41:54,420 --> 00:41:56,900
on just two dimensions.

561
00:41:56,900 --> 00:42:03,380
Now you can see here that the loss is extremely non-convex meaning that it has many, many

562
00:42:03,380 --> 00:42:05,500
local minimum.

563
00:42:05,500 --> 00:42:11,020
That can make using an algorithm like gradient descent very, very challenging because gradient

564
00:42:11,020 --> 00:42:15,620
descent is always going to step closest to the first local minimum but it can always

565
00:42:15,620 --> 00:42:16,780
get stuck there.

566
00:42:16,780 --> 00:42:22,140
So finding how to get to the global minima or a really good solution for your neural

567
00:42:22,140 --> 00:42:28,860
network can often be very sensitive to your hyper parameter such as where the optimizer

568
00:42:28,860 --> 00:42:31,140
starts in this loss landscape.

569
00:42:31,140 --> 00:42:36,060
If it starts in a potentially bad part of the landscape it can very easily get stuck

570
00:42:36,060 --> 00:42:39,820
in one of these local minimum.

571
00:42:39,820 --> 00:42:44,580
Now recall the equation that we talked about for gradient descent.

572
00:42:44,580 --> 00:42:45,940
This was the equation I showed you.

573
00:42:45,940 --> 00:42:52,220
Our next weight update is going to be your current weights minus a small amount called

574
00:42:52,220 --> 00:42:54,500
the learning rate multiplied by the gradient.

575
00:42:54,500 --> 00:43:00,220
So we have this minus sign because we want to step in the opposite direction and we multiply

576
00:43:00,220 --> 00:43:04,860
it by the gradient or we multiply it by the small number called here called eta which

577
00:43:04,860 --> 00:43:08,380
is what we call the learning rate.

578
00:43:08,380 --> 00:43:11,180
How fast do we want to do the learning?

579
00:43:11,180 --> 00:43:15,140
Now it determines actually not just how fast to do the learning that's maybe not the best

580
00:43:15,180 --> 00:43:22,300
way to say it but it tells us how large should each step we take in practice be with regards

581
00:43:22,300 --> 00:43:23,620
to that gradient.

582
00:43:23,620 --> 00:43:27,940
So the gradient tells us the direction but it doesn't necessarily tell us the magnitude

583
00:43:27,940 --> 00:43:29,300
of the direction.

584
00:43:29,300 --> 00:43:36,180
So eta can tell us actually a scale of how much we want to trust that gradient and step

585
00:43:36,180 --> 00:43:38,300
in the direction of that gradient.

586
00:43:38,300 --> 00:43:43,540
In practice setting even eta, this one parameter, this one number can be extremely difficult

587
00:43:43,540 --> 00:43:47,180
and I want to give you a quick example of why.

588
00:43:47,180 --> 00:43:54,700
So if you have a very non-convex or loss landscape where you have local minima, if you set the

589
00:43:54,700 --> 00:43:59,580
learning rate too low then the model can get stuck in these local minima.

590
00:43:59,580 --> 00:44:04,620
It can never escape them because it actually does optimize itself but it optimizes it to

591
00:44:04,620 --> 00:44:12,300
a very non-optimal minima and it can converge very slowly as well.

592
00:44:12,300 --> 00:44:17,660
On the other hand if we increase our learning rate too much then we can actually overshoot

593
00:44:17,660 --> 00:44:25,580
our minima and actually diverge and lose control and basically explode the training process

594
00:44:25,580 --> 00:44:26,780
completely.

595
00:44:26,780 --> 00:44:33,580
One of the challenges is actually how to use stable learning rates that are large enough

596
00:44:33,580 --> 00:44:41,780
to avoid the local minima but small enough so that they don't diverge completely.

597
00:44:42,420 --> 00:44:49,940
So they're small enough to actually converge to that global spot once they reach it.

598
00:44:49,940 --> 00:44:51,940
So how can we actually set this learning rate?

599
00:44:51,940 --> 00:44:57,700
Well one option which is actually somewhat popular in practice is to actually just try

600
00:44:57,700 --> 00:45:01,620
a lot of different learning rates and that actually works.

601
00:45:01,620 --> 00:45:06,540
It is a feasible approach but let's see if we can do something a little bit smarter than

602
00:45:06,540 --> 00:45:08,300
that, more intelligent.

603
00:45:08,340 --> 00:45:14,020
What if we could say instead how can we build an adaptive learning rate that actually looks

604
00:45:14,020 --> 00:45:20,060
at its lost landscape and adapts itself to account for what it sees in the landscape.

605
00:45:20,060 --> 00:45:24,020
There are actually many types of optimizers that do exactly this.

606
00:45:24,020 --> 00:45:26,980
This means that the learning rates are no longer fixed.

607
00:45:26,980 --> 00:45:32,140
They can increase or decrease depending on how large the gradient is in that location

608
00:45:32,260 --> 00:45:39,260
and how fast we want and how fast we're actually learning and many other options.

609
00:45:39,260 --> 00:45:44,260
They could be also with regards to the size of the weights at that point, the magnitudes, etc.

610
00:45:46,260 --> 00:45:52,580
In fact these have been widely explored and published as part of TensorFlow as well and

611
00:45:52,580 --> 00:45:56,420
during your labs we encourage each of you to really try out each of these different types

612
00:45:56,420 --> 00:46:01,940
of optimizers and experiment with their performance in different types of problems so that you

613
00:46:01,940 --> 00:46:07,900
can gain very important intuition about when to use different types of optimizers or what

614
00:46:07,900 --> 00:46:12,900
their advantages are and disadvantages in certain applications as well.

615
00:46:14,900 --> 00:46:18,100
Let's try and put all of this together.

616
00:46:18,100 --> 00:46:25,500
Here we can see a full loop of using TensorFlow to define your model on the first line, define

617
00:46:25,500 --> 00:46:26,740
your optimizer.

618
00:46:26,740 --> 00:46:30,060
Here you can replace this with any optimizer that you want.

619
00:46:30,180 --> 00:46:34,940
Here I'm just using stochastic gradient descent like we saw before.

620
00:46:34,940 --> 00:46:37,260
Feeding it through the model we loop forever.

621
00:46:37,260 --> 00:46:39,340
We're doing this forward prediction.

622
00:46:39,340 --> 00:46:40,780
We predict using our model.

623
00:46:40,780 --> 00:46:43,460
We compute the loss with our prediction.

624
00:46:43,460 --> 00:46:49,100
This is exactly the loss is telling us again how incorrect our prediction is with respect

625
00:46:49,100 --> 00:46:51,700
to the ground truth why.

626
00:46:51,700 --> 00:46:58,500
We compute the gradient of our loss with respect to each of the weights in our neural network.

627
00:46:58,540 --> 00:47:06,020
Then finally we apply those gradients using our optimizer to step and update our weights.

628
00:47:06,020 --> 00:47:10,660
This is really taking everything that we've learned in the class and lecture so far and

629
00:47:10,660 --> 00:47:18,820
applying it into one whole piece of code written in TensorFlow.

630
00:47:18,820 --> 00:47:24,460
So I want to continue this talk and really talk about tips for training these networks

631
00:47:24,500 --> 00:47:31,380
in practice now that we can focus on this very powerful idea of batching your data into

632
00:47:31,380 --> 00:47:33,580
mini batches.

633
00:47:33,580 --> 00:47:39,340
So before we saw it with gradient descent that we have the following algorithm.

634
00:47:39,340 --> 00:47:45,100
This gradient that we saw to compute using back propagation can be actually very intensive

635
00:47:45,100 --> 00:47:50,540
to compute especially if it's computed over your entire training set.

636
00:47:50,620 --> 00:47:54,980
This is a summation over every single data point in the entire data set and most real

637
00:47:54,980 --> 00:47:56,820
life applications.

638
00:47:56,820 --> 00:48:01,660
It is simply not feasible to compute this on every single iteration in your optimization

639
00:48:01,660 --> 00:48:03,780
loop.

640
00:48:03,780 --> 00:48:08,780
Alternatively let's consider a different variant of this algorithm called stochastic gradient

641
00:48:08,780 --> 00:48:09,780
descent.

642
00:48:09,780 --> 00:48:14,380
So instead of computing the gradient over our entire data set let's just pick a single

643
00:48:14,380 --> 00:48:19,980
point compute the gradient of that single point with respect to the weights and then

644
00:48:19,980 --> 00:48:23,820
update all of our weights based on that gradient.

645
00:48:23,820 --> 00:48:28,660
So this has some advantages this is very easy to compute because it's only using one data

646
00:48:28,660 --> 00:48:34,940
point now it's very fast but it's also very noisy because it's only from one data point.

647
00:48:34,940 --> 00:48:40,820
Instead there's a middle ground instead of computing this noisy gradient of a single

648
00:48:40,820 --> 00:48:48,740
point let's get a better estimate of our gradient by using a batch of b data points.

649
00:48:48,740 --> 00:48:54,740
So now let's pick a batch of b data points and we'll compute the gradient estimate simply

650
00:48:54,740 --> 00:48:57,700
as the average over this batch.

651
00:48:57,700 --> 00:49:02,780
So since b here is usually not that large on the order of tens or hundreds of samples

652
00:49:02,780 --> 00:49:08,820
this is much much faster to compute than regular gradient descent and it's also much much more

653
00:49:08,820 --> 00:49:16,940
accurate than purely stochastic gradient descent that only uses a single example.

654
00:49:16,940 --> 00:49:22,380
Now this increases the gradient accuracy estimation which also allows us to converge much more

655
00:49:22,380 --> 00:49:27,660
smoothly it also means that we can trust our gradient more than in stochastic gradient

656
00:49:27,660 --> 00:49:34,940
descent so that we can actually increase our learning rate a bit more as well.

657
00:49:34,940 --> 00:49:40,940
Mini batching also leads to massively parallelizable computation we can split up the batches on

658
00:49:40,940 --> 00:49:46,380
separate workers and separate machines and thus achieve even more parallelization and

659
00:49:46,380 --> 00:49:50,140
speed increases on our GPUs.

660
00:49:50,140 --> 00:49:54,420
Now the last topic I want to talk about is that of overfitting this is also known as

661
00:49:54,420 --> 00:50:01,020
the problem of generalization and is one of the most fundamental problems in all of machine

662
00:50:01,020 --> 00:50:05,660
learning and not just deep learning.

663
00:50:05,660 --> 00:50:11,940
Now overfitting like I said is critical to understand so I really want to make sure that

664
00:50:11,940 --> 00:50:17,780
this is a clear concept in everyone's mind ideally in machine learning we want to learn

665
00:50:17,780 --> 00:50:23,420
a model that accurately describes our test data not the training data even though we're

666
00:50:23,420 --> 00:50:28,140
optimizing this model based on the training data what we really want is for it to perform

667
00:50:28,140 --> 00:50:31,460
well on the test data.

668
00:50:31,460 --> 00:50:38,140
So said differently we want to build representations that can learn from our training data but

669
00:50:38,140 --> 00:50:42,300
still generalize well to unseen test data.

670
00:50:42,300 --> 00:50:49,380
Now assume you want to build a line to describe these points underfitting means that the model

671
00:50:49,380 --> 00:50:56,140
does simply not have enough capacity to represent these points so no matter how good we try

672
00:50:56,140 --> 00:51:01,340
to fit this model it simply does not have the capacity to represent this type of data.

673
00:51:01,340 --> 00:51:05,420
On the far right hand side we can see the extreme other extreme where here the model

674
00:51:05,420 --> 00:51:12,620
is too complex it has too many parameters and it does not generalize well to new data.

675
00:51:12,620 --> 00:51:16,740
In the middle though we can see what's called the ideal fit it's not overfitting it's not

676
00:51:16,740 --> 00:51:23,100
underfitting but it has a medium number of parameters and it's able to fit in a generalizable

677
00:51:23,100 --> 00:51:29,300
way to the output and is able to generalize well to brand new data when it sees it at

678
00:51:29,300 --> 00:51:32,020
test time.

679
00:51:32,020 --> 00:51:37,780
Now to address this problem let's talk about regularization how can we make sure that our

680
00:51:37,780 --> 00:51:43,020
models do not end up overfit because neural networks do have a ton of parameters how can

681
00:51:43,020 --> 00:51:47,540
we enforce some form of regularization to them.

682
00:51:47,540 --> 00:51:49,540
Now what is regularization?

683
00:51:49,540 --> 00:51:53,580
Regularization is a technique that constrains our optimization problem such that we can

684
00:51:53,580 --> 00:51:59,820
discourage these complex models from actually being learned and overfit.

685
00:51:59,820 --> 00:52:01,420
So again why do we need it?

686
00:52:01,420 --> 00:52:06,620
We need it so that our model can generalize to this unseen data set and in neural networks

687
00:52:06,620 --> 00:52:13,020
we have many techniques for actually imposing regularization onto the model.

688
00:52:13,020 --> 00:52:17,100
One very common technique and very simple to understand is called dropout.

689
00:52:17,100 --> 00:52:22,620
This is one of the most popular forms of regularization in deep learning and it's very simple.

690
00:52:22,620 --> 00:52:25,660
Let's revisit this picture of a neural network.

691
00:52:25,660 --> 00:52:32,300
This is a two-layered neural network, two hidden layers and in dropout during training

692
00:52:32,300 --> 00:52:39,220
all we simply do is randomly set some of the activations here to zero with some probability.

693
00:52:39,220 --> 00:52:47,060
So what we can do is let's say we pick our probability to be 50% or 0.5 we can drop randomly

694
00:52:47,060 --> 00:52:51,900
for each of the activations 50% of those neurons.

695
00:52:51,900 --> 00:52:56,340
This is extremely powerful as it lowers the capacity of our neural network so that they

696
00:52:56,340 --> 00:53:02,580
have to learn to perform better on test sets because sometimes on training sets it just

697
00:53:02,580 --> 00:53:05,220
simply cannot rely on some of those parameters.

698
00:53:05,220 --> 00:53:09,900
So it has to be able to be resilient to that kind of dropout.

699
00:53:09,900 --> 00:53:17,100
It also means that they're easier to train because at least on every forward pass of

700
00:53:17,100 --> 00:53:22,100
iterations we're training only 50% of the weights and only 50% of the gradients.

701
00:53:22,100 --> 00:53:27,900
So that also cuts our gradient computation time down by a factor of two.

702
00:53:27,900 --> 00:53:33,740
So because now we only have to compute half the number of neuron gradients.

703
00:53:33,740 --> 00:53:38,220
Now on every iteration we dropped out on the previous iteration 50% of neurons but on the

704
00:53:38,220 --> 00:53:45,700
next iteration we're going to drop out a different set of neurons.

705
00:53:45,700 --> 00:53:50,500
And this gives the network, it basically forces the network to learn how to take different

706
00:53:50,500 --> 00:53:57,180
pathways to get to its answer and it can't rely on any one pathway too strongly and overfit

707
00:53:57,180 --> 00:53:58,180
to that pathway.

708
00:53:58,180 --> 00:54:02,760
This is a way to really force it to generalize to this new data.

709
00:54:02,760 --> 00:54:08,900
The second regularization technique that we'll talk about is this notion of early stopping.

710
00:54:08,900 --> 00:54:12,180
And again here the idea is very basic.

711
00:54:12,180 --> 00:54:18,100
It's basically let's stop training once we realize that our loss is increasing on a

712
00:54:18,100 --> 00:54:22,820
held out validation or let's call it a test set.

713
00:54:22,820 --> 00:54:27,980
So when we start training we all know the definition of overfitting is when our model

714
00:54:27,980 --> 00:54:30,340
starts to perform worse on the test set.

715
00:54:30,340 --> 00:54:36,380
So if we set aside some of this training data to be quote unquote test data we can monitor

716
00:54:36,380 --> 00:54:41,380
how our network is learning on this data and simply just stop before it has a chance

717
00:54:41,380 --> 00:54:42,900
to overfit.

718
00:54:42,900 --> 00:54:46,420
So on the x-axis you can see the number of training iterations and on the y-axis you

719
00:54:46,420 --> 00:54:51,740
can see the loss that we get after training that number of iterations.

720
00:54:51,740 --> 00:54:55,500
So as we continue to train in the beginning both lines continue to decrease.

721
00:54:55,500 --> 00:55:01,220
This is as we'd expect and this is excellent since it means our model is getting stronger.

722
00:55:01,220 --> 00:55:06,540
Eventually though the network's testing loss plateaus and starts to increase.

723
00:55:06,540 --> 00:55:11,260
Note that the training accuracy will always continue to go down as long as the network

724
00:55:11,260 --> 00:55:17,420
has the capacity to memorize the data and this pattern continues for the rest of training.

725
00:55:17,420 --> 00:55:21,020
So it's important here to actually focus on this point here.

726
00:55:21,020 --> 00:55:25,220
This is the point where we need to stop training and after this point assuming that our test

727
00:55:25,220 --> 00:55:30,940
set is a valid representation of the true test set the accuracy of the model will only

728
00:55:30,940 --> 00:55:31,940
get worse.

729
00:55:31,940 --> 00:55:36,180
So we can stop training here take this model and this should be the model that we actually

730
00:55:36,180 --> 00:55:40,500
use when we deploy into the real world.

731
00:55:40,500 --> 00:55:44,220
Anything any model taken from the left hand side is going to be underfit is not going

732
00:55:44,220 --> 00:55:48,660
to be utilizing the full capacity of the network and anything taken from the right hand side

733
00:55:48,660 --> 00:55:56,300
is overfit and actually performing worse than it needs to on that held out test set.

734
00:55:56,300 --> 00:56:02,420
So I'll conclude this lecture by summarizing three key points that we've covered so far.

735
00:56:02,420 --> 00:56:07,340
We started about the fundamental building blocks of neural networks the perceptron.

736
00:56:07,340 --> 00:56:12,780
We learned about stacking and composing these perceptrons together to form complex hierarchical

737
00:56:12,780 --> 00:56:19,100
neural networks and how to mathematically optimize these models with back propagation.

738
00:56:19,100 --> 00:56:23,940
And finally we address the practical side of these models that you'll find useful for

739
00:56:23,940 --> 00:56:29,660
the labs today including adaptive learning rates, batching and regularization.

740
00:56:29,660 --> 00:56:34,180
So thank you for attending the first lecture in 6S191.

741
00:56:34,180 --> 00:56:34,860
Thank you very much.

