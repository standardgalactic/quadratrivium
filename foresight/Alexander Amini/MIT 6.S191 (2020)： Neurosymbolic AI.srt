1
00:00:00,000 --> 00:00:10,280
Thanks so much for inviting me.

2
00:00:10,280 --> 00:00:15,440
This is a great, this is a great class and a great program and I'm really excited to

3
00:00:15,440 --> 00:00:20,000
see deep learning front and center as part of IAP.

4
00:00:20,000 --> 00:00:24,040
I understand you've covered kind of the basics of deep learning and I'm going to tell you

5
00:00:24,040 --> 00:00:30,080
today about something that's a little bit of a mashup on top of sort of standard deep

6
00:00:30,080 --> 00:00:33,080
learning kind of going beyond deep learning.

7
00:00:33,080 --> 00:00:36,960
But before I start, I just want to say something about this word artificial intelligence because

8
00:00:36,960 --> 00:00:39,480
you know I had artificial intelligence in the last slide.

9
00:00:39,480 --> 00:00:42,320
If you look at my business card, it actually says artificial intelligence in two separate

10
00:00:42,320 --> 00:00:45,520
places on that card and I'll have a confession.

11
00:00:45,520 --> 00:00:50,480
I'm a recovering academic so I was a professor at Harvard for ten years.

12
00:00:50,480 --> 00:00:52,560
I just joined IBM about two years ago.

13
00:00:52,560 --> 00:00:53,760
This is my first real job.

14
00:00:53,760 --> 00:00:58,280
My mom's very proud of me that I finally got out of school.

15
00:00:58,280 --> 00:01:04,320
But I will say as an academic researcher working on AI, we hated this term.

16
00:01:04,320 --> 00:01:09,360
2017 and before, we would do anything we could not to say these words.

17
00:01:09,360 --> 00:01:13,440
We'd say machine learning or we'd say deep learning, be more specific.

18
00:01:13,440 --> 00:01:17,520
But 2018 and beyond, for whatever reason, we've all given up and we're all calling it

19
00:01:17,520 --> 00:01:18,520
AI.

20
00:01:18,520 --> 00:01:22,840
We're calling it AI, Google's calling it AI, academics are calling it AI.

21
00:01:22,840 --> 00:01:27,400
But when I got to IBM, they had done something that I really appreciated and it helps kind

22
00:01:27,400 --> 00:01:28,400
of frame the discussion.

23
00:01:28,400 --> 00:01:31,320
It will frame the discussion about what I'm going to tell you about research-wise in a

24
00:01:31,320 --> 00:01:32,320
minute.

25
00:01:32,320 --> 00:01:33,320
And that's just to do something very simple.

26
00:01:33,320 --> 00:01:36,760
This is part of something that IBM does called the global technology outlook, which is like

27
00:01:36,760 --> 00:01:41,440
an annual process where we envision for the company, for the corporation what the future

28
00:01:41,440 --> 00:01:43,480
lies holds ahead.

29
00:01:43,480 --> 00:01:47,160
And they did something very simple just to put adjectives in front of AI, just to distinguish

30
00:01:47,160 --> 00:01:49,400
what we're talking about when we're talking about different things.

31
00:01:49,400 --> 00:01:54,040
So this will be relevant to where we want to push relative to where we are today with

32
00:01:54,040 --> 00:01:56,400
deep learning to where we want to go.

33
00:01:56,400 --> 00:01:59,960
And that's to distinguish what we have today as narrow AI.

34
00:01:59,960 --> 00:02:04,600
So it's not to say it's not powerful or disruptive, but just to say that it's limited in important

35
00:02:04,600 --> 00:02:05,800
ways.

36
00:02:05,800 --> 00:02:09,720
And also to kind of distinguish it from general AI, which is the stuff that the public and

37
00:02:09,720 --> 00:02:11,400
the press likes to talk about sometimes.

38
00:02:11,400 --> 00:02:15,720
When IBM Research, which if you don't know, we're a 4,300-person global research organization,

39
00:02:15,720 --> 00:02:21,200
we have six Nobel Prizes, we've been around for 75 years, when IBM Research tried to decide

40
00:02:21,200 --> 00:02:25,080
when this was going to happen, when general AI was going to happen, we said 2050 and beyond.

41
00:02:25,080 --> 00:02:28,840
And basically when you ask scientists something and they tell you 2050 and beyond when it's

42
00:02:28,840 --> 00:02:32,400
coming, that means we have no idea, but it's no time soon.

43
00:02:32,400 --> 00:02:34,120
But in the middle is this notion of broad AI.

44
00:02:34,120 --> 00:02:39,760
And that's really what we're here today about and what the lab I run is about.

45
00:02:39,760 --> 00:02:45,920
And just to unpack this one level deeper, on one hand, we have general AI.

46
00:02:45,920 --> 00:02:51,880
This is this idea of broadly autonomous systems that can decide what they do on their own.

47
00:02:51,880 --> 00:02:55,840
This is the kind of thing that Elon Musk described as summoning the demon.

48
00:02:55,840 --> 00:02:57,360
So congratulations, everyone.

49
00:02:57,360 --> 00:03:02,160
You're helping to summon the demon, according to Elon Musk, or slightly more level-headed

50
00:03:02,160 --> 00:03:06,280
people like Stephen Hawking, warning that artificial intelligence could end mankind.

51
00:03:06,320 --> 00:03:09,840
This is kind of, you know, maybe we need to worry about this in the future, but actually

52
00:03:09,840 --> 00:03:13,360
what I'll argue in just a minute is that we're actually in quite a bit more limited space

53
00:03:13,360 --> 00:03:14,360
right now.

54
00:03:14,360 --> 00:03:19,120
And really it's this broad AI that we really need to focus on.

55
00:03:19,120 --> 00:03:22,680
So how do we build systems that are multitasking, multi-domain, that can take knowledge from

56
00:03:22,680 --> 00:03:27,000
one place, supply it in another, that can incorporate lots of different kinds of data,

57
00:03:27,000 --> 00:03:31,800
not just images or video, but images, video, text, structure data, unstructured data, it's

58
00:03:31,800 --> 00:03:35,480
distributed, it runs in the cloud, it also runs in edge devices, and it's explainable.

59
00:03:35,480 --> 00:03:37,440
So we can understand what these systems do.

60
00:03:37,440 --> 00:03:41,320
And this is basically then the roadmap for everything that my lab does.

61
00:03:41,320 --> 00:03:46,640
So we're asking, what are the barriers we need to break down to bring in this era where

62
00:03:46,640 --> 00:03:49,720
we can apply AI to all the different kinds of problems that we need to apply to?

63
00:03:49,720 --> 00:03:51,920
So things like explainability.

64
00:03:51,920 --> 00:03:55,360
We need to have systems that aren't just black boxes, but we can look inside and understand

65
00:03:55,360 --> 00:03:57,640
why they make decisions, when they make a right decision.

66
00:03:57,640 --> 00:04:00,880
We know why it made that decision, when they make a wrong decision.

67
00:04:00,880 --> 00:04:05,000
We have the ability to reach in and figure out how we would debug that system.

68
00:04:05,000 --> 00:04:10,400
One interesting thing about the AI revolution, back in the day, people said that software

69
00:04:10,400 --> 00:04:12,200
was going to eat the world.

70
00:04:12,200 --> 00:04:18,560
And these days, Jensen Wang, the CEO of NVIDIA, is on record saying that AI is going to eat

71
00:04:18,560 --> 00:04:19,560
software.

72
00:04:19,560 --> 00:04:21,040
And I think increasingly that's true.

73
00:04:21,040 --> 00:04:26,720
We're going to have data-driven software systems that are based on technology like deep learning.

74
00:04:26,720 --> 00:04:29,040
But the terrifying thing about that is we don't really yet have debuggers.

75
00:04:29,040 --> 00:04:32,640
And it's very hard in many cases to figure out why systems aren't working.

76
00:04:32,640 --> 00:04:36,320
So this is something that's really holding back AI today.

77
00:04:36,320 --> 00:04:39,720
Security, I'll tell you a little bit about the kind of weird world of security we live

78
00:04:39,720 --> 00:04:43,120
in now with AI, where AI systems can be hacked in interesting ways.

79
00:04:43,120 --> 00:04:47,480
We can close those gaps to be able to really realize the full potential of AI.

80
00:04:47,480 --> 00:04:50,440
Systems need to be fair and unbiased.

81
00:04:50,440 --> 00:04:53,720
That's both the thing that's good for the world, but it's also the case that in many

82
00:04:53,720 --> 00:04:59,160
regulated industries, like the kinds of companies that IBM serves, like banks, they're regulated

83
00:04:59,160 --> 00:05:02,440
such that the government insists that their systems be provably fair.

84
00:05:02,440 --> 00:05:06,400
We need to be able to look inside, see, and understand that the decisions the system will

85
00:05:06,400 --> 00:05:07,920
make will be fair and unbiased.

86
00:05:07,920 --> 00:05:13,400
And then on a practical level, I think the real battleground going forward for deep learning

87
00:05:13,400 --> 00:05:18,600
and for AI in general, as much as people talk about big data, actually the most interesting

88
00:05:18,600 --> 00:05:23,400
battlegrounds that we see across many different industries all have to do with small data.

89
00:05:23,400 --> 00:05:27,040
So how do we work with very small amounts of data?

90
00:05:27,040 --> 00:05:31,320
It turns out if you look across all the businesses that make the world run, heavy industries,

91
00:05:31,320 --> 00:05:37,360
healthcare, financial services, most of the problems that those companies faced and that

92
00:05:37,360 --> 00:05:42,520
we face in the world in general don't have enormous, annotated, carefully curated data

93
00:05:42,520 --> 00:05:43,960
sets to go with them.

94
00:05:43,960 --> 00:05:49,280
So if we're going to be able to use AI broadly and tackle all of these hard problems that

95
00:05:49,280 --> 00:05:53,160
we want to solve, we need to be able to learn how to do more with less data.

96
00:05:53,160 --> 00:05:57,320
So part of that has to do with things like transfer learning, learning to transfer from

97
00:05:57,320 --> 00:06:01,960
one domain to another, so learning one domain and then use that knowledge somewhere else.

98
00:06:01,960 --> 00:06:05,320
But increasingly, and this is what I'm going to tell you about today, there's this notion

99
00:06:05,320 --> 00:06:06,640
of reasoning.

100
00:06:06,640 --> 00:06:10,880
So how do we not only extract the structure of the data we're looking at, the data domain

101
00:06:10,880 --> 00:06:16,360
we're interested in, but then also be able to logically and fluidly reason about that

102
00:06:16,360 --> 00:06:17,860
data?

103
00:06:17,860 --> 00:06:20,560
And then finally, just to close it out, just to give you a little bit of a pitch about

104
00:06:20,560 --> 00:06:24,600
what the lab is and what we do, there's also a piece about infrastructure that we think

105
00:06:24,600 --> 00:06:25,760
is really important.

106
00:06:25,760 --> 00:06:32,200
So if you track energy usage from computing year over year, by some estimates by the year

107
00:06:32,200 --> 00:06:37,640
2040, if we keep increasing our energy usage due to computing, we'll exceed the power budget

108
00:06:37,640 --> 00:06:39,360
of the planet Earth.

109
00:06:39,360 --> 00:06:42,760
There won't be enough solar radiation from the sun, not enough stuff we can dig up out

110
00:06:42,760 --> 00:06:46,120
of the Earth and burn to fuel our computing habit.

111
00:06:46,120 --> 00:06:51,960
And AI isn't helping, deep learning is not helping, so many models that we train will

112
00:06:51,960 --> 00:06:58,120
take the equivalent energy of running a whole city for several days, just for one model.

113
00:06:58,120 --> 00:07:01,040
And that's obviously not going to last for a long time.

114
00:07:01,040 --> 00:07:06,320
So we also do work both at the algorithmic level, some of which I'll tell you about today,

115
00:07:06,320 --> 00:07:10,320
but also at the physics level to ask, can we build different kinds of computers?

116
00:07:10,320 --> 00:07:15,360
So this is a diagram of a memristive device, this is an analog computer, which we think

117
00:07:15,360 --> 00:07:19,720
we can get power consumption for deep learning workloads down by maybe a factor of 100 or

118
00:07:19,720 --> 00:07:20,720
even a thousand.

119
00:07:21,360 --> 00:07:22,920
And we're also working in quantum computing.

120
00:07:22,920 --> 00:07:25,320
IBM, as you may know, is one of the leaders in quantum.

121
00:07:25,320 --> 00:07:28,640
We have some of the biggest quantum computers that are available today, and we're asking

122
00:07:28,640 --> 00:07:31,440
how that all interacts with AI.

123
00:07:31,440 --> 00:07:37,160
So when IBM, when we set out to do this challenge of how do we make AI broadly applicable to

124
00:07:37,160 --> 00:07:41,960
all the kinds of problems that we'd like to apply AI to, just as a small plug for the

125
00:07:41,960 --> 00:07:46,080
lab since we're here, we decided we didn't want to do it alone, and we chose a partner.

126
00:07:46,080 --> 00:07:48,640
And in particular, we chose MIT.

127
00:07:48,640 --> 00:07:54,560
And actually, the idea being that this is one of the last standing industrial research

128
00:07:54,560 --> 00:07:59,160
labs of the Bell Lab era, IBM Research, together with MIT, which obviously needs no introduction

129
00:07:59,160 --> 00:08:01,920
because we're here right now, and we're partnering around AI.

130
00:08:01,920 --> 00:08:05,600
And just to give you a little bit of historical context, it actually turns out that IBM and

131
00:08:05,600 --> 00:08:09,960
MIT have been together since the beginning of AI, literally since the term artificial

132
00:08:09,960 --> 00:08:16,200
intelligence was coined, way back in 1956, so right when the very first computers were

133
00:08:16,200 --> 00:08:17,200
being developed.

134
00:08:17,200 --> 00:08:23,120
Nathaniel Rochester, who's the gentleman right there, who developed the IBM 701, which is

135
00:08:23,120 --> 00:08:29,080
one of the first practical computers, got together with MIT professors like John McCarthy,

136
00:08:29,080 --> 00:08:31,760
and dreamed up this future of AI.

137
00:08:31,760 --> 00:08:32,760
And it's actually really fascinating.

138
00:08:32,760 --> 00:08:38,640
I encourage you all to go and find the proposal for this workshop, because a lot of the language,

139
00:08:38,640 --> 00:08:43,400
including neural network language, is all here, like they got a lot of the words right.

140
00:08:43,400 --> 00:08:49,800
They were just a little bit off on the time scale, maybe like seven decades off.

141
00:08:49,800 --> 00:08:51,280
But really interesting.

142
00:08:51,280 --> 00:08:55,600
And the partnership here, the idea here is that we're combining the long horizon, time

143
00:08:55,600 --> 00:09:02,360
horizon that MIT brings to the creation of knowledge, maybe 100-year time horizons, departments

144
00:09:02,360 --> 00:09:05,960
of everything in chemistry, biology, economics, and physics, together with IBM, where we have

145
00:09:05,960 --> 00:09:10,080
a lot of those same departments because we're such a big research organization.

146
00:09:10,080 --> 00:09:13,800
But to bring those together with industry problems, to bring data to the table, so we

147
00:09:13,800 --> 00:09:17,680
can do the kind of research we want to do, and to bring the compute resources along as

148
00:09:17,680 --> 00:09:18,680
well.

149
00:09:18,680 --> 00:09:21,520
So this is what the lab is, and what we do.

150
00:09:21,520 --> 00:09:26,480
We were founded with a quarter-billion-dollar investment over 10 years from IBM.

151
00:09:26,480 --> 00:09:32,960
And we have 50 projects currently, more than 50 projects currently running, over 150 researchers

152
00:09:32,960 --> 00:09:38,600
across MIT and IBM, and their opportunities for undergraduates, for graduate students

153
00:09:38,800 --> 00:09:40,200
to be involved in these projects.

154
00:09:40,200 --> 00:09:45,080
So if you're interested in the things I show you today, we'd love to have you join our

155
00:09:45,080 --> 00:09:48,520
team, either on the MIT side or on the IBM side.

156
00:09:48,520 --> 00:09:52,440
And we're basically drawing from all of the different departments of MIT, and even though

157
00:09:52,440 --> 00:09:56,840
we've only been running for about a year and a half, we have over 100 publications in

158
00:09:56,840 --> 00:09:58,760
top academic conferences and journals.

159
00:09:58,760 --> 00:10:04,560
We had 17 papers in NeurIPS just a few months ago, just to give you a sense of that everything's

160
00:10:04,560 --> 00:10:06,280
up and running.

161
00:10:06,280 --> 00:10:11,200
So this is the evolution, this is where we're going.

162
00:10:11,200 --> 00:10:14,880
So why do I say that today's AI is narrow?

163
00:10:14,880 --> 00:10:15,880
Why would I say that?

164
00:10:15,880 --> 00:10:26,880
Because clearly, AI is powerful, in particular, in 2015 Forbes said that deep learning and

165
00:10:26,880 --> 00:10:30,080
machine intelligence would eat the world.

166
00:10:30,080 --> 00:10:37,720
And I think it's safe to say that progress since 2012 or so has been incredibly rapid.

167
00:10:37,720 --> 00:10:44,240
So this was a paper that really, for me, as a researcher who was working in computer vision,

168
00:10:44,240 --> 00:10:47,560
really convinced me that something dramatic was happening.

169
00:10:47,560 --> 00:10:53,040
So this was a paper from Andre Carpathi, who now leads Tesla's AI program, together with

170
00:10:53,040 --> 00:10:55,760
Fei-Fei Li, who created the ImageNet dataset.

171
00:10:55,760 --> 00:10:59,800
And they built a system which you probably have studied a little bit in this course,

172
00:10:59,800 --> 00:11:04,640
where they can take an image and produce a beautiful natural language caption.

173
00:11:04,640 --> 00:11:09,200
So it takes an input like this, and it produces a caption like, a man in a black shirt is

174
00:11:09,200 --> 00:11:10,800
playing a guitar.

175
00:11:10,800 --> 00:11:15,760
Or you take in this image and you get a construction worker in an orange safety vest is working

176
00:11:15,760 --> 00:11:17,720
on the road.

177
00:11:17,720 --> 00:11:22,720
When I started studying computer vision and AI and machine learning, I wasn't sure we

178
00:11:22,720 --> 00:11:28,280
were going to actually achieve this even in my career or perhaps even in my lifetime.

179
00:11:28,280 --> 00:11:31,640
It seems like such science fiction, it's so commonplace now that we have systems that

180
00:11:31,640 --> 00:11:32,720
can do that.

181
00:11:32,720 --> 00:11:40,960
So it's hard to overstate how important deep learning has been in the progress of AI and

182
00:11:40,960 --> 00:11:41,960
machine learning.

183
00:11:41,960 --> 00:11:47,600
You know, meanwhile, there are very few games left that humans are better than machines

184
00:11:47,600 --> 00:11:48,600
at.

185
00:11:48,600 --> 00:11:55,800
Everything from Jeopardy, which IBM did way back in 2011, to Go with AlphaGo from DeepMind.

186
00:11:55,800 --> 00:12:01,560
I think Carnegie Mellon created a system that could beat the world champion in poker.

187
00:12:01,560 --> 00:12:05,560
And recently, my own company created a system called Project Debater that can actually carry

188
00:12:05,560 --> 00:12:10,320
on a pretty credible natural language debate with a debate champion.

189
00:12:10,320 --> 00:12:15,720
So if you like your computers to argue with you, we can do that for you now.

190
00:12:15,720 --> 00:12:20,440
And even domains like art, which we would have thought maybe would have been privileged

191
00:12:20,440 --> 00:12:25,360
domains for humanity, like surely machines can't create art, right?

192
00:12:26,000 --> 00:12:27,000
That's not the case.

193
00:12:27,000 --> 00:12:32,120
So even way back in 2015, which now feels like a long time ago, Matias Bekka's group

194
00:12:32,120 --> 00:12:36,840
at the Max Planck in Tubingen created the system with Style Transfer, where you could

195
00:12:36,840 --> 00:12:41,880
go from a photograph of your own and then re-render it in the style of any artist you

196
00:12:41,880 --> 00:12:42,880
like.

197
00:12:42,880 --> 00:12:47,280
So this is a very simple Style Transfer model that leveraged the internal representation

198
00:12:47,280 --> 00:12:52,880
of a convolutional neural network up to what we have today, which is, again, just astonishing

199
00:12:52,880 --> 00:12:54,960
how fast progress is moving.

200
00:12:55,560 --> 00:13:00,240
These are the outputs from a system called BigGAN, which came from DeepMind.

201
00:13:00,240 --> 00:13:05,040
And all four of these images are all of things that don't exist in the real world.

202
00:13:05,040 --> 00:13:07,280
So this dog, not a real dog.

203
00:13:07,280 --> 00:13:13,800
You put it in a random vector into the bigGAN, and it generates full cloth, this beautiful

204
00:13:13,800 --> 00:13:17,440
high-resolution dog, or this bubble, or this cup.

205
00:13:17,440 --> 00:13:20,640
And this is actually going to be a problem now, because now we have this notion of deepfakes.

206
00:13:20,640 --> 00:13:25,920
We're getting so good at creating fake images that now we're having to come up with actual

207
00:13:25,920 --> 00:13:26,920
countermeasures.

208
00:13:26,920 --> 00:13:31,880
And that's actually one thing we're working on in the laboratory I run at IBM, where we're

209
00:13:31,880 --> 00:13:37,280
trying to find ganttodotes, like, antidotes, countermeasures against GANs as we move forward.

210
00:13:37,280 --> 00:13:41,760
So clearly, the progress is impressive.

211
00:13:41,760 --> 00:13:46,320
So why am I saying that AI is still narrow today?

212
00:13:47,280 --> 00:13:50,360
Well, does anyone know what this is?

213
00:13:50,360 --> 00:13:52,440
Anyone have any ideas?

214
00:13:52,440 --> 00:13:53,440
Yeah.

215
00:13:53,440 --> 00:13:56,440
Good job, but you're wrong.

216
00:13:56,440 --> 00:13:58,560
It turns out it's a teddy bear.

217
00:13:58,560 --> 00:14:05,600
So if you ask a state-of-the-art ImageNet-trained CNN, and you often see these CNNs described

218
00:14:05,600 --> 00:14:08,880
as being superhuman in their accuracy, has anyone heard that before?

219
00:14:08,880 --> 00:14:12,720
Like, they'll say, object recognition is a solved problem.

220
00:14:12,720 --> 00:14:15,200
These ImageNet-trained CNNs can do better than humans.

221
00:14:16,080 --> 00:14:19,720
And if you've ever actually looked at the ImageNet carefully, the reason that's true

222
00:14:19,720 --> 00:14:24,800
is because ImageNet has huge numbers of categories of dogs, so you basically need to be a dog

223
00:14:24,800 --> 00:14:28,480
show judge to be able to outperform a human at ImageNet.

224
00:14:28,480 --> 00:14:32,160
But this is starting to illustrate a problem.

225
00:14:32,160 --> 00:14:33,160
This image is real.

226
00:14:33,160 --> 00:14:38,600
So this is a piece of art in the Museum of Modern Art in New York by Merit Oppenheim,

227
00:14:38,600 --> 00:14:46,800
called Le Dijonnet en Fereur, a luncheon in Fereur, a little bit unsettling image.

228
00:14:46,800 --> 00:14:50,360
But we, like, who thought it was a teddy bear?

229
00:14:50,360 --> 00:14:51,360
Right?

230
00:14:51,360 --> 00:14:58,680
Like, the most unteddy bear-like image ever, right?

231
00:14:58,680 --> 00:15:02,360
Why did the CNN think this was a teddy bear?

232
00:15:02,360 --> 00:15:04,840
Soft and fluffy.

233
00:15:04,840 --> 00:15:05,840
Soft and fluffy.

234
00:15:05,840 --> 00:15:06,840
It's round.

235
00:15:06,840 --> 00:15:07,840
It's got fur.

236
00:15:07,840 --> 00:15:11,120
So one of the things in the training set would be round and furry teddy bears.

237
00:15:11,120 --> 00:15:14,040
You know, it's a little bit of a garbage-in-garbage-out kind of scenario.

238
00:15:14,040 --> 00:15:18,800
This is, in many ways, you know, people talk about corner cases or edge cases, those rare

239
00:15:18,800 --> 00:15:24,560
things that happen, but are different from the distribution you've trained on previously.

240
00:15:24,560 --> 00:15:27,560
And this is a great example of such a thing.

241
00:15:27,560 --> 00:15:31,400
So this is starting to show that even though deep learning systems we have today are amazing,

242
00:15:31,400 --> 00:15:35,720
and they are amazing, there's, you know, there's room for improvement, there's something missing

243
00:15:35,720 --> 00:15:36,720
here.

244
00:15:36,880 --> 00:15:41,000
Actually, if we dig a little bit deeper, which, you know, a variety of researchers have done,

245
00:15:41,000 --> 00:15:48,120
so this is from Alan Yule's group at Johns Hopkins, even in cases where, you know, the

246
00:15:48,120 --> 00:15:52,440
objects are the standard objects that the system knows how to detect its supposedly

247
00:15:52,440 --> 00:15:57,160
superhuman, you know, sort of levels, if we take this guitar and we put it on top of

248
00:15:57,160 --> 00:16:01,320
this monkey in the jungle, a couple of funny things happen.

249
00:16:01,320 --> 00:16:03,720
One is it thinks the guitar is a bird.

250
00:16:04,320 --> 00:16:06,240
Anyone have an idea why that is?

251
00:16:06,240 --> 00:16:09,080
I hear pieces of the answer all around.

252
00:16:09,080 --> 00:16:11,040
So it's colorful, right?

253
00:16:11,040 --> 00:16:13,080
It's a color that you would expect a tropical bird.

254
00:16:13,080 --> 00:16:17,720
Things that are in the jungle in distribution would tend to be colorful tropical birds.

255
00:16:17,720 --> 00:16:20,960
Interestingly, because you put the guitar in front of the monkey, now the monkey's a

256
00:16:20,960 --> 00:16:21,960
person.

257
00:16:21,960 --> 00:16:28,120
And, you know, again, you know, monkeys don't play guitars in the training set, and that's

258
00:16:28,120 --> 00:16:29,440
clearly messing with the results.

259
00:16:29,440 --> 00:16:36,920
So even though we have no trouble at all telling that these objects are a guitar and a monkey,

260
00:16:36,920 --> 00:16:40,080
the state-of-the-art systems are falling down.

261
00:16:40,080 --> 00:16:45,840
And then even this captioning example, which I highlighted as being, you know, an amazing

262
00:16:45,840 --> 00:16:50,120
success for deep learning, and it is an amazing success for deep learning, when you poke a

263
00:16:50,120 --> 00:16:55,560
little bit harder, which, you know, Josh Tenenbaum and Sam Gershman and Brendan Lake and Tomer

264
00:16:55,560 --> 00:16:57,960
Oman did, you find things like this.

265
00:16:57,960 --> 00:17:04,840
So this image is captioned as a man riding a motorcycle on a beach.

266
00:17:04,840 --> 00:17:11,160
This next one is an airplane is parked on the tarmac at an airport.

267
00:17:11,160 --> 00:17:17,320
And this one, next one, is a group of people standing on top of a beach, which is correct.

268
00:17:17,320 --> 00:17:21,200
So score one for the AI.

269
00:17:21,200 --> 00:17:27,360
But what you can see is there's a strong sense in which the system doesn't really understand

270
00:17:27,360 --> 00:17:32,760
what it's looking at, and that leads to mistakes, and that leads to sort of, you know, missing

271
00:17:32,760 --> 00:17:35,760
the point, you know, in many cases.

272
00:17:35,760 --> 00:17:41,560
And again, this has to do with the fact that the systems are trained on the data, and largely

273
00:17:41,560 --> 00:17:44,680
they're constrained by what data they've seen before, and things that are out of sample,

274
00:17:44,680 --> 00:17:49,320
these edge cases, these corner cases, tend to perform poorly.

275
00:17:49,320 --> 00:17:54,120
Now the success of deep learning, you know, I think it's safe to say it's, you know, two

276
00:17:54,120 --> 00:17:55,120
things happened.

277
00:17:55,120 --> 00:18:00,360
So deep learning, as you already know, is a rebrand of a technology called artificial

278
00:18:00,360 --> 00:18:01,360
neural networks.

279
00:18:01,360 --> 00:18:05,600
It dates all the way back to that Dartmouth conference, at least to the 80s.

280
00:18:05,600 --> 00:18:11,360
You know, a lot of the fundamental map of backprop was worked out in the 80s.

281
00:18:11,360 --> 00:18:16,200
We went through decades of time where it was disreputable to study neural networks, and

282
00:18:16,200 --> 00:18:19,920
I lived through that era where you would try and publish a paper about neural networks,

283
00:18:19,920 --> 00:18:23,080
and people would tell you that everyone knows that neural networks don't work.

284
00:18:23,080 --> 00:18:29,320
So what happened was the amount of data that was available grew enormously, so we digitalized

285
00:18:29,320 --> 00:18:30,320
the world.

286
00:18:30,320 --> 00:18:31,320
We got digital cameras.

287
00:18:31,320 --> 00:18:34,800
Now we're all carrying, I'm carrying like four cameras on me right now.

288
00:18:34,800 --> 00:18:36,240
And we took a lot of images.

289
00:18:36,240 --> 00:18:40,040
And then the compute caught up as well, and particularly graphics processing units, graphics

290
00:18:40,040 --> 00:18:43,360
processing units, GPUs became available, and it turned out they were even better for doing

291
00:18:43,360 --> 00:18:45,720
deep learning than they were for doing graphics.

292
00:18:45,720 --> 00:18:50,200
And really the seminal moment that really flipped the switch and made deep learning take off

293
00:18:50,200 --> 00:18:54,760
was the collection of this data set called ImageNet, which Fei-Fei Li collected, and

294
00:18:54,760 --> 00:19:01,840
it's basically millions of carefully curated images with categories associated with them.

295
00:19:01,840 --> 00:19:06,560
Now you need to have data sets of this scale to make deep learning work.

296
00:19:06,560 --> 00:19:10,880
So if you're working on projects now and you're training neural networks, you'll know that

297
00:19:10,880 --> 00:19:15,800
you need to have thousands to millions of images to be able to train a network and have

298
00:19:15,800 --> 00:19:17,800
it perform well.

299
00:19:17,800 --> 00:19:20,200
That's in stark contrast to how we work.

300
00:19:20,200 --> 00:19:22,720
So does anyone know what this object is?

301
00:19:22,720 --> 00:19:24,560
Just a quick raise of your hands.

302
00:19:24,560 --> 00:19:27,560
Okay, a few people, not so many.

303
00:19:27,560 --> 00:19:31,200
But even though you've never seen this object before, a single training example, you're

304
00:19:31,200 --> 00:19:33,440
now all experts in this object.

305
00:19:33,440 --> 00:19:34,880
Just one training example.

306
00:19:34,880 --> 00:19:39,240
So I can show you to you and ask, is that object present in this image?

307
00:19:39,240 --> 00:19:41,240
I think we all agree, yes.

308
00:19:41,240 --> 00:19:45,440
I can ask you questions like how many are in this image, and I think we'd all agree there

309
00:19:45,440 --> 00:19:47,400
are two.

310
00:19:47,400 --> 00:19:50,520
And I can even ask you, is it present in this image?

311
00:19:50,520 --> 00:19:56,660
And I think you'd all agree, yeah, but it's weird, right?

312
00:19:56,660 --> 00:20:02,880
So not only can you recognize the object from a single training example, not thousands,

313
00:20:02,880 --> 00:20:08,400
not millions, one, you can reason about it now in context where it's just weird, right?

314
00:20:08,400 --> 00:20:12,440
And that's why you can tell that it's a fur-covered saucer cup and spoon and not a teddy bear

315
00:20:12,440 --> 00:20:14,440
because you have this ability to reason out a sample.

316
00:20:14,440 --> 00:20:19,560
And that's really a remarkable ability that we'd love to have because when you get past

317
00:20:19,560 --> 00:20:24,680
imagery, you get past digital images, there are very few data sets that have this kind

318
00:20:24,680 --> 00:20:28,340
of scale that ImageNet has.

319
00:20:28,340 --> 00:20:33,040
But even ImageNet turns out, you know, there's something else wrong with it.

320
00:20:33,040 --> 00:20:37,520
So does anyone notice anything about these chairs in the image, these were all taken

321
00:20:37,520 --> 00:20:39,720
from ImageNet, from the chairs category?

322
00:20:39,720 --> 00:20:42,520
Does anyone notice anything unusual about these?

323
00:20:42,880 --> 00:20:46,640
They're all facing the camera, they're in canonical views, they're all more or less centered.

324
00:20:46,640 --> 00:20:50,040
In the case where there's multiple chairs, they're kind of like almost like a texture

325
00:20:50,040 --> 00:20:51,200
of chairs, right?

326
00:20:51,200 --> 00:20:54,280
So these are very unusual images, actually.

327
00:20:54,280 --> 00:20:57,480
I mean, we look at them and we think these are normal images of chairs, but these are

328
00:20:57,480 --> 00:21:01,720
actually very carefully posed and crafted images.

329
00:21:01,720 --> 00:21:06,480
And one of the projects that we've been working on together with MIT across the MIT IBM Lab

330
00:21:06,480 --> 00:21:12,480
was, this is a project that was led by Boris Katz and Andre Barbou together with our own

331
00:21:12,600 --> 00:21:17,800
Dan Gutfreund, they asked, okay, well, what if we collected a data set where that wasn't

332
00:21:17,800 --> 00:21:22,400
true, so where we didn't have carefully, perfectly centered objects?

333
00:21:22,400 --> 00:21:26,080
And what they did is they enlisted a whole bunch of mechanical turkers on Amazon Mechanical

334
00:21:26,080 --> 00:21:31,880
Turk and they told them, take a hammer, take it into your bedroom, put it on your bed,

335
00:21:31,880 --> 00:21:36,360
and here's a smartphone app and please put it in this bounding box.

336
00:21:36,360 --> 00:21:40,960
So basically you'd have to go and the people get instructions, you know, take your chair,

337
00:21:40,960 --> 00:21:44,280
we want you to put it in the living room, and we want you to put it on its side and

338
00:21:44,280 --> 00:21:45,840
put it in that bounding box.

339
00:21:45,840 --> 00:21:49,360
Or we want you to take a knife out of your kitchen, put it in the bathroom, and make

340
00:21:49,360 --> 00:21:52,800
it fit in that bounding box, or, you know, take that bottle and put it on a chair in

341
00:21:52,800 --> 00:21:53,800
this orientation.

342
00:21:53,800 --> 00:21:57,400
So they went through and they just collected a huge amount of this data, so they collected

343
00:21:57,400 --> 00:22:02,720
50,000 of these images from 300 object classes that overlap with ImageNet, and then they

344
00:22:02,720 --> 00:22:06,960
asked the mechanical turkers to go into four different rooms with those things.

345
00:22:06,960 --> 00:22:12,840
So remember, everyone talks about how ImageNet is state-of-the-art in object categorization

346
00:22:12,840 --> 00:22:17,120
and that's a solved problem, but it turns out when you take these images of these objects

347
00:22:17,120 --> 00:22:22,520
that are not in the right place, humans can perform it well over 95 percent accuracy on

348
00:22:22,520 --> 00:22:28,280
this task, but the AIs, the CNNs that were previously performing, you know, at state-of-the-art

349
00:22:28,280 --> 00:22:33,560
levels, drop all the way down 40 to 45 percent down in their performance.

350
00:22:33,560 --> 00:22:38,240
So there's a very real sense in which, as amazing as deep learning is, and I'm going

351
00:22:38,240 --> 00:22:45,760
to keep saying this, deep learning is amazing, but some of the gains in the, you know, the

352
00:22:45,760 --> 00:22:49,920
sort of declarations of victory are a little bit overstated, and they all circle around

353
00:22:49,920 --> 00:22:56,680
this idea of small data, of corner cases, edge cases, and being able to reason about

354
00:22:56,680 --> 00:22:59,760
situations that are a little bit out of the ordinary.

355
00:22:59,760 --> 00:23:01,680
All right?

356
00:23:01,680 --> 00:23:07,320
And of course, the last piece, you know, that's concerning for anyone who's trying to deploy

357
00:23:07,320 --> 00:23:12,560
neural networks in the real world is that they're weirdly vulnerable to hacking.

358
00:23:12,560 --> 00:23:18,800
So I don't know if you guys covered adversarial examples in this class yet, but here's an

359
00:23:18,800 --> 00:23:25,400
example targeting that same captioning system, so where, you know, the captioning system

360
00:23:25,400 --> 00:23:29,120
can see this picture of a stop sign and produce this beautiful natural language caption, a

361
00:23:29,120 --> 00:23:32,000
red stop sign sitting on the side of a road.

362
00:23:32,000 --> 00:23:37,160
Our own Pinyu Chen, who's an expert in this area at IBM, created this image, which is

363
00:23:37,160 --> 00:23:41,800
a very subtle perturbation of the original, and you can get that to now say a brown teddy

364
00:23:41,800 --> 00:23:44,960
bear lying on top of the bed with high confidence.

365
00:23:44,960 --> 00:23:50,360
So this is a case, again, where there's something divergent between how we perceive images and

366
00:23:50,360 --> 00:23:56,280
understand what the content of an image is and how these end-to-end trained neural networks

367
00:23:56,280 --> 00:23:57,720
do the same.

368
00:23:57,720 --> 00:24:03,120
Like, you know, this kind of, you know, this image, the perturbations of the pixels in

369
00:24:03,120 --> 00:24:06,800
this image were done in such a way that they'd be small so that you couldn't see them, so

370
00:24:06,800 --> 00:24:10,400
they're specifically hidden from us.

371
00:24:10,400 --> 00:24:14,360
But it turns out that you don't have to actually have access to the digital image.

372
00:24:14,360 --> 00:24:18,480
You can also do real-world in-the-wild adversarial attacks.

373
00:24:18,480 --> 00:24:25,000
And this is one that was kind of fun, some folks in my lab and my group decided it'd

374
00:24:25,000 --> 00:24:27,760
be fun to have a t-shirt that was adversarial.

375
00:24:27,760 --> 00:24:33,520
So you took a person detector, and so this is like, you know, it'll detect a person.

376
00:24:33,520 --> 00:24:36,160
You can imagine, like, an AI-powered surveillance system.

377
00:24:36,160 --> 00:24:38,520
If you were to intrude in the building, you might want to have a person detector that could

378
00:24:38,520 --> 00:24:43,880
detect a person and warn somebody, hey, there's a person in your building that doesn't belong.

379
00:24:43,880 --> 00:24:46,720
But what they did is they created this shirt.

380
00:24:46,720 --> 00:24:48,360
So this shirt's very carefully crafted.

381
00:24:48,360 --> 00:24:51,560
It's a very loud, ugly shirt.

382
00:24:51,560 --> 00:24:52,560
We have it in the lab.

383
00:24:52,560 --> 00:24:56,320
If you want to come over any time and try it on, you're welcome to.

384
00:24:56,320 --> 00:24:58,960
But this shirt basically makes you invisible to AI.

385
00:24:58,960 --> 00:25:04,560
So this is Sija, who's our wizard adversarial example.

386
00:25:04,560 --> 00:25:07,520
He's not wearing the shirt, so you can see the person detector is detecting him just

387
00:25:07,520 --> 00:25:08,520
fine.

388
00:25:08,520 --> 00:25:09,880
Tron Fu is wearing the shirt.

389
00:25:09,880 --> 00:25:10,880
Therefore he is invisible.

390
00:25:10,880 --> 00:25:11,880
He is camouflaged.

391
00:25:11,880 --> 00:25:17,480
And you can see, even as you walk around, even if the shirt is folded and bent and wrinkled,

392
00:25:17,480 --> 00:25:19,360
it makes you invisible.

393
00:25:19,360 --> 00:25:23,040
So weird, right?

394
00:25:23,040 --> 00:25:27,880
If anything for us, this ugly-looking shirt makes you even more visible.

395
00:25:27,880 --> 00:25:32,560
So there's something weird about how deep learning seems to work relative to how weird

396
00:25:32,560 --> 00:25:34,400
we work.

397
00:25:34,400 --> 00:25:40,680
But there are also problems where, even under the best of conditions, no adversarial perturbation,

398
00:25:40,680 --> 00:25:45,000
you can have as much training data as you like, where deep learning still struggles.

399
00:25:45,000 --> 00:25:49,080
And these are really interesting for us, because these are cases where, no matter how much

400
00:25:49,080 --> 00:25:53,320
data you have, deep learning just doesn't cut it for some reason.

401
00:25:53,320 --> 00:25:54,320
And we want to know why.

402
00:25:54,320 --> 00:25:59,560
So problems like this, if you ask the question, so I give you a picture, and I ask the question,

403
00:25:59,560 --> 00:26:02,960
how many blocks are on the right of the three-level tower?

404
00:26:02,960 --> 00:26:06,240
Or will the block tower fall if the top block is removed?

405
00:26:06,240 --> 00:26:08,440
Or are there more animals than trees?

406
00:26:08,440 --> 00:26:12,520
Or what is the shape of the object closest to the large cylinder?

407
00:26:12,520 --> 00:26:16,440
These are all questions that even a child could answer, I mean, provided they understand

408
00:26:16,440 --> 00:26:22,880
language and can read and stuff, it's very easy for us to work on these things.

409
00:26:22,880 --> 00:26:25,840
But it turns out that deep learning systems, irrespective of how much training data you

410
00:26:25,840 --> 00:26:28,200
give, struggle.

411
00:26:28,200 --> 00:26:33,000
So that's a case where they're smoking, you kind of want to know where's the fire.

412
00:26:33,000 --> 00:26:36,920
Actually the answer we think, or one of the things we're exploring, is the idea that maybe

413
00:26:36,920 --> 00:26:39,880
the answer lies all the way back in 1956.

414
00:26:39,880 --> 00:26:46,320
So this is a picture of that Dartmouth workshop back in 1956.

415
00:26:46,320 --> 00:26:51,840
And back in this time period, neural networks were already, you know, had already been sort

416
00:26:51,840 --> 00:26:52,840
of born.

417
00:26:52,840 --> 00:26:56,400
We were thinking about neural networks, but we were also thinking about another kind of

418
00:26:56,400 --> 00:26:57,400
AI back then.

419
00:26:57,400 --> 00:27:00,680
And that kind of AI is interesting and different.

420
00:27:00,680 --> 00:27:04,560
It hasn't really enjoyed a resurgence the way that neural networks have.

421
00:27:04,560 --> 00:27:07,680
But just to step back for a moment, this is what you've been studying.

422
00:27:07,680 --> 00:27:10,600
Neural network basically is a nonlinear function approximator.

423
00:27:10,600 --> 00:27:15,960
You take an input in, and you get some output that you want out, and it learns the weights

424
00:27:15,960 --> 00:27:18,440
of the network through training with data.

425
00:27:18,440 --> 00:27:21,400
So this is what an apple looks like to a neural network.

426
00:27:21,400 --> 00:27:26,560
You know, you put an apple picture in, and you light up, you know, a unit that says there's

427
00:27:26,560 --> 00:27:29,360
probably an apple in that scene.

428
00:27:29,360 --> 00:27:33,960
There's another kind of AI that's been around since the beginning called symbolic AI.

429
00:27:33,960 --> 00:27:39,800
And this is from a book by Marvin Minsky in 1991 that was created here.

430
00:27:39,800 --> 00:27:42,800
And this is what an apple looks like to symbolic AI.

431
00:27:42,800 --> 00:27:44,760
So we know things about an apple.

432
00:27:44,760 --> 00:27:46,800
We know that an apple has an origin.

433
00:27:46,800 --> 00:27:47,880
It comes from an apple tree.

434
00:27:47,880 --> 00:27:50,120
We know that an apple is a kind of fruit.

435
00:27:50,120 --> 00:27:51,120
You know, that apple has parts.

436
00:27:51,120 --> 00:27:52,800
It has a body, and it has a stem.

437
00:27:52,800 --> 00:27:54,360
The body has a shape.

438
00:27:54,360 --> 00:27:55,360
It's round.

439
00:27:55,360 --> 00:27:56,360
It has a size.

440
00:27:56,360 --> 00:27:57,360
It can fit in your hand.

441
00:27:57,360 --> 00:27:58,360
It's got a color.

442
00:27:58,360 --> 00:27:59,720
It could be red or green.

443
00:27:59,720 --> 00:28:03,480
We know lots of knowledge about what an apple is.

444
00:28:03,480 --> 00:28:07,880
And that's a very different take on AI.

445
00:28:07,880 --> 00:28:12,920
And you know, basically this field of what we call good old fashioned AI or symbolic

446
00:28:12,920 --> 00:28:19,280
AI has been around since the very beginning, and it just hasn't yet enjoyed that resurgence

447
00:28:19,280 --> 00:28:21,280
that neural networks did.

448
00:28:21,280 --> 00:28:25,320
And one of the central theses that we're exploring is that just the same way that neural

449
00:28:25,320 --> 00:28:29,960
networks have been waiting, they were waiting for compute and data to come along to make

450
00:28:29,960 --> 00:28:31,480
them really work.

451
00:28:31,480 --> 00:28:37,200
We think that symbolic AI has also been waiting, but what it's been waiting for is neural networks.

452
00:28:37,200 --> 00:28:42,280
So now that neural networks work, can we go back to some of these ideas from symbolic

453
00:28:42,280 --> 00:28:45,960
AI and do something different?

454
00:28:45,960 --> 00:28:51,400
And the work I'm going to tell you about is a collaboration as part of the MIT IBM Lab.

455
00:28:51,400 --> 00:28:55,440
Chuangon is one of the researchers in my group together with Josh Tenenbaum, in particular

456
00:28:55,440 --> 00:29:00,560
Jojen Wu, who's now an assistant professor at Stanford, along with some others.

457
00:29:00,560 --> 00:29:05,280
And what they're asking is, can we mix together the ideas of neural networks together with

458
00:29:05,280 --> 00:29:10,080
the ideas from symbolic AI and do something that's more than the sum of its parts?

459
00:29:10,080 --> 00:29:14,920
And this picture that I showed you here, I showed you this earlier, this is actually

460
00:29:14,920 --> 00:29:21,080
a data set called Clever that was basically created to illustrate this problem, where

461
00:29:21,080 --> 00:29:24,720
irrespective of how much training data you have, this very simple kind of question answering

462
00:29:24,720 --> 00:29:27,400
task where you have to answer questions like, are there an equal number of large things

463
00:29:27,400 --> 00:29:30,280
in metal spheres seems to be hard.

464
00:29:30,280 --> 00:29:32,520
So the data set was created to illustrate the problem.

465
00:29:32,520 --> 00:29:37,760
And if you tackle this the way you're supposed to with neural networks and deep learning,

466
00:29:37,760 --> 00:29:43,280
and perhaps you've learned over the course of this IEP course that the best way to train

467
00:29:43,280 --> 00:29:45,480
a system is end to end.

468
00:29:45,480 --> 00:29:50,080
The best way to get what you want is to start from what you have and end with what you need

469
00:29:50,080 --> 00:29:54,280
and don't get in the way in the middle, just let the neural network do its thing.

470
00:29:54,280 --> 00:29:58,080
The problem is that when you build end to end neural networks and try and train them

471
00:29:58,080 --> 00:30:04,280
to go from these inputs to these outputs, for data sets like this it just doesn't work

472
00:30:04,280 --> 00:30:05,400
well at all.

473
00:30:05,400 --> 00:30:08,880
And the reason for that is that the concepts, things like colors and shapes and objects and

474
00:30:08,880 --> 00:30:13,320
things like that, and then the portions of reasoning, like counting the number of objects

475
00:30:13,320 --> 00:30:17,640
or reasoning about the relationships between objects are fundamentally entangled inside

476
00:30:17,640 --> 00:30:20,120
the representation of the neural network.

477
00:30:20,120 --> 00:30:24,040
And then not only does that cause problems where it's very difficult to cover the entire

478
00:30:24,040 --> 00:30:28,760
distribution and not get caught by corner cases, but it also means it's hard to transfer

479
00:30:28,760 --> 00:30:32,960
to other kinds of tasks like image captioning or instance retrieval or other kinds of things.

480
00:30:32,960 --> 00:30:37,080
So fundamentally this end to end approach just doesn't seem to work very well.

481
00:30:37,080 --> 00:30:39,680
So I'm already telling you something that's sort of probably against the advice you've

482
00:30:39,680 --> 00:30:42,360
gotten thus far.

483
00:30:42,360 --> 00:30:46,000
But when you step back and look at well how do we solve this problem of visual reasoning,

484
00:30:46,000 --> 00:30:49,760
you have a question like this, are there an equal number of large things in metal spheres?

485
00:30:50,400 --> 00:30:54,320
When we tackle this problem, well first we read the question and we see there's something

486
00:30:54,320 --> 00:30:55,840
about large things.

487
00:30:55,840 --> 00:30:59,960
We use our visual system to sort of find the large things.

488
00:30:59,960 --> 00:31:03,000
Then we read the question, we see there's something about metal spheres and we use our

489
00:31:03,000 --> 00:31:06,000
visual system to find the metal spheres.

490
00:31:06,000 --> 00:31:11,480
And then critically we do an operation, a symbolic operation, an equality operation

491
00:31:11,480 --> 00:31:15,200
where we decide are these an equal number and we say yes.

492
00:31:15,200 --> 00:31:16,200
So that's what we do.

493
00:31:16,240 --> 00:31:21,080
And if you unpack that, yeah it's got visual perception and CNNs are a great candidate

494
00:31:21,080 --> 00:31:22,400
for doing that.

495
00:31:22,400 --> 00:31:25,960
And yeah it's got question understanding, natural language processing and yeah RNNs are

496
00:31:25,960 --> 00:31:28,280
a great tool for doing that.

497
00:31:28,280 --> 00:31:33,720
But critically it also has this component of logical reasoning where you can very flexibly

498
00:31:33,720 --> 00:31:37,920
apply operations in a compositional way.

499
00:31:37,920 --> 00:31:43,920
So what the team did then was to basically this is kind of like the neurosymbolic hello

500
00:31:43,920 --> 00:31:44,920
world.

501
00:31:45,160 --> 00:31:48,160
It's the first program you write in most programming languages.

502
00:31:48,160 --> 00:31:52,200
This is kind of the simplest example that we could tackle.

503
00:31:52,200 --> 00:31:55,120
And this is the sort of diagram of the flow system and don't worry I'm going to unpack

504
00:31:55,120 --> 00:32:01,680
all this where because neural networks are good at vision we use a CNN to do the vision

505
00:32:01,680 --> 00:32:09,000
part but instead of going straight to an answer it's used to basically de-render the scene.

506
00:32:09,000 --> 00:32:13,280
So rendering goes from a symbolic representation to an image, de-rendering goes from the image

507
00:32:13,320 --> 00:32:17,560
back to some kind of symbolic structure representation so we're going to take apart the image using

508
00:32:17,560 --> 00:32:19,040
the neural network.

509
00:32:19,040 --> 00:32:24,600
And then the question, you'd be crazy not to use something like an LSTM to parse the

510
00:32:24,600 --> 00:32:28,880
language but instead of going from the language straight to an answer or going from the language

511
00:32:28,880 --> 00:32:34,640
to a label or something, the language is going to be parsed into a program, into a symbolic

512
00:32:34,640 --> 00:32:38,560
program which is then going to be executed on the structure representation.

513
00:32:38,560 --> 00:32:42,920
So just to walk you through that, we have a vision part, we have a language part.

514
00:32:42,920 --> 00:32:48,240
You parse the scene using a CNN so you turn, you know, you find the objects in the scene

515
00:32:48,240 --> 00:32:51,920
and you basically create a table that says what are the objects, what are their properties

516
00:32:51,920 --> 00:32:56,920
and where are they and then you do semantic parsing on the language part and again the

517
00:32:56,920 --> 00:33:01,400
goal here is to go from natural language with all of its, you know, sort of vagaries and

518
00:33:01,400 --> 00:33:08,520
messiness to a program, a program that we're going to run in a minute and so you need to

519
00:33:08,520 --> 00:33:13,680
learn how to take this language and turn it into a series of symbolic operations and

520
00:33:13,680 --> 00:33:19,840
then you're going to run that symbolic program on the structured symbolic information and

521
00:33:19,840 --> 00:33:21,960
get an answer.

522
00:33:21,960 --> 00:33:25,640
So you would start by filtering and saying I want to look, the question is asking about

523
00:33:25,640 --> 00:33:29,080
something red so I need to filter on red and then I need to query the shape of that object

524
00:33:29,080 --> 00:33:30,560
that I've just filtered.

525
00:33:30,560 --> 00:33:35,680
So this is basic, you know, sort of program execution.

526
00:33:35,680 --> 00:33:41,600
And critically the system is trained jointly with reinforcement learning so the neural

527
00:33:41,600 --> 00:33:46,400
network that does vision and the neural network that translates from language to a program

528
00:33:46,400 --> 00:33:50,480
fundamentally learns something different by virtue of being part of a hybrid symbolic

529
00:33:50,480 --> 00:33:52,000
system, right?

530
00:33:52,000 --> 00:33:56,280
So it gets reward, it doesn't get reward and you propagate gradients and all that based

531
00:33:56,280 --> 00:33:59,640
on whether or not the symbolic system got the right answer and we use reinforcement

532
00:33:59,640 --> 00:34:05,440
learning of course because you can't differentiate through the symbolic part but, you know, fundamentally

533
00:34:05,480 --> 00:34:09,900
this isn't just a matter of bolting neural networks onto a symbolic reasoner but rather

534
00:34:09,900 --> 00:34:14,240
training them jointly so that the symbolic, so the neural networks learn, extract the

535
00:34:14,240 --> 00:34:19,160
symbols, learn to give the right symbolic representations through experience and learning

536
00:34:19,160 --> 00:34:20,160
on the data.

537
00:34:20,160 --> 00:34:23,240
So this does a couple of really interesting things.

538
00:34:23,240 --> 00:34:26,800
So one of the first things you'll notice, this data set was created, this clever data

539
00:34:26,800 --> 00:34:31,000
set was created because it illustrated a problem with end-to-end learning but it turns out

540
00:34:31,000 --> 00:34:36,800
that with just a dash of symbolic execution now you can be effectively perfect on the

541
00:34:36,800 --> 00:34:41,120
clever data set so clever is now solved and this was actually an oral spotlight paper

542
00:34:41,120 --> 00:34:47,240
at NURBS because this is a big deal, previously unsolved problem now solved, that's good.

543
00:34:47,240 --> 00:34:50,960
But interestingly, more than that, remember I said the biggest problem with deploying

544
00:34:50,960 --> 00:34:54,920
neural networks in the real world is that we rarely have big data, we usually have pretty

545
00:34:54,920 --> 00:34:55,920
small data.

546
00:34:55,920 --> 00:35:01,400
So anything that reduces the sample, improves the sample efficiency of these methods is

547
00:35:01,400 --> 00:35:07,280
really valuable and so here's the number of training examples that the system is given,

548
00:35:07,280 --> 00:35:12,880
this is the accuracy of the system, the neural symbolic system is up here in blue, I'll just

549
00:35:12,880 --> 00:35:19,280
point out several things, one is it's always better and but if you look at, you know, down

550
00:35:19,280 --> 00:35:26,440
here the end-to-end train systems require close to a million examples, they're kind

551
00:35:26,440 --> 00:35:32,040
of, you know, okay results, not perfect but okay, the neural symbolic system again with

552
00:35:32,040 --> 00:35:36,920
just a dash of symbolic mixed in with just one percent of the data can do better than

553
00:35:36,920 --> 00:35:40,200
most of the end-to-end train systems and with just ten percent of the data, one tenth of

554
00:35:40,200 --> 00:35:44,040
the data can perform at effectively perfect performance.

555
00:35:44,040 --> 00:35:51,280
So drastically lower, you know, requirement for data, drastically higher sample efficiency.

556
00:35:51,280 --> 00:35:54,880
And then the last piece, remember I said explainability was super important, you know, people are actually

557
00:35:54,880 --> 00:35:58,600
going to really use AI systems, they need to be able to look inside and understand why

558
00:35:58,600 --> 00:36:03,400
the decision is made, otherwise they won't trust the AI system, they won't use it.

559
00:36:03,400 --> 00:36:08,240
Because the system has a symbolic choke point in the middle where you parse the question

560
00:36:08,240 --> 00:36:13,600
into a series of symbolic operations, we can debug the system the same way you would debug

561
00:36:13,600 --> 00:36:18,640
a traditional coded system, so you can see, okay well what did it do, it filtered on cyan,

562
00:36:18,640 --> 00:36:22,360
it filtered on metal, was that the right thing, you can just, you can understand why it made

563
00:36:22,360 --> 00:36:25,960
the decision it made, and if it made the wrong decision now you have some guidance on what

564
00:36:25,960 --> 00:36:27,720
you'd want to do next.

565
00:36:27,720 --> 00:36:34,160
So that was a paper from this team in 2018, Neurosymbolic BQA, actually since then there's

566
00:36:34,160 --> 00:36:39,960
basically been a parade of papers that have made this more and more sophisticated.

567
00:36:39,960 --> 00:36:45,880
So there was a paper in iClear in 2019 called the Neurosymbolic Concept Learner that relaxed

568
00:36:45,880 --> 00:36:50,360
the requirement that the concepts be pre-coded, there's another paper that just came out in

569
00:36:50,360 --> 00:36:54,400
Europe just a few months ago or last month called the Neurosymbolic Meta Concept Learner

570
00:36:54,400 --> 00:36:59,760
that autonomously learns new concepts, it can sort of use meta concepts to do better,

571
00:36:59,760 --> 00:37:03,960
and we're even now getting this to work in not just these toy images but also in real

572
00:37:03,960 --> 00:37:06,840
world images, which is obviously important.

573
00:37:06,840 --> 00:37:09,680
And we think this is actually a really interesting and profitable direction to go forward.

574
00:37:09,680 --> 00:37:13,880
So here's the Neurosymbolic Concept Learning, basically what's happened is we're relaxing

575
00:37:13,880 --> 00:37:20,480
now these concepts into concept embeddings, so when you look at an object with a CNN you

576
00:37:20,480 --> 00:37:27,000
can now embed into a space of color and then compare that color to stored concept embeddings,

577
00:37:27,000 --> 00:37:32,840
which means you can now learn new concepts dynamically and you can learn them from context.

578
00:37:32,840 --> 00:37:36,560
So you don't need to know that green is a color, you can figure that out and learn that,

579
00:37:36,560 --> 00:37:42,120
this is important because the world's full of new concepts that we're constantly encountering,

580
00:37:42,120 --> 00:37:45,120
so that was the sort of next innovation on the system.

581
00:37:45,120 --> 00:37:49,080
Also remember I said that one of the things that's magical about symbolic AI is that we

582
00:37:49,080 --> 00:37:54,280
can leverage lots of different kinds of knowledge, and in fact we can leverage these sort of

583
00:37:54,280 --> 00:38:01,000
meta relationships between different concepts, we know there are synonyms for instance, which

584
00:38:01,000 --> 00:38:05,800
led to this paper which was just presented last month called the Neurosymbolic Meta Concept

585
00:38:05,800 --> 00:38:11,920
Learner, where you can have a notion of is red the same kind of concept as green or is

586
00:38:11,920 --> 00:38:19,760
cube a synonym of block, which then of course lets you do things like if I go through in

587
00:38:19,760 --> 00:38:26,120
the regular mode here and I'm creating a representation of the object and I'm creating a symbolic program,

588
00:38:26,120 --> 00:38:31,920
I can do the regular thing, but then also critically I can now use relationships I know

589
00:38:31,920 --> 00:38:38,120
about synonyms and concept equivalencies to meta verify these things, and then I can

590
00:38:38,120 --> 00:38:41,480
take advantage of the fact that if I know that there's an airplane then I also know

591
00:38:41,480 --> 00:38:44,920
there's a plane because a plane and an airplane are synonyms.

592
00:38:44,920 --> 00:38:49,000
I can know that if there's any kind of kid and the answer is yes, that is there any kind

593
00:38:49,000 --> 00:38:53,000
of child, I know that's also yes because child and kid are synonyms.

594
00:38:53,000 --> 00:38:57,280
So we can start to see how we can get more and more complex and more and more sophisticated

595
00:38:57,280 --> 00:39:00,680
with our symbolic reasoning and do more and more and more.

596
00:39:00,680 --> 00:39:02,760
Of course it works well.

597
00:39:02,760 --> 00:39:07,520
We're also now extending since clever is now beaten, we're now looking also at, we're

598
00:39:07,520 --> 00:39:13,400
releasing a new data set called video clever, it's called cleverer, which is a very tortured

599
00:39:13,400 --> 00:39:18,720
acronym, looking at the relationships between objects and counterfactuals, what would happen

600
00:39:18,720 --> 00:39:22,560
if this block weren't there, so you can see that we can kind of expand to more and more

601
00:39:22,560 --> 00:39:25,560
sophisticated environments as we go.

602
00:39:25,640 --> 00:39:32,960
I'll just also say that this notion of symbolic program execution isn't the only idea from

603
00:39:32,960 --> 00:39:35,960
symbolic AI that we can bring together with neural networks.

604
00:39:35,960 --> 00:39:40,040
We're also looking at the field of planning, so there's a field of symbolic AI called planning

605
00:39:40,040 --> 00:39:44,680
where you try and start from an initial state and then use an action plan to arrive at some

606
00:39:44,680 --> 00:39:48,960
target state, which is really good for solving problems like the tower of Hanoi which you

607
00:39:48,960 --> 00:39:52,680
may have encountered or these kinds of slider puzzles where you need to produce a series

608
00:39:52,720 --> 00:39:58,600
of operations to achieve a certain end state, like make the picture into the right shape.

609
00:39:58,600 --> 00:40:02,960
Another area of projects that we're working on is mixing these together with neural networks

610
00:40:02,960 --> 00:40:06,920
so that we don't just have to rely on sort of static symbolic representations, but we

611
00:40:06,920 --> 00:40:10,040
can actually work in the latent space of an autoencoder.

612
00:40:10,040 --> 00:40:14,800
We have binary discrete autoencoders and we can actually plan in the latent space of an

613
00:40:14,800 --> 00:40:15,800
autoencoder.

614
00:40:15,800 --> 00:40:20,240
Obviously these are topics that would be a whole talk unto themselves, but I just want

615
00:40:20,280 --> 00:40:24,400
to give you a little bit of a flavor that this idea of mashing up neural networks and

616
00:40:24,400 --> 00:40:32,480
symbolic AI has a lot of range and there's a lot of room to grow and explore and lots

617
00:40:32,480 --> 00:40:36,520
of ideas in symbolic AI now that we can bring together and every time we do we seem to find

618
00:40:36,520 --> 00:40:39,520
that good things happen.

619
00:40:39,520 --> 00:40:45,960
So with that I'll stop just to give you one picture in your mind, these sort of two venerable

620
00:40:46,000 --> 00:40:47,000
traditions of AI.

621
00:40:47,000 --> 00:40:53,160
I think we're coming to a place where we can bring the symbolic stuff out of the closet

622
00:40:53,160 --> 00:40:58,400
dusted off and in many ways the power of neural networks solves many of the problems and they

623
00:40:58,400 --> 00:41:01,760
complement each other's strengths and weaknesses in really important and useful ways.

624
00:41:01,760 --> 00:41:05,520
So with that I'll stop and thank you all for your attention and if you have any questions

625
00:41:05,520 --> 00:41:06,520
I'm very happy to answer them.

626
00:41:06,520 --> 00:41:07,520
Thank you.

627
00:41:07,520 --> 00:41:08,520
Thank you.

628
00:41:08,520 --> 00:41:09,520
Thank you.

629
00:41:09,520 --> 00:41:10,520
Thank you.

630
00:41:10,520 --> 00:41:11,520
Thank you.

631
00:41:11,520 --> 00:41:12,520
Thank you.

632
00:41:12,520 --> 00:41:13,520
Thank you.

633
00:41:13,520 --> 00:41:14,520
Thank you.

634
00:41:14,520 --> 00:41:15,520
Thank you.

