start	end	text
0	17160	Good afternoon, everyone, and welcome to MIT 6S191, Introduction to Deep Learning.
17160	21320	My name is Alexander Amini and I'm so excited to be your instructor this year along with
21320	24640	Aviso Imani in this new virtual format.
24640	29960	6S191 is a two-week boot camp on everything deep learning and will cover a ton of material
30240	31560	in only two weeks.
31560	35160	So I think it's really important for us to dive right in with these lectures.
35160	39600	But before we do that, I do want to motivate exactly why I think this is such an awesome
39600	41360	field to study.
41360	45880	And when we taught this class last year, I decided to try introducing the class very
45880	46880	differently.
46880	53400	And instead of me telling the class how great 6S191 is, I wanted to let someone else do
53400	54480	that instead.
54480	60640	So actually, I want to start this year by showing you how we introduced 6S191 last year.
60640	71440	Hi, everybody, and welcome to MIT 6S191, the official introductory course on deep learning
71440	74440	taught here at MIT.
74440	82720	Deep learning is revolutionizing so many fields from robotics, medicine, and everything
82720	83720	in between.
83720	91520	You'll learn from the medals of this field and how you can build some of these incredible
91520	92520	algorithms.
92520	102720	In fact, this entire speech and video are not real and were created using deep learning
102720	104720	and artificial intelligence.
104720	108720	And in this class, you'll learn how.
109720	119720	It has been an honor to speak with you today, and I hope you enjoyed the course.
119720	123720	So in case you couldn't tell, that was actually not a real video or audio.
123720	128720	And the audio you actually heard was purposely degraded, a bit more to even make it more
128720	132720	obvious that this was not real and avoid some potential misuse.
133720	138720	Even with the purposely degraded audio, that intro went somewhat viral last year after
138720	142420	the course and we got some really great and interesting feedback.
142420	147080	And to be honest, after last year and when we did this, I thought it was going to be
147080	151000	really hard for us to top it this year.
151000	154720	But actually, I was wrong because the one thing I love about this field is that it's
154720	160720	moving so incredibly fast that even within the past year, the state of the art has significantly
160720	161720	advanced.
161720	167480	And the video you saw that we used last year used deep learning, but it was not a particularly
167480	169520	easy video to create.
169520	174680	It required a full video of Obama speaking, and it used this to intelligently stitch
174680	179440	together parts of the scene to make it look and appear like he was mouthing the words
179440	180440	that I said.
180440	185720	And to see the behind the scenes here, now you can see the same video with my voice.
185720	195520	Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning
195520	198960	taught here at MIT.
198960	204380	Now it's actually possible to use just a single static image, not the full video to
204380	207160	achieve the exact same thing.
207160	214800	And now you can actually see eight more examples of Obama now just created using just a single
214800	221000	static image, no more full dynamic videos, but we can achieve the same incredible realism
221000	223440	and result using deep learning.
223440	229200	Now of course, there's nothing restricting us to one person.
229200	234200	This method generalizes to different faces, and there's nothing restricting us even to
234200	240160	humans anymore or individuals that the algorithm has ever seen before.
240160	249440	Hi everybody, and welcome to MIT 6S191, the official introductory course on deep learning
249440	253960	taught here at MIT.
253960	260080	The ability to generate these types of dynamic moving videos from only a single image is
260080	264680	remarkable to me, and it's a testament to the true power of deep learning.
264680	270640	In this class, you're going to actually not only learn about the technical basis of this
270640	279800	technology, but also some of the very important and very important ethical and societal implications
279800	282080	of this work as well.
282080	287720	Now I hope this was a really great way to get you excited about this course and 6S191,
287720	290240	and with that let's get started.
290240	294240	We can actually start by taking a step back and asking ourselves what is deep learning,
294320	298000	deep learning, and the context of intelligence.
298000	304680	Intelligence is actually the ability to process information such that it can be used to inform
304680	306680	a future decision.
306680	312040	Now the field of artificial intelligence, or AI, is a science that actually focuses
312040	317360	on building algorithms to do exactly this, to build algorithms to process information
317360	321640	such that they can inform future predictions.
321640	327440	Now machine learning, you can think of this as just a subset of AI that actually focuses
327440	333160	on teaching an algorithm to learn from experiences without being explicitly programmed.
333160	337480	Now deep learning takes this idea even further, and it's a subset of machine learning that
337480	344120	focuses on using neural networks to automatically extract useful patterns in raw data, and then
344120	349520	using these patterns or features to learn to perform that task.
349520	351800	And that's exactly what this class is about.
351800	357200	This class is about teaching algorithms how to learn a task directly from raw data.
357200	363000	And we want to provide you with a solid foundation both technically and practically for you to
363000	371080	understand under the hood how these algorithms are built and how they can learn.
371080	376480	So this course is split between technical lectures as well as project software labs.
376480	380600	We'll cover the foundation starting today with neural networks, which are really the
380600	384960	building blocks of everything that we'll see in this course.
384960	390880	And this year we also have two brand new really exciting hot topic lectures, focusing on uncertainty
390880	396800	and probabilistic deep learning, as well as algorithmic bias and fairness.
396800	401520	Finally we'll conclude with some really exciting guest lectures and student project presentations
401520	406800	as part of a final project competition that all of you will be eligible to win some really
406800	409440	exciting prizes.
409440	413560	Now a bit of logistics before we dive into the technical side of the lecture.
413560	418240	For those of you taking this course for credit, you will have two options to fulfill your
418240	419800	credit requirement.
419800	427600	The first option will be to actually work in teams of up to four or individually to develop
427600	430880	a cool new deep learning idea.
430920	434960	Now doing so will make you eligible to win some of the prizes that you can see on the
434960	437000	right hand side.
437000	441360	And we realize that in the context of this class, which is only two weeks, that's an
441360	445840	extremely short amount of time to come up with an impressive project or research idea.
445840	450840	So we're not going to be judging you on the novelty of that idea, but rather we're not
450840	455360	going to be judging you on the results of that idea, but rather the novelty of the idea,
455360	459880	your thinking process and how impactful this idea can be.
459880	463560	But not on the results themselves.
463560	468080	On the last day of class, you will actually give a three minute presentation to a group
468080	472000	of judges who will then award the winners and the prizes.
472000	477320	Now again, three minutes is extremely short to actually present your ideas and present
477320	483360	your project, but I do believe that there's an art to presenting and conveying your ideas
483360	486720	concisely and clearly in such a short amount of time.
486720	492720	So we will be holding you strictly to that strict deadline.
492720	498120	The second option to fulfill your grade requirement is to write a one page review on a deep learning
498120	499640	paper.
499640	503040	Here the grade is based more on the clarity of the writing and the technical communication
503040	505000	of the main ideas.
505000	509400	This will be due on Thursday, the last Thursday of the class, and you can pick whatever deep
509400	510880	learning paper you would like.
510880	516520	If you would like some pointers, we have provided some guide papers that can help you get started
516520	520720	if you would just like to use one of those for your review.
520720	528240	In addition to the final project prizes, we'll also be awarding this year three lab prizes,
528240	531880	one associated to each of the software labs that students will complete.
531880	536840	Again, completion of the software labs is not required for grade of this course, but
536840	539640	it will make you eligible for some of these cool prizes.
539640	545840	So please, we encourage everyone to compete for these prizes and get the opportunity to
545840	548920	win them all.
548920	552080	Please post to Piazza if you have any questions.
552080	557200	Visit the course website for announcements and digital recordings of the lectures, etc.
557200	560840	And please email us if you have any questions.
560840	566040	Also there are software labs and office hours right after each of these technical lectures
566040	568080	held in Gather Town.
568080	573040	So please drop by in Gather Town to ask any questions about the software labs, specifically
573040	577320	on those, or more generally about past software labs or about the lecture that occurred that
577320	580520	day.
580520	588200	Now this course has an incredible group of TAs and teaching assistants that you can reach
588200	594040	out to at any time in case you have any issues or questions about the material that you're
594040	596040	learning.
596040	600200	And finally, we want to give a huge thanks to all of our sponsors who, without their
600240	603320	help, this class would not be possible.
603320	607880	This is the fourth year that we're teaching this class, and each year it just keeps getting
607880	612080	bigger and bigger and bigger, and we really give a huge shout out to our sponsors for
612080	615400	helping us make this happen each year.
615400	619600	And especially this year in light of the virtual format.
619600	622080	So now let's start with the fun stuff.
622080	627800	Let's start by asking ourselves a question about why do we all care about deep learning?
627800	632120	And specifically, why do we care right now?
632120	638280	To understand that, it's important to actually understand first why is deep learning or how
638280	642000	is deep learning different from traditional machine learning?
642000	647880	Now traditionally, machine learning algorithms define a set of features in their data.
647880	653440	Usually these are features that are handcrafted or hand engineered, and as a result they tend
653440	656920	to be pretty brittle in practice when they're deployed.
656920	662440	The key idea of deep learning is to learn these features directly from data in a hierarchical
662440	663760	manner.
663760	668640	That is, can we learn, if we want to learn how to detect a face, for example, can we
668640	674880	learn to first start by detecting edges in the image, composing these edges together
674880	680760	to detect mid-level features such as a eye or a nose or a mouth, and then going deeper
680760	686680	and composing these features into structural, facial features, so that we can recognize
686680	689440	this face.
689440	694280	This hierarchical way of thinking is really core to deep learning as core to everything
694280	697880	that we're going to learn in this class.
697880	703480	Actually the fundamental building blocks though of deep learning and neural networks have
703480	705480	actually existed for decades.
705480	709920	So one interesting thing to consider is why are we studying this now?
709920	715760	Now is an incredibly amazing time to study these algorithms, and for one reason is because
715760	718800	data has become much more pervasive.
718800	724640	These models are extremely hungry for data, and at the moment we're living in an era
724640	727480	where we have more data than ever before.
727480	733160	Secondly, these algorithms are massively parallelizable, so they can benefit tremendously
733160	739320	from modern GPU hardware that simply did not exist when these algorithms were developed.
739320	744360	And finally, due to open source toolboxes like TensorFlow, building and deploying these
744360	750200	models has become extremely streamlined.
750200	755180	So let's start actually with the fundamental building block of deep learning and of every
755180	756620	neural network.
756620	760760	That is just a single neuron, also known as a perceptron.
760760	765520	So we're going to walk through exactly what is a perceptron, how it's defined, and we're
765520	769920	going to build our way up to deeper neural networks all the way from there.
769920	774400	So let's start really at the basic building block.
774400	779000	The idea of a perceptron or a single neuron is actually very simple.
779000	783720	So I think it's really important for all of you to understand this at its core.
783720	788200	Let's start by actually talking about the forward propagation of information through
788200	790280	this single neuron.
790280	796840	We can define a set of inputs xi through xm, which you can see on the left-hand side,
796840	801840	and each of these inputs or each of these numbers are multiplied by their corresponding
801840	805040	weight and then added together.
805040	810240	We take this single number, the result of that addition, and pass it through what's
810240	817680	called a nonlinear activation function to produce our final output y.
817680	822400	We can actually, actually this is not entirely correct because one thing I forgot to mention
822400	827200	is that we also have what's called a bias term in here, which allows you to shift your
827200	830160	activation function left or right.
830160	835560	Now on the right-hand side of this diagram, you can actually see this concept illustrated
835560	839240	or written out mathematically as a single equation.
839240	844760	You can actually rewrite this in terms of linear algebra matrix multiplications and
844760	850840	dot products to represent this a bit more concisely.
850840	853120	So let's do that.
853120	859200	Let's now do that with x, capital X, which is a vector of our inputs, x1 through xm,
859200	864040	and capital W, which is a vector of our weights, w1 through wm.
864040	870280	So each of these are vectors of length m, and the output is very simply obtained by taking
870280	879280	their dot product, adding a bias, which in this case is w0, and then applying a nonlinearity,
879280	881880	g.
881880	887080	One thing is that I haven't, I've been mentioning it a couple of times, this nonlinearity, g.
887080	889280	What exactly is it?
889280	893560	Because I've mentioned it now a couple of times, well, it is a nonlinear function.
893560	900160	One common example of this nonlinear activation function is what is known as the sigmoid function.
900240	902760	Defined here on the right.
902760	906120	In fact, there are many types of nonlinear functions.
906120	910560	You can see three more examples here, including the sigmoid function.
910560	915600	And throughout this presentation, you'll actually see these TensorFlow code blocks,
915600	920200	which will actually illustrate how we can take some of the topics that we're learning
920200	927080	in this class and actually practically use them using the TensorFlow software library.
927520	931920	Now the sigmoid activation function, which I presented on the previous slide, is very
931920	934400	popular since it's a function that gives outputs.
934400	940880	It takes as input any real number, any activation value, and it outputs a number always between
940880	942640	zero and one.
942640	947000	So this makes it really, really suitable for problems and probability, because probabilities
947000	949160	also have to be between zero and one.
949160	952920	So this makes them very well suited for those types of problems.
952920	957400	In modern deep neural networks, the ReLU activation function, which you can see on the
957400	961440	right, is also extremely popular because of its simplicity.
961440	964280	In this case, it's a piecewise linear function.
964280	971160	It is zero before when it's in the negative regime, and it is strictly the identity function
971160	973600	in the positive regime.
973600	980280	But one really important question that I hope that you're asking yourselves right now is
980320	983960	why do we even need activation functions?
983960	989440	I think actually throughout this course, I do want to say that no matter what I say in
989440	994640	the course, I hope that always you're questioning why this is a necessary step and why do we
994640	998240	need each of these steps, because often these are the questions that can lead to really
998240	1000720	amazing research breakthroughs.
1000720	1003480	So why do we need activation functions?
1003480	1008040	Now the point of an activation function is to actually introduce non-linearities into
1008080	1011680	our network, because these are non-linear functions.
1011680	1015920	And it allows us to actually deal with non-linear data.
1015920	1021120	This is extremely important in real life, especially because in the real world, data
1021120	1023840	is almost always non-linear.
1023840	1027640	Imagine I told you to separate here the green points from the red points, but all you could
1027640	1030280	use is a single straight line.
1030280	1035480	You might think this is easy with multiple lines or curved lines, but you can only use
1035480	1038000	a single straight line.
1038000	1042040	And that's what using a neural network with a linear activation function would be like.
1042040	1046840	That makes the problem really hard, because no matter how deep the neural network is,
1046840	1052360	you'll only be able to produce a single line decision boundary, and you're only able to
1052360	1056080	separate your space with one line.
1056080	1062080	Now using non-linear activation functions allows your neural network to approximate arbitrarily
1062080	1063840	complex functions.
1063840	1069200	And that's what makes neural networks extraordinarily powerful.
1069200	1073520	Let's understand this with a simple example so that we can build up our intuition even
1073520	1075120	further.
1075120	1081120	Imagine I give you this trained network, now with weights on the left hand side, 3 and
1081120	1083120	negative 2.
1083120	1087040	This network only has two inputs, x1 and x2.
1087040	1091920	If we want to get the output of it, we simply do the same story as I said before.
1092280	1098400	First take a dot product of our inputs with our weights, add the bias, and apply a non-linearity.
1098400	1103760	But let's take a look at what's inside of that non-linearity.
1103760	1111480	It's simply a weighted combination of our inputs in the form of a two dimensional line,
1111480	1114680	because in this case we only have two inputs.
1114680	1119360	So if we want to compute this output, it's the same stories before, we take a dot product
1119360	1123640	of x and w, we add our bias, and apply our non-linearity.
1123640	1127120	What about what's inside of this non-linearity g?
1127120	1131040	Well, this is just a 2D line.
1131040	1135560	In fact, since it's just a two dimensional line, we can even plot it in two dimensional
1135560	1136560	space.
1136560	1138400	This is called the feature space, the input space.
1138400	1144200	In this case, the feature space and the input space are equal because we only have one neuron.
1144200	1147160	So in this plot, let me describe what you're seeing.
1147160	1150440	So on the two axes, you're seeing our two inputs.
1150440	1155640	So on one axis is x1, one of the inputs, on the other axis is x2, our other input.
1155640	1160120	And we can plot the line here, our decision boundary of this trained neural network that
1160120	1163920	I gave you, as a line in this space.
1163920	1168080	Now this line corresponds to actually all of the decisions that this neural network
1168080	1169960	can make.
1169960	1175600	Because if I give you a new data point, for example here I'm giving you negative 1, 2,
1175600	1181080	this point lies somewhere in the space, specifically at x1 equal to negative 1 and x2 equal to
1181080	1182080	2.
1182080	1184280	That's just a point in the space.
1184280	1191600	I want you to compute its weighted combination and I can actually follow the perceptron equation
1191600	1193560	to get the answer.
1193560	1199560	So here we can see that if we plug it into the perceptron equation, we get 1 plus minus
1199560	1202240	3 minus 4.
1202240	1204560	And the result would be minus 6.
1204560	1213400	We plug that into our nonlinear activation function g and we get a final output of 0.002.
1213400	1219520	Now in fact, remember that the sigmoid function actually divides this space into two parts
1219520	1223520	of either because it outputs everything between 0 and 1.
1223520	1231760	It's dividing it between 0.5 and greater than 0.5 and less than 0.5.
1231760	1239720	When the input is less than 0 and greater than 0.5, that's when the input is positive.
1239720	1243840	We can illustrate this space actually, but this feature space, when we're dealing with
1243840	1249400	a small dimensional data, like in this case we only have two dimensions.
1249400	1253880	But soon we'll start to talk about problems where we have thousands or millions or in
1253880	1258840	some cases even billions of weights in our neural network.
1258840	1264880	And then drawing these types of plots becomes extremely challenging and not really possible
1264880	1265880	anymore.
1265880	1269920	But at least when we're in this regime of small number of inputs and small number of
1269920	1273960	weights, we can make these plots to really understand the entire space.
1273960	1279520	And for any new input that we obtain, for example an input right here, we can see exactly
1279520	1287040	that this point is going to be having an activation function less than 0 and its output will be
1287040	1288720	less than 0.5.
1288720	1293320	The magnitude of that actually is computed by plugging it into the perceptron equation.
1293320	1297800	So we can't avoid that, but we can immediately get an answer on the decision boundary, depending
1297800	1304480	on which side of this hyperplane that we lie on when we plug it in.
1304480	1309440	So now that we have an idea of how to build a perceptron, let's start by building neural
1309440	1313960	networks and seeing how they all come together.
1313960	1318180	So let's revisit that diagram of the perceptron that I showed you before.
1318180	1322740	If there's only a few things that you get from this class, I really want everyone to
1322740	1325540	take away how a perceptron works.
1325540	1327900	And there's three steps, remember them always.
1327900	1332980	The dot product, you take a dot product of your inputs and your weights.
1332980	1336700	You add a bias and you apply your non-linearity.
1336700	1339660	There's three steps.
1339660	1341940	Let's simplify this diagram a little bit.
1341940	1345060	Let's clean up some of the arrows and remove the bias.
1345060	1350500	And we can actually see now that every line here has its own associated weight to it.
1350500	1354100	And I'll remove the bias term, like I said, for simplicity.
1354100	1360740	Note that z here is the result of that dot product plus bias, before we apply the activation
1360740	1363140	function, though, g.
1363140	1368660	The final output, though, is simply y, which is equal to the activation function of z, which
1368660	1371580	is our activation value.
1372580	1378180	Now, if we want to define a multi-output neural network, we can simply add another
1378180	1380020	perceptron to this picture.
1380020	1384540	So instead of having one perceptron, now we have two perceptrons and two outputs.
1384540	1389860	Each one is a normal perceptron, exactly like we saw before, taking its inputs from each
1389860	1397060	of the x1's through xm's, taking the dot product, adding a bias, and that's it.
1397060	1398260	Now we have two outputs.
1398260	1401940	Each of those perceptrons, though, will have a different set of weights.
1401940	1402940	Remember that.
1402940	1405660	We'll get back to that.
1405660	1410980	If we want, so actually one thing to keep in mind here is because all the inputs are
1410980	1417580	densely connected, every input has a connection to the weights of every perceptron.
1417580	1422140	These are often called dense layers, or sometimes fully connected layers.
1422140	1427540	Now, through this class, you're going to get a lot of experience actually coding up
1427580	1433540	and practically creating some of these algorithms using a software toolbox called TensorFlow.
1433540	1439020	So now that we have the understanding of how a single perceptron works and how a dense
1439020	1444860	layer works, this is a stack of perceptrons, let's try and see how we can actually build
1444860	1449340	up a dense layer like this all the way from scratch.
1449340	1454580	To do that, we can actually start by initializing the two components of our dense layer, which
1454620	1457620	are the weights and the biases.
1457620	1462500	Now that we have these two parameters of our neural network, of our dense layer, we can
1462500	1466940	actually define the forward propagation of information, just like we saw it and learned
1466940	1468500	about already.
1468500	1473540	That forward propagation of information is simply the dot product or the matrix multiplication
1473540	1480540	of our inputs with our weights, add a bias, that gives us our activation function here,
1481380	1486380	and then we apply this nonlinearity to compute the output.
1486380	1492380	Now, TensorFlow has actually implemented this dense layer for us.
1492380	1497140	So we don't need to do that from scratch, instead we can just call it like shown here.
1497140	1504140	So to create a dense layer with two outputs, we can specify this units equal to two.
1504140	1508380	Now let's take a look at what's called a single layered neural network.
1508380	1513860	This is one we have a single hidden layer between our inputs and our outputs.
1513860	1519140	This layer is called the hidden layer, because unlike an input layer and an output layer,
1519140	1524260	the states of this hidden layer are typically unobserved, they're hidden to some extent,
1524260	1526820	they're not strictly enforced either.
1526820	1531060	And since we have this transformation now from the input layer to the hidden layer and
1531060	1536660	from the hidden layer to the output layer, each of these layers are going to have their
1536740	1539700	own specified weight matrices.
1539700	1545660	We'll call w1 the weight matrices for the first layer and w2 the weight matrix for the
1545660	1548580	second layer.
1548580	1555580	If we take a zoomed in look at one of the neurons in this hidden layer, let's take
1555580	1561540	for example z2 for example, this is the exact same perceptron that we saw before.
1561540	1567700	We can compute its output, again using the exact same story, taking all of its inputs
1567700	1572820	x1 through xm, applying a dot product with the weights, adding a bias and that gives
1572820	1574860	us z2.
1574860	1580340	If we look at a different neuron, let's suppose z3, we'll get a different value here because
1580340	1586100	the weights leading to z3 are probably different than those leading to z2.
1586100	1592860	Now this picture looks a bit messy, so let's try and clean things up a bit more.
1592860	1598020	From now on, I'll just use this symbol here to denote what we call this dense layer or
1598020	1600340	fully connected layers.
1600340	1606820	And here you can actually see an example of how we can create this exact neural network
1606820	1611220	again using TensorFlow with the predefined dense layer notation.
1611220	1615980	Here we're creating a sequential model where we can stack layers on top of each other.
1615980	1624300	This layer with n neurons and the second layer with 2 neurons, the output layer.
1624300	1629020	And if we want to create a deep neural network, all we have to do is keep stacking these layers
1629020	1634940	to create more and more hierarchical models, ones where the final output is computed by
1634940	1638620	going deeper and deeper into the network.
1638620	1644060	And to implement this in TensorFlow again, it's very similar as we saw before, again
1644060	1650540	using the TFKARIS sequential call, we can stack each of these dense layers on top of
1650540	1657260	each other, each one specified by the number of neurons in that dense layer, n1 and 2,
1657260	1662940	but with the last output layer fixed to 2 outputs, if that's how many outputs we have.
1662940	1665620	Okay, so that's awesome.
1665620	1672100	Now we have an idea of not only how to build up a neural network directly from a perceptron,
1672100	1676660	but how to compose them together to form complex deep neural networks.
1676660	1682180	Let's take a look at how we can actually apply them to a very real problem that I believe
1682180	1687140	all of you should care very deeply about.
1687140	1692060	Here's a problem that we want to build an AI system to learn to answer.
1692060	1694300	Will I pass this class?
1694300	1697860	And we can start with a simple two feature model.
1697860	1701780	One feature, let's say, is the number of lectures that you attend as part of this class.
1701780	1707060	And the second feature is the number of hours that you spend working on your final project.
1707060	1712860	You do have some training data from all of the past participants of Success191.
1712860	1716460	And we can plot this data on this feature space like this.
1716460	1721980	The green points here actually indicate students, so each point is one student that has passed
1721980	1727060	the class, and the red points are students that have failed the class.
1727060	1732260	You can see where they are in this feature space depends on the actual number of hours
1732260	1735700	that they attended the lecture, the number of lectures they attended, and the number
1735700	1739260	of hours they spent on the final project.
1739260	1740340	And then there's you.
1740340	1746340	You have attended four lectures, and you have spent five hours on your final project.
1746340	1754460	And you want to understand how can you build a neural network given everyone else in this
1754460	1755740	class?
1755740	1762140	Will you pass or fail this class based on the training data that you see?
1762140	1763140	So let's do it.
1763140	1767300	We have now all of the requirements to do this now.
1767300	1773060	So let's build a neural network with two inputs, x1 and x2, with x1 being the number
1773060	1778340	of lectures that we attend, x2 is the number of hours you spend on your final project.
1778340	1784500	We'll have one hidden layer with three units, and we'll feed those into a final probability
1784500	1791980	output by passing this class, and we can see that the probability that we pass is 0.1,
1791980	1793740	or 10%.
1793740	1800940	That's not great, but the reason is because that this model was never actually trained.
1800940	1804420	It's basically just a baby.
1804420	1806220	It's never seen any data.
1806220	1808900	Even though you have seen the data, it hasn't seen any data.
1808900	1814260	And more importantly, you haven't told the model how to interpret this data.
1814260	1816420	It needs to learn about this problem first.
1816420	1822060	It knows nothing about this class or final projects or any of that.
1822060	1826140	So one of the most important things to do this is actually you have to tell the model
1826140	1832700	when it is making bad predictions in order for it to be able to correct itself.
1832700	1835820	Now the loss of a neural network actually defines exactly this.
1835820	1841140	It defines how wrong a prediction was.
1841140	1845580	So it takes as input the predicted outputs and the ground truth outputs.
1845580	1850140	Now if those two things are very far apart from each other, then the loss will be very
1850140	1851540	large.
1851540	1856300	On the other hand, the closer these two things are from each other, the smaller the loss,
1856300	1859500	and the more accurate the loss the model will be.
1859500	1861580	So we always want to minimize the loss.
1861580	1866940	We want to incur, we want to predict something that's as close as possible to the ground
1866940	1869740	truth.
1869740	1874900	Now let's assume we have not just the data from one student, but as we have in this
1874900	1877260	case the data from many students.
1877260	1883100	We now care about not just how the model did on predicting just one prediction, but how
1883100	1886580	it did on average across all of these students.
1886580	1889260	This is what we call the empirical loss.
1889260	1894820	And it's simply just the mean or the average of every loss from each individual example
1894820	1898460	or each individual student.
1898460	1903340	When training a neural network, we want to find a network that minimizes the empirical
1903340	1910580	loss between our predictions and the true outputs.
1910580	1916660	Now if we look at the problem of binary classification, where the neural network like we want to do
1916660	1922180	in this case, is supposed to answer either yes or no, one or zero.
1922180	1926740	We can use what is called a softmax cross entropy loss.
1926740	1935500	Now the softmax cross entropy loss is actually written out here and it's defined by what's
1935500	1938980	called the cross entropy between two probability distributions.
1938980	1945060	It measures how far apart the ground truth probability distribution is from the predicted
1945060	1948980	probability distribution.
1948980	1953900	Let's suppose instead of predicting binary outputs, will I pass this class or will I
1953900	1955980	not pass this class?
1955980	1962380	Instead you want to predict the final grade as a real number, not a probability or as
1962380	1969700	a percentage, we want the grade that you will get in this class.
1969700	1974540	Now in this case, because the type of the output is different, we also need to use a
1974540	1979220	different loss here, because our outputs are no longer 0, 1, but they can be any real
1979220	1980220	number.
1980220	1985300	They're just the grade that you're going to get on the final class.
1985300	1991100	So for example, here since this is a continuous variable, the grade, we want to use what's
1991100	1992700	called the mean squared error.
1992700	1998660	This measures just the squared error, the squared difference between our ground truth
1998660	2005180	and our predictions, again averaged over the entire data set.
2005180	2011340	Okay great, so now we've seen two loss functions, one for classification, binary outputs, as
2011340	2018100	well as regression, continuous outputs, and the problem now I think that we need to start
2018100	2020820	asking ourselves is how can we take that loss function?
2020820	2024340	We've seen our loss function, we've seen our network, now we have to actually understand
2024340	2026420	how can we put those two things together?
2026420	2031100	How can we use our loss function to train the weights of our neural network such that
2031100	2033700	it can actually learn that problem?
2033700	2038980	Well, what we want to do is actually find the weights of the neural network that will
2038980	2041580	minimize the loss of our data set.
2041580	2048380	That essentially means that we want to find the W's in our neural network that minimize
2048380	2049380	J of W.
2049380	2054900	J of W's are empirical cost function that we saw in the previous slides that average loss
2054900	2058660	over each data point in the data set.
2058660	2065500	Now, remember that W, capital W, is simply a collection of all of the weights in our
2065500	2068940	neural network, not just from one layer, but from every single layer.
2068940	2073540	So that's W0 from the zeroth layer to the first layer to the second layer, all concatenate
2073540	2074540	into one.
2074540	2081220	In this optimization problem we want to optimize all of the W's to minimize this empirical loss.
2082220	2087260	Now, remember our loss function is just a simple function of our weights.
2087260	2093220	If we have only two weights, we can actually plot this entire loss landscape over this
2093220	2094220	grid of weights.
2094220	2098580	So on the one axis on the bottom you can see weight number one and the other one you can
2098580	2101420	see weight zero.
2101420	2104300	There's only two weights in this neural network, very simple neural network.
2104300	2110180	So we can actually plot for every W0 and W1, what is the loss?
2110220	2116940	What is the error that we'd expect to see and obtain from this neural network?
2116940	2122620	Now the whole process of training a neural network, optimizing it, is to find the lowest
2122620	2129580	point in this loss landscape that will tell us our optimal W0 and W1.
2129580	2130740	Now how can we do that?
2130740	2134540	The first thing we have to do is pick a point.
2134540	2137900	So let's pick any W0, W1.
2137940	2144340	Starting from this point we can compute the gradient of the landscape at that point.
2144340	2151540	Now the gradient tells us the direction of highest or steepest ascent.
2151540	2153780	So that tells us which way is up.
2153780	2158220	If we compute the gradient of our loss with respect to our weights, that's the derivative
2158220	2162140	of our gradient for loss with respect to the weights, that tells us the direction of which
2162140	2167580	way is up on that loss landscape from where we stand right now.
2167580	2170740	Instead of going up though, we want to find the lowest loss.
2170740	2178220	So let's take the negative of our gradient and take a small step in that direction.
2178220	2181660	This will move us a little bit closer to the lowest point.
2181660	2182900	And we just keep repeating this.
2182900	2188420	Now we compute the gradient at this point and repeat the process until we converge.
2188420	2191460	And we will converge to a local minimum.
2191460	2195700	We don't know if it will converge to a global minimum, but at least we know that it should
2195700	2198500	in theory converge to a local minimum.
2198500	2202460	Now we can summarize this algorithm as follows.
2202460	2205820	This algorithm is also known as gradient descent.
2205820	2212140	So we start by initializing all of our weights randomly and we loop until convergence.
2212140	2215980	We start from one of those weights, our initial point.
2215980	2217420	We compute the gradient.
2217420	2219180	That tells us which way is up.
2219180	2221300	So we take a step in the opposite direction.
2221300	2223740	We take a small step here.
2223740	2230020	All is computed by multiplying our gradient by this factor eta.
2230020	2232100	And we'll learn more about this factor later.
2232100	2234060	This factor is called the learning rate.
2234060	2235620	We'll learn more about that later.
2235620	2241580	Now again, in TensorFlow, we can actually see this pseudocode of gradient descent algorithm
2241580	2243580	written out in code.
2243580	2246620	We can randomize all of our weights.
2246620	2251060	That basically initializes our search, our optimization process at some point in space.
2251460	2253620	Then we keep looping over and over and over again.
2253620	2258100	We compute the loss, we compute the gradient, and we take a small step of our weights in
2258100	2261500	the direction of that gradient.
2261500	2264260	But now let's take a look at this term here.
2264260	2268260	This is how we actually compute the gradient.
2268260	2273220	This explains how the loss is changing with respect to the weight.
2273220	2276740	But I never actually told you how we compute this.
2276740	2282100	So let's talk about this process, which is actually extremely important in training neural
2282100	2283100	networks.
2283100	2286900	It's known as back propagation.
2286900	2288340	So how does back propagation work?
2288340	2290420	How do we compute this gradient?
2290420	2292380	Let's start with a very simple neural network.
2292380	2295700	This is probably the simplest neural network in existence.
2295700	2301380	It only has one input, one hidden neuron, and one output.
2301380	2306900	Taking the gradient of our loss, j of w, with respect to one of the weights, in this
2306900	2316660	case just w2, for example, tells us how much a small change in w2 is going to affect our
2316660	2318180	loss, j.
2318180	2324660	So if we move around j, infinitesimally small, how will that affect our loss?
2324660	2330100	That's what the gradient is going to tell us, derivative of j of w2.
2330100	2335860	So if we write out this derivative, we can actually apply the chain rule to actually
2335860	2337020	compute it.
2337020	2338700	So what does that look like?
2338700	2351980	Specifically, we can decompose that derivative into the derivative of j dw over dy multiplied
2351980	2357500	by derivative of our output with respect to w2.
2357500	2360820	Now the question here is with the second part.
2360820	2366380	If we want to compute now not the derivative of our loss with respect to w2, but now the
2366380	2370580	loss with respect to w1, we can do the same story as before.
2370580	2375260	We can apply the chain rule now recursively.
2375260	2379540	So now we have to apply the chain rule again to the second part.
2379540	2382780	Now the second part is expanded even further.
2382780	2388620	So the derivative of our output with respect to z1, which is the activation function of
2388620	2390660	this first hidden unit.
2390660	2392700	And we can back propagate this information now.
2392700	2399140	You can see starting from our loss all the way through w2 and then recursively applying
2399140	2402580	this chain rule again to get to w1.
2402580	2408340	And this allows us to see both the gradient at both w2 and w1.
2408340	2415860	So in this case, just to reiterate once again, this is telling us this dj dw1 is telling
2415860	2421120	us how a small change in our weight is going to affect our loss.
2421120	2425420	So we can see if we increase our weight a small amount, it will increase our loss.
2425420	2429380	That means we will want to decrease the weight to decrease our loss.
2429380	2430820	That's what the gradient tells us.
2430820	2438180	Which direction we need to step in order to decrease or increase our loss function.
2438180	2441660	Now we showed this here for just two weights in our neural network because we only have
2441660	2442660	two weights.
2442660	2445580	But imagine we have a very deep neural network.
2445580	2451380	One with more than just two layers of or one layer rather of hidden units.
2451380	2456980	We can just repeat this process of applying, recursively applying the chain rule to determine
2456980	2462540	how every single way in the model needs to change to impact that loss.
2462540	2467420	But really all this boils down to just recursively applying this chain rule formulation that
2467420	2470620	you can see here.
2470620	2472860	And that's the back propagation algorithm.
2472860	2475220	In theory it sounds very simple.
2475220	2482500	It's just a very basic extension on derivatives and the chain rule.
2482500	2487900	But now let's actually touch on some insights from training these networks in practice that
2487900	2492340	make this process much more complicated in practice.
2492340	2497540	And why using back propagation as we saw there is not always so easy.
2497540	2502860	Now in practice training neural networks and optimization of networks can be extremely
2502860	2507420	difficult and it's actually extremely computationally intensive.
2507420	2514420	Here's the visualization of what a loss landscape of a real neural network can look like visualized
2514420	2516900	on just two dimensions.
2516900	2523380	Now you can see here that the loss is extremely non-convex meaning that it has many, many
2523380	2525500	local minimum.
2525500	2531020	That can make using an algorithm like gradient descent very, very challenging because gradient
2531020	2535620	descent is always going to step closest to the first local minimum but it can always
2535620	2536780	get stuck there.
2536780	2542140	So finding how to get to the global minima or a really good solution for your neural
2542140	2548860	network can often be very sensitive to your hyper parameter such as where the optimizer
2548860	2551140	starts in this loss landscape.
2551140	2556060	If it starts in a potentially bad part of the landscape it can very easily get stuck
2556060	2559820	in one of these local minimum.
2559820	2564580	Now recall the equation that we talked about for gradient descent.
2564580	2565940	This was the equation I showed you.
2565940	2572220	Our next weight update is going to be your current weights minus a small amount called
2572220	2574500	the learning rate multiplied by the gradient.
2574500	2580220	So we have this minus sign because we want to step in the opposite direction and we multiply
2580220	2584860	it by the gradient or we multiply it by the small number called here called eta which
2584860	2588380	is what we call the learning rate.
2588380	2591180	How fast do we want to do the learning?
2591180	2595140	Now it determines actually not just how fast to do the learning that's maybe not the best
2595180	2602300	way to say it but it tells us how large should each step we take in practice be with regards
2602300	2603620	to that gradient.
2603620	2607940	So the gradient tells us the direction but it doesn't necessarily tell us the magnitude
2607940	2609300	of the direction.
2609300	2616180	So eta can tell us actually a scale of how much we want to trust that gradient and step
2616180	2618300	in the direction of that gradient.
2618300	2623540	In practice setting even eta, this one parameter, this one number can be extremely difficult
2623540	2627180	and I want to give you a quick example of why.
2627180	2634700	So if you have a very non-convex or loss landscape where you have local minima, if you set the
2634700	2639580	learning rate too low then the model can get stuck in these local minima.
2639580	2644620	It can never escape them because it actually does optimize itself but it optimizes it to
2644620	2652300	a very non-optimal minima and it can converge very slowly as well.
2652300	2657660	On the other hand if we increase our learning rate too much then we can actually overshoot
2657660	2665580	our minima and actually diverge and lose control and basically explode the training process
2665580	2666780	completely.
2666780	2673580	One of the challenges is actually how to use stable learning rates that are large enough
2673580	2681780	to avoid the local minima but small enough so that they don't diverge completely.
2682420	2689940	So they're small enough to actually converge to that global spot once they reach it.
2689940	2691940	So how can we actually set this learning rate?
2691940	2697700	Well one option which is actually somewhat popular in practice is to actually just try
2697700	2701620	a lot of different learning rates and that actually works.
2701620	2706540	It is a feasible approach but let's see if we can do something a little bit smarter than
2706540	2708300	that, more intelligent.
2708340	2714020	What if we could say instead how can we build an adaptive learning rate that actually looks
2714020	2720060	at its lost landscape and adapts itself to account for what it sees in the landscape.
2720060	2724020	There are actually many types of optimizers that do exactly this.
2724020	2726980	This means that the learning rates are no longer fixed.
2726980	2732140	They can increase or decrease depending on how large the gradient is in that location
2732260	2739260	and how fast we want and how fast we're actually learning and many other options.
2739260	2744260	They could be also with regards to the size of the weights at that point, the magnitudes, etc.
2746260	2752580	In fact these have been widely explored and published as part of TensorFlow as well and
2752580	2756420	during your labs we encourage each of you to really try out each of these different types
2756420	2761940	of optimizers and experiment with their performance in different types of problems so that you
2761940	2767900	can gain very important intuition about when to use different types of optimizers or what
2767900	2772900	their advantages are and disadvantages in certain applications as well.
2774900	2778100	Let's try and put all of this together.
2778100	2785500	Here we can see a full loop of using TensorFlow to define your model on the first line, define
2785500	2786740	your optimizer.
2786740	2790060	Here you can replace this with any optimizer that you want.
2790180	2794940	Here I'm just using stochastic gradient descent like we saw before.
2794940	2797260	Feeding it through the model we loop forever.
2797260	2799340	We're doing this forward prediction.
2799340	2800780	We predict using our model.
2800780	2803460	We compute the loss with our prediction.
2803460	2809100	This is exactly the loss is telling us again how incorrect our prediction is with respect
2809100	2811700	to the ground truth why.
2811700	2818500	We compute the gradient of our loss with respect to each of the weights in our neural network.
2818540	2826020	Then finally we apply those gradients using our optimizer to step and update our weights.
2826020	2830660	This is really taking everything that we've learned in the class and lecture so far and
2830660	2838820	applying it into one whole piece of code written in TensorFlow.
2838820	2844460	So I want to continue this talk and really talk about tips for training these networks
2844500	2851380	in practice now that we can focus on this very powerful idea of batching your data into
2851380	2853580	mini batches.
2853580	2859340	So before we saw it with gradient descent that we have the following algorithm.
2859340	2865100	This gradient that we saw to compute using back propagation can be actually very intensive
2865100	2870540	to compute especially if it's computed over your entire training set.
2870620	2874980	This is a summation over every single data point in the entire data set and most real
2874980	2876820	life applications.
2876820	2881660	It is simply not feasible to compute this on every single iteration in your optimization
2881660	2883780	loop.
2883780	2888780	Alternatively let's consider a different variant of this algorithm called stochastic gradient
2888780	2889780	descent.
2889780	2894380	So instead of computing the gradient over our entire data set let's just pick a single
2894380	2899980	point compute the gradient of that single point with respect to the weights and then
2899980	2903820	update all of our weights based on that gradient.
2903820	2908660	So this has some advantages this is very easy to compute because it's only using one data
2908660	2914940	point now it's very fast but it's also very noisy because it's only from one data point.
2914940	2920820	Instead there's a middle ground instead of computing this noisy gradient of a single
2920820	2928740	point let's get a better estimate of our gradient by using a batch of b data points.
2928740	2934740	So now let's pick a batch of b data points and we'll compute the gradient estimate simply
2934740	2937700	as the average over this batch.
2937700	2942780	So since b here is usually not that large on the order of tens or hundreds of samples
2942780	2948820	this is much much faster to compute than regular gradient descent and it's also much much more
2948820	2956940	accurate than purely stochastic gradient descent that only uses a single example.
2956940	2962380	Now this increases the gradient accuracy estimation which also allows us to converge much more
2962380	2967660	smoothly it also means that we can trust our gradient more than in stochastic gradient
2967660	2974940	descent so that we can actually increase our learning rate a bit more as well.
2974940	2980940	Mini batching also leads to massively parallelizable computation we can split up the batches on
2980940	2986380	separate workers and separate machines and thus achieve even more parallelization and
2986380	2990140	speed increases on our GPUs.
2990140	2994420	Now the last topic I want to talk about is that of overfitting this is also known as
2994420	3001020	the problem of generalization and is one of the most fundamental problems in all of machine
3001020	3005660	learning and not just deep learning.
3005660	3011940	Now overfitting like I said is critical to understand so I really want to make sure that
3011940	3017780	this is a clear concept in everyone's mind ideally in machine learning we want to learn
3017780	3023420	a model that accurately describes our test data not the training data even though we're
3023420	3028140	optimizing this model based on the training data what we really want is for it to perform
3028140	3031460	well on the test data.
3031460	3038140	So said differently we want to build representations that can learn from our training data but
3038140	3042300	still generalize well to unseen test data.
3042300	3049380	Now assume you want to build a line to describe these points underfitting means that the model
3049380	3056140	does simply not have enough capacity to represent these points so no matter how good we try
3056140	3061340	to fit this model it simply does not have the capacity to represent this type of data.
3061340	3065420	On the far right hand side we can see the extreme other extreme where here the model
3065420	3072620	is too complex it has too many parameters and it does not generalize well to new data.
3072620	3076740	In the middle though we can see what's called the ideal fit it's not overfitting it's not
3076740	3083100	underfitting but it has a medium number of parameters and it's able to fit in a generalizable
3083100	3089300	way to the output and is able to generalize well to brand new data when it sees it at
3089300	3092020	test time.
3092020	3097780	Now to address this problem let's talk about regularization how can we make sure that our
3097780	3103020	models do not end up overfit because neural networks do have a ton of parameters how can
3103020	3107540	we enforce some form of regularization to them.
3107540	3109540	Now what is regularization?
3109540	3113580	Regularization is a technique that constrains our optimization problem such that we can
3113580	3119820	discourage these complex models from actually being learned and overfit.
3119820	3121420	So again why do we need it?
3121420	3126620	We need it so that our model can generalize to this unseen data set and in neural networks
3126620	3133020	we have many techniques for actually imposing regularization onto the model.
3133020	3137100	One very common technique and very simple to understand is called dropout.
3137100	3142620	This is one of the most popular forms of regularization in deep learning and it's very simple.
3142620	3145660	Let's revisit this picture of a neural network.
3145660	3152300	This is a two-layered neural network, two hidden layers and in dropout during training
3152300	3159220	all we simply do is randomly set some of the activations here to zero with some probability.
3159220	3167060	So what we can do is let's say we pick our probability to be 50% or 0.5 we can drop randomly
3167060	3171900	for each of the activations 50% of those neurons.
3171900	3176340	This is extremely powerful as it lowers the capacity of our neural network so that they
3176340	3182580	have to learn to perform better on test sets because sometimes on training sets it just
3182580	3185220	simply cannot rely on some of those parameters.
3185220	3189900	So it has to be able to be resilient to that kind of dropout.
3189900	3197100	It also means that they're easier to train because at least on every forward pass of
3197100	3202100	iterations we're training only 50% of the weights and only 50% of the gradients.
3202100	3207900	So that also cuts our gradient computation time down by a factor of two.
3207900	3213740	So because now we only have to compute half the number of neuron gradients.
3213740	3218220	Now on every iteration we dropped out on the previous iteration 50% of neurons but on the
3218220	3225700	next iteration we're going to drop out a different set of neurons.
3225700	3230500	And this gives the network, it basically forces the network to learn how to take different
3230500	3237180	pathways to get to its answer and it can't rely on any one pathway too strongly and overfit
3237180	3238180	to that pathway.
3238180	3242760	This is a way to really force it to generalize to this new data.
3242760	3248900	The second regularization technique that we'll talk about is this notion of early stopping.
3248900	3252180	And again here the idea is very basic.
3252180	3258100	It's basically let's stop training once we realize that our loss is increasing on a
3258100	3262820	held out validation or let's call it a test set.
3262820	3267980	So when we start training we all know the definition of overfitting is when our model
3267980	3270340	starts to perform worse on the test set.
3270340	3276380	So if we set aside some of this training data to be quote unquote test data we can monitor
3276380	3281380	how our network is learning on this data and simply just stop before it has a chance
3281380	3282900	to overfit.
3282900	3286420	So on the x-axis you can see the number of training iterations and on the y-axis you
3286420	3291740	can see the loss that we get after training that number of iterations.
3291740	3295500	So as we continue to train in the beginning both lines continue to decrease.
3295500	3301220	This is as we'd expect and this is excellent since it means our model is getting stronger.
3301220	3306540	Eventually though the network's testing loss plateaus and starts to increase.
3306540	3311260	Note that the training accuracy will always continue to go down as long as the network
3311260	3317420	has the capacity to memorize the data and this pattern continues for the rest of training.
3317420	3321020	So it's important here to actually focus on this point here.
3321020	3325220	This is the point where we need to stop training and after this point assuming that our test
3325220	3330940	set is a valid representation of the true test set the accuracy of the model will only
3330940	3331940	get worse.
3331940	3336180	So we can stop training here take this model and this should be the model that we actually
3336180	3340500	use when we deploy into the real world.
3340500	3344220	Anything any model taken from the left hand side is going to be underfit is not going
3344220	3348660	to be utilizing the full capacity of the network and anything taken from the right hand side
3348660	3356300	is overfit and actually performing worse than it needs to on that held out test set.
3356300	3362420	So I'll conclude this lecture by summarizing three key points that we've covered so far.
3362420	3367340	We started about the fundamental building blocks of neural networks the perceptron.
3367340	3372780	We learned about stacking and composing these perceptrons together to form complex hierarchical
3372780	3379100	neural networks and how to mathematically optimize these models with back propagation.
3379100	3383940	And finally we address the practical side of these models that you'll find useful for
3383940	3389660	the labs today including adaptive learning rates, batching and regularization.
3389660	3394180	So thank you for attending the first lecture in 6S191.
3394180	3394860	Thank you very much.
