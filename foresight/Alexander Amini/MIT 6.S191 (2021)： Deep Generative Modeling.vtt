WEBVTT

00:00.000 --> 00:15.440
Hi everyone, and welcome to lecture 4 of MIT 6S191.

00:15.440 --> 00:19.640
In today's lecture, we're going to be talking about how we can use deep learning and neural

00:19.640 --> 00:24.800
networks to build systems that not only look for patterns in data, but actually can go

00:24.800 --> 00:31.760
a step beyond this to generate brand new synthetic examples based on those learned patterns.

00:31.760 --> 00:36.840
And this, I think, is an incredibly powerful idea, and it's a particular subfield of deep

00:36.840 --> 00:42.040
learning that has enjoyed a lot of success and gotten a lot of interest in the past couple

00:42.040 --> 00:47.520
of years, but I think there's still tremendous, tremendous potential of this field of deep

00:47.520 --> 00:52.180
generative modeling in the future and in the years to come, particularly as we see these

00:52.180 --> 00:57.540
types of models and the types of problems that they tackle becoming more and more relevant

00:57.540 --> 01:00.540
in a variety of application areas.

01:00.540 --> 01:08.000
All right, so to get started, I'd like to consider a quick question for each of you.

01:08.000 --> 01:13.500
Here we have three photos of faces, and I want you all to take a moment, look at these

01:13.500 --> 01:18.900
faces, study them, and think about which of these faces you think is real.

01:18.900 --> 01:20.740
Is it the face on the left?

01:20.740 --> 01:23.140
Is it the face in the center?

01:23.140 --> 01:25.300
Is it the face on the right?

01:25.300 --> 01:27.500
Which of these is real?

01:27.500 --> 01:33.340
Well, in truth, each of these faces are not real.

01:33.340 --> 01:34.460
They are all fake.

01:34.460 --> 01:40.040
These are all images that were synthetically generated by a deep neural network.

01:40.040 --> 01:43.660
None of these people actually exist in the real world.

01:43.660 --> 01:50.160
And hopefully, I think you all have appreciated the realism of each of these synthetic images,

01:50.160 --> 01:54.800
and this to me highlights the incredible power of deep generative modeling.

01:54.800 --> 01:57.960
And not only does it highlight the power of these types of algorithms and these types

01:57.960 --> 02:04.560
of models, but it raises a lot of questions about how we can consider the fair use and

02:04.560 --> 02:10.600
the ethical use of such algorithms as they are being deployed in the real world.

02:10.600 --> 02:17.480
So by setting this up and motivating in this way, I'd first, I now like to take a step

02:17.480 --> 02:23.080
back and consider fundamentally what is the type of learning that can occur when we are

02:23.080 --> 02:27.340
training neural networks to perform tasks such as these.

02:27.340 --> 02:32.520
So so far in this course, we've been considering what we call supervised learning problems,

02:32.520 --> 02:38.760
instances in which we are given a set of data and a set of labels associated with that data.

02:38.760 --> 02:44.240
And our goal is to learn a functional mapping that moves from data to labels.

02:44.240 --> 02:47.720
And those labels can be class labels or continuous values.

02:47.720 --> 02:53.120
And in this course, we've been concerned primarily with developing these functional

02:53.120 --> 02:57.200
mappings that can be described by deep neural networks.

02:57.200 --> 03:03.880
But at their core, these mappings could be anything, you know, any sort of statistical function.

03:03.880 --> 03:09.120
The topic of today's lecture is going to focus on what we call unsupervised learning, which

03:09.120 --> 03:12.720
is a new class of learning problems.

03:12.720 --> 03:17.600
And in contrast to supervised settings where we're given data and labels in unsupervised

03:17.600 --> 03:20.800
learning, we're given only data, no labels.

03:20.800 --> 03:25.400
And our goal is to train a machine learning or deep learning model to understand or build

03:25.400 --> 03:30.640
up a representation of the hidden and underlying structure in that data.

03:30.640 --> 03:36.760
And what this can do is it can allow sort of an insight into the foundational structure

03:36.760 --> 03:38.080
of the data.

03:38.080 --> 03:44.960
And then in turn, we can use this understanding to actually generate synthetic examples.

03:44.960 --> 03:50.720
And unsupervised learning beyond this domain of deep generative modeling also extends

03:50.720 --> 03:56.480
to other types of problems and example applications, which you may be familiar with, such as clustering

03:56.480 --> 04:00.880
algorithms or dimensionality reduction algorithms.

04:00.880 --> 04:04.640
Generative modeling is one example of unsupervised learning.

04:04.640 --> 04:12.040
And our goal in this case is to take as input examples from a training set and learn a model

04:12.040 --> 04:18.440
that represents the distribution of the data that is input to that model.

04:18.440 --> 04:21.280
And this can be achieved in two principal ways.

04:21.280 --> 04:25.600
The first is through what is called density estimation, where let's say we are given a

04:25.600 --> 04:31.080
set of data samples and they fall according to some density.

04:31.120 --> 04:37.840
The task for building a deep generative model applied to these samples is to learn the underlying

04:37.840 --> 04:45.240
probability density function that describes how and where these data fall along this distribution.

04:45.240 --> 04:51.480
And we can not only just estimate the density of such a probability density function, but

04:51.480 --> 04:58.320
actually use this information to generate new synthetic samples, where again we are considering

04:58.320 --> 05:04.480
some input examples that fall and are drawn from some training data distribution.

05:04.480 --> 05:11.480
And after building up a model using that data, our goal is now to generate synthetic examples

05:11.480 --> 05:18.920
that can be described as falling within the data distribution modeled by our model.

05:18.920 --> 05:27.840
So the key idea in both these instances is this question of how can we learn a probability

05:27.880 --> 05:36.160
distribution using our model, which we call P model of X, that is so similar to the true

05:36.160 --> 05:43.760
data distribution, which we call P data of X. This will not only enable us to effectively

05:43.760 --> 05:49.440
estimate these probability density functions, but also generate new synthetic samples that

05:49.440 --> 05:56.000
are realistic and match the distribution of the data we're considering.

05:56.000 --> 06:04.080
So this I think summarizes concretely what are the key principles behind generative modeling.

06:04.080 --> 06:10.840
But to understand that how generative modeling may be informative and also impactful, let's

06:10.840 --> 06:17.200
take this idea step further and consider what could be potential impactful applications

06:17.200 --> 06:21.040
and real world use cases of generative modeling.

06:21.040 --> 06:27.640
What generative models enable us as the users to do is to automatically uncover the underlying

06:27.640 --> 06:30.480
structure and features in a data set.

06:30.480 --> 06:35.680
The reason this can be really important and really powerful is often we do not know how

06:35.680 --> 06:40.480
those features are distributed within a particular data set of interest.

06:40.480 --> 06:45.840
So let's say we're trying to build up a facial detection classifier and we're given a data

06:45.840 --> 06:52.080
set of faces, for which we may not know the exact distribution of these faces with respect

06:52.080 --> 06:58.080
to key features like skin tone or pose or clothing items.

06:58.080 --> 07:05.680
And without going through our data set and manually inspecting each of these instances,

07:05.680 --> 07:10.640
our training data may actually be very biased with respect to some of these features without

07:10.640 --> 07:12.960
us even knowing it.

07:12.960 --> 07:19.160
And as you'll see in this lecture and in today's lab, what we can actually do is train generative

07:19.160 --> 07:25.880
models that can automatically learn the landscape of the features in a data set like these,

07:25.880 --> 07:27.600
like that of faces.

07:27.600 --> 07:33.680
And by doing so, actually uncover the regions of the training distribution that are underrepresented

07:33.680 --> 07:39.400
and overrepresented with respect to particular features such as skin tone.

07:39.400 --> 07:45.640
And the reason why this is so powerful is we can actually now use this information to

07:45.640 --> 07:52.280
actually adjust how the data is sampled during training to ultimately build up a more fair

07:52.280 --> 07:59.000
and more representative data set that then will lead to a more fair and unbiased model.

07:59.000 --> 08:03.600
And you'll get practice doing exactly this and implementing this idea in today's lab

08:03.600 --> 08:06.120
exercise.

08:06.120 --> 08:11.560
Another great example in use case where generative models are exceptionally powerful is this

08:11.560 --> 08:16.480
broad class of problems that can be considered outlier or anomaly detection.

08:16.480 --> 08:21.280
One example is in the case of self-driving cars where it's going to be really critical

08:21.280 --> 08:27.400
to ensure that an autonomous vehicle governed and operated by a deep neural network is able

08:27.400 --> 08:33.240
to handle all of the cases that it may encounter on the road, not just, you know, the straight

08:33.240 --> 08:37.800
freeway driving that is going to be the majority of the training data and the majority of the

08:37.800 --> 08:40.880
time the car experiences on the road.

08:40.880 --> 08:46.240
So generative models can actually be used to detect outliers within training distributions

08:46.240 --> 08:51.880
and use this to, again, improve the training process so that the resulting model can be

08:51.880 --> 08:56.480
better equipped to handle these edge cases and rare events.

08:56.480 --> 09:02.280
All right, so hopefully that motivates why and how generative models can be exceptionally

09:02.280 --> 09:06.680
powerful and useful for a variety of real world applications.

09:06.680 --> 09:11.520
To dive into the bulk of the technical content for today's lecture, we're going to discuss

09:11.520 --> 09:14.960
two classes of what we call latent variable models.

09:14.960 --> 09:20.120
Specifically we'll look at autoencoders and generative adversarial networks or GANs.

09:20.120 --> 09:25.560
But before we get into that, I'd like to first begin by discussing why these are called latent

09:25.560 --> 09:32.440
variable models and what we actually mean when we use this word latent.

09:32.440 --> 09:37.080
And to do so, I think really the best example that I've personally come across for understanding

09:37.080 --> 09:43.000
what a latent variable is, is this story that is from Plato's work, the Republic.

09:43.000 --> 09:46.280
And this story is called the myth of the cave or the parable of the cave.

09:46.280 --> 09:48.540
And the story is as follows.

09:48.540 --> 09:54.080
In this myth, there are a group of prisoners and these prisoners are constrained as part

09:54.080 --> 09:57.200
of their prison punishment to face a wall.

09:57.200 --> 10:02.720
And the only things that they can see on this wall are the shadows of particular objects

10:02.720 --> 10:06.000
that are being passed in front of a fire that's behind them.

10:06.000 --> 10:10.200
So behind their heads and out of their line of sight.

10:10.200 --> 10:13.520
And the prisoners, the only thing they're really observing are these shadows on the

10:13.520 --> 10:14.520
wall.

10:14.520 --> 10:16.800
And so to them, that's what they can see.

10:16.800 --> 10:19.680
That's what they can measure and that's what they can give names to.

10:19.680 --> 10:21.320
That's really their reality.

10:21.320 --> 10:24.200
These are their observed variables.

10:24.200 --> 10:29.880
But they can't actually directly observe or measure the physical objects themselves

10:29.880 --> 10:32.280
that are actually casting these shadows.

10:32.280 --> 10:38.640
So those objects are effectively what we can analyze like latent variables.

10:38.640 --> 10:43.560
They're the variables that are not directly observable, but they're the true explanatory

10:43.560 --> 10:48.240
factors that are creating the observable variables, which in this case, the prisoners

10:48.240 --> 10:52.680
are seeing, like the shadows cast on the wall.

10:52.680 --> 10:59.440
And so our question in generative modeling broadly is to find ways of actually learning

10:59.440 --> 11:05.320
these underlying and hidden latent variables in the data, even when we're only given the

11:05.320 --> 11:07.680
observations that are made.

11:07.680 --> 11:14.720
And this is an extremely, extremely complex problem that is very well suited to learning

11:14.720 --> 11:20.800
by neural networks because of their power to handle multidimensional data sets and to

11:20.800 --> 11:26.840
learn combinations of nonlinear functions that can approximate really complex data distributions.

11:26.840 --> 11:28.480
All right.

11:28.480 --> 11:34.040
So we'll first begin by discussing a simple and foundational generative model, which tries

11:34.040 --> 11:40.200
to build up these latent variable representation by actually self encoding the input.

11:40.200 --> 11:43.680
And these models are known as auto encoders.

11:43.680 --> 11:50.280
What an auto encoder is, is it's an approach for learning a lower dimensional latent space

11:50.280 --> 11:52.760
from raw data.

11:52.760 --> 11:58.800
To understand how it works, what we do is we feed in as input raw data, for example,

11:58.800 --> 12:03.120
this image of a two, that's going to be passed through many successive deep neural network

12:03.120 --> 12:04.280
layers.

12:04.280 --> 12:09.320
And at the output of that succession of neural network layers, what we're going to generate

12:09.320 --> 12:13.760
is a low dimensional latent space, a feature representation.

12:13.760 --> 12:17.240
And that's really the goal that we're trying to predict.

12:17.240 --> 12:22.080
And so we can call this portion of the network an encoder, since it's mapping the data,

12:22.080 --> 12:27.960
x, into a encoded vector of latent variables, z.

12:27.960 --> 12:32.200
So let's consider this latent space, z.

12:32.200 --> 12:37.440
If you've noticed, I've represented z as having a smaller size, a smaller dimensionality

12:37.440 --> 12:43.840
as the input x, why would it be important to ensure the low dimensionality of this latent

12:43.840 --> 12:47.120
space, z?

12:47.120 --> 12:51.880
Having a low dimensional latent space means that we are able to compress the data, which

12:51.880 --> 12:57.120
in the case of image data can be, you know, on the order of many, many, many dimensions,

12:57.120 --> 13:03.160
we can compress the data into a small latent vector, where we can learn a very compact and

13:03.160 --> 13:06.600
rich feature representation.

13:06.600 --> 13:09.640
So how can we actually train this model?

13:09.640 --> 13:14.960
Are we going to be able to supervise for the particular latent variables that we're interested

13:14.960 --> 13:15.960
in?

13:15.960 --> 13:21.960
Well, remember that this is an unsupervised problem, where we have training data, but

13:21.960 --> 13:25.320
no labels for the latent space, z.

13:25.320 --> 13:31.280
So in order to actually train such a model, what we can do is learn a decoder network

13:31.280 --> 13:36.640
and build up a decoder network that is used to actually reconstruct the original image

13:36.640 --> 13:40.200
starting from this lower dimensional latent space.

13:40.200 --> 13:46.160
And again, this decoder portion of our autoencoder network is going to be a series of layers,

13:46.160 --> 13:51.240
neural network layers like convolutional layers, that's going to then take this hidden latent

13:51.240 --> 13:55.760
vector and map it back up to the input space.

13:55.760 --> 14:00.520
And we call our reconstructed output x hat, because it's our prediction and it's an imperfect

14:00.520 --> 14:05.360
reconstruction of our input x.

14:05.360 --> 14:09.920
And the way that we can actually train this network is by looking at the original input

14:09.920 --> 14:16.480
x and our reconstructed output x hat and simply comparing the two and minimizing the distance

14:16.480 --> 14:20.040
between these two images.

14:20.040 --> 14:25.120
So for example, we could consider the mean squared error, which in the case of images

14:25.120 --> 14:31.520
means effectively subtracting one image from another and squaring the difference, which

14:31.520 --> 14:36.520
is effectively the pixel wise difference between the input and reconstruction, measuring how

14:36.520 --> 14:41.000
faithful our reconstruction is to the original input.

14:41.000 --> 14:47.360
And again, notice that by using this reconstruction loss, this difference between the reconstructed

14:47.360 --> 14:55.080
output and our original input, we do not require any labels for our data beyond the data itself.

14:56.080 --> 15:05.080
So we can simplify this diagram just a little bit by abstracting away these individual layers

15:05.080 --> 15:07.720
in the encoder and decoder components.

15:07.720 --> 15:13.840
And again, note once again that this loss function does not require any labels, it is

15:13.840 --> 15:18.280
just using the raw data to supervise itself on the output.

15:18.280 --> 15:24.600
And this is a truly powerful idea and a transformative idea because it enables the model to learn

15:24.600 --> 15:30.920
a quantity, the latent variables z, that we're fundamentally interested in, but we

15:30.920 --> 15:35.360
cannot simply observe or cannot readily model.

15:35.360 --> 15:43.160
And when we constrain this latent space to a lower dimensionality, that affects the degree

15:43.160 --> 15:47.440
to which and the faithfulness to which we can actually reconstruct the input.

15:47.440 --> 15:53.160
And the way you can think of this is as imposing a sort of information bottleneck during the

15:53.160 --> 15:55.720
models training and learning process.

15:55.720 --> 16:00.440
And effectively, what this bottleneck does is a form of compression, right?

16:00.440 --> 16:06.280
We're taking the input data, compressing it down to a much smaller latent space, and

16:06.280 --> 16:09.120
then building back up a reconstruction.

16:09.120 --> 16:13.480
And in practice, what this results in is that the lower the dimensionality of your latent

16:13.480 --> 16:18.760
space, the poorer and worse quality reconstruction you're going to get out.

16:18.760 --> 16:20.520
All right.

16:20.520 --> 16:25.960
So in summary, these autoencoder structures use this sort of bottlenecking hidden layer

16:25.960 --> 16:30.040
to learn a compressed latent representation of the data.

16:30.040 --> 16:34.760
And we can self-supervise the training of this network by using what we call a reconstruction

16:34.760 --> 16:42.560
loss that forces the autoencoder network to encode as much information about the data

16:42.560 --> 16:48.840
as possible into a lower dimensional latent space while still being able to build up faithful

16:49.040 --> 16:50.560
reconstructions.

16:50.560 --> 16:56.680
So the way I like to think of this is automatically encoding information from the data into a

16:56.680 --> 17:00.640
lower dimensional latent space.

17:00.640 --> 17:06.040
Let's now expand upon this idea a bit more and introduce this concept and architecture

17:06.040 --> 17:11.400
of variational autoencoders or VAEs.

17:11.400 --> 17:18.200
So as we just saw, traditional autoencoders go from input to reconstructed output.

17:18.200 --> 17:24.000
And if we pay closer attention to this latent layer denoted to here in orange, what you

17:24.000 --> 17:28.640
can hopefully realize is that this is just a normal layer in a neural network, just

17:28.640 --> 17:30.000
like any other layer.

17:30.000 --> 17:31.400
It's deterministic.

17:31.400 --> 17:35.080
If you're going to feed in a particular input to this network, you're going to get the

17:35.080 --> 17:38.280
same output so long as the weights are the same.

17:38.280 --> 17:43.800
So effectively, a traditional autoencoder learns this deterministic encoding, which allows

17:43.800 --> 17:48.000
for reconstruction and reproduction of the input.

17:48.000 --> 17:55.600
In contrast, variational autoencoders impose a stochastic or variational twist on this

17:55.600 --> 17:57.200
architecture.

17:57.200 --> 18:03.880
And the idea behind doing so is to generate smoother representations of the input data

18:03.880 --> 18:10.600
and improve the quality of the not only of reconstructions, but also to actually generate

18:10.600 --> 18:16.320
new images that are similar to the input data set, but not direct reconstructions of the

18:16.320 --> 18:17.800
input data.

18:17.800 --> 18:24.040
And the way this is achieved is that variational autoencoders replace that deterministic layer

18:24.040 --> 18:28.600
Z with a stochastic sampling operation.

18:28.600 --> 18:34.120
What this means is that instead of learning the latent variables Z directly, for each

18:34.120 --> 18:40.680
variable, the variational autoencoder learns a mean and a variance associated with that

18:40.680 --> 18:42.280
latent variable.

18:42.280 --> 18:47.320
And what those means and variances do is that they parametrize a probability distribution

18:47.320 --> 18:49.480
for that latent variable.

18:49.480 --> 18:55.600
So what we've done in going from an autoencoder to a variational autoencoder is going from

18:55.600 --> 19:02.600
a vector of latent variables Z to learning a vector of means mu and a vector of variances

19:02.600 --> 19:10.680
sigma, sigma squared, that parametrize these variables and define probability distributions

19:10.680 --> 19:13.800
for each of our latent variables.

19:13.800 --> 19:20.240
And the way we can actually generate new data instances is by sampling from the distribution

19:20.240 --> 19:29.840
defined by these mus and sigmas to generate a latent sample and get probabilistic representations

19:29.840 --> 19:32.280
of the latent space.

19:32.280 --> 19:35.960
And what I'd like you to appreciate about this network architecture is that it's very

19:35.960 --> 19:41.720
similar to the autoencoder I previously introduced, just that we have this probabilistic twist

19:41.800 --> 19:47.760
where we're now performing the sampling operation to compute samples from each of the latent

19:47.760 --> 19:49.760
variables.

19:49.760 --> 19:57.440
All right, so now because we've introduced this sampling operation, this stochasticity

19:57.440 --> 20:03.640
into our model, what this means for the actual computation and learning process of the network,

20:03.640 --> 20:09.480
the encoder and decoder, is that they're now probabilistic in their nature.

20:09.520 --> 20:14.800
And the way you can think of this is that our encoder is going to be trying to learn

20:14.800 --> 20:23.000
a probability distribution of the latent space Z given the input data X, while the decoder

20:23.000 --> 20:29.440
is going to take that learned latent representation and compute a new probability distribution

20:29.440 --> 20:33.800
of the input X given that latent distribution Z.

20:33.800 --> 20:39.400
And these networks, the encoder, the decoder, are going to be defined by separate sets of

20:39.440 --> 20:42.480
weights, phi and theta.

20:42.480 --> 20:48.920
And the way that we can train this variational autoencoder is by defining a loss function

20:48.920 --> 20:55.920
that's going to be a function of the data X as well as the sets of weights phi and theta.

20:55.920 --> 21:02.680
And what's key to how VAEs can be optimized is that this loss function is now comprised

21:02.680 --> 21:05.160
of two terms instead of just one.

21:05.160 --> 21:10.800
We have the reconstruction loss just as before, which again is going to capture this difference

21:10.800 --> 21:16.920
between the input and the reconstructed output, and also a new term to our loss, which we

21:16.920 --> 21:22.920
call the regularization loss, also called the VAE loss.

21:22.920 --> 21:29.920
And to take a look in more detail at why each of these loss terms represents, let's first

21:29.960 --> 21:36.320
emphasize again that our overall loss function is going to be defined and taken with respect

21:36.320 --> 21:42.320
to the sets of weights of the encoder and decoder and the input X.

21:42.320 --> 21:46.080
The reconstruction loss is very similar to before, right?

21:46.080 --> 21:51.840
And you can think of it as being driven by a log likelihood function, for example, for

21:51.840 --> 21:56.880
image data, the mean squared error between the input and the output.

21:56.880 --> 22:02.400
And we can self-supervise the reconstruction loss just as before to force the latent space

22:02.400 --> 22:08.800
to learn and represent faithful representations of the input data, ultimately resulting in

22:08.800 --> 22:11.520
faithful reconstructions.

22:11.520 --> 22:17.120
The new term here, the regularization term, is a bit more interesting and completely new

22:17.120 --> 22:23.080
at this stage, so we're going to dive in and discuss it further in a bit more detail.

22:23.080 --> 22:28.840
So our probability distribution that's going to be computed by our encoder, q phi of z

22:28.840 --> 22:34.840
of x, is a distribution on the latent space z given the data x.

22:34.840 --> 22:41.440
And what regularization enforces is that as a part of this learning process, we're going

22:41.440 --> 22:48.440
to place a prior on the latent space z, which is effectively some initial hypothesis about

22:49.160 --> 22:53.560
what we expect the distributions of z to actually look like.

22:53.560 --> 22:59.160
And by imposing this regularization term, what we can achieve is that the model will

22:59.160 --> 23:04.600
try to enforce the z's that it learns to follow this prior distribution.

23:04.600 --> 23:08.360
And we're going to denote this prior as p of z.

23:08.360 --> 23:12.520
This term here, d, is the regularization term.

23:12.600 --> 23:20.120
And what it's going to do is it's going to try to enforce a minimization of the divergence

23:20.120 --> 23:26.040
or the difference between what the encoder is trying to infer, the probability distribution

23:26.040 --> 23:33.000
of z given x, and that prior that we're going to place on the latent variables p of z.

23:33.720 --> 23:40.040
And the idea here is that by imposing this regularization factor, we can try to keep the

23:40.040 --> 23:45.640
network from overfitting on certain parts of the latent space by enforcing the fact that

23:45.640 --> 23:51.240
we want to encourage the latent variables to adopt a distribution that's similar to our prior.

23:51.960 --> 23:57.640
So we're going to go through now, you know, both the mathematical basis for this regularization

23:57.640 --> 24:04.200
term as well as a really intuitive walkthrough of what regularization achieves to help give you a

24:04.200 --> 24:09.080
concrete understanding and an intuitive understanding about why regularization is

24:09.080 --> 24:15.880
important and why placing a prior is important. So let's first consider,

24:18.360 --> 24:23.320
yeah, so to re-emphasize once again, this regularization term is going to consider

24:23.320 --> 24:28.600
the divergence between our inferred latent distribution and the fixed prior we're going to place.

24:29.560 --> 24:37.480
So before we to get into this, let's consider what could be a good choice of prior for each of

24:37.480 --> 24:44.840
these latent variables. How do we select p of z? I'll first tell you what's commonly done.

24:45.720 --> 24:51.560
The common choice that's used very extensively in the community is to enforce the latent variables

24:51.560 --> 24:56.600
to roughly follow normal Gaussian distributions, which means that they're going to be a normal

24:57.160 --> 25:03.320
distribution centered around mean zero and have a standard deviation and variance of one.

25:04.600 --> 25:10.680
By placing these normal Gaussian priors on each of the latent variables and therefore on our

25:10.680 --> 25:16.520
latent distribution overall, what this encourages is that the learned encodings learned by the

25:16.520 --> 25:23.400
encoder portion of our VAE are going to be sort of distributed evenly around the center of each

25:23.400 --> 25:30.760
of the latent variables. And if you can imagine and picture when you have sort of a roughly even

25:30.760 --> 25:37.400
distribution around the center of a particular region of the latent space, what this means is

25:37.400 --> 25:43.800
that outside of this region, far away, there's going to be a greater penalty and this can result

25:43.800 --> 25:50.360
in instances from instances where the network is trying to cheat and try to cluster particular

25:50.360 --> 25:56.200
points outside the center, these centers in the latent space, like if it was trying to memorize

25:56.200 --> 26:03.160
particular outliers or edge cases in the data. After we place a normal Gaussian prior on our

26:03.160 --> 26:10.120
latent variables, we can now begin to concretely define the regularization term component of our

26:10.120 --> 26:17.160
loss function. This loss, this term to the loss is very similar in principle to a cross entropy

26:17.160 --> 26:24.360
loss that we saw before, where the key is that we're going to be defining the distance function

26:24.360 --> 26:31.720
that describes the difference or the divergence between the inferred latent distribution q,

26:31.720 --> 26:38.600
phi of z given x and the prior that we're going to be placing p of z. And this term is called

26:38.600 --> 26:44.520
the Kublack-Liebler or KL divergence. And when we choose a normal Gaussian prior,

26:45.800 --> 26:52.920
we, this results in the KL divergence taking this particular form of this equation here,

26:52.920 --> 26:59.240
where we're using the means and sigmas as input and computing this distance metric that captures

26:59.240 --> 27:04.680
the divergence of that learned latent variable distribution from the normal Gaussian.

27:05.320 --> 27:11.160
All right. So now I really want to spend a bit of time to get some, build up some intuition about

27:11.160 --> 27:18.680
how this regularization and works and why we actually want to regularize our VAE and then also

27:18.680 --> 27:25.480
why we select a normal prior. All right. So to do this, let's, let's consider the following question.

27:25.480 --> 27:32.040
What properties do we want this to achieve from regularization? Why are we actually regularizing

27:32.760 --> 27:40.680
our, our network in the first place? The first key property that we want for a generative model like

27:40.680 --> 27:47.480
a VAE is what I can, what I like to think of as continuity, which means that if there are points

27:47.480 --> 27:53.160
that are represented closely in the latent space, they should also result in similar

27:53.800 --> 28:00.760
reconstructions, similar outputs, similar content after they are decoded. You would expect intuitively

28:00.760 --> 28:07.000
that regions in the latent space have some notion of distance or similarity to each other. And this

28:07.000 --> 28:13.080
indeed is a really key property that we want to achieve with our generative model. The second

28:13.080 --> 28:20.040
property is completeness and it's very related to continuity. And what this means is that when

28:20.040 --> 28:27.640
we sample from the latent space to decode the latent space into an output, that should result

28:27.640 --> 28:34.440
in a meaningful reconstruction, a meaningful, uh, sampled content that is, you know, resembling

28:34.440 --> 28:40.920
the original data distribution. You can imagine that if we're sampling from the latent space and

28:40.920 --> 28:47.720
just getting garbage out that has no relationship to our input, this could be a huge, huge problem

28:47.720 --> 28:54.120
for our model. All right. So with these two properties in mind, continuity and completeness,

28:54.840 --> 29:00.440
let's consider the consequences of what can occur if we do not regularize our model.

29:02.040 --> 29:07.720
Well, without regularization, what could end up happening with respect to these two properties

29:07.720 --> 29:14.040
is that there could be instances of points that are close in latent space, but not similarly

29:14.040 --> 29:21.160
decoded. So I'm using this really intuitive illustration where these dots represent abstracted

29:21.160 --> 29:27.880
away sort of regions in the latent space. And the shapes that they relate to, you can think of as

29:27.880 --> 29:34.440
what is going to be decoded after those instances in the latent space are passed through the decoder.

29:35.320 --> 29:41.160
So in this example, we have these two dots, the greenish dot and the reddish dot,

29:41.160 --> 29:45.720
that are physically close in latent space, but result in completely different shapes

29:45.800 --> 29:52.680
when they're decoded. We also have an instance of this purple point, which when it's decoded,

29:52.680 --> 29:59.560
it doesn't result in a meaningful content. It's just a scribble. So by not regularizing,

29:59.560 --> 30:05.800
and I'm abstracting a lot away here, and that's on purpose, we could have these instances where

30:05.800 --> 30:12.440
we don't have continuity and we don't have completeness. Therefore, our goal with regularization

30:12.440 --> 30:19.640
is to be able to realize a model where points that are close in the latent space are not only

30:19.640 --> 30:25.400
similarly decoded, but also meaningfully decoded. So for example, here, we have the red dot and the

30:25.400 --> 30:31.720
orange dot, which result in both triangle-like shapes, but with slight variations on the on the

30:31.720 --> 30:38.520
triangle itself. So this is the intuition about what regularization can enable us to achieve

30:38.520 --> 30:45.800
and what are desired properties for these generative models. Okay, how can we actually

30:45.800 --> 30:53.000
achieve this regularization? And how does the normal prior fit in? As I mentioned, right,

30:53.000 --> 30:59.800
VAEs, they don't just learn the latent variable Z directly. They're trying to encode the inputs

30:59.800 --> 31:05.400
as distributions that are defined by mean and variance. So my first question to you is,

31:05.400 --> 31:10.200
is it going to be sufficient to just learn mean and variance, learn these distributions?

31:10.920 --> 31:17.400
Can that guarantee continuity and completeness? No, and let's understand why.

31:19.240 --> 31:25.720
All right, without any sort of regularization, what could the model try to resort to?

31:27.000 --> 31:34.600
Remember that the VAE or that the VAE, the loss function is defined by both a reconstruction

31:34.600 --> 31:40.520
term and a regularization term. If there is no regularization, you can bet that the model

31:40.520 --> 31:46.280
is going to just try to optimize that reconstruction term. So it's effectively going to learn to

31:46.280 --> 31:52.920
minimize the reconstruction loss, even though we're encoding the latent variables via mean and variance.

31:54.360 --> 32:01.240
And two instances, two consequences of that is that you can have instances where these

32:01.240 --> 32:05.960
learned variances for the latent variable end up being very, very, very small,

32:06.520 --> 32:12.280
effectively resulting in pointed distributions. And you can also have means that are totally

32:12.280 --> 32:17.400
divergent from each other, which result in discontinuities in the latent space. And this can

32:17.400 --> 32:25.240
occur while still trying to optimize that reconstruction loss, direct consequence of not regularizing.

32:25.320 --> 32:32.120
By, in order to overcome these problems, we need to regularize the variance and the mean of these

32:32.120 --> 32:37.720
distributions that are being returned by the encoder. And the normal prior, placing that

32:37.720 --> 32:45.560
normal Gaussian distribution as our prior helps us achieve this. And to understand why exactly

32:45.560 --> 32:51.560
this occurs, is that effectively the normal prior is going to encourage these learned latent

32:51.560 --> 32:58.840
variable distributions to overlap in latent space. Recall, right? Mean zero, variance of one. That

32:58.840 --> 33:04.760
means all the, all the latent variables are going to be enforced to try to have the same mean, a

33:04.760 --> 33:10.120
centered mean, and all the variances are going to be regularized for each and every of the latent

33:10.120 --> 33:16.280
variable distributions. And so this will ensure a smoothness and a regularity and an overlap in

33:16.280 --> 33:23.000
the latent space, which will be very effective in helping us achieve these properties of continuity

33:23.000 --> 33:32.200
and completeness. Centering the means, regularizing the variances. So the regularization via this

33:32.200 --> 33:39.240
normal prior, by centering each of these latent variables, regularizing their, their variances,

33:39.240 --> 33:44.360
is that it helps enforce this continuous and complete gradient of information.

33:44.440 --> 33:49.880
Represented in the latent space, where again points and distances in the latent space have

33:49.880 --> 33:55.640
some relationship to the reconstructions and the content of the reconstructions that result.

33:57.000 --> 34:04.040
Note though that there's going to be a tradeoff between regularizing and reconstructing. The

34:04.040 --> 34:10.360
more we regularize, there's also a risk of suffering, the quality of the reconstruction

34:10.440 --> 34:16.120
and the generation process itself. So in optimizing gaze, there's going to be this tradeoff that's

34:16.120 --> 34:23.240
going to try to be tuned to fit the problem of interest. All right. So hopefully by walking

34:23.240 --> 34:28.600
through this, this example, and considering these points, you've built up a more intuition about

34:28.600 --> 34:34.200
why regularization is important and how specifically the normal prior can help us regularize.

34:34.600 --> 34:39.720
Great. So now we've defined our loss function, we know that we can reconstruct the inputs,

34:39.720 --> 34:45.080
we've understood how we can regularize learning and achieve continuity and completeness via

34:45.080 --> 34:51.000
this normal prior. These are all the components that define a forward pass through the network,

34:51.800 --> 34:57.480
going from input to encoding to decoded reconstruction. But we're still missing a

34:57.480 --> 35:01.880
critical step in putting the whole picture together. And that's, that's a critical step

35:01.880 --> 35:07.800
putting the whole picture together. And that's a back propagation. And the key here is that

35:07.800 --> 35:14.040
because of this fact that we've introduced this stochastic sampling layer, we now have a problem

35:14.040 --> 35:19.720
where we can't back propagate gradients through a sampling layer that has this element of stochasticity.

35:20.840 --> 35:26.360
Back propagation requires deterministic nodes, deterministic layers for which we can iteratively

35:26.360 --> 35:32.440
apply the chain rule to optimize gradients. Optimize the loss via gradient descent.

35:33.720 --> 35:41.320
All right. VAEs introduced sort of a breakthrough idea that solved this issue of not being able

35:41.320 --> 35:47.400
to back propagate through a sampling layer. And the key idea was to actually subtly

35:47.400 --> 35:53.080
reparameterize the sampling operation such that the network could be trained completely end to end.

35:54.040 --> 35:59.480
So as we, as we already learned, right, we're trying to build up this latent distribution

35:59.480 --> 36:06.920
defined by these variables z define it placing a normal prior defined by a mean and a variance.

36:06.920 --> 36:13.480
And we can't simply back propagate gradients through the sampling layer because we can't compute

36:14.040 --> 36:21.160
gradients through this stochastic sample. The key idea instead is to try to consider the

36:21.240 --> 36:31.480
sampled latent vector z as a sum defined by a fixed mu a fixed sigma vector and scale that sigma

36:31.480 --> 36:38.280
vector by random constants that are going to be drawn from a prior distribution such as a normal

36:38.280 --> 36:46.120
Gaussian. And by reparameterizing the sampling operation as, as so, we still have this element

36:46.120 --> 36:52.680
of stochasticity, but that stochasticity is introduced by this random constant epsilon,

36:52.680 --> 36:58.360
which is not occurring within the bottleneck latent layer itself. We've reparameterized

36:58.360 --> 37:04.840
and distributed it elsewhere. To visualize how this looks, let's consider it the following

37:04.840 --> 37:12.280
where originally in the original form of the VAE, we had this deterministic nodes,

37:12.280 --> 37:16.920
which are the weights of the network, as well as an input vector, and we are trying to

37:16.920 --> 37:23.640
back propagate through the stochastic sampling node z. But we can't do that. So now,

37:23.640 --> 37:31.720
by reparameterization, what we've achieved is the following form where our latent variable z

37:31.720 --> 37:41.240
are defined with respect to mu sigma squared, as well as these noise factor epsilon,

37:41.320 --> 37:47.240
such that when we want to do back propagation through the network to update, we can directly

37:47.240 --> 37:53.000
back propagate through z defined by mu and sigma squared, because this epsilon value is just taken

37:53.000 --> 37:59.480
as a constant, it's reparameterized elsewhere. And this is a very, very powerful trick, the

37:59.480 --> 38:05.320
reparameterization trick, because it enables us to train variational auto encoders end to end

38:05.320 --> 38:12.360
by back propagating with respect to z and with respect to the actual weights of the encoder

38:12.360 --> 38:20.920
network. All right. One side effect and one consequence of imposing these distributional

38:20.920 --> 38:26.440
priors on the latent variable is that we can actually sample from these latent variables

38:26.440 --> 38:33.320
and individually tune them while keeping all of the other variables fixed. And what you can do

38:33.320 --> 38:38.760
is you can tune the value of a particular latent variable and run the decoder each time that variable

38:38.760 --> 38:45.960
is changed, each time that variable is perturbed to generate a new reconstructed output. So an

38:45.960 --> 38:53.000
example of that result is in the following, where this perturbation of the latent variables results

38:53.000 --> 39:00.040
in a representation that has some semantic meaning about what the network is maybe learning. So in

39:00.040 --> 39:06.920
this example, these images show variation in head pose. And the different dimensions of z,

39:06.920 --> 39:13.800
the latent space, the different latent variables, are in this way encoding different latent features

39:13.800 --> 39:20.280
that can be interpreted by keeping all other variables fixed and perturbing the value of

39:20.280 --> 39:29.640
one individual latent variable. Ideally, in order to optimize VAEs and try to maximize the information

39:29.640 --> 39:36.120
that they encode, we want these latent variables to be uncorrelated with each other, effectively

39:36.120 --> 39:42.120
disentangled. And what that could enable us to achieve is to learn the richest and most

39:42.120 --> 39:49.000
compact latent representation possible. So in this case, we have head pose on the x axis

39:49.000 --> 39:55.160
and smile on the y axis. And we want these to be as uncorrelated with each other as possible.

39:55.800 --> 40:01.320
One way we can achieve this, that's been shown to achieve this disentanglement,

40:01.320 --> 40:07.640
is rather a quite straightforward approach called beta VAEs. So if we consider the loss of a standard

40:07.640 --> 40:13.800
VAE, again, we have this reconstruction term defined by a log likelihood and a regularization term

40:13.800 --> 40:20.360
defined by the KL divergence. Beta VAEs introduce a new hyper parameter beta,

40:20.360 --> 40:26.440
which controls the strength of this regularization term. And it's been shown mathematically that

40:26.440 --> 40:32.520
by increasing beta, the effect is to place constraints on the latent encoding, such as to

40:32.520 --> 40:37.880
encourage disentanglement. And there have been extensive proofs and discussions as to how exactly

40:37.880 --> 40:44.120
this is achieved. But to consider the results, let's again consider the problem of face

40:44.120 --> 40:51.720
reconstruction. Where using a standard VAE, if we consider the latent variable of head pose

40:51.720 --> 40:57.400
or rotation, in this case where beta equals one, what you can hopefully appreciate is that as the

40:57.400 --> 41:06.120
face pose is changing, the smile of some of these faces is also changing. In contrast, by enforcing

41:06.120 --> 41:13.080
a beta much larger than one, what is able to be achieved is that the smile remains relatively

41:13.080 --> 41:19.960
constant while we can perturb the single latent variable of the head rotation and achieve perturbations

41:19.960 --> 41:27.880
with respect to head rotation alone. All right. So as I motivated and introduced in the beginning

41:27.880 --> 41:33.160
and the introduction of this lecture, one powerful application of generative models and latent

41:33.160 --> 41:39.400
variable models is in model debiasing. And in today's lab, you're actually going to get real hands-on

41:39.400 --> 41:46.120
experience in building a variational autoencoder that can be used to achieve automatic debiasing

41:46.120 --> 41:51.480
of facial classification systems, facial detection systems. And the power and the idea of this

41:52.040 --> 41:59.160
approach is to build up a representation, a learned latent distribution of face data,

41:59.880 --> 42:05.720
and use this to identify regions of that latent space that are going to be overrepresented

42:05.720 --> 42:10.120
or underrepresented. And that's going to all be taken with respect to particular

42:10.120 --> 42:17.560
learned features such as skin tone, pose, objects, clothing. And then from these learned

42:17.560 --> 42:24.760
distributions, we can actually adjust the training process such that we can place greater

42:25.640 --> 42:31.560
weight and greater sampling on those images and on those faces the fall in the regions of

42:31.560 --> 42:37.080
the latent space that are underrepresented automatically. And what's really, really cool

42:37.960 --> 42:43.800
about deploying a VAE or a latent variable model for an application like model debiasing

42:43.800 --> 42:49.640
is that there's no need for us to annotate and prescribe the features that are important

42:49.640 --> 42:55.000
to actually debize against. The model learns them automatically. And this is going to be the

42:55.000 --> 43:02.120
topic of today's lab. And it also opens the door to a much broader space that's going to

43:02.120 --> 43:07.400
be explored further in a later spotlight lecture that's going to focus on algorithmic bias and

43:07.400 --> 43:14.200
machine learning fairness. All right, so to summarize the key points on VAEs, they compress

43:14.200 --> 43:21.560
representation of data into an encoded representation. Reconstruction of the data input allows for

43:21.560 --> 43:29.400
unsupervised learning without labels. We can use the reparameterization trick to train

43:29.400 --> 43:36.200
VAEs end to end. We can take hidden latent variables, perturb them to interpret their

43:36.200 --> 43:41.400
content and their meaning. And finally, we can sample from the latent space to generate new

43:41.400 --> 43:49.000
examples. But what if we wanted to focus on generating samples and synthetic samples that

43:49.000 --> 43:55.640
were as faithful to a data distribution generally as possible? To understand how we can achieve

43:55.640 --> 44:00.200
this, we're going to transition to discuss a new type of generative model called a generative

44:00.200 --> 44:10.040
adversarial network or GAN for short. The idea here is that we don't want to explicitly model

44:10.040 --> 44:16.600
the density or the or the distribution underlying some data, but instead just learn a representation

44:16.600 --> 44:21.720
that can be successful in generating new instances that are similar to the data,

44:23.000 --> 44:28.760
which means that we want to optimize to sample from a very, very complex distribution,

44:29.400 --> 44:35.000
which cannot be learned and modeled directly. Instead, we're going to have to build up some

44:35.000 --> 44:42.440
approximation of this distribution. And the really cool and breakthrough idea of GANs

44:42.440 --> 44:49.320
is to start from something extremely, extremely simple, just random noise and try to build a

44:49.320 --> 44:55.480
neural network, a generative neural network that can learn a functional transformation

44:55.480 --> 45:03.880
that goes from noise to the data distribution. And by learning this functional generative mapping,

45:03.880 --> 45:10.200
we can then sample in order to generate fake instances, synthetic instances that are going

45:10.200 --> 45:17.400
to be as close to the real data distribution as possible. The breakthrough to achieving this

45:17.400 --> 45:23.880
was this structure called GANs, where the key component is to have two neural networks,

45:23.880 --> 45:29.160
a generator network and a discriminator network that are effectively competing against each other,

45:29.160 --> 45:34.760
their adversaries. Specifically, we have a generator network, which I'm going to denote

45:34.760 --> 45:40.920
here on out by G, that's going to be trained to go from random noise to produce an imitation of

45:40.920 --> 45:47.880
the data. And then the discriminator is going to take that synthetic fake data, as well as real

45:47.880 --> 45:53.720
data, and be trained to actually distinguish between fake and real. And in training, these two

45:53.720 --> 46:00.120
networks are going to be competing against each other. And so in doing so, overall, the effect

46:00.120 --> 46:05.000
is that the discriminator is going to get better and better at learning how to classify real and

46:05.000 --> 46:09.880
fake. And the better it becomes at doing that, it's going to force the generator to try to produce

46:09.880 --> 46:14.840
better and better synthetic data to try to fool the discriminator back and forth, back and forth.

46:15.800 --> 46:22.840
So let's now break this down and go from a very simple toy example to get more intuition

46:22.840 --> 46:29.560
about how these GANs work. The generator is going to start, again, from some completely random noise

46:29.560 --> 46:34.840
and produce fake data. And I'm going to show that here by representing these data as points on a

46:34.840 --> 46:40.920
one-dimensional line. The discriminator is then going to see these points, as well as real data.

46:41.880 --> 46:47.960
And then it's going to be trained to output a probability that the data it sees are real,

46:48.600 --> 46:53.560
or if they are fake. And in the beginning, it's not going to be trained very well, right? So its

46:53.560 --> 46:58.200
predictions are not going to be very good. But then you're going to train it, and you're going to train

46:58.200 --> 47:05.560
it. And it's going to start increasing the probabilities of real versus not real appropriately,

47:05.560 --> 47:11.000
such that you get this perfect separation where the discriminator is able to perfectly distinguish

47:11.000 --> 47:17.160
what is real and what is fake. Now it's back to the generator. And the generator is going to come

47:17.160 --> 47:24.680
back. It's going to take instances of where the real data lie as inputs to train. And then it's

47:24.680 --> 47:30.840
going to try to improve its imitation of the data, trying to move the fake data, the synthetic data

47:30.840 --> 47:37.240
that is generated closer and closer to the real data. And once again, the discriminator is now

47:37.240 --> 47:42.600
going to receive these new points. And it's going to estimate a probability that each of these points

47:42.600 --> 47:48.600
is real. And again, learn to decrease the probability of the fake points being real,

47:49.240 --> 47:55.320
further and further. And now we're going to repeat again. And one last time, the generator is going

47:55.320 --> 48:01.400
to start moving these fake points closer and closer to the real data, such that the fake

48:01.400 --> 48:07.240
data are almost following the distribution of the real data. At this point, it's going to be

48:07.240 --> 48:12.120
really, really hard for the discriminator to effectively distinguish between what is real

48:12.120 --> 48:17.880
and what is fake. While the generator is going to continue to try to create fake data instances

48:17.880 --> 48:24.280
to fool the discriminator. And this is really the key intuition behind how these two components of

48:24.280 --> 48:32.120
GANs are essentially competing with each other. All right. So to summarize how we train GANs,

48:32.120 --> 48:37.720
the generator is going to try to synthesize fake instances to fool a discriminator, which is going

48:37.720 --> 48:42.920
to be trained to identify the synthesized instances and discriminate these as fake.

48:42.920 --> 48:49.320
To actually train, we're going to see that we are going to define a loss function that defines

48:50.360 --> 48:55.000
competing and adversarial objectives for each of the discriminator and the generator.

48:55.720 --> 49:01.720
And a global optimum, the best we could possibly do would mean that the generator could perfectly

49:01.720 --> 49:07.000
reproduce the true data distribution, such that the discriminator absolutely cannot tell

49:07.000 --> 49:12.200
what's synthetic versus what's real. So let's go through how the loss function

49:12.200 --> 49:20.280
for GAN breaks down. The loss term for GAN is based on that familiar cross entropy loss.

49:20.280 --> 49:25.800
And it's going to now be defined between the true and generated distributions.

49:25.800 --> 49:30.280
So we're first going to consider the loss from the perspective of the discriminator.

49:30.920 --> 49:38.040
We want to try to maximize the probability that the fake data is identified as fake.

49:38.760 --> 49:45.640
And so to break this down here, G of Z defines the generator's output. And so D of G of Z

49:46.280 --> 49:51.240
is the discriminator's estimate of the probability that a fake instance is actually fake.

49:52.600 --> 49:58.040
D of X is the discriminator's estimate of the probability that a real instance is fake.

49:58.040 --> 50:03.480
So 1 minus D of X is its probability estimate that a real instance is real.

50:04.440 --> 50:09.640
So together, from the point of view of the discriminator, we want to maximize this probability.

50:10.200 --> 50:14.920
Maximize probability fake is fake. Maximize the estimate of probability real is real.

50:16.520 --> 50:21.320
Now let's turn our attention to the generator. Remember that the generator is taking

50:21.880 --> 50:28.120
random noise and generating an instance. It cannot directly affect the term D of X,

50:28.760 --> 50:34.840
which shows up in the loss, right? Because D of X is solely based on the discriminator's operation

50:34.840 --> 50:39.880
on the real data. So for the generator, the generator is going to have the adversarial

50:39.880 --> 50:45.160
objective to the discriminator, which means it's going to try to minimize this term.

50:46.120 --> 50:55.000
Effectively minimizing the probability that the discriminator can distinguish its generated data

50:55.960 --> 51:05.160
as fake. D of G of Z. And the goal for the generator is to minimize this term of the objective.

51:07.720 --> 51:13.720
So the objective of the generator is to try to synthesize fake instances

51:13.720 --> 51:18.600
that fool the discriminator. And eventually, over the course of training the discriminator,

51:18.600 --> 51:24.520
the discriminator is going to be as best as it possibly can be at discriminating real versus

51:24.520 --> 51:30.440
fake. Therefore, the ultimate goal of the generator is to synthesize fake instances that fool the

51:30.440 --> 51:36.520
best discriminator. And this is all put together in this min max objective function, which has

51:36.520 --> 51:42.680
these two components optimized adversarily. And then after training, we can actually use the

51:42.680 --> 51:48.360
generator network, which is now fully trained to produce new data instances that have never been

51:48.360 --> 51:54.840
seen before. So we're going to focus on that now. And what is really cool is that when the

51:54.840 --> 52:01.480
trained generator of again, synthesizes new instances, it's effectively learning a transformation

52:01.480 --> 52:07.240
from a distribution of noise to a target data distribution. And that transformation,

52:07.240 --> 52:12.200
that mapping is going to be what's learned over the course of training. So if we consider one

52:12.200 --> 52:17.560
point from a latent noise distribution, it's going to result in a particular output in the

52:17.560 --> 52:23.800
target data space. And if we consider another point of random noise, feed it through the generator,

52:23.800 --> 52:29.160
it's going to result in a new instance that and that new instance is going to fall somewhere else

52:29.160 --> 52:36.280
on the data manifold. And indeed, what we can actually do is interpolate and trans and traverse

52:36.280 --> 52:42.120
in the space of Gaussian noise to result in interpolation in the target space. And you can

52:42.120 --> 52:48.440
see an example of this result here, where a transformation in series reflects a traversal

52:48.440 --> 52:55.320
across the target data manifold. And that's produced in the synthetic examples that are

52:55.320 --> 53:01.160
outputted by the generator. All right. So in the final few minutes of this lecture, I'm going to

53:01.160 --> 53:07.560
highlight some of the recent advances in GANs and hopefully motivate even further why this approach

53:07.560 --> 53:14.520
is so powerful. So one idea that's been extremely, extremely powerful is this idea of progressive

53:14.520 --> 53:20.840
GANs, progressive growing, which means that we can iteratively build more detail into the

53:20.840 --> 53:28.280
generated instances that are produced. And this is done by progressively adding layers of increasing

53:28.280 --> 53:35.160
spatial resolution in the case of image data. And by incrementally building up both the generator

53:35.160 --> 53:41.240
and discriminator networks in this way as training progresses, it results in very well resolved

53:41.240 --> 53:47.480
synthetic images that are output ultimately by the generator. So some results of this idea of

53:47.480 --> 53:54.760
progressive, a progressive GAN are displayed here. Another idea that has also led to tremendous

53:54.760 --> 54:01.000
improvement in the quality of synthetic examples generated by GANs is a architecture improvement

54:01.000 --> 54:06.440
called style GAN, which combines this idea of progressive growing that I introduced earlier

54:06.440 --> 54:13.000
with principles of style transfer, which means trying to compose an image in the style of another

54:13.000 --> 54:22.520
image. So for example, what we can now achieve is to map input images source A using application of

54:22.520 --> 54:29.080
coarse grained styles from secondary sources onto those targets to generate new instances

54:29.080 --> 54:37.080
that mimic the style of source B. And that result is shown here. And hopefully you can

54:37.080 --> 54:42.680
appreciate that these coarse grained features, these coarse grained styles like age, facial

54:42.680 --> 54:50.200
structure, things like that can be reflected in these synthetic examples. This same style GAN

54:50.200 --> 54:58.440
system has led to tremendously realistic synthetic images in the areas of both face

54:58.440 --> 55:06.920
synthesis as well as for animals, other objects as well. Another extension to the GAN architecture

55:06.920 --> 55:13.880
that has enabled particularly powerful applications for select problems and tasks is this idea of

55:13.880 --> 55:19.880
conditioning, which imposes a bit of additional further structure on the types of outputs that

55:19.880 --> 55:27.240
can be synthesized by GAN. So the idea here is to condition on a particular label by supplying

55:27.240 --> 55:35.320
what is called a conditioning factor denoted here as C. And what this allows us to achieve is instances

55:35.320 --> 55:43.320
like that of paired translation in the case of image synthesis, where now instead of a single input

55:43.960 --> 55:50.040
as training data for our generator, we have pairs of inputs. So for example here we consider

55:50.040 --> 55:56.680
both a driving scene and a corresponding segmentation map to that driving scene. And the discriminator

55:56.680 --> 56:04.440
can in turn be trained to classify fake and real pairs of data. And again the generator is going

56:04.440 --> 56:13.080
to be trained to try to fool the discriminator. Example applications of this idea are

56:14.280 --> 56:20.920
seen as follows where we can now go from an input of a semantic segmentation map to generate a

56:20.920 --> 56:29.000
synthetic street scene mapping according to that segmentation. Or we can go from an aerial view

56:29.000 --> 56:35.160
from a satellite image to a street map view or from particular labels of an architectural

56:35.160 --> 56:41.400
building to a synthetic architectural facade or day to night, black and white to color,

56:41.480 --> 56:46.920
edges to photos, different instances of paired translation that are achieved by conditioning

56:46.920 --> 56:53.880
on particular labels. So another example which I think is really cool and interesting is

56:53.880 --> 57:01.720
translating from Google Street View to a satellite view and vice versa. And we can also achieve this

57:01.720 --> 57:08.120
dynamically. So for example in coloring given an edge input, the network can be trained to

57:08.120 --> 57:14.680
actually synthetically color in the artwork that is resulting from this particular edge sketch.

57:17.240 --> 57:22.760
Another idea instead of paired translation is that of unpaired image to image translation.

57:22.760 --> 57:28.440
And this is going to be achieved by a network architecture called CycleGAN where the model

57:28.440 --> 57:35.640
is taking as input images from one domain and is able to learn a mapping that translates to

57:35.720 --> 57:40.200
another domain without having a paired corresponding image in that other domain.

57:41.240 --> 57:48.120
So the idea here is to transfer the style and the distribution from one domain to another.

57:48.840 --> 57:55.640
And this is achieved by introducing the cyclic relationship and a cyclic loss function where

57:55.640 --> 58:02.520
we can go back and forth between a domain x and a domain y. And in this system there are actually

58:02.520 --> 58:06.920
two generators and two discriminators that are going to be trained on their respective

58:06.920 --> 58:14.200
generation and discrimination tasks. In this example the CycleGAN has been trained to try to

58:14.200 --> 58:20.520
translate from the domain of horses to the domain of zebras. And hopefully you can appreciate that

58:20.520 --> 58:27.480
in this example there's a transformation of the skin of the horse from brown to a zebra-like skin

58:27.480 --> 58:32.440
in stripes. And beyond this there's also a transformation of the surrounding area

58:32.440 --> 58:36.920
from green grass to something that's more brown in the case of the zebra.

58:38.600 --> 58:43.080
And I think to get an intuition about how this CycleGAN transformation is going

58:43.720 --> 58:51.000
is working. Let's go back to the idea that conventional GANs are moving from a distribution

58:51.000 --> 58:57.480
of Gaussian noise to some target data manifold. With CycleGANs the goal is to go from a particular

58:57.480 --> 59:05.080
data manifold x to another data manifold y. And in both cases and I think the underlying

59:05.080 --> 59:10.680
concept that makes GANs so powerful is that they function as very very effective distribution

59:10.680 --> 59:15.320
transformers and it can achieve these distribution transformations.

59:15.480 --> 59:23.720
Finally I'd like to consider one additional application that you may be familiar with

59:24.440 --> 59:30.760
of using CycleGANs and that's to transform speech and to actually use this CycleGAN technique to

59:30.760 --> 59:36.280
synthesize speech in someone else's voice. And the way this is done is by taking a bunch of audio

59:36.280 --> 59:42.840
recordings in one voice and audio recordings in another voice and converting those audio waveforms

59:42.840 --> 59:51.080
into an image representation which was called a spectrogram. We can then train a CycleGAN to

59:51.080 --> 59:58.840
operate on these spectrogram images to transform representations from voice A to make them appear

59:58.840 --> 01:00:05.480
like they appear that they are from another voice, voice B. And this is exactly how we did the

01:00:05.480 --> 01:00:11.800
speech transformation for the synthesis of Obama's voice in the demonstration that Alexander gave in

01:00:11.800 --> 01:00:18.680
the first lecture. So to inspect this further let's compare side by side the original audio

01:00:18.680 --> 01:00:24.840
from Alexander as well as the synthesized version in Obama's voice that was generated using a CycleGAN.

01:00:26.760 --> 01:00:34.120
Hi everybody and welcome to MIT 6S191, you know, facial introductory course

01:00:34.120 --> 01:00:44.120
on speech learning here at MIT. So notice that the spectrogram that results for Obama's voice

01:00:44.680 --> 01:00:50.760
is actually generated by an operation on Alexander's voice and effectively learning a domain

01:00:50.760 --> 01:00:56.680
transformation from Obama domain onto the domain of Alexander domain and the end result is that we

01:00:56.680 --> 01:01:03.240
create and synthesize something that's more Obama-like. All right so to summarize hopefully

01:01:03.240 --> 01:01:08.360
over the course of this lecture you built up understanding of generative modeling and classes

01:01:08.360 --> 01:01:14.920
of generative models that are particularly powerful in enabling probabilistic density estimation

01:01:14.920 --> 01:01:23.000
as well as sample generation. And with that I'd like to close the lecture and introduce you to

01:01:23.000 --> 01:01:29.400
the remainder of today's course which is going to focus on our second lab on computer vision,

01:01:30.280 --> 01:01:35.720
specifically exploring this question of debiasing in facial detection systems

01:01:35.720 --> 01:01:41.240
and using variational autoencoders to actually achieve an approach for automatic

01:01:41.240 --> 01:01:46.600
debiasing of classification systems. So I encourage you to come to the class gather town

01:01:46.600 --> 01:01:53.800
to have your questions on the lab's answered and to discuss further with any of us. Thank you.

