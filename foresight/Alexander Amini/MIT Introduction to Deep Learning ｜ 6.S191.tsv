start	end	text
0	15320	Good afternoon everyone and welcome to MIT Success 191.
15320	19360	My name is Alexander Amini and I'll be one of your instructors for the course this year
19360	21440	along with Ava.
21440	25280	And together we're really excited to welcome you to this really incredible course.
25280	33200	This is a very fast paced and very intense one week that we're about to go through together.
33200	38840	So we're going to cover the foundations of a also very fast paced moving field and a
38840	43640	field that has been rapidly changing over the past eight years that we have taught this
43640	45720	course at MIT.
45720	52240	Now over the past decade in fact, even before we started teaching this course, AI and deep
52240	58320	learning has really been revolutionizing so many different advances and so many different
58320	62860	areas of science, mathematics, physics and so on.
62860	72080	And not that long ago, we were having challenges and problems that we did not think were necessarily
72080	80840	solvable in our lifetimes that AI is now actually solving beyond human performance today.
80840	86400	And each year that we teach this course, this lecture in particular is getting harder
86400	92800	and harder to teach because for an introductory level course, this lecture, lecture number
92800	96560	one, is the lecture that's supposed to cover the foundations and if you think to any other
96560	103040	introductory course like an introductory course 101 on mathematics or biology, those
103040	106280	lecture ones don't really change that much over time.
106280	111960	So we're in a rapidly changing field of AI and deep learning where even these types
111960	115560	of lectures are rapidly changing.
115560	121800	So let me give you an example of how we introduced this course only a few years ago.
121800	131020	Hi everybody and welcome to MIT 6S191, the official introductory course on deep learning
131020	134720	taught here at MIT.
134720	142260	Deep learning is revolutionizing so many fields from robotics to medicine and everything
142260	144360	in between.
144360	151480	You'll learn the fundamentals of this field and how you can build some of these incredible
151480	153480	algorithms.
153480	162240	In fact, this entire speech and video are not real and were created using deep learning
162240	165960	and artificial intelligence.
165960	169420	And in this class, you'll learn how.
169420	178000	It has been an honor to speak with you today and I hope you enjoy the course.
178000	185280	The really surprising thing about that video to me when we first did it was how viral it
185280	187000	went a few years ago.
187000	191440	So just in a couple months about teaching this course a few years ago, that video went
191440	196640	very viral, it got over a million views within only a few months.
196640	202840	People were shocked with a few things but the main one was the realism of AI to be able
202840	210520	to generate content that looks and sounds extremely hyper realistic.
210520	215360	And when we did this video, when we created this for the class only a few years ago, this
215360	222480	video took us about $10,000 in compute to generate just about a minute long video.
222480	227160	If you think about it, I would say it's extremely expensive to compute something what we look
227160	231360	at like that and maybe a lot of you are not really impressed by the technology today because
231360	236520	you see all of the amazing things that AI and deep learning are producing.
236520	241760	Now fast forward today, the progress in deep learning and people were making all kinds
241760	245360	of exciting remarks about it when it came out a few years ago.
245360	251240	Now this is common stuff because AI is really doing much more powerful things than this
251240	254200	fun little introductory video.
254200	260120	So today fast forward four years, about four years to today.
260120	261120	Now where are we?
261120	267760	AI is now generating content with deep learning being so commoditized.
267760	274160	Deep learning is in all of our fingertips now online in our smartphones and so on.
274160	281320	In fact, we can use deep learning to generate these types of hyper realistic pieces of media
281320	286720	and content entirely from English language without even coding anymore.
286720	291640	So before we had to actually go and train these models and really code them to be able
291640	294280	to create that one minute long video.
294280	299240	Today we have models that will do that for us end to end directly from English language.
299240	303480	So we can prompt these models to create something that the world has never seen before.
303480	309320	A photo of an astronaut riding a horse and these models can imagine those pieces of content
309320	311640	entirely from scratch.
311640	317760	My personal favorite is actually how we can now ask these deep learning models to create
317760	322880	new types of software even themselves being software to ask them to create.
322880	327960	For example, to write this piece of TensorFlow code to train a neural network, right?
327960	333160	We're asking a neural network to write TensorFlow code to train another neural network and our
333160	340480	model can produce examples of functional and usable pieces of code that satisfy this
340480	344920	English prompt while walking through each part of the code independently.
344920	349440	So not even just producing it but actually educating and teaching the user on what each
349440	353520	part of these code blocks are actually doing.
353520	355320	You can see an example here.
355320	360000	And really what I'm trying to show you with all of this is that this is just highlighting
360000	366880	how far deep learning has gone even in a couple years since we've started teaching this course.
366880	370760	I mean going back even from before that to eight years ago.
370760	376640	And the most amazing thing that you'll see in this course in my opinion is that what
376640	381600	we try to do here is to teach you the foundations of all of this, how all of these different
381600	387120	types of models are created from the ground up and how we can make all of these amazing
387120	391600	advances possible so that you can also do it on your own as well.
391600	395200	And like I mentioned in the beginning, this introduction course is getting harder and
395200	398400	harder to do and to make every year.
398400	404520	I don't know where the field is going to be next year and I mean that's my honest truth
404520	410200	or even honestly in even one or two months time from now just because it's moving so
410200	411200	incredibly fast.
411200	416760	But what I do know is that what we will share with you in the course as part of this one
416760	421440	week is going to be the foundations of all of the technologies that we have seen up until
421440	426280	this point that will allow you to create that future for yourselves and to design brand new
426280	433520	types of deep learning models using those fundamentals and those foundations.
433520	438920	So let's get started with all of that and start to figure out how we can actually achieve
438920	443520	all of these different pieces and learn all of these different components.
443520	448720	And we should start this by really tackling the foundations from the very beginning and
448720	453160	asking ourselves, you know, we've heard this term, I think all of you obviously before
453160	457000	you've come to this class today, you've heard the term deep learning, but it's important
457000	462720	for you to really understand how this concept of deep learning relates to all of the other
462720	466200	pieces of science that you've learned about so far.
466200	469860	So to do that, we have to start from the very beginning and start by thinking about what
469860	475320	is intelligence at its core, not even artificial intelligence, but just intelligence, right?
475320	482320	So the way I like to think about this is that I like to think that intelligence is the ability
482320	488800	to process information which will inform your future decision making abilities.
488800	492280	Now that's something that we as humans do every single day.
492280	497720	Now artificial intelligence is simply the ability for us to give computers that same
497720	503560	ability to process information and inform future decisions.
503560	508600	Now machine learning is simply a subset of artificial intelligence.
508600	513760	The way you should think of machine learning is just as the programming ability or let's
513760	521720	say even simpler than that, machine learning is the science of trying to teach computers
521720	527200	how to do that processing of information and decision making from data.
527200	532200	So instead of hard coding some of these rules into machines and programming them like we
532200	537040	used to do in software engineering classes, now we're going to try and do that processing
537040	542640	of information and informing a future decision making abilities directly from data.
542640	547720	And then going one step deeper, deep learning is simply the subset of machine learning which
547720	549880	uses neural networks to do that.
549880	556040	It uses neural networks to process raw pieces of data, now unprocessed data, and allows
556040	561600	them to ingest all of those very large data sets and inform future decisions.
561600	565440	Now that's exactly what this class is really all about.
565440	570120	If you think of, if I had to summarize this class in just one line, it's all about teaching
570120	576640	machines how to process data, process information, and inform decision making abilities from
576640	580720	that data and learn it from that data.
580720	585720	Now this program is split between really two different parts.
585720	590800	So you should think of this class as being captured with both technical lectures, which
590800	595120	for example this is one part of, as well as software labs.
595120	599480	We'll have several new updates this year as I mentioned earlier just covering the rapid
599480	604040	changing of advances in AI and especially in some of the later lectures you're going
604040	605120	to see those.
605120	611360	The first lecture today is going to cover the foundations of neural networks themselves,
611360	614800	starting with really the building blocks of every single neural network which is called
614800	616160	the Perceptron.
616160	621520	And finally we'll go through the week and we'll conclude with a series of exciting
621520	625920	guest lectures from industry leading sponsors of the course.
625920	632920	And finally on the software side, after every lecture you'll also get software experience
632920	637440	and project building experience to be able to take what we teach in lectures and actually
637440	643760	deploy them in real code and actually produce based on the learnings that you find in this
643760	647800	lecture and at the very end of the class from the software side you'll have the ability
647800	653680	to participate in a really fun day at the very end which is the project pitch competition.
653680	658400	It's kind of like a shark tank style competition of all of the different projects from all
658400	661720	of you and win some really awesome prizes.
661720	666920	So let's step through that a little bit briefly, this is the syllabus part of the lecture.
666920	671600	So each day we'll have dedicated software labs that will basically mirror all of the
671600	675720	technical lectures that we go through just helping you reinforce your learnings and these
675720	681840	are coupled with each day again coupled with prizes for the top performing software solutions
681840	683420	that are come up in the class.
683420	688400	This is going to start with today with lab one and it's going to be on music generation
688400	693760	so you're going to learn how to build a neural network that can learn from a bunch of musical
693760	701840	songs, listen to them and then learn to compose brand new songs in that same genre.
701840	707200	Tomorrow lab two on computer vision you're going to learn about facial detection systems,
707200	712720	you'll build a facial detection system from scratch using convolutional neural networks,
712720	717960	you'll learn what that means tomorrow and you'll also learn how to actually de-bias,
717960	722960	remove the biases that exist in some of these facial detection systems which is a huge problem
722960	727000	for the state of the art solutions that exist today.
727000	732440	And finally a brand new lab at the end of the course will focus on large language models
732440	738000	where you're actually going to take a billion multi-billion parameter large language model
738000	744440	and fine tune it to build an assistive chatbot and evaluate a set of cognitive abilities
744440	751320	ranging from mathematics abilities to scientific reasoning to logical abilities and so on.
751320	756640	And finally at the very very end there will be a final project pitch competition for up
756640	762400	to five minutes per team and all of these are accompanied with great prizes so definitely
762400	765280	there will be a lot of fun to be had throughout the week.
765280	769520	There are many resources to help with this class, you'll see them posted here you don't
769520	773320	need to write them down because all of the slides are already posted online.
773320	779440	Please post to Piazza if you have any questions and of course we have an amazing team that
779440	783640	is helping teach this course this year and you can reach out to any of us if you have
783640	784640	any questions.
784640	786600	The Piazza is a great place to start.
786600	792760	Myself and Ava will be the two main lectures for this course Monday through Wednesday especially
792760	797160	and we'll also be hearing some amazing guest lectures on the second half of the course
797160	801680	which definitely you would want to attend because they really cover the really state
801720	808920	of the art sides of deep learning that's going on in industry outside of academia.
808920	812840	And very briefly just want to give a huge thanks to all of our sponsors who without
812840	816560	their support this course like every year would not be possible.
816560	822000	Okay so now let's start with the fun stuff and my favorite part of the course which is
822000	828400	the technical parts and let's start by just asking ourselves a question right which is
828400	830400	you know why do we care about all of this?
830400	831620	Why do we care about deep learning?
831620	836920	Why did you all come here today to learn and to listen to this course?
836920	841800	So to understand I think we again need to go back a little bit to understand how machine
841800	849800	learning used to be performed right so machine learning typically would define a set of features
849800	854800	or you can think of these as kind of a set of things to look for in an image or in a
854800	856360	piece of data.
856400	861440	Actually these are hand engineered so humans would have to define these themselves and
861440	865800	the problem with these is that they tend to be very brittle in practice just by nature
865800	867480	of a human defining them.
867480	871360	So the key idea of deep learning and what you're going to learn throughout this entire
871360	877640	week is this paradigm shift of trying to move away from hand engineering features and rules
877640	883040	that computers should look for and instead trying to learn them directly from raw pieces
883040	884040	of data.
884040	889760	What are the patterns that we need to look at in data sets such that if we look at those
889760	894800	patterns we can make some interesting decisions and interesting actions can come out.
894800	900240	So for example if we wanted to learn how to detect faces we might if you think even how
900240	904040	you would detect faces right if you look at a picture what are you looking for to detect
904040	905040	a face?
905040	909320	You're looking for some particular patterns you're looking for eyes and noses and ears
909320	913240	and when those things are all composed in a certain way you would probably deduce that
913240	918440	that's a face right computers do something very similar so they have to understand what
918440	923520	are the patterns that they look for what are the eyes and noses and ears of those pieces
923520	931080	of data and then from there actually detect and predict from them.
931080	937200	So the really interesting thing I think about deep learning is that these foundations for
937200	942480	doing exactly what I just mentioned picking out the building blocks picking out the features
942480	947880	from raw pieces of data and the underlying algorithms themselves have existed for many
947880	949720	many decades.
949720	955760	Now the question I would ask at this point is so why are we studying this now and why
955760	960560	is all of this really blowing up right now and exploding with so many great advances?
960560	965400	Well for one there's three things right number one is that the data that is available to
965400	971160	us today is significantly more pervasive these models are hungry for data you're going to
971200	975880	learn about this more in detail but these models are extremely hungry for data and we're
975880	981560	living in a world right now quite frankly where data is more abundant than it has ever
981560	983840	been in our history.
983840	990080	Now secondly these algorithms are massively compute hungry and they're massively parallelizable
990080	996040	which means that they have greatly benefited from compute hardware which is also capable
996120	1002280	of being parallelized the particular name of that hardware it's called a GPU right GPUs
1002280	1008800	can run parallel processing streams of information and are particularly amenable to deep learning
1008800	1014440	algorithms and the abundance of GPUs and that compute hardware has also pushed forward what
1014440	1019840	we can do in deep learning and finally the last piece is the software right it's the
1019840	1026520	open source tools that are really used as the foundational building blocks of deploying
1026520	1030560	and building all of these underlying models that you're going to learn about in this course
1030560	1035160	and those open source tools have just become extremely streamlined making this extremely
1035160	1042680	easy for all of us to learn about these technologies within an amazing one week course like this.
1042680	1046120	So let's start now with understanding now that we have some of the background let's
1046120	1051200	start with understanding exactly what is the fundamental building block of a neural
1051200	1057720	network now that building block is called a perceptron right every single percept every
1057720	1062960	single neural network is built up of multiple perceptrons and you're going to learn how
1062960	1067800	those perceptrons number one compute information themselves and how they connect to these much
1067800	1072360	larger billion parameter neural networks.
1072360	1077320	So the key idea of a perceptron or even simpler think of this as a single neuron right so
1077320	1083100	a neural network is composed of many many neurons and a perceptron is just one neuron.
1083100	1088200	So that idea of a perceptron is actually extremely simple and I hope that by the end of today
1088200	1094440	this idea and this processing of a perceptron becomes extremely clear to you.
1094440	1099600	So let's start by talking about just the forward propagation of information through a single
1099600	1106360	neuron now single neurons ingest information they can actually ingest multiple pieces of
1106360	1111160	information so here you can see this neuron taking as input three pieces of information
1111160	1119520	x1 x2 and xm right so we define the set of inputs called x1 through m and each of these
1119520	1125160	inputs each of these numbers is going to be element wise multiplied by a particular weight
1125160	1130240	so this is going to be denoted here by w1 through wm so this is a corresponding weight
1130240	1134840	for every single input and you should think of this as really you know every weight being
1134840	1141760	assigned to that input right the weights are part of the neuron itself now you multiply
1141760	1146360	all of these inputs with their weights together and then you add them up we take this single
1146360	1151400	number after that addition and you pass it through what's called a nonlinear activation
1151400	1160560	function to produce your final output which here we're calling y now what I just said
1160560	1165320	is not entirely correct right so I missed out one critical piece of information that
1165320	1169520	piece of information is that we also have what you can see here is called this bias
1169520	1176720	term that bias term is actually what allows your neural neuron to shift its activation
1176720	1182960	function horizontally on that x axis if you think of it right so on the right side you
1182960	1188560	can now see this diagram illustrating mathematically that single equation that I talked through
1188560	1192520	kind of conceptually right now you can see it mathematically written down as one single
1192520	1198000	equation and we can actually rewrite this using linear algebra using vectors and dot
1198000	1203280	products so let's do that right so now our inputs are going to be described by a capital
1203280	1209640	x which is simply a vector of all of our inputs x1 through xm and then our weights are going
1209640	1216960	to be described by a capital w which is going to be w1 through wm the input is obtained
1216960	1223920	by taking the dot product of x and w right that dot product does that element wise multiplication
1223920	1229480	and then adds sums all of the the element wise multiplications and then here's the missing
1229520	1234120	piece is that we're now going to add that bias term here we're calling the bias term
1234120	1240520	w zero right and then we're going to apply the nonlinearity which here denoted is z or g excuse
1240520	1246720	me so I've mentioned this nonlinearity a few times this activation function let's dig into it a
1246720	1251880	little bit more so we can understand what is actually this activation function doing well I
1251880	1256480	said a couple things about it I said it's a nonlinear function right here you can see one
1256520	1264200	example of an activation function one common one commonly used activation function is called
1264200	1268000	the sigmoid function which you can actually see here on the bottom right hand side of the
1268000	1274600	screen the sigmoid function is very commonly used because its outputs right so it takes as input
1274600	1281400	any real number the x-axis is infinite plus or minus but on the y-axis it basically squashes
1281720	1288320	every input x into a number between 0 and 1 so it's actually a very common choice for things
1288320	1292880	like probability distributions if you want to convert your answers into probabilities or learn
1292880	1299040	or teach a neuron to learn a probability distribution but in fact there are actually many
1299040	1303960	different types of nonlinear activation functions that are used in neural networks and here are
1303960	1308040	some common ones and and again throughout this presentation you'll see these little
1308080	1312360	TensorFlow icons actually throughout the entire course you'll see these TensorFlow icons on the
1312360	1318360	bottom which basically just allow you to relate some of the foundational knowledge that we're
1318360	1323360	teaching in the lectures to some of the software labs and this might provide a good starting point
1323360	1328920	for a lot of the pieces that you have to do later on in the software parts of the class so the
1328920	1332840	sigmoid activation which we talked about in the last slide here it's shown on the left-hand side
1332840	1336960	right this is very popular because of the probability distributions right it squashes
1337000	1343320	everything between 0 and 1 but you see two other very common types of activation functions in the
1343320	1349080	middle and the right-hand side as well so the other very very common one probably this is the one
1349080	1353440	now that's the most popular activation function is now on the far right-hand side it's called the
1353440	1358840	relu activation function or also called the rectified linear unit so basically it's linear
1358840	1365160	everywhere except there's a nonlinearity at x equals 0 so there's a kind of a step or a great
1365200	1371160	discontinuity right so benefit of this very easy to compute it still has the nonlinearity which
1371160	1376400	we kind of need and we'll talk about why we need it in one second but it's very fast right just
1376400	1382320	two linear functions piecewise combined with each other okay so now let's talk about why we need
1382320	1387600	a nonlinearity in the first place why why not just deal with a linear function that we pass all
1387600	1393120	of these inputs through so the point of the activation function even at all why do we have
1393160	1401200	this is to introduce nonlinearities in of itself so what we want to do is to allow our neural
1401200	1407600	network to deal with nonlinear data right our neural networks need the ability to deal with
1407600	1414400	nonlinear data because the world is extremely nonlinear right this is important because you
1414400	1419840	know if you think of the real world real datasets this is just the way they are right if you look
1419880	1424600	at datasets like this one green and red points right and I ask you to build a neural network
1424600	1431720	that can separate the green and the red points this means that we actually need a nonlinear
1431720	1437920	function to do that we cannot solve this problem with a single line right in fact if we used linear
1437920	1445160	linear functions as your activation function no matter how big your neural network is it's still
1445160	1449800	a linear function because linear functions combined with linear functions are still linear so
1449840	1454120	no matter how deep or how many parameters your neural network has the best they would be able
1454120	1459160	to do to separate these green and red points would look like this but adding nonlinearities
1459160	1465000	allows our neural networks to be smaller by allowing them to be more expressive and capture
1465000	1471640	more complexities in the datasets and this allows them to be much more powerful in the end so let's
1471640	1476680	understand this with a simple example imagine I give you now this trained neural network so what
1476680	1480720	does it mean trained neural network it means now I'm giving you the weights right not only the
1480720	1485920	inputs but I'm going to tell you what the weights of this neural network are so here let's say the
1485920	1493800	bias term w0 is going to be 1 and our w vector is going to be 3 and negative 2 right these are
1493800	1497160	just the weights of your trained neural network well let's worry about how we got those weights
1497160	1505640	in a second but this network has two inputs x1 and x2 now if we want to get the output of this
1505680	1510400	neural network all we have to do simply is to do the same story that we talked about before
1510400	1519200	right it's dot product inputs with weights add the bias and apply the nonlinearity right and
1519200	1523360	those are the three components that you really have to remember as part of this class right dot
1523360	1530160	product add the bias and apply a nonlinearity that's going to be the process that keeps repeating
1530200	1536040	over and over and over again for every single neuron after that happens that neuron was going
1536040	1542400	to output a single number right now let's take a look at what's inside of that nonlinearity it's
1542400	1549600	simply a weighted combination of those of those inputs with those weights right so if we look
1549600	1557480	at what's inside of g right inside of g is a weighted combination of x and w right added
1557520	1564720	with a bias right that's going to produce a single number right but in reality for any input that
1564720	1569160	this model could see what this really is is a two-dimensional line because we have two
1569160	1577400	parameters in this model so we can actually plot that line we can see exactly how this neuron
1577400	1584480	separates points on these axes between x1 and x2 right these are the two inputs of this model
1584680	1589920	we can see exactly and interpret exactly what this neuron is doing right we can visualize
1589920	1595520	its entire space because we can plot the line that defines this neuron right so here we're
1595520	1602600	plotting when that line equals zero and in fact if I give you if I give that neuron in fact a
1602600	1608120	new data point here the new data point is x1 equals negative one and x2 equals two just an
1608120	1612680	arbitrary point in this two-dimensional space we can plot that point in the two-dimensional space
1612880	1618840	and depending on which side of the line it falls on it tells us you know what the what the answer
1618840	1623440	is going to be what the sign of the answer is going to be and also what the answer itself is
1623440	1628840	going to be right so if we follow that that equation written on the top here and plug in negative
1628840	1635960	one and two we're going to get one minus three minus four which equals minus six right and when
1635960	1644440	I put that into my non-linearity g I'm going to get a final output of 0.002 right so that
1644440	1648360	that don't worry about the final output that's just going to be the output from that sigmoid
1648360	1653880	function but the important point to remember here is that the sigmoid function actually divides
1653880	1660240	the space into these two parts right it squashes everything between zero and one but it defines
1660400	1668160	it implicitly by everything less than 0.5 and greater than 0.5 depending on if it's on if x is
1668160	1674320	less than zero or greater than zero so depending on which side of the line that you fall on remember
1674320	1680000	the line is when x equals zero the input to the sigmoid is zero if you fall on the left side of the
1680000	1687040	line your output will be less than 0.5 because you're falling on the negative side of the line if
1687120	1692800	your output is if your input is on the right side of the line now your output is going to be
1692800	1699120	greater than 0.5 right so here we can actually visualize this space this is called the feature
1699120	1705280	space of a neural network we can visualize it in its completion right we can totally visualize
1705280	1710320	and interpret this neural network we can understand exactly what it's going to do for any input that
1710320	1715520	it sees right but of course this is a very simple neuron right it's not a neural network it's just
1715520	1721120	one neuron and even more than that it's even a very simple neuron it only has two inputs right
1721760	1726960	so in reality the types of neural neurons that you're going to be dealing with in this course
1726960	1733600	are going to be neurons and neural networks with millions or even billions of these parameters
1733600	1739760	right of these inputs right so here we only have two weights w1 w2 but today's neural networks have
1739840	1746640	billions of these parameters so drawing these types of plots that you see here obviously becomes a
1746640	1753280	lot more challenging it's actually not possible but now that we have some of the intuition behind
1753280	1760080	a perceptron let's start now by building neural networks and seeing how all of this comes together
1760080	1765840	so let's revisit that previous diagram of a perceptron now again if there's only one thing
1765840	1772240	to take away from this lecture right now it's to remember how a perceptron works that equation of
1772240	1776800	a perceptron is extremely important for every single class that comes after today and there's
1776800	1783040	only three steps it's dot product with the inputs add a bias and apply your non-linearity
1784080	1790000	let's simplify the diagram a little bit i'll remove the weight labels from this picture and now you
1790000	1796080	can assume that if i show a line every single line has an associated weight that comes with that
1796080	1801840	line right i'll also remove the bias term for simplicity assume that every neuron has that
1801840	1808880	bias term i don't need to show it and now note that the result here now calling it z which is just the
1810080	1816880	dot product plus bias before the non-linearity is the output is going to be linear first of all
1816960	1821680	it's just a it's just a weighted sum of all those pieces we have not applied the non-linearity yet
1821680	1827680	but our final output is just going to be g of z it's the activation function or non-linear
1827680	1835840	activation function applied to z now if we want to step this up a little bit more and say what if
1835840	1841840	we had a multi output function now we don't just have one output but let's say we want to have two
1841840	1848720	outputs well now we can just have two neurons in this network right every neuron say sees
1848720	1854720	all of the inputs that came before it but now you see the top neuron is going to be predicting an
1854720	1858640	answer and the bottom neuron will predict its own answer now importantly one thing you should
1858640	1865440	really notice here is that each neuron has its own weights right each neuron has its own lines
1865440	1870960	that are coming into just that neuron right so they're acting independently but they can later on
1870960	1882720	communicate if you have another layer right so let's start now by initializing this this process
1882720	1887760	a bit further and thinking about it more programmatically right what if we wanted to program
1887760	1892880	this neural network ourselves from scratch right remember that equation I told you didn't sound
1892880	1898720	very complex it's take a dot product add a bias which is a single number and apply a non-linearity
1898720	1904240	let's see how we would actually implement something like that so to define the layer right we're now
1904240	1912720	going to call this a layer which is a collection of neurons right we have to first define how that
1912720	1918160	information propagates through the network so we can do that by creating a call function here first
1918160	1923920	we're going to actually define the weights for that network right so remember every network every
1923920	1928640	neuron I should say every neuron has weights and a bias right so let's define those first
1929760	1936560	we're going to create the call function to actually see how we can pass information through that
1936560	1942080	layer right so this is going to take as input and inputs right this is like what we previously
1942080	1948640	called x and it's the same story that we've been seeing this whole class right we're going to matrix
1948640	1954240	multiply or take a dot product of our inputs with our weights we're going to add a bias
1955120	1960400	and then we're going to apply a non-linearity it's really that simple right we've now created a
1960400	1968640	single layer neural network right so this this line in particular this is the part that allows us to
1968640	1976800	be a powerful neural network maintaining that non-linearity and the important thing here is to
1976800	1984000	note that modern deep learning toolboxes and libraries already implement a lot of these for
1984000	1989680	you right so it's important for you to understand the foundations but in practice all of that layer
1989680	1995440	or architecture and all of that layer logic is actually implemented in tools like TensorFlow
1995440	2001920	and PyTorch through a dense layer right so here you can see an example of calling or creating
2001920	2009840	initializing a dense layer with two neurons right allowing it to feed in an arbitrary set of
2009840	2016240	inputs here we're seeing these two neurons in a layer being fed three inputs right and in code
2016240	2021360	it's only reduced down to this one line of TensorFlow code making it extremely easy and
2021360	2027760	convenient for us to use these functions and call them so now let's look at our single layered
2027760	2033760	neural network this is where we have now one layer between our input and our outputs right so
2033760	2038960	we're slowly and progressively increasing the complexity of our neural network so that we
2038960	2046240	can build up all of these building blocks right this layer in the middle is called a hidden layer
2046240	2050480	right obviously because you don't directly observe it you don't directly supervise it
2050480	2054960	right you do observe the two input and output layers but your hidden layer is just kind of a
2055920	2062000	a neuron layer that you don't directly observe right it just gives your network more capacity
2062000	2067760	more learning complexity and since we now have a transformation function from inputs
2067760	2074480	to hidden layers and hidden layers to output we now have a two layered neural network right
2074480	2081520	which means that we also have two weight matrices right we don't have just the w1 which we previously
2081520	2086000	had to create this hidden layer but now we also have w2 which does the transformation from hidden
2086000	2094000	layer to output layer yes in hidden you have just linear so there's no it's not is it a perceptron
2094000	2100240	or not yes so every hidden layer also has a nonlinearity accompanied with it right and that's
2100240	2104880	a very important point because if you don't have that perceptron then it's just a very large linear
2104880	2110640	function followed by a final nonlinearity at the very end right so you need that cascading
2110640	2116640	and uh you know overlapping application of nonlinearities that occur throughout the network
2118480	2126560	awesome okay so now let's zoom in look at a single unit in the hidden layer take this one for example
2126560	2132080	let's call it z2 right it's the second neuron in the first layer right it's the same perception
2132080	2138880	that we saw before we compute its answer by taking a dot product of its weights with its inputs
2138880	2145440	adding a bias and then applying a nonlinearity if we took a different hidden node like z3 the one
2145440	2151120	right below it we would compute its answer exactly the same way that we computed z2 except its
2151120	2155120	weights would be different than the weights of z2 everything else stays exactly the same it sees
2155120	2160640	the same inputs but of course you know i'm not going to actually show z3 in this picture and now
2160640	2164240	this picture is getting a little bit messy so let's clean things up a little bit more i'm going to
2164240	2170560	remove all the lines now and replace them just with these these boxes these symbols that will denote
2170560	2176000	what we call a fully connected layer right so these layers now denote that everything in our input
2176000	2180400	is connected to everything in our output and the transformation is exactly as we saw it before
2180400	2188720	dot product bias and nonlinearity and again in code to do this is extremely straightforward
2188720	2193440	with the foundations that we've built up from the beginning of the class we can now just define
2193440	2200080	two of these dense layers right our hidden layer on line one with n hidden units and then our output
2200080	2209680	layer with two hidden output units nonlinearity function does not need to be the same through
2209680	2216320	each layer oftentimes it is because of convenience there's there are some cases where you would want
2216320	2222000	it to be different as well especially in lecture two you're going to see nonlinearity is be different
2222000	2230400	even within the same layer let alone different layers but unless for a particular reason generally
2230400	2237520	convention is there's no need to keep them differently now let's keep expanding our knowledge
2237520	2242240	a little bit more if we now want to make a deep neural network not just a neural network like we
2242240	2246960	saw on the previous side now it's deep all that means is that we're now going to stack these layers
2246960	2252880	on top of each other one by one more and more creating a hierarchical model right the ones
2252880	2258560	where the final output is now going to be computed by going deeper and deeper and deeper into the
2258560	2266080	neural network and again doing this in code again follows the exact same story as before just cascading
2266080	2271200	these tensorflow layers on top of each other and just going deeper into the network
2272000	2279040	okay so now this is great because now we have at least a solid foundational understanding of how to
2279040	2283440	not only define a single neuron but how to define an entire neural network and you should be able
2283440	2290320	to actually explain at this point or understand how information goes from input through an entire
2290320	2296320	neural network to compute an output so now let's look at how we can apply these neural networks
2296320	2302640	to solve a very real problem that I'm sure all of you care about so here's a problem on how we
2302640	2308320	want to build an AI system to learn to answer the following question which is will I pass this
2308320	2315440	class right I'm sure all of you are really worried about this question so to do this let's start with
2315440	2321760	a simple input feature model the feature the two features that let's concern ourselves with are
2321760	2329520	going to be number one how many lectures you attend and number two how many hours you spend on your
2329520	2336640	final project so let's look at some of the past years of this class right we can actually observe
2336640	2343360	how different people have lived in this space right between how many lectures and how much
2343360	2348800	time you spent on your final project and you can actually see every point is a person the color of
2348880	2353520	that point is going to be if they passed or failed the class and you can see and visualize
2353520	2359280	kind of this this feature space if you will that we talked about before and then we have you you
2359280	2367440	follow right here you're the point four five right in between the this this feature space you've
2367440	2372080	attended four lectures and you will spend five hours on the final project and you want to build a
2372080	2378080	neural network to determine given everyone else in the class right that I've seen from all of the
2378080	2383840	previous years you want to help you want to have your neural network help you to understand what is
2383840	2390320	your likelihood that you will pass or fail this class so let's do it we now have all of the building
2390320	2395680	blocks to solve this problem using a neural network let's do it so we have two inputs those inputs
2395680	2400800	are number of lectures you attend and number of hours you spend on your final project it's four
2400800	2409040	and five we can pass those two inputs to our two x1 and x2 variables these are fed into this single
2409040	2415680	layered single hidden layered neural network it has three hidden units in the middle and we can see
2415680	2422800	that the final predicted output probability for you to pass this class is 0.1 or 10 right so very
2422800	2431520	bleak outcome it's not a good outcome the actual probability is one right so attending four out
2431520	2435440	of the five lectures and spending five hours on your final project you actually lived in a part of
2435440	2438800	the feature space which was actually very positive right it looked like you were going to pass the
2438800	2444000	class so what happened here anyone have any ideas so why did the neural network get this so terribly
2444000	2451200	wrong right exactly so this neural network is not trained we haven't shown it any of that data the
2451280	2457200	green and red data right so you should really think of neural networks like babies right before
2457200	2463120	they see data they haven't learned anything there's no expectation that we should have for them to be
2463120	2466880	able to solve any of these types of problems before we teach them something about the world
2467440	2472880	so let's teach this neural network something about the problem first right and to train it we first
2472880	2480000	need to tell our neural network when it's making bad decisions right so we need to teach it right
2480000	2485600	really train it to learn exactly like how we as humans learn in some ways right so we have to inform
2485600	2491440	the neural network when it gets the answer incorrect so that it can learn how to get the answer correct
2492080	2498480	right so the closer the answer is to the ground truth so right so for example the actual value
2498480	2504080	for you passing this class was probability one one hundred percent but it predicted a probability
2504080	2510080	of zero point one we compute what's called a loss right so the closer these two things are together
2510080	2514640	the smaller your loss should be and the and the more accurate your model should be
2516720	2522400	so let's assume that we have data not just from one student but now we have data from many students
2522400	2525840	we many students have taken this class before and we can plug all of them into the neural
2525840	2531360	network and show them all to this to this system now we care not only about how the neural network
2531360	2537200	did on just this one prediction but we care about how it predicted on all of these different people
2537200	2542240	that the neural network has shown in the past as well during this training and learning process
2542880	2549680	so when training your neural network we want to find a network that minimizes the empirical loss
2549680	2555120	between our predictions and those ground truth outputs and we're going to do this on average
2555120	2562640	across all of the different inputs that the that the model has seen if we look at this problem
2562640	2569520	of binary classification right between yeses and nos right will I pass the class or will I not pass
2569520	2575680	the class it's a year zero or one probability and we can use what is called the softmax function
2575680	2582080	or the softmax cross entropy function to be able to inform if this network is getting the answer
2582080	2587280	correct or incorrect right the softmax cross or the cross entropy function think of this as a
2587280	2593360	as an objective function it's a loss function that tells our neural network how far away these two
2593360	2598240	probability distributions are right so the output is a probability distribution we're trying to
2598240	2604000	determine how bad of an answer the neural network is predicting so that we can give it feedback to
2604000	2610880	get a better answer now let's suppose instead of training a or predicting a binary output we want
2610880	2616320	to predict a real valued output like a like any number it can take any number plus or minus
2616320	2623520	infinity so for example if you want to predict the grade that you get in a class right it doesn't
2623520	2628560	necessarily need to be between zero and one or zero and a hundred even right you could now use a
2628560	2633840	different loss in order to produce that value because our outputs are no longer a probability
2633840	2639280	distribution right so for example what you might do here is compute a mean squared error probably
2639520	2644800	mean squared error loss function between your true value or your true grade of the class
2644800	2649680	and the predicted grade right these are two numbers they're not probabilities necessarily
2649680	2655600	you compute their difference you square it to look at a distance between the two an absolute
2655600	2663040	distance right sign doesn't matter and then you can minimize this thing right okay great so let's
2663120	2668800	put all of this loss information with this problem of finding our network weights into a
2668800	2673440	unified problem and a unified solution to actually train our neural network
2674880	2680800	so we know that we want to find a neural network that will solve this problem on all this data
2680800	2686640	on average right that's how we contextualize this problem earlier in the lectures this means
2686640	2692640	effectively that we're trying to solve or we're trying to find what are the weights for our neural
2692640	2697920	network what are this this big vector w that we talked about in earlier in the lecture what is
2697920	2705280	this vector w compute this vector w for me based on all of the data that we have seen right now
2705280	2712880	the vector w is also going to determine what is the loss right so given a single vector w
2712880	2719440	we can compute how bad is this neural network performing on our data right so what is the loss
2719440	2724800	what is this deviation from the ground truth of our network uh based on where it should be
2726160	2734560	now remember that w is just a group of a bunch of numbers right it's a very big list of numbers
2734560	2741760	a list of weights uh for every single layer and every single neuron in our neural network right
2741760	2747360	so it's just a very big list or a vector of weights we want to find that vector what is that
2747360	2752400	vector based on a lot of data that's the problem of training a neural network and remember our
2752400	2758880	loss function is just a simple function of our weights if we have only two weights in our neural
2758880	2764800	network like we saw earlier in the slide then we can plot the loss landscape over this two-dimensional
2764800	2771760	space right so we have two weights w one and w two and for every single configuration or setting of
2771760	2777120	those two weights our loss will have a particular value which here we're showing is the height of
2777120	2784480	this graph right so for any w one and w two what is the loss and what we want to do is find the
2784480	2791440	lowest point what is the best loss where what are the weights such that our loss will be as good as
2791440	2796800	possible so the smaller the loss the better so we want to find the lowest point in this graph
2798480	2804400	now how do we do that right so the way this works is we start somewhere in this space we
2804400	2811280	don't know where to start so let's pick a random place to start right now from that place let's
2811280	2817520	compute what's called the gradient of the landscape at that particular point this is a very local
2817520	2825120	estimate of where is going up basically where where is the slope increasing at my current location
2825120	2830240	right that informs us not only where the slope is increasing but more importantly where the slope
2830240	2835520	is decreasing if i negate the direction if i go in the opposite direction i can actually step down
2835520	2842960	into the landscape and change my weights such that i lower my loss so let's take a small step
2842960	2848080	just a small step in the opposite direction of the part that's going up let's take a small step
2848080	2853600	going down and we'll keep repeating this process we'll compute a new gradient at that new point
2853600	2857360	and it will take another small step and we'll keep doing this over and over and over again
2857360	2863440	until we converge at what's called a local minimum right so based on where we started it may not be
2863440	2868720	a global minimum of everywhere in this lost landscape but let's find ourselves now in a local minimum
2868720	2873200	and we're guaranteed to actually converge by following this very simple algorithm at a local
2873200	2879120	minimum so let's summarize now this algorithm this algorithm is called gradient descent let's
2879120	2884720	summarize it first in pseudocode and then we'll look at it in actual code in a second so there's
2884720	2891440	a few steps first step is we initialize our location somewhere randomly in this weight space
2891440	2899200	right we compute the gradient of of our loss at with respect to our weights okay
2899920	2904400	and then we take a small step in the opposite direction and we keep repeating this in a loop
2904400	2908800	over and over and over again and we say we keep we'll keep doing this until convergence right
2908800	2913680	until we stop moving basically and our network basically finds where it's supposed to end up
2914400	2920880	we'll talk about this this small step right so we're multiplying our gradient by what i keep
2920880	2926800	calling is a small step we'll talk about that a bit more about a bit more and later part of this
2926800	2933120	this lecture but for now let's also very quickly show the analogous part in in code as well and
2933120	2938480	it mirrors very nicely right so we'll randomly initialize our weights this happens every time
2938480	2942480	you train a neural network you have to randomly initialize the weights and then you have a loop
2943120	2947600	right here showing it with without even convergence right we're just going to keep looping forever
2948240	2953360	where we say okay we're going to compute the loss at that location compute the gradient so which
2953360	2959360	way is up and then we just negate that gradient multiply it by some what's called learning rate
2959360	2964320	lr denoted here it's a small step and then we take a direction in that small step
2966320	2970560	so let's take a deeper look at this term here this is called the gradient right this tells us
2970560	2976560	which way is up in that landscape and this again it tells us even more than that it tells us how
2976560	2983520	is our landscape how is our loss changing as a function of all of our weights but i actually
2983520	2988320	have not told you how to compute this so let's talk about that process that process is called
2988320	2994240	back propagation we'll go through this very very briefly and we'll start with the simplest neural
2994240	2999440	network uh that's possible right so we already saw the simplest building block which is a single
2999440	3004880	neuron now let's build the simplest neural network which is just a one neuron neural network right
3004880	3010320	so it has one hidden neuron it goes from input to hidden neuron to output and we want to compute
3010320	3016960	the gradient of our loss with respect to this weight w2 okay so i'm highlighting it here
3016960	3023600	so we have two weights let's compute the gradient first with respect to w2 and that tells us how
3023600	3031200	much does a small change in w2 affect our loss does our loss go up or down if we move our w2
3031200	3036880	a little bit in one direction or another so let's write out this derivative we can start by applying
3036880	3044480	the chain rule backwards from the loss through the output and specifically we can actually decompose
3044480	3050960	this law this derivative this gradient into two parts right so the first part we're decomposing
3050960	3062160	it from dj dw2 into dj dy right which is our output multiplied by dy dw2 right this is all
3062160	3068800	possible right it's a chain rule it's a bit i'm just reciting a chain rule here from calculus
3068800	3074160	this is possible because y is only dependent on the previous layer and now let's suppose we don't
3074160	3078800	want to do this for w2 but we want to do it for w1 we can use the exact same process right but now
3078800	3085200	it's one step further right we'll now replace w2 with w1 we need to apply the chain rule yet again
3085200	3089760	once again to decompose the problem further and now we propagate our old gradient that we computed
3089760	3095680	for w2 all the way back one more step uh to the weight that we're interested in which in this
3095680	3101920	case is w1 and we keep repeating this process over and over again propagating these gradients
3101920	3107840	backwards from output to input to compute ultimately what we want in the end is this
3107840	3113520	derivative of every weight so the lot the derivative of our loss with respect to every
3113520	3117680	weight in our neural network this tells us how much does a small change in every single weight
3117680	3122400	in our network affect the loss does our loss go up or down if we change this weight a little bit
3122400	3131840	in this direction or a little bit in that direction yes neuron and perceptron are the same so
3131840	3137040	typically people say neural network which is why like a single neuron it's also gotten popularity
3137040	3142720	but originally a perceptron is the the formal term the two terms are identical
3145200	3150640	okay so now we've covered a lot so we've covered the forward propagation of information through a
3150640	3156560	neuron and through a neural network all the way through and we've covered now the back propagation
3156560	3161680	of information to understand how we should change every single one of those weights in our neural
3161680	3169280	network to improve our loss so that was the back prop algorithm in theory it's actually
3169280	3173760	pretty simple it's just a chain rule right there's nothing there's actually nothing more than than
3173760	3178800	just the chain rule and the nice part is that deep learning libraries actually do this for you so
3178800	3182400	they compute back prop for you you don't actually have to implement it yourself which is very
3182400	3188080	convenient but now it's important to touch on even though the theory is actually not that complicated
3188080	3193200	for back propagation let's touch on it now from practice now thinking a little bit towards your
3193200	3197680	own implementations when you want to implement these neural networks what are some insights
3198400	3203520	so optimization of neural networks in practice is a completely different story it's not
3203520	3208720	straightforward at all and in practice it's very difficult and usually very computationally
3208720	3213840	intensive to do this back prop algorithm so here's an illustration from a paper that came
3213840	3219280	out a few years ago that actually attempted to visualize a very deep neural networks loss
3219280	3225040	landscape so previously we had that other depiction visualization of how a neural network would
3225040	3230960	look in a two-dimensional landscape real neural networks are not two-dimensional they're hundreds
3230960	3237440	or millions or billions of dimensions and now what would those lost landscapes look like you can
3237440	3242080	actually try some clever techniques to actually visualize them this is one paper that attempted
3242160	3249600	to do that and it turns out that they look extremely messy right the important thing is that
3249600	3254240	if you do this algorithm and you start in a bad place depending on your neural network you may
3254240	3259760	not actually end up in the global solution right so your initialization matters a lot
3259760	3264320	and you need to kind of traverse these local minima and try to try and help you find the
3264320	3271280	global minima or even more than that you need to construct neural networks that have lost landscapes
3271280	3275600	that are much more amenable to optimization than this one right so this is a very bad loss
3275600	3280400	landscape there are some techniques that we can apply to our neural networks that smooth out
3280400	3286160	their lost landscape and make them easier to optimize so recall that update equation that
3286160	3290800	we talked about earlier with gradient descent right so there is this parameter here that we
3290800	3295280	didn't talk about we we described this as the little step that you could take right so it's a
3295280	3300400	small number that you multiply with the direction which is your gradient it just tells you okay i'm
3300400	3304400	not going to just go all the way in this direction i'll just take a small step in this direction
3305040	3310800	so in practice even setting this value right it's just one number setting this one number can be
3310800	3318560	rather difficult right if we set the learning rate too small then the model can get stuck
3318560	3323520	in these local minima right so here it starts and it kind of gets stuck in this local minima
3323520	3328400	it converges very slowly even if it doesn't get stuck if the learning rate is too large it can
3328400	3334560	kind of overshoot and in practice it even diverges and explodes and you don't actually ever find
3334560	3341520	any minima now ideally what we want is to use learning rates that are not too small and not too
3341520	3347680	large to so they're large enough to basically avoid those local minima but small enough such
3347680	3353280	that they won't diverge and they will actually still find their way into the global minima so
3353280	3357360	something like this is what you should intuitively have in mind right so something i can overshoot
3357360	3363200	the local minimas but find itself into a better minima and then finally stabilize itself there
3363760	3368080	so how do we actually set these learning rates right in practice what does that process look
3368080	3373840	like now idea number one is is very basic right try a bunch of different learning rates and see
3373840	3379840	what works and that's actually a not a bad process in practice it's one of the processes that people
3379840	3385600	use so that that's uh that's interesting but let's see if we can do something smarter than this
3385600	3392320	and let's see how we can design algorithms that can adapt to the landscapes right so in practice
3392320	3398080	there's no reason why there should be a single number right can we have learning rates that adapt
3398080	3403840	to the model to the data to the landscapes to the gradients that it's seeing around so this means
3403840	3410160	that the learning rate may actually increase or decrease as a function of the gradients in the
3410160	3415680	loss function right how fast we're learning or many other options right there are many different
3415680	3422080	ideas that could be done here and in fact there are many widely used different procedures or
3422080	3428000	methodologies for setting the learning rate and during your labs we actually encourage you to try
3428000	3432480	out some of these different ideas for different types of learning rates and and even play around
3432480	3436720	with you know what what's the effect of increasing or decreasing your learning rate you'll see very
3436720	3441280	striking differences
3448160	3453680	so so a few things what number one is that it's not a closed space right so there's an infinite
3453680	3460080	every every weight can be plus or minus up to infinity right so even if it was a one-dimensional
3460080	3466240	neural network with just one weight it's not a closed space in practice it's even worse than that
3466240	3473920	because you have billions of dimensions right so not only is your space your support system in
3473920	3479520	one dimension is it infinite but you now have billions of infinite dimensions right or billions
3479520	3484800	of infinite support spaces so it's not something that you can just like search every weight every
3484800	3490000	possible weight in your neural in your configuration or what is every possible weight that this neural
3490000	3495760	network could take and let me test them out because it's not practical to do even for a very small
3495760	3503120	neural network in practice so in your labs you can really try to put all of this information
3503920	3511280	in this picture into practice which defines your model number one right here defines your
3511280	3516480	optimizer which previously we denoted as this gradient descent optimizer here we're calling it
3517120	3523040	stochastic gradient center SGD we'll talk about that more in a second and then also note that
3523040	3529200	your optimizer which here we're calling SGD could be any of these adaptive optimizers you can swap
3529200	3533360	them out and you should swap them out you should test different things here to see the impact of
3533360	3540080	these different methods on your training procedure and you'll gain very valuable intuition for the
3540080	3544480	different insights that will come with that as well so I want to continue very briefly just for
3544480	3549840	the end of this lecture to talk about tips for training neural networks in practice and how we
3549920	3556800	can focus on this powerful idea of really what's called batching data right not seeing all of your
3556800	3563520	data but now talking about a topic called batching so to do this let's very briefly revisit this
3563520	3568960	gradient descent algorithm the gradient is actually compute this gradient computation the back prop
3568960	3574800	algorithm I mentioned this earlier it's a very computationally expensive operation and it's
3574800	3579760	even worse because we now are we previously described in a way where we would have to compute
3579760	3585520	it over a summation over every single data point in our entire data set right that's how we defined
3585520	3589760	it with the loss functions an average over all of our data points which means that we're summing
3589760	3595280	over all of our data points the gradients so in most real-life problems this would be completely
3595280	3600480	infeasible to do because our data sets are simply too big and the models are too big to compute those
3600480	3604880	gradients on every single iteration remember this isn't just a one-time thing right it's every
3604880	3609840	single step that you do you keep taking small steps so you keep me you keep needing to repeat this
3609840	3615760	process so instead let's define a new gradient descent algorithm called SGD stochastic gradient
3615760	3622240	descent instead of computing the gradient over the entire data set now let's just pick a single
3622240	3628720	training point and compute that one training points gradient right the nice thing about that is that
3628720	3635040	it's much easier to compute that gradient right it only needs one point and the downside is that
3635040	3640480	it's very noisy it's very stochastic since it was computed using just that one example right so you
3640480	3646560	have that that trade-off that exists so what's the middle ground right the middle ground is to take
3646560	3652320	not one data point and not the full data set but a batch of data right so take a what's called a
3652320	3658400	mini batch right this could be something in practice like 32 pieces of data is a common batch size
3658480	3663120	and this gives us an estimate of the true gradient right so you approximate the gradient
3663120	3669600	by averaging the gradient of these 32 samples it's still fast because 32 is much smaller than
3669600	3675040	the size of your entire data set but it's pretty quick now right it's still noisy but it's okay
3675040	3681200	usually in practice because you can still iterate much faster and since b is normally not that large
3681200	3687120	again think of something like in the tens or the hundreds of samples it's very fast to compute this
3687200	3692400	in practice compared to regular gradient descent and it's also much more accurate compared to
3692400	3698240	stochastic gradient descent and the increase in accuracy of this gradient estimation allows
3698240	3703840	us to converge to our solution significantly faster as well right it's not only about the
3703840	3708880	speed it's just about the increase in accuracy of those gradients allows us to get to our solution
3708880	3714720	much faster which ultimately means that we can train much faster as well and we can save compute
3715280	3721840	and the other really nice thing about mini batches is that they allow for parallelizing our
3721840	3726080	computation right and that was a concept that we had talked about earlier in the class as well
3726080	3731520	and here's where it's coming in we can split up those batches right so those 32 pieces of data
3731520	3737040	let's say if our batch size is 32 we can split them up onto different workers right different
3737040	3743440	parts of the GPU can tackle those different parts of our data points this can allow us to
3743440	3750080	basically achieve even more significant speed-ups using GPU architectures and GPU hardware okay
3750080	3754800	finally the last topic I want to talk about before we end this lecture and move on to lecture number
3754800	3760960	two is overfitting right so overfitting is this idea that is actually not a deep learning
3760960	3765840	centric problem at all it's it's a problem that exists in all of machine learning right the key
3765840	3774800	problem is that and the key problem is actually one that addresses how you can accurately define
3775600	3782800	if your model is is actually capturing your true data set right or if it's just learning kind of
3782800	3789600	the subtle details that are kind of spuriously correlating to your data set so set differently
3789600	3794960	let me say it a bit differently now so let's say we want to build models that can learn
3795040	3801920	representations okay from our training data that still generalize to brand new unseen
3802640	3806800	test points right that's the real goal here is we want to teach our model something based on a
3806800	3811200	lot of training data but then we don't want it to do well in the training day we want it to do well
3811200	3815440	when we deploy it into the real world and it's seeing things that it has never seen during training
3816000	3822560	so the concept of overfitting is exactly addressing that problem overfitting means if
3823520	3829760	if your model is doing very well on your training data but very badly in testing it
3829760	3834800	that means it's overfitting it's overfitting to the training data that it saw on the other hand
3834800	3840080	there's also underfitting right on the left hand side you can see basically not fitting the data
3840080	3844960	enough which means that you know you're going to achieve very similar performance on your testing
3844960	3851200	distribution but both are underperforming the actual capabilities of your system now ideally
3851280	3856320	you want to end up somewhere in the middle which is not too complex where you're memorizing all of
3856320	3861520	the nuances in your training data like on the right but you still want to continue to perform
3862080	3868000	well even based on the brand new data so you're not underfitting as well so to a to actually
3868000	3871520	address this problem in neural networks and in machine learning in general there's a few
3871520	3876240	different ways that you should be aware of and how to do it because you'll need to apply them
3876240	3881360	as part of your solutions in your software labs as well so the key concept here is called
3881360	3887840	regularization right regularization is a technique that you can introduce and said very simply all
3887840	3896880	regularization is is a way to discourage your model from from these nuances in your training
3896880	3901840	data from being learned that's all it is and as we've seen before it's actually critical for our
3901840	3906960	models to be able to generalize you know not just on training data but really what we care about
3906960	3913120	is the testing data so the most popular regularization technique that's important for you to understand
3913120	3918800	is this very simple idea called dropout let's revisit this picture of a deep neural network
3918800	3923840	that we've been seeing all lecture right in dropout our training during training what we're
3923840	3929760	going to do is randomly set some of the activations right these outputs of every single neuron
3930720	3936880	to zero which is randomly going to set them to zero with some probability right so let's say
3936880	3942800	50 percent is our probability that means that we're going to take all of the activation insert
3942800	3948400	in our neural network and with a probability of 50 before we pass that activation on to the next
3948400	3954800	neuron we're just going to set it to zero and not pass on anything so effectively 50 percent of the
3954880	3960720	neurons are going to be kind of shut down or killed in a forward pass and you're only going
3960720	3966560	to forward pass information with the other 50 percent of your neurons so this idea is extremely
3966560	3971360	powerful actually because it lowers the capacity of our neural network it not only lowers the
3971360	3976480	capacity of our neural network but it's dynamically lowering it because on the next iteration we're
3976480	3981280	going to pick a different 50 percent of neurons that we drop out so constantly the network is going
3981280	3987280	to have to learn to build pathways different pathways from input to output and that it can't
3987280	3992880	rely on any small any small part of the features that are present in any part of the training
3992880	3997840	dataset too extensively right because it's constantly being forced to find these different
3997840	4004400	pathways with random probabilities so let's drop out the second regularization technique
4004400	4008720	is going to be this notion called early stopping which is actually something that
4008720	4013040	is model agnostic you can apply this to any type of model as long as you have a testing set that
4013040	4019600	you can play around with so the idea here is that we have already a pretty formal mathematical
4019600	4024880	definition of what it means to overfit right overfitting is just when our model starts to
4024880	4031440	perform worse on our test set that's really all it is right so what if we plot over the course
4031440	4036160	of training so x-axis is as we're training the model let's look at the performance on both the
4036160	4041840	training set and the test set so in the beginning you can see that the training set and the test
4041840	4046960	set are both going down and they continue to go down which is excellent because it means that
4046960	4052720	our model is getting stronger eventually though what you'll notice is that the test loss plateaus
4053360	4058800	and starts to increase on the other hand the training loss there's no reason why the training
4058800	4064320	loss should ever need to stop going down right training losses generally always continue to
4064400	4070640	decay as long as there is capacity in the neural network to learn those differences right but the
4070640	4075680	important point is that this continues for the rest of training and we want to basically
4076400	4080080	we care about this point right here right this is the really important point because
4081040	4085600	this is where we need to stop training right after this point this is the happy medium because
4085600	4091920	after this point we start to overfit on parts of the data where our training accuracy becomes
4091920	4097280	actually better than our testing accuracy so our testing accuracy is going bad it's getting worse
4097280	4101840	but our training accuracy is still improving so it means overfitting on the other hand on the left
4101840	4108560	hand side this is the opposite problem right we have not fully utilized the capacity of our model
4108560	4114480	and the testing accuracy can still improve further right this is a very powerful idea but it's actually
4114480	4119280	extremely easy to implement in practice because all you really have to do is just monitor the loss
4119920	4123040	of over the course of training right and you just have to pick the model
4123040	4125440	where the testing accuracy starts to get worse
4127520	4132560	so i'll conclude this lecture by just summarizing three key points that we've covered in the class
4132560	4137840	so far and this is a very gem packed class so the entire week is going to be like this
4137840	4142960	and today is just the start so so far we've learned the fundamental building blocks of neural
4142960	4147760	networks starting all the way from just one neuron also called a perceptron we learned that we can
4147760	4154160	stack these systems on top of each other to create a hierarchical network and how we can
4154160	4158720	mathematically optimize those types of systems and then finally in the very last part of the
4158720	4163840	class we talked about just techniques tips and techniques for actually training and applying
4163840	4169680	these systems into practice now in the next lecture we're going to hear from Ava on deep
4169680	4177040	sequence modeling using RNNs and also a really new and exciting algorithm and type of model called
4177040	4182880	the transformer which is built off of this principle of attention you're going to learn
4182880	4187600	about in the next class but let's for now just take a brief pause and let's resume in about
4187600	4196400	five minutes just so we can switch speakers and Ava can start her presentation okay thank you
