start	end	text
0	15440	Hi everyone, and welcome to lecture 4 of MIT 6S191.
15440	19640	In today's lecture, we're going to be talking about how we can use deep learning and neural
19640	24800	networks to build systems that not only look for patterns in data, but actually can go
24800	31760	a step beyond this to generate brand new synthetic examples based on those learned patterns.
31760	36840	And this, I think, is an incredibly powerful idea, and it's a particular subfield of deep
36840	42040	learning that has enjoyed a lot of success and gotten a lot of interest in the past couple
42040	47520	of years, but I think there's still tremendous, tremendous potential of this field of deep
47520	52180	generative modeling in the future and in the years to come, particularly as we see these
52180	57540	types of models and the types of problems that they tackle becoming more and more relevant
57540	60540	in a variety of application areas.
60540	68000	All right, so to get started, I'd like to consider a quick question for each of you.
68000	73500	Here we have three photos of faces, and I want you all to take a moment, look at these
73500	78900	faces, study them, and think about which of these faces you think is real.
78900	80740	Is it the face on the left?
80740	83140	Is it the face in the center?
83140	85300	Is it the face on the right?
85300	87500	Which of these is real?
87500	93340	Well, in truth, each of these faces are not real.
93340	94460	They are all fake.
94460	100040	These are all images that were synthetically generated by a deep neural network.
100040	103660	None of these people actually exist in the real world.
103660	110160	And hopefully, I think you all have appreciated the realism of each of these synthetic images,
110160	114800	and this to me highlights the incredible power of deep generative modeling.
114800	117960	And not only does it highlight the power of these types of algorithms and these types
117960	124560	of models, but it raises a lot of questions about how we can consider the fair use and
124560	130600	the ethical use of such algorithms as they are being deployed in the real world.
130600	137480	So by setting this up and motivating in this way, I'd first, I now like to take a step
137480	143080	back and consider fundamentally what is the type of learning that can occur when we are
143080	147340	training neural networks to perform tasks such as these.
147340	152520	So so far in this course, we've been considering what we call supervised learning problems,
152520	158760	instances in which we are given a set of data and a set of labels associated with that data.
158760	164240	And our goal is to learn a functional mapping that moves from data to labels.
164240	167720	And those labels can be class labels or continuous values.
167720	173120	And in this course, we've been concerned primarily with developing these functional
173120	177200	mappings that can be described by deep neural networks.
177200	183880	But at their core, these mappings could be anything, you know, any sort of statistical function.
183880	189120	The topic of today's lecture is going to focus on what we call unsupervised learning, which
189120	192720	is a new class of learning problems.
192720	197600	And in contrast to supervised settings where we're given data and labels in unsupervised
197600	200800	learning, we're given only data, no labels.
200800	205400	And our goal is to train a machine learning or deep learning model to understand or build
205400	210640	up a representation of the hidden and underlying structure in that data.
210640	216760	And what this can do is it can allow sort of an insight into the foundational structure
216760	218080	of the data.
218080	224960	And then in turn, we can use this understanding to actually generate synthetic examples.
224960	230720	And unsupervised learning beyond this domain of deep generative modeling also extends
230720	236480	to other types of problems and example applications, which you may be familiar with, such as clustering
236480	240880	algorithms or dimensionality reduction algorithms.
240880	244640	Generative modeling is one example of unsupervised learning.
244640	252040	And our goal in this case is to take as input examples from a training set and learn a model
252040	258440	that represents the distribution of the data that is input to that model.
258440	261280	And this can be achieved in two principal ways.
261280	265600	The first is through what is called density estimation, where let's say we are given a
265600	271080	set of data samples and they fall according to some density.
271120	277840	The task for building a deep generative model applied to these samples is to learn the underlying
277840	285240	probability density function that describes how and where these data fall along this distribution.
285240	291480	And we can not only just estimate the density of such a probability density function, but
291480	298320	actually use this information to generate new synthetic samples, where again we are considering
298320	304480	some input examples that fall and are drawn from some training data distribution.
304480	311480	And after building up a model using that data, our goal is now to generate synthetic examples
311480	318920	that can be described as falling within the data distribution modeled by our model.
318920	327840	So the key idea in both these instances is this question of how can we learn a probability
327880	336160	distribution using our model, which we call P model of X, that is so similar to the true
336160	343760	data distribution, which we call P data of X. This will not only enable us to effectively
343760	349440	estimate these probability density functions, but also generate new synthetic samples that
349440	356000	are realistic and match the distribution of the data we're considering.
356000	364080	So this I think summarizes concretely what are the key principles behind generative modeling.
364080	370840	But to understand that how generative modeling may be informative and also impactful, let's
370840	377200	take this idea step further and consider what could be potential impactful applications
377200	381040	and real world use cases of generative modeling.
381040	387640	What generative models enable us as the users to do is to automatically uncover the underlying
387640	390480	structure and features in a data set.
390480	395680	The reason this can be really important and really powerful is often we do not know how
395680	400480	those features are distributed within a particular data set of interest.
400480	405840	So let's say we're trying to build up a facial detection classifier and we're given a data
405840	412080	set of faces, for which we may not know the exact distribution of these faces with respect
412080	418080	to key features like skin tone or pose or clothing items.
418080	425680	And without going through our data set and manually inspecting each of these instances,
425680	430640	our training data may actually be very biased with respect to some of these features without
430640	432960	us even knowing it.
432960	439160	And as you'll see in this lecture and in today's lab, what we can actually do is train generative
439160	445880	models that can automatically learn the landscape of the features in a data set like these,
445880	447600	like that of faces.
447600	453680	And by doing so, actually uncover the regions of the training distribution that are underrepresented
453680	459400	and overrepresented with respect to particular features such as skin tone.
459400	465640	And the reason why this is so powerful is we can actually now use this information to
465640	472280	actually adjust how the data is sampled during training to ultimately build up a more fair
472280	479000	and more representative data set that then will lead to a more fair and unbiased model.
479000	483600	And you'll get practice doing exactly this and implementing this idea in today's lab
483600	486120	exercise.
486120	491560	Another great example in use case where generative models are exceptionally powerful is this
491560	496480	broad class of problems that can be considered outlier or anomaly detection.
496480	501280	One example is in the case of self-driving cars where it's going to be really critical
501280	507400	to ensure that an autonomous vehicle governed and operated by a deep neural network is able
507400	513240	to handle all of the cases that it may encounter on the road, not just, you know, the straight
513240	517800	freeway driving that is going to be the majority of the training data and the majority of the
517800	520880	time the car experiences on the road.
520880	526240	So generative models can actually be used to detect outliers within training distributions
526240	531880	and use this to, again, improve the training process so that the resulting model can be
531880	536480	better equipped to handle these edge cases and rare events.
536480	542280	All right, so hopefully that motivates why and how generative models can be exceptionally
542280	546680	powerful and useful for a variety of real world applications.
546680	551520	To dive into the bulk of the technical content for today's lecture, we're going to discuss
551520	554960	two classes of what we call latent variable models.
554960	560120	Specifically we'll look at autoencoders and generative adversarial networks or GANs.
560120	565560	But before we get into that, I'd like to first begin by discussing why these are called latent
565560	572440	variable models and what we actually mean when we use this word latent.
572440	577080	And to do so, I think really the best example that I've personally come across for understanding
577080	583000	what a latent variable is, is this story that is from Plato's work, the Republic.
583000	586280	And this story is called the myth of the cave or the parable of the cave.
586280	588540	And the story is as follows.
588540	594080	In this myth, there are a group of prisoners and these prisoners are constrained as part
594080	597200	of their prison punishment to face a wall.
597200	602720	And the only things that they can see on this wall are the shadows of particular objects
602720	606000	that are being passed in front of a fire that's behind them.
606000	610200	So behind their heads and out of their line of sight.
610200	613520	And the prisoners, the only thing they're really observing are these shadows on the
613520	614520	wall.
614520	616800	And so to them, that's what they can see.
616800	619680	That's what they can measure and that's what they can give names to.
619680	621320	That's really their reality.
621320	624200	These are their observed variables.
624200	629880	But they can't actually directly observe or measure the physical objects themselves
629880	632280	that are actually casting these shadows.
632280	638640	So those objects are effectively what we can analyze like latent variables.
638640	643560	They're the variables that are not directly observable, but they're the true explanatory
643560	648240	factors that are creating the observable variables, which in this case, the prisoners
648240	652680	are seeing, like the shadows cast on the wall.
652680	659440	And so our question in generative modeling broadly is to find ways of actually learning
659440	665320	these underlying and hidden latent variables in the data, even when we're only given the
665320	667680	observations that are made.
667680	674720	And this is an extremely, extremely complex problem that is very well suited to learning
674720	680800	by neural networks because of their power to handle multidimensional data sets and to
680800	686840	learn combinations of nonlinear functions that can approximate really complex data distributions.
686840	688480	All right.
688480	694040	So we'll first begin by discussing a simple and foundational generative model, which tries
694040	700200	to build up these latent variable representation by actually self encoding the input.
700200	703680	And these models are known as auto encoders.
703680	710280	What an auto encoder is, is it's an approach for learning a lower dimensional latent space
710280	712760	from raw data.
712760	718800	To understand how it works, what we do is we feed in as input raw data, for example,
718800	723120	this image of a two, that's going to be passed through many successive deep neural network
723120	724280	layers.
724280	729320	And at the output of that succession of neural network layers, what we're going to generate
729320	733760	is a low dimensional latent space, a feature representation.
733760	737240	And that's really the goal that we're trying to predict.
737240	742080	And so we can call this portion of the network an encoder, since it's mapping the data,
742080	747960	x, into a encoded vector of latent variables, z.
747960	752200	So let's consider this latent space, z.
752200	757440	If you've noticed, I've represented z as having a smaller size, a smaller dimensionality
757440	763840	as the input x, why would it be important to ensure the low dimensionality of this latent
763840	767120	space, z?
767120	771880	Having a low dimensional latent space means that we are able to compress the data, which
771880	777120	in the case of image data can be, you know, on the order of many, many, many dimensions,
777120	783160	we can compress the data into a small latent vector, where we can learn a very compact and
783160	786600	rich feature representation.
786600	789640	So how can we actually train this model?
789640	794960	Are we going to be able to supervise for the particular latent variables that we're interested
794960	795960	in?
795960	801960	Well, remember that this is an unsupervised problem, where we have training data, but
801960	805320	no labels for the latent space, z.
805320	811280	So in order to actually train such a model, what we can do is learn a decoder network
811280	816640	and build up a decoder network that is used to actually reconstruct the original image
816640	820200	starting from this lower dimensional latent space.
820200	826160	And again, this decoder portion of our autoencoder network is going to be a series of layers,
826160	831240	neural network layers like convolutional layers, that's going to then take this hidden latent
831240	835760	vector and map it back up to the input space.
835760	840520	And we call our reconstructed output x hat, because it's our prediction and it's an imperfect
840520	845360	reconstruction of our input x.
845360	849920	And the way that we can actually train this network is by looking at the original input
849920	856480	x and our reconstructed output x hat and simply comparing the two and minimizing the distance
856480	860040	between these two images.
860040	865120	So for example, we could consider the mean squared error, which in the case of images
865120	871520	means effectively subtracting one image from another and squaring the difference, which
871520	876520	is effectively the pixel wise difference between the input and reconstruction, measuring how
876520	881000	faithful our reconstruction is to the original input.
881000	887360	And again, notice that by using this reconstruction loss, this difference between the reconstructed
887360	895080	output and our original input, we do not require any labels for our data beyond the data itself.
896080	905080	So we can simplify this diagram just a little bit by abstracting away these individual layers
905080	907720	in the encoder and decoder components.
907720	913840	And again, note once again that this loss function does not require any labels, it is
913840	918280	just using the raw data to supervise itself on the output.
918280	924600	And this is a truly powerful idea and a transformative idea because it enables the model to learn
924600	930920	a quantity, the latent variables z, that we're fundamentally interested in, but we
930920	935360	cannot simply observe or cannot readily model.
935360	943160	And when we constrain this latent space to a lower dimensionality, that affects the degree
943160	947440	to which and the faithfulness to which we can actually reconstruct the input.
947440	953160	And the way you can think of this is as imposing a sort of information bottleneck during the
953160	955720	models training and learning process.
955720	960440	And effectively, what this bottleneck does is a form of compression, right?
960440	966280	We're taking the input data, compressing it down to a much smaller latent space, and
966280	969120	then building back up a reconstruction.
969120	973480	And in practice, what this results in is that the lower the dimensionality of your latent
973480	978760	space, the poorer and worse quality reconstruction you're going to get out.
978760	980520	All right.
980520	985960	So in summary, these autoencoder structures use this sort of bottlenecking hidden layer
985960	990040	to learn a compressed latent representation of the data.
990040	994760	And we can self-supervise the training of this network by using what we call a reconstruction
994760	1002560	loss that forces the autoencoder network to encode as much information about the data
1002560	1008840	as possible into a lower dimensional latent space while still being able to build up faithful
1009040	1010560	reconstructions.
1010560	1016680	So the way I like to think of this is automatically encoding information from the data into a
1016680	1020640	lower dimensional latent space.
1020640	1026040	Let's now expand upon this idea a bit more and introduce this concept and architecture
1026040	1031400	of variational autoencoders or VAEs.
1031400	1038200	So as we just saw, traditional autoencoders go from input to reconstructed output.
1038200	1044000	And if we pay closer attention to this latent layer denoted to here in orange, what you
1044000	1048640	can hopefully realize is that this is just a normal layer in a neural network, just
1048640	1050000	like any other layer.
1050000	1051400	It's deterministic.
1051400	1055080	If you're going to feed in a particular input to this network, you're going to get the
1055080	1058280	same output so long as the weights are the same.
1058280	1063800	So effectively, a traditional autoencoder learns this deterministic encoding, which allows
1063800	1068000	for reconstruction and reproduction of the input.
1068000	1075600	In contrast, variational autoencoders impose a stochastic or variational twist on this
1075600	1077200	architecture.
1077200	1083880	And the idea behind doing so is to generate smoother representations of the input data
1083880	1090600	and improve the quality of the not only of reconstructions, but also to actually generate
1090600	1096320	new images that are similar to the input data set, but not direct reconstructions of the
1096320	1097800	input data.
1097800	1104040	And the way this is achieved is that variational autoencoders replace that deterministic layer
1104040	1108600	Z with a stochastic sampling operation.
1108600	1114120	What this means is that instead of learning the latent variables Z directly, for each
1114120	1120680	variable, the variational autoencoder learns a mean and a variance associated with that
1120680	1122280	latent variable.
1122280	1127320	And what those means and variances do is that they parametrize a probability distribution
1127320	1129480	for that latent variable.
1129480	1135600	So what we've done in going from an autoencoder to a variational autoencoder is going from
1135600	1142600	a vector of latent variables Z to learning a vector of means mu and a vector of variances
1142600	1150680	sigma, sigma squared, that parametrize these variables and define probability distributions
1150680	1153800	for each of our latent variables.
1153800	1160240	And the way we can actually generate new data instances is by sampling from the distribution
1160240	1169840	defined by these mus and sigmas to generate a latent sample and get probabilistic representations
1169840	1172280	of the latent space.
1172280	1175960	And what I'd like you to appreciate about this network architecture is that it's very
1175960	1181720	similar to the autoencoder I previously introduced, just that we have this probabilistic twist
1181800	1187760	where we're now performing the sampling operation to compute samples from each of the latent
1187760	1189760	variables.
1189760	1197440	All right, so now because we've introduced this sampling operation, this stochasticity
1197440	1203640	into our model, what this means for the actual computation and learning process of the network,
1203640	1209480	the encoder and decoder, is that they're now probabilistic in their nature.
1209520	1214800	And the way you can think of this is that our encoder is going to be trying to learn
1214800	1223000	a probability distribution of the latent space Z given the input data X, while the decoder
1223000	1229440	is going to take that learned latent representation and compute a new probability distribution
1229440	1233800	of the input X given that latent distribution Z.
1233800	1239400	And these networks, the encoder, the decoder, are going to be defined by separate sets of
1239440	1242480	weights, phi and theta.
1242480	1248920	And the way that we can train this variational autoencoder is by defining a loss function
1248920	1255920	that's going to be a function of the data X as well as the sets of weights phi and theta.
1255920	1262680	And what's key to how VAEs can be optimized is that this loss function is now comprised
1262680	1265160	of two terms instead of just one.
1265160	1270800	We have the reconstruction loss just as before, which again is going to capture this difference
1270800	1276920	between the input and the reconstructed output, and also a new term to our loss, which we
1276920	1282920	call the regularization loss, also called the VAE loss.
1282920	1289920	And to take a look in more detail at why each of these loss terms represents, let's first
1289960	1296320	emphasize again that our overall loss function is going to be defined and taken with respect
1296320	1302320	to the sets of weights of the encoder and decoder and the input X.
1302320	1306080	The reconstruction loss is very similar to before, right?
1306080	1311840	And you can think of it as being driven by a log likelihood function, for example, for
1311840	1316880	image data, the mean squared error between the input and the output.
1316880	1322400	And we can self-supervise the reconstruction loss just as before to force the latent space
1322400	1328800	to learn and represent faithful representations of the input data, ultimately resulting in
1328800	1331520	faithful reconstructions.
1331520	1337120	The new term here, the regularization term, is a bit more interesting and completely new
1337120	1343080	at this stage, so we're going to dive in and discuss it further in a bit more detail.
1343080	1348840	So our probability distribution that's going to be computed by our encoder, q phi of z
1348840	1354840	of x, is a distribution on the latent space z given the data x.
1354840	1361440	And what regularization enforces is that as a part of this learning process, we're going
1361440	1368440	to place a prior on the latent space z, which is effectively some initial hypothesis about
1369160	1373560	what we expect the distributions of z to actually look like.
1373560	1379160	And by imposing this regularization term, what we can achieve is that the model will
1379160	1384600	try to enforce the z's that it learns to follow this prior distribution.
1384600	1388360	And we're going to denote this prior as p of z.
1388360	1392520	This term here, d, is the regularization term.
1392600	1400120	And what it's going to do is it's going to try to enforce a minimization of the divergence
1400120	1406040	or the difference between what the encoder is trying to infer, the probability distribution
1406040	1413000	of z given x, and that prior that we're going to place on the latent variables p of z.
1413720	1420040	And the idea here is that by imposing this regularization factor, we can try to keep the
1420040	1425640	network from overfitting on certain parts of the latent space by enforcing the fact that
1425640	1431240	we want to encourage the latent variables to adopt a distribution that's similar to our prior.
1431960	1437640	So we're going to go through now, you know, both the mathematical basis for this regularization
1437640	1444200	term as well as a really intuitive walkthrough of what regularization achieves to help give you a
1444200	1449080	concrete understanding and an intuitive understanding about why regularization is
1449080	1455880	important and why placing a prior is important. So let's first consider,
1458360	1463320	yeah, so to re-emphasize once again, this regularization term is going to consider
1463320	1468600	the divergence between our inferred latent distribution and the fixed prior we're going to place.
1469560	1477480	So before we to get into this, let's consider what could be a good choice of prior for each of
1477480	1484840	these latent variables. How do we select p of z? I'll first tell you what's commonly done.
1485720	1491560	The common choice that's used very extensively in the community is to enforce the latent variables
1491560	1496600	to roughly follow normal Gaussian distributions, which means that they're going to be a normal
1497160	1503320	distribution centered around mean zero and have a standard deviation and variance of one.
1504600	1510680	By placing these normal Gaussian priors on each of the latent variables and therefore on our
1510680	1516520	latent distribution overall, what this encourages is that the learned encodings learned by the
1516520	1523400	encoder portion of our VAE are going to be sort of distributed evenly around the center of each
1523400	1530760	of the latent variables. And if you can imagine and picture when you have sort of a roughly even
1530760	1537400	distribution around the center of a particular region of the latent space, what this means is
1537400	1543800	that outside of this region, far away, there's going to be a greater penalty and this can result
1543800	1550360	in instances from instances where the network is trying to cheat and try to cluster particular
1550360	1556200	points outside the center, these centers in the latent space, like if it was trying to memorize
1556200	1563160	particular outliers or edge cases in the data. After we place a normal Gaussian prior on our
1563160	1570120	latent variables, we can now begin to concretely define the regularization term component of our
1570120	1577160	loss function. This loss, this term to the loss is very similar in principle to a cross entropy
1577160	1584360	loss that we saw before, where the key is that we're going to be defining the distance function
1584360	1591720	that describes the difference or the divergence between the inferred latent distribution q,
1591720	1598600	phi of z given x and the prior that we're going to be placing p of z. And this term is called
1598600	1604520	the Kublack-Liebler or KL divergence. And when we choose a normal Gaussian prior,
1605800	1612920	we, this results in the KL divergence taking this particular form of this equation here,
1612920	1619240	where we're using the means and sigmas as input and computing this distance metric that captures
1619240	1624680	the divergence of that learned latent variable distribution from the normal Gaussian.
1625320	1631160	All right. So now I really want to spend a bit of time to get some, build up some intuition about
1631160	1638680	how this regularization and works and why we actually want to regularize our VAE and then also
1638680	1645480	why we select a normal prior. All right. So to do this, let's, let's consider the following question.
1645480	1652040	What properties do we want this to achieve from regularization? Why are we actually regularizing
1652760	1660680	our, our network in the first place? The first key property that we want for a generative model like
1660680	1667480	a VAE is what I can, what I like to think of as continuity, which means that if there are points
1667480	1673160	that are represented closely in the latent space, they should also result in similar
1673800	1680760	reconstructions, similar outputs, similar content after they are decoded. You would expect intuitively
1680760	1687000	that regions in the latent space have some notion of distance or similarity to each other. And this
1687000	1693080	indeed is a really key property that we want to achieve with our generative model. The second
1693080	1700040	property is completeness and it's very related to continuity. And what this means is that when
1700040	1707640	we sample from the latent space to decode the latent space into an output, that should result
1707640	1714440	in a meaningful reconstruction, a meaningful, uh, sampled content that is, you know, resembling
1714440	1720920	the original data distribution. You can imagine that if we're sampling from the latent space and
1720920	1727720	just getting garbage out that has no relationship to our input, this could be a huge, huge problem
1727720	1734120	for our model. All right. So with these two properties in mind, continuity and completeness,
1734840	1740440	let's consider the consequences of what can occur if we do not regularize our model.
1742040	1747720	Well, without regularization, what could end up happening with respect to these two properties
1747720	1754040	is that there could be instances of points that are close in latent space, but not similarly
1754040	1761160	decoded. So I'm using this really intuitive illustration where these dots represent abstracted
1761160	1767880	away sort of regions in the latent space. And the shapes that they relate to, you can think of as
1767880	1774440	what is going to be decoded after those instances in the latent space are passed through the decoder.
1775320	1781160	So in this example, we have these two dots, the greenish dot and the reddish dot,
1781160	1785720	that are physically close in latent space, but result in completely different shapes
1785800	1792680	when they're decoded. We also have an instance of this purple point, which when it's decoded,
1792680	1799560	it doesn't result in a meaningful content. It's just a scribble. So by not regularizing,
1799560	1805800	and I'm abstracting a lot away here, and that's on purpose, we could have these instances where
1805800	1812440	we don't have continuity and we don't have completeness. Therefore, our goal with regularization
1812440	1819640	is to be able to realize a model where points that are close in the latent space are not only
1819640	1825400	similarly decoded, but also meaningfully decoded. So for example, here, we have the red dot and the
1825400	1831720	orange dot, which result in both triangle-like shapes, but with slight variations on the on the
1831720	1838520	triangle itself. So this is the intuition about what regularization can enable us to achieve
1838520	1845800	and what are desired properties for these generative models. Okay, how can we actually
1845800	1853000	achieve this regularization? And how does the normal prior fit in? As I mentioned, right,
1853000	1859800	VAEs, they don't just learn the latent variable Z directly. They're trying to encode the inputs
1859800	1865400	as distributions that are defined by mean and variance. So my first question to you is,
1865400	1870200	is it going to be sufficient to just learn mean and variance, learn these distributions?
1870920	1877400	Can that guarantee continuity and completeness? No, and let's understand why.
1879240	1885720	All right, without any sort of regularization, what could the model try to resort to?
1887000	1894600	Remember that the VAE or that the VAE, the loss function is defined by both a reconstruction
1894600	1900520	term and a regularization term. If there is no regularization, you can bet that the model
1900520	1906280	is going to just try to optimize that reconstruction term. So it's effectively going to learn to
1906280	1912920	minimize the reconstruction loss, even though we're encoding the latent variables via mean and variance.
1914360	1921240	And two instances, two consequences of that is that you can have instances where these
1921240	1925960	learned variances for the latent variable end up being very, very, very small,
1926520	1932280	effectively resulting in pointed distributions. And you can also have means that are totally
1932280	1937400	divergent from each other, which result in discontinuities in the latent space. And this can
1937400	1945240	occur while still trying to optimize that reconstruction loss, direct consequence of not regularizing.
1945320	1952120	By, in order to overcome these problems, we need to regularize the variance and the mean of these
1952120	1957720	distributions that are being returned by the encoder. And the normal prior, placing that
1957720	1965560	normal Gaussian distribution as our prior helps us achieve this. And to understand why exactly
1965560	1971560	this occurs, is that effectively the normal prior is going to encourage these learned latent
1971560	1978840	variable distributions to overlap in latent space. Recall, right? Mean zero, variance of one. That
1978840	1984760	means all the, all the latent variables are going to be enforced to try to have the same mean, a
1984760	1990120	centered mean, and all the variances are going to be regularized for each and every of the latent
1990120	1996280	variable distributions. And so this will ensure a smoothness and a regularity and an overlap in
1996280	2003000	the latent space, which will be very effective in helping us achieve these properties of continuity
2003000	2012200	and completeness. Centering the means, regularizing the variances. So the regularization via this
2012200	2019240	normal prior, by centering each of these latent variables, regularizing their, their variances,
2019240	2024360	is that it helps enforce this continuous and complete gradient of information.
2024440	2029880	Represented in the latent space, where again points and distances in the latent space have
2029880	2035640	some relationship to the reconstructions and the content of the reconstructions that result.
2037000	2044040	Note though that there's going to be a tradeoff between regularizing and reconstructing. The
2044040	2050360	more we regularize, there's also a risk of suffering, the quality of the reconstruction
2050440	2056120	and the generation process itself. So in optimizing gaze, there's going to be this tradeoff that's
2056120	2063240	going to try to be tuned to fit the problem of interest. All right. So hopefully by walking
2063240	2068600	through this, this example, and considering these points, you've built up a more intuition about
2068600	2074200	why regularization is important and how specifically the normal prior can help us regularize.
2074600	2079720	Great. So now we've defined our loss function, we know that we can reconstruct the inputs,
2079720	2085080	we've understood how we can regularize learning and achieve continuity and completeness via
2085080	2091000	this normal prior. These are all the components that define a forward pass through the network,
2091800	2097480	going from input to encoding to decoded reconstruction. But we're still missing a
2097480	2101880	critical step in putting the whole picture together. And that's, that's a critical step
2101880	2107800	putting the whole picture together. And that's a back propagation. And the key here is that
2107800	2114040	because of this fact that we've introduced this stochastic sampling layer, we now have a problem
2114040	2119720	where we can't back propagate gradients through a sampling layer that has this element of stochasticity.
2120840	2126360	Back propagation requires deterministic nodes, deterministic layers for which we can iteratively
2126360	2132440	apply the chain rule to optimize gradients. Optimize the loss via gradient descent.
2133720	2141320	All right. VAEs introduced sort of a breakthrough idea that solved this issue of not being able
2141320	2147400	to back propagate through a sampling layer. And the key idea was to actually subtly
2147400	2153080	reparameterize the sampling operation such that the network could be trained completely end to end.
2154040	2159480	So as we, as we already learned, right, we're trying to build up this latent distribution
2159480	2166920	defined by these variables z define it placing a normal prior defined by a mean and a variance.
2166920	2173480	And we can't simply back propagate gradients through the sampling layer because we can't compute
2174040	2181160	gradients through this stochastic sample. The key idea instead is to try to consider the
2181240	2191480	sampled latent vector z as a sum defined by a fixed mu a fixed sigma vector and scale that sigma
2191480	2198280	vector by random constants that are going to be drawn from a prior distribution such as a normal
2198280	2206120	Gaussian. And by reparameterizing the sampling operation as, as so, we still have this element
2206120	2212680	of stochasticity, but that stochasticity is introduced by this random constant epsilon,
2212680	2218360	which is not occurring within the bottleneck latent layer itself. We've reparameterized
2218360	2224840	and distributed it elsewhere. To visualize how this looks, let's consider it the following
2224840	2232280	where originally in the original form of the VAE, we had this deterministic nodes,
2232280	2236920	which are the weights of the network, as well as an input vector, and we are trying to
2236920	2243640	back propagate through the stochastic sampling node z. But we can't do that. So now,
2243640	2251720	by reparameterization, what we've achieved is the following form where our latent variable z
2251720	2261240	are defined with respect to mu sigma squared, as well as these noise factor epsilon,
2261320	2267240	such that when we want to do back propagation through the network to update, we can directly
2267240	2273000	back propagate through z defined by mu and sigma squared, because this epsilon value is just taken
2273000	2279480	as a constant, it's reparameterized elsewhere. And this is a very, very powerful trick, the
2279480	2285320	reparameterization trick, because it enables us to train variational auto encoders end to end
2285320	2292360	by back propagating with respect to z and with respect to the actual weights of the encoder
2292360	2300920	network. All right. One side effect and one consequence of imposing these distributional
2300920	2306440	priors on the latent variable is that we can actually sample from these latent variables
2306440	2313320	and individually tune them while keeping all of the other variables fixed. And what you can do
2313320	2318760	is you can tune the value of a particular latent variable and run the decoder each time that variable
2318760	2325960	is changed, each time that variable is perturbed to generate a new reconstructed output. So an
2325960	2333000	example of that result is in the following, where this perturbation of the latent variables results
2333000	2340040	in a representation that has some semantic meaning about what the network is maybe learning. So in
2340040	2346920	this example, these images show variation in head pose. And the different dimensions of z,
2346920	2353800	the latent space, the different latent variables, are in this way encoding different latent features
2353800	2360280	that can be interpreted by keeping all other variables fixed and perturbing the value of
2360280	2369640	one individual latent variable. Ideally, in order to optimize VAEs and try to maximize the information
2369640	2376120	that they encode, we want these latent variables to be uncorrelated with each other, effectively
2376120	2382120	disentangled. And what that could enable us to achieve is to learn the richest and most
2382120	2389000	compact latent representation possible. So in this case, we have head pose on the x axis
2389000	2395160	and smile on the y axis. And we want these to be as uncorrelated with each other as possible.
2395800	2401320	One way we can achieve this, that's been shown to achieve this disentanglement,
2401320	2407640	is rather a quite straightforward approach called beta VAEs. So if we consider the loss of a standard
2407640	2413800	VAE, again, we have this reconstruction term defined by a log likelihood and a regularization term
2413800	2420360	defined by the KL divergence. Beta VAEs introduce a new hyper parameter beta,
2420360	2426440	which controls the strength of this regularization term. And it's been shown mathematically that
2426440	2432520	by increasing beta, the effect is to place constraints on the latent encoding, such as to
2432520	2437880	encourage disentanglement. And there have been extensive proofs and discussions as to how exactly
2437880	2444120	this is achieved. But to consider the results, let's again consider the problem of face
2444120	2451720	reconstruction. Where using a standard VAE, if we consider the latent variable of head pose
2451720	2457400	or rotation, in this case where beta equals one, what you can hopefully appreciate is that as the
2457400	2466120	face pose is changing, the smile of some of these faces is also changing. In contrast, by enforcing
2466120	2473080	a beta much larger than one, what is able to be achieved is that the smile remains relatively
2473080	2479960	constant while we can perturb the single latent variable of the head rotation and achieve perturbations
2479960	2487880	with respect to head rotation alone. All right. So as I motivated and introduced in the beginning
2487880	2493160	and the introduction of this lecture, one powerful application of generative models and latent
2493160	2499400	variable models is in model debiasing. And in today's lab, you're actually going to get real hands-on
2499400	2506120	experience in building a variational autoencoder that can be used to achieve automatic debiasing
2506120	2511480	of facial classification systems, facial detection systems. And the power and the idea of this
2512040	2519160	approach is to build up a representation, a learned latent distribution of face data,
2519880	2525720	and use this to identify regions of that latent space that are going to be overrepresented
2525720	2530120	or underrepresented. And that's going to all be taken with respect to particular
2530120	2537560	learned features such as skin tone, pose, objects, clothing. And then from these learned
2537560	2544760	distributions, we can actually adjust the training process such that we can place greater
2545640	2551560	weight and greater sampling on those images and on those faces the fall in the regions of
2551560	2557080	the latent space that are underrepresented automatically. And what's really, really cool
2557960	2563800	about deploying a VAE or a latent variable model for an application like model debiasing
2563800	2569640	is that there's no need for us to annotate and prescribe the features that are important
2569640	2575000	to actually debize against. The model learns them automatically. And this is going to be the
2575000	2582120	topic of today's lab. And it also opens the door to a much broader space that's going to
2582120	2587400	be explored further in a later spotlight lecture that's going to focus on algorithmic bias and
2587400	2594200	machine learning fairness. All right, so to summarize the key points on VAEs, they compress
2594200	2601560	representation of data into an encoded representation. Reconstruction of the data input allows for
2601560	2609400	unsupervised learning without labels. We can use the reparameterization trick to train
2609400	2616200	VAEs end to end. We can take hidden latent variables, perturb them to interpret their
2616200	2621400	content and their meaning. And finally, we can sample from the latent space to generate new
2621400	2629000	examples. But what if we wanted to focus on generating samples and synthetic samples that
2629000	2635640	were as faithful to a data distribution generally as possible? To understand how we can achieve
2635640	2640200	this, we're going to transition to discuss a new type of generative model called a generative
2640200	2650040	adversarial network or GAN for short. The idea here is that we don't want to explicitly model
2650040	2656600	the density or the or the distribution underlying some data, but instead just learn a representation
2656600	2661720	that can be successful in generating new instances that are similar to the data,
2663000	2668760	which means that we want to optimize to sample from a very, very complex distribution,
2669400	2675000	which cannot be learned and modeled directly. Instead, we're going to have to build up some
2675000	2682440	approximation of this distribution. And the really cool and breakthrough idea of GANs
2682440	2689320	is to start from something extremely, extremely simple, just random noise and try to build a
2689320	2695480	neural network, a generative neural network that can learn a functional transformation
2695480	2703880	that goes from noise to the data distribution. And by learning this functional generative mapping,
2703880	2710200	we can then sample in order to generate fake instances, synthetic instances that are going
2710200	2717400	to be as close to the real data distribution as possible. The breakthrough to achieving this
2717400	2723880	was this structure called GANs, where the key component is to have two neural networks,
2723880	2729160	a generator network and a discriminator network that are effectively competing against each other,
2729160	2734760	their adversaries. Specifically, we have a generator network, which I'm going to denote
2734760	2740920	here on out by G, that's going to be trained to go from random noise to produce an imitation of
2740920	2747880	the data. And then the discriminator is going to take that synthetic fake data, as well as real
2747880	2753720	data, and be trained to actually distinguish between fake and real. And in training, these two
2753720	2760120	networks are going to be competing against each other. And so in doing so, overall, the effect
2760120	2765000	is that the discriminator is going to get better and better at learning how to classify real and
2765000	2769880	fake. And the better it becomes at doing that, it's going to force the generator to try to produce
2769880	2774840	better and better synthetic data to try to fool the discriminator back and forth, back and forth.
2775800	2782840	So let's now break this down and go from a very simple toy example to get more intuition
2782840	2789560	about how these GANs work. The generator is going to start, again, from some completely random noise
2789560	2794840	and produce fake data. And I'm going to show that here by representing these data as points on a
2794840	2800920	one-dimensional line. The discriminator is then going to see these points, as well as real data.
2801880	2807960	And then it's going to be trained to output a probability that the data it sees are real,
2808600	2813560	or if they are fake. And in the beginning, it's not going to be trained very well, right? So its
2813560	2818200	predictions are not going to be very good. But then you're going to train it, and you're going to train
2818200	2825560	it. And it's going to start increasing the probabilities of real versus not real appropriately,
2825560	2831000	such that you get this perfect separation where the discriminator is able to perfectly distinguish
2831000	2837160	what is real and what is fake. Now it's back to the generator. And the generator is going to come
2837160	2844680	back. It's going to take instances of where the real data lie as inputs to train. And then it's
2844680	2850840	going to try to improve its imitation of the data, trying to move the fake data, the synthetic data
2850840	2857240	that is generated closer and closer to the real data. And once again, the discriminator is now
2857240	2862600	going to receive these new points. And it's going to estimate a probability that each of these points
2862600	2868600	is real. And again, learn to decrease the probability of the fake points being real,
2869240	2875320	further and further. And now we're going to repeat again. And one last time, the generator is going
2875320	2881400	to start moving these fake points closer and closer to the real data, such that the fake
2881400	2887240	data are almost following the distribution of the real data. At this point, it's going to be
2887240	2892120	really, really hard for the discriminator to effectively distinguish between what is real
2892120	2897880	and what is fake. While the generator is going to continue to try to create fake data instances
2897880	2904280	to fool the discriminator. And this is really the key intuition behind how these two components of
2904280	2912120	GANs are essentially competing with each other. All right. So to summarize how we train GANs,
2912120	2917720	the generator is going to try to synthesize fake instances to fool a discriminator, which is going
2917720	2922920	to be trained to identify the synthesized instances and discriminate these as fake.
2922920	2929320	To actually train, we're going to see that we are going to define a loss function that defines
2930360	2935000	competing and adversarial objectives for each of the discriminator and the generator.
2935720	2941720	And a global optimum, the best we could possibly do would mean that the generator could perfectly
2941720	2947000	reproduce the true data distribution, such that the discriminator absolutely cannot tell
2947000	2952200	what's synthetic versus what's real. So let's go through how the loss function
2952200	2960280	for GAN breaks down. The loss term for GAN is based on that familiar cross entropy loss.
2960280	2965800	And it's going to now be defined between the true and generated distributions.
2965800	2970280	So we're first going to consider the loss from the perspective of the discriminator.
2970920	2978040	We want to try to maximize the probability that the fake data is identified as fake.
2978760	2985640	And so to break this down here, G of Z defines the generator's output. And so D of G of Z
2986280	2991240	is the discriminator's estimate of the probability that a fake instance is actually fake.
2992600	2998040	D of X is the discriminator's estimate of the probability that a real instance is fake.
2998040	3003480	So 1 minus D of X is its probability estimate that a real instance is real.
3004440	3009640	So together, from the point of view of the discriminator, we want to maximize this probability.
3010200	3014920	Maximize probability fake is fake. Maximize the estimate of probability real is real.
3016520	3021320	Now let's turn our attention to the generator. Remember that the generator is taking
3021880	3028120	random noise and generating an instance. It cannot directly affect the term D of X,
3028760	3034840	which shows up in the loss, right? Because D of X is solely based on the discriminator's operation
3034840	3039880	on the real data. So for the generator, the generator is going to have the adversarial
3039880	3045160	objective to the discriminator, which means it's going to try to minimize this term.
3046120	3055000	Effectively minimizing the probability that the discriminator can distinguish its generated data
3055960	3065160	as fake. D of G of Z. And the goal for the generator is to minimize this term of the objective.
3067720	3073720	So the objective of the generator is to try to synthesize fake instances
3073720	3078600	that fool the discriminator. And eventually, over the course of training the discriminator,
3078600	3084520	the discriminator is going to be as best as it possibly can be at discriminating real versus
3084520	3090440	fake. Therefore, the ultimate goal of the generator is to synthesize fake instances that fool the
3090440	3096520	best discriminator. And this is all put together in this min max objective function, which has
3096520	3102680	these two components optimized adversarily. And then after training, we can actually use the
3102680	3108360	generator network, which is now fully trained to produce new data instances that have never been
3108360	3114840	seen before. So we're going to focus on that now. And what is really cool is that when the
3114840	3121480	trained generator of again, synthesizes new instances, it's effectively learning a transformation
3121480	3127240	from a distribution of noise to a target data distribution. And that transformation,
3127240	3132200	that mapping is going to be what's learned over the course of training. So if we consider one
3132200	3137560	point from a latent noise distribution, it's going to result in a particular output in the
3137560	3143800	target data space. And if we consider another point of random noise, feed it through the generator,
3143800	3149160	it's going to result in a new instance that and that new instance is going to fall somewhere else
3149160	3156280	on the data manifold. And indeed, what we can actually do is interpolate and trans and traverse
3156280	3162120	in the space of Gaussian noise to result in interpolation in the target space. And you can
3162120	3168440	see an example of this result here, where a transformation in series reflects a traversal
3168440	3175320	across the target data manifold. And that's produced in the synthetic examples that are
3175320	3181160	outputted by the generator. All right. So in the final few minutes of this lecture, I'm going to
3181160	3187560	highlight some of the recent advances in GANs and hopefully motivate even further why this approach
3187560	3194520	is so powerful. So one idea that's been extremely, extremely powerful is this idea of progressive
3194520	3200840	GANs, progressive growing, which means that we can iteratively build more detail into the
3200840	3208280	generated instances that are produced. And this is done by progressively adding layers of increasing
3208280	3215160	spatial resolution in the case of image data. And by incrementally building up both the generator
3215160	3221240	and discriminator networks in this way as training progresses, it results in very well resolved
3221240	3227480	synthetic images that are output ultimately by the generator. So some results of this idea of
3227480	3234760	progressive, a progressive GAN are displayed here. Another idea that has also led to tremendous
3234760	3241000	improvement in the quality of synthetic examples generated by GANs is a architecture improvement
3241000	3246440	called style GAN, which combines this idea of progressive growing that I introduced earlier
3246440	3253000	with principles of style transfer, which means trying to compose an image in the style of another
3253000	3262520	image. So for example, what we can now achieve is to map input images source A using application of
3262520	3269080	coarse grained styles from secondary sources onto those targets to generate new instances
3269080	3277080	that mimic the style of source B. And that result is shown here. And hopefully you can
3277080	3282680	appreciate that these coarse grained features, these coarse grained styles like age, facial
3282680	3290200	structure, things like that can be reflected in these synthetic examples. This same style GAN
3290200	3298440	system has led to tremendously realistic synthetic images in the areas of both face
3298440	3306920	synthesis as well as for animals, other objects as well. Another extension to the GAN architecture
3306920	3313880	that has enabled particularly powerful applications for select problems and tasks is this idea of
3313880	3319880	conditioning, which imposes a bit of additional further structure on the types of outputs that
3319880	3327240	can be synthesized by GAN. So the idea here is to condition on a particular label by supplying
3327240	3335320	what is called a conditioning factor denoted here as C. And what this allows us to achieve is instances
3335320	3343320	like that of paired translation in the case of image synthesis, where now instead of a single input
3343960	3350040	as training data for our generator, we have pairs of inputs. So for example here we consider
3350040	3356680	both a driving scene and a corresponding segmentation map to that driving scene. And the discriminator
3356680	3364440	can in turn be trained to classify fake and real pairs of data. And again the generator is going
3364440	3373080	to be trained to try to fool the discriminator. Example applications of this idea are
3374280	3380920	seen as follows where we can now go from an input of a semantic segmentation map to generate a
3380920	3389000	synthetic street scene mapping according to that segmentation. Or we can go from an aerial view
3389000	3395160	from a satellite image to a street map view or from particular labels of an architectural
3395160	3401400	building to a synthetic architectural facade or day to night, black and white to color,
3401480	3406920	edges to photos, different instances of paired translation that are achieved by conditioning
3406920	3413880	on particular labels. So another example which I think is really cool and interesting is
3413880	3421720	translating from Google Street View to a satellite view and vice versa. And we can also achieve this
3421720	3428120	dynamically. So for example in coloring given an edge input, the network can be trained to
3428120	3434680	actually synthetically color in the artwork that is resulting from this particular edge sketch.
3437240	3442760	Another idea instead of paired translation is that of unpaired image to image translation.
3442760	3448440	And this is going to be achieved by a network architecture called CycleGAN where the model
3448440	3455640	is taking as input images from one domain and is able to learn a mapping that translates to
3455720	3460200	another domain without having a paired corresponding image in that other domain.
3461240	3468120	So the idea here is to transfer the style and the distribution from one domain to another.
3468840	3475640	And this is achieved by introducing the cyclic relationship and a cyclic loss function where
3475640	3482520	we can go back and forth between a domain x and a domain y. And in this system there are actually
3482520	3486920	two generators and two discriminators that are going to be trained on their respective
3486920	3494200	generation and discrimination tasks. In this example the CycleGAN has been trained to try to
3494200	3500520	translate from the domain of horses to the domain of zebras. And hopefully you can appreciate that
3500520	3507480	in this example there's a transformation of the skin of the horse from brown to a zebra-like skin
3507480	3512440	in stripes. And beyond this there's also a transformation of the surrounding area
3512440	3516920	from green grass to something that's more brown in the case of the zebra.
3518600	3523080	And I think to get an intuition about how this CycleGAN transformation is going
3523720	3531000	is working. Let's go back to the idea that conventional GANs are moving from a distribution
3531000	3537480	of Gaussian noise to some target data manifold. With CycleGANs the goal is to go from a particular
3537480	3545080	data manifold x to another data manifold y. And in both cases and I think the underlying
3545080	3550680	concept that makes GANs so powerful is that they function as very very effective distribution
3550680	3555320	transformers and it can achieve these distribution transformations.
3555480	3563720	Finally I'd like to consider one additional application that you may be familiar with
3564440	3570760	of using CycleGANs and that's to transform speech and to actually use this CycleGAN technique to
3570760	3576280	synthesize speech in someone else's voice. And the way this is done is by taking a bunch of audio
3576280	3582840	recordings in one voice and audio recordings in another voice and converting those audio waveforms
3582840	3591080	into an image representation which was called a spectrogram. We can then train a CycleGAN to
3591080	3598840	operate on these spectrogram images to transform representations from voice A to make them appear
3598840	3605480	like they appear that they are from another voice, voice B. And this is exactly how we did the
3605480	3611800	speech transformation for the synthesis of Obama's voice in the demonstration that Alexander gave in
3611800	3618680	the first lecture. So to inspect this further let's compare side by side the original audio
3618680	3624840	from Alexander as well as the synthesized version in Obama's voice that was generated using a CycleGAN.
3626760	3634120	Hi everybody and welcome to MIT 6S191, you know, facial introductory course
3634120	3644120	on speech learning here at MIT. So notice that the spectrogram that results for Obama's voice
3644680	3650760	is actually generated by an operation on Alexander's voice and effectively learning a domain
3650760	3656680	transformation from Obama domain onto the domain of Alexander domain and the end result is that we
3656680	3663240	create and synthesize something that's more Obama-like. All right so to summarize hopefully
3663240	3668360	over the course of this lecture you built up understanding of generative modeling and classes
3668360	3674920	of generative models that are particularly powerful in enabling probabilistic density estimation
3674920	3683000	as well as sample generation. And with that I'd like to close the lecture and introduce you to
3683000	3689400	the remainder of today's course which is going to focus on our second lab on computer vision,
3690280	3695720	specifically exploring this question of debiasing in facial detection systems
3695720	3701240	and using variational autoencoders to actually achieve an approach for automatic
3701240	3706600	debiasing of classification systems. So I encourage you to come to the class gather town
3706600	3713800	to have your questions on the lab's answered and to discuss further with any of us. Thank you.
