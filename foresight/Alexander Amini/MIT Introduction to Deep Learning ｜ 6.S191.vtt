WEBVTT

00:00.000 --> 00:15.320
Good afternoon everyone and welcome to MIT Success 191.

00:15.320 --> 00:19.360
My name is Alexander Amini and I'll be one of your instructors for the course this year

00:19.360 --> 00:21.440
along with Ava.

00:21.440 --> 00:25.280
And together we're really excited to welcome you to this really incredible course.

00:25.280 --> 00:33.200
This is a very fast paced and very intense one week that we're about to go through together.

00:33.200 --> 00:38.840
So we're going to cover the foundations of a also very fast paced moving field and a

00:38.840 --> 00:43.640
field that has been rapidly changing over the past eight years that we have taught this

00:43.640 --> 00:45.720
course at MIT.

00:45.720 --> 00:52.240
Now over the past decade in fact, even before we started teaching this course, AI and deep

00:52.240 --> 00:58.320
learning has really been revolutionizing so many different advances and so many different

00:58.320 --> 01:02.860
areas of science, mathematics, physics and so on.

01:02.860 --> 01:12.080
And not that long ago, we were having challenges and problems that we did not think were necessarily

01:12.080 --> 01:20.840
solvable in our lifetimes that AI is now actually solving beyond human performance today.

01:20.840 --> 01:26.400
And each year that we teach this course, this lecture in particular is getting harder

01:26.400 --> 01:32.800
and harder to teach because for an introductory level course, this lecture, lecture number

01:32.800 --> 01:36.560
one, is the lecture that's supposed to cover the foundations and if you think to any other

01:36.560 --> 01:43.040
introductory course like an introductory course 101 on mathematics or biology, those

01:43.040 --> 01:46.280
lecture ones don't really change that much over time.

01:46.280 --> 01:51.960
So we're in a rapidly changing field of AI and deep learning where even these types

01:51.960 --> 01:55.560
of lectures are rapidly changing.

01:55.560 --> 02:01.800
So let me give you an example of how we introduced this course only a few years ago.

02:01.800 --> 02:11.020
Hi everybody and welcome to MIT 6S191, the official introductory course on deep learning

02:11.020 --> 02:14.720
taught here at MIT.

02:14.720 --> 02:22.260
Deep learning is revolutionizing so many fields from robotics to medicine and everything

02:22.260 --> 02:24.360
in between.

02:24.360 --> 02:31.480
You'll learn the fundamentals of this field and how you can build some of these incredible

02:31.480 --> 02:33.480
algorithms.

02:33.480 --> 02:42.240
In fact, this entire speech and video are not real and were created using deep learning

02:42.240 --> 02:45.960
and artificial intelligence.

02:45.960 --> 02:49.420
And in this class, you'll learn how.

02:49.420 --> 02:58.000
It has been an honor to speak with you today and I hope you enjoy the course.

02:58.000 --> 03:05.280
The really surprising thing about that video to me when we first did it was how viral it

03:05.280 --> 03:07.000
went a few years ago.

03:07.000 --> 03:11.440
So just in a couple months about teaching this course a few years ago, that video went

03:11.440 --> 03:16.640
very viral, it got over a million views within only a few months.

03:16.640 --> 03:22.840
People were shocked with a few things but the main one was the realism of AI to be able

03:22.840 --> 03:30.520
to generate content that looks and sounds extremely hyper realistic.

03:30.520 --> 03:35.360
And when we did this video, when we created this for the class only a few years ago, this

03:35.360 --> 03:42.480
video took us about $10,000 in compute to generate just about a minute long video.

03:42.480 --> 03:47.160
If you think about it, I would say it's extremely expensive to compute something what we look

03:47.160 --> 03:51.360
at like that and maybe a lot of you are not really impressed by the technology today because

03:51.360 --> 03:56.520
you see all of the amazing things that AI and deep learning are producing.

03:56.520 --> 04:01.760
Now fast forward today, the progress in deep learning and people were making all kinds

04:01.760 --> 04:05.360
of exciting remarks about it when it came out a few years ago.

04:05.360 --> 04:11.240
Now this is common stuff because AI is really doing much more powerful things than this

04:11.240 --> 04:14.200
fun little introductory video.

04:14.200 --> 04:20.120
So today fast forward four years, about four years to today.

04:20.120 --> 04:21.120
Now where are we?

04:21.120 --> 04:27.760
AI is now generating content with deep learning being so commoditized.

04:27.760 --> 04:34.160
Deep learning is in all of our fingertips now online in our smartphones and so on.

04:34.160 --> 04:41.320
In fact, we can use deep learning to generate these types of hyper realistic pieces of media

04:41.320 --> 04:46.720
and content entirely from English language without even coding anymore.

04:46.720 --> 04:51.640
So before we had to actually go and train these models and really code them to be able

04:51.640 --> 04:54.280
to create that one minute long video.

04:54.280 --> 04:59.240
Today we have models that will do that for us end to end directly from English language.

04:59.240 --> 05:03.480
So we can prompt these models to create something that the world has never seen before.

05:03.480 --> 05:09.320
A photo of an astronaut riding a horse and these models can imagine those pieces of content

05:09.320 --> 05:11.640
entirely from scratch.

05:11.640 --> 05:17.760
My personal favorite is actually how we can now ask these deep learning models to create

05:17.760 --> 05:22.880
new types of software even themselves being software to ask them to create.

05:22.880 --> 05:27.960
For example, to write this piece of TensorFlow code to train a neural network, right?

05:27.960 --> 05:33.160
We're asking a neural network to write TensorFlow code to train another neural network and our

05:33.160 --> 05:40.480
model can produce examples of functional and usable pieces of code that satisfy this

05:40.480 --> 05:44.920
English prompt while walking through each part of the code independently.

05:44.920 --> 05:49.440
So not even just producing it but actually educating and teaching the user on what each

05:49.440 --> 05:53.520
part of these code blocks are actually doing.

05:53.520 --> 05:55.320
You can see an example here.

05:55.320 --> 06:00.000
And really what I'm trying to show you with all of this is that this is just highlighting

06:00.000 --> 06:06.880
how far deep learning has gone even in a couple years since we've started teaching this course.

06:06.880 --> 06:10.760
I mean going back even from before that to eight years ago.

06:10.760 --> 06:16.640
And the most amazing thing that you'll see in this course in my opinion is that what

06:16.640 --> 06:21.600
we try to do here is to teach you the foundations of all of this, how all of these different

06:21.600 --> 06:27.120
types of models are created from the ground up and how we can make all of these amazing

06:27.120 --> 06:31.600
advances possible so that you can also do it on your own as well.

06:31.600 --> 06:35.200
And like I mentioned in the beginning, this introduction course is getting harder and

06:35.200 --> 06:38.400
harder to do and to make every year.

06:38.400 --> 06:44.520
I don't know where the field is going to be next year and I mean that's my honest truth

06:44.520 --> 06:50.200
or even honestly in even one or two months time from now just because it's moving so

06:50.200 --> 06:51.200
incredibly fast.

06:51.200 --> 06:56.760
But what I do know is that what we will share with you in the course as part of this one

06:56.760 --> 07:01.440
week is going to be the foundations of all of the technologies that we have seen up until

07:01.440 --> 07:06.280
this point that will allow you to create that future for yourselves and to design brand new

07:06.280 --> 07:13.520
types of deep learning models using those fundamentals and those foundations.

07:13.520 --> 07:18.920
So let's get started with all of that and start to figure out how we can actually achieve

07:18.920 --> 07:23.520
all of these different pieces and learn all of these different components.

07:23.520 --> 07:28.720
And we should start this by really tackling the foundations from the very beginning and

07:28.720 --> 07:33.160
asking ourselves, you know, we've heard this term, I think all of you obviously before

07:33.160 --> 07:37.000
you've come to this class today, you've heard the term deep learning, but it's important

07:37.000 --> 07:42.720
for you to really understand how this concept of deep learning relates to all of the other

07:42.720 --> 07:46.200
pieces of science that you've learned about so far.

07:46.200 --> 07:49.860
So to do that, we have to start from the very beginning and start by thinking about what

07:49.860 --> 07:55.320
is intelligence at its core, not even artificial intelligence, but just intelligence, right?

07:55.320 --> 08:02.320
So the way I like to think about this is that I like to think that intelligence is the ability

08:02.320 --> 08:08.800
to process information which will inform your future decision making abilities.

08:08.800 --> 08:12.280
Now that's something that we as humans do every single day.

08:12.280 --> 08:17.720
Now artificial intelligence is simply the ability for us to give computers that same

08:17.720 --> 08:23.560
ability to process information and inform future decisions.

08:23.560 --> 08:28.600
Now machine learning is simply a subset of artificial intelligence.

08:28.600 --> 08:33.760
The way you should think of machine learning is just as the programming ability or let's

08:33.760 --> 08:41.720
say even simpler than that, machine learning is the science of trying to teach computers

08:41.720 --> 08:47.200
how to do that processing of information and decision making from data.

08:47.200 --> 08:52.200
So instead of hard coding some of these rules into machines and programming them like we

08:52.200 --> 08:57.040
used to do in software engineering classes, now we're going to try and do that processing

08:57.040 --> 09:02.640
of information and informing a future decision making abilities directly from data.

09:02.640 --> 09:07.720
And then going one step deeper, deep learning is simply the subset of machine learning which

09:07.720 --> 09:09.880
uses neural networks to do that.

09:09.880 --> 09:16.040
It uses neural networks to process raw pieces of data, now unprocessed data, and allows

09:16.040 --> 09:21.600
them to ingest all of those very large data sets and inform future decisions.

09:21.600 --> 09:25.440
Now that's exactly what this class is really all about.

09:25.440 --> 09:30.120
If you think of, if I had to summarize this class in just one line, it's all about teaching

09:30.120 --> 09:36.640
machines how to process data, process information, and inform decision making abilities from

09:36.640 --> 09:40.720
that data and learn it from that data.

09:40.720 --> 09:45.720
Now this program is split between really two different parts.

09:45.720 --> 09:50.800
So you should think of this class as being captured with both technical lectures, which

09:50.800 --> 09:55.120
for example this is one part of, as well as software labs.

09:55.120 --> 09:59.480
We'll have several new updates this year as I mentioned earlier just covering the rapid

09:59.480 --> 10:04.040
changing of advances in AI and especially in some of the later lectures you're going

10:04.040 --> 10:05.120
to see those.

10:05.120 --> 10:11.360
The first lecture today is going to cover the foundations of neural networks themselves,

10:11.360 --> 10:14.800
starting with really the building blocks of every single neural network which is called

10:14.800 --> 10:16.160
the Perceptron.

10:16.160 --> 10:21.520
And finally we'll go through the week and we'll conclude with a series of exciting

10:21.520 --> 10:25.920
guest lectures from industry leading sponsors of the course.

10:25.920 --> 10:32.920
And finally on the software side, after every lecture you'll also get software experience

10:32.920 --> 10:37.440
and project building experience to be able to take what we teach in lectures and actually

10:37.440 --> 10:43.760
deploy them in real code and actually produce based on the learnings that you find in this

10:43.760 --> 10:47.800
lecture and at the very end of the class from the software side you'll have the ability

10:47.800 --> 10:53.680
to participate in a really fun day at the very end which is the project pitch competition.

10:53.680 --> 10:58.400
It's kind of like a shark tank style competition of all of the different projects from all

10:58.400 --> 11:01.720
of you and win some really awesome prizes.

11:01.720 --> 11:06.920
So let's step through that a little bit briefly, this is the syllabus part of the lecture.

11:06.920 --> 11:11.600
So each day we'll have dedicated software labs that will basically mirror all of the

11:11.600 --> 11:15.720
technical lectures that we go through just helping you reinforce your learnings and these

11:15.720 --> 11:21.840
are coupled with each day again coupled with prizes for the top performing software solutions

11:21.840 --> 11:23.420
that are come up in the class.

11:23.420 --> 11:28.400
This is going to start with today with lab one and it's going to be on music generation

11:28.400 --> 11:33.760
so you're going to learn how to build a neural network that can learn from a bunch of musical

11:33.760 --> 11:41.840
songs, listen to them and then learn to compose brand new songs in that same genre.

11:41.840 --> 11:47.200
Tomorrow lab two on computer vision you're going to learn about facial detection systems,

11:47.200 --> 11:52.720
you'll build a facial detection system from scratch using convolutional neural networks,

11:52.720 --> 11:57.960
you'll learn what that means tomorrow and you'll also learn how to actually de-bias,

11:57.960 --> 12:02.960
remove the biases that exist in some of these facial detection systems which is a huge problem

12:02.960 --> 12:07.000
for the state of the art solutions that exist today.

12:07.000 --> 12:12.440
And finally a brand new lab at the end of the course will focus on large language models

12:12.440 --> 12:18.000
where you're actually going to take a billion multi-billion parameter large language model

12:18.000 --> 12:24.440
and fine tune it to build an assistive chatbot and evaluate a set of cognitive abilities

12:24.440 --> 12:31.320
ranging from mathematics abilities to scientific reasoning to logical abilities and so on.

12:31.320 --> 12:36.640
And finally at the very very end there will be a final project pitch competition for up

12:36.640 --> 12:42.400
to five minutes per team and all of these are accompanied with great prizes so definitely

12:42.400 --> 12:45.280
there will be a lot of fun to be had throughout the week.

12:45.280 --> 12:49.520
There are many resources to help with this class, you'll see them posted here you don't

12:49.520 --> 12:53.320
need to write them down because all of the slides are already posted online.

12:53.320 --> 12:59.440
Please post to Piazza if you have any questions and of course we have an amazing team that

12:59.440 --> 13:03.640
is helping teach this course this year and you can reach out to any of us if you have

13:03.640 --> 13:04.640
any questions.

13:04.640 --> 13:06.600
The Piazza is a great place to start.

13:06.600 --> 13:12.760
Myself and Ava will be the two main lectures for this course Monday through Wednesday especially

13:12.760 --> 13:17.160
and we'll also be hearing some amazing guest lectures on the second half of the course

13:17.160 --> 13:21.680
which definitely you would want to attend because they really cover the really state

13:21.720 --> 13:28.920
of the art sides of deep learning that's going on in industry outside of academia.

13:28.920 --> 13:32.840
And very briefly just want to give a huge thanks to all of our sponsors who without

13:32.840 --> 13:36.560
their support this course like every year would not be possible.

13:36.560 --> 13:42.000
Okay so now let's start with the fun stuff and my favorite part of the course which is

13:42.000 --> 13:48.400
the technical parts and let's start by just asking ourselves a question right which is

13:48.400 --> 13:50.400
you know why do we care about all of this?

13:50.400 --> 13:51.620
Why do we care about deep learning?

13:51.620 --> 13:56.920
Why did you all come here today to learn and to listen to this course?

13:56.920 --> 14:01.800
So to understand I think we again need to go back a little bit to understand how machine

14:01.800 --> 14:09.800
learning used to be performed right so machine learning typically would define a set of features

14:09.800 --> 14:14.800
or you can think of these as kind of a set of things to look for in an image or in a

14:14.800 --> 14:16.360
piece of data.

14:16.400 --> 14:21.440
Actually these are hand engineered so humans would have to define these themselves and

14:21.440 --> 14:25.800
the problem with these is that they tend to be very brittle in practice just by nature

14:25.800 --> 14:27.480
of a human defining them.

14:27.480 --> 14:31.360
So the key idea of deep learning and what you're going to learn throughout this entire

14:31.360 --> 14:37.640
week is this paradigm shift of trying to move away from hand engineering features and rules

14:37.640 --> 14:43.040
that computers should look for and instead trying to learn them directly from raw pieces

14:43.040 --> 14:44.040
of data.

14:44.040 --> 14:49.760
What are the patterns that we need to look at in data sets such that if we look at those

14:49.760 --> 14:54.800
patterns we can make some interesting decisions and interesting actions can come out.

14:54.800 --> 15:00.240
So for example if we wanted to learn how to detect faces we might if you think even how

15:00.240 --> 15:04.040
you would detect faces right if you look at a picture what are you looking for to detect

15:04.040 --> 15:05.040
a face?

15:05.040 --> 15:09.320
You're looking for some particular patterns you're looking for eyes and noses and ears

15:09.320 --> 15:13.240
and when those things are all composed in a certain way you would probably deduce that

15:13.240 --> 15:18.440
that's a face right computers do something very similar so they have to understand what

15:18.440 --> 15:23.520
are the patterns that they look for what are the eyes and noses and ears of those pieces

15:23.520 --> 15:31.080
of data and then from there actually detect and predict from them.

15:31.080 --> 15:37.200
So the really interesting thing I think about deep learning is that these foundations for

15:37.200 --> 15:42.480
doing exactly what I just mentioned picking out the building blocks picking out the features

15:42.480 --> 15:47.880
from raw pieces of data and the underlying algorithms themselves have existed for many

15:47.880 --> 15:49.720
many decades.

15:49.720 --> 15:55.760
Now the question I would ask at this point is so why are we studying this now and why

15:55.760 --> 16:00.560
is all of this really blowing up right now and exploding with so many great advances?

16:00.560 --> 16:05.400
Well for one there's three things right number one is that the data that is available to

16:05.400 --> 16:11.160
us today is significantly more pervasive these models are hungry for data you're going to

16:11.200 --> 16:15.880
learn about this more in detail but these models are extremely hungry for data and we're

16:15.880 --> 16:21.560
living in a world right now quite frankly where data is more abundant than it has ever

16:21.560 --> 16:23.840
been in our history.

16:23.840 --> 16:30.080
Now secondly these algorithms are massively compute hungry and they're massively parallelizable

16:30.080 --> 16:36.040
which means that they have greatly benefited from compute hardware which is also capable

16:36.120 --> 16:42.280
of being parallelized the particular name of that hardware it's called a GPU right GPUs

16:42.280 --> 16:48.800
can run parallel processing streams of information and are particularly amenable to deep learning

16:48.800 --> 16:54.440
algorithms and the abundance of GPUs and that compute hardware has also pushed forward what

16:54.440 --> 16:59.840
we can do in deep learning and finally the last piece is the software right it's the

16:59.840 --> 17:06.520
open source tools that are really used as the foundational building blocks of deploying

17:06.520 --> 17:10.560
and building all of these underlying models that you're going to learn about in this course

17:10.560 --> 17:15.160
and those open source tools have just become extremely streamlined making this extremely

17:15.160 --> 17:22.680
easy for all of us to learn about these technologies within an amazing one week course like this.

17:22.680 --> 17:26.120
So let's start now with understanding now that we have some of the background let's

17:26.120 --> 17:31.200
start with understanding exactly what is the fundamental building block of a neural

17:31.200 --> 17:37.720
network now that building block is called a perceptron right every single percept every

17:37.720 --> 17:42.960
single neural network is built up of multiple perceptrons and you're going to learn how

17:42.960 --> 17:47.800
those perceptrons number one compute information themselves and how they connect to these much

17:47.800 --> 17:52.360
larger billion parameter neural networks.

17:52.360 --> 17:57.320
So the key idea of a perceptron or even simpler think of this as a single neuron right so

17:57.320 --> 18:03.100
a neural network is composed of many many neurons and a perceptron is just one neuron.

18:03.100 --> 18:08.200
So that idea of a perceptron is actually extremely simple and I hope that by the end of today

18:08.200 --> 18:14.440
this idea and this processing of a perceptron becomes extremely clear to you.

18:14.440 --> 18:19.600
So let's start by talking about just the forward propagation of information through a single

18:19.600 --> 18:26.360
neuron now single neurons ingest information they can actually ingest multiple pieces of

18:26.360 --> 18:31.160
information so here you can see this neuron taking as input three pieces of information

18:31.160 --> 18:39.520
x1 x2 and xm right so we define the set of inputs called x1 through m and each of these

18:39.520 --> 18:45.160
inputs each of these numbers is going to be element wise multiplied by a particular weight

18:45.160 --> 18:50.240
so this is going to be denoted here by w1 through wm so this is a corresponding weight

18:50.240 --> 18:54.840
for every single input and you should think of this as really you know every weight being

18:54.840 --> 19:01.760
assigned to that input right the weights are part of the neuron itself now you multiply

19:01.760 --> 19:06.360
all of these inputs with their weights together and then you add them up we take this single

19:06.360 --> 19:11.400
number after that addition and you pass it through what's called a nonlinear activation

19:11.400 --> 19:20.560
function to produce your final output which here we're calling y now what I just said

19:20.560 --> 19:25.320
is not entirely correct right so I missed out one critical piece of information that

19:25.320 --> 19:29.520
piece of information is that we also have what you can see here is called this bias

19:29.520 --> 19:36.720
term that bias term is actually what allows your neural neuron to shift its activation

19:36.720 --> 19:42.960
function horizontally on that x axis if you think of it right so on the right side you

19:42.960 --> 19:48.560
can now see this diagram illustrating mathematically that single equation that I talked through

19:48.560 --> 19:52.520
kind of conceptually right now you can see it mathematically written down as one single

19:52.520 --> 19:58.000
equation and we can actually rewrite this using linear algebra using vectors and dot

19:58.000 --> 20:03.280
products so let's do that right so now our inputs are going to be described by a capital

20:03.280 --> 20:09.640
x which is simply a vector of all of our inputs x1 through xm and then our weights are going

20:09.640 --> 20:16.960
to be described by a capital w which is going to be w1 through wm the input is obtained

20:16.960 --> 20:23.920
by taking the dot product of x and w right that dot product does that element wise multiplication

20:23.920 --> 20:29.480
and then adds sums all of the the element wise multiplications and then here's the missing

20:29.520 --> 20:34.120
piece is that we're now going to add that bias term here we're calling the bias term

20:34.120 --> 20:40.520
w zero right and then we're going to apply the nonlinearity which here denoted is z or g excuse

20:40.520 --> 20:46.720
me so I've mentioned this nonlinearity a few times this activation function let's dig into it a

20:46.720 --> 20:51.880
little bit more so we can understand what is actually this activation function doing well I

20:51.880 --> 20:56.480
said a couple things about it I said it's a nonlinear function right here you can see one

20:56.520 --> 21:04.200
example of an activation function one common one commonly used activation function is called

21:04.200 --> 21:08.000
the sigmoid function which you can actually see here on the bottom right hand side of the

21:08.000 --> 21:14.600
screen the sigmoid function is very commonly used because its outputs right so it takes as input

21:14.600 --> 21:21.400
any real number the x-axis is infinite plus or minus but on the y-axis it basically squashes

21:21.720 --> 21:28.320
every input x into a number between 0 and 1 so it's actually a very common choice for things

21:28.320 --> 21:32.880
like probability distributions if you want to convert your answers into probabilities or learn

21:32.880 --> 21:39.040
or teach a neuron to learn a probability distribution but in fact there are actually many

21:39.040 --> 21:43.960
different types of nonlinear activation functions that are used in neural networks and here are

21:43.960 --> 21:48.040
some common ones and and again throughout this presentation you'll see these little

21:48.080 --> 21:52.360
TensorFlow icons actually throughout the entire course you'll see these TensorFlow icons on the

21:52.360 --> 21:58.360
bottom which basically just allow you to relate some of the foundational knowledge that we're

21:58.360 --> 22:03.360
teaching in the lectures to some of the software labs and this might provide a good starting point

22:03.360 --> 22:08.920
for a lot of the pieces that you have to do later on in the software parts of the class so the

22:08.920 --> 22:12.840
sigmoid activation which we talked about in the last slide here it's shown on the left-hand side

22:12.840 --> 22:16.960
right this is very popular because of the probability distributions right it squashes

22:17.000 --> 22:23.320
everything between 0 and 1 but you see two other very common types of activation functions in the

22:23.320 --> 22:29.080
middle and the right-hand side as well so the other very very common one probably this is the one

22:29.080 --> 22:33.440
now that's the most popular activation function is now on the far right-hand side it's called the

22:33.440 --> 22:38.840
relu activation function or also called the rectified linear unit so basically it's linear

22:38.840 --> 22:45.160
everywhere except there's a nonlinearity at x equals 0 so there's a kind of a step or a great

22:45.200 --> 22:51.160
discontinuity right so benefit of this very easy to compute it still has the nonlinearity which

22:51.160 --> 22:56.400
we kind of need and we'll talk about why we need it in one second but it's very fast right just

22:56.400 --> 23:02.320
two linear functions piecewise combined with each other okay so now let's talk about why we need

23:02.320 --> 23:07.600
a nonlinearity in the first place why why not just deal with a linear function that we pass all

23:07.600 --> 23:13.120
of these inputs through so the point of the activation function even at all why do we have

23:13.160 --> 23:21.200
this is to introduce nonlinearities in of itself so what we want to do is to allow our neural

23:21.200 --> 23:27.600
network to deal with nonlinear data right our neural networks need the ability to deal with

23:27.600 --> 23:34.400
nonlinear data because the world is extremely nonlinear right this is important because you

23:34.400 --> 23:39.840
know if you think of the real world real datasets this is just the way they are right if you look

23:39.880 --> 23:44.600
at datasets like this one green and red points right and I ask you to build a neural network

23:44.600 --> 23:51.720
that can separate the green and the red points this means that we actually need a nonlinear

23:51.720 --> 23:57.920
function to do that we cannot solve this problem with a single line right in fact if we used linear

23:57.920 --> 24:05.160
linear functions as your activation function no matter how big your neural network is it's still

24:05.160 --> 24:09.800
a linear function because linear functions combined with linear functions are still linear so

24:09.840 --> 24:14.120
no matter how deep or how many parameters your neural network has the best they would be able

24:14.120 --> 24:19.160
to do to separate these green and red points would look like this but adding nonlinearities

24:19.160 --> 24:25.000
allows our neural networks to be smaller by allowing them to be more expressive and capture

24:25.000 --> 24:31.640
more complexities in the datasets and this allows them to be much more powerful in the end so let's

24:31.640 --> 24:36.680
understand this with a simple example imagine I give you now this trained neural network so what

24:36.680 --> 24:40.720
does it mean trained neural network it means now I'm giving you the weights right not only the

24:40.720 --> 24:45.920
inputs but I'm going to tell you what the weights of this neural network are so here let's say the

24:45.920 --> 24:53.800
bias term w0 is going to be 1 and our w vector is going to be 3 and negative 2 right these are

24:53.800 --> 24:57.160
just the weights of your trained neural network well let's worry about how we got those weights

24:57.160 --> 25:05.640
in a second but this network has two inputs x1 and x2 now if we want to get the output of this

25:05.680 --> 25:10.400
neural network all we have to do simply is to do the same story that we talked about before

25:10.400 --> 25:19.200
right it's dot product inputs with weights add the bias and apply the nonlinearity right and

25:19.200 --> 25:23.360
those are the three components that you really have to remember as part of this class right dot

25:23.360 --> 25:30.160
product add the bias and apply a nonlinearity that's going to be the process that keeps repeating

25:30.200 --> 25:36.040
over and over and over again for every single neuron after that happens that neuron was going

25:36.040 --> 25:42.400
to output a single number right now let's take a look at what's inside of that nonlinearity it's

25:42.400 --> 25:49.600
simply a weighted combination of those of those inputs with those weights right so if we look

25:49.600 --> 25:57.480
at what's inside of g right inside of g is a weighted combination of x and w right added

25:57.520 --> 26:04.720
with a bias right that's going to produce a single number right but in reality for any input that

26:04.720 --> 26:09.160
this model could see what this really is is a two-dimensional line because we have two

26:09.160 --> 26:17.400
parameters in this model so we can actually plot that line we can see exactly how this neuron

26:17.400 --> 26:24.480
separates points on these axes between x1 and x2 right these are the two inputs of this model

26:24.680 --> 26:29.920
we can see exactly and interpret exactly what this neuron is doing right we can visualize

26:29.920 --> 26:35.520
its entire space because we can plot the line that defines this neuron right so here we're

26:35.520 --> 26:42.600
plotting when that line equals zero and in fact if I give you if I give that neuron in fact a

26:42.600 --> 26:48.120
new data point here the new data point is x1 equals negative one and x2 equals two just an

26:48.120 --> 26:52.680
arbitrary point in this two-dimensional space we can plot that point in the two-dimensional space

26:52.880 --> 26:58.840
and depending on which side of the line it falls on it tells us you know what the what the answer

26:58.840 --> 27:03.440
is going to be what the sign of the answer is going to be and also what the answer itself is

27:03.440 --> 27:08.840
going to be right so if we follow that that equation written on the top here and plug in negative

27:08.840 --> 27:15.960
one and two we're going to get one minus three minus four which equals minus six right and when

27:15.960 --> 27:24.440
I put that into my non-linearity g I'm going to get a final output of 0.002 right so that

27:24.440 --> 27:28.360
that don't worry about the final output that's just going to be the output from that sigmoid

27:28.360 --> 27:33.880
function but the important point to remember here is that the sigmoid function actually divides

27:33.880 --> 27:40.240
the space into these two parts right it squashes everything between zero and one but it defines

27:40.400 --> 27:48.160
it implicitly by everything less than 0.5 and greater than 0.5 depending on if it's on if x is

27:48.160 --> 27:54.320
less than zero or greater than zero so depending on which side of the line that you fall on remember

27:54.320 --> 28:00.000
the line is when x equals zero the input to the sigmoid is zero if you fall on the left side of the

28:00.000 --> 28:07.040
line your output will be less than 0.5 because you're falling on the negative side of the line if

28:07.120 --> 28:12.800
your output is if your input is on the right side of the line now your output is going to be

28:12.800 --> 28:19.120
greater than 0.5 right so here we can actually visualize this space this is called the feature

28:19.120 --> 28:25.280
space of a neural network we can visualize it in its completion right we can totally visualize

28:25.280 --> 28:30.320
and interpret this neural network we can understand exactly what it's going to do for any input that

28:30.320 --> 28:35.520
it sees right but of course this is a very simple neuron right it's not a neural network it's just

28:35.520 --> 28:41.120
one neuron and even more than that it's even a very simple neuron it only has two inputs right

28:41.760 --> 28:46.960
so in reality the types of neural neurons that you're going to be dealing with in this course

28:46.960 --> 28:53.600
are going to be neurons and neural networks with millions or even billions of these parameters

28:53.600 --> 28:59.760
right of these inputs right so here we only have two weights w1 w2 but today's neural networks have

28:59.840 --> 29:06.640
billions of these parameters so drawing these types of plots that you see here obviously becomes a

29:06.640 --> 29:13.280
lot more challenging it's actually not possible but now that we have some of the intuition behind

29:13.280 --> 29:20.080
a perceptron let's start now by building neural networks and seeing how all of this comes together

29:20.080 --> 29:25.840
so let's revisit that previous diagram of a perceptron now again if there's only one thing

29:25.840 --> 29:32.240
to take away from this lecture right now it's to remember how a perceptron works that equation of

29:32.240 --> 29:36.800
a perceptron is extremely important for every single class that comes after today and there's

29:36.800 --> 29:43.040
only three steps it's dot product with the inputs add a bias and apply your non-linearity

29:44.080 --> 29:50.000
let's simplify the diagram a little bit i'll remove the weight labels from this picture and now you

29:50.000 --> 29:56.080
can assume that if i show a line every single line has an associated weight that comes with that

29:56.080 --> 30:01.840
line right i'll also remove the bias term for simplicity assume that every neuron has that

30:01.840 --> 30:08.880
bias term i don't need to show it and now note that the result here now calling it z which is just the

30:10.080 --> 30:16.880
dot product plus bias before the non-linearity is the output is going to be linear first of all

30:16.960 --> 30:21.680
it's just a it's just a weighted sum of all those pieces we have not applied the non-linearity yet

30:21.680 --> 30:27.680
but our final output is just going to be g of z it's the activation function or non-linear

30:27.680 --> 30:35.840
activation function applied to z now if we want to step this up a little bit more and say what if

30:35.840 --> 30:41.840
we had a multi output function now we don't just have one output but let's say we want to have two

30:41.840 --> 30:48.720
outputs well now we can just have two neurons in this network right every neuron say sees

30:48.720 --> 30:54.720
all of the inputs that came before it but now you see the top neuron is going to be predicting an

30:54.720 --> 30:58.640
answer and the bottom neuron will predict its own answer now importantly one thing you should

30:58.640 --> 31:05.440
really notice here is that each neuron has its own weights right each neuron has its own lines

31:05.440 --> 31:10.960
that are coming into just that neuron right so they're acting independently but they can later on

31:10.960 --> 31:22.720
communicate if you have another layer right so let's start now by initializing this this process

31:22.720 --> 31:27.760
a bit further and thinking about it more programmatically right what if we wanted to program

31:27.760 --> 31:32.880
this neural network ourselves from scratch right remember that equation I told you didn't sound

31:32.880 --> 31:38.720
very complex it's take a dot product add a bias which is a single number and apply a non-linearity

31:38.720 --> 31:44.240
let's see how we would actually implement something like that so to define the layer right we're now

31:44.240 --> 31:52.720
going to call this a layer which is a collection of neurons right we have to first define how that

31:52.720 --> 31:58.160
information propagates through the network so we can do that by creating a call function here first

31:58.160 --> 32:03.920
we're going to actually define the weights for that network right so remember every network every

32:03.920 --> 32:08.640
neuron I should say every neuron has weights and a bias right so let's define those first

32:09.760 --> 32:16.560
we're going to create the call function to actually see how we can pass information through that

32:16.560 --> 32:22.080
layer right so this is going to take as input and inputs right this is like what we previously

32:22.080 --> 32:28.640
called x and it's the same story that we've been seeing this whole class right we're going to matrix

32:28.640 --> 32:34.240
multiply or take a dot product of our inputs with our weights we're going to add a bias

32:35.120 --> 32:40.400
and then we're going to apply a non-linearity it's really that simple right we've now created a

32:40.400 --> 32:48.640
single layer neural network right so this this line in particular this is the part that allows us to

32:48.640 --> 32:56.800
be a powerful neural network maintaining that non-linearity and the important thing here is to

32:56.800 --> 33:04.000
note that modern deep learning toolboxes and libraries already implement a lot of these for

33:04.000 --> 33:09.680
you right so it's important for you to understand the foundations but in practice all of that layer

33:09.680 --> 33:15.440
or architecture and all of that layer logic is actually implemented in tools like TensorFlow

33:15.440 --> 33:21.920
and PyTorch through a dense layer right so here you can see an example of calling or creating

33:21.920 --> 33:29.840
initializing a dense layer with two neurons right allowing it to feed in an arbitrary set of

33:29.840 --> 33:36.240
inputs here we're seeing these two neurons in a layer being fed three inputs right and in code

33:36.240 --> 33:41.360
it's only reduced down to this one line of TensorFlow code making it extremely easy and

33:41.360 --> 33:47.760
convenient for us to use these functions and call them so now let's look at our single layered

33:47.760 --> 33:53.760
neural network this is where we have now one layer between our input and our outputs right so

33:53.760 --> 33:58.960
we're slowly and progressively increasing the complexity of our neural network so that we

33:58.960 --> 34:06.240
can build up all of these building blocks right this layer in the middle is called a hidden layer

34:06.240 --> 34:10.480
right obviously because you don't directly observe it you don't directly supervise it

34:10.480 --> 34:14.960
right you do observe the two input and output layers but your hidden layer is just kind of a

34:15.920 --> 34:22.000
a neuron layer that you don't directly observe right it just gives your network more capacity

34:22.000 --> 34:27.760
more learning complexity and since we now have a transformation function from inputs

34:27.760 --> 34:34.480
to hidden layers and hidden layers to output we now have a two layered neural network right

34:34.480 --> 34:41.520
which means that we also have two weight matrices right we don't have just the w1 which we previously

34:41.520 --> 34:46.000
had to create this hidden layer but now we also have w2 which does the transformation from hidden

34:46.000 --> 34:54.000
layer to output layer yes in hidden you have just linear so there's no it's not is it a perceptron

34:54.000 --> 35:00.240
or not yes so every hidden layer also has a nonlinearity accompanied with it right and that's

35:00.240 --> 35:04.880
a very important point because if you don't have that perceptron then it's just a very large linear

35:04.880 --> 35:10.640
function followed by a final nonlinearity at the very end right so you need that cascading

35:10.640 --> 35:16.640
and uh you know overlapping application of nonlinearities that occur throughout the network

35:18.480 --> 35:26.560
awesome okay so now let's zoom in look at a single unit in the hidden layer take this one for example

35:26.560 --> 35:32.080
let's call it z2 right it's the second neuron in the first layer right it's the same perception

35:32.080 --> 35:38.880
that we saw before we compute its answer by taking a dot product of its weights with its inputs

35:38.880 --> 35:45.440
adding a bias and then applying a nonlinearity if we took a different hidden node like z3 the one

35:45.440 --> 35:51.120
right below it we would compute its answer exactly the same way that we computed z2 except its

35:51.120 --> 35:55.120
weights would be different than the weights of z2 everything else stays exactly the same it sees

35:55.120 --> 36:00.640
the same inputs but of course you know i'm not going to actually show z3 in this picture and now

36:00.640 --> 36:04.240
this picture is getting a little bit messy so let's clean things up a little bit more i'm going to

36:04.240 --> 36:10.560
remove all the lines now and replace them just with these these boxes these symbols that will denote

36:10.560 --> 36:16.000
what we call a fully connected layer right so these layers now denote that everything in our input

36:16.000 --> 36:20.400
is connected to everything in our output and the transformation is exactly as we saw it before

36:20.400 --> 36:28.720
dot product bias and nonlinearity and again in code to do this is extremely straightforward

36:28.720 --> 36:33.440
with the foundations that we've built up from the beginning of the class we can now just define

36:33.440 --> 36:40.080
two of these dense layers right our hidden layer on line one with n hidden units and then our output

36:40.080 --> 36:49.680
layer with two hidden output units nonlinearity function does not need to be the same through

36:49.680 --> 36:56.320
each layer oftentimes it is because of convenience there's there are some cases where you would want

36:56.320 --> 37:02.000
it to be different as well especially in lecture two you're going to see nonlinearity is be different

37:02.000 --> 37:10.400
even within the same layer let alone different layers but unless for a particular reason generally

37:10.400 --> 37:17.520
convention is there's no need to keep them differently now let's keep expanding our knowledge

37:17.520 --> 37:22.240
a little bit more if we now want to make a deep neural network not just a neural network like we

37:22.240 --> 37:26.960
saw on the previous side now it's deep all that means is that we're now going to stack these layers

37:26.960 --> 37:32.880
on top of each other one by one more and more creating a hierarchical model right the ones

37:32.880 --> 37:38.560
where the final output is now going to be computed by going deeper and deeper and deeper into the

37:38.560 --> 37:46.080
neural network and again doing this in code again follows the exact same story as before just cascading

37:46.080 --> 37:51.200
these tensorflow layers on top of each other and just going deeper into the network

37:52.000 --> 37:59.040
okay so now this is great because now we have at least a solid foundational understanding of how to

37:59.040 --> 38:03.440
not only define a single neuron but how to define an entire neural network and you should be able

38:03.440 --> 38:10.320
to actually explain at this point or understand how information goes from input through an entire

38:10.320 --> 38:16.320
neural network to compute an output so now let's look at how we can apply these neural networks

38:16.320 --> 38:22.640
to solve a very real problem that I'm sure all of you care about so here's a problem on how we

38:22.640 --> 38:28.320
want to build an AI system to learn to answer the following question which is will I pass this

38:28.320 --> 38:35.440
class right I'm sure all of you are really worried about this question so to do this let's start with

38:35.440 --> 38:41.760
a simple input feature model the feature the two features that let's concern ourselves with are

38:41.760 --> 38:49.520
going to be number one how many lectures you attend and number two how many hours you spend on your

38:49.520 --> 38:56.640
final project so let's look at some of the past years of this class right we can actually observe

38:56.640 --> 39:03.360
how different people have lived in this space right between how many lectures and how much

39:03.360 --> 39:08.800
time you spent on your final project and you can actually see every point is a person the color of

39:08.880 --> 39:13.520
that point is going to be if they passed or failed the class and you can see and visualize

39:13.520 --> 39:19.280
kind of this this feature space if you will that we talked about before and then we have you you

39:19.280 --> 39:27.440
follow right here you're the point four five right in between the this this feature space you've

39:27.440 --> 39:32.080
attended four lectures and you will spend five hours on the final project and you want to build a

39:32.080 --> 39:38.080
neural network to determine given everyone else in the class right that I've seen from all of the

39:38.080 --> 39:43.840
previous years you want to help you want to have your neural network help you to understand what is

39:43.840 --> 39:50.320
your likelihood that you will pass or fail this class so let's do it we now have all of the building

39:50.320 --> 39:55.680
blocks to solve this problem using a neural network let's do it so we have two inputs those inputs

39:55.680 --> 40:00.800
are number of lectures you attend and number of hours you spend on your final project it's four

40:00.800 --> 40:09.040
and five we can pass those two inputs to our two x1 and x2 variables these are fed into this single

40:09.040 --> 40:15.680
layered single hidden layered neural network it has three hidden units in the middle and we can see

40:15.680 --> 40:22.800
that the final predicted output probability for you to pass this class is 0.1 or 10 right so very

40:22.800 --> 40:31.520
bleak outcome it's not a good outcome the actual probability is one right so attending four out

40:31.520 --> 40:35.440
of the five lectures and spending five hours on your final project you actually lived in a part of

40:35.440 --> 40:38.800
the feature space which was actually very positive right it looked like you were going to pass the

40:38.800 --> 40:44.000
class so what happened here anyone have any ideas so why did the neural network get this so terribly

40:44.000 --> 40:51.200
wrong right exactly so this neural network is not trained we haven't shown it any of that data the

40:51.280 --> 40:57.200
green and red data right so you should really think of neural networks like babies right before

40:57.200 --> 41:03.120
they see data they haven't learned anything there's no expectation that we should have for them to be

41:03.120 --> 41:06.880
able to solve any of these types of problems before we teach them something about the world

41:07.440 --> 41:12.880
so let's teach this neural network something about the problem first right and to train it we first

41:12.880 --> 41:20.000
need to tell our neural network when it's making bad decisions right so we need to teach it right

41:20.000 --> 41:25.600
really train it to learn exactly like how we as humans learn in some ways right so we have to inform

41:25.600 --> 41:31.440
the neural network when it gets the answer incorrect so that it can learn how to get the answer correct

41:32.080 --> 41:38.480
right so the closer the answer is to the ground truth so right so for example the actual value

41:38.480 --> 41:44.080
for you passing this class was probability one one hundred percent but it predicted a probability

41:44.080 --> 41:50.080
of zero point one we compute what's called a loss right so the closer these two things are together

41:50.080 --> 41:54.640
the smaller your loss should be and the and the more accurate your model should be

41:56.720 --> 42:02.400
so let's assume that we have data not just from one student but now we have data from many students

42:02.400 --> 42:05.840
we many students have taken this class before and we can plug all of them into the neural

42:05.840 --> 42:11.360
network and show them all to this to this system now we care not only about how the neural network

42:11.360 --> 42:17.200
did on just this one prediction but we care about how it predicted on all of these different people

42:17.200 --> 42:22.240
that the neural network has shown in the past as well during this training and learning process

42:22.880 --> 42:29.680
so when training your neural network we want to find a network that minimizes the empirical loss

42:29.680 --> 42:35.120
between our predictions and those ground truth outputs and we're going to do this on average

42:35.120 --> 42:42.640
across all of the different inputs that the that the model has seen if we look at this problem

42:42.640 --> 42:49.520
of binary classification right between yeses and nos right will I pass the class or will I not pass

42:49.520 --> 42:55.680
the class it's a year zero or one probability and we can use what is called the softmax function

42:55.680 --> 43:02.080
or the softmax cross entropy function to be able to inform if this network is getting the answer

43:02.080 --> 43:07.280
correct or incorrect right the softmax cross or the cross entropy function think of this as a

43:07.280 --> 43:13.360
as an objective function it's a loss function that tells our neural network how far away these two

43:13.360 --> 43:18.240
probability distributions are right so the output is a probability distribution we're trying to

43:18.240 --> 43:24.000
determine how bad of an answer the neural network is predicting so that we can give it feedback to

43:24.000 --> 43:30.880
get a better answer now let's suppose instead of training a or predicting a binary output we want

43:30.880 --> 43:36.320
to predict a real valued output like a like any number it can take any number plus or minus

43:36.320 --> 43:43.520
infinity so for example if you want to predict the grade that you get in a class right it doesn't

43:43.520 --> 43:48.560
necessarily need to be between zero and one or zero and a hundred even right you could now use a

43:48.560 --> 43:53.840
different loss in order to produce that value because our outputs are no longer a probability

43:53.840 --> 43:59.280
distribution right so for example what you might do here is compute a mean squared error probably

43:59.520 --> 44:04.800
mean squared error loss function between your true value or your true grade of the class

44:04.800 --> 44:09.680
and the predicted grade right these are two numbers they're not probabilities necessarily

44:09.680 --> 44:15.600
you compute their difference you square it to look at a distance between the two an absolute

44:15.600 --> 44:23.040
distance right sign doesn't matter and then you can minimize this thing right okay great so let's

44:23.120 --> 44:28.800
put all of this loss information with this problem of finding our network weights into a

44:28.800 --> 44:33.440
unified problem and a unified solution to actually train our neural network

44:34.880 --> 44:40.800
so we know that we want to find a neural network that will solve this problem on all this data

44:40.800 --> 44:46.640
on average right that's how we contextualize this problem earlier in the lectures this means

44:46.640 --> 44:52.640
effectively that we're trying to solve or we're trying to find what are the weights for our neural

44:52.640 --> 44:57.920
network what are this this big vector w that we talked about in earlier in the lecture what is

44:57.920 --> 45:05.280
this vector w compute this vector w for me based on all of the data that we have seen right now

45:05.280 --> 45:12.880
the vector w is also going to determine what is the loss right so given a single vector w

45:12.880 --> 45:19.440
we can compute how bad is this neural network performing on our data right so what is the loss

45:19.440 --> 45:24.800
what is this deviation from the ground truth of our network uh based on where it should be

45:26.160 --> 45:34.560
now remember that w is just a group of a bunch of numbers right it's a very big list of numbers

45:34.560 --> 45:41.760
a list of weights uh for every single layer and every single neuron in our neural network right

45:41.760 --> 45:47.360
so it's just a very big list or a vector of weights we want to find that vector what is that

45:47.360 --> 45:52.400
vector based on a lot of data that's the problem of training a neural network and remember our

45:52.400 --> 45:58.880
loss function is just a simple function of our weights if we have only two weights in our neural

45:58.880 --> 46:04.800
network like we saw earlier in the slide then we can plot the loss landscape over this two-dimensional

46:04.800 --> 46:11.760
space right so we have two weights w one and w two and for every single configuration or setting of

46:11.760 --> 46:17.120
those two weights our loss will have a particular value which here we're showing is the height of

46:17.120 --> 46:24.480
this graph right so for any w one and w two what is the loss and what we want to do is find the

46:24.480 --> 46:31.440
lowest point what is the best loss where what are the weights such that our loss will be as good as

46:31.440 --> 46:36.800
possible so the smaller the loss the better so we want to find the lowest point in this graph

46:38.480 --> 46:44.400
now how do we do that right so the way this works is we start somewhere in this space we

46:44.400 --> 46:51.280
don't know where to start so let's pick a random place to start right now from that place let's

46:51.280 --> 46:57.520
compute what's called the gradient of the landscape at that particular point this is a very local

46:57.520 --> 47:05.120
estimate of where is going up basically where where is the slope increasing at my current location

47:05.120 --> 47:10.240
right that informs us not only where the slope is increasing but more importantly where the slope

47:10.240 --> 47:15.520
is decreasing if i negate the direction if i go in the opposite direction i can actually step down

47:15.520 --> 47:22.960
into the landscape and change my weights such that i lower my loss so let's take a small step

47:22.960 --> 47:28.080
just a small step in the opposite direction of the part that's going up let's take a small step

47:28.080 --> 47:33.600
going down and we'll keep repeating this process we'll compute a new gradient at that new point

47:33.600 --> 47:37.360
and it will take another small step and we'll keep doing this over and over and over again

47:37.360 --> 47:43.440
until we converge at what's called a local minimum right so based on where we started it may not be

47:43.440 --> 47:48.720
a global minimum of everywhere in this lost landscape but let's find ourselves now in a local minimum

47:48.720 --> 47:53.200
and we're guaranteed to actually converge by following this very simple algorithm at a local

47:53.200 --> 47:59.120
minimum so let's summarize now this algorithm this algorithm is called gradient descent let's

47:59.120 --> 48:04.720
summarize it first in pseudocode and then we'll look at it in actual code in a second so there's

48:04.720 --> 48:11.440
a few steps first step is we initialize our location somewhere randomly in this weight space

48:11.440 --> 48:19.200
right we compute the gradient of of our loss at with respect to our weights okay

48:19.920 --> 48:24.400
and then we take a small step in the opposite direction and we keep repeating this in a loop

48:24.400 --> 48:28.800
over and over and over again and we say we keep we'll keep doing this until convergence right

48:28.800 --> 48:33.680
until we stop moving basically and our network basically finds where it's supposed to end up

48:34.400 --> 48:40.880
we'll talk about this this small step right so we're multiplying our gradient by what i keep

48:40.880 --> 48:46.800
calling is a small step we'll talk about that a bit more about a bit more and later part of this

48:46.800 --> 48:53.120
this lecture but for now let's also very quickly show the analogous part in in code as well and

48:53.120 --> 48:58.480
it mirrors very nicely right so we'll randomly initialize our weights this happens every time

48:58.480 --> 49:02.480
you train a neural network you have to randomly initialize the weights and then you have a loop

49:03.120 --> 49:07.600
right here showing it with without even convergence right we're just going to keep looping forever

49:08.240 --> 49:13.360
where we say okay we're going to compute the loss at that location compute the gradient so which

49:13.360 --> 49:19.360
way is up and then we just negate that gradient multiply it by some what's called learning rate

49:19.360 --> 49:24.320
lr denoted here it's a small step and then we take a direction in that small step

49:26.320 --> 49:30.560
so let's take a deeper look at this term here this is called the gradient right this tells us

49:30.560 --> 49:36.560
which way is up in that landscape and this again it tells us even more than that it tells us how

49:36.560 --> 49:43.520
is our landscape how is our loss changing as a function of all of our weights but i actually

49:43.520 --> 49:48.320
have not told you how to compute this so let's talk about that process that process is called

49:48.320 --> 49:54.240
back propagation we'll go through this very very briefly and we'll start with the simplest neural

49:54.240 --> 49:59.440
network uh that's possible right so we already saw the simplest building block which is a single

49:59.440 --> 50:04.880
neuron now let's build the simplest neural network which is just a one neuron neural network right

50:04.880 --> 50:10.320
so it has one hidden neuron it goes from input to hidden neuron to output and we want to compute

50:10.320 --> 50:16.960
the gradient of our loss with respect to this weight w2 okay so i'm highlighting it here

50:16.960 --> 50:23.600
so we have two weights let's compute the gradient first with respect to w2 and that tells us how

50:23.600 --> 50:31.200
much does a small change in w2 affect our loss does our loss go up or down if we move our w2

50:31.200 --> 50:36.880
a little bit in one direction or another so let's write out this derivative we can start by applying

50:36.880 --> 50:44.480
the chain rule backwards from the loss through the output and specifically we can actually decompose

50:44.480 --> 50:50.960
this law this derivative this gradient into two parts right so the first part we're decomposing

50:50.960 --> 51:02.160
it from dj dw2 into dj dy right which is our output multiplied by dy dw2 right this is all

51:02.160 --> 51:08.800
possible right it's a chain rule it's a bit i'm just reciting a chain rule here from calculus

51:08.800 --> 51:14.160
this is possible because y is only dependent on the previous layer and now let's suppose we don't

51:14.160 --> 51:18.800
want to do this for w2 but we want to do it for w1 we can use the exact same process right but now

51:18.800 --> 51:25.200
it's one step further right we'll now replace w2 with w1 we need to apply the chain rule yet again

51:25.200 --> 51:29.760
once again to decompose the problem further and now we propagate our old gradient that we computed

51:29.760 --> 51:35.680
for w2 all the way back one more step uh to the weight that we're interested in which in this

51:35.680 --> 51:41.920
case is w1 and we keep repeating this process over and over again propagating these gradients

51:41.920 --> 51:47.840
backwards from output to input to compute ultimately what we want in the end is this

51:47.840 --> 51:53.520
derivative of every weight so the lot the derivative of our loss with respect to every

51:53.520 --> 51:57.680
weight in our neural network this tells us how much does a small change in every single weight

51:57.680 --> 52:02.400
in our network affect the loss does our loss go up or down if we change this weight a little bit

52:02.400 --> 52:11.840
in this direction or a little bit in that direction yes neuron and perceptron are the same so

52:11.840 --> 52:17.040
typically people say neural network which is why like a single neuron it's also gotten popularity

52:17.040 --> 52:22.720
but originally a perceptron is the the formal term the two terms are identical

52:25.200 --> 52:30.640
okay so now we've covered a lot so we've covered the forward propagation of information through a

52:30.640 --> 52:36.560
neuron and through a neural network all the way through and we've covered now the back propagation

52:36.560 --> 52:41.680
of information to understand how we should change every single one of those weights in our neural

52:41.680 --> 52:49.280
network to improve our loss so that was the back prop algorithm in theory it's actually

52:49.280 --> 52:53.760
pretty simple it's just a chain rule right there's nothing there's actually nothing more than than

52:53.760 --> 52:58.800
just the chain rule and the nice part is that deep learning libraries actually do this for you so

52:58.800 --> 53:02.400
they compute back prop for you you don't actually have to implement it yourself which is very

53:02.400 --> 53:08.080
convenient but now it's important to touch on even though the theory is actually not that complicated

53:08.080 --> 53:13.200
for back propagation let's touch on it now from practice now thinking a little bit towards your

53:13.200 --> 53:17.680
own implementations when you want to implement these neural networks what are some insights

53:18.400 --> 53:23.520
so optimization of neural networks in practice is a completely different story it's not

53:23.520 --> 53:28.720
straightforward at all and in practice it's very difficult and usually very computationally

53:28.720 --> 53:33.840
intensive to do this back prop algorithm so here's an illustration from a paper that came

53:33.840 --> 53:39.280
out a few years ago that actually attempted to visualize a very deep neural networks loss

53:39.280 --> 53:45.040
landscape so previously we had that other depiction visualization of how a neural network would

53:45.040 --> 53:50.960
look in a two-dimensional landscape real neural networks are not two-dimensional they're hundreds

53:50.960 --> 53:57.440
or millions or billions of dimensions and now what would those lost landscapes look like you can

53:57.440 --> 54:02.080
actually try some clever techniques to actually visualize them this is one paper that attempted

54:02.160 --> 54:09.600
to do that and it turns out that they look extremely messy right the important thing is that

54:09.600 --> 54:14.240
if you do this algorithm and you start in a bad place depending on your neural network you may

54:14.240 --> 54:19.760
not actually end up in the global solution right so your initialization matters a lot

54:19.760 --> 54:24.320
and you need to kind of traverse these local minima and try to try and help you find the

54:24.320 --> 54:31.280
global minima or even more than that you need to construct neural networks that have lost landscapes

54:31.280 --> 54:35.600
that are much more amenable to optimization than this one right so this is a very bad loss

54:35.600 --> 54:40.400
landscape there are some techniques that we can apply to our neural networks that smooth out

54:40.400 --> 54:46.160
their lost landscape and make them easier to optimize so recall that update equation that

54:46.160 --> 54:50.800
we talked about earlier with gradient descent right so there is this parameter here that we

54:50.800 --> 54:55.280
didn't talk about we we described this as the little step that you could take right so it's a

54:55.280 --> 55:00.400
small number that you multiply with the direction which is your gradient it just tells you okay i'm

55:00.400 --> 55:04.400
not going to just go all the way in this direction i'll just take a small step in this direction

55:05.040 --> 55:10.800
so in practice even setting this value right it's just one number setting this one number can be

55:10.800 --> 55:18.560
rather difficult right if we set the learning rate too small then the model can get stuck

55:18.560 --> 55:23.520
in these local minima right so here it starts and it kind of gets stuck in this local minima

55:23.520 --> 55:28.400
it converges very slowly even if it doesn't get stuck if the learning rate is too large it can

55:28.400 --> 55:34.560
kind of overshoot and in practice it even diverges and explodes and you don't actually ever find

55:34.560 --> 55:41.520
any minima now ideally what we want is to use learning rates that are not too small and not too

55:41.520 --> 55:47.680
large to so they're large enough to basically avoid those local minima but small enough such

55:47.680 --> 55:53.280
that they won't diverge and they will actually still find their way into the global minima so

55:53.280 --> 55:57.360
something like this is what you should intuitively have in mind right so something i can overshoot

55:57.360 --> 56:03.200
the local minimas but find itself into a better minima and then finally stabilize itself there

56:03.760 --> 56:08.080
so how do we actually set these learning rates right in practice what does that process look

56:08.080 --> 56:13.840
like now idea number one is is very basic right try a bunch of different learning rates and see

56:13.840 --> 56:19.840
what works and that's actually a not a bad process in practice it's one of the processes that people

56:19.840 --> 56:25.600
use so that that's uh that's interesting but let's see if we can do something smarter than this

56:25.600 --> 56:32.320
and let's see how we can design algorithms that can adapt to the landscapes right so in practice

56:32.320 --> 56:38.080
there's no reason why there should be a single number right can we have learning rates that adapt

56:38.080 --> 56:43.840
to the model to the data to the landscapes to the gradients that it's seeing around so this means

56:43.840 --> 56:50.160
that the learning rate may actually increase or decrease as a function of the gradients in the

56:50.160 --> 56:55.680
loss function right how fast we're learning or many other options right there are many different

56:55.680 --> 57:02.080
ideas that could be done here and in fact there are many widely used different procedures or

57:02.080 --> 57:08.000
methodologies for setting the learning rate and during your labs we actually encourage you to try

57:08.000 --> 57:12.480
out some of these different ideas for different types of learning rates and and even play around

57:12.480 --> 57:16.720
with you know what what's the effect of increasing or decreasing your learning rate you'll see very

57:16.720 --> 57:21.280
striking differences

57:28.160 --> 57:33.680
so so a few things what number one is that it's not a closed space right so there's an infinite

57:33.680 --> 57:40.080
every every weight can be plus or minus up to infinity right so even if it was a one-dimensional

57:40.080 --> 57:46.240
neural network with just one weight it's not a closed space in practice it's even worse than that

57:46.240 --> 57:53.920
because you have billions of dimensions right so not only is your space your support system in

57:53.920 --> 57:59.520
one dimension is it infinite but you now have billions of infinite dimensions right or billions

57:59.520 --> 58:04.800
of infinite support spaces so it's not something that you can just like search every weight every

58:04.800 --> 58:10.000
possible weight in your neural in your configuration or what is every possible weight that this neural

58:10.000 --> 58:15.760
network could take and let me test them out because it's not practical to do even for a very small

58:15.760 --> 58:23.120
neural network in practice so in your labs you can really try to put all of this information

58:23.920 --> 58:31.280
in this picture into practice which defines your model number one right here defines your

58:31.280 --> 58:36.480
optimizer which previously we denoted as this gradient descent optimizer here we're calling it

58:37.120 --> 58:43.040
stochastic gradient center SGD we'll talk about that more in a second and then also note that

58:43.040 --> 58:49.200
your optimizer which here we're calling SGD could be any of these adaptive optimizers you can swap

58:49.200 --> 58:53.360
them out and you should swap them out you should test different things here to see the impact of

58:53.360 --> 59:00.080
these different methods on your training procedure and you'll gain very valuable intuition for the

59:00.080 --> 59:04.480
different insights that will come with that as well so I want to continue very briefly just for

59:04.480 --> 59:09.840
the end of this lecture to talk about tips for training neural networks in practice and how we

59:09.920 --> 59:16.800
can focus on this powerful idea of really what's called batching data right not seeing all of your

59:16.800 --> 59:23.520
data but now talking about a topic called batching so to do this let's very briefly revisit this

59:23.520 --> 59:28.960
gradient descent algorithm the gradient is actually compute this gradient computation the back prop

59:28.960 --> 59:34.800
algorithm I mentioned this earlier it's a very computationally expensive operation and it's

59:34.800 --> 59:39.760
even worse because we now are we previously described in a way where we would have to compute

59:39.760 --> 59:45.520
it over a summation over every single data point in our entire data set right that's how we defined

59:45.520 --> 59:49.760
it with the loss functions an average over all of our data points which means that we're summing

59:49.760 --> 59:55.280
over all of our data points the gradients so in most real-life problems this would be completely

59:55.280 --> 01:00:00.480
infeasible to do because our data sets are simply too big and the models are too big to compute those

01:00:00.480 --> 01:00:04.880
gradients on every single iteration remember this isn't just a one-time thing right it's every

01:00:04.880 --> 01:00:09.840
single step that you do you keep taking small steps so you keep me you keep needing to repeat this

01:00:09.840 --> 01:00:15.760
process so instead let's define a new gradient descent algorithm called SGD stochastic gradient

01:00:15.760 --> 01:00:22.240
descent instead of computing the gradient over the entire data set now let's just pick a single

01:00:22.240 --> 01:00:28.720
training point and compute that one training points gradient right the nice thing about that is that

01:00:28.720 --> 01:00:35.040
it's much easier to compute that gradient right it only needs one point and the downside is that

01:00:35.040 --> 01:00:40.480
it's very noisy it's very stochastic since it was computed using just that one example right so you

01:00:40.480 --> 01:00:46.560
have that that trade-off that exists so what's the middle ground right the middle ground is to take

01:00:46.560 --> 01:00:52.320
not one data point and not the full data set but a batch of data right so take a what's called a

01:00:52.320 --> 01:00:58.400
mini batch right this could be something in practice like 32 pieces of data is a common batch size

01:00:58.480 --> 01:01:03.120
and this gives us an estimate of the true gradient right so you approximate the gradient

01:01:03.120 --> 01:01:09.600
by averaging the gradient of these 32 samples it's still fast because 32 is much smaller than

01:01:09.600 --> 01:01:15.040
the size of your entire data set but it's pretty quick now right it's still noisy but it's okay

01:01:15.040 --> 01:01:21.200
usually in practice because you can still iterate much faster and since b is normally not that large

01:01:21.200 --> 01:01:27.120
again think of something like in the tens or the hundreds of samples it's very fast to compute this

01:01:27.200 --> 01:01:32.400
in practice compared to regular gradient descent and it's also much more accurate compared to

01:01:32.400 --> 01:01:38.240
stochastic gradient descent and the increase in accuracy of this gradient estimation allows

01:01:38.240 --> 01:01:43.840
us to converge to our solution significantly faster as well right it's not only about the

01:01:43.840 --> 01:01:48.880
speed it's just about the increase in accuracy of those gradients allows us to get to our solution

01:01:48.880 --> 01:01:54.720
much faster which ultimately means that we can train much faster as well and we can save compute

01:01:55.280 --> 01:02:01.840
and the other really nice thing about mini batches is that they allow for parallelizing our

01:02:01.840 --> 01:02:06.080
computation right and that was a concept that we had talked about earlier in the class as well

01:02:06.080 --> 01:02:11.520
and here's where it's coming in we can split up those batches right so those 32 pieces of data

01:02:11.520 --> 01:02:17.040
let's say if our batch size is 32 we can split them up onto different workers right different

01:02:17.040 --> 01:02:23.440
parts of the GPU can tackle those different parts of our data points this can allow us to

01:02:23.440 --> 01:02:30.080
basically achieve even more significant speed-ups using GPU architectures and GPU hardware okay

01:02:30.080 --> 01:02:34.800
finally the last topic I want to talk about before we end this lecture and move on to lecture number

01:02:34.800 --> 01:02:40.960
two is overfitting right so overfitting is this idea that is actually not a deep learning

01:02:40.960 --> 01:02:45.840
centric problem at all it's it's a problem that exists in all of machine learning right the key

01:02:45.840 --> 01:02:54.800
problem is that and the key problem is actually one that addresses how you can accurately define

01:02:55.600 --> 01:03:02.800
if your model is is actually capturing your true data set right or if it's just learning kind of

01:03:02.800 --> 01:03:09.600
the subtle details that are kind of spuriously correlating to your data set so set differently

01:03:09.600 --> 01:03:14.960
let me say it a bit differently now so let's say we want to build models that can learn

01:03:15.040 --> 01:03:21.920
representations okay from our training data that still generalize to brand new unseen

01:03:22.640 --> 01:03:26.800
test points right that's the real goal here is we want to teach our model something based on a

01:03:26.800 --> 01:03:31.200
lot of training data but then we don't want it to do well in the training day we want it to do well

01:03:31.200 --> 01:03:35.440
when we deploy it into the real world and it's seeing things that it has never seen during training

01:03:36.000 --> 01:03:42.560
so the concept of overfitting is exactly addressing that problem overfitting means if

01:03:43.520 --> 01:03:49.760
if your model is doing very well on your training data but very badly in testing it

01:03:49.760 --> 01:03:54.800
that means it's overfitting it's overfitting to the training data that it saw on the other hand

01:03:54.800 --> 01:04:00.080
there's also underfitting right on the left hand side you can see basically not fitting the data

01:04:00.080 --> 01:04:04.960
enough which means that you know you're going to achieve very similar performance on your testing

01:04:04.960 --> 01:04:11.200
distribution but both are underperforming the actual capabilities of your system now ideally

01:04:11.280 --> 01:04:16.320
you want to end up somewhere in the middle which is not too complex where you're memorizing all of

01:04:16.320 --> 01:04:21.520
the nuances in your training data like on the right but you still want to continue to perform

01:04:22.080 --> 01:04:28.000
well even based on the brand new data so you're not underfitting as well so to a to actually

01:04:28.000 --> 01:04:31.520
address this problem in neural networks and in machine learning in general there's a few

01:04:31.520 --> 01:04:36.240
different ways that you should be aware of and how to do it because you'll need to apply them

01:04:36.240 --> 01:04:41.360
as part of your solutions in your software labs as well so the key concept here is called

01:04:41.360 --> 01:04:47.840
regularization right regularization is a technique that you can introduce and said very simply all

01:04:47.840 --> 01:04:56.880
regularization is is a way to discourage your model from from these nuances in your training

01:04:56.880 --> 01:05:01.840
data from being learned that's all it is and as we've seen before it's actually critical for our

01:05:01.840 --> 01:05:06.960
models to be able to generalize you know not just on training data but really what we care about

01:05:06.960 --> 01:05:13.120
is the testing data so the most popular regularization technique that's important for you to understand

01:05:13.120 --> 01:05:18.800
is this very simple idea called dropout let's revisit this picture of a deep neural network

01:05:18.800 --> 01:05:23.840
that we've been seeing all lecture right in dropout our training during training what we're

01:05:23.840 --> 01:05:29.760
going to do is randomly set some of the activations right these outputs of every single neuron

01:05:30.720 --> 01:05:36.880
to zero which is randomly going to set them to zero with some probability right so let's say

01:05:36.880 --> 01:05:42.800
50 percent is our probability that means that we're going to take all of the activation insert

01:05:42.800 --> 01:05:48.400
in our neural network and with a probability of 50 before we pass that activation on to the next

01:05:48.400 --> 01:05:54.800
neuron we're just going to set it to zero and not pass on anything so effectively 50 percent of the

01:05:54.880 --> 01:06:00.720
neurons are going to be kind of shut down or killed in a forward pass and you're only going

01:06:00.720 --> 01:06:06.560
to forward pass information with the other 50 percent of your neurons so this idea is extremely

01:06:06.560 --> 01:06:11.360
powerful actually because it lowers the capacity of our neural network it not only lowers the

01:06:11.360 --> 01:06:16.480
capacity of our neural network but it's dynamically lowering it because on the next iteration we're

01:06:16.480 --> 01:06:21.280
going to pick a different 50 percent of neurons that we drop out so constantly the network is going

01:06:21.280 --> 01:06:27.280
to have to learn to build pathways different pathways from input to output and that it can't

01:06:27.280 --> 01:06:32.880
rely on any small any small part of the features that are present in any part of the training

01:06:32.880 --> 01:06:37.840
dataset too extensively right because it's constantly being forced to find these different

01:06:37.840 --> 01:06:44.400
pathways with random probabilities so let's drop out the second regularization technique

01:06:44.400 --> 01:06:48.720
is going to be this notion called early stopping which is actually something that

01:06:48.720 --> 01:06:53.040
is model agnostic you can apply this to any type of model as long as you have a testing set that

01:06:53.040 --> 01:06:59.600
you can play around with so the idea here is that we have already a pretty formal mathematical

01:06:59.600 --> 01:07:04.880
definition of what it means to overfit right overfitting is just when our model starts to

01:07:04.880 --> 01:07:11.440
perform worse on our test set that's really all it is right so what if we plot over the course

01:07:11.440 --> 01:07:16.160
of training so x-axis is as we're training the model let's look at the performance on both the

01:07:16.160 --> 01:07:21.840
training set and the test set so in the beginning you can see that the training set and the test

01:07:21.840 --> 01:07:26.960
set are both going down and they continue to go down which is excellent because it means that

01:07:26.960 --> 01:07:32.720
our model is getting stronger eventually though what you'll notice is that the test loss plateaus

01:07:33.360 --> 01:07:38.800
and starts to increase on the other hand the training loss there's no reason why the training

01:07:38.800 --> 01:07:44.320
loss should ever need to stop going down right training losses generally always continue to

01:07:44.400 --> 01:07:50.640
decay as long as there is capacity in the neural network to learn those differences right but the

01:07:50.640 --> 01:07:55.680
important point is that this continues for the rest of training and we want to basically

01:07:56.400 --> 01:08:00.080
we care about this point right here right this is the really important point because

01:08:01.040 --> 01:08:05.600
this is where we need to stop training right after this point this is the happy medium because

01:08:05.600 --> 01:08:11.920
after this point we start to overfit on parts of the data where our training accuracy becomes

01:08:11.920 --> 01:08:17.280
actually better than our testing accuracy so our testing accuracy is going bad it's getting worse

01:08:17.280 --> 01:08:21.840
but our training accuracy is still improving so it means overfitting on the other hand on the left

01:08:21.840 --> 01:08:28.560
hand side this is the opposite problem right we have not fully utilized the capacity of our model

01:08:28.560 --> 01:08:34.480
and the testing accuracy can still improve further right this is a very powerful idea but it's actually

01:08:34.480 --> 01:08:39.280
extremely easy to implement in practice because all you really have to do is just monitor the loss

01:08:39.920 --> 01:08:43.040
of over the course of training right and you just have to pick the model

01:08:43.040 --> 01:08:45.440
where the testing accuracy starts to get worse

01:08:47.520 --> 01:08:52.560
so i'll conclude this lecture by just summarizing three key points that we've covered in the class

01:08:52.560 --> 01:08:57.840
so far and this is a very gem packed class so the entire week is going to be like this

01:08:57.840 --> 01:09:02.960
and today is just the start so so far we've learned the fundamental building blocks of neural

01:09:02.960 --> 01:09:07.760
networks starting all the way from just one neuron also called a perceptron we learned that we can

01:09:07.760 --> 01:09:14.160
stack these systems on top of each other to create a hierarchical network and how we can

01:09:14.160 --> 01:09:18.720
mathematically optimize those types of systems and then finally in the very last part of the

01:09:18.720 --> 01:09:23.840
class we talked about just techniques tips and techniques for actually training and applying

01:09:23.840 --> 01:09:29.680
these systems into practice now in the next lecture we're going to hear from Ava on deep

01:09:29.680 --> 01:09:37.040
sequence modeling using RNNs and also a really new and exciting algorithm and type of model called

01:09:37.040 --> 01:09:42.880
the transformer which is built off of this principle of attention you're going to learn

01:09:42.880 --> 01:09:47.600
about in the next class but let's for now just take a brief pause and let's resume in about

01:09:47.600 --> 01:09:56.400
five minutes just so we can switch speakers and Ava can start her presentation okay thank you

