1
00:00:00,000 --> 00:00:15,320
Good afternoon everyone and welcome to MIT Success 191.

2
00:00:15,320 --> 00:00:19,360
My name is Alexander Amini and I'll be one of your instructors for the course this year

3
00:00:19,360 --> 00:00:21,440
along with Ava.

4
00:00:21,440 --> 00:00:25,280
And together we're really excited to welcome you to this really incredible course.

5
00:00:25,280 --> 00:00:33,200
This is a very fast paced and very intense one week that we're about to go through together.

6
00:00:33,200 --> 00:00:38,840
So we're going to cover the foundations of a also very fast paced moving field and a

7
00:00:38,840 --> 00:00:43,640
field that has been rapidly changing over the past eight years that we have taught this

8
00:00:43,640 --> 00:00:45,720
course at MIT.

9
00:00:45,720 --> 00:00:52,240
Now over the past decade in fact, even before we started teaching this course, AI and deep

10
00:00:52,240 --> 00:00:58,320
learning has really been revolutionizing so many different advances and so many different

11
00:00:58,320 --> 00:01:02,860
areas of science, mathematics, physics and so on.

12
00:01:02,860 --> 00:01:12,080
And not that long ago, we were having challenges and problems that we did not think were necessarily

13
00:01:12,080 --> 00:01:20,840
solvable in our lifetimes that AI is now actually solving beyond human performance today.

14
00:01:20,840 --> 00:01:26,400
And each year that we teach this course, this lecture in particular is getting harder

15
00:01:26,400 --> 00:01:32,800
and harder to teach because for an introductory level course, this lecture, lecture number

16
00:01:32,800 --> 00:01:36,560
one, is the lecture that's supposed to cover the foundations and if you think to any other

17
00:01:36,560 --> 00:01:43,040
introductory course like an introductory course 101 on mathematics or biology, those

18
00:01:43,040 --> 00:01:46,280
lecture ones don't really change that much over time.

19
00:01:46,280 --> 00:01:51,960
So we're in a rapidly changing field of AI and deep learning where even these types

20
00:01:51,960 --> 00:01:55,560
of lectures are rapidly changing.

21
00:01:55,560 --> 00:02:01,800
So let me give you an example of how we introduced this course only a few years ago.

22
00:02:01,800 --> 00:02:11,020
Hi everybody and welcome to MIT 6S191, the official introductory course on deep learning

23
00:02:11,020 --> 00:02:14,720
taught here at MIT.

24
00:02:14,720 --> 00:02:22,260
Deep learning is revolutionizing so many fields from robotics to medicine and everything

25
00:02:22,260 --> 00:02:24,360
in between.

26
00:02:24,360 --> 00:02:31,480
You'll learn the fundamentals of this field and how you can build some of these incredible

27
00:02:31,480 --> 00:02:33,480
algorithms.

28
00:02:33,480 --> 00:02:42,240
In fact, this entire speech and video are not real and were created using deep learning

29
00:02:42,240 --> 00:02:45,960
and artificial intelligence.

30
00:02:45,960 --> 00:02:49,420
And in this class, you'll learn how.

31
00:02:49,420 --> 00:02:58,000
It has been an honor to speak with you today and I hope you enjoy the course.

32
00:02:58,000 --> 00:03:05,280
The really surprising thing about that video to me when we first did it was how viral it

33
00:03:05,280 --> 00:03:07,000
went a few years ago.

34
00:03:07,000 --> 00:03:11,440
So just in a couple months about teaching this course a few years ago, that video went

35
00:03:11,440 --> 00:03:16,640
very viral, it got over a million views within only a few months.

36
00:03:16,640 --> 00:03:22,840
People were shocked with a few things but the main one was the realism of AI to be able

37
00:03:22,840 --> 00:03:30,520
to generate content that looks and sounds extremely hyper realistic.

38
00:03:30,520 --> 00:03:35,360
And when we did this video, when we created this for the class only a few years ago, this

39
00:03:35,360 --> 00:03:42,480
video took us about $10,000 in compute to generate just about a minute long video.

40
00:03:42,480 --> 00:03:47,160
If you think about it, I would say it's extremely expensive to compute something what we look

41
00:03:47,160 --> 00:03:51,360
at like that and maybe a lot of you are not really impressed by the technology today because

42
00:03:51,360 --> 00:03:56,520
you see all of the amazing things that AI and deep learning are producing.

43
00:03:56,520 --> 00:04:01,760
Now fast forward today, the progress in deep learning and people were making all kinds

44
00:04:01,760 --> 00:04:05,360
of exciting remarks about it when it came out a few years ago.

45
00:04:05,360 --> 00:04:11,240
Now this is common stuff because AI is really doing much more powerful things than this

46
00:04:11,240 --> 00:04:14,200
fun little introductory video.

47
00:04:14,200 --> 00:04:20,120
So today fast forward four years, about four years to today.

48
00:04:20,120 --> 00:04:21,120
Now where are we?

49
00:04:21,120 --> 00:04:27,760
AI is now generating content with deep learning being so commoditized.

50
00:04:27,760 --> 00:04:34,160
Deep learning is in all of our fingertips now online in our smartphones and so on.

51
00:04:34,160 --> 00:04:41,320
In fact, we can use deep learning to generate these types of hyper realistic pieces of media

52
00:04:41,320 --> 00:04:46,720
and content entirely from English language without even coding anymore.

53
00:04:46,720 --> 00:04:51,640
So before we had to actually go and train these models and really code them to be able

54
00:04:51,640 --> 00:04:54,280
to create that one minute long video.

55
00:04:54,280 --> 00:04:59,240
Today we have models that will do that for us end to end directly from English language.

56
00:04:59,240 --> 00:05:03,480
So we can prompt these models to create something that the world has never seen before.

57
00:05:03,480 --> 00:05:09,320
A photo of an astronaut riding a horse and these models can imagine those pieces of content

58
00:05:09,320 --> 00:05:11,640
entirely from scratch.

59
00:05:11,640 --> 00:05:17,760
My personal favorite is actually how we can now ask these deep learning models to create

60
00:05:17,760 --> 00:05:22,880
new types of software even themselves being software to ask them to create.

61
00:05:22,880 --> 00:05:27,960
For example, to write this piece of TensorFlow code to train a neural network, right?

62
00:05:27,960 --> 00:05:33,160
We're asking a neural network to write TensorFlow code to train another neural network and our

63
00:05:33,160 --> 00:05:40,480
model can produce examples of functional and usable pieces of code that satisfy this

64
00:05:40,480 --> 00:05:44,920
English prompt while walking through each part of the code independently.

65
00:05:44,920 --> 00:05:49,440
So not even just producing it but actually educating and teaching the user on what each

66
00:05:49,440 --> 00:05:53,520
part of these code blocks are actually doing.

67
00:05:53,520 --> 00:05:55,320
You can see an example here.

68
00:05:55,320 --> 00:06:00,000
And really what I'm trying to show you with all of this is that this is just highlighting

69
00:06:00,000 --> 00:06:06,880
how far deep learning has gone even in a couple years since we've started teaching this course.

70
00:06:06,880 --> 00:06:10,760
I mean going back even from before that to eight years ago.

71
00:06:10,760 --> 00:06:16,640
And the most amazing thing that you'll see in this course in my opinion is that what

72
00:06:16,640 --> 00:06:21,600
we try to do here is to teach you the foundations of all of this, how all of these different

73
00:06:21,600 --> 00:06:27,120
types of models are created from the ground up and how we can make all of these amazing

74
00:06:27,120 --> 00:06:31,600
advances possible so that you can also do it on your own as well.

75
00:06:31,600 --> 00:06:35,200
And like I mentioned in the beginning, this introduction course is getting harder and

76
00:06:35,200 --> 00:06:38,400
harder to do and to make every year.

77
00:06:38,400 --> 00:06:44,520
I don't know where the field is going to be next year and I mean that's my honest truth

78
00:06:44,520 --> 00:06:50,200
or even honestly in even one or two months time from now just because it's moving so

79
00:06:50,200 --> 00:06:51,200
incredibly fast.

80
00:06:51,200 --> 00:06:56,760
But what I do know is that what we will share with you in the course as part of this one

81
00:06:56,760 --> 00:07:01,440
week is going to be the foundations of all of the technologies that we have seen up until

82
00:07:01,440 --> 00:07:06,280
this point that will allow you to create that future for yourselves and to design brand new

83
00:07:06,280 --> 00:07:13,520
types of deep learning models using those fundamentals and those foundations.

84
00:07:13,520 --> 00:07:18,920
So let's get started with all of that and start to figure out how we can actually achieve

85
00:07:18,920 --> 00:07:23,520
all of these different pieces and learn all of these different components.

86
00:07:23,520 --> 00:07:28,720
And we should start this by really tackling the foundations from the very beginning and

87
00:07:28,720 --> 00:07:33,160
asking ourselves, you know, we've heard this term, I think all of you obviously before

88
00:07:33,160 --> 00:07:37,000
you've come to this class today, you've heard the term deep learning, but it's important

89
00:07:37,000 --> 00:07:42,720
for you to really understand how this concept of deep learning relates to all of the other

90
00:07:42,720 --> 00:07:46,200
pieces of science that you've learned about so far.

91
00:07:46,200 --> 00:07:49,860
So to do that, we have to start from the very beginning and start by thinking about what

92
00:07:49,860 --> 00:07:55,320
is intelligence at its core, not even artificial intelligence, but just intelligence, right?

93
00:07:55,320 --> 00:08:02,320
So the way I like to think about this is that I like to think that intelligence is the ability

94
00:08:02,320 --> 00:08:08,800
to process information which will inform your future decision making abilities.

95
00:08:08,800 --> 00:08:12,280
Now that's something that we as humans do every single day.

96
00:08:12,280 --> 00:08:17,720
Now artificial intelligence is simply the ability for us to give computers that same

97
00:08:17,720 --> 00:08:23,560
ability to process information and inform future decisions.

98
00:08:23,560 --> 00:08:28,600
Now machine learning is simply a subset of artificial intelligence.

99
00:08:28,600 --> 00:08:33,760
The way you should think of machine learning is just as the programming ability or let's

100
00:08:33,760 --> 00:08:41,720
say even simpler than that, machine learning is the science of trying to teach computers

101
00:08:41,720 --> 00:08:47,200
how to do that processing of information and decision making from data.

102
00:08:47,200 --> 00:08:52,200
So instead of hard coding some of these rules into machines and programming them like we

103
00:08:52,200 --> 00:08:57,040
used to do in software engineering classes, now we're going to try and do that processing

104
00:08:57,040 --> 00:09:02,640
of information and informing a future decision making abilities directly from data.

105
00:09:02,640 --> 00:09:07,720
And then going one step deeper, deep learning is simply the subset of machine learning which

106
00:09:07,720 --> 00:09:09,880
uses neural networks to do that.

107
00:09:09,880 --> 00:09:16,040
It uses neural networks to process raw pieces of data, now unprocessed data, and allows

108
00:09:16,040 --> 00:09:21,600
them to ingest all of those very large data sets and inform future decisions.

109
00:09:21,600 --> 00:09:25,440
Now that's exactly what this class is really all about.

110
00:09:25,440 --> 00:09:30,120
If you think of, if I had to summarize this class in just one line, it's all about teaching

111
00:09:30,120 --> 00:09:36,640
machines how to process data, process information, and inform decision making abilities from

112
00:09:36,640 --> 00:09:40,720
that data and learn it from that data.

113
00:09:40,720 --> 00:09:45,720
Now this program is split between really two different parts.

114
00:09:45,720 --> 00:09:50,800
So you should think of this class as being captured with both technical lectures, which

115
00:09:50,800 --> 00:09:55,120
for example this is one part of, as well as software labs.

116
00:09:55,120 --> 00:09:59,480
We'll have several new updates this year as I mentioned earlier just covering the rapid

117
00:09:59,480 --> 00:10:04,040
changing of advances in AI and especially in some of the later lectures you're going

118
00:10:04,040 --> 00:10:05,120
to see those.

119
00:10:05,120 --> 00:10:11,360
The first lecture today is going to cover the foundations of neural networks themselves,

120
00:10:11,360 --> 00:10:14,800
starting with really the building blocks of every single neural network which is called

121
00:10:14,800 --> 00:10:16,160
the Perceptron.

122
00:10:16,160 --> 00:10:21,520
And finally we'll go through the week and we'll conclude with a series of exciting

123
00:10:21,520 --> 00:10:25,920
guest lectures from industry leading sponsors of the course.

124
00:10:25,920 --> 00:10:32,920
And finally on the software side, after every lecture you'll also get software experience

125
00:10:32,920 --> 00:10:37,440
and project building experience to be able to take what we teach in lectures and actually

126
00:10:37,440 --> 00:10:43,760
deploy them in real code and actually produce based on the learnings that you find in this

127
00:10:43,760 --> 00:10:47,800
lecture and at the very end of the class from the software side you'll have the ability

128
00:10:47,800 --> 00:10:53,680
to participate in a really fun day at the very end which is the project pitch competition.

129
00:10:53,680 --> 00:10:58,400
It's kind of like a shark tank style competition of all of the different projects from all

130
00:10:58,400 --> 00:11:01,720
of you and win some really awesome prizes.

131
00:11:01,720 --> 00:11:06,920
So let's step through that a little bit briefly, this is the syllabus part of the lecture.

132
00:11:06,920 --> 00:11:11,600
So each day we'll have dedicated software labs that will basically mirror all of the

133
00:11:11,600 --> 00:11:15,720
technical lectures that we go through just helping you reinforce your learnings and these

134
00:11:15,720 --> 00:11:21,840
are coupled with each day again coupled with prizes for the top performing software solutions

135
00:11:21,840 --> 00:11:23,420
that are come up in the class.

136
00:11:23,420 --> 00:11:28,400
This is going to start with today with lab one and it's going to be on music generation

137
00:11:28,400 --> 00:11:33,760
so you're going to learn how to build a neural network that can learn from a bunch of musical

138
00:11:33,760 --> 00:11:41,840
songs, listen to them and then learn to compose brand new songs in that same genre.

139
00:11:41,840 --> 00:11:47,200
Tomorrow lab two on computer vision you're going to learn about facial detection systems,

140
00:11:47,200 --> 00:11:52,720
you'll build a facial detection system from scratch using convolutional neural networks,

141
00:11:52,720 --> 00:11:57,960
you'll learn what that means tomorrow and you'll also learn how to actually de-bias,

142
00:11:57,960 --> 00:12:02,960
remove the biases that exist in some of these facial detection systems which is a huge problem

143
00:12:02,960 --> 00:12:07,000
for the state of the art solutions that exist today.

144
00:12:07,000 --> 00:12:12,440
And finally a brand new lab at the end of the course will focus on large language models

145
00:12:12,440 --> 00:12:18,000
where you're actually going to take a billion multi-billion parameter large language model

146
00:12:18,000 --> 00:12:24,440
and fine tune it to build an assistive chatbot and evaluate a set of cognitive abilities

147
00:12:24,440 --> 00:12:31,320
ranging from mathematics abilities to scientific reasoning to logical abilities and so on.

148
00:12:31,320 --> 00:12:36,640
And finally at the very very end there will be a final project pitch competition for up

149
00:12:36,640 --> 00:12:42,400
to five minutes per team and all of these are accompanied with great prizes so definitely

150
00:12:42,400 --> 00:12:45,280
there will be a lot of fun to be had throughout the week.

151
00:12:45,280 --> 00:12:49,520
There are many resources to help with this class, you'll see them posted here you don't

152
00:12:49,520 --> 00:12:53,320
need to write them down because all of the slides are already posted online.

153
00:12:53,320 --> 00:12:59,440
Please post to Piazza if you have any questions and of course we have an amazing team that

154
00:12:59,440 --> 00:13:03,640
is helping teach this course this year and you can reach out to any of us if you have

155
00:13:03,640 --> 00:13:04,640
any questions.

156
00:13:04,640 --> 00:13:06,600
The Piazza is a great place to start.

157
00:13:06,600 --> 00:13:12,760
Myself and Ava will be the two main lectures for this course Monday through Wednesday especially

158
00:13:12,760 --> 00:13:17,160
and we'll also be hearing some amazing guest lectures on the second half of the course

159
00:13:17,160 --> 00:13:21,680
which definitely you would want to attend because they really cover the really state

160
00:13:21,720 --> 00:13:28,920
of the art sides of deep learning that's going on in industry outside of academia.

161
00:13:28,920 --> 00:13:32,840
And very briefly just want to give a huge thanks to all of our sponsors who without

162
00:13:32,840 --> 00:13:36,560
their support this course like every year would not be possible.

163
00:13:36,560 --> 00:13:42,000
Okay so now let's start with the fun stuff and my favorite part of the course which is

164
00:13:42,000 --> 00:13:48,400
the technical parts and let's start by just asking ourselves a question right which is

165
00:13:48,400 --> 00:13:50,400
you know why do we care about all of this?

166
00:13:50,400 --> 00:13:51,620
Why do we care about deep learning?

167
00:13:51,620 --> 00:13:56,920
Why did you all come here today to learn and to listen to this course?

168
00:13:56,920 --> 00:14:01,800
So to understand I think we again need to go back a little bit to understand how machine

169
00:14:01,800 --> 00:14:09,800
learning used to be performed right so machine learning typically would define a set of features

170
00:14:09,800 --> 00:14:14,800
or you can think of these as kind of a set of things to look for in an image or in a

171
00:14:14,800 --> 00:14:16,360
piece of data.

172
00:14:16,400 --> 00:14:21,440
Actually these are hand engineered so humans would have to define these themselves and

173
00:14:21,440 --> 00:14:25,800
the problem with these is that they tend to be very brittle in practice just by nature

174
00:14:25,800 --> 00:14:27,480
of a human defining them.

175
00:14:27,480 --> 00:14:31,360
So the key idea of deep learning and what you're going to learn throughout this entire

176
00:14:31,360 --> 00:14:37,640
week is this paradigm shift of trying to move away from hand engineering features and rules

177
00:14:37,640 --> 00:14:43,040
that computers should look for and instead trying to learn them directly from raw pieces

178
00:14:43,040 --> 00:14:44,040
of data.

179
00:14:44,040 --> 00:14:49,760
What are the patterns that we need to look at in data sets such that if we look at those

180
00:14:49,760 --> 00:14:54,800
patterns we can make some interesting decisions and interesting actions can come out.

181
00:14:54,800 --> 00:15:00,240
So for example if we wanted to learn how to detect faces we might if you think even how

182
00:15:00,240 --> 00:15:04,040
you would detect faces right if you look at a picture what are you looking for to detect

183
00:15:04,040 --> 00:15:05,040
a face?

184
00:15:05,040 --> 00:15:09,320
You're looking for some particular patterns you're looking for eyes and noses and ears

185
00:15:09,320 --> 00:15:13,240
and when those things are all composed in a certain way you would probably deduce that

186
00:15:13,240 --> 00:15:18,440
that's a face right computers do something very similar so they have to understand what

187
00:15:18,440 --> 00:15:23,520
are the patterns that they look for what are the eyes and noses and ears of those pieces

188
00:15:23,520 --> 00:15:31,080
of data and then from there actually detect and predict from them.

189
00:15:31,080 --> 00:15:37,200
So the really interesting thing I think about deep learning is that these foundations for

190
00:15:37,200 --> 00:15:42,480
doing exactly what I just mentioned picking out the building blocks picking out the features

191
00:15:42,480 --> 00:15:47,880
from raw pieces of data and the underlying algorithms themselves have existed for many

192
00:15:47,880 --> 00:15:49,720
many decades.

193
00:15:49,720 --> 00:15:55,760
Now the question I would ask at this point is so why are we studying this now and why

194
00:15:55,760 --> 00:16:00,560
is all of this really blowing up right now and exploding with so many great advances?

195
00:16:00,560 --> 00:16:05,400
Well for one there's three things right number one is that the data that is available to

196
00:16:05,400 --> 00:16:11,160
us today is significantly more pervasive these models are hungry for data you're going to

197
00:16:11,200 --> 00:16:15,880
learn about this more in detail but these models are extremely hungry for data and we're

198
00:16:15,880 --> 00:16:21,560
living in a world right now quite frankly where data is more abundant than it has ever

199
00:16:21,560 --> 00:16:23,840
been in our history.

200
00:16:23,840 --> 00:16:30,080
Now secondly these algorithms are massively compute hungry and they're massively parallelizable

201
00:16:30,080 --> 00:16:36,040
which means that they have greatly benefited from compute hardware which is also capable

202
00:16:36,120 --> 00:16:42,280
of being parallelized the particular name of that hardware it's called a GPU right GPUs

203
00:16:42,280 --> 00:16:48,800
can run parallel processing streams of information and are particularly amenable to deep learning

204
00:16:48,800 --> 00:16:54,440
algorithms and the abundance of GPUs and that compute hardware has also pushed forward what

205
00:16:54,440 --> 00:16:59,840
we can do in deep learning and finally the last piece is the software right it's the

206
00:16:59,840 --> 00:17:06,520
open source tools that are really used as the foundational building blocks of deploying

207
00:17:06,520 --> 00:17:10,560
and building all of these underlying models that you're going to learn about in this course

208
00:17:10,560 --> 00:17:15,160
and those open source tools have just become extremely streamlined making this extremely

209
00:17:15,160 --> 00:17:22,680
easy for all of us to learn about these technologies within an amazing one week course like this.

210
00:17:22,680 --> 00:17:26,120
So let's start now with understanding now that we have some of the background let's

211
00:17:26,120 --> 00:17:31,200
start with understanding exactly what is the fundamental building block of a neural

212
00:17:31,200 --> 00:17:37,720
network now that building block is called a perceptron right every single percept every

213
00:17:37,720 --> 00:17:42,960
single neural network is built up of multiple perceptrons and you're going to learn how

214
00:17:42,960 --> 00:17:47,800
those perceptrons number one compute information themselves and how they connect to these much

215
00:17:47,800 --> 00:17:52,360
larger billion parameter neural networks.

216
00:17:52,360 --> 00:17:57,320
So the key idea of a perceptron or even simpler think of this as a single neuron right so

217
00:17:57,320 --> 00:18:03,100
a neural network is composed of many many neurons and a perceptron is just one neuron.

218
00:18:03,100 --> 00:18:08,200
So that idea of a perceptron is actually extremely simple and I hope that by the end of today

219
00:18:08,200 --> 00:18:14,440
this idea and this processing of a perceptron becomes extremely clear to you.

220
00:18:14,440 --> 00:18:19,600
So let's start by talking about just the forward propagation of information through a single

221
00:18:19,600 --> 00:18:26,360
neuron now single neurons ingest information they can actually ingest multiple pieces of

222
00:18:26,360 --> 00:18:31,160
information so here you can see this neuron taking as input three pieces of information

223
00:18:31,160 --> 00:18:39,520
x1 x2 and xm right so we define the set of inputs called x1 through m and each of these

224
00:18:39,520 --> 00:18:45,160
inputs each of these numbers is going to be element wise multiplied by a particular weight

225
00:18:45,160 --> 00:18:50,240
so this is going to be denoted here by w1 through wm so this is a corresponding weight

226
00:18:50,240 --> 00:18:54,840
for every single input and you should think of this as really you know every weight being

227
00:18:54,840 --> 00:19:01,760
assigned to that input right the weights are part of the neuron itself now you multiply

228
00:19:01,760 --> 00:19:06,360
all of these inputs with their weights together and then you add them up we take this single

229
00:19:06,360 --> 00:19:11,400
number after that addition and you pass it through what's called a nonlinear activation

230
00:19:11,400 --> 00:19:20,560
function to produce your final output which here we're calling y now what I just said

231
00:19:20,560 --> 00:19:25,320
is not entirely correct right so I missed out one critical piece of information that

232
00:19:25,320 --> 00:19:29,520
piece of information is that we also have what you can see here is called this bias

233
00:19:29,520 --> 00:19:36,720
term that bias term is actually what allows your neural neuron to shift its activation

234
00:19:36,720 --> 00:19:42,960
function horizontally on that x axis if you think of it right so on the right side you

235
00:19:42,960 --> 00:19:48,560
can now see this diagram illustrating mathematically that single equation that I talked through

236
00:19:48,560 --> 00:19:52,520
kind of conceptually right now you can see it mathematically written down as one single

237
00:19:52,520 --> 00:19:58,000
equation and we can actually rewrite this using linear algebra using vectors and dot

238
00:19:58,000 --> 00:20:03,280
products so let's do that right so now our inputs are going to be described by a capital

239
00:20:03,280 --> 00:20:09,640
x which is simply a vector of all of our inputs x1 through xm and then our weights are going

240
00:20:09,640 --> 00:20:16,960
to be described by a capital w which is going to be w1 through wm the input is obtained

241
00:20:16,960 --> 00:20:23,920
by taking the dot product of x and w right that dot product does that element wise multiplication

242
00:20:23,920 --> 00:20:29,480
and then adds sums all of the the element wise multiplications and then here's the missing

243
00:20:29,520 --> 00:20:34,120
piece is that we're now going to add that bias term here we're calling the bias term

244
00:20:34,120 --> 00:20:40,520
w zero right and then we're going to apply the nonlinearity which here denoted is z or g excuse

245
00:20:40,520 --> 00:20:46,720
me so I've mentioned this nonlinearity a few times this activation function let's dig into it a

246
00:20:46,720 --> 00:20:51,880
little bit more so we can understand what is actually this activation function doing well I

247
00:20:51,880 --> 00:20:56,480
said a couple things about it I said it's a nonlinear function right here you can see one

248
00:20:56,520 --> 00:21:04,200
example of an activation function one common one commonly used activation function is called

249
00:21:04,200 --> 00:21:08,000
the sigmoid function which you can actually see here on the bottom right hand side of the

250
00:21:08,000 --> 00:21:14,600
screen the sigmoid function is very commonly used because its outputs right so it takes as input

251
00:21:14,600 --> 00:21:21,400
any real number the x-axis is infinite plus or minus but on the y-axis it basically squashes

252
00:21:21,720 --> 00:21:28,320
every input x into a number between 0 and 1 so it's actually a very common choice for things

253
00:21:28,320 --> 00:21:32,880
like probability distributions if you want to convert your answers into probabilities or learn

254
00:21:32,880 --> 00:21:39,040
or teach a neuron to learn a probability distribution but in fact there are actually many

255
00:21:39,040 --> 00:21:43,960
different types of nonlinear activation functions that are used in neural networks and here are

256
00:21:43,960 --> 00:21:48,040
some common ones and and again throughout this presentation you'll see these little

257
00:21:48,080 --> 00:21:52,360
TensorFlow icons actually throughout the entire course you'll see these TensorFlow icons on the

258
00:21:52,360 --> 00:21:58,360
bottom which basically just allow you to relate some of the foundational knowledge that we're

259
00:21:58,360 --> 00:22:03,360
teaching in the lectures to some of the software labs and this might provide a good starting point

260
00:22:03,360 --> 00:22:08,920
for a lot of the pieces that you have to do later on in the software parts of the class so the

261
00:22:08,920 --> 00:22:12,840
sigmoid activation which we talked about in the last slide here it's shown on the left-hand side

262
00:22:12,840 --> 00:22:16,960
right this is very popular because of the probability distributions right it squashes

263
00:22:17,000 --> 00:22:23,320
everything between 0 and 1 but you see two other very common types of activation functions in the

264
00:22:23,320 --> 00:22:29,080
middle and the right-hand side as well so the other very very common one probably this is the one

265
00:22:29,080 --> 00:22:33,440
now that's the most popular activation function is now on the far right-hand side it's called the

266
00:22:33,440 --> 00:22:38,840
relu activation function or also called the rectified linear unit so basically it's linear

267
00:22:38,840 --> 00:22:45,160
everywhere except there's a nonlinearity at x equals 0 so there's a kind of a step or a great

268
00:22:45,200 --> 00:22:51,160
discontinuity right so benefit of this very easy to compute it still has the nonlinearity which

269
00:22:51,160 --> 00:22:56,400
we kind of need and we'll talk about why we need it in one second but it's very fast right just

270
00:22:56,400 --> 00:23:02,320
two linear functions piecewise combined with each other okay so now let's talk about why we need

271
00:23:02,320 --> 00:23:07,600
a nonlinearity in the first place why why not just deal with a linear function that we pass all

272
00:23:07,600 --> 00:23:13,120
of these inputs through so the point of the activation function even at all why do we have

273
00:23:13,160 --> 00:23:21,200
this is to introduce nonlinearities in of itself so what we want to do is to allow our neural

274
00:23:21,200 --> 00:23:27,600
network to deal with nonlinear data right our neural networks need the ability to deal with

275
00:23:27,600 --> 00:23:34,400
nonlinear data because the world is extremely nonlinear right this is important because you

276
00:23:34,400 --> 00:23:39,840
know if you think of the real world real datasets this is just the way they are right if you look

277
00:23:39,880 --> 00:23:44,600
at datasets like this one green and red points right and I ask you to build a neural network

278
00:23:44,600 --> 00:23:51,720
that can separate the green and the red points this means that we actually need a nonlinear

279
00:23:51,720 --> 00:23:57,920
function to do that we cannot solve this problem with a single line right in fact if we used linear

280
00:23:57,920 --> 00:24:05,160
linear functions as your activation function no matter how big your neural network is it's still

281
00:24:05,160 --> 00:24:09,800
a linear function because linear functions combined with linear functions are still linear so

282
00:24:09,840 --> 00:24:14,120
no matter how deep or how many parameters your neural network has the best they would be able

283
00:24:14,120 --> 00:24:19,160
to do to separate these green and red points would look like this but adding nonlinearities

284
00:24:19,160 --> 00:24:25,000
allows our neural networks to be smaller by allowing them to be more expressive and capture

285
00:24:25,000 --> 00:24:31,640
more complexities in the datasets and this allows them to be much more powerful in the end so let's

286
00:24:31,640 --> 00:24:36,680
understand this with a simple example imagine I give you now this trained neural network so what

287
00:24:36,680 --> 00:24:40,720
does it mean trained neural network it means now I'm giving you the weights right not only the

288
00:24:40,720 --> 00:24:45,920
inputs but I'm going to tell you what the weights of this neural network are so here let's say the

289
00:24:45,920 --> 00:24:53,800
bias term w0 is going to be 1 and our w vector is going to be 3 and negative 2 right these are

290
00:24:53,800 --> 00:24:57,160
just the weights of your trained neural network well let's worry about how we got those weights

291
00:24:57,160 --> 00:25:05,640
in a second but this network has two inputs x1 and x2 now if we want to get the output of this

292
00:25:05,680 --> 00:25:10,400
neural network all we have to do simply is to do the same story that we talked about before

293
00:25:10,400 --> 00:25:19,200
right it's dot product inputs with weights add the bias and apply the nonlinearity right and

294
00:25:19,200 --> 00:25:23,360
those are the three components that you really have to remember as part of this class right dot

295
00:25:23,360 --> 00:25:30,160
product add the bias and apply a nonlinearity that's going to be the process that keeps repeating

296
00:25:30,200 --> 00:25:36,040
over and over and over again for every single neuron after that happens that neuron was going

297
00:25:36,040 --> 00:25:42,400
to output a single number right now let's take a look at what's inside of that nonlinearity it's

298
00:25:42,400 --> 00:25:49,600
simply a weighted combination of those of those inputs with those weights right so if we look

299
00:25:49,600 --> 00:25:57,480
at what's inside of g right inside of g is a weighted combination of x and w right added

300
00:25:57,520 --> 00:26:04,720
with a bias right that's going to produce a single number right but in reality for any input that

301
00:26:04,720 --> 00:26:09,160
this model could see what this really is is a two-dimensional line because we have two

302
00:26:09,160 --> 00:26:17,400
parameters in this model so we can actually plot that line we can see exactly how this neuron

303
00:26:17,400 --> 00:26:24,480
separates points on these axes between x1 and x2 right these are the two inputs of this model

304
00:26:24,680 --> 00:26:29,920
we can see exactly and interpret exactly what this neuron is doing right we can visualize

305
00:26:29,920 --> 00:26:35,520
its entire space because we can plot the line that defines this neuron right so here we're

306
00:26:35,520 --> 00:26:42,600
plotting when that line equals zero and in fact if I give you if I give that neuron in fact a

307
00:26:42,600 --> 00:26:48,120
new data point here the new data point is x1 equals negative one and x2 equals two just an

308
00:26:48,120 --> 00:26:52,680
arbitrary point in this two-dimensional space we can plot that point in the two-dimensional space

309
00:26:52,880 --> 00:26:58,840
and depending on which side of the line it falls on it tells us you know what the what the answer

310
00:26:58,840 --> 00:27:03,440
is going to be what the sign of the answer is going to be and also what the answer itself is

311
00:27:03,440 --> 00:27:08,840
going to be right so if we follow that that equation written on the top here and plug in negative

312
00:27:08,840 --> 00:27:15,960
one and two we're going to get one minus three minus four which equals minus six right and when

313
00:27:15,960 --> 00:27:24,440
I put that into my non-linearity g I'm going to get a final output of 0.002 right so that

314
00:27:24,440 --> 00:27:28,360
that don't worry about the final output that's just going to be the output from that sigmoid

315
00:27:28,360 --> 00:27:33,880
function but the important point to remember here is that the sigmoid function actually divides

316
00:27:33,880 --> 00:27:40,240
the space into these two parts right it squashes everything between zero and one but it defines

317
00:27:40,400 --> 00:27:48,160
it implicitly by everything less than 0.5 and greater than 0.5 depending on if it's on if x is

318
00:27:48,160 --> 00:27:54,320
less than zero or greater than zero so depending on which side of the line that you fall on remember

319
00:27:54,320 --> 00:28:00,000
the line is when x equals zero the input to the sigmoid is zero if you fall on the left side of the

320
00:28:00,000 --> 00:28:07,040
line your output will be less than 0.5 because you're falling on the negative side of the line if

321
00:28:07,120 --> 00:28:12,800
your output is if your input is on the right side of the line now your output is going to be

322
00:28:12,800 --> 00:28:19,120
greater than 0.5 right so here we can actually visualize this space this is called the feature

323
00:28:19,120 --> 00:28:25,280
space of a neural network we can visualize it in its completion right we can totally visualize

324
00:28:25,280 --> 00:28:30,320
and interpret this neural network we can understand exactly what it's going to do for any input that

325
00:28:30,320 --> 00:28:35,520
it sees right but of course this is a very simple neuron right it's not a neural network it's just

326
00:28:35,520 --> 00:28:41,120
one neuron and even more than that it's even a very simple neuron it only has two inputs right

327
00:28:41,760 --> 00:28:46,960
so in reality the types of neural neurons that you're going to be dealing with in this course

328
00:28:46,960 --> 00:28:53,600
are going to be neurons and neural networks with millions or even billions of these parameters

329
00:28:53,600 --> 00:28:59,760
right of these inputs right so here we only have two weights w1 w2 but today's neural networks have

330
00:28:59,840 --> 00:29:06,640
billions of these parameters so drawing these types of plots that you see here obviously becomes a

331
00:29:06,640 --> 00:29:13,280
lot more challenging it's actually not possible but now that we have some of the intuition behind

332
00:29:13,280 --> 00:29:20,080
a perceptron let's start now by building neural networks and seeing how all of this comes together

333
00:29:20,080 --> 00:29:25,840
so let's revisit that previous diagram of a perceptron now again if there's only one thing

334
00:29:25,840 --> 00:29:32,240
to take away from this lecture right now it's to remember how a perceptron works that equation of

335
00:29:32,240 --> 00:29:36,800
a perceptron is extremely important for every single class that comes after today and there's

336
00:29:36,800 --> 00:29:43,040
only three steps it's dot product with the inputs add a bias and apply your non-linearity

337
00:29:44,080 --> 00:29:50,000
let's simplify the diagram a little bit i'll remove the weight labels from this picture and now you

338
00:29:50,000 --> 00:29:56,080
can assume that if i show a line every single line has an associated weight that comes with that

339
00:29:56,080 --> 00:30:01,840
line right i'll also remove the bias term for simplicity assume that every neuron has that

340
00:30:01,840 --> 00:30:08,880
bias term i don't need to show it and now note that the result here now calling it z which is just the

341
00:30:10,080 --> 00:30:16,880
dot product plus bias before the non-linearity is the output is going to be linear first of all

342
00:30:16,960 --> 00:30:21,680
it's just a it's just a weighted sum of all those pieces we have not applied the non-linearity yet

343
00:30:21,680 --> 00:30:27,680
but our final output is just going to be g of z it's the activation function or non-linear

344
00:30:27,680 --> 00:30:35,840
activation function applied to z now if we want to step this up a little bit more and say what if

345
00:30:35,840 --> 00:30:41,840
we had a multi output function now we don't just have one output but let's say we want to have two

346
00:30:41,840 --> 00:30:48,720
outputs well now we can just have two neurons in this network right every neuron say sees

347
00:30:48,720 --> 00:30:54,720
all of the inputs that came before it but now you see the top neuron is going to be predicting an

348
00:30:54,720 --> 00:30:58,640
answer and the bottom neuron will predict its own answer now importantly one thing you should

349
00:30:58,640 --> 00:31:05,440
really notice here is that each neuron has its own weights right each neuron has its own lines

350
00:31:05,440 --> 00:31:10,960
that are coming into just that neuron right so they're acting independently but they can later on

351
00:31:10,960 --> 00:31:22,720
communicate if you have another layer right so let's start now by initializing this this process

352
00:31:22,720 --> 00:31:27,760
a bit further and thinking about it more programmatically right what if we wanted to program

353
00:31:27,760 --> 00:31:32,880
this neural network ourselves from scratch right remember that equation I told you didn't sound

354
00:31:32,880 --> 00:31:38,720
very complex it's take a dot product add a bias which is a single number and apply a non-linearity

355
00:31:38,720 --> 00:31:44,240
let's see how we would actually implement something like that so to define the layer right we're now

356
00:31:44,240 --> 00:31:52,720
going to call this a layer which is a collection of neurons right we have to first define how that

357
00:31:52,720 --> 00:31:58,160
information propagates through the network so we can do that by creating a call function here first

358
00:31:58,160 --> 00:32:03,920
we're going to actually define the weights for that network right so remember every network every

359
00:32:03,920 --> 00:32:08,640
neuron I should say every neuron has weights and a bias right so let's define those first

360
00:32:09,760 --> 00:32:16,560
we're going to create the call function to actually see how we can pass information through that

361
00:32:16,560 --> 00:32:22,080
layer right so this is going to take as input and inputs right this is like what we previously

362
00:32:22,080 --> 00:32:28,640
called x and it's the same story that we've been seeing this whole class right we're going to matrix

363
00:32:28,640 --> 00:32:34,240
multiply or take a dot product of our inputs with our weights we're going to add a bias

364
00:32:35,120 --> 00:32:40,400
and then we're going to apply a non-linearity it's really that simple right we've now created a

365
00:32:40,400 --> 00:32:48,640
single layer neural network right so this this line in particular this is the part that allows us to

366
00:32:48,640 --> 00:32:56,800
be a powerful neural network maintaining that non-linearity and the important thing here is to

367
00:32:56,800 --> 00:33:04,000
note that modern deep learning toolboxes and libraries already implement a lot of these for

368
00:33:04,000 --> 00:33:09,680
you right so it's important for you to understand the foundations but in practice all of that layer

369
00:33:09,680 --> 00:33:15,440
or architecture and all of that layer logic is actually implemented in tools like TensorFlow

370
00:33:15,440 --> 00:33:21,920
and PyTorch through a dense layer right so here you can see an example of calling or creating

371
00:33:21,920 --> 00:33:29,840
initializing a dense layer with two neurons right allowing it to feed in an arbitrary set of

372
00:33:29,840 --> 00:33:36,240
inputs here we're seeing these two neurons in a layer being fed three inputs right and in code

373
00:33:36,240 --> 00:33:41,360
it's only reduced down to this one line of TensorFlow code making it extremely easy and

374
00:33:41,360 --> 00:33:47,760
convenient for us to use these functions and call them so now let's look at our single layered

375
00:33:47,760 --> 00:33:53,760
neural network this is where we have now one layer between our input and our outputs right so

376
00:33:53,760 --> 00:33:58,960
we're slowly and progressively increasing the complexity of our neural network so that we

377
00:33:58,960 --> 00:34:06,240
can build up all of these building blocks right this layer in the middle is called a hidden layer

378
00:34:06,240 --> 00:34:10,480
right obviously because you don't directly observe it you don't directly supervise it

379
00:34:10,480 --> 00:34:14,960
right you do observe the two input and output layers but your hidden layer is just kind of a

380
00:34:15,920 --> 00:34:22,000
a neuron layer that you don't directly observe right it just gives your network more capacity

381
00:34:22,000 --> 00:34:27,760
more learning complexity and since we now have a transformation function from inputs

382
00:34:27,760 --> 00:34:34,480
to hidden layers and hidden layers to output we now have a two layered neural network right

383
00:34:34,480 --> 00:34:41,520
which means that we also have two weight matrices right we don't have just the w1 which we previously

384
00:34:41,520 --> 00:34:46,000
had to create this hidden layer but now we also have w2 which does the transformation from hidden

385
00:34:46,000 --> 00:34:54,000
layer to output layer yes in hidden you have just linear so there's no it's not is it a perceptron

386
00:34:54,000 --> 00:35:00,240
or not yes so every hidden layer also has a nonlinearity accompanied with it right and that's

387
00:35:00,240 --> 00:35:04,880
a very important point because if you don't have that perceptron then it's just a very large linear

388
00:35:04,880 --> 00:35:10,640
function followed by a final nonlinearity at the very end right so you need that cascading

389
00:35:10,640 --> 00:35:16,640
and uh you know overlapping application of nonlinearities that occur throughout the network

390
00:35:18,480 --> 00:35:26,560
awesome okay so now let's zoom in look at a single unit in the hidden layer take this one for example

391
00:35:26,560 --> 00:35:32,080
let's call it z2 right it's the second neuron in the first layer right it's the same perception

392
00:35:32,080 --> 00:35:38,880
that we saw before we compute its answer by taking a dot product of its weights with its inputs

393
00:35:38,880 --> 00:35:45,440
adding a bias and then applying a nonlinearity if we took a different hidden node like z3 the one

394
00:35:45,440 --> 00:35:51,120
right below it we would compute its answer exactly the same way that we computed z2 except its

395
00:35:51,120 --> 00:35:55,120
weights would be different than the weights of z2 everything else stays exactly the same it sees

396
00:35:55,120 --> 00:36:00,640
the same inputs but of course you know i'm not going to actually show z3 in this picture and now

397
00:36:00,640 --> 00:36:04,240
this picture is getting a little bit messy so let's clean things up a little bit more i'm going to

398
00:36:04,240 --> 00:36:10,560
remove all the lines now and replace them just with these these boxes these symbols that will denote

399
00:36:10,560 --> 00:36:16,000
what we call a fully connected layer right so these layers now denote that everything in our input

400
00:36:16,000 --> 00:36:20,400
is connected to everything in our output and the transformation is exactly as we saw it before

401
00:36:20,400 --> 00:36:28,720
dot product bias and nonlinearity and again in code to do this is extremely straightforward

402
00:36:28,720 --> 00:36:33,440
with the foundations that we've built up from the beginning of the class we can now just define

403
00:36:33,440 --> 00:36:40,080
two of these dense layers right our hidden layer on line one with n hidden units and then our output

404
00:36:40,080 --> 00:36:49,680
layer with two hidden output units nonlinearity function does not need to be the same through

405
00:36:49,680 --> 00:36:56,320
each layer oftentimes it is because of convenience there's there are some cases where you would want

406
00:36:56,320 --> 00:37:02,000
it to be different as well especially in lecture two you're going to see nonlinearity is be different

407
00:37:02,000 --> 00:37:10,400
even within the same layer let alone different layers but unless for a particular reason generally

408
00:37:10,400 --> 00:37:17,520
convention is there's no need to keep them differently now let's keep expanding our knowledge

409
00:37:17,520 --> 00:37:22,240
a little bit more if we now want to make a deep neural network not just a neural network like we

410
00:37:22,240 --> 00:37:26,960
saw on the previous side now it's deep all that means is that we're now going to stack these layers

411
00:37:26,960 --> 00:37:32,880
on top of each other one by one more and more creating a hierarchical model right the ones

412
00:37:32,880 --> 00:37:38,560
where the final output is now going to be computed by going deeper and deeper and deeper into the

413
00:37:38,560 --> 00:37:46,080
neural network and again doing this in code again follows the exact same story as before just cascading

414
00:37:46,080 --> 00:37:51,200
these tensorflow layers on top of each other and just going deeper into the network

415
00:37:52,000 --> 00:37:59,040
okay so now this is great because now we have at least a solid foundational understanding of how to

416
00:37:59,040 --> 00:38:03,440
not only define a single neuron but how to define an entire neural network and you should be able

417
00:38:03,440 --> 00:38:10,320
to actually explain at this point or understand how information goes from input through an entire

418
00:38:10,320 --> 00:38:16,320
neural network to compute an output so now let's look at how we can apply these neural networks

419
00:38:16,320 --> 00:38:22,640
to solve a very real problem that I'm sure all of you care about so here's a problem on how we

420
00:38:22,640 --> 00:38:28,320
want to build an AI system to learn to answer the following question which is will I pass this

421
00:38:28,320 --> 00:38:35,440
class right I'm sure all of you are really worried about this question so to do this let's start with

422
00:38:35,440 --> 00:38:41,760
a simple input feature model the feature the two features that let's concern ourselves with are

423
00:38:41,760 --> 00:38:49,520
going to be number one how many lectures you attend and number two how many hours you spend on your

424
00:38:49,520 --> 00:38:56,640
final project so let's look at some of the past years of this class right we can actually observe

425
00:38:56,640 --> 00:39:03,360
how different people have lived in this space right between how many lectures and how much

426
00:39:03,360 --> 00:39:08,800
time you spent on your final project and you can actually see every point is a person the color of

427
00:39:08,880 --> 00:39:13,520
that point is going to be if they passed or failed the class and you can see and visualize

428
00:39:13,520 --> 00:39:19,280
kind of this this feature space if you will that we talked about before and then we have you you

429
00:39:19,280 --> 00:39:27,440
follow right here you're the point four five right in between the this this feature space you've

430
00:39:27,440 --> 00:39:32,080
attended four lectures and you will spend five hours on the final project and you want to build a

431
00:39:32,080 --> 00:39:38,080
neural network to determine given everyone else in the class right that I've seen from all of the

432
00:39:38,080 --> 00:39:43,840
previous years you want to help you want to have your neural network help you to understand what is

433
00:39:43,840 --> 00:39:50,320
your likelihood that you will pass or fail this class so let's do it we now have all of the building

434
00:39:50,320 --> 00:39:55,680
blocks to solve this problem using a neural network let's do it so we have two inputs those inputs

435
00:39:55,680 --> 00:40:00,800
are number of lectures you attend and number of hours you spend on your final project it's four

436
00:40:00,800 --> 00:40:09,040
and five we can pass those two inputs to our two x1 and x2 variables these are fed into this single

437
00:40:09,040 --> 00:40:15,680
layered single hidden layered neural network it has three hidden units in the middle and we can see

438
00:40:15,680 --> 00:40:22,800
that the final predicted output probability for you to pass this class is 0.1 or 10 right so very

439
00:40:22,800 --> 00:40:31,520
bleak outcome it's not a good outcome the actual probability is one right so attending four out

440
00:40:31,520 --> 00:40:35,440
of the five lectures and spending five hours on your final project you actually lived in a part of

441
00:40:35,440 --> 00:40:38,800
the feature space which was actually very positive right it looked like you were going to pass the

442
00:40:38,800 --> 00:40:44,000
class so what happened here anyone have any ideas so why did the neural network get this so terribly

443
00:40:44,000 --> 00:40:51,200
wrong right exactly so this neural network is not trained we haven't shown it any of that data the

444
00:40:51,280 --> 00:40:57,200
green and red data right so you should really think of neural networks like babies right before

445
00:40:57,200 --> 00:41:03,120
they see data they haven't learned anything there's no expectation that we should have for them to be

446
00:41:03,120 --> 00:41:06,880
able to solve any of these types of problems before we teach them something about the world

447
00:41:07,440 --> 00:41:12,880
so let's teach this neural network something about the problem first right and to train it we first

448
00:41:12,880 --> 00:41:20,000
need to tell our neural network when it's making bad decisions right so we need to teach it right

449
00:41:20,000 --> 00:41:25,600
really train it to learn exactly like how we as humans learn in some ways right so we have to inform

450
00:41:25,600 --> 00:41:31,440
the neural network when it gets the answer incorrect so that it can learn how to get the answer correct

451
00:41:32,080 --> 00:41:38,480
right so the closer the answer is to the ground truth so right so for example the actual value

452
00:41:38,480 --> 00:41:44,080
for you passing this class was probability one one hundred percent but it predicted a probability

453
00:41:44,080 --> 00:41:50,080
of zero point one we compute what's called a loss right so the closer these two things are together

454
00:41:50,080 --> 00:41:54,640
the smaller your loss should be and the and the more accurate your model should be

455
00:41:56,720 --> 00:42:02,400
so let's assume that we have data not just from one student but now we have data from many students

456
00:42:02,400 --> 00:42:05,840
we many students have taken this class before and we can plug all of them into the neural

457
00:42:05,840 --> 00:42:11,360
network and show them all to this to this system now we care not only about how the neural network

458
00:42:11,360 --> 00:42:17,200
did on just this one prediction but we care about how it predicted on all of these different people

459
00:42:17,200 --> 00:42:22,240
that the neural network has shown in the past as well during this training and learning process

460
00:42:22,880 --> 00:42:29,680
so when training your neural network we want to find a network that minimizes the empirical loss

461
00:42:29,680 --> 00:42:35,120
between our predictions and those ground truth outputs and we're going to do this on average

462
00:42:35,120 --> 00:42:42,640
across all of the different inputs that the that the model has seen if we look at this problem

463
00:42:42,640 --> 00:42:49,520
of binary classification right between yeses and nos right will I pass the class or will I not pass

464
00:42:49,520 --> 00:42:55,680
the class it's a year zero or one probability and we can use what is called the softmax function

465
00:42:55,680 --> 00:43:02,080
or the softmax cross entropy function to be able to inform if this network is getting the answer

466
00:43:02,080 --> 00:43:07,280
correct or incorrect right the softmax cross or the cross entropy function think of this as a

467
00:43:07,280 --> 00:43:13,360
as an objective function it's a loss function that tells our neural network how far away these two

468
00:43:13,360 --> 00:43:18,240
probability distributions are right so the output is a probability distribution we're trying to

469
00:43:18,240 --> 00:43:24,000
determine how bad of an answer the neural network is predicting so that we can give it feedback to

470
00:43:24,000 --> 00:43:30,880
get a better answer now let's suppose instead of training a or predicting a binary output we want

471
00:43:30,880 --> 00:43:36,320
to predict a real valued output like a like any number it can take any number plus or minus

472
00:43:36,320 --> 00:43:43,520
infinity so for example if you want to predict the grade that you get in a class right it doesn't

473
00:43:43,520 --> 00:43:48,560
necessarily need to be between zero and one or zero and a hundred even right you could now use a

474
00:43:48,560 --> 00:43:53,840
different loss in order to produce that value because our outputs are no longer a probability

475
00:43:53,840 --> 00:43:59,280
distribution right so for example what you might do here is compute a mean squared error probably

476
00:43:59,520 --> 00:44:04,800
mean squared error loss function between your true value or your true grade of the class

477
00:44:04,800 --> 00:44:09,680
and the predicted grade right these are two numbers they're not probabilities necessarily

478
00:44:09,680 --> 00:44:15,600
you compute their difference you square it to look at a distance between the two an absolute

479
00:44:15,600 --> 00:44:23,040
distance right sign doesn't matter and then you can minimize this thing right okay great so let's

480
00:44:23,120 --> 00:44:28,800
put all of this loss information with this problem of finding our network weights into a

481
00:44:28,800 --> 00:44:33,440
unified problem and a unified solution to actually train our neural network

482
00:44:34,880 --> 00:44:40,800
so we know that we want to find a neural network that will solve this problem on all this data

483
00:44:40,800 --> 00:44:46,640
on average right that's how we contextualize this problem earlier in the lectures this means

484
00:44:46,640 --> 00:44:52,640
effectively that we're trying to solve or we're trying to find what are the weights for our neural

485
00:44:52,640 --> 00:44:57,920
network what are this this big vector w that we talked about in earlier in the lecture what is

486
00:44:57,920 --> 00:45:05,280
this vector w compute this vector w for me based on all of the data that we have seen right now

487
00:45:05,280 --> 00:45:12,880
the vector w is also going to determine what is the loss right so given a single vector w

488
00:45:12,880 --> 00:45:19,440
we can compute how bad is this neural network performing on our data right so what is the loss

489
00:45:19,440 --> 00:45:24,800
what is this deviation from the ground truth of our network uh based on where it should be

490
00:45:26,160 --> 00:45:34,560
now remember that w is just a group of a bunch of numbers right it's a very big list of numbers

491
00:45:34,560 --> 00:45:41,760
a list of weights uh for every single layer and every single neuron in our neural network right

492
00:45:41,760 --> 00:45:47,360
so it's just a very big list or a vector of weights we want to find that vector what is that

493
00:45:47,360 --> 00:45:52,400
vector based on a lot of data that's the problem of training a neural network and remember our

494
00:45:52,400 --> 00:45:58,880
loss function is just a simple function of our weights if we have only two weights in our neural

495
00:45:58,880 --> 00:46:04,800
network like we saw earlier in the slide then we can plot the loss landscape over this two-dimensional

496
00:46:04,800 --> 00:46:11,760
space right so we have two weights w one and w two and for every single configuration or setting of

497
00:46:11,760 --> 00:46:17,120
those two weights our loss will have a particular value which here we're showing is the height of

498
00:46:17,120 --> 00:46:24,480
this graph right so for any w one and w two what is the loss and what we want to do is find the

499
00:46:24,480 --> 00:46:31,440
lowest point what is the best loss where what are the weights such that our loss will be as good as

500
00:46:31,440 --> 00:46:36,800
possible so the smaller the loss the better so we want to find the lowest point in this graph

501
00:46:38,480 --> 00:46:44,400
now how do we do that right so the way this works is we start somewhere in this space we

502
00:46:44,400 --> 00:46:51,280
don't know where to start so let's pick a random place to start right now from that place let's

503
00:46:51,280 --> 00:46:57,520
compute what's called the gradient of the landscape at that particular point this is a very local

504
00:46:57,520 --> 00:47:05,120
estimate of where is going up basically where where is the slope increasing at my current location

505
00:47:05,120 --> 00:47:10,240
right that informs us not only where the slope is increasing but more importantly where the slope

506
00:47:10,240 --> 00:47:15,520
is decreasing if i negate the direction if i go in the opposite direction i can actually step down

507
00:47:15,520 --> 00:47:22,960
into the landscape and change my weights such that i lower my loss so let's take a small step

508
00:47:22,960 --> 00:47:28,080
just a small step in the opposite direction of the part that's going up let's take a small step

509
00:47:28,080 --> 00:47:33,600
going down and we'll keep repeating this process we'll compute a new gradient at that new point

510
00:47:33,600 --> 00:47:37,360
and it will take another small step and we'll keep doing this over and over and over again

511
00:47:37,360 --> 00:47:43,440
until we converge at what's called a local minimum right so based on where we started it may not be

512
00:47:43,440 --> 00:47:48,720
a global minimum of everywhere in this lost landscape but let's find ourselves now in a local minimum

513
00:47:48,720 --> 00:47:53,200
and we're guaranteed to actually converge by following this very simple algorithm at a local

514
00:47:53,200 --> 00:47:59,120
minimum so let's summarize now this algorithm this algorithm is called gradient descent let's

515
00:47:59,120 --> 00:48:04,720
summarize it first in pseudocode and then we'll look at it in actual code in a second so there's

516
00:48:04,720 --> 00:48:11,440
a few steps first step is we initialize our location somewhere randomly in this weight space

517
00:48:11,440 --> 00:48:19,200
right we compute the gradient of of our loss at with respect to our weights okay

518
00:48:19,920 --> 00:48:24,400
and then we take a small step in the opposite direction and we keep repeating this in a loop

519
00:48:24,400 --> 00:48:28,800
over and over and over again and we say we keep we'll keep doing this until convergence right

520
00:48:28,800 --> 00:48:33,680
until we stop moving basically and our network basically finds where it's supposed to end up

521
00:48:34,400 --> 00:48:40,880
we'll talk about this this small step right so we're multiplying our gradient by what i keep

522
00:48:40,880 --> 00:48:46,800
calling is a small step we'll talk about that a bit more about a bit more and later part of this

523
00:48:46,800 --> 00:48:53,120
this lecture but for now let's also very quickly show the analogous part in in code as well and

524
00:48:53,120 --> 00:48:58,480
it mirrors very nicely right so we'll randomly initialize our weights this happens every time

525
00:48:58,480 --> 00:49:02,480
you train a neural network you have to randomly initialize the weights and then you have a loop

526
00:49:03,120 --> 00:49:07,600
right here showing it with without even convergence right we're just going to keep looping forever

527
00:49:08,240 --> 00:49:13,360
where we say okay we're going to compute the loss at that location compute the gradient so which

528
00:49:13,360 --> 00:49:19,360
way is up and then we just negate that gradient multiply it by some what's called learning rate

529
00:49:19,360 --> 00:49:24,320
lr denoted here it's a small step and then we take a direction in that small step

530
00:49:26,320 --> 00:49:30,560
so let's take a deeper look at this term here this is called the gradient right this tells us

531
00:49:30,560 --> 00:49:36,560
which way is up in that landscape and this again it tells us even more than that it tells us how

532
00:49:36,560 --> 00:49:43,520
is our landscape how is our loss changing as a function of all of our weights but i actually

533
00:49:43,520 --> 00:49:48,320
have not told you how to compute this so let's talk about that process that process is called

534
00:49:48,320 --> 00:49:54,240
back propagation we'll go through this very very briefly and we'll start with the simplest neural

535
00:49:54,240 --> 00:49:59,440
network uh that's possible right so we already saw the simplest building block which is a single

536
00:49:59,440 --> 00:50:04,880
neuron now let's build the simplest neural network which is just a one neuron neural network right

537
00:50:04,880 --> 00:50:10,320
so it has one hidden neuron it goes from input to hidden neuron to output and we want to compute

538
00:50:10,320 --> 00:50:16,960
the gradient of our loss with respect to this weight w2 okay so i'm highlighting it here

539
00:50:16,960 --> 00:50:23,600
so we have two weights let's compute the gradient first with respect to w2 and that tells us how

540
00:50:23,600 --> 00:50:31,200
much does a small change in w2 affect our loss does our loss go up or down if we move our w2

541
00:50:31,200 --> 00:50:36,880
a little bit in one direction or another so let's write out this derivative we can start by applying

542
00:50:36,880 --> 00:50:44,480
the chain rule backwards from the loss through the output and specifically we can actually decompose

543
00:50:44,480 --> 00:50:50,960
this law this derivative this gradient into two parts right so the first part we're decomposing

544
00:50:50,960 --> 00:51:02,160
it from dj dw2 into dj dy right which is our output multiplied by dy dw2 right this is all

545
00:51:02,160 --> 00:51:08,800
possible right it's a chain rule it's a bit i'm just reciting a chain rule here from calculus

546
00:51:08,800 --> 00:51:14,160
this is possible because y is only dependent on the previous layer and now let's suppose we don't

547
00:51:14,160 --> 00:51:18,800
want to do this for w2 but we want to do it for w1 we can use the exact same process right but now

548
00:51:18,800 --> 00:51:25,200
it's one step further right we'll now replace w2 with w1 we need to apply the chain rule yet again

549
00:51:25,200 --> 00:51:29,760
once again to decompose the problem further and now we propagate our old gradient that we computed

550
00:51:29,760 --> 00:51:35,680
for w2 all the way back one more step uh to the weight that we're interested in which in this

551
00:51:35,680 --> 00:51:41,920
case is w1 and we keep repeating this process over and over again propagating these gradients

552
00:51:41,920 --> 00:51:47,840
backwards from output to input to compute ultimately what we want in the end is this

553
00:51:47,840 --> 00:51:53,520
derivative of every weight so the lot the derivative of our loss with respect to every

554
00:51:53,520 --> 00:51:57,680
weight in our neural network this tells us how much does a small change in every single weight

555
00:51:57,680 --> 00:52:02,400
in our network affect the loss does our loss go up or down if we change this weight a little bit

556
00:52:02,400 --> 00:52:11,840
in this direction or a little bit in that direction yes neuron and perceptron are the same so

557
00:52:11,840 --> 00:52:17,040
typically people say neural network which is why like a single neuron it's also gotten popularity

558
00:52:17,040 --> 00:52:22,720
but originally a perceptron is the the formal term the two terms are identical

559
00:52:25,200 --> 00:52:30,640
okay so now we've covered a lot so we've covered the forward propagation of information through a

560
00:52:30,640 --> 00:52:36,560
neuron and through a neural network all the way through and we've covered now the back propagation

561
00:52:36,560 --> 00:52:41,680
of information to understand how we should change every single one of those weights in our neural

562
00:52:41,680 --> 00:52:49,280
network to improve our loss so that was the back prop algorithm in theory it's actually

563
00:52:49,280 --> 00:52:53,760
pretty simple it's just a chain rule right there's nothing there's actually nothing more than than

564
00:52:53,760 --> 00:52:58,800
just the chain rule and the nice part is that deep learning libraries actually do this for you so

565
00:52:58,800 --> 00:53:02,400
they compute back prop for you you don't actually have to implement it yourself which is very

566
00:53:02,400 --> 00:53:08,080
convenient but now it's important to touch on even though the theory is actually not that complicated

567
00:53:08,080 --> 00:53:13,200
for back propagation let's touch on it now from practice now thinking a little bit towards your

568
00:53:13,200 --> 00:53:17,680
own implementations when you want to implement these neural networks what are some insights

569
00:53:18,400 --> 00:53:23,520
so optimization of neural networks in practice is a completely different story it's not

570
00:53:23,520 --> 00:53:28,720
straightforward at all and in practice it's very difficult and usually very computationally

571
00:53:28,720 --> 00:53:33,840
intensive to do this back prop algorithm so here's an illustration from a paper that came

572
00:53:33,840 --> 00:53:39,280
out a few years ago that actually attempted to visualize a very deep neural networks loss

573
00:53:39,280 --> 00:53:45,040
landscape so previously we had that other depiction visualization of how a neural network would

574
00:53:45,040 --> 00:53:50,960
look in a two-dimensional landscape real neural networks are not two-dimensional they're hundreds

575
00:53:50,960 --> 00:53:57,440
or millions or billions of dimensions and now what would those lost landscapes look like you can

576
00:53:57,440 --> 00:54:02,080
actually try some clever techniques to actually visualize them this is one paper that attempted

577
00:54:02,160 --> 00:54:09,600
to do that and it turns out that they look extremely messy right the important thing is that

578
00:54:09,600 --> 00:54:14,240
if you do this algorithm and you start in a bad place depending on your neural network you may

579
00:54:14,240 --> 00:54:19,760
not actually end up in the global solution right so your initialization matters a lot

580
00:54:19,760 --> 00:54:24,320
and you need to kind of traverse these local minima and try to try and help you find the

581
00:54:24,320 --> 00:54:31,280
global minima or even more than that you need to construct neural networks that have lost landscapes

582
00:54:31,280 --> 00:54:35,600
that are much more amenable to optimization than this one right so this is a very bad loss

583
00:54:35,600 --> 00:54:40,400
landscape there are some techniques that we can apply to our neural networks that smooth out

584
00:54:40,400 --> 00:54:46,160
their lost landscape and make them easier to optimize so recall that update equation that

585
00:54:46,160 --> 00:54:50,800
we talked about earlier with gradient descent right so there is this parameter here that we

586
00:54:50,800 --> 00:54:55,280
didn't talk about we we described this as the little step that you could take right so it's a

587
00:54:55,280 --> 00:55:00,400
small number that you multiply with the direction which is your gradient it just tells you okay i'm

588
00:55:00,400 --> 00:55:04,400
not going to just go all the way in this direction i'll just take a small step in this direction

589
00:55:05,040 --> 00:55:10,800
so in practice even setting this value right it's just one number setting this one number can be

590
00:55:10,800 --> 00:55:18,560
rather difficult right if we set the learning rate too small then the model can get stuck

591
00:55:18,560 --> 00:55:23,520
in these local minima right so here it starts and it kind of gets stuck in this local minima

592
00:55:23,520 --> 00:55:28,400
it converges very slowly even if it doesn't get stuck if the learning rate is too large it can

593
00:55:28,400 --> 00:55:34,560
kind of overshoot and in practice it even diverges and explodes and you don't actually ever find

594
00:55:34,560 --> 00:55:41,520
any minima now ideally what we want is to use learning rates that are not too small and not too

595
00:55:41,520 --> 00:55:47,680
large to so they're large enough to basically avoid those local minima but small enough such

596
00:55:47,680 --> 00:55:53,280
that they won't diverge and they will actually still find their way into the global minima so

597
00:55:53,280 --> 00:55:57,360
something like this is what you should intuitively have in mind right so something i can overshoot

598
00:55:57,360 --> 00:56:03,200
the local minimas but find itself into a better minima and then finally stabilize itself there

599
00:56:03,760 --> 00:56:08,080
so how do we actually set these learning rates right in practice what does that process look

600
00:56:08,080 --> 00:56:13,840
like now idea number one is is very basic right try a bunch of different learning rates and see

601
00:56:13,840 --> 00:56:19,840
what works and that's actually a not a bad process in practice it's one of the processes that people

602
00:56:19,840 --> 00:56:25,600
use so that that's uh that's interesting but let's see if we can do something smarter than this

603
00:56:25,600 --> 00:56:32,320
and let's see how we can design algorithms that can adapt to the landscapes right so in practice

604
00:56:32,320 --> 00:56:38,080
there's no reason why there should be a single number right can we have learning rates that adapt

605
00:56:38,080 --> 00:56:43,840
to the model to the data to the landscapes to the gradients that it's seeing around so this means

606
00:56:43,840 --> 00:56:50,160
that the learning rate may actually increase or decrease as a function of the gradients in the

607
00:56:50,160 --> 00:56:55,680
loss function right how fast we're learning or many other options right there are many different

608
00:56:55,680 --> 00:57:02,080
ideas that could be done here and in fact there are many widely used different procedures or

609
00:57:02,080 --> 00:57:08,000
methodologies for setting the learning rate and during your labs we actually encourage you to try

610
00:57:08,000 --> 00:57:12,480
out some of these different ideas for different types of learning rates and and even play around

611
00:57:12,480 --> 00:57:16,720
with you know what what's the effect of increasing or decreasing your learning rate you'll see very

612
00:57:16,720 --> 00:57:21,280
striking differences

613
00:57:28,160 --> 00:57:33,680
so so a few things what number one is that it's not a closed space right so there's an infinite

614
00:57:33,680 --> 00:57:40,080
every every weight can be plus or minus up to infinity right so even if it was a one-dimensional

615
00:57:40,080 --> 00:57:46,240
neural network with just one weight it's not a closed space in practice it's even worse than that

616
00:57:46,240 --> 00:57:53,920
because you have billions of dimensions right so not only is your space your support system in

617
00:57:53,920 --> 00:57:59,520
one dimension is it infinite but you now have billions of infinite dimensions right or billions

618
00:57:59,520 --> 00:58:04,800
of infinite support spaces so it's not something that you can just like search every weight every

619
00:58:04,800 --> 00:58:10,000
possible weight in your neural in your configuration or what is every possible weight that this neural

620
00:58:10,000 --> 00:58:15,760
network could take and let me test them out because it's not practical to do even for a very small

621
00:58:15,760 --> 00:58:23,120
neural network in practice so in your labs you can really try to put all of this information

622
00:58:23,920 --> 00:58:31,280
in this picture into practice which defines your model number one right here defines your

623
00:58:31,280 --> 00:58:36,480
optimizer which previously we denoted as this gradient descent optimizer here we're calling it

624
00:58:37,120 --> 00:58:43,040
stochastic gradient center SGD we'll talk about that more in a second and then also note that

625
00:58:43,040 --> 00:58:49,200
your optimizer which here we're calling SGD could be any of these adaptive optimizers you can swap

626
00:58:49,200 --> 00:58:53,360
them out and you should swap them out you should test different things here to see the impact of

627
00:58:53,360 --> 00:59:00,080
these different methods on your training procedure and you'll gain very valuable intuition for the

628
00:59:00,080 --> 00:59:04,480
different insights that will come with that as well so I want to continue very briefly just for

629
00:59:04,480 --> 00:59:09,840
the end of this lecture to talk about tips for training neural networks in practice and how we

630
00:59:09,920 --> 00:59:16,800
can focus on this powerful idea of really what's called batching data right not seeing all of your

631
00:59:16,800 --> 00:59:23,520
data but now talking about a topic called batching so to do this let's very briefly revisit this

632
00:59:23,520 --> 00:59:28,960
gradient descent algorithm the gradient is actually compute this gradient computation the back prop

633
00:59:28,960 --> 00:59:34,800
algorithm I mentioned this earlier it's a very computationally expensive operation and it's

634
00:59:34,800 --> 00:59:39,760
even worse because we now are we previously described in a way where we would have to compute

635
00:59:39,760 --> 00:59:45,520
it over a summation over every single data point in our entire data set right that's how we defined

636
00:59:45,520 --> 00:59:49,760
it with the loss functions an average over all of our data points which means that we're summing

637
00:59:49,760 --> 00:59:55,280
over all of our data points the gradients so in most real-life problems this would be completely

638
00:59:55,280 --> 01:00:00,480
infeasible to do because our data sets are simply too big and the models are too big to compute those

639
01:00:00,480 --> 01:00:04,880
gradients on every single iteration remember this isn't just a one-time thing right it's every

640
01:00:04,880 --> 01:00:09,840
single step that you do you keep taking small steps so you keep me you keep needing to repeat this

641
01:00:09,840 --> 01:00:15,760
process so instead let's define a new gradient descent algorithm called SGD stochastic gradient

642
01:00:15,760 --> 01:00:22,240
descent instead of computing the gradient over the entire data set now let's just pick a single

643
01:00:22,240 --> 01:00:28,720
training point and compute that one training points gradient right the nice thing about that is that

644
01:00:28,720 --> 01:00:35,040
it's much easier to compute that gradient right it only needs one point and the downside is that

645
01:00:35,040 --> 01:00:40,480
it's very noisy it's very stochastic since it was computed using just that one example right so you

646
01:00:40,480 --> 01:00:46,560
have that that trade-off that exists so what's the middle ground right the middle ground is to take

647
01:00:46,560 --> 01:00:52,320
not one data point and not the full data set but a batch of data right so take a what's called a

648
01:00:52,320 --> 01:00:58,400
mini batch right this could be something in practice like 32 pieces of data is a common batch size

649
01:00:58,480 --> 01:01:03,120
and this gives us an estimate of the true gradient right so you approximate the gradient

650
01:01:03,120 --> 01:01:09,600
by averaging the gradient of these 32 samples it's still fast because 32 is much smaller than

651
01:01:09,600 --> 01:01:15,040
the size of your entire data set but it's pretty quick now right it's still noisy but it's okay

652
01:01:15,040 --> 01:01:21,200
usually in practice because you can still iterate much faster and since b is normally not that large

653
01:01:21,200 --> 01:01:27,120
again think of something like in the tens or the hundreds of samples it's very fast to compute this

654
01:01:27,200 --> 01:01:32,400
in practice compared to regular gradient descent and it's also much more accurate compared to

655
01:01:32,400 --> 01:01:38,240
stochastic gradient descent and the increase in accuracy of this gradient estimation allows

656
01:01:38,240 --> 01:01:43,840
us to converge to our solution significantly faster as well right it's not only about the

657
01:01:43,840 --> 01:01:48,880
speed it's just about the increase in accuracy of those gradients allows us to get to our solution

658
01:01:48,880 --> 01:01:54,720
much faster which ultimately means that we can train much faster as well and we can save compute

659
01:01:55,280 --> 01:02:01,840
and the other really nice thing about mini batches is that they allow for parallelizing our

660
01:02:01,840 --> 01:02:06,080
computation right and that was a concept that we had talked about earlier in the class as well

661
01:02:06,080 --> 01:02:11,520
and here's where it's coming in we can split up those batches right so those 32 pieces of data

662
01:02:11,520 --> 01:02:17,040
let's say if our batch size is 32 we can split them up onto different workers right different

663
01:02:17,040 --> 01:02:23,440
parts of the GPU can tackle those different parts of our data points this can allow us to

664
01:02:23,440 --> 01:02:30,080
basically achieve even more significant speed-ups using GPU architectures and GPU hardware okay

665
01:02:30,080 --> 01:02:34,800
finally the last topic I want to talk about before we end this lecture and move on to lecture number

666
01:02:34,800 --> 01:02:40,960
two is overfitting right so overfitting is this idea that is actually not a deep learning

667
01:02:40,960 --> 01:02:45,840
centric problem at all it's it's a problem that exists in all of machine learning right the key

668
01:02:45,840 --> 01:02:54,800
problem is that and the key problem is actually one that addresses how you can accurately define

669
01:02:55,600 --> 01:03:02,800
if your model is is actually capturing your true data set right or if it's just learning kind of

670
01:03:02,800 --> 01:03:09,600
the subtle details that are kind of spuriously correlating to your data set so set differently

671
01:03:09,600 --> 01:03:14,960
let me say it a bit differently now so let's say we want to build models that can learn

672
01:03:15,040 --> 01:03:21,920
representations okay from our training data that still generalize to brand new unseen

673
01:03:22,640 --> 01:03:26,800
test points right that's the real goal here is we want to teach our model something based on a

674
01:03:26,800 --> 01:03:31,200
lot of training data but then we don't want it to do well in the training day we want it to do well

675
01:03:31,200 --> 01:03:35,440
when we deploy it into the real world and it's seeing things that it has never seen during training

676
01:03:36,000 --> 01:03:42,560
so the concept of overfitting is exactly addressing that problem overfitting means if

677
01:03:43,520 --> 01:03:49,760
if your model is doing very well on your training data but very badly in testing it

678
01:03:49,760 --> 01:03:54,800
that means it's overfitting it's overfitting to the training data that it saw on the other hand

679
01:03:54,800 --> 01:04:00,080
there's also underfitting right on the left hand side you can see basically not fitting the data

680
01:04:00,080 --> 01:04:04,960
enough which means that you know you're going to achieve very similar performance on your testing

681
01:04:04,960 --> 01:04:11,200
distribution but both are underperforming the actual capabilities of your system now ideally

682
01:04:11,280 --> 01:04:16,320
you want to end up somewhere in the middle which is not too complex where you're memorizing all of

683
01:04:16,320 --> 01:04:21,520
the nuances in your training data like on the right but you still want to continue to perform

684
01:04:22,080 --> 01:04:28,000
well even based on the brand new data so you're not underfitting as well so to a to actually

685
01:04:28,000 --> 01:04:31,520
address this problem in neural networks and in machine learning in general there's a few

686
01:04:31,520 --> 01:04:36,240
different ways that you should be aware of and how to do it because you'll need to apply them

687
01:04:36,240 --> 01:04:41,360
as part of your solutions in your software labs as well so the key concept here is called

688
01:04:41,360 --> 01:04:47,840
regularization right regularization is a technique that you can introduce and said very simply all

689
01:04:47,840 --> 01:04:56,880
regularization is is a way to discourage your model from from these nuances in your training

690
01:04:56,880 --> 01:05:01,840
data from being learned that's all it is and as we've seen before it's actually critical for our

691
01:05:01,840 --> 01:05:06,960
models to be able to generalize you know not just on training data but really what we care about

692
01:05:06,960 --> 01:05:13,120
is the testing data so the most popular regularization technique that's important for you to understand

693
01:05:13,120 --> 01:05:18,800
is this very simple idea called dropout let's revisit this picture of a deep neural network

694
01:05:18,800 --> 01:05:23,840
that we've been seeing all lecture right in dropout our training during training what we're

695
01:05:23,840 --> 01:05:29,760
going to do is randomly set some of the activations right these outputs of every single neuron

696
01:05:30,720 --> 01:05:36,880
to zero which is randomly going to set them to zero with some probability right so let's say

697
01:05:36,880 --> 01:05:42,800
50 percent is our probability that means that we're going to take all of the activation insert

698
01:05:42,800 --> 01:05:48,400
in our neural network and with a probability of 50 before we pass that activation on to the next

699
01:05:48,400 --> 01:05:54,800
neuron we're just going to set it to zero and not pass on anything so effectively 50 percent of the

700
01:05:54,880 --> 01:06:00,720
neurons are going to be kind of shut down or killed in a forward pass and you're only going

701
01:06:00,720 --> 01:06:06,560
to forward pass information with the other 50 percent of your neurons so this idea is extremely

702
01:06:06,560 --> 01:06:11,360
powerful actually because it lowers the capacity of our neural network it not only lowers the

703
01:06:11,360 --> 01:06:16,480
capacity of our neural network but it's dynamically lowering it because on the next iteration we're

704
01:06:16,480 --> 01:06:21,280
going to pick a different 50 percent of neurons that we drop out so constantly the network is going

705
01:06:21,280 --> 01:06:27,280
to have to learn to build pathways different pathways from input to output and that it can't

706
01:06:27,280 --> 01:06:32,880
rely on any small any small part of the features that are present in any part of the training

707
01:06:32,880 --> 01:06:37,840
dataset too extensively right because it's constantly being forced to find these different

708
01:06:37,840 --> 01:06:44,400
pathways with random probabilities so let's drop out the second regularization technique

709
01:06:44,400 --> 01:06:48,720
is going to be this notion called early stopping which is actually something that

710
01:06:48,720 --> 01:06:53,040
is model agnostic you can apply this to any type of model as long as you have a testing set that

711
01:06:53,040 --> 01:06:59,600
you can play around with so the idea here is that we have already a pretty formal mathematical

712
01:06:59,600 --> 01:07:04,880
definition of what it means to overfit right overfitting is just when our model starts to

713
01:07:04,880 --> 01:07:11,440
perform worse on our test set that's really all it is right so what if we plot over the course

714
01:07:11,440 --> 01:07:16,160
of training so x-axis is as we're training the model let's look at the performance on both the

715
01:07:16,160 --> 01:07:21,840
training set and the test set so in the beginning you can see that the training set and the test

716
01:07:21,840 --> 01:07:26,960
set are both going down and they continue to go down which is excellent because it means that

717
01:07:26,960 --> 01:07:32,720
our model is getting stronger eventually though what you'll notice is that the test loss plateaus

718
01:07:33,360 --> 01:07:38,800
and starts to increase on the other hand the training loss there's no reason why the training

719
01:07:38,800 --> 01:07:44,320
loss should ever need to stop going down right training losses generally always continue to

720
01:07:44,400 --> 01:07:50,640
decay as long as there is capacity in the neural network to learn those differences right but the

721
01:07:50,640 --> 01:07:55,680
important point is that this continues for the rest of training and we want to basically

722
01:07:56,400 --> 01:08:00,080
we care about this point right here right this is the really important point because

723
01:08:01,040 --> 01:08:05,600
this is where we need to stop training right after this point this is the happy medium because

724
01:08:05,600 --> 01:08:11,920
after this point we start to overfit on parts of the data where our training accuracy becomes

725
01:08:11,920 --> 01:08:17,280
actually better than our testing accuracy so our testing accuracy is going bad it's getting worse

726
01:08:17,280 --> 01:08:21,840
but our training accuracy is still improving so it means overfitting on the other hand on the left

727
01:08:21,840 --> 01:08:28,560
hand side this is the opposite problem right we have not fully utilized the capacity of our model

728
01:08:28,560 --> 01:08:34,480
and the testing accuracy can still improve further right this is a very powerful idea but it's actually

729
01:08:34,480 --> 01:08:39,280
extremely easy to implement in practice because all you really have to do is just monitor the loss

730
01:08:39,920 --> 01:08:43,040
of over the course of training right and you just have to pick the model

731
01:08:43,040 --> 01:08:45,440
where the testing accuracy starts to get worse

732
01:08:47,520 --> 01:08:52,560
so i'll conclude this lecture by just summarizing three key points that we've covered in the class

733
01:08:52,560 --> 01:08:57,840
so far and this is a very gem packed class so the entire week is going to be like this

734
01:08:57,840 --> 01:09:02,960
and today is just the start so so far we've learned the fundamental building blocks of neural

735
01:09:02,960 --> 01:09:07,760
networks starting all the way from just one neuron also called a perceptron we learned that we can

736
01:09:07,760 --> 01:09:14,160
stack these systems on top of each other to create a hierarchical network and how we can

737
01:09:14,160 --> 01:09:18,720
mathematically optimize those types of systems and then finally in the very last part of the

738
01:09:18,720 --> 01:09:23,840
class we talked about just techniques tips and techniques for actually training and applying

739
01:09:23,840 --> 01:09:29,680
these systems into practice now in the next lecture we're going to hear from Ava on deep

740
01:09:29,680 --> 01:09:37,040
sequence modeling using RNNs and also a really new and exciting algorithm and type of model called

741
01:09:37,040 --> 01:09:42,880
the transformer which is built off of this principle of attention you're going to learn

742
01:09:42,880 --> 01:09:47,600
about in the next class but let's for now just take a brief pause and let's resume in about

743
01:09:47,600 --> 01:09:56,400
five minutes just so we can switch speakers and Ava can start her presentation okay thank you

