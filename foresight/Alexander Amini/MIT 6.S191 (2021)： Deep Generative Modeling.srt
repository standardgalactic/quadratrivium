1
00:00:00,000 --> 00:00:15,440
Hi everyone, and welcome to lecture 4 of MIT 6S191.

2
00:00:15,440 --> 00:00:19,640
In today's lecture, we're going to be talking about how we can use deep learning and neural

3
00:00:19,640 --> 00:00:24,800
networks to build systems that not only look for patterns in data, but actually can go

4
00:00:24,800 --> 00:00:31,760
a step beyond this to generate brand new synthetic examples based on those learned patterns.

5
00:00:31,760 --> 00:00:36,840
And this, I think, is an incredibly powerful idea, and it's a particular subfield of deep

6
00:00:36,840 --> 00:00:42,040
learning that has enjoyed a lot of success and gotten a lot of interest in the past couple

7
00:00:42,040 --> 00:00:47,520
of years, but I think there's still tremendous, tremendous potential of this field of deep

8
00:00:47,520 --> 00:00:52,180
generative modeling in the future and in the years to come, particularly as we see these

9
00:00:52,180 --> 00:00:57,540
types of models and the types of problems that they tackle becoming more and more relevant

10
00:00:57,540 --> 00:01:00,540
in a variety of application areas.

11
00:01:00,540 --> 00:01:08,000
All right, so to get started, I'd like to consider a quick question for each of you.

12
00:01:08,000 --> 00:01:13,500
Here we have three photos of faces, and I want you all to take a moment, look at these

13
00:01:13,500 --> 00:01:18,900
faces, study them, and think about which of these faces you think is real.

14
00:01:18,900 --> 00:01:20,740
Is it the face on the left?

15
00:01:20,740 --> 00:01:23,140
Is it the face in the center?

16
00:01:23,140 --> 00:01:25,300
Is it the face on the right?

17
00:01:25,300 --> 00:01:27,500
Which of these is real?

18
00:01:27,500 --> 00:01:33,340
Well, in truth, each of these faces are not real.

19
00:01:33,340 --> 00:01:34,460
They are all fake.

20
00:01:34,460 --> 00:01:40,040
These are all images that were synthetically generated by a deep neural network.

21
00:01:40,040 --> 00:01:43,660
None of these people actually exist in the real world.

22
00:01:43,660 --> 00:01:50,160
And hopefully, I think you all have appreciated the realism of each of these synthetic images,

23
00:01:50,160 --> 00:01:54,800
and this to me highlights the incredible power of deep generative modeling.

24
00:01:54,800 --> 00:01:57,960
And not only does it highlight the power of these types of algorithms and these types

25
00:01:57,960 --> 00:02:04,560
of models, but it raises a lot of questions about how we can consider the fair use and

26
00:02:04,560 --> 00:02:10,600
the ethical use of such algorithms as they are being deployed in the real world.

27
00:02:10,600 --> 00:02:17,480
So by setting this up and motivating in this way, I'd first, I now like to take a step

28
00:02:17,480 --> 00:02:23,080
back and consider fundamentally what is the type of learning that can occur when we are

29
00:02:23,080 --> 00:02:27,340
training neural networks to perform tasks such as these.

30
00:02:27,340 --> 00:02:32,520
So so far in this course, we've been considering what we call supervised learning problems,

31
00:02:32,520 --> 00:02:38,760
instances in which we are given a set of data and a set of labels associated with that data.

32
00:02:38,760 --> 00:02:44,240
And our goal is to learn a functional mapping that moves from data to labels.

33
00:02:44,240 --> 00:02:47,720
And those labels can be class labels or continuous values.

34
00:02:47,720 --> 00:02:53,120
And in this course, we've been concerned primarily with developing these functional

35
00:02:53,120 --> 00:02:57,200
mappings that can be described by deep neural networks.

36
00:02:57,200 --> 00:03:03,880
But at their core, these mappings could be anything, you know, any sort of statistical function.

37
00:03:03,880 --> 00:03:09,120
The topic of today's lecture is going to focus on what we call unsupervised learning, which

38
00:03:09,120 --> 00:03:12,720
is a new class of learning problems.

39
00:03:12,720 --> 00:03:17,600
And in contrast to supervised settings where we're given data and labels in unsupervised

40
00:03:17,600 --> 00:03:20,800
learning, we're given only data, no labels.

41
00:03:20,800 --> 00:03:25,400
And our goal is to train a machine learning or deep learning model to understand or build

42
00:03:25,400 --> 00:03:30,640
up a representation of the hidden and underlying structure in that data.

43
00:03:30,640 --> 00:03:36,760
And what this can do is it can allow sort of an insight into the foundational structure

44
00:03:36,760 --> 00:03:38,080
of the data.

45
00:03:38,080 --> 00:03:44,960
And then in turn, we can use this understanding to actually generate synthetic examples.

46
00:03:44,960 --> 00:03:50,720
And unsupervised learning beyond this domain of deep generative modeling also extends

47
00:03:50,720 --> 00:03:56,480
to other types of problems and example applications, which you may be familiar with, such as clustering

48
00:03:56,480 --> 00:04:00,880
algorithms or dimensionality reduction algorithms.

49
00:04:00,880 --> 00:04:04,640
Generative modeling is one example of unsupervised learning.

50
00:04:04,640 --> 00:04:12,040
And our goal in this case is to take as input examples from a training set and learn a model

51
00:04:12,040 --> 00:04:18,440
that represents the distribution of the data that is input to that model.

52
00:04:18,440 --> 00:04:21,280
And this can be achieved in two principal ways.

53
00:04:21,280 --> 00:04:25,600
The first is through what is called density estimation, where let's say we are given a

54
00:04:25,600 --> 00:04:31,080
set of data samples and they fall according to some density.

55
00:04:31,120 --> 00:04:37,840
The task for building a deep generative model applied to these samples is to learn the underlying

56
00:04:37,840 --> 00:04:45,240
probability density function that describes how and where these data fall along this distribution.

57
00:04:45,240 --> 00:04:51,480
And we can not only just estimate the density of such a probability density function, but

58
00:04:51,480 --> 00:04:58,320
actually use this information to generate new synthetic samples, where again we are considering

59
00:04:58,320 --> 00:05:04,480
some input examples that fall and are drawn from some training data distribution.

60
00:05:04,480 --> 00:05:11,480
And after building up a model using that data, our goal is now to generate synthetic examples

61
00:05:11,480 --> 00:05:18,920
that can be described as falling within the data distribution modeled by our model.

62
00:05:18,920 --> 00:05:27,840
So the key idea in both these instances is this question of how can we learn a probability

63
00:05:27,880 --> 00:05:36,160
distribution using our model, which we call P model of X, that is so similar to the true

64
00:05:36,160 --> 00:05:43,760
data distribution, which we call P data of X. This will not only enable us to effectively

65
00:05:43,760 --> 00:05:49,440
estimate these probability density functions, but also generate new synthetic samples that

66
00:05:49,440 --> 00:05:56,000
are realistic and match the distribution of the data we're considering.

67
00:05:56,000 --> 00:06:04,080
So this I think summarizes concretely what are the key principles behind generative modeling.

68
00:06:04,080 --> 00:06:10,840
But to understand that how generative modeling may be informative and also impactful, let's

69
00:06:10,840 --> 00:06:17,200
take this idea step further and consider what could be potential impactful applications

70
00:06:17,200 --> 00:06:21,040
and real world use cases of generative modeling.

71
00:06:21,040 --> 00:06:27,640
What generative models enable us as the users to do is to automatically uncover the underlying

72
00:06:27,640 --> 00:06:30,480
structure and features in a data set.

73
00:06:30,480 --> 00:06:35,680
The reason this can be really important and really powerful is often we do not know how

74
00:06:35,680 --> 00:06:40,480
those features are distributed within a particular data set of interest.

75
00:06:40,480 --> 00:06:45,840
So let's say we're trying to build up a facial detection classifier and we're given a data

76
00:06:45,840 --> 00:06:52,080
set of faces, for which we may not know the exact distribution of these faces with respect

77
00:06:52,080 --> 00:06:58,080
to key features like skin tone or pose or clothing items.

78
00:06:58,080 --> 00:07:05,680
And without going through our data set and manually inspecting each of these instances,

79
00:07:05,680 --> 00:07:10,640
our training data may actually be very biased with respect to some of these features without

80
00:07:10,640 --> 00:07:12,960
us even knowing it.

81
00:07:12,960 --> 00:07:19,160
And as you'll see in this lecture and in today's lab, what we can actually do is train generative

82
00:07:19,160 --> 00:07:25,880
models that can automatically learn the landscape of the features in a data set like these,

83
00:07:25,880 --> 00:07:27,600
like that of faces.

84
00:07:27,600 --> 00:07:33,680
And by doing so, actually uncover the regions of the training distribution that are underrepresented

85
00:07:33,680 --> 00:07:39,400
and overrepresented with respect to particular features such as skin tone.

86
00:07:39,400 --> 00:07:45,640
And the reason why this is so powerful is we can actually now use this information to

87
00:07:45,640 --> 00:07:52,280
actually adjust how the data is sampled during training to ultimately build up a more fair

88
00:07:52,280 --> 00:07:59,000
and more representative data set that then will lead to a more fair and unbiased model.

89
00:07:59,000 --> 00:08:03,600
And you'll get practice doing exactly this and implementing this idea in today's lab

90
00:08:03,600 --> 00:08:06,120
exercise.

91
00:08:06,120 --> 00:08:11,560
Another great example in use case where generative models are exceptionally powerful is this

92
00:08:11,560 --> 00:08:16,480
broad class of problems that can be considered outlier or anomaly detection.

93
00:08:16,480 --> 00:08:21,280
One example is in the case of self-driving cars where it's going to be really critical

94
00:08:21,280 --> 00:08:27,400
to ensure that an autonomous vehicle governed and operated by a deep neural network is able

95
00:08:27,400 --> 00:08:33,240
to handle all of the cases that it may encounter on the road, not just, you know, the straight

96
00:08:33,240 --> 00:08:37,800
freeway driving that is going to be the majority of the training data and the majority of the

97
00:08:37,800 --> 00:08:40,880
time the car experiences on the road.

98
00:08:40,880 --> 00:08:46,240
So generative models can actually be used to detect outliers within training distributions

99
00:08:46,240 --> 00:08:51,880
and use this to, again, improve the training process so that the resulting model can be

100
00:08:51,880 --> 00:08:56,480
better equipped to handle these edge cases and rare events.

101
00:08:56,480 --> 00:09:02,280
All right, so hopefully that motivates why and how generative models can be exceptionally

102
00:09:02,280 --> 00:09:06,680
powerful and useful for a variety of real world applications.

103
00:09:06,680 --> 00:09:11,520
To dive into the bulk of the technical content for today's lecture, we're going to discuss

104
00:09:11,520 --> 00:09:14,960
two classes of what we call latent variable models.

105
00:09:14,960 --> 00:09:20,120
Specifically we'll look at autoencoders and generative adversarial networks or GANs.

106
00:09:20,120 --> 00:09:25,560
But before we get into that, I'd like to first begin by discussing why these are called latent

107
00:09:25,560 --> 00:09:32,440
variable models and what we actually mean when we use this word latent.

108
00:09:32,440 --> 00:09:37,080
And to do so, I think really the best example that I've personally come across for understanding

109
00:09:37,080 --> 00:09:43,000
what a latent variable is, is this story that is from Plato's work, the Republic.

110
00:09:43,000 --> 00:09:46,280
And this story is called the myth of the cave or the parable of the cave.

111
00:09:46,280 --> 00:09:48,540
And the story is as follows.

112
00:09:48,540 --> 00:09:54,080
In this myth, there are a group of prisoners and these prisoners are constrained as part

113
00:09:54,080 --> 00:09:57,200
of their prison punishment to face a wall.

114
00:09:57,200 --> 00:10:02,720
And the only things that they can see on this wall are the shadows of particular objects

115
00:10:02,720 --> 00:10:06,000
that are being passed in front of a fire that's behind them.

116
00:10:06,000 --> 00:10:10,200
So behind their heads and out of their line of sight.

117
00:10:10,200 --> 00:10:13,520
And the prisoners, the only thing they're really observing are these shadows on the

118
00:10:13,520 --> 00:10:14,520
wall.

119
00:10:14,520 --> 00:10:16,800
And so to them, that's what they can see.

120
00:10:16,800 --> 00:10:19,680
That's what they can measure and that's what they can give names to.

121
00:10:19,680 --> 00:10:21,320
That's really their reality.

122
00:10:21,320 --> 00:10:24,200
These are their observed variables.

123
00:10:24,200 --> 00:10:29,880
But they can't actually directly observe or measure the physical objects themselves

124
00:10:29,880 --> 00:10:32,280
that are actually casting these shadows.

125
00:10:32,280 --> 00:10:38,640
So those objects are effectively what we can analyze like latent variables.

126
00:10:38,640 --> 00:10:43,560
They're the variables that are not directly observable, but they're the true explanatory

127
00:10:43,560 --> 00:10:48,240
factors that are creating the observable variables, which in this case, the prisoners

128
00:10:48,240 --> 00:10:52,680
are seeing, like the shadows cast on the wall.

129
00:10:52,680 --> 00:10:59,440
And so our question in generative modeling broadly is to find ways of actually learning

130
00:10:59,440 --> 00:11:05,320
these underlying and hidden latent variables in the data, even when we're only given the

131
00:11:05,320 --> 00:11:07,680
observations that are made.

132
00:11:07,680 --> 00:11:14,720
And this is an extremely, extremely complex problem that is very well suited to learning

133
00:11:14,720 --> 00:11:20,800
by neural networks because of their power to handle multidimensional data sets and to

134
00:11:20,800 --> 00:11:26,840
learn combinations of nonlinear functions that can approximate really complex data distributions.

135
00:11:26,840 --> 00:11:28,480
All right.

136
00:11:28,480 --> 00:11:34,040
So we'll first begin by discussing a simple and foundational generative model, which tries

137
00:11:34,040 --> 00:11:40,200
to build up these latent variable representation by actually self encoding the input.

138
00:11:40,200 --> 00:11:43,680
And these models are known as auto encoders.

139
00:11:43,680 --> 00:11:50,280
What an auto encoder is, is it's an approach for learning a lower dimensional latent space

140
00:11:50,280 --> 00:11:52,760
from raw data.

141
00:11:52,760 --> 00:11:58,800
To understand how it works, what we do is we feed in as input raw data, for example,

142
00:11:58,800 --> 00:12:03,120
this image of a two, that's going to be passed through many successive deep neural network

143
00:12:03,120 --> 00:12:04,280
layers.

144
00:12:04,280 --> 00:12:09,320
And at the output of that succession of neural network layers, what we're going to generate

145
00:12:09,320 --> 00:12:13,760
is a low dimensional latent space, a feature representation.

146
00:12:13,760 --> 00:12:17,240
And that's really the goal that we're trying to predict.

147
00:12:17,240 --> 00:12:22,080
And so we can call this portion of the network an encoder, since it's mapping the data,

148
00:12:22,080 --> 00:12:27,960
x, into a encoded vector of latent variables, z.

149
00:12:27,960 --> 00:12:32,200
So let's consider this latent space, z.

150
00:12:32,200 --> 00:12:37,440
If you've noticed, I've represented z as having a smaller size, a smaller dimensionality

151
00:12:37,440 --> 00:12:43,840
as the input x, why would it be important to ensure the low dimensionality of this latent

152
00:12:43,840 --> 00:12:47,120
space, z?

153
00:12:47,120 --> 00:12:51,880
Having a low dimensional latent space means that we are able to compress the data, which

154
00:12:51,880 --> 00:12:57,120
in the case of image data can be, you know, on the order of many, many, many dimensions,

155
00:12:57,120 --> 00:13:03,160
we can compress the data into a small latent vector, where we can learn a very compact and

156
00:13:03,160 --> 00:13:06,600
rich feature representation.

157
00:13:06,600 --> 00:13:09,640
So how can we actually train this model?

158
00:13:09,640 --> 00:13:14,960
Are we going to be able to supervise for the particular latent variables that we're interested

159
00:13:14,960 --> 00:13:15,960
in?

160
00:13:15,960 --> 00:13:21,960
Well, remember that this is an unsupervised problem, where we have training data, but

161
00:13:21,960 --> 00:13:25,320
no labels for the latent space, z.

162
00:13:25,320 --> 00:13:31,280
So in order to actually train such a model, what we can do is learn a decoder network

163
00:13:31,280 --> 00:13:36,640
and build up a decoder network that is used to actually reconstruct the original image

164
00:13:36,640 --> 00:13:40,200
starting from this lower dimensional latent space.

165
00:13:40,200 --> 00:13:46,160
And again, this decoder portion of our autoencoder network is going to be a series of layers,

166
00:13:46,160 --> 00:13:51,240
neural network layers like convolutional layers, that's going to then take this hidden latent

167
00:13:51,240 --> 00:13:55,760
vector and map it back up to the input space.

168
00:13:55,760 --> 00:14:00,520
And we call our reconstructed output x hat, because it's our prediction and it's an imperfect

169
00:14:00,520 --> 00:14:05,360
reconstruction of our input x.

170
00:14:05,360 --> 00:14:09,920
And the way that we can actually train this network is by looking at the original input

171
00:14:09,920 --> 00:14:16,480
x and our reconstructed output x hat and simply comparing the two and minimizing the distance

172
00:14:16,480 --> 00:14:20,040
between these two images.

173
00:14:20,040 --> 00:14:25,120
So for example, we could consider the mean squared error, which in the case of images

174
00:14:25,120 --> 00:14:31,520
means effectively subtracting one image from another and squaring the difference, which

175
00:14:31,520 --> 00:14:36,520
is effectively the pixel wise difference between the input and reconstruction, measuring how

176
00:14:36,520 --> 00:14:41,000
faithful our reconstruction is to the original input.

177
00:14:41,000 --> 00:14:47,360
And again, notice that by using this reconstruction loss, this difference between the reconstructed

178
00:14:47,360 --> 00:14:55,080
output and our original input, we do not require any labels for our data beyond the data itself.

179
00:14:56,080 --> 00:15:05,080
So we can simplify this diagram just a little bit by abstracting away these individual layers

180
00:15:05,080 --> 00:15:07,720
in the encoder and decoder components.

181
00:15:07,720 --> 00:15:13,840
And again, note once again that this loss function does not require any labels, it is

182
00:15:13,840 --> 00:15:18,280
just using the raw data to supervise itself on the output.

183
00:15:18,280 --> 00:15:24,600
And this is a truly powerful idea and a transformative idea because it enables the model to learn

184
00:15:24,600 --> 00:15:30,920
a quantity, the latent variables z, that we're fundamentally interested in, but we

185
00:15:30,920 --> 00:15:35,360
cannot simply observe or cannot readily model.

186
00:15:35,360 --> 00:15:43,160
And when we constrain this latent space to a lower dimensionality, that affects the degree

187
00:15:43,160 --> 00:15:47,440
to which and the faithfulness to which we can actually reconstruct the input.

188
00:15:47,440 --> 00:15:53,160
And the way you can think of this is as imposing a sort of information bottleneck during the

189
00:15:53,160 --> 00:15:55,720
models training and learning process.

190
00:15:55,720 --> 00:16:00,440
And effectively, what this bottleneck does is a form of compression, right?

191
00:16:00,440 --> 00:16:06,280
We're taking the input data, compressing it down to a much smaller latent space, and

192
00:16:06,280 --> 00:16:09,120
then building back up a reconstruction.

193
00:16:09,120 --> 00:16:13,480
And in practice, what this results in is that the lower the dimensionality of your latent

194
00:16:13,480 --> 00:16:18,760
space, the poorer and worse quality reconstruction you're going to get out.

195
00:16:18,760 --> 00:16:20,520
All right.

196
00:16:20,520 --> 00:16:25,960
So in summary, these autoencoder structures use this sort of bottlenecking hidden layer

197
00:16:25,960 --> 00:16:30,040
to learn a compressed latent representation of the data.

198
00:16:30,040 --> 00:16:34,760
And we can self-supervise the training of this network by using what we call a reconstruction

199
00:16:34,760 --> 00:16:42,560
loss that forces the autoencoder network to encode as much information about the data

200
00:16:42,560 --> 00:16:48,840
as possible into a lower dimensional latent space while still being able to build up faithful

201
00:16:49,040 --> 00:16:50,560
reconstructions.

202
00:16:50,560 --> 00:16:56,680
So the way I like to think of this is automatically encoding information from the data into a

203
00:16:56,680 --> 00:17:00,640
lower dimensional latent space.

204
00:17:00,640 --> 00:17:06,040
Let's now expand upon this idea a bit more and introduce this concept and architecture

205
00:17:06,040 --> 00:17:11,400
of variational autoencoders or VAEs.

206
00:17:11,400 --> 00:17:18,200
So as we just saw, traditional autoencoders go from input to reconstructed output.

207
00:17:18,200 --> 00:17:24,000
And if we pay closer attention to this latent layer denoted to here in orange, what you

208
00:17:24,000 --> 00:17:28,640
can hopefully realize is that this is just a normal layer in a neural network, just

209
00:17:28,640 --> 00:17:30,000
like any other layer.

210
00:17:30,000 --> 00:17:31,400
It's deterministic.

211
00:17:31,400 --> 00:17:35,080
If you're going to feed in a particular input to this network, you're going to get the

212
00:17:35,080 --> 00:17:38,280
same output so long as the weights are the same.

213
00:17:38,280 --> 00:17:43,800
So effectively, a traditional autoencoder learns this deterministic encoding, which allows

214
00:17:43,800 --> 00:17:48,000
for reconstruction and reproduction of the input.

215
00:17:48,000 --> 00:17:55,600
In contrast, variational autoencoders impose a stochastic or variational twist on this

216
00:17:55,600 --> 00:17:57,200
architecture.

217
00:17:57,200 --> 00:18:03,880
And the idea behind doing so is to generate smoother representations of the input data

218
00:18:03,880 --> 00:18:10,600
and improve the quality of the not only of reconstructions, but also to actually generate

219
00:18:10,600 --> 00:18:16,320
new images that are similar to the input data set, but not direct reconstructions of the

220
00:18:16,320 --> 00:18:17,800
input data.

221
00:18:17,800 --> 00:18:24,040
And the way this is achieved is that variational autoencoders replace that deterministic layer

222
00:18:24,040 --> 00:18:28,600
Z with a stochastic sampling operation.

223
00:18:28,600 --> 00:18:34,120
What this means is that instead of learning the latent variables Z directly, for each

224
00:18:34,120 --> 00:18:40,680
variable, the variational autoencoder learns a mean and a variance associated with that

225
00:18:40,680 --> 00:18:42,280
latent variable.

226
00:18:42,280 --> 00:18:47,320
And what those means and variances do is that they parametrize a probability distribution

227
00:18:47,320 --> 00:18:49,480
for that latent variable.

228
00:18:49,480 --> 00:18:55,600
So what we've done in going from an autoencoder to a variational autoencoder is going from

229
00:18:55,600 --> 00:19:02,600
a vector of latent variables Z to learning a vector of means mu and a vector of variances

230
00:19:02,600 --> 00:19:10,680
sigma, sigma squared, that parametrize these variables and define probability distributions

231
00:19:10,680 --> 00:19:13,800
for each of our latent variables.

232
00:19:13,800 --> 00:19:20,240
And the way we can actually generate new data instances is by sampling from the distribution

233
00:19:20,240 --> 00:19:29,840
defined by these mus and sigmas to generate a latent sample and get probabilistic representations

234
00:19:29,840 --> 00:19:32,280
of the latent space.

235
00:19:32,280 --> 00:19:35,960
And what I'd like you to appreciate about this network architecture is that it's very

236
00:19:35,960 --> 00:19:41,720
similar to the autoencoder I previously introduced, just that we have this probabilistic twist

237
00:19:41,800 --> 00:19:47,760
where we're now performing the sampling operation to compute samples from each of the latent

238
00:19:47,760 --> 00:19:49,760
variables.

239
00:19:49,760 --> 00:19:57,440
All right, so now because we've introduced this sampling operation, this stochasticity

240
00:19:57,440 --> 00:20:03,640
into our model, what this means for the actual computation and learning process of the network,

241
00:20:03,640 --> 00:20:09,480
the encoder and decoder, is that they're now probabilistic in their nature.

242
00:20:09,520 --> 00:20:14,800
And the way you can think of this is that our encoder is going to be trying to learn

243
00:20:14,800 --> 00:20:23,000
a probability distribution of the latent space Z given the input data X, while the decoder

244
00:20:23,000 --> 00:20:29,440
is going to take that learned latent representation and compute a new probability distribution

245
00:20:29,440 --> 00:20:33,800
of the input X given that latent distribution Z.

246
00:20:33,800 --> 00:20:39,400
And these networks, the encoder, the decoder, are going to be defined by separate sets of

247
00:20:39,440 --> 00:20:42,480
weights, phi and theta.

248
00:20:42,480 --> 00:20:48,920
And the way that we can train this variational autoencoder is by defining a loss function

249
00:20:48,920 --> 00:20:55,920
that's going to be a function of the data X as well as the sets of weights phi and theta.

250
00:20:55,920 --> 00:21:02,680
And what's key to how VAEs can be optimized is that this loss function is now comprised

251
00:21:02,680 --> 00:21:05,160
of two terms instead of just one.

252
00:21:05,160 --> 00:21:10,800
We have the reconstruction loss just as before, which again is going to capture this difference

253
00:21:10,800 --> 00:21:16,920
between the input and the reconstructed output, and also a new term to our loss, which we

254
00:21:16,920 --> 00:21:22,920
call the regularization loss, also called the VAE loss.

255
00:21:22,920 --> 00:21:29,920
And to take a look in more detail at why each of these loss terms represents, let's first

256
00:21:29,960 --> 00:21:36,320
emphasize again that our overall loss function is going to be defined and taken with respect

257
00:21:36,320 --> 00:21:42,320
to the sets of weights of the encoder and decoder and the input X.

258
00:21:42,320 --> 00:21:46,080
The reconstruction loss is very similar to before, right?

259
00:21:46,080 --> 00:21:51,840
And you can think of it as being driven by a log likelihood function, for example, for

260
00:21:51,840 --> 00:21:56,880
image data, the mean squared error between the input and the output.

261
00:21:56,880 --> 00:22:02,400
And we can self-supervise the reconstruction loss just as before to force the latent space

262
00:22:02,400 --> 00:22:08,800
to learn and represent faithful representations of the input data, ultimately resulting in

263
00:22:08,800 --> 00:22:11,520
faithful reconstructions.

264
00:22:11,520 --> 00:22:17,120
The new term here, the regularization term, is a bit more interesting and completely new

265
00:22:17,120 --> 00:22:23,080
at this stage, so we're going to dive in and discuss it further in a bit more detail.

266
00:22:23,080 --> 00:22:28,840
So our probability distribution that's going to be computed by our encoder, q phi of z

267
00:22:28,840 --> 00:22:34,840
of x, is a distribution on the latent space z given the data x.

268
00:22:34,840 --> 00:22:41,440
And what regularization enforces is that as a part of this learning process, we're going

269
00:22:41,440 --> 00:22:48,440
to place a prior on the latent space z, which is effectively some initial hypothesis about

270
00:22:49,160 --> 00:22:53,560
what we expect the distributions of z to actually look like.

271
00:22:53,560 --> 00:22:59,160
And by imposing this regularization term, what we can achieve is that the model will

272
00:22:59,160 --> 00:23:04,600
try to enforce the z's that it learns to follow this prior distribution.

273
00:23:04,600 --> 00:23:08,360
And we're going to denote this prior as p of z.

274
00:23:08,360 --> 00:23:12,520
This term here, d, is the regularization term.

275
00:23:12,600 --> 00:23:20,120
And what it's going to do is it's going to try to enforce a minimization of the divergence

276
00:23:20,120 --> 00:23:26,040
or the difference between what the encoder is trying to infer, the probability distribution

277
00:23:26,040 --> 00:23:33,000
of z given x, and that prior that we're going to place on the latent variables p of z.

278
00:23:33,720 --> 00:23:40,040
And the idea here is that by imposing this regularization factor, we can try to keep the

279
00:23:40,040 --> 00:23:45,640
network from overfitting on certain parts of the latent space by enforcing the fact that

280
00:23:45,640 --> 00:23:51,240
we want to encourage the latent variables to adopt a distribution that's similar to our prior.

281
00:23:51,960 --> 00:23:57,640
So we're going to go through now, you know, both the mathematical basis for this regularization

282
00:23:57,640 --> 00:24:04,200
term as well as a really intuitive walkthrough of what regularization achieves to help give you a

283
00:24:04,200 --> 00:24:09,080
concrete understanding and an intuitive understanding about why regularization is

284
00:24:09,080 --> 00:24:15,880
important and why placing a prior is important. So let's first consider,

285
00:24:18,360 --> 00:24:23,320
yeah, so to re-emphasize once again, this regularization term is going to consider

286
00:24:23,320 --> 00:24:28,600
the divergence between our inferred latent distribution and the fixed prior we're going to place.

287
00:24:29,560 --> 00:24:37,480
So before we to get into this, let's consider what could be a good choice of prior for each of

288
00:24:37,480 --> 00:24:44,840
these latent variables. How do we select p of z? I'll first tell you what's commonly done.

289
00:24:45,720 --> 00:24:51,560
The common choice that's used very extensively in the community is to enforce the latent variables

290
00:24:51,560 --> 00:24:56,600
to roughly follow normal Gaussian distributions, which means that they're going to be a normal

291
00:24:57,160 --> 00:25:03,320
distribution centered around mean zero and have a standard deviation and variance of one.

292
00:25:04,600 --> 00:25:10,680
By placing these normal Gaussian priors on each of the latent variables and therefore on our

293
00:25:10,680 --> 00:25:16,520
latent distribution overall, what this encourages is that the learned encodings learned by the

294
00:25:16,520 --> 00:25:23,400
encoder portion of our VAE are going to be sort of distributed evenly around the center of each

295
00:25:23,400 --> 00:25:30,760
of the latent variables. And if you can imagine and picture when you have sort of a roughly even

296
00:25:30,760 --> 00:25:37,400
distribution around the center of a particular region of the latent space, what this means is

297
00:25:37,400 --> 00:25:43,800
that outside of this region, far away, there's going to be a greater penalty and this can result

298
00:25:43,800 --> 00:25:50,360
in instances from instances where the network is trying to cheat and try to cluster particular

299
00:25:50,360 --> 00:25:56,200
points outside the center, these centers in the latent space, like if it was trying to memorize

300
00:25:56,200 --> 00:26:03,160
particular outliers or edge cases in the data. After we place a normal Gaussian prior on our

301
00:26:03,160 --> 00:26:10,120
latent variables, we can now begin to concretely define the regularization term component of our

302
00:26:10,120 --> 00:26:17,160
loss function. This loss, this term to the loss is very similar in principle to a cross entropy

303
00:26:17,160 --> 00:26:24,360
loss that we saw before, where the key is that we're going to be defining the distance function

304
00:26:24,360 --> 00:26:31,720
that describes the difference or the divergence between the inferred latent distribution q,

305
00:26:31,720 --> 00:26:38,600
phi of z given x and the prior that we're going to be placing p of z. And this term is called

306
00:26:38,600 --> 00:26:44,520
the Kublack-Liebler or KL divergence. And when we choose a normal Gaussian prior,

307
00:26:45,800 --> 00:26:52,920
we, this results in the KL divergence taking this particular form of this equation here,

308
00:26:52,920 --> 00:26:59,240
where we're using the means and sigmas as input and computing this distance metric that captures

309
00:26:59,240 --> 00:27:04,680
the divergence of that learned latent variable distribution from the normal Gaussian.

310
00:27:05,320 --> 00:27:11,160
All right. So now I really want to spend a bit of time to get some, build up some intuition about

311
00:27:11,160 --> 00:27:18,680
how this regularization and works and why we actually want to regularize our VAE and then also

312
00:27:18,680 --> 00:27:25,480
why we select a normal prior. All right. So to do this, let's, let's consider the following question.

313
00:27:25,480 --> 00:27:32,040
What properties do we want this to achieve from regularization? Why are we actually regularizing

314
00:27:32,760 --> 00:27:40,680
our, our network in the first place? The first key property that we want for a generative model like

315
00:27:40,680 --> 00:27:47,480
a VAE is what I can, what I like to think of as continuity, which means that if there are points

316
00:27:47,480 --> 00:27:53,160
that are represented closely in the latent space, they should also result in similar

317
00:27:53,800 --> 00:28:00,760
reconstructions, similar outputs, similar content after they are decoded. You would expect intuitively

318
00:28:00,760 --> 00:28:07,000
that regions in the latent space have some notion of distance or similarity to each other. And this

319
00:28:07,000 --> 00:28:13,080
indeed is a really key property that we want to achieve with our generative model. The second

320
00:28:13,080 --> 00:28:20,040
property is completeness and it's very related to continuity. And what this means is that when

321
00:28:20,040 --> 00:28:27,640
we sample from the latent space to decode the latent space into an output, that should result

322
00:28:27,640 --> 00:28:34,440
in a meaningful reconstruction, a meaningful, uh, sampled content that is, you know, resembling

323
00:28:34,440 --> 00:28:40,920
the original data distribution. You can imagine that if we're sampling from the latent space and

324
00:28:40,920 --> 00:28:47,720
just getting garbage out that has no relationship to our input, this could be a huge, huge problem

325
00:28:47,720 --> 00:28:54,120
for our model. All right. So with these two properties in mind, continuity and completeness,

326
00:28:54,840 --> 00:29:00,440
let's consider the consequences of what can occur if we do not regularize our model.

327
00:29:02,040 --> 00:29:07,720
Well, without regularization, what could end up happening with respect to these two properties

328
00:29:07,720 --> 00:29:14,040
is that there could be instances of points that are close in latent space, but not similarly

329
00:29:14,040 --> 00:29:21,160
decoded. So I'm using this really intuitive illustration where these dots represent abstracted

330
00:29:21,160 --> 00:29:27,880
away sort of regions in the latent space. And the shapes that they relate to, you can think of as

331
00:29:27,880 --> 00:29:34,440
what is going to be decoded after those instances in the latent space are passed through the decoder.

332
00:29:35,320 --> 00:29:41,160
So in this example, we have these two dots, the greenish dot and the reddish dot,

333
00:29:41,160 --> 00:29:45,720
that are physically close in latent space, but result in completely different shapes

334
00:29:45,800 --> 00:29:52,680
when they're decoded. We also have an instance of this purple point, which when it's decoded,

335
00:29:52,680 --> 00:29:59,560
it doesn't result in a meaningful content. It's just a scribble. So by not regularizing,

336
00:29:59,560 --> 00:30:05,800
and I'm abstracting a lot away here, and that's on purpose, we could have these instances where

337
00:30:05,800 --> 00:30:12,440
we don't have continuity and we don't have completeness. Therefore, our goal with regularization

338
00:30:12,440 --> 00:30:19,640
is to be able to realize a model where points that are close in the latent space are not only

339
00:30:19,640 --> 00:30:25,400
similarly decoded, but also meaningfully decoded. So for example, here, we have the red dot and the

340
00:30:25,400 --> 00:30:31,720
orange dot, which result in both triangle-like shapes, but with slight variations on the on the

341
00:30:31,720 --> 00:30:38,520
triangle itself. So this is the intuition about what regularization can enable us to achieve

342
00:30:38,520 --> 00:30:45,800
and what are desired properties for these generative models. Okay, how can we actually

343
00:30:45,800 --> 00:30:53,000
achieve this regularization? And how does the normal prior fit in? As I mentioned, right,

344
00:30:53,000 --> 00:30:59,800
VAEs, they don't just learn the latent variable Z directly. They're trying to encode the inputs

345
00:30:59,800 --> 00:31:05,400
as distributions that are defined by mean and variance. So my first question to you is,

346
00:31:05,400 --> 00:31:10,200
is it going to be sufficient to just learn mean and variance, learn these distributions?

347
00:31:10,920 --> 00:31:17,400
Can that guarantee continuity and completeness? No, and let's understand why.

348
00:31:19,240 --> 00:31:25,720
All right, without any sort of regularization, what could the model try to resort to?

349
00:31:27,000 --> 00:31:34,600
Remember that the VAE or that the VAE, the loss function is defined by both a reconstruction

350
00:31:34,600 --> 00:31:40,520
term and a regularization term. If there is no regularization, you can bet that the model

351
00:31:40,520 --> 00:31:46,280
is going to just try to optimize that reconstruction term. So it's effectively going to learn to

352
00:31:46,280 --> 00:31:52,920
minimize the reconstruction loss, even though we're encoding the latent variables via mean and variance.

353
00:31:54,360 --> 00:32:01,240
And two instances, two consequences of that is that you can have instances where these

354
00:32:01,240 --> 00:32:05,960
learned variances for the latent variable end up being very, very, very small,

355
00:32:06,520 --> 00:32:12,280
effectively resulting in pointed distributions. And you can also have means that are totally

356
00:32:12,280 --> 00:32:17,400
divergent from each other, which result in discontinuities in the latent space. And this can

357
00:32:17,400 --> 00:32:25,240
occur while still trying to optimize that reconstruction loss, direct consequence of not regularizing.

358
00:32:25,320 --> 00:32:32,120
By, in order to overcome these problems, we need to regularize the variance and the mean of these

359
00:32:32,120 --> 00:32:37,720
distributions that are being returned by the encoder. And the normal prior, placing that

360
00:32:37,720 --> 00:32:45,560
normal Gaussian distribution as our prior helps us achieve this. And to understand why exactly

361
00:32:45,560 --> 00:32:51,560
this occurs, is that effectively the normal prior is going to encourage these learned latent

362
00:32:51,560 --> 00:32:58,840
variable distributions to overlap in latent space. Recall, right? Mean zero, variance of one. That

363
00:32:58,840 --> 00:33:04,760
means all the, all the latent variables are going to be enforced to try to have the same mean, a

364
00:33:04,760 --> 00:33:10,120
centered mean, and all the variances are going to be regularized for each and every of the latent

365
00:33:10,120 --> 00:33:16,280
variable distributions. And so this will ensure a smoothness and a regularity and an overlap in

366
00:33:16,280 --> 00:33:23,000
the latent space, which will be very effective in helping us achieve these properties of continuity

367
00:33:23,000 --> 00:33:32,200
and completeness. Centering the means, regularizing the variances. So the regularization via this

368
00:33:32,200 --> 00:33:39,240
normal prior, by centering each of these latent variables, regularizing their, their variances,

369
00:33:39,240 --> 00:33:44,360
is that it helps enforce this continuous and complete gradient of information.

370
00:33:44,440 --> 00:33:49,880
Represented in the latent space, where again points and distances in the latent space have

371
00:33:49,880 --> 00:33:55,640
some relationship to the reconstructions and the content of the reconstructions that result.

372
00:33:57,000 --> 00:34:04,040
Note though that there's going to be a tradeoff between regularizing and reconstructing. The

373
00:34:04,040 --> 00:34:10,360
more we regularize, there's also a risk of suffering, the quality of the reconstruction

374
00:34:10,440 --> 00:34:16,120
and the generation process itself. So in optimizing gaze, there's going to be this tradeoff that's

375
00:34:16,120 --> 00:34:23,240
going to try to be tuned to fit the problem of interest. All right. So hopefully by walking

376
00:34:23,240 --> 00:34:28,600
through this, this example, and considering these points, you've built up a more intuition about

377
00:34:28,600 --> 00:34:34,200
why regularization is important and how specifically the normal prior can help us regularize.

378
00:34:34,600 --> 00:34:39,720
Great. So now we've defined our loss function, we know that we can reconstruct the inputs,

379
00:34:39,720 --> 00:34:45,080
we've understood how we can regularize learning and achieve continuity and completeness via

380
00:34:45,080 --> 00:34:51,000
this normal prior. These are all the components that define a forward pass through the network,

381
00:34:51,800 --> 00:34:57,480
going from input to encoding to decoded reconstruction. But we're still missing a

382
00:34:57,480 --> 00:35:01,880
critical step in putting the whole picture together. And that's, that's a critical step

383
00:35:01,880 --> 00:35:07,800
putting the whole picture together. And that's a back propagation. And the key here is that

384
00:35:07,800 --> 00:35:14,040
because of this fact that we've introduced this stochastic sampling layer, we now have a problem

385
00:35:14,040 --> 00:35:19,720
where we can't back propagate gradients through a sampling layer that has this element of stochasticity.

386
00:35:20,840 --> 00:35:26,360
Back propagation requires deterministic nodes, deterministic layers for which we can iteratively

387
00:35:26,360 --> 00:35:32,440
apply the chain rule to optimize gradients. Optimize the loss via gradient descent.

388
00:35:33,720 --> 00:35:41,320
All right. VAEs introduced sort of a breakthrough idea that solved this issue of not being able

389
00:35:41,320 --> 00:35:47,400
to back propagate through a sampling layer. And the key idea was to actually subtly

390
00:35:47,400 --> 00:35:53,080
reparameterize the sampling operation such that the network could be trained completely end to end.

391
00:35:54,040 --> 00:35:59,480
So as we, as we already learned, right, we're trying to build up this latent distribution

392
00:35:59,480 --> 00:36:06,920
defined by these variables z define it placing a normal prior defined by a mean and a variance.

393
00:36:06,920 --> 00:36:13,480
And we can't simply back propagate gradients through the sampling layer because we can't compute

394
00:36:14,040 --> 00:36:21,160
gradients through this stochastic sample. The key idea instead is to try to consider the

395
00:36:21,240 --> 00:36:31,480
sampled latent vector z as a sum defined by a fixed mu a fixed sigma vector and scale that sigma

396
00:36:31,480 --> 00:36:38,280
vector by random constants that are going to be drawn from a prior distribution such as a normal

397
00:36:38,280 --> 00:36:46,120
Gaussian. And by reparameterizing the sampling operation as, as so, we still have this element

398
00:36:46,120 --> 00:36:52,680
of stochasticity, but that stochasticity is introduced by this random constant epsilon,

399
00:36:52,680 --> 00:36:58,360
which is not occurring within the bottleneck latent layer itself. We've reparameterized

400
00:36:58,360 --> 00:37:04,840
and distributed it elsewhere. To visualize how this looks, let's consider it the following

401
00:37:04,840 --> 00:37:12,280
where originally in the original form of the VAE, we had this deterministic nodes,

402
00:37:12,280 --> 00:37:16,920
which are the weights of the network, as well as an input vector, and we are trying to

403
00:37:16,920 --> 00:37:23,640
back propagate through the stochastic sampling node z. But we can't do that. So now,

404
00:37:23,640 --> 00:37:31,720
by reparameterization, what we've achieved is the following form where our latent variable z

405
00:37:31,720 --> 00:37:41,240
are defined with respect to mu sigma squared, as well as these noise factor epsilon,

406
00:37:41,320 --> 00:37:47,240
such that when we want to do back propagation through the network to update, we can directly

407
00:37:47,240 --> 00:37:53,000
back propagate through z defined by mu and sigma squared, because this epsilon value is just taken

408
00:37:53,000 --> 00:37:59,480
as a constant, it's reparameterized elsewhere. And this is a very, very powerful trick, the

409
00:37:59,480 --> 00:38:05,320
reparameterization trick, because it enables us to train variational auto encoders end to end

410
00:38:05,320 --> 00:38:12,360
by back propagating with respect to z and with respect to the actual weights of the encoder

411
00:38:12,360 --> 00:38:20,920
network. All right. One side effect and one consequence of imposing these distributional

412
00:38:20,920 --> 00:38:26,440
priors on the latent variable is that we can actually sample from these latent variables

413
00:38:26,440 --> 00:38:33,320
and individually tune them while keeping all of the other variables fixed. And what you can do

414
00:38:33,320 --> 00:38:38,760
is you can tune the value of a particular latent variable and run the decoder each time that variable

415
00:38:38,760 --> 00:38:45,960
is changed, each time that variable is perturbed to generate a new reconstructed output. So an

416
00:38:45,960 --> 00:38:53,000
example of that result is in the following, where this perturbation of the latent variables results

417
00:38:53,000 --> 00:39:00,040
in a representation that has some semantic meaning about what the network is maybe learning. So in

418
00:39:00,040 --> 00:39:06,920
this example, these images show variation in head pose. And the different dimensions of z,

419
00:39:06,920 --> 00:39:13,800
the latent space, the different latent variables, are in this way encoding different latent features

420
00:39:13,800 --> 00:39:20,280
that can be interpreted by keeping all other variables fixed and perturbing the value of

421
00:39:20,280 --> 00:39:29,640
one individual latent variable. Ideally, in order to optimize VAEs and try to maximize the information

422
00:39:29,640 --> 00:39:36,120
that they encode, we want these latent variables to be uncorrelated with each other, effectively

423
00:39:36,120 --> 00:39:42,120
disentangled. And what that could enable us to achieve is to learn the richest and most

424
00:39:42,120 --> 00:39:49,000
compact latent representation possible. So in this case, we have head pose on the x axis

425
00:39:49,000 --> 00:39:55,160
and smile on the y axis. And we want these to be as uncorrelated with each other as possible.

426
00:39:55,800 --> 00:40:01,320
One way we can achieve this, that's been shown to achieve this disentanglement,

427
00:40:01,320 --> 00:40:07,640
is rather a quite straightforward approach called beta VAEs. So if we consider the loss of a standard

428
00:40:07,640 --> 00:40:13,800
VAE, again, we have this reconstruction term defined by a log likelihood and a regularization term

429
00:40:13,800 --> 00:40:20,360
defined by the KL divergence. Beta VAEs introduce a new hyper parameter beta,

430
00:40:20,360 --> 00:40:26,440
which controls the strength of this regularization term. And it's been shown mathematically that

431
00:40:26,440 --> 00:40:32,520
by increasing beta, the effect is to place constraints on the latent encoding, such as to

432
00:40:32,520 --> 00:40:37,880
encourage disentanglement. And there have been extensive proofs and discussions as to how exactly

433
00:40:37,880 --> 00:40:44,120
this is achieved. But to consider the results, let's again consider the problem of face

434
00:40:44,120 --> 00:40:51,720
reconstruction. Where using a standard VAE, if we consider the latent variable of head pose

435
00:40:51,720 --> 00:40:57,400
or rotation, in this case where beta equals one, what you can hopefully appreciate is that as the

436
00:40:57,400 --> 00:41:06,120
face pose is changing, the smile of some of these faces is also changing. In contrast, by enforcing

437
00:41:06,120 --> 00:41:13,080
a beta much larger than one, what is able to be achieved is that the smile remains relatively

438
00:41:13,080 --> 00:41:19,960
constant while we can perturb the single latent variable of the head rotation and achieve perturbations

439
00:41:19,960 --> 00:41:27,880
with respect to head rotation alone. All right. So as I motivated and introduced in the beginning

440
00:41:27,880 --> 00:41:33,160
and the introduction of this lecture, one powerful application of generative models and latent

441
00:41:33,160 --> 00:41:39,400
variable models is in model debiasing. And in today's lab, you're actually going to get real hands-on

442
00:41:39,400 --> 00:41:46,120
experience in building a variational autoencoder that can be used to achieve automatic debiasing

443
00:41:46,120 --> 00:41:51,480
of facial classification systems, facial detection systems. And the power and the idea of this

444
00:41:52,040 --> 00:41:59,160
approach is to build up a representation, a learned latent distribution of face data,

445
00:41:59,880 --> 00:42:05,720
and use this to identify regions of that latent space that are going to be overrepresented

446
00:42:05,720 --> 00:42:10,120
or underrepresented. And that's going to all be taken with respect to particular

447
00:42:10,120 --> 00:42:17,560
learned features such as skin tone, pose, objects, clothing. And then from these learned

448
00:42:17,560 --> 00:42:24,760
distributions, we can actually adjust the training process such that we can place greater

449
00:42:25,640 --> 00:42:31,560
weight and greater sampling on those images and on those faces the fall in the regions of

450
00:42:31,560 --> 00:42:37,080
the latent space that are underrepresented automatically. And what's really, really cool

451
00:42:37,960 --> 00:42:43,800
about deploying a VAE or a latent variable model for an application like model debiasing

452
00:42:43,800 --> 00:42:49,640
is that there's no need for us to annotate and prescribe the features that are important

453
00:42:49,640 --> 00:42:55,000
to actually debize against. The model learns them automatically. And this is going to be the

454
00:42:55,000 --> 00:43:02,120
topic of today's lab. And it also opens the door to a much broader space that's going to

455
00:43:02,120 --> 00:43:07,400
be explored further in a later spotlight lecture that's going to focus on algorithmic bias and

456
00:43:07,400 --> 00:43:14,200
machine learning fairness. All right, so to summarize the key points on VAEs, they compress

457
00:43:14,200 --> 00:43:21,560
representation of data into an encoded representation. Reconstruction of the data input allows for

458
00:43:21,560 --> 00:43:29,400
unsupervised learning without labels. We can use the reparameterization trick to train

459
00:43:29,400 --> 00:43:36,200
VAEs end to end. We can take hidden latent variables, perturb them to interpret their

460
00:43:36,200 --> 00:43:41,400
content and their meaning. And finally, we can sample from the latent space to generate new

461
00:43:41,400 --> 00:43:49,000
examples. But what if we wanted to focus on generating samples and synthetic samples that

462
00:43:49,000 --> 00:43:55,640
were as faithful to a data distribution generally as possible? To understand how we can achieve

463
00:43:55,640 --> 00:44:00,200
this, we're going to transition to discuss a new type of generative model called a generative

464
00:44:00,200 --> 00:44:10,040
adversarial network or GAN for short. The idea here is that we don't want to explicitly model

465
00:44:10,040 --> 00:44:16,600
the density or the or the distribution underlying some data, but instead just learn a representation

466
00:44:16,600 --> 00:44:21,720
that can be successful in generating new instances that are similar to the data,

467
00:44:23,000 --> 00:44:28,760
which means that we want to optimize to sample from a very, very complex distribution,

468
00:44:29,400 --> 00:44:35,000
which cannot be learned and modeled directly. Instead, we're going to have to build up some

469
00:44:35,000 --> 00:44:42,440
approximation of this distribution. And the really cool and breakthrough idea of GANs

470
00:44:42,440 --> 00:44:49,320
is to start from something extremely, extremely simple, just random noise and try to build a

471
00:44:49,320 --> 00:44:55,480
neural network, a generative neural network that can learn a functional transformation

472
00:44:55,480 --> 00:45:03,880
that goes from noise to the data distribution. And by learning this functional generative mapping,

473
00:45:03,880 --> 00:45:10,200
we can then sample in order to generate fake instances, synthetic instances that are going

474
00:45:10,200 --> 00:45:17,400
to be as close to the real data distribution as possible. The breakthrough to achieving this

475
00:45:17,400 --> 00:45:23,880
was this structure called GANs, where the key component is to have two neural networks,

476
00:45:23,880 --> 00:45:29,160
a generator network and a discriminator network that are effectively competing against each other,

477
00:45:29,160 --> 00:45:34,760
their adversaries. Specifically, we have a generator network, which I'm going to denote

478
00:45:34,760 --> 00:45:40,920
here on out by G, that's going to be trained to go from random noise to produce an imitation of

479
00:45:40,920 --> 00:45:47,880
the data. And then the discriminator is going to take that synthetic fake data, as well as real

480
00:45:47,880 --> 00:45:53,720
data, and be trained to actually distinguish between fake and real. And in training, these two

481
00:45:53,720 --> 00:46:00,120
networks are going to be competing against each other. And so in doing so, overall, the effect

482
00:46:00,120 --> 00:46:05,000
is that the discriminator is going to get better and better at learning how to classify real and

483
00:46:05,000 --> 00:46:09,880
fake. And the better it becomes at doing that, it's going to force the generator to try to produce

484
00:46:09,880 --> 00:46:14,840
better and better synthetic data to try to fool the discriminator back and forth, back and forth.

485
00:46:15,800 --> 00:46:22,840
So let's now break this down and go from a very simple toy example to get more intuition

486
00:46:22,840 --> 00:46:29,560
about how these GANs work. The generator is going to start, again, from some completely random noise

487
00:46:29,560 --> 00:46:34,840
and produce fake data. And I'm going to show that here by representing these data as points on a

488
00:46:34,840 --> 00:46:40,920
one-dimensional line. The discriminator is then going to see these points, as well as real data.

489
00:46:41,880 --> 00:46:47,960
And then it's going to be trained to output a probability that the data it sees are real,

490
00:46:48,600 --> 00:46:53,560
or if they are fake. And in the beginning, it's not going to be trained very well, right? So its

491
00:46:53,560 --> 00:46:58,200
predictions are not going to be very good. But then you're going to train it, and you're going to train

492
00:46:58,200 --> 00:47:05,560
it. And it's going to start increasing the probabilities of real versus not real appropriately,

493
00:47:05,560 --> 00:47:11,000
such that you get this perfect separation where the discriminator is able to perfectly distinguish

494
00:47:11,000 --> 00:47:17,160
what is real and what is fake. Now it's back to the generator. And the generator is going to come

495
00:47:17,160 --> 00:47:24,680
back. It's going to take instances of where the real data lie as inputs to train. And then it's

496
00:47:24,680 --> 00:47:30,840
going to try to improve its imitation of the data, trying to move the fake data, the synthetic data

497
00:47:30,840 --> 00:47:37,240
that is generated closer and closer to the real data. And once again, the discriminator is now

498
00:47:37,240 --> 00:47:42,600
going to receive these new points. And it's going to estimate a probability that each of these points

499
00:47:42,600 --> 00:47:48,600
is real. And again, learn to decrease the probability of the fake points being real,

500
00:47:49,240 --> 00:47:55,320
further and further. And now we're going to repeat again. And one last time, the generator is going

501
00:47:55,320 --> 00:48:01,400
to start moving these fake points closer and closer to the real data, such that the fake

502
00:48:01,400 --> 00:48:07,240
data are almost following the distribution of the real data. At this point, it's going to be

503
00:48:07,240 --> 00:48:12,120
really, really hard for the discriminator to effectively distinguish between what is real

504
00:48:12,120 --> 00:48:17,880
and what is fake. While the generator is going to continue to try to create fake data instances

505
00:48:17,880 --> 00:48:24,280
to fool the discriminator. And this is really the key intuition behind how these two components of

506
00:48:24,280 --> 00:48:32,120
GANs are essentially competing with each other. All right. So to summarize how we train GANs,

507
00:48:32,120 --> 00:48:37,720
the generator is going to try to synthesize fake instances to fool a discriminator, which is going

508
00:48:37,720 --> 00:48:42,920
to be trained to identify the synthesized instances and discriminate these as fake.

509
00:48:42,920 --> 00:48:49,320
To actually train, we're going to see that we are going to define a loss function that defines

510
00:48:50,360 --> 00:48:55,000
competing and adversarial objectives for each of the discriminator and the generator.

511
00:48:55,720 --> 00:49:01,720
And a global optimum, the best we could possibly do would mean that the generator could perfectly

512
00:49:01,720 --> 00:49:07,000
reproduce the true data distribution, such that the discriminator absolutely cannot tell

513
00:49:07,000 --> 00:49:12,200
what's synthetic versus what's real. So let's go through how the loss function

514
00:49:12,200 --> 00:49:20,280
for GAN breaks down. The loss term for GAN is based on that familiar cross entropy loss.

515
00:49:20,280 --> 00:49:25,800
And it's going to now be defined between the true and generated distributions.

516
00:49:25,800 --> 00:49:30,280
So we're first going to consider the loss from the perspective of the discriminator.

517
00:49:30,920 --> 00:49:38,040
We want to try to maximize the probability that the fake data is identified as fake.

518
00:49:38,760 --> 00:49:45,640
And so to break this down here, G of Z defines the generator's output. And so D of G of Z

519
00:49:46,280 --> 00:49:51,240
is the discriminator's estimate of the probability that a fake instance is actually fake.

520
00:49:52,600 --> 00:49:58,040
D of X is the discriminator's estimate of the probability that a real instance is fake.

521
00:49:58,040 --> 00:50:03,480
So 1 minus D of X is its probability estimate that a real instance is real.

522
00:50:04,440 --> 00:50:09,640
So together, from the point of view of the discriminator, we want to maximize this probability.

523
00:50:10,200 --> 00:50:14,920
Maximize probability fake is fake. Maximize the estimate of probability real is real.

524
00:50:16,520 --> 00:50:21,320
Now let's turn our attention to the generator. Remember that the generator is taking

525
00:50:21,880 --> 00:50:28,120
random noise and generating an instance. It cannot directly affect the term D of X,

526
00:50:28,760 --> 00:50:34,840
which shows up in the loss, right? Because D of X is solely based on the discriminator's operation

527
00:50:34,840 --> 00:50:39,880
on the real data. So for the generator, the generator is going to have the adversarial

528
00:50:39,880 --> 00:50:45,160
objective to the discriminator, which means it's going to try to minimize this term.

529
00:50:46,120 --> 00:50:55,000
Effectively minimizing the probability that the discriminator can distinguish its generated data

530
00:50:55,960 --> 00:51:05,160
as fake. D of G of Z. And the goal for the generator is to minimize this term of the objective.

531
00:51:07,720 --> 00:51:13,720
So the objective of the generator is to try to synthesize fake instances

532
00:51:13,720 --> 00:51:18,600
that fool the discriminator. And eventually, over the course of training the discriminator,

533
00:51:18,600 --> 00:51:24,520
the discriminator is going to be as best as it possibly can be at discriminating real versus

534
00:51:24,520 --> 00:51:30,440
fake. Therefore, the ultimate goal of the generator is to synthesize fake instances that fool the

535
00:51:30,440 --> 00:51:36,520
best discriminator. And this is all put together in this min max objective function, which has

536
00:51:36,520 --> 00:51:42,680
these two components optimized adversarily. And then after training, we can actually use the

537
00:51:42,680 --> 00:51:48,360
generator network, which is now fully trained to produce new data instances that have never been

538
00:51:48,360 --> 00:51:54,840
seen before. So we're going to focus on that now. And what is really cool is that when the

539
00:51:54,840 --> 00:52:01,480
trained generator of again, synthesizes new instances, it's effectively learning a transformation

540
00:52:01,480 --> 00:52:07,240
from a distribution of noise to a target data distribution. And that transformation,

541
00:52:07,240 --> 00:52:12,200
that mapping is going to be what's learned over the course of training. So if we consider one

542
00:52:12,200 --> 00:52:17,560
point from a latent noise distribution, it's going to result in a particular output in the

543
00:52:17,560 --> 00:52:23,800
target data space. And if we consider another point of random noise, feed it through the generator,

544
00:52:23,800 --> 00:52:29,160
it's going to result in a new instance that and that new instance is going to fall somewhere else

545
00:52:29,160 --> 00:52:36,280
on the data manifold. And indeed, what we can actually do is interpolate and trans and traverse

546
00:52:36,280 --> 00:52:42,120
in the space of Gaussian noise to result in interpolation in the target space. And you can

547
00:52:42,120 --> 00:52:48,440
see an example of this result here, where a transformation in series reflects a traversal

548
00:52:48,440 --> 00:52:55,320
across the target data manifold. And that's produced in the synthetic examples that are

549
00:52:55,320 --> 00:53:01,160
outputted by the generator. All right. So in the final few minutes of this lecture, I'm going to

550
00:53:01,160 --> 00:53:07,560
highlight some of the recent advances in GANs and hopefully motivate even further why this approach

551
00:53:07,560 --> 00:53:14,520
is so powerful. So one idea that's been extremely, extremely powerful is this idea of progressive

552
00:53:14,520 --> 00:53:20,840
GANs, progressive growing, which means that we can iteratively build more detail into the

553
00:53:20,840 --> 00:53:28,280
generated instances that are produced. And this is done by progressively adding layers of increasing

554
00:53:28,280 --> 00:53:35,160
spatial resolution in the case of image data. And by incrementally building up both the generator

555
00:53:35,160 --> 00:53:41,240
and discriminator networks in this way as training progresses, it results in very well resolved

556
00:53:41,240 --> 00:53:47,480
synthetic images that are output ultimately by the generator. So some results of this idea of

557
00:53:47,480 --> 00:53:54,760
progressive, a progressive GAN are displayed here. Another idea that has also led to tremendous

558
00:53:54,760 --> 00:54:01,000
improvement in the quality of synthetic examples generated by GANs is a architecture improvement

559
00:54:01,000 --> 00:54:06,440
called style GAN, which combines this idea of progressive growing that I introduced earlier

560
00:54:06,440 --> 00:54:13,000
with principles of style transfer, which means trying to compose an image in the style of another

561
00:54:13,000 --> 00:54:22,520
image. So for example, what we can now achieve is to map input images source A using application of

562
00:54:22,520 --> 00:54:29,080
coarse grained styles from secondary sources onto those targets to generate new instances

563
00:54:29,080 --> 00:54:37,080
that mimic the style of source B. And that result is shown here. And hopefully you can

564
00:54:37,080 --> 00:54:42,680
appreciate that these coarse grained features, these coarse grained styles like age, facial

565
00:54:42,680 --> 00:54:50,200
structure, things like that can be reflected in these synthetic examples. This same style GAN

566
00:54:50,200 --> 00:54:58,440
system has led to tremendously realistic synthetic images in the areas of both face

567
00:54:58,440 --> 00:55:06,920
synthesis as well as for animals, other objects as well. Another extension to the GAN architecture

568
00:55:06,920 --> 00:55:13,880
that has enabled particularly powerful applications for select problems and tasks is this idea of

569
00:55:13,880 --> 00:55:19,880
conditioning, which imposes a bit of additional further structure on the types of outputs that

570
00:55:19,880 --> 00:55:27,240
can be synthesized by GAN. So the idea here is to condition on a particular label by supplying

571
00:55:27,240 --> 00:55:35,320
what is called a conditioning factor denoted here as C. And what this allows us to achieve is instances

572
00:55:35,320 --> 00:55:43,320
like that of paired translation in the case of image synthesis, where now instead of a single input

573
00:55:43,960 --> 00:55:50,040
as training data for our generator, we have pairs of inputs. So for example here we consider

574
00:55:50,040 --> 00:55:56,680
both a driving scene and a corresponding segmentation map to that driving scene. And the discriminator

575
00:55:56,680 --> 00:56:04,440
can in turn be trained to classify fake and real pairs of data. And again the generator is going

576
00:56:04,440 --> 00:56:13,080
to be trained to try to fool the discriminator. Example applications of this idea are

577
00:56:14,280 --> 00:56:20,920
seen as follows where we can now go from an input of a semantic segmentation map to generate a

578
00:56:20,920 --> 00:56:29,000
synthetic street scene mapping according to that segmentation. Or we can go from an aerial view

579
00:56:29,000 --> 00:56:35,160
from a satellite image to a street map view or from particular labels of an architectural

580
00:56:35,160 --> 00:56:41,400
building to a synthetic architectural facade or day to night, black and white to color,

581
00:56:41,480 --> 00:56:46,920
edges to photos, different instances of paired translation that are achieved by conditioning

582
00:56:46,920 --> 00:56:53,880
on particular labels. So another example which I think is really cool and interesting is

583
00:56:53,880 --> 00:57:01,720
translating from Google Street View to a satellite view and vice versa. And we can also achieve this

584
00:57:01,720 --> 00:57:08,120
dynamically. So for example in coloring given an edge input, the network can be trained to

585
00:57:08,120 --> 00:57:14,680
actually synthetically color in the artwork that is resulting from this particular edge sketch.

586
00:57:17,240 --> 00:57:22,760
Another idea instead of paired translation is that of unpaired image to image translation.

587
00:57:22,760 --> 00:57:28,440
And this is going to be achieved by a network architecture called CycleGAN where the model

588
00:57:28,440 --> 00:57:35,640
is taking as input images from one domain and is able to learn a mapping that translates to

589
00:57:35,720 --> 00:57:40,200
another domain without having a paired corresponding image in that other domain.

590
00:57:41,240 --> 00:57:48,120
So the idea here is to transfer the style and the distribution from one domain to another.

591
00:57:48,840 --> 00:57:55,640
And this is achieved by introducing the cyclic relationship and a cyclic loss function where

592
00:57:55,640 --> 00:58:02,520
we can go back and forth between a domain x and a domain y. And in this system there are actually

593
00:58:02,520 --> 00:58:06,920
two generators and two discriminators that are going to be trained on their respective

594
00:58:06,920 --> 00:58:14,200
generation and discrimination tasks. In this example the CycleGAN has been trained to try to

595
00:58:14,200 --> 00:58:20,520
translate from the domain of horses to the domain of zebras. And hopefully you can appreciate that

596
00:58:20,520 --> 00:58:27,480
in this example there's a transformation of the skin of the horse from brown to a zebra-like skin

597
00:58:27,480 --> 00:58:32,440
in stripes. And beyond this there's also a transformation of the surrounding area

598
00:58:32,440 --> 00:58:36,920
from green grass to something that's more brown in the case of the zebra.

599
00:58:38,600 --> 00:58:43,080
And I think to get an intuition about how this CycleGAN transformation is going

600
00:58:43,720 --> 00:58:51,000
is working. Let's go back to the idea that conventional GANs are moving from a distribution

601
00:58:51,000 --> 00:58:57,480
of Gaussian noise to some target data manifold. With CycleGANs the goal is to go from a particular

602
00:58:57,480 --> 00:59:05,080
data manifold x to another data manifold y. And in both cases and I think the underlying

603
00:59:05,080 --> 00:59:10,680
concept that makes GANs so powerful is that they function as very very effective distribution

604
00:59:10,680 --> 00:59:15,320
transformers and it can achieve these distribution transformations.

605
00:59:15,480 --> 00:59:23,720
Finally I'd like to consider one additional application that you may be familiar with

606
00:59:24,440 --> 00:59:30,760
of using CycleGANs and that's to transform speech and to actually use this CycleGAN technique to

607
00:59:30,760 --> 00:59:36,280
synthesize speech in someone else's voice. And the way this is done is by taking a bunch of audio

608
00:59:36,280 --> 00:59:42,840
recordings in one voice and audio recordings in another voice and converting those audio waveforms

609
00:59:42,840 --> 00:59:51,080
into an image representation which was called a spectrogram. We can then train a CycleGAN to

610
00:59:51,080 --> 00:59:58,840
operate on these spectrogram images to transform representations from voice A to make them appear

611
00:59:58,840 --> 01:00:05,480
like they appear that they are from another voice, voice B. And this is exactly how we did the

612
01:00:05,480 --> 01:00:11,800
speech transformation for the synthesis of Obama's voice in the demonstration that Alexander gave in

613
01:00:11,800 --> 01:00:18,680
the first lecture. So to inspect this further let's compare side by side the original audio

614
01:00:18,680 --> 01:00:24,840
from Alexander as well as the synthesized version in Obama's voice that was generated using a CycleGAN.

615
01:00:26,760 --> 01:00:34,120
Hi everybody and welcome to MIT 6S191, you know, facial introductory course

616
01:00:34,120 --> 01:00:44,120
on speech learning here at MIT. So notice that the spectrogram that results for Obama's voice

617
01:00:44,680 --> 01:00:50,760
is actually generated by an operation on Alexander's voice and effectively learning a domain

618
01:00:50,760 --> 01:00:56,680
transformation from Obama domain onto the domain of Alexander domain and the end result is that we

619
01:00:56,680 --> 01:01:03,240
create and synthesize something that's more Obama-like. All right so to summarize hopefully

620
01:01:03,240 --> 01:01:08,360
over the course of this lecture you built up understanding of generative modeling and classes

621
01:01:08,360 --> 01:01:14,920
of generative models that are particularly powerful in enabling probabilistic density estimation

622
01:01:14,920 --> 01:01:23,000
as well as sample generation. And with that I'd like to close the lecture and introduce you to

623
01:01:23,000 --> 01:01:29,400
the remainder of today's course which is going to focus on our second lab on computer vision,

624
01:01:30,280 --> 01:01:35,720
specifically exploring this question of debiasing in facial detection systems

625
01:01:35,720 --> 01:01:41,240
and using variational autoencoders to actually achieve an approach for automatic

626
01:01:41,240 --> 01:01:46,600
debiasing of classification systems. So I encourage you to come to the class gather town

627
01:01:46,600 --> 01:01:53,800
to have your questions on the lab's answered and to discuss further with any of us. Thank you.

