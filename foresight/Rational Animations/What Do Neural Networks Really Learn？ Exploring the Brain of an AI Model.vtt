WEBVTT

00:00.000 --> 00:06.880
In 2018, researchers trained an AI to find out if people were at risk of heart conditions

00:06.880 --> 00:09.160
based on pictures of their eyes.

00:09.160 --> 00:15.160
And somehow, the AI also learned to tell people's biological sex with incredibly high accuracy.

00:15.160 --> 00:16.160
How?

00:16.160 --> 00:18.000
We're not entirely sure.

00:18.000 --> 00:22.920
The crazy thing about deep learning is that you can give an AI a set of inputs and outputs,

00:22.920 --> 00:26.960
and it will work out for itself what the relationship between them is.

00:26.960 --> 00:32.960
We didn't teach AIs how to play chess, go, and Atari games by showing them human experts.

00:32.960 --> 00:35.880
We taught them how to work it out for themselves.

00:35.880 --> 00:40.560
And the issue is, now they have worked it out for themselves, and we don't know what

00:40.560 --> 00:43.200
it is they worked out.

00:43.200 --> 00:46.080
Current state-of-the-art AIs are huge.

00:46.080 --> 00:52.320
Meta's largest Llama 2 model uses 70 billion parameters spread across 80 layers, all doing

00:52.320 --> 00:53.840
different things.

00:53.840 --> 00:57.680
It's deep learning models like these, which are being used for everything from hiring

00:57.680 --> 01:03.400
decisions to healthcare and criminal justice, to what YouTube videos get recommended.

01:03.400 --> 01:07.440
Many experts believe that these models might even one day pose a risk to the continued

01:07.440 --> 01:09.360
existence of humanity.

01:09.360 --> 01:13.960
So as these automated processes become more widespread and significant, it will really

01:13.960 --> 01:17.760
matter that we understand how these models make choices.

01:17.760 --> 01:23.040
The good news is, we've got a bit of experience uncovering the mysteries of the universe.

01:23.040 --> 01:28.080
We know that humans are made up of trillions of cells, and by investigating those individual

01:28.080 --> 01:31.960
cells, we've made huge advances in medicine and genetics.

01:31.960 --> 01:36.000
And learning the properties of the atoms, which make up objects, has allowed us to develop

01:36.000 --> 01:40.360
modern material science and high-precision technology like computers.

01:40.360 --> 01:45.080
If you want to understand a complex system with billions of moving parts, sometimes you

01:45.080 --> 01:46.600
have to zoom in.

01:46.600 --> 01:51.640
That's exactly what Chris Ola and his team did, starting in 2015.

01:51.640 --> 01:56.440
They focused on small groups of neurons inside image models, and they were able to find distinct

01:56.440 --> 02:03.040
parts responsible for detecting everything from curves and circles to dog heads and calves.

02:03.040 --> 02:08.160
In this video, we'll briefly explain how convolutional neural networks work, visualize

02:08.160 --> 02:13.600
what individual neurons, the basic building blocks of the neural network, are doing, look

02:13.600 --> 02:19.680
at how neurons combine into circuits to perform tasks, and explore why interpreting networks

02:19.680 --> 02:20.680
is so hard.

02:21.200 --> 02:24.880
There will also be lots of pictures of dogs, like this one.

02:24.880 --> 02:26.360
Let's get going.

02:26.360 --> 02:30.880
We'll start with a brief explanation of how convolutional neural networks are built.

02:30.880 --> 02:34.040
Here's a network that's trained to label images.

02:34.040 --> 02:38.440
An input image comes in on the left, and it flows along through the layers until we get

02:38.440 --> 02:40.120
an output on the right.

02:40.120 --> 02:43.880
The models attempt to classify the image into one of the categories.

02:43.880 --> 02:48.560
This particular model is called Inception V1, and the images it's learned to classify

02:48.560 --> 02:51.680
are from a massive collection called ImageNet.

02:51.680 --> 02:58.480
ImageNet has a thousand different categories of images, like Sandal and Saxophone and Sarong,

02:58.480 --> 03:02.320
which if you don't know is a kind of printed fabric you wrap around your waist.

03:02.320 --> 03:07.960
It also has more than 100 kinds of dog, including 22 types of terrier, so we'll be relevant

03:07.960 --> 03:08.960
later.

03:08.960 --> 03:13.720
But anyway, the model is somehow taking an image and putting out its best guess for which

03:13.720 --> 03:16.080
category the image comes from.

03:16.080 --> 03:17.080
How?

03:17.480 --> 03:21.400
Well, we know exactly what the neurons here on the left are doing.

03:21.400 --> 03:25.600
They're activated by the pixels of the image, and we know exactly what the neurons there

03:25.600 --> 03:27.160
on the right are doing.

03:27.160 --> 03:31.680
Their activations are the model's prediction for each of the possible classifications.

03:31.680 --> 03:34.520
And all these activations are just numbers.

03:34.520 --> 03:36.640
What's happening in between?

03:36.640 --> 03:39.880
The key element is the convolutional layer.

03:39.880 --> 03:44.440
Imagine we take our first layer of input cells, a grid of pixel activations.

03:44.440 --> 03:50.160
What we do is run a little filter across it, and the filter has its own grid of weights.

03:50.160 --> 03:54.000
We multiply the weights of the filter with the activations of the neurons, we add up

03:54.000 --> 03:57.360
the results, and we get a single new value.

03:57.360 --> 04:02.360
So maybe our grid of weights looks like this, a bunch of positive values at the top and

04:02.360 --> 04:04.400
negative values at the bottom.

04:04.400 --> 04:09.200
Then the overall result of the filter is high on parts of the picture where the top is brighter

04:09.200 --> 04:10.200
than the bottom.

04:10.680 --> 04:14.000
It's like it's filtering for a certain kind of edge.

04:14.000 --> 04:18.760
And when we slide this filter across the entire grid of pixel activations, we get a new grid

04:18.760 --> 04:19.760
of activations.

04:19.760 --> 04:24.540
But, instead of representing the input image as is, now it's detecting a certain kind

04:24.540 --> 04:27.880
of edge wherever it appears in the original image.

04:27.880 --> 04:32.960
We also have a bias term, which we just add after applying the filter, because sometimes

04:32.960 --> 04:36.320
we want it to be biased towards a high or low value.

04:36.520 --> 04:40.560
Finally, if the result is negative, we round it up to zero.

04:40.560 --> 04:42.240
That's basically it.

04:42.240 --> 04:46.840
And we have loads of different filters producing different new grids of activations, which we

04:46.840 --> 04:48.960
call channels.

04:48.960 --> 04:54.160
These channels together form a new layer, and we run more filters across them, and those

04:54.160 --> 04:57.360
feed into another layer, and then another layer.

04:57.360 --> 05:02.680
Each layer usually detects more and more abstract properties of the image, until we get to the

05:02.680 --> 05:06.880
last part of the network, which is structured like a traditional fully connected neural

05:06.880 --> 05:07.880
network.

05:07.880 --> 05:12.680
Somehow, the neurons at the end of the network tell you if you're looking at a terrier or

05:12.680 --> 05:14.200
a saxophone.

05:14.200 --> 05:15.960
That's pretty crazy.

05:15.960 --> 05:20.960
If you're wondering how we decide on the weights for the filters, well, we don't.

05:20.960 --> 05:24.480
That's the bit the model works out for itself during training.

05:24.480 --> 05:28.680
So the question is, why does it pick those specific values?

05:28.680 --> 05:32.680
How do we find out what these channels in the middle are representing precisely?

05:32.680 --> 05:35.720
Well, let's pick one deep in the middle.

05:35.720 --> 05:38.520
What's this channel doing?

05:38.520 --> 05:43.080
Maybe one way we can find out is to ask, what does it care about?

05:43.080 --> 05:47.520
Let's take all our images and feed them through the model, and check which images give it

05:47.520 --> 05:49.640
the highest activation.

05:49.640 --> 05:52.120
And look, it's all pictures of dogs.

05:52.120 --> 05:53.620
Lots of dogs.

05:53.620 --> 05:55.680
Maybe this is a dog detecting channel.

05:55.680 --> 05:58.120
Well, it's hard to be sure.

05:58.120 --> 06:03.880
We know that something about dogs is activating it, but we don't know what exactly.

06:03.880 --> 06:08.760
If we wanted to be more sure, we could try to directly optimize an input to activate

06:08.760 --> 06:10.200
a neuron in this channel.

06:10.200 --> 06:14.600
Actually, in pretty much the same way we optimize the network to be accurate.

06:14.600 --> 06:19.920
So we feed the network a totally random bunch of pixels, and we see how much that activates

06:19.920 --> 06:22.160
our maybe dog neuron.

06:22.160 --> 06:26.840
Then we change the input a bit so that it activates the neuron a bit more, and we do

06:26.840 --> 06:31.240
this more and more until the neuron is as activated as possible.

06:31.240 --> 06:35.560
We can also do this with a whole channel at once, a whole grid of neurons doing the same

06:35.560 --> 06:40.400
operation on different parts of the image, by trying to get the highest average activation

06:40.400 --> 06:41.800
for the neurons.

06:41.800 --> 06:43.960
But let's stick with just one neuron for now.

06:43.960 --> 06:48.200
OK, so unfortunately doing just this doesn't work.

06:48.200 --> 06:52.800
What you get is some kind of weird, extremely cursed pile of static.

06:52.800 --> 06:55.480
We don't know why it's that specifically.

06:55.480 --> 06:58.080
Like we said, there's a lot we don't know about neural networks.

06:58.080 --> 07:01.160
But just activating the neuron isn't enough.

07:01.160 --> 07:04.640
So let's add some extra conditions for our optimization process.

07:04.640 --> 07:09.200
What we want is something that wouldn't rule out a sensible image, but would decrease the

07:09.200 --> 07:14.080
chance of getting one of the weird, cursed static piles, so that when we run our optimization

07:14.080 --> 07:19.320
process, the top scorer is more likely to be like a sensible image.

07:19.320 --> 07:23.680
For instance, maybe we can take the input we're optimizing for, and jitter it around

07:23.680 --> 07:28.160
a bit each step, rotate it slightly, scale it up and down a bit.

07:28.160 --> 07:30.640
For a normal picture, this doesn't change much.

07:30.640 --> 07:35.200
A dog head still looks like a dog head, but it seems to really mess with the walls of

07:35.200 --> 07:37.360
static according to the neuron.

07:37.360 --> 07:40.960
The technical term here is transformation robustness.

07:40.960 --> 07:44.240
The image should be robust to being transformed slightly.

07:44.240 --> 07:47.400
And now, images start to take shape.

07:47.400 --> 07:51.920
In the original circuits piece, they also did something called, and bear with me for

07:51.920 --> 07:57.640
a second, preconditioning to optimize in a color-decorrelated Fourier space.

07:57.640 --> 07:58.840
So what does that mean?

07:58.840 --> 08:04.440
Well, in audio processing, a Fourier transform is a way to take something like a chord, or

08:04.440 --> 08:08.520
any other sound, and break it down into the constituent tones.

08:08.520 --> 08:13.000
So instead of splitting a sound up into the amplitude over time, you break it down into

08:13.000 --> 08:16.440
what simple notes make it up.

08:16.440 --> 08:19.600
You can do pretty much exactly the same thing with images.

08:19.600 --> 08:24.400
Instead of thinking about the picture pixel by pixel, you layer a bunch of smooth waves

08:24.400 --> 08:26.040
on top of each other.

08:26.040 --> 08:30.680
So we have the optimizer look for adjustments to the input image that would lead to an increased

08:30.680 --> 08:32.360
neuron activation.

08:32.360 --> 08:36.800
And we let the optimizer work with this wave representation of the input image, instead

08:36.800 --> 08:38.640
of changing it directly.

08:38.640 --> 08:43.880
Turns out that this way, the adjustments are more smooth and less like static noise.

08:43.880 --> 08:47.800
When the optimization is finished, we translate it back to a normal image.

08:47.800 --> 08:51.760
And the resulting images actually look kind of reasonable.

08:51.760 --> 08:55.520
But now our dog detector seems like it isn't actually detecting dogs.

08:55.520 --> 08:59.080
It looks like it's really detecting their snouts.

08:59.080 --> 09:04.040
And the way to fit the most snoutiness into the image is to fit another snout inside the

09:04.040 --> 09:05.040
snout.

09:05.040 --> 09:06.040
Weird, right?

09:06.040 --> 09:07.800
Here's something for you to ponder.

09:07.800 --> 09:11.720
Why is it so clear in the middle, and so fuzzy on the edges?

09:11.720 --> 09:16.320
Well, that's because we're only focusing on one neuron, and that neuron is only looking

09:16.320 --> 09:18.160
at part of the picture.

09:18.160 --> 09:22.440
We'll see later on that trying to maximize the whole channel makes the whole image more

09:22.440 --> 09:23.440
crisp.

09:23.440 --> 09:25.800
But anyway, back to snoutiness.

09:25.800 --> 09:28.400
This is sort of how it is with interpretability.

09:28.400 --> 09:31.480
It's very hard to know what you're actually looking at.

09:31.480 --> 09:35.840
The model is just learning whatever fits the data, and sometimes the thing that works is

09:35.840 --> 09:37.360
a bit surprising.

09:37.360 --> 09:42.640
In that sense, this kind of work is less like formal mathematical proofs, and more like natural

09:42.640 --> 09:43.720
science.

09:43.720 --> 09:48.560
You experiment, you make predictions, and you test them, and slowly you become more

09:48.560 --> 09:49.560
confident.

09:49.560 --> 09:53.040
But this is still just one neuron on its own.

09:53.040 --> 09:55.120
One tiny little part.

09:55.120 --> 09:59.640
How do we get from that to understanding the whole massive, messy network?

09:59.640 --> 10:02.240
Well, we can zoom out a bit.

10:02.240 --> 10:06.480
Let's try doing that with some neurons we understand really, really well.

10:06.480 --> 10:10.200
For instance, let's go up a bit to this layer, mixed 3B.

10:10.200 --> 10:15.080
It has a bunch of neurons, which seem like they're detecting curves with a radius of

10:15.080 --> 10:19.800
about 60 pixels, all in slightly different orientations.

10:19.800 --> 10:24.760
Curve detector neurons, by the way, seem to show up in basically all image classifiers.

10:24.760 --> 10:28.680
They're somehow a very natural thing for models to learn.

10:28.680 --> 10:31.000
We can use the tricks we already used.

10:31.000 --> 10:36.000
The neurons get activated by pictures of curves, and the feature visualization generates pictures

10:36.000 --> 10:37.000
of curves.

10:37.000 --> 10:41.520
Also, there are some tricks we can use for a really simple feature, like a curve, that

10:41.520 --> 10:43.680
don't work for a dog detector.

10:43.680 --> 10:48.100
We can actually read the algorithm in the neuron, and check that it looks like a pixel

10:48.100 --> 10:50.240
by pixel curve detector.

10:50.240 --> 10:54.720
We can even write our own pixel by pixel curve detector to replace it, and check if anything

10:54.720 --> 10:55.720
breaks.

10:55.720 --> 10:59.560
So it really seems like these neurons are curve detectors.

10:59.560 --> 11:03.720
But there's loads of them, all detecting curves in different directions.

11:03.720 --> 11:06.600
And that gives us some new options for investigation.

11:06.600 --> 11:11.360
Like, what if we take a picture of a curve that activates this curve detector here, and

11:11.360 --> 11:12.840
slowly rotate it?

11:12.840 --> 11:17.560
Well, it turns out that, as we rotate it, its activation on this curve detector goes

11:17.560 --> 11:21.920
down, and then the activation on this other one goes up.

11:21.920 --> 11:26.680
So if we arrange them in order, it turns out that these curve detectors are actually picking

11:26.680 --> 11:29.920
up on every possible orientation between them.

11:29.920 --> 11:32.440
They're not just one-off neurons.

11:32.760 --> 11:36.880
They've been developed as part of a circuit, and they're used together.

11:36.880 --> 11:41.400
Remember that each neuron depends on a small grid of neurons in the previous layer.

11:41.400 --> 11:46.080
And what we find is, for instance, a channel that's activated by this kind of curve in

11:46.080 --> 11:51.640
the top left, and also inhibited by that kind of curve in the bottom right, and also activated

11:51.640 --> 11:56.000
by this kind of curve in the top right, and inhibited by it in the bottom left, and so

11:56.000 --> 11:57.000
on.

11:57.000 --> 12:01.360
So all our channels on this layer, which are checking for different parts of curves in

12:01.360 --> 12:06.040
different parts of the image, get combined into a channel on the next layer, which is

12:06.040 --> 12:08.600
looking for entire circles.

12:08.600 --> 12:13.360
And there are also other channels for more complex features, like spirals.

12:13.360 --> 12:17.040
And it seems like this is also true of neurons in later layers.

12:17.040 --> 12:21.000
Remember earlier how about a tenth of the labels are different kinds of dog?

12:21.000 --> 12:24.240
Well, here's how the model recognises dogs.

12:24.240 --> 12:30.160
Here's a pair of neurons in the layer Mixed4A, which are activated by dog heads facing left

12:30.280 --> 12:32.360
and right, respectively.

12:32.360 --> 12:35.400
And they each feed into a general dog head detector.

12:35.400 --> 12:40.680
But also, there's another pair of neurons, which look for combined dog heads and necks,

12:40.680 --> 12:42.680
again facing left or right.

12:42.680 --> 12:47.360
And we can see from the convolutional layer that the model wants left-facing dog heads

12:47.360 --> 12:49.840
to be to the left of necks.

12:49.840 --> 12:55.080
And the left dog head neuron activates the left dog head and neck neuron, but it actually

12:55.080 --> 12:58.520
inhibits the right dog head and neck neuron.

12:58.520 --> 13:02.400
Just like the model is trying to make sure that the neck and the head are the correct

13:02.400 --> 13:08.840
way around, then both the neck and head neurons and the general dog head neuron all feed into

13:08.840 --> 13:11.440
a general dog neck and head neuron.

13:11.440 --> 13:13.640
And there are loads of patterns like these.

13:13.640 --> 13:19.320
For example, we find a neuron that detects car wheels and a neuron that detects car windows

13:19.320 --> 13:21.920
and a neuron that detects car bodies.

13:21.920 --> 13:26.480
And then we find another neuron that lights up for images with windows at the top and

13:26.480 --> 13:29.920
wheels at the bottom with a car body in the middle.

13:29.920 --> 13:32.800
Now you've got a general-purpose car detector.

13:32.800 --> 13:37.640
In fact, you have an entire channel looking for cars in different parts of the image.

13:37.640 --> 13:39.200
Seems easy, right?

13:39.200 --> 13:40.600
Almost too easy.

13:40.600 --> 13:44.280
Well, don't worry, because it turns out it's actually not that simple.

13:44.280 --> 13:48.800
These tricks with feature visualization and high-scoring images do tell us what a neuron

13:48.800 --> 13:54.040
is doing, but they don't tell us if it's everything the neuron is doing.

13:54.040 --> 13:59.720
High semanticity is the technical term for when a neuron or a channel is actually tracking

13:59.720 --> 14:01.800
more than one feature at once.

14:01.800 --> 14:07.120
See, the network needs to learn to recognize a thousand different categories, and the categories

14:07.120 --> 14:09.000
might be quite complicated.

14:09.000 --> 14:13.600
So sometimes the model learns how to cram more than one feature into a neuron.

14:13.600 --> 14:18.720
For instance, here's a channel which is highly activated by pictures of cat faces and fox

14:18.720 --> 14:21.560
faces and also cars.

14:21.560 --> 14:26.120
If we do our feature visualization but modify it to produce several pictures, which all

14:26.120 --> 14:31.280
activate the channel a lot, while being maximally different from each other, we get some weird

14:31.280 --> 14:35.440
visualizations of cats and foxes and cars.

14:35.440 --> 14:36.920
Why cars?

14:36.920 --> 14:38.240
We don't know.

14:38.240 --> 14:43.040
It seems like sometimes polysemanticity occurs because features are very different, so the

14:43.040 --> 14:46.240
model is not likely to see them both in the same image.

14:46.240 --> 14:50.280
But as I say, we really are not sure.

14:50.280 --> 14:54.880
Polysemanticity appears in all kinds of models, including language models, and it really complicates

14:54.880 --> 14:57.320
the task of interpreting a neuron.

14:57.320 --> 15:01.960
Even if we know that a neuron is doing something, it's hard to know what else it might be doing.

15:01.960 --> 15:06.120
There's been some really interesting work on finding out when and how models become

15:06.120 --> 15:10.600
polysemantic, as well as some more recent work on how to discover patterns of neuron

15:10.600 --> 15:13.160
activation which respond to features.

15:13.160 --> 15:16.760
You can check out links to both of these in the video description.

15:16.760 --> 15:18.600
So where does that leave us?

15:18.600 --> 15:23.080
Well, we've talked about how it's possible to at least begin to interpret the individual

15:23.080 --> 15:28.000
neurons of an image classifier by comparing them against dataset examples and generating

15:28.000 --> 15:29.960
inputs that activate them.

15:29.960 --> 15:33.920
We've talked about how these neurons form into circuits, which explain more complex

15:33.920 --> 15:34.920
behavior.

15:34.920 --> 15:40.080
And we've talked about polysemanticity, the fact that sometimes a neuron is tracking multiple

15:40.080 --> 15:41.840
distinct features.

15:41.840 --> 15:47.000
The original collection of articles on circuits was published in May of 2020, before even

15:47.000 --> 15:49.360
GPT-3 had been released.

15:49.360 --> 15:51.840
So the field has developed a lot since then.

15:51.840 --> 15:56.560
The same kind of work we discussed here is being done on language models to try to understand

15:56.560 --> 16:02.120
how they can write poetry and translate things into French and whatever else you might want.

16:02.120 --> 16:08.200
OpenAI actually has a project to use GPT-4 to interpret all the neurons in GPT-2.

16:08.200 --> 16:12.560
We've also started doing some more work on how the models learn, like at what point they

16:12.560 --> 16:17.480
start to go from memorizing patterns to actually generalizing.

16:17.480 --> 16:21.320
And we've made some tentative attempts to actually extract information directly from

16:21.320 --> 16:24.560
the activations of a model rather than its outputs.

16:24.560 --> 16:28.760
We can ask a language model a question and then read off what it thinks is true from

16:28.760 --> 16:30.000
the inside.

16:30.000 --> 16:34.280
And this is often more accurate than the answer the language model actually outputs.

16:34.280 --> 16:38.320
Of course, this is only possible because in some sense language models aren't telling

16:38.320 --> 16:39.840
us what they know.

16:39.840 --> 16:41.720
Make of that what you will.

16:41.720 --> 16:48.240
This kind of work is called mechanistic interpretability, and it's very hands-on with a lot of experimenting.

16:48.240 --> 16:52.120
You might discuss more details about recent developments in future videos.

16:52.120 --> 16:56.440
In the meantime, if you're curious to find out more about how mechanistic interpretability

16:56.440 --> 17:01.600
works or try it out yourself, you can check out this tutorial, which we've also included

17:01.600 --> 17:04.040
a link to in the video description.

17:04.040 --> 17:09.440
As we become increasingly reliant on automated systems, mechanistic interpretability might

17:09.440 --> 17:13.520
be a key tool for understanding the why behind AI decisions.

