start	end	text
0	6880	In 2018, researchers trained an AI to find out if people were at risk of heart conditions
6880	9160	based on pictures of their eyes.
9160	15160	And somehow, the AI also learned to tell people's biological sex with incredibly high accuracy.
15160	16160	How?
16160	18000	We're not entirely sure.
18000	22920	The crazy thing about deep learning is that you can give an AI a set of inputs and outputs,
22920	26960	and it will work out for itself what the relationship between them is.
26960	32960	We didn't teach AIs how to play chess, go, and Atari games by showing them human experts.
32960	35880	We taught them how to work it out for themselves.
35880	40560	And the issue is, now they have worked it out for themselves, and we don't know what
40560	43200	it is they worked out.
43200	46080	Current state-of-the-art AIs are huge.
46080	52320	Meta's largest Llama 2 model uses 70 billion parameters spread across 80 layers, all doing
52320	53840	different things.
53840	57680	It's deep learning models like these, which are being used for everything from hiring
57680	63400	decisions to healthcare and criminal justice, to what YouTube videos get recommended.
63400	67440	Many experts believe that these models might even one day pose a risk to the continued
67440	69360	existence of humanity.
69360	73960	So as these automated processes become more widespread and significant, it will really
73960	77760	matter that we understand how these models make choices.
77760	83040	The good news is, we've got a bit of experience uncovering the mysteries of the universe.
83040	88080	We know that humans are made up of trillions of cells, and by investigating those individual
88080	91960	cells, we've made huge advances in medicine and genetics.
91960	96000	And learning the properties of the atoms, which make up objects, has allowed us to develop
96000	100360	modern material science and high-precision technology like computers.
100360	105080	If you want to understand a complex system with billions of moving parts, sometimes you
105080	106600	have to zoom in.
106600	111640	That's exactly what Chris Ola and his team did, starting in 2015.
111640	116440	They focused on small groups of neurons inside image models, and they were able to find distinct
116440	123040	parts responsible for detecting everything from curves and circles to dog heads and calves.
123040	128160	In this video, we'll briefly explain how convolutional neural networks work, visualize
128160	133600	what individual neurons, the basic building blocks of the neural network, are doing, look
133600	139680	at how neurons combine into circuits to perform tasks, and explore why interpreting networks
139680	140680	is so hard.
141200	144880	There will also be lots of pictures of dogs, like this one.
144880	146360	Let's get going.
146360	150880	We'll start with a brief explanation of how convolutional neural networks are built.
150880	154040	Here's a network that's trained to label images.
154040	158440	An input image comes in on the left, and it flows along through the layers until we get
158440	160120	an output on the right.
160120	163880	The models attempt to classify the image into one of the categories.
163880	168560	This particular model is called Inception V1, and the images it's learned to classify
168560	171680	are from a massive collection called ImageNet.
171680	178480	ImageNet has a thousand different categories of images, like Sandal and Saxophone and Sarong,
178480	182320	which if you don't know is a kind of printed fabric you wrap around your waist.
182320	187960	It also has more than 100 kinds of dog, including 22 types of terrier, so we'll be relevant
187960	188960	later.
188960	193720	But anyway, the model is somehow taking an image and putting out its best guess for which
193720	196080	category the image comes from.
196080	197080	How?
197480	201400	Well, we know exactly what the neurons here on the left are doing.
201400	205600	They're activated by the pixels of the image, and we know exactly what the neurons there
205600	207160	on the right are doing.
207160	211680	Their activations are the model's prediction for each of the possible classifications.
211680	214520	And all these activations are just numbers.
214520	216640	What's happening in between?
216640	219880	The key element is the convolutional layer.
219880	224440	Imagine we take our first layer of input cells, a grid of pixel activations.
224440	230160	What we do is run a little filter across it, and the filter has its own grid of weights.
230160	234000	We multiply the weights of the filter with the activations of the neurons, we add up
234000	237360	the results, and we get a single new value.
237360	242360	So maybe our grid of weights looks like this, a bunch of positive values at the top and
242360	244400	negative values at the bottom.
244400	249200	Then the overall result of the filter is high on parts of the picture where the top is brighter
249200	250200	than the bottom.
250680	254000	It's like it's filtering for a certain kind of edge.
254000	258760	And when we slide this filter across the entire grid of pixel activations, we get a new grid
258760	259760	of activations.
259760	264540	But, instead of representing the input image as is, now it's detecting a certain kind
264540	267880	of edge wherever it appears in the original image.
267880	272960	We also have a bias term, which we just add after applying the filter, because sometimes
272960	276320	we want it to be biased towards a high or low value.
276520	280560	Finally, if the result is negative, we round it up to zero.
280560	282240	That's basically it.
282240	286840	And we have loads of different filters producing different new grids of activations, which we
286840	288960	call channels.
288960	294160	These channels together form a new layer, and we run more filters across them, and those
294160	297360	feed into another layer, and then another layer.
297360	302680	Each layer usually detects more and more abstract properties of the image, until we get to the
302680	306880	last part of the network, which is structured like a traditional fully connected neural
306880	307880	network.
307880	312680	Somehow, the neurons at the end of the network tell you if you're looking at a terrier or
312680	314200	a saxophone.
314200	315960	That's pretty crazy.
315960	320960	If you're wondering how we decide on the weights for the filters, well, we don't.
320960	324480	That's the bit the model works out for itself during training.
324480	328680	So the question is, why does it pick those specific values?
328680	332680	How do we find out what these channels in the middle are representing precisely?
332680	335720	Well, let's pick one deep in the middle.
335720	338520	What's this channel doing?
338520	343080	Maybe one way we can find out is to ask, what does it care about?
343080	347520	Let's take all our images and feed them through the model, and check which images give it
347520	349640	the highest activation.
349640	352120	And look, it's all pictures of dogs.
352120	353620	Lots of dogs.
353620	355680	Maybe this is a dog detecting channel.
355680	358120	Well, it's hard to be sure.
358120	363880	We know that something about dogs is activating it, but we don't know what exactly.
363880	368760	If we wanted to be more sure, we could try to directly optimize an input to activate
368760	370200	a neuron in this channel.
370200	374600	Actually, in pretty much the same way we optimize the network to be accurate.
374600	379920	So we feed the network a totally random bunch of pixels, and we see how much that activates
379920	382160	our maybe dog neuron.
382160	386840	Then we change the input a bit so that it activates the neuron a bit more, and we do
386840	391240	this more and more until the neuron is as activated as possible.
391240	395560	We can also do this with a whole channel at once, a whole grid of neurons doing the same
395560	400400	operation on different parts of the image, by trying to get the highest average activation
400400	401800	for the neurons.
401800	403960	But let's stick with just one neuron for now.
403960	408200	OK, so unfortunately doing just this doesn't work.
408200	412800	What you get is some kind of weird, extremely cursed pile of static.
412800	415480	We don't know why it's that specifically.
415480	418080	Like we said, there's a lot we don't know about neural networks.
418080	421160	But just activating the neuron isn't enough.
421160	424640	So let's add some extra conditions for our optimization process.
424640	429200	What we want is something that wouldn't rule out a sensible image, but would decrease the
429200	434080	chance of getting one of the weird, cursed static piles, so that when we run our optimization
434080	439320	process, the top scorer is more likely to be like a sensible image.
439320	443680	For instance, maybe we can take the input we're optimizing for, and jitter it around
443680	448160	a bit each step, rotate it slightly, scale it up and down a bit.
448160	450640	For a normal picture, this doesn't change much.
450640	455200	A dog head still looks like a dog head, but it seems to really mess with the walls of
455200	457360	static according to the neuron.
457360	460960	The technical term here is transformation robustness.
460960	464240	The image should be robust to being transformed slightly.
464240	467400	And now, images start to take shape.
467400	471920	In the original circuits piece, they also did something called, and bear with me for
471920	477640	a second, preconditioning to optimize in a color-decorrelated Fourier space.
477640	478840	So what does that mean?
478840	484440	Well, in audio processing, a Fourier transform is a way to take something like a chord, or
484440	488520	any other sound, and break it down into the constituent tones.
488520	493000	So instead of splitting a sound up into the amplitude over time, you break it down into
493000	496440	what simple notes make it up.
496440	499600	You can do pretty much exactly the same thing with images.
499600	504400	Instead of thinking about the picture pixel by pixel, you layer a bunch of smooth waves
504400	506040	on top of each other.
506040	510680	So we have the optimizer look for adjustments to the input image that would lead to an increased
510680	512360	neuron activation.
512360	516800	And we let the optimizer work with this wave representation of the input image, instead
516800	518640	of changing it directly.
518640	523880	Turns out that this way, the adjustments are more smooth and less like static noise.
523880	527800	When the optimization is finished, we translate it back to a normal image.
527800	531760	And the resulting images actually look kind of reasonable.
531760	535520	But now our dog detector seems like it isn't actually detecting dogs.
535520	539080	It looks like it's really detecting their snouts.
539080	544040	And the way to fit the most snoutiness into the image is to fit another snout inside the
544040	545040	snout.
545040	546040	Weird, right?
546040	547800	Here's something for you to ponder.
547800	551720	Why is it so clear in the middle, and so fuzzy on the edges?
551720	556320	Well, that's because we're only focusing on one neuron, and that neuron is only looking
556320	558160	at part of the picture.
558160	562440	We'll see later on that trying to maximize the whole channel makes the whole image more
562440	563440	crisp.
563440	565800	But anyway, back to snoutiness.
565800	568400	This is sort of how it is with interpretability.
568400	571480	It's very hard to know what you're actually looking at.
571480	575840	The model is just learning whatever fits the data, and sometimes the thing that works is
575840	577360	a bit surprising.
577360	582640	In that sense, this kind of work is less like formal mathematical proofs, and more like natural
582640	583720	science.
583720	588560	You experiment, you make predictions, and you test them, and slowly you become more
588560	589560	confident.
589560	593040	But this is still just one neuron on its own.
593040	595120	One tiny little part.
595120	599640	How do we get from that to understanding the whole massive, messy network?
599640	602240	Well, we can zoom out a bit.
602240	606480	Let's try doing that with some neurons we understand really, really well.
606480	610200	For instance, let's go up a bit to this layer, mixed 3B.
610200	615080	It has a bunch of neurons, which seem like they're detecting curves with a radius of
615080	619800	about 60 pixels, all in slightly different orientations.
619800	624760	Curve detector neurons, by the way, seem to show up in basically all image classifiers.
624760	628680	They're somehow a very natural thing for models to learn.
628680	631000	We can use the tricks we already used.
631000	636000	The neurons get activated by pictures of curves, and the feature visualization generates pictures
636000	637000	of curves.
637000	641520	Also, there are some tricks we can use for a really simple feature, like a curve, that
641520	643680	don't work for a dog detector.
643680	648100	We can actually read the algorithm in the neuron, and check that it looks like a pixel
648100	650240	by pixel curve detector.
650240	654720	We can even write our own pixel by pixel curve detector to replace it, and check if anything
654720	655720	breaks.
655720	659560	So it really seems like these neurons are curve detectors.
659560	663720	But there's loads of them, all detecting curves in different directions.
663720	666600	And that gives us some new options for investigation.
666600	671360	Like, what if we take a picture of a curve that activates this curve detector here, and
671360	672840	slowly rotate it?
672840	677560	Well, it turns out that, as we rotate it, its activation on this curve detector goes
677560	681920	down, and then the activation on this other one goes up.
681920	686680	So if we arrange them in order, it turns out that these curve detectors are actually picking
686680	689920	up on every possible orientation between them.
689920	692440	They're not just one-off neurons.
692760	696880	They've been developed as part of a circuit, and they're used together.
696880	701400	Remember that each neuron depends on a small grid of neurons in the previous layer.
701400	706080	And what we find is, for instance, a channel that's activated by this kind of curve in
706080	711640	the top left, and also inhibited by that kind of curve in the bottom right, and also activated
711640	716000	by this kind of curve in the top right, and inhibited by it in the bottom left, and so
716000	717000	on.
717000	721360	So all our channels on this layer, which are checking for different parts of curves in
721360	726040	different parts of the image, get combined into a channel on the next layer, which is
726040	728600	looking for entire circles.
728600	733360	And there are also other channels for more complex features, like spirals.
733360	737040	And it seems like this is also true of neurons in later layers.
737040	741000	Remember earlier how about a tenth of the labels are different kinds of dog?
741000	744240	Well, here's how the model recognises dogs.
744240	750160	Here's a pair of neurons in the layer Mixed4A, which are activated by dog heads facing left
750280	752360	and right, respectively.
752360	755400	And they each feed into a general dog head detector.
755400	760680	But also, there's another pair of neurons, which look for combined dog heads and necks,
760680	762680	again facing left or right.
762680	767360	And we can see from the convolutional layer that the model wants left-facing dog heads
767360	769840	to be to the left of necks.
769840	775080	And the left dog head neuron activates the left dog head and neck neuron, but it actually
775080	778520	inhibits the right dog head and neck neuron.
778520	782400	Just like the model is trying to make sure that the neck and the head are the correct
782400	788840	way around, then both the neck and head neurons and the general dog head neuron all feed into
788840	791440	a general dog neck and head neuron.
791440	793640	And there are loads of patterns like these.
793640	799320	For example, we find a neuron that detects car wheels and a neuron that detects car windows
799320	801920	and a neuron that detects car bodies.
801920	806480	And then we find another neuron that lights up for images with windows at the top and
806480	809920	wheels at the bottom with a car body in the middle.
809920	812800	Now you've got a general-purpose car detector.
812800	817640	In fact, you have an entire channel looking for cars in different parts of the image.
817640	819200	Seems easy, right?
819200	820600	Almost too easy.
820600	824280	Well, don't worry, because it turns out it's actually not that simple.
824280	828800	These tricks with feature visualization and high-scoring images do tell us what a neuron
828800	834040	is doing, but they don't tell us if it's everything the neuron is doing.
834040	839720	High semanticity is the technical term for when a neuron or a channel is actually tracking
839720	841800	more than one feature at once.
841800	847120	See, the network needs to learn to recognize a thousand different categories, and the categories
847120	849000	might be quite complicated.
849000	853600	So sometimes the model learns how to cram more than one feature into a neuron.
853600	858720	For instance, here's a channel which is highly activated by pictures of cat faces and fox
858720	861560	faces and also cars.
861560	866120	If we do our feature visualization but modify it to produce several pictures, which all
866120	871280	activate the channel a lot, while being maximally different from each other, we get some weird
871280	875440	visualizations of cats and foxes and cars.
875440	876920	Why cars?
876920	878240	We don't know.
878240	883040	It seems like sometimes polysemanticity occurs because features are very different, so the
883040	886240	model is not likely to see them both in the same image.
886240	890280	But as I say, we really are not sure.
890280	894880	Polysemanticity appears in all kinds of models, including language models, and it really complicates
894880	897320	the task of interpreting a neuron.
897320	901960	Even if we know that a neuron is doing something, it's hard to know what else it might be doing.
901960	906120	There's been some really interesting work on finding out when and how models become
906120	910600	polysemantic, as well as some more recent work on how to discover patterns of neuron
910600	913160	activation which respond to features.
913160	916760	You can check out links to both of these in the video description.
916760	918600	So where does that leave us?
918600	923080	Well, we've talked about how it's possible to at least begin to interpret the individual
923080	928000	neurons of an image classifier by comparing them against dataset examples and generating
928000	929960	inputs that activate them.
929960	933920	We've talked about how these neurons form into circuits, which explain more complex
933920	934920	behavior.
934920	940080	And we've talked about polysemanticity, the fact that sometimes a neuron is tracking multiple
940080	941840	distinct features.
941840	947000	The original collection of articles on circuits was published in May of 2020, before even
947000	949360	GPT-3 had been released.
949360	951840	So the field has developed a lot since then.
951840	956560	The same kind of work we discussed here is being done on language models to try to understand
956560	962120	how they can write poetry and translate things into French and whatever else you might want.
962120	968200	OpenAI actually has a project to use GPT-4 to interpret all the neurons in GPT-2.
968200	972560	We've also started doing some more work on how the models learn, like at what point they
972560	977480	start to go from memorizing patterns to actually generalizing.
977480	981320	And we've made some tentative attempts to actually extract information directly from
981320	984560	the activations of a model rather than its outputs.
984560	988760	We can ask a language model a question and then read off what it thinks is true from
988760	990000	the inside.
990000	994280	And this is often more accurate than the answer the language model actually outputs.
994280	998320	Of course, this is only possible because in some sense language models aren't telling
998320	999840	us what they know.
999840	1001720	Make of that what you will.
1001720	1008240	This kind of work is called mechanistic interpretability, and it's very hands-on with a lot of experimenting.
1008240	1012120	You might discuss more details about recent developments in future videos.
1012120	1016440	In the meantime, if you're curious to find out more about how mechanistic interpretability
1016440	1021600	works or try it out yourself, you can check out this tutorial, which we've also included
1021600	1024040	a link to in the video description.
1024040	1029440	As we become increasingly reliant on automated systems, mechanistic interpretability might
1029440	1033520	be a key tool for understanding the why behind AI decisions.
