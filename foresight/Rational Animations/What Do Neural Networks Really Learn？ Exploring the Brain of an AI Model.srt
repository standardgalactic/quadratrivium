1
00:00:00,000 --> 00:00:06,880
In 2018, researchers trained an AI to find out if people were at risk of heart conditions

2
00:00:06,880 --> 00:00:09,160
based on pictures of their eyes.

3
00:00:09,160 --> 00:00:15,160
And somehow, the AI also learned to tell people's biological sex with incredibly high accuracy.

4
00:00:15,160 --> 00:00:16,160
How?

5
00:00:16,160 --> 00:00:18,000
We're not entirely sure.

6
00:00:18,000 --> 00:00:22,920
The crazy thing about deep learning is that you can give an AI a set of inputs and outputs,

7
00:00:22,920 --> 00:00:26,960
and it will work out for itself what the relationship between them is.

8
00:00:26,960 --> 00:00:32,960
We didn't teach AIs how to play chess, go, and Atari games by showing them human experts.

9
00:00:32,960 --> 00:00:35,880
We taught them how to work it out for themselves.

10
00:00:35,880 --> 00:00:40,560
And the issue is, now they have worked it out for themselves, and we don't know what

11
00:00:40,560 --> 00:00:43,200
it is they worked out.

12
00:00:43,200 --> 00:00:46,080
Current state-of-the-art AIs are huge.

13
00:00:46,080 --> 00:00:52,320
Meta's largest Llama 2 model uses 70 billion parameters spread across 80 layers, all doing

14
00:00:52,320 --> 00:00:53,840
different things.

15
00:00:53,840 --> 00:00:57,680
It's deep learning models like these, which are being used for everything from hiring

16
00:00:57,680 --> 00:01:03,400
decisions to healthcare and criminal justice, to what YouTube videos get recommended.

17
00:01:03,400 --> 00:01:07,440
Many experts believe that these models might even one day pose a risk to the continued

18
00:01:07,440 --> 00:01:09,360
existence of humanity.

19
00:01:09,360 --> 00:01:13,960
So as these automated processes become more widespread and significant, it will really

20
00:01:13,960 --> 00:01:17,760
matter that we understand how these models make choices.

21
00:01:17,760 --> 00:01:23,040
The good news is, we've got a bit of experience uncovering the mysteries of the universe.

22
00:01:23,040 --> 00:01:28,080
We know that humans are made up of trillions of cells, and by investigating those individual

23
00:01:28,080 --> 00:01:31,960
cells, we've made huge advances in medicine and genetics.

24
00:01:31,960 --> 00:01:36,000
And learning the properties of the atoms, which make up objects, has allowed us to develop

25
00:01:36,000 --> 00:01:40,360
modern material science and high-precision technology like computers.

26
00:01:40,360 --> 00:01:45,080
If you want to understand a complex system with billions of moving parts, sometimes you

27
00:01:45,080 --> 00:01:46,600
have to zoom in.

28
00:01:46,600 --> 00:01:51,640
That's exactly what Chris Ola and his team did, starting in 2015.

29
00:01:51,640 --> 00:01:56,440
They focused on small groups of neurons inside image models, and they were able to find distinct

30
00:01:56,440 --> 00:02:03,040
parts responsible for detecting everything from curves and circles to dog heads and calves.

31
00:02:03,040 --> 00:02:08,160
In this video, we'll briefly explain how convolutional neural networks work, visualize

32
00:02:08,160 --> 00:02:13,600
what individual neurons, the basic building blocks of the neural network, are doing, look

33
00:02:13,600 --> 00:02:19,680
at how neurons combine into circuits to perform tasks, and explore why interpreting networks

34
00:02:19,680 --> 00:02:20,680
is so hard.

35
00:02:21,200 --> 00:02:24,880
There will also be lots of pictures of dogs, like this one.

36
00:02:24,880 --> 00:02:26,360
Let's get going.

37
00:02:26,360 --> 00:02:30,880
We'll start with a brief explanation of how convolutional neural networks are built.

38
00:02:30,880 --> 00:02:34,040
Here's a network that's trained to label images.

39
00:02:34,040 --> 00:02:38,440
An input image comes in on the left, and it flows along through the layers until we get

40
00:02:38,440 --> 00:02:40,120
an output on the right.

41
00:02:40,120 --> 00:02:43,880
The models attempt to classify the image into one of the categories.

42
00:02:43,880 --> 00:02:48,560
This particular model is called Inception V1, and the images it's learned to classify

43
00:02:48,560 --> 00:02:51,680
are from a massive collection called ImageNet.

44
00:02:51,680 --> 00:02:58,480
ImageNet has a thousand different categories of images, like Sandal and Saxophone and Sarong,

45
00:02:58,480 --> 00:03:02,320
which if you don't know is a kind of printed fabric you wrap around your waist.

46
00:03:02,320 --> 00:03:07,960
It also has more than 100 kinds of dog, including 22 types of terrier, so we'll be relevant

47
00:03:07,960 --> 00:03:08,960
later.

48
00:03:08,960 --> 00:03:13,720
But anyway, the model is somehow taking an image and putting out its best guess for which

49
00:03:13,720 --> 00:03:16,080
category the image comes from.

50
00:03:16,080 --> 00:03:17,080
How?

51
00:03:17,480 --> 00:03:21,400
Well, we know exactly what the neurons here on the left are doing.

52
00:03:21,400 --> 00:03:25,600
They're activated by the pixels of the image, and we know exactly what the neurons there

53
00:03:25,600 --> 00:03:27,160
on the right are doing.

54
00:03:27,160 --> 00:03:31,680
Their activations are the model's prediction for each of the possible classifications.

55
00:03:31,680 --> 00:03:34,520
And all these activations are just numbers.

56
00:03:34,520 --> 00:03:36,640
What's happening in between?

57
00:03:36,640 --> 00:03:39,880
The key element is the convolutional layer.

58
00:03:39,880 --> 00:03:44,440
Imagine we take our first layer of input cells, a grid of pixel activations.

59
00:03:44,440 --> 00:03:50,160
What we do is run a little filter across it, and the filter has its own grid of weights.

60
00:03:50,160 --> 00:03:54,000
We multiply the weights of the filter with the activations of the neurons, we add up

61
00:03:54,000 --> 00:03:57,360
the results, and we get a single new value.

62
00:03:57,360 --> 00:04:02,360
So maybe our grid of weights looks like this, a bunch of positive values at the top and

63
00:04:02,360 --> 00:04:04,400
negative values at the bottom.

64
00:04:04,400 --> 00:04:09,200
Then the overall result of the filter is high on parts of the picture where the top is brighter

65
00:04:09,200 --> 00:04:10,200
than the bottom.

66
00:04:10,680 --> 00:04:14,000
It's like it's filtering for a certain kind of edge.

67
00:04:14,000 --> 00:04:18,760
And when we slide this filter across the entire grid of pixel activations, we get a new grid

68
00:04:18,760 --> 00:04:19,760
of activations.

69
00:04:19,760 --> 00:04:24,540
But, instead of representing the input image as is, now it's detecting a certain kind

70
00:04:24,540 --> 00:04:27,880
of edge wherever it appears in the original image.

71
00:04:27,880 --> 00:04:32,960
We also have a bias term, which we just add after applying the filter, because sometimes

72
00:04:32,960 --> 00:04:36,320
we want it to be biased towards a high or low value.

73
00:04:36,520 --> 00:04:40,560
Finally, if the result is negative, we round it up to zero.

74
00:04:40,560 --> 00:04:42,240
That's basically it.

75
00:04:42,240 --> 00:04:46,840
And we have loads of different filters producing different new grids of activations, which we

76
00:04:46,840 --> 00:04:48,960
call channels.

77
00:04:48,960 --> 00:04:54,160
These channels together form a new layer, and we run more filters across them, and those

78
00:04:54,160 --> 00:04:57,360
feed into another layer, and then another layer.

79
00:04:57,360 --> 00:05:02,680
Each layer usually detects more and more abstract properties of the image, until we get to the

80
00:05:02,680 --> 00:05:06,880
last part of the network, which is structured like a traditional fully connected neural

81
00:05:06,880 --> 00:05:07,880
network.

82
00:05:07,880 --> 00:05:12,680
Somehow, the neurons at the end of the network tell you if you're looking at a terrier or

83
00:05:12,680 --> 00:05:14,200
a saxophone.

84
00:05:14,200 --> 00:05:15,960
That's pretty crazy.

85
00:05:15,960 --> 00:05:20,960
If you're wondering how we decide on the weights for the filters, well, we don't.

86
00:05:20,960 --> 00:05:24,480
That's the bit the model works out for itself during training.

87
00:05:24,480 --> 00:05:28,680
So the question is, why does it pick those specific values?

88
00:05:28,680 --> 00:05:32,680
How do we find out what these channels in the middle are representing precisely?

89
00:05:32,680 --> 00:05:35,720
Well, let's pick one deep in the middle.

90
00:05:35,720 --> 00:05:38,520
What's this channel doing?

91
00:05:38,520 --> 00:05:43,080
Maybe one way we can find out is to ask, what does it care about?

92
00:05:43,080 --> 00:05:47,520
Let's take all our images and feed them through the model, and check which images give it

93
00:05:47,520 --> 00:05:49,640
the highest activation.

94
00:05:49,640 --> 00:05:52,120
And look, it's all pictures of dogs.

95
00:05:52,120 --> 00:05:53,620
Lots of dogs.

96
00:05:53,620 --> 00:05:55,680
Maybe this is a dog detecting channel.

97
00:05:55,680 --> 00:05:58,120
Well, it's hard to be sure.

98
00:05:58,120 --> 00:06:03,880
We know that something about dogs is activating it, but we don't know what exactly.

99
00:06:03,880 --> 00:06:08,760
If we wanted to be more sure, we could try to directly optimize an input to activate

100
00:06:08,760 --> 00:06:10,200
a neuron in this channel.

101
00:06:10,200 --> 00:06:14,600
Actually, in pretty much the same way we optimize the network to be accurate.

102
00:06:14,600 --> 00:06:19,920
So we feed the network a totally random bunch of pixels, and we see how much that activates

103
00:06:19,920 --> 00:06:22,160
our maybe dog neuron.

104
00:06:22,160 --> 00:06:26,840
Then we change the input a bit so that it activates the neuron a bit more, and we do

105
00:06:26,840 --> 00:06:31,240
this more and more until the neuron is as activated as possible.

106
00:06:31,240 --> 00:06:35,560
We can also do this with a whole channel at once, a whole grid of neurons doing the same

107
00:06:35,560 --> 00:06:40,400
operation on different parts of the image, by trying to get the highest average activation

108
00:06:40,400 --> 00:06:41,800
for the neurons.

109
00:06:41,800 --> 00:06:43,960
But let's stick with just one neuron for now.

110
00:06:43,960 --> 00:06:48,200
OK, so unfortunately doing just this doesn't work.

111
00:06:48,200 --> 00:06:52,800
What you get is some kind of weird, extremely cursed pile of static.

112
00:06:52,800 --> 00:06:55,480
We don't know why it's that specifically.

113
00:06:55,480 --> 00:06:58,080
Like we said, there's a lot we don't know about neural networks.

114
00:06:58,080 --> 00:07:01,160
But just activating the neuron isn't enough.

115
00:07:01,160 --> 00:07:04,640
So let's add some extra conditions for our optimization process.

116
00:07:04,640 --> 00:07:09,200
What we want is something that wouldn't rule out a sensible image, but would decrease the

117
00:07:09,200 --> 00:07:14,080
chance of getting one of the weird, cursed static piles, so that when we run our optimization

118
00:07:14,080 --> 00:07:19,320
process, the top scorer is more likely to be like a sensible image.

119
00:07:19,320 --> 00:07:23,680
For instance, maybe we can take the input we're optimizing for, and jitter it around

120
00:07:23,680 --> 00:07:28,160
a bit each step, rotate it slightly, scale it up and down a bit.

121
00:07:28,160 --> 00:07:30,640
For a normal picture, this doesn't change much.

122
00:07:30,640 --> 00:07:35,200
A dog head still looks like a dog head, but it seems to really mess with the walls of

123
00:07:35,200 --> 00:07:37,360
static according to the neuron.

124
00:07:37,360 --> 00:07:40,960
The technical term here is transformation robustness.

125
00:07:40,960 --> 00:07:44,240
The image should be robust to being transformed slightly.

126
00:07:44,240 --> 00:07:47,400
And now, images start to take shape.

127
00:07:47,400 --> 00:07:51,920
In the original circuits piece, they also did something called, and bear with me for

128
00:07:51,920 --> 00:07:57,640
a second, preconditioning to optimize in a color-decorrelated Fourier space.

129
00:07:57,640 --> 00:07:58,840
So what does that mean?

130
00:07:58,840 --> 00:08:04,440
Well, in audio processing, a Fourier transform is a way to take something like a chord, or

131
00:08:04,440 --> 00:08:08,520
any other sound, and break it down into the constituent tones.

132
00:08:08,520 --> 00:08:13,000
So instead of splitting a sound up into the amplitude over time, you break it down into

133
00:08:13,000 --> 00:08:16,440
what simple notes make it up.

134
00:08:16,440 --> 00:08:19,600
You can do pretty much exactly the same thing with images.

135
00:08:19,600 --> 00:08:24,400
Instead of thinking about the picture pixel by pixel, you layer a bunch of smooth waves

136
00:08:24,400 --> 00:08:26,040
on top of each other.

137
00:08:26,040 --> 00:08:30,680
So we have the optimizer look for adjustments to the input image that would lead to an increased

138
00:08:30,680 --> 00:08:32,360
neuron activation.

139
00:08:32,360 --> 00:08:36,800
And we let the optimizer work with this wave representation of the input image, instead

140
00:08:36,800 --> 00:08:38,640
of changing it directly.

141
00:08:38,640 --> 00:08:43,880
Turns out that this way, the adjustments are more smooth and less like static noise.

142
00:08:43,880 --> 00:08:47,800
When the optimization is finished, we translate it back to a normal image.

143
00:08:47,800 --> 00:08:51,760
And the resulting images actually look kind of reasonable.

144
00:08:51,760 --> 00:08:55,520
But now our dog detector seems like it isn't actually detecting dogs.

145
00:08:55,520 --> 00:08:59,080
It looks like it's really detecting their snouts.

146
00:08:59,080 --> 00:09:04,040
And the way to fit the most snoutiness into the image is to fit another snout inside the

147
00:09:04,040 --> 00:09:05,040
snout.

148
00:09:05,040 --> 00:09:06,040
Weird, right?

149
00:09:06,040 --> 00:09:07,800
Here's something for you to ponder.

150
00:09:07,800 --> 00:09:11,720
Why is it so clear in the middle, and so fuzzy on the edges?

151
00:09:11,720 --> 00:09:16,320
Well, that's because we're only focusing on one neuron, and that neuron is only looking

152
00:09:16,320 --> 00:09:18,160
at part of the picture.

153
00:09:18,160 --> 00:09:22,440
We'll see later on that trying to maximize the whole channel makes the whole image more

154
00:09:22,440 --> 00:09:23,440
crisp.

155
00:09:23,440 --> 00:09:25,800
But anyway, back to snoutiness.

156
00:09:25,800 --> 00:09:28,400
This is sort of how it is with interpretability.

157
00:09:28,400 --> 00:09:31,480
It's very hard to know what you're actually looking at.

158
00:09:31,480 --> 00:09:35,840
The model is just learning whatever fits the data, and sometimes the thing that works is

159
00:09:35,840 --> 00:09:37,360
a bit surprising.

160
00:09:37,360 --> 00:09:42,640
In that sense, this kind of work is less like formal mathematical proofs, and more like natural

161
00:09:42,640 --> 00:09:43,720
science.

162
00:09:43,720 --> 00:09:48,560
You experiment, you make predictions, and you test them, and slowly you become more

163
00:09:48,560 --> 00:09:49,560
confident.

164
00:09:49,560 --> 00:09:53,040
But this is still just one neuron on its own.

165
00:09:53,040 --> 00:09:55,120
One tiny little part.

166
00:09:55,120 --> 00:09:59,640
How do we get from that to understanding the whole massive, messy network?

167
00:09:59,640 --> 00:10:02,240
Well, we can zoom out a bit.

168
00:10:02,240 --> 00:10:06,480
Let's try doing that with some neurons we understand really, really well.

169
00:10:06,480 --> 00:10:10,200
For instance, let's go up a bit to this layer, mixed 3B.

170
00:10:10,200 --> 00:10:15,080
It has a bunch of neurons, which seem like they're detecting curves with a radius of

171
00:10:15,080 --> 00:10:19,800
about 60 pixels, all in slightly different orientations.

172
00:10:19,800 --> 00:10:24,760
Curve detector neurons, by the way, seem to show up in basically all image classifiers.

173
00:10:24,760 --> 00:10:28,680
They're somehow a very natural thing for models to learn.

174
00:10:28,680 --> 00:10:31,000
We can use the tricks we already used.

175
00:10:31,000 --> 00:10:36,000
The neurons get activated by pictures of curves, and the feature visualization generates pictures

176
00:10:36,000 --> 00:10:37,000
of curves.

177
00:10:37,000 --> 00:10:41,520
Also, there are some tricks we can use for a really simple feature, like a curve, that

178
00:10:41,520 --> 00:10:43,680
don't work for a dog detector.

179
00:10:43,680 --> 00:10:48,100
We can actually read the algorithm in the neuron, and check that it looks like a pixel

180
00:10:48,100 --> 00:10:50,240
by pixel curve detector.

181
00:10:50,240 --> 00:10:54,720
We can even write our own pixel by pixel curve detector to replace it, and check if anything

182
00:10:54,720 --> 00:10:55,720
breaks.

183
00:10:55,720 --> 00:10:59,560
So it really seems like these neurons are curve detectors.

184
00:10:59,560 --> 00:11:03,720
But there's loads of them, all detecting curves in different directions.

185
00:11:03,720 --> 00:11:06,600
And that gives us some new options for investigation.

186
00:11:06,600 --> 00:11:11,360
Like, what if we take a picture of a curve that activates this curve detector here, and

187
00:11:11,360 --> 00:11:12,840
slowly rotate it?

188
00:11:12,840 --> 00:11:17,560
Well, it turns out that, as we rotate it, its activation on this curve detector goes

189
00:11:17,560 --> 00:11:21,920
down, and then the activation on this other one goes up.

190
00:11:21,920 --> 00:11:26,680
So if we arrange them in order, it turns out that these curve detectors are actually picking

191
00:11:26,680 --> 00:11:29,920
up on every possible orientation between them.

192
00:11:29,920 --> 00:11:32,440
They're not just one-off neurons.

193
00:11:32,760 --> 00:11:36,880
They've been developed as part of a circuit, and they're used together.

194
00:11:36,880 --> 00:11:41,400
Remember that each neuron depends on a small grid of neurons in the previous layer.

195
00:11:41,400 --> 00:11:46,080
And what we find is, for instance, a channel that's activated by this kind of curve in

196
00:11:46,080 --> 00:11:51,640
the top left, and also inhibited by that kind of curve in the bottom right, and also activated

197
00:11:51,640 --> 00:11:56,000
by this kind of curve in the top right, and inhibited by it in the bottom left, and so

198
00:11:56,000 --> 00:11:57,000
on.

199
00:11:57,000 --> 00:12:01,360
So all our channels on this layer, which are checking for different parts of curves in

200
00:12:01,360 --> 00:12:06,040
different parts of the image, get combined into a channel on the next layer, which is

201
00:12:06,040 --> 00:12:08,600
looking for entire circles.

202
00:12:08,600 --> 00:12:13,360
And there are also other channels for more complex features, like spirals.

203
00:12:13,360 --> 00:12:17,040
And it seems like this is also true of neurons in later layers.

204
00:12:17,040 --> 00:12:21,000
Remember earlier how about a tenth of the labels are different kinds of dog?

205
00:12:21,000 --> 00:12:24,240
Well, here's how the model recognises dogs.

206
00:12:24,240 --> 00:12:30,160
Here's a pair of neurons in the layer Mixed4A, which are activated by dog heads facing left

207
00:12:30,280 --> 00:12:32,360
and right, respectively.

208
00:12:32,360 --> 00:12:35,400
And they each feed into a general dog head detector.

209
00:12:35,400 --> 00:12:40,680
But also, there's another pair of neurons, which look for combined dog heads and necks,

210
00:12:40,680 --> 00:12:42,680
again facing left or right.

211
00:12:42,680 --> 00:12:47,360
And we can see from the convolutional layer that the model wants left-facing dog heads

212
00:12:47,360 --> 00:12:49,840
to be to the left of necks.

213
00:12:49,840 --> 00:12:55,080
And the left dog head neuron activates the left dog head and neck neuron, but it actually

214
00:12:55,080 --> 00:12:58,520
inhibits the right dog head and neck neuron.

215
00:12:58,520 --> 00:13:02,400
Just like the model is trying to make sure that the neck and the head are the correct

216
00:13:02,400 --> 00:13:08,840
way around, then both the neck and head neurons and the general dog head neuron all feed into

217
00:13:08,840 --> 00:13:11,440
a general dog neck and head neuron.

218
00:13:11,440 --> 00:13:13,640
And there are loads of patterns like these.

219
00:13:13,640 --> 00:13:19,320
For example, we find a neuron that detects car wheels and a neuron that detects car windows

220
00:13:19,320 --> 00:13:21,920
and a neuron that detects car bodies.

221
00:13:21,920 --> 00:13:26,480
And then we find another neuron that lights up for images with windows at the top and

222
00:13:26,480 --> 00:13:29,920
wheels at the bottom with a car body in the middle.

223
00:13:29,920 --> 00:13:32,800
Now you've got a general-purpose car detector.

224
00:13:32,800 --> 00:13:37,640
In fact, you have an entire channel looking for cars in different parts of the image.

225
00:13:37,640 --> 00:13:39,200
Seems easy, right?

226
00:13:39,200 --> 00:13:40,600
Almost too easy.

227
00:13:40,600 --> 00:13:44,280
Well, don't worry, because it turns out it's actually not that simple.

228
00:13:44,280 --> 00:13:48,800
These tricks with feature visualization and high-scoring images do tell us what a neuron

229
00:13:48,800 --> 00:13:54,040
is doing, but they don't tell us if it's everything the neuron is doing.

230
00:13:54,040 --> 00:13:59,720
High semanticity is the technical term for when a neuron or a channel is actually tracking

231
00:13:59,720 --> 00:14:01,800
more than one feature at once.

232
00:14:01,800 --> 00:14:07,120
See, the network needs to learn to recognize a thousand different categories, and the categories

233
00:14:07,120 --> 00:14:09,000
might be quite complicated.

234
00:14:09,000 --> 00:14:13,600
So sometimes the model learns how to cram more than one feature into a neuron.

235
00:14:13,600 --> 00:14:18,720
For instance, here's a channel which is highly activated by pictures of cat faces and fox

236
00:14:18,720 --> 00:14:21,560
faces and also cars.

237
00:14:21,560 --> 00:14:26,120
If we do our feature visualization but modify it to produce several pictures, which all

238
00:14:26,120 --> 00:14:31,280
activate the channel a lot, while being maximally different from each other, we get some weird

239
00:14:31,280 --> 00:14:35,440
visualizations of cats and foxes and cars.

240
00:14:35,440 --> 00:14:36,920
Why cars?

241
00:14:36,920 --> 00:14:38,240
We don't know.

242
00:14:38,240 --> 00:14:43,040
It seems like sometimes polysemanticity occurs because features are very different, so the

243
00:14:43,040 --> 00:14:46,240
model is not likely to see them both in the same image.

244
00:14:46,240 --> 00:14:50,280
But as I say, we really are not sure.

245
00:14:50,280 --> 00:14:54,880
Polysemanticity appears in all kinds of models, including language models, and it really complicates

246
00:14:54,880 --> 00:14:57,320
the task of interpreting a neuron.

247
00:14:57,320 --> 00:15:01,960
Even if we know that a neuron is doing something, it's hard to know what else it might be doing.

248
00:15:01,960 --> 00:15:06,120
There's been some really interesting work on finding out when and how models become

249
00:15:06,120 --> 00:15:10,600
polysemantic, as well as some more recent work on how to discover patterns of neuron

250
00:15:10,600 --> 00:15:13,160
activation which respond to features.

251
00:15:13,160 --> 00:15:16,760
You can check out links to both of these in the video description.

252
00:15:16,760 --> 00:15:18,600
So where does that leave us?

253
00:15:18,600 --> 00:15:23,080
Well, we've talked about how it's possible to at least begin to interpret the individual

254
00:15:23,080 --> 00:15:28,000
neurons of an image classifier by comparing them against dataset examples and generating

255
00:15:28,000 --> 00:15:29,960
inputs that activate them.

256
00:15:29,960 --> 00:15:33,920
We've talked about how these neurons form into circuits, which explain more complex

257
00:15:33,920 --> 00:15:34,920
behavior.

258
00:15:34,920 --> 00:15:40,080
And we've talked about polysemanticity, the fact that sometimes a neuron is tracking multiple

259
00:15:40,080 --> 00:15:41,840
distinct features.

260
00:15:41,840 --> 00:15:47,000
The original collection of articles on circuits was published in May of 2020, before even

261
00:15:47,000 --> 00:15:49,360
GPT-3 had been released.

262
00:15:49,360 --> 00:15:51,840
So the field has developed a lot since then.

263
00:15:51,840 --> 00:15:56,560
The same kind of work we discussed here is being done on language models to try to understand

264
00:15:56,560 --> 00:16:02,120
how they can write poetry and translate things into French and whatever else you might want.

265
00:16:02,120 --> 00:16:08,200
OpenAI actually has a project to use GPT-4 to interpret all the neurons in GPT-2.

266
00:16:08,200 --> 00:16:12,560
We've also started doing some more work on how the models learn, like at what point they

267
00:16:12,560 --> 00:16:17,480
start to go from memorizing patterns to actually generalizing.

268
00:16:17,480 --> 00:16:21,320
And we've made some tentative attempts to actually extract information directly from

269
00:16:21,320 --> 00:16:24,560
the activations of a model rather than its outputs.

270
00:16:24,560 --> 00:16:28,760
We can ask a language model a question and then read off what it thinks is true from

271
00:16:28,760 --> 00:16:30,000
the inside.

272
00:16:30,000 --> 00:16:34,280
And this is often more accurate than the answer the language model actually outputs.

273
00:16:34,280 --> 00:16:38,320
Of course, this is only possible because in some sense language models aren't telling

274
00:16:38,320 --> 00:16:39,840
us what they know.

275
00:16:39,840 --> 00:16:41,720
Make of that what you will.

276
00:16:41,720 --> 00:16:48,240
This kind of work is called mechanistic interpretability, and it's very hands-on with a lot of experimenting.

277
00:16:48,240 --> 00:16:52,120
You might discuss more details about recent developments in future videos.

278
00:16:52,120 --> 00:16:56,440
In the meantime, if you're curious to find out more about how mechanistic interpretability

279
00:16:56,440 --> 00:17:01,600
works or try it out yourself, you can check out this tutorial, which we've also included

280
00:17:01,600 --> 00:17:04,040
a link to in the video description.

281
00:17:04,040 --> 00:17:09,440
As we become increasingly reliant on automated systems, mechanistic interpretability might

282
00:17:09,440 --> 00:17:13,520
be a key tool for understanding the why behind AI decisions.

