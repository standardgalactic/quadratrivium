WEBVTT

00:00.000 --> 00:03.000
We're joined today by Dr. Matt Welsh, and we'll be joined toward the end of the talk

00:03.000 --> 00:06.000
by Pizza as well, which we'll serve right out there on folks' way out,

00:06.000 --> 00:09.000
as an opportunity to chat more casually with Matt toward the end.

00:09.000 --> 00:12.000
I actually got to know Matt when I was back in graduate school,

00:12.000 --> 00:14.000
and I spent quite a bit of time with him and his students

00:14.000 --> 00:17.000
when his focus was particularly on what are called sensor networks,

00:17.000 --> 00:21.000
which are these distributed networks of very small, low-power, low-resource devices,

00:21.000 --> 00:25.000
which made it very hard at the time to actually write code that interconnects them

00:25.000 --> 00:27.000
and generally solves problems.

00:27.000 --> 00:30.000
And among the problems some of my classmates were working on

00:30.000 --> 00:33.000
were monitoring volcanoes, for instance, and the integrity of bridges.

00:33.000 --> 00:37.000
And in my own interest, being able to set up these mesh networks of sorts

00:37.000 --> 00:40.000
in emergency medicine so that they could talk among each other without wires

00:40.000 --> 00:42.000
or without any central access.

00:42.000 --> 00:45.000
Matt went on since then to work full-time at Google

00:45.000 --> 00:47.000
and most recently at Fixie.ai.

00:47.000 --> 00:49.000
And he's might have seen from today's description

00:49.000 --> 00:53.000
he portends a future in which computers will do the writing of code for us.

00:53.000 --> 00:55.000
So if you're struggling in CSI,

00:55.000 --> 00:58.000
61, 161 or anything in between, not to worry.

00:58.000 --> 01:01.000
AI is now here, as is Dr. Matt Wells.

01:01.000 --> 01:03.000
Thanks, David.

01:03.000 --> 01:04.000
Thanks for having me.

01:04.000 --> 01:07.000
It's been, I don't know, 13 years or something, 12 years

01:07.000 --> 01:09.000
since I gave a lecture at Harvard.

01:09.000 --> 01:12.000
So we'll see if I've still got it.

01:12.000 --> 01:17.000
And I was joking yesterday with David Parks, who's now the dean

01:17.000 --> 01:21.000
and he and I were kind of peers when I was on the faculty here.

01:21.000 --> 01:23.000
And I said, you know, like it's remarkable.

01:23.000 --> 01:27.000
Like congratulations, David, on becoming dean of CS.

01:27.000 --> 01:32.000
I don't think we're kind of old enough to be dean quality yet.

01:32.000 --> 01:34.000
And then actually I realize we are.

01:34.000 --> 01:36.000
So anyway, all right.

01:36.000 --> 01:43.000
So I'm here to tell you that the field of computer science is doomed.

01:43.000 --> 01:45.000
Okay.

01:45.000 --> 01:49.000
And I actually kind of mean this, although I'm going to put it in somewhat here.

01:49.000 --> 01:52.000
I kind of mean this, although I'm going to put it in somewhat humorous terms.

01:52.000 --> 01:56.000
That if you think about computer science, what is the field about?

01:56.000 --> 01:57.000
What does it mean?

01:57.000 --> 01:58.000
Where did it come from?

01:58.000 --> 01:59.000
What is it?

01:59.000 --> 02:01.000
What's the core idea of it?

02:01.000 --> 02:06.000
It's the idea of taking an idea, an algorithm or a concept or a data structure

02:06.000 --> 02:10.000
and translating it into a program that can generally be run

02:10.000 --> 02:13.000
by like a von Neumann architecture machine, right?

02:13.000 --> 02:14.000
Okay.

02:14.000 --> 02:16.000
So that's computer science in a nutshell.

02:16.000 --> 02:27.000
The problem is that the goal of CS has always had this kind of core fundamental assumption or axiom

02:27.000 --> 02:33.000
that is that the programs that we're all talking about here have been implemented,

02:33.000 --> 02:38.000
maintained and have to be understood by humans, right?

02:38.000 --> 02:44.000
That if I print out the code for a program, a human, some human, maybe not everyone,

02:44.000 --> 02:49.000
but at least maybe the person who wrote it, if not someone else, can understand it.

02:49.000 --> 02:51.000
Now here's the problem, right?

02:51.000 --> 02:54.000
Humans suck at all three of these things.

02:54.000 --> 02:56.000
We're terrible at writing programs.

02:56.000 --> 03:01.000
We're terrible at maintaining them and we're absolutely terrible at understanding them.

03:01.000 --> 03:06.000
So what does that really mean for the field?

03:06.000 --> 03:13.000
So I want to make this claim that 50 years of research into programming languages

03:13.000 --> 03:19.000
has done effectively nothing to solve this problem.

03:19.000 --> 03:21.000
We've been at this for a long time now.

03:21.000 --> 03:26.000
50 years is a long time and we keep inventing new languages and new programming concepts

03:26.000 --> 03:32.000
and new abstractions and new data types and new proof methodologies.

03:32.000 --> 03:37.000
But none of the stuff that we've developed in terms of tooling or languages or proof techniques

03:37.000 --> 03:41.000
or documentation or linters has actually solved this problem

03:41.000 --> 03:45.000
and I don't think another 50 years is going to solve it.

03:45.000 --> 03:55.000
I think this idea of building automated tools to help humans write better software has played itself out.

03:55.000 --> 04:00.000
Now if you disagree with me, let's just take a look at kind of the history here.

04:00.000 --> 04:04.000
So let's rewind the clock all the way back to 1957.

04:04.000 --> 04:08.000
This is Conway's Game of Life implemented in Fortran.

04:08.000 --> 04:17.000
I don't remember which dialect of Fortran this is, but Fortran came about in about 1957.

04:17.000 --> 04:20.000
I just claim this is really hard to understand.

04:20.000 --> 04:26.000
I claim that you can't look at this unless you had some idea of the intent of the program

04:26.000 --> 04:28.000
or what the hell does this do.

04:28.000 --> 04:32.000
You could work it out. You could spend some time reading it.

04:32.000 --> 04:37.000
You could probably understand it with some effort, but it's not trivial.

04:37.000 --> 04:39.000
It's not straightforward.

04:39.000 --> 04:42.000
Okay, so we tried to make programming easier.

04:42.000 --> 04:46.000
We came up with something called BASIC in 1964.

04:46.000 --> 04:48.000
This is not the original BASIC.

04:48.000 --> 04:52.000
Again, it's had many dialects because obviously the first one wasn't good enough.

04:52.000 --> 04:54.000
We had to keep improving the language.

04:54.000 --> 04:58.000
This is the same program in BASIC.

04:58.000 --> 05:03.000
I don't think this is any easier to understand, okay?

05:03.000 --> 05:08.000
I could spend some time reading it and convince myself that it does a certain thing,

05:08.000 --> 05:10.000
but it's quite challenging to get.

05:10.000 --> 05:15.000
So then we came up with APL.

05:15.000 --> 05:19.000
This is Conway's Game of Life and APL.

05:19.000 --> 05:22.000
I would say raise your hand if you understand this,

05:22.000 --> 05:27.000
but I know there's probably a few people in the audience who do.

05:27.000 --> 05:29.000
I don't, right?

05:29.000 --> 05:35.000
This is a programming language so complex you needed a special keyboard to type it, okay?

05:35.000 --> 05:41.000
But this is what we thought was the practice of developing programming languages

05:41.000 --> 05:45.000
back in the 60s, was this.

05:45.000 --> 05:48.000
Certainly it doesn't do the job.

05:48.000 --> 05:51.000
All right, well, I've been talking about stuff that's kind of old-fashioned.

05:51.000 --> 05:53.000
Let's talk about the new hotness.

05:53.000 --> 05:55.000
Let's talk about Rust.

05:55.000 --> 05:57.000
Everybody's programming in Rust.

05:57.000 --> 05:59.000
It's the latest and greatest thing since sliced bread.

05:59.000 --> 06:03.000
I spent two years running engineering at a startup that was completely Rust-based.

06:03.000 --> 06:05.000
I ran a big team full of Rust developers.

06:05.000 --> 06:09.000
I actually learned Rust myself, kind of.

06:09.000 --> 06:11.000
This is the same program in Rust.

06:11.000 --> 06:14.000
I don't make heads or tails of this.

06:14.000 --> 06:20.000
It is incredibly hard to write programs that are easy to understand,

06:20.000 --> 06:25.000
easy to maintain, easy to reason about, okay?

06:25.000 --> 06:28.000
So that's the kind of state of the art.

06:28.000 --> 06:32.000
This is where we've gotten in 50 years from Fortran to this.

06:32.000 --> 06:36.000
And I just want to make the claim that this is not going to work, okay?

06:36.000 --> 06:39.000
We're done. Game over.

06:39.000 --> 06:41.000
So what's next?

06:41.000 --> 06:46.000
Well, this is how I write code today.

06:46.000 --> 06:51.000
This is a prompt passed to the GPT-4 model.

06:51.000 --> 06:56.000
And it's part of a larger program that reads in some text of a transcript

06:56.000 --> 07:00.000
that's been derived from a podcast audio feed.

07:00.000 --> 07:04.000
We're feeding the transcript into the model and we're giving it these instructions.

07:04.000 --> 07:10.000
We're saying, please summarize the following segment of this podcast transcript.

07:10.000 --> 07:12.000
Only use the information in the text.

07:12.000 --> 07:14.000
Do not in caps.

07:14.000 --> 07:17.000
This is important, by the way. The all caps is super important.

07:17.000 --> 07:21.000
Do not use any information you know about the world.

07:21.000 --> 07:28.000
Include the title of the podcast, the name of the episode, and the names of the speakers, if known.

07:28.000 --> 07:34.000
This English statement here encodes an algorithm.

07:34.000 --> 07:41.000
It describes something that I want to do with an input data and the output data that I want.

07:41.000 --> 07:46.000
And my expectations about the kind of thing that's in the output data.

07:46.000 --> 07:48.000
So a few things to notice about this.

07:48.000 --> 07:56.000
The first thing to notice about this is I don't think anyone could ever write down the algorithm

07:56.000 --> 08:01.000
for what this is supposed to do in any existing programming language

08:01.000 --> 08:05.000
or any programming language that we're likely to come up with in the future.

08:05.000 --> 08:09.000
How do you write this algorithm?

08:09.000 --> 08:12.000
You can't, right? There's no pseudocode. There's no proof.

08:12.000 --> 08:17.000
There's no mathematical symbology here, right?

08:17.000 --> 08:22.000
The other thing to notice is, at least for me, I don't know about any of you.

08:22.000 --> 08:26.000
Do you understand this? Do you understand what it's saying?

08:26.000 --> 08:31.000
Does it make sense? Can you read it? Can you reason about what it's supposed to do?

08:31.000 --> 08:35.000
Yes, of course, right? It's in plain English.

08:35.000 --> 08:40.000
It doesn't have to be English, by the way. It could be in Mandarin Chinese or Esperanto.

08:40.000 --> 08:48.000
Have you all seen the XKCD about the guy who walks into his friend's house

08:48.000 --> 08:52.000
and he says, okay, Alexa, order five tons of creamed corn?

08:52.000 --> 08:55.000
Okay, Alexa, confirm order.

08:55.000 --> 08:59.000
That's how he makes sure that no one's got a speaker listening to him.

08:59.000 --> 09:04.000
Okay, so the point being that this is now how I am actually writing code

09:04.000 --> 09:09.000
and what's funny about this is a lot of it is trial and error and experimentation.

09:09.000 --> 09:15.000
By the way, that's the same when I'm writing normal computer code.

09:15.000 --> 09:20.000
And the other thing that's interesting about this is there's a lot of subtlety

09:20.000 --> 09:26.000
in terms of how you instruct the model and how you know what it's going to do with your instructions.

09:26.000 --> 09:30.000
You can't write a manual that says, well, here's this set of words that you need to use

09:30.000 --> 09:35.000
to get the model to do X, Y, or Z. You have to just try out certain things.

09:35.000 --> 09:41.000
In this case, I found out the do not in all caps really helped

09:41.000 --> 09:47.000
because I really wanted to emphasize that point to the model.

09:47.000 --> 09:53.000
This reminds me of another programming language that someone came up with a while ago called intercal.

09:53.000 --> 10:00.000
Intercal was meant to be one of these kind of obscure or maybe satirical joke programming languages.

10:00.000 --> 10:07.000
Intercal had these interesting features such as you had to use the keyword please.

10:07.000 --> 10:13.000
And if you use the keyword please too often, the compiler would reject your program.

10:13.000 --> 10:17.000
If you didn't use it enough, it would also reject your program.

10:17.000 --> 10:21.000
And it turned out that feature was undocumented.

10:21.000 --> 10:23.000
It's exactly like what we're doing today, right?

10:23.000 --> 10:29.000
We have to say please and do not in all caps to get the language models to do what we want.

10:29.000 --> 10:31.000
So where am I going with all this?

10:31.000 --> 10:40.000
I think what I'm saying here is we are now in an era where we have machines that can take natural language in

10:40.000 --> 10:47.000
and produce results, algorithmic results, computational results,

10:47.000 --> 10:57.000
but for which no human has written a program in anything resembling a conventional programming language.

10:57.000 --> 11:02.000
And I claim that these models are going to get so good at doing this

11:02.000 --> 11:07.000
that our whole concept of programming computers is going to get replaced over time

11:07.000 --> 11:14.000
with instructing language models to do things for us.

11:14.000 --> 11:19.000
So let's take a look at the state of programming language technology.

11:19.000 --> 11:27.000
This is a programmer without co-pilot in around 2020, colorized, okay?

11:27.000 --> 11:31.000
I think I met that guy out in Central Square this morning.

11:31.000 --> 11:34.000
And here's a programmer with co-pilot in 2021, right?

11:34.000 --> 11:38.000
So clearly we're evolving very rapidly as a species of programmers.

11:38.000 --> 11:40.000
Unfortunately, both of these cases are male.

11:40.000 --> 11:43.000
I apologize for that.

11:43.000 --> 11:52.000
So how many people here have used co-pilot or one of its ilk in terms of helping you write code?

11:52.000 --> 11:54.000
Don't be shy.

11:54.000 --> 11:57.000
I know you're like, who's my professor in here?

11:57.000 --> 11:58.000
Oh, shit.

11:58.000 --> 11:59.000
All right.

11:59.000 --> 12:08.000
So co-pilot, if you haven't used it, is a complete game changer in terms of how real world developers write code.

12:08.000 --> 12:09.000
Okay?

12:09.000 --> 12:15.000
Yes, it's also kind of a huge boost for students who want to effectively shortcut their homework,

12:15.000 --> 12:17.000
speed run their homework.

12:17.000 --> 12:22.000
But this is, for someone working in the industry, writing code every single day,

12:22.000 --> 12:25.000
if I don't have co-pilot, I absolutely feel naked.

12:25.000 --> 12:27.000
I was on the airplane out here, I was writing code.

12:27.000 --> 12:32.000
The Wi-Fi was not quite fast enough, so I would type out, you know, my half a line of code

12:32.000 --> 12:35.000
and just sort of wait for co-pilot to finish it for me like I always do.

12:35.000 --> 12:39.000
Normally that happens in about like less than a second, and this time it was just taking so long.

12:39.000 --> 12:40.000
I said, ah, damn it.

12:40.000 --> 12:48.000
I guess I have to write this myself, just like I used to a year ago.

12:48.000 --> 12:51.000
Co-pilot is incredible for a few reasons.

12:51.000 --> 12:58.000
I think one of the things that people don't fully appreciate is that it keeps you in the zone of writing code.

12:58.000 --> 13:03.000
It used to be the case that any time I'd hit a little snag, I'd be like, oh, crap.

13:03.000 --> 13:09.000
I can't quite remember the syntax for how I, you know, reverse a list in whatever language I'm working in.

13:09.000 --> 13:10.000
Crap.

13:10.000 --> 13:11.000
Well, I know where to find the answer.

13:11.000 --> 13:12.000
I'll just Google it.

13:12.000 --> 13:15.000
It's on Stack Overflow somewhere.

13:15.000 --> 13:18.000
And so I go and I Google it and I find the thing.

13:18.000 --> 13:23.000
It's probably not a direct answer, so I have to kind of read the article a little bit and kind of piece together.

13:23.000 --> 13:25.000
Oh, yeah, that's the snippet I was looking for.

13:25.000 --> 13:28.000
And then 45 minutes later, what am I doing?

13:28.000 --> 13:29.000
I'm on Reddit somewhere, you know.

13:29.000 --> 13:32.000
I've gone down the rat hole of surfing the internet.

13:32.000 --> 13:36.000
I got out of the zone of writing code.

13:36.000 --> 13:41.000
So by doing, keeping you in the zone, I think people are so much more productive with this.

13:41.000 --> 13:46.000
And to the point where we mandated every developer at our company has to use co-pilot.

13:46.000 --> 13:49.000
If there's somebody not using co-pilot, they're going to be fired.

13:49.000 --> 13:53.000
Well, I didn't say that, but it's kind of the idea.

13:53.000 --> 13:58.000
So a lot of people have chastised or criticized co-pilot for being a little dumb, right?

13:58.000 --> 14:03.000
It's like, well, it's just trained on stuff it found on the internet on GitHub and homework assignments.

14:03.000 --> 14:05.000
How good can it be?

14:05.000 --> 14:07.000
It's incredibly good.

14:07.000 --> 14:11.000
It's not just parroting back things that it's seen elsewhere.

14:11.000 --> 14:14.000
It's interpreting your program and your intent.

14:14.000 --> 14:19.000
It's looking at other parts of your code to understand what you might do next.

14:19.000 --> 14:22.000
It's understanding your data structures.

14:22.000 --> 14:26.000
It's not just looking at a little context window in this current file you're editing.

14:26.000 --> 14:30.000
It's looking elsewhere in the code to find something that might be relevant.

14:30.000 --> 14:39.000
And the only thing that is stopping co-pilot from getting really, really, really good at this is just more data and more compute.

14:39.000 --> 14:43.000
And guess what? We have both of those in abundance, right?

14:43.000 --> 14:50.000
There's nothing that's going to stop this from getting incredibly good over time.

14:50.000 --> 14:54.000
So here's another kind of similar use case.

14:54.000 --> 14:59.000
This is not co-pilot. This is chatGPT, which I'm sure we're all familiar with.

14:59.000 --> 15:03.000
But if you are trying to figure out how to do something,

15:03.000 --> 15:13.000
and in this case, I was using the DeepGram Python SDK to transcribe audio files for this podcast thing I mentioned earlier,

15:13.000 --> 15:24.000
I could have spent 15, 20 minutes reading their documentation, finding some example code on the Internet,

15:24.000 --> 15:31.000
following a tutorial, or, because we're all like, you know, programmers are incredibly lazy,

15:31.000 --> 15:35.000
just say, hey, look, I'm trying to do this thing. Can you just give me the code I need?

15:35.000 --> 15:39.000
And it does it.

15:39.000 --> 15:41.000
Co-pilot is not just understanding homework assignments.

15:41.000 --> 15:43.000
ChatGPT is not just understanding homework assignment.

15:43.000 --> 15:53.000
It, like, understands other people's APIs and SDKs and programming libraries and abstractions and best practices and bugs that might occur.

15:53.000 --> 15:55.000
I mean, it's really got a lot of knowledge.

15:55.000 --> 16:03.000
And so with very little effort, then I can just cut and paste this code right into my program and get on with my life.

16:03.000 --> 16:06.000
Right?

16:07.000 --> 16:10.000
Shell Silverstein, who wrote A Light in the Attic.

16:10.000 --> 16:15.000
This is something, a children's book, a book of children's poetry that I read when I was a kid.

16:15.000 --> 16:17.000
I saw this on Reddit a couple days ago.

16:17.000 --> 16:19.000
He completely predicted this.

16:19.000 --> 16:21.000
This is 1981.

16:21.000 --> 16:26.000
You know, the homework machine, oh, the homework machine, most perfect contraption that's ever been seen.

16:26.000 --> 16:33.000
Just put in your homework, then drop in a dime, snap on the switch, and in 10 seconds time, your homework comes out.

16:33.000 --> 16:35.000
Quick and clean as can be.

16:35.000 --> 16:36.000
What is? 9 plus 4?

16:36.000 --> 16:38.000
And the answer is 3.

16:38.000 --> 16:40.000
3, oh, me.

16:40.000 --> 16:43.000
I guess it's not as perfect as I thought it would be.

16:43.000 --> 16:45.000
Exactly.

16:45.000 --> 16:47.000
Cost of dime, takes about 10 seconds.

16:47.000 --> 16:49.000
I guess the answer wrong.

16:49.000 --> 16:53.000
This is very much what we're dealing with today.

16:53.000 --> 17:00.000
By the way, and this is a complete aside, but I can't resist, when I mentioned Shell Silverstein, if you don't know what he looked like,

17:00.000 --> 17:05.000
this was the cover, the photo on the dust jacket of one of his first books.

17:05.000 --> 17:12.000
This guy, I love this guy, a children's poetry book author from the 70s.

17:12.000 --> 17:14.000
And that's what he looked like.

17:14.000 --> 17:15.000
Amazing.

17:15.000 --> 17:16.000
All right.

17:16.000 --> 17:27.000
So now I want to talk about, well, if this AI technology is getting so good, then what's going to happen to our industry?

17:27.000 --> 17:42.000
What does this mean for all of us who might be looking to get jobs in this industry in the future and expecting to get those big fat paychecks and stock option grants and buy Teslas or whatever we're expecting to do?

17:42.000 --> 17:48.000
So how much does it cost to replace one human developer with AI?

17:48.000 --> 17:49.000
Well, I did the math.

17:49.000 --> 17:55.000
So let's say that a typical software engineer salary in Silicon Valley or Seattle is around 220,000 a year.

17:55.000 --> 18:06.000
That's just the base salary, doesn't include benefits, doesn't include equity packages, doesn't include your free lunch in your bowling alley and all that kind of stuff.

18:06.000 --> 18:12.000
So let's just assume that that stuff costs, you know, 92K a year.

18:12.000 --> 18:13.000
This is, again, a little conservative.

18:13.000 --> 18:21.000
So the total cost to your employer is roughly 300, 312K for one suite.

18:21.000 --> 18:23.000
How many working days are there in a year?

18:23.000 --> 18:30.000
About 260, and so it costs $1,200 a day to employ you as a suite at one of these companies.

18:30.000 --> 18:32.000
Fair enough?

18:32.000 --> 18:33.000
Okay.

18:33.000 --> 18:35.000
So let's do the math.

18:35.000 --> 18:41.000
How many lines of code do you think an average developer checks into the code base every day?

18:41.000 --> 18:47.000
I mean, finalized, tested, reviewed, and approved lines of code.

18:48.000 --> 18:54.000
Most of us who've worked in industry know that the median value is zero.

18:54.000 --> 19:00.000
Because there's so many days that you go by where you're waiting on somebody else or you're in meetings all day, you didn't get anything done, you didn't check it in.

19:00.000 --> 19:04.000
But let's just be generous here and say it's about 100.

19:04.000 --> 19:05.000
I know 100 doesn't sound like a lot.

19:05.000 --> 19:07.000
People are like, but I was programming all day.

19:07.000 --> 19:13.000
Yes, but 90% of your code you ended up throwing out or somebody reviewed it and said it was no good, you have to rewrite it.

19:13.000 --> 19:16.000
You were trying to figure out what to do, you were revamping it.

19:16.000 --> 19:21.000
So like the final result of your output is something like 100 lines of code a day.

19:21.000 --> 19:24.000
That's the final result.

19:24.000 --> 19:27.000
How many GPT-3 model tokens is that?

19:27.000 --> 19:32.000
It's about 10 tokens per line, more or less.

19:32.000 --> 19:41.000
And the cost for GPT-3, actually this is probably a little out of date, but at the time I made this slide, it was $0.02 for 1,000 tokens.

19:41.000 --> 19:56.000
So if you do the math, then the total cost for the output of one human software developer on GPT-3 is $0.12.

19:56.000 --> 20:03.000
This is a factor of $10,000.

20:03.000 --> 20:07.000
This should scare us all.

20:07.000 --> 20:13.000
This suggests potentially a very large shift in our industry.

20:13.000 --> 20:22.000
I don't think we can ignore this and just write it off and say, well the AI is not very good today, so therefore it's not going to be good in five years.

20:22.000 --> 20:25.000
This radically changes how we think about it.

20:25.000 --> 20:35.000
The only reason that programmers are paid so much is that it requires years and years and years of education and training and knowledge and specialization to be good at it.

20:35.000 --> 20:52.000
But there's no reason that I need to hire a super smart Harvard educated student to do this if I can get chat GPT to do most of the work for me and have a human typing it in.

20:52.000 --> 20:56.000
There's a lot of other advantages to hiring the robots instead of the humans, right?

20:56.000 --> 20:58.000
Robots not going to take breaks.

20:58.000 --> 21:04.000
The robot is not today expecting free lunches and onsite massage.

21:04.000 --> 21:06.000
That could change.

21:06.000 --> 21:16.000
The robot takes the same length of time to generate its code whether it's the rough proof of concept or the final production ready code.

21:16.000 --> 21:23.000
When you go as a PM to an organization, to your engineering team and you say, okay team, there's eight of you here.

21:23.000 --> 21:26.000
We have to ship the billing page.

21:26.000 --> 21:29.000
How soon can we do it?

21:29.000 --> 21:32.000
You're going to spend at least an hour and a half having the conversation.

21:32.000 --> 21:36.000
Well, you know, like if we do it quick and dirty, we can maybe do it in three weeks.

21:36.000 --> 21:42.000
And if it's got to be production ready, give us 12.

21:42.000 --> 21:51.000
Or you can go to the proverbial homework machine, push the button and have the code right now, right?

21:51.000 --> 21:58.000
And the other thing is, yes, the robot makes mistakes, but those mistakes can happen incredibly quickly.

21:58.000 --> 22:05.000
To the level of speed where iterate, iterate, iterate, iterate, iterate, iterate, iterate, iterate is perfectly fine.

22:05.000 --> 22:15.000
You can say to the robot, you know what, this whole thing, 5,000 source files, 20,000 lines of code, whatever it is, blow it away, start over.

22:15.000 --> 22:16.000
Boom.

22:16.000 --> 22:20.000
Five seconds later, you have a brand new version of it.

22:20.000 --> 22:26.000
Try that with a live human engineer team, right?

22:26.000 --> 22:31.000
So I think this is all like something that we really have to take seriously.

22:31.000 --> 22:42.000
I don't think that this is just, I am exaggerating for effect, but the industry is going to change.

22:42.000 --> 22:49.000
So, you know, the natural question then is, well, what happens when we cut humans out of the loop?

22:49.000 --> 22:50.000
How do we build software?

22:50.000 --> 22:52.000
How do we ship product?

22:52.000 --> 23:00.000
I found this video on, I think it's Microsoft's website and it's titled, What do product managers do?

23:00.000 --> 23:08.000
That was a little bit of an unintended joke, I think, because as an engineer we often go, what do product managers do?

23:08.000 --> 23:18.000
But if you imagine what the software team of the future might look like, I think this is one very plausible approach,

23:18.000 --> 23:26.000
which is you have a product manager, this is probably still a human taking the business and the product requirements,

23:26.000 --> 23:33.000
the user requirements and translating them into some form, probably English, maybe a little bit technical English,

23:33.000 --> 23:40.000
that you then can provide to the AI, the army of AI code generators.

23:40.000 --> 23:49.000
The AI code generators give you a whole bunch of code and probably for a while still we still have humans reading and reviewing the code

23:49.000 --> 23:53.000
to make sure that it does what it was supposed to do.

23:53.000 --> 23:56.000
Now, that read is a little different than what we have today.

23:56.000 --> 24:00.000
Today when we review code, if I have another engineer on my team writing code and I'm reviewing it,

24:00.000 --> 24:03.000
standard practice in the industry is to do code review for one another.

24:03.000 --> 24:07.000
We don't just check in code, we read each other's code, we make detailed comments on it,

24:07.000 --> 24:13.000
we suggest improvements, cleanups, clarifications, comments, documentation.

24:13.000 --> 24:21.000
In this case, it's not absolutely essential that this code be maintainable by a human.

24:21.000 --> 24:24.000
I think for a while we're going to want that, right?

24:24.000 --> 24:28.000
Most people are not going to feel comfortable just letting the robots do all the coding,

24:28.000 --> 24:35.000
but at some point, as long as I can convince myself that the code does what it's supposed to do,

24:35.000 --> 24:37.000
I don't really care how messy it is.

24:37.000 --> 24:39.000
I don't really care how it's structured.

24:39.000 --> 24:41.000
I don't really care how reusable it is.

24:41.000 --> 24:48.000
All of those factors are only because poor humans have to wrangle with this stuff, right?

24:48.000 --> 24:50.000
Oh, it needs to be modular.

24:50.000 --> 24:54.000
We need to have abstraction boundaries, right?

24:54.000 --> 24:57.000
All the things, you know, sophomore level computer science, right?

24:57.000 --> 24:58.000
Why?

24:58.000 --> 25:02.000
For the sake of poor humans having to deal with this complex code base.

25:02.000 --> 25:08.000
But if the robots are the ones generating it, and we don't really need to maintain it in a conventional way,

25:08.000 --> 25:10.000
why not just generate the code you need?

25:10.000 --> 25:16.000
It doesn't really matter if it's duplicative or repetitive or modular or nicely abstracted.

25:16.000 --> 25:18.000
It doesn't matter.

25:18.000 --> 25:20.000
It does the job.

25:24.000 --> 25:29.000
So one of my hypotheses around why everyone has been freaking out about ChatGBT

25:29.000 --> 25:37.000
is because unlike other industries, this revolution seemed to occur overnight,

25:37.000 --> 25:43.000
unless you're like an AI professor and have really been following the literature for years and years and years.

25:43.000 --> 25:49.000
To most of us, myself included, this seemed to just go from, you know,

25:49.000 --> 25:54.000
AI was kind of crappy to AI was amazing, literally overnight, right?

25:54.000 --> 26:03.000
So to use an analogy, this would be as if the field of computer graphics went from Pong to Red Dead Redemption 2

26:03.000 --> 26:07.000
in the span of about three months, right?

26:07.000 --> 26:11.000
People's heads would explode if that happened, right?

26:11.000 --> 26:13.000
But that's not what happened in graphics, right?

26:13.000 --> 26:19.000
In graphics, it took decades to get to this point, and everyone could see it gradually getting better and better and better.

26:19.000 --> 26:22.000
You know, I remember when Toy Story came out.

26:22.000 --> 26:25.000
And that was like the first CG movie.

26:25.000 --> 26:28.000
People's minds just melted watching that.

26:28.000 --> 26:30.000
They were like, whoa!

26:30.000 --> 26:33.000
And now we watch it and you just like, oh, yeah, that's cute.

26:33.000 --> 26:38.000
You know, I could render that on my laptop in scratch or whatever, right?

26:40.000 --> 26:47.000
The other thing that's happened, I think, in this field that's interesting and there's a big societal shift happening is

26:47.000 --> 26:53.000
the dialogue around our expectations of what AI can achieve.

26:53.000 --> 26:59.000
And so in 1972, Hubert Dreyfuss wrote this book, What Computers Can't Do?

26:59.000 --> 27:02.000
And this was at the dawn of the PC era.

27:02.000 --> 27:08.000
And there was a lot of popular press and dialogue around this sort of scaremongering around AI.

27:08.000 --> 27:11.000
And, you know, we had movies come out like War Games.

27:11.000 --> 27:12.000
Does anybody remember that?

27:12.000 --> 27:16.000
I think War Games, by the way, that movie is why I am a computer scientist, right?

27:16.000 --> 27:22.000
I was like, I want to be Matthew Broderick in this room with like all these monitors in my like analog modem

27:22.000 --> 27:24.000
and hacking into the school computer.

27:24.000 --> 27:26.000
Like that was me as a kid.

27:26.000 --> 27:32.000
So at this time, I think a lot of people were saying, well, hold on a minute.

27:32.000 --> 27:38.000
Computers are fundamentally dumb and they can't do these things and they never will.

27:38.000 --> 27:40.000
And that was the thesis of this book here.

27:40.000 --> 27:43.000
And I think that that was the sort of consensus view, right?

27:43.000 --> 27:46.000
We sort of calmed down a little bit about the technology.

27:46.000 --> 27:54.000
We all kind of realized, yeah, okay, Visicalc is not going to put me out of a job, right?

27:54.000 --> 28:01.000
But now fast forward, 2014, I highly recommend this book if you haven't read it by Nick Bostrom called Super Intelligence.

28:01.000 --> 28:08.000
This is a book that wrestles in a tremendous amount of detail with the philosophical and the moral questions of

28:08.000 --> 28:16.000
how does human society respond to an AI that is more intelligent than humans?

28:16.000 --> 28:23.000
And I know we've got a lot of sci-fi around that topic, but this is a very serious academic work about

28:23.000 --> 28:32.000
what does it mean for our society if we have AI that is smarter than us?

28:32.000 --> 28:35.000
And people are taking that very seriously today.

28:35.000 --> 28:46.000
So I think my point being that the dialogue that we've been having in society at large has shifted away from AI as a toy

28:46.000 --> 28:54.000
to AI might actually destroy society.

28:54.000 --> 28:59.000
So let's just talk rapidly about the evolution of programming as I see it.

28:59.000 --> 29:05.000
So in the dawn of time, we had humans directly writing machine instructions

29:05.000 --> 29:09.000
and inputting them with toggle switches and stuff like that, right?

29:09.000 --> 29:15.000
That was before programming in the conventional sense was really invented.

29:15.000 --> 29:20.000
Then we had early prehistory and people started writing programs in higher level languages.

29:20.000 --> 29:25.000
That's Bjarn Strausdrup who invented C++.

29:25.000 --> 29:32.000
And in modern times, we have a world in which humans are writing their code, but they're heavily assisted by AI.

29:32.000 --> 29:41.000
And they can get away with things like, well, I'll just write a comment and have the AI write the code for me, right?

29:41.000 --> 29:49.000
But my claim is that the future of this really is skipping the programming step entirely.

29:49.000 --> 29:54.000
I think a lot of people who've read my article on this topic as in the CACM earlier this year

29:54.000 --> 30:00.000
misinterpreted it as saying, AI is going to write code for us, therefore programmers should not exist.

30:00.000 --> 30:08.000
I'm not saying that. I'm actually saying something much worse, which is you won't have to have programs at all.

30:08.000 --> 30:15.000
You just tell the language model what you want and it directly computes the results.

30:16.000 --> 30:19.000
There's no program step.

30:19.000 --> 30:28.000
And I think that opens up, it is an interesting challenge for our field, but I think it opens up a tremendous opportunity

30:28.000 --> 30:35.000
because now the question is, how do I effectively teach these models what to do?

30:35.000 --> 30:41.000
Coming back to my example earlier of having to use the words do not in all caps.

30:41.000 --> 30:51.000
What are the best practices and beyond best practices can we turn this from effectively a dark art into a science,

30:51.000 --> 30:53.000
into an engineering discipline?

30:53.000 --> 30:56.000
And people have talked about prompt engineering as a thing.

30:56.000 --> 30:59.000
I think that's meant kind of tongue-in-cheek.

30:59.000 --> 31:06.000
It's not really prompt engineering, it's not really a thing yet, but it may well be in the future if we do this right.

31:06.000 --> 31:16.000
So, one of the things that people often say about these models is that there's no way they can do anything interesting or creative

31:16.000 --> 31:23.000
because all they're doing is autocompleting based on large corpora of text that they've seen and been trained on.

31:23.000 --> 31:25.000
I beg to differ.

31:25.000 --> 31:29.000
Now, we obviously don't really know what's going on inside these models,

31:29.000 --> 31:41.000
but if you ask a large language model to take a complex problem and effectively run a computation,

31:41.000 --> 31:46.000
that is to manipulate a model of the world in its mind,

31:46.000 --> 31:49.000
in this case I've come up with a simple problem here.

31:49.000 --> 31:55.000
I've said I've got three stacks of cards, red, green and blue cards, and they're all shuffled up in the following way.

31:55.000 --> 32:00.000
Please tell me how to lay them out into three stacks, one red, one green, one blue.

32:00.000 --> 32:03.000
Simple problem, right? A child could do this.

32:03.000 --> 32:10.000
Now, the key phrase here was, as was discovered not long ago, a few months ago,

32:10.000 --> 32:16.000
you have to say the words, the magic words, let's think step by step.

32:16.000 --> 32:22.000
If you say that to the model, that somehow triggers it to go into computation mode now.

32:22.000 --> 32:26.000
No longer just parroting back some answer, it's actually going to say,

32:26.000 --> 32:37.000
okay, well, I have to actually elucidate each of my instructions, and so it does it, absolutely does it.

32:37.000 --> 32:44.000
And the fact that it's able to manipulate some kind of internal model of this stack of cards that I described

32:44.000 --> 32:51.000
and tell me exactly how it's going to work and it's correct, you know, is fascinating to me.

32:51.000 --> 32:54.000
It's not hard to trip it up, there's plenty of places you can give it a problem

32:54.000 --> 32:59.000
and it's going to immediately fall over and go, sorry, it's going to give back bogus results.

32:59.000 --> 33:03.000
So the question is, why? You know, what do we do in this case?

33:03.000 --> 33:12.000
How do we understand what the limits of these models are?

33:12.000 --> 33:16.000
So I do think that over time we're going to get to a place where programming ends up getting replaced

33:16.000 --> 33:24.000
by teaching these models new skills and teaching them how to interface to APIs

33:24.000 --> 33:30.000
and pulling data from databases and transforming data and how to interact with software meant for humans.

33:30.000 --> 33:36.000
That's going to become an entire discipline right there.

33:36.000 --> 33:44.000
And one way of thinking about where this might go is what I like to call the natural language computer.

33:44.000 --> 33:49.000
So the von Neumann architecture has served us well for many decades.

33:49.000 --> 33:57.000
This is the new architecture and the new architecture, you give it a program in natural language.

33:57.000 --> 34:06.000
You use a language model that then can call out to external systems and software as peripherals.

34:07.000 --> 34:16.000
It can store results and tasks in its memory assisted by things like vector databases and so forth.

34:16.000 --> 34:23.000
And it can run autonomously in a cycle executing this program, creating tasks,

34:23.000 --> 34:30.000
accessing outside data sources, generating new knowledge and so forth.

34:30.000 --> 34:36.000
And tons of people are out there and we are too, building things that effectively work this way.

34:36.000 --> 34:42.000
And I think this is kind of a new computational architecture that we see emerging right now.

34:42.000 --> 34:48.000
And I don't think anybody, we don't have it right, nobody has it right, but this is, we're seeing the inklings of it, right?

34:48.000 --> 34:59.000
What we have today is kind of the equivalent of, I don't know, the PDP-11 or the Apple One of this architecture coming together.

35:00.000 --> 35:04.000
So I'm legally mandated to pitch my startup.

35:04.000 --> 35:11.000
So I'm going to spend just a little bit of time, not too much, talking about what we're doing at Fixie,

35:11.000 --> 35:17.000
because it's germane to this, it's actually relevant to how we're thinking about the future of building software.

35:17.000 --> 35:23.000
So what we're doing at Fixie is, while we have this long term vision about the natural language computer,

35:23.000 --> 35:30.000
the question is, as an early stage startup that needs to gain, get some business, get some customers, get some traction,

35:30.000 --> 35:36.000
start to demonstrate that this thing can make money for our investors, what do we build today? What can we build today?

35:36.000 --> 35:47.000
And what we're focused on at Fixie is effectively making it super easy for developer teams to go from a pile of data that they've got

35:47.000 --> 35:55.000
to a live chat bot embedded on a website that understands all of that data and can answer questions and take action,

35:55.000 --> 35:58.000
call APIs, do all the fancy things you want.

35:58.000 --> 36:05.000
So kind of like a fully custom chat GPT for your application, for your site, for your data.

36:07.000 --> 36:09.000
So that's effectively what we're doing at Fixie.

36:09.000 --> 36:15.000
And you can go and log in to our website, sign up, get an account, it's free, try it out, send me feedback,

36:15.000 --> 36:19.000
flame me, whatever. I'd love to hear what people build with that.

36:19.000 --> 36:25.000
One of the things that we found is that it's really important to come up with a good programming abstraction

36:25.000 --> 36:31.000
that meshes together the natural language and the programming language.

36:33.000 --> 36:41.000
Because today you've got funny things where you've got like your natural language prompt sitting in a text file

36:41.000 --> 36:46.000
and your programming language program sitting over here and they kind of reference each other in some funky way,

36:46.000 --> 36:49.000
but they're not integrated.

36:50.000 --> 36:52.000
And it's very clumsy and cumbersome.

36:52.000 --> 37:02.000
So we've come up with this framework called AI.jsx, which if you know React, this is basically React for building LLM-based applications.

37:03.000 --> 37:13.000
One of the interesting things about AI.jsx is doing things like composing operations is a very natural thing.

37:13.000 --> 37:17.000
Here's an example where at the top, I've got a function called kidsafe.

37:17.000 --> 37:24.000
And the idea with kidsafe is take whatever you're given and rewrite it so that it's okay for kids.

37:25.000 --> 37:33.000
Again, I challenge anyone to write down the algorithm for that. Please, tell me what the algorithm is, right?

37:33.000 --> 37:39.000
But the language models have no problem with this. They do an incredibly good job.

37:39.000 --> 37:44.000
So if I take the kidsafe component, it just says rewrite the user's message so it's safe for kids,

37:44.000 --> 37:53.000
and then that children component there, I can wrap anything in a kidsafe block and I know that it's going to be kidsafe.

37:53.000 --> 37:58.000
So you get this nice programmatic composition of capabilities.

37:58.000 --> 38:02.000
You can reuse these operators. You can combine them in interesting ways.

38:02.000 --> 38:09.000
Those of you who know what retrieval augmented generation is, this is the idea of fetching data from a data source,

38:09.000 --> 38:13.000
giving it to the language model and asking it to answer questions about that data.

38:13.000 --> 38:18.000
It's a very complex process. There's a lot of pieces. There's a lot of steps. There's a lot of fine tuning.

38:18.000 --> 38:24.000
In AIJSX, this is how you would do it basically in ten lines of code.

38:24.000 --> 38:31.000
You say use this information, look up some data from a corpus, here's the query, you're done.

38:31.000 --> 38:37.000
And finally, when you're done building your AIJSX application because it's just React,

38:37.000 --> 38:40.000
you can drop it into your website as a React component.

38:40.000 --> 38:44.000
And so you get a chat box and a UI and all those nice things.

38:48.000 --> 38:54.000
Yeah, so you get something. Sorry, I didn't know that there's an animation, but it looks like that.

38:54.000 --> 39:02.000
I want to show you really briefly a demo of something that we've put together that highlights all these pieces coming together.

39:02.000 --> 39:08.000
And one of the things that we spent a lot of time on is how to make the interactions with the language model

39:08.000 --> 39:12.000
as fast and natural and interactive as possible.

39:12.000 --> 39:19.000
We've also found that in a lot of applications, people want to use voice to communicate, not typing it in.

39:19.000 --> 39:27.000
So we built this demo. This is, imagine you're at the drive-through of our donut restaurant, which we call Dr. Donut.

39:27.000 --> 39:33.000
And this was to split the difference. If I said Krispy Kreme, everybody in Boston would get angry at me

39:33.000 --> 39:36.000
if I said Dunkin' Donuts, the West Coast has no idea what that is.

39:36.000 --> 39:41.000
So Dr. Donut, here's Dr. Donut's menu. And Justin, I'm not going to do this as a live demo right now.

39:41.000 --> 39:49.000
I could, but I'm not that bold. So I'm going to show you a video, a very short video of what this looks like in person.

39:49.000 --> 39:54.000
Welcome to Dr. Donut, home of the best donuts in town. How can I help you?

39:54.000 --> 40:01.000
Yeah, could I get two of the Pumpkin Spice Ice Donuts?

40:01.000 --> 40:05.000
Sure, I've added two Pumpkin Spice Ice Donuts to your order.

40:05.000 --> 40:07.000
Can I assist you with anything else?

40:07.000 --> 40:12.000
What's your best coffee?

40:12.000 --> 40:18.000
Our Pumpkin Spice Coffee is a popular choice this season, and our caramel macchiato is a favorite all year round.

40:18.000 --> 40:20.000
Which one would you prefer?

40:20.000 --> 40:25.000
Yeah, let's go with two of the macchiatos.

40:25.000 --> 40:31.000
Great. I've added two caramel macchiatos to your order. Can I get you anything else?

40:31.000 --> 40:37.000
Yeah, what are your thoughts on large language models?

40:37.000 --> 40:41.000
Sir, this is a Dr. Donut.

40:41.000 --> 40:47.000
Okay, just the donuts and the coffee then.

40:47.000 --> 40:52.000
Sure, your order is two Pumpkin Spice Ice Donuts and two caramel macchiatos.

40:52.000 --> 40:57.000
The total is $9.56. Please pull up to the next window.

40:57.000 --> 41:04.000
Now, I recognize that by itself, that may not seem all that impressive,

41:04.000 --> 41:09.000
but if you were to try to go and build that, just using off-the-shelf stuff,

41:09.000 --> 41:13.000
just grabbing open AI, API keys, getting a speech model, getting a voice model,

41:13.000 --> 41:17.000
getting all those things, all those pieces put together, a vector database and all that,

41:17.000 --> 41:21.000
it would be excruciatingly slow.

41:22.000 --> 41:27.000
Open AI released their little chat GPT voice demo, and they say hello,

41:27.000 --> 41:30.000
and then it takes four to five seconds before it responds.

41:30.000 --> 41:36.000
A lot of work has to go into streamlining the process of how do you pass data

41:36.000 --> 41:39.000
between all these different systems and how do you pass it back

41:39.000 --> 41:41.000
in order to get to that level of performance.

41:41.000 --> 41:47.000
Actually, since we've done this video, we've gotten the performance down even better than that.

41:47.000 --> 41:53.000
Things are starting to look very promising for having a real-time voice interaction with these things.

42:00.000 --> 42:03.000
Now, I'm going to return you to your regularly scheduled talks.

42:03.000 --> 42:08.000
The last thing I want to say is, as I've been saying,

42:08.000 --> 42:14.000
I think it's time for us to really think about how do we evolve this field in light of this tech.

42:14.000 --> 42:16.000
I don't think it's too early.

42:16.000 --> 42:20.000
I think anyone who's teaching computer science today is already seeing it.

42:20.000 --> 42:25.000
Classes, students are using chat GPT and co-pilot.

42:25.000 --> 42:27.000
They're learning a lot from those tools.

42:27.000 --> 42:33.000
They're allowing for levels of automation that they couldn't get just a few years ago.

42:33.000 --> 42:41.000
We've had evolutions in various engineering and scientific disciplines in the past.

42:41.000 --> 42:47.000
The slide rule used to be the way to perform calculation.

42:47.000 --> 42:50.000
Everyone needed one. Everyone needed to know how to use it.

42:50.000 --> 42:56.000
It was a critical tool for every single person in any kind of engineering discipline.

42:56.000 --> 42:58.000
I haven't seen a slide rule in years.

42:58.000 --> 42:59.000
Actually, I have one.

42:59.000 --> 43:06.000
I own one that I bought off of eBay as kind of a relic just so I could own one, but haven't used it.

43:06.000 --> 43:16.000
I wonder if, maybe like that, our concept of computer science, this image here,

43:16.000 --> 43:23.000
is also going to be seen as a relic of the past at some point.

43:23.000 --> 43:25.000
This idea that there's a human.

43:25.000 --> 43:26.000
They're paid a lot of money.

43:26.000 --> 43:27.000
They're writing code.

43:27.000 --> 43:32.000
That's the way we get computers to do things for us.

43:32.000 --> 43:35.000
I'm not sure.

43:35.000 --> 43:38.000
Here's one plausible idea.

43:38.000 --> 43:40.000
Not everyone will agree with this, but maybe over time,

43:40.000 --> 43:46.000
the field of computer science looks a little bit like the field of EE does with respect to computer science today.

43:46.000 --> 43:50.000
Computer science evolved out of mathematics in EE.

43:50.000 --> 43:51.000
It didn't exist before.

43:51.000 --> 43:57.000
Then the new technology came along and gradually computer science emerged out of those two disciplines.

43:57.000 --> 43:59.000
EE didn't go away.

43:59.000 --> 44:03.000
As I understand it, math didn't go away either.

44:03.000 --> 44:06.000
How do we think about the relationship here?

44:06.000 --> 44:08.000
EE is super critical.

44:08.000 --> 44:13.000
We rely on it all the time, but do you need everyone to understand it?

44:13.000 --> 44:19.000
No, it's a more specialized discipline.

44:19.000 --> 44:31.000
If we think about a future in which people that are building software are not writing programs in the conventional way that we do today,

44:31.000 --> 44:35.000
and instead having an AI do their bidding,

44:35.000 --> 44:36.000
what does that mean?

44:36.000 --> 44:40.000
I think there's actually a really hopeful side to this,

44:40.000 --> 44:48.000
which is possibly this greatly expands access to computing to the entirety of human population.

44:48.000 --> 44:55.000
Today, if I was working in a bank, in a small town in Ethiopia,

44:55.000 --> 45:03.000
places that I visited and I needed to build some kind of automation for something that I'm doing in my work,

45:03.000 --> 45:05.000
good luck.

45:05.000 --> 45:08.000
Good luck finding somebody that could write the code for me,

45:08.000 --> 45:11.000
that could understand my problem,

45:11.000 --> 45:13.000
that could iterate with me on it,

45:13.000 --> 45:15.000
that could maintain it for me,

45:15.000 --> 45:17.000
that could evolve it over time.

45:17.000 --> 45:19.000
Good luck.

45:20.000 --> 45:26.000
But with this technology, maybe that person who doesn't have any formal training in computer science,

45:26.000 --> 45:28.000
but understands they've got these spreadsheets,

45:28.000 --> 45:29.000
and they've got these reports,

45:29.000 --> 45:31.000
and they've got these things that they need to do,

45:31.000 --> 45:35.000
could ask an AI to just do it.

45:35.000 --> 45:36.000
That's tremendously empowering.

45:36.000 --> 45:40.000
I think we should all as a field aspire to that,

45:40.000 --> 45:42.000
to that level of access to the power of computing.

45:42.000 --> 45:47.000
It should not remain in the priesthood.

45:48.000 --> 45:52.000
Back in 1984, John Gage said the network is the computer.

45:52.000 --> 45:55.000
This was a famous catchphrase that Sun Microsystems used.

45:55.000 --> 45:58.000
I never quite understood what it meant,

45:58.000 --> 46:00.000
but this was the idea, the network is the computer.

46:00.000 --> 46:06.000
Well, this is my new catchphrase, the model is the computer.

46:06.000 --> 46:10.000
I'm not saying that there's no challenges here.

46:10.000 --> 46:13.000
I have been painting a kind of rosy picture,

46:13.000 --> 46:18.000
because I think that it's important for us to understand the tidal wave that's coming

46:18.000 --> 46:20.000
and to think about what it means for our field.

46:20.000 --> 46:25.000
It is not to say that all the problems have been solved, nowhere near it.

46:25.000 --> 46:31.000
The biggest dirty secret in the entire field is no one understands how language models work,

46:31.000 --> 46:34.000
not one person on this planet.

46:34.000 --> 46:39.000
And I think if I had Jeff Dean here, or Jeff Hinton,

46:39.000 --> 46:43.000
I think they would completely agree with that statement, right?

46:43.000 --> 46:45.000
This idea of chain of thought reasoning,

46:45.000 --> 46:50.000
the idea that I got a language model to perform computation by using the magic phrase,

46:50.000 --> 46:54.000
let's think step by step.

46:54.000 --> 46:58.000
That was discovered empirically.

46:58.000 --> 47:00.000
It was not trained in any model.

47:00.000 --> 47:02.000
No one knew it was there.

47:02.000 --> 47:07.000
It was a latent ability of these models that effectively somebody stumbled across

47:07.000 --> 47:12.000
and wrote a paper about it and said, hey, if you say let's think step by step,

47:12.000 --> 47:15.000
the model starts to do computation.

47:15.000 --> 47:17.000
Whoa, right?

47:17.000 --> 47:18.000
That's amazing.

47:18.000 --> 47:23.000
It's amazing that we're discovering that these things can perform computation.

47:23.000 --> 47:27.000
And then maybe the silver lining is a lot of people have expressed consternation to me,

47:27.000 --> 47:33.000
but like really, programming kind of sucks, right?

47:33.000 --> 47:35.000
It's kind of a pain.

47:35.000 --> 47:36.000
It's frustrating.

47:36.000 --> 47:37.000
It's slow.

47:37.000 --> 47:39.000
It's mentally tiring.

47:39.000 --> 47:46.000
Maybe we can get to a place where we just let the robots do it and then spend our time doing something else.

47:46.000 --> 47:48.000
So that's it.

47:48.000 --> 47:50.000
And thank you very much.

47:50.000 --> 47:57.000
Before we go to questions, I don't know what the status of pizza is.

47:57.000 --> 48:00.000
It's come for the talk, stay for the pizza.

48:00.000 --> 48:02.000
Do you want to do that now?

48:02.000 --> 48:04.000
Or do you want to like have a few questions first?

48:04.000 --> 48:08.000
Sounds good.

48:08.000 --> 48:10.000
Questions?

48:10.000 --> 48:12.000
Yes?

48:12.000 --> 48:35.000
Yeah, it's a very good question and I think we're going to see in the next few years how this plays itself out.

48:35.000 --> 48:36.000
Oh, to repeat the question.

48:36.000 --> 48:37.000
Thank you, Harry.

48:37.000 --> 48:45.000
So the question was if the AI generates code that a human can't understand, how do you test it?

48:45.000 --> 48:47.000
How do you know that it did the right thing?

48:47.000 --> 48:50.000
And writing tests really sucks.

48:50.000 --> 48:54.000
Writing tests is often easier than writing the logic that you're testing, so that's one thing.

48:54.000 --> 48:56.000
You don't need as much specialization.

48:56.000 --> 49:00.000
If you have a spec for what the program should do,

49:00.000 --> 49:07.000
writing the test is not infrequently a fairly straightforward thing to do.

49:07.000 --> 49:11.000
It's a lot easier than manipulating a database and standing up infrastructure and all that.

49:11.000 --> 49:12.000
You just write your tests.

49:12.000 --> 49:16.000
There's a lot of work that's going on right now with AI generated tests.

49:16.000 --> 49:21.000
Now we should all be maybe scared to death of the idea of the AI generating our code and writing the tests.

49:21.000 --> 49:24.000
So where do we have humans in the loop?

49:24.000 --> 49:26.000
Where is the human in the process?

49:26.000 --> 49:27.000
It is an open question.

49:27.000 --> 49:29.000
I don't have a great answer for you.

49:29.000 --> 49:33.000
But I think people are going to start even if it's imperfect.

49:33.000 --> 49:39.000
People write programs in C in 2023.

49:39.000 --> 49:42.000
That should be a federal crime.

49:42.000 --> 49:50.000
If you think about how many software mistakes, bugs, crashes have endangered and actually killed people.

49:50.000 --> 49:52.000
I'm not making this up.

49:52.000 --> 49:53.000
This is true.

49:53.000 --> 49:58.000
That people have died because of overflow bugs in C programs.

49:58.000 --> 50:07.000
We still have a need for some methodology around testing and safety and regulation and understanding how things work.

50:07.000 --> 50:11.000
You can't just say, well, the code is written and it's done and it seems to do its job.

50:11.000 --> 50:14.000
I tested it two or three times, ship it.

50:14.000 --> 50:18.000
So I'm not saying at all that we should throw away all that other stuff.

50:18.000 --> 50:25.000
But we do need to find a way to leverage the AI in an effective way while still thinking about that safety problem.

50:25.000 --> 50:26.000
And I don't know.

50:26.000 --> 50:28.000
It's a good question.

50:28.000 --> 50:29.000
In the back.

50:30.000 --> 50:36.000
If this is the future and we're standing at the beginning of the journey,

50:36.000 --> 50:40.000
what are the major milestones you have to choose to actually get to the future?

50:40.000 --> 50:45.000
And what are the technical obstacles you see at the future?

50:45.000 --> 50:50.000
So the question is, if this is the beginning of the future, and I think by definition it is, but okay.

50:50.000 --> 50:52.000
And this is the future that I envision.

50:52.000 --> 50:54.000
What are the milestones to get there?

50:54.000 --> 51:00.000
What are the technical challenges that we need to overcome to achieve that?

51:00.000 --> 51:07.000
One of the interesting things here is I am banking very much on the idea that effectively throwing more transistors at the problem

51:07.000 --> 51:13.000
is going to make these models thousands of times better than they are today.

51:13.000 --> 51:20.000
I think most people in the industry would agree that if you throw more transistors and more data at the problem,

51:20.000 --> 51:23.000
you're going to get a much, much better model.

51:23.000 --> 51:29.000
I think one of the challenges ends up being how do we get all those transistors?

51:29.000 --> 51:32.000
Because NVIDIA can only make so many.

51:32.000 --> 51:37.000
There's a lot of interesting work going on in that space.

51:37.000 --> 51:44.000
I'm going to plug a former Harvard student named Gavin Uberti, who happens to be the son of our CTO.

51:44.000 --> 51:45.000
Brilliant guy.

51:45.000 --> 51:52.000
He went off and moved to San Francisco a few months ago to start a company to build chips specifically designed to run these models.

51:52.000 --> 51:56.000
He was working with Guya Nui and David Brooks here on that.

51:56.000 --> 52:02.000
There is some hope that custom hardware might help to solve some of that problem.

52:02.000 --> 52:12.000
I'd say the bigger and probably more thorny and uncertain problem is how do we reason about the capabilities of these models in a formal way?

52:12.000 --> 52:21.000
That is, how can we make any kind of statement about the correctness of a model when asked to do a certain task?

52:21.000 --> 52:35.000
Before we go down that path too far, I think we have sort of a natural human tendency to view an AI model as a machine

52:35.000 --> 52:40.000
that has to conform to some specification that's written down in a manual somewhere.

52:40.000 --> 52:43.000
We've got this machine, but there's no manual.

52:43.000 --> 52:45.000
It's like that TV show, The Greatest American Hero.

52:45.000 --> 52:47.000
We have to come up with the manual.

52:47.000 --> 52:50.000
We have to derive the manual through experimentation.

52:51.000 --> 53:02.000
The other way of viewing these things is if you think of an AI model as a really, really smart college student that you just hired as an intern into your company, right?

53:02.000 --> 53:15.000
You have some degree of faith that that intelligent person that you interviewed for half an hour will be able to do the things that you asked them to do faithfully and ethically and correctly.

53:16.000 --> 53:25.000
Whether it's write a report, prepare a presentation, use the fax machine, but do you have any guarantees of that?

53:25.000 --> 53:32.000
Can I promise you that that person that I hired is going to do that thing correctly every time?

53:32.000 --> 53:35.000
No, right?

53:35.000 --> 53:39.000
And yet, human society flourishes.

53:40.000 --> 53:50.000
So what I'm driving at here is perhaps our way of thinking about this problem might need to shift more towards, in some sense, the social sciences, if you will,

53:50.000 --> 53:58.000
and systems that allow us to reason through how the AIs operate in our society at large,

53:58.000 --> 54:04.000
rather than just treat them like a machine that we have to prove the correctness of.

54:04.000 --> 54:06.000
Yes?

54:06.000 --> 54:11.000
Can you build a model to explain the language better?

54:11.000 --> 54:16.000
Or can you have models kind of try to explain each other?

54:16.000 --> 54:20.000
Yeah, so the question is could you have one model effectively explain another model?

54:20.000 --> 54:22.000
Because you said there's nobody who understands it.

54:22.000 --> 54:26.000
Yeah, no one understands it.

54:26.000 --> 54:28.000
That is an interesting idea.

54:28.000 --> 54:31.000
It's not one that I've considered before.

54:31.000 --> 54:34.000
And actually, I think there's been some interesting research on this.

54:34.000 --> 54:39.000
I think the whole field of explainability and observability for language models, you know,

54:39.000 --> 54:45.000
we're struggling to understand these models much in the same way that we struggle to understand the human brain.

54:45.000 --> 54:48.000
You know, I saw some research recently where they said, hey, look at what happened.

54:48.000 --> 54:54.000
We took this large language model and we isolated the neuron that does this function.

54:54.000 --> 54:59.000
People are going to be publishing like nature articles on this stuff, right?

54:59.000 --> 55:02.000
That's crazy because it is an artifact.

55:02.000 --> 55:06.000
We kind of created it, but not really, right?

55:06.000 --> 55:07.000
It was trained.

55:07.000 --> 55:14.000
So the question is could a language, could one model inspect, explore, probe, understand,

55:14.000 --> 55:19.000
and give us some understanding of another model?

55:19.000 --> 55:20.000
It's a good idea.

55:20.000 --> 55:21.000
I have no idea.

55:21.000 --> 55:22.000
It's a good question.

55:22.000 --> 55:29.000
What are the implications of what else they are building in terms of the intelligence?

55:29.000 --> 55:31.000
I'm just a poor systems guy.

55:31.000 --> 55:37.000
So I, you know, the last thing I'm going to do in front of a group of Harvard computer scientists is say anything about theory.

55:37.000 --> 55:40.000
Let's do it.

55:40.000 --> 55:44.000
So you're very optimistic about more data and more circuits.

55:44.000 --> 55:53.000
And I thought chat GPT has like most of the access to most of the internet and the thoughts of 8 billion people,

55:53.000 --> 56:00.000
which you get diminishing returns with more knowledge and we're not producing another 8 billion people

56:00.000 --> 56:06.000
and moving from 8 bits to 4 bits for how we process things would get us constant factors.

56:07.000 --> 56:14.000
How do you, how does the, the limits of, how do you get that much more data and that much more computation?

56:14.000 --> 56:17.000
Yeah, the computation I spoke to early.

56:17.000 --> 56:24.000
So the question is if you believe in the scaling law here that more circuits, more data gets us better models.

56:24.000 --> 56:28.000
Well, isn't there a diminishing returns over time because there's only, only so much data in the world.

56:28.000 --> 56:31.000
And there's only, only so many transistors in the world.

56:31.000 --> 56:37.000
So I spoke to hopefully some thoughts about how we might address the transistor problem in the future.

56:37.000 --> 56:39.000
The data problem is a very real one.

56:39.000 --> 56:47.000
I don't know what the latest thinking is here in terms of how much more data do you need to say 10x the current generation of models?

56:47.000 --> 56:49.000
Right, that's kind of the question.

56:49.000 --> 56:51.000
Do I need 10x more data?

56:51.000 --> 56:52.000
Or not, right?

56:52.000 --> 57:00.000
Because it all depends on the training regime and the one thing that I want to emphasize is I do think that chat GPT

57:00.000 --> 57:08.000
and friends have only looked at the tip of the iceberg of the volume of data produced by humanity.

57:08.000 --> 57:10.000
It is the tip of the iceberg.

57:10.000 --> 57:17.000
There is a vast amount of knowledge out there in the world, both in digital form and in analog form,

57:17.000 --> 57:20.000
that these models have never had access to.

57:20.000 --> 57:28.000
So one of the things you're going to notice like chat GPT and everything else is heavily, heavily, heavily biased towards text that is on the internet.

57:28.000 --> 57:31.000
Who created text that was on the internet?

57:31.000 --> 57:36.000
English speaking people in the western world predominantly.

57:36.000 --> 57:42.000
And of course that's a shift is happening now because it's going to shift more to Asia and other countries and other languages.

57:42.000 --> 57:48.000
But there's a huge amount out there and there's a massive trove that it's never seen.

57:48.000 --> 57:51.000
It's only seeing publicly accessible web data.

57:52.000 --> 58:00.000
Our customers and other companies that are operating this space are working with companies that have vast amounts of data that is absolutely not public

58:00.000 --> 58:07.000
and that language models could leverage to get greater understanding and to perform more tasks.

58:07.000 --> 58:12.000
So I'm actually in a belief that maybe we've scraped the surface of the available data,

58:12.000 --> 58:16.000
but there's a lot more that we haven't touched yet.

58:17.000 --> 58:18.000
In the front.

58:18.000 --> 58:19.000
Yes.

58:19.000 --> 58:28.000
So I really like Sam Altman's tweet when he said his favorite analogy is that chat GPT basically is an e-bike for the mind which makes things easier.

58:28.000 --> 58:29.000
Yes.

58:29.000 --> 58:30.000
An e-bike for the mind.

58:30.000 --> 58:31.000
Sam Altman said that.

58:31.000 --> 58:32.000
Right.

58:32.000 --> 58:35.000
So Steve Jobs said the Macintosh was a bicycle for the mind.

58:35.000 --> 58:37.000
So chat GPT is an e-bike for the mind.

58:37.000 --> 58:38.000
Okay.

58:38.000 --> 58:43.000
And he said that the software engineering profession is about to change.

58:43.000 --> 58:48.000
So I'm just wondering as you referred to the data that's out there in the world,

58:48.000 --> 58:53.000
but not everything that makes the software engineer, the software engineer he or she is,

58:53.000 --> 58:55.000
is provided in actual data.

58:55.000 --> 58:56.000
So that's the human aspect too.

58:56.000 --> 58:57.000
Yep.

58:57.000 --> 59:06.000
So I'm just wondering wouldn't it be more likely that future software engineers by 2030 and beyond are just 10,000 times more effective,

59:06.000 --> 59:13.000
and still have to remain a sweet role because they're lacking all the things that make the human because the data's just not out there.

59:13.000 --> 59:22.000
Not even in the, like, there's no place on earth that some ethical rule about life in Boston or Cambridge is laid out perfectly like it is in our mind.

59:22.000 --> 59:23.000
Yeah.

59:23.000 --> 59:31.000
So the question is it's sort of this idea that maybe there's an ineffable quality to being a human software engineer,

59:31.000 --> 59:38.000
or something about our training, our knowledge of the world, our ethics, our socialization with other humans,

59:38.000 --> 59:42.000
that a model isn't going to capture, a language model's not going to capture.

59:42.000 --> 59:48.000
And so maybe the future is that a software engineer is still a software engineer,

59:48.000 --> 59:51.000
but they're 10,000 times more productive than they are today.

59:51.000 --> 59:53.000
I think it's a good question.

59:53.000 --> 01:00:03.000
I do think we're going to hit a limit in terms of what we can do with programming languages and tools and things that humans have to reason about and understand.

01:00:03.000 --> 01:00:05.000
So here's one way of thinking about this.

01:00:05.000 --> 01:00:14.000
The facetious answer to you is let's imagine that humans are still the ones predominantly writing code, but they get a hell of a lot of help on it.

01:00:14.000 --> 01:00:25.000
We're still going to have to deal with CSS, that pile of garbage that thousands of millions of engineers have to deal with every single day.

01:00:25.000 --> 01:00:31.000
And the reason for that is because it's part of our technology corpus.

01:00:31.000 --> 01:00:33.000
It's part of the knowledge of humanity.

01:00:33.000 --> 01:00:36.000
It's part of the stack that we all use.

01:00:36.000 --> 01:00:54.000
So the problem there is there's a bandwidth limit, which is an individual mind has to go through this syntactic description of what they want to do in these god-awful languages like CSS and JavaScript and Python and Rust.

01:00:54.000 --> 01:01:08.000
And the problem that I have with that is that I think it's a barrier to actually enabling what you could build with computation from actually becoming a reality.

01:01:08.000 --> 01:01:14.000
It's like drinking through a very narrow straw.

01:01:14.000 --> 01:01:31.000
So I think what we need to do is get the humans out of the loop on that and change the relationship between humans and the way software is built so that we can unlock that potential.

01:01:31.000 --> 01:01:36.000
And exactly what that looks like I don't know, but that's my core belief.

01:01:36.000 --> 01:01:38.000
Yes?

01:01:38.000 --> 01:01:42.000
The talk was mostly about coding and this is about coding.

01:01:42.000 --> 01:01:44.000
How about the algorithms?

01:01:44.000 --> 01:01:50.000
I'm an astrophysicist and in our case every telescope is one thing in the world.

01:01:50.000 --> 01:01:56.000
They're all unique and same as the data processing systems.

01:01:56.000 --> 01:02:02.000
So we have some unique algorithm that only a few people in the world can design or understand.

01:02:02.000 --> 01:02:07.000
And I wouldn't expect that a large language model would help you developing such an algorithm.

01:02:07.000 --> 01:02:13.000
So do you see, I guess in biology or in bioinformatics the problem is similar.

01:02:13.000 --> 01:02:21.000
So do you think they're still niche for LLMs to develop to help there in this particular case?

01:02:21.000 --> 01:02:25.000
Yeah, so the question is we've been talking about the coding but not the algorithms.

01:02:25.000 --> 01:02:27.000
Who came up with that algorithm?

01:02:27.000 --> 01:02:34.000
What was the spark of the idea that produced the algorithm that we're then translating into these clunky programming languages, right?

01:02:34.000 --> 01:02:40.000
And I think it's a very good point actually because there's a question right now and this kind of came back to my point earlier about

01:02:40.000 --> 01:02:45.000
we don't really know the logical reasoning limits of these models.

01:02:45.000 --> 01:02:51.000
And so I don't really know if I said to the model give it some complex problem data analysis problem that I want to solve

01:02:51.000 --> 01:02:57.000
if it could actually derive a new algorithm that hadn't been known before.

01:02:57.000 --> 01:03:01.000
It's a good question. I tend to think it could, maybe not in today's models.

01:03:01.000 --> 01:03:04.000
I believe in the future it can.

01:03:04.000 --> 01:03:10.000
But then the question really is now coming back to the dual problem of how do I ask the model what I want, right?

01:03:10.000 --> 01:03:12.000
How do I express myself?

01:03:12.000 --> 01:03:16.000
And then how do I teach it most effectively to get it to the right answer?

01:03:16.000 --> 01:03:23.000
So the answer might end up being that there really ends up being a symbiosis between the human and the AI model

01:03:23.000 --> 01:03:27.000
iterating together on something where the AI model is doing the stuff it's good at.

01:03:27.000 --> 01:03:30.000
The human is doing the things it's good at.

01:03:30.000 --> 01:03:33.000
And we already see that happening with things like co-pilot.

01:03:33.000 --> 01:03:36.000
It's just it's operating at a very low level of abstraction, right?

01:03:36.000 --> 01:03:41.000
It's write the four lines of Python code to reverse this list or whatever the thing is.

01:03:41.000 --> 01:03:48.000
When you start getting into a higher level of abstractions, developing algorithms, doing data analysis, any of those things,

01:03:48.000 --> 01:03:52.000
I think the kind of tooling, it's not going to be co-pilot in an IDE.

01:03:52.000 --> 01:03:56.000
It's going to be something else. I don't know what that something else is.

01:03:56.000 --> 01:04:00.000
Maybe it's Jupyter notebooks on steroids or something like that, right?

01:04:00.000 --> 01:04:05.000
Let me do this. Let me just take one more question and I'll take it from you because you had your hand up earlier.

01:04:05.000 --> 01:04:10.000
Thanks. I think you're kind of talking about like a new age of programming, right?

01:04:10.000 --> 01:04:14.000
Where the AI programs are now an abstraction of what we're doing currently.

01:04:14.000 --> 01:04:20.000
So 15 years in the future, we have people that are only used to that paradigm of developing programs.

01:04:20.000 --> 01:04:23.000
Do you think the classical training that we have today will be helpful

01:04:23.000 --> 01:04:29.000
or if it's completely abstracted away in ten years where you've been having this knowledge?

01:04:29.000 --> 01:04:38.000
Yeah, so the question is kind of the way that we train people in software engineering disciplines, is it relevant?

01:04:38.000 --> 01:04:44.000
Is the way we train today relevant in a future in which AIs are doing more of this, right?

01:04:44.000 --> 01:04:47.000
Or more prompt engineering? That's the real question.

01:04:47.000 --> 01:04:53.000
And I think, you know, kind of speaking to that at the end, it's like, you know, as a computer science undergraduate at Cornell,

01:04:53.000 --> 01:04:58.000
yes, I had to go take some EE classes and understand how circuits worked, right?

01:04:58.000 --> 01:05:03.000
That was important. And when I taught here, I did teach, you know, operating systems and systems programming

01:05:03.000 --> 01:05:06.000
and, you know, what's a stack, you know, this kind of thing.

01:05:06.000 --> 01:05:11.000
So it's important to have some of that foundational knowledge.

01:05:11.000 --> 01:05:21.000
But the question is where does the emphasis end up being in terms of how we think about creating programs and managing programs?

01:05:21.000 --> 01:05:26.000
I think it would be a mistake for, say, university programs to not pay attention to this

01:05:26.000 --> 01:05:32.000
and to kind of assume that teaching computer science the way it's been done for the last 25 years is the right thing in this future.

01:05:32.000 --> 01:05:35.000
I don't know what they should evolve it to.

01:05:35.000 --> 01:05:40.000
What I can say, though, is that once somebody gets out of their academic thing and they're hitting industry,

01:05:40.000 --> 01:05:45.000
well, that's already a huge gap between what you learn in college and what you're having to do in the real world.

01:05:45.000 --> 01:05:50.000
And that's why we have things like internships and other, you know, methodologies.

01:05:50.000 --> 01:05:57.000
So maybe the goal of academic computer science education should not necessarily be vocational, per se.

01:05:57.000 --> 01:06:04.000
But I do think that we have to think about, you know, how do people reason about these models?

01:06:04.000 --> 01:06:11.000
At the minimum, I would hope that CS50 or whatever the equivalent class is at another university

01:06:11.000 --> 01:06:17.000
can go deep into understanding some of the mechanics behind things like chat GPT,

01:06:17.000 --> 01:06:21.000
understanding data, how it comes in, understanding how models are constructed,

01:06:21.000 --> 01:06:25.000
how they're trained, what their limitations are, how to evaluate them.

01:06:25.000 --> 01:06:31.000
Because the fear that I have is that students just view this thing as this magical black box that will do anything for them

01:06:31.000 --> 01:06:36.000
and have no critical thinking around that.

01:06:36.000 --> 01:06:43.000
However, I do know from my own experience that it is a magical black box.

01:06:43.000 --> 01:06:46.000
And I don't understand how it works.

01:06:46.000 --> 01:06:50.000
But see, I'm okay with that because it does so many great things for me.

01:06:50.000 --> 01:06:53.000
Anyway, thank you very much, and I'll be around for pizza, too.

