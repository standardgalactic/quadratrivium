{"text": " All right. Well, this is CS50 Tech Talk. Thank you all so much for coming. So about a week ago, we circulated the Google Form, as you might have seen, at 10.52 AM. And by, like, 11.52 AM, we had 100 RSVPs, which I think is sort of testament to just how much interest there is in this world of AI, and open AI, and GPT, chat GPT, and the like. And in fact, if you're sort of generally familiar with what everyone's talking about, but you haven't tried it yourself, this is the URL, which you can try out this tool that you've probably heard about, chat GPT. You can sign up for a free account there and start tinkering with what everyone else has been tinkering with. And then if you're more of the app-minded type, which you probably are if you are here with us today, open AI in particular has its own low-level APIs via which you can integrate AI into your own software. But of course, as is the case in computer science, there's all the more abstractions and services that have been built on top of these technologies. And we're so happy today to be joined by our friends from McGill University and Steamship, Sill and Ted, from whom you'll hear in just a moment, to speak to us about how they are making it easier to build, to deploy, to share applications using some of these very same technologies. So our thanks to them for hosting today. Our friends at Plimpton, Jenny Lee and alumna, who's here with us today, but without further ado, allow me to turn things over to Ted and Sill. And pizza will be served shortly after 1 PM outside. All right, over to you, Ted. Thanks a lot. Hey, everybody, it's great to be here. I think we've got a really good talk for you today. Sill's gonna provide some research grounding into how it all works, what's going inside, the brain of GPT as well as other language models. And then I'll show you some examples that we're seeing on the ground of how people are building apps and what apps tend to work in the real world. So our perspective is we're building AWS for AI apps. So we get to talk to a lot of the makers who are building and deploying their apps. And through that, see both the experimental end of the spectrum and also see what kinds of apps are getting pushed out there and turned into companies, turned into side projects. We did a cool hackathon yesterday. Many thanks to Neiman, to David Malin and CS50 for helping us put all of this together, to Harvard for hosting it. And there were two sessions, lots of folks built things. If you go to steamship.com slash hackathon, you'll find a lot of guides, a lot of projects that people built. And you can follow along, we have a text guide as well, just as a quick plug for that, if you want to do it remotely or on your own. So to tee up Sill, we're gonna talk about basically two things today that I hope you'll walk away with and really know how to then use as you develop and as you tinker. One is what is GPT and how is it working? Get a good sense of what's going on inside of it other than as just this magical machine that predicts things. And then two is how are people building with it? And then importantly, how can I build with it too if you're a developer? And if you have CS50 background, you should be able to pick things up and start building some great apps. I've already met some of the CS50 grads yesterday and the things that they were doing were pretty amazing. So hope this is useful. I'm gonna kick it over to Sill and talk about some of the theoretical background of GPT. Yeah, so thank you, Ted. My name is Sill. I'm a graduate student of Digital Humanities at McGill. I study literature and computer science and linguistics in the same breath. And I've published some research over the last couple of years exploring what is possible with language models and culture in particular. And my half or whatever of the presentation is to describe to you what is GPT. That's really difficult to explain in 15 minutes. And there are even a lot of things that we don't know, but a good way to approach that is to first consider all the things that people call GPT by or descriptors. So you can call them large language models. You can call them universal approximators from computer science. You can say that it is a generative AI. We know that they are neural networks. We know that it is an artificial intelligence. To some, it's a simulator of culture. To others, it just predicts text. It's also a writing assistant. If you've ever used ChatGPT, you can plug in a bit of your essay, get some feedback. It's amazing for that. It's a content generator. People use it to do copywriting, Jasper.AI, pseudo-write, et cetera. It's an agent. So the really hot thing right now, if you might have seen it on Twitter, auto-GPT, baby-AGI, people are giving these things tools and letting them run a little bit free in the wild to interact with the world computers, et cetera. We use them as chatbots, obviously. And the actual architecture is a transformer. So there's lots of ways to describe GPT. And any of them is a really perfectly adequate way to begin the conversation. But for our purposes, we can think of it as a large language model, and more specifically, a language model. And a language model is a model of language, if you allow me the tautology, but really what it does is it produces a probability distribution over some vocabulary. So let us imagine that we had the task of predicting the next word of the sequence I am. So if I give a neural network the words I am, what of all words in English is the next most likely word to follow? That, at its very core, is what GPT is trained to answer. And how it does it is it has a vocabulary of 50,000 words, and it knows, roughly, given the entire internet, which words are likely to follow other words of those 50,000 in some sequence, up to 2,000 words, up to 4,000, up to 8,000, and now up to 32,000 GPT4. So you give it a sequence, here I am, and over the vocabulary of 50,000 words, it gives you the likelihood of every single word that follows. So here it's I am, perhaps the word happy is fairly frequent, so we'll get that high probability if we look at all words, all utterances of English. It might be I am sad, maybe that's a little bit less probable, I am school, that really should be at the end because I don't think anybody would ever say that, I am Bjork, that's a little bit, it's not very probable, but it's less probable than happy sad, but there's still some probability attached to it. And when we say it's probable, that's literally a percentage, that's like happy follows I am maybe like 5% of the time, sad follows I am maybe 2% of the time, or whatever. So for every word that we give GPT, it tries to predict what the next word is across 50,000 words, and it gives every single one of those 50,000 words a number that reflects how probable it is. And the really magical thing that happens is you can generate new text, so if you give GPT I am, and it predicts happy as being the most probable word over 50,000, you can then append it to I am, so now you say I am happy, and you feed it into the model again, you sample another word, you feed it into the model again, and again, and again, and again, and there's lots of different ways that I am happy, I am sad, can go, and you add a little bit of randomness, and all of a sudden you have a language model that can write essays, that can talk, and a whole lot of things, which is really unexpected, and something that we didn't predict even five years ago, so this is all relevant. And if we move on, as we scale up the model, and we give it more compute, in 2012 AlexNet came out, and we figured out we can run the model on GPUs, so we can speed up the process, we can give the model lots of information downloaded from the internet, and it learns more and more and more, and the probabilities that it gives you get better as it sees more examples of English on the internet, so we have to train the model to be really large, really wide, and we have to train it for a really long time, and as we do that, the model gets more and more better, and expressive and capable, and it also gets a little bit intelligent, and for reasons we don't understand. So, but the issue is that because it learns to replicate the internet, it knows how to speak in a lot of different genres of text, and a lot of different registers. If you begin the conversation like, chat GPT, can you explain the moon landing to a six year old in a few sentences, GPT three, this is an example drawn from the Instruct GPT paper from OpenAI, GPT three would have just been like, okay, so you're giving me an example, like explain the moon landing to a six year old, I'm gonna give you a whole bunch of similar things because those seem very likely to come in a sequence. It doesn't necessarily understand that it's being asked that question has to respond with an answer. GPT three did not have that apparatus, that interface for responding to questions, and the scientist at OpenAI came up with the solution, and that's, let's give it a whole bunch of examples of question and answers, such that we first train it on the internet, and then we train it with a whole bunch of questions and answers such that it has the knowledge of the internet, but really knows that it has to be answering questions, and that is when chat GPT was born, and that's when it gained 100 million users in one month, I think it beat TikTok's record at 20 million in one month, it was a huge thing, and for a lot of people, they went, oh, this thing is intelligent, I can answer, I can ask it questions, it answers back, we can work together to come to a solution, and that's because it's still predicting words, it's still a language model, but it knows to predict words in the framework of a question and answer, so that's what a prompt is, that's what instruction tuning is, that's a key word, that's what RLHF is, if you've ever seen that acronym, Reinforcement Alignment with Human Feedback, and all those combined means that the models that are coming out today, the types of language predictors that are coming out today, work to operate in a Q and A form. GPT-4 exclusively only has the aligned model available, and this is a really great, solid foundation to build on, because you can do all sorts of things, you can ask chat GPT, can you do this for me, can you do that for me? You might have seen that OpenAI has allowed plugin access to chat GPT, so it can access Wolfram, it can search the web, it can search, it can do Instacart for you, it can look up recipes, once the model knows that not only it has to predict language, but that it has to solve a problem, and the problem here being, give me a good answer to my question, it's suddenly able to interface with the world in a really solid way, and from there on, there's been all sorts of tools that it build on this Q and A form that chat GPT uses, you have auto GPT, you have Langchain, you have React, there was a React paper where a lot of these come from, and turning the model into an agent which to achieve any ambiguous goal is where the future is going, and this is all thanks to instruction tuning, and with that, I think I will hand it off to Ted, who will be giving a demo, or something along those lines, for how to use GPT as an agent, so. All right, so I'm a super applied guy, I kinda look at things and think, okay, how can I add this Lego, add that Lego, and clip them together and build something with it, and right now, if you look back in computer science history, when you look at the kinds of things that were being done in 1970, right after computing was invented, the microprocessors were invented, people were doing research like, how do I sort a list of numbers, and that was meaningful work, and importantly, it was work that's accessible to everybody, because nobody knows what we can build with this new kind of oil, this new kind of electricity, this new kind of unit of computation we've created, and anything was game, and anybody could participate in that game to figure it out, and I think one of the really exciting things about GPT right now is, yes, in and of itself, it's amazing, but then, what could we do with it if we call it over and over again, if we build it into our algorithms, and start to build it into broader software, so the world really is yours to figure out those fundamental questions about what could you do if you could script computation itself over and over again in the way that computers can do, not just talk with it, but build things atop it, so we're a hosting company, we host apps, and these are just some of the things that we see, I'm gonna show you demos of this with code and try to explain some of the thought process, but I wanted to give you a high level overview of, you've probably seen these on Twitter, but when it all sorts out to the top, these are some of the things that we're seeing built and deployed with language models today, companionship, that's everything from I need a friend, to I need a friend with a purpose, I want a coach, I want somebody to tell me, go to the gym and do these exercises, I want somebody to help me study a foreign language, question answering, this is a big one, this is everything from your newsroom, having a slack bot that helps assist you, does this article conform to the style guidelines of our newsroom, all the way through to, and you need help on my homework, or hey, I have some questions that I want you to ask, Wikipedia, combine it with something else, synthesize the answer and give it to me. Utility functions, I would describe this as, there's a large set of things for which human beings can do them, if only, or computers could do them, if only they had access to language computation, language knowledge, an example of this would be, read every tweet on Twitter, tell me the ones I should read, that way I only get to read the ones that actually make sense to me and I don't have to skim through the rest, creativity, image generation, text generation, storytelling, proposing other ways to do things, and then these wild experiments and kind of baby AGI, as people are calling them, in which the AI itself decides what to do and is self-directed, so I'll show you examples of many of these and what the code looks like, and if I were you, I would think about these as categories within which to both think about what you might build and then also seek out starter projects for how you might go about building them online. All right, so I'm just gonna dive straight into demos and code for some of these, because I know that's what's interesting to see as fellow builders, with a high level diagram for some of these as to how it works. So approximately, you can think of a companionship bot as a friend that has a purpose to you, and there are many ways to build all of these things, but one of the ways you can build this is simply to wrap GPT or a language model in an endpoint that additionally injects into the prompt some particular perspective or some particular goal that you want to use. It really is that easy in a way, but it's also very hard because you need to iterate and engineer the prompt so that it consistently performs the way you want it to perform. So a good example of this is something somebody built in the hackathon yesterday, and I just wanted to show you the project that they built. It was a Mandarin idiom coach, and I'll show you what the code looked like first. I'll show you the demo first. I think I already pulled it up. Here we go. So the buddy that this person wanted to create was a friend that if you gave it a particular problem you were having, it would pick a Chinese idiom, a four character Cheng Yu, that described poetically, like here's a particular way you could say this, and it would tell it to her so that the person who built this was studying Chinese and she wanted to learn more about it. So I might say something like I'm feeling very sad, and it would think a little bit, and if everything's up and running, it will generate one of these four character phrases, and it will respond to it with an example. Now, I don't know if this is correct or not, so if somebody can call me out, if this is actually incorrect, please call me out, and it will then finish up with something encouraging, saying hey, you can do it, I know this is hard, keep going. So let me show you how they built this, and I pulled up the code right here. So this was the particular starter replete that folks were using in the hackathon yesterday, and we pulled things up into basically, you have a wrapper around GPT, and there's many things you could do, but we're gonna make it easy for you to do two things. One of them is to inject some personality into the prompt, and I'll explain what that prompt is in a second, and then the second is add tools that might go out and do a particular thing, search the web or generate an image or add something to a database or fetch something from a database. So having done that, now you have something more than GPT. Now you have GPT, which we all know what it is and how we can interact with it, but you've also added a particular lens through which it's talking to you and potentially some tools. So this particular Chinese tutor, all it took to build that was four lines. So here's a question that I think is frying the minds of everybody in the industry right now. So is this something that we'll all do casually and nobody really knows? Will we just all say in the future to the LLM, hey, for the next five minutes, please talk like a teacher, and maybe? But also, definitely in the meantime, and maybe in the future, it makes sense to wrap up these personalized endpoints so that when I'm talking to GPT, I'm not just talking to GPT, I have a whole army of different buddies, of different companions that I can talk to. They're kind of human and kind of talk to me interactively, but because I preloaded them with, hey, by the way, you particular, I want you to be a kind, helpful Chinese teacher that responds to every situation by explaining the Chongyu that fits it. Speak in English and explain the Chongyu in its meaning. Then provide a note of encouragement about learning language. And so just adding something like that, even if you're a non-programmer, you can just type deploy, and it'll pop it up to the web, it'll take it over to a telegram bot that then you can even interact with, hey, I'm feeling too busy, and interact with it over telegram, over the web, and this is the kind of thing that's now within reach for everybody from a CS 101 grad, sorry, I'm using the general purpose framing, all the way through to professionals in the industry, that you can do just with a little bit of manipulation on top of sort of this raw unit of conversation and intelligence. So companionship is one of the first common types of apps that we're seeing. So a second kind of app that we're seeing, and for those of you who are on Twitter followers, this blew up, I think the last few months, is question-answering, and I wanna unpack a couple of different ways this can work, because I know many of you have probably already tried to build some of these kinds of apps, there's a couple of different ways that it works. The general framework is a user queries GPT, and maybe it has general purpose knowledge, maybe it doesn't have general purpose knowledge, but what you want it to say back to you is something specific about an article you wrote, or something specific about your course syllabus, or something specific about a particular set of documents from the United Nations on a particular topic. And so what you're really seeking is what we all hoped the customer service bot would be, like we've all interacted with these customer service bots, and we're kind of smashing our heads on the keyboard as we do it, but pretty soon we're gonna start to see very high fidelity bots that interact with us comfortably, and this is approximately how to do it as an engineer. So here's your game plan as an engineer, step one, take the documents that you want it to respond to. Step two, cut them up. Now, if you're an engineer, this is gonna madden you. You don't cut them up in a way that you would hope. For example, you could cut them up into clean sentences or clean paragraphs, or semantically coherent sections, and that would be really nice. Honestly, the way that most folks do it, and this is a simplification that tends to be just fine, is you window, you have a sliding window that goes over the document, and you just pull out fragments of text. Having pulled out those fragments of text, you turn them into something called an embedding vector. So an embedding vector is a list of numbers that approximate some point of meaning. So you've already all dealt with embedding vectors yourself in regular life, and the reason you have, and I know you have, is because everybody's ordered food from Yelp before. So when you order food from Yelp, you look at what genre of restaurant is it? Is it a pizza restaurant? Is it an Italian restaurant? Is it a Korean barbecue place? You look at how many stars does it have? One, two, three, four, five? You look at where is it? So all of these you can think of as points in space, dimensions in space. Korean barbecue restaurant, four stars near my house. That's a three number vector. That's all this is. So this is a thousand number vector, or a 10,000 number vector. Different models produce different size vectors. All it is is chunking pieces of text, turning it into a vector that approximates meaning, and then you put it in something called a vector database. And a vector database is just a database that stores numbers. But having that database, now when I ask a question, I can search the database, and I can say, hey, the question was, what does CS50 teach? What pieces of text in the database have vectors similar to the question, what does CS50 teach? And there's all sorts of tricks and empires being made on refinements of this general approach. But at the end, you, the developer, model it simply as thus. And then when you have your query, you embed it, you find the document fragments, and then you put them into a prompt. And now we're just back to the personality, the companionship bot. Now it's just a prompt. And the prompt is, you're an expert in answering questions. Please answer user provided question. Using source documents results from the database. That's it. So after all of these decades of engineering and these customer service bots, it turns out with a couple of lines of code. You can build this. So let me show you, I made one just before the class with the CS50 syllabus. So we can pull that up. And I can say, I added the PDF right here. So I just, I searched, I don't know if, I apologize. I don't know if it's an accurate or recent syllabus. I just searched the web for CS50 syllabus PDF. I put the URL in here, it loaded it into here. This is just a 100 line piece of code deployed that will now let me talk to it. And I can say, what will CS50 teach me? So under the hood now, what's happening is exactly what that slide just showed you. It takes that question, what will CS50 teach me? It turns it into a vector. That vector approximates without exactly representing the meaning of that question. It looks into a vector database that steamship hosts of fragments from that PDF. And then it pulls out a document and then passes it to a prompt that says, hey, you're an expert at answering questions. Someone has asked you, what does CS50 teach? Please answer it using only the source documents and source materials I've provided. Now those source materials materials are dynamically loaded into the prompt. It's just basic prompt engineering. And I want to keep harping back onto that. What's amazing about right now as builders is that so many things just boil down into very creative, tactical rearrangement of prompts and then using those over and over again in an algorithm and putting that into software. So the result, and again, it could be lying. It could be making things up. It could be hallucinating. Is CS50 will teach students how to think algorithmically and solve problems efficiently, focusing on topics such as abstraction, dot, dot, dot, dot, dot. And then it returns the source document from which it was found. So this is another big category of which there are tons of potential applications because you can repeat for each context. You can create arbitrarily many of these once it's software because once it's software, you can just repeat it over and over again. So for your dorm, for your club, for your slack, for your telegram, you can start to begin putting pieces of information in and then responding to it. And it doesn't have to be documents. You can also load it straight into the prompt. I think I have it pulled up here. And if I don't, I'll just skip it. Oh, here we go. One other way you can do question answering, because I think it's healthy to always encourage the simplest possible approach to something. You don't need to engineer this giant system. It's great to have a database. It's great to use embeddings. It's great to use this big approach. It's fancy at scales. You can do a lot of things. But you can also get away with a lot by just pushing it all into a prompt. And as an engineer, I'm, you know, that's one of our team who's here always says, like, engineers should aspire to be lazy. And I couldn't agree more. You, as an engineer, should want to set yourself up so that you can pursue the lazy path to something. So here's how you might do the equivalent of a question answering system with a prompt alone. Let's say you have 30 friends. And each friend is good at a particular thing, or you can, you know, this is isomorphic to many other problems. You can simply just say, hey, I know certain things. Here's the things I know. A user's gonna ask me something, how should we respond? And then you load that into an agent. That agent has access to GPT. You can ship deploy it. And now you've got a bot that you can connect to Telegram. You can connect to Slack. And that bot, now it won't always give you the right answer. Because at a certain level, we can't control the variance of the model underneath. But it will tend to answer with respect to this list. And the degree to which it tends to is to a certain extent, something that both industry is working on to just give everybody as a capacity. But also you doing prompt engineering to tighten up the error bars on it. So I'll show you just a few more examples. And then in about eight minutes, I'll turn it over to questions, because I'm sure you've got a lot about how to build things. So just to give you a sense of where we are. This is one, I don't have a demo for you. But if you were to come to me and you were to say, Ted, I want a weekend hustle, man, what should I build? Holy moly. There are a set of applications that I would describe as utility functions. I don't like that name, because it doesn't sound exciting, and this is really exciting. And it's low hanging fruits that automate tasks that require basic language understanding. So examples for this are generate a unit test. I don't know how many of you have ever been writing tests and you're just like, oh, come on, I can get through this, I can get through this. If you're a person who likes writing tests, you're a lucky individual. Looking up the documentation for a function, rewriting a function, making something conform to your company guidelines, doing a brand check. All of these things are things that are kind of relatively context-free operations or scoped context operations on a piece of information that requires linguistic understanding. And really, you can think of them as something that is now available to you as a software builder, as a weekend project builder, as a startup builder. And you just have to build the interface around it and present it to other people in a context in which it's meaningful for them to consume. And so the space of this is extraordinary. I mean, it's the space of all human endeavor, now with this new tool, I think, is the way to think about it. People often joke about how when you're building a company, when you're building a project, you don't want to start with a hammer, because you want to start with a problem instead. And it's generally true, but my God, we've just got a really cool new hammer. And to a certain extent, I would encourage you to at least casually, on the weekends, run around and hit stuff with it and see what can happen from a builder's, from a tinkerers, from an experimentalist's point of view. And then the final one is creativity. This is another huge mega app. Now, I'm primarily living the text world, and so I'm gonna talk about text-based things. I think so far, this has mostly been growing in the imagery world, because we're such visual creatures, and the images you can generate are just staggering with AI. Certainly brings up a lot of questions, too, around IP and artistic style. But the template for this, if you're a builder, that we're seeing in the wild, is approximately the following. And the thing I want to point out is domain knowledge here. This is really the purpose of this slide, is to touch on the importance of the domain knowledge. So, many people approximately find the creative process as follows. Come up with a big idea. Over-generate possibilities. Edit down what you over-generated. Repeat, right? Like anybody who's been a writer knows when you write, you write way too much, and then you have to delete lots of it. And then you revise, and you write way too much, and you have to delete lots of it. This particular task is fantastic for AI. One of the reasons it's fantastic for AI is because it allows the AI to be wrong. You know, you've pre-agreed, you're gonna delete lots of it. And so, if you pre-agreed, hey, I'm just gonna build, generate five possibilities of the story I might tell. Five possibilities of the advertising headline. Five possibilities of what I might write my thesis on. You pre-agreed, it's okay if it's a little wrong, because you are going to be the editor that steps in. And here's the thing that you really should bring to the table, is don't think about this as a technical activity. Think about this as your opportunity not to put GPT in charge. Instead, for you to grasp the steering wheel tighter, I think, at least, in Python, or the language you're using to program, because you have the domain knowledge to wield GPT in the generation of those. So let me show you an example of what I mean by that. So, this is a cool app that someone created for the Writing Atlas project. So Writing Atlas is a set of short stories, and you can think of it as good reads for short stories. So you can go in here, you can browse different stories, and this was something somebody created where you can type in a story description that you like, and this is gonna take about a minute to generate, so I'm gonna talk while it's generating. And while it's working, what it's doing, and I'll show you the code in a second, is it's searching through the collection of stories for similar stories, and here's where the domain knowledge part comes in. Then it uses GPT to look at what it was that you wanted, and use knowledge of how an editor, how a bookseller thinks, to generate a set of suggestions specifically through the lens of that perspective with the goal of writing that beautiful handwritten note that we sometimes see in a local bookstore tacked on underneath a book. And so it doesn't just say, hey, you might like this, here's a general purpose reason why you might like this, but specifically, here's why you might like this with respect to what you gave it. It's either stalling out, or it's taking a long time. Oh, there we go. So here's its suggestions, and in particular, these things, these are things that only a human could know, at least for now, two humans specifically, the human who said they wanted to read a story, that's the text that came in, and then the human who added domain knowledge to script a sequence of interactions with the language model so that you could provide very targeted reasoning over something that was informed by that domain knowledge. So for these utility apps, bring your domain knowledge. Let me actually show you how this looks and code, because I think it's useful to see how simple and accessible this is. This is really a set of prompts. So why might they, like a particular location, well, here's the prompt that did that, this is an open source project, and it has a bunch of examples, and then it says, well, here's the one that we're interested in. Here's the audience, here's a couple of examples of why might people like a particular thing in terms of audience, it's just another prompt. Same for topic, same for explanation, and if you go down here and look at how it was done, suggesting the story is, what is this, line 174 to line 203, it really is, and again, over and over again, I wanna impress upon you, this really is within reach. It's really just what, 20 odd lines of step one, search in the database for similar stories, step two, given that I have similar stories, pull out the data, step three, with my domain knowledge in Python, now run these prompts, step four, prepare that into an output. So the thing we're scripting itself is some approximation of human cognition. If you're willing to go there metaphorically, we're not sure, I'm not gonna weigh in on where we are on this open AI, a life form argument. All right, one really far out there thing, and then I'll tie it up for questions, because I know there's probably a lot, and I also wanna make sure you get great pizza in your bellies, and that is a baby AGI auto GPT is what you might have heard them called on Twitter. I think of them as multi-step planning bots. So everything I showed you so far was approximately one shot interactions with GPT. So this is, the user says they want something, and then either Python mediates interactions with GPT, or GPT itself does some things with the inflection of a personality that you've added from some prompt engineering. Really useful, pretty easy to control. If you wanna go to production, if you wanna build a weekend project, if you wanna build a company, that's a great way to do it right now. This is wild, and if you haven't seen this stuff on Twitter, I would definitely recommend going to search for it. This is what happens, the simple way to put it is, if you put GPT in a for loop, if you let GPT talk to itself, and then tell itself what to do. So it's an emergent behavior, and like all emergent behaviors, it starts with a few simple steps, the Conways game of life, many elements of reality, turn out to be math equations that fit on a t-shirt, but then when you play them forward in time, they generate DNA, they generate human life. So this is approximately, step one, take a human objective, step two, your first task is to write yourself a list of steps, and here's the critical part, repeat. Now do the list of steps. Now you have to embody your agent with the ability to do things. So it's really only limited to do what you give it the tools to do, and what it has the skills to do. So obviously this is still very much a set of experiments that are running right now, and but it's something that we'll see unfold over the coming years, and this is the scenario in which Python stops becoming so important because we've given it the ability to actually self-direct what it's doing, and then it finally gives you a result. And I wanna give you an example still of just, again, impressing upon you how much of this is prompt engineering, which is wild, how little code this is. Let me show you what BabyAGI looks like. So here is a BabyAGI that you can connect to Telegram, and this is an agent that has two tools. So I haven't explained to you what an agent is, I haven't explained to you what tools are, I'll give you a quick one sentence description. An agent is just a word to mean GPT plus some bigger body in which it's living. Maybe that body has a personality, maybe it has tools, maybe it has Python mediating its experience with other things. Tools are simply ways in which the agent can choose to do things. Like imagine if GPT could say order a pizza, and instead of you seeing the text order a pizza, that caused a pizza to be ordered, that's a tool. So these are two tools it has, one tool is generated to-do list, one tool is do a search on the web, and then down here it has a prompt saying, hey, your goal is to build a task list and then do that task list, and then this is just placed into a harness that does it over and over again. So after the next task, kind of unqueue the results of that task and keep it going. And so in doing that, you get this kickstarted loop where essentially you kickstart it, and then the agent is talking to itself. So this, unless I'm wrong, I don't think this has yet reached production in terms of what we're seeing in the field of how people are deploying software, but if you wanna dive into sort of the wildest part of experimentation, this is definitely one of the places you can start, and it's really within reach. All you have to do is download one of the starter projects for it, and you can kind of see right in the prompting, here's how you kickstart that process of iteration. All right, so I know that was super high level. I hope it was useful. It's, I think from the field, from the bottoms up, what we're seeing and what people are building, kind of the high level categories of apps that people are making. All of these apps are apps that are within reach to everybody, which is really, really exciting. And there's, I suggest Twitter is a great place to hang out and build things. There's a lot of AI builders on Twitter publishing. And I think we've got a couple minutes before pizza is arriving, maybe 10 minutes. Keep on going. So if there's any questions, why don't we kick it to that? Because I'm sure there's some questions that you all have, I guess I ended it a little early. Yes? Yeah, so I have a question around hallucination. And so, you know, whenever building these sorts of applications in apps, for example, let's say, I'm giving it like a physics problem from a PSET and we want to do that. Yeah. And, you know, it's 40% of the time just raw. Yeah. Do you have any like actionable recommendations that these developers should be doing to make it hallucinate less? Or maybe even things that like open AI on the back end should be doing to reduce hallucination. So it would be something where you use RLHF. Yeah, I didn't get the answer. So the question was how, approximately, how do you manage the hallucination problem? Like if you give it a physics lecture and you ask it a question, on the one hand, it appears to be answering you correctly. On the other hand, it appears to be wrong to an expert's eye 40% of the time, 70% of the time, 10% of the time. It's a huge problem. And then what are some ways as developers practically you can use to mitigate that? I'll give an answer. So you may have some specific things too. So one high level answer is, the same thing that makes these things capable of synthesizing information is part of the reason why it hallucinates for you. So it's hard to have your cake you need it to to a certain extent. So this is part of the game. In fact, humans do it too. Like people talk about, you know, just folks who kind of are too aggressive in their assumptions about knowledge. I can't remember the name for that phenomenon where you'll just say stuff, right? So we do it too. Some things you can do are kind of a range of activities depending on how much money you really need to spend, how much technical expertise you have, that can range from fine tuning a model to practically, so I'm in the applied world. So I'm very much in the world of duct tape and sort of how developers get stuff done. So some of the answers I'll give you are sort of very duct-tapy answers. Giving it examples tends to work for acute things. If it's behaving in wild ways, the more examples you give it, the better. That's not gonna solve the domain of all of physics. So for the domain of all of physics, I'm gonna bail and give it to you because I think you are far more equipped than me to speak on that. Sure, so the model doesn't have a ground truth. It doesn't know anything. Any sense of meaning that it's derived from the training process is purely out of differentiation. One word is not another word. Words are not used in the same context. It understands everything only through examples given through language. It's like someone who learned English or how to speak, but they grew up in a featureless gray room. They've never seen the outside world. They have nothing to rest on that tells them that something is true and something is not true. So from the model's perspective, everything that it says it's true. It's trying its best to give you the best answer possible. And if it lying a little bit or conflating two different topics is the best way to achieve that, then it will decide to do so. It's a part of the architecture. We can't get around it. There are a number of cheap tricks that surprisingly get it to confabulate or hallucinate less. One of them includes recently, there was a paper that's a little funny. If you get it to prepend to its answer, my best guess is that will actually improve or reduce hallucinations by about 80%. So clearly it has some sense that some things are true and other things are not, but we're not quite sure what that is. To add on to what Ted was saying, a few cheap things you can do include letting it Google or Bing, as in Bing Chat, what they're doing, it cites this information, asking it to make sure its own response is good. If you've ever had JetGBT generate a program, there's some kind of problem, and you ask ChatGBT, I think there's a mistake. Often it'll locate the mistake itself. Why didn't produce the right answer at the very beginning? We're still not sure, but we're moving in the direction of reducing hallucinations. Now with respect to physics, you're gonna have to give it an external database to rest on because internally, for really, really domain specific knowledge, it's not going to be as deterministic as one would like. These things work in continuous spaces. These things, they don't know what is wrong, what is true, and as a result, we have to give it tools. So everything that Ted demoed today is really striving at reducing hallucinations, actually, really, and giving it more abilities. I hope that answers your question. One of the ways to, I mean, I'm a simple guy. Like I tend to think that all of the world tends to be just a few things repeated over and over again, and we have human systems for this. You know, in a team, like companies work, or a team playing sport, and we're not right all the time, even when we aspire to be, and so we have systems that we've developed as humans to deal with things that may be wrong. So, you know, human number one proposes an answer, human number two checks their work, human number three provides the final sign off. This is really common. Anybody who's worked in a company has seen this in practice. The interesting thing about the state of software right now, we tend to be in this mode, in which we're just talking to GPT as one entity. But once we start thinking in terms of teams, so to speak, where each team member is its own agent with its own set of objectives and skills, I suspect we're going to start seeing a programming model in which the way to solve this might not necessarily be, make a single brain smarter, but instead be draw upon the collective intelligence of multiple software agents, each playing a role. And I think that that would certainly follow the human pattern of how we deal with this. To give an analogy, space shuttles, things that go into space, spacecraft, they have to be good. If they're not good, people die. They have no margin for error at all. And as a result, we over engineer in those systems, most spacecraft have three computers and they all have to agree in unison on a particular step to go forward. If one does not agree, then they recalculate, they recalculate, they recalculate until they arrive at something. The good thing is that hallucinations are generally not a systemic problem in terms of its knowledge. It's often a one off, the model, something tripped it up and it just produced a hallucination in that one instance. So if there's three models working in unison, just as Ted is saying, that will, generally speaking, improve your success. Yes, sir. A number of the examples you show have assertions like you are an engineer, you are an AI, you are a teacher. What's the mechanism by which that influences this computation of probabilities? Sure, I'm gonna give you what might be an unsatisfying answer, which is it tends to work. But I think we know why it tends to work, and again, it's because these language models approximate how we talk to each other. So if I were to say to you, hey, help me out, I need you to mock interview me. That's a direct statement I can make that kicks you into a certain mode of interaction. Or if I say to you, help me out, I'm trying to apologize to my wife, she's really mad at me, can you role play with me? That kicks you into another mode of interaction. And so it's really just a shorthand that people have found to kick the agent in, to kick the LLM in, to a certain mode of interaction that it tends to work in the way that I, as a software developer, am hoping it would work. And to really quickly add on to that, being in the digital humanities that I am, I like to think of it as a narrative. A narrative will have a few different characters talking to each other, their roles are clearly defined, two people are not the same. This interaction with GPT, it assumes a personality, it can simulate personalities. It itself is not conscious in any way, but it can certainly predict what a conscious being would react like in a particular situation. So when we're going URX, it is drawing up that personality and talking as though it is that person. Because it is like completing a transcript or completing a story in which that character is present and interacting and is active. So, yeah. I think we got about five minutes until the pizza outside. Eight minutes. Yes, sir. So I'm not a CF person, but it's been a fun thing with this. And I understand the sort of word-by-word generation and the sort of vibe, the feeling of it in the narrative. Some of my friends and I have tried giving it logic problems, like things from the LSAT, for example, and it doesn't work. Like, and I'm just wondering why that would be. So it will generate answers that sound very plausible rhetorically. Like, given this condition X, given this Y, but it'll often, like, even contradict itself in its answers, but it's almost never correct. So I was wondering what, why that would be? Like, it just can't reason, it can't, like, think. And, like, can you, would we get to a place where it can, so to speak? I mean, not, you know what I mean? I don't mean to think, like, it's conscious. I mean, like, have thoughts, not- You want to talk about react? So GPT-4, when GPT-4 released back in March, I think it was, it was passing LSAT. It was. It was, yeah. Yes. Yes, it just passed, as I understand it. Well, maybe it's because we're not GPT. That's one of the weird things. Is that- At GPT. Yeah. If you pay for chat GPT, they give you access to the better model. And one of the interesting things with it is prompting. It's so finicky. If you, it's very sensitive to the way that you prompt. There were earlier on when GPT-3 came out, some people were going, look, I can pass literacy tests, or no, it can't pass literacy tests. And then people who are pro or anti-GPT would be like, I modified the prompt a little bit, suddenly it can't, or suddenly it can't. These things are not conscious. Their ability to reason is like an alien's. They're not us. They don't think like people. They're not human. But they certainly are capable of passing some things empirically, which demonstrates some sort of rationale or logic within the model. But we're still slowly figuring out, like a prompt whisperer, what exactly the right approach is. Yeah? Obviously, having GPT-3 running and prompting it continuously is very expensive in terms of the user. Have you seen instances where it directly creates some sort of business value in for a whisperer, for a company with a real added value of having for these real AI apps in terms of like a review of the drives and the actual digital stuff? Yeah, I mean, we host companies on top of us, who that's their primary product. The value that it adds is like any company. I mean, it's, you know, what is the Y Combinator motto, make something people want? I mean, I wouldn't think of this as GPT inherently provides value for you as a builder. Like that's their product. That's OpenAI's product. You pay chat GPT for prioritized access. Where your product might be is how you take that and combine it with your data, somebody else's data, some domain knowledge, some interface that then helps apply it to something. It is, two things are both true. There are a lot of experiments going on right now, both for fun and people trying to figure out where the economic value is. But folks are also spinning up companies that are 100% supported by applying this to data. Okay, first, a company that wouldn't have, sort of, wouldn't be AI focused as an input, right? As it's just using or developing, like, announcements that use GPT for productivity. I think that it is likely that today we call this GPT and today we call these LLMs and tomorrow it will just slide into the ether. I mean, imagine what the progression is going to be. Today there's one of these that people are primarily playing with. There's many of them that exist, but one, people are primarily bidding on top. Tomorrow we can expect that there will be many of them and the day after that we can expect they're going to be on our phones and they're not even going to be connected to the internet. And for that reason, I think that, like today we don't call our software microprocessor tools or microprocessor apps, like the processor just exists. I think that one useful model, five years out, 10 years out, is to, even if it's only metaphorically true and not literally true, I think it's useful to think of this as a second processor. We had this before with what floating point co-processors and graphics co-processors already as recently as the 90s where it's useful to think of the trajectory of this as just another thing that computers to do can do and it will be incorporated into absolutely everything. Hence the term foundation model, which also crops up. So, pizza's ready? One more quick. Maybe one more and then we'll break for some food. In the glasses right there, sorry. Sorry, I was just being told we need to get two more. So, yeah. It's hard to get it to do that reliably. It's incredibly useful to get it to do reliably. So some tricks you can use are, you can give it examples. You can just ask it directly. Those are two common tricks. And look at the prompts that others have used to work. I mean, there's a lot of art to finding the right prompt right now. A lot of it is magic incantation. Another thing you can do is post-process it so that you can do some checking and you can have a happy path in which it's a one shot and you get your answer and then a sad path in which maybe you fall back on other prompts. So then you're going for the diversity of approach where it's fast by default. It's slow but ultimately converging upon higher likelihood of success if it fails. And then something that I'm sure we'll see and people do later on is fine-tune instruction tuning style models, which are more likely to respond with the computer parsable output. I guess one last question? Sure. So one of the, you talked a couple of things. One is, is you talked about domain expertise here and you were coding a bunch of domain expertise in terms of the prompts that you're going for. What is that, where do those prompts end up? Do those prompts end up back in the gene chat and is there a privacy issue associated with that? That's a great question. So the question was, and I apologize, I just realized we haven't been repeating all the questions for the YouTube listeners. So I'm sorry for the folks on YouTube if you weren't able to hear some of the questions. The question was, what are the privacy implications of some of these prompts? If one of the messages is so much depends upon your prompt and the fine-tuning of this prompt, what does that mean with respect to my IP? Maybe the prompt is my business. I can't offer you the exact answer but I can paint for you what approximately the landscape looks like. So in all of software and so too with AI, what we see is they're the SaaS companies where you're using somebody else's API and you're trusting that their terms of service will be upheld. There's the set of companies in which they provide a model for hosting on one of the big cloud providers and this is a version of the same thing but I think with slightly different mechanics. This tends to be thought of as the enterprise version of software and by and large, the industry has moved over the past 20 years from running my own servers to trusting that Microsoft or Amazon or Google can run servers for me and they say it's my private server even though I know they're running it and I'm okay with that. And you've already started to see that Amazon with Huggingface, Microsoft with OpenAI, Google too with their own version of Bard are going to do these where you'll have the SaaS version and then you'll also have the private VPC version and then there's a third version that I think we haven't yet seen practically emerge but this would be the maximalist. I wanna make sure my IP is maximally safe version of events in which you are running your own machines. You are running your own models and then the question is, is the open source and or privately available version of the model as good as the publicly hosted one and does that matter to me? And the answers right now, realistically, it probably matters a lot. In the fullness of time, you can think of any one particular task you need to achieve as requiring some fixed point of intelligence to achieve. And so over time, what we'll see is the privately obtainable versions of these models will cross that threshold and with respect to that one task, yeah, sure, use the open source version, run it on your own machine, but we'll also see the SaaS intelligence get smarter. It'll probably stay ahead. And then your question is, well, which one do I care more about? Do I want like the better aggregate intelligence or is my task somewhat fixed point and I can just use the open source available one for which I know it'll perform well enough because it's crossed the threshold. So to answer your question specifically, yes, you might be glad to know Chachi PT recently updated their privacy policy to not use prompts for the training process, but up until now, everything went back into the bin to be trained on again, okay. And that's just a fact. So I think pizza is now pizza time. Okay, okay. Okay. Yeah. But we'll have to talk about Q&A's and everything else. Perfect.", "segments": [{"id": 0, "seek": 0, "start": 0.0, "end": 1.56, "text": " All right.", "tokens": [50364, 1057, 558, 13, 50442], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 1, "seek": 0, "start": 1.56, "end": 3.56, "text": " Well, this is CS50 Tech Talk.", "tokens": [50442, 1042, 11, 341, 307, 9460, 2803, 13795, 8780, 13, 50542], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 2, "seek": 0, "start": 3.56, "end": 4.92, "text": " Thank you all so much for coming.", "tokens": [50542, 1044, 291, 439, 370, 709, 337, 1348, 13, 50610], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 3, "seek": 0, "start": 4.92, "end": 6.96, "text": " So about a week ago, we circulated the Google Form,", "tokens": [50610, 407, 466, 257, 1243, 2057, 11, 321, 12515, 770, 264, 3329, 10126, 11, 50712], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 4, "seek": 0, "start": 6.96, "end": 9.66, "text": " as you might have seen, at 10.52 AM.", "tokens": [50712, 382, 291, 1062, 362, 1612, 11, 412, 1266, 13, 17602, 6475, 13, 50847], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 5, "seek": 0, "start": 9.66, "end": 12.6, "text": " And by, like, 11.52 AM, we had 100 RSVPs,", "tokens": [50847, 400, 538, 11, 411, 11, 2975, 13, 17602, 6475, 11, 321, 632, 2319, 25855, 53, 23043, 11, 50994], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 6, "seek": 0, "start": 12.6, "end": 14.68, "text": " which I think is sort of testament to just how much interest", "tokens": [50994, 597, 286, 519, 307, 1333, 295, 35499, 281, 445, 577, 709, 1179, 51098], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 7, "seek": 0, "start": 14.68, "end": 18.1, "text": " there is in this world of AI, and open AI, and GPT, chat", "tokens": [51098, 456, 307, 294, 341, 1002, 295, 7318, 11, 293, 1269, 7318, 11, 293, 26039, 51, 11, 5081, 51269], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 8, "seek": 0, "start": 18.1, "end": 19.2, "text": " GPT, and the like.", "tokens": [51269, 26039, 51, 11, 293, 264, 411, 13, 51324], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 9, "seek": 0, "start": 19.2, "end": 21.84, "text": " And in fact, if you're sort of generally familiar with what", "tokens": [51324, 400, 294, 1186, 11, 498, 291, 434, 1333, 295, 5101, 4963, 365, 437, 51456], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 10, "seek": 0, "start": 21.84, "end": 24.32, "text": " everyone's talking about, but you haven't tried it yourself,", "tokens": [51456, 1518, 311, 1417, 466, 11, 457, 291, 2378, 380, 3031, 309, 1803, 11, 51580], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 11, "seek": 0, "start": 24.32, "end": 26.68, "text": " this is the URL, which you can try out this tool", "tokens": [51580, 341, 307, 264, 12905, 11, 597, 291, 393, 853, 484, 341, 2290, 51698], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 12, "seek": 0, "start": 26.68, "end": 29.12, "text": " that you've probably heard about, chat GPT.", "tokens": [51698, 300, 291, 600, 1391, 2198, 466, 11, 5081, 26039, 51, 13, 51820], "temperature": 0.0, "avg_logprob": -0.17327683585030693, "compression_ratio": 1.606936416184971, "no_speech_prob": 0.054947059601545334}, {"id": 13, "seek": 2912, "start": 29.12, "end": 30.68, "text": " You can sign up for a free account there", "tokens": [50364, 509, 393, 1465, 493, 337, 257, 1737, 2696, 456, 50442], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 14, "seek": 2912, "start": 30.68, "end": 32.800000000000004, "text": " and start tinkering with what everyone else has been", "tokens": [50442, 293, 722, 256, 475, 1794, 365, 437, 1518, 1646, 575, 668, 50548], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 15, "seek": 2912, "start": 32.800000000000004, "end": 33.64, "text": " tinkering with.", "tokens": [50548, 256, 475, 1794, 365, 13, 50590], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 16, "seek": 2912, "start": 33.64, "end": 36.4, "text": " And then if you're more of the app-minded type, which you probably", "tokens": [50590, 400, 550, 498, 291, 434, 544, 295, 264, 724, 12, 23310, 2010, 11, 597, 291, 1391, 50728], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 17, "seek": 2912, "start": 36.4, "end": 39.72, "text": " are if you are here with us today, open AI in particular", "tokens": [50728, 366, 498, 291, 366, 510, 365, 505, 965, 11, 1269, 7318, 294, 1729, 50894], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 18, "seek": 2912, "start": 39.72, "end": 43.24, "text": " has its own low-level APIs via which you can integrate AI", "tokens": [50894, 575, 1080, 1065, 2295, 12, 12418, 21445, 5766, 597, 291, 393, 13365, 7318, 51070], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 19, "seek": 2912, "start": 43.24, "end": 44.56, "text": " into your own software.", "tokens": [51070, 666, 428, 1065, 4722, 13, 51136], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 20, "seek": 2912, "start": 44.56, "end": 46.92, "text": " But of course, as is the case in computer science,", "tokens": [51136, 583, 295, 1164, 11, 382, 307, 264, 1389, 294, 3820, 3497, 11, 51254], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 21, "seek": 2912, "start": 46.92, "end": 49.040000000000006, "text": " there's all the more abstractions and services", "tokens": [51254, 456, 311, 439, 264, 544, 12649, 626, 293, 3328, 51360], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 22, "seek": 2912, "start": 49.040000000000006, "end": 51.2, "text": " that have been built on top of these technologies.", "tokens": [51360, 300, 362, 668, 3094, 322, 1192, 295, 613, 7943, 13, 51468], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 23, "seek": 2912, "start": 51.2, "end": 53.84, "text": " And we're so happy today to be joined by our friends", "tokens": [51468, 400, 321, 434, 370, 2055, 965, 281, 312, 6869, 538, 527, 1855, 51600], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 24, "seek": 2912, "start": 53.84, "end": 56.28, "text": " from McGill University and Steamship,", "tokens": [51600, 490, 21865, 373, 3535, 293, 3592, 4070, 1210, 11, 51722], "temperature": 0.0, "avg_logprob": -0.1304181162516276, "compression_ratio": 1.6567164179104477, "no_speech_prob": 0.001809978042729199}, {"id": 25, "seek": 5628, "start": 56.32, "end": 58.72, "text": " Sill and Ted, from whom you'll hear in just a moment,", "tokens": [50366, 318, 373, 293, 14985, 11, 490, 7101, 291, 603, 1568, 294, 445, 257, 1623, 11, 50486], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 26, "seek": 5628, "start": 58.72, "end": 62.160000000000004, "text": " to speak to us about how they are making it easier to build,", "tokens": [50486, 281, 1710, 281, 505, 466, 577, 436, 366, 1455, 309, 3571, 281, 1322, 11, 50658], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 27, "seek": 5628, "start": 62.160000000000004, "end": 64.68, "text": " to deploy, to share applications using some", "tokens": [50658, 281, 7274, 11, 281, 2073, 5821, 1228, 512, 50784], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 28, "seek": 5628, "start": 64.68, "end": 65.96000000000001, "text": " of these very same technologies.", "tokens": [50784, 295, 613, 588, 912, 7943, 13, 50848], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 29, "seek": 5628, "start": 65.96000000000001, "end": 68.24000000000001, "text": " So our thanks to them for hosting today.", "tokens": [50848, 407, 527, 3231, 281, 552, 337, 16058, 965, 13, 50962], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 30, "seek": 5628, "start": 68.24000000000001, "end": 70.08, "text": " Our friends at Plimpton, Jenny Lee and alumna,", "tokens": [50962, 2621, 1855, 412, 2149, 332, 21987, 11, 20580, 6957, 293, 12064, 629, 11, 51054], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 31, "seek": 5628, "start": 70.08, "end": 72.12, "text": " who's here with us today, but without further ado,", "tokens": [51054, 567, 311, 510, 365, 505, 965, 11, 457, 1553, 3052, 22450, 11, 51156], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 32, "seek": 5628, "start": 72.12, "end": 73.84, "text": " allow me to turn things over to Ted and Sill.", "tokens": [51156, 2089, 385, 281, 1261, 721, 670, 281, 14985, 293, 318, 373, 13, 51242], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 33, "seek": 5628, "start": 73.84, "end": 78.0, "text": " And pizza will be served shortly after 1 PM outside.", "tokens": [51242, 400, 8298, 486, 312, 7584, 13392, 934, 502, 12499, 2380, 13, 51450], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 34, "seek": 5628, "start": 78.0, "end": 79.76, "text": " All right, over to you, Ted.", "tokens": [51450, 1057, 558, 11, 670, 281, 291, 11, 14985, 13, 51538], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 35, "seek": 5628, "start": 79.76, "end": 81.4, "text": " Thanks a lot.", "tokens": [51538, 2561, 257, 688, 13, 51620], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 36, "seek": 5628, "start": 81.4, "end": 83.28, "text": " Hey, everybody, it's great to be here.", "tokens": [51620, 1911, 11, 2201, 11, 309, 311, 869, 281, 312, 510, 13, 51714], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 37, "seek": 5628, "start": 83.28, "end": 85.8, "text": " I think we've got a really good talk for you today.", "tokens": [51714, 286, 519, 321, 600, 658, 257, 534, 665, 751, 337, 291, 965, 13, 51840], "temperature": 0.0, "avg_logprob": -0.1607674569794626, "compression_ratio": 1.603988603988604, "no_speech_prob": 0.005554115865379572}, {"id": 38, "seek": 8580, "start": 85.8, "end": 88.08, "text": " Sill's gonna provide some research grounding", "tokens": [50364, 318, 373, 311, 799, 2893, 512, 2132, 46727, 50478], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 39, "seek": 8580, "start": 88.08, "end": 90.92, "text": " into how it all works, what's going inside,", "tokens": [50478, 666, 577, 309, 439, 1985, 11, 437, 311, 516, 1854, 11, 50620], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 40, "seek": 8580, "start": 90.92, "end": 94.12, "text": " the brain of GPT as well as other language models.", "tokens": [50620, 264, 3567, 295, 26039, 51, 382, 731, 382, 661, 2856, 5245, 13, 50780], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 41, "seek": 8580, "start": 94.12, "end": 96.0, "text": " And then I'll show you some examples that we're seeing", "tokens": [50780, 400, 550, 286, 603, 855, 291, 512, 5110, 300, 321, 434, 2577, 50874], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 42, "seek": 8580, "start": 96.0, "end": 98.24, "text": " on the ground of how people are building apps", "tokens": [50874, 322, 264, 2727, 295, 577, 561, 366, 2390, 7733, 50986], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 43, "seek": 8580, "start": 98.24, "end": 100.38, "text": " and what apps tend to work in the real world.", "tokens": [50986, 293, 437, 7733, 3928, 281, 589, 294, 264, 957, 1002, 13, 51093], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 44, "seek": 8580, "start": 100.38, "end": 103.47999999999999, "text": " So our perspective is we're building AWS for AI apps.", "tokens": [51093, 407, 527, 4585, 307, 321, 434, 2390, 17650, 337, 7318, 7733, 13, 51248], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 45, "seek": 8580, "start": 103.47999999999999, "end": 105.16, "text": " So we get to talk to a lot of the makers", "tokens": [51248, 407, 321, 483, 281, 751, 281, 257, 688, 295, 264, 19323, 51332], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 46, "seek": 8580, "start": 105.16, "end": 106.6, "text": " who are building and deploying their apps.", "tokens": [51332, 567, 366, 2390, 293, 34198, 641, 7733, 13, 51404], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 47, "seek": 8580, "start": 106.6, "end": 108.92, "text": " And through that, see both the experimental end", "tokens": [51404, 400, 807, 300, 11, 536, 1293, 264, 17069, 917, 51520], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 48, "seek": 8580, "start": 108.92, "end": 111.8, "text": " of the spectrum and also see what kinds of apps", "tokens": [51520, 295, 264, 11143, 293, 611, 536, 437, 3685, 295, 7733, 51664], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 49, "seek": 8580, "start": 111.8, "end": 113.72, "text": " are getting pushed out there and turned into companies,", "tokens": [51664, 366, 1242, 9152, 484, 456, 293, 3574, 666, 3431, 11, 51760], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 50, "seek": 8580, "start": 113.72, "end": 115.52, "text": " turned into side projects.", "tokens": [51760, 3574, 666, 1252, 4455, 13, 51850], "temperature": 0.0, "avg_logprob": -0.09946878139789288, "compression_ratio": 1.7840236686390532, "no_speech_prob": 0.0006260875379666686}, {"id": 51, "seek": 11552, "start": 115.56, "end": 119.2, "text": " We did a cool hackathon yesterday.", "tokens": [50366, 492, 630, 257, 1627, 10339, 18660, 5186, 13, 50548], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 52, "seek": 11552, "start": 119.2, "end": 122.0, "text": " Many thanks to Neiman, to David Malin and CS50", "tokens": [50548, 5126, 3231, 281, 1734, 25504, 11, 281, 4389, 5746, 259, 293, 9460, 2803, 50688], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 53, "seek": 11552, "start": 122.0, "end": 123.39999999999999, "text": " for helping us put all of this together,", "tokens": [50688, 337, 4315, 505, 829, 439, 295, 341, 1214, 11, 50758], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 54, "seek": 11552, "start": 123.39999999999999, "end": 124.75999999999999, "text": " to Harvard for hosting it.", "tokens": [50758, 281, 13378, 337, 16058, 309, 13, 50826], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 55, "seek": 11552, "start": 124.75999999999999, "end": 127.92, "text": " And there were two sessions, lots of folks built things.", "tokens": [50826, 400, 456, 645, 732, 11081, 11, 3195, 295, 4024, 3094, 721, 13, 50984], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 56, "seek": 11552, "start": 127.92, "end": 131.1, "text": " If you go to steamship.com slash hackathon,", "tokens": [50984, 759, 291, 352, 281, 2126, 4070, 1210, 13, 1112, 17330, 10339, 18660, 11, 51143], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 57, "seek": 11552, "start": 131.1, "end": 132.4, "text": " you'll find a lot of guides,", "tokens": [51143, 291, 603, 915, 257, 688, 295, 17007, 11, 51208], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 58, "seek": 11552, "start": 132.4, "end": 133.84, "text": " a lot of projects that people built.", "tokens": [51208, 257, 688, 295, 4455, 300, 561, 3094, 13, 51280], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 59, "seek": 11552, "start": 133.84, "end": 136.56, "text": " And you can follow along, we have a text guide as well,", "tokens": [51280, 400, 291, 393, 1524, 2051, 11, 321, 362, 257, 2487, 5934, 382, 731, 11, 51416], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 60, "seek": 11552, "start": 136.56, "end": 138.44, "text": " just as a quick plug for that,", "tokens": [51416, 445, 382, 257, 1702, 5452, 337, 300, 11, 51510], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 61, "seek": 11552, "start": 138.44, "end": 141.24, "text": " if you want to do it remotely or on your own.", "tokens": [51510, 498, 291, 528, 281, 360, 309, 20824, 420, 322, 428, 1065, 13, 51650], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 62, "seek": 11552, "start": 141.24, "end": 143.88, "text": " So to tee up Sill, we're gonna talk about", "tokens": [51650, 407, 281, 33863, 493, 318, 373, 11, 321, 434, 799, 751, 466, 51782], "temperature": 0.0, "avg_logprob": -0.1288113946082608, "compression_ratio": 1.5838709677419356, "no_speech_prob": 0.0003799459955189377}, {"id": 63, "seek": 14388, "start": 143.88, "end": 147.16, "text": " basically two things today that I hope you'll walk away with", "tokens": [50364, 1936, 732, 721, 965, 300, 286, 1454, 291, 603, 1792, 1314, 365, 50528], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 64, "seek": 14388, "start": 147.16, "end": 149.84, "text": " and really know how to then use as you develop", "tokens": [50528, 293, 534, 458, 577, 281, 550, 764, 382, 291, 1499, 50662], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 65, "seek": 14388, "start": 149.84, "end": 150.68, "text": " and as you tinker.", "tokens": [50662, 293, 382, 291, 256, 40467, 13, 50704], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 66, "seek": 14388, "start": 150.68, "end": 153.16, "text": " One is what is GPT and how is it working?", "tokens": [50704, 1485, 307, 437, 307, 26039, 51, 293, 577, 307, 309, 1364, 30, 50828], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 67, "seek": 14388, "start": 153.16, "end": 155.24, "text": " Get a good sense of what's going on inside of it", "tokens": [50828, 3240, 257, 665, 2020, 295, 437, 311, 516, 322, 1854, 295, 309, 50932], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 68, "seek": 14388, "start": 155.24, "end": 158.28, "text": " other than as just this magical machine that predicts things.", "tokens": [50932, 661, 813, 382, 445, 341, 12066, 3479, 300, 6069, 82, 721, 13, 51084], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 69, "seek": 14388, "start": 158.28, "end": 160.72, "text": " And then two is how are people building with it?", "tokens": [51084, 400, 550, 732, 307, 577, 366, 561, 2390, 365, 309, 30, 51206], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 70, "seek": 14388, "start": 160.72, "end": 162.8, "text": " And then importantly, how can I build with it too", "tokens": [51206, 400, 550, 8906, 11, 577, 393, 286, 1322, 365, 309, 886, 51310], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 71, "seek": 14388, "start": 162.8, "end": 163.68, "text": " if you're a developer?", "tokens": [51310, 498, 291, 434, 257, 10754, 30, 51354], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 72, "seek": 14388, "start": 163.68, "end": 165.8, "text": " And if you have CS50 background,", "tokens": [51354, 400, 498, 291, 362, 9460, 2803, 3678, 11, 51460], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 73, "seek": 14388, "start": 165.8, "end": 167.16, "text": " you should be able to pick things up", "tokens": [51460, 291, 820, 312, 1075, 281, 1888, 721, 493, 51528], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 74, "seek": 14388, "start": 167.16, "end": 168.16, "text": " and start building some great apps.", "tokens": [51528, 293, 722, 2390, 512, 869, 7733, 13, 51578], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 75, "seek": 14388, "start": 168.16, "end": 170.07999999999998, "text": " I've already met some of the CS50 grads yesterday", "tokens": [51578, 286, 600, 1217, 1131, 512, 295, 264, 9460, 2803, 2771, 82, 5186, 51674], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 76, "seek": 14388, "start": 170.07999999999998, "end": 171.8, "text": " and the things that they were doing were pretty amazing.", "tokens": [51674, 293, 264, 721, 300, 436, 645, 884, 645, 1238, 2243, 13, 51760], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 77, "seek": 14388, "start": 171.8, "end": 173.0, "text": " So hope this is useful.", "tokens": [51760, 407, 1454, 341, 307, 4420, 13, 51820], "temperature": 0.0, "avg_logprob": -0.09565217928452925, "compression_ratio": 1.7384196185286103, "no_speech_prob": 0.0016482671489939094}, {"id": 78, "seek": 17300, "start": 173.0, "end": 174.32, "text": " I'm gonna kick it over to Sill", "tokens": [50364, 286, 478, 799, 4437, 309, 670, 281, 318, 373, 50430], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 79, "seek": 17300, "start": 174.32, "end": 179.28, "text": " and talk about some of the theoretical background of GPT.", "tokens": [50430, 293, 751, 466, 512, 295, 264, 20864, 3678, 295, 26039, 51, 13, 50678], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 80, "seek": 17300, "start": 179.28, "end": 181.16, "text": " Yeah, so thank you, Ted.", "tokens": [50678, 865, 11, 370, 1309, 291, 11, 14985, 13, 50772], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 81, "seek": 17300, "start": 181.16, "end": 182.0, "text": " My name is Sill.", "tokens": [50772, 1222, 1315, 307, 318, 373, 13, 50814], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 82, "seek": 17300, "start": 182.0, "end": 184.56, "text": " I'm a graduate student of Digital Humanities at McGill.", "tokens": [50814, 286, 478, 257, 8080, 3107, 295, 15522, 10294, 1088, 412, 21865, 373, 13, 50942], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 83, "seek": 17300, "start": 184.56, "end": 186.56, "text": " I study literature and computer science", "tokens": [50942, 286, 2979, 10394, 293, 3820, 3497, 51042], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 84, "seek": 17300, "start": 186.56, "end": 188.48, "text": " and linguistics in the same breath.", "tokens": [51042, 293, 21766, 6006, 294, 264, 912, 6045, 13, 51138], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 85, "seek": 17300, "start": 188.48, "end": 190.52, "text": " And I've published some research over the last couple of years", "tokens": [51138, 400, 286, 600, 6572, 512, 2132, 670, 264, 1036, 1916, 295, 924, 51240], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 86, "seek": 17300, "start": 190.52, "end": 193.8, "text": " exploring what is possible with language models", "tokens": [51240, 12736, 437, 307, 1944, 365, 2856, 5245, 51404], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 87, "seek": 17300, "start": 193.8, "end": 195.64, "text": " and culture in particular.", "tokens": [51404, 293, 3713, 294, 1729, 13, 51496], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 88, "seek": 17300, "start": 195.64, "end": 199.76, "text": " And my half or whatever of the presentation", "tokens": [51496, 400, 452, 1922, 420, 2035, 295, 264, 5860, 51702], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 89, "seek": 17300, "start": 199.76, "end": 201.88, "text": " is to describe to you what is GPT.", "tokens": [51702, 307, 281, 6786, 281, 291, 437, 307, 26039, 51, 13, 51808], "temperature": 0.0, "avg_logprob": -0.11963624220628005, "compression_ratio": 1.586092715231788, "no_speech_prob": 0.0004171421751379967}, {"id": 90, "seek": 20188, "start": 201.88, "end": 204.24, "text": " That's really difficult to explain in 15 minutes.", "tokens": [50364, 663, 311, 534, 2252, 281, 2903, 294, 2119, 2077, 13, 50482], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 91, "seek": 20188, "start": 204.24, "end": 206.72, "text": " And there are even a lot of things that we don't know,", "tokens": [50482, 400, 456, 366, 754, 257, 688, 295, 721, 300, 321, 500, 380, 458, 11, 50606], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 92, "seek": 20188, "start": 206.72, "end": 208.04, "text": " but a good way to approach that", "tokens": [50606, 457, 257, 665, 636, 281, 3109, 300, 50672], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 93, "seek": 20188, "start": 208.04, "end": 210.2, "text": " is to first consider all the things", "tokens": [50672, 307, 281, 700, 1949, 439, 264, 721, 50780], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 94, "seek": 20188, "start": 210.2, "end": 213.32, "text": " that people call GPT by or descriptors.", "tokens": [50780, 300, 561, 818, 26039, 51, 538, 420, 31280, 830, 13, 50936], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 95, "seek": 20188, "start": 213.32, "end": 215.68, "text": " So you can call them large language models.", "tokens": [50936, 407, 291, 393, 818, 552, 2416, 2856, 5245, 13, 51054], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 96, "seek": 20188, "start": 215.68, "end": 218.0, "text": " You can call them universal approximators", "tokens": [51054, 509, 393, 818, 552, 11455, 8542, 3391, 51170], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 97, "seek": 20188, "start": 218.0, "end": 219.04, "text": " from computer science.", "tokens": [51170, 490, 3820, 3497, 13, 51222], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 98, "seek": 20188, "start": 219.04, "end": 222.44, "text": " You can say that it is a generative AI.", "tokens": [51222, 509, 393, 584, 300, 309, 307, 257, 1337, 1166, 7318, 13, 51392], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 99, "seek": 20188, "start": 222.44, "end": 224.04, "text": " We know that they are neural networks.", "tokens": [51392, 492, 458, 300, 436, 366, 18161, 9590, 13, 51472], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 100, "seek": 20188, "start": 224.04, "end": 226.44, "text": " We know that it is an artificial intelligence.", "tokens": [51472, 492, 458, 300, 309, 307, 364, 11677, 7599, 13, 51592], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 101, "seek": 20188, "start": 226.44, "end": 228.16, "text": " To some, it's a simulator of culture.", "tokens": [51592, 1407, 512, 11, 309, 311, 257, 32974, 295, 3713, 13, 51678], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 102, "seek": 20188, "start": 228.16, "end": 230.28, "text": " To others, it just predicts text.", "tokens": [51678, 1407, 2357, 11, 309, 445, 6069, 82, 2487, 13, 51784], "temperature": 0.0, "avg_logprob": -0.07294656846906755, "compression_ratio": 1.6796116504854368, "no_speech_prob": 0.000578879495151341}, {"id": 103, "seek": 23028, "start": 230.32, "end": 231.48, "text": " It's also a writing assistant.", "tokens": [50366, 467, 311, 611, 257, 3579, 10994, 13, 50424], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 104, "seek": 23028, "start": 231.48, "end": 232.84, "text": " If you've ever used ChatGPT,", "tokens": [50424, 759, 291, 600, 1562, 1143, 27503, 38, 47, 51, 11, 50492], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 105, "seek": 23028, "start": 232.84, "end": 235.0, "text": " you can plug in a bit of your essay, get some feedback.", "tokens": [50492, 291, 393, 5452, 294, 257, 857, 295, 428, 16238, 11, 483, 512, 5824, 13, 50600], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 106, "seek": 23028, "start": 235.0, "end": 236.52, "text": " It's amazing for that.", "tokens": [50600, 467, 311, 2243, 337, 300, 13, 50676], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 107, "seek": 23028, "start": 236.52, "end": 237.56, "text": " It's a content generator.", "tokens": [50676, 467, 311, 257, 2701, 19265, 13, 50728], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 108, "seek": 23028, "start": 237.56, "end": 240.76, "text": " People use it to do copywriting, Jasper.AI,", "tokens": [50728, 3432, 764, 309, 281, 360, 5055, 19868, 11, 34023, 610, 13, 48698, 11, 50888], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 109, "seek": 23028, "start": 240.76, "end": 242.2, "text": " pseudo-write, et cetera.", "tokens": [50888, 35899, 12, 21561, 11, 1030, 11458, 13, 50960], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 110, "seek": 23028, "start": 242.2, "end": 243.64, "text": " It's an agent.", "tokens": [50960, 467, 311, 364, 9461, 13, 51032], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 111, "seek": 23028, "start": 243.64, "end": 245.16, "text": " So the really hot thing right now,", "tokens": [51032, 407, 264, 534, 2368, 551, 558, 586, 11, 51108], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 112, "seek": 23028, "start": 245.16, "end": 248.48, "text": " if you might have seen it on Twitter, auto-GPT, baby-AGI,", "tokens": [51108, 498, 291, 1062, 362, 1612, 309, 322, 5794, 11, 8399, 12, 38, 47, 51, 11, 3186, 12, 32, 26252, 11, 51274], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 113, "seek": 23028, "start": 248.48, "end": 250.56, "text": " people are giving these things tools", "tokens": [51274, 561, 366, 2902, 613, 721, 3873, 51378], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 114, "seek": 23028, "start": 250.56, "end": 253.16, "text": " and letting them run a little bit free in the wild", "tokens": [51378, 293, 8295, 552, 1190, 257, 707, 857, 1737, 294, 264, 4868, 51508], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 115, "seek": 23028, "start": 253.16, "end": 256.12, "text": " to interact with the world computers, et cetera.", "tokens": [51508, 281, 4648, 365, 264, 1002, 10807, 11, 1030, 11458, 13, 51656], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 116, "seek": 23028, "start": 256.12, "end": 258.16, "text": " We use them as chatbots, obviously.", "tokens": [51656, 492, 764, 552, 382, 5081, 65, 1971, 11, 2745, 13, 51758], "temperature": 0.0, "avg_logprob": -0.13258694139726324, "compression_ratio": 1.652733118971061, "no_speech_prob": 0.0004043389344587922}, {"id": 117, "seek": 25816, "start": 258.16, "end": 261.84000000000003, "text": " And the actual architecture is a transformer.", "tokens": [50364, 400, 264, 3539, 9482, 307, 257, 31782, 13, 50548], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 118, "seek": 25816, "start": 261.84000000000003, "end": 264.48, "text": " So there's lots of ways to describe GPT.", "tokens": [50548, 407, 456, 311, 3195, 295, 2098, 281, 6786, 26039, 51, 13, 50680], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 119, "seek": 25816, "start": 264.48, "end": 267.76000000000005, "text": " And any of them is a really perfectly adequate way", "tokens": [50680, 400, 604, 295, 552, 307, 257, 534, 6239, 20927, 636, 50844], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 120, "seek": 25816, "start": 267.76000000000005, "end": 269.64000000000004, "text": " to begin the conversation.", "tokens": [50844, 281, 1841, 264, 3761, 13, 50938], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 121, "seek": 25816, "start": 269.64000000000004, "end": 271.32000000000005, "text": " But for our purposes, we can think of it", "tokens": [50938, 583, 337, 527, 9932, 11, 321, 393, 519, 295, 309, 51022], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 122, "seek": 25816, "start": 271.32000000000005, "end": 272.48, "text": " as a large language model,", "tokens": [51022, 382, 257, 2416, 2856, 2316, 11, 51080], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 123, "seek": 25816, "start": 272.48, "end": 274.92, "text": " and more specifically, a language model.", "tokens": [51080, 293, 544, 4682, 11, 257, 2856, 2316, 13, 51202], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 124, "seek": 25816, "start": 274.92, "end": 279.08000000000004, "text": " And a language model is a model of language,", "tokens": [51202, 400, 257, 2856, 2316, 307, 257, 2316, 295, 2856, 11, 51410], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 125, "seek": 25816, "start": 279.08000000000004, "end": 280.36, "text": " if you allow me the tautology,", "tokens": [51410, 498, 291, 2089, 385, 264, 256, 1375, 1793, 11, 51474], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 126, "seek": 25816, "start": 280.36, "end": 281.96000000000004, "text": " but really what it does is it produces", "tokens": [51474, 457, 534, 437, 309, 775, 307, 309, 14725, 51554], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 127, "seek": 25816, "start": 281.96000000000004, "end": 284.76000000000005, "text": " a probability distribution over some vocabulary.", "tokens": [51554, 257, 8482, 7316, 670, 512, 19864, 13, 51694], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 128, "seek": 25816, "start": 284.76000000000005, "end": 287.52000000000004, "text": " So let us imagine that we had the task", "tokens": [51694, 407, 718, 505, 3811, 300, 321, 632, 264, 5633, 51832], "temperature": 0.0, "avg_logprob": -0.14081447330985483, "compression_ratio": 1.676056338028169, "no_speech_prob": 0.00010386202484369278}, {"id": 129, "seek": 28752, "start": 288.03999999999996, "end": 291.0, "text": " of predicting the next word of the sequence I am.", "tokens": [50390, 295, 32884, 264, 958, 1349, 295, 264, 8310, 286, 669, 13, 50538], "temperature": 0.0, "avg_logprob": -0.1063898924355195, "compression_ratio": 1.6, "no_speech_prob": 0.0009103714255616069}, {"id": 130, "seek": 28752, "start": 291.0, "end": 295.35999999999996, "text": " So if I give a neural network the words I am,", "tokens": [50538, 407, 498, 286, 976, 257, 18161, 3209, 264, 2283, 286, 669, 11, 50756], "temperature": 0.0, "avg_logprob": -0.1063898924355195, "compression_ratio": 1.6, "no_speech_prob": 0.0009103714255616069}, {"id": 131, "seek": 28752, "start": 295.35999999999996, "end": 297.88, "text": " what of all words in English", "tokens": [50756, 437, 295, 439, 2283, 294, 3669, 50882], "temperature": 0.0, "avg_logprob": -0.1063898924355195, "compression_ratio": 1.6, "no_speech_prob": 0.0009103714255616069}, {"id": 132, "seek": 28752, "start": 297.88, "end": 300.0, "text": " is the next most likely word to follow?", "tokens": [50882, 307, 264, 958, 881, 3700, 1349, 281, 1524, 30, 50988], "temperature": 0.0, "avg_logprob": -0.1063898924355195, "compression_ratio": 1.6, "no_speech_prob": 0.0009103714255616069}, {"id": 133, "seek": 28752, "start": 300.0, "end": 304.68, "text": " That, at its very core, is what GPT is trained to answer.", "tokens": [50988, 663, 11, 412, 1080, 588, 4965, 11, 307, 437, 26039, 51, 307, 8895, 281, 1867, 13, 51222], "temperature": 0.0, "avg_logprob": -0.1063898924355195, "compression_ratio": 1.6, "no_speech_prob": 0.0009103714255616069}, {"id": 134, "seek": 28752, "start": 304.68, "end": 308.68, "text": " And how it does it is it has a vocabulary of 50,000 words,", "tokens": [51222, 400, 577, 309, 775, 309, 307, 309, 575, 257, 19864, 295, 2625, 11, 1360, 2283, 11, 51422], "temperature": 0.0, "avg_logprob": -0.1063898924355195, "compression_ratio": 1.6, "no_speech_prob": 0.0009103714255616069}, {"id": 135, "seek": 28752, "start": 308.68, "end": 312.03999999999996, "text": " and it knows, roughly, given the entire internet,", "tokens": [51422, 293, 309, 3255, 11, 9810, 11, 2212, 264, 2302, 4705, 11, 51590], "temperature": 0.0, "avg_logprob": -0.1063898924355195, "compression_ratio": 1.6, "no_speech_prob": 0.0009103714255616069}, {"id": 136, "seek": 28752, "start": 312.03999999999996, "end": 314.84, "text": " which words are likely to follow other words", "tokens": [51590, 597, 2283, 366, 3700, 281, 1524, 661, 2283, 51730], "temperature": 0.0, "avg_logprob": -0.1063898924355195, "compression_ratio": 1.6, "no_speech_prob": 0.0009103714255616069}, {"id": 137, "seek": 31484, "start": 315.84, "end": 317.96, "text": " of those 50,000 in some sequence,", "tokens": [50414, 295, 729, 2625, 11, 1360, 294, 512, 8310, 11, 50520], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 138, "seek": 31484, "start": 317.96, "end": 321.2, "text": " up to 2,000 words, up to 4,000, up to 8,000,", "tokens": [50520, 493, 281, 568, 11, 1360, 2283, 11, 493, 281, 1017, 11, 1360, 11, 493, 281, 1649, 11, 1360, 11, 50682], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 139, "seek": 31484, "start": 321.2, "end": 323.64, "text": " and now up to 32,000 GPT4.", "tokens": [50682, 293, 586, 493, 281, 8858, 11, 1360, 26039, 51, 19, 13, 50804], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 140, "seek": 31484, "start": 323.64, "end": 326.2, "text": " So you give it a sequence, here I am,", "tokens": [50804, 407, 291, 976, 309, 257, 8310, 11, 510, 286, 669, 11, 50932], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 141, "seek": 31484, "start": 326.2, "end": 329.12, "text": " and over the vocabulary of 50,000 words,", "tokens": [50932, 293, 670, 264, 19864, 295, 2625, 11, 1360, 2283, 11, 51078], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 142, "seek": 31484, "start": 329.12, "end": 332.47999999999996, "text": " it gives you the likelihood of every single word that follows.", "tokens": [51078, 309, 2709, 291, 264, 22119, 295, 633, 2167, 1349, 300, 10002, 13, 51246], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 143, "seek": 31484, "start": 332.47999999999996, "end": 336.91999999999996, "text": " So here it's I am, perhaps the word happy is fairly frequent,", "tokens": [51246, 407, 510, 309, 311, 286, 669, 11, 4317, 264, 1349, 2055, 307, 6457, 18004, 11, 51468], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 144, "seek": 31484, "start": 336.91999999999996, "end": 338.35999999999996, "text": " so we'll get that high probability", "tokens": [51468, 370, 321, 603, 483, 300, 1090, 8482, 51540], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 145, "seek": 31484, "start": 338.35999999999996, "end": 341.47999999999996, "text": " if we look at all words, all utterances of English.", "tokens": [51540, 498, 321, 574, 412, 439, 2283, 11, 439, 17567, 2676, 295, 3669, 13, 51696], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 146, "seek": 31484, "start": 341.47999999999996, "end": 344.79999999999995, "text": " It might be I am sad, maybe that's a little bit less probable,", "tokens": [51696, 467, 1062, 312, 286, 669, 4227, 11, 1310, 300, 311, 257, 707, 857, 1570, 21759, 11, 51862], "temperature": 0.0, "avg_logprob": -0.09750681910021551, "compression_ratio": 1.6813186813186813, "no_speech_prob": 0.0006663964595645666}, {"id": 147, "seek": 34480, "start": 345.0, "end": 346.88, "text": " I am school, that really should be at the end", "tokens": [50374, 286, 669, 1395, 11, 300, 534, 820, 312, 412, 264, 917, 50468], "temperature": 0.0, "avg_logprob": -0.11324396206222417, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0002530486381147057}, {"id": 148, "seek": 34480, "start": 346.88, "end": 348.8, "text": " because I don't think anybody would ever say that,", "tokens": [50468, 570, 286, 500, 380, 519, 4472, 576, 1562, 584, 300, 11, 50564], "temperature": 0.0, "avg_logprob": -0.11324396206222417, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0002530486381147057}, {"id": 149, "seek": 34480, "start": 348.8, "end": 351.76, "text": " I am Bjork, that's a little bit, it's not very probable,", "tokens": [50564, 286, 669, 49660, 1284, 11, 300, 311, 257, 707, 857, 11, 309, 311, 406, 588, 21759, 11, 50712], "temperature": 0.0, "avg_logprob": -0.11324396206222417, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0002530486381147057}, {"id": 150, "seek": 34480, "start": 351.76, "end": 353.72, "text": " but it's less probable than happy sad,", "tokens": [50712, 457, 309, 311, 1570, 21759, 813, 2055, 4227, 11, 50810], "temperature": 0.0, "avg_logprob": -0.11324396206222417, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0002530486381147057}, {"id": 151, "seek": 34480, "start": 353.72, "end": 355.8, "text": " but there's still some probability attached to it.", "tokens": [50810, 457, 456, 311, 920, 512, 8482, 8570, 281, 309, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11324396206222417, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0002530486381147057}, {"id": 152, "seek": 34480, "start": 355.8, "end": 358.28000000000003, "text": " And when we say it's probable, that's literally a percentage,", "tokens": [50914, 400, 562, 321, 584, 309, 311, 21759, 11, 300, 311, 3736, 257, 9668, 11, 51038], "temperature": 0.0, "avg_logprob": -0.11324396206222417, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0002530486381147057}, {"id": 153, "seek": 34480, "start": 358.28000000000003, "end": 363.28000000000003, "text": " that's like happy follows I am maybe like 5% of the time,", "tokens": [51038, 300, 311, 411, 2055, 10002, 286, 669, 1310, 411, 1025, 4, 295, 264, 565, 11, 51288], "temperature": 0.0, "avg_logprob": -0.11324396206222417, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0002530486381147057}, {"id": 154, "seek": 34480, "start": 363.40000000000003, "end": 366.96000000000004, "text": " sad follows I am maybe 2% of the time, or whatever.", "tokens": [51294, 4227, 10002, 286, 669, 1310, 568, 4, 295, 264, 565, 11, 420, 2035, 13, 51472], "temperature": 0.0, "avg_logprob": -0.11324396206222417, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0002530486381147057}, {"id": 155, "seek": 34480, "start": 366.96000000000004, "end": 371.52, "text": " So for every word that we give GPT,", "tokens": [51472, 407, 337, 633, 1349, 300, 321, 976, 26039, 51, 11, 51700], "temperature": 0.0, "avg_logprob": -0.11324396206222417, "compression_ratio": 1.7413127413127414, "no_speech_prob": 0.0002530486381147057}, {"id": 156, "seek": 37152, "start": 371.52, "end": 373.91999999999996, "text": " it tries to predict what the next word is", "tokens": [50364, 309, 9898, 281, 6069, 437, 264, 958, 1349, 307, 50484], "temperature": 0.0, "avg_logprob": -0.11342333945907465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00018222149810753763}, {"id": 157, "seek": 37152, "start": 373.91999999999996, "end": 376.91999999999996, "text": " across 50,000 words, and it gives every single one", "tokens": [50484, 2108, 2625, 11, 1360, 2283, 11, 293, 309, 2709, 633, 2167, 472, 50634], "temperature": 0.0, "avg_logprob": -0.11342333945907465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00018222149810753763}, {"id": 158, "seek": 37152, "start": 376.91999999999996, "end": 381.91999999999996, "text": " of those 50,000 words a number that reflects how probable it is.", "tokens": [50634, 295, 729, 2625, 11, 1360, 2283, 257, 1230, 300, 18926, 577, 21759, 309, 307, 13, 50884], "temperature": 0.0, "avg_logprob": -0.11342333945907465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00018222149810753763}, {"id": 159, "seek": 37152, "start": 383.15999999999997, "end": 385.35999999999996, "text": " And the really magical thing that happens", "tokens": [50946, 400, 264, 534, 12066, 551, 300, 2314, 51056], "temperature": 0.0, "avg_logprob": -0.11342333945907465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00018222149810753763}, {"id": 160, "seek": 37152, "start": 385.35999999999996, "end": 390.28, "text": " is you can generate new text, so if you give GPT I am,", "tokens": [51056, 307, 291, 393, 8460, 777, 2487, 11, 370, 498, 291, 976, 26039, 51, 286, 669, 11, 51302], "temperature": 0.0, "avg_logprob": -0.11342333945907465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00018222149810753763}, {"id": 161, "seek": 37152, "start": 390.28, "end": 394.24, "text": " and it predicts happy as being the most probable word", "tokens": [51302, 293, 309, 6069, 82, 2055, 382, 885, 264, 881, 21759, 1349, 51500], "temperature": 0.0, "avg_logprob": -0.11342333945907465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00018222149810753763}, {"id": 162, "seek": 37152, "start": 394.24, "end": 397.88, "text": " over 50,000, you can then append it to I am,", "tokens": [51500, 670, 2625, 11, 1360, 11, 291, 393, 550, 34116, 309, 281, 286, 669, 11, 51682], "temperature": 0.0, "avg_logprob": -0.11342333945907465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00018222149810753763}, {"id": 163, "seek": 37152, "start": 397.88, "end": 401.35999999999996, "text": " so now you say I am happy, and you feed it into the model again,", "tokens": [51682, 370, 586, 291, 584, 286, 669, 2055, 11, 293, 291, 3154, 309, 666, 264, 2316, 797, 11, 51856], "temperature": 0.0, "avg_logprob": -0.11342333945907465, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.00018222149810753763}, {"id": 164, "seek": 40136, "start": 401.48, "end": 403.68, "text": " you sample another word, you feed it into the model again,", "tokens": [50370, 291, 6889, 1071, 1349, 11, 291, 3154, 309, 666, 264, 2316, 797, 11, 50480], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 165, "seek": 40136, "start": 403.68, "end": 405.24, "text": " and again, and again, and again,", "tokens": [50480, 293, 797, 11, 293, 797, 11, 293, 797, 11, 50558], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 166, "seek": 40136, "start": 405.24, "end": 407.72, "text": " and there's lots of different ways that I am happy,", "tokens": [50558, 293, 456, 311, 3195, 295, 819, 2098, 300, 286, 669, 2055, 11, 50682], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 167, "seek": 40136, "start": 407.72, "end": 410.92, "text": " I am sad, can go, and you add a little bit of randomness,", "tokens": [50682, 286, 669, 4227, 11, 393, 352, 11, 293, 291, 909, 257, 707, 857, 295, 4974, 1287, 11, 50842], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 168, "seek": 40136, "start": 410.92, "end": 412.52000000000004, "text": " and all of a sudden you have a language model", "tokens": [50842, 293, 439, 295, 257, 3990, 291, 362, 257, 2856, 2316, 50922], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 169, "seek": 40136, "start": 412.52000000000004, "end": 414.84000000000003, "text": " that can write essays, that can talk,", "tokens": [50922, 300, 393, 2464, 35123, 11, 300, 393, 751, 11, 51038], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 170, "seek": 40136, "start": 414.84000000000003, "end": 417.76, "text": " and a whole lot of things, which is really unexpected,", "tokens": [51038, 293, 257, 1379, 688, 295, 721, 11, 597, 307, 534, 13106, 11, 51184], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 171, "seek": 40136, "start": 417.76, "end": 419.12, "text": " and something that we didn't predict", "tokens": [51184, 293, 746, 300, 321, 994, 380, 6069, 51252], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 172, "seek": 40136, "start": 419.12, "end": 421.88, "text": " even five years ago, so this is all relevant.", "tokens": [51252, 754, 1732, 924, 2057, 11, 370, 341, 307, 439, 7340, 13, 51390], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 173, "seek": 40136, "start": 421.88, "end": 426.88, "text": " And if we move on, as we scale up the model,", "tokens": [51390, 400, 498, 321, 1286, 322, 11, 382, 321, 4373, 493, 264, 2316, 11, 51640], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 174, "seek": 40136, "start": 427.52000000000004, "end": 430.68, "text": " and we give it more compute, in 2012 AlexNet came out,", "tokens": [51672, 293, 321, 976, 309, 544, 14722, 11, 294, 9125, 5202, 31890, 1361, 484, 11, 51830], "temperature": 0.0, "avg_logprob": -0.12206207679596957, "compression_ratio": 1.75503355704698, "no_speech_prob": 7.029413245618343e-05}, {"id": 175, "seek": 43068, "start": 430.68, "end": 435.28000000000003, "text": " and we figured out we can run the model on GPUs,", "tokens": [50364, 293, 321, 8932, 484, 321, 393, 1190, 264, 2316, 322, 18407, 82, 11, 50594], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 176, "seek": 43068, "start": 435.28000000000003, "end": 436.92, "text": " so we can speed up the process,", "tokens": [50594, 370, 321, 393, 3073, 493, 264, 1399, 11, 50676], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 177, "seek": 43068, "start": 436.92, "end": 438.96, "text": " we can give the model lots of information", "tokens": [50676, 321, 393, 976, 264, 2316, 3195, 295, 1589, 50778], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 178, "seek": 43068, "start": 438.96, "end": 440.32, "text": " downloaded from the internet,", "tokens": [50778, 21748, 490, 264, 4705, 11, 50846], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 179, "seek": 43068, "start": 440.32, "end": 441.84000000000003, "text": " and it learns more and more and more,", "tokens": [50846, 293, 309, 27152, 544, 293, 544, 293, 544, 11, 50922], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 180, "seek": 43068, "start": 441.84000000000003, "end": 444.52, "text": " and the probabilities that it gives you", "tokens": [50922, 293, 264, 33783, 300, 309, 2709, 291, 51056], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 181, "seek": 43068, "start": 444.52, "end": 446.12, "text": " get better as it sees more examples", "tokens": [51056, 483, 1101, 382, 309, 8194, 544, 5110, 51136], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 182, "seek": 43068, "start": 446.12, "end": 447.6, "text": " of English on the internet,", "tokens": [51136, 295, 3669, 322, 264, 4705, 11, 51210], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 183, "seek": 43068, "start": 447.6, "end": 450.16, "text": " so we have to train the model to be really large,", "tokens": [51210, 370, 321, 362, 281, 3847, 264, 2316, 281, 312, 534, 2416, 11, 51338], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 184, "seek": 43068, "start": 450.16, "end": 453.0, "text": " really wide, and we have to train it for a really long time,", "tokens": [51338, 534, 4874, 11, 293, 321, 362, 281, 3847, 309, 337, 257, 534, 938, 565, 11, 51480], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 185, "seek": 43068, "start": 453.0, "end": 455.84000000000003, "text": " and as we do that, the model gets more and more better,", "tokens": [51480, 293, 382, 321, 360, 300, 11, 264, 2316, 2170, 544, 293, 544, 1101, 11, 51622], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 186, "seek": 43068, "start": 455.84000000000003, "end": 457.72, "text": " and expressive and capable,", "tokens": [51622, 293, 40189, 293, 8189, 11, 51716], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 187, "seek": 43068, "start": 457.72, "end": 459.76, "text": " and it also gets a little bit intelligent,", "tokens": [51716, 293, 309, 611, 2170, 257, 707, 857, 13232, 11, 51818], "temperature": 0.0, "avg_logprob": -0.09829294518248675, "compression_ratio": 1.9925093632958801, "no_speech_prob": 0.0003350161714479327}, {"id": 188, "seek": 45976, "start": 459.76, "end": 462.2, "text": " and for reasons we don't understand.", "tokens": [50364, 293, 337, 4112, 321, 500, 380, 1223, 13, 50486], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 189, "seek": 45976, "start": 462.2, "end": 465.88, "text": " So, but the issue is that because it learns", "tokens": [50486, 407, 11, 457, 264, 2734, 307, 300, 570, 309, 27152, 50670], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 190, "seek": 45976, "start": 465.88, "end": 467.59999999999997, "text": " to replicate the internet,", "tokens": [50670, 281, 25356, 264, 4705, 11, 50756], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 191, "seek": 45976, "start": 467.59999999999997, "end": 471.0, "text": " it knows how to speak in a lot of different genres of text,", "tokens": [50756, 309, 3255, 577, 281, 1710, 294, 257, 688, 295, 819, 30057, 295, 2487, 11, 50926], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 192, "seek": 45976, "start": 471.0, "end": 472.52, "text": " and a lot of different registers.", "tokens": [50926, 293, 257, 688, 295, 819, 38351, 13, 51002], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 193, "seek": 45976, "start": 472.52, "end": 475.12, "text": " If you begin the conversation like, chat GPT,", "tokens": [51002, 759, 291, 1841, 264, 3761, 411, 11, 5081, 26039, 51, 11, 51132], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 194, "seek": 45976, "start": 475.12, "end": 476.76, "text": " can you explain the moon landing to a six year old", "tokens": [51132, 393, 291, 2903, 264, 7135, 11202, 281, 257, 2309, 1064, 1331, 51214], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 195, "seek": 45976, "start": 476.76, "end": 478.76, "text": " in a few sentences, GPT three,", "tokens": [51214, 294, 257, 1326, 16579, 11, 26039, 51, 1045, 11, 51314], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 196, "seek": 45976, "start": 478.76, "end": 481.08, "text": " this is an example drawn from the Instruct GPT paper", "tokens": [51314, 341, 307, 364, 1365, 10117, 490, 264, 2730, 1757, 26039, 51, 3035, 51430], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 197, "seek": 45976, "start": 481.08, "end": 485.64, "text": " from OpenAI, GPT three would have just been like,", "tokens": [51430, 490, 7238, 48698, 11, 26039, 51, 1045, 576, 362, 445, 668, 411, 11, 51658], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 198, "seek": 45976, "start": 485.64, "end": 487.64, "text": " okay, so you're giving me an example,", "tokens": [51658, 1392, 11, 370, 291, 434, 2902, 385, 364, 1365, 11, 51758], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 199, "seek": 45976, "start": 487.64, "end": 489.64, "text": " like explain the moon landing to a six year old,", "tokens": [51758, 411, 2903, 264, 7135, 11202, 281, 257, 2309, 1064, 1331, 11, 51858], "temperature": 0.0, "avg_logprob": -0.15284426682660368, "compression_ratio": 1.7958477508650519, "no_speech_prob": 0.00027778238290920854}, {"id": 200, "seek": 48964, "start": 489.64, "end": 491.47999999999996, "text": " I'm gonna give you a whole bunch of similar things", "tokens": [50364, 286, 478, 799, 976, 291, 257, 1379, 3840, 295, 2531, 721, 50456], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 201, "seek": 48964, "start": 491.47999999999996, "end": 493.64, "text": " because those seem very likely to come in a sequence.", "tokens": [50456, 570, 729, 1643, 588, 3700, 281, 808, 294, 257, 8310, 13, 50564], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 202, "seek": 48964, "start": 493.64, "end": 495.52, "text": " It doesn't necessarily understand that it's being asked", "tokens": [50564, 467, 1177, 380, 4725, 1223, 300, 309, 311, 885, 2351, 50658], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 203, "seek": 48964, "start": 495.52, "end": 498.32, "text": " that question has to respond with an answer.", "tokens": [50658, 300, 1168, 575, 281, 4196, 365, 364, 1867, 13, 50798], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 204, "seek": 48964, "start": 498.32, "end": 501.56, "text": " GPT three did not have that apparatus,", "tokens": [50798, 26039, 51, 1045, 630, 406, 362, 300, 38573, 11, 50960], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 205, "seek": 48964, "start": 501.56, "end": 504.0, "text": " that interface for responding to questions,", "tokens": [50960, 300, 9226, 337, 16670, 281, 1651, 11, 51082], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 206, "seek": 48964, "start": 504.0, "end": 509.0, "text": " and the scientist at OpenAI came up with the solution,", "tokens": [51082, 293, 264, 12662, 412, 7238, 48698, 1361, 493, 365, 264, 3827, 11, 51332], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 207, "seek": 48964, "start": 509.28, "end": 511.64, "text": " and that's, let's give it a whole bunch of examples", "tokens": [51346, 293, 300, 311, 11, 718, 311, 976, 309, 257, 1379, 3840, 295, 5110, 51464], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 208, "seek": 48964, "start": 511.64, "end": 513.24, "text": " of question and answers,", "tokens": [51464, 295, 1168, 293, 6338, 11, 51544], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 209, "seek": 48964, "start": 513.24, "end": 515.64, "text": " such that we first train it on the internet,", "tokens": [51544, 1270, 300, 321, 700, 3847, 309, 322, 264, 4705, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 210, "seek": 48964, "start": 515.64, "end": 517.64, "text": " and then we train it with a whole bunch of questions", "tokens": [51664, 293, 550, 321, 3847, 309, 365, 257, 1379, 3840, 295, 1651, 51764], "temperature": 0.0, "avg_logprob": -0.10248013698693478, "compression_ratio": 1.8303886925795052, "no_speech_prob": 0.00014862741227261722}, {"id": 211, "seek": 51764, "start": 517.64, "end": 521.3199999999999, "text": " and answers such that it has the knowledge of the internet,", "tokens": [50364, 293, 6338, 1270, 300, 309, 575, 264, 3601, 295, 264, 4705, 11, 50548], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 212, "seek": 51764, "start": 521.3199999999999, "end": 524.1999999999999, "text": " but really knows that it has to be answering questions,", "tokens": [50548, 457, 534, 3255, 300, 309, 575, 281, 312, 13430, 1651, 11, 50692], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 213, "seek": 51764, "start": 524.1999999999999, "end": 527.64, "text": " and that is when chat GPT was born,", "tokens": [50692, 293, 300, 307, 562, 5081, 26039, 51, 390, 4232, 11, 50864], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 214, "seek": 51764, "start": 527.64, "end": 530.52, "text": " and that's when it gained 100 million users in one month,", "tokens": [50864, 293, 300, 311, 562, 309, 12634, 2319, 2459, 5022, 294, 472, 1618, 11, 51008], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 215, "seek": 51764, "start": 530.52, "end": 533.36, "text": " I think it beat TikTok's record at 20 million in one month,", "tokens": [51008, 286, 519, 309, 4224, 20211, 311, 2136, 412, 945, 2459, 294, 472, 1618, 11, 51150], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 216, "seek": 51764, "start": 533.36, "end": 536.56, "text": " it was a huge thing, and for a lot of people,", "tokens": [51150, 309, 390, 257, 2603, 551, 11, 293, 337, 257, 688, 295, 561, 11, 51310], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 217, "seek": 51764, "start": 536.56, "end": 538.72, "text": " they went, oh, this thing is intelligent,", "tokens": [51310, 436, 1437, 11, 1954, 11, 341, 551, 307, 13232, 11, 51418], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 218, "seek": 51764, "start": 538.72, "end": 541.4, "text": " I can answer, I can ask it questions, it answers back,", "tokens": [51418, 286, 393, 1867, 11, 286, 393, 1029, 309, 1651, 11, 309, 6338, 646, 11, 51552], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 219, "seek": 51764, "start": 541.4, "end": 543.6, "text": " we can work together to come to a solution,", "tokens": [51552, 321, 393, 589, 1214, 281, 808, 281, 257, 3827, 11, 51662], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 220, "seek": 51764, "start": 543.6, "end": 546.4, "text": " and that's because it's still predicting words,", "tokens": [51662, 293, 300, 311, 570, 309, 311, 920, 32884, 2283, 11, 51802], "temperature": 0.0, "avg_logprob": -0.11751317634856959, "compression_ratio": 1.8194945848375452, "no_speech_prob": 0.0009843636071309447}, {"id": 221, "seek": 54640, "start": 546.4, "end": 548.12, "text": " it's still a language model,", "tokens": [50364, 309, 311, 920, 257, 2856, 2316, 11, 50450], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 222, "seek": 54640, "start": 548.12, "end": 551.4399999999999, "text": " but it knows to predict words in the framework", "tokens": [50450, 457, 309, 3255, 281, 6069, 2283, 294, 264, 8388, 50616], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 223, "seek": 54640, "start": 551.4399999999999, "end": 554.56, "text": " of a question and answer, so that's what a prompt is,", "tokens": [50616, 295, 257, 1168, 293, 1867, 11, 370, 300, 311, 437, 257, 12391, 307, 11, 50772], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 224, "seek": 54640, "start": 554.56, "end": 557.56, "text": " that's what instruction tuning is, that's a key word,", "tokens": [50772, 300, 311, 437, 10951, 15164, 307, 11, 300, 311, 257, 2141, 1349, 11, 50922], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 225, "seek": 54640, "start": 557.56, "end": 561.92, "text": " that's what RLHF is, if you've ever seen that acronym,", "tokens": [50922, 300, 311, 437, 497, 43, 39, 37, 307, 11, 498, 291, 600, 1562, 1612, 300, 39195, 11, 51140], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 226, "seek": 54640, "start": 561.92, "end": 564.76, "text": " Reinforcement Alignment with Human Feedback,", "tokens": [51140, 42116, 9382, 967, 41134, 365, 10294, 33720, 3207, 11, 51282], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 227, "seek": 54640, "start": 564.76, "end": 567.84, "text": " and all those combined means that the models", "tokens": [51282, 293, 439, 729, 9354, 1355, 300, 264, 5245, 51436], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 228, "seek": 54640, "start": 567.84, "end": 568.92, "text": " that are coming out today,", "tokens": [51436, 300, 366, 1348, 484, 965, 11, 51490], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 229, "seek": 54640, "start": 568.92, "end": 570.6, "text": " the types of language predictors", "tokens": [51490, 264, 3467, 295, 2856, 6069, 830, 51574], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 230, "seek": 54640, "start": 570.6, "end": 571.84, "text": " that are coming out today,", "tokens": [51574, 300, 366, 1348, 484, 965, 11, 51636], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 231, "seek": 54640, "start": 571.84, "end": 574.0, "text": " work to operate in a Q and A form.", "tokens": [51636, 589, 281, 9651, 294, 257, 1249, 293, 316, 1254, 13, 51744], "temperature": 0.0, "avg_logprob": -0.12256368001302083, "compression_ratio": 1.7578125, "no_speech_prob": 0.00019705918384715915}, {"id": 232, "seek": 57400, "start": 574.0, "end": 578.4, "text": " GPT-4 exclusively only has the aligned model available,", "tokens": [50364, 26039, 51, 12, 19, 20638, 787, 575, 264, 17962, 2316, 2435, 11, 50584], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 233, "seek": 57400, "start": 578.4, "end": 582.92, "text": " and this is a really great, solid foundation to build on,", "tokens": [50584, 293, 341, 307, 257, 534, 869, 11, 5100, 7030, 281, 1322, 322, 11, 50810], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 234, "seek": 57400, "start": 582.92, "end": 584.36, "text": " because you can do all sorts of things,", "tokens": [50810, 570, 291, 393, 360, 439, 7527, 295, 721, 11, 50882], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 235, "seek": 57400, "start": 584.36, "end": 586.44, "text": " you can ask chat GPT, can you do this for me,", "tokens": [50882, 291, 393, 1029, 5081, 26039, 51, 11, 393, 291, 360, 341, 337, 385, 11, 50986], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 236, "seek": 57400, "start": 586.44, "end": 587.48, "text": " can you do that for me?", "tokens": [50986, 393, 291, 360, 300, 337, 385, 30, 51038], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 237, "seek": 57400, "start": 587.48, "end": 588.68, "text": " You might have seen that OpenAI", "tokens": [51038, 509, 1062, 362, 1612, 300, 7238, 48698, 51098], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 238, "seek": 57400, "start": 588.68, "end": 591.02, "text": " has allowed plugin access to chat GPT,", "tokens": [51098, 575, 4350, 23407, 2105, 281, 5081, 26039, 51, 11, 51215], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 239, "seek": 57400, "start": 591.02, "end": 593.8, "text": " so it can access Wolfram, it can search the web,", "tokens": [51215, 370, 309, 393, 2105, 16634, 2356, 11, 309, 393, 3164, 264, 3670, 11, 51354], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 240, "seek": 57400, "start": 593.8, "end": 596.28, "text": " it can search, it can do Instacart for you,", "tokens": [51354, 309, 393, 3164, 11, 309, 393, 360, 2730, 326, 446, 337, 291, 11, 51478], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 241, "seek": 57400, "start": 596.28, "end": 600.52, "text": " it can look up recipes, once the model knows", "tokens": [51478, 309, 393, 574, 493, 13035, 11, 1564, 264, 2316, 3255, 51690], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 242, "seek": 57400, "start": 600.52, "end": 602.64, "text": " that not only it has to predict language,", "tokens": [51690, 300, 406, 787, 309, 575, 281, 6069, 2856, 11, 51796], "temperature": 0.0, "avg_logprob": -0.1282005991254534, "compression_ratio": 1.72992700729927, "no_speech_prob": 0.0013876017183065414}, {"id": 243, "seek": 60264, "start": 602.64, "end": 605.0, "text": " but that it has to solve a problem,", "tokens": [50364, 457, 300, 309, 575, 281, 5039, 257, 1154, 11, 50482], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 244, "seek": 60264, "start": 605.92, "end": 607.64, "text": " and the problem here being,", "tokens": [50528, 293, 264, 1154, 510, 885, 11, 50614], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 245, "seek": 60264, "start": 607.64, "end": 609.56, "text": " give me a good answer to my question,", "tokens": [50614, 976, 385, 257, 665, 1867, 281, 452, 1168, 11, 50710], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 246, "seek": 60264, "start": 609.56, "end": 611.56, "text": " it's suddenly able to interface with the world", "tokens": [50710, 309, 311, 5800, 1075, 281, 9226, 365, 264, 1002, 50810], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 247, "seek": 60264, "start": 611.56, "end": 613.0, "text": " in a really solid way,", "tokens": [50810, 294, 257, 534, 5100, 636, 11, 50882], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 248, "seek": 60264, "start": 613.0, "end": 615.72, "text": " and from there on, there's been all sorts of tools", "tokens": [50882, 293, 490, 456, 322, 11, 456, 311, 668, 439, 7527, 295, 3873, 51018], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 249, "seek": 60264, "start": 615.72, "end": 619.6, "text": " that it build on this Q and A form that chat GPT uses,", "tokens": [51018, 300, 309, 1322, 322, 341, 1249, 293, 316, 1254, 300, 5081, 26039, 51, 4960, 11, 51212], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 250, "seek": 60264, "start": 619.6, "end": 622.8, "text": " you have auto GPT, you have Langchain,", "tokens": [51212, 291, 362, 8399, 26039, 51, 11, 291, 362, 13313, 11509, 11, 51372], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 251, "seek": 60264, "start": 622.8, "end": 626.84, "text": " you have React, there was a React paper", "tokens": [51372, 291, 362, 30644, 11, 456, 390, 257, 30644, 3035, 51574], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 252, "seek": 60264, "start": 626.84, "end": 628.14, "text": " where a lot of these come from,", "tokens": [51574, 689, 257, 688, 295, 613, 808, 490, 11, 51639], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 253, "seek": 60264, "start": 628.14, "end": 630.88, "text": " and turning the model into an agent", "tokens": [51639, 293, 6246, 264, 2316, 666, 364, 9461, 51776], "temperature": 0.0, "avg_logprob": -0.13110658264160155, "compression_ratio": 1.6825396825396826, "no_speech_prob": 0.000237659813137725}, {"id": 254, "seek": 63088, "start": 631.48, "end": 634.56, "text": " which to achieve any ambiguous goal", "tokens": [50394, 597, 281, 4584, 604, 39465, 3387, 50548], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 255, "seek": 63088, "start": 634.56, "end": 636.08, "text": " is where the future is going,", "tokens": [50548, 307, 689, 264, 2027, 307, 516, 11, 50624], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 256, "seek": 63088, "start": 636.08, "end": 638.36, "text": " and this is all thanks to instruction tuning,", "tokens": [50624, 293, 341, 307, 439, 3231, 281, 10951, 15164, 11, 50738], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 257, "seek": 63088, "start": 638.36, "end": 641.72, "text": " and with that, I think I will hand it off to Ted,", "tokens": [50738, 293, 365, 300, 11, 286, 519, 286, 486, 1011, 309, 766, 281, 14985, 11, 50906], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 258, "seek": 63088, "start": 641.72, "end": 643.32, "text": " who will be giving a demo,", "tokens": [50906, 567, 486, 312, 2902, 257, 10723, 11, 50986], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 259, "seek": 63088, "start": 643.32, "end": 644.56, "text": " or something along those lines,", "tokens": [50986, 420, 746, 2051, 729, 3876, 11, 51048], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 260, "seek": 63088, "start": 644.56, "end": 649.36, "text": " for how to use GPT as an agent, so.", "tokens": [51048, 337, 577, 281, 764, 26039, 51, 382, 364, 9461, 11, 370, 13, 51288], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 261, "seek": 63088, "start": 651.92, "end": 654.64, "text": " All right, so I'm a super applied guy,", "tokens": [51416, 1057, 558, 11, 370, 286, 478, 257, 1687, 6456, 2146, 11, 51552], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 262, "seek": 63088, "start": 654.64, "end": 656.8, "text": " I kinda look at things and think,", "tokens": [51552, 286, 4144, 574, 412, 721, 293, 519, 11, 51660], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 263, "seek": 63088, "start": 656.8, "end": 660.08, "text": " okay, how can I add this Lego, add that Lego,", "tokens": [51660, 1392, 11, 577, 393, 286, 909, 341, 28761, 11, 909, 300, 28761, 11, 51824], "temperature": 0.0, "avg_logprob": -0.1577773218569548, "compression_ratio": 1.5756302521008403, "no_speech_prob": 0.0002611346135381609}, {"id": 264, "seek": 66008, "start": 660.12, "end": 662.5200000000001, "text": " and clip them together and build something with it,", "tokens": [50366, 293, 7353, 552, 1214, 293, 1322, 746, 365, 309, 11, 50486], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 265, "seek": 66008, "start": 662.5200000000001, "end": 666.76, "text": " and right now, if you look back in computer science history,", "tokens": [50486, 293, 558, 586, 11, 498, 291, 574, 646, 294, 3820, 3497, 2503, 11, 50698], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 266, "seek": 66008, "start": 666.76, "end": 668.2, "text": " when you look at the kinds of things", "tokens": [50698, 562, 291, 574, 412, 264, 3685, 295, 721, 50770], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 267, "seek": 66008, "start": 668.2, "end": 670.24, "text": " that were being done in 1970,", "tokens": [50770, 300, 645, 885, 1096, 294, 14577, 11, 50872], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 268, "seek": 66008, "start": 670.24, "end": 671.98, "text": " right after computing was invented,", "tokens": [50872, 558, 934, 15866, 390, 14479, 11, 50959], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 269, "seek": 66008, "start": 671.98, "end": 674.08, "text": " the microprocessors were invented,", "tokens": [50959, 264, 3123, 1513, 340, 45700, 645, 14479, 11, 51064], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 270, "seek": 66008, "start": 674.08, "end": 675.24, "text": " people were doing research like,", "tokens": [51064, 561, 645, 884, 2132, 411, 11, 51122], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 271, "seek": 66008, "start": 675.24, "end": 677.12, "text": " how do I sort a list of numbers,", "tokens": [51122, 577, 360, 286, 1333, 257, 1329, 295, 3547, 11, 51216], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 272, "seek": 66008, "start": 677.12, "end": 678.36, "text": " and that was meaningful work,", "tokens": [51216, 293, 300, 390, 10995, 589, 11, 51278], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 273, "seek": 66008, "start": 678.36, "end": 680.76, "text": " and importantly, it was work that's accessible to everybody,", "tokens": [51278, 293, 8906, 11, 309, 390, 589, 300, 311, 9515, 281, 2201, 11, 51398], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 274, "seek": 66008, "start": 680.76, "end": 682.96, "text": " because nobody knows what we can build", "tokens": [51398, 570, 5079, 3255, 437, 321, 393, 1322, 51508], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 275, "seek": 66008, "start": 682.96, "end": 685.76, "text": " with this new kind of oil, this new kind of electricity,", "tokens": [51508, 365, 341, 777, 733, 295, 3184, 11, 341, 777, 733, 295, 10356, 11, 51648], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 276, "seek": 66008, "start": 685.76, "end": 688.84, "text": " this new kind of unit of computation we've created,", "tokens": [51648, 341, 777, 733, 295, 4985, 295, 24903, 321, 600, 2942, 11, 51802], "temperature": 0.0, "avg_logprob": -0.1056722535027398, "compression_ratio": 1.8137254901960784, "no_speech_prob": 0.0004440589400473982}, {"id": 277, "seek": 68884, "start": 688.88, "end": 690.1600000000001, "text": " and anything was game,", "tokens": [50366, 293, 1340, 390, 1216, 11, 50430], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 278, "seek": 68884, "start": 690.1600000000001, "end": 692.24, "text": " and anybody could participate in that game", "tokens": [50430, 293, 4472, 727, 8197, 294, 300, 1216, 50534], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 279, "seek": 68884, "start": 692.24, "end": 693.08, "text": " to figure it out,", "tokens": [50534, 281, 2573, 309, 484, 11, 50576], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 280, "seek": 68884, "start": 693.08, "end": 694.44, "text": " and I think one of the really exciting things", "tokens": [50576, 293, 286, 519, 472, 295, 264, 534, 4670, 721, 50644], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 281, "seek": 68884, "start": 694.44, "end": 697.24, "text": " about GPT right now is, yes,", "tokens": [50644, 466, 26039, 51, 558, 586, 307, 11, 2086, 11, 50784], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 282, "seek": 68884, "start": 697.24, "end": 698.9200000000001, "text": " in and of itself, it's amazing,", "tokens": [50784, 294, 293, 295, 2564, 11, 309, 311, 2243, 11, 50868], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 283, "seek": 68884, "start": 698.9200000000001, "end": 701.52, "text": " but then, what could we do with it", "tokens": [50868, 457, 550, 11, 437, 727, 321, 360, 365, 309, 50998], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 284, "seek": 68884, "start": 701.52, "end": 703.1600000000001, "text": " if we call it over and over again,", "tokens": [50998, 498, 321, 818, 309, 670, 293, 670, 797, 11, 51080], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 285, "seek": 68884, "start": 703.1600000000001, "end": 704.9, "text": " if we build it into our algorithms,", "tokens": [51080, 498, 321, 1322, 309, 666, 527, 14642, 11, 51167], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 286, "seek": 68884, "start": 704.9, "end": 706.52, "text": " and start to build it into broader software,", "tokens": [51167, 293, 722, 281, 1322, 309, 666, 13227, 4722, 11, 51248], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 287, "seek": 68884, "start": 706.52, "end": 707.96, "text": " so the world really is yours", "tokens": [51248, 370, 264, 1002, 534, 307, 6342, 51320], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 288, "seek": 68884, "start": 707.96, "end": 710.08, "text": " to figure out those fundamental questions", "tokens": [51320, 281, 2573, 484, 729, 8088, 1651, 51426], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 289, "seek": 68884, "start": 710.08, "end": 712.6, "text": " about what could you do if you could script", "tokens": [51426, 466, 437, 727, 291, 360, 498, 291, 727, 5755, 51552], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 290, "seek": 68884, "start": 712.6, "end": 715.2800000000001, "text": " computation itself over and over again", "tokens": [51552, 24903, 2564, 670, 293, 670, 797, 51686], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 291, "seek": 68884, "start": 715.2800000000001, "end": 716.94, "text": " in the way that computers can do,", "tokens": [51686, 294, 264, 636, 300, 10807, 393, 360, 11, 51769], "temperature": 0.0, "avg_logprob": -0.09966761445345944, "compression_ratio": 1.843205574912892, "no_speech_prob": 0.0002098393306368962}, {"id": 292, "seek": 71694, "start": 716.94, "end": 719.5400000000001, "text": " not just talk with it, but build things atop it,", "tokens": [50364, 406, 445, 751, 365, 309, 11, 457, 1322, 721, 412, 404, 309, 11, 50494], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 293, "seek": 71694, "start": 719.5400000000001, "end": 722.1, "text": " so we're a hosting company, we host apps,", "tokens": [50494, 370, 321, 434, 257, 16058, 2237, 11, 321, 3975, 7733, 11, 50622], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 294, "seek": 71694, "start": 722.1, "end": 724.5200000000001, "text": " and these are just some of the things that we see,", "tokens": [50622, 293, 613, 366, 445, 512, 295, 264, 721, 300, 321, 536, 11, 50743], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 295, "seek": 71694, "start": 724.5200000000001, "end": 726.2600000000001, "text": " I'm gonna show you demos of this with code", "tokens": [50743, 286, 478, 799, 855, 291, 33788, 295, 341, 365, 3089, 50830], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 296, "seek": 71694, "start": 726.2600000000001, "end": 728.74, "text": " and try to explain some of the thought process,", "tokens": [50830, 293, 853, 281, 2903, 512, 295, 264, 1194, 1399, 11, 50954], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 297, "seek": 71694, "start": 728.74, "end": 730.74, "text": " but I wanted to give you a high level overview of,", "tokens": [50954, 457, 286, 1415, 281, 976, 291, 257, 1090, 1496, 12492, 295, 11, 51054], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 298, "seek": 71694, "start": 730.74, "end": 732.74, "text": " you've probably seen these on Twitter,", "tokens": [51054, 291, 600, 1391, 1612, 613, 322, 5794, 11, 51154], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 299, "seek": 71694, "start": 732.74, "end": 734.6600000000001, "text": " but when it all sorts out to the top,", "tokens": [51154, 457, 562, 309, 439, 7527, 484, 281, 264, 1192, 11, 51250], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 300, "seek": 71694, "start": 734.6600000000001, "end": 736.2600000000001, "text": " these are some of the things that we're seeing", "tokens": [51250, 613, 366, 512, 295, 264, 721, 300, 321, 434, 2577, 51330], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 301, "seek": 71694, "start": 736.2600000000001, "end": 739.5400000000001, "text": " built and deployed with language models today,", "tokens": [51330, 3094, 293, 17826, 365, 2856, 5245, 965, 11, 51494], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 302, "seek": 71694, "start": 739.5400000000001, "end": 742.5, "text": " companionship, that's everything from I need a friend,", "tokens": [51494, 28009, 1210, 11, 300, 311, 1203, 490, 286, 643, 257, 1277, 11, 51642], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 303, "seek": 71694, "start": 742.5, "end": 743.82, "text": " to I need a friend with a purpose,", "tokens": [51642, 281, 286, 643, 257, 1277, 365, 257, 4334, 11, 51708], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 304, "seek": 71694, "start": 743.82, "end": 745.94, "text": " I want a coach, I want somebody to tell me,", "tokens": [51708, 286, 528, 257, 6560, 11, 286, 528, 2618, 281, 980, 385, 11, 51814], "temperature": 0.0, "avg_logprob": -0.1310656923236269, "compression_ratio": 1.8906752411575563, "no_speech_prob": 0.0014547327300533652}, {"id": 305, "seek": 74594, "start": 745.94, "end": 747.3800000000001, "text": " go to the gym and do these exercises,", "tokens": [50364, 352, 281, 264, 9222, 293, 360, 613, 11900, 11, 50436], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 306, "seek": 74594, "start": 747.3800000000001, "end": 749.62, "text": " I want somebody to help me study a foreign language,", "tokens": [50436, 286, 528, 2618, 281, 854, 385, 2979, 257, 5329, 2856, 11, 50548], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 307, "seek": 74594, "start": 749.62, "end": 751.4000000000001, "text": " question answering, this is a big one,", "tokens": [50548, 1168, 13430, 11, 341, 307, 257, 955, 472, 11, 50637], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 308, "seek": 74594, "start": 751.4000000000001, "end": 753.1, "text": " this is everything from your newsroom,", "tokens": [50637, 341, 307, 1203, 490, 428, 2583, 2861, 11, 50722], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 309, "seek": 74594, "start": 753.1, "end": 755.5400000000001, "text": " having a slack bot that helps assist you,", "tokens": [50722, 1419, 257, 29767, 10592, 300, 3665, 4255, 291, 11, 50844], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 310, "seek": 74594, "start": 755.5400000000001, "end": 759.2600000000001, "text": " does this article conform to the style guidelines", "tokens": [50844, 775, 341, 7222, 18975, 281, 264, 3758, 12470, 51030], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 311, "seek": 74594, "start": 759.2600000000001, "end": 761.5, "text": " of our newsroom, all the way through to,", "tokens": [51030, 295, 527, 2583, 2861, 11, 439, 264, 636, 807, 281, 11, 51142], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 312, "seek": 74594, "start": 761.5, "end": 762.74, "text": " and you need help on my homework,", "tokens": [51142, 293, 291, 643, 854, 322, 452, 14578, 11, 51204], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 313, "seek": 74594, "start": 762.74, "end": 764.5400000000001, "text": " or hey, I have some questions that I want you to ask,", "tokens": [51204, 420, 4177, 11, 286, 362, 512, 1651, 300, 286, 528, 291, 281, 1029, 11, 51294], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 314, "seek": 74594, "start": 764.5400000000001, "end": 766.86, "text": " Wikipedia, combine it with something else,", "tokens": [51294, 28999, 11, 10432, 309, 365, 746, 1646, 11, 51410], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 315, "seek": 74594, "start": 766.86, "end": 768.98, "text": " synthesize the answer and give it to me.", "tokens": [51410, 26617, 1125, 264, 1867, 293, 976, 309, 281, 385, 13, 51516], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 316, "seek": 74594, "start": 768.98, "end": 771.94, "text": " Utility functions, I would describe this as,", "tokens": [51516, 12555, 1140, 6828, 11, 286, 576, 6786, 341, 382, 11, 51664], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 317, "seek": 74594, "start": 771.94, "end": 774.86, "text": " there's a large set of things for which", "tokens": [51664, 456, 311, 257, 2416, 992, 295, 721, 337, 597, 51810], "temperature": 0.0, "avg_logprob": -0.10916574377762644, "compression_ratio": 1.74375, "no_speech_prob": 0.0008556739776395261}, {"id": 318, "seek": 77486, "start": 774.9, "end": 777.58, "text": " human beings can do them, if only,", "tokens": [50366, 1952, 8958, 393, 360, 552, 11, 498, 787, 11, 50500], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 319, "seek": 77486, "start": 777.58, "end": 780.0600000000001, "text": " or computers could do them, if only they had access", "tokens": [50500, 420, 10807, 727, 360, 552, 11, 498, 787, 436, 632, 2105, 50624], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 320, "seek": 77486, "start": 780.0600000000001, "end": 781.82, "text": " to language computation, language knowledge,", "tokens": [50624, 281, 2856, 24903, 11, 2856, 3601, 11, 50712], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 321, "seek": 77486, "start": 781.82, "end": 783.34, "text": " an example of this would be,", "tokens": [50712, 364, 1365, 295, 341, 576, 312, 11, 50788], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 322, "seek": 77486, "start": 783.34, "end": 786.46, "text": " read every tweet on Twitter, tell me the ones I should read,", "tokens": [50788, 1401, 633, 15258, 322, 5794, 11, 980, 385, 264, 2306, 286, 820, 1401, 11, 50944], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 323, "seek": 77486, "start": 786.46, "end": 787.94, "text": " that way I only get to read the ones", "tokens": [50944, 300, 636, 286, 787, 483, 281, 1401, 264, 2306, 51018], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 324, "seek": 77486, "start": 787.94, "end": 789.14, "text": " that actually make sense to me", "tokens": [51018, 300, 767, 652, 2020, 281, 385, 51078], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 325, "seek": 77486, "start": 789.14, "end": 790.74, "text": " and I don't have to skim through the rest,", "tokens": [51078, 293, 286, 500, 380, 362, 281, 1110, 332, 807, 264, 1472, 11, 51158], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 326, "seek": 77486, "start": 790.74, "end": 793.46, "text": " creativity, image generation, text generation,", "tokens": [51158, 12915, 11, 3256, 5125, 11, 2487, 5125, 11, 51294], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 327, "seek": 77486, "start": 793.46, "end": 795.98, "text": " storytelling, proposing other ways to do things,", "tokens": [51294, 21479, 11, 29939, 661, 2098, 281, 360, 721, 11, 51420], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 328, "seek": 77486, "start": 795.98, "end": 799.74, "text": " and then these wild experiments and kind of baby AGI,", "tokens": [51420, 293, 550, 613, 4868, 12050, 293, 733, 295, 3186, 316, 26252, 11, 51608], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 329, "seek": 77486, "start": 799.74, "end": 801.38, "text": " as people are calling them,", "tokens": [51608, 382, 561, 366, 5141, 552, 11, 51690], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 330, "seek": 77486, "start": 801.38, "end": 803.5, "text": " in which the AI itself decides what to do", "tokens": [51690, 294, 597, 264, 7318, 2564, 14898, 437, 281, 360, 51796], "temperature": 0.0, "avg_logprob": -0.10993819982827115, "compression_ratio": 1.7635782747603834, "no_speech_prob": 0.0010646014707162976}, {"id": 331, "seek": 80350, "start": 803.5, "end": 805.3, "text": " and is self-directed, so I'll show you examples", "tokens": [50364, 293, 307, 2698, 12, 44868, 292, 11, 370, 286, 603, 855, 291, 5110, 50454], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 332, "seek": 80350, "start": 805.3, "end": 807.18, "text": " of many of these and what the code looks like,", "tokens": [50454, 295, 867, 295, 613, 293, 437, 264, 3089, 1542, 411, 11, 50548], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 333, "seek": 80350, "start": 807.18, "end": 809.74, "text": " and if I were you, I would think about these", "tokens": [50548, 293, 498, 286, 645, 291, 11, 286, 576, 519, 466, 613, 50676], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 334, "seek": 80350, "start": 809.74, "end": 812.82, "text": " as categories within which to both think about", "tokens": [50676, 382, 10479, 1951, 597, 281, 1293, 519, 466, 50830], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 335, "seek": 80350, "start": 812.82, "end": 817.14, "text": " what you might build and then also seek out starter projects", "tokens": [50830, 437, 291, 1062, 1322, 293, 550, 611, 8075, 484, 22465, 4455, 51046], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 336, "seek": 80350, "start": 817.14, "end": 819.66, "text": " for how you might go about building them online.", "tokens": [51046, 337, 577, 291, 1062, 352, 466, 2390, 552, 2950, 13, 51172], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 337, "seek": 80350, "start": 822.5, "end": 823.82, "text": " All right, so I'm just gonna dive straight into", "tokens": [51314, 1057, 558, 11, 370, 286, 478, 445, 799, 9192, 2997, 666, 51380], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 338, "seek": 80350, "start": 823.82, "end": 825.5, "text": " demos and code for some of these,", "tokens": [51380, 33788, 293, 3089, 337, 512, 295, 613, 11, 51464], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 339, "seek": 80350, "start": 825.5, "end": 827.18, "text": " because I know that's what's interesting to see", "tokens": [51464, 570, 286, 458, 300, 311, 437, 311, 1880, 281, 536, 51548], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 340, "seek": 80350, "start": 827.18, "end": 829.86, "text": " as fellow builders, with a high level diagram", "tokens": [51548, 382, 7177, 36281, 11, 365, 257, 1090, 1496, 10686, 51682], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 341, "seek": 80350, "start": 829.86, "end": 831.58, "text": " for some of these as to how it works.", "tokens": [51682, 337, 512, 295, 613, 382, 281, 577, 309, 1985, 13, 51768], "temperature": 0.0, "avg_logprob": -0.10802723379696116, "compression_ratio": 1.7770034843205575, "no_speech_prob": 0.0007552801980637014}, {"id": 342, "seek": 83158, "start": 831.58, "end": 834.7800000000001, "text": " So approximately, you can think of a companionship bot", "tokens": [50364, 407, 10447, 11, 291, 393, 519, 295, 257, 28009, 1210, 10592, 50524], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 343, "seek": 83158, "start": 834.7800000000001, "end": 837.62, "text": " as a friend that has a purpose to you,", "tokens": [50524, 382, 257, 1277, 300, 575, 257, 4334, 281, 291, 11, 50666], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 344, "seek": 83158, "start": 837.62, "end": 840.1, "text": " and there are many ways to build all of these things,", "tokens": [50666, 293, 456, 366, 867, 2098, 281, 1322, 439, 295, 613, 721, 11, 50790], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 345, "seek": 83158, "start": 840.1, "end": 841.7800000000001, "text": " but one of the ways you can build this", "tokens": [50790, 457, 472, 295, 264, 2098, 291, 393, 1322, 341, 50874], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 346, "seek": 83158, "start": 841.7800000000001, "end": 844.6600000000001, "text": " is simply to wrap GPT or a language model", "tokens": [50874, 307, 2935, 281, 7019, 26039, 51, 420, 257, 2856, 2316, 51018], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 347, "seek": 83158, "start": 844.6600000000001, "end": 847.86, "text": " in an endpoint that additionally injects into the prompt", "tokens": [51018, 294, 364, 35795, 300, 43181, 10711, 82, 666, 264, 12391, 51178], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 348, "seek": 83158, "start": 847.86, "end": 850.7, "text": " some particular perspective or some particular goal", "tokens": [51178, 512, 1729, 4585, 420, 512, 1729, 3387, 51320], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 349, "seek": 83158, "start": 850.7, "end": 851.98, "text": " that you want to use.", "tokens": [51320, 300, 291, 528, 281, 764, 13, 51384], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 350, "seek": 83158, "start": 851.98, "end": 853.82, "text": " It really is that easy in a way,", "tokens": [51384, 467, 534, 307, 300, 1858, 294, 257, 636, 11, 51476], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 351, "seek": 83158, "start": 853.82, "end": 856.1400000000001, "text": " but it's also very hard because you need to iterate", "tokens": [51476, 457, 309, 311, 611, 588, 1152, 570, 291, 643, 281, 44497, 51592], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 352, "seek": 83158, "start": 856.1400000000001, "end": 859.82, "text": " and engineer the prompt so that it consistently performs", "tokens": [51592, 293, 11403, 264, 12391, 370, 300, 309, 14961, 26213, 51776], "temperature": 0.0, "avg_logprob": -0.08101855963468552, "compression_ratio": 1.7395833333333333, "no_speech_prob": 0.00010887604003073648}, {"id": 353, "seek": 85982, "start": 859.82, "end": 862.1400000000001, "text": " the way you want it to perform.", "tokens": [50364, 264, 636, 291, 528, 309, 281, 2042, 13, 50480], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 354, "seek": 85982, "start": 862.1400000000001, "end": 864.7, "text": " So a good example of this is something somebody built", "tokens": [50480, 407, 257, 665, 1365, 295, 341, 307, 746, 2618, 3094, 50608], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 355, "seek": 85982, "start": 864.7, "end": 865.62, "text": " in the hackathon yesterday,", "tokens": [50608, 294, 264, 10339, 18660, 5186, 11, 50654], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 356, "seek": 85982, "start": 865.62, "end": 867.94, "text": " and I just wanted to show you the project that they built.", "tokens": [50654, 293, 286, 445, 1415, 281, 855, 291, 264, 1716, 300, 436, 3094, 13, 50770], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 357, "seek": 85982, "start": 867.94, "end": 869.5400000000001, "text": " It was a Mandarin idiom coach,", "tokens": [50770, 467, 390, 257, 42292, 18014, 298, 6560, 11, 50850], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 358, "seek": 85982, "start": 869.5400000000001, "end": 871.82, "text": " and I'll show you what the code looked like first.", "tokens": [50850, 293, 286, 603, 855, 291, 437, 264, 3089, 2956, 411, 700, 13, 50964], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 359, "seek": 85982, "start": 871.82, "end": 873.86, "text": " I'll show you the demo first.", "tokens": [50964, 286, 603, 855, 291, 264, 10723, 700, 13, 51066], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 360, "seek": 85982, "start": 873.86, "end": 875.3000000000001, "text": " I think I already pulled it up.", "tokens": [51066, 286, 519, 286, 1217, 7373, 309, 493, 13, 51138], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 361, "seek": 85982, "start": 878.1400000000001, "end": 879.3000000000001, "text": " Here we go.", "tokens": [51280, 1692, 321, 352, 13, 51338], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 362, "seek": 85982, "start": 879.3000000000001, "end": 882.82, "text": " So the buddy that this person wanted to create", "tokens": [51338, 407, 264, 10340, 300, 341, 954, 1415, 281, 1884, 51514], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 363, "seek": 85982, "start": 882.82, "end": 887.22, "text": " was a friend that if you gave it a particular problem", "tokens": [51514, 390, 257, 1277, 300, 498, 291, 2729, 309, 257, 1729, 1154, 51734], "temperature": 0.0, "avg_logprob": -0.12027537921243463, "compression_ratio": 1.6956521739130435, "no_speech_prob": 0.0007792568067088723}, {"id": 364, "seek": 88722, "start": 887.22, "end": 889.9, "text": " you were having, it would pick a Chinese idiom,", "tokens": [50364, 291, 645, 1419, 11, 309, 576, 1888, 257, 4649, 18014, 298, 11, 50498], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 365, "seek": 88722, "start": 889.9, "end": 893.1, "text": " a four character Cheng Yu, that described poetically,", "tokens": [50498, 257, 1451, 2517, 24363, 10767, 11, 300, 7619, 20874, 984, 11, 50658], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 366, "seek": 88722, "start": 893.1, "end": 896.1, "text": " like here's a particular way you could say this,", "tokens": [50658, 411, 510, 311, 257, 1729, 636, 291, 727, 584, 341, 11, 50808], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 367, "seek": 88722, "start": 896.1, "end": 897.78, "text": " and it would tell it to her so that the person", "tokens": [50808, 293, 309, 576, 980, 309, 281, 720, 370, 300, 264, 954, 50892], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 368, "seek": 88722, "start": 897.78, "end": 899.7, "text": " who built this was studying Chinese", "tokens": [50892, 567, 3094, 341, 390, 7601, 4649, 50988], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 369, "seek": 88722, "start": 899.7, "end": 901.98, "text": " and she wanted to learn more about it.", "tokens": [50988, 293, 750, 1415, 281, 1466, 544, 466, 309, 13, 51102], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 370, "seek": 88722, "start": 901.98, "end": 906.82, "text": " So I might say something like I'm feeling very sad,", "tokens": [51102, 407, 286, 1062, 584, 746, 411, 286, 478, 2633, 588, 4227, 11, 51344], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 371, "seek": 88722, "start": 908.14, "end": 910.4200000000001, "text": " and it would think a little bit,", "tokens": [51410, 293, 309, 576, 519, 257, 707, 857, 11, 51524], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 372, "seek": 88722, "start": 910.4200000000001, "end": 912.78, "text": " and if everything's up and running,", "tokens": [51524, 293, 498, 1203, 311, 493, 293, 2614, 11, 51642], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 373, "seek": 88722, "start": 912.78, "end": 916.26, "text": " it will generate one of these four character phrases,", "tokens": [51642, 309, 486, 8460, 472, 295, 613, 1451, 2517, 20312, 11, 51816], "temperature": 0.0, "avg_logprob": -0.14904375076293946, "compression_ratio": 1.7126436781609196, "no_speech_prob": 0.012891520746052265}, {"id": 374, "seek": 91626, "start": 916.26, "end": 919.7, "text": " and it will respond to it with an example.", "tokens": [50364, 293, 309, 486, 4196, 281, 309, 365, 364, 1365, 13, 50536], "temperature": 0.0, "avg_logprob": -0.1593676764389564, "compression_ratio": 1.7088607594936709, "no_speech_prob": 0.0009379865950904787}, {"id": 375, "seek": 91626, "start": 919.7, "end": 921.02, "text": " Now, I don't know if this is correct or not,", "tokens": [50536, 823, 11, 286, 500, 380, 458, 498, 341, 307, 3006, 420, 406, 11, 50602], "temperature": 0.0, "avg_logprob": -0.1593676764389564, "compression_ratio": 1.7088607594936709, "no_speech_prob": 0.0009379865950904787}, {"id": 376, "seek": 91626, "start": 921.02, "end": 922.38, "text": " so if somebody can call me out,", "tokens": [50602, 370, 498, 2618, 393, 818, 385, 484, 11, 50670], "temperature": 0.0, "avg_logprob": -0.1593676764389564, "compression_ratio": 1.7088607594936709, "no_speech_prob": 0.0009379865950904787}, {"id": 377, "seek": 91626, "start": 922.38, "end": 925.22, "text": " if this is actually incorrect, please call me out,", "tokens": [50670, 498, 341, 307, 767, 18424, 11, 1767, 818, 385, 484, 11, 50812], "temperature": 0.0, "avg_logprob": -0.1593676764389564, "compression_ratio": 1.7088607594936709, "no_speech_prob": 0.0009379865950904787}, {"id": 378, "seek": 91626, "start": 926.22, "end": 928.42, "text": " and it will then finish up with something encouraging,", "tokens": [50862, 293, 309, 486, 550, 2413, 493, 365, 746, 14580, 11, 50972], "temperature": 0.0, "avg_logprob": -0.1593676764389564, "compression_ratio": 1.7088607594936709, "no_speech_prob": 0.0009379865950904787}, {"id": 379, "seek": 91626, "start": 928.42, "end": 930.98, "text": " saying hey, you can do it, I know this is hard, keep going.", "tokens": [50972, 1566, 4177, 11, 291, 393, 360, 309, 11, 286, 458, 341, 307, 1152, 11, 1066, 516, 13, 51100], "temperature": 0.0, "avg_logprob": -0.1593676764389564, "compression_ratio": 1.7088607594936709, "no_speech_prob": 0.0009379865950904787}, {"id": 380, "seek": 91626, "start": 930.98, "end": 932.34, "text": " So let me show you how they built this,", "tokens": [51100, 407, 718, 385, 855, 291, 577, 436, 3094, 341, 11, 51168], "temperature": 0.0, "avg_logprob": -0.1593676764389564, "compression_ratio": 1.7088607594936709, "no_speech_prob": 0.0009379865950904787}, {"id": 381, "seek": 91626, "start": 932.34, "end": 937.34, "text": " and I pulled up the code right here.", "tokens": [51168, 293, 286, 7373, 493, 264, 3089, 558, 510, 13, 51418], "temperature": 0.0, "avg_logprob": -0.1593676764389564, "compression_ratio": 1.7088607594936709, "no_speech_prob": 0.0009379865950904787}, {"id": 382, "seek": 91626, "start": 940.54, "end": 945.06, "text": " So this was the particular starter replete", "tokens": [51578, 407, 341, 390, 264, 1729, 22465, 3248, 3498, 51804], "temperature": 0.0, "avg_logprob": -0.1593676764389564, "compression_ratio": 1.7088607594936709, "no_speech_prob": 0.0009379865950904787}, {"id": 383, "seek": 94506, "start": 945.06, "end": 947.42, "text": " that folks were using in the hackathon yesterday,", "tokens": [50364, 300, 4024, 645, 1228, 294, 264, 10339, 18660, 5186, 11, 50482], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 384, "seek": 94506, "start": 947.42, "end": 949.9399999999999, "text": " and we pulled things up into basically,", "tokens": [50482, 293, 321, 7373, 721, 493, 666, 1936, 11, 50608], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 385, "seek": 94506, "start": 949.9399999999999, "end": 952.6999999999999, "text": " you have a wrapper around GPT,", "tokens": [50608, 291, 362, 257, 46906, 926, 26039, 51, 11, 50746], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 386, "seek": 94506, "start": 952.6999999999999, "end": 954.38, "text": " and there's many things you could do,", "tokens": [50746, 293, 456, 311, 867, 721, 291, 727, 360, 11, 50830], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 387, "seek": 94506, "start": 954.38, "end": 956.3, "text": " but we're gonna make it easy for you to do two things.", "tokens": [50830, 457, 321, 434, 799, 652, 309, 1858, 337, 291, 281, 360, 732, 721, 13, 50926], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 388, "seek": 94506, "start": 956.3, "end": 960.06, "text": " One of them is to inject some personality into the prompt,", "tokens": [50926, 1485, 295, 552, 307, 281, 10711, 512, 9033, 666, 264, 12391, 11, 51114], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 389, "seek": 94506, "start": 960.06, "end": 962.66, "text": " and I'll explain what that prompt is in a second,", "tokens": [51114, 293, 286, 603, 2903, 437, 300, 12391, 307, 294, 257, 1150, 11, 51244], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 390, "seek": 94506, "start": 962.66, "end": 964.14, "text": " and then the second is add tools", "tokens": [51244, 293, 550, 264, 1150, 307, 909, 3873, 51318], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 391, "seek": 94506, "start": 964.14, "end": 965.5799999999999, "text": " that might go out and do a particular thing,", "tokens": [51318, 300, 1062, 352, 484, 293, 360, 257, 1729, 551, 11, 51390], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 392, "seek": 94506, "start": 965.5799999999999, "end": 968.26, "text": " search the web or generate an image", "tokens": [51390, 3164, 264, 3670, 420, 8460, 364, 3256, 51524], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 393, "seek": 94506, "start": 968.26, "end": 969.6999999999999, "text": " or add something to a database", "tokens": [51524, 420, 909, 746, 281, 257, 8149, 51596], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 394, "seek": 94506, "start": 969.6999999999999, "end": 971.78, "text": " or fetch something from a database.", "tokens": [51596, 420, 23673, 746, 490, 257, 8149, 13, 51700], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 395, "seek": 94506, "start": 971.78, "end": 973.0999999999999, "text": " So having done that,", "tokens": [51700, 407, 1419, 1096, 300, 11, 51766], "temperature": 0.0, "avg_logprob": -0.08452721575757007, "compression_ratio": 1.7293729372937294, "no_speech_prob": 0.0007307037594728172}, {"id": 396, "seek": 97310, "start": 973.1, "end": 975.34, "text": " now you have something more than GPT.", "tokens": [50364, 586, 291, 362, 746, 544, 813, 26039, 51, 13, 50476], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 397, "seek": 97310, "start": 975.34, "end": 977.78, "text": " Now you have GPT, which we all know what it is", "tokens": [50476, 823, 291, 362, 26039, 51, 11, 597, 321, 439, 458, 437, 309, 307, 50598], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 398, "seek": 97310, "start": 977.78, "end": 979.1, "text": " and how we can interact with it,", "tokens": [50598, 293, 577, 321, 393, 4648, 365, 309, 11, 50664], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 399, "seek": 97310, "start": 979.1, "end": 981.7, "text": " but you've also added a particular lens", "tokens": [50664, 457, 291, 600, 611, 3869, 257, 1729, 6765, 50794], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 400, "seek": 97310, "start": 981.7, "end": 982.7, "text": " through which it's talking to you", "tokens": [50794, 807, 597, 309, 311, 1417, 281, 291, 50844], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 401, "seek": 97310, "start": 982.7, "end": 983.66, "text": " and potentially some tools.", "tokens": [50844, 293, 7263, 512, 3873, 13, 50892], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 402, "seek": 97310, "start": 983.66, "end": 987.14, "text": " So this particular Chinese tutor,", "tokens": [50892, 407, 341, 1729, 4649, 35613, 11, 51066], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 403, "seek": 97310, "start": 987.14, "end": 990.32, "text": " all it took to build that was four lines.", "tokens": [51066, 439, 309, 1890, 281, 1322, 300, 390, 1451, 3876, 13, 51225], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 404, "seek": 97310, "start": 990.32, "end": 991.66, "text": " So here's a question that I think", "tokens": [51225, 407, 510, 311, 257, 1168, 300, 286, 519, 51292], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 405, "seek": 97310, "start": 991.66, "end": 994.7, "text": " is frying the minds of everybody in the industry right now.", "tokens": [51292, 307, 24596, 264, 9634, 295, 2201, 294, 264, 3518, 558, 586, 13, 51444], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 406, "seek": 97310, "start": 995.7, "end": 998.3000000000001, "text": " So is this something that we'll all do casually", "tokens": [51494, 407, 307, 341, 746, 300, 321, 603, 439, 360, 34872, 51624], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 407, "seek": 97310, "start": 998.3000000000001, "end": 999.3000000000001, "text": " and nobody really knows?", "tokens": [51624, 293, 5079, 534, 3255, 30, 51674], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 408, "seek": 97310, "start": 999.3000000000001, "end": 1001.26, "text": " Will we just all say in the future to the LLM,", "tokens": [51674, 3099, 321, 445, 439, 584, 294, 264, 2027, 281, 264, 441, 43, 44, 11, 51772], "temperature": 0.0, "avg_logprob": -0.10394612559071788, "compression_ratio": 1.6798679867986799, "no_speech_prob": 0.0005031974869780242}, {"id": 409, "seek": 100126, "start": 1001.26, "end": 1002.5, "text": " hey, for the next five minutes,", "tokens": [50364, 4177, 11, 337, 264, 958, 1732, 2077, 11, 50426], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 410, "seek": 100126, "start": 1002.5, "end": 1005.14, "text": " please talk like a teacher, and maybe?", "tokens": [50426, 1767, 751, 411, 257, 5027, 11, 293, 1310, 30, 50558], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 411, "seek": 100126, "start": 1005.14, "end": 1007.18, "text": " But also, definitely in the meantime,", "tokens": [50558, 583, 611, 11, 2138, 294, 264, 14991, 11, 50660], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 412, "seek": 100126, "start": 1007.18, "end": 1008.14, "text": " and maybe in the future,", "tokens": [50660, 293, 1310, 294, 264, 2027, 11, 50708], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 413, "seek": 100126, "start": 1008.14, "end": 1011.1, "text": " it makes sense to wrap up these personalized endpoints", "tokens": [50708, 309, 1669, 2020, 281, 7019, 493, 613, 28415, 917, 20552, 50856], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 414, "seek": 100126, "start": 1011.1, "end": 1012.5, "text": " so that when I'm talking to GPT,", "tokens": [50856, 370, 300, 562, 286, 478, 1417, 281, 26039, 51, 11, 50926], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 415, "seek": 100126, "start": 1012.5, "end": 1013.9, "text": " I'm not just talking to GPT,", "tokens": [50926, 286, 478, 406, 445, 1417, 281, 26039, 51, 11, 50996], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 416, "seek": 100126, "start": 1013.9, "end": 1015.7, "text": " I have a whole army of different buddies,", "tokens": [50996, 286, 362, 257, 1379, 7267, 295, 819, 30649, 11, 51086], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 417, "seek": 100126, "start": 1015.7, "end": 1018.14, "text": " of different companions that I can talk to.", "tokens": [51086, 295, 819, 28009, 300, 286, 393, 751, 281, 13, 51208], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 418, "seek": 100126, "start": 1018.14, "end": 1019.06, "text": " They're kind of human", "tokens": [51208, 814, 434, 733, 295, 1952, 51254], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 419, "seek": 100126, "start": 1019.06, "end": 1020.7, "text": " and kind of talk to me interactively,", "tokens": [51254, 293, 733, 295, 751, 281, 385, 4648, 3413, 11, 51336], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 420, "seek": 100126, "start": 1020.7, "end": 1022.54, "text": " but because I preloaded them with,", "tokens": [51336, 457, 570, 286, 659, 2907, 292, 552, 365, 11, 51428], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 421, "seek": 100126, "start": 1022.54, "end": 1025.1, "text": " hey, by the way, you particular,", "tokens": [51428, 4177, 11, 538, 264, 636, 11, 291, 1729, 11, 51556], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 422, "seek": 100126, "start": 1025.1, "end": 1026.94, "text": " I want you to be a kind, helpful Chinese teacher", "tokens": [51556, 286, 528, 291, 281, 312, 257, 733, 11, 4961, 4649, 5027, 51648], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 423, "seek": 100126, "start": 1026.94, "end": 1028.46, "text": " that responds to every situation", "tokens": [51648, 300, 27331, 281, 633, 2590, 51724], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 424, "seek": 100126, "start": 1028.46, "end": 1030.26, "text": " by explaining the Chongyu that fits it.", "tokens": [51724, 538, 13468, 264, 43040, 17623, 300, 9001, 309, 13, 51814], "temperature": 0.0, "avg_logprob": -0.1369140895866078, "compression_ratio": 1.69364161849711, "no_speech_prob": 0.0012059041764587164}, {"id": 425, "seek": 103026, "start": 1030.3, "end": 1032.78, "text": " Speak in English and explain the Chongyu in its meaning.", "tokens": [50366, 27868, 294, 3669, 293, 2903, 264, 43040, 17623, 294, 1080, 3620, 13, 50490], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 426, "seek": 103026, "start": 1032.78, "end": 1035.74, "text": " Then provide a note of encouragement about learning language.", "tokens": [50490, 1396, 2893, 257, 3637, 295, 25683, 466, 2539, 2856, 13, 50638], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 427, "seek": 103026, "start": 1035.74, "end": 1038.02, "text": " And so just adding something like that,", "tokens": [50638, 400, 370, 445, 5127, 746, 411, 300, 11, 50752], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 428, "seek": 103026, "start": 1038.02, "end": 1039.9, "text": " even if you're a non-programmer,", "tokens": [50752, 754, 498, 291, 434, 257, 2107, 12, 32726, 936, 11, 50846], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 429, "seek": 103026, "start": 1039.9, "end": 1041.5, "text": " you can just type deploy,", "tokens": [50846, 291, 393, 445, 2010, 7274, 11, 50926], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 430, "seek": 103026, "start": 1044.22, "end": 1046.5, "text": " and it'll pop it up to the web,", "tokens": [51062, 293, 309, 603, 1665, 309, 493, 281, 264, 3670, 11, 51176], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 431, "seek": 103026, "start": 1046.5, "end": 1048.14, "text": " it'll take it over to a telegram bot", "tokens": [51176, 309, 603, 747, 309, 670, 281, 257, 4304, 1342, 10592, 51258], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 432, "seek": 103026, "start": 1048.14, "end": 1049.54, "text": " that then you can even interact with,", "tokens": [51258, 300, 550, 291, 393, 754, 4648, 365, 11, 51328], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 433, "seek": 103026, "start": 1049.54, "end": 1052.1, "text": " hey, I'm feeling too busy,", "tokens": [51328, 4177, 11, 286, 478, 2633, 886, 5856, 11, 51456], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 434, "seek": 103026, "start": 1053.34, "end": 1055.94, "text": " and interact with it over telegram, over the web,", "tokens": [51518, 293, 4648, 365, 309, 670, 4304, 1342, 11, 670, 264, 3670, 11, 51648], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 435, "seek": 103026, "start": 1055.94, "end": 1058.58, "text": " and this is the kind of thing that's now within reach", "tokens": [51648, 293, 341, 307, 264, 733, 295, 551, 300, 311, 586, 1951, 2524, 51780], "temperature": 0.0, "avg_logprob": -0.11529247577373798, "compression_ratio": 1.691449814126394, "no_speech_prob": 0.00030460068956017494}, {"id": 436, "seek": 105858, "start": 1058.58, "end": 1061.9399999999998, "text": " for everybody from a CS 101 grad,", "tokens": [50364, 337, 2201, 490, 257, 9460, 21055, 2771, 11, 50532], "temperature": 0.0, "avg_logprob": -0.11738080024719239, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000129250533063896}, {"id": 437, "seek": 105858, "start": 1061.9399999999998, "end": 1064.3799999999999, "text": " sorry, I'm using the general purpose framing,", "tokens": [50532, 2597, 11, 286, 478, 1228, 264, 2674, 4334, 28971, 11, 50654], "temperature": 0.0, "avg_logprob": -0.11738080024719239, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000129250533063896}, {"id": 438, "seek": 105858, "start": 1064.3799999999999, "end": 1066.9399999999998, "text": " all the way through to professionals in the industry,", "tokens": [50654, 439, 264, 636, 807, 281, 11954, 294, 264, 3518, 11, 50782], "temperature": 0.0, "avg_logprob": -0.11738080024719239, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000129250533063896}, {"id": 439, "seek": 105858, "start": 1066.9399999999998, "end": 1069.58, "text": " that you can do just with a little bit of manipulation", "tokens": [50782, 300, 291, 393, 360, 445, 365, 257, 707, 857, 295, 26475, 50914], "temperature": 0.0, "avg_logprob": -0.11738080024719239, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000129250533063896}, {"id": 440, "seek": 105858, "start": 1069.58, "end": 1071.9399999999998, "text": " on top of sort of this raw unit", "tokens": [50914, 322, 1192, 295, 1333, 295, 341, 8936, 4985, 51032], "temperature": 0.0, "avg_logprob": -0.11738080024719239, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000129250533063896}, {"id": 441, "seek": 105858, "start": 1071.9399999999998, "end": 1074.62, "text": " of conversation and intelligence.", "tokens": [51032, 295, 3761, 293, 7599, 13, 51166], "temperature": 0.0, "avg_logprob": -0.11738080024719239, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000129250533063896}, {"id": 442, "seek": 105858, "start": 1077.1, "end": 1080.1799999999998, "text": " So companionship is one of the first", "tokens": [51290, 407, 28009, 1210, 307, 472, 295, 264, 700, 51444], "temperature": 0.0, "avg_logprob": -0.11738080024719239, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000129250533063896}, {"id": 443, "seek": 105858, "start": 1080.1799999999998, "end": 1082.74, "text": " common types of apps that we're seeing.", "tokens": [51444, 2689, 3467, 295, 7733, 300, 321, 434, 2577, 13, 51572], "temperature": 0.0, "avg_logprob": -0.11738080024719239, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000129250533063896}, {"id": 444, "seek": 105858, "start": 1084.82, "end": 1087.78, "text": " So a second kind of app that we're seeing,", "tokens": [51676, 407, 257, 1150, 733, 295, 724, 300, 321, 434, 2577, 11, 51824], "temperature": 0.0, "avg_logprob": -0.11738080024719239, "compression_ratio": 1.5982905982905984, "no_speech_prob": 0.000129250533063896}, {"id": 445, "seek": 108778, "start": 1087.78, "end": 1092.78, "text": " and for those of you who are on Twitter followers,", "tokens": [50364, 293, 337, 729, 295, 291, 567, 366, 322, 5794, 13071, 11, 50614], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 446, "seek": 108778, "start": 1092.86, "end": 1095.8999999999999, "text": " this blew up, I think the last few months,", "tokens": [50618, 341, 19075, 493, 11, 286, 519, 264, 1036, 1326, 2493, 11, 50770], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 447, "seek": 108778, "start": 1095.8999999999999, "end": 1096.98, "text": " is question-answering,", "tokens": [50770, 307, 1168, 12, 43904, 278, 11, 50824], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 448, "seek": 108778, "start": 1096.98, "end": 1098.58, "text": " and I wanna unpack a couple of different ways", "tokens": [50824, 293, 286, 1948, 26699, 257, 1916, 295, 819, 2098, 50904], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 449, "seek": 108778, "start": 1098.58, "end": 1100.62, "text": " this can work, because I know many of you", "tokens": [50904, 341, 393, 589, 11, 570, 286, 458, 867, 295, 291, 51006], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 450, "seek": 108778, "start": 1100.62, "end": 1102.82, "text": " have probably already tried to build", "tokens": [51006, 362, 1391, 1217, 3031, 281, 1322, 51116], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 451, "seek": 108778, "start": 1102.82, "end": 1103.78, "text": " some of these kinds of apps,", "tokens": [51116, 512, 295, 613, 3685, 295, 7733, 11, 51164], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 452, "seek": 108778, "start": 1103.78, "end": 1105.5, "text": " there's a couple of different ways that it works.", "tokens": [51164, 456, 311, 257, 1916, 295, 819, 2098, 300, 309, 1985, 13, 51250], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 453, "seek": 108778, "start": 1105.5, "end": 1109.74, "text": " The general framework is a user queries GPT,", "tokens": [51250, 440, 2674, 8388, 307, 257, 4195, 24109, 26039, 51, 11, 51462], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 454, "seek": 108778, "start": 1109.74, "end": 1111.3, "text": " and maybe it has general purpose knowledge,", "tokens": [51462, 293, 1310, 309, 575, 2674, 4334, 3601, 11, 51540], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 455, "seek": 108778, "start": 1111.3, "end": 1113.1399999999999, "text": " maybe it doesn't have general purpose knowledge,", "tokens": [51540, 1310, 309, 1177, 380, 362, 2674, 4334, 3601, 11, 51632], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 456, "seek": 108778, "start": 1113.1399999999999, "end": 1115.98, "text": " but what you want it to say back to you", "tokens": [51632, 457, 437, 291, 528, 309, 281, 584, 646, 281, 291, 51774], "temperature": 0.0, "avg_logprob": -0.12617149637706243, "compression_ratio": 1.7785714285714285, "no_speech_prob": 0.00012336549116298556}, {"id": 457, "seek": 111598, "start": 1115.98, "end": 1118.66, "text": " is something specific about an article you wrote,", "tokens": [50364, 307, 746, 2685, 466, 364, 7222, 291, 4114, 11, 50498], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 458, "seek": 111598, "start": 1118.66, "end": 1121.5, "text": " or something specific about your course syllabus,", "tokens": [50498, 420, 746, 2685, 466, 428, 1164, 48077, 11, 50640], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 459, "seek": 111598, "start": 1121.5, "end": 1124.66, "text": " or something specific about a particular set of documents", "tokens": [50640, 420, 746, 2685, 466, 257, 1729, 992, 295, 8512, 50798], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 460, "seek": 111598, "start": 1124.66, "end": 1126.94, "text": " from the United Nations on a particular topic.", "tokens": [50798, 490, 264, 2824, 16459, 322, 257, 1729, 4829, 13, 50912], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 461, "seek": 111598, "start": 1126.94, "end": 1128.26, "text": " And so what you're really seeking is", "tokens": [50912, 400, 370, 437, 291, 434, 534, 11670, 307, 50978], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 462, "seek": 111598, "start": 1128.26, "end": 1130.46, "text": " what we all hoped the customer service bot would be,", "tokens": [50978, 437, 321, 439, 19737, 264, 5474, 2643, 10592, 576, 312, 11, 51088], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 463, "seek": 111598, "start": 1130.46, "end": 1132.58, "text": " like we've all interacted with these customer service bots,", "tokens": [51088, 411, 321, 600, 439, 49621, 365, 613, 5474, 2643, 35410, 11, 51194], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 464, "seek": 111598, "start": 1132.58, "end": 1134.18, "text": " and we're kind of smashing our heads", "tokens": [51194, 293, 321, 434, 733, 295, 43316, 527, 8050, 51274], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 465, "seek": 111598, "start": 1134.18, "end": 1135.82, "text": " on the keyboard as we do it,", "tokens": [51274, 322, 264, 10186, 382, 321, 360, 309, 11, 51356], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 466, "seek": 111598, "start": 1135.82, "end": 1138.46, "text": " but pretty soon we're gonna start to see", "tokens": [51356, 457, 1238, 2321, 321, 434, 799, 722, 281, 536, 51488], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 467, "seek": 111598, "start": 1138.46, "end": 1141.5, "text": " very high fidelity bots that interact with us comfortably,", "tokens": [51488, 588, 1090, 46404, 35410, 300, 4648, 365, 505, 25101, 11, 51640], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 468, "seek": 111598, "start": 1141.5, "end": 1143.58, "text": " and this is approximately how to do it as an engineer.", "tokens": [51640, 293, 341, 307, 10447, 577, 281, 360, 309, 382, 364, 11403, 13, 51744], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 469, "seek": 111598, "start": 1143.58, "end": 1145.9, "text": " So here's your game plan as an engineer,", "tokens": [51744, 407, 510, 311, 428, 1216, 1393, 382, 364, 11403, 11, 51860], "temperature": 0.0, "avg_logprob": -0.13129772186279298, "compression_ratio": 1.8666666666666667, "no_speech_prob": 0.0010647093877196312}, {"id": 470, "seek": 114590, "start": 1145.9, "end": 1150.66, "text": " step one, take the documents that you want it to respond to.", "tokens": [50364, 1823, 472, 11, 747, 264, 8512, 300, 291, 528, 309, 281, 4196, 281, 13, 50602], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 471, "seek": 114590, "start": 1151.74, "end": 1153.66, "text": " Step two, cut them up.", "tokens": [50656, 5470, 732, 11, 1723, 552, 493, 13, 50752], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 472, "seek": 114590, "start": 1153.66, "end": 1155.98, "text": " Now, if you're an engineer, this is gonna madden you.", "tokens": [50752, 823, 11, 498, 291, 434, 364, 11403, 11, 341, 307, 799, 5244, 1556, 291, 13, 50868], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 473, "seek": 114590, "start": 1155.98, "end": 1158.5400000000002, "text": " You don't cut them up in a way that you would hope.", "tokens": [50868, 509, 500, 380, 1723, 552, 493, 294, 257, 636, 300, 291, 576, 1454, 13, 50996], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 474, "seek": 114590, "start": 1158.5400000000002, "end": 1160.5400000000002, "text": " For example, you could cut them up", "tokens": [50996, 1171, 1365, 11, 291, 727, 1723, 552, 493, 51096], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 475, "seek": 114590, "start": 1160.5400000000002, "end": 1162.74, "text": " into clean sentences or clean paragraphs,", "tokens": [51096, 666, 2541, 16579, 420, 2541, 48910, 11, 51206], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 476, "seek": 114590, "start": 1162.74, "end": 1164.66, "text": " or semantically coherent sections,", "tokens": [51206, 420, 4361, 49505, 36239, 10863, 11, 51302], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 477, "seek": 114590, "start": 1164.66, "end": 1166.3400000000001, "text": " and that would be really nice.", "tokens": [51302, 293, 300, 576, 312, 534, 1481, 13, 51386], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 478, "seek": 114590, "start": 1166.3400000000001, "end": 1168.42, "text": " Honestly, the way that most folks do it,", "tokens": [51386, 12348, 11, 264, 636, 300, 881, 4024, 360, 309, 11, 51490], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 479, "seek": 114590, "start": 1168.42, "end": 1172.0600000000002, "text": " and this is a simplification that tends to be just fine,", "tokens": [51490, 293, 341, 307, 257, 6883, 3774, 300, 12258, 281, 312, 445, 2489, 11, 51672], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 480, "seek": 114590, "start": 1172.0600000000002, "end": 1174.46, "text": " is you window, you have a sliding window", "tokens": [51672, 307, 291, 4910, 11, 291, 362, 257, 21169, 4910, 51792], "temperature": 0.0, "avg_logprob": -0.1095074468584203, "compression_ratio": 1.718978102189781, "no_speech_prob": 0.00028678294620476663}, {"id": 481, "seek": 117446, "start": 1174.46, "end": 1175.98, "text": " that goes over the document,", "tokens": [50364, 300, 1709, 670, 264, 4166, 11, 50440], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 482, "seek": 117446, "start": 1175.98, "end": 1178.8600000000001, "text": " and you just pull out fragments of text.", "tokens": [50440, 293, 291, 445, 2235, 484, 29197, 295, 2487, 13, 50584], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 483, "seek": 117446, "start": 1178.8600000000001, "end": 1180.66, "text": " Having pulled out those fragments of text,", "tokens": [50584, 10222, 7373, 484, 729, 29197, 295, 2487, 11, 50674], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 484, "seek": 117446, "start": 1180.66, "end": 1183.02, "text": " you turn them into something called an embedding vector.", "tokens": [50674, 291, 1261, 552, 666, 746, 1219, 364, 12240, 3584, 8062, 13, 50792], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 485, "seek": 117446, "start": 1183.02, "end": 1186.1000000000001, "text": " So an embedding vector is a list of numbers", "tokens": [50792, 407, 364, 12240, 3584, 8062, 307, 257, 1329, 295, 3547, 50946], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 486, "seek": 117446, "start": 1186.1000000000001, "end": 1189.26, "text": " that approximate some point of meaning.", "tokens": [50946, 300, 30874, 512, 935, 295, 3620, 13, 51104], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 487, "seek": 117446, "start": 1189.26, "end": 1190.7, "text": " So you've already all dealt", "tokens": [51104, 407, 291, 600, 1217, 439, 15991, 51176], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 488, "seek": 117446, "start": 1190.7, "end": 1192.6200000000001, "text": " with embedding vectors yourself in regular life,", "tokens": [51176, 365, 12240, 3584, 18875, 1803, 294, 3890, 993, 11, 51272], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 489, "seek": 117446, "start": 1192.6200000000001, "end": 1194.42, "text": " and the reason you have, and I know you have,", "tokens": [51272, 293, 264, 1778, 291, 362, 11, 293, 286, 458, 291, 362, 11, 51362], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 490, "seek": 117446, "start": 1194.42, "end": 1197.02, "text": " is because everybody's ordered food from Yelp before.", "tokens": [51362, 307, 570, 2201, 311, 8866, 1755, 490, 398, 28591, 949, 13, 51492], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 491, "seek": 117446, "start": 1197.02, "end": 1198.54, "text": " So when you order food from Yelp,", "tokens": [51492, 407, 562, 291, 1668, 1755, 490, 398, 28591, 11, 51568], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 492, "seek": 117446, "start": 1198.54, "end": 1201.3400000000001, "text": " you look at what genre of restaurant is it?", "tokens": [51568, 291, 574, 412, 437, 11022, 295, 6383, 307, 309, 30, 51708], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 493, "seek": 117446, "start": 1201.3400000000001, "end": 1202.7, "text": " Is it a pizza restaurant?", "tokens": [51708, 1119, 309, 257, 8298, 6383, 30, 51776], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 494, "seek": 117446, "start": 1202.7, "end": 1203.82, "text": " Is it an Italian restaurant?", "tokens": [51776, 1119, 309, 364, 10003, 6383, 30, 51832], "temperature": 0.0, "avg_logprob": -0.07894650141398112, "compression_ratio": 1.8766666666666667, "no_speech_prob": 0.001244529732502997}, {"id": 495, "seek": 120382, "start": 1203.82, "end": 1205.5, "text": " Is it a Korean barbecue place?", "tokens": [50364, 1119, 309, 257, 6933, 21877, 1081, 30, 50448], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 496, "seek": 120382, "start": 1205.5, "end": 1206.98, "text": " You look at how many stars does it have?", "tokens": [50448, 509, 574, 412, 577, 867, 6105, 775, 309, 362, 30, 50522], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 497, "seek": 120382, "start": 1206.98, "end": 1208.86, "text": " One, two, three, four, five?", "tokens": [50522, 1485, 11, 732, 11, 1045, 11, 1451, 11, 1732, 30, 50616], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 498, "seek": 120382, "start": 1208.86, "end": 1209.98, "text": " You look at where is it?", "tokens": [50616, 509, 574, 412, 689, 307, 309, 30, 50672], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 499, "seek": 120382, "start": 1209.98, "end": 1212.82, "text": " So all of these you can think of as points in space,", "tokens": [50672, 407, 439, 295, 613, 291, 393, 519, 295, 382, 2793, 294, 1901, 11, 50814], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 500, "seek": 120382, "start": 1212.82, "end": 1214.06, "text": " dimensions in space.", "tokens": [50814, 12819, 294, 1901, 13, 50876], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 501, "seek": 120382, "start": 1214.06, "end": 1217.1, "text": " Korean barbecue restaurant, four stars near my house.", "tokens": [50876, 6933, 21877, 6383, 11, 1451, 6105, 2651, 452, 1782, 13, 51028], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 502, "seek": 120382, "start": 1217.1, "end": 1220.82, "text": " That's a three number vector.", "tokens": [51028, 663, 311, 257, 1045, 1230, 8062, 13, 51214], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 503, "seek": 120382, "start": 1220.82, "end": 1221.82, "text": " That's all this is.", "tokens": [51214, 663, 311, 439, 341, 307, 13, 51264], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 504, "seek": 120382, "start": 1221.82, "end": 1223.6599999999999, "text": " So this is a thousand number vector,", "tokens": [51264, 407, 341, 307, 257, 4714, 1230, 8062, 11, 51356], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 505, "seek": 120382, "start": 1223.6599999999999, "end": 1224.78, "text": " or a 10,000 number vector.", "tokens": [51356, 420, 257, 1266, 11, 1360, 1230, 8062, 13, 51412], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 506, "seek": 120382, "start": 1224.78, "end": 1227.1, "text": " Different models produce different size vectors.", "tokens": [51412, 20825, 5245, 5258, 819, 2744, 18875, 13, 51528], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 507, "seek": 120382, "start": 1227.1, "end": 1230.26, "text": " All it is is chunking pieces of text,", "tokens": [51528, 1057, 309, 307, 307, 16635, 278, 3755, 295, 2487, 11, 51686], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 508, "seek": 120382, "start": 1230.26, "end": 1232.48, "text": " turning it into a vector that approximates meaning,", "tokens": [51686, 6246, 309, 666, 257, 8062, 300, 8542, 1024, 3620, 11, 51797], "temperature": 0.0, "avg_logprob": -0.10840463313926645, "compression_ratio": 1.7448275862068965, "no_speech_prob": 0.00036822244874201715}, {"id": 509, "seek": 123248, "start": 1232.48, "end": 1234.24, "text": " and then you put it in something called a vector database.", "tokens": [50364, 293, 550, 291, 829, 309, 294, 746, 1219, 257, 8062, 8149, 13, 50452], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 510, "seek": 123248, "start": 1234.24, "end": 1236.52, "text": " And a vector database is just a database", "tokens": [50452, 400, 257, 8062, 8149, 307, 445, 257, 8149, 50566], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 511, "seek": 123248, "start": 1236.52, "end": 1238.52, "text": " that stores numbers.", "tokens": [50566, 300, 9512, 3547, 13, 50666], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 512, "seek": 123248, "start": 1238.52, "end": 1242.44, "text": " But having that database, now when I ask a question,", "tokens": [50666, 583, 1419, 300, 8149, 11, 586, 562, 286, 1029, 257, 1168, 11, 50862], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 513, "seek": 123248, "start": 1242.44, "end": 1244.68, "text": " I can search the database, and I can say, hey, the question", "tokens": [50862, 286, 393, 3164, 264, 8149, 11, 293, 286, 393, 584, 11, 4177, 11, 264, 1168, 50974], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 514, "seek": 123248, "start": 1244.68, "end": 1247.28, "text": " was, what does CS50 teach?", "tokens": [50974, 390, 11, 437, 775, 9460, 2803, 2924, 30, 51104], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 515, "seek": 123248, "start": 1247.28, "end": 1250.52, "text": " What pieces of text in the database", "tokens": [51104, 708, 3755, 295, 2487, 294, 264, 8149, 51266], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 516, "seek": 123248, "start": 1250.52, "end": 1256.0, "text": " have vectors similar to the question, what does CS50 teach?", "tokens": [51266, 362, 18875, 2531, 281, 264, 1168, 11, 437, 775, 9460, 2803, 2924, 30, 51540], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 517, "seek": 123248, "start": 1256.0, "end": 1258.32, "text": " And there's all sorts of tricks and empires", "tokens": [51540, 400, 456, 311, 439, 7527, 295, 11733, 293, 4012, 3145, 51656], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 518, "seek": 123248, "start": 1258.32, "end": 1261.6, "text": " being made on refinements of this general approach.", "tokens": [51656, 885, 1027, 322, 44395, 6400, 295, 341, 2674, 3109, 13, 51820], "temperature": 0.0, "avg_logprob": -0.12940346110950818, "compression_ratio": 1.800796812749004, "no_speech_prob": 0.00015841051936149597}, {"id": 519, "seek": 126160, "start": 1261.6399999999999, "end": 1266.3999999999999, "text": " But at the end, you, the developer, model it simply as thus.", "tokens": [50366, 583, 412, 264, 917, 11, 291, 11, 264, 10754, 11, 2316, 309, 2935, 382, 8807, 13, 50604], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 520, "seek": 126160, "start": 1266.3999999999999, "end": 1268.84, "text": " And then when you have your query, you embed it,", "tokens": [50604, 400, 550, 562, 291, 362, 428, 14581, 11, 291, 12240, 309, 11, 50726], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 521, "seek": 126160, "start": 1268.84, "end": 1271.28, "text": " you find the document fragments, and then you put them", "tokens": [50726, 291, 915, 264, 4166, 29197, 11, 293, 550, 291, 829, 552, 50848], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 522, "seek": 126160, "start": 1271.28, "end": 1271.8799999999999, "text": " into a prompt.", "tokens": [50848, 666, 257, 12391, 13, 50878], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 523, "seek": 126160, "start": 1271.8799999999999, "end": 1275.8, "text": " And now we're just back to the personality, the companionship", "tokens": [50878, 400, 586, 321, 434, 445, 646, 281, 264, 9033, 11, 264, 28009, 1210, 51074], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 524, "seek": 126160, "start": 1275.8, "end": 1276.3999999999999, "text": " bot.", "tokens": [51074, 10592, 13, 51104], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 525, "seek": 126160, "start": 1276.3999999999999, "end": 1277.8, "text": " Now it's just a prompt.", "tokens": [51104, 823, 309, 311, 445, 257, 12391, 13, 51174], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 526, "seek": 126160, "start": 1277.8, "end": 1280.9199999999998, "text": " And the prompt is, you're an expert in answering questions.", "tokens": [51174, 400, 264, 12391, 307, 11, 291, 434, 364, 5844, 294, 13430, 1651, 13, 51330], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 527, "seek": 126160, "start": 1280.9199999999998, "end": 1284.04, "text": " Please answer user provided question.", "tokens": [51330, 2555, 1867, 4195, 5649, 1168, 13, 51486], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 528, "seek": 126160, "start": 1284.04, "end": 1287.12, "text": " Using source documents results from the database.", "tokens": [51486, 11142, 4009, 8512, 3542, 490, 264, 8149, 13, 51640], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 529, "seek": 126160, "start": 1287.12, "end": 1288.76, "text": " That's it.", "tokens": [51640, 663, 311, 309, 13, 51722], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 530, "seek": 126160, "start": 1288.76, "end": 1290.6, "text": " So after all of these decades of engineering", "tokens": [51722, 407, 934, 439, 295, 613, 7878, 295, 7043, 51814], "temperature": 0.0, "avg_logprob": -0.1736856460571289, "compression_ratio": 1.711191335740072, "no_speech_prob": 7.721464498899877e-05}, {"id": 531, "seek": 129060, "start": 1290.6, "end": 1291.6799999999998, "text": " and these customer service bots, it", "tokens": [50364, 293, 613, 5474, 2643, 35410, 11, 309, 50418], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 532, "seek": 129060, "start": 1291.6799999999998, "end": 1293.36, "text": " turns out with a couple of lines of code.", "tokens": [50418, 4523, 484, 365, 257, 1916, 295, 3876, 295, 3089, 13, 50502], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 533, "seek": 129060, "start": 1293.36, "end": 1294.12, "text": " You can build this.", "tokens": [50502, 509, 393, 1322, 341, 13, 50540], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 534, "seek": 129060, "start": 1294.12, "end": 1297.36, "text": " So let me show you, I made one just before the class", "tokens": [50540, 407, 718, 385, 855, 291, 11, 286, 1027, 472, 445, 949, 264, 1508, 50702], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 535, "seek": 129060, "start": 1297.36, "end": 1299.08, "text": " with the CS50 syllabus.", "tokens": [50702, 365, 264, 9460, 2803, 48077, 13, 50788], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 536, "seek": 129060, "start": 1299.08, "end": 1303.76, "text": " So we can pull that up.", "tokens": [50788, 407, 321, 393, 2235, 300, 493, 13, 51022], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 537, "seek": 129060, "start": 1303.76, "end": 1307.04, "text": " And I can say, I added the PDF right here.", "tokens": [51022, 400, 286, 393, 584, 11, 286, 3869, 264, 17752, 558, 510, 13, 51186], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 538, "seek": 129060, "start": 1307.04, "end": 1309.1999999999998, "text": " So I just, I searched, I don't know if, I apologize.", "tokens": [51186, 407, 286, 445, 11, 286, 22961, 11, 286, 500, 380, 458, 498, 11, 286, 12328, 13, 51294], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 539, "seek": 129060, "start": 1309.1999999999998, "end": 1311.24, "text": " I don't know if it's an accurate or recent syllabus.", "tokens": [51294, 286, 500, 380, 458, 498, 309, 311, 364, 8559, 420, 5162, 48077, 13, 51396], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 540, "seek": 129060, "start": 1311.24, "end": 1313.9199999999998, "text": " I just searched the web for CS50 syllabus PDF.", "tokens": [51396, 286, 445, 22961, 264, 3670, 337, 9460, 2803, 48077, 17752, 13, 51530], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 541, "seek": 129060, "start": 1313.9199999999998, "end": 1316.84, "text": " I put the URL in here, it loaded it into here.", "tokens": [51530, 286, 829, 264, 12905, 294, 510, 11, 309, 13210, 309, 666, 510, 13, 51676], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 542, "seek": 129060, "start": 1316.84, "end": 1319.9199999999998, "text": " This is just a 100 line piece of code deployed", "tokens": [51676, 639, 307, 445, 257, 2319, 1622, 2522, 295, 3089, 17826, 51830], "temperature": 0.0, "avg_logprob": -0.18626588142958264, "compression_ratio": 1.6598639455782314, "no_speech_prob": 0.00041727538336999714}, {"id": 543, "seek": 131992, "start": 1319.96, "end": 1322.2, "text": " that will now let me talk to it.", "tokens": [50366, 300, 486, 586, 718, 385, 751, 281, 309, 13, 50478], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 544, "seek": 131992, "start": 1322.2, "end": 1327.64, "text": " And I can say, what will CS50 teach me?", "tokens": [50478, 400, 286, 393, 584, 11, 437, 486, 9460, 2803, 2924, 385, 30, 50750], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 545, "seek": 131992, "start": 1327.64, "end": 1329.24, "text": " So under the hood now, what's happening", "tokens": [50750, 407, 833, 264, 13376, 586, 11, 437, 311, 2737, 50830], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 546, "seek": 131992, "start": 1329.24, "end": 1330.88, "text": " is exactly what that slide just showed you.", "tokens": [50830, 307, 2293, 437, 300, 4137, 445, 4712, 291, 13, 50912], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 547, "seek": 131992, "start": 1330.88, "end": 1333.3600000000001, "text": " It takes that question, what will CS50 teach me?", "tokens": [50912, 467, 2516, 300, 1168, 11, 437, 486, 9460, 2803, 2924, 385, 30, 51036], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 548, "seek": 131992, "start": 1333.3600000000001, "end": 1335.2, "text": " It turns it into a vector.", "tokens": [51036, 467, 4523, 309, 666, 257, 8062, 13, 51128], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 549, "seek": 131992, "start": 1335.2, "end": 1338.88, "text": " That vector approximates without exactly representing", "tokens": [51128, 663, 8062, 8542, 1024, 1553, 2293, 13460, 51312], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 550, "seek": 131992, "start": 1338.88, "end": 1341.16, "text": " the meaning of that question.", "tokens": [51312, 264, 3620, 295, 300, 1168, 13, 51426], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 551, "seek": 131992, "start": 1341.16, "end": 1343.64, "text": " It looks into a vector database that", "tokens": [51426, 467, 1542, 666, 257, 8062, 8149, 300, 51550], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 552, "seek": 131992, "start": 1343.64, "end": 1347.76, "text": " steamship hosts of fragments from that PDF.", "tokens": [51550, 2126, 4070, 1210, 21573, 295, 29197, 490, 300, 17752, 13, 51756], "temperature": 0.0, "avg_logprob": -0.1186865893277255, "compression_ratio": 1.6822033898305084, "no_speech_prob": 6.013637539581396e-05}, {"id": 553, "seek": 134776, "start": 1347.76, "end": 1350.12, "text": " And then it pulls out a document and then passes it", "tokens": [50364, 400, 550, 309, 16982, 484, 257, 4166, 293, 550, 11335, 309, 50482], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 554, "seek": 134776, "start": 1350.12, "end": 1352.92, "text": " to a prompt that says, hey, you're an expert", "tokens": [50482, 281, 257, 12391, 300, 1619, 11, 4177, 11, 291, 434, 364, 5844, 50622], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 555, "seek": 134776, "start": 1352.92, "end": 1354.48, "text": " at answering questions.", "tokens": [50622, 412, 13430, 1651, 13, 50700], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 556, "seek": 134776, "start": 1354.48, "end": 1357.08, "text": " Someone has asked you, what does CS50 teach?", "tokens": [50700, 8734, 575, 2351, 291, 11, 437, 775, 9460, 2803, 2924, 30, 50830], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 557, "seek": 134776, "start": 1357.08, "end": 1359.92, "text": " Please answer it using only the source documents", "tokens": [50830, 2555, 1867, 309, 1228, 787, 264, 4009, 8512, 50972], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 558, "seek": 134776, "start": 1359.92, "end": 1361.8, "text": " and source materials I've provided.", "tokens": [50972, 293, 4009, 5319, 286, 600, 5649, 13, 51066], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 559, "seek": 134776, "start": 1361.8, "end": 1363.64, "text": " Now those source materials materials", "tokens": [51066, 823, 729, 4009, 5319, 5319, 51158], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 560, "seek": 134776, "start": 1363.64, "end": 1365.56, "text": " are dynamically loaded into the prompt.", "tokens": [51158, 366, 43492, 13210, 666, 264, 12391, 13, 51254], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 561, "seek": 134776, "start": 1365.56, "end": 1366.72, "text": " It's just basic prompt engineering.", "tokens": [51254, 467, 311, 445, 3875, 12391, 7043, 13, 51312], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 562, "seek": 134776, "start": 1366.72, "end": 1369.28, "text": " And I want to keep harping back onto that.", "tokens": [51312, 400, 286, 528, 281, 1066, 2233, 3381, 646, 3911, 300, 13, 51440], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 563, "seek": 134776, "start": 1369.28, "end": 1371.72, "text": " What's amazing about right now as builders", "tokens": [51440, 708, 311, 2243, 466, 558, 586, 382, 36281, 51562], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 564, "seek": 134776, "start": 1371.72, "end": 1374.0, "text": " is that so many things just boil down", "tokens": [51562, 307, 300, 370, 867, 721, 445, 13329, 760, 51676], "temperature": 0.0, "avg_logprob": -0.10109918975830078, "compression_ratio": 1.667808219178082, "no_speech_prob": 0.0005357048939913511}, {"id": 565, "seek": 137400, "start": 1374.04, "end": 1379.4, "text": " into very creative, tactical rearrangement of prompts", "tokens": [50366, 666, 588, 5880, 11, 26323, 39568, 518, 295, 41095, 50634], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 566, "seek": 137400, "start": 1379.4, "end": 1381.88, "text": " and then using those over and over again in an algorithm", "tokens": [50634, 293, 550, 1228, 729, 670, 293, 670, 797, 294, 364, 9284, 50758], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 567, "seek": 137400, "start": 1381.88, "end": 1383.04, "text": " and putting that into software.", "tokens": [50758, 293, 3372, 300, 666, 4722, 13, 50816], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 568, "seek": 137400, "start": 1383.04, "end": 1385.36, "text": " So the result, and again, it could be lying.", "tokens": [50816, 407, 264, 1874, 11, 293, 797, 11, 309, 727, 312, 8493, 13, 50932], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 569, "seek": 137400, "start": 1385.36, "end": 1386.28, "text": " It could be making things up.", "tokens": [50932, 467, 727, 312, 1455, 721, 493, 13, 50978], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 570, "seek": 137400, "start": 1386.28, "end": 1387.52, "text": " It could be hallucinating.", "tokens": [50978, 467, 727, 312, 35212, 8205, 13, 51040], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 571, "seek": 137400, "start": 1387.52, "end": 1389.84, "text": " Is CS50 will teach students how to think algorithmically", "tokens": [51040, 1119, 9460, 2803, 486, 2924, 1731, 577, 281, 519, 9284, 984, 51156], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 572, "seek": 137400, "start": 1389.84, "end": 1391.68, "text": " and solve problems efficiently, focusing on topics", "tokens": [51156, 293, 5039, 2740, 19621, 11, 8416, 322, 8378, 51248], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 573, "seek": 137400, "start": 1391.68, "end": 1393.44, "text": " such as abstraction, dot, dot, dot, dot, dot.", "tokens": [51248, 1270, 382, 37765, 11, 5893, 11, 5893, 11, 5893, 11, 5893, 11, 5893, 13, 51336], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 574, "seek": 137400, "start": 1393.44, "end": 1395.6, "text": " And then it returns the source document", "tokens": [51336, 400, 550, 309, 11247, 264, 4009, 4166, 51444], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 575, "seek": 137400, "start": 1395.6, "end": 1396.6, "text": " from which it was found.", "tokens": [51444, 490, 597, 309, 390, 1352, 13, 51494], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 576, "seek": 137400, "start": 1396.6, "end": 1399.12, "text": " So this is another big category of which there", "tokens": [51494, 407, 341, 307, 1071, 955, 7719, 295, 597, 456, 51620], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 577, "seek": 137400, "start": 1399.12, "end": 1402.72, "text": " are tons of potential applications", "tokens": [51620, 366, 9131, 295, 3995, 5821, 51800], "temperature": 0.0, "avg_logprob": -0.1937402958492581, "compression_ratio": 1.7580645161290323, "no_speech_prob": 0.0006666179397143424}, {"id": 578, "seek": 140272, "start": 1402.72, "end": 1405.1200000000001, "text": " because you can repeat for each context.", "tokens": [50364, 570, 291, 393, 7149, 337, 1184, 4319, 13, 50484], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 579, "seek": 140272, "start": 1405.1200000000001, "end": 1407.8, "text": " You can create arbitrarily many of these once it's software", "tokens": [50484, 509, 393, 1884, 19071, 3289, 867, 295, 613, 1564, 309, 311, 4722, 50618], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 580, "seek": 140272, "start": 1407.8, "end": 1410.84, "text": " because once it's software, you can just repeat it", "tokens": [50618, 570, 1564, 309, 311, 4722, 11, 291, 393, 445, 7149, 309, 50770], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 581, "seek": 140272, "start": 1410.84, "end": 1411.68, "text": " over and over again.", "tokens": [50770, 670, 293, 670, 797, 13, 50812], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 582, "seek": 140272, "start": 1411.68, "end": 1414.6000000000001, "text": " So for your dorm, for your club, for your slack,", "tokens": [50812, 407, 337, 428, 12521, 11, 337, 428, 6482, 11, 337, 428, 29767, 11, 50958], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 583, "seek": 140272, "start": 1414.6000000000001, "end": 1417.56, "text": " for your telegram, you can start to begin putting", "tokens": [50958, 337, 428, 4304, 1342, 11, 291, 393, 722, 281, 1841, 3372, 51106], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 584, "seek": 140272, "start": 1417.56, "end": 1420.56, "text": " pieces of information in and then responding to it.", "tokens": [51106, 3755, 295, 1589, 294, 293, 550, 16670, 281, 309, 13, 51256], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 585, "seek": 140272, "start": 1420.56, "end": 1422.04, "text": " And it doesn't have to be documents.", "tokens": [51256, 400, 309, 1177, 380, 362, 281, 312, 8512, 13, 51330], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 586, "seek": 140272, "start": 1422.04, "end": 1424.94, "text": " You can also load it straight into the prompt.", "tokens": [51330, 509, 393, 611, 3677, 309, 2997, 666, 264, 12391, 13, 51475], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 587, "seek": 140272, "start": 1426.2, "end": 1427.8, "text": " I think I have it pulled up here.", "tokens": [51538, 286, 519, 286, 362, 309, 7373, 493, 510, 13, 51618], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 588, "seek": 140272, "start": 1427.8, "end": 1430.2, "text": " And if I don't, I'll just skip it.", "tokens": [51618, 400, 498, 286, 500, 380, 11, 286, 603, 445, 10023, 309, 13, 51738], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 589, "seek": 140272, "start": 1430.2, "end": 1431.04, "text": " Oh, here we go.", "tokens": [51738, 876, 11, 510, 321, 352, 13, 51780], "temperature": 0.0, "avg_logprob": -0.09567312455513108, "compression_ratio": 1.789090909090909, "no_speech_prob": 0.0009398118127137423}, {"id": 590, "seek": 143104, "start": 1432.04, "end": 1434.3999999999999, "text": " One other way you can do question answering,", "tokens": [50414, 1485, 661, 636, 291, 393, 360, 1168, 13430, 11, 50532], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 591, "seek": 143104, "start": 1435.52, "end": 1437.6, "text": " because I think it's healthy to always encourage", "tokens": [50588, 570, 286, 519, 309, 311, 4627, 281, 1009, 5373, 50692], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 592, "seek": 143104, "start": 1437.6, "end": 1440.52, "text": " the simplest possible approach to something.", "tokens": [50692, 264, 22811, 1944, 3109, 281, 746, 13, 50838], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 593, "seek": 143104, "start": 1440.52, "end": 1442.96, "text": " You don't need to engineer this giant system.", "tokens": [50838, 509, 500, 380, 643, 281, 11403, 341, 7410, 1185, 13, 50960], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 594, "seek": 143104, "start": 1442.96, "end": 1444.2, "text": " It's great to have a database.", "tokens": [50960, 467, 311, 869, 281, 362, 257, 8149, 13, 51022], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 595, "seek": 143104, "start": 1444.2, "end": 1445.24, "text": " It's great to use embeddings.", "tokens": [51022, 467, 311, 869, 281, 764, 12240, 29432, 13, 51074], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 596, "seek": 143104, "start": 1445.24, "end": 1446.48, "text": " It's great to use this big approach.", "tokens": [51074, 467, 311, 869, 281, 764, 341, 955, 3109, 13, 51136], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 597, "seek": 143104, "start": 1446.48, "end": 1447.68, "text": " It's fancy at scales.", "tokens": [51136, 467, 311, 10247, 412, 17408, 13, 51196], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 598, "seek": 143104, "start": 1447.68, "end": 1449.48, "text": " You can do a lot of things.", "tokens": [51196, 509, 393, 360, 257, 688, 295, 721, 13, 51286], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 599, "seek": 143104, "start": 1449.48, "end": 1451.96, "text": " But you can also get away with a lot", "tokens": [51286, 583, 291, 393, 611, 483, 1314, 365, 257, 688, 51410], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 600, "seek": 143104, "start": 1451.96, "end": 1453.8799999999999, "text": " by just pushing it all into a prompt.", "tokens": [51410, 538, 445, 7380, 309, 439, 666, 257, 12391, 13, 51506], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 601, "seek": 143104, "start": 1453.8799999999999, "end": 1456.28, "text": " And as an engineer, I'm, you know,", "tokens": [51506, 400, 382, 364, 11403, 11, 286, 478, 11, 291, 458, 11, 51626], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 602, "seek": 143104, "start": 1456.28, "end": 1457.72, "text": " that's one of our team who's here always says,", "tokens": [51626, 300, 311, 472, 295, 527, 1469, 567, 311, 510, 1009, 1619, 11, 51698], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 603, "seek": 143104, "start": 1457.72, "end": 1459.2, "text": " like, engineers should aspire to be lazy.", "tokens": [51698, 411, 11, 11955, 820, 41224, 281, 312, 14847, 13, 51772], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 604, "seek": 143104, "start": 1459.2, "end": 1460.8, "text": " And I couldn't agree more.", "tokens": [51772, 400, 286, 2809, 380, 3986, 544, 13, 51852], "temperature": 0.0, "avg_logprob": -0.14401049701714078, "compression_ratio": 1.7714285714285714, "no_speech_prob": 0.00015842072025407106}, {"id": 605, "seek": 146080, "start": 1460.8799999999999, "end": 1463.8799999999999, "text": " You, as an engineer, should want to set yourself up", "tokens": [50368, 509, 11, 382, 364, 11403, 11, 820, 528, 281, 992, 1803, 493, 50518], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 606, "seek": 146080, "start": 1463.8799999999999, "end": 1467.36, "text": " so that you can pursue the lazy path to something.", "tokens": [50518, 370, 300, 291, 393, 12392, 264, 14847, 3100, 281, 746, 13, 50692], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 607, "seek": 146080, "start": 1467.36, "end": 1470.72, "text": " So here's how you might do the equivalent", "tokens": [50692, 407, 510, 311, 577, 291, 1062, 360, 264, 10344, 50860], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 608, "seek": 146080, "start": 1470.72, "end": 1472.6, "text": " of a question answering system with a prompt alone.", "tokens": [50860, 295, 257, 1168, 13430, 1185, 365, 257, 12391, 3312, 13, 50954], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 609, "seek": 146080, "start": 1472.6, "end": 1475.2, "text": " Let's say you have 30 friends.", "tokens": [50954, 961, 311, 584, 291, 362, 2217, 1855, 13, 51084], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 610, "seek": 146080, "start": 1475.2, "end": 1477.08, "text": " And each friend is good at a particular thing,", "tokens": [51084, 400, 1184, 1277, 307, 665, 412, 257, 1729, 551, 11, 51178], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 611, "seek": 146080, "start": 1477.08, "end": 1479.0, "text": " or you can, you know, this is isomorphic", "tokens": [51178, 420, 291, 393, 11, 291, 458, 11, 341, 307, 307, 32702, 299, 51274], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 612, "seek": 146080, "start": 1479.0, "end": 1480.56, "text": " to many other problems.", "tokens": [51274, 281, 867, 661, 2740, 13, 51352], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 613, "seek": 146080, "start": 1480.56, "end": 1483.06, "text": " You can simply just say, hey, I know certain things.", "tokens": [51352, 509, 393, 2935, 445, 584, 11, 4177, 11, 286, 458, 1629, 721, 13, 51477], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 614, "seek": 146080, "start": 1483.06, "end": 1484.72, "text": " Here's the things I know.", "tokens": [51477, 1692, 311, 264, 721, 286, 458, 13, 51560], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 615, "seek": 146080, "start": 1484.72, "end": 1489.1599999999999, "text": " A user's gonna ask me something, how should we respond?", "tokens": [51560, 316, 4195, 311, 799, 1029, 385, 746, 11, 577, 820, 321, 4196, 30, 51782], "temperature": 0.0, "avg_logprob": -0.09724815567927574, "compression_ratio": 1.6288659793814433, "no_speech_prob": 5.649193190038204e-05}, {"id": 616, "seek": 148916, "start": 1489.16, "end": 1490.8400000000001, "text": " And then you load that into an agent.", "tokens": [50364, 400, 550, 291, 3677, 300, 666, 364, 9461, 13, 50448], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 617, "seek": 148916, "start": 1490.8400000000001, "end": 1493.48, "text": " That agent has access to GPT.", "tokens": [50448, 663, 9461, 575, 2105, 281, 26039, 51, 13, 50580], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 618, "seek": 148916, "start": 1493.48, "end": 1494.8400000000001, "text": " You can ship deploy it.", "tokens": [50580, 509, 393, 5374, 7274, 309, 13, 50648], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 619, "seek": 148916, "start": 1494.8400000000001, "end": 1497.68, "text": " And now you've got a bot that you can connect to Telegram.", "tokens": [50648, 400, 586, 291, 600, 658, 257, 10592, 300, 291, 393, 1745, 281, 14889, 1342, 13, 50790], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 620, "seek": 148916, "start": 1497.68, "end": 1499.3600000000001, "text": " You can connect to Slack.", "tokens": [50790, 509, 393, 1745, 281, 37211, 13, 50874], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 621, "seek": 148916, "start": 1499.3600000000001, "end": 1502.52, "text": " And that bot, now it won't always give you the right answer.", "tokens": [50874, 400, 300, 10592, 11, 586, 309, 1582, 380, 1009, 976, 291, 264, 558, 1867, 13, 51032], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 622, "seek": 148916, "start": 1502.52, "end": 1503.48, "text": " Because at a certain level,", "tokens": [51032, 1436, 412, 257, 1629, 1496, 11, 51080], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 623, "seek": 148916, "start": 1503.48, "end": 1506.66, "text": " we can't control the variance of the model underneath.", "tokens": [51080, 321, 393, 380, 1969, 264, 21977, 295, 264, 2316, 7223, 13, 51239], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 624, "seek": 148916, "start": 1506.66, "end": 1510.24, "text": " But it will tend to answer with respect to this list.", "tokens": [51239, 583, 309, 486, 3928, 281, 1867, 365, 3104, 281, 341, 1329, 13, 51418], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 625, "seek": 148916, "start": 1510.24, "end": 1513.1200000000001, "text": " And the degree to which it tends to is to a certain extent,", "tokens": [51418, 400, 264, 4314, 281, 597, 309, 12258, 281, 307, 281, 257, 1629, 8396, 11, 51562], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 626, "seek": 148916, "start": 1513.1200000000001, "end": 1515.0800000000002, "text": " something that both industry is working on", "tokens": [51562, 746, 300, 1293, 3518, 307, 1364, 322, 51660], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 627, "seek": 148916, "start": 1515.0800000000002, "end": 1517.64, "text": " to just give everybody as a capacity.", "tokens": [51660, 281, 445, 976, 2201, 382, 257, 6042, 13, 51788], "temperature": 0.0, "avg_logprob": -0.12314450990903628, "compression_ratio": 1.734006734006734, "no_speech_prob": 0.0004044083470944315}, {"id": 628, "seek": 151764, "start": 1517.64, "end": 1519.72, "text": " But also you doing prompt engineering", "tokens": [50364, 583, 611, 291, 884, 12391, 7043, 50468], "temperature": 0.0, "avg_logprob": -0.1363388208242563, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.604784721275792e-05}, {"id": 629, "seek": 151764, "start": 1519.72, "end": 1523.3400000000001, "text": " to tighten up the error bars on it.", "tokens": [50468, 281, 17041, 493, 264, 6713, 10228, 322, 309, 13, 50649], "temperature": 0.0, "avg_logprob": -0.1363388208242563, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.604784721275792e-05}, {"id": 630, "seek": 151764, "start": 1526.9, "end": 1529.2800000000002, "text": " So I'll show you just a few more examples.", "tokens": [50827, 407, 286, 603, 855, 291, 445, 257, 1326, 544, 5110, 13, 50946], "temperature": 0.0, "avg_logprob": -0.1363388208242563, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.604784721275792e-05}, {"id": 631, "seek": 151764, "start": 1529.2800000000002, "end": 1531.5200000000002, "text": " And then in about eight minutes,", "tokens": [50946, 400, 550, 294, 466, 3180, 2077, 11, 51058], "temperature": 0.0, "avg_logprob": -0.1363388208242563, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.604784721275792e-05}, {"id": 632, "seek": 151764, "start": 1531.5200000000002, "end": 1532.6200000000001, "text": " I'll turn it over to questions,", "tokens": [51058, 286, 603, 1261, 309, 670, 281, 1651, 11, 51113], "temperature": 0.0, "avg_logprob": -0.1363388208242563, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.604784721275792e-05}, {"id": 633, "seek": 151764, "start": 1532.6200000000001, "end": 1534.2800000000002, "text": " because I'm sure you've got a lot about how to build things.", "tokens": [51113, 570, 286, 478, 988, 291, 600, 658, 257, 688, 466, 577, 281, 1322, 721, 13, 51196], "temperature": 0.0, "avg_logprob": -0.1363388208242563, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.604784721275792e-05}, {"id": 634, "seek": 151764, "start": 1534.2800000000002, "end": 1537.0200000000002, "text": " So just to give you a sense of where we are.", "tokens": [51196, 407, 445, 281, 976, 291, 257, 2020, 295, 689, 321, 366, 13, 51333], "temperature": 0.0, "avg_logprob": -0.1363388208242563, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.604784721275792e-05}, {"id": 635, "seek": 151764, "start": 1541.0, "end": 1543.2, "text": " This is one, I don't have a demo for you.", "tokens": [51532, 639, 307, 472, 11, 286, 500, 380, 362, 257, 10723, 337, 291, 13, 51642], "temperature": 0.0, "avg_logprob": -0.1363388208242563, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.604784721275792e-05}, {"id": 636, "seek": 151764, "start": 1543.2, "end": 1546.5, "text": " But if you were to come to me and you were to say, Ted,", "tokens": [51642, 583, 498, 291, 645, 281, 808, 281, 385, 293, 291, 645, 281, 584, 11, 14985, 11, 51807], "temperature": 0.0, "avg_logprob": -0.1363388208242563, "compression_ratio": 1.5909090909090908, "no_speech_prob": 6.604784721275792e-05}, {"id": 637, "seek": 154650, "start": 1546.5, "end": 1549.54, "text": " I want a weekend hustle, man, what should I build?", "tokens": [50364, 286, 528, 257, 6711, 34639, 11, 587, 11, 437, 820, 286, 1322, 30, 50516], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 638, "seek": 154650, "start": 1549.54, "end": 1551.1, "text": " Holy moly.", "tokens": [50516, 6295, 705, 356, 13, 50594], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 639, "seek": 154650, "start": 1551.1, "end": 1553.3, "text": " There are a set of applications", "tokens": [50594, 821, 366, 257, 992, 295, 5821, 50704], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 640, "seek": 154650, "start": 1553.3, "end": 1555.06, "text": " that I would describe as utility functions.", "tokens": [50704, 300, 286, 576, 6786, 382, 14877, 6828, 13, 50792], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 641, "seek": 154650, "start": 1555.06, "end": 1556.26, "text": " I don't like that name,", "tokens": [50792, 286, 500, 380, 411, 300, 1315, 11, 50852], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 642, "seek": 154650, "start": 1556.26, "end": 1557.46, "text": " because it doesn't sound exciting,", "tokens": [50852, 570, 309, 1177, 380, 1626, 4670, 11, 50912], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 643, "seek": 154650, "start": 1557.46, "end": 1559.18, "text": " and this is really exciting.", "tokens": [50912, 293, 341, 307, 534, 4670, 13, 50998], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 644, "seek": 154650, "start": 1559.18, "end": 1562.38, "text": " And it's low hanging fruits that automate tasks", "tokens": [50998, 400, 309, 311, 2295, 8345, 12148, 300, 31605, 9608, 51158], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 645, "seek": 154650, "start": 1562.38, "end": 1564.22, "text": " that require basic language understanding.", "tokens": [51158, 300, 3651, 3875, 2856, 3701, 13, 51250], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 646, "seek": 154650, "start": 1564.22, "end": 1568.14, "text": " So examples for this are generate a unit test.", "tokens": [51250, 407, 5110, 337, 341, 366, 8460, 257, 4985, 1500, 13, 51446], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 647, "seek": 154650, "start": 1568.14, "end": 1570.86, "text": " I don't know how many of you have ever been writing tests", "tokens": [51446, 286, 500, 380, 458, 577, 867, 295, 291, 362, 1562, 668, 3579, 6921, 51582], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 648, "seek": 154650, "start": 1570.86, "end": 1572.26, "text": " and you're just like, oh, come on,", "tokens": [51582, 293, 291, 434, 445, 411, 11, 1954, 11, 808, 322, 11, 51652], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 649, "seek": 154650, "start": 1572.26, "end": 1573.98, "text": " I can get through this, I can get through this.", "tokens": [51652, 286, 393, 483, 807, 341, 11, 286, 393, 483, 807, 341, 13, 51738], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 650, "seek": 154650, "start": 1573.98, "end": 1575.18, "text": " If you're a person who likes writing tests,", "tokens": [51738, 759, 291, 434, 257, 954, 567, 5902, 3579, 6921, 11, 51798], "temperature": 0.0, "avg_logprob": -0.11243242966501336, "compression_ratio": 1.7232704402515724, "no_speech_prob": 0.001064772019162774}, {"id": 651, "seek": 157518, "start": 1575.18, "end": 1576.8600000000001, "text": " you're a lucky individual.", "tokens": [50364, 291, 434, 257, 6356, 2609, 13, 50448], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 652, "seek": 157518, "start": 1576.8600000000001, "end": 1578.6200000000001, "text": " Looking up the documentation for a function,", "tokens": [50448, 11053, 493, 264, 14333, 337, 257, 2445, 11, 50536], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 653, "seek": 157518, "start": 1578.6200000000001, "end": 1580.5800000000002, "text": " rewriting a function, making something conform", "tokens": [50536, 319, 19868, 257, 2445, 11, 1455, 746, 18975, 50634], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 654, "seek": 157518, "start": 1580.5800000000002, "end": 1583.6200000000001, "text": " to your company guidelines, doing a brand check.", "tokens": [50634, 281, 428, 2237, 12470, 11, 884, 257, 3360, 1520, 13, 50786], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 655, "seek": 157518, "start": 1583.6200000000001, "end": 1584.8600000000001, "text": " All of these things are things", "tokens": [50786, 1057, 295, 613, 721, 366, 721, 50848], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 656, "seek": 157518, "start": 1584.8600000000001, "end": 1589.46, "text": " that are kind of relatively context-free operations", "tokens": [50848, 300, 366, 733, 295, 7226, 4319, 12, 10792, 7705, 51078], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 657, "seek": 157518, "start": 1589.46, "end": 1593.14, "text": " or scoped context operations on a piece of information", "tokens": [51078, 420, 795, 27277, 4319, 7705, 322, 257, 2522, 295, 1589, 51262], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 658, "seek": 157518, "start": 1593.14, "end": 1595.14, "text": " that requires linguistic understanding.", "tokens": [51262, 300, 7029, 43002, 3701, 13, 51362], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 659, "seek": 157518, "start": 1596.3, "end": 1599.5800000000002, "text": " And really, you can think of them as something", "tokens": [51420, 400, 534, 11, 291, 393, 519, 295, 552, 382, 746, 51584], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 660, "seek": 157518, "start": 1599.5800000000002, "end": 1602.42, "text": " that is now available to you as a software builder,", "tokens": [51584, 300, 307, 586, 2435, 281, 291, 382, 257, 4722, 27377, 11, 51726], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 661, "seek": 157518, "start": 1602.42, "end": 1604.18, "text": " as a weekend project builder,", "tokens": [51726, 382, 257, 6711, 1716, 27377, 11, 51814], "temperature": 0.0, "avg_logprob": -0.1482011406822542, "compression_ratio": 1.762081784386617, "no_speech_prob": 0.0004727759805973619}, {"id": 662, "seek": 160418, "start": 1604.18, "end": 1606.22, "text": " as a startup builder.", "tokens": [50364, 382, 257, 18578, 27377, 13, 50466], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 663, "seek": 160418, "start": 1606.22, "end": 1608.74, "text": " And you just have to build the interface around it", "tokens": [50466, 400, 291, 445, 362, 281, 1322, 264, 9226, 926, 309, 50592], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 664, "seek": 160418, "start": 1608.74, "end": 1611.46, "text": " and present it to other people in a context", "tokens": [50592, 293, 1974, 309, 281, 661, 561, 294, 257, 4319, 50728], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 665, "seek": 160418, "start": 1611.46, "end": 1614.18, "text": " in which it's meaningful for them to consume.", "tokens": [50728, 294, 597, 309, 311, 10995, 337, 552, 281, 14732, 13, 50864], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 666, "seek": 160418, "start": 1614.18, "end": 1617.22, "text": " And so the space of this is extraordinary.", "tokens": [50864, 400, 370, 264, 1901, 295, 341, 307, 10581, 13, 51016], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 667, "seek": 160418, "start": 1617.22, "end": 1619.38, "text": " I mean, it's the space of all human endeavor,", "tokens": [51016, 286, 914, 11, 309, 311, 264, 1901, 295, 439, 1952, 34975, 11, 51124], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 668, "seek": 160418, "start": 1619.38, "end": 1620.9, "text": " now with this new tool, I think,", "tokens": [51124, 586, 365, 341, 777, 2290, 11, 286, 519, 11, 51200], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 669, "seek": 160418, "start": 1620.9, "end": 1622.18, "text": " is the way to think about it.", "tokens": [51200, 307, 264, 636, 281, 519, 466, 309, 13, 51264], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 670, "seek": 160418, "start": 1622.18, "end": 1624.78, "text": " People often joke about how when you're building a company,", "tokens": [51264, 3432, 2049, 7647, 466, 577, 562, 291, 434, 2390, 257, 2237, 11, 51394], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 671, "seek": 160418, "start": 1624.78, "end": 1625.8200000000002, "text": " when you're building a project,", "tokens": [51394, 562, 291, 434, 2390, 257, 1716, 11, 51446], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 672, "seek": 160418, "start": 1625.8200000000002, "end": 1627.42, "text": " you don't want to start with a hammer,", "tokens": [51446, 291, 500, 380, 528, 281, 722, 365, 257, 13017, 11, 51526], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 673, "seek": 160418, "start": 1627.42, "end": 1629.5, "text": " because you want to start with a problem instead.", "tokens": [51526, 570, 291, 528, 281, 722, 365, 257, 1154, 2602, 13, 51630], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 674, "seek": 160418, "start": 1629.5, "end": 1631.98, "text": " And it's generally true, but my God,", "tokens": [51630, 400, 309, 311, 5101, 2074, 11, 457, 452, 1265, 11, 51754], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 675, "seek": 160418, "start": 1631.98, "end": 1633.8200000000002, "text": " we've just got a really cool new hammer.", "tokens": [51754, 321, 600, 445, 658, 257, 534, 1627, 777, 13017, 13, 51846], "temperature": 0.0, "avg_logprob": -0.11857106226571598, "compression_ratio": 1.819047619047619, "no_speech_prob": 0.00028682430274784565}, {"id": 676, "seek": 163382, "start": 1633.82, "end": 1635.82, "text": " And to a certain extent, I would encourage you", "tokens": [50364, 400, 281, 257, 1629, 8396, 11, 286, 576, 5373, 291, 50464], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 677, "seek": 163382, "start": 1635.82, "end": 1637.34, "text": " to at least casually, on the weekends,", "tokens": [50464, 281, 412, 1935, 34872, 11, 322, 264, 23595, 11, 50540], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 678, "seek": 163382, "start": 1637.34, "end": 1638.58, "text": " run around and hit stuff with it", "tokens": [50540, 1190, 926, 293, 2045, 1507, 365, 309, 50602], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 679, "seek": 163382, "start": 1638.58, "end": 1640.58, "text": " and see what can happen from a builder's,", "tokens": [50602, 293, 536, 437, 393, 1051, 490, 257, 27377, 311, 11, 50702], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 680, "seek": 163382, "start": 1640.58, "end": 1643.98, "text": " from a tinkerers, from an experimentalist's point of view.", "tokens": [50702, 490, 257, 256, 40467, 433, 11, 490, 364, 17069, 468, 311, 935, 295, 1910, 13, 50872], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 681, "seek": 163382, "start": 1647.9399999999998, "end": 1651.02, "text": " And then the final one is creativity.", "tokens": [51070, 400, 550, 264, 2572, 472, 307, 12915, 13, 51224], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 682, "seek": 163382, "start": 1651.02, "end": 1652.9399999999998, "text": " This is another huge mega app.", "tokens": [51224, 639, 307, 1071, 2603, 17986, 724, 13, 51320], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 683, "seek": 163382, "start": 1652.9399999999998, "end": 1655.6599999999999, "text": " Now, I'm primarily living the text world,", "tokens": [51320, 823, 11, 286, 478, 10029, 2647, 264, 2487, 1002, 11, 51456], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 684, "seek": 163382, "start": 1655.6599999999999, "end": 1657.9399999999998, "text": " and so I'm gonna talk about text-based things.", "tokens": [51456, 293, 370, 286, 478, 799, 751, 466, 2487, 12, 6032, 721, 13, 51570], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 685, "seek": 163382, "start": 1657.9399999999998, "end": 1661.4199999999998, "text": " I think so far, this has mostly been growing", "tokens": [51570, 286, 519, 370, 1400, 11, 341, 575, 5240, 668, 4194, 51744], "temperature": 0.0, "avg_logprob": -0.1384142557779948, "compression_ratio": 1.5687732342007434, "no_speech_prob": 0.00010552707681199536}, {"id": 686, "seek": 166142, "start": 1661.42, "end": 1664.22, "text": " in the imagery world, because we're such visual creatures,", "tokens": [50364, 294, 264, 24340, 1002, 11, 570, 321, 434, 1270, 5056, 12281, 11, 50504], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 687, "seek": 166142, "start": 1664.22, "end": 1665.6200000000001, "text": " and the images you can generate", "tokens": [50504, 293, 264, 5267, 291, 393, 8460, 50574], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 688, "seek": 166142, "start": 1665.6200000000001, "end": 1668.54, "text": " are just staggering with AI.", "tokens": [50574, 366, 445, 42974, 365, 7318, 13, 50720], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 689, "seek": 166142, "start": 1668.54, "end": 1670.26, "text": " Certainly brings up a lot of questions, too,", "tokens": [50720, 16628, 5607, 493, 257, 688, 295, 1651, 11, 886, 11, 50806], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 690, "seek": 166142, "start": 1670.26, "end": 1672.16, "text": " around IP and artistic style.", "tokens": [50806, 926, 8671, 293, 17090, 3758, 13, 50901], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 691, "seek": 166142, "start": 1673.14, "end": 1675.66, "text": " But the template for this, if you're a builder,", "tokens": [50950, 583, 264, 12379, 337, 341, 11, 498, 291, 434, 257, 27377, 11, 51076], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 692, "seek": 166142, "start": 1675.66, "end": 1677.5800000000002, "text": " that we're seeing in the wild,", "tokens": [51076, 300, 321, 434, 2577, 294, 264, 4868, 11, 51172], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 693, "seek": 166142, "start": 1677.5800000000002, "end": 1678.8200000000002, "text": " is approximately the following.", "tokens": [51172, 307, 10447, 264, 3480, 13, 51234], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 694, "seek": 166142, "start": 1678.8200000000002, "end": 1681.54, "text": " And the thing I want to point out is domain knowledge here.", "tokens": [51234, 400, 264, 551, 286, 528, 281, 935, 484, 307, 9274, 3601, 510, 13, 51370], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 695, "seek": 166142, "start": 1681.54, "end": 1682.98, "text": " This is really the purpose of this slide,", "tokens": [51370, 639, 307, 534, 264, 4334, 295, 341, 4137, 11, 51442], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 696, "seek": 166142, "start": 1682.98, "end": 1686.76, "text": " is to touch on the importance of the domain knowledge.", "tokens": [51442, 307, 281, 2557, 322, 264, 7379, 295, 264, 9274, 3601, 13, 51631], "temperature": 0.0, "avg_logprob": -0.12335317642962346, "compression_ratio": 1.6559139784946237, "no_speech_prob": 0.0010983828688040376}, {"id": 697, "seek": 168676, "start": 1686.76, "end": 1690.76, "text": " So, many people approximately", "tokens": [50364, 407, 11, 867, 561, 10447, 50564], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 698, "seek": 168676, "start": 1690.76, "end": 1692.96, "text": " find the creative process as follows.", "tokens": [50564, 915, 264, 5880, 1399, 382, 10002, 13, 50674], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 699, "seek": 168676, "start": 1692.96, "end": 1694.22, "text": " Come up with a big idea.", "tokens": [50674, 2492, 493, 365, 257, 955, 1558, 13, 50737], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 700, "seek": 168676, "start": 1695.56, "end": 1698.4, "text": " Over-generate possibilities.", "tokens": [50804, 4886, 12, 21848, 473, 12178, 13, 50946], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 701, "seek": 168676, "start": 1698.4, "end": 1701.24, "text": " Edit down what you over-generated.", "tokens": [50946, 33241, 760, 437, 291, 670, 12, 21848, 770, 13, 51088], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 702, "seek": 168676, "start": 1701.24, "end": 1702.72, "text": " Repeat, right?", "tokens": [51088, 28523, 11, 558, 30, 51162], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 703, "seek": 168676, "start": 1702.72, "end": 1704.24, "text": " Like anybody who's been a writer", "tokens": [51162, 1743, 4472, 567, 311, 668, 257, 9936, 51238], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 704, "seek": 168676, "start": 1704.24, "end": 1706.48, "text": " knows when you write, you write way too much,", "tokens": [51238, 3255, 562, 291, 2464, 11, 291, 2464, 636, 886, 709, 11, 51350], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 705, "seek": 168676, "start": 1706.48, "end": 1708.24, "text": " and then you have to delete lots of it.", "tokens": [51350, 293, 550, 291, 362, 281, 12097, 3195, 295, 309, 13, 51438], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 706, "seek": 168676, "start": 1708.24, "end": 1709.96, "text": " And then you revise, and you write way too much,", "tokens": [51438, 400, 550, 291, 44252, 11, 293, 291, 2464, 636, 886, 709, 11, 51524], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 707, "seek": 168676, "start": 1709.96, "end": 1711.52, "text": " and you have to delete lots of it.", "tokens": [51524, 293, 291, 362, 281, 12097, 3195, 295, 309, 13, 51602], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 708, "seek": 168676, "start": 1711.52, "end": 1714.64, "text": " This particular task is fantastic for AI.", "tokens": [51602, 639, 1729, 5633, 307, 5456, 337, 7318, 13, 51758], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 709, "seek": 168676, "start": 1714.64, "end": 1716.68, "text": " One of the reasons it's fantastic for AI", "tokens": [51758, 1485, 295, 264, 4112, 309, 311, 5456, 337, 7318, 51860], "temperature": 0.0, "avg_logprob": -0.13347367084387576, "compression_ratio": 1.7782101167315174, "no_speech_prob": 9.914145630318671e-05}, {"id": 710, "seek": 171668, "start": 1716.68, "end": 1718.4, "text": " is because it allows the AI to be wrong.", "tokens": [50364, 307, 570, 309, 4045, 264, 7318, 281, 312, 2085, 13, 50450], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 711, "seek": 171668, "start": 1718.4, "end": 1719.72, "text": " You know, you've pre-agreed,", "tokens": [50450, 509, 458, 11, 291, 600, 659, 12, 559, 265, 292, 11, 50516], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 712, "seek": 171668, "start": 1719.72, "end": 1720.64, "text": " you're gonna delete lots of it.", "tokens": [50516, 291, 434, 799, 12097, 3195, 295, 309, 13, 50562], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 713, "seek": 171668, "start": 1720.64, "end": 1723.5600000000002, "text": " And so, if you pre-agreed, hey, I'm just gonna build,", "tokens": [50562, 400, 370, 11, 498, 291, 659, 12, 559, 265, 292, 11, 4177, 11, 286, 478, 445, 799, 1322, 11, 50708], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 714, "seek": 171668, "start": 1723.5600000000002, "end": 1726.24, "text": " generate five possibilities of the story I might tell.", "tokens": [50708, 8460, 1732, 12178, 295, 264, 1657, 286, 1062, 980, 13, 50842], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 715, "seek": 171668, "start": 1726.24, "end": 1728.52, "text": " Five possibilities of the advertising headline.", "tokens": [50842, 9436, 12178, 295, 264, 13097, 28380, 13, 50956], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 716, "seek": 171668, "start": 1728.52, "end": 1732.8, "text": " Five possibilities of what I might write my thesis on.", "tokens": [50956, 9436, 12178, 295, 437, 286, 1062, 2464, 452, 22288, 322, 13, 51170], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 717, "seek": 171668, "start": 1732.8, "end": 1734.44, "text": " You pre-agreed, it's okay if it's a little wrong,", "tokens": [51170, 509, 659, 12, 559, 265, 292, 11, 309, 311, 1392, 498, 309, 311, 257, 707, 2085, 11, 51252], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 718, "seek": 171668, "start": 1734.44, "end": 1736.96, "text": " because you are going to be the editor that steps in.", "tokens": [51252, 570, 291, 366, 516, 281, 312, 264, 9839, 300, 4439, 294, 13, 51378], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 719, "seek": 171668, "start": 1736.96, "end": 1738.8, "text": " And here's the thing that you really", "tokens": [51378, 400, 510, 311, 264, 551, 300, 291, 534, 51470], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 720, "seek": 171668, "start": 1738.8, "end": 1741.0, "text": " should bring to the table, is don't think about this", "tokens": [51470, 820, 1565, 281, 264, 3199, 11, 307, 500, 380, 519, 466, 341, 51580], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 721, "seek": 171668, "start": 1741.0, "end": 1742.0, "text": " as a technical activity.", "tokens": [51580, 382, 257, 6191, 5191, 13, 51630], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 722, "seek": 171668, "start": 1742.0, "end": 1744.3200000000002, "text": " Think about this as your opportunity", "tokens": [51630, 6557, 466, 341, 382, 428, 2650, 51746], "temperature": 0.0, "avg_logprob": -0.11751606406235113, "compression_ratio": 1.8778877887788779, "no_speech_prob": 0.00022337050177156925}, {"id": 723, "seek": 174432, "start": 1744.32, "end": 1746.84, "text": " not to put GPT in charge.", "tokens": [50364, 406, 281, 829, 26039, 51, 294, 4602, 13, 50490], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 724, "seek": 174432, "start": 1746.84, "end": 1750.08, "text": " Instead, for you to grasp the steering wheel tighter,", "tokens": [50490, 7156, 11, 337, 291, 281, 21743, 264, 14823, 5589, 30443, 11, 50652], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 725, "seek": 174432, "start": 1750.08, "end": 1752.52, "text": " I think, at least, in Python,", "tokens": [50652, 286, 519, 11, 412, 1935, 11, 294, 15329, 11, 50774], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 726, "seek": 174432, "start": 1752.52, "end": 1754.52, "text": " or the language you're using to program,", "tokens": [50774, 420, 264, 2856, 291, 434, 1228, 281, 1461, 11, 50874], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 727, "seek": 174432, "start": 1754.52, "end": 1756.6, "text": " because you have the domain knowledge", "tokens": [50874, 570, 291, 362, 264, 9274, 3601, 50978], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 728, "seek": 174432, "start": 1756.6, "end": 1759.08, "text": " to wield GPT in the generation of those.", "tokens": [50978, 281, 35982, 26039, 51, 294, 264, 5125, 295, 729, 13, 51102], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 729, "seek": 174432, "start": 1759.08, "end": 1761.24, "text": " So let me show you an example of what I mean by that.", "tokens": [51102, 407, 718, 385, 855, 291, 364, 1365, 295, 437, 286, 914, 538, 300, 13, 51210], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 730, "seek": 174432, "start": 1761.24, "end": 1766.24, "text": " So, this is a cool app that someone created", "tokens": [51210, 407, 11, 341, 307, 257, 1627, 724, 300, 1580, 2942, 51460], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 731, "seek": 174432, "start": 1766.6399999999999, "end": 1767.8, "text": " for the Writing Atlas project.", "tokens": [51480, 337, 264, 32774, 32485, 1716, 13, 51538], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 732, "seek": 174432, "start": 1767.8, "end": 1770.8, "text": " So Writing Atlas is a set of short stories,", "tokens": [51538, 407, 32774, 32485, 307, 257, 992, 295, 2099, 3676, 11, 51688], "temperature": 0.0, "avg_logprob": -0.12886240585990574, "compression_ratio": 1.6015936254980079, "no_speech_prob": 0.0008039354579523206}, {"id": 733, "seek": 177080, "start": 1771.8, "end": 1775.2, "text": " and you can think of it as good reads for short stories.", "tokens": [50414, 293, 291, 393, 519, 295, 309, 382, 665, 15700, 337, 2099, 3676, 13, 50584], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 734, "seek": 177080, "start": 1775.2, "end": 1778.0, "text": " So you can go in here, you can browse different stories,", "tokens": [50584, 407, 291, 393, 352, 294, 510, 11, 291, 393, 31442, 819, 3676, 11, 50724], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 735, "seek": 177080, "start": 1778.0, "end": 1779.48, "text": " and this was something somebody created", "tokens": [50724, 293, 341, 390, 746, 2618, 2942, 50798], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 736, "seek": 177080, "start": 1779.48, "end": 1782.76, "text": " where you can type in a story description that you like,", "tokens": [50798, 689, 291, 393, 2010, 294, 257, 1657, 3855, 300, 291, 411, 11, 50962], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 737, "seek": 177080, "start": 1782.76, "end": 1784.36, "text": " and this is gonna take about a minute to generate,", "tokens": [50962, 293, 341, 307, 799, 747, 466, 257, 3456, 281, 8460, 11, 51042], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 738, "seek": 177080, "start": 1784.36, "end": 1786.28, "text": " so I'm gonna talk while it's generating.", "tokens": [51042, 370, 286, 478, 799, 751, 1339, 309, 311, 17746, 13, 51138], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 739, "seek": 177080, "start": 1786.28, "end": 1791.28, "text": " And while it's working, what it's doing,", "tokens": [51138, 400, 1339, 309, 311, 1364, 11, 437, 309, 311, 884, 11, 51388], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 740, "seek": 177080, "start": 1791.44, "end": 1792.9199999999998, "text": " and I'll show you the code in a second,", "tokens": [51396, 293, 286, 603, 855, 291, 264, 3089, 294, 257, 1150, 11, 51470], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 741, "seek": 177080, "start": 1792.9199999999998, "end": 1795.3999999999999, "text": " is it's searching through the collection of stories", "tokens": [51470, 307, 309, 311, 10808, 807, 264, 5765, 295, 3676, 51594], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 742, "seek": 177080, "start": 1795.3999999999999, "end": 1796.8, "text": " for similar stories, and here's where", "tokens": [51594, 337, 2531, 3676, 11, 293, 510, 311, 689, 51664], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 743, "seek": 177080, "start": 1796.8, "end": 1798.52, "text": " the domain knowledge part comes in.", "tokens": [51664, 264, 9274, 3601, 644, 1487, 294, 13, 51750], "temperature": 0.0, "avg_logprob": -0.09511562629982277, "compression_ratio": 1.830935251798561, "no_speech_prob": 0.00029589893529191613}, {"id": 744, "seek": 179852, "start": 1798.6399999999999, "end": 1802.6, "text": " Then it uses GPT to look at what it was that you wanted,", "tokens": [50370, 1396, 309, 4960, 26039, 51, 281, 574, 412, 437, 309, 390, 300, 291, 1415, 11, 50568], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 745, "seek": 179852, "start": 1802.6, "end": 1804.84, "text": " and use knowledge of how an editor,", "tokens": [50568, 293, 764, 3601, 295, 577, 364, 9839, 11, 50680], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 746, "seek": 179852, "start": 1804.84, "end": 1807.16, "text": " how a bookseller thinks, to generate", "tokens": [50680, 577, 257, 1446, 405, 4658, 7309, 11, 281, 8460, 50796], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 747, "seek": 179852, "start": 1807.16, "end": 1810.2, "text": " a set of suggestions specifically through the lens", "tokens": [50796, 257, 992, 295, 13396, 4682, 807, 264, 6765, 50948], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 748, "seek": 179852, "start": 1810.2, "end": 1812.36, "text": " of that perspective with the goal of writing", "tokens": [50948, 295, 300, 4585, 365, 264, 3387, 295, 3579, 51056], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 749, "seek": 179852, "start": 1812.36, "end": 1813.96, "text": " that beautiful handwritten note", "tokens": [51056, 300, 2238, 1011, 26859, 3637, 51136], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 750, "seek": 179852, "start": 1813.96, "end": 1816.2, "text": " that we sometimes see in a local bookstore", "tokens": [51136, 300, 321, 2171, 536, 294, 257, 2654, 43478, 51248], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 751, "seek": 179852, "start": 1816.2, "end": 1819.48, "text": " tacked on underneath a book.", "tokens": [51248, 9426, 292, 322, 7223, 257, 1446, 13, 51412], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 752, "seek": 179852, "start": 1819.48, "end": 1821.84, "text": " And so it doesn't just say, hey, you might like this,", "tokens": [51412, 400, 370, 309, 1177, 380, 445, 584, 11, 4177, 11, 291, 1062, 411, 341, 11, 51530], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 753, "seek": 179852, "start": 1821.84, "end": 1824.98, "text": " here's a general purpose reason why you might like this,", "tokens": [51530, 510, 311, 257, 2674, 4334, 1778, 983, 291, 1062, 411, 341, 11, 51687], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 754, "seek": 179852, "start": 1824.98, "end": 1827.8, "text": " but specifically, here's why you might like this", "tokens": [51687, 457, 4682, 11, 510, 311, 983, 291, 1062, 411, 341, 51828], "temperature": 0.0, "avg_logprob": -0.0967457275982051, "compression_ratio": 1.778181818181818, "no_speech_prob": 0.0006666018161922693}, {"id": 755, "seek": 182780, "start": 1827.84, "end": 1829.6399999999999, "text": " with respect to what you gave it.", "tokens": [50366, 365, 3104, 281, 437, 291, 2729, 309, 13, 50456], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 756, "seek": 182780, "start": 1829.6399999999999, "end": 1832.68, "text": " It's either stalling out, or it's taking a long time.", "tokens": [50456, 467, 311, 2139, 19633, 278, 484, 11, 420, 309, 311, 1940, 257, 938, 565, 13, 50608], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 757, "seek": 182780, "start": 1832.68, "end": 1833.52, "text": " Oh, there we go.", "tokens": [50608, 876, 11, 456, 321, 352, 13, 50650], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 758, "seek": 182780, "start": 1834.6, "end": 1839.6, "text": " So here's its suggestions, and in particular, these things,", "tokens": [50704, 407, 510, 311, 1080, 13396, 11, 293, 294, 1729, 11, 613, 721, 11, 50954], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 759, "seek": 182780, "start": 1839.8, "end": 1841.56, "text": " these are things that only a human could know,", "tokens": [50964, 613, 366, 721, 300, 787, 257, 1952, 727, 458, 11, 51052], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 760, "seek": 182780, "start": 1841.56, "end": 1845.24, "text": " at least for now, two humans specifically,", "tokens": [51052, 412, 1935, 337, 586, 11, 732, 6255, 4682, 11, 51236], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 761, "seek": 182780, "start": 1845.24, "end": 1847.36, "text": " the human who said they wanted to read a story,", "tokens": [51236, 264, 1952, 567, 848, 436, 1415, 281, 1401, 257, 1657, 11, 51342], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 762, "seek": 182780, "start": 1847.36, "end": 1848.72, "text": " that's the text that came in,", "tokens": [51342, 300, 311, 264, 2487, 300, 1361, 294, 11, 51410], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 763, "seek": 182780, "start": 1848.72, "end": 1851.68, "text": " and then the human who added domain knowledge", "tokens": [51410, 293, 550, 264, 1952, 567, 3869, 9274, 3601, 51558], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 764, "seek": 182780, "start": 1851.68, "end": 1854.1599999999999, "text": " to script a sequence of interactions", "tokens": [51558, 281, 5755, 257, 8310, 295, 13280, 51682], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 765, "seek": 182780, "start": 1854.1599999999999, "end": 1856.76, "text": " with the language model so that you could provide", "tokens": [51682, 365, 264, 2856, 2316, 370, 300, 291, 727, 2893, 51812], "temperature": 0.0, "avg_logprob": -0.12078997844786156, "compression_ratio": 1.7032967032967032, "no_speech_prob": 0.00013979320647194982}, {"id": 766, "seek": 185676, "start": 1856.8, "end": 1859.44, "text": " very targeted reasoning over something", "tokens": [50366, 588, 15045, 21577, 670, 746, 50498], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 767, "seek": 185676, "start": 1859.44, "end": 1861.52, "text": " that was informed by that domain knowledge.", "tokens": [50498, 300, 390, 11740, 538, 300, 9274, 3601, 13, 50602], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 768, "seek": 185676, "start": 1861.52, "end": 1865.5, "text": " So for these utility apps, bring your domain knowledge.", "tokens": [50602, 407, 337, 613, 14877, 7733, 11, 1565, 428, 9274, 3601, 13, 50801], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 769, "seek": 185676, "start": 1869.64, "end": 1872.16, "text": " Let me actually show you how this looks and code,", "tokens": [51008, 961, 385, 767, 855, 291, 577, 341, 1542, 293, 3089, 11, 51134], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 770, "seek": 185676, "start": 1872.16, "end": 1874.72, "text": " because I think it's useful to see how simple", "tokens": [51134, 570, 286, 519, 309, 311, 4420, 281, 536, 577, 2199, 51262], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 771, "seek": 185676, "start": 1874.72, "end": 1875.94, "text": " and accessible this is.", "tokens": [51262, 293, 9515, 341, 307, 13, 51323], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 772, "seek": 185676, "start": 1875.94, "end": 1878.4, "text": " This is really a set of prompts.", "tokens": [51323, 639, 307, 534, 257, 992, 295, 41095, 13, 51446], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 773, "seek": 185676, "start": 1878.4, "end": 1882.24, "text": " So why might they, like a particular location,", "tokens": [51446, 407, 983, 1062, 436, 11, 411, 257, 1729, 4914, 11, 51638], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 774, "seek": 185676, "start": 1882.24, "end": 1883.64, "text": " well, here's the prompt that did that,", "tokens": [51638, 731, 11, 510, 311, 264, 12391, 300, 630, 300, 11, 51708], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 775, "seek": 185676, "start": 1883.64, "end": 1885.8799999999999, "text": " this is an open source project,", "tokens": [51708, 341, 307, 364, 1269, 4009, 1716, 11, 51820], "temperature": 0.0, "avg_logprob": -0.13549810926490855, "compression_ratio": 1.6491935483870968, "no_speech_prob": 0.00013550458243116736}, {"id": 776, "seek": 188588, "start": 1885.88, "end": 1887.5200000000002, "text": " and it has a bunch of examples,", "tokens": [50364, 293, 309, 575, 257, 3840, 295, 5110, 11, 50446], "temperature": 0.0, "avg_logprob": -0.12927838503304173, "compression_ratio": 1.6382113821138211, "no_speech_prob": 7.253880175994709e-05}, {"id": 777, "seek": 188588, "start": 1887.5200000000002, "end": 1891.0, "text": " and then it says, well, here's the one that we're interested in.", "tokens": [50446, 293, 550, 309, 1619, 11, 731, 11, 510, 311, 264, 472, 300, 321, 434, 3102, 294, 13, 50620], "temperature": 0.0, "avg_logprob": -0.12927838503304173, "compression_ratio": 1.6382113821138211, "no_speech_prob": 7.253880175994709e-05}, {"id": 778, "seek": 188588, "start": 1891.0, "end": 1892.88, "text": " Here's the audience, here's a couple of examples", "tokens": [50620, 1692, 311, 264, 4034, 11, 510, 311, 257, 1916, 295, 5110, 50714], "temperature": 0.0, "avg_logprob": -0.12927838503304173, "compression_ratio": 1.6382113821138211, "no_speech_prob": 7.253880175994709e-05}, {"id": 779, "seek": 188588, "start": 1892.88, "end": 1895.0800000000002, "text": " of why might people like a particular thing", "tokens": [50714, 295, 983, 1062, 561, 411, 257, 1729, 551, 50824], "temperature": 0.0, "avg_logprob": -0.12927838503304173, "compression_ratio": 1.6382113821138211, "no_speech_prob": 7.253880175994709e-05}, {"id": 780, "seek": 188588, "start": 1895.0800000000002, "end": 1897.4, "text": " in terms of audience, it's just another prompt.", "tokens": [50824, 294, 2115, 295, 4034, 11, 309, 311, 445, 1071, 12391, 13, 50940], "temperature": 0.0, "avg_logprob": -0.12927838503304173, "compression_ratio": 1.6382113821138211, "no_speech_prob": 7.253880175994709e-05}, {"id": 781, "seek": 188588, "start": 1902.3200000000002, "end": 1904.5200000000002, "text": " Same for topic, same for explanation,", "tokens": [51186, 10635, 337, 4829, 11, 912, 337, 10835, 11, 51296], "temperature": 0.0, "avg_logprob": -0.12927838503304173, "compression_ratio": 1.6382113821138211, "no_speech_prob": 7.253880175994709e-05}, {"id": 782, "seek": 188588, "start": 1904.5200000000002, "end": 1907.3200000000002, "text": " and if you go down here and look at how it was done,", "tokens": [51296, 293, 498, 291, 352, 760, 510, 293, 574, 412, 577, 309, 390, 1096, 11, 51436], "temperature": 0.0, "avg_logprob": -0.12927838503304173, "compression_ratio": 1.6382113821138211, "no_speech_prob": 7.253880175994709e-05}, {"id": 783, "seek": 188588, "start": 1909.3400000000001, "end": 1911.5600000000002, "text": " suggesting the story is, what is this,", "tokens": [51537, 18094, 264, 1657, 307, 11, 437, 307, 341, 11, 51648], "temperature": 0.0, "avg_logprob": -0.12927838503304173, "compression_ratio": 1.6382113821138211, "no_speech_prob": 7.253880175994709e-05}, {"id": 784, "seek": 188588, "start": 1911.5600000000002, "end": 1914.8400000000001, "text": " line 174 to line 203, it really is,", "tokens": [51648, 1622, 3282, 19, 281, 1622, 945, 18, 11, 309, 534, 307, 11, 51812], "temperature": 0.0, "avg_logprob": -0.12927838503304173, "compression_ratio": 1.6382113821138211, "no_speech_prob": 7.253880175994709e-05}, {"id": 785, "seek": 191484, "start": 1914.84, "end": 1916.32, "text": " and again, over and over again,", "tokens": [50364, 293, 797, 11, 670, 293, 670, 797, 11, 50438], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 786, "seek": 191484, "start": 1916.32, "end": 1919.1799999999998, "text": " I wanna impress upon you, this really is within reach.", "tokens": [50438, 286, 1948, 6729, 3564, 291, 11, 341, 534, 307, 1951, 2524, 13, 50581], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 787, "seek": 191484, "start": 1919.1799999999998, "end": 1923.72, "text": " It's really just what, 20 odd lines of step one,", "tokens": [50581, 467, 311, 534, 445, 437, 11, 945, 7401, 3876, 295, 1823, 472, 11, 50808], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 788, "seek": 191484, "start": 1923.72, "end": 1926.6, "text": " search in the database for similar stories,", "tokens": [50808, 3164, 294, 264, 8149, 337, 2531, 3676, 11, 50952], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 789, "seek": 191484, "start": 1926.6, "end": 1929.8799999999999, "text": " step two, given that I have similar stories,", "tokens": [50952, 1823, 732, 11, 2212, 300, 286, 362, 2531, 3676, 11, 51116], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 790, "seek": 191484, "start": 1929.8799999999999, "end": 1932.48, "text": " pull out the data, step three,", "tokens": [51116, 2235, 484, 264, 1412, 11, 1823, 1045, 11, 51246], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 791, "seek": 191484, "start": 1932.48, "end": 1935.1599999999999, "text": " with my domain knowledge in Python,", "tokens": [51246, 365, 452, 9274, 3601, 294, 15329, 11, 51380], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 792, "seek": 191484, "start": 1935.1599999999999, "end": 1937.6, "text": " now run these prompts, step four,", "tokens": [51380, 586, 1190, 613, 41095, 11, 1823, 1451, 11, 51502], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 793, "seek": 191484, "start": 1937.6, "end": 1939.02, "text": " prepare that into an output.", "tokens": [51502, 5940, 300, 666, 364, 5598, 13, 51573], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 794, "seek": 191484, "start": 1939.02, "end": 1941.4399999999998, "text": " So the thing we're scripting itself", "tokens": [51573, 407, 264, 551, 321, 434, 5755, 278, 2564, 51694], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 795, "seek": 191484, "start": 1941.4399999999998, "end": 1944.6799999999998, "text": " is some approximation of human cognition.", "tokens": [51694, 307, 512, 28023, 295, 1952, 46905, 13, 51856], "temperature": 0.0, "avg_logprob": -0.118381404075302, "compression_ratio": 1.6615384615384616, "no_speech_prob": 0.00037991887074895203}, {"id": 796, "seek": 194468, "start": 1944.68, "end": 1946.5600000000002, "text": " If you're willing to go there metaphorically,", "tokens": [50364, 759, 291, 434, 4950, 281, 352, 456, 19157, 984, 11, 50458], "temperature": 0.0, "avg_logprob": -0.20587509473164875, "compression_ratio": 1.579150579150579, "no_speech_prob": 0.00024532474344596267}, {"id": 797, "seek": 194468, "start": 1946.5600000000002, "end": 1948.72, "text": " we're not sure, I'm not gonna weigh in", "tokens": [50458, 321, 434, 406, 988, 11, 286, 478, 406, 799, 13843, 294, 50566], "temperature": 0.0, "avg_logprob": -0.20587509473164875, "compression_ratio": 1.579150579150579, "no_speech_prob": 0.00024532474344596267}, {"id": 798, "seek": 194468, "start": 1948.72, "end": 1953.72, "text": " on where we are on this open AI, a life form argument.", "tokens": [50566, 322, 689, 321, 366, 322, 341, 1269, 7318, 11, 257, 993, 1254, 6770, 13, 50816], "temperature": 0.0, "avg_logprob": -0.20587509473164875, "compression_ratio": 1.579150579150579, "no_speech_prob": 0.00024532474344596267}, {"id": 799, "seek": 194468, "start": 1956.16, "end": 1959.92, "text": " All right, one really far out there thing,", "tokens": [50938, 1057, 558, 11, 472, 534, 1400, 484, 456, 551, 11, 51126], "temperature": 0.0, "avg_logprob": -0.20587509473164875, "compression_ratio": 1.579150579150579, "no_speech_prob": 0.00024532474344596267}, {"id": 800, "seek": 194468, "start": 1959.92, "end": 1962.3600000000001, "text": " and then I'll tie it up for questions,", "tokens": [51126, 293, 550, 286, 603, 7582, 309, 493, 337, 1651, 11, 51248], "temperature": 0.0, "avg_logprob": -0.20587509473164875, "compression_ratio": 1.579150579150579, "no_speech_prob": 0.00024532474344596267}, {"id": 801, "seek": 194468, "start": 1962.3600000000001, "end": 1963.48, "text": " because I know there's probably a lot,", "tokens": [51248, 570, 286, 458, 456, 311, 1391, 257, 688, 11, 51304], "temperature": 0.0, "avg_logprob": -0.20587509473164875, "compression_ratio": 1.579150579150579, "no_speech_prob": 0.00024532474344596267}, {"id": 802, "seek": 194468, "start": 1963.48, "end": 1966.24, "text": " and I also wanna make sure you get great pizza", "tokens": [51304, 293, 286, 611, 1948, 652, 988, 291, 483, 869, 8298, 51442], "temperature": 0.0, "avg_logprob": -0.20587509473164875, "compression_ratio": 1.579150579150579, "no_speech_prob": 0.00024532474344596267}, {"id": 803, "seek": 194468, "start": 1966.24, "end": 1971.24, "text": " in your bellies, and that is a baby AGI auto GPT", "tokens": [51442, 294, 428, 4549, 530, 11, 293, 300, 307, 257, 3186, 316, 26252, 8399, 26039, 51, 51692], "temperature": 0.0, "avg_logprob": -0.20587509473164875, "compression_ratio": 1.579150579150579, "no_speech_prob": 0.00024532474344596267}, {"id": 804, "seek": 194468, "start": 1972.18, "end": 1973.92, "text": " is what you might have heard them called on Twitter.", "tokens": [51739, 307, 437, 291, 1062, 362, 2198, 552, 1219, 322, 5794, 13, 51826], "temperature": 0.0, "avg_logprob": -0.20587509473164875, "compression_ratio": 1.579150579150579, "no_speech_prob": 0.00024532474344596267}, {"id": 805, "seek": 197392, "start": 1973.92, "end": 1976.1200000000001, "text": " I think of them as multi-step planning bots.", "tokens": [50364, 286, 519, 295, 552, 382, 4825, 12, 16792, 5038, 35410, 13, 50474], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 806, "seek": 197392, "start": 1976.1200000000001, "end": 1978.3200000000002, "text": " So everything I showed you so far", "tokens": [50474, 407, 1203, 286, 4712, 291, 370, 1400, 50584], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 807, "seek": 197392, "start": 1978.3200000000002, "end": 1982.52, "text": " was approximately one shot interactions with GPT.", "tokens": [50584, 390, 10447, 472, 3347, 13280, 365, 26039, 51, 13, 50794], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 808, "seek": 197392, "start": 1982.52, "end": 1985.4, "text": " So this is, the user says they want something,", "tokens": [50794, 407, 341, 307, 11, 264, 4195, 1619, 436, 528, 746, 11, 50938], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 809, "seek": 197392, "start": 1985.4, "end": 1989.2, "text": " and then either Python mediates interactions with GPT,", "tokens": [50938, 293, 550, 2139, 15329, 17269, 1024, 13280, 365, 26039, 51, 11, 51128], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 810, "seek": 197392, "start": 1989.2, "end": 1992.64, "text": " or GPT itself does some things with the inflection", "tokens": [51128, 420, 26039, 51, 2564, 775, 512, 721, 365, 264, 1536, 5450, 51300], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 811, "seek": 197392, "start": 1992.64, "end": 1994.3200000000002, "text": " of a personality that you've added", "tokens": [51300, 295, 257, 9033, 300, 291, 600, 3869, 51384], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 812, "seek": 197392, "start": 1994.3200000000002, "end": 1996.4, "text": " from some prompt engineering.", "tokens": [51384, 490, 512, 12391, 7043, 13, 51488], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 813, "seek": 197392, "start": 1996.4, "end": 1999.48, "text": " Really useful, pretty easy to control.", "tokens": [51488, 4083, 4420, 11, 1238, 1858, 281, 1969, 13, 51642], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 814, "seek": 197392, "start": 1999.48, "end": 2000.8400000000001, "text": " If you wanna go to production,", "tokens": [51642, 759, 291, 1948, 352, 281, 4265, 11, 51710], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 815, "seek": 197392, "start": 2000.8400000000001, "end": 2002.2, "text": " if you wanna build a weekend project,", "tokens": [51710, 498, 291, 1948, 1322, 257, 6711, 1716, 11, 51778], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 816, "seek": 197392, "start": 2002.2, "end": 2003.04, "text": " if you wanna build a company,", "tokens": [51778, 498, 291, 1948, 1322, 257, 2237, 11, 51820], "temperature": 0.0, "avg_logprob": -0.07910969108343124, "compression_ratio": 1.7224199288256228, "no_speech_prob": 8.479546522721648e-05}, {"id": 817, "seek": 200304, "start": 2003.04, "end": 2005.1599999999999, "text": " that's a great way to do it right now.", "tokens": [50364, 300, 311, 257, 869, 636, 281, 360, 309, 558, 586, 13, 50470], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 818, "seek": 200304, "start": 2006.36, "end": 2009.96, "text": " This is wild, and if you haven't seen this stuff on Twitter,", "tokens": [50530, 639, 307, 4868, 11, 293, 498, 291, 2378, 380, 1612, 341, 1507, 322, 5794, 11, 50710], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 819, "seek": 200304, "start": 2009.96, "end": 2012.26, "text": " I would definitely recommend going to search for it.", "tokens": [50710, 286, 576, 2138, 2748, 516, 281, 3164, 337, 309, 13, 50825], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 820, "seek": 200304, "start": 2012.26, "end": 2015.5, "text": " This is what happens, the simple way to put it is,", "tokens": [50825, 639, 307, 437, 2314, 11, 264, 2199, 636, 281, 829, 309, 307, 11, 50987], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 821, "seek": 200304, "start": 2015.5, "end": 2017.92, "text": " if you put GPT in a for loop,", "tokens": [50987, 498, 291, 829, 26039, 51, 294, 257, 337, 6367, 11, 51108], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 822, "seek": 200304, "start": 2017.92, "end": 2019.68, "text": " if you let GPT talk to itself,", "tokens": [51108, 498, 291, 718, 26039, 51, 751, 281, 2564, 11, 51196], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 823, "seek": 200304, "start": 2019.68, "end": 2022.06, "text": " and then tell itself what to do.", "tokens": [51196, 293, 550, 980, 2564, 437, 281, 360, 13, 51315], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 824, "seek": 200304, "start": 2022.06, "end": 2026.56, "text": " So it's an emergent behavior,", "tokens": [51315, 407, 309, 311, 364, 4345, 6930, 5223, 11, 51540], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 825, "seek": 200304, "start": 2026.56, "end": 2028.04, "text": " and like all emergent behaviors,", "tokens": [51540, 293, 411, 439, 4345, 6930, 15501, 11, 51614], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 826, "seek": 200304, "start": 2028.04, "end": 2029.6, "text": " it starts with a few simple steps,", "tokens": [51614, 309, 3719, 365, 257, 1326, 2199, 4439, 11, 51692], "temperature": 0.0, "avg_logprob": -0.11685075838703754, "compression_ratio": 1.688034188034188, "no_speech_prob": 0.000245328206801787}, {"id": 827, "seek": 202960, "start": 2029.6, "end": 2033.52, "text": " the Conways game of life, many elements of reality,", "tokens": [50364, 264, 2656, 942, 1216, 295, 993, 11, 867, 4959, 295, 4103, 11, 50560], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 828, "seek": 202960, "start": 2033.52, "end": 2036.48, "text": " turn out to be math equations that fit on a t-shirt,", "tokens": [50560, 1261, 484, 281, 312, 5221, 11787, 300, 3318, 322, 257, 256, 12, 15313, 11, 50708], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 829, "seek": 202960, "start": 2036.48, "end": 2038.04, "text": " but then when you play them forward in time,", "tokens": [50708, 457, 550, 562, 291, 862, 552, 2128, 294, 565, 11, 50786], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 830, "seek": 202960, "start": 2038.04, "end": 2040.9599999999998, "text": " they generate DNA, they generate human life.", "tokens": [50786, 436, 8460, 8272, 11, 436, 8460, 1952, 993, 13, 50932], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 831, "seek": 202960, "start": 2040.9599999999998, "end": 2044.52, "text": " So this is approximately,", "tokens": [50932, 407, 341, 307, 10447, 11, 51110], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 832, "seek": 202960, "start": 2044.52, "end": 2047.34, "text": " step one, take a human objective,", "tokens": [51110, 1823, 472, 11, 747, 257, 1952, 10024, 11, 51251], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 833, "seek": 202960, "start": 2047.34, "end": 2050.56, "text": " step two, your first task is to write yourself", "tokens": [51251, 1823, 732, 11, 428, 700, 5633, 307, 281, 2464, 1803, 51412], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 834, "seek": 202960, "start": 2050.56, "end": 2054.2999999999997, "text": " a list of steps, and here's the critical part, repeat.", "tokens": [51412, 257, 1329, 295, 4439, 11, 293, 510, 311, 264, 4924, 644, 11, 7149, 13, 51599], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 835, "seek": 202960, "start": 2054.2999999999997, "end": 2056.0, "text": " Now do the list of steps.", "tokens": [51599, 823, 360, 264, 1329, 295, 4439, 13, 51684], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 836, "seek": 202960, "start": 2056.0, "end": 2058.58, "text": " Now you have to embody your agent", "tokens": [51684, 823, 291, 362, 281, 42575, 428, 9461, 51813], "temperature": 0.0, "avg_logprob": -0.13539001497171693, "compression_ratio": 1.6313725490196078, "no_speech_prob": 0.00016863824566826224}, {"id": 837, "seek": 205858, "start": 2058.58, "end": 2060.02, "text": " with the ability to do things.", "tokens": [50364, 365, 264, 3485, 281, 360, 721, 13, 50436], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 838, "seek": 205858, "start": 2060.02, "end": 2061.86, "text": " So it's really only limited to do what you give it", "tokens": [50436, 407, 309, 311, 534, 787, 5567, 281, 360, 437, 291, 976, 309, 50528], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 839, "seek": 205858, "start": 2061.86, "end": 2064.7, "text": " the tools to do, and what it has the skills to do.", "tokens": [50528, 264, 3873, 281, 360, 11, 293, 437, 309, 575, 264, 3942, 281, 360, 13, 50670], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 840, "seek": 205858, "start": 2064.7, "end": 2067.34, "text": " So obviously this is still very much", "tokens": [50670, 407, 2745, 341, 307, 920, 588, 709, 50802], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 841, "seek": 205858, "start": 2067.34, "end": 2069.2999999999997, "text": " a set of experiments that are running right now,", "tokens": [50802, 257, 992, 295, 12050, 300, 366, 2614, 558, 586, 11, 50900], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 842, "seek": 205858, "start": 2069.2999999999997, "end": 2071.34, "text": " and but it's something that we'll see unfold", "tokens": [50900, 293, 457, 309, 311, 746, 300, 321, 603, 536, 17980, 51002], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 843, "seek": 205858, "start": 2071.34, "end": 2073.18, "text": " over the coming years, and this is the scenario", "tokens": [51002, 670, 264, 1348, 924, 11, 293, 341, 307, 264, 9005, 51094], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 844, "seek": 205858, "start": 2073.18, "end": 2075.18, "text": " in which Python stops becoming so important", "tokens": [51094, 294, 597, 15329, 10094, 5617, 370, 1021, 51194], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 845, "seek": 205858, "start": 2075.18, "end": 2076.54, "text": " because we've given it the ability", "tokens": [51194, 570, 321, 600, 2212, 309, 264, 3485, 51262], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 846, "seek": 205858, "start": 2076.54, "end": 2079.42, "text": " to actually self-direct what it's doing,", "tokens": [51262, 281, 767, 2698, 12, 44868, 437, 309, 311, 884, 11, 51406], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 847, "seek": 205858, "start": 2079.42, "end": 2081.38, "text": " and then it finally gives you a result.", "tokens": [51406, 293, 550, 309, 2721, 2709, 291, 257, 1874, 13, 51504], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 848, "seek": 205858, "start": 2081.38, "end": 2083.38, "text": " And I wanna give you an example still of just, again,", "tokens": [51504, 400, 286, 1948, 976, 291, 364, 1365, 920, 295, 445, 11, 797, 11, 51604], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 849, "seek": 205858, "start": 2083.38, "end": 2085.88, "text": " impressing upon you how much of this is prompt engineering,", "tokens": [51604, 6729, 278, 3564, 291, 577, 709, 295, 341, 307, 12391, 7043, 11, 51729], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 850, "seek": 205858, "start": 2085.88, "end": 2088.14, "text": " which is wild, how little code this is.", "tokens": [51729, 597, 307, 4868, 11, 577, 707, 3089, 341, 307, 13, 51842], "temperature": 0.0, "avg_logprob": -0.12480153356279645, "compression_ratio": 1.8328445747800586, "no_speech_prob": 0.005056736059486866}, {"id": 851, "seek": 208814, "start": 2088.14, "end": 2092.24, "text": " Let me show you what BabyAGI looks like.", "tokens": [50364, 961, 385, 855, 291, 437, 9425, 32, 26252, 1542, 411, 13, 50569], "temperature": 0.0, "avg_logprob": -0.12715288798014324, "compression_ratio": 1.6942148760330578, "no_speech_prob": 9.313677583122626e-05}, {"id": 852, "seek": 208814, "start": 2093.7599999999998, "end": 2097.94, "text": " So here is a BabyAGI that you can connect to Telegram,", "tokens": [50645, 407, 510, 307, 257, 9425, 32, 26252, 300, 291, 393, 1745, 281, 14889, 1342, 11, 50854], "temperature": 0.0, "avg_logprob": -0.12715288798014324, "compression_ratio": 1.6942148760330578, "no_speech_prob": 9.313677583122626e-05}, {"id": 853, "seek": 208814, "start": 2101.2599999999998, "end": 2103.96, "text": " and this is an agent that has two tools.", "tokens": [51020, 293, 341, 307, 364, 9461, 300, 575, 732, 3873, 13, 51155], "temperature": 0.0, "avg_logprob": -0.12715288798014324, "compression_ratio": 1.6942148760330578, "no_speech_prob": 9.313677583122626e-05}, {"id": 854, "seek": 208814, "start": 2103.96, "end": 2105.5, "text": " So I haven't explained to you what an agent is,", "tokens": [51155, 407, 286, 2378, 380, 8825, 281, 291, 437, 364, 9461, 307, 11, 51232], "temperature": 0.0, "avg_logprob": -0.12715288798014324, "compression_ratio": 1.6942148760330578, "no_speech_prob": 9.313677583122626e-05}, {"id": 855, "seek": 208814, "start": 2105.5, "end": 2107.22, "text": " I haven't explained to you what tools are,", "tokens": [51232, 286, 2378, 380, 8825, 281, 291, 437, 3873, 366, 11, 51318], "temperature": 0.0, "avg_logprob": -0.12715288798014324, "compression_ratio": 1.6942148760330578, "no_speech_prob": 9.313677583122626e-05}, {"id": 856, "seek": 208814, "start": 2107.22, "end": 2108.9, "text": " I'll give you a quick one sentence description.", "tokens": [51318, 286, 603, 976, 291, 257, 1702, 472, 8174, 3855, 13, 51402], "temperature": 0.0, "avg_logprob": -0.12715288798014324, "compression_ratio": 1.6942148760330578, "no_speech_prob": 9.313677583122626e-05}, {"id": 857, "seek": 208814, "start": 2108.9, "end": 2112.18, "text": " An agent is just a word to mean GPT", "tokens": [51402, 1107, 9461, 307, 445, 257, 1349, 281, 914, 26039, 51, 51566], "temperature": 0.0, "avg_logprob": -0.12715288798014324, "compression_ratio": 1.6942148760330578, "no_speech_prob": 9.313677583122626e-05}, {"id": 858, "seek": 208814, "start": 2112.18, "end": 2114.7799999999997, "text": " plus some bigger body in which it's living.", "tokens": [51566, 1804, 512, 3801, 1772, 294, 597, 309, 311, 2647, 13, 51696], "temperature": 0.0, "avg_logprob": -0.12715288798014324, "compression_ratio": 1.6942148760330578, "no_speech_prob": 9.313677583122626e-05}, {"id": 859, "seek": 208814, "start": 2114.7799999999997, "end": 2117.1, "text": " Maybe that body has a personality, maybe it has tools,", "tokens": [51696, 2704, 300, 1772, 575, 257, 9033, 11, 1310, 309, 575, 3873, 11, 51812], "temperature": 0.0, "avg_logprob": -0.12715288798014324, "compression_ratio": 1.6942148760330578, "no_speech_prob": 9.313677583122626e-05}, {"id": 860, "seek": 211710, "start": 2117.14, "end": 2118.98, "text": " maybe it has Python mediating its experience", "tokens": [50366, 1310, 309, 575, 15329, 17269, 990, 1080, 1752, 50458], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 861, "seek": 211710, "start": 2118.98, "end": 2120.1, "text": " with other things.", "tokens": [50458, 365, 661, 721, 13, 50514], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 862, "seek": 211710, "start": 2120.1, "end": 2122.7599999999998, "text": " Tools are simply ways in which the agent", "tokens": [50514, 30302, 366, 2935, 2098, 294, 597, 264, 9461, 50647], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 863, "seek": 211710, "start": 2122.7599999999998, "end": 2124.02, "text": " can choose to do things.", "tokens": [50647, 393, 2826, 281, 360, 721, 13, 50710], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 864, "seek": 211710, "start": 2124.02, "end": 2126.62, "text": " Like imagine if GPT could say order a pizza,", "tokens": [50710, 1743, 3811, 498, 26039, 51, 727, 584, 1668, 257, 8298, 11, 50840], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 865, "seek": 211710, "start": 2126.62, "end": 2128.86, "text": " and instead of you seeing the text order a pizza,", "tokens": [50840, 293, 2602, 295, 291, 2577, 264, 2487, 1668, 257, 8298, 11, 50952], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 866, "seek": 211710, "start": 2128.86, "end": 2132.1, "text": " that caused a pizza to be ordered, that's a tool.", "tokens": [50952, 300, 7008, 257, 8298, 281, 312, 8866, 11, 300, 311, 257, 2290, 13, 51114], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 867, "seek": 211710, "start": 2132.1, "end": 2133.3399999999997, "text": " So these are two tools it has,", "tokens": [51114, 407, 613, 366, 732, 3873, 309, 575, 11, 51176], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 868, "seek": 211710, "start": 2133.3399999999997, "end": 2135.2799999999997, "text": " one tool is generated to-do list,", "tokens": [51176, 472, 2290, 307, 10833, 281, 12, 2595, 1329, 11, 51273], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 869, "seek": 211710, "start": 2135.2799999999997, "end": 2138.42, "text": " one tool is do a search on the web,", "tokens": [51273, 472, 2290, 307, 360, 257, 3164, 322, 264, 3670, 11, 51430], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 870, "seek": 211710, "start": 2142.36, "end": 2146.06, "text": " and then down here it has a prompt saying,", "tokens": [51627, 293, 550, 760, 510, 309, 575, 257, 12391, 1566, 11, 51812], "temperature": 0.0, "avg_logprob": -0.1667608980272637, "compression_ratio": 1.6653386454183268, "no_speech_prob": 0.00012337815132923424}, {"id": 871, "seek": 214606, "start": 2146.1, "end": 2148.54, "text": " hey, your goal is to build a task list", "tokens": [50366, 4177, 11, 428, 3387, 307, 281, 1322, 257, 5633, 1329, 50488], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 872, "seek": 214606, "start": 2148.54, "end": 2149.9, "text": " and then do that task list,", "tokens": [50488, 293, 550, 360, 300, 5633, 1329, 11, 50556], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 873, "seek": 214606, "start": 2149.9, "end": 2152.54, "text": " and then this is just placed into a harness", "tokens": [50556, 293, 550, 341, 307, 445, 7074, 666, 257, 19700, 50688], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 874, "seek": 214606, "start": 2152.54, "end": 2153.74, "text": " that does it over and over again.", "tokens": [50688, 300, 775, 309, 670, 293, 670, 797, 13, 50748], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 875, "seek": 214606, "start": 2153.74, "end": 2156.22, "text": " So after the next task, kind of unqueue the results", "tokens": [50748, 407, 934, 264, 958, 5633, 11, 733, 295, 517, 1077, 622, 264, 3542, 50872], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 876, "seek": 214606, "start": 2156.22, "end": 2158.82, "text": " of that task and keep it going.", "tokens": [50872, 295, 300, 5633, 293, 1066, 309, 516, 13, 51002], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 877, "seek": 214606, "start": 2158.82, "end": 2162.0, "text": " And so in doing that, you get this kickstarted loop", "tokens": [51002, 400, 370, 294, 884, 300, 11, 291, 483, 341, 4437, 24419, 292, 6367, 51161], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 878, "seek": 214606, "start": 2162.0, "end": 2164.14, "text": " where essentially you kickstart it,", "tokens": [51161, 689, 4476, 291, 4437, 24419, 309, 11, 51268], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 879, "seek": 214606, "start": 2164.14, "end": 2167.2999999999997, "text": " and then the agent is talking to itself.", "tokens": [51268, 293, 550, 264, 9461, 307, 1417, 281, 2564, 13, 51426], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 880, "seek": 214606, "start": 2167.2999999999997, "end": 2169.02, "text": " So this, unless I'm wrong,", "tokens": [51426, 407, 341, 11, 5969, 286, 478, 2085, 11, 51512], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 881, "seek": 214606, "start": 2169.02, "end": 2171.14, "text": " I don't think this has yet reached production", "tokens": [51512, 286, 500, 380, 519, 341, 575, 1939, 6488, 4265, 51618], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 882, "seek": 214606, "start": 2171.14, "end": 2172.7, "text": " in terms of what we're seeing in the field", "tokens": [51618, 294, 2115, 295, 437, 321, 434, 2577, 294, 264, 2519, 51696], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 883, "seek": 214606, "start": 2172.7, "end": 2174.52, "text": " of how people are deploying software,", "tokens": [51696, 295, 577, 561, 366, 34198, 4722, 11, 51787], "temperature": 0.0, "avg_logprob": -0.1407246916261438, "compression_ratio": 1.7620689655172415, "no_speech_prob": 0.005551595706492662}, {"id": 884, "seek": 217452, "start": 2174.52, "end": 2177.2, "text": " but if you wanna dive into sort of the wildest part", "tokens": [50364, 457, 498, 291, 1948, 9192, 666, 1333, 295, 264, 4868, 377, 644, 50498], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 885, "seek": 217452, "start": 2177.2, "end": 2178.84, "text": " of experimentation, this is definitely one", "tokens": [50498, 295, 37142, 11, 341, 307, 2138, 472, 50580], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 886, "seek": 217452, "start": 2178.84, "end": 2181.9, "text": " of the places you can start, and it's really within reach.", "tokens": [50580, 295, 264, 3190, 291, 393, 722, 11, 293, 309, 311, 534, 1951, 2524, 13, 50733], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 887, "seek": 217452, "start": 2181.9, "end": 2183.88, "text": " All you have to do is download one", "tokens": [50733, 1057, 291, 362, 281, 360, 307, 5484, 472, 50832], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 888, "seek": 217452, "start": 2183.88, "end": 2185.72, "text": " of the starter projects for it,", "tokens": [50832, 295, 264, 22465, 4455, 337, 309, 11, 50924], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 889, "seek": 217452, "start": 2185.72, "end": 2187.44, "text": " and you can kind of see right in the prompting,", "tokens": [50924, 293, 291, 393, 733, 295, 536, 558, 294, 264, 12391, 278, 11, 51010], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 890, "seek": 217452, "start": 2187.44, "end": 2191.12, "text": " here's how you kickstart that process of iteration.", "tokens": [51010, 510, 311, 577, 291, 4437, 24419, 300, 1399, 295, 24784, 13, 51194], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 891, "seek": 217452, "start": 2197.96, "end": 2200.2, "text": " All right, so I know that was super high level.", "tokens": [51536, 1057, 558, 11, 370, 286, 458, 300, 390, 1687, 1090, 1496, 13, 51648], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 892, "seek": 217452, "start": 2200.2, "end": 2202.0, "text": " I hope it was useful.", "tokens": [51648, 286, 1454, 309, 390, 4420, 13, 51738], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 893, "seek": 217452, "start": 2202.0, "end": 2204.04, "text": " It's, I think from the field, from the bottoms up,", "tokens": [51738, 467, 311, 11, 286, 519, 490, 264, 2519, 11, 490, 264, 43413, 493, 11, 51840], "temperature": 0.0, "avg_logprob": -0.13553639221191408, "compression_ratio": 1.72265625, "no_speech_prob": 7.030642154859379e-05}, {"id": 894, "seek": 220404, "start": 2204.04, "end": 2205.36, "text": " what we're seeing and what people are building,", "tokens": [50364, 437, 321, 434, 2577, 293, 437, 561, 366, 2390, 11, 50430], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 895, "seek": 220404, "start": 2205.36, "end": 2208.8, "text": " kind of the high level categories of apps", "tokens": [50430, 733, 295, 264, 1090, 1496, 10479, 295, 7733, 50602], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 896, "seek": 220404, "start": 2208.8, "end": 2210.12, "text": " that people are making.", "tokens": [50602, 300, 561, 366, 1455, 13, 50668], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 897, "seek": 220404, "start": 2210.12, "end": 2212.22, "text": " All of these apps are apps that are within reach", "tokens": [50668, 1057, 295, 613, 7733, 366, 7733, 300, 366, 1951, 2524, 50773], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 898, "seek": 220404, "start": 2212.22, "end": 2214.94, "text": " to everybody, which is really, really exciting.", "tokens": [50773, 281, 2201, 11, 597, 307, 534, 11, 534, 4670, 13, 50909], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 899, "seek": 220404, "start": 2214.94, "end": 2216.88, "text": " And there's, I suggest Twitter is a great place", "tokens": [50909, 400, 456, 311, 11, 286, 3402, 5794, 307, 257, 869, 1081, 51006], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 900, "seek": 220404, "start": 2216.88, "end": 2219.84, "text": " to hang out and build things.", "tokens": [51006, 281, 3967, 484, 293, 1322, 721, 13, 51154], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 901, "seek": 220404, "start": 2219.84, "end": 2222.84, "text": " There's a lot of AI builders on Twitter publishing.", "tokens": [51154, 821, 311, 257, 688, 295, 7318, 36281, 322, 5794, 17832, 13, 51304], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 902, "seek": 220404, "start": 2222.84, "end": 2224.32, "text": " And I think we've got a couple minutes", "tokens": [51304, 400, 286, 519, 321, 600, 658, 257, 1916, 2077, 51378], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 903, "seek": 220404, "start": 2224.32, "end": 2226.56, "text": " before pizza is arriving, maybe 10 minutes.", "tokens": [51378, 949, 8298, 307, 22436, 11, 1310, 1266, 2077, 13, 51490], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 904, "seek": 220404, "start": 2226.56, "end": 2227.92, "text": " Keep on going.", "tokens": [51490, 5527, 322, 516, 13, 51558], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 905, "seek": 220404, "start": 2227.92, "end": 2230.84, "text": " So if there's any questions, why don't we kick it to that?", "tokens": [51558, 407, 498, 456, 311, 604, 1651, 11, 983, 500, 380, 321, 4437, 309, 281, 300, 30, 51704], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 906, "seek": 220404, "start": 2230.84, "end": 2233.68, "text": " Because I'm sure there's some questions that you all have,", "tokens": [51704, 1436, 286, 478, 988, 456, 311, 512, 1651, 300, 291, 439, 362, 11, 51846], "temperature": 0.0, "avg_logprob": -0.14030121809599416, "compression_ratio": 1.7375, "no_speech_prob": 0.00043048965744674206}, {"id": 907, "seek": 223368, "start": 2233.72, "end": 2235.04, "text": " I guess I ended it a little early.", "tokens": [50366, 286, 2041, 286, 4590, 309, 257, 707, 2440, 13, 50432], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 908, "seek": 223368, "start": 2235.04, "end": 2235.8799999999997, "text": " Yes?", "tokens": [50432, 1079, 30, 50474], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 909, "seek": 223368, "start": 2235.8799999999997, "end": 2238.9199999999996, "text": " Yeah, so I have a question around hallucination.", "tokens": [50474, 865, 11, 370, 286, 362, 257, 1168, 926, 35212, 2486, 13, 50626], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 910, "seek": 223368, "start": 2238.9199999999996, "end": 2241.16, "text": " And so, you know, whenever building these sorts", "tokens": [50626, 400, 370, 11, 291, 458, 11, 5699, 2390, 613, 7527, 50738], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 911, "seek": 223368, "start": 2241.16, "end": 2243.96, "text": " of applications in apps, for example, let's say,", "tokens": [50738, 295, 5821, 294, 7733, 11, 337, 1365, 11, 718, 311, 584, 11, 50878], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 912, "seek": 223368, "start": 2245.44, "end": 2247.44, "text": " I'm giving it like a physics problem from a PSET", "tokens": [50952, 286, 478, 2902, 309, 411, 257, 10649, 1154, 490, 257, 8168, 4850, 51052], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 913, "seek": 223368, "start": 2247.44, "end": 2248.44, "text": " and we want to do that.", "tokens": [51052, 293, 321, 528, 281, 360, 300, 13, 51102], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 914, "seek": 223368, "start": 2248.44, "end": 2249.2799999999997, "text": " Yeah.", "tokens": [51102, 865, 13, 51144], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 915, "seek": 223368, "start": 2249.2799999999997, "end": 2253.04, "text": " And, you know, it's 40% of the time just raw.", "tokens": [51144, 400, 11, 291, 458, 11, 309, 311, 3356, 4, 295, 264, 565, 445, 8936, 13, 51332], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 916, "seek": 223368, "start": 2253.04, "end": 2253.8799999999997, "text": " Yeah.", "tokens": [51332, 865, 13, 51374], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 917, "seek": 223368, "start": 2253.8799999999997, "end": 2255.6, "text": " Do you have any like actionable recommendations", "tokens": [51374, 1144, 291, 362, 604, 411, 45098, 10434, 51460], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 918, "seek": 223368, "start": 2255.6, "end": 2257.44, "text": " that these developers should be doing", "tokens": [51460, 300, 613, 8849, 820, 312, 884, 51552], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 919, "seek": 223368, "start": 2257.44, "end": 2258.7599999999998, "text": " to make it hallucinate less?", "tokens": [51552, 281, 652, 309, 35212, 13923, 1570, 30, 51618], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 920, "seek": 223368, "start": 2258.7599999999998, "end": 2261.8799999999997, "text": " Or maybe even things that like open AI on the back end", "tokens": [51618, 1610, 1310, 754, 721, 300, 411, 1269, 7318, 322, 264, 646, 917, 51774], "temperature": 0.0, "avg_logprob": -0.3282854027218289, "compression_ratio": 1.609271523178808, "no_speech_prob": 0.0004033937584608793}, {"id": 921, "seek": 226188, "start": 2261.88, "end": 2263.6400000000003, "text": " should be doing to reduce hallucination.", "tokens": [50364, 820, 312, 884, 281, 5407, 35212, 2486, 13, 50452], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 922, "seek": 226188, "start": 2263.6400000000003, "end": 2266.48, "text": " So it would be something where you use RLHF.", "tokens": [50452, 407, 309, 576, 312, 746, 689, 291, 764, 497, 43, 39, 37, 13, 50594], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 923, "seek": 226188, "start": 2267.84, "end": 2269.2000000000003, "text": " Yeah, I didn't get the answer.", "tokens": [50662, 865, 11, 286, 994, 380, 483, 264, 1867, 13, 50730], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 924, "seek": 226188, "start": 2269.2000000000003, "end": 2271.8, "text": " So the question was how, approximately,", "tokens": [50730, 407, 264, 1168, 390, 577, 11, 10447, 11, 50860], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 925, "seek": 226188, "start": 2271.8, "end": 2273.7200000000003, "text": " how do you manage the hallucination problem?", "tokens": [50860, 577, 360, 291, 3067, 264, 35212, 2486, 1154, 30, 50956], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 926, "seek": 226188, "start": 2273.7200000000003, "end": 2276.08, "text": " Like if you give it a physics lecture", "tokens": [50956, 1743, 498, 291, 976, 309, 257, 10649, 7991, 51074], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 927, "seek": 226188, "start": 2276.08, "end": 2278.7200000000003, "text": " and you ask it a question, on the one hand,", "tokens": [51074, 293, 291, 1029, 309, 257, 1168, 11, 322, 264, 472, 1011, 11, 51206], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 928, "seek": 226188, "start": 2278.7200000000003, "end": 2280.7200000000003, "text": " it appears to be answering you correctly.", "tokens": [51206, 309, 7038, 281, 312, 13430, 291, 8944, 13, 51306], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 929, "seek": 226188, "start": 2280.7200000000003, "end": 2283.32, "text": " On the other hand, it appears to be wrong", "tokens": [51306, 1282, 264, 661, 1011, 11, 309, 7038, 281, 312, 2085, 51436], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 930, "seek": 226188, "start": 2283.32, "end": 2285.48, "text": " to an expert's eye 40% of the time,", "tokens": [51436, 281, 364, 5844, 311, 3313, 3356, 4, 295, 264, 565, 11, 51544], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 931, "seek": 226188, "start": 2285.48, "end": 2287.32, "text": " 70% of the time, 10% of the time.", "tokens": [51544, 5285, 4, 295, 264, 565, 11, 1266, 4, 295, 264, 565, 13, 51636], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 932, "seek": 226188, "start": 2287.32, "end": 2288.36, "text": " It's a huge problem.", "tokens": [51636, 467, 311, 257, 2603, 1154, 13, 51688], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 933, "seek": 226188, "start": 2288.36, "end": 2291.1600000000003, "text": " And then what are some ways as developers practically", "tokens": [51688, 400, 550, 437, 366, 512, 2098, 382, 8849, 15667, 51828], "temperature": 0.0, "avg_logprob": -0.21266560838711973, "compression_ratio": 1.7239057239057238, "no_speech_prob": 0.0003052976098842919}, {"id": 934, "seek": 229116, "start": 2291.16, "end": 2293.2, "text": " you can use to mitigate that?", "tokens": [50364, 291, 393, 764, 281, 27336, 300, 30, 50466], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 935, "seek": 229116, "start": 2293.2, "end": 2294.04, "text": " I'll give an answer.", "tokens": [50466, 286, 603, 976, 364, 1867, 13, 50508], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 936, "seek": 229116, "start": 2294.04, "end": 2295.3999999999996, "text": " So you may have some specific things too.", "tokens": [50508, 407, 291, 815, 362, 512, 2685, 721, 886, 13, 50576], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 937, "seek": 229116, "start": 2295.3999999999996, "end": 2297.0, "text": " So one high level answer is,", "tokens": [50576, 407, 472, 1090, 1496, 1867, 307, 11, 50656], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 938, "seek": 229116, "start": 2297.0, "end": 2298.44, "text": " the same thing that makes these things", "tokens": [50656, 264, 912, 551, 300, 1669, 613, 721, 50728], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 939, "seek": 229116, "start": 2298.44, "end": 2300.2, "text": " capable of synthesizing information", "tokens": [50728, 8189, 295, 26617, 3319, 1589, 50816], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 940, "seek": 229116, "start": 2300.2, "end": 2302.16, "text": " is part of the reason why it hallucinates for you.", "tokens": [50816, 307, 644, 295, 264, 1778, 983, 309, 35212, 259, 1024, 337, 291, 13, 50914], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 941, "seek": 229116, "start": 2302.16, "end": 2303.96, "text": " So it's hard to have your cake you need it to", "tokens": [50914, 407, 309, 311, 1152, 281, 362, 428, 5908, 291, 643, 309, 281, 51004], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 942, "seek": 229116, "start": 2303.96, "end": 2305.2799999999997, "text": " to a certain extent.", "tokens": [51004, 281, 257, 1629, 8396, 13, 51070], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 943, "seek": 229116, "start": 2305.2799999999997, "end": 2306.64, "text": " So this is part of the game.", "tokens": [51070, 407, 341, 307, 644, 295, 264, 1216, 13, 51138], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 944, "seek": 229116, "start": 2306.64, "end": 2308.16, "text": " In fact, humans do it too.", "tokens": [51138, 682, 1186, 11, 6255, 360, 309, 886, 13, 51214], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 945, "seek": 229116, "start": 2308.16, "end": 2309.92, "text": " Like people talk about, you know,", "tokens": [51214, 1743, 561, 751, 466, 11, 291, 458, 11, 51302], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 946, "seek": 229116, "start": 2309.92, "end": 2312.3999999999996, "text": " just folks who kind of are too aggressive", "tokens": [51302, 445, 4024, 567, 733, 295, 366, 886, 10762, 51426], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 947, "seek": 229116, "start": 2312.3999999999996, "end": 2313.68, "text": " in their assumptions about knowledge.", "tokens": [51426, 294, 641, 17695, 466, 3601, 13, 51490], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 948, "seek": 229116, "start": 2313.68, "end": 2315.68, "text": " I can't remember the name for that phenomenon", "tokens": [51490, 286, 393, 380, 1604, 264, 1315, 337, 300, 14029, 51590], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 949, "seek": 229116, "start": 2315.68, "end": 2316.72, "text": " where you'll just say stuff, right?", "tokens": [51590, 689, 291, 603, 445, 584, 1507, 11, 558, 30, 51642], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 950, "seek": 229116, "start": 2316.72, "end": 2317.6, "text": " So we do it too.", "tokens": [51642, 407, 321, 360, 309, 886, 13, 51686], "temperature": 0.0, "avg_logprob": -0.09710235144259662, "compression_ratio": 1.71976401179941, "no_speech_prob": 0.0006877064588479698}, {"id": 951, "seek": 231760, "start": 2318.56, "end": 2322.2, "text": " Some things you can do are kind of a range of activities", "tokens": [50412, 2188, 721, 291, 393, 360, 366, 733, 295, 257, 3613, 295, 5354, 50594], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 952, "seek": 231760, "start": 2322.2, "end": 2323.7599999999998, "text": " depending on how much money you really need to spend,", "tokens": [50594, 5413, 322, 577, 709, 1460, 291, 534, 643, 281, 3496, 11, 50672], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 953, "seek": 231760, "start": 2323.7599999999998, "end": 2324.92, "text": " how much technical expertise you have,", "tokens": [50672, 577, 709, 6191, 11769, 291, 362, 11, 50730], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 954, "seek": 231760, "start": 2324.92, "end": 2328.48, "text": " that can range from fine tuning a model to practically,", "tokens": [50730, 300, 393, 3613, 490, 2489, 15164, 257, 2316, 281, 15667, 11, 50908], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 955, "seek": 231760, "start": 2328.48, "end": 2329.7999999999997, "text": " so I'm in the applied world.", "tokens": [50908, 370, 286, 478, 294, 264, 6456, 1002, 13, 50974], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 956, "seek": 231760, "start": 2329.7999999999997, "end": 2331.68, "text": " So I'm very much in the world of duct tape", "tokens": [50974, 407, 286, 478, 588, 709, 294, 264, 1002, 295, 25954, 7314, 51068], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 957, "seek": 231760, "start": 2331.68, "end": 2333.2, "text": " and sort of how developers get stuff done.", "tokens": [51068, 293, 1333, 295, 577, 8849, 483, 1507, 1096, 13, 51144], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 958, "seek": 231760, "start": 2333.2, "end": 2334.4, "text": " So some of the answers I'll give you", "tokens": [51144, 407, 512, 295, 264, 6338, 286, 603, 976, 291, 51204], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 959, "seek": 231760, "start": 2334.4, "end": 2336.12, "text": " are sort of very duct-tapy answers.", "tokens": [51204, 366, 1333, 295, 588, 25954, 12, 83, 569, 88, 6338, 13, 51290], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 960, "seek": 231760, "start": 2336.12, "end": 2339.04, "text": " Giving it examples tends to work for acute things.", "tokens": [51290, 28983, 309, 5110, 12258, 281, 589, 337, 24390, 721, 13, 51436], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 961, "seek": 231760, "start": 2339.04, "end": 2340.64, "text": " If it's behaving in wild ways,", "tokens": [51436, 759, 309, 311, 35263, 294, 4868, 2098, 11, 51516], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 962, "seek": 231760, "start": 2340.64, "end": 2343.04, "text": " the more examples you give it, the better.", "tokens": [51516, 264, 544, 5110, 291, 976, 309, 11, 264, 1101, 13, 51636], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 963, "seek": 231760, "start": 2343.04, "end": 2346.0, "text": " That's not gonna solve the domain of all of physics.", "tokens": [51636, 663, 311, 406, 799, 5039, 264, 9274, 295, 439, 295, 10649, 13, 51784], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 964, "seek": 231760, "start": 2346.0, "end": 2347.36, "text": " So for the domain of all of physics,", "tokens": [51784, 407, 337, 264, 9274, 295, 439, 295, 10649, 11, 51852], "temperature": 0.0, "avg_logprob": -0.11505946658906482, "compression_ratio": 1.8258258258258258, "no_speech_prob": 0.00024533254327252507}, {"id": 965, "seek": 234736, "start": 2347.8, "end": 2348.92, "text": " I'm gonna bail and give it to you", "tokens": [50386, 286, 478, 799, 19313, 293, 976, 309, 281, 291, 50442], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 966, "seek": 234736, "start": 2348.92, "end": 2350.4, "text": " because I think you are far more equipped than me", "tokens": [50442, 570, 286, 519, 291, 366, 1400, 544, 15218, 813, 385, 50516], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 967, "seek": 234736, "start": 2350.4, "end": 2351.2400000000002, "text": " to speak on that.", "tokens": [50516, 281, 1710, 322, 300, 13, 50558], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 968, "seek": 234736, "start": 2351.2400000000002, "end": 2354.6800000000003, "text": " Sure, so the model doesn't have a ground truth.", "tokens": [50558, 4894, 11, 370, 264, 2316, 1177, 380, 362, 257, 2727, 3494, 13, 50730], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 969, "seek": 234736, "start": 2354.6800000000003, "end": 2356.08, "text": " It doesn't know anything.", "tokens": [50730, 467, 1177, 380, 458, 1340, 13, 50800], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 970, "seek": 234736, "start": 2356.08, "end": 2357.6, "text": " Any sense of meaning that it's derived", "tokens": [50800, 2639, 2020, 295, 3620, 300, 309, 311, 18949, 50876], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 971, "seek": 234736, "start": 2357.6, "end": 2361.96, "text": " from the training process is purely out of differentiation.", "tokens": [50876, 490, 264, 3097, 1399, 307, 17491, 484, 295, 38902, 13, 51094], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 972, "seek": 234736, "start": 2361.96, "end": 2363.36, "text": " One word is not another word.", "tokens": [51094, 1485, 1349, 307, 406, 1071, 1349, 13, 51164], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 973, "seek": 234736, "start": 2363.36, "end": 2366.1600000000003, "text": " Words are not used in the same context.", "tokens": [51164, 32857, 366, 406, 1143, 294, 264, 912, 4319, 13, 51304], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 974, "seek": 234736, "start": 2366.1600000000003, "end": 2369.0, "text": " It understands everything only through examples", "tokens": [51304, 467, 15146, 1203, 787, 807, 5110, 51446], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 975, "seek": 234736, "start": 2369.0, "end": 2369.84, "text": " given through language.", "tokens": [51446, 2212, 807, 2856, 13, 51488], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 976, "seek": 234736, "start": 2369.84, "end": 2372.6800000000003, "text": " It's like someone who learned English or how to speak,", "tokens": [51488, 467, 311, 411, 1580, 567, 3264, 3669, 420, 577, 281, 1710, 11, 51630], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 977, "seek": 234736, "start": 2372.6800000000003, "end": 2374.6800000000003, "text": " but they grew up in a featureless gray room.", "tokens": [51630, 457, 436, 6109, 493, 294, 257, 4111, 1832, 10855, 1808, 13, 51730], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 978, "seek": 234736, "start": 2374.6800000000003, "end": 2376.4, "text": " They've never seen the outside world.", "tokens": [51730, 814, 600, 1128, 1612, 264, 2380, 1002, 13, 51816], "temperature": 0.0, "avg_logprob": -0.11308315822056361, "compression_ratio": 1.6439169139465875, "no_speech_prob": 0.0001313046959694475}, {"id": 979, "seek": 237640, "start": 2376.4, "end": 2377.92, "text": " They have nothing to rest on that tells them", "tokens": [50364, 814, 362, 1825, 281, 1472, 322, 300, 5112, 552, 50440], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 980, "seek": 237640, "start": 2377.92, "end": 2380.84, "text": " that something is true and something is not true.", "tokens": [50440, 300, 746, 307, 2074, 293, 746, 307, 406, 2074, 13, 50586], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 981, "seek": 237640, "start": 2380.84, "end": 2382.48, "text": " So from the model's perspective,", "tokens": [50586, 407, 490, 264, 2316, 311, 4585, 11, 50668], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 982, "seek": 237640, "start": 2382.48, "end": 2383.88, "text": " everything that it says it's true.", "tokens": [50668, 1203, 300, 309, 1619, 309, 311, 2074, 13, 50738], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 983, "seek": 237640, "start": 2383.88, "end": 2386.56, "text": " It's trying its best to give you the best answer possible.", "tokens": [50738, 467, 311, 1382, 1080, 1151, 281, 976, 291, 264, 1151, 1867, 1944, 13, 50872], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 984, "seek": 237640, "start": 2386.56, "end": 2389.08, "text": " And if it lying a little bit", "tokens": [50872, 400, 498, 309, 8493, 257, 707, 857, 50998], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 985, "seek": 237640, "start": 2389.08, "end": 2390.84, "text": " or conflating two different topics", "tokens": [50998, 420, 1497, 75, 990, 732, 819, 8378, 51086], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 986, "seek": 237640, "start": 2390.84, "end": 2391.92, "text": " is the best way to achieve that,", "tokens": [51086, 307, 264, 1151, 636, 281, 4584, 300, 11, 51140], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 987, "seek": 237640, "start": 2391.92, "end": 2393.8, "text": " then it will decide to do so.", "tokens": [51140, 550, 309, 486, 4536, 281, 360, 370, 13, 51234], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 988, "seek": 237640, "start": 2393.8, "end": 2394.92, "text": " It's a part of the architecture.", "tokens": [51234, 467, 311, 257, 644, 295, 264, 9482, 13, 51290], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 989, "seek": 237640, "start": 2394.92, "end": 2396.32, "text": " We can't get around it.", "tokens": [51290, 492, 393, 380, 483, 926, 309, 13, 51360], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 990, "seek": 237640, "start": 2396.32, "end": 2398.56, "text": " There are a number of cheap tricks", "tokens": [51360, 821, 366, 257, 1230, 295, 7084, 11733, 51472], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 991, "seek": 237640, "start": 2398.56, "end": 2402.04, "text": " that surprisingly get it to confabulate or hallucinate less.", "tokens": [51472, 300, 17600, 483, 309, 281, 1497, 455, 5256, 420, 35212, 13923, 1570, 13, 51646], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 992, "seek": 237640, "start": 2402.04, "end": 2403.32, "text": " One of them includes recently,", "tokens": [51646, 1485, 295, 552, 5974, 3938, 11, 51710], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 993, "seek": 237640, "start": 2403.32, "end": 2405.04, "text": " there was a paper that's a little funny.", "tokens": [51710, 456, 390, 257, 3035, 300, 311, 257, 707, 4074, 13, 51796], "temperature": 0.0, "avg_logprob": -0.09690880179405212, "compression_ratio": 1.7522935779816513, "no_speech_prob": 1.777465149643831e-05}, {"id": 994, "seek": 240504, "start": 2405.04, "end": 2408.4, "text": " If you get it to prepend to its answer,", "tokens": [50364, 759, 291, 483, 309, 281, 2666, 521, 281, 1080, 1867, 11, 50532], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 995, "seek": 240504, "start": 2408.4, "end": 2411.52, "text": " my best guess is that will actually improve", "tokens": [50532, 452, 1151, 2041, 307, 300, 486, 767, 3470, 50688], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 996, "seek": 240504, "start": 2411.52, "end": 2414.44, "text": " or reduce hallucinations by about 80%.", "tokens": [50688, 420, 5407, 35212, 10325, 538, 466, 4688, 6856, 50834], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 997, "seek": 240504, "start": 2414.44, "end": 2415.88, "text": " So clearly it has some sense", "tokens": [50834, 407, 4448, 309, 575, 512, 2020, 50906], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 998, "seek": 240504, "start": 2415.88, "end": 2417.56, "text": " that some things are true and other things are not,", "tokens": [50906, 300, 512, 721, 366, 2074, 293, 661, 721, 366, 406, 11, 50990], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 999, "seek": 240504, "start": 2417.56, "end": 2419.24, "text": " but we're not quite sure what that is.", "tokens": [50990, 457, 321, 434, 406, 1596, 988, 437, 300, 307, 13, 51074], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 1000, "seek": 240504, "start": 2419.24, "end": 2420.68, "text": " To add on to what Ted was saying,", "tokens": [51074, 1407, 909, 322, 281, 437, 14985, 390, 1566, 11, 51146], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 1001, "seek": 240504, "start": 2420.68, "end": 2423.24, "text": " a few cheap things you can do include", "tokens": [51146, 257, 1326, 7084, 721, 291, 393, 360, 4090, 51274], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 1002, "seek": 240504, "start": 2423.24, "end": 2425.12, "text": " letting it Google or Bing,", "tokens": [51274, 8295, 309, 3329, 420, 30755, 11, 51368], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 1003, "seek": 240504, "start": 2425.12, "end": 2426.4, "text": " as in Bing Chat, what they're doing,", "tokens": [51368, 382, 294, 30755, 27503, 11, 437, 436, 434, 884, 11, 51432], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 1004, "seek": 240504, "start": 2426.4, "end": 2428.36, "text": " it cites this information,", "tokens": [51432, 309, 269, 3324, 341, 1589, 11, 51530], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 1005, "seek": 240504, "start": 2428.36, "end": 2431.44, "text": " asking it to make sure its own response is good.", "tokens": [51530, 3365, 309, 281, 652, 988, 1080, 1065, 4134, 307, 665, 13, 51684], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 1006, "seek": 240504, "start": 2431.44, "end": 2434.72, "text": " If you've ever had JetGBT generate a program,", "tokens": [51684, 759, 291, 600, 1562, 632, 28730, 8769, 51, 8460, 257, 1461, 11, 51848], "temperature": 0.0, "avg_logprob": -0.15434963173336452, "compression_ratio": 1.6339869281045751, "no_speech_prob": 0.00024135777493938804}, {"id": 1007, "seek": 243472, "start": 2434.72, "end": 2435.8399999999997, "text": " there's some kind of problem,", "tokens": [50364, 456, 311, 512, 733, 295, 1154, 11, 50420], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1008, "seek": 243472, "start": 2435.8399999999997, "end": 2438.4399999999996, "text": " and you ask ChatGBT, I think there's a mistake.", "tokens": [50420, 293, 291, 1029, 27503, 8769, 51, 11, 286, 519, 456, 311, 257, 6146, 13, 50550], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1009, "seek": 243472, "start": 2438.4399999999996, "end": 2440.9199999999996, "text": " Often it'll locate the mistake itself.", "tokens": [50550, 20043, 309, 603, 22370, 264, 6146, 2564, 13, 50674], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1010, "seek": 243472, "start": 2440.9199999999996, "end": 2443.7999999999997, "text": " Why didn't produce the right answer at the very beginning?", "tokens": [50674, 1545, 994, 380, 5258, 264, 558, 1867, 412, 264, 588, 2863, 30, 50818], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1011, "seek": 243472, "start": 2443.7999999999997, "end": 2444.7999999999997, "text": " We're still not sure,", "tokens": [50818, 492, 434, 920, 406, 988, 11, 50868], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1012, "seek": 243472, "start": 2444.7999999999997, "end": 2445.7999999999997, "text": " but we're moving in the direction", "tokens": [50868, 457, 321, 434, 2684, 294, 264, 3513, 50918], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1013, "seek": 243472, "start": 2445.7999999999997, "end": 2446.8399999999997, "text": " of reducing hallucinations.", "tokens": [50918, 295, 12245, 35212, 10325, 13, 50970], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1014, "seek": 243472, "start": 2446.8399999999997, "end": 2448.7999999999997, "text": " Now with respect to physics,", "tokens": [50970, 823, 365, 3104, 281, 10649, 11, 51068], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1015, "seek": 243472, "start": 2448.7999999999997, "end": 2451.22, "text": " you're gonna have to give it an external database", "tokens": [51068, 291, 434, 799, 362, 281, 976, 309, 364, 8320, 8149, 51189], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1016, "seek": 243472, "start": 2451.22, "end": 2453.8399999999997, "text": " to rest on because internally,", "tokens": [51189, 281, 1472, 322, 570, 19501, 11, 51320], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1017, "seek": 243472, "start": 2453.8399999999997, "end": 2456.9199999999996, "text": " for really, really domain specific knowledge,", "tokens": [51320, 337, 534, 11, 534, 9274, 2685, 3601, 11, 51474], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1018, "seek": 243472, "start": 2456.9199999999996, "end": 2461.9199999999996, "text": " it's not going to be as deterministic as one would like.", "tokens": [51474, 309, 311, 406, 516, 281, 312, 382, 15957, 3142, 382, 472, 576, 411, 13, 51724], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1019, "seek": 243472, "start": 2462.3199999999997, "end": 2464.12, "text": " These things work in continuous spaces.", "tokens": [51744, 1981, 721, 589, 294, 10957, 7673, 13, 51834], "temperature": 0.0, "avg_logprob": -0.12245133994282156, "compression_ratio": 1.5950155763239875, "no_speech_prob": 6.602717621717602e-05}, {"id": 1020, "seek": 246412, "start": 2464.12, "end": 2466.56, "text": " These things, they don't know what is wrong,", "tokens": [50364, 1981, 721, 11, 436, 500, 380, 458, 437, 307, 2085, 11, 50486], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1021, "seek": 246412, "start": 2466.56, "end": 2470.3599999999997, "text": " what is true, and as a result, we have to give it tools.", "tokens": [50486, 437, 307, 2074, 11, 293, 382, 257, 1874, 11, 321, 362, 281, 976, 309, 3873, 13, 50676], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1022, "seek": 246412, "start": 2470.3599999999997, "end": 2472.7599999999998, "text": " So everything that Ted demoed today is really", "tokens": [50676, 407, 1203, 300, 14985, 10723, 292, 965, 307, 534, 50796], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1023, "seek": 246412, "start": 2474.52, "end": 2476.7599999999998, "text": " striving at reducing hallucinations, actually, really,", "tokens": [50884, 36582, 412, 12245, 35212, 10325, 11, 767, 11, 534, 11, 50996], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1024, "seek": 246412, "start": 2476.7599999999998, "end": 2478.2, "text": " and giving it more abilities.", "tokens": [50996, 293, 2902, 309, 544, 11582, 13, 51068], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1025, "seek": 246412, "start": 2478.2, "end": 2480.56, "text": " I hope that answers your question.", "tokens": [51068, 286, 1454, 300, 6338, 428, 1168, 13, 51186], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1026, "seek": 246412, "start": 2480.56, "end": 2482.7599999999998, "text": " One of the ways to, I mean, I'm a simple guy.", "tokens": [51186, 1485, 295, 264, 2098, 281, 11, 286, 914, 11, 286, 478, 257, 2199, 2146, 13, 51296], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1027, "seek": 246412, "start": 2482.7599999999998, "end": 2485.4, "text": " Like I tend to think that all of the world", "tokens": [51296, 1743, 286, 3928, 281, 519, 300, 439, 295, 264, 1002, 51428], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1028, "seek": 246412, "start": 2485.4, "end": 2487.92, "text": " tends to be just a few things repeated over and over again,", "tokens": [51428, 12258, 281, 312, 445, 257, 1326, 721, 10477, 670, 293, 670, 797, 11, 51554], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1029, "seek": 246412, "start": 2487.92, "end": 2489.72, "text": " and we have human systems for this.", "tokens": [51554, 293, 321, 362, 1952, 3652, 337, 341, 13, 51644], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1030, "seek": 246412, "start": 2489.72, "end": 2491.6, "text": " You know, in a team, like companies work,", "tokens": [51644, 509, 458, 11, 294, 257, 1469, 11, 411, 3431, 589, 11, 51738], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1031, "seek": 246412, "start": 2491.6, "end": 2493.52, "text": " or a team playing sport,", "tokens": [51738, 420, 257, 1469, 2433, 7282, 11, 51834], "temperature": 0.0, "avg_logprob": -0.19064342653429187, "compression_ratio": 1.696078431372549, "no_speech_prob": 5.8283505495637655e-05}, {"id": 1032, "seek": 249352, "start": 2493.6, "end": 2494.72, "text": " and we're not right all the time,", "tokens": [50368, 293, 321, 434, 406, 558, 439, 264, 565, 11, 50424], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1033, "seek": 249352, "start": 2494.72, "end": 2495.96, "text": " even when we aspire to be,", "tokens": [50424, 754, 562, 321, 41224, 281, 312, 11, 50486], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1034, "seek": 249352, "start": 2495.96, "end": 2499.0, "text": " and so we have systems that we've developed as humans", "tokens": [50486, 293, 370, 321, 362, 3652, 300, 321, 600, 4743, 382, 6255, 50638], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1035, "seek": 249352, "start": 2499.0, "end": 2501.12, "text": " to deal with things that may be wrong.", "tokens": [50638, 281, 2028, 365, 721, 300, 815, 312, 2085, 13, 50744], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1036, "seek": 249352, "start": 2501.12, "end": 2503.88, "text": " So, you know, human number one proposes an answer,", "tokens": [50744, 407, 11, 291, 458, 11, 1952, 1230, 472, 2365, 4201, 364, 1867, 11, 50882], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1037, "seek": 249352, "start": 2503.88, "end": 2505.8, "text": " human number two checks their work,", "tokens": [50882, 1952, 1230, 732, 13834, 641, 589, 11, 50978], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1038, "seek": 249352, "start": 2505.8, "end": 2508.32, "text": " human number three provides the final sign off.", "tokens": [50978, 1952, 1230, 1045, 6417, 264, 2572, 1465, 766, 13, 51104], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1039, "seek": 249352, "start": 2508.32, "end": 2509.48, "text": " This is really common.", "tokens": [51104, 639, 307, 534, 2689, 13, 51162], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1040, "seek": 249352, "start": 2509.48, "end": 2510.64, "text": " Anybody who's worked in a company", "tokens": [51162, 19082, 567, 311, 2732, 294, 257, 2237, 51220], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1041, "seek": 249352, "start": 2510.64, "end": 2511.96, "text": " has seen this in practice.", "tokens": [51220, 575, 1612, 341, 294, 3124, 13, 51286], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1042, "seek": 249352, "start": 2511.96, "end": 2515.44, "text": " The interesting thing about the state of software right now,", "tokens": [51286, 440, 1880, 551, 466, 264, 1785, 295, 4722, 558, 586, 11, 51460], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1043, "seek": 249352, "start": 2515.44, "end": 2516.92, "text": " we tend to be in this mode,", "tokens": [51460, 321, 3928, 281, 312, 294, 341, 4391, 11, 51534], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1044, "seek": 249352, "start": 2516.92, "end": 2520.08, "text": " in which we're just talking to GPT as one entity.", "tokens": [51534, 294, 597, 321, 434, 445, 1417, 281, 26039, 51, 382, 472, 13977, 13, 51692], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1045, "seek": 249352, "start": 2520.08, "end": 2522.64, "text": " But once we start thinking in terms of teams,", "tokens": [51692, 583, 1564, 321, 722, 1953, 294, 2115, 295, 5491, 11, 51820], "temperature": 0.0, "avg_logprob": -0.10002625373102003, "compression_ratio": 1.7138461538461538, "no_speech_prob": 0.00017948696040548384}, {"id": 1046, "seek": 252264, "start": 2522.64, "end": 2525.6, "text": " so to speak, where each team member is its own agent", "tokens": [50364, 370, 281, 1710, 11, 689, 1184, 1469, 4006, 307, 1080, 1065, 9461, 50512], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1047, "seek": 252264, "start": 2525.6, "end": 2527.8799999999997, "text": " with its own set of objectives and skills,", "tokens": [50512, 365, 1080, 1065, 992, 295, 15961, 293, 3942, 11, 50626], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1048, "seek": 252264, "start": 2527.8799999999997, "end": 2530.68, "text": " I suspect we're going to start seeing a programming model", "tokens": [50626, 286, 9091, 321, 434, 516, 281, 722, 2577, 257, 9410, 2316, 50766], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1049, "seek": 252264, "start": 2530.68, "end": 2533.72, "text": " in which the way to solve this might not necessarily be,", "tokens": [50766, 294, 597, 264, 636, 281, 5039, 341, 1062, 406, 4725, 312, 11, 50918], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1050, "seek": 252264, "start": 2533.72, "end": 2535.7999999999997, "text": " make a single brain smarter,", "tokens": [50918, 652, 257, 2167, 3567, 20294, 11, 51022], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1051, "seek": 252264, "start": 2535.7999999999997, "end": 2539.04, "text": " but instead be draw upon the collective intelligence", "tokens": [51022, 457, 2602, 312, 2642, 3564, 264, 12590, 7599, 51184], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1052, "seek": 252264, "start": 2539.04, "end": 2542.08, "text": " of multiple software agents, each playing a role.", "tokens": [51184, 295, 3866, 4722, 12554, 11, 1184, 2433, 257, 3090, 13, 51336], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1053, "seek": 252264, "start": 2542.08, "end": 2544.6, "text": " And I think that that would certainly follow", "tokens": [51336, 400, 286, 519, 300, 300, 576, 3297, 1524, 51462], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1054, "seek": 252264, "start": 2544.6, "end": 2546.56, "text": " the human pattern of how we deal with this.", "tokens": [51462, 264, 1952, 5102, 295, 577, 321, 2028, 365, 341, 13, 51560], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1055, "seek": 252264, "start": 2546.56, "end": 2549.16, "text": " To give an analogy, space shuttles,", "tokens": [51560, 1407, 976, 364, 21663, 11, 1901, 5309, 23995, 11, 51690], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1056, "seek": 252264, "start": 2549.16, "end": 2551.72, "text": " things that go into space, spacecraft,", "tokens": [51690, 721, 300, 352, 666, 1901, 11, 22910, 11, 51818], "temperature": 0.0, "avg_logprob": -0.0869109837088998, "compression_ratio": 1.65359477124183, "no_speech_prob": 0.00012726896966341883}, {"id": 1057, "seek": 255172, "start": 2551.72, "end": 2553.2, "text": " they have to be good.", "tokens": [50364, 436, 362, 281, 312, 665, 13, 50438], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1058, "seek": 255172, "start": 2553.2, "end": 2554.48, "text": " If they're not good, people die.", "tokens": [50438, 759, 436, 434, 406, 665, 11, 561, 978, 13, 50502], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1059, "seek": 255172, "start": 2554.48, "end": 2558.0, "text": " They have no margin for error at all.", "tokens": [50502, 814, 362, 572, 10270, 337, 6713, 412, 439, 13, 50678], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1060, "seek": 255172, "start": 2558.0, "end": 2560.68, "text": " And as a result, we over engineer in those systems,", "tokens": [50678, 400, 382, 257, 1874, 11, 321, 670, 11403, 294, 729, 3652, 11, 50812], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1061, "seek": 255172, "start": 2560.68, "end": 2562.68, "text": " most spacecraft have three computers", "tokens": [50812, 881, 22910, 362, 1045, 10807, 50912], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1062, "seek": 255172, "start": 2562.68, "end": 2564.3599999999997, "text": " and they all have to agree in unison", "tokens": [50912, 293, 436, 439, 362, 281, 3986, 294, 517, 2770, 50996], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1063, "seek": 255172, "start": 2564.3599999999997, "end": 2566.7599999999998, "text": " on a particular step to go forward.", "tokens": [50996, 322, 257, 1729, 1823, 281, 352, 2128, 13, 51116], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1064, "seek": 255172, "start": 2566.7599999999998, "end": 2569.3199999999997, "text": " If one does not agree, then they recalculate,", "tokens": [51116, 759, 472, 775, 406, 3986, 11, 550, 436, 850, 304, 2444, 473, 11, 51244], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1065, "seek": 255172, "start": 2569.3199999999997, "end": 2570.72, "text": " they recalculate, they recalculate", "tokens": [51244, 436, 850, 304, 2444, 473, 11, 436, 850, 304, 2444, 473, 51314], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1066, "seek": 255172, "start": 2570.72, "end": 2572.12, "text": " until they arrive at something.", "tokens": [51314, 1826, 436, 8881, 412, 746, 13, 51384], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1067, "seek": 255172, "start": 2572.12, "end": 2574.24, "text": " The good thing is that hallucinations", "tokens": [51384, 440, 665, 551, 307, 300, 35212, 10325, 51490], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1068, "seek": 255172, "start": 2574.24, "end": 2575.72, "text": " are generally not a systemic problem", "tokens": [51490, 366, 5101, 406, 257, 23789, 1154, 51564], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1069, "seek": 255172, "start": 2575.72, "end": 2577.3599999999997, "text": " in terms of its knowledge.", "tokens": [51564, 294, 2115, 295, 1080, 3601, 13, 51646], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1070, "seek": 255172, "start": 2577.3599999999997, "end": 2580.2, "text": " It's often a one off, the model, something tripped it up", "tokens": [51646, 467, 311, 2049, 257, 472, 766, 11, 264, 2316, 11, 746, 1376, 3320, 309, 493, 51788], "temperature": 0.0, "avg_logprob": -0.11335927561709755, "compression_ratio": 1.8075601374570447, "no_speech_prob": 0.00010887640382861719}, {"id": 1071, "seek": 258020, "start": 2580.2, "end": 2582.8399999999997, "text": " and it just produced a hallucination in that one instance.", "tokens": [50364, 293, 309, 445, 7126, 257, 35212, 2486, 294, 300, 472, 5197, 13, 50496], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1072, "seek": 258020, "start": 2582.8399999999997, "end": 2584.6, "text": " So if there's three models working in unison,", "tokens": [50496, 407, 498, 456, 311, 1045, 5245, 1364, 294, 517, 2770, 11, 50584], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1073, "seek": 258020, "start": 2584.6, "end": 2587.8799999999997, "text": " just as Ted is saying, that will, generally speaking,", "tokens": [50584, 445, 382, 14985, 307, 1566, 11, 300, 486, 11, 5101, 4124, 11, 50748], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1074, "seek": 258020, "start": 2587.8799999999997, "end": 2589.24, "text": " improve your success.", "tokens": [50748, 3470, 428, 2245, 13, 50816], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1075, "seek": 258020, "start": 2590.7999999999997, "end": 2591.64, "text": " Yes, sir.", "tokens": [50894, 1079, 11, 4735, 13, 50936], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1076, "seek": 258020, "start": 2591.64, "end": 2593.9199999999996, "text": " A number of the examples you show have assertions", "tokens": [50936, 316, 1230, 295, 264, 5110, 291, 855, 362, 19810, 626, 51050], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1077, "seek": 258020, "start": 2593.9199999999996, "end": 2598.12, "text": " like you are an engineer, you are an AI, you are a teacher.", "tokens": [51050, 411, 291, 366, 364, 11403, 11, 291, 366, 364, 7318, 11, 291, 366, 257, 5027, 13, 51260], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1078, "seek": 258020, "start": 2598.12, "end": 2600.8399999999997, "text": " What's the mechanism by which that influences", "tokens": [51260, 708, 311, 264, 7513, 538, 597, 300, 21222, 51396], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1079, "seek": 258020, "start": 2600.8399999999997, "end": 2603.24, "text": " this computation of probabilities?", "tokens": [51396, 341, 24903, 295, 33783, 30, 51516], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1080, "seek": 258020, "start": 2603.24, "end": 2605.08, "text": " Sure, I'm gonna give you what might be", "tokens": [51516, 4894, 11, 286, 478, 799, 976, 291, 437, 1062, 312, 51608], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1081, "seek": 258020, "start": 2605.08, "end": 2608.2, "text": " an unsatisfying answer, which is it tends to work.", "tokens": [51608, 364, 2693, 25239, 1840, 1867, 11, 597, 307, 309, 12258, 281, 589, 13, 51764], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1082, "seek": 258020, "start": 2608.2, "end": 2610.16, "text": " But I think we know why it tends to work,", "tokens": [51764, 583, 286, 519, 321, 458, 983, 309, 12258, 281, 589, 11, 51862], "temperature": 0.0, "avg_logprob": -0.17910193053769394, "compression_ratio": 1.6601941747572815, "no_speech_prob": 0.00019706024613697082}, {"id": 1083, "seek": 261016, "start": 2610.16, "end": 2612.72, "text": " and again, it's because these language models approximate", "tokens": [50364, 293, 797, 11, 309, 311, 570, 613, 2856, 5245, 30874, 50492], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1084, "seek": 261016, "start": 2612.72, "end": 2614.16, "text": " how we talk to each other.", "tokens": [50492, 577, 321, 751, 281, 1184, 661, 13, 50564], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1085, "seek": 261016, "start": 2614.16, "end": 2616.56, "text": " So if I were to say to you, hey, help me out,", "tokens": [50564, 407, 498, 286, 645, 281, 584, 281, 291, 11, 4177, 11, 854, 385, 484, 11, 50684], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1086, "seek": 261016, "start": 2616.56, "end": 2619.0, "text": " I need you to mock interview me.", "tokens": [50684, 286, 643, 291, 281, 17362, 4049, 385, 13, 50806], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1087, "seek": 261016, "start": 2619.0, "end": 2620.44, "text": " That's a direct statement I can make", "tokens": [50806, 663, 311, 257, 2047, 5629, 286, 393, 652, 50878], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1088, "seek": 261016, "start": 2620.44, "end": 2622.68, "text": " that kicks you into a certain mode of interaction.", "tokens": [50878, 300, 21293, 291, 666, 257, 1629, 4391, 295, 9285, 13, 50990], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1089, "seek": 261016, "start": 2622.68, "end": 2624.8799999999997, "text": " Or if I say to you, help me out,", "tokens": [50990, 1610, 498, 286, 584, 281, 291, 11, 854, 385, 484, 11, 51100], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1090, "seek": 261016, "start": 2624.8799999999997, "end": 2626.8399999999997, "text": " I'm trying to apologize to my wife,", "tokens": [51100, 286, 478, 1382, 281, 12328, 281, 452, 3836, 11, 51198], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1091, "seek": 261016, "start": 2626.8399999999997, "end": 2629.2799999999997, "text": " she's really mad at me, can you role play with me?", "tokens": [51198, 750, 311, 534, 5244, 412, 385, 11, 393, 291, 3090, 862, 365, 385, 30, 51320], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1092, "seek": 261016, "start": 2629.2799999999997, "end": 2631.16, "text": " That kicks you into another mode of interaction.", "tokens": [51320, 663, 21293, 291, 666, 1071, 4391, 295, 9285, 13, 51414], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1093, "seek": 261016, "start": 2631.16, "end": 2634.3599999999997, "text": " And so it's really just a shorthand that people have found", "tokens": [51414, 400, 370, 309, 311, 534, 445, 257, 402, 2652, 474, 300, 561, 362, 1352, 51574], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1094, "seek": 261016, "start": 2634.3599999999997, "end": 2636.68, "text": " to kick the agent in, to kick the LLM in,", "tokens": [51574, 281, 4437, 264, 9461, 294, 11, 281, 4437, 264, 441, 43, 44, 294, 11, 51690], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1095, "seek": 261016, "start": 2636.68, "end": 2638.48, "text": " to a certain mode of interaction", "tokens": [51690, 281, 257, 1629, 4391, 295, 9285, 51780], "temperature": 0.0, "avg_logprob": -0.06744705972495986, "compression_ratio": 1.8528428093645486, "no_speech_prob": 0.0003051862877327949}, {"id": 1096, "seek": 263848, "start": 2638.48, "end": 2639.92, "text": " that it tends to work in the way", "tokens": [50364, 300, 309, 12258, 281, 589, 294, 264, 636, 50436], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1097, "seek": 263848, "start": 2639.92, "end": 2643.56, "text": " that I, as a software developer, am hoping it would work.", "tokens": [50436, 300, 286, 11, 382, 257, 4722, 10754, 11, 669, 7159, 309, 576, 589, 13, 50618], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1098, "seek": 263848, "start": 2643.56, "end": 2646.4, "text": " And to really quickly add on to that,", "tokens": [50618, 400, 281, 534, 2661, 909, 322, 281, 300, 11, 50760], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1099, "seek": 263848, "start": 2646.4, "end": 2649.08, "text": " being in the digital humanities that I am,", "tokens": [50760, 885, 294, 264, 4562, 36140, 300, 286, 669, 11, 50894], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1100, "seek": 263848, "start": 2649.08, "end": 2650.52, "text": " I like to think of it as a narrative.", "tokens": [50894, 286, 411, 281, 519, 295, 309, 382, 257, 9977, 13, 50966], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1101, "seek": 263848, "start": 2650.52, "end": 2652.36, "text": " A narrative will have a few different characters", "tokens": [50966, 316, 9977, 486, 362, 257, 1326, 819, 4342, 51058], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1102, "seek": 263848, "start": 2652.36, "end": 2655.28, "text": " talking to each other, their roles are clearly defined,", "tokens": [51058, 1417, 281, 1184, 661, 11, 641, 9604, 366, 4448, 7642, 11, 51204], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1103, "seek": 263848, "start": 2655.28, "end": 2656.76, "text": " two people are not the same.", "tokens": [51204, 732, 561, 366, 406, 264, 912, 13, 51278], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1104, "seek": 263848, "start": 2657.64, "end": 2660.36, "text": " This interaction with GPT, it assumes a personality,", "tokens": [51322, 639, 9285, 365, 26039, 51, 11, 309, 37808, 257, 9033, 11, 51458], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1105, "seek": 263848, "start": 2660.36, "end": 2662.0, "text": " it can simulate personalities.", "tokens": [51458, 309, 393, 27817, 25308, 13, 51540], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1106, "seek": 263848, "start": 2662.0, "end": 2663.88, "text": " It itself is not conscious in any way,", "tokens": [51540, 467, 2564, 307, 406, 6648, 294, 604, 636, 11, 51634], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1107, "seek": 263848, "start": 2663.88, "end": 2667.72, "text": " but it can certainly predict what a conscious being", "tokens": [51634, 457, 309, 393, 3297, 6069, 437, 257, 6648, 885, 51826], "temperature": 0.0, "avg_logprob": -0.11707406184252571, "compression_ratio": 1.7039473684210527, "no_speech_prob": 4.263582013663836e-05}, {"id": 1108, "seek": 266772, "start": 2667.72, "end": 2669.6, "text": " would react like in a particular situation.", "tokens": [50364, 576, 4515, 411, 294, 257, 1729, 2590, 13, 50458], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1109, "seek": 266772, "start": 2669.6, "end": 2674.2, "text": " So when we're going URX, it is drawing up that personality", "tokens": [50458, 407, 562, 321, 434, 516, 624, 49, 55, 11, 309, 307, 6316, 493, 300, 9033, 50688], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1110, "seek": 266772, "start": 2674.2, "end": 2676.04, "text": " and talking as though it is that person.", "tokens": [50688, 293, 1417, 382, 1673, 309, 307, 300, 954, 13, 50780], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1111, "seek": 266772, "start": 2676.04, "end": 2678.3199999999997, "text": " Because it is like completing a transcript", "tokens": [50780, 1436, 309, 307, 411, 19472, 257, 24444, 50894], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1112, "seek": 266772, "start": 2678.3199999999997, "end": 2681.6, "text": " or completing a story in which that character is present", "tokens": [50894, 420, 19472, 257, 1657, 294, 597, 300, 2517, 307, 1974, 51058], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1113, "seek": 266772, "start": 2681.6, "end": 2684.52, "text": " and interacting and is active.", "tokens": [51058, 293, 18017, 293, 307, 4967, 13, 51204], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1114, "seek": 266772, "start": 2684.52, "end": 2686.2799999999997, "text": " So, yeah.", "tokens": [51204, 407, 11, 1338, 13, 51292], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1115, "seek": 266772, "start": 2686.2799999999997, "end": 2689.12, "text": " I think we got about five minutes until the pizza outside.", "tokens": [51292, 286, 519, 321, 658, 466, 1732, 2077, 1826, 264, 8298, 2380, 13, 51434], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1116, "seek": 266772, "start": 2689.12, "end": 2689.9599999999996, "text": " Eight minutes.", "tokens": [51434, 17708, 2077, 13, 51476], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1117, "seek": 266772, "start": 2693.12, "end": 2693.9599999999996, "text": " Yes, sir.", "tokens": [51634, 1079, 11, 4735, 13, 51676], "temperature": 0.0, "avg_logprob": -0.18850090026855468, "compression_ratio": 1.6140350877192982, "no_speech_prob": 0.0006657480844296515}, {"id": 1118, "seek": 269396, "start": 2694.96, "end": 2699.7200000000003, "text": " So I'm not a CF person, but it's been a fun thing with this.", "tokens": [50414, 407, 286, 478, 406, 257, 21792, 954, 11, 457, 309, 311, 668, 257, 1019, 551, 365, 341, 13, 50652], "temperature": 0.0, "avg_logprob": -0.21189723580570544, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.0029697930440306664}, {"id": 1119, "seek": 269396, "start": 2699.7200000000003, "end": 2703.08, "text": " And I understand the sort of word-by-word generation", "tokens": [50652, 400, 286, 1223, 264, 1333, 295, 1349, 12, 2322, 12, 7462, 5125, 50820], "temperature": 0.0, "avg_logprob": -0.21189723580570544, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.0029697930440306664}, {"id": 1120, "seek": 269396, "start": 2703.08, "end": 2706.44, "text": " and the sort of vibe, the feeling of it in the narrative.", "tokens": [50820, 293, 264, 1333, 295, 14606, 11, 264, 2633, 295, 309, 294, 264, 9977, 13, 50988], "temperature": 0.0, "avg_logprob": -0.21189723580570544, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.0029697930440306664}, {"id": 1121, "seek": 269396, "start": 2707.4, "end": 2711.08, "text": " Some of my friends and I have tried giving it logic problems,", "tokens": [51036, 2188, 295, 452, 1855, 293, 286, 362, 3031, 2902, 309, 9952, 2740, 11, 51220], "temperature": 0.0, "avg_logprob": -0.21189723580570544, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.0029697930440306664}, {"id": 1122, "seek": 269396, "start": 2711.08, "end": 2713.2, "text": " like things from the LSAT, for example,", "tokens": [51220, 411, 721, 490, 264, 36657, 2218, 11, 337, 1365, 11, 51326], "temperature": 0.0, "avg_logprob": -0.21189723580570544, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.0029697930440306664}, {"id": 1123, "seek": 269396, "start": 2713.2, "end": 2714.84, "text": " and it doesn't work.", "tokens": [51326, 293, 309, 1177, 380, 589, 13, 51408], "temperature": 0.0, "avg_logprob": -0.21189723580570544, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.0029697930440306664}, {"id": 1124, "seek": 269396, "start": 2714.84, "end": 2716.92, "text": " Like, and I'm just wondering why that would be.", "tokens": [51408, 1743, 11, 293, 286, 478, 445, 6359, 983, 300, 576, 312, 13, 51512], "temperature": 0.0, "avg_logprob": -0.21189723580570544, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.0029697930440306664}, {"id": 1125, "seek": 269396, "start": 2716.92, "end": 2719.96, "text": " So it will generate answers that sound", "tokens": [51512, 407, 309, 486, 8460, 6338, 300, 1626, 51664], "temperature": 0.0, "avg_logprob": -0.21189723580570544, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.0029697930440306664}, {"id": 1126, "seek": 269396, "start": 2719.96, "end": 2721.6, "text": " very plausible rhetorically.", "tokens": [51664, 588, 39925, 24182, 284, 984, 13, 51746], "temperature": 0.0, "avg_logprob": -0.21189723580570544, "compression_ratio": 1.583011583011583, "no_speech_prob": 0.0029697930440306664}, {"id": 1127, "seek": 272160, "start": 2721.6, "end": 2724.52, "text": " Like, given this condition X, given this Y,", "tokens": [50364, 1743, 11, 2212, 341, 4188, 1783, 11, 2212, 341, 398, 11, 50510], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1128, "seek": 272160, "start": 2724.52, "end": 2727.6, "text": " but it'll often, like, even contradict itself", "tokens": [50510, 457, 309, 603, 2049, 11, 411, 11, 754, 28900, 2564, 50664], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1129, "seek": 272160, "start": 2727.6, "end": 2730.8399999999997, "text": " in its answers, but it's almost never correct.", "tokens": [50664, 294, 1080, 6338, 11, 457, 309, 311, 1920, 1128, 3006, 13, 50826], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1130, "seek": 272160, "start": 2730.8399999999997, "end": 2733.7999999999997, "text": " So I was wondering what, why that would be?", "tokens": [50826, 407, 286, 390, 6359, 437, 11, 983, 300, 576, 312, 30, 50974], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1131, "seek": 272160, "start": 2733.7999999999997, "end": 2737.04, "text": " Like, it just can't reason, it can't, like, think.", "tokens": [50974, 1743, 11, 309, 445, 393, 380, 1778, 11, 309, 393, 380, 11, 411, 11, 519, 13, 51136], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1132, "seek": 272160, "start": 2737.04, "end": 2741.08, "text": " And, like, can you, would we get to a place where it can,", "tokens": [51136, 400, 11, 411, 11, 393, 291, 11, 576, 321, 483, 281, 257, 1081, 689, 309, 393, 11, 51338], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1133, "seek": 272160, "start": 2741.08, "end": 2741.92, "text": " so to speak?", "tokens": [51338, 370, 281, 1710, 30, 51380], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1134, "seek": 272160, "start": 2741.92, "end": 2743.12, "text": " I mean, not, you know what I mean?", "tokens": [51380, 286, 914, 11, 406, 11, 291, 458, 437, 286, 914, 30, 51440], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1135, "seek": 272160, "start": 2743.12, "end": 2744.72, "text": " I don't mean to think, like, it's conscious.", "tokens": [51440, 286, 500, 380, 914, 281, 519, 11, 411, 11, 309, 311, 6648, 13, 51520], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1136, "seek": 272160, "start": 2744.72, "end": 2746.64, "text": " I mean, like, have thoughts, not-", "tokens": [51520, 286, 914, 11, 411, 11, 362, 4598, 11, 406, 12, 51616], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1137, "seek": 272160, "start": 2746.64, "end": 2748.7999999999997, "text": " You want to talk about react?", "tokens": [51616, 509, 528, 281, 751, 466, 4515, 30, 51724], "temperature": 0.0, "avg_logprob": -0.22604633199757543, "compression_ratio": 1.7286821705426356, "no_speech_prob": 0.0018957267748191953}, {"id": 1138, "seek": 274880, "start": 2748.8, "end": 2753.1600000000003, "text": " So GPT-4, when GPT-4 released back in March,", "tokens": [50364, 407, 26039, 51, 12, 19, 11, 562, 26039, 51, 12, 19, 4736, 646, 294, 6129, 11, 50582], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1139, "seek": 274880, "start": 2753.1600000000003, "end": 2755.6800000000003, "text": " I think it was, it was passing LSAT.", "tokens": [50582, 286, 519, 309, 390, 11, 309, 390, 8437, 36657, 2218, 13, 50708], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1140, "seek": 274880, "start": 2755.6800000000003, "end": 2756.52, "text": " It was.", "tokens": [50708, 467, 390, 13, 50750], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1141, "seek": 274880, "start": 2756.52, "end": 2757.36, "text": " It was, yeah.", "tokens": [50750, 467, 390, 11, 1338, 13, 50792], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1142, "seek": 274880, "start": 2757.36, "end": 2758.2000000000003, "text": " Yes.", "tokens": [50792, 1079, 13, 50834], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1143, "seek": 274880, "start": 2758.2000000000003, "end": 2760.32, "text": " Yes, it just passed, as I understand it.", "tokens": [50834, 1079, 11, 309, 445, 4678, 11, 382, 286, 1223, 309, 13, 50940], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1144, "seek": 274880, "start": 2760.32, "end": 2762.5600000000004, "text": " Well, maybe it's because we're not GPT.", "tokens": [50940, 1042, 11, 1310, 309, 311, 570, 321, 434, 406, 26039, 51, 13, 51052], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1145, "seek": 274880, "start": 2762.5600000000004, "end": 2763.84, "text": " That's one of the weird things.", "tokens": [51052, 663, 311, 472, 295, 264, 3657, 721, 13, 51116], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1146, "seek": 274880, "start": 2763.84, "end": 2764.6800000000003, "text": " Is that-", "tokens": [51116, 1119, 300, 12, 51158], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1147, "seek": 274880, "start": 2764.6800000000003, "end": 2765.52, "text": " At GPT.", "tokens": [51158, 1711, 26039, 51, 13, 51200], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1148, "seek": 274880, "start": 2765.52, "end": 2766.84, "text": " Yeah.", "tokens": [51200, 865, 13, 51266], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1149, "seek": 274880, "start": 2766.84, "end": 2768.6400000000003, "text": " If you pay for chat GPT, they give you access", "tokens": [51266, 759, 291, 1689, 337, 5081, 26039, 51, 11, 436, 976, 291, 2105, 51356], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1150, "seek": 274880, "start": 2768.6400000000003, "end": 2769.84, "text": " to the better model.", "tokens": [51356, 281, 264, 1101, 2316, 13, 51416], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1151, "seek": 274880, "start": 2769.84, "end": 2773.6400000000003, "text": " And one of the interesting things with it is prompting.", "tokens": [51416, 400, 472, 295, 264, 1880, 721, 365, 309, 307, 12391, 278, 13, 51606], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1152, "seek": 274880, "start": 2773.6400000000003, "end": 2774.76, "text": " It's so finicky.", "tokens": [51606, 467, 311, 370, 962, 20539, 13, 51662], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1153, "seek": 274880, "start": 2774.76, "end": 2777.88, "text": " If you, it's very sensitive to the way that you prompt.", "tokens": [51662, 759, 291, 11, 309, 311, 588, 9477, 281, 264, 636, 300, 291, 12391, 13, 51818], "temperature": 0.0, "avg_logprob": -0.2509113311767578, "compression_ratio": 1.6296296296296295, "no_speech_prob": 0.0006066972273401916}, {"id": 1154, "seek": 277788, "start": 2777.88, "end": 2780.48, "text": " There were earlier on when GPT-3 came out,", "tokens": [50364, 821, 645, 3071, 322, 562, 26039, 51, 12, 18, 1361, 484, 11, 50494], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1155, "seek": 277788, "start": 2780.48, "end": 2781.56, "text": " some people were going,", "tokens": [50494, 512, 561, 645, 516, 11, 50548], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1156, "seek": 277788, "start": 2781.56, "end": 2783.04, "text": " look, I can pass literacy tests,", "tokens": [50548, 574, 11, 286, 393, 1320, 23166, 6921, 11, 50622], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1157, "seek": 277788, "start": 2783.04, "end": 2785.2400000000002, "text": " or no, it can't pass literacy tests.", "tokens": [50622, 420, 572, 11, 309, 393, 380, 1320, 23166, 6921, 13, 50732], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1158, "seek": 277788, "start": 2785.2400000000002, "end": 2788.1600000000003, "text": " And then people who are pro or anti-GPT would be like,", "tokens": [50732, 400, 550, 561, 567, 366, 447, 420, 6061, 12, 38, 47, 51, 576, 312, 411, 11, 50878], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1159, "seek": 277788, "start": 2788.1600000000003, "end": 2789.48, "text": " I modified the prompt a little bit,", "tokens": [50878, 286, 15873, 264, 12391, 257, 707, 857, 11, 50944], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1160, "seek": 277788, "start": 2789.48, "end": 2791.4, "text": " suddenly it can't, or suddenly it can't.", "tokens": [50944, 5800, 309, 393, 380, 11, 420, 5800, 309, 393, 380, 13, 51040], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1161, "seek": 277788, "start": 2791.4, "end": 2793.6, "text": " These things are not conscious.", "tokens": [51040, 1981, 721, 366, 406, 6648, 13, 51150], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1162, "seek": 277788, "start": 2793.6, "end": 2796.0, "text": " Their ability to reason is like an alien's.", "tokens": [51150, 6710, 3485, 281, 1778, 307, 411, 364, 12319, 311, 13, 51270], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1163, "seek": 277788, "start": 2796.0, "end": 2796.84, "text": " They're not us.", "tokens": [51270, 814, 434, 406, 505, 13, 51312], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1164, "seek": 277788, "start": 2796.84, "end": 2797.8, "text": " They don't think like people.", "tokens": [51312, 814, 500, 380, 519, 411, 561, 13, 51360], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1165, "seek": 277788, "start": 2797.8, "end": 2798.96, "text": " They're not human.", "tokens": [51360, 814, 434, 406, 1952, 13, 51418], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1166, "seek": 277788, "start": 2798.96, "end": 2802.36, "text": " But they certainly are capable of passing some things", "tokens": [51418, 583, 436, 3297, 366, 8189, 295, 8437, 512, 721, 51588], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1167, "seek": 277788, "start": 2802.36, "end": 2804.6800000000003, "text": " empirically, which demonstrates some sort of", "tokens": [51588, 25790, 984, 11, 597, 31034, 512, 1333, 295, 51704], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1168, "seek": 277788, "start": 2804.6800000000003, "end": 2806.6800000000003, "text": " rationale or logic within the model.", "tokens": [51704, 41989, 420, 9952, 1951, 264, 2316, 13, 51804], "temperature": 0.0, "avg_logprob": -0.12935670816673422, "compression_ratio": 1.792763157894737, "no_speech_prob": 9.31112517719157e-05}, {"id": 1169, "seek": 280668, "start": 2806.68, "end": 2808.56, "text": " But we're still slowly figuring out,", "tokens": [50364, 583, 321, 434, 920, 5692, 15213, 484, 11, 50458], "temperature": 0.0, "avg_logprob": -0.3959026336669922, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00011952448403462768}, {"id": 1170, "seek": 280668, "start": 2808.56, "end": 2809.8799999999997, "text": " like a prompt whisperer,", "tokens": [50458, 411, 257, 12391, 26018, 260, 11, 50524], "temperature": 0.0, "avg_logprob": -0.3959026336669922, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00011952448403462768}, {"id": 1171, "seek": 280668, "start": 2809.8799999999997, "end": 2811.56, "text": " what exactly the right approach is.", "tokens": [50524, 437, 2293, 264, 558, 3109, 307, 13, 50608], "temperature": 0.0, "avg_logprob": -0.3959026336669922, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00011952448403462768}, {"id": 1172, "seek": 280668, "start": 2814.12, "end": 2814.96, "text": " Yeah?", "tokens": [50736, 865, 30, 50778], "temperature": 0.0, "avg_logprob": -0.3959026336669922, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00011952448403462768}, {"id": 1173, "seek": 280668, "start": 2816.16, "end": 2821.2, "text": " Obviously, having GPT-3 running and prompting it", "tokens": [50838, 7580, 11, 1419, 26039, 51, 12, 18, 2614, 293, 12391, 278, 309, 51090], "temperature": 0.0, "avg_logprob": -0.3959026336669922, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00011952448403462768}, {"id": 1174, "seek": 280668, "start": 2821.2, "end": 2824.64, "text": " continuously is very expensive in terms of the user.", "tokens": [51090, 15684, 307, 588, 5124, 294, 2115, 295, 264, 4195, 13, 51262], "temperature": 0.0, "avg_logprob": -0.3959026336669922, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00011952448403462768}, {"id": 1175, "seek": 280668, "start": 2824.64, "end": 2828.7599999999998, "text": " Have you seen instances where it directly creates", "tokens": [51262, 3560, 291, 1612, 14519, 689, 309, 3838, 7829, 51468], "temperature": 0.0, "avg_logprob": -0.3959026336669922, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00011952448403462768}, {"id": 1176, "seek": 280668, "start": 2828.7599999999998, "end": 2831.8799999999997, "text": " some sort of business value in for a whisperer,", "tokens": [51468, 512, 1333, 295, 1606, 2158, 294, 337, 257, 26018, 260, 11, 51624], "temperature": 0.0, "avg_logprob": -0.3959026336669922, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00011952448403462768}, {"id": 1177, "seek": 280668, "start": 2831.8799999999997, "end": 2835.48, "text": " for a company with a real added value of having", "tokens": [51624, 337, 257, 2237, 365, 257, 957, 3869, 2158, 295, 1419, 51804], "temperature": 0.0, "avg_logprob": -0.3959026336669922, "compression_ratio": 1.5260869565217392, "no_speech_prob": 0.00011952448403462768}, {"id": 1178, "seek": 283548, "start": 2835.48, "end": 2839.48, "text": " for these real AI apps in terms of", "tokens": [50364, 337, 613, 957, 7318, 7733, 294, 2115, 295, 50564], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1179, "seek": 283548, "start": 2839.48, "end": 2842.6, "text": " like a review of the drives and the actual digital stuff?", "tokens": [50564, 411, 257, 3131, 295, 264, 11754, 293, 264, 3539, 4562, 1507, 30, 50720], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1180, "seek": 283548, "start": 2842.6, "end": 2844.96, "text": " Yeah, I mean, we host companies on top of us,", "tokens": [50720, 865, 11, 286, 914, 11, 321, 3975, 3431, 322, 1192, 295, 505, 11, 50838], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1181, "seek": 283548, "start": 2844.96, "end": 2847.48, "text": " who that's their primary product.", "tokens": [50838, 567, 300, 311, 641, 6194, 1674, 13, 50964], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1182, "seek": 283548, "start": 2847.48, "end": 2851.48, "text": " The value that it adds is like any company.", "tokens": [50964, 440, 2158, 300, 309, 10860, 307, 411, 604, 2237, 13, 51164], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1183, "seek": 283548, "start": 2851.48, "end": 2853.6, "text": " I mean, it's, you know, what is the Y Combinator motto,", "tokens": [51164, 286, 914, 11, 309, 311, 11, 291, 458, 11, 437, 307, 264, 398, 2432, 13496, 1639, 32680, 11, 51270], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1184, "seek": 283548, "start": 2853.6, "end": 2854.64, "text": " make something people want?", "tokens": [51270, 652, 746, 561, 528, 30, 51322], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1185, "seek": 283548, "start": 2854.64, "end": 2857.84, "text": " I mean, I wouldn't think of this as GPT inherently", "tokens": [51322, 286, 914, 11, 286, 2759, 380, 519, 295, 341, 382, 26039, 51, 27993, 51482], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1186, "seek": 283548, "start": 2857.84, "end": 2860.04, "text": " provides value for you as a builder.", "tokens": [51482, 6417, 2158, 337, 291, 382, 257, 27377, 13, 51592], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1187, "seek": 283548, "start": 2860.04, "end": 2861.12, "text": " Like that's their product.", "tokens": [51592, 1743, 300, 311, 641, 1674, 13, 51646], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1188, "seek": 283548, "start": 2861.12, "end": 2862.16, "text": " That's OpenAI's product.", "tokens": [51646, 663, 311, 7238, 48698, 311, 1674, 13, 51698], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1189, "seek": 283548, "start": 2862.16, "end": 2865.08, "text": " You pay chat GPT for prioritized access.", "tokens": [51698, 509, 1689, 5081, 26039, 51, 337, 14846, 1602, 2105, 13, 51844], "temperature": 0.0, "avg_logprob": -0.2751824172226699, "compression_ratio": 1.6360544217687074, "no_speech_prob": 0.0002611279778648168}, {"id": 1190, "seek": 286508, "start": 2865.08, "end": 2867.88, "text": " Where your product might be is how you take that", "tokens": [50364, 2305, 428, 1674, 1062, 312, 307, 577, 291, 747, 300, 50504], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1191, "seek": 286508, "start": 2867.88, "end": 2870.7599999999998, "text": " and combine it with your data, somebody else's data,", "tokens": [50504, 293, 10432, 309, 365, 428, 1412, 11, 2618, 1646, 311, 1412, 11, 50648], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1192, "seek": 286508, "start": 2870.7599999999998, "end": 2873.48, "text": " some domain knowledge, some interface", "tokens": [50648, 512, 9274, 3601, 11, 512, 9226, 50784], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1193, "seek": 286508, "start": 2873.48, "end": 2876.3199999999997, "text": " that then helps apply it to something.", "tokens": [50784, 300, 550, 3665, 3079, 309, 281, 746, 13, 50926], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1194, "seek": 286508, "start": 2876.3199999999997, "end": 2878.0, "text": " It is, two things are both true.", "tokens": [50926, 467, 307, 11, 732, 721, 366, 1293, 2074, 13, 51010], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1195, "seek": 286508, "start": 2878.0, "end": 2880.96, "text": " There are a lot of experiments going on right now,", "tokens": [51010, 821, 366, 257, 688, 295, 12050, 516, 322, 558, 586, 11, 51158], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1196, "seek": 286508, "start": 2880.96, "end": 2883.64, "text": " both for fun and people trying to figure out", "tokens": [51158, 1293, 337, 1019, 293, 561, 1382, 281, 2573, 484, 51292], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1197, "seek": 286508, "start": 2883.64, "end": 2885.24, "text": " where the economic value is.", "tokens": [51292, 689, 264, 4836, 2158, 307, 13, 51372], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1198, "seek": 286508, "start": 2885.24, "end": 2887.16, "text": " But folks are also spinning up companies", "tokens": [51372, 583, 4024, 366, 611, 15640, 493, 3431, 51468], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1199, "seek": 286508, "start": 2887.16, "end": 2890.0, "text": " that are 100% supported by applying this to data.", "tokens": [51468, 300, 366, 2319, 4, 8104, 538, 9275, 341, 281, 1412, 13, 51610], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1200, "seek": 286508, "start": 2890.0, "end": 2893.7599999999998, "text": " Okay, first, a company that wouldn't have,", "tokens": [51610, 1033, 11, 700, 11, 257, 2237, 300, 2759, 380, 362, 11, 51798], "temperature": 0.0, "avg_logprob": -0.1492401861375378, "compression_ratio": 1.6262975778546713, "no_speech_prob": 0.00017393143207300454}, {"id": 1201, "seek": 289376, "start": 2893.76, "end": 2897.5200000000004, "text": " sort of, wouldn't be AI focused as an input, right?", "tokens": [50364, 1333, 295, 11, 2759, 380, 312, 7318, 5178, 382, 364, 4846, 11, 558, 30, 50552], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1202, "seek": 289376, "start": 2897.5200000000004, "end": 2899.6800000000003, "text": " As it's just using or developing, like,", "tokens": [50552, 1018, 309, 311, 445, 1228, 420, 6416, 11, 411, 11, 50660], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1203, "seek": 289376, "start": 2899.6800000000003, "end": 2903.6800000000003, "text": " announcements that use GPT for productivity.", "tokens": [50660, 23785, 300, 764, 26039, 51, 337, 15604, 13, 50860], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1204, "seek": 289376, "start": 2905.48, "end": 2910.0, "text": " I think that it is likely that today we call this GPT", "tokens": [50950, 286, 519, 300, 309, 307, 3700, 300, 965, 321, 818, 341, 26039, 51, 51176], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1205, "seek": 289376, "start": 2910.0, "end": 2911.36, "text": " and today we call these LLMs", "tokens": [51176, 293, 965, 321, 818, 613, 441, 43, 26386, 51244], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1206, "seek": 289376, "start": 2911.36, "end": 2913.6400000000003, "text": " and tomorrow it will just slide into the ether.", "tokens": [51244, 293, 4153, 309, 486, 445, 4137, 666, 264, 37096, 13, 51358], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1207, "seek": 289376, "start": 2913.6400000000003, "end": 2916.0800000000004, "text": " I mean, imagine what the progression is going to be.", "tokens": [51358, 286, 914, 11, 3811, 437, 264, 18733, 307, 516, 281, 312, 13, 51480], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1208, "seek": 289376, "start": 2916.0800000000004, "end": 2917.6800000000003, "text": " Today there's one of these", "tokens": [51480, 2692, 456, 311, 472, 295, 613, 51560], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1209, "seek": 289376, "start": 2917.6800000000003, "end": 2919.36, "text": " that people are primarily playing with.", "tokens": [51560, 300, 561, 366, 10029, 2433, 365, 13, 51644], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1210, "seek": 289376, "start": 2919.36, "end": 2920.6000000000004, "text": " There's many of them that exist,", "tokens": [51644, 821, 311, 867, 295, 552, 300, 2514, 11, 51706], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1211, "seek": 289376, "start": 2920.6000000000004, "end": 2922.6000000000004, "text": " but one, people are primarily bidding on top.", "tokens": [51706, 457, 472, 11, 561, 366, 10029, 39702, 322, 1192, 13, 51806], "temperature": 0.0, "avg_logprob": -0.2761975801908053, "compression_ratio": 1.6884057971014492, "no_speech_prob": 0.00033527688356116414}, {"id": 1212, "seek": 292260, "start": 2922.6, "end": 2925.7999999999997, "text": " Tomorrow we can expect that there will be many of them", "tokens": [50364, 17499, 321, 393, 2066, 300, 456, 486, 312, 867, 295, 552, 50524], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1213, "seek": 292260, "start": 2925.7999999999997, "end": 2927.36, "text": " and the day after that we can expect", "tokens": [50524, 293, 264, 786, 934, 300, 321, 393, 2066, 50602], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1214, "seek": 292260, "start": 2927.36, "end": 2928.44, "text": " they're going to be on our phones", "tokens": [50602, 436, 434, 516, 281, 312, 322, 527, 10216, 50656], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1215, "seek": 292260, "start": 2928.44, "end": 2930.72, "text": " and they're not even going to be connected to the internet.", "tokens": [50656, 293, 436, 434, 406, 754, 516, 281, 312, 4582, 281, 264, 4705, 13, 50770], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1216, "seek": 292260, "start": 2930.72, "end": 2933.6, "text": " And for that reason, I think that,", "tokens": [50770, 400, 337, 300, 1778, 11, 286, 519, 300, 11, 50914], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1217, "seek": 292260, "start": 2933.6, "end": 2936.7599999999998, "text": " like today we don't call our software microprocessor tools", "tokens": [50914, 411, 965, 321, 500, 380, 818, 527, 4722, 3123, 1513, 340, 25432, 3873, 51072], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1218, "seek": 292260, "start": 2936.7599999999998, "end": 2940.08, "text": " or microprocessor apps, like the processor just exists.", "tokens": [51072, 420, 3123, 1513, 340, 25432, 7733, 11, 411, 264, 15321, 445, 8198, 13, 51238], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1219, "seek": 292260, "start": 2940.08, "end": 2943.64, "text": " I think that one useful model, five years out,", "tokens": [51238, 286, 519, 300, 472, 4420, 2316, 11, 1732, 924, 484, 11, 51416], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1220, "seek": 292260, "start": 2943.64, "end": 2947.08, "text": " 10 years out, is to, even if it's only metaphorically true", "tokens": [51416, 1266, 924, 484, 11, 307, 281, 11, 754, 498, 309, 311, 787, 19157, 984, 2074, 51588], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1221, "seek": 292260, "start": 2947.08, "end": 2950.3199999999997, "text": " and not literally true, I think it's useful", "tokens": [51588, 293, 406, 3736, 2074, 11, 286, 519, 309, 311, 4420, 51750], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1222, "seek": 292260, "start": 2950.3199999999997, "end": 2952.12, "text": " to think of this as a second processor.", "tokens": [51750, 281, 519, 295, 341, 382, 257, 1150, 15321, 13, 51840], "temperature": 0.0, "avg_logprob": -0.09300736586252849, "compression_ratio": 1.881720430107527, "no_speech_prob": 6.812850187998265e-05}, {"id": 1223, "seek": 295212, "start": 2952.12, "end": 2955.16, "text": " We had this before with what floating point co-processors", "tokens": [50364, 492, 632, 341, 949, 365, 437, 12607, 935, 598, 12, 41075, 830, 50516], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1224, "seek": 295212, "start": 2955.16, "end": 2958.8399999999997, "text": " and graphics co-processors already as recently as the 90s", "tokens": [50516, 293, 11837, 598, 12, 41075, 830, 1217, 382, 3938, 382, 264, 4289, 82, 50700], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1225, "seek": 295212, "start": 2958.8399999999997, "end": 2961.6, "text": " where it's useful to think of the trajectory of this", "tokens": [50700, 689, 309, 311, 4420, 281, 519, 295, 264, 21512, 295, 341, 50838], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1226, "seek": 295212, "start": 2961.6, "end": 2964.6, "text": " as just another thing that computers to do can do", "tokens": [50838, 382, 445, 1071, 551, 300, 10807, 281, 360, 393, 360, 50988], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1227, "seek": 295212, "start": 2964.6, "end": 2967.04, "text": " and it will be incorporated into absolutely everything.", "tokens": [50988, 293, 309, 486, 312, 21654, 666, 3122, 1203, 13, 51110], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1228, "seek": 295212, "start": 2967.04, "end": 2969.7999999999997, "text": " Hence the term foundation model, which also crops up.", "tokens": [51110, 22229, 264, 1433, 7030, 2316, 11, 597, 611, 16829, 493, 13, 51248], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1229, "seek": 295212, "start": 2971.44, "end": 2974.0, "text": " So, pizza's ready?", "tokens": [51330, 407, 11, 8298, 311, 1919, 30, 51458], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1230, "seek": 295212, "start": 2974.0, "end": 2974.98, "text": " One more quick.", "tokens": [51458, 1485, 544, 1702, 13, 51507], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1231, "seek": 295212, "start": 2974.98, "end": 2977.72, "text": " Maybe one more and then we'll break for some food.", "tokens": [51507, 2704, 472, 544, 293, 550, 321, 603, 1821, 337, 512, 1755, 13, 51644], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1232, "seek": 295212, "start": 2980.52, "end": 2982.08, "text": " In the glasses right there, sorry.", "tokens": [51784, 682, 264, 10812, 558, 456, 11, 2597, 13, 51862], "temperature": 0.0, "avg_logprob": -0.24465979877700153, "compression_ratio": 1.56993006993007, "no_speech_prob": 7.719799759797752e-05}, {"id": 1233, "seek": 298212, "start": 2983.12, "end": 2988.56, "text": " Sorry, I was just being told we need to get two more.", "tokens": [50414, 4919, 11, 286, 390, 445, 885, 1907, 321, 643, 281, 483, 732, 544, 13, 50686], "temperature": 0.0, "avg_logprob": -0.21940176170992565, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00014648045180365443}, {"id": 1234, "seek": 298212, "start": 2988.56, "end": 2989.4, "text": " So, yeah.", "tokens": [50686, 407, 11, 1338, 13, 50728], "temperature": 0.0, "avg_logprob": -0.21940176170992565, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00014648045180365443}, {"id": 1235, "seek": 298212, "start": 2998.44, "end": 3000.08, "text": " It's hard to get it to do that reliably.", "tokens": [51180, 467, 311, 1152, 281, 483, 309, 281, 360, 300, 49927, 13, 51262], "temperature": 0.0, "avg_logprob": -0.21940176170992565, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00014648045180365443}, {"id": 1236, "seek": 298212, "start": 3000.08, "end": 3002.3599999999997, "text": " It's incredibly useful to get it to do reliably.", "tokens": [51262, 467, 311, 6252, 4420, 281, 483, 309, 281, 360, 49927, 13, 51376], "temperature": 0.0, "avg_logprob": -0.21940176170992565, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00014648045180365443}, {"id": 1237, "seek": 298212, "start": 3002.3599999999997, "end": 3005.72, "text": " So some tricks you can use are, you can give it examples.", "tokens": [51376, 407, 512, 11733, 291, 393, 764, 366, 11, 291, 393, 976, 309, 5110, 13, 51544], "temperature": 0.0, "avg_logprob": -0.21940176170992565, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00014648045180365443}, {"id": 1238, "seek": 298212, "start": 3005.72, "end": 3007.38, "text": " You can just ask it directly.", "tokens": [51544, 509, 393, 445, 1029, 309, 3838, 13, 51627], "temperature": 0.0, "avg_logprob": -0.21940176170992565, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00014648045180365443}, {"id": 1239, "seek": 298212, "start": 3008.48, "end": 3010.7599999999998, "text": " Those are two common tricks.", "tokens": [51682, 3950, 366, 732, 2689, 11733, 13, 51796], "temperature": 0.0, "avg_logprob": -0.21940176170992565, "compression_ratio": 1.6167664670658684, "no_speech_prob": 0.00014648045180365443}, {"id": 1240, "seek": 301076, "start": 3010.76, "end": 3013.2000000000003, "text": " And look at the prompts that others have used to work.", "tokens": [50364, 400, 574, 412, 264, 41095, 300, 2357, 362, 1143, 281, 589, 13, 50486], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1241, "seek": 301076, "start": 3013.2000000000003, "end": 3015.8, "text": " I mean, there's a lot of art to finding the right prompt", "tokens": [50486, 286, 914, 11, 456, 311, 257, 688, 295, 1523, 281, 5006, 264, 558, 12391, 50616], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1242, "seek": 301076, "start": 3015.8, "end": 3016.6400000000003, "text": " right now.", "tokens": [50616, 558, 586, 13, 50658], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1243, "seek": 301076, "start": 3016.6400000000003, "end": 3019.2400000000002, "text": " A lot of it is magic incantation.", "tokens": [50658, 316, 688, 295, 309, 307, 5585, 834, 394, 399, 13, 50788], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1244, "seek": 301076, "start": 3019.2400000000002, "end": 3022.44, "text": " Another thing you can do is post-process it", "tokens": [50788, 3996, 551, 291, 393, 360, 307, 2183, 12, 41075, 309, 50948], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1245, "seek": 301076, "start": 3022.44, "end": 3023.84, "text": " so that you can do some checking", "tokens": [50948, 370, 300, 291, 393, 360, 512, 8568, 51018], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1246, "seek": 301076, "start": 3023.84, "end": 3025.4, "text": " and you can have a happy path", "tokens": [51018, 293, 291, 393, 362, 257, 2055, 3100, 51096], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1247, "seek": 301076, "start": 3025.4, "end": 3027.28, "text": " in which it's a one shot and you get your answer", "tokens": [51096, 294, 597, 309, 311, 257, 472, 3347, 293, 291, 483, 428, 1867, 51190], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1248, "seek": 301076, "start": 3027.28, "end": 3029.48, "text": " and then a sad path in which maybe you fall back", "tokens": [51190, 293, 550, 257, 4227, 3100, 294, 597, 1310, 291, 2100, 646, 51300], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1249, "seek": 301076, "start": 3029.48, "end": 3030.32, "text": " on other prompts.", "tokens": [51300, 322, 661, 41095, 13, 51342], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1250, "seek": 301076, "start": 3030.32, "end": 3032.4, "text": " So then you're going for the diversity of approach", "tokens": [51342, 407, 550, 291, 434, 516, 337, 264, 8811, 295, 3109, 51446], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1251, "seek": 301076, "start": 3032.4, "end": 3034.6800000000003, "text": " where it's fast by default.", "tokens": [51446, 689, 309, 311, 2370, 538, 7576, 13, 51560], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1252, "seek": 301076, "start": 3034.6800000000003, "end": 3036.6400000000003, "text": " It's slow but ultimately converging", "tokens": [51560, 467, 311, 2964, 457, 6284, 9652, 3249, 51658], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1253, "seek": 301076, "start": 3036.6400000000003, "end": 3038.96, "text": " upon higher likelihood of success if it fails.", "tokens": [51658, 3564, 2946, 22119, 295, 2245, 498, 309, 18199, 13, 51774], "temperature": 0.0, "avg_logprob": -0.10687915328281378, "compression_ratio": 1.7737704918032786, "no_speech_prob": 0.0004581547109410167}, {"id": 1254, "seek": 303896, "start": 3039.16, "end": 3040.88, "text": " And then something that I'm sure we'll see", "tokens": [50374, 400, 550, 746, 300, 286, 478, 988, 321, 603, 536, 50460], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1255, "seek": 303896, "start": 3040.88, "end": 3044.04, "text": " and people do later on is fine-tune instruction", "tokens": [50460, 293, 561, 360, 1780, 322, 307, 2489, 12, 83, 2613, 10951, 50618], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1256, "seek": 303896, "start": 3044.04, "end": 3046.96, "text": " tuning style models, which are more likely to respond", "tokens": [50618, 15164, 3758, 5245, 11, 597, 366, 544, 3700, 281, 4196, 50764], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1257, "seek": 303896, "start": 3046.96, "end": 3049.7200000000003, "text": " with the computer parsable output.", "tokens": [50764, 365, 264, 3820, 21156, 712, 5598, 13, 50902], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1258, "seek": 303896, "start": 3049.7200000000003, "end": 3051.36, "text": " I guess one last question?", "tokens": [50902, 286, 2041, 472, 1036, 1168, 30, 50984], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1259, "seek": 303896, "start": 3051.36, "end": 3052.2, "text": " Sure.", "tokens": [50984, 4894, 13, 51026], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1260, "seek": 303896, "start": 3052.2, "end": 3055.0, "text": " So one of the, you talked a couple of things.", "tokens": [51026, 407, 472, 295, 264, 11, 291, 2825, 257, 1916, 295, 721, 13, 51166], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1261, "seek": 303896, "start": 3055.0, "end": 3057.64, "text": " One is, is you talked about domain expertise here", "tokens": [51166, 1485, 307, 11, 307, 291, 2825, 466, 9274, 11769, 510, 51298], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1262, "seek": 303896, "start": 3057.64, "end": 3060.28, "text": " and you were coding a bunch of domain expertise", "tokens": [51298, 293, 291, 645, 17720, 257, 3840, 295, 9274, 11769, 51430], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1263, "seek": 303896, "start": 3060.28, "end": 3063.04, "text": " in terms of the prompts that you're going for.", "tokens": [51430, 294, 2115, 295, 264, 41095, 300, 291, 434, 516, 337, 13, 51568], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1264, "seek": 303896, "start": 3063.04, "end": 3065.0, "text": " What is that, where do those prompts end up?", "tokens": [51568, 708, 307, 300, 11, 689, 360, 729, 41095, 917, 493, 30, 51666], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1265, "seek": 303896, "start": 3065.0, "end": 3067.48, "text": " Do those prompts end up back in the gene chat", "tokens": [51666, 1144, 729, 41095, 917, 493, 646, 294, 264, 12186, 5081, 51790], "temperature": 0.0, "avg_logprob": -0.32689836290147567, "compression_ratio": 1.7272727272727273, "no_speech_prob": 0.0005031637847423553}, {"id": 1266, "seek": 306748, "start": 3068.08, "end": 3071.64, "text": " and is there a privacy issue associated with that?", "tokens": [50394, 293, 307, 456, 257, 11427, 2734, 6615, 365, 300, 30, 50572], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1267, "seek": 306748, "start": 3071.64, "end": 3072.48, "text": " That's a great question.", "tokens": [50572, 663, 311, 257, 869, 1168, 13, 50614], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1268, "seek": 306748, "start": 3072.48, "end": 3073.68, "text": " So the question was, and I apologize,", "tokens": [50614, 407, 264, 1168, 390, 11, 293, 286, 12328, 11, 50674], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1269, "seek": 306748, "start": 3073.68, "end": 3074.84, "text": " I just realized we haven't been repeating", "tokens": [50674, 286, 445, 5334, 321, 2378, 380, 668, 18617, 50732], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1270, "seek": 306748, "start": 3074.84, "end": 3076.72, "text": " all the questions for the YouTube listeners.", "tokens": [50732, 439, 264, 1651, 337, 264, 3088, 23274, 13, 50826], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1271, "seek": 306748, "start": 3076.72, "end": 3078.12, "text": " So I'm sorry for the folks on YouTube", "tokens": [50826, 407, 286, 478, 2597, 337, 264, 4024, 322, 3088, 50896], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1272, "seek": 306748, "start": 3078.12, "end": 3080.2400000000002, "text": " if you weren't able to hear some of the questions.", "tokens": [50896, 498, 291, 4999, 380, 1075, 281, 1568, 512, 295, 264, 1651, 13, 51002], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1273, "seek": 306748, "start": 3080.2400000000002, "end": 3082.12, "text": " The question was, what are the privacy implications", "tokens": [51002, 440, 1168, 390, 11, 437, 366, 264, 11427, 16602, 51096], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1274, "seek": 306748, "start": 3082.12, "end": 3082.96, "text": " of some of these prompts?", "tokens": [51096, 295, 512, 295, 613, 41095, 30, 51138], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1275, "seek": 306748, "start": 3082.96, "end": 3086.08, "text": " If one of the messages is so much depends upon your prompt", "tokens": [51138, 759, 472, 295, 264, 7897, 307, 370, 709, 5946, 3564, 428, 12391, 51294], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1276, "seek": 306748, "start": 3086.08, "end": 3087.56, "text": " and the fine-tuning of this prompt,", "tokens": [51294, 293, 264, 2489, 12, 83, 37726, 295, 341, 12391, 11, 51368], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1277, "seek": 306748, "start": 3087.56, "end": 3089.8, "text": " what does that mean with respect to my IP?", "tokens": [51368, 437, 775, 300, 914, 365, 3104, 281, 452, 8671, 30, 51480], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1278, "seek": 306748, "start": 3089.8, "end": 3092.36, "text": " Maybe the prompt is my business.", "tokens": [51480, 2704, 264, 12391, 307, 452, 1606, 13, 51608], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1279, "seek": 306748, "start": 3092.36, "end": 3094.2, "text": " I can't offer you the exact answer", "tokens": [51608, 286, 393, 380, 2626, 291, 264, 1900, 1867, 51700], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1280, "seek": 306748, "start": 3094.2, "end": 3096.28, "text": " but I can paint for you what approximately", "tokens": [51700, 457, 286, 393, 4225, 337, 291, 437, 10447, 51804], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1281, "seek": 306748, "start": 3096.28, "end": 3097.42, "text": " the landscape looks like.", "tokens": [51804, 264, 9661, 1542, 411, 13, 51861], "temperature": 0.0, "avg_logprob": -0.11457197847422103, "compression_ratio": 1.8186968838526911, "no_speech_prob": 0.0001971231249626726}, {"id": 1282, "seek": 309742, "start": 3097.42, "end": 3100.58, "text": " So in all of software and so too with AI,", "tokens": [50364, 407, 294, 439, 295, 4722, 293, 370, 886, 365, 7318, 11, 50522], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1283, "seek": 309742, "start": 3100.58, "end": 3102.54, "text": " what we see is they're the SaaS companies", "tokens": [50522, 437, 321, 536, 307, 436, 434, 264, 49733, 3431, 50620], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1284, "seek": 309742, "start": 3102.54, "end": 3104.94, "text": " where you're using somebody else's API", "tokens": [50620, 689, 291, 434, 1228, 2618, 1646, 311, 9362, 50740], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1285, "seek": 309742, "start": 3104.94, "end": 3106.82, "text": " and you're trusting that their terms of service", "tokens": [50740, 293, 291, 434, 28235, 300, 641, 2115, 295, 2643, 50834], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1286, "seek": 309742, "start": 3106.82, "end": 3108.42, "text": " will be upheld.", "tokens": [50834, 486, 312, 493, 23504, 13, 50914], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1287, "seek": 309742, "start": 3108.42, "end": 3111.58, "text": " There's the set of companies in which they provide a model", "tokens": [50914, 821, 311, 264, 992, 295, 3431, 294, 597, 436, 2893, 257, 2316, 51072], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1288, "seek": 309742, "start": 3111.58, "end": 3113.94, "text": " for hosting on one of the big cloud providers", "tokens": [51072, 337, 16058, 322, 472, 295, 264, 955, 4588, 11330, 51190], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1289, "seek": 309742, "start": 3113.94, "end": 3116.02, "text": " and this is a version of the same thing", "tokens": [51190, 293, 341, 307, 257, 3037, 295, 264, 912, 551, 51294], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1290, "seek": 309742, "start": 3116.02, "end": 3117.7000000000003, "text": " but I think with slightly different mechanics.", "tokens": [51294, 457, 286, 519, 365, 4748, 819, 12939, 13, 51378], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1291, "seek": 309742, "start": 3117.7000000000003, "end": 3119.5, "text": " This tends to be thought of as the enterprise version", "tokens": [51378, 639, 12258, 281, 312, 1194, 295, 382, 264, 14132, 3037, 51468], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1292, "seek": 309742, "start": 3119.5, "end": 3121.26, "text": " of software and by and large,", "tokens": [51468, 295, 4722, 293, 538, 293, 2416, 11, 51556], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1293, "seek": 309742, "start": 3121.26, "end": 3123.3, "text": " the industry has moved over the past 20 years", "tokens": [51556, 264, 3518, 575, 4259, 670, 264, 1791, 945, 924, 51658], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1294, "seek": 309742, "start": 3123.3, "end": 3125.06, "text": " from running my own servers to trusting", "tokens": [51658, 490, 2614, 452, 1065, 15909, 281, 28235, 51746], "temperature": 0.0, "avg_logprob": -0.11995069354983932, "compression_ratio": 1.7287066246056781, "no_speech_prob": 0.00021645775996148586}, {"id": 1295, "seek": 312506, "start": 3125.06, "end": 3127.82, "text": " that Microsoft or Amazon or Google can run servers for me", "tokens": [50364, 300, 8116, 420, 6795, 420, 3329, 393, 1190, 15909, 337, 385, 50502], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1296, "seek": 312506, "start": 3127.82, "end": 3129.54, "text": " and they say it's my private server", "tokens": [50502, 293, 436, 584, 309, 311, 452, 4551, 7154, 50588], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1297, "seek": 312506, "start": 3129.54, "end": 3130.82, "text": " even though I know they're running it", "tokens": [50588, 754, 1673, 286, 458, 436, 434, 2614, 309, 50652], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1298, "seek": 312506, "start": 3130.82, "end": 3131.98, "text": " and I'm okay with that.", "tokens": [50652, 293, 286, 478, 1392, 365, 300, 13, 50710], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1299, "seek": 312506, "start": 3131.98, "end": 3134.66, "text": " And you've already started to see that Amazon", "tokens": [50710, 400, 291, 600, 1217, 1409, 281, 536, 300, 6795, 50844], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1300, "seek": 312506, "start": 3134.66, "end": 3136.7, "text": " with Huggingface, Microsoft with OpenAI,", "tokens": [50844, 365, 46892, 3249, 2868, 11, 8116, 365, 7238, 48698, 11, 50946], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1301, "seek": 312506, "start": 3136.7, "end": 3138.7799999999997, "text": " Google too with their own version of Bard", "tokens": [50946, 3329, 886, 365, 641, 1065, 3037, 295, 26841, 51050], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1302, "seek": 312506, "start": 3138.7799999999997, "end": 3141.66, "text": " are going to do these where you'll have the SaaS version", "tokens": [51050, 366, 516, 281, 360, 613, 689, 291, 603, 362, 264, 49733, 3037, 51194], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1303, "seek": 312506, "start": 3141.66, "end": 3143.82, "text": " and then you'll also have the private VPC version", "tokens": [51194, 293, 550, 291, 603, 611, 362, 264, 4551, 691, 12986, 3037, 51302], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1304, "seek": 312506, "start": 3143.82, "end": 3145.1, "text": " and then there's a third version", "tokens": [51302, 293, 550, 456, 311, 257, 2636, 3037, 51366], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1305, "seek": 312506, "start": 3145.1, "end": 3147.18, "text": " that I think we haven't yet seen practically emerge", "tokens": [51366, 300, 286, 519, 321, 2378, 380, 1939, 1612, 15667, 21511, 51470], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1306, "seek": 312506, "start": 3147.18, "end": 3149.14, "text": " but this would be the maximalist.", "tokens": [51470, 457, 341, 576, 312, 264, 49336, 468, 13, 51568], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1307, "seek": 312506, "start": 3149.14, "end": 3152.54, "text": " I wanna make sure my IP is maximally safe version of events", "tokens": [51568, 286, 1948, 652, 988, 452, 8671, 307, 5138, 379, 3273, 3037, 295, 3931, 51738], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1308, "seek": 312506, "start": 3152.54, "end": 3154.62, "text": " in which you are running your own machines.", "tokens": [51738, 294, 597, 291, 366, 2614, 428, 1065, 8379, 13, 51842], "temperature": 0.0, "avg_logprob": -0.13353940122616217, "compression_ratio": 1.8058823529411765, "no_speech_prob": 0.0014546985039487481}, {"id": 1309, "seek": 315462, "start": 3154.62, "end": 3156.14, "text": " You are running your own models", "tokens": [50364, 509, 366, 2614, 428, 1065, 5245, 50440], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1310, "seek": 315462, "start": 3156.14, "end": 3157.7799999999997, "text": " and then the question is,", "tokens": [50440, 293, 550, 264, 1168, 307, 11, 50522], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1311, "seek": 315462, "start": 3157.7799999999997, "end": 3160.3399999999997, "text": " is the open source and or privately available version", "tokens": [50522, 307, 264, 1269, 4009, 293, 420, 31919, 2435, 3037, 50650], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1312, "seek": 315462, "start": 3160.3399999999997, "end": 3163.2999999999997, "text": " of the model as good as the publicly hosted one", "tokens": [50650, 295, 264, 2316, 382, 665, 382, 264, 14843, 19204, 472, 50798], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1313, "seek": 315462, "start": 3163.2999999999997, "end": 3164.54, "text": " and does that matter to me?", "tokens": [50798, 293, 775, 300, 1871, 281, 385, 30, 50860], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1314, "seek": 315462, "start": 3164.54, "end": 3166.46, "text": " And the answers right now, realistically,", "tokens": [50860, 400, 264, 6338, 558, 586, 11, 40734, 11, 50956], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1315, "seek": 315462, "start": 3166.46, "end": 3168.18, "text": " it probably matters a lot.", "tokens": [50956, 309, 1391, 7001, 257, 688, 13, 51042], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1316, "seek": 315462, "start": 3168.18, "end": 3169.62, "text": " In the fullness of time,", "tokens": [51042, 682, 264, 45262, 295, 565, 11, 51114], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1317, "seek": 315462, "start": 3169.62, "end": 3172.54, "text": " you can think of any one particular task you need to achieve", "tokens": [51114, 291, 393, 519, 295, 604, 472, 1729, 5633, 291, 643, 281, 4584, 51260], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1318, "seek": 315462, "start": 3172.54, "end": 3175.62, "text": " as requiring some fixed point of intelligence to achieve.", "tokens": [51260, 382, 24165, 512, 6806, 935, 295, 7599, 281, 4584, 13, 51414], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1319, "seek": 315462, "start": 3175.62, "end": 3177.74, "text": " And so over time, what we'll see is", "tokens": [51414, 400, 370, 670, 565, 11, 437, 321, 603, 536, 307, 51520], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1320, "seek": 315462, "start": 3177.74, "end": 3179.96, "text": " the privately obtainable versions of these models", "tokens": [51520, 264, 31919, 12701, 712, 9606, 295, 613, 5245, 51631], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1321, "seek": 315462, "start": 3179.96, "end": 3181.3399999999997, "text": " will cross that threshold", "tokens": [51631, 486, 3278, 300, 14678, 51700], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1322, "seek": 315462, "start": 3181.3399999999997, "end": 3183.54, "text": " and with respect to that one task,", "tokens": [51700, 293, 365, 3104, 281, 300, 472, 5633, 11, 51810], "temperature": 0.0, "avg_logprob": -0.11480167933872767, "compression_ratio": 1.764516129032258, "no_speech_prob": 0.0006260972586460412}, {"id": 1323, "seek": 318354, "start": 3183.54, "end": 3185.2599999999998, "text": " yeah, sure, use the open source version,", "tokens": [50364, 1338, 11, 988, 11, 764, 264, 1269, 4009, 3037, 11, 50450], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1324, "seek": 318354, "start": 3185.2599999999998, "end": 3186.54, "text": " run it on your own machine,", "tokens": [50450, 1190, 309, 322, 428, 1065, 3479, 11, 50514], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1325, "seek": 318354, "start": 3186.54, "end": 3189.58, "text": " but we'll also see the SaaS intelligence get smarter.", "tokens": [50514, 457, 321, 603, 611, 536, 264, 49733, 7599, 483, 20294, 13, 50666], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1326, "seek": 318354, "start": 3189.58, "end": 3190.7799999999997, "text": " It'll probably stay ahead.", "tokens": [50666, 467, 603, 1391, 1754, 2286, 13, 50726], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1327, "seek": 318354, "start": 3190.7799999999997, "end": 3192.02, "text": " And then your question is,", "tokens": [50726, 400, 550, 428, 1168, 307, 11, 50788], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1328, "seek": 318354, "start": 3192.02, "end": 3193.62, "text": " well, which one do I care more about?", "tokens": [50788, 731, 11, 597, 472, 360, 286, 1127, 544, 466, 30, 50868], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1329, "seek": 318354, "start": 3193.62, "end": 3195.74, "text": " Do I want like the better aggregate intelligence", "tokens": [50868, 1144, 286, 528, 411, 264, 1101, 26118, 7599, 50974], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1330, "seek": 318354, "start": 3195.74, "end": 3197.7799999999997, "text": " or is my task somewhat fixed point", "tokens": [50974, 420, 307, 452, 5633, 8344, 6806, 935, 51076], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1331, "seek": 318354, "start": 3197.7799999999997, "end": 3200.02, "text": " and I can just use the open source available one", "tokens": [51076, 293, 286, 393, 445, 764, 264, 1269, 4009, 2435, 472, 51188], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1332, "seek": 318354, "start": 3200.02, "end": 3201.66, "text": " for which I know it'll perform well enough", "tokens": [51188, 337, 597, 286, 458, 309, 603, 2042, 731, 1547, 51270], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1333, "seek": 318354, "start": 3201.66, "end": 3203.18, "text": " because it's crossed the threshold.", "tokens": [51270, 570, 309, 311, 14622, 264, 14678, 13, 51346], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1334, "seek": 318354, "start": 3203.18, "end": 3205.16, "text": " So to answer your question specifically,", "tokens": [51346, 407, 281, 1867, 428, 1168, 4682, 11, 51445], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1335, "seek": 318354, "start": 3205.16, "end": 3207.58, "text": " yes, you might be glad to know", "tokens": [51445, 2086, 11, 291, 1062, 312, 5404, 281, 458, 51566], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1336, "seek": 318354, "start": 3207.58, "end": 3210.1, "text": " Chachi PT recently updated their privacy policy", "tokens": [51566, 761, 21791, 35460, 3938, 10588, 641, 11427, 3897, 51692], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1337, "seek": 318354, "start": 3210.1, "end": 3212.94, "text": " to not use prompts for the training process,", "tokens": [51692, 281, 406, 764, 41095, 337, 264, 3097, 1399, 11, 51834], "temperature": 0.0, "avg_logprob": -0.14979790718324723, "compression_ratio": 1.6837606837606838, "no_speech_prob": 0.00013971759472042322}, {"id": 1338, "seek": 321294, "start": 3212.94, "end": 3215.1, "text": " but up until now, everything went back into the bin", "tokens": [50364, 457, 493, 1826, 586, 11, 1203, 1437, 646, 666, 264, 5171, 50472], "temperature": 0.0, "avg_logprob": -0.4085332653190516, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.00029945801361463964}, {"id": 1339, "seek": 321294, "start": 3215.1, "end": 3217.66, "text": " to be trained on again, okay.", "tokens": [50472, 281, 312, 8895, 322, 797, 11, 1392, 13, 50600], "temperature": 0.0, "avg_logprob": -0.4085332653190516, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.00029945801361463964}, {"id": 1340, "seek": 321294, "start": 3217.66, "end": 3219.3, "text": " And that's just a fact.", "tokens": [50600, 400, 300, 311, 445, 257, 1186, 13, 50682], "temperature": 0.0, "avg_logprob": -0.4085332653190516, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.00029945801361463964}, {"id": 1341, "seek": 321294, "start": 3219.3, "end": 3222.2200000000003, "text": " So I think pizza is now pizza time.", "tokens": [50682, 407, 286, 519, 8298, 307, 586, 8298, 565, 13, 50828], "temperature": 0.0, "avg_logprob": -0.4085332653190516, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.00029945801361463964}, {"id": 1342, "seek": 321294, "start": 3222.2200000000003, "end": 3223.38, "text": " Okay, okay.", "tokens": [50828, 1033, 11, 1392, 13, 50886], "temperature": 0.0, "avg_logprob": -0.4085332653190516, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.00029945801361463964}, {"id": 1343, "seek": 321294, "start": 3223.38, "end": 3224.2200000000003, "text": " Okay.", "tokens": [50886, 1033, 13, 50928], "temperature": 0.0, "avg_logprob": -0.4085332653190516, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.00029945801361463964}, {"id": 1344, "seek": 321294, "start": 3224.2200000000003, "end": 3225.2200000000003, "text": " Yeah.", "tokens": [50928, 865, 13, 50978], "temperature": 0.0, "avg_logprob": -0.4085332653190516, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.00029945801361463964}, {"id": 1345, "seek": 321294, "start": 3225.2200000000003, "end": 3228.34, "text": " But we'll have to talk about Q&A's and everything else.", "tokens": [50978, 583, 321, 603, 362, 281, 751, 466, 1249, 5, 32, 311, 293, 1203, 1646, 13, 51134], "temperature": 0.0, "avg_logprob": -0.4085332653190516, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.00029945801361463964}, {"id": 1346, "seek": 321294, "start": 3228.34, "end": 3229.18, "text": " Perfect.", "tokens": [51134, 10246, 13, 51176], "temperature": 0.0, "avg_logprob": -0.4085332653190516, "compression_ratio": 1.3855421686746987, "no_speech_prob": 0.00029945801361463964}], "language": "en"}