1
00:00:00,000 --> 00:00:08,080
I want to introduce our first keynote speaker this morning who's going to talk to us about

2
00:00:08,080 --> 00:00:16,720
data in the age of AI. David Weinberger is Harvard's Brookman Klein Center for Internet

3
00:00:16,720 --> 00:00:23,600
and Society. He's participated in with that group for many years. He's an author of quite a few books,

4
00:00:24,480 --> 00:00:31,440
Everyday Chaos, Everything is Miscellaneous, Too Big to Know, and many years ago the Clutrain

5
00:00:31,440 --> 00:00:40,640
Manifesto. But he's also a columnist for KM World, so he keeps his eye on what's happening

6
00:00:40,640 --> 00:00:48,800
in the KM World. He has spoken at KM World a number of times on many different topics,

7
00:00:48,800 --> 00:00:53,280
but he hasn't for a while, so we're very happy to have him with us this year.

8
00:00:54,000 --> 00:00:57,520
And please welcome David Weinberger.

9
00:01:04,560 --> 00:01:14,800
Thank you so much. I guess I can take this off. It's wonderful to be back in this community

10
00:01:15,440 --> 00:01:21,600
and amazing and wonderful to have participants from 38 countries, and I would now like to welcome

11
00:01:21,600 --> 00:01:28,160
all of them in each of their 25 different languages, so just bear with me. Good morning,

12
00:01:28,160 --> 00:01:32,800
which is the one I've really mastered, and I'll move on from there. Sorry, I'm an American.

13
00:01:33,680 --> 00:01:40,240
So I want to talk about data in the age of AI, and I should warn you that this is a topic,

14
00:01:40,320 --> 00:01:45,120
it's part of a larger topic that I've been trying to work through for the past few years,

15
00:01:45,120 --> 00:01:47,440
and you'll see I have not fully worked it through.

16
00:01:50,560 --> 00:01:54,400
Whoops, sorry, I've got to get the right clicker. The right clicker.

17
00:01:56,400 --> 00:02:03,840
I found the right clicker. I'm not looking for your approval, but still. Thank you. Thank you very

18
00:02:03,840 --> 00:02:09,120
much. So I'm also going to include metadata, because metadata is the type of data,

19
00:02:10,960 --> 00:02:15,920
and it's really important from the beginning to me that you understand this is not a technical

20
00:02:15,920 --> 00:02:20,240
talk, it's not a talk about the technicalities of data, it's an attempt to try to follow

21
00:02:21,840 --> 00:02:29,280
how we think about data, not as data scientists, which I absolutely am not, sorry, humanities major,

22
00:02:29,280 --> 00:02:37,600
and how that might be affecting how we view our world. So don't be alarmed at all the technical

23
00:02:37,600 --> 00:02:44,320
errors, and I didn't fill in everything and the like. Sorry, enough disclaimers? So good. So this

24
00:02:44,320 --> 00:02:52,240
is Martin Heidegger, a German philosopher who died in 1976. In 1954, he wrote an essay called

25
00:02:52,240 --> 00:02:57,520
The Question Concerning Technology, except it was in German, and in it he says in a typical

26
00:02:57,520 --> 00:03:06,240
Heideggerian way, technology discloses being. Heidegger was a terrible, terrible writer. He was

27
00:03:06,240 --> 00:03:13,440
also, please do not look him up in Wikipedia. He was a horrible, horrible person, and I feel

28
00:03:13,440 --> 00:03:18,000
ashamed even introducing him. I did do my doctoral dissertation on him, but that was a long time

29
00:03:18,000 --> 00:03:24,480
ago, a very long time ago. Anyway, so my understanding, my way of understanding this is that our

30
00:03:24,480 --> 00:03:34,000
engagement with technology casts a light on the world. We see our world through what technology

31
00:03:34,000 --> 00:03:42,320
shows us of it, but it also casts a shadow, which is very prominent in how AI, which is certainly

32
00:03:42,320 --> 00:03:48,800
a two-sided technology when it comes to good and evil. It's certainly true of AI. So the sort of

33
00:03:48,800 --> 00:03:53,040
thing that I think he's talking about, but I don't really care because I want to talk about it, and

34
00:03:53,040 --> 00:04:00,480
it's not a Heidegger lecture, is that say in the 17th and 18th century, where the pinnacle technology

35
00:04:00,480 --> 00:04:08,880
was watches, which were amazing creations, incredibly complex, handmade, just mind-blowingly,

36
00:04:08,880 --> 00:04:12,720
they were the chat GPT of the time. It was unbelievable that these things could work,

37
00:04:13,440 --> 00:04:18,320
but because they were the pinnacle technology, the dominant technology, we began to see the

38
00:04:18,320 --> 00:04:24,480
universe through that lens, so to speak. So the entire universe, we started talking about the

39
00:04:24,480 --> 00:04:28,960
clockwork universe. The universe itself started to look like a clockwork, which meant that there

40
00:04:28,960 --> 00:04:37,040
were very simple rules that govern it and govern it perfectly, beautifully. This was aided and abetted

41
00:04:37,040 --> 00:04:44,160
by Newton's laws, which were the mechanisms, the mechanics of the universe. So that's an example

42
00:04:44,240 --> 00:04:50,560
of how we interpret our world through our tech. And so there's, I think, a big question that asked,

43
00:04:51,200 --> 00:04:57,520
which is, okay, so how are we going to interpret our world in the age of AI? And I'm picking a subset

44
00:04:57,520 --> 00:05:07,120
of it, which is in light of data. So I'm going to begin with a really quick overview of starting

45
00:05:07,120 --> 00:05:14,160
with mainframes working up to the internet and then AI, about how data has affected the sort of

46
00:05:14,160 --> 00:05:20,720
public idea of data has affected our view of how the world works. So starting with mainframes,

47
00:05:22,000 --> 00:05:30,160
mainframes, in the age of mainframes, data was really, really scarce, and it was a resource.

48
00:05:30,160 --> 00:05:34,320
It had to be carefully managed. And just as with the punch cards of the day,

49
00:05:35,280 --> 00:05:43,360
data was structured and it was a reduction. A typical human resources record was really

50
00:05:43,360 --> 00:05:47,600
sparse. There was hardly any information in it because of the capacity of the computers.

51
00:05:47,600 --> 00:05:52,160
And it was, of course, completely regular. Everybody's record was structured the same way.

52
00:05:55,280 --> 00:06:01,520
I don't think it's a coincidence that one of the large cultural divides of the time was between the

53
00:06:01,520 --> 00:06:09,840
IBM rep in the blue suit, absolute symbol of conformity with the rebel, the symbol of rebellion

54
00:06:09,840 --> 00:06:17,680
here. And so it looked like our culture was divided into two parts. In the 1960s, this blue suit guy

55
00:06:17,680 --> 00:06:23,280
remained, but the angry beaten it got replaced by a stoned and overly happy hippie. But it was the

56
00:06:23,280 --> 00:06:33,520
same sort of tension. And so it was the tension between conformity, individual control and spontaneity

57
00:06:33,520 --> 00:06:39,520
and reduction of information so we could manage it and part of control and wild excess often for

58
00:06:39,520 --> 00:06:45,200
its own sake. As I mentioned oddly to somebody just a few minutes ago, this does not come up very

59
00:06:45,200 --> 00:06:53,360
often, I was at the original Woodstock. So I'm talking from experience here. Oh, my mastering a

60
00:06:53,360 --> 00:06:58,320
remote doesn't count for anything, but Woodstock gets applause? What sort of group are you?

61
00:07:01,200 --> 00:07:09,120
So age of computers of PCs, as we all know, I think, what drove, well, those of us who lived

62
00:07:09,200 --> 00:07:15,840
through the era can confirm that what drove the adoption of PCs was spreadsheets, killer app,

63
00:07:15,840 --> 00:07:21,520
absolute killer app drove the hardware and spreadsheets present data in a matrix. Of

64
00:07:21,520 --> 00:07:29,280
course, I'm you're all familiar with spreadsheets. There's a model of in this case how the business

65
00:07:29,280 --> 00:07:37,280
works and the relationship among the pieces of that model. But all the way back in 1984, Stephen

66
00:07:37,280 --> 00:07:46,000
Levy, who's a remarkably good writer about tech, he's still writing very a lot, always interesting.

67
00:07:46,000 --> 00:07:55,440
1984, Levy said that what was actually interesting and important about the adoption of spreadsheets,

68
00:07:55,440 --> 00:08:01,600
reason people were adopting them so enthusiastically wasn't because they modeled business, it was

69
00:08:01,600 --> 00:08:06,720
because you could play with that model. It was the what if factor. And I think he correctly says,

70
00:08:06,800 --> 00:08:13,280
in 1984, did I mention it was 1984 when he said this? He said that it's already this ability to

71
00:08:13,280 --> 00:08:17,680
alter the model to play with it was already changing organizational structures. They're

72
00:08:17,680 --> 00:08:25,440
incredibly early and insightful. So I think there's a tremendous amount of truth in this.

73
00:08:25,440 --> 00:08:31,680
And so I'm going to pretty arbitrarily say that if you want to talk about data, the change in data

74
00:08:31,680 --> 00:08:38,480
in the era of PCs, it goes from this controlled minimize thing to still pretty minimal amount

75
00:08:38,480 --> 00:08:44,480
of data that it handled. But we want to play with it. And so data becomes detached, it's not

76
00:08:44,480 --> 00:08:48,880
just a read out of the world. It's a read out of the world that you can you can mess around with,

77
00:08:48,880 --> 00:08:55,040
you can try other worlds, you can try other forms of your business. And this liberates data from

78
00:08:55,040 --> 00:09:02,640
feeling like a direct read out of the world that data are facts that we can just read out and they

79
00:09:02,640 --> 00:09:11,200
are the reality. So then there's a sort of a micro era of big data, where one way of thinking about

80
00:09:11,200 --> 00:09:18,000
this is the data becomes a source of surprises. And the canonical example of this is the discovery

81
00:09:18,000 --> 00:09:23,120
that people who buy diapers also often buy beer. There's a correlation.

82
00:09:26,640 --> 00:09:36,960
That's our granddaughter. I know. Anyway, she doesn't drink beer. I see where I went wrong

83
00:09:36,960 --> 00:09:44,960
in this slide. We are not giving her beer. She doesn't even like beer. I mean, she likes it,

84
00:09:44,960 --> 00:09:54,160
but she's not crazy about it. Oh, Poppy, I'm sorry. Anyway, so this actually, it doesn't

85
00:09:54,160 --> 00:09:59,040
matter. It's a good example, but it's not a real example. It was actually a correlation that was

86
00:09:59,040 --> 00:10:03,760
discovered in 1992, which is generally before the era of big data. And it was not discovered through

87
00:10:03,760 --> 00:10:08,960
deep data analysis. Somebody noticed that people buy a lot of diapers and they did a sequel and

88
00:10:08,960 --> 00:10:13,920
ask you L query to see what is correlated with that. It's beer. That turns out to be

89
00:10:13,920 --> 00:10:21,120
an unlikely in reality, not really a helpful correlation. Although there is some debate

90
00:10:21,120 --> 00:10:28,080
about it. Okay. So data in the age of the internet, the next two sections are going to get a little

91
00:10:28,080 --> 00:10:34,240
bit longer. Sorry. So internet and AI. So in the age of the internet, data gets really confusing

92
00:10:35,200 --> 00:10:41,280
because it's one sense. It shows up as a type of smog that we omit, which is an unfortunate image,

93
00:10:41,280 --> 00:10:47,440
I guess. But as we're browsing and doing all the other internet things and corporations are

94
00:10:47,440 --> 00:10:52,080
gathering all that data and compiling it and manipulating it and using it against us,

95
00:10:53,760 --> 00:10:59,760
it was the overwhelming amount of data was so much that it actually seemed at times not to

96
00:10:59,840 --> 00:11:05,840
clarify things but to make where to even start unless you were a professional data analyst

97
00:11:05,840 --> 00:11:10,800
like at the platforms. So that's, I'm going to give you a few different ways, I think,

98
00:11:10,800 --> 00:11:16,560
of characterizing data in the age of the internet. At the same time that it felt like smog,

99
00:11:17,280 --> 00:11:23,360
so much and dangerous, the internet is all about links, which is sort of up from the data level.

100
00:11:23,360 --> 00:11:28,160
It's more at the information level, if you want, where links are just about the opposite. I mean,

101
00:11:28,160 --> 00:11:34,560
links are obviously and deeply human and form a structure that we can navigate at will and

102
00:11:34,560 --> 00:11:42,640
notice how things are connected. Very different from the smoggy idea of data. And so I think

103
00:11:43,760 --> 00:11:51,760
we've gotten a similar sort of polarity in which there's the smoggy. We all, I think,

104
00:11:51,840 --> 00:11:59,680
maybe not. Most of us are concerned, let's say, about the use of the data that we are

105
00:11:59,680 --> 00:12:06,480
surrendering unknowingly. Again, we click yes in order to, you know, that we've surrendered

106
00:12:06,480 --> 00:12:11,040
unwillingly. We don't know exactly what's being captured, but we know that it's being used to

107
00:12:11,040 --> 00:12:18,400
influence our decisions by people who don't have our interests usually as their interests. And on

108
00:12:18,400 --> 00:12:25,040
the other hand, links, which is 100%, individuals who are able to control and connect with others

109
00:12:26,160 --> 00:12:30,240
in an intensely social way, and both those things go on at the same time. And I think a lot of us

110
00:12:30,240 --> 00:12:39,680
have this sort of divided understanding of data in the age of AI. But we also saw, if only because

111
00:12:39,680 --> 00:12:45,040
of the gigantic amount of data that suddenly was there and seemed useful, we started unstructuring

112
00:12:45,040 --> 00:12:51,520
our databases. We did this in all sorts of ways, a semantic web with linked open data from Tim Berners-Lee,

113
00:12:51,520 --> 00:13:01,760
but also on structured databases like MongoDB and in graphs and data lakes and JSON,

114
00:13:01,760 --> 00:13:09,120
this enormous unstructuring of databases, which has enabled a lot. Okay, so I want to take,

115
00:13:09,600 --> 00:13:17,680
it's not really a detour, but I don't care. It's a detour. It's not about metadata in the

116
00:13:17,680 --> 00:13:22,480
age of the internet. It's not a detour because it actually reinforces the same point. So

117
00:13:24,160 --> 00:13:30,400
as you know, have you all tried a search engine? Because they're really amazing. If you haven't

118
00:13:30,400 --> 00:13:34,880
tried a search engine, oh, I'm telling you, you really should. There's like a guluan, a bing. So

119
00:13:34,880 --> 00:13:40,880
then you know that if you don't know who wrote Moby Dick, you can ask, it'll tell you it's Melville.

120
00:13:40,880 --> 00:13:45,440
If you don't know what Melville wrote, you could ask, it'll tell you Moby Dick and some other stuff.

121
00:13:45,440 --> 00:13:49,360
If you don't know either of those things, you can ask it about the content of Moby Dick.

122
00:13:50,640 --> 00:13:54,640
And you can even misspell the content. Call me Ishmael. Where is that? It's in Moby Dick

123
00:13:54,640 --> 00:14:00,160
by Herman Melville. So each of these questions contain a piece of metadata that linked to data.

124
00:14:00,720 --> 00:14:12,240
And that destructures metadata, which used to be a label of fix to things. And so it turns out,

125
00:14:12,240 --> 00:14:17,600
as we have discovered, that the difference between metadata and data is only functional,

126
00:14:17,600 --> 00:14:23,280
only operational. It's how you're using it. Because so that metadata is what you know. Well,

127
00:14:23,280 --> 00:14:28,160
you know it was by Herman Melville, but you don't know what the book is. And data is what you're

128
00:14:28,160 --> 00:14:33,280
looking for. Oh, it's Moby Dick. That's the only difference between them. So these, which is an

129
00:14:33,280 --> 00:14:40,560
incredibly powerful and liberating thing when everything can be metadata for something else.

130
00:14:40,560 --> 00:14:46,400
And then in turn be the thing, the data that some other metadata is looking for. So this is why it

131
00:14:46,400 --> 00:14:53,280
wasn't actually a detour. It's a, we're seeing the unstructuring of metadata, just as we are

132
00:14:53,280 --> 00:15:01,120
seeing, have seen already the unstructuring of data. So now let's talk about AI, where again,

133
00:15:01,120 --> 00:15:07,760
I'm going to have a few ways of characterizing it in a phrase, a bumper sticker. So the first is

134
00:15:09,040 --> 00:15:16,320
data is generative in the age of AI, right? And we all, we all know this because we know

135
00:15:17,040 --> 00:15:25,280
that in the old days before AI, a spreadsheet or any other sort of program generates data for sure.

136
00:15:26,480 --> 00:15:32,880
But it generates it because humans have constructed the model. Whereas with AI, as we all know,

137
00:15:32,880 --> 00:15:38,640
when I say AI, I really mean machine learning. I hope that's, I should have said that, but now I have.

138
00:15:39,200 --> 00:15:47,360
With AI, data creates the model, which is insane and seemed completely implausible until, you

139
00:15:47,360 --> 00:15:53,920
know, about 15 years ago, we found out, oh, yeah, that works. We can get more accurate classification

140
00:15:55,520 --> 00:15:59,840
of objects in photos that way, even though it makes no sense. We're not going to tell

141
00:15:59,840 --> 00:16:03,920
it anything at all about what we know about objects and how you recognize them. We'll just

142
00:16:04,000 --> 00:16:12,320
give it data. It's insane, but it works. So data creates the model, the model then generates

143
00:16:13,680 --> 00:16:18,800
new data, but it is remarkable that now data is generative this way.

144
00:16:21,520 --> 00:16:26,560
Although I've already mentioned chat GPT once and thus have fulfilled my legal requirement,

145
00:16:26,560 --> 00:16:33,600
I'm going to mention it again. So I asked it, this is about metadata. So I asked it the other day,

146
00:16:34,320 --> 00:16:39,200
Dante's Inferno has three levels. Are there any other, give me five other artworks that have three,

147
00:16:40,160 --> 00:16:45,200
that's, you know, a trio of things. And it did a good job. It's unbelievable. It's amazing.

148
00:16:46,160 --> 00:16:50,960
It's amazing because we don't have a sense of scale. Humans can't, you know, at least I can't

149
00:16:50,960 --> 00:16:54,800
think it's scale. So I'm amazed and surprised by what scale can do. But it is, of course,

150
00:16:54,800 --> 00:17:01,680
incredible. It gave me five. It gave, they're good. They gave pretty good explanations of why,

151
00:17:01,680 --> 00:17:09,600
which I hadn't even asked for. And so if we think about this in terms of metadata, the metadata in

152
00:17:09,600 --> 00:17:16,160
my query was artworks that show something in three parts. And the data that it fetched was

153
00:17:16,160 --> 00:17:23,600
that text that you just saw. And what is, I think, amazing about this is the metadata now is generating

154
00:17:23,600 --> 00:17:32,000
data. The metadata generated that content. It's not something I've seen before. And if we have,

155
00:17:32,000 --> 00:17:36,480
I'd be really interested in some other field or some other way. I'd really like to know about it.

156
00:17:37,360 --> 00:17:44,080
This is, this is mind blowing. We have metadata that will generate its own data. And generally,

157
00:17:44,720 --> 00:17:50,480
it's good. We can't, it doesn't know when the data isn't good. Basically, as you know,

158
00:17:50,480 --> 00:17:56,400
basically every, not basically, everything that chat AI says is an hallucination. It's just that

159
00:17:56,400 --> 00:18:02,560
most of those hallucinations are true hallucinations. It doesn't know that the ones that it's making up

160
00:18:02,560 --> 00:18:08,000
and we often have trouble telling. So that's a terrible problem. So let me give you an example.

161
00:18:10,080 --> 00:18:16,560
So in January 2022, researchers at the University of Leeds and some other institutions published a

162
00:18:16,560 --> 00:18:24,720
paper that said we built a model from retinal scans and some really basic medical information,

163
00:18:24,720 --> 00:18:31,520
like age, weight and the like, really a very small set of it. Does anybody know about this?

164
00:18:33,440 --> 00:18:40,480
It's sort of mind blowing. Because it works. I'm sorry. Let me be more precise. What works is

165
00:18:41,280 --> 00:18:48,160
the AI is able to predict with some degree of accuracy the likelihood of an individual

166
00:18:48,160 --> 00:18:59,200
having a heart attack, myocardial infarction, based upon the retinal scan. Nobody, data scientists,

167
00:19:00,000 --> 00:19:06,480
AI people, doctors of all stripes have tried to figure out what about those images, presumably,

168
00:19:06,480 --> 00:19:13,280
but who knows? Presumably, it's the veins, but we don't know. We can't figure out how it's doing it.

169
00:19:14,240 --> 00:19:19,920
It is, at the moment, inexplicable. And I know that there are actually bunches of people here

170
00:19:19,920 --> 00:19:27,520
working on making machine learning less inexplicable at dinner with Beth Truden and a bunch of other

171
00:19:27,520 --> 00:19:32,800
people at that table last night where this was a lively topic of conversation and Beth's company

172
00:19:32,800 --> 00:19:40,560
has a way of keeping the sources, the citations and sources of the knowledge

173
00:19:41,280 --> 00:19:47,920
with the output and is generating it from a more fact-based and reliable set of information.

174
00:19:47,920 --> 00:19:54,240
Is that approximately right? Okay. It's approximately right. There's tons and tons of work in all

175
00:19:54,240 --> 00:20:04,480
areas to try to make AI less inexplicable. But as it stands, let me put it like this,

176
00:20:04,480 --> 00:20:10,560
that inexplicability is one of the two original sins of AI, by which I mean,

177
00:20:12,400 --> 00:20:20,000
and I know the sentence doesn't actually make sense, but it may make sense. You'll be the judge.

178
00:20:20,000 --> 00:20:25,680
So left on its own, AI would tend towards inexplicability. There are interventions and

179
00:20:25,680 --> 00:20:30,480
structurings of all kinds that we might be able to do to prevent that, but it doesn't care if we

180
00:20:30,480 --> 00:20:36,320
understand that has not been its mission. It's this mission has been to give accurate predictions

181
00:20:36,320 --> 00:20:41,360
based upon data guided by a ton of human decisions about what we meet, what we're looking for and

182
00:20:41,360 --> 00:20:51,680
what we will accept. So inexplicability is pretty common so far in AI models. Beth, how much trouble

183
00:20:51,680 --> 00:20:57,920
am I in? You'll tell me afterwards. Okay. So in this regard, I think a second formulation of data

184
00:20:57,920 --> 00:21:09,360
in the age of AI would be to say that, okay, we'll do it by hand. I have forgotten how to use my

185
00:21:09,360 --> 00:21:23,360
clicker. A few moments, it was great. Okay. That AI, data in the era of AI is a source of secrets,

186
00:21:23,360 --> 00:21:28,160
like, oh, there's secret information in a retina that can let us see what's going on with the left

187
00:21:28,160 --> 00:21:34,160
ventricle, which is an indicator of heart health. Didn't know it. It's there, but it's secret. But

188
00:21:34,240 --> 00:21:42,400
actually, I would think I would prefer the formulation that data in the era of AI is a

189
00:21:42,400 --> 00:21:46,800
keeper of mysteries, because secrets, once you know them, generally, you know why you know them,

190
00:21:46,800 --> 00:21:53,280
how you know them, why they're true and all that. When mysteries, you know, but it remains a mystery

191
00:21:53,360 --> 00:22:01,040
how it happened. And so far, that seems to be at least some of what AI does.

192
00:22:02,160 --> 00:22:07,040
So let's for the moment, we're going to just overstate and say AI tends to be inexplicable

193
00:22:08,320 --> 00:22:16,000
at the moment. And we can argue later, we won't. And then we can play the five-year-old game.

194
00:22:16,000 --> 00:22:21,200
Now, the baby you saw, we do have a five-year-old grandchild as well. So that little drunken baby

195
00:22:21,200 --> 00:22:26,480
that you saw would not be doing this, but she will be in a few years. So you can ask why. Just

196
00:22:26,480 --> 00:22:31,360
keep asking why. So why isn't it inexplicable? And the answer is because the model is just too

197
00:22:31,360 --> 00:22:35,840
complex. Okay, why? Why is the model too complex? Because there are too many factors, there are

198
00:22:35,840 --> 00:22:40,960
too many variables, there's too much data that's connecting to too many others. You know, ChatGPT

199
00:22:40,960 --> 00:22:48,960
has 175 billion parameters, which are weights, weighting the relationship, the importance of the

200
00:22:49,680 --> 00:22:57,200
relationship of words, 175 billion. So if you want to know why it chose one word over another,

201
00:22:57,200 --> 00:23:02,960
why it called a house you're looking at luxurious rather than upscale, you're never going to know,

202
00:23:02,960 --> 00:23:06,320
at least at this point, you're not going to know, there's just too much going on there.

203
00:23:07,120 --> 00:23:13,360
So, okay, well, why are there so many factors connecting to too many others? Well, that's a

204
00:23:13,360 --> 00:23:22,960
good question. Because actually, that's how the world works. The world is really complex. I mean,

205
00:23:22,960 --> 00:23:29,840
the universe is the single most complex thing in the universe. And it is really, really complex.

206
00:23:29,840 --> 00:23:34,960
And the reason that these complex models work is that they are capturing something about the

207
00:23:34,960 --> 00:23:40,960
complexity of the world that we live in. They go wrong in all sorts of ways, they're dangerous and

208
00:23:41,040 --> 00:23:46,640
how they go wrong. But when they work, they're capturing something mysterious about the world

209
00:23:46,640 --> 00:23:52,400
we live in, which is there's so much stuff and everything affects everything else all the time

210
00:23:52,400 --> 00:24:02,240
forever, everywhere, everything, everything. It's simultaneously. It's not like a clock. If only

211
00:24:02,320 --> 00:24:11,520
it was like a clock, but it's not. It's intensely beyond imagination complex in its interrelationships.

212
00:24:12,560 --> 00:24:18,400
Okay, so why didn't we notice this before? And lots of people have, I think we all have,

213
00:24:18,960 --> 00:24:26,400
it's not news that the universe is complex, but it doesn't register. And I think it's

214
00:24:26,400 --> 00:24:30,880
for two reasons. So, I mean, the sort of complexity I have in mind is, and this is a relatively simple

215
00:24:30,880 --> 00:24:39,040
case, what determined why those people were going to be in the crosswalk with you this morning,

216
00:24:39,040 --> 00:24:46,480
exactly where they are. There's no hope of figuring that out. It's way, way too complex. It's too

217
00:24:46,480 --> 00:24:54,160
complex for any one person. So why haven't we done anything? Why haven't we come to grips with this?

218
00:24:54,160 --> 00:24:58,080
Why isn't this the baseline of all of our thought and thinking? Well, I think for one thing,

219
00:24:58,080 --> 00:25:02,880
it's because we couldn't do anything with that sort of information. And so we just ignored it.

220
00:25:03,680 --> 00:25:08,800
Generally, we said, well, no, that's an accident. It's just, oh, who knows? It's chance. It's

221
00:25:08,800 --> 00:25:14,560
coincidence. We have a whole vocabulary for dismissing complex, the results of complex

222
00:25:14,560 --> 00:25:21,040
interactions is not worth our attention because we couldn't do anything with it. And second of all,

223
00:25:21,040 --> 00:25:27,360
because it didn't fit well with our old human models. So I'm using our here as the West.

224
00:25:28,320 --> 00:25:35,200
I just have to limit my domain because it's basically the only thing I know enough about

225
00:25:35,200 --> 00:25:43,760
to be talking about, any legs to stand on. So in the West, this view does not fit very well

226
00:25:43,760 --> 00:25:50,080
because for thousands of years, if we take the Greek ancient Greece as the origins of

227
00:25:50,080 --> 00:25:56,960
Western civilization, which is controversial, but traditional to do that. Back then, the idea was

228
00:25:56,960 --> 00:26:00,560
there's all this mess in the world that seems chaotic, but underneath it, there are laws and

229
00:26:00,560 --> 00:26:04,720
the laws are simple and understandable. There are laws, there are rules, there's universals,

230
00:26:05,280 --> 00:26:11,200
there are principles, there are overall, there are generalizations. That's what we hang on to

231
00:26:11,200 --> 00:26:19,120
because we can use those. So in the West, traditionally, we have viewed generalizations

232
00:26:19,120 --> 00:26:25,760
of various forms. They explain what's explained, which is the particulars. This is a chart of why

233
00:26:25,760 --> 00:26:34,080
we have preferred the general because they are generalizations reduce information. They're simpler

234
00:26:34,640 --> 00:26:38,640
and it just so happens that the laws of the universe happen to be simple enough for humans

235
00:26:38,640 --> 00:26:44,320
to understand what a coincidence, but they simplify something that's very complex, the realm of

236
00:26:46,000 --> 00:26:52,400
particulars. We have thought that these laws are eternal. They've always held. They explained

237
00:26:52,400 --> 00:26:56,080
everything going back to the Big Bang, even before we knew about a Big Bang.

238
00:26:57,440 --> 00:27:02,560
And ultimately, they are the truth. In our tradition, the Western tradition, we have

239
00:27:02,560 --> 00:27:07,920
looked up into the skies for the eternal truths. In the case of the ancient Greeks, more or less

240
00:27:07,920 --> 00:27:15,280
literally, we still do it. We valorize the eternal over the particulars, which are sort of just,

241
00:27:15,280 --> 00:27:20,080
they're over like that. They change all the time. There's no real abiding truth in them.

242
00:27:20,400 --> 00:27:28,880
But I think that we are entering an age with the age of AI. For me, I think the most important

243
00:27:28,880 --> 00:27:35,840
sort of change in how we view things is that we are getting a more particularized view. We're

244
00:27:35,840 --> 00:27:45,040
taking particulars more seriously. We are letting them have voice with models themselves. I should

245
00:27:45,280 --> 00:27:50,800
pay attention to my slides. The models themselves, machine learning models, being literally

246
00:27:50,800 --> 00:27:55,760
generalizations. But they're generalizations that are made up of patterns that have been derived

247
00:27:55,760 --> 00:28:00,000
from particulars. Those patterns can be so small in particular that there are billions of them

248
00:28:00,000 --> 00:28:04,560
that get sorted through and generalized in various ways. But they stay true to letting

249
00:28:04,560 --> 00:28:12,240
the particulars speak. And this is why machine learning can make, can sort animal photos better

250
00:28:12,240 --> 00:28:20,160
than handwritten code has been able to. Okay, so if we are going to get more used to the idea of

251
00:28:20,160 --> 00:28:26,800
particulars as being real and important and in some ways determinative and having their own voice,

252
00:28:27,440 --> 00:28:34,800
then how will that happen? And I think here it may be because of the light that is cast

253
00:28:35,760 --> 00:28:43,040
by, by the shadows that are cast by AI. It's our fear. So I want to give two examples.

254
00:28:43,040 --> 00:28:47,920
One is a common sort of fear gets expressed variously, but we don't know how it works,

255
00:28:47,920 --> 00:28:53,760
which can be genuinely scary and important to recognize, right? But it, there's two words at

256
00:28:53,760 --> 00:28:58,880
the end of the sentence that are really crucial. So when you hear, we don't know how it works,

257
00:28:58,880 --> 00:29:07,360
we also hear it works, which is amazing. So this fear may be moving us towards an embrace

258
00:29:07,360 --> 00:29:13,040
of the particular because we then hear when we ask, well, why don't we know? We're told

259
00:29:13,040 --> 00:29:18,960
because it's way too complex. Well, in hearing, we don't know how it works. We may be being led to

260
00:29:18,960 --> 00:29:24,640
believe, not on purpose, but being led to believe that it works because the world is also wildly,

261
00:29:24,640 --> 00:29:33,120
complexly particular. So I want to give you a slight example of what it might mean to understand

262
00:29:33,120 --> 00:29:40,400
something outside of the realm of AI in terms of particularity. So in this case, I picked a simple

263
00:29:40,400 --> 00:29:44,080
little topic, morality. I think I have like three slides and we'll be done with morality and that

264
00:29:44,080 --> 00:29:48,880
will be great. So typically in the West, traditionally in the West, we have had, we've resorted to

265
00:29:48,880 --> 00:29:55,920
ethical frameworks. So religious framework says, do what God commands you to do. Reason one says,

266
00:29:55,920 --> 00:30:02,400
do follow principles that are based in reason. And then the utilitarian one says, the framework says,

267
00:30:02,400 --> 00:30:06,160
do that which will bring the most happiness to the most people. Pretty rough, but you know what I

268
00:30:06,160 --> 00:30:10,480
mean. So it's a framework. It tells you why some things are good and some things are bad. And it

269
00:30:10,480 --> 00:30:16,000
tells you what to do in particular situations, except they don't. They don't work. I'm going to

270
00:30:16,080 --> 00:30:25,520
give you the world's briefest and simplest example, I think. So you're you, you have a friend, A,

271
00:30:25,520 --> 00:30:31,600
who is angry, tells you that they're angry at your mutual friend, B. But A says, but don't tell

272
00:30:31,600 --> 00:30:37,520
anybody about it. Shortly thereafter, your mutual friend C comes along and says, oh, I'm very excited.

273
00:30:37,520 --> 00:30:43,520
I'm putting together the seating plan for our wedding. And I'm going to put C here. I'm going

274
00:30:43,760 --> 00:30:47,360
I know A and B are such good friends. I want to put them next to each other. Do you tell

275
00:30:48,800 --> 00:30:54,240
do you tell C that that's a bad idea because of the fight? I don't know. And I'm going to say,

276
00:30:54,240 --> 00:30:58,720
neither do you, because we don't know the particulars. You're going to think about this,

277
00:30:58,720 --> 00:31:05,600
and you're going to think, well, yeah, how vindictive is A? Is A forgiving? Would A understand if I

278
00:31:05,600 --> 00:31:12,080
didn't explain why? How deep is the rift between A and B? How how upset is C going to be if there's

279
00:31:12,080 --> 00:31:17,920
some minor tension between two people at their wedding and so forth? Without that, you don't

280
00:31:17,920 --> 00:31:23,840
know what to do. And considering what to do morally, you have to think about the details. In fact,

281
00:31:23,840 --> 00:31:29,520
if weirdly, the next next week, you run into different set of friends who have exactly the

282
00:31:29,520 --> 00:31:33,360
same sort of formal thing that one's angry and don't tell and the rest of it, but it has

283
00:31:33,360 --> 00:31:37,600
their different people and it's not a wedding. It's whatever. It's a camping trip.

284
00:31:38,240 --> 00:31:44,800
What you did in the first case is not going to help you decide what to do in the second case.

285
00:31:44,800 --> 00:31:49,680
You've got to rethink the particulars all again. That's what you would do. You would say, well,

286
00:31:49,680 --> 00:31:55,440
I didn't, I violated my promise to A because A is forgiving, but C, oh, C would never forgive me

287
00:31:55,440 --> 00:32:01,920
if I did that, for example. It comes down to particulars. So the philosopher Martha Nussbaum,

288
00:32:01,920 --> 00:32:07,440
1999, in a book called Love's Knowledge, which is a serious philosophy work with one of the

289
00:32:07,440 --> 00:32:13,200
great titles and very apt title. Anyway, she talks about this and she says moral situations

290
00:32:13,200 --> 00:32:19,120
are not commensurable. You can't compare them. And that's because they are so particular.

291
00:32:19,120 --> 00:32:27,840
So the second example is of a fear that may be telling us inadvertently what the world under AI

292
00:32:27,840 --> 00:32:34,960
type of data is. So it's like what the light is that's being shown. So AI is biased. It is biased.

293
00:32:34,960 --> 00:32:44,800
It is the second of its original sins. Left on its own, AI will be biased. We have to guard against

294
00:32:44,800 --> 00:32:50,400
it. It's very hard to guard against it, as I'm sure everybody here knows, but we can ask, okay,

295
00:32:50,400 --> 00:32:55,840
well, why is it biased? We're told this and then it's explained to us why it's biased because

296
00:32:55,840 --> 00:33:03,040
data tends to reflect societal biases and the like. And what we hear from this is, oh,

297
00:33:03,040 --> 00:33:08,240
oh, we get to select the data. So data is human stuff. It's not a readout of the world. It's

298
00:33:08,240 --> 00:33:14,560
not the facts about the world. Facts aren't exactly facts either. Different topic. It's stuff that we

299
00:33:14,560 --> 00:33:21,280
decide, data is stuff that we read off of meters, meters that we have decided to plant. Where and

300
00:33:21,280 --> 00:33:25,840
why and to what degree of accuracy and what we do with the data are all human decisions.

301
00:33:26,960 --> 00:33:29,760
If I say data is human stuff, I don't mean it's

302
00:33:32,240 --> 00:33:37,520
unreliable, but I sort of mean it's unreliable that there's a human element of decision

303
00:33:37,520 --> 00:33:43,360
which includes unconscious biases and occasionally conscious, but generally the issue in AI is

304
00:33:43,360 --> 00:33:50,720
unconscious, unaware biases of what's going on, how what we're doing might be taken in any of the

305
00:33:51,280 --> 00:33:54,960
38 other countries other than the one that we happen to be in right now, for example.

306
00:33:56,480 --> 00:34:01,760
Data is human. It's human stuff, obviously. Okay, so how does that lead to anything? Well,

307
00:34:01,760 --> 00:34:07,840
it may be that in the discussion of bias, we get to the point of proxies because the first

308
00:34:07,840 --> 00:34:12,160
response is, well, let me just don't record if you worried about bias against women or whatever

309
00:34:12,160 --> 00:34:17,360
protected class or whatever you want to call it, then don't collect, don't have a column for women

310
00:34:17,360 --> 00:34:22,240
and then you have to explain, well, no, but there are proxies for women in this case.

311
00:34:24,320 --> 00:34:33,360
That's, proxies are really interesting, at least I think so, in part because it's proxies work by

312
00:34:34,960 --> 00:34:39,120
putting shape around the whole, the thing you're trying not to have affect your,

313
00:34:39,120 --> 00:34:43,120
it's like the missing piece in the jigsaw puzzle. You can see what that shape is and that can be

314
00:34:43,280 --> 00:34:48,240
part, that becomes in a sense part of the data. I know I'm not putting this technically correctly,

315
00:34:48,240 --> 00:34:53,200
but I'm trying to talk about how this will perhaps appear to people who are not technical.

316
00:34:54,640 --> 00:35:02,480
And there are only proxies because things are so interrelated. They're so interrelated that the

317
00:35:02,480 --> 00:35:12,560
absence of something can be the presence of something. That's maybe one way through this fear

318
00:35:12,560 --> 00:35:19,520
justifiable and terrible and correctly terrible fear of bias. You don't have to, I got myself

319
00:35:19,520 --> 00:35:25,360
started on it, I'm going to back away from it. That this, the explanation of it, the understanding

320
00:35:25,360 --> 00:35:31,600
of the first question, well, why is there bias? Why can't you just lead you to understand the deep,

321
00:35:32,160 --> 00:35:38,960
deep, multi-dimensional interrelationship of all of the data and all of the world?

322
00:35:39,520 --> 00:35:44,160
So if we ask what, the original question of this talk, which is,

323
00:35:46,400 --> 00:35:51,520
if tech casts light and shadow on the world, can we think about how our world is already beginning

324
00:35:51,520 --> 00:36:01,120
to look in the light of this change in data? So way too short what I'm about to say, I understand

325
00:36:01,120 --> 00:36:04,960
that, but it's just to give you a sense of it. So if you look at an enterprise in light of particulars,

326
00:36:05,920 --> 00:36:11,120
I think that we begin to see black swans, you all know black swans, you all know black swans,

327
00:36:11,120 --> 00:36:19,360
the unexpected things that happen, drop out of the sky and destroy a supplier's factory

328
00:36:19,360 --> 00:36:23,200
in your supply chain and suddenly your business is in great danger.

329
00:36:24,640 --> 00:36:32,320
Literal lightning struck. Yes, that's right, I think there are black swans, but look that through

330
00:36:32,320 --> 00:36:38,000
in this light, in the light of AI and data, everything's a black swan. That's the nature of

331
00:36:38,000 --> 00:36:46,640
particulars. You can't, it's not just the big events, everything that happens is a mashup of

332
00:36:46,640 --> 00:36:52,960
everything that happens all at once. So everything's a black swan and except some things are

333
00:36:52,960 --> 00:36:58,080
butterflies in the chaos theory sense in which other butterfly alights on a plant in Brazil and

334
00:36:58,080 --> 00:37:04,480
triggers a tornado in Kansas, right, the standard example. It seems pretty implausible, but the

335
00:37:04,480 --> 00:37:12,080
idea is very validated and sound, which is that a small event can create cascades by which it picks

336
00:37:12,080 --> 00:37:19,600
up energy and has surprising and important results. And by the way, it's really hard to go backwards

337
00:37:19,600 --> 00:37:25,120
from the tornado to pin it on that damn butterfly and sort of pin it to a wall for what it did.

338
00:37:25,840 --> 00:37:29,920
That would be, often that's what we're trying to do with AI to understand it.

339
00:37:31,440 --> 00:37:38,240
Okay, so more specifically, but still not very specific, this sort of thinking I would imagine

340
00:37:38,240 --> 00:37:43,920
should have important effects on really important business topics beyond pinning butterflies,

341
00:37:44,480 --> 00:37:48,880
including strategy, obviously, if we're living in this sort of chaotic world in which

342
00:37:48,880 --> 00:37:54,480
each particular affects every other throughout the universe basically, then strategic thinking

343
00:37:54,480 --> 00:38:00,560
takes on a different tone, especially in terms of it's committing people to long term strategies,

344
00:38:01,680 --> 00:38:08,560
design of products, managing people, all of these things, and more, it seems to me

345
00:38:09,680 --> 00:38:16,560
can get rethought in the light of particulars. And then moving from business to life, I know

346
00:38:16,560 --> 00:38:21,840
that business is part of life, but broader. So we talked, I talked about morality,

347
00:38:24,080 --> 00:38:29,680
excuse me, but exactly the same sorts of considerations that you have to look at

348
00:38:29,680 --> 00:38:35,440
the particulars I think holds for every decision that we make where we realize we're making a

349
00:38:35,440 --> 00:38:39,600
decision. So there are decisions I'm making about waving my hands now, which I'm not doing

350
00:38:39,600 --> 00:38:44,480
with any conscious awareness of, but decisions that we actually deliberate on even a little bit,

351
00:38:44,480 --> 00:38:48,560
whether it's what items to pick up from the lovely buffet for breakfast,

352
00:38:49,200 --> 00:38:54,400
too much more important things. Decision making is also, like morality,

353
00:38:55,680 --> 00:39:01,680
all in the particulars. I mean, the muffins looked really good, the croissants look good,

354
00:39:01,680 --> 00:39:06,160
but were they? I need to crinkle them first, and then I'm going to do sort of a carb,

355
00:39:06,800 --> 00:39:14,080
sort of a balance, and what did I have last night? It's particulars, all decision making, and,

356
00:39:14,080 --> 00:39:25,040
you know, love. We don't love our loved ones in general. I don't, that's a very weird thought.

357
00:39:25,040 --> 00:39:28,640
We love them for who they are, and who they are as particulars, and that

358
00:39:29,600 --> 00:39:40,720
year and a half old baby is extremely particular in both ways. So I think this line of thought,

359
00:39:40,800 --> 00:39:45,280
this is actually what I've been working on, and more or less consumes me,

360
00:39:46,480 --> 00:39:51,520
is how this changes our ideas about creativity, which is particularly relevant in the age of

361
00:39:51,520 --> 00:39:58,560
generative AI. I think it changes our ideas about free will, which I know is not something that we

362
00:39:58,560 --> 00:40:05,040
generally talk or even think about, but it is a background concept of considerable lineage in

363
00:40:05,040 --> 00:40:12,960
the West. And I think the model that we are seeing, the light that AI is casting on a role,

364
00:40:13,600 --> 00:40:20,400
should have us rethinking the age-old impossible argument about free will. Knowledge for sure.

365
00:40:21,360 --> 00:40:25,920
I hope I don't have to say anything more. I mean, what does it mean to know in an age where

366
00:40:25,920 --> 00:40:32,080
particulars dominate where we will continue to want to generalize? Of course, there's a

367
00:40:32,080 --> 00:40:37,600
rhythm here, but one in which the particulars have become more dominant and recognized than they

368
00:40:37,600 --> 00:40:45,440
have is, I think, a really important question. And I actually think that many of you here at

369
00:40:45,440 --> 00:40:51,840
KM World are managing that question, although not in the terms I'm trying to push on you.

370
00:40:53,600 --> 00:41:00,480
I'm going to guess that much of your life as KM people is, in fact, engaged very practically

371
00:41:00,480 --> 00:41:06,480
with this question. Mind and body, I'm going to guess, is not a burning, you know, the relation

372
00:41:06,480 --> 00:41:11,520
to mind and body. It's probably not a burning question for everyone, but it's, again, a key

373
00:41:12,480 --> 00:41:16,240
shaping and background thought in the Western culture. And even the nature of reality, which

374
00:41:16,240 --> 00:41:22,400
has become, to everyone's surprise, it's not a topic that has been on the top 10 list in philosophy

375
00:41:22,720 --> 00:41:29,760
for a while, but with the rise of simulations suddenly, thanks to AI, and the questions in

376
00:41:30,560 --> 00:41:37,920
many of the questions of simulations also have to do with how particulars show themselves,

377
00:41:38,800 --> 00:41:47,520
rather than about big generalizations. So, finally, data in light of AI.

378
00:41:48,080 --> 00:41:53,520
I think that we recognize it's a human artifact. It's something we make. It's

379
00:41:53,520 --> 00:41:58,480
something we participate in making anyway, by the decisions that we make, and then what we do with

380
00:41:58,480 --> 00:42:04,560
that data as well. We can't just accept it as we did in the 1950s, which was part of the weight

381
00:42:04,560 --> 00:42:10,960
that was on that poor, who was the guy in that, the actor on the left?

382
00:42:11,040 --> 00:42:11,680
Gregory Peck.

383
00:42:12,800 --> 00:42:13,200
Is that good?

384
00:42:14,640 --> 00:42:16,240
Exactly right. Gregory Peck, thank you.

385
00:42:19,520 --> 00:42:23,840
I forgot where I was going, but I now know it was Gregory Peck. Appreciate that.

386
00:42:28,720 --> 00:42:38,320
Anyway, so it's, Gregory Peck took data as the bedrock, that is, the IBM generation.

387
00:42:38,320 --> 00:42:42,960
Took data as a bedrock. Now we see how much of human decision making, how much of human

388
00:42:44,880 --> 00:42:53,120
assumptions plays in the creation of data, and how we use it in AI especially, directly,

389
00:42:54,000 --> 00:42:59,280
is the responsibility of humans who make decisions about it, even if they shirk those decisions.

390
00:42:59,280 --> 00:43:04,960
And thankfully, most people that I know who are doing AI take those decisions really seriously

391
00:43:04,960 --> 00:43:10,240
now, but it's still an enormous problem. Seeing data as a black box of relationships,

392
00:43:10,240 --> 00:43:18,960
from which we withdraw what we need as human artifacts. And I think the pithiest way I can put

393
00:43:18,960 --> 00:43:26,720
it is, if particulars are becoming more important to us outside of the world of AI, but in part

394
00:43:26,720 --> 00:43:34,240
because of AI, then I think we can think about data as particulars. But there are particulars

395
00:43:34,240 --> 00:43:37,840
that have been rendered machine learning. They gain something from that, I'm sorry,

396
00:43:37,840 --> 00:43:41,360
from machine readable. They gain something from that, but they also learn something.

397
00:43:41,360 --> 00:43:49,040
They gain some capabilities from that. So I think we're in a world in which

398
00:43:51,760 --> 00:43:57,760
particulars are rising. I'm not sure if it's obvious. I think this is a really important

399
00:43:57,760 --> 00:44:05,600
corrective to a Western focus on generalizations. It's because particulars are a reality.

400
00:44:06,960 --> 00:44:08,960
So, thank you very much.

